{
  "title": "Deep Learning in EEG-Based BCIs: A Comprehensive Review of Transformer Models, Advantages, Challenges, and Applications",
  "url": "https://openalex.org/W4388240351",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2425650114",
      "name": "Berdakh Abibullaev",
      "affiliations": [
        "Nazarbayev University"
      ]
    },
    {
      "id": "https://openalex.org/A4384100861",
      "name": "Aigerim Keutayeva",
      "affiliations": [
        "Nazarbayev University"
      ]
    },
    {
      "id": "https://openalex.org/A84679562",
      "name": "Amin Zollanvari",
      "affiliations": [
        "Nazarbayev University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2164497299",
    "https://openalex.org/W2169866873",
    "https://openalex.org/W2025252425",
    "https://openalex.org/W4280597469",
    "https://openalex.org/W2145947508",
    "https://openalex.org/W2794345050",
    "https://openalex.org/W3092342532",
    "https://openalex.org/W2963355311",
    "https://openalex.org/W2119163516",
    "https://openalex.org/W3214897310",
    "https://openalex.org/W2144910141",
    "https://openalex.org/W2087704839",
    "https://openalex.org/W2135825876",
    "https://openalex.org/W2147854680",
    "https://openalex.org/W2901024347",
    "https://openalex.org/W1967765022",
    "https://openalex.org/W2061862684",
    "https://openalex.org/W2045561515",
    "https://openalex.org/W2332563861",
    "https://openalex.org/W2146222196",
    "https://openalex.org/W2065045219",
    "https://openalex.org/W2766672259",
    "https://openalex.org/W4313461267",
    "https://openalex.org/W3011419281",
    "https://openalex.org/W3217060357",
    "https://openalex.org/W2105953877",
    "https://openalex.org/W1968417898",
    "https://openalex.org/W6774336087",
    "https://openalex.org/W2908826490",
    "https://openalex.org/W3167195439",
    "https://openalex.org/W4281662280",
    "https://openalex.org/W4312193421",
    "https://openalex.org/W4285138439",
    "https://openalex.org/W6910546390",
    "https://openalex.org/W3091225957",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W6685893538",
    "https://openalex.org/W4286581999",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W2557301950",
    "https://openalex.org/W2915893085",
    "https://openalex.org/W4289822162",
    "https://openalex.org/W3206495896",
    "https://openalex.org/W2920993277",
    "https://openalex.org/W4384158208",
    "https://openalex.org/W4327642337",
    "https://openalex.org/W3154435685",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6766904570",
    "https://openalex.org/W6762287338",
    "https://openalex.org/W6797674293",
    "https://openalex.org/W4379805270",
    "https://openalex.org/W4379055511",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4289538860",
    "https://openalex.org/W4225725965",
    "https://openalex.org/W4291910522",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2103494817",
    "https://openalex.org/W2048777612",
    "https://openalex.org/W1971412067",
    "https://openalex.org/W2093534731",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2154777661",
    "https://openalex.org/W1973275441",
    "https://openalex.org/W2938320352",
    "https://openalex.org/W2054294508",
    "https://openalex.org/W2534443592",
    "https://openalex.org/W2128495200",
    "https://openalex.org/W2116308679",
    "https://openalex.org/W2106822551",
    "https://openalex.org/W4250543537",
    "https://openalex.org/W1998302573",
    "https://openalex.org/W2492102122",
    "https://openalex.org/W3138947062",
    "https://openalex.org/W2039621768",
    "https://openalex.org/W2976149155",
    "https://openalex.org/W3018645360",
    "https://openalex.org/W2110863214",
    "https://openalex.org/W3005093966",
    "https://openalex.org/W2066527749",
    "https://openalex.org/W3084230554",
    "https://openalex.org/W3015212512",
    "https://openalex.org/W2998106320",
    "https://openalex.org/W2793531120",
    "https://openalex.org/W2948418702",
    "https://openalex.org/W2077189121",
    "https://openalex.org/W3020978940",
    "https://openalex.org/W2032492549",
    "https://openalex.org/W2566775857",
    "https://openalex.org/W3083582638",
    "https://openalex.org/W3171527412",
    "https://openalex.org/W4296022954",
    "https://openalex.org/W4206239953",
    "https://openalex.org/W2904408089",
    "https://openalex.org/W2767033786",
    "https://openalex.org/W4385892652",
    "https://openalex.org/W4285135037",
    "https://openalex.org/W6772310668",
    "https://openalex.org/W4385453220",
    "https://openalex.org/W2105957367",
    "https://openalex.org/W2098330912",
    "https://openalex.org/W2158956772",
    "https://openalex.org/W1990719440",
    "https://openalex.org/W2106006415",
    "https://openalex.org/W2106480861",
    "https://openalex.org/W3212769229",
    "https://openalex.org/W2030358371",
    "https://openalex.org/W1999841683",
    "https://openalex.org/W2168217710",
    "https://openalex.org/W6683994600",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3150989405",
    "https://openalex.org/W3035442710",
    "https://openalex.org/W4200240231",
    "https://openalex.org/W3133316833",
    "https://openalex.org/W3194668998",
    "https://openalex.org/W4292731283",
    "https://openalex.org/W4386175841",
    "https://openalex.org/W4312892255",
    "https://openalex.org/W4377138983",
    "https://openalex.org/W4383815027",
    "https://openalex.org/W4312597583",
    "https://openalex.org/W4380355219",
    "https://openalex.org/W2925836809",
    "https://openalex.org/W4328028519",
    "https://openalex.org/W4312705096",
    "https://openalex.org/W4385299413",
    "https://openalex.org/W3177342940",
    "https://openalex.org/W4321066998",
    "https://openalex.org/W2128404967",
    "https://openalex.org/W4312508188",
    "https://openalex.org/W2912885887",
    "https://openalex.org/W4361186050",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W4205466227",
    "https://openalex.org/W4385064726",
    "https://openalex.org/W4318817474",
    "https://openalex.org/W4312808724",
    "https://openalex.org/W3028543194",
    "https://openalex.org/W2739095506",
    "https://openalex.org/W6797185979",
    "https://openalex.org/W4387146055",
    "https://openalex.org/W4312729969",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2967205117",
    "https://openalex.org/W3194020089",
    "https://openalex.org/W3108203525",
    "https://openalex.org/W2167716931",
    "https://openalex.org/W3154396742",
    "https://openalex.org/W1941755770",
    "https://openalex.org/W1513763202",
    "https://openalex.org/W4303422666",
    "https://openalex.org/W2164699598",
    "https://openalex.org/W4378804858",
    "https://openalex.org/W3203998598",
    "https://openalex.org/W1947251450",
    "https://openalex.org/W3120178332",
    "https://openalex.org/W4323927812",
    "https://openalex.org/W4364302686",
    "https://openalex.org/W4365448982",
    "https://openalex.org/W4385574794",
    "https://openalex.org/W4283068693",
    "https://openalex.org/W4294308527",
    "https://openalex.org/W4312457096",
    "https://openalex.org/W2786768213",
    "https://openalex.org/W2599124244",
    "https://openalex.org/W2002055708",
    "https://openalex.org/W4295430543",
    "https://openalex.org/W4205558134",
    "https://openalex.org/W4200042752",
    "https://openalex.org/W2122098299",
    "https://openalex.org/W4383371208",
    "https://openalex.org/W4316171088",
    "https://openalex.org/W4377089477",
    "https://openalex.org/W4220904310",
    "https://openalex.org/W4283727927",
    "https://openalex.org/W4321021925",
    "https://openalex.org/W4385847649",
    "https://openalex.org/W4372347554",
    "https://openalex.org/W3009259306",
    "https://openalex.org/W4377089398",
    "https://openalex.org/W2805246634",
    "https://openalex.org/W4372270146",
    "https://openalex.org/W4309242529",
    "https://openalex.org/W4384080511",
    "https://openalex.org/W4205713197",
    "https://openalex.org/W4221080763",
    "https://openalex.org/W4315490805",
    "https://openalex.org/W3206860915",
    "https://openalex.org/W2804957865",
    "https://openalex.org/W4386443400",
    "https://openalex.org/W3080551539",
    "https://openalex.org/W3156561767",
    "https://openalex.org/W3092353205",
    "https://openalex.org/W3041698047",
    "https://openalex.org/W3194836659",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6803432384",
    "https://openalex.org/W6798335185",
    "https://openalex.org/W2995523160",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W4385800817",
    "https://openalex.org/W4361186095",
    "https://openalex.org/W4379617038",
    "https://openalex.org/W4385697570",
    "https://openalex.org/W3011962222",
    "https://openalex.org/W4362664071",
    "https://openalex.org/W4287977455",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W3104324110",
    "https://openalex.org/W3009208582",
    "https://openalex.org/W2962699674",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W3010079658",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W2084680213",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3173912422",
    "https://openalex.org/W3162735870",
    "https://openalex.org/W1536620489",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Brain-computer interfaces (BCIs) have undergone significant advancements in recent years. The integration of deep learning techniques, specifically transformers, has shown promising development in research and application domains. Transformers, which were originally designed for natural language processing, have now made notable inroads into BCIs, offering a unique self-attention mechanism that adeptly handles the temporal dynamics of brain signals. This comprehensive survey delves into the application of transformers in BCIs, providing readers with a lucid understanding of their foundational principles, inherent advantages, potential challenges, and diverse applications. In addition to discussing the benefits of transformers, we also address their limitations, such as computational overhead, interpretability concerns, and the data-intensive nature of these models, providing a well-rounded analysis. Furthermore, the paper sheds light on the myriad of BCI applications that have benefited from the incorporation of transformers. These applications span from motor imagery decoding, emotion recognition, and sleep stage analysis to novel ventures such as speech reconstruction. This review serves as a holistic guide for researchers and practitioners, offering a panoramic view of the transformative potential of transformers in the BCI landscape. With the inclusion of examples and references, readers will gain a deeper understanding of the topic and its significance in the field.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nDeep Learning in EEG-Based BCIs: A\nComprehensive Review of Transformer\nModels, Advantages, Challenges, and\nApplications\nBERDAKH ABIBULLAEV,1 (Senior Member, IEEE), AIGERIM KEUTAYEVA2,\nAMIN ZOLLANVARI3, (Senior Member, IEEE)\n1Department of Robotics Engineering, Nazarbayev University, 010000, Kazakhstan (e-mail: berdakh.abibullaev@nu.edu.kz)\n2Department of Robotics Engineering, Nazarbayev University, 010000, Kazakhstan (e-mail: aigerim.keutayeva@nu.edu.kz)\n3Department of Electrical and Computer Engineering, Nazarbayev University, 010000, Kazakhstan (e-mail: amin.zollanvari@nu.edu.kz)\nABSTRACT Brain-computer interfaces (BCIs) have undergone significant advancements in recent years.\nThe integration of deep learning techniques, specifically transformers, has shown promising development\nin research and application domains. Transformers, which were originally designed for natural language\nprocessing, have now made notable inroads into BCIs, offering a unique self-attention mechanism that\nadeptly handles the temporal dynamics of brain signals. This comprehensive survey delves into the\napplication of transformers in BCIs, providing readers with a lucid understanding of their foundational\nprinciples, inherent advantages, potential challenges, and diverse applications. In addition to discussing the\nbenefits of transformers, we also address their limitations, such as computational overhead, interpretability\nconcerns, and the data-intensive nature of these models, providing a well-rounded analysis. Furthermore,\nthe paper sheds light on the myriad of BCI applications that have benefited from the incorporation of\ntransformers. These applications span from motor imagery decoding, emotion recognition, and sleep\nstage analysis to novel ventures such as speech reconstruction. This review serves as a holistic guide for\nresearchers and practitioners, offering a panoramic view of the transformative potential of transformers in\nthe BCI landscape. With the inclusion of examples and references, readers will gain a deeper understanding\nof the topic and its significance in the field.\nINDEX TERMS Deep Learning, Brain-Computer Interfaces, Review, Transformer Architecture, EEG,\nEmotion Recognition, Seizure Detection, Self-Attention Mechanism, Neural Networks, Motor Imagery,\nSleep Stage Analysis, Transformer Models, CNN, BCI.\nI. INTRODUCTION\nBrain-computer interfaces (BCI) enable communication be-\ntween the human brain and external devices without the\nintervention of peripheral nerves and muscles. They provide\na direct channel to translate mental processes into tangible\nactions, fundamentally reshaping how humans interact with\ntechnology. The concept of a BCI dates back to the early\n1970s [1], [2], but it wasn’t until the advent of sophisticated\nsignal processing techniques and computational power in the\nlate 20th century that significant progress was made [3]–[10].\nEarly BCIs were primarily experimental, used in controlled\nlaboratory settings, and often involved invasive procedures\nwhere electrodes were implanted directly into the brain [11],\n[12].\nBCIs are generally categorized into three types, each dif-\nfering in the degree to which they interface with the brain.\nInvasive BCIs necessitate the surgical implantation of elec-\ntrodes directly into the brain [13], [14]. Although this type\nprovides high-resolution neural signals, due to the inherent\nrisks of surgery and the potential formation of scar tissue, it is\nseldom used in non-medical applications. Most non-invasive\nBCIs rely on Electroencephalography (EEG) to obtain real-\ntime data on neural activity by placing electrodes on the scalp\n[15]. This technique enables immediate interfacing between\nthe brain and external devices, which is invaluable in BCI ap-\nplications. However, the spatial resolution is generally lower\ndue to its non-invasive nature, as the signals have to pass\nthrough the skull and scalp. On the other hand, partially inva-\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nsive BCIs are based on implanted electrodes within the skull,\nwhile the remaining electrodes are outside the brain [16].\nThese BCIs balance signal quality and medical risk, serving\nas a compromise between the invasive and non-invasive\ntypes. Nevertheless, these devices require neural surgery,\nworth considering the potential benefits they offer. There are\nalso other types of BCIs that use different techniques, such as\nmagnetoencephalography (MEG) [17], functional magnetic\nresonance imaging (fMRI) [18], and functional near-infrared\nspectroscopy (fNIRS) [19]. These methods allow for more\nprecise measurements of brain activity, but they are often\nmore expensive and less accessible than EEG-based BCIs.\nBCIs find applicability in diverse domains such as med-\nical rehabilitation [20], [21], entertainment [22], [23], and\ncommunication for those with motor or speech constraints\n[24]. For patients with paralysis or limb amputation, BCIs\ncan help in controlling prosthetic limbs and wheelchairs or\neven restore speech [25], [26]. Gamers can use BCIs for a\nmore immersive experience, where mental states or focus\ncan control game elements. Moreover, BCIs are particularly\nuseful for individuals with severe motor or verbal limitations,\nsuch as those with advanced ALS [27]. Nevertheless, BCI\ntechnology comes with challenges, including poor signal-to-\nnoise ratio, user training, and hardware limitations. Accu-\nrately decoding brain signals can be difficult due to noise,\nand huge variability in data, especially in non-invasive BCIs\n[28]–[30]. Many BCIs require users to undergo training to\nuse the system effectively [31]. Miniaturization and improve-\nment in electrode technology are ongoing challenges [32]–\n[34].\nA. EMERGENCE AND RELEVANCE OF DEEP\nLEARNING IN BCIS\nBCIs have evolved significantly with the integration of ad-\nvanced computational techniques. A prominent factor in this\nevolution is deep learning—a machine learning paradigm\nthat utilizes multi-layered artificial neural networks to an-\nalyze data [35], [36]. Given its ability to handle extensive\ndata sets and decode complex patterns, deep learning has\nbecome increasingly relevant to BCI applications [8], [37]–\n[39]. Deep learning, inherently inspired by the human brain’s\nstructure, leverages interconnected nodes in multiple layers\nto automatically learn and extract features from raw data.\nThis capability becomes especially advantageous in the con-\ntext of BCIs. Traditionally, BCIs relied on manual feature\nextraction and classical machine learning methods, which of-\nten required domain-specific expertise and were constrained\nby the limited capacity to process high-dimensional data\n[6], [15]. However, with deep learning, automated feature\nextraction from raw neural data became feasible, minimiz-\ning the need for manual intervention and domain-specific\npreprocessing [40], [41], showcasing enhanced performance\nin tasks such as motor imagery classification [42], [43].\nBeyond enhancing accuracy in standard tasks, deep learning\nhas expanded BCIs’ scope. BCIs that utilize deep learning\nalgorithms have improved the reaction time and accuracy\nof prosthetics and exoskeletons, particularly for individuals\nwith mobility challenges [44]. Additionally, these systems\ncan offer invaluable insights into a person’s cognitive state\nduring therapeutic scenarios [45]. The application of deep\nlearning has also made BCIs more versatile and user-friendly,\nbroadening their applicability to fields such as gaming and\nmindfulness practices [46], [47].\nNevertheless, deep learning models require significant\namounts of data, which is problematic as brain data is often\nlimited. The “black-box” nature of these models can also\nimpede interpretation, raising concerns about the use of brain\ndata in decision-making [48]. Additionally, deep learning\nmodels may overfit due to the high dimensionality of BCI\ndata and the potential scarcity of samples.\nB. INTRODUCTION TO TRANSFORMER MODELS\nThe transformer architecture, introduced in the groundbreak-\ning paper “Attention Is All You Need” by Vaswani et al. in\n2017 [49], has redefined the landscape of natural language\nprocessing. It has led to the development of models such\nas BERT, GPT, and many others, pushing the boundaries of\nmachine learning tasks across various domains beyond just\nNLP [50]–[54].\nTransformers are known for several unique and innova-\ntive components, most notably the self-attention mechanism,\nwhich allows the model to weigh the importance of different\nparts of the input data relative to each other, and the positional\nencoding, which gives the model a sense of order and ensures\nthat it can account for the position of data in a sequence.\nTransformers also use multi-head attention, allowing the\nmodel to focus on different parts of the input simultaneously\n[55]. The benefits of transformer architectures include paral-\nlelization, scalability, and flexibility. Transformers process all\ndata points in parallel, leading to faster training times. They\nare highly scalable, with large models capable of capturing\nintricate patterns in massive datasets, leading to state-of-the-\nart performance in various tasks. Although initially designed\nfor NLP tasks, transformers have shown great potential in\nother domains, such as vision and BCIs [56]–[60]. The suc-\ncess of the initial transformer model led to the development\nof numerous variants tailored for different tasks, such as\nBERT, GPT, and Vision Transformers [52]–[54], [58], [61]–\n[63].\nWhile the impact of the transformer architecture on ma-\nchine learning and BCI applications are evident, it is also\nimportant to understand its inherent challenges. Primarily,\nthe intensive computational requirements of transformers can\npose a barrier to individual researchers or smaller teams\nwith limited computational resources. Additionally, the risk\nof overfitting, especially when working with the relatively\nsmaller datasets frequently encountered in the BCI domain,\nis a pertinent concern. Nevertheless, the versatility of the\ntransformer model and its capacity to address diverse prob-\nlems underscore its significance in research. For optimal\napplication in BCI decoding tasks, a clear understanding of\nboth the strengths and limitations of transformer architecture\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nis essential. With this balanced perspective, the academic\ncommunity can harness the full capabilities of transformers,\nensuring continued progress in machine learning research.\nThis work provides a comprehensive overview of the rele-\nvance and emergence of transformer models in non-invasive\nEEG-based BCI systems. We discuss several technical and\npractical considerations related to BCIs and deep learning, in-\ncluding signal accuracy, invasiveness, hardware limitations,\nand interpretation complexity. Overall, this study provides an\noverview of the current state of research into BCIs and deep\nlearning, highlighting their potential and the challenges that\nmust be overcome to realize their full potential.\nThe paper is divided into four main sections. Section 1\nfocuses on transformer architecture, which is a type of deep\nlearning model applied in various domains, including BCI.\nThis section lists several papers exploring transformers’ use\nin BCI-related tasks. Section 2 discusses emotion recognition\nwith BCIs. The research listed in this section investigates\nthe use of EEG signals to recognize different emotions,\nwith various deep-learning techniques used to achieve this\ngoal. The papers also highlight the challenges and limitations\nof using BCIs for emotion recognition. Section 3 explores\nthe use of BCIs for various cognitive tasks, such as motor\nimagery. Deep learning techniques are used to classify EEG\nsignals related to these tasks, and the papers discuss the\nchallenges and limitations of using BCIs for such tasks.\nSection 4 discusses the benefits and potential of deep learning\nand the challenges and limitations that must be addressed to\nimprove the accuracy and interpretability of deep learning\nmodels. The research listed in this section provides insights\ninto these issues.\nII. BCI FUNDAMENTALS\nThe brain-computer interface, alternatively recognized as a\nbrain-machine interface or direct neural interface, represents\na communication channel established directly between the\nhuman brain and external machinery. The premise of BCI\ntechnology centers on harnessing and interpreting neural\nsignals, subsequently converting them into actionable com-\nmands that can drive external devices, ranging from comput-\ners to prosthetic devices [13], [64], [65].\nHistorical accounts pinpoint the genesis of BCI research to\nthe early 1970s, marking the period when pioneering exper-\niments involving animals were undertaken [66]. Progressing\ntowards the close of the 20th century, the inaugural human-\ncentric BCI experiments emerged, predominantly catering to\nmedical interventions, especially for individuals grappling\nwith neuromuscular impediments [67], [68]. Spanning sev-\neral decades, the synergy of neuroscience, engineering, and\ncomputational disciplines has dynamically driven the devel-\nopment and democratization of BCI modalities [4].\nBCIs are complex systems with multiple critical compo-\nnents, as detailed by Gerven [69]. Primarily, data acquisition\ninvolves capturing neural signals from the brain using a vari-\nety of techniques. Invasive techniques might utilize micro-\nelectrode arrays, positioned directly on the brain’s surface\nEEG or NIRS\nECoG\nMEG\nData Acquisition\nData Processing\nData Preprocessing\nMachine Learning\n- noise filtering\n- artifact removal\n- segmentation\n- Deep Learning models\n- CNN\n- RNN\n- Transformers\nFeature Extraction\n- spectral power\n- coherence\n- event-related potentials\n- motor imagery (ERD/ERS)\nDevice Control\nFeedback\n- visual\n    - auditory\n   - haptic\n  - electrical\n - thermal\n - tactile\n - vibration\nControl data\nFIGURE 1. Illustration of the Brain-Computer Interface (BCI) Framework. The\ncomponents represent the entirety of the BCI Framework, detailing the critical\nstages: Data Acquisition, Data Preprocessing, Feature Extraction, Machine\nLearning, and Feedback. Each stage is pivotal in determining the interpretive\nand responsive functionalities of BCI systems.\n[70], while non-invasive methods could employ EEG, where\nelectrodes are placed on the scalp [71]. However, acquired\nneural data frequently contains noise and necessitates careful\npreprocessing. Essential preprocessing tasks include noise\nfiltering, artifact removal, and segmentation [72]. Noise fil-\ntering seeks to eliminate undesired signals that might corrupt\nthe true neural signals [73]. In contrast, artifact removal\ntargets extraneous signals not originating from the brain,\nfor instance, those originating from muscle contractions or\neye movements [74]. Segmentation entails subdividing the\ncontinuous data into segments for further processing [75].\nSubsequent to these preprocessing steps, the data undergoes\nfurther analysis to extract meaningful patterns or features,\nindicative of underlying neural activities. In EEG data, these\npatterns may include spectral power, coherence, event-related\npotentials, and motor imagery patterns such as event-related\ndesynchronization (ERD) and event-related synchronization\n(ERS) [76]. Spectral power reflects the strength of the neural\nsignals in different frequency bands [77]. Coherence indi-\ncates the degree of synchronization between various brain\nregions [78]. Event-related potentials represent the brain’s\nresponse to a specific stimulus or task [79]. Meanwhile,\nERD patterns indicate a decrease in power, usually related to\nmotor preparation or movement initiation [80], whereas ERS\npatterns denote an increase in power, typically associated\nwith motor termination or post-movement processes [81].\nThese patterns are then used in subsequent analysis within\npredictive models to classify them into different classes, such\nas motor imagery classification.\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nMachine learning algorithms play a crucial role in analyz-\ning brain data [6] and decoding and interpreting specific brain\nstates or intentions, enhancing the precision and effectiveness\nof BCI systems [38]. Once a mental intention is decoded, the\nBCI translates it into a specific action for the corresponding\nexternal device, with the user receiving real-time feedback.\nThis enables an interactive loop, enhancing the usability of\nBCIs.\nFigure 1 depicts the various components integral to the\nBCI framework, including data acquisition, data prepro-\ncessing, feature extraction, machine learning, and feedback.\nEach of these components is vital, collectively ensuring the\nefficacy and accuracy of the BCI’s operations. While the\nBCI ecosystem spans from the initial data acquisition to\nthe final real-time feedback, significant research gravitates\ntowards the interpretation and prediction of brain data [35].\nThis concentrated focus underscores the critical importance\nof data analysis in harnessing the full capabilities of BCIs,\nenabling their effective deployment across a spectrum of\napplications [8].\nA. MAIN APPLICATIONS\nBCIs are useful systems that have had a significant impact in\nvarious domains. In the field of medical rehabilitation, they\ncan provide a new level of independence for patients who are\ndealing with paralysis, neuromuscular disorders, or limb am-\nputations. BCIs allow these individuals to control prosthetic\nlimbs or wheelchairs in real-time [82]. This advancement\nhas a profound effect on their quality of life, giving them\na restored sense of autonomy [20], [21], [83]. Furthermore,\nBCIs have the potential to restore lost sensory feedback,\nespecially for those who have lost the sense of touch or\nproprioception [84]–[86]. This allows patients to regain some\nof their lost motor skills, leading to a more independent daily\nlife [31].\nSimultaneously, in the domain of communication, BCIs\ncan offer a practical means of communication [87], [88].\nPeople with locked-in syndrome, advanced ALS, or other\nsimilar conditions are often unable to move or speak, making\ncommunication very challenging [82]. To date, most BCIs\nhave been designed specifically for these individuals, to\nsupport them to communicate by translating their mental\nactivities into commands. These systems can be used to type\nmessages on a computer screen or even control external\ndevices, such as wheelchairs or prosthetic limbs [89]. In\nsummary, the development of BCI systems has helped to im-\nprove the quality of life for many individuals to communicate\neffectively.\nA relatively novel application space for BCIs is emotion\nrecognition. This area primarily involves the analysis of\nneural signals to understand and classify human emotional\nstates [90]–[92]. By identifying distinct patterns in brain\nactivity associated with various emotions, BCIs can offer\na novel approach to detecting and analyzing these states.\nPotential applications include improved mental health di-\nagnostics, where accurate emotion detection could provide\nclinicians with valuable insights. Moreover, in the domain of\nmedia, real-time emotion feedback can guide content adapta-\ntion, leading to user-specific experiences. Similarly, adaptive\nenvironments in educational or occupational settings can be\ndeveloped based on emotional feedback, potentially enhanc-\ning learning and productivity. As research progresses, it is\nexpected that the integration of BCIs in emotion recognition\nwill open new avenues for further exploration and applica-\ntion.\nAdditionally, the gaming industry is actively investigating\nthe use of BCI to create a more immersive gaming experience\n[93], [94]. BCIs allow players to navigate virtual environ-\nments using their thoughts and feelings. This means gamers\nmight soon control game characters and execute actions just\nby thinking. Such an approach can increase the feeling of\nbeing “in” the game, making gameplay even more enjoy-\nable. BCIs also offer a chance for people with disabilities,\nespecially those who can not use regular game controllers\nor keyboards, to engage in gaming [22], [95], [96]. The\nintegration of BCI in gaming suggests a future where games\nbecome more interactive and inclusive.\nB. EMERGING APPLICATIONS\nBeyond traditional applications, BCIs have expanded into\ncognitive enhancement, sleep analysis, seizure detection, and\nspeech reconstruction.\nResearchers are exploring the potential of BCIs in cogni-\ntive enhancement, specifically to improve concentration [97],\n[98]. Neurofeedback is one such technique that offers indi-\nviduals real-time feedback on their neural activities, facilitat-\ning the enhancement of particular cognitive functions. This\nBCI application shows promise for individuals with attention\ndisorders. As advancements in the field continue, neurofeed-\nback and associated techniques might play a pivotal role in\nadvancing our comprehension of cognitive enhancement and\noverall brain functionality [90]–[92], [99]–[101].\nSleep research can also benefit from BCI methods, partic-\nularly in the classification of sleep stages—a pivotal aspect of\ndiagnosing sleep disorders [102]. Using EEG data, machine\nlearning algorithms can discern sleep stages such as Wake,\nREM, and the non-REM stages (N1, N2, N3) with pro-\nnounced accuracy [103]. By decoding the EEG signatures of\nvarious sleep phases, machine learning methods used in BCIs\ncan present diagnostic insights and therapeutic interventions.\nMoreover, combining BCIs with wearable technology holds\npromise for non-invasive, real-time sleep monitoring, war-\nranting further investigation.\nAnother crucial application area is epileptic seizure detec-\ntion. Given the characteristic irregular brain activities during\nseizures, BCIs, equipped with EEG monitoring, emerge as\nimportant tools for capturing these anomalies [104]. BCI-\ndriven algorithms can pinpoint these atypical patterns, fa-\ncilitating early seizure detection and intervention [105]. The\nprospective ability of BCIs to predict seizures before their\nonset can enhance the management of epilepsy, providing\npatients with preemptive alerts. The conjunction of BCIs\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nwith wearables emphasizes its importance in neurology and\nbiomedical engineering research.\nIn speech reconstruction, BCIs are employed to decode\nneural activity related to auditory processes and produce\nintended speech [106]. This is particularly valuable for in-\ndividuals who cannot communicate verbally due to specific\nconditions. By integrating machine learning models with\nBCIs, researchers are working to convert EEG-based neural\npatterns into understandable speech [107]. Although chal-\nlenges, such as EEG noise and variations in individual neural\npatterns, persist, initial studies indicate the potential of BCIs\nin this area [108].\nC. BCI CHALLENGES\nSignal Accuracy: Acquiring accurate and consistent neural\nsignals is fundamental for both neuroscience and BCI re-\nsearch. However, achieving consistent and precise recordings\nis challenging, particularly due to external interferences such\nas electronic devices [109]. Moreover, inherent variability\nin EEG data, attributed to individual brain differences [71],\nas well as inconsistencies across trials due to factors such\nas attention fluctuations [110], poses significant analytical\nchallenges.\nFactors such as inter-subject differences in brain struc-\ntures, intra-subject variability, inaccuracies in electrode\nplacement [111], and device-specific biases [112] are notable\ncontributors to data variability. External environmental con-\nditions can further complicate the data collection process.\nHowever, by employing rigorous methodologies and leverag-\ning advanced techniques, researchers can effectively mitigate\nthese challenges, ensuring the integrity of neural data and\nthereby advancing the neuroscience domain.\nInvasiveness: The invasiveness of BCIs poses one of the\nprimary challenges in the realm of neural interfacing. While\nnon-invasive methods such as EEG-based systems are widely\nadopted due to their relative safety and ease of application,\nthey often compromise on signal quality and precision. In\ncontrast, invasive methods, which involve the direct implan-\ntation of electrodes into or on the surface of the brain,\ncan yield higher signal fidelity and specificity [113], [114].\nHowever, these methods introduce increased medical risks,\nincluding potential complications from surgery and long-\nterm biocompatibility concerns. The ethical considerations\nsurrounding invasive procedures, especially in non-medical\nor elective contexts, further compound the challenges. Thus,\ndetermining the optimal balance between invasiveness and\nfunctionality remains a pivotal challenge in advancing BCI\ntechnology.\nHardware Limitations: While current BCI systems have\nmade significant progress in helping individuals control de-\nvices with their brain activities, there is still much room\nfor improvement. One area that particularly stands out is\nminiaturization [34], [115]. By reducing the size of BCI\nsystems, they can become more portable and less obtrusive,\nallowing users to integrate them into their daily lives more\neasily. These improvements can lead to greater adoption\nand utilization of BCI technology in the future, ultimately\nbenefiting individuals who rely on these systems for commu-\nnication and independence.\nInterpretation Complexity: The brain, which is the cen-\ntral organ of the nervous system, is incredibly complex in\nits structure and function. It is responsible for receiving and\ninterpreting signals from various parts of the body, and it\nperforms this task with remarkable efficiency. In fact, the\nbrain works in tandem with the spinal cord to form the\ncentral nervous system, which controls all the functions\nof the body, including movement, sensation, and cognition\n[116], [117]. To accurately and consistently interpret the\nsignals generated by the brain, scientists and researchers\nhave developed sophisticated algorithms and models. These\nmodels are capable of processing vast amounts of data and\nidentifying complex patterns that are simply impossible for\nhumans to discern. However, the development of such models\nrequires significant computational power, which can be a\nchallenging task [118], [119]. Despite the complexity of\nthe brain and the challenges associated with interpreting its\nsignals, researchers are committed to unlocking its mysteries\nby developing new algorithms and models. This will help us\nbetter understand how the brain works and how we can use\nthis knowledge to improve our lives and the world around us.\nBCIs stand at the intersection of neuroscience and technol-\nogy and hold the promise of fundamentally reshaping various\nsectors, particularly healthcare. As BCI technology continues\nto evolve, it is essential to approach its applications with\na balance of optimism and caution, addressing challenges\nhead-on.\nD. DEEP LEARNING IN BCIS\nDeep Learning is a subset of machine learning, which, in\nturn, falls under the broader category of artificial intelligence\n[15], [120], [121]. It’s characterized by the use of deep neural\nnetworks – layered computational structures inspired by the\nbiological neurons in the human brain. These networks,\ncomprising of multiple layers, enable the algorithm to learn\nrepresentations of data through multiple levels of abstraction\nautomatically. Classic examples include Convolutional Neu-\nral Networks (CNNs) used in image recognition or recurrent\nneural networks used in sequence prediction tasks [122].\nWhere traditional machine learning techniques might re-\nquire feature engineering – a manual process where the most\nrelevant features of data are selected for model training –\ndeep learning models are known for their ability to automat-\nically extract and learn features directly from raw data. This\nmakes them particularly powerful for tasks involving large\nand complex datasets, such as images, speech, and, notably\nfor this context, brain signals [8], [37], [39], [40], [123]–\n[127].\n1) Importance and Impact on of transformers in BCI\nResearch\n• Automated Feature Extraction: The neural data from\nthe brain is highly complex and multi-dimensional.\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nDeep learning, with its ability for automatic feature\nextraction, has made it possible to interpret raw brain\nsignals without the need for extensive manual feature\nengineering. This reduces the potential for human bias\nand error and simplifies the process of model develop-\nment.\n• Enhanced Accuracy : BCIs demand high accuracy to\nbe practically useful, especially in medical or assistive\ncontexts. Deep learning models, given their capability\nto handle vast datasets and complex structures, have\nconsistently demonstrated superior accuracy in decod-\ning brain signals compared to traditional methods.\n• Real-time Processing : With advancements in hard-\nware, such as GPUs, deep learning models can process\nand interpret brain signals in real time. This is crucial\nfor BCIs where fast command output is key, such as in\nprosthetic limb control or communication aids.\n• Scalability: Deep learning models are scalable. As\nmore data becomes available – from a wider variety\nof subjects and conditions – these models can continue\nlearning and refining their interpretations, improving the\nrobustness and versatility of BCIs.\n• Addressing Subject Variability: One of the core chal-\nlenges in BCI is the variability of signals between\ndifferent individuals. Deep learning, with architectures\nsuch as convolutional layers or attention mechanisms,\nhas shown potential in capturing these latent temporal\npatterns, paving the way for subject-independent BCIs.\nDeep learning has played an important role in advancing\nBCI research, enabling the automatic decoding of intricate\nneural patterns. This has resulted in the creation of more\ndependable, effective, and robust BCIs. With the continuous\nimprovement of computational techniques and the expansion\nof BCI datasets, the integration of deep learning and BCIs has\nthe potential to produce even more promising developments\nin the coming years.\nIII. TRANSFORMER ARCHITECTURE\nHistorically, sequence data processing in neural networks\nbegan with Recurrent Neural Networks (RNNs). These ar-\nchitectures were equipped with an inherent ‘memory’ mecha-\nnism, retaining hidden states across sequence steps. Yet, their\nefficacy weakened with longer sequences due to issues such\nas the vanishing and exploding gradient problems, which\nimpeded successful training. Addressing these limitations,\nLong Short-Term Memory Networks (LSTMs) emerged as\nan advanced form of RNNs. By employing gated cells,\nLSTMs adeptly controlled information flow, enhancing the\nnetwork’s capacity to recognize long-term dependencies in\nsequential data [122]. Nevertheless, inherent to their design,\nLSTMs processed sequences serially, constraining parallel\nprocessing possibilities across sequence elements.\nA paradigm shift occurred with the introduction of the\nTransformer architecture, as presented by Vaswani et al. in\n2017 [49]. Eliminating the recurrent structure, Transform-\ners capitalized on parallel processing capabilities, process-\ning entire sequences concurrently. This adjustment signifi-\ncantly accelerated training phases, especially on contempo-\nrary hardware optimized for parallel computation. Central to\nTransformers is the attention mechanism, particularly self-\nattention, which intelligently assigns different weights to se-\nquence elements based on their task-specific relevance. This\nmechanism enables the model to selectively emphasize parts\nof the input, optimizing comprehension and representation of\ndata sequences.\nA. TRANSFORMER’S ARCHITECTURE FOR EEG\nCLASSIFICATION\nThis section presents the “standard” approach for utilizing\nthe Transformer encoder to classify EEG patterns for BCIs.\n1) Input Standardization and Positional Encoding\nLet the set of pairsDtrain = {(X1, y1), . . . ,(Xn, yn)} denote\nn trials of EEG recordings where yi is the scaler class\nvariable with L possible labels (e.g., target and non-target in\na binary classificatiion) and Xi ∈ Rc×p is the collection of\nEEG observations in the ith trial over c channels and p time\npoints; that is to say,\nXi = [xi1, xi2, . . . ,xic]T , i= 1, . . . , n ,(1)\nwith xij = [ xij1, . . . , xijp]T ∈ Rp×1, j= 1 , . . . , c, where\nxijk, k= 1, . . . , pdenotes the kth element of vector xij, and\nT denotes the transpose operator. The goal is to useDtrain and\ntrain a classifier ψ : Rc×p → {0, 1, . . . , L− 1} that maps a\ngiven X to a possible value of the class variable.\nIt is common to apply standardization for each channel to\nmake the sensory data across all channel comparable (see\n[128]–[130]). In this regard, each Xi is converted to ˆXi\nwhere\nˆXi = [ˆxi1, ˆxi2, . . . ,ˆxic]T , i= 1, . . . , n ,(2)\nand where ˆxij = [ˆxij1, . . . ,ˆxijp]T such that\nˆxijk = xijk − mij\nsij\n, (3)\nwith mij and sij being the sample mean and sample standard\ndeviation of vector xij given by\nmij = 1\np\npX\nk=1\nxijk , (4)\nsij =\nvuut1\np\npX\nk=1\n(xijk − mij)2 , (5)\nrespectively.\nIn order for the Transformer to make use of EEG recording\norders, it is common to encode some information about\nthe position of sequence elements in its input [49]. This\npositional encoding is generally realized by adding each ˆXi\nto a matrix P ∈ Rc×p that is defined based on trigonometric\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nFIGURE 2. Visual summary underscores the transformative role of transformers models in Brain-Computer Interfaces (BCI), highlighting key application\ndomains—including motor imagery decoding, emotion recognition, sleep stage analysis, as well as the emerging applications —and encapsulating both the\nadvantages and inherent challenges in the BCI landscape.\nfunctions with different frequencies for each channel [49]. As\na result, we obtain\n˜Xi = ˆXi + P, i= 1, . . . , n, (6)\nwhere the element on row (channel) j = 1 , . . . , c, and\ncolumn (time index) k = 1, . . . , p, of P, denoted pjk is given\nby\npjk =\n\n\n\nsin\n\u0010\nk/10000j/c\n\u0011\n, for even j,\ncos\n\u0010\nk/10000j−1/c\n\u0011\n, for odd j.\n(7)\n2) Self-Attention Mechanisms: Capturing Contexts for EEG\nClassification\nCapturing contexts is the essential concept that makes atten-\ntion mechanism a promising operation for EEG classifica-\ntion. A context is simply another representation of an element\nof the input sequence (here one column of each ˜Xi) based on\nits compatbility with other elements within the sequence. The\nmost widely used attention operation for EEG classification\nis scaled dot-product self-attention, denoted SAd\nV,K,Q( ˜Xi) :\nRc×p → Rd×p, which was initially proposed and used for\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ntranslation tasks [49]. In particular,\nSAd\nV,K,Q( ˜Xi) = V ˜Xi × softmax\n\u0010 ˜XT\ni KT Q ˜Xi\n√q\n\u0011\n, (8)\nwhere V ∈ Rd×c, K ∈ Rq×c, Q ∈ Rq×c are projection\nmatricces that are learned in the training process, q is known\nas attention dimensionality, andd, which is generally a tuning\nparameter, denotes the dimensionality of the columns of\nthe output matrix ( context vectors ). We use superscript d\nin SA d\nV,K,Q( ˜Xi) to highlight the dimensilaity of context\nvectors.\n3) Multi-Head Self-Attention\nRather than a single self-attention operation, it is generally\nbeneficial to apply multiple self-attentions in parallel. Us-\ning this operation, we view the compatibility of sequence\nelements using different learned projections. In this context,\nit is also common to refer to the output matrix of each\nself-attention as a head. In particular, the multi-head self-\nattention, denoted MSHA( ˜Xi) : Rc×p → Rdh×p, is defined\nas\nMSHAdh ( ˜Xi) = (9)\nW[SAd\nV1,K1,Q1 ( ˜Xi)T , . . . ,SAd\nVm,Km,Qm ( ˜Xi)T ]T ,\nwhere W ∈ Rdh×md is another learnable projection matrix,\nm is the number of self-attentions used in (10), which is also\nknown as the number of heads, and dh is the dimensionality\nof columns in the output of MSHAdh ( ˜Xi) operation.\n4) Identity Skip-Connection and Layer Normalization\nTo ensure the stability and efficacy of the training pro-\ncess, especially with the complex nature of EEG data, the\nTransformer encoder utilizes identity skip-connections [68]\nfollowed by layer normalization [131]. Here we define these\noperations. Let SKP\n\u0000\nLAY(Y)\n\u0001\n: Ra×b → Ra×b denote\nthe identity skip-connection around a layer LAY (Y) (an\noperation) that operates on an input Y ∈ Ra×b to produce\nan output of the same size as the input. Then\nSKP\n\u0000\nLAY(Y)\n\u0001\n= Y + LAY(Y) . (10)\nThat is to say, we simply add the output of LAY (Y) to its\ninput. Furthemore, let LN (Y) : Ra×b → Ra×b denote the\nlayer normalization applied to an (a >1) × b matrix Y with\nelements yjk, j= 1 , . . . , a, k= 1 , . . . , bwhere each row\nrecords measurements for a “features” (here, channel). Then,\nLN(Y) produces ˚Y, which is a matrix of the same size ˚Y\nwith elements ˚yjk where\n˚yjk = yjk − mk\nsk\n, (11)\nand where\nmk = 1\na\naX\nj=1\nyjk , (12)\nsk =\nvuut1\na\naX\nj=1\n(yjk − mk)2 . (13)\nIn other words, ˚Y is a type of standardization where the\nsample mean and sample standard deviation are computed\nfor column of Y (in the EEG context means for each time\npoint in the sequence) over all features. One place that these\noperations are used in transformer encoder is to produce ˚Xi\nas follows:\n˚Xi = LN\n\u0010\nSKP\n\u0000\nMSHAc( ˜Xi)\n\u0001\u0011\n; (14)\nthat is, the skip-connecction is used around the multi-head\nself-attention, which is then followed by layer normalization.\nNote that the use of skip-connection in (14) enforces setting\ndh defined in (10) to c, which is the number of channels.\n5) Position-wise Feed-Forward Networks\nThe Transformer encoder utilizes fully connected feed-\nforward network that transforms each element of a given\nsequence inidivudally. Let Y ∈ Ra×b is the generic matrix\ndefined before. The effect of this position-wise feed-forward\nnetwork operated on an input Y, denoted FFN(Y, is:\nFFNs(Y) = [g(y1), . . . , g(yb)] , (15)\nwhere yk, k= 1, . . . , bare columns of Y and\ng(yk) = W2 × f(W1yk + b1) + b2 , (16)\nwhere f(.) denote an element-wise nonlinear activation func-\ntion (e.g., ReLU), and W1 ∈ Rr×a, W2 ∈ Rs×r, b1 ∈\nRr×1, and b1 ∈ Rs×1 are learnable matrices and vectors—\nr is generally a tuning parameter. We use superscript s in\nFFNs(Y) to highlight the dimensionality of output vectors in\n(15). In the Transformer encoder, position-wise feed-forward\nnetwork is used to produce an output Oi from ˚Xi obtained\nin (14), which is then added to its input through the skip-\nconnection, followed by layer normalization. This operation\nis characterized as follows:\nOi = LN\n\u0010\nSKP\n\u0000\nFFNc(˚Xi)\n\u0001\u0011\n. (17)\nNote that the use of skip-connection in (17) enforces setting\ns defined in (15) to c. The classification can be performed by\nvectorizing Oi and using that as the input to a fully connected\nlayer with a softmax activation function 1..\n1Supplementary Material provides a sample code in PyTorch that comple-\nments the mathematical formulation discussed in this section\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nSpatial / Temporal CNN\nHybrid CNN and Transformer model\nLinear\nSoftMax\n⨁∼\nPositional \nEncoding\nOutputs \nProbabilities\nInputs \n(n, trials, eeg channels, time)\nN ⨯\nMulti-Head \nAttention\nFeed \nForward\nQ K V\nAdd & Norm\nAdd & Norm\nTransformer Encoder\nEEG channels\nTime\nCNN (1 ⨯ 25), 40, same \nCNN (1 ⨯ 15), 40, valid \nAverage Pooling 1 ⨯5\nSpatial CNN\nCNN (ch ⨯1), 40, same \nAverage Pooling 1 ⨯5\nTemporal CNN\nCNN (1 ⨯ 25), 40, same \nAverage Pooling 1 ⨯75\nCNN (ch ⨯1), 40, same\nSpatio-Temporal CNN\nFIGURE 3. Architectural Configurations of Convolutional Neural Network (CNN) and Transformer Encoder Components for EEG Analysis. The diagram illustrates\nthe diverse possible configurations combining CNNs and Transformers, tailored to extract temporal, spatial, or integrated spatiotemporal features using advanced\ntransformer methodologies.\nB. BENEFITS OF THE TRANSFORMER MODELS\nTransformers offer a significant advantage in parallelization.\nUnlike traditional RNNs that process sequences one element\nat a time, transformers can handle all sequence elements si-\nmultaneously due to their non-recurrent nature. This parallel\nprocessing approach is highly optimized for parallel hard-\nware such as GPUs, leading to noticeably reduced training\ntimes [158]. Additionally, transformers are adept at handling\nlong-range dependencies within sequences, due to their at-\ntention mechanisms. These mechanisms allow the model to\neffectively associate distant elements in sequences, offering\na notable improvement over conventional models such as\nLSTMs in capturing the context within extended texts [159].\nOn the scalability front, transformer-based models, including\nBERT [63] and GPT [50], have demonstrated the ability\nto scale up to billions of parameters. This scalability has\nplayed a key role in setting new performance benchmarks\nacross a wide array of tasks. Moreover, the flexibility of\ntransformer models is prominent. Even though they were\ninitially designed for sequence-to-sequence tasks, they have\nbeen successfully adapted for a variety of applications, rang-\ning from classification to image processing [160]. Their\ncapability to cater to diverse data types underscores their\npotential as a versatile tool in machine learning research.\nThe combination of parallel processing, efficient handling of\nlong dependencies, scalability, and broad adaptability make\ntransformer models a popular choice for various tasks.\nIV. APPLICATIONS OF TRANSFORMERS IN BCIS\nWith the advent of the Transformer architecture, there has\nbeen an increasing interest in leveraging these state-of-the-\nart machine-learning models to advance BCIs. In the subse-\nquent sections, we will explore how transformer models have\nsignificantly advanced the BCI domain. Figure 2 provides an\noverview of the fundamental principles, areas of application,\nbenefits, and challenges of transformers in BCIs. It highlights\nthe potential of transformers in tasks such as motor imagery\ndecoding and sleep stage analysis, where they can effectively\nhandle brain signal dynamics. We will discuss the various\nways in which transformers can be utilized in BCIs, poten-\ntially leading to beneficial developments in both research and\npractical applications.\nFigure 3 presents diverse architectural configurations that\nintegrate the Convolutional Neural Network (CNN) and\nTransformer Encoder components, as discussed in this study,\ntailored for EEG data analysis for BCIs. These configurations\ndiffer in their approach to feature extraction from EEG sig-\nnals: while some models emphasize temporal characteristics,\nothers concentrate on spatial patterns. Notably, there are also\nhybrid models designed to capture both spatial and temporal\nfeatures concurrently. By fusing the capabilities of CNNs\nwith Transformer techniques, these architectures underscore\nthe potential and adaptability of advanced transformer ap-\nproaches in EEG data processing. Furthermore, in Table\n1, we list abbreviations used in the work. It includes full\nterms in the first and third columns and their corresponding\nabbreviations in the second and fourth columns. The terms\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 1. List of Abbreviations Used in this Work\nFull Term Abbreviation Full Term Abbreviation\nLeft hand LH Subject independent SI\nRight hand RH Subject dependent SD\nBoth feet BF Eyes open EO\nTongue T Eyes closed EC\nAverage A VG Physical action PHY\nPrecision PREC Imagined action IMA\nAccuracy ACC Cross-subject experiment CSE\nSensitivity SENS Multi-Head Self-Attention MHSA\nSpecificity SPEC Multi-Head Attention MHA\nFully Connected FC Feedforward FF\nRectified Linear Unit ReLU Batch Normalization BN\nExponential Linear Units ELU Global Average Pooling GAP\nPosition Encoding PE Virtual Reality VR\nMotor Imagery MI Brain-Computer Interface BCI\nLong Short-Term Memory LSTM Convolutional Neural Network CNN\nParticle Swarm optimization PSO EEG channel-attention ECA\nLayer Normalization LN Matthews Correlation Coefficient MCC\nPearson Correlation Coefficient PCC\nTABLE 2. Summary of Datasets Used in MI EEG-based Studies\nDataset Name Detailed Info Channels Subjects Targets Studies Used\nBCI Competition IV 2a [10] 9 individuals, 22 channels, 288 trials, 500/1000 time samples 22 9 LH, RH, BF, T [129], [132]–[141]\nBCI Competition IV 2b [10] 9 individuals, 3 channels, 5 sessions, 120-160 trials/session 3 9 LH, RH [129], [133]–[135]\nBCI Comp. 2003 Data III [142] 1 subject (25 y.o. female), 7 sessions, 40 trials/session 3 1 LH, RH [143]\nBCI competition IV dataset 1 [144] 7 subjects, 59 channels, and 200 trials. 59 7 LH, RH, BF [145]\nOpenBMI [146] 54 subjects, 62 channels, 400 trials 62 54 LH, RH [133], [139], [147]\nPhysionet [148] 109 subjects, 64 channels, 11354 samples, 656-time steps 64 109 LH, RH, EO, BF [60], [141], [149], [150]\nPrivate Datasets\nBy Huashan Hospital 108 trials, 7 stroke patients, 31 channels 31 7 Motor, Reset [151]\nPrivate Dataset 1 25 subjects, 60 channels, 320 trials 60 25 RH, BF [145]\nPrivate Dataset 2 5 subjects, 8 channels, 300 trials (AO + MI) 8 5 LH, RH, No Move [152]\nPrivate Dataset 3 20 subjects, 59 channels, 26400 epochs, 6 skeleton points 59 20 LH, RH [153]\nPrivate Dataset 4 40 subjects, 64 channels, 50 trials/class 64 40 LH, RH [140]\nOther Datasets\nBrain-Visual [154] 6 subjects, 128 channels, 11964 samples, 500-time steps 128 6 40 Classes [149]\nASU (Speech Imagery) [155] 6 subjects, 60 channels, 2 classes, 100 trials/class 60 6 In, Cooperate [140]\ncover relevant topics in BCI research and are crucial for\nunderstanding the discussed components and techniques. For\nexample, it includes abbreviations for movements such as\n“Left Hand” (LH) and “Both Feet” (BF), metrics such as\n“Average” (A VG) and “Precision” (PREC), and technical\nterms such as “Multi-Head Self-Attention” (MHSA) and\n“Batch Normalization” (BN).\nA. MOTOR IMAGERY EEG DECODING\nMotor Imagery (MI) refers to the cognitive process where\nindividuals mentally visualize and rehearse specific motor\ntasks without physically moving. This mental representation\nis important in BCIs, particularly in creating effective com-\nmunication and control methods for people with severe motor\ndisabilities. MI-BCIs aim to convert these imagined motor\ntasks into control signals that can be used to operate external\ndevices, such as computer cursors, robotic prosthetics, and\nelectric wheelchairs [7], [161]–[164].\nHowever, there are several challenges associated with the\ndecoding process of MI:\n• Intra and Inter-Subject Variability: EEG patterns that\ncorrespond with MI manifest considerable variability.\nThis variability is discernible not only within individ-\nual sessions but also across different individuals. Such\ninconsistencies render the formulation of universally\napplicable models a complex undertaking [129], [132],\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 3. Overview of Transformer Models in Motor Imagery Brain-Computer Interfaces (BCIs) (Continued on next page)\nRef Protocol EEG Data Model Description Performance Highlights\n[147] LH vs. RH;\nSI / SD tests.\nOpenBMI dataset:\n400 trials x 62 ch\nTwo-level hierarchical transformer:\n- LLT: Extracts short-term features\n- HLT: Focuses on relevant features\n81.3% (SI) (Acc),\n82.1% (SD) (Acc)\nHierarchical Attention: Introduced\na two-level transformer that sepa-\nrately focuses on short-term and rel-\nevant features. Outperformed con-\nventional CNN models in the\nsubject-independent test.\n[132] LH vs. RH vs. BF\nvs. T;\nCSE\nBCI Competition IV 2a\ndataset:\n288 trials x 22 ch x 500\nsamples\nV AT-TransEEGNet:\n- V AT regularization and PSO\n- Adds self-attention to EEGNet\n63.56% (CSE) (Avg\nAcc)\nRegularization and Particle Swarm:\nIntegrated Variational Adversarial\nTraining with Particle Swarm Opti-\nmization in EEGNet, enhancing ro-\nbustness and outperforming multi-\nple baseline models.\n[151] Motor vs. Reset\nstate\nDataset by Huashan Hos-\npital:\n108 trials x 31 ch\nST with ECA:\n- Powerful ML block\n- Shifted window-based MSA\nmodule\n87.67% (Avg Prec) Utilized the Swin Transformer’s\nshifted window-based multi-head\nself-attention (MHSA) module for\nbetter EEG representation. Outper-\nformed other standard models such\nas CSP and CNN.\n[133] LH vs. RH;\nSI tests.\nBCI Competition IV\n2a/2b,\nOpenBMI\nSMT:\n- CNN, MSA block\n- Mirror network for data\naugmentation\n2a - 67.28% (Avg\nPrec),\n2b - 76.41% (Avg\nPrec),\nOpenBMI - 79.76%\n(Avg Prec)\nProposed a shallow transformer\nmodel combined with a mirror\nnetwork for EEG data augmenta-\ntion. Highlighted the potential of\nmulti-head self-attention in subject-\nindependent (SI) motor imagery\nBCI tasks.\n[135] LH vs. RH vs. BF\nvs. T;\nCSE\nBCI Competition IV 2a/2b GAT:\n- Feature extractor, global adaptor\n- Domain discriminator, classifier\n2a - 76.58% (Avg\nAcc),\n2b - 84.44% (Avg\nAcc)\nDomain Adaptation with Attention:\nProposed a global adaptive trans-\nformer emphasizing domain adap-\ntation using attention mechanisms.\n[134] Multiple\nProtocols\nMultiple Datasets EEG Conformer:\n- CNN, AvgPooling\n- Self-attention, classifier\n2a - 78.66% (Avg\nAcc)\n2b - 84.63% (Avg\nAcc)\nAchieved impressive results across\nboth motor imagery and emo-\ntion recognition tasks using the\nEEG Conformer model, showcas-\ning versatility.\n[136] LH vs. RH vs. BF\nvs. T;\nSI tests.\nBCI Competition IV 2a\ndataset\nCRAM:\n- Two stacked recurrent networks, a\nself-attention module\n- Classification block.\n59.10% (Avg Acc) Outperformed a series of models as\nCNN, RNN, and Attention-based\nmodels by integrating stacked\nrecurrent networks with a self-\nattention mechanism.\n[149] Multiple\nProtocols,\nSI / SD tests.\nMI Physionet dataset:\n64 ch x 11354 samples\nBrain-visual dataset:\n128 ch x 11964 samples\nGated Transformer:\n- PreLN Transformer and Post-LN\nTransformer\n- Input Embedding, PE, Encoder\nBlock, and a Classifier\n- Encoder Block consists of a LN,\nMHA, a Gating layer, and a FF\nlayer\nPhysionet - 55.40%\n(Avg Acc)\nBrain-visual - 61.11%\n(Avg Acc)\nTemporal-Spatial Decomposition:\nUsed a two-encoder transformer\nstrategy focusing separately on\ntemporal and spatial EEG features,\nachieving outstanding results\nacross different EEG states.\n[150] Four states:\nEO, EC, PHY ,\nIMA.\nPhysionet dataset ETST:\n- TTE: A temporal transformer\nencoder\n- STE: A spatial transformer\nencoder\nSingle state (Avg\nAcc):\nEO - 100%\nEC - 99.96%\nPHY - 99.97%\nIMA - 100%\nTwo states (Avg\nAcc):\nPHY - 97.29%,\nIMA - 97.45%\nAll states (Avg\nAcc):\n99.90%\nProvided three sub-experiments,\nand outperformed many baseline\nmodels and shows strong general-\nization ability.\n[137] LH vs. RH vs. BF\nvs. T;\nBCI Competition IV 2a\ndataset:\n288 trials x 22 ch x 1000\nsamples\nCNN-Transformer:\n- Temporal 1D-CNN, MaxPooling,\nReLU activation, BN layer\n- PE, Transformer Encoder, Trans-\nformer Decoder, Classifier\n99.29% (Avg Acc) Integrated the entire vanilla trans-\nformer architecture with convolu-\ntional layers, achieving top-tier per-\nformance against other models such\nas SNN-LSTM.\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 4. (continued from previous page) Overview of Transformer Models in Motor Imagery Brain-Computer Interfaces (BCIs)\nRef Protocol EEG Data Model Description Performance Highlights\n[143]\nLH vs. RH BCI Competition 2003\nData III:\n280 trials.\nTransformers with Auto-Encoders:\n- FBCSP Feature Extraction\n- AE Dimensionality Reduction\n- Vanilla Transformer, Classifier\n91.30% (Avg Acc) Transformer with Auto-Encoders:\nImproved classification results\nby combining transformers with\nauto-encoders, outperforming\nKNN, LDA+KNN, and standalone\ntransformer models.\n[60] Two-class:\nleft fist/ right fist\n(L/R),\nThree-class:\nleft fist/ right\nfist/eyes open\n(L/R/O),\nFour-class:\nleft fist/ right fist/\neyes open/ feet\n(L/R/O/F)\nSI tests.\nMI Physionet dataset:\n3s of data 480 samples,\n6s of data 960 samples.\nFive different models:\n- s-Trans: spatial-Transformer\n- t-Trans: temporal-Transformer\n- s-CTrans: spatial-CNN,\nTransformer\n- t-CTrans: temporal-CNN,\nTransformer\n- f-CTrans: fusion-CNN,\nTransformer\nHighest Acc:\n3s data (f-CTrans):\nL/R: 83.31% (s-\nCTrans),\nL/R/O:\n74.44%,\nL/R/O/F:\n64.22%\n6s data (t-CTrans):\nL/R: 87.80%,\nL/R/O: 78.98%,\nL/R/O/F: 68.54%\nPresented five transformer-based\nmodels, focusing on spatial and\ntemporal EEG aspects. Achieved\nthe highest performance with\nfusion-based CNN and Transformer\nmodels on subject-independent\ntasks.\n[138] 2a:\nLH vs. RH vs.\nBF vs. T;\nWithin-subject\nclassification\nBCI Competition IV 2a:\n576 trials x 22 ch x 1000\nsamples\nHybrid CNN-Transformer:\n- Spatial Transformer,\n- Spectral Transformer (FBCSP),\n- CNNs, Temporal Transformer\n(FF, MHA),\n- Classifier.\nEarly stopping was applied.\n83.91% (Acc) Outperforming Benchmark:\nAchieved superior results on\nthe BCI IV dataset compared\nto previous state-of-the-art\nmethods, reinforcing the model’s\neffectiveness. [156]\n[145] Private dataset:\nRH vs. BF;\nBCI IV-1:\nLH vs. RH vs.\nBF\nPrivate dataset:\n320 trials x 60 ch\nBCI competition IV\ndataset 1:\n200 trials x 59 ch\nTransEEG:\n- CNN encoder: 2D-Conv layers,\nBN layer, ELU activation, Max-\nPooling, Dropout\n- Three transformer blocks with\ngraph embedding\nPrivate - 89.5% (Avg\nAcc)\nBCI IV-1 - 77.4%\n(Avg Acc)\nGraph Embedding for EEG:\nIntroduced graph embedding to\nrepresent multichannel EEG data,\nachieving robust and precise results\nby capturing spatial relationships\nbetween channels.\n[139] 2a:\nLH vs. RH vs.\nBF vs. T;\nOpenBMI:\nLH vs. RH;\nsession-\ndependent /\nsession-\nindependent\nBCI Competition IV 2a\ndataset:\n576 trials x 22 ch\nOpenBMI dataset: 400\ntrials x 20 ch\nCNN with an attention mechanism:\n- Spatial convolutional layers, BN,\nand ELU.\n- Temporal segmentation and fea-\nture extraction,\n- Temporal attention module, depth-\nwise separable convolution and\nMHA.\n- Classifier\nEarly stopping was applied.\n2a (Avg Acc):\nSession-dependent:\n82.32%\nSession-independent:\n79.48%\nOpenBMI (Avg\nAcc):\nSession-dependent:\n77.52%\nSession-independent:\n70.43%\nTemporal Dependencies: By lever-\naging attention mechanisms and a\n2D map for feature extraction, the\nmodel successfully exploited tem-\nporal dependencies in MI-EEG, en-\nhancing performance across multi-\nple datasets.\n[141] Multiple\nProtocols\nTUEG dataset;\nMI PhysioNet dataset;\nMI BCI IV 2a dataset;\nERN dataset: 56 ch\nP300 dataset: 64 ch\nSSC dataset: 2 ch.\nBENDR:\n- A series of 1D convolutions with\nshort-receptive fields and a trans-\nformer encoder.\nPhysioNet (Acc):\n86.7%\nBCI IV 2a (Acc):\n42.6%\nERN (AUROC):\n0.65%\nP300 (Acc): 0.72%\nSSC (AUROC):\n0.72%\nUniversal EEG Learning: Devel-\noped a strategy to learn from a wide\nrange of EEG data without requir-\ning labels. The approach promises\nmore generalized EEG models by\ntapping into broader EEG data\ndistributions.\n[129] 2a:\nLH vs. RH vs.\nBF vs. T;\n2b:\nLH vs. RH;\nSI / SD tests.\nBCI competition IV 2a and\n2b datasets\nTST:\n- ICA filter\n- Temporal and Spatial transforma-\ntions using an attention mechanism\n- Classifier: FC layer and GAP\nSD tests (Avg Acc):\n2a:\nTST: 96.11%\nTST-ICA: 97.77%\n2b:\nTST: 84.89%\nTST-ICA: 85.90%\n5-fold CV\n(TST-ICA) (Avg\nAcc):\n2a: 88.75%\n2b: 84.20%\nLOSO (TST-ICA)\n(Avg Acc):\n2a: 93.94%\n2b: 87.29%\nDual Approach with ICA: Demon-\nstrated the prowess of using both\ntemporal and spatial transformers,\nwith enhanced results when in-\ntegrating Independent Component\nAnalysis (ICA).\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 5. (continued from previous page) Overview of Transformer Models in Motor Imagery Brain-Computer Interfaces (BCIs)\nRef Protocol EEG Data Model Description Performance Highlights\n[152] AO+MI session:\nLH vs. RH;\nMI-FB sessions:\nLH vs. RH vs.\nNo Movement\nPrivate dataset:\nAO + MI session: 300 tri-\nals\nMI-FB sessions: 900 trials\nThe study incorporates TSTN, pro-\nposed by Song et al. (2021) [156], it\nfocuses on:\n- AO+MI: Action Observation and\nMI\n- MI-FB: MI with Feedback\n- Continual Learning strategy.\nAO+MI: 0.63 (Avg\nAcc)\n1st MI-FB: 0.68 (Avg\nAcc)\n2nd MI-FB: 0.75 (Avg\nAcc)\n3rd MI-FB: 0.77 (Avg\nAcc)\nVirtual Reality & Motor Imagery:\nGathered EEG data using Virtual\nReality (VR), exploring the poten-\ntial of combining action observa-\ntion with motor imagery tasks for\nricher EEG interpretations.\n[153] Multiple\nProtocols\nPrivate dataset:\n59 ch x 26400 epochs\nMITRT:\n- Transformer encoder: pre-\nprocessed EEG input\n- Transformer decoder: input\ncorrected joint points location input\n0.975 (PCC) Motion Trajectory in 3D: Success-\nfully reconstructed upper limb mo-\ntion trajectories from MI EEG sig-\nnals, showcasing potential applica-\ntions in understanding Chinese sign\nlanguage in a 3D space.\n[140] Multiple\nProtocols\nPrivate dataset:\n64 ch (MI and VI)\n200 trials (SI)\nBCI competition IV\n2a,\nArizona State University\n(ASU) dataset:\n200 trials\nMultiscale convolutional\ntransformer:\n- Temporal convolutional blocks\nbased on TSception,\n- A temporal transformer encoder,\n- Parallel spatial convolutional\nblocks,\n- A spatial transformer encoder,\n- A fusion convolutional block.\nPrivate: 0.62 (Acc)\nBCI IV 2a: 0.70 (Acc)\nASU: 0.70 (Acc)\nMultiscale Imagery Paradigm:\nDesigned a unique experimental\nparadigm encompassing motor,\nvisual, and speech imagery tasks,\npresenting a comprehensive\napproach for EEG data collection\nand interpretation.\n[157] LH, RH\nSI tests.\nBCI competition IV 2a:\n2592 trials x 22 ch x 321\nsamples\nBCI competition IV\n2b:\n6520 trials x 3 ch x 321\nsamples\nMI Physionet dataset:\n4683 trials x 64 ch x 201\nsamples\nWeibo dataset:\n1580 trials x 60 ch x 321\nsamples\nCNN with Vision Transformers:\n- s-CViT: Spatial CNN, Vision\nTransformer,\n- t-CViT: Temporal CNN, Vision\nTransformer,\n- st-CViT: Spatio-Temporal CNN,\nVision Transformer,\n- Classifier\n2a: 80.44% (Avg Acc)\n2b: 74.73% (Avg\nAcc)\nPhysionet: 83.08%\n(Avg Acc)\nWeibo: 73.88% (Avg\nAcc)\nPresented a realistic approach to\nbuilding subject-independent BCIs\nusing nested LOSO method and\ncombination of CNN and Vision\nTransformers.\n[133], [135], [136], [147], [149].\n• Susceptibility to Noise : Inherent noise within EEG\nsignals, compounded by external artifacts originating\nfrom muscle twitches, eye movements, or other external\nelectromagnetic interferences, can convolute the accu-\nrate decoding of MI patterns [129], [135], [140], [143],\n[153].\n• Non-stationarity: Overextended durations, EEG pat-\nterns linked with MI may undergo alterations at-\ntributable to factors such as fatigue, learning adapta-\ntions, or other dynamic neural processes, leading to\npotential decrements in decoding efficacy [132], [135],\n[143].\n• High Dimensionality: Given the time-series nature of\nEEG recordings, the data encapsulates high dimen-\nsionality. This necessitates the deployment of refined,\ncomputationally intensive algorithms to sift through,\nprocess, and efficaciously decode the embedded infor-\nmation [143], [147], [153].\nIt is crucial to take a comprehensive and diverse approach\nto the development and optimization of MI-BCI systems to\nensure their effectiveness in various real-world situations,\ngiven the aforementioned challenges.\nB. TRANSFORMER-BASED MODELS IN MI-BCI\nThe transformer architecture has been widely adopted in the\nresearch community to enhance Motor Imagery (MI) decod-\ning in BCIs. A number of EEG analysis models have been\ndevised and assessed in this context, with technical details\nof certain models summarized in Table 3, Table 4, and Table\n5. These tables outline distinct approaches, datasets used for\ntesting, and their corresponding performance outcomes. In\naddition to the technical details of the studies, we include\nTable 2 to provide a better understanding of the data sources\nused. This table outlines each dataset’s specific details and is\norganized with various columns that offer a clear overview\nof each dataset. The “Private Datasets” section is proprietary\nand absent from public access, while the “Other Datasets”\nsection covers datasets with applications beyond Motor Im-\nagery, such as speech imagery.\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nA study conducted by Ma et al. [138] demonstrates signif-\nicant progress in this field. Their hybrid CNN-Transformer\nmodel, which includes spatial, spectral, and temporal trans-\nformers, achieved an impressive MI-EEG decoding perfor-\nmance improvement with an accuracy of 83.91%. Following\na similar path, Wu et al. [145] introduced the TransEEG\nmodel. This model combines a CNN encoder with trans-\nformer blocks and enhances it further with graph embedding.\nIt achieved accuracies of 89. 5% and 77. 4% in their private\ndata set and the BCI IV-1 dataset, respectively. Ma et al.\npresented important work [139], where their CNN model\nequipped with an attention mechanism decoded temporal\nEEG features. Their work performed well on the BCI Com-\npetition IV 2a and OpenBMI datasets, achieving session-\ndependent accuracies of 82.32% and 77.52% and session-\nindependent accuracies of 79.48% and 70.43%, respectively.\nKostas et al. [141] presented the BENDR methodology, a\nunique approach that uses unlabeled EEG data. Their results\nwere impressive across several MI datasets, notably achiev-\ning 86.7% accuracy on the PhysioNet dataset. Moving for-\nward, Hameed et al. [129] introduced the Temporal-Spatial\nTransformer model. This model incorporates an ICA filter\nand attention mechanisms, achieving remarkable accuracies\nof 96.11% and 84.89% in the BCI IV 2a and 2b datasets,\nrespectively. In a novel merger of BCI and VR, Lee et al.\n[152] employed the TSTN model with continuous learn-\ning. Through VR-aided MI tasks, they achieved progressive\naccuracy increments in AO+MI sessions, showcasing the\ntransformative power of VR in MI EEG research. Wang et\nal. [153] embarked on a unique endeavor, using MI EEG\nsignals to decode sign language. With their Motion Imagery\nTrajectory Reconstruction Transformer model, they achieved\nan impressive accuracy of 0.975 in reconstructing motion\ntrajectories. Ahn et al. [140] further contributed to the field\nwith their multiscale convolutional transformer, which com-\nbines various imagery tasks. This model achieved notable\naccuracies of 0.62, 0.70, and 0.70 on their private dataset, the\nBCI IV 2a dataset, and the Arizona State University (ASU)\ndataset, respectively. The aforementioned studies show the\nremarkable potential of transformer models in MI-EEG de-\ncoding.\nSeveral other research groups have proposed novel trans-\nformer models for MI-BCI systems. One notable innovation\nis the hierarchical transformer introduced by Deny et al.\n[147], which partitions attention across two levels: a low-\nlevel transformer for feature extraction and a high-level trans-\nformer for highlighting crucial features. Tan et al. [132] com-\nbined V AT regularization with Particle Swarm Optimization\nto enhance EEGNet with self-attention. Wang et al. [151]\nfused EEG channel attention with the Swin Transformer,\ndiverging from traditional CSP and CNN models. Addition-\nally, Luo et al. [133] ventured into data augmentation and\nprobability ensembling with a shallow mirror transformer.\nSong et al. [135] developed an attention-based domain adap-\ntation model to enhance decoding across subjects. Tailoring\ntransformer models to BCI’s unique requirements, Tao et\nal. [149] demonstrated the efficacy of gated Transformer\nmodels. In contrast, Jiang et al. [143] combined Auto En-\ncoders with FBCSP for efficient feature extraction. In addi-\ntion to these contributions, Xie et al. [60] combined spatial\nand temporal CNN transformers, achieving good results in\nsubject-independent scenarios. Ma et al. [138] refined the\nspectral transformer, while Wu et al. [145] explored graph\nembeddings for dynamic extraction. Reflecting the versa-\ntility of transformer models, Kostas et al. [141] delivered\na methodology that spans multiple EEG datasets. In an\nimaginative blend, Lee et al. [152] integrated their model\nwithin the immersive realm of virtual reality, creating a data\nset rooted in VR-based motor imagery tasks. Other novel\napplications include Wang et al.’s [153] effort to interpret\n3D motion from Chinese sign language, and Ahn et al.’s\n[140] multiscale convolutional transformer for diverse mental\nimagery decoding.\nWhen considering the specific uses of transformer models\nin the context of MI-BCIs, we can identify several important\nutilities:\n• Feature Extraction: One of the prominent advantages\nof transformers is their ability to automatically extract\nfeatures from raw EEG signals. This largely eliminates\nthe need for laborious and complex manual feature\nengineering, which has traditionally been a significant\npart of BCI research [129], [132], [150].\n• Robustness to Noise : Transformers utilize multi-head\nattention mechanisms to focus on the relevant portions\nof an EEG sequence selectively. This feature contributes\nto substantial reductions in the influence of noise and\nother artifacts, leading to more reliable BCI outputs\n[129], [132], [135], [153].\n• Temporal Dynamics: The Transformer architecture is\nparticularly well-suited for handling the sequential na-\nture of EEG data. This enables it to capture the temporal\ndynamics of Motor Imagery (MI) with greater efficacy\nthan traditional models such as CNNs or RNNs. This\nhas been shown to improve decoding accuracy across\nmultiple studies [129], [134], [136], [137], [147], [149],\n[153].\n• Transfer Learning: The adaptability of Transformer\nmodels also allows them to benefit from pre-training\non large datasets. This enables these models to start at\na more advanced point when tailored for specific BCI\ntasks, potentially mitigating challenges related to inter-\nsubject variability [135].\n• Hybrid Models: Some research efforts have explored\nthe combination of Transformers with other machine\nlearning architectures, such as CNNs. These hybrid\nmodels aim to capture spatial and temporal features\nmore effectively for enhanced MI decoding [132]–\n[134], [137], [150], [165].\nThe research landscape in MI-EEG decoding shows a\ngrowing interest in transformer models. Various models have\nshown promising results in different datasets and methodolo-\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\ngies. These advancements highlight the significant impact of\nthese models and the wide potential of BCI research.\nC. EMOTION RECOGNITION\nBCIs, originally designed to aid communication and control\nfor those with motor impairments, are now expanding in their\npotential applications [166]. Recent advancements in BCIs\nhave shown promise in recognizing and decoding emotions\ndirectly from the brain, with potential uses in entertainment\nand medical therapy [167], [168]. By accurately detecting\nhuman emotions, machines can respond in a more empathetic\nmanner, resulting in more natural and personalized interac-\ntions [169]. Furthermore, continuous emotional monitoring\ncan facilitate the early detection of mood disorders and severe\nconditions such as post-traumatic stress disorder (PTSD)\n[164]. The entertainment industry, particularly gaming and\nvirtual reality, can capitalize on this technology to customize\nuser experiences based on their emotions, leading to height-\nened engagement and satisfaction [170]. BCIs can also offer\nreal-time emotional feedback for patients undergoing therapy\nfor trauma or emotional disorders, enabling therapists to de-\nvise more effective and tailored treatment plans [171]. Emo-\ntion recognition BCIs utilize EEG data for its high temporal\nresolution [170]. With advancements in signal processing and\nmachine learning, including the use of Transformers [171],\nemotion decoding accuracy has significantly improved.\nD. TRANSFORMER MODELS IN EMOTION\nRECOGNITION\nSeveral EEG datasets related to emotions were examined as\nsummarized in Table 6. These datasets vary in the number\nof subjects, trials per session, EEG channels used, emo-\ntional states examined, and relevant academic literature. The\nDEAP and SEED datasets have gained popularity for EEG-\nbased emotion recognition, as indicated by insights from\nthese studies. A number of studies on the applications of\ntransformers in BCI-based emotion recognition have been\ndevised and assessed in this context, with technical details\nof certain models summarized in Table 7, Table 8, and Table\n9. These tables outline distinct approaches, datasets used\nfor testing, and their corresponding performance outcomes.\nStudies presented in the tables have empirically demonstrated\nthe effectiveness of Transformers in capturing long-term\ndependencies in EEG data for emotion recognition tasks.\nThey also highlight the challenges and potential for further\noptimization in this domain. Transformers, when combined\nwith other techniques such as convolution or tailored for\nspecific spatial-temporal features, can achieve impressive\nresults. However, a common challenge across these studies is\nthe decrease in performance in subject-independent classifi-\ncation tasks, underscoring the need for models that generalize\nwell across different individuals.\nFor instance, Li et al. [61] achieved superior classifi-\ncation accuracies by utilizing the DEAP and DREAMER\ndatasets. Their proposed Transformer Neural Architecture\nSearch model, which integrated a Supernet with a Multi-\nObjective Evolutionary Algorithm (MOEA) and a classifier,\nachieved the highest average accuracy in emotion classifi-\ncation for the DREAMER dataset. Koorathota et al. [185]\nintroduced the Multimodal Neurophysiological Transformer\n(MNT) on the DEAP dataset. By integrating raw time se-\nries with extracted features, their model demonstrated the\npotential for sequential modeling of EEG data, achieving\nnotable results for valence and arousal. Xiao et al. [173]\nworked with the DEAP, SEED, and SEED-IV datasets. Their\nfour-dimensional attention-based neural network, which in-\ntegrated spectral, spatial, and temporal attention mecha-\nnisms, achieved commendable performance across multiple\ndatasets. Wang et al. [190] addressed binary and four-class\nemotion classifications using the DEAP and MAHNOB-HCI\ndatasets. Their Hierarchical Spatial Learning Transformer,\nfocusing on electrodes and brain-region-level spatial learn-\ning, highlighted the contribution of brain regions in capturing\nenhanced spatial dependencies. Sun et al. [62] leveraged\nthe DEAP, SEED, and SEED-IV datasets and proposed a\nDual-Branch Dynamic Graph Convolution with Adaptive\nTransformer Feature Fusion. Their proposed model achieved\nimpressive results on SEED and SEED-IV , showcasing its\npotential. Arjun et al. [187] reported exceptionally high ac-\ncuracies using Continuous Wavelet Transform (CWT) and\nraw EEG signal for emotion classification. Although specific\nmodel details were not provided, their study demonstrated\nthe effectiveness of these techniques. In addition to these\nstudies, Song et al. [134] presented an EEG Conformer model\nthat effectively generalized across both Motor Imagery (MI)\nand emotion recognition tasks. Liu et al. [191] proposed the\nEEG Emotion Recognition Transformer (EeT) and achieved\nnoteworthy results, particularly on the SEED dataset.\nThe aforementioned studies highlight the effectiveness of\nTransformers in utilizing a multi-head attention mechanism\nto focus on relevant parts of an EEG sequence. This improves\nthe model’s ability to filter out noise, leading to enhanced ac-\ncuracy in emotion recognition. Additionally, studies demon-\nstrate that Transformer models are scalable, benefiting from\npre-training on extensive datasets and fine-tuning on specific\nBCI emotion datasets. This allows the models to leverage\nprior knowledge and achieve more precise emotion decoding.\nThese models are also flexible and can be customized to\nrecognize a wide range of emotions, including subtle shifts in\nemotional states. This capability enables a detailed emotional\nspectrum instead of simple or binary classifications.\nThe use of BCIs for emotion recognition holds transforma-\ntive potential across various sectors. As our understanding of\nthe neural basis of emotions and Transformer architectures\ndeepens, we can expect continuous improvements in the\naccuracy, reliability, and adaptability of emotion recognition\nBCIs. With these advancements, significant progress is antic-\nipated in this field in the coming years.\nE. OTHER EMERGING APPLICATIONS\nThe potential of transformer models in the field of BCIs\nextends beyond motor imagery decoding and emotion recog-\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 6. Overview of EEG Datasets Utilized for Emotion Analysis with Transformers\nDataset Name Subjects Channels Emotions Studies Used\nSEED [172] 15 62 Positive, Negative, Neutral [134], [173]–[181]\nSEED-IV [182] 15 62 Neutral, Sad, Fear, Happy [173]–[177], [181]\nDREAMER [183] 23 14 Potency, Arousal, Dominance [61], [177], [180]\nDEAP [184] 32 32 Valence, Arousal, Liking, Dominance [61], [173], [174], [185], [186] [178], [180], [181], [187]\nMAHNOB-HCI [188] 27 32 Arousal, Valence, Dominance, Predictability, Emotional Keywords [186]\nPrivate Datasets\nPrivate dataset 1 (HIED) 30 64 Happiness, Inspiration, Neutral, Anger, Fear, Sadness [178]\nPrivate dataset 2 32 32 Positive, Negative, Neutral [189]\nnition. Several lesser-known but promising areas of EEG\nresearch are utilizing transformer models, hinting at a future\nwhere our understanding and interaction with the brain will\nbe transformed. In this section, we will outline some emerg-\ning applications that harness the power of transformers.\nWhile motor imagery is already extensively explored,\nanother promising avenue is language reconstruction from\nneural data. The goal is to reconstruct perceived or imagined\nspeech directly from neural signals. Achieving this would\nhave profound implications, enabling locked-in patients to\ncommunicate or even translating thoughts into understand-\nable speech. Additionally, sleep stage classification can ben-\nefit from transformer intervention. Traditionally, EEG data\nhas been used to classify different sleep phases. Transform-\ners, with their ability to process and assign significance to\ntemporal data points, offer improved accuracy in discerning\nsleep stages.\nIn Tables 10 and 11, we present studies covering emerging\napplication areas of transformer models including person\nidentification, sleep stage classification, speech reconstruc-\ntion, epilepsy prediction, Alzheimer’s disease detection, and\nseizure detection. A study by Du et al. [150] focused on using\nan EEG temporal-spatial transformer for person identifica-\ntion. They used an EEG temporal-spatial transformer, which\nconsisted of a temporal transformer encoder (TTE) and a\nspatial transformer encoder (STE) and achieved impressive\nresults, with accuracies ranging from 97.29% to 100% for\ndifferent states of EEG signals. Transformers have also\nshown promise in sleep stage classification. Dai et al. [56]\nproposed a multi-channel sleep network that combined trans-\nformer encoders with other feature extraction techniques.\nTheir approach achieved accuracies of 85.0% to 87.5% on\ndifferent datasets, demonstrating a strong correlation with\nthe physiological features of sleep stages. Kostas et al.\n[141] presented a methodology called BENDR that utilized\nTransformers for analyzing multiple domains of EEG data,\nincluding motor imagery, event-related potentials, and sleep\nstaging. Their approach achieved competitive performance\non different datasets, showcasing the ability to learn from\ndiverse EEG tasks and generalize well. In another study,\nLee et al. [193] explored the classification of imagined\nspeech and overt speech using EEG signals. They proposed\na classification framework that incorporated convolution lay-\ners, separable convolution layers, self-attention mechanisms,\nand feed-forward networks. The results indicated that overt\nspeech recognition outperformed imagined speech, although\nthe difference was not as significant as initially anticipated.\nSeveral other studies have investigated the use of Trans-\nformers for seizure detection and prediction. Hussein et\nal. [194] introduced MViT, a multi-channel vision Trans-\nformer, for epileptic seizure prediction. Their model achieved\nhigh prediction sensitivity across different public datasets.\nSimilarly, Hu et al. [195] proposed a hybrid Transformer\nmodel for epilepsy prediction, which demonstrated excel-\nlent performance compared to CNN-based structures. The\napplication of Transformers in Alzheimer’s disease detec-\ntion has also been explored. Ravikanti [196] developed\nEEGAlzheimer’sNet, a transformer-based attention LSTM\nnetwork for detecting Alzheimer’s disease using EEG sig-\nnals. The model achieved high accuracy and demonstrated\npotential for early diagnosis. Transformers have been applied\nto speech recognition and motor action recognition tasks as\nwell. Murphy et al. [106] successfully decoded unigram and\nbigram parts-of-speech tags from single-trial EEG data using\nTransformers. Kaushik et al. [197] proposed an ensemble of\nBLSTM-LSTM and EEG-Transformer models for motor ac-\ntion recognition, achieving superior performance compared\nto existing methods. These studies represent just a fraction of\nthe emerging applications of Transformers in EEG research.\nThe versatility and effectiveness of Transformers in handling\nEEG data offer exciting possibilities for advancing our under-\nstanding of brain dynamics and developing innovative EEG-\nbased applications in various domains.\nV. ADVANTAGES OF TRANSFORMER-BASED MODELS\nFOR BCIS\nBased on the earlier reviewed studies, the integration of\ntransformer-based models in BCIs is a remarkable achieve-\nment. This is due to their proven effectiveness in processing\nsequential data. In the subsequent section, we will outline the\nessential advantages of utilizing transformer architectures in\nBCI research.\nA. HANDLING OF TEMPORAL SEQUENCES IN EEG\nEEG data, intrinsic to many BCI applications, is inherently\ntemporal. It captures the dynamic changes in the brain’s elec-\ntrical activity over time. Traditional models such as RNNs\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 7. Summary of Reviewed Studies on the Applications of Transformers in BCI-based Emotion Recognition (Continued on next page)\nRef Protocol EEG Data Model Description Performance Highlights\n[61] arousal, valence,\ndominance\nDEAP:\n40 videos x 32 ch x 8064\ndata\nDREAMER:\n18 videos x 14 ch x 25472\ndata\nTNAS model:\n- Supernet with a Multi-Objective\nEvolutionary Algorithm (MOEA)\n- Incorporates an Emotion\nClassifier\nDEAP (Avg Acc):\nA(98.66%)\nV(98.68%)\nD(98.67%)\nDREAMER (Avg\nAcc):\nA(96.95%)\nV(96.41%)\nD(96.90%)\nThe proposed model set a new\nbenchmark in emotion classifica-\ntion using the DREAMER dataset,\nshowcasing its potential for real-\nworld applications.\n[185] valence, arousal DEAP:\n40 videos x 32 ch\nMNT:\n- Adaptable Conv1D\n- Crossmodal Transformer: PPG,\nEEG, GSR, Freq.\n- Self-attention, Classifier\nV(58.0%) (Acc)\nA(69.4%) (Acc)\nMultimodal Integration: Introduced\na novel transformer designed\nspecifically for multimodal\nneurophysiological data, achieving\ncompetitive results by leveraging\nEEG, PPG, and GSR data streams.\n[173] DEAP:\nvalence, arousal\nSEED:\npositive, neutral,\nnegative\nSEED-IV:\nneutral, sad, fear,\nhappy\nIntra-subject\nsplitting\nDEAP;\nSEED;\nSEED-IV .\n4D-aNN:\n- Spectral and spatial attention\nmechanisms\n- CNN for spectral and spatial infor-\nmation of the 4D representations.\n- Temporal attention mechanism\nis integrated into a bidirectional\nLSTM\nDEAP (Acc):\nV(96.90%),\nA(97.39%)\nSEED (Acc): 96.25%\nSEED-IV (Acc):\n86.77%\nAttention-Driven Adaptability:\nThe 4D attention-based neural\nnetwork adeptly harnessed\nattention mechanisms to recognize\ndiscriminative EEG patterns,\noptimizing model performance\nacross several datasets.\n[190] DEAP:\narousal, valence,\ndominance\nMAHNOB-\nHCI:\narousal, valence,\ndominance,\npredictability,\nemotional\nkeywords\nSI tests.\nDEAP:\n40 trials x 32 ch\nMAHNOB-HCI:\n32 ch\nBinary classification\n(samples):\n34599 (DEAP)\n29952 (MAHNOB-HCI)\nFour-class classification\n(samples):\n12635 (DEAP)\n12338 (MAHNOB-HCI)\nHSLT:\n- EEG feature extraction (PSD\nfeatures)\n- Division of the electrode patches\n(pre-frontal, frontal, and so on, 9\nclusters),\n- Electrode-level spatial learning\n(Linear embedding + Transformer\nencoder)\n- Brain-region-level spatial learning\n(Linear embedding + Transformer\nencoder)\n- Inspired from Vision\nTransformers.\nBinary (Acc):\nDEAP:\nA(65.75%),\nV(66.51%)\nMAHNOB-HCI:\nA(66.20%),\nV(66.63%)\nSpatial Hierarchies in EEG:\nThe hierarchical spatial learning\ntransformer effectively delineated\nbrain region contributions and inter-\nregion dependencies, enhancing\nthe model’s understanding of\nspatial EEG data and achieving\ncommendable binary classification\nresults.\n[62] SEED:\nnegative,\npositive, neutral\nSEED-IV:\nneutral, sad, fear,\nhappy\nvalence,\narousal, liking,\ndominance\nDEAP;\nSEED;\nSEED-IV .\nDBGC-ATFFNet-AFTL:\n- Dual-branch dynamic graph\nconvolution\n- Adaptive transformer feature\nfusion\nSEED (Acc): 97.31%\nSEED-IV (Acc):\n89.97%\nDEAP (Acc):\nV(95.91%),\nA(94.61%)\nThe DBGC-ATFFNet-AFTL\nmodel combined dual-branch\ndynamic graph convolution with\nadaptive transformer feature fusion,\nenhancing the model’s ability to\ncapture EEG channel connections\nand efficiently fuse features.\n[187] DEAP:\nvalence,\narousal, liking,\ndominance\nDEAP ViT for CWT images and the raw\nEEG signal:\n- Patch Embeddings, PE\n- Transformer Encoder, MLP,\nClassifier\nCWT (Avg Acc):\nV(97%), A(95.75%)\nRaw EEG (Avg\nAcc):\nV(99.4%), A(99.1%)\nUtilizing both CWT image repre-\nsentations and raw EEG signals,\nthe proposed ViT model achieved\nexceptionally high accuracies for\nvalence and arousal classifications,\nshowcasing the versatility of the\napproach.\n[134] MI:\nLH vs. RH vs.\nBF vs. T\nSEED\n(emotions):\npositive, neutral,\nnegative\nSD tests\nBCI competition IV 2a\nand 2b datasets (MI);\nSEED:\n62 ch x 3394 samples\nEEG Conformer:\n- Convolution module, average\npooling\n- Self-attention module, classifier\n2a: 78.66% (Avg)\n2b: 84.63% (Avg Acc)\nSEED: 95.30% (Avg\nAcc)\nDemonstrated high accuracy across\nboth motor imagery (MI) and emo-\ntion recognition tasks, showcasing\nthe model’s generalizability across\nparadigms.\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 8. (continued from previous page) Summary of Reviewed Studies on the Applications of Transformers in BCI-based Emotion Recognition\nRef Protocol EEG Data Model Description Performance Highlights\n[191] DEAP:\nvalence, arousal\nSEED:\npositive, neutral,\nnegative\nSEED-IV:\nneutral, sad, fear,\nhappy\nDEAP;\nSEED;\nSEED-IV .\nEeT:\n- S: Spatial attention\n- T: Temporal attention\n- S-T: Sequential spatial-temporal\nattention\n- S+T: Simultaneous spatial-\ntemporal attention\n- DNN, Classifier\nDEAP (Avg Acc):\nA(93.34%),\nV(92.86%)\nSEED (Avg Acc):\n96.28%\nSEED-IV (Avg\nAcc): 83.27%\nReported that the simultaneous\nspatio-temporal attention gets the\nbest results among the four de-\nsigned structures, the result is also\nbetter than most state-of-the-art\nmethods.\n[175] SEED:\nnegative,\npositive, neutral\nSEED-IV:\nneutral, sad, fear,\nhappy\nSEED:\n45 trials x 62 ch x 5076\nsamples\nSEED-IV:\n72 trials x 62 ch x 5043\nsamples\nACTNN:\n- Feature extration, Spatial projec-\ntion\n- Spatial and spectral attention\nbranch\n- Spatial-spectral convolution part\n- Temporal encoding, Classifier\nSEED (Avg Acc):\n98.47%\nSEED-IV (Avg\nAcc):\n91.90%\nThe ACTNN achieved the best\nclassification rates, particularly ex-\ncelling in distinguishing positive\nand neutral states in SEED and\nidentifying sadness in SEED-IV .\n[192] valence,\narousal, liking,\ndominance\nCSE.\nDEAP:\n32 ch x 2400 samples\nADDA-TF:\n- Domain adaptation, attention\nmechanism\n- Feature-Channel Transformer\n- Global Temporal Transformer\nV(0.61), A(0.64) (Avg\nAcc)\nThe ADDA-TF reached the high-\nest performance than single TF or\nADDA, proving the advantage of\ntheir combination.\n[176] SEED:\nnegative,\npositive, neutral\nSEED-IV:\nneutral, sad, fear,\nhappy\nSEED:\n75 trials x 62 ch\nSEED-IV:\n72 trials x 62 ch\nBi-ViTNet:\n- Spatial-frequency feature extrac-\ntion branch\n- Spatial-temporal feature extrac-\ntion branch\n- Each branch: Linear Embedding\nand Transformer Encoder\n- Classifier\nSEED (Avg Acc):\n97.55%\nSEED-IV (Avg\nAcc):\n88.08%\nDual-branch Efficiency: The Bi-\nViTNet’s spatial-frequency branch\noutperformed its spatial-temporal\ncounterpart, underscoring the sig-\nnificance of spatial-frequency fea-\ntures in emotion recognition.\n[177] SEED:\nnegative,\npositive, neutral\nSEED-IV:\nneutral, sad,\nfear, and happy\n/ valence and\narousal ratings.\nDREAMER:\npotency, arousal,\ndominance\nCSE\nSEED;\nSEED-IV;\nDREAMER.\nSTGATE:\n- TLB: Transformer learning block,\nutilizes 2D-CNN and Transformer\nEncoder\n- STGAT: Spatial-temporal Graph\nAttention (STGAT) mechanism, to\nlearn temporal information\nSEED (Avg Acc):\n90.37%\nSEED-IV (Avg\nAcc):\n76.43%\nDREAMER: 76.35%\nTopological Learning: STGATE’s\napproach tackled non-Euclidean\ndata with a topological graph\nstructure, addressing the limitations\ninherent in CNNs and boosting\nperformance across multiple\ndatasets.\n[178] HIED:\nhappiness,\ninspiration,\nneutral, anger,\nfear, sadness\nSEED:\nnegative,\npositive, neutral\nDEAP:\nvalence, arousal\nSI / SD tests.\nPrivate dataset (HIED):\n54000 samples\nSEED:\n13500 samples\nDEAP:\n51200 samples\nSECT:\n- CT and S-CT: LN, MLP\n- Classifier\nHIED (Avg Acc):\nPSD: 82.51% (SD)\nDE: 84.76% (SD)\nPSD(50.12%) (SI)\nDE(52.94%) (SI)\nSEED (Avg Acc):\n85.43% (SI)\nDEAP (Avg Acc):\nV(66.83%),\nA(65.31%) (SI)\nSuperior Decoding Efficiency:\nDemonstrated leading performance\nacross various datasets and states\nwith notable standard deviation\nimprovements, particularly for\nhearing-impaired subjects using\ndifferential entropy (DE).\n18 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 9. (continued from previous page) Summary of Reviewed Studies on the Applications of Transformers in BCI-based Emotion Recognition\nRef Protocol EEG Data Model Description Performance Highlights\n[180] DEAP:\narousal,\nvalence, liking,\ndominance\nDreamer:\narousal, valence\nSEED:\npositive,\nnegative, neutral\nDEAP:\n40 trials x 32 ch x 2400\nsamples/subject\nDreamer;\nSEED.\nTSFFN:\n- Transformer, 3D-CNN\n- Temporal and spatial feature ex-\ntraction modules\n- Temporal-spatial feature fusion\nmodule.\nDEAP (Acc):\nTSDFN: A(96.14%),\nV(95.76%)\nTSFFN: A(98.53%),\nV(98.27%)\nDreamer (Acc):\nTSFFN: A(97.74%),\nV(96.80%)\nSEED (Acc):\nTSFFN: 97.64%\nProven Model Adequacy: With the\nTSFFN, strong performance was\nconsistently observed across DEAP,\nDreamer, and SEED datasets, vali-\ndating the proposed model’s utility.\n[179] SEED:\npositive,\nnegative, neutral\nSI / SD tests.\nSEED:\n45 trials\nDCoT:\n- DW-CONV: Depthwise convo-\nlutionlayer, PE, learnable embed-\ndings, Transformer encoders, linear\nlayers.\n- Transformer encoder: LN, MHSA,\nFF\n93.83% (SD) (Avg\nAcc)\n83.03% (SI) (Avg\nAcc)\nInterpretable Feature Extraction:\nThe DCoT model offers not just\nhigh performance but also deeper\ninsights into significant brain areas\nduring emotional activities.\n[189] Ternary\nclassification:\npositive, negatve,\nneutral\nBinary\nclassification:\nPositive, negative\nPrivate dataset:\n32 ch x 512 samples/epoch\n240 epochs (binary),\n360 epochs (ternary)\nSpatial-temporal transformer:\n- Two channels, spatial and tempo-\nral EEG epochs\n- Linear Projection, Transformer\nEncoder\n- Weighted sum of the outputs from\nthe two channels.\nBinary (Avg Acc):\n97.3%\nTernary (Avg Acc):\n97.1%\nHybrid Model Enhancement:\nDemonstrated that combining\nspatial and temporal EEG data\nwith transformers can significantly\nimprove emotion recognition\naccuracy.\n[181] DEAP:\nvalence,\narousal, liking,\ndominance.\nSEED:\nnegative,\npositive, neutral.\nSEED-IV:\nhappy, sad, fear,\nneutral.\nDEAP:\n40 videos x 32 ch x 60\nfeatures x 5 freq bands.\nSEED:\n45 videos x 62 ch x 185\nfeatures x 5 freq bands.\nSEED-IV:\nsession1:\n45 videos x 62 ch x 10\nfeatures x 5 freq bands,\nsession2:\n45 videos x 62 ch x 12\nfeatures x 5 freq bands,\nsession3:\n45 videos x 62 ch x 14\nfeatures x 5 freq bands.\nMSDTTs:\n- MST: multi-domain spatial trans-\nformer module\n- DTT: dynamic temporal trans-\nformer module\nDEAP (Avg Acc):\nV(98.91%),\nA(98.89%),\nβ frequency band\nmax\nSEED (Avg Acc):\n97.52%, γ frequency\nband max\nSEED-IV (Avg\nAcc):\n96.70%,γ\nPositive Emotion Recognition Bias:\nIn multi-domain analysis, positive\nemotions consistently showed\neasier recognition and higher\naccuracies than their negative\ncounterparts.\nand LSTMs were initially favored for such sequence data.\nHowever, transformers, through their self-attention mecha-\nnisms, offer a more adaptive way to weigh different time\npoints based on their importance, ensuring that recent data\npoints do not outweigh crucial temporal patterns.\nB. ABILITY TO CAPTURE LONG-TERM DEPENDENCIES\nTransformers excel at recognizing long-term dependencies in\ndata due to their parallel processing and dynamic weighting\nthrough attention mechanisms. In the context of BCIs, brain\nsignals often carry patterns where an earlier signal might\ninfluence a much later one, and recognizing this relationship\ncan be important for accurate decoding. While LSTMs were\ndesigned to mitigate the vanishing gradient problem of RNNs\nand capture such dependencies, transformers excel in this\ndomain due to their parallel processing of sequences and the\ndynamic weighting through attention mechanisms.\nC. SCALABILITY AND PERFORMANCE BENEFITS\nTransformer models offer numerous advantages for analyz-\ning large EEG datasets. With sufficient computational infras-\ntructure, these models can be trained effectively on extensive\nEEG recordings, leading to the extraction of complex neural\npatterns that are often overlooked by less capable models.\nUnlike sequential models such as RNNs, transformer models\ncan be trained faster due to their capacity for parallel pro-\ncessing. Additionally, streamlined models such as distilled\ntransformers help mitigate computational restrictions. Hence,\ntransformer-based architectures can be favored in BCIs, espe-\ncially when EEG data exhibit high variability and noise.\nD. VERSATILITY ACROSS DIVERSE BCI TASKS\nThe architectural design of transformers makes them adapt-\nable. Whether it is motor imagery tasks, emotion recogni-\ntion, or sleep stage classification, the underlying principles\nof transformers remain consistent. This versatility ensures\nVOLUME 4, 2016 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 10. Overview of Emerging Applications Using Transformers Across a Spectrum of EEG-driven Applications (Continued on next page)\nRef Application areaProtocol EEG Data Model Description Performance Highlights\n[150] Person\nidentification\nFour states:\nEO, EC, PHY ,\nIMA\nMI Physionet dataset ETST:\n- TTE: A temporal transformer\nencoder\n- STE: A spatial transformer\nencoder\nSingle state (Avg\nAcc):\nEO - 100%\nEC - 99.96%\nPHY - 99.97%\nIMA - 100%\nTwo states (Avg\nAcc):\nPHY - 97.29%,\nIMA - 97.45%\nAll states (Avg\nAcc):\n99.90%\nExceptional Sensitivity: Demon-\nstrated robust performance across\nvarious states, especially excelling\nin the MI Physionet dataset.\n[56] Sleep Stage\nclassification\nW, N1, N2, N3,\nREM.\nSleepEDF-20:\n43141 samples\nSleepEDF-78:\n196350 samples\nSHHS:\n324854 samples\nMultiChannelSleepNet:\n- Single-channel feature extraction\n(PE + Transformer Encoder);\n- Multichannel feature fusion (LN,\nPE, Transformer Encoder);\n- Classifier\nSleepEDF-20 (Acc):\n87.2%\nSleepEDF-78 (Acc):\n85.0%\nSHHS (Acc): 87.5%\nPhysiological Correlation:\nAchieved a correlation with\nphysiological sleep stage features\nusing MultiChannelSleepNet.\n[141] Multiple\ndomains:\nMI, ERN, ERP,\nsleep staging\nMultiple\nProtocols\nTUEG dataset;\nMI PhysioNet dataset;\nMI BCI IV 2a dataset;\nERN dataset: 56 ch\nP300 dataset: 64 ch\nSSC dataset: 2 ch.\nBENDR:\n- A series of 1D convolutions with\nshort-receptive fields and a trans-\nformer encoder.\nPhysioNet (Acc):\n86.7%\nBCI IV 2a (Acc):\n42.6%\nERN (AUROC):\n0.65%\nP300 (Acc): 0.72%\nSSC (AUROC):\n0.72%\nVersatility in Unlabeled Data:\nBENDR showcases the ability\nto learn across varied EEG data\ndistributions, spanning multiple\npeople, sessions, and tasks without\nlabeled data.\n[193] Imagined speech\nand overt speech\n12 words\n(ambulance,\nclock, hello, help\nme, light, pain,\nstop, thank you,\ntoilet, TV , water,\nand yes) and\nresting state.\nPrivate dataset:\n9 subjects, 300\ntrials/condition, 25\nexperiments/every 12\nwords.\n- Convolution layers, separable con-\nvolution layers;\n- Self-attention, FF, Dropout;\n- Residual connection with subse-\nquent LN.\nImagined speech:\n35.07% (Avg Acc)\nOvert speech:\n49.5% (Avg Acc)\nComparative Analysis: Overt\nspeech EEG showed marginally\nbetter performance compared\nto imagined speech, contrary to\nsignificant differences expected.\n[102] Sleep Stage\nclassification\nW, REM, N1, N2,\nN3\nPrivate dataset:\n6 EEG channels, only 1\nused.\nTrain (1590 patients),\nVal (341 patients),\nTest (343 patients)\n- Transformer\n- Transformer + RNN (CNN +\nTransformer Encoder)\n- Inner + Outer Transformer (Trans-\nformer Encoders, where the output\nof first is the input of the second)\nSingle-Epoch (Acc):\nTransformer\n(89.50%)\nMulti-Epoch (Acc):\nTransformer + RNN\n(91.38%)\nInner + Outer\nTransformer\n(91.45%)\nAdvanced Hybrid Approaches: The\nmulti-epoch model variants, espe-\ncially the Inner + Outer Trans-\nformer, exhibited superior perfor-\nmance in sleep stage classification.\n[198] Sleep Stage\nclassification\nW, N1, N2, N3,\nREM\nMontreal Archive of Sleep\nStudies (MASS),\nSleep-EDF dataset\nResidual based attention model:\n- Feature extractor: CNN, Maxpool-\ning\n- Encoder: residual blocks, GAP\nlayer\n- Decoder: MHA, temporal context\nMASS: 86.5% (Acc)\nSleep-EDF:\n80.7% (Acc)\nAccelerated Processing: The\nresidual-based attention model\nprovides both training and\ninference speeds that are over\nten times faster than other methods.\n[199] Sleep Stage\nclassification\nSleep-EDF:\nN1, N2, N3,\nWake, REM\nSubject-specific\ntraining\nSleep-EDF:\n148471 samples,\n197 whole-night PSG\nrecordings.\nCNN-Transformer DL model:\n- Sequential Conv layers (to extract\ntime-invariant data)\n- Transformer (MHA, Dense layers)\n(to learn time-variant data)\n- Classifier.\nBasic training (Acc):\n77.5%\nSubject-specific\ntraining (Acc):\n79.5%\nEfficiency and Portability: Compa-\nrable performance to state-of-the-\nart methods but at significantly\nlower computational costs, with\nsuccessful testing on a low-cost Ar-\nduino board.\n[195] Epilepsy\nPrediction\nPreictal/Interictal CHB-MIT scalp EEG\ndatabase:\n24 cases from 23 pediatric\npatients, 18 EEG channels\nHybrid Transformer model:\n- Rhythm embedding block (8 Conv\nlayers, Avg Pool, 2 FC, Conv layer),\n- PE, Self-attention block (MHA,\nFFN)\n- Classifier block.\n91.7% (SENS),\n0.00/h (FPR)\nOutperforming CNNs: The hybrid\ntransformer model in epilepsy pre-\ndiction demonstrated superior re-\nsults over pure CNN structures.\n[106] Speech\nRecognition\nBinary\nclassification:\nopen class vs.\nclosed class\nwords\nUniversity of Birmingham\nprivate dataset:\n4479 sentences, 75\nsessions\nLinear SVMs and Transformers:\n- Four encoder blocks and a dense\nlayer\n- Pretraining used.\n68% (Avg Acc) Decoding Word Types: Success-\nfully decoded unigram and bigram\nPoS tags from single-trial EEG\ndata, highlighting the efficiency of\ntransformers over SVMs.\n20 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nTABLE 11. (continued from previous page) Overview of Emerging Applications Using Transformers Across a spectrum of EEG-driven applications.\nRef Application areaProtocol EEG Data Model Description Performance Highlights\n[194] Epileptic Seizure\nPrediction\nPreictal vs.\nInterictal\nCHB–MIT Scalp EEG\nDataset:\n22 patients, 198 seizure\nevents\nKaggle/American\nEpilepsy Society (AES)\nInvasive EEG Dataset:\n2 adult human and 5\ncanine subjects.\nKaggle/Melbourne\nUniversity (MU) Invasive\nEEG Dataset:\n3 patients with epilepsy\nMViT:\n- Stack of N transformer encoders\n- Each encoder processes image to-\nkens from an individual EEG chan-\nnel\n- MLP for classification.\nSurface EEG -\n99.80% (Avg SENS)\nInvasive EEG data -\n90.28–91.15% (Avg\nSENS)\nCHB–MIT Scalp\nEEG Dataset:\n99.8% (Acc),\n99.8% (SENS),\n0.004/h (FPR)\nAES - 90.28%\n(SENS)\nMU - 91.15% (SENS)\nAchieved a high sensitivity of\n90.28–99.80% across three inde-\npendent datasets.\n[153] Decoding the\nContinuous\nMotion Imagery\nTrajectories of\nUpper Limb\nSkeleton Points\nleft and right\nshoulders, elbow,\nwrist skeletons\n30 Chinese\nsign language\nsentences\nPrivate dataset\n20 subjects, 26400 epochs,\n59 ch, 6 key skeleton\npoints.\nMITRT:\n- Transformer encoder\n- Corrected joint points location for\nTransformer decoder\n- Similar to Vanilla Transformer.\n0.975 (PCC) Introduced a novel approach using\nMI EEG signals to reconstruct 3D\nmotion trajectories of upper limb\nbased on Chinese sign language.\n[196] Alzheimer\ndisease detection\nNormal vs.\nAbnormal\nPrivate dataset from [200] EEGAlzheimer’sNet:\n- Transformer-based attention,\n- LSTM network.\n96% (Acc)\n98% (MCC)\nAccording to the results, the ac-\ncuracy of the suggested model is\n4% greater than CNN, 2.6% greater\nthan RNN, 2.5% greater than SVM,\nand 0.2% greater than A-LSTM.\n[201] Seizure Detection non-seizure vs.\nseizure\n2023 ICASSP Signal\nProcessing Grand\nChallenge dataset:\nbhe-EEG for training, and\na subset of the Temple\nUniversity Hospital\n(TUH) Seizure Corpus\n(469 seizure events across\n43 patients) to pretrain.\nA mixed Transformer and CNN:\n- several transformer blocks (MHA,\nFFN)\n- Avg pooling, Classifier\n100% (SENS),\n1.78/h (FPR)\nAchieved high sensitivity and low\nfalse alarm rates, highlighting the\npower of pre-trained transformer\nmodels on seizure classification.\n[202] Epilepsy\nDetection\nCHB-MIT Scalp\nEEG dataset:\nnon-seizure vs.\nseizure\nCHB-MIT Scalp EEG\ndataset:\nEEGformer:\n- Raw EEG, Input embedding\n- PE, Transformer Encoder, Output\n65.5% (SENS),\n99.9% (SPEC),\n0.8/h (FPR)\nPresented an EEGformer that offers\nshorter detection latency in epilepsy\ndetection and aligns well with state-\nof-the-art, especially when consid-\nering an artifact-removal stage.\n[105] Automated detec-\ntion of epilepsy\nNormal vs.\nEpilepsy\nKaggle/Turkish Epilepsy\nEEG dataset:\n71 healthy subjects,\n50 epileptic patients. The\ndataset is provided in\nKaggle.\nEpilepsyNet:\n- Pearson Product-moment\nCorrelation Coefficients,\n- Correlation Coefficients\nEmbedding,\n- PE, Transformer Encoder,\nClassifier\n85% (Acc)\n82% (SENS)\n87% (SPEC)\n82% (Positive Pred)\nIntroduced EpilepsyNet, a less\ncomputationally intensive solution,\nas an effective tool for the\nclassification of AD patients\nvs. control subjects.\n[203] Alzheimer\nDetection\nAD/CN\nFTD/CN\nPrivate dataset:\n36 AD, 23 Frontotemporal\ndementia (FTD), and 29\nhealthy individuals (CN).\n19 scalp electrodes EEG\nDICE-net:\n- Convolution, Transformer En-\ncoder, and FF layers\nAD/CN: 83.28%\n(Acc)\nFTD/CN: 74.96%\n(Acc)\nReported the proposed model can\neffectively capture the complex fea-\ntures of EEG signals for the classi-\nfication of AD patients vs. control\nsubjects.\n[204] Sleep Staging Wake, N1, N2,\nN3, REM\nSHHS:\n5736 subjects\nWake (2371496 samples),\nN1 (166619 samples),\nN2 (809155 samples),\nN3 (732389 samples),\nREM (214985 samples)\nA Transformer-Based Spatial-\nTemporal Sleep Staging Model:\n- Inception Module, 30s Sequence\npatches, Linear projection\n- Transformer model (PE,\nTransformer Encoder), SoftMax\nClassifier\nF1-score:\nWake (0.92),\nN1 (0.34),\nN2 (0.85),\nN3 (0.84),\nREM (0.76)\nProposed model outperforms state-\nof-the-art in classifying Wake, N2,\nand N3 sleep stages and offers a\nfully automatic system from feature\nextraction to staging.\n[205] Seizure\nprediction\nSeizure vs. Non-\nseizure\nCHB-MIT:\n22 patients with epilepsy,\n198 seizures\nA personalized seizure prediction\nmodel:\n- Vision Transformer: EEG, STFT,\nVision Transformer\nchb21 patient:\n94.6% (Acc)\n98.6% (Recall)\n89.8% (SPEC)\n90.5% (PREC)\n0.989 (AUC)\nPersonalized seizure prediction\nmodel showcased potential for\nearly epilepsy prediction and\nholds promise for the diagnosis of\nseizures in epileptic patients.\n[197] Motor Action\nRecognition\nnewline walking,\nsitting, chewing,\nblinking, boxing,\nfist closing, fist\nopening, drop,\nEC, EO, hand\non, hand out,\nlift, pull, push,\nsquats, standing.\nPrivate dataset:\n20 subjects,\n17 day-to-day motor\nactivities\nEnsemble of BLSTM-LSTM and\nEEG-Transformer\nBLSTM-LSTM:\n97.9% (Acc)\nEEG-Transformer:\n96.7% (Acc)\nEnsemble: 98.5%\n(Acc)\nEnsemble of BLSTM-LSTM and\nEEG-Transformer achieved 98.5%\naccuracy in motor action recogni-\ntion, marking a significant advance-\nment over current methods.\nVOLUME 4, 2016 21\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nthat researchers do not have to reinvent the wheel for every\nunique BCI challenge but can adapt and fine-tune established\ntransformer models.\nTransformers are pivotal in BCI research for handling\ntemporal sequences and discerning long-term dependencies.\nTheir versatility spans numerous tasks. As these models\nevolve and integrate, they further broaden the horizons for\nBCI potential.\nVI. CHALLENGES AND LIMITATIONS\nWhile Transformer architectures undeniably augment the\nperformance of BCIs, their successful integration into the\nBCIs is not without challenges. Key among these challenges\nare computational efficiency, data variability, and model\ninterpretability. Addressing these concerns is essential to\nharnessing the full potential of Transformers in BCI devel-\nopment and application.\nA. NEED FOR LARGE DATASETS\nTransformers, given their huge parameters, often require\nvast datasets for effective training. In the domain of BCIs,\nobtaining large and high-quality datasets is challenging due\nto several reasons. Firstly, there is a trade-off between in-\nvasive and non-invasive methods. Invasive methods, using\nmicro-electrode arrays, provide fine-grained neural data but\ninvolve surgical procedures and are typically limited to an-\nimal models or specific clinical cases. On the other hand,\nnon-invasive methods such as EEG are more common but\nprovide lower spatial resolution data. Collecting brain data\nalso requires adherence to strict ethical guidelines, adding\nan additional challenge. Moreover, conducting experiments\nto collect BCI data is time and cost-intensive, requiring\nspecialized equipment, trained personnel, and often lengthy\nsessions with participants. Technical challenges include en-\nsuring consistent electrode placement, handling equipment-\nrelated issues, and ensuring participant comfort and safety.\nFinally, the complexity of brain signals adds to the challenge.\nThe brain’s activity is multidimensional and complex, and\ncapturing all relevant information, especially in real-world\nscenarios outside controlled lab environments, is difficult.\nPotential Solutions\nAddressing the challenges in acquiring extensive, high-\nquality datasets for BCIs requires a multifaceted approach.\nAdopting a hybrid approach, combining non-invasive meth-\nods, e.g., EEG with fNIRS or MEG, can enhance spatial and\ntemporal resolutions [206]. Centralized, open-source reposi-\ntories can facilitate data sharing [207]–[210], while advanced\ndata augmentation techniques, such as Generative Adversar-\nial Networks (GANs), can artificially enlarge datasets [211],\n[212]. Another approach would be using crowdsourcing to\ngather BCI data from a broader population. This approach\ncan help in obtaining diverse datasets, capturing a wide\nrange of neural activities and conditions [213]. Additionally,\ntransfer learning allows models to adapt using smaller, task-\nrelated datasets [214], [215]. Together, these solutions can\nlead the BCI community towards better data practices, setting\nthe stage for advanced, reliable future applications.\nB. COMPUTATIONAL OVERHEAD\nThe complex architecture of Transformers, especially in their\nadvanced configurations, places substantial demands on com-\nputational capabilities and memory allocation. When these\nmodels are tasked with training or fine-tuning on complex\nand high-dimensional EEG data, the computational burden\nbecomes especially pronounced. This demand can establish\nsignificant limitations for research groups with limited re-\nsources from fully utilizing the capabilities of these models.\nSuch computational requirements could potentially limit the\ndemocratization of transformer-based BCIs.\nMitigating Strategies\nSeveral strategies can be employed to counteract these chal-\nlenges. One such approach is model distillation, where a\nsmaller, more manageable model is trained to mimic the be-\nhavior and performance of its larger counterpart, allowing for\nefficient deployment without a drastic decline in performance\n[216]. Additionally, there has been a surge in research focus-\ning on creating optimized versions of transformer architec-\ntures that are specifically designed to maintain performance\nwhile being more computationally efficient [217], [218].\nTechniques that focus on effective training strategies, sparse\nactivations, and model pruning are also being explored to re-\nduce the computational overhead associated with these mod-\nels [219]. By adopting these methods, the broader research\ncommunity can harness the potential of transformers in BCIs\nwithout being burdened by computational constraints.\nC. MODEL INTERPRETABILITY\nThe interpretability of transformer models is a crucial aspect\nof deep learning that has been extensively researched [220].\nDespite their ability to process vast amounts of data and\ngenerate exceptional results across a range of tasks, these\nmodels can be incredibly complex, making it difficult to\nunderstand how they arrive at their predictions. This lack of\ntransparency in decision-making processes can have serious\nconsequences in fields such as healthcare, where EEG data\nplays a significant role, and errors in interpretation can result\nin incorrect medical interventions. Consequently, it is imper-\native to improve the transparency and interpretability of these\nmodels, ensuring that the rationale behind their decisions is\naccessible and understandable.\nAddressing the Challenge\nAs the deep learning community acknowledges the impor-\ntance of interpretability, several methods are emerging to\nunderstand the inner workings of transformers and other deep\nlearning models: [218], [221]–[223]:\n• Attention Visualization: Given that transformers uti-\nlize attention mechanisms, visualizing attention weights\ncan offer insights into which parts of the input data the\nmodel deems significant during predictions.\n22 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n• Saliency Maps: These graphical representations high-\nlight input features that are most influential for a given\nprediction, providing a visual guide to a model’s focus\nareas.\n• Feature Attribution: By determining the contribution\nof individual features to the final output, we can gain a\nclearer understanding of what drives model decisions.\nCollaborative efforts between neuroscientists and machine\nlearning researchers can also be beneficial in bridging the\ninterpretability gap.\nD. TRANSFER LEARNING AND DOMAIN ADAPTATION\nConsistent performance across various subjects and devices\nremains a significant challenge in the BCI field. Training a\nmodel on data from a specific group of subjects or a particular\ndevice may result in reduced efficiency when applied to a\ndifferent cohort or another device. This inconsistency can be\nattributed to several factors.\n• Inter-Subject Variability : Every individual’s brain\nhas unique characteristics and patterns. Differences in\nanatomy, functional organization, and neural plasticity\ncan lead to distinct EEG signal patterns, even for similar\ntasks.\n• Inter-Trial Variablity: A single individual’s brain sig-\nnals can exhibit variations over different trials and even\nsessions, due to factors such as fatigue, attention levels,\nor even the time of day.\n• Electrode Placement Inconsistencies : Minor discrep-\nancies in electrode placement across sessions or indi-\nviduals can introduce variability. This can arise due to\nhuman error, differences in head shape, or hair density.\n• Device-Specific Biases : Different EEG devices might\nhave unique calibration settings, sampling rates, or\nsignal-to-noise ratios, which can introduce discrepan-\ncies in the recorded data.\n• Environmental Noise: External factors, such as ambi-\nent light, noise, or even the room’s temperature, can\ninfluence an individual’s brain signals and further com-\nplicate inter-subject comparisons.\nAddressing the Challenge\nAddressing this challenge requires sophisticated techniques\nthat can normalize and adapt to the inherent variabilities\npresent, ensuring that BCI models are robust, generalizable,\nand not just narrowly tailored to a specific dataset. Do-\nmain adaptation techniques, which adjust a model trained on\none domain to perform well on a different but related do-\nmain, can be explored [224]–[226]. Fine-tuning on smaller,\ntarget-specific datasets or employing strategies such as meta-\nlearning [227], where models are trained to quickly adapt to\nnew tasks, can also be beneficial.\nWhile Transformers show promise for BCI research, it is\nimportant to be aware of their limitations and work to address\nthem. Collaboration among researchers is key to finding\ninnovative solutions and advancing BCIs to greater efficiency\nand applicability as the field evolves. For researchers looking\nto leverage the benefits provided by Transformer models for\nBCIs, in Appendix X, we provide a checklist that can serve\nas a practical guide.\nVII. FUTURE DIRECTIONS\nThe application of transformer architectures to BCIs re-\nmains a developing domain with significant potential. With\nadvancements in technology and research methodologies,\nthere are several exploration avenues that promise enhanced\nintegration of transformers in BCIs.\nEFFICIENT TRANSFORMER VARIANTS\nResearchers are currently focusing on enhancing the effec-\ntiveness of transformer architectures while maintaining their\nperformance. This area of exploration is rapidly advancing,\nwith new adaptations seeking to minimize computational\nburdens. This is a crucial stage in enabling their application\nin real-time BCI situations. The emergence of these variants,\nprimarily targeted at natural language processing, opens up\npossibilities for customizations tailored for BCI endeavors.\nMethods such as pruning (purging redundant model parame-\nters) or quantization (reducing parameter precision) could be\nrecalibrated considering the nuances of BCI data.\nA. FUSION WITH OTHER MODALITIES\nThe technological development in data acquisition has\nsparked interest in combining EEG data with other modali-\nties, such as functional near-infrared spectroscopy (fNIRS),\nmagnetoencephalography (MEG), or even peripheral phys-\niological metrics such as heart rate or skin conductance.\nSuch integrative efforts can amplify the richness of data,\npotentially catalyzing superior model outputs. Transformers,\nwith their inherent ability to handle sequential data from\ndiverse sources, can play a foundational role in processing\nmulti-modal data. By synchronizing and processing features\nfrom various modalities, transformers can offer a more com-\nprehensive perspective, enabling the discernment of complex\npatterns in the synergized data.\nB. REAL-TIME PROCESSING AND FEEDBACK\nInstantaneous processing of brain signals is essential for\nvarious applications, particularly those related to assistive\ntechnologies or neurofeedback systems. Such processing\nprovides immediate feedback, making BCIs more interactive\nand intuitive. Nevertheless, achieving real-time processing\nrequires precise, fast, and efficient models. However, com-\nputational intensity makes it challenging to meet this level\nof performance. To tackle this issue, efficient transformer\nderivatives, combined with hardware optimizations, could\noffer a way forward for real-time BCI applications. Further-\nmore, exploring sparse transformers or those designed explic-\nitly for streaming data processing could prove invaluable in\nthis regard.\nIn summary, the potential for transformers in BCIs is\npromising, offering opportunities to redefine boundaries. By\nVOLUME 4, 2016 23\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nnavigating constraints and exploring new trajectories, trans-\nformers have the potential to impact the BCI field.\nVIII. DISCUSSION\nBCI research has transitioned through various phases of\ndevelopment. From its early reliance on basic signal process-\ning techniques for EEG analysis, it has evolved to embrace\nadvanced machine learning algorithms, particularly deep\nlearning models such as Transformers. The integration of\nTransformer architectures into BCIs represents a significant\nmilestone. However, there are still challenges that need to be\naddressed to fully leverage the potential of Transformers in\nBCIs. These challenges include efficient training techniques,\nreducing computational overhead, ensuring interpretability,\naddressing the EEG data deficiency, and exploring transfer\nlearning.\nThe examined studies underscore the adaptability and\nefficacy of Transformer architectures in diverse BCI ap-\nplications. They have demonstrated enhanced performance\nin capturing temporal dependencies in tasks as motor im-\nagery decoding. Notable models, such as V AT-TransEEGNet\n[132] and Swin Transformer with ECA [151], have outper-\nformed traditional methods by implementing sophisticated\ntechniques such as V AT regularization and particle swarm\noptimization. Different protocols and tests are also used to\nevaluate the generalizability and robustness of these models\nacross various datasets and tasks.\nThe review extends to applications of transformers in emo-\ntion recognition, showing high accuracies in classification\ntasks with models such as the Transformer Neural Architec-\nture Search model [61] and Multimodal Neurophysiological\nTransformer [185]. It emphasizes the capability of transform-\ners to handle complex cognitive tasks, capture spatial depen-\ndencies, and improve feature extraction processes effectively.\nFurthermore, Transformers have proven useful in medical\ndiagnostics and sleep stage classification, with models such\nas EEG temporal-spatial transformer [150] and MultiChan-\nnelSleepNet [56] paving the way for innovative applications\nin EEG, ranging from refined speech differentiation to com-\nprehensive sleep analysis.\nWhile promising, the incorporation of transformers in BCI\nis not devoid of challenges, primarily due to their exten-\nsive computational requirements and the complexity of the\nmodels. These challenges are augmented by the necessity\nfor large datasets and the intricate balance needed to avoid\noverfitting, especially crucial for real-time and cross-subject\napplications.\nAs BCI research integrates more deeply into human-\nmachine interaction frameworks, the question is no longer\nif Transformers can be used but rather how they should be\nimplemented to maximize their advantages while mitigating\ntheir limitations. For researchers and practitioners contem-\nplating the incorporation of Transformers into BCIs, several\nkey considerations come into play:\n1) Is the computational trade-off justified by the enhanced\nperformance?\n2) How can we efficiently collect and preprocess large,\nhigh-quality BCI datasets that can feed into these de-\nmanding models?\n3) How can the model’s complexity be managed to suit\nreal-time and cross-subject applications?\n4) What strategies can be employed to make these models\nmore interpretable, given that interpretability is often\ncrucial for clinical applications?\nAnswering these questions is crucial for determining the\nfeasibility, efficacy, and broader implications of integrating\ncomplex Transformer architectures with the ever-advancing\nBCI domain.\nIX. CONCLUSION\nThis review has thoroughly examined the applications of\ntransformer models for EEG classification tasks. The unique\nself-attention mechanism of transformers allows them to han-\ndle long-term dependencies in EEG sequences, improving\nthe accuracy of BCIs in various applications such as motor\nimagery decoding, emotional state recognition, sleep stage\nclassification, and epilepsy prediction.\nWhile integrating transformers into BCIs has many ad-\nvantages, there are challenges to consider. These include\ncomputational intensity, the need for large datasets, and the\ncomplexity that may limit real-time applications or cross-\nsubject compatibility. However, ongoing research and devel-\nopment offer reasons for optimism. More resource-efficient\ntransformer variants, opportunities for multi-modal data fu-\nsion, and improvements in real-time processing algorithms\nare emerging. The combination of Transformers with BCIs\nhas the potential to bring significant advancements in sectors\nsuch as healthcare, entertainment, and assistive technologies.\nIt demonstrates promising applications in emotion recogni-\ntion, sleep stage classification, epilepsy prediction, and other\nareas within the BCI field through detailed EEG analysis.\nREFERENCES\n[1] J. J. Vidal, “Toward direct brain-computer communication,” Annu Rev\nBiophys Bioeng, vol. 2, pp. 157–180, 1973.\n[2] M. A. Lebedev and M. A. L. Nicolelis, “Brain-machine interfaces: past,\npresent and future,” Trends Neurosci, vol. 29, no. 9, pp. 536–546, Sep.\n2006.\n[3] S. Makeig, C. Kothe, T. Mullen, N. Bigdely-Shamlo, Z. Zhang, and\nK. Kreutz-Delgado, “Evolving Signal Processing for Brain–Computer\nInterfaces,” Proceedings of the IEEE, vol. 100, no. Special Centennial\nIssue, pp. 1567–1584, May 2012, conference Name: Proceedings of the\nIEEE.\n[4] J. L. Collinger and D. J. Krusienski, “The 8th international brain-\ncomputer interface meeting, BCIs: the next frontier,” Brain-Computer\nInterfaces, vol. 9, no. 2, pp. 67–68, Apr. 2022, publisher: Taylor\n& Francis _eprint: https://doi.org/10.1080/2326263X.2022.2066853.\n[Online]. Available: https://doi.org/10.1080/2326263X.2022.2066853\n[5] A. Bashashati, M. Fatourechi, R. K. Ward, and G. E. Birch, “A survey\nof signal processing algorithms in brain-computer interfaces based on\nelectrical brain signals,” J Neural Eng, vol. 4, no. 2, pp. R32–57, Jun.\n2007.\n[6] F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rako-\ntomamonjy, and F. Yger, “A review of classification algorithms for EEG-\nbased brain–computer interfaces: a 10 year update,” Journal of neural\nengineering, vol. 15, no. 3, p. 031005, 2018, publisher: IOP Publishing.\n24 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[7] A. Al-Saegh, S. A. Dawwd, and J. M. Abdul-Jabbar, “Deep learning for\nmotor imagery EEG-based classification: A review,” Biomedical Signal\nProcessing and Control, vol. 63, p. 102172, 2021, publisher: Elsevier.\n[8] Y . Roy, H. Banville, I. Albuquerque, A. Gramfort, T. H. Falk,\nand J. Faubert, “Deep learning-based electroencephalography analysis:\na systematic review,” J. Neural Eng., vol. 16, no. 5, p. 051001,\nAug. 2019, publisher: IOP Publishing. [Online]. Available: https:\n//dx.doi.org/10.1088/1741-2552/ab260c\n[9] A. Shrestha and A. Mahmood, “Review of deep learning algorithms and\narchitectures,” IEEE access, vol. 7, pp. 53 040–53 065, 2019, publisher:\nIEEE.\n[10] M. Tangermann, K.-R. Müller, A. Aertsen, N. Birbaumer, C. Braun,\nC. Brunner, R. Leeb, C. Mehring, K. J. Miller, and G. Mueller-Putz,\n“Review of the BCI competition IV,” Frontiers in neuroscience, p. 55,\n2012, publisher: Frontiers.\n[11] R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” Commu-\nnications of the ACM, vol. 63, no. 12, pp. 54–63, 2020.\n[12] J. L. Collinger, B. Wodlinger, J. E. Downey, W. Wang, E. C. Tyler-\nKabara, D. J. Weber, A. J. C. McMorland, M. Velliste, M. L. Boninger,\nand A. B. Schwartz, “High-performance neuroprosthetic control by an\nindividual with tetraplegia,” Lancet, vol. 381, no. 9866, pp. 557–564, Feb.\n2013.\n[13] L. R. Hochberg, D. Bacher, B. Jarosiewicz, N. Y . Masse, J. D. Simeral,\nJ. V ogel, S. Haddadin, J. Liu, S. S. Cash, P. van der Smagt, and J. P.\nDonoghue, “Reach and grasp by people with tetraplegia using a neurally\ncontrolled robotic arm,” Nature, vol. 485, no. 7398, pp. 372–375, May\n2012.\n[14] E. C. Leuthardt, G. Schalk, J. R. Wolpaw, J. G. Ojemann, and D. W.\nMoran, “A brain-computer interface using electrocorticographic signals\nin humans,” J Neural Eng, vol. 1, no. 2, pp. 63–71, Jun. 2004.\n[15] B. He, B. Baxter, B. J. Edelman, C. C. Cline, and W. W. Ye, “Noninvasive\nBrain-Computer Interfaces Based on Sensorimotor Rhythms,” Proceed-\nings of the IEEE, vol. 103, no. 6, pp. 907–925, Jun. 2015, conference\nName: Proceedings of the IEEE.\n[16] A. Gunduz and G. Schalk, “Ecog-based bcis,” Brain–Computer Interfaces\nHandbook, pp. 297–322, 2018, publisher: CRC Press.\n[17] J. Mellinger, G. Schalk, C. Braun, H. Preissl, W. Rosenstiel, N. Bir-\nbaumer, and A. Kübler, “An meg-based brain–computer interface (bci),”\nNeuroimage, vol. 36, no. 3, pp. 581–593, 2007.\n[18] B. Sorger, J. Reithler, B. Dahmen, and R. Goebel, “A real-time fmri-\nbased spelling device immediately enabling robust motor-independent\ncommunication,” Current Biology, vol. 22, no. 14, pp. 1333–1338, 2012.\n[19] N. Naseer and K.-S. Hong, “fnirs-based brain-computer interfaces: a\nreview,” Frontiers in human neuroscience, vol. 9, p. 3, 2015.\n[20] N. A. Bhagat, A. Venkatakrishnan, B. Abibullaev, E. J. Artz, N. Yozbati-\nran, A. A. Blank, J. French, C. Karmonik, R. G. Grossman, and M. K.\nO’Malley, “Design and optimization of an EEG-based brain machine\ninterface (BMI) to an upper-limb exoskeleton for stroke survivors,”\nFrontiers in neuroscience, vol. 10, p. 122, 2016, publisher: Frontiers\nMedia SA.\n[21] A. Venkatakrishnan, G. E. Francisco, and J. L Contreras-Vidal, “Applica-\ntions of brain–machine interface systems in stroke recovery and rehabil-\nitation,” Current physical medicine and rehabilitation reports, vol. 2, pp.\n93–105, 2014, publisher: Springer.\n[22] D. Marshall, D. Coyle, S. Wilson, and M. Callaghan, “Games, Gameplay,\nand BCI: The State of the Art,” IEEE Transactions on Computational\nIntelligence and AI in Games, vol. 5, no. 2, pp. 82–99, Jun. 2013,\nconference Name: IEEE Transactions on Computational Intelligence and\nAI in Games.\n[23] B. Kerous, F. Skola, and F. Liarokapis, “EEG-based BCI and\nvideo games: a progress report,” Virtual Reality, vol. 22, no. 2,\npp. 119–135, Jun. 2018. [Online]. Available: https://doi.org/10.1007/\ns10055-017-0328-x\n[24] J. Pan, X. Chen, N. Ban, J. He, J. Chen, and H. Huang, “Advances\nin P300 brain–computer interface spellers: toward paradigm design and\nperformance evaluation,” Frontiers in Human Neuroscience, vol. 16,\n2022. [Online]. Available: https://www.frontiersin.org/articles/10.3389/\nfnhum.2022.1077717\n[25] M. Vilela and L. R. Hochberg, “Applications of brain-computer interfaces\nto the control of robotic and prosthetic arms,” Handbook of clinical\nneurology, vol. 168, pp. 87–99, 2020, publisher: Elsevier.\n[26] C. Guger, B. Z. Allison, and A. Gunduz, Brain-computer interface\nresearch: a state-of-the-art summary 10. Springer, 2021.\n[27] F. Nijboer, N. Birbaumer, and A. Kübler, “The influence of psychological\nstate and motivation on brain-computer interface performance in patients\nwith amyotrophic lateral sclerosis - a longitudinal study,” Front Neurosci,\nvol. 4, p. 55, 2010.\n[28] M. Ahn and S. C. Jun, “Performance variation in motor imagery\nbrain–computer interface: a brief review,” Journal of neuroscience meth-\nods, vol. 243, pp. 103–110, 2015, publisher: Elsevier.\n[29] L. M. M. Roijendijk, “Variability and nonstationarity in brain computer\ninterfaces,” 2009.\n[30] B. Abibullaev and A. Zollanvari, “Learning discriminative spatiospectral\nfeatures of ERPs for accurate brain–computer interfaces,” IEEE Journal\nof Biomedical and Health Informatics, vol. 23, no. 5, pp. 2009–2020,\n2019, publisher: IEEE.\n[31] X. Gao, Y . Wang, X. Chen, and S. Gao, “Interface, interaction, and in-\ntelligence in generalized brain–computer interfaces,” Trends in cognitive\nsciences, vol. 25, no. 8, pp. 671–684, 2021, publisher: Elsevier.\n[32] M. Kim, S. Yoo, and C. Kim, “Miniaturization for wearable EEG sys-\ntems: recording hardware and data processing,” Biomedical Engineering\nLetters, vol. 12, no. 3, pp. 239–250, 2022, publisher: Springer.\n[33] G. Niso, E. Romero, J. T. Moreau, A. Araujo, and L. R. Krol, “Wireless\nEEG: A survey of systems and studies,” NeuroImage, vol. 269, p. 119774,\n2023, publisher: Elsevier.\n[34] W. Byun, M. Je, and J.-H. Kim, “Advances in Wearable Brain-Computer\nInterfaces From an Algorithm-Hardware Co-Design Perspective,” IEEE\nTransactions on Circuits and Systems II: Express Briefs, vol. 69, no. 7,\npp. 3071–3077, Jul. 2022, conference Name: IEEE Transactions on\nCircuits and Systems II: Express Briefs.\n[35] J. Schmidhuber, “Deep Learning in Neural Networks: An Overview,”\nNeural Networks, vol. 61, pp. 85–117, Jan. 2015, arXiv:1404.7828 [cs].\n[Online]. Available: http://arxiv.org/abs/1404.7828\n[36] Y . LeCun, Y . Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,\nno. 7553, pp. 436–444, 2015, publisher: Nature Publishing Group UK\nLondon.\n[37] V . J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,\nand B. J. Lance, “EEGNet: a compact convolutional neural network for\nEEG-based brain–computer interfaces,” Journal of neural engineering,\nvol. 15, no. 5, 2018, publisher: IOP Publishing.\n[38] P. Bashivan, I. Rish, M. Yeasin, and N. Codella, “Learning represen-\ntations from EEG with deep recurrent-convolutional neural networks,”\narXiv preprint arXiv:1511.06448, 2015.\n[39] B. Abibullaev, K. Kunanbayev, and A. Zollanvari, “Subject-independent\nclassification of P300 event-related potentials using a small number\nof training subjects,” IEEE Transactions on Human-Machine Systems,\nvol. 52, no. 5, pp. 843–854, 2022, publisher: IEEE.\n[40] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,\nK. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball,\n“Deep learning with convolutional neural networks for EEG decoding\nand visualization,” Human Brain Mapping, vol. 38, pp. 5391–5420, 2017.\n[41] Y . R. Tabar and U. Halici, “A novel deep learning approach for classifi-\ncation of EEG motor imagery signals,” J Neural Eng, vol. 14, no. 1, p.\n016003, Feb. 2017.\n[42] A. Craik, Y . He, and J. L. P. Contreras-Vidal, “Deep learning for\nElectroencephalogram (EEG) classification tasks: A review,” Journal of\nneural engineering, 2019, publisher: IOP Publishing.\n[43] I. Dolzhikova, B. Abibullaev, R. Sameni, and A. Zollanvari, “Subject-\nindependent classification of motor imagery tasks in eeg using multisub-\nject ensemble cnn,” IEEE Access, vol. 10, pp. 81 355–81 363, 2022.\n[44] N. Robinson, R. Mane, T. Chouhan, and C. Guan, “Emerging trends in\nbci-robotics for motor control and rehabilitation,” Current Opinion in\nBiomedical Engineering, vol. 20, p. 100354, 2021.\n[45] T. Karácsony, J. P. Hansen, H. K. Iversen, and S. Puthusserypady,\n“Brain computer interface for neuro-rehabilitation with deep learning\nclassification and virtual reality feedback,” in Proceedings of the 10th\nAugmented Human International Conference 2019, 2019, pp. 1–8.\n[46] V . More, M. A. Khalil, and K. George, “Using motor imagery and\ndeeping learning for brain-computer interface in video games,” in 2023\nIEEE World AI IoT Congress (AIIoT). IEEE, 2023, pp. 0711–0716.\n[47] B. Liu, “Deep learning for meditation’s impact on brain-computer in-\nterface performance,” in 2022 International Communication Engineering\nand Cloud Computing Conference (CECCC). IEEE, 2022, pp. 64–69.\n[48] F.-L. Fan, J. Xiong, M. Li, and G. Wang, “On Interpretability of Ar-\ntificial Neural Networks: A Survey,” IEEE Transactions on Radiation\nand Plasma Medical Sciences, vol. 5, no. 6, pp. 741–760, Nov. 2021,\nVOLUME 4, 2016 25\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nconference Name: IEEE Transactions on Radiation and Plasma Medical\nSciences.\n[49] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nKaiser, and I. Polosukhin, “Attention is All you Need,” in Advances in\nNeural Information Processing Systems, vol. 30. Curran Associates,\nInc., 2017. [Online]. Available: https://papers.nips.cc/paper_files/paper/\n2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[50] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[51] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the Limits of Transfer Learning\nwith a Unified Text-to-Text Transformer,” Jul. 2020, arXiv:1910.10683\n[cs, stat]. [Online]. Available: http://arxiv.org/abs/1910.10683\n[52] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, and N. Houlsby, “An Image is\nWorth 16x16 Words: Transformers for Image Recognition at\nScale,” Jun. 2021, arXiv:2010.11929 [cs]. [Online]. Available:\nhttp://arxiv.org/abs/2010.11929\n[53] J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-language tasks,” Advances\nin neural information processing systems, vol. 32, 2019.\n[54] Y . You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,\nJ. Demmel, K. Keutzer, and C.-J. Hsieh, “Large batch optimiza-\ntion for deep learning: Training bert in 76 minutes,” arXiv preprint\narXiv:1904.00962, 2019.\n[55] A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, “Dive into Deep\nLearning,” Feb. 2023, arXiv:2106.11342 [cs]. [Online]. Available:\nhttp://arxiv.org/abs/2106.11342\n[56] Y . Dai, X. Li, S. Liang, L. Wang, Q. Duan, H. Yang, C. Zhang, X. Chen,\nL. Li, X. Li, and X. Liao, “MultiChannelSleepNet: A Transformer-based\nModel for Automatic Sleep Stage Classification with PSG,” IEEE Journal\nof Biomedical and Health Informatics, pp. 1–12, 2023, conference Name:\nIEEE Journal of Biomedical and Health Informatics.\n[57] Z. Miao, M. Zhao, X. Zhang, and D. Ming, “LMDA-Net: A lightweight\nmulti-dimensional attention network for general EEG-based brain-\ncomputer interfaces and interpretability,” NeuroImage, p. 120209, 2023,\npublisher: Elsevier.\n[58] T. Lin, Y . Wang, X. Liu, and X. Qiu, “A survey of transformers,” AI Open,\n2022, publisher: Elsevier.\n[59] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: A survey,” ACM computing surveys (CSUR),\nvol. 54, no. 10s, pp. 1–41, 2022, publisher: ACM New York, NY .\n[60] J. Xie, J. Zhang, J. Sun, Z. Ma, L. Qin, G. Li, H. Zhou, and Y . Zhan,\n“A Transformer-Based Approach Combining Deep Learning Network\nand Spatial-Temporal Information for Raw EEG Classification,” IEEE\nTransactions on Neural Systems and Rehabilitation Engineering, vol. 30,\npp. 2126–2136, 2022, conference Name: IEEE Transactions on Neural\nSystems and Rehabilitation Engineering.\n[61] C. Li, Z. Zhang, X. Zhang, G. Huang, Y . Liu, and X. Chen, “Eeg-based\nemotion recognition via transformer neural architecture search,” IEEE\nTransactions on Industrial Informatics, vol. 19, no. 4, p. 6016–6025, Apr\n2023.\n[62] M. Sun, W. Cui, S. Yu, H. Han, B. Hu, and Y . Li, “A Dual-Branch Dy-\nnamic Graph Convolution Based Adaptive TransFormer Feature Fusion\nNetwork for EEG Emotion Recognition,” IEEE Transactions on Affective\nComputing, vol. 13, no. 4, pp. 2218–2228, 2022, publisher: IEEE.\n[63] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding,” May\n2019, arXiv:1810.04805 [cs]. [Online]. Available: http://arxiv.org/abs/\n1810.04805\n[64] J. R. Wolpaw and D. J. McFarland, “Control of a two-dimensional\nmovement signal by a noninvasive brain-computer interface in humans,”\nProc Natl Acad Sci U S A, vol. 101, no. 51, pp. 17 849–17 854, Dec.\n2004. [Online]. Available: https://www.ncbi.nlm.nih.gov/pmc/articles/\nPMC535103/\n[65] J. Wolpaw and E. W. Wolpaw, Brain-computer interfaces: principles and\npractice. OUP USA, 2012.\n[66] E. E. Fetz, “Operant conditioning of cortical unit activity,” Science, vol.\n163, no. 3870, pp. 955–958, Feb. 1969.\n[67] N. Birbaumer, N. Ghanayim, T. Hinterberger, I. Iversen, B. Kotchoubey,\nA. Kübler, J. Perelmouter, E. Taub, and H. Flor, “A spelling device for\nthe paralysed,” Nature, vol. 398, no. 6725, pp. 297–298, Mar. 1999,\nnumber: 6725 Publisher: Nature Publishing Group. [Online]. Available:\nhttps://www.nature.com/articles/18581\n[68] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image\nRecognition,” Dec. 2015, arXiv:1512.03385 [cs]. [Online]. Available:\nhttp://arxiv.org/abs/1512.03385\n[69] M. Gerven, J. Farquhar, R. Schaefer, R. Vlek, J. Geuze, and \\emphet al.,\n“The brain-computer interface cycle,” J. Neural. Eng., vol. 6, no. 4, pp.\n1–9, 2009.\n[70] G. Buzsáki, C. A. Anastassiou, and C. Koch, “The origin of extracellular\nfields and currents– eeg, ecog, lfp and spikes,” Nature reviews neuro-\nscience, vol. 13, no. 6, pp. 407–420, 2012.\n[71] E. Niedermeyer and F. L. da Silva, Electroencephalography: basic prin-\nciples, clinical applications, and related fields. Lippincott Williams &\nWilkins, 2005.\n[72] W. Zhang, C. Tan, F. Sun, H. Wu, and B. Zhang, “A review of eeg-\nbased brain-computer interface systems design,” Brain Science Ad-\nvances, vol. 4, no. 2, pp. 156–167, 2018.\n[73] A. Widmann, E. Schröger, and B. Maess, “Digital filter design for\nelectrophysiological data–a practical approach,” Journal of neuroscience\nmethods, vol. 250, pp. 34–46, 2015.\n[74] M. K. Islam, A. Rastegarnia, and Z. Yang, “Methods for artifact de-\ntection and removal from scalp eeg: A review,” Neurophysiologie Clin-\nique/Clinical Neurophysiology, vol. 46, no. 4-5, pp. 287–305, 2016.\n[75] A. Delorme and S. Makeig, “Eeglab: an open source toolbox for analysis\nof single-trial eeg dynamics including independent component analysis,”\nJournal of neuroscience methods, vol. 134, no. 1, pp. 9–21, 2004.\n[76] G. Pfurtscheller and F. L. Da Silva, “Event-related eeg/meg synchroniza-\ntion and desynchronization: basic principles,” Clinical neurophysiology,\nvol. 110, no. 11, pp. 1842–1857, 1999.\n[77] P. Welch, “The use of fast fourier transform for the estimation of power\nspectra: a method based on time averaging over short, modified peri-\nodograms,” IEEE Transactions on audio and electroacoustics, vol. 15,\nno. 2, pp. 70–73, 1967.\n[78] P. L. Nunez and R. Srinivasan, Electric fields of the brain: the neuro-\nphysics of EEG. Oxford University Press, USA, 2006.\n[79] S. J. Luck, An introduction to the event-related potential technique. MIT\npress, 2014.\n[80] H. Gastaut, R. Naquet, and Y . Gastaut, “A study of mu rhythm in\nsubjects lacking one or more limbs,” in Electroencephalography and\nClinical Neurophysiology, vol. 18, no. 7. ELSEVIER SCI IRELAND\nLTD CUSTOMER RELATIONS MANAGER, BAY 15, SHANNON . . . ,\n1965, p. 720.\n[81] C. Neuper, A. Schlögl, and G. Pfurtscheller, “Enhancement of left-right\nsensorimotor eeg differences during feedback-regulated motor imagery,”\nJournal of Clinical Neurophysiology, vol. 16, no. 4, pp. 373–382, 1999.\n[82] U. Chaudhary, N. Birbaumer, and A. Ramos-Murguialday,\n“Brain–computer interfaces in the completely locked-in state and\nchronic stroke,” Progress in Brain Research, vol. 228, pp. 131–161,\n2016, publisher: Elsevier.\n[83] M. J. Young, D. J. Lin, and L. R. Hochberg, “Brain-computer\ninterfaces in neurorecovery and neurorehabilitation,” Semin Neurol,\nvol. 41, no. 2, pp. 206–216, Apr. 2021. [Online]. Available:\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC8768507/\n[84] A. J. Suminski, D. C. Tkach, A. H. Fagg, and N. G. Hatsopoulos,\n“Incorporating feedback from multiple sensory modalities enhances\nbrain–machine interface control,” Journal of Neuroscience, vol. 30,\nno. 50, pp. 16 777–16 787, 2010, publisher: Soc Neuroscience.\n[85] M. M. Shanechi, “Brain–machine interfaces from motor to mood,”\nNature neuroscience, vol. 22, no. 10, pp. 1554–1564, 2019, publisher:\nNature Publishing Group US New York.\n[86] P. D. Ganzer, S. C. Colachis, M. A. Schwemmer, D. A. Friedenberg,\nC. F. Dunlap, C. E. Swiftney, A. F. Jacobowitz, D. J. Weber,\nM. A. Bockbrader, and G. Sharma, “Restoring the Sense of\nTouch Using a Sensorimotor Demultiplexing Neural Interface,” Cell,\nvol. 181, no. 4, pp. 763–773.e12, May 2020. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0092867420303470\n[87] N. Birbaumer, “Breaking the silence: brain-computer interfaces (BCI) for\ncommunication and motor control,” Psychophysiology, vol. 43, no. 6, pp.\n517–532, Nov. 2006.\n[88] D. J. McFarland, “Brain-computer interfaces for amyotrophic lateral\nsclerosis,” Muscle & Nerve, vol. 61, no. 6, pp. 702–707, 2020, _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/mus.26828. [Online].\nAvailable: https://onlinelibrary.wiley.com/doi/abs/10.1002/mus.26828\n26 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[89] E. Sellers and E. Donchin, “A p300-based brain-computer interface:\nInitial tests by als patients,” Clinical neurophysiology : official journal\nof the International Federation of Clinical Neurophysiology, vol. 117, pp.\n538–48, 04 2006.\n[90] J. Marín-Morales, C. Llinares, J. Guixeres, and M. Alcañiz, “Emotion\nrecognition in immersive virtual reality: From statistics to affective\ncomputing,” Sensors, vol. 20, no. 18, p. 5163, 2020, publisher: MDPI.\n[91] F. Dehais, A. Lafont, R. Roy, and S. Fairclough, “A neuroergonomics\napproach to mental workload, engagement and human performance,”\nFrontiers in neuroscience, vol. 14, p. 268, 2020, publisher: Frontiers\nMedia SA.\n[92] M.-P. Deiber, R. Hasler, J. Colin, A. Dayer, J.-M. Aubry, S. Baggio,\nN. Perroud, and T. Ros, “Linking alpha oscillations, attention and in-\nhibitory control in adult ADHD with EEG neurofeedback,” NeuroImage:\nClinical, vol. 25, p. 102145, 2020, publisher: Elsevier.\n[93] Z. Wang, Y . Yu, M. Xu, Y . Liu, E. Yin, and Z. Zhou, “Towards\na Hybrid BCI Gaming Paradigm Based on Motor Imagery and\nSSVEP,” International Journal of Human–Computer Interaction, vol. 35,\nno. 3, pp. 197–205, Feb. 2019, publisher: Taylor & Francis _eprint:\nhttps://doi.org/10.1080/10447318.2018.1445068. [Online]. Available:\nhttps://doi.org/10.1080/10447318.2018.1445068\n[94] G. A. M. Vasiljevic and L. C. de Miranda, “Brain–Computer Interface\nGames Based on Consumer-Grade EEG Devices: A Systematic\nLiterature Review,” International Journal of Human–Computer\nInteraction, vol. 36, no. 2, pp. 105–142, Jan. 2020, publisher: Taylor\n& Francis _eprint: https://doi.org/10.1080/10447318.2019.1612213.\n[Online]. Available: https://doi.org/10.1080/10447318.2019.1612213\n[95] E. M. Holz, J. Höhne, P. Staiger-Sälzer, M. Tangermann, and A. Kübler,\n“Brain–computer interface controlled gaming: Evaluation of usability\nby severely motor restricted end-users,” Artificial Intelligence in\nMedicine, vol. 59, no. 2, pp. 111–120, Oct. 2013. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0933365713001140\n[96] A. Nijholt, D. P.-O. Bos, and B. Reuderink, “Turning shortcomings\ninto challenges: Brain–computer interfaces for games,” Entertainment\nComputing, vol. 1, no. 2, pp. 85–94, Apr. 2009. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S187595210900010X\n[97] J. H. Gruzelier, “EEG-neurofeedback for optimising performance. I:\na review of cognitive and affective outcome in healthy participants,”\nNeurosci Biobehav Rev, vol. 44, pp. 124–141, Jul. 2014.\n[98] R. Sitaram, T. Ros, L. Stoeckel, S. Haller, F. Scharnowski, J. Lewis-\nPeacock, N. Weiskopf, M. L. Blefari, M. Rana, E. Oblak, N. Birbaumer,\nand J. Sulzer, “Closed-loop brain training: the science of neurofeedback,”\nNat Rev Neurosci, vol. 18, no. 2, pp. 86–100, Feb. 2017, number:\n2 Publisher: Nature Publishing Group. [Online]. Available: https:\n//www.nature.com/articles/nrn.2016.164\n[99] G. Papanastasiou, A. Drigas, C. Skianis, and M. Lytras, “Brain computer\ninterface based applications for training and rehabilitation of students\nwith neurodevelopmental disorders. A literature review,” Heliyon, vol. 6,\nno. 9, 2020, publisher: Elsevier.\n[100] G. Viviani and A. Vallesi, “EEG-neurofeedback and executive function\nenhancement in healthy adults: A systematic review,” Psychophysiology,\nvol. 58, no. 9, p. e13874, 2021, publisher: Wiley Online Library.\n[101] A. U. Patil, D. Madathil, Y .-T. Fan, O. J. Tzeng, C.-M. Huang, and H.-W.\nHuang, “Neurofeedback for the education of children with ADHD and\nspecific learning disorders: A Review,” Brain Sciences, vol. 12, no. 9, p.\n1238, 2022, publisher: MDPI.\n[102] D. Kim, J. Lee, Y . Woo, J. Jeong, C. Kim, and D.-K. Kim, “Deep\nlearning application to clinical decision support system in sleep stage\nclassification,” Journal of Personalized Medicine, vol. 12, no. 2, p. 136,\nJan 2022.\n[103] D. Jiang, Y .-n. Lu, M. Yu, and W. Yuanyuan, “Robust sleep stage classi-\nfication with single-channel eeg signals using multimodal decomposition\nand hmm-based refinement,” Expert Systems with Applications, vol. 121,\npp. 188–203, 2019.\n[104] M.-P. Hosseini, D. Pompili, K. Elisevich, and H. Soltanian-Zadeh, “Op-\ntimized deep learning for eeg big data and seizure prediction bci via\ninternet of things,” IEEE Transactions on Big Data, vol. 3, no. 4, pp.\n392–404, 2017.\n[105] O. S. Lih, V . Jahmunah, E. E. Palmer, P. D. Barua, S. Dogan,\nT. Tuncer, S. García, F. Molinari, and U. R. Acharya, “Epilepsynet:\nNovel automated detection of epilepsy using transformer model with\neeg signals from 121 patient population,” Computers in Biology\nand Medicine, vol. 164, p. 107312, Sep 2023. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0010482523007771\n[106] A. Murphy, B. Bohnet, R. McDonald, and U. Noppeney, “Decoding part-\nof-speech from human eeg signals,” in Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (V olume\n1: Long Papers). Dublin, Ireland: Association for Computational\nLinguistics, May 2022, p. 2201–2210. [Online]. Available: https:\n//aclanthology.org/2022.acl-long.156\n[107] G. Krishna, C. Tran, M. Carnahan, and A. H. Tewfik, “Eeg\nbased continuous speech recognition using transformers,” no.\narXiv:2001.00501, May 2020, arXiv:2001.00501 [cs, eess, stat].\n[Online]. Available: http://arxiv.org/abs/2001.00501\n[108] A. Kamble, P. H. Ghare, V . Kumar, A. Kothari, and A. G. Keskar,\n“Spectral Analysis of EEG Signals for Automatic Imagined Speech\nRecognition,” IEEE Transactions on Instrumentation and Measurement,\nvol. 72, pp. 1–9, 2023, conference Name: IEEE Transactions on Instru-\nmentation and Measurement.\n[109] R. Oostenveld and P. Praamstra, “The five percent electrode system for\nhigh-resolution EEG and ERP measurements,” Clin Neurophysiol, vol.\n112, no. 4, pp. 713–719, Apr. 2001.\n[110] W. Klimesch, “Eeg alpha and theta oscillations reflect cognitive and\nmemory performance: a review and analysis,” Brain research reviews,\nvol. 29, no. 2-3, pp. 169–195, 1999.\n[111] E. S. Kappenman and S. J. Luck, “The effects of electrode impedance on\ndata quality and statistical significance in erp recordings,” Psychophysi-\nology, vol. 47, no. 5, pp. 888–904, 2010.\n[112] M. De V os, K. Gandras, and S. Debener, “Towards a truly mobile auditory\nbrain–computer interface: exploring the p300 to take away,” International\njournal of psychophysiology, vol. 91, no. 1, pp. 46–53, 2014.\n[113] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M.\nVaughan, “Brain-computer interfaces for communication and control,”\nClin Neurophysiol, vol. 113, no. 6, pp. 767–791, Jun. 2002.\n[114] A. Jackson and E. E. Fetz, “Compact movable microwire array for long-\nterm chronic unit recording in cerebral cortex of primates,” J Neurophys-\niol, vol. 98, no. 5, pp. 3109–3118, Nov. 2007.\n[115] A. B. Rapeaux and T. G. Constandinou, “Implantable brain machine\ninterfaces: first-in-human studies, technology challenges and trends,”\nCurr Opin Biotechnol, vol. 72, pp. 102–111, Dec. 2021.\n[116] G. Székely, “An approach to the complexity of the brain,” Brain research\nbulletin, vol. 55, no. 1, pp. 11–28, 2001, publisher: Elsevier.\n[117] T. Takahashi, “Complexity of spontaneous brain activity in mental dis-\norders,” Progress in Neuro-Psychopharmacology and Biological Psychi-\natry, vol. 45, pp. 258–266, 2013, publisher: Elsevier.\n[118] T. M. Mitchell, S. V . Shinkareva, A. Carlson, K.-M. Chang, V . L. Malave,\nR. A. Mason, and M. A. Just, “Predicting human brain activity associated\nwith the meanings of nouns,” science, vol. 320, no. 5880, pp. 1191–1195,\n2008, publisher: American Association for the Advancement of Science.\n[119] O. Sporns, Networks of the Brain. MIT press, 2016.\n[120] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning. Shelter\nIsland: MIT Press, 2016.\n[121] Y . Bengio, A. Courville, and P. Vincent, “Representation Learning:\nA Review and New Perspectives,” Apr. 2014, arXiv:1206.5538 [cs].\n[Online]. Available: http://arxiv.org/abs/1206.5538\n[122] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[123] R. Ma, T. Yu, X. Zhong, Z. L. Yu, Y . Li, and Z. Gu, “Capsule Network\nfor ERP Detection in Brain-Computer Interface,” IEEE Trans Neural Syst\nRehabil Eng, vol. 29, pp. 718–730, 2021.\n[124] J. León, J. J. Escobar, A. Ortiz, J. Ortega, J. González, P. Martín-\nSmith, J. Q. Gan, and M. Damas, “Deep learning for EEG-based Motor\nImagery classification: Accuracy-cost trade-off,” Plos one, vol. 15, no. 6,\np. e0234178, 2020, publisher: Public Library of Science San Francisco,\nCA USA.\n[125] M. Zabcikova, Z. Koudelkova, R. Jasek, and J. J. Lorenzo Navarro,\n“Recent advances and current trends in brain-computer interface research\nand their applications,” Int J Dev Neurosci, vol. 82, no. 2, pp. 107–123,\nApr. 2022.\n[126] B. Abibullaev and A. Zollanvari, “A systematic deep learning model\nselection for P300-based brain–computer interfaces,” IEEE Transactions\non Systems, Man, and Cybernetics: Systems, vol. 52, no. 5, pp. 2744–\n2756, 2021, publisher: IEEE.\n[127] Y . Zhang, P. Ti ˇno, A. Leonardis, and K. Tang, “A survey on neural\nnetwork interpretability,” IEEE Transactions on Emerging Topics in\nComputational Intelligence, vol. 5, no. 5, pp. 726–742, 2021, publisher:\nIEEE.\nVOLUME 4, 2016 27\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[128] Y . Du, Y . Xu, X. Wang, L. Liu, and P. Ma, “EEG temporal–spatial\ntransformer for person identification,” Sci Rep, vol. 12, no. 1, p. 14378,\nAug. 2022, number: 1 Publisher: Nature Publishing Group. [Online].\nAvailable: https://www.nature.com/articles/s41598-022-18502-3\n[129] A. Hameed, R. Fourati, B. Ammar, A. Ksibi, A. S. Alluhaidan,\nM. B. Ayed, and H. K. Khleaf, “Temporal–spatial transformer based\nmotor imagery classification for bci using independent component\nanalysis,” Biomedical Signal Processing and Control, vol. 87, p. 105359,\nJan 2024. [Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/S1746809423007929\n[130] H.-J. Ahn, D.-H. Lee, J.-H. Jeong, and S.-W. Lee, “Multiscale Convolu-\ntional Transformer for EEG Classification of Mental Imagery in Different\nModalities,” IEEE Transactions on Neural Systems and Rehabilitation\nEngineering, pp. 1–1, 2022, conference Name: IEEE Transactions on\nNeural Systems and Rehabilitation Engineering.\n[131] J. Lei Ba, J. Ryan Kiros, and G. E. Hinton, “Layer normalization,” 2016,\narXiv:1607.06450.\n[132] X. Tan, D. Wang, J. Chen, and M. Xu, “Transformer-based network\nwith optimization for cross-subject motor imagery identification,”\nBioengineering, vol. 10, no. 55, p. 609, May 2023. [Online]. Available:\nhttps://www.mdpi.com/2306-5354/10/5/609\n[133] J. Luo, Y . Wang, S. Xia, N. Lu, X. Ren, Z. Shi, and X. Hei,\n“A shallow mirror transformer for subject-independent motor imagery\nbci,” Computers in Biology and Medicine, vol. 164, p. 107254, Sep\n2023. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/\nS0010482523007199\n[134] Y . Song, Q. Zheng, B. Liu, and X. Gao, “Eeg conformer: Convolutional\ntransformer for eeg decoding and visualization,” IEEE Transactions on\nNeural Systems and Rehabilitation Engineering, vol. 31, p. 710–719,\n2023.\n[135] Y . Song, Q. Zheng, Q. Wang, X. Gao, and P.-A. Heng, “Global adaptive\ntransformer for cross-subject enhanced eeg classification,” IEEE Trans-\nactions on Neural Systems and Rehabilitation Engineering, vol. 31, p.\n2767–2777, 2023.\n[136] D. Zhang, L. Yao, K. Chen, and J. Monaghan, “A convolutional recur-\nrent attention model for subject-independent eeg signal analysis,” IEEE\nSignal Processing Letters, vol. 26, no. 5, p. 715–719, May 2019.\n[137] H. Liu, Y . Liu, Y . Wang, B. Liu, and X. Bao, “Eeg classification algo-\nrithm of motor imagery based on cnn-transformer fusion network,” in\n2022 IEEE International Conference on Trust, Security and Privacy in\nComputing and Communications (TrustCom), Dec 2022, p. 1302–1309.\n[138] Y . Ma, Y . Song, and F. Gao, “A novel hybrid cnn-transformer model for\neeg motor imagery classification,” in 2022 International Joint Conference\non Neural Networks (IJCNN), Jul 2022, p. 1–8.\n[139] X. Ma, W. Chen, Z. Pei, J. Liu, B. Huang, and J. Chen, “A temporal\ndependency learning cnn with attention mechanism for mi-eeg decoding,”\nIEEE Transactions on Neural Systems and Rehabilitation Engineering,\nvol. 31, p. 3188–3200, 2023.\n[140] H.-J. Ahn, D.-H. Lee, J.-H. Jeong, and S.-W. Lee, “Multiscale convolu-\ntional transformer for eeg classification of mental imagery in different\nmodalities,” IEEE Transactions on Neural Systems and Rehabilitation\nEngineering, vol. 31, p. 646–656, 2023.\n[141] D. Kostas, S. Aroca-Ouellette, and F. Rudzicz, “Bendr: Using\ntransformers and a contrastive self-supervised learning task to learn\nfrom massive amounts of eeg data,” Frontiers in Human Neuroscience,\nvol. 15, 2021. [Online]. Available: https://www.frontiersin.org/articles/\n10.3389/fnhum.2021.653659\n[142] “BCI Competition II Data Set III.” https://www.bbci.de/competition/ii/.\n[143] R. Jiang, L. Sun, X. Wang, and Y . Xu, “Application of transformer\nwith auto-encoder in motor imagery eeg signals,” in 2022 14th Interna-\ntional Conference on Wireless Communications and Signal Processing\n(WCSP), Nov 2022, p. 1–7.\n[144] B. Blankertz, G. Dornhege, M. Krauledat, K.-R. Müller, and G. Curio,\n“The non-invasive berlin brain-computer interface: Fast acquisition of\neffective performance in untrained subjects,” NeuroImage, vol. 37, pp.\n539–50, 09 2007.\n[145] Z. Wu, B. Sun, and X. Zhu, “Coupling convolution, transformer and\ngraph embedding for motor imagery brain-computer interfaces,” in 2022\nIEEE International Symposium on Circuits and Systems (ISCAS), May\n2022, p. 404–408.\n[146] M.-H. Lee, O.-Y . Kwon, Y .-J. Kim, H.-K. Kim, Y .-E. Lee, J. Williamson,\nS. Fazli, and S.-W. Lee, “Eeg dataset and openbmi toolbox for three\nbci paradigms: An investigation into bci illiteracy,” GigaScience, vol. 8,\nno. 5, p. giz002, 2019.\n[147] P. Deny and K. W. Choi, “Hierarchical transformer for brain computer\ninterface,” in 2023 11th International Winter Conference on Brain-\nComputer Interface (BCI), Feb 2023, p. 1–5.\n[148] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. C.\nIvanov, R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E.\nStanley, “Physiobank, physiotoolkit, and physionet,” Circulation, vol.\n101, no. 23, pp. e215–e220, 2000.\n[149] Y . Tao, T. Sun, A. Muhamed, S. Genc, D. Jackson, A. Arsanjani, S. Yad-\ndanapudi, L. Li, and P. Kumar, “Gated transformer for decoding human\nbrain eeg signals,” in 2021 43rd Annual International Conference of the\nIEEE Engineering in Medicine Biology Society (EMBC), Nov 2021, p.\n125–130.\n[150] Y . Du, Y . Xu, X. Wang, L. Liu, and P. Ma, “Eeg temporal–spatial\ntransformer for person identification,” Scientific Reports, vol. 12, no. 11,\np. 14378, Aug 2022. [Online]. Available: https://www.nature.com/\narticles/s41598-022-18502-3\n[151] H. Wang, L. Cao, C. Huang, J. Jia, Y . Dong, C. Fan, and V . H. C.\nde Albuquerque, “A novel algorithmic structure of eeg channel attention\ncombined with swin transformer for motor patterns classification,” IEEE\nTransactions on Neural Systems and Rehabilitation Engineering, vol. 31,\np. 3132–3141, 2023.\n[152] P.-L. Lee, S.-H. Chen, T.-C. Chang, W.-K. Lee, H.-T. Hsu, and\nH.-H. Chang, “Continual learning of a transformer-based deep\nlearning classifier using an initial model from action observation\neeg data to online motor imagery classification,” Bioengineering,\nvol. 10, no. 22, p. 186, Feb 2023. [Online]. Available: https:\n//www.mdpi.com/2306-5354/10/2/186\n[153] P. Wang, P. Gong, Y . Zhou, X. Wen, and D. Zhang, “Decoding the\ncontinuous motion imagery trajectories of upper limb skeleton points for\neeg-based brain–computer interface,” vol. 72, p. 1–12, 2023.\n[154] S. Palazzo, C. Spampinato, I. Kavasidis, D. Giordano, J. Schmidt, and\nM. Shah, “Decoding brain representations by multimodal learning of neu-\nral activity and visual features,” IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. PP, pp. 1–1, 05 2020.\n[155] C. Nguyen, G. Karavas, and P. Artemiadis, “Inferring imagined speech\nusing eeg signals: a new approach using riemannian manifold features,”\nJournal of Neural Engineering, vol. 15, 07 2017.\n[156] Y . Song, X. Jia, L. Yang, and L. Xie, “Transformer-based spatial-\ntemporal feature learning for eeg decoding,” no. arXiv:2106.11170,\nJun 2021, arXiv:2106.11170 [cs, eess]. [Online]. Available: http:\n//arxiv.org/abs/2106.11170\n[157] A. Keutayeva and B. Abibullaev, “Exploring the potential of attention\nmechanism-based deep learning for robust subject-independent motor-\nimagery based bcis,” IEEE Access, pp. 1–1, 2023.\n[158] S. M. Jain, “Introduction to transformers,” in Introduction to Trans-\nformers for NLP: With the Hugging Face Library and Models to Solve\nProblems. Springer, 2022, pp. 19–36.\n[159] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov,\n“Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext,” Jun. 2019, arXiv:1901.02860 [cs, stat]. [Online]. Available:\nhttp://arxiv.org/abs/1901.02860\n[160] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition at\nscale,” arXiv preprint arXiv:2010.11929, 2020.\n[161] R. Alazrai, M. Abuhijleh, H. Alwanni, and M. I. Daoud, “A deep\nlearning framework for decoding motor imagery tasks of the same hand\nusing EEG signals,” IEEE Access, vol. 7, pp. 109 612–109 627, 2019,\npublisher: IEEE.\n[162] H. Altaheri, G. Muhammad, M. Alsulaiman, S. U. Amin, G. A. Altuwai-\njri, W. Abdul, M. A. Bencherif, and M. Faisal, “Deep learning techniques\nfor classification of electroencephalogram (EEG) motor imagery (MI)\nsignals: A review,” Neural Computing and Applications, vol. 35, no. 20,\npp. 14 681–14 722, 2023, publisher: Springer.\n[163] S. Gong, K. Xing, A. Cichocki, and J. Li, “Deep learning in EEG:\nAdvance of the last ten-year critical period,” IEEE Transactions on\nCognitive and Developmental Systems, vol. 14, no. 2, pp. 348–365, 2021,\npublisher: IEEE.\n[164] C. Mühl, B. Allison, A. Nijholt, and G. Chanel, “A survey of affective\nbrain computer interfaces: principles, state-of-the-art, and challenges,”\nBrain-Computer Interfaces, vol. 1, no. 2, pp. 66–84, 2014, publisher:\nTaylor & Francis.\n[165] J. Sun, J. Xie, and H. Zhou, “Eeg classification with transformer-based\nmodels,” Mar 2021, p. 92–93.\n28 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n[166] S. Koelstra, A. Yazdani, M. Soleymani, C. Mühl, J.-S. Lee, A. Nijholt,\nT. Pun, T. Ebrahimi, and I. Patras, “Single Trial Classification of EEG and\nPeripheral Physiological Signals for Recognition of Emotions Induced\nby Music Videos,” in Brain Informatics, ser. Lecture Notes in Computer\nScience, Y . Yao, R. Sun, T. Poggio, J. Liu, N. Zhong, and J. Huang, Eds.\nBerlin, Heidelberg: Springer, 2010, pp. 89–100.\n[167] J. LeDoux, “The emotional brain, fear, and the amygdala,” Cell Mol\nNeurobiol, vol. 23, no. 4-5, pp. 727–738, Oct. 2003.\n[168] B. Chakravarthi, S.-C. Ng, M. R. Ezilarasan, and M.-F. Leung, “EEG-\nbased emotion recognition using hybrid CNN and LSTM classification,”\nFrontiers in Computational Neuroscience, vol. 16, p. 1019776, 2022,\npublisher: Frontiers.\n[169] M. Soleymani, M. Pantic, and T. Pun, “Multimodal Emotion Recognition\nin Response to Videos,” IEEE Transactions on Affective Computing,\nvol. 3, no. 2, pp. 211–223, Apr. 2012, conference Name: IEEE Trans-\nactions on Affective Computing.\n[170] D. Wu, B.-L. Lu, B. Hu, and Z. Zeng, “Affective brain–computer inter-\nfaces (abcis): A tutorial,” Proceedings of the IEEE, 2023.\n[171] H. Liu, Y . Zhang, Y . Li, and X. Kong, “Review on emotion recognition\nbased on electroencephalography,” Frontiers in Computational Neuro-\nscience, vol. 15, p. 84, 2021.\n[172] W.-L. Zheng and B.-L. Lu, “Investigating critical frequency bands and\nchannels for eeg-based emotion recognition with deep neural networks,”\nIEEE Transactions on Autonomous Mental Development, vol. 7, no. 3,\npp. 162–175, 2015.\n[173] G. Xiao, M. Shi, M. Ye, B. Xu, Z. Chen, and Q. Ren, “4d attention-based\nneural network for eeg emotion recognition,” Cognitive Neurodynamics,\nvol. 16, p. 1–14, Aug 2022.\n[174] M. Sun, W. Cui, S. Yu, H. Han, B. Hu, and Y . Li, “A dual-branch dynamic\ngraph convolution based adaptive transformer feature fusion network for\neeg emotion recognition,” IEEE Transactions on Affective Computing,\nvol. 13, no. 4, p. 2218–2228, Oct 2022.\n[175] L. Gong, M. Li, T. Zhang, and W. Chen, “Eeg emotion recognition using\nattention-based convolutional transformer neural network,” Biomedical\nSignal Processing and Control, vol. 84, p. 104835, Jul 2023.\n[Online]. Available: https://www.sciencedirect.com/science/article/pii/\nS1746809423002689\n[176] W. Lu, T.-P. Tan, and H. Ma, “Bi-branch vision transformer network for\neeg emotion recognition,” IEEE Access, vol. 11, p. 36233–36243, 2023.\n[177] J. Li, W. Pan, H. Huang, J. Pan, and F. Wang, “Stgate: Spatial-temporal\ngraph attention network with a transformer encoder for eeg-based\nemotion recognition,” Frontiers in Human Neuroscience, vol. 17, 2023.\n[Online]. Available: https://www.frontiersin.org/articles/10.3389/fnhum.\n2023.1169949\n[178] Z. Bai, F. Hou, K. Sun, Q. Wu, M. Zhu, Z. Mao, Y . Song, and Q. Gao,\n“Sect: A method of shifted eeg channel transformer for emotion recogni-\ntion,” IEEE Journal of Biomedical and Health Informatics, p. 1–9, 2023.\n[179] J.-Y . Guo, Q. Cai, J.-P. An, P.-Y . Chen, C. Ma, J.-H. Wan, and Z.-K.\nGao, “A transformer based neural network for emotion recognition and\nvisualizations of crucial eeg channels,” Physica A: Statistical Mechanics\nand its Applications, vol. 603, p. 127700, Oct 2022. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S0378437122004642\n[180] J. Sun, X. Wang, K. Zhao, S. Hao, and T. Wang, “Multi-channel eeg\nemotion recognition based on parallel transformer and 3d-convolutional\nneural network,” Mathematics, vol. 10, no. 1717, p. 3131, Jan 2022.\n[Online]. Available: https://www.mdpi.com/2227-7390/10/17/3131\n[181] C. Cheng, Y . Zhang, L. Liu, W. Liu, and L. Feng, “Multi-domain\nencoding of spatiotemporal dynamics in eeg for emotion recognition,”\nIEEE Journal of Biomedical and Health Informatics, vol. 27, no. 3, p.\n1342–1353, Mar 2023.\n[182] W.-L. Zheng, W. Liu, Y . Lu, B.-L. Lu, and A. Cichocki, “Emotionmeter:\nA multimodal framework for recognizing human emotions,” IEEE Trans-\nactions on Cybernetics, vol. 49, no. 3, pp. 1110–1122, 2019.\n[183] S. Katsigiannis and N. Ramzan, “Dreamer: A database for emotion\nrecognition through eeg and ecg signals from wireless low-cost off-\nthe-shelf devices,” IEEE Journal of Biomedical and Health Informatics,\nvol. 22, no. 1, pp. 98–107, 2018.\n[184] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,\nT. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion analysis\n;using physiological signals,” IEEE Transactions on Affective Comput-\ning, vol. 3, no. 1, pp. 18–31, 2012.\n[185] S. Koorathota, Z. Khan, P. Lapborisuth, and P. Sajda, “Multimodal\nneurophysiological transformer for emotion recognition,” in 2022 44th\nAnnual International Conference of the IEEE Engineering in Medicine\nBiology Society (EMBC), Jul 2022, p. 3563–3567.\n[186] Z. Wang, Y . Wang, C. Hu, Z. Yin, and Y . Song, “Transformers for eeg-\nbased emotion recognition: A hierarchical spatial information learning\nmodel,” IEEE Sensors Journal, vol. 22, no. 5, p. 4359–4368, Mar 2022.\n[187] A. Arjun, A. S. Rajpoot, and M. Raveendranatha Panicker, “Introducing\nattention mechanism for eeg signals: Emotion recognition with vision\ntransformers,” in 2021 43rd Annual International Conference of the\nIEEE Engineering in Medicine Biology Society (EMBC), Nov 2021,\np. 5723–5726.\n[188] M. Soleymani, J. Lichtenauer, T. Pun, and M. Pantic, “A multimodal\ndatabase for affect recognition and implicit tagging,” IEEE Transactions\non Affective Computing, vol. 3, no. 1, pp. 42–55, 2012.\n[189] Y . Zhou and J. Lian, “Identification of emotions evoked by music via\nspatial-temporal transformer in multi-channel eeg signals,” Frontiers\nin Neuroscience, vol. 17, 2023. [Online]. Available: https://www.\nfrontiersin.org/articles/10.3389/fnins.2023.1188696\n[190] Z. Wang, Y . Wang, C. Hu, Z. Yin, and Y . Song, “Transformers for EEG-\nbased emotion recognition: A hierarchical spatial information learning\nmodel,” IEEE Sensors Journal, vol. 22, no. 5, pp. 4359–4368, 2022,\npublisher: IEEE.\n[191] J. Liu, H. Wu, L. Zhang, and Y . Zhao, “Spatial-temporal transformers\nfor eeg emotion recognition,” in Proceedings of the 6th International\nConference on Advances in Artificial Intelligence, ser. ICAAI ’22.\nNew York, NY , USA: Association for Computing Machinery, 2023, p.\n116–120. [Online]. Available: https://doi.org/10.1145/3571560.3571577\n[192] S. Sartipi and M. Cetin, “Adversarial discriminative domain adaptation\nand transformers for eeg-based cross-subject emotion recognition,” in\n2023 11th International IEEE/EMBS Conference on Neural Engineering\n(NER), Apr 2023, p. 1–4.\n[193] Y .-E. Lee and S.-H. Lee, “Eeg-transformer: Self-attention from trans-\nformer architecture for decoding eeg of imagined speech,” in 2022 10th\nInternational Winter Conference on Brain-Computer Interface (BCI), Feb\n2022, p. 1–4.\n[194] R. Hussein, S. Lee, and R. Ward, “Multi-channel vision transformer for\nepileptic seizure prediction,” Biomedicines, vol. 10, no. 77, p. 1551, Jul\n2022. [Online]. Available: https://www.mdpi.com/2227-9059/10/7/1551\n[195] S. Hu, J. Liu, R. Yang, Y . Wang, A. Wang, K. Li, W. Liu, and C. Yang,\n“Exploring the applicability of transfer learning and feature engineering\nin epilepsy prediction using hybrid transformer model,” IEEE Trans-\nactions on Neural Systems and Rehabilitation Engineering, vol. 31, p.\n1321–1332, 2023.\n[196] D. k. Ravikanti and S. S., “Eegalzheimer’snet: Development of\ntransformer-based attention long short term memory network for detect-\ning alzheimer disease using eeg signal,” Biomedical Signal Processing\nand Control, vol. 86, p. 105318, Sep 2023. [Online]. Available:\nhttps://www.sciencedirect.com/science/article/pii/S1746809423007516\n[197] P. Kaushik, I. Tripathi, and P. P. Roy, “Motor activity recognition using\neeg data and ensemble of stacked blstm-lstm network and transformer\nmodel,” in ICASSP 2023 - 2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), Jun 2023, p. 1–5.\n[198] W. Qu, Z. Wang, H. Hong, Z. Chi, D. D. Feng, R. Grunstein, and C. Gor-\ndon, “A residual based attention model for eeg based sleep staging,”\nIEEE journal of biomedical and health informatics, vol. 24, no. 10, p.\n2833–2843, Oct 2020.\n[199] Z. Yao and X. Liu, “A cnn-transformer deep learning model for real-time\nsleep stage classification in an energy-constrained wireless device*,” in\n2023 11th International IEEE/EMBS Conference on Neural Engineering\n(NER), Apr 2023, p. 1–4.\n[200] G. Fiscon, E. Weitschek, A. Cialini, G. Felici, P. Bertolazzi, S. De Salvo,\nA. Bramanti, P. Bramanti, and M. De Cola, “Combining eeg signal pro-\ncessing with supervised methods for alzheimer’s patients classification,”\nBMC Medical Informatics and Decision Making, vol. 35, 05 2018.\n[201] S. Panchavati, S. V . Dussen, H. Semwal, A. Ali, J. Chen, H. Li,\nC. Arnold, and W. Speier, “Pretrained transformers for seizure detection,”\nin ICASSP 2023 - 2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), Jun 2023, p. 1–2.\n[202] P. Busia, A. Cossettini, T. M. Ingolfsson, S. Benatti, A. Burrello,\nM. Scherer, M. A. Scrugli, P. Meloni, and L. Benini, “Eegformer:\nTransformer-based epilepsy detection on raw eeg traces for low-channel-\ncount wearable continuous monitoring devices,” in 2022 IEEE Biomedi-\ncal Circuits and Systems Conference (BioCAS), Oct 2022, p. 640–644.\n[203] A. Miltiadous, E. Gionanidis, K. D. Tzimourta, N. Giannakeas, and\nA. T. Tzallas, “Dice-net: A novel convolution-transformer architec-\nVOLUME 4, 2016 29\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nture for alzheimer detection in eeg signals,” IEEE Access, vol. 11, p.\n71840–71858, 2023.\n[204] G. Shi, Z. Chen, and R. Zhang, “A transformer-based spatial-temporal\nsleep staging model through raw eeg,” in 2021 International Conference\non High Performance Big Data and Intelligent Systems (HPBDIS), Dec\n2021, p. 110–115.\n[205] X. Zhang and H. Li, “Patient-specific seizure prediction from scalp eeg\nusing vision transformer,” in 2022 IEEE 6th Information Technology and\nMechatronics Engineering Conference (ITOEC), vol. 6, Mar 2022, p.\n1663–1667.\n[206] Y . Li, X. Zhang, and D. Ming, “Early-stage fusion of eeg and fnirs\nimproves classification of motor imagery,” Frontiers in Neuroscience,\nvol. 16, p. 1062889, 2023.\n[207] C. J. Markiewicz, K. J. Gorgolewski, F. Feingold, R. Blair, Y . O.\nHalchenko, E. Miller, N. Hardcastle, J. Wexler, O. Esteban, M. Goncavles\net al., “The openneuro resource for sharing of neuroscience data,” Elife,\nvol. 10, p. e71774, 2021.\n[208] V . Jayaram and A. Barachant, “Moabb: trustworthy algorithm bench-\nmarking for bcis,” Journal of neural engineering, vol. 15, no. 6, p. 066011,\n2018.\n[209] P. Dreyer, A. Roc, L. Pillette, S. Rimbert, and F. Lotte, “A large\neeg database with users’ profile information for motor imagery brain-\ncomputer interface research,” Scientific Data, vol. 10, no. 1, p. 580, 2023.\n[210] M.-H. Lee, O.-Y . Kwon, Y .-J. Kim, H.-K. Kim, Y .-E. Lee, J. Williamson,\nS. Fazli, and S.-W. Lee, “Eeg dataset and openbmi toolbox for three\nbci paradigms: An investigation into bci illiteracy,” GigaScience, vol. 8,\nno. 5, p. giz002, 2019.\n[211] F. Fahimi, S. Dosen, K. K. Ang, N. Mrachacz-Kersting, and C. Guan,\n“Generative adversarial networks-based data augmentation for brain–\ncomputer interface,” IEEE transactions on neural networks and learning\nsystems, vol. 32, no. 9, pp. 4039–4051, 2020.\n[212] K. Kunanbayev, B. Abibullaev, and A. Zollanvari, “Data augmentation\nfor p300-based brain-computer interfaces using generative adversar-\nial networks,” in 2021 9th International Winter Conference on Brain-\nComputer Interface (BCI). IEEE, 2021, pp. 1–7.\n[213] S. Dikker, G. Michalareas, M. Oostrik, A. Serafimaki, H. M. Kahraman,\nM. E. Struiksma, and D. Poeppel, “Crowdsourcing neuroscience: inter-\nbrain coupling during face-to-face interactions outside the laboratory,”\nNeuroImage, vol. 227, p. 117436, 2021.\n[214] D. Wu, Y . Xu, and B.-L. Lu, “Transfer learning for eeg-based brain–\ncomputer interfaces: A review of progress made since 2016,” IEEE\nTransactions on Cognitive and Developmental Systems, vol. 14, no. 1,\npp. 4–19, 2020.\n[215] W. Li, W. Huan, B. Hou, Y . Tian, Z. Zhang, and A. Song, “Can emotion be\ntransferred?—a review on transfer learning for eeg-based emotion recog-\nnition,” IEEE Transactions on Cognitive and Developmental Systems,\nvol. 14, no. 3, pp. 833–846, 2021.\n[216] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n“Training data-efficient image transformers & distillation through atten-\ntion,” in International conference on machine learning. PMLR, 2021,\npp. 10 347–10 357.\n[217] Y . Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri, and M. Nadai, “Efficient\ntraining of visual transformers with small datasets,” Advances in Neural\nInformation Processing Systems, vol. 34, pp. 23 818–23 830, 2021.\n[218] Z. Miao, M. Zhao, X. Zhang, and D. Ming, “LMDA-Net:A lightweight\nmulti-dimensional attention network for general EEG-based brain-\ncomputer interfaces and interpretability,” NeuroImage, vol. 276, p.\n120209, Aug. 2023. [Online]. Available: https://www.sciencedirect.com/\nscience/article/pii/S1053811923003609\n[219] C. Zhu, W. Ping, C. Xiao, M. Shoeybi, T. Goldstein, A. Anandkumar, and\nB. Catanzaro, “Long-short transformer: Efficient transformers for lan-\nguage and vision,” Advances in neural information processing systems,\nvol. 34, pp. 17 723–17 736, 2021.\n[220] D. Gunning, M. Stefik, J. Choi, T. Miller, S. Stumpf, and G.-Z. Yang,\n“Xai—explainable artificial intelligence,” Science robotics, vol. 4, no. 37,\np. eaay7120, 2019.\n[221] H. Chefer, S. Gur, and L. Wolf, “Transformer interpretability beyond\nattention visualization,” in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2021, pp. 782–791.\n[222] P. Komorowski, H. Baniecki, and P. Biecek, “Towards evaluating expla-\nnations of vision transformers for medical imaging,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2023, pp. 3725–3731.\n[223] V . Mun and B. Abibullaev, “Explainable deep learning for brain-\ncomputer interfaces through layerwise relevance propagation,” in 2023\n11th International Winter Conference on Brain-Computer Interface\n(BCI). IEEE, 2023, pp. 1–5.\n[224] D. Zhang, H. Li, and J. Xie, “Mi-cat: A transformer-based domain\nadaptation network for motor imagery classification,” Neural Networks,\n2023.\n[225] D. Zhang, H. Li, J. Xie, and D. Li, “Mi-dagsc: A domain adaptation\napproach incorporating comprehensive information from mi-eeg signals,”\nNeural Networks, vol. 167, pp. 183–198, 2023.\n[226] J. J. Bird, J. Kobylarz, D. R. Faria, A. Ekárt, and E. P. Ribeiro, “Cross-\ndomain mlp and cnn transfer learning for biological signal processing:\nEeg and emg,” IEEE Access, vol. 8, pp. 54 789–54 801, 2020.\n[227] J. Li, F. Wang, H. Huang, F. Qi, and J. Pan, “A novel semi-supervised\nmeta learning method for subject-transfer brain–computer interface,”\nNeural Networks, vol. 163, pp. 195–204, 2023.\nX. APPENDIX\nCHECKLIST FOR RESEARCHERS ADOPTING\nTRANSFORMER MODELS IN BCIS\nThe incorporation of Transformer models into EEG-based\nBCIs is not a straightforward task and requires careful con-\nsideration of several key aspects to ensure effective and\nreliable system performance. For researchers interested in\nleveraging the benefits of Transformers for BCIs, the follow-\ning questions may serve as a critical checklist:\n• Data Availability: How much EEG data is available for\ntraining the Transformer model?\n- Depending on the complexity of the task, you may need\nhundreds to thousands of labeled EEG samples. Insuffi-\ncient data can lead to overfitting and poor generalization\nto new, unseen data, making this a critical first step.\n• Feature Engineering: Will the Transformer model han-\ndle feature extraction from raw EEG data, or will some\nform of feature engineering be necessary?\n- Transformers can handle raw EEG data, but prepro-\ncessing steps such as filtering and normalization often\nimprove performance. Your choice between manual fea-\nture engineering and automated extraction will signifi-\ncantly impact model complexity and interpretability.\n• Noise Handling: How well can the Transformer model\nadapt to noisy EEG signals and artifacts?\n- Transformers can be sensitive to noise; consider pre-\nprocessing techniques or noise-reduction layers. Ro-\nbust handling of noise is essential for the model to\nbe applicable in real-world, noisy conditions typically\nencountered in EEG data.\n• Inter-Subject Variability: How does the model per-\nform across different subjects, and is there a need for\ndomain adaptation or transfer learning?\n- Transfer learning or domain adaptation techniques may\nbe necessary for better performance across subjects.\nThe ability to generalize across subjects is crucial for\nbuilding models that are more widely applicable and\ncost-effective.\n• Real-Time Processing : Can the Transformer model\nprocess EEG data in real-time, considering its complex-\nity and computational requirements?\n- Real-time processing is possible but may require hard-\n30 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nware acceleration due to the Transformer’s computa-\ntional complexity. Real-time processing is vital for in-\nteractive applications such as neuroprosthetics or live\nemotional feedback systems.\n• Temporal Dynamics: How effectively can the Trans-\nformer model capture temporal dependencies in EEG\nsignals?\n- Transformers excel in capturing long-term dependen-\ncies, but attention mechanisms should be appropriately\nconfigured. Temporal information is often key in EEG-\nbased tasks, such as sleep stage classification or motor\nimagery tasks.\n• Model Complexity : Given the Transformer architec-\nture’s complexity, how will it impact computational\nefficiency and deployment feasibility?\n- Deploying the model on low-resource devices might\nrequire lightweight versions of the Transformer model.\nComputational efficiency is paramount for real-world\napplications where resources may be limited.\n• Generalization: Can the model generalize well to new,\nunseen data or different BCI tasks?\n- Regularization techniques and data augmentation can\nimprove the model’s ability to generalize to new tasks\nor data. The utility of a model increases significantly if\nit can adapt to new, unseen conditions.\n• Hyperparameter Tuning: What hyperparameters are\nmost crucial for this application, and how should they\nbe selected?\n- Attention heads, the number of layers, and learning rates\nare critical hyperparameters. Their optimal settings can\nsignificantly impact the model’s performance, making\nthis an important aspect of model tuning.\n• Evaluation Metrics : What metrics will be used to\nevaluate the model’s performance, such as accuracy,\nprecision, recall, and computational time?\n- Consider using a combination of accuracy, precision,\nrecall, and F1-score, but also explore domain-specific\nmeasures when needed. The choice of metrics provides\na nuanced understanding of both the model’s strengths\nand weaknesses.\n• Comparison with Existing Models : How does the\nTransformer-based approach compare with existing\nmethods such as CNNs, RNNs, or traditional machine\nlearning algorithms in terms of performance, inter-\npretability, and usability?\n- Performance comparisons with state-of-the-art models\nin terms of accuracy, computational time, and inter-\npretability are essential. This ensures that the Trans-\nformer model either outperforms or offers specific ad-\nvantages over existing methods.\n• Interpretability: How do transformer models ensure in-\nterpretability in EEG and BCIs, and why is it essential?\n- To enhance the interpretability of transformer models,\nresearchers can employ attention visualization and fea-\nture attribution methods, crucial for validating model\ndecisions in critical applications.\n• User Experience: How user-friendly is the BCI system\nwith the Transformer model, and how much calibration\nis required for a new user?\n- The system should require minimal calibration and pro-\nvide intuitive feedback to users. A user-friendly system\nis more likely to gain acceptance and be adopted for\npractical applications.\n• Scalability: How scalable is the model in terms of\nadding more EEG channels or dealing with longer time\nseries data?\n- While Transformers scale well with more data, they may\nrequire proportionally more computational resources.\nThe model’s scalability is vital when extending the\napplication to more complex tasks or larger datasets.\nConsidering these questions can provide valuable insights\ninto the applicability and limitations of using Transformer\nmodels in BCIs.\nBERDAKH ABIBULLAEV (M’12, SM’ 19)\nearned his M.Sc. and Ph.D. in electronic engineer-\ning from Yeungnam University, South Korea in\n2006 and 2010. He worked at Daegu Gyeongbuk\nInstitute of Science and Technology (2010-2013)\nand Samsung Medical Center (2013-2014), and in\n2014 he joined the University of Houston, TX,\nUSA, as a Postdoctoral Research Fellow II sup-\nported by the National Institute of Health. He is\ncurrently an Associate Professor at the Robotics\nDepartment, Nazarbayev University, Kazakhstan. His research is centered\naround developing machine learning techniques to solve inference problems\nin Brain-Computer Interfaces. He is an Associate Editor of IEEE ACCESS.\nAIGERIM KEUTAYEVA received the B.S. in\nRobotics and Mechatronics and M.S. in Robotics\nfrom Nazarbayev University, Astana, Kazakhstan,\nin 2021 and 2023, respectively. Since 2019, she\nhas been a Research Assistant with the School of\nEngineering and Digital Sciences, at Nazarbayev\nUniversity, and a member of the Young Re-\nsearchers Alliance, Astana, Kazakhstan. Her cur-\nrent research interests include machine learn-\ning, brain-computer interfaces, pattern recogni-\ntion, and digital twins.\nAMIN ZOLLANVARI(M’ 10, SM’ 19) received\nB.Sc. and M.Sc. degrees in electrical engineering\nfrom Shiraz University, Iran, and a Ph.D. degree\nin electrical engineering from Texas A&M Uni-\nversity, College Station, TX, in 2010. He held a\npostdoctoral position at Harvard Medical School\nand Brigham and Women’s Hospital, Boston, MA\n(2010-2012), and then joined the Department of\nStatistics at Texas A&M University as an Assistant\nResearch Scientist (2012-2014). He is currently an\nAssociate Professor in the Department of Electrical and Computer Engi-\nneering at Nazarbayev University, Kazakhstan. His research interests include\nmachine learning, signal processing, and biomedical informatics.\nVOLUME 4, 2016 31\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3329678\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7433212399482727
    },
    {
      "name": "Interpretability",
      "score": 0.6389608979225159
    },
    {
      "name": "Transformer",
      "score": 0.6108241677284241
    },
    {
      "name": "Brain–computer interface",
      "score": 0.5904037952423096
    },
    {
      "name": "Electroencephalography",
      "score": 0.40152761340141296
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3905189633369446
    },
    {
      "name": "Human–computer interaction",
      "score": 0.38616806268692017
    },
    {
      "name": "Data science",
      "score": 0.3462303876876831
    },
    {
      "name": "Cognitive science",
      "score": 0.3208356499671936
    },
    {
      "name": "Neuroscience",
      "score": 0.16187220811843872
    },
    {
      "name": "Engineering",
      "score": 0.15913620591163635
    },
    {
      "name": "Psychology",
      "score": 0.1179056465625763
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60559429",
      "name": "Nazarbayev University",
      "country": "KZ"
    }
  ],
  "cited_by": 73
}