{
    "title": "VinVL: Revisiting Visual Representations in Vision-Language Models",
    "url": "https://openalex.org/W3134873017",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2370394005",
            "name": "Zhang, Pengchuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2221596280",
            "name": "LI Xiu-Jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2349235520",
            "name": "Hu Xiaowei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1658219663",
            "name": "Yang Jianwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1842339085",
            "name": "Zhang Lei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1578702133",
            "name": "Wang Li-juan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2149953915",
            "name": "Choi, Yejin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119363152",
            "name": "Gao, Jianfeng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2994818707",
        "https://openalex.org/W2964727037",
        "https://openalex.org/W3104279398",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W3092663126",
        "https://openalex.org/W2995460200",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W3091177855",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W2975501350",
        "https://openalex.org/W3014611590",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W2983943451",
        "https://openalex.org/W3035497460",
        "https://openalex.org/W2990397898",
        "https://openalex.org/W2963954913",
        "https://openalex.org/W2998356391",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W2986670728",
        "https://openalex.org/W2277195237",
        "https://openalex.org/W1931639407",
        "https://openalex.org/W2739181657",
        "https://openalex.org/W2899505139",
        "https://openalex.org/W3035284526",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W2185175083",
        "https://openalex.org/W3038476992",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W3118500473",
        "https://openalex.org/W3034727271",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W2965848243",
        "https://openalex.org/W3029678209",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W3035688398",
        "https://openalex.org/W2962964995"
    ],
    "abstract": "This paper presents a detailed study of improving visual representations for vision language (VL) tasks and develops an improved object detection model to provide object-centric representations of images. Compared to the most widely used \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new model is bigger, better-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple public annotated object detection datasets. Therefore, it can generate representations of a richer collection of visual objects and concepts. While previous VL research focuses mainly on improving the vision-language fusion model and leaves the object detection model improvement untouched, we show that visual features matter significantly in VL models. In our experiments we feed the visual features generated by the new object detection model into a Transformer-based VL fusion model \\oscar \\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the VL model and fine-tune it on a wide range of downstream VL tasks. Our results show that the new visual features significantly improve the performance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. We will release the new object detection model to public.",
    "full_text": "VinVL: Revisiting Visual Representations\nin Vision-Language Models\nPengchuan Zhang♥† Xiujun Li♥♠† Xiaowei Hu♥ Jianwei Yang♥ Lei Zhang♥\nLijuan Wang♥ Yejin Choi♠ Jianfeng Gao♥\nMarch 11, 2021\nAbstract\nThis paper presents a detailed study of improving visual representations for vision language (VL)\ntasks and develops an improved object detection model to provide object-centric representations of im-\nages. Compared to the most widely used bottom-up and top-down model [2], the new model is bigger,\nbetter-designed for VL tasks, and pre-trained on much larger training corpora that combine multiple\npublic annotated object detection datasets. Therefore, it can generate representations of a richer col-\nlection of visual objects and concepts. While previous VL research focuses mainly on improving the\nvision-language fusion model and leaves the object detection model improvement untouched, we show\nthat visual features matter signiﬁcantly in VL models. In our experiments we feed the visual features\ngenerated by the new object detection model into a Transformer-based VL fusion model O SCAR [21],\nand utilize an improved approach O SCAR + to pre-train the VL model and ﬁne-tune it on a wide range\nof downstream VL tasks. Our results show that the new visual features signiﬁcantly improve the per-\nformance across all VL tasks, creating new state-of-the-art results on seven public benchmarks. Code,\nmodels and pre-extracted features are released at https://github.com/pzzhang/VinVL.\n1 Introduction\nVision language pre-training (VLP) has proved effective for a wide range of vision-language (VL) tasks\n[26, 36, 4, 34, 20, 19, 45, 21]. VLP typically consists of two stages: (1) an object detection model is\npre-trained to encode an image and the visual objects in the image to feature vectors, and (2) a cross-\nmodal fusion model is pre-trained to blend text and visual features. While existing VLP research focuses\nmainly on improving the cross-modal fusion model, this paper focuses on improving the object-centric visual\nrepresentations and presents a comprehensive empirical study to demonstrate that visual features matter in\nVL models.\nAmong the aforementioned work, a widely-used object detection (OD) model [2] is trained on the Visual\nGenome dataset [16]. The OD model provides an object-centric representation of images, and has been used\nin many VL models as a black box. In this work, we pre-train a large-scale object-attribute detection model\nbased on the ResNeXt-152 C4 architecture (short as X152-C4). Compared to the OD model of [2], the new\nmodel is better-designed for VL tasks, and is bigger and trained on much larger amounts of data, combining\nmultiple public object detection datasets, including COCO [25], OpenImages (OI) [17], Objects365 [31]\n♥Microsoft Corporation ♠University of Washington †indicates equal contributions.\n1\narXiv:2101.00529v2  [cs.CV]  10 Mar 2021\nVisual feature VQA GQA Image Captioning NoCaps Image Retrieval Text Retrieval NLVR2\ntest-dev test-stdtest-dev test-std B@4 M C S C S R@1 R@5 R@10 R@1 R@5 R@10 dev test-P\nAndersonet al.[2] 73.16 73 .44 61.58 61 .62 40.5 29 .7 137.6 22 .8 86.58 12.38 54.0 80 .8 88 .5 70.0 91 .1 95 .5 78.07 78 .36\nOurs 75.95 76.12 65.05 64.65 40.9 30.9 140.6 25.1 92.46 13.07 58.1 83.2 90.1 74.6 92.6 96.3 82.05 83.08\n∆ 2.79↑2.68↑ 3.47↑3.03↑ 0.4↑1.2↑3.0↑ 2.3↑ 5.9↑ 0.7↑ 4.1↑2.4↑ 1.6↑ 4.6↑1.5↑ 0.8↑ 3.98↑4.71↑\nTable 1: Uniform improvements on seven VL tasks by replacing visual features from Anderson et al. [2]\nwith ours. The NoCaps baseline is from VIVO [9], and our results are obtained by directly replacing the\nvisual features. The baselines for rest tasks are from O SCAR [21], and our results are obtained by replacing\nthe visual features and performing O SCAR + pre-training. All models are BERT-Base size. As analyzed in\nSection 5.2, the new visual features contributes 95% of the improvement.\nFigure 1: Predictions from an X152-FPN model trained on OpenImages (Left) and our X152-C4 model\ntrained on four public object detection datasets (Right). Our model contains much richer semantics, such\nas richer visual concepts and attribute information, and the detected bounding boxes cover nearly all se-\nmantically meaningful regions. Compared with those from the common object classes in typical OD\nmodels (Left), the rich and diverse region features from our model (Right) are crucial for vision-language\ntasks. For concepts detected by both models, e.g., “ boy”, attributes from our model offer richer informa-\ntion, e.g., “young barefoot shirtless standing surfing smiling little playing\nlooking blond boy”. There are object concepts that are detected by our model but not by the Open-\nImages model, including fin, wave, foot, shadow, sky, hair, mountain, water, (bare, tan,\nlight, beige) back, (blue, colorful, floral, multi colored, patterned) trunk, sand,\nbeach, ocean, ( yellow, gold) bracelet, logo, hill, head, ( black, wet) swim trunks,\nblack, wet swim trunks. Compared to the R101-C4 model of [2], our model produces more accurate\nobject-attribute detection results and better visual features for VL applications; see Appendix A for the full\npictures and predictions from [2].\nand Visual Genome (VG) [16]. As a result, our OD model achieves much better results on a wide range\nof VL tasks, as shown in Table 1. Compared to other typical OD models, such as X152-FPN trained on\nOpenImages, our new model can encode a more diverse collection of visual objects and concepts (e.g.,\nproducing visual representations for 1848 object categories and 524 attribute categories), as illustrated by\nan example in Figure 1.\nTo validate the effectiveness of the new OD model, we pre-train a Transformer-based cross-modal fusion\nmodel OSCAR + [21] on a public dataset consisting of 8.85 million text-image pairs, where the visual repre-\nsentations of these images are produced by the new OD model and are ﬁxed during O SCAR + pre-training.\n2\nWe then ﬁne-tune the pre-trained OSCAR + for a wide range of downstream tasks, including VL understand-\ning tasks such as VQA [8], GQA [13], NLVR2 [35], and COCO text-image retrieval [25], and VL generation\ntasks such as COCO image captioning [25] and NoCaps [1]. Our results show that the object-centric repre-\nsentations produced by the new OD model signiﬁcantly improve the performance across all the VL tasks,\noften by a large margin over strong baselines using the classical OD model [2], creating new state of the\narts on all these tasks, including GQA on which none of the published pre-trained models has surpassed the\ndeliberately designed neural state machine (NSM) [12]. We will release the new OD model to the research\ncommunity.\nThe main contributions of this work can be summarized as follows: (i) We present a comprehensive\nempirical study to demonstrate that visual features matter in VL models. (ii) We have developed a new\nobject detection model that can produce better visual features of images than the classical OD model [2] and\nsubstantially uplifts the state-of-the-art results on all major VL tasks across multiple public benchmarks.\n(iii) We provide a detailed ablation study of our pre-trained object detection model to investigate the relative\ncontribution to the performance improvement due to different design choices regarding diversity of object\ncategories, visual attribute training, training data scale, model size, and model architecture.\n2 Improving Vision (V) in Vision Language (VL)\nDeep learning-based VL models typically consist of two modules: an image understanding module Vision\nand a cross-modal understanding module VL:\n(q, v) = Vision(Img), y = VL(w, q, v), (1)\nwhere Img and ware the inputs of the vision and language modalities, respectively. The output of the\nVision module consists of qand v. qis the semantic representation of the image, such as tags or detected\nobjects, and vthe distributional representation of the image in a high-dimensional latent space represented\nusing e.g., the box or region 1 features produced by a VG-pre-trained Faster-RCNN model [2]. Most VL\nmodels use only the visual features v, while the recently proposed O SCAR [21] model shows that qcan\nserve as anchors for learning better vision-language joint representations and and thus can improve the\nperformance on various VL tasks. wand y of the VL module of Equation (1) vary among different VL\ntasks. In VQA, wis a question and y is an answer to be predicted. In text-image retrieval, wis a sentence\nand y is the matching score of a sentence-image pair. In image captioning, wis not given and y is a caption\nto be generated.\nInspired by the great success of pre-trained language models to various natural language processing\ntasks, vision-language pre-training (VLP) has achieved remarkable success in improving the performance\nof the cross-modal understanding module VL by (1) unifying vision and language modeling VL with\nTransformer and (2) pre-training the uniﬁed VL with large-scale text-image corpora. However, most recent\nworks on VLP treat the image understanding module Vision as a black box and leave the visual feature\nimprovement untouched since the development of the classical OD model [2] three years ago, despite that\nthere has been much research progress on improving object detection by 1) developing much more diverse,\nricher, and larger training datasets (e.g. OpenImages and Objects 365), 2) gaining new insights in object\ndetection algorithms such as feature pyramid network [23], one-stage dense prediction [24], and anchor-free\ndetectors [37], and 3) leveraging more powerful GPUs for training bigger models.\nIn this work, we focus on improving Vision for better visual representations. We developed a new OD\nmodel by enriching the visual object and attribute categories, enlarging the model size and training on a\n1We use the terms region and box interchangeably.\n3\nmuch larger OD dasetset, and thus advanced the state of the arts on a wide range of VL tasks. We detail\nhow the new OD model is developed in the rest of this section and then describe the use of OSCAR + for VL\npre-training in Section 3.\n2.1 Object Detection Pre-training\nTo improve the OD model for VL tasks, we utilize four public object detection datasets. As most datasets\ndo not have attribute annotations, we adopt a pre-training and ﬁne-tuning strategy to build our OD model.\nWe ﬁrst pre-train an OD model on a large-scale corpus consisting of four public datasets, and then ﬁne-tune\nthe model with an additional attribute branch on Visual Genome, making it capable of detecting both objects\nand attributes.\nData. Table 2 summarizes the statistics of the four public datasets used in our object detection pre-training,\nincluding COCO, OpenImagesV5 (OI), Objects365V1, and Visual Genome (VG). These datasets have com-\nplementary characters, and are extremely unbalanced in terms of data size, object vocabulary, and the num-\nber of annotations in each class. For example, the VG dataset has a rich and diverse set of annotations for\nboth objects and their attributes with an open vocabulary. But its annotations are noisy and suffer from the\nmissing-annotation problem. The COCO dataset, on the other hand, is very well annotated. But the cover-\nage of visual objects and attributes is much lower than that in VG although we use both its 80 object classes\nand 91 stuff classes to include as diverse visual concepts as possible. We take the following steps to build a\nuniﬁed corpus by combining the four datasets.\n1. First of all, to enhance visual concepts of tail classes, we perform class-aware sampling for Open-\nImages and Objects365 to get at least 2000 instances per class, resulting in 2.2M and 0.8M images,\nrespectively.\n2. To balance the contribution of each dataset, we merge the four datasets with 8 copies of COCO\n(8×0.11M), 8 copies of VG (8 ×0.1M), 2 copies of class-aware sampled Objects365 (2 ×0.8M) and\none copy of the class-aware sampled OpenImages (2.2M).\n3. To unify their object vocabularies, we use the VG vocabulary and its object aliases as the base vocab-\nulary, merge a class from the other three datasets into a VG class if their class names or aliases match,\nand add a new class if no match is found.\n4. Finally, we keep all VG classes that contain at least 30 instances, resulting in 1594 VG classes and\n254 classes from the other three datasets that cannot be mapped to the VG vocabulary, resulting in a\nmerged object detection dataset that contains 1848 classes.\nSource VG COCO w/ stuff Objects365 OpenImagesV5 Total\nImage 97k 111k 609k 1.67M 2.49M\nclasses 1594 171 365 500 1848\nSampling ×8 ×8 CA-2k, ×2 CA-2k 5.43M\nTable 2: Statistics of the Vision pre-training datasets. In sampling, ×k means k copies in one epoch and\n“CA-2k” means class-aware sampling with at least 2000 instances per class.\n4\nModel Architecture (FPN vs C4). Although [23] shows that the FPN model outperforms the C4 model for\nobject detection, recent studies [14] demonstrate that FPN does not provide more effective region features\nfor VL tasks than C4, which is also conﬁrmed by our experimental results 2. We thus conduct a set of\ncarefully designed experiments, as to be detailed in Appendix E, and ﬁnd two main reasons for this. The ﬁrst\nis that all layers in the C4 model used for region feature extraction are pre-trained using the ImageNet dataset\nwhile the multi-layer-perceptron (MLP) head of the FPN model are not. It turns out that the VG dataset is\nstill too small to train a good enough visual features for VL tasks and using ImageNet-pre-trained weights\nis beneﬁcial. The second is due to the different network architectures (CNN vs. MLP). The convolutional\nhead used in C4 has a better inductive bias for encoding visual information than the MLP head of FPN.\nTherefore, in this study we use C4 architecture for VLP.\nModel Pre-Training. Following the common practice in object detection training, we freeze the ﬁrst con-\nvolution layer, the ﬁrst residual block, and all the batch-norm layers. We also use several data augmentation\nmethods, including horizontal ﬂipping and multi-scale training. To train a detection model with the X152-\nC4 architecture, we initialize the model backbone from an ImageNet-5K checkpoint [40] and train for 1.8M\niterations with a batch size of 16 images.\n2.2 Injecting attribute information into the model\nFollowing [2], we add an attribute branch to the pre-trained OD model, and then ﬁne-tune the OD model\non VG to inject attribute information (524 classes). Since the object representations are pre-trained in the\nobject detection pre-training stage, we can focus the VG ﬁne-tuning on learning attributes by picking a much\nlarger attribute loss weight 1.25, compared to 0.5 used in [2, 14]. Thus, our ﬁne-tuned model signiﬁcantly\noutperforms previous models [2, 14] in detecting objects and attributes on VG.\n2.3 Efﬁcient region feature extractor for VL tasks\nWith a richer set of visual objects and attributes, the classical class-aware non-maximal suppression (NMS)\npost-processing takes a signiﬁcantly larger amount of time to remove overlapped bounding boxes, mak-\ning the feature extraction process extremely slow. To improve the efﬁciency, we replace the class-aware\nNMS with the class-agnostic NMS that only conducts the NMS operation once 3. We also replace the time-\nconsuming conv layers with dilation=2 used in [2] with conv layers without dilation. These two replacements\nmake the region feature extraction process much faster than that in [2] without any accuracy drop on VL\ndownstream tasks. We report the end-to-end inference time of VL models with different vision models on a\nTitan-X GPU and a CPU with a single thread in Table 21 in Appendix F.\nIn summary, the pre-trained OD model serves as the image understanding module, as in Equation (1), to\nproduce vision presentations (q, v) for downstream VL tasks. Here, qis the set of detected object names (in\ntext) and vis the set of region features. Each region feature is denoted as(ˆv, z), where ˆv is a P-dimensional\nrepresentation from the input of the last linear classiﬁcation layer of the detection head ( i.e., P = 2048)\nand z is a R-dimensional position encoding of the region (i.e., R = 6)4.\n2We ﬁnd in our experiments that using the same training process, the X152-C4 model even produces better object detection\nresult than the X152-FPN model. See Appendix E for details.\n3Counting the NMS in the RPN module, there are in total 2 NMS operations in our efﬁcient region feature extractor.\n4It includes coordinates of the bounding boxes, and height & width.\n5\n3 O SCAR + Pre-training\nThe success of VLP lies in the use of a unifying model architecture for a wide range of VL tasks and the\nlarge-scale pre-training of the uniﬁed model using objectives that correlate with the performance metrics\nof these downstream VL tasks. In this study we pre-train an improved version of O SCAR [21], known as\nOSCAR + models, to learn the joint image-text representations using image tags as anchors for image-text\nalignment.\n3.1 Pre-training corpus\nWe build our pre-training corpus based on three types of existing vision and VL datasets: (1) image cap-\ntioning datasets with human-annotated captions as wand machine-generated 5 image tags as q, including\nCOCO [25], Conceptual Captions (CC) [32], SBU captions [28] and ﬂicker30k [42]; (2) visual QA datasets\nwith questions as wand human-annotated answers as q, including GQA [13], VQA [8] and VG-QAs; (3)\nimage tagging datasets with machine-generated 6 captions as wand human-annotated tags as q, including\na subset of OpenImages (1.67M images). In total, the corpus contains 5.65 million unique images, 8.85\nmillion text-tag-image triples. The detailed statistics are presented in Table 17 in the Appendix. The size\nof the pre-training corpus could have been signiﬁcantly increased by combining large-scale image tagging\ndatasets, such as the full set of OpenImages (9M images) and YFCC (92M images). We leave it to future\nwork to leverage much larger corpora for model pre-training.\nLoss (w, q/q′, v) (w/w′, q, v) 3-way contrastive\nw′/q′ All q’s (OSCAR ) q’s from QA All w’s All (OSCAR +) q’s from QA\nVQA (vqa-dev) 69.8±0.08 70.1±0.08 69.5±0.05 69.8±0.06 69.7±0.06\nCOCO-IR 73.9±0.2 75.0±0.2 75.0±0.7 78.3±0.3 77.7±0.7\nTable 3: Effects of different pre-training contrastive losses on downstream tasks (R50-C4 asVision module\nand 4-layer Transformer as VL module in (1) ). COCO-IR metric is Image-to-Text retrieval R@1 at COCO\n1K test set. Blue indicates the best result for a task and Black indicates the runner-up.\n3.2 Pre-training Objectives\nThere are two terms in the OSCAR + pre-training loss as in Equation (2).\nLPre-training = LMTL + LCL3. (2)\nLMTL is the Masked Token Loss deﬁned on the text modality ( w and q), following closely [21]. (See\nAppendix B.2 for details.) LCL3 is a novel 3-way Contrastive Loss. Different from the binary contrastive\nloss used in OSCAR [21], the proposed 3-way Contrastive Lossto effectively optimize the training objectives\nused for VQA [41] and text-image matching [6] 7. As shown in Equation 3, LCL3 takes into account two\ntypes of training samples x: the {caption, image-tags, image-features}triplets of the image captioning and\nimage tagging data, and the {question, answer, image-features}triplets of the VQA data.\n5We use the same model to extract visual features.\n6We use the captioning model released by OSCAR [21].\n7[6] uses a deep-learning-based text-image matching model to select the best caption candidate for a given image.\n6\nx≜ ( w\ncaption\n, q, v  \ntags&image\n) or ( w, q  \nQ&A\n, v\nimage\n) (3)\nTo compute contrastive losses, negative examples need to be constructed. We construct two types of\nnegative (unmatched) triplets for the two types of training samples, respectively. One is the polluted “cap-\ntions” (w′, q, v) and the other the polluted “answers” (w, q′, v). To classify whether a caption-tags-image\ntriplet contains a polluted caption is a text-image matching task. To classify whether a question-answer-\nimage triplet contains a polluted answer is an answer selection task for VQA. Since the encoding of [CLS]\ncan be viewed as a representation of the triplet (w, q, v), we apply a fully-connected (FC) layer on top of it\nas a 3-way classiﬁer f(.) to predict whether the triplet is matched (c = 0), contains a polluted w(c = 1), or\ncontains a polluted q(c = 2). The 3-way contrastive loss is deﬁned as\nLCL3 = −E(w,q,v;c)∼˜Dlog p(c|f(w, q, v)), (4)\nwhere the dataset (w, q, v; c) ∈ ˜Dcontains 50% matched triples, 25% w-polluted triples, and 25% q-\npolluted triples. For efﬁcient implementation, the polluted w′is uniformly sampled from all w’s (captions\nand questions) and q′is uniformly sampled from all q’s (tags and answers) in the corpus. As demonstrated\nin Table 3, when only the answer-polluted triplets are used, i.e., (w, q′, v) with q′sampled from q’s from\nQA corpus, the contrastive loss simulates closely the objective for the VQA task but not the text-image\nretrieval task. As a result, the pre-trained model can be effectively adapted to VQA, but not so to text-image\nretrieval. By contrast, the proposed 3-way contrastive loss transfers well to both tasks.\n3.3 Pre-trained models\nWe pre-train two model variants, denoted as O SCAR +B and O SCAR +L, which are initialized with param-\neters θBERT of BERT base ( L = 12 , H = 768 , A= 12 ) and large ( L = 24 , H = 1024 , A= 16 ),\nrespectively, where L is the number of layers, H the hidden size, and A the number of self-attention heads.\nTo ensure that the image region features have the same input embedding size as BERT, we transform the\nposition-augmented region features using a linear projection via matrix W. The trainable parameters are\nθ= {θBERT, W}. O SCAR +B is trained for at least 1M steps, with learning rate 1e−4 and batch size 1024.\nOSCAR +L is trained for at least 1M steps, with learning rate 3e−5 and batch size 1024. The sequence length\nof language tokens [w, q] and region features vare 35 and 50, respectively.\n4 Adapting to VL Tasks\nWe adapt the pre-trained models to seven downstream VL tasks, including ﬁve understanding tasks and two\ngeneration tasks. Each task poses different challenges for adaptation. This section brieﬂy introduces the\ntasks and our ﬁne-tuning strategy. We refer the readers to Appendix C for details.\nVQA & GQA These two are the most widely used understanding task for evaluating VL models in the\nresearch community. The tasks require the model to answer natural language questions based on an image.\nIn this study, we perform experiments on the widely-used VQA v2.0 dataset [8] and GQA dataset [13],\nFollowing the setting of [2], for each question, the model picks an answer from a shared answer set (i.e.,\n3, 129 candidates for VQA, 1, 852 candidates for GQA). When adapting a VLP model to the VQA task, we\n7\nconstruct the input by concatenating a given question, object tags and object region features, and then feed\nthe [CLS] output from OSCAR + to a task-speciﬁc linear classiﬁer with a softmax layer for answer prediction.\nImage Captioning & NoCaps The captioning task is to generate a natural language caption for an im-\nage. This is the most widely used VL generation task in the research community – the Image Captioning\nLeaderboard 8 hosts more than 260 models as of December 10, 2020. To enable caption generation, we\nﬁne-tune OSCAR + using the seq2seq objective. Each training sample is converted to a triplet consisting of a\ncaption, a set of image region features, and a set of object tags. We randomly mask out 15% of the caption\ntokens, and use the encoding of the remaining context (the triplet) to predict the masked tokens. Similar to\nVLP [21, 45], the self-attention mask is constrained such that a caption token can only attend to the tokens\nbefore its position to simulate a uni-directional generation process. All caption tokens have full attentions\nto image regions and object tags but not the other way around. During inference, we ﬁrst encode the image\nregions, object tags, and a special token [CLS] as input. Then the model starts to generate a caption by\nfeeding in a [MASK] token and sampling a token from a vocabulary based on the token probability output.\nNext, the [MASK] token in the previous input sequence is replaced with the sampled token and a new [MASK]\nis appended for the next word prediction. The generation process terminates when the model outputs the\n[STOP] token or the generated sentence exceeds a pre-deﬁned max length. We perform image captioning\nexperiments on the COCO image captioning dataset [25]. Novel Object Captioning at Scale [1] extends\nthe image captioning task to test a model’s capability of describing novel objects from the Open Images\ndataset [17] which are unseen in the training corpus. Following the restriction guideline of NoCaps, we use\nthe predicted Visual Genome and Open Images labels to form the input tag sequences, and directly train\nOSCAR + on COCO without the initialization from pre-training. VIVO [9] proposed a VLP technique by\nonly using image tagging data, and achieved SOTA results on NoCaps by ﬁne-tuning on COCO captions.\nWe reproduced VIVO with only one change, i.e., replacing its original vision model with our new vision\nmodel, and improved the VIVO performance signiﬁcantly (short as VinVL+VIVO), as reported in Table 9.\nImage(-to-Text) Retrieval & Text(-to-Image) Retrieval Both tasks require the model to calculate a simi-\nlarity score between an image and a sentence. Thus, the task is widely used to directly measure the quality of\nthe cross-modal VL representation. Following [21], we formulate the task as a binary classiﬁcation problem,\nwhere given a matched image-text pair, we randomly select a different image or a different sentence to form\nan unmatched pair. The representation of[CLS] is used as the input to a classiﬁer to predict a score indicating\nhow likely the given pair is matched. In testing, the predicted score is used to rank a given image-text pairs\nof a query. Following [19], we report the top-K retrieval results on both the 1K and 5K COCO test sets.\nNLVR2 The dataset is developed for joint reasoning about natural language and images [35]. The task is\nto determine whether a text description is true about a pair of images. For ﬁne-tuning, we ﬁrst construct two\ninput sequences, each containing the concatenation of the given text description and one of the images, and\nthen two [CLS] outputs from OSCAR + are concatenated to form the input to a binary classiﬁer for prediction.\n8Image Captioning Leaderboard: https://competitions.codalab.org/competitions/3221\n8\n5 Experiments & Analysis\n5.1 Main Results\nTo account for model parameter efﬁciency, we group the SoTA models in three categories: (i) SoTAS\nindicates the best performance achieved by small models prior to the Transformer-based VLP models. (ii)\nSoTAB indicates the best performance produced by VLP models of a similar size to BERT base.(iii) SoTAL\nindicates the best performance yielded by VLP models that have a similar size to BERT large.\nTable 4 gives an overview of the results of O SCAR + with V INVL(short for V INVL) on seven VL\ntasks, compared to previous SoTAs9. V INVLoutperforms previous SoTA models on all tasks10, often by a\nsigniﬁcantly large margin. The result demonstrates the effectiveness of the region features produced by the\nnew OD model.\nTask VQA GQA Image Captioning NoCaps Image Retrieval Text Retrieval NLVR2\ntest-dev test-std test-dev test-std B@4 M C S C S R@1 R@5 R@10 R@1 R@5 R@10 dev test-P\nSoTAS 70.55 70 .92 − 63.17 38.9 29 .2 129 .8 22 .4 61.5 9 .2 39.2 68 .0 81 .3 56.6 84 .5 92 .0 54.10 54 .80\nSoTAB 73.59 73 .67 61.58 61 .62 40.5 29 .7 137 .6 22 .8 86.58 12 .38 54.0 80 .8 88 .5 70.0 91 .1 95 .5 78.39 79 .30\nSoTAL 74.75 74 .93 − − 41.7 30.6 140 .0 24 .5 − − 57.5 82 .8 89 .8 73.5 92 .3 96 .0 79.76 81 .47\nVINVLB 75.95 76.12 65.05 64.65 40.9 30.9 140.6 25.1 92.46 13.07 58.1 83.2 90.1 74.6 92.6 96.3 82.05 83.08\nVINVLL 76.52 76.60 − − 41.0 31.1 140.9 25.2 − − 58.8 83.5 90.3 75.4 92.9 96.2 82.67 83.98\n∆ 1.77↑1.67↑ 3.47↑1.48↑ 0.7 ↓0.5↑ 0.9↑ 0.7↑ 5.9↑ 0.7↑ 1.3↑0.7↑0.5↑ 1.9↑0.6↑ 0.3↑ 2.91↑2.51↑\nTable 4: An overall comparison with SoTAs on seven tasks. ∆ indicates the improvement over SoTA.\nSoTA with subscript S, B, L indicates performance achieved by small models, and models with the model\nsize similar to BERT base and large, respectively. SoTAs: VQA is from ERNIE-VIL [43], GQA is from\nNSM [12], NoCaps is from VIVO [9], NLVR2 is from VILLA [7], the rest tasks are from OSCAR [21].\nMethod ViLBERT VL-BERT VisualBERT LXMERT 12-in-1 UNITER O SCAR VILLA ERNIE-V IL InterBERT OSCAR+w/ VINVL\nBase Base Base Base Base Base Large Base Large Base Large Base Large Ensemble* Base Large\nTest-dev 70.63 70 .50 70 .80 72 .42 73 .15 72 .27 73.24 73.16 73.61 73.59 73.69 72.62 74 .75 - 75.95 76.52\nTest-std 70.92 70 .83 71 .00 72 .54 − 72.46 73.40 73.44 73.82 73.67 74.87 72.85 74 .93 76.10 76.12 76.60\nTable 5: Evaluation results on VQA. * denotes the No.1 ensemble model of InterBERT Large on the VQA\nleaderboard.\nMethod LXMERT MMN [3] 12-in-1 O SCAR B NSM [12] OSCAR +B w/ VINVL\nTest-dev 60.00 − − 61.58 − 65.05\nTest-std 60.33 60 .83 60 .65 61 .62 63.17 64.65\nTable 6: Evaluation results on GQA.\nIn Tables 5 to 11, we report the detailed results for each downstream task, respectively. (i) The VQA\nresults are shown in Table 5, where our single O SCAR +B model outperforms the best ensemble model (In-\nterBERT large [22]) on the VQA leaderboard as of Dec. 12, 2020 11. (ii) The GQA results are shown\nin Table 6, where O SCAR +w/VINVLis the ﬁrst VLP model that outperforms the neural state machine\n(NSM) [12] which contains some sophisticated reasoning components deliberately designed for the task.\n(iii) The Image Captioning results on the public “Karpathy” 5k test split are shown in Table 7. Table 8\nshows on a concise version of the COCO image captioning online leaderboard12. The online testing setting\n9All the (single-model) SoTAs are from the published results. For all the tables in this paper,Blue indicates the best result for a\ntask, and gray background indicates results produced by VINVL.\n10The only exception is B@4 on image captioning.\n11VQA leaderboard: https://eval.ai/web/challenges/challenge-page/514/leaderboard/1386\n12Image Captioning Leaderboard: https://competitions.codalab.org/competitions/3221#results\n9\nMethod cross-entropy optimization CIDEr optimization\nB@4 M C S B@4 M C S\nBUTD [2] 36.2 27 .0 113 .5 20 .3 36.3 27 .7 120 .1 21 .4\nVLP [45] 36.5 28 .4 117 .7 21 .3 39.5 29 .3 129 .3 23 .2\nAoANet [10] 37.2 28 .4 119 .8 21 .3 38.9 29 .2 129 .8 22 .4\nOSCAR B [21] 36.5 30 .3 123 .7 23 .1 40.5 29 .7 137 .6 22 .8\nOSCAR L [21] 37.4 30.7 127.8 23.5 41.7 30.6 140 .0 24 .5\nOSCAR +B w/ VINVL 38.2 30.3 129.3 23.6 40.9 30.9 140.4 25.1\nOSCAR +L w/ VINVL 38.5 30.4 130.8 23.4 41.0 31.1 140.9 25.2\nTable 7: Image captioning evaluation results (single model) on COCO “Karpathy” test split. (Note: B@4:\nBLEU@4, M: METEOR, C: CIDEr, S: SPICE.)\nMethod BLEU@1 BLEU@2 BLEU@3 BLEU@4 METEOR ROUGE-L CIDEr-D\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nBUTD [2] 80.2 95 .2 64.1 88 .8 49.1 79 .4 36.9 68 .5 27.6 36 .7 57.1 72 .4 117.9 120 .5\nAoANet [10] 81.0 95 .0 65.8 89 .6 51.4 81 .3 39.4 71 .2 29.1 38 .5 58.9 74 .5 126.9 129 .6\nX-Transformer [29] 81.9 95 .7 66.9 90 .5 52.4 82 .5 40.3 72 .4 29.6 39 .2 59.5 75 .0 131.1 133 .5\nOSCAR + w/ VINVL 81.9 96.9 66.9 92.4 52.6 84.7 40.4 74.9 30.6 40.8 60.4 76.8 134.7 138.7\nTable 8: Leaderboard of the state-of-the-art image captioning models on the COCO online testing.\nMethod in-domain near-domain out-of-domain overall in-domain near-domain out-of-domain overall\nCIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE CIDEr SPICE\nValidation Set Test Set\nUpDown+ 79.3 12 .4 73.8 11 .4 71.7 9 .9 74.3 11 .2 76.0 11 .8 74.2 11 .5 66.7 9 .7 73.1 11 .2\nOSCAR B* 83.4 12 .0 81.6 12 .0 77.6 10 .6 81.1 11 .7 81.3 11 .9 79.6 11 .9 73.6 10 .6 78.8 11 .7\nOSCAR L* 85.4 11 .9 84.0 11 .7 80.3 10 .0 83.4 11 .4 84.8 12 .1 82.1 11 .5 73.8 9 .7 80.9 11 .3\nHuman [1] 84.4 14.3 85.0 14.3 95.7 14 .0 87.1 14.2 80.6 15.0 84.6 14.7 91.6 14 .2 85.3 14.6\nVIVO* [9] 92.2 12 .9 87.8 12 .6 87.5 11.5 88.3 12 .4 89.0 12 .9 87.8 12 .6 80.1 11.1 86.6 12.4\nVinVL* 96.8 13.5 90.7 13.1 87.4 11.6 90.9 12.8 93.8 13.3 89.0 12.8 66.1 10.9 85.5 12.5\nVinVL+VIVO 103.7 13.7 95.6 13.4 83.8 11.9 94.3 13.1 98.0 13.6 95.2 13.4 78.0 11.5 92.5 13.1\nTable 9: NoCaps evaluation results. All the models are trained on COCO without additional image-caption\npairs following the restriction of NoCaps. (UpDown + is UpDown+ELMo+CBS, the models with * is\n+SCST+CBS, VinVL+VIVO is with SCST only.)\nMethod ↓ BERT\n1K Test Set 5K Test Set\nText Retrieval Image Retrieval Text Retrieval Image Retrieval\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nUnicoder-VL [19] B 84.3 97 .3 99 .3 69.7 93 .5 97 .2 62.3 87 .1 92 .8 46.7 76 .0 85 .3\nUNITER [4] B − − − − − − 63.3 87 .0 93 .1 48.4 76 .7 85 .9\nL − − − − − − 66.6 89 .4 94 .3 51.7 78 .4 86 .9\nOSCAR B 88.4 99 .1 99 .8 75.7 95 .2 98 .3 70.0 91 .1 95 .5 54.0 80 .8 88 .5\nL 89.8 98.8 99 .7 78.2 95.8 98 .3 73.5 92 .2 96 .0 57.5 82 .8 89 .8\nOSCAR + w/ VINVL B 89.8 98.8 99 .7 78.2 95.6 98 .0 74.6 92.6 96 .3 58.1 83.2 90 .1\nL 90.8 99.0 99 .8 78.8 96.1 98 .5 75.4 92.9 96 .2 58.8 83.5 90 .3\nTable 10: Text and Image retrieval evaluation on the COCO1K and 5K test sets. (B for Base, L for Large)\nMethod MAC VisualBERT LXMERT 12-in-1 UNITER O SCAR VILLA OSCAR +w/ VINVL\nbase base base base large base large base large base large\nDev 50.8 67 .40 74 .90 − 77.14 78 .40 78 .07 79 .12 78 .39 79 .76 82.05 82.67\nTest-P 51.4 67 .00 74 .50 78 .87 77 .87 79 .50 78 .36 80 .37 79 .47 81 .47 83.08 83.98\nTable 11: Evaluation results on NLVR2.\n10\nvision\nvl no VLP\nOSCAR B\n[21]\nOSCAR +B\n(ours)\nR101-C4 [2] 68.52 ±0.11 72.38 72.46 ±0.05\nVinVL (ours) 71.34 ±0.17 – 74.90 ±0.05\nTable 12: Effects of vision (V) and vision-language (VL) pre-training on VQA.\nreports the results on 40K images, with 5 reference captions (c5) and 40 reference captions (c40) per image.\nAt the time of submitting this paper, our single model achieves No.1 on the entire leaderboard, outperform-\ning all 263 models, including many ensemble (and anonymous) models. (iv) The Novel Object Captioning\n(NoCaps) results are shown in Table 9. Without any VLP, i.e. by directly training a BERT-based captioning\nmodel on COCO, the model with our new visual features (denoted as VinVL) already surpasses the human\nperformance in CIDEr13. By adding VIVO [9] pre-training, our VinVL improves the original VIVO result\nby 6 CIDEr points and creates a new SoTA. (v) Overall, on all these tasks (VQA in Table 5, Image Cap-\ntioning in Table 7, NoCaps in Table 9, Image-Text Retrieval in Table 10, NLVR2 in Table 11), we show that\nOSCAR +B can match or outperform previous SoTA large models, and O SCAR +L substantially uplifts the\nSoTA.\n5.2 Ablation Analysis\nWe select the VQA task for the ablation study because its evaluation metric is well-deﬁned and the task has\nbeen used as a testbed for all VLP models. To assist our analysis, we create a local validation set, vqa-dev,\nout of the standard validation set to select the best model during training for evaluation. vqa-dev contains\nrandomly sampled 2K images and their corresponding questions, amounting to 10.4K image-QA pairs in\ntotal. Except for Table 4 and 5, all our VQA results are reported on this vqa-dev set. Unless otherwise\nspeciﬁed, the reported STD is half of the difference of two runs of the VQA training with different random\nseeds.\nIn VQA, the VL modely = VL(w, q, v) has was the question and y as the answer. We focus on study-\ning the effect of visual features vproduced by different Vision models Vision(Img) to better understand\ntheir relative contribution in the VQA performance. To eliminate the impact of using different tags q, we\nuse the same tags in the VQA models of O SCAR [21]. All the ablation experiments are conducted using\nmodels of the BERT-base size.\nHow much do the V and VL matter to the SoTA? Table 12 shows the VQA results with different vision\nmodels, i.e., R101-C4 model from [2] and our X152-C4 model pre-trained with 4 datasets (VinVL), and with\ndifferent VLP methods, i.e., no VLP, OSCAR [21] and our OSCAR +. Taking the OSCAR B model with R101-\nC4 features as the baseline, the OSCAR +B model with our X152-C4 features improves the absolute accuracy\nfrom 72.38 to 74.90, in which the O SCAR + pre-training contributes 5% of the gain (i.e., 72.38 →72.46)\nand the vision pre-training (improved visual features) 95% (i.e., 72.46 →74.90). This demonstrates that\nvision representations matter signiﬁcantly in VLP and downstream tasks.\nTaking the “no VLP” model with R101-C4 features as the baseline, Table 12 shows that the gains of\nVinVL (71.34−68.52 = 2.82) and VLP (72.46−68.52 = 3.94) are additive (74.90−68.52 ≈2.82+3 .94).\nThis is intuitive because vision pre-training and VLP improve the Vision model Vision(Img) and VL\n13NoCaps leaderboard: https://eval.ai/web/challenges/challenge-page/355/leaderboard/1011\n11\ndata\nmodel R50-FPN R50-C4 R101-C4 [2] X152-C4\nVG 67.35±0.26 67.86±0.31 68.52 ±0.11 69.10±0.06\n4Sets→VG 68.3±0.11 68.39±0.16 – 71.34 ±0.17\nTable 13: Ablation of model size and data size on training vision models.\nModel R50-FPN R50-C4 X152-C4\nPre-training dataset ImageNet 4Sets ImageNet 4Sets ImageNet5k 4Sets\nCOCO mAP 40.2 [40] 44.78 * 38.4 [40] 42.4 42.17 50.51\nVG obj mAP50\nattr mAP with gt boxes\n9.6\n5.4\n11.3\n5.5\n9.6\n6.3\n12.1\n6.1\n11.2\n6.6\n13.8\n7.1\n* Since our four pre-training datasets contain Objects365, it is not surprising that we obtain\nbetter results than 42.3 mAP50 in [31], which is obtained by pre-training on Objects365.\nTable 14: Effect of vision pre-training on object detection tasks.\nmodel VL(w, q, v) separately. This also indicates that our pre-trained vision model can be utilized in any\nVL models by directly replacing their vision models, such as R101-C4 [2], with ours.\nHow much do data and model sizes matter to the new vision model? The improvement of VQA from\nR101-C4 [2] to VinVL (ours) in Table 12 is a compound effect of increasing model size (from R101-C4\nto X152-C4) and data size (from VG to our merged four OD datasets). Table 13 shows the ablation of the\ntwo factors without VLP. Although VG’s large object and attribute vocabulary allows to learn rich semantic\nconcepts, VG does not contain large amounts of annotations for effective training of deep models. Vision\nmodels trained using the merged four OD datasets perform much better than VG-only-trained models, and\nthe improvement is larger with the increase of the model size.14\nHow much does OD model architecture matter? The choice of model architecture affects the VQA\nperformance. Table 13 shows that R50-FPN under-performs R50-C5 when they are trained only on VG; but\nthe performance gap diminishes when both are trained on the merged dataset (4Sets). A detailed comparison\nbetween FPN and C4 architectures is presented in Appendix E.\nHow much does OD pre-training matter for object detection tasks? Table 14 presents the object de-\ntection results on COCO and the object-attribute detection results on VG (1594 object classes, 524 attribute\nclasses). The results show that OD pre-training beneﬁts the object detection tasks. Note that the mAP on\nVG is much lower than that on typical OD datasets (such as COCO) due to two reasons: (1) VG contains\na large number of object classes with limited and extremely unbalanced annotations, (2) there are many\nmissing annotations in the VG evaluation data. 15 Although the mAP numbers are low, the detection result\nusing X152-C4 is reasonably good; see Appendix A for more visualizations. We also see that FPN models\n14The R101-C4 model in Table 13 is exactly the VG-pre-pretrained model from [2]. We do not train this model on our merged\nOD dataset because this model architecture is old-fashioned and is slow to train.\n15As a reference, the R101-C4 model from [2] on VG with 1600 objects and 400 attributes has mAP of 8.7/7.8 evaluated in our\ncode, whereas it was reported as 10.2/7.8 due to differences in OD evaluation pipeline.\n12\nDataset name ImageNet VG-obj VG w/o attr VG [2] VG 4Sets→VG\n#obj & #attr 1000 & 0 317 & 0 1594 & 0 1600 & 400 1594 & 524 1848 & 524\nR50-C4 + BERTB 66.13±0.04 64.25±0.16 66.51±0.11 67.63±0.25 67.86±0.31 68.39±0.16\nTable 15: Effect of object-attribute vocabulary. We use all grid features (maximal 273) for the ImageNet\nclassiﬁcation model (ﬁrst column), and maximal 50 region features for OD models (other columns).\nperform consistently worse in attribute detection than C4 models, neither do FPN models show any advan-\ntage in object detection on VG. This contributes to the inferior performance of FPN, compared to C4, on\ndownstream VL tasks, as discussed in Section 2.1.\nHow much does the diversity of visual concepts, i.e., object and attribute vocabularies, matter? We\ndirectly train vision models on different datasets, including (1) standard ImageNet with 1K classes (Ima-\ngeNet), (2) Visual Genome with 317 object classes (VG-obj) that are shared with COCO 80 classes and\nOpenImagesV5 500 classes, (3) VG with all 1594 object classes (VG w/o attr), (4) VG with 1594 object\nclasses and 524 attribute classes (VG), and (5) the merged OD dataset (4Sets) for pre-training and VG for\nﬁne-tuning. For all the OD models (the last four columns in Table 15), we initialize the OD training with\nan ImageNet-pre-trained classiﬁcation model, and use maximal 50 region features per image as input to the\nVL fusion module. For the ImageNet pre-trained classiﬁcation model (the second column in Table 15), we\nuse all the grid features (maximal 273) for each image16. The results show that\n• In general, vocabularies with richer objects lead to better VQA results: VG-obj < ImageNet < VG w/o\nattr. The VG-obj vocabulary contains 79 of 80 COCO classes (only missingpotted plant) and 313 of\n500 OpenImagesV5 classes, and is a good approximation of common object classes of typical OD tasks.\nHowever, our results show that this vocabulary is not rich enough for VL tasks because it misses many\nimportant visual concepts (e.g., sky, water, mountain, etc.) which are crucial for VL tasks, as also\nillustrated by the comparison of detected regions in Figure 1. 17.\n• Attribute information is crucial to VL tasks: models trained with attributes (VG and 4Sets →VG) are\nsigniﬁcantly better than those without attributes.\n• Even for the small vision model R50-C4, vision pre-training improves visual features for VQA, i.e.,\n4Sets→VG is the best performer.\nIn Table 16, we use different kinds of region proposals to extract image features. COCO groundtruth\nobject regions (GT-Obj, 80 classes) and object-stuff regions (GT-Obj&Stuff, 171 classes) are perfect in\nterms of localization, but their vocabulary sizes are limited. Regions proposed by VG-trained models ([2]\nand VinVL) are imperfect in localization but using a larger vocabulary. For the VQA task, COCO GT boxes\nare much worse than the proposals generated by VG-trained models. The result demonstrates the difference\nbetween the typical OD tasks and the OD tasks in VL: OD in VL requires much richer visual semantics\nto align with the rich semantics in the language modality. This further echoes our claim that an image\nunderstanding module trained using richer vocabularies performs better for VL tasks.\n16Our use of grid feature follows PixelBert [11]. See Appendix F for details.\n17Using the same training procedure on VG, we trained an R50-C4 model on the OpenImagesV5 dataset (500 classes). Using\nthe region features produced by this model, the VQA performance is 63.55±0.14. The result is slightly worse than that of VG-obj\nbecause both VG and VQA images are from the COCO dataset but OpenImages images are not.\n13\nmodel\nregion GT-Obj GT-Obj&Stuff\nAnderson\net al. [2] VinVL (ours)\nAnderson\net al. [2] 63.81 ±0.94 66.68 ±0.16 68.52 ±0.11 69.05 ±0.06\nVinVL (ours) 65.60 ±0.21 68.13 ±0.26 70.25 ±0.05 71.34 ±0.17\nTable 16: Effect of different region proposals on VQA.\n6 Conclusion\nIn this paper we have presented a new recipe to pre-train an OD model for VL tasks. Compared to the most\nwidely used bottom-up and top-down model [2], the new model is bigger, better-designed for VL tasks, and\npre-trained on much larger text-image corpora, and thus can generate visual features for a richer collection\nof visual objects and concepts that are crucial for VL tasks. We validate the new model via a comprehensive\nempirical study where we feed the visual features to a VL fusion model which is pre-trained on a large-scale\npaired text-image corpus and then ﬁne-tuned on seven VL tasks. Our results show that the new OD model\ncan substantially uplift the SoTA results on all seven VL tasks across multiple public benchmarks. Our\nablation study shows that the improvement is mainly attributed to our design choices regarding diversity of\nobject categories, visual attribute training, training data scale, model size, and model architecture.\nAcknowledgement\nWe thank Xi Yin for her contributions to this project while she was in Microsoft. We thank Xiyang Dai for\nhis conjecture that C4 arch is better than FPN because C4 arch makes better use of ImageNet initialization\nweights.\nReferences\n[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, 2019. 3, 8, 10, 23\n[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018. 1, 2, 3,\n5, 7, 10, 11, 12, 13, 14, 17, 18, 20, 21, 22, 23, 26, 28, 30\n[3] Wenhu Chen, Zhe Gan, Linjie Li, Yu Cheng, William Wang, and Jingjing Liu. Meta module network for\ncompositional visual reasoning. arXiv preprint arXiv:1910.03230, 2019. 9, 22\n[4] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019. 1, 10, 24\n[5] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improved visual-semantic embed-\ndings. arXiv preprint arXiv:1707.05612, 2(7):8, 2017. 24\n[6] Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K Srivastava, Li Deng, Piotr Doll´ar, Jianfeng Gao, Xiaodong\nHe, Margaret Mitchell, John C Platt, et al. From captions to visual concepts and back. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 1473–1482, 2015. 6\n[7] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for\nvision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020. 9\n[8] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter:\nElevating the role of image understanding in visual question answering. In CVPR, 2017. 3, 6, 7, 22\n14\n[9] Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, and Zicheng Liu. Vivo: Sur-\npassing human performance in novel object captioning with visual vocabulary pre-training. arXiv preprint\narXiv:2009.13682, 2020. 2, 8, 9, 10, 11\n[10] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. Attention on attention for image captioning. In\nICCV, 2019. 10, 23\n[11] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels\nwith text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849, 2020. 13\n[12] Drew Hudson and Christopher D Manning. Learning by abstraction: The neural state machine. In NeurIPS,\n2019. 3, 9\n[13] Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compo-\nsitional question answering. arXiv preprint arXiv:1902.09506, 2019. 3, 6, 7, 22\n[14] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen. In defense of grid features\nfor visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10267–10276, 2020. 5, 26\n[15] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR,\n2015. 23, 24\n[16] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis\nKalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowd-\nsourced dense image annotations. International Journal of Computer Vision, 123(1):32–73, 2017. 1, 2\n[17] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,\nStefan Popov, Matteo Malloci, Tom Duerig, et al. The open images dataset v4: Uniﬁed image classiﬁcation,\nobject detection, and visual relationship detection at scale. arXiv preprint arXiv:1811.00982, 2018. 1, 8, 23\n[18] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text\nmatching. In ECCV, 2018. 24\n[19] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-VL: A universal encoder for vision\nand language by cross-modal pre-training. arXiv preprint arXiv:1908.06066, 2019. 1, 8, 10, 23, 24\n[20] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and perfor-\nmant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 1\n[21] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li\nDong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for vision-language\ntasks. In European Conference on Computer Vision, pages 121–137. Springer, 2020. 1, 2, 3, 6, 8, 9, 10, 11, 19,\n20, 22, 23, 27\n[22] Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, and Hongxia Yang. Interbert: Vision-and-\nlanguage interaction for multi-modal pretraining. arXiv preprint arXiv:2003.13198, 2020. 9\n[23] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. InProceedings of the IEEE conference on computer vision and pattern recognition,\npages 2117–2125, 2017. 3, 5\n[24] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense object detection.\nIn Proceedings of the IEEE international conference on computer vision, pages 2980–2988, 2017. 3\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 1, 3, 6, 8, 22, 23\n[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. VilBERT: Pretraining task-agnostic visiolinguistic repre-\nsentations for vision-and-language tasks. In NeurIPS, 2019. 1\n[27] Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-Task vision and\nlanguage representation learning. arXiv preprint arXiv:1912.02315, 2019. 24\n[28] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing images using 1 million captioned\nphotographs. In NeurIPS, 2011. 6\n[29] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10971–10980, 2020. 10\n15\n[30] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence\ntraining for image captioning. In CVPR, 2017. 23\n[31] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Ob-\njects365: A large-scale, high-quality dataset for object detection. In Proceedings of the IEEE international\nconference on computer vision, pages 8430–8439, 2019. 1, 12\n[32] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed,\nimage alt-text dataset for automatic image captioning. In Annual Meeting of the Association for Computational\nLinguistics, 2018. 6, 23\n[33] Botian Shi, Lei Ji, Pan Lu, Zhendong Niu, and Nan Duan. Knowledge aware semantic concept expansion for\nimage-text matching. In IJCAI, 2019. 24\n[34] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training of generic\nvisual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019. 1\n[35] Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about\nnatural language grounded in photographs. arXiv preprint arXiv:1811.00491, 2018. 3, 8, 24\n[36] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers.\nEMNLP, 2019. 1\n[37] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: A simple and strong anchor-free object detector. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 2020. 3\n[38] Yaxiong Wang, Hao Yang, Xueming Qian, Lin Ma, Jing Lu, Biao Li, and Xin Fan. Position focused attention\nnetwork for image-text matching. arXiv preprint arXiv:1907.09748, 2019. 24\n[39] Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, and Jing Shao. CAMP: Cross-\nModal adaptive message passing for text-image retrieval. In ICCV, 2019. 24\n[40] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://\ngithub.com/facebookresearch/detectron2, 2019. 5, 12, 29\n[41] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image\nquestion answering. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n21–29, 2016. 6\n[42] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denota-\ntions: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for\nComputational Linguistics, 2:67–78, 2014. 6\n[43] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced\nvision-language representations through scene graph. arXiv preprint arXiv:2006.16934, 2020. 9\n[44] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, and Yi-Dong Shen. Dual-path convolutional image-text\nembedding with instance loss. arXiv preprint arXiv:1711.05535, 2017. 24\n[45] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. Uniﬁed vision-\nlanguage pre-training for image captioning and VQA. AAAI, 2020. 1, 8, 10, 22, 23\n16\nFigure 2: Predictions from X152-FPN trained on OpenImages. Test image: COCO test2015 000000028839\nA Qualitative study of three pre-trained vision models\nWe apply three (pre-trained) object detection models on the image in Figure 1 and list their detection results\nfor a more detailed comparison.\nDetections from X152-FPN trained on Open Images V5. See Figure 2:\nSurfboard; Surfboard; Surfboard; Surfboard; Man; Human leg; Human leg;\nSwimwear; Swimwear; Shorts; Shorts; Boy; Human arm.\nDetections from R101-C4 trained on VG by Andersonet al. [2]. There are obviously wrong detections,\nmarked in red. See Figure 3 (top):\nblack shorts; young, shirtless, standing, barefoot, surfing, little,\nplaying boy; shirtless, standing, barefoot, walking, wet, surfing, young\nman; tan, bare, shirtless back; blue, clear, cloudy, hazy, light blue sky;\nyoung, shirtless, standing, surfing, barefoot, little boy; brown, short,\nwet, blond hair; brown, short, wet, blond hair; small, crashing wave;\nwhite, wet surfboard; white, crashing, big, rolling wave;\nwet, tan surfboard; green, blue fin; blue, calm, choppy, wavy, ocean,\nsplashing, foamy, water, rough, sandy, wet ocean; wet, calm, sandy,\nsplashing, wavy water; white, wet surfboard; bare, wet foot;\nblue, colorful, multi colored, floral shorts; calm, choppy, water, rough,\n17\nFigure 3: Predictions from R101-C4 trained on VG from [2] (top), X152-C4 pre-trained on 4 OD datasets\nand ﬁnetuned on VG (bottom). Test image: COCO test2015 000000028839\n18\nfoamy, wavy water; distant, rocky, hazy mountains; standing, shirtless,\nyoung, barefoot, wet, surfing, walking, smiling boy; calm ocean; distant,\nrocky mountain; white, bare, wet surfboard; wet, sandy, calm, tan beach;\ngray, big rock; blue, calm background; wet, brown, tan, sandy sand;\nwet shadow; blue, colorful, floral, multi colored swim trunks;\nyellow, plastic hand.\nDetections from our pre-trained X152-C4 model pre-trained on four datasets and ﬁne-tuned on VG.\nThere are some repetitive detections, but no obvious wrong detections. See Figure 3 (bottom):\nblue, green fin; young, barefoot, shirtless, standing, surfing, smiling,\nlittle, playing, looking, blond boy; young, barefoot, standing, shirtless,\nsmiling, surfing, blond, playing, looking, little, walking, riding boy;\nshirtless, barefoot, standing, young, smiling, surfing, walking, wet, playing\nman; bare, wet foot; black, white surfboard; small, large, white, crashing,\nbig, water, rolling, splashing, rough, foamy wave; bare, wet foot; dark,\nblack, wet, cast shadow; blue, clear, hazy, cloudy, cloudless sky; black,\ngray, white, raised surfboard; black, wet, short short; brown, short, blond,\nwet, curly, wavy hair; distant, brown, large, rocky, hazy, big mountain;\nbrown, short, dark, blond, wet hair; blue, white, calm, wavy, choppy, ocean,\nsplashing, water, rough, clear, shallow water; bare, tan, light, beige back;\nblack, blue, wet surfboard; small, dark, water, crashing, rolling, splashing,\nbig wave; wet, white, sandy, tan surfboard; blue, colorful, floral, multi\ncolored, patterned trunk; wet, brown, sandy, tan sand; white, blue, calm,\nfoamy, choppy, splashing, wavy, ocean, rough, water, clear, shallow water;\nwet, brown, sandy, calm, tan, shallow, smooth, muddy, rough beach; black,\nwhite, young board; shirtless, young, standing, barefoot, smiling, surfing,\nlooking, walking, playing boy; blue, calm, choppy, wavy, ocean, clear, rough,\nsplashing, water, foamy, shallow, rippled ocean; yellow, gold bracelet; white,\nsilver, black logo; wet, bare, bent, tan, crossed, hairy, short, skinny,\nback, muscular, extended, outstretched leg; black, gray, white board; brown,\ndistant, large, rocky, big hill; brown, short, blond, wet, curly head; red,\nblack logo; bare, raised, extended, holding, open, up, bent, outstretched\nhand; black, wet swim trunks; bare, wet, bent, tan, crossed, skinny, short,\nback, muscular leg; wet, brown, muddy, sandy, tan, shallow reflection.\nB O SCAR + pre-training\nB.1 Pre-training Corpus\nTable 17 shows the statistics of image and text of the pre-training corpora. In our ablation study, we use\ncorpora of three different sizes: ‘Small’, ‘Medium’, ‘Large’. Different from O SCAR [21], we make use\nof image tagging datasets OpenImages, by generating captions using O SCAR ’s image captioning model to\nform triplets of (generated caption, image tags, image features) for O SCAR + pre-training. By self-training\ntechnique, our pre-training corpora can be scaled to a much larger amount by making use of large-scale\nimage tagging datasets, e.g., OpenImages (9M) and YFCC (92M).\n19\nSmall 0.22M Images, 2.5M QAs, 0.7M captions\nMedium 1.89M Images, 2.5M QAs, 0.7M captions, 1.67M pseudo-captions\nLarge 5.65M Images, 2.5M QAs, 4.68M captions, 1.67M pseudo-captions\nSource VQA GQA VG-QA COCO Flicker30k OpenImages CC SBU\n(train) (bal-train) (train) (train) (train) (od train) (train) (all)\nImage/Text 83k/545k 79k/1026k 87k/931k 112k/559k 29k/145k 1.67M/1.67M 3.1M/3.1M 875k/875k\nw,q,v Question, Answer, ImageFeatures (Generated) Caption, (Generated) ImageTags, ImageFeatures\nTable 17: Statistics of the pre-training corpus.\nB.2 O SCAR + pre-training objectives\nMasked Token Loss: A Loss Mimics Image Captioning. The word tokens of image captions (questions)\nwand word tokens of object tags (answers) qshare the same linguistic semantic space, and the Masked\nToken Loss (MTL) is applied on tokens of both wand q. We deﬁne the discrete token sequence as h ≜\n[w, q], and apply the Masked Token Loss (MTL) for pre-training. At each iteration, we randomly mask each\ninput token in hwith probability 15%, and replace the masked one hi with a special token [MASK]. The goal\nof training is to predict these masked tokens based on their surrounding tokens h\\i and image features vby\nminimizing the negative log-likelihood:\nLMTL = −E(v,h)∼Dlog p(hi|h\\i, v) (5)\nThis is the same MTL as in O SCAR [21] and similar to the masked language model used by BERT. The\nmasked word or tag needs to be recovered from its surrounding context, with additional image information\nto help ground the learned word embeddings in the vision context.\n3-way Contrastive Loss: A Loss Mimics Text-Image Retrieval and Visual Question Answering Simul-\ntaneously. We present our 3-way contrastive loss in Section 3.2 in the main paper.\nB.3 Ablation of the two new techniques\nEffect of self-training: Leveraging Image Tagging data. In Figure 4, we show the effect of self-training\nby making use of tagging data in O SCAR +, by ﬁne-tuning O SCAR + pre-training checkpoints on VQA.\nCompared with “O SCAR +, Small; VinVL” (green), “O SCAR +, Medium; VinVL” (yellow) adds the 1.7M\nOpenImages Tagging data into pre-training and its performance gets improved signiﬁcantly, demonstrating\nthe effect of self-training by making use of tagging data. As baselines, we also provide performance of\nOSCAR and OSCAR + with image features from [2], which clearly demonstrates that the new image features\npre-trained by VinVL matter signiﬁcantly in the VL pre-training and VL downstream tasks.\nEffect of the new 3-way contrastive loss. As illustrated in Table 3, with the new 3-way contrastive loss,\nthe VQA performance is the same as the O SCAR pre-training, while the Text-Image Retrieval performance\nimproves signiﬁcantly compared with the OSCAR pre-training.\n20\nFigure 4: Effect of O SCAR + pre-training corpus size and effect of self-training by making use of tagging\ndata in OSCAR +. Each curve, with legend “VLP, Corpus; VisionFeature”, denotes a VLP experiment where\nthe VLP method is either O SCAR or O SCAR +, the VLP pre-training Corpus is Small/Medium/Large (de-\nﬁned in Table 17), and VisionFeature is either our new vision features (VinVL for short) or those from [2]\n([2] for short). X-axis denotes the pre-training iterations of O SCAR + checkpoints. Y-axix is the vqa-dev\naccuracy of a VQA model initialized from the corresponding pre-training checkpoint and ﬁne-tuned with a\nﬁxed scheme. Compared with “O SCAR +, Small; VinVL” (green), “O SCAR +, Medium; VinVL” (yellow)\nadds the 1.7M OpenImages Tagging data into the pre-training and its performance gets improved signif-\nicantly, demonstrating the effect of self-training by making use of tagging data. The “O SCAR +, Large;\nVinVL” (blue) further scales up the pre-training corpus by adding Google Conceptual Captions and SBU\ndatasets with generated tags and its performance gets further improved, demonstrating the effect of OSCAR +\npre-training corpus size. As baselines, we also provide performance of O SCAR and O SCAR + with image\nfeatures from [2], which clearly demonstrates that our new image features (VinVL) matter signiﬁcantly in\nthe VL pre-training and VL downstream tasks.\nOverall improvement from O SCAR to O SCAR +. We point out that the improvement from O SCAR to\nOSCAR + with image features from [2] is minor, because (1) we only add 1.7M OpenImages’ tagging data\n21\nto enlarge the pre-training corpus, which is a small portion compared with O SCAR ’s original pre-training\ncorpus (i.e., Large\\OI, 3.98M images and 7.18M image-caption pairs), and (2) the new 3-way contrastive\nloss has more signiﬁcant improvements in Text-Image Retrieval tasks than that in the VQA task, as illus-\ntrated in Table 3. We would expect much more signiﬁcant improvements when we scale up the O SCAR +’s\npre-training corpus to a much larger scale by adding large scale image tagging datasets, e.g., OpenImages\n(9M) and YFCC (92M).\nC Downstream Tasks Fine-tuning\nWe follow the downstream task ﬁne-tuning recipes in OSCAR [21].\nC.1 VQA\nGiven an image and a question, the task is to select the correct answer from a multi-choice list, it requires\nthe model to answer natural language questions based on an image. Here we conduct experiments on the\nwidely-used VQA v2.0 dataset [8], which is built on the MSCOCO [25] images. Following [2], for each\nquestion, the model picks the corresponding answer from a shared set of 3, 129 candidates.\nWhen ﬁne-tuning on the VQA task, the input sequence contains the concatenation of a given question,\nobject tags and object region features, and then the[CLS] output from OSCAR + is fed to a task-speciﬁc linear\nclassiﬁer for answer prediction. Similarly as the literature [2], we treat VQA as a multi-label classiﬁcation\nproblem – assigning a soft target score to each answer based on its relevancy to the human answer responses,\nand then we ﬁne-tune the model by minimizing the cross-entropy loss computed using the predicted scores\nand the soft target scores. During inference, we simply use Softmax for answer prediction.\nFor VQA training, we random sample a set of 2k images from the MS COCO validation set as our\nvalidation set, the rest of images in the training and validation are used in the VQA ﬁne-tuning. For the\nOSCAR +B model, we ﬁne-tune for 25 epochs with a learning rate of 5e−5 and a batch size of 128. For the\nOSCAR +L model, we ﬁne-tune for 25 epochs with a learning rate of 3e−5 and a batch size of 96.\nC.2 GQA\nSimilarly as VQA, GQA tests the reasoning capability of the model to answer a question. We conduct\nexperiments on the public GQA dataset [13]. For each question, the model chooses an answer from a shared\nset of 1, 852 candidates. Our ﬁne-tuning procedure is following Oscar [21, 3], which ﬁrst ﬁne-tunes the\nmodel on unbalanced “all-split” for 5 epochs with a learning rate of 5e−5 and a batch size of 128, and then\nﬁne-tuned on the “balanced-split” for 2 epochs.\nC.3 Image Captioning\nAn image captioning model generates a natural language description for a given image. To enable sentence\ngeneration, we ﬁne-tune O SCAR + using the seq2seq objective. The input samples are processed to triples\nconsisting of image region features, captions, and object tags, in the same way as that during the pre-training.\nWe randomly mask out 15% of the caption tokens and use the corresponding output representations to\nperform classiﬁcation to predict the token ids. Similar to previous works [21, 45], the self-attention mask\nis constrained such that a caption token can only attend to the tokens before its position to simulate a uni-\ndirectional generation process. Note that all caption tokens will have full attentions to image regions and\nobject tags but not the other way around.\n22\nDuring inference, we ﬁrst encode the image regions, object tags, and a special token[CLS] as input. Then\nthe model starts the generation by feeding in a[MASK] token and selecting a token from the vocabulary based\non the likelihood output. Next, the [MASK] token in the previous input sequence is replaced with the selected\ntoken and a new [MASK] is appended for the next word prediction. The generation process terminates when\nthe model outputs the [SEP] token. We use beam search ( i.e., beam size = 5) [2] in our experiments and\nreport our results on the COCO image captioning dataset.\nThough the training objective ( i.e., seq2seq) for image captioning is different from that used in pre-\ntraining (i.e., bidirectional attention-based masked token loss), we directly ﬁne-tune O SCAR + for image\ncaptioning on COCO without additional pre-training on Conceptual Captions [32]. This is to validate the\ngeneralization ability of the OSCAR + models for generation tasks. We use the same Karpathy split [15]. For\nthe O SCAR +B model, we ﬁne-tune with cross-entropy loss for 30 epochs with a batch size of 256 and an\ninitial learning rate of 1e−5 and then with CIDEr optimization [30] for 10 epochs with a batch size of 128\nand initial learning rate of2e−6. We compare with several existing methods, including BUTD [2], VLP [45],\nAoANet [10], OSCAR [21].\nC.4 NoCaps\nNovel Object Captioning [1] extends the image captioning task, is to test models’ capability of describing\nnovel objects from the Open Images dataset [17] which are not seen in the training corpus. Following the\nrestriction guideline of NoCaps, we train OSCAR + on COCO without the initialization from pre-training, so\nno additional image-text pairs are used for training except COCO.\nSince NoCaps images are collected from Open Images, we train an object detector using the Open\nImages training set and apply it to generate the tags. We conduct experiments from BERT model directly\nwithout pre-training as required by the task guidelines. For the O SCAR +B model, we train 30 epochs with\na batch size of 256 and learning rate 1e−4; further we perform CIDEr optimization with learning rate 5e−6\nand batch size 112 for 10 epochs. During inference, we use constrained beam search for decoding. We\ncompare OSCAR + with OSCAR [21] on this task.\nC.5 Image-Text Retrieval\nThere are two sub-tasks: image retrieval and text retrieval, depending on which modality is used as the\nretrieved target. Both tasks calculate a similarity score between an image and a sentence, which heavily\nrelies on the cross-modal representations.\nFollowing Oscar [21], we formulate the retrieval as a binary classiﬁcation problem, where given an\naligned image-text pair, we randomly select a different image or a different sentence to form an unaligned\npair. The ﬁnal representation of [CLS] is used as the input to the classiﬁer to predict whether the given pair\nis aligned or not. In the testing stage, the probability score is used to rank the given image-text pairs of a\nquery.\nFollowing [19], we report the top- K retrieval results on both the 1K and 5K COCO test sets. We\nadopt the widely used Karpathy split [15] on the COCO caption dataset [25] to conduct our experiments.\nSpeciﬁcally, the dataset consists of 113, 287 images for training, 5, 000 images for validation, and 5, 000\nimages for testing. Each image is associated with 5 human-generated captions. For the O SCAR +B model,\nwe ﬁne-tune with a batch size of 256 for 40 epochs. The initial learning rate is set to 2e−5 and linearly\ndecreases. For the OSCAR +L model, we ﬁne-tune with a batch size of128 for 40 epochs. The initial learning\nrate is set to 1e−5 and linearly decreases. We use the validation set for parameter tuning. We compare with\n23\nFigure 5: Overall comparison of vocabulary effect on VQA. X-axis: how the R50-C4 model is trained; Y-\naxis: how the feature is extracted (grid or region features, different kinds of boxes to extract region features).\nAll region features have maximal 50 regions. The top row “Mean” is the average over all rows, showing\nthe overall quality of different vision models. The far-right column “Mean” is the average over all columns,\nshowing the overall quality of different feature extraction methods.\nseveral existing methods, including DVSA [15], VSE++ [5], DPC [44], CAMP [39], SCAN [18], SCG [33],\nPFAN [38], Unicoder-VL [19], 12-in-1 [27], UNITER [4].\nC.6 NLVR2\nGiven a pair of images and a natural language, the goal of NLVR2 [35] is to determine whether the natural\nlanguage statement is true about the image pair. For NLVR2 ﬁne-tuning, we ﬁrst construct two input se-\nquences, each containing the concatenation of the given sentence (the natural language description) and one\nimage, and then two [CLS] outputs from OSCAR + are concatenated as the joint input for a binary classiﬁer,\nimplemented by an MLP.\nFor the OSCAR +B model, we ﬁne-tune for 20 epochs with learning rate {2e−5, 3e−5, 5e−5}and a batch\nsize of 72. For the O SCAR +L model, we ﬁne-tune for 20 epochs with learning rate of {2e−5, 3e−5}and a\nbatch size of 48.\nD More on the Effect of the Object-Attribute Vocabulary Size: disentan-\ngling the effects of region proposals and model weights\nIn Section 5.2, we demonstrate that the more diverse the visual concepts (object and attribute vocabularies)\nare, the better the visual region features for VL tasks. The better performance may come from the more\ndiverse proposed regions where the region features are extracted (see the comparison in Figure 1, “region”\nfor short), or from the better model weights that can produce better high-dimensional region representation\neven for the same region (“model” for short). In this section, we disentangle effects of region proposals\nand model weights, by performing synthetic experiments in which we use region proposals from one vision\nmodel and model weights from another vision model. Our results show that both the region proposals and\nmodel weights matter for VL tasks.\n24\nFigure 6: Left: comparison of object vocab and attribute vocab, average over all types of bounding boxes.\nRight: comparison of feature extraction methods, average over all types of pre-trained vision models. X-axis\nis the number of iterations when we take the checkpoint for evaluation. Y-axis is the VQA accuracy on our\nvqa-dev.\nD.1 Disentangling the effects of region proposals and model weights on R50-C4\nAs in Section 5.2, We train vision models v= Vision(Img) on different datasets, i.e., OpenImages with\n500 object classes (OI:O500), standard ImageNet with 1K classes (ImageNet:O1000), Visual Genome with\n317 object classes (VG-obj), Visual Genome with 1594 object classes (VG:O1594), VG with 1594 ob-\nject classes and 524 attribute classes (VG:O1594A524), pretrain on the merged 4 datasets and ﬁnetune on\nVG:O1594A524 (4Sets→VG:O1594A524). For each model, we also try different ways to extract features:\n(1) region features from different models’ proposed regions (same notations with models) where each image\nhas maximal 50 region features, and (2) grid features where we use all grid features (Grid-273) or ran-\ndomly sampled 50 grid features (Grid-50) for each image. We present the results of these model-region\ncross-combination experiments in Figure 5. We also present the mean accuracy over all box types to obtain\na robust ranking of different checkpoints and the mean accuracy over all checkpoints to obtain a robust\nranking of different box types. We have the following observations:\n• The richer the object vocabulary is, the better for VQA: OI:500 ≈VG-obj:O317 < ImageNet:O1000 <\nVG:O1594.\n• Attribute information is crucial to VL tasks: all features trained with attributes (Columns with VG:O1594A524)\nare signiﬁcantly better than those without attributes.\n• Even for small vision backbone R50, vision pre-training makes vision features better: Column\n“4Sets→VG:O1594A524” are better than all other columns. Notice that the vision pre-training improves\nboth the region features and the grid features.\n• It is crucial to extract features from semantically diverse regions: regions from OI and VG-obj are signif-\nicantly worse than all other regions, and is even worse than grid features.\n25\n• Grid features perform worse than region features with regions proposed by VG models. By comparing\nRow “Grid-273” with rows with VG regions, it seems hopeful to close this gap while paying more hard-\nware memory and computational cost in cross-modal models VL. It is three times slower to train the\n“Grid-273” models than training models with region features.\nIn Figure 6, instead of just showing one ﬁnal number, we provide the mean evaluation curves along\ntraining trajectories to demonstrate the ranking, as an even more robust evidence. These results further\nconﬁrm the conclusions we draw in Section 5.2.\nD.2 Disentangling the effects of region proposals and model weights on the SoTA model\nIn Table 18, we alternate the combination of region proposals and model weights, and evaluate them on\nVQA. As we can see, the improvement of using boxes from the R101-C4 model [2] to extract features from\nour X152-C4 model is much bigger than that of using boxes from our X152-C4 model to extract features\nfrom the R101-C4 model [2], indicating pre-trained model weights are more important than regions. Inspired\nby this analysis, we propose the class-agnostic NMS for region selection in the box head of the OD model,\nwhich does not sacriﬁce any VQA performance but greatly improves the model’s inference speed. This\nanalysis also suggests that large-scale OD pre-training should improve performance for grid-feature based\nVL models, as supported by more results in Appendix F.\nIn Table 18, We also report VQA results with COCO groundtruth object regions (GT-Obj, 80 classes)\nand object-stuff regions (GT-Obj&Stuff, 171 classes). For VQA task, COCO GT boxes are much worse than\nproposals from VG trained models. This shows the difference between typical OD tasks and OD in VL: OD\nin VL requires much richer visual semantics to align with the rich semantics in the language modality. This\nfurther echoes with our claim that an image understanding module trained with rich semantics is crucial for\nVL tasks.\nmodel\nregion GT-Obj GT-Obj&Stuff\nAnderson\net al. [2] VinVL (ours)\nAnderson\net al. [2] 63.81 ±0.94 66.68 ±0.16 68.52 ±0.11 69.05 ±0.06\nVinVL (ours) 65.60 ±0.21 68.13 ±0.26 70.25 ±0.05 71.34 ±0.17\nTable 18: Ablation of region and model on VQA.\nE More on FPN and Comparison of C4 and FPN\nE.1 Two reasons why FPN performs worse than C4 on VL tasks.\nOur experimental results conﬁrm the conclusion of [14] that the FPN model does not provide better region\nfeatures for VL tasks than the C4 model (Columns “R50C4” vs. “R50FPN” in Table 19). Our analysis\nreveals two reasons. First of all, all layers involved in feature extraction in the C4 model have been pre-\ntrained using ImageNet while the MLP head of FPN does not. It turns out that the VG dataset is still small\nto train a good visual features for VL tasks and using ImageNet-pre-trained weights is beneﬁcial. This can\nbe veriﬁed by two experiments: (1) When the R50-C4 model is trained on VG with its box head randomly\ninitialized (VG-trained - R50C4 w/ box head randomly initialized), the C4 model’s performance is the same\nas FPN; and (2) C4 and FPN achieve the same performance after vision pre-training on 4 datasets (68.3 vs.\n68.2). The second reason is due the network architecture (CNN vs. MLP) of the box head in the OD model.\n26\nno image feature\nw\nR50-C4w/ box head\nrandomly initialized R50-FPN R50-C4 4Sets →R50-FPN 4Sets→R50-C4\nVG-trained – 67.6 ±0.13 67.6±0.30 68.0±0.16 68.3±0.11 68.2±0.05\nInitial 55.5±0.50 61.8±0.47 57.6±0.16 64.8±0.44 66.1±0.23 66.8±0.21\nTable 19: C4 vs FPN architecture on VQA. Boxes used to extract features vand tags qused in VL model\nare the same with those used in O SCAR [21]. Row “Initial” means using the initialization model without\nVG training for feature extraction.\nThe convolutional head in C4 has a better inductive bias in encoding visual information than the MLP head\nin FPN. This can be veriﬁed by the fact that when vision features from randomly initialized models are\nused (Row “Initial” in Table 19), R50-C4 performs much better than R50-FPN, indicating that the initial\nC4 features encode much more useful visual information than the inital FPN features. The “random” C4\nfeatures nearly match the feature from ImageNet pre-trained model (Row “Initial” Column “R50C4”), while\n“random” FPN features are close to the performance without visual features as input (Row “Initial” Column\n“no image feature w”).\nE.2 Effect of pooling methods in FPN on VQA performance.\nDifferent from C4 models that extract region features from a single scale (the end of C4 block), FPN models\nextract region features from multiple scales adaptively based on the area of the region. Therefore, there is\nsome in-homogeneity in FPN’s region features since they may come from different scales. In Figure 7, we\nshow that this is not the cause of FPN’s worse performance than C4 on the VQA task. More speciﬁcally,\nwe experiment with 4 pooling methods for FPN architecture. (1) adapt: the original FPN’s pooling method\nthat extract features adaptively from different scales; (2) max: extract features from all scales and then do a\nmax-pool; (3) avg: extract features from all scales and then do an average-pool; (4) concat: extract features\nfrom all scales and then concatenate them together. We also train multiple FPN models on VG with these\npooling methods, with or without pre-training on the Objects365 dataset. We experiment on all possible\ncombinations (in total 8 ×4) of 8 vision models and 4 pooling methods on the VQA task. When there is a\nparameter dimension mis-match, e.g., non-concat FPN models but use concat pooling methods in VQA and\nvice versa, we specify those parameter randomly with PyTorch’s default initialization method. The results\nin Figure 7 shows that (1) there is no obvious difference in different pooling methods, with the default\n“adapt” and the “concat” methods perform slightly better than “max” and “avg”; (2) (without surprise) the\nperformance is signiﬁcantly worse when there is a parameter dimension mismatch between vision models\nand VL task feature extraction methods, i.e., non-concat FPN models but use concat pooling methods in\nVQA and vice versa. These results show that the pooling method (no matter in vision model training or in\nVL task feature extraction) is not the root cause of FPN’s worse performance than C4 on the VQA task.\nE.3 Large-scale object-detection pre-training of C4 and FPN models\nIn this paper, we have trained R50-C4, R50-FPN, R152-C4 and R152-FPN models on the merged object\ndetection datasets described in Table 2. In Figure 8, we report the mAP50 of checkpoints from these 4\nexperiments on 4 validation sets: COCO with stuff (top left), Objects365 (top right), OpenImages (bottom\nleft) and Visual Genome (1594 object classes, bottom right). For R50 models, the R50-FPN model is slightly\nbetter than C4 on COCO and Objects365 but slightly worse than C4 on Visual Genome. For R152 models,\n27\nFigure 7: Pooling methods in FPN feature extraction are not the root cause of FPN’s worse performance\nthan C4. X-axis: the pooling method when extracting features for VL tasks; Y-axis: the pooling method\n(vision model) when pre-training the visual feature extraction model. All experiments are using regions\nfrom the Bottum-up Top-down model [2]. Each combination is experimented twice with two random seeds,\ni.e. seed=42 on the left and seed=88 on the right. The results from two random seeds are consistent.\nFigure 8: Checkpoints’ mAP50 on 4 validation sets: COCO with stuff (top left), Objects365 (top right),\nOpenImages (bottom left) and Visual Genome (1594 object classes, bottom right). For R50 models, the\nR50-FPN model is slightly better than C4 on COCO and Objects365 but slightly worse than C4 on Visual\nGenome. For R152 models, the R152-FPN model is consistently worse than the R152-C4 model on all 4\ndifferent datasets.\nthe R152-FPN model is consistently worse than the R152-C4 model on all 4 different datasets. Therefore,\n28\nImageNet-5k\n[40] 4Sets VG with Attr 4Sets →VG\ngrid feature (273) 68.3±0.29 65.2±2.47 67.5±0.20 69.4*\nregion feature (50) 67.7±0.16 68.5±0.13 69.8±0.23 70.6±0.13\n* The other run failed and thus there is no std for this experiment.\nTable 20: Ablation study of X152 models on VQA. Vision models in the last three columns are trained with\ninitialization from the ImageNet-5k checkpoint in the ﬁrst column. All the region features are extracted with\nboxes proposed by our best X152-C4 model (pre-trained on 4Sets and ﬁne-tuned on VG). By comparing the\nﬁrst column and the last column, we see that our proposed vision pre-training (ﬁrst on 4 sets and then on VG\nwith attributes) improves performance for both the grid-feature based model and the region-feature based\nmodel. Since the X152 backbone is much larger than the R50 backbone in Figure 5, the larger model can\nmake better use of the large pre-training datasets and thus have more signiﬁcant improvements.\nwe ﬁnally use the R152-C4 model for downstream vision-language tasks.\nF Grid feature\nIn Table 20, we train grid-feature based and region-feature based X152 models for VQA, with the vision\nmodels pre-trained on different vision datasets, i.e., “ImageNet-5k” from [40], our 4-dataset merged OD\ndataset 2 (4Sets), our VG dataset with 1594 object classes and 524 attribute classes (VG with Attr), and ﬁrst\n4Sets and then VG (4Sets →VG). Vision models in the last three cases are trained with initialization from\nthe same ImageNet-5k checkpoint from [40]. All the region features are extracted with boxes proposed by\nour best X152-C4 model (pre-trained on 4Sets and ﬁne-tuned on VG). By comparing “ImageNet-5k” and\n“4Sets→VG”, we see that our proposed vision pre-training improves performance for both the grid-feature\nbased model and the region-feature based model. Since the X152 backbone is much larger than the R50\nbackbone in Figure 5, the larger model makes better use of the large pre-training datasets and thus has\nmore signiﬁcant improvements. It is interesting to see that for grid-feature based models, the “ImageNet-\n5k” model performs better than the “4Sets” model and the “VG with Attr”, while it is not the case for\nregion-feature based models. This may indicate that how the vision model is trained (grid-feature wise or\nregion-feature wise) may have big impact on the downstream VL tasks.\nG End-to-end inference efﬁciency\nWe report the end-to-end inference time of different VQA models on a Titan-X GPU and a Xeon E5 CPU\nin Table 21. For CPU evaluation, we force that the inference use only one CPU thread. The input image\nsize is 800 ×1333, and we run the inference with batch size 1 (one image-question pair per batch). We\ncan see that (1) vision models dominate the inference time, especially for large models; (2) models based\non grid-feature are faster than those based on region feature; (3) with our proposed fast inference trick,\nregion-feature models are greatly sped up and their inference time can be brought to within 3 times of that\nof grid-feature models on GPU. We ﬁnd that on CPU with a single thread, our class-agnostic trick does not\nlead to time saving, because nearly all inference time is taken by the backbone and C4 head and the time\nfrom NMS operations is nearly ignorable on CPU.\n29\nModel R50-C4 R101-C4 [2] X152-C4\nVision VL Vision VL Vision VL\nGrid-50 0.059±0.018 0.029±0.002 0.083±0.025 0.030±0.003 0.355±0.022 0.031±0.003\nGrid-273 0.056±0.005 0.027±0.002 0.082±0.022 0.034±0.001 0.344±0.036 0.037±0.004\nObject 0.373±0.040 0.031±0.005 0.663±0.042 0.034±0.003 0.687±0.064 0.036±0.005\nObject-eff 0.165±0.029 0.029±0.002 0.442±0.119 0.036±0.003 0.475±0.049 0.037±0.005\nGrid-50 (cpu) 1.943±0.244 0.480±0.042 4.050±0.398 0.469±0.046 17.765±1.693 0.501±0.047\nGrid-273 (cpu) 2.032±0.230 1.368±0.056 4.052±0.372 1.283±0.067 17.664±1.713 1.326±0.053\nObject (cpu) 11.808±1.322 0.500±0.045 31.863±7.932 0.585±0.044 29.641±3.097 0.565±0.044\nObject-eff (cpu) 11.729±1.280 0.510±0.044 31.791±8.027 0.587±0.043 29.687±3.011 0.574±0.036\nTable 21: Time cost of end-to-end inference on VQA. All cross-modal models are BERT-Base. On the\nSOTA number obtained with X152-C4 region features, the performance keeps the same when changing to\nthe efﬁcient way to extract the feature while the efﬁciency greatly improves on GPU. The efﬁcient version\ndoes not lead to time saving on CPU, because nearly all inference time is taken by the backbone and C4\nhead and the time from NMS operations is nearly ignorable on CPU.\n30"
}