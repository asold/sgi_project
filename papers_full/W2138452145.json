{
  "title": "RNN language model with word clustering and class-based output layer",
  "url": "https://openalex.org/W2138452145",
  "year": 2013,
  "authors": [
    {
      "id": "https://openalex.org/A2101254038",
      "name": "Shi Yongzhe",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2304494554",
      "name": "Wei-Qiang Zhang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2007693339",
      "name": "Jia Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2115770336",
      "name": "Michael T. Johnson",
      "affiliations": [
        "Marquette University"
      ]
    },
    {
      "id": "https://openalex.org/A2101254038",
      "name": "Shi Yongzhe",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2304494554",
      "name": "Wei-Qiang Zhang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2007693339",
      "name": "Jia Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2115770336",
      "name": "Michael T. Johnson",
      "affiliations": [
        "Marquette University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1970689298",
    "https://openalex.org/W4235505822",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2080018251",
    "https://openalex.org/W3141239769",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W2143719855",
    "https://openalex.org/W83522546",
    "https://openalex.org/W136846643",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2140723372",
    "https://openalex.org/W2930957955",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2168489430",
    "https://openalex.org/W2121227244"
  ],
  "abstract": "The recurrent neural network language model (RNNLM) has shown significant promise for statistical language modeling. In this work, a new class-based output layer method is introduced to further improve the RNNLM. In this method, word class information is incorporated into the output layer by utilizing the Brown clustering algorithm to estimate a class-based language model. Experimental results show that the new output layer with word clustering not only improves the convergence obviously but also reduces the perplexity and word error rate in large vocabulary continuous speech recognition.",
  "full_text": "Shi et al. EURASIP Journal on Audio, Speech, and Music Processing2013, 2013:22\nhttp://asmp.eurasipjournals.com/content/2013/1/22\nRESEARCH Open Access\nRNN language model with word clustering and\nclass-based output layer\nYongzheShi1*,Wei-QiangZhang 1,JiaLiu 1 andMichaelTJohnson 2\nAbstract\nTherecurrentneuralnetworklanguagemodel(RNNLM)hasshownsignificantpromiseforstatisticallanguage\nmodeling.Inthiswork,anewclass-basedoutputlayermethodisintroducedtofurtherimprovetheRNNLM.Inthis\nmethod,wordclassinformationisincorporatedintotheoutputlayerbyutilizingtheBrownclusteringalgorithmto\nestimateaclass-basedlanguagemodel.Experimentalresultsshowthatthenewoutputlayerwithwordclusteringnot\nonlyimprovestheconvergenceobviouslybutalsoreducestheperplexityandworderrorrateinlargevocabulary\ncontinuousspeechrecognition.\nKeywords: Brownwordclustering;RNNlanguagemodel;Speechrecognition\n1 Introduction\nStatistical language models estimate the probability of a\nword occurring in a given context, which plays an impor-\ntant role in many natural language processing applica-\ntions such as speech recognition, machine translation, and\ninformation retrieval. Standardn-gram back-off language\nmodels (LMs) are widely used for their simplicity and effi-\nciency. However, in this approach, words are modeled as\ndiscrete symbols with richer linguistic information, such\nas syntax and semantic, ignored completely. Additionally,\nlarge numbers of parameters need to be estimated, and\ndue to the sparsity characteristics of natural language,\nthe probability of low- and zero-frequency events is esti-\nmated crudely and inaccurately using various smoothing\nalgorithms.\nThe distributional hypothesis in linguistics states that\nwords occurring in the same context tend to have simi-\nlar meanings. It is a reasonable assumption that similar\nwords occur in the same context with similar probability,\nfor example, ‘ America, ’ ‘China, ’ and ‘Japan’ which usually\ncome after the same preposition or as the subject of a\nsentence. Based on this assumption, neural network lan-\nguage models (NNLMs) [1,2] project the discrete word\n*Correspondence:shiyz09@gmail.com\n1TsinghuaNationalLaboratoryforInformationScienceandTechnology,\nDepartmentofElectronicEngineering,TsinghuaUniversity,100084Beijing,\nChina\nFulllistofauthorinformationisavailableattheendofthearticle\nindices into a continuous space where similar words occur\nclose together. The predicted probability of the next word\nis returned by a smooth function of the context rep-\nresentation, which alleviates the sparsity issue to some\nextent and leads to better generalization for unseenn-\ngrams. In 2010, Elman’s recurrent neural network was\nfirst used for language modeling by Mikolov [3] and then\nan extension of this model was proposed in 2011 [4,5].\nThe recurrent neural network language model (RNNLM)\nhas a longer memory and has recently performed bet-\nter than other modeling methods [3,4,6]. Accordingly,\nwe select the RNNLM as our baseline approach in this\npaper.\nOne key issue is the heavy computational cost for the\nRNNLM. As the output layer contains one unit for each\nword in the vocabulary, it is infeasible to train the model\nfor large vocabulary with hundreds of thousands of words.\nTherefore, reducing the complexity of neural network\nlanguage models has been an important topic. Perhaps\none method is to estimate the several thousand most\nfrequent words (the shortlist) via NNLMs, while other\nwords are estimated vian-gram back-off models. Unfor-\ntunately, it has been shown that this technique causes\nsevere degradation of performance for a small shortlist\n[1]. Other tree-structured output layer methods have also\nbeen proposed to speed up the NNLMs [7,8]. In these\n©2013Shietal.;licenseeSpringer. ThisisanOpenAccessarticledistributedunderthetermsoftheCreativeCommons\nAttributionLicense(http://creativecommons.org/licenses/by/2.0),whichpermitsunrestricteduse,distribution,andreproduction\ninanymedium,providedtheoriginalworkisproperlycited.\nShi et al. EURASIP Journal on Audio, Speech, and Music Processing2013, 2013:22 Page 2 of 7\nhttp://asmp.eurasipjournals.com/content/2013/1/22\nmethods, the tree structure of the output layer needs to\nbe constructed carefully using linguistic knowledge such\nas WordNet [9] or word continuous representation. In\ngeneral, speed and performance need to be balanced so\nthat training and testing process is accelerated as much\nas possible, without deteriorating the performance of the\nmodel.\nIn this paper, we introduce a new method for con-\nstructing a class-based output layer using the Brown\nclustering algorithm. The closest previous work to this\nis a simple frequency-based word factorizing algo-\nrithm used to construct the output layer [4]. Words\nare roughly clustered according to their frequencies in\nthis method, with training speed increasing but per-\nformance degraded. We extend this work to improve\nthe performance of RNNLM and speed up the train-\ning. Words are clustered off-line and then the word\nclasses are embedded into the output layer to estimate\nthe class-based language model, where the RNN is used\nto estimate the conditional probability of classes and\nwords.\nThis paper is organized as follows: In Section 2, we\nintroduce our baseline RNNLM and the proposed Brown\nclustering method for constructing the output layer. Per-\nplexity evaluation on a public corpus is performed in\nSection 3. Our proposed model is further evaluated on\nthe Wall Street Journal (WSJ) and Switchboard speech-to-\ntext tasks in Sections 4 and 5. Finally, Section 6 concludes\nthis paper and gives the future work.\n2 Model description\n2.1 RNN language model\nAn Elman recurrent neural network (RNN) [10] is shown\nin Figure 1. The hidden state is a function of the entire\ninput history. RNNs are well known for their long mem-\nory and are widely used for dynamic system modeling and\nsequence prediction. LetV denotes the vocabulary with\nFigure 1Recurrent neural network language model.\n‘1-of-|V|’ coding used in the input layer, so that theith\nword of the vocabulary is encoded as a binary|V|-dim\nvector, where the ith element is set as 1 and all oth-\ners are 0. Let ht = sigmoid(Wihxt + Whhht−1),w h e r e\nxt and ht denote the input and the hidden activation\nat the current time step, respectively. The hidden state\nht is activated by the current input xt and the previ-\nous hidden activation ht−1.D e f i n eP(wt|wt−1\n1 ) = ot =\nsoftmax(Whoht),w h e r ewt and wt−1\n1 denote the next word\nand the context. The output layerot corresponds to the\npredicted probability of all words in the vocabulary. To\nspeed up the training of RNNLM, a frequency-based\nextension of RNNLM is introduced in [4], which is a class-\nb a s e dm o d e la ss h o w ni nF i g u r e2 .L e tP(wt|wt−1\n1 ) =\nP(C(wt)|wt−1\n1 )P(wt|C(wt), wt−1\n1 ),w h e r eC(wt) denotes the\nclass of the wordwt. The predicted probabilities of classes\nand specific words are estimated to decrease the compu-\ntational complexity.\nTo speed this up, a simple frequency-based factoriz-\ni n gm e t h o di su s e dt oc o n s t r u c tt h ee q u i v a l e n c ec l a s so f\nwords in the class-based model. Compared with RNNLM\nwithout a class-based layer, the perplexity (PPL) of the\nmodel shown in Table 1 is 10% higher for the Penn Tree-\nbank Corpus (one million words). Details can be found in\nSection 3.\n2.2 Word clustering for output layer\n2.2.1 Frequency-based clustering\nFrequency-based word clustering is referred to as the fre-\nquency binning factorization method [4], where words\nare assigned to classes proportionally. Figure 3 gives the\nunigram cumulative probability distribution for the Penn\nTreebank Corpus, which describes the well-known phe-\nnomenon of Zipf’s law in natural language. This method\ndivides the cumulative probability into K partitions to\nform K frequency binnings which correspond toK clus-\nters, a very rough word partition which only considers\nfrequency. Zipf’s law states that given some corpus of\nnatural language utterances, the frequency of any word\nis inversely proportional to its rank in the frequency\ntable. This means that the very few high-frequency words\noccupy most of the text corpus. It can be seen in Figure 3\nthat the most frequent 150 words and 2,800 words occupy\nmore than 50% and 80% of the text corpus, respectively.\nIn order to have 100 equal clusters, the top 150 words\nmake up the first 50 clusters and the top 2,800 words make\nup the first 80 clusters, which means the last 7,200 words\nform 20 clusters. Therefore, clusters containing high-\nfrequency words are very small, possibly even containing\none word. In contrast, most words are in the remaining\nclusters, each containing hundreds or even thousands of\nwords. Thus, the clustering results of this method depend\nseverely on frequency distribution of the training corpus,\nleading to unsatisfactory clustering results.\nShi et al. EURASIP Journal on Audio, Speech, and Music Processing2013, 2013:22 Page 3 of 7\nhttp://asmp.eurasipjournals.com/content/2013/1/22\nFigure 2Class-based output layer for RNNLM.\n2.2.2 Brown clustering\nBrown clustering is a data-driven hierarchical word clus-\ntering algorithm [11,12], which is widely used in natural\nlanguage processing. The input to this algorithm is text,\nwhich is a sequence of wordsw1, w2, ...,wn,a n dt h eo u t -\nput of the clustering algorithm is a binary tree, the leaves\nof which are words. In this paper, we interpret all leaves\nwith the same parent node as a cluster in the tree. The\nBrown clustering algorithm was first proposed to estimate\na class-based language model [11]. LetV, C,a n dT denote\nthe vocabulary, the word clusters and the text corpus,\nrespectively. The optimization object is the cross entropy\nof the text corpus:\nloss(C) =− 1\n|T| log P(w1...w|T|)\n=− 1\n|T| log\n|T|∏\ni=1\nP(C(wi)|C(wi−1)P(wi|C(wi)))\n(1)\nwhere |T| denotes the length of text and C (·) maps\nthe words to the specific clusters. P(C(wi)|C(wi−1)) and\nP(wi|C(wi)) can be estimated by frequency. Therefore, the\nobject loss function can be rewritten as follows:\nloss(C) =− 1\n|T|\n|T|∑\ni=1\nlog nci,ci−1\nnci−1\n∗ nwi\nnci\n=−\n∑\nc,c′∈C\nP(c, c\n′\n) log P(c, c\n′\n)\nP(c)P(c\n′\n)\n−\n∑\nw∈V\nP(w) log P(w),\n(2)\nTable 1 Perplexities on test set of Penn Treebank Corpus\nModel Perplexity\nRNNLM(class100) 135.49\nRNNLM(noclasslayer) 123.00\nwhere nwi denotes the occurrence count of patternwi that\noccurs in the corpus.\nInitially, the algorithm starts with each word in its own\ncluster. As long as there are more than one cluster left, the\nalgorithm merges the two clusters that minimizes the loss\nof the clustering result as shown in Equation 2. The naive\nalgorithm has time complexityO(|V|3) [11] and is imprac-\ntical for hundreds of thousands of words. Fortunately, a\nvariant algorithm with time complexityO(|V|K2 +| T|)\nwas proposed in [13], whereK denotes the number of\nclusters. The algorithm is described as follows with details\navailable in [13].\nInput: text corpusT, the number of clustersK\nOutput: K word clusters\n• Take theK most frequent words, put each into its\nown clusterc1, c2, ...,cK\n• For i = K + 1: |V|\n1. Create a new clustercK+1 for theith most\nfrequent word, givingK + 1 clusters.\nFigure 3Cumulative unigram probability distribution for Penn\nTreebank Corpus with about one million words (Zipf’s law).\nShi et al. EURASIP Journal on Audio, Speech, and Music Processing2013, 2013:22 Page 4 of 7\nhttp://asmp.eurasipjournals.com/content/2013/1/22\n2. Choose two clusters fromc1, c2, ...,cK+1 to merge,\nselecting these that minimize Equation 2.\n• Implement |V| − K merges to create a full\nhierarchical word clusters.\nIn this paper, we partition the words into clusters using\nthis algorithm and demonstrate its effectiveness for RNN\nlanguage modeling in terms of both perplexity and word\nerror rate.\n3 Penn Treebank Corpus evaluation\nThe proposed language model is evaluated on the Wall\nStreet Journal portion of the Penn Treebank which is pre-\nprocessed by lowercasing words, removing punctuation\nand replacing numbers with the ‘N’ symbol. Sections 00-\n20 (930K words) are used as training sets, sections 21-22\nas validation sets (74K words), and sections 23-24 as test\nsets (82K words). The vocabulary size is 10K, including a\nspecial token for unknown words.\nWe compare the proposed model with the baseline\nRNNLM model [5,6]. We denote our proposed model\nas RNNLM-Brown and the baseline as RNNLM-Freq\nfor convenience, where the ‘Brown’ and ‘Freq’ mean the\nBrown clustering and the frequency-based clustering,\nrespectively. Both models have the same basic configu-\nration (200 hidden units) for comparisons in the follow-\ning experiments. The truncated backpropagation through\ntime algorithm (BPTT) is used for training the RNNLMs\nusing ten time steps. When the perplexity decreases\nvery slowly or increases, the learning rate is halved.\nThe basic 5-gram back-off language model (LM-KN5) is\ntrained with the modified Kneser-Ney smoothing algo-\nrithm. Figure 4 shows the convergence process of the\nFigure 4PPL convergence of RNNLM-Freq/Brown on validation\nsets.\nTable 2 Comparisons of perplexities on test set of Penn\nTreebank Corpus with different sizes of class layer\nClass RNNLM-Freq / +KN5 RNNLM-Brown / +KN5\n(words per second) (words per second)\n30 135.57/113.13(744) 131.46/110.83(567)\n50 136.39/113.53(938) 129.79/109.96(862)\n100 135.49/113.07(1,047) 128.36/109.33(970)\n200 136.03/112.89(1,013) 128.52/109.13(1,000)\n400 135.75/113.04(847) 128.03/109.09(906)\n800 134.98/112.51(645) 128.09/109.23(710)\n1,600 133.44/111.93(367) 128.67/109.47(480)\n10,000(full) 123.00/106.00(65) 123.00/106.00(65)\nThe full model use the whole 10K vocabulary as the class layer, which is the\nsame for both models. Perplexity of LM-KN5 on test set is 141.46.\nvalidation set’s perplexity for RNNLM-Freq and RNNLM-\nBrown, where RNNLM-Brown with 13 epochs obtains the\nsame perplexity as RNNLM-Freq with 24 epochs. We can\nsee that the proposed RNNLM-Brown converges twice\nfaster and obtains lower perplexity on the validation set.\nAccordingly, appropriate word clustering for the output\nlayer can speed up the convergence, which is especially\nimportant for a large training corpus.\nIn the following experiments, perplexity and training\nspeed are evaluated with different sizes of class layers,\nas shown in Table 2. The first two columns refer to the\nbaseline (RNNLM-Freq) and the interpolated model with\nLM-KN5 (RNNLM-Freq + KN5), respectively, which is\nconsistent with the results reported in [4]. The last two\ncolumns correspond to the proposed language model\n(RNNLM-Brown) and the interpolated version with LM-\nKN5 (RNNLM-Brown + KN5). The full model uses the\nentire vocabulary for the class layer, in which each word\nFigure 5Detailed comparisons of perplexity and speed for\ndifferent RNNLMs on Penn Treeback Corpus.\nShi et al. EURASIP Journal on Audio, Speech, and Music Processing2013, 2013:22 Page 5 of 7\nhttp://asmp.eurasipjournals.com/content/2013/1/22\nTable 3 Comparisons of perplexities on test set of five\nmillion training corpus with different sizes of class layer\nClass RNNLM-Freq / +KN5 RNNLM-Brown / +KN5\n50 218.13/178.10 206.19/172.08\n100 220.47/178.50 208.37/172.06\n200 219.60/178.21 206.54/171.21\n400 219.73/178.09 205.02/170.56\nPerplexity of LM-KN5 with the same training text (five million words) on test set\nis 231.02.\ncorresponds to a separate cluster. The interpolated coeffi-\ncients are determined according to the validation set. To\nmake the observation easier, we plot the test perplexity\nand the speed of training in Figure 5, where the blue and\nred lines correspond to the lefty-axis for perplexity and\nthe green lines correspond to the righty-axis for the train-\ning speed. The training speed is evaluated by the number\nof words processed per second on a machine with an\nIntel® Core™2 Quad CPU Q9400 at 2.66 GHz, 8-GB RAM.\nIn practice, the training speed first increases and then\ndecreases with the increasing number of clusters. There\nis a trade-off between the perplexity and the training\nspeed, especially for a larger corpus. From Figure 5, 100\nor 200 clusters are the best choices balancing this, and\nthe training speed is 15 times faster than that of the full\nmodel. Empirically, the best number of clusters is around√|V|,w h e r e|V| denotes the size of vocabulary. In the\nexperiments, the perplexity is reduced by about 5% for\nboth single and interpolated models without reducing the\nspeed of training, compared with the baseline. Moreover,\nthe performance of the proposed RNNLM-Brown is much\ncloser to that of the full model without the class layer.\n4 WSJ speech recognition experiment\nTo evaluate the performance of the proposed language\nmodel for speech recognition, we use the WSJ task which\nis a standard task for language model evaluation. The\nacoustic model is a 6,000-tied-state continuous model\nwith 32 Gaussians, trained on the WSJ1 Corpus; the\ndetails of which can be found in [14]. The LM training\ntext contains 26 million words from the entire NYT-\n1994 section of the English Gigaword Corpus. The top\n64,000 most frequent words are selected as the vocabulary\nTable 4 WER for development set rescored with different\nRNNLMs and LM-KN5\nClass\nModel 50 100 200 400\nLM-KN5+RNNLM-Freq(%) 11.7 11.6 11.6 11.7\nLM-KN5+RNNLM-Brown(%) 11.5 11 ˙5 11.5 11.4\nThe WER of 1-best hypothesis is 13.4% and the WER for LM-KN5 is 12.9%.\nTable 5 WER for evaluation set rescored with different\nRNNLMs and LM-KN5\nClass\nModel 50 100 200 400\nLM-KN5+RNNLM-Freq(%) 13.0 13.1 13.0 13.1\nLM-KN5+RNNLM-Brown(%) 12.7 12.7 12.7 12.4\nThe WER of 1-best hypothesis is 14.1% and the WER for LM-KN5 is 13.8%.\nand other words are mapped to a special token. Trigram\n(LM-KN3) and 5-gram (LM-KN5) models are trained on\nthe text using the MITLM toolkit [15] with the modi-\nfied Kneser-Ney smoothing algorithm for decoding and\nrescoring.\nIn the experiment, we use the NIST 1993 CSR Hub and\nSpoke Benchmark Test Corpora [16] as our test bed. We\nselect the hub 1 and spoke 1, 2 and 4 sections for evalu-\nations, which contain 1,251 utterances (about 25K words,\n2.5 h of voice data) All the utterances are divided equally\ninto two parts as the development set and the evaluation\nset.\nDue to the complexity of training the neural network\nlanguage model, this requires several days or an even\nlonger time period to converge for several million train-\ning words. Thus, we randomly select about five million\nwords from the NYT-1994 section of English Gigaword\nCorpus as the training data, 500K words as the validation\ndata for early stopping and 500K words as the test data\nfor perplexity evaluation. The top 30K frequent words are\nselected as the vocabulary. The truncated BPTT is used\nfor training the different RNNLMs with ten time steps.\nTwo hundred hidden units are used. The learning rate is\ninitially set to 0.1 and halved when the perplexity of the\nvalidation data is increased. The detailed results are given\nFigure 6PPL convergence of RNNLM-Freq/Brown on\nHub5’00-SWB set.\nShi et al. EURASIP Journal on Audio, Speech, and Music Processing2013, 2013:22 Page 6 of 7\nhttp://asmp.eurasipjournals.com/content/2013/1/22\nFigure 7WER convergence of 100-best rescoring with\nRNNLM-Freq/Brown on Hub5’00-SWB set.\nin Table 3, where consistent improvements are observed\nwith different class sizes for the larger training corpus.\nThe perplexity is reduced by approximately 5%, compared\nwith the baseline.\nIn the following experiments, the 200-best hypothe-\nses are generated using back-off trigram trained on the\nentire 26 million words and then rescored with different\nlanguage models for comparisons. The interpolated coef-\nf i c i e n to fR N N L Ma n dL M - K N 5i sd e t e r m i n e do nt h e\ndevelopment set. Detailed results can be found in Tables 4\nand 5. We can see that the word error rate (WER) for\nevaluation set is consistently reduced by 0.3% to 0.7%\nabsolutely, compared with that of RNNLM-Freq, and 1.4%\nto 1.7% compared with the 1-best hypothesis (i.e. more\nthan 10% relative reduction of WER).\n5 Switchboard speech recognition experiment\nIn this section, the effectiveness of our proposed model on\nthe task of speech-to-text transcription is evaluated on the\n309h Switchboard-I training set [17], a larger corpus than\nWSJ Corpus. The system uses 13-dimensional PLP fea-\ntures with rolling-window mean-variance normalization\nand up to third-order derivatives, reduced to 39 dimen-\nsions by HLDA. The speaker-independent three-state\ncross-word triphones share 9,308 CART-tied states. The\nGMM-HMM baseline system has 40-Gaussian mixtures\nper state, trained with maximum likelihood and refined\ndiscriminatively with the boosted maximum-mutual-\ninformation criterion.\nThe data for system development is the 1831-segment\nSWB part of the NIST 2000 Hub5 evaluation set\n(Hub5’00-SWB). The FSH half of the 6.3 h Spring 2003\nNIST Rich Transcription set (RT03S-FSH) acts as the\nevaluation set. Based on Kneser-Ney smoothing, a back-\noff trigram language model (LM-KN3) was trained on\nthe 2000h Fisher transcripts containing 20 million tokens\nfor decoding, where the vocabulary is limited to 53K\nwords and unknown words are mapped into a spe-\ncial token <unk>. Note that no other unkown text is\nused to train LMs for interpolations so that the follow-\ning experimental results are easily repeatable. The pro-\nnouncing dictionary comes from the CMU pronouncing\ndictionary [18].\nTwo models (RNNLM-Freq/Brown) with 300 hidden\nunits are trained on the entire training text for com-\nparisons. The perplexity convergence of RNNLM-Freq\nand RNNLM-Brown on Hub5’00-SWB set is shown in\nFigure 6. It can be seen that waiting long enough, the\nRNNLM with brown clustering converges better than\nthat with frequency partitions. Moreover, the perplexity\nof RNNLM-Brown with 9 epochs competes with that of\nRNNLM-Freq with 16 epochs; it means that RNNLM-\nBrown converges twice faster than RNNLM-Freq.\nSubsequently, these models are further compared to\nrescore N-best hypotheses. For convenience, 100-best\nhypotheses are generated from Hub5’00-SWB and RT03S-\nFSH and rescored by different LMs. The interpolation\nweights are tuned on Hub5’00-SWB, and the perfor-\nmances of these LMs are evaluated on RT03S-FSH. These\nintermediate models during the training are used for\nrescoring, and the performance of these models in word\nerror rate is plotted in Figure 7 for comparisons, where the\nRNNLM-Brown converges much faster and better than\nTable 6 100-Best rescoring with different LMs on Hub5’00-SWB and RT03S-FSH\nModel Perplexity WER (%, absolute change)\nHub5’00-SWB RT03S-FSH Hub5’00-SWB RT03S-FSH\nLM-KN3 89.40 66.76 24.5 27.5\nLM-KN5 86.78 63.80 24.1( −0.4) 27.1( −0.4)\nRNNLM-Freq 72.47 55.76 22.9( −1.6) 25.9( −1.6)\nRNNLM-Freq +LM-KN5 67.66 52.15 22.4( −2.1) 25.5( −2.0)\nRNNLM-Brown 69.91 54.48 22.6( −1.9) 25.7( −1.8)\nRNNLM-Brown+LM-KN5 66.00 51.24 22.2 ( −2.3) 25.3 ( −2.2)\nValues in italics indicate the lowest perplexity and WER on Hub5’00-SWB and RT03S-FSH.\nShi et al. EURASIP Journal on Audio, Speech, and Music Processing2013, 2013:22 Page 7 of 7\nhttp://asmp.eurasipjournals.com/content/2013/1/22\nRNNLM-Freq. We can see that RNNLM-Brown with 9\nepochs obtains the same WER as RNNLM-Freq with 16\nepochs.\nThe performances of different LMs in perplexity and\nword error rate are shown in Table 6, where Hub5’00-\nSWB and RT03S-FSH are used for validation and eval-\nuation, respectively. We can see that the WER of our\nproposed model RNNLM-Brown interpolated with LM-\nKN5 obtains the lowest perplexity 51.24 and word error\nrate 25.3% on the evaluation set. In a word, our pro-\nposed RNNLM-Brown converges faster and better in the\nexperiment.\n6C o n c l u s i o n s\nIn this paper, the Brown word clustering algorithm is pro-\nposed to construct a class layer for the RNNLM. Exper-\nimental results show that our proposed RNNLM-Brown\nimproves the perplexity and decreases the word error rate\nobviously. The performance of our proposed RNNLM-\nBrown is much closer to that of the full model without\na class layer. Moreover, our proposed RNNLM-Brown\nconverges twice faster than RNNLM-Freq, which is crit-\nical for a large-scale dataset. Additionally, we notice that\nthe outputs of the brown clustering algorithm include a\nbinary tree structure of clusters, which is not used in\nthis paper. In future work, we will further investigate this\ntree-structured output layer according to the hierarchical\nword cluster results. Additionally, we will also investigate\nwhether soft clustering of words [19] can be incorporated\ninto the RNNLM to further improve its performance.\nAbbreviations\nASR:Automaticspeechrecognition;BPTT:Backpropagationthroughtime\nalgorithm;LM:Standardback-off n-gramlanguagemodel;NNLM:Neural\nnetworklanguagemodel;RNNLM:Recurrentneuralnetworklanguagemodel;\nFreq:Freq-basedwordclustering;Brown:Brownwordclustering;KN:Kneser-\nNeysmoothingalgorithm;LM-KN5:5-grambasedonKneser-Neysmoothing\nalgorithm;PPL:Perplexity;WER:Worderrorrate;WSJ:WallStreetJournal.\nCompeting interests\nTheauthorsdeclarethattheyhavenocompetinginterests.\nAcknowledgements\nThisworkwassupportedbytheNationalNaturalScienceFoundationofChina\nundergrantnos.61273268,61005019and90920302,andinpartbyBeijing\nNaturalScienceFoundationProgramundergrantno.KZ201110005005.\nAuthor details\n1TsinghuaNationalLaboratoryforInformationScienceandTechnology,\nDepartmentofElectronicEngineering,TsinghuaUniversity,100084Beijing,\nChina.\n2DepartmentofElectricalEngineering,MarquetteUniversity,WI53201,\nMilwaukee,USA.\nReceived: 8 October 2012 Accepted: 2 July 2013\nPublished: 22 July 2013\nReferences\n1. FBalado,NJHurley,EPMccarthy,GCMSilvestre,Continuousspace\nlanguagemodels.Comput.Speech.Lang. 21(3),492–518(2007)\n2. LHSon,RAllauzen,GWisniewski,FYvon,in Proceedings of the Conference\non Empirical Methods in Natural Language Processing (EMNLP).Training\ncontinuousspacelanguagemodels:somepracticalissues.\nMassachusetts,9–11October2010,pp.778–788\n3. MTomas,KMartin,BLukas,HGJan,KSanjeev,in Proceedings of the Annual\nConference of International Speech Communication Association\n(INTERSPEECH).Recurrentneuralnetworkbasedlanguagemodel.Chiba,\n26–30September2010,pp.1045–1048\n4. MTomas,KStefan,BLukas,HGJan,KSanjeev,in Proceedings of IEEE\nInternational Conference on Acoustic, Speech and Signal Processing (ICASSP).\nExtensionsofrecurrentneuralnetworklanguagemodel.Prague,22–27\nMay2011\n5. MTomas,DAnoop,KStefan,BLukas,HGJan,in Proceedings of Automatic\nSpeech Recognition and Understanding Workshop (ASRU).RNNLM-\nrecurrentneuralnetworklanguagemodelingtoolkit.Waikoloa,11–15\nDecember2011\n6. MTomas,DAnoop,KStefan,BLukas,HGJan,in Proceedings of the Annual\nConference of International Speech Communication Association\n(INTERSPEECH 2011).Empiricalevaluationandcombinationofadvanced\nlanguagemodelingtechniques.Florence,27–31August2011\n7. FMorin,YBengio,in Proceedings of AISTATS.Hierarchicalprobabilistic\nneuralnetworklanguagemodel.Christchurch,6–8January2005,pp.\n246–252\n8. HSLe,IOparin,AAllauzen,JLGauvain,FYvon,in Proceedings of IEEE\nInternational Conference on Acoustic, Speech and Signal Processing (ICASSP).\nStructuredOutputLayerneuralnetworklanguagemodel.Prague,22–27\nMay2011,pp.5524–5527\n9. CFellbaum, WordNet: An Electronic Lexical Database.(MITPress,\nCambridge,1998)\n10. JLElman,Findingstructureintime.Cogn.Sci. 14(2),179–211(1990)\n11. PFBrown,PVdeSouza,RLMercer,VJDPietra,JCLai,Class-basedn-gram\nmodelsfornaturallanguage.Comput.Linguist. 18(4),467–479(1992)\n12. SMartin,JLiermann,HNey,Algorithmsforbigramandtrigramword\nclustering.SpeechCommun. 24,1253–1256(1998)\n13. PLiang,Semi-supervisedlearningfornaturallanguageprocessing.\nMaster’sthesis,MIT,2005.http://dspace.mit.edu/handle/1721.1/33296\n14. KVertanen, Baseline WSJ acoustic models for HTK and Sphinx: training\nrecipes and recognition experiments,(2006).http://keithv.com/software/\n15. BJPHsu,MITlanguagemodelingtoolkit(2009).http://code.google.com/\np/mitlm/\n16. 1993ARPACSRHubandSpokeBenchmarkTestsCorpora(1993).http://\nwww.ldc.upenn.edu/Catalog/readme_files/csr2.readme.html\n17. JGodfrey,EHolliman, Switchboard-1 Release,vol.2.(LinguisticData\nConsortium,Philadelphia,1997)\n18. TheCMUPronouncingDictionaryRelease0.7a(2007).http://www.\nspeech.cs.cmu.edu/cgi-bin/cmudict\n19. YSu,in Proceedings of IEEE International Conference on Acoustic, Speech\nand Signal Processing (ICASSP).Bayesianclass-basedlanguagemodels.\nPrague,22–27May2011\ndoi:10.1186/1687-4722-2013-22\nCite this article as:Shi et al.: RNN language model with word clustering\nand class-based output layer.EURASIP Journal on Audio, Speech, and Music\nProcessing 20132013:22.\nSubmit your manuscript to a \njournal and beneﬁ t from:\n7 Convenient online submission\n7 Rigorous peer review\n7 Immediate publication on acceptance\n7 Open access: articles freely available online\n7 High visibility within the ﬁ  eld\n7 Retaining the copyright to your article\n    Submit your next manuscript at 7 springeropen.com",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9638364315032959
    },
    {
      "name": "Computer science",
      "score": 0.8186976909637451
    },
    {
      "name": "Language model",
      "score": 0.7898154258728027
    },
    {
      "name": "Cluster analysis",
      "score": 0.7059101462364197
    },
    {
      "name": "Word (group theory)",
      "score": 0.6761928200721741
    },
    {
      "name": "Layer (electronics)",
      "score": 0.6568369269371033
    },
    {
      "name": "Vocabulary",
      "score": 0.6028406620025635
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5784028172492981
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5372728109359741
    },
    {
      "name": "Word error rate",
      "score": 0.5269811153411865
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4597302973270416
    },
    {
      "name": "Natural language processing",
      "score": 0.4504464864730835
    },
    {
      "name": "Speech recognition",
      "score": 0.40102651715278625
    },
    {
      "name": "Artificial neural network",
      "score": 0.40066099166870117
    },
    {
      "name": "Linguistics",
      "score": 0.10585668683052063
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I102461120",
      "name": "Marquette University",
      "country": "US"
    }
  ],
  "cited_by": 28
}