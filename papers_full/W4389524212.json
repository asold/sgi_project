{
  "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
  "url": "https://openalex.org/W4389524212",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2016723844",
      "name": "Hongli Zhan",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2314947771",
      "name": "Desmond Ong",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    },
    {
      "id": "https://openalex.org/A2129999472",
      "name": "Junyi Li",
      "affiliations": [
        "The University of Texas at Austin"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385573523",
    "https://openalex.org/W2963847595",
    "https://openalex.org/W2741447225",
    "https://openalex.org/W4249521496",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W170526017",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W4388282970",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4306294746",
    "https://openalex.org/W2970443729",
    "https://openalex.org/W2804575779",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2182854643",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3034720707",
    "https://openalex.org/W1768593022",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W810968520",
    "https://openalex.org/W4321524280",
    "https://openalex.org/W1986740180",
    "https://openalex.org/W4385565276",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W1682639223",
    "https://openalex.org/W2042863830",
    "https://openalex.org/W4226218072",
    "https://openalex.org/W2892118011",
    "https://openalex.org/W3037013468",
    "https://openalex.org/W4378499145",
    "https://openalex.org/W4321594342",
    "https://openalex.org/W2153222072",
    "https://openalex.org/W3014241248",
    "https://openalex.org/W4226353085",
    "https://openalex.org/W3034323190",
    "https://openalex.org/W4389523980",
    "https://openalex.org/W3212398293",
    "https://openalex.org/W578982847",
    "https://openalex.org/W3094293990",
    "https://openalex.org/W4286748532",
    "https://openalex.org/W4289704658",
    "https://openalex.org/W2098335992",
    "https://openalex.org/W4390509281",
    "https://openalex.org/W2109474021",
    "https://openalex.org/W3166185110",
    "https://openalex.org/W2145665826",
    "https://openalex.org/W2141766660",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3104080943",
    "https://openalex.org/W112197792",
    "https://openalex.org/W3105391665",
    "https://openalex.org/W4256366562",
    "https://openalex.org/W2062877045",
    "https://openalex.org/W2319003035",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W2102998034",
    "https://openalex.org/W2071936644",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W2990881710",
    "https://openalex.org/W2807032982"
  ],
  "abstract": "The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models — excelling at a wide range of NLP tasks — to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14418–14446\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEvaluating Subjective Cognitive Appraisals of Emotions\nfrom Large Language Models\nHongli Zhan1 Desmond C. Ong2 Junyi Jessy Li1\n1Department of Linguistics, The University of Texas at Austin\n2Department of Psychology, The University of Texas at Austin\n{honglizhan, desmond.ong, jessy}@utexas.edu\nAbstract\nThe emotions we experience involve complex\nprocesses; besides physiological aspects, re-\nsearch in psychology has studied cognitive\nappraisals where people assess their situa-\ntions subjectively, according to their own val-\nues (Scherer, 2005). Thus, the same situa-\ntion can often result in different emotional\nexperiences. While the detection of emotion\nis a well-established task, there is very lim-\nited work so far on the automatic prediction\nof cognitive appraisals. This work fills the\ngap by presenting COVID ET-APPRAISALS , the\nmost comprehensive dataset to-date that as-\nsesses 24 appraisal dimensions, each with a\nnatural language rationale, across 241 Reddit\nposts. COVID ET-APPRAISALS presents an\nideal testbed to evaluate the ability of large\nlanguage models — excelling at a wide range\nof NLP tasks — to automatically assess and\nexplain cognitive appraisals. We found that\nwhile the best models are performant, open-\nsourced LLMs fall short at this task, present-\ning a new challenge in the future develop-\nment of emotionally intelligent models. We\nrelease our dataset at https://github.com/\nhonglizhan/CovidET-Appraisals-Public.\n1 Introduction\nEmotions constitute a crucial aspect of people’s\nlives, and understanding them has a profound im-\npact on improving public mental health problems as\nwell as policy-making (Choudhury and De, 2014;\nGjurkovi´c and Šnajder, 2018; Arora et al., 2021;\nUban et al., 2021). The emotions we experience\ninvolve complex processes: the same situation\ncan often result in different emotional experiences,\nbased on an individual’s subjective evaluations.\nThese are called cognitive appraisals, and have\nbeen extensively studied in psychology through\ntheoretical, behavioral, and hand-coded studies\n(Arnold, 1960; Lazarus, 1966; Lazarus et al., 1980;\nRoseman, 1984; Scherer et al., 1984; Smith and\n             I've recently heard of stories that people who are getting mildly sick\n             aųer being vaccinated are still coming down with long covid. People on\n             the covid long hauler subreddit are saying that we're going to have a \nhuge problem on our hands because no vaccine protects against long covid… \nThis isn't going to end, isn't it? Right as I think I've tasted freedom, I ůnd out \ninformation that makes me want to stay inside forever like a hermit...\n[Scale]: 1 → [Rationale]: The narrator does not talk about \nthemselves in relation to the COVID situation in a way that \nmakes it seem like they believe they are responsible.\n1. To what extent did the narrator think that THEY \nwere responsible for causing the situation?\n[Scale]: 2 → [Rationale]: The narrator mentions how many \npeople who are vaccinated and get mildly sick still contract \nlong COVID, but they do not talk about them in a way that \nseems like they are blaming them for the situation.\n2. To what extent did the narrator think that OTHER \nPEOPLE were responsible for causing the situation?\n…\n[Scale]: 3 → [Rationale]: The narrator really didn't expect \nthis situation since they mention being able to taste \nfreedom, believing the pandemic is ending, when suddenly \nthey heard news that vaccinated people are still getting \nlong covid and now they think the pandemic will never end.\n24. To what extent did the narrator EXPECT the situation to occur?\n[Scale]: 9 → [Rationale]: The narrator mentions long COVID and the \ninability to currently protect against long COVID. This is a factor that \nis still outside of anyone's control, including doctors and nurses, and \nthe situation continues to evolve in uncontrollable ways.\n3. To what extent did the narrator think that CIRCUMSTANCES BEYOND \nANYONE’S CONTROL were responsible for causing the situation?\nFigure 1: An example from COVID ET-APPRAISALS .\nThe fact that the narrator is blaming nobody but cir-\ncumstances beyond anyone’s control for causing long-\nCOVID contributes to their feeling of sadness. We\nshowcase an annotation together with LLMs’ responses\nin Appendix §A.\nEllsworth, 1985; Weiner, 1985; Clore and Ortony,\n2000; Roseman and Smith, 2001; Scherer et al.,\n2001; Ellsworth and Scherer, 2003; Sander et al.,\n2005; Ong et al., 2015, 2019; Ortony et al., 2022;\nYeo and Ong, 2023). For instance, being fired from\na job, if judged to be due to one’s own controllable\nmistakes, could result in regret; if evaluated to be\nunfair and due to someone else’s intentional actions,\nwould make one feel angry; and if appraised to be\nleaving a toxic work environment, could instead re-\nsult in relief and even happiness. The different di-\nmensions along which people subjectively inter-\n14418\npret or appraise the situation characterizes the\nspecific emotions they feel (Moors et al., 2013).\nAlthough emotion detection is a well-established\nNLP task (Strapparava and Mihalcea, 2007; Mi-\nhalcea and Strapparava, 2012; Wang et al., 2012;\nLei et al., 2014; Abdul-Mageed and Ungar, 2017;\nKhanpour and Caragea, 2018; Liu et al., 2019;\nSosea and Caragea, 2020; Demszky et al., 2020;\nDesai et al., 2020; Sosea et al., 2022), it mostly\ninvolves classification from text to emotion labels\ndirectly, skipping the appraisal step that is neces-\nsary to interpret why the emotion is experienced by\nan individual in a particular event. Hence, we do\nnot yet have a data-driven understanding of these\ncognitive appraisals in textual data. Yet recent work\nhas started to show its necessity: Hofmann et al.\n(2020) showed that appraisals are informative for\nan emotion detection model; Zhan et al. (2022)\nfurther recognized appraisals to be an integral part\nof emotion triggers, though appraisals were not\nexplicit in their work.\nThis work aims at construing an empirical, ex-\nplicit understanding of perceived cognitive ap-\npraisals in human readers and large language mod-\nels (LLMs) alike, via a comprehensive 24 di-\nmensions, along with their corresponding natural\nlanguage rationales. A language model’s capa-\nbility of assessing cognitive appraisals reflects a\nmore nuanced understanding of emotions, where\nit could contextualize individual subjectivity in re-\nsponses to the same situation, while offering ex-\nplanations (“they are feeling [emotion] because of\n[appraisal]”). This could be groundwork for emo-\ntional support agents, e.g., one capable of positive\nreframing (Ziems et al., 2022) or producing empa-\nthetic responses.\nWe first introduce COVID ET-APPRAISALS , a\ndataset of 24 appraisal dimensions annotated across\n241 Reddit posts sourced from Zhan et al. (2022)\nabout COVID-19. Each post was manually anno-\ntated with 24 appraisal dimensions from a recent\nmeta-analysis covering all appraisal dimensions\nproposed and studied in the literature (Yeo and Ong,\n2023). For each appraisal dimension, annotators\nnot only rated the extent to which they perceived\nthe narrator is experiencing the said dimension, but\nalso provided a rationale in their own language\nto justify their rating selection. An example from\nCOVID ET-APPRAISALS is shown in Figure 1.\nCOVID ET-APPRAISALS serves as an ideal\ntestbed to evaluate the capability of a model to un-\ncover implicit information for emotion understand-\ning. Benchmarking on COVID ET-APPRAISALS ,\nwe evaluate the performance of LLMs to (1) pro-\nvide Likert-scale ratings for the appraisal dimen-\nsions; and (2) generate natural language rationales\nfor their ratings. The elicitation of the rationales\ncan be seen as a way of probing (Le Scao and Rush,\n2021; Gu et al., 2022), where we prefix a question\nwith an elaborated situation. We evaluate a range of\nLLMs, including ChatGPT, Flan-T5 (Chung et al.,\n2022), Alpaca (Taori et al., 2023), Dolly (Conover\net al., 2023). With an extensive human evalua-\ntion of the natural language rationales from LLMs\nas well as our annotators, we find that ChatGPT\nperforms on par with (and in some cases better\nthan) human-annotated data; this opens a new av-\nenue of investigation to improve its performance\non emotion-related tasks (Koco´n et al., 2023). In\ncomparison, other open-sourced LLMs fall short\non this task, presenting a new challenge in the fu-\nture development of emotionally intelligent open\nmodels.\nWe publicly release our annotated dataset\nCOVID ET-APPRAISALS , model outputs, and our\nhuman evaluation data at https://github.com/\nhonglizhan/CovidET-Appraisals-Public.\n2 Background and Related Work\nCognitive Appraisal Theories. The cognitive ap-\npraisal theories of emotion state that emotions arise\nfrom an individual’s subjective understanding and\ninterpretation of situations that hold personal im-\nportance for their overall well-being (Arnold, 1960;\nLazarus, 1966; Lazarus et al., 1980; Roseman,\n1984; Scherer et al., 1984; Smith and Ellsworth,\n1985; Weiner, 1985; Clore and Ortony, 2000; Rose-\nman and Smith, 2001; Scherer et al., 2001; Sander\net al., 2005; Ortony et al., 2022). In practical terms,\npeople interpret and appraise situations along a\nrange of different dimensions, and it is the specific\nmanner in which they appraise their situations that\ngive rise to the distinct emotions they experience.\nThe primary focus of cognitive appraisal theories\nof emotions revolves around the identification of\nthese appraisal dimensions that are associated with\nspecific emotional experiences and how these di-\nmensions contribute to distinguishing between dif-\nferent emotional states (Lazarus, 1993; Roseman,\n1996; Scherer et al., 2001; Moors, 2010; Scherer\nand Moors, 2019).\nWhile appraisal theorists agree on the impor-\n14419\ntance of motivationally-relevant appraisals in trig-\ngering emotions, they have not reached a consensus\non the specific appraisal dimensions that play a sig-\nnificant role in this process (Yeo and Ong, 2023).\nVarious theories have put forth distinct sets of ap-\npraisal dimensions that are considered crucial in\ntriggering and distinguishing emotions. From prior\nliterature, Yeo and Ong (2023) identified and as-\nsembled a taxonomy of all appraisal dimensions\nthat have been studied, and produced a condensed\nlist of 24 cognitive appraisal dimensions which we\nfocus on in this paper.\nCognitive Appraisals in NLP. Appraisals pro-\nvide the necessary computational structure allow-\ning for the distillation of real-life situations that\ndepend on a multitude of factors into a (large but)\nfinite set of appraisal dimensions (Ong et al., 2015).\nDespite its importance, however, few works have\nexplored the implications of cognitive appraisals\non emotions in NLP. Hofmann et al. (2020) ex-\nperimented with a small set of cognitive appraisal\ndimensions (including attention, certainty, effort,\npleasantness, responsibility, control, and circum-\nstance) to assist the automatic detection of emo-\ntions in text, and found that accurate predictions of\nappraisal dimensions boost emotion classification\nperformance. They introduced a dataset of 1,001\nsentences following the template “I feel [emotion],\nwhen ...” (average sentence length: 27 tokens). In\ncomparison, our work covers a much wider range\nof 24 appraisal dimensions found in prior literature,\nover lengthy (176 tokens on average) Reddit posts\nthat were natural and emotionally charged. We also\ncollect natural language rationales as a key contri-\nbution to reveal human’s in-depth understanding of\nsuch cognitive appraisals in context.\nRecent studies (Zhan et al., 2022; Sosea et al.,\n2023) acknowledged both what happened and how\none appraised the situation as inherent compo-\nnents of emotion triggers, although the appraisal\nof events was not explicit in their work. Instead\nwe provide datasets and perform evaluation on ap-\npraisals explicitly, such that language models can\nbuild on this work to achieve a comprehensive and\nexplicit understanding of cognitive appraisals from\nwritten text.\nLLMs on Emotion-Related Tasks. Autoregres-\nsive LLMs have been explored extensively in\nemotion-related tasks such as sentiment analysis\n(Zhong et al., 2023; Qin et al., 2023; Susnjak,\n2023), emotion recognition (Koco ´n et al., 2023),\ndisclosing the representation of human emotions\nencapsulated in LLMs (Li et al., 2023), and inter-\npreting mental health analysis (Yang et al., 2023).\nHowever, few have tapped into the understand-\ning of cognitive appraisals of emotions innate in\nLLMs. In this work, we dive into the extent to\nwhich LLMs comprehend the profound cognitive\nappraisals underlying emotions in situations, and\nfurther elicit natural language rationales from the\nlanguage models to disclose the reason behind such\npredictions from the otherwise baffling black-box\nLLMs (Gilpin et al., 2018). Aligning with Maraso-\nvi´c et al. (2020) who performed human evaluation\non rationales generated by GPT, we additionally\nperform an in-depth human evaluation of the ratio-\nnales from human annotators and LLMs alike on\nthe novel task of providing natural language expla-\nnations for cognitive appraisals of situations that\nunderlie narrators’ emotional experiences.\n3 The COVID ET-APPRAISALS Dataset\nCOVID ET-APPRAISALS contains 241 Reddit posts\nsampled from the COVID ET dataset (Zhan et al.,\n2022), where the Reddit posts are sourced from\nr/COVID19_support. Each post is manually anno-\ntated with one or more of the 7 emotions: anger,\nanticipation, joy, trust, fear, sadness, and disgust.\nThe 241 posts in COVID ET-APPRAISALS have an\naverage of 175.82 tokens and 2.67 emotions per\npost. From Yeo and Ong (2023)’s work, we iden-\ntify 24 cognitive emotion appraisal dimensions (Ta-\nble 1). We provide the instructions given to the\nannotators (including the full questions for each of\nthese 24 dimensions) in Appendix §B.\nAnnotators. We recruited 2 linguistics students\nat a university to work on our annotation task; both\nof them are native speakers of English. Both an-\nnotators underwent training using a set of posts\nalready annotated by our group. Throughout the\nannotation, we monitored the inter-annotator agree-\nment and provided feedback on their work.\nInstructions. Given a Reddit post from\nCOVID ET, annotators are asked to judge 24\nemotion appraisal dimensions pertaining to how\nthe narrator feels about and views the situation that\nthey are going through (e.g., whether the narrator\nfeels the situation they are in is something they\ncould control). For each appraisal dimension,\nannotators need to select a Likert rating on the\n14420\nID Abbrv. Reader-Friendly Labels\n1 srsp Self-responsibility\n2 orsp Other-responsibility\n3 crsp Circumstances-responsibility\n4 pfc Problem-focused coping\n5 grlv Goal Relevance\n6 attn Attentional activity\n7 efc Emotion-focused coping\n8 scrl Self-Controllable\n9 ocrl Other-Controllable\n10 ccrl Circumstances-Controllable\n11 prd Predictability\n12 thr Threat\n13 pls Pleasantness\n14 crt Certainty\n15 gcnd Goal Conduciveness\n16 fair Fairness\n17 fex Future expectancy\n18 csn Consistency with social norms\n19 loss Loss\n20 fml Familiarity\n21 eff Effort\n22 chl Challenge\n23 civ Consistency with internal values\n24 exp Expectedness\nTable 1: The 24 appraisal dimensions and their abbrevi-\nations we used throughout this paper. See Appendix §B\nfor full questions for each dimension, and Figure 1 for\nan example of how the items for 1: self-responsibility,\n2: other-responsibility, 3: circumstances-responsibility,\nand 24: expectedness were framed.\nciv\nfair\ncsn\nprd\nfex\neff\nexp\nfml\ngrlv\nocrl\nefc\norsp\ngcnd\nchl\nloss\nattn\nthr\nscrl\ncrsp\npls\nccrl\ncrt\nsrsp\npfc\nDimensions\n0\n20\n40\n60\n80Percentage of NA values\nNA Values Percentage in Each Dimension\nFigure 2: Percentage of “not mentioned” labels in each\ndimension in COVID ET-APPRAISALS .\nscales of 1 to 9. A “ not mentioned” (NA) option\nis provided in case the dimension being asked is\nabsent in the given post. In addition, we also ask\nthe annotators to provide rationales for their ratings\nin the form of natural language explanations.\nOn average, our trained annotators spent around\n30 minutes to complete the annotation of one post.\nOwing to the immense effort involved, we dou-\nbly annotate 40 posts to measure inter-annotator\nagreement while leaving the rest annotated by one\nannotator.\nPost-Processing and Aggregation. Given a\nfixed topic (COVID-19 in our case), it is highly\nlikely that certain dimensions frequently don’t ap-\nply (Yeo and Ong, 2023). This can be seen in\nFigure 2 which plots the percentage of NA labels:\ndimensions such as civ (consistency with internal\nvalues), fair (fairness), and csn (consistency with\nsocial norms) contain mostly NA labels (around\n80%). Therefore, we remove these dimensions\nfrom subsequent analyses and evaluations of the\ndataset. This results in a total of 21 applicable\nappraisal dimensions in COVID ET-APPRAISALS .\nWe collected 241 posts in total. For the subset of\n40 posts that are doubly annotated, we aggregate\nthe Likert-scale ratings by taking the mean of each\npost’s ratings for each appraisal dimension (if an\nannotator labels a dimension as NA, we then ex-\nclude the particular dimension of that post that they\nannotate). In terms of the rationales, we consider\nboth rationales as ground truth references and use\nmulti-reference metrics in our experiments.\nInter-Annotator Agreement. We report inter-\nannotator agreement on the Likert-scale ratings.\nSince there is no reliable, automatic way to evaluate\nnatural language rationales (as discussed in §4), we\nevaluate them with human validation in §7.2.\nTo measure the agreement for selecting the NA\nlabel, we average the Fleiss’ Kappa values (Fleiss,\n1971; Randolph, 2005) across all 24 appraisal di-\nmensions, yielding a value of 0.769 indicating sub-\nstantial agreement (Artstein and Poesio, 2008).\nFor the 1-9 Likert-scale ratings, we report on\nthe 21 applicable dimensions: (1) Spearman’s ρbe-\ntween our two annotators, calculated per dimension\nthen averaged across all dimensions; (2) Krippen-\ndorff’s alpha (using interval distance) (Krippen-\ndorff, 1980); and (3) mean absolute difference (abs.\ndelta). Here the agreement is calculated if neither\nannotator gave a NA judgment. Krippendorff’s\nalpha yields a value of 0.647 indicating substan-\ntial agreement (Artstein and Poesio, 2008). The\naverage Spearman’s correlation is 0.497 with sig-\nnificance, and the absolute delta values also have\na small mean of 1.734. These measures indicate\nthat while the task is subjective, annotators do align\nwith each other with only a small difference com-\npared to the scale of ratings (1-9). Agreement val-\nues differ by dimension, which we showcase in\nAppendix C.\n4 Dataset Analysis\nHow do the scales distribute across dimensions\nand emotions? The distribution of the Likert-\nscale ratings is shown in Figure 3. The rat-\n14421\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\n1\n2\n3\n4\n5\n6\n7\n8\n9Scale\nFigure 3: Distribution of the ratings for each dimension.\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\nAGR\nFER\nTRS\nDSG\nJOY\nSDN\nANC 2\n4\n6\n8\nFigure 4: Mean Likert-scale ratings for each dimension\nin each emotion.\nings for some dimensions are consistent (e.g., di-\nmensions crsp (circumstances-responsibility), ccrl\n(circumstances-controllable), and chl (challenge)),\nwhereas for some other dimensions, the ratings\nhave higher variance (e.g., dimensions ocrl (other-\ncontrollable) and loss).\nWe analyze the connections between our Likert-\nscale annotations and COVID ET’s emotion annota-\ntions. Figure 4 shows the mean Likert-scale rating\nfor each dimension within each post with respect\nto the perceived emotion. While it is evident that\nmost dimensions show consistency (the posts are\nall related to COVID-19), some emotions stand out\ndistinctly in particular dimensions. For example,\ntrust and joy have higher Likert-scale ratings on\ndimensions pfc (problem-focused coping) and gcnd\n(goal conduciveness) compared to other emotions,\nsuggesting the inter-correlation between these ap-\npraisal dimensions with positive emotions. We\nfurther explore whether appraisal dimensions alone\nare indicative of perceived emotions already anno-\ntated in COVID ET in Appendix §D.1.\nWhat are the characteristics of the natural lan-\nguage rationales? On average, each rationale\nis 1.2 sentences (std.dev = 0.4) and 28.9 tokens\n(std.dev = 10.0) long. Following Marfurt and Hen-\nderson (2021), we also measure the abstractiveness\nof the rationales from our human annotators by\ncalculating the percentage of novel bigrams in the\nrationales with respect to the Reddit posts and in-\nRATIONALE\nBLEU-4 ROUGE-L BERTS C\nANNOTATORS 0.042 0.253 0.357\nBASELINE -P 0.060 0.261 0.336\nBASELINE -D 0.059 0.247 0.332\nTable 2: Automatic measures of similarity on the nat-\nural language rationales of COVID ET-APPRAISALS .\nBASELINE -P denotes “baseline (same dimension, differ-\nent posts)”, and BASELINE -D denotes “baseline (same\npost, different dimensions)”.\nstructions (i.e., evaluating a specific appraisal di-\nmension) that the annotators were given. As shown\nin Table 4, our human annotators attain a % of\nnovel bigrams of 86.7%, indicating a high abstrac-\ntiveness. We showcase the most prominent topics\nextracted from the annotated rationales using La-\ntent Dirichlet Allocation (LDA) (Blei et al., 2003)\nin Appendix §D.2.\nAre rationales repetitive? We also look into\nautomatic measures of similarity to assess how\nmuch rationales from different annotators, or from\ndifferent dimensions/posts, differ from one an-\nother. Specifically, we calculate BLEU-4 (Papineni\net al., 2002), ROUGE-L (Lin, 2004), and re-scaled\nBERTScore (Zhang et al., 2019) between our two\nannotators’ rationales. We establish 2 random base-\nlines for comparison: (1) rationales of the same\ndimension from different posts; (2) rationales from\ndifferent dimensions within the same post. In each\ncase we report similarity between 3 randomly sam-\npled rationales and the annotated ones.\nTable 2 shows that the textual similarity in\nall conditions are somewhat low; the BLEU and\nROUGE scores show that there is very little lex-\nical overlap, although BERTScore shows higher\nsemantic similarity between two annotators for the\nsame dimension within the same post. Upon closer\ninspection, we observe that these commonly used\nautomatic measures do not adequately capture se-\nmantic similarity in our dataset (see Appendix §D.3\nfor an example). This adds to the challenge of eval-\nuating rationales; as a result, we resort to the human\nevaluation in §7.2.\n5 Can LLMs understand emotional\nappraisals?\nCOVID ET-APPRAISALS provides an ideal testbed\nthat evaluates models’ performance on predict-\ning both the Likert ratings, as well as their nat-\n14422\nural language explanations. Using COVID ET-\nAPPRAISALS , we evaluate the zero-shot perfor-\nmance of LLMs in an attempt to evaluate their\ninnate ability to comprehend emotional appraisals\nfrom social media text without in-context learning.\nModels. We evaluate the following instruction-\ntuned LLMs1: 1) ChatGPT, i.e., GPT-3.5-Turbo;\n2) FLAN-T5-XXL (11B) (Chung et al., 2022),\nwhich is the instruction fine-tuned version of T5\n(Raffel et al., 2020); 3) Alpaca (7B, 13B) (Taori\net al., 2023) is fine-tuned from LLaMA (7B and\n13B) (Touvron et al., 2023) on 52K instruction-\nfollowing examples created with GPT text-davinci-\n003 in the manner of self-instruct (Wang et al.,\n2022); 4) Dolly-V2 (7B, 12B) (Conover et al.,\n2023) is an instruction-tuned LLM trained on ~15k\ndemonstrations consisting of both instructions and\nresponses.\nPrompts and Setup. The templates for prompt-\ning the LLMs are shown in Appendix Figure 17.\nAfter extensive experimentation, we found that\nonly ChatGPT is able to generate both a rating\nand a rationale with a single prompt; this type of\n“1-step” prompting leads to ill-formed responses\nfor other models. Thus, for models other than Chat-\nGPT, we instead use a pipeline or “2-step” prompt-\ning similar to the strategy used in Press et al. (2022):\nwe first elicit the rating for the appraisal dimension,\nthen conditioned on the response for the rating we\nfurther elicit the rationale for the selection.\nWe carry out all our experiments on 4 Nvidia\nA40 GPUs. We use the HuggingFace Transformers\n(Wolf et al., 2020) library for model inference. We\nset the temperature value of all models to 0.1.2 To\nenable a fair comparison of models, we sample\nfrom the LLMs five times with different model\ninitializations and report average values for both\nscales and rationales.\n6 Evaluation: Likert-Scale Ratings\nWe report model performance for Likert-scale rat-\nings on the 21 applicable dimensions using two\n1While we have also experimented with non-instruction-\ntuned LLMs (including GPT-3 davinci and LLaMA (7B and\n13B), they largely fail to generate sensible outputs for this task.\nWe showcase examples of responses from non-instruction-\ntuned models in Appendix §A. For these reasons, we do not\ninclude their results in this paper.\n2We experimented with higher temperatures on a validation\nset consisting of10 Reddit posts annotated by our group which\nare not included in COVID ET-APPRAISALS , and the models\nyielded worse and more unstable performance.\nSCALE NA\nMAE SPEARMAN ’S ρ F1\nCHATGPT 1.694 0.388†† 0.918\nFLAN -T5 3.266 0.225† 0.852\nALPACA -7B 2.353 0.081 0.918\nALPACA -13 B 3.872 −0.035 0.602\nDOLLY-7B 2.812 −0.013 0.645\nDOLLY-12 B 2.747 0.022 0.711\nTable 3: Experiment results from LLMs. † indicates\np < 0.1 for Spearman correlation, and †† indicates\np <0.05. In addition, we also provide the results of\nthe F1 score on measuring the agreement between the\nmodels’ ratings and the gold ratings for selecting the\n“not mentioned” label across all 24 dimensions.\nstandard regression metrics: Mean Absolute Error\n(MAE) and Spearman’s correlation. We treat the\nselection of the NA labels as a binary classification\ntask and report F1 measures across all 24 dimen-\nsions. For the 40 gold examples that were doubly\nannotated by human annotators, we consider a di-\nmension as NA when both annotators select the\nlabel.\nResults. To evaluate the performance, we clean\nthe responses elicited from the LLMs. Specifi-\ncally, we use regular expressions to extract the first\nnumeric value ranging from 1-9 from the scale re-\nsponses3. The results of the models’ performance\nare shown in Table 3. We showcase examples of\nthe models’ responses in Appendix §A. Additional\nanalyses of the LLMs’ responses are shown in Ap-\npendix §G.\nFor the NA labels (Table 3, right), ChatGPT and\nAlpaca-7B score the highest with an F1 of 0.918.\nIn general, the average performance across the lan-\nguage models we evaluate is 0.774 for F1, indi-\ncating these models are performant at predicting\nwhether a dimension applies.\nFor the Likert-rating predictions, results show\nthat ChatGPT-3.5 consistently yields the highest\nperformance compared to the other language mod-\nels, with a significant Spearman’s correlation of\n0.388 and an MAE of 1.694. We note that FLAN-\nT5-XXL is the second best-performing model. Al-\npaca and Dolly perform poorly on our task, with\nnegative correlations with the gold labels4. Inter-\n3For example, one of Alpaca-7B’s scale responses is “The\nnarrator thought that Circumstances Beyond Anyone’s Control\nwere responsible for causing the situation to a moderate extent\n(4 on a scale of 1-9).</s>” . After cleaning, the response is\nformatted to “4”.\n4As shown in Appendix Figure 9, the ratings generated by\n14423\nLENGTH ABSTRACTIVENESS AUTO EVAL HUMAN EVAL\n# TOKENS %NOVEL BIGRAMS BLEU-4 ROUGE-L BERTSC FAC REL JUS USE\nANNOTATORS 28.9 86.7% —— 0.73 0.88 0.95 0.72\nCHATGPT 58.0 81.8% 0.044 0.224 0.347 0.84 0.88 0.93 0.85\nFLAN-T5 45.3 16.0% 0.008 0.066 0.053 0.40 0.29 0.24 0.13\nALPACA-7B 48.6 71.9% 0.040 0.230 0.297 0.55 0.82 0.82 0.51\nTable 4: Experiment results from LLMs. Additional evaluations of all language models (including Alpaca-13B,\nDolly-7B, and Dolly-12B) are provided in Table 11. A more comprehensive report of the automatic metrics\nBLEU-4, ROUGE-L, and BERTS CORE is provided in Table 9, Appendix §F.\nestingly, we notice a drop in performance when the\nsize of the model parameters increases for Alpaca.\nThe results highlight the challenging nature of our\ntask, and the gap between open-sourced LLMs vs.\nChatGPT (Gudibande et al., 2023).\nAdditionally, we also measure the systems’ per-\nformance on all 24 appraisal dimensions, including\nthe 3 appraisal dimensions where the NA rates are\naround 80%. Results revealed marginal change in\nperformance across all LLMs. For most LLMs the\nperformance dropped as expected: measured with\nSpearman’s ρ, ChatGPT-3.5 (↓0.018), Alpaca-7B\n(↓0.008), and Dolly-12B (↓0.007). On the other\nhand, the performance of FLAN-T5 ( ↑ 0.005),\nAlpaca-13B ( ↑ 0.027), and Dolly-7B ( ↑ 0.020)\nincreased.\n7 Evaluation: Rationales\nAs rationalizing emotional appraisals with natural\nlanguage is a novel task, we perform both auto-\nmatic (§7.1) and human evaluation (§7.2).\n7.1 Automatic Evaluation\nWe use commonly used automatic reference-based\nmetrics including BLEU (Papineni et al., 2002),\nROUGE (Lin, 2004) and BERTScore (Zhang et al.,\n2019), comparing generated rationales vs. anno-\ntated ones (in a multi-reference fashion).\nResults. Similar to the performance in selecting\nLikert-scale ratings, ChatGPT remains the best-\nperforming language model in providing natural\nlanguage rationales (Table 4). The values Chat-\nGPT achieves are lower than, though compara-\nble to, those between different rationales from our\ntwo annotators. Alpaca-7B also achieves compa-\nrable performance in these automatic measures,\nthe language models (specifically, Alpaca-7B and Dolly-12B)\nfor some of the dimensions lack variance (i.e., they gave a\nconstant rating for certain appraisal dimensions). Therefore,\nthe Spearman correlation is set to zero in these dimensions,\nindicating no correlation.\ndespite its relatively poor capability in terms of\nselecting Likert-scale ratings. We note that FLAN-\nT5 lags behind considerably compared to Chat-\nGPT and Alpaca-7B. We provide the additional\nauto-evaluation statistics for other LLMs includ-\ning Dolly-7B, Dolly-12B, and Alpaca-13B in Ap-\npendix Table 11.\nHow long and how abstractive are the rationales\ngenerated by LLMs? In addition, we also mea-\nsure the length and abstractiveness of the rationales\ngenerated by LLMs. Following the setup in §4, we\nevaluate abstractiveness using % of novel bigrams,\ncomparing LLMs’ generated rationales against the\nReddit posts as well as the prompts (i.e., evaluating\na specific appraisal dimension) they were given. As\nshown in Table 4, rationales generated by LLMs\nare at least 1.5x longer than those provided by our\nannotators, with ChatGPT being the most verbose.\nThe LLMs also provide rationales that are more\nextractive compared to our annotators, with FLAN-\nT5 being the most extractive.\n7.2 Human Evaluation\nData. Because the natural language rationales are\nexplanations for a particular rating, we only eval-\nuate and analyze LLM-generated rationales when\nthe model made a near-correct prediction of the\nLikert-scale rating for that particular dimension\ncompared against the gold human ratings. Specifi-\ncally, we sample the intersection of (post, dimen-\nsion) tuples where the 3 best-performing LLMs’\n(i.e., ChatGPT, FLAN-T5, and Alpaca-7B) ratings\nfall in the range of an absolute difference of 1 to\none of the annotated scale-ratings. In cases where\nthere are 2 gold annotations for a particular dimen-\nsion, both are evaluated. In Appendix §F we also\nshow the human evaluation of rationales for such\nintersection of all LLMs. We additionally evaluate\nhuman-written rationales as well, and we mix\nthose (in random order) with LLMs’ responses.\nThe above desiderata results in an evaluation\n14424\nof 108 rationales annotated by human annotators\nand 65 natural language rationales from each LLM.\nThe evaluation covers 19 out of the 21 applicable\ndimensions (no such overlap is found for dimen-\nsions crsp (circumstances-responsibility) and pls\n(pleasantness)). Moreover, we make sure that there\nare no ground truth labels annotated by the human\nannotators in which the rating is NA.\nInstructions. Given a Reddit post and the scale\nprovided by the human annotators or the LLM\n(blinded to the annotators), annotators are asked\nto judge the rationales pertaining to the emotion\nappraisal dimension regarding the post as well as\nthe stated scale. The rationales are distributed to\nannotators at random. We evaluate the natural lan-\nguage rationales based on the following criteria. In\nAppendix §H, We provide the detailed instructions\nand examples given to the annotators, together with\nthe layout of the human evaluation task.\n1) Factuality: For the rationale, the model may\nnot generate something that is factual: sometimes\nit generates rationales for the sole purpose of jus-\ntifying its answer (Ye and Durrett, 2022). There-\nfore, we include the aspect of hallucination and\nfactuality as one of our evaluation criteria, and ask\nevaluators whether the rationale faithfully reflects\nwhat’s stated in the post. Options of “Yes”, “Minor\nError”, and “No” are provided.\n2) Relevance: We evaluate whether the rationale\ndirectly addresses the specific appraisal dimension\nquestion that is being asked about the post. We\nask evaluators on a Likert-scale of 1 to 5, with 1\nbeing “least relevant” and 5 being “most relevant”,\nwhether the rationale focuses on the specific aspect\nof the post that is being appraised, and whether it\nstrays off-topic or provides irrelevant information.\n3) Justification : We ask human evaluators\nwhether the rationale justifies the selected scale\nby adequately explaining why the selected rating\nscale is the most appropriate or relevant one to use\nfor the aspect being evaluated. Annotators need to\nselect either “Yes” or “No”.\n4) Usefulness: Finally, we evaluate whether the\nrationale provides useful or informative insights\nor explanations of useful information pertaining to\nthe appraisal dimension being judged. Options of\n“Yes”, “Maybe”, and “No” can be selected.\nAnnotators. We recruit annotators from the\nAmazon Mechanical Turk (MTurk) to work on our\nhuman evaluation task. The crowd workers were\ninvolved in a pre-annotation qualification as well\nas training process before commencing the evalua-\ntion of the natural language rationales. We assign 2\ncrowd workers per natural language rationale eval-\nuation. We ensure that the crowd workers earn a\nminimum salary of $10 per hour.\nWe report the inter-evaluator agreement using\nKrippendorff’s Alpha with interval distance in Ta-\nble 5, showing substantial agreement (Artstein and\nPoesio, 2008) across all criteria.\nLabel Transformation. For the convenience of\nmeasuring inter-annotator agreement as well as in-\nterpreting the results, we convert the labels of each\ncriterion to numeric values within the range of 0 to\n1. Specifically, for criteria Factuality, Justification,\nand Usefulness, “Yes” is converted to 1, “Minor\nError/Maybe” to 0.5, and “ No” to 0. As for the\ncriterion Relevance which is judged on a 5-scale\nLikert rating, we map the Likert scale of 1 into 0, 2\ninto 0.25, 3 into 0.5, 4 into 0.75, and 5 into 1.\nResults. The result of the mean ratings for each\ncriterion from the human evaluation task is pro-\nvided in Table 4. We provide box plots of the\nratings as well as the human evaluation results for\nthe rationales from all 6 LLMs in Appendix §F.\nFrom Table 4 we observe that our human anno-\ntators and ChatGPT provide natural language ratio-\nnales of the highest quality among all models, ac-\ncording to human evaluators. Surprisingly, we find\nChatGPT performs on par with our human anno-\ntators, with (slightly) better performance in terms\nof factuality and usefulness. This can be attributed\nto the verbosity and extractiveness of ChatGPT (as\nshown in Table 4), especially in dimensions where\nthe scale rating is low. We showcase an example in\nAppendix §I.\nAlpaca-7B attains lower results compared to the\nother LLMs, especially in terms of the criteria fac-\ntuality and usefulness. FLAN-T5, on the other\nhand, ranks the worst on all criteria among the\nLLMs. Further analysis reveals that FLAN-T5 oc-\ncasionally generates responses for natural language\nrationales that are the same as its scale answers,\nresulting in irrelevant and useless rationales.\n8 Conclusion\nTo achieve a more accurate and holistic understand-\ning of emotions from written text, NLP models\nneed to work towards understanding the subjec-\ntive cognitive appraisals of emotions underlying\n14425\nFAC REL JUS USE\nEVALUATORS 0.590 0 .718 0 .576 0 .668\nTable 5: Inter-annotator agreement statistics for the\nhuman evaluation task, measured using Krippendorff’s\nAlpha with interval distance.\nsituations. In this work, we construe an empirical\nand explicit understanding of perceived cognitive\nappraisals in human readers and LLMs alike. We\npresent COVID ET-APPRAISALS , a dataset of 241\nReddit posts annotated with a comprehensive range\nof 24 subjective cognitive appraisals that follow\na situation, along with their corresponding natu-\nral language rationales. Experiments reveal that\nCOVID ET-APPRAISALS is a vital resource to eval-\nuate the capability of a language model to uncover\nimplicit information for emotional understanding.\nOur thorough evaluation of LLMs’ performance\non assessing emotion appraisal dimensions empha-\nsizes that COVID ET-APPRAISALS is a challenging\nbenchmark, and our in-depth human evaluation of\nthe natural language rationales indicates potential\nareas of improvement (e.g., improving the factual-\nity and usefulness of the rationales) for open-source\nLLMs.\nLimitations\nThis work presents a new dataset entitled\nCOVID ET-APPRAISALS to evaluate LLMs’ capa-\nbility in cognitive emotion appraisals. Due to the\nhighly demanding nature of our task (e.g., the same\nsituation can result in different subjective evalua-\ntions), COVID ET-APPRAISALS is annotated by 2\nannotators. Future work can explore a larger pool\nof annotators. Furthermore, it should be acknowl-\nedged that COVID ET-APPRAISALS is restricted\nto social media posts during the COVID-19 pan-\ndemic, and they are written in English solely. This\nmakes it challenging to evaluate LLMs’ ability in\nother domains as well as languages. Also, we note\nthe appraisals we collect are from the perceived\nend, which are not subjective appraisals from the\nnarrators and authors themselves.\nWe note that the size ofCOVID ET-APPRAISALS\nis relatively small. We have not intended this\ndataset to be one for supervised model training\nbut rather a very high-quality dataset for evaluation\n(since this is the first dataset of its kind). A key rea-\nson is that the collection of appraisal annotations is\nboth challenging and time-consuming: we have 24\ndimensions to analyze per post, and the annotation\nfor one post for one trained annotator takes half\nan hour. Future work may establish the validity\nof training data obtained from LLMs, and explore\napproaches such as distillation.\nIn addition, we experiment with LLMs under a\nzero-shot setup only, while we highlight that this is\nthe first work towards the assessment of cognitive\nappraisals of emotions in language models, and it\nlays the foundation for future research on decipher-\ning the intrinsic emotional dynamics that remain\nunexplored in current state-of-the-art models. We\nbelieve that this warrants a careful construction of\nthe dataset with thorough analysis; and we leave\nthese interesting engineering questions to future\nwork.\nAcknowledgements\nThis research was partially supported by National\nScience Foundation (NSF) grant IIS-2107524. We\nthank Kathryn Kazanas and Keziah Kaylyn Reina\nfor their dedication and hard work on the anno-\ntation of COVID ET-APPRAISALS . We also thank\nour reviewers for their insightful feedback and com-\nments.\nReferences\nMuhammad Abdul-Mageed and Lyle Ungar. 2017.\nEmoNet: Fine-grained emotion detection with gated\nrecurrent neural networks. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 718–728,\nVancouver, Canada. Association for Computational\nLinguistics.\nMagda B Arnold. 1960. Emotion and personality.\nColumbia University Press.\nAnshika Arora, Pinaki Chakraborty, M. P. S. Bhatia, and\nPrabhat Mittal. 2021. Role of Emotion in Excessive\nUse of Twitter During COVID-19 Imposed Lock-\ndown in India. Journal of Technology in Behavioral\nScience, 6(2):370–377.\nRon Artstein and Massimo Poesio. 2008. Survey article:\nInter-coder agreement for computational linguistics.\nComputational Linguistics, 34(4):555–596.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of Machine\nLearning Research, 3(Jan):993–1022.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2020. Evaluation of text generation: A survey. arXiv\npreprint arXiv:2006.14799.\n14426\nMunmun De Choudhury and Sushovan De. 2014. Men-\ntal health discourse on reddit: Self-disclosure, social\nsupport, and anonymity. In ICWSM.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nGerald L Clore and Andrew Ortony. 2000. Cognition\nin emotion: Always, sometimes, or never. Cognitive\nneuroscience of emotion, pages 24–61.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world’s first truly open instruction-\ntuned llm.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. GoEmotions: A dataset of fine-grained emo-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4040–4054, Online. Association for Computational\nLinguistics.\nShrey Desai, Cornelia Caragea, and Junyi Jessy Li. 2020.\nDetecting perceived emotions in hurricane disasters.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 5290–\n5305, Online. Association for Computational Lin-\nguistics.\nPhoebe C Ellsworth and Klaus R Scherer. 2003. Ap-\npraisal processes in emotion. Oxford University\nPress.\nJL Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378–382.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das,\nKaustubh Dhole, et al. 2021. The gem benchmark:\nNatural language generation, its evaluation and\nmetrics. In Proceedings of the 1st Workshop on\nNatural Language Generation, Evaluation, and\nMetrics (GEM 2021), pages 96–120.\nLeilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Ba-\njwa, Michael A. Specter, and Lalana Kagal. 2018. Ex-\nplaining explanations: An overview of interpretabil-\nity of machine learning. 2018 IEEE 5th International\nConference on Data Science and Advanced Analytics\n(DSAA), pages 80–89.\nMatej Gjurkovi´c and Jan Šnajder. 2018. Reddit: A gold\nmine for personality prediction. In Proceedings of\nthe Second Workshop on Computational Modeling\nof People’s Opinions, Personality, and Emotions in\nSocial Media, pages 87–97, New Orleans, Louisiana,\nUSA. Association for Computational Linguistics.\nYuling Gu, Bhavana Dalvi, and Peter Clark. 2022.\nDREAM: Improving situational QA by first elab-\norating the situation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1115–1127, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023. The false promise of imitating\nproprietary llms. arXiv preprint arXiv:2305.15717.\nJan Hofmann, Enrica Troiano, Kai Sassenberg, and Ro-\nman Klinger. 2020. Appraisal theories for emotion\nclassification in text. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 125–138, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nHamed Khanpour and Cornelia Caragea. 2018. Fine-\ngrained emotion detection in health-related online\nposts. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1160–1166, Brussels, Belgium. Association\nfor Computational Linguistics.\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, Anna Koco ´n, Bartłomiej Koptyra, Wik-\ntoria Mieleszczenko-Kowszewicz, Piotr Miłkowski,\nMarcin Oleksy, Maciej Piasecki, Łukasz Radli´nski,\nKonrad Wojtasik, Stanisław Wo´ zniak, and Prze-\nmysław Kazienko. 2023. ChatGPT: Jack of all trades,\nmaster of none. Information Fusion, 99:101861.\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, et al. 2023. Chatgpt: Jack of all trades,\nmaster of none. Information Fusion, page 101861.\nKlaus Krippendorff. 1980. Content analysis : an intro-\nduction to its methodology . Sage commtext series.\nSage Publications, Beverly Hills.\nRichard S Lazarus. 1966. Psychological stress and the\ncoping process. McGraw-Hill.\nRichard S Lazarus. 1993. From psychological stress\nto the emotions: A history of changing outlooks.\nAnnual review of psychology, 44(1):1–22.\nRichard S Lazarus, Allen D Kanner, and Susan Folkman.\n1980. Emotions: A cognitive–phenomenological\nanalysis. In Theories of emotion , pages 189–217.\nElsevier.\n14427\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2627–2636,\nOnline. Association for Computational Linguistics.\nJingsheng Lei, Yanghui Rao, Qing Li, Xiaojun Quan,\nand Liu Wenyin. 2014. Towards building a social\nemotion detection system for online news. Future\nGeneration Computer Systems, 37:438–448.\nMing Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng,\nXin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin,\nXiaozhi Wang, Zhiyuan Liu, and Dan Zhang. 2023.\nHuman emotion knowledge representation emerges\nin large language models and supports discrete emo-\ntion inference. arXiv preprint arXiv:2302.09582.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nChen Liu, Muhammad Osama, and Anderson De An-\ndrade. 2019. DENS: A dataset for multi-class emo-\ntion analysis. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 6293–6298, Hong Kong, China. Association\nfor Computational Linguistics.\nAna Marasovi´c, Chandra Bhagavatula, Jae sung Park,\nRonan Le Bras, Noah A. Smith, and Yejin Choi.\n2020. Natural language rationales with full-stack\nvisual reasoning: From pixels to semantic frames to\ncommonsense graphs. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n2810–2829, Online. Association for Computational\nLinguistics.\nAndreas Marfurt and James Henderson. 2021. Sentence-\nlevel planning for especially abstractive summariza-\ntion. In Proceedings of the Third Workshop on New\nFrontiers in Summarization, pages 1–14, Online and\nin Dominican Republic. Association for Computa-\ntional Linguistics.\nRada Mihalcea and Carlo Strapparava. 2012. Lyrics,\nmusic, and emotions. In Proceedings of the 2012\nJoint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural\nLanguage Learning, pages 590–599, Jeju Island, Ko-\nrea. Association for Computational Linguistics.\nAgnes Moors. 2010. Theories of emotion causation: A\nreview. Psychology Press.\nAgnes Moors, Phoebe C. Ellsworth, Klaus R. Scherer,\nand Nico H. Frijda. 2013. Appraisal theories of emo-\ntion: State of the art and future development. Emo-\ntion Review, 5(2):119–124.\nDesmond C. Ong, Zhengxuan Wu, Zhi-Xuan Tan, Mar-\nianne C. Reddan, Isabella Kahhalé, Alison Mattek,\nand Jamil Zaki. 2019. Modeling emotion in complex\nstories: The stanford emotional narratives dataset.\nIEEE Transactions on Affective Computing, 12:579–\n594.\nDesmond C. Ong, Jamil Zaki, and Noah D. Goodman.\n2015. Affective cognition: Exploring lay theories of\nemotion. Cognition, 143:141–162.\nAndrew Ortony, Gerald L Clore, and Allan Collins.\n2022. The cognitive structure of emotions . Cam-\nbridge university press.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. arXiv preprint arXiv:2210.03350.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nJustus J Randolph. 2005. Free-marginal multirater\nkappa (multirater k [free]): An alternative to fleiss’\nfixed-marginal multirater kappa. Online submission.\nIra J Roseman. 1984. Cognitive determinants of emo-\ntion: A structural theory. Review of personality &\nsocial psychology.\nIra J Roseman. 1996. Appraisal determinants of emo-\ntions: Constructing a more accurate and comprehen-\nsive theory. Cognition & Emotion, 10(3):241–278.\nIra J Roseman and Craig A Smith. 2001. Appraisal the-\nory. Appraisal processes in emotion: Theory, meth-\nods, research, pages 3–19.\nDavid Sander, Didier Grandjean, and Klaus R Scherer.\n2005. A systems approach to appraisal mechanisms\nin emotion. Neural networks, 18(4):317–352.\nKlaus R. Scherer. 2005. What are emotions? and how\ncan they be measured? Social Science Information,\n44:695 – 729.\nKlaus R Scherer and Agnes Moors. 2019. The emotion\nprocess: Event appraisal and component differentia-\ntion. Annual review of psychology, 70:719–745.\n14428\nKlaus R Scherer et al. 1984. On the nature and function\nof emotion: A component process approach. Ap-\nproaches to emotion, 2293(317):31.\nKR Scherer, A Schorr, and T Johnstone. 2001. Ap-\npraisal theory: Overview, assumptions, varieties.\nCraig A Smith and Phoebe C Ellsworth. 1985. Pat-\nterns of cognitive appraisal in emotion. Journal of\npersonality and social psychology, 48(4):813.\nTiberiu Sosea and Cornelia Caragea. 2020. Cancer-\nEmo: A dataset for fine-grained emotion detection.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8892–8904, Online. Association for Computa-\ntional Linguistics.\nTiberiu Sosea, Chau Pham, Alexander Tekle, Cornelia\nCaragea, and Junyi Jessy Li. 2022. Emotion analysis\nand detection during COVID-19. In Proceedings of\nthe Thirteenth Language Resources and Evaluation\nConference, pages 6938–6947, Marseille, France. Eu-\nropean Language Resources Association.\nTiberiu Sosea, Hongli Zhan, Junyi Jessy Li, and Cor-\nnelia Caragea. 2023. Unsupervised extractive sum-\nmarization of emotion triggers. In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n9550–9569, Toronto, Canada. Association for Com-\nputational Linguistics.\nCarlo Strapparava and Rada Mihalcea. 2007. SemEval-\n2007 task 14: Affective text. In Proceedings of the\nFourth International Workshop on Semantic Evalua-\ntions (SemEval-2007), pages 70–74, Prague, Czech\nRepublic. Association for Computational Linguistics.\nTeo Susnjak. 2023. Applying bert and chatgpt for senti-\nment analysis of lyme disease in scientific literature.\narXiv preprint arXiv:2302.06474.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAna-Sabina Uban, Berta Chulvi, and Paolo Rosso. 2021.\nAn emotion and cognitive based analysis of men-\ntal health disorders from social media data. Future\nGeneration Computer Systems, 124:480–494.\nWenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,\nand A. Sheth. 2012. Harnessing twitter “big data” for\nautomatic emotion identification. 2012 International\nConference on Privacy, Security, Risk and Trust and\n2012 International Confernece on Social Computing,\npages 587–592.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nBernard Weiner. 1985. An attributional theory of\nachievement motivation and emotion. Psychologi-\ncal review, 92(4):548.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nKailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie,\nand Sophia Ananiadou. 2023. Towards interpretable\nmental health analysis with large language models.\narXiv preprint arXiv:2304.03347.\nXi Ye and Greg Durrett. 2022. The unreliability of ex-\nplanations in few-shot prompting for textual reason-\ning. In Advances in Neural Information Processing\nSystems.\nGerard Yeo and Desmond C. Ong. 2023. A meta-\nanalytic review of the associations between cognitive\nappraisals and emotions in cognitive appraisal theory.\nPsyArXiv.\nHongli Zhan, Tiberiu Sosea, Cornelia Caragea, and\nJunyi Jessy Li. 2022. Why do you feel this way?\nsummarizing triggers of emotions in social media\nposts. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9436–9453, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with BERT. In International Confer-\nence on Learning Representations.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too?\na comparative study on chatgpt and fine-tuned bert.\narXiv preprint arXiv:2302.10198.\nCaleb Ziems, Minzhi Li, Anthony Zhang, and Diyi\nYang. 2022. Inducing positive perspectives with text\nreframing. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3682–3700, Dublin,\nIreland. Association for Computational Linguistics.\n14429\nA Dataset Example and LLM Responses\nIn Figure 10, Figure 11, and Figure 12, we show-\ncase an annotation from COVID ET-APPRAISALS\ntogether with LLMs’ responses regarding dimen-\nsion 3 crsp (circumstances-responsibility). In ad-\ndition to LLMs evaluated in this paper (including\nChatGPT, FLAN-T5-XXL, Alpaca (7B, 13B), and\nDolly-V2 (7B, 12B)), we also present responses\nelicited from other non-instruction-tuned models\nsuch as GPT-3-davinci (a vanilla base model of\nGPT-3) and LLaMA (7B, 13B) (Touvron et al.,\n2023) using the “2-step” prompting template given\nin Figure 17. As the example shows, these non-\ninstruction-tuned LLMs perform poorly on our task\nof cognitive emotion appraisal, generating nonsen-\nsical responses for both selecting Likert-scale rat-\nings as well as providing natural language ratio-\nnales.\nB Dataset Annotation Framework\nWe provide the instructions given to the annota-\ntors in Figure 13. In addition, we also provide the\nlayout for the annotation task (which includes the\nfull questions for each of the 24 cognitive emotion\nappraisal dimensions abbreviated in Table 1) in\nFigures 14, 15, 16.\nC Inter-Annotator Agreement by\nDimension in COVID ET-APPRAISALS\nTo better understand the inter-annotator agreement\npertaining to each emotion appraisal dimension in\nCOVID ET-APPRAISALS , we measure Spearman’s\nρand Krippendorff’s alpha on each of the 21 appli-\ncable dimensions. We provide the inter-annotator\nagreement statistics per dimension in Figure 5. As\nthe plot shows, the human annotators have strong\nagreement on dimensions such as efc (emotion-\nfocused coping) and pfc (problem-focused coping),\nwhilst disagreeing with each other most often on\ndimensions grlv (goal relevance), exp (expected-\nness), and loss. This can be attributed to the nature\nof our domain: in these Reddit posts, the narrator\nis mainly sharing their experiences in life around\nCOVID-19, while preserving doubts about the fu-\nture.\nefc\npfc\nsrsp\nccrl\npls\nocrl\nchl\norsp\nthr\neff\ngcnd\ncrsp\ncrt\nfex\nscrl\nprd\nattn\nfml\nloss\nexp\ngrlv\nDimensions\n0.0\n0.2\n0.4\n0.6\n0.8Spearman's ρ\nExpert Linguists' Inter-Annotator Agreement\n0.0\n0.2\n0.4\n0.6\nkrippendorff's Alpha\nFigure 5: Inter-annotator agreement of the Likert-scale\nratings within each dimension. The dimensions are\nranked by the order of Spearman’s ρ, and the colors in-\ndicate the inter-annotator agreement measured by Krip-\npendorff’s alpha using interval distance.\nAGR DSG FER JOY SDN TRS ANC AVG\nF1 0.18 0.13 0.40 0.26 0.29 0.06 0.23 0.22\nTable 6: F1 scores of each emotion using the trained\nlogistic regression model on the test set.\nD Additional Dataset Analyses\nD.1 Are the Dimensions Informative for\nEmotions?\nThe cognitive appraisal theories provide insights\ninto the nature of the appraisal dimensions in distin-\nguishing various emotions (Hofmann et al., 2020;\nYeo and Ong, 2023): while different individuals\nmay appraise the same situation distinctively, they\nare more likely to experience the same emotion\nwhen a consistent appraisal pattern emerges. For\nexample, the cognitive dimension pls (pleasant-\nness) is often linked to joy, but unlikely to be as-\nsociated with disgust (Smith and Ellsworth, 1985).\nTherefore, specific emotions are hypothesized to\nstem from corresponding appraisal patterns (Yeo\nand Ong, 2023). By understanding how individ-\nuals appraise the situations they experience, we\ncan subsequently make predictions regarding their\nemotional state. As a result, appraisal dimensions\nare valuable in differentiating emotional states, es-\npecially in cases where the emotions are highly\ninterchangeable (e.g., disgust and anger).\nHere, using the cognitive appraisal dimensions\nannotated in COVID ET-APPRAISALS , we further\nexplore and validate whether these appraisal di-\nmensions alone are indicative of perceived emo-\ntions already annotated in COVID ET. While in the\nideal scenario, both the appraisal and the objec-\ntive event need to be present for emotion predic-\ntion, this small experiment will allow us to gauge\nwhich dimensions are more likely discriminative\nfor a particular emotion. For each of the 7 emo-\n14430\nID Abbrv. Reader-Friendly Labels Anger Fear Joy Sadness Disgust\n1 srsp Self-responsibility + + +\n2 orsp Other-responsibility + + +\n3 crsp Circumstances-responsibility + +\n4 pfc Problem-focused coping - - + ††\n5 grlv Goal Relevance +† + + +\n6 attn Attentional activity + + + +\n7 efc Emotion-focused coping - + -\n8 scrl Self-Controllable - + -\n9 ocrl Other-Controllable +\n10 ccrl Circumstances-Controllable + +\n11 prd Predictability - - -\n12 thr Threat +† + - + +\n13 pls Pleasantness - - + - -\n14 crt Certainty - + -\n15 gcnd Goal Conduciveness - + - +\n17 fex Future expectancy +\n19 loss Loss + + - +\n20 fml Familiarity - -\n21 eff Effort + - +\n22 chl Challenge\n24 exp Expectedness +\nTable 7: Cognitive emotion appraisal dimensions that are predictive of emotions (including anger, fear, joy,\nsadness, and disgust), identified by a recent meta-analysis conducted by Yeo and Ong (2023). + indicates appraisal\ndimensions that are significantly positively predictive of emotions, and −indicates appraisal dimensions that are\nsignificantly negatively predictive of emotions. We highlight in red the indicative appraisal dimensions captured by\nour logistic regression models that are in line with Yeo and Ong (2023)’s findings. † signifies weights in our logistic\nregression models with p< 0.1, and †† signifies significant weights with p< 0.05.\ntion classes labeled in COVID ET, we train a lo-\ngistic regression model using the scales of the an-\nnotated 21 applicable appraisal dimensions as fea-\ntures. We split COVID ET-APPRAISALS using a\nrandom 80:20 train-test partitioning, and aggregate\nthe Likert-scale ratings for the40 posts that are dou-\nbly annotated by our human annotators following\nthe aggregation setup discussed in §3. We down-\nsample the training data for each logistic regression\nmodel to handle class imbalance issues. In addi-\ntion, we encode the“not mentioned” (NA) labels as\nan independent real-valued feature, and substitute\ntheir values with 0. To prevent features of different\nscales or magnitudes from having a disproportion-\nate influence on the models, we Z-normalize the\nscale ratings within each dimension for each anno-\ntator.\nThe F1 scores for each emotion using the trained\nlogistic regression models on the test set are re-\nported in Table 6. We observe that the models are\nmost capable at predicting emotions such as fear\nand sadness, whilst performing poorly on emotions\ndisgust and trust. This is possibly due to the do-\nmain of our dataset: in COVID ET, fear and sadness\nare the most commonly found emotions whereas\ndisgust and trust are scarcely present. On average,\nthe classifiers achieve an average F1 of 0.22 on the\ntest set across all emotions.\nTo reveal the appraisal dimensions that are in-\ndicative of each emotion, we examine the weights\nfrom the trained logistic regression models. Specif-\nically, we aim to validate the emotion appraisal\ndimensions that Yeo and Ong (2023) identified to\nbe predictive of emotions (including anger, fear,\njoy, sadness, and disgust) from prior studies in psy-\nchology. In Table 7, we show the appraisal dimen-\nsions found to be either positively predictive ( +)\nor negatively predictive (−) of emotions. Please\nnote that these indications are extracted from a re-\ncent meta-analysis from Yeo and Ong (2023) with\nsignificance (p <0.05). In Table 7, we highlight\nthe indicative appraisal dimensions captured by our\nlogistic regression models that are in line with Yeo\nand Ong (2023)’s findings. We observe a certain\ndegree of overlap between Yeo and Ong (2023)’s\nidentified emotion appraisal dimensions that are\npredictive of emotions and those captured by our\nlogistic regression models. It should be noted that\nsome appraisal dimensions may not be useful for all\nemotions included in Table 7, since in COVID ET\nthere are no Reddit posts annotated with neutral\nemotions: for example, as shown in Table 7, crsp\n(circumstances-responsibility) is found to be posi-\ntively indicative for fear and sadness, while neutral\n14431\nsrsp orsp crsp pfc grlv attn efc\nbelieve responsible control cope finds attend cope\nresponsible people believes believe concerns believes emotionally\ndoes believes circumstances doesn highly need somewhat\ndoesn does covid coping relevant want feeling\ncausing covid responsible having covid believe struggling\nfocused vaccinated blame vaccine infected covid believe\nreaction believe delta believes stuck advice covid\nbelieves somewhat outside covid dose asking believes\nsomewhat blame pandemic difficult ending pandemic doesn\nvaccinated causing worried time pandemic trying coping\nscrl ocrl ccrl prd thr pls crt\ncontrol people control happen threatened finds uncertain\nbelieve control covid believe covid unpleasant unsure\ndoes believes believes predict feels feeling certain\nbelieves wait circumstances doesn does covid consequences\ndoesn vaccine outside covid express pandemic vaccine\ncovid covid delta don feeling worried covid\nfeel somewhat understands unable health pleasant understand\nvaccine does understand prediction threat confused somewhat\nvaccinated believe believe makes somewhat feel delta\npandemic september pandemic information sense vaccine fully\ngcnd fex loss fml eff chl exp\nwant worse sense subject effort finds occur\nfinds better does information deal challenging did\ninconsistent believe express meaning mental covid expect\ncovid does loss advice believes vaccinated mentions\nhighly believes lost asking lot highly somewhat\nwants getting believes mentions exert pandemic expected\nvaccinated covid covid unfamiliar try vaccine covid\ndon delta pandemic familiar believe worried expecting\nfeel worried vaccinated covid covid delta mention\ntrying variant opportunity somewhat need variant vaccinated\nTable 8: LDA results on the annotated rationales for each appraisal dimension.\nfor all other emotions. However, when compared to\nneutral emotions (i.e., in texts where no emotions\nare present), crsp (circumstances-responsibility)\nmay be a negative indicator for disgust. Therefore,\nexperimenting with COVID ET-APPRAISALS may\nnot reveal the extensive range of appraisal dimen-\nsions indicative of each emotion. Further investi-\ngations are needed to explore the predictability of\nthese appraisal dimensions for emotions compared\nagainst neutral emotions.\nD.2 Topic Variations in Rationales\nWe use Latent Dirichlet Allocation (LDA) (Blei\net al., 2003) to extract topics from the natu-\nral language rationales annotated in COVID ET-\nAPPRAISALS . Stop-words such as common En-\nglish function words and words that occur fre-\nquently in our instructions (e.g., narrator, situ-\nation) are removed prior to the topic modeling.\nThe most prominent topic extracted by the LDA\nmodel for each dimension is shown in Table 8. We\nnotice clear patterns of topics related to the ap-\npraisal dimension being assessed. For example, in\ndimension crsp (circumstances-responsibility) we\nobserve narrators of Reddit posts worrying about\nand blaming Delta, a COVID-19 variant, for caus-\ning the status quo, whereas in dimension fml (fa-\nmiliarity) we note people are generally unfamiliar\nwith the situation, as they are prone to seek advice\nand probe for information on the forum.\nD.3 An Example of Semantic Similarity\nAs discussed in §4, commonly used automatic\nmeasures such as BLEU (Papineni et al., 2002),\nROUGE (Lin, 2004), and BERTScore (Zhang et al.,\n2019) do not adequately capture semantic similar-\nity in COVID ET-APPRAISALS . Taking the post\nin Figure 1 for example. Both rationales for di-\nmension 24, namely “The narrator mentions how\npeople who are vaccinated and mildly sick are still\nexperiencing long COVID symptoms. They seem\nsurprised by the continued COVID symptoms peo-\nple are experiencing and how the situation seems\nto evolve. ”and “The narrator really didn’t expect\n14432\nBLEU ROUGE BERTSCORE\nBLEU-2 BLEU-3 BLEU-4 ROUGE-1 ROUGE-2 ROUGE-L BERTSCORE RE-SCALED\nCHATGPT 0.147 0.078 0.044 0.317 0.111 0.224 0.890 0.347\nALPACA-7B 0.136 0.069 0.040 0.292 0.101 0.230 0.881 0.297\nALPACA-13B 0.007 0.004 0.003 0.019 0.005 0.017 0.842 0.066\nDOLLY-7B 0.067 0.034 0.020 0.185 0.047 0.142 0.858 0.157\nDOLLY-12B 0.086 0.043 0.024 0.223 0.066 0.165 0.865 0.199\nFLAN-T5-XXL 0.026 0.014 0.008 0.091 0.018 0.066 0.840 0.053\nTable 9: The full rationale statistics measured for LLMs’ responses against the gold annotations, measured across 5\nindependent runs.\nFAC REL JUS USE\nEVALUATORS 0.721 0 .711 0 .632 0 .672\nTable 10: Inter-annotator agreement statistics for the\nhuman evaluation task, measured using Krippendorff’s\nAlpha with interval distance.\nthis situation since they mention being able to taste\nfreedom, believing the pandemic is ending, when\nsuddenly they heard news that vaccinated people\nare still getting long covid and now they think the\npandemic will never end. ”convey the reasons for\nwhy the narrator fails to expect the situation to oc-\ncur. However, the automatic metrics reveal low\nagreement between these two rationales, with a\nBLEU-4 score of 0.018, ROUGE-L of 0.231, and\na re-scaled BERTS CORE of 0.237. This finding is\nin line with work showing the challenges of eval-\nuating generation (Gehrmann et al., 2021; Celiky-\nilmaz et al., 2020); we similarly conclude that au-\ntomatic evaluation metrics may poorly reflect the\ncorrectness of a rationale for a subjective emotion\nappraisal dimension.\nE Prompt Templates\nThe templates for prompting the LLMs are shown\nin Figure 17. We use “1-step” prompting to elicit\nboth a rating and a rationale with a single prompt\nfrom ChatGPT. For all other language models, we\napply “2-step” prompting, which first elicits the\nrating for the appraisal dimension, then conditioned\non the response for the rating we further elicit the\nrationale for the selection.\nF Full LLM Rationale Measures\nRationale Automatic Evaluation. We provide\nthe full statistics of the automatic rationale agree-\nment measured using BLEU (Papineni et al., 2002),\nROUGE (Lin, 2004), and BERTScore (Zhang et al.,\n2019) for the all 6 LLMs’ responses against the\ngold annotations in Table 9.\nAs discussed in §7.1, ChatGPT is the most per-\nformant language model in providing natural lan-\nguage rationales, with values from these metrics\ncomparable to those between different rationales\nfrom our two annotators. Alpaca-7B also achieves\ncomparable performance in these automatic mea-\nsures, despite its relatively poor capability in terms\nof selecting Likert-scale ratings.\nIn addition, we observe that other language mod-\nels such as FLAN-T5 and Dolly lag behind con-\nsiderably compared to ChatGPT and Alpaca-7B.\nEnchantingly, the automatic metrics suggest that\nAlpaca-13B is the worst language model among our\nLLMs under assessment, with a markable degrada-\ntion from Alpaca-7B. Further investigation reveals\nthat Alpaca-13B tends to respond with “ Tell us\nwhy.</s>” when prompted to generate the natural\nlanguage rationale for the Likert-scale rating it se-\nlects, which takes up more than84% of its rationale\nresponses. The debasement of the Alpaca model\nin spite of the increase in the model’s scale raises\nquestions regarding the scaling law in our current\ntask of appraising cognitive emotion dimensions in\ncontext.\nRationale Human Evaluation. We provide the\nbox plots of the results from the human evaluation\nfor the most-performant 3 language models (i.e.,\nChatGPT, Alpaca-7B, and FLAN-T5) in Figure 6.\nFurthermore, we also provide the results for the\nhuman evaluation regardingall 6 LLMs assessed in\nthis paper. Following the setup in §7.2, we evaluate\nand analyze LLM-generated rationales when the\nmodel made a near-correct prediction of the Likert-\nscale rating for that particular dimension compared\nagainst the gold human ratings. Specifically, we\nsample the intersection of dimensions (post, di-\nmension) tuples where all 6 LLMs’ (i.e., ChatGPT,\nFLAN-T5, Alpaca-7B, Alpaca-13B, Dolly-7B, and\nDolly-12B) ratings fall in the range of an abso-\n14433\nLENGTH ABSTRACTIVENESS AUTO EVAL HUMAN EVAL\n# TOKENS %NOVEL BIGRAMS BLEU-4 ROUGE-L BERTSC FAC REL JUS USE\nANNOTATORS 28.9 86.7% —— 0.68 4.43 0.92 0.77\nCHATGPT 58.0 81.8% 0.044 0.224 0.347 0.88 4.42 0.85 0.88\nFLAN-T5 45.3 16.0% 0.008 0.066 0.053 0.44 2.27 0.25 0.19\nALPACA-7B 48.6 71.9% 0.040 0.230 0.297 0.57 4.23 0.79 0.64\nALPACA-13B 19.7 10.9% 0.003 0.017 0.066 0.03 1.13 0.02 0.02\nDOLLY-7B 79.7 51.3% 0.020 0.142 0.157 0.32 2.44 0.21 0.18\nDOLLY-12B 73.3 55.1% 0.024 0.165 0.199 0.38 2.79 0.56 0.38\nTable 11: Experiment results from LLMs. We report the average performance across five independent runs. A more\ncomprehensive report of the automatic metrics BLEU-4 , ROUGE-L , and BERTS CORE is provided in Table 9,\nAppendix §F.\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000049/uni00000044/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni0000004d/uni00000058/uni00000056/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000048/uni00000056\n/uni0000004a/uni00000052/uni0000004f/uni00000047\n/uni00000046/uni0000004b/uni00000044/uni00000057/uni0000004a/uni00000053/uni00000057/uni00000049/uni0000004f/uni00000044/uni00000051/uni00000010/uni00000057/uni00000018/uni00000010/uni0000005b/uni0000005b/uni0000004f/uni00000044/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000055/uni00000048/uni0000004f/uni00000048/uni00000059/uni00000044/uni00000051/uni00000046/uni00000048\n/uni0000004a/uni00000052/uni0000004f/uni00000047\n/uni00000046/uni0000004b/uni00000044/uni00000057/uni0000004a/uni00000053/uni00000057/uni00000049/uni0000004f/uni00000044/uni00000051/uni00000010/uni00000057/uni00000018/uni00000010/uni0000005b/uni0000005b/uni0000004f/uni00000044/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000058/uni00000056/uni00000048/uni00000049/uni00000058/uni0000004f/uni00000051/uni00000048/uni00000056/uni00000056\nFigure 6: Box plots of the results from the human evalu-\nation task for the most-performant 3 LLMs (i.e., Chat-\nGPT, Alpaca-7B, and FLAN-T5).\nlute difference of 1 to one of the annotated scale-\nratings. This results in 30 rationales annotated by\nhuman annotators and 26 natural language ratio-\nnales from each LLM. We report the inter-evaluator\nagreement using Krippendorff’s Alpha with inter-\nval distance in Table 10, which shows substantial\nagreement (Artstein and Poesio, 2008) across all\ncriteria.\nResults from the human evaluation for all 6\nLLMs are reported in Table 11. We observe that\napart from ChatGPT and Alpaca-7B, all other\nLLMs including FLAN-T5, Alpaca-13B, Dolly-7B,\nand Dolly-12B achieve similarly low performance\non providing natural language rationales for cogni-\ntive emotion appraisals. We provide the box plots\nof the results from the human evaluation for all 6\nlanguage models in Figure 7.\nG Model Responses Analyses\nThe LLMs’ performance in terms of Likert-scale\nrating selections measured using Spearman corre-\nlation and Krippendorff’s alpha against the gold\nannotations are shown in Figure 8. Additionally,\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000049/uni00000044/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni0000004d/uni00000058/uni00000056/uni00000057/uni0000004c/uni00000049/uni0000004c/uni00000048/uni00000056\n/uni0000004a/uni00000052/uni0000004f/uni00000047\n/uni00000046/uni0000004b/uni00000044/uni00000057/uni0000004a/uni00000053/uni00000057/uni00000049/uni0000004f/uni00000044/uni00000051/uni00000010/uni00000057/uni00000018/uni00000010/uni0000005b/uni0000005b/uni0000004f/uni00000044/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000044/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000047/uni00000052/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000059/uni00000015/uni00000010/uni0000001a/uni00000025/uni00000047/uni00000052/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000059/uni00000015/uni00000010/uni00000014/uni00000015/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000055/uni00000048/uni0000004f/uni00000048/uni00000059/uni00000044/uni00000051/uni00000046/uni00000048\n/uni0000004a/uni00000052/uni0000004f/uni00000047\n/uni00000046/uni0000004b/uni00000044/uni00000057/uni0000004a/uni00000053/uni00000057/uni00000049/uni0000004f/uni00000044/uni00000051/uni00000010/uni00000057/uni00000018/uni00000010/uni0000005b/uni0000005b/uni0000004f/uni00000044/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni0000001a/uni00000025/uni00000044/uni0000004f/uni00000053/uni00000044/uni00000046/uni00000044/uni00000010/uni00000014/uni00000016/uni00000025/uni00000047/uni00000052/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000059/uni00000015/uni00000010/uni0000001a/uni00000025/uni00000047/uni00000052/uni0000004f/uni0000004f/uni0000005c/uni00000010/uni00000059/uni00000015/uni00000010/uni00000014/uni00000015/uni00000025\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000058/uni00000056/uni00000048/uni00000049/uni00000058/uni0000004f/uni00000051/uni00000048/uni00000056/uni00000056\nFigure 7: Box plots of the results from the human evalu-\nation task for all 6 LLMs.\nthe box plots for each LLM’s Likert-scale ratings\nare shown in Figure 9.\nH Human Evaluation Framework\nWe provide the instructions given to the human\nevaluators of the rationales (described in §7.2) in\nFigure 18 and Figure 19. Additionally, we show-\ncase the human evaluation task layout in Figure\n20.\nI Why Does ChatGPT Perform (Slightly)\nBetter Than Human Annotators in\nProviding Rationales?\nAs discussed in §7.2, ChatGPT was scored slightly\nhigher in terms of factuality and usefulness on pro-\nviding natural language rationales than our human\nannotators, according to human evaluators. This\ncan be attributed to ChatGPT’s wordiness and ex-\ntractiveness (as shown in Table 4), especially in\ncognitive emotion appraisal dimensions where the\nscale rating is low. As an example, we showcase\nin Table 12 where both ChatGPT and our human\nannotator give the same rating for a dimension, but\nChatGPT scores higher than our human experts on\n14434\npfc\nefc\nchl\ngcnd\npls\nthr\norsp\nscrl\nfex\neff\nocrl\nloss\ncrt\nsrsp\ngrlv\nccrl\ncrsp\nfml\nattn\nprd\nexp\nDimensions\n0.0\n0.2\n0.4\n0.6Spearman's ρ\nchatgpt: Per Dimension Performance\n0.0\n0.2\n0.4\n0.6\nKrippendorff's Alpha\npls\ngcnd\npfc\norsp\nfex\ncrt\neff\nefc\nsrsp\nthr\nexp\nfml\nccrl\nchl\nloss\nocrl\ngrlv\nscrl\ncrsp\nattn\nprd\nDimensions\n0.0\n0.2\n0.4Spearman's ρ\nflan-t5-xxl: Per Dimension Performance\n−0.50\n−0.25\n0.00\n0.25\n0.50\nKrippendorff's Alpha\npfc\nefc\nscrl\npls\nattn\nfex\ncrsp\nloss\ncrt\ngcnd\nthr\nchl\nsrsp\norsp\nocrl\ngrlv\nccrl\nfml\nexp\neff\nprd\nDimensions\n0.0\n0.2\n0.4Spearman's ρ\nalpaca-7B: Per Dimension Performance\n−0.4\n−0.2\n0.0\nKrippendorff's Alpha\ncrt\ngcnd\npfc\nfml\nefc\ngrlv\nfex\nprd\nchl\ncrsp\nscrl\nccrl\nloss\nthr\norsp\nsrsp\npls\nexp\nattn\nocrl\neff\nDimensions\n−0.2\n0.0\n0.2\nSpearman's ρ\nalpaca-13B: Per Dimension Performance\n−0.4\n−0.2\n0.0\nKrippendorff's Alpha\npfc\nccrl\nprd\nefc\npls\nfml\nsrsp\nexp\ngrlv\ngcnd\ncrsp\neff\nchl\nfex\nthr\nloss\nscrl\ncrt\norsp\nattn\nocrl\nDimensions\n−0.1\n0.0\n0.1\nSpearman's ρ\ndolly-v2-7B: Per Dimension Performance\n−0.4\n−0.2\n0.0\nKrippendorff's Alpha\ncrt\nexp\nprd\nfml\nsrsp\npls\nchl\npfc\nattn\ncrsp\nefc\nscrl\nccrl\nthr\nloss\neff\ngrlv\nfex\nocrl\ngcnd\norsp\nDimensions\n−0.05\n0.00\n0.05\n0.10\nSpearman's ρ\ndolly-v2-12B: Per Dimension Performance\n−0.6\n−0.4\n−0.2\n0.0\nKrippendorff's Alpha\nFigure 8: LLMs’ performance in terms of Spearman correlation and Krippendorff’s alpha (using interval distance)\nagainst the gold annotations within each group of dimensions (averaged performance across 5 independent runs).\nmetrics factuality and usefulness.\nAs shown in the example, given the same Red-\ndit post as well as the instruction to evaluate the\ncognitive emotion appraisal dimension orcl (other-\ncontrollable), both our human annotator and Chat-\nGPT give a Likert rating of 1 indicating a small\nextent to which the narrator believes other people\nwere controlling what was happening in the situa-\ntion. Similarly, in their natural language rationales,\nboth our annotator and ChatGPT express that there\nis no mention of other people controlling the sit-\nuation in the Reddit post. However, in the post,\nalthough to a small degree, the narrator is hinting\nthat other people are in control of the situation since\ntheir school is deciding on the mask policy for the\nschool year. This lowers the human annotation to a\nmean factuality rating of 0.25 and a mean useful-\nness rating of 0.50. On the other hand, ChatGPT\niterates and summarizes the information provided\nin the post, mentioning that the narrator is “seek-\ning advice on where to find masks that are more\nsuitable for their needs” . The provision of such\ninformation thus leads to a higher score than our\nannotator, resulting in a mean rating of 0.75 for\nboth factuality and usefulness.\nREDDIT POST\nI want to clarify that I am in no way shape an anti masker, I\nbelieve we do need them during the pandemic and any future\noutbreaks. However, the masks are a nightmare for me person-\nally. They trigger off my migraines, I could hardly concentrate\nat school because of it. I also have a sensory disability, which\nmakes wearing masks 10 times harder. Everything is brighter,\nlouder, and everything feels more crowded when I wear them.\nIts horrible. And it looks like we have to wear them for the\nnext school year, and its my final year. I cant risk it. So I\nwant to know if any you know where I can get masks that are\nmigraine/sensory friendly? While still doing the job a normal\nmedical mask does. Thanks.\nAPPRAISAL DIMENSION ocrl\nTo what extent did the narrator think that OTHER PEOPLE\nwere controlling what was happening in the situation?\nHUMAN ANNOTATIONS\n→SCALE: 1\n→RATIONALE: The narrator does not mention anyone else\nas controlling what’s happening in the situation.\nFACTUALITY: 0.25 (Minor Error+ No)\nUSEFULNESS : 0.50 (Maybe+ Maybe)\nCHATGPT’S RESPONSES\n→SCALE: 1\n→RATIONALE: The text does not suggest that other people\nare controlling the situation. The narrator is simply seeking\nadvice on where to find masks that are more suitable for their\nneeds.\nFACTUALITY: 0.75 (Yes+ Minor Error)\nUSEFULNESS : 0.75 (Yes+ Maybe)\nTable 12: An example found in our human evaluation\nwhere ChatGPT scores higher than our human annota-\ntors on metrics factuality and usefulness.\n14435\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9Scale\nchatgpt: Scale Distribution of Each Dimension\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9Scale\nflan-t5-xxl: Scale Distribution of Each Dimension\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9Scale\nalpaca-7B: Scale Distribution of Each Dimension\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9Scale\nalpaca-13B: Scale Distribution of Each Dimension\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9Scale\ndolly-v2-7B: Scale Distribution of Each Dimension\nsrsp\norsp\ncrsp\npfc\ngrlv\nattn\nefc\nscrl\nocrl\nccrl\nprd\nthr\npls\ncrt\ngcnd\nfex\nloss\nfml\neff\nchl\nexp\nDimensions\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9Scale\ndolly-v2-12B: Scale Distribution of Each Dimension\nFigure 9: Box-plots for the LLMs’ Likert-scale rating responses, measured across 5 independent runs.\n14436\nR e d d i t P o s t I ' v e r e c e n t l y h e a r d o f s t o r i e s t h a t p e o p l e w h o a r e g e t t i n g m i l d l y s i c k a f t e r b e i n g v a c c i n a t e d a r e s t i l l c o m i n g d o w n w i t h l o n g c o v i d . P e o p l e o n t h e c o v i d l o n g h a u l e r s u b r e d d i t a r e s a y i n g t h a t w e ' r e g o i n g t o h a v e a h u g e p r o b l e m o n o u r h a n d s b e c a u s e n o v a c c i n e p r o t e c t s a g a i n s t l o n g c o v i d . . . . T h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? R i g h t a s I t h i n k I ' v e t a s t e d f r e e d o m , I ﬁ n d o u t i n f o r m a t i o n t h a t m a k e s m e w a n t t o s t a y i n s i d e f o r e v e r l i k e a h e r m i t … \nC o g n i t i v e E m o t i o n A p p r a i s a l D i m e n s i o n D i m e n s i o n 3 : T o w h a t e x t e n t d i d t h e n a r r a t o r t h i n k t h a t C I R C U M S T A N C E S B E Y O N D A N Y O N E ’ S C O N T R O L w e r e r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n ? ( F o r e x a m p l e , i f t h e n a r r a t o r t h o u g h t t h a t t h e s i t u a t i o n w a s c a u s e d b y l u c k , o r b y w e a t h e r , o r b y s o m e o t h e r t h i n g s o u t s i d e a n y o n e ' s c o n t r o l . ) \nE x a m p l e A n n o t a t i o n s a n d M o d e l s ’ R e s p o n s e s [A n example fr om Co vidE T -A ppr aisals] 󰢨󰢧H u m a n A n n o t a t o r s ➔[ S c a l e ] : 8 ➔[ R a t i o n a l e ] : T h e n a r r a t o r m e n t i o n s l o n g C O V I D a n d t h e i n a b i l i t y t o c u r r e n t l y p r o t e c t a g a i n s t l o n g C O V I D . T h i s i s a f a c t o r t h a t i s s t i l l o u t s i d e o f a n y o n e ' s c o n t r o l , i n c l u d i n g d o c t o r s a n d n u r s e s , a n d t h e s i t u a t i o n c o n t i n u e s t o e v o l v e i n u n c o n t r o l l a b l e w a y s . \n [Responses fr om LLMs e v aluated in the paper] 🤖C h a t G P T ➔[ S c a l e ] : 7 ➔[ R a t i o n a l e ] : T h e n a r r a t o r s e e m s t o b e a c k n o w l e d g i n g t h a t t h e s i t u a t i o n i s b e y o n d a n y o n e ' s c o n t r o l , a s t h e y m e n t i o n “ s t o r i e s ” a n d i n f o r m a t i o n t h a t t h e y h a v e h e a r d . T h e y a l s o e x p r e s s a s e n s e o f h e l p l e s s n e s s a n d r e s i g n a t i o n , s t a t i n g t h a t “ T h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? ” a n d f e e l i n g l i k e t h e y w a n t t o “ s t a y i n s i d e f o r e v e r l i k e a h e r m i t . ” H o w e v e r , t h e y d o n o t e x p l i c i t l y s t a t e t h a t c i r c u m s t a n c e s a r e c o m p l e t e l y r e s p o n s i b l e , a s t h e y m a y s t i l l b e l i e v e t h a t t h e r e a r e s o m e a c t i o n s t h a t c a n b e t a k e n t o m i t i g a t e t h e s i t u a t i o n . \n 🤖F L A N - T 5 - X X L ( 1 1 B ) ➔[ S c a l e ] : 1 ➔[ R a t i o n a l e ] : 1 \n 🤖A l p a c a ( 7 B ) ➔[ S c a l e ] : 5 ➔[ R a t i o n a l e ] : T h e n a r r a t o r b e l i e v e s t h a t c i r c u m s t a n c e s b e y o n d a n y o n e ' s c o n t r o l w e r e r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n t o a m o d e r a t e e x t e n t . T h e y b e l i e v e t h a t l u c k , w e a t h e r , a n d o t h e r e x t e r n a l f a c t o r s p l a y e d a r o l e i n c r e a t i n g t h e c u r r e n t s i t u a t i o n , b u t t h a t i t w a s n o t c o m p l e t e l y d u e t o l u c k o r e x t e r n a l f a c t o r s . \n 🤖A l p a c a ( 1 3 B ) ➔[ S c a l e ] : 0 ➔[ R a t i o n a l e ] : T e l l u s w h y . \n 🤖D o l l y - V 2 ( 7 B ) ➔[ S c a l e ] : 0 ➔[ R a t i o n a l e ] : I d o n ' t k n o w w h a t y o u ' r e t a l k i n g a b o u t , b u t I ' m g o i n g t o g o o u t s i d e a n d p l a y w i t h m y n e w r e m o t e - c o n t r o l l e d h e l i c o p t e r . \n 🤖D o l l y - V 2 ( 1 2 B ) ➔[ S c a l e ] : 7 ➔[ R a t i o n a l e ] : I t h i n k c i r c u m s t a n c e s w e r e c o m p l e t e l y r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r b e l i e v e s t h a t g e t t i n g v a c c i n a t e d g i v e s y o u t h e s a m e i m m u n i t y t o t h e l o n g c o v i d a s a n a t u r a l i n f e c t i o n w o u l d . H o w e v e r , t h e r e i s r e s e a r c h t o s u g g e s t t h a t t h i s i s n o t t h e c a s e . T h e n a r r a t o r i s a l s o a s s u m i n g t h a t e v e r y o n e w h o h a s g o t t e n v a c c i n a t e d h a s g o t t e n t h e s a m e i n e f f e c t i v e v a c c i n a t i o n a s h i m , w h i c h i s n o t t h e c a s e . A d d i t i o n a l l y , t h e t e x t s u g g e s t s t h a t t h e l o n g \n 1 \nFigure 10: A comprehensive example from COVID ET-APPRAISALS for Dimension 3 (circumstances-responsibility)\n(Page 1/3).\n14437\nc o v i d i s n o t g o i n g t o e n d , w h i c h i s a l s o n o t t h e c a s e . T h e l o n g c o v i d i s a p a n d e m i c t h a t i s g o i n g t o l a s t f o r m a n y y e a r s , n o t j u s t a f e w m o n t h s . T h e p a n d e m i c i s g o i n g t o c a u s e a h u g e p r o b l e m o n t h e U S p o p u l a t i o n , b e c a u s e t h e r e i s n o v a c c i n e t h a t p r o t e c t s a g a i n s t l o n g c o v i d , a n d t h e l o n g c o v i d c a n c a u s e s e r i o u s h e a l t h i s s u e s . \n [Responses fr om other LLMs] 🤖L L a M A ( 7 B ) ➔[ S c a l e ] : I t h i n k t h e n a r r a t o r t h i n k s t h a t c i r c u m s t a n c e s b e y o n d a n y o n e ' s c o n t r o l a r e r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r s a y s t h a t p e o p l e w h o a r e g e t t i n g m i l d l y s i c k a f t e r b e i n g v a c c i n a t e d a r e s t i l l c o m i n g d o w n w i t h l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t p e o p l e o n t h e c o v i d l o n g h a u l e r s u b r e d d i t a r e s a y i n g t h a t w e ' r e g o i n g t o h a v e a h u g e p r o b l e m o n o u r h a n d s b e c a u s e n o v a c c i n e p r o t e c t s a g a i n s t l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? R i g h t a s I t h i n k I ' v e t a s t e d f r e e d o m , I ﬁ n d o u t i n f o r m a t i o n t h a t m a k e s m e w a n t t o s t a y i n s i d e f o r e v e r l i k e a h e r m i t . \\ n \\ n # # # I n s t r u c t i o n : \\ n T o w h a t e x t e n t d i d t h e n a r r a t o r t h i n k t h a t P E O P L E ' S B E H A V I O R w a s r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n ? ( F o r e x a m p l e , i f t h e n a r r a t o r t h o u g h t t h a t t h e s i t u a t i o n w a s c a u s e d b y p e o p l e ' s b a d d e c i s i o n s , o r b y p e o p l e ' s l a c k o f k n o w l e d g e , o r b y p e o p l e ' s l a c k o f e f f o r t , o r b y p e o p l e ' s l a c k o f c o n c e r n , o r b y p e o p l e ' s l a c k o f c o o p e r a t i o n , o r b y p e o p l e ' s l a c k o f s e l f - c o n t r o l , o r b y p e o p l e ' s l a c k o f s e l f - d i s ➔[ R a t i o n a l e ] : I t h i n k t h e n a r r a t o r t h i n k s t h a t p e o p l e ' s b e h a v i o r i s r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r s a y s t h a t p e o p l e w h o a r e g e t t i n g m i l d l y s i c k a f t e r b e i n g v a c c i n a t e d a r e s t i l l c o m i n g d o w n w i t h l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t p e o p l e o n t h e c o v i d l o n g h a u l e r s u b r e d d i t a r e s a y i n g t h a t w e ' r e g o i n g t o h a v e a h u g e p r o b l e m o n o u r h a n d s b e c a u s e n o v a c c i n e p r o t e c t s a g a i n s t l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? R i g h t a s I t h i n k I ' v e t a s t e d f r e e d o m , I ﬁ n d o u t i n f o r m a t i o n t h a t m a k e s m e w a n t t o s t a y i n s i d e f o r e v e r l i k e a h e r m i t . \\ n \\ n # # # I n s t r u c t i o n : \\ n T o w h a t e x t e n t d i d t h e n a r r a t o r t h i n k t h a t P E O P L E ' S B E H A V I O R w a s r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n ? ( F o r e x a m p l e , i f t h e n a r r a t o r t h o u g h t t h a t t h e s i t u a t i o n w a s c a u s e d b y p e o p l e ' s b a d d e c i s i o n s , o r b y p e o p l e ' s l a c k o f k n o w l e d g e , o r b y p e o p l e ' s l a c k o f e f f o r t , o r b y p e o p l e ' s l a c k o f c o n c e r n , o r b y p e o p l e ' s l a c k o f c o o p e r a t i o n , o r b y p e o p l e ' s l a c k o f s e l f - c o n t r o l , o r b y p e o p l e ' s l a c k o f s e l f - d i s . T e l l \n 🤖L L a M A ( 1 3 B ) ➔[ S c a l e ] : I t h i n k t h a t t h e n a r r a t o r t h i n k s t h a t t h e c i r c u m s t a n c e s w e r e n o t a t a l l r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r s a y s t h a t p e o p l e a r e g e t t i n g m i l d l y s i c k a f t e r b e i n g v a c c i n a t e d , a n d t h a t p e o p l e a r e s t i l l g e t t i n g l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t p e o p l e a r e s a y i n g t h a t w e a r e g o i n g t o h a v e a h u g e p r o b l e m o n o u r h a n d s b e c a u s e n o v a c c i n e p r o t e c t s a g a i n s t l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , a n d t h a t p e o p l e a r e s a y i n g t h a t w e a r e g o i n g t o h a v e a h u g e p r o b l e m o n o u r h a n d s b e c a u s e n o v a c c i n e p r o t e c t s a g a i n s t l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? T h e n a r r a t o r a l s o s a y s t h a t r i g h t a s t h e y t h i n k t h e y h a v e t a s t e d f r e e d o m , t h e y ﬁ n d o u t i n f o r m a t i o n t h a t m a k e s t h e m w a n t t o s t a y i n s i d e f o r e v e r l i k e a h e r m i t . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? T h e n a r r a t o r a l s o s a y s t h a t r i g h t a s t h e y t h i n k t h e y h a v e t a s t e d f r e e d o m , t h e y ﬁ n d o u t i n f o r m a t i o n t h a t m a k e s t h e m w a n t t o s t a y i n s i d e f o r e v e r l i k e a h e r m i t . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? T h e n a r r a t o r a l s o s a y s t h a t r i g h t ➔[ R a t i o n a l e ] : I t h i n k t h a t t h e n a r r a t o r t h i n k s t h a t t h e c i r c u m s t a n c e s w e r e n o t a t a l l r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r s a y s t h a t p e o p l e a r e g e t t i n g m i l d l y s i c k a f t e r b e i n g v a c c i n a t e d , a n d t h a t p e o p l e a r e s t i l l g e t t i n g l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t p e o p l e a r e s a y i n g t h a t w e a r e g o i n g t o h a v e a h u g e p r o b l e m o n o u r h a n d s b e c a u s e n o v a c c i n e p r o t e c t s a g a i n s t l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , a n d t h a t p e o p l e a r e s a y i n g t h a t w e a r e g o i n g t o h a v e a h u g e p r o b l e m o n o u r h a n d s b e c a u s e n o v a c c i n e p r o t e c t s a g a i n s t l o n g c o v i d . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? T h e n a r r a t o r a l s o s a y s t h a t r i g h t a s t h e y t h i n k t h e y h a v e t a s t e d f r e e d o m , t h e y ﬁ n d o u t i n f o r m a t i o n t h a t m a k e s t h e m w a n t t o s t a y i n s i d e f o r e v e r l i k e a h e r m i t . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? T h e n a r r a t o r a l s o s a y s t h a t r i g h t a s t h e y t h i n k t h e y h a v e t a s t e d f r e e d o m , t h e y ﬁ n d o u t i n f o r m a t i o n t h a t m a k e s t h e m w a n t t o s t a y i n s i d e f o r e v e r l i k e a h e r m i t . T h e n a r r a t o r a l s o s a y s t h a t t h i s i s n ' t g o i n g t o e n d , i s n ' t i t ? T h e n a r r a t o r a l s o s a y s t h a t r i g h t \n 🤖G P T - 3 - d a v i n c i ( v a n i l l a m o d e l ) ➔[ S c a l e ] : \\ n T h e n a r r a t o r t h i n k s t h a t C I R C U M S T A N C E S B E Y O N D A N Y O N E ’ S C O N T R O L a r e r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r t h i n k s t h a t t h e s i t u a t i o n i s c a u s e d b y l u c k . T h e n a r r a t o r t h i n k s t h a t t h e s i t u a t i o n i s c a u s e d b y w e a t h e r . T h e n a r r a t o r t h i n k s t h a t t h e s i t u a t i o n i s c a u s e d b y s o m e o t h e r t h i n g s o u t s i d e a n y o n e ' s c o n t r o l . T h e n a r r a t o r d o e s n ' t t h i n k t h a t t h e s i t u a t i o n i s c a u s e d b y C I R C U M S T A N C E S B E Y O N D A N Y O N E ’ S C O N T R O L . \\ n \\ n T h e n a r r a t o r t h i n k s t h a t C I R C U M S T A N C E S B E Y O N D A N Y O N E ’ S C O N T R O L a r e r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r t h i n k s t h a t t h e s i t u a t i o n i s c a u s e d b y l u c k . T h e n a r r a t o r t h i n k s t h a t t h e s i t u a t i o n i s c a u s e d b y w e a t h e r . T h e n a r r a t o r t h i n k s t h a t t h e s i t u a t i o n i s c a u s e d b y s o m e o t h e r t h i n g s o u t s i d e a n y o n e ' s c o n t r o l . T h e n a r r a t o r d o e s n ' t t h i n k t h a t t h e s i t u a t i o n i s c a u s e d b y C I R C U M S T A N C E S B E Y O N D A N Y O N E ’ S C O N T R O L . \\ n \\ n T h e n a r r a t o r t h i n k s t h a t C I R C U M S T A N C E S B E Y O N D A N Y O N E ’ S C O N T R O L a r e r e s p o n s i b l e f o r c a u s i n g t h e s i t u a t i o n . T h e n a r r a t o r t h i n k s t h a t 2 \nFigure 11: A comprehensive example from COVID ET-APPRAISALS for Dimension 3 (circumstances-responsibility)\n(Page 2/3).\n14438\nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nl \nu \nc \nk \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nw \ne \na \nt \nh \ne \nr \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \ns \no \nm \ne \no \nt \nh \ne \nr \nt \nh \ni \nn \ng \ns \no \nu \nt \ns \ni \nd \ne \na \nn \ny \no \nn \ne \n' \ns \nc \no \nn \nt \nr \no \nl \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \n➔\n[ \nR \na \nt \ni \no \nn \na \nl \ne \n] \n: \n\\ \nn \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nC \nI \nR \nC \nU \nM \nS \nT \nA \nN \nC \nE \nS \nB \nE \nY \nO \nN \nD \nA \nN \nY \nO \nN \nE \n’ \nS \nC \nO \nN \nT \nR \nO \nL \na \nr \ne \nr \ne \ns \np \no \nn \ns \ni \nb \nl \ne \nf \no \nr \nc \na \nu \ns \ni \nn \ng \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nl \nu \nc \nk \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nw \ne \na \nt \nh \ne \nr \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \ns \no \nm \ne \no \nt \nh \ne \nr \nt \nh \ni \nn \ng \ns \no \nu \nt \ns \ni \nd \ne \na \nn \ny \no \nn \ne \n' \ns \nc \no \nn \nt \nr \no \nl \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nd \no \ne \ns \nn \n' \nt \nt \nh \ni \nn \nk \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nC \nI \nR \nC \nU \nM \nS \nT \nA \nN \nC \nE \nS \nB \nE \nY \nO \nN \nD \nA \nN \nY \nO \nN \nE \n’ \nS \nC \nO \nN \nT \nR \nO \nL \n. \n\\ \nn \n\\ \nn \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nC \nI \nR \nC \nU \nM \nS \nT \nA \nN \nC \nE \nS \nB \nE \nY \nO \nN \nD \nA \nN \nY \nO \nN \nE \n’ \nS \nC \nO \nN \nT \nR \nO \nL \na \nr \ne \nr \ne \ns \np \no \nn \ns \ni \nb \nl \ne \nf \no \nr \nc \na \nu \ns \ni \nn \ng \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nl \nu \nc \nk \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nw \ne \na \nt \nh \ne \nr \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \ns \no \nm \ne \no \nt \nh \ne \nr \nt \nh \ni \nn \ng \ns \no \nu \nt \ns \ni \nd \ne \na \nn \ny \no \nn \ne \n' \ns \nc \no \nn \nt \nr \no \nl \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nd \no \ne \ns \nn \n' \nt \nt \nh \ni \nn \nk \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nC \nI \nR \nC \nU \nM \nS \nT \nA \nN \nC \nE \nS \nB \nE \nY \nO \nN \nD \nA \nN \nY \nO \nN \nE \n’ \nS \nC \nO \nN \nT \nR \nO \nL \n. \n\\ \nn \n\\ \nn \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nC \nI \nR \nC \nU \nM \nS \nT \nA \nN \nC \nE \nS \nB \nE \nY \nO \nN \nD \nA \nN \nY \nO \nN \nE \n’ \nS \nC \nO \nN \nT \nR \nO \nL \na \nr \ne \nr \ne \ns \np \no \nn \ns \ni \nb \nl \ne \nf \no \nr \nc \na \nu \ns \ni \nn \ng \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nl \nu \nc \nk \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \nw \ne \na \nt \nh \ne \nr \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \nt \nh \ni \nn \nk \ns \nt \nh \na \nt \nt \nh \ne \ns \ni \nt \nu \na \nt \ni \no \nn \ni \ns \nc \na \nu \ns \ne \nd \nb \ny \ns \no \nm \ne \no \nt \nh \ne \nr \nt \nh \ni \nn \ng \ns \no \nu \nt \ns \ni \nd \ne \na \nn \ny \no \nn \ne \n' \ns \nc \no \nn \nt \nr \no \nl \n. \nT \nh \ne \nn \na \nr \nr \na \nt \no \nr \n3 \nFigure 12: A comprehensive example from COVID ET-APPRAISALS for Dimension 3 (circumstances-responsibility)\n(Page 3/3).\n14439\nHelp Us Understand Emotion \nAppraisals!\nClose\n×\nY\nou'll read a Reddit post about someone's experiences\nduring COVID-19.\nW\ne ask you to judge \n24 aspects\n about how they feel\nabout and view the situation. (For example, whether\nthey felt the situation they were in was something they\ncould control.)\nFor each of these aspects, please select \na rating using\nthe scales we provide (from 1 to 9)\n. \nY\nou can also\nselect the \n[NOT MENTIONED]\n option in case the aspect\nis not mentioned in the given post.\nIn addition\n, please tell us why you picked that rating!\nExamples\n are provided below\n.\nPlease read the following examples:\nReddit Post\n: “Just recently went grocery shopping. I also\nforgot to bring my receipt to the tailor\n, so I had to go back\nhome after grocery shopping just to pick it up and deliver it\nto her to pick my trimmed down pants. \nAnd now I feel like\nI'm spreading COVID-19 to my parents, despite myself\ngetting vaccinated against it. Like, I know the best I'm\ngoing to experience is mild symptoms anytime I *do* get\nit, plus my parents are vaccinated as well, so either they'll\nexperience mild symptoms or no symptoms at all any time\nI spread it to them. But COVID-19 has the potential to\nmutate and thus evade our immune systems much more\neasily\n. \nAnd with that many unvaccinated people spreading\nit among each other for the sake of their otherwise\nnonexistent “freedoms”, it's going to mutate and infect all\nof us vaccinated people and kill us all. So is there anyone\nwho will help me with this? \nThanks!”\nQuestion\n: T\no what extent did the narrator think that\nTHEY\n were responsible for causing the situation?\n--> \nRating\n: 6 (out of 9)\n--> \nReason \n: \nThe narrator expresses concern about\npotentially spreading COVID-19 to their parents, even\nthough they have been vaccinated and their parents have\nalso been vaccinated. \nThey seem to recognize that there\nis a potential for the virus to mutate and evade immunity\n,\nbut also seem to feel some level of personal responsibility\nfor this outcome. \nThe text suggests that the narrator feels\nsome level of guilt or responsibility for causing the\nsituation.\nFigure 13: Instructions to annotators for C OVID ET-APPRAISALS .\n14440\nFigure 14: Annotation task layout for C OVID ET-APPRAISALS (Page 1/3).\n14441\nFigure 15: Annotation task layout for C OVID ET-APPRAISALS (Page 2/3).\n14442\nFigure 16: Annotation task layout for C OVID ET-APPRAISALS (Page 3/3).\nC\nh\na\nt\nG\nP\nT\n:\n1\n-\nS\nt\ne\np\nP\nr\no\nm\np\nt\ni\nn\ng\nO\nt\nh\ne\nr\nL\nL\nM\ns\n:\n2\n-\nS\nt\ne\np\nP\nr\no\nm\np\nt\ni\nn\ng\n{\nR\ne\nd\nd\ni\nt\nP\no\ns\nt\n}\nG\ni\nv\ne\nn\nt\nh\ne\na\nb\no\nv\ne\nt\ne\nx\nt\n,\nt\no\nw\nh\na\nt\ne\nx\nt\ne\nn\nt\nd\ni\nd\nt\nh\ne\nn\na\nr\nr\na\nt\no\nr\nt\nh\ni\nn\nk\nt\nh\na\nt\nT\nH\nE\nY\nw\ne\nr\ne\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\nf\no\nr\nc\na\nu\ns\ni\nn\ng\nt\nh\ne\ns\ni\nt\nu\na\nt\ni\no\nn\n?\nR\na\nt\ne\no\nn\na\ns\nc\na\nl\ne\nf\nr\no\nm\n1\nt\no\n9\n,\nw\ni\nt\nh\n1\nb\ne\ni\nn\ng\n“\nN\na\nr\nr\na\nt\no\nr\nt\nh\no\nu\ng\nh\nt\nt\nh\ne\ny\nw\ne\nr\ne\n:\nN\no\nt\na\nt\na\nl\nl\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\n”\na\nn\nd\n9\nb\ne\ni\nn\ng\n“\nN\na\nr\nr\na\nt\no\nr\nt\nh\no\nu\ng\nh\nt\nt\nh\ne\ny\nw\ne\nr\ne\n:\nC\no\nm\np\nl\ne\nt\ne\nl\ny\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\n”\n.\nI\nf\nt\nh\ne\nt\ne\nx\nt\nd\no\ne\ns\nn\n'\nt\na\nd\nd\nr\ne\ns\ns\nt\nh\ni\ns\nq\nu\ne\ns\nt\ni\no\nn\n,\np\nl\ne\na\ns\ne\nm\na\nr\nk\ni\nt\na\ns\n“\nN\nA\n”\n.\nA\nd\nd\ni\nt\ni\no\nn\na\nl\nl\ny\n,\nt\ne\nl\nl\nu\ns\nw\nh\ny\n.\nT\nh\ne\nf\no\nr\nm\na\nt\no\nf\nt\nh\ne\na\nn\ns\nw\ne\nr\ns\nh\no\nu\nl\nd\nb\ne\na\ns\nf\no\nl\nl\no\nw\ns\n:\n<\nl\ni\nk\ne\nr\nt\n>\n[\n]\n<\n/\nl\ni\nk\ne\nr\nt\n>\n<\nr\na\nt\ni\no\nn\na\nl\ne\n>\n[\n]\n<\n/\nr\na\nt\ni\no\nn\na\nl\ne\n>\n1\ns\nt\n-\nS\nt\ne\np\n:\nB\ne\nl\no\nw\ni\ns\na\nn\ni\nn\ns\nt\nr\nu\nc\nt\ni\no\nn\nt\nh\na\nt\nd\ne\ns\nc\nr\ni\nb\ne\ns\na\nt\na\ns\nk\n,\np\na\ni\nr\ne\nd\nw\ni\nt\nh\na\nn\ni\nn\np\nu\nt\nt\nh\na\nt\np\nr\no\nv\ni\nd\ne\ns\nf\nu\nr\nt\nh\ne\nr\nc\no\nn\nt\ne\nx\nt\n.\nW\nr\ni\nt\ne\na\nr\ne\ns\np\no\nn\ns\ne\nt\nh\na\nt\na\np\np\nr\no\np\nr\ni\na\nt\ne\nl\ny\nc\no\nm\np\nl\ne\nt\ne\ns\nt\nh\ne\nr\ne\nq\nu\ne\ns\nt\n.\n#\n#\n#\ni\nn\np\nu\nt\n:\n{\nR\ne\nd\nd\ni\nt\nP\no\ns\nt\n}\n#\n#\n#\nI\nn\ns\nt\nr\nu\nc\nt\ni\no\nn\n:\nT\no\nw\nh\na\nt\ne\nx\nt\ne\nn\nt\nd\ni\nd\nt\nh\ne\nn\na\nr\nr\na\nt\no\nr\nt\nh\ni\nn\nk\nt\nh\na\nt\nT\nH\nE\nY\nw\ne\nr\ne\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\nf\no\nr\nc\na\nu\ns\ni\nn\ng\nt\nh\ne\ns\ni\nt\nu\na\nt\ni\no\nn\n?\nR\na\nt\ne\no\nn\na\ns\nc\na\nl\ne\nf\nr\no\nm\n1\nt\no\n9\n,\nw\ni\nt\nh\n1\nb\ne\ni\nn\ng\n“\nN\na\nr\nr\na\nt\no\nr\nt\nh\no\nu\ng\nh\nt\nt\nh\ne\ny\nw\ne\nr\ne\n:\nN\no\nt\na\nt\na\nl\nl\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\n”\na\nn\nd\n9\nb\ne\ni\nn\ng\n“\nN\na\nr\nr\na\nt\no\nr\nt\nh\no\nu\ng\nh\nt\nt\nh\ne\ny\nw\ne\nr\ne\n:\nC\no\nm\np\nl\ne\nt\ne\nl\ny\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\n”\n.\nI\nf\nt\nh\ne\nt\ne\nx\nt\nd\no\ne\ns\nn\n'\nt\na\nd\nd\nr\ne\ns\ns\nt\nh\ni\ns\nq\nu\ne\ns\nt\ni\no\nn\n,\np\nl\ne\na\ns\ne\nm\na\nr\nk\ni\nt\na\ns\n“\nN\nA\n”\n.\n#\n#\n#\nR\ne\ns\np\no\nn\ns\ne\n:\n2\nn\nd\n-\nS\nt\ne\np\n:\nB\ne\nl\no\nw\ni\ns\na\nn\ni\nn\ns\nt\nr\nu\nc\nt\ni\no\nn\nt\nh\na\nt\nd\ne\ns\nc\nr\ni\nb\ne\ns\na\nt\na\ns\nk\n,\np\na\ni\nr\ne\nd\nw\ni\nt\nh\na\nn\ni\nn\np\nu\nt\nt\nh\na\nt\np\nr\no\nv\ni\nd\ne\ns\nf\nu\nr\nt\nh\ne\nr\nc\no\nn\nt\ne\nx\nt\n.\nW\nr\ni\nt\ne\na\nr\ne\ns\np\no\nn\ns\ne\nt\nh\na\nt\na\np\np\nr\no\np\nr\ni\na\nt\ne\nl\ny\nc\no\nm\np\nl\ne\nt\ne\ns\nt\nh\ne\nr\ne\nq\nu\ne\ns\nt\n.\n#\n#\n#\ni\nn\np\nu\nt\n:\n{\nR\ne\nd\nd\ni\nt\nP\no\ns\nt\n}\n#\n#\n#\nI\nn\ns\nt\nr\nu\nc\nt\ni\no\nn\n:\nT\no\nw\nh\na\nt\ne\nx\nt\ne\nn\nt\nd\ni\nd\nt\nh\ne\nn\na\nr\nr\na\nt\no\nr\nt\nh\ni\nn\nk\nt\nh\na\nt\nT\nH\nE\nY\nw\ne\nr\ne\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\nf\no\nr\nc\na\nu\ns\ni\nn\ng\nt\nh\ne\ns\ni\nt\nu\na\nt\ni\no\nn\n?\nR\na\nt\ne\no\nn\na\ns\nc\na\nl\ne\nf\nr\no\nm\n1\nt\no\n9\n,\nw\ni\nt\nh\n1\nb\ne\ni\nn\ng\n“\nN\na\nr\nr\na\nt\no\nr\nt\nh\no\nu\ng\nh\nt\nt\nh\ne\ny\nw\ne\nr\ne\n:\nN\no\nt\na\nt\na\nl\nl\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\n”\na\nn\nd\n9\nb\ne\ni\nn\ng\n“\nN\na\nr\nr\na\nt\no\nr\nt\nh\no\nu\ng\nh\nt\nt\nh\ne\ny\nw\ne\nr\ne\n:\nC\no\nm\np\nl\ne\nt\ne\nl\ny\nr\ne\ns\np\no\nn\ns\ni\nb\nl\ne\n”\n.\nI\nf\nt\nh\ne\nt\ne\nx\nt\nd\no\ne\ns\nn\n'\nt\na\nd\nd\nr\ne\ns\ns\nt\nh\ni\ns\nq\nu\ne\ns\nt\ni\no\nn\n,\np\nl\ne\na\ns\ne\nm\na\nr\nk\ni\nt\na\ns\n“\nN\nA\n”\n.\nT\nh\ne\ns\ne\nl\ne\nc\nt\ne\nd\ns\nc\na\nl\ne\ni\ns\n{\ns\nc\na\nl\ne\na\nn\ns\nw\ne\nr\nf\nr\no\nm\nt\nh\ne\n1\ns\nt\n-\ns\nt\ne\np\n}\n.\nT\ne\nl\nl\nu\ns\nw\nh\ny\n.\n#\n#\n#\nR\ne\ns\np\no\nn\ns\ne\n:\nFigure 17: Prompt templates (taking dimension 1 as an example).\n14443\nI\nn\ns\nt\nr\nu\nc\nt\ni\no\nn\ns\na\nn\nd\nE\nx\na\nm\np\nl\ne\ns\nf\no\nr\nE\nv\na\nl\nu\na\nt\ni\nn\ng\nR\na\nt\ni\no\nn\na\nl\ne\ns\nf\no\nr\nE\nm\no\nt\ni\no\nn\nD\ni\nm\ne\nn\ns\ni\no\nn\ns\n[\nI\nn\ns\nt\nr\nu\nc\nt\ni\no\nn\ns\n]\nT\nh\ni\ns\ni\ns\na\nn\na\nn\nn\no\nt\na\nt\ni\no\nn\nt\na\ns\nk\nf\no\nr\ne\nv\na\nl\nu\na\nt\ni\nn\ng\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\ns\nf\no\nr\ns\ne\nl\ne\nc\nt\ne\nd\nr\na\nt\ni\nn\ng\ns\n.\nD\nu\nr\ni\nn\ng\nt\nh\ne\ne\nv\na\nl\nu\na\nt\ni\no\nn\n,\ny\no\nu\nw\ni\nl\nl\nf\ni\nr\ns\nt\nr\ne\na\nd\na\nR\ne\nd\nd\ni\nt\np\no\ns\nt\n.\nT\nh\ne\nn\n,\ny\no\nu\nw\ni\nl\nl\ns\ne\ne\na\np\na\ni\nr\no\nf\n(\nq\nu\ne\ns\nt\ni\no\nn\n,\na\nn\ns\nw\ne\nr\n)\nr\ne\nl\na\nt\ni\nn\ng\nt\no\nt\nh\ne\ne\nm\no\nt\ni\no\nn\na\nl\ns\nt\na\nt\ne\no\nf\nt\nh\ne\na\nu\nt\nh\no\nr\no\nf\nt\nh\ne\np\no\ns\nt\n.\nT\nh\ne\na\nn\ns\nw\ne\nr\nw\ni\nl\nl\nf\ni\nr\ns\nt\ng\ni\nv\ne\na\nr\na\nt\ni\nn\ng\n(\nt\nh\na\nt\ni\ns\nv\ne\nr\ni\nf\ni\ne\nd\nt\no\nb\ne\nc\no\nr\nr\ne\nc\nt\n)\no\nn\na\ns\nc\na\nl\ne\no\nf\n1\nt\no\n9\n(\ni\nn\nc\nl\nu\nd\ni\nn\ng\na\n“\nN\no\nt\nM\ne\nn\nt\ni\no\nn\ne\nd\n”\nl\na\nb\ne\nl\ni\ns\np\nr\no\nv\ni\nd\ne\nd\ni\nn\nc\na\ns\ne\nt\nh\ne\na\ns\np\ne\nc\nt\ni\ns\nn\no\nt\nm\ne\nn\nt\ni\no\nn\ne\nd\ni\nn\nt\nh\ne\np\no\ns\nt\n)\n,\nf\no\nl\nl\no\nw\ne\nd\nb\ny\na\nr\na\nt\ni\no\nn\na\nl\ne\ne\nx\np\nl\na\ni\nn\ni\nn\ng\nw\nh\ny\nt\nh\ne\nr\na\nt\ni\nn\ng\ni\ns\ns\ne\nl\ne\nc\nt\ne\nd\n.\nT\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\ni\ns\nt\nh\ne\np\no\nr\nt\ni\no\nn\nw\ne\na\ns\nk\ny\no\nu\nt\no\ne\nv\na\nl\nu\na\nt\ne\n.\nT\nh\ne\ne\nv\na\nl\nu\na\nt\ni\no\nn\nw\ni\nl\nl\nb\ne\nc\no\nn\nd\nu\nc\nt\ne\nd\nb\na\ns\ne\nd\no\nn\n4\nc\nr\ni\nt\ne\nr\ni\na\n,\nn\na\nm\ne\nl\ny\n“\nf\na\nc\nt\nu\na\nl\nc\no\nn\ns\ni\ns\nt\ne\nn\nc\ny\n”\n,\n“\nr\ne\nl\ne\nv\na\nn\nc\ne\n”\n,\n“\nj\nu\ns\nt\ni\nf\ni\na\nb\ni\nl\ni\nt\ny\n”\n,\na\nn\nd\n“\nu\ns\ne\nf\nu\nl\nn\ne\ns\ns\n”\n.\nT\nh\ne\nd\ne\nt\na\ni\nl\ne\nd\ni\nn\ns\nt\nr\nu\nc\nt\ni\no\nn\ns\nf\no\nr\ne\na\nc\nh\nq\nu\ne\ns\nt\ni\no\nn\na\nr\ne\ns\nh\no\nw\nn\nb\ne\nl\no\nw\n.\n1\n)\nI\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nf\na\nc\nt\nu\na\nl\nl\ny\nc\no\nn\ns\ni\ns\nt\ne\nn\nt\nw\ni\nt\nh\nt\nh\ne\np\no\ns\nt\n?\n★\nW\nh\ne\nt\nh\ne\nr\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nf\na\ni\nt\nh\nf\nu\nl\nl\ny\nr\ne\nf\nl\ne\nc\nt\ns\nw\nh\na\nt\nʼ\ns\ns\nt\na\nt\ne\nd\ni\nn\nt\nh\ne\np\no\ns\nt\n.\nI\nn\no\nt\nh\ne\nr\nw\no\nr\nd\ns\n,\nd\no\ne\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\na\nc\nc\nu\nr\na\nt\ne\nl\ny\nd\ne\ns\nc\nr\ni\nb\ne\nw\nh\na\nt\nt\nh\ne\np\no\ns\nt\ni\ns\ns\na\ny\ni\nn\ng\n,\no\nr\nd\no\ne\ns\ni\nt\nm\ni\ns\nr\ne\np\nr\ne\ns\ne\nn\nt\no\nr\nh\na\nl\nl\nu\nc\ni\nn\na\nt\ne\nt\nh\ne\nc\no\nn\nt\ne\nn\nt\ni\nn\ns\no\nm\ne\nw\na\ny\n?\n○ \n“\nY\ne\ns\n”\n:\ni\nf\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\ni\ns\na\nc\nc\nu\nr\na\nt\ne\nw\ni\nt\nh\nn\no\ne\nr\nr\no\nr\ns\n○ \n“\nM\ni\nn\no\nr\nE\nr\nr\no\nr\n”\n:\ni\nf\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nc\no\nn\nt\na\ni\nn\ns\ns\no\nm\ne\nm\ni\nn\no\nr\ne\nr\nr\no\nr\ns\no\nr\no\nm\ni\ns\ns\ni\no\nn\ns\n○ \n“\nN\no\n”\n:\ni\nf\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nc\no\nn\nt\na\ni\nn\ns\ns\ni\ng\nn\ni\nf\ni\nc\na\nn\nt\ne\nr\nr\no\nr\ns\n,\nm\ni\ns\nr\ne\np\nr\ne\ns\ne\nn\nt\na\nt\ni\no\nn\ns\n,\no\nr\ns\ni\ng\nn\ni\nf\ni\nc\na\nn\nt\nh\na\nl\nl\nu\nc\ni\nn\na\nt\ni\no\nn\ns\nt\no\nt\nh\ne\nq\nu\ne\ns\nt\ni\no\nn\n2\n)\nI\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nr\ne\nl\ne\nv\na\nn\nt\nt\no\nt\nh\ne\na\ns\np\ne\nc\nt\nq\nu\ne\ns\nt\ni\no\nn\nb\ne\ni\nn\ng\na\ns\nk\ne\nd\n?\n★\nW\nh\ne\nt\nh\ne\nr\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nd\ni\nr\ne\nc\nt\nl\ny\na\nd\nd\nr\ne\ns\ns\ne\ns\nt\nh\ne\ns\np\ne\nc\ni\nf\ni\nc\nq\nu\ne\ns\nt\ni\no\nn\nt\nh\na\nt\ni\ns\nb\ne\ni\nn\ng\na\ns\nk\ne\nd\na\nb\no\nu\nt\nt\nh\ne\np\no\ns\nt\n.\nT\nh\ni\ns\nm\ne\na\nn\ns\nt\nh\na\nt\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\ns\nh\no\nu\nl\nd\nb\ne\nf\no\nc\nu\ns\ne\nd\no\nn\nt\nh\ne\ns\np\ne\nc\ni\nf\ni\nc\na\ns\np\ne\nc\nt\no\nf\nt\nh\ne\np\no\ns\nt\nt\nh\na\nt\ni\ns\nb\ne\ni\nn\ng\ne\nv\na\nl\nu\na\nt\ne\nd\n,\na\nn\nd\ns\nh\no\nu\nl\nd\nn\no\nt\ns\nt\nr\na\ny\no\nﬀ\n-\nt\no\np\ni\nc\no\nr\np\nr\no\nv\ni\nd\ne\ni\nr\nr\ne\nl\ne\nv\na\nn\nt\ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\n.\n(\nM\no\ns\nt\nR\ne\nl\ne\nv\na\nn\nt\n)\n(\nL\ne\na\ns\nt\nR\ne\nl\ne\nv\na\nn\nt\n)\n5\n4\n3\n2\n1\n○ \n⸻\n○ \n⸻\n○ \n⸻\n○ \n⸻\n○ \n3\n)\nD\no\ne\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nj\nu\ns\nt\ni\nf\ny\nt\nh\ne\ns\ne\nl\ne\nc\nt\ne\nd\ns\nc\na\nl\ne\n?\n★\nD\no\ne\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\na\nd\ne\nq\nu\na\nt\ne\nl\ny\ne\nx\np\nl\na\ni\nn\nw\nh\ny\nt\nh\ne\ns\ne\nl\ne\nc\nt\ne\nd\nr\na\nt\ni\nn\ng\ns\nc\na\nl\ne\ni\ns\nt\nh\ne\nm\no\ns\nt\na\np\np\nr\no\np\nr\ni\na\nt\ne\no\nr\nr\ne\nl\ne\nv\na\nn\nt\no\nn\ne\nt\no\nu\ns\ne\nf\no\nr\nt\nh\ne\na\ns\np\ne\nc\nt\nb\ne\ni\nn\ng\ne\nv\na\nl\nu\na\nt\ne\nd\n?\nY\ne\ns\nN\no\n○ \n⸻\n⸻\n○ \n4\n)\nI\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nu\ns\ne\nf\nu\nl\n(\ni\nn\nf\no\nr\nm\na\nt\ni\nv\ne\n)\n?\n★\nW\nh\ne\nt\nh\ne\nr\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\np\nr\no\nv\ni\nd\ne\ns\nu\ns\ne\nf\nu\nl\no\nr\ni\nn\nf\no\nr\nm\na\nt\ni\nv\ne\ni\nn\ns\ni\ng\nh\nt\ns\no\nr\ne\nx\np\nl\na\nn\na\nt\ni\no\nn\ns\nr\ne\nl\na\nt\ne\nd\nt\no\nt\nh\ne\na\ns\np\ne\nc\nt\nb\ne\ni\nn\ng\ne\nv\na\nl\nu\na\nt\ne\nd\n.\nI\nn\no\nt\nh\ne\nr\nw\no\nr\nd\ns\n,\nd\no\ne\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\np\nr\no\nv\ni\nd\ne\nh\ne\nl\np\nf\nu\nl\ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\no\nr\ni\nn\ns\ni\ng\nh\nt\ns\nt\nh\na\nt\nc\na\nn\na\ni\nd\ni\nn\ne\nv\na\nl\nu\na\nt\ni\nn\ng\nt\nh\ne\na\ns\np\ne\nc\nt\nq\nu\ne\ns\nt\ni\no\nn\nb\ne\ni\nn\ng\na\ns\nk\ne\nd\n?\n○ \n“\nY\ne\ns\n”\n:\ni\nf\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\np\nr\no\nv\ni\nd\ne\ns\nc\nl\ne\na\nr\na\nn\nd\ni\nn\nf\no\nr\nm\na\nt\ni\nv\ne\ne\nx\np\nl\na\nn\na\nt\ni\no\nn\ns\no\nr\ni\nn\ns\ni\ng\nh\nt\ns\nf\no\nr\nt\nh\ne\nr\ne\na\ns\no\nn\ns\nw\nh\ny\nt\nh\ne\nL\ni\nk\ne\nr\nt\n-\ns\nc\na\nl\ne\nr\na\nt\ni\nn\ng\ni\ns\ns\ne\nl\ne\nc\nt\ne\nd\n○ \n“\nM\na\ny\nb\ne\n”\n:\ni\nf\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\np\nr\no\nv\ni\nd\ne\ns\ns\no\nm\ne\ni\nn\nf\no\nr\nm\na\nt\ni\no\nn\nt\nh\na\nt\nm\na\ny\nb\ne\nu\ns\ne\nf\nu\nl\n,\nb\nu\nt\ni\ns\nn\no\nt\ne\nn\nt\ni\nr\ne\nl\ny\nc\nl\ne\na\nr\no\nr\nm\na\ny\nn\no\nt\nf\nu\nl\nl\ny\na\nd\nd\nr\ne\ns\ns\nt\nh\ne\nr\ne\na\ns\no\nn\ns\nw\nh\ny\nt\nh\ne\nL\ni\nk\ne\nr\nt\n-\ns\nc\na\nl\ne\nr\na\nt\ni\nn\ng\ni\ns\ns\ne\nl\ne\nc\nt\ne\nd\n○ \n“\nN\no\n”\n:\ni\nf\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nd\no\ne\ns\nn\no\nt\np\nr\no\nv\ni\nd\ne\na\nn\ny\nu\ns\ne\nf\nu\nl\no\nr\ni\nn\nf\no\nr\nm\na\nt\ni\nv\ne\ni\nn\ns\ni\ng\nh\nt\ns\no\nr\ne\nx\np\nl\na\nn\na\nt\ni\no\nn\ns\nf\no\nr\nt\nh\ne\nr\ne\na\ns\no\nn\ns\nw\nh\ny\nt\nh\ne\nL\ni\nk\ne\nr\nt\n-\ns\nc\na\nl\ne\nr\na\nt\ni\nn\ng\ni\ns\ns\ne\nl\ne\nc\nt\ne\nd\nFigure 18: Instructions for the human evaluation described in §7.2 (Page 1/2).\n14444\n[\nE\nx\na\nm\np\nl\ne\ns\n]\nR\ne\nd\nd\ni\nt\nP\no\ns\nt\n:\nI\n'\nm\nb\ne\ni\nn\ng\nv\na\ng\nu\ne\na\ns\nt\no\nn\no\nt\ng\ni\nv\ne\na\nw\na\ny\nm\ny\ne\nm\np\nl\no\ny\ne\nr\nb\nu\nt\nI\n'\nm\nf\nr\nu\ns\nt\nr\na\nt\ne\nd\na\nn\nd\nw\no\nn\nd\ne\nr\ni\nn\ng\nh\no\nw\ny\no\nu\n'\nr\ne\nc\no\np\ni\nn\ng\nw\ni\nt\nh\nb\ne\ni\nn\ng\no\nn\ne\no\nf\nt\nh\ne\no\nn\nl\ny\nf\ni\ne\nl\nd\ns\ni\nn\nC\nA\nt\nh\na\nt\nw\ni\nl\nl\nb\ne\ns\nt\nu\nc\nk\ni\nn\nt\nh\ne\np\na\ns\nt\nw\ni\nt\nh\nn\no\ne\nn\nd\nd\na\nt\ne\ni\nn\ns\ni\ng\nh\nt\n.\nI\nh\na\nv\ne\na\nl\no\nt\no\nf\nv\na\nr\ni\no\nu\ns\nt\nr\na\nu\nm\na\ns\nf\nr\no\nm\nt\nh\ni\ns\n,\na\ns\nI\n'\nm\ns\nu\nr\ne\nm\na\nn\ny\nd\no\n,\na\nn\nd\nb\ne\ni\nn\ng\nl\ne\n\u0000\no\nu\nt\na\ns\nt\nh\ne\nr\ne\ns\nt\no\nf\nm\ny\n(\na\nl\nl\nv\na\nc\nc\ni\nn\na\nt\ne\nd\n,\nm\ny\ns\ne\nl\nf\ni\nn\nc\nl\nu\nd\ne\nd\n)\nf\na\nm\ni\nl\ny\ng\ne\nt\ns\nt\no\nf\ni\nn\na\nl\nl\ny\nu\nn\nm\na\ns\nk\na\nt\nw\no\nr\nk\ni\ns\nd\ne\nf\ni\nn\ni\nt\ne\nl\ny\nn\no\nt\nh\ne\nl\np\ni\nn\ng\nm\ny\nm\ne\nn\nt\na\nl\ns\nt\na\nt\ne\n.\nE\nx\na\nm\np\nl\ne\nR\na\nt\ni\no\nn\na\nl\ne\ns\nt\no\nE\nv\na\nl\nu\na\nt\ne\n:\nD\ni\nm\ne\nn\ns\ni\no\nn\n2\n0\n:\nT\no\nw\nh\na\nt\ne\nx\nt\ne\nn\nt\nd\ni\nd\nt\nh\ne\nn\na\nr\nr\na\nt\no\nr\nt\nh\ni\nn\nk\nt\nh\na\nt\nt\nh\ne\ns\ni\nt\nu\na\nt\ni\no\nn\nw\na\ns\nF\nA\nM\nI\nL\nI\nA\nR\n?\n(\nF\no\nr\ne\nx\na\nm\np\nl\ne\n,\ni\nf\nt\nh\ne\nn\na\nr\nr\na\nt\no\nr\nt\nh\no\nu\ng\nh\nt\nt\nh\na\nt\nt\nh\ne\ny\nh\na\nd\ne\nx\np\ne\nr\ni\ne\nn\nc\ne\nd\nt\nh\ni\ns\ns\ni\nt\nu\na\nt\ni\no\nn\nb\ne\nf\no\nr\ne\ni\nn\nt\nh\ne\np\na\ns\nt\n.\n)\n➔\n[\nS\nc\na\nl\ne\n]\n:\nS\ni\nt\nu\na\nt\ni\no\nn\nw\na\ns\n(\nN\no\nt\na\nt\na\nl\nl\nf\na\nm\ni\nl\ni\na\nr\n)\n(\nC\no\nm\np\nl\ne\nt\ne\nl\ny\nf\na\nm\ni\nl\ni\na\nr\n)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nN\no\nt\nm\ne\nn\nt\ni\no\nn\ne\nd\n○ \n—\n○ \n—\n○ \n—\n○ \n—\n○ \n—\n◉\n—\n○ \n—\n○ \n—\n○ \n⸻\n○ \n➔\n[\nR\na\nt\ni\no\nn\na\nl\ne\n]\n:\nT\nh\ne\nn\na\nr\nr\na\nt\no\nr\ns\ne\ne\nm\ns\nt\no\nb\ne\nf\na\nm\ni\nl\ni\na\nr\nw\ni\nt\nh\nt\nh\ne\ns\ni\nt\nu\na\nt\ni\no\nn\no\nf\nb\ne\ni\nn\ng\nf\nr\nu\ns\nt\nr\na\nt\ne\nd\na\nn\nd\nt\nr\na\nu\nm\na\nt\ni\nz\ne\nd\nd\nu\ne\nt\no\nt\nh\ne\ni\nr\np\ne\nr\ns\no\nn\na\nl\nh\ne\na\nl\nt\nh\nc\no\nn\nd\ni\nt\ni\no\nn\ns\n,\na\ns\ni\nn\nd\ni\nc\na\nt\ne\nd\nb\ny\na\nr\na\nt\ni\nn\ng\no\nf\n6\n.\nT\nh\ni\ns\ni\nn\nd\ni\nc\na\nt\ne\ns\nt\nh\na\nt\nt\nh\ne\nn\na\nr\nr\na\nt\no\nr\nb\ne\nl\ni\ne\nv\ne\ns\nt\nh\na\nt\nt\nh\ne\ns\ni\nt\nu\na\nt\ni\no\nn\nc\na\nn\nn\no\nt\nb\ne\nr\ne\nv\ne\nr\ns\ne\nd\na\nn\nd\nt\nh\na\nt\ns\no\nm\ne\nt\nh\ni\nn\ng\no\nf\nv\na\nl\nu\ne\nh\na\ns\nb\ne\ne\nn\np\ne\nr\nm\na\nn\ne\nn\nt\nl\ny\nl\no\ns\nt\n.\nH\nu\nm\na\nn\nE\nv\na\nl\nu\na\nt\ni\no\nn\nE\nx\na\nm\np\nl\ne\n:\n1\n)\nI\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nf\na\nc\nt\nu\na\nl\nl\ny\nc\no\nn\ns\ni\ns\nt\ne\nn\nt\nw\ni\nt\nh\nt\nh\ne\np\no\ns\nt\n?\nY\ne\ns\nM\ni\nn\no\nr\nE\nr\nr\no\nr\nN\no\n○ \n⸻\n⸻\n◉\n⸻\n⸻\n○ \n2\n)\nI\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nr\ne\nl\ne\nv\na\nn\nt\nt\no\nt\nh\ne\nq\nu\ne\ns\nt\ni\no\nn\nb\ne\ni\nn\ng\na\ns\nk\ne\nd\n?\nM\no\ns\nt\nR\ne\nl\ne\nv\na\nn\nt\nL\ne\na\ns\nt\nR\ne\nl\ne\nv\na\nn\nt\n5\n4\n3\n2\n1\n○ \n⸻\n○ \n⸻\n◉\n⸻\n○ \n⸻\n○ \n3\n)\nD\no\ne\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nj\nu\ns\nt\ni\nf\ny\nt\nh\ne\ns\ne\nl\ne\nc\nt\ne\nd\ns\nc\na\nl\ne\n?\nY\ne\ns\nN\no\n◉\n⸻\n⸻\n○ \n4\n)\nI\ns\nt\nh\ne\nr\na\nt\ni\no\nn\na\nl\ne\nu\ns\ne\nf\nu\nl\n(\ni\nn\nf\no\nr\nm\na\nt\ni\nv\ne\n)\n?\nY\ne\ns\nM\na\ny\nb\ne\nN\no\n○ \n⸻\n⸻\n◉\n⸻\n⸻\n○ \nFigure 19: Instructions for the human evaluation described in §7.2 (Page 2/2).\n14445\nFigure 20: Task layout for the human evaluation.\n14446",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.6719264984130859
    },
    {
      "name": "Cognition",
      "score": 0.6455695629119873
    },
    {
      "name": "Cognitive appraisal",
      "score": 0.565159261226654
    },
    {
      "name": "Computer science",
      "score": 0.555482804775238
    },
    {
      "name": "Testbed",
      "score": 0.5210853815078735
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5163787603378296
    },
    {
      "name": "Psychology",
      "score": 0.4221246838569641
    },
    {
      "name": "Work (physics)",
      "score": 0.41016343235969543
    },
    {
      "name": "World Wide Web",
      "score": 0.09155580401420593
    },
    {
      "name": "Engineering",
      "score": 0.08328267931938171
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I86519309",
      "name": "The University of Texas at Austin",
      "country": "US"
    }
  ],
  "cited_by": 3
}