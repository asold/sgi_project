{
    "title": "UNetFormer: A UNet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery",
    "url": "https://openalex.org/W4283450732",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2229830886",
            "name": "Wang Libo",
            "affiliations": [
                "Wuhan University",
                "Ministry of Natural Resources",
                "Ministry of Education of the People's Republic of China"
            ]
        },
        {
            "id": "https://openalex.org/A1898319288",
            "name": "Li Rui",
            "affiliations": [
                "University of Warwick"
            ]
        },
        {
            "id": "https://openalex.org/A2224301382",
            "name": "Zhang Ce",
            "affiliations": [
                "UK Centre for Ecology & Hydrology",
                "Lancaster University"
            ]
        },
        {
            "id": "https://openalex.org/A2314294709",
            "name": "Fang Shenghui",
            "affiliations": [
                "Wuhan University"
            ]
        },
        {
            "id": "https://openalex.org/A2487494428",
            "name": "Duan Chenxi",
            "affiliations": [
                "University of Twente"
            ]
        },
        {
            "id": "https://openalex.org/A2362230200",
            "name": "Meng Xiao-liang",
            "affiliations": [
                "Ministry of Natural Resources",
                "Wuhan University",
                "Ministry of Education of the People's Republic of China"
            ]
        },
        {
            "id": "https://openalex.org/A4287551698",
            "name": "Atkinson Peter M.",
            "affiliations": [
                "Chinese Academy of Sciences",
                "University of Southampton",
                "Lancaster University",
                "Institute of Geographic Sciences and Natural Resources Research"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963995737",
        "https://openalex.org/W2963881378",
        "https://openalex.org/W3128592650",
        "https://openalex.org/W6799693294",
        "https://openalex.org/W3211329537",
        "https://openalex.org/W6748481559",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W6802321102",
        "https://openalex.org/W3007268491",
        "https://openalex.org/W3103092912",
        "https://openalex.org/W6754852571",
        "https://openalex.org/W3202923600",
        "https://openalex.org/W2954896312",
        "https://openalex.org/W2950925862",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3128776197",
        "https://openalex.org/W6797691975",
        "https://openalex.org/W3108450508",
        "https://openalex.org/W6756709046",
        "https://openalex.org/W6721886648",
        "https://openalex.org/W2760340275",
        "https://openalex.org/W6758232405",
        "https://openalex.org/W3129029680",
        "https://openalex.org/W2161236525",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2995766874",
        "https://openalex.org/W4213200979",
        "https://openalex.org/W6791768887",
        "https://openalex.org/W4226076631",
        "https://openalex.org/W6798387234",
        "https://openalex.org/W3200075728",
        "https://openalex.org/W3009297390",
        "https://openalex.org/W2778539913",
        "https://openalex.org/W6792155083",
        "https://openalex.org/W6640054144",
        "https://openalex.org/W3028752951",
        "https://openalex.org/W6799107526",
        "https://openalex.org/W2940726923",
        "https://openalex.org/W2538244214",
        "https://openalex.org/W2793461576",
        "https://openalex.org/W2963659230",
        "https://openalex.org/W3018169007",
        "https://openalex.org/W6772565011",
        "https://openalex.org/W2963294354",
        "https://openalex.org/W3077024029",
        "https://openalex.org/W2155632266",
        "https://openalex.org/W4200547174",
        "https://openalex.org/W2885354990",
        "https://openalex.org/W2762439315",
        "https://openalex.org/W3019771380",
        "https://openalex.org/W6781424222",
        "https://openalex.org/W2970773259",
        "https://openalex.org/W6789425149",
        "https://openalex.org/W6795103355",
        "https://openalex.org/W2901442382",
        "https://openalex.org/W4214760051",
        "https://openalex.org/W3034971973",
        "https://openalex.org/W2991488782",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4285137742",
        "https://openalex.org/W3190334976",
        "https://openalex.org/W6746034047",
        "https://openalex.org/W6797399245",
        "https://openalex.org/W2804732189",
        "https://openalex.org/W6793958814",
        "https://openalex.org/W3177272171",
        "https://openalex.org/W3162977387",
        "https://openalex.org/W2760835880",
        "https://openalex.org/W3196904463",
        "https://openalex.org/W6754123467",
        "https://openalex.org/W6768371451",
        "https://openalex.org/W2965383240",
        "https://openalex.org/W3090286019",
        "https://openalex.org/W2994256272",
        "https://openalex.org/W6805641521",
        "https://openalex.org/W6730342312",
        "https://openalex.org/W2267317359",
        "https://openalex.org/W2750722971",
        "https://openalex.org/W6788620109",
        "https://openalex.org/W3093142463",
        "https://openalex.org/W3034427230",
        "https://openalex.org/W6753182481",
        "https://openalex.org/W2764034829",
        "https://openalex.org/W6766618836",
        "https://openalex.org/W6767923612",
        "https://openalex.org/W3137572916",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3138136606",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W4362597616",
        "https://openalex.org/W3201623325",
        "https://openalex.org/W4241449389",
        "https://openalex.org/W3184761517",
        "https://openalex.org/W4236965008",
        "https://openalex.org/W3100449589",
        "https://openalex.org/W3183174367",
        "https://openalex.org/W3104370314",
        "https://openalex.org/W2782522152",
        "https://openalex.org/W3186032668",
        "https://openalex.org/W4205365435",
        "https://openalex.org/W3047725879",
        "https://openalex.org/W2952793010"
    ],
    "abstract": null,
    "full_text": "1 \nUNetFormer: A UNet-like Transformer for Efficient \nSemantic Segmentation of Remote Sensing Urban \nScene Imagery \nLibo Wang1, 2, Rui Li1, Ce Zhang3, 4, Shenghui Fang1*, Chenxi Duan5, Xiaoliang Meng1, 2 and \nPeter M. Atkinson3, 6, 7 \n1) School of Remote Sensing and Information Engineering, Wuhan University, 129 Luoyu Road, \nWuhan, Hubei 430079, China. \n2) Key Laboratory of Natural Resources Monitoring in Tropical and Subtropical Area of South \nChina, Ministry of Natural Resources, Guangzhou, Guangdong 510000, China. \n3) Lancaster Environment Centre, Lancaster University, Lancaster LA1 4YQ, UK. \n4) UK Centre for Ecology & Hydrology, Library Avenue, Lancaster LA1 4AP, UK. \n5) Faculty of Geo-Information Science and Earth Observation, University of Twente, Enschede, \nthe Netherlands. \n6) Geography and Environmental Science, University of Southampton, Highfield, Southampton \nSO17 1BJ, UK. \n7) Institute of Geographic Sciences and Natural Resources Research , Chinese Academy of \nSciences, 11A Datun Road, Beijing 100101, China. \n*Corresponding author.  \n  \n2 \nAbstractâ€”Semantic segmentation of remotely sensed urban scene images is required in a wide \nrange of practical applications, such as land cover mapping, urban change detection, \nenvironmental protection , and economic assessment. Driven by rapid developments in  deep \nlearning technologies, the convolutional neural network (CNN) has dominated semantic \nsegmentation for many years. CNN adopt s hierarchical feature representation, demonstrating \nstrong capabilities for local information extraction. However, the local property of the convolution \nlayer limits the network from capturing the global context. Recently, as a hot topic in the domain \nof computer vision, Transformer has demonstrated its great potential in global information \nmodelling, boosting many vision-related tasks such as image classification, object detection, and \nparticularly semantic segmentation. In this paper, we propose a Transformer-based decoder and \nconstruct an UNet-like Transformer (UNetFormer) for real -time urban scene segmentation. For \nefficient s egmentation, t he UNetFormer selects the lightweight ResNet18 as the  encoder and \ndevelops an efficient global-local attention mechanism to model both global and local information \nin the decoder . Extensive experiments reveal that our method not only runs fas ter but also \nproduces higher accuracy compared with state -of-the-art lightweight models. Specifically, the \nproposed UNetFormer achieved 67.8%  and 52.4% mIoU on the UA Vid and LoveDA datasets, \nrespectively, while the inference speed can achieve up to 322.4 FPS with a 512ïƒ512 input on a \nsingle NVIDIA GTX 3090 GPU. In further exploration, the proposed Transformer-based decoder \ncombined with a Swin Transformer encoder also achieves the state-of-the-art result (91.3% F1 \nand 84.1% mIoU) on the Vaihingen dataset. The source code will be freely available at  \nhttps://github.com/WangLibo1995/GeoSeg. \nIndex Termsâ€”Semantic Segmentation, Remote Sensing, Vision Transformer, Hybrid Structure, \nGlobal-local Context, Urban Scene. \n3 \n1. Introduction \nDriven by advances in sensor technology, fine-resolution remotely sensed urban scene images \nhave been captured increasingly across the globe, with abundant spatial details and rich potential \nsemantic contents. Urban scene images have been subjected extensively to semantic segmentation, \nthe task of pixel -level segmentation and classification, leading to various urban- related \napplications, including land cover mapping (Li et al., 2022b; Maggiori et al., 2016; Marcos et al., \n2018), change detection (Xing et al., 2018; Yin et al., 2018), environmental protection (Samie et \nal., 2020), road and building extraction (Griffiths and Boehm, 2019; Shamsolmoali et al., 2020; \nVakalopoulou et al., 2015) and many other practical applications (Picoli et al., 2018; Shen et al., \n2019). Recently, a growing wave of deep learning technology (LeCun et al., 2015), in particular \nthe convolutional neural network (CNN), has dominated the task of semantic segmentation (Chen \net al., 2014; Chen et al., 2018b; Long et al., 2015; Ronneberger et al., 2015; Zhao et al., 2017a) . \nCompared with traditional machine learning methods for segmentation, such as the support vector \nmachine (SVM) (Guo et al., 2018), random forest (Pal, 2005) and conditional random field (CRF) \n(KrÃ¤henbÃ¼hl and Koltun, 2011), CNN-based methods are capable of capturing more fine-grained \nlocal context information, w hich underpins its huge capabilities in feature representation and \npattern recognition (Zhang et al., 2020a; Zhang et al., 2020b). \nDespite the above advantages, the convolution operation with a fixed receptive view is \ndesigned to extract local patterns and lacks the ability to model global contextual information or \nlong-range dependencies in its nature . As for semantic segmentation, per -pixel classification is \noften ambiguous if only local information is modelled, while the semantic content of each pixel \n4 \nbecomes more accurate with the help of global contextual information (Yang et al., 2021a) (Li et \nal., 2021c). The global and local contextual information is illustrated in Fig. 1. Although the self-\nattention mechanism alleviates the above issue (Vaswani et al., 2017)  (Wang et al., 2018), they \nnormally require significant computational time and memory to capture the global context, thus, \nreducing their efficiency and restricting their potential for real-time urban applications. \n \nFig. 1 Illustration of the global and local contextual information. The local contextual information \nis modelled by convolutions (yellow). The global contextual information is modelled by long -\nrange window-wise dependencies (red). \nIn this paper, we aim to achieve precise urban scene segmentation while ensuring the efficiency \nof the network simultaneously. Inspired by the recent breakthrough of Transformers in computer \nvision, we propose  a UNet-like Transformer (UNetFormer) to address such a challenge. The \nUNetFormer innovatively adopts a hybrid architecture consisting of a CNN-based encoder and a \nspecifically designed  Transformer-based decoder. Specifically, we adopt the ResNet18 as the \nencoder and design a global-local Transformer block (GLTB) to construct the decoder. Unlike the \n\n5 \nconventional self-attention block in the standard Transformer, the proposed GLTB develops an \nefficient global-local attention mechanism with an attentional global branch and a convolutional \nlocal branch to capture both global and local contexts for visual perception, as illustrated in Fig. \n2. In the global branch, the window -based multi-head self-attention and cross-shaped window \ncontext interaction module are introduced to capture global contexts with low complexity (Liu et \nal., 2021) . In the global branch, convolutional layers are applied to extract the local context . \nFinally, to effectively fuse the spatial details and context information as well as further refine the \nfeature maps, a feature refinement head (FRH) is proposed and attached at the end of the network. \nThe trade-off between accuracy and efficiency as well as effective feature refinement allows the \nproposed method to exceed the state-of-the-art lightweight networks for efficient segmentation of \nremotely sensed urban scene images, demonstrated by four public datasets: the UA Vid (Lyu et al., \n2020), ISPRS Vaihingen and Potsdam datasets, as well as the LoveDA (Wang et al., 2021a). \nThe remainder of this paper is organized as follows. In Section 2, we review the related work \non CNN-based and Transformer-based urban scene segmentation and global context modelling. \nIn Section 3, we present the structure of our UNetFormer and introduce the proposed GLTB and \nFRH. In Section 4, we conduct an ablation study to demonstrate the effectiveness of GLTB and \nFRH as well as the novel hybrid structure and compare the results with a set of state- of-the-art \nmodels applied to the four datasets. In Section 5, we provide a comprehensive discussion. Section \n6 is a summary and conclusion. \n6 \n \nFig. 2 Illustration of (a) the standard Transformer block and (b) the global-local Transformer block. \n2. Related work \n2.1 CNN-based semantic segmentation methods \nThe fully convolutional network (FCN) (Long et al., 2015) is the first effective CNN structure \nto address semantic segmentation problems in an end -to-end manner. Since then, CNN-based \nmethods have dominated the semantic segmentation task in the remote sensing field (Kemker et \nal., 2018; Kotaridis and Lazaridou, 2021; Ma et al., 2019; Tong et al., 2020; Zhao and Du, 2016; \nZhu et al., 2017) . However, the over -simplified decoder of FCN leads to a coar se-resolution \nsegmentation, limiting the fidelity and accuracy. \nTo address this problem, an encoder -decoder network , i.e., the UNet, was proposed  for \nsemantic segmentation, with two symmetric paths named the contracting path and the expanding \npath (Ronneberger et al., 2015). The contracting path extracts hierarchical features by gradually \ndownsampling the spatial resolution of the feature maps, while the expanding path learns more \n\n7 \ncontextual information by progressively restoring the spatial resolution. Subse quently, the \nencoder-decoder framework has become the standard structure of remote sensing image \nsegmentation networks (Badrinarayanan et al., 2017; Chen et al., 2018a) (Sun et al., 2019). Based \non encoder-decoder structure, (Diakogiannis et al., 2020; Y ue et al., 2019; Zhou et al., 2018)  \ndesigned different skip connections to capture more abundant context, while (Liu et al., 2018; \nZhao et al., 2017b) (Shen et al., 2019) developed various decoders to retain semantic information. \nThe encoder-decoder CNN-based methods, although have achieved encouraging performance, \nencounter bottlenecks in urban scene interpretation (Sherrah, 2016)  (Marmanis et al.,  2018; \nNogueira et al., 2019). To be specific, CNN-based segmentation networks with limited receptive \nfields can only extract local semant ic features and lack the capability to model the global \ninformation from the whole image. However, within fine-resolution remotely sensed urban scene \nimages, complicated patterns and human- made objects occur frequently (Kampffmeyer et al., \n2016; Marcos et al., 2018) (Audebert et al., 2018). It is difficult to identify these complex objects \nif only relying on the local infromation. \n2.2 Global contextual information modelling \nTo liberate the network from the local pattern focus of CNN s, many attempts have been \nconducted to modelling global contextual information , while t he most popular way is \nincorporating attention mechanisms into networks. For example, Wang et al. modified the dot -\nproduct self-attention mechanism and applied it to computer vision domains (Wang et al., 2018). \nFu et al. appended two types of attention modules on top of a dilated FCN to adaptively integrate \nlocal features with their global dependencies (Fu et al., 2019). Huang et al. proposed a criss-cross \n8 \nattention block to aggregate informative global featur es (Huang et al., 2020) . Yuan et al. \ndeveloped an object context block to explore object-based global relations (Yuan et al., 2020). \nAttention mechanisms also improve the performance of remote sensing image segmentation \nnetworks. Yang et al. propose d an attent ion-fused network to fuse high -level and low -level \nsemantic features and obtain state- of-the-art results in the semantic segmentation of fine-\nresolution remote sensing images (Yang et al., 2021b). Li et al. integrated lightweight spatial and \nchannel attention modules to refine semantic features adaptively for high -resolution remotely \nsensed image segmentation (Li et al., 2020). Ding et al. designed a local attention block with an \nembedding module to capture richer contextual information (Ding et al., 2021). Li et al. developed \na linear attention mechanism to reduce the computational complexity while improving \nperformance (Li et al., 2021a). However, the above attention modules restrict the global feature \nrepresentation due to over-reliance on convolutional operations. Furthermore, a single attention \nmodule cannot model the global information at multi-level semantic features in the decoder. \n2.3 Transformer-based semantic segmentation methods \nRecently, several attempts were made to apply the Transformer for global inform ation \nextraction (Vaswani et al., 2017). Different from the CNN structure, the Transformer translates \n2D image-based tasks into 1D sequence-based tasks. Due to the powerful sequence-to-sequence \nmodelling ability, the Transformer demonstrates superior character ization of extracting global \ncontext than the above -mentioned attention-alone models and obtains state -of-the-art results on \nfundamental vision tasks, such as image classification (Dosovitskiy et al., 2020), object detection \n(Zhu et al., 2020)  and semantic seg mentation (Zheng et al., 2021) . Driven by this , many \n9 \nresearchers in the remote sensing field  have applied the Transformer for remote sensing image \nscene classification (Bazi et al., 2021; Deng et a l., 2021) , hyperspectral image classification \n(Hong et al., 2021) (He et al., 2021), object detection (Li et al., 2022a), change detection (Chen \net al., 2021a), and especially semantic segmentation (Wang et al., 2022; Wang et al., 2021b). \nMost of the existing Transformers for semantic segmentation still follow the encoder-decoder \nframework. According to different encoder-decoder combinations, they can be divided into two \ncategories. The first is constructed by a Transformer-based encoder and a Transformer-based \ndecoder, namely the pure Transformer structure. Typical models include the Segmenter (Strudel \net al., 2021), SegFormer (Xie et al., 2021) and SwinUNet (Cao et al., 2021). The second adopts a \nhybrid structure, which is composed of a Transformer-based encoder and a CNN-based decoder. \nTransformer-based semantic segmentation methods commonly follow the second structure. For \nexample, the TransUNet employed the hybrid vision Transformer (Dosovitskiy et al., 2020) as the \nencoder for stronger feature extraction and obtains state -of-the-art results in medical image \nsegmentation (Chen et al., 2021b). The DC-Swin introduced Swin Transformer (Liu et al., 2021) \nas the encoder and designs a densely connected convolutional decoder for fine-resolution remote \nsensing image segmentation, surpassing the CNN -based methods by a large gap (Wang et al., \n2022). (Panboonyuen et al., 2021) also selected the Swin Transformer as the encoder and utilizes \nvarious CNN-based decoders, such as UNet (Ronneberger et al., 2015), FPN (Kirillov et al., 2019) \nand PSP (Zhao et al., 2017a) , for semantic segmentation of remotely sensed images, obtai ning \nadvanced accuracy. \nDespite the above advantages, the computational complexity of the Transformer-based encoder \n10 \nis much higher than the CNN -based encoder due to its square- complexity self-attention \nmechanism (Vaswani et al., 2017), which seriously affects its potential and feasibility for urban-\nrelated real -time applications. Thus, t o fully harness the global context extraction ability of \nTransformers without resulting in high computational complexity, i n this paper, we present a \nUNet-like Transformer with a CNN-based encoder and a Transformer-based decoder for efficient \nsemantic segmentation of remotely sensed urban scene images. Specifically, for our UNetFormer, \nwe select the lightweight backbone, i.e. ResNet18, as the encoder and develop an efficient global-\nlocal attention mechanism to construct Transformer blocks in the decoder. The proposed efficient \nglobal-local attention mechanism adopts a dual-branch structure, i.e. a global branch and a local \nbranch. Such a structure allows the attention block to  capture both global and local contexts , \nthereby surpassing the single-branch efficient attention mechanisms in Transformers that only \ncapture global contexts (Liu et al., 2021; Zhang and Yang, 2021). \n3. Methodology \nAs illustrated in Fig. 3, the proposed UNetFormer is constructed using a CNN-based encoder \nand a Transformer-based decoder. A detailed description of each component is given in the \nfollowing sections. \n11 \n \nFig. 3. An overview of the UNetFormer. \n3.1 CNN-based encoder \nAs the  ResNet18 (He et al., 2016)  has demonstrat ed effectiveness and efficiency \nsimultaneously in a wide range of real-time semantic segmentation tasks, we select the pre-trained \nResNet18 as the encoder  here to  extract multi-scale semantic features with significantly low \ncomputational cost. ResNet18 consists of four-stage Resblocks, with each stage down-sampling \nthe feature map with a scale factor of 2. In the proposed UNetFormer, the feature maps generated \n\n12 \nby each stage are fused with the corresponding feature maps of the decoder by a 1ïƒ1 convolution \nwith the channel dimension in 64, i.e., the skip connection. Specifically, the semantic features \nproduced by the Resblocks are aggregated with the features generated by the GLTB of the decoder \nusing a weighted sum operation. The weighted sum operation weights the two features selectively \nbased on their contributions to segmentation accuracy, thereby learning more generalized fusion \nfeatures (Tan et al., 2020). The formulation of the weighted sum operation can be denoted as: \nğ…ğ…ğ…ğ… = Î±âˆ™ğ‘ğ‘ğ…ğ…+ (1 âˆ’Î±) âˆ™ğ†ğ†ğ†ğ†ğ…ğ… (1) \nwhere ğ…ğ…ğ…ğ… represents the fused feature, ğ‘ğ‘ğ…ğ… denotes the feature produced by the Resblocks, and \nğ†ğ†ğ†ğ†ğ…ğ… indicates the feature generated by the global-local Transformer block. \n3.2 Transformer-based decoder \nComplicated human-made objects occur frequently in fine -resolution remotely sensed urban \nimages, which makes it difficult to  achieve precise real -time segmentation without global \nsemantic information. To capture the global context, mainstream solutions focus on attaching a \nsingle attention block at the end of the network  (Wang et al., 2018) or introducing Transformers \nas the encoder  (Chen et al., 2021b) . The former cannot capture multi-scale global features, \nwhereas the latter significantly increases the complexity of the network and loses spatial details. \nIn contrast, in the proposed UNetFormer, we utilize three global-local Transformer blocks and a \nfeature refinement head to build a lightweight T ransformer-based decoder, as shown in Fig. 3. \nWith such a hierarchical and lightweight design, the decoder is capable of capturing both global \nand local contexts at multiple scales while maintaining high efficiency. \n13 \n3.2.1 Global-local Transformer block (GLTB) \nThe global-local Transformer block consists of the global-local attention, multilayer perceptron, \ntwo batch normalization layers and two additional operations, as shown in Fig. 1 (b). \nEfficient Global- local attention : Although the global context is crucial for semantic \nsegmentation of complex urban scenes, local information is still essential to preserve rich spatial \ndetails. In this regard, the proposed efficient global- local attention constructs two parallel \nbranches to extract the global and local contexts, respectively, as shown in Fig. 4 (a). \nAs a relatively shallow structure, the local branch employs two parallel convolutional layers \nwith kernel sizes of 3 and 1 to extract the local context. Two batch normalization operations are \nthen attached before the final sum operation. \nThe global branch deploys the window -based multi -head self -attention to capture g lobal \ncontext. As illustrated in Fig 4. (b), we first use a standard 1ïƒ1 convolution to expand the channel \ndimension of the input 2D feature map âˆˆ â„ğµğµÃ—ğ¶ğ¶Ã—ğ»ğ»Ã—ğ‘Šğ‘Š to three times. Then, we apply the window \npartition operation to split the 1D sequence âˆˆ â„ï¿½3Ã—ğµğµÃ—ğ»ğ»\nğ‘¤ğ‘¤Ã—ğ‘Šğ‘Š\nğ‘¤ğ‘¤Ã—â„ï¿½Ã—(ğ‘¤ğ‘¤Ã—ğ‘¤ğ‘¤)Ã—ğ¶ğ¶\nâ„ into the query (Q), key \n(K) and value (V) vectors. The channel dimension C is set to 64. The window size w  and the \nnumber of heads h are both set to 8. The details of the window -based multi-head self-attention \ncan refer to Swin Transformer (Liu et al., 2021). \n14 \n \nFig. 4. Illustration of the efficient global-local attention. \nPerforming self -attention in a non- overlapping local window, although being efficient, can \n\n15 \ndestroy the spatial consistency of urban scenes  due to the lack of interactions across windows.  \nThe Swin Transformer introduces an extra shifted Transformer block to mine the relationship \nbetween local windows. Although the ability to capture cross- window relations increases, the \ncomputation significantly surges accordingly. In this paper, we propose a cross -shaped window \ncontext interaction module to capture the cross -window relations  with high computationa l \nefficiency. As illustrated in Fig. 4 (c), the cross-shaped window context interaction module fuses \nthe two feature maps produced by a horizontal average pooling layer and a vertical average \npooling layer, thereby capturing the global context. Specifically, the horizontal average pool layer \nestablishes the horizontal relationship between Windows, such as  ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Š1 = ğ»ğ»(ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Š2). For any \npoint P1\n(ğ‘šğ‘š,ğ‘›ğ‘›) in Window 1, its dependency with P2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤,ğ‘›ğ‘›) in Window 2 can be modelled as: \n P1\n(ğ‘šğ‘š,ğ‘›ğ‘›) =\nâˆ‘ P1\n(ğ‘šğ‘š+ğ‘–ğ‘–,ğ‘›ğ‘›)ğ‘¤ğ‘¤âˆ’ğ‘šğ‘šâˆ’1\nğ‘–ğ‘–=0 + âˆ‘ P2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤âˆ’ğ‘—ğ‘—,ğ‘›ğ‘›)ğ‘šğ‘š\nğ‘—ğ‘—=0\nğ‘¤ğ‘¤  (2) \n P1\n(ğ‘šğ‘š+ğ‘–ğ‘–,ğ‘›ğ‘›) = ğ·ğ·ğ‘–ğ‘– ï¿½P1\n(ğ‘šğ‘š,ğ‘›ğ‘›)ï¿½ (3) \n P2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤âˆ’ğ‘—ğ‘—,ğ‘›ğ‘›) = ğ·ğ·ğ‘—ğ‘— ï¿½P2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤,ğ‘›ğ‘›)ï¿½ (4) \n P1\n(ğ‘šğ‘š,ğ‘›ğ‘›) =\nâˆ‘ ğ·ğ·ğ‘–ğ‘– ï¿½P1\n(ğ‘šğ‘š,ğ‘›ğ‘›)ï¿½ğ‘¤ğ‘¤âˆ’ğ‘šğ‘šâˆ’1\nğ‘–ğ‘–=0 + âˆ‘ ğ·ğ·ğ‘—ğ‘— ï¿½P2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤,ğ‘›ğ‘›)ï¿½ğ‘šğ‘š\nğ‘—ğ‘—=0\nğ‘¤ğ‘¤  (5) \nWhere w is the window size. D  denotes the self -attention computation, which can model \ndependencies of pixel pairs in a local window. Thus, for any other point P1\n(ğ‘šğ‘š+ğ‘–ğ‘–,ğ‘›ğ‘›) in the red path \nof Window 1, its dependency with P1\n(ğ‘šğ‘š,ğ‘›ğ‘›)  can be  modelled by Eq.(3). For any other point \nP2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤âˆ’ğ‘—ğ‘—,ğ‘›ğ‘›) in the green path of Window 2, its dependency with P2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤,ğ‘›ğ‘›) can be modelled by \nEq.(4). Eq.(2) can be rewritten as Eq.(5), i.e. the dependency between P1\n(ğ‘šğ‘š,ğ‘›ğ‘›) and P2\n(ğ‘šğ‘š+ğ‘¤ğ‘¤,ğ‘›ğ‘›) is \nmodelled. Based on this cross- window pixel -wise dependency, the horizontal relationship \nbetween Windows 1 and 2 can be established. Similarly, the vertical relationship between Window \n16 \n1 and 3 can be established in the same way, i.e. ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Š1 = ğ‘‰ğ‘‰(ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Š3), and for Window 4, ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Š1 =\nğ‘‰ğ‘‰(ğ»ğ»(ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Š4)) + ğ»ğ»(ğ‘‰ğ‘‰(ğ‘Šğ‘Šğ‘Šğ‘Šğ‘Šğ‘Š4)) . Generalized to an MïƒM input (M denotes the number of \nwindows), by connecting more intermedia windows like Window 2 and Window 3, the long-range \ndependency between any two windows can be modelled. Thus, the cross-shaped window context \ninteraction module can model the window -wise long-range dependencies, thereby capturing the \nglobal context. \nBesides, the global context in the global branch is further aggregated with the local context in \nthe local branch to produce the global-local context. Finally, we employ a depth-wise convolution, \na batch normalization operation and a standard 1ïƒ1 convolution to characterize the fine-grained \nglobal-local context. \n3.2.2 Feature refinement head (FRH) \nThe shallow feature produced by the first Resblock preserves rich spatial details of urban scenes, \nbut lacks semantic content, while the deep global -local feature provides precise semantic \ninformation, but with a coarse spatial resolution. Hence, a direct sum  operation on these two \nfeatures, although fast, can reduce segmentation accuracy (Poudel et al., 2018; Poudel et al., 2019; \nYu et al., 2018). In this paper, we develop a feature refinement head to shrink the semantic gap \nbetween the two features for further accuracy improvement. \nAs can be seen in Fig. 5, we perform a weighted sum operation on the two features first to take \nfull advantage of the precise semantic information and spatial details. The fused feature is then \nselected as the input of the FRH, as shown in Fig. 3. Second, we construct two paths to strengthen \nthe channel-wise and spatial-wise feature representation. Specifically, the channel path employs \n17 \na global average pooling layer to generate a channel-wise attentional map ğ‘ªğ‘ª âˆˆ â„1Ã—1Ã—ğ‘ğ‘, where c \ndenotes the channel dimension. The reduce & expand operation contains two 1ïƒ1 convolutional \nlayers, which first reduces the channel dimension c by a factor of 4 and then expands it to the \noriginal. The spatial path utilizes a depth -wise convolution to produce a spatial -wise attentional \nmap ğ‘ºğ‘º âˆˆ â„â„Ã—ğ‘¤ğ‘¤Ã—1 , where h and w represent the spatial resolution of the feature map. The \nattentional features generated by the two paths are further fused using a sum operation. Finally, a \npost-processing 1ïƒ1 convolutional layer and an upsampling operation are applied to produce the \nfinal segmentation map. Notably, a residual connection is introduced to prevent network \ndegradation. \n \nFig. 5. The feature refinement head.  \n\n18 \n3.3 Loss function \nIn the training phase, we employ not only the primary feature refinement head but also build \nan extra auxiliary head to optimize the global-local Transformer blocks, as shown in Fig. 3. This \nmulti-head segmentation architecture has been demonstrated to be effective in previous research \n(Yu et al., 2020; Zhu et al., 2019). Based on the multi-head design, we apply a principal loss and \nan auxiliary loss to train the entire network. The principal loss â„’ğ‘ğ‘ is a combination of a dice loss \nâ„’ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘ğ‘ğ‘‘ğ‘‘ and a cross-entropy loss â„’ğ‘ğ‘ğ‘‘ğ‘‘, which can be formulated as: \nâ„’ğ‘ğ‘ğ‘‘ğ‘‘ = âˆ’ 1\nğ‘ğ‘ï¿½ ï¿½ ğ‘¦ğ‘¦ğ‘˜ğ‘˜\n(ğ‘›ğ‘›)logğ‘¦ğ‘¦ï¿½ğ‘˜ğ‘˜\n(ğ‘›ğ‘›)ğ¾ğ¾\nğ‘˜ğ‘˜=1\nğ‘ğ‘\nğ‘›ğ‘›=1\n (6) \nâ„’ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘ğ‘ğ‘‘ğ‘‘ = 1 âˆ’ 2\nğ‘ğ‘ï¿½ ï¿½ ğ‘¦ğ‘¦ï¿½ğ‘˜ğ‘˜\n(ğ‘›ğ‘›)ğ‘¦ğ‘¦ğ‘˜ğ‘˜\n(ğ‘›ğ‘›)\nğ‘¦ğ‘¦ï¿½ğ‘˜ğ‘˜\n(ğ‘›ğ‘›) + ğ‘¦ğ‘¦ğ‘˜ğ‘˜\n(ğ‘›ğ‘›)\nğ¾ğ¾\nğ‘˜ğ‘˜=1\nğ‘ğ‘\nğ‘›ğ‘›=1\n (7) \nâ„’ğ‘ğ‘ = â„’ğ‘ğ‘ğ‘‘ğ‘‘ + â„’ğ‘‘ğ‘‘ğ‘–ğ‘–ğ‘ğ‘ğ‘‘ğ‘‘ (8) \nwhere N and K denote the number of samples  and the number of categories, respectively. ğ‘¦ğ‘¦(ğ‘›ğ‘›) \nand ğ‘¦ğ‘¦ï¿½(ğ‘›ğ‘›)  represent the one -hot encoding of the true semantic labels and the corresponding \nsoftmax output of the network, n âˆˆ [1, â‹¯, N]. ğ‘¦ğ‘¦ï¿½ğ‘˜ğ‘˜\n(ğ‘Šğ‘Š) is the confidenc e of sample n  belonging to the \ncategory k. We select the cross-entropy loss as the auxiliary loss â„’ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ and deploy it on the auxiliary \nhead. The auxiliary head takes the fused feature of the three global -local Transformer blocks as \nthe input and const ructs a 3ïƒ3 convolution layer with batch normalization and ReLU, a 1ïƒ 1 \nconvolution layer and an upsampling operation to generate the output. For a better combination \nwith the principle loss, the auxiliary is further multiplied by a factor Î±. Thus, the overall loss â„’ \ncan be formulated as: \n19 \nâ„’ = â„’ğ‘ğ‘ + Î±Ã— â„’ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ (9) \nwhere Î± is set to 0.4 by default. \n4. EXPERIMENTS \n4.1 Experimental settings \n4.1.1 Datasets \nU AVi d: As a fine-resolution Unmanned Aerial Vehicle (UA V) semantic segmentation dataset, \nthe UA Vid dataset focuses on urban street scenes with two spatial resolutions (3840ïƒ2160 and \n4096ïƒ2160) and eight classes (Lyu et al., 2020). Segmentation of UA Vid is challenging due to \nthe fine spatial resolution of images, heterogeneous spatial variation, vague categories an d \ngenerally complex scenes. To be specific, there are 42 sequences with a total of 420 images in the \ndataset, where 200 images are used for training, 70 images for validation and the officially \nprovided 150 images for testing. In our experiments, each imag e was padded and cropped into \neight 1024ïƒ1024 px patches. \nVaihingen: The Vaihingen dataset consists of 33 very fine spatial resolution TOP image tiles \nat an average size of 2494ïƒ2064 pixels. Each TOP image tile has three multispectral bands (near \ninfrared, red, green) as well as a digital surface model (DSM) and normalized digital surface \nmodel (NDSM) with a 9 cm ground sampling distance (GSD). The dataset involves five \nforeground classes (impervious surface, building, low vegetation, tree, car) and one bac kground \nclass (clutter). In our experiments, only the TOP image tiles were used without the DSM and \nNDSM. And we utilized ID: 2, 4, 6, 8, 10, 12, 14, 16, 20, 22, 24, 27, 29, 31, 33, 35, 38 for testing, \n20 \nand the remaining 16 images for training. The image tiles were cropped into 1024ïƒ1024 px \npatches. \nPotsdam: The Potsdam dataset contains 38 very fine spatial resolution TOP image tiles (GSD \n5 cm) at a size of 6000ïƒ6000 pixels and involves the same category information as the Vaihingen \ndataset. Four multispectral bands (red, green, blue, and near infrared), as well as the DSM and \nNDSM, are provided in the dataset. We utilized ID: 2_13, 2_14, 3_13, 3_14, 4_13, 4_14, 4_15, \n5_13, 5_14, 5_15, 6_13, 6_14, 6_15, 7_13 for testing, and the remaining 23 images (except image \n7_10 with error annotations)  for training. Similarly,  only three bands (red, green, blue)  were \nutilized and the original image tiles were cropped into 1024ïƒ1024 px patches in the experiments. \nLoveDA: The LoveDA dataset contains 5987 fine-resolution optical remote sensing image s \n(GSD 0.3 m) at a size of 1024ïƒ1024 pixels and includes 7 landcover categories, i.e. building, \nroad, water, barren, forest, agriculture and background  (Wang et al., 2021a). Specifically, 2522 \nimages are used for training, 1669 images for validation and the officially provided 1796 images \nfor testing. The dataset encompasses two scenes (urban and rural) which are collected from three \ncities (Nanjing, Changzhou and Wuhan) in China. Therefore, considerable challenges are brought \ndue to the multi-scale objects, complex background and inconsistent class distributions. \n4.1.2 Implementation Details \nAll models in the experiments were implemented with the PyTorch framework on a single \nNVIDIA GTX 3090 GPU. For fast convergence, we deployed the AdamW optimizer to train all \nmodels in the experiments. The base learning rate was set to 6e -4. The cosine strategy was \nemployed to adjust the learning rate.  \n21 \nFor the UA Vid dataset, random vertical flip, random horizontal flip and random brightness were \nused to the input in the size of 1024ïƒ1024 for data augmentation in the training period, while the \ntraining epoch was  set as  40 and the batch size was  8. In the test procedure, t he test -time \naugmentation (TTA) strategies like vertical flip and horizontal flip were used. \nFor the Vaihinge, Potsdam and LoveDA datasets, the images we re randomly cropped into \n512ïƒ512 patches. For training, the augmentation techniques like random scale  ([0.5, 0.75, 1.0, \n1.25, 1.5]), random vertical flip, random horizontal flip and random rotate were adopted during \nthe training process, while the training epoch was set as 100 and t he batch size was 16. During \nthe test phase, multi-scale and random flip augmentations were used. \n4.1.3 Evaluation metrics \nThe evaluation metrics used in our experiments include d two major categories. The first one \nwas to evaluate the accuracy of the network including the overall accuracy (OA), mean F1 score \n(F1) and mean intersection over union (mIoU) . The second one was to evaluate the scale of the \nnetwork, including the floating point operation count (Flops) to evaluate the complexity , the \nframes per second (FPS) to evaluate the speed , the memory footprint  (MB) and the number of \nmodel parameters (M) to evaluate the memory requirement. \n4.1.4 Models for comparison \nWe selected a comprehensive set of benchmark methods for quantitative comparison including  \n(i) CNN-based lightweight networks developed for efficient semantic segmentation : \ncontext aggregation network (CANe t) (Yang et al., 2021a) , bilateral segmentation \n22 \nnetwork (BiSeNet) (Yu et al., 2018), ShelfNet (Zhuang et al., 2019), SwiftNet (OrÅ¡iÄ‡ \nand Å egviÄ‡, 2021), Fast-SCNN (Poudel et al., 2019), DABNet (Li et al., 2019), ERFNet \n(Romera et al., 2017) and ABCNet (Li et al., 2021c). \n(ii) CNN-based attentional networks: dual attention network (DANet) (Fu et al., 2019) , \nfast attention network (FANet) (Hu et al., 2020), local attention network (LANet) (Ding \net al., 2021), criss-cross network (CCNet) (Huang et al ., 2020), multi-stage attention \nresidual UNet (MAResU-Net) (Li et al., 2021a) and multi-attention network (MANet) \n(Li et al., 2021b), \n(iii) CNN-based networks for semantic segmentation of remote sensing images: DST_5 \n(Sherrah, 2016), V-FuseNet (Audebert et al., 2018), CASIA2 (Liu et al., 2018), DLR_9 \n(Marmanis et al., 2018), RoteEqNet (Marcos et al., 2018), UFMG_4 (Nogueira et al., \n2019), HUSTW5 (Sun et al., 2019) , TreeUNet (Y ue et al., 2019) , ResUNet-a \n(Diakogiannis et al., 2020) , S-RA-FCN (Mou et al., 2020) , DDCM-Net (Liu et al., \n2020), EaNet (Zheng et al., 2020a) , HMANet (Niu et al., 2021)  and AFNet (Yang et \nal., 2021b), \n(iv) hybrid Transformer-based networks with a Transformer -based encoder and a CNN -\nbased decoder: TransUNet (Chen et al., 2021b), SwinUperNet (Liu et al., 2021), DC-\nSwin (Wang et al., 2022), STranFuse (Gao et al., 2021) , SwinB-CNN+BD (Zhang et \nal., 2022) , SwinTF-FPN (Panboonyuen et al., 2021) , BANet (Wang et al., 2021b) , \nCoaT (Xu et al., 2021) , BoTNet (Srinivas et al., 2021)  and ResT (Zhang and Yang, \n2021), \n23 \n(v) fully Transformer-based networks with a Transformer -based encoder and a \nTransformer-based decoder: SwinUNet (Cao et al., 2021), SegFormer (Xie et al., 2021) \nand Segmenter (Strudel et al., 2021). \n4.2 Ablation study \n4.2.1 Each component of UNetFormer \nTo evaluate the performance of each component of the proposed UNetFormer separately, we \nconducted a series of ablation experiments on the UA Vid, Vaihingen and Potsdam datasets. For a \nfair comparison, t he test time a ugmentation strategies and auxiliary loss  were not used in all \nablation studies. The results are illustrated in TABLE 1. \nTABLE 1. Ablation study of each component of the UNetFormer. \nDataset Method mIoU \nU AVi d \nBaseline 65.4 \nBaseline+GLTB-SUM 67.8 \nBaseline+GLTB 68.8 \nBaseline+GLTB+FRH 70.0 \nVaihingen \nBaseline 77.1 \nBaseline+GLTB-SUM 79.4 \nBaseline+GLTB 80.6 \nBaseline+GLTB+FRH 81.6 \nPotsdam \nBaseline 82.5 \nBaseline+GLTB-SUM 83.8 \nBaseline+GLTB 84.9 \nBaseline+GLTB+FRH 85.5 \n \nBaseline: The baseline was constructed by the U -Net with a ResNet18 backbone, which only \nmodels the local contextual information in the decoder. \n24 \nThe global-local Transformer block (GLTB): Three global-local Transformer blocks were \nincorporated into the baseline to build the Baseline+GLTB . Meanwhile, t o illustrate the \ncontribution of the cross-shaped window context interaction module in the GLTB, we remove it \nand apply a direct sum operation on the window context and local context, thereby constructing \na simple variant Baseline+GLTB-SUM. As shown in TABLE 1 , the deployment of GLTB \nprovides a significant increase of mIoU by 3. 4% on the UA Vid validation set, where the \ncontribution of the cross-shaped window context interaction module to increase accuracy is 1.0%. \nMeanwhile, Baseline+GLTB achieves an increase of greater than 2.4% in mIoU on the Vaihingen \nand Potsdam test sets, where the increase provided by the cross- shaped window context \ninteraction module is 1.2% and 1.1%, respectively. To sum up, the results not only demonstrate \nthe effectiveness of GLTB but also indicate the necessity of applying the cross- shaped window \ncontext interaction module. \nThe feature refinement module  (FRH): We inserted the feature refinement head into \nBaseline+GLTB to generate the entire UNetFormer (indicated as Baseline+GLTB+FRH). As \nshown in TABLE 1 , with the employment of FRH, the mIoU is boosted by 1.0% at least, \ndemonstrating the validity of the proposed feature refinement module. \n4.2.2 Efficient global-local attention \nTo demonstrate the advantages of the proposed efficient global-local attention, we replaced \nit with other advanced attention mechanisms to  reconstruct the variants of UNetformer for  \nablation studies. Benefiting from the dual-branch structure and the captured global-local context, \nthe deployment of our global-local attention achieves the highest mIoU  (70.0%) on the UA Vid \n25 \nvalidation set, as listed in TABLE 2. Besides, the proposed global -local attention also \ndemonstrates superiority in terms of complexity, memory requirement, parameters and inference \nspeed. Especially, our method is more accurate and faster than the efficient attention mechanisms \nin Transformers, i.e. the shifted window attention and the efficient multi-head self-attention. \nTABLE 2. Ablation studies of different attention mechanisms on the UA Vid dataset. We \nreport the speed with an input size of 1024ïƒ1024 on a single NVIDIA GTX 3090 GPU. The \nbest values in the column are in bold. \nAttention mechanism Complexity(G) Memory(MB) Parameters(M) Speed(FPS) mIoU \nDual attention (Fu et al., 2019) 68.9 2416.4 12.6 53.8 67.3 \nCriss-cross attention (Huang et al., 2020) 67.2 1318.4 12.4 79.9 68.3 \nLinear attention (Li et al., 2021b) 67.8 1339.5 12.5 91.5 69.0 \nPatch attention (Ding et al., 2021) 66.8 1320.5 12.3 95.7 68.9 \nEfficient multi-head self-attention (Zhang and Yang, 2021) 67.5 2444.2 12.5 63.6 67.9 \nShifted window attention (Liu et al., 2021) 72.7 1652.0 13.1 67.0 68.5 \nEfficient global-local attention (Sudre et al.) 46.9 1003.8 11.7 115.6 70.0 \n4.2.3 Network stability \nTo evaluate the network stability, we train ed the UNetFormer  with different input sizes,  \nincluding square inputs like 512ïƒ512, 1024ïƒ1024 and 2048ïƒ2048 as well as rectangular inputs \nlike 512ïƒ1024 and 1024ïƒ2048. From the experimental results in TABLE 3 , the UNetFormer \ndemonstrates stability when performing different input sizes, while the deviation of the mIoU is \nless than  0.7%. The middle input size of 1024ïƒ1024 obtains the best mIoU on the UA Vid \nvalidation set. Furthermore, the square inputs yield relatively higher scores than the rectangular \ninputs, and too large input size like 2048ïƒ2048 can reduce the IoU of very small object â€œhumanâ€. \nTABLE 3 Ablation studies of different input sizes on the UA Vid dataset. \n26 \nInput size Clutter Building Road Tree Vegetation MovingCar StaticCar Human mIoU \n512ïƒ512 63.1 90.7 76.4 77.4 68.1 70.3 65.9 46.2 69.8 \n512ïƒ1024 61.9 91.0 74.9 76.9 69.1 70.4 65.6 44.3 69.3 \n1024ïƒ1024 63.6 91.2 76.4 77.7 68.2 71.6 66.1 44.8 70.0 \n1024ïƒ2048 63.0 91.2 76.2 77.5 68.7 69.8 65.2 44.6 69.5 \n2048ïƒ2048 63.4 91.2 76.0 77.9 70.1 70.4 65.7 42.5 69.7 \n4.2.4 Encoder choice \nCurrent Transformer-based segmentation networks commonly apply the Transformer as the \nencoder. This choice, although has been justified for accurate semantic information, reduces the \nexecution speed of the network significantly, which is not suitable for real -time applications. To \ndemonstrate it, we replace our ResNet18 encoder with lightweight Transformers, i.e. ViT-Tiny \n(Dosovitskiy et al., 2020) , Swin-Tiny (Liu et al., 2021)  and CoaT-Mini (Xu et al., 2021) , for \nablation studies (TABLE 4). The results reveal that introducing lightweight Transformers as the \nencoder provides a limited improvement of accuracy  (within 0.6% in mIoU)  but reduce s the \ninference speed of the UNetFormer seriously. Thus, for real-time urban scene segmentation, the \napplication of a lightweight CNN-based encoder like ResNet18 is the currently best scheme. \nTABLE 4 Ablation studies of different encoders on the UA Vid dataset. The complexity and \nspeed are measured by a 1024Ã—1024 input on a single NVIDIA GTX 3090 GPU. \nMethod Encoder Complexity(G) Parameters(M) Speed(FPS) mIoU \nUNetFormer \nViT-Tiny (Dosovitskiy et al., 2020) 35.31 8.6 30.2 69.1 \nSwin-Tiny (Liu et al., 2021) 104.4 28.0 28.8 70.6 \nCoaT-Mini (Xu et al., 2021) 159.7 10.6 10.6 70.5 \nResNet18 46.9 11.7 115.6 70.0 \n \n27 \n4.2.5 Encoder-decoder combination \nTo illustrate the superiority of our hybrid structure for efficient semantic segmentation, we \nselected the UNet, SwinUNet and TransUNet for ablation experiments on the UA Vid dataset. \nSince the SwinUNet require s huge GPU memory, the input size was  all set as 512 ïƒ512 for \ntraining. The results from TABLE 5 reveal that the proposed UNetFormer exceeds the compared \nnetworks significantly in terms of complexity and speed while providing a competitive accuracy \non the UA Vid validation set. Specifically, in comparison with the UNet constructed by the pure \nCNN structure, the UNetFormer achieves an increase of 4.3% in mIoU. Co mpared to the pure \nTransformer network SwinUNet, the UNetFormer saves 80% of the computational complexity. \nAlthough the TransUNet constructed by a Transformer-based encoder and a CNN-based decoder \nsurpasses ours by 0.5% in mIoU, it is 7 times slower and ha s much more parameters due to its \nheavy and complicated Transformer-based encoder. For real-time urban application scenarios, the \nhigh execution speed and lightweight model volume are much more important than the slight \naccuracy reduction. Thus, in compari son with other combinations, the advantage of our hybrid \nstructure, i.e. CNN-based encoder and Transformer-based decoder, is significant. \nTABLE 5 Ablation studies of different encoder-decoder combinations on the UA Vid dataset. \nThe complexity and speed are measured by a 512Ã—512 input on a single NVIDIA GTX 3090 \nGPU. \nMethod Backbone Encoder Decoder Complexity(G) Memory(MB) Parameters(M) Speed(FPS) mIoU \nUNet (Ronneberger et al., 2015) - CNN CNN 184.6 1622.0 31.0 50.9 65.5 \nSwinUNet (Cao et al., 2021) Swin-Tiny Transformer Transformer 237.4 2001.5 41.4 46.9 68.3 \nTransUNet (Chen et al., 2021b) ViT-R50 Transformer CNN 233.7 1245.7 90.7 43.2 70.3 \nUNetFormer ResNet18 CNN Transformer 11.7 250.9 11.7 322.4 69.8 \n28 \n4.3 Experiment results \n4.3.1 Comparison of network efficiency \nComplexity and speed are critical for evaluating a network, especially in real -time urban \napplications. We compared our UNetFormer with efficient segmentation networks based on the \nmIoU, GPU memory footprint, complexity, parameters and speed on the official UA Vid test set. \nThe comparison results are listed in Table 6. In comparison with the fastest and most shallow \nmodel Fast-SCNN, the proposed UNetFormer outperforms it by a large margin of 21.0% in mIoU. \nIn comparison with the state-of-the-art CNN-based models of the same volume, our UNetFormer \nachieves a competitive inference speed of 115.6 FPS, while surpassing other networks by more \nthan 4.0% in mIoU. Notably, our method exceeds the advanced hybrid Transformer network CoaT \nby 2.0% in mIoU while being 10 times faster. Meanwhile, the proposed method outperforms the \npure Transformer network Segmenter by 9.1% in mIoU while being 7 times faster. The \noutstanding trade -off between accuracy and speed demonstrates the efficiency of our hybrid \nstructure and the effectiveness of the proposed GLTB and FRH. \nTABLE 6. Quantitative comparison results on the UA Vid test set with state-of-the-art \nlightweight networks. The complexity and speed are measured by a 1024Ã—1024 input on a \nsingle NVIDIA GTX 3090 GPU. \nMethod Backbone Memory(MB) Parameters(M) Complexity(G) Speed mIoU \nFast-SCNN (Poudel et al., 2019) - 619.4 1.1 3.4 222.7 45.9 \nSegmenter (Strudel et al., 2021) ViT-Tiny 828.6 6.7 26.8 14.7 58.7 \nBiSeNet (Yu et al., 2018) ResNet18 970.6 12.9 51.8 121.9 61.5 \nDANet (Fu et al., 2019) ResNet18 611.1 12.6 39.6 189.4 60.6 \nFANet (Hu et al., 2020) ResNet18 971.9 13.6 86.8 94.9 - \n29 \nShelfNet (Zhuang et al., 2019) ResNet18 579.0 14.6 46.7 141.4 47.0 \nSwiftNet (OrÅ¡iÄ‡ and Å egviÄ‡, 2021) ResNet18 835.8 11.8 51.6 138.7 61.1 \nMANet (Li et al., 2021b) ResNet18 1169.2 12.0 51.7 75.6 62.6 \nABCNet (Li et al., 2021c) ResNet18 1105.1 14.0 62.9 102.2 63.8 \nSegFormer (Xie et al., 2021) MiT-B1 933.2 13.7 63.3 31.3 66.0 \nBoTNet (Srinivas et al., 2021) ResNet18 710.5 17.6 49.9 135.0 63.2 \nCoaT (Xu et al., 2021) CoaT-Mini 3133.8 11.1 104.8 10.6 65.8 \nUNetFormer ResNet18 1003.7 11.7 46.9 115.6 67.8 \n4.3.2 Results on the UA Vid dataset \nUA Vid is a large-scale urban scene segmentation dataset, where the images are captured by \nunmanned aerial vehicles in different cities and under different lighting conditions. Thus, it is \nchallenging to obtain high scores on this dataset. We trained several advanced efficient \nsegmentation networks and provide a detailed comparison of results on the official UA Vid test set. \nAs illustrated in Table 7, our method yields the best mIoU (67.8%) while maintaining the \nadvantages in the per -class IoU. Specifically, the proposed UNetFormer not only exceeds the \nefficient CNN-based network ABCNet by 4.0% in mIoU but also outperforms the recent hybrid \nTransformer-based networks BANet and BoTNet by 3.2 % and 4.6%, respectively. Particularly, \nthe â€œhumanâ€ class is hard to handle since it is an extremely small object. Nonetheless, the IoU of \nthis class achieved by our UNetFormer is at least 8.6% higher than for other methods. Furthermore, \nthe segmentation results from the UA Vid validation set (Fig. 6) and the visualization results from \nthe UA Vid test set (Fig. 7) also demonstrate the effectiveness of our UNetFormer. \nTABLE 7. Quantitative comparison of results on the UA Vid test set with state-of-the-art \nlightweight models. The best values in the column are in bold. \nMethod Backbone Clutter Building Road Tree Vegetation MovingCar StaticCar Human mIoU \nMSD (Lyu et al., 2020) - 57.0 79.8 74.0 74.5 55.9 62.9 32.1 19.7 57.0 \n30 \nCANet (Yang et al., 2021a) - 66.0 86.6 62.1 79.3 78.1 47.8 68.3 19.9 63.5 \nDANet (Fu et al., 2019) ResNet18 64.9 85.9 77.9 78.3 61.5 59.6 47.4 9.1 60.6 \nSwiftNet (OrÅ¡iÄ‡ and Å egviÄ‡, 2021) ResNet18 64.1 85.3 61.5 78.3 76.4 51.1 62.1 15.7 61.1 \nBiSeNet (Yu et al., 2018) ResNet18 64.7 85.7 61.1 78.3 77.3 48.6 63.4 17.5 61.5 \nMANet (Li et al., 2021b) ResNet18 64.5 85.4 77.8 77.0 60.3 67.2 53.6 14.9 62.6 \nABCNet (Li et al., 2021c) ResNet18 67.4 86.4 81.2 79.9 63.1 69.8 48.4 13.9 63.8 \nSegmenter (Strudel et al., 2021) ViT-Tiny 64.2 84.4 79.8 76.1 57.6 59.2 34.5 14.2 58.7 \nSegFormer (Xie et al., 2021) MiT-B1 66.6 86.3 80.1 79.6 62.3 72.5 52.5 28.5 66.0 \nBANet (Wang et al., 2021b) ResT-Lite 66.7 85.4 80.7 78.9 62.1 69.3 52.8 21.0 64.6 \nBoTNet (Srinivas et al., 2021) ResNet18 64.5 84.9 78.6 77.4 60.5 65.8 51.9 22.4 63.2 \nCoaT (Xu et al., 2021) CoaT-Mini 69.0 88.5 80.0 79.3 62.0 70.0 59.1 18.9 65.8 \nUNetFormer ResNet18 68.4 87.4 81.5 80.2 63.5 73.6 56.4 31.0 67.8 \n \n \nFig. 6. Segmentation results from the UA Vid validation set. The first column represents the input \n\n31 \nRGB images. The second column denotes the ground reference. The third column shows the \nsegmentation maps produced by our method. \n \nFig. 7. Enlarged visualization of results from the UA Vid test set. The first column represents the \ninput RGB images. The second column denotes the segmentation results of the baseline. The third \ncolumn shows the segmentation maps of our method. \n4.3.3 Results on the Vaihingen and Potsdam dataset \nThe ISPRS Vaihingen and Potsdam are  two widely-used datasets for segmentation task s. \nNumerically high accuracies have been achieved by the specially designed models on these two \n\n32 \ndatasets. In this section, we demonstrate that our UNetFormer can not onl y surpass lightweight \nmodels but also obtain competitive scores in comparison with leading networks. \nAs illustrated in Table 8, the proposed UNetFormer delivers the best F1, OA and mIoU on the \nVaihingen test set, outperforming other CNN-based and Transformer-based lightweight networks \nby a significant margin. It is worth noting that our method yields an 88.5% F1 score on the â€œcarâ€ \nclass, exceeding other networks by more than 1.7%. Moreover, the prediction results of ID 2 and \n22 are shown in Fig. 8, while th e enlarged visualization of results is illustrated in Fig. 9 (Top), \nwhich also demonstrates the effectiveness of our method. \nTABLE 8. Quantitative comparison results on the Vaihingen test set with state-of-the-art \nlightweight networks. The best values in the column are in bold. \nMethod Backbone Imp.surf. Building Lowveg. Tree Car MeanF1 OA mIoU \nDABNet (Li et al., 2019) - 87.8 88.8 74.3 84.9 60.2 79.2 84.3 70.2 \nERFNet (Romera et al., 2017) - 88.5 90.2 76.4 85.8 53.6 78.9 85.8 69.1 \nBiSeNet (Yu et al., 2018) ResNet18 89.1 91.3 80.9 86.9 73.1 84.3 87.1 75.8 \nPSPNet (Zhao et al., 2017a) ResNet18 89.0 93.2 81.5 87.7 43.9 79.0 87.7 68.6 \nDANet (Fu et al., 2019) ResNet18 90.0 93.9 82.2 87.3 44.5 79.6 88.2 69.4 \nFANet (Hu et al., 2020) ResNet18 90.7 93.8 82.6 88.6 71.6 85.4 88.9 75.6 \nEaNet (Zheng et al., 2020a) ResNet18 91.7 94.5 83.1 89.2 80.0 87.7 89.7 78.7 \nShelfNet (Zhuang et al., 2019) ResNet18 91.8 94.6 83.8 89.3 77.9 87.5 89.8 78.3 \nMAResU-Net (Li et al., 2021a) ResNet18 92.0 95.0 83.7 89.3 78.3 87.7 90.1 78.6 \nSwiftNet (OrÅ¡iÄ‡ and Å egviÄ‡, 2021) ResNet18 92.2 94.8 84.1 89.3 81.2 88.3 90.2 79.6 \nABCNet (Li et al., 2021c) ResNet18 92.7 95.2 84.5 89.7 85.3 89.5 90.7 81.3 \nBoTNet (Srinivas et al., 2021) ResNet18 89.9 92.1 81.8 88.7 71.3 84.8 88.0 74.3 \nBANet (Wang et al., 2021b) ResT-Lite 92.2 95.2 83.8 89.9 86.8 89.6 90.5 81.4 \nSegmenter (Strudel et al., 2021) ViT-Tiny 89.8 93.0 81.2 88.9 67.6 84.1 88.1 73.6 \nUNetFormer ResNet18 92.7 95.3 84.9 90.6 88.5 90.4 91.0 82.7 \n \nFor a comprehensive evaluation, we further conducted experiments on the Postdam dataset. \n33 \nAs shown in Table 10, our UNetFormer achieves a 92.8% mean F1 score and an 86.8% mIoU on \nthe Potsdam test set. The results of the UNetFormer not only exceed the excellent convolutional \nlightweight network ABCNet (Li et al., 2021c)  but also out perform recent Transformer-based \nlightweight networks, such as Segmenter (Strudel et al., 2021) and BANet (Wang et al., 2021b). \nWe also provide segmentation results for ID 3_14 and 2_13 (Fig. 9) and an enlarged visualization \nof the results (Fig. 10 Bottom) to show the preferential performance of our network. \nTABLE 9 Quantitative comparison results on the Potsdam test set with state-of-the-art \nlightweight networks. The best values in the column are in bold. \nMethod Backbone Imp.surf. Building Lowveg. Tree Car MeanF1 OA mIoU \nERFNet (Romera et al., 2017) - 88.7 93.0 81.1 75.8 90.5 85.8 84.5 76.2 \nDABNet (Li et al., 2019) - 89.9 93.2 83.6 82.3 92.6 88.3 86.7 79.6 \nBiSeNet (Yu et al., 2018) ResNet18 90.2 94.6 85.5 86.2 92.7 89.8 88.2 81.7 \nEaNet (Zheng et al., 2020a) ResNet18 92.0 95.7 84.3 85.7 95.1 90.6 88.7 83.4 \nMAResU-Net (Li et al., 2021a) ResNet18 91.4 95.6 85.8 86.6 93.3 90.5 89.0 83.9 \nDANet (Fu et al., 2019) ResNet18 91.0 95.6 86.1 87.6 84.3 88.9 89.1 80.3 \nSwiftNet (OrÅ¡iÄ‡ and Å egviÄ‡, 2021) ResNet18 91.8 95.9 85.7 86.8 94.5 91.0 89.3 83.8 \nFANet (Hu et al., 2020) ResNet18 92.0 96.1 86.0 87.8 94.5 91.3 89.8 84.2 \nShelfNet (Zhuang et al., 2019) ResNet18 92.5 95.8 86.6 87.1 94.6 91.3 89.9 84.4 \nABCNet (Li et al., 2021c) ResNet18 93.5 96.9 87.9 89.1 95.8 92.7 91.3 86.5 \nSegmenter (Strudel et al., 2021) ViT-Tiny 91.5 95.3 85.4 85.0 88.5 89.2 88.7 80.7 \nBANet (Wang et al., 2021b) ResT-Lite 93.3 96.7 87.4 89.1 96.0 92.5 91.0 86.3 \nSwinUperNet (Liu et al., 2021) Swin-Tiny 93.2 96.4 87.6 88.6 95.4 92.2 90.9 85.8 \nUNetFormer ResNet18 93.6 97.2 87.7 88.9 96.5 92.8 91.3 86.8 \n \n34 \n \nFig. 8. Visualization results of ID 2 and 22 from the Vaihingen test set. The first column denotes \nthe input RGB images. The second column represents the ground truth. The third column shows \nthe segmentation results of the proposed UNetFormer. \n\n35 \n \nFig. 9. Visualization results of ID 3_14 and 2_13 from the Potsdam test set. The first column \ndenotes the input RGB images. The second column represents the ground truth. The third column \nshows the segmentation results of the proposed UNetFormer. \n\n36 \n \nFig. 10. Enlarged visualization of results from the Vaihingen (top) and Potsdam (bottom) test set. \n\n37 \n4.3.4 Results on the LoveDA dataset \nWe undertook experiments on the LoveDA dataset to further  evaluate the performance of the \nUNetFormer. Benefiting from the captured global-local context, the UNetFormer can handle both \nurban and rural scenes well in the LoveDA dataset. The comparison results are listed in TABLE \n10. Remarkably, the UNetFormer obta ins the highest mIoU ( 52.4%) with the least complexity  \nand the fastest speed. Visualized comparisons are exhibited in Fig. 11. \nTABLE 10. Quantitative comparison results on the LoveDA test set with other networks. The \ncomplexity and speed are measured by a 1024Ã—1024 input on a single NVIDIA GTX 3090 \nGPU. The best values in the column are in bold. \nMethod Backbone Background Building Road Water. Barren Forest Agriculture mIoU Complexity Speed \nPSPNet (Zhao et al., 2017a) ResNet50 44.4 52.1 53.5 76.5 9.7 44.1 57.9 48.3 105.7 52.2 \nDeepLabV3+ (Chen et al., 2018a) ResNet50 43.0 50.9 52.0 74.4 10.4 44.2 58.5 47.6 95.8 53.7 \nSemanticFPN (Kirillov et al., 2019) ResNet50 42.9 51.5 53.4 74.7 11.2 44.6 58.7 48.2 103.3 52.7 \nFarSeg (Zheng et al., 2020b) ResNet50 43.1 51.5 53.9 76.6 9.8 43.3 58.9 48.2 - 47.8 \nFactSeg (Ma et al., 2021) ResNet50 42.6 53.6 52.8 76.9 16.2 42.9 57.5 48.9 - 46.7 \nBANet (Wang et al., 2021b) ResT-Lite 43.7 51.5 51.1 76.9 16.6 44.9 62.5 49.6 52.6 11.5 \nTransUNet (Chen et al., 2021b) ViT-R50 43.0 56.1 53.7 78.0 9.3 44.9 56.9 48.9 803.4 13.4 \nSegmenter (Strudel et al., 2021) ViT-Tiny 38.0 50.7 48.7 77.4 13.3 43.5 58.2 47.1 26.8 14.7 \nSwinUperNet (Liu et al., 2021) Swin-Tiny 43.3 54.3 54.3 78.7 14.9 45.3 59.6 50.0 349.1 19.5 \nDC-Swin (Wang et al., 2022) Swin-Tiny 41.3 54.5 56.2 78.1 14.5 47.2 62.4 50.6 183.8 23.6 \nUNetFormer ResNet18 44.7 58.8 54.9 79.6 20.1 46.0 62.5 52.4 46.9 115.3 \n38 \n \nFig. 11. Visualization comparisons on the LoveDA validation set. \n\n39 \n5. Discussion \n5.1 Global-local context \nThe advantage of the dual -branch structure of the proposed efficient global-local attention is \nthat it can extract sufficient global contextual  information while preserving fine -grained local \ninformation. To demonstrate this, we visualise the feature maps from the efficient global -local \nattention in Fig. 12. As can be seen, the local context extracted by the local branch preserves the \nabundant local features but lacks spatial consistency , while the global context captured by the \nglobal branch has a more consistent character but lacks locality. Meanwhile, for the global branch, \nperforming the self -attention operation within a local window also causes jagged edges in the \nwindow context. We address this issue by employing a cross-shaped window context interaction \nmodule for context aggregation. By this m eans, the interaction between windows  is enhanced, \nthereby resolving the jaggedness issue. Notably, the extracted global -local context with both \nlocality and spatial consistency is visibly superior to the single global context or local context. \nFig. 12. V isualization of the local context, window context, global context and global -local \ncontext in the proposed efficient global-local attention. \n\n40 \n5.2 Model efficiency \nThe proposed UNetFormer adopts a hybrid structure with a CNN -based encoder and a \nTransformer-based decoder to achieve real -time performance. This hybrid design demonstrat es \nsuperiority compared to other encoder-decoder combinations (TABLE 5). Moreover, the efficient \nglobal-local attention  module utilizes the cross-shaped window context interaction module to \nreplace the shift window attention for capturing cross- window relationships, which  further \nincreases the efficiency (TABLE 2). T he superior  trade-off between accuracy and efficiency \nbrings advantages, such as the potential for the proposed UNetFormer to process real-time UA V \nimages for environmental perception and monitoring in urban areas. \n5.3 Transformer-based encoder \nAs shown in TABLEs 4 and 5, Transformers make strong encoders but greatly reduce the \nspeed. Although Transformer-based encoders are not suitable for real -time applications, \ndemonstrate advantages in pursuing high precision. Thus, we construct a fully Transformer-based \nnetwork (FT-UNetFormer) to further explore the potential of the propos ed Transformer-based \ndecoder. To compare with state -of-the-art models at a similar level, we replace the lightweight \nResNet18 encoder with he Swin Transformer (Swin-Base) (Liu et al., 2021). As listed in TABLE \n11, the FT-UNetFormer yields the state-of-the-art results (91.3% F1 score and 84.1% mIoU) on \nthe Vaihingen test set and outperforms other networks by at least 0.3%  in F1 score . For the \nPotsdam dataset, our method also achieves competitive results (TABLE 12). These results further \ndemonstrate the effectiveness of the proposed Transformer -based decoder and its potential in a  \nfully Transformer structure. \n41 \nTABLE 11. Quantitative comparison results on the Vaihingen test set with the state-of-the-art \nnetworks. \nTABLE 12 Quantitative comparison results on the Potsdam test set with state-of-the-art \nnetworks. \nMethod Backbone Imp.surf. Building Low. veg. Tree Car MeanF1 OA mIoU \nDST_5 (Sherrah, 2016) FCN 92.5 96.4 86.7 88.0 94.7 91.7 90.3 - \nV-FuseNet (Audebert et al., 2018) FuseNet 92.7 96.3 87.3 88.5 95.4 92.0 90.6 - \nSWJ_2 ResNet101 94.4 97.4 87.8 87.6 94.7 92.4 91.7 - \nAMA_1 - 93.4 96.8 87.7 88.8 96.0 92.5 91.2 - \nUFMG_4 (Nogueira et al., 2019) - 90.8 95.6 84.4 84.3 92.4 89.5 87.9 - \nS-RA-FCN (Mou et al., 2020) VGG16 91.3 94.7 86.8 83.5 94.5 90.2 88.6 82.4 \nHUSTW4 (Sun et al., 2019) ResegNets 93.6 97.6 88.5 88.8 94.6 92.6 91.6 - \nTreeUNet (Yue et al., 2019) - 93.1 97.3 86.8 87.1 95.8 92.0 90.7 - \nResUNet-a (Diakogiannis et al., 2020) - 93.5 97.2 88.2 89.2 96.4 92.9 91.5 - \nMethod Backbone Imp.surf. Building Lowveg. Tree Car MeanF1 OA mIoU \nCASIA2 (Liu et al., 2018) ResNet101 93.2 96.0 84.7 89.9 86.7 90.1 91.1 - \nV-FuseNet (Audebert et al., 2018) FuseNet 91.0 94.4 84.5 89.9 86.3 89.2 90.0 - \nDLR_9 (Marmanis et al., 2018) SegNet+VGG+FCN 92.4 95.2 83.9 89.9 81.2 88.5 90.3 - \nRoteEqNet (Marcos et al., 2018) - 89.5 94.8 77.5 86.5 72.6 84.2 87.5 - \nUFMG_4 (Nogueira et al., 2019) - 91.1 94.5 82.9 88.0 81.3 87.7 89.4 - \nHUSTW5 (Sun et al., 2019) SegNet 93.3 96.1 86.4 90.8 74.6 88.2 91.6 - \nTreeUNet (Yue et al., 2019) - 92.5 94.9 83.6 89.6 85.9 89.3 90.4 - \nEaNet (Zheng et al., 2020a) ResNet101 93.4 96.2 85.6 90.5 88.3 90.8 91.2 - \nDDCM-Net (Liu et al., 2020) ResNet50 92.7 95.3 83.3 89.4 88.3 89.8 90.4 - \nMANet (Li et al., 2021b) ResNe50 93.0 95.5 84.6 90.0 88.9 90.4 91.0 82.7 \nAFNet (Yang et al., 2021b) ResNet50+18 93.1 96.5 85.8 90.6 88.8 91.0 91.7 - \nHMANet (Niu et al., 2021) ResNet101 93.5 95.9 85.4 90.4 89.6 91.0 91.4 83.5 \nSTransFuse (Gao et al., 2021) - 88.3 91.46 79.0 85.5 77.1 78.7 86.1 66.7 \nBoTNet (Srinivas et al., 2021) ResNet50 92.2 95.3 83.9 90.0 85.5 89.4 90.5 81.1 \nSwinUperNet (Liu et al., 2021) Swin-Small 92.8 95.6 85.1 90.6 85.1 89.8 91.0 81.8 \nSwinB-CNN+BD (Zhang et al., 2022) Swin-Base 95.3 86.9 83.6 92.2 89.6 89.5 90.4 - \nDC-Swin (Wang et al., 2022) Swin-Small 93.6 96.2 85.8 90.4 87.6 90.7 91.6 83.2 \nFT-UNetFormer Swin-Base 93.5 96.0 85.6 90.8 90.4 91.3 91.6 84.1 \n42 \nDDCM-Net (Liu et al., 2020) ResNet50 92.9 96.9 87.7 89.4 94.9 92.3 90.8 - \nLANet (Ding et al., 2021) ResNet50 93.1 97.2 87.3 88.0 94.2 92.0 90.8 - \nAFNet (Yang et al., 2021b) ResNet50+18 94.1 97.6 88.7 89.7 97.1 93.4 92.1 - \nHMANet (Niu et al., 2021) ResNet101 93.9 97.6 88.7 89.1 96.8 93.2 92.2 87.3 \nSTransFuse (Gao et al., 2021) - 89.8 93.9 82.9 83.6 88.5 82.1 86.7 71.5 \nSwinB-CNN+BD (Zhang et al., 2022) Swin-Base 92.2 95.3 83.6 89.2 86.9 89.4 90.4 - \nSwinTF-FPN (Panboonyuen et al., 2021) Swin-Small 93.3 96.8 87.8 88.8 95.0 92.3 91.1 85.9 \nResT (Zhang and Yang, 2021) ResT-Base 92.7 96.1 87.5 88.6 94.8 91.9 90.6 85.2 \nFT-UNetFormer Swin-Base 93.9 97.2 88.8 89.8 96.6 93.3 92.0 87.5 \n \n6. Conclusion \nIn this paper, we  proposed a novel Transformer-based decoder and constructed a UNet-like \nTransformer (UNetFormer) for efficient semantic segmentation of remotely sensed  urban scene \nimages. Since global and local contexts are both crucial for urban scene segmentation , we \ndesigned a global -local Transformer block (GLTB)  to construct the decoder  and developed a \nfeature refinement head (FRH) to optimize the extracted global -local context . For efficient \nsegmentation, the proposed Transformer-based decoder was combined with a lightweight CNN-\nbased encoder. A comprehensive set of benchmark experiments and ablation studies on the ISPRS \nVaihingen and Potsdam datasets and the UA Vid dataset  as well as the LoveDA  dataset \ndemonstrated the effectiveness and efficiency of the proposed method for real -time urban \napplications. Furthermore, the proposed Transformer-based decoder also works well in a fully \nTransformer structure and obtains state-of-the-art performance on the Vaihingen dataset. In future \nresearch, we will continue to explore the potential and feasibility of the Transformer for geospatial \nvision tasks. \n43 \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial  interests or personal \nrelationships that could have appeared to influence the work reported in this paper. \nReferences \n \n \nAudebert, N., Le Saux, B., LefÃ¨vre, S., 2018. Beyond RGB: Very high resolution urban remote \nsensing with multimodal deep networks. ISPRS Journal of Photogrammetry and Remote Sensing \n140, 20-32. \nBadrinarayanan, V ., Kendall, A., Cipolla, R., 2017. Segnet: A deep convolutional encoder-decoder \narchitecture for image segmentation. IEEE transactions on pattern analysis and machine \nintelligence 39, 2481-2495. \nBazi, Y ., Bashmal, L., Rahhal, M.M.A., Dayil, R.A., Ajlan, N.A., 2021. Vision transformers for \nremote sensing image classification. Remote Sensing 13, 516. \nCao, H., Wang, Y ., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M., 2021. Swin-unet: Unet-like \npure transformer for medical image segmentation. arXiv preprint arXiv:2105.05537. \nChen, H., Qi, Z., Shi, Z., 2021a. Remote sensing image change detection with transformers. IEEE \nTransactions on Geoscience and Remote Sensing. \nChen, J., Lu, Y ., Yu, Q., Luo, X., Adeli, E., Wang, Y ., Lu, L., Yuille, A.L., Zhou, Y ., 2021b. \nTransunet: Transformers make strong encoders for medical image segmentation. arXiv preprint \narXiv:2102.04306. \nChen, L.- C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2014. Semantic image \nsegmentation with deep convolutional nets and fully connected crfs. arXiv preprint \narXiv:1412.7062. \nChen, L.-C., Zhu, Y ., Papandreou, G., Schroff, F., Adam, H., 2018a. Encoder-decoder with atrous \nseparable convolution for semantic image segmentation, Proceedings of the European conference \non computer vision (ECCV), pp. 801-818. \nChen, L., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L., 2018b. DeepLab: Semantic \nImage Segmentation with Deep Convolutional Nets, Atr ous Convolution, and Fully Connected \nCRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence 40, 834-848. \nDeng, P., Xu, K., Huang, H., 2021. When CNNs meet vision transformer: A joint framework for \nremote sensing scene classification. IEEE Geoscience and Remote Sensing Letters 19, 1-5. \nDiakogiannis, F.I., Waldner, F., Caccetta, P., Wu, C., 2020. Resunet-a: a deep learning framework \nfor semantic segmentation of remotely sensed data. ISPRS Journal of Photogrammetry and \nRemote Sensing 162, 94-114. \nDing, L., Tang, H., Bruzzone, L., 2021. LANet: Local Attention Embedding to Improve the \n44 \nSemantic Segmentation of Remote Sensing Images. IEEE Transactions on Geoscience and \nRemote Sensing 59, 426-435. \nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, \nM., Minderer, M., Heigold, G., Gelly, S., 2020. An image is worth 16x16 words: Transformers \nfor image recognition at scale. arXiv preprint arXiv:2010.11929. \nFu, J., Liu, J., Tian, H., Li, Y ., Bao, Y ., Fang, Z., Lu, H., 2019. Dual attention network for scene \nsegmentation, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, \npp. 3146-3154. \nGao, L., Liu, H., Yang, M., Chen, L., Wan, Y ., Xiao, Z., Qian, Y ., 2021. STransFuse: Fusing Swin \nTransformer and Convolutional Neural Network for Remote Sensing Image Semantic \nSegmentation. IEEE Journal of Selected Topics in Applied Earth Observations and Remote \nSensing 14, 10990-11003. \nGriffiths, D., Boehm, J., 2019. Improving public data for building segm entation from \nConvolutional Neural Networks (CNNs) for fused airborne lidar and image data using active \ncontours. ISPRS Journal of Photogrammetry and Remote Sensing 154, 70-83. \nGuo, Y ., Jia, X., Paull, D., 2018. Effective Sequential Classifier Training for  SVM-Based \nMultitemporal Remote Sensing Image Classification. IEEE Transactions on Image Processing 27, \n3036-3048. \nHe, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition, \nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. \nHe, X., Chen, Y ., Lin, Z., 2021. Spatial-spectral transformer for hyperspectral image classification. \nRemote Sensing 13, 498. \nHong, D., Han, Z., Yao, J., Gao, L., Zhang, B., Plaza, A., Chanussot, J., 2021. SpectralFormer: \nRethinking hyperspectral image classification with transformers. IEEE Transactions on \nGeoscience and Remote Sensing. \nHu, P., Perazzi, F., Heilbron, F.C., Wang, O., Lin, Z., Saenko, K., Sclaroff, S., 2020. Real -time \nsemantic segmentation with fast attention. IEEE Robotics and Automation Letters 6, 263-270. \nHuang, Z., Wang, X., Wei, Y ., Huang, L., Shi, H., Liu, W., Huang, T.S., 2020. CCNet: Criss-Cross \nAttention for Semantic Segmentation. IEEE Transactions on Pattern Analysis and Machine \nIntelligence. \nKampffmeyer, M., Salberg, A.-B., Jenssen, R., 2016. Semantic segmentation of small objects and \nmodeling of uncertainty in urban remote sensing images using deep convolutional neural \nnetworks, Proceedings of the IEEE conference on computer vision and pattern recognition \nworkshops, pp. 1-9. \nKemker, R., Salvaggio, C., Kanan, C., 2018. Algorithms for semantic segmentation of \nmultispectral remote sensing imagery using deep learning. ISPRS journal of photogrammetry and \nremote sensing 145, 60-77. \nKirillov, A., Girs hick, R., He, K., DollÃ¡r, P ., 2019. Panoptic feature pyramid networks, \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. \n6399-6408. \nKotaridis, I., Lazaridou, M., 2021. Remote sensing image segmentation advances: A meta -\nanalysis. ISPRS Journal of Photogrammetry and Remote Sensing 173, 309-322. \nKrÃ¤henbÃ¼hl, P., Koltun, V ., 2011. Efficient inference in fully connected crfs with gaussian edge \n45 \npotentials. Advances in neural information processing systems 24, 109-117. \nLeCun, Y ., Bengio, Y ., Hinton, G.J.n., 2015. Deep learning. Nature 521, 436-444. \nLi, G., Yun, I., Kim, J., Kim, J., 2019. Dabnet: Depth -wise asymmetric bottleneck for real -time \nsemantic segmentation. arXiv preprint arXiv:1907.11357. \nLi, H., Qiu, K., Chen, L., Mei, X., Hong, L., Tao, C., 2020. SCAttNet: Semantic segmentation \nnetwork with spatial and channel attention mechanism for high-resolution remote sensing images. \nIEEE Geoscience and Remote Sensing Letters 18, 905-909. \nLi, Q., Chen, Y ., Zeng, Y ., 2022a. Transformer with Transfer CNN for Remote -Sensing-Image \nObject Detection. Remote Sensing 14, 984. \nLi, R., Zheng, S., Duan, C., Su, J., Zhang, C., 2021a. Multistage Attention ResU-Net for Semantic \nSegmentation of Fine-Resolution Remote Sensing Images. IEEE Geoscience and Remote Sensing \nLetters. \nLi, R., Zheng, S., Duan, C., Wang, L., Zhang, C., 2022b. Land cover classification from remote \nsensing images based on multi -scale fully convolutional network. Geo -spatial Information \nScience, 1-17. \nLi, R., Zheng, S., Zhang, C ., Duan, C., Su, J., Wang, L., Atkinson, P.M., 2021b. Multiattention \nnetwork for semantic segmentation of fine-resolution remote sensing images. IEEE Transactions \non Geoscience and Remote Sensing. \nLi, R., Zheng, S., Zhang, C., Duan, C., Wang, L., Atkinson, P.M., 2021c. ABCNet: Attentive \nbilateral contextual network for efficient semantic segmentation of Fine -Resolution remotely \nsensed imagery. ISPRS Journal of Photogrammetry and Remote Sensing 181, 84-98. \nLiu, Q., Kampffmeyer, M., Jenssen, R., Salberg, A.- B., 2020. Dense dilated convolutions â€™ \nmerging network for land cover classification. IEEE Transactions on Geoscience and Remote \nSensing 58, 6309-6320. \nLiu, Y ., Fan, B., Wang, L., Bai, J., Xiang, S., Pan, C., 2018. Semantic labeling in very high \nresolution images via a self -cascaded convolutional neural network. ISPRS journal of \nphotogrammetry and remote sensing 145, 78-95. \nLiu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., Guo, B., 2021. Swin transformer: \nHierarchical vision transformer using shifted windows, Proceedings of the IEEE/CVF \nInternational Conference on Computer Vision, pp. 10012-10022. \nLong, J., Shelhamer, E., Darrell, T., 2015. Fully convolutional networks for semantic \nsegmentation, Proceedings of the IEEE conference on computer vision and pattern recognition, \npp. 3431-3440. \nLyu, Y ., V osselman, G., Xia, G.- S., Yilmaz, A., Yang, M.Y ., 2020. UA Vid: A semantic \nsegmentation dataset for UA V imagery. ISPRS Journal of Photogrammetry and Remote Sensing \n165, 108-119. \nMa, A., Wang, J., Zhong, Y ., Zheng, Z., 2021. Factseg: Foreground activation-driven small object \nsemantic segmentation in large-scale remote sensing imagery. IEEE Transactions on Geoscience \nand Remote Sensing. \nMa, L., Liu, Y ., Zhang, X., Ye, Y ., Yin, G., Johnson, B.A., 2019. Deep learning in remote sensing \napplications: A meta-analysis and review. ISPRS Journal of Photogrammetry and Remote Sensing \n152, 166-177. \nMaggiori, E., Tarabalka, Y ., Charpiat, G., Alliez, P., 2016. Convolutional neural networks for \n46 \nlarge-scale remote-sensing image classification. IEEE Transactions on Geoscience and Remote \nSensing 55, 645-657. \nMarcos, D., V olpi, M., Kellenberger, B., Tuia, D., 2018. Land cover mapping at very high \nresolution with rotation equivariant CNNs: Towards small yet accurate models. ISPRS Journal of \nPhotogrammetry and Remote Sensing 145, 96-107. \nMarmanis, D., Schindler, K., Wegner, J.D., Galliani, S., Datcu, M., Stilla, U., 2018. Classification \nwith an edge: Improving semantic image segmentation with boundary detection. ISPRS Journal \nof Photogrammetry and Remote Sensing 135, 158-172. \nMou, L., Hua, Y ., Zhu, X.X., 2020. Relation Matters: Relational Context -Aware Fully \nConvolutional Network for Semantic Segmentation of High- Resolution Aerial Images. IEEE \nTransactions on Geoscience and Remote Sensing 58, 7557-7569. \nNiu, R., Sun, X., Tian, Y ., Diao, W., Chen, K., Fu, K., 2021. Hybrid multiple attention network \nfor semantic segmentation in aerial images. IEEE Transactions on Geoscience an d Remote \nSensing 60, 1-18. \nNogueira, K., Dalla Mura, M., Chanussot, J., Schwartz, W.R., Dos Santos, J.A., 2019. Dynamic \nmulticontext segmentation of remote sensing images based on convolutional networks. IEEE \nTransactions on Geoscience and Remote Sensing 57, 7503-7520. \nOrÅ¡iÄ‡, M., Å egviÄ‡, S., 2021. Efficient semantic segmentation with pyramidal fusion. Pattern \nRecognition 110, 107611. \nPal, M., 2005. Random forest classifier for remote sensing classification. International Journal of \nRemote Sensing 26, 217-222. \nPanboonyuen, T., Jitkajornwanich, K., Lawawirojwong, S., Srestasathiern, P., Vateekul, P., 2021. \nTransformer-Based Decoder Designs for Semantic Segmentation on Remotely Sensed Images. \nRemote Sensing 13, 5100. \nPicoli, M.C.A., Camara, G., Sanches, I., Sim Ãµes, R., Carvalho, A., Maciel, A., Coutinho, A., \nEsquerdo, J., Antunes, J., Begotti, R.A., 2018. Big earth observation time series analysis for \nmonitoring Brazilian agriculture. ISPRS journal of photogrammetry and remote sensing 145, 328-\n339. \nPoudel, R.P., Bonde, U., Liwicki, S., Zach, C., 2018. Contextnet: Exploring context and detail for \nsemantic segmentation in real-time. arXiv preprint arXiv:1805.04554. \nPoudel, R.P., Liwicki, S., Cipolla, R., 2019. Fast-scnn: Fast semantic segmentation network. arXiv \npreprint arXiv:1902.04502. \nRomera, E., Alvarez, J.M., Bergasa, L.M., Arroyo, R., 2017. Erfnet: Efficient residual factorized \nconvnet for real -time semantic segmentation. IEEE Transactions on Intelligent Transportation \nSystems 19, 263-272. \nRonneberger, O., Fi scher, P., Brox, T., 2015. U -Net: Convolutional Networks for Biomedical \nImage Segmentation. Springer International Publishing, Cham, pp. 234-241. \nSamie, A., Abbas, A., Azeem, M.M., Hamid, S., Iqbal, M.A., Hasan, S.S., Deng, X., 2020. \nExamining the impacts of future land use/land cover changes on climate in Punjab province, \nPakistan: implications for environmental sustainability and economic growth. Environmental \nScience and Pollution Research 27, 25415-25433. \nShamsolmoali, P., Zareapoor, M., Zhou, H., Wang, R., Yang, J., 2020. Road segmentation for \nremote sensing images using adversarial spatial pyramid networks. IEEE Transactions on \n47 \nGeoscience and Remote Sensing. \nShen, Y ., Chen, J., Xiao, L., Pan, D., 2019. Optimizing multiscale segmentation with local spectral \nheterogeneity measure for high resolution remote sensing images. ISPRS Journal of \nPhotogrammetry and Remote Sensing 157, 13-25. \nSherrah, J., 2016. Fully convolutional networks for dense semantic labelling of high -resolution \naerial imagery. arXiv preprint arXiv:1606.02585. \nSrinivas, A., Lin, T.- Y ., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A., 2021. Bottleneck \ntransformers for visual recognition, Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition, pp. 16519-16529. \nStrudel, R., Garcia, R., Laptev, I., Schmid, C., 2021. Segmenter: Transformer for semantic \nsegmentation, Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. \n7262-7272. \nSudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Jorge Cardoso, M ., 2017. Generalised dice \noverlap as a deep learning loss function for highly unbalanced segmentations, Deep learning in \nmedical image analysis and multimodal learning for clinical decision support. Springer, pp. 240-\n248. \nSun, Y ., Tian, Y ., Xu, Y ., 2019. Problems of encoder-decoder frameworks for high -resolution \nremote sensing image segmentation: Structural stereotype and insufficient learning. \nNeurocomputing 330, 297-304. \nTan, M., Pang, R., Le, Q.V ., 2020. Efficientdet: Sc alable and efficient object detection, \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10781-\n10790. \nTong, X.- Y ., Xia, G.- S., Lu, Q., Shen, H., Li, S., You, S., Zhang, L., 2020. Land- cover \nclassification with high -resolution remote sensing images using transferable deep models. \nRemote Sensing of Environment 237, 111322. \nVakalopoulou, M., Karantzalos, K., Komodakis, N., Paragios, N., 2015. Building detection in \nvery high resolution multispectral data with deep learning f eatures, 2015 IEEE international \ngeoscience and remote sensing symposium (IGARSS). IEEE, pp. 1873-1876. \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å., \nPolosukhin, I., 2017. Attention is all you need, Advances in neural information processing systems, \npp. 5998-6008. \nWang, J., Zheng, Z., Ma, A., Lu, X., Zhong, Y ., 2021a. LoveDA: A Remote Sensing Land-Cover \nDataset for Domain Adaptive Semantic Segmentation. arXiv preprint arXiv:2110.08733. \nWang, L., Li, R., Duan, C., Z hang, C., Meng, X., Fang, S., 2022. A Novel Transformer Based \nSemantic Segmentation Scheme for Fine-Resolution Remote Sensing Images. IEEE Geoscience \nand Remote Sensing Letters 19, 1-5. \nWang, L., Li, R., Wang, D., Duan, C., Wang, T., Meng, X., 2021b. Transformer Meets Convolution: \nA Bilateral Awareness Network for Semantic Segmentation of Very Fine Resolution Urban Scene \nImages. Remote Sensing 13, 3065. \nWang, X., Girshick, R., Gupta, A., He, K., 2018. Non-local neural networks, Proceedings of the \nIEEE conference on computer vision and pattern recognition, pp. 7794-7803. \nXie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P., 2021. SegFormer: Simple and \nefficient design for semantic segmentation with transformers. Advances in Neural Information \n48 \nProcessing Systems 34. \nXing, J., Sieber, R., Caelli, T., 2018. A scale-invariant change detection method for land use/cover \nchange research. ISPRS Journal of Photogrammetry and Remote Sensing 141, 252-264. \nXu, W., Xu, Y ., Chang, T., Tu, Z., 2021. Co- Scale Con v-Attentional Image Transformers. \nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 9981-9990. \nYang, M.Y ., Kumaar, S., Lyu, Y ., Nex, F., 2021a. Real-time Semantic Segmentation with Context \nAggregation Network. ISPRS Journal of Photogrammetry and Remote Sensing 178, 124-134. \nYang, X., Li, S., Chen, Z., Chanussot, J., Jia, X., Zhang, B., Li, B., Chen, P., 2021b. An attention-\nfused network for semantic segmentation of very-high-resolution remote sensing imagery. ISPRS \nJournal of Photogrammetry and Remote Sensing 177, 238-262. \nYin, H., Pflugmacher, D., Li, A., Li, Z., Hostert, P ., 2018. Land use and land cover change in Inner \nMongolia-understanding the effects of China's re -vegetation programs. Remote Sensing of \nEnvironment 204, 918-930. \nYu, C., Gao, C., Wang, J., Yu, G., Shen, C., Sang, N., 2020. Bisenet v2: Bilateral network with \nguided aggregation for real-time semantic segmentation. arXiv preprint arXiv:2004.02147. \nYu, C., Wang, J., Peng, C., Gao, C., Yu, G., Sang, N., 2018. Bise net: Bilateral segmentation \nnetwork for real -time semantic segmentation, Proceedings of the European conference on \ncomputer vision (ECCV), pp. 325-341. \nYuan, Y ., Chen, X., Wang, J., 2020. Object-contextual representations for semantic segmentation, \nComputer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, \nProceedings, Part VI 16. Springer, pp. 173-190. \nYue, K., Yang, L., Li, R., Hu, W., Zhang, F., Li, W., 2019. TreeUNet: Adaptive Tree convolutional \nneural networks for subdecimeter aerial image segmentation. ISPRS Journal of Photogrammetry \nand Remote Sensing 156, 1-13. \nZhang, C., Atkinson, P.M., George, C., Wen, Z., Diazgranados, M., Gerard, F., 2020a. Identifying \nand mapping individual plants in a highly diverse high-elevation ecosystem using UA V imagery \nand deep learning. ISPRS Journal of Photogrammetry and Remote Sensing 169, 280-291. \nZhang, C., Harrison, P.A., Pan, X., Li, H., Sargent, I., Atkinson, P.M., 2020b. Scale Sequence \nJoint Deep Learning (SS -JDL) for land use and land cover classification. Remote Sensing of \nEnvironment 237, 111593. \nZhang, C., Jiang, W.S., Zhang, Y ., Wang, W., Zhao, Q., Wang, C.J., 2022. Transformer and CNN \nHybrid Deep Neural Network for Semantic Segmentation of Very -high-resolution Remote \nSensing Imagery. IEEE Transactions on Geoscience and Remote Sensing. \nZhang, Q., Yang, Y ., 2021. ResT: An Efficient Transformer for Visual Recognition. arXiv preprint \narXiv:2105.13677. \nZhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017a. Pyramid scene parsing network, Proceedings of \nthe IEEE conference on computer vision and pattern recognition, pp. 2881-2890. \nZhao, W., Du, S., 2016. Learning multiscale and deep representations for classifying remotely \nsensed imagery. ISPRS Journal of Photogrammetry and Remote Sensing 113, 155-165. \nZhao, W., Du, S., Wang, Q., Emery, W.J., 2017b. Contextually guided very- high-resolution \nimagery classification with semantic segments. ISPRS Journal of Photogrammetry and Remote \nSensing 132, 48-60. \nZheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y ., Fu, Y ., Feng, J., Xiang, T., Torr, P.H., 2021. \n49 \nRethinking semantic segmentation from a sequence- to-sequence perspective with transformers, \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. \n6881-6890. \nZheng, X., Huan, L., Xia, G.-S., Gong, J., 2020a. Parsing very high resolution urban scene images \nby learning deep ConvNets with edge-aware loss. ISPRS Journal of Photogrammetry and Remote \nSensing 170, 15-28. \nZheng, Z., Zhong, Y ., Wang, J., Ma, A., 2020b. Foreground-aware relation network for geospatial \nobject segmentation in high spatial resolution remote sensing imagery, Proceedings of the \nIEEE/CVF conference on computer vision and pattern recognition, pp. 4096-4105. \nZhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., 2018. Unet++: A nested u-net architecture \nfor medical image segmentation, Deep Learning in Medical Image Analysis and Multimodal \nLearning for Clinical Decision Support. Springer, pp. 3-11. \nZhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J., 2020. Deformable DETR: Deformable \nTransformers for End-to-End Object Detection. arXiv preprint arXiv:2010.04159. \nZhu, X.X., Tuia, D., Mou, L., Xia, G.-S., Zhang, L., Xu, F., Fraundorfer, F., 2017. Deep learning \nin remote sensing: A comprehensive review and list of resources. IEEE Geoscience and Remote \nSensing Magazine 5, 8-36. \nZhu, Z., Xu, M., Bai, S., Huang, T., Bai, X., 2019. Asymmetric non- local neural networks for \nsemantic segmentation, Proceedings of the IEEE/CVF  International Conference on Computer \nVision, pp. 593-602. \nZhuang, J., Yang, J., Gu, L., Dvornek, N., 2019. Shelfnet for fast semantic segmentation, \nProceedings of the IEEE/CVF International Conference on Computer Vision Workshops, pp. 0-0. \n "
}