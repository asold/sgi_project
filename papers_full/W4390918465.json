{
  "title": "YOLO-based CAD framework with ViT transformer for breast mass detection and classification in CESM and FFDM images",
  "url": "https://openalex.org/W4390918465",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3112549832",
      "name": "Nada M. Hassan",
      "affiliations": [
        "Arab Academy for Science, Technology, and Maritime Transport"
      ]
    },
    {
      "id": "https://openalex.org/A2131264066",
      "name": "Safwat Hamad",
      "affiliations": [
        "Ain Shams University",
        "Saint Mary's College of California"
      ]
    },
    {
      "id": "https://openalex.org/A2211471628",
      "name": "Khaled Mahar",
      "affiliations": [
        "Arab Academy for Science, Technology, and Maritime Transport"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4300690596",
    "https://openalex.org/W2129835033",
    "https://openalex.org/W3140854437",
    "https://openalex.org/W4285049160",
    "https://openalex.org/W2885195348",
    "https://openalex.org/W4313583499",
    "https://openalex.org/W3096265120",
    "https://openalex.org/W2090139176",
    "https://openalex.org/W4293163051",
    "https://openalex.org/W4294975187",
    "https://openalex.org/W4362603432",
    "https://openalex.org/W4220891613",
    "https://openalex.org/W2898907031",
    "https://openalex.org/W2626257914",
    "https://openalex.org/W1998865404",
    "https://openalex.org/W2024124973",
    "https://openalex.org/W4287010178",
    "https://openalex.org/W2896811665",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2163922914",
    "https://openalex.org/W2908052439",
    "https://openalex.org/W4210395586",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W2755773539",
    "https://openalex.org/W3005203394",
    "https://openalex.org/W3170769871",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2964189045",
    "https://openalex.org/W3018760760",
    "https://openalex.org/W3137419840",
    "https://openalex.org/W2963299996",
    "https://openalex.org/W3004137453",
    "https://openalex.org/W4283010142",
    "https://openalex.org/W2801188218",
    "https://openalex.org/W2791942584",
    "https://openalex.org/W2898319174",
    "https://openalex.org/W2753693376",
    "https://openalex.org/W3010419441",
    "https://openalex.org/W2057121930",
    "https://openalex.org/W2133059825",
    "https://openalex.org/W2058333183",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W4386076325",
    "https://openalex.org/W4205900964",
    "https://openalex.org/W3131524208",
    "https://openalex.org/W4376614709",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W4377241880",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W4312443924",
    "https://openalex.org/W4224318308",
    "https://openalex.org/W4206930139",
    "https://openalex.org/W2587761589",
    "https://openalex.org/W2809504579",
    "https://openalex.org/W2745869571",
    "https://openalex.org/W3033750579",
    "https://openalex.org/W4207057737",
    "https://openalex.org/W3152157423",
    "https://openalex.org/W3103895001"
  ],
  "abstract": "Abstract Breast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of the lesions’ size and shape, especially with the existence of high fibro-glandular tissues. The revolution of deep learning and computer vision contributes recently in introducing systems that can provide an automated diagnosis for breast cancer that can act as a second opinion for doctors/radiologists. The most of previously proposed deep learning-based Computer-Aided Diagnosis (CAD) systems mainly utilized Convolutional Neural Networks (CNN) that focuses on local features. Recently, vision transformers (ViT) have shown great potential in image classification tasks due to its ability in learning the local and global spatial features. This paper proposes a fully automated CAD framework based on YOLOv4 network and ViT transformers for mass detection and classification of Contrast Enhanced Spectral Mammography (CESM) images. CESM is an evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast tissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that are INbreast and CDD-CESM that provides both FFDM and CESM images. The model achieved at mass detection a mean Average Precision (mAP) score of 98.69%, 81.52%, and 71.65% and mass classification accuracy of 95.65%, 97.61%, and 80% for INbreast, CE-CESM, and DM-CESM, respectively. The proposed framework showed competitive results regarding the state-of-the-art models in INbreast. It outperformed the previous work in the literature in terms of the F1-score by almost 5% for mass detection in CESM. Moreover, the experiments showed that the CESM could provide more morphological features that can be more informative, especially with the highly dense breast tissues.",
  "full_text": "ORIGINAL ARTICLE\nYOLO-based CAD framework with ViT transformer for breast mass\ndetection and classification in CESM and FFDM images\nNada M. Hassan1 • Safwat Hamad2,3 • Khaled Mahar4\nReceived: 7 December 2022 / Accepted: 7 December 2023 / Published online: 16 January 2024\n/C211The Author(s) 2024\nAbstract\nBreast cancer detection is considered a challenging task for the average experienced radiologist due to the variation of the\nlesions’ size and shape, especially with the existence of high ﬁbro-glandular tissues. The revolution of deep learning and\ncomputer vision contributes recently in introducing systems that can provide an automated diagnosis for breast cancer that\ncan act as a second opinion for doctors/radiologists. The most of previously proposed deep learning-based Computer-Aided\nDiagnosis (CAD) systems mainly utilized Convolutional Neural Networks (CNN) that focuses on local features. Recently,\nvision transformers (ViT) have shown great potential in image classiﬁcation tasks due to its ability in learning the local and\nglobal spatial features. This paper proposes a fully automated CAD framework based on YOLOv4 network and ViT\ntransformers for mass detection and classiﬁcation of Contrast Enhanced Spectral Mammography (CESM) images. CESM is\nan evolution type of Full Field Digital Mammography (FFDM) images that provides enhanced visualization for breast\ntissues. Different experiments were conducted to evaluate the proposed framework on two different datasets that are\nINbreast and CDD-CESM that provides both FFDM and CESM images. The model achieved at mass detection a mean\nAverage Precision (mAP) score of 98.69%, 81.52%, and 71.65% and mass classiﬁcation accuracy of 95.65%, 97.61%, and\n80% for INbreast, CE-CESM, and DM-CESM, respectively. The proposed framework showed competitive results\nregarding the state-of-the-art models in INbreast. It outperformed the previous work in the literature in terms of the F1-\nscore by almost 5% for mass detection in CESM. Moreover, the experiments showed that the CESM could provide more\nmorphological features that can be more informative, especially with the highly dense breast tissues.\nKeywords CESM /C1 Vision transformer /C1 Mass detection /C1 Mass classiﬁcation\nAbbreviations\nACR-BIRADS American College of Radiology Breast\nImaging Reporting and Data System\nACS American Cancer Society\nAUC Area Under Curve\nCAD Computer-Aided (Diagnoses/Detection)\nCBIS-DDSM Curated Breast Imaging Subset of DDSM\nCC Carnio Caudal\nCDD-CESM Categorized Digital Database for Low\nenergy and Subtracted Contrast\nEnhanced Spectral Mammography\nCE Contrast Enhanced\nCESM Contrast Enhanced Spectral\nMammography\nCL Clip limit\nCLAHE Clip limit adaptive histogram\nequalization\nCNN Convolutional Neural Networks\nCSP-Darknet Cross Stage Partial Darknet\nCSV Comma-Separated Values\nDEiT Data Efﬁcient Transformer\nDM Digital Mammography\nELM Extreme Learning Machine\nFaster-RCNN Faster Region-Convolutional Neural\nNetwork\nFFDM Full Field Digital Mammography\nFN False Negative\n& Nada M. Hassan\nnadamahmoud@aast.edu\n1 College of Computing and Information Technology, Arab\nAcademy for Science and Technology, Cairo, Egypt\n2 Faculty of Computer and Information Sciences, Ain Shams\nUniversity, Cairo, Egypt\n3 Saint Mary’s College of California, Moraga, CA 94575, USA\n4 College of Computing and Information Technology, Arab\nAcademy for Science and Technology, Alexandria, Egypt\n123\nNeural Computing and Applications (2024) 36:6467–6496\nhttps://doi.org/10.1007/s00521-023-09364-5(0123456789().,-volV)(0123456789().,- volV)\nFNR False Positive Rate\nFP False Positive\nFPR False Positive Rate\nFRCN Full Resolution Convolutional Network\nGLCM Gray Level Co-occurrence Matrix\nIoU Intersection over Union\nLWT Lifting Wavelet Transform\nmAP Mean Average Precision\nMIAS Ammographic Image Analysis Society\nMLO Medio Lateral Oblique\nMLP Multilayer Perceptron\nNLP Nature Language Processing\nROC Receiver Operating Characteristics\nROI Region of Interest\nSFM Screen Film Mammography\nSVM Support Vector Machine\nTN True Negative\nTP True Positive\nTPR True Positive Rate\nViT Vision transformer\nYOLO You Look Only Once\n1 Introduction\nCancer diseases are considered one of the most common\ndiseases worldwide; breast cancer is one of the cancers that\nhits women hugely. Breast cancer develops in the breast\ntissues and occurs when the cells grow abnormally, form-\ning a mass or lump, as these cells divide themselves rapidly\ncompared to the other healthy ones.\nThe American Cancer Society (ACS) published an\nestimation for the breast cancer cases among females in the\nUSA only (2022), and it stated that about 290,560 new\ncases would be diagnosed with breast cancer and 43,780\nwould die. Most cases mainly occur between 45 and\n62 years old [ 1]. The substantial support for awareness\nabout the risks of breast cancer helped a lot in the early\ndiagnosis of this disease. This happens through encourag-\ning regular check-ups, which can be done via different\napproaches, such as regular screening. There are various\nmodalities for breast screening, but mammogram is the\nmost widely used and effective one. Although screening\ncannot prevent breast cancer, it can help a lot in detecting\nthe lesions early, even before the symptoms start to\ndevelop.\nMammograms are X-ray pictures of the breast with two\ndifferent views, Medio Lateral Oblique (MLO) and Carnio\nCaudal (CC), for each breast side. Various abnormalities\ncan be detected through mammogram interpretation; these\nabnormalities can be classiﬁed into masses and calciﬁca-\ntions. Those abnormalities are diagnosed as benign or\nmalignant according to their appearance and morphological\nfeatures such as shape and pattern. Benign masses have an\noval or circular shape with well-deﬁned edges, while\nmalignant masses look like it has spikes out from their\ncenter.\nThe accurate interpretation of the mammogram leads to\nprecise diagnosis; Radiologists spend a lot of time and\neffort in the interpretation process. Interpretation of many\ncases can cause inaccurate diagnosing for some cases,\nespecially with radiologists with fewer years of experience\n[2].\nWith the rapid development in machine learning and\ncomputer vision techniques, and because of the above\nproblems, Computer-Aided Diagnosis (CAD) systems\nappeared and developed in the past decades. These systems\nare developed to process different forms of data such as\nmedical images, clinical data, and genetic data. Recently,\ndeep learning models have made these systems work more\nconsistently, especially now there are a variety of neural\nnetwork architectures that can be used in different ways\ndepending on the data type in order to get the most out of\nthem. For example, CNN models ﬁt more the medical\nimaging tasks [ 3], while RNN [ 4, 5] models seem to be\nmore effective with the sequential data such as genetic data\n[6]. Therefore, the data type is critical in determining\nwhich architecture will be used to design the CAD system.\nThis work mainly focuses on the mammographic CAD\nsystems. Mammographic CAD systems are divided into\nComputer-Aided Diagnosis and Computer-Aided Detec-\ntion systems; the ﬁrst one mainly focuses on interpreting\nthe predeﬁned abnormality (benign or malignant). On the\nother hand, the second type of these systems aims to detect\nand localize the abnormality within mammography. Dif-\nferent factors make detecting and classifying the masses\nvery challenging for the CAD system, such as shape, mass\nsize variation ranging from too small to very large, and the\nnature of the breast tissues that sometimes mask the mas-\nses, especially with the highly dense breast tissues.\nMammograms were generated using systems based on\nphosphorescent screen-ﬁlm technology until the US Food\nand Drug Administration approved the Full Field Digital\nMammography (FFDM) systems. The FFDM systems have\nsigniﬁcantly improved the quality of mammographic ima-\nges and the sensitivity of breast cancer detection. Even the\nmodels that were trained on FFDM images provided better\nresults than the ones provided through the Screen Film\nMammography technique (SFM) [ 7]. However, the con-\ntrast of the FFDM images depends on the differences\nbetween breast tissues; those images can provide just\nstructural information. One of the limitations of this type of\n6468 Neural Computing and Applications (2024) 36:6467–6496\n123\nmammographic image is that the highly dense breast tis-\nsues can mask the mass, as most of the ﬁbro-glandular\ntissues have the same image gray levels of the lesions. This\ncan lower the sensitivity in detecting tumors when breast\ndensity increases [ 8]. Recently, Contrast Enhanced Spec-\ntral Mammography (CESM) was introduced in 2011 as a\nnew image technique for mammogram screening. CESM\nprovides improved visualization for mammographic ima-\nges by combining low and high-energy breast images.\nFigure 1 illustrates the difference between the FFDM and\nCESM images and how the CESM can provide more\nmorphological features that can enhance breast cancer\ndetection, especially with masked masses in highly dense\nbreasts. Although the crucial information that these images\ncan provide clinically, few studies have been proposed for\ndeveloping deep learning-based CAD systems in CESM\nimages. Almost all of the previously introduced studies by\nresearchers used FFDM images or SFM images [ 9, 10].\nThese various studies proposed different models for\nmass detection and classiﬁcation based on different deep\nlearning techniques and with the use of transfer learning\nconcept. As deep learning has great capability to auto-\nmatically extract the deep features from the mammograms\nwith no need for hand-crafted features, in addition to that,\ntransfer learning reduces the computational cost and the\nneed of large datasets for training.\nConvolutional Neural Networks (CNN) ruled the\ndetection and classiﬁcation tasks in medical imaging\ndiagnosis through the last few years in the most of these\nstudies, relying on the idea of the dependency on the\nimmediate neighboring pixels which represent the local\nfeatures of the image (such as color, contrast, etc.). This\nallows the model to learn and extract the essential features\nand edges only without learning the details of each pixel\nand the global context of the features. However, learning\nthe entire image, rather than the parts that the ﬁlter extracts,\nmay increase the chances of obtaining better performance\nfrom the model.\nRecently, vision transformers have shown competitive\nperformance compared to CNNs; vision transformers are\nbuilt on the attention mechanism, which focuses on the\nlocal and global spatial features [ 11]. Few studies adopted\nthe ViT transformers in medical imaging tasks [ 12–14]\nespecially in breast cancer diagnosis. Furthermore, few\nworks proposed a fully automated model for detecting and\nclassifying the masses. Based on the aforementioned\npoints, this work proposes a fully automated framework in\nan end-to-end training fashion for mass detection and\nclassiﬁcation based on YOLOv4 and ViT transformers in\nCESM mammographic images and FFDM images. This\nwork leverages the ViT transformer to extract the global\nfeatures alongside with the local features to enhance the\naccuracy of the diagnosis.\nMoreover, the experiments are designed to explore the\nefﬁcacy of the automated interpretation of CESM and how\nit can increase the sensitivity of breast cancer detection\nmore than FFDM.\nThe novelty and contributions of this work can be\nsummarized in the following points:\n1. Integrate the YOLOv4 with ViT transformers for mass\ndetection and classiﬁcation, to utilize the capability of\nViT in learning the local and global spatial features in\nthe mass classiﬁcation instead of the CNN models.\n2. To the best of our knowledge, this is the ﬁrst fully\nautomated CAD framework for mass detection and\nFig. 1 a Low-energy FFDM\nimage shows negative ﬁndings;\nb recombination CESM image\nthat clearly shows the existence\nof mass [ 8]\nNeural Computing and Applications (2024) 36:6467–6496 6469\n123\nclassiﬁcation in CESM images, speciﬁcally in the\nCDD-CESM dataset [ 15].\n3. Evaluate how CESM can have the potential to enhance\nthe diagnostic accuracy comparing to Digital Mam-\nmography (DM).\n4. Assess the performance of YOLOv4, YOLOv7, and\nYOLOv8 in mass detection on CDD-CESM and\nINbreast dataset based on a comprehensive evaluation\nthrough different experiments.\n5. Conduct different experiments to evaluate and compare\nthe performance of different vision transformer models\nand different CNN models at mass classiﬁcation in\nmammograms.\n6. Utilize the newly introduced CDD-CESM dataset [ 15]\nfor mass detection and classiﬁcation to evaluate the\nperformance of the proposed model.\nThis paper is organized as follows: Sect. 2 presents the\nliterature survey, while Sect. 3 demonstrates the methods\nand materials employed in this work. Section 4 shows the\nexperimental design and results, and Sect. 5 discusses the\nresults. Finally, Sect. 6 presents the conclusion, while\nSect. 7 discusses the advantages, limitations, and directions\nfor future work.\n2 Related work\nThe rapid development of deep learning techniques hugely\naffected the researchers’ contribution in developing more\naccurate CAD systems. Over the past years, many attempts\nhave been made to introduce reliable systems for breast\ncancer diagnosis and prognosis, especially with the\ndependency on using mammograms as a ﬁrst tool for the\ninitial diagnosis. The proposed techniques in this literature\nmainly focused on one or more of three tasks; detection,\nsegmentation, and classiﬁcation.\nDetection can be described as the process of localizing\nthe abnormal area or spots within the mammogram images,\nwhile segmentation mainly targets the pixel-by-pixel\nannotation of the abnormal ﬁndings; ﬁnally, the classiﬁ-\ncation is considered as the process of classifying the ﬁnd-\nings into (Normal/Abnormal) or (Benign/Malignant). Some\nstudies focused on observing the morphological features\n(texture, color, brightness, etc.) in their works to extract the\nROIs and then classify them into benign or malignant using\nfeature-based/conventional machine learning techniques\n[16–20].\nHowever, feature-based techniques have been used for a\nlong time. Still, they have some drawbacks as those tech-\nniques mainly depend on classical feature engineering that\nis affected by different factors such as the subject knowl-\nedge of the developer, his intuition, and his skills in the\nmathematical models. This process is considered a time-\nconsuming process; moreover, this may not capture all the\nrelevant features in the image as those techniques cannot\nautomatically learn the most discriminant features [ 21].\nFurthermore, feature-based techniques are often designed\nto capture speciﬁc characteristics or patterns through the\ntraining phase. Accordingly, this may not generalize well\nto unseen data, especially since those techniques struggle\nwith large and high-dimensional feature space datasets; and\nthis can lead to more computational overhead.\nOn the other side, with the appearance of deep learning,\nCNNs replaced the traditional hand engineering approa-\nches for feature extraction, as the CNNs can automatically\nextract and learn complex features in more detail and in a\nmore efﬁcient way that ﬁts the required task [ 22, 23]. The\ninitial convolutional layers in the deep CNN can effectively\nextract the low-level features, while subsequent layers\npropagate these features to extract more complex and\nabstract features. Through the training process, the ﬁlters\nand pooling operations automatically select the most dis-\ncriminant and informative features. Using deep learning in\nfeature extraction provides beneﬁts that can overcome the\nproblems of feature-based techniques. Automating the\nfeature extraction process through deep learning saves the\ntime and effort needed to extract hand-crafted features. The\ndeep learning techniques capture the features at multiple\nlevels of abstraction, and this hierarchal representation\nallows more informative features to be extracted. More-\nover, transfer learning reduces the computational cost of\nlearning the features from scratch; as the pre-trained\nmodels can be used as a feature extractor for related tasks\n[24].\nDifferent studies adopted various architectures in their\ndeveloped systems that integrates CNN with conventional\nmachine learning techniques. In [ 25], they proposed a\nsystem based on CNN and SVM for mass and microcal-\nciﬁcation segmentation and classiﬁcation that achieved a\nclassiﬁcation accuracy of 80.5% and 87.2% on DDSM and\nCBIS-DDSM, respectively. One of the points that needs to\nbe investigated in this work is employing a deep learning\nmodel for the segmentation phase to allow the model to\nlearn more discriminant features about masses and calci-\nﬁcations; and accordingly, this may enhance the results.\nIn [ 26], the authors also proposed a CAD system for\nwhole mammogram classiﬁcation based on deep CNNs for\nfeature extraction and SVM for classiﬁcation. They con-\nducted their experiments on two different datasets, MIAS\nand INbreast, that is composed of FFDM images; their\napproach achieved an accuracy of 97.93% and 96.64%,\nrespectively.\nIn 2015, a giant leap occurred in object detection tech-\nniques, especially with the appearance of new deep learn-\ning-based models that can detect multiple objects within\n6470 Neural Computing and Applications (2024) 36:6467–6496\n123\none image. Those models are categorized into one-shot and\ntwo-shot detectors; the most well known among them is\nYou Look Only Once (YOLO) [ 27] because of its perfor-\nmance at both accuracy and computational time levels. Al-\nMansi et al. [ 28] were one of the pioneers who exploited\nYOLO in their work; they introduced a YOLO-based CAD\nsystem that achieved an accuracy of 85.2% for detection. In\n[29], they proposed a CAD system for mass detection,\nsegmentation, and classiﬁcation; they utilized YOLO for\ndetection, then segmented the detected masses using Full\nResolution Convolutional Network (FRCN) and AlexNet\narchitecture-based classiﬁer for classiﬁcation. Their\napproach achieved a detection accuracy of 97.2%, seg-\nmentation accuracy of 92.97%, and classiﬁcation accuracy\nof 95.3%. However, their model was straggling with small\nmass detection, moreover, one of the drawbacks in their\nmodel is the manual elimination of the false localized\nmasses before segmentation phase, which is impractical for\nautomated diagnosis.\nIn [ 30], the authors developed a YOLO-based CAD\nsystem to detect the tumors and classify them into masses\nand calciﬁcation; their experiments were conducted on two\ndifferent datasets, INbreast and CBIS-DDSM. The model\nachieved 98.1% and 95.7%, respectively. Their model isn’t\nproviding a diagnosis about the malignancy of the detected\nmasses. Additionally, the model has high inference rate/\nimage relative to the other recent similar studies.\nAlso, Faster-RCNN [31] was one of the object detection\nmodels that showed promising performance in some stud-\nies. In [ 32], Ribli et al. also utilized the two-shot detector\nFaster-RCNN in their developed system for mass detection\nand classiﬁcation. Their system detected 90% of the\nmalignant masses with a classiﬁcation accuracy of 95%.\nAgarwal et al. [ 33] also proposed a Faster-RCNN-based\nmodel for mass detection; the model achieved a sensitivity\nof 95–71% and a speciﬁcity of 70%. Comparing to the\nresults of the models that adopted the YOLO, these models\nhave lower detection sensitivity. Additionally, Faster-\nRCNN consumes more time at detection than YOLO.\nCao et al. [ 34] developed a novel model that detects\nbreast masses based on anchor-free technique named\nFSAF, an enhanced model of RetinaNet [ 35]. Moreover,\nthey proposed a new augmentation method to increase the\nsize of the dataset. This augmentation technique enhanced\ntheir results; however, it has more computational cost\nrather than the traditional augmentation techniques. The\nmodel attained 0.495 False Positive Rate (FPR)/image for\nINbreast and 0.599 FPR/image for DDSM.\nShen et al. [ 36] proposed a framework for mass detec-\ntion with an attempt to automate the process of mass\nannotation in mammographic images. The model mainly\ndepended on adversarial learning; the experiments were\ndone over a private dataset and INbreast; it achieved an\nAUC of 0.9083 and 0.8522 for each dataset, respectively.\nHowever, their training strategy addressed the problem of\noscillation and the limitation of small batch size, this\napproach needs to be experimented on more different\nmedical imaging datasets.\n2.1 Insights from related work\nThe mammograms’ quality affected breast cancer detection\nsensitivity; most of the proposed work was done over\nmammographic datasets of SFM or FFDM mammographic\nimages. FFDM images provide better quality, so this type\nof image has replaced the SFM images in recent years;\nhowever, one of the problems still exists is the masked\nmasses in cases with high breast density.\nIn FFDM, the gray levels of the glandular tissues have\nsimilar values as the masses. Accordingly, this makes the\nmass hide within the dense tissues and decreases the visi-\nbility of the masses. CESM is a relatively new technique\nfor obtaining mammographic images; this technique\ndepends on getting a new mammographic image by sub-\ntracting the low-energy image from the high-energy image.\nStudies showed that the CESM could provide more\nmorphological features and higher sensitivity in detecting\nlesions than the FFDM and the SFM [ 8, 37] especially for\ncases with highly dense tissues. Song et al. [ 38] proposed a\ndeep-information bottleneck-based network for classifying\nthe CESM images; their model aimed to learn the relevant\nfeatures between the images. Their approach achieved an\naccuracy of 97.2%; they used the CDD-CESM dataset in\ntheir experiments. Other studies [ 39–41] introduced dif-\nferent classiﬁcation models based on deep learning and\nconventional machine learning for CESM images. From\nTable 1, it can be noticeable that most of the proposed\nwork was done mainly on the FFDM images, few studies\nonly have been proposed CAD systems for CESM. There is\nnot enough evaluation of the performance of CAD systems\non CESM images, speciﬁcally in mass detection.\nMoreover, based on the previously reviewed work, the\nproposed techniques in the literature mainly did not yet\nexplore the potential of the transformers in learning the\nglobal context of the pixels in the classiﬁcation task of\nabnormal ﬁndings in mammographic images. Therefore, a\nNeural Computing and Applications (2024) 36:6467–6496 6471\n123\ncomprehensive assessment has been done between CNN\nmodels and vision transformers models to exploit the\nability of the vision transformers in learning the long-range\ndependencies and the relationship between image pixels in\nmass classiﬁcation.\nConsequently, this work proposes a fully automated\nframework for mass detection and classiﬁcation in CESM\nimages by integrating the power of YOLO with the ViT\ntransformer. Furthermore, the performance of the model\nwas also evaluated on FFDM images. Also, the work\nTable 1 Summary of recent studies for both deep learning and feature-based techniques\nReference Methodology Feature\nextraction\nDataset Image\ntype\nDetection Classiﬁcation Results\n[28] YOLOv1 Deep\nlearning\nDDSM FM 44 Detection Acc: 96.33%\nClassiﬁcation Acc: 85.52%\n[29] YOLO ? FRCN ? Inception\nResNet v2\nDeep\nlearning\nINbreast FFDM 44 Detection Acc: 97.27%\nClassiﬁcation Acc: 95.32%\n[30] YOLO v3 Deep\nlearning\nINbreast\nCBIS-\nDDSM\nFFDM\nFM\n4 — Detection Acc: 98.1%, 95.7%\n[33] Faster-RCNN Deep\nlearning\nINbreast\nOPTIMAM\nFFDM\nFFDM\n4 — Detection TPR @ FPI:\n0.99 ± 0.03 at 1.17 for\nmalignant and 0.85 ± 0.08\nat 1.0 for benign (INbreast),\n0.91 ± 0.06 at 1.69\n(OPTIMAM)\n[32] Faster-RCNN Deep\nlearning\nDDSM\nINbreast\nFM\nFFDM\n44 Detection of 90% of the\nmalignant masses\n[25] AlexNet-based CNN\nlv ? SVM\nDeep\nlearning\nDDSM\nCBIS-\nDDSM\nFM — 4 Classiﬁcation Acc: 80.5%,\n87.2%\n[38] Multi-feature deep-\ninformation bottleneck\nDeep\nlearning\nCDD-CESM CESM — 4 Classiﬁcation Acc: 97.2%\n[39] Multilayer Perceptron (MLP) Feature-\nbased\nPrivate\ndataset\nCESM — 4 Classiﬁcation acc: 84.8%\n[41] AlexNet and RawNet Deep\nlearning\nPrivate\ndataset\nCESM — 4 Classiﬁcation sensitivity:\n100%\nClassiﬁcation speciﬁcity: 66%\n[42] Three different features\nextraction methods\n(Statistical ? GLCM\nfeatures, two methods based\non Wavelet Transformer\nWT) ? SVM\nFeature-\nbased\nDDSM FM — 4 Classiﬁcation Acc: 98.69%\n[18] Extract intensity, texture, and\nmorphological features using\nmathematical\nexpressions ? SVM for\nclassiﬁcation\nFeature-\nbased\nDDSM\nMIAS\nFM\nSFM\n__ 4 Classiﬁcation Acc: 96.6%\nClassiﬁcation Acc: 97.5%\n[43] Lifting Wavelet Transform\n(LWT) for feature extraction\nfrom ROIs ? Extreme\nLearning Machine (ELM)\nwith Moth Flame\nOptimization (MFO) for\nclassiﬁcation\nFeature-\nbased\nMIAS\nDDSM\nSFM\nFM\n— 4 Classiﬁcation Acc: 99.94%\nClassiﬁcation Acc: 99.68%\n6472 Neural Computing and Applications (2024) 36:6467–6496\n123\nprovides a comprehensive study of the potential of CESM\nvs. FFDM in mass detection and classiﬁcation. Table 1\nsummarizes recent studies in FFDM and CESM images for\nmass detection and/or (mass/whole image) classiﬁcation.\n3 Methods and materials\nThis work proposes a fully automated framework for mass\ndetection and classiﬁcation in end-to-end training strategy\nin CESM and FFDM images; deep learning models are\nadopted in both phases. The proposed CAD system can be\ndivided into three parts, as shown in Fig. 2; pre-processing,\ndetection, and classiﬁcation. The pre-processing steps were\ninspired by the technique that was used by [ 34]; three\nmethods were applied at this phase Gaussian Filter [ 44],\nOtsu’s thresholding [45] and clip limit adaptive histogram\nequalization (CLAHE) [ 46] as shown in Fig. 2. YOLOv4\narchitecture was adopted for mass detection while the\nclassiﬁcation network of YOLO was replaced with the\nViT-transformer network; Fig. 3 illustrates the proposed\nframework.\n3.1 Pre-processing phase\n3.1.1 Gaussian filter and Otsu’s thresholding\nFirstly, the Gaussian ﬁlter was used to reduce the noise and\nblurring of the images. Gaussian ﬁlter is a linear ﬁlter with\na symmetric kernel with an odd size that passes through\neach pixel in the image. The values inside the kernels are\ncalculated as shown in Eq. 1, where ( x, y) represents the\npixel coordinate, r is the standard deviation of the Gaus-\nsian distribution.\nFig. 2 Flow diagram for the\nsteps of the proposed\nframework for mass detection\nand classiﬁcation\nNeural Computing and Applications (2024) 36:6467–6496 6473\n123\nGx ; yðÞ ¼ 1\n2pr2 e/C0 x2 þ y2\n2r2 ð1Þ\nOtsu thresholding was also used to ﬁnd the suit-\nable threshold for separating the foreground pixels and the\nbackground pixels to minimize the area of the background\nto crop the Region of Interest (RoI) of the breast.\n3.1.2 Clip limit adaptive histogram equalization (CLAHE)\nCLAHE was used to enhance the contrast of the mam-\nmographic image; CLAHE mainly enhances the local\ncontrast of the image. It works at small tiles of the image\nrather than the whole image. This algorithm is used to\nenhance the contrast of the medical images to improve the\nvisual appearance of the mammogram. The clip limit (CL)\nis an essential parameter for CLAHE, as this parameter\ncontrols the image’s brightness level. In the pre-processing\nphase, two clip limits were used, as shown in Fig. 2.\n3.2 Detection phase\nAt this phase, You Look Only Once (YOLO) model has\nbeen selected for mass detection. YOLO is a well-known\nobject detection architecture known as a single-shot\ndetector, as the image is processed in one shot to detect\nmultiple objects within it. One of the advantages of YOLO\nis looking at the complete image, which means less\ninformation loss, and this is considered one of the crucial\npoints that affect the interpretation of medical images.\nFurthermore, the detection and classiﬁcation are done\nsimultaneously, making the YOLO faster than other\ndetectors. Figure 4 illustrates the architecture of YOLO.\nThere are different ofﬁcial versions of YOLO, which are\nYOLOv1, YOLOv2 [ 47], YOLOv3 [ 48], YOLOv4 [ 49],\nYOLOv7 [ 50], and YOLOv8 [ 51]. Different studies\n[7, 52, 53] show that YOLOv4 performs better than the\nother elder versions, YOLOv1, YOLOv2, and YOLOv3.\nThe improvements that were introduced in YOLOv4\nenhanced the accuracy and detection time. However, the\nrecent versions, YOLOv7 and YOLOv8, were introduced\nwith some improvements to enhance the trade-off between\naccuracy and time.\n3.2.1 YOLOv4 vs. YOLOv7 and YOLOv8\nAn experiment was conducted to select the most suit-\nable version of YOLO that ﬁt the used datasets in this work\namong the recently introduced YOLO versions. The\nexperiments use the same datasets for YOLOv4, YOLOv7,\nand YOLOv8 to evaluate the performance of these recent\nmodels on medical images, speciﬁcally mammograms.\nAccording to the experiments, YOLOv4 outperformed\nFig. 3 A proposed integrated framework of YOLO and ViT transformer for mass detection and classiﬁcation\n6474 Neural Computing and Applications (2024) 36:6467–6496\n123\nYOLOv7 and YOLOv8 regarding mAP, recall, and preci-\nsion; however, YOLOv7 showed competitive results on the\nINbreast dataset. On the other side, YOLOv7 and YOLOv8\nprovide faster performance than YOLOv4, as shown in\nTable 2.\nBased on the conducted experiments, YOLOv7 strug-\ngles in small mass detection, especially with crowded\nmammographic scenes, whether false mass detection or\nmissed mass detection, especially with CESM-CDD data-\nset. It is not performed well in detecting masses at different\nscales, as it struggles with masses that are very large or\nvery small regarding the other masses in the mammo-\ngraphic image. In addition, lighting changes can cause\nsigniﬁcant variations in the appearance of masses;\nYOLOv7 is adversely affected by the changes in lighting\n[54], which makes it inconvenient for mass detection,\nespecially in mammographic scanning, where lighting\nvariations are common.\nRegarding YOLOv8, the main change is adopting a new\nanchor-free detection mechanism [ 55]. Also, the model is\nFig. 4 YOLO v4 Architecture [ 33]\nTable 2 Detection results of\nYOLOv4, YOLOv7, and\nYOLOv8 on INbreast and\nCESM-CDD datasets\nDataset Model mAP(%)@ IoU = 0.5 Recall (%) Precision (%) Inference/image\nINbreast YOLOv4 98.96% 100 92 0.0892 s\nYOLOv7 97.60% 95.60 95.70 0.0377 s\nYOLOv8 89.3% 85.7 86.8 0.0545 s\nCE-CDD YOLOv4 81.52% 79 77 0.0833 s\nYOLOv7 61.70% 64.10 66.30 0.0469 s\nYOLOv8 50.60% 54.80 58.50 0.0578 s\nDM-CDD YOLOv4 71.65% 71 68 0.0144 s\nYOLOv7 58.10% 54.10 67.30 0.0297 s\nYOLOv8 46.20% 45.20 50.70 0.0758 s\nNeural Computing and Applications (2024) 36:6467–6496 6475\n123\nbuilt based on a new modiﬁed version of CSP-DarkNet-53\nas the backbone [ 51] such as YOLOv4. On the other hand,\nthe model is still under construction and development, so\nits performance is not stable yet. Also, the model struggles\nwith small object detection in complex scenes [ 56], espe-\ncially with the probability of the overlapping between the\nsmall-size objects and other size objects that may partially\nblock its appearance.\nDue to the consequences of the above reasons, at the\ndetection phase, YOLOv4 was used to mainly detect the\nmasses existing in the mammograms. It splits the input\nmammographic image into grid cells (s 9 s) cells; if the\nmass falls within the cell, it is considered responsible for\ndetecting this mass. A ﬁxed number of bounding boxes is\npredicted for each cell with their conﬁdence score; each\nbox’s conﬁdence score represents the probability of con-\ntaining a mass multiplied by the Intersection over Union\n(IoU) between the ground truth and the predicted box.\nMany different conﬁgurations were introduced for\nYOLO; these conﬁgurations are set up according to the\napplication domain and the used datasets in the experi-\nments. The most important and effective step before\ntraining the YOLO is adjusting the anchor boxes according\nto the dataset and the resolution of the input image;\naccordingly, the K-Means clustering algorithm was used\nfor that in all conducted experiments. The anchors were\ngenerated in these experiments for each dataset separately\nand based on the different resolutions used.\n3.3 Classification phase\n3.3.1 Vision transformers\nThe transformer is considered a de facto architecture for\nNatural Language Processing (NLP) tasks. The trans-\nformers generally are built on the self-attention mechanism\nthat allows the model to learn the global dependencies\nbetween the inputs and outputs. This mechanism mainly\nlets the inputs interact with each other to know which\nfeatures the model should pay more attention to.\nThe promising results achieved with transformers in\nNLP tasks, especially with the transfer learning on down-\nstream tasks, opened the door to introducing the trans-\nformers to the computer vision tasks. Many versions of\nvision transformers have been introduced recently; Doso-\nvitskiy et al. [ 57] were the ﬁrst ones who introduced the\nViT transformer. After that, many were proposed, such as\nDeiT (Data Efﬁcient Transformer) [ 58], Swin–Transformer\n[59], and ConvNeXt [ 60]. Furthermore, different studies\nrecently utilized transformers in various medical imaging\ntasks such as classiﬁcation, segmentation, and detection. In\n[61], the authors proposed a model for predicting breast\ntumor malignancy using a convNeXt transformer over\nultrasound images. Van et al. [ 62] also introduced a model\nthat utilized transformers to build a cross-view transformer\nmodel that was tested on multi-view medical images from\ntwo datasets CBIS-DDSM for breast cancer mammography\nand CheXpert for chest X-rays. In addition, different\nmodels based on transformers were proposed for COVID-\n19 diagnosis [ 63, 64]. Accordingly, and due to its success\nin prediction and classiﬁcation, one of the main objectives\nof this work is to utilize the transformers in the detected\nmasses classiﬁcation.\n3.3.2 Why transformers?\nThe kernels of the convolutional networks focus only on\nthe local texture, which represents a local subset of pixels\nfrom the image, and that enforces the network to ignore the\nglobal context of the features as the network fails to encode\nthe relative position of the features; accordingly, different\nstudies were proposed recently to overcome this problem\nby utilizing attention mechanisms and pyramid networks.\nTransformers are one of the recent architectures that\nbuilt on the concept of the self-attention mechanism. The\nsigniﬁcant advantage of transformers that can be exploited\nin medical image diagnosis is the model’s ability to\nunderstand the pixels’ global context through learning the\nlong-term dependencies between the data. For example, in\nthis work, the surrounding area of the mass provides more\ninformation that can help the model observe more dis-\ncriminant features about the global context and the corre-\nlation between the masses and its surrounding tissues; this\ncan provide a more accurate diagnosis.\n3.3.3 How vision transformer (ViT) works\nThe ViT Transformer is used for classiﬁcation in this work;\nas shown in Fig. 5, at the classiﬁcation phase, there are\nthree main components of the ViT network; patch\nembedding, feature extraction using stacked transformer\nencoders, and the classiﬁcation head that is built with\nMultilayer Perceptron (MLP).\nThe mammography image is reshaped into a sequence of\npatches. Then, the patches are ﬂattened and mapped to\ndimensions with linear projection, as a constant latent\n6476 Neural Computing and Applications (2024) 36:6467–6496\n123\nvector is used through all the layers of the transformer\nnetwork. Each image x 2 RH/C2W/C2C (where H, W, and C are\nrepresenting height, width, and number of channels of the\nimage) is split into N non-overlapped patches, each of size\n16 9 16. These patches are reshaped into this form x\np 2\nRN /C2 P2/C1 CðÞ to be in sequence of 2D patches in n line vec-\ntors of the shape of (1, P2 /C1 CÞ; where P is the resolution of\neach patch, while N represents the input sequence length\n(number of patches) that will be fed to the transformer after\napplying linear projection according to Eq. 2.\nz0 ¼ xclass; x1\npE; x2\npE; /C1/C1/C1 ; xN\np E\nhi\nþ Epos;\nE 2 R P2/C1 CðÞ /C2 D; Epos 2 R N þ 1ðÞ /C2 D\nð2Þ\nAt the patch embedding phase and after splitting the\nimage into n2 patches of shape ( P, P, C), the ﬂattened\npatches are multiplied by a trainable embedding tensor\nwhich is in a shape of ( P2 /C1 C; d) to learn how to project\neach patch linearly into d dimension (where d is a constant\nvalue with the network architecture). The embedded pat-\nches are aggregated with the positional embeddings gen-\nerated through E\npos, a trainable positional embedding\ntensor. The resultant z0 is fed into the transformer enco-\nders; the encoders learn the global features from the\nembedded patches of the mass through multi-headed self-\nattention layers. The encoders generate a sequence of\ntokens. Those tokens are fed into the MLP to generate the\nprediction of the class label if it is a benign mass or a\nmalignant mass.\n3.4 Transfer learning\nThis work adopted the transfer learning concept due to the\nlack of publicly available mammograms and to minimize\nthe training time. Pre-trained weights for different net-\nworks were used to initialize the parameters of these net-\nworks through the training phase.\nThe YOLO network was ﬁne-tuned for detection, and\nthe pre-trained weights on the COCO dataset were trans-\nferred to initialize the network weights for training on the\ntwo mammographic datasets. On the other side, the models\nthat were used for classiﬁcation were trained on ImageNet\nand ﬁne-tuned to predict the class of each mass.\nIn the conducted experiments for classiﬁcation, six\nclassiﬁcation models were used; three of them are trans-\nformer based, which are ViT, SWIN, and ConvNeXt, while\nthe other three are CNN-based, which are VGG16, Rese-\nNet50, and Inception v3.\nThe proposed model used ViT transformer; this model\nwas pre-trained on ImageNet21K. The input image size of\nthe model is 224 9 224, and the input images in this model\nare divided into 16 9 16 patches. The Adam optimizer was\nused through training. A linear layer was added on the top\nof the pre-trained encoder to downstream the model for the\nmass classiﬁcation task. Additionally, the MLP head was\nmodiﬁed to just generate two outputs that are malignant\nand benign.\nFurthermore, for the other classiﬁcation models, some\nmodiﬁcations have been done to transfer the pre-trained\nweights of those models into the required task. The base\nFig. 5 Modiﬁed ViT transformer for mass classiﬁcation\nNeural Computing and Applications (2024) 36:6467–6496 6477\n123\nmodel of SWIN was used; the model was pre-trained on\nImageNet1K, and the model took an input image size of\n224 9 224. Also, the Adam optimizer was used with\nweight decay to avoid the overﬁtting problem, and the\nMLP head was also modiﬁed to output malignant or\nbenign.\nConvNeXt architecture was adopted in the experiments\nto explore the potential of this model, as the architecture of\nthis model integrates the ConvNeXt based on ResNet50\nwith the design of the training approaches of the vision\ntransformer, the network was pre-trained on ImageNet22K,\nwith an input image size of 224 9 224.\nResNet50 was modiﬁed; the last dense layer was\nexcluded from the feature extraction layers. Moreover, the\nlayers of ResNet were frozen to use the pre-trained weights\nduring the training phase. A fully connected layer was\nadded to obtain the ﬁnal prediction. Binary cross-entropy\nloss function and Adam optimizer were used.\nVGG16 was also modiﬁed as the classiﬁcation layer was\nremoved and replaced by a fully connected layer to classify\nthe masses into malignant and benign. Furthermore, the\nbase layers of VGG16 were frozen to restore the pre-\ntrained weights of the ImageNet.\nThe ﬁnal tested model was Inception v3, used in the\nclassiﬁcation phase; the model took input images of size\n224 9 224. The base layers were set to be not trainable,\nand the last layer was also replaced with a fully connected\nlayer to ﬁt the mass classiﬁcation task. For ResNet50,\nVGG16, and Inception v3, the Adam optimizer was used\nwith binary cross-entropy loss function; also, the three\nmodels were pre-trained on ImageNet.\n4 Experimental design and results\nThe experiments were conducted through three main pha-\nses; the ﬁrst phase is the pre-processing of the datasets,\nfollowed by the mass detection phase, and ﬁnally, the mass\nclassiﬁcation phase. For mass detection, the YOLOv4\nmodel was trained to detect the masses regardless of their\ntype; then, the detected masses are fed into a model based\non the vision transformers architecture to classify those\nmasses into benign and malignant. Furthermore, different\nclassiﬁcation models were used through the experiments to\nevaluate the performance of the transformer-based models\nversus the CNN-based models on mammographic\ndiagnosing.\n4.1 Dataset\nThis work used two datasets: INbreast for FFDM images\nand CDD-CESM for both FFDM and CESM images. Each\ndataset was split into training, validation, and testing sets\nusing the splitting ratio of 70%–10% and 20%, respec-\ntively. During the splitting process, the images for the same\npatients were included in the same set to prevent the results\nfrom being biased. The same sets were used across both\ndetection and classiﬁcation tasks to guarantee fair evalua-\ntion for the whole model.\n4.1.1 INbreast\nThis dataset is composed of mammographic images that\nwere produced using the FFDM technique. It includes 410\nimages in DICOM format for 115 cases with both views,\nMedio Lateral Oblique (MLO) and Carnio Caudal (CC).\nHowever, the dataset did not provide both views for each\npatient. Ninety cases of this dataset were diagnosed with\ncancer; 107 images were diagnosed with benign and\nmalignant masses, and those are included in this work’s\nexperiments. The masses were classiﬁed into benign and\nmalignant based on the BI-RADS score that is provided\nwith the dataset, where 1, 2, and 3 are considered benign;\non the other hand, 3, 4, and 5 are counted as malignant. The\nmass annotations were extracted from the XML ﬁles that\nare provided with the dataset, as those annotations were\nconverted to be in the accepted form for YOLO annotation.\nAs each image should be attached with its corresponding\ntxt ﬁle that has the bounding boxes of the existing masses,\nwhere each box is represented by ( C\nx, Cy, w, h); (Cx, Cy)\nrepresents the coordinates of the center point of the\nbounding box, w is the width, and h is the height of this\nbox. Table 3 illustrates the distribution of the dataset over\ntraining, validation, and testing sets that have been used for\nboth detection and classiﬁcation.\n4.1.2 CDD-CESM\nCDD-CESM is a newly introduced publicly available\ndataset that provides images in two types FDDM and\nCESM. It is the ﬁrst publicly available dataset that contains\nCESM images. The dataset includes 2006 images divided\ninto 1003 low-energy images (FDDM) and 1003 subtracted\nimages representing the (CESM). The dataset has MLO\nand CC view images; the images were acquired using two\ndifferent scanners that are G.E. Healthcare Senographe DS\nTable 3 INbreast dataset splitting distribution for the experiments\nTraining Validation Testing\nBenign 25 6 4\nMalignant 51 3 18\nTotal 76 9 22\n6478 Neural Computing and Applications (2024) 36:6467–6496\n123\nand Hologic Selenia Dimensions Mammography Systems.\nThe average resolution of the images is 2355 9 1315; the\nimages were obtained from 326 patients, all of whom are\nfemales between 18 and 90 years old. The dataset provides\nmanual segmentation annotations for the existing abnor-\nmalities in the mammographic images, which are provided\naccording to the ACR-BIRADS lexicon. The images are\navailable in JPEG format with a CSV ﬁle for the annota-\ntions; the medical reports are also attached to the dataset.\nThe dataset includes 310 FFDM images with masses and\n333 CESM images with masses; in the conducted experi-\nments, we used 310 images for the experiments on FFDM\nand CESM. Table 4 shows the data distribution of CESM\nin the training, validation, and testing sets. The corre-\nsponding images (cases) to those used in the CESM\nmodality were included in the experiments conducted on\nthe FFDM modality to guarantee a fair evaluation of each\nmodality’s efﬁcacy in the automated breast cancer diag-\nnosis process.\nAs the provided annotations in this dataset were done for\nsegmentation, there are three forms for those annotations,\npolygon, circle, and ellipse. Accordingly, some pre-pro-\ncessing was done to convert those annotations into\nbounding boxes; the polygon points for X and Y were used\nto get X\nmin, Xmax, Ymin, and Ymax from those lists, and those\ncoordinates were used as coordinates for obtaining the\nbounding boxes.\nFor the circle annotations, the dataset provides center x\n(cx), center y (cy), and the radius r;S oE q s .( 3) and ( 4)\nwere used to get, x1, and, y1 of the box; to get the width and\nthe height the r multiplied by 2 then the width were added\nto x1 to get, x2 and height to y1 to get y2.\nx1 ¼ cx /C0 r ð3Þ\ny1 ¼ cy /C0 r ð4Þ\nTo get the bounding box of the ellipse segmentation\nannotation, the provided values for cx, cy, rx, and ry were\nused to get x1, y1, x2, and, y2 according to Eqs. (5), (6), (7),\nand (8).\nx1 ¼ cx /C0 rx ð5Þ\nx2 ¼ cx þ rx ð6Þ\ny1 ¼ Cy /C0 ry ð7Þ\ny2 ¼ Cy þ ry ð8Þ\nAfter converting the segmentation annotations into\nbounding boxes, those boxes’ coordinates are altered to ﬁt\nthe format of the YOLO annotations, as mentioned before.\nTable 4 illustrates the distribution of the dataset over\ntraining, validation, and testing sets that have been used for\nboth detection and classiﬁcation.\n4.2 Implementation environment\nThe experiments were conducted on a single machine with\nNVIDIA GeForce RTX3080Ti GPU with 12 vRAM,\nIntel/C210Core i7-11700 k processor with 3.200 GHz fre-\nquency, and 32 GB RAM. C ?? , python 3.8, and\nTensorFlow were used to implement the proposed system\non Windows 10 operating system.\n4.3 Implementation set-up\nThe proposed framework was implemented over different\nphases: pre-processing, augmentation, detection, and clas-\nsiﬁcation. The following subsections demonstrate the\nimplementation set-up of each phase.\n4.3.1 Pre-processing and augmentation\nFor INbreast images, the images were converted from\nDICOM images into JPEG images; however, this step was\nskipped for CDD-CESM as the images are already in the\nJPEG format. The breast region was cropped to minimize\nthe background area. The images were normalized to make\nthe pixel intensity distribution in the range from (0–255).\nThen CLAHE was used at two different clip limits (1 and\n2), and the generated images were mixed up with the\nnormalized image to reconstruct new colored images. This\nstep was done for both datasets to obtain new images with\nenhanced visibility for the mammographic images in\naddition to the original images in the training phase. The\nimages were augmented to increase the number of images\nin the training phase, especially with the existence of\nimbalanced distribution between malignant and benign\ncases; four augmentation techniques were applied (vertical\nﬂip. Horizontal ﬂip, multiplicative noise, and random\nrotation) for both images the original and the newly\nreconstructed one. At the detection phase, one more aug-\nmentation technique (mosaic) was used and selected based\non previous experiments by [ 65], as it showed better per-\nformance in detection than other techniques that are used\nwith YOLO. Algorithms 1 and 2 show the pre-processing\nalgorithmic steps for CDD-CESM and INbreast.\nTable 4 CDD-CESM dataset splitting distribution for experiments\nTraining Validation Testing\nBenign 50 9 13\nMalignant 173 16 49\nTotal 223 25 62\nNeural Computing and Applications (2024) 36:6467–6496 6479\n123\nAlgorithm 1 Pre-processing CDD- CESM mammograms algorithm\n6480 Neural Computing and Applications (2024) 36:6467–6496\n123\nAlgorithm 2 Pre-processing INbreast mammograms algorithm\n4.3.2 Mass detection\nThe YOLOv4 is used in this work for the detection task;\nDarknet with CSP53 is used as a backbone for the network.\nSome network parameters were set up and modiﬁed at the\nnetwork layers based on the domain and the used datasets\nin the experiments.\nTable 5 shows the set-up conﬁgurations that were used\nin this work for YOLOv4 layers. In this work, different\nexperiments were conducted to select the suitable input\nimage size for the network, so the experiments were done\nTable 5 Set up conﬁgurations for YOLOv4 network layers on\n416 9 416 and 640 9 640-input sizes\nImage size (416 9 416), (640 9 640)\nNom. of anchors 9\nBatches 64\nSubdivisions 16 for 416 image size, 64 for 640 image size\nNumber of classes 1, 2\nLearning rate (LR) 0.001\nScales 0.1, 0.1\nSteps 3200, 3600\nMax. batches 4000\nNo of ﬁlters (18), (21)\nNeural Computing and Applications (2024) 36:6467–6496 6481\n123\nfor two input sizes (416 9 416) and (640 9 640). More-\nover, the model performance was evaluated by two dif-\nferent scenarios; the ﬁrst one was to detect the existence of\nthe masses regardless of their type (benign/malignant),\nwhile the other one was designed to detect the benign\nmasses and malignant masses separately. Accordingly, the\nnumber of classes in some experiments was 1 class, and in\nother experiments was 2. This consequently affects the\nnumber of ﬁlters to be 18 with the 1 class detection and 21\nwith the 2 classes detection based on the equation men-\ntioned in [ 10]. The learning rate was selected to be 0.001\nbased on different experiments. Two scales, 0.1 and 0.1,\nwere used to change the learning rate at two different steps\nthat are 3200 and 3600. Finally, the max batches were set\nup to be 4000 as recommended in [ 27] (max\nbatches = number of classes 9 2000). Also, K-means\nclustering was used to select the anchors’ sizes based on\nthe dataset; nine anchors were used for those experiments.\n4.3.3 Mass classification\nSix experiments were conducted for classiﬁcation with the\nsame sets for training, validation, and testing that were\nused in detection for each dataset. The experiments were\ndone to evaluate the performance of transformers versus\nthe CNN-based networks. The selected CNN-based net-\nworks are VGG16, ResNet50, and Inception v3, as those\nnetworks showed promising results in other studies for\nmass classiﬁcation. This work uses three transformer-based\nnetworks: ViT transformer, SWIN, and ConvNeXt. The\ninput size for all networks was 224 9 224, Adam opti-\nmizer was used with binary cross-entropy loss function,\nand all the models were modiﬁed to transfer their pre-\ntrained weights to ﬁt the mass classiﬁcation task. Further-\nmore, weight decay was used for all transformer models to\navoid the overﬁtting problem. Algorithm 3 shows the\nalgorithmic steps for the mass detection and classiﬁcation\nprocess.\n6482 Neural Computing and Applications (2024) 36:6467–6496\n123\nAlgorithm 3 Mass detection and classiﬁcation algorithm\n4.4 Evaluation metrics\nDifferent evaluation metrics were used to evaluate the\nperformance of the proposed model. The Intersection over\nUnion (IoU), mean Average Precision (mAP), F1-score,\nprecision, and recall are used for detection. Moreover, the\nTrue Positive Rate (TPR) and False Negative Rate (FNR)\nwere calculated using True Positive (TP), False Negative\n(FN), and False Positive (FP). The performance was\nevaluated for classiﬁcation through accuracy, sensitivity,\nspeciﬁcity, confusion matrix, Area Under the Curve AUC,\nand Receiver Operating Characteristics (ROC).\nIoU was used in evaluating the detection; IoU represents\nthe overlapping between the predicted bounding box and\nNeural Computing and Applications (2024) 36:6467–6496 6483\n123\nthe ground truth at a speciﬁc threshold; the threshold used\nin the proposed model is 0.5.\nFor detection, only TP, FP, and FN were deﬁned as the\ndetected mass is considered to be TP if the IoU [ = 0.5\nand considered to be FP if the IoU is \\ 0.5. If the model\nfails to detect an existing mass, this is counted as FN.\nThe mAP is also calculated to estimate the mean of all\naverage precisions over all classes in the used dataset,\nwhere mAP is calculated using the following Equation.\nmAp ¼ 1\nN\nXN\ni ¼ 1\nAPðÞ i; where N is the number of classes\nð9Þ\nAlso, the precision was measured to deﬁne the per-\ncentage of the truly detected masses regarding the total\nnumber of actual existing masses, as shown in Eq. ( 10).\nPrecision ¼ TP\nTP þ FP ð10Þ\nFurthermore, the recall (sensitivity) and FNR were cal-\nculated as shown in Eqs. ( 11) and ( 13).\nThe dataset sets suffer from an imbalanced class prob-\nlem, as shown from the data distribution in Tables 3 and 4\nin Sects. 4.1.1 and 4.1.2. Accordingly, F1-score was cal-\nculated as it is used as one of the useful evaluation matrices\nin this case, as shown in Eq. ( 12).\nRecall senstivity =TPRðÞ ¼\nTP\nTP þ FN ð11Þ\nF1 /C0 score ¼ 2 /C2 precision /C2 recall\nprecision þ recall ð12Þ\nFNR ¼ FN\nTP þ FN ð13Þ\nDifferent classiﬁcation metrics were used for evaluation,\nand the accuracy score of the model was calculated for both\nvalidation and testing according to Eq. ( 14). The confusion\nmatrix was used to represent mainly four attributes based\non the classiﬁcation results that are:\nTrue Positive (TP): this represents the number of benign\nmasses that were classiﬁed correctly as benign.\nTrue Negative (TN): this indicates the number of\nmalignant masses that were classiﬁed as malignant.\nFalse Positive (FP): this represents the number of the\nmisclassiﬁed masses as benign, and they are malignant.\nFalse Negative (FN): this represents the number of the\nmisclassiﬁed masses as malignant, and they are benign.\nAccuracy ¼\nTP þ TN\nTN þ TP þ FP þ FN /C2 100 ð14Þ\nMoreover, speciﬁcity was calculated as shown in\nEq. (15), also FPR and TPR were calculated to plot the\nROC curve and calculate the AUC score. The ROC curve\nrepresents the trade-off between FPR and TPR; the higher\nthe AUC score, the better the ability of the model to dif-\nferentiate between benign and malignant masses.\nSpecificity ¼ TN\nTN þ FP ð15Þ\nFPR ¼ TP\nTP þ FN ð16Þ\n4.5 Mass detection results\nThis section illustrates the detection results on both data-\nsets INbreast and CDD-CESM, with its two image types\n(DM-CESM). Table 6 shows the Benign vs. malignant\nmass detection results on INbreast. In contrast, Tables 7\nand 8 show the results of mass detection regardless of its\ntype on INbreast and CESM, respectively, at different\nresolution input sizes. Moreover, Fig. 6 presents false\ndetection on a mammographic image from the INbreast\ndataset. At the same time, Fig. 7 illustrates detection\nresults on some images on FFDM vs. its corresponding\nCESM images from the CDD-CESM dataset.\nTables 6 and 7 show that the performance of YOLO is\nbetter at detecting the existence of masses regardless its\ntype. Also, Tables 7 and 8 show that the higher input image\nsize improves the detection accuracy as the best mAP is\nachieved at input size of 640. The 640-input size achieved\nmAP of 98.96% in INbreast dataset, 81.52% on CESM\nimages and 71.65% on DM images from CDD_CESM\ndataset. Moreover, the experimental results showed that\nDM lower detection sensitivity than CESM.\nIt can be seen from Fig. 6 an example of a false\ndetection mass of a mammographic image from the\nINbreast dataset. The ground truth based on the provided\nannotations has only one mass for this case with patient_id\n(22,614,236), while the model detected two masses. One of\nthem is truly detected with conﬁdence score of 94% and the\nother one is a false detected mass.\nFor further demonstration of the impact of mass detec-\ntion on CESM, Fig. 7 illustrates the effectiveness of using\nCESM in enhancing the sensitivity of mass detection rather\nthan FFDM images. The ﬁgure provides samples of images\nfrom CDD-CESM dataset in forms of CESM and their\ncorresponding DM to illustrate the performance of the\nmodel at mass detection in these images regarding their\nground truth.\n4.6 Mass classification results\nThis section provides the results of the mass classiﬁcation\nusing six modiﬁed models; three of them are CNN based,\nand the other three are vision transformer-based models.\n6484 Neural Computing and Applications (2024) 36:6467–6496\n123\nTable 9 shows each network’s results in different evalua-\ntion metrics. Moreover, Figs. 8, 9, 10, 11, 12, and 13 show\nthe ROC and Confusion matrix for the results on each\ndataset; INbreast, CE-CESM, and CDD-CESM,\nrespectively.\nTable 9 shows that ViT transformer outperforms the\nother networks, speciﬁcally the CNN-based ones. It can be\nTable 6 Detection results on INbreast for Benign masses vs. Malignant masses in terms of mAP, F1-score, TP, FP, FN, recall, and precision\nDataset Input image\nsize\nmAP(%)@\nIoU = 0.5\nF1-score\n(%)\nTP FP FN Recall\n(%)\nPrecision\n(%)\nB-AP\n(%)\nM-AP\n(%)\nTP-FP\n(B)\nTP-FP\n(M)\nINbreast 416 9 416 84.43% 86 19 3 3 86 86 75.00 93.86 3–2 16–1\nTable 7 INbreast mass detection results regardless of its type (Benign/Malignant) at different input image sizes in terms of mAP, F1-score, TP,\nFP, FN, recall, precision, and FNR\n# Input image\nsize\nNormal cases mAP(%)@IoU = 0.5 F1-score\n(%)\nTP FP FN Recall\n(%)\nPrecision\n(%)\nFNR\nTrial 1 416 9 416 Without normal\nimages\n97.78% 96 22 1 1 96 96 0.0454\nTrial 2 416 9 416 With normal images 95.27% 94 22 2 1 96 92 0.0454\nTrial 3 640 9 640 Without normal\nimages\n98.96% 96 23 20 100 92 0\nThe best results that were obtained during the experiments are indicated by bold\nTable 8 Detection results for CDD-CESM for FDDM and CESM images at different input image sizes in terms of mAP, F1-score, TP, F.P.,\nF.N., recall, precision, and FNR\n# Input image size Image type mAP(%)@IoU = 0.5 F1-score (%) TP FP F.N Recall (%) Precision (%) FNR\nTrial 1 416 9 416 DM 70.27% 72 73 24 33 69 75 0.311\nTrial 2 640 9 640 DM 71.65% 69 75 34 31 71 68 0.292\nTrial 3 416 9 416 CESM 73.71% 74 71 16 35 67 82 0.330\nTrial 4 640 9 640 CESM 81.52% 78 84 25 22 79 77 0.207\nThe best results that were obtained during the experiments are indicated by bold\nFig. 6 False detection for a mammographic image from the INbreast dataset on the left, followed by its corresponding ground truth and then its\ndetection result (The purple box indicates the detected mass while the red box indicates the ground truth)\nNeural Computing and Applications (2024) 36:6467–6496 6485\n123\nnoticed that model achieved competitive accuracy scores\non the testing set with 95.65% in INbreast, 97.61% in\nCESM, and 80% in DM. Also, it provides the highest AUC\nscore among the other experimented networks, it achieved\n88%, 90%, and 70% in INbreast CESM and DM, respec-\ntively. Figures 8, 10, and 12 show that ViT has the ability\nto differentiate better between benign and malignant mas-\nses comparing to the other networks. However, SWIN\ntransformer and ConvNext also provide higher AUC scores\nthan the CNN-based models (ResNet50, VGG16, and\nInceptionv3). Furthermore, Confusion matrix in Figs. 9,\n11, and 13 demonstrates that most of the misclassiﬁed\nmasses is benign masses and this may have happened\nbecause the datasets lacked sufﬁcient samples of benign\nmasses.\nFig. 7 Detection results for CESM images with their corresponding ground truth on the left. While on the right the same images in FFDM with\nits detection results and their corresponding ground truth. (Purple box indicates to the detected mass while red box indicates to the ground truth)\n6486 Neural Computing and Applications (2024) 36:6467–6496\n123\n4.6.1 INbreast mass classification results in terms of ROC\nand confusion matrix\nSee Figs. 8, 9 here.\nTable 9 Mass classiﬁcation results for INbreast, DM-CESM, and CE-CESM in terms of validation accuracy, testing accuracy, sensitivity,\nspeciﬁcity, precision, F1-score, and AUC\nDataset Network Validation accuracy\n(%)\nTesting accuracy\n(%)\nSensitivity\n(%)\nSpeciﬁcity\n(%)\nPrecision\n(%)\nF1-score\n(%)\nAUC\n(%)\nINbreast VIT transformer 78.50 95.65 100 75 95 97.43 88\nSWIN-\nTransformer\n82.80 91.30 100 50 90.47 95 75\nConvNeXt 85.70 86.90 100 25 86.36 92.68 62\nReseNet50 97.50 90.87 73.68 25 82.35 77.77 49\nVGG16 67.14 73.91 84.21 25 84.21 84.21 55\nInception v3 42.85 86.96 94.73 0 81.81 87.80 47\nCE-CDD VIT transformer 95.55 97.61 100 80 97.36 98.66 90\nSWIN-\nTransformer\n95.87 94.04 98.64 60 94.80 96.68 79\nConvNeXt 96.34 94.04 97.29 70 82.81 85.48 84\nReseNet50 93.33 88.09 100 10 89.15 94.26 50\nVGG16 89.36 92.85 97.29 60 94.73 95.99 79\nInception v3 92.70 84.52 94.59 10 88.60 91.50 52\nDM-\nCDD\nVIT transformer 72.83 80.00 86.66 53.33 88.13 87.39 70\nSWIN-\nTransformer\n76.41 77.33 83.33 53.33 87.71 85.47 68\nConvNeXt 71.69 76.00 88.33 26.66 82.81 85.48 57\nReseNet50 60.38 82.66 100 13.33 82.19 90.22 57\nVGG16 66.98 70.66 78.33 40 83.92 81.03 59\nInception v3 68.68 76 76.66 20 79.31 77.96 48\nThe best results that were obtained during the experiments are indicated by bold\nFig. 8 ROC curve for the classiﬁcation models on INbreast\nNeural Computing and Applications (2024) 36:6467–6496 6487\n123\n4.6.2 CESM-CDD mass classification results in terms of ROC\nand confusion matrix\nSee Figs. 10, 11 here.\nFig. 9 Confusion Matrix (CM) for the classiﬁcation models on INbreast\nFig. 10 ROC curve for the classiﬁcation models on CE images from CDD-CESM\n6488 Neural Computing and Applications (2024) 36:6467–6496\n123\n4.6.3 DM-CDD mass classification results in terms of ROC\nand confusion matrix\nSee Figs. 12, 13 here.\nFig. 11 Confusion Matrix (CM) for the classiﬁcation models on CE images from CDD-CESM\nFig. 12 ROC curve for the classiﬁcation models on DM images in CDD-CESM\nNeural Computing and Applications (2024) 36:6467–6496 6489\n123\n5 Discussion\nThis section provides a discussion of the results. This work\nconducted several experiments to evaluate the model at\nmass detection and classiﬁcation. For mass detection,\nvarious experiments were carried on to explore the sig-\nniﬁcance of the higher-resolution input size of YOLO. In\naddition, evaluating the network’s performance at detecting\nthe existence of the masses versus detecting benign and\nmalignant masses in both CE and FFDM images. At the\nclassiﬁcation phase, the classiﬁcation network of YOLO\nwas replaced by different Networks. Furthermore, the\nexperiments aimed to explore the potential of the trans-\nformers regarding the CNN-based models that were used\nfor mass classiﬁcation.\n5.1 Mass detection\nFour experiments are conducted on INbreast. The ﬁrst two\nexperiments were done to evaluate the model in two cases;\nthe ﬁrst was to detect the benign and malignant masses,\nwhile the other aimed to detect the existence of the mass\nregardless it was benign or malignant. Considering the\nresults of Tables 6 and 7, it can be deduced that the model\nshowed better results at detecting the masses regardless of\nits type. The performance enhanced signiﬁcantly by almost\n’ 13% in terms of mAP. Moreover, this approach\nimproved the number of the true detected masses from 19\nto 22 with less FP and FN, as shown in Tables 6 and 7.\nTable 7 also shows that the input image size affected the\nmodel’s performance as the higher resolution showed\nbetter mAP. The experiments are done for an input image\nsize of 416 9 416 and 640 9 640; the proposed model in\nTrial 3 detected all 23 existing masses in the testing set;\nhowever, there were 2 FP masses. This model achieved\n98.96% mAP with a sensitivity of 100%. In Trial 1, the\nmodel achieved mAP of 97.78%, as the model failed to\ndetect one of the existing masses, and there was one falsely\ndetected mass. Higher resolution means less information\nloss, and this affected the detection, especially with the\nexistence of small masses and high ﬁbro-glandular tissues.\nAccordingly, this can clarify why the model performed\nbetter in Trial 3 than in Trial 1. Table 8 also showed that\nthe higher input resolution provides higher mAP with more\nTP and less FN for both trials on DM and CE images from\nCDD-CESM. However, the higher resolution needs more\ntime through training as this affects the batch size and\nsubdivision values to allow enough memory through\ntraining.\nThe model in Trial 2 from Table 7 was trained with\nsome normal images within the training set; it showed\nlower mean Average Precision than in Trial 1. However,\nFig. 13 Confusion Matrix (CM) for the classiﬁcation models on DM images in CDD-CESM\n6490 Neural Computing and Applications (2024) 36:6467–6496\n123\nthe model succeeded in detecting 22 masses out of the 23\nfrom the testing set, but it detected 1 extra false detection\nmore than in Trial 1. This may explain why the mAP\nbecame lower, as this false detection was in one of the\nnormal images.\nThe CESM is considered a relatively recent technique\nfor screening mammograms; based on the literature, no\nCAD system model for mass detection in CESM images\nhas been introduced yet, speciﬁcally on the CDD-CESM\ndataset. Accordingly, in this work, we conducted experi-\nments to explore the potential of using those images in\ndetecting the masses. Moreover, the experiments evaluate\nthe detection in contrast-enhanced mammography and its\nequivalent digital mammography for the same dataset. The\nresults of Table 8 showed that mass detection in CESM has\npromising results and outperformed the model’s perfor-\nmance on the FFDM images. In Table 8, Trials 2 and 4\nshowed that the mAP of the detection model was improved\nby 3.4%–9.8% on the CESM images for an input image\nsize of 416 and 640, respectively. Furthermore, the model\nsucceeded in enhancing the true mass detections, sensi-\ntivity, and precision.\nMoreover, Fig. 7 illustrates the results on FFDM images\nand their corresponding CESM images, and it can be\nnoticed that according to ground truth, CESM provides\nmore accurate detection results. Apparently, from those\nconducted experiments, this can show that the CESM can\nreveal more morphological features rather than FFDM\nimages. And accordingly, this helped the model to learn\nmore discriminant features during the training phase. These\nimprovements can be demonstrated from even the trials\nthat were done on the images of size 416 9 416.\n5.2 Mass classification\nFor classiﬁcation, six models were experimented on each\ndataset. From Table 9, it can be deduced that the vision\ntransformer (ViT) outperformed the other classiﬁers in\nclassifying the detected masses. As shown in Table 9, the\nvision transformers achieved the highest accuracy for\nINbreast, DM images, and CE images from the CDD-\nCESM dataset. The model achieved a classiﬁcation accu-\nracy of 95.65% for INbreast, 97.61% for CE images, and\n80% for FFDM. For INbreast and CE images, the results of\nViT outperformed the other classiﬁers in all terms (accu-\nracy, sensitivity, speciﬁcity, and AUC).\nThe ResNet 50 achieved the highest accuracy score for\nthe D.M. images with 82%; however, it showed a lower\nAUC score than ViT. The ViT achieved 80% on DM\nimages, but its performance is still considered the best\namong the other classiﬁers regarding the ROC and AUC\nvalues. The model achieved the highest score of 70%, as\nshown in Fig. 12. This means that the model can\ndifferentiate between benign and malignant masses more\nthan the other classiﬁers. On the other hand, ResNet50\nachieved an AUC of 57% and sensitivity of 100%, which\nmeans that the model is useless because it considered most\nof the cases as malignant. It showed bad performance at\npredicting the benign masses rightly, as shown in the\nconfusion matrix from Fig. 13. Moreover, the results show\nthat the ViT model can be generalized; based on the vali-\ndation accuracy and testing accuracy scores in all trials, as\nthe model provided a higher performance on the testing set.\nSWIN and ConvNeXt performed relatively better than\nthe CNN-based classiﬁers, especially in AUC scores. The\ntransformer interprets the images as a matrix of patches,\nnot a matrix of pixels, and this allowed the model to pre-\nserve long global relationships between the patches and\nobtain more semantic information rather than CNN-based\nmodels. However, the SWIN transformer provides some\nimprovements on the regular vision transformer (ViT); it\nshowed lower performance than the ViT. The architecture\nof ViT is mainly based on observing the relationships\nbetween each patch (image token) and all of the rest pat-\nches of the input image. On the other hand, SWIN is built\non the idea of the shifted window design; accordingly, it\nlooks only at the relationships between the patch and only\nthe other patches in the windowed area. The main task for\nthe classiﬁcation part of this work is to classify the cropped\nmass from the detection phase into benign or malignant. In\nthis case, the input image is the masses itself, not the whole\nmammographic images. Accordingly, the relationship\nbetween each patch and all other patches for the mass\nmatters; it is not only about the relation of the patch and its\nneighboring patches of the same window. And so, this can\nclarify why ViT outperformed the SWIN in the conducted\nexperiments. Maybe the approach of SWIN can increase\nthe computation efﬁciency of the model; however, this was\nnot so inﬂuential in these experiments as the images were\nnot with high resolution, especially with the fact that they\nrepresent a cropped part of the original images. ConvNeXt\narchitecture is mainly based on ResNet architecture with\nthe same training approaches as the basic vision trans-\nformer. As shown in Table 9, ResNet provides low per-\nformance compared to the other transformer-based\nclassiﬁers; which means that the architecture of the ResNet\nwas not performed well with the mass classiﬁcation task,\nand this can justify why the performance of ConvNeXt is\nlower than ViT and SIWN. However, it showed better\nperformance than ResNet50.\nFrom Fig. 9, it can be deduced that only one benign\nmass was misclassiﬁed with ViT in INbreast Dataset; the\nmodel succeeded in classifying all the malignant masses.\nAlso, from Fig. 11 for CE images, the proposed model\nrightly predicted all the malignant masses. Only two benign\nNeural Computing and Applications (2024) 36:6467–6496 6491\n123\nmasses were misclassiﬁed; Fig. 14 shows the misclassiﬁed\nmasses in INbreast and CE images from CESM-CDD.\nIt can be noticed from Fig. 13 that the most signiﬁcant\nratio of the misclassiﬁed masses was the benign ones\ncompared to the ratio of the misclassiﬁed malignant mas-\nses. This may happen because the existing datasets do not\nhave enough cases with benign masses compared to the\nnumber of existing malignant masses, and accordingly, the\nmodel trained in a better way to classify the malignant\nmasses. However, augmentation techniques were used to\novercome this problem, but those techniques do not pro-\nvide too much realistic transformation for the images. And\nthis can be considered one of the limitations facing breast\ncancer CAD systems.\nViT transformers utilize the idea of parallel processing,\nmaking the transformers provide more computational efﬁ-\nciency than CNN-based models. Table 10 shows that the\ninference time that the proposed framework took per image\nis less than the time YOLOv4 took before replacing the\nclassiﬁcation layers of YOLO with the ViT.\nTables 11 and 12 show a comparison between the pro-\nposed work and other recent studies on INbreast and CDD-\nCESM. The results in those tables illustrate that the pro-\nposed work shows promising and competitive results\nregarding the previous work. The proposed model achieved\na detection accuracy of 98.96% and a classiﬁcation accu-\nracy of 95.64% in INbreast, which is almost the same result\nprovided by the model introduced by [ 66]. Moreover,\nTable 12 shows that the proposed model outperformed the\nproposed model by [ 15] in terms of F1-score for mass\ndetection in CE images by almost 5%; however, it achieved\nthe same score as DM images.\n6 Conclusion\nVision transformers are considerably revolutionizing\ncomputer vision tasks, especially image classiﬁcation.\nUtilizing the power of transformers in medical image\ninterpretation can help in enhancing the performance of\nCAD systems. This work proposed a novel framework for\nmass detection and classiﬁcation based on integrating the\nYOLOV4 with the basic architecture of the vision trans-\nformer (ViT).\nCESM images are a relatively new type of mammo-\ngraphic images that need more investigation in the direc-\ntion of developing CAD systems that can utilize the\nmorphological features of these images. Accordingly, this\nwork introduces the ﬁrst automated CAD system for mass\ndetection and classiﬁcation in CESM images. Furthermore,\nthe model also was evaluated on FFDM images. The\nINbreast and the newly introduced CDD-CESM datasets\nwere used in the experiments. The conducted experiments\nshowed that the CESM images could improve the CAD\nsystem’s performance at both detection and classiﬁcation\nlevels, as they showed better results than FFDM images.\nThe proposed model achieved detection accuracy of\n98.96% and 81.52%; moreover, it achieved a classiﬁcation\naccuracy of 95.65% and 97.61% for INbreast and CESM,\nrespectively.\nThe experiments also showed that the image size\naffected the detection results speciﬁcally for the CDD-\nCESM dataset as the image size of 640 enhanced the mAP\nfor DM images by 3.4% and 9.8% for CE images compared\nto the image size of 416.\nThe proposed model utilized the potential of the vision\ntransformers with mammographic images in classifying the\nmasses detected using YOLOv4. Integrating the ViT\narchitecture into our model has not only boosted its per-\nformance, but also revealed its potential in learning global\nand semantic features, crucial for the task at hand. ViT\ntransformer showed very promising results compared to the\nother experimented models; it shows the best AUC score\nfor INbreast and CDD-CESM datasets. Vit achieved AUC\nscores of 88%, 90%, 70%, and F1-score of 97.43%,\nFig. 14 Misclassiﬁed masses\na INbreast b, c CE-CDD (truth:\nBenign, prediction: Malignant)\nTable 10 Inference time per image for the proposed framework\nMethod Inference time/image ( s)\nYOLOv4 ? ViT transformer 0.0378\nYOLOv4 0.0652\n6492 Neural Computing and Applications (2024) 36:6467–6496\n123\n98.66%, 87.39% for INbreast, CE-CESM, and DM-CESM,\nrespectively.\n7 Advantages, limitations, and future work\nBased on the conducted experiments, transformers showed\nbetter results than the CNN-based models; the transformers\nshowed great capability at learning the global features\nalong with the semantic features, especially with the\nexistence of the attention mechanism that can help the\nmodel to learn which features to pay attention to at the\nclassiﬁcation task. Moreover, these experiments showed\nthe efﬁcacy of CESM images in developing breast cancer\nCAD systems.\nHowever, some limitations need to be considered in\nfuture work; the available datasets suffer from the imbal-\nance class problem even with using augmentation tech-\nniques as they did not provide natural transformations.\nFurthermore, the size of the publicly available datasets is\nrelatively small. Moreover, the proposed model’s compu-\ntational time can be considered another limitation that can\nbe enhanced in the future regarding the performance of the\nrecent versions of YOLO.\nBased on the results, this work can be extended in dif-\nferent directions; CESM images can be used along with\nFDDM for introducing a multimodal CAD system that can\nutilize both of these types to improve the performance of\nthe CAD systems for breast cancer diagnosis. Also, the\nnewly introduced YOLO models conﬁgurations need more\ninvestigations, especially with the signiﬁcant improvement\nin the computational time and performance, so this work\ncan be extended to investigate the potential of using these\nmodels on medical images, especially with the beneﬁt of\nthe ability to use them for detection or segmentation.\nAuthor contribution NMH was involved in conceptualization,\nmethodology, software, data curation, writing—original draft. SH\nhelped in conceptualization, methodology, writing—reviewing and\nediting, supervision. Khaled Mahar contributed to conceptualization,\nvalidation, writing—reviewing and editing, supervision.\nFunding Open access funding provided by The Science, Technology\n& Innovation Funding Authority (STDF) in cooperation with The\nEgyptian Knowledge Bank (EKB).\nData availability The datasets analyzed during the current study are\navailable in the Cancer Imaging archive repository, https://wiki.can\ncerimagingarchive.net/pages/viewpage.action?pageId=109379611,\nand in the Kaggle repository, https://www.kaggle.com/datasets/rama\nnathansp20/inbreast-dataset.\nTable 11 Comparison of detection and classiﬁcation results between proposed work and related studies on INbreast with YOLO\nReference Method Dataset Splitting ratio (training-validation-\ntesting) (%)\nDetection\naccuracy (%)\nClassiﬁcation\naccuracy (%)\n[7] YOLOv3 ? Inception v3 INbreast, DDSM 80–20 89.5 95.5\n[66] YOLO INbreast 75–6.25–18.75 98.96 95.6\n[30] YOLO-based Fusion models INbreast, CBIS-\nDDSM\n70–10–20 98.1, 95.7 –\n[67] YOLO-based ROI Classiﬁer CBIS-DDSM – 90 93.5\n[68] YOLO ? InceptionResNetv2 INbreast 70–10–20 97.27 97.50\nProposed\nmodel\nYOLOv4 ? ViT transformer INbreast 70–10–20 98.96 95.65\nCE-CESM 81.52 97.61\nDM-CESM 71.65 80.00\nTable 12 Comparison of detection and classiﬁcation results between proposed work and related studies on CDD-CESM\nReference Method Dataset Segmentation/detection (F1-score) Classiﬁcation accuracy\n[15] EffecientNetB0 ? GradCAM CDD-CESM DM: 72% _\nC.E.: 73%\nProposed model YOLOv4 ? ViT transformer INbreast 96% 95.65%\nCE-CESM 78% 97.61%\nDM-CESM 72% 80.00%\nNeural Computing and Applications (2024) 36:6467–6496 6493\n123\nDeclarations\nCompeting interests The authors declare that they have no known\ncompeting financial interests or personal relationships that could have\nappeared to influence the work reported in this paper.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate\nif changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\n1. Giaquinto AN, Sung H, Miller KD et al (2022) Breast cancer\nstatistics, 2022. CA Cancer J Clin 72:524–541. https://doi.org/10.\n3322/CAAC.21754\n2. Miglioretti DL, Smith-Bindman R, Abraham L et al (2007)\nRadiologist characteristics associated with interpretive perfor-\nmance of diagnostic mammography. J Natl Cancer Inst\n99:1854–1863. https://doi.org/10.1093/JNCI/DJM238\n3. Alzubaidi L, Zhang J, Humaidi AJ et al (2021) Review of deep\nlearning: concepts, CNN architectures, challenges, applications,\nfuture directions. J Big Data 8:53. https://doi.org/10.1186/\ns40537-021-00444-8\n4. Kumar R (2023) Memory recurrent elman neural network-based\nidentiﬁcation of time-delayed nonlinear dynamical system. IEEE\nTrans Syst Man Cybern Syst 53:753–762. https://doi.org/10.\n1109/TSMC.2022.3186610\n5. Sherstinsky A (2020) Fundamentals of recurrent neural network\n(RNN) and long short-term memory (LSTM) network. Phys D\nNonlinear Phenom 404:132306\n6. Nasser M, Yusof UK (2023) Deep learning based methods for\nbreast cancer diagnosis: a systematic review and future direction.\nDiagnostics 13:161. https://doi.org/10.3390/\nDIAGNOSTICS13010161\n7. Aly GH, Marey M, El-Sayed SA, Tolba MF (2021) YOLO based\nbreast masses detection and classiﬁcation in full-ﬁeld digital\nmammograms. Comput Methods Programs Biomed 200:105823.\nhttps://doi.org/10.1016/J.CMPB.2020.105823\n8. Sensakovic WF, Carnahan MB, Czaplicki CD et al (2021) Con-\ntrast-enhanced mammography: how does it work? Radiographics\n41:829–839. https://doi.org/10.1148/RG.2021200167/ASSET/\nIMAGES/LARGE/RG.2021200167.TBL2.JPEG\n9. Wei J, Hadjiiski LM, Sahiner B et al (2007) Computer-aided\ndetection systems for breast masses: comparison of performances\non full-ﬁeld digital mammograms and digitized screen-ﬁlm\nmammograms. Acad Radiol 14:659–669. https://doi.org/10.1016/\nJ.ACRA.2007.02.017\n10. Hassan NM, Hamad S, Mahar K (2022) Mammogram breast\ncancer CAD systems for mass detection and classiﬁcation: a\nreview. Multimed Tools Appl 81:20043–20075. https://doi.org/\n10.1007/S11042-022-12332-1/FIGURES/5\n11. Raghu M, Unterthiner T, Kornblith S et al (2021) Do vision\ntransformers see like convolutional neural networks? Neural Inf\nProcess Syst 34:12116–12128\n12. He K, Gan C, Li Z et al (2022) Transformers in medical image\nanalysis: a review. Intell Med. https://doi.org/10.1016/J.IMED.\n2022.07.002\n13. Gheﬂati B, Rivaz H (2022) Vision transformers for classiﬁcation\nof breast ultrasound images. In: Proceedings of the annual\ninternational conference of the IEEE engineering in medicine and\nbiology society, EMBS 2022-July, pp 480–483. https://doi.org/\n10.1109/EMBC48229.2022.9871809\n14. Shamshad F, Khan S, Zamir SW et al (2023) Transformers in\nmedical imaging: a survey. Med Image Anal 88:102802. https://\ndoi.org/10.1016/j.media.2023.102802\n15. Khaled R, Helal M, Alfarghaly O et al (2022) Categorized con-\ntrast enhanced mammography dataset for diagnostic and artiﬁcial\nintelligence research. Sci Data 9:122. https://doi.org/10.1038/\nS41597-022-01238-0\n16. Suhail Z, Denton ERE, Zwiggelaar R (2018) Classiﬁcation of\nmicro-calciﬁcation in mammograms using scalable linear Fisher\ndiscriminant analysis. Med Biol Eng Comput 56:1475–1485.\nhttps://doi.org/10.1007/S11517-017-1774-Z/TABLES/2\n17. Punitha S, Amuthan A, Joseph KS (2018) Benign and malignant\nbreast cancer segmentation using optimized region growing\ntechnique. Future Comput Inform J 3:348–358. https://doi.org/10.\n1016/J.FCIJ.2018.10.005\n18. Mughal B, Sharif M, Muhammad N (2017) Bi-model processing\nfor early detection of breast tumor in CAD system. Eur Phys J\nPlus 132:1–14. https://doi.org/10.1140/EPJP/I2017-11523-8\n19. Rouhi R, Jafari M, Kasaei S, Keshavarzian P (2015) Benign and\nmalignant breast tumors classiﬁcation based on region growing\nand CNN segmentation. Expert Syst Appl 42:990–1002. https://\ndoi.org/10.1016/J.ESWA.2014.09.020\n20. Dong M, Lu X, Ma Y et al (2015) An efﬁcient approach for\nautomated mass segmentation and classiﬁcation in mammo-\ngrams. J Digit Imaging 28:613–625. https://doi.org/10.1007/\nS10278-015-9778-4\n21. Montenegro L, Abreu M, Fred A, Machado JM (2022) Human-\nassisted vs. deep learning feature extraction: an evaluation of\nECG features extraction methods for arrhythmia classiﬁcation\nusing machine learning. Appl Sci (Switzerland) 12:7404. https://\ndoi.org/10.3390/app12157404\n22. Dara S, Tumma P (2018) Feature extraction by using deep\nlearning: a survey. In: Proceedings of the 2nd international con-\nference on electronics, communication and aerospace technology,\nICECA 2018, pp 1795–1801. https://doi.org/10.1109/ICECA.\n2018.8474912\n23. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature\n521:436–444. https://doi.org/10.1038/nature14539\n24. Bengio Y, Courville A, Vincent P (2012) Representation learn-\ning: a review and new perspectives. IEEE Trans Pattern Anal\nMach Intell 35(8):1798–1828\n25. Ragab DA, Sharkas M, Marshall S, Ren J (2019) Breast cancer\ndetection using deep convolutional neural networks and support\nvector machines. PeerJ 7:e6201. https://doi.org/10.7717/PEERJ.\n6201\n26. Sannasi Chakravarthy SR, Bharanidharan N, Rajaguru H (2022)\nMulti-deep CNN based experimentations for early diagnosis of\nbreast cancer. IETE J Res. https://doi.org/10.1080/03772063.\n2022.2028584\n27. Redmon J, Divvala S, Girshick R, Farhadi A (2016) You only\nlook once: uniﬁed, real-time object detection\n28. Al-Masni MA, Al-Antari MA, Park JM et al (2017) Detection and\nclassiﬁcation of the breast abnormalities in digital mammograms\nvia regional convolutional neural network. In: Annual interna-\ntional conference IEEE engineering medicine and biology soci-\nety, pp 1230–1233. https://doi.org/10.1109/EMBC.2017.8037053\n29. Al-antari MA, Al-masni MA, Kim TS (2020) Deep learning\ncomputer-aided diagnosis for breast lesion in digital\n6494 Neural Computing and Applications (2024) 36:6467–6496\n123\nmammogram. Adv Exp Med Biol 1213:59–72. https://doi.org/10.\n1007/978-3-030-33128-3_4\n30. Baccouche A, Garcia-Zapirain B, Olea CC, Elmaghraby AS\n(2021) Breast lesions detection and classiﬁcation via YOLO-\nbased fusion models. Comput Mater Contin 69:1407–1425.\nhttps://doi.org/10.32604/CMC.2021.018461\n31. Ren S, He K, Girshick R, Sun J (2017) Faster R-CNN: towards\nreal-time object detection with region proposal networks. IEEE\nTrans Pattern Anal Mach Intell 39:1137–1149. https://doi.org/10.\n1109/TPAMI.2016.2577031\n32. Ribli D, Horva´th A, Unger Z et al (2018) Detecting and classi-\nfying lesions in mammograms with deep learning. Sci Rep 8:1–7.\nhttps://doi.org/10.1038/s41598-018-22437-z\n33. Agarwal R, Dı´az O, Yap MH et al (2020) Deep learning for mass\ndetection in full ﬁeld digital mammograms. Comput Biol Med\n121:103774. https://doi.org/10.1016/J.COMPBIOMED.2020.\n103774\n34. Cao H, Pu S, Tan W, Tong J (2021) Breast mass detection in\ndigital mammography based on anchor-free architecture. Comput\nMethods Programs Biomed 205:106033. https://doi.org/10.1016/\nJ.CMPB.2021.106033\n35. Zhu C, He Y, Savvides M (2019) Feature selective anchor-free\nmodule for single-shot object detection\n36. Shen R, Yao J, Yan K et al (2020) Unsupervised domain adap-\ntation with adversarial learning for mass detection in mammo-\ngram. Neurocomputing 393:27–37. https://doi.org/10.1016/j.\nneucom.2020.01.099\n37. Mohamed SAS, Moftah SG, Chalabi NAEM, Salem MAAW\n(2021) Added value of contrast-enhanced spectral mammography\nin symptomatic patients with dense breasts. Egypt J Radiol\nNuclear Med 52:1–10. https://doi.org/10.1186/S43055-020-\n00372-2/FIGURES/4\n38. Song J, Zheng Y, Wang J et al (2022) Multi-feature deep infor-\nmation bottleneck network for breast cancer classiﬁcation in\ncontrast enhanced spectral mammography. Pattern Recognit\n131:108858. https://doi.org/10.1016/J.PATCOG.2022.108858\n39. Danala G, Patel B, Aghaei F et al (2018) Classiﬁcation of breast\nmasses using a computer-aided diagnosis scheme of contrast\nenhanced digital mammograms. Ann Biomed Eng 46:1419–1431.\nhttps://doi.org/10.1007/s10439-018-2044-4\n40. Gao F, Wu T, Li J et al (2018) SD-CNN: a shallow-deep CNN for\nimproved breast cancer diagnosis. Comput Med Imaging Graph\n70:53–62. https://doi.org/10.1016/J.COMPMEDIMAG.2018.09.\n004\n41. Perek S, Kiryati N, Zimmerman-Moreno G et al (2019) Classi-\nﬁcation of contrast-enhanced spectral mammography (CESM)\nimages. Int J Comput Assist Radiol Surg 14:249–257. https://doi.\norg/10.1007/s11548-018-1876-6\n42. Berbar MA (2018) Hybrid methods for feature extraction for\nbreast masses classiﬁcation. Egypt Inform J 19:63–73. https://doi.\norg/10.1016/j.eij.2017.08.001\n43. Muduli D, Dash R, Majhi B (2020) Automated breast cancer\ndetection in digital mammograms: a moth ﬂame optimization\nbased ELM approach. Biomed Signal Process Control 59:10192.\nhttps://doi.org/10.1016/j.bspc.2020.101912\n44. D’Haeyer JPF (1989) Gaussian ﬁltering of images: a regulariza-\ntion approach. Signal Process 18:169–181. https://doi.org/10.\n1016/0165-1684(89)90048-0\n45. Otsu N (1979) Threshold selection method from gray-level his-\ntograms. IEEE Trans Syst Man Cybern SMC 9:62–66. https://doi.\norg/10.1109/TSMC.1979.4310076\n46. Pizer SM, Amburn EP, Austin JD et al (1987) Adaptive his-\ntogram equalization and its variations. Comput Vis Graph Image\nProcess 39:355–368. https://doi.org/10.1016/S0734-\n189X(87)80186-X\n47. Redmon J, Farhadi A (2016) YOLO9000: better, faster, stronger.\nIn: Proceedings–30th IEEE conference on computer vision and\npattern recognition, CVPR 2017-January, pp 6517–6525. https://\ndoi.org/10.1109/CVPR.2017.690\n48. Redmon J, Farhadi A (2018) YOLOv3: an incremental\nimprovement\n49. Bochkovskiy A, Wang C-Y, Liao H-YM (2020) YOLOv4:\noptimal speed and accuracy of object detection\n50. Wang C-Y, Bochkovskiy A, Liao H-YM (2022) YOLOv7:\ntrainable bag-of-freebies sets new state-of-the-art for real-time\nobject detectors\n51. Reis D, Kupec J, Hong J, Daoudi A (2023) Real-time ﬂying\nobject detection with YOLOv8\n52. Nepal U, Eslamiat H (2022) Comparing YOLOv3, YOLOv4 and\nYOLOv5 for autonomous landing spot detection in faulty UAVs.\nSensors 22:464. https://doi.org/10.3390/S22020464\n53. Ismail A, Mehri M, Sahbani A et al (2021) Performance bench-\nmarking of YOLO architectures for vehicle license plate detec-\ntion from real-time videos captured by a mobile robot. Sorbonne\nUniversity, Paris\n54. Zhou S, Cai K, Feng Y et al (2023) An accurate detection model\nof Takifugu rubripes using an improved YOLO-V7 network.\nJ Mar Sci Eng 11:1051. https://doi.org/10.3390/jmse11051051\n55. Tian Z, Shen C, Chen H, He T (2019) FCOS: fully convolutional\none-stage object detection\n56. Lou H, Duan X, Guo J et al (2023) DC-YOLOv8: small-size\nobject detection algorithm based on camera sensor. Electronics\n(Switzerland) 12:2323. https://doi.org/10.3390/\nelectronics12102323\n57. Dosovitskiy A, Beyer L, Kolesnikov A et al (2022) An image is\nworth 16 9 16 words: transformers for image recognition at scale\n58. Touvron H, Cord M, Douze M et al (2021) Training data-efﬁcient\nimage transformers & distillation through attention\n59. Liu Z, Lin Y, Cao Y et al (2021) Swin transformer: hierarchical\nvision transformer using shifted windows. In: Proceedings of the\nIEEE international conference on computer vision 9992–10002.\nhttps://doi.org/10.48550/arxiv.2103.14030\n60. Liu Z, Mao H, Wu C-Y et al (2022) A ConvNet for the 2020s\n61. Hassanien MA, Singh VK, Puig D, Abdel-Nasser M (2022)\nPredicting breast tumor malignancy using deep ConvNeXt\nradiomics and quality-based score pooling in ultrasound\nsequences. Diagnostics (Basel) 12:1053. https://doi.org/10.3390/\nDIAGNOSTICS12051053\n62. van Tulder G, Tong Y, Marchiori E (2021) Multi-view analysis of\nunregistered medical images using cross-view transformers.\nLecture notes in computer science (including subseries lecture\nnotes in artiﬁcial intelligence and lecture notes in bioinformatics)\n12903 LNCS. Springer, Cham, pp 104–113\n63. Fan X, Feng X, Dong Y, Hou H (2022) COVID-19 CT image\nrecognition algorithm based on transformer and CNN. Displays\n72:102150. https://doi.org/10.1016/J.DISPLA.2022.102150\n64. Al-Rahhal MM, Bazi Y, Jomaa RM et al (2022) COVID-19\ndetection in CT/X-ray imagery using vision transformers. J Pers\nMed 12:310. https://doi.org/10.3390/JPM1202031\n65. Hassan NM, Hamad S, Mahar K (2022) A deep learning model\nfor mammography mass detection using mosaic and recon-\nstructed multichannel images. Lecture notes in computer science\n(including subseries lecture notes in artiﬁcial intelligence and\nlecture notes in bioinformatics). Springer, Cham, pp 544–559\n66. Al-antari MA, Al-masni MA, Choi MT et al (2018) A fully\nintegrated computer-aided diagnosis system for digital X-ray\nmammograms via deep learning detection, segmentation, and\nclassiﬁcation. Int J Med Inform 117:44–54. https://doi.org/10.\n1016/J.IJMEDINF.2018.06.003\n67. Platania R, Zhang J, Shams S et al (2017) Automated breast\ncancer diagnosis using deep learning and region of interest\nNeural Computing and Applications (2024) 36:6467–6496 6495\n123\ndetection (BC-DROID). In: ACM-BCB 2017–proceedings of the\n8th ACM international conference on bioinformatics, computa-\ntional biology, and health informatics, pp 536–543. https://doi.\norg/10.1145/3107411.3107484\n68. Al-antari MA, Han SM, Kim TS (2020) Evaluation of deep\nlearning detection and classiﬁcation towards computer-aided\ndiagnosis of breast lesions in digital X-ray mammograms.\nComput Methods Programs Biomed 196:105584. https://doi.org/\n10.1016/J.CMPB.2020.105584\nPublisher’s Note Springer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\n6496 Neural Computing and Applications (2024) 36:6467–6496",
  "topic": "CAD",
  "concepts": [
    {
      "name": "CAD",
      "score": 0.769287645816803
    },
    {
      "name": "Computer science",
      "score": 0.7228171825408936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7161040306091309
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6486195921897888
    },
    {
      "name": "Digital mammography",
      "score": 0.6407267451286316
    },
    {
      "name": "Deep learning",
      "score": 0.5690202713012695
    },
    {
      "name": "Mammography",
      "score": 0.5466083884239197
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.528087317943573
    },
    {
      "name": "Computer-aided diagnosis",
      "score": 0.4544740617275238
    },
    {
      "name": "Breast cancer",
      "score": 0.41320958733558655
    },
    {
      "name": "Machine learning",
      "score": 0.3925671875476837
    },
    {
      "name": "Medicine",
      "score": 0.1351962387561798
    },
    {
      "name": "Cancer",
      "score": 0.12467145919799805
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    },
    {
      "name": "Engineering drawing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I59272784",
      "name": "Arab Academy for Science, Technology, and Maritime Transport",
      "country": "EG"
    },
    {
      "id": "https://openalex.org/I107720978",
      "name": "Ain Shams University",
      "country": "EG"
    }
  ]
}