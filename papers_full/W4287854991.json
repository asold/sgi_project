{
  "title": "Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models",
  "url": "https://openalex.org/W4287854991",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5044059653",
      "name": "Patrick Huber",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A5049259877",
      "name": "Giuseppe Carenini",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3099609223",
    "https://openalex.org/W2299976354",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2970507703",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3101190870",
    "https://openalex.org/W2758822623",
    "https://openalex.org/W3172254535",
    "https://openalex.org/W1894075015",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2763886647",
    "https://openalex.org/W3034503989",
    "https://openalex.org/W182831726",
    "https://openalex.org/W3004117589",
    "https://openalex.org/W2998696494",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2142972908",
    "https://openalex.org/W3185980407",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2586597293",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3167227435",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2939507640",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W4253555784",
    "https://openalex.org/W2986341143",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2741164290",
    "https://openalex.org/W3116461613",
    "https://openalex.org/W3184402450",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2109783296",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2470507356",
    "https://openalex.org/W3210694367",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2251293245",
    "https://openalex.org/W2166957049",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3167799121",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3206557162",
    "https://openalex.org/W3173886731",
    "https://openalex.org/W3153791507",
    "https://openalex.org/W3099716694",
    "https://openalex.org/W3101717721",
    "https://openalex.org/W2950415592",
    "https://openalex.org/W1842565584",
    "https://openalex.org/W3171719338",
    "https://openalex.org/W2158211888",
    "https://openalex.org/W2115242108"
  ],
  "abstract": "In this paper, we extend the line of BERTology work by focusing on the important, yet less explored, alignment of pre-trained and fine-tuned PLMs with large-scale discourse structures. We propose a novel approach to infer discourse information for arbitrarily long documents. In our experiments, we find that the captured discourse information is local and general, even across a collection of fine-tuning tasks. We compare the inferred discourse trees with supervised, distantly supervised and simple baselines to explore the structural overlap, finding that constituency discourse trees align well with supervised models, however, contain complementary discourse information.Lastly, we individually explore self-attention matrices to analyze the information redundancy. We find that similar discourse information is consistently captured in the same heads.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2376 - 2394\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nTowards Understanding Large-Scale Discourse Structures in Pre-Trained\nand Fine-Tuned Language Models\nPatrick Huber and Giuseppe Carenini\nDepartment of Computer Science\nUniversity of British Columbia\nVancouver, BC, Canada, V6T 1Z4\n{huberpat, carenini}@cs.ubc.ca\nAbstract\nWith a growing number of BERTology works\nanalyzing different components of pre-trained\nlanguage models, we extend this line of re-\nsearch through an in-depth analysis of dis-\ncourse information in pre-trained and fine-\ntuned language models. We move beyond prior\nwork along three dimensions: First, we de-\nscribe a novel approach to infer discourse struc-\ntures from arbitrarily long documents. Second,\nwe propose a new type of analysis to explore\nwhere and how accurately intrinsic discourse\nis captured in the BERT and BART models.\nFinally, we assess how similar the generated\nstructures are to a variety of baselines as well as\ntheir distributions within and between models.\n1 Introduction\nTransformer-based machine learning models are\nan integral part of many recent improvements in\nNatural Language Processing (NLP). With their\nrise spearheaded by Vaswani et al. (2017), the\npre-training/fine-tuning paradigm has gradually\nreplaced previous approaches based on architec-\nture engineering, with transformer models such as\nBERT (Devlin et al., 2019), BART (Lewis et al.,\n2020), RoBERTa (Liu et al., 2019) and others deliv-\nering state-of-the-art performance on a wide variety\nof tasks. Besides their strong empirical results on\nmost real-world problems, such as summarization\n(Zhang et al., 2020; Xiao et al., 2021a), question-\nanswering (Joshi et al., 2020; O ˘guz et al., 2021)\nand sentiment analysis (Adhikari et al., 2019; Yang\net al., 2019), uncovering what kind of linguistic\nknowledge is captured by this new type of pre-\ntrained language models (PLMs) has become a\nprominent question by itself. As part of this line of\nresearch, called BERTology (Rogers et al., 2020),\nresearchers explore the amount of linguistic under-\nstanding encapsulated in PLMs, exposed through\neither external probing tasks (Raganato and Tiede-\nmann, 2018; Zhu et al., 2020; Koto et al., 2021a)\nor unsupervised methods (Wu et al., 2020; Pandia\net al., 2021). Previous work thereby either focuses\non analyzing the syntactic structures (e.g., Hewitt\nand Manning (2019); Wu et al. (2020)), relations\n(Papanikolaou et al., 2019), ontologies (Michael\net al., 2020) or, to a more limited extend, discourse\nrelated behaviour (Zhu et al., 2020; Koto et al.,\n2021a; Pandia et al., 2021).\nGenerally speaking, while most previous\nBERTology works has focused on either sentence\nlevel phenomena or connections between adja-\ncent sentences, large-scale semantic and pragmatic\nstructures (oftentimes represented as discourse\ntrees or graphs) have been less explored. These\nstructures (e.g., discourse trees) play a fundamen-\ntal role in expressing the intent of multi-sentential\ndocuments and, not surprisingly, have been shown\nto benefit many NLP tasks such as summarization\n(Gerani et al., 2019), sentiment analysis (Bhatia\net al., 2015; Nejat et al., 2017; Hogenboom et al.,\n2015) and text classification (Ji and Smith, 2017).\nWith multiple different theories for discourse\nproposed in the past, the RST discourse theory\n(Mann and Thompson, 1988) and the lexicalized\ndiscourse grammar (Webber et al., 2003) (underly-\ning PDTB (Prasad et al., 2008)) have received most\nattention. While both theories propose tree-like\nstructures, the PDTB framework postulates par-\ntial trees up to the between-sentence level, while\nRST-style discourse structures consist of a single\nrooted tree covering whole documents, comprising\nof: (1) The tree structure, combining clause-like\nsentence fragments (Elementary Discourse Units,\nshort: EDUs) into a discourse constituency tree,\n(2) Nuclearity, assigning every tree-branch primary\n(Nucleus) or peripheral (Satellite) importance in a\nlocal context and (3) Relations, defining the type\nof connection holding between siblings in the tree.\nGiven the importance of large-scale discourse\nstructures, we extend the area of BERTology re-\nsearch with novel insights regarding the amount of\n2376\nintrinsic discourse information captured in estab-\nlished PLMs. More specifically, we aim to better\nunderstand to what extend RST-style discourse in-\nformation is stored as latent trees in encoder self-\nattention matrices1. While we focus on the RST\nformalism in this work, our presented methods are\ntheory-agnostic and, hence, applicable to discourse\nstructures in a broader sense, including other tree-\nbased theories, such as the lexicalized discourse\ngrammar. Our contributions in this paper are:\n(1) A novel approach to extract discourse informa-\ntion from arbitrarily long documents with standard\ntransformer models, inherently limited by their in-\nput size. This is a non-trivial issue, which has been\nmostly by-passed in previous work through the use\nof proxy tasks like connective prediction, relation\nclassification, sentence ordering, EDU segmenta-\ntion, cloze story tests and others.\n(2) An exploration of discourse information locality\nacross pre-trained and fine-tuned language models,\nfinding that discourse structures are consistently\ncaptured in a fixed subset of self-attention heads.\n(3) An in-depth analysis of the discourse quality in\npre-trained language models and their fine-tuned\nextensions. We compare constituency and depen-\ndency structures of 2 PLMs fine-tuned on 4 tasks\nand 7 fine-tuning datasets to gold-standard dis-\ncourse trees, finding that the captured discourse\nstructures outperform simple baselines by a large\nmargin, even showing superior performance com-\npared to distantly supervised models.\n(4) A similarity analysis between PLM inferred dis-\ncourse trees and supervised, distantly supervised\nand simple baselines. We reveal that PLM con-\nstituency discourse trees do align relatively well\nwith previously proposed supervised models, but\nalso capture complementary information.\n(5) A detailed look at information redundancy in\nself-attention heads to better understand the struc-\ntural overlap between self-attention matrices and\nmodels. Our results indicate that similar discourse\ninformation is consistently captured in the same\nheads, even across fine-tuning tasks.\n2 Related Work\nAt the base of our work are two of the most pop-\nular and frequently used PLMs: BERT (Devlin\net al., 2019) and BART (Lewis et al., 2020). We\nchoose these two popular approaches in our study\n1Please note that we focus on discourse structure and nu-\nclearity here, leaving relation classification for future work.\ndue to their complementary nature (encoder-only\nvs. encoder-decoder) and based on previous work\nby Zhu et al. (2020) and Koto et al. (2021a), show-\ning the effectiveness of BERT and BART models\nfor discourse related tasks.\nOur work is further related to the field of dis-\ncourse parsing. With a rich history of traditional\nmachine learning models (e.g., Hernault et al.\n(2010); Ji and Eisenstein (2014); Joty et al. (2015);\nWang et al. (2017), inter alia), recent approaches\nslowly shifted to successfully incorporate a vari-\nety of PLMs into the process of discourse predic-\ntion, such as ELMo embeddings (Kobayashi et al.,\n2019), XLNet (Nguyen et al., 2021), BERT (Koto\net al., 2021b), RoBERTa (Guz et al., 2020) and\nSpanBERT (Guz and Carenini, 2020). Despite\nthese works showing the usefulness of PLMs for\ndiscourse parsing, all of them cast the task into\na “local\" problem, using only partial information\nthrough the shift-reduce framework (Guz et al.,\n2020; Guz and Carenini, 2020), natural document\nbreaks (e.g. paragraphs Kobayashi et al. (2020))\nor by framing the task as an inter-EDU sequence\nlabelling problem on partial documents (Koto et al.,\n2021b). However, we believe that the true benefit\nof discourse information emerges when complete\ndocuments are considered, leading us to propose\na new approach to connect PLMs and discourse\nstructures in a “global” manner, superseding the lo-\ncal proxy-tasks with a new methodology to explore\narbitrarily long documents.\nAiming to better understand what information\nis captured in PLMs, the line of BERTology re-\nsearch has recently emerged (Rogers et al., 2020),\nwith early work mostly focusing on the syntac-\ntic capacity of PLMs (Hewitt and Manning, 2019;\nJawahar et al., 2019; Kim et al., 2020), in parts\nalso exploring the internal workings of transformer-\nbased models (e.g., self-attention matrices (Ra-\nganato and Tiedemann, 2018; Mareˇcek and Rosa,\n2019)). More recent work started to explore the\nalignment of PLMs with discourse information, en-\ncoding semantic and pragmatic knowledge. Along\nthose lines, Wu et al. (2020) present a parameter-\nfree probing task for both, syntax and discourse.\nWith their tree inference approach being computa-\ntionally expensive and limited to the exploration of\nthe outputs of the BERT model, we significantly\nextend this line of research by exploring the inter-\nnal self-attention matrices of PLMs with a more\ncomputationally feasible approach. More tradi-\n2377\nFigure 1: Small-scale example of the discourse ex-\ntraction approach. Purple=EDUs, green=sub-word em-\nbeddings, red=input slices of size tmax, orange=PLM,\nblue=self-attention values, grey-scale=frequency count.\ntionally, Zhu et al. (2020) use 24 hand-crafted\nrhetorical features to execute three different su-\npervised probing tasks, showing promising per-\nformance of the BERT model. Similarly, Pan-\ndia et al. (2021) aim to infer pragmatics through\nthe prediction of discourse connectives by analyz-\ning the model inputs and outputs and Koto et al.\n(2021a) analyze discourse in seven PLMs through\nseven supervised probing tasks, finding that BART\nand BERT contain most information related to dis-\ncourse. In contrast to the approach taken by both\nZhu et al. (2020) and Koto et al. (2021a), we use\nan unsupervised methodology to test the amount\nof discourse information stored in PLMs (which\ncan also conveniently be used to infer discourse\nstructures for new and unseen documents) and ex-\ntend the work by Pandia et al. (2021) by taking\na closer look at the internal workings of the self-\nattention component. Looking at prior work an-\nalyzing the amount of discourse information in\nPLMs, structures are solely explored through the\nuse of proxy tasks, such as connective prediction\n(Pandia et al., 2021), relation classification (Kur-\nfalı and Östling, 2021), and others (Koto et al.,\n2021a). However, despite the difficulties of en-\ncoding arbitrarily long documents, we believe that\nto systematically explore the relationship between\nPLMs and discourse, considering complete docu-\nments is imperative. Along these lines, recent work\nstarted to tackle the inherent input-length limitation\nof general transformer models through additional\nrecurrence in the Transformer-XL model (Dai et al.,\n2019), compression modules (Rae et al., 2020) or\nsparse patterns (e.g., as in the Reformer (Kitaev\net al., 2020), BigBird (Zaheer et al., 2020), and\nLongformer (Beltagy et al., 2020) models). While\nall these approaches to extend the maximum doc-\nument length of transformer-based models are im-\nportant to create more globally inspired models, the\ndocument-length limitation is still practically and\ntheoretically in place, with models being limited\nto a fixed number of pre-defined tokens the model\ncan process. Furthermore, with many proposed\nsystems still based on more established PLMs (e.g.,\nBERT) and with no single dominant solution for\nthe general problem of the input length-limitation\nyet, we believe that even with the restriction being\nactively tackled, an in-depth analysis of traditional\nPLMs with discourse is highly valuable to establish\na solid understanding of the amount of semantic\nand pragmatic information captured.\nBesides the described BERTology work, we got\nencouraged to explore fine-tuned extensions of stan-\ndard PLMs through previous work showing the\nbenefit of discourse parsing for many downstream\ntasks, such as summarization (Gerani et al., 2019),\nsentiment analysis (Bhatia et al., 2015; Nejat et al.,\n2017; Hogenboom et al., 2015) and text classifica-\ntion (Ji and Smith, 2017). Conversely, we recently\nshowed promising results when inferring discourse\nstructures from related downstream tasks, such as\nsentiment analysis (Huber and Carenini, 2020) and\nsummarization (Xiao et al., 2021b). Given this\nbidirectional synergy between discourse and the\nmentioned downstream tasks, we move beyond tra-\nditional experiments focusing on standard PLMs\nand additionally explore discourse structures of\nPLMs fine-tuned on a variety of auxiliary tasks.\n3 Discourse Extraction Method\nWith PLMs rather well analyzed according to their\nsyntactic capabilities, large-scale discourse struc-\ntures have been less explored. One reason for this is\nthe input length constraint of transformer models.\nWhile this is generally not prohibitive for intra-\n2378\nsentence syntactic structures (e.g., presented in Wu\net al. (2020)), it does heavily influence large-scale\ndiscourse structures, operating on complete (poten-\ntially long) documents. Overcoming this limitation\nis non-trivial, since traditional transformer-based\nmodels only allow for fixed, short inputs.\nAiming to systematically explore the ability of\nPLMs to capture discourse, we investigate a novel\nway to effectively extract discourse structures from\nthe self-attention component of the BERT and\nBART models. We thereby extend our previously\nproposed tree-generation methodology (Xiao et al.,\n2021b) to support the input length constraints of\nstandard PLMs using a sliding-window approach in\ncombination with matrix frequency normalization\nand an EDU aggregation method. Figure 1 visual-\nizes the complete process on a small scale example\nwith 3 EDUs and 7 sub-word embeddings.\nThe Tree Generation Procedurewe previously\nproposed in Xiao et al. (2021b) explores a two-\nstage approach to obtain discourse structures from\na transformer model, by-passing the input-length\nconstraint. Using the intuition that the self-\nattention score between any two EDUs is an in-\ndicator of their semantic/pragmatic relatedness, in-\nfluencing their distance in a projective discourse\ntree, they use the CKY dynamic programming\napproach (Jurafsky and Martin, 2014) to gener-\nate constituency trees based on the internal self-\nattention of the transformer model. To generate\ndependency trees, we apply the same intuition used\nto infer discourse trees with the Eisner algorithm\n(Eisner, 1996). Since we explore the discourse\ninformation captured in standard PLMs, we can’t\ndirectly transfer our two-stage approach in Xiao\net al. (2021b), first encoding individual EDUs us-\ning BERT and subsequently feeding the dense rep-\nresentations into a fixed-size transformer model.\nInstead, we propose a new method to overcome the\nlength-limitation of the transformer model2.\nThe Sliding-Window Approach is at the core\nof our new methodology to overcome the input-\nlength constraint. We first tokenize arbitrarily long\ndocuments with n EDUs E = {e1, ..., en}into the\nrespective sequence of m sub-word tokens T =\n{t1, ...tm}with n ≪m, according to the PLM\ntokenization method (WordPiece for BERT, Byte-\nPair-Encoding for BART), as show at the top of\n2For more information on the general tree-generation ap-\nproach using the Eisner algorithm we refer interested readers\nto Xiao et al. (2021b).\nFigure 1. Using the sliding window approach, we\nsubdivide the m sub-word tokens into sequences of\nmaximum input length tmax, defined by the PLM\n(tmax = 512for BERT, tmax = 1024for BART).\nUsing a stride of 1, we generate (m −tmax) + 1\nsliding windows W, feed them into the PLM, and\nextract the resulting tmax ×tmax partial square self-\nattention matrices (MP in Figure 1) for a specific\nself-attention head3.\nThe Frequency Normalization Methodallows\nus to combine the partially overlapping self-\nattention matrices MP into a single document-level\nmatrix MD of size m×m. To this end, we combine\nmultiple overlapping windows, generated due to\nthe stride size of 1, by adding up the self-attention\ncells, while keeping track of the number of over-\nlaps in a separate m ×m frequency matrix MF .\nWe then divide MD by the frequency matrix MF ,\nto generate a frequency normalized self-attention\nmatrix MA (see bottom of Figure 1).\nThe EDU Aggregation is the final processing\nstep to obtain the document-level self-attention\nmatrix. In this step, the m sub-word tokens\nT = {t1, ...tm}are aggregated back into n EDUs\nE = {e1, ..., en}by computing the average bidirec-\ntional self-attention score between any two EDUs\nin MA. For example, in Figure 1, we aggregate\nthe scores in cells MA[0:1, 5:6] to compute the fi-\nnal output of cell [0, 2] (purple matrix in Figure 1)\nand MA[5:6, 0:1] to generate the value of cell[0, 2].\nThis way, we obtain the average bidirectional self-\nattention scores between EDU1 and EDU3. We\nuse the resulting n ×n matrix as the input to the\nCKY/Eisner discourse tree generation methods.\n4 Experimental Setup\n4.1 Pre-Trained Models\nWe select the BERT-base (110 million parameters)\nand BART-large(406 million parameters) models\nfor our experiments. We choose these models for\ntheir diverse objectives (encoder-only vs. encoder-\ndecoder), popularity for diverse fine-tuning tasks,\nand their prior successful exploration in regards to\ndiscourse information (Zhu et al., 2020; Koto et al.,\n2021a). For the BART-large model, we limit our\nanalysis to the encoder, as motivated in Koto et al.\n(2021a), leaving experiments with the decoder and\ncross-attention for future work.\n3We omit the self-attention indexes for better readability.\n2379\nDataset Task Domain\nIMDB(2014) Sentiment Movie Reviews\nYelp(2015) Sentiment Reviews\nSST-2(2013) Sentiment Movie Reviews\nMNLI(2018) NLI Range of Genres\nCNN-DM(2016) Summarization News\nXSUM(2018) Summarization News\nSQuAD(2016) Question-Answering Wikipedia\nTable 1: The seven fine-tuning datasets used in this work\nalong with the underlying tasks and domains.\n4.2 Fine-Tuning Tasks and Datasets\nWe explore the BERT model fine-tuned on two\nclassification tasks, namely sentiment analysis and\nnatural language inference (NLI). For our analysis\non BART, we select the abstractive summarization\nand question answering tasks. Table 1 summarizes\nthe 7 datasets used to fine-tune PLMs in this work,\nalong with their underlying tasks and domains4.\n4.3 Evaluation Treebanks\nRST-DT (Carlson et al., 2002) is the largest En-\nglish RST-style discourse treebank, containing 385\nWall-Street-Journal articles, annotated with full\nconstituency discourse trees. To generate addi-\ntional dependency trees, we apply the conversion\nalgorithm proposed in Li et al. (2014).\nGUM (Zeldes, 2017) is a steadily growing treebank\nof richly annotated texts. In the current version 7.3,\nthe dataset contains 168 documents from 12 gen-\nres, annotated with full RST-style constituency and\ndependency discourse trees.\nAll evaluations shown in this paper are executed\non the 38 and 20 documents in the RST-DT and\nGUM test-sets, to be comparable with previous\nbaselines and supervised models. A similarly-sized\nvalidation-set is used where mentioned to deter-\nmine the best performing self-attention head.\n4.4 Baselines and Evaluation Metrics\nSimple Baselines:We compare the inferred con-\nstituency trees against right- and left-branching\nstructures. For dependency trees, we evaluate\nagainst simple chain and inverse chain structures.\nDistantly Supervised Baselines:We compare our\nresults obtained in this paper against our previous\napproach presented in Xiao et al. (2021b), using\nsimilar CKY and Eisner tree-generation methods to\ninfer constituency and dependency tree structures\n4We exclusively analyze published models provided on the\nhuggingface platform, further specified in Appendix A.\n(a) BERT: PLM, +IMDB, +Yelp, +SST-2, +MNLI\n(b) BART: PLM, +CNN-DM, +XSUM, +SQuAD\nFigure 2: Constituency (top) and dependency (bottom)\ndiscourse tree evaluation of BERT (a) and BART (b)\nmodels on GUM. Purple=high score, Blue=low score.\nLeft-to-right: self-attention heads, top-to-bottom: high\nlayers to low layers. + indicates fine-tuning dataset.\nfrom a summarization model trained on the CNN-\nDM and New York Times (NYT) corpora (referred\nto as SumCNN-DM and SumNYT)5.\nSupervised Baseline:We select the popular Two-\nStage discourse parser (Wang et al., 2017) as our\nsupervised baseline, due to its strong performance,\navailable model checkpoints and code6, as well as\nthe traditional architecture. We use the published\nTwo-Stage parser checkpoint on RST-DT (from\nhere on called Two-StageRST-DT) and re-train the\ndiscourse parser on GUM ( Two-StageGUM). We\nconvert the generated constituency structures into\ndependency trees following Li et al. (2014).\nEvaluation Metrics:We apply the original parse-\nval score to compare discourse constituency struc-\ntures with gold-standard treebanks, as argued in\nMorey et al. (2017). To evaluate the generated\ndependency structures, we use the Unlabeled At-\ntachment Score (UAS).\n5 Experimental Results\n5.1 Discourse Locality\nOur discourse tree generation approach described\nin section 3 directly uses self-attention matrices\nto generate discourse trees. The standard BERT\n5www.github.com/Wendy-Xiao/summ_\nguided_disco_parser\n6www.github.com/yizhongw/StageDP\n2380\nmodel contains 144 of those self-attention matri-\nces (12 layers, 12 self-attention heads each), all\nof which potentially encode discourse structures.\nFor the BART model, this number is even higher,\nconsisting of 12 layers with 16 self-attention heads\neach. With prior work suggesting the locality of\ndiscourse information in PLMs (e.g., Raganato and\nTiedemann (2018); Mareˇcek and Rosa (2019); Xiao\net al. (2021b)), we analyze every self-attention ma-\ntrix individually to gain a better understanding of\ntheir alignment with discourse information.\nBesides investigating standard PLMs, we also\nexplore the robustness of discourse information\nacross fine-tuning tasks. We believe that this is an\nimportant step to better understand if the captured\ndiscourse information is general and robust, or if it\nis “re-learned” from scratch for downstream tasks.\nTo the best of our knowledge, no previous analysis\nof this kind has been performed in the literature.\nTo this end, Figure 2 shows the constituency and\ndependency structure overlap of the generated dis-\ncourse trees from individual self-attention heads\nwith the gold-standard tree structures of the GUM\ndataset7. The heatmaps clearly show that con-\nstituency discourse structures are mostly captured\nin higher layers, while dependency structures are\nmore evenly distributed across layers. Comparing\nthe patterns between models, we find that, despite\nbeing fine-tuned on different downstream tasks, the\ndiscourse information is consistently encoded in\nthe same self-attention heads. Even though the\nbest performing self-attention matrix is not con-\nsistent, discourse information is clearly captured\nin a “local\" subset of self-attention heads across\nall presented fine-tuning tasks. This plausibly sug-\ngests that the discourse information in pre-trained\nBERT and BART models is robust and general, re-\nquiring only minor adjustments depending on the\nfine-tuning task.\n5.2 Discourse Quality\nWe now focus on assessing the discourse informa-\ntion captured in the single best-performing self-\nattention head. In Table 2, we compare the dis-\ncourse structure quality of pre-trained and fine-\ntuned PLMs in the context of supervised models,\ndistantly supervised approaches and simple base-\nlines. We show the oracle-picked best head on the\ntest-set, analyzing the upper-bound for the poten-\n7The analysis on RST-DT shows similar trends and can be\nfound in Appendix B.\nModel RST-DT GUM\nSpan UAS Span UAS\nBERT\nrand. init ↓25.5 ↓13.3 ↓23.2 ↓12.4\nPLM •35.7 •45.3 •33.0 •45.2\n+ IMDB ↓35.4 ↓42.8 •33.0 ↓43.3\n+ Yelp ↓34.7 ↓42.3 ↓32.6 ↓43.7\n+ SST-2 ↓35.5 ↓42.9 ↓32.6 ↓43.5\n+ MNLI ↓34.8 ↓41.8 ↓32.4 ↓43.3\nBART\nrand. init ↓25.3 ↓12.5 ↓23.2 ↓12.2\nPLM •39.1 •41.7 •31.8 •41.8\n+ CNN-DM ↑40.9 ↑44.3 ↑32.7 ↑42.8\n+ XSUM ↑40.1 ↑41.9 ↑32.1 ↓39.9\n+ SQuAD ↑40.1 ↑43.2 ↓31.3 ↓40.7\nBaselines\nRB / Chain 9.3 40.4 9.4 41.7\nLB / Chain-1 7.5 12.7 1.5 12.2\nSumCNN-DM 21.4 20.5 17.6 15.8\nSumNYT 24.0 15.7 18.2 12.6\nTwo-StageRST-DT 72.0 71.2 54.0 54.5\nTwo-StageGUM 65.4 61.7 58.6 56.7\nTable 2: Original parseval (Span) and Unlabelled At-\ntachment Score (UAS) of the single best performing\nself-attention matrix of the BERT and BART models\ncompared with baselines and previous work. ↑, •, ↓\nindicate better, same, worse performance compared to\nthe PLM. “rand. init\"=Randomly initialized transformer\nmodel of similar architecture as the PLM, RB=Right-\nBranching, LB=Left-Branching, Chain-1=Inverse chain.\ntial performance of PLMs on RST-style discourse\nstructures. This is not a realistic scenario, as the\nbest performing head is generally not known a-\npriori. Hence, we also explore the performance\nusing a small-scale validation set to pick the best-\nperforming self-attention matrix. In this more re-\nalistic scenario for discourse parsing, we find that\nscores on average drop by 1.55 points for BERT\nand 1.33% for BART compared to the oracle-\npicked performance of a single self-attention ma-\ntrix. We show detailed results of this degradation in\nAppendix C8. Our results in Table 2 are separated\ninto three sub-tables, showing the results for BERT,\nBART and baseline models on the RST-DT and\nGUM treebanks, respectively. In the BERT and\nBART sub-table, we further annotate each perfor-\nmance with ↑, •, ↓, indicating the relative perfor-\nmance to the standard pre-trained model as supe-\n8For a more detailed analysis of the min., mean, median\nand max. self-attention performances see Appendix D.\n2381\nrior, equal, or inferior.\nTaking a look at the top sub-table (BERT) we\nfind that, as expected, the randomly initialized\ntransformer model achieves the worst performance.\nFine-tuned models perform equal or worse than the\nstandard PLM. Despite the inferior results of the\nfine-tuned models, the drop is rather small, with\nthe sentiment analysis models consistently outper-\nforming NLI. This seems reasonable, given that\nthe sentiment analysis objective is intuitively more\naligned with discourse structures (e.g., long-form\nreviews with potentially complex rhetorical struc-\ntures) than the between-sentence NLI task, not in-\nvolving multi-sentential text.\nIn the center sub-table (BART), a different trend\nemerges. While the worst performing model is still\n(as expected) the randomly initialized system, fine-\ntuned models mostly outperform the standard PLM.\nInterestingly, the model fine-tuned on the CNN-\nDM corpus consistently outperforms the BART\nbaseline, while the XSUM model performs bet-\nter on all but the GUM dependency structure eval-\nuation. On one hand, the superior performance\nof both summarization models on the RST-DT\ndataset seems reasonable, given that the fine-tuning\ndatasets and the evaluation treebank are both in the\nnews domain. The strong results of the CNN-DM\nmodel on the GUM treebank, yet inferior perfor-\nmance of XSUM, potentially hints towards depen-\ndency discourse structures being less prominent\nwhen fine-tuning on the extreme summarization\ntask, compared to the longer summaries in the\nCNN-DM corpus. The question-answering task\nevaluated through the SQuAD fine-tuned model un-\nderperforms the standard PLM on GUM, however\nreaches superior performance on RST-DT. Since\nthe SQuAD corpus is a subset of Wikipedia articles,\nmore aligned with news articles than the 12 genres\nin GUM, we believe the stronger performance on\nRST-DT (i.e., news articles) is again reasonable,\nyet shows weaker generalization capabilities across\ndomains (i.e., on the GUM corpus). Interestingly,\nthe question-answering task seems more aligned\nwith dependency than constituency trees, in line\nwith what would be expected from a factoid-style\nquestion-answering model, focusing on important\nentities, rather than global constituency structures.\nDirectly comparing the BERT and BART mod-\nels, the former performs better on three out of four\nmetrics. At the same time, fine-tuning hurts the\nperformance for BERT, however, improves BART\nFigure 3: PLM discourse constituency (left) and depen-\ndency (right) structure overlap with baselines and gold\ntrees (e.g., BERT ↔Two-Stage (RST-DT)) according\nto the original parseval and UAS metrics.\nmodels. Plausibly, these seemingly unintuitive re-\nsults may be caused by the following co-occurring\ncircumstances: (1) The inferior performance of\nBART can potentially be attributed to the decoder\ncomponent capturing parts of the discourse struc-\ntures, as well as the larger number of self-attention\nheads “diluting” the discourse information. (2)\nThe different trends regarding fine-tuned models\nmight be directly influenced by the input-length\nlimitation to 512 (BERT) and 1024 (BART) sub-\nword tokens during the fine-tuning stage, hamper-\ning the ability to capture long-distance semantic\nand pragmatic relationships. This, in turn, limits\nthe amount of discourse information captured, even\nfor document-level datasets (e.g., Yelp, CNN-DM,\nSQuAD). With this restriction being more promi-\nnent in BERT, it potentially explains the compara-\nbly low performance of the fine-tuned models.\nFinally, the bottom sub-table puts our results\nin the context of previously proposed supervised\nand distantly-supervised models, as well as sim-\nple baselines. Compared to simple right- and left-\nbranching trees (Span), the PLM-based models\nreach clearly superior performance. Looking at\nthe chain/inverse chain structures (UAS), the im-\nprovements are generally lower, however, the vast\nmajority still outperforms the baseline. Comparing\nthe first two sub-tables against completely super-\nvised methods (Two-StageRST-DT, Two-StageGUM),\nthe BERT- and BART-based models are, unsurpris-\ningly, inferior. Lastly, compared to the distantly\nsupervised SumCNN-DM and SumNYT models, the\nPLM-based discourse performance shows clear im-\nprovements over the 6-layer, 8-head standard trans-\nformer.\n2382\n(a) Head-aligned\n (b) Model-aligned\nFigure 4: Nested aggregation approach for discourse\nsimilarity. (a) Grey cells contain same-head, white cells\nindicate different heads. (b) Grey cells contain same-\nmodel, white cells indicate different models. Column\nindices equal row indices.\n5.3 Discourse Similarity\nFurther exploring what kind of discourse informa-\ntion is captured in the PLM self-attention matrices,\nwe directly compare the emergent discourse struc-\ntures with trees inferred from existing discourse\nparsers and simple baselines. This way, we aim to\nbetter understand if the information encapsulated\nin PLMs is complementary to existing methods, or\nif the PLMs solely capture trivial discourse phe-\nnomena and simple biases (e.g., resemble right-\nbranching constituency trees). Since the GUM\ndataset contains a more diverse set of test docu-\nments (12 genres) than the RST-DT corpus (exclu-\nsively news articles), we perform our experiments\nfrom here on only on the GUM treebank.\nFigure 3 shows the micro-average structural over-\nlap of discourse constituency (left) and dependency\n(right) trees between the PLM-generated discourse\nstructures and existing methods, baselines, as well\nas gold-standard trees. Noticeably, the generated\nconstituency trees (on the left) are most aligned\nwith the structures predicted by supervised dis-\ncourse parsers, showing only minimal overlap to\nsimple structures (i.e., right- and left-branching\ntrees). Taking a closer look at the generated de-\npendency structures presented on the right side\nin Figure 3, the alignment between PLM inferred\ndiscourse trees and the simple chain structure is\npredominant, suggesting a potential weakness in\nregards to the discourse exposed by the Eisner algo-\nrithm in the BERT and BART model. Not surpris-\ningly, the highest overlap between PLM-generated\ntrees and the chain structure occurs when fine-\ntuning on the CNN-DM dataset, well-known to\ncontain a strong lead-bias (Xing et al., 2021).\nTo better understand if the PLM-based con-\nstituency structures are complementary to existing,\n(a) Constituency Similarity\n (b) Dependency Similarity\nFigure 5: BERT self-attention similarities on GUM.\nTop: Visual analysis of head-aligned ( I&III ) and\nmodel-aligned (II &IV ) heatmaps. Yellow=high struc-\ntural overlap, purple=low structural overlap.\nBottom: Aggregated similarity of same heads, same\nmodels, different heads and different models showing\nthe min, max and quartiles of the underlying distribution.\n*Significantly better than respective ̸=Head/̸=Model\nperformance with p-value < 0.05.\nsupervised discourse parsers, we further analyze\nthe correctly predicted overlap. More specifically,\nwe compute the intersection between PLM gener-\nated structures and gold-standard trees as well as\npreviously proposed models and the gold-standard.\nSubsequently, we intersect the two resulting sets\n(e.g., BERT ∩Gold Trees ↔Two-Stage (RST-DT)\n∩Gold Trees). This way, we explore if the cor-\nrectly predicted PLM discourse structures are a\nsubset of the correctly predicted trees by super-\nvised approaches, or if complementary discourse\ninformation is captured. We find that > 20% and\n> 16% of the correctly predicted constituency and\ndependency structures of our PLM discourse in-\nference approach are not captured by supervised\nmodels, making the exploration of ensemble meth-\nods a promising future avenue. A detailed version\nof Fig. 3 as well as more specific results regarding\nthe correctly predicted overlap of discourse struc-\ntures are shown in Appendix E.\n5.4 Discourse Redundancy\nUp to this point, our quantitative analysis of the\nability of PLMs to capture discourse information\nhas been limited to the single best-performing head.\nHowever, looking at individual models, the dis-\ncourse performance distribution in Figure 2 sug-\ngests that a larger subset of self-attention heads\nperforms similarly well (i.e., there are several dark\npurple cells in each heatmap). This leads to the\ninteresting questions if the information captured\n2383\nin different, top-performing self-attention heads is\nredundant or complementary. Similarly, Figure 2\nindicates that the same heads perform well across\ndifferent fine-tuning tasks, leading to the question\nif the discourse structures captured in a single self-\nattention matrix of different fine-tuned models is\nconsistent, or varies depending on the underlying\ntask. Hence, we take a detailed look at the simi-\nlarity of model self-attention heads in regards to\ntheir alignment with discourse information and ex-\nplore if (1) the top performing heads hi, ..., hk of\na specific model mm capture redundant discourse\nstructures, and if (2) the discourse information cap-\ntured by a specific head hi across different models\nmm, ..., mo contain similar discourse information.\nSpecifically, we pick the top 10 best performing\nself-attention matrices of each model, remove self-\nattention heads that don’t appear in at least two\nmodels (since no comparisons can be made), and\ncompare the generated discourse structures in a\nnested aggregation approach.\nFigure 4 shows a small-scale example of our\nnested visualization methodology. For the self-\nattention head-aligned approach (Figure 4 (a)),\nhigh similarity values (calculated as the micro-\naverage structural overlap) along the diagonal (grey\ncells) would be expected if the same head hi en-\ncodes consistent discourse information across dif-\nferent fine-tuning tasks and datasets. Inversely, the\nmodel-aligned matrix (Figure 4 (b)) should show\nhigh values along the diagonal if different heads\nhi, ..., hk in the same model mk capture redundant\ndiscourse information. Besides the visual inspec-\ntion methodology presented in Figure 4, we also\ncompare aggregated similarities between the same\nhead (=Head) against different heads (̸=Head) and\nbetween the same model (=Model) against dif-\nferent models ( ̸=Model) (i.e., grey cells (=) and\nwhite cells (̸=) in Figure 4 (a) and (b)). In order\nto assess the statistical significance of the result-\ning differences in the underlying distributions, we\ncompute a two-sided, independent t-test between\nsame/different models and same/different heads9.\nThe resulting redundancy evaluations for BERT\nare presented in Figure 5 10. It appears that the\nsame self-attention heads hi consistently encode\nsimilar discourse information across models indi-\ncated by: (1) High similarities (yellow) along the\ndiagonal in heatmaps I&III and (2) through the\n9Prior to running the t-test we confirm similar variance and\nthe assumption of normal distribution (Shapiro-Wilk test).\n10Evaluations for BART can be found in Appendix F.\nstatistically significant difference in distributions\nat the bottom of Figure 5 (a) and (b). However,\ndifferent self-attention heads hi, ..., hk of the same\nmodel mm encode different discourse information\n(heatmaps II &IV ). While the trend is stronger\nfor constituency tree structures, there is a single\ndependency self-attention head which does gen-\nerally not align well between models and heads\n(purple line in heatmap III ). Plausibly, this spe-\ncific self-attention head encodes fine-tuning task\nspecific discourse information, making it a prime\ncandidate for further investigations in future work.\nFurthermore, the similarity patterns observed in\nFigure 5 (a) and (b) point towards an opportunity to\ncombine model self-attention heads to improve the\ndiscourse inference performance compared to the\nscores shown in Table 2, where each self-attention\nhead was assessed individually, in future work.\n6 Conclusions\nIn this paper, we extend the line ofBERTologywork\nby focusing on the important, yet less explored,\nalignment of pre-trained and fine-tuned PLMs with\nlarge-scale discourse structures. We propose a\nnovel approach to infer discourse information for\narbitrarily long documents. In our experiments,\nwe find that the captured discourse information is\nconsitently local and general, even across a collec-\ntion of fine-tuning tasks. We compare the inferred\ndiscourse trees with supervised, distantly super-\nvised and simple baselines to explore the structural\noverlap, finding that constituency discourse trees\nalign well with supervised models, however, con-\ntain complementary discourse information. Lastly,\nwe individually explore self-attention matrices to\nanalyze the information redundancy. We find that\nsimilar discourse information is consistently cap-\ntured in the same heads.\nIn the future, we intend to explore additional dis-\ncourse inference strategies based on the insights we\ngained in this analysis. Specifically, we want to ex-\nplore more sophisticated methods to extract a single\ndiscourse tree from multiple self-attention matrices,\nrather than only the single best-performing head.\nFurther, we want to investigate the relationship\nbetween supervised discourse parsers and PLM\ngenerated discourse trees and more long term, we\nplan to analyze PLMs with enhanced input-length\nlimitations.\n2384\nAcknowledgements\nWe thank the anonymous reviewers and the UBC\nNLP group for their insightful comments and sug-\ngestions. This research was supported by the Lan-\nguage & Speech Innovation Lab of Cloud BU,\nHuawei Technologies Co., Ltd and the Natural\nSciences and Engineering Research Council of\nCanada (NSERC). Nous remercions le Conseil de\nrecherches en sciences naturelles et en génie du\nCanada (CRSNG) de son soutien.\nReferences\nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and\nJimmy Lin. 2019. Docbert: Bert for document classi-\nfication. arXiv preprint arXiv:1904.08398.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nParminder Bhatia, Yangfeng Ji, and Jacob Eisenstein.\n2015. Better document-level sentiment analysis from\nRST discourse parsing. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2212–2218, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nLynn Carlson, Mary Ellen Okurowski, and Daniel\nMarcu. 2002. RST discourse treebank. Linguistic\nData Consortium, University of Pennsylvania.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nQiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-\nder J. Smola, Jing Jiang, and Chong Wang. 2014.\nJointly modeling aspects, ratings and sentiments for\nmovie recommendation (jmars). In Proceedings of\nthe 20th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, KDD ’14,\npage 193–202, New York, NY , USA. Association for\nComputing Machinery.\nJason M. Eisner. 1996. Three new probabilistic models\nfor dependency parsing: An exploration. In COLING\n1996 Volume 1: The 16th International Conference\non Computational Linguistics.\nShima Gerani, Giuseppe Carenini, and Raymond T. Ng.\n2019. Modeling content and structure for abstractive\nreview summarization. Computer Speech & Lan-\nguage, 53:302–331.\nGrigorii Guz and Giuseppe Carenini. 2020. Corefer-\nence for discourse parsing: A neural approach. In\nProceedings of the First Workshop on Computational\nApproaches to Discourse , pages 160–167, Online.\nAssociation for Computational Linguistics.\nGrigorii Guz, Patrick Huber, and Giuseppe Carenini.\n2020. Unleashing the power of neural discourse\nparsers - a context and structure aware approach us-\ning large scale pretraining. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 3794–3805, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nHugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,\net al. 2010. Hilda: A discourse parser using support\nvector machine classification. Dialogue & Discourse,\n1(3).\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for finding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlexander Hogenboom, Flavius Frasincar, Franciska\nde Jong, and Uzay Kaymak. 2015. Using rhetori-\ncal structure in sentiment analysis. Commun. ACM,\n58(7):69–77.\nPatrick Huber and Giuseppe Carenini. 2020. MEGA\nRST discourse treebanks with structure and nuclear-\nity from scalable distant sentiment supervision. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7442–7457, Online. Association for Computa-\ntional Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651–3657, Florence, Italy. Association for\nComputational Linguistics.\nYangfeng Ji and Jacob Eisenstein. 2014. Represen-\ntation learning for text-level discourse parsing. In\nProceedings of the 52nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 13–24, Baltimore, Maryland.\nAssociation for Computational Linguistics.\n2385\nYangfeng Ji and Noah A. Smith. 2017. Neural dis-\ncourse structure for text categorization. In Proceed-\nings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 996–1005, Vancouver, Canada. Association for\nComputational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77.\nShafiq Joty, Giuseppe Carenini, and Raymond T. Ng.\n2015. CODRA: A novel discriminative framework\nfor rhetorical analysis. Computational Linguistics,\n41(3):385–435.\nDan Jurafsky and James H Martin. 2014. Speech and\nlanguage processing, volume 3. Pearson London.\nTaeuk Kim, Jihun Choi, Daniel Edmiston, and Sang goo\nLee. 2020. Are pre-trained language models aware\nof phrases? simple but strong baselines for grammar\ninduction. In International Conference on Learning\nRepresentations.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In Inter-\nnational Conference on Learning Representations.\nNaoki Kobayashi, Tsutomu Hirao, Hidetaka Kamigaito,\nManabu Okumura, and Masaaki Nagata. 2020. Top-\ndown rst parsing utilizing granularity levels in doc-\numents. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 34, pages 8099–8106.\nNaoki Kobayashi, Tsutomu Hirao, Kengo Nakamura,\nHidetaka Kamigaito, Manabu Okumura, and Masaaki\nNagata. 2019. Split or merge: Which is better for\nunsupervised RST parsing? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5797–5802, Hong Kong,\nChina. Association for Computational Linguistics.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2021a.\nDiscourse probing of pretrained language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3849–3864, Online. Association for Computa-\ntional Linguistics.\nFajri Koto, Jey Han Lau, and Timothy Baldwin. 2021b.\nTop-down discourse parsing via sequence labelling.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume, pages 715–726, Online.\nAssociation for Computational Linguistics.\nMurathan Kurfalı and Robert Östling. 2021. Prob-\ning multilingual language models for discourse. In\nProceedings of the 6th Workshop on Representation\nLearning for NLP (RepL4NLP-2021) , pages 8–19,\nOnline. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nSujian Li, Liang Wang, Ziqiang Cao, and Wenjie Li.\n2014. Text-level discourse dependency parsing. In\nProceedings of the 52nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 25–35, Baltimore, Maryland.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nWilliam C Mann and Sandra A Thompson. 1988.\nRhetorical structure theory: Toward a functional the-\nory of text organization. Text-Interdisciplinary Jour-\nnal for the Study of Discourse, 8(3):243–281.\nDavid Mare ˇcek and Rudolf Rosa. 2019. From\nbalustrades to pierre vinken: Looking for syntax in\ntransformer self-attentions. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 263–\n275, Florence, Italy. Association for Computational\nLinguistics.\nJulian Michael, Jan A. Botha, and Ian Tenney. 2020.\nAsking without telling: Exploring latent ontologies\nin contextual representations. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6792–6812,\nOnline. Association for Computational Linguistics.\nMathieu Morey, Philippe Muller, and Nicholas Asher.\n2017. How much progress have we made on RST\ndiscourse parsing? a replication study of recent re-\nsults on the RST-DT. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1319–1324, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇa˘glar GuÌ‡lçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond. In Proceedings of The 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\n2386\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nBita Nejat, Giuseppe Carenini, and Raymond Ng. 2017.\nExploring joint neural model for sentence level dis-\ncourse parsing and sentiment analysis. In Proceed-\nings of the 18th Annual SIGdial Meeting on Dis-\ncourse and Dialogue, pages 289–298, Saarbrücken,\nGermany. Association for Computational Linguistics.\nThanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty,\nand Xiaoli Li. 2021. RST parsing from scratch. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1613–1625, Online. Association for Computa-\ntional Linguistics.\nBarlas O˘guz, Kushal Lakhotia, Anchit Gupta, Patrick\nLewis, Vladimir Karpukhin, Aleksandra Piktus,\nXilun Chen, Sebastian Riedel, Wen-tau Yih,\nSonal Gupta, et al. 2021. Domain-matched pre-\ntraining tasks for dense retrieval. arXiv preprint\narXiv:2107.13602.\nLalchand Pandia, Yan Cong, and Allyson Ettinger. 2021.\nPragmatic competence of pre-trained language mod-\nels through the lens of discourse connectives. In Pro-\nceedings of the 25th Conference on Computational\nNatural Language Learning, pages 367–379, Online.\nAssociation for Computational Linguistics.\nYannis Papanikolaou, Ian Roberts, and Andrea Pierleoni.\n2019. Deep bidirectional transformers for relation\nextraction without supervision. In Proceedings of\nthe 2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019) , pages 67–75,\nHong Kong, China. Association for Computational\nLinguistics.\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-\nsakaki, Livio Robaldo, Aravind Joshi, and Bonnie\nWebber. 2008. The Penn Discourse TreeBank 2.0.\nIn Proceedings of the Sixth International Conference\non Language Resources and Evaluation (LREC’08),\nMarrakech, Morocco. European Language Resources\nAssociation (ELRA).\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learning\nRepresentations.\nAlessandro Raganato and Jörg Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nYizhong Wang, Sujian Li, and Houfeng Wang. 2017.\nA two-stage parsing method for text-level discourse\nanalysis. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 2: Short Papers) , pages 184–188, Vancouver,\nCanada. Association for Computational Linguistics.\nBonnie Webber, Matthew Stone, Aravind Joshi, and Al-\nistair Knott. 2003. Anaphora and discourse structure.\nComputational Linguistics, 29(4):545–587.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nZhiyong Wu, Yun Chen, Ben Kao, and Qun Liu. 2020.\nPerturbed masking: Parameter-free probing for ana-\nlyzing and interpreting BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4166–4176, Online. Asso-\nciation for Computational Linguistics.\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman\nCohan. 2021a. Primer: Pyramid-based masked sen-\ntence pre-training for multi-document summarization.\narXiv preprint arXiv:2110.08499.\nWen Xiao, Patrick Huber, and Giuseppe Carenini.\n2021b. Predicting discourse trees from transformer-\nbased neural summarizers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4139–4152, On-\nline. Association for Computational Linguistics.\n2387\nLinzi Xing, Wen Xiao, and Giuseppe Carenini. 2021.\nDemoting the lead bias in news summarization via al-\nternating adversarial learning. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 948–954, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nManzil Zaheer, Guru Prashanth Guruganesh, Avi Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Minh Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Mahmoud El Houssieny Ahmed.\n2020. Big bird: Transformers for longer sequences.\nIn Advances in Neural Information Processing Sys-\ntems.\nAmir Zeldes. 2017. The GUM corpus: Creating mul-\ntilayer resources in the classroom. Language Re-\nsources and Evaluation, 51(3):581–612.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Proceedings of the 28th International\nConference on Neural Information Processing Sys-\ntems - Volume 1, NIPS’15, page 649–657, Cambridge,\nMA, USA. MIT Press.\nZining Zhu, Chuer Pan, Mohamed Abdalla, and Frank\nRudzicz. 2020. Examining the rhetorical capacities\nof neural language models. In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 16–32,\nOnline. Association for Computational Linguistics.\n2388\nA Huggingface Models\nWe investigate 7 fine-tuned BERT and BART models from the huggingface model library, as well as the\ntwo pre-trained models. The model names and links are provided in Table 3\nPre-Trained Fine-Tuned Link\nBERT-base – https://huggingface.co/bert-base-uncased\nBERT-base IMDB https://huggingface.co/textattack/bert-base-uncased-imdb\nBERT-base Yelp https://huggingface.co/fabriceyhc/bert-base-uncased-yelp_polarity\nBERT-base SST-2 https://huggingface.co/textattack/bert-base-uncased-SST-2\nBERT-base MNLI https://huggingface.co/textattack/bert-base-uncased-MNLI\nBART-large – https://huggingface.co/facebook/bart-large\nBART-large CNN-DMhttps://huggingface.co/facebook/bart-large-cnn\nBART-large XSUM https://huggingface.co/facebook/bart-large-xsum\nBART-large SQuAD https://huggingface.co/valhalla/bart-large-finetuned-squadv1\nTable 3: Huggingface pre-trained and fine-tuned model links.\nB Test-Set Results on RST-DT and GUM\n(a) BERT: PLM, +IMDB, +Yelp, +MNLI, +SST-2\n(b) BART: PLM, +CNN-DM, +XSUM, +SQuAD\nFigure 6: Constituency (top) and dependency (bottom) discourse tree evaluation of BERT (a) and BART (b) models\non RST-DT (test). Purple=high score, blue=low score. + indicates fine-tuning dataset.\n2389\n(a) BERT: PLM, +IMDB, +Yelp, +MNLI, +SST-2\n(b) BART: PLM, +CNN-DM, +XSUM, +SQuAD\nFigure 7: Constituency (top) and dependency (bottom) discourse tree evaluation of BERT (a) and BART (b) models\non GUM (test). Purple=high score, blue=low score. + indicates fine-tuning dataset.\n2390\nC Oracle-picked self-attention head compared to validation-picked matrix\nModel RST-DT GUM\nSpan UAS Span UAS\nBERT\nrand. init 25.5 (-0.0) 13.3 (-0.0) 23.2 (-0.0) 12.4 (-0.0)\nPLM 35.7 (-1.6) 45.3 (-4.9) 33.0 (-0.4) 45.2 (-0.0)\n+ IMDB 35.4 (-1.8) 42.8 (-2.4) 33.0 (-3.8) 43.3 (-0.1)\n+ Yelp 34.7 (-1.0) 42.3 (-1.9) 32.6 (-3.6) 43.7 (-0.0)\n+ SST-2 35.5 (-1.9) 42.9 (-2.5) 32.6 (-0.3) 43.5 (-0.9)\n+ MNLI 34.8 (-1.7) 41.8 (-1.4) 32.4 (-0.3) 43.3 (-0.5)\nBART\nrand. init 25.3 (-0.0) 12.5 (-0.0) 23.2 (-0.0) 12.2 (-0.0)\nPLM 39.1 (-0.4) 41.7 (-2.7) 31.8 (-0.3) 41.8 (-0.0)\n+ CNN-DM 40.9 (-0.0) 44.3 (-4.0) 32.7 (-0.3) 42.8 (-0.7)\n+ XSUM 40.1 (-0.9) 41.9 (-3.4) 32.1 (-1.7) 39.9 (-0.0)\n+ SQuAD 40.1 (-0.0) 43.2 (-4.6) 31.3 (-2.1) 40.7 (-0.1)\nBaselines\nRight-Branch/Chain 9.3 40.4 9.4 41.7\nLeft-Branch/Chain-1 7.5 12.7 1.5 12.2\nSumCNN-DM(2021b) 21.4 20.5 17.6 15.8\nSumNYT(2021b) 24.0 15.7 18.2 12.6\nTwo-StageRST-DT(2017) 72.0 71.2 54.0 54.5\nTwo-StageGUM 65.4 61.7 58.6 56.7\nTable 4: Original parseval (Span) and Unlabelled Attachment Score (UAS) of the single best performing oracle\nself-attention matrix and validation-set picked head (in brackets) of the BERT and BART models compared with\nbaselines and previous work. “rand. init\"=Randomly initialized transformer model of similar architecture as the\nPLM.\n2391\nD Detailed Self-Attention Statistics\nModel Span Eisner\nMin Med Mean Max Min Med Mean Max\nRST-DT\nrand. init 21.7 23.4 23.4 25.5 7.5 10.3 10.3 13.3\nPLM 19.3 27.0 27.4 35.7 6.6 17.4 21.6 45.3\n+ IMDB 19.7 26.9 27.2 35.4 6.6 16.9 21.3 42.8\n+ YELP 20.2 26.6 26.9 34.7 7.0 16.5 21.0 42.3\n+ SST-2 19.5 27.3 27.7 35.5 7.3 17.6 21.9 42.9\n+ MNLI 18.5 26.9 27.1 34.8 6.9 17.5 21.5 41.8\nGUM\nrand. init 18.6 21.0 21.0 23.2 7.9 10.1 10.1 12.4\nPLM 17.8 24.2 24.3 32.6 6.7 16.0 21.2 45.2\n+ IMDB 18.1 23.8 24.1 32.7 6.1 15.9 21.0 43.3\n+ YELP 18.6 24.0 23.9 32.3 7.0 15.8 20.7 43.7\n+ SST-2 18.2 24.6 24.7 32.3 6.5 16.5 21.6 43.5\n+ MNLI 17.4 23.9 24.2 32.1 6.8 16.6 21.3 43.3\nTable 5: Minimum, median, mean and maximum performance of the self-attention matrices on RST-DT and GUM\nfor the BERT model.\nModel Span Eisner\nMin Med Mean Max Min Med Mean Max\nRST-DT\nrand. init 20.3 23.3 23.3 25.3 8.5 10.6 10.6 12.5\nPLM 20.3 28.3 28.5 39.1 4.1 15.8 19.2 41.7\n+ CNN-DM 20.5 28.6 28.7 40.9 3.6 15.2 19.2 44.3\n+ XSUM 20.2 27.6 28.3 40.1 4.8 14.8 18.7 41.9\n+ SQuAD 20.5 27.6 28.2 40.1 2.8 14.8 18.8 43.2\nGUM\nrand. init 18.6 21.0 21.0 23.2 8.0 10.2 10.2 12.2\nPLM 16.7 23.4 23.8 31.5 2.6 15.2 18.7 41.8\n+ CNN-DM 15.9 23.7 24.1 32.4 3.7 14.7 18.9 42.8\n+ XSUM 16.4 23.2 23.9 31.8 3.0 14.1 18.1 39.9\n+ SQuAD 16.1 23.4 23.8 31.0 2.4 14.8 18.3 40.7\nTable 6: Minimum, median, mean and maximum performance of the self-attention matrices on RST-DT and GUM\nfor the BART model.\n2392\nE Details of Structural Discourse Similarity\nFigure 8: Detailed PLM discourse constituency (left) and dependency (right) structure overlap with baselines and\ngold trees according to the original parseval and UAS metrics.\nFigure 9: Detailed PLM discourse constituency (left) and dependency (right) structure performance of intersection\nwith gold trees (e.g., BERT ∩Gold Trees ↔Two-Stage (RST-DT)∩Gold Trees) according to the original parseval\nand UAS metrics.\n2393\nF Intra- and Inter-Model Self-Attention Comparison\nHeatmaps sorted by heads (left) and models (right)\n0.4 0.6 0.8\n=Head*\n̸ =Head\n=Model\n̸ =Model\n(a) BERT constituency tree similarity on GUM\nHeatmaps sorted by heads (left) and models (right)\n0.7 0.8 0.9 1\n=Head*\n̸ =Head\n=Model\n̸ =Model\n(b) BERT dependency tree similarity on GUM\nHeatmaps sorted by heads (left) and models (right)\n0.3 0.4 0.5 0.6 0.7\n=Head*\n̸ =Head\n=Model*\n̸ =Model\n(c) BART constituency tree similarity on GUM\nHeatmaps sorted by heads (left) and models (right)\n0.7 0.8 0.9 1\n=Head*\n̸ =Head\n=Model*\n̸ =Model\n(d) BART dependency tree similarity on GUM\nFigure 10: Top: Visual analysis of sorted heatmaps. Yellow=high score, purple=low score.\nBottom: Aggregated similarity of same heads, same models, different heads and different models. *=Head/=Model\nsignificantly better than ̸=Head/̸=Model performance with p-value < 0.05.\n2394",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.752577543258667
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.5778017640113831
    },
    {
      "name": "Natural language processing",
      "score": 0.577786922454834
    },
    {
      "name": "Artificial intelligence",
      "score": 0.553389310836792
    },
    {
      "name": "Language model",
      "score": 0.4878721237182617
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4474848210811615
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.42286232113838196
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}