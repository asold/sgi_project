{
  "title": "SSDT: Scale-Separation Semantic Decoupled Transformer for Semantic Segmentation of Remote Sensing Images",
  "url": "https://openalex.org/W4394627318",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2258751060",
      "name": "Chengyu Zheng",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2133400790",
      "name": "Yanru Jiang",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A1990528846",
      "name": "Xiaowei Lv",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2310812270",
      "name": "Jie Nie",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2120107323",
      "name": "Xinyue Liang",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2098844654",
      "name": "Zhiqiang Wei",
      "affiliations": [
        "Ocean University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2947263797",
    "https://openalex.org/W2767943769",
    "https://openalex.org/W2963833733",
    "https://openalex.org/W1990372984",
    "https://openalex.org/W2609077090",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W6640295612",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6739696289",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W3177052299",
    "https://openalex.org/W3103092912",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W3161559204",
    "https://openalex.org/W3035526186",
    "https://openalex.org/W3166177850",
    "https://openalex.org/W4214674383",
    "https://openalex.org/W3036843520",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W4372259934",
    "https://openalex.org/W4296079303",
    "https://openalex.org/W4386138397",
    "https://openalex.org/W4321484009",
    "https://openalex.org/W4385256752",
    "https://openalex.org/W4382176353",
    "https://openalex.org/W4313525856",
    "https://openalex.org/W4312977443",
    "https://openalex.org/W4280563105",
    "https://openalex.org/W4283450732",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W4205138939",
    "https://openalex.org/W4214708455",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W2990775046",
    "https://openalex.org/W3109196706",
    "https://openalex.org/W3130788620",
    "https://openalex.org/W3043645330",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3166525903",
    "https://openalex.org/W3107113572",
    "https://openalex.org/W4312572778",
    "https://openalex.org/W6941273861",
    "https://openalex.org/W6799579066",
    "https://openalex.org/W2889985731",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W3206476077",
    "https://openalex.org/W3099319035",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3127751679"
  ],
  "abstract": "As we all know, semantic segmentation of remote sensing (RS) images is to classify the images pixel by pixel to realize the semantic decoupling of the images. Most traditional semantic decoupling methods only decouple and do not perform scale-separation operations, which leads to serious problems. In the semantic decoupling process, if the feature extractor is too large, it will ignore the small-scale targets; if the feature extractor is too small, it will lead to the separation of large-scale target objects and reduce the segmentation accuracy. To address this concern, we propose a scale-separated semantic decoupled transformer (SSDT), which first performs scale-separation in the semantic decoupling process and uses the obtained scale information-rich semantic features to guide the Transformer to extract features. The network consists of five modules, scale-separated patch extraction (SPE), semantic decoupled transformer (SDT), scale-separated feature extraction (SFE), semantic decoupling (SD), and multiview feature fusion decoder (MFFD). In particular, SPE turns the original image into a linear embedding sequence of three scales; SD divides pixels into different semantic clusters by K-means, and further obtains scale information-rich semantic features; SDT improves the intraclass compactness and interclass looseness by calculating the similarity between semantic features and image features, the core of which is decoupled attention. Finally, MFFD is proposed to fuse salient features from different perspectives to further enhance the feature representation. Our experiments on two large-scale fine-resolution RS image datasets (Vaihingen and Potsdam) demonstrate the effectiveness of the proposed SSDT strategy in RS image semantic segmentation tasks.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nSSDT: Scale-separation Semantic Decoupled\nTransformer for Semantic Segmentation of Remote\nSensing Images\nChengyu Zheng†, Yanru Jiang†, Xiaowei Lv,\nJie Nie*, Member, IEEE, Xinyue Liang*, Member, IEEE, Zhiqiang Wei, Member, IEEE\nAbstract—As we all know, semantic segmentation of remote\nsensing (RS) images is to classify the images pixel by pixel to\nrealize the semantic decoupling of the images. Most traditional\nsemantic decoupling methods only decouple and do not perform\nscale-separation operations, which leads to serious problems.\nIn the semantic decoupling process, if the feature extractor is\ntoo large, it will ignore the small-scale targets; if the feature\nextractor is too small, it will lead to the separation of large-\nscale target objects and reduce the segmentation accuracy. To\naddress this concern, we propose a Scale-separated Seman-\ntic Decoupled Transformer(SSDT), which first performs scale-\nseparation in the semantic decoupling process and uses the\nobtained scale information-rich semantic features to guide the\nTransformer to extract features. The network consists of five\nmodules, Scale-separated Patch Extraction (SPE), Semantic De-\ncoupled Transformer (SDT), Scale-separated Feature Extraction\n(SFE), Semantic Decoupling(SD), and Multi-view Feature Fusion\nDecoder (MFFD). In particular, SPE turns the original image into\na linear embedding sequence of three scales; SD divides pixels\ninto different semantic clusters by K-means, and further obtains\nscale information-rich semantic features; SDT improves the intra-\nclass compactness and inter-class looseness by calculating the\nsimilarity between semantic features and image features, the\ncore of which is Decouped Attention. Finally, MFFD is proposed\nto fuse salient features from different perspectives to further\nenhance the feature representation. Our experiments on two\nlarge-scale fine-resolution RS image datasets (Vaihingen and\nPotsdam) demonstrate the effectiveness of the proposed SSDT\nstrategy in RS image semantic segmentation tasks.\nIndex Terms —Article submission, IEEE, IEEEtran, journal,\nLATEX, paper, template, typesetting.\nI. I NTRODUCTION\nW\nITH the rapid development of new technologies such\nas satellite sensors and aerospace, remote sensing (RS)\ntechnology continues to progress and image resolution is\nincreasing. Rational analysis and use of high-resolution remote\nThis work was supported in part by the National Natural Science Foundation\nof China (62072418,62172376) the Fundamental Research Funds for the\nCentral Universities (202042008), the Major Scientific and Technological\nInnovation Project (2019JZZY020705), the Key Research and Development\nProgram of Qingdao Science and Technology Plan (21-1-2-18-xx) and the\nCentral Government Guide Local Science and Technology Development\nSpecial Fund Project (YDZX2022028). (†:Chengyu Zheng and Yanru Jiang\nare co-first authors.) (*:Corresponding author: Jie Nie and Xinyue Liang.)\nThe authors are with the College of Information Sci-\nence and Engineering, Ocean University of China. (e-mail:\nzhengchengyu@stu.ouc.edu.cn; jiangyanru@stu.ouc.edu.cn; lvxi-\naowei@stu.ouc.edu.cn; niejie@ouc.edu.cn;liangxinyue@ouc.edu.cn\nweizhiqiang@ouc.edu.cn).\nFig. 1. Here, we illustrate the current problems, (a) is the unsatisfactory\ndecoupling result caused by the traditional decoupling method. The red box\nshows semantic entanglement in remote sensing images: the ”Tree” in the\ngreen box is coupled above the ”Low Vegetation” in the blue box. In the\nprocess of decoupling the two semantics, if the feature extractor is too large,\nsuch as the convolution kernel of CNN is set large, it will mainly decouple\nthe Low Vegetation semantics in the blue box, resulting in ignoring the small-\nscale target object tree. If the feature extractor is too small, it will decouple\nthe Tree semantics in the green box, which will lead to the separation of the\nLow Vegetation of the big-scale target object and decrease the segmentation\naccuracy. (b) shows our semantic decoupling method. In the process of\nsemantic decoupling, a scale-separation operation is carried out to avoid the\ninterference of scale information on decoupling, and the most ideal decoupling\nresult can be obtained.\nsensing images are significant to monitoring disaster forecast-\ning, autonomous driving, and national land resource protection\n[1]–[4]. Semantic segmentation [5] is an important topic in\ncomputer vision, which aims to achieve region segmentation\nby determining the class of individual pixels in an image and\nthen recognizing the semantic information of that class to\nsuperimpose high-level semantics on the segmentation result.\nIn recent years, deep learning has led to breakthroughs in\nthe field of semantic segmentation, but the task of semantic\nsegmentation of remote-sensing images remains challenging\ndue to the differences between remote-sensing optical images\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nand ordinary images, and the existence of a large number of\ntarget objects with different scales and different semantics in\nremote sensing images.\nIn recent years, most of the cutting-edge semantic segmenta-\ntion models have been based on convolutional neural networks\n(CNNs), which further extend the scope of semantic segmen-\ntation and improve the accuracy of distinguishing recognized\nobjects. The classical deep learning semantic segmentation\nnetworks include CNN-based FCN [6], UNet [7], SegNet [8],\netc, in which the ”Encoder-Decoder” paradigm [7]–[10] is the\nmain network structure framework. Since contextual informa-\ntion is the most critical factor in improving the performance\nof semantic segmentation, Chen et al. proposed the DeepLab\nseries [11]–[14] to explore multi-scale contextual information\nto improve target recognition at different scales. In addition,\nusing attention mechanisms [15]–[18] to capture contextual\ninformation or feature extraction through graph convolutional\nnetworks(GCNs) [19]–[24] can further model the relationships\nbetween target objects. MSCG-Net [24] is based on GCN\nto establish connections between pixels by building nodes\nand edges and can integrate context information to obtain\nbetter performance. With Transformer’s excellent performance\nin NLP, the introduction of ViT [25] led the field of computer\nvision to take a significant step forward, and [26]–[29] fully\nexplored the segmentation capability of ViT and improved\nthe global long-range modeling capability of the network.\nThe most popular of these methods is the Swin Transformer\n[29], which proposes a hierarchical transformer that brings\nhigher efficiency by restricting the self-attentive computation\nto non-overlapping local windows, while still allowing cross-\nwindow connections. Swin-UNet [30] adds the idea of UNet\non its basis and constructs a purely transformer-based U-\nshaped structure.\nAlthough the above techniques have contributed to promot-\ning progress in the field of semantic segmentation, the above\nmethods still have shortcomings when used in the field of\nremote sensing images: they cannot segment the coupled target\nobjects accurately on the scale entangled feature space. Tra-\nditional methods such as Swin-UNet [30], although the inter-\nrelationship between individual semantic features is extracted\nusing the attention mechanism to achieve semantic decoupling,\nno scale-separation operation is performed in the semantic\ndecoupling process. This will lead to a serious problem that\nthe semantics of each scale are mixed and the cluttered scale\ninformation will affect the semantic judgment. In addition,\ndue to the different imaging principles, remote sensing images\nwill contain more information than ordinary images, which\nmakes it more difficult the recognition of target objects.\nTherefore, it is not reliable to use only semantic information\nfor remote sensing image semantic segmentation, which limits\nthe semantic extraction ability of the segmentation model.\nFig.1-(a) shows the unsatisfactory decoupling result caused\nby the traditional decoupling method. It is obvious that the\nremote sensing image has two coupled semantics in the same\nlocation space: the ”Tree” in the green box is coupled above\nthe ”Low Vegetation” in the blue box. In the process of\ndecoupling the two semantics, if the feature extractor is too\nlarge, such as the convolution kernel of CNN is set large, it\nwill mainly decouple the semantics in the blue box, and ignore\nthe small target object ”Tree” in the decoupling process. If the\nfeature extractor is too small, it will decouple the semantics\nin the green box, which will cause the big target object ”Low\nVegetation” to be separated, and the segmentation accuracy\nwill be reduced. From the above two points, it can be seen\nthat semantic segmentation in remote sensing images with\nseriously entangled semantic information scales is prone to\nmis-segmentation.\nTo solve the above problems, we propose semantic in-\nformation based on scale-separation to guide Transformer\ndecoupling model, to improve the accuracy of the model\ndescription of coupling target objects. Fig.1-(b) shows our\nsemantic decoupling method. In the process of semantic de-\ncoupling, scale-separation operations are carried out to avoid\nthe interference of scale information on the decoupling, and\nthe most ideal decoupling results can be obtained. First, we\nstill inherit the traditional Swin Transformer module to ensure\nthe interaction of global contextual information, using Swin-\nUNet [30] as a baseline, unlike Swin Transformer which\nuses a single scale for feature extract, this paper proposes\nScale-separated Patch Extraction (SPE), which chunks the\noriginal image at different scales sizes to generate three\nscales of linear mapping embedding sequences for global\ncontextual representation modeling. Secondly, Scale-separated\nFeature Extraction (SFE) and Semantic Decoupling (SD) are\nproposed to use scale information to divide pixels into different\nsemantic clusters by clustering methods, which can obtain\nscale information-rich semantic features. It is worth noting that\nthe semantic information we add is the semantic information\nafter scale-separation modeling extracted from the image itself,\nand it is not supervised by additional word embedding (such\nas Segmenter [27]), which avoids the difference between the\nmodes between the word and the image. In addition, Semantic\nDecoupled Transformer (SDT) is proposed to extract intra-\nclass feature interdependencies and reduce inter-class feature\nassociations by computing the similarity between semantic\nfeatures and image features, with Decouped Attention at its\ncore. Finally, Multi-view Feature Fusion Decoder (MFFD) is\nproposed to fuse salient features from different perspectives\nto further enhance the feature representation. We compare\nour method with previous methods on two public datasets.\nExperiments show that our method outperforms the state-of-\nthe-art semantic segmentation models. The main contributions\nare as follows:\n1) We propose a Scale-separation Semantic Decoupled\nTransformer for semantic segmentation of remote sensing\nimages (SSDT), which implements a semantic decoupling\nmodule within the Transformer. This can effectively avoid\nthe influence of scale coupling on semantic judgments, and\nnot only helps the Transformer to provide effective semantic\nfeatures but also helps to compensate for the lack of spatial\nlocation of the Transformer.\n2) We propose five modules: Scale-separated Patch Extrac-\ntion (SPE), Semantic Decoupled Transformer (SDT), Scale-\nseparated Feature Extraction (SFE), Semantic Decoupling\n(SD), and Multi-view Feature Fusion Decoder (MFFD). The\ntotal network framework superimposed by each module can\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nobtain the scale information-rich semantic features and use\nthe semantic features to guide and reduce the correlation of\ninter-class features to solve the problem of severe coupling of\nsemantic information scales within remote sensing images.\n3) We validated the validity of the proposed method on\nthe Potsdam dataset and the Vaihingen dataset. Several com-\nparative and ablation experiments were carried out to prove\nthe effectiveness of the scale-separated semantic decoupling\nTransformer framework in remote sensing image segmenta-\ntion.\nThe rest of this article is organized as follows. The second\nsection introduces the work related to the semantic segmen-\ntation of remote-sensing images. Section III provides details\nof the SSDT. The fourth section describes the corresponding\nexperimental results and analysis. Finally, we discuss the\ncontribution of this article and summarize it in section V .\nII. RELATED WORK\nA. Semantic segmentation based on convolutional neural net-\nworks\nIn recent years, common semantic segmentation models are\nbased on convolutional neural networks (CNNs), which further\nextend the scope of semantic segmentation and improve the\naccuracy of distinguishing recognized objects, among which\nthe classical deep learning semantic segmentation networks\ninclude FCN [6], UNet [7], SegNet [8], etc. In particular, FCN\n[6] brings semantic segmentation into an end-to-end training\nto achieve the pixel-level classification of images, thus solving\nsemantic-level image segmentation. But context information\nis the most crucial factor in improving semantic segmentation\nperformance, as different objects and scenes exhibit different\ncontextual relationships at different scales. To capture scale\ninformation between pixels more effectively, an ”Encoder-\nDecoder” paradigm is proposed [7]–[10]. UNet [7] adds low-\nlevel spatial features to high-level semantic features through\nskip-connection to achieve feature fusion at different scales,\nand PSPNet [10] adds a spatial pyramid pooling module\nto obtain a set of feature maps with different sensory field\nsizes to fuse features at different scales. To improve the\nutilization of global information, the DeepLab series [11]–\n[14] proposed by Chen et al. explores multi-scale contex-\ntual information to improve target recognition at different\nscales and increase segmentation accuracy. DeepLabV1 [11]\nproposed the concept of Dilated Convolution, increasing the\nsize of the Receive Field to enable networks to capture a\nlarger range of contextual information. However, the model\nhas two main issues: continuous pooling operations leading to\na decrease in spatial resolution, and the invariance of spatial\ntransformations required for the classifier to obtain object-\ncentered decisions. To address these issues, DeepLabV2 [12]\nintroduced atmosphere spatial pyramid pooling, which allows\nthe network to perform feature extraction at multiple different\nsampling rates, thereby segmenting objects more accurately.\nDeepLabV3 [13] improved the ResNet structure based on\nV2, enabling it to use dilated convolutions and pyramid\ndilated convolutions. This improvement enables the network\nto maintain the size of the feature map while maintaining the\nreceptive field, thereby better preserving spatial information.\nDeepLabV3+ [14] introduces dilated spatial pyramid pool-\ning and improves the upsampling method to achieve higher\nresolution and more accurate segmentation results. However,\nthere are still some challenges and limitations, such as high\ncomputational complexity and the need to improve model\nrobustness.\nFurther, some researchers perform feature extraction\nthrough graph convolution [19]–[23] to model the relationship\nbetween target objects. Other researchers have concentrated on\nattention mechanisms [15]–[18] to capture contextual informa-\ntion, which allows networks to ignore irrelevant information\nand focus on priority information, including spatial domain\nattention, channel domain attention, layer domain attention,\nhybrid domain attention, temporal domain attention, and self-\nattention mechanisms. For example, Ding et al. proposed\nLocal Attention Network(LANet) [16], which combines Patch\nAttention Module (PAM) and Attention Embedding Module\n(AEM) to obtain the degree of inter-location dependency by\ncalculating the pixel-to-pixel similarity and incorporating the\nneighboring pixel information into the computed pixels, thus\nfreeing the fusion of global information from the limitation\nof image distance. SENet [17] can be interpreted as making\nthe model focus more on a certain aspect of features and\ncorrespondingly will assign weights to each channel, thus dis-\ntinguishing the importance of different features and achieving\nthe effect of reinforcing a certain feature. DANet [18] attached\ntwo attention modules to the Dilated FCN based on a self-\nattentive mechanism, modeling semantic dependencies in the\nspatial and channel dimensions, respectively. However, the\nmulti-scale features extracted by Atrous convolution or pyra-\nmidal pooling are limited, so LoG-CAN [31] which improves\nthe segmentation performance of remote sensing images by\nutilizing local details and global semantic information of the\nimage is proposed. Hang et al. [32] uses a ”multi-scale pro-\ngressive segmentation network” to gradually segment objects\ninto small, large, and other scales, which solves the problem\nthat due to the limited learning capacity of each CNN, it tends\nto make trade-offs when segmenting objects of different scales.\nWang et al. [33] proposed Structure-driven Relation Graph\nNetworks, which utilize graph networks to model complex\nrelationships between objects and capture subtle differences\nbetween them through a structure-driven approach, thereby\nimproving the accuracy of fine-grained recognition.\nIn addition, models for hyperspectral data are also grow-\ning.SDEnet [34] utilizes single-source hyperspectral data and\nunlabeled data in the target scene to design a generator that\nincludes a semantic coder and a morphological coder for the\npurpose of classifying hyperspectral images in the target scene\nand achieves good performance on the cross-scene hyperspec-\ntral image classification task. The FHS-SSL [35] algorithm\nis a self-supervised learning method for hyperspectral image\nclassification using unlabeled data. The algorithm introduces\nmigration learning and meta-learning to further improve the\nclassification accuracy. However, the above model requires a\nlarge amount of unlabeled data for training, and some labeled\ndata need to be manually selected for supervised learning.\nTherefore, GACP [36] using graph neural networks, ARMA\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nfilters, and parallel convolutional neural networks has been\nproposed to be able to combine the spatial structure and\nspectral features of hyperspectral data to improve the accuracy\nof hyperspectral image classification. However, the GACP\nalgorithm has high computational complexity and requires a\nlong training time. Then the cross-scene hyperspectral image\nclassification method LDGnet [37] is proposed, which is a\njoint modeling method using linguistic modalities and visual\nmodalities with prior knowledge of remotely sensed features.\nThe core idea is to align visual and linguistic features category-\nby-category to output the classification prediction probability\nof visual modalities, which can improve classification accuracy\nand reduce the dependence on domain knowledge.\nAlthough discriminable feature learning methods based on\nattention mechanisms can extract discriminative information in\nimages. However, there are still two problems: first, in the field\nof semantic segmentation of remote sensing images, there is no\nrelevant research that considers both intra-class compactness\nand inter-class looseness of remote sensing objects; second,\nalthough the existing multi-scale based semantic segmentation\nmethods are relatively mature, the scale coupling phenomenon\nof semantic information is serious, which makes the results\nof multi-scale prediction models unreliable. Therefore, the\nabove methods cannot maximize the accuracy of semantic\nsegmentation.\nB. Semantic segmentation based on Transformer\nWith the excellent performance of Transformer in NLP, the\nintroduction of ViT [25] led the field of computer vision to\ntake a big step forward. Zheng et al. proposed SETR [26] to\nanalyze image segmentation from a sequence perspective and\nthoroughly explored the segmentation capability of ViT. Robin\net al. proposed a semantic segmentation method(Segmenter)\n[27] using only Transformer for context modeling, which has\nthe advantage of capturing global interactions between scene\nelements using the global image context at each layer of the\nmodel and improving global dependencies between features\nby using predefined class embeddings in the decoding part to\ncapture semantic information by masking Transformer to ob-\ntain class labels. SegFormat [28] proposed by Xie et al. used a\nnew position-free coded and hierarchical Transformer encoder\nwith a lightweight ALL-MLP decoder design to achieve better\nresults. Swin Transformer [29] is another collision of Trans-\nformer in the field of vision, whose main idea is to divide the\nfeature map into multiple disjoint regions and to perform self-\nattentive computation only within this window to reduce the\ncomputation, especially when the shallow feature map is large.\nStill, at the same time, it also reduces the information transfer\nbetween spatial locations. MPViT [38] proposed a multi-\nscale block coding and multi-path structure, where blocks\nof different sizes are coded simultaneously by overlapping\nconvolution operations to produce features with the same\nsequence length, and then the resulting features are fed into\nthe Transformer structure in parallel to produce better results.\nSOT-Net [39] utilizes ultra-high resolution remote sensing and\nLiDAR data for structured analysis, which strengthens the\nsemantic association of multi-source information, and achieves\nan adaptive fusion of multi-modal data through the cross-\nattention mechanism, which can achieve higher classification\naccuracy.\nIn the field of computer vision such as semantic seg-\nmentation of remote sensing images, CNNs show excellent\nperformance, mainly due to convolution operations. The ability\nto collect local features of different layers for better feature\nrepresentation. However, the use of global information needs\nto be improved. At the same time, Transformer is being ap-\nplied to computer vision, enabling self-attention mechanisms\nand multilayer sensing machine (MLP) structures to reflect\ncomplex spatial transformations and long-distance feature de-\npendencies. Hence some work [40]–[43] proposed combining\nthe two for feature extraction to fuse global and local in-\nformation interactively. UNetFormer [40] uses only the UNet\narchitecture and simply splices the CNN with the Transformer-\nbased decoder, aiming to improve the global feature extraction\ncapability of neural networks on raw images for efficient\nsemantic segmentation of remote sensing urban scene images.\nValanarasu et al. [44] solves the problem of lack of long-range\ndependencies in the model due to the inherent inductive bias\nof the convolutional architecture by using the Transformer as\na baseline and improving the self-attention mechanism into a\ngated axial-attention model. TransUnet [41] is a combination\nof Transformer and UNet, which uses Transformer to process\nthe CNN feature map into a sequence, captures the global\ninformation with the help of a self-attention operation, up-\nsamples this information, and then fuses it with the high-\nresolution feature map, which effectively improves the seg-\nmentation task and achieves accurate localization. Inspired by\nthe UNet architecture, Swin-UNet [30] constructs a U-shaped\nstructure based purely on the Swin transformer, which replaces\nthe traditional convolutional feature extraction encoder with\nSwin transformer to extract features. ST-UNet [42] not only\nembeds the Swin transformer into the classical CNN-based\nUNet but also proposes three new strategies to enhance the\nfeature representation of the occluded objects and reduce\nthe loss of detailed information.Conformer [43] is a hybrid\nnetwork structure that relies on feature coupling units (FCUs)\nto enhance the learning of feature representations by com-\nbining convolutional operations and self-attentive mechanisms\ninteractively and using a parallel structure to fuse local and\nglobal feature representations at different resolutions.\nC. Semantic segmentation of remote sensing images based on\ndecoupling\nIn the era of big data, deep learning, known for its effi-\ncient autonomous implicit feature extraction capability, has\ntriggered a boom in a new generation of artificial intelli-\ngence, yet the unexplainable black box behind it has become\na key bottleneck problem limiting its further development.\nTherefore, the idea of decoupling is crucial, and decoupled\nrepresentation learning decouples multi-level and multi-scale\ndata information from different perspectives, prompting deep\nlearning models to perceive data autonomously like humans,\nand gradually becoming an important new research method.\nWe classify the decoupling strategies in the semantic segmen-\ntation domain into the following three main types: decoupling\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nusing the coarse-to-fine paradigm, decoupling using intra-class\nand extra-class relations, and decoupling using edge supervi-\nsion. To decouple the paradigm from coarse-to-fine, ACFNet\n[45] uses attention to first perform coarse segmentation results\non the original image and then uses the coarse segmentation\nresults to calculate the class center of each class to correct\nthe misclassified classes. CDGCNet [46] uses coarse seg-\nmentation predictions as class masks to extract node features\nand performs dynamic graph convolution to learn inter-class\nfeature aggregation, which can effectively exploit long-term\ncontextual dependencies and aggregate usage information to\nbetter predict pixel labels. CCANet [47] proposed a new class\nconstraint following a coarse-to-fine paradigm of attention\ndepth network that enables the formation of class information\nconstraints with explicit remote contextual information. For\ndecoupling using intra- and extra-class relationships, CGFDN\n[48] encodes co-occurrence relationships between different\nclass objects in a scene as convolutional features and in-\nfers segmentation results based on the decoupled features.\nGlove [49] can consider co-occurrence relationships between\ndifferent words in the encoding process. [50] proposed a\ncontinuous learning scheme shaping the latent space to reduce\nforgetting while improving the recognition of new classes.\nFor decoupling using edge supervision, [51] proposed a new\nsemantic segmentation paradigm by explicitly sampling pixels\nfrom different parts (body or edge) and further optimizing the\nbody features and remaining edge features of the target object\nobtained under decoupled supervision. BGCNet [20] uses the\nBGC module to guide the construction of graphs using node\nfeatures and boundary predictions After convolving the graph,\nthe inferred features and the input features are fused to obtain\nthe segmentation results. Nie et al. proposes scale-relation\njoint decoupling network (SRJDN) [52] by simultaneously\nconsidering decoupling scales and decoupling relationships to\nexcavate more complete relationships of multiscale RS objects.\nInspired by these excellent works, we adopt a semantic\nclustering module with scale-separation to provide semantic\nfeatures to guide the semantic decoupling of the Swin-UNet,\nwhich ensures the interaction of global contextual information\nwhile fully exploiting the coupled target object features. To\nthe best of our knowledge, the proposed SSDT is the first\nto extract the semantic information within the image to guide\nthe Swin-UNet network applied to the RS image segmentation\ntask, which makes up for the shortcomings of the traditional\nSwin-UNet and improves the segmentation accuracy.\nIII. PROPOSED METHOD\nIn this section, we first introduce the general structure of\nthe proposed SSDT and describe the motivation and archi-\ntecture involved. Next, five important modules in SSDT are\nintroduced, namely, SPE, SFE, SD, SDT, and MFFD. Finally,\nwe explain the loss function used for network training.\nA. Overview of the Proposed SSDT\nTo address the challenge of spatial scale coupling of seman-\ntic information features presented in Chapter 1, we propose\nSSDT, a network that uses scale-separated semantic informa-\ntion for decoupling as a way to guide the Swin Transformer\nin extracting features. The network consists of five modules,\nScale-separated Patch Extraction (SPE), Semantic Decoupled\nTransformer (SDT), Scale-separated Feature Extraction (SFE),\nSemantic Decoupling(SD), and Multi-view Feature Fusion\nDecoder (MFFD). In particular, SPE transforms the original\nimage into a linear mapping embedding sequence of three\nscales; SD divides pixels into different semantic clusters based\non the scale information extracted by SFE through clustering\nmethods to further obtain scale information-rich semantic fea-\ntures; SDT extracts intra-class feature interdependencies and\nreduces the association of inter-class features by calculating\nthe similarity between semantic features and image features,\nthe core of which is Decouped Attention. Finally, Multi-view\nFeature Fusion Decoder (MFFD) is proposed to fuse salient\nfeatures from different perspectives to further enhance the\nfeature representation.\nSpecifically, the framework SSDT is shown in Fig.2. For\nthe input original image X, it is divided into two branches,\nwhich are respectively used to construct global context in-\nformation and extract semantic features of scale-separation.\nIn the upper branch, the original image X is sliced at dif-\nferent scale sizes by SPE to generate three scales of linear\nembedding sequences Ei for global contextual representation\nmodeling, where i = {small, medium, big}.Subsequently, Ei\nwill undergo two stages of processing to obtain the attention\nfeature FZ: stage I is to input the linear mapping embedding\nsequence Ei of three scales generated by SPE to the traditional\nTransformer Block can get the feature Fi, and the feature\nFi of three scales, big, medium and small, are merged to\nget multi-patch feature F1, which is used to extract the deep\nrepresentative information of the image. The core of stage 2\nis Decouped Attention, the details of which are the similarity\ncalculation between the linear mapping embedding sequence\nEi and output of SD module semantics Si\nc to obtain scale\nsemantic information-rich attention features Zi\nc, after merged\ninto attention features FZ for saliency extraction of semantic\ninformation on different scale features. Secondly, the lower\nbranch goes through DCNN and then enters SFE to get\nmulti-scale feature FX, which can be modeled sub-scale in\nthe feature space of scale entanglement and solve the scale\nentanglement problem effectively. After that, the SD module\nis based on FX, the pixel is divided into different semantic\nclusters by clustering method to generate semantics Si\nc (the cth\nsemantic feature on the ith scale), and then concat according to\ndifferent semantics to get multi-semantic feature FS. Finally,\nMFFD was used to integrate significant features from different\nperspectives to further improve the representation ability of\nfeatures, including the output attention feature FZ of the SDT\nmodule, the output multi-scale feature FX of the SFE module,\nand the output multi-semantic feature FS of the SD module,\nto obtain the final output feature F = [FZ, FX, FS].\nB. Scale-separated Patch Extraction (SPE)\nSince context information is the most critical factor to\nimprove semantic segmentation performance, this paper pro-\nposed that the SPE module cut patches of different sizes to\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nFig. 2. The architecture of the proposed SSDT. The framework is divided into two branches, which are used to construct global contextual information and\nextract scale-separated semantic features, respectively. First, the upper branch slices the original image X into patches of different scales by SPE to generate\na linear mapping of three scales. After that, the semantic features obtained from SD are imported into SDT for guided modeling to obtain attention features.\nSecond, the lower branch goes into SFE after DCNN to get the scale feature, and the pixels are divided into different semantic clusters by the K-means\nclustering method based on this scaling feature in the SD module to get the semantic features rich in scale information. Finally, MFFD is utilized to integrate\nthe meaning features from different perspectives, including the attention features output from the SDT module, the multi-scale features output from the SFE\nmodule, and the multi-semantic features output from the SD module, to obtain the final output features.\nmatch different sensitivity fields of the SFE module. SPE\ndivides the input image X ∈ RH×W×C into three scales\npi can obtain N = HW/(pi)2 image blocks, where i =\n{small, medium, big}, pi is the length and width of small\nimage blocks, and they are mapped into a linear projection\nsequence, represented as follows:\nEi = [ei\n1 + pi\n1, ei\n2 + pi\n2, ..., ei\nN + pi\nN ], (1)\nWhere ei\n1 is image embedding and pi\n1 is image position\nembedding. Finally, we take the output of SPE: the sequence\nof embedding at three scales(Big, Medium, and Small), as the\ninput of the SDT for the next operation.\nC. Semantic Decoupled Transformer(SDT)\nSDT is proposed to solve the problem that the scale of\nsemantic information in remote sensing images is seriously en-\ntangled. Based on the deep network characterization of DCNN,\nmultiple scale features can be obtained through convolutional\nlayers with different void convolution rates. Traditional feature\nextraction methods can directly extract semantic information\nin remote sensing images, but their methods cannot solve\nthe problem of severe coupling of various semantic scales.\nCompared with traditional methods, our method uses scale\nmodeling and separate semantic decoupling for each scale\ninformation, which can obtain multi-perspective and multi-\nscale semantic information and further assist the Swin-UNet\nencoder to carry out accurate feature characterization.\nThe module includes two stages, the Traditional Trans-\nformer Block, and Decouped Attention. Stage 1:The sequence\nEi ∈ RN×(pi)2C obtained from SPE can be input into the\nTraditional Transformer Block to obtain output features Fi,\nwhere i = {small, medium, big}. And the feature Fi of three\nscales, big, medium and small, are merged to get multi-patch\nfeature F1, as shown below:\nF1 = [FSmall, FMedium , FBig], (2)\nThe essential module in the Traditional Transformer Block is\nthe MSA module, which consists of multiple self attention\nmechanisms, whose inputs include three vectors Qi, Ki, Vi ∈\nRN×(pi)2C, as shown below:\nQi = EiWQ, Ki = EiWK, Vi = EiWV , (3)\nWhere WQ, WKWV is a learnable parameter and Ei is a\nscale-separated linear mapping embedded sequence. The self-\nattention mechanism consists of a calculation between three\nvectors, calculated as follows:\nMSA (Qi, Ki, Vi) =softmax(Qi(KT )i\n√\nd\n)V i, (4)\nWhere d is the dimension of vector K. The output feature\nAi\nl−1 of the multiplex attention mechanism at layer l−1 is sent\ninto the multilayer perceptron (MLP), and the layer norm (LN)\nis applied before each block. After the residual connection, the\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nFig. 3. The architecture of the proposed Decouped Attention. This module\nis different from the traditional MSA module, the vectors ˆQi, ˆKi, ˆV i of\nDecouped Attention add the semantic information of scale-separation ex-\ntracted from the original image. Based on the multi-stream original image\nsequence ˆQi and the scale-information-rich semantic sequence V i\nc , dot\nproduct calculation is carried out to obtain the scale-semantic information-\nrich attention feature Zi\nc. Then, the three scales Zi\nc of each semantic are\ncombined into c attention features FZc .\noutput feature of the coding region can be obtained by loop l\ntimes. The calculation formula is as follows:\nAi\nl−1 = MSA (LN(Fi\nl−1)) +Fi\nl−1, (5)\nFi\nl = MSA (LN(Ai\nl−1)) +Ai\nl−1. (6)\nThe network architecture of Decouped Attention is shown in\nFig.3. It is different from the Traditional Transformer module.\nFor the Traditional Transformer module, the three vectors\nQi, Ki, Vi of the attention mechanism are composed of multi-\nstream linear mapping embedded sequences, while the vectors\nˆQi, ˆKi, ˆV i ∈ RN×(pi)2C of Decouped Attention add the\nsemantic information of scale-separation extracted from the\noriginal image, the calculation process is as follows:\nˆQi = Ei ˆWQ, ˆKi = Si\nc ˆWK, ˆV i = softmax(\nˆQi( ˆKi)T\n√\nd\n), (7)\nWhere ˆWQ, ˆWK is a learnable parameter and Ei is a scale-\nseparated linear mapping embedded sequence, and Si\nc ∈\nRN×(pi)2C is the c semantics with rich scale information out-\nput by the SD module after reshaping to the linear embedding\nsequence with the same width as Ei. ˆV i ∈ RN×c is a linear\nembedding sequence of c semantic information obtained from\nthe calculation of similarity between ˆQi and ˆKi. Different\nfrom the Traditional Transformer module, we conduct Split\nand Expend on vector ˆV i, that is, divide N × c into N × 1\nof c one-dimensional semantic information and then expand\nit to N × (pi)2C. The extended linear embedding sequence\nV i\nc ∈ RN×(pi)2C of each semantic can be obtained, and the\ncalculation process is as follows:\nV i\nc = Expend(Split( ˆV i)), (8)\nBased on the scale-separated original image sequence ˆQi and\nthe scale-information-rich semantic sequence V i\nc , dot product\ncalculation is carried out, as shown in Formula (14), to obtain\nthe scale-semantic information-rich attention feature Zi\nc. Then,\nthe three scales Zi\nc of each semantic are combined into c\nattention features FZc , and the calculation process is as\nfollows:\nZi\nc = ˆQi ⊙ V i\nc , (9)\n\n\n\nFZ1\n= [ZSmall\n1 , ZMedium\n1 , ZBig\n1 ]\n...\nFZc\n= [ZSmall\nc , ZMedium\nc , ZBig\nc ],\n(10)\nFZ = [FZ1\n, ..., FZc\n]. (11)\nD. Scale-separated Feature Extraction (SFE)\nThe SFE module is proposed for scale adaptation with SPE,\nclassifying the scale sentences as Big, Medium, and Small, and\nthe module is to obtain multiple-scale features X ∈ RH×W×C\nby passing the original image through the convolution layer of\ndifferent void convolution rates after passing through DCNN.\nThe multi-scale feature FX can be obtained by combining the\nthree scale features Xi.\nXi = Atrous(X), (12)\nFX = [XSmall, XMedium , XBig], (13)\nWhere AtrousConvRate = {1, 6, 12}.\nE. Semantic Decoupling (SD)\nThe SD module is proposed to improve the compactness of\nintra-class features and expand the dispersion of inter-class\nfeatures. The feature information based on scale-separation\ndivides pixels into different semantic clusters by clustering\nmethod. In the process of clustering, for intra-class features,\npixels in the same class are close to the clustering center\nand the intra-class distance is shortened, thus realizing robust\nintra-class modeling. For inter-class features, since there are\ndifferent clustering centers between classes, pixels repel each\nother, and the distance between classes is elongated. Therefore,\nsemantic features can be fully utilized to excavate the differ-\nences between classes. Compared with the traditional feature\nextraction method using a convolutional neural network, the\nclustering method is used to extract semantic information.\nOur method can realize parameterless training and generate\nfeature representations of each category in the context of less\ncomputation, thus realizing semantic decoupling better. SD\nmodule is based on scale features Xi ∈ RH×W×C. The\noriginal image at each scale is divided into different semantic\nclusters by the clustering method. The c semantic Si\nc on the ith\nscale is output, and then the three scales Si\nc of each semantic\nare combined into c semantic features FS\nc .\nSi\nc = Cluter(Xi), (14)\n\n\n\nFS1\n= [SSmall\n1 , SMedium\n1 , SBig\n1 ]\n...\nFSc\n= [SSmall\nc , SMedium\nc , SBig\nc ]\n(15)\nFS = [FS1\n, ..., FSc\n] (16)\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nF . Multi-view Feature Fusion Decoder(MFFD)\nThe Multi-view Feature Fusion Decoding module integrates\nthe saliency features of different views, including the output\nattention feature FZ of the SDT module, the output multi-\nscale feature FX of the SFE module, and the output multi-\nsemantic feature FS of the SD module, to obtain the final\noutput F = [FZ, FX, FS].\nThe SSDT architecture proposed in this paper extracts the\nscale semantic information-rich attention features by calculat-\ning the similarity between the semantic features rich in scale\ninformation and the original image features, representing the\npixel-level features with the similarity features, and decoupling\nthe coupled semantic information one by one. It can be seen\nthat, compared with the traditional transformer model, the\nfeature representation of the SSDT guided by the semantic\ninformation of similarity is no longer a single image pixel-level\ninformation, but establishes the correlation between semantic\nfeatures and image features. The model is more robust and\nimproves the accuracy of the model’s description of coupling\ntarget objects.\nG. The Loss Function\nIn this paper, the standard multi-classification cross-entropy\nloss function is adopted, which is expressed as:\nLCE = 1\nn\nnX\ni=1\n(−ˆYilog(Yi) − (1 − ˆYi)log(1 − Yi), (17)\nWhere n refers to all pixels of the remote sensing image, Yi\nis the prediction result generated by the model, and −ˆYi is the\nmulti-classification label.\nIV. EXPERIMENTAL RESULTS AND ANALYSIS\nA. Datasets\nThe effectiveness of the SSDT is tested using the Inter-\nnational Society for Photogrammetry and Remote Sensing\n(ISPRS) Potsdam dataset and the ISPRS Vaihingen dataset\n[54].\nPotsdam: The Potsdam dataset contains 38 true orthophotos\n(TOP) images and their corresponding DSMs, which are\nobtained from a historic city. The spatial sizes of the data files\nare 6000 × 6000 pixels, and the ground sampling distance\n(GSD) is 5 cm. There are four spectral bands in each TOP\nimage, including near-infrared, red, green, and blue bands, and\none band in each DSM. Note that we use only the red, green,\nand near-infrared channels in our experiments. We utilize 24\nimages for training and the remaining 14 images for testing.\nThe Potsdam dataset is labeled according to seven semantic\ntypes, which include Impervious surfaces (white), Buildings\n(blue), Low vegetation (cyan), Trees (green), Cars (yellow),\nClutter (red), and Undefined (back).\nVaihingen: The Vaihingen dataset contains 33 TOP images\nand their corresponding DSMs, which are obtained from a\nsmall village. The spatial sizes of the data files are 2494 ×\n2064 pixels, and the GSD is 5 cm. Different from the Potsdam\ndataset, there are three spectral bands in each TOP image,\nincluding near-infrared, red, and green bands, and one band\nin each DSM. We utilize TOP tiles in our experiments without\nthe DSMs. We utilize 16 images for training and the remaining\n17 images for testing. The Vaihingen dataset is split into the\nsame seven categories as those of the Potsdam dataset.\nLoveDA: The LoveDA (Land-cOVE dataset for Domain\nAdaptation) [55] dataset was created by the State Key Lab-\noratory of Information Engineering in Surveying, Mapping,\nand Remote Sensing at Wuhan University. The purpose of\nthis dataset is to promote semantic segmentation and transfer\nlearning tasks. It contains 5987 high-resolution images with\n0.3m resolution and 166,768 annotated semantic objects from\nthree different cities: Nanjing, Changzhou, and Wuhan. The\nLoveDA dataset involves two different domains, namely urban\nand rural, which results in complexity and diversity, such\nas multi-scale objects, complicated background samples, and\ninconsistent class distributions.\nB. Evaluation Metrics\nThe performance of the SSDT is evaluated by using pixel\naccuracy (PA), mean pixel accuracy (MPA), and mean in-\ntersection over union (mIoU). In addition, we use the F1\nscore (F1) and the frequency-weighted intersection over union\n(FWIoU) to further evaluate the network performance, where\nF1 is a comprehensive indicator considering both Precision\nand Recall, and FWIoU is an improvement of mIoU, taking\ninto account the frequency of each class. Among all evaluation\nmetrics, mIoU is the most commonly used metric due to its\nsimplicity and strong representation.\nC. Implementation Details\nIn this section, we focus on the implementation details of\nthe proposed method. To train our network, we crop the image\ninto 1000 random patches of 256 × 256 space size. Random\nflip or mirror for data expansion to better train the network.\nIn addition, we use argumentation library [56] for enhanced\ndata, and all the training images belonging to one become [0.0,\n1.0]. The K-means algorithm is unsupervised learning, with\nK=7 per data set. In addition, p parameters corresponding to\nsmall and big in MIPS are 4/8/16 respectively. All models were\nimplemented using the PyTorch and Adam optimizer with a\nlearning rate of 1e-5. We set the batch size to 10 and train the\nmodel with about 300 epochs. All experiments were conducted\non an NVIDIA 2080Ti GPU server.\nD. Baselines\nFCN [6]: This network can save spatial information, can\nrealize point-to-point learning and end-to-end training, and\ngreatly reduce the time cost compared with CNNs.\nUNet [7]: This network is an encoder-decoder structure\nthat consists of a contracting path and an expanding path.\nThe contracting path can extract abstract features while the\nexpanding path restores the location information.\nSegNet [8]: The network consists of an encoder network,\na corresponding decoder network, and a pixel-level classifi-\ncation layer. The decoder uses the pooled index calculated\nin the maximum pooling step of the corresponding encoder\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nTABLE I\nCOMPARATIVE EXPERIMENT RESULTS ON POTSDAM DATASET.\nMethod Impervious\nSurface Building Low Vegetation Tree Car PA(%) MPA (%) mIoU (%) FWIoU (%) F1\nscore\nFCN [6] 78.09 85.80 70.63 66.58 75.07 84.26 73.04 75.23 73.01 70.80\nUNet [7] 75.33 80.67 70.92 66.70 75.88 83.18 75.05 73.90 71.45 70.15\nSegNet [8] 78.61 84.73 67.79 63.72 78.13 84.08 73.23 74.60 73.34 70.88\nPSPNet [10] 78.00 85.80 70.58 57.72 76.02 83.10 77.28 73.62 74.11 65.47\nDeeplabV3+ [14] 76.97 90.99 76.54 72.41 80.18 87.20 74.48 80.62 78.75 72.37\nLoG-CAN [31] 83.59 92.35 75.97 71.31 80.18 88.01 78.53 81.49 78.97 76.54\nMSCG-Net [24] 76.66 84.93 69.63 65.65 77.98 86.25 83.93 73.18 74.97 72.47\nDANet [18] 76.54 81.34 69.62 62.95 77.20 82.90 72.70 73.53 71.87 69.38\nLANet [16] 81.66 87.99 74.97 67.64 81.66 86.56 75.58 78.78 75.94 74.52\nCCANet [53] 81.97 86.04 74.34 70.19 70.68 85.10 68.73 76.64 75.75 67.78\nViT [25] 71.19 77.18 72.50 51.98 56.79 79.53 63.67 65.93 69.38 60.67\nUNetformer [40] 83.33 90.66 77.05 70.54 82.53 87.54 76.12 80.82 77.17 75.97\nSwin Transformer [29] 67.61 74.88 58.78 45.08 56.38 75.80 60.74 60.54 62.68 58.57\nSwin-UNet [30] 84.79 89.45 78.01 73.77 83.42 88.35 78.90 81.89 79.30 76.02\nSSDT(Ours) 84.88 91.90 78.79 75.20 82.97 88.45 77.49 82.75 79.55 74.78\nTABLE II\nCOMPARATIVE EXPERIMENT RESULTS ON VAIHINGEN DATASET.\nMethod Impervious\nSurface Building Low Vegetation Tree Car PA(%) MPA (%) mIoU (%) FWIoU (%) F1\nscore\nFCN [6] 79.53 83.31 66.86 69.60 62.22 85.59 77.98 72.31 75.26 70.27\nUNet [7] 81.41 84.69 68.94 68.37 61.11 86.76 72.31 72.90 77.16 84.03\nSegNet [8] 80.68 85.11 69.78 64.75 61.70 86.08 71.30 72.40 76.10 83.68\nPSPNet [10] 79.75 84.40 62.26 6409 4976 83.82 80.44 68.11 72.98 80.36\nDeeplabV3+ [14] 85.32 84.13 72.05 69.25 68.19 88.42 83.88 76.65 79.76 75.46\nLoG-CAN [31] 84.71 87.48 71.79 72.60 69.37 88.61 73.78 77.19 80.02 86.93\nMSCG-Net [24] 81.04 85.70 67.02 68.25 63.94 86.25 72.28 73.19 76.34 70.76\nDANet [18] 81.72 82.10 64.30 63.70 50.15 84.56 69.30 68.40 74.07 80.60\nLANet [16] 84.97 86.20 68.48 67.41 65.68 87.20 72.03 74.55 77.85 85.12\nCCANet [53] 81.12 88.49 70.36 70.55 51.86 87.22 71.72 72.48 78.06 83.42\nViT [25] 69.61 76.46 60.47 64.93 37.49 80.53 76.39 61.79 67.87 75.47\nUNetformer [40] 85.00 88.10 71.33 67.44 70.04 88.00 73.49 76.38 79.18 86.35\nSwin Transformer [29] 76.61 75.09 64.36 65.66 46.46 82.49 67.01 65.64 70.57 78.71\nSwin-UNet [30] 84.24 88.03 69.09 70.83 68.73 87.71 72.87 76.18 78.67 86.24\nSSDT(Ours) 86.66 87.70 74.54 72.18 70.43 89.73 89.90 78.30 81.93 75.13\nTABLE III\nCOMPARATIVE EXPERIMENT RESULTS ON LOVE DA DATASET.\nMethod PA\n(%)\nMPA\n(%)\nmIoU\n(%)\nFWIoU\n(%)\nF1\nscore\nFCN [6] 67.75 73.62 57.04 41.97 56.49\nUNet [7] 62.32 67.40 47.61 39.12 50.59\nSegNet [8] 61.36 64.55 48.80 42.07 47.20\nPSPNet [10] 57.27 43.93 46.62 32.40 44.32\nDeeplabV3+ [14] 66.59 63.67 55.98 43.47 59.92\nLoG-CAN [31] 63.03 74.98 55.77 38.68 58.92\nMSCG-Net [24] 60.28 52.03 48.38 37.17 49.74\nDANet [18] 51.90 51.66 39.34 25.59 47.95\nLANet [16] 62.11 74.09 51.05 44.18 51.38\nCCANet [53] 63.63 67.07 55.63 31.40 59.38\nViT [25] 60.68 46.97 46.92 36.94 52.99\nSwin Transformer [29] 51.41 55.48 42.74 20.93 48.08\nSSDT(Ours) 72.84 76.72 56.72 46.42 67.99\nto perform nonlinear upsampling, which reduces the number\nof parameters and computation and eliminates the need for\nlearning upsampling compared with deconvolution.\nPSPNet [10]: The pyramid scene parsing network is applied\nto capture different subregion representations, followed by\nupsampling and concatenation layers to form the final pre-\ndictions.\nDeepLabV3+ [14]: To introduce multi-scale information in\nDeeplabv3+, the main body of the Encoder is DCNN with\nhole convolution. Then there is the Atrous Spatial Pyramid\nPooling (ASPP) module with atrous convolution. Compared\nwith DeepLabv3, v3+ introduces the Decoder module, which\nfurther integrates the low-level features with the high-level\nfeatures.\nLoG-CAN [31] is a local-global class-aware network for\nsemantic segmentation of remote sensing images, which com-\nbines local and global contextual information to improve the\nsegmentation performance of remote sensing images.\nMSCG-Net [24]: This network is proposed based on GCN,\nwhich uses multiple views to explicitly utilize rotation invari-\nance in airborne images, fuses global context information of\nmultiple views, and verifies the influence of multiple angles\non remote sensing image segmentation.\nDANet [18]: This network captures feature dependencies\nin spatial dimension and channel dimension based on the self-\nattention mechanism, and adds two kinds of attention modules\nto dilated FCN to model semantic dependencies in spatial\ndimension and channel dimension respectively.\nLANet [16]: This network obtains the degree of inter-\nlocation dependency by calculating the pixel-to-pixel sim-\nilarity and incorporating the neighboring pixel information\ninto the computed pixels, thus freeing the fusion of global\ninformation from the limitation of image distance.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nTABLE IV\nPERFORMANCE OF DIFFERENT MODULE ON VAIHINGEN DATASETS .\nMethod Impervious\nSurface Building Low\nVegetation Tree Car PA\n(%)\nMPA\n(%)\nmIoU\n(%)\nFWIoU\n(%)\nF1\nscore\nSwin Transformer 84.24 88.03 69.09 70.83 68.73 87.72 72.87 76.18 78.67 86.24\nSFE+SPE 86.37 89.35 72.90 68.67 70.62 88.88 84.05 77.58 80.48 82.27\nSD 86.22 90.48 73.67 68.98 69.14 89.33 73.80 77.70 81.25 87.17\nSD+SDT 84.08 87.76 71.76 72.36 69.88 88.60 89.55 77.17 79.94 75.42\nSFE+SPE+SD 84.46 87.65 70.97 72.57 70.50 88.68 80.30 77.23 80.19 72.47\nSFE+SPE+SD+SDT(our SSDT) 86.66 87.70 74.54 72.18 70.43 89.73 89.90 78.30 81.93 75.13\nCCANet [53] : This network proposes a new attention\ndepth network that follows a coarse-to-fine paradigm for class\nconstraints, which enables the formation of class information\nconstraints to obtain clear remote contextual information.\nViT [25] : ViT is a pioneering work based on Transformer\nfor computer vision tasks, completely changing the field of\ncomputer vision. Although ViT is an excellent substitute for\nCNN, it lacks the inherent inductive bias of CNNs, such as\ntranslation, which makes its generalization ability poor when\ntraining on insufficient data.\nUNetFormer [40] uses only the UNet architecture and\nsimply splices the CNN with the Transformer-based decoder,\naiming to improve the global feature extraction capability\nof neural networks on raw images for efficient semantic\nsegmentation of remote sensing urban scene images.\nSwin Transformer [29]: The main idea of Swin Trans-\nformer is to divide the graph into disjoint regions and only\nperform self-attention calculations in this window to reduce\ncomputational complexity. Especially when the shallow feature\nmap is large, it reduces computational complexity and isolates\ninformation transmission between different windows.\nSwin-UNet [30] : This network was inspired by the UNet\narchitecture and replaced the feature extraction encoder with a\nSwin Transformer to extract features, constructing a pure Swin\nTransformer based U-shaped encoding and decoding structure.\nE. Comparison with State-of-the-Art Methods\nIn this section, we compare our method with nine baselines\non the ISPRS Potsdam and Vaihingen datasets. The experi-\nmental results are listed in Table I and II, where the first five\ncolumns of data are the results of mIoU in each category, and\nthe last five columns are the experimental results of common\nindicators. We can conduct the following analysis.\nFirst, the first three rows are FCN-based methods such\nas UNet and SegNet, which perform the worst due to the\nextraction of features without considering low-level seman-\ntic features, ignoring shallow detail information and spatial\ninformation, as well as the fusion method is too simple and\ncrude. As shown in Table I, the mIOU of UNet is only 73.90%\non the Potsdam dataset, and our proposed SSDT is 8.85%\nhigher than it. Secondly, DeepLabv3+ performs better than the\nFCN-based approach because it utilizes Atrous convolution to\nachieve multi-scale feature mining. However, the multi-scale\nfeatures extracted by Atrous convolution or pyramidal pooling\nare limited, so graph convolution-based methods such as\nMSCG-Net are proposed, which are based on GCN to interact\npixels with each other by constructing nodes and edges to\nestablish connections and fuse contextual information to obtain\nbetter performance. In addition, attention-based mechanisms\nsuch as DANet, LANet, and CCANet are proposed, where\nCCANet proposes a new class constraint following the coarse-\nto-refine paradigm of attention-depth networks for semantic\ndecoupling. The above methods have achieved specific ef-\nfects, but generally not as good as Swin Transformer, Swin\nTransformer has the best performance among all baselines.\nTherefore, we proposed a Scale-separation Semantic Decou-\npled Transformer(SSDT) based on this method. PA, mIoU,\nand FWIoU of SSDT on the Potsdam dataset are 0.10%,\n0.86%, and 0.25% higher than the second-best model Swin\nTransformer, respectively. On the Vaihingen dataset, PA, MPA,\nmIoU, and FWIoU increased by 2.01%, 17.03%, 2.21%, and\n3.62%, respectively. It proves the Scale-separation Semantic\nDecoupled Transformer is fully effective. We also noticed that\nSSDT produced a lower mean F1 scores, which we analyzed\ndue to the imbalanced category in the test data. Therefore,\nwhen small objects like cars are accurately segmented, the\nSSDT model has higher mIoU values, while larger objects\nlike bare ground with accurate segmentation have higher F1\nscores. These results also indicate that the proposed SSDT\nis superior in processing small categories of remote sensing\nobjects.\nIn addition, we also verify the proposed method on a large-\nscale dataset LoveDA, and the relevant results are shown in\nTable III. Compared to the best segmentation model LoG-\nCAN, the proposed SSDT achieved a 2.32% improvement in\nthe MPA evaluation metric. It can also be noted that con-\nsidering the Transformer-based method, our network exceeds\nViT by 20.89% with a significant promotion. Thus, the above\ndata sufficiently demonstrate the effectiveness of the scale-\nseparation semantic decoupled Transformer mechanism.\nF . Performance of SFE, SPE, SD, and SDT\nIn this section, we conduct a set of experiments to verify\nthe effectiveness of the proposed modules SFE, SPE, SD, and\nSDT, as shown in Table IV. It is worth noting that the first row\nof data in the table represents the initial semantic segmentation\nnetwork without any module added, i.e., the baseline Swin\nTransformer. Based on Table IV, several sets of observations\ncan be obtained.\nBy comparing the first three rows of Table IV, it can be\nobserved that both the scale separation modules SFE+SPE and\nthe semantic decoupling module SD contribute to the enhance-\nment of model performance. The mIoU evaluation metric is\nimproved by 1.84% and 2.00% respectively when compared\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nFig. 4. Examples of semantic segmentation results on the Potsdam dataset.\nto the Swin Transformer method. Besides, as shown in the last\ntwo rows, the SDT module enhances model accuracy by 1.39%\nfor the mIoU evaluation metric, thus proving the effectiveness\nof the SDT module. However, after comparing the results\nof SFE+SPE and SFE+SPE+SD, we surprisingly found that\nintegrating SD resulted in a decrease in semantic segmentation\naccuracy. We found that the reason for the reduced accuracy in\nSFE+SPE+SD is due to considering both scale and semantic\ndecoupling only on the CNN architecture instead of the\nTransformer architecture, which causes cognitive confusion\nin the segmentation, eventually reducing the performance of\nthe model. Besides, we believe that the lower performance of\nSD+SDT compared to SD is because the labels in SD cannot\nmodel different scale semantic information, resulting in poor\nrobustness of the labels and thus failing to supervise SDT\neffectively. In summary, through analyzing the above data,\nit can be concluded that every module is indispensable, and\nas indicated in the last row of the table, the segmentation\nperformance reaches its peak by integrating all modules.\nG. Performance with Different Clustering Methods\nTo verify the effectiveness of the SD module, we conducted\nthe following experiments, as shown in Table V. Here, we\nbriefly explain the various abbreviated clustering methods in\nthe table. The first row of AGNES is hierarchical clustering.\nThe specific steps are as follows: (1) Each object is regarded\nas a class and the minimum distance between the two pairs is\ncalculated. (2) Merge the two classes with the smallest distance\ninto a new class; (3) recalculate the distance between the new\nclass and all classes; (4) Repeat (2) and (3) until all classes are\nfinally merged into one class in which the number of output\ncluster partition K=7. The second row of MinBatch K-Meams\nis a variant of the K-Means algorithm, which uses a small\nbatch of data subsets to reduce the computation time while still\ntrying to optimize the objective function. The specific steps\nare: (1) randomly select some data from the data set to form\na small batch and assign them to the nearest center of mass;\n(2) Update the centroid, where the output number of cluster\npartition K=7. The data is updated on every small sample set\ncompared to the K-means algorithm. For each small batch,\nthe updated centroid is obtained by calculating the average\nvalue, and the data in the small batch is allocated to the\ncentroid. With the increase in the number of iterations, the\nchange of this centroid is gradually reduced until the centroid\nis stable or the specified number of iterations is reached, and\nthe calculation is stopped. As shown in Table V, the K-means\nclustering method we used performed best in both datasets\nfor the mIoU metric. Compared to AGNES and MinBatch K-\nmeans, K-means is 0.59% and 0.82% higher on the Vaihingen\ndataset and 0.17% and 0.37% higher on the Potsdam dataset.\nThis is attributable to the following advantages of the K-\nmeans algorithm: firstly, it can determine the classification\nof some samples based on the categories of fewer known\nclustered samples; secondly, to overcome the inaccuracy of\nclustering a small number of samples, the algorithm itself\nhas an optimization iteration function, which iterates again\non the clusters already obtained to determine the clusters of\nsome samples, optimizing the initial supervised learning of the\nunreasonable classification of samples. Therefore, we used the\nK-means clustering method to effectively extract rich semantic\nclustering information, and the experimental results proved the\neffectiveness of the method.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nFig. 5. Examples of semantic segmentation results on the Vaihingen dataset.\nTABLE V\nPERFORMANCE OF DIFFERENT CLUSTERING METHODS ON VAIHINGEN\nAND POTSDAM DATASETS .\nClustering Method mIoU (%)\nVaihingen Postdam\nAGNES 77.71 82.57\nMinBatch K-means 77.48 82.37\nK-means(Ours) 78.30 82.74\nH. Performance with Different Fusion Methods\nIn this section, We conducted three experiments on the\nPotsdam dataset and the Vaihingen dataset to verify the\neffectiveness of the fusion methods used within the MFFD\nmodule. In this experiment, three different fusion methods,\nElement-wise Addition (Add), Matrix Multiplication (Mul),\nand concatenation (Concat), were used to fuse the output atten-\ntional features FZ of the SDT module, the output multiscale\nfeatures FX of the SFE module and the SD module’s output\nmulti-semantic features FS are fused.\nAs shown in Table VI, the first row is Element-wise\nAddition, which performs direct element-by-element addition\nof features. The second row is Matrix Multiplication, which\nperforms the interaction of information between elements by\nmatrix multiplication. Since the above methods fuse in a too\ncrude way, the performance is not satisfactory in both datasets.\nThe Concat fusion method we used performed best in mIoU\nmetrics on both datasets. Compared to Add and Mul, Concat\nis 1.30% and 1.89% higher on the Vaihingen dataset and\n0.42% and 0.67% higher on the Potsdam dataset, respectively.\nTherefore, we used the Concat method to effectively fuse the\nmulti-view features, and the experimental results proved the\neffectiveness of the method.\nTABLE VI\nPERFORMANCE OF DIFFERENT FUSION METHODS ON VAIHINGEN AND\nPOTSDAM DATASETS .\nFusion Method mIoU (%)\nVaihingen Postdam\nElement-wise Addition 77.00 82.32\nMatrix Multiplication 76.41 82.07\nConcat(Ours) 78.30 82.74\nI. Boxplot Analysis of Scale-separated Results\nTo verify the necessity of scale-separation within seman-\ntic decoupling, we randomly selected 20 images within the\nVahingen dataset for input to the two models, and the resulting\nmIoU data were generated as boxplots. The difference between\nthe two models is whether the scale-separation operation is\nadded or not. As shown in Fig. 4 boxplot, the vertical axis\nis the mIoU metric and the horizontal axis is the individ-\nual semantics within the dataset. (a) is our proposed SSDT\nmethod, which performs the scale-separation operation within\nthe semantic decoupling and avoids the influence of scale\ninformation on the semantic decoupling; (b) is the method\nwithout scale information, which ignores all scale information,\ndoes not perform scale-separation, and only performs semantic\ndecoupling. Boxplot is a statistical graph used as a display\nof information about the dispersion of a set of data, which\ncan reflect the characteristics of the data distribution, and also\nallows comparison of the characteristics of the distribution of\nmultiple sets of data. The boxes plotted for each semantic in\nthe figure include the upper edge, lower edge, median, and\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nFig. 6. Boxplot of scale-separated results.(a) is our proposed SSDT method;\n(b) is the method without scale information.\ntwo quartiles of a set of data; the box is connecting the two\nquartiles; the upper and lower edges are connected to the box,\nand the median is in the middle of the box. The analysis\nfollows.\nFirst, it is obvious from observing the two boxplots that\n(b) has four more small circles, or outliers, than (a). Outliers\nin a batch of data deserve attention, and it is very dangerous\nto ignore the existence of outliers. Including outliers in the\nprocess of calculating and analyzing data without eliminating\nthem can have adverse effects on the results. Second, since\nthe SSDT method incorporates a scale-separation operation,\nthe obtained scale information-rich semantic features are used\nto guide the Transformer model to extract features, resulting\nin improved intra-class compactness and smaller intra-class\nvariance, such as the quartile size of (a), which is a shorter\nbox length. Meanwhile, the median line of each semantics in\n(a) is significantly higher than that in (b), which indicates that\nthe SSDT method is more stable and the overall mIoU index is\nhigher than that of the ”w/o SD+SPE” method. In addition, the\nlength between the upper and lower edges of each semantic\nin (a) is significantly smaller than that of the box in (b),\nindicating that the distribution of the normal values of mIoU\nis more concentrated in the SSDT method with the addition\nof the scale-separation operation. Finally, the experimental\nresults demonstrate the necessity of scale-separation within\nthe semantic decoupling.\nJ. Qualitative Analysis of the Semantic Segmentation Results\nHere, we give the visualizations of the predicted semantic\nsegmentation maps for the Potsdam and Vaihingen datasets. As\nshown in Fig.5 and Fig.6, the first column is input Raw Image,\nthe second column is Ground Truth, the remaining columns\nin the middle are the viewable views of each baseline, and\nthe last column presents the viewable views of SSDT in this\npaper.\nFirst, Fig. 5. is the visualization of the Potsdam dataset: the\nthird four or five columns are for the FCN-based approach.\nFor example, due to the simplistic and crude way of UNet\nfusion, the semantics of ”Building” and ”Tree” in the black\nbox in the fourth column are not recognized as they should be;\nthe SegNet extraction of features ignores the shallow semantic\ndetail information and spatial information, resulting in severe\nblurring in the black box in the fifth column and a large\nnumber of pixel mis-segmented within the image. The sixth\nseven or eight columns are scale-specific approaches, such\nas DeepLabv3+ for multi-scale feature extraction using null\nconvolution, PSPNet using pyramid pooling, and LoG-CAN\nusing global class-aware (GCA) modules and local class-aware\n(LCA) modules. However, the scale information extracted by\nthe above methods is limited, for example, the sixth column\nshould have identified only ’Low Vegetation’, but PSPNet\nclassified both ’Tree’ and ’Low Vegetation’ semantics; the two\nred boxes in the seventh column where ’Clutter’ should have\nappeared have been misclassified as another semantic; in the\neighth column, the semantics of ”Building” in the red box are\nmissing and not fully segmented.Therefore, graph convolution-\nbased methods such as MSCG-Net (ninth column) are pro-\nposed, which is based on GCN to interact pixels with each\nother by constructing nodes and edges to establish connections\nand fuse contextual information to obtain better performance.\nBut it cannot optimally solve the semantic coupling problem,\nas shown in the green box in the figure, there is a messy split in\nwhat should be a clear diagram. In addition, the tenth, eleventh,\nand twelfth columns are the attention mechanism-based meth-\nods DANet, LANet, and CCANet, DANet improves the seg-\nmentation accuracy based on the self-attention mechanism to\nobtain the dependence of features in spatial dimension and\nchannel dimension. LANet calculates the similarity between\npixels to obtain the dependence degree between locations so\nthat the fusion of global information is not limited by image\ndistance. The above method achieves specific results, but does\nnot consider the influence of scale information on semantic\ninformation, and ignores important contextual information,\nsuch as some confusion areas appearing in the yellow box\nin the figure. Moreover , among several ViT-based models,\nSwin-UNet performs the best, in which UNetFormer uses only\nthe UNet architecture and simply splices the CNN with the\nTransformer-based decoder, with unsatisfactory results such as\nmis-segmentation in the purple box; nearly half of the pixels\nin the Swin Transformer’s visualization are misclassified as\n”Impervious Surface”. It is worth noting that our proposed\nSSDT based on Swin-UNet shows extremely high intra-class\ncompactness and inter-class relaxation for each semantic class,\nand also achieves pleasing details in spatial consistency.\nSecond, Fig.6 is a visualization of the Vaihingen dataset:\nObserving the first three rows, it can be seen that some\nSmall Scale target objects in the red box are ignored. This\nphenomenon is not only present in the CNN-based network\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\nFig. 7. Examples of semantic segmentation results on the Vaihingen dataset.\n(UNet) but also in the Transformer-based method. This is be-\ncause the model is modeled for Big Scale target objects with-\nout the scale-separation operation in the semantic decoupling\nprocess, and thus the Small Scale targets are overwhelmed.\nSemantic entanglement occurs in the black boxes in the middle\nthree lines, either other semantics are newly recognized, or\nnew semantic connections appear on the original semantics\nwith unclear boundary segmentation. The purple boxes in\nthe last four rows show some confusing results, such as the\nunclear edge segmentation of the ’Tree’ semantics, which is a\nconsequence of too little similarity within the same semantics\nand too much similarity between different semantics in the\nmodeling process. It is worth noting that the method proposed\nin this paper (SSDT) has the most convincing visualization\nboth in terms of semantic decoupling for each scale size and\nin terms of boundary detail regions.\nSpecifically, as shown in Fig.7, we identified some visual-\nizations with significant semantic interweaving in the Vaihin-\ngen dataset to highlight the advantages of SSDT effectively.\nOn the one hand, when the scale is too small, as shown in\nthe above figure, a ”Tree” that should not appear appears;\nOn the other hand, when the scale is too large, it causes\nsemantic space entanglement, and the edges that should have\nbeen mixed, resulting in a decrease in semantic segmentation\nresults. On the contrary, our proposed method SSDT has\nsignificant advantages over the traditional methods mentioned\nabove. Not only does it avoid semantic entanglement that\nshould not occur, but it also has clear edges and achieves\nbetter segmentation results.\nK. Complexity Analysis\nWe also performed complexity analysis experiments and\nshow the results in Table VII. The “time cost” in the table\nrepresents the time for the model to segment an image.\nAs can be seen from the table, FCN has the lowest time\ncomplexity with 0.08 due to its simple network architecture.\nCompared with our model, the training time cost has slightly\nincreased and we analyse that is because we have built a two-\nlayer architecture of CNN and Transformer and achieved joint\ndecoupling of scale and semantics. In addition, we also provide\nthe parameters of the baselines and our SSDT model with ten\nmillion measurement units. As can be seen from the table,\nMSCG-Net has the smallest parameters since the applied GCN\ninvolves fewer parameters. In addition, because our model\nconsiders scale separation in both CNN and Transformer\nTABLE VII\nCOMPLEXITY RESULTS\nMethod Time Cost(s) Parameters (10 m)\nFCN [6] 0.08 0.19\nUNet [7] 0.12 0.17\nSegNet [8] 0.09 0.29\nPSPNet [10] 0.11 0.28\nDeeplabV3+ [14] 0.12 0.59\nLoG-CAN [31] 0.10 0.31\nMSCG-Net [24] 0.11 0.10\nDANet [18] 0.11 0.50\nLANet [16] 0.09 0.24\nCCANet [53] 0.10 0.59\nViT [25] 0.14 0.89\nUNetformer [40] 0.09 0.12\nSwin Transformer [29] 0.11 0.88\nSwin-UNet [30] 0.08 0.41\nSSDT(Ours) 0.18 1.86\narchitectures, it has relatively large training parameters. In the\nfuture, we will continue to work on reducing the computational\ncost.\nV. C ONCLUSION\nTo solve the serious entanglement of semantic information\nscale in remote sensing images, a new idea is proposed\nin this paper: scale decoupling in the process of semantic\ndecoupling can effectively avoid the impact of scale coupling\non semantic judgment. Meanwhile, the semantic decoupling\nmodule is implemented in the Swin transformer. This will\nnot only help the Swin transformer provide effective semantic\nfeatures but also help make up for the lack of spatial location\nin the Swin transformer. We creatively came up with SSDT,\nwhich consists of five modules, Scale-separated Patch Extrac-\ntion (SPE), Semantic Decoupled Transformer (SDT), Scale-\nseparated Feature Extraction (SFE) Semantic Decoupling(SD),\nand Multi-view Feature Fusion Decoder (MFFD), using scale\ninformation to divide pixels into different semantic clusters\nby clustering method, semantic features rich in scale infor-\nmation can be obtained, and semantic features can be used\nas guidance, and the similarity between image features can be\ncalculated to mining the interdependency of features within the\nclass, and the correlation between features between the classes\ncan be reduced. The problem of serious coupling of semantic\ninformation scales in remote sensing images is solved.\nWe conduct multiple sets of comparison experiments and\nablation experiments on the Potsdam dataset and the Vaihingen\ndataset to verify the effectiveness of the proposed method.\nQualitative and quantitative results demonstrate the effective-\nness of the semantic decoupling Transformer framework with\nscale-separation in remote sensing image segmentation tasks.\nSpecifically, the proposed SSDT outperformed the state-of-the-\nart Swin Transformer by 0.86% and 2.12% on the Potsdam and\nthe Vaihingen datasets for mIoU evaluation, respectively.\nREFERENCES\n[1] M. H. Hesamian, W. Jia, X. He, and P. Kennedy, “Deep learning tech-\nniques for medical image segmentation: Achievements and challenges,”\nJournal of Digital Imaging, vol. 32, no. 8, 2019.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\n[2] N. Keiller, S. G. Fadel, I. C. Dourado, D. Rafael, J. Munoz, O. Penatti,\nR. T. Calumby, L. T. Li, S. Dos, and S. Da, “Exploiting convnet diversity\nfor flooding identification,” IEEE Geoscience Remote Sensing Letters,\n2017.\n[3] M. Siam, S. Elkerdawy, M. Jagersand, and S. Yogamani, “Deep se-\nmantic segmentation for automated driving: Taxonomy, roadmap and\nchallenges,” 2017.\n[4] M. Leena and K. Kirsi, “Segment-based land cover mapping of a\nsuburban area—comparison of high-resolution remotely sensed datasets\nusing classification trees and test field points,” Remote Sensing, vol. 3,\nno. 8, pp. 1777–1804, 2011.\n[5] A. Garcia-Garcia, S. Orts-Escolano, S. Oprea, V . Villena-Martinez, and\nJ. Garcia-Rodriguez, “A review on deep learning techniques applied to\nsemantic segmentation,” 2017.\n[6] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 39, no. 4, pp. 640–651, 2015.\n[7] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in Medical Image Computing\nand Computer-Assisted Intervention–MICCAI 2015: 18th International\nConference, Munich, Germany, October 5-9, 2015, Proceedings, Part III\n18. Springer, 2015, pp. 234–241.\n[8] V . Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep con-\nvolutional encoder-decoder architecture for image segmentation,” IEEE\nTransactions on Pattern Analysis Machine Intelligence, pp. 1–1, 2017.\n[9] M. Zeiler and R. Fergus, “Visualizing and understanding convolutional\nnetworks,” in ECCV 2014, 2014.\n[10] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\nnetwork,” IEEE Computer Society, 2016.\n[11] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n“Semantic image segmentation with deep convolutional nets and fully\nconnected crfs,” arXiv preprint arXiv:1412.7062, 2014.\n[12] ——, “Deeplab: Semantic image segmentation with deep convolutional\nnets, atrous convolution, and fully connected crfs,” IEEE transactions on\npattern analysis and machine intelligence, vol. 40, no. 4, pp. 834–848,\n2017.\n[13] L. C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous\nconvolution for semantic image segmentation,” 2017.\n[14] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmen-\ntation,” in Proceedings of the European conference on computer vision\n(ECCV), 2018, pp. 801–818.\n[15] Q. Hou, D. Zhou, and J. Feng, “Coordinate attention for efficient\nmobile network design,” in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2021, pp. 13 713–13 722.\n[16] L. Ding, H. Tang, and L. Bruzzone, “Lanet: Local attention embedding\nto improve the semantic segmentation of remote sensing images,”\nIEEE Trans. Geosci. Remote. Sens., vol. 59, no. 1, pp. 426–435, 2021.\n[Online]. Available: https://doi.org/10.1109/TGRS.2020.2994150\n[17] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 7132–7141.\n[18] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and H. Lu, “Dual attention\nnetwork for scene segmentation,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2019, pp. 3146–\n3154.\n[19] T. N. Kipf and M. Welling, “Semi-supervised classification with graph\nconvolutional networks,” arXiv preprint arXiv:1609.02907, 2016.\n[20] H. Hu, J. Cui, and H. Zha, “Boundary-aware graph convolution for\nsemantic segmentation,” in 2020 25th International Conference on\nPattern Recognition (ICPR). IEEE, 2021, pp. 1828–1835.\n[21] X. Li, Y . Yang, Q. Zhao, T. Shen, Z. Lin, and H. Liu, “Spatial pyramid\nbased graph reasoning for semantic segmentation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 8950–8959.\n[22] S.-Y . Pan, C.-Y . Lu, S.-P. Lee, and W.-H. Peng, “Weakly-supervised\nimage semantic segmentation using graph convolutional networks,” in\n2021 IEEE International Conference on Multimedia and Expo (ICME).\nIEEE, 2021, pp. 1–6.\n[23] H. Huang, L. Lin, Y . Zhang, Y . Xu, J. Zheng, X. Mao, X. Qian, Z. Peng,\nJ. Zhou, Y .-W. Chen et al., “Graph-bas3net: Boundary-aware semi-\nsupervised segmentation network with bilateral graph convolution,” in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 7386–7395.\n[24] Q. Liu, M. C. Kampffmeyer, R. Jenssen, and A.-B. Salberg, “Multi-\nview self-constructing graph convolutional networks with adaptive class\nweighting loss for semantic segmentation,” in Proceedings of the\nIEEE/CVF Conference on computer vision and pattern recognition\nWorkshops, 2020, pp. 44–45.\n[25] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in 9th International\nConference on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=YicbFdNTTy\n[26] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr et al., “Rethinking semantic segmentation from a\nsequence-to-sequence perspective with transformers,” in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2021, pp. 6881–6890.\n[27] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Trans-\nformer for semantic segmentation,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision, 2021, pp. 7262–7272.\n[28] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n“Segformer: Simple and efficient design for semantic segmentation\nwith transformers,” Advances in Neural Information Processing Systems,\nvol. 34, pp. 12 077–12 090, 2021.\n[29] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in Proceedings of the IEEE/CVF international conference on\ncomputer vision, 2021, pp. 10 012–10 022.\n[30] H. Cao, Y . Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and\nM. Wang, “Swin-unet: Unet-like pure transformer for medical image\nsegmentation,” in Computer Vision - ECCV 2022 Workshops - Tel\nAviv, Israel, October 23-27, 2022, Proceedings, Part III, ser. Lecture\nNotes in Computer Science, L. Karlinsky, T. Michaeli, and K. Nishino,\nEds., vol. 13803. Springer, 2022, pp. 205–218. [Online]. Available:\nhttps://doi.org/10.1007/978-3-031-25066-8 \\\n9\n[31] X. Ma, M. Ma, C. Hu, Z. Song, Z. Zhao, T. Feng, and W. Zhang,\n“Log-can: local-global class-aware network for semantic segmentation\nof remote sensing images,” in ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2023, pp. 1–5.\n[32] R. Hang, P. Yang, F. Zhou, and Q. Liu, “Multiscale progressive seg-\nmentation network for high-resolution remote sensing imagery,” IEEE\nTransactions on Geoscience and Remote Sensing, vol. 60, pp. 1–12,\n2022.\n[33] S. Wang, Z. Wang, H. Li, J. Chang, W. Ouyang, and Q. Tian, “Accu-\nrate fine-grained object recognition with structure-driven relation graph\nnetworks,” International Journal of Computer Vision (IJCV), 2023.\n[34] Y . Zhang, W. Li, W. Sun, R. Tao, and Q. Du, “Single-source domain\nexpansion network for cross-scene hyperspectral image classification,”\nIEEE Transactions on Image Processing, vol. 32, pp. 1498–1512, 2023.\n[35] Z. Li, H. Guo, Y . Chen, C. Liu, Q. Du, and Z. Fang, “Few-shot\nhyperspectral image classification with self-supervised learning,” IEEE\nTransactions on Geoscience and Remote Sensing, 2023.\n[36] J. Yang, J. Sun, Y . Ren, S. Li, S. Ding, and J. Hu, “Gacp: graph neural\nnetworks with arma filters and a parallel cnn for hyperspectral image\nclassification,” International Journal of Digital Earth, vol. 16, no. 1, pp.\n1770–1800, 2023.\n[37] Y . Zhang, M. Zhang, W. Li, S. Wang, and R. Tao, “Language-aware\ndomain generalization network for cross-scene hyperspectral image\nclassification,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 61, pp. 1–12, 2023.\n[38] Y . Lee, J. Kim, J. Willette, and S. J. Hwang, “Mpvit: Multi-path vision\ntransformer for dense prediction,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022, pp.\n7287–7296.\n[39] M. Zhang, W. Li, Y . Zhang, R. Tao, and Q. Du, “Hyperspectral and\nlidar data classification based on structural optimization transmission,”\nIEEE Transactions on Cybernetics, 2022.\n[40] L. Wang, R. Li, C. Zhang, S. Fang, C. Duan, X. Meng, and P. M.\nAtkinson, “Unetformer: A unet-like transformer for efficient semantic\nsegmentation of remote sensing urban scene imagery,” ISPRS Journal\nof Photogrammetry and Remote Sensing, vol. 190, pp. 196–214, 2022.\n[41] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and\nY . Zhou, “Transunet: Transformers make strong encoders for medical\nimage segmentation,” arXiv preprint arXiv:2102.04306, 2021.\n[42] X. He, Y . Zhou, J. Zhao, D. Zhang, R. Yao, and Y . Xue, “Swin\ntransformer embedding unet for remote sensing image semantic segmen-\ntation,” IEEE Transactions on Geoscience and Remote Sensing, vol. 60,\npp. 1–15, 2022.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16\n[43] W. Wang, T. Zhou, F. Yu, J. Dai, E. Konukoglu, and L. Gool, “2021\nieee/cvf international conference on computer vision (iccv),” 2021.\n[44] J. M. J. Valanarasu, P. Oza, I. Hacihaliloglu, and V . M. Patel, “Medical\ntransformer: Gated axial-attention for medical image segmentation,” in\nMedical Image Computing and Computer Assisted Intervention–MICCAI\n2021: 24th International Conference, Strasbourg, France, September\n27–October 1, 2021, Proceedings, Part I 24. Springer, 2021, pp. 36–46.\n[45] F. Zhang, Y . Chen, Z. Li, Z. Hong, J. Liu, F. Ma, J. Han, and E. Ding,\n“Acfnet: Attentional class feature network for semantic segmentation,”\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 6798–6807.\n[46] H. Hu, D. Ji, W. Gan, S. Bai, W. Wu, and J. Yan, “Class-wise dynamic\ngraph convolution for semantic segmentation,” in Computer Vision–\nECCV 2020: 16th European Conference, Glasgow, UK, August 23–28,\n2020, Proceedings, Part XVII 16. Springer, 2020, pp. 1–17.\n[47] G. Deng, Z. Wu, C. Wang, M. Xu, and Y . Zhong, “Ccanet: Class-\nconstraint coarse-to-fine attentional deep network for subdecimeter aerial\nimage semantic segmentation,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 60, pp. 1–20, 2021.\n[48] F. Zhou, R. Hang, and Q. Liu, “Class-guided feature decoupling network\nfor airborne image segmentation,”IEEE Transactions on Geoscience and\nRemote Sensing, vol. 59, no. 3, pp. 2245–2255, 2020.\n[49] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors\nfor word representation,” in Proceedings of the 2014 conference on\nempirical methods in natural language processing (EMNLP), 2014, pp.\n1532–1543.\n[50] U. Michieli and P. Zanuttigh, “Continual semantic segmentation via\nrepulsion-attraction of sparse and disentangled latent representations,”\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2021, pp. 1114–1124.\n[51] X. Li, X. Li, L. Zhang, G. Cheng, J. Shi, Z. Lin, S. Tan, and\nY . Tong, “Improving semantic segmentation via decoupled body and\nedge supervision,” in Computer Vision–ECCV 2020: 16th European\nConference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII\n16. Springer, 2020, pp. 435–452.\n[52] J. Nie, C. Zheng, C. Wang, Z. Zuo, X. Lv, S. Yu, and Z. Wei, “Scale–\nrelation joint decoupling network for remote sensing image semantic\nsegmentation,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 60, pp. 1–12, 2022.\n[53] G. Deng, Z. Wu, C. Wang, M. Xu, and Y . Zhong, “Ccanet: Class-\nconstraint coarse-to-fine attentional deep network for subdecimeter\naerial image semantic segmentation,” IEEE Trans. Geosci. Remote.\nSens., vol. 60, pp. 1–20, 2022. [Online]. Available: https://doi.org/10.\n1109/TGRS.2021.3055950\n[54] F. Rottensteiner, G. Sohn, M. Gerke, and J. D. Wegner, “Isprs semantic\nlabeling contest,” ISPRS: Leopoldsh¨ohe, Germany, vol. 1, no. 4, 2014.\n[55] J. Wang, Z. Zheng, A. Ma, X. Lu, and Y . Zhong, “Loveda: A remote\nsensing land-cover dataset for domain adaptive semantic segmentation,”\narXiv preprint arXiv:2110.08733, 2021.\n[56] A. Buslaev, V . I. Iglovikov, E. Khvedchenya, A. Parinov, M. Druzhinin,\nand A. A. Kalinin, “Albumentations: fast and flexible image augmenta-\ntions,” Information, vol. 11, no. 2, p. 125, 2020.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3383066\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.749592661857605
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6149473786354065
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5809669494628906
    },
    {
      "name": "Pixel",
      "score": 0.5499979853630066
    },
    {
      "name": "Segmentation",
      "score": 0.5425480008125305
    },
    {
      "name": "Feature extraction",
      "score": 0.5232143402099609
    },
    {
      "name": "Semantic feature",
      "score": 0.4859296381473541
    },
    {
      "name": "Semantic similarity",
      "score": 0.4283060133457184
    },
    {
      "name": "Transformer",
      "score": 0.41845810413360596
    },
    {
      "name": "Computer vision",
      "score": 0.3971972167491913
    },
    {
      "name": "Data mining",
      "score": 0.3639256954193115
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}