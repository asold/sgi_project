{
  "title": "Cross-view Geo-localization with Evolving Transformer",
  "url": "https://openalex.org/W3178560723",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5103561888",
      "name": "Hongji Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5030640379",
      "name": "Xiufan Lu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5068185303",
      "name": "Yingying Zhu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965143341",
    "https://openalex.org/W2572697301",
    "https://openalex.org/W2479919622",
    "https://openalex.org/W2199890863",
    "https://openalex.org/W1998728242",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2981670313",
    "https://openalex.org/W2744749505",
    "https://openalex.org/W3092933908",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2992077842",
    "https://openalex.org/W2963703618",
    "https://openalex.org/W2971132234",
    "https://openalex.org/W2956261196",
    "https://openalex.org/W3035158519",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1552672384",
    "https://openalex.org/W2932425724",
    "https://openalex.org/W2799087793",
    "https://openalex.org/W3035422681",
    "https://openalex.org/W3093471646",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2179042386"
  ],
  "abstract": "In this work, we address the problem of cross-view geo-localization, which estimates the geospatial location of a street view image by matching it with a database of geo-tagged aerial images. The cross-view matching task is extremely challenging due to drastic appearance and geometry differences across views. Unlike existing methods that predominantly fall back on CNN, here we devise a novel evolving geo-localization Transformer (EgoTR) that utilizes the properties of self-attention in Transformer to model global dependencies, thus significantly decreasing visual ambiguities in cross-view geo-localization. We also exploit the positional encoding of Transformer to help the EgoTR understand and correspond geometric configurations between ground and aerial images. Compared to state-of-the-art methods that impose strong assumption on geometry knowledge, the EgoTR flexibly learns the positional embeddings through the training objective and hence becomes more practical in many real-world scenarios. Although Transformer is well suited to our task, its vanilla self-attention mechanism independently interacts within image patches in each layer, which overlooks correlations between layers. Instead, this paper propose a simple yet effective self-cross attention mechanism to improve the quality of learned representations. The self-cross attention models global dependencies between adjacent layers, which relates between image patches while modeling how features evolve in the previous layer. As a result, the proposed self-cross attention leads to more stable training, improves the generalization ability and encourages representations to keep evolving as the network goes deeper. Extensive experiments demonstrate that our EgoTR performs favorably against state-of-the-art methods on standard, fine-grained and cross-dataset cross-view geo-localization tasks.",
  "full_text": "Cross-view Geo-localization with Evolving\nTransformer\nHongji Yang, Xiufan Lu, Yingying Zhu∗\nCollege of Computer Science and Software Engineering,\nShenzhen University\n{2070276033, luxiufan2019}@email.szu.edu.cn, zhuyy@szu.edu.cn\nAbstract\nIn this work, we address the problem of cross-view geo-localization, which esti-\nmates the geospatial location of a street view image by matching it with a database\nof geo-tagged aerial images. The cross-view matching task is extremely challenging\ndue to drastic appearance and geometry differences across views. Unlike existing\nmethods that predominantly fall back on CNN, here we devise a novel evolving\ngeo-localization Transformer (EgoTR) that utilizes the properties of self-attention\nin Transformer to model global dependencies, thus signiﬁcantly decreasing visual\nambiguities in cross-view geo-localization. We also exploit the positional encoding\nof Transformer to help the EgoTR understand and correspond geometric conﬁgu-\nrations between ground and aerial images. Compared to state-of-the-art methods\nthat impose strong assumption on geometry knowledge, the EgoTR ﬂexibly learns\nthe positional embeddings through the training objective and hence becomes more\npractical in many real-world scenarios. Although Transformer is well suited to\nour task, its vanilla self-attention mechanism independently interacts within image\npatches in each layer, which overlooks correlations between layers. Instead, this\npaper propose a simple yet effective self-cross attention mechanism to improve\nthe quality of learned representations. The self-cross attention models global de-\npendencies between adjacent layers, which relates between image patches while\nmodeling how features evolve in the previous layer. As a result, the proposed\nself-cross attention leads to more stable training, improves the generalization abil-\nity and encourages representations to keep evolving as the network goes deeper.\nExtensive experiments demonstrate that our EgoTR performs favorably against\nstate-of-the-art methods on standard, ﬁne-grained and cross-dataset cross-view\ngeo-localization tasks. Codes are available in the supplementary material.\n1 Introduction\nEstimating the geospatial location of a given image is of paramount importance for robot naviga-\ntion [11], 3D reconstruction [12] and autonomous driving [5]. Recently, cross-view geo-localization,\nwhich aims to match query ground images with geo-tagged database aerial/satellite images, has\nemerged as a promising proposal to address this problem. Despite its appealing application prospect,\nthe cross-view matching task is extremely challenging due to drastic viewpoint changes between\nground and aerial images. Thus, it is critical to understand and correspond both image content\n(appearance and semantics) and spatial layout across views.\nTowards the above goal, several recent works incorporate convolutional neural networks (CNNs)\nwith NetVlad layers [ 8], capsule networks [ 20] or attention mechanisms [ 2, 16] to learn visually\ndiscriminative representations. However, the locality assumption of their CNN architectures hinders\n∗Corresponding author.\nPreprint. Under review.\narXiv:2107.00842v2  [cs.CV]  5 Jul 2021\ntheir performance in complex scenarios, where visual interferences such as obstacles and transient\nobjects (e.g., cars and pedestrian) may exist. Instead, human visual system utilizes not only local\ninformation but also global context to make more accurate predictions when visual signals are\nambiguous or incomplete. Another branch of works exploits geometry prior knowledge to reduce\nambiguities caused by geometric misalignments. Though promising, these methods either rely\nheavily on predeﬁned orientation prior [9], or make a restrictive assumption that ground and aerial\nimages are orientation-aligned [16]. As a result, such strong assumptions limit the applicability of\nthese approaches, which prompts us to seek a more ﬂexible approach for encoding position-aware\nrepresentations.\nMotivated by these observations, we introduce Transformer [21], which exceeds in global contextual\nreasoning and thus can be naturally employed to reduce visual ambiguities in cross-view geo-\nlocalization. Besides, the positional encoding of Transformer enables our network to ﬂexibly learn\nposition-dependent representations. Speciﬁcally, our proposed evolving geo-localization Transformer\n(EgoTR) is built upon two independent Vision Transformer (ViT) [4] branches, which split a feature\nmap into several sub-patches while modeling interactions between arbitrary patches. We show in\nthe experiment that due to its context- and position-dependent natures, such a Transformer-based\nnetwork is a well suited candidate for cross-view geo-localization and shows its superiority compared\nto the dominant CNN-based counterparts.\nWe also take a deep look at self-attention map, which is the integral part of Transformer and is\nindependently learned in each Transformer block. Nevertheless, such an independent learning\nstrategy overlooks correlations between layers. Speciﬁcally, relating features from adjacent layers\ncould improve the representation ability of network [13]. Compared to simply fusing features from\nall layers, interacting between features from adjacent layers produces less information redundancy\nand noise interference as they are in closer abstraction level. To explore cross-layer correlations,\nwe replace the self-attention with a novel self-cross attention mechanism. Simple yet effective, the\nproposed self-cross attention learns pairwise similarities between features of adjacent blocks rather\nthan of the same block. On one hand, such a cross-block interaction strategy eases the information\nﬂow across Transformer blocks, thus leading to more stable and effective network optimization. On\nthe other hand, the self-cross attention map not only models intra-relations of an image, but also\nreﬂects how the learned representations evolve in the previous block. As shown in the experiments,\nthis could facilitate representations to keep evolving in deep layers, thus improving the quality of\nimage representation.\nThe key contributions of this work are as follows.\n• To the best of our knowledge, the EgoTR is the ﬁrst model using Transformer for cross-view\ngeo-localization. The globally context-aware nature of the EgoTR effectively reduces visual\nambiguities in cross-view geo-localization, while the positional encoding endows the EgoTR\nwith the notion of geometry, thus decreasing ambiguities caused by geometry misalignments.\nSince the position embeddings are learned without imposing strong assumption on the\nposition knowledge, the EgoTR has wider practical applicability compared with state-of-the-\nart models.\n• We propose a novel self-cross attention mechanism, which interacts within cross-layer\npatches to ensure effective information ﬂow across Transformer blocks and to encourage\nrepresentations to keep evolving. This simple yet effective design consistently enhances\nthe representation and the generalization ability of the EgoTR, without adding additional\ncomputational cost.\n• Extensive experiments demonstrate that our EgoTR brings consistent and signiﬁcant perfor-\nmance improvements for a wide range of cross-view matching tasks, including standard,\nﬁne-grained and cross-dataset cross-view geo-localization. On all these tasks, the EgoTR\nexhibits its superiority of learning visually discriminative and position-aware representations\nand achieves a new state-of-the-art performance.\n2 Related Work\nThe key to cross-view geo-localization is to understand and correspond both image content (appear-\nance and semantics) and spatial layout across views. To this end, existing cross-view geo-localization\nmethods can be roughly grouped to two categories: content-based and geometry-based.\n2\nContent-based methods focus on learning image representations that are discriminative enough to\ndistinguish between similar looking images. Leveraging on the success of CNNs, Workman and\nJacobs [23] ﬁrst introduce CNNs to the cross-view matching task. Later on, Hu et al. [8] incorporate\na two-branch VGG [ 19] backbone network with NetVlad layers [ 1] to learn viewpoint-invariant\nrepresentations. They also devise a weighted soft-margin triplet loss, which can speed up the network\ntraining. Sun et al. [20] apply the powerful ResNet [7] as backbone networks. Coupled with capsule\nlayers [15], their proposed GeoCapsNet is capable of modeling high-level semantics. To steer\nwhere to focus in images, the attention mechanism is introduced to the ﬁeld of cross-view geo-\nlocalization. Cai et al. [2] introduce a lightweight attention module that combines spatial and channel\nattention mechanisms to emphasize visually salient features. They also propose a novel reweighting\nloss that adaptively allocates weights to triplets according to their difﬁculties, thus improving the\nquality of network training. SAFA [16] employs a multi-head spatial attention module to aggregate\ninformative and diverse embedding maps. While promising, few of the above methods pay enough\nattention to the global dependencies of cross-view images, which hinders the discriminativeness\nof their embedded features. Different from existing methods, this work makes the ﬁrst exploration\nto introduce Transformer [21] to cross-view geo-localization. We demonstrate the importance of\nconsidering global dependencies for reducing visual ambiguities.\nGeometry-based methods aim to correspond geometric conﬁgurations between ground and aerial\nimages, which helps to reduce ambiguities caused by geometry misalignments. To this end, Liu\nand Li [9] explicitly inject per-pixel orientation information into the network. Nevertheless, this is\nbased on the assumption of accessibility of the groundtruth orientation, which is not always satisﬁed\nin practice. Shi et al. [16] employ polar transform algorithm to warp satellite images so that aerial\nimages are geometrically aligned with ground images. However, this method is only applicable to the\nideal case where the ground images are orientation-aligned panoramas. Even though the dynamic\nsimilarity module proposed in [ 17] overcomes this limitation, the bruteforce warping strategy of\nthe polar transform overlooks the depth of the scene content and results in obvious appearance\ndistortions, which hinders the performance improvement. Regmi and Shah [14] attempt to tackle this\nproblem by synthesizing the corresponding satellite image from a ground query using conditional\nGANs (cGANs), but the synthesized images are always granulated and lack details. In this paper,\nour EgoTR explicitly encodes learnable positional embeddings into the network, without imposing\nstrong assumption. Different from the previous works [9, 16, 17], as shown in the experiments, our\nEgoTR not only learns relative positional information but also considers the scene context when\ncorresponding geometric conﬁgurations across views.\n3 Method: EgoTR\nIn this paper, we propose a novel evolving geo-localization Transformer (EgoTR) architecture with\nthe self-cross attention mechanism for cross-view geo-localization. The following sections detail our\nproblem setting, objective, the EgoTR architecture and our proposed self-cross attention.\n3.1 Problem Formulation and Objective\nThe goal of cross-view geo-localization is to localize a query ground image by matching it with a set\nof geo-tagged aerial images. We formulate this problem in the same way as prior works [9, 18, 16, 17].\nAssume we have a training set D= {(g1,a1),..., (gN,aN)}containing N cross-view image pairs\nof ground images gand aerial images a. To simplify the problem, during training phase, let each\nground image gi corresponds to only one ground-truth aerial image ai (i∈{1,2,...,N }). Given a\ncross-view image pair (gi,ai), we infer the corresponding image representations as (Fg\ni,Fa\ni). Then,\nthe weighted soft-margin triplet loss [8] Lof the ith exemplar can be deﬁned as follows:\nL= log(1 +eα(d(Fg\ni ,Fa\ni )−d(Fg\ni ,Fa\nj ))) (1)\nwhere j ∈{1,2,...,N }and j ̸= i. αis a hyperparameter used to speed up training convergence, and\nd(·,·) denotes the L2 distance.\n3\nFigure 1: (a) Overview of our evolving geo-localization Transformer (EgoTR). (b) Illustration of the\nencoder layer with the self-cross attention in the EgoTR. xl−1 denotes the input of layer l.\n3.2 Transformer for Cross-view Geo-localization\nWe seek to develop an EgoTR architecture that explores the global context and the positional\ninformation of cross-view images.\nPreliminaries: Vision Transformer.We ﬁrst describe the Vision Transformer (ViT) [4] architecture\nas background. Given an image, the ViT ﬁrst splits it into several patches. Then, the ViT receives as\ninput a sequence of linear projected patch embeddingsx ∈RN×D, where N is the number of patches,\nand Dis the patch embedding size. After prepending a learnable class embedding xclass ∈RD,\nwhose state at the output of the ViT is the image representation, and adding positional embeddings\nxpos to x, we gain x0 = [xclass; x] +xpos and feed it into a L-layer Transformer encoder. Each\nlayer consists of a Multihead Self-Attention module (MSA), Feed Forward Networks (FFN) and\nLayerNorm blocks (LN). Note that, the MSA is consist of multiple self-attention heads and a linear\nprojection block. In order to make a clear comparison with our proposed self-cross attention head,\nwe denote the input of layer l(l∈{1,...,L }) as xl−1 and formulate a single self-attention head, the\ncore of the vanilla MSA, as follows:\nzl = LN(xl−1)\nQl = zlWq\nl,Kl = zlWk\nl,Vl = zlWv\nl\nAl = softmax(QlKT\nl√\nD\n)Vl\n(2)\nwhere Wq\nl, Wk\nl and Wv\nl are linear projection matrices.\nDomain-speciﬁc Transformer. The drastic domain gap between ground and aerial images hints\nthat it is difﬁcult to match the ground and aerial representations in the same data space. To suit\nthe cross-view geo-localization task, we adopt a domain-speciﬁc Siamese-like architecture with\ntwo independent ViT branches of the same structure to separately learn ground and aerial image\nrepresentations. The network overview is illustrated in Figure 1 (a). Each branch is a hybrid structure\nconsisting of a ResNet backbone extracting CNN feature map from an image input and a ViT\nmodeling global context from the CNN feature map. The linear projection of patch embedding in the\nViT is applied to the CNN feature map by regarding each 1 ×1 feature as a patch.\nLearnable positional embedding. Geometric cue can greatly simplify the cross-view geo-\nlocalization task [9, 16]. Instead of imposing a pre-deﬁned orientation knowledge on the network,\nthis paper applies an efﬁcient and ﬂexible way to endow the network with the notion of geometry.\nSpeciﬁcally, we use learnable 1D positional embeddings in the ViT, i.e. xpos ∈R(N+1)×D. By\nadding the positional embeddings to the linear patch embeddings, the transformed features become\nposition-dependent. Furthermore, since we do not impose any assumption on the position knowledge\n4\nbut learn it through our learning objective, our EgoTR has wider practical applicability. As shown in\nthe experiment, incorporating the learnable positional embeddings helps to capture relative positional\ninformation, which generalizes better to orientation-unknown images than absolute positional infor-\nmation. In addition, our EgoTR takes the scene content into account when corresponding cross-view\ngeometry, which is complementary to the polar transform [16, 17] and leads to a better localization\nperformance.\n3.3 Self-cross Attention\nIn the vanilla ViT, the attention map is calculated independently in each layer. However, as mentioned\nbefore, such an independent learning strategy hinders the model’s representation ability. To improve\nthe quality of the learned representations, this paper proposes a novel self-cross attention mechanism\nto interact features between adjacent layers. Speciﬁcally, the attention map of layer lis learned not\nonly based on xl−1, but also xl−2. Formally, in layer l, the self-cross attention can be represented as:\nzl = LN(xl−1),zl−1 = LN(xl−2),\nQl = zlWq\nl,Kl = zl−1Wk\nl,Vl = zlWv\nl\nAl = softmax(QlKT\nl√\nD\n)Vl\n(3)\nNote that, for l = 1, we set zl−1 = LN(xl−1). In Figure 1 (b), we illustrate the structure of the\nself-cross attention-based encoder layer.\nFigure 2: Recall accuracy curve on CVUSA\ntest set with increasing training epochs.\nHow the self-cross attention affects feature\nlearning. Compared to the self-attention in Eq. 2,\nour proposed self-cross attention creates short path\nbetween adjacent layers, thus allowing information\nﬂow effectively across layers. In fact, this shares\nthe similar spirit with the ResNet [7]. To investigate\nhow this affects our Transformer-based network,\nwe plot the recall accuracy curve on CVUSA test\nset [24] with increasing training epochs in Figure 2.\nAs shown, during the early training stage, the lo-\ncalization performance of the self-attention-based\nmodel ﬂuctuates a lot, while our EgoTR exhibits\nmore stable performance improvements as training\nprocesses. As a result, the stable training of the\nEgoTR could improves the network performance.\nFurthermore, by interacting cross-layer features, the attention map models how features evolve in the\nprevious layer, which could decrease the representation similarity between layers, thus improving the\nnetwork’s representation ability. This is further discussed in the experiment.\n4 Experiment\nWe ﬁrst introduce three benchmark datasets we used to evaluate our EgoTR, evaluation protocols\nand implement details of our network. Then we compare our EgoTR with state-of-the-art models\nin Section 4.3 and present ablation studies to illustrate the advantages of the proposed EgoTR in\nSection 4.4. Finally, we provide qualitative results in Section 4.5 to demonstrate the effectiveness of\nthe positional embeddings in the EgoTR.\n4.1 Dataset and Evaluation Protocol\nDataset. To verify the effectiveness our model, we conduct extensive experiments on three widely\nused benchmarks: CVUSA [24] and CV ACT [9] (including CV ACT_val and CV ACT_test). CVUSA\ndataset provides 35,532 image pairs for training and 8,884 image pairs for testing. CV ACT dataset\ncontains 35,532 pairs for training and 8,884 pairs for validation (denoted as CV ACT_val). Addition-\nally, to support ﬁne-grained city-scale geo-localization, CV ACT also provides 92,802 image pairs\nwith accurate geo-tags for testing (denoted as CV ACT_test).\n5\nTable 1: Comparisons with state-of-the-art models on CV ACT_val (standard cross-view geo-\nlocalization) and CV ACT_test (ﬁne-grained geo-localization) datasets. Other results are quoted\nfrom [16] and [17]. “PT” indicates whether the model applies (w/) polar transform [ 16] to aerial\nimages or not (w/o).\nPT Model\nCV ACT_val CV ACT_test\nCode r@1 r@5 r@10 r@1% r@1 r@5 r@10 r@1%\nLength (%) (%) (%) (%) (%) (%) (%) (%)\nw/o\nCVM-Net [8] 4096 20.15 45.00 56.87 87.57 5.41 14.79 25.63 54.53\nLiu and Li [9] 1536 46.96 68.28 75.48 92.01 19.21 35.97 43.30 60.69\nCVFT [18] 4096 61.05 81.33 86.52 95.93 26.12 45.33 53.80 71.69\nSAFA [16] 4096 78.28 91.60 93.79 98.15 - - - -\nEgoTR 768 83.14 93.84 95.51 98.40 58.33 84.23 88.60 95.83\nw/\nSAFA [16] 4096 81.03 92.80 94.84 98.17 55.50 79.94 85.08 94.49\nShi et al. [17] 4096 82.49 92.44 93.99 97.32 35.63 60.07 69.10 84.75\nPolar-EgoTR 768 84.89 94.59 95.96 98.37 60.72 85.85 89.88 96.12\nTable 2: Comparisons with state-of-the-art methods on\nCVUSA [24] dataset. For all the compared methods, we cite\nthe results from [17] and [16] if not specialized.\nPT Model r@1 r@5 r@10 r@1%\n(%) (%) (%) (%)\nw/o\nWorkman et al. [23] - - - 34.30\nV o and Hays [22] - - - 63.70\nZhai et al. [24] - - - 43.20\nCVM-Net [8] 22.47 49.98 63.18 93.62\nLiu and Li [9] 40.79 66.82 76.36 96.12\nZheng et al. [25] 43.91 66.38 74.58 91.78\nRegmi and Shah [14] 48.75 - 81.27 95.98\nSiam-FCANet [2] - - - 98.30\nCVFT [18] 61.43 84.69 90.49 99.02\nSAFA [16] 81.15 94.23 96.85 99.49\nEgoTR 91.99 97.68 98.65 99.75\nw/\nSAFA [16] 89.84 96.93 98.14 99.64\nShi et al. [17] 91.93 97.50 98.54 99.67\nPolar-EgoTR 94.05 98.27 98.99 99.67\nEvaluation protocol. In line with\n[8, 9, 18, 16, 17], we evaluate\nour model by recall accuracy at\ntop K (r@K for short, K ∈\n{1,5,10,1%}), which represents\nthe probability of correct match(es)\nranking within the ﬁrst K results.\nNote that r@1% means the recall\naccuracy at top 1% of test set.\nFor CVUSA and CV ACT_val, a\nquery ground image corresponds\nto a single aerial image, while for\nCV ACT_test, aerial images that lo-\ncate within 5 meters of the query\nground image can be seen as the\ncorrect matches.\n4.2 Implementation Detail\nIf not speciﬁed, the ground and\naerial image size are set to 128 ×512 and 256 ×256, respectively. We empirically set model\ndepth Lto 12 and initialize our EgoTR with pretrained parameters on ImageNet [ 3]. The model\nis trained using AdamW [10] with cosine learning rate schedule on a 32GB NVIDIA V100 GPU.\nThe learning rate is set to 1e-4, the weight decay is chosen to 0.03 and the batch size is 32. For the\nweighted soft-margin triplet loss [8], αis set to 10.\n4.3 Comparing EgoTR with State-of-the-art Models\nHere we compare our method with several state-of-the-art methods on CVUSA [24], CV ACT_val [9]\nand CV ACT_test [9] datasets. Unlike state-of-the-art methods that predominantly fall back on\nCNN, our proposed EgoTR makes the ﬁrst effort to introduce Transformer to the ﬁeld of cross-view\ngeo-localization to learn globally context- and position-aware representations. Below, we verify\nthat our EgoTR exceeds in learning visually discriminative and position-aware representations, thus\nachieving outstanding performance in various cross-view geo-localization tasks. Note that for fair\ncomparison with works [16, 17] that use polar transform [16], a kind of data pre-processing algorithm,\nwe apply the same warping strategy to aerial images before feeding them into the network (denoted\nas Polar-EgoTR) when comparing with these works. In this case, ground and warped aerial images\nare resized to 128 ×512.\nStandard cross-view geo-localization. We ﬁrst evaluate our EgoTR on standard cross-view geo-\nlocalization. Table 1 and 2 show experimental results on CV ACT_val and CVUSA datasets, respec-\ntively. From the results, we could conclude that our EgoTR signiﬁcantly surpasses the competing\n6\napproaches in learning visually discriminative representations and corresponding geometric conﬁgu-\nrations across views. In particular, without applying the polar transform, our EgoTR achieves r@1\nof 83.14% on CV ACT_val dataset compared to 78.28% obtained by the second best method, while\non CVUSA dataset the EgoTR surpasses the second best method by a signiﬁcant margin of 10.84\npoints at r@1. Moreover, when applying the polar transform, which geometrically aligns cross-view\nimages, our EgoTR outperforms the competing methods, gaining 84.89% and 94.05% on CV ACT_val\nand CVUSA, respectively. The results indicates that the EgoTR is capable of capturing visually\ndiscriminative features by modeling global context. Furthermore, we could also ﬁnd that removing\nthe polar transform algorithm leads to signiﬁcant performance degradation in SAFA ( −4.21% on\nCV ACT_val and−8.69% on CVUSA) while less of a degradation is noted in our EgoTR (−1.75%\non CV ACT_val and−2.06% on CVUSA). Namely, the EgoTR does not have to rely excessively on\nthe polar transform to establish cross-view geometric correspondence, which could save considerable\nimage pre-processing time on large-scale datasets. This is because, adding the positional embeddings\nto the linear patch embeddings enables the EgoTR to learn position-aware representations and to\ncorrespond geometric conﬁgurations between ground and aerial images. Additional qualitative\nevidence for this is provided in Section 4.5.\nTable 3: Cross-dataset cross-view geo-localization. The results are\ngained by retraining and evaluating the compared models using the\nreleased codes provided by their authors.\nModel Task r@1 r@5 r@10 r@1%\n(%) (%) (%) (%)\nSAFA [16]\nCVUSA→CV ACT\n30.40 52.93 62.29 85.82\nShi et al. [17] 33.66 52.17 59.74 79.67\nPolar-EgoTR 47.55 70.58 77.39 91.39\nSAFA [16]\nCV ACT→CVUSA\n21.45 36.55 43.79 69.83\nShi et al. [17] 18.47 34.46 42.28 69.01\nPolar-EgoTR 33.00 51.87 60.63 84.79\nFine-grained cross-view\ngeo-localization. To evaluate\nthe representation ability\nof our model, we verify\nthe EgoTR on ﬁne-grained\ncross-view geo-localization\ntask. Speciﬁcally, we\ncompare the EgoTR with\nstate-of-the-art methods on\nthe challenging large-scale\nCV ACT_test dataset, which is\nfully gps-tagged for accurate\nlocalization. Table 1 shows\nthe experimental results. Our\nEgoTR performs consistently\nbetter than all the competitors, achieving 58.33% and 60.76% at r@1 without and with the polar\ntransform, respectively. These results further demonstrate that our EgoTR has strong representation\ncapability.\nTable 4: Ablation studies of the EgoTR.\nModel r@1 r@5 r@10 r@1%\n(%) (%) (%) (%)\nPolar-EgoTR 94.05 98.27 98.99 99.67\nw/o self-cross att. 93.26 97.91 98.78 99.68\nw/o positional emb. 90.90 97.48 98.40 99.62\nEgoTR 91.99 97.68 98.65 99.75\nw/o positional emb. 89.04 96.88 98.44 99.61\nCross-dataset cross-view geo-\nlocalization. In the context of\ncross-view geo-localization, the\ntransferring performance determines\nwhether a model could be practically\nusable for real-life scenarios, where\na query image may be dramatically\ndifferent from the training ground\nimages. As CV ACT and CVUSA\ndatasets are collected from two\ndifferent countries, they have distinctly different scene styles. Based on this observation, to verify\nthe transferring performance of our model, we train the EgoTR on CVUSA dataset and test it on\nCV ACT_val (denoted as CVUSA→CV ACT), and vice versa. Results are reported in Table 3. We\ncould ﬁnd that our EgoTR outperforms the second best model at r@1 by a large margin of 13.89\npoints on the CVUSA→CV ACT task, while achieves 33.00% at r@1 compared to 21.45% gained\nby the second best model on the CV ACT→CVUSA task. The transferring results demonstrate the\noutstanding generalization ability and practical applicability of our EgoTR.\nComparison in terms of code length. To further illustrate the advantage of our method, we compare\nthe EgoTR with state-of-the-art methods in terms of image descriptor dimension (also called code\nlength) in Table 1. We observe that our EgoTR has extremely short code length of 768, which is ﬁve\ntimes shorter than that of the SAFA [16], CVFT [18] and CVM-Net [8]. A shorter code length not\nonly implies the effective information encoding capability of the EgoTR, but also means that the\n7\nEgoTR provides an alternative with less storage space, lower computational complexity and shorter\nrunning time to cross-view geo-localization.\n4.4 Ablation Study\nTo investigate the effectiveness of the positional embeddings and the self-cross attention mechanism,\nwe conduct ablation studies by considering three scenarios: 1) where the positional embeddings are\nremoved, 2) where the polar transform is removed and 3) where the self-cross attention is replaced by\nthe self-attention.\nTable 5: Few-shot cross-view geo-localization on CVUSA [ 24]. We\ndescribe the number of training pairs of each subset and its proportion\n(Prop.) to the original CVUSA dataset.\nTraining Prop. Model r@1 r@5 r@10 r@1%\nPairs (%) (%) (%) (%)\n7,106 20% Polar-EgoTR 76.01 90.67 94.01 98.85\nw/o self-cross att. 75.37 90.42 92.85 98.66\n14,212 40% Polar-EgoTR 86.06 95.80 97.16 99.38\nw/o self-cross att. 85.54 95.14 97.07 99.42\n21,319 60% Polar-EgoTR 90.30 96.96 98.26 99.67\nw/o self-cross att. 88.74 96.60 98.09 99.66\n35,532 100% Polar-EgoTR 94.05 98.27 98.99 99.67\nw/o self-cross att. 93.26 97.91 98.78 99.68\nPositional encoding. We\nﬁrst analyze the impor-\ntance of the positional\nembeddings and report\nthe ablation studies on\nCVUSA dataset in Table 4.\nFrom the results, we can\nmake the following ob-\nservations. First, the po-\nsitional encoding endows\nthe network with the con-\ncept of position and yields\nconsistent improvements.\nIn particular, adding po-\nsitional embeddings im-\nproves the r@1 performance of the EgoTR from 89.04% to 91.99% and brings 3.15 points im-\nprovement at r@1 to the Polar-EgoTR. Second, we could also ﬁnd that the positional embeddings are\ncomplementary to the polar transform [16, 17]. Speciﬁcally, combining the polar transform with our\nEgoTR improves the r@1 accuracy from 91.99% to 94.05% (+2.06%), while adding the positional\nembeddings to Polar-EgoTR boosts the r@1 performance from 90.90% to 94.05% (+3.15%). An\ninterpretation of this is that, incorporating the learnable positional embeddings is capable of consider-\ning the scene content when corresponding cross-view geometric conﬁgurations, which is overlooked\nin polar transform algorithm as mentioned before. As a result, the polar transform and the positional\nencoding could jointly improve the network performance. In Section 4.5, we show qualitative results\nto verify this inference.\nFigure 3: Cross-layer similarity between the\nlast layer and previous layers.\nSelf-cross attention. In Table 4, we ablate the self-\ncross attention mechanism by replacing it with the\nself-attention on CVUSA dataset. We can observe\nfrom Table 4 that, without imposing increase in\nmodel complexity, the self-cross attention mecha-\nnism improves r@1 performance from 93.26% to\n94.05%, which manifests the effectiveness of the\nself-cross attention. Moreover, in Table 5, we ab-\nlates the self-cross attention on few-shot cross-view\ngeo-localization task. The few-shot task aims to\nlearn a model that can achieve generalization from\nonly a small number of training examples [6]. To\nsupport this task, we randomly select a certain per-\ncentage (20%/40%/60%) of samples from CVUSA\ndataset to generate three subsets. The size of each\nsubset and its corresponding proportion to CVUSA\ndataset are illustrated in Table 5. Results show that,\nreplacing the self-cross attention with the self-attention consistently harms the network performance\non few-shot task. This indicates that, the self-cross attention not only improves network performance,\nbut also enhances its generalization ability. Furthermore, we also investigate how the self-cross\nattention affects feature learning. In Figure 3, we compare the ﬁnal representation with the output\nof each intermediate layer by measuring their cosine similarity. As can be observed, replacing the\nself-attention with our proposed self-cross attention signiﬁcantly and consistently decreases the\nrepresentation similarity between layers. This result implies that the self-cross attention could prevent\n8\n(a) The ground PE of the\nEgoTR\n(b) The aerial PE of the\nEgoTR\n(c) The ground PE of the\nPolar-EgoTR\n(d) The aerial PE of the\nPolar-EgoTR\nFigure 4: Dot products between the learnable positional embeddings (PE). Yellow indicates the two\npositional embeddings are closer. Better viewed in color and with zoom-in.\nthe learned representations of Transformer layers from being overly similar with each others and\nencourage the model to evolve its representations layer by layer, thus improving the performance of\nthe ﬁnal image representations.\n4.5 Qualitative Analysis\nWe conduct detailed qualitative analysises on the learnable positional embeddings to investigate\nwhether they encode and correspond geometric conﬁgurations across views and which positional\ninformation they can learn. To this end, we calculate dot products between two arbitrary positional\nembeddings of the EgoTR. From Figure 4(a) and 4(b), we could ﬁnd that each positional embedding\nis close to its neighbors with small location offsets. This implies that, incorporating the learnable\npositional embeddings captures relative positional information. Additionally, we could clearly\nobserve that, the visualization maps of the EgoTR are distinctly different across views in Figure\n4(a) and 4(b), while the visualization maps of the Polar-EgoTR look similar to each other in Figure\n4(c) and 4(d). Such similar results of the Polar-EgoTR are reasonable, since cross-view images are\ngeometrically aligned by the polar transform. This result further conﬁrms that the positional encoding\ncould capture cross-view geometric conﬁgurations. Furthermore, in Figure 4(b), it is obvious that the\npositional vectors near the center of each aerial image are close to their neighbors that have fairly\nsmall distance offsets. In contrast, the positional embeddings far away from the center are close\nto their neighbors with relatively larger offsets. Such an offset difference is learned because of the\ngeometric perspective of ground images. Speciﬁcally, two objects look farther away from each other\nin a ground image when they lie near the center of the corresponding aerial image, but look nearer to\neach other in the ground image when they lie far away from the center. This result unveils that the\nEgoTR could consider the scene content when corresponding geometric conﬁgurations across views.\n5 Conclusion and Future Work\nIn this paper, we propose a novel EgoTR architecture capable of learning globally context- and\nposition-aware representations. We also propose a novel self-cross attention to facilitate information\nﬂow across layers and encourage representations to keep evolving as the network layer goes deeper.\nExtensive experiments demonstrate that the EgoTR outperforms state-of-the-art methods in standard,\nﬁne-grained and cross-dataset cross-view geo-localization tasks. In addition, we also conduct ablation\nstudies and qualitative analyses to verify the effectiveness of the learnable positional embeddings\nand the self-cross attention. One main limitation of the EgoTR is its large demand on GPU memory.\nMoreover, the EgoTR is built on top of the pretrained Transformer, which requires a large amount of\ndata for training. For future work, we aim to develop a data-efﬁcient Transformer-based model with\nless memory consumption for cross-view geo-localization.\n9\nReferences\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn\narchitecture for weakly supervised place recognition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5297–5307, 2016.\n[2] Sudong Cai, Yulan Guo, Salman Khan, Jiwei Hu, and Gongjian Wen. Ground-to-aerial image\ngeo-localization with a hard exemplar reweighting triplet loss. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 8391–8400, 2019.\n[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248–255. Ieee, 2009.\n[4] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[5] Christian Häne, Lionel Heng, Gim Hee Lee, Friedrich Fraundorfer, Paul Furgale, Torsten Sattler,\nand Marc Pollefeys. 3d visual perception for self-driving cars using a multi-camera system:\nCalibration, mapping, localization, and obstacle detection. Image and Vision Computing, 68:\n14–27, 2017.\n[6] Jun He, Richang Hong, Xueliang Liu, Mingliang Xu, Zheng-Jun Zha, and Meng Wang. Memory-\naugmented relation network for few-shot learning. InProceedings of the 28th ACM International\nConference on Multimedia, pages 1236–1244, 2020.\n[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 770–778, 2016.\n[8] Sixing Hu, Mengdan Feng, Rang MH Nguyen, and Gim Hee Lee. Cvm-net: Cross-view\nmatching network for image-based ground-to-aerial geo-localization. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 7258–7267, 2018.\n[9] Liu Liu and Hongdong Li. Lending orientation to neural networks for cross-view geo-\nlocalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5624–5633, 2019.\n[10] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam. 2018.\n[11] Colin McManus, Winston Churchill, Will Maddern, Alexander D Stewart, and Paul Newman.\nShady dealings: Robust, long-term visual localisation using illumination invariance. In 2014\nIEEE international conference on robotics and automation (ICRA), pages 901–906. IEEE, 2014.\n[12] Sven Middelberg, Torsten Sattler, Ole Untzelmann, and Leif Kobbelt. Scalable 6-dof localization\non mobile devices. In European conference on computer vision, pages 268–283. Springer, 2014.\n[13] Youwei Pang, Xiaoqi Zhao, Lihe Zhang, and Huchuan Lu. Multi-scale interactive network for\nsalient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9413–9422, 2020.\n[14] Krishna Regmi and Mubarak Shah. Bridging the domain gap for ground-to-aerial image\nmatching. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npages 470–479, 2019.\n[15] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic routing between capsules. In\nProceedings of the 31st International Conference on Neural Information Processing Systems,\npages 3859–3869, 2017.\n[16] Yujiao Shi, Liu Liu, Xin Yu, and Hongdong Li. Spatial-aware feature aggregation for image\nbased cross-view geo-localization. Advances in Neural Information Processing Systems, 32:\n10090–10100, 2019.\n10\n[17] Yujiao Shi, Xin Yu, Dylan Campbell, and Hongdong Li. Where am i looking at? joint location\nand orientation estimation by cross-view matching. InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4064–4072, 2020.\n[18] Yujiao Shi, Xin Yu, Liu Liu, Tong Zhang, and Hongdong Li. Optimal feature transport for cross-\nview image geo-localization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvolume 34, pages 11990–11997, 2020.\n[19] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\n[20] Bin Sun, Chen Chen, Yingying Zhu, and Jianmin Jiang. Geocapsnet: Ground to aerial view\nimage geo-localization using capsule network. In 2019 IEEE International Conference on\nMultimedia and Expo (ICME), pages 742–747. IEEE, 2019.\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need.arXiv preprint arXiv:1706.03762,\n2017.\n[22] Nam N V o and James Hays. Localizing and orienting street views using overhead imagery. In\nEuropean conference on computer vision, pages 494–509. Springer, 2016.\n[23] Scott Workman, Richard Souvenir, and Nathan Jacobs. Wide-area image geolocalization with\naerial reference imagery. In Proceedings of the IEEE International Conference on Computer\nVision, pages 3961–3969, 2015.\n[24] Menghua Zhai, Zachary Bessinger, Scott Workman, and Nathan Jacobs. Predicting ground-level\nscene layout from aerial imagery. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 867–875, 2017.\n[25] Zhedong Zheng, Yunchao Wei, and Yi Yang. University-1652: A multi-view multi-source\nbenchmark for drone-based geo-localization. In Proceedings of the 28th ACM international\nconference on Multimedia, pages 1395–1403, 2020.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7302333116531372
    },
    {
      "name": "Transformer",
      "score": 0.6042078137397766
    },
    {
      "name": "Exploit",
      "score": 0.5758389234542847
    },
    {
      "name": "Artificial intelligence",
      "score": 0.567868709564209
    },
    {
      "name": "Geospatial analysis",
      "score": 0.4575244188308716
    },
    {
      "name": "Computer vision",
      "score": 0.3707565665245056
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33499085903167725
    },
    {
      "name": "Geography",
      "score": 0.12622517347335815
    },
    {
      "name": "Cartography",
      "score": 0.08842724561691284
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": []
}