{
    "title": "Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs",
    "url": "https://openalex.org/W4391971084",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2017065509",
            "name": "Li Wang",
            "affiliations": [
                "West China Hospital of Sichuan University",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A1983188002",
            "name": "Xi Chen",
            "affiliations": [
                "West China Hospital of Sichuan University",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2159692215",
            "name": "Xiangwen Deng",
            "affiliations": [
                "Tsinghua University",
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A1992417556",
            "name": "Hao Wen",
            "affiliations": [
                "Tsinghua University",
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A3207092055",
            "name": "Mingke You",
            "affiliations": [
                "West China Hospital of Sichuan University",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2106596301",
            "name": "Weizhi Liu",
            "affiliations": [
                "West China Hospital of Sichuan University",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2097477061",
            "name": "Qi Li",
            "affiliations": [
                "Sichuan University",
                "West China Hospital of Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2087928687",
            "name": "Jian Li",
            "affiliations": [
                "West China Hospital of Sichuan University",
                "Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2017065509",
            "name": "Li Wang",
            "affiliations": [
                "Sichuan University",
                "West China Hospital of Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A1983188002",
            "name": "Xi Chen",
            "affiliations": [
                "Sichuan University",
                "West China Hospital of Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2159692215",
            "name": "Xiangwen Deng",
            "affiliations": [
                "University Town of Shenzhen",
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A1992417556",
            "name": "Hao Wen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3207092055",
            "name": "Mingke You",
            "affiliations": [
                "Sichuan University",
                "University Town of Shenzhen",
                "West China Hospital of Sichuan University",
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2106596301",
            "name": "Weizhi Liu",
            "affiliations": [
                "Sichuan University",
                "West China Hospital of Sichuan University"
            ]
        },
        {
            "id": "https://openalex.org/A2097477061",
            "name": "Qi Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2087928687",
            "name": "Jian Li",
            "affiliations": [
                "West China Hospital of Sichuan University",
                "Sichuan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4366368023",
        "https://openalex.org/W4384007380",
        "https://openalex.org/W4380730209",
        "https://openalex.org/W4381092249",
        "https://openalex.org/W4380997513",
        "https://openalex.org/W4378953660",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W4387821331",
        "https://openalex.org/W4292213411",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W4386867830",
        "https://openalex.org/W4388452200",
        "https://openalex.org/W4387326101",
        "https://openalex.org/W4381104250",
        "https://openalex.org/W2972593257",
        "https://openalex.org/W2116781287",
        "https://openalex.org/W4385647263",
        "https://openalex.org/W4386110374",
        "https://openalex.org/W4386172820",
        "https://openalex.org/W4385827730",
        "https://openalex.org/W4388425524",
        "https://openalex.org/W4379093714",
        "https://openalex.org/W4383311938",
        "https://openalex.org/W4387496544",
        "https://openalex.org/W4388287351",
        "https://openalex.org/W4388486391",
        "https://openalex.org/W4387373157",
        "https://openalex.org/W4387327030",
        "https://openalex.org/W4368340908",
        "https://openalex.org/W4379958452",
        "https://openalex.org/W1984813075",
        "https://openalex.org/W2903031628",
        "https://openalex.org/W2076646346",
        "https://openalex.org/W4387972449",
        "https://openalex.org/W2477907264",
        "https://openalex.org/W4301393026",
        "https://openalex.org/W2985473123",
        "https://openalex.org/W4389101165"
    ],
    "abstract": "Abstract The use of large language models (LLMs) in clinical medicine is currently thriving. Effectively transferring LLMs’ pertinent theoretical knowledge from computer science to their application in clinical medicine is crucial. Prompt engineering has shown potential as an effective method in this regard. To explore the application of prompt engineering in LLMs and to examine the reliability of LLMs, different styles of prompts were designed and used to ask different LLMs about their agreement with the American Academy of Orthopedic Surgeons (AAOS) osteoarthritis (OA) evidence-based guidelines. Each question was asked 5 times. We compared the consistency of the findings with guidelines across different evidence levels for different prompts and assessed the reliability of different prompts by asking the same question 5 times. gpt-4-Web with ROT prompting had the highest overall consistency (62.9%) and a significant performance for strong recommendations, with a total consistency of 77.5%. The reliability of the different LLMs for different prompts was not stable (Fleiss kappa ranged from −0.002 to 0.984). This study revealed that different prompts had variable effects across various models, and the gpt-4-Web with ROT prompt was the most consistent. An appropriate prompt could improve the accuracy of responses to professional medical questions.",
    "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-024-01029-4\nPrompt engineering in consistency and\nreliability with the evidence-based\nguideline for LLMs\nCheck for updates\nLi Wang 1,2,4, Xi Chen1,2,4, XiangWen Deng3, Hao Wen3,M i n g K eY o u1,2,W e i Z h iL i u1,2,Q iL i 1,2 &\nJian Li 1,2\nThe use of large language models (LLMs) in clinical medicine is currently thriving. Effectively\ntransferring LLMs’pertinent theoretical knowledge from computer science to their application in\nclinical medicine is crucial. Prompt engineering has shown potential as an effective method in this\nregard. To explore the application of prompt engineering in LLMs and to examine the reliability of\nLLMs, different styles of prompts were designed and used to ask different LLMs about their agreement\nwith the American Academy of Orthopedic Surgeons (AAOS) osteoarthritis (OA) evidence-based\nguidelines. Each question was asked 5 times. We compared the consistency of theﬁndings with\nguidelines across different evidence levels for different prompts and assessed the reliability of different\nprompts by asking the same question 5 times. gpt-4-Web with ROT prompting had the highest overall\nconsistency (62.9%) and a signiﬁcant performance for strong recommendations, with a total\nconsistency of 77.5%. The reliability of the different LLMs for different prompts was not stable (Fleiss\nkappa ranged from−0.002 to 0.984). This study revealed that different prompts had variable effects\nacross various models, and the gpt-4-Web with ROT prompt was the most consistent. An appropriate\nprompt could improve the accuracy of responses to professional medical questions.\nLarge language models (LLMs) have shown good performance in various\nnatural language processing (NLP) tasks, such as summarizing, translating,\ncode synthesis, and even logical reasoning\n1– 3. There is growing interest in\nexploring the potential of LLMs in medicine. They have been used in related\nmedical studies in case diagnoses, medical examinations, and guideline\nconsistency assessments\n4– 7.\nHowever, the current performance of LLMs in the medicalﬁeld is\nnot perfect. In the diagnosis of complex cases, 39% of the GPT-4-\nrelated diagnoses were consistent with the ﬁnal diagnosis, and an\naverage consistency of 60% was shown with the guidelines for\ndigestive system diseases\n4,6. Eighteen percent of the Med-PaLM-\nrelated answers were judged to contain inappropriate or incorrect\ncontent\n8. Moreover, LLMs may generate different answers to the same\nquestion, and self-consistency has always been a crucial parameter for\nassessing the performance of LLMs 9,10. Further research and\nexploration on how to optimize its performance in the medicalﬁeld\nare necessary1,4,6,8.\nPrompt engineering is a new discipline that focuses on the develop-\nment and optimization of prompt words, thereby helping users apply LLMs\nto various scenarios and researchﬁelds. In computer science, LLMs can\nobtain ideal and stable answers through prompt engineering, and adopting\ndifferent prompts will affect the performance of LLMs, which is somewhat\nreﬂected in mathematical problems\n9,11– 13. The newly used prompt designs\ncurrently include chain of thoughts (COT) prompting and tree of thoughts\n(TOT) prompting12,13. With the proposal of the COT and TOT theories in\nthe computer science LLMﬁeld, corresponding prompts have been devel-\noped and exhibited improved performance in mathematical problems12,13.\nIn clinical medicine, a few studies have shown the application of\nprompts such as COT prompting, few-shot prompting and self-consistency\nprompting in the study of Karan et al.8.I na d d i t i o n ,t h es t u d yo fB e r t a l a ne ta l .\n14. Summarizes the current state of research on prompt engineering and\nprovides a tutorial for prompt engineering for medical professionals. Overall,\nfew studies have focused on the differences in the performance of different\nprompts in medical questions or examined whether there is a need to develop\n1Sports Medicine Center, West China Hospital, Sichuan University, Chengdu, China.2Department of Orthopedics and Orthopedic Research Institute, West China\nHospital, Sichuan University, Chengdu, China.3Shenzhen International Graduate School, Tsinghua University, Beijing, China.4These authors contributed equally:\nLi Wang, Xi Chen. e-mail: liqi_sports@scu.edu.cn; lijian_sportsmed@163.com\nnpj Digital Medicine|            (2024) 7:41 1\n1234567890():,;\n1234567890():,;\nprompts speciﬁcally for medical questions. Insummary, the application of\nL L M si nm e d i c i n ei sc u r r e n t l yt h r iving. However, most of the current\nr e s e a r c hs e e m st of o c u sm o r eo nt h er e s u l t so fu s i n gL L M sr a t h e rt h a nh o wt o\nbetter use LLMs in clinical medicine. Testing the reliability of LLMs in\nanswering medical questions, using different prompts, and even developing\nprompts speciﬁcally for medical questions could change the application of\nLLMs in medicine and future research. It is important to investigate whether\nand how prompt engineering may improve the performance of LLMs in\nanswering medical-related questions. Additionally, other factors, such as the\ntype of model architecture, model parameters, training data, andﬁne-tuning\ntechniques, can inﬂuence the performance of LLMs\n15– 17.\nTo explore the inﬂuence of different types of prompts combined with\nother factors on the performance of LLMs, we conducted a pilot study on\nosteoarthritis (OA)-related questions. The 2019 GlobalBurden of Disease\ntool identiﬁed OA as one of the most prevalent and debilitating diseases\n18.I n\nterms of prevalence and impact, OA is one of the most prevalent muscu-\nloskeletal disorders and affects a substantial portion of the global population,\nespecially elderly individuals19. This widespread impact makes it a public\nhealth concern of signiﬁcant importance, and the management of OA is\ncomplex and multifaceted, encompassing pain control, physical therapy,\nlifestyle modiﬁcations, and, in some cases, surgical interventions20.G i v e n\nthat it is a common disease with a large patient population and complex\nmanagement, patients and doctors may seek relevant professional knowl-\nedge online, which includes LLMs. Therefore, investigating the performance\nof LLMs with respect to OA-related questions could serve as an appropriate\nexample of how to improve answer quality through prompt engineering.\nThe potential of prompt engineering to assist both doctors and patients in\nmedical queries of common diseases could also be explored by using LLMs.\nOur research applied the same set of prompts to different LLMs, asking\nOA-related questions and aiming to explore the effectiveness of prompt\nengineering. We hypothesized that different prompts would result in dif-\nferent consistency and reliability and that the effectiveness of prompts on\nLLMs would be inﬂuenced by various factors.\nResults\nConsistency\nThe results indicated that gpt-4-Weboutperformed the other models, as\ns h o w ni nF i g .1. The consistency rates for the four prompts in gpt-4-Web\nranged between 50.6% and 63%. Other consistency rates were also observed\nwith IO prompting in the gpt-3.5-ft-0 at 55.3% and ROT prompting in gpt-\n4-API-0 at 51.2%. The consistency rates for the other models were all less\nthan 50% (4.7% to 45.9%).\nThe combination of gpt-4-Web and ROT generated the treatment\nrecommendation most adherent to the clinical guidelines. The top 10\ncombinations of prompts and models are shown in Fig.2. Speciﬁcally, the\nconsistency of different prompts with the guidelines in a series of GPT-4\nmodels ranged from 8.8% to 62.9%; in a series of GPT-3.5 models, including\nﬁne-tuned versions, it ranged from 4.7% to 55.3%. For different prompts in\nBard, the consistency ranged from 19.4% to 44.1%. For the three versions of\nthe GPT-4, the ROT prompting was consistently the best prompt, ranging\nfrom 35.3% to 63%. Forﬁve versions of the GPT-3.5, except for P-COT\nprompting, which was the best prompt for gpt-3.5-Web at 43.5%, the best\nprompt for the other versions was IO prompting (ranging from 27.1% to\n55.3%). For Bard, the best prompt was 0-COT prompting at 44.1%.\nSubgroup analysis\nThe AAOS categorizes recommendation levels based on the strength of\nsupporting evidence, ranging from strong to moderate, limited, and con-\nsensus. We hypothesized that different levels of evidence strength might lead\nto variations in consistency. To explore this, we conducted a subgroup\nanalysis to examine the performance differences of various prompts across\ndifferent evidence strength levels. Within the same model, we conducted\nmultiple comparisons between different prompts, with a focus on the per-\nformance of the outperformed gpt-4-Web across various evidence\nstrengths. The results of the subgroup analysis and the multiple compar-\nisons within each model can be found in Supplementary Table 1.\nStrong level. The consistency of the different prompts in the different\nmodels at the strong level is shown in Fig.3a. Eight pieces of advice are\nrated as strong by the AAOS guidelines, with 40 responses for each\nprompt. According to the multiple comparisons of consistency in gpt-4-\nWeb, the percentage differences in the ROT prompting (77.5%) and\nP-COT prompting (75%) scores were signiﬁcantly greater than that in the\nIO prompting (30%). According to the other models, the consistency of\nthe IO prompting at gpt-3.5-ft and gpt-3.5-ft-0 was 77.5% and 75%,\nrespectively.\nFig. 1 | Consistency of different prompts in different models.Detailed information of each model could be found in Table3.\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 2\nFig. 2 | Top 10 consistency.The vertical axis represents the combination of the chosen model and prompt, for example,‘gpt-4-Web-ROT’indicates that the selected model is\ngpt-4-Web, and the prompt is ROT prompting.\nFig. 3 | Consistency in different levels. aStrong; b moderate; c limited; d consensus.\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 3\nModerate level. The consistency of the different prompts in the different\nmodels at the moderate level is shown in Fig.3b. Eight pieces of advice\nwere rated as moderate, with 40 responses for each prompt. According to\nthe multiple comparisons of consistency in gpt-4-Web (30% to 40%),\nthere was no signiﬁcant difference between the groups. According to the\nother models, the consistency of the IO prompting in Bards is 75%.\nLimited level. The consistency of the different prompts in the different\nmodels at the limited level is shown in Fig.3c. Sixteen pieces of advice had\na limited recommendation rating, with 80 responses for each prompt.\nAccording to the multiple comparisons of consistency in gpt-4-Web,\nafter Bonferroni correction, the percentage of patients with a 0-point\ndifference in P-COT prompting (50%) was signiﬁcantly lower than that\nin ROT prompting (75%) and IO prompting (82.5%). In the other\nmodels, all the consistency is lower than 70%.\nConsensus level. The two pieces of advice were recommended upon\nconsensus. Considering the small sample size, no statistical test was\nconducted, and the consistency of different prompts in different models\nis shown in Fig.3d.\nReliability of LLMs\nThe Fleiss kappa values of the 4 prompts in the 9 models are shown in\nTable 1. and the values ranged from−0.002 to 0.984. Detailed statistical\ndata are shown in Supplementary Table 2.\nThe kappa values for IO prompting in gpt-3.5-ft-0 and gpt-3.5-API-0\nwere nearly 1 (0.982 and 0.984, respectively). In the corresponding scatter\nplots, as shown in Fig.4g, i, points that match the answers with the guide-\nlines fall on the baseline (level difference = 0). A positive difference indicates\nbeing above the baseline, while a negative difference indicates being below it.\nStarting from theﬁrst data point of IO prompting in Fig.4g, i shows that\nalmost every set ofﬁve points lies on a horizontal line. This pattern indicates\nthat the models consistently generate the same responseﬁv et i m e si nar o w .\nIn contrast, the responses in other circumstances exhibit more variability.\nThe kappa of P-COT prompting in response to the gpt-4-API-0 was 0.660.\nThe other kappa values are all lower than 0.6. For the gpt-4-Web, the Fleiss\nkappa results indicate that the reliability of each prompt is fair to moderate\n(0.334 to 0.525). Overall, IO prompting in the gpt-3.5-ft-0 and gpt-3.5-API-\n0 trials demonstrated perfect reliability. P-COT prompting in the gpt-4-\nAPI-0 indicated substantial reliability, and others were moderate or lower.\nInvalid data and corresponding processing measures\nThere were three categories of invalid data: Category A: theﬁnal rating was\nnot provided. Category B: the rating was not an integer. All the invalid data\nwere processed according to the invalid data procedure21.I nt h ec a l c u l a t i o n\nof Fleiss kappa, all invalid data in category A are considered to constitute an\nindependent classiﬁcation, and the invalid data in category B are treated as\ndifferent classiﬁcations based on the values (if the rating is‘2o r3’,i ti s\nrecorded as 2.5) generated by the LLMs. In the creation of the scatter plot\n(Fig. 4), invalid data from category A were labeled missing data. Notably, a\nsigniﬁcant amount of invalid data from category A was observed in multiple\ndatasets; for instance, 81.1% of the responses to 0-COT prompting were\nrecorded in gpt-3.5-API-0. Conversely, the proportion of invalid data in gpt-\n4-Web was relatively small (a total of 14 out of 680 across all four prompts).\nDiscussion\nThe results of this study suggested that prompt engineering may change the\naccuracy of LLMs in answering medical questions. Additionally, LLMs do\nnot always provide the same answer tot h es a m em e d i c a lq u e s t i o n s .T h e\ncombination of ROT prompting and gpt-4-Web outperformed the other\ncombinations in providing professional OA knowledge consistent with\nclinical guidelines.\nWe have summarized the current performance of LLMs in diagnosing\npatients, querying patients, and examining patients within clinical medicine\nin Supplementary Table 3. Indeed, GPT-4 has shown superior results and\nexhibited superior performance compared to both GPT-3.5 and Bard in the\nﬁeld of clinical medicine\n16,22– 29. In our study, by combining the performance\nof the four types of prompts across different models, as shown in Fig.1,g p t -\n4-Web, also known as ChatGPT-4, demonstrated a more balanced and\nprominent performance.\nPrevious research has primarily assessed GPT-4 through web inter-\nfaces in clinical medicine. The study of Fares et al.\n30 accessed GPT-4 via the\nAPI and set different temperatures (temperature = 0, 0.3, 0.7, 1) and found\nthat the model set at a temperature of 0.3 performed better in answering\nophthalmology-related questions. Our study revealed differences in con-\nsistency and reliability between GPT-4 scores accessed via the web and\nGPT-4 scores accessed through the API. In our study, we found that among\nthe gpt-4-Web products with speciﬁc parameter settings, gpt-4-API with a\ntemperature of 0 (gpt-4-API-0) and gpt-4-API with a temperature of 1, gpt-\n4-Web exhibited the most prominent performance. This indicated that\nadjusting the internal parameters of LLMs during different tasks can alter\nthe performance of LLMs.\nTable 1 | Fleiss Kappa of different prompts in different models\nModel Prompt Fleiss Kappa 95% CI\ngpt-4-Web IO 0.525 0.523 0.527\n0-COT 0.450 0.448 0.452\nP-COT 0.334 0.332 0.337\nROT 0.467 0.465 0.470\ngpt-4-API IO 0.288 0.286 0.290\n0-COT 0.067 0.065 0.069\nP-COT 0.331 0.330 0.333\nROT 0.205 0.203 0.206\ngpt-4-API-0 IO 0.525 0.523 0.526\n0-COT 0.285 0.283 0.287\nP-COT 0.660 0.658 0.661\nROT 0.451 0.449 0.453\nBard IO 0.374 0.372 0.376\n0-COT 0.355 0.353 0.357\nP-COT 0.323 0.321 0.326\nROT 0.180 0.178 0.182\ngpt-3.5-Web IO 0.409 0.407 0.411\n0-COT −0.002 −0.004 0.000\nP-COT 0.276 0.274 0.278\nROT 0.016 0.014 0.018\ngpt-3.5-API IO 0.188 0.186 0.190\n0-COT 0.004 0.002 0.006\nP-COT 0.031 0.029 0.033\nROT 0.014 0.012 0.016\ngpt-3.5-API-0 IO 0.984 0.983 0.986\n0-COT 0.461 0.459 0.464\nP-COT 0.533 0.531 0.535\nROT 0.581 0.578 0.583\ngpt-3.5-ft IO 0.162 0.160 0.164\n0-COT 0.021 0.020 0.023\nP-COT 0.065 0.063 0.067\nROT 0.033 0.032 0.035\ngpt-3.5-ft-0 IO 0.982 0.980 0.984\n0-COT 0.412 0.410 0.414\nP-COT 0.355 0.353 0.356\nROT 0.398 0.396 0.400\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 4\nT oo u rk n o w l e d g e ,t h e r eh a sn o ty e tbeen research exploring the impact\nof ﬁne-tuning ChatGPT on clinical medicine. For other LLMs, in the study\nby Karan et al.8, Med-PaLM, which is a version of Flan-PaLM that has been\ninstruction prompt-tuned and is not currently publicly available, was\nevaluated by a panel of clinicians.They found that 92.6% of the answers\ngenerated by Med-PaLM were consistent with the scientiﬁc consensus. For\nour study, in theﬁne-tuning versions of GPT-3.5, where IO prompting is\nused as the input part of the dataset duringﬁne-tuning, the 2ﬁne-tuning\nmodels achieve consistencies of 55.3% and 45.9% when IO prompting is\nused for inputs. However, when other types of prompts are used as inputs in\nthe ﬁne-tuning models, the performance deteriorates (22.4% to 34.1%).\nFurthermore,ﬁne-tuning could not ensure that GPT-3.5 fully understood\nthe rationale behind each piece of advice in the dataset. As a result, answers\ncan be generated with incorrect rationales. The less-than-idealﬁne-tuning\nresults in our study might be due to the setup of theﬁne-tuning dataset, the\ncapability of the base model or theﬁne-tuning methods employed by\nOpenAI.\nOverall, the comparison of nine LLMs indicates that parameter settings\nand ﬁne-tuning, along with prompt engineering, could inﬂuence the per-\nformance of LLMs. Improving LLMs inclinical medicine requires a com-\nbination of multiple approaches, accounting for various factors, including\nmodel architecture, parameter settings, andﬁne-tuning techniques.\nSupplementary Table 4 brieﬂy summarizes the current application of\ndifferent types of prompts in clinical medicine. Studies on the topic of\nprompt engineering in clinical medicine are limited, and most studies pri-\nmarily apply prompt engineering techniques directly\n31 or provide an\noverview of prompt engineering14,32,33 in clinical medicine. The study of\nKaran et al.8 did not signiﬁcantly differ between the COT and few-shot\nprompting strategies. However, self-consistency prompting, particularly in\nthe context of the MedQA dataset, showed an improvement of more than\n7%. Conversely, self-consistency led to a decrease in performance for the\nPubMedQA dataset. Wan et al.\n31 demonstrated that few-shot prompting\nand zero-shot prompting exhibit different levels of sensitivity and speciﬁcity\nin converting symptom narratives using the ChatGPT-4.\nThis study, built upon previous research, further indicated that prompt\nengineering could inﬂuence the performance of LLMs in clinical medicine.\nBased on current theories of prompt engineering, we developed a new\nprompting framework, ROT prompting, which demonstrated good per-\nformance on the gpt-4-Web. As shown in Fig.2, ROT prompting achieved\nthe highest consistency rate. According to our subgroup analysis, compared\nto those of the other three types of prompts within gpt-4-Web, the ROT\nprompting performed more evenly and prominently. In terms of‘strong’\nintensity, ROT prompting is superior to IO prompting, and it is not sig-\nniﬁcantly inferior to other prompts at other levels. In contrast, although\nanswers of P-COT prompting at‘strong’intensity are better than those of IO\nprompting, its performance at the‘limited’ intensity level is signiﬁcantly\nworse than that of other prompts.\nHowever, ROT promoting is not necessarily the best prompt for other\nLLMs. For instance, for ﬁve versions of GPT-3.5, except for P-COT\nprompting being the best prompt for GPT-3.5-Web, the best prompt for\nother versions was IO prompting. For Bard, the best prompt was 0-COT.\nThis indicated that we could try different prompting strategies to obtain the\nbest responses.\nThe ROT prompting asked LLM to return to previous thoughts and\nexamine if they were appropriate, which may improve the robustness of the\nanswer. Furthermore, the ROT-based design can minimize the occurrence\nof egregiously incorrect answers from the gpt-4-Web. For instance,\nregarding a ‘strong’ level suggestion, “Lateral wedge insoles are not\nrecommended for patients with knee osteoarthritis.” ROT prompting\nprovided four‘strong’answers and one‘moderate’answer inﬁve responses.\nInitially, in this‘moderate’response (Supplementary Note 1), two“experts”\nprovided “limited” answers, and one“expert” answered “moderate”.A f t e r\n“discussion”,a l l“experts” agreed on a‘moderate’ recommendation. The\nﬁnal reason was that even though there was high-quality evidence to support\nt h ea d v i c e ,t h e r em i g h ts t i l lb es l i g h tp o t e n t i a lb e n eﬁts for some individuals.\nNotably, the reasons given by the two experts for“limiting” seem to be more\nin line with the statement“Lateral wedge insoles are recommended for\npatients with knee osteoarthritis.” This implies that these two“experts” did\nnot fully understand the medical advice correctly, as“Expert C” mentioned\nin stepﬁve: “Observes that the results are somewhat mixed, but there’sa\ngeneral agreement that the beneﬁts, if any, from lateral wedge insoles are\nlimited.” However, after the“discussion”,t h eﬁnal revised recommendation\nand reason were deemed acceptable. Referring to the application of TOT in\nthe 24-point game13, the prompt designed in the style of TOT as well as the\nFig. 4 | Scatter plots of each answer. agpt-4-Web; b gpt-4-API; c gpt-4-API-0; d Bard; e gpt-3.5-Web; f gpt-3.5-API; g gpt-3.5-API-0; h gpt-3.5-ft; i gpt-3.5-ft-0.\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 5\nROT prompting in this study could offer more possibilities at every step of\nthe task, and LLM could be asked to return to previous thoughts, aiming to\ninduce LLM to generate more accurate answers.\nIn future studies, considering the varying effectiveness of the ROT\nprompting across different models, a potential direction might involve\noptimizing it based on model differenc e s .I nt h ef u t u r e ,t h ed e s i g no ft h e\nROT prompting needs to be more closely aligned with different clinical\nscenarios. For instance, setting up roles with various professional back-\ngrounds in disease diagnosis and treatment could provide more specialized\nadvice. Additionally, incorporating different clinical application scenarios,\nsuch as testing and improving the effectiveness of ROT prompting in disease\ndiagnosis and patient treatment plan formulation, will be crucial.\nThree previous studies\n6,7,34 brieﬂy described reliability. Yoshiyasu et al.7\nreproduced inaccurate responses only. Walker et al.6 reported that the\ninternal concordance of the provided information was complete (100%)\naccording to human evaluation. In the study of Fares et al.34,t h ea u t h o r s\nrepeated the experiments 3 times and extracted the responses from\nChatGPT-3.5; theκ values were 0.769 for the BCSC set and 0.798 for the\nOphthoQuestions set.\nIn this study, reliability was investigated by asking LLMs the same\nquestionﬁve times, and according to the results of our study, it is suggested\nthat LLMs cannot always provide consistent answers to the same medical\nquestion (Table1 and Fig.4). The study used the strength of recommen-\ndation of the AAOS as an evaluation standard and found that LLMs always\nprovide different strengths for the same advice in multiple answers. Only IO\nprompting in gpt-3.5-API-0 and gpt-3.5-ft-0, both of which were set at a\ntemperature of 0, demonstrated perfect reliability.\nBased on the description on the ofﬁcial OpenAI website regarding the\nendpoint of Audio (https://platform.openai.com/docs/api-reference/audio/\ncreateTranscription), “The sampling temperature, between 0 and 1, affects\nrandomness. Higher values, such as 0.8, increase randomness, while lower\nvalues, such as 0.2, make outputs more focused and deterministic. A setting\nof 0 allows the model to automaticallyadjust the temperature based on log\nprobability until certain thresholds are met.” We hypothesize that this\nmechanism also applies to the endpoint of Chat (https://platform.openai.\ncom/docs/api-reference/chat/object), although this is not explicitly stated in\nthe corresponding section. The speciﬁc thresholds for GPT-3.5 and GPT-4\nmight differ, and the prompts could inﬂuence these thresholds, as consistent\nresponses were observed only in the two groups corresponding to the IO\nprompting in gpt-3.5-API-0 and gpt-3.5-ft-0. Therefore, it is recommended\nthat LLMs be asked the same questions several times to obtain more\ncomprehensive answers and that they keep asking the ChatGPT-4 the same\nquestion until it does not provide any additional information.\nIn future research, within the clinical application of LLMs, particularly\nfrom the patient’s perspective, OA is a common and frequently occurring\ncondition associated with various treatment methods. Hence, prompt\nengineering could play a crucial role in guiding patients to ask medical\nquestions correctly, potentially enhancing patient education and answering\ntheir queries more effectively. Onthe side of doctors, our study demon-\nstrated that the ROT developed for the web version of the gpt-4 generated\nbetter results. However, multiple variables, such as different model archi-\ntectures and parameters, can complicate outcomes. Therefore, we believe\nthat prompt engineering should be combined with model development,\nparameter adjustment, andﬁne-tuning techniques to develop specialized\nLLMs with medical expertise, whichcould assist physicians in making\nclinical decisions.\nThe application of prompt engineering faces several challenges in the\nfuture. First, there is the issue of therobustness of prompts. Prompts based\non the same framework may yield different answers due to minor changes in\na few words\n35. Patients or doctors might receive different answers even when\nusing prompts from the same framework. Second, prompt engineering\nperformance depends on the inherent capabilities of the LLM itself. Prompts\neffective for one model may not be suitable for another. Guidelines for\nprompt engineering tailored for patients and doctors need to be developed\naccording to the corresponding requirements. Overall, future related studies\nshould examine the applicability androbustness of prompts and formulate\nrelevant guidelines.\nImportantly, our research does notinclude real-time interactions or\nvalidations with healthcare professionals or patients. However, our\napproach to data collection relies on nonhuman subjective scoring, objec-\ntively assessing the consistency and reliability of LLM responses. Further-\nmore, the study was designed based on expected answers derived from\nguidelines and lacked prospective validation. Nevertheless, we acknowledge\nthat thisﬁeld remains underexplored and that a multitude of techniques\nwarrant further investigation. Our study represents only a preliminary foray\ninto this vast domain.\nGiven these limitations, future research should aim to develop both an\nobjective benchmark evaluationframework for LLM responses and a\nh u m a ne v a l u a t i o nf r a m e w o r k\n8 involving healthcare professionals and\npatients.\nOur work represents an initial step into this expansive domain, high-\nlighting the importance of continuing research to reﬁne and enhance the\napplication of large language models in healthcare. Future studies should\nfurther explore various methodologies to improve the effectiveness and\nreliability of LLMs in medical settings.\nThis study revealed that different prompts had variable effects across\nvarious models, and gpt-4-Web with ROT prompting had the highest\nconsistency. An appropriate prompt may improve the accuracy of responses\nto professional medical questions. Moreover, it is advisable to pose the input\nquestions multiple times to gather more comprehensive insights, as\nresponses may vary with each inquiry. In the future of AI healthcare\ninvolving LLMs, prompt engineering will serve as a crucial bridge in com-\nmunication between LLMs and patients, as well as between LLMs and\ndoctors.\nMethod\nDisease selection and evidence-based CPG selection\nThe American Academy of Orthopedic Surgeons (AAOS) evidence-based\nclinical practice guidelines (CPGs) for OA were used to test the consistency\nof the answers given by the LLMs. With more than 39,000 members, the\nAAOS is the world’s largest medical association of musculoskeletal\nspecialists\n36, and the OA guidelines provided by the AAOS are supported by\ndetailed evidence and review reports37. The OA guidelines include a detailed\nevidence assessment system based on research evidence and cover various\nmanagement recommendations, including drug treatment for OA, physical\ntherapy, and patient education. It is an authoritative and comprehensive\nguide with detailed content. More detailed information can be found in the\ncomplete version of the OA guidelines\n38.\nPrompt design\nBased on the current application of prompting engineering in computer\nscience and the task of this study, four types of prompts were applied for this\nstudy: IO prompting, 0-COT prompting, P-COT prompting and ROT\nprompting. These types of prompts were developed to test the compliance of\nLLMs’answers regarding the AAOS guidelines and to assess the reliability of\nthe answers in repeated requests. LLMs were tasked with generating an\nanswer that included the rating score as theﬁnal output.\nA brief illustration and examples of each prompt type are shown in\nFig. 5 and Table2. For the detailed content of the four prompts, please refer\nto Supplementary Table 5.\nModel setting\nWe utilized a total of 9 LLMs, the details of which are shown in Table3.T h e\ndefault web versions of GPT-4, GPT-3.5 and Bard were accessed via web\ninterfaces, while other LLMs were accessed through the Application Pro-\ngramming Interface (API). Theﬁne-tuning and calling of an API were\nconducted as described in the OpenAI platform. For theﬁne-tuning data,\nthe IO prompting and the rationale of each advice in AAOS were used to\nform theﬁne-tuning data, and all theﬁne-tuning data can be found in\nSupplementary Table 6.\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 6\nFig. 5 | The schematic diagram of four prompt words guiding LLMs to output answers. aIO prompting；b 0-COT prompting;c P-COT prompting;d ROT prompting.\nThe design of thisﬁgure was inspired by the study of Yao et al.13, and the copyright is authorized under the CC BY 4.0 DEED (https://creativecommons.org/licenses/by/4.0/).\nTable 2 | Deﬁnition and explanation of each prompt\nPrompt De ﬁnition Brief explanation\nInput-output (IO) prompting Input the instruction directly Consider the following medical advice:\n<insert the advice>\nRate the medical advice using the following criteria, and make a\nselection from integer 1,2,3,4\n<insert the criteria>\n0-shot-Chain of thought (0-\nCOT) prompting\nUse “Think it step by step” on the base of IO to steer the LLM\ncomplete reasoning.\n<Describe your task>\nComplete the task above step by step.\nPerformed-Chain of thought\n(P-COT) prompting\nBreak down the task into different steps to perform what rea-\nsoning processes need to be conducted by the LLM.\n<Describe your task>\nComplete the task above step by step:\nStep 1… ..\nStep 2… ..\n……\nShow your work of each step.\nReﬂection of thoughts (ROT)\nprompting\nBreak down the task into different steps and steer the LLM to\nbacktrack previous steps by let the LLM simulates the mode of\ndiscussion.\n<Describe your task>\nImagine 3 medical experts are completing the task above step by\nstep: Step 1 to Step X: Each expert independently completes rea-\nsoning.\nAfter step X: Experts discuss together and backtrack previous steps\nand ﬁnally reach agreement.\nTable 3 | Details of included models\nModel name Version name Details\nGPT-4 gpt-4-Web The default web version of GPT-4 and the release notes were on July 20, 2023.\ngpt-4-API gpt-4-0613 with parameters when assessing API (temperature = 1).\ngpt-4-API-0 gpt-4-0613 with setting temperature as 0 when assessing API.\nBard Bard Assess through web and the release notes were on October 30, 2023.\nGPT-3.5 gpt-3.5-Web Assess through web and the release notes were on October 17, 2023.\ngpt-3.5-API gpt-3.5-turbo-0613 with default parameters when assessing API.\ngpt-3.5-API-0 gpt-3.5-turbo-0613 with setting temperature as 0 when assessing API.\ngpt-3.5-ft gpt-3.5-turbo-0613 with ﬁne-tuning techniques and default parameters (temperature=1) when assessing API.\ngpt-3.5-ft-0 gpt-3.5-ft with setting temperature as 0 when assessing API.\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 7\nData collection and data processing\nEach item from the AAOS guidelines was reformatted as an instruction for\nassessing the strength of the recommendation to different LLMs, and the\nresults showed the level of recommendation. The AAOS’s level of recom-\nmendation was based on the level of evidence, and any upgrade or down-\ngrade of the recommendation strength based on evidence to the decision\nframework requires supermajority approval by the AAOS working group\n36.\nThe answers provided by the LLMs were compared to those of the AAOS\nguidelines, and each level provided by the LLMs was offset from the cor-\nresponding AAOS level, as shown in Table4.\nWe extracted 34 items (Supplementary Table 7) from the evidenced-\nbased OA CPG provided by the AAOS. Each piece of advice was asked 5\ntimes. When assessing via web interfaces, each question was asked in a\nseparate dialog box to avoid the inﬂuence of context on the answers. When\nassessing the API, the process was completed by means of codes in Python\n(version 3.9.7). Finally, each prompt was asked a total of 170 times, and the\nfour prompts were asked a total of 680 times for each LLM. The answer to\neach question was recorded. Answers that did not follow the instructions of\nthe prompt were considered invalid data.\nOutcome measures and statistical analysis\nStatistical analysis was conducted using SPSS 23.0 (IBM, New York, NY,\nUSA) and Python (version 3.9.7). Consistency and reliability were used to\nevaluate the performance of the LLMs. Consistency is deﬁned as the pro-\nportion of instances where the level gap equals zero. To compare con-\nsistency, we grouped the categorical data collected into a category with a\nrank difference of 0 and another with a rank difference not equal to 0 and\nthen conducted the chi-square test, Fisher’se x a c tt e s t ,o rY a t e s’sc o n t i n u i t y\ncorrection\n39,40. Bonferroni correction was used for multiple comparisons41.\nReliability refers to the repeatability of responses to the same questions and\nwas assessed using the Fleiss kappa test. The values of Fleiss kappa, as\ninterpreted based on previous studies\n42,43, are considered to indicate no\nreliability (<0.01), slight reliability (0.01– 0.2), fair reliability (0.21– 0.40),\nmoderate reliability (0.41– 0.60), substantial reliability (0.61– 0.80), or almost\nperfect reliability (0.81– 1.00). Invalid data were treated according to invalid\ndata procedures in the statistical analysis21.\nReporting summary\nFurther information on research design is available in the Nature Research\nReporting Summary linked to this article.\nData availability\nThe original answers for the gpt-4-Web can be found in the supplementary\nﬁles of the preprint version of this article athttps://www.researchsquare.\ncom/article/rs-3336823/v1, and others are available athttps://doi.org/10.\n6084/m9.ﬁgshare.25232381on ﬁgShare.\nCode availability\nThe codes for data analysis and API calls are available athttps://doi.org/10.\n6084/m9.ﬁgshare.25232381on ﬁgShare.\nReceived: 8 September 2023;Accepted: 5 February 2024;\nReferences\n1. Lee, P., Bubeck, S. & Petro, J. Beneﬁts, Limits, and Risks of GPT-4 as\nan AI Chatbot for Medicine.N. Engl. J. Med.388, 1233–1239 (2023).\n2. Waisberg, E. et al. GPT-4: a new era of artiﬁcial intelligence in\nmedicine. Ir. J. Med. Sci.192, 3197–3200 (2023).\n3. Scanlon, M., Breitinger, F., Hargreaves, C., Hilgert, J.-N. & Sheppard, J.\nChatGPT for digital forensic investigation: The good, the bad, and the\nunknown. Forensic Science International: Digital Investigation (2023).\n4. Kanjee, Z., Crowe, B. & Rodman, A. Accuracy of a Generative Artiﬁcial\nIntelligence Model in a Complex Diagnostic Challenge.JAMA 330,\n78–80 (2023).\n5. Cai, L. Z. et al. Performance of Generative Large Language Models on\nOphthalmology Board Style Questions.Am. J. Ophthalmol.254,\n141–149 (2023).\n6. Walker, H. L. et al. Reliability of Medical Information Provided by\nChatGPT: Assessment Against Clinical Guidelines and Patient\nInformation Quality Instrument.J. Med. Internet Res.25, e47479 (2023).\n7. Yoshiyasu, Y. et al. GPT-4 accuracy and completeness against\nInternational Consensus Statement on Allergy and Rhinology:\nRhinosinusitis. Int Forum Allergy Rhinol.13, 2231–2234 (2023).\n8. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172–180 (2023).\n9. Wang, X. et al. Self-Consistency Improves Chain of Thought\nReasoning in Language Models. Published as a conference paper at\nICLR 2023.https://iclr.cc/media/iclr-2023/Slides/11718.pdf (2023).\n10. Omiye, J. A., Lester, J. C., Spichak, S., Rotemberg, V. & Daneshjou, R.\nLarge language models propagate race-based medicine.NPJ digital\nMed. 6,1 –4 (2023).\n11. Strobelt, H. et al. Interactive and Visual Prompt Engineering for Ad-\nhoc Task Adaptation with Large Language Models.IEEE Trans. Vis.\nComput. Graph. 29, 1146–1156 (2023).\n12. Wei, J. et al. Chain-of-Thought Prompting Elicits Reasoning in Large\nLanguage Models. Preprint at:https://arxiv.org/abs/2201.11903(2023).\n13. Yao, S. et al. Tree of Thoughts: Deliberate Problem Solving with Large\nLanguage Models. Preprint at:https://arxiv.org/abs/2305.10601(2023).\n14. Meskó, B. Prompt Engineering as an Important Emerging Skill for\nMedical Professionals: Tutorial.J. Med. Internet Res.25,\ne50638 (2023).\n15. Fischer, M., Bartler, A. & Yang, B. Prompt tuning for parameter-efﬁcient\nmedical image segmentation.Med. image Anal.91, 103024 (2023).\n16. Toyama, Y. et al. Performance evaluation of ChatGPT, GPT-4, and\nBard on the ofﬁcial board examination of the Japan Radiology\nSociety. Jpn. J. Radiol.42, 201–207 (2023).\n17. Kozachek, D. Investigating the Perception of the Future in GPT-3, -3.5\nand GPT-4. C&C′23: Creativity and Cognition, 282–287 (2023).\n18. 2019 Global Burden of Disease (GBD) study, https://vizhub.\nhealthdata.org/gbd-results/ (2019).\nTable 4 | An example of calculating level difference\nInput Output\nConsider the following medical advice:\nDry needling may be used to improve pain and function in patients with knee\nosteoarthritis.\nRate the medical advice using the following criteria, and make a selection from\ninteger 1,2,3,4:\nScore 1 (Consensus): There is no supporting evidence, and the medical advice\nis possibly based on common practice or expert opinion.\nScore 2 (Limited): Supported by evidence from one or more low-quality studies\nwith consistentﬁndings, or evidence from a single moderate-quality study.\nScore 3 (Moderate): Supported by evidence from two or more moderate-quality\nstudies with consistentﬁndings, or evidence from a single high-quality study.\nScore 4 (Strong): Supported by evidence from two or more high-quality studies\nwith consistentﬁndings.\nAs of my knowledge cut-off in September 2021, the use of dry needling for the\nimprovement of pain and function in patients with knee osteoarthritis may be rated as 2\n(Limited). The evidence for this practice comes from a few low to moderate-quality stu-\ndies showing some potential beneﬁts. However, more rigorous, high-quality studies are\nneeded to conﬁrm theseﬁndings and establish a solid consensus in the medical com-\nmunity.\nPlease consult with a healthcare professional for updated information or advice as the\nstate of medical knowledge and research is constantly evolving.\nRecommendation level of AAOS: 2\nLevel difference: 2-2= 0\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 8\n19. Sa ﬁri, S. et al. Global, regional and national burden of osteoarthritis\n1990-2017: a systematic analysis of the Global Burden of Disease\nStudy 2017.Ann. Rheum. Dis.79, 819–828 (2020).\n20. Perruccio, A. V. et al. Osteoarthritis Year in Review 2023: Epidemiology &\ntherapy.Osteoarthr. Cartil.S1063-4584, 00990–00991 (2023).\n21. Pigott, T. D. A Review of Methods for Missing Data.Educ. Res. Eval.7,\n353–383 (2001).\n22. Koga, S., Martin, N. B. & Dickson, D. W. Evaluating the performance of\nlarge language models: ChatGPT and Google Bard in generating\ndifferential diagnoses in clinicopathological conferences of\nneurodegenerative disorders.Brain Pathol., e13207,https://doi.org/\n10.1111/bpa.13207 (2023).\n23. Lim, Z. W. et al. Benchmarking large language models’performances\nfor myopia care: a comparative analysis of ChatGPT-3.5, ChatGPT-\n4.0, and Google Bard.EBioMedicine 95, 104770 (2023).\n24. Fraser, H. et al. Comparison of Diagnostic and Triage Accuracy of Ada\nHealth and WebMD Symptom Checkers, ChatGPT, and Physicians\nfor Patients in an Emergency Department: Clinical Data Analysis\nStudy. JMIR mHealth uHealth11, e49995 (2023).\n25. Ali, R. et al. Performance of ChatGPT and GPT-4 on Neurosurgery\nWritten Board Examinations.Neurosurgery 93, 1353–1365 (2023).\n26. Fowler, T., Pullen, S. & Birkett, L. Performance of ChatGPT and Bard\non the ofﬁcial part 1 FRCOphth practice questions.Br. J. Ophthalmol.,\nbjo-2023-324091, https://doi.org/10.1136/bjo-2023-324091 (2023).\n27. Passby, L., Jenko, N. & Wernham, A. Performance of ChatGPT on\ndermatology Specialty Certiﬁcate Examination multiple choice questions.\nClin. Exp. Dermatol., llad197,https://doi.org/10.1093/ced/llad197(2023).\n28. Smith, J., Choi, P. M. & Buntine, P. Will code one day run a code?\nPerformance of language models on ACEM primary examinations and\nimplications. Emerg. Med. Australas.35, 876–878 (2023).\n29. Pushpanathan, K. et al. Popular large language model chatbots’\naccuracy, comprehensiveness, and self-awareness in answering\nocular symptom queries.iScience 26, 108163 (2023).\n30. Antaki, F. et al. Capabilities of GPT-4 in ophthalmology: an analysis of\nmodel entropy and progress towards human-level medical question\nanswering. Br J Ophthalmol, bjo-2023-324438,https://doi.org/10.\n1136/bjo-2023-324438 (2023).\n31. Wei, W. I. et al. Extracting symptoms from free-text responses using\nChatGPT among COVID-19 cases in Hong Kong.Clin. Microbiol\nInfect. https://doi.org/10.1016/j.cmi.2023.11.002 (2023).\n32. Kleinig, O. et al. How to use large language models in ophthalmology:\nfrom prompt engineering to protecting conﬁdentiality. Eye https://doi.\norg/10.1038/s41433-023-02772-w (2023).\n33. Akinci D\n’Antonoli, T. et al. Large language models in radiology:\nfundamentals, applications, ethical considerations, risks, and future\ndirections. Diagnostic Interventional Radiol.https://doi.org/10.4274/\ndir.2023.232417 (2023).\n34. Antaki, F., Touma, S., Milad, D., El-Khoury, J. & Duval, R. Evaluating\nthe Performance of ChatGPT in Ophthalmology: An Analysis of Its\nSuccesses and Shortcomings.Ophthalmol. Sci.3, 100324 (2023).\n35. Zhu, K. et al. PromptBench: Towards Evaluating the Robustness of\nLarge Language Models on Adversarial Prompts. Preprint athttps://\narxiv.org/abs/2306.04528v4 (2023).\n36. Newsroom. AAOS Updates Clinical Practice Guideline for\nOsteoarthritis of the Knee, https://www.aaos.org/aaos-home/\nnewsroom/press-releases/aaos-updates-clinical-practice-\nguideline-for-osteoarthritis-of-the-knee/ (2021).\n37. Osteoarthritis of the Knee.Clinical Practice Guideline on Management of\nOsteoarthritis of the Knee.3 r de d ,https://www.aaos.org/quality/quality-\nprograms/lower-extremity-programs/osteoarthritis-of-the-knee/(2021).\n38. The American Academy of Orthopaedic Surgeons Board of Directors.\nManagement of Osteoarthritis of the Knee (Non-Arthroplasty)https://\nwww.aaos.org/globalassets/quality-and-practice-resources/\nosteoarthritis-of-the-knee/oak3cpg.pdf (2019).\n39. Goldstein, M., Wolf, E. & Dillon, W. On a test of independence for\ncontingency tables.Commun. Stat. Theory Methods5,\n159–169 (1976).\n40. Gurcan, A. T. & Seymen, F. Clinical and radiographic evaluation of\nindirect pulp capping with three different materials: a 2-year follow-up\nstudy. Eur. J. Paediatr. Dent.20, 105–110 (2019).\n41. Armstrong, R. A. When to use the Bonferroni correction.Ophthalmic\nPhysiol. Opt.34, 502–508 (2014).\n42. Pokutnaya, D. et al. Inter-rater reliability of the infectious disease\nmodeling reproducibility checklist (IDMRC) as applied to\nCOVID-19 computational modeling research.BMC Infect. Dis.23,\n733 (2023).\n43. Zapf, A., Castell, S., Morawietz, L. & Karch, A. Measuring inter-rater\nreliability for nominal data– which coefﬁcients and conﬁdence\nintervals are appropriate?BMC Med. Res. Methodol.16, 93 (2016).\nAcknowledgements\nThis work was funded by the Key Research and Development Fund of\nSichuan Science and Technology Planning Department. Grant number\n2022YFS0372 (received by J.L.) and the Youth Research Fund of Sichuan\nScience and Technology Planning Department. Grant number\n23NSFSC4894 (received by X.C.).\nAuthor contributions\nLi Wang and Xi Chen are the main designers and executors of the study and\nmanuscript. They have accessed and veriﬁed the data and share theﬁrst\nauthorship. Jian Li is responsible for proposing revisions of the manuscript\nand the decision to submit the manuscript. Qi Li contributed to the study in\nmanaging and supervising the revision work and providing critical feedback\nduring the major revision process. XiangWen Deng and HaoWen are\nconsultants of knowledge related about computer science. MingKe You and\nWeiZhi Liu participate in the drafting of the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-024-01029-4\n.\nCorrespondenceand requests for materials should be addressed to Qi Li or\nJian Li.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s41746-024-01029-4 Article\nnpj Digital Medicine|            (2024) 7:41 9"
}