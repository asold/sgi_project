{
    "title": "GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification.",
    "url": "https://openalex.org/W3157351319",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2098142281",
            "name": "Haoyuan Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095600869",
            "name": "Chen Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108653549",
            "name": "Xiaoyan Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102880302",
            "name": "Weiming Hu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100593492",
            "name": "Yixin Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2129794430",
            "name": "Wanli Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2155679603",
            "name": "Changhao Sun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2118837791",
            "name": "Yudong Yao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2006254856",
            "name": "Marcin Grzegorzek",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2107166114",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2789541106",
        "https://openalex.org/W2531409750",
        "https://openalex.org/W2770331144",
        "https://openalex.org/W2965191493",
        "https://openalex.org/W1549358575",
        "https://openalex.org/W3119997354",
        "https://openalex.org/W2769806078",
        "https://openalex.org/W2912993657",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3081006013",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2460937040",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2622826443",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2826389836",
        "https://openalex.org/W1983356840",
        "https://openalex.org/W3131151352",
        "https://openalex.org/W2963207607",
        "https://openalex.org/W3009225866",
        "https://openalex.org/W3027869849",
        "https://openalex.org/W2333126212",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2340742421",
        "https://openalex.org/W1606540526",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W437886722",
        "https://openalex.org/W10603447",
        "https://openalex.org/W2612445135",
        "https://openalex.org/W2243397390",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2043291249",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3172509117",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W2404139044",
        "https://openalex.org/W2963656735",
        "https://openalex.org/W3187418919",
        "https://openalex.org/W2149529367",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W2147800946",
        "https://openalex.org/W2982220924",
        "https://openalex.org/W2907329590",
        "https://openalex.org/W2964253222",
        "https://openalex.org/W2141619730",
        "https://openalex.org/W2344480160",
        "https://openalex.org/W3014102590",
        "https://openalex.org/W2988966271"
    ],
    "abstract": "Existing deep learning methods for diagnosis of gastric cancer commonly use convolutional neural network. Recently, the Visual Transformer has attracted great attention because of its performance and efficiency, but its applications are mostly in the field of computer vision. In this paper, a multi-scale visual transformer model, referred to as GasHis-Transformer, is proposed for Gastric Histopathological Image Classification (GHIC), which enables the automatic classification of microscopic gastric images into abnormal and normal cases. The GasHis-Transformer model consists of two key modules: A global information module and a local information module to extract histopathological features effectively. In our experiments, a public hematoxylin and eosin (H&E) stained gastric histopathological dataset with 280 abnormal and normal images are divided into training, validation and test sets by a ratio of 1 : 1 : 2. The GasHis-Transformer model is applied to estimate precision, recall, F1-score and accuracy on the test set of gastric histopathological dataset as 98.0%, 100.0%, 96.0% and 98.0%, respectively. Furthermore, a critical study is conducted to evaluate the robustness of GasHis-Transformer, where ten different noises including four adversarial attack and six conventional image noises are added. In addition, a clinically meaningful study is executed to test the gastrointestinal cancer identification performance of GasHis-Transformer with 620 abnormal images and achieves 96.8% accuracy. Finally, a comparative study is performed to test the generalizability with both H&E and immunohistochemical stained images on a lymphoma image dataset and a breast cancer dataset, producing comparable F1-scores (85.6% and 82.8%) and accuracies (83.9% and 89.4%), respectively. In conclusion, GasHisTransformer demonstrates high classification performance and shows its significant potential in the GHIC task.",
    "full_text": "GasHis-Transformer: A Multi-scale Visual Transformer\nApproach for Gastric Histopathological Image Detection\nHaoyuan Chena, Chen Lia,∗, Ge Wangb,∗, Xiaoyan Lic, Md Rahamana,\nHongzan Sunc, Weiming Hua, Yixin Lia, Wanli Liua, Changhao Suna,d,\nShiliang Aia, Marcin Grzegorzeke\naMicroscopic Image and Medical Image Analysis Group, Northeastern University, China\nbDepartment of Biomedical Engineering, Rensselaer Polytechnic Institute, US\ncDepartment of Pathology, China Medical University, China\ndShenyang Institute of Automation, Chinese Academy of Sciences, China\neInstitute of Medical Informatics, University of L¨ ubeck, Germany\nAbstract\nIn this paper, a multi-scale visual transformer model, referred as GasHis-\nTransformer, is proposed forGastric Histopathological Image Detection(GHID),\nwhich enables the automatic global detection of gastric cancer images. GasHis-\nTransformer model consists of two key modules designed to extract global\nand local information using a position-encoded transformer model and a\nconvolutional neural network with local convolution, respectively. A pub-\nlicly available hematoxylin and eosin (H&E) stained gastric histopatholog-\nical image dataset is used in the experiment. Furthermore, a Dropconnect\nbased lightweight network is proposed to reduce the model size and training\ntime of GasHis-Transformer for clinical applications with improved conﬁ-\ndence. Moreover, a series of contrast and extended experiments verify the\n∗Corresponding author:\nEmail addresses: lichen201096@hotmail.com (Chen Li), wangg6@rpi.edu (Ge\nWang)\nPreprint submitted to Pattern Recognition June 9, 2022\narXiv:2104.14528v7  [cs.CV]  8 Jun 2022\nrobustness, extensibility and stability of GasHis-Transformer. In conclu-\nsion, GasHis-Transformer demonstrates high global detection performance\nand shows its signiﬁcant potential in GHID task.\nKeywords: Gastric histropathological image, Multi-scale visual\ntransformer, Image detection\n1. Introduction\nCancer is a malignant tumor that originates from epithelial tissue and is\none of the deadliest diseases, which caused approximately 9.6 million deaths\nin 2018—the highest number since records began in the 1970s. Among all\nthe cancer categories, gastric cancer has the second-highest rate globally in\nterms of morbidity and mortality. Gastric cancer is a collection of abnormal\ncells that form tumors in the stomach. In histopathology, the most common\ntype of gastric cancer is adenocarcinoma, which starts in mucous-producing\ncells in the stomach’s inner layer that invade the stomach wall, inﬁltrating\nthe muscular mucosa and then invade the outer layer. According to World\nHealth Organization statistics, about 800,000 people die due to cancer every\nyear [1]. Therefore, medical staﬀ needs to diagnose gastric cancer accurately\nand eﬃciently.\nThe diagnosis of gastric cancer is performed by carefully examining Hema-\ntoxylin and Eosin (H&E) sections by pathologists under a microscope. This\nconventional process is time-consuming and subjective. Because of these\nshortcomings, pathologists face diﬃculties with accurate screening and diag-\nnosis of gastric cancer. Thus, computer-aided diagnosis (CAD) that began in\nthe 1980s can overcome these shortcomings by making diagnostic decisions\nwith improving eﬃciency. CAD aims to improve medical doctors’ examina-\n2\ntion quality and eﬃciency by image processing, pattern recognition, machine\nlearning, and computer vision methods [2]. Currently, the most widespread\napplication of CAD is cancer global detection, which is implemented by image\nclassiﬁcation methods in computer vision [3].\nWith the advent of artiﬁcial intelligence, deep learning has become the\nmost extensive and widely used method for CAD [2]. Deep learning has\nbeen proved successful in many research ﬁelds, such as data mining, natural\nlanguage processing and computer vision. It enables a computer to imi-\ntate human activities, solve complex pattern recognition problems and make\nexcellent progress in artiﬁcial intelligence-related techniques. Convolutional\nNeural Network (CNN) models are the dominant type of deep learning that\ncan be applied to many computer vision tasks. However, there are some\nshortcomings of CNN models, one of which is that CNN models do not han-\ndle global information well. In contrast, the novel Visual Transformer (VT)\nmodels applied in the ﬁeld of computer vision can extract more abundant\nglobal information. In medicine, the composition of histopathological images\nare complex, with some abnormal images having a large portion of abnormal\nsections and some having a tiny portion of abnormal sections. Therefore, the\nmodel used for histopathological image global detection tasks must have a\nstrong ability to extract global and local information. Considering the facts\nof CNN and VT models, a hybrid model has been heuristically proposed for\nGastric Histopathological Image Detection (GHID) tasks, namely GasHis-\nTransformer, to integrate the local and global information into an organic\nwhole (Fig. 1).\nThe whole GasHis-Transformer model comprises two modules: Global\n3\n(a) Train Image (b) Data Augmentation\n (c) Training Process\nGasHis-Transformer\nLIM\nClassification\nModel\n(d) Test Image\n(e) Predict Result\nNormal\nAbnormal\nGIM\n or\nStandard\nModule\nLightweight\nModule\nFigure 1: The architecture of the GasHis-Transformer model.\nInformation Module (GIM) and Local Information Module (LIM). First, fol-\nlowing the idea of BoTNet-50 [4], we have designed GIM to extract abundant\nglobal information to describe a gastric histopathological image as a whole.\nThen, the parallel structure idea of Inception-V3 [5] is followed to obtain\nmulti-scale local information to represent the details of a gastric histopatho-\nlogical image.\nThe contributions of this paper are as follows: Firstly, consid-\nering the advantages of VT and CNN models, GasHis-Transformer model\nintegrates the describing capability of global and local information of VT’s\nand CNN’s. Secondly, in GasHis-Transformer, the idea of multi-scale im-\nage analysis is introduced to describe the details of gastric histopathological\nimages under a microscope. Furthermore, a lightweight module using the\nquantization method [6] and Dropconnect strategy [7] is heuristically pro-\nposed to reduce the model parameter size and training time for clinical ap-\nplications with improved conﬁdence. Finally, GasHis-Transformer not only\nobtains good global detection performance on gastric histopathological im-\nages but also shows an excellent generalization ability on histopathological\nimage staging tasks for other cancers.\n4\n2. Related Work\nThere have been many applications of GHID tasks in the ﬁeld of pattern\nrecognition. Traditional machine learning is an eﬀective method that has\nbeen used for many years [2]. In the study of [8], random forest classiﬁer is\nused to detect 332 global graph features including the mean, variance, skew-\nness, kurtosis and other features extracted from gastric cancer histopatholog-\nical images. In recent years, deep learning methods have become increasingly\nused in GHID tasks [2]. In the study of [9], an improved ResNet-v2 network\nis used to detect images by adding an average pooling layer and a convolu-\ntion layer. In the study of [10], 2166 whole slide images are detected by a\ndeep learning model based on DeepLab-V3 with ResNet-50 architecture as\nthe backbone.\nBesides, there are many potential deep learning methods are possible\nfor GHID tasks, such as AlexNet [11], VGG models [12], Inception-V3 net-\nwork [13], ResNet models [14] and Xception network [15]. Especially, novel\nattention mechanisms show good global detection performance in image de-\ntection tasks, such as Non-local+Resnet [16], CBAM+Resnet [17], SENet+CNN [18],\nGCNet+Resnet [19], HCRF-AM [20] and VT models [21]. VT models are\nmore and more used in image detection ﬁeld [22]. There are two main forms of\nVT models in image detection tasks, that is the pure self-attention structure\nrepresented by Vision Transformer (ViT) [23] and the self-attention struc-\nture combined CNN models represented by BoTNet-50 [4], TransMed [24]\nand LeViT [25]. The biggest advantage of VT models is that they perfectly\nsolve the shortcomings of CNN models, where VT models can better describe\nthe global information of images and have a good ability to extract global\n5\ninformation by introducing an attention mechanism.\n3. GasHis-Transformer\n3.1. Vision Transformer (ViT)\nThe ﬁrst model using a transformer encoder instead of standard convo-\nlution in the computer vision ﬁeld is ViT [23, 26]. An overview of the ViT\nmodel is shown in Fig. 2 (a). Image classiﬁcation using ViT model can be\ndivided into two stages: feature extraction stage and classiﬁcation stage. In\nfeature extraction stage, in order to handle a 2D image as a 1D sequence, 2D\npatch sequence xp ∈RN×(P2×C) is obtained by reshaping the original image\nx ∈RH×W×C. C is the number of image’s channels, ( H ×W) is the size of\neach original image, ( P2) is the size of each image patch, N = HW/P2 is\nthe sum of patch number as the same as input sequence length of the trans-\nformer encoder. The invariant hidden vector size D used in the transformer\ngoes through all layers, where all patches are ﬂattened to D dimensions, and\nD dimensions (patch embeddings) are mapped by a linear projection that\ncan be trained. To retain positional information, the sequence of embedding\nvectors combines standard 1D position embedding, and patch embeddings\nare selected to be the input of the transformer encoder.\nThe transformer encoder is composed of multiple alternative Multi Head\nSelf-Attention (MHSA) blocks [4] and multilayer perceptron (MLP) blocks [21].\nThe structure of the transformer encoder is shown in Fig. 2 (b). Layernorm\n(LN) is used in front of each layer and connected to the following block\nthrough residual connection. MLP block has two network layers connected\nby a non-linear Gaussian error linear units (GELU) activation function. Fi-\nnally, in the classiﬁcation stage, the output features after the feature extrac-\n6\ntion stage are passed through the fully connected layer composed of MLP to\nobtain the classiﬁcation conﬁdence.\nOriginal Image\nLinear Projection of Flattened Patches \nPosition + Patches\nEmbedding\nMLP\nClassification\nTransformer Encoder\n0 1 2 3 4 5 6 7 8 9\nEmbeddings \nPatches\nL×\nLN\nMHSA\n+\nLN\nMLP\n+\n(a) (b)\nFigure 2: The details of ViT model. (a) is an overview of ViT model training process. (b)\nis a structure of transformer encoder. This architecture follows the idea of Fig. 1 in [26].\n3.2. BoTNet\nBoTNet-50 [4] is a VT model which combines ResNet-50 with the MHSA\nlayer. Because the usage of the MHSA layers reduces massive parameters,\nBoTNet-50 is a network with a simple structure and powerful functions. The\narchitecture of BoTNet-50 model compared to ResNet-50 is shown in Fig. 3.\nThe process of the feature extraction blocks of BoTNet-50, which is the same\nas that of ResNet-50, is divided into ﬁve stages which are 1 set of stage c1,\n3 sets of stage c2, 4 sets of stage c3, 6 sets of stage c4 and 3 sets of stage c5.\nSimilarly to the hybrid model of ViT [26], in which input sequence extracted\nfrom CNN models to alter raw image patches, BoTNet-50 remains the model\nof ResNet-50 in advance of stage c4 and using the MHSA layers substitute for\nthe last three 3 ×3 spatial convolutions in stage c5 of the model of ResNet.\nThus, BoTNet-50 obtains the global self attention in 2D feature maps. The\nlatter part is the same as ResNet-50. The average pooling layer and fully\n7\nconnected (FC) layer are used to extract features and obtain classiﬁcation\nresults.\n64 × 112 × 112\nOriginal image\n3 × 224 × 224 256 × 56 × 56 512 × 28 × 28 1024 × 14 × 14 2048 × 7 × 7 1 × 2048\nStage\nResNet-50\nBoTNet-50\nClassificationc1 ×1\n7×7,64,conv\nMax pool\n7×7,64,conv\nMax pool\n1×1,64,conv\n3×3,64,conv\n1×1,256,conv\n1×1,64,conv\n3×3,64,conv\n1×1,256,conv\n1×1,128,conv\n3×3,128,conv\n1×1,512,conv\n1×1,128,conv\n3×3,128,conv\n1×1,512,conv\n1×1,256,conv\n3×3,256,conv\n1×1,1024,conv\n1×1,256,conv\n3×3,256,conv\n1×1,1024,conv\n1×1,512,conv\n3×3,512,conv\n1×1,2048,conv\n1×1,512,conv\nMHSA\n1×1,2048,conv\nc2 ×3 c3 ×4 c4 ×6 c5 ×3\nFigure 3: The architecture of BoTNet-50 and ResNet-50.\nThere are multiple diﬀerences between BoTNet-50 and ViT. The main\ndiﬀerence is that the MHSA of ViT uses standard 2D patch sequence po-\nsition encoding, while BoTNet-50 uses 2D relative position encoding. The\nlatest results [27] show that relative position encoding is more suitable for\nimage classiﬁcation tasks than traditional encoding. A structure of relative\nposition encoding of the MHSA is shown in Fig. 4 (a). There are four sets\nof single-headed attention in each MHSA layer of BoTNet-50. At present,\nthe structure of Fig. 4 (a) only takes one single-headed attention as example.\nFirst, for a given pixel xi,j ∈R , we extract a,b ∈Nk(i,j) from the spatial\nextent k which centered on xi,j. Second, WQ, WK and WV are deﬁned as\nthe learnable transforms and can compute the queries qi,j = WQxi,j, keys\nka,b = WKxa,b and values va,b = WV xa,b, which are linear transformations of\nthe pixels of spatial extent. The content information multiply the queries\nand keys value vectors. Thirdly, Rh and Rw are deﬁned as the separable\nrelative position encodings of height and weight are expressed by row oﬀ-\nset (a−i) and column oﬀset ( b−j). The row oﬀset and column oﬀset are\n8\nshown in Fig. 4 (b) and they are connected with an embedding ra−i and\nrb−j. The row oﬀset and column oﬀset embeddings (position information)\nare connected to form ra−i,b−j. Finally, the content information and position\ninformation are accumulated, and then the spatial-relative attention yi,j of\nthe pixel xi,j is obtained by multiplying the aggregation results with values\nthrough softmax [28] as shown in Eq. 1.\nyi,j =\n∑\na,b∈Nk(i,j)\nsoftmaxa,b(q⊤\ni,jka,b + q⊤\ni,jra−i,b−j)va,b. (1)\nWQ:1×1 Wk:1×1 Wv:1×1\nx\nH×W×d\nq k v\ncontent-content\nH×W×d H×W×d H×W×d\nr\nRwRh\ncontent-position\nSoftmax\nH×1×d 1×W×d\nH×W×d\nH*W× H*W H*W× H*W\nqrT qkT\nH*W× H*W\ny\nH×W×d\n(a) (b)\n-1, -1 -1, 0 -1, 1 -1, 2\n0, -1 0, 0 0, 1 0, 2\n1, -1 1, 0 1, 1 1, 2\nFigure 4: (a) is the structure of relative position encoding of the MHSA. ⊕and ⊗\nexpress sum and matrix multiply respectively. Blue blocks are location encoding. Green\nblocks are content encoding. (b) is is A single example of relative distance computation.\nThe relative distance in the ﬁgure is calculated according to the bright position. Red is\nrow oﬀset and blue is column oﬀset.\nThe number of parameters in the MHSA layer is diﬀerent from that in the\nconvolution layer. The number of parameters in convolution increases at a\nquadratic rate with the increase of spatial extent, while the MHSA layer does\nnot change with the change of spatial extent. When the sizes of input and\noutput are the same, the computational cost of the MHSA layer is far less\nthan that of convolution in the same spatial extent. For example, when the\n9\ninput and output are 128-dimensional, the computational cost of 3 spatial\nextents in the convolution layer is the same as that of 19 spatial extents in\nthe MHSA layer [28]. Therefore, the parameters and computation time of\nBoTNet-50 is less than that of ResNet-50 [4].\n3.3. GasHis-Transformer\nThe GasHis-Transformer model and its lightweight version (LW-GasHis-\nTransformer) are proposed to detect gastric cancer in histopathological im-\nages, shown in Fig. 5. The details of each block in LIM and GIM of GasHis-\nTransformer model are shown in Table 1. GasHis-Transformer applies image\nnormalization to improve the image quality. This operation keeps the global\ninformation, only modiﬁes the pixels to a speciﬁed range to accelerate the\nconvergence of the training model.\nIn Fig. 5(a), normal and abnormal gastric histopathological images are\nused as training data for GasHis-Transformer.\nIn Fig. 5(b),ﬁrst, because of the multi-scale characteristics of histopatho-\nlogical images under the microscope, GasHis-Transformer augments the im-\nages by rotating and mirroring operations. Furthermore, GasHis-Transformer\nmodel applies image normalization to avoid this situation and speed up the\nmodel learning process and image normalization process is deﬁned in Eq. 2.\nINPUTRGB = N(IMGRGB), (2)\nwhere INPUT RGB and IMG RGB represent original image and image which\ninput into LIM and GIM, respectively. N() is the image normalization pro-\ncessing. The shallow color of gastric cancer histopathological images and the\nimplicit boundary characteristics of the nucleus results in poor image qual-\nity. This phenomenon is represented by the fact that any region of the whole\n10\n(a) Training image\nNormal\n Abnormal\n(b) Image pre-processing\nNormal\n Abnormal\nGIM\nConv Layer\n(c) GasHis-Transformer\nLIM\nConv Layer\nBatch Norm\nReLU Layer\nStage c2\nResidual\nModel\nStage c3\nResidual\nModel\nStage c4\nResidual\nModel\nStage c5\nResidual\nModel\n×3\n×4\n×6\n×3\n×3\nMax Pooling\nStage c2 to c4 Residual Model\nConv Layer\nMax Pooling\n×2\nInceptionA ×3\nInceptionB ×1\nInceptionC ×4\nInceptionD ×1\nInceptionE ×2\nMax Pooling\nDropout Layer\nConv Layer\nConv Layer\nConv Layer\nStage c5 Residual Model\nConv Layer\nMHSA\nConv Layer\nFC Layer\nSoftmax Layer\nSaving optimal model \nparameters\n(d) Test image\nNormalAbnormal\nEvaluation\nPrecision\nRecall\nF1-Score\nAccuracy\nNormalization process\nMax Pooling\nGasHis-Transformer\nDropconnect Layer\nFC Layer\nSoftmax Layer\nLW-GasHis-Transformer\nQuantizationClassification \nstage\nFigure 5: The structure of GasHis-Transformer model. (a) are training images including\nnormal and abnormal. (b) is data pre-processing, which uses rotation and mirroring\nmethods for data augmentation of training images. (c) are GasHis-Transformer and its\nlightweight version including the backbone network and the classiﬁcation stage. (d) are\ntest images including normal and abnormal.\nimage has a similar mean and standard deviation. The mean and standard\ndeviation of a image are deﬁned in Eq. 3 and Eq. 4.\nµ(IMGRGB) =\n∑N\ni=1,j=1 IMGRGB(i,j)\nN ×N , (3)\nσ(IMGRGB) =\n√∑N\ni=1,j=1[IMGRGB(i,j) −µ(IMGRGB)]2\nN ×N , (4)\nwhere IMGRGB is an original image with N ×N sizes and IMG RGB(i,j) is\nthe pixel of this image, µ(IMGRGB) and σ(IMGRGB) represent the mean and\nstandard deviation of this image, respectively. According to the theory of\nconvex optimization and data probability distribution, the image is operated\naccording to Eq. 5 to ﬁnally obtain a normalized image with a mean of 0 and\n11\na standard deviation of 1.\nINPUTRGB = IMGRGB −µ(IMGRGB)\nσ(IMGRGB) . (5)\nWhen the input image pixels of all samples are positive, the weights from\nthe same convolution kernel can only increase or decrease simultaneously, and\nReLU layer can be shielded negative weights from convolution layer, resulting\nin a slow learning speed. Each pixel in images using image normalization is\nrelated to the global mean and standard deviation, preserving the image’s\nglobal information and nonlinear features. It enables GasHis-Transformer\nmodel to detect the region of interest faster when training the model, thus\nimproving convergence speed and detection accuracy of the model.\nIn Fig. 5(c), images are used to train the proposed model, and this\nstep is the core of the whole structure. GasHis-Transformer includes two\nparts: Global Information Module (GIM) and Local Information Module\n(LIM). In GIM, GasHis-Transformer follows the idea of BoTNet-50 such that\nconvolution layer in the last residual layer of the ResNet-50 model is replaced\nby the MHSA [4]. GIM retains all the structures before c5 stage of BoTNet-\n50 and 2048-dimensional global features are extracted in the last pooling\nlayer of GIM. In LIM, GasHis-Transformer follows the idea of Inception-\nV3 and carries out a series of modiﬁcations to the traditional model. To\nmatch the standard input of GIM and make the features extracted by GIM\nand LIM in the whole network with the same measurement, LIM modiﬁes the\nstandard input size of Inception-V3 [5] model from 299×299 to 224×224 and\nmodiﬁes the standard output size of every convolution layer and pooling layer\nin GasHis-Transformer. Similar to GIM, 2048-dimensional local features are\nextracted in the last pooling layer of LIM. Compared to GasHis-Transformer,\n12\n32-bit ﬂoating-point GasHis-Transformer parameters are degraded to 16-bit\nﬂoating-point numbers via quantization in the training stage of LW-GasHis-\nTransformer [6]. At the end of GIM and LIM, the global and local features\nare fused to obtain the 4096-dimensional splicing feature as the ﬁnal feature\ntrained.\nIn the classiﬁcation stage, an optimization layer is added to suppress\nthe risk of overﬁtting and retain the global and local information before the\nFC layer. The optimization layer optimizes the model during the training\nand testing stages: (1) In GasHis-Transformer, the optimization layer uses\nDropout. In the testing process, the model with Dropout can be considered\nas performing simultaneous prediction using multiple classiﬁcation networks\nwith shared parameters, which can signiﬁcantly improve the generalizability\nof the classiﬁcation task [29]. (2) In LW-GasHis-Transformer, the optimiza-\ntion layer uses Dropconnect, which is a generalization of Dropout for reg-\nularizing neural networks. Dropconnect replaces Dropout for approximate\nBayesian inference being more capable of extracting uncertainty [30]. Drop-\nconnect discards the weights between hidden layers according to a ﬁxed prob-\nability instead of simply discarding hidden nodes and samples the weights\nof each node with a Gaussian distribution in the testing stage [7]. Dropcon-\nnect can eﬀectively solve problems caused by model quantization. Finally,\nthe fused features go through the FC and Softmax layers to obtain the ﬁnal\nclassiﬁcation conﬁdence.\nIn Fig. 5(d), test images including normal and abnormal are used to\nevaluate the global detection performance of GasHis-Transformer.\n13\nTable 1: The details of each block in GIM and LIM of GasHis-Transformer model.\nGIM LIM\nBlock Output Feature Block Output Feature\nConv(7,7) 112 × 112 × 64 Conv(3,3) 111 × 111 × 32\nBatch Norm 112 × 112 × 64 Conv(3,3) 109 × 109 × 32\nReLU Layer 112 × 112 × 64 Conv(3,3) 109 × 109 × 64\nMax Pooling 56 × 56 × 64 Max Pooling 54 × 54 × 64\nStage c2\n×3 56 × 56 × 256\nConv(1,1) 54 × 54 × 80\nResidual Model Conv(3,3) 52 × 52 × 192\nStage c3\n×4 28 × 28 × 512\nMax Pooling 25 × 25 × 192\nResidual Model Inception A ×3 25 × 25 × 256\nStage c4\n×6 14 × 14 × 1024\nInception B ×1 25 × 25 × 288\nResidual Model Inception C ×4 12 × 12 × 768\nStage c5\n×3 7 × 7 × 2048\nInception D ×1 5 × 5 × 1024\nResidual Model Inception E ×2 5 × 5 × 2048\n4. Experiment Results and Analysis\n4.1. Experimental Settings\n4.1.1. Dataset\nIn this paper, an open-source Hematoxylin and Eosin (H&E) stained gas-\ntric histopathological image dataset(HE-GHI-DS) is used in the experiment\nto evaluate the global detection performance of GasHis-Transformer 1. The\nhematoxylin-stained solution is alkaline, which makes chromatin in the nu-\ncleus and ribosome in cytoplasm purple-blue; Eosin-stained solution is acidic,\nwhich makes the components in the cytoplasm and extracellular matrix red.\nThe images are part of the whole slide images in ‘*.tiﬀ’ format, where they\nare magniﬁed 20 times and the image size is 2048×2048 pixels [31]. HE-GHI-\nDS includes 140 normal images and 560 abnormal images. Some examples of\nnormal and abnormal gastric histopathological images are shown in Fig. 6.\nIn the normal images, the nuclei are stable and arranged regularly and the\n1This dataset is open access on: Sun, C. and Li, C. and Li, Y, Data for hcrf,\nhttps://data.mendeley.com/datasets/thgf23xgy7/2\n14\nnucleo-cytoplasmic ratio is small [2]. On the contrary, in the abnormal im-\nages, the nucleus is abnormally large and irregular in features of dish or\ncrater.\n(a) Normal\n(b) Abnormal\nFigure 6: Normal and abnormal examples in the HE-GHI-DS.\n4.1.2. Data Settings\nDue to the imbalance of the initial training data in HE-GHI-DS, deep\nlearning models only learn the characteristics of one category, leading to low\nclassiﬁcation accuracy and weak generalization ability of models [32]. In\norder to balance the training images, 140 abnormal images are randomly se-\nlected from all 560 abnormal images to match the number of normal images.\nIn the GHID task, GasHis-Transformer equally uses 140 abnormal images\nand 140 normal images. Moreover, the abnormal and normal images in the\ndataset are randomly partitioned into training, validation and test sets with\na ratio of 1: 1: 2. Furthermore, all images are ﬂipped horizontally and\nvertically and rotated 90, 180, 270 degrees to augment the training, valida-\ntion and test datasets to six times. In addition, although some information\nis lost by direct image resize operations, it shows that deep learning net-\nworks are robust to diﬀerent sizes of pathological images in our previous\nstudy [33]. Therefore, all images are resized into 224 ×224 pixels by bilinear\n15\ninterpolation. Unlike some other GHID tasks, because rotation and mirror-\ning operations change the relative positions of cancers in histopathological\nimages, both the validation and test sets are expanded to verify the multi-\nscale generalization ability of the GasHis-Transformer. The data settings and\naugmentation results are shown in Table 2.\nTable 2: Data setting for training, validation and test sets.\nImage Type Training Validation Test Sum\nNormal\nOringin 35 35 70 140\nAugmented 210 210 420 840\nAbnormal\nOringin 35 35 70 140\nAugmented 210 210 420 840\n4.1.3. Hyper-parameter Setting\nGasHis-transformer and LW-GasHis-Transformer are used to train the\ngastric histopathological image dataset for 75 epochs. In each epoch, batch\nsize is set to 16. It uses an approach to train from scratch for the GHID\ntask. AdamW optimizer is used for optimization and its parameters are set\nas: 2 e−3 learning rate, 1e−8 eps, [0.9,0.999] betas and 1e−2 weight decay.\nEspecially, VGGNets are trained at the learning rate of 2e−4. AdamW solves\nthe problem of parameter over-ﬁtting with Adam optimizer by introducing\nL2 regularization terms of parameters in the loss function. It is the fastest\noptimizer for gradient descent speed and training neural networks which are\nused in all models. A learning rate adjustment strategy is used, that is, if\nthe set loss function is not decreased within 15 epochs, the learning rate is\nreduced by ten times. In addition, the ratio of Dropout and Dropconnect\nare both set to 0.5 in the training process.\n16\n4.1.4. Evaluation Criteria\nPrecision (Pre), recall (Rec), F1-score (F1) and accuracy (Acc) are used\nto evaluate the GasHis-Transformer model, where true positive (TP), true\nnegative (TN), false positive (FP) and false negative (FN) are used in the\ndeﬁnition of these four criteria in Table 3.\nTable 3: Criteria and corresponding deﬁnitions for image global detection evaluation.\nCriterion Deﬁnition Criterion Deﬁnition\nPre TP\nTP+FP Rec TP\nTP+FN\nF1 2×TP\n2×TP+FP+FN Acc TP+TN\nTP+TN+FP+FP\n4.2. Evaluation Results of GasHis-Transformer\n4.2.1. Experimental Results\nThe criteria of the GasHis-Transformer and LW-GasHis-Transformer are\ncalculated respectively to determine whether the models converge and have\ngeneralize well. The average confusion matrix of ﬁve randomized experiments\non GasHis-Transformer and LW-GasHis-Transformer is shown in Fig. 7. In\nFig. 7(a), 409 abnormal images and 414 normal images are correctly classiﬁed\ninto the correct categories. Only 11 abnormal images are incorrectly reported\nas normal, and 6 normal image are incorrectly detected as abnormal. Overall,\nPre, Rec, F1 and Acc of the global detection using GasHis-Transformer on the\ntest set are 98.55%, 97.38%, 97.97% and 97.97%, respectively. In Fig. 7(b),\n407 abnormal images and 403 normal images are correctly classiﬁed into\nthe correct categories. Only 13 abnormal images are incorrectly reported as\nnormal, and 17 normal images are incorrectly detected as abnormal. Pre,\nRec, F1 and Acc of the global detection using LW-GasHis-Transformer are\n95.99%, 96.90%, 96.43% and 96.43%, respectively.\n17\n409\n49.69%\n6\n0.81%\n98.55%\n1.45%\n11\n0.31%\n414\n49.29%\n97.41%\n2.59%\n97.38%\n2.62%\n98.57%\n1.43%\n97.97%\n2.02%\nPredicted Class\nAbnormalNormal\nAbnormal Normal\nActual Class\n(a) Confusion matrix using GasHis-Transformer.\n407\n48.45%\n17\n2.02%\n95.99%\n4.01%\n13\n1.55%\n403\n47.98%\n96.88%\n3.12%\n96.90%\n3.10%\n95.95%\n4.05%\n96.43%\n3.57%\nPredicted Class\nActual Class\n(b) Confusion matrix using LW-GasHis-Transformer.\nAbnormalNormal\nAbnormal Normal\nFigure 7: Confusion matrix using GasHis-Transformer and LW-GasHis-Transformer,\nrespectively. Green and red numbers are the percentage of correct and incorrect cases,\nrespectively.\nIn order to explain the performance of GasHis-Transformer in the GHID\ntask and to analyze the causes of misidentiﬁcation, we compress the 4096-\ndimensional feature vectors of each image to a 2-dimensional (2-D) space\nusing the t-SNE method for analysis of the ﬁrst randomized experiment [34].\nThe 2-D vector scatter plots obtained using t-SNE method are shown in\nFig. 8(a), while Fig. 8(b)-(g) show the images represented by diﬀerent posi-\ntions in the scatter plots and their feature maps, respectively.\nFirst, features extracted by GasHis-Transformer model can distinguish\nmost abnormal and normal images as shown in Fig. 8(a). For example, in\nFig. 8(b) and Fig. 8(g), they are taken from clusters of abnormal and normal\nimages, respectively. Most abnormal images have extensive carcinoma areas\nand are highly diﬀerentiated without prominent lumen structures, just like\nFig. 8(b). Most normal images have the lumen structures excluding the\ninterstitium and background, just like Fig. 8(g). Meanwhile, the feature\nmaps in Fig. 8(b) and Fig. 8(g) show that the weights of GasHis-Transformer\n18\ntend to favor cancerous regions and lumen structures for most abnormal and\nnormal images, respectively. It demonstrates the eﬀectiveness of GasHis-\nTransformer model in the GHID task.\nAdditionally, there are a small number of abnormal and normal images\nwith similar 2-D feature vectors, which is the cause of misidentiﬁcation by\nthe GasHis-Transformer model. For example, as in Fig. 8(c)-(f), these four\nimages have 2-D similar feature vectors. Fig. 8(c) only contains a small\nportion of the cancerous regions. The feature maps clearly show that the\nmodel detects many background regions but still accurately detects the can-\ncerous regions, which indicates that the GasHis-Transformer model is robust\nto background information and prefers to detect large contiguous regions.\nFig. 8(d) has a smaller cancerous region than Fig. 8(c). The feature map\nshows that the GasHis-Transformer model, which detects large connected re-\ngions, has diﬃculty detecting the tiny cancerous regions, leading to the ﬁnal\nidentiﬁcation mistake. Fig. 8(e) shows intestinal epithelial metaplasia, which\nis the last normal staging before cancerous. Fig. 8(e) and Fig. 8(d)-(c) are\nalready very similar in visual perspective, so the GasHis-Transformer model\nmisidentiﬁed this image. The feature map in Fig. 8(f) shows that although\ninterstitium is detected in the feature map, the model assigns more weight to\nthe lumen structure, indicating that GasHis-Transformer model is robust not\nonly to the background information but also to the interstitial information.\n4.2.2. Contrast Experiment of GHID\nIn order to show the eﬀectiveness of GasHis-Transformer and LW-GasHis-\nTransformer in the GHID task, a series of comparative experiments are car-\nried out using CNNs and attention models on the testing set. In addition,\n19\nActual: Abnormal Predicted: Abnormal\nActual: Abnormal Predicted: Abnormal\nActual: Abnormal Predicted: Normal\n(b)\n(c)\n(d) (a) \n(e)\nActual: Normal  Predicted: Abnormal\n(f)\nActual: Normal  Predicted: Normal\n(g)\nActual: Normal  Predicted: Normal\nFigure 8: Visualization analysis of the misidentiﬁcation results. (a) is 2-D vector scatter\nplots using the t-SNE method. (b)-(d) are abnormal images. Left column are original\nimages, middle column are pixel-level ground truth images and right column are four\nfeature maps of each image. (e)-(g) are normal images. Left column are original images\nand right column are four feature maps of each image. Light blue color indicates\ncorrectly detected images, orange color indicates incorrectly detected images.\nclassical CNNs and attention models are compared with and without image\nnormalization, where all hyper-parameters are set to the same values as that\nin Sub-section 4.1.3.\nComparison with Other Models: . The GHID results of all models are\ncompared in Table 4. Frist, it is obvious that GasHis-Transformer achieves\ngood performance in terms of Pre, F1 and Acc. GasHis-Transformer has some\nimprovement compared to Xception, which shows good performance com-\npared to other traditional CNN models. GasHis-Transformer has a higher\nRec, F1, and Acc than the Xception model. Second, although the perfor-\nmance of LW-GasHis-Transformer is degraded compared with that of GasHis-\nTransformer, it still is better than CNN and attention models. In addition,\nsince GasHis-Transformer and LW-GasHis-Transformer can extract multi-\nscale features, they have more stable training results and minor variance\n20\nthan CNN and attention mechanism models that extract features on the same\nscale. Finally, although Transformer models using sequential CNN such as\nBotNet-50 [4], TransMed [24] and LeViT [25] have higher results than pure\nTransformer models, they lose more information than GasHis-Transformer\nmodel. Therefore, Transformer models using sequential CNN are less eﬀec-\ntive than GasHis-Transformer model.\nTable 4: A comparison of diﬀerent models on the HE-GHI-DS test set. ([In %].)\nModels Pre Rec F1 Acc\nGasHis-Transformer 98.55 ± 1.07 97.38 ± 1.33 97.97 ± 0.78 97.97 ± 0.74\nLW-GasHis-Transformer 95.99 ± 2.64 96.90 ± 2.96 96.43 ± 0.98 96.43 ± 1.39\nXception [15] 94.48 ± 3.21 97.78 ± 2.25 95.98 ± 1.31 95.94 ± 1.36\nResNet-50 [14] 93.40 ± 2.44 95.26 ± 1.94 94.26 ± 1.43 94.24 ± 1.43\nInception-V3 [5] 93.64 ± 2.80 94.40 ± 3.83 93.96 ± 0.51 93.80 ± 0.54\nVGG-16 [12] 90.82 ± 3.73 94.48 ± 3.86 92.38 ± 2.79 92.34 ± 2.79\nVGG-19 [12] 88.68 ± 2.66 94.68 ± 3.32 91.34 ± 2.10 91.24 ± 2.25\nViT [26] 86.10 ± 3.88 83.72 ± 6.09 84.88 ± 1.24 84.78 ± 1.27\nBotNet-50 [4] 87.72 ± 2.29 90.56 ± 2.88 88.84 ± 0.60 88.88 ± 0.64\nTransMed [24] 94.34 ± 2.06 97.06 ± 2.27 95.58 ± 0.64 95.58 ± 0.64\nLeViT [25] 91.90 ± 1.28 90.50 ± 3.10 91.26 ± 1.63 91.26 ± 1.60\nHCRF-AM [20] 92.90 ± 2.51 91.94 ± 8.26 92.06 ± 5.50 94.24 ± 1.83\nGCNet+Resnet [19] 96.82 ± 2.64 96.40 ± 2.89 95.26 ± 1.19 96.48 ± 0.17\nSENet+CNN [18] 95.94 ± 1.36 95.94 ± 1.36 95.94 ± 1.36 95.94 ± 1.36\nCBAM+Resnet [17] 94.22 ± 2.83 96.10 ± 2.91 94.00 ± 3.06 95.04 ± 1.91\nNon-local+Resnet [16] 94.46 ± 2.63 97.00 ± 2.78 94.20 ± 2.75 95.58 ± 1.10\nEﬀect of Normalization on Model Performance: . The comparison\nresults using normalization during pre-processing is shown in Fig. 9. There\nis an increasing trend of Pre, Rec, F1 score, and Acc when using normaliza-\ntion. For Pre, all models have improvement except Inception-V3, and the\nproposed GasHis-Transformer and LW-GasHis-Transformer have improved\nby 1.11% and 2.46%, respectively. For Rec, all models have better results\nexcept Xception and BoTNet-50, and GasHis-Transformer and LW-GasHis-\nTransformer have improved by 0.44% and 0.55%, respectively. In summary,\nnormalization in image preprocessing can improve detection performance.\n21\n98.26\n96.06 \n94.48 \n93.40 93.64 \n90.82 \n88.68 \n86.10 \n87.72 \n97.13 \n93.62 \n91.44 91.48 \n95.92 \n89.15 \n86.98 \n85.75 85.40 \n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nCompared in precision\nNormalization Without Normalization\n97.34 97.00 \n97.78 \n95.26 \n94.40 94.48 94.68 \n83.72 \n90.56 \n96.91 96.44 \n98.09 \n94.55 \n91.78 \n90.62 \n91.43 \n82.79 \n92.35 \n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nCompared in recall\nNormalization Without Normalization\n97.76 \n96.46 95.94 \n94.24 93.80 \n92.34 \n91.24 \n84.78 \n88.88 \n97.13 \n94.19 94.35 \n92.28 \n93.65 \n89.67 88.98 \n84.24 \n88.28 \n75\n80\n85\n90\n95\n100\nCompared in accuracy\nNormalization Without Normalization\n97.78 \n96.10 95.98 \n94.26 93.96 \n92.38 \n91.34 \n84.88 \n88.84 \n97.14 \n95.46 \n94.75 \n92.48 \n93.59 \n89.75 \n89.09 \n84.32 \n88.25 \n80\n82\n84\n86\n88\n90\n92\n94\n96\n98\n100\nCompared in F1-score\nNormalization Without Normalization\nFigure 9: Eﬀect of normalization in during preprocessing raw data. All images in\ntraining, validation and test sets are normalized according to Eq. 5.\n4.2.3. Robustness Test of GasHis-Transformer\nRobustness is a property that maintains the stability of a model under pa-\nrameter ingestion and measures the behavior of systems under non-standard\nconditions. Robustness is deﬁned by community as the degree to which a\nsystem operates correctly in the presence of exceptional inputs or stressful\nenvironmental conditions. The robustness test aims to work correctly with\neach functional module when handling incorrect data and abnormal problems\n(through adding noise or taking other datasets), enhancing models’ fault re-\nsistance. To test the robustness of the proposed GasHis-Transformer model,\nten diﬀerent adversarial attacks and conventional noises are added to the\nHE-GHI-DS test set. Adversarial attacks are subtle interference added to\nthe input sample that causes CNN model to give an incorrect output with a\nhigh conﬁdence level [35]. Adversarial attacks include FGM [36], FSGM [37],\nPGD [38] and Deepfool [39]; conventional noises include Gaussian, Salt &\n22\nPepper, uniform, exponential, Rayleigh and Erlang noise. First, the epsilons\nare performed with nine levels in [0 .001,0.256] using 0.001 as initialization\nand the powers of 2 as step length. Then, Pre, Rec, F1 and Acc are used\nto evaluate the robustness of GasHis-Tranformer in the GHID task. Fig. 10\nshows four criteria under diﬀerent epsilons and noise.\nFigure 10: Robustness test of GasHis-Transformer under ten adversarial attack and\nconventional noises.\nFor adversarial attacks noises, ﬁrst, GasHis-Transformer is optimally ro-\nbust when FGM is increased, and diﬀerent epsilons have almost no eﬀect\non the model. Secondly, although criteria obtained by adding FSGM have\nsome diﬀerences compared with adding FGM, in general, the performance is\npositive. When epsilon is higher than 0.032, the criteria converge to stabil-\nity, even Rec increases slightly. Finally, adding Deepfool and PGD of any\nmagnitude of epsilons results in a poor detection of the model. In summary,\nfor noise generation by adversarial attacks, GasHis-Transformer has better\nrobustness to FGM and FSGM.\n23\nFor conventional noises, ﬁrst, while four criteria of adding Erlang noise\nand uniform noise decrease compared to those of FGM in adversarial attacks,\nthey are relatively constant compared to those with other convention noises.\nTherefore, adding them does not aﬀect the robustness of the model in general.\nIn addition, when epsilon is lower than 0.1, the model is barely aﬀected by\nadding Gaussian, Rayleigh and Salt & Pepper noise. However, when epsilon\nis higher than 0.1, the test set’s Pre, Rec and F1 drop to 0, indicating that all\nabnormal images are incorrectly detected as normal. This suggests that in\nthe case of strong image noise, GasHis-Transformer tends to predict a more\nlikely to normal category. In summary, for conventional noises, GasHis-\nTransformer has strong robustness to both Erlang noise and uniform noise.\nSimilarly, the model also has strong robustness of epsilon between 0 and 0.1\nfor Gaussian, Rayleigh and Salt & Pepper noise.\n4.3. Extended Experiment\nFirstly, an extended experiment for gastrointestinal cancer detection is\nperformed using additional 620 gastrointestinal images. Then, illustrative\nexperiments are performed on a publicly available breast cancer dataset\nBreakHis and a lymphoma dataset immunohistochemical (IHC) stained lym-\nphoma histopathological image dataset (IHC-LI-DS). The experimental setup\nof the extended experiments is generally based on the gastric cancer dataset\nand slightly adjusted with their respective characteristics. Finally, repeata-\nbility experiments are performed to demonstrate the stability of GasHis-\nTransformer.\n24\n4.3.1. Extended Experiment for Gastrointestinal Cancer Detection\nGastrointestinal cancer includes gastric cancer and colorectal cancer. Due\nto gastric and colorectal organs have glands, their histopathological im-\nages have many similar features. Some examples of gastrointestinal can-\ncer histopathological images is shown in Fig. 11. This extended experiment\nis used to evident that not only does the GasHis-Transformer model has\noutstanding performance in the GHID task, but it also has an excellent per-\nformance in the gastrointestinal cancer detection task. Based on the main\nexperiment in Sub-section 4.2.1, medical doctors often focus more on de-\ntecting abnormal categories. If images detect as abnormal by deep learning\nmodels, doctors need to conduct operations such as staging benign and ma-\nlignant lesions, determining the area of the lesion, and determining whether\nit has spread extensively. Therefore, doctors frequently prefer models with\nhigh detection rates in the abnormal category in clinical applications.\n(a) Gastric Cancer (b) Colorectal Cancer\nNormal Abnormal Normal Abnormal\nFigure 11: Some examples of gastrointestinal histopathological image.\nIn the gastrointestinal cancer detection task, it includes gastric cancer\ndetection and colorectal cancer detection. In the gastric cancer detection,\ntraining set, validation set and model parameters are followed the main exper-\niment. The test data use the remaining 420 abnormal images in the dataset.\nIn the colorectal cancer detection, the colorectal dataset contains 800 images\nof two diﬀerent categories including abnormal category and normal category\n25\nwith image-level labels, which are provided by a medical doctor from China\nMedical University. 800 images are randomly assigned to training, validation\nand test sets with a ratio of 1: 1: 2 and the training set is expanded to six\ntimes. The model parameters are obtained by training GasHis-Transformer\non the training set. Above all, 620 gastrointestinal cancer images including\n420 gastric cancer images and 200 colorectal cancer images are used to test\nGasHis-Transformer. The gastrointestinal cancer detection results are shown\nin Table 5: For gastric cancer images, 409 images are correctly detected by\nthe model with only 11 images are not detected, and Acc of the model for\ngastric cancer images reaches 97.97%; for colorectal cancer images, 196 im-\nages are detected by the model with only 4 images not detected, and Acc of\nthe model for colorectal images reaches 98.00%. In summary, for 620 gas-\ntrointestinal images we have 600 detected images and 20 undetected images\nwith a detection Acc of 97.58%.\nTable 5: The result in the gastrointestinal cancer detection task. ([In %].)\nCancer Type Correct Incorrect Acc\nGastrointestinal\nGastric 409 11 97.97\nColorectal 196 4 98.00\nSum 605 15 97.58\n4.3.2. Extended Experiment for Breast Cancer Image Classiﬁcation\nBreast cancer is associated with a high mortality rate in comparison\nwith other cancers. We further demonstrate the well-performance of GasHis-\nTransformer in breast cancer image classiﬁcation using BreakHis dataset [40].\nIn this paper, malignant tumors with a magniﬁcation of 200 ×are used for\nthe four classiﬁcations including ductal carcinoma (DC), lobular carcinoma\n(LC), mucinous carcinoma (MC) and papillary carcinoma (PC) of the breast.\n26\nAn example of 200×BreakHis images is shown in Fig. 12 and the data setting\nis shown in Table 6.\n(b) DC (c) LC (d) MC\n (e) PC\n(a) Normal\nFigure 12: An example of 200 ×BreakHis Images.\nTable 6: Data setting of BreakHis dataset for training, validation and test sets.\nImage Type Training Validation Test Sum\nDC\nOringin 538 179 179 896\nAugmented 3228 1074 1074 5376\nLC\nOringin 98 33 32 163\nAugmented 588 198 192 978\nMC\nOringin 118 39 39 196\nAugmented 708 234 234 1176\nPC\nOringin 81 27 27 135\nAugmented 486 162 162 810\nThe same experimental parameter setting is used for the classiﬁcation\nof BreakHis data as that for HE-GHI-DS. The best classiﬁcation results of\nthe traditional CNN models on the BreakHis dataset is VGG-16, by the\nfour criteria including Pre, Rec, F1 and Acc , which are 81.32%, 79.20%,\n79.86% and 85.74%, respectively. Compared with VGG-16, Pre, Rec, F1\nand Acc of GasHis-Transformer are increased by 2.60%, 3.96%, 3.62% and\n2.36%, respectively and Pre, Rec, F1 and Acc of LW-GasHis-Transformer\nare increased by 3.22%, 3.79%, 3.83% and 2.19%, respectively. It shows that\nGasHis-Transformer and LW-GasHis-Transformer have better image classiﬁ-\ncation performance on the BreakHis dataset. A comparison in extended ex-\nperiments using diﬀerent models on the BreakHis test set is shown in Table 7.\nThe experimental results further demonstrate that GasHis-Transformer and\n27\nLW-GasHis-Transformer is outstanding in the GHID tasks as well as in other\nH&E histopathological image classiﬁcation tasks.\nTable 7: A comparison of image classiﬁcation results on the BreakHis test set. ([In %].)\nModels Pre Rec F1 Acc\nGasHis-Transformer 83.92 ± 1.71 83.16 ± 1.74 83.48 ± 1.38 88.10 ± 0.83\nLW-GasHis-Transformer 84.54 ± 3.00 82.99 ± 3.19 83.69 ± 3.09 87.93 ± 2.05\nXception [15] 79.24 ± 1.35 78.62 ± 1.23 78.84 ± 1.10 85.33 ± 0.75\nResNet-50 [14] 74.60 ± 2.48 76.88 ± 1.78 75.54 ± 2.15 82.66 ± 1.47\nInception-V3 [5] 79.18 ± 4.76 79.84 ± 2.60 79.02 ± 2.98 84.62 ± 1.93\nVGG-16 [12] 81.32 ± 1.89 79.20 ± 1.79 79.86 ± 1.39 85.74 ± 1.26\nVGG-19 [12] 78.42 ± 1.70 77.96 ± 3.78 77.14 ± 2.72 84.39 ± 1.29\nViT [26] 74.28 ± 1.89 76.26 ± 1.09 75.04 ± 1.06 82.16 ± 1.13\nBotNet-50 [4] 79.20 ± 2.39 80.72 ± 3.69 79.50 ± 2.97 85.32 ± 1.65\n4.3.3. Extended Experiment for Lymphoma Image Classiﬁcation\nMalignant lymphoma is one type of deadly cancer that aﬀects the lymph\nnodes. Three types of malignant lymphoma are representative in the im-\nmunohistochemical (IHC) stained lymphoma histopathological image dataset\n(IHC-LI-DS)2: chronic lymphocytic leukemia (CLL), follicular lymphoma\n(FL) and mantle cell lymphoma (MCL). An example of IHC-LI-DS is shown\nin Fig. 13. A total of 374 images are available in IHC-LI-DS and the data\nsetting is shown in Table 8. Since three diﬀerent types of lymphoma are\nclassiﬁed according to the shape of lymphocytes, directly resizing the im-\nage of lymphoma makes it challenging to distinguish the small and dense\nlymphocytes. Therefore, we crop the whole lymphoma image into patches\nof 224 ×224 pixels as the standard input to GasHis-Transformer and LW-\nGasHis-Transformer, which augment the datasets by 24 times.\n2This dataset is open access on: Jaﬀe, E. and Orlov,\nN, NIA Intramural Research Program Laboratory of Genetics,\nhttps://ome.grc.nia.nih.gov/iicbu2008/lymphoma/index.html\n28\n(b) CLL (c) FL (d) MCL\n(a) Normal\nFigure 13: An example of IHC-LI-DS Images.\nTable 8: Data setting of IHC-LI-DS for training, validation and test sets.\nImage Type Training Validation Test Sum\nCLL\nOringin 40 30 43 113\nAugmented 960 720 1032 2712\nFL\nOringin 40 30 69 139\nAugmented 960 720 1656 3336\nMCL\nOringin 40 30 52 122\nAugmented 960 720 1248 2928\nTable 9 summarizes the experimental results in the same experimental pa-\nrameter setting of HE-GHI-DS to classify the three-class IHC-LI-DS. GasHis-\nTransformer has the best performance in Rec, F1 and Acc while LW-GasHis-\nTransformer has the best performance in Pre compared with the other mod-\nels. In the IHC-LI-DS, the best performance of the traditional models is\nXception, which has the highest Pre, Rec, F1 and Acc among the tradi-\ntional CNN models, reaching 80.72%, 80.58%, 80.00% and 81.48%, respec-\ntively. However, the performance of GasHis-Transformer and LW-GasHis-\nTransformer is even better than that of Xception. Pre, Rec, F1 and Acc\nof GasHis-Transformer reach an outstanding 82.42%, 83.30%, 83.16% and\n84.34% respectively while LW-GasHis-Transformer reach 82.66%, 82.70%,\n82.38% and 83.64%, respectively. Therefore, GasHis-Transformer and LW-\nGasHis-Transformer not only have an excellent classiﬁcation performance on\nH&E stained datasets, but also do well in IHC stained datasets.\n29\nTable 9: A comparison of image classiﬁcation results on the IHC-LI-DS test set. ([In %].)\nModels Pre Rec F1 Acc\nGasHis-Transformer 82.42 ± 1.97 83.30 ± 1.37 83.16 ± 1.02 84.34 ± 0.72\nLW-GasHis-Transformer 82.66 ± 0.90 82.70 ± 0.82 82.38 ± 0.63 83.64 ± 0.78\nXception [15] 80.72 ± 0.92 80.58 ± 0.74 80.00 ± 0.98 81.48 ± 1.12\nResNet-50 [14] 77.66 ± 0.81 78.06 ± 0.87 77.36 ± 0.81 78.58 ± 0.77\nInception-V3 [5] 78.26 ± 1.35 78.78 ± 1.28 78.22 ± 1.49 79.17 ± 1.46\nVGG-16 [12] 76.48 ± 0.71 77.00 ± 0.76 76.58 ± 0.73 77.81 ± 0.68\nVGG-19 [12] 77.04 ± 1.70 77.34 ± 1.56 76.78 ± 1.66 77.91 ± 2.18\nViT [26] 73.24 ± 1.90 74.10 ± 1.76 73.12 ± 1.98 74.33 ± 2.03\nBotNet-50 [4] 77.54 ± 2.39 76.86 ± 2.12 76.78 ± 3.00 77.20 ± 2.40\n4.4. Experimental Environment and Computational Time\nA workstation with Windows 10, AMD Ryzen 7 4800HS 2.90GHz, GeForce\nRTX 2060 6GB and 16 GB RAM is utilized in the experiment. Matlab\nR2020b is used to do image pre-processing. Python 3.6, Pytorch 1.7.0 and\ntorchvision 0.8.0 are used for deep learning. Table 10 shows the parame-\nter size and training time on three datasets of eight deep learning models.\nIt takes 0.86 and 0.67 hours to train GasHis-Transformer and LW-GasHis-\nTransformer with 840 training and validation images in 75 epochs, and only\n30 Sec to test GasHis-Transformer and LW-GasHis-Transformer on 840 im-\nages (0.036 Sec per image).\nTable 10: The parameter size (MB) and training time (hour) in all experiments.\nModels Parameter Size\nTraining Time\nGastry Breast Lymphoma\nGasHis-Transformer 155 0.86 2.78 2.39\nLW-GasHis-Transformer 77 0.67 2.14 1.60\nXception [15] 79 0.75 2.36 1.75\nResNet-50 [14] 90 0.69 1.73 1.46\nInception-V3 [5] 83 0.73 1.58 1.58\nVGG-16 [12] 268 0.78 2.39 1.78\nVGG-19 [12] 298 0.81 2.76 2.01\nBotNet-50 [4] 72 0.69 1.72 1.46\nViT [26] 48 0.69 1.58 1.29\n30\n5. Conclusion and Future Work\nWe have proposed a GasHis-Transformer model and its lightweight ver-\nsion called LW-GasHis-Transformer to detect gastric cancer in histopatho-\nlogical images. This approach combines the advantages of the classical CNN\nmodel in extracting local information and uses the most recent Transformer\nmodel in capturing long-range correlation considering the global and local\nassociations of images in a uniﬁed context. In the experiments, GasHis-\nTransformer and LW-GasHis-Transformer are tested on a gastric cancer histopatho-\nlogical dataset with accuracies of 97.97% and 96.43%, respectively, showing\ntheir potential in the GHID tasks. Also, we have conducted a gastroin-\ntestinal cancer detection task to demonstrate that GasHis-Transformer and\nLW-GasHis-Transformer have excellent cancer detection abilities. Finally,\nwe have extended tests for classiﬁcation tasks on a breast cancer dataset\nand a lymphoma dataset with excellent accuracies 88.10% and 84.34% for\nGasHis-Transformer and 87.93% and 83.64% for LW-GasHis-Transformer,\nrespectively, demonstrating an encouraging generalizability both in H&E\nstained and IHC stained histopathological images. Since a small dataset in\nthis experiment may lead to a degradation of the classiﬁcation performance\nof GasHis-Transformer model, we consider using methods such as domain\nadaptation and few-shot learning to solve this problem in future work.\nAcknowledgements and Conﬂict of Interest\nThis work is supported by National Natural Science Foundation of China\n(No. 61806047). We thank Miss Zixian Li and Mr. Guoxian Li for their\nimportant discussion. We also thank Mr. Jinghua Zhang for his contribution\nto the revision of this paper. There is no conﬂict of interest in this paper.\n31\nReferences\n[1] P. S. Hegde, D. S. Chen, Top 10 challenges in cancer immunotherapy, Immunity 52 (1)\n(2020) 17–35.\n[2] S. Ai, X. Li, C. Li, et.al, A state-of-the-art review for gastric histopathology image\nanalysis approaches and future development, BioMed Research International (2021)\n1–31.\n[3] C. L. Srinidhi, O. Ciga, A. L. Martel, Deep neural network models for computational\nhistopathology: A survey, Medical Image Analysis 67 (2021) 101813.\n[4] A. Srinivas, T.-Y. Lin, N. Parmar, J. Shlens, P. Abbeel, A. Vaswani, Bottleneck\ntransformers for visual recognition, in: Proc. of CVPR 2021, 2021, pp. 16519–16529.\n[5] C. Szegedy, V. Vanhoucke, S. Ioﬀe, et.al, Rethinking the inception architecture for\ncomputer vision, in: Proc. of CVPR 2016, 2016, pp. 2818–2826.\n[6] T. Choudhary, V. Mishra, A. Goswami, J. Sarangapani, A comprehensive survey\non model compression and acceleration, Artiﬁcial Intelligence Review 53 (7) (2020)\n5113–5155.\n[7] A. Mobiny, P. Yuan, S. K. Moulik, N. Garg, C. C. Wu, H. Van Nguyen, Dropconnect\nis eﬀective in modeling uncertainty of bayesian deep networks, Scientiﬁc reports 11 (1)\n(2021) 1–14.\n[8] H. Sharma, N. Zerbe, C. B¨ oger, et.al, A comparative study of cell nuclei attributed\nrelational graphs for knowledge description and categorization in histopathological\ngastric cancer whole slide images, in: Proc. of CBMS 2017, 2017, pp. 61–66.\n[9] S. Wang, Y. Zhu, L. Yu, et.al, Rmdl: Recalibrated multi-instance deep learning for\nwhole slide gastric image classiﬁcation, Medical image analysis 58 (2019) 101549.\n[10] Z. Song, S. Zou, W. Zhou, et.al, Clinically applicable histopathological diagnosis\nsystem for gastric cancer detection using deep learning, Nature communications 11 (1)\n(2020) 1–9.\n32\n[11] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classiﬁcation with deep convo-\nlutional neural networks, Communications of the ACM 60 (6) (2017) 84–90.\n[12] K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image\nrecognition, arXiv: 1409.1556 (2014).\n[13] C. Szegedy, P. Liu, Y. Jia, et.al, Going deeper with convolutions, in: Proc. of CVPR\n2015, 2015, pp. 1–9.\n[14] K. He, X. Zhang, S. Ren, et.al, Deep residual learning for image recognition, in: Proc.\nof CVPR 2016, 2016, pp. 770–778.\n[15] F. Chollet, Xception: Deep learning with depthwise separable convolutions, in: Proc.\nof CVPR 2017, 2017, pp. 1251–1258.\n[16] X. Wang, R. Girshick, A. Gupta, et.al, Non-local neural networks, in: Proc. of CVPR\n2018, 2018, pp. 7794–7803.\n[17] S. Woo, J. Park, J. Lee, et.al, Cbam: Convolutional block attention module, in: Proc.\nof ECCV 2018, 2018, pp. 3–19.\n[18] J. Hu, L. Shen, G. Sun, Squeeze-and-excitation networks, in: Proc. of CVPR 2018,\n2018, pp. 7132–7141.\n[19] Y. Cao, J. Xu, S. Lin, et.al, Gcnet: Non-local networks meet squeeze-excitation\nnetworks and beyond, in: Proc. of ICCVW 2019, 2019, pp. 1–8.\n[20] Y. Li, X. Wu, C. Li, et.al, A hierarchical conditional random ﬁeld-based attention\nmechanism approach for gastric histopathology image classiﬁcation, Applied Intelli-\ngence (2021).\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,  L. Kaiser,\nI. Polosukhin, Attention is all you need, in: Advances in neural information processing\nsystems, 2017, pp. 5998–6008.\n33\n[22] K. Han, Y. Wang, H. Chen, et.al, A survey on visual transformer, arXiv: 2012.12556\n(2020).\n[23] S. Khan, M. Naseer, M. Hayat, et.al, Transformers in vision: A survey, arXiv:\n2101.01169 (2021).\n[24] Y. Dai, Y. Gao, F. Liu, Transmed: Transformers advance multi-modal medical image\nclassiﬁcation, Diagnostics 11 (8) (2021) 1384.\n[25] B. Graham, A. El-Nouby, H. Touvron, et.al, Levit: a vision transformer in convnet’s\nclothing for faster inference, arXiv preprint arXiv:2104.01136 (2021).\n[26] A. Dosovitskiy, L. Beyer, A. Kolesnikov, et.al, An image is worth 16x16 words: Trans-\nformers for image recognition at scale, in: Proc. of ICLR 2021, 2021.\n[27] I. Bello, B. Zoph, A. Vaswani, et.al, Attention augmented convolutional networks, in:\nProc. of ICCV 2019, 2019, pp. 3286–3295.\n[28] P. Shaw, J. Uszkoreit, A. Vaswani, Self-attention with relative position representa-\ntions, in: Proc. of NAACL-HLT 2018, 2018, pp. 464–468.\n[29] N. Srivastava, G. Hinton, A. Krizhevsky, et.al, Dropout: a simple way to prevent\nneural networks from overﬁtting, The journal of machine learning research 15 (1)\n(2014) 1929–1958.\n[30] Q. Lyu, C. T. Whitlow, G. Wang, Softdropconnect (sdc)–eﬀective and eﬃcient quan-\ntiﬁcation of the network uncertainty in deep mr image analysis, arXiv preprint\narXiv:2201.08418 (2022).\n[31] Y. Li, X. Li, X. Xie, et.al, Deep learning based gastric cancer identiﬁcation, in: Proc.\nof ISBI 2018, 2018, pp. 182–185.\n[32] K. Kim, Normalized class coherence change-based knn for classiﬁcation of imbalanced\ndata, Pattern Recognition 120 (2021) 108126.\n34\n[33] W. Liu, C. Li, M. M. Rahaman, et.al, Is the aspect ratio of cells important in deep\nlearning? a robust comparison of deep learning methods for multi-scale cytopathology\ncell image classiﬁcation: From convolutional neural networks to visual transformers,\nComputers in Biology and Medicine (2021) 105026.\n[34] L. Van der Maaten, G. Hinton, Visualizing data using t-sne., Journal of machine\nlearning research 9 (11) (2008).\n[35] A. Ghosh, S. S. Mullick, S. Datta, et.al, A black-box adversarial attack strategy with\nadjustable sparsity and generalizability for deep image classiﬁers, Pattern Recognition\n122 (2022) 108279.\n[36] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing adversarial exam-\nples, stat 1050 (2015) 20.\n[37] A. Kurakin, I. Goodfellow, S. Bengio, Adversarial examples in the physical world\n(2016).\n[38] A. Madry, A. Makelov, L. Schmidt, et.al, Towards deep learning models resistant to\nadversarial attacks, in: Proc. of ICLR 2018, 2018.\n[39] S. Moosavi-Dezfooli, A. Fawzi, P. Frossard, Deepfool: a simple and accurate method\nto fool deep neural networks, in: Proc. of CVPR 2016, 2016, pp. 2574–2582.\n[40] F. Spanhol, L. Oliveira, C. Petitjean, et.al, A dataset for breast cancer histopatho-\nlogical image classiﬁcation, Ieee transactions on biomedical engineering 63 (7) (2015)\n1455–1462.\n35"
}