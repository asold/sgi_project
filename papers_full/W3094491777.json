{
  "title": "AdapterDrop: On the Efficiency of Adapters in Transformers",
  "url": "https://openalex.org/W3094491777",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2741032870",
      "name": "Andreas Rücklé",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3094594157",
      "name": "Gregor Geigle",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2798882287",
      "name": "Max Glockner",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2949974153",
      "name": "Tilman Beck",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2655736194",
      "name": "Jonas Pfeiffer",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2132681681",
      "name": "Nils Reimers",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    },
    {
      "id": "https://openalex.org/A40109512",
      "name": "Iryna Gurevych",
      "affiliations": [
        "Technical University of Darmstadt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W3112302586",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W2763323349",
    "https://openalex.org/W2911300548",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3214578205",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3023528699",
    "https://openalex.org/W3173374050",
    "https://openalex.org/W3026990524",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2995983533",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3105421296",
    "https://openalex.org/W2979736514",
    "https://openalex.org/W2963809228"
  ],
  "abstract": "Massively pre-trained transformer models are computationally expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7930–7946\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n7930\nAdapterDrop: On the Efﬁciency of Adapters in Transformers\nAndreas Rücklé∗ and Gregor Geigle and Max Glockner,\nTilman Beck and Jonas Pfeiffer and Nils Reimers and Iryna Gurevych\nUbiquitous Knowledge Processing Lab (UKP)\nDepartment of Computer Science, Technische Universität Darmstadt\nwww.ukp.tu-darmstadt.de\nAbstract\nTransformer models are expensive to ﬁne-tune,\nslow for inference, and have large storage re-\nquirements. Recent approaches tackle these\nshortcomings by training smaller models, dy-\nnamically reducing the model size, and by\ntraining light-weight adapters. In this paper,\nwe propose AdapterDrop, removing adapters\nfrom lower transformer layers during training\nand inference, which incorporates concepts\nfrom all three directions. We show that Adap-\nterDrop can dynamically reduce the compu-\ntational overhead when performing inference\nover multiple tasks simultaneously, with mini-\nmal decrease in task performances. We further\nprune adapters from AdapterFusion, which im-\nproves the inference efﬁciency while maintain-\ning the task performances entirely.\n1 Introduction\nWhile transfer learning has become the go-to\nmethod for solving NLP tasks (Pan and Yang, 2010;\nTorrey and Shavlik, 2010; Ruder, 2019; Howard\nand Ruder, 2018; Peters et al., 2018), transformer-\nbased models are notoriously deep requiring mil-\nlions or even billions of parameters (Radford et al.,\n2018; Devlin et al., 2019; Radford et al., 2019; Liu\net al., 2019; Brown et al., 2020). This results in\nslow inference and large storage requirements.\nAt least three independent lines of research\nhave recently evolved to tackle these shortcom-\nings. (1) Smaller and faster models that are ei-\nther distilled or trained from scratch (Sanh et al.,\n2019; Sun et al., 2020; Bai et al., 2021; Wang et al.,\n2020). (2) Robustly trained transformers in which\nthe model depth can be reduced at run-time, thereby\ndecreasing inference time dynamically (Fan et al.,\n2020; Elbayad et al., 2020; Xin et al., 2020; Hou\net al., 2020). (3) Adapters, which, instead of fully\nﬁne-tuning the model, only train a newly intro-\nduced set of weights at every layer, thereby sharing\n∗Work done prior to joining Amazon.\nthe majority of parameters between tasks (Houlsby\net al., 2019; Bapna and Firat, 2019; Pfeiffer et al.,\n2020a). Adapters have been shown to work well\nfor machine translation (Bapna and Firat, 2019),\ncross-lingual transfer (Pfeiffer et al., 2020b, 2021b;\nÜstün et al., 2020; Vidoni et al., 2020; Ansell et al.,\n2021), community QA (Rücklé et al., 2020), and\ntask composition for transfer learning (Stickland\nand Murray, 2019; Pfeiffer et al., 2021a; Lauscher\net al., 2020; Wang et al., 2021; Poth et al., 2021).\nDespite their recent popularity, the computational\nefﬁciency of adapters has not been explored beyond\nparameter efﬁciency.\nWe close this gap and establish the computa-\ntional efﬁciency of two adapter architectures at\ntraining and inference time. We investigate differ-\nent strategies to further improve the efﬁciency of\nadapter-based models by incorporating ideas from\nall three directions mentioned above. Our strategies\nrely on dropping out adapters from transformers,\nat training and inference time, resulting in models\nthat are dynamically adjustable regarding the avail-\nable computational resources. Our approaches are\nagnostic to the pre-trained transformer model (e.g.,\nbase, large), which makes them broadly applicable.\nContributions:\n1. We are the ﬁrst to establish the computational ef-\nﬁciency of adapters compared to full ﬁne-tuning.\nWe show that the training steps of adapters can\nbe up to 60% faster than full model ﬁne-tuning\nwith common hyperparameter choices, while be-\ning 4–6% slower at inference. Hence, adapters\nare a suitable choice for researchers interested\nin achieving faster training times, or when re-\nquiring extensive hyperparameter tuning.\n2. We propose AdapterDrop, the efﬁcient and dy-\nnamic removal of adapters with minimal im-\npact on the task performances. We show that\ndropping adapters from lower transformer lay-\ners considerably improves the inference speed in\n7931\nSetting Adapter Relative speed (for Seq.Len./Batch)\n128/16 128/32 512/16 512/32\nTraining Houlsby 1.48 1.53 1.36 1.33\nPfeiffer 1.57 1.60 1.41 1.37\nInference Houlsby 0.94 0.94 0.96 0.96\nPfeiffer 0.95 0.95 0.96 0.96\nTable 1: Relative speed of adapters compared to fully\nﬁne-tuned models. For example, 1.6 for training with\nthe Pfeiffer adapter means that we can perform 1.6\ntraining steps with this adapter in the time of one train-\ning step with full model ﬁne-tuning.\nmulti-task settings. For example, with adapters\ndropped from the ﬁrst ﬁve layers, AdapterDrop\nis 39% faster when performing inference on 8\ntasks simultaneously. This can be beneﬁcial\nfor researchers working on models that need to\nmake multiple predictions on each input.\n3. We prune adapters from adapter compositions in\nAdapterFusion (Pfeiffer et al., 2021a) and retain\nonly the most important adapters after trans-\nfer learning, resulting in faster inference while\nmaintaining the task performances entirely. This\nis suitable for settings with little labeled training\ndata, where AdapterFusion can achieve ample\nimprovements over standard single task models.\n2 Efﬁciency of Adapters\nWe ﬁrst establish the computational efﬁciency of\nadapters without AdapterDrop. As illustrated in\nFigure 1, signiﬁcant differences exist in the for-\nward and backward pass when ﬁne-tuning adapters\ncompared to fully ﬁne-tuning the model. In the\nforward pass, adapters add complexity with the ad-\nditional components; however, it is not necessary\nto backpropagate through the entire model during\nthe backward pass. We compare the training and\ninference speed of full model ﬁne-tuning against\nthe adapter architectures of Houlsby et al. (2019)\nand Pfeiffer et al. (2021a) (depicted in Figure 1)\nusing the AdapterHub.ml framework (Pfeiffer et al.,\n2020a). We conduct our measurements with the\ntransformer conﬁguration of BERT base and verify\nthem with different GPUs.1\nWe provide measurements corresponding to\ncommon experiment conﬁgurations in Table 1.\nTraining. Adapters can be considerably faster\ncompared to full model ﬁne-tuning—60% faster\n1We experiment with newer and older GPUs, Nvidia V100\nand Titan X, respectively. See Appendix A.1 for details.\nin some conﬁgurations. The two adapter architec-\ntures differ only marginally in terms of training\nefﬁciency: due to its simpler architecture, train-\ning steps of the Pfeiffer adapters are slightly faster.\nThe magnitude of the differences depends on the\ninput size; the available CUDA cores are the pri-\nmary bottleneck.2 We do not observe any particular\ndifferences between adapters and full ﬁne-tuning\nregarding the training convergence.3\nThe training speedup can be explained by the\ndecreased overhead of gradient computation. Most\nof the parameters are frozen when using adapters\nand it is not necessary to backpropagate through\nthe ﬁrst components (see Figure 1).\nInference. The two adapter architectures are 94–\n96% as fast as fully ﬁne-tuned models, which varies\ndepending on the input size. This can have a con-\nsiderable impact when deployed at scale.\n3 AdapterDrop\nWe have established that adapters are more efﬁ-\ncient in terms of training time, however, there is a\nperpetuate need for sustainable and efﬁcient mod-\nels (Strubell et al., 2019). Backpropagating through\nas few layers as possible would further improve the\nefﬁciency of training adapters. The efﬁciency for\ninference can be improved by sharing representa-\ntions at lower transformer layers when simultane-\nously performing inference for multiple tasks—in\nother words, when performing multiple indepen-\ndent classiﬁcations on the same input. We establish\nthis in Table 2, ﬁnding that models are up to 8.4%\nfaster with every shared layer (16 tasks).\nMotivated by these observations, we propose\nAdapterDrop: Dynamically removing adapters\nfrom lower transformer layers (depicted in Fig-\nure 1). AdapterDrop is similar to dropping out\nentire transformer layers (Fan et al., 2020), how-\never, specialized to adapter settings—where lower\nlayers often have a small impact on the task perfor-\nmances (Houlsby et al., 2019).\nWe study two training methods for AdapterDrop:\n(1) Specialized AdapterDrop: Removing adapters\nfrom the ﬁrst n transformer layers, where n is ﬁxed\nduring training. This yields separate models for\neach possible n. (2) Robust AdapterDrop: Draw-\ning the integer n randomly from [0, 11] for each\n2We include detailed plots in Appendix G.1.\n3We also pre-train adapters with masked language model-\ning, ﬁnding that this does not yield better results (Appendix B).\n7932\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nFF Down\nFF Up\nAdd & Norm\nEmb\nHead\n...\nEmb\nHead\n...\nFF Down\nFF Up\nQuery\nAdd & Norm\nSoftMax\nAdapter\nAdapterFusion\nAdd & Norm\nKeyValue\nEmb\nHead\n...\nEmb\nHead\n...\nForward Backward\nForward Backward\nForward Backward\nForward Backward\nFigure 1: Standard adapter ﬁne-tuning vs. Adapter-\nDrop ﬁne-tuning. The left model includes adapters\nat every layer whereas the right model has adapters\ndropped at the ﬁrst layer. The arrows to the right of\neach model indicate the information ﬂow for the For-\nward and Backward pass through the model.\nSimultaneous Tasks 2 4 8 16\nSpeedup (each layer) 4.3% 6.6% 7.8% 8.4%\nTable 2: Speedup for each shared transformer layer\nwhen performing inference for multiple tasks simulta-\nneously (details are given in Appendix G.2)\ntraining batch.4 This yields one robustmodel that\nis applicable to a varying number of dropped lay-\ners. We study the effectiveness of AdapterDrop on\nthe devsets of the GLUE benchmark (Wang et al.,\n2018) using RoBERTa base (Liu et al., 2019).5\nFigure 2 shows that specialized AdapterDrop\nmaintains good results even with several dropped\nlayers. With the ﬁrst ﬁve layers dropped, special-\nized AdapterDrop maintains 97.1% of the origi-\nnal performance (averaged over all eight GLUE\ntasks; see Table 8). Moreover, robust AdapterDrop\nachieves comparable results, and with ﬁve layers\ndropped it maintains 95.4% of the original perfor-\nmance (on avg). The advantage of robust over\nspecialized AdapterDrop is that the robust variant\ncan be dynamically scaled. Based on current avail-\nable computational resources, robust AdapterDrop\ncan (de)activate layers with the same set of param-\neters, whereas specialized AdapterDrop needs to\nbe trained for every setting explicitly.\nThe efﬁciency gains can be large. When perform-\ning inference for multiple tasks simultaneously, we\nmeasure inference speedups of 21–42% with ﬁve\n4We also explored dropping adapters from randomly cho-\nsen layers (instead of early layers). This generally performs\nworse and it requires selecting a suitable dropout rate.\n5The detailed setup is listed in Appendix A.2.\n0 2 4 6 8 10\n40\n60\n80Accuracy\nMNLI\n0 2 4 6 8 10\n60\n80Accuracy\nQNLI\n0 2 4 6 8 10\nAdapterDrop (layers)\n0\n25\n50\n75Spearman \nSTS-B\n0 2 4 6 8 10\nAdapterDrop (layers)\n0\n20\n40\n60MCC\nCoLA\n12 specialized adapters\nRobust adapter\nStandard adapter\nFigure 2: Task performances in relation to dropped lay-\ners during evaluation (Figure 13 shows all tasks). ‘Stan-\ndard adapter’ is trained with no dropped layers.\ndropped layers—depending on the number of si-\nmultaneous tasks (Table 2).6 Training of our robust\nadapters is also more efﬁcient, which increases the\nspeed of training steps by 26%.7\n4 Efﬁciency of AdapterFusion\nAdapterFusion (Pfeiffer et al., 2021a) leverages the\nknowledge of several adapters from different tasks\nand learns an optimal combination of the adapters’\noutput representations for a single target task (see\nFigure 3). AdapterFusion (AF) is particularly use-\nful for small training sets where learning adequate\nmodels is difﬁcult. Despite its effectiveness, AF\nis computationally expensive because all included\nadapters are passed through sequentially.8\nTable 3 shows that the differences can be substan-\ntial for both training and inference. For instance,\ncompared to a fully ﬁne-tuned model, AF with\neight adapters is around 47% slower at training\ntime and 62% slower at inference.9\n5 AdapterDrop for AdapterFusion\nThere exists considerable potential for improving\nthe efﬁciency of AF, especially at inference time.\nWe address this with two variants of AdapterDrop\n6For more details see Appendix G.2\n7Every dropped adapter improves the speed of training\nsteps by 4.7% and we drop on average 5.5 adapters when\ntraining robust adapter models (more hyperparameter settings\nand details are given in Appendix G.2).\n8We also test AF with parallel operations and found no\nefﬁciency gains (see Appendix H).\n9All with Pfeiffer adapter and depending on the input size.\nWe provide more measurements in Appendix G.3.\n7933\nAF vs. Full FT AF vs. Adapter\nAdapters Training Inference Training Inference\n2 0.92 0.64 0.57 0.68\n8 0.53 0.38 0.33 0.40\n16 0.33 0.24 0.21 0.26\nTable 3: Relative speed of AdapterFusion (with 2/8/16\nadapters) compared to a fully ﬁne-tuned model and\ncompared to a single-task adapter (right). Measured\nwith a batch size of 32, and a sequence length of 128.\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nFF Down\nFF Up\nAdd & Norm\nEmb\nHead\n...\nEmb\nHead\n...\nFF Down\nFF Up\nQuery\nAdd & Norm\nSoftMax\nAdapter\nAdapterFusion\nAdd & Norm\nKeyValue\nEmb\nHead\n...\nEmb\nHead\n...\nForward Backward\nForward Backward\nFigure 3: Standard AdapterFusion vs. AdapterFusion\npruning, each with 3 adapters initially. The left model\nincludes all adapters at every layer whereas the right\nmodel has one adapter pruned at every layer.\nfor AF by (1) removing entire AF layers; (2) prun-\ning the least important adapters from AF models.\n5.1 Removing AdapterFusion Layers\nWe fuse the adapters from all eight GLUE tasks and\nobserve the largest gains of AF on RTE and CoLA.\nWe additionally train robust AF models with the\nsame procedure as in §3. We investigate from how\nmany lower layers we can remove AF at test time\nwhile still outperforming the corresponding single-\ntask adapter (without AdapterDrop).\nFigure 4 shows that AF performs better than the\n0 2 4 6 8 10\nDropped Layers\n0.6\n0.8Accuracy\nRTE\n0 2 4 6 8 10\nDropped Layers\n0.0\n0.2\n0.4\n0.6MCC\nCoLA\nStandard Fusion\nAdapterDrop Training\nSingle Task Adapter\nFigure 4: Comparison of AdapterFusion with (orange)\nand without (blue) AdapterDrop training during infer-\nence when omitting early AF layers.\n8 7 6 5 4 3 2 1\nIncluded Adapters\n75\n80\n85Accuracy\nRTE\n8 7 6 5 4 3 2 1\nIncluded Adapters\n62\n63\n64\n65MCC\nCoLA\nAdapterFusion Single Task Adapter\nFigure 5: Task performance of AdapterFusion Pruning.\nAF is trained with eight adapters, and we gradually re-\nmove the least important from the model.\nsingle-task adapter on RTE until removing AF from\nthe ﬁrst ﬁve layers. This improves the inference ef-\nﬁciency by 26%.10 On CoLA, we observe a differ-\nent trend. Removing AF from the ﬁrst layer results\nin more noticeable performance decreases, achiev-\ning lower task performances than the single-task\nadapter. This is in line with recent work showing\nthat some linguistic tasks heavily rely on infor-\nmation from the ﬁrst layers (Vuli ´c et al., 2020).\nWe deliberately highlight that AdapterDrop might\nnot be suitable for all tasks. However, Figure 13\nshows that CoLA represents the most extreme case.\nNevertheless, our results suggest that researchers\nneed to be cautious when removing AdapterFusion\nlayers as there may exist a considerable perfor-\nmance/efﬁciency tradeoff.\n5.2 AdapterFusion Pruning\nThe inference efﬁciency of AF largely depends on\nthe number of fused adapters, see Table 3. We\ncan, therefore, achieve efﬁciency improvements\nby pruning adapters from the trained AF models\n(depicted in Figure 3). Our hypothesis is that we\ncan safely remove adapters if they are not usually\nactivated by AF, which means that they do not\ncontribute much to the output representations. In\neach fusion layer, we record the average adapter\nactivations—their relative importance—using all\ninstances of the respective AF training set. We then\nremove the adapters with lowest activations.\nFigure 5 demonstrates that we can remove most\nadapters in AF without affecting the task perfor-\nmance. With two remaining adapters, we achieve\ncomparable results to the full AF models with eight\nadapters and improve the inference speed by 68%.\nWe therefore recommend performing Adaper-\nFusion pruning before deploying these models in\npractice. This is a simple yet effective technique\n10We include detailed measurements in Appendix G.4.\n7934\nto achieve efﬁciency gains even when aiming at\nmaintaining performance entirely.\n6 Conclusion\nAdapters have emerged as a suitable alternative\nto full model ﬁne-tuning, and their most widely\nclaimed computational advantage is the small\nmodel size. In this work, we have demonstrated\nthat the advantages of adapters go far beyond mere\nparameter efﬁciency. Even without our extensions,\nthe training steps of two common adapter architec-\ntures are up to 60% faster. However, these improve-\nments come at the cost of 4–6% slower inference\nspeed. Thus, if training is more important, adapters\ncan be advantageous over full model ﬁne-tuning.\nAdapterDrop expands these advantages by drop-\nping a variable number of adapters from lower\ntransformer layers. We dynamically reduce the\ncomputational overhead at run-time when perform-\ning inference over multiple tasks and maintain task\nperformances to a large extent. This beneﬁts re-\nsearchers working on models that need to make\nmultiple independent predictions on a single input.\nFinally, we also investigated the computational\nefﬁciency of AdapterFusion models. We ﬁnd\nthat dropping entire AdapterFusion layers comes\nat a considerable performance/efﬁciency tradeoff,\nwhereas pruning of the least activated adapters in\neach layer can improve the model efﬁciency while\nmaintaining performance entirely.\nWe believe that our work can be widely extended\nand that there exist many more directions to obtain\nefﬁcient adapter-based models. For instance, we\ncould explore more efﬁcient pre-trained adapters,11\nsharing the adapter weights across layers,12 or prun-\ning adapters from AdapterFusion at training time.13\nIn the Appendix to this paper, we present prelim-\ninary results for several related ideas, which may\nserve as a starting point for future work.\nAcknowledgments\nThis work has received ﬁnancial support from mul-\ntiple sources. (1) The German Federal Ministry of\n11In Appendix B, we evaluate MLM pre-trained adapters.\nOur results suggest that different strategies are necessary for\nadapters as compared to fully ﬁne-tuned transformers, which\ncan serve as a starting point for further experiments.\n12Appendix D shows that adapter with shared weights\nacross layers achieves comparable results to a standard adapter\nwhile drastically reducing the number of parameters.\n13Appendix E shows that we can randomly dropout 75%\nof the adapters during AdapterFusion training with a minimal\nimpact on the task performance.\nEducation and Research and the Hessian Ministry\nof Higher Education, Research, Science and the\nArts within their joint support of the National Re-\nsearch Center for Applied Cybersecurity ATHENE.\n(2) The European Regional Development Fund\n(ERDF) and the Hessian State Chancellery – Hes-\nsian Minister of Digital Strategy and Develop-\nment under the promotional reference 20005482\n(TexPrax). (3) The German Research Foundation\n(DFG) as part of the Research Training Group\nKRITIS No. GRK 2222. (4) The German Fed-\neral Ministry of Education and Research (BMBF)\nas part of the Software Campus program under\nthe promotional reference 01|S17050. (5) The\nLOEWE initiative (Hesse, Germany) within the\nemergenCITY center. (6) The German Research\nFoundation (DFG) as part of the UKP-SQuARE\nproject (grant GU 798/29-1). Finally, we gratefully\nacknowledge the support of NVIDIA Corporation\nwith the donation of the Titan X Pascal GPU used\nfor this research.\nReferences\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-\nbastian Ruder, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2021. MAD-G: Multilingual Adapter\nGeneration for Efﬁcient Cross-Lingual Transfer. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021.\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin,\nXin Jiang, Qun Liu, Michael Lyu, and Irwin King.\n2021. BinaryBERT: Pushing the limit of BERT\nquantization. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (ACL 2021), pages\n4334–4348.\nAnkur Bapna and Orhan Firat. 2019. Simple, Scalable\nAdaptation for Neural Machine Translation. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP 2019), pages 1538–1548.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language Models are Few-Shot\nLearners. arXiv preprint arXiv:2005.14165.\n7935\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL 2019), pages 4171–4186.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2020. Depth-adaptive transformer. In 8th\nInternational Conference on Learning Representa-\ntions (ICLR 2020).\nAngela Fan, Edouard Grave, and Armand Joulin.\n2020. Reducing Transformer Depth on Demand\nwith Structured Dropout. In 8th International Con-\nference on Learning Representations, (ICLR 2020).\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic\nBERT with adaptive width and depth. In 34th Con-\nference on Neural Information Processing Systems\n(NeurIPS 2020).\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzkeb-\nski, Bruna Morrone, Quentin de Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-Efﬁcient Transfer Learning for\nNLP. In Proceedings of the 36th International Con-\nference on Machine Learning (ICML 2019).\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal Language Model Fine-tuning for Text Classiﬁca-\ntion. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics, (ACL\n2018), pages 328–339.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In 8th Inter-\nnational Conference on Learning Representations\n(ICLR 2020).\nAnne Lauscher, Olga Majewska, Leonardo F. R.\nRibeiro, Iryna Gurevych, Nikolai Rozanov, and\nGoran Glavaš. 2020. Common sense or world\nknowledge? investigating adapter-based knowledge\ninjection into pretrained transformers. In Proceed-\nings of Deep Learning Inside Out (DeeLIO): The\nFirst Workshop on Knowledge Extraction and Inte-\ngration for Deep Learning Architectures, pages 43–\n49.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach . arXiv preprint arXiv:1907.11692.\nSinno Jialin Pan and Qiang Yang. 2010. A Survey on\nTransfer Learning. IEEE Transactions on Knowl-\nedge and Data Engineering, 22(10):1345–1359.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, (NAACL 2018), pages 2227–2237.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021a.\nAdapterFusion: Non-Destructive Task Composition\nfor Transfer Learning. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics (EACL 2021),\npages 487–503.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterHub: A Framework for Adapting Transform-\ners. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP 2020): Systems Demonstrations, pages 46–\n54.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-based\nFramework for Multi-task Cross-lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2020), pages 7654–7673.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021b. UNKs Everywhere: Adapting\nMultilingual Language Models to New Scripts. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Online, November , 2021.\nClifton Poth, Jonas Pfeiffer, Andreas Rücklé, and Iryna\nGurevych. 2021. What to pre-train on? efﬁcient\nintermediate task selection. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2021).\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving Language Under-\nstanding by Generative Pre-Training. Technical re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners. Tech-\nnical report, OpenAI.\nAndreas Rücklé, Jonas Pfeiffer, and Iryna Gurevych.\n2020. MultiCQA: Exploring the Zero-Shot Trans-\nfer of Text Matching Models on a Massive Scale. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2020), pages 2471–2486.\nSebastian Ruder. 2019. Neural Transfer Learning for\nNatural Language Processing. Ph.D. thesis, Na-\ntional University of Ireland, Galway.\n7936\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand PALs: Projected Attention Layers for Efﬁcient\nAdaptation in Multi-Task Learning. In Proceedings\nof the 36th International Conference on Machine\nLearning, (ICML 2019), pages 5986–5995.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and Policy Considerations for\nDeep Learning in NLP. In Proceedings of the 57th\nConference of the Association for Computational\nLinguistics, (ACL 2019), pages 3645–3650.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie\nLiu, Yiming Yang, and Denny Zhou. 2020. Mo-\nbileBERT: a Compact Task-Agnostic BERT for\nResource-Limited Devices. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL 2020), pages 2158–2170.\nLisa Torrey and Jude Shavlik. 2010. Transfer learn-\ning. In Handbook of research on machine learning\napplications and trends: algorithms, methods, and\ntechniques, pages 242–264. IGI Global.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, and\nGertjan van Noord. 2020. UDapter: Language\nAdaptation for Truly Universal Dependency Parsing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2020), pages 2302–2315.\nMarko Vidoni, Ivan Vuli ´c, and Goran Glavaš.\n2020. Orthogonal language and task adapters in\nzero-shot cross-lingual transfer. arXiv preprint\narXiv:2012.06460.\nIvan Vuli ´c, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavaš, and Anna Korhonen. 2020. Probing\nPretrained Language Models for Lexical Semantics.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2020), pages 7222–7240.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A Multi-Task Benchmark and Analysis Plat-\nform for Natural Language Understanding. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2021. K-Adapter: Infusing Knowl-\nedge into Pre-Trained Models with Adapters. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 1405–1418.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In 34th Conference on\nNeural Information Processing Systems (NeurIPS\n2020).\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics (ACL 2020), pages 2246–\n2251.\nA Measuring Computational and Task\nPerformance\nA.1 Computational Efﬁciency\nWe use Python 3.6, PyTorch 1.5.1, CUDA 10.1\nfor all measurements. We repeat them with\ntwo different GPUs: NVIDIA Tesla V100 PCIe\n(32GB) and a NVIDIA Titan X Pascal (12GB). We\nmake use of the torch.cuda.Event class and\ntorch.cuda.synchronize to measure only\nthe exact period of time of a training (or inference)\nstep.14 For both inference and training, we repeat\nthe respective step 300 times. We report the median\nto mitigate the impact of outliers caused by GPU\nwarmup.\nRelativ speed. We deﬁne the relative speed of\nan adapter compared full model ﬁne-tuning as: Sa\nSf\nwhere Sa and Sf are the time of one step with\nthe adapter model and the fully ﬁne-tuned model,\nrespectively. For example, a relative speed of 1.5\nmeans that the adapter model can perform 1.5 steps\nin the time the fully ﬁne-tuned model performs one\nstep.\nSpeedup. Speedup describes the positive change\nin relative speed of an adapter model when using\nAdapterDrop (or another method). A speedup of\np% means that the adapter model with Adapter-\nDrop requires only (1 − p/100)× of the runtime\nthan the adapter model without AdapterDrop.\nThe speedup of AdapterDrop (and AdapterFu-\nsion) are additive. If dropping one layer results in\np% speedup, dropping two layers results in 2p%\nspeedup, etc.\nA.2 Task Performances\nWe study the task performances of adapter mod-\nels on the popular GLUE benchmark (Wang et al.,\n2018). Following Devlin et al. (2019), we exclude\n14This is necessary due to the asynchronous nature of the\ncommand execution on CPU and GPU.\n7937\nthe WNLI because of the problematic data construc-\ntion.15 We perform our analyses using RoBERTa\nbase (Liu et al., 2019) as our pre-trained model and\nreport the mean and standard deviation over three\nruns of the best development performance evalu-\nated after every epoch. We train larger data sets\n(SST-2, MNLI, QNLI, and QQP) for 10 epochs and\nthe rest of the data sets for 20 epochs. We use a\nbatch size of 32 and, if not otherwise noted, the\ndefault hyperparameters for adapter ﬁne-tuning as\nin (Pfeiffer et al., 2021a).\nB Adapter Initialization and\nConvergence\nBesides measuring training and inference time, we\nare interested in (1) how using adapters compare\nto standard RoBERTa-base with regards to down-\nstream task convergence, and (2) if initializing\nadapters with pre-trained weights using masked\nlanguage modelingcan lead to faster convergence.\nFirst, we compare RoBERTa-base with adapter\nmodels using the architecture proposed by Pfeif-\nfer et al. (2021a). Second, we pretrain an adapter\nwith masked language modeling (MLM) using doc-\numents from the English Wikipedia.16 The results\nfor both experiments are visualized in Figure 12.\nWhen comparing RoBERTa-base with randomly\ninitialized adapters, We ﬁnd that adapters do not\ncome at the cost of requiring more training steps\nfor convergence (1). For several of the eight GLUE\ntasks, we observe similar convergence behavior\nwith the standard RoBERTa-base model and its\ncounterpart using adapters.\nFurther, we observe across all tasks that initial-\nizing the adapter weights with MLM pre-training\ndoes not have a substantial impact on the down-\nstream task convergence (compared to a randomly\ninitialized adapter). Thus, we ﬁnd no evidence that\npre-training of adapters with our masked language\nmodeling objective leads to better convergence per-\nformance in our experiments (2).\nC Detailed Results: AdapterDrop Task\nPerformances\nWe plot the detailed task performances of Adapter-\nDrop with the different training strategies in Fig-\nure 13. The relative differences of AdapterDrop to\n15See https://gluebenchmark.com/faq\n16We used a recent dump of English Wikipedia. We train\nwith a batch size of 64 and for 250k steps such that no sentence\nwas used twice.\na standard adapter with no AdapterDrop are given\nin Table 8.\nD Adapter with Cross-Layer Parameter\nSharing\nWe can further reduce the number of parameters\nrequired for each task by sharing the weights of the\nadapters across all transformer layers. This is simi-\nlar to weight sharing in ALBERT (Lan et al., 2020),\nbut specialized on adapters and can therefore be\napplied to a wide range of pre-trained models.\nWe use the Pfeiffer adapter architecture in our\nexperiments with the same hyperparameters as in\nAppendix A.2. Because cross-layer parameter shar-\ning reduces the capacity of adapter models, we\nstudy the impact of the adapter compression rate.\nThe compression rate refers to the down-projection\nfactor in the adapter’s bottleneck layer and thus\nimpacts the its capacity (the compression rate spec-\niﬁes by how much ‘FF Down’ in Figure 1 com-\npresses the representations). The standard com-\npression rate is 16, and smaller values result in a\nlarger model capacity.\nTable 6 shows that cross-layer parameter shar-\ning with the same compression rate of 16 largely\nmaintains the performance compared to separate\nweights with an average difference of 2.35%. With\na smaller compression rate of 4, we close this\ngap by more than 50% while still requiring 66%\nfewer parameters.17 The resulting models are light-\nweight: our shared adapter with a compression rate\nof 16 requires only 307KB storage space.\nE Training AdapterFusion with Dropout\nWe investigate the random dropout of adapters from\nAdapterFusion during training (using our eight task\nadapters as in §4) to improve the speed of train-\ning steps. Each layer randomly selects different\nadapters to drop out. This means that the model\nitself may still use the knowledge from all tasks,\nalthough not in the layers individually.\nTable 7 shows the results for the four small-\nest GLUE tasks in terms of training data size.\nThe speedup that we achieve with AdapterFusion\ndropout can be substantial: with a dropout rate of\n75% (i.e., dropping out 6 out of our 8 adapters)\neach training step is 74% faster on average (with a\nsequence length of 128, a batch size of 32). We ob-\nserve no clear trend in terms of task performances.\nFusion dropout leads to consistent decreases on\n17Even smaller compression rates do not yield similar gains.\n7938\nRTE and CoLA, only a small impact on STS-B (no\ndifference when dropping out 25% of adapters),\nand yields improvements on MRPC.\nThe effectiveness of Fusion dropout, thus, de-\npends on the individual downstream task. Nev-\nertheless, we believe that this methods could be\nsuitable, e.g., for resource-constrained settings.\nF Detailed Results: Removing\nAdapterFusion Layers\nThe computational overhead of AF can be re-\nduced during inference by decreasing the number\nof adapters. We investigate how dropping AF lay-\ners impacts the performance on the four smallest\nGLUE tasks (MRPC, STS-B, CoLA, RTE) and\nvisualize the results in Figure 7.\nIn this experiment we compare the performance\nof AF with and without AdapterDrop during train-\ning. For both, we use standard adapters as well\nas adapters created via AdapterDrop as basis for\nAF. Unsurprisingly, the performance of AF without\nAdapterDrop within the adapters or fusion drops\nfastest on all four datasets. Using AdapterDrop\nwhen creating the adapters, applying AdapterDrop\non AF, or the combination of both signiﬁcantly re-\nduces the performance drop when omitting fusion\nlayers during inference. On RTE and MRPC, multi-\nple AF layers can be omitted while still performing\nen par with or better compared to a single task\nadapter. We further ﬁnd this robustness to be task\ndependent. Even AF with AdapterDrop shows a\nsteep fall in performance on RTE and CoLA, while\nbeing relatively stable on MRPC and STS-B, even\nwith most layers omitted.\nG Detailed Efﬁciency Measurements\nIn this section, we present detailed results of our ef-\nﬁciency measurements for V100 and TitanX GPUs.\nG.1 Adapters\nWe present the efﬁciency results for adapters and\nfully ﬁne-tuned models in Figure 6, where we plot\nthe required time (absolute numbers) during train-\ning and inference. The relative speed of adapters\ncompared to fully ﬁne-tuned models is given in\nTable 9.\nG.2 AdapterDrop\nMulti-task inference. In Figure 8, we plot the\nspeed of adapters in a multi-task setting compared\nto fully ﬁne-tuned models with sequential process-\ning of inputs. In Table 11, we present the rel-\native speed of adapters in this setting and show\nthe speedup gained with AdapterDrop for each\ndropped layer. The average speedup in Table 2\nis calculated as the average speedup over the batch\nsizes 16, 32 and 64 in Table 11.\nTraining adapters with dropped layers. Table\n5 shows the speedup of AdapterDrop when training\na single adapter. The average speedup for training\nwith AdapterDrop is 4.7% per layer for the V100\nand 4.5% for the TitanX. This is the average result\nover batch sizes 16, 32, 64 and sequence length 64,\n128, 256, and 256 (see Table 5).\nG.3 AdapterFusion\nWe plot the speed of AdapterFusion with differ-\nent numbers of included adapters in Figure 9. In\nTable 10, we present the relative speed of Adapter-\nFusion compared to a fully-ﬁnetuned model and a\nmodel with one adapter. This also shows the com-\nputational overhead (slowdown) that results from\nadding more adapters to AdapterFusion.\nG.4 AdapterDrop for AdapterFusion\nTable 4 shows the speedup gained with Adapter-\nDrop for AdapterFusion during training and in-\nference. Figure 10 shows the required time as a\nfunction of the dropped layers.\nH Parallel Implementation of\nAdapterFusion\nAdapterHub’s implementation of AdapterFusion\npasses through each task adapter sequentially.\nWe hypothesized that a better efﬁciency can be\nachieved with parallel processing of adapters. We\nimplement the parallel computation of the different\nadapters by reformulation the linear layers as two\nconvolutions.\nThe ﬁrst convolution is a convolution with a ker-\nnel size equal to the hidden dimension of the trans-\nformer and output channels equal to the number of\nadapters times the downprojection dimension of the\nadapters. The second convolution is a grouped con-\nvolution18 which processes the channels in blocks\nthe size of the downprojection dimension. It out-\nputs channels equal to the number of adapters times\nthe hidden dimension.\n18Using the ’groups’ parameter in Pytorch ( https:\n//pytorch.org/docs/stable/generated/\ntorch.nn.Conv1d.html#torch.nn.Conv1d)\n7939\nSpeedup (per dropped layer)\nInference Training\nAdapters V100 TitanX V100 TitanX\n2 3.0% 3.1% 6.3% 6.4%\n4 4.0% 4.1% 6.8% 6.8%\n8 5.2% 5.2% 7.3% 7.3%\n16 6.3% 6.3% 7.8% -\nTable 4: The speedup for each dropped layer for\nAdapterFusion during training and inference. Mea-\nsurements were conducted with a batch size of 32 and\nsequence length of 128. Missing values are due to in-\nsufﬁcient GPU memory.\nSpeedup\nBatch Size Seq. Len V100 TitanX\n16 64 4.6% 4.4%\n16 128 4.6% 4.6%\n16 256 4.8% 4.6%\n16 512 4.7% -\n32 64 4.6% 4.5%\n32 128 4.7% 4.5%\n32 256 4.6% 4.7%\n32 512 4.8% -\n64 64 4.7% 4.5%\n64 128 4.6% 4.5%\n64 256 4.7% -\n64 512 - -\nTable 5: Speedup for each dropped layer during train-\ning with AdapterDrop on the V100 and TitanX.\nWe show in Figure 11 and in Table 12 that the\niterative implementation is faster than the parallel\nimplementation for larger input sizes (e.g., batch\nsizes greater than). This indicates that once the in-\nput can no longer be processed entirely in parallel\non the GPU (due to limited CUDA cores) the itera-\ntive implementation seems to be more efﬁcient.\nStandard Cross-Layer Parameter Sharing\nCompression rate = 16 1.33 4 16\nSST-2 94.7 ±0.3 94.2 ±0.3 94.2 ±0.1 94.1 ±0.4\nQNLI 93.0 ±0.2 92.4 ±0.1 93.1 ±0.1 90.6 ±1.4\nMNLI 87.3 ±0.1 87.0 ±0.1 87.1 ±0.0 86.2 ±0.2\nQQP 90.6 ±0.0 90.8 ±0.1 90.2 ±0.0 88.6 ±0.5\nCoLA 62.6 ±0.9 60.3 ±1.6 60.8 ±0.4 57.2 ±1.0\nMRPC 88.4 ±0.1 88.2 ±0.7 88.5 ±1.1 86.8 ±0.5\nRTE 75.9 ±2.2 69.4 ±0.5 71.5 ±2.7 71.5 ±1.0\nSTS-B 90.3 ±0.1 89.5 ±0.1 89.7 ±0.3 89.0 ±0.7\nAverage 85.35 83.98 84.39 83.0\nParams 884k 884k 295k 74k\nTable 6: Task performance scores of the standard ap-\nproach with separate adapter weights vs. cross-layer\nparameter sharing. The compression rate denotes the\nfactor by which ‘FF Down’ in Figure 1 compresses the\nrepresentations. The number of parameters is given\nwithout classiﬁcation heads.\nFusion Dropout\n0% 25% 50% 75%\nCoLA 63.9 ±0.6 62.9 ±0.8 62.4 ±0.7 60.4 ±0.2\nMRPC 88.4 ±0.1 89.2 ±0.5 89.2 ±0.4 89.3 ±0.1\nRTE 85.4 ±0.7 82.8 ±1.9 82.1 ±0.3 80.9 ±1.1\nSTS-B 90.2 ±0.1 90.2 ±0.1 90.1 ±0.1 89.9 ±0.1\nSpeedup (8) - 15.9% 39.4% 73.7%\nSpeedup (16) - 22.5% 58.2% 120.6%\nTable 7: Development scores of AdapterFusion (com-\npression rate 16x) with or without fusion dropout dur-\ning training. Fusion dropout of 50% means that each\nadapter has a 50% chance of not being used as input\nto the fusion layer. The speedup depends on the total\nnumber of adapters used in AdapterFusion (8 adapters\nin our setting here, 16 used by Pfeiffer et al. (2021a))\n7940\nDropped Layers\n0 1 2 3 4 5 6 7 8 9 10 11\nStandard adapter 100.0 98.5 97.1 95.3 92.0 89.0 82.2 74.6 64.5 54.5 49.3 43.3\nSpecialized AdapterDrop (12 models) 100.0 99.5 98.9 98.2 97.6 97.1 95.9 95.3 95.1 94.3 92.5 82.9\nRobust AdapterDrop 98.5 97.7 97.3 96.8 96.1 95.4 94.5 93.3 92.2 89.9 85.9 62.0\nTable 8: Model performances with AdapterDrop in relation to a standard adapter with no dropped layers. We\nreport the percentage of retained task performance compared to the standard adapter with no dropped layers during\nevaluation. The results are averaged over all eight GLUE task. A value of 97.1 for specialized AdapterDrop with\nﬁve dropped layers means that the model achieves 97.1% of the performance compared to the standard adapter\nwith no dropped layers. Performance scores for each task can be found in Figure 13.\nV100 TitanX\nSequence Len. Batch Size Training Inference Training Inference\nHoulsby Pfeiffer Houlsby Pfeiffer Houlsby Pfeiffer Houlsby Pfeiffer\n64 16 0.98 1.70 0.92 0.94 1.61 1.69 0.93 0.94\n64 32 1.70 1.81 0.94 0.95 1.48 1.55 0.93 0.94\n64 64 1.46 1.54 0.94 0.95 1.40 1.46 0.94 0.94\n64 128 1.48 1.55 0.95 0.96 1.37 1.42 0.94 0.94\n128 16 1.48 1.57 0.94 0.95 1.45 1.52 0.93 0.94\n128 32 1.53 1.60 0.94 0.95 1.38 1.44 0.94 0.95\n128 64 1.47 1.53 0.95 0.96 1.35 1.40 0.94 0.95\n128 128 1.42 1.48 0.95 0.96 - - - -\n256 16 1.42 1.49 0.94 0.95 1.34 1.38 0.94 0.95\n256 32 1.40 1.46 0.95 0.96 1.31 1.36 0.94 0.96\n256 64 1.40 1.45 0.95 0.96 - - - -\n256 128 - - - - - - - -\n512 16 1.36 1.41 0.96 0.96 - - - -\n512 32 1.33 1.37 0.96 0.96 - - - -\n512 64 - - - - - - - -\n512 128 - - - - - - - -\nTable 9: Relative speed of adapters compared to fully ﬁne-tuned models. Missing values are due to insufﬁcient\nGPU memory.\nV100 TitanX\nvs. FF vs. Adap. Slowdown vs. FF vs. Adap Slowdown\nSeq. Len Batch Size Tr. Inf. Tr. Inf. Tr. Inf. Tr. Inf. Tr. Inf. Tr. Inf.\n64 16 0.77 0.62 0.45 0.66 8.2% 10.6% 0.88 0.62 0.52 0.66 10.3% 10.2%\n64 32 1.03 0.64 0.57 0.68 12.0% 11.1% 0.80 0.61 0.52 0.64 11.2% 11.0%\n64 64 0.87 0.64 0.57 0.67 12.6% 12.0% 0.76 0.61 0.52 0.65 11.6% 11.4%\n128 16 0.91 0.65 0.58 0.69 12.0% 11.0% 0.80 0.61 0.53 0.65 10.9% 10.8%\n128 32 0.92 0.64 0.57 0.68 12.5% 11.8% 0.76 0.62 0.53 0.66 11.4% 11.1%\n128 64 0.87 0.65 0.57 0.68 12.5% 11.6% - - - - - -\n256 16 0.88 0.66 0.59 0.69 12.1% 11.3% 0.77 0.65 0.56 0.68 10.8% 10.4%\n256 32 0.86 0.68 0.59 0.70 11.9% 11.3% - - - - - -\n256 64 - - - - - - - - - - - -\n512 16 0.87 0.69 0.62 0.72 11.2% 10.1% - - - - - -\n512 32 - - - - - - - - - - - -\n512 64 - - - - - - - - - - - -\nTable 10: Relative speed of AdapterFusion for different sequence lengths and batch sizes. We compute the\ntraining (Tr.) speed and inference ( Inf.) speed with two adapters in AdapterFusion. We compare this to: FF, a\nfully ﬁne-tuned model; Adap, an adapter model (Pfeiffer architecture). The slowdown denotes the computational\noverhead of each additional adaptercomposed in AdapterFusion (calculated as the average slowdown for adding\none adapter to AF consisting of 2–16 adapters). Missing values are due to insufﬁcient GPU memory.\n7941\n50 100 150 200 250 300\n0\n2\n4\n6\n8\n10\n12\n14\nbatch size:1\n50 100 150 200 250 300\n0\n10\n20\n30\n40\n50\nbatch size:8\n50 100 150 200 250 300\n0\n20\n40\n60\n80\n100\nbatch size:16\n50 100 150 200 250 300\n0\n50\n100\n150\n200\nbatch size:32\n50 100 150 200 250 300\n0\n100\n200\n300\n400\nbatch size:64\n50 75 100 125 150 175 200\n0\n100\n200\n300\n400\n500\nbatch size:128\n0.0 0.2 0.4 0.6 0.8 1.0\nSequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Inference Time in ms\nno adapter pfeiffer houlsby\n(a) V100 Inference\n50 100 150 200 250 300\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nbatch size:1\n50 100 150 200 250 300\n0\n50\n100\n150\nbatch size:16\n50 100 150 200 250\n0\n50\n100\n150\n200\n250\nbatch size:32\n60 80 100 120 140\n0\n50\n100\n150\n200\n250\n300\nbatch size:64\n0.0 0.2 0.4 0.6 0.8 1.0\nSequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Inference Time in ms\nno adapter pfeiffer houlsby\n(b) TitanX Inference\n50 100 150 200 250 300\n0\n20\n40\n60\n80\nbatch size:1\n50 100 150 200 250 300\n0\n25\n50\n75\n100\n125\n150\n175\nbatch size:8\n50 100 150 200 250 300\n0\n50\n100\n150\n200\n250\n300\nbatch size:16\n50 100 150 200 250 300\n0\n100\n200\n300\n400\n500\n600\nbatch size:32\n50 100 150 200 250 300\n0\n200\n400\n600\n800\n1000\nbatch size:64\n0.0 0.2 0.4 0.6 0.8 1.0\nSequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Training Step in ms\nno adapter pfeiffer houlsby\n(c) V100 Training\n50 100 150 200 250 300\n0\n10\n20\n30\n40\n50\n60\nbatch size:1\n50 100 150 200 250 300\n0\n100\n200\n300\n400\nbatch size:16\n50 100 150 200 250\n0\n200\n400\n600\nbatch size:32\n60 80 100 120 140\n0\n200\n400\n600\n800\nbatch size:64\n0.0 0.2 0.4 0.6 0.8 1.0\nSequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Training Step in ms\nno adapter pfeiffer houlsby\n(d) TitanX Training\nFigure 6: The absolute time for each inference or training step. We compare a transformer model without\nadapters and an adapter model with Pfeiffer or Houlsby architectures. We note that for small inputs, i.e., batch\nsize 1 or 8, the time does not increase with the sequence length because the GPU is not working at capacity. Figure\n(b) with batch size 1 shows the transition from working under and working at capacity.\n7942\n0 2 4 6 8 10\nDropped Layers\n0.6\n0.8Accuracy\nRTE\n0 2 4 6 8 10\nDropped Layers\n0.0\n0.2\n0.4\n0.6MCC\nCoLA\n0 2 4 6 8 10\nDropped Layers\n0.4\n0.6\n0.8Accuracy\nMRPC\n0 2 4 6 8 10\nDropped Layers\n0.00\n0.25\n0.50\n0.75Spearman\nSTS-B\nStandard Fusion\nAdapterDrop Training\nSingle Task Adapter\nAdapter Training\nStandard\nAdapterDrop\nFigure 7: Performance of AF by the number of dropped\nAF layers. We show the results for AF and the used\nadapters (both with and without AdapterDrop), and\ncompare the performance with a standard single task\nadapter.\nDevice Batch Size Adapters Inference Speedup\nV100\n1 2 1.25 2.6%\n1 4 1.97 3.7%\n1 8 2.80 4.9%\n1 16 2.97 6.5%\n16 2 1.13 4.1%\n16 4 1.14 6.5%\n16 8 1.20 7.7%\n16 16 1.16 8.4%\n32 2 1.08 4.5%\n32 4 1.14 6.6%\n32 8 1.11 7.9%\n32 16 1.11 8.5%\n64 2 1.08 4.3%\n64 4 1.05 6.7%\n64 8 1.06 7.9%\n64 16 1.06 8.4%\nTitanX\n32 2 1.07 4.4%\n32 4 1.09 6.6%\n32 8 1.09 7.8%\n32 16 1.06 8.4%\nCPU\n1 2 0.98 4.2%\n1 4 1.03 6.5%\n1 8 1.05 7.7%\n1 16 1.06 8.4%\nTable 11: The relative inference speed of simultaneous\nprocessing of multiple tasks with adapters compared\nto sequential processing of tasks with fully ﬁne-tuned\nmodels. Gray columns show the speedup of Adapter-\nDrop for every additional dropped layer. All measure-\nments use a sequence length of 128. Batch size 1 for the\nV100 is an outlier in both speedup and relative speed\ncompared to the other results due to the small input size\n(compare with Figure 8).\nRel. Speed\nAdapters Seq. Len Batch Size V100 TitanX\n2 100 1 0.93 0.94\n3 100 1 0.89 0.88\n5 100 1 0.77 0.76\n10 100 1 0.60 1.29\n2 100 16 1.02 1.44\n3 100 16 1.12 1.58\n5 100 16 1.17 1.80\n10 100 16 1.27 2.14\n2 100 32 1.01 1.48\n3 100 32 1.17 1.62\n5 100 32 1.23 1.85\n10 100 32 1.32 2.24\n2 200 1 0.93 1.24\n3 200 1 0.88 1.37\n5 200 1 0.77 1.55\n10 200 1 0.52 1.87\n2 200 16 1.01 1.46\n3 200 16 1.17 1.59\n5 200 16 1.23 1.82\n10 200 16 1.32 2.21\n2 200 32 1.00 1.11\n3 200 32 1.18 1.17\n5 200 32 1.26 -\n10 200 32 1.34 -\n2 300 1 0.93 1.37\n3 300 1 0.88 1.50\n5 300 1 0.91 1.70\n10 300 1 0.94 2.03\n2 300 16 1.00 1.48\n3 300 16 1.16 1.63\n5 300 16 1.22 1.88\n10 300 16 1.32 -\n2 300 32 1.00 -\n3 300 32 1.20 -\n5 300 32 1.27 -\n10 300 32 1.36 -\n2 400 1 1.04 1.39\n3 400 1 1.09 1.51\n5 400 1 1.10 1.74\n10 400 1 1.10 2.08\n2 400 16 1.00 -\n3 400 16 1.18 -\n5 400 16 1.25 -\n10 400 16 1.34 -\n2 400 32 1.00 -\n3 400 32 1.20 -\n5 400 32 1.27 -\n10 400 32 - -\nTable 12: Relative speed ofAdapterFusion with the it-\nerative implementation versus the parallel implemen-\ntation with different batch sizes, sequence lengths and\nnumbers of adapters for the V100 and TitanX. The par-\nallel implementation is faster if the input is sufﬁciently\nsmall (batch size 1 or 2 adapters) as the GPU is not\nworking at capacity and is able to use the parallel im-\nplementation.\n7943\n0 2 4 6 8 10\n2000\n4000\n6000\n8000\n10000\n12000Inference Time in ms\nbatch size: 1 (CPU)\n0 2 4 6 8 10\n0\n25\n50\n75\n100\n125\nbatch size: 1 (V100)\n0 2 4 6 8 10\n0\n500\n1000\n1500\n2000Inference Time in ms\nbatch size: 32 (TitanX)\n0 2 4 6 8 10\n0\n200\n400\n600\n800\n1000\n1200\nbatch size: 32 (V100)\n0.0 0.2 0.4 0.6 0.8 1.0\nDropped Layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2 adapter\n4 adapter\n8 adapter\n16 adapter\nParallelized N FF models\nFigure 8: The absolute time required for performing inference for multiple tasks on the same input. The measure-\nments are conducted with a sequence length of 128. N FF models denotes N fully ﬁne-tuned models, executed\nsequentially. Parallelized denotes the time required by N fully ﬁne-tuned models running fully parallelized. Batch\nsize 1 on the V100 is an outlier compared to the other results with a smaller speedup for each dropped layer but a\nhigher relative speed compared to the ﬁne-tuned models due to the small input size.\n2 4 6 8 10 12 14 16\n0\n50\n100\n150\n200\n250\n300Inference Time in ms\nInference\n2 4 6 8 10 12 14 16\n0\n200\n400\n600\n800Training Step in ms\nTraining\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of adapters\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdapterFusion No adapter 1 adapter (no fusion)\n(a) V100\n2 4 6 8 10 12 14 16\n0\n50\n100\n150\n200\n250Inference Time in ms\nInference\n2 4 6 8 10 12 14 16\n0\n100\n200\n300\n400\n500\n600Training Step in ms\nTraining\n0.0 0.2 0.4 0.6 0.8 1.0\nNumber of adapters\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAdapterFusion No adapter 1 adapter (no fusion)\n(b) TitanX\nFigure 9: Absolute time measurements for AdapterFusion at inference (left) and training (right) as a function of\nthe number of adapters. The measurements were conducted with a batch size of 32 (V100) and 16 (TitanX), and a\nsequence length of 128.\n7944\n0 2 4 6 8 10\n0\n50\n100\n150\n200\n250\n300Inference Time in ms\nInference\n0 2 4 6 8 10\n0\n200\n400\n600Training Step in ms\nTraining\n0.0 0.2 0.4 0.6 0.8 1.0\nDropped layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2 adapter 4 adapter 8 adapter 16 adapter no adapter 1 adapter (no fusion)\n(a) V100\n0 2 4 6 8 10\n0\n100\n200\n300\n400Inference Time in ms\nInference\n0 2 4 6 8 10\n0\n200\n400\n600\n800\n1000\n1200Training Step in ms\nTraining\n0.0 0.2 0.4 0.6 0.8 1.0\nDropped layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n2 adapter 4 adapter 8 adapter 16 adapter no adapter 1 adapter (no fusion)\n(b) TitanX\nFigure 10: Absolute time measurements for AdapterFusion with AdapterDrop at inference (left) and training\n(right) as a function of the number of dropped layers. The measurements were conducted with a batch size of 32\nand a sequence length of 128. We additionally plot the time of an adapter (without AdapterDrop) and a model\nwithout adapters to provide a more thorough comparison.\n100 200 300 400 500\n5\n0\n5\n10\n15\nbatch size: 1\n50 100 150 200 250 300\n200\n150\n100\n50\n0\nbatch size: 32\n0.0 0.2 0.4 0.6 0.8 1.0\nSequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Delta Inference Time\n2 adapter 3 adapter 5 adapter 10 adapter\n(a) V100\n100 200 300 400 500\n60\n40\n20\n0\nbatch size: 1\n50 75 100 125 150 175 200\n300\n200\n100\nbatch size: 16\n0.0 0.2 0.4 0.6 0.8 1.0\nSequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Delta Inference Time\n2 adapter 3 adapter 5 adapter 10 adapter\n(b) TitanX\nFigure 11: The difference in inference time between iterative and parallel implementations of AdapterFusion.\nNegative values indicate that the iterative implementation is faster. We calculate the difference as ti − tp, where\nti, tp are the times for iterative and parallel implementation, respectively. In Figure (a), the parallel implementation\nis faster if the input is sufﬁciently small as the GPU is not working at capacity and is able to use the parallel\nimplementation.\n7945\nFigure 12: Evaluation performance of ﬁne-tuning RoBERTA-base in comparison with different initialization strate-\ngies for adapters (randomly initialized vs. pre-trained on masked language modeling task). Training was conducted\nfor 10k steps with a learning rate of 5e-05 for RoBERTa-base and 0.0001 for adapters, respectively.\n7946\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n30\n40\n50\n60\n70\n80\n90Accuracy\nMNLI\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n40\n50\n60\n70\n80\n90Accuracy\nQQP\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n50\n60\n70\n80\n90Accuracy\nSST-2\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n50\n60\n70\n80\n90Accuracy\nQNLI\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n0\n20\n40\n60\n80Spearman \nSTS-B\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n40\n50\n60\n70\n80\n90Accuracy\nMRPC\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n0\n20\n40\n60MCC\nCoLA\n0 1 2 3 4 5 6 7 8 9 10 11\nAdapterDrop (layers)\n50\n60\n70\n80Accuracy\nRTE\n12 specialized adapters\nRobust adapter\nStandard adapter\nFigure 13: The AdapterDrop task performances for all eight GLUE tasks in relation to the dropped layers. ‘12 spe-\ncialized adapters’ refers to the performance of indiviudal models trained for each AdapterDrop setting separately\n(i.e., 12 models); ‘Standard adapter’ refers to the adapter that is trained with no dropped layers; AdapterDrop\ntraining refers to the adapter that is trained with our proposed training procedure.",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.8816020488739014
    },
    {
      "name": "Transformer",
      "score": 0.8239222764968872
    },
    {
      "name": "Computer science",
      "score": 0.7689869403839111
    },
    {
      "name": "Task (project management)",
      "score": 0.5695370435714722
    },
    {
      "name": "Massively parallel",
      "score": 0.4715847074985504
    },
    {
      "name": "Training set",
      "score": 0.4427235722541809
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41849708557128906
    },
    {
      "name": "Machine learning",
      "score": 0.3721989691257477
    },
    {
      "name": "Parallel computing",
      "score": 0.15484929084777832
    },
    {
      "name": "Engineering",
      "score": 0.1284959316253662
    },
    {
      "name": "Electrical engineering",
      "score": 0.0912041962146759
    },
    {
      "name": "Voltage",
      "score": 0.0747307538986206
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31512782",
      "name": "Technical University of Darmstadt",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I32021983",
      "name": "Christian-Albrechts-Universität zu Kiel",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I206945453",
      "name": "Paderborn University",
      "country": "DE"
    }
  ]
}