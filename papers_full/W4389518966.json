{
  "title": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models",
  "url": "https://openalex.org/W4389518966",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4307311814",
      "name": "Kazemnejad, Amirhossein",
      "affiliations": [
        "Mila - Quebec Artificial Intelligence Institute",
        "McGill University"
      ]
    },
    {
      "id": "https://openalex.org/A4224409695",
      "name": "Rezagholizadeh, Mehdi",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A4287194065",
      "name": "Parthasarathi, Prasanna",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A3188554438",
      "name": "Chandar, Sarath",
      "affiliations": [
        "Canadian Institute for Advanced Research",
        "Mila - Quebec Artificial Intelligence Institute",
        "Polytechnique Montréal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4285239949",
    "https://openalex.org/W4286903575",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4306384900",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4385573594",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4283765342",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3175870271",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3124034626",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W4385572404",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4229005866"
  ],
  "abstract": "While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM’s parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model’s possessed knowledge, avoiding confounding factors like insufficient signal. As an instantiation, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps - in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Overall, our study provides insights into PLMs’ capabilities beyond their acquired knowledge.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4305–4319\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMeasuring the Knowledge Acquisition-Utilization Gap in Pre-trained\nLanguage Models\nAmirhossein Kazemnejad1,2 Mehdi Rezagholizadeh3\nPrasanna Parthasarathi3† Sarath Chandar2,4,5†\n1McGill University; 2Mila - Quebec AI; 3Huawei Noah’s Ark Lab;\n4École Polytechnique de Montréal; 5Canada CIFAR AI Chair;\namirhossein.kazemnejad@mail.mcgill.ca\nAbstract\nWhile pre-trained language models (PLMs)\nhave shown evidence of acquiring vast amounts\nof knowledge, it remains unclear how much of\nthis parametric knowledge is actually usable in\nperforming downstream tasks. We propose a\nsystematic framework to measure parametric\nknowledge utilization in PLMs. Our framework\nfirst extracts knowledge from a PLM’s parame-\nters and subsequently constructs a downstream\ntask around this extracted knowledge. Perfor-\nmance on this task thus depends exclusively\non utilizing the model’s possessed knowledge,\navoiding confounding factors like insufficient\nsignal. Employing this framework, we study\nfactual knowledge of PLMs and measure uti-\nlization across 125M to 13B parameter PLMs.\nWe observe that: (1) PLMs exhibit two gaps\n- in acquired vs. utilized knowledge, (2) they\nshow limited robustness in utilizing knowledge\nunder distribution shifts, and (3) larger mod-\nels close the acquired knowledge gap but the\nutilized knowledge gap remains.\n1 Introduction\nRecent research has demonstrated that language\nmodels pre-trained on vast amounts of internet data\nacquire a broad range of knowledge about linguis-\ntic structures (Tenney et al., 2019b; Blevins et al.,\n2022), encyclopedic relations (Petroni et al., 2019;\nHao et al., 2022), levels of commonsense (Zhou\net al., 2020; Liu et al., 2022a) , and even coding\nand reasoning rules (Chen et al., 2021; Wei et al.,\n2022b). Recent studies on behavioral parametric\nprobing and prompting (Jiang et al., 2020; Qin and\nEisner, 2021; Brown et al., 2020) has demonstrated\nthat such knowledge, collectively referred to as\n“parametric knowledge,” resides reliably within a\nsubset of trained parameters in pre-trained models\n(PLMs). Importantly, this knowledge can be iden-\ntified without additional finetuning. For instance,\n†Equal advising.\n125M 350M 1.3B 2.7B 7B 13B\n0\n0.2\n0.4\n0.6\n0.8\n1\nFraction of Encyclopedic Facts\nOPT\nIdentiﬁable Usable\n7B 13B\nNumber of Parameters (log scale)\nLLaMa\nFigure 1: Parametric knowledge of PLMs Gap 1\nrepresents the missing facts in the model’s parametric\nknowledge (what the model knows). Gap 2 exists in\nhow much of this knowledge can actually be utilized\nin downstream tasks (the usable knowledge). We find\nthat although the first gap mostly shrinks, the second\nremains as we increase the model’s size.\ngiven the prompt “The capital of France is”,\na PLM can be queried to complete the input and\nextract the fact “Paris”.\nA common assumption about parametric knowl-\nedge is that if the model poses a certain type of\nknowledge, it utilizes it when performing down-\nstream tasks related to that knowledge. For exam-\nple, if a model knows about X and Y (such that X\nand Y are similar), and is taught to perform a task\non X, the convention is that the model generalizes\nthe application of the task onY and all other similar\nknowledge. Such is the foundation for the recent\ninterest in instruction tuning (Wei et al., 2022a;\nChung et al., 2022), and the SFT-RLHF pipeline\n(Ouyang et al., 2022). In this paradigm, LLMs are\nfinetuned to learn how to follow instructions on a\nfew tasks the model is capable of and are subse-\nquently expected to generalize and follow instruc-\ntions for novel tasks by utilizing their pre-training\nknowledge (residing in their parameters).\nHowever, it is not clear to what extent this as-\n4305\nPLM\nMθ\nModel’s\nParametric Knowledge (Dθ)\n. . .\n⟨Barack Obama,graduated_from,Harvard⟩\n. . .\n. . .\n⟨Alen Turing,born_in,London⟩\n. . .\nCreate\nDown-\nstream\nTask\nDownstream Task Consistent\nwith Model’s Knowledge (Kθ)\n. . .\nWhere did Barack Obama graduate from?\nObama graduated from Harvard.\nObama went to Stanford.\nBill Gates studied at Harvard.\nObama left Harvard.\n. . .\n. . .\n. . .\nWhere is the Alan Turing’s birthplace?\nParis was the birthplace of Alan Turing\nAlan Turing was born in London.\n. . .\nSteve Jobs originated from London\nMahatma Gandhi died in London.\n. . .\n. . .\nKθ\ntrain\nKθ\ntest\nFigure 2: XTRAEVAL Framework: (1) From a pretrained LM, Mθ, the model’s parametric knowledge are\nextracted as Dθ. (2) Following which downstream task training and test split, Kθ\ntrain and Kθ\ntest, are created from Dθ.\n(3) The evaluation on the application of acquired knowledge is estimated through the performance on the test split,\nafter finetuning Mθ on the downstream task.\nsumption holds in practice, giving rise to a central\nquestion: how much of parametric knowledge will\nget applied in downstream tasks? If the causal link\nbetween \"identifiable knowledge\" and its practical\napplication in downstream tasks is not established\n(Kulmizev and Nivre, 2021), the mere presence of\nknowledge within a model’s parameters does not\nnecessarily guarantee its utilization in such tasks.\nThis raises questions about the assertion of pre-\ntrained language models (PLMs) as differentiable\nknowledge bases (Hao et al., 2022) and their overall\ncapabilities. For instance, as demonstrated by Qin\net al. (2023), ChatGPT’s performance lags behind\nits foundational model, GPT-3.5, in areas including\ncommonsense and logical reasoning tasks.\nPrevious studies have investigated this question\nwithin linguistic domains and have demonstrated\nthat although PLMs have the capacity to encode\nlinguistic knowledge, they may not effectively em-\nploy it in downstream tasks. For example, McCoy\net al. (2019) illustrates that PLMs employ syntactic\nheuristics to solve NLI even though they are able\nto represent proper linguistic hierarchies (Tenney\net al., 2019a), even after finetuning (Merchant et al.,\n2020; Zhou and Srikumar, 2022). Warstadt et al.\n(2020) provide evidence that RoBERTa requires\ndata inoculation or pre-training with extensive data\nin order to effectively utilize its hierarchical lin-\nguistic knowledge. In a more recent study, Lover-\ning et al. (2021) demonstrate that the quantity of\n“evidence” presented in the finetuning dataset in-\nfluences the features that PLMs rely on during the\nfinetuning process. Specifically, the model may re-\nsort to lexical heuristics when the finetuning signal\ntoward linguistic features is insufficient.\nIn this work, we are interested in a more general\nsense of knowledge and propose XTRAEVAL —\nEXTRACT , TRAIN , AND EVALUATE — to system-\natically measure how much of parametric knowl-\nedge is utilized in downstream tasks. XTRAEVAL\nsidesteps potential confounders (such as shortcuts\nor insufficient signal) that arise from the nature of\narbitrary crowd-sourced tasks used in prior work\nby carefully creating the downstream task from\nthe model’s own knowledge. Specifically, given\na pre-trained language model, our framework first\nidentifies and extracts knowledge residing in its pa-\nrameters. Subsequently, using the extracted knowl-\nedge, we construct a downstream task on which\nwe finetune the model. Finally, we measure knowl-\nedge utilization based on its performance on the\ndownstream task. By constructing the task based\non the model’s pre-existing knowledge, we ensure\nthat (1) the model is evaluated solely on its pos-\nsessed knowledge, avoiding penalties for lacking\ninformation and (2) successful task completion re-\nlies explicitly on utilizing the model’s parametric\nknowledge, eliminating the insufficient training sig-\n4306\nnal issue and dataset shortcuts.\nIn this paper, we provide the first instantiation\nof this paradigm based on encyclopedic knowledge\nfacts and conduct an extensive study to measure\nknowledge utilization of PLMs across a wide range\nof parametric scales (ranging from 125M to 13B).\nWe observe the following:\n• PLMs show two different but equally impor-\ntant gaps: (1) The gap in the acquired knowl-\nedge and (2) and the gap in parametric knowl-\nedge that can be actively applied to down-\nstream tasks (Section 3).\n• PLMs are not robust to finetuning distribution\nshifts, and failure to utilize knowledge wors-\nens with such shifts, further questioning their\ngeneralization capabilities (Section 4).\n• Although scaling the number of parameters\nhelps to close the first gap, the second still\nremains in larger sizes (Section 5).\nIn the next sections, we first describe our frame-\nwork and its instantiation in detail (Section 2),\nand finally present our experimental results in Sec-\ntions 3 to 5.\n2 Framework\n2.1 EXTRACT , TRAIN , AND EVALUATE\nPrinciples The primary objective of our evalu-\nation framework is to measure how much of the\nknowledge present in the model’s parameters is\nactually usable in downstream tasks. Ideally, down-\nstream tasks must be designed in a way that solely\nattributes any success to the model’s knowledge\nbeing used, while ensuring that failure in perform-\ning the task is not due to a lack of pre-training\nknowledge.\nThe Paradigm To this end, we propose EX-\nTRACT , TR AIN , AND EVALUATE , which consists\nof three main steps:\nStep 1. Given a pre-trained model Mθ with\nparameters θ and a diagnostic dataset D(e.g. a\nset of encyclopedic facts or coding problems), we\nfirst extract and identify parametric knowledge as\na set of data instances x∈D the model can solve\nwithout further training (zero-shot). We denote\nsuch a set as Dθ, a realization of Mθ’s parametric\nknowledge w.r.tD.\nStep 2. We construct a downstream task K\naround the model’s own knowledge Dθ (e.g. fact\nretrieval or following instructions in coding) such\nthat the model can only solve the task by utilizing\nthe knowledge identified in the first step. More\nformally, we create Kθ\ntrain and Kθ\ntest as the non-\noverlapping train and test sets of downstream task\nK, where the model learns the task from Kθ\ntrain.\nStep 3. Finally, the performance on the test set\nKθ\ntest is used as a measure of the model’s ability to\nutilize its knowledge.\nConstructing the downstream task based on the\nmodel’s knowledge ensures that the model is not\nevaluated on the knowledge it did not acquire dur-\ning pre-training. Also, the I.I.D. nature of this\nparadigm (i.e. the model is only exposed to inputs\nit is already familiar with) allows us to measure\nwhether the model can utilize its knowledge at all.\n2.2 Encyclopedic Knowledge\nFactual parametric knowledge as in encyclopedic\nfacts is well-studied in PLMs (Petroni et al., 2019;\nJiang et al., 2020) and allows for an objective and\nsystematic evaluation of our framework (Figure 2).\nTherefore, in this paper, we instantiateXTRAEVAL\nto measure the utilization of parametric knowledge\nconcerning encyclopedic facts. In this case, the\ndiagnostic dataset Dis a set of encyclopedic facts\nD= {⟨h,r,t⟩i}n\ni=1 acquired from an off-the-shelf\nknowledge base (e.g. Wikipedia). Each fact xi ∈\nDis a tuple of the form⟨head,relation,tail⟩, such\nas ⟨Barack Obama,GraduatedFrom,Harvard⟩.\nIn the extraction phase, a pre-trained model Mθ\nhas to zero-shot predict the tail entity tgiven the\nhead entity h and the relation r. We use soft-\nprompting (Qin and Eisner, 2021) to obtain the\nmodel’s predictions, as it enhances prediction con-\nsistency compared to discrete prompts, particularly\nfor moderate-sized models. The extracted knowl-\nedge Dθ ⊂D is the subset of tuples the model can\npredict correctly.\nOur downstream task Kis a standard document\nretrieval task (Karpukhin et al., 2020). Given a\nquery q, the model retrieves the relevant document\nfrom a set of candidates. We construct Kθ from the\nextracted knowledge in Dθ by converting each fact\nx ∈ Dθ into a retrieval instance k ∈ Kθ. This\nconditions the downstream task on the model’s\nknowledge. The conversion generates a query q\nby removing the tail entity t from x. It then gen-\nerates relevant and irrelevant documents using a\nstochastic generator\nd∼P(d|H = h,R = r,T = t), (1)\n4307\nEncyclopedic Fact:\nx=⟨h,r,t⟩=⟨Barack Obama,GraduatedFrom,Harvard⟩\nInput Sampled Document\n(h,r,t) Barack Obama graduated from Harvard.\nGold document (d+)\n(h,r,·) Barack Obama earned a degree from Stanford.\nRandomly replacing the tail entity.\n(·,r,t) Bill Gates received his degree from Harvard.\nRandomly replacing the head entity.\n(h,·,t) Barack Obama was born in Harvard.\nRandomly replacing the relation.\n(·,·,t) Steve Jobs died in Harvard.\nKeeping the tail entity and sampling others entities.\n(·,r,·) McGill is the alma mater of Justin Trudeau.\nKeeping the relation and sampling others entities.\n(h,·,·) Barack Obama is located in London.\nKeeping the head entity and sampling others entities.\n(·,·,·) Michael jordan was a football player by profession.\nUnconditional sampling.\nTable 1: All possible inputs to the document generator\nP(d |H,R,T ) per each fact x and examples of the\ncorresponding sampled documents. The dot means that\nthe corresponding entity or relation is not given, and the\ndocument generator will randomly choose it from Dθ.\nThe gray text provides an explanation of the sampled\ndocument. Note that we do not force the document\ngenerator to generate a factual document and the model\nitself has to predict the relevancy of each document.\nwhere ddepends on the head entity h, relation r,\nand tail entity t. The document generator, P(d|·),\nselects a template at random and fills in the blanks\nwith the input entities. If H, R, or T are missing,\nthe generator chooses a random entity from Dθ\nto complete the input. Specifically, we generate\nrelevant document d+ by sampling from P(d |·)\nwith gold entities inxas input, and create irrelevant\ndocuments d− by omitting one or more entities.\nEach k comprises a tuple (q,{d+,d−\n1 ,...,d −\nm}),\nwhere mis the number of irrelevant documents.\nWe partition Dθ randomly (60%-40%) to gen-\nerate Kθ\ntrain and Kθ\ntest, which serve as the training\nand test sets for the downstream task, respectively.\nWe finetune the model on Kθ\ntrain in cross-encoder\nsetup (Nogueira and Cho, 2020) with the InfoNCE\nobjective (van den Oord et al., 2019):\nL(k) =−log exp(sim(q,d+))∑\nd∈{d+,d−\n1 ,...,d−\nm}exp(sim(q,d)).\nThe similarity score sim(.,) is computed as\nsim(q,d) =h(Mθ([CLS]; q; d)),\nwhere his a randomly initialized value head that\ntakes the representation of the [CLS] token (or the\nlast token for decoder-only models) and outputs a\nscalar as the similarity measure (Figure A.1). Fi-\nnally, we evaluate the model onKθ\ntest by measuring\nits accuracy in retrieving the relevant document d+\namong {d+,d−\n1 ,...,d −\nm}for a given query q.\nThe task design ensures that the association be-\ntween knowledge query qi and gold fact document\nd+\ni relies solely on the parametric knowledge repre-\nsented by xi ∈Dθ This is because other variables,\nlike text overlap, are randomly sampled from the\nsame distribution for both query and documents.\nThus, the model can only solve the task by uti-\nlizing its internal knowledge. Finetuning on Kθ\ntrain\nshould only trigger the utilization of the parametric\nknowledge.\nTraining The document generator P(d |·) can\ngenerate various types of documents for each fact\nx∈Dθ. Please refer to Table 1 for a list of all the\ntypes. For training, we use three types for negative\ndocuments d−’s with uniform weights: (h,r,·),\n(·,r,t), and (h,·,t) as they are the hardest ones\nsince they only differ in one entity from the query.\nTo keep the GPU memory usage under control, we\nsample four documents per each type (refer to Sec-\ntion 3.1 for the effect of the number of negatives\non the results), which results in a total of m= 12\nnegatives. We resample the documents on each\nepoch to avoid overfitting and use a validation set\nto choose the best checkpoint. Also, we keep the\nlearning rate low and use no weight decay to pre-\nvent any forgetting. We use three seeds for the\nextraction phase, three seeds for splitting Dθ into\ntrain and test, and three seeds for finetuning on the\ndownstream task, which results in 27 different runs\nper each model.\nInference During inference, the model must iden-\ntify the gold document d+ amidst distractor docu-\nments d−’s. To ascertain that the model genuinely\nrecognizes the correct answer, we employ a varied\nassortment of distractors. Initially, we use doc-\nument type (h,r,·), ensuring all non-gold tails\nare included. Subsequently, we utilize the remain-\ning non-gold document types listed in Table 1 as\ndistractors, sampling 50 documents for each type.\nLastly, we also sample 50 irrelevant but factually\ncorrect documents from the test set to assess the\nmodel’s sensitivity to factual accuracy.\nWe evaluate pre-trained models across various fam-\nilies: OPT (Zhang et al., 2022), GPT-Neo (Black\net al., 2021), RoBERTa (Liu et al., 2019), and\n4308\nBERT RoBERTa GPT-Neo OPT\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy on Diagnostic Set ( D)\n0.47\n0.37 0.35 0.34\n(a) Encyclopedic Knowledge (Zero-shot)\nBERT RoBERTa GPT-Neo OPT\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy on Downstream Task ( Kθ\ntest)\n0.78 0.79 0.81 0.82 (b) Knowledge Utilization in Downstream (Finetuned)\nFigure 3: (a) The fraction of encyclopedic facts the pre-trained LM can predict correctly without any training.\nReported over three seeds (standard deviationσ≤0.004 for all models). (b) The model performance in downstream\ntask (created based on correctly predicted facts) measured as top-1 retrieval accuracy. Averaged over27 runs per\neach model (σ≤0.011 for all models). Refer to Appendix B for detailed results.\nBERT (Devlin et al., 2019). Unless otherwise\nstated, we use the base size (125M) of these mod-\nels. We investigate the scaling behavior of OPT\nand LLaMa (Touvron et al., 2023) in Section 5.\nWe initialize the diagnostic dataset Dfrom LAMA\n(Petroni et al., 2019), which has 34K facts over\n40 relations. Our results are reported over 1134\nfinetuning runs (Refer to Appendix A for detailed\nhyperparameters.)\n3 Evaluating the Knowledge Utilization\nWe separately report the fraction of facts (D) that\ncan be extracted and the downstream performance\nof models in Figure 3.\nFirst, we find that, on par with previous work\n(Qin and Eisner, 2021), there is a significant gap\nin the encyclopedic facts the models can correctly\npredict and the entire facts present in the diagnostic\ndataset D(Figure 3a). Note that one can arbitrarily\nincrease the number of correctly predicted by con-\nsidering a prediction as correct if the gold entity is\namong the model’s top-kpredictions. However, we\nonly consider k= 1to only focus on the facts that\nthe model can confidently predict. Nonetheless, we\nfind that BERT and RoBERTa extract slightly more\nencyclopedic facts than GPT-Neo and OPT.\nCritically, all models demonstrate a pronounced\ngap in downstream task performance, or knowledge\nutilization, (Figure 3b). This unexpected outcome\noccurs despite the downstream task being seem-\ningly simple since (1) models are trained and eval-\nuated on examples based on their accurate encyclo-\npedic knowledge predictions, and (2) both Kθ\ntrain\nand Kθ\ntest are sampled from the same distributions\n(I.I.D), so the models only encounter seen entities.\nNotably, OPT and GPT-Neo manage to outperform\n0.2 0.4 0.6 0.8 1\n(a) Fraction of Dθ\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy\n6 12 18 24 30\n(b) Number of Negatives\n6 12 18 24 30\n(b) Number of Negatives\nModel\nBERT RoBERTa GPT-Neo OPT\nFigure 4: (a) Knowledge utilization when using dif-\nferent fractions of parametric knowledge to create the\ndownstream task. (b) The effect of number of negative\ntraining documents (d−’s) used for creating the down-\nstream task.\nBERT and RoBERTa by a small margin.\nThis finding suggests that models struggle to\nutilize their entire parametric knowledge in down-\nstream tasks. In the next sections, we investigate\nthe potential causes of this gap.\n3.1 Role of Downstream Training Data\nThe effect of initial knowledgeDθ As we utilize\nDθ to create the downstream task, examining the\nimpact of its size (|Dθ|) on knowledge utilization\nis crucial. If consistent behavior is observed for\ndifferent sizes, it implies that the utilization gap\ndoes not stem from the amount of initial knowl-\nedge and must be rooted in inductive biases (e.g.,\nthe model or finetuning process), allowing us to\nmeasure and compare utilization with different ini-\ntial knowledge.\nTo measure such effect, for each model, we first\ncompute Dθ, and then instead of directly using it\nfor Kθ, we sub-sample smaller sets of it at various\n4309\nfractions and construct the downstream task using\neach sub-sampled Dθ. In Figure 4.a, we observe\nthe knowledge utilization is fairly consistent (at\nleast for fractions >0.4) across different sizes of\nDθ for all models. Larger fractions seem to have\nless variance as well. This suggests that the uti-\nlization performance is intrinsic to the downstream\nknowledge transfer rather than the initial knowl-\nedge residing in the model.\nThe effect of the number of negatives The\nmodel learns to apply its parametric knowledge\nby optimizing the retrieval objective. To ensure\nthe training signal, produced by contrastive loss\non Kθ\ntrain, is strong, we vary the number of nega-\ntive documents for creating Kθ\ntrain. If the training\nsignal is weak, we expect knowledge utilization to\nimprove with more negatives.\nTo answer this question, we follow the same\nsetup as described in Section 2 and increase the\nnumber of negative documents sampled per type\nfrom 4 to 10. We also consider reducing it to 2 neg-\natives per type to better understand its effectiveness.\nWe keep the initial knowledge Dθ fixed.\nFigure 4.b summarizes our findings. Knowledge\nutilization remains the same for all models as we\nincrease the number of negatives. This pattern is\nobserved even with as few as two negatives per\ntype. This suggests that the training signal is strong\nenough across the board and the gap in knowledge\nutilization is not rooted in the training objective.\n3.2 Gap 1 vs. Gap 2\nFindings in Section 3.1 shows that the gap in knowl-\nedge utilization (i.e. accuracy on Kθ\ntest) does not\ndepend on the size of Dθ and is fairly consistent\nacross different number of negatives. Moreover, we\nfind that the variation across the random splitting of\nDθ to create train and test sets of the downstream\ntask is negligible.\nThe robustness to such design choices allows\nus to define Usable Knowledge, which basically\nindicates the portion of facts fromDthat the model\ncan actually utilize in the downstream task. We\ncompute this metric by multiplying the accuracy\non Kθ\ntest by the fraction of correctly predicted facts\nin D. We report the results in Figure 5.\nThese results clearly demonstrate that there exist\ntwo gaps in the models’ knowledge. Gap 1 is in\nhow many facts the model knows after pre-training.\nGap 2 is in how many of facts the model knows\ncan be truly utilized in downstream tasks. Indeed,\nBERT RoBERTa GPT-Neo OPT\n0\n0.2\n0.4\n0.6\n0.8\n1\nFraction of Encyclopedic Facts\n0.37\n0.29 0.29 0.28\nIdentiﬁable Usable\nFigure 5: Gaps in parametric knowledge Gap 1\nrepresents the missing facts in parametric knowledge\nDθ (what the model knows). Gap 2 exists in how\nmany of the known facts the model can actually utilize\nin downstream tasks (the usable knowledge).\nwe see that although RoBERTa manages to extract\nmore facts than GPT-Neo, due to Gap2, it performs\nthe same as GPT-Neo in downstream tasks.\n4 Robustness of Knowledge Utilization\nWe intentionally design the downstream task Kθ\nto be straightforward and free of any distributional\nshift as we want to measure the maximum knowl-\nedge utilization of the model. However, in real-\nworld applications, it is likely that the model en-\ncounter samples that are different from the training\ndistribution. In this section, we investigate the ro-\nbustness of knowledge application in the presence\nof such distributional shifts.\n4.1 Non-I.I.D. Kθ\ntrain and Kθ\ntest\nRecall that we randomly divide Dθ into two sets\nas the data source for the creation of Kθ\ntrain and\nKθ\ntest. In this experiment, however, we split Dθ\nsuch that the relation types (r) in Kθ\ntrain and Kθ\ntest\nare disjoint. Specifically, we randomly select 60%\nof the relations and their corresponding facts for\nKθ\ntrain and the rest for Kθ\ntest. We repeat this pro-\ncess over three seeds to create three different splits.\nWe still follow the same procedure for converting\nknowledge triples to document retrieval examples\nas explained in Section 2. In this way, we ensure we\ndon’t change the task’s nature, i.e. the model still\nneeds to apply its parametric knowledge to solve\nthe task, but the distributional shift between Kθ\ntrain\nand Kθ\ntest can represent real-world scenarios. If\nthe model learns to systematically apply its knowl-\nedge, we expect its downstream performance to be\nsimilar to or close to the I.I.D. setting (Section 3).\nWe observe downstream task performance drops\n4310\nBERT RoBERTa GPT-Neo OPT\n0\n0.2\n0.4\n0.6\n0.8\n1\nAccuracy on Downstream Task ( Kθ\ntest)\nIID OOD Relation ( r)\nFigure 6: Robustness to distributional shift In the\nOOD setting, we produce a distributional shift (over the\nrelation types) between the examples in the train and\ntest set of the downstream task Kθ. All models fail to\ngeneralize to unseen relations. The IID setting is the\nsame as the one described in Section 2 and repeated\nfrom Figure 3b for comparison.\nsignificantly for all models when evaluated OOD\n(Figure 6). This indicates the models cannot use\ntheir knowledge on examples with unseen relation\ntypes, though all relations and facts originate in\nDθ. Thus, knowledge usage in downstream tasks is\nsensitive to distribution shifts, suggesting failure to\napply pre-training knowledge may be more severe\nin real-world applications.\n5 Effect of Scaling law On The Gaps\nRecent NLP success has come from scaling up pre-\ntraining model parameters (Brown et al., 2020).\nWith larger models and increased compute, capa-\nbilities such as in-context learning and chain-of-\nthought reasoning emerge (Wei et al., 2022b). The\nexpanded capacity allows these models to absorb\nmore knowledge from pre-training data, improving\ntheir usefulness as knowledge sources. However,\nit remains uncertain if scaling boosts the propor-\ntion of pre-training knowledge applicable to down-\nstream tasks. Ideally, we like to see a narrowing\ngap in pre-training knowledge alongside superior\nknowledge utilization.\nTo investigate this, we evaluate XTRAEVAL\non increasing sizes of OPT and LLaMa models.\nSpecifically, at each scale, we first extract the\nmodel’s parametric knowledge and then create the\ndownstream task based on it using the same proce-\ndure as described in Section 2. Figure 1 reports the\nresults of this experiment.\nFirst, we confirm that a greater fraction of knowl-\nedge triples in Dcan be identified in larger models,\n125M 350M 1.3B 2.7B 7B 13B\nModel Size (log scale)\n0\n0.25\n0.50\n0.75\n1\nFraction of Encyclopedic Facts Ideal gap\nModel Family\nOPT LLaMa\nGap Type\nGap 1 Gap 2\n125M 350M 1.3B 2.7B 7B 13B\nModel Size (log scale)\n0\n0.25\n0.50\n0.75\n1\nFraction of Encyclopedic Facts Ideal gap\nModel Family\nOPT LLaMa\nGap Type\nGap 1 Gap 2\n125M 350M 1.3B 2.7B 7B 13B\nModel Size (log scale)\n0\n0.25\n0.50\n0.75\n1\nFraction of Encyclopedic Facts Ideal gap\nModel Family\nOPT LLaMa\nGap Type\nGap 1 Gap 2\nFigure 7: Gaps in parametric knowledge Knowl-\nedge gaps directly compute across different model sizes.\nSpecifically, we use 1 −(Accuracy on Dθ) for Gap 1\nand (Accuracy on Dθ)×(1−downstream accuracy) for\nGap 2†.\nsuggesting they acquire more knowledge from pre-\ntraining data. Secondly, we find that the gap be-\ntween identifiable and usable knowledge persists in\nlarger models, and their ability to apply knowledge\nin downstream tasks does not improve with scaling.\nFigure 7 illustrates these gaps directly, demonstrat-\ning that while Gap 1 decreases in larger models,\nGap 2 remains relatively unchanged.\nThe results suggest that while PLMs, even at\nsmall scales, pose considerable knowledge, ex-\ntracting an equivalent amount of usable knowl-\nedge necessitates much larger models. For in-\nstance, OPT-125M accurately predicts 34% of en-\ncyclopedic facts, but only OPT-13B (approximately\n100×larger) can reliably apply the same volume\nin downstream tasks. Enhanced pre-training rou-\ntines, including the use of more data or higher qual-\nity data, can bolster knowledge acquisition, as is\nclearly demonstrated by LLaMa models. Notably,\nLLaMa-7B significantly outperforms OPT-13B.\nWhile LLaMa models possess a greater amount of\nusable knowledge due to superior initial knowledge,\na gap in knowledge utilization remains discernible\n(Figure 7).\n6 Discussion\nLately, pre-trained language models with chat-\nbot interfaces have increasingly been served as\nknowledge bases (Ouyang et al., 2022). These\nchatbots typically employ the model’s parametric\n†We conducted the experiments for OPT-6.7B multiple\ntimes, and observed a dip in performance in all runs. We\nsuspect that this consistent decline may be attributed to issues\nthat arose during the pre-training phase.\n4311\nknowledge to respond to queries and offer infor-\nmation. Our study examines the dependability of\nthis knowledge and its impact on downstream task\nperformance. We discover that, regardless of in-\nductive biases, PLMs face difficulty utilizing their\nfull knowledge in downstream tasks (Section 3).\nThis unreliability of parametric knowledge could\nconstrain the concept of “PLMs as differentiable\nknowledge bases.”\nAdditionally, our findings show that the utiliza-\ntion gap persists even with scaling (Section 5). No-\ntably, while models at each scale capture more\nknowledge from pre-training data, obtaining the\nsame amount of usable knowledge requires sig-\nnificantly larger models. This exposes a potential\nconstraint in the recent trend of adopting mid-sized\nPLMs (Li et al., 2023).\nLastly, we discover that knowledge utilization\ndepends on the peculiarities of finetuning data for\ndownstream tasks. Specifically, as seen in Sec-\ntion 4, PLMs struggle to apply their knowledge\nto relation types not encountered during finetun-\ning, even if they accurately predicted such facts\nin step 1. This generalization gap could highlight\nchallenges within the recent SFT-RLHF paradigm\n(Ouyang et al., 2022). For instance, the model\nmay only adhere to instructions and excel at tasks\nresembling the finetuning data. Consequently, it\nmight be necessary to meticulously craft finetuning\ndata to activate and utilize all aspects of parametric\nknowledge in downstream tasks. However, it re-\nquires elaborate studies to establish the systematic\nissues in knowledge application beyond encyclope-\ndic knowledge like procedural and task knowledge.\n7 Related Work\nParametric Knowledge Petroni et al. (2019)\nconstructed a probing dataset to measure the fac-\ntual knowledge present in PLMs. They showed that\nmany encyclopedic facts can be extracted without\nfurther training of the model and proposed PLMs as\na new type of knowledge base, which can be trained\non the unstructured text and queried using natural\nlanguage. Follow-up work improves the methods\nfor probing and extracting world knowledge from\nPLMs (Jiang et al., 2020; Shin et al., 2020; Qin and\nEisner, 2021; Newman et al., 2022). Apart from\nencyclopedic facts, studies have explored PLMs’\nparametric knowledge in other areas, such as lin-\nguistic structures (Tenney et al., 2019b; Blevins\net al., 2022), and commonsense (Zhou et al., 2020;\nLiu et al., 2022a). Recently, the emergent abilities\nof LLMs have shown that they acquire skills like\ncoding (Chen et al., 2021), reasoning (Chowdh-\nery et al., 2022), and in-context learning (Brown\net al., 2020), in addition to the previously men-\ntioned knowledge.\nUsing the Parametric KnowledgeRoberts et al.\n(2020) finetune a pre-trained T5 model for question\nanswering in a closed-book setting and showed that\nit can perform on par or better than models that use\nexplicit knowledge bases. Wang et al. (2021) made\na similar observation for the BART model. More\nrecently, PLMs are being used to generate facts\nand documents for knowledge-intensive tasks (Li\net al., 2022; Liu et al., 2022b; Yu et al., 2023). In\nthis paradigm, in order to answer factual questions,\ninstead of retrieving relevant documents, the model\nhas to first generate the facts and then answer the\nquestion with those facts as context. This paradigm\nshows that the models may not be able to use their\nparametric knowledge on their own and need ex-\nplicit grounding to be able to use it. Furthermore,\nthere is a plethora of work that investigates whether\nthe model employs its linguistic knowledge when\nsolving downstream language understanding tasks.\nMcCoy et al. (2019) shows that RoBERTa does not\nuse its linguistic knowledge for solving NLI. In-\nstead, it relies on shallow heuristics. Lovering et al.\n(2021)’s observation aligns with this finding and\nshows the training data used for the downstream\ntask needs to have enough evidence to trigger the\nmodel’s linguistic knowledge. In our work, we use\na more general notation of parametric knowledge\nand investigate utilization in cases where sufficient\nevidence is present in the finetuning data.\n8 Conclusion\nIn this study, we presented EXTRACT , TR AIN ,\nAND EVALUATE (XTRAEVAL ), a framework de-\nsigned to assess the parametric knowledge of pre-\ntrained language models. Employing XTRAEVAL\nwe identified a previously unnoticed gap in what\nmodels know and how much of it they can actually\nuse. Our findings reveal that this gap exists not\nonly in smaller models but also persists in larger\nones. Additionally, we demonstrate that a distri-\nbutional shift in finetuning data can result in even\nlarger gaps between the model’s knowledge and its\npractical application in downstream tasks.\n4312\nLimitations\nAlthough XTRAEVAL is agnostic to the specific\ntype of parametric knowledge, our work primarily\nfocuses on encyclopedic facts as one type of world\nknowledge that PLMs can acquire. It is plausible\nthat similar results would hold for other knowledge\ntypes, however, further work is needed for a precise\ninvestigation.\nWhile there are various downstream tasks that\ncould be evaluated, we primarily focus on doc-\nument retrieval as it allows us to systematically\ndemonstrate the key issue of knowledge applica-\ntion that we aim to highlight. We also acknowledge\nthat our study was limited to a few model families\nand parameter scales due to compute constraints.\nHowever, our evaluation protocol is model agnostic,\nenabling future work to explore this phenomenon\non other tasks and with different models.\nReferences\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. Zenodo.\nTerra Blevins, Hila Gonen, and Luke Zettlemoyer. 2022.\nPrompting language models for linguistic structure.\nArXiv preprint, abs/2211.07830.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems 33: Annual Conference on Neural\nInformation Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, Alex Ray, Raul Puri, et al. 2021. Evalu-\nating large language models trained on code. ArXiv\npreprint, abs/2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, et al. 2022. Scal-\ning instruction-finetuned language models. ArXiv\npreprint, abs/2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nShibo Hao, Bowen Tan, Kaiwen Tang, Bin Ni, Hengzhe\nZhang, Eric P Xing, and Zhiting Hu. 2022. Bert-\nnet: Harvesting knowledge graphs from pretrained\nlanguage models. ArXiv preprint, abs/2206.14268.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2023. Opt-iml: Scaling language\nmodel instruction meta learning through the lens of\ngeneralization. ArXiv preprint.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nArtur Kulmizev and Joakim Nivre. 2021. Schrödinger’s\ntree - on syntax and neural language models. ArXiv\npreprint, abs/2110.08887.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim,\nQian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo,\nThomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, João Monteiro, Oleh Shliazhko,\nNicolas Gontier, et al. 2023. Starcoder: may the\nsource be with you! ArXiv preprint, abs/2305.06161.\nYanyang Li, Jianqiao Zhao, Michael Lyu, and Li-\nwei Wang. 2022. Eliciting knowledge from large\npre-trained models for unsupervised knowledge-\ngrounded conversation. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10551–10564, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nJiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Pe-\nter West, Ronan Le Bras, Yejin Choi, and Hannaneh\nHajishirzi. 2022a. Generated knowledge prompting\nfor commonsense reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n4313\n3154–3169, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai\nPrabhumoye, Wei Ping, Mohammad Shoeybi, and\nBryan Catanzaro. 2022b. Multi-stage prompting for\nknowledgeable dialogue generation. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1317–1337, Dublin, Ireland. Association\nfor Computational Linguistics.\nCharles Lovering, Rohan Jha, Tal Linzen, and Ellie\nPavlick. 2021. Predicting inductive biases of pre-\ntrained models. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick, and\nIan Tenney. 2020. What happens to BERT embed-\ndings during fine-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP, pages 33–44,\nOnline. Association for Computational Linguistics.\nBenjamin Newman, Prafulla Kumar Choubey, and\nNazneen Rajani. 2022. P-adapters: Robustly extract-\ning factual information from language models with\ndiverse prompts. In The Tenth International Con-\nference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nRodrigo Nogueira and Kyunghyun Cho. 2020. Pas-\nsage re-ranking with bert. ArXiv preprint ,\nabs/1901.04085.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. ArXiv preprint, abs/2203.02155.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? ArXiv preprint, abs/2302.06476.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019b. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv\npreprint, abs/2302.13971.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019.\nRepresentation learning with contrastive predictive\ncoding. ArXiv preprint, abs/1807.03748.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book QA? In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3241–3251, Online.\nAssociation for Computational Linguistics.\n4314\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020. Learning which fea-\ntures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n217–235, Online. Association for Computational Lin-\nguistics.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022b. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In The Eleventh Inter-\nnational Conference on Learning Representations.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models. ArXiv preprint,\nabs/2205.01068.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 9733–9740. AAAI Press.\nYichu Zhou and Vivek Srikumar. 2022. A closer look\nat how fine-tuning changes BERT. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1046–1061, Dublin, Ireland. Association for\nComputational Linguistics.\n4315\nValue Head\nPre-trained Model \nWhere did Obama graduate from?Obama graduated from Harvad.\nSimilarity Score\nFigure A.1: Cross-encoder document retrieval setup\n(Nogueira and Cho, 2020). For decoder-only models,\nthe value head takes the representation of the last input\ntoken.\nA Training Details\nA.1 Knowledge Extraction\nWe adopt the same procedure as outlined by Qin\nand Eisner (2021) for extracting knowledge facts\nfrom a frozen PLM. Specifically, we utilize soft-\nprompts instead of discrete prompts. We insert\nthree soft prompts before and after the head entity\nand allocate distinct soft-prompts for each rela-\ntion type. We then train them using the training\nset provided by Qin and Eisner (2021) and em-\nploy a validation set to select the best checkpoint.\nThe hyperparameters used in this stage, borrowed\nfrom Qin and Eisner (2021), are summarized in\nTable A.1.\nA.2 Finetuning\nFor fine-tuning the models, we follow a straight-\nforward procedure, training the models in a\ncross-encoder setup (Figure A.1). The hyper-\nparameters used for fine-tuning are listed in Ta-\nble A.2. In initial experiments, we tried lr ∈\n1 ×10−5,3 ×10−5,5 ×10−5, but we did not find\nany significant difference between them for all\nmodels. Thus, we opted to use the same learning\nrate for all models.\nA.3 Dataset Details\nWe employ the LAMA dataset (Petroni et al., 2019)\nas our diagnostic set, consisting of 34,039 facts.\nThe training and validation sets for soft-prompt\ntraining, provided by Qin and Eisner (2021), con-\ntain 29,029 and 7,255 triples, respectively. The size\nof the downstream dataset varies from one model\ninstance to another, as we construct the downstream\ntask based on its extracted knowledge. The size\nof such datasets can be easily calculated by multi-\nplying the number of facts in Dby the knowledge\nParameter Value\nOptimizer AdamW\nLearning rate 1 ×10−4\nWeight Decay 0\nBatch size 64\nLearning Rate Scheduler Polynomial\nWarm Up 6% of training steps\n# Train Epochs 20\nTable A.1: Summary of hyperparameters used in knowl-\nedge extraction stage (stage 1).\nParameter Value\nOptimizer AdamW\nLearning rate 1 ×10−5\nWeight Decay 0\nBatch size 32\nLearning Rate Scheduler Polynomial\nWarm Up 6% of training steps\n# Train Epochs 20\nTable A.2: Summary of hyperparameters used in fine-\ntuning on downstream task (stage 2).\nextraction accuracy obtained by the model.\nA.4 Scaling Experiment Details\nWe adhere to the same procedure as other models\nin the scaling experiments of OPT and LLaMa.\nFor larger models, we only increase the batch size\nfollowing Iyer et al. (2023) to ensure better fine-\ntuning stability. Due to the extensive computational\nand cost requirements of fully fine-tuning the 7B\nand 13B models, we limit the number of seeds\nfor these variants. Specifically, we trained four\nseeds for LLaMa-7B, two for OPT-13B, and one\nfor LLaMa-13B.\nA.5 Computational Resources\nFor the experiments in this study, we exclusively\nuse NVIDIA V100-32GB GPUs. Models with ≤\n350M parameters were trained on a single GPU.\nFor knowledge extraction, we utilized parallel train-\ning implemented by DeepSpeed for larger parame-\nter sizes. Table A.4 displays the number of GPUs\nused and the approximate duration of a single train-\ning run.\nA.6 Reproducibility\nFor the experiments in this study, we exclusively\nuse NVIDIA V100-32GB GPUs. Models with ≤\n350M parameters were trained on a single GPU.\nFor knowledge extraction, we utilized parallel train-\n4316\nModel Knowledge Extraction Downstream Finetuning\nOPT\n125M 64 32\n350M 64 32\n1.3B 64 32\n2.7B 64 32\n6.7B 128 64\n13B 128 64\nLLaMa\n7B 128 64\n13B 128 64\nTable A.3: Batch size used in the scaling experiments.\nModel Knowledge Extraction Downstream Finetuning\nOPT\n125M 1 ( 0h 30m) 1 ( 0h 30m)\n350M 1 ( 0h 30m) 1 ( 1h 0m)\n1.3B 2 ( 1h 40m) 2 ( 1h 40m)\n2.7B 4 ( 3h 30m) 4 ( 3h 50m)\n6.7B 8 ( 1h 20m) 8 ( 8h 40m)\n13B 8 ( 5h 20m) 8 ( 1d 4h 40m)\nLLaMa\n7B 8 ( 8h 30m) 8 ( 17h 40m)\n13B 8 ( 17h 30m) 8 ( 1d 15h 15m)\nTable A.4: Number of GPU and approximate training\ntime for each model size.\ning implemented by DeepSpeed for larger parame-\nter sizes. Table A.4 displays the number of GPUs\nused and the approximate duration of a single train-\ning run.\nB Full Results\nB.1 Detailed IID Results\nWe present the comprehensive results of our exper-\niments from Section 3 in Tables B.5 to B.8.\nB.2 Detailed OOD Results\nTable B.9 provides a detailed account of the experi-\nment results in Section 4.\nB.3 Detailed Scaling Results\nThe detailed results of Section 5 are presented in\nTables B.10 and B.11.\n4317\nModel Knowledge Extraction Accuracy Downstream Accuracy\nBERT 0.4722 ±0.0006 0.7760 ±0.0112\nRoBERTa 0.3681 ±0.0007 0.7852 ±0.0092\nGPT-Neo 0.3531 ±0.0005 0.8081 ±0.0099\nOPT 0.3444 ±0.0038 0.8177 ±0.0083\nTable B.5: Mean±standard deviation of results presented in Figure 3a and Figure 3b Each number is computed over\n27 runs and models are in 125M parameter regime.\nDownstream Accuracy (Per Number of Negatives)\nModel 6 12 18 27 30\nBERT 0.7604 ±0.0190 0.7760 ±0.0112 0.7908 ±0.0075 0.7965 ±0.0081 0.7970 ±0.0055\nRoBERTa 0.7660 ±0.0192 0.7852 ±0.0092 0.7889 ±0.0086 0.7961 ±0.0100 0.7927 ±0.0117\nGPT-Neo 0.8000 ±0.0115 0.8081 ±0.0099 0.8129 ±0.0128 0.7577 ±0.2186 0.7586 ±0.2189\nOPT 0.7966 ±0.0109 0.8177 ±0.0083 0.8179 ±0.0113 0.8294 ±0.0054 0.8269 ±0.0095\nTable B.6: Mean±standard deviation of results presented in Figure 4. Each number is computed over 27 runs and\nmodels are in 125M parameter regime.\nDownstream Accuracy (Per Fraction of Dθ)\nModel 0.2 0.4 0.6 0.8 1\nBERT 0.5371 ±0.0707 0.7015 ±0.0272 0.7376 ±0.0173 0.7678 ±0.0138 0.7760 ±0.0112\nRoBERTa 0.7329 ±0.0554 0.7763 ±0.0153 0.7824 ±0.0122 0.7769 ±0.0145 0.7852 ±0.0092\nGPT-Neo 0.8139 ±0.0633 0.8313 ±0.0268 0.8149 ±0.0318 0.7948 ±0.0164 0.8081 ±0.0099\nOPT 0.7542 ±0.0710 0.8312 ±0.0135 0.8100 ±0.0158 0.8065 ±0.0119 0.8177 ±0.0083\nTable B.7: Mean±standard deviation of results presented in Figure 4. Each number is computed over 27 runs and\nmodels are in 125M parameter regime.\nFraction of Encyclopedic Facts Knowledge Gaps\nModel Identifiable Usable Gap 1 Gap 2\nBERT 0.4722 ±0.0006 0.3665 ±0.0052 0.5277 ±0.0006 0.1058 ±0.0053\nRoBERTa 0.3681 ±0.0007 0.2890 ±0.0035 0.6319 ±0.0008 0.0791 ±0.0034\nGPT-Neo 0.3531 ±0.0005 0.2854 ±0.0035 0.6468 ±0.0006 0.0678 ±0.0035\nOPT 0.3444 ±0.0038 0.2816 ±0.0036 0.6556 ±0.0038 0.0628 ±0.0030\nTable B.8: Full results including mean±standard deviation of experiments presented in Figure 5.\nDownstream Accuracy\nBERT RoBERTa GPT-Neo OPT\nIID 0.7760 ±0.0112 0.7852 ±0.0092 0.8081 ±0.0099 0.8177 ±0.0083\nOOD Relation (r) - All 0.2467 ±0.0396 0.3559 ±0.0415 0.1312 ±0.0365 0.0976 ±0.0458\nOOD Relation (r) - Seen entities 0.2451 ±0.0337 0.3778 ±0.0474 0.1330 ±0.0346 0.1001 ±0.0489\nOOD Relation (r) - Unseen entities 0.2361 ±0.0608 0.3176 ±0.0616 0.1317 ±0.0503 0.0971 ±0.0499\nTable B.9: Full results including mean±standard deviation of experiments presented in Figure 6. Each number is\ncomputed over 27 runs.\n4318\nModel Knowledge Extraction Accuracy Downstream Accuracy\nOPT\n125M 0.3677 ±0.0045 0.6836 ±0.0049\n350M 0.3839 ±0.0017 0.7451 ±0.0053\n1.3B 0.4403 ±0.0052 0.7485 ±0.0026\n2.7B 0.4761 ±0.0019 0.7458 ±0.0149\n6.7B 0.5090 ±0.0016 0.3819 ±0.0220\n13B 0.5407 ±0.0008 0.7155 ±0.0013\nLLaMa\n7B 0.6952 ±0.0078 0.7548 ±0.0099\n13B 0.7014 ±0.0025 0.7508\nTable B.10: Results of stage 1 and stage 2 for various parameter sizes, which is used to present plots in Figures 1\nand 7\nFraction of Encyclopedic Facts Knowledge Gaps\nModel Identifiable Usable Gap 1 Gap 2\nOPT\n125M 0.3677 ±0.0045 0.2513 ±0.0013 0.6323 ±0.0033 0.0941 ±0.0234\n350M 0.3839 ±0.0017 0.2860 ±0.0019 0.6161 ±0.0015 0.0811 ±0.0174\n1.3B 0.4403 ±0.0052 0.3289 ±0.0037 0.5606 ±0.0037 0.0945 ±0.0166\n2.7B 0.4761 ±0.0019 0.3551 ±0.0077 0.5239 ±0.0016 0.1029 ±0.0196\n6.7B 0.5090 ±0.0016 0.1944 ±0.0112 0.4910 ±0.0014 0.2712 ±0.0467\n13B 0.5407 ±0.0008 0.3874 ±0.0007 0.4586 ±0.0000 0.1325 ±0.0249\nLLaMa\n7B 0.6952 ±0.0078 0.5282 ±0.0062 0.3002 ±0.0010 0.1382 ±0.0358\n13B 0.7014 ±0.0025 0.5288 0.2957 ±0.0000 0.1503 ±0.0356\nTable B.11: Mean±standard deviation of results presented in Figures 1 and 7.\n4319",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.725445032119751
    },
    {
      "name": "USable",
      "score": 0.6824685335159302
    },
    {
      "name": "Domain knowledge",
      "score": 0.5731803178787231
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5388069152832031
    },
    {
      "name": "Parametric statistics",
      "score": 0.5154107809066772
    },
    {
      "name": "Task (project management)",
      "score": 0.4843786656856537
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.48271772265434265
    },
    {
      "name": "Knowledge acquisition",
      "score": 0.477802574634552
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45393335819244385
    },
    {
      "name": "Knowledge extraction",
      "score": 0.4468626379966736
    },
    {
      "name": "Machine learning",
      "score": 0.44505298137664795
    },
    {
      "name": "Data mining",
      "score": 0.2533281445503235
    },
    {
      "name": "Engineering",
      "score": 0.11556503176689148
    },
    {
      "name": "Mathematics",
      "score": 0.07456099987030029
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5023651",
      "name": "McGill University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210164802",
      "name": "Mila - Quebec Artificial Intelligence Institute",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I109736498",
      "name": "Canadian Institute for Advanced Research",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I45683168",
      "name": "Polytechnique Montréal",
      "country": "CA"
    }
  ],
  "cited_by": 2
}