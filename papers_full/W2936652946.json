{
  "title": "Dynamic Evaluation of Transformer Language Models",
  "url": "https://openalex.org/W2936652946",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288577835",
      "name": "Krause, Ben",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Kahembwe, Emmanuel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746582101",
      "name": "Murray Iain",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742255037",
      "name": "Renals Steve",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963430354",
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2798702047",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2964269252",
    "https://openalex.org/W2963735467",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2525246036",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2963494889"
  ],
  "abstract": "This research note combines two methods that have recently improved the state of the art in language modeling: Transformers and dynamic evaluation. Transformers use stacked layers of self-attention that allow them to capture long range dependencies in sequential data. Dynamic evaluation fits models to the recent sequence history, allowing them to assign higher probabilities to re-occurring sequential patterns. By applying dynamic evaluation to Transformer-XL models, we improve the state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04 bits/char, and WikiText-103 from 18.3 to 16.4 perplexity points.",
  "full_text": "DYNAMICEVALUATION OFTRANSFORMERLANGUAGEMODELS\nBen Krause, Emmanuel Kahembwe, Iain Murray, & Steve Renals\nSchool of Informatics, University of Edinburgh\nEdinburgh, Scotland, UK\nben.krause,e.kahembwe,i.murray,s.renals@ed.ac.uk\nABSTRACT\nThis research note combines two methods that have recently improved the state of\nthe art in language modeling: Transformers and dynamic evaluation. Transformers\nuse stacked layers of self-attention that allow them to capture long range depen-\ndencies in sequential data. Dynamic evaluation ﬁts models to the recent sequence\nhistory, allowing them to assign higher probabilities to re-occurring sequential\npatterns. By applying dynamic evaluation to Transformer-XL models, we improve\nthe state of the art on enwik8 from 0.99 to 0.94 bits/char, text8 from 1.08 to 1.04\nbits/char, and WikiText-103 from 18.3 to 16.4 perplexity points. Code to replicate\nour results is available1.\n1 I NTRODUCTION\nLanguage modeling is a commonly used machine learning benchmark with applications to speech\nrecognition, machine translation, text generation, and unsupervised learning in natural language\nprocessing tasks. LSTMs (Hochreiter and Schmidhuber, 1997) conventionally used for language\nmodeling have been shown to use relatively shorter contexts to make predictions (Khandelwal\net al., 2018). Several recent improvements to language modeling have resulted from models with\nincreased ability to use long-range dependencies. This work combines two speciﬁc advances:\nTransformers (Vaswani et al., 2017) and dynamic evaluation (Mikolov et al., 2010; Krause et al.,\n2018). Transformers can model long range dependencies through stacked layers of self-attention, and\ndynamic evaluation exploits certain types of long range dependencies by adapting parameters based\non the observed sequence history. Dynamic evaluation can be applied to any language model at test\ntime, but to our knowledge, no previous work has applied dynamic evaluation to Transformers.\nTransformers use a combination of a self-attention mechanism and positional embeddings to encode\ninformation about the sequence history (Vaswani et al., 2017). The use of self-attention provides\nshorter paths for information to travel, which is conjectured to be one of the main reasons that\ntransformers achieve better results on common language modeling benchmarks compared to other\nmodels (Dai et al., 2019). Moreover, transformers trained on very large datasets can generalize to\nother NLP tasks, and generate realistic samples that are coherent over long time frames (Radford\net al., 2019).\nDynamic evaluation adapts models to the recent sequence history via gradient descent in order to\nexploit re-occurring sequential patterns. Natural language tends to have long range dependencies\nassociated with the style and word usage of particular passages of texts; and dynamic evaluation\ncan exploit these dependencies via online model adaptation. Transformers with a large memory\ncache also potentially have the capability of adapting to the style of the recent sequence history,\nalthough it is unclear to what extent they learn to do this in practice. Dynamic evaluation and\nTransformers have each shown their respective capabilities to use thousands of timesteps of context\nto improve predictions (Krause et al., 2018; Dai et al., 2019), but it is unclear how much overlap there\nis between the type of long-range dependencies exploited by Transformers and dynamic evaluation.\nIf Transformers are able to fully adapt to the style of the recent sequence history, there should be\nlittle to no advantage of using dynamic evaluation. Therefore, in this work, we explore the utility of\napplying dynamic evaluation to Transformers.\n1www.github.com/benkrause/dynamiceval-transformer\n1\narXiv:1904.08378v1  [cs.LG]  17 Apr 2019\n2 T RANSFORMERS\nA number of variants of Transformers have been suggested for language modeling (Al-Rfou et al.,\n2018; Liu et al., 2018; Baevski and Auli, 2019; Radford et al., 2018), but in this work, we focus on the\nTransformer-XL architecture of Dai et al. (2019), which uses segment-level attention recurrence and\na relative positional encoding mechanism to generalize to longer attention lengths than seen during\ntraining. Transformer-XL has recently improved state-of-the-art results on a number of common\nlanguage modeling benchmarks.\nThe Transformer-XL, like the regular Transformer, contains stacked self-attention layers and position-\nwise feedforward operations. The Transformer-XL processes sequence segments in parallel across\ntime in each forward pass. The hidden states from these sequence segments are cached in a memory\nso that future sequence segments can apply attention over them. We refer to Dai et al. (2019) for the\nfull details of the model.\n3 D YNAMIC EVALUATION\nDynamic evaluation is a gradient descent based adaptation method that can be applied to auto-\nregressive sequence modeling problems. Auto-regressive sequence models use the following factor-\nization to assign a probability to a sequence x1:T = {x1,...,x T }:\nP(x1:T ) =P(x1)\nT∏\nt=2\nP(xt|x1:t−1). (1)\nThe model predicts a distribution over the next sequence element P(xt|x1:t−1) (or a sequence\nsegment xt1:t2 |x1:t1−1). The model observes the true xt and takes a loss based on the cross entropy\nprediction error, Lt. The gradient ∇Lt is then used to update the network before proceeding to the\nnext sequence element. As in all autoregressive models, dynamic evaluation only conditions on\nsequence elements that it has already predicted, and so evaluates a valid log-probability for each\nsequence. Dynamic evaluation is illustrated graphically in ﬁgure 1.\nThe gradient descent adjusted weights can be interpreted as a memory that can better capture re-\noccurring patterns that occur in linguistic sequences. Dynamic evaluation updates were shown to have\nthe ability to increase probabilities of words that occur in a sequence, as well as words with similar\nembeddings to words that occur in the sequence (Krause et al., 2018). This capability gives dynamic\nevaluation the potential to better model recently seen words, as well as to adapt more broadly to the\nstyle and topic of a sequence.\nFollowing Krause et al. (2018), which applies dynamic evaluation to RNNs at the sequence segment\nlevel, we apply dynamic evaluation to Transformer-XL models at the sequence segment level. Since\nTransformer-XL models are designed to process sequences in segments, we align the sequence\nsegments used for Transformer-XL (Dai et al., 2019) with the sequence segments used to compute\nthe gradient for dynamic evaluation. The gradient is computed once for each sequence segment\n(after taking a loss on the segment), and backpropagation is truncated to be contained within a single\nsequence segment.\nThere is a large space of potential optimizers that can be used for dynamic evaluation, and we evaluate\ntwo in this work. We consider a simple baseline that uses stochastic gradient descent with a ﬁxed\nlearning rate to update the weights of the network on each segment. We also consider the more\ncomplex dynamic evaluation optimizer (Krause et al., 2018) which uses an update rule related to\nRMSprop (Tieleman and Hinton, 2012), except that gradient statistics are computed from the training\ndata, and weights are decayed back to the original parameters learned during training.\n4 E XPERIMENTS\nWe applied dynamic evaluation to pretrained Transformer-XL models from Dai et al. (2019) on\ntwo character-level datasets and one word-level dataset. We chose these 3 datasets because they all\n2\ns1=x1:n s2=xn+1:2n s3=x2n+1:3n\nP(s1|θl\n0) P(s2|s1,θl\n1) P(s3|s2,s1,θl\n2)\nhidden \nstate\nhidden \nstate\nᵘL(s\n1)\nmodel(s2,θl\n1) model(s3,θl\n2)model(s1,θl\n0)\nᵘL(s\n2)\nFigure 1: Illustration of dynamic evaluation (ﬁgure from Krause et al. (2018)). The model evaluates\nthe probability of sequence segments si of length n. The gradient ∇L(si) with respect to the log\nprobability of si is used to update the model parameters θi−1\nl to θi\nl before the model progresses to the\nnext sequence segment. Dashed edges are what distinguish dynamic evaluation from static (normal)\nevaluation.\ncontain long-range dependencies that span across sentences and paragraphs. Details of the model\ntraining can be found in Dai et al. (2019), and we downloaded models using their code2.\nWe measured the performance of two types of dynamic evaluation; one which used the optimizer from\nKrause et al. (2018), which we refer to as “RMS dynamic eval + decay”, and one that used stochastic\ngradient descent, which we refer to as “SGD dynamic eval”. Following Krause et al. (2018), we tuned\nhyperparameters for dynamic evaluation on the validation sets before evaluating on the test sets.\n4.1 C HARACTER -LEVEL EXPERIMENTS\nWe use two datasets to evaluate dynamic evaluation on character-level Transformer-XL models;\nenwik8 (Hutter, 2006) and text83. enwik8 is a byte-level data set derived from Wikipedia that in\naddition to English text, also includes markup, special characters, and text in other languages. enwik8\ncontains 90M characters for training, 5M for validation, and 5M for testing. We noticed a slight\nanomaly in the preprocessing of enwik8 in the code released by Dai et al. (2019) that caused it to have\n204 unique tokens (rather than the standard 205 tokens used in most results, for instance in Graves\n(2013)), and our results also contain this anomaly since we use pretrained models from their work.\ntext8 is derived from the same data as enwik8, but is preprocessed to only contain an alphabet of 27\ncharacters (lowercase a–z plus spaces). text8 also uses a 90M–5M–5M split for training, validation,\nand testing. Following Dai et al. (2019), we used sequence segments of 128 and a memory cache\nof length 3800 for both datasets. Results for enwik8 and text8 are reported in Table 1 and Table 2\n2https://github.com/kimiyoung/transformer-xl\n3http://mattmahoney.net/dc/textdata\n3\nModel # of params test\nHyper LSTM (Ha et al., 2017) 25M 1.34\nHM-LSTM (Chung et al., 2017) 35M 1.32\nRecurrent highway networks (Zilly et al., 2017) 46M 1.27\nFS-LSTM (Mujika et al., 2017) 47M 1.25\nawd-LSTM (Merity et al., 2018) 47M 1.23\nTransformer + aux losses (Al-Rfou et al., 2018) 235M 1.06\nMultiplicative LSTM (Krause et al., 2016) 46M 1.24\nMultiplicative LSTM + dynamic eval (Krause et al., 2018) 46M 1.08\nTransformer-XL (Dai et al., 2019) 277M 0.993\nTransformer-XL + SGD dynamic eval 277M 0.946\nTransformer-XL + RMS dynamic eval + decay 277M 0.940\nTable 1: Character-level cross-entropy (bits/char) on enwik8. As noted in Section 4.1, there is a slight\ndifference in the data used in the ﬁnal three results and previous work.\nModel # of params test\nHM-LSTM (Chung et al., 2017) 35M 1.29\nRecurrent highway networks (Zilly et al., 2017) 45M 1.27\nTransformer + aux losses (Al-Rfou et al., 2018) 235M 1.13\nMultiplicative LSTM (Krause et al., 2016) 45M 1.27\nMultiplicative LSTM + dynamic eval (Krause et al., 2018) 45M 1.19\nTransformer-XL (Dai et al., 2019) 277M 1.085\nTransformer-XL + SGD dynamic eval 277M 1.042\nTransformer-XL + RMS dynamic eval + decay 277M 1.038\nTable 2: Character-level cross-entropy (bits/char) on text8.\nrespectively. Applying Dynamic evaluation improves the Transformer-XL by a noticeable margin,\nachieving state of the art on both of these character-level datasets.\n4.2 W ORD -LEVEL EXPERIMENTS\nWe evaluate dynamic evaluation on word-level Transformer-XL using the WikiText-103 dataset\n(Merity et al., 2017), which is also comprised of Wikipedia text. WikiText-103 contains 103 million\ntraining tokens, and a vocabulary size of 268k. Given the large vocabulary size, the pretrained model\nwe re-evaluate from Dai et al. (2019) used an adaptive softmax output layer (Grave et al., 2017a)\nto make training faster. Results for WikiText-103 are reported in Table 3. There was no noticeable\nvalidation advantage to using a decay rate, so we refer to the dynamic evaluation optimizer for this\nexperiment simply as “RMS dynamic eval”, since the decay rate was tuned to be set to zero. Dynamic\nevaluation gave a 9% perplexity improvement to Transformer-XL on WikiText-103.\nThe results on WikiText-103 are the ﬁrst that we know of that apply dynamic evaluation with an\nadaptive softmax output layer. Adaptive softmax reduces the computational expense of the output\nlayer at the cost of giving the model less expressiveness at modeling rare words. When training a\nnetwork from scratch, such a trade-off is sensible, since it is difﬁcult to learn a good representation of\nrare words. However, when dynamically adapting to the recent sequence history, the adaptive softmax\nlayer may make adapting to recent rare words more challenging. There is potential for future work\nimproving the combination of dynamic evaluation and adaptive softmax, for instance by hybridizing\nit with the neural cache method (Grave et al., 2017b). The neural cache learns a non-parametric\n4\nModel # of params valid test\nLSTM+ neural cache (Grave et al., 2017b) - - 40.8\nGCNN-14 (Dauphin et al., 2017) - - 37.2\nQRNN (Merity et al., 2018) 151M 32.0 33.0\nLSTM + hebbian + cache (Rae et al., 2018) - 29.7 29.9\nTransformer + adaptive input (Baevski and Auli, 2019) 247M 19.8 20.5\nTransformer-XL ∗ (Dai et al., 2019) 257M 17.3 18.1\nTransformer-XL + SGD dynamic eval 257M 16.3 17.0\nTransformer-XL + RMS dynamic eval 257M 15.8 16.4\nTable 3: Word-level perplexity on WikiText-103.∗We report our results using the pretrained model\nfrom Dai et al. (2019) using a batch size of 1, and achieved a slightly lower perplexity than in the\noriginal paper (18.1 vs 18.3).\noutput layer that is independent of the network’s output layer, which may potentially allow for more\nexpressive adaptation to rare words in models with an adaptive softmax.\n5 C ONCLUSION\nDynamic evaluation was able to give moderate improvements to strong Transformer network baselines,\nand improves the state of the art on all three datasets evaluated. These results demonstrate that the\ntypes of long range dependencies used by dynamic evaluation and Transformers are somewhat\ndifferent, as applying dynamic evaluation to Transformers leads to further improvements. These\nimprovements are not nearly as large as when dynamic evaluation has been applied to weaker models,\nsuggesting that Transformers are by themselves somewhat more capable of modeling re-occurring\npatterns in sequences than past architectures. However, Transformers still struggle to fully exploit\nthese repetitions, even in these experiments where training and testing data came from the same\ndomain. Transformers may struggle to adapt even more when there is a shift between training and\ntesting data. Our results therefore motivate future work on enhancements and architectures for\nadaptive sequence modeling, as current Transformer models cannot fully deal with adaptation on\ntheir own.\nREFERENCES\nAl-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones, L. (2018). Character-level language\nmodeling with deeper self-attention. arXiv:1808.04444.\nBaevski, A. and Auli, M. (2019). Adaptive input representations for neural language modeling. ICLR.\nChung, J., Ahn, S., and Bengio, Y . (2017). Hierarchical multiscale recurrent neural networks.ICLR.\nDai, Z., Yang, Z., Yang, Y ., Cohen, W. W., Carbonell, J., Le, Q. V ., and Salakhutdinov, R. (2019).\nTransformer-XL: Attentive language models beyond a ﬁxed-length context. arXiv:1901.02860.\nDauphin, Y . N., Fan, A., Auli, M., and Grangier, D. (2017). Language modeling with gated\nconvolutional networks. In ICML.\nGrave, E., Joulin, A., Cissé, M., Jégou, H., et al. (2017a). Efﬁcient softmax approximation for GPUs.\nIn ICML.\nGrave, E., Joulin, A., and Usunier, N. (2017b). Improving neural language models with a continuous\ncache. ICLR.\nGraves, A. (2013). Generating sequences with recurrent neural networks. arXiv:1308.0850.\nHa, D., Dai, A., and Lee, Q. (2017). Hypernetworks. ICLR.\n5\nHochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9:1735–\n1780.\nHutter, M. (2006). The human knowledge compression prize. URL http://prize.hutter1.net.\nKhandelwal, U., He, H., Qi, P., and Jurafsky, D. (2018). Sharp nearby, fuzzy far away: How neural\nlanguage models use context. arXiv preprint arXiv:1805.04623.\nKrause, B., Kahembwe, E., Murray, I., and Renals, S. (2018). Dynamic evaluation of neural sequence\nmodels. ICML.\nKrause, B., Lu, L., Murray, I., and Renals, S. (2016). Multiplicative LSTM for sequence modelling.\narXiv:1609.07959.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepassi, R., Kaiser, L., and Shazeer, N. (2018). Generating\nwikipedia by summarizing long sequences. ICLR.\nMerity, S., Keskar, N. S., and Socher, R. (2018). An analysis of neural language modeling at multiple\nscales. arXiv:1803.08240.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R. (2017). Pointer sentinel mixture models. ICLR.\nMikolov, T., Karaﬁát, M., Burget, L., Cernock `y, J., and Khudanpur, S. (2010). Recurrent neural\nnetwork based language model. In Interspeech, volume 2, page 3.\nMujika, A., Meier, F., and Steger, A. (2017). Fast-slow recurrent neural networks. NIPS.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). Improving language understand-\ning by generative pre-training. URL https://openai.com/blog/language-unsupervised/.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are\nunsupervised multitask learners. URL https://openai.com/blog/better-language-models/.\nRae, J. W., Dyer, C., Dayan, P., and Lillicrap, T. P. (2018). Fast parametric learning with activation\nmemorization. ICML.\nTieleman, T. and Hinton, G. E. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2).\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\nPolosukhin, I. (2017). Attention is all you need. In NIPS.\nZilly, J. G., Srivastava, R. K., Koutník, J., and Schmidhuber, J. (2017). Recurrent highway networks.\nICLR.\n6",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9560538530349731
    },
    {
      "name": "Transformer",
      "score": 0.8052459955215454
    },
    {
      "name": "Computer science",
      "score": 0.6472340226173401
    },
    {
      "name": "Language model",
      "score": 0.6175634860992432
    },
    {
      "name": "Char",
      "score": 0.5456098914146423
    },
    {
      "name": "Dynamic range",
      "score": 0.4423706531524658
    },
    {
      "name": "Algorithm",
      "score": 0.3464052677154541
    },
    {
      "name": "Artificial intelligence",
      "score": 0.314497172832489
    },
    {
      "name": "Engineering",
      "score": 0.18946030735969543
    },
    {
      "name": "Electrical engineering",
      "score": 0.1382891833782196
    },
    {
      "name": "Voltage",
      "score": 0.13496345281600952
    },
    {
      "name": "Computer vision",
      "score": 0.09244635701179504
    },
    {
      "name": "Pyrolysis",
      "score": 0.0
    },
    {
      "name": "Waste management",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 32
}