{
  "title": "Unsupervised Paraphrase Generation using Pre-trained Language Models",
  "url": "https://openalex.org/W3035051717",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Hegde, Chaitra",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Patil, Shrikumar",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2962953307",
    "https://openalex.org/W2531908596",
    "https://openalex.org/W2963343509",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2061433145",
    "https://openalex.org/W2963847417",
    "https://openalex.org/W2748868227",
    "https://openalex.org/W2963463583",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2996171631",
    "https://openalex.org/W2798664956",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2129468719",
    "https://openalex.org/W2964146920",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2963558220",
    "https://openalex.org/W174630521",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2970559004",
    "https://openalex.org/W2972136110",
    "https://openalex.org/W3102273025",
    "https://openalex.org/W2999003541",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W2952481978",
    "https://openalex.org/W2133512280"
  ],
  "abstract": "Large scale Pre-trained Language Models have proven to be very powerful approach in various Natural language tasks. OpenAI's GPT-2 \\cite{radford2019language} is notable for its capability to generate fluent, well formulated, grammatically consistent text and for phrase completions. In this paper we leverage this generation capability of GPT-2 to generate paraphrases without any supervision from labelled data. We examine how the results compare with other supervised and unsupervised approaches and the effect of using paraphrases for data augmentation on downstream tasks such as classification. Our experiments show that paraphrases generated with our model are of good quality, are diverse and improves the downstream task performance when used for data augmentation.",
  "full_text": "arXiv:2006.05477v1  [cs.CL]  9 Jun 2020\nUN S U P E RV I S E D PA R A P H R A S E GE N E R AT IO N U S I N G\nPR E -T R A IN E D LA N G UAG E MO D E L S\nChaitra Hegde\nFidelity Investments\nBoston, MA\nchaitra.vishwanathahegde@fmr.com\nShrikumar Patil\nFidelity Investments\nBoston, MA\nshrikumarrajendra.patil@fmr.com\nJune 11, 2020\nABSTRACT\nLarge scale Pre-trained Language Models have proven to be ve ry powerful approach in various\nNatural language tasks. OpenAI’s GPT -2 [1] is notable for it s capability to generate ﬂuent, well\nformulated, grammatically consistent text and for phrase c ompletions. In this paper we leverage this\ngeneration capability of GPT -2 to generate paraphrases wit hout any supervision from labelled data.\nW e examine how the results compare with other supervised and unsupervised approaches and the\neffect of using paraphrases for data augmentation on downst ream tasks such as classiﬁcation. Our\nexperiments show that paraphrases generated with our model are of good quality, are diverse and\nimproves the downstream task performance when used for data augmentation.\n1 Introduction\nParaphrasing is well known problem in NLP which has wide gamu t of applications including data augmentation, data\ncuration, intent mapping, semantic understanding. Paraph rasing can be used to generate synthetic data to augment\nexisting scarse datasets for training, and to enhance perfo rmance of downstream tasks such as classiﬁcation. It can\nalso be used to extend existing datasets to more variations, so that it becomes more generalizeable, because of the\nlinguistic variability added by paraphrasing.\nThe supervised approach towards paraphrasing suffers from lack of domain speciﬁc parallel data. The unsupervised\napproaches towards paraphrasing is heavily reliant on mach ine translation which still requires bilingual parallel da ta\ncorpus. Also, performance of machine-translation based ba ck-translation depends on performance constituting ma-\nchine translation systems. Both supervised approach and ma chine translation-based paraphrasing suffers when there\nis a severe domain shift during inference [2].\nGPT -2 [1] is exceptional at language generation. It predict s the next token in the sequence given all the tokens before\nit, i.e it optimizes for P (Xi|X<i). The pre-trained version of GPT -2 generates bland text with out any goal. By making\nthe model to generate T arget T by conditioning on the Source S , the language generation capability of GPT -2 can be\nutilized for generating meaningful text.\nIn this paper, we propose a novel unsupervised paraphrasing technique using GPT -2. As this is a unsupervised ap-\nproach, the model can be trained on the domain speciﬁc indepe ndent sentences at hand and generate paraphrases\nwithout suffering from domain shift. W e formulate the unsup ervised paraphrasing task as sentence reconstruction task\nfrom corrupted input. From a sentence, we omit all the stop wo rds to form a corrupted sentence, lets call it Source S,\nand the original sentence is used as T arget T . W e use GPT -2 to g enerate the T arget sentence given Source. i.e P (T |S)\n2 Related W ork\nThe task of textual paraphrasing has been of high interest in NLP research and has been tackled using variety of\napproaches. Some earlier approaches towards paraphrasing like [3], [4] and [5] were mostly based on linguistic\nA P RE P RIN T - J U N E 11, 2020\nstructures. In last few years, there has been a lot of work on f raming generation of paraphrases as a Seq2seq task\n[6][7][8]. Paraphrasing has also been explored as a means to improve Question answering in [9] and [10] by making\nthe training more robust by incorporating reformulations.\nDeep generative models and discrete latent structures for p araphrasing [11] have also been explored, and [12] proposed\na combined the approaches of V AE and Seq2Seq model, by condit ioning both encoder and decoder on the source\nsentence, Else ways, [13] and [14] involve reinforcement le arning.[15] suggests using multiple generators to generat e\nmore diverse and robust variations, whereas [16] investiga tes generating paraphrases that conform to a certain target\nsyntactic form.\nUnsupervised paraphrasing is picking up a lot of pace in para phrasing related research work. Approaches such as\n[17] and [18] employ variational autoencoders for sampling paraphrases from learned latent space. [19] use V ector-\nQuantized AutoEncoders [20] based model, claiming ability to generate more diverse while semantically closer sen-\ntences from input. [21] formulate paraphrasing as a stochas tic searching problem and involve unsupervised searching\nalgorithm, simulated annealing to generate paraphrases. D ifference between these unsupervised approaches, and our\napproach is that, we aim to explore the power of large-scale p re-trained language model, which due to the sheer\nsize and breadth of corpus they are trained on, perform extre mely well on downstream tasks. As these models are\nprimarily trained in a way to effectively learn to predict ne xt word by understanding context, we explore it’s effec-\ntiveness in paraphrasing problem setup. The only other work to this end involving use of pre-trained large language\nmodel for paraphrasing is in [22]. Our work, although being s imilar to architecture proposed in [22], the approach\ntaken is completely different, as they ﬁne-tune GPT -2 on lab elled paraphrasing dataset, by contrast, we aim to do\nit completely unsupervised, by making the model learn more o n reconstruction task rather than learn from labelled\nparaphrasing data. The evaluation in [22] is also signiﬁcan tly different, we run exhaustive quantitative evaluation a nd\ncompare with reported standard metric numbers from other me thods, including supervised and unsupervised methods,\non same datasets, to better demonstrate its effectiveness, along with evaluation of paraphrasing quality, diversity a nd\ncorrectness.\n3 Methodology\n3.1 Paraphrase Generation using GPT -2\nParaphrasing is a technique of generating a T arget sentence T for a the reference sentence P where the newly generated\nT arget Sentence T is semantically similar to reference sent ence P .\nGPT -2 [1] is a auto-regressive model which has achieved stat e-of-the art result in many of the benchmark tasks. It\nhas up-to 1.5 Billion parameters and was trained simply to pr edict the next word in 40GB of Internet text. W e utilize\nHuggingface Transformers Library [23] to ﬁnetune the GPT -2 model on a sentence reconstruction task to generate\nparaphrases.\n3.2 Data Preparation\nW e take unsupervised approach towards Paraphrase generati on and hence no parallel corpora for paraphrasing has\nbeen utilized, instead we create a dummy dataset for sentenc e re-construction task. W e corrupt the input sentence by\nremoving all the stop words, further we randomly shufﬂe and r eplace the remaining words 20% of the time. W e call\nthe corrupted sentence as source sequence S and the original uncorrupted sequence as target T . Goal is to reconstruct\nthe sentence T from the keywords/corrupted sentence S, P (T |S).\nA : Set of stop words; T : Sentence\nS : T − A;\nR : P araphrase/Reconstructed Sentence;\nOur stop word set has total of 252 words. After removing the st op words from sentences to form Source S, we\nencourage model to generate diverse paraphrases by shufﬂin g the words 20% of the time. T o encourage the model to\nhave new words in the reconstructed sentence that were not in the original sentence, 20% of the words are randomly\nreplaced with a synonym using syn-net.\n3.3 Model Fine-tuning for Paraphrase Generation\nW e utilize pre-trained GPT -2 checkpoint from Huggingface L ibrary and we further ﬁnetune the model on Sentence\nReconstruction task. W e concatenate the Source and T arget s equence separated by a special symbol to form the input\n2\nA P RE P RIN T - J U N E 11, 2020\nT able 1: Paraphrasing Examples : Output from our model\nT ype Sentence\nIn how do you send a private message to someone you’re followi ng on quora?\nCorrupted send private message following quora ?\nOut how do i send a private message through quora?\nOut how do i send a private message to a few of my friends on quor a?\nIn do you believe donald trump can make america great again?\nCorrupted believe donald trump america great ?\nOut do you believe donald trump is doing everything that he ca n to make america great again?\nIn if we see something in our dreams and it happens to come out t rue after few days, what does that mean?\nCorrupted dreams happens come out true after days, mean?\nOut if dreams happens to come out true in a few days, what does i t mean?\nIn what do i gift my boyfriend for his birthday?\nCorrupted gift boyfriend birthday?\nOut what gift can i get my boyfriend for his birthday?\nOut what gifts should i give my boyfriend on his birthday?\nsequence X. Let X=x0, x1, x2...xm,[sep], xm+1, ...xn. W e denote the Source sequence as S=x0, x1, ...xm and T arget\nsequence as T =xm+1, xm+2, ...xn. Special character [SEP ] is added to distinguish between source tokens and target\ntokens and it serves as a promt during inference.\n3.4 Candidate Paraphrase Generation and Scoring\nT o generate more than one paraphrase for a given input senten ce, we use top-k sampling [24]. W e generate 10 para-\nphrases for every input sentence, lets calls the paraphrase d sentence as R. W e eliminate the ones that are same as\noriginal sentence T or is different by only few characters. T o make sure that the paraphrases are semantically similar\nto the input sentence T , we use Sentence Transformers Librar y [25] to embed the Sentence T and Paraphrase R and\ncompute cosine similarity between them. W e form a valid cand idate set by retaining only those paraphrases that has\nthreshold above 0.75 and hence making sure the meaning is pre served. From now on, candidate set refers to valid\ncandidate set. Few examples of paraphrases can be seen in the table 1.\n4 Experiment\n4.1 Dataset\nFor training and evaluation purpose, we use Quora Question P air(QQP) dataset 1 . QQP dataset has 400k labelled\nexamples out of which 140k examples are actual paraphrases, and 300k unlabeled sentence pairs. Each entry in\nQQP dataset has ( question1, question2, is_duplicate) ﬁelds where question1 and question2 are sentences and\nis_duplicate indicates whether or not they are paraphrases. Out of 140k ac tual paraphrase sentence pairs, we take\nrandom 30k sentence pairs for our test set as done in [21] and [ 18] for the sake of easier comparison. W e take all the\nunique question1 from remaining 370k labelled and 300k unla beled sample giving total of 523010 unique sentences.\nW e take 463010 sentences for training and 60000 sentences fo r validating the sentence reconstruction task.\nT o demonstrate the performance enhancement of using paraph rasing for data augmentation, we use Stanford Sentiment\nTreebank (SST -2) dataset [26] as done in [19] which has 6920 t rain examples and 1821 test examples.\n4.2 T raining\nW e initialize our model with GPT -2 medium checkpoint which h as 345M parameters. The sentences are corrupted by\nremoving the stop words. The model is ﬁnetuned on sentence re construction task the same way GPT -2 was trained\nin [1]. validation set’s perplexity is used to do early stopp ing. The model is ﬁne-tuned on one Nvidia V100 DGX\nmachine for 2-5 epochs which takes 28 mins per epoch.\n1 https://www .kaggle.com/c/quora-question-pairs\n3\nA P RE P RIN T - J U N E 11, 2020\nT able 2: Paraphrasing Results - Compared with Supervised mo dels\nModel ↑METEOR ↓self-BLEU4\nD-P AGE 28.54 85.41\nPG-BS 28.88 61.89\nqian-etal-2019[15] 29.28 40.55\nOurs(best candidate) 50.48 N/A\nOurs(taking top 3 candidates) 49.45 30.27\nOur Model Human Evaluation: (A verage Accuracy)= 75.5%\nT able 3: Paraphrasing Results - Compared with unsupervised and unsupervised models\nModel ↑ROUGE-1 ↑ROUGE-2\nResidualLSTM 59.22 32.40\nSupervised V AE-SVG-eq 59.98 33.30\nPointer-generator 61.96 36.07\nTransformer 60.25 33.45\nTransformer+copy 63.34 37.31\nDNPG 63.73 37.75\nV AE 44.55 22.64\nCGMH 48.73 26.12\nUnsupervised UPSA 56.51 30.69\nOurs(best candidate) 60.33 34.59\nOurs(top 3 candidates) 59.43 33.61\n4.3 Evaluation Metric\nW e use different evaluation metrics to measure quality, cor rectness, diversity and usefulness of the generated para-\nphrases.\n• Quality : W e use ROUGE-L [27] and METEOR [28] for measuring th e quality of generated paraphrase. while\nBLEU-n calculates n gram overlap, ROUGE-L measures longest matching sequence, and it does not require\nconsecutive matches but in-sequence matches that reﬂect se ntence level word order. METEOR compares the\ntexts using not just word matching, but stemming and synonym matching, which is more desirable to gauge\nin paraphrasing quality.\n• Diversity : T o measure the diversity of candidate paraphras es, we use self-BLEU from [29]. Given set of\nparaphrase candidates for an input sentence, every candida te is picked once and others are used as references\nto compute BLEU score. And the average of all the BLEU scores c omputed for this candidate set is self-\nBLEU. A lower self-BLEU score indicates better diversity.\n• Usefulness : Using paraphrases for data augmentation for a d ownstream task can answer questions such\nas how good/diverse are paraphrases and do they provide any s ignal that was not provided by the original\nsentence and hence improving the downstream task’s perform ance. W e report % increment in performance\nof the downstream task.\n• Correctness : Though we ﬁlter the paraphrase candidates usi ng high threshold on cosine similarity, it doesn’t\npromise 100% correctness. T o measure what percent of the gen erated paraphrases are semantically similar to\ninput sentence, we make human evaluation of the model output .\n4.4 Results\n4.4.1 Quality and Diversity\nW e compute METEOR between generated paraphrase R and question2 from QQP test set for all 30k examples. W e\nconduct multiple experiments with respect to ﬁnal paraphra se selection from candidate paraphrases. In one of the\nexperiments, we only take the best one out of all the candidat es, where as in the second one we compute values for\neach candidate against question2 and take the average of them. W e have further calculate the se lf-BLEU among the\nparaphrase candidates. All these values can be seen in T able 2.\n4\nA P RE P RIN T - J U N E 11, 2020\nT able 4: Data Augmentation for Classiﬁcation\nSST -2\nModel Metric Acc. F1\nRoy and Grangier(2019)[19 ]: Baseline 81.93 83.15\nNB-SVM (trigram) Roy and Grangier(2019)[19 ] Enhanced 82.1 2 83.23\n% increment 0.23% 0.1%\nOurs Baseline 77.14 77.56\nNB-SVM (trigram) Ours Enhanced 79.39 79.71\nOurs % increment 2.92% 2.77%\nOurs Baseline 0.62 0.62\nTFIDF+RF Ours Enhanced 0.68 0.67\nOurs % increment 9.68% 8.06%\nW e compare our results with both supervised and un-supervis ed methods. The selective supervised methods we\ncompare our method to in table 2, are D-P AGE [30] and Pointer g enerator with Beam-search (PG-BS) [31] and [15].\nV alues seen in the table 2 are reported as per [15] for all thes e methods. The unsupervised methods that we compare\nour results with are V AE [17], CGMH [18] and UPSA [21]. The val ues for these papers seen in table 3 are taken from\n[21].\nWhile we don’t have access to exact test set used in [21], we fo llow the exact guideline from [21], [18] and [15] where\nthey mention taking random 30k sample out of 140k paraphrase s form QQP dataset.\n4.4.2 Usefulness\nT o demonstrate the usefulness of paraphrases on a downstrea m task, we trained NB-SVM and Random Forest Classiﬁer\non SST -2 and TREC dataset. W e have reported the percentage ch ange in accuracy before and after using paraphrases\nfor data augmentation. T able 4 shows that the improvement in performance is signiﬁcant when compared to [19]. Our\nbaseline results doesn’t match with [19] but absolute numbe rs shouldn’t matter as we keep all the settings same before\nand after data augmentation for measuring the % increment ex cept for the training data.\n4.4.3 Human Evaluation for Correctness: Higher[/Lower] BL EU does \"not\" mean better[/worse]\nParaphrase\nBLEU score measures how similar is the candidate text is to th e reference texts by looking at the overlapping n-grams.\nIf an example xi has lower BLEU when compared against the paraphrase given in evaluation set, it doesn’t mean\nthat xi is not the correct paraphrase, but it means that xi has different set of tokens than the reference sentence from\nevaluation set. While self-BLUE captures diversity, we use human evaluation to measure correctness.\nW e form two human evaluation sets containing 100 examples ea ch from QQP dataset having model generated para-\nphrase sentence R and ground truth paraphrase sentence( question2), we asked 2 humans who are ﬂuent in English to\nlabel whether or not R and question2 are paraphrases. Each of the annotators were asked to label i t as 0 or 1, where\nLabel=0 means not a paraphrase and Label=1 means they are par aphrases. W e compute average accuracy and saw that\n75.5% of the time, model generated paraphrase was correct.\n4.4.4 T akeaway\nT able 2 shows that our model has signiﬁcantly higher METEOR s core than supervised models, highlighting the higher\nquality of paraphrases. Low self-BLEU among candidates gen erated by our model indicate that the candidate sentences\nare more diverse than the candidate paraphrases generated b y other supervised methods. table 3 compares quality of\nparaphrase generated by our model with more model performan ces on Quora dataset with ROUGE metric,and our\nmodel does better in comparison to all unsupervised methods here as well, with being very much closely comparable\nto the supervised methods. Human evaluation of the paraphra ses by our model shows them to be correct 75.5% of\nthe time. W e further show that the generated paraphrases are not redundant but useful by showing improvement in\nperformance on two tasks using two models when paraphrases a re used for data augmentation on a downstream task.\nFinally, in table 4 we report that augmenting the training se t of SST -2 dataset with generated paraphrases from our\nmodel, sees much higher gain as compared to experiments repo rted in [19].\n5\nA P RE P RIN T - J U N E 11, 2020\n5 Conclusion and Future W ork\nIn this paper, we have introduced an unsupervised paraphras ing model that generates good quality, diverse and helpful\nparaphrases. W e demonstrate the usefulness of paraphrases by using them to augment the training data of downstream\ntasks. Since the model doesn’t need labelled data, it can be t rained as sentence reconstruction task on independent\nsentences pertaining to any domain without suffering from d omain shift problem faced by most of the supervised\nmodels.\nThe sentence corruption technique has a lot of scope for furt her exploration and improvement, and as future work, we’d\nlike to explore more complex and sophisticated way of corrup ting the input sentence, and it’s effect on the robustness\nin terms of reconstruction and generated paraphrases. W e ha ve seen that number of epochs for training the model is a\ntrade-off between correctness and diversity, and we’d like to investigate more on that too in subsequent experiments.\nReferences\n[1] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amo dei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. 2019.\n[2] Philipp Koehn and Rebecca Knowles. Six challenges for ne ural machine translation. In Proceedings of the First\nW orkshop on Neural Machine T ranslation , pages 28–39, V ancouver, August 2017. Association for Comp utational\nLinguistics.\n[3] Regina Barzilay and Lillian Lee. Learning to paraphrase : An unsupervised approach using multiple-sequence\nalignment. In Proceedings of the 2003 Human Language T echnology Conferen ce of the North American Chapter\nof the Association for Computational Linguistics , pages 16–23, 2003.\n[4] Chris Quirk, Chris Brockett, and William Dolan. Monolin gual machine translation for paraphrase generation.\nIn Proceedings of the 2004 Conference on Empirical Methods in N atural Language Processing , pages 142–149,\nBarcelona, Spain, July 2004. Association for Computationa l Linguistics.\n[5] Pablo Duboue and Jennifer Chu-Carroll. Answering the qu estion you wish they had asked: The impact of\nparaphrasing for question answering. In Proceedings of the Human Language T echnology Conference of the\nNAACL, Companion V olume: Short P apers , pages 33–36, New Y ork City, USA, June 2006. Association for\nComputational Linguistics.\n[6] Ziqiang Cao, Chuwei Luo, W enjie Li, and Sujian Li. Joint c opying and restricted generation for paraphrase. In\nProceedings of the Thirty-First AAAI Conference on Artiﬁci al Intelligence , AAAI’17, page 3152–3158. AAAI\nPress, 2017.\n[7] Aaditya Prakash, Sadid A. Hasan, Kathy Lee, V ivek Datla, Ashequl Qadir, Joey Liu, and Oladimeji Farri. Neural\nparaphrase generation with stacked residual LSTM networks . In Proceedings of COLING 2016, the 26th Interna-\ntional Conference on Computational Linguistics: T echnica l P apers, pages 2923–2934, Osaka, Japan, December\n2016. The COLING 2016 Organizing Committee.\n[8] Y u Su and Xifeng Y an. Cross-domain semantic parsing via p araphrasing. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing , pages 1235–1246, Copenhagen, Denmark, September\n2017. Association for Computational Linguistics.\n[9] Christian Buck, Jannis Bulian, Massimiliano Ciaramita , Andrea Gesmundo, Neil Houlsby, W ojciech Gajewski,\nand W ei W ang. Ask the right questions: Active question refor mulation with reinforcement learning. ArXiv,\nabs/1705.07830, 2017.\n[10] Li Dong, Jonathan Mallinson, Siva Reddy, and Mirella La pata. Learning to paraphrase for question answering.\nArXiv, abs/1708.06022, 2017.\n[11] Y ao Fu, Y ansong Feng, and John P . Cunningham. Paraphras e generation with latent bag of words. ArXiv,\nabs/2001.01941, 2019.\n[12] Ankush Gupta, Arvind Agarwal, Prawaan Singh, and Piyus h Rai. A deep generative framework for paraphrase\ngeneration. ArXiv, abs/1709.05074, 2018.\n[13] Zichao Li, Xin Jiang, Lifeng Shang, and Huang Li. Paraph rase generation with deep reinforcement learning. In\nEMNLP, 2018.\n[14] T adashi Nomoto. Generating paraphrases with lean voca bulary. In Proceedings of the 12th International Con-\nference on Natural Language Generation , pages 438–442, T okyo, Japan, October–November 2019. Asso ciation\nfor Computational Linguistics.\n6\nA P RE P RIN T - J U N E 11, 2020\n[15] Lihua Qian, Lin Qiu, W einan Zhang, Xin Jiang, and Y ong Y u . Exploring diverse expressions for paraphrase\ngeneration. In Proceedings of the 2019 Conference on Empirical Methods in N atural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3173–3182,\nHong Kong, China, November 2019. Association for Computati onal Linguistics.\n[16] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettl emoyer. Adversarial example generation with syntacti-\ncally controlled paraphrase networks. In NAACL-HLT, 2018.\n[17] Samuel R. Bowman, Luke V ilnis, Oriol V inyals, Andrew M. Dai, Rafal Józefowicz, and Samy Bengio. Generat-\ning sentences from a continuous space. In CoNLL, 2016.\n[18] Ning Miao, Hao Zhou, Lili Mou, Rui Y an, and Lei Li. Cgmh: C onstrained sentence generation by metropolis-\nhastings sampling. In AAAI, 2018.\n[19] Aurko Roy and David Grangier. Unsupervised paraphrasi ng without translation. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lingui stics, pages 6033–6039, Florence, Italy, July 2019.\nAssociation for Computational Linguistics.\n[20] Aäron van den Oord, Oriol V inyals, and Koray Kavukcuogl u. Neural discrete representation learning. In NIPS,\n2017.\n[21] Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou , and Sen Song. Unsupervised paraphrasing by\nsimulated annealing. ArXiv, abs/1909.03588, 2019.\n[22] Sam Witteveen and Martin Andrews. Paraphrasing with la rge language models. In NGT@EMNLP-IJCNLP,\n2019.\n[23] Thomas W olf, Lysandre Debut, V ictor Sanh, Julien Chaum ond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew . Hug gingface’s transformers: State-of-the-art\nnatural language processing. ArXiv, abs/1910.03771, 2019.\n[24] Angela Fan, Mike Lewis, and Y ann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Lingui stics (V olume 1: Long P apers) , pages 889–898,\nMelbourne, Australia, July 2018. Association for Computat ional Linguistics.\n[25] Nils Reimers and Iryna Gurevych. Sentence-bert: Sente nce embeddings using siamese bert-networks. In Pro-\nceedings of the 2019 Conference on Empirical Methods in Natu ral Language Processing . Association for Com-\nputational Linguistics, 11 2019.\n[26] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher\nPotts. Recursive deep models for semantic compositionalit y over a sentiment treebank. In Proceedings of the\n2013 Conference on Empirical Methods in Natural Language Pr ocessing, pages 1631–1642, Seattle, W ashington,\nUSA, October 2013. Association for Computational Linguist ics.\n[27] Chin-Y ew Lin. ROUGE: A package for automatic evaluatio n of summaries. In T ext Summarization Branches\nOut, pages 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.\n[28] Alon Lavie and Abhaya Agarwal. METEOR: An automatic met ric for MT evaluation with high levels of corre-\nlation with human judgments. In Proceedings of the Second W orkshop on Statistical Machine T ranslation, pages\n228–231, Prague, Czech Republic, June 2007. Association fo r Computational Linguistics.\n[29] Pengfei Zhu, Zhuosheng Zhang, Jiangtong Li, Y afang Hua ng, and Hai Zhao. Lingke: a ﬁne-grained multi-turn\nchatbot for customer service. In Proceedings of the 27th International Conference on Comput ational Linguistics:\nSystem Demonstrations , pages 108–112, Santa Fe, New Mexico, August 2018. Associat ion for Computational\nLinguistics.\n[30] Qiongkai Xu, Juyan Zhang, Lizhen Qu, Lexing Xie, and Ric hard Nock. D-page: Diverse paraphrase generation.\nArXiv, abs/1808.04364, 2018.\n[31] Abigail See, Peter J. Liu, and Christopher D. Manning. G et to the point: Summarization with pointer-generator\nnetworks. ArXiv, abs/1704.04368, 2017.\n7",
  "topic": "Paraphrase",
  "concepts": [
    {
      "name": "Paraphrase",
      "score": 0.9658794403076172
    },
    {
      "name": "Natural language processing",
      "score": 0.6745376586914062
    },
    {
      "name": "Computer science",
      "score": 0.5803611278533936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5725678205490112
    },
    {
      "name": "Language model",
      "score": 0.48447155952453613
    },
    {
      "name": "Linguistics",
      "score": 0.35168129205703735
    },
    {
      "name": "Philosophy",
      "score": 0.08372750878334045
    }
  ],
  "institutions": []
}