{
  "title": "SkinViT: A transformer based method for Melanoma and Nonmelanoma classification",
  "url": "https://openalex.org/W4390265148",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3036345322",
      "name": "Somaiya Khan",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2107620561",
      "name": "Ali Khan",
      "affiliations": [
        "Nanjing University of Aeronautics and Astronautics"
      ]
    },
    {
      "id": "https://openalex.org/A3036345322",
      "name": "Somaiya Khan",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2107620561",
      "name": "Ali Khan",
      "affiliations": [
        "Nanjing University of Aeronautics and Astronautics"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2895763047",
    "https://openalex.org/W3046082810",
    "https://openalex.org/W3001584199",
    "https://openalex.org/W4294549670",
    "https://openalex.org/W4298149788",
    "https://openalex.org/W3037439917",
    "https://openalex.org/W4309680622",
    "https://openalex.org/W2965191493",
    "https://openalex.org/W2895232172",
    "https://openalex.org/W4309689151",
    "https://openalex.org/W4210257375",
    "https://openalex.org/W4293240934",
    "https://openalex.org/W3173926515",
    "https://openalex.org/W4306404511",
    "https://openalex.org/W3197638240",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W4200323263",
    "https://openalex.org/W4200204081",
    "https://openalex.org/W2971897287",
    "https://openalex.org/W4281730194",
    "https://openalex.org/W2914959431",
    "https://openalex.org/W3044795413",
    "https://openalex.org/W3029164997",
    "https://openalex.org/W3018821700",
    "https://openalex.org/W4206342161",
    "https://openalex.org/W4221060026",
    "https://openalex.org/W3213922069",
    "https://openalex.org/W6797153837",
    "https://openalex.org/W6655798345",
    "https://openalex.org/W4312847199",
    "https://openalex.org/W4295308583",
    "https://openalex.org/W2020502398",
    "https://openalex.org/W4288720451",
    "https://openalex.org/W3046918297",
    "https://openalex.org/W4362665647",
    "https://openalex.org/W4283526860"
  ],
  "abstract": "Over the past few decades, skin cancer has emerged as a major global health concern. The efficacy of skin cancer treatment greatly depends upon early diagnosis and effective treatment. The automated classification of Melanoma and Nonmelanoma is quite challenging task due to presence of high visual similarities across different classes and variabilities within each class. According to the best of our knowledge, this study represents the classification of Melanoma and Nonmelanoma utilising Basal Cell Carcinoma (BCC) and Squamous Cell Carcinoma (SCC) under the Nonmelanoma class for the first time. Therefore, this research focuses on automated detection of different skin cancer types to provide assistance to the dermatologists in timely diagnosis and treatment of Melanoma and Nonmelanoma patients. Recently, artificial intelligence (AI) methods have gained popularity where Convolutional Neural Networks (CNNs) are employed to accurately classify various skin diseases. However, CNN has limitation in its ability to capture global contextual information which may lead to missing important information. In order to address this issue, this research explores the outlook attention mechanism inspired by vision outlooker, which improves important features while suppressing noisy features. The proposed SkinViT architecture integrates an outlooker block, transformer block and MLP head block to efficiently capture both fine level and global features in order to enhance the accuracy of Melanoma and Nonmelanoma classification. The proposed SkinViT method is assessed by different performance metrics such as recall, precision, classification accuracy, and F1 score. We performed extensive experiments on three datasets, Dataset1 which is extracted from ISIC2019, Dataset2 collected from various online dermatological database and Dataset3 combines both datasets. The proposed SkinViT achieved 0.9109 accuracy on Dataset1, 0.8911 accuracy on Dataset3 and 0.8611 accuracy on Dataset2. Moreover, the proposed SkinViT method outperformed other SOTA models and displayed higher accuracy compared to the previous work in the literature. The proposed method demonstrated higher performance efficiency in classification of Melanoma and Nonmelanoma dermoscopic images. This work is expected to inspire further research in implementing a system for detecting skin cancer that can assist dermatologists in timely diagnosing Melanoma and Nonmelanoma patients.",
  "full_text": "RESEA RCH ARTICL E\nSkinViT: A transformer based method for\nMelanoma and Nonmelanoma classification\nSomaiya Khan\nID\n1☯\n, Ali Khan\n2☯\n*\n1 School of Electronics Engineeri ng, Beijing University of Posts and Telecomm unications , Beijing, China,\n2 School of Computer Science and Technolog y, Nanjing University of Aeronautic s and Astrona utics, Nanjing,\nChina\n☯ These authors contribu ted equally to this work.\n* kaa503@ zjnu.edu.cn\nAbstract\nOver the past few decades, skin cancer has emerged as a major global health concern. The\nefficacy of skin cancer treatment greatly depends upon early diagnosis and effective treat-\nment. The automated classification of Melanoma and Nonmelanoma is quite challenging\ntask due to presence of high visual similarities across different classes and variabilities\nwithin each class. According to the best of our knowledge, this study represents the classifi-\ncation of Melanoma and Nonmelanoma utilising Basal Cell Carcinoma (BCC) and Squa-\nmous Cell Carcinoma (SCC) under the Nonmelanoma class for the first time. Therefore, this\nresearch focuses on automated detection of different skin cancer types to provide assis-\ntance to the dermatologists in timely diagnosis and treatment of Melanoma and Nonmela-\nnoma patients. Recently, artificial intelligence (AI) methods have gained popularity where\nConvolutional Neural Networks (CNNs) are employed to accurately classify various skin dis-\neases. However, CNN has limitation in its ability to capture global contextual information\nwhich may lead to missing important information. In order to address this issue, this\nresearch explores the outlook attention mechanism inspired by vision outlooker, which\nimproves important features while suppressing noisy features. The proposed SkinViT archi-\ntecture integrates an outlooker block, transformer block and MLP head block to efficiently\ncapture both fine level and global features in order to enhance the accuracy of Melanoma\nand Nonmelanoma classification. The proposed SkinViT method is assessed by different\nperformance metrics such as recall, precision, classification accuracy, and F1 score. We\nperformed extensive experiments on three datasets, Dataset1 which is extracted from\nISIC2019, Dataset2 collected from various online dermatological database and Dataset3\ncombines both datasets. The proposed SkinViT achieved 0.9109 accuracy on Dataset1,\n0.8911 accuracy on Dataset3 and 0.8611 accuracy on Dataset2. Moreover, the proposed\nSkinViT method outperformed other SOTA models and displayed higher accuracy com-\npared to the previous work in the literature. The proposed method demonstrated higher per-\nformance efficiency in classification of Melanoma and Nonmelanoma dermoscopic images.\nThis work is expected to inspire further research in implementing a system for detecting skin\ncancer that can assist dermatologists in timely diagnosing Melanoma and Nonmelanoma\npatients.\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 1 / 19\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Khan S, Khan A (2023) SkinViT: A\ntransforme r based method for Melanoma and\nNonmelanom a classifica tion. PLoS ONE 18(12):\ne0295151. https://d oi.org/10.1371/j ournal.\npone.029515 1\nEditor: Beatrice Nardone , Northwester n University\nFeinberg School of Medicine Galter Health\nSciences Library, UNITED STATES\nReceived: May 24, 2023\nAccepted: November 14, 2023\nPublished: December 27, 2023\nCopyright: © 2023 Khan, Khan. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: The data used in this\nstudy is publicly available and can be accessed\nthrough https://www.k aggle.com/ datasets/\nsalviohex ia/isic-2019-skin -lesion- images-\nforclassific ation, https://www.k aggle.com/ datasets/\nfarhatullah83 98/skin-lesion -dermis-datas et, https://\nwww.kaggl e.com/datas ets/kliuiev/ph2d atabaseaddi\nand https://ww w.kaggle.com/ datasets/\nthomasrade maker/derm netnz-100-cle aned.\nFunding: The authors received no specific funding\nfor this study.\n1 Introduction\nCancer has become a major concern in the healthcare sector, with a projected global inci-\ndence of 18.1 million in 2020 [1]. As per World Health Organization (WHO) report [2], can-\ncer has emerged as one of the top killers worldwide, responsible for approximately 10\nmillion fatalities in 2020 alone. Nowadays, skin cancer is one of the most common types of\nthe disease to be detected, with an estimated 1.2 million cases reported globally in 2020.\nAmong the two primary kinds of skin cancer, that is Melanoma and Nonmelanoma, Mela-\nnoma is more fatal than the latter. According to American Cancer Society, the estimated\nannual incidence rate of Melanoma in United States is about 100,000 people, with around\n7,650 succumbing to it [3]. Melanoma has one of the highest incidence rates in New Zealand,\nwith 6000 people diagnosed annually and accounting for almost 80% of all skin cancer deaths\n[4]. Cancer Research UK statistics show that the relative survival rate for skin cancer after 5\nyear is 90% [5]. The survival rate for skin cancer can be increased by early detection and\ntreatment.\nThe healthcare sector has been revolutionized with the recent advancements in Artificial\nIntelligence (AI) [6]. The emergence of machine learning in computer vision has opened up\nnew avenues for Computer Aided Diagnosis (CAD) [7, 8]. Over the years, CAD has made con-\nsiderable progress, especially in diagnosis of cancer such as lung [9], breast [10], thyroid [11],\nbrain [12], diabetic retinopathy [13], liver [14] etc. Due to the alarming increase in skin cancer\nincidence rate, there is a shortage of experienced dermatologists, which can lead to difficulties\nin timely skin cancer identification. Moreover, CAD tools are more efficient compared to\nexisting clinical approaches, saving both time and cost. Therefore, a CAD system for skin can-\ncer detection is essential.\nThe rapid developments and improvements in deep learning (DL) have exponentially\nimpacted the performance of CAD systems [15]. Convolutional Neural Network (CNN), a DL\nalgorithm, has been employed extensively in applications such as classification of images and\nobject detection. With the advancements in CNN, DL has seen significant rise in real-world\napplications such as surveillance [16, 17], smart city applications [18, 19], healthcare [20] etc.\nThe emergence of self-attention in vision based applications has paved the way for the success\nof transformers [21]. Vision Transformer, which employs self-attention mechanism, has dis-\nplayed promising results when trained on large datasets [22].\nCNN faces problems distinguishing low-level features that may result in missing crucial\ninformation. Furthermore, minimum false alarms are vital for accurate diagnosis in the\nmedical field. We propose transformer based approach which employs outlook attention\nmechanism to generate fine level features for token representation that helps improve the\nmodel performance. Unlike other vision transformers that use dot product attention com-\nputation, outlook attention approach utilizes linear projection to aggregate surrounding\ntokens from anchor token features. This characteristic of outlook attention mechanism\nenhances the cost-effectiveness of the model. Furthermore, SVM with L2 kernel classifier is\nutilized for the classification task. The proposed SkinViT model ensures Melanoma and\nNonmelanoma detection with higher accuracy. The major contributions of this research are\ngiven below:\n1. Distinghuising between Melanoma and Nonmelanoma is a significant challenge owing to\nconsiderable visual interclass similarities and intraclass variations. To the best of our knowl-\nedge, this is the first work on Melanoma and Nonmelanoma classification using BCC and\nSCC in the Nonmelanoma class. Therefore, this work focuses on the different types of skin\ncancer detection with the help of CAD.\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 2 / 19\nCompeting interests : The authors have declared\nthat no competing interests exist.\n2. The datasets used for this research have imbalance class distribution which can lead to the\nmisinterpretation of the class with the fewer image samples. Thus, we perform data aug-\nmentation technique to address the imbalance dataset issue.\n3. While the visual differences in skin lesions may seem small and localized, it is crucial to\nconsider fine level global context information for efficient recognition of the skin lesions.\nTherefore, we present a novel DL model named as SkinViT to efficiently integrate fine level\ninformation with outlooker and global information with transformer for more reliable clas-\nsification method of Melanoma and Nonmelanoma images.\n4. Furthermore, a detailed analysis of the proposed SkinViT approach with different optimiz-\ners and classifiers on three datasets to detect Melanoma and Nonmelanoma is presented\nand compared with other SOTA models and existing research to validate the efficacy of the\nSkinViT model.\nThe remaining sections are structured as follows: Section 2 presents the related work. Next,\nSection 3 describes the datasets used in this research and the proposed method. Further, Sec-\ntion 4 presents simulation setup and results, Section 5 provides discussion and conclusion.\n2 Related work\nOver the past few years, the massive increase in skin cancer cases has overwhelmed dermatolo-\ngists. To help in the accurate differentiation and diagnosis of melanoma from other skin\nlesions, International Society for Digital Imaging of Skin (ISDIS) took initiative to tackle the\nproblem of the increasing incidence rate of skin cancer by introducing an annual challenge\nknown as International Skin Imaging Collaboration (ISIC) [23]. With all these efforts, there\nhave been considerable interest among researchers in exploring computer vision techniques\nfor skin cancer detection [24, 25].\nIn [26], Pham et al. conducted comparative analysis of different data processing methods,\nfeature extraction methods and classifiers for melanoma classification. In their analysis, Linear\nNormalization as data processing, HSV as feature extraction and Balanced Random Forest\nclassifier performed best with 74.75% accuracy on the HAM10000 dataset. In [27], Shen et al.\nproposed high performance data augmentation, which can be integrated to any deep learning\nmethod to classify skin lesions. Their proposed approach with efficienetb0 showed the best\nresults with 85.3% accuracy on ISIC2018/HAM10000 dataset. In [28], Zhang et al. proposed\nconvolutional neural network with attention residual learning (ARL-CNN) to classify skin dis-\neases. Their proposed method achieved 91.8% AUC on the ISIC2017 dataset for binary classifi-\ncation task. Liu et al. [29] proposed a mid level feature representation method for learning\nfeatures, and the CNN model is used as an extractor of ROI images. Their proposed method\nachieved 92.1% AUC in classifying melanoma and S. keratosis using the ISIC2017 dataset. In\n[30], Zhou et al. proposed convolutional spiking neural networks (SNN) employing spike-\ntime-dependent plasticity (STDP) learning rate for melanoma skin lesions classification. Their\nproposed method showed an accuracy of 87.7% in classifying malignant melanoma and nevus\nskin lesions using the ISIC2018 dataset.\nGouda et al. [31] proposed pre-trained deep learning models based on transfer learning\nsuch as CNN, ResNet50, InceptionV3 and Inception ResNet for skin cancer classification. In\ntheir analysis, InceptionV3 showed the highest accuracy of 85.76% in classifying malignant\nand benign skin lesions using the ISIC2018 dataset. In [32], Damian et al. proposed MobileNet\nbased model transfer learning for melanoma and nevus skin lesion classification. Their pro-\nposed method an achieved accuracy of 89.7% using the ISIC2018 dataset. In [33], Indraswari\net al. proposed transfer learning technique based on the MobileNetV2 model for melanoma\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 3 / 19\nclassification. Their proposed method showed 85% accuracy on the ISIC archive dataset.\nHoang et al. [34] proposed EW-FCM+ShuffleNet based hybrid method, entropy-based weight-\ning and first-order cumulative moment (EW-FCM) is used for segmentation and wide-Shuf-\nfleNet for classification. In their proposed method EW-FCM with wide-ShuffleNet performed\nbest with 86.33% accuracy for multi-class classification on the HAM10000 dataset. In [35],\nLopez et al. proposed CNN model based on transfer learning such as VGGNet (VGG16), for\nskin lesion classification. Their proposed method achieved 81.33% accuracy in classifying\nmalignant and benign skin lesions using the ISIC2016 dataset.\nIn [36], Xie et al. proposed a Swin-SimAM based hybrid method for detecting melanoma\nwhere a Swin transformer is used for feature extraction and SimAM is parameter-free atten-\ntion module. Their proposed method displayed 90% AUC in classifying melanoma and non-\nmelanoma (nevus and seborrheic keratosis). In [37], Naeem et al. introduced SCDNet\napproach that integrates VGG16 architecture with convolutional neural networks. Their pro-\nposed SCDNet method showed the accuracy of 96.91% on ISIC2019 dataset for multi-class\nskin cancer classification. Tahir et al. [38] proposed DSCC_Net which utilizes convolutional\nneural networks (CNN). Their proposed method demonstrated promising result with an accu-\nracy of 94.17% on three publicly available datasets (ISIC2020, HAM10000 and DermIS) for the\ntask of classification of multi- class skin cancer types. Table 1 details the comprehensive analy-\nsis of all the work in the literature.\nThe reviewed approaches for melanoma detection showed promising results, mainly in\ndetecting melanoma from benign dermoscopic images. The existing research predominantly\nfocused on detecting malignant and benign lesions or melanoma and benign with either nevus\nor seborrheic keratosis in the benign class which are non-cancerous skin lesion types. To the\nbest of our knowledge, no existing research considered the Melanoma and Nonmelanoma\n(BCC and SCC) classes which are the most common skin cancer types for the classification\ntask. Discriminating Melanoma and Nonmelanoma is quite challenging due to high intraclass\ndifferences. Although the previous research showed promising results, but for efficient skin\ncancer detection, there is a further need to research Melanoma and Nonmelanoma where\nTable 1. Comparativ e analysis of the related work.\nAuthor Method Dataset Classes Accuracy Precision Recall AUC\nPham et al. [26] LN+HSV+ Balanced Random\nForest\nHAM100 00 Melanoma and Benign 74.75% – 90.09% –\nShen et al. [27] Augmen tation+Effic ientNet HAM100 00 Multi-cl ass 85.3% – 78.9% –\nZhang et al. [28] ARL-CN N ISIC2017 Melanoma and S. Keratosis – – – 91.8%\nLiu et al. [29] Mid-le vel features+S VM ISIC2017 Melanoma and S. Keratosis – – – 92.1%\nZhou et al. [30] STDP based SNN ISIC2018/\nHAM100 00\nMelanoma and Nevus 87.7% 84.6% 90.3% 83.6%\nGouda et al. [31] Incept ionV3 ISIC2018 Malignan t and Benign 85.76% – – 86%\nDamian et al. [32] MobileNet ISIC2018/\nHAM100 00\nMelanoma and Nevus 89.7% 92.08% 86.41% 89.64%\nIndraswar i et al.\n[33]\nMobileNet V2 ISIC Archive Malignan t and Benign 85% 83% 85% –\nHoang et al. [34] EW-FCM+ ShuffleNe t HAM100 00 Multi-cl ass 86.33% – 86.33% –\nLopez et al. [35] VGGNet (VGG16) ISIC2016 Malignan t and Benign 81.33% 79.74% 78.66% –\nXie et al. [36] Swin-SimA M ISIC2017 Melanoma and Nonmelanom a (Nevus+S.\nKeratosis )\n– – – 90%\nNaeem et al. [37] SCDNet (VGG16+CNN ) ISIC2019 Multi-cl ass 96.91% 92.19% 92.18% –\nTahir et al. [38] DSCC_N et ISIC2020 Multi-cl ass 94.17% 94.28% 93.76% 99.43%\nhttps://do i.org/10.1371/j ournal.pone .0295151.t001\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 4 / 19\nNonmelanoma class has BCC and SCC. The previous work primarily focused on CNN models.\nCNN faces problems distinguishing low level features that may result in missing crucial infor-\nmation. Furthermore, minimum false alarm is vital for accurate medical diagnosis. The focus\nof this research is on the automatic and accurate detection of Melanoma and Nonmelanoma,\nwhich can help reduce the mortality rate due to skin cancer by early diagnosis and also help\nease the burden on dermatologists. For efficient Melanoma and Nonmelanoma classification,\nwe propose the SkinViT model based on outlooker and transformer and further aid in devel-\nopment of automated skin cancer detection.\n3 Materials and methods\nThis section details the proposed SkinViT method and dermoscopic image datasets utilized for\nMelanoma and Nonmelanoma classification.\n3.1 Dataset acquisition\nThis study considers the binary classification problem of Melanoma and Nonmelanoma. For\nthis research, we considered three dermoscopic image datasets: Melanoma class and Nonmela-\nnoma class, where Nonmelanoma comprising of Basal Cell Carcinoma and Squamous Cell\nCarcinoma. Dataset1 [39] is a public dataset that contains 25,331 dermoscopic images in 8 dif-\nferent classes. We considered only two classes, with 4521 Melanoma dermoscopic images and\n3952 Nonmelanoma images. Dataset2 contains dermoscopic images collected from various\nonline dermotological database such as DermIS [40], PH2 [41] and Dermnet-NZ [42], to get\nmore representation of the considered classes for classification task. We considered two classes\nwith 410 Melanoma images and 672 Nonmelanoma images. For Dataset3, we combined both\ndatasets, Dataset1 and Dataset2, to have 4930 Melanoma images and 4624 Nonmelanoma\nimages.\nMelanoma is comparatively less common but the most fatal form of skin cancer. It begins\nin the melanocytes which is responsible for producing melanin, a pigment that gives color to\nthe skin. It is usually a dark colored mole and changes shape, size or color over the time. Non-\nmelanoma is the most common kind of skin cancer and can be categorized as Basal Cell Carci-\nnoma (BCC) and Squamous Cell Carcinoma (SCC). BCC affects the basal cell of the epidermis\nskin layer. BCC can have varied appearances but often appear as small pinkish or pearly-white\nbump. It can also be a red scaly patch sometimes with brown or black pigment within the\npatch. SCC is the development of keratinocytes in the squamous cell of epidermis skin layer.\nSCC can have variety of appearances where it typically appears as red to pink rough or scaly\npatch and also look like raised wart-like growth sometimes with a spiky horn-like surface stick-\ning out. The sample images of Melanoma and Nonmelanoma are shown in Fig 1.\nFig 1. Sample Image of a) Melanoma and b) Nonmelano ma (BCC) and c) Nonmelan oma (SCC).\nhttps://d oi.org/10.1371/j ournal.pon e.0295151.g0 01\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 5 / 19\nFig 1(a) shows a melanoma skin lesion where it can be seen a dark brown asymmetrical\nshape having a diameter greater than 6mm. Fig 1(b) illustrates a BCC skin lesion having a\npinkish bump with a dark brown pigment. Fig 1(c) shows an SCC skin lesion having crusty\nraised growth with a spiky surface sticking out.\n3.2 Dataset preprocessing\nData preprocessing is one of the key steps in deep learning models. The dermoscopic images\nof Melanoma are labelled as 1, whereas the dermoscopic images of Nonmelanoma are labelled\nas 0. Dataset1 has a total of 8473 dermoscopic images, with 4521 images for the Melanoma\nclass and 3952 images for the Nonmelanoma class. Dataset2 comprises 1082 dermoscopic\nimages for the considered binary classification task, where 410 images are assigned to the Mel-\nanoma class whereas 672 images are assigned to the Nonmelanoma class. Dataset3 consists of\n4930 images for Melanoma and 4624 images for the Nonmelanoma class, totaling 9554 images.\nThe datasets are split into training and testing by applying the 80:20 splitting rule, where 80%\nis for training and 20% for testing purpose as depicted in Table 2. The images in datasets are of\ndifferent sizes so it is essential to convert the images to a single image size to match the input\nof the deep learning model. Therefore, all the images in our research are converted to the\nimage size of 224 × 224.\n3.3 Data augmentation\nThe size of the data has significant impact on the performance of the deep learning models.\nThe more the data size, the greater the chances of deep learning models to perform better. The\ndatasets considered for our research have imbalanced data in the considered classes which can\ngreatly impact the performance of the model. Therefore, data augmentation technique is\napplied to handle the imbalanced data, which can cause misinterpretation of the class with\nfewer sample images. For our research, we performed geometric, also known as position aug-\nmentation, on the training data, as depicted in Table 3. The images are transformed by 180˚\nrotation, horizontal flip and shear transformation by a factor of 0.2.\n3.4 Proposed SkinViT architecture\nThe proposed SkinViT model is designed to classify Melanoma and Nonmelanoma. The archi-\ntecture of the proposed SkinViT model is depicted in Fig 2. Inspired by VOLO [43] and ViT\n[44], the proposed SkinViT model combines the outlooker block, transformer block and Skin-\nViT multi-layer perceptron (MLP) head block. The proposed method first converted images\ninto patches of size 8 × 8 which are then passed through the outlooker for generating the fine-\nlevel token representation. After that, the tokens are further down sampled using a patch\nembedding module which is then passed to the Transformer encoder for processing. Then the\nTable 2. Splitting of data.\nDataset Classes Training Testing Total\nDataset1 Melanoma 3616 905 4521\nNonmelanoma 3162 790 3952\nDataset2 Melanoma 328 82 410\nNonmelanoma 538 134 672\nDataset3 Melanoma 3944 986 4930\nNonmelanoma 3700 924 4624\nhttps://do i.org/10.1371/j ournal.pone .0295151.t002\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 6 / 19\noutput is passed into the Multi-Layer Perceptron (MLP) head, which in our proposed SkinViT\nconsists of flatten layer, dense layer with Swish function and a classification layer with SVM\nlinear kernel L2 to output the prediction for skin cancer type. The details of the proposed Skin-\nViT architecture is presented as follows:\n1. Outlooker: The outlooker is responsible for generating fine level features for tokenization.\nThe outlooker comprises of outlook attention layer, which encodes spatial information and\nMLP, which is responsible for inter-channel information interaction. The outlook atten-\ntion, unlike self-attention, computes the similarity between each spatial location (i, j) and\nneighboring elements to focus on fine level features. For given input X, each C–dimensional\nfeature is projected with two layers of linear weights; A 2 R\nH �W �K\n4\nas outlook weights and\nV 2 R\nH×W×C\nas value representations.\nSuppose the value representations within the local window at (i, j) are V\nDi;j\n2 R\nC�K\n2\nwhere\nV\nDi;j\n¼ V\niþp\u0000\nK\n2\nb c\n;jþq\u0000\nK\n2\nb c\nn o\n; 0 � p; q < K ð1Þ\nThe outlook weight is reshaped into\n^\nA\nði;jÞ\n2 R\nK\n2\n�K\n2\nto obtain the aggregated value of atten-\ntion weight. The value projection is the weighted average of outlook weights and can be cal-\nculated as follows:\nY\nDi;j\n¼ Mat MulðSof tmaxð\n^\nA\ni;j\nÞ; V\nDi;j\nÞ ð2Þ\nTable 3. Data augmentatio ns.\nAugmen tation Value\nRotation 180˚\nShear Transforma tion 0.2\nHorizont al Flip True\nhttps://d oi.org/10.1371/j ournal.pon e.0295151.t00 3\nFig 2. Architect ure of SkinViT.\nhttps://doi.o rg/10.1371/j ournal.pone .0295151.g002\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 7 / 19\nThe outlook weight is reshaped into\n^\nA\nði;jÞ\n2 R\nK\n2\n�K\n2\nto obtain the aggregated value of atten-\ntion weight. The value projection is the weighted average of outlook weights and can be cal-\nculated as follows:\nY\nDi;j\n¼ Mat MulðSof tmaxð\n^\nA\ni;j\nÞ; V\nDi;j\nÞ ð3Þ\nUnlike self-attention, which is dependent on query key matrix multiplications, the outlook\nattention matrix can be generated by attention weights within the local window located at\n(i, j) followed by reshape operation. Each layer of Outlooker can be written as;\n~\nX ¼ Out Att ðLN ðX ÞÞ þ X ð4Þ\nZ ¼ MLPðLN ð\n~\nX ÞÞ þ\n~\nX ð5Þ\n2. Transformer: The transformer encoder consists of multi-head outlook attention layers,\nlayer normalization and MLP. The architecture of the transformer is similar to ViT, but\nunlike the ViT, which uses self-attention mechanism, it uses outlook attention mechanism.\nThe multi head outlook attention is obtained by combining the computed outlook weight\nA\nn\nand value embedding V\nn\n. For N number of heads, the outlook weight and value embed-\ndings are given as A\nn\n2 R\nH �W �K\n4\nand V\nn\n2 R\nH �W �C\nN\nrespectively. Here n = 1, 2, . . ...,N rep-\nresents the dimensions of each head. The MLP in the transformer encoder in the proposed\nmodel has two layers with GeLU. Layer normalization is added before each block which\nhelps to enhance the training performance.\n3. SkinViT MLP head: The transformer encoder output is fed into the newly designed Skin-\nViT MLP head to classify Melanoma and Nonmelanoma skin cancer. The MLP head com-\nprises of flatten layers to flatten the output, a dense layer with Swish activation function and\nSVM linear kernel L2 as a classifier. Swish is a nonlinear and continuous function. It has a\nnon-zero gradient for negative inputs, which allows better optimization during training.\nThe Swish function can be written as:\nf ðX Þ ¼ X � sigm oidðX Þ ð6Þ\nWhere X is the input and sigmoid(X) is a sigmoid function that outputs the value between\n(0, 1).\nSVM with L2 Kernel: We employed linear kernel L2 [45] to implement SVM in our pro-\nposed method because it helps to handle the multicollinearity issue (correlated independent\nvariables) by reducing the coefficient and maintaining all the variables. The linear kernel\nperforms the best in the case of a large number of features. In contrast to L1 which uses the\nmedian of the data to estimate, linear kernel L2 makes a prediction based on the mean of\ndata to prevent overfitting. L2 kernel includes the penalty to the cost function as the squared\nvalue of weights and learns complex patterns. L2 is computationally efficient and improves\nprediction accuracy when the output is the function of all input variables. L2 kernel can be\ncalculated by:\nL2reg ¼ l\nX\nn\ni¼0\nw\n2\ni\nð7Þ\nWhere W\ni\nis the weight and λ represents the regularization parameter.\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 8 / 19\n4. Optimizer: The proposed SkinViT model used Adam as an optimizer. The Adam optimizer\nworks by computing the exponential moving average of the gradients of the parameters\nwith respect to the loss function. It is the combination of gradient descent and momentum.\nThe equation for the Adam optimizer is as follows:\nW\nt\n¼ W\nt \u0000 1\n\u0000 Z\n^\nM\nt\nffiffiffiffiffi ffi\n^\nV\nt\nq\nþ �\nð8Þ\nHere W is the model weights, η is the step size,\n^\nM\nt\nis the unbiased estimate of the moving\naverage of the gradient,\n^\nV\nt\nis the unbiased estimate of the moving average of the squared\ngradient and � is the constant used for numerical stability having a value of 10\n−8\n.\n5. Loss function: The binary cross entropy (L\nBCE\n) loss or log loss (L\nL\n) is often used for binary\nclassification tasks. The L\nBCE\nhelps evaluate the model accuracy by determining prediction\nprobability. The L\nBCE\ncomputes the difference between actual probability and prediction\nprobability and can be calculated as:\nL\nBCE\n¼ x � log ð ^x Þ þ ð1 \u0000 xÞ � log ð1 \u0000 ^x Þ ð9Þ\nWhere x is the label i.e. 1 for Melanoma and 0 for Nonmelanoma, and ^x is the predicted\nprobability of x.\nPseudocode:\nStep 1: Data Preparation\ndataset = preprocess_images(labeled_ images) // [N × H × W × C] array of preprocessed\nimages\ntraining_set, test_set = split_dataset(dataset) // [N_train, H, W, C], [N_test, H, W, C] arrays\nStep 2: Model Architecture\nmodel = create_SkinViT(num_transfor mer_blocks, num_outlooker_blocks, embedding_-\nsize, num_attention_heads) // SkinViT model\nStep 3: Training\ninitialize_weights(model ) // initialize model weights randomly\nfor epoch in 1 to num_epochs:\nfor batch in training_set:\naccuracy = calculate_accuracy(mo del, batch) // calculate accuracy between model predic-\ntions and ground truth labels\nupdate_weights(model, accuracy) // update model weights using Adam\ntest_accuracy = evaluate(model, test_set) // calculate test accuracy\nif test_accuracy does not improve for num_epochs_to_stop:\nbreak // Early stopping\nend\nend\nend\nStep 4: Hyperparameter Tuning\nhyperparameters = learning_rate: [1e-2, 1e-5], batch_size: [16, 32, 64], num_transformer_-\nblocks: [6, 12] // candidate hyperparameters\nbest_hyperparam eters = grid_search(model, hyperparameters, training_set, test_set) // find\nbest hyperparameters using grid search\nStep 5: Evaluation\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 9 / 19\ntest_accuracy = evaluate(model, test_set) // calculate test accuracy\nmetrics = calculate_metrics(mod el, test_set) // calculate precision, recall and F1 score\n3.5 Performance metrics\nTo evaluate the performance of the proposed SkinViT, we considered performance metrics\nwhich are as follows:\nAccu racy ¼\nT\npos\nþ T\nneg\nT\npo s\nþ T\nneg\nþ F\npos\nþ F\nneg\nð10Þ\nReca ll ¼\nT\npo s\nðT\npos\nþ F\nneg\nÞ\nð11Þ\nPrec isio n ¼\nT\npos\nðT\npos\nþ F\npos\nÞ\nð12Þ\nF 1scor e ¼ 2 ∗\nðPrec isi on∗Re callÞ\nðPrec isio n þ Reca llÞ\n� �\nð13Þ\nThe T\nneg\nrepresents the true negative, which means the accurate classification of Nonmela-\nnoma images and T\npos\nshows the true positive, meaning the accurately classified Melanoma\nimages by the proposed model. False positive F\npos\nis the wrongly classified Nonmelanoma\nimage as Melanoma while false negative F\nneg\nis the opposite of F\npos\n, meaning misclassification\nof the Melanoma image as Nonmelanoma. Recall measures how often it correctly predicts a\nresult for all samples that should have been classified positive, whereas Precision measures how\noften a method predicts a positive result. The F1 − score is the harmonic mean of precision and\nrecall, which shows how the classifier predicts correctly.\n4 Simulation setup and results\nThis section details the simulation setup and results of the proposed SkinViT.\n4.1 Simulation setup\nThe proposed SkinViT model is implemented in the Anaconda environment using Python 3.8\nwith Tensorflow, Keras, Scikit-Learn, Matplotlib and Numpy libraries installed on Windows\nOS with system configuration Intel Core i7-11800H @2.3GHz, 16GB DDR4, NVIDIA RTX\n3060. The SkinViT model is trained on 3 datasets, as described in the dataset acquisition sec-\ntion with 8473 dermoscopic images in dataset1, 1082 in dataset2 and 9555 in dataset3, as\ndepicted in Table 2. Moreover, we augmented the dataset as mentioned in data augmentation\nsection to avoid overfitting at the same time increase the classifier’s efficiency on unseen\nimages. Furthermore, Adam is employed as an optimizer to update SkinViT parameters in the\nproposed work during the model training. The epochs and batch size are set to 70 and 16,\nrespectively. The learning rate for the proposed work is set to 1e − 5.\n4.2 Simulation results\nThis section first discusses various ablation studies related to the proposed SkinViT model.\nNext, a comprehensive analysis of the performance of the proposed SkinViT is carried out and\ncompared with other SOTA models.\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 10 / 19\n4.3 Ablations\nThe ablations of this work comprise of 1) determining the best value of the L2 kernel; 2) train-\ning the model with different optimizers to determine the optimum one; 3) the effect of aug-\nmentations and without augmentations on the proposed method; 4) comparative analysis with\ndifferent kernels classifiers.\n1. Tuning the L2 kernel classifier: In this simulation, we experimented with different values of\nL2 to select the best value of L2 (Eq 6) for the proposed model. We changed the value of L2\nfrom 0.01 to 1.0 with five intervals in total to select the best result. Table 4 depicts the per-\nformance of the proposed SkinViT on different values of L2. From Table 5 it can be\nobserved the best result was obtained on 0.1 with 0.9109 on dataset1, 0.8611 on dataset2\nand 0.8911 on dataset3.\n2. Selection of Optimizer: In this simulation, the proposed model is trained by employing dif-\nferent optimizers to evaluate the classification performance as given in Table 5. The Skin-\nViT performed best with the Adam optimizer achieving the classification accuracy of\n0.9109 on Dataset1, 0.8611 on Dataset2 and 0.8911 on Dataset3, which is superior to\nRMSprop, achieved 0.8996 on Dataset1, 0.8518 on Dataset2 and 0.8733 on Dataset3.\n3. Effect of Augmentations on Proposed SkinViT: In this simulation, we evaluated the effect\nof classification accuracy using the augmentations on training datasets. It can be observed\nfrom Table 6 that the proposed SkinViT performed better while using augmentations, this\nis due to the fact that there was more representation of the training samples for generaliza-\ntion. From the results, it can be seen that augmentations helped the proposed SkinViT to\nexceed accuracy by 2.72% on Dataset1, 3.24% on Dataset2 and 1.52% on Dataset3.\n4. Selection of classifier: In this simulation, we used various classifiers to select the optimal\nclassifier to classify Melanoma and Nonmelanoma and to achieve the best results on the test\nset. Table 7 details the results of different classifier, it can be observed that the proposed\nSkinViT achieves the best performance using L2 kernel for the classification task which\nexceed using the Gaussian kernel by 1.72% on Dataset1, 7.4% on Dataset2 and 0.37% on\nDataset3. Here it can be observed SkinViT on Dataset2 achieved better accuracy while\nusing the L1 kernel than the Gaussian kernel, which is due to the fact that Dataset2 has a\nTable 4. Performance comparis on of SkinViT using different values of L2.\nDataset 0.01 0.05 0.1 0.5 1.0\nDataset1 0.8961 0.8991 0.9109 0.8943 0.8872\nDataset2 0.8148 0.8148 0.8611 0.8564 0.8379\nDataset3 0.8838 0.8905 0.8911 0.8942 0.8832\nhttps://do i.org/10.1371/j ournal.pone .0295151.t004\nTable 5. Performance comparis on of SkinViT using different optimizers.\nOptimizer Dataset Melanoma Nonme lanoma Overall\nAdam Datase t1 0.9082 0.9139 0.9109\nDatase t2 0.7683 0.9179 0.8611\nDatase t3 0.9087 0.8723 0.8911\nRMSprop Datase t1 0.8894 0.9114 0.8996\nDatase t2 0.8902 0.8283 0.8518\nDatase t3 0.9402 0.8019 0.8733\nhttps://do i.org/10.1371/j ournal.pone .0295151.t005\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 11 / 19\nsmall number of training samples, so the more complex Gaussian kernel performed poor\nwhile L1, which is not complex kernel achieved better accuracy. Overall from Table 8, it can\nbe seen that using the L2 kernel has increased the performance of the proposed SkinViT.\n4.4 SkinViT performance analysis\nThis section describes the performance analysis of the proposed SkinViT on considered data-\nsets. The feasibility of using pretrained model on our custom dataset is dependent upon the\nnature and characteristics of the dataset on which the model was trained. The lack of unique\nmedical related features means that transfer learning cannot achieve higher level of classifica-\ntion accuracy. Therefore, the proposed SkinViT model is trained from scratch on considered\ndatasets. From Table 8, it can be noticed that SkinViT performed best on Datasset1 with an\noverall accuracy of 0.9109, whereas it achieved 0.9082 accuracy for Melanoma class and 0.9139\naccuracy for the Nonmelanoma class. Dataset1 displayed overall recall of 0.9082, precision of\n0.9235 and F1-score of 0.9158. Dataset2 achieved an accuracy of 0.7682 in the Melanoma class\nand 0.9179 in the Nonmelanoma class, which is the highest accuracy of Nonmelanoma. The\noverall accuracy of SkinViT on Dataset2 came out to be 0.8611 with an overall recall of 0.7683.\nMoreover, SkinViT achieved on Dataset2 overall precision of 0.8514 and F1-score of 0.8077.\nDataset3 achieved an accuracy of 0.9087 in the Melanoma class, which is slightly higher than\nTable 6. Performan ce comparis on of SkinViT using different optimizer s.\nDataset With Augmentat ion Without Augmentation\nDataset1 0.9109 0.8837\nDataset2 0.8611 0.8287\nDataset3 0.8911 0.8759\nhttps://d oi.org/10.1371/j ournal.pon e.0295151.t00 6\nTable 7. Compariso n of SkinViT performan ce on different kernels classifiers.\nDataset SVM (L2 Kernel) SVM (Gaussian Kernel) L1 Kernel\nDataset1 0.9109 0.8937 0.8931\nDataset2 0.8611 0.7870 0.8425\nDataset3 0.8911 0.8874 8780\nhttps://d oi.org/10.1371/j ournal.pon e.0295151.t00 7\nTable 8. Performance Comparis on of SkinViT on different datasets.\nDataset Classes Accuracy Recall precision F1-score\nDataset1 Melanoma 0.9082 0.9082 0.9235 0.9158\nNonmelanoma 0.9139 0.9139 0.8969 0.9053\nOverall 0.9109 0.9082 0.9235 0.9158\nDataset2 Melanoma 0.7682 0.7683 0.8514 0.8077\nNonmelanoma 0.9179 0.9179 0.8662 0.8913\nOverall 0.8611 0.7683 0.8514 0.8077\nDataset3 Melanoma 0.9087 0.9087 0.8836 0.8960\nNonmelanoma 0.8723 0.8723 0.8996 0.8857\nOverall 0.8911 0.9087 0.8836 0.8960\nhttps://do i.org/10.1371/j ournal.pone .0295151.t008\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 12 / 19\nDataset1 and 0.8723 in the Nonmelanoma class with an overall accuracy of 0.8911. Moreover,\nSkinViT on Dataset3 displayed an overall precision of 0.8836, recall of 0.9087 and F1-score of\n0.8960. Dataset1 displayed the highest accuracy, precision and F1-score, while Dataset3 slightly\nhigher recall compared to Dataset1, which is due to higher false negatives because of the mini-\nmum similar representation in the training samples of those false classified. Subsequently,\nDataset3 displayed the second-best accuracy with the second-best precision and F1-score.\nSometimes accuracy alone does not give the complete picture of how well the model is per-\nforming. To answer this, the confusion matrix which is graphical representation of the model\nperformance on individual classes, is plotted. From Fig 3, it can be noted that SkinViT on\nDataset1 has 821 T\npos\n, 722 T\nneg\n, 83 F\nneg\nand 68 F\npos\n. The proposed SkinViT on Dataset2 has 63\nT\npos\n, 123 T\nneg\n, 19 F\nneg\nand 11 F\npos\n, whereas on Dataset3, SkinViT has 896 T\npos\n, 806 T\nneg\n, 90 F\nneg\nand 118 F\npos\n. It can be seen that the proposed SkinViT has the highest accuracy of 0.9109 on\nDataset1, followed by 0.8911 on Dataset3 and 0.8611 on Dataset2.\nWe have also plotted the Receiver Operating Characteristic (ROC) curve, which tells the\nmodel’s ability of predicting over a range of thresholds. The ROC curve is plotted by TPR\n(True Positive Rate) on the y-axis while FPR (False Positive Rate) is on the x-axis. The area\nunder the curve (AUC) represents how well the model distinguishes the classes. The AUC of\n0.9711 on Dataset1, as illustrated in Fig 4(a), shows that SkinViT has 97.11% chance to accu-\nrately classify Melanoma and Nonmelanoma classes. The AUC of 0.9459 on Dataset2, as illus-\ntrated in Fig 4(b), means SkinViT has 94.59% chance to accurately classify Melanoma and\nNonmelanoma classes. The AUC of 0.9595, as illustrated in Fig 4(c), shows that SkinViT has\n95.95% chance to accurately classify Melanoma and Nonmelanoma classes. We also plotted\nthe Precision-Recall (PR) curve, unlike ROC, which considers TN, to examine how well the\nproposed SkinViT performed while predicting Melanoma images. It can be noted from Fig 5\nthat the curves are near the top right corner which shows that SkinViT performed well in iden-\ntifying Melanoma images. SkinViT on Dataset1 achieved 0.9657 AP (Fig 5(a)) while on Data-\nset2 achieved 0.9649 AP (Fig 5(b)) and on Dataset3 achieved 0.9509 (Fig 5(c)).\n4.5 Comparative analysis with SOTA models\nIn this section, we present the performance comparison of the proposed SkinViT with other\nSOTA models to validate the efficiency of the SkinViT in terms of accuracy metric. We evalu-\nated the performance of different models such as EfficientNetv2 [46], MaxViT [47], Mobile-\nViTV2 [48] and ViT [44] on all the considered datasets. It can be seen from Table 9 that the\nproposed SkinViT performed best on Dataset1 with an accuracy of 0.9109, followed by the sec-\nond best on ViT 0.8636 which is a gain of 4.73%, recall of 0.9082 which is a gain of 0.77% from\nFig 3. Confusion Matrix of SkinViT on a) Dataset1, b) Dataset2 and c) Dataset3.\nhttps://do i.org/10.1371/j ournal.pon e.0295151.g0 03\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 13 / 19\nViT with 0.9004, precision of 0.9235 which is a gain of 2.81% from the second best 0.8954 dis-\nplayed by EfficientNetV2 and the F1-score of 0.9158 which is a gain of 4.01% from the second\nbest 0.8757 by ViT. SkinViT on Dataset2 displayed an accuracy of 0.8611, followed by the sec-\nond best on ViT 0.8241, which is a gain of 3.7%, recall of 0.7683 which is a gain of 10.98%\nfrom the second best 0.6585 displayed by EfficientNetV2, the precision of 0.8514 which is loss\nof 2.79% from the best 0.8793 achieved by ViT and F1-score of 0.8077 which is gain of 8.17%\nfrom the second best 0.7286 achieved by ViT. Similarly, SkinViT on Dataset3 showed the best\naccuracy of 0.8911, which is a gain of 4.55% from the second best accuracy of 0.8456 displayed\nby MaxViT, recall of 0.9087 which is a gain of 9.73% from the second best 0.8114 by Efiicient-\nNetV2, the precision of 0.8836 which is a loss of 0.59% from the best 0.8895 by MaxViT and\nF1-score of 0.8960 which is a gain of 5.35% from the second best of 0.8425 displayed by\nMaxViT.\n4.6 SkinViT performance on HAM10000\nFurther, to validate the performance of our proposed SkinViT, we evaluated the performance\nof the HAM10000 dataset which is used by most of the previously published work. It can be\nseen from Table 10 that the proposed SkinViT obtained an accuracy of 0.9254, followed by\nMobileNet by Damian et al. with 0.897 accuracy and STDP based SNN by Zhou et al. with\n0.877 accuracy. The higher accuracy of SkinViT is due to the lesser false classifications, which\nFig 4. ROC curve of SkinViT on a) Dataset1, b) Dataset2 and c) Dataset3.\nhttps://doi.o rg/10.1371/j ournal.pone .0295151.g004\nFig 5. PR curve of SkinViT on a) Dataset1, b) Dataset2 and c) Dataset3.\nhttps://doi.o rg/10.1371/j ournal.pone .0295151.g005\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 14 / 19\nis because of the attention mechanism that helps model learn the desired features more\nefficiently.\n4.7 Discussion\nIn this work, SkinViT displayed the ability of outlooker and self-attention to diagnose Mela-\nnoma and Nonmelanoma through dermoscopic images. It can be seen from the results that\nSkinViT performed better compared to the other CNN and Transformer based models. In\ncontrast to transformers, which can compute the attention of any patch, regardless of its dis-\ntance, a CNN alone needs to perform additional convolutions to increase the receptive field in\norder to determine the relationship between any two neighboring pixels, resulting in difficult\nto possess the ability to perform long-range computation. In SkinViT, outlooker block is used\ninstead of patch embedding component in ViT to learn features whereas self-attention is used\nto learn important features and ignoring the noisy ones. Results show that the SkinViT per-\nformed better compared to CNN and Transformer based models, which validates its superior-\nity over other models.\nFrom the results, it can be noted that the SkinViT performed better on both Melanoma and\nNonmelanoma classes. However, CNN model EfficientNetV2 was better in predicting Nonme-\nlanoma images while performed poor in classification of Melanoma as given in Table 9. More-\nover, Transformer based method MaxViT and ViT performed better in classifying Melanoma\nimages whereas another hybrid model MobileViTV2 performed well on classifying Nonmela-\nnoma images compared to the Melanoma images. Whereas, SkinViT was equally good in clas-\nsifying both the classes indicating that SKinViT is robust than using CNN or transformer\nbased models alone in dealing with imbalanced datasets. We also observed from the results\nTable 9. Comparativ e analaysis of SkinViT and other models .\nDataset Method Accurac y Recall Precision F1-score\nDataset1 Efficie ntNetV2 [46] 0.8412 0.7954 0.8954 0.8424\nMaxViT [47] 0.8418 0.8252 0.8715 0.8477\nMobile ViTV2 [48] 0.8141 0.8695 0.7996 0.8331\nViT [44] 0.8636 0.9004 0.8524 0.8757\nSkinViT (ours) 0.9109 0.9082 0.9235 0.9158\nDataset2 Efficie ntNetV2 [46] 0.8102 0.6585 0.8060 0.7248\nMaxViT [47] 0.7638 0.5365 0.7719 0.6328\nMobile ViTV2 [48] 0.8009 0.6707 0.7746 0.7139\nViT [44] 0.8241 0.6219 0.8793 0.7286\nSkinViT (ours) 0.8611 0.7683 0.8514 0.8077\nDataset3 Efficie ntNetV2 [46] 0.8424 0.8114 0.8743 0.8417\nMaxViT [47] 0.8456 0.8002 0.8895 0.8425\nMobile ViTV2 [48] 0.8031 0.7809 0.8280 0.8038\nViT [44] 0.8099 0.7769 0.8427 0.8084\nSkinViT (ours) 0.8911 0.9087 0.8836 0.8960\nhttps://do i.org/10.1371/j ournal.pone .0295151.t009\nTable 10. Comparat ive analysis of SkinViT with previous work.\nPerformanc e Metrics SkinViT (ours) MobileNet (Damian et al.) [32] STDP based SNN (Zhou et al.) [30]\nAccuracy 0.9254 0.897 0.877\nhttps://do i.org/10.1371/j ournal.pone .0295151.t010\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 15 / 19\nthat the EfficientNetV2 performed well compared to other Transformer based methods such\nas MaxViT and ViT.\nMost of the researcher used CNN models for Melanoma and Nonmelanoma detection\nwhile some used transformer alone architecture for the considered task. To the best of our\nknowledge this is the first time to use outlooker and transformers for skin cancer detection\ntask with Melanoma and Nonmelanoma (SCC and BCC) classes which are the most frequently\ndiagnosed skin cancer types. Additionally, previous research results were compared with the\nproposed work as depicted in Table 10. It is essential to point out that each researcher used dif-\nferent classes for their respective problems. Although MobileNet and STDP based SNN\nshowed good accuracy of 0.897 and 0.877 compared to others which took into account Mela-\nnoma and Nonmelanoma (Nevus) and our proposed method outperformed both with an\naccuracy of 0.9254 on the same dataset which validates the efficiency of our proposed method\nSkinViT. However, our work focused on binary classification problem of Melanoma and Non-\nmelanoma (BCC and SCC). Moreover, the high accuracy by the proposed method can help\nearly diagnosis of skin cancer and ease burden on dermotologist. This research can benefit\nresearchers to further improve the methodology for the image segmentation to detect abnor-\nmalities in dermoscopic images in terms of Melanoma and Nonmelanoma.\nDespite the great performance of the proposed SkinViT model compared to SOTA, there\nare some limitations and challenges in this research. Firstly, the data used for training the pro-\nposed SkinViT model from scratch is of moderate size. The size of data significantly impacts\nthe efficacy of training a deep learning model for optimal performance. The greater the\namount of data, the higher the efficiency of the model. Therefore, for future research, a large\ndataset should be curated by combining the publicly available datasets (ISIC archive,\nHAM10000 etc.) to improve the model efficiency. Furthermore, the dataset used has huge\nclass imbalance which can highly impact the performance of the proposed model. The current\nresearch employed the geometric data augmentation technique to handle the class imbalance\nissue. The future research will explore the use of generative adversarial network (GAN) or\nadvanced data augmentation techniques like MixUp and CutMix which involves combining\nthe multiple images or patches to create new training samples for representation and enhance\nthe model generalization.\n5 Conclusion\nThe focus of this research is on the automated detection of skin cancer types, Melanoma and\nNonmelanoma (BCC and SCC), which can help reduce the mortality rate by early diagnosis\nand also help ease the burden on dermatologists. To achieve this goal, we devised a novel deep\nlearning model named SkinViT, which employs transformer blocks, outlooker blocks and\nMLP Head for classification. Further, the proposed SkinViT eliminates the requirement for\nhigh computational power due to its fewer params (around 27.1million), as opposed to other\npopular classification models such as ViT and MaxViT.\nThe total number of training samples were enhanced by employing augmentations such as\nhorizontal flip, shear transformation and rotation to resolve class imbalance problem. More-\nover, the use of an SVM classifier, specifically the L2 kernel, has increased the optimality of the\nprediction value by taking the mean of the data to avoid overfitting. We performed multiple\nsimulations to assess SkinViT model performance. It is evident from Tables 9 and 10 that the\nSkinViT achieved higher classification accuracy in comparison to other methods. This is per-\nhaps due to the outlooker block in SkinViT, unlike ViT, efficiently encodes fine level features\nby measuring the likeness between token pair representations which is efficient in terms of\nparameters learning features than convolutions. Moreover, the sliding window adopted in\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 16 / 19\noutlook attention locally encodes token representations and preserves important positional\ninformation for classification task. Furthermore, the outlook attention weight generation is\nsimple reshaping operation, unlike self-attention which is dependent on query key matrix mul-\ntiplications. Finally, the proposed MLP head has the SVM L2 kernel classifier that further opti-\nmizes the model, which takes the mean of the values for the prediction score. This provides the\nSkinViT with better feature learning ability which results in higher accuracy in classifying Mel-\nanoma and Nonmelanoma in this proposed work.\nFor the skin cancer detection problem, it is crucial that the false classifications should be\nminimal to ensure the model’s applicability and reliability in the real-world scenarios. For Mel-\nanoma detection, it is imperative to minimize false negative as it may lead to treatment delays\nand subsequently diminish the 5-year survival rate. While the false positive would only necessi-\ntate further diagnostic procedures such as biopsy. The reason for the false classification of Mel-\nanoma images as Nonmelanoma (F\nneg\n) could be due to insufficient representation of the\nMelanoma images. As it can be observed from Table 2 that the number of images is Melanoma\nis significantly lower than that of Nonmelanoma. This can make it difficult for the model to\ngeneralize the instances for which it is not trained. Another reason for the false classification\ncan be the excessive noise such as hair and air bubble in the images which make it challenging\nfor the model to learn the important features. The proposed SkinViT method can further be\nimproved in the future using additional datasets available publicly with segmentation task to\ndetect skin diseases. Furthermore, the image quality of the dermoscopic images for Melanoma\nand Nonmelanoma can further be improved. Also, the classification results of SkinViT can be\nutilised for implementing a Melanoma and Nonmelanoma recognition system to assist the\ndermatologists in diagnosis.\nAuthor Contributions\nConceptualization: Somaiya Khan.\nData curation: Somaiya Khan.\nFormal analysis: Somaiya Khan, Ali Khan.\nMethodology: Somaiya Khan, Ali Khan.\nSoftware: Somaiya Khan, Ali Khan.\nValidation: Somaiya Khan.\nWriting – original draft: Somaiya Khan, Ali Khan.\nWriting – review & editing: Somaiya Khan, Ali Khan.\nReferences\n1. Worldwid e cancer data. [cited 1 January 2023]. In: Cancer trends [Interne t]. Available from: https://\nwww.wcr f.org/cancer -trends/wor ldwide-can cer-data/\n2. Cancer. 2022 February 3 [cited 1 January 2023]. In: WHO Newsro om Fact sheet [Internet]. Available\nfrom: https://www .who.int/new s-room/fact- sheets/detai l/cancer\n3. Key Statistics for Melanoma Skin Cancer. [cited 1 January 2023]. In: American Cancer Society [Inter-\nnet]. Availab le from: https://ww w.cancer.org /cancer/ty pes/melano ma-skin-can cer/abou t/key-statis tics.\nhtml\n4. Facts and risk factors. [cited 1 January 2023]. In: Melanoma New Zealand [Interne t]. Availab le from:\nhttps://m elanoma.org.n z/all-about-mel anoma/fac ts-and-risk -factors/\n5. Survival. [cited 1 January 2023]. In: Cancer Resea rch UK [Interne t]. Available from: https://www .\ncancerre searchuk.or g/about-canc er/melanom a/survival\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 17 / 19\n6. Yu KH, Beam AL, Kohane IS. Artifici al intelligence in healthcare . Nature biomedi cal engineerin g. 2018;\n2(10):719– 731. https:// doi.org/10.10 38/s4155 1-018-030 5-z PMID: 310156 51\n7. Wang Z, Luo Y, Xin J, Zhang H, Qu L, Wang Z, et al. Comp uter-aided diagnosis based on extreme\nlearning machine: a review. IEEE Access. 2020; 8:141657–141 673. https:/ /doi.org/10.11 09/ACC ESS.\n2020.30120 93\n8. Qayyum A, Qadir J, Bilal M, Al-Fuqaha A. Secure and robust machine learning for healthcar e: A survey.\nIEEERe views in Biomedic al Engineering. 2020; 14:156–180.\n9. Agnes SA, Anitha J, Solomon AA. Two-st age lung nodule detection frameworkusin g enhance d UNet\nand convolutio nal LSTM networks in CT images. Computers in Biology and Medicine. 2022;\n149:106059 . https://doi.or g/10.1016/ j.compbiomed .2022.10 6059\n10. Chattopadh yay S, Dey A, Singh PK, Oliva D, Cuevas E, Sarkar R. MTRRE- Net:A deep learning model\nfor detection of breast cancer from histopatho logical images. Computers in Biology and Medicine. 2022;\n150:106155 . https://doi.or g/10.1016/ j.compbiomed .2022.10 6155 PMID: 3624059 5\n11. Abdolali F, Kapur J, Jaremko JL, Noga M, Hareend ranathan AR, Punithak umar K. AUtomated thyroid\nnodule detection from ultrasou nd imaging using deep convolutio nal neural networks. Comput ers in Biol-\nogy and Medicine. 2020; 122:103871 https://d oi.org/10.101 6/j.compbio med.2020 .103871 PMID:\n32658741\n12. Kluk J, Ogiela MR. AI Approaches in Comp uter-AIded Diagnosis and Recognition of Neoplast ic\nChanges in MRI Brain Images. Applied Sciences. 2022; 12(23):118 0. https://doi.or g/10.339 0/\napp12231188 0\n13. Asiri N, Hussain M, Al Adel F, Alzaidi N. Deep learning based computer-aide d diagnosis systems for\ndiabetic retinopathy: A survey. Artifici al intellige nce in medicine. 2019; 99:1017 01. https://doi.or g/10.\n1016/j.artm ed.2019.07 .009 PMID: 31606116\n14. Xu SSD, Chang CC, Su CT, Phu PQ. Classifi cation of liver diseases based on ultrasou nd image texture\nfeatures. Applied Sciences. 2019; 9(2):342. https://d oi.org/10.339 0/app902 0342\n15. Kadhim YA, Khan MU, Mishra A. Deep Learning- Based Comput er-Aided Diagnosis (CAD): Applica tions\nfor Medical Image Datase ts. Sensor s. 2022; 22(22):899 9. https://doi.or g/10.3390/ s22228999 PMID:\n36433595\n16. Khan A, Khan S, Hassan B, Zheng Z. CNN-based smoker classification and detection in smart city\napplicatio n. Sensors. 2022; 22(3):892. https://doi.or g/10.339 0/s22030892 PMID: 35161637\n17. Khan A, Hassan B, Khan S, Ahmed R, Abuassba A. DeepFire: A Novel Datase t and Deep Transfer\nLearning Benchmar k for Forest Fire Detection. Mobile Information System s. 2022; 2022. https://doi.or g/\n10.1155/ 2022/5358359\n18. Khan S, Teng Y, Cui J. Pedestr ian traffic lights classifica tion using transfer learning in smart city applica-\ntion. In: 2021 13th International conferen ce on commu nication software and networks (ICCSN). IEEE;\n2021. p.352-35 6.\n19. Khan S, Khan A. FFireNet: Deep Learning Based Forest Fire Classifica tion and Detection in Smart Cit-\nies. Symme try. 2022; 14(10):21 55. https://doi.or g/10.3390 /sym14102155\n20. Alissa M, Lones MA, Cosgrove J, Alty JE, Jamieson S, Smith SL, et al. Parkinson’s disease diagno sis\nusing convolutio nal neural networks and figure-copying tasks. Neural Comput ing and Applications .\n2022; p.1–21.\n21. Wang X, Girshick R, Gupta A, He K. Non-local neural networks. In: Proceedings of the IEEE conferenc e\non computer vision and pattern recognition; 2018. p. 7794-7803.\n22. Condonni er JB, Loukas A, Jaggi M. On the relationship between self-atten tion and convolutio nal layers.\narXiv preprint arXiv:191103 584. 2019.\n23. ISIC Challenge . [cited 1 January 2023]. In: ISIC Challeng e [Interne t]. Available from: https://challen ge.\nisic-archi ve.com/\n24. Das K, Cockerell CJ, Patil A, Pietkiewicz P, Giulini M, Grabbe S, et al. Machine learning and its applica-\ntion in skin cancer. International Journal of Environ mental Research and Public Health. 2021; 18\n(24):13409 . https://doi. org/10.3390/i jerph18241 3409 PMID: 34949015\n25. Abunadi I, Senan EM. Deep learning and machine learning technique s of diagnosis dermosc opy\nimages for early detection of skin diseases. Electronic s. 2021; 10(24):315 8. https://doi.or g/10.3390/\nelectronic s10243158\n26. Pham TC, Tran GS, Nghien TP, Doucet A, Luong Cm, Hoang VD. A comparati ve study for classif ication\nof skin cancer. In: 2019 Internation al Conferen ce on System Science and Engineeri ng (ICSSE). IEEE;\n2019. p. 267-272 .\n27. Shen S, Xu M, Zhang F, Shao P, Liu H, Xu L, et al. A low-cost high-perfo rmance data augmentat ion for\ndeep learning-b ased skin lesion classif ication. Frontiers . 2022; 2022. https:// doi.org/10.34 133/2022 /\n9765307 PMID: 37850173\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 18 / 19\n28. Zhang J, Xie Y, Xia Y, Shen C. Attentio n residual learning for skin lesion classificat ion. IEEE transac-\ntions on medical imaging . 2019; 38(9):2092 –2103. https://doi.or g/10.1109 /TMI.2019.28 93944 PMID:\n30668469\n29. Liu L, Mou L, Zhu XX, Mandal M. Automatic skin lesion classif ication based on mid-level feature learn-\ning. Compute rized Medical Imaging and Graphics . 2020; 84:1017 65. https://doi.or g/10.101 6/j.\ncompmed imag.2020.1 01765 PMID: 32810817\n30. Zhou Q, Shi Y, Xu Z, Qu R, Xu G. Classifyin g melanoma skin lesions using convolutio nal spiking neural\nnetworks with unsuperv ised STDP learning rule. IEEE Access. 2020; 8:10130 9–101319. https://d oi.\norg/10.1109/ ACCESS.2 020.2998098\n31. Gouda W, Sama NU, Al-Waak id G, Humayun M, Jhanjhi NZ. Detection of skin cancer based on skin\nlesion images using deep learning. In: Healthcare . vol. 10. MDPI; 2022. p. 1183.\n32. Almaraz-D amian JA, Ponoma ryov V, Sadovn ychiy S, Castillejos-F ernandez H. Melaoma and nevus\nskin lesion classifica tion using handcraf t and deep learning feature fusion via mutual information mea-\nsures. Entropy. 2020; 22(4):484. https://doi.or g/10.3390/ e22040484 PMID: 332862 57\n33. Indraswar i R, Rokhana R, Herulamba ng W. Melanoma image classification based on MobileNetV 2 net-\nwork. Procedia computer science. 2022; 197:198–207. https:// doi.org/10.10 16/j.proc s.2021.12 .132\n34. Hoang L, Lee SH, Lee EJ, Kwon KR. Multiclas s skin lesion classificat ion using a novel lightweigh t deep\nlearning framework for smart healthcare . Applied Sciences. 2022; 12(5):2677 . https://do i.org/10.3390 /\napp12052677\n35. Lopez AR, Giro-i Nieto X, Burdick J, Marque s O. Skin lesion classif ication from dermoscopi c images\nusing deep learning technique s. In: 2017 13th IASTED internationa l conferen ce on biomedi cal engi-\nneering (BioMed). IEEE; 2017. p. 49-54.\n36. Xie J, Wu Z, Zhu R, Zhu H. Melanoma detection based on swin transfor mer and SimAm. In: 2021 IEEE\n5th Informatio n Technolo gy, Networ king, Electronic and Automatio n Control Conferen ce (ITNEC ). vol.\n5. IEEE; 2021. p. 1517-1521.\n37. Naeem A, Anees T, Fiza M, Naqvi RA, Lee SW. SCDNet: A Deep Learning- Based Framework for the\nMulticlass ification of Skin Cancer Using Dermos copy Images. vol. 22. MDPI; 2022. p. 5652.\n38. Tahir M, Naeem A, Malik H, Tanveer J, Naqvi RA, Lee SW. DSCCN et: Multi-Class ification Deep learn-\ning Models for Diagnosing of Skin Cnacer Using Dermosc opic Images. vol. 15. MDPI; 2023. p. 2179.\n39. Maharan a P. ISIC 2019 Skin Lesion images for classification. [cited 3 August 2022]. In: Kaggle Datasets\n[Internet ]. Available from: https://www .kaggle.com/d atasets/salv iohexia/isic- 2019-skin-lesio n-image s-\nfor-classifi cation\n40. Ullah F. Skin Lesion Dermis Dataset. [cited 3 August 2022]. In: Kaggle Dataset s [Internet] . Available\nfrom: https://www .kaggle.com/da tasets /farhatullah8 398/skin-l esion-dermis -dataset\n41. Kliuiev P. PH2-da tabase-AD DI. [cited 3 August 2022]. In: Kaggle Datase ts [Interne t]. Available from:\nhttps://www .kaggle.com /datasets/k liuiev/ph2datab aseaddi\n42. Rademak er T. DermNet NZ_cleaned. [cited 3 August 2022]. In: Kaggle Datasets [Internet]. Available\nfrom: https://www .kaggle.com/da tasets /thomasrad emaker/de rmnetnz-1 00-cleaned\n43. Yuan L, Hou Q, Jiang Z, Feng J, Yan S. Volo: Vision outlooker for visual recognition. IEEE Transaction s\non Pattern Analysis and Machine Intelligenc e. 2022. https://d oi.org/10.110 9/TPAMI.20 22.3206 108\n44. Dosovitskiy A, Beyer L, Kolesniko v A, Weissenbor n D, Zhai X, Unterthine r T, et al. An image is worth\n16x16 words: Transfor mers for image recognit ion at scale. arXiv preprint arXiv:201011 929. 2020.\n45. Kim J, Scott C. L Kernel Classifica tion. IEEE Transaction s on Pattern Analysis and Machine Intelli-\ngence. 2009; 32(10):182 2–1831.\n46. Tan M, Le Q. Efficientn etv2: Smaller models and faster training. In: Interna tional conferen ce on machine\nlearning. PMLR; 2021. p. 10096–10106 .\n47. Tu Z, Talebi H, Zhang H, Yang F, Milanfar P, Bovik A, et al. Maxvit: Multi-axis vision transfor mer. In:\nComputer Vision–EC CV 2022: 17th European Conferen ce, Tel Aviv, Israel, October 23–27, 2022, Pro-\nceeding s, Part XXIV. Springer; 2022. p. 459–479.\n48. Mehta S, Rastegari M. Separable self-atten tion for mobile vision transfor mers. arXiv preprint\narXiv:220602 680. 2022;.\nPLOS ONE\nSkinViT: A transforme r based method for Melanoma and Nonmelano ma classifica tion\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.02951 51 December 27, 2023 19 / 19",
  "topic": "Skin cancer",
  "concepts": [
    {
      "name": "Skin cancer",
      "score": 0.8087124824523926
    },
    {
      "name": "Computer science",
      "score": 0.6566107273101807
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6070115566253662
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6014605164527893
    },
    {
      "name": "Melanoma",
      "score": 0.5416011214256287
    },
    {
      "name": "Basal cell carcinoma",
      "score": 0.5187616348266602
    },
    {
      "name": "Deep learning",
      "score": 0.47596949338912964
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.47438839077949524
    },
    {
      "name": "Basal cell",
      "score": 0.4427052140235901
    },
    {
      "name": "Precision and recall",
      "score": 0.4224660098552704
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3903712034225464
    },
    {
      "name": "Machine learning",
      "score": 0.3643955588340759
    },
    {
      "name": "Medicine",
      "score": 0.3417903780937195
    },
    {
      "name": "Cancer",
      "score": 0.2599239647388458
    },
    {
      "name": "Pathology",
      "score": 0.1385679841041565
    },
    {
      "name": "Internal medicine",
      "score": 0.08573713898658752
    },
    {
      "name": "Mathematics",
      "score": 0.08212536573410034
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Cancer research",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139759216",
      "name": "Beijing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I9842412",
      "name": "Nanjing University of Aeronautics and Astronautics",
      "country": "CN"
    }
  ],
  "cited_by": 22
}