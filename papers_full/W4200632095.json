{
    "title": "CTIN: Robust Contextual Transformer Network for Inertial Navigation",
    "url": "https://openalex.org/W4200632095",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3133101212",
            "name": "Bingbing Rao",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2187910074",
            "name": "Ehsan Kazemi",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2231534204",
            "name": "Yifan Ding",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A4373656176",
            "name": "Devu M. Shila",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2703243921",
            "name": "Frank M. Tucker",
            "affiliations": [
                "United States Department of the Army",
                "United States Army"
            ]
        },
        {
            "id": "https://openalex.org/A2097892043",
            "name": "Liqiang Wang",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A3133101212",
            "name": "Bingbing Rao",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2187910074",
            "name": "Ehsan Kazemi",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A2231534204",
            "name": "Yifan Ding",
            "affiliations": [
                "University of Central Florida"
            ]
        },
        {
            "id": "https://openalex.org/A4373656176",
            "name": "Devu M. Shila",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2703243921",
            "name": "Frank M. Tucker",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097892043",
            "name": "Liqiang Wang",
            "affiliations": [
                "University of Central Florida"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2216550548",
        "https://openalex.org/W2008891225",
        "https://openalex.org/W6747834056",
        "https://openalex.org/W3135524347",
        "https://openalex.org/W2056427752",
        "https://openalex.org/W2395463083",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6666761814",
        "https://openalex.org/W6762215367",
        "https://openalex.org/W2618011341",
        "https://openalex.org/W2091790851",
        "https://openalex.org/W2774754582",
        "https://openalex.org/W6784863454",
        "https://openalex.org/W6769290819",
        "https://openalex.org/W1974531087",
        "https://openalex.org/W6755564432",
        "https://openalex.org/W6629857507",
        "https://openalex.org/W6679436768",
        "https://openalex.org/W1525308596",
        "https://openalex.org/W6714931315",
        "https://openalex.org/W3015093387",
        "https://openalex.org/W6746034047",
        "https://openalex.org/W6746970611",
        "https://openalex.org/W3152857436",
        "https://openalex.org/W3207208339",
        "https://openalex.org/W3158065011",
        "https://openalex.org/W6697304529",
        "https://openalex.org/W6777239595",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3034619943",
        "https://openalex.org/W2963423603",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W3173631815",
        "https://openalex.org/W3167705651",
        "https://openalex.org/W2565104410",
        "https://openalex.org/W2963677766",
        "https://openalex.org/W2411412724",
        "https://openalex.org/W4254751698",
        "https://openalex.org/W4246614213",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4297785061",
        "https://openalex.org/W4298395628",
        "https://openalex.org/W2296228853",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W2962994355",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3101037136",
        "https://openalex.org/W2897886597",
        "https://openalex.org/W2963907629",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W4300434718",
        "https://openalex.org/W1564768010"
    ],
    "abstract": "Recently, data-driven inertial navigation approaches have demonstrated their capability of using well-trained neural networks to obtain accurate position estimates from inertial measurement units (IMUs) measurements. In this paper, we propose a novel robust Contextual Transformer-based network for Inertial Navigation (CTIN) to accurately predict velocity and trajectory. To this end, we first design a ResNet-based encoder enhanced by local and global multi-head self-attention to capture spatial contextual information from IMU measurements. Then we fuse these spatial representations with temporal knowledge by leveraging multi-head attention in the Transformer decoder. Finally, multi-task learning with uncertainty reduction is leveraged to improve learning efficiency and prediction accuracy of velocity and trajectory. Through extensive experiments over a wide range of inertial datasets (e.g., RIDI, OxIOD, RoNIN, IDOL, and our own), CTIN is very robust and outperforms state-of-the-art models.",
    "full_text": "CTIN: Robust Contextual Transformer Network for Inertial Navigation\nBingbing Rao1, Ehsan Kazemi1, 2, Yifan Ding1, Devu M. Shila2, Frank M. Tucker3, Liqiang Wang1\n1 Department of Computer Science, University of Central Florida, Orlando, FL, USA\n2 Unknot.id Inc., Orlando, FL, USA\n3 U.S. Army CCDC SC, Orlando, FL, USA\nAbstract\nRecently, data-driven inertial navigation approaches have\ndemonstrated their capability of using well-trained neural\nnetworks to obtain accurate position estimates from inertial\nmeasurement units (IMUs) measurements. In this paper, we\npropose a novel robust Contextual Transformer-based net-\nwork for Inertial Navigation (CTIN) to accurately predict ve-\nlocity and trajectory. To this end, we ﬁrst design a ResNet-\nbased encoder enhanced by local and global multi-head self-\nattention to capture spatial contextual information from IMU\nmeasurements. Then we fuse these spatial representations\nwith temporal knowledge by leveraging multi-head attention\nin the Transformer decoder. Finally, multi-task learning with\nuncertainty reduction is leveraged to improve learning ef-\nﬁciency and prediction accuracy of velocity and trajectory.\nThrough extensive experiments over a wide range of inertial\ndatasets (e.g., RIDI, OxIOD, RoNIN, IDOL, and our own),\nCTIN is very robust and outperforms state-of-the-art models.\nIntroduction\nInertial navigation is a never-ending endeavor to estimate\nthe states (i.e., position and orientation) of a moving sub-\nject (e.g., pedestrian) by using only IMUs attached to it.\nAn IMU sensor, often a combination of accelerometers and\ngyroscopes, plays a signiﬁcant role in a wide range of ap-\nplications from mobile devices to autonomous systems be-\ncause of its superior energy efﬁciency, mobility, and ﬂexi-\nbility (Lymberopoulos et al. 2015). Nevertheless, the con-\nventional Newtonian-based inertial navigation methods re-\nveal not only poor performance, but also require unrealistic\nconstraints that are incompatible with everyday usage sce-\nnarios. For example, strap-down inertial navigation systems\n(SINS) may obtain erroneous sensor positions by perform-\ning double integration of IMU measurements, duo to expo-\nnential error propagation through integration (Titterton, We-\nston, and Weston 2004). Step-based pedestrian dead reck-\noning (PDR) approaches can reduce this accumulated error\nby leveraging the prior knowledge of human walking mo-\ntion to predict trajectories (Tian et al. 2015). However, an\nIMU must be attached to a foot in the zero-velocity update\n(Foxlin 2005) or a subject must walk forward so that the\nmotion direction is constant in the body frame (Brajdic and\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nHarle 2013). In addition, inertial sensors are often combined\nwith additional sensors and models using Extended Kalman\nFilter (Bloesch et al. 2015) to provide more accurate es-\ntimations, where the typical sensors include WiFi (Ahme-\ntovic et al. 2016), Bluetooth (Li, Guo, and Li 2017), Li-\nDAR (Zhang and Singh 2014), or camera sensors (Leuteneg-\nger et al. 2015). Nonetheless, these combinations with ad-\nditional sensors are posing new challenges about instru-\nment installations, energy efﬁciency, and data privacy. For\ninstance, Visual-Inertial Odometry (VIO) substantially de-\npends on environmental factors such as lighting conditions,\nsignal quality, blurring effects (Usenko et al. 2016).\nRecently, a growing number of data-driven approaches\nsuch as IONet (Chen et al. 2018), RoNIN (Herath, Yan,\nand Furukawa 2020), and IDOL (Sun, Melamed, and Ki-\ntani 2021) have demonstrated their capability of using well-\ntrained neural networks to obtain accurate estimates from\nIMU measurements with competitive performance over the\naforementioned methods. However, grand challenges still\nexist when applying neural network techniques to IMU mea-\nsurements: 1) most existing data-driven approaches lever-\nage sequence-based models (e.g., LSTM (Hochreiter and\nSchmidhuber 1997)) to learn temporal correlations but fail\nto capture spatial relationships between multivariate time-\nseries. 2) There is few research work to explore rich contex-\ntual information among IMU measurements in dimensions\nof spatial and temporal for inertial feature representation. 3)\nUsually, uncertainties of IMU measurements and model out-\nput are assumed to be a ﬁxed covariance matrix in these pure\nand black-box neural inertial models, which brings signiﬁ-\ncant inaccuracy and much less robustness because they can\nﬂuctuate dramatically and unexpectedly in nature.\nIn response to the observations and concerns raised above,\na novel robust contextual Transformer network is proposed\nto regress velocity and predict trajectory from IMU mea-\nsurements. Particularly, CTIN extends the ideas of ResNet-\n18 (He et al. 2016) and Transformer (Vaswani et al. 2017) to\nexploit spatial and longer temporal information among IMU\nobservations and then uses the attention technique to fuse\nthis information for inertial navigation. The major contribu-\ntions of this paper are summarized as follows:\n• Extending ResNet-18 with attention mechanisms is to\nexplore and encode spatial information of IMU samples.\n• A novel self-attention mechanism is proposed to extract\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n5413\ncontextual features of IMU measurements.\n• Multi-Task learning using novel loss functions is to im-\nprove learning efﬁciency and reduce models’ uncertainty.\n• Comprehensive qualitative and quantitative comparisons\nwith the existing baselines indicate that CTIN outper-\nforms state-of-the-art models.\n• A new IMU dataset with ground-truth trajectories under\nnatural human motions is provided for future reference.\n• To the best of our knowledge, CTIN is the ﬁrst\nTransformer-based model for inertial navigation.\nBackground\nIMU models\nTechnically, 3D angular velocity (!) and 3D acceleration (\u000b)\nprovided by IMUs are subjected to bias and noise based on\nsome sensor properties, as shown in Equation 1 & 2:\n!t = r!\nt + b!\nt + n!\nt (1)\n\u000bt = r\u000b\nt + b\u000b\nt + n\u000b\nt (2)\nwhere r!\nt and r\u000b\nt are real sensor values measured by the gy-\nroscope and accelerometer at timestamp t, respectively; b!\nt\nand b\u000b\nt are time-varying bias; n!\nt and n\u000b\nt are noise values,\nwhich usually follow a zero-mean gaussian distribution.\nInertial Tracking\nAccording to Newtonian mechanics (Kok, Hol, and Sch ¨on\n2017), states (i.e., position and orientation) of a moving\nsubject (e.g., pedestrian) can be estimated from a history\nof IMU measurements, as shown in Equation 3:\nRn\nb(t) = Rn\nb(t−1) ⊗\n(t) (3a)\n\n(t) = exp(dt\n2 !(t−1)) (3b)\nvn(t) = vn(t−1) + \u0001(t) (3c)\n\u0001(t) = (Rn\nb(t−1) ⊙\u000b(t−1) −gn)dt (3d)\nPn(t) = Pn(t−1) + vn(t−1)dt (3e)\nHere, the orientation Rn\nb(t) at timestamp tis updated with\na relative orientation (\n(t)) between two discrete instants t\nand t−1 according to Equation 3a & 3b, where !(t−1)\nmeasures proper angular velocity of an object at timestamp\n(t −1) in the body frame (denoted by b) with respect to\nthe navigation frame (denoted by n). Rn\nb can be used to ro-\ntate a measurement x ∈[!;\u000b] from the body frame b to\nthe navigation frame n, which is denoted by an expression\nRn\nb ⊙x = Rn\nb ⊗x⊗(Rn\nb)T where ⊗is a hamilton prod-\nuct between two quaternions. The navigation frame in our\ncase is deﬁned such that Z axis is aligned with earth’s grav-\nity gn and the other two axes are determined according to\nthe initial orientation of the body frame. In Equation 3c &\n3d, velocity vector vn(t) is updated with its temporal dif-\nference \u0001(t), which is obtained by rotating \u000b(t−1) to the\nnavigation frame using Rw\nb (t−1) and discarding the con-\ntribution of gravity forces gn. Finally, positions Pn(t) are\nobtained by integrating velocity in Equation 3e. Therefore,\ngiven current IMU measurements (i.e., \u000b, !), the new sys-\ntem states (i.e., Pn, vn and Rn\nb) can be obtained from the\nprevious states using a function of f in Equation 4, where f\nrepresents transformations in Equation 3.\n[Pn;vn;Rn\nb]t = f([Pn;vn;Rn\nb]t−1;[\u000b;!]t) (4)\nDrawback and Solution:However, using IMUs for local-\nization results in signiﬁcant drift due to that the bias and\nnoise intrinsic to the gyroscope and accelerometer sensing\ncan explode quickly in the double integration process. Using\npure data-driven models with IMU measurements for Iner-\ntial Navigation has shown promising results in pedestrian\ndead-reckoning systems. To tackle the problems of error\npropagation in Equation 4, we break the cycle of continuous\nintegration and segment inertial measurements into indepen-\ndent windows, then leverage a sequence-to-sequence neural\nnetwork architecture (Sutskever, Vinyals, and Le 2014; Bah-\ndanau, Cho, and Bengio 2015; Wu et al. 2016; Vaswani et al.\n2017) to predict velocities and positions from an input win-\ndow mof IMU measurements, as shown in Equation 5.\n[Pn;vn]1:m = F\u0012(Pn\n0 ;vn\n0 ;[Rn\nb;\u000b;! ]1:m) (5)\nwhere F\u0012 represents a latent neural system that learns the\ntransformation from IMU samples to predict positions and\nvelocities, where Pn\n0 , vn\n0 are initial states.\nAttention Mechanism\nAttention can be considered as a query procedure that maps\na query Q for a set of key-value pairs (K;V ) to an output\n(Vaswani et al. 2017; Han et al. 2020), which is denoted\nby ATT(Q;K;V ) = \r(Q;K) ×V. Typically, the output\nis computed as a sum of weighted values (V ), where the\nweights \r(Q;K) are computed according to a compatibility\nfunction of Qand K. There are two kinds of \r used in this\npaper (Bahdanau, Cho, and Bengio 2015; Wang et al. 2018):\n(1) we perform adot productbetween Qand K, divides each\nresulting element by\n√\nd, and applies a softmax function to\nobtain the weights: \r(Q;K) = softmax( QKT\n√\nd ) where d\nis the dimension size of vectors Q, K and V. (2) Inspired\nby Relation Networks (Santoro et al. 2017), we investigate\na form of concatenation: \r(Q;K) = ReLU(W\r[Q;K]),\nwhere [·;·]denotes concatenation and W\r is a weight vec-\ntor that projects the concatenated vector to a scalar. Self-\nattention networks compute a representation of an input se-\nquence by applying attention to each pair of tokens from\nthe sequence, regardless of their distance (Vaswani et al.\n2017). Technically, given IMU samplesX ∈Rm×d, we can\nperform the following transformation on X directly to ob-\ntain Q, K and V: Q;K;V = XWQ;XWK;XWV, where\n{WQ;WK;WV}∈Rd×d are trainable parameters. Usually,\nthese intermediate vectors are split into different represen-\ntation subspaces at different positions (i.e., h = 8;d k =\nd\nh), e.g., K = [ K1;:::;K h] with Ki ∈ Rm×dk . For a\nsubspace, the attention output is calculated by headi =\nATT(Qi;Ki;V i). The ﬁnal output representation is the\nconcatenation of outputs generated by multiple attention\nheads: MultiHead(Q;K;V ) = [headi;:::;head h].\nOur Approach\nSystem Overall\nThe Attention-based architecture for inertial navigation is\nshown in Figure 1 and its workﬂow is depicted as follows:\nData Preparation.Initially, an IMU sample is the con-\ncatenation of data from gyroscope and accelerometer. To ex-\nploit temporal characteristics of IMU samples, we leverage\n5414\n1x1 Conv\nAdd & ReLU\n3x3 Local \nSelf‐attention\nMulti‐Head\nAttention\nAdd & Norm\nFeed Forward\nAdd & Norm\nTemporal Decoder\nSpatial \nEmbedding\nTemporal\nEmbedding\nPositional \nEncoding\nMLP\nIMU IMU Buffer\n~\nR n1 :[,] nm\n\n\n1:[,] m\n\n1x1 Conv\n1x1 Global \nSelf‐attention\n1x1 Conv\nMasked \nSelf attention\nAdd & Norm\nSpatial Encoder\nMLP\nRotation Matrix Selector\nKey: 1x1\nQuery: 1x1\nValue: 1x1\nX\n+\nSoftmax\n*\nY\nKey: 3x3\nX\n*\nConcat\nValue: 1x1\nCov: 1x1\nAttention\nY\n(a). Global Self‐attention   (b). Local Self‐attention  \nQuery*\nReLU\nz\nh\n Multi‐Task \nLoss\nVel head\nCov head\n1:cov m\n1:mvel\nFigure 1: Overall workﬂow of the proposed contextual transformer model for inertial navigation.\na sliding window with size mto prepare datasets at times-\ntamp t, denoted by X1:m\nt = [ xt−m+1 ;:::;x t]. Similarly,\nwe adopt this rolling mechanism with the same window size\nto build the ground truth of velocities: gt1:m\nvel . Usually, IMU\nsamples in each window are rotated from the body frame\n(i.e., !b;\u000bb) to the navigation frame (i.e.,!n;\u000bn) using pro-\nvided orientations. Rotation Matrix Selector is designed to\nselect sources of orientation for training and testing auto-\nmatically. Typically, we use the device orientation estimated\nfrom IMU for testing.\nEmbedding. We need to compute feature representations\nfor IMU samples before feeding them into encoder and de-\ncoder. Spatial Embedding uses a 1D convolutional neural\nnetwork followed by batch normalization and linear layers\nto learn spatial representations;Temporal Embeddingadopts\na 1-layer bidirectional LSTM model to exploit temporal in-\nformation, and then adds positional encoding provided by a\ntrainable neural network.\nSpatial Encoder. The encoder comprises a stack of N\nidentical layers, which maps an input sequence of X1:m\nt to\na sequence of continuous representations z = (z1;:::;z m).\nTo capture spatial knowledge of IMU samples at each times-\ntamp, we strengthen the functionality of the core bottleneck\nblock in ResNet-18 (He et al. 2016) by replacing spatial\nconvolution with a local self-attention layer and inserting a\nglobal self-attention module before the last 1 ×1 downsam-\npling convolution (cf. in Section ). All other structures, in-\ncluding the number of layers and spatial downsampling, are\npreserved. The modiﬁed bottleneck layer is repeated multi-\nple times to form Spatial Encoder, with the output of one\nblock being the input of the next one.\nTemporal Decoder.The decoder also comprises a stack\nof N identical layers. Within each layer, we ﬁrst perform\na masked self-attention sub-layer to extract dependencies in\nthe temporal dimension. The masking emphasizes a fact that\nthe output at timestamp t can depend only on IMU sam-\nples at timestamp less than t. Next, we conduct a multi-head\nattention sub-layer over the output of the encoder stack to\nfuse spatial and temporal information into a single vector\nrepresentation and then pass through a position-wise fully\nconnected feed-forward sub-layer. We also employ residual\nconnections around each of the sub-layers, followed by layer\nnormalization.\nVelocity and Covariance. Finally, two MLP-based\nbranch heads regress 2D velocity (vel 1:m\nt ) and the corre-\nsponding covariance matrix (cov1:m\nt ) using the input of h,\nrespectively. Position can be obtained by the integration of\nvelocity. The model of the covariance, denoted by \u0006 : x→\nR2×2 where x is a system state, can describe the distribu-\ntion difference between ground-truth velocity and the corre-\nsponding predictions of them during training. Given that, the\nprobability of a velocity yv considering current system state\nxcan be approximated by a multivariate Gaussian distribu-\ntion (Russell and Reale 2021):\npc(yv|x) = 1\np\n(2\u0019)2|\u0006(x)|\n×\nexp(−1\n2(yv −F\u0012(x))T\u0006(x)−1(yv −F\u0012(x)))\n(6)\nIt is worthwhile to mention that we also leverage multi-task\nlearning with uncertainty reduction to accomplish the de-\nsired performance (See details in Section ).\nAttention In Inertial Navigation\nIn this paper, the encoder and decoder rely entirely on atten-\ntion mechanism with different settings for embedding ma-\ntrix {WQ;WK;WV}and \r to explore spatial and temporal\nknowledge from IMU samples.\nGlobal self-attention in Encoder.It triggers the feature\ninteractions across different spatial locations, as shown in\nFigure 1(a). Technically, we ﬁrst transform X into Q, K,\nand V using three separated 1D 1 ×1 convolutions, re-\nspectively. After that, we obtain the global attention ma-\ntrix (i.e., \r(Q;K)) between K and Q using a Dot Prod-\nuct version of \r. Finally, the ﬁnal output Y is computed by\n\r(Q;K)×V. In addition, we also adopt multi-head attention\nto jointly summarize information from different sub-space\nrepresentations at different spatial positions.\n5415\nDataset Year IMU\nCarrier\nSample\nFrequency\nNo of\nSubjects\nNo of\nSequences\nGround\nTruth\nMotion\nContext\nRIDI 2017 Leno v\no Phab2 Pro 200 Hz 10 98 Google Tango phone Four attachments: leg pocket,\nbag, hand, body\nOxIOD 2018 iPhone 5/6, 7\nPlus,\nNexus 5 100 Hz 5 158 Vicon Four attachments: handheld, pocket,\nhandbag, trolley\nRoNIN 2019 Galaxy S9,\nPixel 2 XL 200 Hz 100 276 Asus Zenfone AR Attaching devices naturally\nIDOL 2020 iPhone 8\n100 Hz 15 84 Kaarta Stencil Attaching devices naturally\nCTIN 2021 Samsung Note,\nGalaxy 200 Hz 5 100 Google ARCore Attaching devices naturally\nTable 1: Description of public datasets used for evaluation of navigation models.\nLocal self-attention in Encoder.Although performing a\nglobal self-attention over the whole feature map can achieve\ncompetitive performance, it not only scales poorly but also\nmisses contextual information among neighbor keys. Be-\ncause it treats queries and keys as a group of isolated pairs\nand learns their pairwise relations independently without ex-\nploring the rich contexts between them. To alleviate this is-\nsue, a body of research work (Hu et al. 2019; Ramachandran\net al. 2019; Zhao, Jia, and Koltun 2020; Li et al. 2021; Yao\net al. 2022) employs self-attention within the local region\n(i.e., 3 ×3 grid) to boost self-attention learning efﬁciently,\nand strengthen the representative capacity of the output ag-\ngregated feature map. In this paper, we follow up this track\nand design a novel local self-attention for inertial navigation,\nas shown in Figure 1(b). In particular, we ﬁrst employ 3 ×3\ngroup convolution over all the neighbor keys within a grid\nof 3 ×3 to extract local contextual representations for each\nkey, denoted by C1 = XWK;3×3 . After that, the attention\nmatrix (i.e., \r(Q;C1)) is achieved through a concatenation\nversion of \r in which W\r is a 1 ×1 convolution and Qis\ndeﬁned as X. Next, we calculate the attended feature map\nC2 by \r(Q;C1) ×V, which captures the global contextual\ninteractions among all IMU samples. The ﬁnal output Y is\nfused by an attention mechanism between local context C1\nand global context C2.\nMulti-head attention in Decoder. We inherit settings\nfrom vanilla Transformer Decoder for attention mechanisms\n(Vaswani et al. 2017). In other words, we take three sepa-\nrated linear layers to generate Q, K and V from X, respec-\ntively, and leverage a pairwise function of Dot product to\ncalculate attention matrix (i.e., \r(Q;K)). Finally, the ﬁnal\noutput Y is computed by \r(Q;K) ×V.\nJointly Learning Velocity and Covariance\nWe leverage multi-task learning with uncertainty reduction\nto improve learning efﬁciency and prediction accuracy of\nthe two regression tasks: prediction of 2D velocity and its\ncovariance. Inspired by (Kendall, Gal, and Cipolla 2018;\nLiu et al. 2020; Yao et al. 2021; Yang et al. 2021), we de-\nrive a multi-task loss function by maximizing the Gaussian\nlikelihood with uncertainty (Kendall and Gal 2017). First,\nwe deﬁne our likelihood as a Gaussian with mean given by\nthe model output as pu(y|F\u0012(x)) = N(F\u0012(x);\u000e2), where \u000e\nis an observation noise scalar. Next, we derive the model’s\nminimization objective as a Negative Log-Likelihood (NLL)\nof two model outputs yv (velocity) and yc (covariance):\nL(F\u0012;\u000ev;\u000ec)\n= −log(pu(yv;yc|F\u0012(x)))\n= −log(pu(yv|F\u0012(x)) ×pu(yc|F\u0012(x)))\n= −(log(pu(yv|F\u0012(x))) + log(pu(yc|F\u0012(x)))\n= −(log(N(yv; F\u0012(x);\u000e2\nv)) + log(N(yc; F\u0012(x);\u000e2\nc)))\n∝∥yv −F\u0012(x) ∥2\n2\u000e2v\n+ log\u000ev\n| {z }\nVelocity\n+ ∥yc −F\u0012(x) ∥2\n2\u000e2c\n+ log\u000ec\n| {z }\nCovariance\n= 1\n2\u000e2v\nLv + 1\n2\u000e2c\nLc + log\u000ev\u000ec\n(7)\nwhere \u000ev and \u000ec are observation noises for velocity and co-\nvariance, respectively. Their loss functions are denoted by\nLv and Lc, and depicted as follows:\nIntegral Velocity Loss (IVL,Lv). Instead of performing\nmean square error (MSE) between predicted velocity (^v) and\nthe ground-truth value (v), we ﬁrst integrate predicted posi-\ntions from ^v (cf. Equation 3e), and then deﬁne a L2 norm\nagainst the ground-truth positional difference within same\nsegment of IMU samples, denoted by Lp\nv. In addition, we\ncalculate cumulative error between ^vand v, denoted by Le\nv.\nFinally, Lv is deﬁned as Lp\nv + Le\nv.\nCovariance NLL Loss (CNL,Lc). According to the covari-\nance matrix in Equation 6, We deﬁne the Maximum Likeli-\nhood loss as the NLL of the velocity with consideration of\nits corresponding covariance \u0006:\nLc = −log(pc(yv|x))\n= 1\n2(yv −f(x))T\u0006(x)−1(yv −f(x)) + 1\n2 ln |\u0006(x)|\n= 1\n2 ∥yv −f(x) ∥2\n\u0006(x) +1\n2 ln |\u0006(x)|\n(8)\nThere is a rich body of research work to propose various\ncovariance parametrizations for neural network uncertainty\nestimation (Liu et al. 2020; Russell and Reale 2021). In this\nstudy, we simply deﬁne the variances along the diagonal,\nwhich are parametrized by two coefﬁcients of a velocity.\nExperiments\nWe evaluate CTIN on ﬁve datasets against four representa-\ntive prior research works. CTIN was implemented in Pytorch\n1.7.1 (Paszke et al. 2019) and trained using Adam optimizer\n(Kingma and Ba 2014). During training, early stopping with\n30 patience (Prechelt 1998; Wang et al. 2020) is leveraged\nto avoid overﬁtting according to model performance on the\n5416\nDataset Test\nSubject Metric\nPerformance (meter) Perf. Improvement\nSINS PDR RIDI RoNIN CTIN CTIN improvement\nover RoNIN\nR-LSTM R-ResNet R-TCN R-LSTM R-ResNet R-TCN\nRIDI\nSeen\nATE 6.34 22.76 8.18 2.55 2.33 3.25 1.39 45.36% 40.10% 57.13%\nT-RTE 8.13 24.89 9.34 2.34 2.36 2.64 1.99 15.00% 15.78% 24.80%\nD-RTE 0.52 1.39 0.97 0.16 0.16 0.17 0.11 32.47% 32.26% 35.91%\nUnseen\nATE 4.62 20.56 8.18 2.78 1.97 2.06 1.86 33.07% 5.40% 9.68%\nT-RTE 4.58 31.17 10.51 2.95 2.47 2.43 2.49 15.66% -0.70% -2.36%\nD-RTE 0.36 1.19 1.09 0.15 0.14 0.14 0.11 28.00% 21.22% 22.72%\nOxIOD\nSeen\nATE 15.36 9.78 3.78 3.87 2.40 3.33 2.32 40.10% 3.52% 30.27%\nT-RTE 11.02 8.51 3.99 1.56 1.83 1.49 0.62 60.40% 66.27% 58.67%\nD-RTE 0.96 1.16 2.30 0.20 0.56 0.19 0.07 61.94% 86.67% 61.21%\nUnseen\nATE 13.90 17.72 7.16 5.22 3.51 6.16 3.34 35.90% 4.61% 45.69%\nT-RTE 10.51 17.21 7.65 2.65 2.51 2.61 1.33 50.00% 47.18% 49.15%\nD-RTE 0.89 1.10 2.62 0.29 0.49 0.24 0.13 55.57% 73.45% 45.48%\nRoNIN\nSeen\nATE 7.89 26.64 16.82 5.11 3.99 6.18 4.62 9.49% -15.81% 25.23%\nT-RTE 5.30 23.82 19.50 3.05 2.83 3.27 2.81 7.70% 0.69% 13.91%\nD-RTE 0.42 0.98 4.99 0.22 0.19 0.20 0.18 18.94% 2.75% 10.15%\nUnseen\nATE 7.62 23.49 15.75 8.73 5.76 7.49 5.61 35.77% 2.60% 25.11%\nT-RTE 5.12 23.07 19.13 4.87 4.50 4.70 4.48 8.04% 0.42% 4.61%\nD-RTE 0.43 1.00 5.37 0.29 0.25 0.26 0.25 12.63% 0% 4.83%\nIDOL\nSeen\nATE 21.54 18.44 9.79 4.57 4.44 4.68 2.90 36.49% 34.63% 37.98%\nT-RTE 14.93 14.53 7.97 1.72 1.58 1.77 1.35 21.47% 14.54% 23.46%\nD-RTE 1.07 1.14 0.97 0.19 0.26 0.18 0.13 28.39% 48.21% 25.12%\nUnseen\nATE 20.34 16.83 9.54 5.60 3.81 5.89 3.69 34.19% 3.28% 37.40%\nT-RTE 18.48 15.67 9.07 1.99 1.67 2.21 1.65 16.73% 1.02% 25.30%\nD-RTE 1.36 1.31 1.04 0.20 0.22 0.20 0.15 25.36% 30.14% 25.52%\nCTIN Seen\nATE 5.63 12.05 4.88 2.22 2.39 2.02 1.28 42.25% 46.45% 36.68%\nT-RTE 5.34 16.39 4.21 2.10 2.01 1.73 1.29 38.54% 35.87% 25.55%\nD-RTE 0.50 0.79 0.18 0.11 0.16 0.11 0.08 28.91% 50.56% 24.61%\nTable 2: Overall Trajectory Prediction Accuracy. The best result is shown in bold font.\nvalidation dataset. To be consistent with the experimental\nsettings of baselines, we conduct both training and testing\non NVIDIA RTX 2080Ti GPU.\nDataset and Baseline\nAs shown in Table 1, all selected datasets with rich motion\ncontexts (e.g., handheld, pocket) are collected by multiple\nsubjects using two devices: one is to collect IMU measure-\nments and the other provides ground truth (i.e., position).\nAll datasets are split into training, validation, and testing\ndatasets in a ratio of 8:1:1. For testing datasets except in\nCTIN, there are two sub-sets: one for subjects that are also\nincluded in the training and validation sets, the other for un-\nseen subjects. The selected baseline models are listed below:\n• Strap-down Inertial Navigation System (SINS): The sub-\nject’s position can be obtained from double integration\nof linear accelerations (with earth’s gravity subtracted).\nTo this end, we need to rotate the accelerations from the\nbody frame to the navigation frame using device orien-\ntations and perform an integral operation on the rotated\naccelerations twice to get positions (Savage 1998).\n• Pedestrian Dead Reckoning (PDR): We leverage Adap-\ntiv1 to detect foot-steps and update positions per step\nalong the device heading direction. We assume a stride\nlength of 0.67m/step.\n1An Adaptive Jerk Pace Buffer Step Detection Algorithm\n• Robust IMU Double Integration (RIDI):We use the orig-\ninal implementation (Yan, Shan, and Furukawa 2018) to\ntrain a separate model for each device attachment in RIDI\nand OxIOD datasets. For the rest of the datasets, we train\na uniﬁed model for each dataset separately, since attach-\nments during data acquisition in these datasets are mixed.\n• Robust Neural Inertial Navigation (RoNIN): We use the\noriginal implementation (Herath, Yan, and Furukawa\n2020) to evaluate all three RoNIN variants (i.e., R-\nLSTM, R-ResNet, and R-TCN) on all datasets.\nEvaluation Metrics\nUsually, positions in trajectory can be calculated by per-\nforming integration of velocity predicted by CTIN. The ma-\njor metric used to evaluate the accuracy of positioning is a\nRoot Mean Squared Error (RMSE) with various deﬁnitions\nof estimation error: RMSE =\nq\n1\nm\nPm\nt=1 ∥Et(xt;~xt) ∥,\nwhere mmeans the number of data points; Et(xt;~xt) rep-\nresents an estimation error between a position (i.e.,xt) in the\nground truth trajectory at timestamp tand its corresponding\none (i.e., ~xt) in the predicted path. In this study, we deﬁne\nthe following metrics (Sturm et al. 2011):\n• Absolute Trajectory Error (ATE)is the RMSE of esti-\nmation error: Et = xt −~xt. The metric shows a global\nconsistency between the trajectories and the error is in-\ncreasing by the path length.\n5417\n(a)\n (b)\nFigure 2: Performance Comparison of CTIN and RoNIN variant models on CTIN dataset\n(a)\n (b)\nFigure 3: The effectiveness of proposed attention layers on CTIN dataset. “*-atts” means CTIN or R-ResNet models with\nattention functionalities; “*-Conv” represents the models using a conventional spatial convolution instead.\n• Time-Normalized Relative Traj. Error (T-RTE)is the\nRMSE of average errors over a time-interval window\nspan (i.e., ti= 60 seconds in our case). The estimation er-\nror is deﬁned formally as Et = (xt+ti −xt) −(~xt+ti −\n~xt). This metric measures the local consistency of esti-\nmated and ground truth path.\n• Distance Normalized Relative Traj. Error (D-RTE)\nis the RMSE across all corresponding windows when\na subject travels a certain distance d, like d is set to\n1 meter in our case. The estimation error is given by\nEt = (xt+td −xt) −(^xt+td −^xt) where td is the time\ninterval needed to traverse a distance of d.\n• Position Drift Error (PDE) measures ﬁnal position\n(at timestamp m) drift over the total distance traveled\n(i.e., traj:\nlen): (∥xm −^xm ∥)=traj: len\nOverall Performance\nTable 2 shows experimental trajectory errors across entire\ntest datasets. It demonstrates that CTIN can achieve the best\nresults on most datasets in terms of ATE, T-RTE, and D-RTE\nmetrics, except for two cases in RoNIN and RIDI datasets.\nR-TCN can get a smaller T-RTE number than CTIN in the\nRIDI-unseen test case; R-ResNet reports the smallest ATE\nof 3.99 for RoNIN-seen. In particular, CTIN improves an\naverage ATE on all seen test datasets by 34.74%, 21.78%,\nand 37.46% over R-LSTM, R-ResNet, and R-TCN, respec-\ntively; the corresponding numbers for all unseen test datasets\nare 34.73%, 3.97%, and 29.47%.\nThe main limitation of RoNIN variants (i.e.,R-LSTM, R-\nResNet, and R-TCN) is that they do not capture the spectral\ncorrelations across time-series which hampers the perfor-\nmance of the model. Therefore, it is convincing that CTIN\nachieves better performance over these baselines. Table 2\nalso shows that CTIN generalizes well to unseen test sets,\nand outperforms all other models on test sets. PDR shows a\npersistent ATE due to the consistent and precise updates ow-\ning to the jerk computations. This mechanism leads to PDR\nfailure on long trajectories. Over time, the trajectory tends\nto drift owing to the accumulated heading estimation and the\ndrift would increase dramatically, which results in decentral-\nized motion trajectory shapes. R-LSTM does not show sat-\nisfactory results over large-scale trajectories. The margin of\nthe outperforms of CTIN compared to R-LSTM and R-TCN\nis notable. The results for SINS show a large drift that high-\nlights the noisy sensor measurements from smartphones.\nAblation Study\nModel Behaviors. The experimental results about perfor-\nmance comparisons between CTIN and three RoNIN vari-\nants are shown in Figure 2 and Table 3. In Figure 2a, each\nplot shows the cumulative density function (CDF) of the\nchosen metric on the entire test datasets. The blue line of\nCTIN is steeper than other plots, which indicates that CTIN\nshows signiﬁcantly lower overall errors than all RoNIN vari-\nants for all presented metrics. As shown in Figure 2b, al-\nthough CTIN’s overall MSE is higher than R-Resnet and\nsmaller than R-LSTM and R-TCN, its position drift error\n(i.e., PDE (%)) is the smallest (i.e., the best). In Table 3,\nwe show the number of parameters for each model, GFLOPs\nperformed by GPU during testing, the average GPU execu-\ntion time for testing a sequence of IMU samples (excluding\nthe time to load data and generate trajectories after model\n5418\nFigure 4: The performance of CTIN network with different\nloss functions evaluated on CTIN dataset.\nModel #Params\n(106)\nGFLOP\n(109=s)\nAvg. GPU\ntime (ms)\nTrajectory Error (meter)\nATE T-RTE D-RTE\nCTIN 0.5571 7.27 65.96 1.28 1.29 0.08\nR-LSTM 0.2058 7.17 704.23 2.22 2.10 0.11\nR-TCN 2.0321 33.17 19.05 2.02 1.73 0.11\nR-ResNet 4.6349 9.16 75.89 2.39 2.01 0.16\nTable 3: Models’ Evaluation Performance on CTIN dataset\nprediction) and trajectory errors. Overall, CTIN possesses a\nsigniﬁcantly smaller number of parameters than R-TCN and\nR-ResNet, and more parameters than R-LSTM, achieving a\ncompetitive runtime performance with lower trajectory er-\nrors in a real deployment. Therefore, CTIN performs better\nthan all RoNIN variants.\nAttention Effectiveness. In this paper, we propose a\nnovel attention mechanism to exploit local and global de-\npendencies among the spatial feature space, and then lever-\nage the multi-head attention layer to combine spatial and\ntemporal information for better accuracy of velocity predic-\ntion. To evaluate their effectiveness, we conduct a group of\nexperiments using CTIN/R-ResNet and their variant with-\nout/with the capability of attention mechanism. The experi-\nmental results are shown in Figure 3. Figure 3a shows that\nCTIN-Atts and R-ResNet-Atts models outperform the mod-\nels without attention layer. Furthermore, CTIN-Atts perform\nthe best for all metrics, and the performance of CTIN-Conv\nis better than all R-ResNet variants. In Figure 3b,CTIN-Atts\nand R-ResNet-Atts have lower average MSE loss of velocity\nprediction and smallest PDE thanCTIN-Conv and R-ResNet-\nConv. Overall, CTIN and R-ResNet can beneﬁt from the pro-\nposed attention mechanism.\nLoss function. In this section, we evaluate the perfor-\nmance of multi-task loss (i.e., IVL+CNL) by performing\na group comparison experiments using different loss func-\ntions, such as mean square error (MSE), Integral Veloc-\nity Loss (IVL) and Covariance NLL Loss (CNL), to train\nthe models. As shown in Figure 4, CTIN with a loss of\nIVL+CNL achieves the best performance for ATE and D-\nRTE metrics.\nRelated Work\nConventional Newtonian-based solutionsto inertial nav-\nigation can beneﬁt from IMU sensors to approximate po-\nsitions and orientations (Kok, Hol, and Sch ¨on 2017). In\na strap-down inertial navigation system (SINS) (Savage\n1998), accelerometer measurements are rotated from the\nbody to the navigation frame using a rotation matrix pro-\nvided by an integration process of gyroscope measurements,\nthen subtracted the earth’s gravity. After that, positions\ncan be obtained from double-integrating the corrected ac-\ncelerometer readings (Shen, Gowda, and Roy Choudhury\n2018). However, the multiple integrations can lead to ex-\nponential error propagation. To compensate for this cumula-\ntive error, step-based pedestrian dead reckoning (PDR) ap-\nproaches rely on the prior knowledge of human walking mo-\ntion to predict trajectories by detecting steps, estimating step\nlength and heading, and updating locations per step (Tian\net al. 2015).\nData-Driven approach.Recently, a growing number of\nresearch works leverage deep learning techniques to extract\ninformation from IMU measurements and achieve com-\npetitive results in position estimation (Chen et al. 2018,?;\nHerath, Yan, and Furukawa 2020; Dugne-Hennequin,\nUchiyama, and Lima 2021). IoNeT (Chen et al. 2018) ﬁrst\nproposed an LSTM structure to regress relative displacement\nin 2D polar coordinates and concatenate to obtain the posi-\ntion. In RIDI (Yan, Shan, and Furukawa 2018) and RoNIN\n(Herath, Yan, and Furukawa 2020), IMU measurements are\nﬁrst rotated from the body frame to the navigation from us-\ning device orientation. While RIDI regressed a velocity vec-\ntor from the history of IMU measurements to optimize bias,\nthen performed double integration from the corrected IMU\nsamples to estimate positions. RoNIN regressed 2D velocity\nfrom a sequence of IMU sensor measurements directly, and\nthen integrate positions.\nIn addition to using networks solely for pose estimates,\nan end-to-end differentiable Kalman ﬁlter framework is pro-\nposed in Backprop KF (Haarnoja et al. 2016), in which the\nnoise parameters are trained to produce the best state esti-\nmate, and do not necessarily best capture the measurement\nerror model since loss function is on the accuracy of the ﬁl-\nter outputs. TLIO provides a neural model to regress the ve-\nlocity prediction and uncertainties jointly (Liu et al. 2020).\nIn IDOL (Sun, Melamed, and Kitani 2021) two separate\nnetworks in an end-to-end manner are exploited. The ﬁrst\nmodel is used to predict orientations to circumvent the inac-\ncuracy in the orientation estimations with smartphone APIs.\nNext, the IMU measurements in the world frame are used to\npredict the velocities using the second model.\nConclusion\nIn this paper, we propose CTIN, a novel robust contextual\nAttention-based model to regress accurate 2D velocity from\nsegments of IMU measurements. We ﬁrst design a ResNet-\nbased encoder to capture spatial contextual information from\nIMU measurements, further fuse these spatial representa-\ntions with temporal knowledge by leveraging attention in\nthe Transformer decoder. Finally, multi-task learning using\nuncertainty is leveraged to improve learning efﬁciency and\nprediction accuracy of 2D velocity. Through extensive ex-\nperiments over a wide range of inertial datasets, CTIN is\nvery robust and outperforms state-of-the-art models.\n5419\nAcknowledgments\nThis material is based upon work supported by the U.S.\nArmy Combat Capabilities Development Command Sol-\ndier Center (CCDC SC) Soldier Effectiveness Directorate\n(SED) SFC Paul Ray Smith Simulation & Training Tech-\nnology Center (STTC) under contract No. W912CG-21-P-\n0009. Any opinions, ﬁndings, and conclusions, or recom-\nmendations expressed in this material are those of the au-\nthor(s) and do not necessarily reﬂect the views of the CCDC-\nSC-SED-STTC.\nReferences\nAhmetovic, D.; Gleason, C.; Ruan, C.; Kitani, K.; Takagi,\nH.; and Asakawa, C. 2016. NavCog: a navigational cogni-\ntive assistant for the blind. In Proceedings of the 18th Inter-\nnational Conference on Human-Computer Interaction with\nMobile Devices and Services, 90–99.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural ma-\nchine translation by jointly learning to align and translate. In\n3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Confer-\nence Track Proceedings.\nBloesch, M.; Omari, S.; Hutter, M.; and Siegwart, R. 2015.\nRobust visual inertial odometry using a direct EKF-based\napproach. In 2015 IEEE/RSJ international conference on\nintelligent robots and systems (IROS), 298–304. IEEE.\nBrajdic, A.; and Harle, R. 2013. Walk detection and step\ncounting on unconstrained smartphones. In Proceedings of\nthe 2013 ACM international joint conference on Pervasive\nand ubiquitous computing, 225–234.\nChen, C.; Lu, X.; Markham, A.; and Trigoni, N. 2018. Ionet:\nLearning to cure the curse of drift in inertial odometry. In\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 32.\nDugne-Hennequin, Q. A.; Uchiyama, H.; and Lima, J. P. S.\nD. M. 2021. Understanding the Behavior of Data-Driven In-\nertial Odometry With Kinematics-Mimicking Deep Neural\nNetwork. IEEE Access, 9: 36589–36619.\nFoxlin, E. 2005. Pedestrian tracking with shoe-mounted in-\nertial sensors. IEEE Computer graphics and applications,\n25(6): 38–46.\nHaarnoja, T.; Ajay, A.; Levine, S.; and Abbeel, P. 2016.\nBackprop kf: Learning discriminative deterministic state es-\ntimators. In Advances in neural information processing sys-\ntems, 4376–4384.\nHan, K.; Wang, Y .; Chen, H.; Chen, X.; Guo, J.; Liu, Z.;\nTang, Y .; Xiao, A.; Xu, C.; Xu, Y .; et al. 2020. A Survey on\nVisual Transformer. arXiv preprint arXiv:2012.12556.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHerath, S.; Yan, H.; and Furukawa, Y . 2020. RoNIN: Robust\nNeural Inertial Navigation in the Wild: Benchmark, Evalua-\ntions, & New Methods. In 2020 IEEE International Con-\nference on Robotics and Automation (ICRA), 3146–3152.\nIEEE.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation, 9(8): 1735–1780.\nHu, H.; Zhang, Z.; Xie, Z.; and Lin, S. 2019. Local rela-\ntion networks for image recognition. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n3464–3473.\nKendall, A.; and Gal, Y . 2017. What uncertainties do we\nneed in bayesian deep learning for computer vision? Ad-\nvances in neural information processing systems, 30.\nKendall, A.; Gal, Y .; and Cipolla, R. 2018. Multi-task learn-\ning using uncertainty to weigh losses for scene geometry and\nsemantics. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 7482–7491.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKok, M.; Hol, J. D.; and Sch ¨on, T. B. 2017. Using iner-\ntial sensors for position and orientation estimation. arXiv\npreprint arXiv:1704.06053.\nLeutenegger, S.; Lynen, S.; Bosse, M.; Siegwart, R.; and\nFurgale, P. 2015. Keyframe-based visual–inertial odometry\nusing nonlinear optimization. The International Journal of\nRobotics Research, 34(3): 314–334.\nLi, J.; Guo, M.; and Li, S. 2017. An indoor localization sys-\ntem by fusing smartphone inertial sensors and bluetooth low\nenergy beacons. In 2017 2nd International Conference on\nFrontiers of Sensors Technologies (ICFST), 317–321. IEEE.\nLi, Y .; Yao, T.; Pan, Y .; and Mei, T. 2021. Contextual\ntransformer networks for visual recognition. arXiv preprint\narXiv:2107.12292.\nLiu, W.; Caruso, D.; Ilg, E.; Dong, J.; Mourikis, A. I.; Dani-\nilidis, K.; Kumar, V .; and Engel, J. 2020. TLIO: Tight\nLearned Inertial Odometry. IEEE Robotics and Automation\nLetters, 5(4): 5653–5660.\nLymberopoulos, D.; Liu, J.; Yang, X.; Choudhury, R. R.;\nHandziski, V .; and Sen, S. 2015. A realistic evaluation and\ncomparison of indoor location technologies: Experiences\nand lessons learned. In Proceedings of the 14th interna-\ntional conference on information processing in sensor net-\nworks, 178–189.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information pro-\ncessing systems, 32: 8026–8037.\nPrechelt, L. 1998. Early stopping-but when? In Neural Net-\nworks: Tricks of the trade, 55–69. Springer.\nRamachandran, P.; Parmar, N.; Vaswani, A.; Bello, I.; Lev-\nskaya, A.; and Shlens, J. 2019. Stand-alone self-attention in\nvision models. arXiv preprint arXiv:1906.05909.\nRussell, R. L.; and Reale, C. 2021. Multivariate uncertainty\nin deep learning. IEEE Transactions on Neural Networks\nand Learning Systems.\nSantoro, A.; Raposo, D.; Barrett, D. G.; Malinowski, M.;\nPascanu, R.; Battaglia, P.; and Lillicrap, T. 2017. A sim-\nple neural network module for relational reasoning. arXiv\npreprint arXiv:1706.01427.\n5420\nSavage, P. G. 1998. Strapdown inertial navigation integra-\ntion algorithm design part 2: Velocity and position algo-\nrithms. Journal of Guidance, Control, and dynamics, 21(2):\n208–221.\nShen, S.; Gowda, M.; and Roy Choudhury, R. 2018. Closing\nthe gaps in inertial motion tracking. In Proceedings of the\n24th Annual International Conference on Mobile Computing\nand Networking, 429–444.\nSturm, J.; Magnenat, S.; Engelhard, N.; Pomerleau, F.; Co-\nlas, F.; Cremers, D.; Siegwart, R.; and Burgard, W. 2011. To-\nwards a benchmark for RGB-D SLAM evaluation. In Rgb-\nd workshop on advanced reasoning with depth cameras at\nrobotics: Science and systems conf.(rss).\nSun, S.; Melamed, D.; and Kitani, K. 2021. IDOL: Iner-\ntial Deep Orientation-Estimation and Localization. arXiv\npreprint arXiv:2102.04024.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. In Advances in\nneural information processing systems, 3104–3112.\nTian, Q.; Salcic, Z.; Kevin, I.; Wang, K.; and Pan, Y . 2015.\nAn enhanced pedestrian dead reckoning approach for pedes-\ntrian tracking using smartphones. In 2015 IEEE Tenth In-\nternational Conference on Intelligent Sensors, Sensor Net-\nworks and Information Processing (ISSNIP), 1–6. IEEE.\nTitterton, D.; Weston, J. L.; and Weston, J. 2004.Strapdown\ninertial navigation technology, volume 17. IET.\nUsenko, V .; Engel, J.; St ¨uckler, J.; and Cremers, D. 2016.\nDirect visual-inertial odometry with stereo cameras. In2016\nIEEE International Conference on Robotics and Automation\n(ICRA), 1885–1892. IEEE.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998–6008.\nWang, D.; Li, Y .; Wang, L.; and Gong, B. 2020. Neural\nnetworks are more productive teachers than human raters:\nActive mixup for data-efﬁcient knowledge distillation from\na blackbox model. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 1498–\n1507.\nWang, X.; Girshick, R.; Gupta, A.; and He, K. 2018. Non-\nlocal neural networks. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 7794–\n7803.\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google’s neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144.\nYan, H.; Shan, Q.; and Furukawa, Y . 2018. RIDI: Robust\nIMU double integration. In Proceedings of the European\nConference on Computer Vision (ECCV), 621–636.\nYang, Y .; Xing, W.; Wang, D.; Zhang, S.; Yu, Q.; and Wang,\nL. 2021. AEVRNet: Adaptive exploration network with\nvariance reduced optimization for visual tracking. Neuro-\ncomputing, 449: 48–60.\nYao, J.; Wang, D.; Hu, H.; Xing, W.; and Wang, L. 2022.\nADCNN: Towards learning adaptive dilation for convolu-\ntional neural networks. Pattern Recognition, 123: 108369.\nYao, J.; Xing, W.; Wang, D.; Xing, J.; and Wang, L. 2021.\nActive dropblock: Method to enhance deep model accuracy\nand robustness. Neurocomputing, 454: 189–200.\nZhang, J.; and Singh, S. 2014. LOAM: Lidar Odometry and\nMapping in Real-time. In Robotics: Science and Systems,\nvolume 2.\nZhao, H.; Jia, J.; and Koltun, V . 2020. Exploring self-\nattention for image recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 10076–10085.\n5421"
}