{
  "title": "On the Sentence Embeddings from Pre-trained Language Models",
  "url": "https://openalex.org/W3098400973",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2678089479",
      "name": "Li, Bohan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106821041",
      "name": "Zhou Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746867007",
      "name": "He, Junxian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1917300918",
      "name": "Wang Mingxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098367636",
      "name": "Yang Yiming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978908041",
      "name": "Li, Lei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2996657533",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2133458109",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W2964020555",
    "https://openalex.org/W2969262720",
    "https://openalex.org/W2125031621",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2907252220",
    "https://openalex.org/W2994610548",
    "https://openalex.org/W2963462013",
    "https://openalex.org/W2587690726",
    "https://openalex.org/W2970134931",
    "https://openalex.org/W2963139417",
    "https://openalex.org/W2752172973",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W2992005611",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2963748441"
  ],
  "abstract": "Pre-trained contextual representations like BERT have achieved great success in natural language processing. However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences. In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited. We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically. We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity. To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective. Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks. The code is available at https://github.com/bohanli/BERT-flow.",
  "full_text": "On the Sentence Embeddings from Pre-trained Language Models\nBohan Li†,‡∗, Hao Zhou†, Junxian He‡, Mingxuan Wang†, Yiming Yang‡, Lei Li†\n†ByteDance AI Lab\n‡Language Technologies Institute, Carnegie Mellon University\n{zhouhao.nlp,wangmingxuan.89,lileilab}@bytedance.com\n{bohanl1,junxianh,yiming}@cs.cmu.edu\nAbstract\nPre-trained contextual representations like\nBERT have achieved great success in natu-\nral language processing. However, the sen-\ntence embeddings from the pre-trained lan-\nguage models without ﬁne-tuning have been\nfound to poorly capture semantic meaning of\nsentences. In this paper, we argue that the se-\nmantic information in the BERT embeddings\nis not fully exploited. We ﬁrst reveal the the-\noretical connection between the masked lan-\nguage model pre-training objective and the se-\nmantic similarity task theoretically, and then\nanalyze the BERT sentence embeddings em-\npirically. We ﬁnd that BERT always induces\na non-smooth anisotropic semantic space of\nsentences, which harms its performance of\nsemantic similarity. To address this issue,\nwe propose to transform the anisotropic sen-\ntence embedding distribution to a smooth and\nisotropic Gaussian distribution through nor-\nmalizing ﬂows that are learned with an un-\nsupervised objective. Experimental results\nshow that our proposed BERT-ﬂow method ob-\ntains signiﬁcant performance gains over the\nstate-of-the-art sentence embeddings on a va-\nriety of semantic textual similarity tasks. The\ncode is available at https://github.com/\nbohanli/BERT-flow.\n1 Introduction\nRecently, pre-trained language models and its vari-\nants (Radford et al., 2019; Devlin et al., 2019; Yang\net al., 2019; Liu et al., 2019) like BERT (Devlin\net al., 2019) have been widely used as represen-\ntations of natural language. Despite their great\nsuccess on many NLP tasks through ﬁne-tuning,\nthe sentence embeddings from BERT without ﬁne-\ntuning are signiﬁcantly inferior in terms of se-\nmantic textual similarity (Reimers and Gurevych,\n∗ The work was done when BL was an intern at\nByteDance.\n2019) – for example, they even underperform the\nGloVe (Pennington et al., 2014) embeddings which\nare not contextualized and trained with a much sim-\npler model. Such issues hinder applying BERT\nsentence embeddings directly to many real-world\nscenarios where collecting labeled data is highly-\ncosting or even intractable.\nIn this paper, we aim to answer two major ques-\ntions: (1) why do the BERT-induced sentence em-\nbeddings perform poorly to retrieve semantically\nsimilar sentences? Do they carry too little semantic\ninformation, or just because the semantic meanings\nin these embeddings are not exploited properly? (2)\nIf the BERT embeddings capture enough semantic\ninformation that is hard to be directly utilized, how\ncan we make it easier without external supervision?\nTowards this end, we ﬁrst study the connection\nbetween the BERT pretraining objective and the se-\nmantic similarity task. Our analysis reveals that the\nsentence embeddings of BERT should be able to\nintuitively reﬂect the semantic similarity between\nsentences, which contradicts with experimental ob-\nservations. Inspired by Gao et al. (2019) who ﬁnd\nthat the language modeling performance can be\nlimited by the learned anisotropic word embedding\nspace where the word embeddings occupy a narrow\ncone, and Ethayarajh (2019) who ﬁnd that BERT\nword embeddings also suffer from anisotropy, we\nhypothesize that the sentence embeddings from\nBERT – as average of context embeddings from last\nlayers1 – may suffer from similar issues. Through\nempirical probing over the embeddings, we further\nobserve that the BERT sentence embedding space\nis semantically non-smoothing and poorly deﬁned\nin some areas, which makes it hard to be used di-\nrectly through simple similarity metrics such as dot\n1In this paper, we compute average of context embeddings\nfrom last one or two layers as our sentence embeddings since\nthey are consistently better than the [CLS] vector as shown\nin (Reimers and Gurevych, 2019).\narXiv:2011.05864v1  [cs.CL]  2 Nov 2020\nproduct or cosine similarity.\nTo address these issues, we propose to transform\nthe BERT sentence embedding distribution into a\nsmooth and isotropic Gaussian distribution through\nnormalizing ﬂows (Dinh et al., 2015), which is\nan invertible function parameterized by neural net-\nworks. Concretely, we learn a ﬂow-based genera-\ntive model to maximize the likelihood of generating\nBERT sentence embeddings from a standard Gaus-\nsian latent variable in a unsupervised fashion. Dur-\ning training, only the ﬂow network is optimized\nwhile the BERT parameters remain unchanged.\nThe learned ﬂow, an invertible mapping function\nbetween the BERT sentence embedding and Gaus-\nsian latent variable, is then used to transform the\nBERT sentence embedding to the Gaussian space.\nWe name the proposed method as BERT-ﬂow.\nWe perform extensive experiments on 7 stan-\ndard semantic textual similarity benchmarks with-\nout using any downstream supervision. Our empir-\nical results demonstrate that the ﬂow transforma-\ntion is able to consistently improve BERT by up\nto 12.70 points with an average of 8.16 points in\nterms of Spearman correlation between cosine em-\nbedding similarity and human annotated similarity.\nWhen combined with external supervision from\nnatural language inference tasks (Bowman et al.,\n2015; Williams et al., 2018), our method outper-\nforms the sentence-BERT embeddings (Reimers\nand Gurevych, 2019), leading to new state-of-the-\nart performance. In addition to semantic sim-\nilarity tasks, we apply sentence embeddings to\na question-answer entailment task, QNLI (Wang\net al., 2019), directly without task-speciﬁc super-\nvision, and demonstrate the superiority of our ap-\nproach. Moreover, our further analysis implies that\nBERT-induced similarity can excessively correlate\nwith lexical similarity compared to semantic sim-\nilarity, and our proposed ﬂow-based method can\neffectively remedy this problem.\n2 Understanding the Sentence\nEmbedding Space of BERT\nTo encode a sentence into a ﬁxed-length vector with\nBERT, it is a convention to either compute an aver-\nage of context embeddings in the last few layers of\nBERT, or extract the BERT context embedding at\nthe position of the [CLS] token. Note that there is\nno token masked when producing sentence embed-\ndings, which is different from pretraining.\nReimers and Gurevych (2019) demonstrate that\nsuch BERT sentence embeddings lag behind the\nstate-of-the-art sentence embeddings in terms of\nsemantic similarity. On the STS-B dataset, BERT\nsentence embeddings are even less competitive to\naveraged GloVe (Pennington et al., 2014) embed-\ndings, which is a simple and non-contextualized\nbaseline proposed several years ago. Nevertheless,\nthis incompetence has not been well understood\nyet in existing literature.\nNote that as demonstrated by Reimers and\nGurevych (2019), averaging context embeddings\nconsistently outperforms the [CLS] embedding.\nTherefore, unless mentioned otherwise, we use av-\nerage of context embeddings as BERT sentence\nembeddings and do not distinguish them in the rest\nof the paper.\n2.1 The Connection between Semantic\nSimilarity and BERT Pre-training\nWe consider a sequence of tokens x1:T =\n(x1,...,x T). Language modeling (LM) factor-\nizes the joint probability p(x1:T) in an autoregres-\nsive way, namely log p(x1:T) = ∑T\nt=1 log p(xt|ct)\nwhere the context ct = x1:t−1. To capture bidirec-\ntional context during pretraining, BERT proposes\na masked language modeling (MLM) objective,\nwhich instead factorizes the probability of noisy\nreconstruction p(¯x|ˆx) = ∑T\nt=1 mt p(xt|ct), where\nˆxis a corrupted sequence, ¯xis the masked tokens,\nmt is equal to 1 whenxt is masked and 0 otherwise.\nThe context ct = ˆx.\nNote that both LM and MLM can be reduced to\nmodeling the conditional distribution of a token x\ngiven the context c, which is typically formulated\nwith a softmax function as,\np(x|c) = exp h⊤\nc wx∑\nx′exp h⊤c wx′\n. (1)\nHere the context embedding hc is a function of\nc, which is usually heavily parameterized by a deep\nneural network (e.g., a Transformer (Vaswani et al.,\n2017)); The word embedding wx is a function of\nx, which is parameterized by an embedding lookup\ntable.\nThe similarity between BERT sentence embed-\ndings can be reduced to the similarity between\nBERT context embeddings hT\nc hc′2. However, as\n2This is because we approximate BERT sentence embed-\ndings with context embeddings, and compute their dot product\n(or cosine similarity) as model-predicted sentence similarity.\nDot product is equivalent to cosine similarity when the em-\nshown in Equation 1, the pretraining of BERT does\nnot explicitly involve the computation of hT\nc hc′.\nTherefore, we can hardly derive a mathematical\nformulation of what h⊤\nc hc′ exactly represents.\nCo-Occurrence Statistics as the Proxy for Se-\nmantic Similarity Instead of directly analyzing\nhT\nc h′\nc, we consider h⊤\nc wx, the dot product between\na context embedding hc and a word embeddingwx.\nAccording to Yang et al. (2018), in a well-trained\nlanguage model, h⊤\nc wx can be approximately de-\ncomposed as follows,\nh⊤\nc wx ≈log p∗(x|c) + λc (2)\n= PMI(x,c) + logp(x) + λc. (3)\nwhere PMI(x,c) = log p(x,c)\np(x)p(c) denotes the point-\nwise mutual information between xand c, log p(x)\nis a word-speciﬁc term, and λc is a context-speciﬁc\nterm.\nPMI captures how frequently two events co-\noccur more than if they independently occur. Note\nthat co-occurrence statistics is a typical tool to\ndeal with “semantics” in a computational way —\nspeciﬁcally, PMI is a common mathematical sur-\nrogate to approximate word-level semantic simi-\nlarity (Levy and Goldberg, 2014; Ethayarajh et al.,\n2019). Therefore, roughly speaking, it is seman-\ntically meaningful to compute the dot product be-\ntween a context embedding and a word embedding.\nHigher-Order Co-Occurrence Statistics as\nContext-Context Semantic Similarity. During\npretraining, the semantic relationship between two\ncontexts cand c′could be inferred and reinforced\nwith their connections to words. To be speciﬁc,\nif both the contexts c and c′ co-occur with the\nsame word w, the two contexts are likely to share\nsimilar semantic meaning. During the training\ndynamics, when cand woccur at the same time,\nthe embeddings hc and xw are encouraged to be\ncloser to each other, meanwhile the embedding hc\nand xw′ where w′̸= ware encouraged to be away\nfrom each other due to normalization. A similar\nscenario applies to the context c′. In this way, the\nsimilarity between hc and hc′ is also promoted.\nWith all the words in the vocabulary acting as\nhubs, the context embeddings should be aware of\nits semantic relatedness to each other.\nbeddings are normalized to unit hyper-sphere.\nHigher-order context-context co-occurrence\ncould also be inferred and propagated during pre-\ntraining. The update of a context embedding hc\ncould affect another context embedding hc′ in the\nabove way, and similarly hc′ can further affect an-\nother hc′′. Therefore, the context embeddings can\nform an implicit interaction among themselves via\nhigher-order co-occurrence relations.\n2.2 Anisotropic Embedding Space Induces\nPoor Semantic Similarity\nAs discussed in Section 2.1, the pretraining of\nBERT should have encouraged semantically mean-\ningful context embeddings implicitly. Why BERT\nsentence embeddings without ﬁnetuning yield un-\nsatisfactory performance?\nTo investigate the underlying problem of the fail-\nure, we use word embeddings as a surrogate be-\ncause words and contexts share the same embed-\nding space. If the word embeddings exhibits some\nmisleading properties, the context embeddings will\nalso be problematic, and vice versa.\nGao et al. (2019) and Wang et al. (2020) have\npointed out that, for language modeling, the max-\nimum likelihood training with Equation 1 usually\nproduces an anisotropic word embedding space.\n“Anisotropic” means word embeddings occupy a\nnarrow cone in the vector space. This phenomenon\nis also observed in the pretrained Transformers like\nBERT, GPT-2, etc (Ethayarajh, 2019).\nIn addition, we have two empirical observations\nover the learned anisotropic embedding space.\nObservation 1: Word Frequency Biases the\nEmbedding Space We expect the embedding-\ninduced similarity to be consistent to semantic sim-\nilarity. If embeddings are distributed in different\nregions according to frequency statistics, the in-\nduced similarity is not useful any more.\nHowever, as discussed by Gao et al. (2019),\nanisotropy is highly relevant to the imbalance of\nword frequency. They prove that under some\nassumptions, the optimal embeddings of non-\nappeared tokens in Transformer language models\ncan be extremely far away from the origin. They\nalso try to roughly generalize this conclusion to\nrarely-appeared words.\nTo verify this hypothesis in the context of BERT,\nwe compute the mean ℓ2 distance between the\nBERT word embeddings and the origin (i.e., the\nmean ℓ2-norm). In the upper half of Table 1, we\nobserve that high-frequency words are all close to\nRank of word frequency (0,100) [100,500) [500,5K) [5 K,1K)\nMeanℓ2-norm 0.95 1.04 1.22 1.45\nMeank-NNℓ2-dist. (k= 3) 0.77 0.93 1.16 1.30\nMeank-NNℓ2-dist. (k= 5) 0.83 0.99 1.22 1.34\nMeank-NNℓ2-dist. (k= 7) 0.87 1.04 1.26 1.37\nMeank-NN dot-product. (k= 3) 0.73 0.92 1.20 1.63\nMeank-NN dot-product. (k= 5) 0.73 0.91 1.19 1.61\nMeank-NN dot-product. (k= 7) 0.72 0.90 1.17 1.60\nTable 1: The mean ℓ2-norm, as well as their distance to theirk-nearest neighbors (among all the word embeddings)\nof the word embeddings of BERT, segmented by ranges of word frequency rank (counted based on Wikipedia\ndump; the smaller the more frequent).\nthe origin, while low-frequency words are far away\nfrom the origin.\nThis observation indicates that the word embed-\ndings can be biased to word frequency. This coin-\ncides with the second term in Equation 3, the log\ndensity of words. Because word embeddings play\na role of connecting the context embeddings during\ntraining, context embeddings might be misled by\nthe word frequency information accordingly and its\npreserved semantic information can be corrupted.\nObservation 2: Low-Frequency Words Dis-\nperse Sparsely We observe that, in the learned\nanisotropic embedding space, high-frequency\nwords concentrates densely and low-frequency\nwords disperse sparsely.\nThis observation is achieved by computing the\nmean ℓ2 distance of word embeddings to their\nk-nearest neighbors. In the lower half of Ta-\nble 1, we observe that the embeddings of low-\nfrequency words tends to be farther to their k-\nNN neighbors compared to the embeddings of\nhigh-frequency words. This demonstrates that low-\nfrequency words tends to disperse sparsely.\nDue to the sparsity, many “holes” could be\nformed around the low-frequency word embed-\ndings in the embedding space, where the semantic\nmeaning can be poorly deﬁned. Note that BERT\nsentence embeddings are produced by averaging\nthe context embeddings, which is a convexity-\npreserving operation. However, the holes violate\nthe convexity of the embedding space. This is a\ncommon problem in the context of representation\nlearining (Rezende and Viola, 2018; Li et al., 2019;\nGhosh et al., 2020). Therefore, the resulted sen-\ntence embeddings can locate in the poorly-deﬁned\nareas, and the induced similarity can be problem-\natic.\nInvertible mapping\nThe BERT sentenceembedding spaceStandard Gaussian latent space (isotropic)\n! \"\nFigure 1: An illustration of our proposed ﬂow-based\ncalibration over the original sentence embedding space\nof BERT.\n3 Proposed Method: BERT-ﬂow\nTo verify the hypotheses proposed in Section 2.2,\nand to circumvent the incompetence of the BERT\nsentence embeddings, we proposed a calibration\nmethod called BERT-ﬂow in which we take ad-\nvantage of an invertible mapping from the BERT\nembedding space to a standard Gaussian latent\nspace. The invertibility condition assures that the\nmutual information between the embedding space\nand the data examples does not change.\n3.1 Motivation\nA standard Gaussian latent space may have favor-\nable properties which can help with our problem.\nConnection to Observation 1 First, standard\nGaussian satisﬁes isotropy. The probabilistic den-\nsity in standard Gaussian distribution does not vary\nin terms of angle. If the ℓ2 norm of samples from\nstandard Gaussian are normalized to 1, these sam-\nples can be regarded as uniformly distributed over\na unit sphere.\nWe can also understand the isotropy from a sin-\ngular spectrum perspective. As discussed above,\nthe anisotropy of the embedding space stems from\nthe imbalance of word frequency. In the literature\nof traditional word embeddings, Mu et al. (2017)\ndiscovers that the dominating singular vectors can\nbe highly correlated to word frequency, which mis-\nleads the embedding space. By ﬁtting a mapping\nto an isotropic distribution, the singular spectrum\nof the embedding space can be ﬂattened. In this\nway, the word frequency-related singular directions,\nwhich are the dominating ones, can be suppressed.\nConnection to Observation 2 Second, the prob-\nabilistic density of Gaussian is well deﬁned over\nthe entire real space. This means there are no “hole”\nareas, which are poorly deﬁned in terms of proba-\nbility. The helpfulness of Gaussian prior for mit-\nigating the “hole” problem has been widely ob-\nserved in existing literature of deep latent variable\nmodels (Rezende and Viola, 2018; Li et al., 2019;\nGhosh et al., 2020).\n3.2 Flow-based Generative Model\nWe instantiate the invertible mapping with ﬂows.\nA ﬂow-based generative model (Kobyzev et al.,\n2019) establishes an invertible transformation from\nthe latent space Zto the observed space U. The\ngenerative story of the model is deﬁned as\nz ∼pZ(z),u = fφ(z)\nwhere z ∼pZ(z) the prior distribution, and f :\nZ→U is an invertible transformation. With the\nchange-of-variables theorem, the probabilistic den-\nsity function (PDF) of the observable x is given\nas,\npU(u) = pZ(f−1\nφ (u)) |det\n∂f−1\nφ (u)\n∂u |\nIn our method, we learn a ﬂow-based generative\nmodel by maximizing the likelihood of generat-\ning BERT sentence embeddings from a standard\nGaussian latent latent variable. In other words, the\nbase distribution pZis a standard Gaussian and we\nconsider the extracted BERT sentence embeddings\nas the observed space U. We maximize the like-\nlihood of U’s marginal via Equation 4 in a fully\nunsupervised way.\nmaxφ Eu=BERT(sentence),sentence∼D\nlog pZ(f−1\nφ (u)) + log |det\n∂f−1\nφ (u)\n∂u |,\n(4)\nHere Ddenotes the dataset, in other words, the\ncollection of sentences. Note that during training,\nonly the ﬂow parameters are optimized while the\nBERT parameters remain unchanged. Eventually,\nwe learn an invertible mapping functionf−1\nφ which\ncan transform each BERT sentence embedding u\ninto a latent Gaussian representation z without loss\nof information.\nThe invertible mapping fφ is parameterized as\na neural network, and the architectures are usu-\nally carefully designed to guarantee the invertibil-\nity (Dinh et al., 2015). Moreover, its determi-\nnant |det\n∂f−1\nφ (u)\n∂u |should also be easy to compute\nso as to make the maximum likelihood training\ntractable. In our experiments, we follows the de-\nsign of Glow (Kingma and Dhariwal, 2018). The\nGlow model is composed of a stack of multiple\ninvertible transformations, namely actnorm, invert-\nible 1 ×1 convolution, and afﬁne coupling layer3.\nWe simplify the model by replacing afﬁne coupling\nwith additive coupling (Dinh et al., 2015) to reduce\nmodel complexity, and replacing the invertible1×1\nconvolution with random permutation to avoid nu-\nmerical errors. For the mathematical formula of\nthe ﬂow model with additive coupling, please refer\nto Appendix A.\n4 Experiments\nTo verify our hypotheses and demonstrate the effec-\ntiveness of our proposed method, in this section we\npresent our experimental results for various tasks\nrelated to semantic textual similarity under multiple\nconﬁgurations. For the implementation details of\nour siamese BERT models and ﬂow-based models,\nplease refer to Appendix B.\n4.1 Semantic Textual Similarity\nDatasets. We evaluate our approach extensively\non the semantic textual similarity (STS) tasks.\nWe report results on 7 datasets, namely the STS\nbenchmark (STS-B) (Cer et al., 2017) the SICK-\nRelatedness (SICK-R) dataset (Marelli et al., 2014)\nand the STS tasks 2012 - 2016 (Agirre et al., 2012,\n2013, 2014, 2015, 2016). We obtain all these\ndatasets via the SentEval toolkit (Conneau and\nKiela, 2018). These datasets provide a ﬁne-grained\ngold standard semantic similarity between 0 and 5\nfor each sentence pair.\nEvaluation Procedure. Following the procedure\nin previous work like Sentence-BERT (Reimers\nand Gurevych, 2019) for the STS task, the predic-\n3For concrete mathamatical formulations, please refer to\nTable 1 of Kingma and Dhariwal (2018)\nDataset STS-B SICK-R STS-12 STS-13 STS-14 STS-15 STS-16\nPublished in (Reimers and Gurevych, 2019)\nAvg. GloVe embeddings 58.02 53.76 55.14 70.66 59.73 68.25 63.66\nAvg. BERT embeddings 46.35 58.40 38.78 57.98 57.98 63.15 61.06\nBERT CLS-vector 16.50 42.63 20.16 30.01 20.09 36.88 38.03\nOur Implementation\nBERTbase 47.29 58.21 49.07 55.92 54.75 62.75 65.19\nBERTbase-last2avg 59.04 63.75 57.84 61.95 62.48 70.95 69.81\nBERTbase-ﬂow (NLI∗) 58.56 ( ↓) 65.44 (↑) 59.54 (↑) 64.69 (↑) 64.66 (↑) 72.92 (↑) 71.84 (↑)\nBERTbase-ﬂow (target) 70.72 (↑) 63.11(↓) 63.48 ( ↑) 72.14 (↑) 68.42 (↑) 73.77 (↑) 75.37 (↑)\nBERTlarge 46.99 53.74 46.89 53.32 49.27 56.54 61.63\nBERTlarge-last2avg 59.56 60.22 57.68 61.37 61.02 68.04 70.32\nBERTlarge-ﬂow (NLI∗) 68.09 ( ↑) 64.62 (↑) 61.72 (↑) 66.05 (↑) 66.34 (↑) 74.87 (↑) 74.47 (↑)\nBERTlarge-ﬂow (target) 72.26 (↑) 62.50 (↑) 65.20 (↑) 73.39 (↑) 69.42 (↑) 74.92 (↑) 77.63 (↑)\nTable 2: Experimental results on semantic textual similarity without using NLI supervision. We report the Spear-\nman’s rank correlation between the cosine similarity of sentence embeddings and the gold labels on multiple\ndatasets. Numbers are reported as ρ×100. ↑denotes outperformance over its BERT baseline and↓denotes under-\nperformance. Our proposed BERT-ﬂow method achieves the best scores. Note that our BERT-ﬂow use -last2avg\nas default setting. ∗: Use NLI corpus for the unsupervised training of ﬂow; supervision labels of NLI are NOT\nvisible.\ntion of similarity consists of two steps: (1) ﬁrst, we\nobtain sentence embeddings for each sentence with\na sentence encoder, and (2) then, we compute the\ncosine similarity between the two embeddings of\nthe input sentence pair as our model-predicted sim-\nilarity. The reported numbers are the Spearman’s\ncorrelation coefﬁcients between the predicted simi-\nlarity and gold standard similarity scores, which is\nthe same way as in (Reimers and Gurevych, 2019).\nExperimental Details. We consider both\nBERTbase and BERT large in our experiments.\nSpeciﬁcally, we use an average pooling over BERT\ncontext embeddings in the last one or two layers\nas the sentence embedding which is found to\noutperform the [CLS] vector. Interestingly, our\npreliminary exploration shows that averaging the\nlast two layers of BERT (denoted by -last2avg)\nconsistently produce better results compared to\nonly averaging the last one layer. Therefore, we\nchoose -last2avg as our default conﬁguration when\nassessing our own approach.\nFor the proposed method, the ﬂow-based objec-\ntive (Equation 4) is maximized only to update the\ninvertible mapping while the BERT parameters re-\nmains unchanged. Our ﬂow models are by default\nlearned over the full target dataset (train + valida-\ntion + test). We denote this conﬁguration as ﬂow\n(target). Note that although we use the sentences of\nthe entire target dataset, learning ﬂow does not use\nany provided labels for training, thus it is a purely\nunsupervised calibration over the BERT sentence\nembedding space.\nWe also test our ﬂow-based model learned on\na concatenation of SNLI (Bowman et al., 2015)\nand MNLI (Williams et al., 2018) for comparison\n(ﬂow (NLI)). The concatenated NLI datasets com-\nprise of tremendously more sentence pairs (SNLI\n570K + MNLI 433K). Note that “ﬂow (NLI)” does\nnot require any supervision label. When ﬁtting\nﬂow on NLI corpora, we only use the raw sen-\ntences instead of the entailment labels. An intu-\nition behind the ﬂow (NLI) setting is that, com-\npared to Wikipedia sentences (on which BERT is\npretrained), the raw sentences of both NLI and STS\nare simpler and shorter. This means the NLI-STS\ndiscrepancy could be relatively smaller than the\nWikipedia-STS discrepancy.\nWe run the experiments on two settings: (1)\nwhen external labeled data is unavailable. This\nis the natural setting where we learn ﬂow parame-\nters with the unsupervised objective (Equation 4),\nmeanwhile BERT parameters are unchanged. (2)\nwe ﬁrst ﬁne-tune BERT on the SNLI+MNLI tex-\ntual entailment classiﬁcation task in a siamese fash-\nion (Reimers and Gurevych, 2019). For BERT-\nﬂow, we further learn the ﬂow parameters. This\nsetting is to compare with the state-of-the-art re-\nsults which utilize NLI supervision (Reimers and\nGurevych, 2019). We denote the two different mod-\nels as BERT-NLI and BERT-NLI-ﬂow respectively.\nResults w/o NLI Supervision. As shown in Ta-\nble 2, the original BERT sentence embeddings\n(with both BERT base and BERTlarge) fail to out-\nperform the averaged GloVe embeddings. And\nDataset STS-B SICK-R STS-12 STS-13 STS-14 STS-15 STS-16\nPublished in (Reimers and Gurevych, 2019)\nInferSent - Glove 68.03 65.65 52.86 66.75 62.15 72.77 66.86\nUSE 74.92 76.69 64.49 67.80 64.61 76.83 73.18\nSBERTbase-NLI 77.03 72.91 70.97 76.53 73.19 79.09 74.30\nSBERTlarge-NLI 79.23 73.75 72.27 78.46 74.90 80.99 76.25\nSRoBERTabase-NLI 77.77 74.46 71.54 72.49 70.80 78.74 73.69\nSRoBERTalarge-NLI 79.10 74.29 74.53 77.00 73.18 81.85 76.82\nOur Implementation\nBERTbase-NLI 77.08 72.62 66.23 70.22 72.15 77.35 73.91\nBERTbase-NLI-last2avg 78.03 74.07 68.37 72.44 73.98 79.15 75.39\nBERTbase-NLI-ﬂow (NLI∗) 79.10 ( ↑) 78.03 (↑) 67.75 (↓) 76.73 (↑) 75.53 (↑) 80.63 (↑) 77.58 (↑)\nBERTbase-NLI-ﬂow (target) 81.03 (↑) 74.97 (↑) 68.95 (↑) 78.48 (↑) 77.62 (↑) 81.95 (↑) 78.94 (↑)\nBERTlarge-NLI 77.80 73.44 66.87 73.91 74.04 79.14 75.35\nBERTlarge-NLI-last2avg 78.45 74.93 68.69 75.63 75.55 80.35 76.81\nBERTlarge-NLI-ﬂow (NLI∗) 79.89 (↑) 77.73 (↑) 69.61 (↑) 79.45 (↑) 77.56 (↑) 82.48 (↑) 79.36 (↑)\nBERTlarge-NLI-ﬂow (target)81.18 (↑) 74.52 (↓) 70.19 (↑) 80.27 (↑) 78.85 (↑) 82.97 (↑) 80.57 (↑)\nTable 3: Experimental results on semantic textual similarity with NLI supervision. Note that our ﬂows are still\nlearned in a unsupervised way. InferSent (Conneau et al., 2017) is a siamese LSTM train on NLI, Universal\nSentence Encoder (USE) (Cer et al., 2018) replace the LSTM with a Transformer and SBERT (Reimers and\nGurevych, 2019) further use BERT. We report the Spearman’s rank correlation between the cosine similarity of\nsentence embeddings and the gold labels on multiple datasets. Numbers are reported as ρ×100. ↑denotes\noutperformance over its BERT baseline and ↓denotes underperformance. Our proposed BERT-ﬂow (i.e., the\n“BERT-NLI-ﬂow” in this table) method achieves the best scores. Note that our BERT-ﬂow use-last2avg as default\nsetting. ∗: Use NLI corpus for the unsupervised training of ﬂow; supervision labels of NLI are NOT visible.\naveraging the last-two layers of the BERT model\ncan consistently improve the results. For BERTbase\nand BERTlarge, our proposed ﬂow-based method\n(BERT-ﬂow (target)) can further boost the perfor-\nmance by 5.88 and 8.16 points on average respec-\ntively. For most of the datasets, learning ﬂows\non the target datasets leads to larger performance\nimprovement than on NLI. The only exception is\nSICK-R where training ﬂows on NLI is better. We\nthink this is because SICK-R is collected for both\nentailment and relatedness. Since SNLI and MNLI\nare also collected for textual entailment evaluation,\nthe distribution discrepancy between SICK-R and\nNLI may be relatively small. Also due to the much\nlarger size of the NLI datasets, it is not surpris-\ning that learning ﬂows on NLI results in stronger\nperformance.\nResults w/ NLI Supervision. Table 3 shows the\nresults with NLI supervisions. Similar to the fully\nunsupervised results before, our isotropic embed-\nding space from invertible transformation is able\nto consistently improve the SBERT baselines in\nmost cases, and outperforms the state-of-the-art\nSBERT/SRoBERTa results by a large margin. Ro-\nbustness analysis with respect to random seeds are\nprovided in Appendix C.\n4.2 Unsupervised Question-Answer\nEntailment\nIn addition to the semantic textual similarity\ntasks, we examine the effectiveness of our\nmethod on unsupervised question-answer entail-\nment. We use Question Natural Language Infer-\nence (QNLI, Wang et al. (2019)), a dataset compris-\ning 110K question-answer pairs (with 5K+ for test-\ning). QNLI extracts the questions as well as their\ncorresponding context sentences from SQUAD (Ra-\njpurkar et al., 2016), and annotates each pair as ei-\nther entailment or no entailment. In this paper, we\nfurther adapt QNLI as an unsupervised task. The\nsimilarity between a question and an answer can\nbe predicted by computing the cosine similarity of\ntheir sentence embeddings. Then we regard entail-\nment as 1 and no entailment as 0, and evaluate the\nperformance of the methods with AUC.\nAs shown in Table 4, our method consistently\nimproves the AUC on the validation set of QNLI.\nAlso, learning ﬂow on the target dataset can pro-\nduce superior results compared to learning ﬂows\non NLI.\nMethod AUC\nBERTbase-NLI-last2avg 70.30\nBERTbase-NLI-ﬂow (NLI∗) 72.52 ( ↑)\nBERTbase-NLI-ﬂow (target) 76.17 (↑)\nBERTlarge-NLI-last2avg 70.41\nBERTlarge-NLI-ﬂow (NLI∗) 74.19 (↑)\nBERTlarge-NLI-ﬂow (target)77.09 (↑)\nTable 4: AUC on QNLI evaluated on the validation\nset. ∗: Use NLI corpus for the unsupervised training\nof ﬂow; supervision labels of NLI are NOT visible.\nMethod Correlation\nBERTbase 47.29\n+ SN 55.46\n+ NATSV (k= 1) 51.79\n+ NATSV (k= 10) 60.40\n+ SN + NATSV (k= 1) 56.02\n+ SN + NATSV (k= 6) 63.51\nBERTbase-ﬂow (target) 65.62\nTable 5: Comparing ﬂow-based method with baselines\non STS-B. kis selected among1 ∼20 on the validation\nset. We report the Spearman’s rank correlation (×100).\n4.3 Comparison with Other Embedding\nCalibration Baselines\nIn the literature of traditional word embeddings,\nArora et al. (2017) and Mu et al. (2017) also dis-\ncover the anisotropy phenomenon of the embed-\nding space, and they provide several methods to\nencourage isotropy:\nStandard Normalization (SN). In this idea, we\nconduct a simple post-processing over the embed-\ndings by computing the mean µ and standard devi-\nation σ of the sentence embeddings u’s, and nor-\nmalizing the embeddings by u−µ\nσ .\nNulling Away Top-kSingular Vectors (NATSV).\nMu et al. (2017) ﬁnd out that sentence embeddings\ncomputed by averaging traditional word embed-\ndings tend to have a fast-decaying singular spec-\ntrum. They claim that, by nulling away the top- k\nsingular vectors, the anisotropy of the embeddings\ncan be circumvented and better semantic similarity\nperformance can be achieved.\nWe compare with these embedding calibration\nmethods on STS-B dataset and the results are\nshown in Table 5. Standard normalization (SN)\nhelps improve the performance but it falls behind\nnulling away top-ksingular vectors (NATSV). This\nmeans standard normalization cannot fundamen-\ntally eliminate the anisotropy. By combining the\ntwo methods, and carefully tuning kover the vali-\ndation set, further improvements can be achieved.\nSimilarity Edit distance Gold similarity\nGold similarity -24.61 100.00\nBERT-induce similarity -50.49 59.30\nFlow-induce similarity -28.01 74.09\nTable 6: Spearman’s correlation ρ between various\nsentence similarities on the validation set of STS-B.\nWe can observe that BERT-induced similarity is highly\ncorrelated to edit distance, while the correlation with\nedit distance is less evident for gold standard or ﬂow-\ninduced similarity.\nNevertheless, our method still produces much bet-\nter results. We argue that NATSV can help elimi-\nnate anisotropy but it may also discard some useful\ninformation contained in the nulled vectors. On the\ncontrary, our method directly learns an invertible\nmapping to isotropic latent space without discard-\ning any information.\n4.4 Dicussion: Semantic Similarity Versus\nLexical Similarity\nIn addition to semantic similarity, we further study\nlexical similarity induced by different sentence em-\nbeddings. Speciﬁcally, we use edit distance as the\nmetric for lexical similarity between a pair of sen-\ntences, and focus on the correlations between the\nsentence similarity and edit distance. Concretely,\nwe compute the cosine similarity in terms of BERT\nsentence embeddings as well as edit distance for\neach sentence pair. Within a dataset consisting of\nmany sentence pairs, we compute the Spearman’s\ncorrelation coefﬁcient ρbetween the similarities\nand the edit distances, as well as between similari-\nties from different models. We perform experiment\non the STS-B dataset and include the human anno-\ntated gold similarity into this analysis.\nBERT-Induced Similarity Excessively Corre-\nlates with Lexical Similarity. Table 6 shows\nthat the correlation between BERT-induced similar-\nity and edit distance is very strong (ρ= −50.49),\nconsidering that gold standard labels maintain a\nmuch smaller correlation with edit distance ( ρ =\n−24.61). This phenomenon can also be observed\nin Figure 2. Especially, for sentence pairs with\nedit distance ≤4 (highlighted with green), BERT-\ninduced similarity is extremely correlated to edit\ndistance. However, it is not evident that gold stan-\ndard semantic similarity correlates with edit dis-\ntance. In other words, it is often the case where\nthe semantics of a sentence can be dramatically\n0 2 4\nSimilarity\n0\n5\n10\n15\n20\n25Edit distance\nGold standard\n0.50 0.75 1.00\nSimilarity\n0\n5\n10\n15\n20\n25Edit distance\nBERT-induced\n0.0 0.5 1.0\nSimilarity\n0\n5\n10\n15\n20\n25Edit distance\nFlow-induced\nFigure 2: A scatterplot of sentence pairs, where the horizontal axis represents similarity (either gold standard\nsemantic similarity or embedding-induced similarity), the vertical axis represents edit distance. The sentence pairs\nwith edit distance ≤4 are highlighted with green, meanwhile the rest of the pairs are colored with blue. We can\nobserved that lexically similar sentence pairs tends to be predicted to be similar by BERT embeddings, especially\nfor the green pairs. Such correlation is less evident for gold standard labels or ﬂow-induced embeddings.\nchanged by modifying a single word. For example,\nthe sentences “I like this restaurant” and “I dislike\nthis restaurant” only differ by one word, but convey\nopposite semantic meaning. BERT embeddings\nmay fail in such cases. Therefore, we argue that the\nlexical proximity of BERT sentence embeddings\nis excessive, and can spoil their induced semantic\nsimilarity.\nFlow-Induced Similarity Exhibits Lower Cor-\nrelation with Lexical Similarity. By transform-\ning the original BERT sentence embeddings into\nthe learned isotropic latent space with ﬂow, the\nembedding-induced similarity not only aligned bet-\nter with the gold semantic semantic similarity, but\nalso shows a lower correlation with lexical simi-\nlarity, as presented in the last row of Table 6. The\nphenomenon is especially evident for the examples\nwith edit distance ≤4 (highlighted with green in\nFigure 2). This demonstrates that our proposed\nﬂow-based method can effectively suppress the ex-\ncessive inﬂuence of lexical similarity over the em-\nbedding space.\n5 Conclusion and Future Work\nIn this paper, we investigate the deﬁciency of the\nBERT sentence embeddings on semantic textual\nsimilarity, and propose a ﬂow-based calibration\nwhich can effectively improve the performance. In\nthe future, we are looking forward to diving in\nrepresentation learning with ﬂow-based generative\nmodels from a broader perspective.\nAcknowledgments\nThe authors would like to thank Jiangtao Feng,\nWenxian Shi, Yuxuan Song, and anonymous re-\nviewers for their helpful comments and suggestion\non this paper.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, et al. 2015. SemEval-2015 task 2: Se-\nmantic textual similarity, english, spanish and pilot\non interpretability. In Proceedings of SemEval.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. SemEval-2014 task 10: Multilingual\nsemantic textual similarity. In Proceedings of Se-\nmEval.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona\nDiab, Aitor Gonzalez Agirre, Rada Mihalcea, Ger-\nman Rigau Claramunt, and Janyce Wiebe. 2016.\nSemEval-2016 task 1: Semantic textual similarity,\nmonolingual and cross-lingual evaluation. In Pro-\nceedings of SemEval.\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. SemEval-2012 task 6: A\npilot on semantic textual similarity. In Proceedings\nof SemEval.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. * SEM 2013 shared\ntask: Semantic textual similarity. In Proceedings of\nSemEval.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In Proceedings of ICLR.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of EMNLP.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of LREC.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nEMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL.\nLaurent Dinh, David Krueger, and Yoshua Bengio.\n2015. NICE: Non-linear independent components\nestimation. In Proceedings of ICLR.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? comparing the geome-\ntry of bert, elmo, and gpt-2 embeddings. In Proceed-\nings of EMNLP-IJCNLP.\nKawin Ethayarajh, David Duvenaud, and Graeme Hirst.\n2019. Towards understanding linear word analogies.\nIn Proceedings of ACL.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-\nYan Liu. 2019. Representation degeneration prob-\nlem in training natural language generation models.\nIn Proceedings of ICLR.\nPartha Ghosh, Mehdi SM Sajjadi, Antonio Vergari,\nMichael Black, and Bernhard Scholkopf. 2020.\nFrom variational to deterministic autoencoders. In\nProceedings of ICLR.\nDurk P Kingma and Prafulla Dhariwal. 2018. Glow:\nGenerative ﬂow with invertible 1x1 convolutions. In\nProceedings of NeurIPS.\nIvan Kobyzev, Simon Prince, and Marcus A Brubaker.\n2019. Normalizing ﬂows: Introduction and ideas.\narXiv preprint arXiv:1908.09257.\nOmer Levy and Yoav Goldberg. 2014. Neural word\nembedding as implicit matrix factorization. In Pro-\nceedings of NeurIPS.\nBohan Li, Junxian He, Graham Neubig, Taylor Berg-\nKirkpatrick, and Yiming Yang. 2019. A surprisingly\neffective ﬁx for deep latent variable modeling of text.\nIn Proceedings of EMNLP-IJCNLP.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, Roberto Zamparelli,\net al. 2014. A SICK cure for the evaluation of com-\npositional distributional semantic models. In Pro-\nceedings of LREC.\nJiaqi Mu, Suma Bhat, and Pramod Viswanath. 2017.\nAll-but-the-top: Simple and effective postprocessing\nfor word representations. In Proceedings of ICLR.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. GloVe: Global vectors for word rep-\nresentation. In Proceedings of EMNLP.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8).\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nEMNLP.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using siamese BERT-\nnetworks. In Proceedings of EMNLP-IJCNLP.\nDanilo Jimenez Rezende and Fabio Viola. 2018. Tam-\ning V AEs.arXiv preprint arXiv:1810.00597.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of ICLR.\nLingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,\nGuangtao Wang, and Quanquan Gu. 2020. Improv-\ning neural language generation with spectrum con-\ntrol. In Proceedings of ICLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of ACL.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. In Pro-\nceedings of ICLR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019. XLNet: Generalized autoregressive pretrain-\ning for language understanding. In Proceedings of\nNeurIPS.\nA Mathematical Formula of the Invertible Mapping\nGenerally, ﬂow-based model is a stacked sequence of many invertible transformation layers: f =\nf1 ◦f2 ◦... ◦fK. Speciﬁcally, in our approach, each transformation fi : x→yis an additive coupling\nlayer, which can be mathematically formulated as follows.\ny1:d = x1:d (5)\nyd+1:D = xd+1:D + gψ(x1:d). (6)\nHere gψ can be parameterized with a deep neural network for the sake of expressiveness.\nIts inverse function f−1\ni : y→xcan be explicitly written as:\nx1:d = y1:d (7)\nxd+1:D = yd+1:D −gψ(y1:d). (8)\nB Implementation Details\nThroughout our experiment, we adopt the ofﬁcial Tensorﬂow code of BERT 4 as our codebase. Note\nthat we clip the maximum sequence length to 64 to reduce the costing of GPU memory. For the NLI\nﬁnetuning of siamese BERT, we folllow the settings in (Reimers and Gurevych, 2019) (epochs = 1,\nlearning rate = 2e−5, and batch size = 16). Our results may vary from their published one. The\nauthors mentioned in https://github.com/UKPLab/sentence-transformers/issues/50 that this is\na common phenonmenon and might be related the random seed. Note that their implementation relies on\nthe Transformers repository of Huggingface5. This may also lead to discrepancy between the speciﬁc\nnumbers.\nOur implementation of ﬂows is adapted from both the ofﬁcial repository of GLOW 6 as well as the\nimplementation fo the Tensor2tensor library 7. The hyperparameters of our ﬂow models are given in\nTable 7. On the target datasets, we learn the ﬂow parameters for 1 epoch with learning rate 1e−3. On\nNLI datasets, we learn the ﬂow parameters for 0.15 epoch with learning rate 2e−5. The optimizer that\nwe use is Adam.\nIn our preliminary experiments on STS-B, we tune the hyperparameters on the dev set of STS-B.\nEmpirically, the performance does not vary much with regard to the architectural hyperparameters\ncompared to the learning schedule. Afterwards, we do not tune the hyperparameters any more when\nworking on the other datasets. Empirically, we ﬁnd the hyperparameters of ﬂow are not sensitive across\nthe datasets.\nCoupling architecture in 3-layer CNN with residual connection\nCoupling width 32\n#levels 2\nDepth 3\nTable 7: Flow hyperparameters.\n4https://github.com/google-research/bert\n5https://github.com/huggingface/transformers\n6https://github.com/openai/glow\n7https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/\nresearch/glow.py\nC Results with Different Random Seeds\nWe perform 5 runs with different random seeds in the NLI-supervised setting on STS-B. Results with\nstandard deviation and median are demonstrated in Table 8. Although the variance of NLI ﬁnetuning is\nnot negligible, our proposed ﬂow-based method consistently leads to improvement.\nMethod Spearman’s ρ\nBERT-NLI-large 77.26 ±1.76 (median: 78.19)\nBERT-NLI-large-last2avg 78.07 ±1.50 (median: 78.68)\nBERT-NLI-large-last2avg + ﬂow-target 81.10 ±0.55 (median: 81.35)\nTable 8: Results with different random seeds.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7955647706985474
    },
    {
      "name": "Sentence",
      "score": 0.7396842241287231
    },
    {
      "name": "Natural language processing",
      "score": 0.7133774161338806
    },
    {
      "name": "Semantic similarity",
      "score": 0.6980394721031189
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6424537301063538
    },
    {
      "name": "Embedding",
      "score": 0.6183536052703857
    },
    {
      "name": "Meaning (existential)",
      "score": 0.5188349485397339
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5151427984237671
    },
    {
      "name": "Language model",
      "score": 0.47368553280830383
    },
    {
      "name": "Image (mathematics)",
      "score": 0.12008434534072876
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ],
  "cited_by": 23
}