{
  "title": "Knowledge Unlearning for Mitigating Privacy Risks in Language Models",
  "url": "https://openalex.org/W4385572399",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3092799944",
      "name": "Joel Jang",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2312254425",
      "name": "DongKeun Yoon",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2253211678",
      "name": "Sohee Yang",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2518609154",
      "name": "Sungmin Cha",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2513695887",
      "name": "Moontae Lee",
      "affiliations": [
        "University of Illinois System"
      ]
    },
    {
      "id": "https://openalex.org/A2396426249",
      "name": "Lajanugen Logeswaran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2252216270",
      "name": "Minjoon Seo",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288283361",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W4283805899",
    "https://openalex.org/W4385573569",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W2109426455",
    "https://openalex.org/W3175115403",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W2963956191",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W1488996941",
    "https://openalex.org/W95183648",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2083773633",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W4385572901",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3174532363",
    "https://openalex.org/W3035644192",
    "https://openalex.org/W4283364113"
  ],
  "abstract": "Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, Minjoon Seo. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14389–14408\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nKnowledge Unlearning for Mitigating Privacy Risks in Language Models\nJoel Jang1∗ Dongkeun Yoon 1 Sohee Yang1 Sungmin Cha3\nMoontae Lee2,4 Lajanugen Logeswaran2 Minjoon Seo1\n1KAIST 2LG AI Research 3 Seoul National University 4University of Illinois\njoeljang@kaist.ac.kr\nAbstract\nPretrained Language Models (LMs) memorize\na vast amount of knowledge during initial pre-\ntraining, including information that may vio-\nlate the privacy of personal lives and identities.\nPrevious work addressing privacy issues for\nLMs has mostly focused on data preprocessing\nand differential privacy methods, both requir-\ning re-training the underlying LM. We propose\nknowledge unlearning as an alternative method\nto reduce privacy risks for LMs post hoc. We\nshow that simply performing gradient ascent\non target token sequences is effective at for-\ngetting them with little to no degradation of\ngeneral language modeling performances for\nlarger-sized LMs. We also find that sequential\nunlearning is better than trying to unlearn all\nthe data at once and that unlearning is highly\ndependent on which kind of data (domain) is\nforgotten. By showing comparisons with pre-\nvious methods known to mitigate privacy risks\nfor LMs, we show that our approach can give\na stronger empirical privacy guarantee in sce-\nnarios where the data vulnerable to extraction\nattacks are known a priori while being much\nmore efficient and robust 1.\n1 Introduction\nRecent work has shown that an adversary can ex-\ntract training data from Pretrained Language Mod-\nels (LMs) including Personally Identifiable Infor-\nmation (PII) such as names, phone numbers, and\nemail addresses, and other information such as li-\ncensed code, private clinical notes, and 128-bit\nUUIDs (Carlini et al., 2021; Lee et al., 2022; Huang\net al., 2022; Lehman et al., 2021). In 2021, an AI\nchatbot Iruda became the first AI system to be sued\nfor violating the Personal Information Protection\nAct after generating the exact home addresses and\nbank account numbers of actual individuals unin-\ntentionally (Park, 2021). Heikkilä (2022) has also\n∗work done during internship at LG AI Research.\n1We release the code and dataset needed to replicate our\nresults at https://github.com/joeljang/knowledge-unlearning.\nshown that GPT-3 (Brown et al., 2020), one of\nthe most well-known LM currently in commercial\nuse, offered detailed private information about the\nEditor-in-Chief of MIT Technology Review includ-\ning his family members, work address, and phone\nnumber. Considering findings that show extract-\ning training data gets easier as LMs scale to larger\nsizes (Carlini et al., 2022a) and that it is common\npractice for practitioners to release billion parame-\nters pretrained LMs for public use (Gao et al., 2020;\nBlack et al., 2021; Zhang et al., 2022), it has be-\ncome important to provide privacy guarantees for\nlarge LMs.\nPractitioners are required to delete personal in-\nformation from the LMs by individuals’ request\nbecause each individual has the “Right To Be For-\ngotten (RTBF)\" (Mantelero, 2013; Graves et al.,\n2021) and can limit the direct and indirect commer-\ncial use of their personal information (Villaronga\net al., 2018). Previous methods addressing privacy\nrisks for language models attempt to remove all\nprivate information from the training data (data pre-\nprocessing) (Aura et al., 2006; Dernoncourt et al.,\n2017; Lison et al., 2021; Kandpal et al., 2022) or at-\ntempt to design algorithms that ensure differential\nprivacy (DP) (Dwork, 2008; Dwork et al., 2006;\nAbadi et al., 2016; Anil et al., 2021; Li et al., 2022;\nYu et al., 2022). Both approaches require retrain-\ning the underlying LM every time individuals want\nto practice their RTBF, which makes them inade-\nquate for large LMs that are extremely costly to\nretrain. Furthermore, as pointed out by Brown\net al. (2022), data preprocessing methods assume\nprivate information to be easily identifiable, spec-\nified, and removed and DP algorithms can only\nguarantee protection for information that has clear\nprivacy borders, which makes them inadequate in\nthe real-world scenarios where the standard of pri-\nvacy might differ by each individual.\nTo this end, we propose knowledge unlearning\n(Figure 1) as an efficient solution that can be ap-\n14389\nName: Bob\nAge: 27\nMarital Status: Single\nSSN: 123 - 4567 - 8910\nDetails: Got divorced by ex-wife named  \nAlice and is currently undergoing custody \nbattles.\nNet Worth: $5,000,000\nSensitive Personal Information\nData \nPreprocessing\nDifferential \nPrivacy\nKnowledge \nUnlearning\nLM\nFind and Remove\nRe-train LM after sanitization \n(~900 A100 GPU days)\nLM\nRe-train LM with DP Algorithm \n(~1800 A100 GPU days)\nLM\nPerform a few token updates \n(~0.001 A100 GPU days)\nI practice my Right To  \nBe Forgotten (RTBF)!\nPretraining \nCorpora\nPretraining \nCorpora\nToken  \nSequences\nOur Proposed Approach\nBob\nFigure 1: Comparison of previous approaches and knowledge unlearning when an individual practices his/her\nRight-To-Be-Forgotten (RTBF).\nplied with just a few parameter updates instead\nof pretraining the underlying LM again. We per-\nform experiments on GPT-Neo LMs (125M, 1.3B,\n2.7B) (Black et al., 2021) and show that simply\nchanging the gradient descent to the opposite di-\nrection during language modeling (which can also\nbe seen as maximizing instead of minimizing the\nloss function) is effective at protecting target se-\nquences from extraction attacks with little to no\nperformance degradation on the initial LM capa-\nbilities measured via 13 downstream NLP tasks: 9\ncommon classification benchmarks and 4 dialogue\ntasks. For some cases, knowledge unlearning un-\nexpectedly shows significant improvements in LM\nperformance for some of the benchmarks.\nWe compare our approach with data deduplica-\ntion method (Kandpal et al., 2022) and differential\nprivacy decoding method (Majmudar et al., 2022)\nwhich are both known to mitigate privacy risks and\nshow the effectiveness of knowledge unlearning\nby providing strong privacy protection while being\nmuch more efficient and robust. We also provide\na general guideline that can be used to quantify\nthe memorization and extraction likelihood of tar-\nget token sequences and suggest when we can em-\npirically consider them to have been “forgotten”.\nSpecifically, we introduce a novel metric that mea-\nsures the extraction likelihood by varying the prefix\nlength of the target token sequence and quantifying\nhow much of the suffix is actually extracted from\nthe LM.\nSurprisingly, for knowledge unlearning, we find\nthat it is easier to forget a chunk of instances se-\nquentially rather than trying to forget them all at\nonce. We provide further analysis and show that the\ndifficulty of knowledge unlearning depends heavily\non the target data being forgotten, especially the\ndomain of the target data. We also provide em-\npirical examples of performing extraction attacks\nand how exactly knowledge unlearning provides\nprivacy protection for the LM.\nTo summarize, our main contributions are four-\nfold:\n• We compare knowledge unlearning with two\napproaches from literature known to mitigate\nprivacy risks: a data preprocessing approach\nand a Differential Privacy (DP) Decoding ap-\nproach. We show that our approach results in\nlittle to no performance degradation of gen-\neral capabilities (sometimes resulting in im-\nprovement) while providing strong privacy\nprotections in situations individuals practice\ntheir RTBF whereas the data preprocessing\napproach provides weaker privacy protection\nwhile being orders of magnitude computa-\ntionally demanding and the DP Decoding ap-\nproach results in severe degradation of LM\nperformance.\n14390\n• We perform additional experiments to deter-\nmine which factors contribute to the difficulty\nof knowledge unlearning and find that (1) try-\ning to forget many samples at once results\nin substantial LM performance degradation\nwhich can be mitigated by sequentially forget-\nting chunks of data and that (2) the domain\nof the target data (Code, License, Wikipedia,\netc.) plays a critical role in determining how\nhard they are to forget.\n• We provide a novel metric and a general guide-\nline for quantifying the privacy risks for LMs\nand determine when they should be consid-\nered to have “forgotten\" a given target se-\nquence.\n• Knowledge unlearning surprisingly seems to\nmake LMs stronger where the extreme cases\nbring +8.0% (37.6% → 45.6%), +10.1%\n(57.4% →67.5%), and +7.9% (62.2% →\n70.1%) improvements on Lambada for GPT-\nNEO 125M, 1.3B, and 2.7B, respectively.\n2 Related Work\n2.1 Privacy Methods for Language Models\nPrior work that tries to mitigate privacy risks for\nLMs can be divided mainly into data pre/post-\nprocessing methods and differential privacy meth-\nods.\n(Data) Pre/Post-Processing Data preprocessing\naims to sanitize the training data; it aims to get\nrid of all data that might violate any kind of pri-\nvacy from the training data prior to training. These\nmethods mostly utilize measures such as parsers\nand classification models that try to identify and\npredict patterns that constitute private information.\nThis is effective at identifying well-formatted pri-\nvate information such as social security numbers or\nspecial forms of medical notes (Aura et al., 2006;\nDernoncourt et al., 2017; Lison et al., 2021; Kand-\npal et al., 2022). However, as pointed out by Brown\net al. (2022), considering that private information is\nmostly context-dependent and sometimes in a non-\nspecific format, data preprocessing methods cannot\nfully claim that they provide privacy guarantees,\nespecially guarantees that match each individual’s\nstandards. Methods that attempt to utilize post-\nprocessing methods such as applying censorship to\nthe LM outputs still face the same limitations.\nIn this work, we compare our proposed method\nwith a data preprocessing approach proposed by\nKandpal et al. (2022) which shows that deduplicat-\ning the training corpora before pretraining helps\npretrain LMs that show stronger robustness against\nextraction attacks than an LM pretrained under the\nsame circumstances without deduplicating the pre-\ntraining corpora. However, we highlight that this\napproach, which may still be effective at mitigating\nthe overall privacy risks, is not the most suitable\napproach when considering a realistic scenario of\nindividuals requesting the removal of their infor-\nmation from the implicit parameters of the LMs.\nDifferential Privacy Differential Privacy (DP)\naims to guarantee that the effect of an individ-\nual input on the output of a specific function is\nbounded (Dwork, 2008; Dwork et al., 2006). In\nthe context of deep neural networks, DP, which\nneeds to be applied during the training phase, aims\nto construct models that can provide general guar-\nantees that the individual information within the\ntraining data cannot be inferred (Abadi et al., 2016).\nWhile DP has shown to be surprisingly effective at\nfine-tuning LMs (Li et al., 2022; Yu et al., 2022),\npretraining LMs with DP still suffers from sub-\nstantial performance gap, expensive computation,\nand slow convergence (Anil et al., 2021). Further-\nmore, as pointed out by Brown et al. (2022), DP\ncan only provide limited guarantees for LMs be-\ncause DP requires a unified definition for privacy\nboundaries, which is inherently impossible for nat-\nural language data. Most importantly, in a realis-\ntic scenario where individuals may practice their\nRight-To-Be-Forgotten (RTBF) dynamically after\nmodel deployment, it is nontrivial to apply existing\ndescent-based DP algorithms such as DP-SGD to\nonly protection against targeted extraction attacks.\n2.2 Machine Unlearning\nMachine unlearning has received attention as an\nalternative approach to overcome data privacy is-\nsues in machine learning (Cao and Yang, 2015;\nGinart et al., 2019; Bourtoule et al., 2021; Graves\net al., 2021). Several studies attempt to explore\nmachine unlearning for deep neural networks (Go-\nlatkar et al., 2020; Mehta et al., 2022). However,\nthey mostly focus on proposing algorithms for im-\nage classification models where they aim to forget\na whole class; that is, achieve random performance\nfor specific image classes such as “cats” or “ships”.\nWe are the first, to the best of our knowledge, to\n14391\nexplore unlearning a specific sequence of tokens\nfor LMs which is a quite different set-up from tradi-\ntional image classification models (∼tens of image\nclasses vs. a sequence of tokens that can each be\nclassified into V ∈R∼50,000). In this work, we\ncoin this approach as knowledge unlearning since\nwe are more focused on forgetting specific knowl-\nedge represented by sequences of tokens.\nZhou et al. (2022) focus on how forgetting can\nbe leveraged to improve the performance of the un-\nderlying model. They propose “forget-and-relearn”\nthat unifies existing iterative training algorithms\nby selectively removing undesirable information\nand re-learning good features, helping boost per-\nformance for the task of image classification and\nmulti-agent emergence communication. The under-\nlying assumption is that it is often easier to define\nand stop unwanted behavior than to teach good be-\nhavior. We also show this phenomenon in Section\n4 where we unintentionally find unlearning just a\nfew sequences of tokens sometimes boosts general\nLM capabilities.\n2.3 Memorization in Language Models\nPrevious work that explores to which extent LMs\nhave memorized their training data approach the\nphenomenon with two different viewpoints. Some\nwork view memorization of LMs simply as a threat\nto individual privacy (Carlini et al., 2021, 2022a;\nJagielski et al., 2022) and utilize metrics that quan-\ntify how much the LMs are susceptible to adver-\nsarial attacks. These metrics are mostly dependent\non the specific types of attacks such as the mem-\nbership inference attack (Shokri et al., 2017) and\nmeasure the privacy risks of LMs by quantifying\nthe success rate of these attacks. In our work, we\ninstead focus on more targeted extraction attacks.\nAnother line of work simply quantifies how\nmuch knowledge is accumulated and forgotten dur-\ning pretraining by extracting relational knowledge\nabout the world (Petroni et al., 2019; Lazaridou\net al., 2021; Jang et al., 2022b,a). This line of\nwork does not view memorization as a negative\ntrait, but as a positive one that can be leveraged to\nextract world knowledge from its implicit param-\neters and perform knowledge-intensive tasks such\nas question answering or training knowledgeable\nconversation agents.\nOur work is highly related to Jagielski et al.\n(2022)’s work where they also assert that forget-\nting can be a relaxed version of differential privacy.\nHowever, there are two main differences between\nour work and theirs. First, they only analyze for-\ngetting as a passive form of mitigating privacy,\nasserting that data seen early in large-scale training\nobtain privacy benefits, whereas we suggest a more\nactive form of forgetting. Second, they only show\nanalysis results with image classification and audio\ngeneration models while we specifically focus on\nlarge LMs.\n3 Knowledge Unlearning\n3.1 Methodology\nWe propose simply negating the original training\nobjective of minimizing the negative log-likelihood\nof the token sequences as our main method of\nknowledge unlearning in LMs. Specifically, given\na sequence of tokens x = ( x1,...,x T), our un-\nlearning training objective is simply maximizing\nthe following loss function:\nLUL(fθ,x) = −\nT∑\nt=1\nlog(pθ(xt|x<t)) (1)\nwhere x<t denotes the token sequence x =\n(x1,...,x t−1) and pθ(xt|x<t) denotes the condi-\ntional probability of predicting the next token to be\nxt when given x<t to an LM f with parameters θ.\n3.2 Quantifying Privacy Risks of Language\nModels\nIn this subsection, we introduce two metrics we\nuse to quantify the privacy risks given a specific\ntoken sequence and how we empirically define the\ntoken sequence to be forgotten. In this work, we do\nnot utilize metrics such as membership inference\nattack recall (Shokri et al., 2017) since we are not\ninterested in quantifying the general privacy risks\nof LMs, but instead the privacy risks on the specific\ntarget token sequences.\nExtraction Likelihood (EL) We first introduce\na new metric, EL. Given a sequence of tokens\nx = ( x1,...,x T) and an LM f with pre-trained\nparameters θ, we define EL to be as follows:\nELn(x) =\n∑T−n\nt=1 OVERLAP n(fθ(x<t),x≥t)\nT −n\n(2)\nOVERLAP n(a,b) =\n∑\nc∈ng(a) 1 {c∈ng(b)}\n|ng(a)|\n(3)\n14392\nwhere ng(·) denotes the list ofn-grams in the given\ntoken sequence and fθ(x<t) denotes the output\ntoken sequences from the LM fθ when given x<t\nas input that can have max lengths |x≥t|but may\nbe shorter when the EOS (end-of-sequence) token\nis generated beforehand.\nThe process of varying the prefix length |x<t|\ncan be seen as varying the strength of adversarial\nattacks. This is based on the assumption that the\nmore prior information is provided about the target\ntoken sequence, the easier the LM will be able to\nextract it. Overall, EL can be seen as estimating the\ngeneral extraction likelihood since we are measur-\ning the average success rate of varying extraction\nattacks quantified via getting the n-gram overlap of\ngenerated and target token sequences. While pre-\nvious metrics quantifying the privacy risks of LMs\nare dependent on specific adversarial attacks, this\ncharacteristic of EL allows it to quantify the general\nlikelihood of extraction without any dependency\non specific extraction attacks.\nWe regard n to be a hyper-parameter that can\nbe varied depending on the stringency of privacy\nstandards. The higher nis set, the stricter we set\nthe standard for a successful extraction attack.\nMemorization Accuracy (MA) First proposed\nby Tirumala et al. (2022), Memorization Accuracy\n(MA) is defined as follows:\nMA(x) =\n∑T−1\nt=1 1 {argmax(pθ(·|x<t)) = xt}\nT −1\n(4)\nMA quantifies how much fθ has memorized the\ngiven token sequences and can be used to analyze\nthe training dynamics of large LMs.\nEmpirical Definition of Forgetting By utilizing\nboth ELn and MA, we empirically define a specific\ntoken sequence x to be forgotten and is no longer\nsusceptible to extraction attacks when both of the\nfollowing conditions are met:\nELn(x) ≤ 1\n|D′|\n∑\nx′∈D′\nELn(x′) (5)\nand\nMA(x) ≤ 1\n|D′|\n∑\nx′∈D′\nMA(x′) (6)\nwhere D′represents a validation corpora not seen\nduring training. In other words, we define x to\nbe forgotten when the ELn(x) and MA(x) reach\na value that is lower than the average ELn and\nMA on token sequences that were not seen during\ntraining.\n4 Experiments\n4.1 Models, Datasets, and Configurations\nBaselines For the experiments, we use the GPT-\nNEO (125M, 1.3B, 2.7B) LMs (Black et al., 2021)\ninitially pretrained on all of the Pile corpora\n(825GB) (Gao et al., 2020), and the OPT (125M,\n1.3B, 2.7B) LMs (Zhang et al., 2022), pretrained\non a subset of the deduplicated version of the Pile\nas well as other corpora from different domains.\nFor the experiments, we perform unlearning the\nGPT-N EO LMs and quantify the privacy risks of\nthe target data compared to the OPT LMs to mea-\nsure how effective our proposed approach is in\ncontrast to deduplicating the training corpora be-\nfore pretraining the underlying LM Kandpal et al.\n(2022). We do not use the exact LMs from Kand-\npal et al. (2022) because the LMs were not open-\nsourced, and thus use the OPT LMs instead. We\nalso consider the Differential Privacy (DP) Decod-\ning (Majmudar et al., 2022) as one of the base-\nlines; This approach proposes a decoding strategy\nthat performs linear interpolation of the original\nlogits with the uniform distribution and performs\nnucleus sampling, which they theoretically show\nprovides DP guarantees. λis set as the linear in-\nterpolation weight where λ= 0 performs nucleus\nsampling from the uniform distribution and λ= 1\nperforms regular nucleus sampling, using the logits\nas weights during random sampling.\nTarget Data For the actual target data used to\nquantify the privacy risks of the LMs, we sample\ninstances from the Training Data Extraction Chal-\nlenge 2 where 15,000 examples (each are 200 token\nsequences long) from 16 different domains of the\nPile corpora that are identified to be somewhat easy-\nto-extract are provided. For our experiments, we\nrandomly sample s samples from the 15,000 ex-\namples and make the underlying LM forget the s\nsamples at once. As a default, we show the average\nresults of 5 random samplings of s samples for all\nof our experimental settings. We only provide the\naverage of the 5 samplings and do not separately\nreport the standard deviation. Instead, we provide\nthe results of each individual run in Appendix A.\n2https://github.com/google-research/lm-extraction-benchmark\n14393\nEvaluation Datasets Providing stronger privacy\nprotections for LMs may become meaningless if it\nrequires sacrificing their original capabilities. Thus,\nwhile quantifying the privacy risks of LMs, we\nalso quantify the original LM capabilities by eval-\nuating the LMs on 9 classification tasks quantify-\ning the general capabilities: Hellaswag (Zellers\net al., 2019) and Lambada (Paperno et al., 2016)\nbenchmarks to measure linguistic reasoning abil-\nities, Winogrande (Sakaguchi et al., 2021) and\nCOPA (Gordon et al., 2012) to measure common-\nsense reasoning abilities, and ARC-Easy (Clark\net al., 2018), ARC-Challenge (Clark et al., 2018),\nPiqa (Bisk et al., 2020), MathQA (Amini et al.,\n2019), PubmedQA (Jin et al., 2019) benchmarks\nto measure the scientific reasoning abilities. We\nalso evaluate on 4 dialogue tasks (Wizard of\nWikipedia (Dinan et al., 2019), Empathetic Di-\nalogues (Rashkin et al., 2019), Blended Skill\nTalk (Smith et al., 2020), and Wizard of Inter-\nnet (Komeili et al., 2022)) to evaluate the gener-\nation capabilities of the LMs. We use the test set\nfor Lambada and the validation set for the rest of\nthe datasets. We also show the results of measuring\nthe perplexity on the validation corpora of Pile and\nWikitext in Appendix B. We do not include mea-\nsuring perplexity as one of the main evaluations\nbecause perplexity might not be the most suitable\nmetric for quantifying general LM performance,\nespecially in the case of unlearning (further ex-\nplanation given in Appendix B. We evaluate DP\nDecoding only on the 4 dialogue tasks because the\ndecoding strategy cannot be applied for perform-\ning the classification tasks which is evaluated by\nutilizing a verbalizer.\nConfigurations For the learning rate, we set it to\n5e-5. We show the effect of varying learning rates\nin Appendix D. We use a constant learning rate\nscheduling throughout the run. We fix the global\nbatch size to be the same as s (how many samples\nare forgotten at once) because having global batch\nsizes smaller than sproved to degrade general LM\ncapabilities 3. For ELn, we set n=10 which means\nEL measures the extraction likelihood of extract-\ning n consecutive tokens of varying extraction at-\ntack 4. For calculating EL10 and MA, we use a\n3In Section 4.3, We show that s plays a critical role in determining how\nmuch the unlearning will degrade in general capabilities of the LM since s =\n128 shows to result in much degradation. Method to mitigate this is proposed\nin Section 4.3 as well.\n4We set the n value to 10 since we empirically consider an extraction to be\nsuccessful when 10 consecutive token sequences are successfully generated by\nthe LM. We show varying then with values from [5,10,20,40] in Appendix I.\nModel (Size) EL10(%) MA(%)\nThreshold Threshold\nGPT-N EO (125M) 4.99 29 .94\nGPT-N EO (1.3B) 5.68 33 .27\nGPT-N EO (2.7B) 5.53 34 .02\nTable 1: Forgetting Threshold for GPT-N EO LMs\nnaïve greedy decoding strategy. We set both the\ndropout and weight decay rates to 0. Lastly, while\nwe provide a guideline of empirically deciding a\nsingle token sequence to be forgotten in Section\n3.2, for considering a chunk of stoken sequences\nto be forgotten, we use the average EL10 and MA\nas an approximation of the individual EL10 and\nMA.\n4.2 Main Experiments\nForgetting Threshold First, we show how we\nget the Forgetting Threshold for EL10 and MA, the\nvalues where we consider the token sequence to be\nforgotten and unsusceptible from extraction attacks,\nfor all model sizes of GPT-N EO LMs in Table\n1. For D′, we perform weighted sampling (same\ndomain distribution as the Pile training corpora)\nof 10,000 instances each with token lengths 200\nfrom the Pile validation corpora, and measure the\naverage EL10 and MA (Equation 5-6), which are\nempirically set as the Forgetting Threshold values.\nMain Results Table 2 shows the main results of\nperforming unlearning on LMs of varying sizes\nand the baselines. While we provide the average\nperformances of the 5 random samplings in Table\n2, we provide each individual runs in Appendix A\nfor reference.\nWe highlight five main observations regarding\nthe results. (1) OPT LMs show a much lower\nEL10 and MA than GPT-N EO LMs, confirming\nthat deduplicating the pretraining corpora is in-\ndeed helpful for mitigating privacy risks. (2) +\nDPD + enables effective protection against extrac-\ntion attacks demonstrated via the lowest EL and\nMA score; however, it brings severe degradation\nof generation capabilities measured via the average\nF1 score of the 4 dialogue generation tasks. (3)\n+ UL + results in severe degradation of both clas-\nsification and dialogue tasks for the 125M, only\nsevere degradation of dialogue tasks for 1.3B LM\nwhile for the 2.7B LMs, it enables retaining most\nof its previous capabilities. (4) While the LMs\nscale to larger sizes, it takes fewer epochs for the\n14394\nModel # EL10 MA Classification Avg. Dialogue Avg. EpochParams (%) ↓ (%) ↓ (ACC) ↑ (F1) ↑\nOPT 125M 8.6 52.9 42.4 10.2 -\nNEO 125M 30.9 77.4 43.4 9.4 -\n+ DPD + 125M 0.0 27.4 N/A 7.3 -\n+ UL 125M 3.7 50.1 42.6 8.0 11.0\n+ UL+ 125M 1.0 27.4 39.9 2.6 17.2\nOPT 1.3B 23.3 67.1 50.6 12.4 -\nNEO 1.3B 67.6 92.2 49.8 11.5 -\n+ DPD + 1.3B 0.0 21.4 N/A 7.1 -\n+ UL 1.3B 11.0 62.2 49.7 11.6 8.0\n+ UL+ 1.3B 1.9 30.4 49.7 8.5 13.8\nOPT 2.7B 25.6 69.2 52.7 12.9 -\nNEO 2.7B 70.4 93.4 52.3 11.5 -\n+ DPD + 2.7B 0.0 24.2 N/A 6.9 -\n+ UL 2.7B 13.0 66.0 52.3 12.5 5.4\n+ UL+ 2.7B 1.6 31.0 51.9 11.1 10.8\nTable 2: Main Results showing the average of 5 random sampling of s = 32 (forgetting 32 samples at once).\nOPT represents the LM with deduplication applied. NEO denotes the initial GPT-N EO LM, + DPD + represents\napplying the DP Decoding strategy by varying the λto match the forgetting criteria, + UL represents performing\nunlearning on the initial NEO until it provides stronger security for the target sequences thanOPT, + UL+ represents\nperforming unlearning on NEO until target sequences match the forgetting criteria, Classification Avg. denotes the\naverage accuracy of the 9 classification datasets, and Dialogue Avg. denotes the average F1 score of the 4 dialogue\ndatasets. The best comparable performances are bolded and second best underlined.\ntarget sequences to be forgotten. Together with (3),\nthis implies that larger LMs are strong unlearners.\n(5) While + UL+ provides stronger privacy protec-\ntion than OPT without sacrificing its performance\nfrom NEO for the 2.7B LM, it is much more com-\nputationally efficient (3,500,000x) than re-training\nthe underlying LM, which is required for all data\npreprocessing approaches 5.\nOverall, results show unlearning to be an effec-\ntive approach to providing strong privacy protec-\ntion while retaining and sometimes even improving\ngeneral LM capabilities.\nSequential Unlearning is more Stable than\nBatch Unlearning We show the effect of varying\ns(the # of data instances to be forgotten at once)\nin Figure 2 across model scales. We denote this\napproach as batch unlearning. As shown by the\ns = 128 results, it is harder to forget more sam-\nples at once, resulting in substantial degradation of\naverage LM performance regardless of how large\nthe LM is. Since s ≤32 does not show much\n5Computational efficiency is measured via FLOPs which is calculated by\n(6 × Total Training Tokens× Parameter Size) as in Brown et al. (2020). FLOPs\nfor OPT LMs were estimated using information from Zhang et al. (2022). We\nprovide the FLOPs for the methods in Appendix C.\ndegradation, we explore if sequentially unlearning\ncan be a solution. In Figure 2b, we show the result\nof dividing the 128 samples into 4 chunks of 32\nand performing sequential unlearning; we unlearn\neach chunk at a time until the chunk reaches the\nforgetting threshold. Surprisingly, as shown by the\nperformance gap at s = 128 between the dotted\nlines (the s= 128 performance of Figure 2a) and\nstraight lines, the end result is vastly different even\nthough exactly the same instances were forgotten.\nSequential unlearning shows almost no degrada-\ntion of average LM performance. In Appendix H,\nwe show that chunks once forgotten stay forgot-\nten and that later chunks are forgotten much faster\ncompared to the initial chunk. This result hints\nat the generalization of unlearning, which we do\nnot further explore in the scope of this work. The\nresult also suggests that knowledge unlearning can\nbe continually applied to LMs when needed.\n4.3 Analysis of Knowledge Unlearning\nTo measure why some instances are harder to for-\nget, we perform 5 random samplings of s = 8\nfrom 8 different domains from the Training Data\n14395\n0 1 4 8 32 128\n# of Samples Forgotten at Once\n35\n40\n45\n50Avg. Performance\n125M\n1.3B\n2.7B\n(a) Batch Unlearning\n0 32 64 96 128\nTotal # of Samples Sequentially Forgotten\n35\n40\n45\n50Avg. Performance\n125M\n1.3B\n2.7B (b) Sequential Unlearning\nFigure 2: Average LM performance on the 9 classification benchmarks when varying the total number of samples\nforgotten at once is shown in (a) and the average LM performances when the 128 samples are divided into 4 chunks\nand are forgotten sequentially is shown in (b). The lines denote the average performances of 5 random samplings\nand the standard deviation is shown as the shaded regions. The dotted lines in (b) denotes the s= 128 performance\nin (a) for comparison purposes.\nDomains Initial Final Hella. Lamba. Wino. COPA ARC-E ARC-C Piqa MathQ PubQ Avg.\nEL10 EL10 (ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) (ACC )\nINITIAL - - 37.0 57.4 54.9 70.0 56.6 25.8 70.4 21.9 53.8 49.8 (0.0)\nFREELAW 60.4 12.1 37.2 52.2 53.9 68.4 55.5 26.2 70.1 21.7 53.5 48.7 (-1.1)\nGIT. ( CODE ) 63.9 0.6 37.3 53.4 54.4 69.2 56.3 26.0 69.9 21.5 49.8 48.7 (-1.1)\nGIT. ( LICENSE ) 75.8 0.0 37.1 52.0 54.2 69.0 56.4 26.4 70.1 21.8 51.8 48.8 (-1.0)\nENRON EMAILS 77.3 0.0 36.9 57.2 54.8 68.4 55.8 26.3 69.8 21.8 53.1 49.4 (-0.4)\nBOOKS 3 70.2 0.0 36.4 49.5 54.2 70.8 55.6 25.5 69.9 21.7 47.4 47.9 (-1.9)\nPILE CC 67.8 0.0 35.7 45.9 53.8 70.4 54.2 26.9 69.7 21.8 52.0 47.8 (-2.0)\nUSPTO BACK . 59.4 0.0 33.7 44.7 53.5 67.0 45.9 24.0 67.0 21.5 50.3 45.3 (-4.5)\nPUBMED CENT . 71.8 0.0 36.5 44.5 54.1 69.6 55.6 24.8 70.0 21.9 46.4 47.0 (-2.8)\nTable 3: Unlearning GPT-N EO 1.3B on token sequences sampled from 8 different domains. We fix the epoch to 10,\nset s= 8, and show the result of the average of 5 random samplings. Italicized () denotes the ∆ from INITIAL .\nExtraction Challenge 6 and perform unlearning on\nthe GPT-N EO 1.3B LM. We also show the re-\nsults of each individual run in Appendix A. As\nshown in Table 3, despite undergoing the same\nnumber of token updates (10 epochs of unlearn-\ning), different domains result in vastly different\noutcomes; ENRON EMAILS results in the average\nLM performance degradation of only -0.4% while\nUSPTO BACKGROUNDS results in -4.5% degrada-\ntion. Furthermore, the final EL10 varies depend-\ning on the domain, suggesting that some domains\n(e.g., FREELAW ) are harder to forget than others.\nLastly, domains that are more structured, which\nmeans the data consists of some kind of patterns\nsuch as a list of emails (ENRON EMAILS ) or code\n(GITHUB (CODE )), seem to result in less degrada-\ntion of LM performance in contrast to domains that\nare more unstructured, which means the data con-\nsist of mostly raw English text such as a review\nfor journal submission (PUBMED CENTRAL ). For\nfurther analysis, we provide examples from each\n6https://github.com/google-research/lm-extraction-benchmark\ndomain in Appendix F as well as the individual task\nperformance change during knowledge unlearning\nin Appendix E.\n5 Conclusion\nIn this paper, we propose knowledge unlearning\nas a method for mitigating privacy risks in LMs\nthat provides a strong privacy protection with lit-\ntle to no degradation of general LM capabilities\nmeasured by evaluating on 9 common LM classi-\nfication benchmarks and 4 dialogue benchmarks\nfor the larger sized LMs. As large LMs expand\ntheir use cases, potentially affecting the daily lives\nof people, the research community should make\nsure that the privacy of individuals is not violated\nintentionally or unintentionally by the knowledge\nstored in the implicit parameters of these models.\nSince it is inherently impossible to prevent and\npredict all future privacy concerns prior to pretrain-\ning the LM, we suggest the community consider\nknowledge unlearning for ensuring privacy upon\nindividuals’ requests post hoc pretraining.\n14396\n6 Limitations\nWhile we provide a privacy guarantee through un-\nlearning, our Forgetting Threshold is dependent\non which data samples are chosen as D′. Further-\nmore, varying the prefix length can be seen as a\nnaïve way of varying the strength of the extrac-\ntion attacks. In a real-world scenario, extraction\nattacks may be more complicated and may require\nother prevention methods. Also, we could not di-\nrectly compare our approach with a Differential\nPrivacy (DP) (Anil et al., 2021) approach because\nthere are no open-sourced LMs pretrained with\na DP algorithm. We could not replicate the pre-\ntrainig phase because of the heavy computational\nresources needed to pretrain an LM with DP which\nis estimated to require thousands of GPU hours.\nWe leave this comparison for future work. Finally,\na recent work (Carlini et al., 2022b) has suggested\nthat machine unlearning (for the vision domain)\ncan bring negative effects harming the privacy of\nother users. Future work should explore this phe-\nnomenon in the setting of performing unlearning\non large LMs as well.\n7 Acknowledgements\nThis work was partly supported by Institute of In-\nformation communications Technology Planning\nEvaluation (IITP) grant funded by the Korea gov-\nernment (MSIT) (No.2022-0-00113, Developing\na Sustainable Collaborative Multi-modal Lifelong\nLearning Framework, 80%; No.2021-0-02068, Ar-\ntificial Intelligence Innovation Hub, 20%).\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security,\npages 308–318.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2357–2367, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar,\nand Pasin Manurangsi. 2021. Large-scale differen-\ntially private bert. arXiv preprint arXiv:2108.01624.\nTuomas Aura, Thomas A Kuhn, and Michael Roe. 2006.\nScanning electronic documents for personally iden-\ntifiable information. In Proceedings of the 5th ACM\nworkshop on Privacy in electronic society, pages 41–\n50.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nIf you use this software, please cite it using these\nmetadata, 58.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A\nChoquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\nZhang, David Lie, and Nicolas Papernot. 2021. Ma-\nchine unlearning. In 2021 IEEE Symposium on Secu-\nrity and Privacy (SP), pages 141–159. IEEE.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? arXiv preprint arXiv:2202.05520.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYinzhi Cao and Junfeng Yang. 2015. Towards making\nsystems forget with machine unlearning. In 2015\nIEEE Symposium on Security and Privacy , pages\n463–480. IEEE.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022a. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nNicholas Carlini, Matthew Jagielski, Chiyuan Zhang,\nNicolas Papernot, Andreas Terzis, and Florian\nTramer. 2022b. The privacy onion effect: Memoriza-\ntion is relative. In Advances in Neural Information\nProcessing Systems.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv,\nabs/1803.05457.\n14397\nFranck Dernoncourt, Ji Young Lee, Ozlem Uzuner,\nand Peter Szolovits. 2017. De-identification of pa-\ntient notes with recurrent neural networks. Journal\nof the American Medical Informatics Association ,\n24(3):596–606.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In International Conference on Learning\nRepresentations.\nCynthia Dwork. 2008. Differential privacy: A survey\nof results. In International conference on theory and\napplications of models of computation, pages 1–19.\nSpringer.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and\nAdam Smith. 2006. Calibrating noise to sensitivity\nin private data analysis. In Theory of cryptography\nconference, pages 265–284. Springer.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAntonio Ginart, Melody Guan, Gregory Valiant, and\nJames Y Zou. 2019. Making ai forget you: Data\ndeletion in machine learning. Advances in neural\ninformation processing systems, 32.\nAditya Golatkar, Alessandro Achille, and Stefano\nSoatto. 2020. Eternal sunshine of the spotless net: Se-\nlective forgetting in deep networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9304–9312.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roem-\nmele. 2012. SemEval-2012 task 7: Choice of plau-\nsible alternatives: An evaluation of commonsense\ncausal reasoning. In *SEM 2012: The First Joint\nConference on Lexical and Computational Seman-\ntics – Volume 1: Proceedings of the main conference\nand the shared task, and Volume 2: Proceedings of\nthe Sixth International Workshop on Semantic Eval-\nuation (SemEval 2012) , pages 394–398, Montréal,\nCanada. Association for Computational Linguistics.\nLaura Graves, Vineel Nagisetty, and Vijay Ganesh.\n2021. Amnesiac machine learning. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 35, pages 11516–11524.\nMelissa Heikkilä. 2022. What does gpt-3 \"know\" about\nme?\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.\n2022. Are large pre-trained language models leak-\ning your personal information? arXiv preprint\narXiv:2205.12628.\nMatthew Jagielski, Om Thakkar, Florian Tramèr,\nDaphne Ippolito, Katherine Lee, Nicholas Carlini,\nEric Wallace, Shuang Song, Abhradeep Thakurta,\nNicolas Papernot, et al. 2022. Measuring forget-\nting of memorized training examples. arXiv preprint\narXiv:2207.00099.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022a. Temporalwiki: A lifelong\nbenchmark for training and evaluating ever-evolving\nlanguage models. arXiv preprint arXiv:2204.14211.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun KIM, Stanley Jungkyu\nChoi, and Minjoon Seo. 2022b. Towards continual\nknowledge learning of language models. In Interna-\ntional Conference on Learning Representations.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. In Pro-\nceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th In-\nternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 2567–2577.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks in\nlanguage models. arXiv preprint arXiv:2202.06539.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\nAngeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,\nDevang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d’Autume, Tomas Ko-\ncisky, Sebastian Ruder, et al. 2021. Mind the gap:\nAssessing temporal generalization in neural language\nmodels. Advances in Neural Information Processing\nSystems, 34:29348–29363.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424–8445, Dublin, Ireland. Association for\nComputational Linguistics.\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav Gold-\nberg, and Byron C. Wallace. 2021. Does bert pre-\ntrained on clinical notes reveal sensitive data? In\nNAACL-HLT, pages 946–959.\nXuechen Li, Florian Tramer, Percy Liang, and Tatsunori\nHashimoto. 2022. Large language models can be\nstrong differentially private learners. In International\nConference on Learning Representations.\nPierre Lison, Ildikó Pilán, David Sanchez, Montser-\nrat Batet, and Lilja Øvrelid. 2021. Anonymisation\nmodels for text data: State of the art, challenges and\nfuture directions. In Proceedings of the 59th Annual\n14398\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4188–4203, Online. Association for\nComputational Linguistics.\nJimit Majmudar, Christophe Dupuy, Charith Peris, Sami\nSmaili, Rahul Gupta, and Richard Zemel. 2022. Dif-\nferentially private decoding in large language models.\narXiv preprint arXiv:2205.13621.\nAlessandro Mantelero. 2013. The eu proposal for a\ngeneral data protection regulation and the roots of\nthe ‘right to be forgotten’. Computer Law & Security\nReview, 29(3):229–235.\nRonak Mehta, Sourav Pal, Vikas Singh, and Sathya N\nRavi. 2022. Deep unlearning via randomized con-\nditionally independent hessians. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10422–10431.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525–1534, Berlin, Germany.\nAssociation for Computational Linguistics.\nJasmine Park. 2021. South korea: The first case where\nthe personal information protection act was applied\nto an ai system.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381, Florence, Italy. Association for\nComputational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99–106.\nReza Shokri, Marco Stronati, Congzheng Song, and Vi-\ntaly Shmatikov. 2017. Membership inference attacks\nagainst machine learning models. In 2017 IEEE sym-\nposium on security and privacy (SP) , pages 3–18.\nIEEE.\nEric Michael Smith, Mary Williamson, Kurt Shuster,\nJason Weston, and Y-Lan Boureau. 2020. Can you\nput it all together: Evaluating conversational agents’\nability to blend skills. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2021–2030, Online. Association\nfor Computational Linguistics.\nKushal Tirumala, Aram H Markosyan, Luke Zettle-\nmoyer, and Armen Aghajanyan. 2022. Memoriza-\ntion without overfitting: Analyzing the training dy-\nnamics of large language models. arXiv preprint\narXiv:2205.10770.\nEduard Fosch Villaronga, Peter Kieseberg, and Tiffany\nLi. 2018. Humans forget, machines remember: Arti-\nficial intelligence and the right to be forgotten. Com-\nputer Law & Security Review, 34(2):304–313.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\nSergey Yekhanin, and Huishuai Zhang. 2022. Differ-\nentially private fine-tuning of language models. In\nInternational Conference on Learning Representa-\ntions.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint\narXiv:1905.07830.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nHattie Zhou, Ankit Vani, Hugo Larochelle, and Aaron\nCourville. 2022. Fortuitous forgetting in connec-\ntionist networks. In International Conference on\nLearning Representations.\nA Full Results\nWe provide all of the results for the 5 random sam-\nplings for our main experimental setting in Table 4\nand the full results for the domain analysis setting\nin Table 5. We also provide the evaluation of the\n4 dialogue tasks for s= 32 for all model sizes in\nTable 6.\nB Measuring Pile and Wikitext Perplexity\nTable 7 shows the results of measuring perplexity\non 500 samples from the validation set of Pile and\nWikitext corpora on the LMs from the main exper-\nimental setting (Table 2). Results show that LMs\nthat underwent knowledge unlearning show higher\nperplexity while the main experimental table (Ta-\nble 2) does not show degradation of performance\non 9 different LM benchmarks. We believe the\ndiscrepancy to be due to the inherent attributes of\nperforming unlearning: since we are doing gradient\nascent, we are likely softening the probability to\ngenerate each token from the vocabulary, giving\nit a more uniform distribution that will inevitably\nresult in a higher perplexity. However, since it does\n14399\nTable 4: All of the individual runs for the Main Results\nModel (s) # EL10 MA Hella. Lamba. Wino. COPA ARC-E ARC-C Piqa MathQ PubQ Avg. EpochParams (%) ↓ (%) ↓ (ACC) (ACC) (ACC) (ACC) (ACC) (ACC) (ACC) (ACC) (ACC) (ACC)\nNEO 125M 30.9 77.4 28.2 37.6 51.8 62.0 45.6 22.0 63.3 22.5 57.6 43.4 -\n∆ - - - +0.2 +8.0 +1.9 +5.0 +0.0 +2.2 +0.0 +0.3 +0.0 +2.0 -\nNEO + UL+ (s = 1)\n125M 3.1 28.1 28.1 41.0 52.5 62.0 43.2 21.0 63.0 22.8 57.6 43.5 14.0\n125M 0.0 27.6 28.1 24.9 50.8 67.0 42.3 23.7 62.8 21.9 57.6 42.1 10.0\n125M 0.0 27.1 28.1 42.1 52.5 63.0 44.1 20.3 62.6 22.5 57.6 43.7 5.0\n125M 0.0 25.6 28.2 44.9 52.0 62.0 41.8 21.4 62.6 22.2 57.6 43.6 11.0\n125M 0.0 28.1 28.4 33.9 51.5 66.0 44.8 21.7 62.8 22.3 57.6 43.2 10.0\nNEO + UL+ (s = 4)\n125M 0.9 28.8 27.8 44.1 51.9 52.0 37.4 19.7 60.5 22.3 57.6 41.5 16.0\n125M 0.0 28.6 27.4 2.5 49.4 59.0 38.6 23.1 60.5 21.2 43.8 36.2 19.0\n125M 3.6 28.8 27.7 33.4 51.8 55.0 37.7 21.0 61.0 22.3 57.6 40.8 20.0\n125M 2.6 28.9 27.6 29.9 52.4 50.0 36.5 19.0 60.3 22.2 57.6 39.5 18.0\n125M 0.0 28.4 27.6 6.7 49.7 61.0 42.5 22.7 61.0 21.4 50.6 38.1 16.0\nNEO + UL+ (s = 8)\n125M 0.0 28.5 27.6 35.0 51.8 51.0 37.6 18.0 60.1 22.4 57.6 40.1 16.0\n125M 2.2 28.1 27.7 5.4 49.6 62.0 40.6 21.0 61.2 21.8 52.4 38.0 19.0\n125M 0.3 29.6 28.0 41.2 52.2 55.0 40.2 21.4 61.0 21.9 57.6 42.0 18.0\n125M 5.0 25.3 27.4 1.3 49.6 65.0 37.6 24.4 59.2 21.2 33.8 35.5 23.0\n125M 0.0 28.2 27.9 5.3 50.5 61.0 41.6 22.4 60.7 21.5 51.4 38.0 18.0\nNEO + UL+ (s = 32)\n125M 0.3 28.4 27.2 42.3 53.7 56.0 38.1 21.0 59.7 22.4 57.6 42.0 20.0\n125M 0.8 27.1 27.0 17.1 52.4 53.0 34.0 20.0 59.8 21.5 57.6 38.0 18.0\n125M 0.2 24.1 27.3 45.6 51.9 50.0 38.6 20.7 59.6 22.6 57.6 41.5 13.0\n125M 3.0 28.7 27.5 2.6 49.2 59.0 37.7 21.4 58.4 20.9 46.8 35.9 20.0\n125M 0.7 28.5 27.3 44.5 53.0 54.0 39.0 20.3 59.5 22.5 57.6 42.0 15.0\nNEO + UL+ (s = 128)\n125M 1.3 28.1 27.1 4.6 50.5 58.0 37.9 21.3 57.5 21.4 47.8 36.2 16.0\n125M 3.1 27.5 26.9 1.8 50.5 60.0 36.4 22.3 56.6 21.2 41.8 35.3 18.0\n125M 3.9 26.7 27.0 3.9 50.9 59.0 35.2 21.3 56.0 21.3 49.6 36.0 17.0\n125M 2.4 26.6 26.9 2.7 50.2 56.0 35.9 22.3 57.2 21.2 43.8 35.1 16.0\n125M 3.8 27.3 27.0 6.4 50.9 57.0 37.3 21.3 57.2 21.2 52.0 36.7 17.0\nNEO 1.3B 67.6 92.2 37.0 57.4 54.8 70.0 56.6 25.8 70.4 21.9 53.8 49.8 -\n∆ - - - +0.4 +10.1 +2.1 +2.0 +1.1 +3.4 +0.3 +0.4 +3.8 +2.6 -\nNEO + UL+ (s = 1)\n1.3B 0.0 27.6 36.8 52.1 54.7 72.0 55.9 27.8 69.7 21.5 53.0 49.3 9.0\n1.3B 0.0 30.2 36.6 54.6 54.9 69.0 55.4 26.8 70.7 21.7 53.4 49.2 6.0\n1.3B 0.0 29.7 36.7 58.2 55.4 70.0 56.1 25.4 69.9 22.0 53.2 49.7 4.0\n1.3B 0.0 32.2 37.1 52.4 53.7 68.0 56.1 24.4 70.1 21.8 54.2 48.6 8.0\n1.3B 0.0 27.6 37.3 60.1 55.6 70.0 57.5 25.1 70.0 21.7 55.2 50.3 10.0\nNEO + UL+ (s = 4)\n1.3B 0.0 30.3 37.3 48.3 54.4 70.0 55.0 29.2 69.9 20.6 56.0 49.0 12.0\n1.3B 0.0 29.7 36.8 49.4 53.4 69.0 55.2 26.8 70.6 21.4 52.8 48.4 9.0\n1.3B 1.0 29.2 36.8 51.3 54.9 70.0 55.2 26.8 70.3 21.5 54.0 49.0 10.0\n1.3B 4.8 31.4 37.2 59.2 54.8 71.0 54.9 25.8 69.5 21.9 50.2 49.4 10.0\n1.3B 1.7 31.8 37.0 58.4 54.4 71.0 57.7 24.7 70.2 22.0 54.0 49.9 9.0\nNEO + UL+ (s = 8)\n1.3B 0.3 29.7 37.1 66.5 54.5 70.0 52.0 26.8 69.4 21.7 56.8 50.5 13.0\n1.3B 1.9 29.5 36.8 43.0 53.1 71.0 51.3 27.5 70.4 21.0 42.4 46.3 13.0\n1.3B 0.2 26.2 37.2 47.3 54.2 72.0 55.2 25.8 70.4 21.8 54.8 48.7 12.0\n1.3B 3.1 32.0 37.4 57.6 54.3 70.0 56.1 26.8 69.8 21.5 54.8 49.8 14.0\n1.3B 1.4 32.0 37.1 57.4 54.5 71.0 57.0 26.1 70.0 21.9 54.2 49.9 11.0\nNEO + UL+ (s = 32)\n1.3B 0.7 33.0 36.5 63.2 55.9 70.0 52.4 25.1 69.7 21.8 55.4 50.0 13.0\n1.3B 1.7 29.8 36.7 50.9 53.5 71.0 56.3 27.8 70.7 22.0 39.4 47.6 14.0\n1.3B 0.7 28.4 37.0 64.8 56.9 69.0 54.3 26.4 69.1 21.9 55.8 50.6 13.0\n1.3B 4.2 31.2 35.8 67.5 55.3 67.0 51.5 25.4 68.1 21.3 56.6 49.8 14.0\n1.3B 2.1 29.5 35.8 63.9 55.7 70.0 54.1 26.4 69.5 22.3 56.8 50.5 15.0\nNEO + UL+ (s = 128)\n1.3B 0.4 24.5 31.1 54.2 55.2 69.0 53.2 24.7 66.1 21.9 56.4 48.0 6.0\n1.3B 4.9 19.8 27.8 2.2 54.8 69.0 50.9 23.3 57.9 21.8 55.8 40.4 8.0\n1.3B 4.2 30.2 30.6 41.6 55.1 69.0 54.4 26.0 63.8 22.1 55.0 46.4 6.0\n1.3B 2.9 23.6 27.6 8.8 52.9 68.0 44.5 18.9 57.7 21.6 57.4 39.7 9.0\n1.3B 1.3 23.1 28.5 48.6 55.5 69.0 48.8 21.6 62.3 22.2 57.6 46.0 8.0\nNEO 2.7B 70.4 93.4 40.8 62.2 56.4 75.0 59.6 25.4 73.0 21.4 57.0 52.3 -\n∆ - - - +0.8 +7.9 +1.0 +0.0 +1.5 +4.3 +0.3 +1.1 +1.0 +2.0 -\nNEO + UL+ (s = 1)\n2.7B 0.0 3.0 40.8 62.2 56.6 72.0 55.7 26.4 73.1 21.8 57.6 51.8 10.0\n2.7B 0.0 23.6 40.5 56.8 54.4 74.0 59.6 26.1 72.8 21.3 56.6 51.3 8.0\n2.7B 0.0 27.6 40.6 62.5 57.0 75.0 59.1 24.7 73.0 21.5 56.6 52.2 6.0\n2.7B 0.0 20.6 40.5 60.3 55.8 74.0 58.9 25.8 73.0 21.7 57.2 51.9 10.0\n2.7B 0.0 29.7 40.6 62.2 56.4 72.0 58.0 27.1 72.2 21.2 57.4 51.9 9.0\nNEO + UL+ (s = 4)\n2.7B 0.4 22.6 41.5 60.0 54.9 72.0 55.0 26.4 69.9 21.3 57.8 51.0 12.0\n2.7B 0.0 30.0 41.6 46.5 53.4 71.0 55.6 25.1 72.0 21.3 57.2 49.3 9.0\n2.7B 0.7 23.7 40.4 59.7 54.9 74.0 58.7 23.7 72.5 20.8 57.4 51.3 9.0\n2.7B 3.2 32.4 41.2 67.2 56.0 73.0 57.3 28.1 73.3 22.3 57.2 52.8 8.0\n2.7B 0.2 31.9 40.3 61.2 55.7 74.0 60.0 27.5 72.0 21.4 57.2 52.1 10.0\nNEO + UL+ (s = 8)\n2.7B 0.3 29.5 41.2 64.6 55.4 71.0 52.9 27.1 69.5 21.7 58.0 51.3 10.0\n2.7B 2.1 26.4 40.6 48.7 52.9 67.0 55.0 25.8 72.1 21.8 57.2 49.0 11.0\n2.7B 0.5 31.2 41.1 54.1 55.0 74.0 59.3 25.1 72.5 22.1 57.4 51.2 11.0\n2.7B 1.9 33.8 40.7 65.7 57.4 72.0 58.4 27.1 72.6 21.9 57.0 52.5 8.0\n2.7B 0.0 20.4 40.0 60.7 55.8 73.0 60.1 28.5 72.5 21.5 57.2 52.2 11.0\nNEO + UL+ (s = 32)\n2.7B 0.6 31.7 40.8 68.2 56.1 68.0 54.4 28.0 71.9 21.4 57.0 51.8 11.0\n2.7B 1.1 32.4 40.9 56.9 55.6 69.0 58.1 26.7 71.8 22.1 56.8 50.9 10.0\n2.7B 1.2 29.0 41.5 65.8 56.9 68.0 59.3 27.0 72.0 22.3 57.8 52.3 11.0\n2.7B 3.4 29.9 39.7 70.1 57.7 68.0 54.8 29.7 71.6 22.0 57.6 52.4 11.0\n2.7B 1.9 31.9 41.4 61.6 56.6 73.0 61.1 26.4 72.7 21.7 57.0 52.4 11.0\nNEO + UL+ (s = 128)\n2.7B 0.4 31.5 35.3 64.2 56.8 68.3 51.8 26.7 70.2 21.9 56.7 50.2 10.0\n2.7B 3.8 16.5 26.0 0.4 51.6 57.7 29.0 16.6 54.2 20.0 57.9 34.8 10.0\n2.7B 0.6 31.4 34.9 58.9 55.2 69.2 54.8 24.7 70.0 22.5 57.7 49.8 9.0\n2.7B 2.2 31.1 31.3 22.9 50.6 62.5 40.0 18.2 60.8 21.3 40.9 38.7 8.0\n2.7B 4.7 29.0 33.5 56.5 55.0 66.3 51.9 23.6 68.6 22.4 57.7 48.4 9.0\n14400\nTable 5: All of the individual runs for the Domain Analysis Results for GPT-N EO 1.3B LM.\nDomains Initial Final Hella. Lamba. Wino. COPA ARC-E ARC-C Piqa MathQ PubQ Avg.\nEL10 EL10 (ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) ( ACC ) (ACC )\nINITIAL - - 37.0 57.4 54.9 70.0 56.6 25.8 70.4 21.9 53.8 49.8\nFREELAW\n64.6 4.8 37.3 53.5 54.1 68.0 57.5 27.1 70.5 21.5 54.0 49.3\n52.0 2.4 37.3 62.9 54.2 67.0 52.9 26.1 69.2 21.5 54.4 49.5\n60.6 15.2 36.8 42.0 54.5 67.0 56.6 25.1 70.1 21.7 51.4 47.2\n55.2 13.8 37.3 51.4 53.5 69.0 55.4 26.8 70.5 21.9 54.6 48.9\n69.5 24.1 37.4 51.4 53.2 71.0 54.9 26.1 70.0 21.8 53.0 48.7\nGITHUB (CODE )\n67.0 1.2 37.3 51.1 54.1 71.0 57.3 27.1 70.1 21.3 41.2 47.8\n56.7 0.3 37.1 49.9 54.9 68.0 56.1 26.4 69.1 21.4 48.4 47.9\n62.0 0.2 37.2 50.2 54.2 68.0 56.6 25.8 70.5 21.8 54.4 48.7\n60.4 1.1 37.5 59.7 54.7 68.0 55.9 25.4 70.1 21.9 53.8 49.7\n73.6 0.0 37.3 55.9 54.1 71.0 55.4 25.4 69.9 21.2 51.4 49.1\nGITHUB (LICENSE )\n87.5 0.2 37.5 57.4 54.5 68.0 56.8 26.4 70.1 21.8 53.8 49.6\n74.3 0.0 37.3 48.9 54.1 70.0 57.1 27.1 70.7 21.7 48.4 48.4\n70.7 0.0 36.4 40.6 53.1 70.0 55.2 25.4 70.2 21.8 49.0 46.9\n74.8 0.0 37.3 60.3 54.8 69.0 55.9 27.1 70.0 21.5 55.6 50.2\n71.8 0.0 37.0 52.6 54.3 68.0 56.8 26.1 69.5 22.0 52.2 48.7\nENRON EMAILS\n81.6 0.0 36.4 59.8 55.2 69.0 53.6 27.5 69.0 21.9 54.8 49.7\n70.3 0.0 37.2 54.9 54.5 68.0 57.5 25.4 70.1 22.4 51.8 49.1\n74.2 0.0 37.1 56.3 55.0 68.0 55.6 25.1 69.8 21.6 54.2 49.2\n83.9 0.0 36.7 55.2 54.8 69.0 55.9 25.4 70.4 21.7 52.2 49.0\n76.8 0.0 36.9 60.0 54.6 68.0 56.4 28.1 69.9 21.5 52.4 49.7\nBOOKS 3\n59.7 0.0 36.2 39.4 53.9 72.0 55.2 24.4 69.9 21.9 50.0 47.0\n65.4 0.0 35.9 65.2 55.7 67.0 53.3 25.1 69.9 21.6 55.8 49.9\n71.7 0.0 37.1 47.4 54.6 74.0 57.0 26.8 69.8 21.7 44.2 48.1\n74.7 0.0 36.4 40.7 53.4 70.0 55.7 25.4 69.6 21.6 41.2 46.0\n79.5 0.0 36.7 54.9 53.6 71.0 56.6 25.8 70.2 21.8 46.0 48.5\nPILE CC\n74.9 0.0 35.3 30.7 53.0 68.0 55.2 26.4 69.9 22.1 50.4 45.7\n68.0 0.0 36.3 45.9 53.4 72.0 55.6 27.1 69.6 21.7 51.4 48.1\n71.6 0.0 36.3 48.9 52.9 70.0 55.9 26.4 70.2 21.9 51.8 48.3\n57.8 0.0 34.0 66.3 55.7 69.0 49.9 26.1 69.0 21.4 57.4 49.9\n66.6 0.0 36.4 37.7 54.0 73.0 54.5 28.1 69.9 22.1 49.2 47.2\nUSPTO BACKGROUNDS\n53.7 0.0 30.7 48.4 53.4 68.0 39.0 22.0 64.2 20.7 55.2 44.6\n56.7 0.0 31.0 19.4 50.6 69.0 36.9 24.1 63.3 21.2 33.4 38.8\n64.9 0.0 36.0 51.4 54.1 68.0 50.8 24.4 70.0 22.1 56.6 48.2\n54.6 0.0 35.5 57.2 55.1 65.0 52.0 23.7 68.9 22.0 56.2 48.4\n67.2 0.0 35.3 47.4 54.3 65.0 50.8 25.8 68.4 21.7 50.2 46.5\nPUBMED CENTRAL\n73.8 0.0 35.7 39.0 53.5 69.0 55.6 25.1 69.6 21.9 44.2 46.0\n75.1 0.0 36.1 36.3 53.2 69.0 54.1 25.1 69.8 22.6 44.4 45.6\n67.4 0.0 37.0 47.5 54.0 71.0 56.3 24.4 69.9 21.1 48.4 47.7\n71.1 0.0 37.2 55.3 55.6 68.0 57.0 24.7 70.0 22.0 51.0 49.0\n71.9 0.0 36.8 44.4 54.1 71.0 55.0 24.7 70.6 22.1 43.8 46.9\nnot show much degradation in the LM benchmarks,\nit also means that the argmax of the most likely\ntoken to be generated has not changed much. How-\never, further exploration of what exactlyknowledge\nunlearning does to the representations of the LM\nshould be done in future work.\nC Computation Comparison Between\nDEDUPLICATION and Knowledge\nUnlearning\nWe show the FLOPs of pretraining OPT denoted\nas DEDUPLICATION and the average FLOPs of\nperforming knowledge unlearning until s = 32\ntoken sequences reach the Forgetting Threshold\ndenoted as UNLEARNING in Table 8. We calculate\nFLOPs by (6 × Total Training Tokens× Parameter\nSize) following Brown et al. (2020).\nD Varying the Learning Rate\nIn Figure 3, we show the results of varying the\nlearning rate for knowledge unlearning where we\nfix the total epoch to 10 and perform 3 random\nruns with s= 32 on the GPT-N EO 1.3B. Overall,\nwe observe that higher learning rates lead to faster\nforgetting, but with substantial LM performance\ndegradation. While lower learning rates retain the\nLM performance, they fail to meet the Forgetting\nThreshold within 10 epochs. Thus, we set the learn-\ning rate to 5e-5 for our experiments to get the best\ntrade-off.\nE Individual Task Performance During\nKnowledge Unlearning\nTo show exactly what happens to the LM during\nknowledge unlearning, we show how the perfor-\nmance of each of the LM benchmarks changes as\n14401\nTable 6: All of the individual runs for s= 32 for the dialogue tasks in the Main Results.\nModel (s) # EL10 MA WoW ED BST WoI Avg. EpochParams (%) ↓ (%) ↓ (F1) (F1) (F1) (F1) (F1)\nNEO 125M 30.9 77.4 8.4 8.4 9.6 11.2 9.4 -\n∆ - - - +0.0 +0.0 +0.0 +0.0 +0.0 -\nNEO + UL + (s = 32)\n125M 0.3 28.4 1.6 1.8 0.9 1.8 1.5 20.0\n125M 0.8 27.1 0.1 0.1 0.0 0.0 0.0 18.0\n125M 0.2 24.1 6.9 6.7 7.0 7.9 7.1 13.0\n125M 3.0 28.7 2.1 2.5 1.4 2.3 2.1 20.0\n125M 0.7 28.5 2.0 3.5 1.3 2.2 2.2 15.0\nNEO 1.3B 67.6 92.2 9.6 10.5 12.2 13.7 11.5 -\n∆ - - - +2.3 +0.0 +0.0 +0.0 +0.0 -\nNEO + UL + (s = 32)\n1.3B 0.7 33.0 10.0 8.4 9.3 10.9 9.6 13.0\n1.3B 1.7 29.8 11.9 8.4 10.6 12.4 10.8 14.0\n1.3B 0.7 28.4 10.0 8.3 9.5 10.8 9.6 13.0\n1.3B 4.2 31.2 6.4 5 4.9 6.8 5.8 14.0\n1.3B 2.1 29.5 6.9 5.9 5.9 7.5 6.5 15.0\nNEO 2.7B 70.4 93.4 9.2 10.9 12.4 13.6 11.5 -\n∆ - - - +3.8 +1.8 +0.0 +0.5 +1.5 -\nNEO + UL + (s = 32)\n2.7B 0.6 31.7 10.8 8.6 9.6 11.1 10.1 11.0\n2.7B 1.1 32.4 11.9 9.7 11.5 12.1 11.3 10.0\n2.7B 1.2 29.0 12.4 10.5 12.0 13.3 12.1 11.0\n2.7B 3.4 29.9 8.8 8.2 8.4 10.3 8.9 11.0\n2.7B 1.9 31.9 13.0 12.7 12.4 14.1 13.0 11.0\nTable 7: Measuring perplexity on Pile and Wikitext\ncorpora for the main unlearning experiments (Table 2).\nModel # Pile Wikitext\nParams (PPL ) ↓ (PPL ) ↓\nNEO 125M 17.83 38.27\nNEO + UL 125M 34.02 75.24\nNEO + UL+ 125M 577.56 1986.07\nOPT 125M 32.26 38.74\nNEO 1.3B 11.46 18.63\nNEO + UL 1.3B 15.56 20.26\nNEO + UL+ 1.3B 15.83 26.82\nOPT 1.3B 19.55 19.39\nNEO 2.7B 10.44 16.15\nNEO + UL 2.7B 11.32 16.84\nNEO + UL+ 2.7B 17.93 21.13\nOPT 2.7B 17.81 16.81\nwe perform 10 runs of unlearning to the GPT-N EO\n(1.3B) model (each run with s= 1) in Figure 4. As\nshown in the figure, the LM performance for each\nbenchmark varies tremendously on which sample\nis chosen to be forgotten. Furthermore, the end-\ning time of each run is different, indicating that\nsome samples are forgotten faster than others. We\nalso show empirical examples of performing ac-\ntual extraction attacks with prefix length of 100 in\nAppendix G.\nTable 8: Training compute comparison of methods miti-\ngating privacy risks in LMs for sizes 125M, 1.3B, and\n2.7B measured via FLOPs.\nMethod (Size) FLOPs\nDEDUPLICATION (125M) 2.25E+20\nUNLEARNING (125M) 5.28E+13\nDEDUPLICATION (1.3B) 2.34E+21\nUNLEARNING (1.3B) 6.69E+14\nDEDUPLICATION (2.7B) 4.86E+21\nUNLEARNING (2.7B) 1.12E+15\nF Text Example from Each Domain\nWe show an example token sequence from each of\nthe 8 domains used for the analysis section in Table\n9.\nG More examples of performing\nextraction attacks\nIn addition to the extraction attack example shown\nin the analysis section, we provide 3 additional\nexamples to provide readers with more empirical\nexamples of how knowledge unlearning ensures\nprotection against extraction attacks in Table 10.\n14402\n0 2 4 6 8\nEpochs\n0\n20\n40\n60\nAvg. LM Performance\nAvg. EL10\n(a) 1E-4\n0 2 4 6 8\nEpochs\n0\n20\n40\n60\nAvg. LM Performance\nAvg. EL10 (b) 8E-5\n0 2 4 6 8\nEpochs\n0\n20\n40\n60\nAvg. LM Performance\nAvg. EL10\n(c) 5E-5\n0 2 4 6 8\nEpochs\n20\n40\n60\nAvg. LM Performance\nAvg. EL10 (d) 3E-5\n0 2 4 6 8\nEpochs\n20\n40\n60\nAvg. LM Performance\nAvg. EL10\n(e) 1E-5\nFigure 3: Varying the learning rate for unlearning the GPT-N EO 1.3B with s = 32 . We report the average\nof 3 random samplings and display the standard deviations as the shaded regions. Red dotted lines denote the\nmemorization accuracy forgetting threshold of the 1.3B model reported in Table 1.\nH Additional Results of Sequential\nKnowledge Unlearning\nWe show how the EL10 of each individual chunks\nand the average LM performance change as we\nperform sequential unlearning in Figure 5. Results\nshow that the chunks that are forgotten stay forgot-\nten and that later chunks are forgotten much faster\n(one or two epochs) compared to the initial chunk.\nWe hypothesize that this might be because of the\nsimilarity of the token sequences from the 15,000\nexamples from the Training Extraction Challenge\nBenchmark. Also, this result hints at the gener-\nalization of unlearning, which we do not further\nexplore because of the scope of this work.\nI The Effect of Varying N for Extraction\nLikelihood (EL) Metric\nFirst, we show the Extraction Likelihood (EL) For-\ngetting Threshold values for n=[5,10,20,40] by\nmeasuring the value on the 10,000 validation in-\nstances unseen during training in Table 11. Next,\nwe show the average LM performance (on the 9\nclassification benchmarks) where we perform un-\nlearning on the LM on 32 samples until the target\ntoken sequences are forgotten (the EL & MA value\nare both lower than the threshold values) in Table\n12. Performance shows the average of 5 random\n14403\n36.25%\n36.50%\n36.75%\n37.00%\n37.25%\n0 3 6 9 12\nAccuracy\nEpochs\n(a) Hellaswag\n51%\n54%\n57%\n60%\n0 3 6 9 12\nAccuracy\nEpochs (b) Lambada\n54.0%\n54.5%\n55.0%\n55.5%\n0 3 6 9 12\nAccuracy\nEpochs (c) Winogrande\n62.5%\n65.5%\n68.5%\n71.5%\n74.5%\n0 3 6 9 12\nAccuracy\nEpochs\n(d) COPA\n52.5%\n55.0%\n57.5%\n60.0%\n0 3 6 9 12\nAccuracy\nEpochs (e) ARC-Easy\n22.50%\n23.75%\n25.00%\n26.25%\n27.50%\n0 3 6 9 12\nAccuracy\nEpochs (f) ARC-Challenge\n69.0%\n69.5%\n70.0%\n70.5%\n71.0%\n0 3 6 9 12\nAccuracy\nEpochs\n(g) Piqa\n45%\n50%\n55%\n60%\n0 3 6 9 12\nAccuracy\nEpochs (h) MathQA\n47.5%\n50.0%\n52.5%\n55.0%\n57.5%\n0 3 6 9 12\nAccuracy\nEpochs (i) PubMedQA\nFigure 4: Performance on the 9 classification benchmarks as we perform 10 different unlearning runs on GPT-N EO\n1.3B where s= 1.\nTable 9: Examples from each of the 8 domains from the Pile corpora.\nDomain Text\nFREELAW\nU. S. (2010) 1 Opinion of the Court NOTICE: This opinion is subject to formal revision before publication in the preliminary print of the\nUnited States Reports. Readers are requested to notify the Reporter of Decisions, Supreme Court of the United States, Washington, D. C. 20543,\nof any typographical or other formal errors, in order that corrections may be made before the preliminary print goes to press. SUPREME COURT\nOF THE UNITED STATES\nGITHUB (CODE )\n= pc func (iov *Iovec) SetLen(length int) { iov.Len = uint64(length) } func (msghdr *Msghdr) SetControllen(length int) { msghdr.Controllen\n= uint64(length) } func (cmsg *Cmsghdr) SetLen(length int) { cmsg.Len = uint64(length) } //sys poll(fds *PollFd, nfds int, timeout int)\n(n int, err error) func Poll(fds []PollFd, timeout int) (n int, err error) { if len(fds) == 0 { return poll(nil, 0, timeout) } return poll(&fds[0],\nlen(fds), timeout)\nGITHUB (LICENSE )\n## Permission is hereby granted, free of charge, to any person obtaining a copy # of this software and associated documentation files\n(the \"Software\"), to deal # in the Software without restriction, including without limitation the rights # to use, copy, modify, merge, publish,\ndistribute, sublicense, and/or sell # copies of the Software, and to permit persons to whom the Software is # furnished to do so, subject to the\nfollowing conditions: ## The above copyright notice and this permission notice shall be included in # all copies or substantial portions of the\nSoftware. ## THE SOFTW ARE IS PROVIDED \"AS IS\", WITHOUT W ARRANTY OF ANY KIND, EXPRESS OR # IMPLIED,\nINCLUDING BUT NOT LIMITED TO THE W ARRANTIES OF MERCHANTABILITY , # FITNESS FOR A PARTICULAR PURPOSE\nENRON EMAILS\nTo: Hedy Govenar hgovenar@govadv.com , Mike Day MDay@GMSSR.com , Bev Hansen bhansen@lhom.com , Jeff Dasovich jdasovic@\nenron.com , Susan J Mara smara@enron.com , Joseph Alamo JAlamo@enron.com , Paul Kaufman paul.kaufman@enron.com , David Parquet\nDavid.Parquet@enron.com , Rick Johnson rick.johnson@enron.com , Marcie Milner mmilner@enron.com , Sandra\nMcCubbin Sandra.McCubbin@enron.com , Tim Belden Tim.Belden@enron.com\nBOOKS 3\nAbout the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\nhttp://www.harpercollinsebooks.com.au Canada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\nhttp://www.harpercollinsebooks.ca New Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\nhttp://www.harpercollinsebooks.co.nz United Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK\nhttp://www.harpercollinsebooks.co.uk\nPILE CC\nThis website and its associated newspaper adheres to the Independent Press Standards Organisation’s Editors’ Code of Practice. If you have\na complaint about editorial content which relates to inaccuracy or intrusion, then contact the Editor by clicking here. If you remain dissatisfied\nwith the response provided then you can contact the IPSO by clicking here. Bury Free Press provides news, events and sport features from the\nBury St Edmunds area. For the best up to date information relating to Bury St Edmunds and the surrounding areas visit us at Bury Free Press\nregularly or bookmark this page. For you to enjoy all the features of this website Bury Free Press requires permission to use cookies. Find Out\nMore What is a Cookie? What is a Flash Cookie? Can I opt out of receiving Cookies?\nUSPTO BACKGROUNDS\nThe pharmaceutical formulations of the present invention, which may conveniently be presented in unit dosage form, may be prepared\naccording to conventional techniques well known in the pharmaceutical industry. Such techniques include the step of bringing into\nassociation the active ingredients with the pharmaceutical carrier(s) or excipient(s). In general the formulations are prepared by uniformly\nand intimately bringing into association the active ingredients with liquid carriers or finely divided solid carriers or both, and then, if necessary,\nshaping the product. The compositions of the present invention may be formulated into any of many possible dosage forms such as, but not\nlimited to, tablets, capsules, gel capsules, liquid syrups, soft gels, suppositories, and enemas.\nPUBMED CENTRAL\nI am pleased to inform you that your manuscript has been formally accepted for publication in PLOS Computational Biology. Your manuscript\nis now with our production department and you will be notified of the publication date in due course. The corresponding author will soon\nreceiving a typeset proof for review, to ensure errors have not been introduced during production. Please review the PDF proof of your manuscript\ncarefully, as this is the last chance to correct any errors. Please note that major changes, or those which affect the scientific understanding of the\nwork, will likely cause delays to the publication date of your manuscript. Soon after your final files are uploaded, unless you have opted out, the\nearly version of your manuscript will be published online. The date of the early version will be your article´s publication date.\n14404\nTable 10: Examples performing extraction attacks on token sequences, showing knowledge unlearning provides\nprotection against extraction attacks. Underlined denotes the model generated text given the prefix of length 100 as\ninput. For the extraction attack, we utilize a naïve greedy decoding strategy.\nDomain Status Text\nBOOKS 3\nOriginal\nAbout the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\nhttp://www.harpercollinsebooks.com.au Canada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\nhttp://www.harpercollinsebooks.ca New Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\nhttp://www.harpercollinsebooks.co.nz United Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK\nhttp://www.harpercollinsebooks.co.uk\nText\nBefore\nAbout the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\nhttp://www.harpercollinsebooks.com.au Canada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\nhttp://www.harpercollinsebooks.ca New Zealand HarperCollins Publishers (New Zealand) Limited P.O. Box 1 Auckland, New Zealand\nhttp://www.harpercollinsebooks.co.nz United Kingdom HarperCollins Publishers Ltd. 77-85 Fulham Palace Road London, W6 8JB, UK\nhttp://www.harpercollinsebooks.co.uk\nUnlearning\nAfter\nAbout the Publisher Australia HarperCollins Publishers (Australia) Pty. Ltd. 25 Ryde Road (PO Box 321) Pymble, NSW 2073, Australia\nhttp://www.harpercollinsebooks.com.au Canada HarperCollins Publishers Ltd. 55 Avenue Road, Suite 2900 Toronto, ON, M5R, 3L2, Canada\nhttp://www.harpercollins.com.au/Publishers/ Publisher: level three Level two is levels one and two together. The new face of a already great\ntitle! Level one: Just right. Level two: Great. Level three: Awesome. The BloomsburyPublishersPublishers.com.au/PublishersPublishers\nLevels are for bibliographic information or advanced level. s\nUnlearning\nPILE CC\nOriginal\nJames Gurney This daily weblog by Dinotopia creator James Gurney is for illustrators, plein-air painters, sketchers, comic artists, animators,\nart students, and writers. You’ll find practical studio tips, insights into the making of the Dinotopia books, and first-hand reports from art\nschools and museums. CG Art Contact or by email:gurneyjourney (at) gmail.com Sorry, I can’t give personal art advice or portfolio reviews.\nIf you can, it’s best to ask art questions in the blog comments. Permissions All images and text are copyright 2015 James Gurney and/or their\nrespective owners. Dinotopia is a registered trademark of James Gurney. For use of text or images in traditional print media or for any commercial\nlicensing rights, please email me for permission. However, you can quote images or text without\nText\nBefore\nJames Gurney This daily weblog by Dinotopia creator James Gurney is for illustrators, plein-air painters, sketchers, comic artists, animators,\nart students, and writers. You’ll find practical studio tips, insights into the making of the Dinotopia books, and first-hand reports from art\nschools and museums. CG Art Contact or by email:gurneyjourney (at) gmail.com Sorry, I can’t give personal art advice or portfolio reviews.\nIf you can, it’s best to ask art questions in the blog comments. Permissions All images and text are copyright 2015 James Gurney and/or their\nrespective owners. Dinotopia is a registered trademark of James Gurney. For use of text or images in traditional print media or for any commercial\nlicensing rights, please email me for permission. However, you can quote images or text without\nUnlearning\nAfter\nJames Gurney This daily weblog by Dinotopia creator James Gurney is for illustrators, plein-air painters, sketchers, comic artists, animators,\nart students, and writers. You’ll find practical studio tips, insights into the making of the Dinotopia books, and first-hand reports from art\nschools and museums. CG Art Contact or by email:gurneyjourney (at) gmail.com I’ve been working on a CG art project for a while now, and I’ve been\nworking on it for a while now. I’ve been working on it for a while now, and I’ve been working on it for a while now. I’ve been working on it for a\nwhile now, and I’ve been working on it for a while now. I’ve been working on a CG art project for a while now, and I’ve been working on it for a while\nUnlearning\nENRON EMAILS\nOriginal\nRick Shapiro rshapiro@enron.com , Jim Steffes james.d.steffes@enron.com , Alan Comnes acomnes@enron.com , Chris Calger ccalger@enron.com ,\nMary Hain mary.hain@enron.com , Joe Hartsoe Joe.Hartsoe@enron.com , Donna Fulton Donna.Fulton@enron.com , Steven Kean Steven.J.Kean@\nenron.com , Karen Denne kdenne@enron.com , Beverly Aden beverly.aden@enron.com , Bill V otaw bill.votaw@enron.com , Carol Moffett carol.\nmoffett@enron.com , Debora Whitehead deb\nText\nBefore\nRick Shapiro rshapiro@enron.com , Jim Steffes james.d.steffes@enron.com , Alan Comnes acomnes@enron.com , Chris Calger ccalger@enron.com ,\nMary Hain mary.hain@enron.com , Joe Hartsoe Joe.Hartsoe@enron.com , Donna Fulton Donna.Fulton@enron.com , Steven Kean Steven.J.Kean@\nenron.com , Karen Denne kdenne@enron.com , Beverly Aden beverly.aden@enron.com , Bill V otaw bill.votaw@enron.com , Carol Moffett carol.\nmoffett@enron.com , Debora Whitehead\nUnlearning\nAfter Rick Shapiro rshapiro@enron.com , Jim Steffes james.d.steffes@enron.com , Alan Comnes acomnes@enron.com , Chris Calger ccalger@enron.com ,\nMary Hain mary.hain@enron.com , Joe Hartsoe Joe.Hartsoe@enron.com , Donna Fulton Dabat, state+[D@calenergy.com]Unlearning\nPILE CC\nOriginal\n? About Me Alvin McEwen is 46-year-old African-American gay man who resides in Columbia, SC. McEwen’s blog, Holy Bullies and Headless\nMonsters, and writings have been mentioned by Americablog.com, Goodasyou.org, People for the American Way, PageOneQ.com, The Washington\nPost, Raw Story, The Advocate, Media Matters for America, Crooksandliars.com, Thinkprogress.org, Andrew Sullivan’s Daily Dish, Melissa Harris-\nPerry, The Last Word with Lawrence O’Donnell, Newsweek, The Daily Beast, The Washington Blade, and Foxnews.com. In addition, he is also a\npast contributor to Pam’s House Blend,Justice For All, LGBTQ Nation, and Alternet.org. He is a present contributor to the Daily Kos and the Huffington\nPost, He is the 2007 recipient of the Harriet Daniels Hancock V olunteer of the Year Award and\nText\nBefore\n? About Me Alvin McEwen is 46-year-old African-American gay man who resides in Columbia, SC. McEwen’s blog, Holy Bullies and Headless\nMonsters, and writings have been mentioned by Americablog.com, Goodasyou.org, People for the American Way, PageOneQ.com, The Washington\nPost, Raw Story, The Advocate, Media Matters for America, Crooksandliars.com, Thinkprogress.org, Andrew Sullivan’s Daily Dish, Melissa Harris-\nPerry, The Last Word with Lawrence O’Donnell, Newsweek, The Daily Beast, The Washington Blade, and Foxnews.com. In addition, he is also a\npast contributor to Pam’s House Blend,Justice For All, LGBTQ Nation, and Alternet.org. He is a present contributor to the Daily Kos and the Huffington\nPost, He is the 2007 recipient of the Harriet Daniels Hancock V olunteer of the Year Award and\nUnlearning\nAfter\n? About Me Alvin McEwen is 46-year-old African-American gay man who resides in Columbia, SC. McEwen’s blog, Holy Bullies and Headless\nMonsters, and writings have been mentioned by Americablog.com, Goodasyou.org, People for the American Way, PageOneQ.com, The Washington\nPost, Raw Story, The Advocate, Media Matters for America, Crooksandliars.com, Thinkprogress, and more. The British singer has been in the news\nfor his recent singles, including “I’m Not Sure” and “What Makes You Beautiful.” The singer has been in the news for his recent singles, including\n“I’m Not Sure” and “What Makes You Beautiful.” The singer has been in the news for his recent singles, including “I’m Not Sure”\nUnlearning\nTable 11: Forgetting Threshold for GPT-N EO LMs for varying n.\nModel (Size) EL5(%) EL10(%) EL20(%) EL40(%) MA(%)\nThreshold Threshold Threshold Threshold Threshold\nGPT-N EO (1.3B) 7.85 5 .68 4 .07 2 .66 33.27\nsamplings.\n14405\n0 3 6 9 12 15 18\nEpochs\n0\n10\n20\n30\n40\n(a) 125M\n0 3 6 9 12 15\nEpochs\n0\n20\n40\n60\n (b) 1.3B\n0 2 4 6 8\nEpochs\n0\n20\n40\n60\nEL10 1\nEL10 2\nEL10 3\nEL10 4\nAvg. LM Performance\n(c) 2.7B\nFigure 5: Additional results of sequential unlearning for GPT-N EO 125M, 1.3B, and 2.7B. Red dotted lines denote\nthe memorization accuracy forgetting threshold reported of each model in Table 1.\nTable 12: The average of the 9 classification tasks for\nGPT-N EO + UL + for the 1.3B LM when performing\nunlearning until the Forgetting Threshold for each n.\nModel (Size) LM Avg. (Acc)\nEL5 49.93\nEL10 49.93\nEL20 49.85\nEL40 49.88\n14406\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 6\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4 and Section 6\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14407\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4 and Section 6\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4 and Section 6\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nSection 6\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nSection 6\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nSection 6\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nWe are not aware of the demographics of the annotators, as Amazon Mechanical Turk does not\nprovide such information for crowdsourcing.\n14408",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5550942420959473
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.47384148836135864
    },
    {
      "name": "Computational linguistics",
      "score": 0.44256097078323364
    },
    {
      "name": "Knowledge management",
      "score": 0.3913528025150299
    },
    {
      "name": "Natural language processing",
      "score": 0.24623849987983704
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210099236",
      "name": "Kootenay Association for Science & Technology",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I2801919071",
      "name": "University of Illinois System",
      "country": "US"
    }
  ]
}