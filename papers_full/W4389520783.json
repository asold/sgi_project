{
  "title": "Generating Data for Symbolic Language with Large Language Models",
  "url": "https://openalex.org/W4389520783",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2289886577",
      "name": "Jiacheng Ye",
      "affiliations": [
        "University of Hong Kong",
        "Bridge University"
      ]
    },
    {
      "id": "https://openalex.org/A2124928543",
      "name": "Chengzu Li",
      "affiliations": [
        "Bridge University",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2154593323",
      "name": "Lingpeng Kong",
      "affiliations": [
        "University of Hong Kong",
        "Bridge University"
      ]
    },
    {
      "id": "https://openalex.org/A2097645590",
      "name": "Tao Yu",
      "affiliations": [
        "University of Hong Kong",
        "Bridge University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963545046",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4312091426",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3100268441",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W3152729555",
    "https://openalex.org/W4385572953",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2964271186",
    "https://openalex.org/W4287659415",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W4385571886",
    "https://openalex.org/W2891691255",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W3016828850",
    "https://openalex.org/W4385567101",
    "https://openalex.org/W4311726128",
    "https://openalex.org/W2251957808",
    "https://openalex.org/W4200634276",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3035331128",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4404783301",
    "https://openalex.org/W4320839455",
    "https://openalex.org/W2963655793",
    "https://openalex.org/W4280558670",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3174828871",
    "https://openalex.org/W2163274265",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4385573314",
    "https://openalex.org/W3173157360",
    "https://openalex.org/W3175422428",
    "https://openalex.org/W4281790610",
    "https://openalex.org/W2892248135",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4287024925",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W3118969025",
    "https://openalex.org/W4226242393",
    "https://openalex.org/W3170014162",
    "https://openalex.org/W2970544186",
    "https://openalex.org/W3036646712",
    "https://openalex.org/W4389519620",
    "https://openalex.org/W4289445724",
    "https://openalex.org/W3206547074",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W1559723967",
    "https://openalex.org/W3102020135",
    "https://openalex.org/W4226053975"
  ],
  "abstract": "While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks, and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen takes a step toward data generation for annotation-expensive complex tasks, and we release the code at URL.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8418–8443\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nGenerating Data for Symbolic Language\nwith Large Language Models\nJiacheng Ye♠, Chengzu Li♣, Lingpeng Kong♠, Tao Yu♠\n♠The University of Hong Kong ♣University of Cambridge\n{jcye2,lpk,tyu}@cs.hku.hk, cl917@cam.ac.uk\nAbstract\nWhile large language models (LLMs) bring\nnot only performance but also complexity, re-\ncent work has started to turn LLMs into data\ngenerators rather than task inferencers, where\nanother affordable task model is trained for\nefficient deployment and inference. How-\never, such an approach has primarily been\napplied to natural language tasks, and has\nnot yet been explored for symbolic language\ntasks with complex structured outputs (e.g.,\nsemantic parsing and code generation). In\nthis paper, we propose SYMGEN which uti-\nlizes LLMs for generating various annotation-\nexpensive symbolic language data. SYMGEN\nconsists of an informative prompt to steer gen-\neration and an agreement-based verifier to im-\nprove data correctness. We conduct exten-\nsive experiments on six symbolic language\ntasks across various settings. Compared with\nthe LLMs, we demonstrate the 1%-sized task\nmodel can achieve comparable or better perfor-\nmance, largely cutting inference and deploy-\nment costs. We also show that generated data\nwith only a few human demonstrations can\nbe as effective as over 10 times the amount\nof human-annotated data when training the\ntask model, saving a considerable amount of\nannotation effort. SYMGEN takes a step to-\nward data generation for annotation-expensive\ncomplex tasks, and we release the code at\nhttps://github.com/HKUNLP/SymGen.\n1 Introduction\nIn the natural language processing (NLP) literature,\nthe march of scaling language models has been an\nunending yet predictable trend, with new models\nconstantly surpassing previous ones in not only per-\nformance but also complexity (Radford et al., 2019;\nBrown et al., 2020; Chowdhery et al., 2022). Such\nlarge language models (LLMs), however, incur\na large computational cost in practice, especially\nwhen deployed in resource-restricted systems and\ninference in low-latency applications (Bommasani\nSpider [SQL Query]\nGeoQuery [Prolog Command]\nMTOP [TOP Representation]\nBreak [QDMR]\nInput: what is the population of montana?\nOutput: answer(A,(population(B,A),\n                               const(B,stateid(montana))))\nInput: Resume the timer in 10 seconds\nOutput: [IN:RESUME_TIMER [SL:METHOD_TIMER \ntimer ] [SL:DATE_TIME in 10 seconds ] ]\nInput: List the type of bed and name of all traditional rooms.\nOutput: SELECT roomName,bedType FROM Rooms \nWHERE decor = “traditional”\nInput: How many large metallic items are there?\nOutput: 1#) return items 2#) return #1 that are large 3#) \nreturn #2 that are metallic 4#) return number of #3\nFigure 1: Sample symbolic language datasets with com-\nplex structured outputs. The names of the symbolic\nlanguages are shown in square brackets.\net al., 2021).\nInstead of treating LLMs as edge task infer-\nencers, a recent line of work leverage LLMs as\ndata generators, with the generated data being used\nto train more affordable task-specific models for\nefficient deployment and inference (Schick and\nSchütze, 2021; Meng et al., 2022; Ye et al., 2022b,\ninter alia). With only a few or even without demon-\nstration examples, the LLMs can generate high-\nquality data via in-context learning (Brown et al.,\n2020) or prompting (Radford et al., 2019). The task\nmodels trained on these generated data can achieve\ncomparable or even better performance than the\nLLMs and enjoy a low inference cost at the same\ntime.\nHowever, previous work mainly focuses on gen-\nerating natural language data. To what extent this\napproach works for complex structured data, such\nas meaning representation and codes (Figure 1),\nremains an open question. The investigation of\ndata generation via LLMs in the context of such\nsymbolic language tasks is also extremely intrigu-\n8418\ning for two reasons: 1) the human annotation pro-\ncedure for these tasks requires expensive domain\nexpert efforts (Clarke et al., 2010) and carefully-\ndesigned strategies (Wang et al., 2015; Iyer et al.,\n2017; Herzig and Berant, 2019, inter alia); 2) con-\nventional data augmentation methods aiming at en-\nriching datasets for these tasks require handcrafted\nrules, a considerable number of expert demonstra-\ntion examples, and are mostly task-specific (Jia and\nLiang, 2016; Yu et al., 2018a; Andreas, 2020, inter\nalia).\nTo address these issues, we propose Symbolic\ndata Generation (SYMGEN) for various annotation-\nexpensive symbolic language tasks. SYMGEN\nworks with an LLM trained on code (i.e., Codex-\n175B; Chen et al. 2021) and optional task-specific\nstructure knowledge (e.g., database for SQL; Iyer\net al. 2017) through prompting or in-context learn-\ning. SYMGEN also comprises an agreement-based\nverification module, in which the outputs are ver-\nified by execution (e.g., programs, logical forms)\nor formatting (e.g., pseudo-logical forms), to en-\nsure high-quality generations. With the generated\ndata, we train efficient task models with around 1%\nsize of Codex for task inference (e.g., T5 with size\n770M and 3B; Raffel et al. 2020).\nWe experiment on six symbolic languages which\nare SQL, Bash, Python, Prolog, Task Oriented Pars-\ning representation (TOP-representation; Gupta et al.\n2018) and Question Decomposition Meaning Rep-\nresentation (QDMR; Wolfson et al. 2020). We con-\nsider generating data in zero-shot, few-shot, and\nfull data settings. Our key research findings include\nthe following:\n• Compared with the gigantic LLM inferencer,\nthe approximately 1%-sized task inferencer\ncan achieve comparable or superior per-\nformance, producing state-of-the-art perfor-\nmance in some tasks (§3.4);\n• Compared with human annotations, the per-\nformance on few-shot generated data is com-\nparable with that on more than 10x or 100x\nhuman-annotated data, greatly reducing anno-\ntation effort for complex tasks (§3.5);\n• In a zero-shot setting, the small task model\ncan surpass the same model trained with full\nhuman annotations as well as the larger Codex\non specific symbolic language such as SQL\n(§3.6);\n%geobase.pl\n:- ensure_loaded(library('lists')).\n:- ensure_loaded(library('ordsets')).\n:- ensure_loaded(geobase).\ncountry(countryid(usa)).\nstate(stateid(State)) :- state(State,_,_,_,_,_,_,_,_,_).\ncity(cityid(City,St)) :- city(_,St,City,_).\nriver(riverid(R)) :- river(R,_,_).\n…\n%Translate the natural language description to prolog commands.\n%Natural Language: what state is the biggest?\nanswer(A,largest(A,state(A)))\n%Natural Language: what is the population of montana?\nanswer(A,(population(B,A),const(B,stateid(montana))))\n…\n%Natural Language: name the states that the mississippi river \nflows through .\nanswer(A,(river(B),traverse(B,A),const(B,riverid(mississippi))))\nSymbolic Knowledge\nNatural Instruction\nDemonstrations\nGenerated Example\nOverall Prompt\nFigure 2: An example of an overall prompt that consists\nof symbolic knowledge, natural instruction and demon-\nstrations, and a newly generated example by Codex for\nthe GeoQuery dataset.\n• During data generation for symbolic language,\nsymbolic knowledge and demonstrations have\na greater impact than natural language instruc-\ntions, and verification is essential to ensure\ndata quality (§3.7, §4.2).\n2 S YMGEN\nTo automatically curate numerous data for vari-\nous annotation-expensive symbolic language tasks,\nwe propose a unified pipeline, named SYMGEN.\nSYMGEN comprises data generation by prompt-\ning LLMs and data verification by executing or\nformatting.\n2.1 Prompt-based Generation\nHuman annotators need to carefully review the an-\nnotation instructions to perform annotation, and\nthe same is true for LLMs. We include natural\nlanguage instructions, task-related symbolic knowl-\nedge (i.e., database, ontology), and a few labeled\nexamples into prompt construction to steer the gen-\neration. An example of the prompt is shown in\nFigure 2. We display prompts for each task in Ap-\npendix G.\nDifferent from previous work in classifica-\ntion (Schick and Schütze, 2021; Meng et al., 2022;\nYe et al., 2022b) where one of the limited label de-\nscriptions is used to guide the generation of xi for\nclassification tasks, the output structures for sym-\nbolic language is unenumerable and requires task-\n8419\nspecific strategies to construct. Hence, we first gen-\nerate input xi and then the output yi conditioned\non the generated xi. LLMs may generate erroneous\noutputs due to not satisfying different grammatical\nconstraints defined in different symbolic languages.\nTherefore, we over-generate multiple candidates\nfor further verification. After prompt-based gen-\neration, we have a dataset D= {(xi, {yi,j})}for\neach task.\n2.2 Agreement-based Verification\nIn this work, we adopt an over-generation and ver-\nification approach to improve generation quality.\nFormally, given a set of sampled output answers\n{yi,j}for input xi, we verify each answer yi,j by\ncalculating:\nwi,j =\n∑\nk\nsim(exec(yi,j), exec(yi,k)), (1)\nwhere exec = ( ·) is a task-specific execution or\nformatting function (e.g., executing python pro-\ngram, formatting QDMR into graph representa-\ntion), sim = (·, ·) ∈[0, 1] is a similarity function\nto compare the two results after running function\nexec. A large value of wi,j indicates the j-th an-\nswer is highly similar to others, and is thus less\nprone to be mistakenly labeled. The value of the\nmost confident answer wi = max wi,j is used to\nmeasure the quality of the input-output pair, and\nwe only keep those with wi larger than a certain\nthreshold T, indicating the input-output pair is suf-\nficiently confident.\nIn practice, when performing exec, we discard\nyi,j that fails in exec, which means it contains\ngrammatical errors. When using Exact-Match\n(EM) as the similarity function, the similarity score\nranges in {0, 1}, with 1 indicating that the two ex-\necution results are exactly the same. If multiple\nanswers have the same value, we choose the an-\nswer that has the maximum log-likelihood during\ngeneration.\n3 Experiments\n3.1 Datasets and Evaluation Metrics\nWe consider five datasets that cover a range of\nprogramming languages and symbolic meaning\nrepresentations: Spider (SQL; Yu et al. 2018b),\nNL2Bash (Bash; Lin et al. 2018), MBPP (Python;\nAustin et al. 2021), MTOP (TOP-representation;\nLi et al. 2021) and Break (QDMR; Wolfson et al.\n2020). We summarize the choice of the execution\nDataset exec(·) sim( ·,·) Evaluation\nSpider Execution EM EM, EX\nNL2Bash Bashlex BLEU BLEU\nMBPP Execution EM EX\nGeoQuery Execution EM EM, EX\nMTOP TOP Tree EM EM, Template\nBreak QDMR Graph EM LF-EM\nTable 1: Summary of evaluation metric(s), execution\nfunction exec(·) and similarity function sim(·, ·) used\nin verification module for each task. EM and EX refers\nto Exact-Match and Execution accuracy.\nor execution function exec, similarity function sim,\nand evaluation metrics for each dataset in Table 1.\nDetails of the datasets and evaluation metrics are\nillustrated in Appendix A.\n3.2 Comparison Methods\nWe generate data under various settings such as\nzero-shot and few-shot, and then train task models,\ne.g., T5-large and T5-3B, for inference. We com-\npare the performance of the task models with both\nLLM inferencers and the task models that are di-\nrectly finetuned with human-annotated data rather\nthan LLM-generated data:\n• Codex (Chen et al., 2021). The tuning-\nfree method that performs prompt-based in-\ncontext learning with Codex. Due to the\nlength restriction, we perform prompt retrieval\nto include as many similar examples as possi-\nble in the full data setting.\n• Codex + Verification. The method is similar\nto the above but further includes the answer\nverification module as discussed in § 2.2.\n• T5-Large (Raffel et al., 2020). The tuning-\nbased method that directly fine-tunes T5-large\nmodel with few or full human-annotated data\ninstead of generated data.\n• T5-3B (Raffel et al., 2020). The same method\nas the one above, but using a T5-3B model.\n• SOTA. The state-of-the-art models for each\ndataset. The models and the corresponding\nnumber of parameters are Spider (Scholak\net al. 2021; 3B), NL2Bash (Shi et al. 2022;\n175B), MBPP (Chen et al. 2022; 175B),\nMTOP (Xie et al. 2022; 3B), Break (Has-\nson and Berant 2021; ∼300M) and GeoQuery\n(Qiu et al. 2022b; 11B).\n8420\nModel Spider NL2Bash MBPP GeoQuery MTOP Break ∆EM EX Char-BLEU EX EM EX Templete EM LF-EM\nFull data setting\n#Human annotations 7,000 8,090 374 600 15,667 44,321\n#SYMGEN 75,845 47,803 36,367 44,266 36,085 46,839\nSOTA 75.50 71.90 58.50 67.90 93.60 - 87.74 83.76 46.90\nCodex 58.03 64.41 67.68 60.00 79.64 94.29 85.77 78.61 47.40\nCodex + Verification60.54 70.21 75.40 65.56 79.29 94.64 87.20 80.63 50.10 ↑3.08\nT5-Large 66.63 64.12 65.95 13.33 83.93 91.79 86.85 83.04 52.70\nT5-Large + SYMGEN 70.21 69.63 67.17 43.33 84.64 96.07 88.59 84.83 54.50 ↑5.68\nT5-3B 71.76 68.38 65.97 - 83.21 89.29 87.74 83.76 53.30\nT5-3B + SYMGEN 73.40 73.11 67.26 - 84.29 96.07 88.50 84.47 55.20 ↑2.36\nFew-shot setting\n#Human annotations 10 10 10 10 10 10\n#SYMGEN 77,818 39,585 46,793 31,968 40,673 41,385\nCodex 53.77 65.76 61.58 58.89 39.64 65.00 27.52 18.97 26.10\nCodex + Verification53.97 67.89 64.16 67.78 41.79 72.50 29.31 20.49 28.20 ↑3.21\nT5-Large 0.00 0.00 17.65 0.00 10.00 12.86 8.01 4.25 0.60\nT5-Large + SYMGEN 51.84 59.48 57.83 35.56 43.21 77.14 30.34 23.85 30.30 ↑39.58\nT5-3B 0.97 1.26 28.11 - 7.86 10.71 5.50 2.50 1.20\nT5-3B + SYMGEN 58.51 66.83 57.21 - 44.64 77.50 30.56 23.49 31.10 ↑41.47\nTable 2: Results of data generation for training a task model under full data and few-shot settings. The top-scored\nresults for each setting are bold. We show the average improvement with SYMGEN across all tasks in the last\ncolumn.\n3.3 Implementation Details\nWe use code-davinci-002 version for Codex and\nT5-large and T5-3B for task models. Details are\nelaborated in Appendix B.\n3.4 S YMGEN + T5 vs. Codex Inferencer\nIn this section, we consider generating data in few-\nshot and full data settings and report the model\nperformance in Table 2. Firstly, we can see the per-\nformance of T5-Large consistently increases after\nadding data from SYMGEN on all tasks. Notably,\nwe can achieve an on-average 40% performance\nboost in the few-shot setting. Secondly, though\nprompting-based inference has become the de-facto\nstandard to use LLMs on downstream tasks, we find\nuse LLMs as data generators and training a much\nsmaller task model can achieve comparable (e.g.,\nSpider and NL2Bash) or better (e.g., Geoquery,\nMTOP, and Break) performance. The reasons can\nbe twofold: 1) As recent work proves in-context\nlearning is an extreme approximation of fine-tuning\nwith a single-step gradient descent (von Oswald\net al., 2022; Dai et al., 2022), LLM inferencer fails\nin utilizing the valuable human annotations, even\nwith prompt retrieval. For example, Codex can\nsurpass T5-3B on Spider in a few-shot setting but\ncannot in full data setting; 2) the obtained knowl-\nedge (i.e., generated data) from interacting with the\nverifier is not explicitly learned by the LLMs, mean-\ning it never learns to correct its own mistakes, and\nsuch knowledge also improves LLMs themselves\nas shown in Haluptzok et al. (2022). In compari-\nson, the task model can learn from those success-\nful interactions. Finally, we find an exception on\nMBPP, where Codex inferencer significantly out-\nperforms T5, indicating that long-code generation\nis still challenging for small-sized models.\n3.5 S YMGEN vs. Human Annotations\nA key benefit of SYMGEN is reducing annota-\ntion effort when training a task-specific model.\nWe show the performance of the trained T5-large\nmodel under various scales of human-annotated\nand few-shot generated data by SYMGEN in Fig-\nure 3. When using human annotations, the model\nperformance grows linearly with exponentially in-\ncreased data size, which mirrors the power-law in\nneural models (Kaplan et al., 2020). While for\n10-shot generated data, the slope, which indicates\nthe quality of generated data, varies for different\nsymbolic languages. For example, it’s relatively\neasier for Codex to generate SQL and Bash than\nTOP-representation and QDMR, which we hypoth-\nesize is due to the large amount of SQL and Bash\ncommands in the pretraining GitHub corpus (Chen\net al., 2021). In the extremely low resource scenario\nwhere only 10 human annotations are given, the\nperformance SYMGEN can achieve, as indicated by\n8421\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000036/uni00000053/uni0000004c/uni00000047/uni00000048/uni00000055\n/uni00000014/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000010/uni00000044/uni00000051/uni00000051/uni00000052/uni00000057/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000031/uni0000002f/uni00000015/uni00000025/uni00000044/uni00000056/uni0000004b\n/uni00000014/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000010/uni00000044/uni00000051/uni00000051/uni00000052/uni00000057/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000030/uni00000025/uni00000033/uni00000033\n/uni00000014/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000010/uni00000044/uni00000051/uni00000051/uni00000052/uni00000057/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni0000002a/uni00000048/uni00000052/uni00000034/uni00000058/uni00000048/uni00000055/uni0000005c\n/uni00000014/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000010/uni00000044/uni00000051/uni00000051/uni00000052/uni00000057/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000030/uni00000037/uni00000032/uni00000033\n/uni00000014/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000010/uni00000044/uni00000051/uni00000051/uni00000052/uni00000057/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni00000014/uni00000013/uni00000014\n/uni00000014/uni00000013/uni00000015\n/uni00000014/uni00000013/uni00000016\n/uni00000014/uni00000013/uni00000017\n/uni00000014/uni00000013/uni00000018\n/uni00000036/uni00000046/uni00000044/uni0000004f/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013\n/uni00000025/uni00000055/uni00000048/uni00000044/uni0000004e\n/uni00000014/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057/uni00000003/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\n/uni0000002b/uni00000058/uni00000050/uni00000044/uni00000051/uni00000010/uni00000044/uni00000051/uni00000051/uni00000052/uni00000057/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000047/uni00000044/uni00000057/uni00000044\nFigure 3: T5-large performance trained under various scales of human-annotated and few-shot generated data by\nSYMGEN. The horizontal line indicates the performance of the model trained on SYMGEN with only 10 initial\nhuman-annotated examples, which is comparable with that on more than 10x (GeoQuery, MTOP, Break) or 100x\n(Spider, NL2Bash, MBPP) human annotations.\nthe green horizontal line, significantly outperforms\nthe model trained solely on these 10 given data\npoints. Moreover, the intersection point of the hori-\nzontal and vertical lines indicates the performance\nachieved by training the model on the data gener-\nated by SYMGEN is comparable to that on at least\n100 (e.g., MTOP) and up to several thousand (e.g.,\nSpider) human-labeled data. This shows the poten-\ntial of SYMGEN to greatly reduce the annotation\neffort on complex tasks.\n3.6 S YMGEN for Zero-shot Learning\nGiven the striking ability of SYMGEN in few-shot\ndata generation, we take a step forward to see\nwhether it can generate high-quality dataset with-\nout any human annotations. We found it hard to\ncontrol the format in generating most symbolic lan-\nguages without demonstrations, but we succeed\nin generating SQL, as shown in Table 3. We find\nwith appropriate prompt and verification, one can\nachieve a high zero-shot performance of 67.21, out-\nperforming the supervised T5-Large model. We\nalso note that the EM metric is much lower than\nthat of the T5 models, indicating Codex mostly\ngenerates grammatically different but semantically\ncorrect SQLs. Note for pre-trained models, leak-\nage of the test data is a potential concern (Barbalau\nModel EM EX\nFull data setting\nT5-Large 66.63 64.12\nT5-3B 71.76 68.38\nSOTA (Scholak et al., 2021) 75.50 71.90\nZero-shot setting\n#Human annotations 0\nCodex 45.45 64.89\nCodex + verification 45.65 67.21\nT5-Large 0.00 0.00\nT5-Large + SYMGEN(140db, 71k) 45.74 56.38\nT5-Large + SYMGEN(160db, 103k) 50.29 65.67\nT5-3B 0.00 0.00\nT5-3B + SYMGEN(140db, 71k) 48.55 61.03\nT5-3B+ SYMGEN(160db, 103k) 53.38 69.25\nTable 3: Results for zero-shot data generation on Spider.\nWe generate 71k data using databases from the training\nset (140 databases), and 103k data using both the train-\ning and development sets (a total of 160 databases).\net al., 2020; Carlini et al., 2021; Rajkumar et al.,\n2022). Based on the much lower EM accuracy,\nwe attribute the success of zero-shot learning to\nprompt engineering rather than memorization.\nMoreover, it has been shown that adapting to the\nnew environment significantly outperforms data\naugmentation in the training environment by Zhong\net al. (2020b). Given no human-annotated data on\n8422\nthe development environment, we further generate\ndata for those 20 databases as surrogate knowl-\nedge for adaptation. We can see the results signif-\nicantly increase after training on those additional\ndata, and even outperform the large Codex as well\nas the human-supervised T5-Large model, indicat-\ning SYMGEN can be used for zero-shot adaptation\nfor specific symbolic languages such as SQL.\n3.7 Prompt Engineering in S YMGEN\nRecent work highlights PLM sensitivity to the nat-\nural instructions (Zhao et al., 2021; Liu et al., 2022;\nGao et al., 2021). In this section, we study the\ninfluence of symbolic knowledge (e.g., database\nand ontology), natural instruction, demonstration,\nand language reformulation on answer generation.\nAn example of these four types of information in\nprompts is shown in Figure 2. We report the re-\nsults of removing certain types of information in\nTable 4. Removing symbolic knowledge or demon-\nstrations has a greater impact on the answer quality\nthan natural instructions, suggesting symbolic lan-\nguage prediction benefits more from the provided\nsymbolic knowledge and exemplar pairs. An excep-\ntion is on Spider where removing demonstrations\nslightly hurt performance, which is mainly because\nSpider is a cross-domain dataset and the provided\nfew-shot examples are from different domains (see\nexample in Figure 10).\nAs also discussed in §3.4 that Codex is more\nfamiliar with SQL than Prolog, we further experi-\nment on GeoQuery-SQL dataset (Iyer et al., 2017)\nwhich converts Prolog commands to SQL com-\nmands. We show a comparison of the two prompts\nin Appendix Figure 21. We found altering Pro-\nlog to SQL in prompts increases the performance\ndramatically, indicating aligning the expression of\nprompts with pre-training corpus can be another\neffective way of prompt engineering.\n4 Analysis\n4.1 How does S YMGEN compare with data\naugmentation methods?\nFor training a better task model for symbolic lan-\nguage tasks, data recombination (Jia and Liang,\n2016) has been the common choice due to its\ncompositional characteristics. We further compare\nSYMGEN with two competitive baselines for se-\nmantic parsing: Jia and Liang (2016) which uses an\nSCFG induced by domain-specific heuristics, and\nAndreas (2020) which compositionally reuses pre-\nSpider NL2Bash MTOP Break\nDatasets\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5Improvement\nexec\nexec+sim\n10 64 128 15667\n#Human Demonstrations\n20\n40\n60\n80EM\nCodex\nT5-large+SymGen\nFigure 4: (a) Comparison of different verification meth-\nods. We show improvement over the baseline which\ndirectly takes the answer with maximum log-probability\nas output without verification; (b) Results for Codex\nwith in-context learning and T5-large with SYMGEN us-\ning different numbers of human annotations on MTOP\ndataset.\nviously observed sequence fragments in novel en-\nvironments. We generate 1,000 instances for each\nmethod and report the results in Table 5. We can\nsee SYMGEN provides a larger boost, especially in\nthe few-shot setting, where Andreas (2020) failed\ndue to the lack of initial seed data.\n4.2 How does the verification method affect\nperformance?\nWe now investigate the effectiveness of the veri-\nfication method discussed in §2.2. Figure 4 (a)\nshows various answer verification methods, com-\npared with picking the top-likelihood candidate\nwithout verification. We observed that verifying\nbased on agreements of self-generated candidates\n(sim(·, ·)) surpasses the without-verification base-\nline, and also improves answer quality on all the\ntasks more than simply checking grammar correct-\nness (exec(·)). Besides answer verification, we also\nshow filtering low-confidence questions in Table 7,\nwhere the model trained on a much smaller size of\ndata can outperform the one trained on the original\ndata. This further indicates that low-quality data\ncan interfere with the training process.\n4.3 How does a different number of human\nannotations affect SYMGEN?\nBy far we have compared the few-shot results of\nCodex with in-context learning and T5-large with\nSYMGEN using 10 human annotations. In this\nsection, we experiment with various amounts of\nhuman annotations and report the results in Fig-\nure 4 (b). We found the gap in performance be-\ntween Codex and T5-Large remains virtually un-\nchanged, which indicates the performance gain\n8423\nPrompt Types Spider MTOP GeoQuery GeoQuery-SQL\nEM EX Templete EM EM EX EM EX\nFull prompt 53.97 67.89 29.31 20.49 41.79 72.50 71.43 85.36\n- w/o natural instruction 54.64 67.02 25.19 15.21 34.29 71.43 68.57 83.57\n- w/o symbolic knowledge 24.47 26.40 21.12 13.15 31.07 45.00 56.43 67.50\n- w/o instruction & knowledge 23.60 26.31 16.51 10.38 25.00 41.79 55.00 66.43\n- w/o demonstrations 45.65 67.21 0.00 0.00 0.00 17.86 39.29 61.79\nTable 4: Results of few-shot answer generation with different prompts. GeoQuery-SQL refers to converting the\nlanguage of few-shot examples from the original Prolog commands in GeoQuery dataset to SQL. We found symbolic\nknowledge and language reformulation both play key roles in generation quality, and the effect of natural instruction\nvaries for different symbolic languages.\nModel Few-shot Full-data\nEM Exec EM Exec\nT5-Large 10.00 12.86 83.93 91.79\n+ Jia and Liang (2016) 19.29 25.00 85.36 92.14\n+ Andreas (2020) - - 83.21 91.79\n+ SYMGEN 36.79 57.86 87.50 92.86\nTable 5: Comparison of different data augmentation\nmethods on GeoQuery dataset. SYMGEN provides a\nlarger boost to the performance, especially in the few-\nshot setting.\nobtained from pipeline alternation (i.e., from in-\ncontext learning to data generation and supervised\ntuning in SYMGEN) maintains as the size of human\nannotations grows. This further proves that one can\nalways apply SYMGEN in different real scenarios\nfrom little to relatively more annotated data.\n4.4 Data Analysis\nWe further conduct statistical and human evalu-\nations on the quality of generated data from the\nperspective of question diversity, answer complex-\nity, and data pair quality, based on the generated\ndata for MBPP in the full data setting and Spider\nin the few-shot setting.\nQuestion Diversity We measure the question di-\nversity of the generated data for Spider and MBPP\nby question length and question distribution. As\nshown in Figure 5 (a), we find that the questions\ngenerated by SYMGEN are distributed similarly to\nthe original dataset with more coverage. We also\nfind the average length of the generated questions\nis longer than the original dataset for Spider but\nsimilar for MBPP as shown in Appendix E.1.\nAnswer Complexity We first measure the com-\nplexity of answers based on their response lengths.\nFor Spider, as shown in Figure 5 (b), the answers\ngenerated by SYMGEN are longer on average than\n0.0 0.2 0.4 0.6 0.8 1.0\nMBPP Question Distribution\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSymGen\nOriginal\n0 50 100 150 200\nSpider Answer Length\n0.00\n0.01\n0.02\n0.03\n0.04\nSymGen\nOriginal\nFigure 5: (a) TSNE visualization of data generated by\nSYMGEN (randomly sample 5000 examples) and the\noriginal data in the MBPP dataset. (b) Comparison of\nthe length distribution of answers between the original\ndata and SYMGEN on Spider, with the length as x axis\nand the probability density as y axis. More visualiza-\ntions are presented in Appendix E.1.\nthe original dataset. Moreover, we measure the\nanswer by their hardness, which is defined by the\nnumber of keywords following (Yu et al., 2018b).\nOf all the 77,828 data generated by SYMGEN,\n14.15% examples are easy SQLs, 35.83% exam-\nples are of medium level, 28.13% examples are\nhard examples and 21.89% examples belong to\nextra hard SQLs. For MBPP, we found that al-\nthough the generated answers have similar distri-\nbution with human-annotated data in token-level\nlength, SYMGEN tends to generate code with more\nnumber of rows compared to human-annotated data\n(see Figure 8 in the Appendix). This indicates that\nSYMGEN can generate answers in different com-\nplexity levels, especially harder ones compared to\nthe original human-annotated data.\nHuman Evaluation of Data-pairs In order to\nevaluate the quality of generated data, we also\npresent human evaluations on the data-pair qual-\nity of generated Spider and MBPP datasets. We\nrandomly sample 100 examples from SYMGEN\nfor both datasets and manually review the sampled\n8424\ndata. We find 81 and 79 examples are correct for\nMBPP and Spider, respectively. Apart from that,\nwe also find that SYMGEN generates more oper-\nators such as julianday, union in SQL compared\nto the original dataset, and the generated questions\ncovered a wide range of data structures including\ndict, list, and queue for MBPP.\nHowever, there are mainly three issues that exist\nin the data generated by SYMGEN in both MBPP\nand Spider. First, SYMGEN may generate ambigu-\nous and under-specified questions (examples in Ap-\npendix E.3). Secondly, the answers sometimes can\nbe meaninglessly complex. In Spider, SYMGEN\ntends to generate SQL queries with multiple JOIN\nclauses, therefore making the response sequences\nlonger compared to the original dataset. Similarly.\nthe generated Python codes tend to use for-loop\nand recursion instead of the built-in functions of\nPython (e.g. max, min). Thirdly, it can be difficult\nto verify the correctness of the generated answers\nbased on either the original databases in Spider or\nthe test cases that are generated along with Python\nsolutions for MBPP. A quarter of the generated\nSQL queries have empty execution results on the\noriginal databases of Spider and more than 10% of\nthe generated python codes have wrong test cases.\nWe hope these could help to shed light on possible\nimprovements for future works.\n5 Related Work\n5.1 Prompting LLMs\nIn recent years, large pre-trained language mod-\nels (LLMs) have shown promising performance on\nzero-shot and few-shot learning tasks by prompt-\nbased in-context learning (Radford et al., 2019;\nBrown et al., 2020, inter alia). By explicitly cu-\nrating to include code (programming language)\ninto pre-training corpora (Wang, 2021; Chen et al.,\n2021; Chowdhery et al., 2022, inter alia), LLMs\nexhibit surprising ability in symbolic tasks such\nas semantic parsing (Shin and Van Durme, 2022)\nand code generation (Austin et al., 2021; Poesia\net al., 2021; Rajkumar et al., 2022). Nevertheless,\nprompt-based inference with LLMs suffers from\nseveral problems including low inference efficiency\nand expensive deployment cost. In this work, we\nemploy LLMs as data generators rather than direct\ninferencer, which generate supervised data with\nminimal human effort to improve the performance\nof much smaller models for efficient inference on\ndownstream tasks.\n5.2 Data Generation\nData generation is an alternative to data augmenta-\ntion by creating entirely new examples instead of\ncombining original ones (Jia and Liang, 2016; An-\ndreas, 2020, inter alia) (see Appendix F for details).\nConventional approaches adopt fine-tuned genera-\ntive models (Zhong et al., 2020b; Guo et al., 2021;\nWang et al., 2021a, inter alia) as input generators,\nwith a semantic parser (e.g., PCFG grammar) for\nsampling symbolic outputs. Considering the diffi-\nculty in designing grammar to sample useful sym-\nbolic forms in complex domains, Yang et al. (2022)\nassumes access to an unlabeled corpus of symbolic\nlanguage, which is represented in canonical forms,\nand simulates natural language inputs via LLMs.\nIn comparison, we explore directly generating sym-\nbolic forms as well as natural languages without\nthe need to design task-specific grammars for sym-\nbolic forms or synchronous context-free grammars\n(SCFG) that map between canonical forms and\nsymbolic forms. Data generation via LLM has\nalso been explored under various contexts, e.g.,\ncross-lingual semantic parsing (Rosenbaum et al.,\n2022), python program (Haluptzok et al., 2022),\ninstruction generation (Wang et al., 2022), and mul-\ntimodal tasks (Liu et al., 2023; Pi et al., 2023), in\ncontrast, we aim to unify the data generation proce-\ndure for various symbolic languages tasks. Further-\nmore, for simple classification tasks, it has been\nfound a smaller model trained on data generated\nwith a few or even zero human demonstrations can\nachieve better performance than the LLMs (Schick\nand Schütze, 2021; Meng et al., 2022; Ye et al.,\n2022b,a; Gao et al., 2023). This work fills in the\ngap by exploring such an approach to complex sym-\nbolic language tasks.\n6 Conclusion\nIn this work, we treat LLMs as data generators\nrather than task inferencer for complex symbolic\nlanguage tasks, with the generated data being used\nto train much affordable model for depolyment and\ninference. We demonstrate that a 1%-sized model\ntrained under SYMGEN can achieve superior per-\nformance to the LLM inferencers. We especially\nshow the effectiveness in low-resource scenarios,\nwhich is a common situation for symbolic language\ntasks due to the annotation-expensive characteris-\ntics. Additionally, we also reveal the possibility\nof obtaining a well-performed task model through\nSYMGEN even without any human annotations.\n8425\nLimitations\nThis work is based on prompting and in-context\nlearning with informative prompts for symbolic\ndata generation. However, the information that can\nbe packed into the prompt is hard limited by the\nprompt length, as language models are created and\ntrained only to handle sequences of a certain length.\nThe problem becomes more acute for symbolic\nlanguages with complex grammar and is rarely\nseen by the LLMs during the pre-training stage.\nPossible solutions are internalizing the grammar\nknowledge into the output rather than input through\nconstrained decoding algorithms (Scholak et al.,\n2021; Wu et al., 2021; Shin et al., 2021; Shin and\nVan Durme, 2022), identifying limited relevant doc-\numentation when generating data (Agarwal et al.,\n2020; Zhou et al., 2022), or improving the architec-\ntures of LLMs to handle long inputs (Katharopou-\nlos et al., 2020; Peng et al., 2020; Press et al., 2021).\nIn addition, alternative evaluation metrics such\nas tree edit distances or Smatch (Cai and Knight,\n2013) can be employed to reflect the similarity be-\ntween two symbolic languages when execution is\nimpractical.\nReferences\nMayank Agarwal, Jorge J Barroso, Tathagata\nChakraborti, Eli M Dow, Kshitij Fadnis, Borja Godoy,\nMadhavan Pallan, and Kartik Talamadupula. 2020.\nProject clai: Instrumenting the command line as\na new environment for ai agents. arXiv preprint\narXiv:2002.00762.\nEkin Akyürek, Afra Feyza Akyürek, and Jacob Andreas.\n2020. Learning to recombine and resample data for\ncompositional generalization. In International Con-\nference on Learning Representations.\nJacob Andreas. 2020. Good-enough compositional data\naugmentation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7556–7566, Online. Association for\nComputational Linguistics.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021.\nProgram synthesis with large language models. arXiv\npreprint arXiv:2108.07732.\nAntonio Barbalau, Adrian Cosma, Radu Tudor Ionescu,\nand Marius Popescu. 2020. Black-box ripper: Copy-\ning black-box models using generative evolutionary\nalgorithms. Advances in Neural Information Process-\ning Systems, 33:20120–20129.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nShu Cai and Kevin Knight. 2013. Smatch: an evaluation\nmetric for semantic feature structures. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), pages 748–752, Sofia, Bulgaria. Association\nfor Computational Linguistics.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,\nZeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2022.\nCodet: Code generation with generated tests. arXiv\npreprint arXiv:2207.10397.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\n8426\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJames Clarke, Dan Goldwasser, Ming-Wei Chang, and\nDan Roth. 2010. Driving semantic parsing from\nthe world’s response. In Proceedings of the Four-\nteenth Conference on Computational Natural Lan-\nguage Learning, pages 18–27, Uppsala, Sweden. As-\nsociation for Computational Linguistics.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui,\nand Furu Wei. 2022. Why can gpt learn in-context?\nlanguage models secretly perform gradient descent as\nmeta optimizers. arXiv preprint arXiv:2212.10559.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning and\nstochastic optimization. Journal of machine learning\nresearch, 12(7).\nSteven Y . Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush V osoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation\napproaches for NLP. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 968–988, Online. Association for Computa-\ntional Linguistics.\nJiahui Gao, Renjie Pi, LIN Yong, Hang Xu, Jiacheng\nYe, Zhiyong Wu, WEIZHONG ZHANG, Xiaodan\nLiang, Zhenguo Li, and Lingpeng Kong. 2023. Self-\nguided noise-free data generation for efficient zero-\nshot learning. In The Eleventh International Confer-\nence on Learning Representations.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nDemi Guo, Yoon Kim, and Alexander Rush. 2020.\nSequence-level mixed sample data augmentation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5547–5552, Online. Association for Computa-\ntional Linguistics.\nYinuo Guo, Hualei Zhu, Zeqi Lin, Bei Chen, Jian-\nGuang Lou, and Dongmei Zhang. 2021. Revisit-\ning iterative back-translation from the perspective of\ncompositional generalization. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume\n35-9, pages 7601–7609.\nSonal Gupta, Rushin Shah, Mrinal Mohit, Anuj Ku-\nmar, and Mike Lewis. 2018. Semantic parsing for\ntask oriented dialog using hierarchical representa-\ntions. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2787–2792, Brussels, Belgium. Association\nfor Computational Linguistics.\nPatrick Haluptzok, Matthew Bowers, and Adam Tauman\nKalai. 2022. Language models can teach themselves\nto program better. arXiv preprint arXiv:2207.14502.\nMatan Hasson and Jonathan Berant. 2021. Question\ndecomposition with dependency graphs. In 3rd Con-\nference on Automated Knowledge Base Construction.\nJonathan Herzig and Jonathan Berant. 2019. Don’t para-\nphrase, detect! rapid and effective data collection for\nsemantic parsing. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 3810–3820, Hong Kong, China. Association\nfor Computational Linguistics.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant\nKrishnamurthy, and Luke Zettlemoyer. 2017. Learn-\ning a neural semantic parser from user feedback. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 963–973, Vancouver, Canada.\nAssociation for Computational Linguistics.\nRobin Jia and Percy Liang. 2016. Data recombination\nfor neural semantic parsing. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n12–22, Berlin, Germany. Association for Computa-\ntional Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156–5165. PMLR.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2950–2962, Online. Association for Computa-\ntional Linguistics.\nXi Victoria Lin, Chenglong Wang, Luke Zettlemoyer,\nand Michael D. Ernst. 2018. NL2Bash: A corpus\nand semantic parser for natural language interface\nto the linux operating system. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\n8427\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B\nDolan, Lawrence Carin, and Weizhu Chen. 2022.\nWhat makes good in-context examples for gpt-3?\nIn Proceedings of Deep Learning Inside Out (Dee-\nLIO 2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding. CoRR,\nabs/2202.04538.\nPanupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.\nControllable semantic parsing via retrieval augmen-\ntation. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 7683–7698, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2020.\nRandom feature attention. In International Confer-\nence on Learning Representations.\nRenjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze\nDong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang\nXu, and Lingpeng Kong Tong Zhang. 2023. Detgpt:\nDetect what you need via reasoning. arXiv preprint\narXiv:2305.14167.\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari,\nGustavo Soares, Christopher Meek, and Sumit Gul-\nwani. 2021. Synchromesh: Reliable code generation\nfrom pre-trained language models. In International\nConference on Learning Representations.\nOfir Press, Noah Smith, and Mike Lewis. 2021. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Pawel\nNowak, Tal Linzen, Fei Sha, and Kristina Toutanova.\n2022a. Improving compositional generalization with\nlatent structure and data augmentation. In Proceed-\nings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n4341–4362, Seattle, United States. Association for\nComputational Linguistics.\nLinlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi,\nJonathan Herzig, Emily Pitler, Fei Sha, and Kristina\nToutanova. 2022b. Evaluating the impact of model\nscale for compositional generalization in semantic\nparsing. arXiv preprint arXiv:2205.12253.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nNitarshan Rajkumar, Raymond Li, and Dzmitry Bah-\ndanau. 2022. Evaluating the text-to-sql capabil-\nities of large language models. arXiv preprint\narXiv:2204.00498.\nAndy Rosenbaum, Saleh Soltan, Wael Hamza, Amir\nSaffari, Macro Damonte, and Isabel Groves. 2022.\nClasp: Few-shot cross-lingual data augmentation for\nsemantic parsing. arXiv preprint arXiv:2210.07074.\nTimo Schick and Hinrich Schütze. 2021. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTorsten Scholak, Nathan Schucher, and Dzmitry Bah-\ndanau. 2021. PICARD: Parsing incrementally for\nconstrained auto-regressive decoding from language\nmodels. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9895–9901, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nFreda Shi, Daniel Fried, Marjan Ghazvininejad, Luke\nZettlemoyer, and Sida I Wang. 2022. Natural lan-\nguage to code translation with execution. arXiv\npreprint arXiv:2204.11454.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699–7715, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nRichard Shin and Benjamin Van Durme. 2022. Few-\nshot semantic parsing with language models trained\non code. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5417–5425, Seattle, United States.\nAssociation for Computational Linguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. Advances in\nNeural Information Processing Systems, 33:16857–\n16867.\n8428\nJohannes von Oswald, Eyvind Niklasson, Ettore Ran-\ndazzo, João Sacramento, Alexander Mordvintsev, An-\ndrey Zhmoginov, and Max Vladymyrov. 2022. Trans-\nformers learn in-context by gradient descent. arXiv\npreprint arXiv:2212.07677.\nBailin Wang, Wenpeng Yin, Xi Victoria Lin, and Caim-\ning Xiong. 2021a. Learning to synthesize data for\nsemantic parsing. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2760–2766, Online. As-\nsociation for Computational Linguistics.\nBen Wang. 2021. Mesh-Transformer-JAX: Model-\nParallel Implementation of Transformer Lan-\nguage Model with JAX. https://github.com/\nkingoflolz/mesh-transformer-jax.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C.H.\nHoi. 2021b. CodeT5: Identifier-aware unified pre-\ntrained encoder-decoder models for code understand-\ning and generation. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 8696–8708, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nYushi Wang, Jonathan Berant, and Percy Liang. 2015.\nBuilding a semantic parser overnight. In Proceedings\nof the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1332–1342, Beijing,\nChina. Association for Computational Linguistics.\nTomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-\nner, Yoav Goldberg, Daniel Deutch, and Jonathan\nBerant. 2020. Break it down: A question understand-\ning benchmark. Transactions of the Association for\nComputational Linguistics, 8:183–198.\nShan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun,\nWeipeng Zhang, Jiansong Chen, Fan Yang, and Xun-\nliang Cai. 2021. From paraphrasing to semantic pars-\ning: Unsupervised semantic parsing via synchronous\nsemantic decoding. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5110–5121, Online. Association\nfor Computational Linguistics.\nZhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Ling-\npeng Kong. 2022. Self-adaptive in-context learning.\narXiv preprint arXiv:2212.10375.\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,\nTorsten Scholak, Michihiro Yasunaga, Chien-Sheng\nWu, Ming Zhong, Pengcheng Yin, Sida I Wang,\net al. 2022. Unifiedskg: Unifying and multi-tasking\nstructured knowledge grounding with text-to-text lan-\nguage models. arXiv preprint arXiv:2201.05966.\nKevin Yang, Olivia Deng, Charles Chen, Richard Shin,\nSubhro Roy, and Benjamin Van Durme. 2022. Ad-\ndressing resource and privacy constraints in semantic\nparsing through data augmentation. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 3685–3695, Dublin, Ireland. Association\nfor Computational Linguistics.\nJiacheng Ye, Jiahui Gao, Jiangtao Feng, Zhiyong Wu,\nTao Yu, and Lingpeng Kong. 2022a. Progen: Pro-\ngressive zero-shot dataset generation via in-context\nfeedback. arXiv preprint arXiv:2210.12329.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022b. Zerogen: Efficient zero-shot learning via\ndataset generation. CoRR, abs/2202.07922.\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu,\nand Lingpeng Kong. 2023. Compositional ex-\nemplars for in-context learning. arXiv preprint\narXiv:2302.05698.\nTao Yu, Chien-Sheng Wu, Xi Victoria Lin, Yi Chern Tan,\nXinyi Yang, Dragomir Radev, Caiming Xiong, et al.\n2020. Grappa: Grammar-augmented pre-training for\ntable semantic parsing. In International Conference\non Learning Representations.\nTao Yu, Michihiro Yasunaga, Kai Yang, Rui Zhang,\nDongxu Wang, Zifan Li, and Dragomir Radev. 2018a.\nSyntaxSQLNet: Syntax tree networks for complex\nand cross-domain text-to-SQL task. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1653–1663, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018b. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3911–3921, Brussels, Bel-\ngium. Association for Computational Linguistics.\nJohn M Zelle and Raymond J Mooney. 1996. Learning\nto parse database queries using inductive logic pro-\ngramming. In Proceedings of the national conference\non artificial intelligence, pages 1050–1055.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\nRuiqi Zhong, Tao Yu, and Dan Klein. 2020a. Semantic\nevaluation for text-to-SQL with distilled test suites.\nIn Proceedings of the 2020 Conference on Empirical\n8429\nMethods in Natural Language Processing (EMNLP),\npages 396–411, Online. Association for Computa-\ntional Linguistics.\nVictor Zhong, Mike Lewis, Sida I. Wang, and Luke\nZettlemoyer. 2020b. Grounded adaptation for zero-\nshot executable semantic parsing. In Proceedings\nof the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6869–\n6882, Online. Association for Computational Lin-\nguistics.\nShuyan Zhou, Uri Alon, Frank F Xu, Zhengbao JIang,\nand Graham Neubig. 2022. Doccoder: Generating\ncode by retrieving and reading docs. arXiv preprint\narXiv:2207.05987.\nA Datasets\nSpider The Spider dataset (Yu et al., 2018b) is a\nmulti-domain and cross-database dataset for text-\nto-SQL parsing. There are 7,000 examples for\ntraining and 1,034 for development. We report\nperformance on the development set as noted by\nRajkumar et al. (2022) that evaluating on the held-\nout test set risks inadvertently leaking them for\nretraining of Codex. 1 We determine model per-\nformance based on surface form Exact Set Match\n(EM; Yu et al. (2018b)) and test-suite Execution\naccuracy (EX; Zhong et al. (2020a)) which ex-\ntends execution to multiple database instances per\nSQL schema to provide the best approximation of\nsemantic accuracy.\nNL2Bash The NL2Bash dataset (Lin et al., 2018)\naims to translate natural language to bash com-\nmands. There are 8,090 examples for training and\n609 for development. Because it is difficult to ex-\necute bash commands in a sandbox, we evaluate\nthe a bash command by parsing and tokenizing\nwith bashlex2, and calculating token-level BLEU-\n4 score between commands as the estimation of\nexecution result similarity. Following Lin et al.\n(2018), commands are evaluated with character-\nlevel BLEU-4 score.\nMBPP The MBPP dataset (Austin et al., 2021) is\na python programming task, where text description\nis mapped to python program containing multiple\nlines. MBPP consists of 974 examples, with 500\nof them used for testing and the rest for training\nor few-shot prompting. We evaluate with execu-\ntion accuracy (EX), where a program is consid-\nered as passing if all three associated test cases\n1Unless otherwise mentioned, we also report the results on\nthe development set for the other datasets.\n2https://pypi.org/project/bashlex/\nare correct. We don’t include surface-level metrics\n(e.g., BLEU) as semantically identical programs\ncan potentially have very low n-gram overlap (e.g.,\nidentifier renaming) (Austin et al., 2021).\nGeoQuery The GeoQuery dataset (Zelle and\nMooney, 1996) contains human-authored questions\npaired with prolog logic programming language\nabout U.S. geography, with 600 examples for train-\ning and 280 for testing. We report Exact Match\n(EM) and Execution (EX) accuracy by running\nwith SWI-Prolog3 and pyswip4.\nMTOP MTOP (Li et al., 2021) is a semantic pars-\ning dataset, focused on multilingual task-oriented\ndialogues, where commands are mapped to com-\nplex nested queries across 11 domains. Similar to\nprevious work (Pasupat et al., 2021), we use the\nEnglish subset, which contains 15,667 training ex-\namples and 2,235 development set examples. We\nevaluate with Exact Match (EM), i.e., whether the\nprediction string is identical to the reference string,\nand Template Accuracy where the query tokens are\ndiscarded (e.g., the template of [IN:A [SL:B text]]\nis [IN:A [SL:B]]).\nBreak The Break dataset Wolfson et al. (2020)\ncontains complex natural language questions sam-\npled from 10 QA datasets, and they are decom-\nposed into an ordered list of atomic steps. We use\nthe low-level subset, which contains 44,321 train-\ning examples and 8,000 development set examples.\nWe randomly sample 1,000 examples to construct\na new development set for evaluation. We evalu-\nate model performance with LF-EM (Hasson and\nBerant, 2021), which is proposed as an improve-\nment to Exact Match (EM) to measure whether two\nmeaning representations are semantically equiva-\nlent.\nB Implementation Details\nFor prompting or in-context learning with Codex,\nwe use code-davinci-002 and a maximum context\nsize of 7000. For all the tasks, we set the tempera-\nture to 0.8 and the number of samplings to 30 for\nanswer generation. When generating questions, we\nconstruct initial 200 prompts by randomly select-\ning in-context examples5 and use the mixture of\ntemperature (i.e., 0.6, 0.8, and 1) with a number of\n3https://www.swi-prolog.org/\n4https://github.com/yuce/pyswip\n5In few-shot settings, we random sample permutation of\nall the examples to infuse diversity.\n8430\nsamplings of 100 to generate at most 60k questions.\nFor Spider, we generate 200 questions for each of\nthe 140 databases in the training set, which results\nin at most 84k data pairs using three temperatures.\nWe set the number of shots to 10 in the few-shot\nsetting. In the full-data setting, as found by prior\nwork that including similar exemplars helps in an-\nswer prediction (Liu et al., 2022; Wu et al., 2022;\nYe et al., 2023), we use all-mpnet-base-v2 (Song\net al., 2020) 6 to encode questions and Faiss 7 to\nsearch similar examples. We truncate the number\nof in-context examples based on the maximum con-\ntext size and order the examples from least to most\nbased on similarity score.\nWe mainly use T5-large (770M) and T5-3B (Raf-\nfel et al., 2020) as task models for all the datasets.\nFor MBPP (python) dataset, we find the original\ntokenizers of T5 is based on SentencePiece and\nwould remove the indentations and blankspaces in\nthe codes when doing tokenization, and therefore\nwould influence the execution of Python program\nwhen generating the code string. Based on this\nreason, we use CodeT5-large (770M; Wang et al.\n2021b) on MBPP dataset.\nFor training T5, we adopt the setting from Xie\net al. (2022), where we use a batch size of 32, an\nAdafactor (Duchi et al., 2011) optimizer for T5-\nlarge, an AdamW (Loshchilov and Hutter, 2018)\noptimizer for T5-3B, a learning rate of 5e-5, a linear\nlearning rate decay and a maximum number of\ntraining epochs of 50 with early-stopping patience\nof 5. In the full-data setting, we use the strategy\nof first tuning on the mixture of synthesized and\nhuman-annotated data, then continue tuning it with\nonly the human annotation data. We find this two-\nstage training performs better than the importance-\nweighted loss (see Appendix C for details).\nC Training Strategy\nWe compare the training strategies when we have\nboth full human annotated data and generated data\nin Table 6. We can see the two-stage training proce-\ndure that first trains on the mixture on both datasets\nand then solely on human annotated data outper-\nforms the weighted training baselines.\n6https://huggingface.co/sentence-transformers/all-mpnet-\nbase-v2\n7https://github.com/facebookresearch/faiss\nMTOP NL2bash Break Spider\n#Data (Gold+Syn.) 15k+36k 8k+47k 45k+41k 7k+82k\nGold 83.04 65.95 52.70 64.12\nMix 1:1 81.88 62.88 51.60 68.38\nMix 1:3 83.27 66.50 53.10 68.57\nMix 1:1→Gold 84.83 67.17 54.50 69.63\nTable 6: Training strategies to use full human-annotated\ndata and synthetic data using T5-Large. The two-stage\ntraining strategy (last row) performs better than the\nimportance-weighted loss.\nD Question Verification Results\nWe measure the quality of a question through an-\nswer consistency, where more generated answers\nare semantically equivalent means the question is\nless ambiguous and considered as high quality. We\nshow the effect of the threshold used to filter am-\nbiguous question in Table 7. We can see the model\ntrained on a much smaller size of data can outper-\nform the one trained on original data, indicating\nlow quality data can interfere with the training pro-\ncess.\nThre. NL2Bash MTOP Break\n#data CBLEU #data EM #data LF-EM\nT=0 58k 56.83 40k 23.13 56k 29.90\nT=3 46k 56.49 34k 23.85 41k 30.30\nT=5 39k 57.83 27k 22.10 29k 28.30\nTable 7: Results on filtering generated questions with\nvarying threshold T on few-shot setting and training\nT5-large. We found filtering questions that have low-\nconfidence answers results in a smaller dataset but im-\nproves model performance.\nE Data Analysis\nE.1 Question Diversity\n0 10 20 30 40 50\nSpider\n0.00\n0.02\n0.04\n0.06\n0.08\nSymGen\nOriginal\n0 10 20 30 40 50\nMBPP\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14 SymGen\nOriginal\nFigure 6: Comparison on token-level length distribution\nof the questions on Spider and MBPP.\n8431\n0.0 0.2 0.4 0.6 0.8 1.0\nSpider\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 SymGen\nOriginal\n0.0 0.2 0.4 0.6 0.8 1.0\nMBPP\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 SymGen\nOriginal\nFigure 7: Comparison on the distribution of the ques-\ntions’ embedding (obtained by SBERT) in Spider (ran-\ndomly sample one database) and MBPP from SYMGEN\n(randomly sample 5000) and the original datasets.\nE.2 Response Complexity\n0 5 10 15 20 25 30\nMBPP\n0.00\n0.05\n0.10\n0.15\n0.20\nSymGen\nOriginal\nFigure 8: Comparison on row-level lengths of the an-\nswers between SYMGEN generated data and the original\nMBPP dataset.\nE.3 Human Evaluation on Data Pair Quality\nIn this section we present some typical examples\nof the question ambiguity and underspecification\nproblems in data generation by SYMGEN. The\ngenerated questions may be underspecified and\nambiguous, even sometimes unreasonable, there-\nfore influencing the generation of corresponding\nanswers. Some examples are presented as follows.\n# Spider\n(1) How to modify table Customer_Orders\nsuch that for every row, system will\nautomatically insert a row to table\nCustomer_ord\n(2) List the name and the year in which\nthe party was first elected for the\nparties that have been elected in at\nleast 2 counties.\n# MBPP\n(1) Write a function to convert a string\nto a list. (Didn't mention in character-\n-level or word-level)\n(2) Write a function to split the given\ncomma-separated values of list into two\nlists. (Didn't tell the rule to split\nthe list)\nF Related Work on Data Augmentation\nThere is a large body of research on general data\naugmentation (Feng et al., 2021), which assume the\noutputs remain unchanged. For symbolic-language\nprediction tasks, instead of holding outputs fixed,\nwe would like to apply simultaneous transforma-\ntions to inputs and outputs to increase the coverage\nof output structures. Data recombination method\n(Jia and Liang, 2016; Andreas, 2020; Akyürek\net al., 2020; Guo et al., 2020; Qiu et al., 2022a)\nalong both dimensions of inputs and outputs are\nproposed, where different fragments of input and\noutput from different examples are re-combined to\ncreate hard (Jia and Liang, 2016; Andreas, 2020;\nAkyürek et al., 2020; Qiu et al., 2022a) or soft (Guo\net al., 2020) augmented examples. Yu et al. (2018a,\n2020) follow the same spirit and use a hand-crafted\nSCFG grammar to generate new parallel data. How-\never, rule-based heuristics or a large pool of seed\nexamples are needed to induce the grammar.\nG Prompt Examples\n8432\nCREATE TABLE \"Rooms\" (\"RoomId\" TEXT PRIMARY KEY, \"roomName\" TEXT, \"beds\" INTEGER, \"bedType\" TEXT, \"maxOccupancy\" INTEGER, \"basePrice\" INTEGER, \"decor\"\nTEXT)\n/*\n3 example rows from table Rooms:\nSELECT * FROM Rooms LIMIT 3;\nRoomId roomName beds bedType maxOccupancy basePrice decor\nHBB Harbinger but bequest 1 Queen 2 100 modern\nTAA Thrift and accolade 1 Double 2 75 modern\nRTE Riddle to exculpate 2 Queen 4 175 rustic\n*/\nCREATE TABLE \"Reservations\" (\"Code\" INTEGER PRIMARY KEY, \"Room\" TEXT, \"CheckIn\" TEXT, \"CheckOut\" TEXT, \"Rate\" REAL, \"LastName\" TEXT, \"FirstName\" TEXT,\n\"Adults\" INTEGER, \"Kids\" INTEGER, FOREIGN KEY (Room) REFERENCES Rooms(RoomId))\n/*\n3 example rows from table Reservations:\nSELECT * FROM Reservations LIMIT 3;\nCode Room CheckIn CheckOut Rate LastName FirstName Adults Kids\n60313 CAS 28-OCT-10 30-OCT-10 218.75 SLONE LARITA 1 1\n81473 RND 01-FEB-10 02-FEB-10 127.50 EVERITT YUK 1 1\n35546 TAA 19-SEP-10 24-SEP-10 67.50 YUK TIM 1 0\n*/\n-- Write a question that can be answered based on the above tables.\n-- Question: List the type of bed and name of all traditional rooms.\n** EXAMPLE SEPARATOR **\nCREATE TABLE \"department\" (\"Department_ID\" int, \"Name\" text, \"Creation\" text, \"Ranking\" int, \"Budget_in_Billions\" real, \"Num_Employees\" real, PRIMARY\nKEY (\"Department_ID\"))\n/*\n3 example rows from table department:\nSELECT * FROM department LIMIT 3;\nDepartment_ID Name Creation Ranking Budget_in_Billions Num_Employees\n7 Commerce 1903 7 6.2 36000.0\n3 Defense 1947 3 439.3 3000000.0\n15 Homeland Security 2002 15 44.6 208000.0\n*/\nCREATE TABLE \"head\" (\"head_ID\" int, \"name\" text, \"born_state\" text, \"age\" real, PRIMARY KEY (\"head_ID\"))\n/*\n3 example rows from table head:\nSELECT * FROM head LIMIT 3;\nhead_ID name born_state age\n8 Nick Faldo California 56.0\n7 Stewart Cink Florida 50.0\n5 Jeff Maggert Delaware 53.0\n*/\nCREATE TABLE \"management\" (\"department_ID\" int, \"head_ID\" int, \"temporary_acting\" text, PRIMARY KEY (\"Department_ID\",\"head_ID\"), FOREIGN KEY (\"\nDepartment_ID\") REFERENCES `department`(\"Department_ID\"), FOREIGN KEY (\"head_ID\") REFERENCES `head`(\"head_ID\"))\n/*\n3 example rows from table management:\nSELECT * FROM management LIMIT 3;\ndepartment_ID head_ID temporary_acting\n7 3 No\n15 4 Yes\n11 10 No\n*/\n-- Write a question that can be answered based on the above tables.\n-- Question:\nFigure 9: Example prompt for generating questions for Spider, only single in-context example is shown for\nillustration.\n8433\nCREATE TABLE \"Rooms\" (\"RoomId\" TEXT PRIMARY KEY, \"roomName\" TEXT, \"beds\" INTEGER, \"bedType\" TEXT, \"maxOccupancy\" INTEGER, \"basePrice\" INTEGER, \"decor\"\nTEXT)\n/*\n3 example rows from table Rooms:\nSELECT * FROM Rooms LIMIT 3;\nRoomId roomName beds bedType maxOccupancy basePrice decor\nHBB Harbinger but bequest 1 Queen 2 100 modern\nTAA Thrift and accolade 1 Double 2 75 modern\nRTE Riddle to exculpate 2 Queen 4 175 rustic\n*/\nCREATE TABLE \"Reservations\" (\"Code\" INTEGER PRIMARY KEY, \"Room\" TEXT, \"CheckIn\" TEXT, \"CheckOut\" TEXT, \"Rate\" REAL, \"LastName\" TEXT, \"FirstName\" TEXT,\n\"Adults\" INTEGER, \"Kids\" INTEGER, FOREIGN KEY (Room) REFERENCES Rooms(RoomId))\n/*\n3 example rows from table Reservations:\nSELECT * FROM Reservations LIMIT 3;\nCode Room CheckIn CheckOut Rate LastName FirstName Adults Kids\n60313 CAS 28-OCT-10 30-OCT-10 218.75 SLONE LARITA 1 1\n81473 RND 01-FEB-10 02-FEB-10 127.50 EVERITT YUK 1 1\n35546 TAA 19-SEP-10 24-SEP-10 67.50 YUK TIM 1 0\n*/\n-- Using valid SQLite, answer the following questions for the tables provided above.\n-- Question: List the type of bed and name of all traditional rooms.\nSELECT roomName , bedType FROM Rooms WHERE decor = \"traditional\";\n** EXAMPLE SEPARATOR **\nCREATE TABLE \"department\" (\"Department_ID\" int, \"Name\" text, \"Creation\" text, \"Ranking\" int, \"Budget_in_Billions\" real, \"Num_Employees\" real, PRIMARY\nKEY (\"Department_ID\"))\n/*\n3 example rows from table department:\nSELECT * FROM department LIMIT 3;\nDepartment_ID Name Creation Ranking Budget_in_Billions Num_Employees\n1 State 1789 1 9.96 30266.0\n2 Treasury 1789 2 11.10 115897.0\n3 Defense 1947 3 439.30 3000000.0\n*/\nCREATE TABLE \"head\" (\"head_ID\" int, \"name\" text, \"born_state\" text, \"age\" real, PRIMARY KEY (\"head_ID\"))\n/*\n3 example rows from table head:\nSELECT * FROM head LIMIT 3;\nhead_ID name born_state age\n1 Tiger Woods Alabama 67.0\n2 Sergio Garcia California 68.0\n3 K. J. Choi Alabama 69.0\n*/\nCREATE TABLE \"management\" (\"department_ID\" int, \"head_ID\" int, \"temporary_acting\" text, PRIMARY KEY (\"Department_ID\",\"head_ID\"), FOREIGN KEY (\"\nDepartment_ID\") REFERENCES `department`(\"Department_ID\"), FOREIGN KEY (\"head_ID\") REFERENCES `head`(\"head_ID\"))\n/*\n3 example rows from table management:\nSELECT * FROM management LIMIT 3;\ndepartment_ID head_ID temporary_acting\n2 5 Yes\n15 4 Yes\n2 6 Yes\n*/\n-- Using valid SQLite, answer the following questions for the tables provided above.\n-- Question: What are the names of the heads who manage the department with ID 15?\nSELECT\nFigure 10: Example prompt for generating SQL queries for Spider, only single in-context example is shown for\nillustration.\n8434\nTranslate the natural language description to bash commands.\nNatural Language: Recursively removes all files and folders named '.svn' in a current folder, handling content of removed folder before folder inself.\nNatural Language: find all executable files in /home directory.\nNatural Language: Locate files that reside in the /u/bill directory tree and were last accessed between 2 and 6 minutes ago\nNatural Language: Search the current directory tree for files whose names match regular expression '.*packet.*', ignoring the case\nNatural Language: List all the emptry files in thecurrent directory only.\nNatural Language: Find all files under current directory whose status was changed less than 3 days ago and show last 5 lines of output\nNatural Language: Find files that were modified more than 7 days ago and archive them\nNatural Language: Set variable 'file' to the base name of first argument to script or function, that is the part following the last slash.\nNatural Language: Connect to host \"$USER_AT_HOST\" in master mode in the background without executing any commands and set the ControlPath to \"\n$SSHSOCKET\"\nNatural Language: Print input \"your, text, here\" formatted to fit 70 characters per line breaking at spaces\nNatural Language:\nFigure 11: Example prompt for generating questions for NL2Bash.\nTranslate the natural language description to bash commands.\nNatural Language: Recursively removes all files and folders named '.svn' in a current folder, handling content of removed folder before folder inself.\nBash commands: find . -depth -name .svn -exec rm -fr {} \\;\nNatural Language: find all executable files in /home directory.\nBash commands: find /home -type f -perm /a=x\nNatural Language: Locate files that reside in the /u/bill directory tree and were last accessed between 2 and 6 minutes ago\nBash commands: find /u/bill -amin +2 -amin -6\nNatural Language: Search the current directory tree for files whose names match regular expression '.*packet.*', ignoring the case\nBash commands: find . -iregex \".*packet.*\"\nNatural Language: List all the emptry files in thecurrent directory only.\nBash commands: find . -maxdepth 1 -empty\nNatural Language: Find all files under current directory whose status was changed less than 3 days ago and show last 5 lines of output\nBash commands: find . -type f -ctime -3 | tail -n 5\nNatural Language: Find files that were modified more than 7 days ago and archive them\nBash commands: find . -type f -mtime +7 | xargs tar -cvf `date '+%d%m%Y'_archive.tar`\nNatural Language: Set variable 'file' to the base name of first argument to script or function, that is the part following the last slash.\nBash commands: file=`basename \"$1\"`\nNatural Language: Connect to host \"$USER_AT_HOST\" in master mode in the background without executing any commands and set the ControlPath to \"\n$SSHSOCKET\"\nBash commands: ssh -M -f -N -o ControlPath=\"$SSHSOCKET\" \"$USER_AT_HOST\"\nNatural Language: Print input \"your, text, here\" formatted to fit 70 characters per line breaking at spaces\nBash commands: echo 'your, text, here' | fold -sw 70\nNatural Language: Find files with names that start with 'input' and end with a single character 'a' or 'b' in the current directory and all its\nsubdirectories\nBash commands:\nFigure 12: Example prompt for generating bash commands for NL2Bash.\n8435\nTranslate the natural language instructions to Python codes.\nNatural Language Instruction for Python Code: Write a function to find squares of individual elements in a list using lambda function.\nNatural Language Instruction for Python Code: Write a function to find all words which are at least 4 characters long in a string by using regex.\nNatural Language Instruction for Python Code: Write a python function to find the minimum number of rotations required to get the same string.\nNatural Language Instruction for Python Code: Write a python function to check whether the two numbers differ at one bit position only or not.\nNatural Language Instruction for Python Code: Write a python function to identify non-prime numbers.\nNatural Language Instruction for Python Code: Write a function to find the largest integers from a given list of numbers using heap queue algorithm.\nNatural Language Instruction for Python Code: Write a function to get the n smallest items from a dataset.\nNatural Language Instruction for Python Code: Write a function to find the number of ways to fill it with 2 x 1 dominoes for the given 3 x n board.\nNatural Language Instruction for Python Code: Write a function to find the minimum cost path to reach (m, n) from (0, 0) for the given cost matrix cost\n[][] and a position (m, n) in cost[][].\nNatural Language Instruction for Python Code: Write a function to find the similar elements from the given two tuple lists.\nNatural Language Instruction for Python Code:\nFigure 13: Example prompt for generating question descriptions for MBPP.\n8436\n\"\"\"\nWrite a python function to check whether the word is present in a given sentence or not.\n\"\"\"\ndef is_Word_Present(sentence,word):\ns = sentence.split(\" \")\nfor i in s:\nif (i == word):\nreturn True\nreturn False\n# 3 test cases\nassert is_Word_Present(\"machine learning\",\"machine\") == True\nassert is_Word_Present(\"easy\",\"fun\") == False\nassert is_Word_Present(\"python language\",\"code\") == False\n\"\"\"\nWrite a function to find the cumulative sum of all the values that are present in the given tuple list.\n\"\"\"\ndef cummulative_sum(test_list):\nres = sum(map(sum, test_list))\nreturn (res)\n# 3 test cases\nassert cummulative_sum([(1, 3), (5, 6, 7), (2, 6)]) == 30\nassert cummulative_sum([(2, 4), (6, 7, 8), (3, 7)]) == 37\nassert cummulative_sum([(3, 5), (7, 8, 9), (4, 8)]) == 44\n\"\"\"\nWrite a python function to find the average of a list.\n\"\"\"\ndef Average(lst):\nreturn sum(lst) / len(lst)\n# 3 test cases\nassert Average([15, 9, 55, 41, 35, 20, 62, 49]) == 35.75\nassert Average([4, 5, 1, 2, 9, 7, 10, 8]) == 5.75\nassert Average([1,2,3]) == 2\n...... (Some in-context examples omitted here for simplicity)\n\"\"\"\nWrite a function to create a list taking alternate elements from another given list.\n\"\"\"\ndef alternate_elements(list1):\nresult=[]\nfor item in list1[::2]:\nresult.append(item)\nreturn result\n# 3 test cases\nassert alternate_elements([\"red\", \"black\", \"white\", \"green\", \"orange\"])==['red', 'white', 'orange']\nassert alternate_elements([2, 0, 3, 4, 0, 2, 8, 3, 4, 2])==[2, 3, 0, 8, 4]\nassert alternate_elements([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])==[1,3,5,7,9]\n\"\"\"\nWrite a function to find the minimum total path sum in the given triangle.\n\"\"\"\ndef min_sum_path(A):\nmemo = [None] * len(A)\nn = len(A) - 1\nfor i in range(len(A[n])):\nmemo[i] = A[n][i]\nfor i in range(len(A) - 2, -1,-1):\nfor j in range( len(A[i])):\nmemo[j] = A[i][j] + min(memo[j],\nmemo[j + 1])\nreturn memo[0]\n# 3 test cases\nassert min_sum_path([[ 2 ], [3, 9 ], [1, 6, 7 ]]) == 6\nassert min_sum_path([[ 2 ], [3, 7 ], [8, 5, 6 ]]) == 10\nassert min_sum_path([[ 3 ], [6, 4 ], [5, 2, 7 ]]) == 9\n\"\"\"\nWrite a python function to count occurences of a character in a repeated string.\n\"\"\"\ndef count_Char(str,x):\ncount = 0\nfor i in range(len(str)):\nif (str[i] == x) :\ncount += 1\nn = 10\nrepititions = n // len(str)\ncount = count * repititions\nl = n % len(str)\nfor i in range(l):\nif (str[i] == x):\ncount += 1\nreturn count\n# 3 test cases\nassert count_Char(\"abcac\",'a') == 4\nassert count_Char(\"abca\",'c') == 2\nassert count_Char(\"aba\",'a') == 7\n\"\"\"\nWrite a function to find the difference between two lists.\n\"\"\"\nFigure 14: Example prompt for generating python program queries for MBPP.\n8437\nIN:GET: MESSAGE, WEATHER, ALARM, INFO_RECIPES, STORIES_NEWS, REMINDER, RECIPES, EVENT, CALL_TIME, LIFE_EVENT, INFO_CONTACT, CONTACT, TIMER,\nREMINDER_DATE_TIME, AGE, SUNRISE, EMPLOYER, EDUCATION_TIME, JOB, AVAILABILITY, CATEGORY_EVENT, CALL, EMPLOYMENT_TIME, CALL_CONTACT, LOCATION,\nTRACK_INFO_MUSIC, SUNSET, MUTUAL_FRIENDS, UNDERGRAD, REMINDER_LOCATION, ATTENDEE_EVENT, MESSAGE_CONTACT, REMINDER_AMOUNT, DATE_TIME_EVENT,\nDETAILS_NEWS, EDUCATION_DEGREE, MAJOR, CONTACT_METHOD, LIFE_EVENT_TIME, LYRICS_MUSIC, AIRQUALITY, LANGUAGE, GENDER, GROUP | IN:SEND: MESSAGE | IN\n:SET: UNAVAILABLE, RSVP_YES, AVAILABLE, DEFAULT_PROVIDER_MUSIC, RSVP_INTERESTED, DEFAULT_PROVIDER_CALLING, RSVP_NO | IN:DELETE: REMINDER, ALARM,\nTIMER, PLAYLIST_MUSIC | IN:CREATE: ALARM, REMINDER, CALL, PLAYLIST_MUSIC, TIMER | IN:QUESTION: NEWS, MUSIC | IN:PLAY: MUSIC, MEDIA | IN:END: CALL\n| IN:IGNORE: CALL | IN:UPDATE: CALL, REMINDER_DATE_TIME, REMINDER_TODO, TIMER, METHOD_CALL, ALARM, REMINDER_LOCATION, REMINDER | IN:PAUSE: MUSIC\n, TIMER | IN:ANSWER: CALL | IN:SNOOZE: ALARM | IN:IS: TRUE_RECIPES | IN:REMOVE: FROM_PLAYLIST_MUSIC | IN:ADD: TIME_TIMER, TO_PLAYLIST_MUSIC | IN:\nSHARE: EVENT | IN:PREFER: | IN:START: SHUFFLE_MUSIC | IN:SILENCE: ALARM | IN:SWITCH: CALL | IN:SUBTRACT: TIME_TIMER | IN:PREVIOUS: TRACK_MUSIC |\nIN:HOLD: CALL | IN:SKIP: TRACK_MUSIC | IN:LIKE: MUSIC | IN:RESTART: TIMER | IN:RESUME: TIMER, CALL, MUSIC | IN:MERGE: CALL | IN:REPLAY: MUSIC |\nIN:LOOP: MUSIC | IN:STOP: MUSIC, SHUFFLE_MUSIC | IN:UNLOOP: MUSIC | IN:CANCEL: MESSAGE, CALL | IN:REWIND: MUSIC | IN:REPEAT: ALL_MUSIC,\nALL_OFF_MUSIC | IN:FAST: FORWARD_MUSIC | IN:DISLIKE: MUSIC | IN:DISPREFER: | IN:HELP: REMINDER | IN:FOLLOW: MUSIC\nSL:CONTACT: , ADDED, RELATED, REMOVED, METHOD | SL:TYPE: CONTENT, RELATION, CONTACT | SL:RECIPIENT: | SL:LOCATION: | SL:DATE: TIME | SL:ORDINAL: |\nSL:CONTENT: EXACT | SL:RECIPES: ATTRIBUTE, DISH, COOKING_METHOD, INCLUDED_INGREDIENT, TYPE, UNIT_NUTRITION, EXCLUDED_INGREDIENT, DIET,\nUNIT_MEASUREMENT, TYPE_NUTRITION, MEAL, RATING, QUALIFIER_NUTRITION, SOURCE, CUISINE | SL:PERSON: REMINDED | SL:TODO: | SL:NEWS: TYPE, CATEGORY,\nTOPIC, REFERENCE, SOURCE | SL:SENDER: | SL:MUSIC: TYPE, ARTIST_NAME, PLAYLIST_TITLE, TRACK_TITLE, PROVIDER_NAME, GENRE, ALBUM_TITLE, RADIO_ID,\nALBUM_MODIFIER, REWIND_TIME, PLAYLIST_MODIFIER | SL:NAME: APP | SL:WEATHER: ATTRIBUTE, TEMPERATURE_UNIT | SL:CATEGORY: EVENT | SL:METHOD: TIMER,\nRETRIEVAL_REMINDER, RECIPES | SL:LIFE: EVENT | SL:AMOUNT: | SL:EMPLOYER: | SL:PERIOD: | SL:EDUCATION: DEGREE | SL:TITLE: EVENT | SL:TIMER:\nNAME | SL:JOB: | SL:PHONE: NUMBER | SL:ATTRIBUTE: EVENT | SL:ALARM: NAME | SL:SCHOOL: | SL:SIMILARITY: | SL:GROUP: | SL:AGE: | SL:ATTENDEE:\nEVENT, | SL:USER: ATTENDEE_EVENT | SL:MAJOR: | SL:GENDER:\nTranslate the natural language description to logical form with the above arguments.\nNatural Language: Every day my alarm is set for what time?\nNatural Language: top news stories\nNatural Language: should I bring the plants in tonight\nNatural Language: Resume the timer in 10 seconds\nNatural Language: Message Ben to see if he can come to my birthday party\nNatural Language: Do I have a friend that works in Los Angeles?\nNatural Language: give me the news update for around the world.\nNatural Language: Set the sleep timer for 10 minutes\nNatural Language: i need to change my podiatrist appointment reminder to 5pm instead of 5:30\nNatural Language: take 11 minutes off the timer\nNatural Language:\nFigure 15: Example prompt for generating questions for MTOP.\n8438\nIN:GET: MESSAGE, WEATHER, ALARM, INFO_RECIPES, STORIES_NEWS, REMINDER, RECIPES, EVENT, CALL_TIME, LIFE_EVENT, INFO_CONTACT, CONTACT, TIMER,\nREMINDER_DATE_TIME, AGE, SUNRISE, EMPLOYER, EDUCATION_TIME, JOB, AVAILABILITY, CATEGORY_EVENT, CALL, EMPLOYMENT_TIME, CALL_CONTACT, LOCATION,\nTRACK_INFO_MUSIC, SUNSET, MUTUAL_FRIENDS, UNDERGRAD, REMINDER_LOCATION, ATTENDEE_EVENT, MESSAGE_CONTACT, REMINDER_AMOUNT, DATE_TIME_EVENT,\nDETAILS_NEWS, EDUCATION_DEGREE, MAJOR, CONTACT_METHOD, LIFE_EVENT_TIME, LYRICS_MUSIC, AIRQUALITY, LANGUAGE, GENDER, GROUP | IN:SEND: MESSAGE | IN\n:SET: UNAVAILABLE, RSVP_YES, AVAILABLE, DEFAULT_PROVIDER_MUSIC, RSVP_INTERESTED, DEFAULT_PROVIDER_CALLING, RSVP_NO | IN:DELETE: REMINDER, ALARM,\nTIMER, PLAYLIST_MUSIC | IN:CREATE: ALARM, REMINDER, CALL, PLAYLIST_MUSIC, TIMER | IN:QUESTION: NEWS, MUSIC | IN:PLAY: MUSIC, MEDIA | IN:END: CALL\n| IN:IGNORE: CALL | IN:UPDATE: CALL, REMINDER_DATE_TIME, REMINDER_TODO, TIMER, METHOD_CALL, ALARM, REMINDER_LOCATION, REMINDER | IN:PAUSE: MUSIC\n, TIMER | IN:ANSWER: CALL | IN:SNOOZE: ALARM | IN:IS: TRUE_RECIPES | IN:REMOVE: FROM_PLAYLIST_MUSIC | IN:ADD: TIME_TIMER, TO_PLAYLIST_MUSIC | IN:\nSHARE: EVENT | IN:PREFER: | IN:START: SHUFFLE_MUSIC | IN:SILENCE: ALARM | IN:SWITCH: CALL | IN:SUBTRACT: TIME_TIMER | IN:PREVIOUS: TRACK_MUSIC |\nIN:HOLD: CALL | IN:SKIP: TRACK_MUSIC | IN:LIKE: MUSIC | IN:RESTART: TIMER | IN:RESUME: TIMER, CALL, MUSIC | IN:MERGE: CALL | IN:REPLAY: MUSIC |\nIN:LOOP: MUSIC | IN:STOP: MUSIC, SHUFFLE_MUSIC | IN:UNLOOP: MUSIC | IN:CANCEL: MESSAGE, CALL | IN:REWIND: MUSIC | IN:REPEAT: ALL_MUSIC,\nALL_OFF_MUSIC | IN:FAST: FORWARD_MUSIC | IN:DISLIKE: MUSIC | IN:DISPREFER: | IN:HELP: REMINDER | IN:FOLLOW: MUSIC\nSL:CONTACT: , ADDED, RELATED, REMOVED, METHOD | SL:TYPE: CONTENT, RELATION, CONTACT | SL:RECIPIENT: | SL:LOCATION: | SL:DATE: TIME | SL:ORDINAL: |\nSL:CONTENT: EXACT | SL:RECIPES: ATTRIBUTE, DISH, COOKING_METHOD, INCLUDED_INGREDIENT, TYPE, UNIT_NUTRITION, EXCLUDED_INGREDIENT, DIET,\nUNIT_MEASUREMENT, TYPE_NUTRITION, MEAL, RATING, QUALIFIER_NUTRITION, SOURCE, CUISINE | SL:PERSON: REMINDED | SL:TODO: | SL:NEWS: TYPE, CATEGORY,\nTOPIC, REFERENCE, SOURCE | SL:SENDER: | SL:MUSIC: TYPE, ARTIST_NAME, PLAYLIST_TITLE, TRACK_TITLE, PROVIDER_NAME, GENRE, ALBUM_TITLE, RADIO_ID,\nALBUM_MODIFIER, REWIND_TIME, PLAYLIST_MODIFIER | SL:NAME: APP | SL:WEATHER: ATTRIBUTE, TEMPERATURE_UNIT | SL:CATEGORY: EVENT | SL:METHOD: TIMER,\nRETRIEVAL_REMINDER, RECIPES | SL:LIFE: EVENT | SL:AMOUNT: | SL:EMPLOYER: | SL:PERIOD: | SL:EDUCATION: DEGREE | SL:TITLE: EVENT | SL:TIMER:\nNAME | SL:JOB: | SL:PHONE: NUMBER | SL:ATTRIBUTE: EVENT | SL:ALARM: NAME | SL:SCHOOL: | SL:SIMILARITY: | SL:GROUP: | SL:AGE: | SL:ATTENDEE:\nEVENT, | SL:USER: ATTENDEE_EVENT | SL:MAJOR: | SL:GENDER:\nTranslate the natural language description to logical form with the above arguments.\nNatural Language: Every day my alarm is set for what time?\nLogical Form: [IN:GET_ALARM [SL:PERIOD Every day ] ]\nNatural Language: top news stories\nLogical Form: [IN:GET_STORIES_NEWS [SL:NEWS_REFERENCE top ] [SL:NEWS_TYPE news stories ] ]\nNatural Language: should I bring the plants in tonight\nLogical Form: [IN:GET_WEATHER [SL:WEATHER_ATTRIBUTE plants ] [SL:DATE_TIME in tonight ] ]\nNatural Language: Resume the timer in 10 seconds\nLogical Form: [IN:RESUME_TIMER [SL:METHOD_TIMER timer ] [SL:DATE_TIME in 10 seconds ] ]\nNatural Language: Message Ben to see if he can come to my birthday party\nLogical Form: [IN:SEND_MESSAGE [SL:RECIPIENT Ben ] [SL:CONTENT_EXACT he can come to my birthday party ] ]\nNatural Language: Do I have a friend that works in Los Angeles?\nLogical Form: [IN:GET_CONTACT [SL:CONTACT_RELATED I ] [SL:TYPE_RELATION friend ] [SL:LOCATION Los Angeles ] ]\nNatural Language: give me the news update for around the world.\nLogical Form: [IN:GET_STORIES_NEWS [SL:NEWS_TYPE news ] ]\nNatural Language: Set the sleep timer for 10 minutes\nLogical Form: [IN:CREATE_TIMER [SL:TIMER_NAME sleep ] [SL:METHOD_TIMER timer ] [SL:DATE_TIME for 10 minutes ] ]\nNatural Language: i need to change my podiatrist appointment reminder to 5pm instead of 5:30\nLogical Form: [IN:UPDATE_REMINDER_DATE_TIME [SL:PERSON_REMINDED my ] [SL:TODO podiatrist appointment ] [SL:DATE_TIME to 5 pm ] [SL:DATE_TIME of 5 : 30\n] ]\nNatural Language: take 11 minutes off the timer\nLogical Form: [IN:SUBTRACT_TIME_TIMER [SL:DATE_TIME 11 minutes ] [SL:METHOD_TIMER timer ] ]\nNatural Language: What is my alarm set to every day.\nLogical Form:\nFigure 16: Example prompt for generating TOP-representations for MTOP.\nBreak down a question into the requisite steps for computing its answer.\nQuestion: If both images show mainly similar-shaped orange-and-white striped fish swimming among anemone tendrils.\nQuestion: If two seals are lying in the sand in the image on the right.\nQuestion: If the right image shows a single dog sitting.\nQuestion: How many large metallic items are there?\nQuestion: who was the leader of the north during the vietnam war?\nQuestion: What actor played in both the Trial of Michael Jackson and The Wiz?\nQuestion: If the bed set in the left image has a pink canopy above it.\nQuestion: Give the name of the student in the History department with the most credits.\nQuestion: when did lil wayne first start singing?\nQuestion: If there are no less than five dogs\nQuestion:\nFigure 17: Example prompt for generating questions for Break.\n8439\nBreak down a question into the requisite steps for computing its answer.\nQuestion: If both images show mainly similar-shaped orange-and-white striped fish swimming among anemone tendrils.\nAnswer Steps: 1#) return fish 2#) return #1 that are similar-shaped 3#) return #2 that are orange-and-white striped 4#) return anemone tendrils 5#)\nreturn #3 swimming among #4 6#) return if #5 are mainly in both images\nQuestion: If two seals are lying in the sand in the image on the right.\nAnswer Steps: 1#) return right image 2#) return seals in #1 3#) return sand in #1 4#) return #2 that are lying in #3 5#) return number of #4 6#) return\nif #5 is equal to two\nQuestion: If the right image shows a single dog sitting.\nAnswer Steps: 1#) return the right image 2#) return dogs in #1 3#) return #2 that are sitting 4#) return number of #3 5#) return if #4 is equal to one\nQuestion: How many large metallic items are there?\nAnswer Steps: 1#) return items 2#) return #1 that are large 3#) return #2 that are metallic 4#) return number of #3\nQuestion: who was the leader of the north during the vietnam war?\nAnswer Steps: 1#) return north vietnam 2#) return leader of #1 3#) return the vietnam war 4#) return #2 during #3\nQuestion: What actor played in both the Trial of Michael Jackson and The Wiz?\nAnswer Steps: 1#) return Trial of Michael Jackson 2#) return The Wiz 3#) return actor of both #1 and #2\nQuestion: If the bed set in the left image has a pink canopy above it.\nAnswer Steps: 1#) return left image 2#) return bed set in #1 3#) return canopy in #1 4#) return #3 that is pink 5#) return #4 that is above #2 6#)\nreturn number of #5 7#) return if #6 is at least one\nQuestion: Give the name of the student in the History department with the most credits.\nAnswer Steps: 1#) return students 2#) return #1 in History department 3#) return credits of #2 4#) return number of #3 for each #2 5#) return #2 where\n#4 is highest 6#) return name of #5\nQuestion: when did lil wayne first start singing?\nAnswer Steps: 1#) return lil wayne 2#) return date that #1 start singing\nQuestion: If there are no less than five dogs\nAnswer Steps: 1#) return dogs 2#) return number of #1 3#) return if #2 is at least five\nQuestion: when was the last time the steelers won back to back super bowls\nAnswer Steps: 1#)\nFigure 18: Example prompt for generating question decompositions for Break.\n8440\n%geobase.pl\n:- ensure_loaded(library('lists')).\n:- ensure_loaded(library('ordsets')).\n:- ensure_loaded(geobase).\ncountry(countryid(usa)).\nstate(stateid(State)) :- state(State,_,_,_,_,_,_,_,_,_).\ncity(cityid(City,St)) :- city(_,St,City,_).\nriver(riverid(R)) :- river(R,_,_).\nlake(lakeid(R)) :- lake(R,_,_).\nmountain(mountainid(M)) :- mountain(_,_,M,_).\nplace(placeid(P)) :- highlow(_,_,P,_,_,_).\nplace(placeid(P)) :- highlow(_,_,_,_,P,_).\nabbreviation(stateid(State), Ab) :- state(State,Ab,_,_,_,_,_,_,_,_).\nabbreviation(Ab) :- abbreviation(_,Ab).\ncapital(stateid(State), cityid(Cap,St)) :- state(State,St,Cap,_,_,_,_,_,_,_).\ncapital(Cap) :- capital(_,Cap).\nloc(X,countryid(usa)) :- city(X) ; state(X) ; river(X) ; place(X) ; lake(X); mountain(X).\nloc(cityid(City,St), stateid(State)) :- city(State, St, City,_).\nloc(cityid(City,St), stateid(State)) :- state(State,St,City,_,_,_,_,_,_,_).\nloc(placeid(P), stateid(S)) :- highlow(S,_,P,_,_,_).\nloc(placeid(P), stateid(S)) :- highlow(S,_,_,_,P,_).\nloc(mountainid(P), stateid(S)) :- mountain(S,_,P,_).\nloc(riverid(R), stateid(S)) :- river(R,_,States), member(S,States).\nloc(lakeid(L),stateid(S)) :- lake(L,_,States), member(S,States).\ntraverse(riverid(R), stateid(S)) :- river(R,_,States), member(S,States).\ntraverse(riverid(R), countryid(usa)).\nhigh_point(countryid(usa), placeid('mount mckinley')).\nhigh_point(stateid(S), placeid(P)) :- highlow(S,_,P,_,_,_).\nlow_point(countryid(usa), placeid('death valley')).\nlow_point(stateid(S), placeid(P)) :- highlow(S,_,_,_,P,_).\narea(stateid(X),Areal) :- state(X,_,_,_,Area,_,_,_,_,_), Areal is float(Area).\narea(countryid(X),Areal) :- country(X,_,Area), Areal is float(Area).\nmajor(cityid(C,S)) :- X = cityid(C,S), city(X), population(X,P), P > 150000.\nmajor(riverid(R)) :- X = riverid(R), river(X), len(X,L), L > 750.\n(omitted to save space)\n%Translate the natural language description to prolog commands.\n%Natural Language: what state is the biggest ?\n%Natural Language: what is the population of montana ?\n%Natural Language: what are the major cities in delaware ?\n%Natural Language: what is the capital of michigan ?\n%Natural Language: name the rivers in arkansas .\n%Natural Language: what state has the largest urban population ?\n%Natural Language: what is the longest river that flows through colorado ?\n%Natural Language: what is the biggest state in continental us ?\n%Natural Language: which states have points that are higher than the highest point in texas ?\n%Natural Language: what is the longest river in the smallest state in the usa ?\n%Natural Language:\nFigure 19: Example prompt for generating questions for GeoQuery.\n8441\n%geobase.pl\n:- ensure_loaded(library('lists')).\n:- ensure_loaded(library('ordsets')).\n:- ensure_loaded(geobase).\ncountry(countryid(usa)).\nstate(stateid(State)) :- state(State,_,_,_,_,_,_,_,_,_).\ncity(cityid(City,St)) :- city(_,St,City,_).\nriver(riverid(R)) :- river(R,_,_).\nlake(lakeid(R)) :- lake(R,_,_).\nmountain(mountainid(M)) :- mountain(_,_,M,_).\nplace(placeid(P)) :- highlow(_,_,P,_,_,_).\nplace(placeid(P)) :- highlow(_,_,_,_,P,_).\nabbreviation(stateid(State), Ab) :- state(State,Ab,_,_,_,_,_,_,_,_).\nabbreviation(Ab) :- abbreviation(_,Ab).\ncapital(stateid(State), cityid(Cap,St)) :- state(State,St,Cap,_,_,_,_,_,_,_).\ncapital(Cap) :- capital(_,Cap).\nloc(X,countryid(usa)) :- city(X) ; state(X) ; river(X) ; place(X) ; lake(X); mountain(X).\nloc(cityid(City,St), stateid(State)) :- city(State, St, City,_).\nloc(cityid(City,St), stateid(State)) :- state(State,St,City,_,_,_,_,_,_,_).\nloc(placeid(P), stateid(S)) :- highlow(S,_,P,_,_,_).\nloc(placeid(P), stateid(S)) :- highlow(S,_,_,_,P,_).\nloc(mountainid(P), stateid(S)) :- mountain(S,_,P,_).\nloc(riverid(R), stateid(S)) :- river(R,_,States), member(S,States).\nloc(lakeid(L),stateid(S)) :- lake(L,_,States), member(S,States).\ntraverse(riverid(R), stateid(S)) :- river(R,_,States), member(S,States).\ntraverse(riverid(R), countryid(usa)).\nhigh_point(countryid(usa), placeid('mount mckinley')).\nhigh_point(stateid(S), placeid(P)) :- highlow(S,_,P,_,_,_).\nlow_point(countryid(usa), placeid('death valley')).\nlow_point(stateid(S), placeid(P)) :- highlow(S,_,_,_,P,_).\narea(stateid(X),Areal) :- state(X,_,_,_,Area,_,_,_,_,_), Areal is float(Area).\narea(countryid(X),Areal) :- country(X,_,Area), Areal is float(Area).\nmajor(cityid(C,S)) :- X = cityid(C,S), city(X), population(X,P), P > 150000.\nmajor(riverid(R)) :- X = riverid(R), river(X), len(X,L), L > 750.\n(omitted to save space)\n%Translate the natural language description to prolog commands.\n%Natural Language: what state is the biggest ?\nanswer(A,largest(A,state(A)))\n%Natural Language: what is the population of montana ?\nanswer(A,(population(B,A),const(B,stateid(montana))))\n%Natural Language: what are the major cities in delaware ?\nanswer(A,(major(A),city(A),loc(A,B),const(B,stateid(delaware))))\n%Natural Language: what is the capital of michigan ?\nanswer(A,(capital(A),loc(A,B),const(B,stateid(michigan))))\n%Natural Language: name the rivers in arkansas .\nanswer(A,(river(A),loc(A,B),const(B,stateid(arkansas))))\n%Natural Language: what state has the largest urban population ?\nanswer(A,largest(B,(state(A),population(A,B))))\n%Natural Language: what is the longest river that flows through colorado ?\nanswer(A,longest(A,(river(A),traverse(A,B),const(B,stateid(colorado)))))\n%Natural Language: what is the biggest state in continental us ?\nanswer(A,largest(A,(state(A),loc(A,B),const(B,countryid(usa)))))\n%Natural Language: which states have points that are higher than the highest point in texas ?\nanswer(A,(state(A),loc(B,A),higher(B,C),highest(C,(place(C),loc(C,D),const(D,stateid(texas))))))\n%Natural Language: what is the longest river in the smallest state in the usa ?\nanswer(A,longest(A,(river(A),loc(A,B),smallest(B,(state(B),loc(B,C),const(C,countryid(usa)))))))\n%Natural Language: count the states which have elevations lower than what alabama has .\nanswer(\nFigure 20: Example prompt for generating prolog commands for GeoQuery.\n8442\n%geobase.pl:- ensure_loaded(library('lists')).:- ensure_loaded(library('ordsets')).:- ensure_loaded(geobase).country(countryid(usa)).state(stateid(State)) :- state(State,_,_,_,_,_,_,_,_,_).city(cityid(City,St)) :- city(_,St,City,_).river(riverid(R)) :- river(R,_,_).…%Translate the natural language description to prolog commands.%Natural Language: what state is the biggest?answer(A,largest(A,state(A)))%Natural Language: what is the population of montana?answer(A,(population(B,A),const(B,stateid(montana))))%Natural Language: count the states which have elevations lower than what alabama has .answer(\nCREATE TABLE \"border_info\" (\"state_name\" text, \"border\" text)/*3 example rows from table border_info:SELECT * FROM border_info LIMIT 3;state_name    border   alabama tennessee   alabama   georgia   alabama   florida*/…-- Using valid SQLite, answer the following questions for the tables provided above.-- Question: what state is the biggest ?SELECT state.state_name FROM state WHERE state.area=(SELECT max(state.area) FROM state);-- Question: what is the population of montana ?SELECT state.population FROM state WHERE state.state_name='montana';-- Question: count the states which have elevations lower than what alabama has .SELECT\n(a) Prompt for GeoQuery. (b) Prompt for GeoQuery-SQL.\nFigure 21: Prompt comparison for GeoQuery and GeoQuery-SQL. Only two demonstrations and a part of symbolic\nknowledge are shown for simplicity.\n8443",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8199641704559326
    },
    {
      "name": "Task (project management)",
      "score": 0.6567175984382629
    },
    {
      "name": "Correctness",
      "score": 0.6013498306274414
    },
    {
      "name": "Natural language generation",
      "score": 0.5775749087333679
    },
    {
      "name": "Inference",
      "score": 0.5457973480224609
    },
    {
      "name": "Annotation",
      "score": 0.520236611366272
    },
    {
      "name": "Natural language processing",
      "score": 0.4995131492614746
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4960101544857025
    },
    {
      "name": "Parsing",
      "score": 0.47195377945899963
    },
    {
      "name": "Natural language",
      "score": 0.41592806577682495
    },
    {
      "name": "Language model",
      "score": 0.4142773747444153
    },
    {
      "name": "Programming language",
      "score": 0.36197707056999207
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}