{
    "title": "Performance of large language models on Thailand’s national medical licensing examination: a cross-sectional study",
    "url": "https://openalex.org/W4410292044",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5093478175",
            "name": "Prut Saowaprut",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4308839013",
            "name": "Romen Samuel Wabina",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2130181035",
            "name": "Junwei Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5115622332",
            "name": "Lertboon Siriwat",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4392749797",
        "https://openalex.org/W4392359953",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4319460874",
        "https://openalex.org/W4378647938",
        "https://openalex.org/W4391644915",
        "https://openalex.org/W4392702964",
        "https://openalex.org/W4393069045",
        "https://openalex.org/W4389795202",
        "https://openalex.org/W4389431826",
        "https://openalex.org/W4394819503",
        "https://openalex.org/W4391890163",
        "https://openalex.org/W4405689421"
    ],
    "abstract": "Purpose: This study aimed to evaluate the feasibility of general-purpose large language models (LLMs) in addressing inequities in medical licensure exam preparation for Thailand’s National Medical Licensing Examination (ThaiNLE), which currently lacks standardized public study materials.Methods: We assessed 4 multi-modal LLMs (GPT-4, Claude 3 Opus, Gemini 1.0/1.5 Pro) using a 304-question ThaiNLE Step 1 mock examination (10.2% image-based), applying deterministic API configurations and 5 inference repetitions per model. Performance was measured via micro- and macro-accuracy metrics compared against historical passing thresholds.Results: All models exceeded passing scores, with GPT-4 achieving the highest accuracy (88.9%; 95% confidence interval, 88.7–89.1), surpassing Thailand’s national average by more than 2 standard deviations. Claude 3.5 Sonnet (80.1%) and Gemini 1.5 Pro (72.8%) followed hierarchically. Models demonstrated robustness across 17 of 20 medical domains, but variability was noted in genetics (74.0%) and cardiovascular topics (58.3%). While models demonstrated proficiency with images (Gemini 1.0 Pro: +9.9% vs. text), text-only accuracy remained superior (GPT-4o: 90.0% vs. 82.6%).Conclusion: General-purpose LLMs show promise as equitable preparatory tools for ThaiNLE Step 1. However, domain-specific knowledge gaps and inconsistent multi-modal integration warrant refinement before clinical deployment.",
    "full_text": null
}