{
  "title": "Dynamically Adjusting Transformer Batch Size by Monitoring Gradient Direction Change",
  "url": "https://openalex.org/W3034687015",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5049783368",
      "name": "Hongfei Xu",
      "affiliations": [
        "German Research Centre for Artificial Intelligence",
        "Saarland University"
      ]
    },
    {
      "id": "https://openalex.org/A5049194403",
      "name": "Josef van Genabith",
      "affiliations": [
        "German Research Centre for Artificial Intelligence",
        "Saarland University"
      ]
    },
    {
      "id": "https://openalex.org/A5055232825",
      "name": "Deyi Xiong",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5056590711",
      "name": "Qiuhui Liu",
      "affiliations": [
        "China Mobile (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297785678",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2963456262",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W222053410",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2760389211",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2963702144",
    "https://openalex.org/W2921311659",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W3104636952",
    "https://openalex.org/W2296701362",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2146502635",
    "https://openalex.org/W2766164908",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964345285",
    "https://openalex.org/W2797569913"
  ],
  "abstract": "The choice of hyper-parameters affects the performance of neural models. While much previous research (Sutskever et al., 2013; Duchi et al., 2011; Kingma and Ba, 2015) focuses on accelerating convergence and reducing the effects of the learning rate, comparatively few papers concentrate on the effect of batch size. In this paper, we analyze how increasing batch size affects gradient direction, and propose to evaluate the stability of gradients with their angle change. Based on our observations, the angle change of gradient direction first tends to stabilize (i.e. gradually decrease) while accumulating mini-batches, and then starts to fluctuate. We propose to automatically and dynamically determine batch sizes by accumulating gradients of mini-batches and performing an optimization step at just the time when the direction of gradients starts to fluctuate. To improve the efficiency of our approach for large models, we propose a sampling approach to select gradients of parameters sensitive to the batch size. Our approach dynamically determines proper and efficient batch sizes during training. In our experiments on the WMT 14 English to German and English to French tasks, our approach improves the Transformer with a fixed 25k batch size by +0.73 and +0.82 BLEU respectively.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3519–3524\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n3519\nDynamically Adjusting Transformer Batch Size by\nMonitoring Gradient Direction Change\nHongfei Xu1,2 Josef van Genabith1,2 Deyi Xiong3 Qiuhui Liu4∗\n1Saarland University / Saarland, Germany\n2German Research Center for Artiﬁcial Intelligence / Saarland, Germany\n3Tianjin University / Tianjin, China\n4China Mobile Online Services / Henan, China\nhfxunlp@foxmail.com, Josef.Van Genabith@dfki.de,\ndyxiong@tju.edu.cn, liuqhano@foxmail.com\nAbstract\nThe choice of hyper-parameters affects the per-\nformance of neural models. While much pre-\nvious research (Sutskever et al., 2013; Duchi\net al., 2011; Kingma and Ba, 2015) focuses on\naccelerating convergence and reducing the ef-\nfects of the learning rate, comparatively few\npapers concentrate on the effect of batch size.\nIn this paper, we analyze how increasing batch\nsize affects gradient direction, and propose to\nevaluate the stability of gradients with their an-\ngle change. Based on our observations, the an-\ngle change of gradient direction ﬁrst tends to\nstabilize (i.e. gradually decrease) while accu-\nmulating mini-batches, and then starts to ﬂuc-\ntuate. We propose to automatically and dynam-\nically determine batch sizes by accumulating\ngradients of mini-batches and performing an\noptimization step at just the time when the di-\nrection of gradients starts to ﬂuctuate. To im-\nprove the efﬁciency of our approach for large\nmodels, we propose a sampling approach to\nselect gradients of parameters sensitive to the\nbatch size. Our approach dynamically deter-\nmines proper and efﬁcient batch sizes during\ntraining. In our experiments on the WMT\n14 English to German and English to French\ntasks, our approach improves the Transformer\nwith a ﬁxed 25k batch size by +0.73 and\n+0.82 BLEU respectively.\n1 Introduction\nThe performance of neural models is likely to be\naffected by the choice of hyper-parameters. While\nmuch previous research (Sutskever et al., 2013;\nDuchi et al., 2011; Kingma and Ba, 2015) focuses\non accelerating convergence and reducing the ef-\nfects of the learning rate, comparatively few papers\nconcentrate on the effect of batch size.\nHowever, batch size is also an important hyper-\nparameter, and some batch sizes empirically lead\nto better performance than the others.\n∗ Corresponding author.\nSpeciﬁcally, it has been shown that the perfor-\nmance of the Transformer model (Vaswani et al.,\n2017) for Neural Machine Translation (NMT) (Bah-\ndanau et al., 2015; Gehring et al., 2017; Vaswani\net al., 2017) relies heavily on the batch size (Popel\nand Bojar, 2018; Ott et al., 2018; Abdou et al.,\n2017; Zhang et al., 2019a).\nThe inﬂuence of batch size on performance\nraises the question, how to dynamically ﬁnd proper\nand efﬁcient batch sizes during training? In this\npaper, we investigate the relationship between the\nbatch size and gradients, and propose a dynamic\nbatch size approach by monitoring gradient direc-\ntion changes. Our contributions are as follows:\n•We observe the effects on gradients with in-\ncreasing batch size, and ﬁnd that a large batch\nsize stabilizes the direction of gradients;\n•We propose to automatically determine dy-\nnamic batch sizes in training by monitoring\nthe gradient direction change while accumu-\nlating gradients of small batches;\n•To measure gradient direction change efﬁ-\nciently with large models, we propose an ap-\nproach to dynamically select those gradients\nof parameters/layers which are sensitive to the\nbatch size;\n•In machine translation experiments, our ap-\nproach improves the training efﬁciency and\nthe performance of the Transformer model.\n2 Gradient Direction Change and\nAutomated Batch Size\nGradients indicate the direction and size of param-\neter updates to minimize the loss function in train-\ning. To reveal the effects of the batch size in opti-\nmization, we evaluate its inﬂuence on the direction\nchange of gradients.\n3520\nk 1 2 3 4 5 6 7 8 9 10\nSize 4064 8994 12768 17105 21265 25571 29411 33947 38429 43412\na(gk−1\n0 ,gk\n0 ) 51.52 30.37 27.42 22.61 20.87 19.80 19.59 18.92 19.23\na(gk−3\n0 ,gk\n0 ) 59.53 44.20 41.77 35.34 32.19 32.10 34.29\nTable 1: The direction change of gradients while accumulating mini-batches.\n2.1 Gradient Direction Change with\nIncreasing Batch Size\nTo investigate the inﬂuence of batch size on gradi-\nent direction, we gradually accumulate gradients of\nsmall mini-batches as the gradients of a large batch\nthat consists of those mini-batches, and observe\nhow the direction of gradients varies.\nLet dj\ni : (xj\ni,yj\ni) stands for the large batch con-\ncatenated from the ith mini-batch to the jth mini-\nbatch, where xj\ni and yj\ni are inputs and targets. Then\nthe gradients gj\ni of model parameters θon dj\ni are:\ngj\ni = ∂L(θ,xj\ni,yj\ni)\n∂θ (1)\nIn gradient accumulation, the gradients gk\n0 are\nthe sum of gk−1\n0 and gk\nk:\ngk\n0 = gk−1\n0 + gk\nk (2)\nTo measure the change of gradient direction\nduring accumulation, we regard the two gradients\ngk−1\n0 and gk\n0 as 2 vectors, and compute the angle\na(gk−1\n0 ,gk\n0 ) between them:\na(gk−1\n0 ,gk\n0 ) = arccos( gk−1\n0 •gk\n0\n|gk−1\n0 ||gk\n0 |\n) (3)\nwhere “•” indicates inner-product of vectors.\nWe use the angle of 2 vectors rather than cosine\nsimilarity because:\n•The angle indicates the change between gradi-\nent directions;\n•When the angle is small, a signiﬁcant change\nin the angle only results in a subtle difference\nin cosine similarity.1\nWe observe the gradient direction varying dur-\ning accumulating gradients of a Transformer model\ntraining on the WMT 14 English-German task fol-\nlowing the setting of Vaswani et al. (2017) with a\nbatch size of around 50ktarget tokens. To achieve\nthe gradient of the large batch size, we gradually\n1cos(5◦) ≈ 0.9961, cos(10◦) ≈ 0.9848.\naccumulate gradients of mini-batches with around\n4ktarget tokens.\nTable 1 shows a typical example: (i) gradient\nchange is high at the beginning, (ii) gradient change\nreduces with increasing batch size and (iii) eventu-\nally it will start ﬂuctuating (here at k=10).2\nIntuitively, the less the direction of accumulated\ngradients is moved by the gradients of a new mini-\nbatch, the more certainty there is about the gradient\ndirection. Thus we propose that the magnitude\nof the angle ﬂuctuation relates to the certainty of\nthe model parameter optimization direction, and\nmay therefore serve as a measure of optimization\ndifﬁculty.\n2.2 Automated Batch Size with Gradient\nDirection Change\nTable 1 shows that the optimization direction is less\nstable with a small batch than with a large batch.\nBut after the direction of gradients has stabilized,\naccumulating more mini-batches seems useless as\nthe gradient direction starts to ﬂuctuate.\nThus, we suggest to compute dynamic and ef-\nﬁcient batch sizes by accumulating gradients of\nmini-batches, while evaluating the gradient direc-\ntion change with each new mini-batch, and stop\naccumulating more mini-batches and perform an\noptimization step when the gradient direction ﬂuc-\ntuates.\nIn practice, we only monitor a(gk−1\n0 ,gk\n0 ) for efﬁ-\nciency. We record the minimum angle changeamin\nwhile accumulating gradients, and suppose the gra-\ndient direction starts to ﬂuctuate, stop accumulating\nmore mini-batches when a(gk−1\n0 ,gk\n0 ) >amin ∗α.\nIn this way we can achieve a dynamic batch size\n(the size of dk\n0), where αis a pre-speciﬁed hyper-\nparameter.\n2By comparing\nn∑\ni=0\na(gk−i−1\n0 ,gk−i\n0 ) with a(gk−n−1\n0 ,gk\n0 ),\nwe can ﬁnd the direction changes fromgk−i−1\n0 to gk\n0 are incon-\nsistent. Otherwise,\nn∑\ni=0\na(gk−i−1\n0 ,gk−i\n0 ) ≈ a(gk−n−1\n0 ,gk\n0 ).\n3521\n2.3 Efﬁciently Monitoring Gradient\nDirection Change\nIn practice, a model may have a large amount of\nparameters, and the cost of computing the cosine\nsimilarity between two corresponding gradient vec-\ntors are relatively high. To tackle this issue, we\npropose to divide model parameters into groups,\nand monitor gradient direction change only on a\nselected group in each optimization step. For a\nmulti-layer model, i.e. the Transformer, a group\nmay consist of parameters of 1 layer or several\nlayers.\nTo select the parameter group which is sensitive\nto the batch size, we record the angles of gradient\ndirection change a(g0\n0,g1\n0),...,a (gk−1\n0 ,gk\n0 ) in the\ngradient accumulation, and deﬁne amax and amin\nas the maximum and minimum direction change:\namax = max(a(g0\n0,g1\n0),...,a (gk−1\n0 ,gk\n0 )) (4)\namin = min(a(g0\n0,g1\n0),...,a (gk−1\n0 ,gk\n0 )) (5)\nWe then use ∆ato measure the uncertainty re-\nduction in the optimization direction:\n∆a= amax −amin (6)\nIntuitively, the optimization direction of the pa-\nrameter group which results in a larger ∆aproﬁts\nmore from the batch size, and the group with a\nlarger ∆ashould be more frequently sampled.\nWe average the recent history of ∆ak of the\nkth parameter group into ∆ak. Inspired by Gum-\nbel (1954); Maddison et al. (2014); Zhang et al.\n(2019b), we ﬁrst add Gumble noise to each ∆ak to\nprevent the selection falling into a ﬁxed group:\n∆a∗\nk = ∆ak −log(−log u) (7)\nwhere u∈(0,1) is a uniform distribution.\nThen we zero negative values3 in ∆a∗\n1, ..., ∆a∗\nn\nand normalize them into a probability distribution:\npk = ∆a∗\nk\nβ\nn∑\ni=1\n∆a∗\ni\nβ\n(8)\nWe use pk as the probability to sample the kth\ngroup, and β is a hyper-parameter to sharpen the\nprobability distribution. We do not use softmax\n3∆ak is positive, but after adding Gumble noise, there is a\nsmall possibility that it turns negative. In our case, negative\nvalues only occur very few times.\nBatch Size En-De En-Fr Time\n25k 27.38 39.34 35h21m\n50k 27.93 39.97 60h38m\ndyn 28.11† 40.16† 33h37m\nTable 2: Performance. Time is the training time on the\nWMT 14 En-De task for 100k training steps. †indi-\ncates p< 0.01 in the signiﬁcance test.\nEn-De En-Fr\nmin 7069 8025\navg 26264.19 30248.90\nmax 102165 103352\nTable 3: Statistics of Batch Size.\nbecause it would heavily sharpen the distribution\nwhen the gap between values is large, and makes it\nalmost impossible to select and evaluate the other\ngroups in addition to the one with highest ∆a∗\nk.4\n3 Experiments\nWe implemented our approaches based on the Neu-\ntron implementation (Xu and Liu, 2019) of the\nTransformer translation model. We applied our ap-\nproach to the training of the Transformer, and to\ncompare with Vaswani et al. (2017), we conducted\nour experiments on the WMT 14 English to Ger-\nman and English to French news translation tasks\non 2 GTX 1080Ti GPUs. Hyper parameters were\ntuned on the development set (newstest 2012 and\n2013). We followed all settings of Vaswani et al.\n(2017) except for the batch size. We used a beam\nsize of 4 for decoding, and evaluated case-sensitive\ntokenized BLEU5 with signiﬁcance test (Koehn,\n2004).\nWe used an αof 1.1 to determine the ﬂuctuation\nof gradient direction by default. We regarded each\nencoder/decoder layer as a parameter group, and\nused a βof 3 for the parameter group selection.\n3.1 Performance\nWe compared the results of our dynamic batch size\napproach to two ﬁxed batch size baselines, the 25k\n4For example, the result of softmax over [22, 31, 60] is\n[3.13e-17, 2.54e-13, 1.00], the last element takes almost all\npossibility mass. But we later ﬁnd that if ∆ais normalized\n(∆a = (amax − amin)/amax) in Equation 6, the softmax\nworks comparably well, which avoids using the hyper parame-\nter βin Equation 8.\n5https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ngeneric/multi-bleu.perl\n3522\n \n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\nFigure 1: Distribution of Dynamic Batch Sizes. Values\non y-axis are percentages.\n \n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39\nFigure 2: Minimum Gradient Direction Change during\nTraining. X-axis 2.5k training steps, y averaged amin\n(Equation 5).\nbatch size is the empirical value of Vaswani et al.\n(2017), while Zhang et al. (2019a) investigate 50k\nbatch size. Results are shown in Table 2 with the\nstatistics of batch sizes of our approach shown in\nTable 3 and the detailed distribution of batch sizes\nfor the En-De task shown in Figure 1.\nTable 2 and 3 show that our approach outper-\nforms both the ﬁxed 25k and 50k batch size set-\ntings with an average batch size around 26k, and\nour approach is slightly faster than the 25ksetting\ndespite of the additional cost for monitoring gradi-\nent direction change.6\nFigure 1 shows an interesting fact that the most\nfrequently used automated batch sizes were close\nto the ﬁxed value (25k) of Vaswani et al. (2017).\n3.2 Analysis of Minimum Gradient Direction\nChange\nIn order to observe the varying of minimum gradi-\nent direction change during training, we averaged\nthe minimum angle for every 2.5ktraining steps.\n6It is hard to accumulate an accurate 25ktarget tokens in\na batch, and in fact, the ﬁxed 25ksetting results in an average\nbatch size of 26729.79.\nα\nBatch Size\nBLEU Timeavg max\n1.0 19367.76 60945 27.90 24h50m\n1.1 26264.19 102165 28.11 33h37m\n1.2 36208.47 164908 28.39 46h04m\n1.3 51470.34 205210 28.37 63h56m\nTable 4: Effects of Different α.\nResults are shown in Figure 2.\nFigure 2 shows that the minimum direction\nchange of gradients was small at the beginning,\nand gradually increased with training. Given that\na small angle change indicates that there is more\ncertainty in the gradient direction, this observation\nis consistent with the fact that ﬁnding the optimiza-\ntion direction is harder and harder with training.\n3.3 Effects of α\nWe studied the effects of differentαvalues on the\nEn-De task, and results are shown in Table 4.7\nTable 4 shows that with increasingα, the average\nbatch size and the time cost increases along with\nthe performance. A wide range of values works\nrelatively well indicating that its selection is robust,\nand 1.1 seems to be a good trade off between the\ncost and the performance in our experiments.8 It is\nalso worth noting that α= 1 outperforms the 25k\nbaseline while being 1.42 times faster (Table 2).\n4 Related Work\nPopel and Bojar (2018) demonstrate that the batch\nsize affects the performance of the Transformer,\nand a large batch size tends to beneﬁt performance,\nbut they use ﬁxed batch sizes during training. Ab-\ndou et al. (2017) propose to use a linearly increas-\ning batch size from 65 to 100 which slightly outper-\nforms their baseline. Smith et al. (2018) show that\nthe same learning curve on both training and test\nsets can be obtained by increasing the batch size\nduring training instead of decaying the learning\nrate.\nFor fast convergence, Balles et al. (2017) pro-\npose to approximately estimate the mean value of\nthe batch size for the next batch by maximizing\nthe expected gain with a sample gradient variance\n(||g||2) computed on the current batch, while our\n7We observed that the minimum batch size does not change\nsigniﬁcantly with increasing α, so we omit it for space.\n8For α= 1.2 on the En-Fr task, the corresponding values\nare: 44294.16, 185972, 40.35 and 54h12m.\n3523\napproach compares the gradient direction of change\n(a(gk−1\n0 ,gk\n0 )) during accumulation of mini-batches\nin the assembling of a large batch.\nWe suggest our approach is complementary to\nSutskever et al. (2013); Duchi et al. (2011); Kingma\nand Ba (2015), as their approaches decide the mag-\nnitude of the move in the optimization direction,\nwhile our approach provides reliable gradient di-\nrection.\n5 Conclusion\nIn this paper, we analyze the effects of accumulated\nbatches on the gradient direction, and propose to\nachieve efﬁcient automated batch sizes by monitor-\ning change in gradient accumulation and perform-\ning an optimization step when the accumulated\ngradient direction is almost stable. To improve the\nefﬁciency of our approach with large models, we\npropose a sampling approach to select gradients of\nparameters sensitive to the batch size.\nOur approach improves the Transformer with a\nﬁxed 25k batch size by +0.73 and +0.82 BLEU\non the WMT 14 English to German and English\nto French tasks respectively while preserving efﬁ-\nciency.\nAcknowledgments\nWe thank anonymous reviewers for their insight-\nful comments. Hongfei Xu acknowledges the sup-\nport of China Scholarship Council ([2018]3101,\n201807040056). Deyi Xiong is supported by\nthe National Natural Science Foundation of\nChina (Grant No. 61861130364), the Natu-\nral Science Foundation of Tianjin (Grant No.\n19JCZDJC31400) and the Royal Society (London)\n(NAF\\R1\\180122). Hongfei Xu and Josef van\nGenabith are supported by the German Federal\nMinistry of Education and Research (BMBF) un-\nder the funding code 01IW17001 (Deeplee).\nReferences\nMostafa Abdou, Vladan Glon ˇc´ak, and Ond ˇrej Bojar.\n2017. Variable mini-batch sizing and pre-trained\nembeddings. In Proceedings of the Second Con-\nference on Machine Translation , pages 680–686,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nLukas Balles, Javier Romero, and Philipp Hennig.\n2017. Coupling adaptive batch sizes with learning\nrates. In Proceedings of the Thirty-Third Conference\non Uncertainty in Artiﬁcial Intelligence, UAI 2017,\nSydney, Australia, August 11-15, 2017. AUAI Press.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. Journal of Machine\nLearning Research, 12:2121–2159.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1243–1252, International\nConvention Centre, Sydney, Australia. PMLR.\nEmil Julius Gumbel. 1954. Statistical theory of ex-\ntreme values and some practical applications. NBS\nApplied Mathematics Series, 33.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceed-\nings of the 2004 Conference on Empirical Meth-\nods in Natural Language Processing , pages 388–\n395, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nChris J Maddison, Daniel Tarlow, and Tom Minka.\n2014. A ∗sampling. In Z. Ghahramani, M. Welling,\nC. Cortes, N. D. Lawrence, and K. Q. Weinberger,\neditors, Advances in Neural Information Processing\nSystems 27 , pages 3086–3094. Curran Associates,\nInc.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation: Research Papers , pages 1–9,\nBrussels, Belgium. Association for Computational\nLinguistics.\nMartin Popel and Ond ˇrej Bojar. 2018. Training tips\nfor the transformer model. The Prague Bulletin of\nMathematical Linguistics, 110:43–70.\nSamuel L. Smith, Pieter-Jan Kindermans, and Quoc V .\nLe. 2018. Don’t decay the learning rate, increase the\nbatch size. In International Conference on Learning\nRepresentations.\nIlya Sutskever, James Martens, George Dahl, and Geof-\nfrey Hinton. 2013. On the importance of initializa-\ntion and momentum in deep learning. In Proceed-\nings of the 30th International Conference on Ma-\n3524\nchine Learning, volume 28 of Proceedings of Ma-\nchine Learning Research, pages 1139–1147, Atlanta,\nGeorgia, USA. PMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nHongfei Xu and Qiuhui Liu. 2019. Neutron: An Im-\nplementation of the Transformer Translation Model\nand its Variants. arXiv preprint arXiv:1903.07402.\nBiao Zhang, Ivan Titov, and Rico Sennrich. 2019a.\nImproving deep transformer with depth-scaled ini-\ntialization and merged attention. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 898–909, Hong\nKong, China. Association for Computational Lin-\nguistics.\nWen Zhang, Yang Feng, Fandong Meng, Di You, and\nQun Liu. 2019b. Bridging the gap between train-\ning and inference for neural machine translation. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4334–\n4343, Florence, Italy. Association for Computational\nLinguistics.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6731226444244385
    },
    {
      "name": "Computer science",
      "score": 0.6713159084320068
    },
    {
      "name": "Batch processing",
      "score": 0.5141094326972961
    },
    {
      "name": "Convergence (economics)",
      "score": 0.4464791715145111
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33758431673049927
    },
    {
      "name": "Biological system",
      "score": 0.32614588737487793
    },
    {
      "name": "Voltage",
      "score": 0.241043359041214
    },
    {
      "name": "Physics",
      "score": 0.09569081664085388
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I33256026",
      "name": "German Research Centre for Artificial Intelligence",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I91712215",
      "name": "Saarland University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I180662265",
      "name": "China Mobile (China)",
      "country": "CN"
    }
  ]
}