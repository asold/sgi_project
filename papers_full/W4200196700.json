{
    "title": "Deep learning with language models improves named entity recognition for PharmaCoNER",
    "url": "https://openalex.org/W4200196700",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5102828553",
            "name": "Cong Sun",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5051597671",
            "name": "Zhihao Yang",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100736263",
            "name": "Lei Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100343737",
            "name": "Yin Zhang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5023931221",
            "name": "Hongfei Lin",
            "affiliations": [
                "Dalian University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100370452",
            "name": "Jian Wang",
            "affiliations": [
                "Dalian University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2610394652",
        "https://openalex.org/W2145870108",
        "https://openalex.org/W4213016248",
        "https://openalex.org/W6681542663",
        "https://openalex.org/W2168041406",
        "https://openalex.org/W6684484140",
        "https://openalex.org/W2985294119",
        "https://openalex.org/W2168990503",
        "https://openalex.org/W2101553882",
        "https://openalex.org/W6600654476",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W6697155078",
        "https://openalex.org/W2205981794",
        "https://openalex.org/W2963428321",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W6748634344",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2986178925",
        "https://openalex.org/W6769577755",
        "https://openalex.org/W2983842045",
        "https://openalex.org/W4234762960",
        "https://openalex.org/W2946558277",
        "https://openalex.org/W2989464093",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W4385681388",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2949445525",
        "https://openalex.org/W2963997788"
    ],
    "abstract": "Abstract Background The recognition of pharmacological substances, compounds and proteins is essential for biomedical relation extraction, knowledge graph construction, drug discovery, as well as medical question answering. Although considerable efforts have been made to recognize biomedical entities in English texts, to date, only few limited attempts were made to recognize them from biomedical texts in other languages. PharmaCoNER is a named entity recognition challenge to recognize pharmacological entities from Spanish texts. Because there are currently abundant resources in the field of natural language processing, how to leverage these resources to the PharmaCoNER challenge is a meaningful study. Methods Inspired by the success of deep learning with language models, we compare and explore various representative BERT models to promote the development of the PharmaCoNER task. Results The experimental results show that deep learning with language models can effectively improve model performance on the PharmaCoNER dataset. Our method achieves state-of-the-art performance on the PharmaCoNER dataset, with a max F1-score of 92.01%. Conclusion For the BERT models on the PharmaCoNER dataset, biomedical domain knowledge has a greater impact on model performance than the native language (i.e., Spanish). The BERT models can obtain competitive performance by using WordPiece to alleviate the out of vocabulary limitation. The performance on the BERT model can be further improved by constructing a specific vocabulary based on domain knowledge. Moreover, the character case also has a certain impact on model performance.",
    "full_text": "Deep learning with language models \nimproves named entity recognition \nfor PharmaCoNER\nCong Sun1, Zhihao Yang1*, Lei Wang2*, Yin Zhang2, Hongfei Lin1 and Jian Wang1 \nFrom The 5th workshop on BioNLP Open Shared Tasks Hong Kong, China. 4 November 2019\nAbstract \nBackground: The recognition of pharmacological substances, compounds and \nproteins is essential for biomedical relation extraction, knowledge graph construction, \ndrug discovery, as well as medical question answering. Although considerable efforts \nhave been made to recognize biomedical entities in English texts, to date, only few lim-\nited attempts were made to recognize them from biomedical texts in other languages. \nPharmaCoNER is a named entity recognition challenge to recognize pharmacological \nentities from Spanish texts. Because there are currently abundant resources in the field \nof natural language processing, how to leverage these resources to the PharmaCoNER \nchallenge is a meaningful study.\nMethods: Inspired by the success of deep learning with language models, we com-\npare and explore various representative BERT models to promote the development of \nthe PharmaCoNER task.\nResults: The experimental results show that deep learning with language models can \neffectively improve model performance on the PharmaCoNER dataset. Our method \nachieves state-of-the-art performance on the PharmaCoNER dataset, with a max \nF1-score of 92.01%.\nConclusion: For the BERT models on the PharmaCoNER dataset, biomedical domain \nknowledge has a greater impact on model performance than the native language (i.e., \nSpanish). The BERT models can obtain competitive performance by using WordPiece to \nalleviate the out of vocabulary limitation. The performance on the BERT model can be \nfurther improved by constructing a specific vocabulary based on domain knowledge. \nMoreover, the character case also has a certain impact on model performance.\nKeywords: Named entity recognition, NER, Language model, BERT, Text mining\nOpen Access\n© The Author(s), 2021. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602 \nhttps://doi.org/10.1186/s12859-021-04260-y\n*Correspondence:   \nyangzh@dlut.edu.cn; wanglei.\nwlbihami@gmail.com \n1 School of Computer \nScience and Technology, \nDalian University \nof Technology, Dalian, China\n2 Beijing Institute of Health \nAdministration and Medical \nInformation, Beijing, China\nPage 2 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\nBackground\nEffectively recognizing biomedical entities from texts is of great value to biomedical \nresearch [1]. With the rapid increase in literature scale, it is no longer possible to rec -\nognize biomedical entities from texts through manual annotations. Therefore, using \nnatural language processing (NLP) methods to recognize these entities automatically \nhas attracted plenties of attention. Biomedical named entity recognition (BioNER) is \nsuch an NLP task. The importance of biomedical entity recognition motivated several \nshared tasks, such as the CHEMDNER track [2], the SemEval challenge [3], and the \ni2b2 challenge [4]. Most biomedical and clinical NLP studies are conducted on English \ntexts, while only few works are done using non-English texts. However, it is essential \nto note that many texts are published in non-English, especially in clinical case reports, \nmostly written in the native language. Therefore, it is necessary to recognize biomedi -\ncal named entities in non-English literature. PharmaCoNER [5] is the first BioNER chal -\nlenge devoted to recognizing chemical and protein entities from biomedical literature in \nSpanish. The primary purpose is to promote non-English BioNER tools, determine the \nbest performing method, and compare the systems that obtain state-of-the-art (SOTA) \nperformance [5]. The PharmaCoNER challenge consists of two sub-tracks: NER offset \nand entity classification and concept indexing. In this work, we only focus on the first \nsub-track.\nIn the previous works, the implementation of BioNER methods [6, 7] mainly depended \non feature engineering, i.e., using various NLP tools and external resources to construct \nfeatures. This is a skill-dependent and laborious task. To overcome the limitations, neu -\nral network methods with automatic feature learning abilities have been widely pro -\nposed [8–11]. These methods use pre-trained word embeddings [12–14] to learn the \nsemantic information of each word and combine neural network models such as LSTMs \nand CNNs to encode the context information to implement BioNER tasks. However, \nonce the word embeddings are pre-trained, the word will be mapped to a specific vec -\ntor, and therefore, the word embeddings can only learn context-independent representa-\ntions. Recently, neural language models [15–17] have improved the performance of NLP \nmethods to a new level. Unlike traditional word embeddings such as Word2Vec [12, 13] \nand GloVe [14], the word embeddings pre-trained by language models depend on the \ncontext. Therefore, the same word can have different semantic information in different \ncontexts. Due to the great success of language models, it has gradually developed into \nthe mainstream method to implement BioNER tasks.\nDuring the PharmaCoNER challenge, a total of 22 teams participated in the NER \nsharing task, and the top three models ranked by performance were all based on lan -\nguage models. Specifically, Xiong et al. [18] achieved the best performance, reaching an \nF1-score of 91.05%. In their approach, they first employed Multilingual BERT [17] as \nlanguage representations, and then combined the character-level representation, part-\nof-speech (POS) representation and word shape representation of each word to the \nBERT representation. Finally, a conditional random field (CRF) layer is appended to \nthese representations for the BioNER task. Stoeckel et al. [19] obtained the second-best \nperformance. They trained a BiLSTM-CRF sequence tagger with stacked pooled con -\ntextualized embeddings, word embeddings and sub-word embeddings using the FLAIR \nframework [16, 20]. Sun et al. [21] leveraged Multilingual BERT [17] and BioBERT [22] \nPage 3 of 16\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n \nto implement solutions for the PharmaCoNER challenge, and their solutions obtained \nthird-place performance. From the PharmaCoNER challenge, neural language mod -\nels, especially BERT, obtain SOTA performance in the NER task. Compared with other \nmethods (i.e., CRF and BiLSTM-CRF), neural language models can effectively learn \nlatent context information and improve model performance. BERT has become the most \nrepresentative language model with its powerful performance and abundant resources \namong these language models. Leveraging existing BERTs to obtain SOTA performance \nhas important research implications for non-English NER tasks with fewer resources. \nAlthough some BERT models have been employed during the PharmaCoNER challenge, \nthere are still many representative BERT models in the NLP community that have not \nbeen explored. In this article, we compare and explore the impact of these BERTs on the \nPharmaCoNER corpus.\nMethods\nPharmaCoNER\nThe goal of the PharmaCoNER task is to recognize chemical and protein entities from \na given input sentence or article in Spanish. The PharmaCoNER corpus is a partial col -\nlection of the Spanish Clinical Case Corpus (SPACCC). It contains 1000 clinical cases, \nof which 500 are used as the training set, 250 as the development set, and 250 as the test \nset. Each clinical case is composed of two standoff-style annotation documents, i.e., a \n‘txt’ document used for describing the clinical record, and an ‘ann’ document used for \ntagging biomedical entities of the case. In this work, the input of the BERT model is sen -\ntences, which are obtained by splitting the documents from the PharmaCoNER corpus \naccording to sentence symbols (e.g. ‘.!?’). There are three types of entities to be evaluated \nin the PharmaCoNER corpus, namely ‘NORMALIZABLES’ entities, ‘NO_NORMAL -\nIZABLES’ entities, and ‘PROTEINAS’ entities. The ‘NORMALIZABLES’ entities repre -\nsent chemical entities that can be manually standardized as unique concept identifiers \n(primarily SNOMED-CT). The ‘NO_NORMALIZABLES’ entities represent chemical \nentities that cannot be manually standardized as unique concept identifiers. The ‘PRO -\nTEINAS’ entities denote protein and gene entities that can be annotated according to \nthe BioCreative GPRO track guidelines [23], and it also includes peptides, peptide hor -\nmones and antibodies. Furthermore, the PharmaCoNER corpus also contains a type of \n‘UNCLEAR’ entities, which denote general substance entities of clinical or biomedical \nrelevance, including pharmaceutical formulations, general treatments, chemotherapy \nprograms, vaccines. The ‘UNCLEAR’ entities are not used to evaluate the Pharma -\nCoNER task but as additional annotations of biomedical relevance. Table 1 illustrates the \nstatistical information of the PharmaCoNER corpus.\nFigure 1 shows the flowchart of our approach. We use Begin, Inside, Outside (BIO) \nscheme to tag the input sequence and formulate the PharmaCoNER task as a multi-\nclass classification problem. Take the “C1q y fibrinógeno fueron negativos. ” sentence \nfrom the training set as an example. Because ‘C1q’ and ‘fibrinógeno’ are ‘PRO -\nTEINAS’ entities and other tokens are not biomedical entities, the corresponding BIO \ntags can be expressed as “B-PROTEINAS O B-PROTEINAS O O O” . Moreover, BERT \nuses WordPiece to alleviate the out-of-vocabulary (OOV) problem. Therefore, in the \ntraining phase, the input sentence needs to be further processed by the WordPiece \nPage 4 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\ntokenizer, and the final processed tokens are used as the model input. Correspond -\ningly, the BIO tags predicted by the BERT model also need to be processed by De-\nWordPiece to obtain the BIO tags of the original sentence in the test phase. Formally, \nTable 1 The statistical information of the PharmaCoNER corpus\nSet Training Development Test Total\nDocuments 500 250 250 1000\nSentences 7003 3454 3403 13860\nNORMALIZABLES 2304 1121 973 4398\nNO_NORMALIZABLES 24 16 10 50\nPROTEINAS 1405 745 859 3009\nUNCLEAR 89 44 34 167\nFig. 1 The processing flowchart of our approach\nPage 5 of 16\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n \ngiven an input sequence S  = {w 1 ,··· ,w i,··· ,w n} , the objective of PharmaCoNER is \nto estimate the probability P (t|w i) , where w i is the i -th word/token, T  = {O, B-NOR -\nMALIZABLES, I-NORMALIZABLES, B-NO_NORMALIZABLES, I-NO_NORMAL -\nIZABLES, B-PROTEINAS, I-PROTEINAS}, t ∈ T  , and 1 ≤ i ≤ n .\nThe performance on the PharmaCoNER challenge is measured with the precision \n(P), recall (R), and micro-averaged F1-score (F1). The formulas are:\nwhere TP, FP and FN denote true positive, false positive, and false negative, respectively.\nBERT architecture\nBERT [17], which stands for bidirectional encoder representations from Transform -\ners [24], is a contextualized word representation model. It aims to pre-train a deep \nbidirectional context representation based on the left and right contexts of all layers. \nBecause BERT has been widely used in various NLP tasks, and our implementation \nis effectively identical to the original, we refer readers to read the original paper [17] \nfor more details about BERT. In this work, we only use the BERT model to imple -\nment solutions for the PharmaCoNER task. Figure  2 shows the architecture of the \nBERT model on the PharmaCoNER task. The BERT model first uses the WordPiece \ntokenizer [25] to tokenize the input sentence and adds unique tokens ‘[CLS]’ and \n‘[SEP]’ to indicate the head and tail of the sentence. Then, the representation of each \ntoken in the input sentence is constructed by summing the corresponding token, seg -\nment, and position embeddings, and further fed into multiple layers of Transformers. \nNote that the segment embeddings can use different values to distinguish whether the \ninput sequence is a single sentence or a sentence pair. We only use the single sentence \nas the model input in the experiments, so the segment embeddings share the same \nvalue. Afterward, the hidden representations of the L -th layer of the BERT model (the \nnumber of BERT layers is denoted as L ) are used by the softmax function to predict \ntoken classifications. Finally, the BERT model predicts the BIO tags of the original \nsentence after the De-WordPiece process.\nAccording to different scales, BERT provides two model sizes: BERT BASE and \nBERTLARGE . For each model size, the number of layers L , the hidden size H , and the \nnumber of self-attention heads A  are listed as follows:\n• BERTBASE : L=12, H=768, A=12, Total Parameters= 110M.\n• BERTLARGE : L=24, H=1024, A=16, Total Parameters= 340M.\n(1)P = TP\nTP + FP\n(2)R = TP\nTP + FN\n(3)F1 = 2 ·P ·R\nP + R\nPage 6 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\nDue to the limitation of computing resources, the BERT BASE model is more widely \nused than the BERT LARGE model. Therefore, we mainly explore the BERT BASE model \nin this research.\nPre‑training procedure\nBERT is pre-trained using two unsupervised prediction tasks, masked language model \n[26] and next sentence prediction. The masked language model predicts randomly \nmasked words in the input sequence and, therefore, can be used to learn bidirectional \nrepresentations. The next sentence prediction can be employed to learn the relationship \nbetween sentences. As a general-purpose language representation model, the original \nBERT model was pre-trained on English Wikipedia (2.5B words) and BooksCorpus (0.8B \nwords) [27]. However, biomedical texts contain a large number of biomedical entities \n(e.g., ‘3-(4,5-dimethylthiazol-2yl)-2,5-diphenyltetrazolium bromide’ , ‘nitrato de plata’), \nwhich are generally only understood by specific researchers. Therefore, the performance \non models designed for general English understanding may not be satisfactory. To solve \nthis problem, biomedical researchers use the corpus of their domain to pre-train the \nBERT model. As a result, many different BERT models have appeared in the NLP com -\nmunity based on diverse pre-training corpus or methods. Figure  3 illustrates the repre -\nsentative BERT models, and Table 2 shows the detailed comparison of these models.\nFig. 2 The architecture of the BERT model\nPage 7 of 16\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n \nFig. 3 Overview of the pre-training process of various BERT models. Panels adapted from Lee et al. [22]\nTable 2 Comparison of existing BERTs\nModel Corpus combination Vocabulary\nBERT(Cased) Wiki+Books(Original) BERT\nBERT(Uncased) Wiki+Books(Original) BERT\nNCBI BERT(+P ,Uncased) Original+PubMed BERT\nNCBI BERT(+P+M,Uncased) Original+PubMed+MIMIC-III BERT\nSpanish BERT(Cased) Original+Spanish Wikipedia+OPUS Spanish BERT\nSpanish BERT(Uncased) Original+Spanish Wikipedia+OPUS Spanish BERT\nMultiBERT(Cased) Multilingual Wikipedia MultiBERT\nMultiBERT(Uncased) Multilingual Wikipedia MultiBERT\nSciBERT(BertVoc,Cased) Original+Biomedical+Scientific BERT\nSciBERT(BertVoc,Uncased) Original+Biomedical+Scientific BERT\nSciBERT(SciVob,Cased) Original+Biomedical+Scientific SciBERT\nSciBERT(SciVob,Uncased) Original+Biomedical+Scientific SciBERT\nBioBERTv1.0(+P ,Cased) Original+PubMed BERT\nBioBERTv1.0(+PMC,Cased) Original+PMC BERT\nBioBERTv1.0(+P+PMC,Cased) Original+PubMed+PMC BERT\nBioBERTv1.1(+P ,Cased) Original+PubMed BERT\nPage 8 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\nNCBI BERT\nNCBI BERT [28] is an uncased BERT model pre-trained using biomedical domain \ncorpora (PubMed or MIMIC-III). It uses the original BERT model to initialize the \nweights and further exploits its vocabulary, sequence length, and other configurations \nto pre-train the model. There are two versions of NCBI BERT based on BERT BASE , \nnamely NCBI BERT(P ,Uncased) and NCBI BERT(P+ M,Uncased), where ‘P’ denotes \nPubMed and ‘M’ denotes MIMIC-III, respectively. The NCBI BERT(P ,Uncased) model \nwas pre-trained with 5M steps on PubMed, and the NCBI BERT(P+ M,Uncased) \nmodel was pre-trained with 5M steps on PubMed and 0.2M steps on MIMIC-III.\nSpanish BERT\nSpanish BERT (also called es-BERT) [29] is a BERT model pre-trained on a large \nSpanish general domain corpus. This BERT model is slightly different from BERT BASE , \nand it has 12 transformer layers with 16 self-attention heads each layer, using 1024 \nas the hidden size. For pre-training Spanish BERT, the authors leveraged all the data \nfrom Spanish Wikipedia and all the sources of the OPUS Project [30] that have text in \nSpanish.\nMultilingual BERT\nMultilingual BERT [17] is a BERT BASE model pre-trained using the top 104 languages \nin Wikipedia, and its model structure is the same as BERT BASE . Furthermore, Multi -\nlingual BERT uses a 110k shared WordPiece vocabulary as its vocabulary. Because the \nsize of Wikipedia for a given language varies greatly, low-resource languages may be \n“under-represented” in terms of the neural network model under the assumption that \nlanguages compete for limited model capacity. To overcome this limitation, Multilin -\ngual BERT performed exponentially smoothed weighting of the data during the pre-\ntraining phase to balance the sampling of high-resource languages and low-resource \nlanguages. As a result, high-resource languages like English will be under-sampled, \nand low-resource languages like Icelandic will be over-sampled.\nSciBERT\nSciBERT [31] is a pre-trained contextualized language model based on BERT BASE to \naddress the lack of high-quality, large-scale labeled scientific data. It exploits unsu -\npervised pre-training on a large computer science domain and biomedical domain \ncorpora to improve performance on downstream NLP tasks. The authors of SciBERT \nused the original BERT model to train SciBERT with the same configuration and size \nas BERT BASE . They trained four different versions in total based on cased/uncased \ncharacter and BERT/SciBERT vocabulary. The models using SciBERT vocabulary are \npre-trained from scratch, while the models using BERT vocabulary are initialized \nfrom BERT weights.\nBioBERT\nBioBERT [22] is another BERT model trained on biomedical domain corpora \n(e.g., PubMed and PMC), and its structure is basically the same as BERT BASE . \nPage 9 of 16\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n \nBioBERT uses the original vocabulary of BERT BASE as its vocabulary, and it is \na cased model. There are four versions of BioBERT based on different corpora \nfor pre-training, namely BioBERTv1.0(+ P ,Cased), BioBERTv1.0(+ PMC,Cased), \nBioBERTv1.0(+P+PMC,Cased), and BioBERTv1.1(+ P ,Cased), where ‘P’ means \nPubMed, and ‘+ ’ denotes a new corpus in addition to BooksCorpus and English \nWikipedia. Specifically, BioBERTv1.0(+ P+PMC,Cased) is a version pre-trained \non 470K steps. When using both the PubMed and PMC corpora, the authors of \nBioBERT found that 200K and 270K pre-training steps were optimal for Pub -\nMed and PMC, respectively. Therefore, the ablated versions which were pre-\ntrained on only PubMed for 200K steps (i.e., BioBERTv1.0(+ P ,Cased)) and PMC \nfor 270K steps (BioBERTv1.0(+ PMC,Cased)) were provided. Moreover, the authors \nalso provided a BioBERT version pre-trained on PubMed for 1M steps, namely \nBioBERTv1.1(+P ,Cased).\nFine‑tuning procedure\nWith minimal architectural modification, various existing BERT models can be used \nfor downstream NLP tasks. As shown in Fig.  2, BERT in the figure represents a BERT \nmodel pre-trained using specific corpora (e.g., BioBERT, SciBERT). In this work, we \nuse the PharmaCoNER dataset to fine-tune the BERT model. Specifically, the sen -\ntence processed by the WordPiece tokenizer is used as the input to the BERT model \nin the training phase. The BERT model learns the input feature of each token and \ndynamically tune model parameters, and then classify each token through the Soft -\nmax function. The BIO tag of each input token/word can be obtained after the De-\nWordPiece process. The cross-entropy loss function calculates the loss value between \nthe predicted token tags and the ground-truth tags at the training time. Finally, as \nshown in Fig.  1, the fine-tuned BERT predicts the final token BIO tags based on the \ninput test sentences in the test phase.\nResults and discussion\nExperimental settings\nIn the experiments, all BERTs are implemented using the transformer framework \n(https:// github. com/ huggi ngface/ trans forme rs) based on the PyTorch library (https:// \npytor ch. org). For fair comparisons, we repeat the same experiment five times with the \nsame hyper-parameters, and report the max and average precision, recall, F1-score, as \nwell as the standard deviation. Like most participating teams, we also combined the \noriginal training set and development set as the training set. Then we randomly sam -\npled 10% of the training set as the validation set to tune the hyper-parameters. Spe -\ncifically, the training set and validation set consisted of 9411 and 1046 sentences as \nthe input in our experiments, respectively. The test set is only used to test the model, \nwith 3403 sentences as the model input. The detailed experimental settings are listed \nin Table  3. Note that the sequence length is expressed as the maximum word/token \nlength of each sentence allowed by the model.\nPage 10 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\nExperimental results\nTable 4 shows the experimental results in detail. The first two methods are provided by \nthe PharmaCoNER organizers. These two methods are based on the PharmaCoNER \ntagger [32], a neural network (LSTM-CRF) based tool for automatically recognizing \nchemical and protein entities in Spanish medical texts. The Baseline-Glove used word \nembeddings trained by GloVe [14] on the Spanish Billion Word Corpus, and the Base -\nline-Med leveraged word embeddings from the Medical Word Embeddings for Spanish \n[33]. Baseline-Glove and Baseline-Med obtain F1-scores of 82.11% and 85.34%, respec -\ntively. These experimental results demonstrate that the performance of combining tradi-\ntional word embeddings and LSTM-CRF to implement solutions for the PharmaCoNER \nchallenge is not satisfactory. In addition to the first two methods, the others are all lan -\nguage model-based methods. Sun et al. [21] employed Multilingual BERT and obtain an \nF1-score of 89.24% during the PharmaCoNER challenge. Stoeckel et al. [19] combined \na BiLSTM-CRF sequence tagger with pooled contextualized embeddings, word embed -\ndings and sub-word embeddings using the framework FLAIR. Their method obtains an \nF1-score of 90.52%. Xiong et al. [18] combined Multilingual BERT, character-level rep -\nresentation, POS representation and word-shape representation to achieve results on \nthe PharmaCoNER challenge. Their method obtains an F1-score of 91.05%. It can be \nseen that language models are of great value to the PharmaCoNER challenge. Whether \nit is through the use of contextualized character embeddings (e.g., Stoeckel’s work) or \ncontext word representations (e.g., Sun’s work and Xiong’s work), language models can \ngreatly increase the ability to recognize biomedical entities in Spanish texts. Further -\nmore, note that all these works during the challenge were submitted blindly (i.e., the test \nTable 3 Detailed experimental settings\nParameters Tune range Optimal\nSequence length [128, 256, 300] 300\nTrain batch size [8, 16, 32] 16\nDev batch size 16 16\nTest batch size 16 16\nLearning rate [1e−05, 2e−05, 3e−05] 2e−05\nEpoch number [10, 20, 30, 50] 20\nWarmup 0.1 0.1\nDropout 0.1 0.1\nTable 4 Performance comparison on the PharmaCoNER dataset\n‘P’ denotes PubMed\nThe highest values are shown in bold\nMethod P (%) R (%) F1 (%)\nBaseline-Glove [32] 83.26 81.00 82.11\nBaseline-Med [32] 87.02 83.71 85.34\nSun et al. [21] 90.46 88.06 89.24\nStoeckel et al. [19] 90.79 90.30 90.52\nXiong et al. [18] 91.23 90.88 91.05\nOur method (BioBERTv1.1(+P ,Cased)) 92.44 91.59 92.01\nPage 11 of 16\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n \nset unknown). In this work, we employed BioBERTv1.1(+P ,Cased) to generate biomedi-\ncal contextualized representations to implement solutions for the PharmaCoNER task. \nOur method achieves the best F1-score of 92.01% from five runs, which is currently the \nbest performance on the PharmaCoNER dataset. These experimental results show that \nthe domain pre-training of language models is important for the PharmaCoNER task. \nThe SOTA performance can be obtained by BioBERT using only biomedical domain \nknowledge and the WordPiece tokenizer.\nPerformance of different BERTs\nIn this section, we further explore the impact of pre-training on BERT from four aspects: \ndomain corpus, language, vocabulary, and character case. Table  5 shows the perfor -\nmance comparison of various BERT models. The BERT model can be regarded as a base-\nline model. It can be seen that the BERT model pre-trained using the biomedical domain \ncorpus (e.g., SciBERT and BioBERT) or native language (e.g., MultiBERT and Spanish \nBERT) achieves higher performance than the BERT model pre-trained using the English \ngeneral corpus. This experimental result shows that using the biomedical domain corpus \nor native language (i.e., Spanish) to pre-train BERT can improve model performance on \nthe PharmaCoNER task. Compared with MultiBERT and Spanish BERT, the best ver -\nsion of SciBERT and BioBERT can obtain higher performance. This shows that domain \nknowledge is more helpful to improve model performance compared with the native \nlanguage. Furthermore, we also observe an interesting experimental result, i.e., the per -\nformance of NCBI BERT is even lower than the original BERT. It may be caused by the \nTable 5 Performance comparison of various BERTs\n‘P’ and ‘M’ denote PubMed and MIMIC-III, respectively. The table is sorted according to the average F1-score, and the highest \nvalues are shown in bold\n*Significant difference between the means of two models according to the T-TEST statistical test. Specifically, it indicates \nthe model has a significant difference compared with BioBERTv1.1(+P ,Cased), with more than 95% confidence interval ( p < \n0.05)\nMethod Mean ± SD Max\nP (%) R (%) F1 (%) P (%) R (%) F1 (%)\nBERT(Cased) 89.31 ± 0.26 88.00 ± 0.16 88.65 ± 0.12 89.51 88.06 88.78∗\nBERT(Uncased) 89.60 ± 0.81 88.13 ± 0.40 88.86 ± 0.57 90.32 88.65 89.48∗\nNCBI BERT(P+M,Uncased) 89.29 ± 0.67 87.11 ± 0.60 88.18 ± 0.35 89.58 87.30 88.42∗\nNCBI BERT(P ,Uncased) 90.20 ± 0.38 88.88 ± 0.52 89.53 ± 0.37 90.76 89.58 90.16∗\nSpanish BERT(Uncased) 89.69 ± 0.74 90.56 ± 0.58 90.12 ± 0.37 90.47 90.72 90.59∗\nSpanish BERT(Cased) 90.42 ± 0.77 90.51 ± 0.69 90.47 ± 0.69 91.76 91.31 91.54\nMultiBERT(Cased) 89.53 ± 0.27 89.99 ± 0.43 89.76 ± 0.19 89.75 90.34 90.04∗\nMultiBERT(Uncased) 90.74 ± 0.35 90.39 ± 0.37 90.56 ± 0.25 91.02 90.77 90.89\nSciBERT(Bertvoc,Cased) 90.36 ± 0.75 89.55 ± 0.30 89.96 ± 0.40 91.66 89.52 90.58∗\nSciBERT(Bertvoc,Uncased) 91.07 ± 0.71 89.00 ± 0.45 90.02 ± 0.55 91.85 89.36 90.59∗\nSciBERT(Scivoc,Uncased) 90.75 ± 0.86 90.27 ± 0.32 90.51 ± 0.40 92.03 90.28 91.15\nSciBERT(Scivoc,Cased) 91.25 ± 0.69 90.30 ± 0.58 90.77 ± 0.40 92.40 89.74 91.05\nBioBERTv1.0(+PMC,Cased) 90.54 ± 0.71 89.59 ± 0.31 90.06 ± 0.45 91.09 89.90 90.49∗\nBioBERTv1.0(+P ,Cased) 90.44 ± 0.34 89.98 ± 0.64 90.21 ± 0.36 90.75 90.55 90.65∗\nBioBERTv1.0(+P+PMC,Cased) 91.08 ± 0.86 89.76 ± 0.52 90.41 ± 0.42 91.13 90.34 90.73\nBioBERTv1.1(+P ,Cased) 91.40 ± 0.81 90.90 ± 0.47 91.15 ± 0.60 92.44 91.59 92.01\nPage 12 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\nlarge difference between the corpora of MIMIC-III and PharmaCoNER. This experi -\nmental result indicates that only the domain knowledge related to the PharmaCoNER \ndataset can promote the improvement of model performance. Next, we observe that \nall BERT models obtain competitive performance, demonstrating that BERT can take \nadvantage of WordPiece to alleviate the OOV limitation. However, the max F1-scores \nof SciBERT(Scivoc,Cased) (91.05%) and SciBERT(Scivoc,UnCased) (91.15%) are higher \nthan those of SciBERT(Bertvoc,Cased) (90.58%) and SciBERT(Bertvoc,Uncased) \n(90.59%). This experimental result indicates that although BERT can use WordPiece \nto alleviate the OOV limitation, using the vocabulary designed for the domain corpus \ncan further improve model performance. Finally, we compare the effect of the charac -\nter case on BERT models. As shown in Table  5, among these models, BERT, MultiB -\nERT, SciBERT, and Spanish BERT have Cased and Uncased models, while NCBI BERT \nand BioBERT only have Uncased or Cased models. From the average F1-score, the per -\nformance of BERT (Uncased) and MultiBERT (Uncased) is better than that of BERT \n(Cased) and MultiBERT (Cased). However, the performance on the Cased and Uncased \nmodels is not much different for Spanish BERT and SciBERT. Therefore, as far as exist -\ning BERT models are concerned, it can only be concluded that the character case has \na certain impact on model performance. The specific impact trend needs more experi -\nments to reveal.\nDiscussion\nPerformance of each type for PharmaCoNER\nTable 6 lists the highest precision, recall and F1-score of BioBERTv1.1(+P ,Cased) on \nthe PharmaCoNER challenge. Among the three types of entities evaluated for Phar -\nmaCoNER, BioBERTv1.1(+P ,Cased) performed worst on NO_NORMALIZABLES \n(16.67% in F1-score). As shown in Table  1, there are only 50 NO_NORMALIZABLES \nentities in the PharmaCoNER dataset. Because the quantity is insufficient, it is difficult \nfor BioBERTv1.1(+P ,Cased) to effectively learn latent features of this type of mention. In \ncontrast, BioBERTv1.1(+P ,Cased) performed well on the recognition for NORMALIZA-\nBLES and PROTEINAS entities, achieving F1-scores of 94.83% and 89.87%, respectively. \nThe reason may be that these two types of entities are in sufficient quantity and their \nstructure has been standardized.\nSoftmax versus CRF\nBecause CRF can optimize the path of sequence labeling problems, most previous neural \nmodels (e.g., LSTM-CRF) used CRF to learn label constraints. In this study, we com -\npared the performance of BERT-softmax and BERT-CRF. As illustrated in Table  7, the \nTable 6 Performance of each type for PharmaCoNER\nMethod P (%) R (%) F1 (%)\nNORMALIZABLES 95.33 94.35 94.83\nNO_NORMALIZABLES 14.29 20.00 16.67\nPROTEINAS 90.45 89.29 89.87\nOverall 92.44 91.59 92.01\nPage 13 of 16\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n \nperformance of BERT-softmax is superior to that of BERT-CRF. The reason may be that \nthe token representation already contains context information, and using these repre -\nsentations can obtain promising performance.\nError analysis\nWe further performed error analysis to explore the entities constituting false negatives \n(FNs) and false positives (FPs). The best run of BioBERTv1.1(+P ,Cased) (with the F1-score \nof 92.01%) produced a total of 155 FNs and 138 FPs. We concluded four representative \ntypes of errors by analyzing these FNs and FPs. Table 8 lists these types of errors. The first \nexample represents a type of FNs, which is caused by incorrectly recognizing the ground-\ntruth ‘PROTEINAS’ type as the ‘O’ type. This type of error accounts for 49% (i.e., 76/155) \nof all FNs. Similarly, the second example represents a type of FPs, which is caused by incor-\nrectly recognizing the ground-truth ‘O’ type as the ‘PROTEINAS’ type. This type of error \naccounts for 42% (i.e., 58/138) of all FPs. Furthermore, boundary recognition errors are a \ntypical type of error. As for the third example, the BioBERTv1.1(+P ,Cased) model incor-\nrectly recognizes some modifying words (i.e., ‘de bajo pesomolecular’) as the chemical \nTable 7 Performance comparison of BERT-CRF and BERT-Softmax\n‘BERT’ refers to BioBERTv1.1(+P ,Cased)\nThe highest values are shown in bold\nMethod Mean ± SD Max\nP (%) R (%) F1 (%) P (%) R (%) F1 (%)\nBERT-CRF 90.42 ± 1.16 89.59 ± 0.36 90.00 ± 0.68 91.69 89.90 90.79\nBERT-Softmax 91.40 ± 0.81 90.90 ± 0.47 91.15 ± 0.60 92.44 91.59 92.01\nTable 8 Examples of errors in recognizing biomedical entities by BioBERTv1.1(+P ,Cased)\n‘Gold’ denotes the gold standard, and ‘Pred’ denotes the prediction results. Bold represents the gold standard entities and \nbolditalic denotes the predicted entities. If not specified, it defaults to the ‘O’ type, which means it is not a chemical/protein \nentity\nError examples Number \nof \nerrors in \nthis type\nGold: Se solicita serología de [Anticuerpos Echinococcus](PROTEINAS)/Hemag que es POSITIVO a \ncifras superiores 1/2,621,440\n76\nPred: Se solicita serología de [Anticuerpos Echinococcus](O)/Hemag que es POSITIVO a cifras supe-\nriores 1/2,621,440\nGold: A esto se añadía alteración de [enzimas hepáticas](O) 58\nPred: A esto se añadía alteración de [enzimas hepáticas](PROTEINAS)\nGold: ... a dosis plenas (1 mg/kg/día) y [heparina](NORMALIZABLES) de bajo peso molecular, con nor-\nmalización progresiva de las deposiciones .\n39\nPred: ... a dosis plenas (1 mg/kg/día) y [heparina de bajo peso molecular](NORMALIZABLES) , con nor-\nmalización progresiva de las deposiciones\nGold: La ecografía mostró derrame pleural loculado, administrándose en consecuencia 200,000 UI \nde [urokinasa](PROTEINAS) durante dos días consecutivos por el tubo de toracocentesis\n9\nPred: La ecografía mostró derrame pleural loculado, administrándose en consecuencia 200,000 UI \nde [urokinasa](NORMALIZABLES) durante dos días consecutivos por el tubo de toracocentesis\nPage 14 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\nentity (i.e., ‘heparina’). The fourth example also represents a type of error, i.e., the chemical \nand protein cross-recognition errors. In the gold standard, ‘urokinasa’ is annotated as a pro-\ntein entity, but the BioBERTv1.1(+P ,Cased) model incorrectly recognizes it as a chemical \ntype (i.e., the ‘NORMALIZABLES’ type).\nConclusion\nIn this article, we have compared and explored various representative BERTs on the Phar-\nmaCoNER dataset in detail. Our method achieves SOTA performance, with an F1-score \nof 92.01%. The experimental results show that the introduction of language models such \nas BERT can effectively improve model performance on the PharmaCoNER task. For the \nBERT model, the performance of the model pre-trained using the biomedical domain cor-\npus is superior to the model pre-trained using the native language. Although BERT can use \nWordPiece to alleviate the OOV limitation, the use of a vocabulary designed for specific \ndomain corpora can further improve model performance. Furthermore, the character case \nalso has a certain effect on model performance. In future work, we would like to explore \nthe performance of BERT pre-trained using the Spanish PubMed corpus on the Pharma-\nCoNER dataset.\nAbbreviations\nNER:: named entity recognition; NLP:: natural language processing; SOTA:: state-of-the-art; OOV:: out-of-vocabulary; \nBioNER;: biomedical named entity recognition; POS:: part-of-speech; LSTM:: long short-term memory; CNN:: covolution \nneural network; CRF:: conditional random field; BiLSTM:: bidirectional long short-term memory; SPACCC:: Spanish Clinical \nCase Corpus..\nAcknowledgements\nThe authors want to thank the anonymous reviewers for their helpful comments and suggestions.\nAbout this supplement\nThis article has been published as part of BMCBioinformatics Volume 22, Supplement 1 2021: Recent Progresses withBi-\noNLP Open Shared Tasks—Part 2. The full contents of the supplement areavailable at https:// bmcbi oinfo rmati cs. biome \ndcent ral. com/ artic les/ suppl ements/ volume- 22- suppl ement-1.\nAuthor’s contributions\nCS processed the data, designed the experiment, implemented the programming codes, performed the analysis, drafted \nthe paper and revised the paper. ZY provided feedback on the structure of paper and revised the paper. LW and YZ \nshared ideas on solving the problem. HL and JW checked the paper. All authors have read and approved the final version \nof the manuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nThe PharmaCoNER corpus can be downloaded at: https:// temu. bsc. es/ pharm aconer/. The transformer framework are \navailable at: https:// github. com/ huggi ngface/ trans forme rs. The PyTorch library are available at: https:// pytor ch. org. Our \ndata and codes are available at https:// github. com/ CongS un- dlut/ Pharm aCoNER.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nReceived: 19 May 2021   Accepted: 31 May 2021\nPublished: 17 December 2021\nPage 15 of 16\nSun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n \nReferences\n 1. Krallinger M, Rabal O, Lourenco A, et al. Information retrieval and text mining technologies for chemistry. Chem Rev. \n2017;117(12):7673–761.\n 2. Krallinger M, Leitner F, Rabal O, et al. CHEMDNER: The drugs and chemical names extraction challenge. J Chemin-\nform. 2015;7(1):1–11.\n 3. Elhadad N, Pradhan S, Gorman S, et al. SemEval-2015 task 14: analysis of clinical text. In: Proceedings of the 9th \ninternational workshop on semantic evaluation (SemEval 2015). Denver: Association for Computational Linguistics; \n2015. p. 303–10.\n 4. Uzuner Ö, South BR, Shen S, et al. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. J Am \nMed Inform Assoc. 2011;18(5):552–6.\n 5. Agirre AG, Marimon M, Intxaurrondo A, et al. Pharmaconer: Pharmacological substances, compounds and proteins \nnamed entity recognition track. In: Proceedings of The 5th workshop on BioNLP open shared tasks. Hong Kong: \nAssociation for Computational Linguistics; 2019; p. 1–10.\n 6. Leaman R, Wei CH, Lu Z. tmChem: a high performance approach for chemical named entity recognition and nor-\nmalization. J Cheminform. 2015;7(1):1–10.\n 7. Rocktäschel T, Weidlich M, Leser U. ChemSpot: a hybrid system for chemical named entity recognition. Bioinformat-\nics. 2012;28(12):1633–40.\n 8. Huang Z, Xu W, Yu K. Bidirectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv: 1508. 01991 (2015).\n 9. Lample G, Ballesteros M, Subramanian S, et al. Neural architectures for named entity recognition. In: Proceedings \nof the 2016 conference of the North American chapter of the association for computational linguistics: human \nlanguage technologies. San Diego: Association for Computational Linguistics; 2016. p. 260–70.\n 10. Li L, Jin L, Jiang Z, et al. Biomedical named entity recognition based on extended recurrent neural networks. In: IEEE \ninternational conference on bioinformatics and biomedicine (BIBM). IEEE; 2015. p. 649–52.\n 11. Chalapathy R, Borzeshi EZ, Piccardi M. An investigation of recurrent neural architectures for drug name recognition. \narXiv preprint arXiv: 1609. 07585 (2016).\n 12. Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space. arXiv preprint arXiv: \n1301. 3781 (2013).\n 13. Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality. In: \nAdvances in neural information processing systems; 2013. p. 3111–9.\n 14. Pennington J, Socher R, Manning CD. Glove: global vectors for word representation. In: Proceedings of the 2014 \nconference on empirical methods in natural language processing (EMNLP); 2014. p. 1532–43.\n 15. Peters M, Neumann M, Iyyer M, et al. Deep contextualized word representations. In: Proceedings of the conference \nof the North American chapter of the association for computational linguistics; 2018. p. 2227–37.\n 16. Akbik A, Blythe D, Vollgraf R. Contextual string embeddings for sequence labeling. In: Proceedings of the 27th inter-\nnational conference on computational linguistics; 2018. p. 1638–49.\n 17. Devlin J, Chang M-W, Lee K et al. BERT: Pre-training of deep bidirectional transformers for language understanding. \nIn: Proceedings of the conference of the North American chapter of the association for computational linguistics; \n2019. p. 4171–4186.\n 18. Xiong Y, Shen Y, Huang Y, et al. A deep learning-based system for PharmaCoNER. In: Proceedings of The 5th work-\nshop on BioNLP open shared tasks. Hong Kong: Association for Computational Linguistics; 2019. p. 33–7.\n 19. Stoeckel M, Hemati W, Mehler A. When specialization helps: using pooled contextualized embeddings to detect \nchemical and biomedical entities in Spanish. In: Proceedings of the 5th workshop on BioNLP open shared tasks. \nHong Kong: Association for Computational Linguistics; 2019. p. 11–5.\n 20. Akbik A, Bergmann T, Vollgraf R. Pooled contextualized embeddings for named entity recognition. In: Proceedings \nof the 2019 conference of the North American chapter of the association for computational linguistics: human \nlanguage technologies, vol. 1 (Long and Short Papers); 2019. p. 724–8.\n 21. Sun C, Yang Z. Transfer learning in biomedical named entity recognition: an evaluation of BERT in the PharmaCoNER \ntask. In: Proceedings of The 5th workshop on BioNLP open shared tasks. Hong Kong: Association for Computational \nLinguistics; 2019. p. 100–4.\n 22. Lee J, Yoon W, Kim S, et al. BioBERT: a pre-trained biomedical language representation model for biomedical text \nmining. Bioinformatics. 2020;36(4):1234–40.\n 23. Pérez-Pérez M, Rabal O, Pérez-Rodríguez G, et al. Evaluation of chemical and gene/protein entity recognition sys-\ntems at BioCreative V. 5: the CEMP and GPRO patents tracks; 2017. p. 1–8.\n 24. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. In: Advances in neural information processing \nsystems; 2017. p. 5998–6008.\n 25. Wu Y, Schuster M, Chen Z, et al. Google’s neural machine translation system: bridging the gap between human and \nmachine translation. arXiv preprint arXiv: 1609. 08144 (2016).\n 26. Taylor WL. “Cloze procedure’’: a new tool for measuring readability. J Q. 1953;30(4):415–33.\n 27. Zhu Y, Kiros R, Zemel R, et al. Aligning books and movies: towards story-like visual explanations by watching movies \nand reading books. In: Proceedings of the IEEE international conference on computer vision; 2015. p. 19–27.\n 28. Peng Y, Yan S, Lu Z. Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo \non ten benchmarking datasets. In: Proceedings of the 2019 workshop on biomedical natural language processing \n(BioNLP 2019); 2019. p. 58–65.\n 29. Canete J, Chaperon G, Fuentes R, et al. Spanish pre-trained bert model and evaluation data. PML4DC at ICLR, 2020.\n 30. Tiedemann J. Parallel data, tools and interfaces in OPUS. In: Lrec; 2012. p. 2214–18.\n 31. Beltagy I, Lo K, Cohan A. SciBERT: a pretrained language model for scientific text. In: Conference on empirical meth-\nods in natural language processing. Hong Kong: Association for Computational Linguistics; 2019. p. 3613–18.\n 32. Armengol-Estapé J, Soares F, Marimon M, et al. PharmacoNER Tagger: a deep learning-based tool for automatically \nfinding chemicals and drugs in Spanish medical texts. Genomics Inform. 2019;17(2):e15.\nPage 16 of 16Sun et al. BMC Bioinformatics  2021, 22(Suppl 1):602\n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 33. Soares F, Villegas M, Gonzalez-Agirre A, et al. Medical word embeddings for Spanish: development and evaluation. \nIn: Proceedings of the 2nd clinical natural language processing workshop. Minneapolis: Association for Computa-\ntional Linguistics; 2019. p. 124–33\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}