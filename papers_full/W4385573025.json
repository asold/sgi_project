{
    "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
    "url": "https://openalex.org/W4385573025",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5055466746",
            "name": "Evangelia Spiliopoulou",
            "affiliations": [
                "Amazon (United States)",
                "Carnegie Mellon University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A5018367168",
            "name": "Artidoro Pagnoni",
            "affiliations": [
                "Amazon (United States)",
                "Carnegie Mellon University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A5041302228",
            "name": "Yonatan Bisk",
            "affiliations": [
                "Amazon (United States)",
                "Carnegie Mellon University",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A5060225743",
            "name": "Eduard Hovy",
            "affiliations": [
                "Amazon (United States)",
                "Carnegie Mellon University",
                "University of Washington"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4293566037",
        "https://openalex.org/W2951976932",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W4302570567",
        "https://openalex.org/W3152571257",
        "https://openalex.org/W3120860016",
        "https://openalex.org/W2962781380",
        "https://openalex.org/W2984812384",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W2796136333",
        "https://openalex.org/W4288243162",
        "https://openalex.org/W3034937117",
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2138162238",
        "https://openalex.org/W2145755360",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3176195078",
        "https://openalex.org/W2963101081",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1525961042",
        "https://openalex.org/W3101605968",
        "https://openalex.org/W2889317091",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W2971253865",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W3100307207",
        "https://openalex.org/W3174464510",
        "https://openalex.org/W2963983586",
        "https://openalex.org/W3104041537",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4288262262",
        "https://openalex.org/W2980282514"
    ],
    "abstract": "This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1982–1997\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nEVENTS REALM: Event Reasoning of Entity States via Language Models\nEvangelia Spiliopoulou∗†\nAmazon\nAWS, AI Labs\nspilieva@amazon.com\nArtidoro Pagnoni∗\nUniv. of Washington\nartidoro@cs.\nwashington.edu\nYonatan Bisk\nCarnegie Mellon\nUniversity\nybisk@cs.cmu.edu\nEduard Hovy\nCarnegie Mellon\nUniversity\nhovy@cs.cmu.edu\nAbstract\nThis paper investigates models of event impli-\ncations. Specifically, how well models pre-\ndict entity state-changes, by targeting their un-\nderstanding of physical attributes. Nominally,\nLarge Language models (LLM) have been ex-\nposed to procedural knowledge about how ob-\njects interact, yet our benchmarking shows they\nfail to reason about the world. Conversely, we\nalso demonstrate that existing approaches often\nmisrepresent the surprising abilities of LLMs\nvia improper task encodings and that proper\nmodel prompting can dramatically improve per-\nformance of reported baseline results across\nmultiple tasks. In particular, our results indi-\ncate that our prompting technique is especially\nuseful for unseen attributes (out-of-domain) or\nwhen only limited data is available.1\n1 Introduction\nModeling the effect of actions on entities ( event\nimplications) is a fundamental problem in AI span-\nning computer vision, cognitive science and natural\nlanguage understanding. Most commonly referred\nto as the Frame Problem (McCarthy and Hayes,\n1981), early solutions relied on a set of handcrafted\nrules and logical statements to model event impli-\ncations. However, such methods require substantial\nmanual effort and fail to generalize. More recently,\nmodeling event implications has reemerged under\nthe guise of common sense reasoning within NLP\n(Sap et al., 2019b; Bisk et al., 2020b; Talmor et al.,\n2019) and action anticipation in Computer Vision\n(Damen et al., 2018; Bakhtin et al., 2019).\nPredicting event implications is a particularly\ndifficult problem due to the complex nature of lan-\nguage and implicit knowledge required to answer\nsuch questions. For example, if we are given the\n* Equal contribution.\n† Work completed before joining AWS AI Labs.\n1 https://github.com/spilioeve/eventsrealm\nContext: \nThe robot holds a laptop.\nThe robot forcefully throws the \nlaptop.\nContext: \nPick up the yogurt, bananas, and sorbet. \nPlace the ingredients in a blender. \nBlend the mixture until it's smooth in \ntexture.\nPiGLET Open PI\nWhat attributes changed: \nLaptop is broken, picked-up and \nits location is different. \nWhat attributes changed: \n1. The cleanness, weight, volume and \nfullness of the blender changed.\n2. The texture and appearance of the \nmixture changed.\n● 14 attributes\n● AI2 Thor Simulator\n● 5k/2k/2k train/dev/test\n● 51 in-domain attributes\n● 40 out-of-domain attributes\n● WikiHow articles\n● 11k/1k/2k train/dev/test\nEntities: blender, mixtureEntity: Laptop\nFigure 1: We use the PiGLET and OpenPI datasets\nto probe if LLMs contain the necessary grounded and\nworld knowledge to reason about event implications.\nsentence the mug fell on the floor and we want to\ndetermine whether the mug is broken, we need to\nknow of several facts such as the material of the\nmug, the fragility of ceramics, the hardness of the\nfloor, etc. and also how to combine these facts to-\ngether to reason whether the mug will break or\nnot. None of this knowledge is explicitly stated, in-\nstead being classified as common sense knowledge,\nand is traditionally acquired from observations or\ninteractions with objects and the environment.\nCore to this line of work is the assumption that\nevents can be learned via language, not depending\non other forms of perception. To explore the utility\nof other modalities and interaction, Zellers et al.\n(2021) train a language model to predict physical\nchanges in a virtual environment. While intuitively\nnecessary (Bisk et al., 2020a), in this work we\nshow that the purported limitations of language-\nonly models are not always well founded. Key to\ntheir success (or failure) are (1) How we use the\nlanguage models, and (2) The difficulty of the task\ndomain and dataset.\nWe find that the difficulty of the task is often a\nstand-in for whether reasoning is required. Others\nhave also noted that despite the tremendous gains\n1982\nin NLU made possible by Large Language mod-\nels (LLM), they still stumble when reasoning is\nrequired (Brown et al., 2020). If reasoning can be\ncodified as patterns, we are presented with two new\nchallenges: (1) Can we test pattern acquisition via\nbenchmarking generalization, and (2) How can new\npatterns or context be provided to the model? The\nnascent field of “prompting\" (Liu et al., 2021; Wei\net al., 2021; Ouyang et al., 2022) hints at a possible\napproach for humans to encode novel reasoning\npatterns for models, however the best structure and\nthe amount of information to convey via a prompt\nfor a given task still remain open questions.\nThis work makes three contributions to the lit-\nerature of event implications. First, we show that\nlanguage by itself provides enough information\nto predict event implications in current datasets,\nwithout the need of a physical interaction model.\nSecond, we establish the difficulty of the problem\nand limitations of current models by showing ex-\ntreme differences in performance across different\ndatasets: PiGLET (Zellers et al., 2021), based on\na virtual environment, and OpenPI (Tandon et al.,\n2020), based on procedural text from WikiHow.\nThird, we explore how different prompting tech-\nniques affect model performance in terms of their\ninformation content and model nature. Finally, we\ndiscuss the generalization properties of our mod-\nels to unseen attributes (out-of-domain) and how\nthis shows their ability to extract implicit reasoning\nmechanisms.\n2 Related Work\nRelated work in commonsense follows two direc-\ntions: (1) predict event implications or track en-\ntity changes, and (2) use commonsense knowledge\nabout events and their implications as necessary\nintermediate steps in reasoning.\nResearch that directly studies event implications\nmostly explores causality between social events\nand emotional states, based on social norm expec-\ntations (Rashkin et al., 2018; Sap et al., 2019b;\nForbes et al., 2020; Emelin et al., 2020; Hwang\net al., 2020). Jiang et al. (2021) study specific\nlinguistic phenomena such as contradiction and\nnegation, while Sap et al. (2019a) study the role of\nsocial biases and predicting implications of social\nevents. Although this line of research highlights the\ndifficulty of predicting cause-effect relations, so-\ncial scenarios are typically ambiguous and require\nknowledge of event chains. For example, in order\nto answer whether X gives a gift to Y implies that X\nhugs Y, we must be aware of the relation between\nX and Y , their personalities, and the social context.\nOn the other hand, event implications as physical\nchanges of state of entities are, mostly, objective\nand depend on simple relations that a model could\nknow a priori (e.g., the material of a mug), allowing\nus to isolate and study the reasoning abilities of a\nmodel.\nCloser to our task is the prediction of physical\nimplications of events. This problem often takes\nthe form of entity changes in procedural text, such\nas in cooking recipes (Bosselut et al., 2017) or\nWikiHow articles (Tandon et al., 2020). However,\nmost datasets primarily focus on changes in loca-\ntion compared to other attributes, such as ProPara\n(Mishra et al., 2018) and bAbI (Weston et al., 2015).\nModeling approaches in both areas of common-\nsense explore the generation of explanations in a\nmulti-task setting (Dalvi et al., 2019), the use of ex-\nternal knowledge graph (Tandon et al., 2018), and\nautomatic knowledge base construction to keep a\nrepresentation of the state of the world and gener-\nate novel knowledge (Bosselut et al., 2019; Henaff\net al., 2016; Hwang et al., 2020).\nThe second type of commonsense reasoning in-\ncludes question answering tasks that assume knowl-\nedge of commonsense relations and their implica-\ntions on the context. This line of work includes\nshort questions, such as OpenBookQA (Mihaylov\net al., 2018), CommonSenseQA (Talmor et al.,\n2019), SWAG (Zellers et al., 2018) and COPA\n(Roemmele et al., 2011), or questions based on a\nprovided document (Huang et al., 2019) or knowl-\nedge base (Clark et al., 2018).\n3 Task and Datasets\nThe problem of predicting event implications can\nbe formulated in several ways, with varying levels\nof difficulty. For example, Tandon et al. (2020)\ngenerate triplets ofentity, attribute, post-stategiven\nsome context, while Zellers et al. (2021) are given\nan entity, attribute, pre-state, and context, to only\npredict the post-state of the entity.\nOur task follows a similar formulation to\nPiGLET, where the model is given a context (i.e., a\nsmall paragraph followed by an action-sentence),\nan entity of interest and a list of attributes. Then,\nthe model needs to determine whether a change-\nof-state occurred for the entity with respect to the\ngiven list of attributes (see Figure 1). However,\n1983\nunlike Zellers et al. (2021), we do not use the pre-\nstate encoding of the entity, instead we assume that\nthe relevant information is better conveyed through\nthe natural language description of the context.\n3.1 PiGLET\nPiGLET (Zellers et al., 2021) consists of encodings\nof the pre- and post-state of entities as a result of\nan action. Each instance is accompanied by the\ncontext: a natural language description of the pre-\nstate of the entities, followed by a description of\nthe action. PiGLET is a small dataset (5k training\nexamples), which studies entity change-of-state\nwith respect to 14 attributes, caused by 8 distinct\nevents.\nPiGLET is a semi-artificial dataset, where the\nentity, pre-state, post-state, action tuple was gener-\nated by exploring the virtual environment AI2 Thor\n(Kolve et al., 2017). A natural language context\nwas constructed by human annotators, who were\nprovided with the tuples generated by the virtual\nenvironment. This results in simpler concise state-\nments compared to the ambiguous language that\nhumans naturally use to communicate.\n3.2 OpenPI\nOpen PI (Tandon et al., 2020) also studies the\nchange-of-state of entities with respect to physi-\ncal attributes. However, unlike PiGLET, Open PI is\nbased on articles from WikiHow, containing realis-\ntic descriptions of physical changes. The context in\nthis dataset is the entire WikiHow article preceding\nthe action sentence from the article.\nOpen PI is a substantially larger dataset, con-\ntaining an initial set of 51 pre-defined attributes\nfrom WordNet (Fellbaum, 2010), then augmented\nby human annotators. Although the total number\nreaches ∼800 unique attributes, the initial 51 at-\ntributes cover more than 80% of instances. Further-\nmore, the vast majority of the newly introduced\nattributes appear only once and many of them con-\ntain typos or abbreviations. All our models are\ntrained in the initial set of 51 attributes.\n4 Methodology\nNext, we introduce our prompting techniques,\nwhich vary with respect to per-instance informa-\ntion content. Each technique is tested with different\nLLMs and fine-tuning methods. The goal of each\nprompting mechanism is to show how model perfor-\nmance and generalization vary based on the infor-\nmation conveyed in our queries. Our study focuses\non four prompting methods depicted in Figure 2:\nzero-prompt, single-attribute, multi-attribute, and a\nvariant of the latter, the k-attribute prompt.\nOur approach builds on literature demonstrating\nbenefits in using prompting to distinguish differ-\nent tasks, when a model is trained in a multi-task\nsetting (Raffel et al., 2020; Wei et al., 2021). In\nour study, however, we explore how to use prompts\nas a medium to convey the task-specific informa-\ntion that a model must know in order to solve the\ntask, similar to how one would ask a human. To\nthe best of our knowledge, we are the first ones to\ndemonstrate advantages and disadvantages of dif-\nferent ways to codify intermediate steps required\nfor reasoning via prompting and use them to study\nLLMs’ understanding of event implications.\n4.1 Large Language Models\nWe explore three transformer-based language\nmodels: an autoregressive, an autoencoder, and\na seq-to-seq model. We include models with\ndifferent architectures to investigate the effect of\nour prompting strategies across model families.\nOur goal is to use each model in combination with\nprompts that enhance their individual strengths,\nbased on their pretraining schemes.\nRoBERTa (Liu et al., 2019): is an autoencoder\nmodel widely used in classification tasks.\nT5 (Raffel et al., 2019): is a seq-to-seq model that\nhas shown excellent performance in multi-tasking\nby using the task description as a prompt. T5 is\nused for both text classification and generation.\nGPT-3 (Brown et al., 2020): is an autoregressive\nmodel and is primarily used in zero and few-shot\nsettings due to its substantially larger size. GPT-3\nis used in language generation and classification,\nand has shown excellent performance in few-shot\nsettings when queried with appropriate prompts.\nThese backbone models are used with one of the\nthree prompting techniques, as described in the\nfollowing paragraphs and shown in Figure 2.\n4.2 Multi-label Classifier: Zero-prompt\nOur baseline model is a multi-label classifier with\nno explicit information about the nature of the task\nor the attributes themselves. The model takes the\ncontext and the prompt Now what happens next to\nthe [entity]? as inputs, and predicts a binary vector,\n1984\nQuery:  “”\nTarget: n-dim binary vector, n = #attributes\nQuery each attribute in candidate list\nQuery1: Is the location of the mug different?                 \nTarget: The location of the mug is different.\nQuery2: Is the temperature of the mug different?\nTarget: The temperature of the mug is unchanged.\nQuery: Consider the attributes: location, temperature, shape .... \nTarget: The location, composition and shape of the mug \nchanged.\nZero-prompt\nSingle-attr. \nprompt \nMulti-attr. \nprompt: \nall-attribute \nSplit attributes to subsets\nQuery1: Consider the attributes: location, shape.       \nTarget: The location and shape of the mug changed. \nQuery2: Consider the attributes: temperature, composition.\nTarget: The composition of the mug changed.\nContext: The robot throws the mug to the ground. What happens next to the mug?\nMulti-attr. \nprompt: \nk-attribute \nFigure 2: Prompting techniques used in our models.\nMulti-attribute prompt improves performance by learn-\ning dependencies among attributes.\nwhere entries correspond to changes in specific\nattributes. We test this mechanism with RoBERTa,\nas it performs well in classification tasks.\nWith this model we test the traditional “finetun-\ning assumption” that, given enough data, the model\ncan learn the correspondence between attributes\nand dimensions in the output vector and correctly\npredict their changes. This model serves as a base-\nline of how a LLM performs when fine-tuned to a\nspecific task. Crucially, it does not have the ability\nto generalize to new attributes as the output vector\nis of fixed size.\n4.3 LM as Classifier: Single-attribute Prompt\nOur second prompting technique provides informa-\ntion about individual attributes. Via this technique\nwe evaluate whether a model benefits from the ver-\nbalization of each attribute, as a means to retain\nuseful information from the context. Unlike the\nzero-prompt model, this model can be used out-of-\ndomain, with unseen attributes.\nIn this setup, we query the model about each\nindividual attribute separately, for every context-\nentity pair, as shown in Figure 2. This mechanism\nwas tested with all three models: RoBERTa (fine-\ntuned and zero-shot), T5 (fine-tuned) and GPT-3\n(few-shot).\nBy querying each attribute individually, the\nmodel is able to focus only on information related\nto that specific attribute. This can both benefit and\nhurt performance, as we show in section 5. On\none hand, the model pays more attention to the\nsentence semantics related to the queried attribute.\nBy using the attribute as a bottleneck, the model\nlearns which aspect of meaning is important in that\ninstance. This is particularly beneficial in limited-\ndata scenarios where generalization is necessary.\nOn the other hand, by querying only a single at-\ntribute per instance, the model does not learn corre-\nlations across attributes. This weakness becomes\nmore apparent in scenarios with many correlated\nattributes.\n4.4 LM as Generator: Multi-attribute Prompt\nOur final prompting technique focuses on retrieving\ninformation about a set of attributes, by querying\nmultiple attributes together. This technique com-\nbines strengths of the zero-prompt and the single-\nattribute prompt models, as it is able to both ver-\nbalize the attributes and capture correlations across\nthem. Unlike other mechanisms, this method al-\nlows us to control the information content per in-\nstance, by varying the set of queried attributes.\nAs we show in sections 5 and 6.2, varying the at-\ntribute queries across training instances is crucial\nto achieve generalization.\nFor this technique, the prompt lists the attributes\nthat the model should consider. This list is dataset\nspecific and can vary between training and test-\ning (i.e., out-of-domain) or even across training\ninstances. The model is trained to generate the at-\ntributes that changed, as shown in Figure 2. This\ntechnique works with text generation models and\nwas tested on both T5 (fine-tuned) and GPT-3 (few-\nshot).\nThe first version of this model, the all-attribute\nprompt, queries all attributes that could change in\nthe same instance. However, the risk with this\napproach is that, because the prompt is fixed, the\nmodel learns to pay little attention to the specific\nattributes that appear in it. We therefore propose a\nvariant of this method, the k-attribute prompt, aim-\ning to achieve high performance in both in-domain\nand out-of-domain scenarios. The objective is to\nlearn about attribute dependencies but also force\nthe model to pay attention to the specific attributes\nbeing prompted. To achieve this, we prompt the\nmodel with krandom attributes and train it to pre-\ndict changes only among these kattributes. More\nspecifically, for each training example, we partition\nthe 51 attributes into qrandom groups where qis\na random integer between 1 and 5. krefers to the\nnumber of attributes in each partition. This method\nensures that the model is queried with krandom at-\ntributes and that all 51 attributes are always queried\nfor each example.\n1985\nAll attributes Per-attribute F1\nModel Pr Re F1 Dist Size Mass Temp isBroken\nPhysical Interaction, (PiGLET) 97.4 91.6 94.4 93.6 79.2 98.3 99.6 92.8\nn-gram LogReg (baseline) 87.8 88.0 87.9 78.8 74.7 97.8 94.0 79.4\nRoBERTa-base, zero-prompt 95.2 92.6 93.9 90.6 82.7 100.0 95.3 94.7\nT5-base, all-attribute prompt 93.0 95.4 94.1 91.7 83.5 100.0 95.8 90.3\nTable 1: Micro-Precision, Recall and F1 scores across all 14 attributes in PiGLET. Per-attribute F1 scores for\nchallenging attributes, as in (Zellers et al., 2021). Language-only models perform competitively with PiGLET.\n5 Experiments & Results\nOur task is a multi-label classification where, given\nsome context and an entity of interest, we need to\nidentify which attributes change. Due to the signifi-\ncant label imbalance, in our experiments we report\nmicro- Precision, Recall, and F1 for the positive in-\nstances, across labels. In addition to these metrics,\nwe measure per-attribute Precision, Recall and F1\nfor both datasets (details in subsection A.4).\n5.1 PiGLET\nBaselines: The strongest baseline is the PiGLET\nmodel, which is a combination of physical interac-\ntion and language model, based on GPT-2 (Radford\net al., 2019). It was proposed in the paper intro-\nducing the dataset and is currently state-of-the-art.\nUnlike the other models, it learns by interacting\nwith a simulator and has access to the pre-state of\neach entity. We also use a simple n-gram Logistic\nRegression baseline to both establish the overall\ndifficulty of the dataset and measure benefits due\nto the pre-training of LLMs.\nResults: As shown in Table 1, all models per-\nform relatively well on the PiGLET dataset. The\nextremely small margin in performance between\nPhysical Interaction and the proposed models\n(RoBERTa zero-prompt and T5 all-attribute) indi-\ncates that language models can learn about physical\nattributes even without the need of physically inter-\nacting with the environment. However, we should\nhighlight that this conclusion holds for datasets\nsimilar to PiGLET and the importance of physical\ninteractions remains an open question that must be\ntested in more realistic and challenging datasets.\nDespite the high performance of our proposed\nmodels, previously reported baselines on PiGLET\nshow significantly lower performance than the\nPhysical Interaction model. Notably, their baseline\nusing T5-base achieves only 53.9% in hard accu-\nracy, compared to 81.1% of the Physical Interaction\nmodel (Zellers et al., 2021). Unfortunately we can-\nnot directly compare these results to our proposed\nmodels due to their choice of metric (hard accuracy)\nand different problem formulation, where the input\nand output is the encoding of the pre- and post-state\nof the entity. Despite the use of different metrics,\nwe observe a minimal performance difference be-\ntween language-only models and PiGLET. This\nhighlights the importance of using proper prompt-\ning techniques and task formulation to take full\nadvantage of LLMs and draw valid conclusions.\nOur final observation is that there is a larger gap\nbetween the n-gram LogReg model and the rest of\nthe models. This shows that, although language is\nvery useful to predict physical event implications,\npre-trained language models still have an advan-\ntage due to the information they have previously\nseen. This raises the question of how can we better\nexploit the relations that pre-trained language mod-\nels already know, which we explore via the next set\nof experiments.\n5.2 OpenPI\nSince our results in PiGLET show that it is not a\nchallenging dataset, we use Open PI to compare the\nproposed prompting techniques. With the excep-\ntion of the GPT-3 models, all models have relatively\nsimilar sizes, ranging from 123M (RoBERTa-base)\nto 354M (RoBERTa-large) parameters.\nFew-shot: For each instance in the test set, we\npick 10 examples from the training set to be in-\ncluded in the prompt - there are marginal improve-\nments beyond four (Min et al., 2022). Performance\nin complex tasks like QA is sensitive to prompt\nselection (Liu et al., 2022). Following previous\nwork, we pick the relevant examples based on se-\nmantic similarity (Reimers and Gurevych, 2019).\nIn the single-attribute prompt setting, we include\nexamples querying the same attribute, and balance\nboth positives and negatives.\nIn-domain vs out-domain: All our models are\n1986\nIn-domain Out-domain\nTraining Model Pr Re F1 Pr Re F1\nZero-shot RoBERTa-large, single-attribute prompt 3.1 63.3 5.9 2.4 68.8 4.6\nFew-shot GPT-3-Babbage, single-attribute prompt 3.7 82.4 7.1 - - -\nGPT-3-DaVinci, all-attribute prompt 37.6 24.5 29.7 28.3 12.9 17.7\nFine-tuned\nGPT-2 (baseline in Open PI) 49.8 11.8 19.1 - - -\nRoBERTa-large, zero prompt 65.1 40.1 49.6 - - -\nRoBERTa-base, single-attribute prompt 40.3 55.1 46.6 21.3 26.2 23.5\nT5-base, single-attribute prompt 34.6 53.3 42.0 15.9 21.5 18.2\nT5-base, all-attribute prompt 47.5 56.0 51.4 25.0 1.2 2.2\nT5-base, k-attribute prompt 52.8 50.0 51.4 16.8 22.7 19.3\nTable 2: Micro-Precision, Recall and F1 scores for Open PI. In-domain attributes refers to the 51 originally curated\nattributes, while out-domain to the 41 attributes introduced by human annotators.\ntrained on the initial 51 attributes (subsection 3.2).\nFor in-domain experiments, the models are tested\non the same set of attributes, while for out-of-\ndomain on the new attributes introduced by hu-\nman annotators. After removal of rare attributes\nand merging of synonyms, the out-of-domain set\nconsists of 41 unique attributes.\nResults: As shown in Table 2, the best per-\nforming models in-domain are the multi-attribute\nprompt models. The performance difference be-\ntween the multi-attribute models and the zero-\nprompt baseline shows that the verbalization of\nattributes has a positive impact on performance,\nwhich is further supported by our findings in sub-\nsection 6.1. Furthermore, our models beat the GPT-\n2 model, proposed by Tandon et al. (2020) along\nwith the Open PI dataset. This model generates\nsentences describing entity state changes but, un-\nlike our models, does not verbalize the attributes.\nFinally, we observe a drop in performance for both\nT5 and RoBERTa single-attribute prompt, which\nconfirms that attribute dependencies are important\nin our task.\nDespite its good performance in previously seen\nattributes, the zero-prompt model cannot classify\nout-of-domain attributes because its output is a\nfixed-dimension binary vector. The best out-of-\ndomain performance is achieved by RoBERTa\nsingle-attribute, followed by T5 k-attribute prompt.\nWe observe that, despite the very low out-\nof-domain performance of the T5 all-attribute\nprompt model, the other two variants of the same\nprompting technique (GPT-3 all-attribute and T5k-\nattribute) perform competitively. This confirms our\nhypothesis that fine-tuning with a fixed query hurts\nthe generalization properties of the model, some-\nthing that can be avoided with few-shot learning\nor by shifting focus to different attributes during\ntraining (i.e., single-attribute or k-attribute).\n6 Discussion\nWe further study the models’ behavior with respect\nto the type of attributes they see and their general-\nization properties. This analysis serves to uncover\nadvantages and disadvantages of each technique\nand suggest promising methods for future work to\nenhance both model performance and robustness.\nFor all our experiments we use Open PI. Due to\nits greater diversity of attributes and larger size, it\nis a better candidate than PiGLET to analyze the\nlimitations of the models.\n6.1 Reasoning with Rare Attributes\nSince some attributes are significantly more fre-\nquent than others, fine-tuned models have been\nexposed to more data about them, which influences\nperformance. For example, performance across all\nfine-tuned models for the most frequent attribute\nlocation is substantially higher compared to other\nattributes (F1 = 0.65-0.75). Although most mod-\nels are expected to perform well on such high fre-\nquency attributes, our analysis provides useful in-\nsights on the models’ ability to learn reasoning\npatterns in limited-data scenarios.\nWe study per-attribute model performance based\non each attribute’s frequency in training data for\nthe three prompting techniques: RoBERTa zero-\nprompt, RoBERTa single-attribute, and T5 all-\nattribute. After clustering each attribute with re-\nspect to its frequency and its F1 score, we ob-\n1987\nFigure 3: Performance per attribute frequency in train-\ning data. Each bar shows the weighted-F1 score across\nall attributes in the same frequency category.\nRoBERTa,\nzero-prompt\nRoBERTa,\nsingle-attribute\nT5,\nall-attribute\nSpearman\ncorrelation ρ= 0.82 ρ= 0.80 ρ= 0.51\nTable 3: Spearman correlation between attribute fre-\nquency and F1 score. High correlation means the model\nlearns primarily high-frequency attributes. All results\nhave p-value < 0.001.\nserve four distinct clusters: low (<100 instances),\nmedium-low (100-400), medium-high (400-1000)\nand high (>1000) frequency. In Figure 3 we plot\nthe weighted-F1 score per cluster for the three mod-\nels. Our first observation is that performance across\nall models increases for attributes with higher fre-\nquency. This conclusion is also supported by the\nper-attribute Spearman correlation between perfor-\nmance and frequency, shown in Table 3. This con-\nfirms our hypothesis from PiGLET that LLMs can\nlearn physical interactions and achieve higher per-\nformance when there is sufficient labeled data to\nfine-tune on.\nOur second observation is that, although perfor-\nmance in high-frequency attributes is similar across\nall models, it significantly drops for RoBERTa zero-\nprompt when frequency decreases. This shows\nthat the model struggles to learn with fewer ex-\namples. This difference is most striking in the\nlow-frequency cluster, where the model learns\nnothing (F1 = 0.0). On the other hand, both\nRoBERTa single-attribute and T5 all-attribute have\nrelatively high performance in low-frequency at-\ntributes, where some attributes are easier to learn\nthan others. This supports one of our main hypoth-\nesis in this paper that, by verbalizing and querying\nspecific attributes, models pay attention to each\nk\n40\n45\n50\n55\n60\n0 10 20 30 40\nF1 Precision Recall\nFigure 4: F1, Precision, and Recall scores as a function\nof the number of attributes used in the prompt during\nevaluation for the k-attribute model\nattribute and learn reasoning patterns, a crucial\nstep in limited-data scenarios.\n6.2 Prompt Diversification via the k-attribute\nPrompt Model\nThrough manual inspection we find that the all-\nattribute models have an inherent bias towards gen-\nerating attributes that appeared in the training data,\neven when prompted with new ones. Their perfor-\nmance is in fact poor in the out-of-domain setting\n(2.2 F1, Table 2). Now the question is whether\nthis is a limitation of the reasoning abilities of the\nmulti-attribute models or a bias introduced by its\ntraining scheme.\nWe propose the k-attribute model to alleviate\ntraining biases by randomizing the queried at-\ntributes. Notably, this model still maintains the\ncore assumptions behind the multi-attribute prompt\nmodel of querying multiple attributes at once. We\nobserve that this simple technique results in the\nsame in-domain F1 score as the all-attribute prompt\nmodel, while significantly improving its out-of-\ndomain performance. This shows that the observed\nlimitations with the all-attribute prompt model are\ndue to training biases that prevent the model from\ngeneralizing to unseen attributes.\nOnce trained, the k-attribute prompt model can\nbe queried with varying number of attributes. In\nFigure 4, we plot the performance of the model\nas a function of the number of attributes used in\nthe query during evaluation. We observe a drop in\nperformance when the model is queried with a sin-\ngle attribute (similar to the single-attribute prompt\nmodels). The performance is highest around 10\nattributes and drops slightly beyond that. We also\nobserve that by varying k, we can modulate pre-\ncision and recall, suggesting that there are both\n1988\nlower and upper bounds on the optimal number of\nattributes that LLMs can consider at once.\nWe also experimented by grouping attributes in\na prompt based on their semantic similarity, but\nthis did not yield any significant changes in perfor-\nmance. We leave it to future work to investigate\nfurther how to optimally choose the groups to use\nin a prompt during training and inference.\n6.3 Semantic Similarity and Generalization\nA major obstacle for NLP models is to apply the\nreasoning patterns they have learned to unseen at-\ntributes. Although the overall performance is lower\nin out-of-domain (best F1 = 23.5) compared to in-\ndomain experiments (best F1 = 51.4), we observe\nthat it varies significantly across different attributes.\nIn this part of our analysis, we investigate the mod-\nels’ generalization abilities to out-of-domain at-\ntributes, based on their relation to in-domain at-\ntributes.\nEssentially we identify two types of out-of-\ndomain attributes: (1) these that are semantically\nsimilar to some in-domain attribute(s), and (2)\nthese that have no similarity to any in-domain at-\ntribute. These two groups of attributes also evaluate\nthe degree of the model’s generalization abilities, as\nit is easier to generalize to different verbalizations\nof a previously seen attribute than to a completely\nnew concept. For this part of the analysis we use\nthe RoBERTa single-attribute prompt model, as it\nhas the best out-of-domain performance.\nTo identify related attributes, we firstly use co-\nsine similarity distance on top of an encoder trained\nfor semantic similarity (Reimers and Gurevych,\n2019). After manual curation, we identify 21 out-\nof-domain attributes that are closely related to in-\ndomain attributes (Group Matched), as we see in\nTable 7. The 20 remaining out-of-domain attributes\nare more dissimilar and do not have matching in-\ndomain attributes (Group Dissimilar).\nFor each of the two groups (Group Matched and\nGroup Dissimilar), we estimate the weighted-F1\nscore. We observe that Group Matched reaches\nF1 = 29.4, while Group Dissimilar F1 = 13.6. For\nGroup Matched, we also verify that the model’s\nperformance on closely related attributes is similar\nby measuring their Pearson correlation, which is\nr = 0.67 (p-value < 0.05). Both results indicate\nthat the model understands the semantics of the\nattributes despite different verbalizations, however,\nit struggles with more complex reasoning mecha-\nQuantifierT emporalMaterial\nEntity specificSens. Percept.\nBehavioral\nSpatial T otal\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7 Out-domain\nIn-domain\nFigure 5: F1 scores per attribute semantic type.\nnisms, such as applying the acquired patterns to\nentirely new attributes.\n6.4 Challenging Semantic Types\nIn this part of our analysis, we explore why some\nclasses of physical attributes appear to be inher-\nently more difficult for LLMs. More specifically,\nwe manually design an ontology of attributes into\nseven major semantic types and then group each\nin-domain and out-of-domain attribute according\nto the information it encodes, as seen in subsec-\ntion A.6. Via this analysis we aim to identify eval-\nuate each semantic type with respect to: (1) in-\ndomain performance, and (2) generalization to un-\nseen attributes. For this analysis we use RoBERTa\nsingle-attribute prompt, as it has the best out-of-\ndomain performance.\nFigure 5 shows that the model particularly strug-\ngles to predict attributes of the Quantifiers and\nTemporal semantic types (in-domain). These at-\ntributes are known to be challenging for current\nLLMs (Ravichander et al., 2019).\nWe further observe that the Entity-specific and\nMaterial semantic types are equally challenging\nfor both in-domain and out-of-domain attributes.\nThese semantic types describe inherent properties\nof an entity, such as fullness, that can only change\ndue to very specific events, such as put X into\nY. On the other hand, the Spatial and Behavioral\ntypes show a large discrepancy between in-domain\nand out-of-domain performance. This is surpris-\ning given that these semantic types contain high-\nfrequency attributes, like location. This highlights\nthe limitations of current models to predict physical\nchanges outside of controlled environments.\n6.5 Error Analysis\nTo identify the cause of low out-of-domain perfor-\nmance and study the models’ generalization abil-\n1989\nities, we perform a manual error analysis of out-\nof-domain outputs from the best performing mod-\nels: T5 k-attribute and RoBERTa single-attribute\nprompt.\nWe identify four major types of errors indicating\na varying degree of understanding of context and\nentities involved. The results of this analysis are\nshown in Table 4.\nFalse negatives: correct predictions that are miss-\ning from the annotations. This error type does\nnot reflect a failure of the models, but rather of\nthe dataset which was crowd sourced. Since out-\nof-domain attributes were introduced by workers\non Amazon Mechanical Turk, each annotator may\nintroduce attributes that were not considered by\nothers while annotating different instances. This\nis particularly prominent among similar concepts,\nsuch as width and size, which oftentimes change\ntogether. As we see in Table 4, false negatives\nare responsible for 41.5% of errors made by T5\nk-attribute prompt and 25.4% of those made by\nRoBERTa single-attribute. This highlights that the\ngap between out-of-domain and in-domain perfor-\nmance is narrower than what our automated evalu-\nation showed.\nFalse negative errors can be divided into two\nsubcategories. The first category accounts for\npredicted attributes that are synonyms of the\nannotated attributes and could replace them in\nthe particular instance. The second category\ncomprises predicted attributes that significantly\ndiffer but complement the annotated attributes,\nsuch as flexibility and size. We found that the first\ncategory of synonyms is responsible for 53% (T5\nk-attribute prompt) and 44% (RoBERTa single-\nattribute) of the instances withfalse negativeerrors.\nWrong context: predictions that could be correct\nfor the given entity, but incorrect given the context.\nThis error represents the models’ challenges with\nrespect to event implications and reasoning.\nWrong entity: wrong attribute change predictions\nfor the given entity in any context. This is the\nmost severe error since it shows that the model\nis not able to link the attributes to the entity.\nWhile this error is very rare for the T5 k-attribute\nmodel (only 2.7%), it is frequent for the RoBERTa\nsingle-attribute model (20.7%).\nNo prediction : instances with null predictions.\nThis is the most frequent error type for both\nError Type T5 RoBERTa\nk-attribute single-attribute\nFalse negatives 41.5% 25.4%\nWrong context 7.6 % 6.5%\nWrong entity 2.7% 20.7%\nNo prediction 48.2% 47.4 %\nTable 4: Error categories and prevalence of each cate-\ngory as a percentage of the number of instances. Based\non out-of-domain attributes. Wrong context implies the\nprediction could be correct for the given entity but is\nincorrect in the given context. Wrong entity means the\nattribute change does not apply to the given entity in\nany context.\nmodels, accounting for almost half of the errors.\nThis error occurs when the model decides that\nthere is no attribute change from the given list of\nattributes, which results in a significant drop in\nrecall. This highlights that both models struggle\nto identify which out-of-domain attributes are\nrelevant to a particular context and entity.\n7 Conclusion\nPredicting physical changes due to events is a chal-\nlenging problem for current models, especially\nin out-of-domain or limited-data scenarios. We\nshow that, by using proper task formulation, LLMs\ncan learn physical event implications even without\nphysical interactions. Future work should explore\nthe question of whether physical interactions are\nnecessary in more complex and realistic settings,\nby (1) providing more challenging datasets that test\nthe model limitations, and (2) ensure a fair compar-\nison of the language-only baselines.\nFurthermore, we show that the performance of\na LLM may significantly vary based on how we\nuse it, and, overall, LLMs can benefit from: (1)\nverbalizing the attributes, (2) varying the prompt\ninformation content across instances, and (3) query-\ning multiple attributes in the same instance. By\nfollowing these guidelines, we show significant im-\nprovements in unseen attributes and attributes of\nlow-frequency. Last, our error analysis and dis-\ncussion sections provide useful insights for future\nwork, with respect to prompt content and short-\ncomings of the current datasets that study physical\nevent implications.\n1990\n8 Acknowledgments\nWe thank the anonymous reviewers for their helpful\nfeedback. We also thank Alan Ritter and Lori Levin\nfor their comments and feedback.\n9 Limitations\nComputing resources The different prompting\nmethods have trade-offs in terms of computational\ncosts. In particular, the all-attribute and zero-\nattribute query all changes at once. With the k-\nattribute prompt, we query attributes in smaller\ngroups requiring on average #attributes /k times\nmore computations than for the all-attribute model\n(in our case five times). The single-attribute model\nencodes each attribute separately requiring #at-\ntributes times more computations. We were un-\nable to test GPT-3 for single-attribute because of\nthe cost of the larger number of queries it would\nhave required. The experiments that did not involve\nGPT-3 were run on two NVIDIA K-80 GPUs with\n12Gb memory.\nDataset limitations Given the complex nature\nthe event implication task, both datasets have sev-\neral limitations. PiGLET, which is based on a vir-\ntual environment, has relatively simple language\nthat is not representative of naturally occurring text.\nFurthermore, because it is a relatively small dataset\nwith respect to number of attributes and entities,\nthe training set covers a large subset of the possible\nconfigurations in that virtual environment. This\nexplains the very high performance of all models.\nAlthough Open PI does not suffer from such\nlimitations, we discovered several inconsistencies\nin the annotations. These inconsistencies mainly\ninvolve: (1) wrong attributes, (2) inconsistent la-\nbeling, and (3) duplication of attributes. Although\nwe manually edited several of these problems by\nmerging and filtering attributes, we could not ad-\ndress the inconsistencies in labeling. This resulted\ncould have influenced model performance.\nAutomatic Prompt Generation In this work, we\ndid not explore whether prompts can be automat-\nically generated. There have been several recent\nstudies aiming at generating either discrete or soft\nprompts (Shin et al., 2020; Lester et al., 2021).\nIn our case, the changes in information content\ninvolved a deeper understanding of the task and re-\nquired human involvement. As the field of prompt\ngeneration matures, future work could investigate\nautomating the process of finding prompts with\nvariable information content.\nMulti-task learning We do not directly explore\nbenefits from multi-task learning even though Raf-\nfel et al. (2020); Wei et al. (2021) show that this can\nsignificantly improve zero-shot and few-shot per-\nformance. However, the GPT-3 model that we used\nin our experiments is the Instruct GPT-3 model\nwhich is the result of additional prompt-based fine-\ntuning.\nReferences\nAnton Bakhtin, Laurens van der Maaten, Justin Johnson,\nLaura Gustafson, and Ross Girshick. 2019. Phyre: A\nnew benchmark for physical reasoning.\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob\nAndreas, Yoshua Bengio, Joyce Chai, Mirella Lap-\nata, Angeliki Lazaridou, Jonathan May, Aleksandr\nNisnevich, Nicolas Pinto, and Joseph Turian. 2020a.\nExperience Grounds Language. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP).\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020b. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nAntoine Bosselut, Omer Levy, Ari Holtzman, Corin\nEnnis, Dieter Fox, and Yejin Choi. 2017. Simulating\naction dynamics with neural process networks. arXiv\npreprint arXiv:1711.05313.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. arXiv preprint\narXiv:1906.05317.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nBhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-\ntau Yih, and Peter Clark. 2019. Everything happens\nfor a reason: Discovering the purpose of actions in\nprocedural text. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\n1991\non Natural Language Processing (EMNLP-IJCNLP),\npages 4496–4505.\nDima Damen, Hazel Doughty, Giovanni Maria Farinella,\nSanja Fidler, Antonino Furnari, Evangelos Kazakos,\nDavide Moltisanti, Jonathan Munro, Toby Perrett,\nWill Price, and Michael Wray. 2018. Scaling egocen-\ntric vision: The epic-kitchens dataset. In European\nConference on Computer Vision (ECCV).\nDenis Emelin, Ronan Le Bras, Jena D Hwang, Maxwell\nForbes, and Yejin Choi. 2020. Moral stories: Situated\nreasoning about norms, intents, actions, and their\nconsequences. arXiv preprint arXiv:2012.15738.\nChristiane Fellbaum. 2010. Wordnet. In Theory and ap-\nplications of ontology: computer applications, pages\n231–243. Springer.\nMaxwell Forbes, Jena D Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social chem-\nistry 101: Learning to reason about social and moral\nnorms. arXiv preprint arXiv:2011.00620.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2016. Tracking the world\nstate with recurrent entity networks. arXiv preprint\narXiv:1612.03969.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos qa: Machine reading com-\nprehension with contextual commonsense reasoning.\narXiv preprint arXiv:1909.00277.\nJena D Hwang, Chandra Bhagavatula, Ronan Le Bras,\nJeff Da, Keisuke Sakaguchi, Antoine Bosselut, and\nYejin Choi. 2020. Comet-atomic 2020: On symbolic\nand neural commonsense knowledge graphs. arXiv\npreprint arXiv:2010.05953.\nLiwei Jiang, Antoine Bosselut, Chandra Bhagavatula,\nand Yejin Choi. 2021. \" i’m not mad\": Commonsense\nimplications of negation and contradiction. arXiv\npreprint arXiv:2104.06511.\nEric Kolve, Roozbeh Mottaghi, Winson Han, Eli Van-\nderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon,\nYuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017.\nAi2-thor: An interactive 3d environment for visual ai.\narXiv preprint arXiv:1712.05474.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJohn McCarthy and Patrick J Hayes. 1981. Some philo-\nsophical problems from the standpoint of artificial\nintelligence. In Readings in artificial intelligence ,\npages 431–450. Elsevier.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint.\nBhavana Dalvi Mishra, Lifu Huang, Niket Tandon,\nWen-tau Yih, and Peter Clark. 2018. Tracking state\nchanges in procedural text: a challenge dataset and\nmodels for process paragraph comprehension. arXiv\npreprint arXiv:1805.06975.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nHannah Rashkin, Maarten Sap, Emily Allaway, Noah A\nSmith, and Yejin Choi. 2018. Event2mind: Com-\nmonsense inference on events, intents, and reactions.\narXiv preprint arXiv:1805.06939.\n1992\nAbhilasha Ravichander, Aakanksha Naik, Carolyn Rose,\nand Eduard Hovy. 2019. EQUATE: A benchmark\nevaluation framework for quantitative reasoning in\nnatural language inference. In Proceedings of the\n23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 349–361, Hong\nKong, China. Association for Computational Lin-\nguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI spring symposium: logical formal-\nizations of commonsense reasoning, pages 90–95.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky,\nNoah A Smith, and Yejin Choi. 2019a. Social bias\nframes: Reasoning about social and power implica-\ntions of language. arXiv preprint arXiv:1911.03891.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019b.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 33, pages\n3027–3035.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158.\nNiket Tandon, Bhavana Dalvi Mishra, Joel Grus, Wen-\ntau Yih, Antoine Bosselut, and Peter Clark. 2018.\nReasoning about actions and state changes by in-\njecting commonsense knowledge. arXiv preprint\narXiv:1808.10012.\nNiket Tandon, Keisuke Sakaguchi, Bhavana Dalvi,\nDheeraj Rajagopal, Peter Clark, Michal Guerquin,\nKyle Richardson, and Eduard Hovy. 2020. A dataset\nfor tracking entities in open domain procedural text.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 6408–6417.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexan-\nder M Rush, Bart Van Merriënboer, Armand Joulin,\nand Tomas Mikolov. 2015. Towards ai-complete\nquestion answering: A set of prerequisite toy tasks.\narXiv preprint arXiv:1502.05698.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. arXiv preprint\narXiv:1808.05326.\nRowan Zellers, Ari Holtzman, Matthew E Peters,\nRoozbeh Mottaghi, Aniruddha Kembhavi, Ali\nFarhadi, and Yejin Choi. 2021. Piglet: Language\ngrounding through neuro-symbolic interaction in a\n3d world. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 2040–2050.\n1993\nA Appendix\nOur experiments are built on top of the Hugging-\nface library (Wolf et al., 2019).\nA.1 Metrics\nOur task is a multi-label classification where, given\nsome context and an entity of interest, we need to\nidentify which attributes change. For most pairs\ncontext, entity, event implications affect only 1-2\nattributes. This results in a few positive instances\n(i.e., attributes that change) and a large number\nof negative instances (i.e., attributes that do not\nchange). Furthermore, we observe that the number\nof positive instances significantly varies across at-\ntributes: for example, in the training set of Open PI,\nlocation has 4505 positive instances, whiledistance\nonly 53. Due to the significant label imbalance, in\nour experiments we report micro- Precision, Recall,\nand F1 for the positive instances, across labels. In\naddition to these metrics, we measure per-attribute\nPrecision, Recall and F1 for both datasets.\nA.2 Hyperparameters\nWe performed hyperparameter search in the follow-\ning way. Based on the model size, we picked the\nlargest batch size that could fit on our GPUs. Then\nwe performed hyperparameter search on the dev set\n(6 values in range [10−3,10−6]), label smoothing\n(0, 0.1, 0.2) via grid search. We report in Table 5\nthe hyperparameters we use in each case. We used\nthe default values in the transformer library for the\nrest. For T5 we also varied the task prefix and its\nposition based on the relevant pre-training tasks,\nwithout observing significant differences. We use\nAdam with betas (0.9,0.999) and ϵ=1e-08 for T5\nexperiments. The runtime for each hyperparameter\ncombination in Open PI is: about 2 hours for multi-\nattribute, about one hour for zero-prompt, about\ntwo days for single-attribute (T5 and RoBERTa\nhave similar runtime).\nData Model Epochs Batch size Learning Rate Label Smoothing\nPiGLET RoBERTa,zero-prompt 30 20 4e-05 0.0T5 all-attr 50 32 3e-05 0.1Open PI RoBERTa,zero-prompt 20 32 1e-05 0.0RoBERTa,single-attr 6 16 1e-05 0.1T5 single-attr 8 16 5e-05 0.1T5 all-attr 8 16 5e-05 0.1T5k-attr 10 16 5e-05 0.1\nTable 5: Hyperparameters\nTo verify that model size differences do not\nimpact our results, we also did experiments\nwith RoBERTa-base zero-prompt, which shows\nvery similar performance to RoBERTa-large zero-\nprompt.\nA.3 In-domain Attributes and their\nFrequency\nAttribute Train Dev Test\nlocation 4505 360 803\ncleanness 1255 117 167\nwetness 1211 80 215\ntemperature 1184 91 184\nweight 1073 84 124\nfullness 694 62 122\nvolume 676 56 174\ncomposition 662 48 90\nshape 538 55 65\ntexture 515 34 74\nknowledge 409 27 119\norientation 330 15 45\ncolor 292 13 33\nsize 264 26 50\npower 245 11 18\norganization 242 14 37\nmotion 242 15 33\nownership 212 6 19\navailability 195 30 63\nstep 171 8 13\nspeed 151 3 18\npressure 148 4 14\ntaste 145 8 14\nlength 122 9 17\nelectric conductivity 121 9 18\nsmell 120 7 43\nsound 68 6 6\nbrightness 65 0 7\nthickness 64 4 16\nstrength 64 2 14\nhardness 63 5 10\nskill 62 3 4\nopenness 55 2 16\ncoverage 54 3 7\nstability 54 6 14\nfocus 53 4 5\ncost 53 6 9\ndistance 53 0 11\nappearance 44 8 8\ncomplexity 44 1 5\namount 40 3 16\nTable 6: Attribute occurrences in training, validation,\nand test sets.\n1994\nA.4 In-domain performance, per-attribute\nIn Figure 6 we show the in-domain F1 score per\nattribute for RoBERTa zero-prompt and T5 multi-\nattribute prompt models in Open PI. The attributes\nare sorted according to their frequency (decreas-\ning).\nWe observe that RoBERTa zero-prompt com-\npletely ignores all attributes with less than 150\ninstances. Furthermore, the only attributes that\nRoBERTa zero-prompt performs better are loca-\ntion, cleanness, temperature, size and power. Al-\nthough for 4/5 of these attributes the difference in\nF1 score between the two models is marginal, the\nfact that 3/5 belong to the most frequent attributes\n(more than 1000 instances) influences the overall\nmicro-F1.\n0 10 20 30 40 50 60 70 80\nAMOUNT\nCOMPLEXITY\nAPPEARANCE\nDISTANCE\nCOST\nFOCUS\nSTABILITY\nCOVERAGE\nOPENNESS\nSKILL\nHARDNESS\nSTRENGTH\nTHICKNESS\nBRIGHTNESS\nSOUND\nSMELL\nELECTRIC CONDUCTIVITY\nLENGTH\nTASTE\nPRESSURE\nSPEED\nSTEP\nAVAILABILITY\nOWNERSHIP\nMOTION\nORGANIZATION\nPOWER\nSIZE\nCOLOR\nORIENTATION\nKNOWLEDGE\nTEXTURE\nSHAPE\nCOMPOSITION\nVOLUME\nFULLNESS\nWEIGHT\nTEMPERATURE\nWETNESS\nCLEANNESS\nLOCATION\nF1 Score\nT5, multi-attr RoBERTa, zero-prompt\nFigure 6: F1 score per attribute for RoBERTa zero-\nprompt and T5 multi-attribute prompt models in Open\nPI.\nA.5 Semantically Similar Attributes\nIn Table 7 we show for every out-of-domain at-\ntribute, the most semantically similar in-domain\nattribute. This list contains only out-of-domain at-\ntributes that had a synonym from the in-domain\ngroup (Group Matched). This group was formed\nafter manual inspection of the automatically gener-\nated synonym pairs.\nOut-of-domain In-domain\nattribute synonym/antonym\nactivity motion\nangle orientation\narea shape\nbalance weight\ncapacity amount\nconsistency stability\ncontents composition\ndirection orientation\nflexibility stability\ngranularity composition\nheight length\nhydration wetness\nintensity brightness\nquantity amount\nsafety speed\nsoftness hardness\ntenseness pressure\ntension pressure\nthermal conductivity electric conductivity\ntightness pressure\nwidth length\nTable 7: The most semantically similar in-domain at-\ntribute, each out-of-domain attribute.\n1995\nSemantic Cluster In-domain Attributes Out-of-domain Attributes\nSpatial location, volume, shape, orientation,\nsize, length, distance, organization\nangle, direction, area, height, width,\npose, posture, spacial relation\nMaterial\ntexture, electric conductivity,\nthickness, hardness, strength,\npressure\ntenseness, tension, tightness,\nsoftness, material, flexibility,\nthermal conductivity, density,\ngranularity\nEntity-Specific\ncleanness, wetness, fullness,\nownership, openness, cost,\ncomposition, coverage, focus\ncontents, wholeness, capacity,\nhydration, consumption,\ndocumentation, emotional state,\npain, usage\nBehavioral knowledge, speed, motion, stability,\ncomplexity, skill\nactivity, balance, consistency,\nsafety, familiarity, exposure,\nviability, resistance\nQuantifier amount intensity, quantity, magnitude\nTemporal availability age, life, existence, time\nSensory Perception visibility color, taste, temperature, smell, sound,\nappearance, weight, brightness\nTable 8: Semantic clusters of attributes, both in-domain and out-of-domain.\nA.6 Semantic Clusters of Attributes\nTable 8 shows the semantic clusters of attributes\nwhich are the result of agglomerative clustering and\nmanually curation of in-domain and out-of-domain\nattributes. These clusters help better understand\nour attributes and performance based on their se-\nmantics. The clusters were used in Section 6.3.\nA.7 OpenPI Real Examples\nExamples from out-of-domain with model pre-\ndictions from the T5 k-attribute prompt and the\nRoBERTa single-attribute prompt models. In many\ninstances the predicted attribute is correct, but the\nannotations fail to reflect this.\nIn Table 9, we show some real instances that\nwe used in our error analysis. Although for\neach instance all the out-of-domain attributes were\nqueried, for brevity we only show attributes that\nwere identified as changed by either model or by\nthe annotations. We observe that in many of these\nexamples the models predict attribute changes that\nare correct, despite not being captured by the an-\nnotations. Such cases are Example 2, Example 4\nand Example 5, where the T5 k-attribute prompt\ncorrectly predicts attributes that were not identified\nby the annotators. These attributes are not neces-\nsarily related to the annotated attribute, such as\nwidth and resistance in Example 2, or hydration\nand softness in Example 4. However, some other\ninstances may have predicted attributes that are\nclosely related to the annotated attribute, as we see\nin Example 1, where posture and angle oftentimes\nchange together.\nOur final observation from Table 9 is that the\nmodels are able to correctly predict attributes that\nrequire some common sense knowledge, which\nwas not part of the provided context. For exam-\nple, T5 k-attribute prompt predicts in Example 4\nthat soaking beans implies that softness changes,\nsomething that is not as an obvious conclusion as\nthe change of hydration. Even more, in Example\n5 we observe that the model is able to understand\nthe intent of the paragraph, which is to change the\nsoftness of lips. These examples show that the T5\nk-attribute prompt model is able to perform some\ndegree of reasoning, even for predictions that were\nconsidered wrong due to missing annotations.\n1996\nExample 1\nContext:Begin by standing in Mountain Pose. Bend your right leg back and holdEntity:person\non to the inside of your foot behind you with your right hand. Annotated Attributes:balance\nT5k-attribute prompts:Consider the following attributes: flexibility, angle,T5k-attribute output:posture, flexibility,\nhydration, consumption. Which attribute changed for the person? angle, pose\nRoBERTa single-attribute prompts:Is the flexibility of the person different?RoBERTa single-attribute output:No\nIs the viability of the person different? Yes\nExample 2\nContext:Cut off a corner of a yeast packet. Entity:packet\nAnnotated Attributes:resistance\nT5k-attribute prompts:Consider the following attributes: contents, angle,T5k-attribute output:contents, width\nwidth, resistance, softness. Which attribute changed for the packet?\nRoBERTa single-attribute prompts:Is the width of the packet different? RoBERTa single-attribute output:Yes\nIs the resistance of the packet different? No\nExample 3\nContext:Drink a glass of hot milk. Entity:body\nAnnotated Attributes:thermal conductivity\nT5k-attribute prompts:Consider the following attributes: contents, hydration,T5k-attribute output:thermal conductivity\nthermal conductivity. Which attribute changed for the body?\nRoBERTa single-attribute prompts:Is the thermal conductivity of the body different?RoBERTa single-attribute output:No\nIs the hydration of the body different? Yes\nExample 4\nContext:Soak the dried beans and lentils overnight in a large bowl.Entity:beans\nAnnotated Attributes:hydration\nT5k-attribute prompts:Consider the following attributes: softness, contents,T5k-attribute output:softness\ngranularity, hydration. Which attribute changed for the beans?\nRoBERTa single-attribute prompts:Is the hydration of the beans different?RoBERTa single-attribute output:No\nIs the softness of the beans different? No\nExample 5\nContext:Take the honey and mix it with the sugar, then add in a little bit of Vaseline orEntity:lips\npetroleum jelly. When the mixture is all gritty, apply it on to your lips as you would withAnnotated Attributes:granularity\nlip balm. Leave on the mixture for about one minute.\nT5k-attribute prompts:Consider the following attributes: softness, pain,T5k-attribute output:softness, pain\ngranularity. Which attribute changed for the lips?\nRoBERTa single-attribute prompts:Is the softness of the lips different? RoBERTa single-attribute output:No\nIs the granularity of the lips different? No\nTable 9: Examples from out-of-domain and model predictions for the T5 k-attribute prompt and the RoBERTa\nsingle-attribute prompt models.\n1997"
}