{
  "title": "CMTR: Cross-modality Transformer for Visible-infrared Person Re-identification",
  "url": "https://openalex.org/W3207070197",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2565945365",
      "name": "Liang, Tengfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119042070",
      "name": "Jin Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076480306",
      "name": "Gao Ya-jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109330677",
      "name": "Liu Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1211077569",
      "name": "Feng, Songhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1962440459",
      "name": "Wang Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2742203536",
      "name": "Li Yidong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3043280139",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2520774990",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3004990178",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2982170673",
    "https://openalex.org/W3174246757",
    "https://openalex.org/W3034494316",
    "https://openalex.org/W2954773727",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2596603442",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3006129144",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3100506510"
  ],
  "abstract": "Visible-infrared cross-modality person re-identification is a challenging ReID task, which aims to retrieve and match the same identity's images between the heterogeneous visible and infrared modalities. Thus, the core of this task is to bridge the huge gap between these two modalities. The existing convolutional neural network-based methods mainly face the problem of insufficient perception of modalities' information, and can not learn good discriminative modality-invariant embeddings for identities, which limits their performance. To solve these problems, we propose a cross-modality transformer-based method (CMTR) for the visible-infrared person re-identification task, which can explicitly mine the information of each modality and generate better discriminative features based on it. Specifically, to capture modalities' characteristics, we design the novel modality embeddings, which are fused with token embeddings to encode modalities' information. Furthermore, to enhance representation of modality embeddings and adjust matching embeddings' distribution, we propose a modality-aware enhancement loss based on the learned modalities' information, reducing intra-class distance and enlarging inter-class distance. To our knowledge, this is the first work of applying transformer network to the cross-modality re-identification task. We implement extensive experiments on the public SYSU-MM01 and RegDB datasets, and our proposed CMTR model's performance significantly surpasses existing outstanding CNN-based methods.",
  "full_text": "CMTR: Cross-modality Transformer for Visible-infrared Person Re-identiﬁcation\nTengfei Liang1, Yi Jin1†, Yajun Gao1, Wu Liu2, Songhe Feng1, Tao Wang1, Yidong Li1\n1School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China\n2JD AI Research, Beijing, China\n{tengfei.liang, yjin, yajun.gao}@bjtu.edu.cn, liuwu1@jd.com, {shfeng, twang, ydli}@bjtu.edu.cn\nAbstract\nVisible-infrared cross-modality person re-identiﬁcation is a\nchallenging ReID task, which aims to retrieve and match\nthe same identity’s images between the heterogeneous vis-\nible and infrared modalities. Thus, the core of this task is\nto bridge the huge gap between these two modalities. The\nexisting convolutional neural network-based methods mainly\nface the problem of insufﬁcient perception of modalities’ in-\nformation, and can not learn good discriminative modality-\ninvariant embeddings for identities, which limits their perfor-\nmance. To solve these problems, we propose a cross-modality\ntransformer-based method (CMTR) for the visible-infrared\nperson re-identiﬁcation task, which can explicitly mine the in-\nformation of each modality and generate better discriminative\nfeatures based on it. Speciﬁcally, to capture modalities’ char-\nacteristics, we design the novel modality embeddings, which\nare fused with token embeddings to encode modalities’ infor-\nmation. Furthermore, to enhance representation of modality\nembeddings and adjust matching embeddings’ distribution,\nwe propose a modality-aware enhancement loss based on the\nlearned modalities’ information, reducing intra-class distance\nand enlarging inter-class distance. To our knowledge, this is\nthe ﬁrst work of applying transformer network to the cross-\nmodality re-identiﬁcation task. We implement extensive ex-\nperiments on the public SYSU-MM01 and RegDB datasets,\nand our proposed CMTR model’s performance signiﬁcantly\nsurpasses existing outstanding CNN-based methods.\nIntroduction\nPerson Re-Identiﬁcation (ReID) task aims to retrieve the\ngiven person’s images among multiple different cameras\nwith viewpoint and illumination changes, pose variations,\netc (Zheng, Yang, and Hauptmann 2016). It has been stud-\nied for many years, and the corresponding methods achieve\ngood performance (Luo et al. 2019; Wang et al. 2018; Ye\net al. 2021). However, existing ReID methods mainly focus\non the person retrieval in the single visible modality under\nRGB cameras, which constrains methods to be used only\nduring the daytime. To achieve full-time intelligent video\nsurveillance, based on the mechanism of existing surveil-\nlance cameras to automatically switch to infrared mode at\nnight, the visible-infrared cross-modality person ReID (VI-\nReID) task (Wu et al. 2017; Nguyen et al. 2017) is recently\n†Corresponding author.\nPull\nPush\nVisible ME Infrared ME\nFigure 1: Conceptual illustration of CMTR’s strategy. It ex-\nplicitly perceives modality information and generates corre-\nsponding embeddings. Based on enhanced modality embed-\ndings (ME), it can pull intra-class features and push inter-\nclass features to learn effective embeddings for retrieval.\nproposed to expand the application scope and attracts in-\ncreasing researchers’ attention in this ﬁeld. The VI-ReID re-\nquires methods that can match images of the same identity\nbetween the visible and infrared modalities, which is more\nchallenging because of the huge heterogeneous gap.\nThe visible and infrared images are generated by cameras\nthat capture light in different wavelength ranges. The former\nconsists of three channels (red, green, and blue) with the\ncolor information, while the latter only contains one chan-\nnel with infrared light radiation. They are intrinsically het-\nerogeneous and different. To reduce the huge modality gap,\na natural strategy proposed by researchers is to transform\nimages of one modality into another. There are some GAN-\nbased methods (Dai et al. 2018; Wang et al. 2019b, 2020,\n2019a) that attempt to learn the modality translation map-\nping. However, due to the heterogeneous imaging process,\nthe same gray in infrared images can be totally different col-\nors in visible images. Therefore, there is no reliable map-\nping relationship to support the generative model. Some re-\ncent methods have turned attention to the structural design\nof the convolutional neural network (CNN) model for more\neffective feature extraction. Based on the two-stream archi-\ntecture, some models (Ye et al. 2018; Hao et al. 2019b; Liu\net al.2020; Ye et al. 2021) are designed to use shallow layers\nwith unshared weights to extract shared features for different\narXiv:2110.08994v1  [cs.CV]  18 Oct 2021\n40 45 50 55 60mAP\n45\n50\n55\n60Rank-1 accuracy\nHPILN\nAlignGAN\nFMSP\nAGW cm-SSFT\nDEF\nX-Modality\nCMM+CML\nDDAGHAT\nTSLFN+HC\nCMTR\nFigure 2: Comparison with the existing CNN-based meth-\nods on SYSU-MM01 dataset. The Rank-1 and mAP of our\nCMTR model surpasses the others by a large margin on the\nmost difﬁcult single-shot setting of all-search mode.\nmodalities and use deep layers with shared weights to learn\ndiscriminative features. However, this strategy cannot well\nguarantee the envisaged layers’ function and ignore the suf-\nﬁcient perception and deeper mining of the built-in modality\ncharacteristics, resulting in limited performance.\nTo solve the aforementioned problems and break through\nthe limitations of the CNN-based approaches, we explore the\nnew architecture. Compared with CNN, transformer model\nshows advantages in single-modality ReID (He et al. 2021),\nobtaining global receptive ﬁeld with self-attention modules\nand complete spatial features without pooling layers, but it\nstill can’t solve the gap problem of this cross-modality task.\nIn this paper, we propose the cross-modality transformer\n(CMTR) model, which can capture modality characteristics\nby learnable embeddings in an explicit way and generate\nmore effective matching embeddings on this basis. Specif-\nically, to mine modalities’ characteristics, we ﬁrst introduce\nthe modality embeddings (ME) to our method. Similar to the\nidea of position embeddings in plain transformer (Vaswani\net al. 2017), our ME can be integrated into the input phase\nof the transformer framework by adding to the patches’ to-\nken embeddings. As shown in Figure 1, corresponding to\nthe visible and infrared modality, we deﬁne two learnable\nembeddings. They are used to learn information of each\nmodality, which can be helpful to the subsequent learning\nprocess of modality-invariant embeddings. To enhance the\nconstraint on the learnable ME and optimize matching em-\nbeddings’ distribution, we further design a novel loss func-\ntion, the modality-aware enhancement (MAE) loss. It con-\nsists of the modality-aware center loss and the modality-\naware ID loss with modality removal process by subtracting\nthe learned modalities’ knowledge from the ME, trying to\npull intra-class features and push inter-class features.\nCompared with the state-of-the-art models, our proposed\nCMTR method shows markedly outstanding performance\n(as shown in Figure 2). In general, the contributions of our\npaper can be mainly summarized as follows:\n• We propose a new cross-modality transformer (CMTR)\nnetwork, which is the ﬁrst transformer-based exploration\nfor the visible-infrared person re-identiﬁcation task.\n• We introduce the learnable modality embeddings (ME)\nto the CMTR network, which directly mine modalities’\ninformation and can be used effectively to alleviate the\ngap between the heterogeneous images.\n• We design a novel modality-aware enhancement (MAE)\nloss function that enforces the ME to capture more help-\nful characteristics of each modality and assist the gener-\nation of discriminative features.\n• Extensive experiments are conducted on SYSU-MM01\nand RegDB benchmarks and demonstrate our method’s\nsuperior performance against the existing methods.\nRelated Work\nVisible-infrared Person Re-identiﬁcation. The Visible-\ninfrared person re-identiﬁcation attempts to recognize visi-\nble and infrared images of a person under cameras of differ-\nent modalities. For the ﬁrst time, Wu et al. clearly deﬁned\nthe VI-ReID task (Wu et al. 2017). They contributed the\nchallenging large-scale SYSU-MM01 dataset and proposed\nthe basic Zero-Padding method. After that, many researchers\nput forward some new methods. For GAN-based methods,\nDai et al. designed the cmGAN (Dai et al. 2018) that uses\ngenerative adversarial networks (GAN) to learn discrimi-\nnative common representations for this cross-modality task.\nWang et al. proposed the Dual-level Discrepancy Reduction\nLearning (D2RL) method (Wang et al. 2019b), trying to han-\ndle the uniﬁed multi-spectral representation by image trans-\nlation. And Wang et al. introduced the AlignGAN method\n(Wang et al. 2019a) to jointly exploit pixel alignment and\nfeature alignment and reduce intra-modality variations. For\nthe two-stream or multi-stream CNN methods, Ye et al. suc-\ncessively proposed the BDTR framework (Ye et al. 2018),\nthe MAC model (Ye, Lan, and Leng 2019), the two-stream\nAGW (Ye et al. 2021), and the DDAG method (Ye et al.\n2020), using special structure and loss constraints to make\nthem learn discriminative features implicitly. Li et al. (Li\net al. 2020) designed a three-stream structure and introduced\nan auxiliary X modality to pull different modalities’ images.\nHowever, the above methods ignore the direct mining and\nutilization of modality information, which limits their per-\nformance. Different from them, our method can effectively\novercome this limitation with promising performance.\nResearch of Transformer. The Transformer method was\nﬁrst proposed by Vaswani et al. (Vaswani et al. 2017) to\nsolve machine translation tasks in the ﬁeld of natural lan-\nguage processing. It and its variants dominate the ﬁeld for a\nlong time. In recent years, researchers successfully applied\nit to lots of visual tasks, showing its superiority to convolu-\ntional neural networks, such as image classiﬁcation (Doso-\nvitskiy et al. 2021), object detection (Carion et al. 2020),\nsemantic segmentation (Zheng et al. 2021), etc. Based on\nthe vision transformer model, our proposed method focuses\non the modalities’ heterogeneous gap of the cross-modality\nvisible-infrared person re-identiﬁcation, which is the ﬁrst\nexploration in this task and achieves good results.\nLinear Projection of Flattened Patches\nVision Transformer (ViT)\n…\n1 2 3 4 5 6 7 8 N…Position\nEmbedding\n* Extra learnable\n[class] embedding\n0 *\nID Loss\nWRT Loss\nMAE Loss\nOverlapping Patches\nModality\nEmbedding\n(a) Visible-infrared Cross-modality Transformer (CMTR) Network\n(b) Visible-infrared Modality Embeddings(ME)\n(c) Modality-aware Enhancement(MAE) Loss\nModality-aware \nCenter Loss \nModality-aware\nID Loss \nID - 1\nID - 2Visible Infrared\nBN FC\nFigure 3: The framework of our proposed method. (a) The overall structure of visible-infrared cross-modality transformer\n(CMTR) network, which is based on the Vision Transformer (ViT) backbone with multiple loss constraints. (b) The concep-\ntual illustration of the designed visible-infrared modality embeddings (ME). (c) The diagram of modality-aware enhancement\n(MAE) loss, which contains two components, the modality-aware center loss and the modality-aware ID loss.\nThe Proposed Method\nIn this section, the proposed visible-infrared cross-modality\ntransformer (CMTR) network is explained in detail. We in-\ntroduce the overall network structure in the ﬁrst subsection.\nThen we focus on the designed modality embeddings and\nmodality-aware enhancement loss and explain their deﬁni-\ntion and function in the next two subsections. In the last\nsubsection of this part, we give the overall formula of the\nobjective function during the process of optimization.\nOverall Network Structure\nOur CMTR network is built with the vision transformer\nframework (Dosovitskiy et al. 2021), and we adapt it to the\nVI-ReID task. For input images, we let visand irrepresent\nthe visible modality and infrared modality. Thus, the visible\nimage set is denoted as Xvis = {xvis|xvis ∈RC×H×W},\nand the infrared image set is denoted as Xir = {xir|xir ∈\nRC×H×W}. The C, H, W denote images’ channel, height\nand width respectively. In a training batch, there are B im-\nages with the same number of xvis\ni and xir\ni , where i ∈\n{1,2,...,B/ 2}. As shown in Figure 3a, our method mainly\ncontains three stages from the bottom to the top: input em-\nbedding, feature extraction, and multi-loss constraint.\nIn the stage of input embedding, as illustrated at the bot-\ntom of Figure 3a, here is an example when the input is a\nvisible image (it is similar when inputting an infrared im-\nage). The input image xvis\ni is ﬁrst split into a sequence of\npatches with the shape of RN×C×P×P, where P denotes\nthe size and N denotes the length of this sequence. Be-\nsides, following (Yuan et al. 2021; He et al. 2021)’s strat-\negy, we generate the patches with overlapping by stride S\n(S < P) (soft split) to enhance the correlation among adja-\ncent patches. The patches are reshaped to ﬂattened embed-\ndings with shape of RN×(C×P2). Then through the linear\nprojection, they are converted to a sequence of token em-\nbeddings (RN×D, Ddenotes the embedding dimension). An\nextra learnable [class] token embedding is merged into the\nsequence to capture the global attention of the whole image.\nIn the CMTR network, before being sent to the transformer,\nthe token embedding sequence is fused with position em-\nbeddings and the designed modality embeddings (ME).\nIn feature extraction stage, the vision transformer (ViT)\n(Dosovitskiy et al. 2021) model is used as the backbone\nextractor. By using multi-layer self-attention modules, the\nmodel can perceive more effective global features than\nCNN-based methods. As the top of Figure 3a shows, cor-\nresponding to the location of [class] token, we can get the\nimage vector for each xm\ni in the backbone’s output. Let I\nand Fdenote the process of input embedding and feature\nextraction. The image vector can be extracted as follows:\nvm\ni = F(I(xm\ni )) m∈{vis,ir} (1)\nDuring the last stage of multi-loss constraint, these image\nvectors vm\ni obtained from the training batch pass through\nbatch normalization (BN) layer and fully connected (FC)\nlayer. And after these different layers, the method calculates\nmultiple losses, the identity (ID) loss, weighted regulariza-\ntion triplet (WRT) loss and our proposed modality-aware en-\nhancement Loss (MAE) loss, jointly constraining the vec-\ntors’ distribution to generate more discriminative ID embed-\ndings that are invariant to the visible-infrared modalities.\n…\nInput\nToken\nEmbeddings …*\nPosition\nEmbeddings 1 2 3 4 5 N…0\nModality\nEmbeddings …\nInput\nToken\nEmbeddings …*\nPosition\nEmbeddings 1 2 3 4 5 N…0\nModality\nEmbeddings\n…\n…\n…\nInput\nToken\nEmbeddings …*\nPosition\nEmbeddings 1 2 3 4 5 N…0\nModality\nEmbeddings …\nInput\nToken\nEmbeddings …*\nPosition\nEmbeddings 1 2 3 4 5 N…0\nModality\nEmbeddings\n…\n…\nFigure 4: The CMTR’s input embeddings. The input em-\nbeddings are the sum of the token embeddings, the position\nembeddings and the designed modality embeddings.\nVisible-infrared Modality Embeddings\nPerception of modality characteristics is helpful to generate\nmodality-invariant features. However, this key is ignored by\nmany existing methods. To achieve this, we introduce the\nmodality embeddings (ME) into our CMTR model, which\ndirectly aim to learn and capture each modality’s inherent\ninformation and characteristics.\nInspired by the idea of the position embeddings in trans-\nformer (Vaswani et al. 2017) or the segmentation embed-\ndings in BERT (Devlin et al. 2019) that can learn posi-\ntional information or segmented information, our modal-\nity embeddings are introduced in a similar way with the\ndifferent purpose to encode modalities’ information. This\ndesign can be naturally integrated into the transformer\nframework, which CNN-based models do not have this ad-\nvantage. As shown in Figure 4, our CMTR’s input em-\nbeddings are calculated with three components, i.e., to-\nken embeddings, position embeddings and modality em-\nbeddings. The ﬁrst two are consistent with previous meth-\nods. For modality embeddings, images in each modality\nshare the same embeddings with all patches. Speciﬁcally, let\n{xm\ni,p1,xm\ni,p2,xm\ni,p3,··· ,xm\ni,pN}denote the patch sequence of\nimage xm\ni . The {epos\np1 ,epos\np2 ,epos\np3 ,··· ,epos\npN}denotes position\nembeddings. The stage of input embedding I(xm\ni ) can be\nformulated as follows:\nI(xm\ni ) =LP({xm\ni,p1,xm\ni,p2,xm\ni,p3,··· ,xm\ni,pN})\n+ {epos\np1 ,epos\np2 ,epos\np3 ,··· ,epos\npN}\n+\n{{evis,evis,evis,··· ,evis}, if mis vis.\n{eir ,eir ,eir , ··· ,eir}, if mis ir.\n(2)\nwhere the LPdenotes the linear projection in Figure 3a,\nconverting patches’ information into token embeddings.\nAnd the evis and eir denote the visible modality embedding\nand the infrared modality embeddings respectively.\nAs shown in Equation 2, these different types of embed-\ndings are fused together in an additive manner. The posi-\ntion embeddings epos vary among patches, while the modal-\nity embeddings em (m ∈{vis,ir}) vary between images’\nmodalities, perceiving different types of information.\nModality-aware Enhancement Loss\nThe above-mentioned approach of how to use the modal-\nity embeddings makes them capture modalities’ characteris-\ntics semantically, but the constraint of this way is relatively\nweak. To further enhance ME’s ability to learn the modal-\nity information, and let the learned ME assist in generating\nmore effective modality-invariant embeddings, we propose\nthe Modality-aware Enhancement (MAE) loss.\nAs shown at the top of Figure 3, the MAE loss acts on\nthe extracted features after batch normalization (BN), which\nare used as the matching features during testing. We let\nfm\ni = BN(vm\ni ) denote the extracted features. The MAE\nloss consists of two parts (Figure 3c): the modality-aware\ncenter loss and the modality-aware ID loss, which are de-\nsigned to pull intra-class features and push inter-class fea-\ntures based on the modality embeddings. And our MAE loss\nLMAE is calculated by adding them as follows:\nLMAE = LMAC + LMAID (3)\nwhere LMAC denotes the modality-aware center loss, and\nLMAID denotes the modality-aware ID loss.\nFor the deﬁnition of LMAC, it focuses on reducing the\ngap between different modalities under the same identity,\nand utilize the learned knowledge from ME to narrow the\nintra-class features’ distance. During training, we sample Q\nidentities’ images in a batch. Each identity contains K/2\nvisible images and K/2 infrared images. Speciﬁcally, the\nLMAC can be formulated by:\nLMAC =\nQ∑\nq=1\nK∑\nk=1\nlog (1 + expD(fm\nq,k−φm(em),fm\nq,c))\nfm\nq,c = 1\nK\nK∑\nk=1\n(fm\nq,k −φm(em)) m∈{vis,ir}\n(4)\nwhere fm\nq,k denotes the extracted feature from qidentity’sk\nimage with mmodality. φm(·) denotes the mapping to mine\nthe knowledge of modality embeddings em, and in practice,\nwe use the full connection layer to implement it. In this for-\nmula, we let the fm\nq,k subtract the corresponding φm(em) di-\nrectly to remove modality-speciﬁc information and ﬁlter out\nmodality-invariant features. The fm\nq,c denotes center feature\nvector of the q identity, which is the mean value of the im-\nage features after modality removal. The LMAC pulls the\ndistance between id’s image features and its center feature\nvector, and we use the cosine distance D(·,·) to measure\ntheir difference. Besides, LMAC use the soft-margin con-\nstraint to avoid setting hyperparameter of the hard margin in\ntraditional distance loss functions (Schroff, Kalenichenko,\nand Philbin 2015; Luo et al. 2019). Through the constraint\nof modality-aware center loss LMAC, our method extracts\nmore compact cross-modality features for each identity.\nThe modality-aware ID loss LMAID aims at learning dis-\ncriminative features among different identities, which is also\nbased on the learned ME’s information, designed to push\nthe distance between ids’ image features. The equation of\nLMAID can be formulated as follows:\nLMAID =\nQ∑\nq=1\nK∑\nk=1\nCrossEntropy(pm\nq,k,tm\nq,k)\npm\nq,k = Softmax(FCid(fm\nq,k −φm(em)))\n(5)\nwhere tm\nq,k denotes the one-hot target label for q identity.\nThe predicted label pm\nq,k is calculated from the image fea-\ntures fm\nq,k −φm(em) with the same modality removal pro-\ncess as LMAC. We use the auxiliary FC layer FCid to\ngenerate logits for classiﬁcation, and the pm\nq,k is obtained\nthrough the Softmax operation on logits. The LMAID cal-\nculates CrossEntropy(·,·) between predictions and tar-\ngets, attempting to classify different identities’ input images.\nWith the modality-aware ID loss LMAID’s constraint, the\nfeatures extracted by the model are given stronger distin-\nguishing ability to achieve more accurate matching.\nBy optimizing the modality-aware enhancement loss\nLMAE, ﬁrstly, the network can utilize the modality re-\nmoval process to enforce ME to mine more useful modality-\nspeciﬁc characteristics, which is a more direct way to en-\nhance the ME’s representation. Secondly, the ME-based loss\nfunctions can adjust the distribution of feature embeddings\nto be more discriminative for the image retrieval and less\naffected by the heterogeneous cross-modality gap.\nOverall Objective Function\nAs shown at the top of Figure 3a, our CMTR network is\nconstrained by three kinds of losses, and these constraints\nare jointly optimized. The overall objective function can be\ndeﬁned as follows:\nLoverall = LID + LWRT + λ·LMAE (6)\nwhere LID and LWRT denote the identity (ID) loss and\nweighted regularization triplet (WRT) loss respectively, and\nthey are common loss constraints (Luo et al. 2019; Ye et al.\n2021) in ReID task. Thus, we adopt these losses in similar\nlocations. As for our proposed modality-aware enhancement\nloss LMAE, it is added to the formula with weight, and the\nhyperparameter λcan control the proportion of this loss.\nExperiments\nDatasets and Settings\nDatasets: Our experiments are performed on two pub-\nlic datasets, SYSU-MM01 (Wu et al. 2017) and RegDB\n(Nguyen et al. 2017), which are the standard benchmarks\nand commonly used by existing methods in VI-ReID task.\nSYSU-MM01 is currently the largest and most challeng-\ning visible-infrared cross-modality person ReID dataset. It\ntotally consists of 29,033 visible images and 15,712 infrared\nimages of 491 identities, which are collected by 4 visible\ncameras and 2 infrared ones from indoors and outdoors. The\ntraining set contains 22,258 visible images and 11,909 in-\nfrared images of 395 identities, and the testing set contains\n96 identitie’ images. Following (Wu et al. 2017), 3,803 in-\nfrared images of these testing identities are used to form the\nquery set. Corresponding to the single-shot or multi-shot set-\nting, 1 or 10 images of each identity under each visible cam-\nera are randomly selected to form the gallery set. Besides,\nthere are two testing modes: the all-search mode is evaluated\nwith the indoor and outdoor images, while the indoor-search\nmode is evaluated with only indoor images.\nRegDB is collected by dual aligned visible and far-\ninfrared cameras, including 412 identities’ images. Each\nidentity has 10 visible images and 10 far-infrared images.\nConsistent with previous methods (Wang et al. 2019a; Lu\net al. 2020; Zhao et al. 2021), we equally divide the dataset\ninto two parts as the training set and testing set by ran-\ndom selection. Each set contains 2,060 visible images and\n2,060 far-infrared images. In testing set, when performing\nVisible to Thermal/Thermal to Visible mode, all the 2,060\nvisible/far-infrared images are used as query set, and all the\n2,060 far-infrared/visible images are used as gallery set.\nEvaluation Metrics: We evaluate methods with two\nwidely used metrics of this task: the Cumulative Matching\nCharacteristics (CMC) curve and the mean Average Preci-\nsion (mAP). The CMC is denoted as Rank-k (Rk for short)\nto measure the correct rate in the k-nearest matching results,\nand we calculate R1, R10, R20 in the experiments. Besides,\naccording to (Ye et al. 2021), the mean inverse negative\npenalty (mINP) is also used as an auxiliary metric in our\nablation study. Following (Wu et al. 2017), we conduct re-\npeated random selections of gallery on SYSU-MM01 for 10\ntimes to get the more stable average result. Similarly, our\nexperiments on RegDB average the results from 10 times\nrepeated random partition of training and testing sets.\nImplementation Details: Our proposed method is imple-\nmented with the PyTorch (Paszke et al. 2019) deep learn-\ning framework. For the transformer backbone, we use the\nViT-Base (Dosovitskiy et al. 2021) model with pretrained\nweights on the ImageNet (Deng et al. 2009) dataset. Before\nentering the network, the visible and infrared images are re-\nsized to 3×256×128 (C×H×W). We repeat the infrared\nimage’s single channel three times to make it contain three\nchannels. The patches are generated with 16 ×16 size fol-\nlowing (Dosovitskiy et al. 2021), and the stride S is set to\n8 for half overlap. During training, we adopt the common\ndata augmentation strategies: the random horizontal ﬂip and\nrandom erasing (Zhong et al. 2020). In a mini-batch, we ran-\ndomly sample 8 identities’ images and each identity has 4\nvisible images and 4 infrared images. We use the AdamW\n(Loshchilov and Hutter 2019) optimizer with weight de-\ncay set to 0.0005. The whole model is totally trained for\n70 epochs, and the base learning rate is initialized at 0.001\nwith decay by 0.1 at epoch 15 and 30. Besides, we make all\npretrained layers’ learning rate to be 0.1 times of the base\nlearning rate. The trade-off hyperparameter λ in objective\nfunction is empirically set to 4. During testing, all the query\nand gallery images are sent into the model to extract feature\nembeddings with cosine distance to rank retrieval results.\nMethods Venue\nAll-Search Indoor-Search\nSingle-Shot Multi-Shot Single-Shot Multi-Shot\nR1 R10 R20 mAP R1 R10 R20 mAP R1 R10 R20 mAP R1 R10 R20 mAP\ncmGAN (Dai et al. 2018) IJCAI 18 26.97 67.51 80.56 27.80 31.49 72.74 85.01 22.27 31.63 77.23 89.18 42.19 37.00 80.94 92.11 32.76\nD2RL (Wang et al. 2019b) CVPR 19 28.90 70.60 82.40 29.20 - - - - - - - - - - - -\nHi-CMD (Choi et al. 2020) CVPR 20 34.94 77.58 35.94 - - - - - - - - - - - -\nJSIA (Wang et al. 2020) AAAI 20 38.10 80.70 89.90 36.90 45.10 85.70 93.80 29.50 43.80 86.20 94.20 52.90 52.70 91.10 96.40 42.70\nAlignGAN (Wang et al. 2019a) ICCV 19 42.40 85.00 93.70 40.70 51.50 89.40 95.70 33.90 45.90 87.60 94.40 54.30 57.10 92.70 97.40 45.30\nTS-GAN (Zhang et al. 2020) ArXiv 20 49.80 87.30 93.80 47.40 56.10 90.20 96.30 38.50 50.40 90.80 96.80 63.10 59.30 91.20 97.80 50.20\nZero-Padding (Wu et al. 2017) ICCV 17 14.80 54.12 71.33 15.95 19.13 61.40 78.41 10.89 20.58 68.38 85.79 26.92 24.43 75.86 91.32 18.64\nBDTR (Ye et al. 2018) IJCAI 18 17.01 55.43 71.96 19.66 - - - - - - - - - - - -\nD-HSME (Hao et al. 2019b) AAAI 19 20.68 62.74 77.95 23.12 - - - - - - - - - - - -\nSDL (Kansal et al. 2020) TCSVT 20 28.12 70.23 83.67 29.01 - - - - 32.56 80.45 90.67 39.56 - - - -\nMAC (Ye, Lan, and Leng 2019) MM 19 33.26 79.04 90.09 36.22 - - - - - - - - - - - -\nMSR (Feng, Lai, and Xie 2020) TIP 19 37.35 83.40 93.34 38.11 43.86 86.94 95.68 30.48 39.64 89.29 97.66 50.88 46.56 93.57 98.80 40.08\nHPILN (Zhao et al. 2019) IET IP 19 41.36 84.78 94.51 42.95 47.56 88.13 95.98 36.08 45.77 91.82 98.46 56.52 53.05 93.71 98.93 47.48\nFMSP (Wu et al. 2020) IJCV 20 43.56 - - 44.98 - - - - 48.62 - - 57.50 - - - -\nAGW (Ye et al. 2021) TPAMI 21 47.50 84.39 92.14 47.65 - - - - 54.17 91.14 95.98 62.97 - - - -\ncm-SSFT (Lu et al. 2020) CVPR 20 47.70 - - 54.10 57.40 - - 59.10 - - - - - - - -\nDEF (Hao et al. 2019a) MM 19 48.71 88.86 95.27 48.59 54.63 91.62 96.83 42.14 52.25 89.86 95.85 59.68 59.62 94.45 98.07 50.60\nX-Modality (Li et al. 2020) AAAI 20 49.92 89.79 95.96 50.73 - - - - - - - - - - - -\nCMM+CML (Ling et al. 2020) MM 20 51.80 92.72 97.71 51.21 56.27 94.08 98.12 43.39 54.98 94.38 99.41 63.70 60.42 96.88 99.50 53.52\nDDAG (Ye et al. 2020) ECCV 20 54.75 90.39 95.81 53.02 - - - - 61.02 94.06 98.41 67.98 - - - -\nHAT (Ye, Shen, and Shao 2021) TIFS 21 55.29 92.14 97.36 53.89 - - - - 62.10 95.75 99.20 69.37 - - - -\nTSLFN+HC (Zhu et al. 2020) NeuroC 20 56.96 91.50 96.82 54.95 62.09 93.74 97.85 48.02 59.74 92.07 96.22 64.91 69.76 95.85 98.90 57.81\nCMTR (Ours) - 62.58 93.79 98.01 61.33 68.39 95.73 98.82 55.69 67.02 96.86 99.40 73.78 75.40 98.37 99.52 66.84\nTable 1: Comparison with CNN-based cross-modality ReID methods on the SYSU-MM01 dataset. We roughly divide the\nexisting methods into two categories: GAN-based methods and two/multi-stream CNN methods with different loss functions.\nComparison with State-of-the-art Methods\nIn experiments, we compare our proposed CMTR model\nwith the existing outstanding CNN-based methods in the\nvisible-infrared cross-modality person ReID task. As shown\nin Table 1, we present the quantitative comparison on the\nSYSU-MM01 dataset. Our CMTR method signiﬁcantly out-\nperforms the other existing methods in all evaluation modes,\nincluding all-search/indoor-search with single-shot/multi-\nshot modes. In the most challenging single-shot all-search\nmode, the CMTR achieves 62.58% Rank-1 and 61.33%\nmAP, and it is worth noting that even compared with the\nstrong model TSLFN+HC (Zhu et al. 2020) using a com-\nplicated local blocking strategy, our method has a large im-\nprovement (+5.62% Rank-1, +6.38% mAP) with only global\nfeature extraction. From Figure 2, we can also intuitively ob-\nserve the superior performance of the CMTR.\nIn Table 2, we show the comparison results on the\nRegDB dataset. Our CMTR method shows great per-\nformance, which achieves 80.62% (+8.79%) Rank-1 and\n74.42% (+6.86%) mAP under Visible to Thermal (V to T)\nmode and 81.06% (+11.04%) Rank-1 and 73.75% (+7.45%)\nmAP under Thermal to Visible (T to V) mode, surpassing\nthe latest HAT (Ye, Shen, and Shao 2021) method by a large\nmargin. All the experimental results demonstrate the effec-\ntiveness and robustness of our proposed method.\nMethods Venue V to T T to V\nR1 mAP R1 mAP\nZero-Padding (Wu et al. 2017) ICCV 17 17.75 18.90 16.63 17.82\nSDL (Kansal et al. 2020) TCSVT 20 26.47 23.58 25.74 22.89\nBDTR (Ye et al. 2018) IJCAI 18 33.47 31.83 32.72 31.10\nMAC (Ye, Lan, and Leng 2019) MM 19 36.43 37.03 36.20 36.63\nHSME (Hao et al. 2019b) AAAI 19 41.34 38.82 40.67 37.50\nD2RL (Wang et al. 2019b) CVPR 19 43.40 44.10 - -\nMSR (Feng, Lai, and Xie 2020) TIP 19 48.43 48.67 - -\nJSIA (Wang et al. 2020) AAAI 20 48.50 49.30 48.10 48.90\nD-HSME (Hao et al. 2019b) AAAI 19 50.85 47.00 50.15 46.16\nAlignGAN (Wang et al. 2019a) ICCV 19 57.90 53.60 56.30 53.40\nFMSP (Wu et al. 2020) IJCV 20 65.07 64.50 - -\nCMM+CML (Ling et al. 2020) MM 20 - - 59.81 60.86\nX-Modality (Li et al. 2020) AAAI 20 - - 62.21 60.18\ncm-SSFT (Lu et al. 2020) CVPR 20 65.40 65.60 63.80 64.20\nDDAG (Ye et al. 2020) ECCV 20 69.34 63.46 68.06 61.80\nDEF (Hao et al. 2019a) MM 19 70.13 69.14 67.99 66.70\nHi-CMD (Choi et al. 2020) CVPR 20 70.93 66.04 - -\nHAT (Ye, Shen, and Shao 2021) TIFS 21 71.83 67.56 70.02 66.30\nCMTR (Ours) - 80.62 74.42 81.06 73.75\nTable 2: Comparison with CNN-based on RegDB dataset.\n(a)\n (b)\n (c)\n (d)\nFigure 5: Feature distributions visualized with t-SNE method. (a) Feature distribution of BASE. (b) Feature distribution of\nBASE+MAC. (c) Feature distribution of BASE+MAID. (d) Feature distribution of BASE+MAE (MAC&MAID).\nIndex BASE ME MAE R1 R10 R20 mAP mINPMAC MAID\n1 \u0013 \u0017 \u0017 \u0017 54.28 90.66 96.43 53.97 41.43\n2 \u0013 \u0013 \u0017 \u0017 57.40 89.31 95.13 56.16 43.86\n3 \u0013 \u0013 \u0013 \u0017 59.53 92.63 97.22 59.40 47.90\n4 \u0013 \u0013 \u0017 \u0013 60.53 91.37 96.24 58.73 45.71\n5 \u0013 \u0013 \u0013 \u0013 62.58 93.79 98.01 61.33 49.01\nTable 3: Ablation studies on the SYSU-MM01 dataset.\nAblation Study\nThe ablation experiments are designed to evaluate the in-\nﬂuence of our proposed modality embeddings (ME) and\nmodality-aware enhancement (MAE) loss. We conduct these\nexperiments on the SYSU-MM01 dataset with the difﬁcult\nsingle-shot setting of all-search mode.\nEffectiveness of the ME and MAE Loss. As shown in\nTable 3, the ”BASE” represents the baseline ViT network\ntrained with the common LID and LWRT (Index-1). By in-\ntroducing modality embeddings (ME) into this network, the\nmodel (Index-2) achieves 3.12% Rank-1 and 2.19% mAP\nimprovements. For the study of modality-aware enhance-\nment (MAE) loss, we verify the effects of its components\nLMAC and LMAID respectively (Index-3 & 4). Compared\nwith ”BASE”+”ME”, the MAC loss brings +2.13% Rank-1\nand +3.24% mAP and the MAID loss brings +3.13% Rank-1\nand +2.57% mAP. What’s more, through jointly optimizing\nthe MAC loss and the MAID loss, i.e., the MAE loss, our\nmethod’s performance is further improved (+5.18% Rank-1\nand +5.17% mAP), which presents the complementarity and\npotentiality between these two kinds of losses.\nAnalysis of the Modality Embeddings. To better under-\nstand the inﬂuence of modality embeddings, we visualized\nattention maps of the BASE and BASE+ME models. As il-\nlustrated in Figure 6, the 6a and 6b show visible and in-\nfrared images of an identify in the test set. We use the Grad-\nCAM (Selvaraju et al. 2017) method to generate attention\nmaps (Figure 6c-f) on them. Compared with Figure 6c, we\ncan notice that Figure 6e captures more id’s proﬁle and tex-\nture information (such as the pattern on this man’s shirt),\nwhich are modality-invariant. For infrared image 6b, it is dif-\n(a)\n (b)\n (c)\n (d)\n (e)\n (f)\nFigure 6: Visualization with Grad-CAM method. (a/b) Vis-\nible/Infrared image. (c/d) BASE’s CAM on visible/infrared\nimage. (e/f) BASE+ME’s CAM on visible/infrared image.\nﬁcult for human eyes to distinguish the pattern on the shirt,\nBASE+ME can still perceive the information of this posi-\ntion in Figure 6f. Besides, the consistency of 6e and 6f is\nhigher than that of 6c and 6d, which demonstrates the auxil-\niary ability of ME to reduce the modality difference.\nAnalysis of the MAE Loss. We show feature distributions\nof the learned embeddings with t-SNE (Van der Maaten and\nHinton 2008) method in Figure 5. Different colors represent\ndifferent IDs. The solid square and hollow circle represent\ntwo modalities respectively. From Figure 5a to 5b and 5c,\nthe MAC loss helps reduce the intra-class differences, and\nthe MAID loss helps expand the inter-class differences. As\nshown in Figure 5d, when using the MAE loss, embeddings\nhave better distribution with compact intra-class distances\nand uniformly larger inter-class distances. Thus, MAE loss\nhelps to generate more effective embeddings for retrieval.\nConclusion\nIn this paper, we propose a novel method for the VI-ReID\ntask, the cross-modality transformer (CMTR) network. By\nintroducing the modality embeddings (ME), the model can\ndirectly perceive characteristics of each modality. Further-\nmore, we design the modality-aware enhancement Loss,\nwhich can enhance the ME’s learning ability and help to\ngenerate better discriminative modality-invariant embed-\ndings. The method shows great experimental performance\nagainst CNN-based methods on SYSU-MM01 and RegDB\ndatasets. We believe that the proposed strategy will provide\npromising solutions for other cross-modality vision task.\nReferences\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-End Object Detection\nwith Transformers. InECCV 2020, volume 12346, 213–229.\nChoi, S.; Lee, S.; Kim, Y .; Kim, T.; and Kim, C. 2020.\nHi-CMD: Hierarchical Cross-Modality Disentanglement for\nVisible-Infrared Person Re-Identiﬁcation. In CVPR 2020,\n10254–10263.\nDai, P.; Ji, R.; Wang, H.; Wu, Q.; and Huang, Y . 2018. Cross-\nModality Person Re-Identiﬁcation with Generative Adver-\nsarial Training. In IJCAI 2018, 677–683.\nDeng, J.; Dong, W.; Socher, R.; Li, L.; Li, K.; and Li, F.\n2009. ImageNet: A large-scale hierarchical image database.\nIn CVPR 2009, 248–255.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT 2019, volume 1,\n4171–4186.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR 2021.\nFeng, Z.; Lai, J.; and Xie, X. 2020. Learning Modality-\nSpeciﬁc Representations for Visible-Infrared Person Re-\nIdentiﬁcation. IEEE Transactions on Image Process, 29:\n579–590.\nHao, Y .; Wang, N.; Gao, X.; Li, J.; and Wang, X. 2019a.\nDual-alignment Feature Embedding for Cross-modality Per-\nson Re-identiﬁcation. In ACM MM 2019, 57–65.\nHao, Y .; Wang, N.; Li, J.; and Gao, X. 2019b. HSME: Hy-\npersphere Manifold Embedding for Visible Thermal Person\nRe-Identiﬁcation. In AAAI 2019, 8385–8392.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang,\nW. 2021. TransReID: Transformer-based Object Re-\nIdentiﬁcation. CoRR, abs/2102.04378.\nKansal, K.; Subramanyam, A. V .; Wang, Z.; and Satoh, S.\n2020. SDL: Spectrum-Disentangled Representation Learn-\ning for Visible-Infrared Person Re-Identiﬁcation. IEEE\nTransactions on Circuits and Systems for Video Technology,\n30(10): 3422–3432.\nLi, D.; Wei, X.; Hong, X.; and Gong, Y . 2020. Infrared-\nVisible Cross-Modal Person Re-Identiﬁcation with an X\nModality. In AAAI 2020, 4610–4617.\nLing, Y .; Zhong, Z.; Luo, Z.; Rota, P.; Li, S.; and Sebe, N.\n2020. Class-Aware Modality Mix and Center-Guided Metric\nLearning for Visible-Thermal Person Re-Identiﬁcation. In\nACM MM 2020, 889–897.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In ICLR 2019.\nLu, Y .; Wu, Y .; Liu, B.; Zhang, T.; Li, B.; Chu, Q.; and\nYu, N. 2020. Cross-Modality Person Re-Identiﬁcation With\nShared-Speciﬁc Feature Transfer. In CVPR 2020, 13376–\n13386.\nLuo, H.; Gu, Y .; Liao, X.; Lai, S.; and Jiang, W. 2019.\nBag of Tricks and a Strong Baseline for Deep Person Re-\nIdentiﬁcation. In CVPR 2019, 1487–1495.\nNguyen, D. T.; Hong, H. G.; Kim, K.; and Park, K. R.\n2017. Person Recognition System Based on a Combination\nof Body Images from Visible Light and Thermal Cameras.\nSensors, 17(3): 605.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\nDesmaison, A.; K¨opf, A.; Yang, E.; DeVito, Z.; Raison, M.;\nTejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;\nand Chintala, S. 2019. PyTorch: An Imperative Style, High-\nPerformance Deep Learning Library. In NeurIPS 2019,\n8024–8035.\nSchroff, F.; Kalenichenko, D.; and Philbin, J. 2015. FaceNet:\nA uniﬁed embedding for face recognition and clustering. In\nCVPR 2015, 815–823.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-CAM: Visual Explana-\ntions from Deep Networks via Gradient-Based Localization.\nIn ICCV 2017, 618–626.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In NIPS 2017, 5998–6008.\nWang, G.; Yuan, Y .; Chen, X.; Li, J.; and Zhou, X. 2018.\nLearning Discriminative Features with Multiple Granulari-\nties for Person Re-Identiﬁcation. In ACM MM 2018, 274–\n282.\nWang, G.; Zhang, T.; Cheng, J.; Liu, S.; Yang, Y .; and\nHou, Z. 2019a. RGB-Infrared Cross-Modality Person Re-\nIdentiﬁcation via Joint Pixel and Feature Alignment. In\nICCV 2019, 3622–3631.\nWang, G.; Zhang, T.; Yang, Y .; Cheng, J.; Chang, J.; Liang,\nX.; and Hou, Z. 2020. Cross-Modality Paired-Images Gen-\neration for RGB-Infrared Person Re-Identiﬁcation. In AAAI\n2020, 12144–12151.\nWang, Z.; Wang, Z.; Zheng, Y .; Chuang, Y .; and Satoh, S.\n2019b. Learning to Reduce Dual-Level Discrepancy for\nInfrared-Visible Person Re-Identiﬁcation. In CVPR 2019,\n618–626.\nWen, Y .; Zhang, K.; Li, Z.; and Qiao, Y . 2016. A Discrimina-\ntive Feature Learning Approach for Deep Face Recognition.\nIn ECCV 2016, volume 9911, 499–515.\nWu, A.; Zheng, W.; Gong, S.; and Lai, J. 2020. RGB-IR Per-\nson Re-identiﬁcation by Cross-Modality Similarity Preser-\nvation. International Journal of Computer Vision, 128(6):\n1765–1785.\nWu, A.; Zheng, W.; Yu, H.; Gong, S.; and Lai, J. 2017. RGB-\nInfrared Cross-Modality Person Re-identiﬁcation. In ICCV\n2017, 5390–5399.\nYe, M.; Lan, X.; and Leng, Q. 2019. Modality-aware\nCollaborative Learning for Visible Thermal Person Re-\nIdentiﬁcation. In ACM MM 2019, 347–355.\nYe, M.; Shen, J.; Crandall, D. J.; Shao, L.; and Luo, J. 2020.\nDynamic Dual-Attentive Aggregation Learning for Visible-\nInfrared Person Re-identiﬁcation. In ECCV 2020, volume\n12362, 229–247.\nYe, M.; Shen, J.; Lin, G.; Xiang, T.; Shao, L.; and Hoi, S.\nC. H. 2021. Deep Learning for Person Re-identiﬁcation: A\nSurvey and Outlook. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 1–1.\nYe, M.; Shen, J.; and Shao, L. 2021. Visible-Infrared Person\nRe-Identiﬁcation via Homogeneous Augmented Tri-Modal\nLearning. IEEE Transactions on Information Forensics and\nSecurity, 16: 728–739.\nYe, M.; Wang, Z.; Lan, X.; and Yuen, P. C. 2018. Visi-\nble Thermal Person Re-Identiﬁcation via Dual-Constrained\nTop-Ranking. In IJCAI 2018, 1092–1099.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Tay, F. E. H.;\nFeng, J.; and Yan, S. 2021. Tokens-to-Token ViT: Train-\ning Vision Transformers from Scratch on ImageNet. CoRR,\nabs/2101.11986.\nZhang, Z.; Jiang, S.; Huang, C.; Li, Y .; and Xu, R. Y . D.\n2020. RGB-IR Cross-modality Person ReID based on\nTeacher-Student GAN Model. CoRR, abs/2007.07452.\nZhao, Y .; Lin, J.; Xuan, Q.; and Xi, X. 2019. HPILN: a\nfeature learning framework for cross-modality person re-\nidentiﬁcation. IET Image Process, 13(14): 2897–2904.\nZhao, Z.; Liu, B.; Chu, Q.; Lu, Y .; and Yu, N. 2021. Joint\nColor-irrelevant Consistency Learning and Identity-aware\nModality Adaptation for Visible-infrared Cross Modality\nPerson Re-identiﬁcation. In AAAI 2021, 3520–3528.\nZheng, L.; Yang, Y .; and Hauptmann, A. G. 2016. Per-\nson Re-identiﬁcation: Past, Present and Future. CoRR,\nabs/1610.02984.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In CVPR 2021, 6881–6890.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom Erasing Data Augmentation. InAAAI 2020, 13001–\n13008.\nZhu, Y .; Yang, Z.; Wang, L.; Zhao, S.; Hu, X.; and Tao,\nD. 2020. Hetero-Center loss for cross-modality person Re-\nidentiﬁcation. Neurocomputing, 386: 97–109.\nAppendix\nA. Comparison on Stride of Patch Generation\nWhen generating patch sequences, we do this in an overlap-\nping manner, as suggested in the previous research (Yuan\net al. 2021; He et al. 2021). Speciﬁcally, consistent with the\ninput of the base ViT model (Dosovitskiy et al. 2021), the\npatch size used in our method is 16 ×16 (P = 16), and we\nconduct comparative experiments with different strides on\nour cross-modality transformer (CMTR) network.\nAs shown in Table 4, a series of experiments are imple-\nmented with stride S set to {16,14,12,10,8}in turn. Note\nthat N represents the length of patch sequence. When S is\nset to 16 ( S = P), there is no overlap between patches.\nFrom Table 4, we can notice that this method (Index-1)\nachieves relatively low performance with 57.38% Rank-1\nand 56.56% mAP. As S decreases, the overlapping area be-\ntween adjacent patches will gradually increase. In the mean-\ntime, the performance of the model is gradually improved.\nCompared with the Index-1 model, other models (Index-\n2,3,4,5) gain +0.95%,+2.51%,+3.18%,+5.20% Rank-1 and\n+0.91%,+2.00%,+2.90%,+4.77% mAP, respectively. These\nexperimental results verify the effectiveness of the overlap-\nping patch strategy in this VI-ReID task.\nWith a smaller stride S, the model can generate a longer\npatch sequence with largerN(shown in the second and third\ncolumns of the Table 4). Through overlapping patches and\nlong sequences, the multi-head self-attention modules of the\ntransformer model can learn richer features. In practice, we\nuse the stride set to 8, so that half of the patches overlap\neach other, ensuring better performance, while controlling\nthe method’s consumption of computing resources.\nB. Study of the Parameter λin Objective Function\nIn the deﬁnition of overall objective function Loverall, the\nweight λis used as a hyperparameter to balance the propor-\ntion between losses. During the experiment, we evaluate the\neffect of this hyperparameter λ.\nAs illustrated in Figure 7, the hyperparameter λis set to\ndifferent values {0,1,2,3,4,5,6}. With λ set to zero, the\nnetwork is optimized without (w/o) the proposed modality-\naware enhancement loss. We plot its Rank-1 and mAP line\nas a benchmark. By introducing the modality-aware en-\nhancement loss with hyperparameter λ greater than zero,\nwe can observe a signiﬁcant performance improvement on\nboth Rank-1 and mAP, and it is relatively stable with differ-\nent λvalues. The experimental results demonstrate that our\nmethod is robust to this weight in objective function.\nC. Comparative Experiments of Loss Constraints\nThere exist some studies that focus on adjusting the dis-\ntribution of matching embeddings through loss constraints,\nsuch as the center loss (Wen et al. 2016) and Hetero-Center\n(HC) loss (Zhu et al. 2020). During the experiment, we com-\npare our proposed modality-aware enhancement (MAE) loss\nwith these loss constraints in the cross-modality transformer\n(CMTR) network to check their performance.\nIndex Stride N R1 R10 R20 mAP mINP\n1 S = 16 128 57.38 91.78 97.19 56.56 44.07\n2 S = 14 162 58.33 91.85 96.62 57.47 45.04\n3 S = 12 210 59.89 92.91 97.60 58.56 46.04\n4 S = 10 300 60.56 92.81 97.12 59.46 46.73\n5 S = 8 465 62.58 93.79 98.01 61.33 49.01\nTable 4: Experimental results with different strides of patch\ngeneration (SYSU-MM01 & all-search single-shot mode).\n0 1 2 3 4 5 6\nHyperparameter \n56\n57\n58\n59\n60\n61\n62\n63Rank-1 Score\nwith MAE\nw/o MAE\n0 1 2 3 4 5 6\nHyperparameter \n55\n56\n57\n58\n59\n60\n61\n62Mean Average Precision\nwith MAE\nw/o MAE\nFigure 7: Comparison of CMTRs’ performance with differ-\nent settings on hyperparameter λin objective function.\nIndex Methods R1 R10 R20 mAP mINP\n1 Baseline+ME 57.40 89.31 95.13 56.16 43.86\n2 Baseline+ME+Center 58.09 93.01 97.63 57.88 45.61\n3 Baseline+ME+HC 58.21 92.98 97.63 58.05 45.98\n4 Baseline+ME+MAE 62.58 93.79 98.01 61.33 49.01\nTable 5: Comparative study on loss constraints under the\ncross-modality transformer (CMTR) method.\nAs shown in Table 5, the ”Baseline+ME” denotes the\nbaseline ViT network with modality embeddings (ME) opti-\nmized by the common LID and LWRT losses. We add cen-\nter loss (”Center” for short), Hetero-Center loss (”HC”), and\nthe proposed modality-aware enhancement (”MAE”) loss to\nthis basic network (Index-2,3,4 of Table 5). From quanti-\ntative evaluation results, the center loss and HC loss bring\nlittle performance improvement. The former brings +0.69%\nRank-1 and +1.72% mAP, the latter brings +0.81% Rank-\n1 and +1.89% mAP. Compared with ”Baseline+ME”, the\nMAE loss shows a prominent effect (Index-4), and its Rank-\n1 and mAP results are greatly improved.\nThe traditional loss constraints directly act on extracted\nembeddings. However, they do not consider the effective\nmining and rational utilization of modalities’ characteristics\nand information, which limits their performance. In con-\ntrast, by using modality embeddings (ME), our designed\nmodality-aware enhancement (MAE) loss overcomes the\nshortcomings of existing losses and provides better guidance\nfor the adjustment of matching embeddings’ distribution.\nMethods Venue Visible to Thermal Thermal to Visible\nR1 R10 R20 mAP R1 R10 R20 mAP\nZero-Padding (Wu et al. 2017) ICCV 17 17.75 34.21 44.35 18.90 16.63 34.68 44.25 17.82\nSDL (Kansal et al. 2020) TCSVT 20 26.47 51.34 61.22 23.58 25.74 50.23 59.66 22.89\nBDTR (Ye et al. 2018) IJCAI 18 33.47 58.42 67.52 31.83 32.72 57.96 68.86 31.10\nMAC (Ye, Lan, and Leng 2019) MM 19 36.43 62.36 71.63 37.03 36.20 61.68 70.99 36.63\nHSME (Hao et al. 2019b) AAAI 19 41.34 65.21 75.13 38.82 40.67 65.35 75.27 37.50\nD2RL (Wang et al. 2019b) CVPR 19 43.40 66.10 76.30 44.10 - - - -\nMSR (Feng, Lai, and Xie 2020) TIP 19 48.43 70.32 79.95 48.67 - - - -\nJSIA (Wang et al. 2020) AAAI 20 48.50 - - 49.30 48.10 - - 48.90\nD-HSME (Hao et al. 2019b) AAAI 19 50.85 73.36 81.66 47.00 50.15 72.40 81.07 46.16\nAlignGAN (Wang et al. 2019a) ICCV 19 57.90 - - 53.60 56.30 - - 53.40\nFMSP (Wu et al. 2020) IJCV 20 65.07 - - 64.50 - - - -\nCMM+CML (Ling et al. 2020) MM 20 - - - - 59.81 80.39 88.69 60.86\nX-Modality (Li et al. 2020) AAAI 20 - - - - 62.21 83.13 91.72 60.18\ncm-SSFT (Lu et al. 2020) CVPR 20 65.40 - - 65.60 63.80 - - 64.20\nDDAG (Ye et al. 2020) ECCV 20 69.34 86.19 91.49 63.46 68.06 85.15 90.31 61.80\nDEF (Hao et al. 2019a) MM 19 70.13 86.32 91.96 69.14 67.99 85.56 91.41 66.70\nHi-CMD (Choi et al. 2020) CVPR 20 70.93 86.39 - 66.04 - - - -\nHAT (Ye, Shen, and Shao 2021) TIFS 21 71.83 87.16 92.16 67.56 70.02 86.45 91.61 66.30\nCMTR (Ours) - 80.62 92.93 96.21 74.42 81.06 93.36 96.52 73.75\nTable 6: Comparison with existing CNN-based cross-modality ReID methods on the RegDB dataset.\nStudy of MAE Loss R1 R10 R20 mAP mINP\nφm(·)\nIdentity Mapping 55.59 91.92 97.49 54.83 41.61\nFully Connection 62.58 93.79 98.01 61.33 49.01\nD(·,·)\nL1 57.65 92.50 97.43 57.10 44.48\nL2 59.40 93.66 97.88 59.09 46.69\nSmooth-L1 59.89 93.45 97.81 58.97 46.28\nCos 62.58 93.79 98.01 61.33 49.01\nTable 7: Comparison of multiple designs for MAE loss.\nD. More Experiments on the Design of MAE Loss\nDuring the experiment, we compare different implementa-\ntion schemes of MAE loss, and explore the inﬂuence of sev-\neral designs for the mapping function φm(·) and distance\nmeasurement method D(·,·) in Equation 4 and 5.\nThe φm(·) is the function to mine the information from\nmodality embeddings (ME). As shown in Table 7, we com-\npare two implementations, the identity mapping and fully\nconnection. With the identity mapping, the modality re-\nmoval operation uses the extracted features to directly sub-\ntract the ME of the corresponding modality, which is a hard\nway to eliminate the learned modalities’ characteristics. Dif-\nferent from this way, the introduction of fully connection\nlayer can further mine helpful information and knowledge\nin the ME, which can support modality removal in a soft\nway. And the experimental results in the Table 7 show the\npositive effect of fully connection.\nAs for D(·,·) in the deﬁnition of loss function (Equation\n4), we compare various distance calculation methods, in-\ncluding the L1, L2, Smooth-L1 and cosine distance. Our\ndesigned MAE loss acts on the features extracted by the\nmodel after the BN layer, and the features at this location\nare constrained by the common ID loss at the same time. As\ndiscussed in paper (Luo et al. 2019), the ID loss mainly op-\ntimizes the cosine distance. Thus, our MAE loss should also\nact as a constraint with distance of the same kind. The lower\npart of Table 7 shows the experimental results with different\ndistance metrics. Among all these settings, the cosine dis-\ntance has a signiﬁcant advantage, which is also consistent\nwith paper (Luo et al. 2019)’s analysis and conclusion.\nE. Complete experimental results on the RegDB\ndataset with CNN-based methods\nIn the text, due to space constraints, the results under\nall evaluation metrics are not fully displayed in Table 2.\nWe show the complete experimental results on the RegDB\ndataset with existing methods in Table 6, including the cal-\nculated Rank-1,10,20 and mAP. In addition to Rank-1 (R1)\nand mAP mentioned in the main text, we can observe that\nour CMTR method also has notable performance improve-\nment on Rank-10 (R10) and Rank-20 (R20) under these two\nevaluation modes, which veriﬁes robustness of the method.",
  "topic": "Modalities",
  "concepts": [
    {
      "name": "Modalities",
      "score": 0.77193284034729
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.7225369215011597
    },
    {
      "name": "Computer science",
      "score": 0.6826439499855042
    },
    {
      "name": "Discriminative model",
      "score": 0.6690808534622192
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6315257549285889
    },
    {
      "name": "Transformer",
      "score": 0.587378978729248
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5316757559776306
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4763343930244446
    },
    {
      "name": "Task (project management)",
      "score": 0.4211897850036621
    },
    {
      "name": "Machine learning",
      "score": 0.3722413182258606
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}