{
    "title": "UniTranSeR: A Unified Transformer Semantic Representation Framework for Multimodal Task-Oriented Dialog System",
    "url": "https://openalex.org/W4285119160",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100356305",
            "name": "Zhiyuan Ma",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5017694287",
            "name": "Jianjun Li",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100337866",
            "name": "Guohui Li",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5108814057",
            "name": "Yongjing Cheng",
            "affiliations": [
                "National University of Defense Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3087232873",
        "https://openalex.org/W2949141958",
        "https://openalex.org/W3117865619",
        "https://openalex.org/W2968124245",
        "https://openalex.org/W3173362020",
        "https://openalex.org/W2951450739",
        "https://openalex.org/W2970260827",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3035301094",
        "https://openalex.org/W3034441005",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3177289308",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3093418251",
        "https://openalex.org/W3015063797",
        "https://openalex.org/W2964077278",
        "https://openalex.org/W4295249402",
        "https://openalex.org/W4287642620",
        "https://openalex.org/W2914248611",
        "https://openalex.org/W3207886649",
        "https://openalex.org/W3173119568",
        "https://openalex.org/W2963748384",
        "https://openalex.org/W3174793239",
        "https://openalex.org/W3115336344",
        "https://openalex.org/W4287547182",
        "https://openalex.org/W3197770791",
        "https://openalex.org/W2962886331",
        "https://openalex.org/W3155285635",
        "https://openalex.org/W2956125353",
        "https://openalex.org/W2963789888",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2897182555",
        "https://openalex.org/W3106274079"
    ],
    "abstract": "As a more natural and intelligent interaction manner, multimodal task-oriented dialog system recently has received great attention and many remarkable progresses have been achieved. Nevertheless, almost all existing studies follow the pipeline to first learn intra-modal features separately and then conduct simple feature concatenation or attention-based feature fusion to generate responses, which hampers them from learning inter-modal interactions and conducting cross-modal feature alignment for generating more intention-aware responses. To address these issues, we propose UniTranSeR, a Unified Transformer Semantic Representation framework with feature alignment and intention reasoning for multimodal dialog systems. Specifically, we first embed the multimodal features into a unified Transformer semantic space to prompt inter-modal interactions, and then devise a feature alignment and intention reasoning (FAIR) layer to perform cross-modal entity alignment and fine-grained key-value reasoning, so as to effectively identify user's intention for generating more accurate responses. Experimental results verify the effectiveness of UniTranSeR, showing that it significantly outperforms state-of-the-art approaches on the representative MMD dataset.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 103 - 114\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nUniTranSeR: A Uniﬁed Transformer Semantic Representation\nFramework for Multimodal Task-Oriented Dialog Systems\nZhiyuan Ma1, Jianjun Li1∗, Guohui Li1, Yongjing Cheng2\n1 Huazhong University of Science and Technology (HUST), China\n2 National University of Defense Technology (NUDT), China\n{zhiyuanma,jianjunli,guohuili}@hust.edu.cn\ndavidcheng1001@163.com\nAbstract\nAs a more natural and intelligent interac-\ntion manner, multimodal task-oriented dia-\nlog system recently has received great atten-\ntion and many remarkable progresses have\nbeen achieved. Nevertheless, almost all ex-\nisting studies follow the pipeline to ﬁrst\nlearn intra-modal features separately and\nthen conduct simple feature concatenation or\nattention-based feature fusion to generate re-\nsponses, which hampers them from learning\ninter-modal interactions and conducting cross-\nmodal feature alignment for generating more\nintention-aware responses. To address these\nissues, we propose UniTranSeR, a Uni ﬁed\nTransformer Se mantic R epresentation frame-\nwork with feature alignment and intention rea-\nsoning for multimodal dialog systems. Specif-\nically, we ﬁrst embed the multimodal features\ninto a uniﬁed Transformer semantic space to\nprompt inter-modal interactions, and then de-\nvise a feature alignment and intention reason-\ning (FAIR) layer to perform cross-modal en-\ntity alignment and ﬁne-grained key-value rea-\nsoning, so as to effectively identify user’s in-\ntention for generating more accurate responses.\nExperimental results verify the effectiveness\nof UniTranSeR, showing that it signiﬁcantly\noutperforms state-of-the-art approaches on the\nrepresentative MMD dataset.\n1 Introduction\nThe multimodal task-oriented dialog systems are\ndesigned to help users achieve speciﬁc goals such\nas clothing recommendation or restaurant reserva-\ntion, which is in growing demand in the current\nbusiness environment. As a leading study, Saha\net al. (2018) released a multimodal dialog dataset\n(MMD) in the online retail domain. Based on such\na benchmark dataset, many multimodal dialog mod-\nels incorporating domain knowledge have recently\nbeen proposed (Chauhan et al., 2019; Zhang et al.,\n∗Corresponding author.\nAttributes Value\nNAME jacket\nMATERIAL leather\nCOLOR black\nIMAGE\n... ...\njacket\njeans\nsling bag necklace\nplatform heels\nPassion for jacket\ncelebritiesCelebrities HistogramStyle tips Graph Attributes Table\nMultimodal Knowledge Base\nShow some similar jackets in black color.\nHi\nHello, how can i help you?\nFound some similar black leather-jackets for you. \nI like the 2nd one, will it go well with jeans? Is it popular among celebrities?\nYes, it is a good match, and it's universally popular among celebrities.\nMultimodal Dialogue System\nFigure 1: Example of multimodal task-oriented dialog\nincluding multimodal entity alignment and knowledge\nquery from the MMD dataset (Saha et al., 2018). Note\nthat red marks the entities to be queried in the mul-\ntimodal knowledge base and blue marks the acquired\nknowledge information.\n2019, 2021), which basically exploit taxonomy-\nbased method (Liao et al., 2018; Cui et al., 2019)\nor attention-based method (Nie et al., 2019; He\net al., 2020) to incorporate knowledge base (KB)\ninformation for better performance.\nThough achieving remarkable progress, existing\nmultimodal task-oriented dialog systems still suf-\nfer from the following three limitations. Firstly,\nprior models only learn the intra-modal features\n(including textual features, visual features and do-\nmain knowledge) separately before fusing them.\nSince these multimodal cues in general can enhance\nand complement each other, projecting them into\na uniﬁed semantic space to learn the inter-modal\nfeatures, with no doubt, can help improve the abil-\nities of natural language understanding, which in\nturn will beneﬁt the response generation. Sec-\nondly, prior models only conduct simple feature\nconcatenation (Saha et al., 2018; Nie et al., 2019) or\nattention-based feature fusion (Cui et al., 2019) af-\n103\nter acquiring intra-modal representations, but with-\nout learning ﬁne-grained alignment between differ-\nent modalities before fusion, which is not favorable\nto query knowledge for accurate multimodal re-\nsponse generation. Take the dialog in Figure 1 as\nan example, when answering the user’s query on\nsimilar style of jackets, the model is expected to\nalign the word “jackets” with the corresponding vi-\nsual features for proper semantic complement and\nentity enhancement. Thirdly, prior models basi-\ncally lack the capability of entity-level reasoning,\nwhich prevents them from performing reasoning\nover crucial entities to guide intention-aware re-\nsponse generation. For example, in Figure 1, when\nthe user asks “show some similar jackets in black\ncolor”, the chatbot is expected to properly explore\nthe pivot attribute “black” that connects the start\nquery cue “jackets” with the target recommended\nproduct images. Speciﬁcally, the model needs to\nperform a 2-hop reasoning over triples (jacket_q,\nattribute, black_v) and (black_q, image, jacket_v)\nand obtain the intended 4 images.\nTo address the aforementioned limitations, we\npropose a Uniﬁed Transformer Semantic Repre-\nsentation framework with feature alignment and\nintention reasoning, UniTranSeR for short. Specif-\nically, to address the ﬁrst limitation, we stand on\nthe shoulder of Vision-and-Language Pre-training\n(VLP) methods (Lu et al., 2019; Li et al., 2019;\nChen et al., 2020; Li et al., 2021) to propose a\nuniﬁed-modal Transformer encoder, which is used\nto project all the multimodal features into a uniﬁed\nsemantic space to prompt inter-modality interac-\ntions, with the objective of learning better repre-\nsentations. Based on the uniﬁed encoder, we fur-\nther address the second limitation by designing a\nfeature alignment module to perform cross-modal\nfeature alignment. Finally, to address the third\nlimitation, we devise a ﬁne-grained intention rea-\nsoning module for capturing users’ real intentions,\nby leveraging a key-value attention based memory\nmechanism to perform multi-hop knowledge query\nfor generating text or image responses.\nWe conduct experiments on MMD, one of the\nmost inﬂuential benchmark datasets for multimodal\ndialog generation. We follow the mainstream eval-\nuation script of dialog generation and demonstrate\nthat UniTranSeR signiﬁcantly outperforms the cur-\nrent state-of-the-art baselines. Ablation study also\nshows the efﬁcacy of each component in improving\nthe performance of dialog generation, and a further\ncase study reveals that our model can effectively\nperform ﬁne-grained token-level feature alignment\nfor multimodal dialog generation.\n2 Related Work\n2.1 Unimodal Dialog Systems\nRecent years has witnessed the remarkable success\nin textual dialog systems, which can be roughly\ndivided into two categories: open-domain conver-\nsations with casual chi-chat (Song et al., 2020;\nGangal et al., 2021; Chan et al., 2021; Yang et al.,\n2021) and task-oriented dialog systems (Pei et al.,\n2021; Santra et al., 2021; Wang et al., 2021; Mi\net al., 2021; Madotto et al., 2021; Gou et al., 2021;\nRaghu et al., 2021), which are designed to help\nusers achieve speciﬁc goals. Early efforts mainly\nadopt a sequence-to-sequence (Seq2Seq) architec-\nture, but cannot work well in KB retrieval and rea-\nsoning. To alleviate this problem, copy mecha-\nnism (Eric and Manning, 2017) have been adopted\nand many memory augmented Seq2Seq models\nhave been proposed (Bordes et al., 2017; Wen et al.,\n2018; Madotto et al., 2018; Wu et al., 2019; Reddy\net al., 2019; Qin et al., 2019; Wang et al., 2020;\nQin et al., 2020), which achieve promising results.\n2.2 Multimodal Dialog Systems\nWith the ﬂourishing of social media platforms,\nmassive amounts of multimedia data are gener-\nated daily, which poses great demand for mul-\ntimodal dialog systems. However, due to the\nlack of large-scale multimodal dialog datasets, re-\nsearches in this domain have been limited. To\nthis end, Saha et al. (2018) provided a vertical re-\ntail domain dataset MMD to promote the research\nand proposed a multimodal hierarchical encoder-\ndecoder model (MHRED) as a baseline. Based\non MHRED, Liao et al. (2018) incorporated the\nstyle tips into a knowledge-aware multimodal di-\nalog model (KMD). Cui et al. (2019) designed\na user attention-guided multimodal dialog system\n(UMD) by additionally considering the hierarchi-\ncal product taxonomy and user’s attention to prod-\nucts. Chauhan et al. (2019) introduced an ordi-\nnal and attribute aware multimodal dialog system\n(OAM) by employing a novel position and attribute\naware attention mechanism. Later, Nie et al. (2019)\nproposed a multimodal dialog system with adaptive\ndecoders (MAGIC), which can incorporate differ-\nent forms of domain knowledge to generate differ-\nent kinds of responses. Recently, combining with\n104\nshow some similarT-shirts but in a lighter color T-shirt_231color nattier_blue\nnattier_blue_styleimg img_url\nUnified-modal Transformer Semantic Encoder\n[CLS]\n0/1\nK VT VI\nT-shirt_232colordark-grey\ndark_grey_styleimg img_url\nQ\nAlign Reason\n \n \ngeneral response intention-aware response image output\nHierarchical Transformer Response Decoder \nHTR Decoder\nUTS Encoder\nshowFAIR Layer\nText Image\nKnowledge\nFigure 2: The Proposed Framework.\nTransformer (Vaswani et al., 2017), He et al. (2020)\nadvanced a multimodal dialog system via capturing\ncontext-aware dependencies of semantic elements\n(MATE) for textual response generation.\nMost existing multimodal dialog systems learn\nintra-modal features separately for later feature con-\ncatenation or fusion. Different from them, our pro-\nposed UniTranSeR can project all the multimodal\nfeatures into a uniﬁed semantic space to perform\nﬁne-grained feature alignment and intention rea-\nsoning, which can lead to more accurate responses.\nVision-and-Language Pre-training (VLP) (Lu et al.,\n2019; Li et al., 2021) is another line of research\nrelevant to our work, but different from ours in that\nit focuses more on boosting the performance of\nrepresentation learning, while the multimodal dia-\nlog systems focus more on multi-turn multimodal\ninteraction between users and agents.\n3 Methodology\nThe proposed UniTranSeR mainly comprises three\nparts: Uniﬁed-modal Transformer Semantic (UTS)\nencoder (Sec. 3.1), Feature Alignment and Inten-\ntion Reasoning (FAIR) layer (Sec. 3.2), and Hi-\nerarchical Transformer Response (HTR) decoder\n(Sec. 3.3), as shown in Figure 2. We deﬁne\nthe multimodal dialog generation task as gener-\nating the most likely response sequence Y =\n{y1,y2,··· ,yn}and selecting top-k most matched\nimages, giving multimodal context utterances U =\n{u1,u2,...,u |U|}and multimodal knowledge base\nBas inputs. The probability of a textual response\ncan be formally deﬁned as,\nP(Y|U,B) =\nn∏\nt=1\nP(yt|y1,...,y t−1,U,B ) (1)\nwhere yt represents the current token decoded by\nthe HTR decoder.\nThe UTS encoder is used to project all the mul-\ntimodal features into a uniﬁed vector space for\ninter-modal interactions, while the FAIR layer is\ndesigned to align cross-modal hidden features, with\ntextual features and visual features from previous\nUTS encoder as inputs. Similar to MAGIC (Nie\net al., 2019), our HTR decoder is designed to de-\ncode three types of responses: general responses\nthat refer to the highly frequent responses (e.g.,\ncourtesy greetings) in the conversation, such as\n“How can I help you?”; intention-aware responses\nthat refer to the task-oriented utterances, such as\n“Found some similar black leather-jackets for you”;\nand multimodal responses that refer to the intention-\naware responses with image output. The response\ntype is determined by a query vector Q from the\nFAIR layer, in which an intention classiﬁer is\ntrained to decide which kind of response should be\ngiven out.\n3.1 UTS Encoder\nWe ﬁrst use a text embedder and an image embed-\nder to extract textual features and visual features,\nrespectively, and extract informative features from\nexternal knowledge by utilizing both text and image\nembedders. Afterwards, we feed these three kinds\nof features into a uniﬁed Transformer encoder for\nuniﬁed-modal semantic representation learning.\nText Embedder. To learn textual intra-modal\nfeatures, we use a BERT tokenizer to split the in-\nput sentence into words and exploit a single trans-\nformer layer to obtain these words’ initial embed-\ndings. Note the self-attention mechanism in Trans-\nformer is order-less. So, it is necessary to encode\nthe words’ position as additional inputs. The ﬁnal\n105\nrepresentation for each word is derived via sum-\nming up its word embedding and position embed-\nding, followed by a layer normalization (LN) layer.\nImage Embedder. To learn visual intra-modal\nfeatures, we use a contour slicer to cut the input\nimages into patches and exploit ResNet-50 (He\net al., 2016) to extract these patches’ visual fea-\ntures. We notice that people usually focus on\nfour parts of a clothing image: head, upper body,\nlower body, and feet, so we intuitively use an\nequal-height mode to slice an image into four\npatches, which efﬁciently solves the problem\nof region feature extraction, without using com-\nplex target detection networks such as Faster R-\nCNN (Ren et al., 2015). Then, we feed the patches\ninto ResNet-50 to get the patches’ initial embed-\ndings. Similarly, we also encode the position\nfeatures for each patch via a 4-dimensional vec-\ntor [image_index,patch_index,width,height ].\nBoth visual and position features are then fed\nthrough a fully-connected (FC) layer, to be pro-\njected into the same embedding space. The ﬁnal\nvisual embedding for each patch is obtained by ﬁrst\nsumming up the two FC outputs, and then passing\nthem through an LN layer.\nKnowledge Embedder. To integrate informa-\ntive features from external knowledge 1 into the\ntask-oriented dialog, we equip the product knowl-\nedge base for each utterance through searching a\nfashion item table provided by MMD. We then\ntreat these searched knowledge entries into the\nsame triplet format, i.e., (product, match, product),\n(product, attribute, value), (product, celebrity, pas-\nsion_score). Next, for the text and image elements\nof these triples, we use the text and image embed-\nders to obtain their respective representations.\nUniﬁed Transformer Encoder. After obtaining\nthe multimodal initial embeddings, denoted as ht,\nhv and hk respectively, we project them into a\nuniﬁed semantic space to obtain interactive repre-\nsentations by using a uniﬁed Transformer encoder.\nSpeciﬁcally, in each utterance, the textual features,\nvisual features and informative features correspond\nto ltokens with “[TXT]”, 4 tokens2 with “[IMG]”\nand 4 tokens3 with “[KNG]”. In order to integrate\n1External knowledge of MMD includes: style tips graph,\nattributes table and celebrities histogram, as shown in Figure 1.\n2Note when an utterance contains multiple images, it can\nbe unrolled into a sequence of utterances, each containing a\nsingle image, the same as previous work.\n3Including 3 textual features and 1 visual features.\nUnified-modal MLM\nshow some similar [MASK]... \nT -shirts \nUnified-modal MPM\nshow some similar ... T -shirts \nFigure 3: Illustration of MLM and MPM.\ndialog history of previous rounds, we initialize\nthe current [CLS]p by using the representation of\nthe previous round [CLS] p−1. The output hidden\nstate representations can then be phrased as:\nHp = f\n(\n[CLS]p−1hp\nt[TXT]hp\nv[IMG]hp\nk[KNG]\n)\n(2)\nwhere f(·) denotes the Transformer encoder, Hp\n0\ndenotes the hidden state representation of the cur-\nrent round [CLS]p, which is regarded as the con-\ntextual semantic vector of the entire utterance in\nthis round, Hp\n1:l denotes the representations for the\ntext sequence, Hp\nl+1:l+4 denotes the representations\nfor the patch sequence, and Hp\nl+5:l+8 denotes the\nrepresentations for knowledge entries. Note the su-\nperscript pis omitted for simplicity if no confusion\noccurs in the following discussion.\nTo obtain better representations, we introduce\nthe Masked Language Modeling (MLM) loss and\nMasked Patch Modeling (MPM) loss to train them.\nWe denote the input words as w = {w1,...,w l},\nthe image patches as v= {v1,...,v 4}, the knowl-\nedge elements as k= {k1,...,k 4}, and the mask\nindices as m∈NL, where N is the natural numbers\nand Lis the length of masked tokens. In MLM, we\nrandomly mask out the input words with a probabil-\nity of 15%, and replace the masked oneswm with a\nspecial token “[MASK]”, as illustrated in Figure 3.\nThe goal is to predict these masked words by atten-\ntively integrating the information of their surround-\ning words w\\m, image patches v and knowledge\nelements k, by minimizing the following loss:\nLMLM(θ) =−E(w,v,k)∼U log Pθ\n(\nwm|w\\m,v,k\n)\n(3)\nSimilar to MLM, in MPM, we also randomly mask\nout the image patches and use zeros tensor to re-\nplace them, as shown in Figure 3. Unlike textual\nwords that can be categorized as discrete labels,\nvisual features are high-dimensional and continu-\nous tensors, thus cannot be supervised via a nega-\ntive log-likelihood loss. Following UNITER (Chen\net al., 2020), we built the MPM loss as:\nLMPM(θ) =E(w,v,k)∼Ugθ\n(\nvm|v\\m,w,k\n)\n(4)\nwhere vm are masked image patches and v\\m are\nremaining patches. Note here gθ is deﬁned as an\n106\nL2 regression function, where\ngθ\n(\nvm|v\\m,w,k\n)\n=\nL∑\ni=1\nfθ\n(\nv(i)\nm\n)\n−hv(i)\nm\n\n2\n2\n(5)\n3.2 The FAIR Layer\nTo align the cross-modal features for accurate in-\ntention classiﬁcation and knowledge query, we de-\nvise a feature alignment and intention reasoning\n(FAIR) layer. In feature alignment, we use Image-\nText Matching (ITM) and Word-Patch Alignment4\n(WPA) to conduct a two-level alignment. That is,\nITM is used to align text and image in sentence-\nlevel, while WPA is used to align each split word\nand each sliced patch in token-level. In intention\nreasoning, we fuse f([CLS]) and aligned entities’\nhidden state representations to obtain a query vec-\ntor Q, which is then used for intention classiﬁcation\nand knowledge query.\n3.2.1 Feature Alignment\nImage-Text Matching (ITM). In ITM, we use\nthe output f([CLS]) of the uniﬁed Transformer\nencoder to compute the match probability of the\nsampled pair. Speciﬁcally, we feed f([CLS]) into\nan FC layer and a sigmoid function to predict a\nprobability score Pθ(w,v), which is between 0 and\n1. During training, we sample a positive or negative\npair (w,v) from the dataset D at each step. The\nnegative pair is created by randomly replacing the\nimage or text in the same batch. We employ a\nbinary cross-entropy loss for optimization:\nLITM(θ) =−E(w,v)∼D[ylog Pθ(w,v)+\n(1 −y) log (1−Pθ(w,v))]\n(6)\nwhere yis a binary truth label. Note here we only\nuse ITM to train image-text pairs but without con-\nsidering the knowledge vector, because it has al-\nready matched the textual sequence when being\nsearched out.\nWord-Patch Alignment (WPA). For more ﬁne-\ngrained alignment between each word and image\npatch, we introduce a WPA technology, which is\nused to train the consistency and exclusiveness be-\ntween these cross-modal features to prompt align-\nment. We use a WPA loss to supervise the process,\n4A modiﬁed version of the previous Word-Region Align-\nment (WRA), which can be adapted to the alignment between\ntextual words and visual patches.\nwhich is deﬁned as:\nLWPA(θ) =−\n∑l\ni=1\n∑4\nj=1\nTij·φ(wi,vj) (7)\nwhere φ denotes the cos(·) similarity function,\nT ∈Rl×4 is a ground truth table and eachTij ∈T\nis a binary label 0 or 1. During training, we sample\npositive or negative pairs (wi,vj) from each multi-\nmodal utterance to construct a probability table, as\nshown in Figure 2. The above loss function LWPA\nis then used to update the parameters θ. During\ninference, we continue to fuse aligned entities’ hid-\nden state representation and f([CLS]) to obtain a\nuniﬁed query vector Q, which contains multimodal\nquery information with entity enhancement, and\nwill be used for subsequent intention reasoning.\n3.2.2 Intention Reasoning\nIntention Classify (IC). Given the query vector\nQ, this component aims to understand the users’\nintention and thereafter determine which type of\nresponse should be generated. To be clear, there\nare a total of 17 types labeled in the MMD dataset,\nand each user’s utterance is labeled with a speciﬁc\nintention type. Following MAGIC, we customize\nthe type of response speciﬁcally for each intention,\nas shown in Table 1. Subsequently, we leverage an\nMLP layer to predict Q’s probability distribution\nand select the highest probability to generate a re-\nsponse. Besides, a cross-entropy loss is applied to\noptimizing the intention classiﬁer:\nLIC(θ) =\n∑|U|\ni=1\n∑17\nj=1\nI∗\nij log Pθ(Iij |Q) (8)\nwhere Pθ(Iij |Q) denotes the probability of being\npredicted as intention Iij, and I∗\nij is a ground truth\nlabel. The intention classiﬁer is trained by the loss\nfunction LIC(θ) to update parameter θ, and ﬁnally\noutputs a reliable intention prediction result I in\nthe inference phase.\nKnowledge Query (KQ). Given the predicted\nintention result I, this component ﬁrst determines\nwhether knowledge query is required based on Ta-\nble 1. If required, we adopt a key-value mem-\nory mechanism to query all embedded knowledge\ntriples5. Speciﬁcally, these embedded knowledge\ntriples are divided into key parts and value parts,\nwhich are respectively denoted as vector K and\nvector V. Note here K is obtained through a linear\n5The triple is in the form of (head, relation, tail)\n107\nId Intention categories Response type Component Id Intention categories Response type Component\n1 greeting general IC 10 ask-attribute intention-aware IC+KQ\n2 self-info general IC 11 suited-for intention-aware IC+KQ\n3 give-criteria multimodal IC+KQ+MR 12 celebrity intention-aware IC+KQ\n4 show-image multimodal IC+KQ+MR 13 ﬁlter-results multimodal IC+KQ+MR\n5 give-description multimodal IC+KQ+MR 14 sort-results multimodal IC+KQ+MR\n6 show-more multimodal IC+KQ+MR 15 switch-synset general IC\n7 show-orientation multimodal IC+KQ+MR 16 buy general IC\n8 show-similar multimodal IC+KQ+MR 17 exit general IC\n9 goes-with intention-aware IC+KQ\nTable 1: The categories of user’s intentions, their corresponding response types and required components.\nfusion of the embedded head-entities and relations.\nThe knowledge query process is as follows:\nαi = Softmax\n(\nQT ·Ki\n)\n(9)\nVT =\n∑|M|\ni=1\nαiVi (10)\nwhere αi denotes the attentive probability score for\nKi, |M|is the number of knowledge triples, and\nVT is a weighted sum ofVi, which will be used for\ntextual decoding in an intention-aware response.\nMulti-hop Recommend (MR). Given the pre-\ndicted intention result I and one-hop query re-\nsult VT, this component ﬁrst needs to determine\nwhether an image recommendation is required\nbased on Table 1. If required, we continue to use\nVT as a query vector to perform another hop query\nover the entire knowledge base, which implies that\nthe product images will be recommended, if the\nkey parts of their corresponding triples have high\nsimilarity to VT. Speciﬁcally,\nβi = Softmax\n(\nVT\nT ·Ki\n)\n(11)\nAfter deriving βi, we use VI = {qi}, an image\npointer vector, to select images with top βi for\nrecommendation, where\nqi =\n{ 1, if Vi = 11×512\n0, otherwise (12)\nand 11×512 is a column vector with each element\nequal to 1, which denotes for the special token\n[URL] of the image’s link. Note here 512 is the\nembedding size in our uniﬁed Transformer encoder.\nIt is not difﬁcult to see that UniTranSeR can extend\nthe above one-hop knowledge query to multi-hop\nby iteratively performing attention-based key-value\nreasoning and ultimately achieve multi-hop image\nrecommendation.\n3.3 HTR Decoder\nAs mentioned earlier, we used a hierarchy mech-\nanism to decode different types of response se-\nquences, including general responses, intention-\naware responses and multimodal responses. They\nDataset Statistics Train Valid Test\nDialogs 105,439 22,595 22,595\nProportion 70% 15% 15%\nTable 2: Statistics of the MMD dataset.\nshare the same uni-directional Transformer layer,\nbut the semantic representations fed to this de-\ncoder are different. Speciﬁcally, for general re-\nsponses, we just take the sentence-level represen-\ntations f([CLS]) as input. For intention-aware re-\nsponses, we take the concatenation of f([CLS])\nand attentive vector VT followed by an FC layer\nas input. For multimodal responses, we take the\ninput for the intention-aware responses, as well as\nVI, the image pointer vector, as input.\n4 Experimental Setup\n4.1 Datasets and Metrics\nTo evaluate the performance of UniTranSeR, we\nconduct experiments on the widely-used bench-\nmark dataset MMD contributed by Saha et al.\n(2018). The MMD dataset consists of over 150k\nconversations between users and chatbots in the\nretail domain, and each conversation describes a\ncomplete online shopping process. During the con-\nversations, the user proposes his/her requirements\nin multimodal utterances and the chatbot introduces\ndifferent products step by step until they make\na deal. In our experiments, we follow Nie et al.\n(2019) to partition MMD. The statistics the dataset\nafter partition are presented in Table 2, and more\ndetailed statistics can be found in Appendix A.4.\nFollowing several previous work (Nie et al.,\n2019; He et al., 2020; Zhang et al., 2021), we\nuse Bleu-n, Nist and Recall@ k to evaluate our\nmodel over two basic tasks separately, i.e., text\ntask and image task. For the text task, we employ\nthe proposed HTR decoder to produce all general\nresponses and intention-aware responses. As the\nlength of 20.07% target responses in MMD is less\nthan 4, such as “Hello!” and “Thanks a lot!”, we\nfollow Nie et al. (2019) to calculate Bleu- n by\n108\nMethods Text Task Image Task\nBleu-1 Bleu-2 Bleu-3 Bleu-4 Nist Recall@1 Recall@2 Recall@3\nPrevious\nMethods\nMHRED (Saha et al., 2018) 32.60 25.14 23.21 20.52 3.0901 0.7980 0.8859 0.9345\nKMD (Liao et al., 2018) - - - - - 0.9198 0.9552 0.9755\nUMD (Cui et al., 2019) 42.78 33.69 28.06 23.73 - 0.9796 0.9980 0.9990\nOAM (Chauhan et al., 2019) 48.30 38.24 32.03 27.42 4.3236 - - -\nMAGIC (Nie et al., 2019) 50.71 39.57 33.15 28.57 4.2135 0.9813 0.9927 0.9965\nMATE (He et al., 2020) 56.55 47.89 42.48 38.06 - - - -\nOurs UniTranSeR 63.27 55.93 51.31 48.07 4.9774 0.9983 0.9995 0.9998\nTable 3: Main results. Relevance (higher better) between generated responses and golden responses. Note all our\nresults are statistically signiﬁcant with p< 0.05 under t-test.\nvarying n from 1 to 4. Note higher Bleu and Nist\nscores indicate that more n-gram overlaps exist\nbetween the predicted and target responses, and\nhence are more favorable. For the image task, we\nadopt Recall@k to evaluate the efﬁcacy of image\nresponse, where k is varied from 1 to 3. Note the\nimage response is correct only if the positive image\nis recommended in the top-k product images.\n4.2 Baselines\nWe compare our model with the following state-of-\nthe-art baselines.\n• MHRED (Saha et al., 2018)6 is the ﬁrst base-\nline work to integrate the visual features into a\nhierarchical encoder-decoder model for their\nconstructed MMD dataset.\n• KMD (Liao et al., 2018) incorporates the style\ntips into the memory augmented neural model\nand adopts deep reinforcement learning to\nboost the performance.\n• UMD (Cui et al., 2019) 7 proposes a user\nattention-guided multimodal dialog system by\nconsiderring the hierarchical product taxon-\nomy and the user’s attention to products.\n• OAM (Chauhan et al., 2019) proposes a novel\nordinal and attribute aware attention mecha-\nnism for multimodal dialog generation.\n• MAGIC (Nie et al., 2019)8 adopts the adap-\ntive decoders with intention understanding to\nexplicitly generate three types of responses.\n• MATE (He et al., 2020) 9 utilizes a multi-\nmodal element-level encoder to integrate dia-\nlog context and leverages a knowledge-aware\ntwo-stage decoder for response generation,\nand achieves state-of-the-art performance.\n6https://github.com/amritasaha1812/MMD_Code\n7https://github.com/ChenTsuei/UMD\n8https://acmmultimedia.wixsite.com/magic.\n9https://github.com/githwd2016/MATE/tree/dev\n4.3 Implementation Details\nFollowing Saha et al. (2018) and Nie et al. (2019),\nwe utilize two-turn utterances prior to the target\nresponse as the context, and set the vocabulary size\nto 26,422. In our trainings, the batch size is set to\n64, learning rate is set to 1e−4 and the max number\nof training epoches is set to 1e4. Adam optimizer\nis used to optimize all models. All experiments are\nconducted with PyTorch. More details about hyper-\nparameter settings can be found in Appendix A.1.\n5 Evaluation Results\n5.1 Response Quality Evaluation\nAutomatic Evaluation Following KMD, UMD\nand MAGIC, we evaluate model performance auto-\nmatically from two aspects: text response and im-\nage response. From the results in Table 3, we can\nobserve that our model UniTranSeR achieves the\nstate-of-the-art performance on both tasks. Speciﬁ-\ncally, in text task, UniTranSeR exhibits the highest\nBleu-n with varying n from 1 to 4 compared with\nother baselines, indicating that our model can gen-\nerate responses closer to the golden ones. More-\nover, our model outperforms MATE, a recent model\nthat can capture context-aware dependencies of se-\nmantic elements, by 26.3% in Bleu-4 score, which\nveriﬁes the effectiveness of our model in learning\ncross-modal feature alignment and conduct inten-\ntion reasoning to generate more accurate and infor-\nmative responses. In image task, an extremely dif-\nﬁcult performance improvement can be observed,\nwhich further veriﬁes the superiority of our model.\nHuman Evaluation The human evaluation\nmainly focuses on four aspects: ﬂuency, relevance,\ncorrectness, and informativeness, which are all im-\nportant for task-oriented dialogue systems (Cui\net al., 2019; Nie et al., 2019; He et al., 2020). We\nﬁrst randomly selected 200 dialogs from the MMD\ndatasets, and used different models to generate re-\nsponses, including UMD, OAM, MAGIC, MATE\n109\nModel Flue. Rele. Corr. Info. Overall\nAverage\nAchieve\nRatio\nUMD 2.25 2.84 3.20 2.20 2.62 54.1%\nOAM 2.45 2.90 3.38 3.10 2.96 61.2%\nMAGIC 2.20 3.15 3.45 3.88 3.17 65.5%\nMATE 3.24 3.08 3.56 4.12 3.50 72.3%\nUniTranSeR 3.65 4.00 3.92 4.22 3.95 81.6%\nGolden 4.95 4.82 4.85 4.75 4.84 100%\nTable 4: Human evaluation of responses on ﬂuency\n(Flue.), relevance (Rele.), correctness (Corr.), informa-\ntiveness (Info.) on randomly selected dialogs.\nMethods Bleu-4 Nist\nTest ∆ Test ∆\nUniTranSeR Complete 48.07 - 4.9774 -\n-UTS Encoder -Trans. 42.07 12.48% 4.2620 14.37%\n-HTR Decoder -Trans. 45.35 5.66% 4.6291 7.00%\n-FA Module -ITM 40.20 16.37% 3.9580 20.48%\n-WPA 38.82 19.24% 3.5567 28.54%\n-IR Module -IC+KQ 21.65 54.96% 2.2804 54.18%\nTable 5: Ablation study on MMD dataset.\nand UniTranSeR. Then, we hired human experts to\nscore the responses and golden responses in blind\nreview on a scale from 1 to 5, which simulated\na real-life multimodal task-oriented conversation\nscenario. By calculating the average score of the\nabove metrics, we obtained the ﬁnal manual evalua-\ntion results, as shown in Table 4. It can be observed\nthat UniTranSeR consistently outperforms the other\nfour models on all metrics, which is in line with\nthe results of automatic evaluation.\n5.2 Ablation Study\nIn this part, we perform ablation experiments to\nevaluate the effectiveness of each component. We\nfocus on ﬁve crucial components and set them ac-\ncordingly: 1) w/o UTS Encoder denotes that we use\na BiGRU to replace the uniﬁed-modal Transformer\nencoder for multimodal encoding; 2) w/o HTR De-\ncoder denotes that we use a Uni-directional GRU\nto replace the hierarchical Transformer decoder for\nresponse generation; 3) w/o ITM denotes that we\nremove the LITM loss to make the parameters not\nupdated; 4) w/o WPA denotes that we remove the\nLWPA loss and just regard the sentence-level rep-\nresentation f([CLS]) as query vector Q to query\nknowledge; 5) w/o IR Module denotes that we re-\nmove the IC and KQ components and just adopt the\ncontext vector f([CLS]) to generate responses10;\nFrom Table 5, we can observe that removing each\ncomponent will result in a performance degrada-\ntion. Speciﬁcally, w/o IR Module causes 54.96%\ndrops in Bleu-4 score and 54.18% drops in Nist\n10Equivalent to generating general responses, since there is\nno knowledge query.\n \n(2) I like similar outfits: sunglasses, a short-sleeved \nT-shirt, long jeans and chunky sandals.\n(1) Show more suggestions with T-shirt, short jeans,\nbackpack and flat shoes.\nT-shirt short jeans\nbackpack flat shoes\nsunglassesshort-sleeved T-shirt\nlong jeanschunky sandals\nFigure 4: Visualization of Feature Alignment.\nscore, which veriﬁes the great efﬁcacy of intention\nclassify and knowledge query components. More-\nover, w/o WPA, w/o ITM and w/o UTS Encoder\nrespectively cause 28.54%, 20.48% and 14.37%\ndrops in Nist score, which further demonstrates the\neffectiveness of cross-modal feature alignment and\nuniﬁed-modal semantic encoding.\n5.3 Case Study and Visualization\nTo better illustrate the advantage of our model and\nunderstand what the feature alignment module has\nlearned, we visualize several examples of text-to-\nimage attention, as shown in Figure 4. It can be ob-\nserved that our model is able to capture ﬁne-grained\nentity alignment between different modalities. The\nreason may be that: 1) We adopt a uniﬁed-modal\nTransformer semantic encoder, which enables to\nmap different modalities of semantic cues into a\nsame vector space to prompt inter-modality inter-\nactions for better representations; 2) Based on the\nobtained representations, the WPA technology can\nhelp supervise ﬁne-grained word-patch alignment,\nwhich is beneﬁcial to identifying user’s real inten-\ntion and generate more intention-aware responses.\n6 Conclusion\nIn this paper, we propose a Uniﬁed Transformer\nSemantic Representation framework with feature\nalignment and intention reasoning, referred to Uni-\nTranSeR. Speciﬁcally, we project the multimodal\nfeatures into a uniﬁed semantic space by utilizing\na Transformer encoder to prompt inter-modal inter-\nactions. We further design a feature alignment and\nintention reasoning layer to conduct cross-modal\nfeature alignment and ﬁne-grained intention rea-\n110\nsoning, with the objective of generating more accu-\nrate and intention-aware responses. Experiments\non the representative MMD dataset demonstrate\nthe effectiveness and superior performance of our\nUniTranSeR model in both automatic and human\nevaluation.\nReferences\nAntoine Bordes, Y-Lan Boureau, and Jason Weston.\n2017. Learning end-to-end goal-oriented dialog.\nIn 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings . OpenRe-\nview.net.\nZhangming Chan, Lemao Liu, Juntao Li, Haisong\nZhang, Dongyan Zhao, Shuming Shi, and Rui Yan.\n2021. Enhancing the open-domain dialogue eval-\nuation in latent space. In Findings of the Associ-\nation for Computational Linguistics: ACL/IJCNLP\n2021, Online Event, August 1-6, 2021 , volume\nACL/IJCNLP 2021 ofFindings of ACL, pages 4889–\n4900. Association for Computational Linguistics.\nHardik Chauhan, Mauajama Firdaus, Asif Ekbal, and\nPushpak Bhattacharyya. 2019. Ordinal and attribute\naware response generation in a multimodal dialogue\nsystem. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 5437–5447. Association\nfor Computational Linguistics.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: universal image-text\nrepresentation learning. In Computer Vision - ECCV\n2020 - 16th European Conference, Glasgow, UK, Au-\ngust 23-28, 2020, Proceedings, Part XXX , volume\n12375 of Lecture Notes in Computer Science, pages\n104–120. Springer.\nChen Cui, Wenjie Wang, Xuemeng Song, Minlie\nHuang, Xin-Shun Xu, and Liqiang Nie. 2019. User\nattention-guided multimodal dialog systems. In Pro-\nceedings of the 42nd International ACM SIGIR Con-\nference on Research and Development in Informa-\ntion Retrieval, SIGIR 2019, Paris, France, July 21-\n25, 2019, pages 445–454. ACM.\nMihail Eric and Christopher D. Manning. 2017. A\ncopy-augmented sequence-to-sequence architecture\ngives good performance on task-oriented dialogue.\nIn Proceedings of the 15th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, EACL 2017, Valencia, Spain, April 3-7,\n2017, Volume 2: Short Papers , pages 468–473. As-\nsociation for Computational Linguistics.\nVarun Gangal, Harsh Jhamtani, Eduard H. Hovy, and\nTaylor Berg-Kirkpatrick. 2021. Improving auto-\nmated evaluation of open domain dialog via diverse\nreference augmentation. In Findings of the Associ-\nation for Computational Linguistics: ACL/IJCNLP\n2021, Online Event, August 1-6, 2021 , volume\nACL/IJCNLP 2021 ofFindings of ACL, pages 4079–\n4090. Association for Computational Linguistics.\nYanjie Gou, Yinjie Lei, Lingqiao Liu, Yong Dai,\nand Chunxu Shen. 2021. Contextualize knowledge\nbases with transformer for end-to-end task-oriented\ndialogue systems. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2021, Virtual Event / Punta\nCana, Dominican Republic, 7-11 November, 2021 ,\npages 4300–4310. Association for Computational\nLinguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2016, Las Ve-\ngas, NV , USA, June 27-30, 2016 , pages 770–778.\nIEEE Computer Society.\nWeidong He, Zhi Li, Dongcai Lu, Enhong Chen, Tong\nXu, Baoxing Huai, and Jing Yuan. 2020. Multi-\nmodal dialogue systems via capturing context-aware\ndependencies of semantic elements. In MM ’20:\nThe 28th ACM International Conference on Multi-\nmedia, Virtual Event / Seattle, WA, USA, October\n12-16, 2020, pages 2755–2764. ACM.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A\nsimple and performant baseline for vision and lan-\nguage. CoRR, abs/1908.03557.\nWei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao\nLiu, Jiachen Liu, Hua Wu, and Haifeng Wang.\n2021. UNIMO: towards uniﬁed-modal understand-\ning and generation via cross-modal contrastive learn-\ning. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume\n1: Long Papers), Virtual Event, August 1-6, 2021 ,\npages 2592–2607. Association for Computational\nLinguistics.\nLizi Liao, Yunshan Ma, Xiangnan He, Richang Hong,\nand Tat-Seng Chua. 2018. Knowledge-aware multi-\nmodal dialogue systems. In 2018 ACM Multimedia\nConference on Multimedia Conference, MM 2018,\nSeoul, Republic of Korea, October 22-26, 2018 ,\npages 801–809. ACM.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems 32: Annual Conference on Neural Infor-\nmation Processing Systems 2019, NeurIPS 2019, De-\ncember 8-14, 2019, Vancouver, BC, Canada , pages\n13–23.\nAndrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Se-\nungwhan Moon, Paul A. Crook, Bing Liu, Zhou Yu,\n111\nEunjoon Cho, Pascale Fung, and Zhiguang Wang.\n2021. Continual learning in task-oriented dialogue\nsystems. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021 , pages 7452–\n7467. Association for Computational Linguistics.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung.\n2018. Mem2seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog sys-\ntems. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2018, Melbourne, Australia, July 15-20, 2018, Vol-\nume 1: Long Papers, pages 1468–1478. Association\nfor Computational Linguistics.\nFei Mi, Wanhao Zhou, Lingjing Kong, Fengyu Cai,\nMinlie Huang, and Boi Faltings. 2021. Self-training\nimproves pre-training for few-shot learning in task-\noriented dialog systems. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 7-11 November,\n2021, pages 1887–1898. Association for Computa-\ntional Linguistics.\nLiqiang Nie, Wenjie Wang, Richang Hong, Meng\nWang, and Qi Tian. 2019. Multimodal dialog sys-\ntem: Generating responses via adaptive decoders. In\nProceedings of the 27th ACM International Confer-\nence on Multimedia, MM 2019, Nice, France, Octo-\nber 21-25, 2019, pages 1098–1106. ACM.\nJiahuan Pei, Pengjie Ren, and Maarten de Rijke. 2021.\nA cooperative memory network for personalized\ntask-oriented dialogue systems with incomplete user\nproﬁles. In WWW ’21: The Web Conference 2021,\nVirtual Event / Ljubljana, Slovenia, April 19-23,\n2021, pages 1552–1561. ACM / IW3C2.\nLibo Qin, Yijia Liu, Wanxiang Che, Haoyang Wen,\nYangming Li, and Ting Liu. 2019. Entity-consistent\nend-to-end task-oriented dialogue system with KB\nretriever. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 133–\n142. Association for Computational Linguistics.\nLibo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, and\nTing Liu. 2020. Dynamic fusion network for multi-\ndomain end-to-end task-oriented dialog. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020 , pages 6344–6354. Association for\nComputational Linguistics.\nDinesh Raghu, Atishya Jain, Mausam, and Sachindra\nJoshi. 2021. Constraint based knowledge base distil-\nlation in end-to-end task oriented dialogs. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-\n6, 2021, volume ACL/IJCNLP 2021 of Findings of\nACL, pages 5051–5061. Association for Computa-\ntional Linguistics.\nRevanth Reddy, Danish Contractor, Dinesh Raghu, and\nSachindra Joshi. 2019. Multi-level memory for task\noriented dialogs. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2019, Minneapo-\nlis, MN, USA, June 2-7, 2019, Volume 1 (Long and\nShort Papers) , pages 3744–3754. Association for\nComputational Linguistics.\nShaoqing Ren, Kaiming He, Ross B. Girshick, and\nJian Sun. 2015. Faster R-CNN: towards real-time\nobject detection with region proposal networks. In\nAdvances in Neural Information Processing Systems\n28: Annual Conference on Neural Information Pro-\ncessing Systems 2015, December 7-12, 2015, Mon-\ntreal, Quebec, Canada, pages 91–99.\nAmrita Saha, Mitesh M. Khapra, and Karthik Sankara-\nnarayanan. 2018. Towards building large scale mul-\ntimodal domain-aware conversation systems. InPro-\nceedings of the Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence, (AAAI-18), the 30th innova-\ntive Applications of Artiﬁcial Intelligence (IAAI-18),\nand the 8th AAAI Symposium on Educational Ad-\nvances in Artiﬁcial Intelligence (EAAI-18), New Or-\nleans, Louisiana, USA, February 2-7, 2018 , pages\n696–704. AAAI Press.\nBishal Santra, Potnuru Anusha, and Pawan Goyal.\n2021. Hierarchical transformer for task oriented dia-\nlog systems. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2021, Online, June 6-11,\n2021, pages 5649–5658. Association for Computa-\ntional Linguistics.\nHaoyu Song, Yan Wang, Wei-Nan Zhang, Zhengyu\nZhao, Ting Liu, and Xiaojiang Liu. 2020. Proﬁle\nconsistency identiﬁcation for open-domain dialogue\nagents. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2020, Online, November 16-20, 2020 ,\npages 6651–6662. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nJian Wang, Junhao Liu, Wei Bi, Xiaojiang Liu, Ke-\njing He, Ruifeng Xu, and Min Yang. 2020. Dual\ndynamic memory network for end-to-end multi-turn\ntask-oriented dialog systems. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, COLING 2020, Barcelona, Spain (On-\nline), December 8-13, 2020, pages 4100–4110. Inter-\nnational Committee on Computational Linguistics.\n112\nJianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yun-\njie Gu. 2021. Modelling hierarchical structure be-\ntween dialogue policy and natural language genera-\ntor with option framework for task-oriented dialogue\nsystem. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nHaoyang Wen, Yijia Liu, Wanxiang Che, Libo Qin,\nand Ting Liu. 2018. Sequence-to-sequence learning\nfor task-oriented dialogue with dialogue state repre-\nsentation. In Proceedings of the 27th International\nConference on Computational Linguistics, COLING\n2018, Santa Fe, New Mexico, USA, August 20-26,\n2018, pages 3781–3792. Association for Computa-\ntional Linguistics.\nChien-Sheng Wu, Richard Socher, and Caiming Xiong.\n2019. Global-to-local memory pointer networks for\ntask-oriented dialogue. In 7th International Confer-\nence on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net.\nZe Yang, Wei Wu, Huang Hu, Can Xu, Wei Wang, and\nZhoujun Li. 2021. Open domain dialogue genera-\ntion with latent images. In Thirty-Fifth AAAI Con-\nference on Artiﬁcial Intelligence, AAAI 2021, Thirty-\nThird Conference on Innovative Applications of Ar-\ntiﬁcial Intelligence, IAAI 2021, The Eleventh Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2021, Virtual Event, February 2-9,\n2021, pages 14239–14247. AAAI Press.\nHaoyu Zhang, Meng Liu, Zan Gao, Xiaoqiang Lei, Yin-\nglong Wang, and Liqiang Nie. 2021. Multimodal di-\nalog system: Relational graph-based context-aware\nquestion understanding. In MM ’21: ACM Multime-\ndia Conference, Virtual Event, China, October 20 -\n24, 2021, pages 695–703. ACM.\nZheng Zhang, Lizi Liao, Minlie Huang, Xiaoyan Zhu,\nand Tat-Seng Chua. 2019. Neural multimodal be-\nlief tracker with adaptive attention for dialogue sys-\ntems. In The World Wide Web Conference, WWW\n2019, San Francisco, CA, USA, May 13-17, 2019 ,\npages 2401–2412. ACM.\n113\nA Appendices\nA.1 Hyperparameters Setting\nThe hyperparameters used for MMD dataset are\nshown in Table 6.\nHyperparameter Name MMD\nBatch Size 64\nEpoches 10,000\nText Embedding Size 512\nImage Embedding Size 512\nTransformer Embedding Size 512\nLearning Rate 0.0001\nDropout Ratio 0.15\nTeacher Forcing Ratio 0.9\nMask Length 6\nMask Probability 0.15\nReplace Probability 0.15\nV ocabulary Size 26,422\nTable 6: Hyperparameters we used for MMD.\nA.2 Description of Special Tokens\nThe special tokens used in our experiments are\nshown in Table 7.\nA.3 Loss Function\nOur total loss functionLTotal comprises three parts:\nUTS encoder loss LE, FAIR layer loss LF and\nHTR decoder loss LD, which can be calculated as\nfollows:\nLTotal = γELE + γFLF + γDLD (13)\nwhere γE, γF and γD are hyperparameters, and are\ninitialized equally, i.e., 0.33, 0.33 and 0.33. Then,\nwe tune them on the veriﬁcation set to obtain a\nbetter weight setting of 0.30, 0.35 and 0.35.\nThe UTS encoder loss LE contains two parts:\nLMLM and LMPM,\nLE = LMLM + LMPM (14)\nthe FAIR layer loss contains three parts: LITM,\nLWPA and LIC:\nLF = LITM + LWPA + LIC (15)\nand the HTR decoder loss is divided into two types:\nthe textual decoding loss LTXT for text task and\nimage recommend lossLIMG for image task, which\nis consistent with previous work (Nie et al., 2019).\nLD = LTXT + LIMG (16)\nToken Description\n[CLS] Utterances classﬁcation token\n[TXT] Text token\n[IMG] Image token\n[KNG] Knowledge token\n[MASK] Mask token\n[URL] Image link token\n[PAD] Padding token\n[UNK] Unknown token\nTable 7: Description of special tokens in our experi-\nments.\nDataset Statistics Train Valid Test\nDialogs 105,439 22,595 22,595\nProportion 70% 15% 15%\nQuestions 2M 446K 445K\nImage Responses 904K 194K 193K\nText Responses 1.54M 331K 330K\nAvg. Utterances 40 40 40\nAvg. Pos. Images 4 4 4\nAvg. Neg. Images 4 4 4\nAvg. Words in Question 12 12 12\nAvg. Words in Response 14 14 14\nTable 8: Detailed statistics of the MMD dataset.\nA.4 Dateset Statistics\nA detailed statistics of the MMD dataset is pre-\nsented in Table 8.\nA.5 Error Analysis\nTo better understand the limitations of our model,\nwe conduct an error analysis on UniTranSeR. We\nrandomly select 100 responses generated by Uni-\nTranSeR that achieve low human evaluation scores\nin the test set of MMD. We report several reasons\nfor the low scores, which can roughly be classiﬁed\ninto four categories. (1) KB information in the\ngenerated responses is incorrect (38%), especially\nwhen the corresponding equipped knowledge base\nis large and complex. (2) The sentence structure\nof the generated responses is incorrect and there\nare serious grammatical and semantic errors (24%).\n(3) The model makes incomplete response when\nthere are multiple intentions contained in users’ ut-\nterances (21%). (4) The model selects incorrect\nproduct images since different products have simi-\nlar attributes (17%).\n114"
}