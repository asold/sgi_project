{
  "title": "Spatial–temporal transformer for end-to-end sign language recognition",
  "url": "https://openalex.org/W4318940682",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2137059325",
      "name": "Zhenchao Cui",
      "affiliations": [
        "Hebei University"
      ]
    },
    {
      "id": "https://openalex.org/A2102451058",
      "name": "Wenbo Zhang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Hebei University",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099260433",
      "name": "Zhaoxin Li",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2157067467",
      "name": "Zhaoqi Wang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2137059325",
      "name": "Zhenchao Cui",
      "affiliations": [
        "Hebei University"
      ]
    },
    {
      "id": "https://openalex.org/A2102451058",
      "name": "Wenbo Zhang",
      "affiliations": [
        "Hebei University",
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2099260433",
      "name": "Zhaoxin Li",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2157067467",
      "name": "Zhaoqi Wang",
      "affiliations": [
        "Institute of Computing Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3160382762",
    "https://openalex.org/W3009828227",
    "https://openalex.org/W2895488603",
    "https://openalex.org/W2781400102",
    "https://openalex.org/W2188882108",
    "https://openalex.org/W2759302818",
    "https://openalex.org/W2964253156",
    "https://openalex.org/W3092042812",
    "https://openalex.org/W3045196480",
    "https://openalex.org/W2997931247",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2908497602",
    "https://openalex.org/W6602143840",
    "https://openalex.org/W6601141708",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3203480968",
    "https://openalex.org/W4307123709",
    "https://openalex.org/W4224216771",
    "https://openalex.org/W3034765865",
    "https://openalex.org/W2807840688",
    "https://openalex.org/W2941870244",
    "https://openalex.org/W3147467731",
    "https://openalex.org/W3203359574",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3094831145",
    "https://openalex.org/W3109271037"
  ],
  "abstract": "Abstract Continuous sign language recognition (CSLR) is an essential task for communication between hearing-impaired and people without limitations, which aims at aligning low-density video sequences with high-density text sequences. The current methods for CSLR were mainly based on convolutional neural networks. However, these methods perform poorly in balancing spatial and temporal features during visual feature extraction, making them difficult to improve the accuracy of recognition. To address this issue, we designed an end-to-end CSLR network: Spatial–Temporal Transformer Network (STTN). The model encodes and decodes the sign language video as a predicted sequence that is aligned with a given text sequence. First, since the image sequences are too long for the model to handle directly, we chunk the sign language video frames, i.e., ”image to patch”, which reduces the computational complexity. Second, global features of the sign language video are modeled at the beginning of the model, and the spatial action features of the current video frame and the semantic features of consecutive frames in the temporal dimension are extracted separately, giving rise to fully extracting visual features. Finally, the model uses a simple cross-entropy loss to align video and text. We extensively evaluated the proposed network on two publicly available datasets, CSL and RWTH-PHOENIX-Weather multi-signer 2014 (PHOENIX-2014), which demonstrated the superior performance of our work in CSLR task compared to the state-of-the-art methods.",
  "full_text": "Complex & Intelligent Systems (2023) 9:4645–4656\nhttps://doi.org/10.1007/s40747-023-00977-w\nORIGINAL ARTICLE\nSpatial–temporal transformer for end-to-end sign language\nrecognition\nZhenchao Cui 1,2 · Wenbo Zhang 1,2,3 · Zhaoxin Li 3 · Zhaoqi Wang 3\nReceived: 3 July 2022 / Accepted: 8 January 2023 / Published online: 3 February 2023\n© The Author(s) 2023\nAbstract\nContinuous sign language recognition (CSLR) is an essential task for communication between hearing-impaired and people\nwithout limitations, which aims at aligning low-density video sequences with high-density text sequences. The current methods\nfor CSLR were mainly based on convolutional neural networks. However, these methods perform poorly in balancing spatial\nand temporal features during visual feature extraction, making them difﬁcult to improve the accuracy of recognition. To\naddress this issue, we designed an end-to-end CSLR network: Spatial–Temporal Transformer Network (STTN). The model\nencodes and decodes the sign language video as a predicted sequence that is aligned with a given text sequence. First, since the\nimage sequences are too long for the model to handle directly, we chunk the sign language video frames, i.e., ”image to patch”,\nwhich reduces the computational complexity. Second, global features of the sign language video are modeled at the beginning\nof the model, and the spatial action features of the current video frame and the semantic features of consecutive frames in the\ntemporal dimension are extracted separately, giving rise to fully extracting visual features. Finally, the model uses a simple\ncross-entropy loss to align video and text. We extensively evaluated the proposed network on two publicly available datasets,\nCSL and RWTH-PHOENIX-Weather multi-signer 2014 (PHOENIX-2014), which demonstrated the superior performance\nof our work in CSLR task compared to the state-of-the-art methods.\nKeywords Spatial–temporal encoder · Continuous sign language recognition · Transformer · Patched image\nIntroduction\nSign language is the primary language of the hearing-\nimpaired community and consists of various gestural move-\nments, facial expressions, and head movements. According\nZhenchao Cui and Wenbo Zhang have contributed equally to this work.\nB Zhaoxin Li\ncszli@hotmail.com\nZhenchao Cui\ncuizhenchao@gmail.com\nWenbo Zhang\nzhangwenbo1225@gmail.com\nZhaoqi Wang\nzqwang@ict.ac.cn\n1 School of Cyber Security and Computer, Hebei University,\nLianchi District, Baoding 071002, Hebei, China\n2 Hebei Machine Vision Engineering Research Center, Hebei\nUniversity, Lianchi, Baoding 071002, Hebei, China\n3 Institute of Computing Technology, Chinese Academy of\nSciences, Haidian, Beijing 100190, Beijing, China\nto World Health Organization (WHO) [ 1], 466 million peo-\nple worldwide suffer from hearing loss, accounting for more\nthan 5% of the world’s population, and nearly 2.5 billion peo-\nple are expected to suffer from hearing impairment by 2050.\nTherefore, the development of sign language recognition\n(SLR) technology is of great importance for daily commu-\nnication between hearing-impaired and hearing people as\nwell as for social development. Traditional SLR methods\nare limited to static gestures and isolated words [ 2–4]. In\ncontrast, continuous sign language recognition (CSLR) is a\nbetter way to meet the needs of hearing-impaired commu-\nnication. In comparison with SLR, CSLR methods process\nsign language video that contains rich semantic movements\nof sign language [ 5,6], and the magnitude of the movements\nis more localized and detailed.\nVideo sequences for CSLR are longer and more complex,\nand require feature and semantic learning in sequential frame\nsequences [ 6,7]. It is challenging to map low-density sign\nlanguage video sequences to the corresponding high-density\nnatural language sequences. In real-world scenarios, sign lan-\nguage videos contain complex life scenes [ 8], and thus, there\n123\n4646 Complex & Intelligent Systems (2023) 9:4645–4656\nare long-term semantic dependencies in the videos. Each\nvideo frame is correlated not only with adjacent video frames\nbut also with distant video frames. Typically, CSLR requires\nthe detection of key frames in sign language videos. Spatial\nfeature sequences are extracted from key frames using convo-\nlutional neural networks (CNN), and then, temporal features\nare fused by recurrent neural networks (RNN).\nTo achieve a high recognition accuracy, the feature extrac-\ntion of sign language sequences is especially critical. How-\never, existing methods [ 7–10] have difﬁculties in capturing\ndetailed temporal dynamics over long intervals due to insuf-\nﬁcient feature extraction. Therefore, adequately capturing\nvisual features in sign language videos, especially long-term\nsemantic dependencies, and extracting the corresponding\nvideo contextual features are key issues in CSLR. In addi-\ntion, the contribution of all values in the output coding vector\nC of the RNN encoder [ 11] is the same, which leads to infor-\nmation loss in long sequence data. The impossibility of the\nmodel to be executed in parallel is also a major problem.\nIn contrast, Transformer [ 12] has a strong semantic feature\nextraction capability and long-range feature capture capabil-\nity, not only focusing on local information, but also seeing\nthe global information from the low-level feature, and then\nconstructing the global connection between key points.\nTo solve the long-term semantic dependence of sign\nlanguage video, we propose a temporal and spatial fea-\nture extraction method based on Transformer. The proposed\nmodel can capture the spatial feature information of video\nframes while focusing on the contextual semantic informa-\ntion of consecutive frames. The model can extract the rich\nsign language features more efﬁciently and thus improve the\nrecognition accuracy. The work in this paper is based on the\ntraditional Transformer model and combines the character-\nistics of sign language video sequences for network design.\nSpeciﬁcally, we perform a patch chunking operation on sign\nlanguage video frames to facilitate model learning and train-\ning and propose a Spatial–Temporal Transformer model for\nCSLR (shown in Fig. 1).\nThe main contributions of this paper are as follows:\n1. We propose a deconvolutional sign language recognition\nnetwork that contains a spatial–temporal (ST) feature\nencoder and a dynamic decoder. Where the ST feature\nencoder can distinguish temporal and spatial features,\npart of the attention module focuses only on the contex-\ntual features in the temporal dimension, and the other\npart of the attention module extracts the spatial dynamic\nfeatures of the video frames. By such a design, the extrac-\ntion of sign language video features can be enhanced by\naggregating the attention results from different heads.\n2. For the long frame sequences of sign language videos,\na patch operation is designed to map them into easy-\nto-process sequences. This operation can reduce the\ncomputational complexity and facilitates the processing\nof sign language videos.\n3. We designed a model progressive learning strategy to\nexplore the effectiveness of frame size and patch size on\nrecognition results. We conducted experimental evalua-\ntions on two widely used datasets, CSL and PHOENIX-\n2014 dataset, and obtained competitive performance for\nour method compared to several recently proposed meth-\nods.\nThe remainder of this paper is organized as follows: after\npresenting the related work in Section II, we present the\narchitectural system implemented in Section III. In Section\nIV , we present the experimental results. Finally, in Section\nV , we draw conclusions and look ahead to our work.\nRelated work\nSign language recognition can be classiﬁed into isolated word\nrecognition [ 3–5] and continuous sign language recognition\n[6,7] based on whether it is continuous or not. Early SLR\nrelied on manual extraction of features, including handshape,\nappearance, and motion trajectory [ 8]. The sign language\nvideo is ﬁrst converted into a high-dimensional feature vec-\ntor by a visual encoder, and then, the feature mapping of this\nfeature vector to semantic text is learned by a decoder. Ini-\ntially, CSLR was also based on the recognition of individual\nisolated words. This CSLR based on isolated words involves\nalgorithms related to temporal segmentation [ 9], which is a\ncomplex process and has a high misclassiﬁcation rate due to\ntemporal segmentation. With the development of deep learn-\ning, recent CSLR methods have turned into the automatic\nextraction of sign language features using deep neural net-\nworks.\nAccording to the input modality of recognition, meth-\nods can be divided into single modality and mixed modality,\nwhere single modality refers to RGB video as the input and\nmixed modality adds skeleton, depth, optical ﬂow, and other\ninformation to RGB video [ 8]. Current recognition meth-\nods mainly focus on a single modality. To extract the visual\nfeatures of sign language videos, most research used convo-\nlutional networks to extract feature sequences from videos,\nwhich generally means extracting spatial features using two-\ndimensional convolution (2DCNN) or three-dimensional\nconvolutional networks (3DCNN), and then modeling tem-\nporal information dependence using RNN.\nOscar Koller et al. [ 10] embedded a model combines\nCNN and Long Short-Term Memory (LSTM) in each Hidden\nMarkov Model (HMM) stream, relying on sequence con-\nstraints of HMM independent streams, to learn sign language,\nmouth shape, and hand shape classiﬁers using sequential\nparallelism, which reduced the single-stream HMM Word\n123\nComplex & Intelligent Systems (2023) 9:4645–4656 4647\nFig. 1 Our proposed end-to-end\nSpatial–Temporal Transformer\nNetwork for CSLR\nError Rate (WER) to 26.5% and the dual-stream WER\nto 24.1% on the RWTH-PHOENIX-Weather multi-signer\n2014T dataset (PHOENIX-2014-T, an extended version of\nPHOENIX-2014, is mainly used for sign language translation\ntasks). Cihan Camgoz et al. [ 7] proposed a depth-based and\nend-to-end CSLR framework using the SubUNets approach\nto improve the learning process of intermediate representa-\ntion learning. Cui et al. [ 13] developed a CSLR framework\nusing a combined CNN and Bi-directional Long Short-Term\nMemory (Bi-LSTM) model using an iterative optimization\nstrategy to obtain representative features from the CNN,\nand experiments were conducted on the PHOENIX-2014\ndatabase and SIGNUM signer-dependent set, with WERs\ndecreased to 24.43% and 3.58%. The V AC network proposed\nby Min et al. [ 14] uses 2DCNN to extract frame fea-\ntures and then uses one-dimensional convolutional networks\n(1DCNN) into local feature extraction with the addition\nof two auxiliary modules for alignment supervision. In the\nPHOENIX-2014 dataset, the WER was reduced to 21.2%.\nSince sign language sequences require strong tempo-\nral correlation between frames, 3D convolution has been\nadopted for temporal dimensional feature extraction. Pu et al.\n[11] proposed a CNN-based model for continuous dynamic\nCSLR from RGB video input. They generated pseudo-labels\nfor video clips from sequence learning model with Con-\nnectionist Temporal Classiﬁcation (CTC), and ﬁnetune the\n3D-ResNet with the supervision of pseudo-labels for a better\nfeature representation. Their method was evaluated on the\nPHOENIX-2014 dataset and reduced the WER to 38.0%.\nHuang et al. [ 15] proposed a video-based CSLR method\nwithout temporal segmentation, which is based on a 3DCNN\nnetwork and a hierarchical attention network for recognition.\nYang et al. [ 16] proposed a shallow hybrid CNN, which uses\nboth 2D and 3D convolutions, and is coupled with two LSTM\nnetworks for glossy and sentence-level sequence modeling,\nrespectively.\nAlthough CNN has a strong feature extraction capability,\nit is limited to feature extraction of single-frame images. The\nlimited perceptual domain of 3D convolution leads to insuf-\nﬁcient extraction of long-term time-dependent features. The\nconvolutional network generates a single feature vector rep-\nresenting the whole video with the average pooling layer,\ncompletely ignoring the sequential relationship of video\nframes, which will lose the temporal and contextual informa-\ntion of the sign language video. And convolutional networks\naccumulate multiple layers in counting global information,\nwhich can lead to problems such as low learning rate and\ndifﬁculty in transmitting information over long distances.\nWith the explosive application of Transformer [ 17]i nt h e\nﬁeld of machine translation, its feature of being good at mod-\neling long-range sequences is widely used in the ﬁeld of\nvision, which can alleviate part of the feature extraction prob-\nlems existing in CSLR. From 2020, transformer started to\nmake a splash in the Computer Vision (CV) ﬁeld: Vit [ 18]f o r\nimage classiﬁcation, DETR [ 19] for target detection, seman-\ntic segmentation (SETR [ 20], MedT [ 21]), image generation\n(GANsformer [ 22]), and video understanding (Timesformer\n[23]), among others. M. Rosso et al. [ 24] employed ViT\nfor the ﬁrst time within the road tunnel assessment ﬁeld,\nthe vision transformer provides overwhelming results for\nthe automatic road tunnel defects classiﬁcation paradigm.\nL. Tanzi et al. [ 25] applied a ViT architecture to femur\nfracture classiﬁcation. It outperformed the state-of-the-art\napproaches based on CNN. Camgoz et al. [ 26] introduced\nthe Transformer architecture for joint end-to-end CSLR and\ntranslation, with superior translation performance on the\nPHOENIX-2014-T dataset.\n123\n4648 Complex & Intelligent Systems (2023) 9:4645–4656\nThe length of the sign language sequence creates a high\ndegree of complexity in the computation of the transformer, it\nis impractical to spread the sign language video sequence as\nthe Transformer input. To this end, we improved the trans-\nformer structure. We proposed a patch operation for video\nframes, which reduces the input dimension of video frame\nsequences and can alleviate the problem of computational\npower in the ﬁrst place, while facilitating feature extraction.\nSince it is an impossibility for the transformer to distin-\nguish between temporal and spatial features of sign language\nvideos, we designed an ST dual-channel feature extraction\nnetwork to extract contextual features and dynamic features,\nrespectively, with more adequate visual feature extraction.\nSpatial–temporal transformer networks for\nsign language recognition\nWe propose to consider CSLR as a vector mapping from a\nlow-density video sequence to a sign language high-density\ntext sequence. The mapping is presented in Eq. ( 1)\n{\nY s\n1 , Y s\n2 , Y s\n3 ... Y s\nm | Y s\ni ∈ RdY\n}\n= F\n({\nX t\n1, X t\n2, X t\n3 ... X t\nn | X t\ni ∈ RdX\n})\n, (1)\nwhere X , Y represent the video sequence and text sequence; s\nand t represent the dimensions; m and n represent the length.\nIn this work, we proposed a new end-to-end Spatial-\nTemporal fusion Transformer Network (STTN) for CSLR. Its\narchitecture is shown in Fig. 1, and it is already presented in\nthe fourth paragraph of the introduction. The model consists\nof three main parts: sign language video sequence vector-\nization, ST feature extraction, and feature decryption. First,\nthe video frames are extracted uniformly, and the extracted\nvideo frames are patched and position coded. Second, the\npatch sequences with the position information are input to\nthe encoder part of the model. Third, the temporal and spatial\nfeatures are extracted and fused by the ST encoder. Finally,\nthe fused features are fed to the decoder part for decoding\nand predicting.\nVectorization processing\nPatch embedding\nThe standard transformer input is a one-dimensional token\nembedding. The input in our method is a vector sequence of\nf ∈ R B×T ×C×H ×W dimension (where B is batch size, T is\nthe number of frames, C is the number of channels, and H\nand W represent the height and width of the sign language\nframe, respectively), we reshape each frame Z C×H ×W\ni of the\nT-frame sign language video frame into a 2D block of dimen-\nsion (h × w) × ( p1 × p2 × C), where H = h × p1, W =\nw × p2. h × w is the number of blocks per frame, which\ndirectly affects the length of the input sequence, and a con-\nstant hidden vector dmodel is used on all layers to map the\nblock spreading projection to the size of dmodel = D, where\nD represents the speciﬁc dimensional value. The output of\nthis projection is the patch embedding. At this point, the size\nof the feature map is B ×T × N × D, where N is the product\nof h, w. The output of patch embedding is noted as: X( p,t ),\nwhere p is the number of patches and t is the number of\nframes.\nPositional embedding\nTo prevent the position-related loss in networks, the feature\nmap with dimension f0 ∈ R B×T ×N ×D needs to be position\nencoded before entering the encoder for feature extraction.\nPosition encoding requires that each position has unique posi-\ntion information and the relationship between two positions\ncan be modeled by afﬁne transformations between their posi-\ntions. And it is experimentally veriﬁed that satisﬁes pose\nembedding functions as presented in Eqs. ( 2), ( 3)\nPE\n( pos ,2i ) = sin\n( pos\n100002i /dmodel\n)\n, (2)\nPE ( pos ,2i +1) = cos\n( pos\n100002i +1/dmodel\n)\n, (3)\nwhere pos indicates the position of the token in the sequence,\nthe starting token position is 0, 2 i and 2 i + 1 indi-\ncates even and odd positions, and i takes the value range\n[0,..., dmodel /2). We marked the resulting positional encod-\ning information as e pos\n( p,t ). As shown in Fig. 2, the left image is\nwithout the position coding, and since the dimension values\nof each position are the same, it is impossible to distinguish\nthe information on different positions; the right image is the\nresult after adding the position coding, where the dimension\nvalues on each position are unique, so the information on\neach position can be labeled.\nPositional Encoding and Patch-Embedding have the same\ndimensionality d\nmodel , so these two can be directly summed.\nThe vector after positional-embedding is noted as Z i\n( p,t ) as\nspeciﬁed below, in Eq. ( 4)\nZ i\n( p,t ) = E · X( p,t ) + e pos\n( p,t ), (4)\nwhere X denotes the vector corresponding to each patch, and\nX is multiplied with a learnable matrix E . The result of the\nmultiplication is added to the position code e pos\n( p,t ).\n123\nComplex & Intelligent Systems (2023) 9:4645–4656 4649\nFig. 2 Comparison chart with (right) and without (left) positional encoding\nFig. 3 Q, K, V projection\nprocess\nFig. 4 Schematic diagram of\nmulti-head attention splicing\nEncoder\nThe original Transformer structure like most seq2seq models\n[32] consists of an encoder and a decoder. The encoder and\ndecoder consists of N = 6 identical layers, each layer con-\nsisting of two sub_layers: multi-headed self-attention (MHA)\nmechanism and fully connected feedforward network (FFN).\nEach sub_layer is connected with residual connection and\nnormalization, and the equation is expressed as Eq. ( 5)\nsub _layer _out put\n= Layer Normalization (x + (sub _Layer (x ))) . (5)\nThe transformer’s attention is a linear combination ∑\ni ai vi\nof all word vectors vi in the encoded sentence based on the\nlearned attention weight matrix ai , to perform decoded pre-\ndiction with attention. Multi-head self-attention, on the other\n123\n4650 Complex & Intelligent Systems (2023) 9:4645–4656\nFig. 5 Output representation of self-attentive layer\nhand, is the projection of “query” Q, “key” K, and “value” V\nby means of using different linear transformations of heads\n(“heads” is the number of attention heads). The process of V\nis projected shown in Fig. 3.\nThe results of different attentions are attached together as\nshown in Eq. ( 6) and schematically visualized in Fig. 4\nMHA (Q, K , V ) = Concat (head 1,..., head h ) W o, (6)\nwhere head i = Attention (QW Q\ni , KW K\ni , VW V\ni ). In addi-\ntion, the calculation of attention mostly uses scaled dot-\nproduct (the calculation process is shown in Fig. 5), the\nequation is shown in Eq. ( 7)\nAttention (Q, K , V ) = sof tmax\n( QK T\n√dk\n)\nV . (7)\nIn general, since that CSLR is highly ST dependent, and\nit is difﬁcult capture temporal features as well as captur-\ning spatial features. In this paper, to remedy the problem\nof temporal and spatial features, we propose an ST encoder\nstructure for the dynamic spatial correlation and long-term\ntemporal correlation of sign language videos. As presented\nin Fig. 6, the structure of proposed ST encoder is composed\nof spatial-attention block and temporal-attention block. The\nincoming sign language video vector is divided into two\nchannels for processing temporal and spatial attention, and\nthen, the extracted features are attached together. Using\ndynamic directed spatial correlation and long-term tempo-\nral correlation, the model can be enhanced to extract and\nencode the features of sign language video frames.\nSpatial self-attention block performs MSA calculation\nonly for different tokens of the same frame, and the attention\nvalue of each patch (p, t) spatial dimension is calculated\n[\nZ\ni\n( p,t )\n]space\n= sof tmax\n⎛\n⎝\n(\nqi\n( p,t )√Dh\n) S\n·\n⎡\n⎣ki\n(0,0)\n{\nki(\np′,t\n)\n}\np′=1,...,N\n⎤\n⎦\n⎞\n⎠ , (8)\nwhere N is the number of patches.\nTemporal self-attention block only calculates MSA for\ntokens at the same position in different frames, and calculates\nFig. 6 Architecture diagram of the proposed time encoder\nattention in the time dimension\n[\nZ i\n( p,t )\n]time\n= sof tmax\n⎛\n⎝\n(\nqi\n( p,t )√Dh\n) T\n·\n[\nki\n(0,0)\n{\nki(\np,t ′)\n}\nt ′=1,...,M\n])\n, (9)\nwhere M is the number of frames. And then, the calcu-\nlated temporal attention and spatial attention are connected\ntogether\nZ\ni\n( p,t ) =\n[\nZ i\n( p,t )\n]space\n+\n[\nZ i\n( p,t )\n]time\n. (10)\nDecoder\nThe role of the decoder is to characterize the next possible\n’value’ based on the results of the encoder and the previous\nprediction. As shown in Fig. 7, each decoder consists of three\nsub _layers : the ﬁrst sub _layer includes a multi-headed\nself-attention layer, a normalization layer, and a residual con-\nnection layer; the second sub _layer includes a multi-headed\ncross-attentive layer, a normalization layer, and a residual\nconnection layer; the third sub _layer contains an FFN, a\nnormalization layer, and a residual connection layer. There\nis only one output at the encoder side, and each decoder\nlayer passes into the decoder part acts as the K, V of the\nmulti-headed attention mechanism in the second of these\n123\nComplex & Intelligent Systems (2023) 9:4645–4656 4651\nFig. 7 The decoder structure\ndiagram\nsub _layers . The three sub_layers are computed as shown\nin Eqs. ( 11)(12)(13)\nsel f _attn : Q1\ni =MHA\n(\n˜Qi −1, ˜Qi −1, ˜Qi −1\n)\n, (11)\ncr oss _attn : Q2\ni =MHA\n(\n˜Qi −1, ˜F , F\n)\n, (12)\nFFN : Qi =FFN\n(\nQ2\ni\n)\n. (13)\nTo decode the encoded feature vectors, we employ the\nfollowing three operations on them: self-attention, cross-\nattention, and linear mapping. In addition, for the subsequent\nalignment operation, we perform positional encoding of the\nsign language text to ensure the normal text language order,\nas shown in Eqs. ( 14)(15)\nPE\n( pos ,2i ) = sin\n( pos\n100002i /dmodel\n)\n, (14)\nPE ( pos ,2i +1) = cos\n( pos\n100002i +1/dmodel\n)\n. (15)\nThe position encoding in this section is the same as the posi-\ntion encoding module used in the encoder section for video\nframes. To ensure that each token may only use its prede-\ncessors while extracting contextual information, we used a\nmask operation on the attention computation. To facilitate\nthe probability calculation, we linearly map the vector gen-\nerated by the decoder stack to a larger vector, which becomes\nthe ’logits’ vector. Then, we utilized the sof tmax function\nto perform the maximum probability calculation and select\nthe word corresponding to the highest probability cell as the\noutput of the current time step, as shown in Fig. 8.\nExperiment\nDataset\nAs previously stated, to validate the proposed method,\nthis study conducts experiments on two publicly available\ndatasets: PHOENIX-2014 [ 6], and CSL [ 9]. The data com-\nposition of the two public datasets and the division of training\nand test samples are shown in Table 1.\nFig. 8 The probability prediction diagram for the current time step\nPHOENIX-2014 is a sign language dataset recorded over\n6 years (2009-2014) at the RWTH Aachen University in\nGermany. Recorded during the sign language commentary\non Phoenix Public Television’s daily news and weather pro-\ngrams. All sign language videos were recorded at 25 frames\nper second, which are divided into two versions: 2012 and\n2014. The 2014 dataset is an extension of the 2012 dataset,\nand we experimented on the 2014 version of the dataset.\nThe 2014 dataset with 190 sign language samples containing\n965940 frames and a total of 1558 words combined into 6861\nconsecutive utterances.\nThe Chinese Sign Language dataset (CSL) contains video\ninstances from 50 signers, each repeated 5 times, contain-\ning 25 K tags, for a total of over 100 time-length videos.\nThe dataset is divided into isolated words and continuous\nutterances, containing RGB, depth, and skeleton node data,\nwith 500 classes of words, each containing 250 samples, and\na sequence of 21 skeleton node coordinates. There are 100\nsentences and a total of 25,000 videos; each sentence contains\nan average of 4 to 8 words. Each video example is labeled\nby a professional CSL teacher.\nEvaluation metric\nFor CSLR, substitution, deletion, or insertion of certain\nwords is necessary to maintain consistency between the rec-\n123\n4652 Complex & Intelligent Systems (2023) 9:4645–4656\nTable 1 Statistical data on PHOENIX-2014 and CSL datasets\nStatistics PHOENIX-2014 CSL\nTrain Dev Test Train Dev Test\nSigners 9 9 9 30 10 10\nV ocabulary 1231 460 460 504 504 504\nVideos 5672 540 629 15000 5000 5000\nognized word sequence and the standard word sequence. The\nWER is a metric to measure the performance of a CSLR. It\ncompares the model output at the current parameter values\nwith the actual correct sign language sentence vector and is\ndeﬁned as: WER =\nS+D+I\nN , where S is the number of sub-\nstitutions, D is the number of deletions, I is the number of\ninsertions, and N is the number of words in the reference; an\nexample diagram is shown in Fig. 9.\nImplementation detail\nOur model construction is based on the Pytorch platform [ 33],\nand the experimental environment is a 12 GB Nvidia GTX\n3060 GPU. On the CSL dataset, we extract 60 frames for\neach sign language using uniform frame extraction, and then\ndiscard 12 frames randomly, and use the last 48 frames as\nvalid frame inputs. The size of each frame is ﬁrst adjusted to\n256 ×256. Adam optimizer is used and the learning rate and\nweight decay are set to 10\n−4 and 10 −5, respectively. What\ncan be clearly seen in Table 2 are the default parameters\nfor our experiments (where, D_model is the dimensionality\nof the patch embeddings). Dropout is set to 0 .5 to mitigate\noverﬁtting. We apply the cross-entropy classiﬁcation loss on\nthe predictions p with ground-truth targets t to train the model\nas follows (Eq. ( 16)):\nLoss =− [tlogp + (1 − t )log (1 − p)], (16)\nwhere t is the true label value and p is the predicted probability\nvalue. It characterizes the difference between the true sample\nlabel and the predicted probability.\nAblation experiments\nImpact of video size\nThe different sizes of the extracted video frames will have\nan impact on the extracted features, which in turn will affect\nthe ﬁnal results of the model. To explore the effect of the\nmodel under different cropping ratios, we set three sizes of\n224×224, 112×112 and 256×256 for comparison. The com-\nparison was performed in three aspects: model size, WER,\nand experiment time. According to the results in Table 3,t h e\nmodel size does not change under the three scales. 112 ×112\ntakes up the least amount of video memory compared to\n224 ×224, which is 40.5% lower, and takes the shortest time\nper 200 iterations, which is 47.11% lower, but the effect is\nnot as good as 224 ×224, which is 34.2% lower. 256 ×256 is\nimpossible to experiment with successfully, because it takes\nup too much memory. According to the experimental results,\nthe best results were achieved when the setting was 224 ×224,\nand the WER was reduced to 19.94.\nImpact of patch size\nSign language videos often contain long sequences, which\ncan cause computational difﬁculties when fed directly into\nthe network. To solve this problem, we divide the sign lan-\nguage video frames into small pieces, i.e., image to patch.\nTo investigate the effect of different P (the P is the num-\nber of video frames divided into blocks) on the experimental\nresults, we set the size to 8, 16, and 32, respectively, for exper-\niment comparison. Experiments are carried out based on two\nperspectives: accuracy and time. The size of extracted video\nframes is rescaled uniformly: 224 × 224.\nFig. 9 An example of sentence\nrecognition results from the\nChinese Sign Language dataset\n(CSL). For each Chinese word,\nwe have also provided the\ncorresponding English\ntranslation in brackets. It shows\nﬁve predicted sentences with\ndecreasing error rates from\nprediction 1 to 5. The ﬁrst row\nrepresents the input frame\nsequence. The boxes with a red\nbackground indicate wrong\npredictions. The red symbols\nindicate the wrong predicted\nwords.“ D”, “S”, and “I”\nrepresent deletion, substitution,\nand insertion, respectively\n123\nComplex & Intelligent Systems (2023) 9:4645–4656 4653\nTable 2 The default parameters\nfor the experiment Image size Patch size Batch size Encoder layer(s) Decoder layer(s)\n224*224 32 1 4 4\nHead(s) D_model FF_model Learning_ratio Dropout\n4 512 2048 0.0001 0.5\nTable 3 Performance comparison for the three cropping sizes\nRescale para(MB) Memory WER Times(ms)\n112 × 112 38.908 4.9 26.76 3,867.32\n224 × 224 38.908 11.5 19.94 6,827.97\n256 × 256 38.908 – – –\nTable 4 Performance comparison for the four patch sizes\nPatchs para(MB) Memory WER Times(ms)\n8 42.924 – – –\n16 38.098 11.5 20.98 6,924.03\n28 38.909 9.7 20.51 6,538.18\n32 39.278 6.3 19.94 6,642.84\nTable 5 Ablation experiments on ST. The baseline network is com-\npared with the baseline+designed ST. The metrics include WER and\nthe running time taken to complete 200 iterations\nMethod Dev(%) Test(%) Time(s)\nbaseline 25.11 24.74 3995.11\nbaseline+ST 19.94 19.98 2394.14\nAccording to Table 4, the size of the patch directly affects\nthe length of the sequence, but it has a smaller effect on the\nparameters of the model. When the patch size is 32, the num-\nber of patches is the smallest, and the computation occupies\nthe less memory. Although the computation time is slightly\nincreased, but the model can be computed faster by increas-\ning the batch size. Through experimental comparison, we\nconclude that the best result is achieved when the patch size\nset to 32.\nImpact of the proposed modules\nIn this section, we further verify the effectiveness of the ST\nmodule in the STTN network architecture. In this part of the\nexperiment, we set the video size to 224 ×224 and the patch\nsize to 32. As shown in Table 5, the ﬁrst row represents the\noriginal transformer network, and we can see that the best\nresult of WER is 25.11%, and the second row represents\nthe architecture after adding the ST encoder we designed,\nthe WER drops to 19.94%, which is a relative improvement\nFig. 10 Comparison of the effect of the spatial–temporal (ST) module\non the PHOENIX-2014 dataset\nof 5.17 percentage point, and running time is also reduced.\nFrom Fig. 10, we can see that the curve after adding the ST\nmodule is signiﬁcantly better than before, not only achieving\na lower WER, but also ﬁtting faster. These results suggest\nthe effectiveness of our method.\nComparison with state-of-the-art\nWe compared the proposed algorithm with the state-of-the-\nart CSLR methods (Min et al. [ 29] 2021; Pu et al. [ 9] 2020;\nHao et al. [ 30] 2021; Camgöz et al. 2020 [ 24]) using the\nmost general metric WER, the results of PHOENIX-2014\nare shown in Table 6, and the results of CSL are shown in\nTable 7. Pu et al. feature enhancement by the aid of edit\nreal video text pairs and generate corresponding pseudo-\ncorrespondence pairs, which, although achieving good result\n(WER dropped to 21.3%), did not take full advantage of\nthe visual properties of the sign language videos themselves.\nMin and Hao et al. proposed a visual alignment constraint\n(V AC) method based on the ResNet18 network to enhance\nfeature extraction by additional alignment supervision, and\nproposed a Self-Mutual Knowledge Distillation (SMKD)\nmethod which enforces the visual and contextual modules to\nfocus on short-term and long-term information and enhances\nthe discriminative power of both modules simultaneously.\nTheir proposed method of V AC reduces the WER to 21.2%\nand SMKD’s method reduces the WER to 20.8%, which\nshows that their method is effective and also demonstrates\n123\n4654 Complex & Intelligent Systems (2023) 9:4645–4656\nTable 6 Performance\ncomparison on PHOENIX-2014\ndataset\nMethods Backbone Dev(%) T est(%)\ndel/ins WER del/ins WER\nSubUNet [ 7] CaffeNet 14.6/4.0 40.8 14.3/4.0 40.7\nDilates [ 27] 3D-ResNet 8.3/4.8 38.0 7.6/4.8 37.3\nCNN-LSTM-HMM [ 28] GoogLeNet – 26.0 – 26.0\nSLT [26] Transformer 11.7/6.5 24.9 11.2/6.1 24.6\nFCN [ 10] Custom – 23.7 – 23.9\nCMA [ 9] GoogLeNet 7.3/2.7 21.3 7.3/2.4 21.9\nVAC [29] ResNet18 7.9/2.5 21.2 8.4/2.6 22.3\nSTMC* [ 11] Custom 7.7/3.4 21.1 7.4/2.6 20.7\nSMKD [ 30] ResNet18 6.8/2.5 20.8 6.3/2.3 21.0\nSTTN(ours) Transformer 4.6/2.5 19.94 4.8/2.4 19.98\nThe best results are marked in bold\nThe entries denoted by “*” used extra clues (such as keypoints and tracked face regions)\nTable 7 Performance\ncomparison on the CSL dataset.\nThe entries denoted by “*” used\nextra clues (such as keypoints\nand tracked face regions)\nMethods WER(%)\nLS-HAN [ 15] 17.3\nSubUNet [ 7] 11.0\nHLSTM-attn [ 31]7 . 1\nSF-Net [ 16]3 . 8\nFCN [ 10]3 . 0\nSTMC* [ 11]2 . 1\nVAC [29]1 . 6\nOurs(STTN) 1.2\nthe need to pay more attention to the visual features of the\nsign language video itself. Camgöz et al. proposed an “SLT”\nmodel to do joint end-to-end sign language recognition and\ntranslation, but this method impossibility to distinguish and\nextract temporal and spatial features of sign language videos\nwas very well, yet these features are crucial.\nIn Tables 6 and 7, it can be seen that our proposed method\n(STTN) achieves good performance, with WERs falling to\n19.94% and 1.2% on the PEOENIX-2014 and CSL datasets,\nrespectively. This demonstrates that the long-term temporal\ndependence and dynamic spatial dependence of joint sign\nlanguage videos can better learn the visual properties of sign\nlanguage videos.\nResults’ visualization\nFor the purpose of better understand the learning process,\nwe selected a sentence from the CSL dataset for sequence\nvisualization, as shown in Fig. 9, as already mentioned in\nthe previous sections, where different prediction sequences\ncorrespond to different WER values. As can be seen from the\nFig. 11, that after the 12th epoch, the WER ﬂuctuates slightly\nFig. 11 Plot of the effect of validation WER on the CSL dataset\nFig. 12 Sample of data in the PHOENIX-2014 dataset\n123\nComplex & Intelligent Systems (2023) 9:4645–4656 4655\nFig. 13 Plot of the effect of training WER on the PHOENIX-2014\ndataset\naround 1.2, which indicates that we achieve better results than\nthe previous model. We also selected a random sample of\ndata in the PHOENIX-2014 dataset, which is demonstrated in\nFig. 12, and we can see the movements of the signer displayed\nin each frame. In addition, we visualized the training effect\n(WER change curve during train, and validation) in Fig. 13,\nin which the WER decreases faster during training, and the\nvalue of WER reached a low point of 14.26 in the 29th epoch.\nIn addition, the validated experimental data show that the\ndecline of WER slows down, and stops decrease after the\n30th epoch reaches 19.94%.\nConclusions\nInadequate feature extraction is one of the major problems\nin current CSLR tasks, which directly leads to poor recogni-\ntion of sign languages. In this study, we propose to enhance\nthe feature extraction capability of the network model in\nboth temporal and spatial dimensions, and to patch the sign\nlanguage video frames to reduce the computational effort\nwhile enhancing the generalization capability of the model,\nthus allowing the CSLR network to be trained end-to-end.\nThe proposed method does not require a text-related induc-\ntive bias module and aligns video and text using a simple\ncross-entropy loss. The experiments show that our proposed\nachieves state-of-the-art performance on CSL dataset and\nPHOENIX-2014 dataset, offering new perspectives on vision\nand natural language processing.\nAcknowledgements This work was supported by National Key Rese-\narch and Development Program of China (No. 2020YFC1523302), the\nResearch Initiation Project for High-Level Talents of Hebei University,\nContract No. 521100221081, National Natural Science Foundation of\nChina under Grant No. 62172392, Provincial Science and Technology\nProgram of Hebei Province (No. 22370301D), and compute services\nfrom Hebei Artiﬁcial Intelligence Computing Center.\nData Availability The data that support the ﬁndings of this study are\navailable from the corresponding author, [Zhaoxin Li], upon reasonable\nrequest.\nDeclarations\nConﬂict of interest The authors have no competing interests to declare\nthat are relevant to the content of this article.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing, adap-\ntation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indi-\ncate if changes were made. The images or other third party material\nin this article are included in the article’s Creative Commons licence,\nunless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your\nintended use is not permitted by statutory regulation or exceeds the\npermitted use, you will need to obtain permission directly from the copy-\nright holder. To view a copy of this licence, visit http://creativecomm\nons.org/licenses/by/4.0/.\nReferences\n1. Organization WH (2020) Deafness and hearing loss.\n[Online]. Available: https://www.who.int/health-topics/hearing-\nloss/. Accessed 3 Mar 2021\n2. Slimane FB, Bouguessa M (2021) “Context Matters: Self-Attention\nfor Sign Language Recognition,” 2020 25th International Confer-\nence on Pattern Recognition (ICPR), pp. 7884-7891, https://doi.\norg/10.1109/ICPR48806.2021.9412916\n3. Li D, Opazo CR, Y u X, Li H (2020) Word-level Deep Sign Lan-\nguage Recognition from Video: A New Large-scale Dataset and\nMethods Comparison. IEEE Winter Conference on Applications\nof Computer Vision (W ACV) 2020:1448–1458. https://doi.org/10.\n1109/W ACV45572.2020.9093512\n4. Konstantinidis D, Dimitropoulos K, Daras P (2018) “SIGN LAN-\nGUAGE RECOGNITION BASED ON HAND AND BODY\nSKELETAL DA TA,” 2018 - 3DTV-Conference: The True Vision\n- Capture, Transmission and Display of 3D Video (3DTV-CON),\npp. 1-4, https://doi.org/10.1109/3DTV .2018.8478467\n5. Cao C, Zhang Y , Wu Y , Lu H, Cheng J (2017) Egocentric Gesture\nRecognition Using Recurrent 3D Convolutional Neural Networks\nwith Spatiotemporal Transformer Modules. IEEE International\nConference on Computer Vision (ICCV) 2017:3783–3791. https://\ndoi.org/10.1109/ICCV .2017.406\n6. Oscar K, Jens F, Hermann N (2015) Continuous sign language\nrecognition: towards large vocabulary statistical recognition sys-\ntems handling multiple signers. Computer Vision and Image\nUnderstanding 141:108–125. https://doi.org/10.1016/j.cviu.2015.\n09.013.( ISSN 1077-3142 )\n7. Camgoz NC, Hadﬁeld S, Koller O, Bowden R (2017) SubUNets:\nEnd-to-End Hand Shape and Continuous Sign Language Recogni-\ntion. IEEE International Conference on Computer Vision (ICCV)\n2017:3075–3084. https://doi.org/10.1109/ICCV .2017.332\n8. Huang J, Zhou W, Zhang Q, Li H, Li W (2018) Video-Based\nSign Language Recognition Without Temporal Segmentation. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, 32(1).\nhttps://doi.org/10.1609/aaai.v32i1.11903\n9. Pu J, Zhou W, Hu H, et al (2020) Boosting continuous sign language\nrecognition via cross modality augmentation[C]. Proceedings of\nthe 28th ACM International Conference on Multimedia. 1497-1505\n123\n4656 Complex & Intelligent Systems (2023) 9:4645–4656\n10. Cheng KL, Yang Z, Chen Q, Tai YW (2020) Fully Convolutional\nNetworks for Continuous Sign Language Recognition. In: V edaldi,\nA., Bischof, H., Brox, T., Frahm, JM. (eds) Computer Vision -\nECCV 2020. ECCV 2020. Lecture Notes in Computer Science(),\nvol 12369. Springer, Cham. https://doi.org/10.1007/978-3-030-\n58586-0_41\n11. Zhou H, Zhou W, Zhou Y , Li H (2020) Spatial-Temporal Multi-Cue\nNetwork for Continuous Sign Language Recognition. Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence 34(07):13009–\n13016. https://doi.org/10.1609/aaai.v34i07.7001\n12. Zihang D, Zhilin Y , Yiming Y , Jaime C, Quoc L, Ruslan S (2019)\nTransformer-XL: Attentive Language Models beyond a Fixed-\nLength Context. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages 2978-2988,\nFlorence, Italy. Association for Computational Linguistics. https://\ndoi.org/10.18653/v1/P19-1285\n13. Cui R, Liu H, Zhang C (2019) A Deep Neural Framework\nfor Continuous Sign Language Recognition by Iterative Train-\ning. IEEE Trans Multimedia 21(7):1880–1891. https://doi.org/10.\n1109/TMM.2018.2889563\n14. Xie P , Cui Z, Du Y , et al (2021) Multi-Scale Local-Temporal Simi-\nlarity Fusion for Continuous Sign Language Recognition[J]. arXiv\npreprint arXiv:2107.12762\n15. Huang J, Zhou W, Zhang Q, Li H, Li W (2018) Video-Based\nSign Language Recognition Without Temporal Segmentation. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence, 32(1).\nhttps://doi.org/10.1609/aaai.v32i1.11903\n16. Yang Z, Shi Z, Shen X, et al (2019) SF-Net: Structured feature net-\nwork for continuous sign language recognition[J]. arXiv preprint\narXiv:1908.01341\n17. V aswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob,\nJones Llion, Gomez Aidan N, Kaiser Łukasz, Polosukhin Illia\n(2017) Attention is all you need. In Proceedings of the 31st Inter-\nnational Conference on Neural Information Processing Systems\n(NIPS’17). Curran Associates Inc., Red Hook, NY , USA, 6000-\n6010\n18. Alexey D, Lucas B, Alexander K, Dirk W, Xiaohua Z, Thomas U,\nMostafa D, Matthias M, Georg H, Sylvain G et al (2021) An image\nis worth 16x16 words: Transformers for image recognition at scale.\nI nI C L R ,1 ,2 ,3 ,5 ,7\n19. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko\nS (2020) End-to-End Object Detection with Transformers. In:\nV edaldi, A., Bischof, H., Brox, T., Frahm, JM. (eds) Computer\nVision - ECCV 2020. ECCV 2020. Lecture Notes in Computer Sci-\nence(), vol 12346. Springer, Cham. https://doi.org/10.1007/978-3-\n030-58452-8_13\n20. Zheng S et al (2021) Rethinking Semantic Segmentation from a\nSequence-to-Sequence Perspective with Transformers. IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR) 2021:6877–6886. https://doi.org/10.1109/CVPR46437.\n2021.00681\n21. V alanarasu JMJ, Oza P , Hacihaliloglu I, Patel VM (2021) Medical\nTransformer: Gated Axial-Attention for Medical Image Segmenta-\ntion. In: , et al. Medical Image Computing and Computer Assisted\nIntervention - MICCAI 2021. MICCAI 2021. Lecture Notes in\nComputer Science(), vol 12901. Springer, Cham. https://doi.org/\n10.1007/978-3-030-87193-2_4\n22. Hudson DA, Zitnick L (2021) Generative adversarial transform-\ners[C]. International Conference on Machine Learning. PMLR,\n4487-4499. https://proceedings.mlr.press/v139/hudson21a.html\n23. Bertasius G, Wang H, Torresani L (2021) Is space-time atten-\ntion all you need for video understanding[J]. arXiv preprint\narXiv:2102.05095, 2(3):4\n24. Rosso M, Marasco G, Aiello S et al. Convolutional networks and\ntransformers for intelligent road tunnel investigations, Computers\nand Structures, https://doi.org/10.1016/j.compstruc.2022.106918\n25. Tanzi L, Audisio A, Cirrincione G, Aprato A, V ezzetti E\n(2021) Vision Transformer for femur fracture classiﬁcation.\narXiv:2108.03414\n26. Cihan Camgöz N, Koller O, Hadﬁeld S, Bowden R (2020) “Sign\nLanguage Transformers: Joint End-to-End Sign Language Recog-\nnition and Translation”, 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 10020-10030, https://\ndoi.org/10.1109/CVPR42600.2020.01004\n27. Pu Junfu, Zhou Wengang, Li Houqiang (2018) Dilated convo-\nlutional network with iterative optimization for continuous sign\nlanguage recognition. In Proceedings of the 27th International Joint\nConference on Artiﬁcial Intelligence (IJCAI’18). AAAI Press,\n885-891\n28. Koller O, Camgoz NC, Ney H, Bowden R (1 Sept. 2020) “Weakly\nSupervised Learning with Multi-Stream CNN-LSTM-HMMs to\nDiscover Sequential Parallelism in Sign Language Videos”, in\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 42, no. 9, pp. 2306-2320, https://doi.org/10.1109/TPAMI.\n2019.2911077\n29. Min Y , Hao A, Chai X, Chen X (2021) Visual Alignment Constraint\nfor Continuous Sign Language Recognition. IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV) 2021:11522–\n11531. https://doi.org/10.1109/ICCV48922.2021.01134\n30. Hao A, Min Y , Chen X (2021) Self-Mutual Distillation Learning for\nContinuous Sign Language Recognition. IEEE/CVF International\nConference on Computer Vision (ICCV) 2021:11283–11292.\nhttps://doi.org/10.1109/ICCV48922.2021.01111\n31. Guo Dan, Zhou Wengang, Li Houqiang, Wang Meng (2018) Hier-\narchical LSTM for sign language translation. In Proceedings of\nthe Thirty-Second AAAI Conference on Artiﬁcial Intelligence and\nThirtieth Innovative Applications of Artiﬁcial Intelligence Confer-\nence and Eighth AAAI Symposium on Educational Advances in\nArtiﬁcial Intelligence (AAAI’18/IAAI’18/EAAI’18). AAAI Press,\nArticle 838, 6845-6852\n32. Cho Kyunghyun, van Merriënboer Bart, Gulcehre Caglar, Bah-\ndanau Dzmitry, Bougares Fethi, Schwenk Holger, Bengio Y oshua\n(2014) Learning Phrase Representations using RNN Encoder-\nDecoder for Statistical Machine Translation. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1724-1734, Doha, Qatar. Association for\nComputational Linguistics\n33. Paszke Adam, Sam Gross, Soumith Chintala, Gregory Chanan,\nEdward Yang, Zach DeVito, Zeming Lin, Alban Desmaison,\nLuca Antiga, Adam Lerer (2017) “Automatic differentiation in\nPyTorch.”\nPublisher’s Note Springer Nature remains neutral with regard to juris-\ndictional claims in published maps and institutional afﬁliations.\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8196624517440796
    },
    {
      "name": "Sign language",
      "score": 0.6799219846725464
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5407050251960754
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4797816872596741
    },
    {
      "name": "Transformer",
      "score": 0.4740324318408966
    },
    {
      "name": "Feature extraction",
      "score": 0.42982620000839233
    },
    {
      "name": "Speech recognition",
      "score": 0.4243302345275879
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4229659140110016
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}