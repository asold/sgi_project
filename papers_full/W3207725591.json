{
  "title": "Explainable Transformer-Based Neural Network for the Prediction of Survival Outcomes in Non-Small Cell Lung Cancer (NSCLC)",
  "url": "https://openalex.org/W3207725591",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4308395596",
      "name": "Elly Kipkogei",
      "affiliations": [
        "AstraZeneca (Netherlands)"
      ]
    },
    {
      "id": "https://openalex.org/A4380036607",
      "name": "Gustavo Alonso Arango Argoty",
      "affiliations": [
        "AstraZeneca (Netherlands)"
      ]
    },
    {
      "id": "https://openalex.org/A111302283",
      "name": "Ioannis Kagiampakis",
      "affiliations": [
        "AstraZeneca (Netherlands)"
      ]
    },
    {
      "id": "https://openalex.org/A2479822347",
      "name": "Arijit Patra",
      "affiliations": [
        "AstraZeneca (Brazil)"
      ]
    },
    {
      "id": "https://openalex.org/A2098608953",
      "name": "Etai Jacob",
      "affiliations": [
        "AstraZeneca (Netherlands)"
      ]
    },
    {
      "id": "https://openalex.org/A4308395596",
      "name": "Elly Kipkogei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4380036607",
      "name": "Gustavo Alonso Arango Argoty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A111302283",
      "name": "Ioannis Kagiampakis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2479822347",
      "name": "Arijit Patra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098608953",
      "name": "Etai Jacob",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3103855458",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3147114346",
    "https://openalex.org/W2115709314",
    "https://openalex.org/W3147894994",
    "https://openalex.org/W2999445610",
    "https://openalex.org/W3097349486",
    "https://openalex.org/W2753919178",
    "https://openalex.org/W2611463039",
    "https://openalex.org/W3216203738",
    "https://openalex.org/W2771006570",
    "https://openalex.org/W2909679049",
    "https://openalex.org/W2125782079",
    "https://openalex.org/W2506122519",
    "https://openalex.org/W1966455406",
    "https://openalex.org/W2346651667",
    "https://openalex.org/W2561459036",
    "https://openalex.org/W3103673446",
    "https://openalex.org/W2898416788",
    "https://openalex.org/W3152976695",
    "https://openalex.org/W3017125932",
    "https://openalex.org/W2136787567",
    "https://openalex.org/W2057197272",
    "https://openalex.org/W2577490495",
    "https://openalex.org/W2950993016",
    "https://openalex.org/W2910111050",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W1966327575",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2580767461",
    "https://openalex.org/W1900820545",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W3175417087",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2949066452",
    "https://openalex.org/W3158236124",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1580788756",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2900619723",
    "https://openalex.org/W3016170992",
    "https://openalex.org/W2804935370",
    "https://openalex.org/W2941531368"
  ],
  "abstract": "Abstract In this paper, we introduce the “Clinical Transformer” - a recasting of the widely used transformer architecture as a method for precision medicine to model relations between molecular and clinical measurements, and the survival of cancer patients. Although the emergence of immunotherapy offers a new hope for cancer patients with dramatic and durable responses having been reported, only a subset of patients demonstrate benefit. Such treatments do not directly target the tumor but recruit the patient’s immune system to fight the disease. Therefore, the response to therapy is more complicated to understand as it is affected by the patient’s physical condition, immune system fitness and the tumor. As in text, where the semantics of a word is dependent on the context of the sentence it belongs to, in immuno-therapy a biomarker may have limited meaning if measured independent of other clinical or molecular features. Hence, we hypothesize that the transformer-inspired model may potentially enable effective modelling of the semantics of different biomarkers with respect to patients’ survival time. Herein, we demonstrate that this approach can offer an attractive alternative to the survival models utilized in current practices as follows: (1) We formulate an embedding strategy applied to molecular and clinical data obtained from the patients. (2) We propose a customized objective function to predict patient survival. (3) We show the applicability of our proposed method to bioinformatics and precision medicine. Applying the clinical transformer to several immuno-oncology clinical studies, we demonstrate how the clinical transformer outperforms other linear and non-linear methods used in current practice for survival prediction. We also show that when initializing the weights of a domain-specific transformer by the weights of a cross-domain transformer, we further improve the predictions. Lastly, we show how the attention mechanism successfully captures some of the known biology behind these therapies.",
  "full_text": "Explainable Transformer-Based Neural Network for\nthe Prediction of Survival Outcomes in Non-Small\nCell Lung Cancer (NSCLC)\nElly Kipkogei∗\nEarly Oncology\nAstraZeneca\nelly.kipkogei@astrazeneca.com\nGustavo Alonso Arango Argoty∗†\nEarly Oncology\nAstraZeneca\ngustavo.arango@astrazeneca.com\nIoannis Kagiampakis\nEarly Oncology\nAstraZeneca\nioannis.kagiampakis@astrazeneca.com\nArijit Patra\nCPSS\nAstraZeneca\narijit.patra@astrazeneca.com\nEtai Jacob†\nEarly Oncology\nAstraZeneca\netai.jacob@astrazeneca.com\nAbstract\nIn this paper, we introduce the “Clinical Transformer” - a recasting of the widely\nused transformer architecture as a method for precision medicine to model relations\nbetween molecular and clinical measurements, and the survival of cancer patients.\nAlthough the emergence of immunotherapy offers a new hope for cancer patients\nwith dramatic and durable responses having been reported, only a subset of patients\ndemonstrate beneﬁt. Such treatments do not directly target the tumor but recruit\nthe patient’s immune system to ﬁght the disease. Therefore, the response to\ntherapy is more complicated to understand as it is affected by the patient’s physical\ncondition, immune system ﬁtness and the tumor. As in text, where the semantics\nof a word is dependent on the context of the sentence it belongs to, in immuno-\ntherapy a biomarker may have limited meaning if measured independent of other\nclinical or molecular features. Hence, we hypothesize that the transformer-inspired\nmodel may potentially enable effective modelling of the semantics of different\nbiomarkers with respect to patients’ survival time. Herein, we demonstrate that\nthis approach can offer an attractive alternative to the survival models utilized in\ncurrent practices as follows: (1) We formulate an embedding strategy applied to\nmolecular and clinical data obtained from the patients. (2) We propose a customized\nobjective function to predict patient survival. (3) We show the applicability of our\nproposed method to bioinformatics and precision medicine. Applying the clinical\ntransformer to several immuno-oncology clinical studies, we demonstrate how the\nclinical transformer outperforms other linear and non-linear methods used in current\npractice for survival prediction. We also show that when initializing the weights\nof a domain-speciﬁc transformer by the weights of a cross-domain transformer,\nwe further improve the predictions. Lastly, we show how the attention mechanism\nsuccessfully captures some of the known biology behind these therapies.\n∗Equal contribution.\n†Corresponding author.\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nFigure 1: General framework for the Clinical Transformer. A) Data preprocessing. B) Training of\npan-cancer model using a cross-domain clinical transformer. C) Training of cancer-speciﬁc model on\n80% of the NSCLC. D) Evaluation of clinical transformer using concordance index. E) Applications\nof transformer attentions across different bioinformatics analysis.\nKeywords : Survival Outcomes, Attention-Mechanism, Deep Learning\n1 Introduction\nIn recent years, computational modelling of sequences with variable lengths and long-range depen-\ndencies received a boost through the introduction of a new class of architectures called transformers.\nThese models were effective in contextual modelling on account of their self-attention architecture\nthat allowed them to differentially focus on relevant parts of an input sequence. Such an ability to\nutilize relevant parts of an input, where such salient sub-sections may not be sequentially located,\nenables the transformer to approach the modelling of variable length inputs using the semantic context\nimplied therein.\nMotivated by the applicability of self-attention, we extend to the domain of precision medicine and\nask whether it is possible to predict treatment outcome and response from individualized records\nof patients undergoing immuno-oncology interventions. Such records aggregate disparate informa-\ntion about patient clinical assessments, demographics, treatment history, along with various tumor\nmolecular measurements and can be combined in a sequence and formulated as a machine learning\nproblem with a response variable such as overall survival (continuous) or response (categorized or\nbinary). Importantly, at the single patient level, these clinical and molecular features conceal complex\ninterrelationships that are intertwined with the patient outcome. The question then distils down to the\nchoice of a suitable learning machine that can approximate such undetermined interactions across the\nspace of the available clinical and molecular data for individual subjects. This is achieved in our case\nthrough a novel interpretation of the transformer architecture proposed in this work.\nA frequent problem in clinical studies, especially in early stage (that is, phase I and II) trials, is\nthe small sample size which poses a great challenge to building reliable and interpretable predic-\ntion models. In addition, the complexity of the underlying biology and the heterogeneity of the\npatient population impair the ability to deﬁne biomarkers that can be used for patient selection in\nsubsequent trials. Therefore, the ability to utilize transfer learning, an attribute that comes as part\nof the transformer architecture, is of great importance, especially since the majority on the cancer\ndrugs, including immuno-oncology compounds are only effective in a small subset of the patients.\nImportantly, transferable representations enable us to extend automated survival analysis to conditions\nwhere limited patient data is available in a manner that was not possible in traditional approaches.\nThis also allows future extensions of our precision medicine approach for orphan or rare diseases\nwhere the personalization of care is often also hampered by limited data availability.\n2 Related Work\nThe ability of transformer architectures to handle sequences without requiring intermediate storage\nrepresents a signiﬁcant shift in how machine learning models process such sequences, as is often\na requirement in several applications in systems biology. Transformer architectures introduced a\n2\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nsigniﬁcant acceleration in the development of computational models for holistic understanding of\nsequential data. Originally proposed for NLP by Vaswani et al. [2017], transformers have been\nextensively applied to other ﬁelds ( Chaudhari et al. [2019]). Furthermore, a number of studies have\nutilized transformers or a modiﬁed version to address biological problems. For example, Widrich\net al. [2020] implement a modern Hopﬁeld Network with transformer-like attention for immune\nrepertoire classiﬁcation. Elnaggar et al. [2020] trained Trasformer XL (Dai et al. [2019]), BERT\n( Devlin et al. [2018]) and ALBERT (Lan et al. [2020]) on protein sequences to predict secondary\nstructures ; Avsec et al. [2021] implemented enhancer transformer to predict gene expression from\nDNA sequence; Jumper et al. [2020] predicted protein structure using 3D equivariant transformers.\nSurvival analysis is an area that has been dominated by statistical approaches ( Harrell Jr [2015],\nCox [1972]) such as multi-variate modelling using Cox proportional hazards (CoxPH) for predicting\npatients’ response to therapy based on individual’s genomic and clinical features ( Anagnostou et al.\n[2020]). Several non-linear approaches have also been proposed including gradient boosting machines\n( Pölsterl [2020]) among others . In recent years, deep neural networks have also been applied to\nsurvival analysis. For instance, Katzman et al. [2018], developed DeepSurv, a personalized treatment\nrecommender system using a CoxPH deep network. Youseﬁ et al. [2017] implemented SurvivalNet\nthat uses Cox partial-likelihood to train a neural network to predict survival outcomes. Hu et al.\n[2021] developed a transformer-based survival model that utilizes ordinal regression to optimize\nsurvival probabilities over time. However, none of these methods take into account the interactions\nbetween the features in an explicit way as part of the model or enable reviewing of the model at the\nsingle patient level.\nWith the objective of personalized risk prediction in mind, we propose our key contributions as\nfollows:\n• A novel approach based on transformer architectures that can handle both clinical and\nmolecular features and incorporate a custom loss function that links together the input\nfeatures, internal attention mechanisms and clinical endpoints (time to a clinical event).\n• A framework that explains and summarizes the mutual relationships between different\nfeatures and the relationships between features and clinical endpoints, as opposed to the\nstate-of-the-art competing frameworks that render themselves incompatible with downstream\nexplainability as required in several precision medicine applications Maciejko et al. [2017].\n• A widely applicable framework in computational biology that is validated on public data\nsets with demonstrated use cases in precision medicine and bioinformatics.\n3 Methods\n3.1 Training and Dataset\nThe general framework for clinical transformer training and testing is described in Figure 1. A large\ncohort of 1661 patients from the Memorial Sloan Kettering Cancer Center (MSKCC) comprising of\nadvanced cancer patients treated with Immune Checkpoint inhibitor (Figure 1A) was downloaded\nfor analysis (Samstein et al. [2019]). Each patient entry included genomic information of 468 genes\n(those are, mutation calls for each of the 468 genes) and 8 clinical features (e.g. age and sample type)\n(See further details in Samstein et al. [2019]. To reduce sparsity in the data, genes with mutations in\nless than 5% of the total population were excluded from further analysis, resulting in 55 features with\ngenomic information instead of the initial 468. A cross-domain clinical transformer (Figure 1B) was\npre-trained on 10 cancer types (N=1266) and a “snapshot” of NSCLC patients (50 randomly selected\nNSCLC patient out of a total of 344 patients). Later, speciﬁc clinical transformer (Figure 1C) was\ntrained with 80% of the NSCLC data excluding the 50 patients that were used for the model in B.\nThis transformer was initialized with the weights from the pan cancer transformer. Evaluation was\nperformed by the concordance index over the testing set (20%) of the NSCLC clinical transformer B\n(Figure 1D). To account for stability, this process was repeated 100 times with randomly generated\n80/20 splits. Transformer attentions capture interactions between input features in the context of\npatients’ survival (Figure 1E). These attention weights can be used for different bioinformatics\napplications, including network analysis in systems biology, gene set enrichment to identify dominant\nbiological pathways with clinical relevance, clustering and other pattern recognition methods for\npatient stratiﬁcation.\n3\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \n3.2 Numerical embedding of the clinical and genomic feature space\nThe initial transformer architecture proposed by Vaswani et al. [2017] uses positional encoding\nvectors to account for the absolute location of tokens in the sequence. In our case, since we are\nnot dealing with sequential data, we excluded the positional encoding vectors from the transformer\narchitecture. To process numerical data, we projected the initial set of features F ∈RN×1 (F is a\nvector of 65 features in our analysis) to an embedding space E ∈RN×dk E is a matrix of 65 by\n32 in our analysis) by feeding the feature space F into a dense layer (Figure 2A). Therefore, each\ncomponent of the feature space F is decomposed into a linear combination of the learned weights\nthat is then fed into the transformer. The rationale behind this decomposition is to embed numerical\ndata into a ﬁxed vector size.\n3.3 Scaled Dot-Product Attention\nThe scaled dot-product attention in a transformer, enables the model to selectively focus on relevant\nfeatures from the input space by identifying similarities among the input features while associating\nthose similarities with the model outcome. The attention is deﬁned as:\nAttention(Q,K,V ) = Softmax\n(QKT\n√dk\n)\nV\nwhere Q,K,V ∈RN×dk . N is the number of input features and dk is the dimension of key, query\nand value vectors (see Figure 2B).\n3.4 Loss function\nTo optimize model parameters towards patient survival outcomes, instead of a binary response or\ntext translation, we utilized the concordance metric in survival analysis workﬂow as a measure of\nmodel discrimination (Figure 2C). Harrell’s concordance index C is deﬁned as the proportion of\nobservations that the model can order correctly in terms of survival times ( Steck et al. [2008]). This\ncan be interpreted as a generalization of the area under the ROC curve (AUC) that considers censored\ndata. It represents the global encapsulation of the model discrimination power and its ability to\nprovide a reliable ranking of the survival times based on the individual risk scores. In our workﬂow,\nthe concordance-based model discrimination was implementing using a loss function with a sigmoid\napproximation of Harrell’s C-index ( Schmid et al. [2016], Mayr and Schmid [2014]). This led to an\nobjective of the form:\n∑\nij\nwij\n1\n1 + exp(ηj−ηi\nσ )\nwij = ∆iI(Ti <Tj)∑\nij∆iI(Ti <Tj)\nwhere the indices iand jrefer to pairs of observations in the sample, ∆i = 0 if censored and 1 if\ndeceased, T is the corresponding survival time, ηis the predicted scores from clinical transformer\nand σis a smoothing parameter for the sigmoid approximation. ηcan be thought of as xβ where\nβ ∈Rdn are the weights from the transformer’s last linear projection layer anddn is the number of\nnodes.\nThe ﬁnal outputs from clinical transformer are: (1) Predicted scores - output from the ﬁnal layer of\nfully connected neural network with linear activation function. (2) A list of attention matrices of the\nform softmax(QKT\n√dk\n). The length of this list corresponds to the number of transformer layers.\n3.5 Hyperparameter tuning\nWe tested the clinical transformer using different settings, including different number of heads, layers,\nepochs and learning rates. We identiﬁed that the model quickly overﬁts after 200 epochs. We also\nobserved that the model starts to overﬁt as the number of layers increase. We did not observe a\n4\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nFigure 2: Schematic representation of the Clinical Transformer. The numerical embedding layer (A)\ntakes tabular data as input and projects each feature into a d dimensional vector. The transformer\nlayer (B) takes as input N by Dmatrix and projects it into the K, Qand V matrices that are fed into\nthe transformer layer. Output of the transformer layer is ﬂattened and translated into a risk score. The\nmodel is trained using the c-index-based loss function which takes into account survival time and\ncensorship information.\nsigniﬁcant difference in performance when using 4 or more heads per layer (see Figure S7). The\noptimal setting of the clinical transformer was set to 2 layers, 4 heads, 100 epochs and 0.001 learning\nrate. We trained our models in one machine with one Tesla 32GB V100 GPU.\n4 Results and Discussion\n4.1 The clinical transformer outperforms current practice of survival analysis in the ﬁeld\nThe predicted scores from the transformer are directly proportional to patients’ survival time and\ntherefore the scores can be analyzed using standard survival analysis approaches such as the concor-\ndance index (note that a concordance index of 0.5 indicates random predictions) Mayr and Schmid\n[2014]. Alternatively, we can stratify patients into low and high scores using the predicted scores\nwhile maximizing the separation of the Kaplan Meier (KM) curves between these groups. Here,\nwe utilized these two approaches to assess the performance of the predictions. We computed the\nconcordance index (CI) for every test set prediction in the 100 training-test random splits (referred\nto as “folds” in the rest of the paper). We also performed patient stratiﬁcation using the test sets\npredicted scores and computed corresponding hazard ratios comparing high vs low scores. To test\nmodel’s consistency in assigning scores to different patients, we tracked each patient in the test set\nacross the folds. These patients were then subdivided into four ‘survival’ groups: super-responders\n(>18 months), two intermediate group (≥12 months & ≤18 months, ≥6 months & <12 months)\nand fast-progressors (<6 months). The distribution of predicted scores for each group shows that\nthe model is consistent in assigning these scores to the patients regardless of the fold and that the\npredicted scores are proportional to the progression time (see Figure S3).\nFor the MSK NSCLC datasets ( Samstein et al. [2019]), we have found that clinical transformer-based\nmodels performed signiﬁcantly better (Figure 3) than regularized CoxPH, XGBoost, DeepSurv, Hu et\nal Default (with original hyperparameters as listed in the paper) and Hu et al Adj (hyperparameters\nadjusted to closely match clinical transformer’s). The results from shufﬂed input features and target\nclinical endpoints (mean c-index of 0.51 ±0.097 and 0.48 ±0.092 respectively) demonstrate that\nclinical transformer recognizes the negative impact on performance when underlying input-output\nrelationships are signiﬁcantly changed. Clinical transformer without transfer learning (mean c-index\nof 0.59 ±0.094) showed signiﬁcantly worse performance compared to clinical transformer models\nwith transfer learning (mean c-indexes of 0.61 ±0.099 and 0.61 ±0.082). We noted insigniﬁcant\nreduction in performance for the clinical transformer with snapshot compared to the one without (P =\n0.56). However, the model without the snapshot (TL) demonstrated a bias towards those variables\nthat are related to the other cancer types when applied to NSCLC. This problem was mitigated\nby introducing a “snapshot” of NSCLC patients to the initial transfer learning phase (see Figure\n1 for more details). We chose clinical transformer (Transfer Learning with snapshot) as our main\nmodel and regularized multivariate CoxPH as the baseline model. The hazard ratios from the main\n5\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nFigure 3: Concordance index comparison between different models from 100 different training-test\nrandom splits. The c-index distribution from transfer learning clinical transformer with snapshot\n(TLS) is compared to the c-index from the other models using Mann Whitney U two-sided test.\nmodel were signiﬁcantly better (that is, lower values) (Figure 4) than those from CoxPH model\n(N=59 patients, Mann-Whitney P<0.0006). We also observed that the predicted scores from clinical\ntransformer showed better patient stratiﬁcation compared to scores from the CoxPH model (Figure\n4B and C). Clinical transformer model was also consistent in assigning scores to the 4 survival groups\n(Figure S3).\n4.2 Transfer learning based on cross-domain data improves the performance of\ndomain-speciﬁc transformers\nWe sought to test if transfer learning could improve the prediction of overall survival that was based\nonly on domain-speciﬁc data set. To train a cross-domain clinical transformer (Figure 2B) that is\nindependent of the data set utilized to train the domain-speciﬁc transformer, we used data from\nten different cancer types (N = 1266). The weights from this model were then used to initialize\na NSCLC cancer-speciﬁc transformer. (Figure 3) demonstrates that the clinical transformer with\nthe transfer learning is signiﬁcantly better than the clinical transformer without transfer learning\n(“clinical transformer with transfer learning” with or without a snapshot vs. “clinical transformer –\ndefault”, with a p-val < 0.0035 and p-val < 0.03 respectively (Table S2)). These ﬁndings indicate\nthe potential advantage of transfer learning to improve the ability to use the clinical transformers in\nclinical settings where a relatively small data set is available for analysis.\n4.3 Interpretation of transformer predictions derived from attention weights\nIn contrast to linear models (e.g. CoxPH), where the importance of independent variables can only\nbe explicitly derived, in models such as deep neural networks, that can be achieved only implicitly.\nHere, we demonstrate how the attention weights in the clinical transformer could be used to measure\nthe strength of association of the variables with other variables and their effect on the prediction of\nthe clinical outcome. To summarize the attention scores, we propose the Variable Interaction Score\n(VIS) that considers the interactions among the input features and the association with survival.\nThe VIS for a given population P and a variable f is deﬁned as the sum of all the top attention pairs\nbetween input features F normalized by the number of patients (P) across the attention heads H for\na given layer L:\n• Attention matrices in the layer Lare aggregated by heads H and population P to obtain a\nsingle matrix of averaged attention per population (or a single patient when P = 1).\n• Then top ten interactions scores from each input variable f are selected\n6\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nFigure 4: Evaluation of the predicted scores from clinical transformer and regularized cox model.\nA, Hazard ratios (HR) of predicted scores from the two models were computed for all the 100\ntraining-test random splits. The HRs were computed using the optimal cut-off that maximizes the\nseparation between high risk and low risk patients. Hazard ratios from clinical transformer were\nsigniﬁcantly lower than those from cox model (N=59 patients, Mann-Whitney P< 0.0006). B −C,\nPredicted scores from clinical transformer showed better patient stratiﬁcation compared to the scores\nfrom cox model. While the two models show that patients with low-risk scores tend to have longer\nsurvival, clinical transformer shows that this group has signiﬁcantly longer survival (HR=0.348, 95%\nCI:0.0169-0.716, log-rank P=0.0028). The two Kaplan Meier plots were generated from a single\nrandomly selected fold (see Figure S1 and S2 for more examples)\n• The VIS for variable f is the sum of these interaction scores.\nVISLPf = 1\nPH\nP∑\np\nH∑\nh\nAttLphf\nSince the VIS can be calculated at the single patient level, this metric can capture in addition to\nprevalent population of patients, a rare event – a small group of patients with a strong prognostic or\npredictive molecular or clinical characteristics.\n4.4 Feature importance recapitulates current knowledge\nTo understand the extent to which our clinical transformer captures known cancer biology, we ordered\nthe features used in the model by VIS (the measurement of importance described earlier in the text)\nand compared the top ones to known biology. (Figure 5) describes the importance of each variable in\nthe model. As expected, the most important gene according to VIS for response to immunotherapy is\nTP53. TP53 is a key member in the cell – it encodes the p53 protein which increases under cell stress\nand is involved in multiple biological pathways including senescence, DNA repair and apoptosis\n(Aubrey et al. [2016]). As results of deleterious mutations in TP53, cells in the body accumulate\nDNA mutations. This abnormal process increases the prevalence of neoantigen production which\nmay stimulates the immune response in the tumor microenvironment and thus introduce an advantage\nfor immune-therapy. It has been previously reported by Dong et al. [2017], Lin et al. [2020], that\nTP53 mutations can be used as a prognostic biomarkers for improved outcome in immune checkpoint\ninhibitors. Notably, the other top hits that came up based on the VIS metric have been proposed as\nbiomarkers for immunotherapy for example TMB (Greillier et al. [2018]), MGA (Sun et al. [2021])\nand KEAP1 (Papillon-Cavanagh et al. [2020]). Further, we examined whether the CoxPH and the\nclinical transformer models accorded similar levels of importance to the input features. Although the\ntwo models are different in terms of their underlying modelling assumptions and complexity level, we\nexpected to see some level of agreement in variable rankings, especially in well-known biomarkers in\nImmunotherapy. A summary of the comparison results is shown in Figure S4. We found a reasonable\nconcordance in the importance levels of the variables between the two models (Pearson correlation of\n0.53). Notably, the prominent features such as TMB ( Samstein et al. [2019]) and TP53 associated\nwith response to treatment for NSCLC exhibited a high rank in both models ( Lin et al. [2020]).\n7\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nFigure 5: Feature importance ranked by the median absolute magnitude of Variable Interaction\nScore (VIS). Clinical transformer ranks both TMB and TP53 among the most important variables,\nrecapitulating known ﬁndings on strong association between these features and survival outcomes for\nNSCLC patients. We denoted some variables with truncated name where necessary, otherwise, we\nkept the original notation as found in the datasets\n4.5 Attention weights interpretation reveals key biological insights\nFinally, we sought to test the possible connection of the attention score with the underlying tumor\nbiology considering the survival time of the patients. A common practice in biology is to explore\nthe patterns arising from data derived from biological experiments in different contexts (the overall\nsurvival time in this work) ( Brunet et al. [2004]). For example, mRNA gene expression measured\nacross many samples is used to calculate the correlation matrix between all gene pairs. This matrix\nis then used downstream to explore co-expression of genes through cluster analysis and can be\nassociated with the conditions of the performed experiments (that is, the context) ( Langfelder and\nHorvath [2008], Van Dam et al. [2018]).\nA primary interest in clinical trials is to identify biomarkers for a given treatment that can predict\nfast-progressor and supper-responders; the former since those patients may beneﬁt from a different\nclass of anticancer drugs; and the latter because those are the true intent-to-treat population that\nwould most beneﬁt from immunotherapy. As done earlier in this work, we split the patient population\ninto four ‘survival’ groups. Since these four groups respond differently to the treatment, biological\nfunctions that are known to be strongly associated with cancer progression may contribute to these\nsub-populations in different magnitude. Therefore, we grouped the genomic features (that is, the\ngenes) in our data based on their known function to two different key functionalities that are strongly\nrelated to cancer progression; onco-suppressors genes ( Repana et al. [2019]) and growth pathways\ngenes ( Reimand et al. [2019]). Furthermore, since the genes measured in our data sets, were pre-\nselected based on their strong association with cancer (see Figure 1 for further details), we included a\nset of all genes as another group in our analysis (Figures S8A). Lastly, we grouped three key clinical\nfeatures together to assess their differential interactions across the four patients’ sub-populations.\nAs expected, these four feature groups demonstrate monotonic decrease (Figure 6A and Figures\nS8A,S8B) or increase (Figure 6B) as a function of patient survival. We are currently exploring the\nincorporation of VIS into pathway interaction networks and other common practices in systems\nbiology.\n5 Summary\nIn this work we have introduced a utilization of the attention mechanism for clinical applications in\noncology. Cancer is a complex disease with wide variations across patients and many random events\nwhich accumulate in the lifetime of the disease. In addition, each patient has a different proﬁle, at the\nmolecular level (e.g., germline mutations) and physical (e.g., immune system, body mass and more).\nHere, we have demonstrated the ability of the clinical transformer to capture the semantics in patients’\n8\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nFigure 6: Attention weights reveal biological insights. Test population is divided into four intervals\naccording to their survival time as indicated in the main text. Self-attention score (attention that is\ngiven to the same variable, or the diagonal of the attention matrix) is represented by the color of the\nnodes (yellow: low attention, orange: high attention). Edges represent variable interactions and their\nthickness describes the level of VIS (described in the main text). Sizes of the node represent the\nrelative value of the input variable (e.g., number of mutations, TMB score).\nproﬁles in the context of their survival. We have shown the superiority of this approach over other\nmethods and the added value of using the interaction scores between features to bring system level\ninsights. Furthermore, using a variant of transfer learning in our model, an option that is not feasible\nin static ML methods like coxPH and others, we have demonstrated a signiﬁcant improvement over\nthe baseline clinical transformer. Hence, enabling better modelling of small cohorts in clinical studies.\n5.1 Limitations.\nAs it is well established, the dot-product attention operation inside the transformer uses O(n2) time\nand space making it computational expensive for long input feature spaces. Therefore, the main\nlimitation of the clinical transformer in its current implementation is the input feature dimension\nwith a hard cap of 300 features. However, alternative strategies such as the low rank matrix attention\nproposed in the LinFormer Wang et al. [2020] can be used to reduce the time and space complexity\nto O(n) in the clinical transformer. Moreover, cancer treatment outcomes can be affected by various\nfactors including patient ﬁtness, tumor stage, pharmacokinetics, pharmacodynamics, tumor microen-\nvironment and tumor molecular characteristic, among others. As in many clinical studies, including\nthe studies analyzed here, there were only a sub-set of clinical and mutational features available. We\nrecognize that these do not cover the complete underlying system complexity, which in many areas\nof biology is the case. Lastly, an important aspect in clinical trials is to ﬁnd predictive biomarkers - a\nfunction that changes the hazard rate depending on the treatment. Discovering predictive biomarkers\nrequires studies with at least two arms (one of which is a control group). In this work, we have\nanalyzed data comprising a single family of treatments that is not from a randomized study. Thus,\nour ﬁndings here could indicate a prognostic value rather then predictive.\n5.2 Broader impact.\nThe personalization of clinical treatment is increasingly becoming critical towards expanding the\ndelivery of optimum healthcare and enhancing the quality of life for individuals, particularly those\nwith severe illnesses that require pro-active diagnosis and management. The ability to automate\nsurvival prognosis at an early stage at a personalized level has been a key goal in precision medicine\nbecause the early identiﬁcation of potential trajectories of such patients and factors inﬂuencing their\noutcomes has vital ramiﬁcations towards optimal treatment planning. Algorithmic automation in\nthis regard lowers the barriers to entry for such precision medicine systems and potentially extends\nto point-of-care applications in the developing world as well. A potential source of risk towards\n9\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nwidespread deployment may stem from the demographic compositions in source datasets, and future\ntranslational studies would aim at curating geographically diverse datasets towards studying such\neffects comprehensively and mitigating any unwanted biases to that end.\n5.3 Availability\nCode will be available upon request to the authors\nReferences\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nSneha Chaudhari, Varun Mithal, Gungor Polatkan, and Rohan Ramanath. An attentive survey of\nattention models. arXiv preprint arXiv:1904.02874, 2019.\nMichael Widrich, Bernhard Schäﬂ, Hubert Ramsauer, Milena Pavlovi ´c, Lukas Gruber, Markus\nHolzleitner, Johannes Brandstetter, Geir Kjetil Sandve, Victor Greiff, Sepp Hochreiter, and Günter\nKlambauer. Modern hopﬁeld networks and attention for immune repertoire classiﬁcation, 2020.\nAhmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi, Yu Wang, Llion Jones, Tom\nGibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Towards cracking the\nlanguage of life’s code through self-supervised deep learning and high performance computing.\narXiv preprint arXiv:2007.06225, 2020.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V . Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations, 2020.\nZiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska,\nKyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene\nexpression prediction from sequence by integrating long-range interactions. bioRxiv, 2021.\nJohn Jumper, R Evans, A Pritzel, T Green, M Figurnov, K Tunyasuvunakool, O Ronneberger, R Bates,\nA Zidek, A Bridgland, et al. High accuracy protein structure prediction using deep learning.\nFourteenth Critical Assessment of Techniques for Protein Structure Prediction (Abstract Book), 22:\n24, 2020.\nFrank E Harrell Jr. Regression modeling strategies: with applications to linear models, logistic and\nordinal regression, and survival analysis. Springer, 2015.\nDavid R Cox. Regression models and life-tables. Journal of the Royal Statistical Society: Series B\n(Methodological), 34(2):187–202, 1972.\nValsamo Anagnostou, Noushin Niknafs, Kristen Marrone, Daniel C Bruhm, James R White, Jarushka\nNaidoo, Karlijn Hummelink, Kim Monkhorst, Ferry Lalezari, Mara Lanis, et al. Multimodal\ngenomic features predict outcome of immune checkpoint blockade in non-small-cell lung cancer.\nNature cancer, 1(1):99–111, 2020.\nSebastian Pölsterl. scikit-survival: A library for time-to-event analysis built on top of scikit-learn.\nJournal of Machine Learning Research, 21(212):1–6, 2020. URL http://jmlr.org/papers/\nv21/20-729.html.\nJared L Katzman, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval\nKluger. Deepsurv: personalized treatment recommender system using a cox proportional hazards\ndeep neural network. BMC medical research methodology, 18(1):1–12, 2018.\n10\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nSafoora Youseﬁ, Fatemeh Amrollahi, Mohamed Amgad, Chengliang Dong, Joshua E Lewis, Con-\ngzheng Song, David A Gutman, Sameer H Halani, Jose Enrique Velazquez Vega, Daniel J Brat,\net al. Predicting clinical outcomes from large scale cancer genomic proﬁles with deep survival\nmodels. Scientiﬁc reports, 7(1):1–11, 2017.\nShi Hu, Egill Fridgeirsson, Guido van Wingen, and Max Welling. Transformer-based deep survival\nanalysis. In Survival Prediction-Algorithms, Challenges and Applications, pages 132–148. PMLR,\n2021.\nLaura Maciejko, Munisha Smalley, and Aaron Goldman. Cancer immunotherapy and personal-\nized medicine: emerging technologies and biomarker-based approaches. Journal of molecular\nbiomarkers & diagnosis, 8(5), 2017.\nRobert M Samstein, Chung-Han Lee, Alexander N Shoushtari, Matthew D Hellmann, Ronglai\nShen, Yelena Y Janjigian, David A Barron, Ahmet Zehir, Emmet J Jordan, Antonio Omuro, et al.\nTumor mutational load predicts survival after immunotherapy across multiple cancer types. Nature\ngenetics, 51(2):202–206, 2019.\nHarald Steck, Balaji Krishnapuram, Cary Dehing-Oberije, Philippe Lambin, and Vikas C Raykar. On\nranking in survival analysis: Bounds on the concordance index. In Advances in neural information\nprocessing systems, pages 1209–1216. Citeseer, 2008.\nMatthias Schmid, Marvin N Wright, and Andreas Ziegler. On the use of harrell’s c for clinical risk\nprediction via random survival forests. Expert Systems with Applications, 63:450–459, 2016.\nAndreas Mayr and Matthias Schmid. Boosting the concordance index for survival data–a uniﬁed\nframework to derive and evaluate biomarker combinations. PloS one, 9(1):e84483, 2014.\nBrandon J Aubrey, Andreas Strasser, and Gemma L Kelly. Tumor-suppressor functions of the tp53\npathway. Cold Spring Harbor perspectives in medicine, 6(5):a026062, 2016.\nZhong-Yi Dong, Wen-Zhao Zhong, Xu-Chao Zhang, Jian Su, Zhi Xie, Si-Yang Liu, Hai-Yan Tu,\nHua-Jun Chen, Yue-Li Sun, Qing Zhou, et al. Potential predictive value of tp53 and kras mutation\nstatus for response to pd-1 blockade immunotherapy in lung adenocarcinoma. Clinical cancer\nresearch, 23(12):3012–3024, 2017.\nXinqing Lin, Liqiang Wang, Xiaohong Xie, Yinyin Qin, Zhanhong Xie, Ming Ouyang, and Chengzhi\nZhou. Prognostic biomarker tp53 mutations for immune checkpoint blockade therapy and its asso-\nciation with tumor microenvironment of lung adenocarcinoma. Frontiers in molecular biosciences,\n7, 2020.\nLaurent Greillier, Pascale Tomasini, and Fabrice Barlesi. The clinical utility of tumor mutational\nburden in non-small cell lung cancer. Translational lung cancer research, 7(6):639, 2018.\nLei Sun, Man Li, Ling Deng, Yuchun Niu, Yichun Tang, Yu Wang, and Linlang Guo. Mga mutation\nas a novel biomarker for immune checkpoint therapies in non-squamous non-small cell lung cancer.\nFrontiers in pharmacology, 12:564, 2021.\nSimon Papillon-Cavanagh, Parul Doshi, Radu Dobrin, Joseph Szustakowski, and Alice M Walsh.\nStk11 and keap1 mutations as prognostic biomarkers in an observational real-world lung adenocar-\ncinoma cohort. ESMO open, 5(2):e000706, 2020.\nJean-Philippe Brunet, Pablo Tamayo, Todd R Golub, and Jill P Mesirov. Metagenes and molecular\npattern discovery using matrix factorization. Proceedings of the national academy of sciences, 101\n(12):4164–4169, 2004.\nPeter Langfelder and Steve Horvath. Wgcna: an r package for weighted correlation network analysis.\nBMC bioinformatics, 9(1):1–13, 2008.\nSipko Van Dam, Urmo V osa, Adriaan van der Graaf, Lude Franke, and Joao Pedro de Magalhaes.\nGene co-expression analysis for functional classiﬁcation and gene–disease predictions. Brieﬁngs\nin bioinformatics, 19(4):575–592, 2018.\n11\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint \nDimitra Repana, Joel Nulsen, Lisa Dressler, Michele Bortolomeazzi, Santhilata Kuppili Venkata,\nAikaterini Tourna, Anna Yakovleva, Tommaso Palmieri, and Francesca D Ciccarelli. The network\nof cancer genes (ncg): a comprehensive catalogue of known and candidate cancer genes from\ncancer sequencing screens. Genome biology, 20(1):1–12, 2019.\nJüri Reimand, Ruth Isserlin, Veronique V oisin, Mike Kucera, Christian Tannus-Lopes, Asha Rostami-\nanfar, Lina Wadi, Mona Meyer, Jeff Wong, Changjiang Xu, et al. Pathway enrichment analysis\nand visualization of omics data using g: Proﬁler, gsea, cytoscape and enrichmentmap. Nature\nprotocols, 14(2):482–517, 2019.\nSinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\n12\n . CC-BY-NC 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 14, 2021. ; https://doi.org/10.1101/2021.10.11.21264761doi: medRxiv preprint ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.576158881187439
    },
    {
      "name": "Computer science",
      "score": 0.5003988742828369
    },
    {
      "name": "Lung cancer",
      "score": 0.47760650515556335
    },
    {
      "name": "Sentence",
      "score": 0.4728144705295563
    },
    {
      "name": "Medicine",
      "score": 0.4462328851222992
    },
    {
      "name": "Immunotherapy",
      "score": 0.4260687828063965
    },
    {
      "name": "Clinical trial",
      "score": 0.4170834422111511
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40139150619506836
    },
    {
      "name": "Oncology",
      "score": 0.36336028575897217
    },
    {
      "name": "Machine learning",
      "score": 0.34839117527008057
    },
    {
      "name": "Internal medicine",
      "score": 0.32616597414016724
    },
    {
      "name": "Cancer",
      "score": 0.20761311054229736
    },
    {
      "name": "Engineering",
      "score": 0.14855673909187317
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}