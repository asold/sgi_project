{
    "title": "An Experimental Evaluation of Transformer-based Language Models in the Biomedical Domain",
    "url": "https://openalex.org/W3120441516",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5071772870",
            "name": "Paul Grouchy",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5103707005",
            "name": "Shobhit Jain",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100333916",
            "name": "Michael C. Liu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5102384978",
            "name": "Kuhan Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5091716693",
            "name": "Max Tian",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5089267565",
            "name": "Nidhi Arora",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5034225316",
            "name": "Hillary Ngai",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5009939236",
            "name": "Faiza Khan Khattak",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5061175035",
            "name": "Elham Dolatabadi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5026969522",
            "name": "Sedef Akinli Koçak",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2912817604",
        "https://openalex.org/W2982424689",
        "https://openalex.org/W2924690340",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2887872604",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2947567669",
        "https://openalex.org/W2963923670",
        "https://openalex.org/W2969740599",
        "https://openalex.org/W2071879021",
        "https://openalex.org/W2970636124",
        "https://openalex.org/W2626667877",
        "https://openalex.org/W2427527485",
        "https://openalex.org/W2081056190",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2974058876",
        "https://openalex.org/W2169099542",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W2048296798",
        "https://openalex.org/W2987154291",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2953773463",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W2921848006"
    ],
    "abstract": "With the growing amount of text in health data, there have been rapid advances in large pre-trained models that can be applied to a wide variety of biomedical tasks with minimal task-specific modifications. Emphasizing the cost of these models, which renders technical replication challenging, this paper summarizes experiments conducted in replicating BioBERT and further pre-training and careful fine-tuning in the biomedical domain. We also investigate the effectiveness of domain-specific and domain-agnostic pre-trained models across downstream biomedical NLP tasks. Our finding confirms that pre-trained models can be impactful in some downstream NLP tasks (QA and NER) in the biomedical domain; however, this improvement may not justify the high cost of domain-specific pre-training.",
    "full_text": "AN EXPERIMENTAL EVALUATION OF TRANSFORMER -BASED\nLANGUAGE MODELS IN THE BIOMEDICAL DOMAIN\nPaul Grouchy∗\nUntether AI\nShobhit Jain∗\nManulife\nMichael Liu∗\nTealbook\nKuhan Wang∗\nCIBC\nMax Tian∗\nAdeptmind\nNidhi Arora∗\nIntact\nHillary Ngai∗\nUniversity of Toronto\nFaiza Khan Khattak†\nManulife\nElham Dolatabadi‡\nVector Institute\nSedef Akinli Kocak§\nVector Institute\nJanuary 1, 2021\nABSTRACT\nWith the growing amount of text in health data, there have been rapid advances in large pre-trained\nmodels that can be applied to a wide variety of biomedical tasks with minimal task-speciﬁc modi-\nﬁcations. Emphasizing the cost of these models, which renders technical replication challenging,\nthis paper summarizes experiments conducted in replicating BioBERT and further pre-training and\ncareful ﬁne-tuning in the biomedical domain. We also investigate the effectiveness of domain-speciﬁc\nand domain-agnostic pre-trained models across downstream biomedical NLP tasks. Our ﬁnding\nconﬁrms that pre-trained models can be impactful in some downstream NLP tasks (QA and NER) in\nthe biomedical domain; however, this improvement may not justify the high cost of domain-speciﬁc\npre-training.\nKeywords BioBERT ·Biomedical data ·NLP downstream tasks ·Transformer-based models\n1 Introduction\nThere have been increased exploration and extension of transformer-based deep learning models for NLP [1, 2, 3, 4]\nrecently. Using transformer-based models, one can pre-train a deep learning model on large datasets and then easily\nﬁne-tune it to adapt to downstream NLP tasks. There are three factors that affect the performance of these models:\n(a) the size of the dataset, (b) the availability of computational resources, and (c) the expressiveness of the model\narchitecture [2, 1]. Because of these factors, the cost and complexity of developing pre-trained models are rising quickly\nand limit the capability of reproducing results when sufﬁcient resources are not available.\nThese language models are trained on text corpora of general domains. For example, BERT [3], Bidirectional Encoder\nRepresentations from Transformers has been trained on Wikipedia and BooksCorpus. There has also been a rapid\ngrowth in NLP for the biomedical domain [5] and many new methods including transformer-based methods are being\nused for different biomedical tasks involving NLP.\nThe performance of language models that have been trained on general domains are not yet fully investigated in\nmore speciﬁc domains such as biomedical, ﬁnance or legal. Therefore, it is worth investigating if large amounts of\ndomain-speciﬁc data may help in getting better results, or if similar results may be acquired by using smaller-sized data.\nWe therefore focus on biomedical domain in order to answer the following questions:\n∗Equal Contribution\n†Corresponding Author: faizakhankhattak@gmail.com\n‡Corresponding Author: elham.dolatabadi@vectorinstitute.ai\n§Corresponding Author: sedef.kocak@vectorinstitute.ai\narXiv:2012.15419v1  [cs.CL]  31 Dec 2020\nA PREPRINT - JANUARY 1, 2021\n1. Does domain-speciﬁc training improve performance compared to baseline models trained on domain-agnostic\ncorpora?\n2. Is it possible to obtain comparable results from a domain-speciﬁc BERT model pre-trained on smaller-sized\ndata? While it is established fact that with transfer learning, a model can be trained once and be reused for\nseveral tasks but there are a few cases when the model needs to be retrained. For example, when the data\nis dynamic and may change over time due to the data-shift; data can belong to a wide variety of domains,\ntherefore the model may need to be retrained for the new domain data or even sub-domain; data can be\nconﬁdential to an organization hence the model needs to be retrained for that organization-speciﬁc needs.\nThe domain and/or organization speciﬁc datasets can be very small. Moreover, not every organization has\nextensive computational resources required to train large models. In such cases it is helpful to know if a small\ndomain-speciﬁc data can be used to get comparable results.\nThe rest of this paper is organized as follows. We review the existing studies related to our work in Section 2. We\nexplain our pre-training experiments and the results in Section 3.1 and ﬁne-tuning experiments and results in Section\n3.2. We complete our paper with discussing and conclusion in Section 4 where we discuss our results and answering\nour questions.\n2 Related Work\nLarge-scale pre-trained language models such as BERT[3], GPT-2 [6], RoBERTa [2] and GPT-3 [7] have shown to\noutperform state-of-the-art performance in many NLP tasks such as Named Entity Recognition (NER) and Question\nAnswering (QA). Moreover, several studies have used transfer learning and ﬁne-tuning of these models on English NLP\ntasks (e.g., [ 8, 9]). These language models are trained on text corpora of general domains but recently there has been a\ntrend of training language models on the domain-speciﬁc data. For example, ﬁnancial version of BERT was introduced\nby Araci [10] where he only studied sentiment classiﬁcation task. Ma et al [11] ﬁne-tuned BERT on legal documents\ncoming from their proprietary corpus. BioBERT [4] was introduced as an extension of BERT that is further pre-trained\non the domain-speciﬁc biomedical corpora including PubMed and PubMed Central (PMC).\nIn specialized domains like biomedical, recent work has shown that using domain speciﬁc data can provide improvement\nover general-domain language models [12]. In this regard, Wang et al. [13] showed that word-embeddings trained on\nbiomedical corpora captured the semantics of medical terms better than those trained on general domain corpora, but\nmay not generalize well to downstream biomedical NLP tasks such as biomedical information retrieval. Zhao et al [14]\nshowed that word2vec [15] trained on a smaller and in-domain medical data resulted in better performance than the\nword2vec trained on a large and general domain dataset. Also, [16] found that performance decreases after 4 million\ndistinct words of training data based on experiments with medical data from PubMed abstracts5. In a recent study, Gu\net al [12] introduced PubMedBERT. They pre-trained the BERT from scratch with PubMed articles and a customized\nvocabulary (constructed from the PubMed articles). This study indicates that a proper vocabulary helps the performance\nof downstream tasks in speciﬁc domains. However, training the model from scratch is extremely expensive in terms of\ndata and computation. Researchers have speciﬁcally built adaptations of BERT that attempt to address different domain\nrelated problems but the most effective pretraining process remains an open research problem [12]. We replicated some\nof the BioBERT original study results, and better tune the training of BioBERT for better understanding domain speciﬁc\ntraining.\n3 Methods and Experimental Results\nWe set BERTBASE as our baseline model. We started with BERT BASE, then pre-trained it on the PubMed abstracts\ndata (BERTbase+PM) and leveraged for evaluation of downstream tasks: Name-entity recognition (NER, relational\nextraction (RE) (Table 1) and question answering (QA) (Table 2). This allows us to conduct a fair comparison between\ndomain-speciﬁc pre-training and ﬁne-tuning. PyTorch implementation of BERT6 [17] was leveraged and the replication\nexperiments were conducted based on the work by McDermott et. al [18].\n3.1 Pre-training language representations in the biomedical domain\nThe PubMed corpora was used for pre-training consists of paper abstract from millions of samples of biomedical text.\nWhile the original BioBERT study considers combined pre-training on PubMed, PMC, and Pubmed+PMC together,\n5https://www.ncbi.nlm.nih.gov/pubmed/\n6https://github.com/huggingface/pytorch-transformers\n2\nA PREPRINT - JANUARY 1, 2021\nour model was pre-trained only on PubMed [19] to check the performance based on smaller data and meet the shared\ncomputing resources.\nThe PubMed data was processed into a format amenable for pre-training. The raw data consisted of approximately 200\nmillion sentences in 30 GBs. The raw sentence data was batch processed into 111 chunks of ready to consume input\ndata for BERT pre-training7. Technically, the NSP task was lower-cased sentences with a maximum length of 512 and\nmasked at the sub-token level analogous to the original BERTBASE(uncased). The original BERT models were trained\non data with maximum sequence lengths of 128 for the initial 90% of the training and 512 for the remainder. The full\ntraining loss is shown in Figure 1. Generally, a longer sequence length is preferable if the corpora tends to have longer\nFigure 1: Training loss. The effect of chunking the data into shards is seen in the ﬂuctuation of the training loss. The\nvertical line indicates separation between batch size 28 and 14 in the training paradigm. The top (bottom) x-axis indicate\ncalendar date (global step).\npassages but the amount of data and time to train is also increased. In this experiment, the training was maintained at\nthe maximum length of 512 tokens throughout.\nThe pre-training experiment was designed to accommodate some realistic computing resource limitations. In a time-\nshared computing cluster GPU resources were allocated on limited priority basis by Slurm Workload Manager. In order\nto facilitate stable training and to accommodate a hardware environment where resources may be reallocated to higher\npriority users at any time and without warning, the following training features were implemented: (1) storing model\nweights and gradients at regular time intervals during training; (2) querying and automating job submission from system\ntask scheduler; (3) automatically restoring training from data chunk and step as GPU resources became available.\n3.2 Fine-tuning language representations in the biomedical domain\nFollowing previous work, our domain speciﬁc pre-trained model (BERTbase+PM) as well as BERTBASE(uncased) were\nevaluated on common biomedical NLP tasks as described below.\n3.2.1 Named-entity recognition (NER)\nThe biomedical NER task involves the extraction of biomedical named entities such as genes, proteins, diseases and\nspecies from unstructured biomedical text. This is a challenging task because of the unique characteristics of biomedical\nnamed entities such as sharing of head nouns, spelling forms per entity, ambiguous abbreviations, descriptive naming\nconvention, and nested names [20]. In the original BioBERT study nine different biomedical NER datasets were tested\nand their best model (BERTBASE+PM+PMC) was able to outperform state-of-the-art on the majority. In this experiment,\nthree NER datasets including NCBI Disease [ 21], JNLPBA [22], and Species-800 [23] were used. The model was\n7This implies that the sampling of the second sentence for the Next Sentence Prediction (NSP) training task is not from the entire\ncorpus but from within its respective chunk. As each chunk is still close to 2 million separate sentences, this was judged to be an\nacceptable compromise.\n3\nA PREPRINT - JANUARY 1, 2021\ntrained on 50 tokens long sentences with a learning rate of 5x10−5 . These experiments were repeated several times\nand F1 scores corresponding to the epoch with the minimum validation loss are reported in Table 1. The standard\ndeviation in F1 scores across different runs make deﬁnitive comparisons with the results reported in BioBERT study,\nbut a surprising outcome is that the scores for the BERTBASE starting point models are higher on average than the ones\nﬁne-tuned with our BERTbase+PM. This runs counter to the notion that pre-training on biomedical domains corpora\nimproves downstream NER.\n3.2.2 Relation Extraction (RE)\nThe GAD dataset [24] was used for the RE experiment. The goal of RE on the GAD dataset is to correctly classify\nwhether a gene and disease are related, based on a given sentence. Two models were ﬁne-tuned on GAD, one starting\nwith BERTBASE and the other starting with BERTbase+PM. A learning rate of 5x10−5 was used for all runs. The best\nvalidation F1 score over several runs for each fold is reported. Our BERTbase+PM outperformed other BERT models\nbut the differences between models are not signiﬁcant (see Table 1).\nBioBERT [4] Our Experiments\nTasks Datasets Metrics BERT base +PubMed BERT base BERTbase+PM\nNER\nNCBI Disease F1 85.63 89.71 84.08±2.07 80.33 ±2.40\nJNLPBA F1 74.94 77.49 76.43±2.29 75.16 ±3.63\nSpecies 800 F1 71.63 74.06 78.38±7.87 74.59±5.28\nRE GAD F1 79.30 79.83 79.60 81.50\nTable 1: NER and RE Performance Results. Our experiments including ﬁne-tuning of the BERTBASE(uncased) model\nand the BERTBASE(uncased) model further pre-trained on PubMed abstracts (+PM). For comparison, the results of\nBioBERT study using BERTBASE (i.e. Wiki+Books) and the BioBERT version of the PubMed trained model (+PubMed)\nare also shown. Best scores are shown in bold.\n3.2.3 Question Answering (QA)\nThe goal of QA tasks is to automatically ﬁnd the answer to a question posed in human language, usually from a context\nparagraph. In this study, we explored the performance of BERT and BioBERT on two common biomedical QA tasks\nincluding BioASQ [25] and PubmedQA [26]. For this task, three versions of BERT model (BERTlarge, BERTbase, and\nour BERTbase+PM) were ﬁne-tuned on the QA datasets.\nBioASQ: As was suggested in the BioBERT study [4], all BERT models were initially ﬁne-tuned on the SQuAD [27]\ndataset (with intermediate evaluations), and then on the BioASQ training set before ﬁnally evaluating on the BioASQ\ntest sets (Table 2). It has been also shown in [28] that pre-training BERT on SQuAD 1.1 generated better results when\nﬁne-tuned on BioASQ in comparison to the model pre-trained on SQuAD 2.0. For tuning the model on SQuAD 1.1\nand SQuAD 2.0, we took inspiration from the training schemes outlined in [2] and [29] to adjust the hyperparameters,\nnamely the learning rate, β2, learning rate schedule, batch size, and number of training epochs. We found that training\nwith a cosine learning rate schedule with no warm-up steps withβ2 = 0.98 consistently resulted in the best performance,\nand thus carried over the heuristics generated from ﬁne-tuning on SQuAD to further ﬁne-tune our models on BioASQ.\nTraining on domain-speciﬁc data: Overall, there is some evidence (Table 2) that pre-training on biomedical domain\ncorpus improves performance on the downstream BioASQ QA task. However, the improvement is not so large as to be\nentirely convincing and carefully ﬁne-tuned BERT models (such as BERTlarge in our case) can perform comparably to\nBioBERT.\nZero-shot setting: We also conducted an experiment in order to evaluate the performance of the BERT models on\nBioASQ datasets in a zero-shot setting where BERTBASE was ﬁne-tuned on SQuAD and not on BioASQ training data.\nWe averaged the scores over the 5 test sets within Strict Accuracy (S), Lenient Accuracy (L), and Mean Reciprocal Rank\n(M) metrics and the results were 31.37, 46.8, and 37.16, respectively. As we can see the zero-shot evaluation results on\ntest sets are worse than the results indicated in Table 2 which emphasizes the effects of ﬁne-tuning on BioASQ.\nNumber of epochs: Additionally, we found that ﬁne-tuning BERTlarge model on BioASQ for more epochs than the\ntypically recommended 1-3 epochs resulted in much better results. Our best results across the three evaluation metrics\non BioASQ came from ﬁne-tuning BERTlarge for 20 epochs and BERTbase for 4 epochs with initial learning rate of\n5x10−6.\nPubMedQA: All three versions of the BERT models were ﬁne-tuned on PQA-L i.e., 1k expert-annotated generated\nQA instances of the PubMedQA dataset with the results shown in Table 2. Since PubMedQA hasn’t been experimented\n4\nA PREPRINT - JANUARY 1, 2021\nBioBERT [4] Our Experiments\nDatasets Metrics BERTbase +PubMed BERTlarge∗ BERTbase BERTbase+PM\nBioASQ (4b)\nS 27.33 27.95 31.8 31.2 31.54\nL 44.72 44.10 51.8 44.58 48.36\nM 33.77 34.72 40.0 36.25 38.39\nBioASQ (5b)\nS 39.33 46.00 43.0 41.61 46.05\nL 52.67 60.00 55.8 56.8 57.94\nM 44.27 51.64 48.2 47.86 50.54\nBioASQ (6b)\nS 33.54 42.86 35.8 36.55 37.57\nL 51.55 57.77 54.4 59.3 58.87\nM 40.88 48.43 43.4 47.59 46.5\nAverage S 33.4 38.93 36.86 37.48 38.39\nover L 49.65 52.68 55.12 53.56 55.06\n4b,5b, and 6b M 39.6 44.93 43.86 43.9 45.14\nPubMedQA K-Fold Acc — 57.28 56.52 55.20 56.20\nK-Fold F1 — 28.70 26.14 23.71 23.98\nTable 2: The performance results of three versions of BERT model (BERT large, BERTbase and BERTbase+PM) on\ntwo distinct QA datasets. For comparison, the results of BioBERT study using BERTBASE (i.e. Wiki+Books) and the\nBioBERT version of the PubMed trained model (+PubMed) are also shown. Since, we already explored latest versions\nof BERTlarge and BERTbase, we didn’t examine the BERTbase version used in the BioBERT study. Best scores are shown\nin bold. *Used BERTlarge as other versions were not providing good results.\nin the BioBERT study, in order to make a fair comparison between our study and the BioBERT study [ 4], we ran\nan additional experiment where we ﬁne-tuned PubMed trained model (+PubMed) from the BioBERT study on the\nPubMedQA. 10-fold cross-validation was performed with only 450 training instances in each fold of validation. As seen\nin Table 2, the BioBERT version of the PubMed trained model (+PubMed) has the highest accuracy 57.28 and F1 Score\n28.27. This suggests that pre-training on biomedical corpus improves the performance of the downstream PubMedQA\ntask. However, the performance improvement compared to BERTlarge, BERTbase, and BERTbase+PM is minimal.\n3.2.4 Text Summarization\nText summarization refers to automatic generation of summary of a given text. Extractive summarization is done\nby extracting the most important sentences from the document that summarize the whole document. Abstractive\nsummarization refers to condensing the document into shorter versions while preserving its meaning [30].\nCNN/DailyMail XSum BioASQ 7b (Our experiments)\n# docs (train/val/test) 196,961/ /12,148/10,397 204,045/11,332/11,334 22,462/4,814/4,800\nROUGE-1 ROUGE-2 ROUGE-3 ROUGE-1 ROUGE-2 ROUGE-3 ROUGE-1 ROUGE-2 ROUGE-3\nAbs 41.72 19.39 38.76 38.81 16.33 31.15 38.20 27.12 34.15\nExtAbs 42.13 19.6 39.18 38.76 16.50 31.27 38.89 29.29 36.01\nExt 43.25 20.24 39.63 † † † 33.30 26.50 32.20\nTable 3: Text summarization F1 score. †Results were poor hence were not reported by the authors [31]\nBERTSum [32, 31] is a simple variant of BERT, for text summarization that produces abstractive and extractive\nsummaries. BERTSumAbs is based on an encoder-decoder architecture where the encoder is pre-trained whereas\ndecoder is a randomly initialized transformer which is trained from scratch. There are three versions of BERTSum.\nBERTSumAbs produces abstractive summary, BERTSumExt outputs extractive summary, and BERTSumExtAbs also\nproduces abstractive summary but is based on two-stage approach where the encoder is ﬁne-tuned twice, ﬁrst with an\nextractive objective followed by an abstractive one.\nWe explored the performance of these methods on BioASQ 7b8 dataset consisting of links to research papers as well as\nsummaries are embedded in the json ﬁle itself. BERT was trained from scratch on BioASQ 7b. The results are shared in\nTable 3. For CNN/DailyMail dataset all three methods outperform on BioASQ dataset according to ROUGE-2 score,\n8http://bioasq.org/participate/challenges_year_7\n5\nA PREPRINT - JANUARY 1, 2021\nwhile produce comparable results in all other cases. For XSum dataset [33], we have comparable results according to\nROUGE-1, while BioASQ 7b performs better according to ROUGE-2 and ROUGE-3 (for Ext summary results were\nnot reported by the authors [31]). This maybe due to the fact that BioASQ dataset is much smaller in size as compared\nto the other two datasets. This also shows that smaller-sized dataset can be used for text-summarization.\n4 Discussion and Conclusion\nIn this study, we present our ﬁndings for experiments conducted in conjunction with further pre-training of BERT\nmodel in the biomedical domain as well as evaluating both the domain speciﬁc and domain agnostic pre-trained models\nacross downstream biomedical NLP tasks. Our experiments also included considering data and optimization related\nfactors. Further pre-training was conducted on PubMed abstracts only to evaluate the performance of the models trained\non smaller-size datasets and also the effects of learning rate was carefully evaluated for ﬁne-tuning tasks.\n(1) Does domain-speciﬁc training improves performance? This study conﬁrms that unsupervised pre-training in general\ncould improve the performance on ﬁne-tuning tasks. However, the effectiveness of domain speciﬁc pre-training as\na way of further improving the performance of supervised downstream tasks does not signiﬁcantly outperform the\neffectiveness of domain agnostic pre-trained models considering the high cost of domain-speciﬁc pre-training which\nmakes it challenging for most of researchers and NLP developers. In the biomedical domain, however, this conclusion\nmay not be wholly substantiated owing to a lack of consistent evidence particularly in downstream NER, QA, and Text\nSummarization tasks. For the SQuAD task, it has been shown that ﬁne-tuning results depend on the size and duration of\ntraining [34] [35]. However, given the small size of the biomedical QA (BioASQ) datasets, it is not possible to run the\nsame experiments, as the current pre-trained models easily overﬁt. Therefore, it should be beneﬁcial for the biomedical\ncommunity to curate and expand the magnitude of benchmark datasets. Understandably it is difﬁcult and expensive, but\nwith the ever increasing size of the deep-learning models and signiﬁcant advances in the development of pre-training\nlanguage representations, it is necessary in order to facilitate reproducible research for health.\n(2) Is it possible to obtain comparable results using BERT model pre-trained on smaller-sized data? We present results\nof the experiments that were conducted using the models pre-trained on the PubMed abstracts only. These results were\ncomparable to the results produced by the model trained on the PubMed, PMC, and Pubmed+PMC together (please\ncheck [ 4]). Although it requires further investigation but empirically it shows that small-sized datasets may be used\nas a surrogate. Using small-sized dataset can be especially useful when the models need to retrained instead of using\npre-trained publicly available models due to (a) data-shift, (b) wide variety of data-domains, (c) conﬁdential data not\npublicly available to train the model on, (d) small-size of the domain speciﬁc data available, and (e) a lack of computing\nresources.\nWe position these experiments as complementary to existing literature on the applications of transformer-based models\nfor biomedical NLP. Our results provide some evidence for the validity and the limitations of existing language\nrepresentations for pre-training & ﬁne-tuning in biomedical domain.\nWe would like to expand our experiments on other variants of BERT, with more domain speciﬁc datasets and a variety\nof downstream tasks. This would also allow us to use our experiments for other medical corpora9 for external validation\nand generalization of the results.\nAcknowledgements\nWe want to thank Vector Institute industry sponsors, researchers and technical staff who participated in the Vector\nInstitute’s NLP Project (https://vectorinstitute.ai/wp-content/uploads/2020/12/nlp-report-final.\npdf).\nReferences\n[1] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\n[2] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n9http://www.nactem.ac.uk/resources.php\n6\nA PREPRINT - JANUARY 1, 2021\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[4] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\nBiobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics,\n36(4):1234–1240, 2020.\n[5] Faiza Khan Khattak, Serena Jeblee, Chloé Pou-Prom, Mohamed Abdalla, Christopher Meaney, and Frank Rudzicz.\nA survey of word embeddings for clinical text. Journal of Biomedical Informatics: X, page 100057, 2019.\n[6] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv\npreprint arXiv:2005.14165, 2020.\n[8] Matthew E Peters, Sebastian Ruder, and Noah A Smith. To tune or not to tune? adapting pretrained representations\nto diverse tasks. arXiv preprint arXiv:1903.05987, 2019.\n[9] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end\nopen-domain question answering with bertserini. arXiv preprint arXiv:1902.01718, 2019.\n[10] Dogu Araci. Finbert: Financial sentiment analysis with pre-trained language models. arXiv preprint\narXiv:1908.10063, 2019.\n[11] Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. Domain adaptation with bert-based\ndomain classiﬁcation and data selection. In Proceedings of the 2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019), pages 76–83, 2019.\n[12] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. Domain-speciﬁc language model pretraining for biomedical natural language processing.\narXiv preprint arXiv:2007.15779v3, 2020.\n[13] Yanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul Kingsbury,\nand Hongfang Liu. A comparison of word embeddings for the biomedical natural language processing. Journal of\nbiomedical informatics, 87:12–20, 2018.\n[14] Mengnan Zhao, Aaron J Masino, and Christopher C Yang. A framework for developing and evaluating word\nembeddings of drug-named entity. In Proceedings of the BioNLP 2018 workshop, pages 156–160, 2018.\n[15] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector\nspace. arXiv preprint arXiv:1301.3781, 2013.\n[16] Yongjun Zhu, Erjia Yan, and Fei Wang. Semantic relatedness and similarity of biomedical terms: examining the\neffects of recency, size, and section of biomedical publications on the performance of word2vec. BMC medical\ninformatics and decision making, 17(1):95, 2017.\n[17] Hugging Face. A library of state-of-the-art pretrained models for natural language processing (nlp). https:\n//github.com/huggingface/pytorch-transformers, 2019.\n[18] Matthew McDermott, Shirly Wang, Nikki Marinsek, Rajesh Ranganath, Marzyeh Ghassemi, and Luca Foschini.\nReproducibility in machine learning for health. arXiv preprint arXiv:1907.01463, 2019.\n[19] US National Library of Medicine National Institute of Health. Pubmed, pubmed central, (accessed June, 2019).\nhttps://www.ncbi.nlm.nih.gov/pubmed/.\n[20] Amy Neustein, S Sagar Imambi, Mário Rodrigues, António Teixeira, and Liliana Ferreira. Application of text\nmining to biomedical knowledge extraction: analyzing clinical narratives and medical literature. Text mining of\nweb-based medical content. De Gruyter, Berlin, 50, 2014.\n[21] Rezarta Islamaj Do ˘gan, Robert Leaman, and Zhiyong Lu. Ncbi disease corpus: a resource for disease name\nrecognition and concept normalization. Journal of biomedical informatics, 47:1–10, 2014.\n[22] Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka Tateisi, and Nigel Collier. Introduction to the bio-entity\nrecognition task at jnlpba. In Proceedings of the international joint workshop on natural language processing in\nbiomedicine and its applications, pages 70–75. Citeseer, 2004.\n[23] Evangelos Paﬁlis, Sune P Frankild, Lucia Fanini, Sarah Faulwetter, Christina Pavloudi, Aikaterini Vasileiadou,\nChristos Arvanitidis, and Lars Juhl Jensen. The species and organisms resources for fast and accurate identiﬁcation\nof taxonomic names in text. PLoS One, 8(6):e65390, 2013.\n7\nA PREPRINT - JANUARY 1, 2021\n[24] Kevin G Becker, Kathleen C Barnes, Tiffani J Bright, and S Alex Wang. The genetic association database. Nature\ngenetics, 36(5):431, 2004.\n[25] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R\nAlvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et al. An overview of\nthe bioasq large-scale biomedical semantic indexing and question answering competition. BMC bioinformatics,\n16(1):138, 2015.\n[26] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: A dataset for\nbiomedical research question answering. arXiv preprint arXiv:1909.06146v1, 2019.\n[27] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine\ncomprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[28] Wonjin Yoon, Jinhyuk Lee, Donghyeon Kim, Minbyul Jeong, and Jaewoo Kang. Pre-trained language model for\nbiomedical question answering. arXiv preprint arXiv:1909.08229, 2019.\n[29] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A\nlite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n[30] Vishal Gupta and Gurpreet Singh Lehal. A survey of text summarization extractive techniques. Journal of\nemerging technologies in web intelligence, 2(3):258–268, 2010.\n[31] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. arXiv preprint arXiv:1908.08345,\n2019.\n[32] Yang Liu. Fine-tune bert for extractive summarization. arXiv preprint arXiv:1903.10318, 2019.\n[33] Shashi Narayan, Shay B Cohen, and Mirella Lapata. Don’t give me the details, just the summary! topic-aware\nconvolutional neural networks for extreme summarization. arXiv preprint arXiv:1808.08745, 2018.\n[34] Alon Talmor and Jonathan Berant. Multiqa: An empirical investigation of generalization and transfer in reading\ncomprehension. CoRR, abs/1905.13453, 2019.\n[35] Georg Wiese, Dirk Weissenborn, and Mariana L. Neves. Neural domain adaptation for biomedical question\nanswering. CoRR, abs/1706.03610, 2017.\n8"
}