{
  "title": "Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems",
  "url": "https://openalex.org/W3049346316",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222272762",
      "name": "Madotto, Andrea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2358019462",
      "name": "Liu Zihan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202063432",
      "name": "Lin, Zhaojiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743188307",
      "name": "Fung, Pascale",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3044597194",
    "https://openalex.org/W3007894275",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3033415048",
    "https://openalex.org/W3020629500",
    "https://openalex.org/W2945978556",
    "https://openalex.org/W2950765829",
    "https://openalex.org/W3030754432",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3189817881",
    "https://openalex.org/W2964588180",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3020827637",
    "https://openalex.org/W3016625483",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2803609229",
    "https://openalex.org/W2948727657",
    "https://openalex.org/W3101203785"
  ],
  "abstract": "Task-oriented dialogue systems use four connected modules, namely, Natural Language Understanding (NLU), a Dialogue State Tracking (DST), Dialogue Policy (DP) and Natural Language Generation (NLG). A research challenge is to learn each module with the least amount of samples (i.e., few-shots) given the high cost related to the data collection. The most common and effective technique to solve this problem is transfer learning, where large language models, either pre-trained on text or task-specific data, are fine-tuned on the few samples. These methods require fine-tuning steps and a set of parameters for each task. Differently, language models, such as GPT-2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020), allow few-shot learning by priming the model with few examples. In this paper, we evaluate the priming few-shot ability of language models in the NLU, DST, DP and NLG tasks. Importantly, we highlight the current limitations of this approach, and we discuss the possible implication for future work.",
  "full_text": "Language Models as Few-Shot Learner for\nTask-Oriented Dialogue Systems\nAndrea Madotto, Zihan Liu, Zhaojiang Lin, Pascale Fung\nCenter for Artiﬁcial Intelligence Research (CAiRE)\nThe Hong Kong University of Science and Technology\namadotto@connect.ust.hk\nAbstract\nTask-oriented dialogue systems use four con-\nnected modules, namely, Natural Language\nUnderstanding (NLU), a Dialogue State Track-\ning (DST), Dialogue Policy (DP) and Natural\nLanguage Generation (NLG). A research chal-\nlenge is to learn each module with the least\namount of samples (i.e., few-shots) given the\nhigh cost related to the data collection. The\nmost common and effective technique to solve\nthis problem is transfer learning, where large\nlanguage models, either pre-trained on text or\ntask-speciﬁc data, are ﬁne-tuned on the few\nsamples. These methods require ﬁne-tuning\nsteps and a set of parameters for each task.\nDifferently, language models, such as GPT-\n2 (Radford et al., 2019) and GPT-3 (Brown\net al., 2020), allow few-shot learning by prim-\ning the model with few examples. In this pa-\nper, we evaluate the priming few-shot ability\nof language models in the NLU, DST, DP and\nNLG tasks. Importantly, we highlight the cur-\nrent limitations of this approach, and we dis-\ncuss the possible implication to future work.\nAcknowledgments\nI would like to thanks Jason Wu for providing an\neasy to use code in ToD-BERT and for clariﬁcation\nabout the code and tasks, Baolin Peng for the easy\nto use repository FewShotNLG and for providing\nhelp with the scorer, and Sumanth Dathathri for the\ndiscussion and insight about the limitation of the\nLM priming few-shots.\n1 Introduction\nModularized task-oriented dialogues systems are\nthe core of the current smart speaker generation\n(e.g., Alexa, Siri etc.). The main modules of\nsuch systems are Natural Language Understand-\ning (NLU), Dialogue State Tracking (DST), Dia-\nlogue Policy (DP) and Natural Language Genera-\nFigure 1: Language model priming for few-shot intent\nrecognition. Image inspired by OpenAI GPT-3 (Brown\net al., 2020). Few examples are provided along with\nthe sample to be predicted as the preﬁx to the language\nmodel.\ntion (NLG), each of which is trained separately us-\ning supervised and/or reinforcement learning. Thus\na data collection process is required, which for\nsome of the tasks can be laborious and expensive.\nFor example, dialogue policy annotation has to be\ndone by an expert, better by a professional linguist.\nTherefore, having a model that requires only few\nsamples to actually perform well in the tasks is\nessential.\nThe most successful approach in few-shot learn-\ning for task-oriented dialogue systems is notably\ntransfer learning, where a large model is ﬁrstly\npre-trained on a large corpus to be then ﬁne-tuned\non speciﬁc tasks. For task-oriented dialogue sys-\ntems, Wu et al. (2020) proposed TOD-BERT a\nlarge pre-trained model which can achieve better\nperformance than BERT (Devlin et al., 2019) in\nfew-shots NLU, DST and DP. Liu et al. (2020) pro-\nposed a two-step classiﬁcation for few-shot slot-\nﬁlling, a key task for the NLU module. Sim-\nilarly, Peng et al. (2020b) introduced a bench-\nmark for few-shot NLG and a pre-trained lan-\nguage model (SC-GPT) specialized for the task.\narXiv:2008.06239v2  [cs.CL]  20 Aug 2020\nFurther, a template rewriting schema based on\nT5 (Raffel et al., 2019) was developed by Kale\nand Rastogi (2020) for few-shot NLG in two well-\nknown datasets. Peng et al. (2020a) proposed\na pre-trained language model (LM) for end-to-\nend pipe-lined task-oriented dialogue systems. In\ntheir experiments, they showed promising few-shot\nlearning performance in MWoZ (Budzianowski\net al., 2018). Finally, several meta-learning ap-\nproaches have been proposed for DP (Xu et al.,\n2020), NLG/ACT (Mi et al., 2019), pipelined end-\nto-end models (Qian and Yu, 2019) and personal-\nized dialogue systems (Madotto et al., 2019).\nFor performing few-shot learning, existing meth-\nods require a set of task-speciﬁc parameters since\nthe model is ﬁne-tuned with few samples. Differ-\nently, in this paper, we perform few-shot learning\nby priming LMs with few-examples (Radford et al.,\n2019; Brown et al., 2020). In this setting, no pa-\nrameters are updated, thus allowing a single model\nto perform multiple tasks at the same time. In this\npaper, we evaluate the few-shot ability of LM prim-\ning on the four task-oriented tasks previously men-\ntioned (i.e., NLU, DST, DP, and NLG). Currently,\nGPT-3 (Brown et al., 2020) is not available to the\npublic; thus we experiment on different sizes GPT-\n2 (Radford et al., 2019) models such as SMALL\n(117M), LARGE (762M), and XL (1.54B). All the\nexperiments are run on a single NVIDIA 1080Ti\nGPU.\n2 Basic Notation and Tasks\nLet us deﬁne dialogue as the alternation of utter-\nances between two speakers denoted by U and S\nrespectively. An utterance is a sequence of words\nX = x1, ··· , xn and the concatenation of t utter-\nances denotes a dialogue with t\n2 turns. In this paper,\nwe focus on the four task-oriented dialogue system\ntasks, and we brieﬂy introduce the input-output of\neach task.\nNLU This task aims to extract slot-value pairs\n(SLOT-FILLING) and the intent (INTENT) from\na user utterance S. In the literature, the most com-\nmon approach for NLU is to learn a BIO tagger\nfor the slot-value pairs, and to learn a multi-class\nclassiﬁer for the intent. SLOT-FILLING gets as\ninput a user utterance X and produces a dictio-\nnary M = {s1 = v1, ··· , sn = vn}, where si\nis a slot and vi is the possible value. Note that\nvi can also be None since some slots may not be\nmentioned in the utterance. The INTENT task gets\na user utterance X and classiﬁes it into an intent\nclass denoted by Y ∈{y1, ··· , yn}. Sometimes,\nthe intent-classiﬁcation is mixed with the domain\nclassiﬁcation.\nDST This task extracts slot-value pairs for a\ngiven dialogue, which can be considered as a\ndialogue-level of the NLU. Given a dialogue\nwith t turns as a sequence of utterance D =\n{X1\nU , X1\nS, ··· , Xt\nU }a DST model predicts a dic-\ntionary Mt = {s1 = v1, ··· , sn = vn}as in the\nNLU. Note that most of the existing DST models\nuse the previously generated Mt−1 and just update\nthe slots required using an NLU tagger.\nACT This task predicts the next speech-act (e.g.,\nINFORM, REQUEST etc.) given the current dia-\nlogue state, in the form of a dialogue or dictionary\nof slot-value pairs. This is usually stated as a rein-\nforcement learning task in both online and ofﬂine\nsettings. In this paper, we simplify the tasks, and\ninstead of learning a dialogue policy, we perform\ndialogue act classiﬁcation. This a multi-label clas-\nsiﬁcation task, since more than one speech-act can\nbe used in an utterance. This task gets as input\na user utterance X and classiﬁes it in to a set of\npossible speech-acts in I ∈{I1, ··· , In}.\nNLG This task maps a dialogue-act, which is\nmade of a speech-act plus a dictionary of slot-value\npairs, into natural language. The model gets as\ninput a speech-act concatenated with a slot-value\ndictionary overall denoted as I(s1 = v1, ··· , sn =\nvn) and it generates as output an utterance X.\nIn the few-shot setting, a small number of input-\noutput pairs is provided to the model, expecting a\nhigh degree of generalization.\n3 Priming the LM for few-shot learning\nDifferently from ﬁne-tuning, few-shot learning\nwith LMs requires designing preﬁxes to perform\nfew-shot learning. In our four tasks, we use three\ncategories of preﬁxes: binary, value-based and\ngenerative. In the following notation, we use X\nto represent a generic input and Xi for the i-th\nshot samples, thus implying that the preﬁx remains\nﬁxed during the inference and X can become any\ninput. These preﬁxes are provided to the LM and\nthe generate tokens become the actual prediction,\nFigure 1 show an example of intent recognition.\nBinary preﬁxes are used for classiﬁcation\n(namely for intent-classiﬁcation and speech-act de-\nturn on the light→name=None\nadd to playlist kojak→name=kojak\nadd tune to my hype playlist→name=\nFigure 2: Example of 1-shot LM priming for the\nSLOT-FILLING task and results in the task. CT, RZT,\nCoach are from (Liu et al., 2020) and they use 20-shots.\ntection). We treat every classiﬁcation as binary,\neven multi-class. To perform the few-shot priming,\nwe use the following preﬁx:\nX1 →True X∗\n1 →False ··· X → (1)\nwhere Xi one of the few-shot samples and X∗\ni is\nfrom other classes or from the false class if it exists.\nTo predict n classes, a set of n preﬁxes is used and\nthus n forwards is required.\nValue-based preﬁxes are used to assign the value\nof a certain slot given an utterance, or None if no\nvalue is provided. We deﬁne a preﬁx for each slot,\nsimilar to TRADE (Wu et al., 2019), which requires\nforwarding the model n times for decoding n-slots.\nTo perform the few-shot priming of one slot s, we\nuse the following preﬁx:\nX1 →s = v1 X∗\n1 →s = None ··· X →s =\n(2)\nwhere v1 is the assigned value from the few-shot\ntraining. This process is repeated for each slot to\ngenerate the dictionary M.\nGenerative preﬁxes are used to instruct the\nmodel to generate natural language given source in-\nformation (e.g., NLG). The preﬁx is the following:\nX1 →Y1 ··· Xk →Yk X → (3)\nwhere Xi and Yi are generic sequences of words.\nyes, your booking is successful→booked=True\nwhat type of food?→booked=False\ni do not seem to be finding anything→booked=\nFigure 3: Example of 1-shot LM priming for the ACT\ntask and results in the task. BERT and ToD-BERT are\nfrom (Wu et al., 2020) and they use 500-shots.\n4 Experiments and Results\nWe use different preﬁx styles depending on the\ntask and we compare the results of LM few-shot\npriming with those of the existing ﬁnetuning-base\nmodels. In all the experiments, we use different\nnumber of shots since different tasks may ﬁt more\nor fewer samples in the 1024 max input size of\nGPT-2.\nNLU We use the SNIPS (Coucke et al.,\n2018) dataset for evaluating the SLOT-FILLING\nand INTENT recognition tasks. For the\nSLOT-FILLING task, we follow the few-shot\nsetting of Liu et al. (2020), and we use the of-\nﬁcial CoNLL F1 scorer as the evaluation met-\nric. For the INTENT classiﬁcation, we ﬁne-tune\nRoBERTa (Liu et al., 2019) with 10 samples and\nuse accuracy as the evaluation metric. We use a\nvalue-based LM preﬁx for the SLOT-FILLING\ntask with a maximum of 15 shots, and binary LM\npreﬁx for the INTENT classiﬁcation task with a\nmaximum of 10 shots. An example of a preﬁx for\nthe SLOT-FILLING task and the few-shot perfor-\nmance evaluation are shown in Figure 2. Table 1\nand 2 and Figure 5 show more detailed results.\nDST We use the MultiWoZ (Budzianowski et al.,\n2018; Eric et al., 2019; Zang et al., 2020) dataset\nfor evaluating the DST task. Differently from other\nworks, we use the last user utterance only as in-\nput to the model, and we update the predicted-\nDST through turns. For the few-shot evaluation,\ninform(name=hilton...)→the hilton ...\ninform(name=ocean park...)→the phone number\ninform(name=’super 8...)→\nFigure 4: Example of 1-shot LM priming for the NLG\ntask and results in the task. SC-LSTM, GPT-2, and SC-\nGPT-2 are from Peng et al. (2020b).\nwe follow the setting of Wu et al. (2020), and\nwe report the joint and slot accuracy. As base-\nlines, we use TOD-BERT (Wu et al., 2020) and\nBERT (Devlin et al., 2019) ﬁne-tuned with 10%\nof the training data, which is equivalent to 500 ex-\namples. We use a value-based LM preﬁx, as for\nthe SLOT-FILLING task, with a maximum of 15\nshots due to limited context. Table 4 and Figure 6\nshow more detailed results.\nACT We use the MultiWoZ dataset for evaluat-\ning the speech ACT identiﬁcation task. Differently\nfrom other works, only the system utterance is used\nas input to the model, instead of including the dia-\nlogue history and the user utterance as in Wu et al.\n(2020). For the few-shot evaluation, we follow the\nsetting of Wu et al. (2020), i.e., F1-score. As base-\nlines, we use TOD-BERT (Wu et al., 2020) and\nBERT (Devlin et al., 2019), ﬁne-tuned with 10% of\nthe training data, which is equivalent to 500 exam-\nples. We use a binary LM preﬁx, as for the intent\nclassiﬁcation task, with a maximum of 15 shots due\nto limited context. An example of a preﬁx for the\nACT tasks and the few-shot performance evaluation\nis shown in Figure 3. Table 3 and Figure 7 show\nmore detailed results.\nNLG We use the FewShotWOZ (Peng et al.,\n2020b) dataset for evaluating the NLG task. For\nthe few-shot evaluation, we follow the setting of\nPeng et al. (2020b) and use the BLEU and slot error\nrate (SLR) as metrics. We use SC-LSTM, GPT-2,\nand SC-GPT-2 (Peng et al., 2020b) as baselines, all\nﬁne-tuned with 50 examples from the training data.\nWe use a generative LM preﬁx with a maximum of\n20 shots due to limited context. An example of pre-\nﬁx for the NLG task and the few-shot performance\nevaluation is shown in Figure 4. Table 5 and 6, and\nFigure 8 show more detailed results.\n5 Analysis and Limitation\nFrom the experimental results, we observe that:\n•The larger the model the better the performance\nin both the NLU and NLG tasks, while, instead, in\nthe DST and ACT tasks, GPT-2 LARGE (762M)\nperforms better than the XL (1.54B) version.\nThis is quite counterintuitive given the results\nreported for GPT-3. Further investigation is re-\nquired to understand whether changing the preﬁx\ncan help to improve the performance of larger\nmodels;\n•In the NLU, ACT and NLG, LM priming few-\nshot learning shows promising results, achiev-\ning similar or better performance than the weak-\nest ﬁnetuning-based baseline, which also uses a\nlarger number of shots. On the other hand, in\nDST the gap with the existing baseline is still\nlarge.\nWe also observe two limitations of the LM priming:\n•Using binary and value-based generation re-\nquires as many forwards as the number of classes\nor slots. Although these forward passes are inde-\npendent, achieving few-shot learning this way is\nnot as effective as directly generating the class\nor the tag (e.g., NLU). In early experiments, we\ntried to covert all the tasks into a generative for-\nmat, thus making the model directly generate\nthe sequence of tags or the class label. Unfortu-\nnately, the results in the generative format were\npoor, but we are unsure if larger LMs such as\nGPT-3 can perform better.\n•The current max-input length of GPT-2 (1024\ntokens) greatly limits the number of shots that\ncan be provided to the model. Indeed, in most\nof the tasks, no more than 15 shots can be pro-\nvided, thus making it incomparable with existing\nmodels that use a larger number of shots.\n6 Conclusion\nIn this paper, we demonstrate the potential of LM\npriming few-shot learning in the most common\ntask-oriented dialogue system tasks (NLU, DST,\nACT and NLG). Our experiments show that in most\nof the tasks larger LMs are better few-shot learners,\nconﬁrming the hypothesis in Brown et al. (2020)\nand, in some cases, they can also achieve simi-\nlar or better results than the weakest ﬁnetuning-\nbased baseline. Finally, we unveil two limita-\ntions of the current LM priming few-shot learning\nthe computational cost and the limited word con-\ntext size. In future work, we plan to benchmark\ndialogue-speciﬁc models (e.g., DialGPT) and LM\nwith longer context size (e.g., Transformer XL (Dai\net al., 2019), LongFormer (Beltagy et al., 2020),\nand BigBird (Zaheer et al., 2020) etc.). We also\nplan to investigate adversarial triggers (Wallace\net al., 2019) for improving the few-shot ability of\nLMs, and to benchmark end-to-end dialogue tasks.\nReferences\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I ˜nigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz-a large-\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016–5026.\nAlice Coucke, Alaa Saade, Adrien Ball, Th ´eodore\nBluche, Alexandre Caulier, David Leroy, Cl ´ement\nDoumouro, Thibault Gisselbrecht, Francesco Calta-\ngirone, Thibaut Lavril, et al. 2018. Snips voice plat-\nform: an embedded spoken language understanding\nsystem for private-by-design voice interfaces. arXiv\npreprint arXiv:1805.10190.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nMihail Eric, Rahul Goel, Shachi Paul, Adarsh Ku-\nmar, Abhishek Sethi, Peter Ku, Anuj Kumar\nGoyal, Sanchit Agarwal, Shuyang Gao, and Dilek\nHakkani-Tur. 2019. Multiwoz 2.1: A consoli-\ndated multi-domain dialogue dataset with state cor-\nrections and state tracking baselines. arXiv preprint\narXiv:1907.01669.\nMihir Kale and Abhinav Rastogi. 2020. Few-shot\nnatural language generation by rewriting templates.\narXiv preprint arXiv:2004.15006.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZihan Liu, Genta Indra Winata, Peng Xu, and Pas-\ncale Fung. 2020. Coach: A coarse-to-ﬁne ap-\nproach for cross-domain slot ﬁlling. arXiv preprint\narXiv:2004.11727.\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, and\nPascale Fung. 2019. Personalizing dialogue agents\nvia meta-learning. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5454–5459.\nFei Mi, Minlie Huang, Jiyong Zhang, and Boi Faltings.\n2019. Meta-learning for low-resource natural lan-\nguage generation in task-oriented dialogue systems.\nIn Proceedings of the 28th International Joint Con-\nference on Artiﬁcial Intelligence , pages 3151–3157.\nAAAI Press.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2020a. Soloist:\nFew-shot task-oriented dialog with a single pre-\ntrained auto-regressive model. arXiv preprint\narXiv:2005.05298.\nBaolin Peng, Chenguang Zhu, Chunyuan Li, Xiu-\njun Li, Jinchao Li, Michael Zeng, and Jianfeng\nGao. 2020b. Few-shot natural language gener-\nation for task-oriented dialog. arXiv preprint\narXiv:2002.12328.\nKun Qian and Zhou Yu. 2019. Domain adaptive di-\nalog generation via meta learning. arXiv preprint\narXiv:1906.03520.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162.\nChien-Sheng Wu, Steven Hoi, Richard Socher, and\nCaiming Xiong. 2020. Tod-bert: Pre-trained natural\nlanguage understanding for task-oriented dialogues.\narXiv preprint arXiv:2004.06871.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-\nAsl, Caiming Xiong, Richard Socher, and Pascale\nFung. 2019. Transferable multi-domain state gen-\nerator for task-oriented dialogue systems. arXiv\npreprint arXiv:1905.08743.\nYumo Xu, Chenguang Zhu, Baolin Peng, and Michael\nZeng. 2020. Meta dialogue policy learning. arXiv\npreprint arXiv:2006.02588.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\nRaghav Gupta, Jianguo Zhang, and Jindong Chen.\n2020. Multiwoz 2.2: A dialogue dataset with addi-\ntional annotation corrections and state tracking base-\nlines. arXiv preprint arXiv:2007.12720.\nA Appendices\nSLOT-FILLING\nadd tune to my hype playlist → entity name = None\nadd to playlist confidence boost here comes → entity name = here comes\nadd the track bg knocc out to the rapcaviar playlist→ entity name =\nINTENT\nlisten to westbam alumb allergic on google music→ playmusic = true\nrate this novel 4 points out of 6 → playmusic = false\nadd sabrina salerno to the grime instrumentals playlist→ playmusic =\nFigure 5: Example of 1-shot LM priming for the SLOT-FILLING and INTENT task and results in the task. CT,\nRZT, and Coach are from Liu et al. (2020) and they use 20-shots.\ni need a cab by 12:30 too → leave at = 12:30\ni would like the taxi to pick me up from the hotel→ leave at = None\ni would like a taxi from saint john s college → leave at =\nFigure 6: Example of 1-shot LM priming for the DST task and results in the task. BERT and ToD-BERT are from\nWu et al. (2020) and they use 500 shots.\nyes your booking is successful and your reference number is ri4vvzyc .→offerbooked=True\nwhat type of food are you looking for ?→offerbooked=False\ni do not seem to be finding anything→offerbooked=\nFigure 7: Example of 1-shot LM priming for the ACT task and results in the task. BERT and ToD-BERT are from\nWu et al. (2020) and they use 500 shots.\ninform(name=hilton;area=chinatown)→the hilton is near chinatown\ninform(name=ocean park;phone=4155667020)→the phone number for ocean park is 4155667020.\ninform(name=super 8 san francisco;phone=8005369326)→\nFigure 8: Example of 1-shot LM priming for the NLG task and results in the task. SC-LSTM, GPT-2, and SC-GPT-\n2 are from Peng et al. (2020b). BLEU the higher the better; SLOT ERROR RATE the lower the better.\nModel Shots PlayL Rest. Weather PlayM. RateBook SearchC. Find. Avg\ngpt2 1 31.9008 8.0350 16.4160 32.8150 43.6023 20.4974 24.4994 25.3951\ngpt2 10 46.2546 21.6707 19.1909 21.4724 56.2280 38.0345 41.7234 34.9392\ngpt2 15 54.7410 26.4663 17.7377 28.3369 63.8482 41.3968 47.0525 39.9399\ngpt2-large 1 54.7548 39.4418 23.5223 20.8827 38.3591 26.6576 43.0562 35.2392\ngpt2-large 10 71.6635 39.2936 27.7395 48.1905 61.4562 44.4720 53.8340 49.5213\ngpt2-large 15 71.6569 45.5142 30.7992 46.3439 61.7858 42.8394 61.1420 51.4402\ngpt2-xl 1 53.8250 26.2185 23.1651 28.7647 37.1651 37.4536 31.0224 33.9449\ngpt2-xl 10 70.4698 40.5039 34.7138 40.4731 74.3899 52.0532 64.4166 53.8600\ngpt2-xl 15 67.9448 46.9853 30.8481 44.4646 77.1531 51.8732 67.0917 55.1944\nTable 1: Results in terms of CoNNL F1-score the SLOT-FILLING task.\nModel Shots Micro Macro Acc\ngpt2 1 0.1600 0.1553 16.0000\ngpt2 2 0.1671 0.1034 16.7143\ngpt2 5 0.3443 0.3223 34.4286\ngpt2 10 0.3600 0.3715 36.0000\ngpt2-large 1 0.1343 0.1188 13.4286\ngpt2-large 2 0.3786 0.3946 37.8571\ngpt2-large 5 0.3300 0.3175 33.0000\ngpt2-large 10 0.5514 0.5871 55.1429\ngpt2-xl 1 0.1700 0.1346 17.0000\ngpt2-xl 2 0.3586 0.3166 35.8571\ngpt2-xl 5 0.4671 0.4371 46.7143\ngpt2-xl 10 0.7300 0.7450 73.0000\nTable 2: Results in terms of F1-score (Micro and Macro) and Accuracy in the INTENT recognition task.\nModel Shots Micro Macro Acc\ngpt2 5 73.9364 54.7965 0.7394\ngpt2 10 78.5699 59.6442 0.7857\ngpt2 15 78.0943 59.8866 0.7809\ngpt2-large 5 78.7105 62.2181 0.7871\ngpt2-large 10 83.5762 68.6824 0.8358\ngpt2-large 15 83.5102 68.2287 0.8351\ngpt2-xl 5 63.1241 52.8427 0.6312\ngpt2-xl 10 75.4120 62.2672 0.7541\ngpt2-xl 15 77.7434 63.0193 0.7774\nTable 3: Results in terms of F1-score (Micro and Macro) and Accuracy in the ACT detection task.\nModel Shots Joint Slot\ngpt2 5 0.7 79.8\ngpt2 10 0.8 78.7\ngpt2 15 0.6 79.7\ngpt2-large 5 2.5 82.7\ngpt2-large 10 2.6 83.2\ngpt2-large 15 3.5 83.5\ngpt2-xl 5 2.2 81.4\ngpt2-xl 10 2.1 80.4\ngpt2-xl 15 2.0 81.8\nTable 4: Results in terms of Joint and Slot Accuracy in the DST task.\nModel Shots restaurant laptop hotel tv attraction train taxi Avg\nSC-LSTM 50 15.90 21.98 31.30 22.39 7.76 6.08 11.61 16.71\nGPT-2 50 29.48 27.43 35.75 28.47 16.11 13.72 16.27 23.89\nSC-GPT 50 38.08 32.73 38.25 32.95 20.69 17.21 19.70 28.51\ngpt2 5 9.93 17.75 14.85 16.29 5.50 0.26 5.01 9.94\ngpt2 10 8.10 17.75 16.85 16.29 5.84 1.30 4.71 10.12\ngpt2 20 10.68 17.75 19.15 16.29 4.89 3.24 7.28 11.32\ngpt2-large 5 10.60 24.42 13.92 24.58 7.38 0.73 7.86 12.78\ngpt2-large 10 13.10 24.42 20.68 24.58 6.68 3.18 6.25 14.13\ngpt2-large 20 11.47 24.42 16.13 24.58 7.97 5.30 9.36 14.18\ngpt2-xl 5 13.65 23.39 14.26 26.61 6.96 0.74 6.59 13.17\ngpt2-xl 10 14.51 23.39 19.42 26.61 8.21 4.00 6.40 14.65\ngpt2-xl 20 17.02 23.39 21.30 26.61 6.43 5.68 9.06 15.64\nTable 5: Results in terms of BLEU score for the NLG task. SC-LSTM, GPT-2, and SC-GPT-2 are from Peng et al.\n(2020b).\nModel Shots restaurant laptop hotel tv attraction train taxi Avg\nSC-LSTM 50 48.02 80.48 31.54 64.62 367.12 189.88 61.45 120.44\nGPT-2 50 13.47 11.26 11.54 9.44 21.10 19.26 9.52 13.65\nSC-GPT 50 3.89 3.39 2.75 3.38 12.72 7.74 3.57 5.35\ngpt2 5 60.48 60.84 73.63 72.66 81.79 60.54 66.67 68.09\ngpt2 10 72.75 60.84 78.02 72.66 80.49 88.75 59.52 73.29\ngpt2 20 70.36 60.84 74.18 72.66 67.20 68.96 55.95 67.16\ngpt2-large 5 55.39 36.33 84.62 44.02 64.31 58.11 44.05 55.26\ngpt2-large 10 57.49 36.33 62.09 44.02 52.31 73.27 25.00 50.07\ngpt2-large 20 48.20 36.33 85.71 44.02 56.07 61.35 32.14 51.98\ngpt2-xl 5 44.61 29.99 67.03 37.92 67.63 55.82 44.05 49.58\ngpt2-xl 10 46.41 29.99 47.80 37.92 50.87 62.36 22.62 42.57\ngpt2-xl 20 44.61 29.99 68.68 37.92 56.50 52.93 30.95 45.94\nTable 6: Results in terms of SLOT ERROR RATE for the NLG task. SC-LSTM, GPT-2, and SC-GPT-2 are from\nPeng et al. (2020b).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7993026971817017
    },
    {
      "name": "Task (project management)",
      "score": 0.7236147522926331
    },
    {
      "name": "Natural language processing",
      "score": 0.597523033618927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5829193592071533
    },
    {
      "name": "Natural language understanding",
      "score": 0.5659986138343811
    },
    {
      "name": "Priming (agriculture)",
      "score": 0.541414737701416
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5382335782051086
    },
    {
      "name": "Natural language generation",
      "score": 0.5278515815734863
    },
    {
      "name": "Natural language",
      "score": 0.45671573281288147
    },
    {
      "name": "Language model",
      "score": 0.4349867105484009
    },
    {
      "name": "Shot (pellet)",
      "score": 0.41882458329200745
    },
    {
      "name": "One shot",
      "score": 0.41815781593322754
    },
    {
      "name": "Training set",
      "score": 0.4115979075431824
    },
    {
      "name": "Programming language",
      "score": 0.09195029735565186
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Germination",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 36
}