{
  "title": "Face4Rag: Factual Consistency Evaluation for Retrieval Augmented Generation in Chinese",
  "url": "https://openalex.org/W4400267554",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2110147940",
      "name": "Yunqi Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103508815",
      "name": "Tianchi Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3071565888",
      "name": "Jiyan Jiang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A4321835420",
      "name": "Xierui Song",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2983309655",
    "https://openalex.org/W6607919353",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3213990450",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3201915713",
    "https://openalex.org/W4385572464",
    "https://openalex.org/W4221143046"
  ],
  "abstract": "The prevailing issue of factual inconsistency errors in conventional\\nRetrieval Augmented Generation (RAG) motivates the study of Factual Consistency\\nEvaluation (FCE). Despite the various FCE methods proposed earlier, these\\nmethods are evaluated on datasets generated by specific Large Language Models\\n(LLMs). Without a comprehensive benchmark, it remains unexplored how these FCE\\nmethods perform on other LLMs with different error distributions or even unseen\\nerror types, as these methods may fail to detect the error types generated by\\nother LLMs. To fill this gap, in this paper, we propose the first comprehensive\\nFCE benchmark \\\\emph{Face4RAG} for RAG independent of the underlying LLM. Our\\nbenchmark consists of a synthetic dataset built upon a carefully designed\\ntypology for factuality inconsistency error and a real-world dataset\\nconstructed from six commonly used LLMs, enabling evaluation of FCE methods on\\nspecific error types or real-world error distributions. On the proposed\\nbenchmark, we discover the failure of existing FCE methods to detect the\\nlogical fallacy, which refers to a mismatch of logic structures between the\\nanswer and the retrieved reference. To fix this issue, we further propose a new\\nmethod called \\\\emph{L-Face4RAG} with two novel designs of logic-preserving\\nanswer decomposition and fact-logic FCE. Extensive experiments show L-Face4RAG\\nsubstantially outperforms previous methods for factual inconsistency detection\\non a wide range of tasks, notably beyond the RAG task from which it is\\noriginally motivated. Both the benchmark and our proposed method are publicly\\navailable.\\\\footnote{\\\\url{https://huggingface.co/datasets/yq27/Face4RAG}\\\\label{link_face4rag}}\\n",
  "full_text": null,
  "topic": "Consistency (knowledge bases)",
  "concepts": [
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.8090474009513855
    },
    {
      "name": "Information retrieval",
      "score": 0.5256900787353516
    },
    {
      "name": "Computer science",
      "score": 0.4869168996810913
    },
    {
      "name": "Natural language processing",
      "score": 0.34498652815818787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3276804983615875
    }
  ],
  "institutions": [],
  "cited_by": 4
}