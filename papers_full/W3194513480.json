{
  "title": "Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers",
  "url": "https://openalex.org/W3194513480",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2115077148",
      "name": "Dong Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2226718446",
      "name": "Wang Wenhai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221722476",
      "name": "Fan, Deng-Ping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1612430329",
      "name": "Li Jinpeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750508952",
      "name": "Fu, Huazhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101740539",
      "name": "Shao, Ling",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3203606893",
    "https://openalex.org/W3164098653",
    "https://openalex.org/W4295916856",
    "https://openalex.org/W3048524582",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W2285968993",
    "https://openalex.org/W2560328367",
    "https://openalex.org/W3173503349",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W4287103623",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3146991503",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W4287391869",
    "https://openalex.org/W3157655791",
    "https://openalex.org/W2021088830",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W3118152168",
    "https://openalex.org/W3187378100",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3124583721",
    "https://openalex.org/W2346062110",
    "https://openalex.org/W1965415664",
    "https://openalex.org/W2750023899",
    "https://openalex.org/W3180624251",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3008070655",
    "https://openalex.org/W3116302332",
    "https://openalex.org/W3204076343",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2034269173",
    "https://openalex.org/W2077474654",
    "https://openalex.org/W3090492687",
    "https://openalex.org/W3104061658",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3173959091",
    "https://openalex.org/W3120333390",
    "https://openalex.org/W4290715209",
    "https://openalex.org/W3204575409",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3203875822",
    "https://openalex.org/W3094918035",
    "https://openalex.org/W3177004386",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3112951650",
    "https://openalex.org/W3138796575",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2979600871",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W3204182250",
    "https://openalex.org/W2963529609",
    "https://openalex.org/W3202715235",
    "https://openalex.org/W2884436604",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034684132",
    "https://openalex.org/W3157358059",
    "https://openalex.org/W3158064031",
    "https://openalex.org/W3173477118",
    "https://openalex.org/W2008359794",
    "https://openalex.org/W4295936768",
    "https://openalex.org/W3203497085",
    "https://openalex.org/W4287546904",
    "https://openalex.org/W3199914841",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W3202406646",
    "https://openalex.org/W4308456711",
    "https://openalex.org/W2987175876",
    "https://openalex.org/W3175214565",
    "https://openalex.org/W3098995103",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4287251532",
    "https://openalex.org/W3135262214",
    "https://openalex.org/W2963112696",
    "https://openalex.org/W3176326329",
    "https://openalex.org/W2999580839",
    "https://openalex.org/W4295934810",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4285050439",
    "https://openalex.org/W1994922096",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4295915728",
    "https://openalex.org/W2979515228",
    "https://openalex.org/W3181013887",
    "https://openalex.org/W3082954719",
    "https://openalex.org/W2998449272",
    "https://openalex.org/W3189999308",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3204995672",
    "https://openalex.org/W3092344722",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3177407085",
    "https://openalex.org/W3095725246",
    "https://openalex.org/W2962927567",
    "https://openalex.org/W3202263958",
    "https://openalex.org/W4287186610",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W3203166921",
    "https://openalex.org/W3124994365",
    "https://openalex.org/W3106168076",
    "https://openalex.org/W3134036841",
    "https://openalex.org/W2954300568",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W2962943776",
    "https://openalex.org/W3143663586",
    "https://openalex.org/W3177764818",
    "https://openalex.org/W2997286550"
  ],
  "abstract": "Most polyp segmentation methods use CNNs as their backbone, leading to two key issues when exchanging information between the encoder and decoder: 1) taking into account the differences in contribution between different-level features and 2) designing an effective mechanism for fusing these features. Unlike existing CNN-based methods, we adopt a transformer encoder, which learns more powerful and robust representations. In addition, considering the image acquisition influence and elusive properties of polyps, we introduce three standard modules, including a cascaded fusion module (CFM), a camouflage identification module (CIM), and a similarity aggregation module (SAM). Among these, the CFM is used to collect the semantic and location information of polyps from high-level features; the CIM is applied to capture polyp information disguised in low-level features, and the SAM extends the pixel features of the polyp area with high-level semantic position information to the entire polyp area, thereby effectively fusing cross-level features. The proposed model, named Polyp-PVT, effectively suppresses noises in the features and significantly improves their expressive capabilities. Extensive experiments on five widely adopted datasets show that the proposed model is more robust to various challenging situations (e.g., appearance changes, small objects, rotation) than existing representative methods. The proposed model is available at https://github.com/DengPingFan/Polyp-PVT.",
  "full_text": "1\nPolyp-PVT: Polyp Segmentation with Pyramid\nVision Transformers\nBo Dong, Wenhai Wang, Deng-Ping Fan, Jinpeng Li, Huazhu Fu, and Ling Shao\nAbstract‚ÄîMost polyp segmentation methods use CNNs as their\nbackbone, leading to two key issues when exchanging information\nbetween the encoder and decoder: 1) taking into account the\ndifferences in contribution between different-level features and\n2) designing an effective mechanism for fusing these features.\nUnlike existing CNN-based methods, we adopt a transformer\nencoder, which learns more powerful and robust representations.\nIn addition, considering the image acquisition influence and\nelusive properties of polyps, we introduce three standard mod-\nules, including a cascaded fusion module (CFM), a camouflage\nidentification module (CIM), and a similarity aggregation module\n(SAM). Among these, the CFM is used to collect the semantic and\nlocation information of polyps from high-level features; the CIM\nis applied to capture polyp information disguised in low-level\nfeatures, and the SAM extends the pixel features of the polyp area\nwith high-level semantic position information to the entire polyp\narea, thereby effectively fusing cross-level features. The proposed\nmodel, named Polyp-PVT, effectively suppresses noises in the\nfeatures and significantly improves their expressive capabilities.\nExtensive experiments on five widely adopted datasets show\nthat the proposed model is more robust to various challenging\nsituations (e.g., appearance changes, small objects, rotation) than\nexisting representative methods. The proposed model is available\nat https://github.com/DengPingFan/Polyp-PVT.\nIndex Terms ‚ÄîPolyp segmentation, pyramid vision trans-\nformer, colonoscopy, computer vision\nI. I NTRODUCTION\nColonoscopy is the gold standard for detecting colorectal\nlesions since it enables colorectal polyps to be identified and\nremoved in time, thereby preventing further spread. As a\nfundamental task in medical image analysis, polyp segmen-\ntation (PS) aims to locate polyps accurately in the early stage,\nwhich is of great significance in the clinical prevention of\nrectal cancer. Traditional PS models mainly rely on low-level\nfeatures, e.g., texture [1], geometric features [2], simple linear\niterative clustering superpixels [3]. However, these methods\nyield low-quality results and suffer from poor generaliza-\ntion ability. With the development of deep learning, PS has\nachieved promising progress. In particular, the U-shaped [4]\nhas attracted significant attention due to its ability to adopt\nmulti-level features for reconstructing high-resolution results.\nB. Dong and D.-P. Fan are with the Nankai University, Tianjin, 300350,\nChina. * Corresponding author (dengpfan@gmail.com).\nW. Wang is with Shanghai Artificial Intelligence Laboratory, Shanghai,\n200232, China\nJ. Li is with Computer Vision Lab, Inception Institute of Artificial Intelli-\ngence, Abu Dhabi 00000, UAE\nH. Fu is with Institute of High Performance Computing, Agency for\nScience, Technology and Research, Singapore 138632, Singapore\nL. Shao is with UCAS-Terminus AI Lab, Terminus Group, Chongqing\n400042, China\nImage GT Ours SANet\nFig. 1. The segmentation examples of our model and SANet [7] with\ndifferent challenge cases, e.g., camouflage ( 1st and 2nd rows) and image\nacquisition influence ( 3rd row). The images from top to bottom are from\nClinicDB [8], ETIS [9], and ColonDB [10], which show that our model has\nbetter generalization ability.\nPraNet [5] employs a two-stage segmentation approach, adopt-\ning a parallel decoder to predict rough regions and an attention\nmechanism to restore a polyp‚Äôs edges and internal structure for\nfine-grained segmentation. ThresholdNet [6] is a confidence-\nguided data enhancement method based on a hybrid manifold\nfor solving the problems caused by limited annotated data and\nimbalanced data distributions.\nAlthough these methods have greatly improved accuracy\nand generalization ability compared to traditional methods,\nit is still challenging for them to locate the boundaries of\npolyps, as shown in Fig. 1, for several reasons: (1) Image\nnoise. During the data collection process, the lens rotates in the\nintestine to obtain polyp images from different angles, which\nalso causes motion blur and reflector problems. As a result,\nthis greatly increases the difficulty of polyp detection; (2)\nCamouflage. The color and texture of polyps are very similar\nto surrounding tissues, with low contrast, providing them\nwith powerful camouflage properties [11], [12], and making\nthem difficult to identify; (3) Polycentric data. Current models\nstruggle to generalize to multicenter (or unseen) data with\ndifferent domains/distributions. To address the above issues,\nour contributions in this paper are as follows:\n‚Ä¢ We present a novel polyp segmentation framework,\ntermed Polyp-PVT. Unlike existing CNN-based methods,\nwe adopt the pyramid vision transformer as an encoder\nto extract more robust features.\n‚Ä¢ To support our framework, we introduce three simple\nmodules. Specifically, the cascaded fusion module (CFM)\narXiv:2108.06932v8  [eess.IV]  19 Feb 2024\n2\ncollects polyps‚Äô semantic and location information from\nthe high-level features through progressive integration.\nMeanwhile, the camouflage identification module (CIM)\nis applied to capture polyp cues disguised in low-level\nfeatures, using an attention mechanism to pay more at-\ntention to potential polyps, reducing incorrect information\nin the lower features. We further introduce the similarity\naggregation module (SAM) equipped with a non-local\nand convolutional graph layer to mine local pixels and\nglobal semantic cues from the polyp area.\n‚Ä¢ Finally, we conduct extensive experiments on five chal-\nlenging benchmark datasets, including Kvasir-SEG [13],\nClinicDB [8], ColonDB [10], Endoscene [14], and\nETIS [9], to evaluate the performance of the proposed\nPolyp-PVT. On ColonDB, our method achieves a mean\nDice (mDic) of 0.808, which is 5.5% higher than the\nexisting cutting-edge method SANet [7]. On the ETIS\ndataset, our model achieves a mean Dice (mDic) of 0.787,\nwhich is 3.7% higher than SANet [7].\nII. R ELATED WORKS\nA. Polyp Segmentation\nTraditional Methods. Computer-aided detection is an effec-\ntive alternative to manual detection, and a detailed survey has\nbeen conducted on detecting ulcers, polyps, and tumors in\nwireless capsule endoscopy imaging [15]. Early solutions for\npolyp segmentation were mainly based on low-level features,\nsuch as texture [2], geometric features [2], or simple linear\niterative clustering superpixels [3]. However, these methods\nhave a high risk of missed or false detection due to the high\nsimilarity between polyps and surrounding tissues.\nDeep Learning-Based Methods. Deep learning tech-\nniques [16]‚Äì[25] have greatly promoted the development of\npolyp segmentation tasks. Akbari et al. [26] proposed a polyp\nsegmentation model using a fully convolutional neural net-\nwork, whose segmentation results are significantly better than\ntraditional solutions. Brandao et al. [27] used the shape from\nthe shading strategy to restore depth, merging the result into\nan RGB model to provide richer feature representations. More\nrecently, encoder-decoder-based models, such as U-Net [4],\nUNet++ [28], and ResUNet++ [29], have gradually come to\ndominate the field with excellent performance. Sun et al. [30]\nintroduced a dilated convolution to extract and aggregate high-\nlevel semantic features with resolution retention to improve\nthe encoder network. Psi-Net [31] introduced a multi-task\nsegmentation model that combines contour and distance map\nestimation to assist segmentation mask prediction. Hemin et\nal. [32] first attempted to use a deeper feature extractor to\nperform polyp segmentation based on Mask R-CNN [33].\nDifferent from the methods based on U-Net [4], [28], [34],\nPraNet [5] uses reverse attention modules to mine boundary\ninformation with a global feature map, which is generated\nby a parallel partial decoder from high-level features. Polyp-\nNet [35] proposed a dual-tree wavelet pooling CNN with\na local gradient-weighted embedding level set, effectively\navoiding erroneous information in high signal areas, thereby\nsignificantly reducing the false positive rate. Rahim et al. [36]\nproposed to use different convolution kernels for the same hid-\nden layer for deeper feature extraction with MISH and rectified\nlinear unit activation functions for deep feature propagation\nand smooth non-monotonicity. In addition, they adopted joint\ngeneralized intersections, which overcome scale invariance,\nrotation, and shape differences. Jha et al. [37] designed a\nreal-time polyp segmentation method called ColonSNet. For\nthe first time, Ahmed et al. [38] applied the generative\nadversarial network to the field of polyp segmentation. An-\nother interesting idea proposed by Thambawita et al. [39]\nis introducing pyramid-based augmentation into the polyp\nsegmentation task. Further, Tomar et al. [40] designed a\ndual decoder attention network based on ResUNet++ for\npolyp segmentation. More recently, MSEG [41] improved\nthe PraNet and proposed a simple encoder-decoder structure.\nSpecifically, they used Hardnet [42] to replace the original\nbackbone network Res2Net50 backbone network and removed\nthe attention mechanism to achieve faster and more accurate\npolyp segmentation. As an early attempt, Transfuse [43] was\nthe first to employ a two-branch architecture combining CNNs\nand transformers in a parallel style. DCRNet [44] uses external\nand internal context relations modules to separately estimate\nthe similarity between each location and all other locations\nin the same and different images. MSNet [45] introduced a\nmulti-scale subtraction network to eliminate redundancy and\ncomplementary information between the multi-scale features.\nProviding a comprehensive review of polyp segmentation is\nbeyond the scope of this paper. In Tab. I, however, we briefly\nsurvey representative works related to ours.\nB. Vision Transformer\nTransformers use multi-head self-attention (MHSA) layers\nto model long-term dependencies. Unlike the convolutional\nlayer, the MHSA layer has dynamic weights and a global\nreceptive field, making it more flexible and effective. The\ntransformer [65] was first proposed by Vaswani et al. for the\nmachine translation task and has since extensively influenced\nthe natural language processing field. To apply transformers\nto computer vision tasks, Dosovitskiy et al. [66] proposed a\nvision transformer (ViT), which was the first pure transformer\nfor image classification. ViT divides an image into multiple\npatches, which are sequentially sent to a transformer encoder\nafter being encoded, and then an MLP is used to perform\nimage classification. HVT [67] is based on a hierarchical pro-\ngressive pooling method to compress the sequence length of a\ntoken and reduce the redundancy and number of calculations in\nViT. The pooling-based vision transformer [68] draws on the\nprinciple of CNNs whereby, as the depth increases, the number\nof feature map channels increases, and the spatial dimension\ndecreases. Yuan et al. [69] pointed out that the simple token\nstructure in ViT cannot capture important local features, such\nas edges and lines, which reduces the training efficiency and\nleads to redundant attention mechanisms. T2T ViT was thus\nproposed to use layer-by-layer tokens-to-token transformation\nto gradually merge neighboring tokens and model local fea-\ntures while reducing the token‚Äôs length. TNT [70] employs\na transformer suitable for fine-grained image tasks, which\n3\nTABLE I\nA SURVEY ON POLYP SEGMENTATION . CL = CVC-CLINIC, EL = ETIS-L ARIB , C6 = CVC-612, AM = ASU-M AYO [46], [47], ES = E NDO SCENE , DB\n= COLON DB, CV = CVC-V IDEO CLINIC DB, C = C OLON , ED = E NDOTECT 2020, KS = K VASIR -SEG, KCS = K VASIR CAPSULE -SEG, P RANET =\nSAME TO DATASETS USED IN PRANET [5], IS = IMAGE SEGMENTATION , VS = VIDEO SEGMENTATION , CF = CLASSFICATION , OD = OBJECT DETECTION ,\nOWN = PRIVATE DATA . CSCPD [1], APD [2], SBCP [3], FCN [26], D-FCN [27], UN ET++ [28], P SI-NET [31], M ASK R-CNN [32], UDC [30],\nTHRESHOLD NET [6], MI2GAN [48], ACSN ET [49], P RANET [5], GAN [38], APS [50], PFA [39], MMT [51], U-N ET-RESNET50 [34], S URVEY [15],\nPOLYP -NET [35], D EEP CNN [36], EU-N ET [52], DSAS [53], U-N ET-MOBILE NETV2 [54], DCRN ET [44], MSEG [41], FSSN ET [55],\nAG-CUR ESNEST [56], MPAPS [57], R ESUNET++ [58], N ANO NET [59], C OLON SEGNET [37], S EGTRAN [60], DDAN ET [40], UACAN ET [61],\nDIVERGENT NET [62], DWH IERA SEG [63], T RANSFUSE [43], SAN ET [7], PNS-N ET [64].\nNo. Model Publication Code Type Dataset Core Components\n1 CSCPD IJPRAI N/A IS Own Adaptive-scale candidate\n2 APD TMI N/A IS Own Geometrical analysis, binary classifier\n3 SBCP SPMB N/A IS Own Superpixel\n4 FCN EMBC N/A IS DB FCN and patch selection\n5 D-FCN JMRR N/A IS CL, EL, AM, and DB FCN and Shape-from-Shading (SfS)\n6 UNet++ DLMIA PyTorch IS AM Skip pathways and deep supervision\n7 Psi-Net EMBC PyTorch IS Endovis Shape and boundary aware\n8 Mask R-CNN ISMICT N/A IS C6, EL, and DB Deep feature extractors\n9 UDC ICMLA N/A IS C6 and EL Dilation convolution\n10 ThresholdNet TMI PyTorch IS ES and WCE Learn to threshold\nConfidence-guided manifold mixup\n11 MI2GAN MICCAI N/A IS C6 and EL GAN based model\n12 ACSNet MICCAI PyTorch IS ES and KS Adaptive context selection\n13 PraNet MICCAI PyTorch IS PraNet Parallel partial decoder attention\n14 GAN MediaEval N/A IS KS Image-to-image translation\n15 APS MediaEval N/A IS KS Variants of U-shaped structure\n16 PFA MediaEval PyTorch IS KS Pyramid focus augmentation\n17 MMT MediaEval N/A IS KS Competition introduction\n18 U-Net-ResNet50 MediaEval N/A IS KS Variants of U-shaped structure\n19 Survey CMIG N/A CF Own Classification\n20 Polyp-Net TIM N/A IS DB and CV Multimodel fusion network\n21 Deep CNN BSPC N/A OD EL Convolutional neural network\n22 EU-Net CRV PyTorch IS PraNet Semantic information enhancement\n23 DSAS MIDL Matlab IS KS Stochastic activation selection\n24 U-Net-MobileNetV2 arXiv N/A IS KS Variants of U-shaped structure\n25 DCRNet ISBI PyTorch IS ES, KS, and\nPICCOLO\nWithin-image\nand cross-image contextual relations\n26 MSEG arXiv PyTorch IS PraNet Hardnet and partial decoder\n27 FSSNet arXiv N/A IS C6 and KS Meta-learning\n28 AG-CUResNeSt RIVF N/A IS PraNet ResNeSt, attention gates\n29 MPAPS JBHI PyTorch IS DB, KS, and EL Mutual-prototype adaptation network\n30 ResUNet++ JBHI PyTorch IS, VS PraNet and AM ResUNet++, CRF and TTA\n31 NanoNet CBMS PyTorch IS, VS ED, KS, and KCS Real-Time polyp segmentation\n32 ColonSegNet Access PyTorch IS KS Residual block and SENet\n33 Segtran IJCAI PyTorch IS C6 and KS Transformer\n34 DDANet ICPR PyTorch IS KS Dual decoder attention network\n35 UACANet ACM MM PyTorch IS PraNet Uncertainty augmented\nContext attention network\n36 DivergentNet ISBI PyTorch IS EndoCV 2021 Combine multiple models\n37 DWHieraSeg MIA PyTorch IS ES Dynamic-weighting\n38 Transfuse MICCAI PyTorch IS PraNet Transformer and CNN\n39 SANet MICCAI PyTorch IS PraNet Shallow attention network\n40 PNS-Net MICCAI PyTorch VS C6, KS, ES, and AM Progressively normalized\nself-attention network\ndivides the original image patch and conducts self-attention\nmechanism calculations in smaller units. Meanwhile, external\nand internal transformers are used to extract global and local\nfeatures.\nTo adapt to dense prediction tasks such as semantic seg-\nmentation, several methods [71]‚Äì[77] have also introduced\nthe pyramid structure of CNNs to the design of transformer\nbackbones. For instance, PVT-based models [71], [72] use\na hierarchical transformer with four stages, showing that a\npure transformer backbone can be as versatile as its CNN\ncounterparts, and performs better in detection and segmenta-\ntion tasks. In this work, we design a new transformer-based\npolyp segmentation framework, which can accurately locate\nthe boundaries of polyps even in extreme scenarios.\n4\nConv\nConv\nConv\nConv\nC\nConv\n(d) SAM\nC\nConv\nOut Out\nùë¢ùëù√ó2\nùë¢ùëù√ó2\nùë¢ùëù√ó2\nùë¢ùëù√ó4 ùë¢ùëù√ó2\nConv\nOut Conv 1√ó1\nConv 3√ó3+BN+ReLu\nHadamard Product\nC Concatenation\n(b) CFM\n(c) CIM\nCIM\nùêª\n4 √óùëä\n4 √ó64\nùêª\n8 √óùëä\n8 √ó128\nùêª\n16√ó ùëä\n16√ó320\nùêª\n32√ó ùëä\n32√ó512\nùêª√óùëä √ó3\n(a) PVT Encoder\nChannel Spatial\n1√ó1√óùê∂‚Ä≤ ùêª‚Ä≤√óùëä‚Ä≤√ó1\n‚Ñíùëöùëéùëñùëõ/‚Ñíùëéùë¢ùë•\nùêª‚Ä≤√óùëä‚Ä≤√óùê∂‚Ä≤ùêª‚Ä≤√óùëä‚Ä≤√óùê∂‚Ä≤\nùëá1\nùëá2\nùëã1\nùëã2\n‚Ä≤\nùëã3\n‚Ä≤\nùëã4\n‚Ä≤\nùëã34\nùëÉ1 ùëÉ2\nF\nConv\nConv\nFig. 2. Framework of our Polyp-PVT, which consists of a pyramid vision transformer (PVT) (a) as the encoder network, (b) cascaded fusion module (CFM)\nfor fusing the high-level feature, (c) camouflage identification module (CIM) to filter out the low-level information, and (d) similarity aggregation module\n(SAM) for integrating the high- and low-level features for the final output.\nIII. P ROPOSED POLYP -PVT\nA. Overall Architecture\nAs shown in Fig. 2, our Polyp-PVT consists of 4 key mod-\nules: namely, a pyramid vision transformer (PVT) encoder,\ncascaded fusion module (CFM), camouflage identification\nmodule (CIM), and similarity aggregation module (SAM).\nSpecifically, the PVT extracts multi-scale long-range depen-\ndencies features from the input image. The CFM is employed\nto collect semantic cues and locate polyps by aggregating high-\nlevel features progressively. The CIM is designed to remove\nnoise and enhance low-level representation information of\npolyps, including texture, color, and edges. The SAM is\nadopted to fuse the low- and high-level features provided by\nthe CIM and CFM, effectively transmitting the information\nfrom the pixel-level polyp to the entire polyp.\nGiven an input image I ‚àà RH√óW√ó3, we use the\ntransformer-based backbone [71] to extract four pyramid fea-\ntures Xi ‚ààR\nH\n2i+1 √ó W\n2i+1 √óCi, where Ci ‚àà{64,128,320,512}\nand i‚àà{1,2,3,4}. Then, we adjust the channel of three high-\nlevel features X2, X3 and X4 to 32 through three convolu-\ntional units and feed them ( i.e., X\n‚Ä≤\n2, X\n‚Ä≤\n3, and X\n‚Ä≤\n4) to CFM\nto fuse, leading a feature map T1 ‚ààR\nH\n8 √óW\n8 √ó32. Meanwhile,\nlow-level features X1 are converted to T2 ‚ààR\nH\n4 √óW\n4 √ó64 by\nthe CIM. After that, the T1 and T2 are aligned and fused by\nSAM, yielding the final feature map F ‚ààR\nH\n8 √óW\n8 √ó32. Finally,\nF is fed into a 1 √ó1 convolutional layer to predict the polyp\nsegmentation result P2. We use the sum of P1 and P2 as the\nfinal prediction. During training, we optimize the model with\na main loss Lmain and an auxiliary loss Laux. The main loss\nis calculated between the final segmentation result P2 and the\nground truth (GT), which is used to optimize the final polyp\nsegmentation result. Similarly, the auxiliary loss is used to\nsupervise the intermediate result P1 generated by the CFM.\nB. Transformer Encoder\nDue to uncontrolled factors in their acquisition, polyp\nimages tend to contain significant noise, such as motion blur,\nrotation, and reflection. Some recent works [78], [79] have\nfound that the vision transformer [66], [71], [72] demonstrates\nstronger performance and better robustness to input distur-\nbances than CNNs [16], [17]. Inspired by this, we use a\nvision transformer as our backbone network to extract more\nrobust and powerful features for polyp segmentation. Different\nfrom [66], [73] that uses a fixed ‚Äúcolumnar‚Äù structure or\nshifted windowing manner, the PVT [71] is a pyramid architec-\nture whose representation is calculated with spatial-reduction\nattention operations; thus it enables to reduce the resource\nconsumption. Note that the proposed model is backbone-\nindependent; other famous transformer backbones are feasible\nin our framework. Specifically, we adopt the PVTv2 [72],\nwhich is the improved version of PVT with a more power-\nful feature extraction ability. To adapt PVTv2 to the polyp\nsegmentation task, we remove the last classification layer\nand design a polyp segmentation head on top of four multi-\nscale feature maps ( i.e., X1, X2, X3, and X4) generated\nby different stages. Among these feature maps, X1 gives\ndetailed appearance information of polyps, and X2, X3, and\nX4 provide high-level features.\nC. Cascaded Fusion Module\nTo balance the accuracy and computational resources, we\nfollow recent popular practices [5], [80] to implement the\ncascaded fuse module (CFM). Specifically, we define F(¬∑) as a\nconvolutional unit composed of a3√ó3 convolutional layer with\npadding set to 1, batch normalization [81] and ReLU [82]. As\nshown in Fig. 2 (b), the CFM mainly consists of two cascaded\nparts, as follows:\n5\n(1) In part one, we up-sample the highest-level feature map\nX\n‚Ä≤\n4 to the same size as X\n‚Ä≤\n3 and then pass the result through\ntwo convolutional units F1(¬∑) and F2(¬∑), yieldings: X1\n4 and\nX2\n4 . Then, we multiply X1\n4 and X\n‚Ä≤\n3 and concatenate the result\nwith X2\n4 . Finally, we use a convolution unit F3(¬∑) to smooth\nthe concatenated feature, yielding fused feature map X34 ‚àà\nR\nH\n16 √óW\n16 √ó32. The process can be summarized as Eqn. 1.\nX34 = F3(Concat(F1(X\n‚Ä≤\n4) ‚äôX\n‚Ä≤\n3,F2(X\n‚Ä≤\n4))), (1)\nwhere ‚Äú ‚äô‚Äù denotes the Hadamard product, and Concat (¬∑) is\nthe concatenation operation along the channel dimension.\n(2) As shown Eqn. 2, the second part follows a similar\nprocess to part one. Firstly, we up-sample X\n‚Ä≤\n4, X\n‚Ä≤\n3, X34 to the\nsame size as X\n‚Ä≤\n2, and smooth them using convolutional units\nF4(¬∑), F5(¬∑), and F6(¬∑), respectively. Then, we multiply the\nsmoothed X\n‚Ä≤\n4 and X\n‚Ä≤\n3 with X\n‚Ä≤\n2, and concatenate the resulting\nmap with up-sampled and smoothed X34. Finally, we feed the\nconcatenated feature map into two convolutional units ( i.e.,\nF7(¬∑) and F8(¬∑)) to reduce the dimension, and obtain T1 ‚àà\nR\nH\n8 √óW\n8 √ó32, which is also the output of the CFM.\nT1 = F8(F7(Concat(F4(X\n‚Ä≤\n4) ‚äôF5(X\n‚Ä≤\n3) ‚äôX\n‚Ä≤\n2,F6(X34)))),\n(2)\nD. Camouflage Identification Module\nLow-level features often contain rich detail information,\nsuch as texture, color, and edges. However, polyps tend to\nbe very similar in appearance to the background. Therefore,\nwe need a powerful extractor to identify the polyp details.\nAs shown in Fig. 2 (c), we introduce a camouflage identi-\nfication module (CIM) to capture the details of polyps from\ndifferent dimensions of the low-level feature map X1. Specif-\nically, the CIM consists of a channel attention operation [83]\nAttc(¬∑) and a spatial attention operation [84] Att s(¬∑), which\ncan be formulated as:\nT2 = Atts(Attc(X1)) , (3)\nThe channel attention operation Att c(¬∑) can be written as\nfollow:\nAttc(x) =œÉ(H1 (Pmax (x)) +H2 (Pavg (x))) ‚äôx, (4)\nwhere x is the input tensor and œÉ(¬∑) is the Softmax function.\nPmax(¬∑) and Pavg(¬∑) denote adaptive maximum pooling and\nadaptive average pooling functions, respectively. Hi(¬∑),i ‚àà\n{1,2}shares parameters and consists of a convolutional layer\nwith 1 √ó1 kernel size to reduce the channel dimension\n16 times, followed by a ReLU layer and another 1 √ó1\nconvolutional layer to recover the original channel dimension.\nThe spatial attention operation Att s(¬∑) can be formulated as:\nAtts(x) =œÉ(G(Concat(Rmax(x),Ravg(x)))) ‚äôx, (5)\nwhere Rmax(¬∑) and Ravg(¬∑) represent the maximum and aver-\nage values obtained along the channel dimension, respectively.\nG(¬∑) is a 7 √ó7 convolutional layer with padding set to 3.\n  \n      \n       \n     \n  \nZ\nQ K V\n            \n      \n      \nf\nf\n  \n    \n          \n           \n      \nElement-wise Addition\nMatrix Multiplication\n          \n          \n                              \n         \n          \n       \n       \n          \n          \n  \n \nFig. 3. Details of the introduced SAM. It is composed of GCN and non-local,\nwhich extend the pixel features of polyp regions with high-level semantic\nlocation cues to the entire region.\nE. Similarity Aggregation Module\nTo explore high-order relations between the lower-level\nlocal features from CIM and higher-level cues from CFM.\nWe introduce the non-local [85], [86] operation under graph\nconvolution domain [87] to implement our similarity aggre-\ngation module (SAM). As a result, SAM can inject detailed\nappearance features into high-level semantic features using\nglobal attention.\nGiven the feature map T1, which contains high-level se-\nmantic information, and T2 with rich appearance details, we\nfuse them through self-attention. First, two linear mapping\nfunctions WŒ∏(¬∑) and Wœï(¬∑) are applied on T1 to reduce the\ndimension and obtain feature maps Q ‚àà R\nH\n8 √óW\n8 √ó16 and\nK ‚ààR\nH\n8 √óW\n8 √ó16. Here, we take a convolution operation with\na kernel size of 1 √ó1 as the linear mapping process. This\nprocess can be expressed as follows:\nQ= WŒ∏(T1),K = Wœï(T1). (6)\nFor T2, we use a convolutional unit Wg(¬∑) to reduce the\nchannel dimension to 32 and interpolate it to the same size\nas T1. Then, we apply a Softmax function on the channel\ndimension and choose the second channel as the attention map,\nleading to T\n‚Ä≤\n2 ‚ààR\nH\n8 √óW\n8 √ó1. These operations are represented\nas F (¬∑) in Fig. 3. Next, we calculate the Hadamard product\nbetween K and T\n‚Ä≤\n2. This operation assigns different weights\nto different pixels, increasing the weight of edge pixels. After\nthat, we use an adaptive pooling operation to reduce the\ndisplacement of features and apply a center crop on it to obtain\nthe feature map V ‚ààR4√ó4√ó16. In summary, the process can\n6\n0 200 400 600 800 1000 1200 1400\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0loss value\nIters\n auxiliary loss, lr=1e-3, decay_rate=0.5\n main loss, lr=1e-3, decay_rate=0.5\n total loss, lr=1e-3, decay_rate=0.5\n auxiliary loss, lr=1e-4, decay_rate=0.1\n main loss, lr=1e-4, decay_rate=0.1\n total loss, lr=1e-4, decay_rate=0.1\nFig. 4. Loss curves under different training parameter settings.\nbe formulated as follows:\nV = AP(K‚äôF(Wg(T2))), (7)\nwhere AP(¬∑) denotes the pooling and crop operations.\nThen, we establish the correlation between each pixel in V\nand K through an inner product, which is written as follows:\nf = œÉ(VT ‚äóK), (8)\nwhere ‚Äù ‚äó‚Äù denotes the inner product operation. VT is the\ntranspose of V and f is the correlation attention map.\nAfter obtaining the correlation attention map f, we multiply\nit with the feature map Q, and the result features are fed\nto the graph convolutional layer [86] GCN (¬∑), leading to\nG ‚ààR4√ó4√ó16. Same to [86], we calculate the inner product\nbetween f and G as Eqn. 9, reconstructing the graph domain\nfeatures into the original structural features:\nY‚Ä≤= fT ‚äóGCN(fT ‚äóQ). (9)\nThe reconstructed feature map Y‚Ä≤ is adjusted to the same\nchannel sizes with Y by a convolutional layer Wz(¬∑) with\n1 √ó1 kernel size, and then combined with the feature T1 to\nobtain the final output Z ‚ààR\nH\n8 √óW\n8 √ó32 of the SAM. Eqn. 10\nsummarizes the details of this process:\nZ = T1 + Wz(Y‚Ä≤). (10)\nF . Loss Function\nOur loss function can be formulated as Eqn. 11:\nL= Lmain + Laux, (11)\nwhere Lmain and Laux are the main loss and auxiliary loss,\nrespectively. The main loss Lmain is calculated between the\nfinal segmentation result P2 and ground truth G, which can\nbe written as:\nLmain = Lw\nIoU(P2,G) +Lw\nBCE(P2,G). (12)\nTABLE II\nPARAMETER SETTING DURING THE TRAINING STAGE .\nOptimizer Learning Rate (lr) Multi-scale Clip\nAdamW 1e-4 [0.75,1,1.25] 0.5\nDecay rate Weight decay Epochs Input Size\n0.1 1e-4 100 352 √ó 352\nThe auxiliary loss Laux is calculated between the interme-\ndiate result P1 from the CFM and ground truth G, which can\nbe formulated as:\nLaux = Lw\nIoU(P1,G) +Lw\nBCE(P1,G). (13)\nLw\nIoU(¬∑) and Lw\nBCE(¬∑) are the weighted intersection over union\n(IoU) loss [88] and weighted binary cross entropy (BCE)\nloss [88], which restrict the prediction map in terms of the\nglobal structure (object-level) and local details (pixel-level)\nperspectives. Unlike the standard BCE loss function, which\ntreats all pixels equally, Lw\nBCE(¬∑) considers the importance\nof each pixel and assigns higher weights to hard pixels.\nFurthermore, compared to the standard IoU loss, Lw\nIoU(¬∑) pays\nmore attention to the hard pixels.\nG. Implementation Details\nWe implement our Polyp-PVT with the PyTorch framework\nand use a Tesla P100 to accelerate the calculations. Consid-\nering the differences in the sizes of each polyp image, we\nadopt a multi-scale strategy [5], [41] in the training stage.\nThe hyperparameter details are as follows. To update the\nnetwork parameters, we use the AdamW [89] optimizer, which\nis widely used in transformer networks [71]‚Äì[73]. The learning\nrate is set to 1e-4 and the weight decay is adjusted to 1e-4\ntoo. Further, we resize the input images to 352 √ó352 with a\nmini-batch size of 16 for 100 epochs. More details about the\ntraining loss cures, parameter setting, and network parameters\nare shown in Fig. 4, Tab. II, and Tab. III, respectively. The\ntotal training time is nearly 3 hours to achieve the best (e.g., 30\nepochs) performance. For testing, we only resize the images to\n352√ó352 without any post-processing optimization strategies.\nIV. E XPERIMENTS\nA. Evaluation Metrics\nWe employ six widely-used evaluation metrics, including\nDice [90], IoU, mean absolute error (MAE), weighted F-\nmeasure ( Fw\nŒ≤ ) [91], S-measure ( SŒ±) [92], and E-measure\n(EŒæ) [93], [94] to evaluate the model performances. Among\nthese metrics, Dice and IoU are similarity measures at the re-\ngional level, which mainly focus on the internal consistency of\nsegmented objects. Here, we report the mean value of Dice and\nIoU, denoted as mDic and mIoU, respectively. MAE is a pixel-\nby-pixel comparison indicator that represents the average value\nof the absolute error between the predicted value and the true\nvalue. Weighted F-measure ( Fw\nŒ≤ ) comprehensively considers\nthe recall and precision and eliminates the effect of considering\neach pixel equally in conventional indicators. S-measure ( SŒ±)\nfocuses on the structural similarity of target prospects at the\n7\nTABLE III\nNETWORK PARAMETERS OF EACH MODULE . NOTE THAT THE ENCODER\nPARAMETERS ARE THE SAME AS PVT WITHOUT ANY CHANGES .\nBASIC CONV 2D AND CONV 2D WITH THE PARAMETERS [IN CHANNEL ,\nOUT CHANNEL , KERNEL SIZE , PADDING ] AND GCN [ NUM STATE,\nNUM NODE ].\nEncoder SAM\npatch size [4] AvgPool2d [6]\nembed dims [64, 128, 320, 512] Conv2d [32,16,1,1]\nnum heads [1, 2, 5, 8] Conv2d [32,16,1,1]\nmlp ratios [8, 8, 4, 4] Conv2d [16,32,1,1]\ndepths [3, 4, 18, 3] GCN [16,16]\nsr ratios [8, 4, 2, 1] BasicConv2d [64,32,1,0]\ndrop rate [0]\ndrop path rate [0.1]\nCFM CIM\nBasicConv2d [32,32,3,1] AvgPool2d [1]\nBasicConv2d [32,32,3,1] AvgPool2d [1]\nBasicConv2d [32,32,3,1] Conv2d [64,4,1,0]\nBasicConv2d [32,32,3,1] ReLU\nBasicConv2d [64,64,3,1] Conv2d [4,64,1,0]\nBasicConv2d [64,64,3,1] Sigmoid\nBasicConv2d [96,96,3,1] Conv2d [2,1,7,3]\nBasicConv2d [96,32,3,1] Sigmoid\nregion and object level. E-measure (EŒæ) is used to evaluate the\nsegmentation results at the pixel and image level. We report\nthe mean and max value of E-measure, denoted as mEŒæ and\nmaxEŒæ, respectively. The evaluation toolbox is derived from\nhttps://github.com/DengPingFan/PraNet.\nB. Datasets and Compared Models\nDatasets. Following the experimental setups in PraNet [5],\nwe adopt five challenging public datasets, including Kvasir-\nSEG [13], ClinicDB [8], ColonDB [10], Endoscene [14] and\nETIS [9] to verify the effectiveness of our framework.\nModels. We collect several open source models from the\nfield of polyp segmentation, for a total of nine compara-\ntive models, including U-Net [4], UNet++ [28], PraNet [5],\nSFA [95], MSEG [41], ACSNet [49], DCRNet [44], EU-\nNet [52] and SANet [7]. For a fair comparison, we use their\nopen-source codes to evaluate the same training and testing\nsets. Note that the SFA results are generated using the released\ntest model.\nC. Analysis of Learning Ability\nSettings. We use the ClinicDB and Kvasir-SEG datasets to\nevaluate the learning ability of the proposed model. ClinicDB\ncontains 612 images, which are extracted from 31 colonoscopy\nvideos. Kvasir-SEG is collected from the polyp class in the\nKvasir dataset and includes 1,000 polyp images. Following\nPraNet, we adopt the same 900 and 548 images from ClinicDB\nand Kvasir-SEG datasets as the training set, and the remaining\n64 and 100 images are employed as the respective test sets.\nResults. As can be seen in Tab. IV, our model is superior to\nthe current methods, demonstrating that it has a better learning\nability. On the Kvasir-SEG dataset, the mDic score of our\nmodel is 1.3% higher than that of the second-best model,\nSANet, and 1.9% higher than that of PraNet. On the ClinicDB\n0.521\n0.566 0.549\n0.728\n0.756\n0.728 0.713\n0.763 0.759\n0.814\n0.343\n0.444\n0.509\n0.639\n0.711\n0.603 0.584\n0.715\n0.765\n0.791\n0.33\n0.38\n0.43\n0.48\n0.53\n0.58\n0.63\n0.68\n0.73\n0.78\n0.83max Dice\nColonDB ETIS\nFig. 5. Evaluation of model generalization ability. We provide the max Dice\nresults on ColonDB and ETIS.\ndataset, the mDic score of our model is 2.1% higher than that\nof SANet and 3.8% higher than that of PraNet.\nD. Analysis of Generalization Ability\nSettings. To verify the generalization performance of the\nmodel, we test it on three unseen ( i.e., Polycentric) datasets,\nnamely ETIS, ColonDB, and EndoScene. There are 196 im-\nages in ETIS, 380 images in ColonDB, and 60 images in\nEndoScene. It is worth noting that the images in these datasets\nbelong to different medical centers. In other words, the model\nhas not seen their training data, which is different from the\nverification methods of ClinicDB and Kvasir-SEG.\nResults. The results are shown in Tab. VI and Tab. V. As\ncan be seen, our Polyp-PVT achieves a good generalization\nperformance compared with the existing models. And our\nmodel generalizes easily to multicentric (or unseen) data with\ndifferent domains/distributions. On ColonDB, it is ahead of the\nsecond-best SANet and classical PraNet by 5.5% and 9.6%,\nrespectively. On ETIS, we exceed the SANet and PraNet by\n3.7% and 15.9%, respectively. In addition, on EndoScene, our\nmodel is better than SANet and PraNet by 1.2% and 2.9%,\nrespectively. Moreover, to prove the generalization ability of\nPolyp-PVT, we present the max Dice results in Fig. 5, where\nour model shows a steady improvement on both ColonDB\nand ETIS. In addition, we show the standard deviation (SD)\nof the mean dice (mDic) between our model and others in\nTab. VII. As seen, there is not much difference in SD between\nour model and the comparison model, and they are both stable\nand balanced.\nE. Qualitative Analysis\nFig. 6 and Fig. 7 show the visualization results of our model\nand the compared models. We find that our results have two\nadvantages.\n‚Ä¢ Our model is able to adapt to data under different\nconditions. That is, it maintains a stable recognition and\nsegmentation ability under different acquisition environ-\nments (different lighting, contrast, reflection, motion blur,\nsmall objects, and rotation).\n‚Ä¢ The model segmentation results have internal consistency\nand predicted edges are closer to the ground-truth labels.\n8\nTABLE IV\nQUANTITATIVE RESULTS OF THE TEST DATASETS , i.e., KVASIR -SEG AND CLINIC DB.\nKvasir-SEG [13] ClinicDB [8]\nModel mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE\nMICCAI‚Äô15 U-Net 0.818 0.746 0.794 0.858 0.881 0.893 0.055 0.823 0.755 0.811 0.889 0.913 0.954 0.019\nDLMIA‚Äô18 UNet++ 0.821 0.743 0.808 0.862 0.886 0.909 0.048 0.794 0.729 0.785 0.873 0.891 0.931 0.022\nMICCAI‚Äô19 SFA 0.723 0.611 0.670 0.782 0.834 0.849 0.075 0.700 0.607 0.647 0.793 0.840 0.885 0.042\narXiv‚Äô21 MSEG 0.897 0.839 0.885 0.912 0.942 0.948 0.028 0.909 0.864 0.907 0.938 0.961 0.969 0.007\narXiv‚Äô21 DCRNet 0.886 0.825 0.868 0.911 0.933 0.941 0.035 0.896 0.844 0.890 0.933 0.964 0.978 0.010\nMICCAI‚Äô20 ACSNet 0.898 0.838 0.882 0.920 0.941 0.952 0.032 0.882 0.826 0.873 0.927 0.947 0.959 0.011\nMICCAI‚Äô20 PraNet 0.898 0.840 0.885 0.915 0.944 0.948 0.030 0.899 0.849 0.896 0.936 0.963 0.979 0.009\nCRV‚Äô21 EU-Net 0.908 0.854 0.893 0.917 0.951 0.954 0.028 0.902 0.846 0.891 0.936 0.959 0.965 0.011\nMICCAI‚Äô21 SANet 0.904 0.847 0.892 0.915 0.949 0.953 0.028 0.916 0.859 0.909 0.939 0.971 0.976 0.012\nPolyp-PVT (Ours) 0.917 0.864 0.911 0.925 0.956 0.962 0.023 0.937 0.889 0.936 0.949 0.985 0.989 0.006\nTABLE V\nQUANTITATIVE RESULTS OF THE TEST DATASETS COLON DB AND ETIS. T HE SFA RESULT IS GENERATED USING THE PUBLISHED CODE .\nColonDB [10] ETIS [9]\nModel mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE\nMICCAI‚Äô15 U-Net 0.512 0.444 0.498 0.712 0.696 0.776 0.061 0.398 0.335 0.366 0.684 0.643 0.740 0.036\nDLMIA‚Äô18 UNet++ 0.483 0.410 0.467 0.691 0.680 0.760 0.064 0.401 0.344 0.390 0.683 0.629 0.776 0.035\nMICCAI‚Äô19 SFA 0.469 0.347 0.379 0.634 0.675 0.764 0.094 0.297 0.217 0.231 0.557 0.531 0.632 0.109\nMICCAI‚Äô20 ACSNet 0.716 0.649 0.697 0.829 0.839 0.851 0.039 0.578 0.509 0.530 0.754 0.737 0.764 0.059\narXiv‚Äô21 MSEG 0.735 0.666 0.724 0.834 0.859 0.875 0.038 0.700 0.630 0.671 0.828 0.854 0.890 0.015\narXiv‚Äô21 DCRNet 0.704 0.631 0.684 0.821 0.840 0.848 0.052 0.556 0.496 0.506 0.736 0.742 0.773 0.096\nMICCAI‚Äô20 PraNet 0.712 0.640 0.699 0.820 0.847 0.872 0.043 0.628 0.567 0.600 0.794 0.808 0.841 0.031\nCRV‚Äô21 EU-Net 0.756 0.681 0.730 0.831 0.863 0.872 0.045 0.687 0.609 0.636 0.793 0.807 0.841 0.067\nMICCAI‚Äô21 SANet 0.753 0.670 0.726 0.837 0.869 0.878 0.043 0.750 0.654 0.685 0.849 0.881 0.897 0.015\nPolyp-PVT (Ours) 0.808 0.727 0.795 0.865 0.913 0.919 0.031 0.787 0.706 0.750 0.871 0.906 0.910 0.013\nImage GT Ours SANet PraNet ACSNet DCRNet\nFig. 6. Visualization results with the current models. Green indicates a correct polyp. Yellow is the missed polyp. Red is the wrong prediction. As we can\nsee, the proposed model can accurately locate and segment polyps, regardless of size.\n9\nImage GT Ours EU-Net HarDNet SFA U-Net UNet++\nFig. 7. Visualization results with the current models.\nTABLE VI\nQUANTITATIVE RESULTS OF THE TEST DATASET ENDOSCENE . THE SFA\nRESULT IS GENERATED USING THE PUBLISHED CODE .\nEndoscene [14]\nModel mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE\nU-Net 0.710 0.627 0.684 0.843 0.847 0.875 0.022\nUNet++ 0.707 0.624 0.687 0.839 0.834 0.898 0.018\nSFA 0.467 0.329 0.341 0.640 0.644 0.817 0.065\nMSEG 0.874 0.804 0.852 0.924 0.948 0.957 0.009\nACSNet 0.863 0.787 0.825 0.923 0.939 0.968 0.013\nDCRNet 0.856 0.788 0.830 0.921 0.943 0.960 0.010\nPraNet 0.871 0.797 0.843 0.925 0.950 0.972 0.010\nEU-Net 0.837 0.765 0.805 0.904 0.919 0.933 0.015\nSANet 0.888 0.815 0.859 0.928 0.962 0.972 0.008\nPolyp-PVT 0.900 0.833 0.884 0.935 0.973 0.981 0.007\nWe also provide FROC curves on ColonDB in Fig. 8, and\nour result is at the top, indicating that our effect achieves\nthe best.\nF . Ablation Study\nWe describe in detail the effectiveness of each component\non the overall model. The training, testing, and hyperparameter\nsettings are the same as mentioned in Sec. III-G. The results\nare shown in Tab. VIII.\nComponents. We use PVTv2 [72] as our baseline (Bas.)\nand evaluate module effectiveness by removing or replacing\ncomponents from the complete Polyp-PVT and comparing\nthe variants with the standard version. The standard version\nis denoted as ‚ÄúPolyp-PVT (PVT+CFM+CIM+SAM)‚Äù, where\n‚ÄúCFM‚Äù, ‚ÄúCIM‚Äù and ‚ÄúSAM‚Äù indicate the usage of the CFM,\nCIM, and SAM, respectively.\nEffectiveness of CFM. To analyze the effectiveness of\nthe CFM, a version of ‚ÄúPolyp-PVT (w/o CFM)‚Äù is trained.\n2500 5000 7500 10000 12500 15000 17500 20000\nAverage number of false positives per image\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0True Positive Rate\nFROC performence on the test set of ColonDB\nACSNet\nDCRNet\nEU-Net\nMSEG\nPraNet\nSANet\nSFA\nUNet\nUNet++\nPolypPVT\nFig. 8. FROC curves of different methods on ColonDB.\nImage GT Input Feature Channel Attention Spatial Attention\nFig. 9. Visualization of the feature map in the CIM module.\n10\nTABLE VII\nTHE STANDARD DEVIATION (SD) OF THE MEAN DICE (MDIC) OF OUR MODEL AND THE COMPARISON MODELS .\nDatasets Kvasir-SEG ClinicDB ColonDB ETIS Endoscene\nMetrics mDic ¬± SD mDic ¬± SD mDic ¬± SD mDic ¬± SD mDic ¬± SD\nMICCAI‚Äô15 U-Net .818 ¬± .039 .823 ¬± .047 .483 ¬± .034 .398 ¬± .033 .710 ¬± .049\nDLMIA‚Äô18 UNet++ .821 ¬± .040 .794 ¬± .044 .456 ¬± .037 .401 ¬± .057 .707 ¬± .053\nMICCAI‚Äô19 SFA .723 ¬± .052 .701 ¬± .054 .444 ¬± .037 .297 ¬± .025 .468 ¬± .050\narXiv‚Äô21 MSEG .897 ¬± .041 .910 ¬± .048 .735 ¬± .039 .700 ¬± .039 .874 ¬± .051\nMICCAI‚Äô20 ACSNet .898 ¬± .045 .882 ¬± .048 .716 ¬± .040 .578 ¬± .035 .863 ¬± .055\narXiv‚Äô21 DCRNet .886 ¬± .043 .896 ¬± .049 .704 ¬± .039 .556 ¬± .039 .857 ¬± .052\nMICCAI‚Äô20 PraNet .898 ¬± .041 .899 ¬± .048 .712 ¬± .038 .628 ¬± .036 .871 ¬± .051\nCRV‚Äô21 EU-Net .908 ¬± .042 .902 ¬± .048 .756 ¬± .040 .687 ¬± .039 .837 ¬± .049\nMICCAI‚Äô21 SANet .904 ¬± .042 .916 ¬± .049 .752 ¬± .040 .750 ¬± .047 .888 ¬± .054\nPolyp-PVT (Ours) .917 ¬± .042 .937 ¬± .050 .808 ¬± .043 .787 ¬± .044 .900 ¬± .052\nTABLE VIII\nQUANTITATIVE RESULTS FOR ABLATION STUDIES .\nDataset Metric Bas. w/o CFM w/o CIM w/o SAM Final\nEndoscene mDic 0.869 0.892 0.882 0.874 0.900\nmIoU 0.792 0.826 0.808 0.801 0.833\nClinicDB mDic 0.903 0.915 0.930 0.930 0.937\nmIoU 0.847 0.865 0.881 0.877 0.889\nColonDB mDic 0.796 0.802 0.805 0.779 0.808\nmIoU 0.707 0.721 0.724 0.696 0.727\nETIS mDic 0.759 0.771 0.785 0.778 0.787\nmIoU 0.668 0.690 0.711 0.693 0.706\nKvasir-SEG mDic 0.910 0.922 0.910 0.910 0.917\nmIoU 0.856 0.872 0.858 0.853 0.864\nTABLE IX\nABLATION STUDY OF GCN IN THE SAM MODULE .\nSetting Endoscene ClinicDB ColonDB ETIS Kvasir-SEG\nw/o GCN 0.876 0.928 0.784 0.725 0.894\nw/ Conv 0.894 0.919 0.787 0.742 0.909\nw/ GCN 0.900 0.937 0.808 0.787 0.917\nTab. VIII shows that the model without the CFM drops sharply\non all five datasets compared to the standard Polyp-PVT.\nIn particular, the mDic is reduced from 0.937 to 0.915 on\nClinicDB.\nEffectiveness of CIM. To demonstrate the ability of the\nCIM, we also remove it from Polyp-PVT, denoting this as\n‚ÄúPolyp-PVT (w/o CIM)‚Äù. As shown in Tab. VIII, this vari-\nant performs worse than the overall Polyp-PVT. Specifically,\nremoving the CIM causes the mDic to decrease by 1.8% on\nEndoscene. Meanwhile, it is obvious that the lack of the CIM\nintroduces significant noise (please refer to Fig. 10). In order to\nfurther explore the internal of CIM, the feature visualizations\nof the two main configurations inside the CIM are shown in\nFig 9. It can be seen that the low-level features have a large\namount of detailed information. Still, the differences between\npolyps and other normal tissues cannot be mined directly from\nthis information. Thanks to the channel attention and spatial\nattention mechanism, information such as details and edges\nof polyps can be discerned from a large amount of redundant\ninformation.\nEffectiveness of SAM. Similarly, we test the effectiveness\nof the SAM module by removing it from the overall Polyp-\nPVT and replacing it with an element-wise addition operation,\nTABLE X\nABOUT THE ABLATION EXPERIMENTS OF THE POWERFUL ROTATION\nADAPTABILITY . ALL EXPERIMENTS ARE UNDER THE CONDITION OF\nLARGE ROTATION (15 DEGREES ).\nSetting Endoscene ClinicDB ColonDB ETIS Kvasir-SEG\nw/o GCN 0.857 0.909 0.756 0.667 0.894\nw/ Conv 0.865 0.898 0.789 0.719 0.893\nw/ GCN 0.874 0.929 0.806 0.744 0.915\nwhich is denoted as ‚ÄúPolyp-PVT (w/o SAM)‚Äù. The perfor-\nmance of the complete Polyp-PVT shows an improvement of\n2.9% and 3.1% in terms of mDic and mIoU, respectively, on\nColonDB. Fig. 10 shows the benefits of SAM more intuitively.\nIt is found that the lack of the SAM leads to more detailed\nerrors or even missed inspections. As reported in Tab IX,\nwe add more results on the GCN in the SAM module. The\nexperimental results further illustrate that GCN plays a key\nrole. The effect of the lack of GCN is significantly reduced,\nand the effect is improved after replacing it with convolution.\nStill, GCN can significantly exceed the capabilities of the\nconvolution module. The experimental results also verified\nthe importance of GCN‚Äôs large receptive field and rotation\ninsensitivity to polyp segmentation. The rotational robustness\nof GCN is stronger than convolutions. As shown in Tab X,\nunder the condition of large rotation (15 degrees), GCN has\nbetter adaptability to image rotation than convolutions. To\nfurther explore the role of SAM, we visualized P1 and P2,\nand the results of P1 and P2 are shown in Fig 11. Compared\nwith P1, P2 has higher reliability in error recognition and\nidentification of uncertain regions. This is mainly due to the\nlarge number of low-level details collected by CIM and mining\nlocal pixels and global semantic cues from the polyp area of\nSAM.\nG. Video Polyp Segmentation\nTo validate the superiority of the proposed model, we\nconduct experiments on the video polyp segmentation datasets.\nFor a fair comparison, we re-train our model with the same\ntraining datasets and use the same testing set as PNS-Net [64],\n[97]. We compare our model on three standard benchmarks\n(i.e., CVC-300-TV [96], CVC-612-T [8], and CVC-612-V [8])\nagainst six cutting-edge approaches, including U-Net [4],\n11\nImage GT Bas. w/o CFM w/o CIM w/o SAM Ours\nFig. 10. Visualization of the ablation study results, which are converted from the output into heat maps. As can be seen, removing any module leads to\nmissed or incorrectly detected results.\nTABLE XI\nTHE RESULT OF VIDEO POLYP SEGMENTATION ON THE i.e., CVC-612-T AND CVC-612-V.\nCVC-612-T [8] CVC-612-V [8]\nModel mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE\nMICCAI‚Äô15 U-Net 0.711 0.618 0.694 0.810 0.836 0.853 0.058 0.709 0.597 0.680 0.826 0.855 0.872 0.023\nTMI‚Äô19 UNet++ 0.697 0.603 0.688 0.800 0.817 0.865 0.059 0.668 0.557 0.642 0.805 0.830 0.846 0.025\nISM‚Äô19 ResUNet++ 0.616 0.512 0.604 0.727 0.758 0.760 0.084 0.750 0.646 0.717 0.829 0.877 0.879 0.023\nMICCAI‚Äô20 ACSNet 0.780 0.697 0.772 0.838 0.864 0.866 0.053 0.801 0.710 0.765 0.847 0.887 0.890 0.054\nMICCAI‚Äô20 PraNet 0.833 0.767 0.834 0.886 0.904 0.926 0.038 0.857 0.793 0.855 0.915 0.936 0.965 0.013\nMICCAI‚Äô21 PNS-Net 0.837 0.765 0.838 0.903 0.903 0.923 0.038 0.851 0.769 0.836 0.923 0.944 0.962 0.012\nPolyp-PVT (Ours) 0.846 0.776 0.850 0.895 0.908 0.926 0.037 0.882 0.810 0.874 0.924 0.963 0.967 0.012\nImage GT P1\n P2\nFig. 11. visualization of the P1 and P2 predictions.\nUNet++ [28], ResUNet++ [29], ACSNet [49], PraNet [5],\nPNS-Net [64], in Tab. XI and Tab. XII. Note that PNS-Net\nprovides all the prediction maps of the compared methods. As\nseen, our method is very competitive and far ahead of the best\nTABLE XII\nVIDEO POLYP SEGMENTATION RESULTS ON THE CVC-300-TV.\nCVC-300-TV [96]\nModel mDic mIoU Fw\nŒ≤ SŒ± mEŒæ maxEŒæ MAE\nU-Net 0.631 0.516 0.567 0.793 0.826 0.849 0.027\nUNet++ 0.638 0.527 0.581 0.796 0.831 0.847 0.024\nResUNet++ 0.533 0.410 0.469 0.703 0.718 0.720 0.052\nACSNet 0.732 0.627 0.703 0.837 0.871 0.875 0.016\nPraNet 0.716 0.624 0.700 0.833 0.852 0.904 0.016\nPNS-Net 0.813 0.710 0.778 0.909 0.921 0.942 0.013\nOurs 0.880 0.802 0.869 0.915 0.961 0.965 0.011\nexisting model, PNS-Net, by 3.1% and 6.7% on CVC-612-V\nand CVC-300-TV , respectively, in terms of mDice.\nH. Limitations\nAlthough the proposed Polyp-PVT model surpasses existing\nalgorithms, it still performs poorly in certain cases. We present\nsome failure cases in Fig. 12. As can be seen, one major\n12\nImage GT Ours SANet PraNet\nFig. 12. Visualization of some failure cases. Green indicates a correct polyp. Yellow is the missed polyp. Red is the wrong prediction.\nlimitation is the inability to detect accurate polyp boundaries\nwith overlapping light and shadow ( 1st row). Our model can\nidentify the location information of polyps (green mask in\n1st row), but it regards the light and shadow part of the\nedge as the polyp (red mask in 1st row). More deadly, our\nmodel incorrectly predicts the reflective point as a polyp (red\nmask in 2nd and 3rd rows). We notice that the reflective\npoints are very salient in the image. Therefore, we speculate\nthat the prediction may be based on only these points. More\nimportantly, we believe that a simple way is to convert the\ninput image into a gray image, which can eliminate the\nreflection and overlap of light and shadow to assist the model\nin judgment.\nV. C ONCLUSION\nIn this paper, we propose a new image polyp segmentation\nframework, named Polyp-PVT, which utilizes a pyramid vision\ntransformer backbone as the encoder to explicitly extract more\npowerful and robust features. Extensive experiments show that\nPolyp-PVT consistently outperforms all current cutting-edge\nmodels on five challenging datasets without any pre-/post-\nprocessing. In particular, for the unseen ColonDB dataset, the\nproposed model reaches a mean Dice score of above 0.8 for\nthe first time. Interestingly, we also surpass the current cutting-\nedge PNS-Net in terms of the video polyp segmentation\ntask, demonstrating excellent learning ability. Specifically, we\nobtain the above-mention achievements by introducing three\nsimple components, i.e., a cascaded fusion module (CFM),\na camouflage identification module (CIM), and a similarity\naggregation module (SAM), which effectively extract high and\nlow-level cues separately, and effectively fuse them for the\nfinal output. We hope this research will stimulate more novel\nideas for solving the polyp segmentation task.\nREFERENCES\n[1] M. Fiori, P. Mus ¬¥e, and G. Sapiro, ‚ÄúA complete system for candidate\npolyps detection in virtual colonoscopy,‚Äù IJPRAI, vol. 28, no. 07, p.\n1460014, 2014.\n[2] A. V . Mamonov, I. N. Figueiredo, P. N. Figueiredo, and Y .-H. R. Tsai,\n‚ÄúAutomated polyp detection in colon capsule endoscopy,‚Äù IEEE TMI ,\nvol. 33, no. 7, pp. 1488‚Äì1502, 2014.\n[3] O. H. Maghsoudi, ‚ÄúSuperpixel based segmentation and classification of\npolyps in wireless capsule endoscopy,‚Äù in IEEE SPMB, 2017.\n[4] O. Ronneberger, P. Fischer, and T. Brox, ‚ÄúU-net: Convolutional networks\nfor biomedical image segmentation,‚Äù in MICCAI, 2015.\n[5] D.-P. Fan, G.-P. Ji, T. Zhou, G. Chen, H. Fu, J. Shen, and L. Shao,\n‚ÄúPranet: Parallel reverse attention network for polyp segmentation,‚Äù in\nMICCAI, 2020.\n[6] X. Guo, C. Yang, Y . Liu, and Y . Yuan, ‚ÄúLearn to threshold: Thresholdnet\nwith confidence-guided manifold mixup for polyp segmentation,‚Äù IEEE\nTMI, vol. 40, no. 4, pp. 1134‚Äì1146, 2020.\n[7] J. Wei, Y . Hu, R. Zhang, Z. Li, S. K. Zhou, and S. Cui, ‚ÄúShallow\nattention network for polyp segmentation,‚Äù in MICCAI, 2021.\n[8] J. Bernal, F. J. S ¬¥anchez, G. Fern ¬¥andez-Esparrach, D. Gil, C. Rodr ¬¥ƒ±guez,\nand F. Vilari Àúno, ‚ÄúWm-dova maps for accurate polyp highlighting in\ncolonoscopy: Validation vs. saliency maps from physicians,‚Äù CMIG,\nvol. 43, pp. 99‚Äì111, 2015.\n[9] J. Silva, A. Histace, O. Romain, X. Dray, and B. Granado, ‚ÄúToward\nembedded detection of polyps in wce images for early diagnosis of\ncolorectal cancer,‚Äù IJCARS, vol. 9, no. 2, pp. 283‚Äì293, 2014.\n[10] N. Tajbakhsh, S. R. Gurudu, and J. Liang, ‚ÄúAutomated polyp detection\nin colonoscopy videos using shape and context information,‚Äù IEEE TMI,\nvol. 35, no. 2, pp. 630‚Äì644, 2015.\n[11] D.-P. Fan, G.-P. Ji, M.-M. Cheng, and L. Shao, ‚ÄúConcealed object\ndetection,‚Äù IEEE TPAMI, 2021.\n[12] D.-P. Fan, G.-P. Ji, G. Sun, M.-M. Cheng, J. Shen, and L. Shao,\n‚ÄúCamouflaged object detection,‚Äù in CVPR, 2020.\n13\n[13] D. Jha, P. H. Smedsrud, M. A. Riegler, P. Halvorsen, T. de Lange,\nD. Johansen, and H. D. Johansen, ‚ÄúKvasir-seg: A segmented polyp\ndataset,‚Äù in MMM, 2020.\n[14] D. V ¬¥azquez, J. Bernal, F. J. S ¬¥anchez, G. Fern ¬¥andez-Esparrach, A. M.\nL¬¥opez, A. Romero, M. Drozdzal, and A. Courville, ‚ÄúA benchmark for\nendoluminal scene segmentation of colonoscopy images,‚Äù JHE, vol.\n2017, 2017.\n[15] T. Rahim, M. A. Usman, and S. Y . Shin, ‚ÄúA survey on contemporary\ncomputer-aided tumor, polyp, and ulcer detection methods in wireless\ncapsule endoscopy imaging,‚Äù CMIG, p. 101767, 2020.\n[16] K. He, X. Zhang, S. Ren, and J. Sun, ‚ÄúDeep residual learning for image\nrecognition,‚Äù in CVPR, 2016.\n[17] K. Simonyan and A. Zisserman, ‚ÄúVery deep convolutional networks for\nlarge-scale image recognition,‚Äù in ICLR, 2015.\n[18] X. Li, W. Wang, X. Hu, and J. Yang, ‚ÄúSelective kernel networks,‚Äù in\nCVPR, 2019.\n[19] W. Wang, X. Li, T. Lu, and J. Yang, ‚ÄúMixed link networks,‚Äù in IJCAI,\n2018.\n[20] J. Long, E. Shelhamer, and T. Darrell, ‚ÄúFully convolutional networks\nfor semantic segmentation,‚Äù in CVPR, 2015.\n[21] L. Cai, M. Wu, L. Chen, W. Bai, M. Yang, S. Lyu, and Q. Zhao, ‚ÄúUsing\nguided self-attention with local information for polyp segmentation,‚Äù in\nMICCAI. Springer, 2022.\n[22] N. K. Tomar, D. Jha, U. Bagci, and S. Ali, ‚ÄúTganet: Text-guided attention\nfor improved polyp segmentation,‚Äù in MICCAI. Springer, 2022.\n[23] R. Zhang, P. Lai, X. Wan, D.-J. Fan, F. Gao, X.-J. Wu, and G. Li,\n‚ÄúLesion-aware dynamic kernel for polyp segmentation,‚Äù in MICCAI.\nSpringer, 2022.\n[24] J.-H. Shi, Q. Zhang, Y .-H. Tang, and Z.-Q. Zhang, ‚ÄúPolyp-mixer: An\nefficient context-aware mlp-based paradigm for polyp segmentation,‚Äù\nIEEE TCSVT, 2022.\n[25] X. Zhao, Z. Wu, S. Tan, D.-J. Fan, Z. Li, X. Wan, and G. Li,\n‚ÄúSemi-supervised spatial temporal attention network for video polyp\nsegmentation,‚Äù in MICCAI. Springer, 2022.\n[26] M. Akbari, M. Mohrekesh, E. Nasr-Esfahani, S. R. Soroushmehr,\nN. Karimi, S. Samavi, and K. Najarian, ‚ÄúPolyp segmentation in\ncolonoscopy images using fully convolutional network,‚Äù inIEEE EMBC,\n2018.\n[27] P. Brandao, O. Zisimopoulos, E. Mazomenos, G. Ciuti, J. Bernal,\nM. Visentini-Scarzanella, A. Menciassi, P. Dario, A. Koulaouzidis,\nA. Arezzo et al. , ‚ÄúTowards a computed-aided diagnosis system in\ncolonoscopy: automatic polyp segmentation using convolution neural\nnetworks,‚Äù JMRR, vol. 3, no. 02, p. 1840002, 2018.\n[28] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, ‚ÄúUnet++: A\nnested u-net architecture for medical image segmentation,‚Äù in DLMIA,\n2018.\n[29] D. Jha, P. H. Smedsrud, M. A. Riegler, D. Johansen, T. de Lange,\nP. Halvorsen, and H. D. Johansen, ‚ÄúResunet++: An advanced architecture\nfor medical image segmentation,‚Äù in IEEE ISM, 2019.\n[30] X. Sun, P. Zhang, D. Wang, Y . Cao, and B. Liu, ‚ÄúColorectal polyp\nsegmentation by u-net with dilation convolution,‚Äù inIEEE ICMLA, 2019.\n[31] B. Murugesan, K. Sarveswaran, S. M. Shankaranarayana, K. Ram,\nJ. Joseph, and M. Sivaprakasam, ‚ÄúPsi-net: Shape and boundary aware\njoint multi-task deep network for medical image segmentation,‚Äù in IEEE\nEMBC, 2019.\n[32] H. A. Qadir, Y . Shin, J. Solhusvik, J. Bergsland, L. Aabakken, and\nI. Balasingham, ‚ÄúPolyp detection and segmentation using mask r-cnn:\nDoes a deeper feature extractor cnn always perform better?‚Äù in ISMICT,\n2019.\n[33] K. He, G. Gkioxari, P. Doll ¬¥ar, and R. Girshick, ‚ÄúMask r-cnn,‚Äù in ICCV,\n2017.\n[34] S. Alam, N. K. Tomar, A. Thakur, D. Jha, and A. Rauniyar, ‚ÄúAutomatic\npolyp segmentation using u-net-resnet50,‚Äù in MediaEvalW, 2020.\n[35] D. Banik, K. Roy, D. Bhattacharjee, M. Nasipuri, and O. Krejcar,\n‚ÄúPolyp-net: A multimodel fusion network for polyp segmentation,‚ÄùIEEE\nTIM, vol. 70, pp. 1‚Äì12, 2020.\n[36] T. Rahim, S. A. Hassan, and S. Y . Shin, ‚ÄúA deep convolutional neural\nnetwork for the detection of polyps in colonoscopy images,‚Äù BSPC,\nvol. 68, p. 102654, 2021.\n[37] D. Jha, S. Ali, N. K. Tomar, H. D. Johansen, D. Johansen, J. Rittscher,\nM. A. Riegler, and P. Halvorsen, ‚ÄúReal-time polyp detection, localization\nand segmentation in colonoscopy using deep learning,‚Äù IEEE Access ,\nvol. 9, pp. 40 496‚Äì40 510, 2021.\n[38] A. M. A. Ahmed, ‚ÄúGenerative adversarial networks for automatic polyp\nsegmentation,‚Äù in MediaEvalW, 2020.\n[39] V . Thambawita, S. Hicks, P. Halvorsen, and M. A. Riegler, ‚ÄúPyramid-\nfocus-augmentation: Medical image segmentation with step-wise focus,‚Äù\nin MediaEvalW, 2020.\n[40] N. K. Tomar, D. Jha, S. Ali, H. D. Johansen, D. Johansen, M. A. Riegler,\nand P. Halvorsen, ‚ÄúDdanet: Dual decoder attention network for automatic\npolyp segmentation,‚Äù in ICPRW, 2021.\n[41] C.-H. Huang, H.-Y . Wu, and Y .-L. Lin, ‚ÄúHardnet-mseg: A simple\nencoder-decoder polyp segmentation neural network that achieves over\n0.9 mean dice and 86 fps,‚Äù arXiv preprint arXiv:2101.07172 , 2021.\n[42] P. Chao, C.-Y . Kao, Y .-S. Ruan, C.-H. Huang, and Y .-L. Lin, ‚ÄúHardnet:\nA low memory traffic network,‚Äù in CVPR, 2019.\n[43] Y . Zhang, H. Liu, and Q. Hu, ‚ÄúTransfuse: Fusing transformers and cnns\nfor medical image segmentation,‚Äù in MICCAI, 2021.\n[44] Z. Yin, K. Liang, Z. Ma, and J. Guo, ‚ÄúDuplex contextual relation\nnetwork for polyp segmentation,‚Äù in IEEE ISBI, 2022.\n[45] Z. Xiaoqi, Z. Lihe, and L. Huchuan, ‚ÄúAutomatic polyp segmentation via\nmulti-scale subtraction network,‚Äù in MICCAI, 2021.\n[46] Z. Zhou, J. Shin, L. Zhang, S. Gurudu, M. Gotway, and J. Liang, ‚ÄúFine-\ntuning convolutional neural networks for biomedical image analysis:\nactively and incrementally,‚Äù in CVPR, 2017.\n[47] N. Tajbakhsh, J. Y . Shin, S. R. Gurudu, R. T. Hurst, C. B. Kendall, M. B.\nGotway, and J. Liang, ‚ÄúConvolutional neural networks for medical image\nanalysis: Full training or fine tuning?‚Äù IEEE TMI , vol. 35, no. 5, pp.\n1299‚Äì1312, 2016.\n[48] X. Xie, J. Chen, Y . Li, L. Shen, K. Ma, and Y . Zheng, ‚ÄúMi 2gan:\nGenerative adversarial network for medical image domain adaptation\nusing mutual information constraint,‚Äù in MICCAI, 2020.\n[49] R. Zhang, G. Li, Z. Li, S. Cui, D. Qian, and Y . Yu, ‚ÄúAdaptive context\nselection for polyp segmentation,‚Äù in MICCAI, 2020.\n[50] N. K. Tomar, ‚ÄúAutomatic polyp segmentation using fully convolutional\nneural network,‚Äù in MediaEvalW, 2020.\n[51] D. Jha, S. Hicks, K. Emanuelsen, H. D. Johansen, D. Johansen,\nT. de Lange, M. A. Riegler, and P. Halvorsen, ‚ÄúMedico multimedia task\nat mediaeval 2020: Automatic polyp segmentation,‚Äù in MediaEvalW,\n2020.\n[52] K. Patel, A. M. Bur, and G. Wang, ‚ÄúEnhanced u-net: A feature\nenhancement network for polyp segmentation,‚Äù in CRV, 2021.\n[53] A. Lumini, L. Nanni, and G. Maguolo, ‚ÄúDeep ensembles based on\nstochastic activation selection for polyp segmentation,‚Äù in MIDL, 2021.\n[54] M. V . Branch and A. S. Carvalho, ‚ÄúPolyp segmentation in colonoscopy\nimages using u-net-mobilenetv2,‚Äù arXiv preprint arXiv:2103.15715 ,\n2021.\n[55] R. Khadga, D. Jha, S. Ali, S. Hicks, V . Thambawita, M. A. Riegler,\nand P. Halvorsen, ‚ÄúFew-shot segmentation of medical images based on\nmeta-learning with implicit gradients,‚Äù arXiv preprint arXiv:2106.03223,\n2021.\n[56] D. V . Sang, T. Q. Chung, P. N. Lan, D. V . Hang, D. Van Long, and N. T.\nThuy, ‚ÄúAg-curesnest: A novel method for colon polyp segmentation,‚Äù in\nIEEE RIVF, 2021.\n[57] C. Yang, X. Guo, M. Zhu, B. Ibragimov, and Y . Yuan, ‚ÄúMutual-prototype\nadaptation for cross-domain polyp segmentation,‚Äù IEEE JBHI, 2021.\n[58] D. Jha, P. H. Smedsrud, D. Johansen, T. de Lange, H. D. Johansen,\nP. Halvorsen, and M. A. Riegler, ‚ÄúA comprehensive study on colorectal\npolyp segmentation with resunet++, conditional random field and test-\ntime augmentation,‚Äù IEEE JBHI, vol. 25, no. 6, pp. 2029‚Äì2040, 2021.\n[59] D. Jha, N. K. Tomar, S. Ali, M. A. Riegler, H. D. Johansen, D. Johansen,\nT. de Lange, and P. Halvorsen, ‚ÄúNanonet: Real-time polyp segmentation\nin video capsule endoscopy and colonoscopy,‚Äù in IEEE CBMS, 2021.\n[60] S. Li, X. Sui, X. Luo, X. Xu, L. Yong, and R. S. M. Goh, ‚ÄúMedical im-\nage segmentation using squeeze-and-expansion transformers,‚Äù in IJCAI,\n2021.\n[61] T. Kim, H. Lee, and D. Kim, ‚ÄúUacanet: Uncertainty augmented context\nattention for polyp semgnetaion,‚Äù in ACM MM, 2021.\n[62] V . Thambawita, S. A. Hicks, P. Halvorsen, and M. A. Riegler, ‚ÄúDiver-\ngentnets: Medical image segmentation by network ensemble,‚Äù in ISBI\n& EndoCV, 2021.\n[63] G. Xiaoqing, Y . Chen, and Y . Yixuan, ‚ÄúDynamic-weighting hierarchical\nsegmentation network for medical images,‚Äù MIA, p. 102196, 2021.\n[64] G.-P. Ji, Y .-C. Chou, D.-P. Fan, G. Chen, D. Jha, H. Fu, and L. Shao,\n‚ÄúPns-net: Progressively normalized self-attention network for video\npolyp segmentation,‚Äù in MICCAI, 2021.\n[65] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù in NeurIPS,\n2017.\n[66] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n14\n‚ÄúAn image is worth 16x16 words: Transformers for image recognition\nat scale,‚Äù in ICLR, 2021.\n[67] Z. Pan, B. Zhuang, J. Liu, H. He, and J. Cai, ‚ÄúScalable visual trans-\nformers with hierarchical pooling,‚Äù in ICCV, 2021.\n[68] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, ‚ÄúRethinking\nspatial dimensions of vision transformers,‚Äù in ICCV, 2021.\n[69] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. Tay, J. Feng,\nand S. Yan, ‚ÄúTokens-to-token vit: Training vision transformers from\nscratch on imagenet,‚Äù in ICCV, 2021.\n[70] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, ‚ÄúTransformer\nin transformer,‚Äù Advances in Neural Information Processing Systems ,\nvol. 34, pp. 15 908‚Äì15 919, 2021.\n[71] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, ‚ÄúPyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,‚Äù in ICCV, 2021.\n[72] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, ‚ÄúPvt v2: Improved baselines with pyramid vision transformer,‚Äù\nCVMJ, vol. 8, no. 3, pp. 415‚Äì424, 2022.\n[73] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, ‚ÄúSwin transformer: Hierarchical vision transformer using shifted\nwindows,‚Äù in ICCV, 2021.\n[74] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n‚ÄúCvt: Introducing convolutions to vision transformers,‚Äù in ICCV, 2021.\n[75] W. Xu, Y . Xu, T. Chang, and Z. Tu, ‚ÄúCo-scale conv-attentional image\ntransformers,‚Äù in ICCV, 2021.\n[76] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and\nC. Shen, ‚ÄúTwins: Revisiting the design of spatial attention in vision\ntransformers,‚Äù Advances in Neural Information Processing Systems ,\nvol. 34, pp. 9355‚Äì9366, 2021.\n[77] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. J ¬¥egou,\nand M. Douze, ‚ÄúLevit: a vision transformer in convnet‚Äôs clothing for\nfaster inference,‚Äù in ICCV, 2021.\n[78] S. Bhojanapalli, A. Chakrabarti, D. Glasner, D. Li, T. Unterthiner,\nand A. Veit, ‚ÄúUnderstanding robustness of transformers for image\nclassification,‚Äù in ICCV, 2021.\n[79] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n‚ÄúSegformer: Simple and efficient design for semantic segmentation\nwith transformers,‚Äù Advances in Neural Information Processing Systems,\nvol. 34, pp. 12 077‚Äì12 090, 2021.\n[80] Z. Wu, L. Su, and Q. Huang, ‚ÄúCascaded partial decoder for fast and\naccurate salient object detection,‚Äù in CVPR, 2019.\n[81] S. Ioffe and C. Szegedy, ‚ÄúBatch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,‚Äù in ICML, 2015.\n[82] X. Glorot, A. Bordes, and Y . Bengio, ‚ÄúDeep sparse rectifier neural\nnetworks,‚Äù in AISTATS, 2011.\n[83] S. Woo, J. Park, J.-Y . Lee, and I. So Kweon, ‚ÄúCbam: Convolutional\nblock attention module,‚Äù in ECCV, 2018.\n[84] J. Hu, L. Shen, and G. Sun, ‚ÄúSqueeze-and-excitation networks,‚Äù in\nCVPR, 2018.\n[85] X. Wang, R. Girshick, A. Gupta, and K. He, ‚ÄúNon-local neural net-\nworks,‚Äù in CVPR, 2018.\n[86] G. Te, Y . Liu, W. Hu, H. Shi, and T. Mei, ‚ÄúEdge-aware graph represen-\ntation learning and reasoning for face parsing,‚Äù in ECCV, 2020.\n[87] Y . Lu, Y . Chen, D. Zhao, and J. Chen, ‚ÄúGraph-fcn for image semantic\nsegmentation,‚Äù in ISNN, 2019.\n[88] J. Wei, S. Wang, and Q. Huang, ‚ÄúF 3net: Fusion, feedback and focus for\nsalient object detection,‚Äù in AAAI, 2020.\n[89] I. Loshchilov and F. Hutter, ‚ÄúDecoupled weight decay regularization,‚Äù\nin ICLR, 2019.\n[90] F. Milletari, N. Navab, and S.-A. Ahmadi, ‚ÄúV-net: Fully convolutional\nneural networks for volumetric medical image segmentation,‚Äù in 3DV,\n2016.\n[91] R. Margolin, L. Zelnik-Manor, and A. Tal, ‚ÄúHow to evaluate foreground\nmaps?‚Äù in CVPR, 2014.\n[92] M.-M. Chen and D.-P. Fan, ‚ÄúStructure-measure: A new way to evaluate\nforeground maps,‚Äù IJCV, vol. 129, pp. 2622‚Äì2638, 2021.\n[93] D.-P. Fan, G.-P. Ji, X. Qin, and M.-M. Cheng, ‚ÄúCognitive vision inspired\nobject segmentation metric and loss function,‚Äù SSI, 2021.\n[94] D.-P. Fan, C. Gong, Y . Cao, B. Ren, M.-M. Cheng, and A. Borji,\n‚ÄúEnhanced-alignment measure for binary foreground map evaluation,‚Äù\nin IJCAI, 2018.\n[95] Y . Fang, C. Chen, Y . Yuan, and K.-y. Tong, ‚ÄúSelective feature aggre-\ngation network with area-boundary constraints for polyp segmentation,‚Äù\nin MICCAI, 2019.\n[96] J. Bernal, J. S ¬¥anchez, and F. Vilarino, ‚ÄúTowards automatic polyp\ndetection with a polyp appearance model,‚Äù PR, vol. 45, no. 9, pp. 3166‚Äì\n3182, 2012.\n[97] G.-P. Ji, G. Xiao, Y .-C. Chou, D.-P. Fan, K. Zhao, G. Chen, and\nL. Van Gool, ‚ÄúVideo polyp segmentation: A deep learning perspective,‚Äù\nMIR, vol. 19, no. 06, pp. 531‚Äì549, 2022.\nNordic Machine Intelligence, vol. 10, pp. 11‚Äì13, 2021\nReceived 1 Jan 2021 / published 21 Jul 2021\nhttps://doi.org/10.2478/nmi-some-doi\nThis is your template for Nordic\nMachine Intelligence\nBo Dong1, Wenhai Wang2 and Jinpeng Li3\n1. College of Biomedical Engineering & Instrument Science, Zhejiang University, Zhejiang, China.E\n2. Inception Institute of Artificial Intelligence, Abu Dhabi, UAE\n3. E-mail any correspondence to: myname@domain.com\nAbstract\nWe present our solutions to the MedAI: Transparency in Medi-\ncalImageSegmentation, forallthethreetasks: polypsegmen-\ntation task, instrument segmentation task, transparency task.\nWe use the same pipeline to process the two segmentation\ntasks of polyps and surgery, and use the same pre-processing\nand post-processing operations. The key improvement over\nlast year is newer state-of-the-art vision architectures, espe-\nciallytransformerswhichsignificantlyoutperformConvNetsfor\nthe medical image segmentation task. Our solution consists of\nmulti-model fusion, and each model uses a transformer as the\nbackbone network. We finished the all the tasks and get the\nbest score of 0.91 in instrument segmentation task and 0.83\nin polyp segmentation task. Meanwhile, we provide complete\nsolutions in https://github.com/dongbo811/MedAI-2021.\nKeywords: artificial intelligence; machine learning; deep\nlearning; robotics\nIntroduction\nColorectal cancer is one of the deadly cancers in the\nworld. Colonoscopy is the standard treatment to check,\nlocateandremovecolorectalpolyps. However, ithasbeen\nshown that the missed diagnosis rate of colorectal polyps\nduring colonoscopy is between 6% and 27%. The use\nof automatic, accurate and real-time polyp segmentation\nduring colonoscopy can help clinicians eliminate missing\nlesions and prevent the further development of colorectal\ncancer.\nIn recent years, significant progress has been seen\nin robot-assisted surgery and computer-assisted surgery.\nThe segmentation of surgical instruments can accurately\nlocate robotic instruments and estimate their posture,\nwhich is essential for the navigation of surgical robots.\nIn addition, the segmentation results can be used to\npredict dangerous operations and reduce surgical risks.\nAt the same time, it can provide a variety of automated\nsolutions for postoperative work, such as objective\nskill evaluation, surgical report generation, and surgical\nprocess optimization, which are of great significance to\nclinical work.\nModel\nThe method in this paper integrates three models for\nfusion, namely Polyp-PVT, Sinv2-PVT, and Transfuse-\nPVT. The official Polyp-PVT [dong2021PolypPVT] is\ndesigned for polyp segmentation and achieves SOTA seg-\nmentation capabilities and generalization performance.\nIt uses transformer as the backbone network to ex-\ntract richer features and solves the impact of colorec-\ntal image acquisition. Here, we adopt the standard\nstructure without any modification. For the Trans-\nfuse [zhang2021transfuse], which is also adopted in\nthe polyp segmentation, we improve it by replacing\nthe transformer part with the PVT [wang2021pyramid,\nwang2021pvtv2] to enhance its performance. The\nofficial Sinv2 [fan2021concealed] proposes an end-to-\nend network for search and recognition the concealed\nObject, which obtains considerable segmentation per-\nformance. This task is similar to the polyp seg-\nmentation and surgical instrument segmentation, so\nwe adopt. Here we employ a stronger PVT trans-\nformer [wang2021pyramid, wang2021pvtv2] to replace\nthe original res2net [gao2019res2net] backbone to ex-\ntract more powerful features.\nHyperparameter settings\nWe use the PyTorch framework to implement our model,\nand use Tesla V100 to accelerate the calculation. Taking\nintoaccountthedifferenceinthesizeofeachpolypimage,\nwe adopted a multi-scale strategy in the training phase.\nThe hyperparameter details are as follows. To update\nthe network parameters, we use the AdamW optimizer,\nwhichiswidelyusedintransformernetworks. Thelearning\nrate is set to 1e-4, and the weight attenuation is also\n¬© 2021 Author(s). This is an open access article licensed under the Creative Commons Attribution License 4.0.\n(http://creativecommons.org/licenses/by/4.0/).\narXiv:2108.06932v8  [eess.IV]  19 Feb 2024\nAuthor et al.: Short version of title. NMI, 10, 11‚Äì13, 2021\nTable 1: 5-fold cross-validation results of surgical instrument segmentation. Intersection-Over-Union (Jaccard index),\nDice coefficient, and the pixel accuracy are abbreviated as IoU, Dice, PA respectively.\nmodels Metrics Flod-1 Flod-2 Flod-3 Flod-4 Flod-5 Mean\nPolyp-PVT\nIoU 0.9012 0.8744 0.9310 0.9529 0.9522 0.9224\nDice 0.9427 0.9226 0.9616 0.9754 0.9748 0.9554\nPA 0.9915 0.9900 0.9939 0.9937 0.9934 0.9925\nSinv2-PVT\nIoU 0.8971 0.8760 0.9315 0.9521 0.9519 0.9217\nDice 0.9392 0.9237 0.9622 0.9751 0.9745 0.9549\nPA 0.9908 0.9897 0.9940 0.9936 0.9934 0.9923\nTransfuse-PVT\nIoU 0.9034 0.8691 0.9313 0.9497 0.9511 0.9209\nDice 0.9443 0.9207 0.9611 0.9734 0.9742 0.9547\nPA 0.9917 0.9891 0.9931 0.9925 0.9930 0.9919\nPrediction Prediction\nFigure 1: Qualitative description of our predication.\nadjusted to 1e-4. In addition, we adjusted the size of\nthe input image to 352√ó352, the mini-batch size was\n16, and it lasted 100 epochs. The total training time\nis close to 4 hours to achieve the best (for example,\n40 epochs) performance. For testing, we only adjust\nthe image size to 352√ó352 without any post-processing\noptimization strategy. It is worth noting that all three\nmodels use the same hyperparameter settings.\nTraining stage:\nWe use the same training strategy to train Polyp-PVT,\nSinv2-PVT, and Transfuse-PVT. Specifically, we first\ndivide the dataset into 5 groups (folds). At each time,\nwe use 4 groups of dataset as the training set and the\nremaining one as the validation set. During training, we\nkeepthemodelweightswiththebestIoUonthevalidation\nset. After 5-fold training, all of Polyp-PVT, Sinv2-PVT,\nand Transfuse-PVT have 5 model weights, and a total of\n15 different model weights are obtained.\nInference stage\nIn the inference stage, for the input images, we only resize\nthe images to352 √ó352 without any data enhancement.\nFortheoutput, weupsampleittotheoriginalfeaturesize.\nSo, we can obtain 15 different prediction results of test\ndataset without any data enhancement.\nIn order to obtain a more stable prediction, we\nmerge the 15 prediction results with a minority voting\nmethod. Because the voting strategy will produce many\nPrediction\nPrediction\nPrediction Prediction\nFigure 2: Some failure cases in surgical instrument\nsegmentation\nindependent noise pixels on the edge of polyps or surgical\ninstruments. We use open operation and counting to\nremove. First, the open operation is used to remove\nindependent noise points, and then the area of the block\nin the prediction image is counted to remove relatively\nsmall noise blocks in the prediction image, so as to obtain\nthe final prediction result.\nResult\nWe show the qualitative results in Fig. 1, and give the\nresults of the evaluations in Tab. 1. At the same time,\nwe shared our failure cases in Fig. 2.\nDiscussion\nIt can be found that both of our improved algorithms\nSinv2-PVT and Transfuse-PVT have the same perfor-\nmance as polyp-pvt. In the 5-fold cross-validation, the\nthree results are relatively stable. There are results above\n0.92 on the IoU, and we give some visual results in 1.\nHowever, there are certain shortcomings shown in 2.\nOur results almost correctly segment the equipment, but\nintroduce some noise. One characteristic of these noises\nis that they are biased towards black. This is mainly\ndue to the fact that there are more black instruments\nin the entire data set. Therefore, small black areas (food\nresiduals, etc.) in the colonoscope will be identified as\nsurgical instruments. These noises can be filtered by the\n12\nAuthor et al.: Short version of title. NMI, 10, 11‚Äì13, 2021\nsize of the area to achieve the purpose of noise removal.\nConclusion\nIn this paper, we propose a robust generalized medical\nimage segmentation framework, which is composed of\nmultiple models, and uses the pyramid vision transformer\nbackboneasanencodertoexplicitlyextractmorepowerful\nand powerful features.\nAcknowledgments\nI would first like to thank the researcher Dengping Fan\nin IIAI, whose expertise was invaluable in formulating the\nresearch questions and methodology.\nConflict of interest\nAuthors state no conflict of interest. (Either keep this\nsentence or describe any comflict of interest.)\n13",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.6108114719390869
    },
    {
      "name": "Computer vision",
      "score": 0.6007756590843201
    },
    {
      "name": "Segmentation",
      "score": 0.5974014401435852
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.566136360168457
    },
    {
      "name": "Computer science",
      "score": 0.44214388728141785
    },
    {
      "name": "Mathematics",
      "score": 0.13415169715881348
    },
    {
      "name": "Geometry",
      "score": 0.04671528935432434
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116052",
      "name": "Inception Institute of Artificial Intelligence",
      "country": "AE"
    }
  ]
}