{
  "title": "TEASEL: A Transformer-Based Speech-Prefixed Language Model",
  "url": "https://openalex.org/W3200069083",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2268602084",
      "name": "Arjmand Mehdi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289620459",
      "name": "Dousti, Mohammad Javad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2249714624",
      "name": "Moradi, Hadi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093051361",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W3128412859",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2962931510",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2808631503",
    "https://openalex.org/W2753840835",
    "https://openalex.org/W2990629080",
    "https://openalex.org/W3037572520",
    "https://openalex.org/W3176724088",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2465534249",
    "https://openalex.org/W2798965674",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963349408",
    "https://openalex.org/W3094498998",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3088209263",
    "https://openalex.org/W2738581557",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2808359495",
    "https://openalex.org/W2937328183",
    "https://openalex.org/W2619383789",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2883409523",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2962718314",
    "https://openalex.org/W3049723069",
    "https://openalex.org/W3148757058",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3177174258",
    "https://openalex.org/W2122563357",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2767249564",
    "https://openalex.org/W1494198834"
  ],
  "abstract": "Multimodal language analysis is a burgeoning field of NLP that aims to simultaneously model a speaker's words, acoustical annotations, and facial expressions. In this area, lexicon features usually outperform other modalities because they are pre-trained on large corpora via Transformer-based models. Despite their strong performance, training a new self-supervised learning (SSL) Transformer on any modality is not usually attainable due to insufficient data, which is the case in multimodal language learning. This work proposes a Transformer-Based Speech-Prefixed Language Model called TEASEL to approach the mentioned constraints without training a complete Transformer model. TEASEL model includes speech modality as a dynamic prefix besides the textual modality compared to a conventional language model. This method exploits a conventional pre-trained language model as a cross-modal Transformer model. We evaluated TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset. Extensive experiments show that our model outperforms unimodal baseline language models by 4% and outperforms the current multimodal state-of-the-art (SoTA) model by 1% in F1-score. Additionally, our proposed method is 72% smaller than the SoTA model.",
  "full_text": "TEASEL : A TRANSFORMER-BASED SPEECH-PREFIXED LANGUAGE MODEL\nMehdi Arjmand Mohammad Javad Dousti Hadi Moradi\nUniversity of Tehran\nABSTRACT\nMultimodal language analysis is a burgeoning ﬁeld of NLP\nthat aims to simultaneously model a speaker’s words, acous-\ntical annotations, and facial expressions. In this area, lex-\nicon features usually outperform other modalities because\nthey are pre-trained on large corpora via Transformer-based\nmodels. Despite their strong performance, training a new\nself-supervised learning (SSL) Transformer on any modal-\nity is not usually attainable due to insufﬁcient data, which\nis the case in multimodal language learning. This work pro-\nposes a Transformer-Based Speech-Preﬁxed Language Model\ncalled T EASEL to approach the mentioned constraints with-\nout training a complete Transformer model. T EASEL model\nincludes speech modality as a dynamic preﬁx besides the\ntextual modality compared to a conventional language model.\nThis method exploits a conventional pre-trained language\nmodel as a cross-modal Transformer model. We evaluated\nTEASEL for the multimodal sentiment analysis task deﬁned\nby CMU-MOSI dataset. Extensive experiments show that our\nmodel outperforms unimodal baseline language models by\n4% and outperforms the current multimodal state-of-the-art\n(SoTA) model by 1% in F1-score. Additionally, our proposed\nmethod is 72% smaller than the SoTA model.\n1. INTRODUCTION\nHuman language is not limited to communicating with words;\nit employs acoustical annotations, body and head movements,\nand facial expressions besides using words to reinforce our\nintentions. For instance, people may strengthen their oppos-\ning viewpoints by emphasizing words or changing their vocal\npitch. With the growth of social media data and the essential\nrole of additional modalities in expressing an opinion, a new\nresearch area in Natural Language Processing (NLP) called\nmultimodal language analysis has emerged [1]. This area in-\ncludes multimodal sentiment analysis [2, 3] and multimodal\nemotion recognition [4, 5].\nIn multimodal language analysis, different methods aim\nto combine heterogeneous sources of unimodal features.\nThese methods leverage a spectrum of learning tasks, such as\nsupervised [6], multitask learning [7], reinforcement learn-\nings [8], and self-supervised [9, 7] approaches. However, in\nmost cases, the textual modality outperforms other modali-\nties; This superiority is mainly because the textual modalities\nText\nSpeech\nCNN Pre-trained \nBERT-style \nTransformer \nLanguage \nModel\nTokenizer\nxL \nlayers\nSentiment \nPredictionLightweight\nAttentive \nAggregation \nBackward\nForward\nBackward\nForward\nForward\nFig. 1: Process of ﬁne-tuning T EASEL model on multimodal\nsentiment analysis task. T EASEL receives raw speech and\ntext modalities as input and feed the speech preﬁxes and tex-\ntual tokens to a pre-trained BERT-style language model. The\n[CLS] output token will be used for ﬁne-tuning.\nare mainly from large language models pre-trained on huge\ncorpora, while speech and visual features mainly use feature\nengineering-based approaches.\nRecent large language models use Self-Supervised Learn-\ning (SSL) and Transformer-based [10] methods utilizing\nlarge number of data points for pre-training in textual [11,\n12, 13, 14], speech[15], vision [16] and multimodal visual-\ntextual settings [17]. Collecting large amount of data is not\neasily attainable for most multimodal contexts, particularly\nmultimodal language. Also, adding a new modality would\nrequire new data collection and SSL pre-training, which is\nnot commonly feasible.\nThis work proposes aTransformer-Based Speech-Preﬁxed\nLanguage Model (TEASEL ) as an approach to generalize a\npre-trained language model as a cross-modal attention mod-\nule. T EASEL introduces speech as a low-resource modality\nto a pre-trained Transformer-based language model without\nthe need of pre-training the whole Transformer. In this ap-\nproach, we pre-train a lightweight spatial encoder for speech\nto represent the speech modality as dynamic preﬁxes to a pre-\ntrained language model and ﬁne-tune this module besides the\nTransformer encoder layers in a downstream task. With this\napproach, T EASEL model would exploit the pre-trained lan-\nguage model as a cross-modal multi-head attention during the\npre-training and ﬁne-tuning to a downstream task. T EASEL\ndoes not require any data alignment. This is a key beneﬁt in\nmultimodal language analysis, where data alignment results\nin losing crucial intramodal information [6, 7].\nWe evaluate TEASEL for the multimodal sentiment analy-\narXiv:2109.05522v1  [cs.CL]  12 Sep 2021\nsis task on CMU-MOSI [18] dataset. TEASEL shows 4% per-\ncent improvement over the unimodal baseline and state-of-\nthe-art (SoTA) performance gain of 1% compared to the cur-\nrent multimodal SoTA using extensive experiments. TEASEL\ndoes not need a fully-trained modality-speciﬁc Transformer\nand achieves better performance than late fusing of two fully-\ntrained Transformers in a low-resource setting such as mul-\ntimodal sentiment analysis. Moreover, T EASEL uses approx-\nimately 72% fewer parameters compared to late fusing ap-\nproaches.\n2. RELATED WORK\n2.1. Human Multimodal Language Analysis\nMultimodal language analysis is a modern ﬁeld of study in\nNLP which intends to model language interactions through\ndifferent parallel simultaneous modalities. These multimodal\nstudies typically contain text, audio, and visual features. Mul-\ntimodal human language analysis applications include, but are\nnot limited to, sentiment analysis [18], emotion recognition\n[5, 19], humor detection [20], and sarcasm detection [21].\n[22] has deﬁned representations, translation, alignment,\nfusion, and co-learning as ﬁve core challenges in multimodal\nmachine learning. Among these challenges, [22] have de-\nscribed fusion as the concept of integrating information from\nmultiple modalities to predict an outcome measure and clas-\nsiﬁes the fusion methods as early fusion, hybrid fusions, and\nlate fusion.\nWith the success of large Transformer-based language\nmodels, there are three common options to utilize these lan-\nguage models in multimodal language settings as described\nnext.\nFirst, certain multimodal language approaches freeze\na Transformer and aim to fuse all modalities using ten-\nsor outer-products [23, 24], Canonical Correlation Anal-\nysis based methods [25], Attentive LSTM based methods\n[26, 27, 8, 28, 19], sequence to sequence based methods\n[29], cross-modal Transformer-based methods [6, 30, 31],\ngraph-based method [32], and multi-task learning [7].\nSecond, as ﬁne-tuning large pre-trained Transformer-\nbased language models improve their performances on the\ndownstream tasks signiﬁcantly, recently, some approaches\naim to employ this advantage in multimodal settings. [33]\nhave proposed a method to fuse other modalities in the mid-\ndle layer of a pre-trained Transformer-based language model\nin an aligned manner using a Multimodal Adaption Gate\n(MAG) module. Later, with the popularity of Transformer-\nbased models in Speech, [34] has examined jointly ﬁne-\ntuning lexicon and speech Transformer on the multimodal\nlanguage task. They implemented Co-Attention fusions and\nShallow-Fusion using an attentive and a straightforward late\nfusion of two BERT-style [11] Transformers, respectively.\nThird, [9] have proposed a BERT-style pre-training\nscheme for multimodal language settings, using text, speech,\nand visual features. They pre-trained their proposed model us-\ning one of the most extensive multimodal language datasets,\nconsisting of around 1 million video segments [35]. Never-\ntheless, this amount of data is much smaller than the textual\ncorpora size and limits their model’s performance.\nOur method is a variation of the second approach, i.e., it\nﬁne-tunes a Transformer-based language model in a multi-\nmodal setting. In contrast to previous work, we utilize a pre-\ntrained Transformer language model as cross-modal attention\nwithout needing an entirely different Transformer.\n2.2. Transformers and BERT-style Models\nA Transformer network is a model which utilizes the self-\nattention mechanism to encode sequential feature representa-\ntions. [10] have introduced the Transformer encoder-decoder\nnetwork and attained a signiﬁcant gain in Neural Machine\nTranslations (NMT) task. Regarding this considerable im-\nprovement, [11] have proposed Bidirectional Encoder Repre-\nsentations from Transformers (BERT) model, which uses 12\nlayers of Transformer encoder. They pre-trained the model\nwith Masked Language Model (MaskedLM) and Next Sen-\ntence Prediction (NSP) tasks in a self-supervised manner.\nBERT has signiﬁcantly improved after ﬁne-tuning on the\nGLUE benchmark [36], consisting of nine Natural Language\nUnderstanding (NLU) tasks, including sentiment analysis\nand question answering. We refer the reader to [11] for more\ndetails on the pre-training process. Moreover, [12] introduced\nRobustly optimized BERT approach (RoBERTa) model,\nwhich pre-trained BERT Transformer solely on MaskedLM\ntask using larger corpora, and more effective training param-\neters.\nWith regards to this notable success of BERT-style models\nin NLP, [15] introduced wav2vec 2.0, which contains a con-\nvolution layer as a temporal encoder, a quantization method,\nand a BERT-style Transformer as the backbone of the spa-\ntial encoder for speech encoding. They pre-trained the model\non LIBRI SPEECH dataset [37] containing 960 hours of speech\naudio. Subsequently, they achieved a signiﬁcant enhancement\non Automatic Speech Recognition (ASR) task.\nThe key point of training these BERT-style models is the\ntremendous number of training data points. Accessing this\nmassive number of data points is not always feasible for a\nmultimodal language setting, and additionally, pre-training\na new Transformer is a challenging task. This work em-\nploys a low-resource approach to include other modalities to\na pre-trained BERT-style Transformer. Speciﬁcally, we use\na method to introduce another modality as preﬁxes into a\npre-trained BERT-style Transformer in the ﬁne-tuning pro-\ncess. As such, there is no need to train a new task-speciﬁc\nTransformer.\nConvolution \nLayer\nanyhow It was really good\nanyhow It [mask] really \nRoberta Tokenizer\nanyhow It was really good\nCNN\n[SEP][CLS]\nLigthweight Attentive Aggregation \nPositional \nEmbiddings\nFixed\nRoBERTa\nFixed Encoder Layer\n[mask] \nψ\nZ L\nCA\nFig. 2: The architecture of T EASEL during the pre-training\nprocess. T EASEL takes ψ as a raw speech and L as corre-\nsponding text and calculates the MaskedLM task on the tex-\ntual tokens to train LAA module.\n2.3. Generalization of Pre-Trained Language Model\nWith the accomplishment of Transformers as language mod-\nels on several textual tasks, [38] have examined the gen-\neralizations of ﬁne-tuning these language models on vari-\nous non-verbal modalities downstream tasks, such as image\nclassiﬁcations and protein folding. They introduced Frozen\nPre-trained Transformer (FPT), using the GPT-2 [13] lan-\nguage model as the backbone, and re-trained a small part of\nthe Transformer layers. Their exhaustive set of evaluations\ndemonstrate that it is feasible to achieve comparable results\nto a fully trained Transformer by only ﬁne-tuning a small part\nof the pre-trained model on the downstream task. Further-\nmore, following the success of the GPT-3 [14] auto-regressive\nTransformer in few-shot learnings, [39] have introduced an\nimage encoder that can generate preﬁx tokens for large frozen\nauto-regressive Transformer. They have achieved a method\nthat performs better than baselines on several multimodal vi-\nsual tasks, containing few-shot image classiﬁcations and the\nvisual question answering.\nInspired by [39] approach, we propose a method to train\nLightweight Attentive Aggregation (LAA) module, which\ngenerates speech preﬁxes for a pre-trained language model.\nHowever, in contrast to [39], we ﬁne-tune LAA module and\nthe pre-tranined language model on the downstream task.\n3. METHODOLOGY\nThis section describes our proposed T EASEL model , Fig-\nure 2, that exploits conventional pre-trained language models\nas a cross-modal attention module. Fundamentally, T EASEL\nmodel focus on representing speech features as preﬁxes to a\npre-trained BERT-style language model (i.e., RoBERTa). We\nchoose the speech modality as our additional modal among\nthe speech and visual (facial expressions) modalities for three\nmain reasons. First, speech datasets are more comprehensive.\nSecond, working on raw speech data is less computationally\nintensive. Finally, previous works have shown that the speech\nmodality serves more information in multimodal language\nsettings [6, 7].\nOur method contains two steps, pre-training and ﬁne-\ntuning. As for the pre-training step, we learn a representation\nto insert an additional modality to a ﬁxed pre-trained lan-\nguage model using LAA module with relatively few training\nsteps (8,000 total steps). Afterward, we ﬁxed most part of\nLAA module and ﬁne-tuned the pre-trained Transformer as\na cross-modal Transformer on CMU-MOSI dataset for the\nmultimodal sentiment analysis downstream task. We discuss\nthe parts to ﬁne-tune and parts to keep frozen in the ablation\nstudies section.\nIn the rest of this section, we present the methodology of\nBERT-style Transformers, which serve as the backbone of our\nmethod. Respectively, we describe the speech feature extrac-\ntion and LAA module. Finally, we describe TEASEL model’s\npre-training and ﬁne-tuning process.\n3.1. BERT-style Language Models\nWe choose RoBERTa model [12] for our conventional BERT-\nstyle encoder, because it has trained more robustly on much\nlarger datasets. RoBERTa tokenizer decomposes sentence L\nas\ntokenizer(L) = {[CLS],l1,l2,...,l TL,[SEP]}, (1)\nwhere li ∈Rd represents a byte-level token, TL denotes the\nnumber of time-steps in the textual modality, and [CLS] and\n[SEP] designate the beginning and the end of the sequence\nsymbols, respectively. The standard published RoBERTa\nmodels consist of a base and a large model containing 12 and\n24 layers, respectively, both of which are exclusively trained\non the MaskedLM task [11]. MaskedLM task intends to\npredict randomly masked tokens using the entire unmasked\nwords of the sentence. MaskedLM loss masks 15% of the\ntokens using a variety of masking methods. The standard\nMaskedLM [11] uses the [MASK] token 80% of the time,\na random token 10% of the time, and the unchanged token\n10% of the time; forcing the language model to predict the\noutput token conﬁdently. We use standard MaskedLM for\nour pre-training process. We refer the reader to [11, 12] for\nadditional speciﬁcations of the pre-training process.\n3.2. Speech Module\nAs discussed, we need a unimodal pre-trained speech feature\nextractor model and then learn a relatively small spatial head\nat the top of representations to encode speech signals as a\nword token.\nBi-GRU\nProjection\nLayerNorm\nProjection\nLayerNorm LayerNorm\nProjection\nAttention Module\nLayerNorm LayerNormShared \nparameters\nFeedForward \nPath\nAttention Module\nZ\nˆZ\nα22\nα2TA\nα12\nα11\nCA\nΦ\nFig. 3: The architecture of Lightweight Attentive Aggrega-\ntion (LAA) to aggregate the speech latent representation as\npreﬁxes with bidirectional attentive manner.\n3.2.1. Speech Temporal Encoder\nwav2vec 2.0 model [15] utilizes ﬁve layers of Convolutional\nNeural Networks (CNN) as a temporal feature encoder and\na BERT-style Transformer as the contextualized encoder. In\nthis work, we select wav2vec 2.0 pre-trained ﬁxed-parameter\nCNN as an audio feature encoder.\nCNNθW(ψ) = {z1,z2,...,z TA ; zi ∈RdA}, (2)\nIn Equation 2 the CNNθW would take ψas a raw speech vec-\ntor and represents zi as a speech latent feature for TA time-\nsteps, and θW denotes pre-trained parameters from wav2vec\n2.0.\n3.2.2. Lightweight Attentive Aggregation (LAA)\nAlthough wav2vec 2.0 contains a well-deﬁned Transformer\nnetwork as a contextual encoder, for the sake of generaliza-\ntion, we train a small contextual encoder at the top of ﬁxed\nfeatures to simulate the effect of not having a fully trained\nTransformer for low-resources modalities.\nEssentially, the speech LAA aims to encode Z as pre-\nﬁx tokens for RoBERTa encoder. As Figure 3 illustrates, this\nmodule performs a Bi-directional Gated Recurrent Unit (Bi-\nGRU) at the top of speech ﬁxed features to capture the in-\nformation in a bidirectional behavior. More speciﬁcally, two\noutput sequences of the BiGRU (Φ) are deﬁned as follows.\nˆZ = LayerNorm(Z) (3)\nΦ = BiGRU(W⊺\n1 ˆZ+ b1), (4)\nΦ = {{φ1,1,...,φ 1,TA},{φ2,1,...,φ 2,TA}}, (5)\nwhere W1 ∈ RdA×dA and b1 ∈ RdA respectively denotes\nprojection weight and bias and Φ ∈A2×TA×dA denotes the\nBiGRU output sequences.\nNext, we need an aggregation module to interpret the\nﬁnal preﬁxed tokens. Accordingly, we design LAA module\nbased on the encoder used in the encoder-decoder models in\nthe NMT ﬁeld [40]. Our LAA module calculates a dynamic\nweighted sum over Φ as,\nuk,i = σ(W⊺\nAgg1 φk,i + bAgg1 ),k ∈{1,2}, (6)\nαk = Softmax(W⊺\nAgg2 uk + bAgg2 ),αk ∈[0,1], (7)\nCA =\nT∑\ni=1\nαk,iφk,i, (8)\nwhere WAgg1 ∈ RdA×dA, bAgg1 ∈ RdA, WAgg2 ∈ RdA,\nbAgg2 ∈R, are feed forward’s weights and biases, σ is ac-\ntivation function, and CA ∈R2×da are two RoBERTa-ready\nspeech-preﬁxes.\n3.3. Training Process\n3.3.1. Pre-Training Process\nSimilar to the training process mentioned in [39], we used a\nﬁxed pre-trained language model as our Transformer encoder.\nWe feed the sequence\n{[CLS],CA,l1,l2,..., [MASK],...,l TL,[SEP]}\nto a pre-trained RoBERTa model. In the mentioned sequence,\n[MASK] token is the MaskedLM masked token, and it only\napplies to lexicon tokens. We only calculate the loss func-\ntion on the verbal output tokens, and its gradient only affects\nLAA module. In particular, the MaskedLM aims to predict\n[MASK] tokens using other words and speech preﬁxes. In\ncontrast to [39], we used a BERT-style language model, ab-\nsolute position embedding, and MaskedLM loss to train our\nLAA module. Moreover, in the ablation studies section, we\nstudy the importance of pre-training on the model’s perfor-\nmance.\n3.3.2. Fine-tuning on Downstream Task\nAs Figure 1 illustrates, we feed the speech preﬁxes and tex-\ntual tokens to the pre-trained model and ﬁne-tune it using an\nadditional head at the top of the [CLS] token. In contrast\nto [39] which Freezes their model on both pre-training and\ndownstream tasks, we ﬁne-tune a subset of our model besides\nthe language model encoder on the downstream task. In the\nAlgorithm 1: TEASEL Model\nInput: speech ψ, sentences as L, number of iterations\nepocht, epochf, learning rateηt, ηf\nParameter: Initialize θLM with pre-trained RoBERTa, θW\nwith pre-trained wav2vec 2.0, and θA, θh randomly\n1: ZA ←CNNθW(ψ)\n2: XL ←tokenizer(L)\n3: // Pre-training process\n4: for iteration = 1,2,...,epoch t do\n5: ΦA ←BiGRUθA(ZA) // dropped Eq. 3 for brevity\n6: αi ←AttentionθA(φi)\n7: CA ←∑T\ni=1 αiφi\n8: [CLS; HA; HL] ←RoBERTaθLM ([CA; LL])\n9: ∇θA ←backpropagate LMLM(HL)\n10: θA ←θA −ηp∇θA\n11: end for\n12: // Pre-training process\n13: // Fine-tune θLM and θA on the downstream task.\n14: for iteration = 1,2,...,epoch g do\n15: ΦA ←BiGRUθA(ZA)\n16: αi,k ←AttentionθA(φi,k)\n17: CA ←∑T\ni=1 αk,iφk,i\n18: [CLS; HA; HL] ←RoBERTaθLM ([CA; LL])\n19: ˜y←ClassificationHeadθh(CLS)\n20: ∇θA,LM,h ←backpropagate LMSE(˜y,y)\n21: θA,LM,h ←θA,LM,h −ηf∇θA,LM,h\n22: end for\n23: return θA,LM,h as TEASEL\nablation studies section, we demonstrate the effect of freez-\ning each part of the proposed model on the downstream task.\nFurthermore, Algorithm 1 shows a pseudocode for the whole\nproposed model.\n4. EXPERIMENTAL SETUP\n4.1. Dataset and Evaluation Methods\nWe evaluated T EASEL on CMU-MOSI [18] for the multi-\nmodal sentiment analysis task.\n4.1.1. CMU-MOSI\nMultimodal Opinion-level Sentiment Intensity dataset(CMU-\nMOSI) [18] contains 2199 video segments. Each has a senti-\nment intensity label with intensity in the range of [−3,+3];\n+3 denotes the solid positive sentiment, and −3 indicates the\nsolid negative sentiment. To be consistent with prior work, we\nuse the authors’ published train/validation/test sets on CMU-\nMultimodal SDK [26].\nWe followed the evaluation approach presented in [6]\nusing binary accuracy and F1-scores on the non-zero human-\nannotated labels. More concretely, [−3,0) labels expresses\nnegative and (0,+3] predictions indicating positive senti-\nments. Additionally, we report the 7-class accuracy (Acc 7),\nMean Absolute Error (MAE), and the correlation of the pre-\ndicted labels with target labels to be consistent with the prior\nwork [23, 24, 41].\n4.2. Baseline\nWe consider a variety of SoTA unimodal and multimodal\nTransformers and fusion methods. Experimental comparisons\nare reported in three sections:\nFirst, we compare T EASEL against unimodal (text only)\nlarge textual Transformers,\n• RoBERTa [12] is a robustly pre-trained version of\nBERT Transformer. [12] have published a base and\nlarge version of RoBERTa, using 12 and 24 layers,\nrespectively. We used the base and large version of\nRoBERTa as a text-only baseline.\nSecond, we compare TEASEL against popular multimodal\nfusion methods which use a Transformer as a feature extrac-\ntor. These methods are as follows.\n• Tensor Fusion Network (TFN) utilizes tensor outer-\nproduct to combine three modalities in unimodal, bi-\nmodal, and trimodal fashion. Furthermore, it concate-\nnates all six ﬂattened vectors as a multimodal represen-\ntation [23] .\n• Low-rank Multimodal Fusion (LFM) optimizes the\nTFN time complexity from exponential to linear using\nmodality-speciﬁc low-rank factors [24].\n• Memory Fusion Network (MFM) trains modality-\nspeciﬁc generative factorized representations and joint\nmultimodal features across all modalities for multi-\nmodal tasks [41].\n• Modality-Invariant and -Speciﬁc Representations\nfor Multimodal Sentiment Analysis (MISA) factor-\nizes modalities into modality-invariant and modality-\nspeciﬁc features and then fuses them to predict a label\n[42].\n• Interaction Canonical Correlation Network (ICCN)\napplies Deep CCA to dissolve hidden relationships\nbetween BERT sentence embeddings, audio sequence,\nand facial features [25].\nFinally, we compare TEASEL performance against multi-\nmodal fusion Transformer-based methods.\n• Multimodal Adaptation Gate-Transformer (MAG-\nTransformer) is a module to generate a shift to the\ninternal representation of Transformer models using\nvisual and acoustic modalities. Combining the MAG\nmodule with a pre-trained Transformer would let them\nintegrate speech and visual modalities to the middle\nlevel of the pre-trained lexicon Transformer [33].\n• Multimodal Transformer (MulT) performs direc-\ntional cross-modal attention mechanism followed by\nself-attention for every two modalities to combine the\nheterogeneous multimodal signals using supervised\nsettings [6].\n• Self-Supervised Multi-task Multimodal (Self-MM)\nemploys multi-task learning and self-supervised learn-\ning to generate unimodal labels. Later, training the\nmultimodal and unimodal representations to learn both\nvariations and agreements among the modalities [7].\n• Shallow-Fusion method jointly ﬁne-tunes roberta-\nlarge and vq-wav2vec models on downstream tasks\nusing a late fusion method [34] .\n4.3. Implementation Details\nIn this subsection, we review the implementation speciﬁ-\ncation of both pre-training and ﬁne-tuning steps. We im-\nplemented T EASEL using PyTorch [43] and HuggingFace\nframeworks [44] with a ﬁxed random seed to guarantee re-\nproducibility. We applied grid search for hyperparameter\ntuning during ﬁne-tuning with learning rates in range of {1e-\n4, 5e-5, 2e-5, 1e-5}, LAA dropout in the range of {0.0, 0.05,\n0.10, 0.2, 0.3, 0.4, 0.5 }, batch sizes in the range of {8, 16,\n32}, and warm-up steps using {0%, 10%, 20%, 30%, 40% }\nof total step. We used the validation set of CMU-MOSI to\nﬁnd the best hyperparameters. The best setting was 2e-5 as\nlearning rate, 0.1 as LAA dropout, and warmup step of 10%\nof total ﬁne-tuning steps. Also, we used a batch size of 32\nand 16 during pre-training and ﬁne-tuning, respectively. We\npre-trained and ﬁne-tuned T EASEL using AdamW optimizer\n[45] , GELU activation function [46], cosine scheduler with\nafrosaid warmup steps. We pre-trained T EASEL using 100\nhours of LIBRI SPEECH in 8,000 cumulative steps (9 epochs)\nand ﬁne-tuned T EASEL on CMU-MOSI with 3 epochs.\n5. DISCUSSION OF EXPERIMENTAL RESULTS\n5.1. Quantitative Analysis\nIn Table 1, we present the results of experiments on CMU-\nMOSI dataset on several benchmarks. Benchmark results are\nfrom their corresponding paper discussed in the Baseline sec-\ntion, except TFN, LMF, and MFM models, which [42] re-\nproduced using BERT sentence representations as a textual\nmodality.\nFirst, we evaluate T EASEL against the text-only popular\nTransformers. T EASEL has performed better than text-only\nRoBERTa base and large models. The p-value of the student\nTable 1: Results for multimodal sentiment analysis on CMU-\nMOSI. (B) and (R) indicate sentence representations are from\nBERT and RoBERTa, respectively.\nCMU-MOSI\nMAEl Corrh Acch\n2 F1h Acch\n7\nTFN†(B) 0.901 0.698 80.82 80.77 34.94\nLMF†(B) 0.917 0.695 82.47 82.47 33.23\nMFM†(B) 0.877 0.706 81.72 81.64 35.42\nICCN(B) 0.860 0.710 83.00 83.00 39.00\nMulT 0.871 0.698 83.00 82.80 40.00\nBERT‡ 0.739 0.782 85.20 85.20 -\nSelf-MM(B) 0.713 0.798 85.98 85.95 -\nMAG-BERT(B) 0.739 0.796 86.10 86.00 -\nMAG-XLNet 0.675 0.821 87.90 87.90 -\nShallow-Fusion(R) 0.577 - 88.27 88.57 48.92\nroberta-base‡‡ 0.704 0.807 85.31 85.37 46.36\nroberta-large‡‡ 0.687 0.835 86.89 86.91 46.65\nTEASEL††(R) (ours) 0.644 0.842 89.33 89.31 47.52\nhindicates higher is better.\nlindicates lower is better.\n†Models are proposed by [42].\n‡Models are suggested by [33].\n††Models are produced using the same environment.\nt-test between T EASEL and RoBERTa base is p < 0.05,\ndemonstrating signiﬁcant improvement over RoBERTa base\nmodel.\nSecond, in our experiments, we signiﬁcantly improve over\nmethods that apply the frozen Transformer-based features,\nprincipally because ﬁne-tuning pre-trained Transformers for\nthe downstream task would signiﬁcantly improve the perfor-\nmance of Transformers.\nEventually, we examine TEASEL against networks which\nﬁne-tune a Transformer for the downstream task. Our method\nhas outperformed Self-MM [7], MAG-BERT, MAG-XLNet\n[33], and Shallow-fusion [34] methods in most metrics. Un-\nlike MAG-Transformer methods, T EASEL does not require\nan aligned feature and explicitly feeds speech representations\nto the Transformer. As for Shallow Fusion, T EASEL outper-\nforms their best setup in F 1-score, Acc 2 metrics, but their\nmethod has a superior result for the Acc 7 and MAE utiliz-\ning a fully trained speech BERT-style Transformer. Moreover,\nTEASEL uses 133.4M parameters while Shallow-Fusion uses\napproximately 483.6M parameters, which indicated T EASEL\nuses 72.4% fewer parameters.\n5.2. Ablation Studies\nWe study the following subjects to examine the usefulness of\nsome decisions in designing TEASEL .\nspeech-prefix-leftspeech-prefix-right\nuh I\nwill Iwill\nactually\nsaythisrightnowitwasa\ngoodwaste\noflike12\nLeft speech prefix attention \nRight speech prefix attention \nspeech-prefix-right\nspeech-prefix-left\nuh\nI\nI\nwill\nwill\nactually\nthis\nsay\nright\nnow\nwas\nit\na\ngood\nof\nwaste\nlike\n12\nFig. 4: The attention activation layer of a random data point in the test set. Left plots are LAA speech attention values and\nthe right ﬁgure demonstrates the attention activation layer in the attention encoder. As the cross-modal attention demonstrates,\ndifferent word tokens are attending to speech preﬁxes.\n5.2.1. Question 1: Does pre-training effects performance of\nTEASEL ?\nWe examined the effect of pre-training process on the multi-\nmodal sentiment analysis downstream task. We trained LAA\nmodule using 100 hours of L IBRI SPEECH dataset (around\n28k data points) for 8,000 training steps. Subsequently, we\nsaved the model every 2,000 steps and ﬁne-tuned the saved\ncheckpoints for the multimodal sentiment analysis on CMU-\nMOSI dataset. As Figure 5 demonstrates, pre-training LAA\nmodule improves TEASEL .\n0 2,000 4,000 6,000 8,0000.84\n0.85\n0.86\n0.87\n0.88\n0.89\n0.9\n0.91\nPre-Training Step\nPerformance on CMU-MOSI\nValid F1-Score\nFig. 5: Results of ﬁne-tuning different checkpoints of pre-\ntrained T EASEL using the same condition. Performance of\neach ﬁne-tuned model on validation set is evaluated. For each\ncheckpoint, F1-score of different pre-trained checkpoints is\nreported.\nTable 2: Results of ﬁxing or ﬁne-tuning different subsections\nof Lightweight Attentive Aggregation. + in each row indicates\nthat we ﬁne-tuned that module and all of the above ones to-\ngether. For instance, in the second row we ﬁne-tuned roberta-\nencoder and Attention module and keep the rest parts ﬁxed.\nAlso, We understand ﬁne-tuning the embedding layer would\nhurt the model’s performance.\nFrozen or Fine-tuning Parts of TEASEL\nMAEl Corrh Acch\n2 F1h Acch\n7\nrobert-encoder 0.6861 0.8228 87.80 87.80 47.81\n+ Attention 0.6443 0.8423 89.33 89.31 47.52\n+ BiGRU 0.6531 0.8334 88.41 88.40 48.98\n+ Projection 0.7142 0.8192 85.37 85.28 47.08\nAll Parameters 0.6556 0.8360 88.57 88.57 48.69\nroberta-base 0.7042 0.8072 85.31 85.37 46.36\nroberta-large 0.6870 0.8347 86.89 86.91 46.65\n5.2.2. Question 2: Does the whole model needs ﬁne-tuning?\nWe trained the new classiﬁcation head along with some parts\nof pre-trained T EASEL model. We have done exhaustive ex-\nperiments on which parts of LAA modules should be ﬁxed\nfor the ﬁne-tuning part. As Table 2 represents, we obtained\nthe best result on solely ﬁne-tuning RoBERTa encoder and\nLAA besides the classiﬁcation head. We Believe this is pri-\nmarily because CMU-MOSI does not contain many data\npoints (2,199), and ﬁne-tuning BiGRU and Projection hurt\nthe model’s performance signiﬁcantly.\n5.2.3. Question 3: Why we need an aggregation for the\nspeech preﬁxes tokens?\nThe Transformer’s attention complexity isO(n2), where nis\nthe length of a sequence [11]. Letting the pre-trained Trans-\nformer use additional sequences might improve the entire pro-\ncess, but that would signiﬁcantly increase the computation\ncomplexity. As we aggregate speech features before feeding\nthem to a pre-trained Transformer, this would not increase\nthe Transformer’s complexity quadratically. Moreover, our at-\ntention module uses linear computation complexity with re-\nspect to sequence length in order to attend BiGRU’s output se-\nquences. We illustrate and discuss the effectiveness of speech\npreﬁxes in the qualitative analysis section.\n5.3. Qualitative Analysis\nWe illustrate T EASEL ’s random attention activation layer to\ninvestigate whether the ﬁne-tuned Transformer encoder has\nlearned to attend the speech preﬁxes. Figure 4 exhibits a ran-\ndom datapoint from CMU-MOSI test set, ”lXPQBPVc5Cw16”,\nin which the speaker says ” uh I will I will actually say this\nright now it was a good waste of like 12 (dollars) ” with a\nnon-negative voice tone, which caused the label to be weakly\npositive (i.e., +0.2). In Figure 4, RoBERTa attention masks\ndemonstrate that different word tokens attend to speech pre-\nﬁxes that are from an attentive module. While in BERT-style\nTransformers, there is excellent attention between each to-\nken and [CLS]/[SEP] tokens, we removed the [CLS] and\n[SEP] tokens for better illustration.\n6. CONCLUSION\nWe proposed a Transformer-Based Speech-Preﬁxed Lan-\nguage Model called T EASEL to model multimodal sentiment\nanalysis without training a new cross-modal Transformer. We\nimplemented a Lightweight Attentive Aggregation module\nto create an efﬁcient spatial encoding, which creates speech\npreﬁxes for a pre-trained Transformer. T EASEL successfully\nutilized a pre-trained Transformer as a cross-modal atten-\ntion module. Comprehensive analyses validated that TEASEL\noutperforms Transformer based text-only language model by\n4% and achieves an approximately 1% superior result than\nthe current multimodal SoTA. Extensive experiments ver-\nify the effectiveness of various steps of T EASEL model. We\nhope this work can provide a new perspective on pre-trained\nTransformers’ generalization for multimodal language envi-\nronments.\nReferences\n[1] “Second grand-challenge and workshop on multimodal\nlanguage (challenge-hml),” Seattle, USA, July 2020,\nAssociation for Computational Linguistics.\n[2] Louis-Philippe Morency, Rada Mihalcea, and Payal\nDoshi, “Towards Multimodal Sentiment Analysis: Har-\nvesting Opinions from The Web,” inInternational Con-\nference on Multimodal Interfaces (ICMI 2011) , Ali-\ncante, Spain, Nov. 2011.\n[3] Mohammad Soleymani, David Garcia, Brendan Jou,\nBj¨orn Schuller, Shih-Fu Chang, and Maja Pantic, “A\nsurvey of multimodal sentiment analysis,” Image and\nVision Computing, vol. 65, pp. 3–14, 2017.\n[4] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-\nria, Erik Cambria, and Louis-Philippe Morency, “Mul-\ntimodal language analysis in the wild: CMU-MOSEI\ndataset and interpretable dynamic fusion graph,” inPro-\nceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\nMelbourne, Australia, jul 2018, pp. 2236–2246, Associ-\nation for Computational Linguistics.\n[5] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jeannette N\nChang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap: Interactive emotional dyadic motion capture\ndatabase,” Language resources and evaluation, vol. 42,\nno. 4, pp. 335–359, 2008.\n[6] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov, “Multimodal transformer for unaligned\nmultimodal language sequences,” in Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) , Florence,\nItaly, 7 2019, Association for Computational Linguis-\ntics.\n[7] Wenmeng Yu, Hua Xu, Yuan Ziqi, and Wu Jiele,\n“Learning modality-speciﬁc representations with self-\nsupervised multi-task learning for multimodal sentiment\nanalysis,” inProceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence, 2021.\n[8] Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Bal-\ntruˇsaitis, Amir Zadeh, and Louis-Philippe Morency,\n“Multimodal sentiment analysis with word-level fusion\nand reinforcement learning,” in Proceedings of the 19th\nACM International Conference on Multimodal Interac-\ntion, 2017, pp. 163–171.\n[9] Aparna Khare, Srinivas Parthasarathy, and Shiva Sun-\ndaram, “Self-supervised learning with cross-modal\ntransformers for emotion recognition,” in 2021 IEEE\nSpoken Language Technology Workshop (SLT) . IEEE,\n2021, pp. 381–388.\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin, “Attention is all you need,” in Ad-\nvances in neural information processing systems, 2017,\npp. 5998–6008.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[12] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov, “Roberta: A ro-\nbustly optimized bert pretraining approach,” arXiv\npreprint arXiv:1907.11692, 2019.\n[13] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al., “Language models\nare unsupervised multitask learners,” OpenAI blog, vol.\n1, no. 8, pp. 9, 2019.\n[14] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\net al., “Language models are few-shot learners,” arXiv\npreprint arXiv:2005.14165, 2020.\n[15] Alexei Baevski, Henry Zhou, Abdelrahman Mohamed,\nand Michael Auli, “wav2vec 2.0: A framework for self-\nsupervised learning of speech representations,” arXiv\npreprint arXiv:2006.11477, 2020.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, et al., “An image is worth\n16x16 words: Transformers for image recognition at\nscale,” arXiv preprint arXiv:2010.11929, 2020.\n[17] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee,\n“Vilbert: Pretraining task-agnostic visiolinguistic rep-\nresentations for vision-and-language tasks,” arXiv\npreprint arXiv:1908.02265, 2019.\n[18] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-\nPhilippe Morency, “Mosi: multimodal corpus of sen-\ntiment intensity and subjectivity analysis in online opin-\nion videos,” arXiv preprint arXiv:1606.06259, 2016.\n[19] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Po-\nria, Erik Cambria, and Louis-Philippe Morency, “Multi-\nmodal language analysis in the wild: Cmu-mosei dataset\nand interpretable dynamic fusion graph,” inProceedings\nof the 56th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) , 2018,\npp. 2236–2246.\n[20] Md Kamrul Hasan, Wasifur Rahman, Amir Zadeh,\nJianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe\nMorency, et al., “Ur-funny: A multimodal language\ndataset for understanding humor,” arXiv preprint\narXiv:1904.06618, 2019.\n[21] Santiago Castro, Devamanyu Hazarika, Ver´onica P´erez-\nRosas, Roger Zimmermann, Rada Mihalcea, and Sou-\njanya Poria, “Towards multimodal sarcasm detec-\ntion (an obviously perfect paper),” arXiv preprint\narXiv:1906.01815, 2019.\n[22] Tadas Baltru ˇsaitis, Chaitanya Ahuja, and Louis-Philippe\nMorency, “Multimodal machine learning: A survey and\ntaxonomy,” IEEE transactions on pattern analysis and\nmachine intelligence, vol. 41, no. 2, pp. 423–443, 2018.\n[23] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cam-\nbria, and Louis-Philippe Morency, “Tensor fusion net-\nwork for multimodal sentiment analysis,”arXiv preprint\narXiv:1707.07250, 2017.\n[24] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshmi-\nnarasimhan, Paul Pu Liang, Amir Zadeh, and Louis-\nPhilippe Morency, “Efﬁcient low-rank multimodal fu-\nsion with modality-speciﬁc factors,” arXiv preprint\narXiv:1806.00064, 2018.\n[25] Zhongkai Sun, Prathusha Sarma, William Sethares, and\nYingyu Liang, “Learning relationships between text, au-\ndio, and video via deep canonical correlation for multi-\nmodal language analysis,” in Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 2020, vol. 34, pp.\n8992–8999.\n[26] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek\nVij, Erik Cambria, and Louis-Philippe Morency, “Multi-\nattention recurrent network for human communication\ncomprehension,” in Thirty-Second AAAI Conference on\nArtiﬁcial Intelligence, 2018.\n[27] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Sou-\njanya Poria, Erik Cambria, and Louis-Philippe Morency,\n“Memory fusion network for multi-view sequential\nlearning,” Proceedings of the Thirty-Second AAAI Con-\nference on Artiﬁcial Intelligence, 2018.\n[28] Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang,\nAmir Zadeh, and Louis-Philippe Morency, “Words can\nshift: Dynamically adjusting word representations us-\ning nonverbal behaviors,” in Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 2019, vol. 33, pp.\n7216–7223.\n[29] Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-\nPhilippe Morency, and Barnab ´as P ´oczos, “Found in\ntranslation: Learning robust joint representations by\ncyclic translations between modalities,” in Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, 2019,\nvol. 33, pp. 6892–6899.\n[30] Amir Zadeh, Chengfeng Mao, Kelly Shi, Yiwei Zhang,\nPaul Pu Liang, Soujanya Poria, and Louis-Philippe\nMorency, “Factorized multimodal transformer for\nmultimodal sequential learning,” arXiv preprint\narXiv:1911.09826, 2019.\n[31] Md Kamrul Hasan, Sangwu Lee, Wasifur Rahman,\nAmir Zadeh, Rada Mihalcea, Louis-Philippe Morency,\nand Ehsan Hoque, “Humor knowledge enriched trans-\nformer for understanding multimodal humor,” in Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, 2021, vol. 35, pp. 12972–12980.\n[32] Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu,\nAzaan Rehman, Amir Zadeh, Soujanya Poria, and\nLouis-Philippe Morency, “Mtgat: Multimodal tem-\nporal graph attention networks for unaligned human\nmultimodal language sequences,” arXiv preprint\narXiv:2010.11985, 2020.\n[33] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee,\nAmir Zadeh, Chengfeng Mao, Louis-Philippe Morency,\nand Ehsan Hoque, “Integrating multimodal information\nin large pretrained transformers,” in Proceedings of the\nconference. Association for Computational Linguistics.\nMeeting. NIH Public Access, 2020, vol. 2020, p. 2359.\n[34] Shamane Siriwardhana, Andrew Reis, Rivindu\nWeerasekera, and Suranga Nanayakkara, “Jointly\nﬁne-tuning” bert-like” self supervised models to im-\nprove multimodal speech emotion recognition,” arXiv\npreprint arXiv:2008.06682, 2020.\n[35] Joon Son Chung, Arsha Nagrani, and Andrew Zisser-\nman, “V oxceleb2: Deep speaker recognition,” arXiv\npreprint arXiv:1806.05622, 2018.\n[36] Alex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman, “Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding,” arXiv preprint\narXiv:1804.07461, 2018.\n[37] Vassil Panayotov, Guoguo Chen, Daniel Povey, and San-\njeev Khudanpur, “Librispeech: an asr corpus based on\npublic domain audio books,” in2015 IEEE international\nconference on acoustics, speech and signal processing\n(ICASSP). IEEE, 2015, pp. 5206–5210.\n[38] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mor-\ndatch, “Pretrained transformers as universal computa-\ntion engines,” arXiv preprint arXiv:2103.05247, 2021.\n[39] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill, “Multimodal\nfew-shot learning with frozen language models,” arXiv\npreprint arXiv:2106.13884, 2021.\n[40] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio, “Neural machine translation by jointly learning to\nalign and translate,” arXiv preprint arXiv:1409.0473 ,\n2014.\n[41] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh,\nLouis-Philippe Morency, and Ruslan Salakhutdinov,\n“Learning factorized multimodal representations,”\narXiv preprint arXiv:1806.06176, 2018.\n[42] Devamanyu Hazarika, Roger Zimmermann, and Sou-\njanya Poria, “Misa: Modality-invariant and-speciﬁc rep-\nresentations for multimodal sentiment analysis,” inPro-\nceedings of the 28th ACM International Conference on\nMultimedia, 2020, pp. 1122–1131.\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga,\net al., “Pytorch: An imperative style, high-performance\ndeep learning library,” Advances in neural information\nprocessing systems, vol. 32, pp. 8026–8037, 2019.\n[44] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, et al., “Huggingface’s transformers: State-of-\nthe-art natural language processing,” arXiv preprint\narXiv:1910.03771, 2019.\n[45] Ilya Loshchilov and Frank Hutter, “Decoupled\nweight decay regularization,” arXiv preprint\narXiv:1711.05101, 2017.\n[46] Dan Hendrycks and Kevin Gimpel, “Gaussian error\nlinear units (gelus),” arXiv preprint arXiv:1606.08415,\n2016.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7977778315544128
    },
    {
      "name": "Transformer",
      "score": 0.7297005653381348
    },
    {
      "name": "Language model",
      "score": 0.6634025573730469
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5721375942230225
    },
    {
      "name": "Speech recognition",
      "score": 0.5086332559585571
    },
    {
      "name": "Lexicon",
      "score": 0.4773971140384674
    },
    {
      "name": "Natural language processing",
      "score": 0.4764328896999359
    },
    {
      "name": "Prefix",
      "score": 0.4438377022743225
    },
    {
      "name": "Linguistics",
      "score": 0.13833656907081604
    },
    {
      "name": "Engineering",
      "score": 0.08582845330238342
    },
    {
      "name": "Voltage",
      "score": 0.07938989996910095
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 2
}