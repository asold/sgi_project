{
  "title": "Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model",
  "url": "https://openalex.org/W4383377722",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Yuan, Mingruo",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2757596404",
      "name": "Kao Ben",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Wu, Tien-Hsuan",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Cheung, Michael M. K.",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Chan, Henry W. H.",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Cheung, Anne S. Y.",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Chan, Felix W. H.",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2066032828",
      "name": "Chen Yongxi",
      "affiliations": [
        "Australian National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2921026077",
    "https://openalex.org/W2547809915",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2422149132",
    "https://openalex.org/W6600511658",
    "https://openalex.org/W2891946694",
    "https://openalex.org/W3003604522",
    "https://openalex.org/W6633568089",
    "https://openalex.org/W187979605",
    "https://openalex.org/W2138989966",
    "https://openalex.org/W3200288876",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W2804292122",
    "https://openalex.org/W4281679314",
    "https://openalex.org/W2904146647",
    "https://openalex.org/W4288059420",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3101798106"
  ],
  "abstract": null,
  "full_text": "Bringing Legal Knowledge to the Public by\nConstructing a Legal Question Bank Using\nLarge-scale Pre-trained Language Model\nMingruo Yuan1, Ben Kao1, Tien-Hsuan Wu1, Michael M.K.\nCheung2, Henry W.H. Chan2, Anne S.Y. Cheung2, Felix\nW.H. Chan2 and Yongxi Chen3\n1Department of Computer Science, The University of Hong\nKong, Pokfulam, Hong Kong, China.\n2Faculty of Law, The University of Hong Kong, Pokfulam, Hong\nKong, China.\n3College of Law, The Australian National University, ACT 2601,\nCanberra, Australia.\nContributing authors: mryuan@cs.hku.hk; kao@cs.hku.hk;\nthwu@cs.hku.hk; michaelmkcheung@hku.hk; hwhchan@hku.hk;\nanne.cheung@hku.hk; fwhchan@hku.hk; yongxi.chen@anu.edu.au;\nAbstract\nAccess to legal information is fundamental to access to justice. Yet acces-\nsibility refers not only to making legal documents available to the public,\nbut also rendering legal information comprehensible to them. A vexing\nproblem in bringing legal information to the public is how to turn for-\nmal legal documents such as legislation and judgments, which are often\nhighly technical, to easily navigable and comprehensible knowledge to\nthose without legal education. In this study, we formulate a three-step\napproach for bringing legal knowledge to laypersons, tackling the issues\nof navigability and comprehensibility. First, we translate selected sections\nof the law into snippets (called CLIC-pages), each being a small piece\nof article that focuses on explaining certain technical legal concept in\nlayperson’s terms. Second, we construct a Legal Question Bank (LQB),\nwhich is a collection of legal questions whose answers can be found in\nthe CLIC-pages. Third, we design an interactive CLIC Recommender\n(CRec). Given a user’s verbal description of a legal situation that requires\n1\narXiv:2505.04132v1  [cs.CL]  7 May 2025\n2 Article Title\na legal solution, CRec interprets the user’s input and shortlists questions\nfrom the question bank that are most likely relevant to the given legal sit-\nuation and recommends their corresponding CLIC pages where relevant\nlegal knowledge can be found. In this paper we focus on the technical\naspects of creating an LQB. We show how large-scale pre-trained lan-\nguage models, such as GPT-3, can be used to generate legal questions. We\ncompare machine-generated questions (MGQs) against human-composed\nquestions (HCQs) and find that MGQs are more scalable, cost-effective,\nand more diversified, while HCQs are more precise. We also show a\nprototype of CRec and illustrate through an example how our 3-step\napproach effectively brings relevant legal knowledge to the public.\nKeywords: Legal Knowledge Dissemination, Navigability and\nComprehensibility of Legal Information, Machine Question Generation,\nPre-trained Language Model\n1 Introduction\nWith advances in information technology, legislation and judgments (i.e., the\n“primary legal sources” used by legal professionals), are available online for\npublic accesses. Considerable attention has been given to online dissemina-\ntion of legal information. Namely, the free access to law movement (FALM)1\nis an international movement that created Legal Information Institutes (LIIs)\naround the world, each providing online databases of primary legal sources to\nthe general public. The public availability of legal information in these for-\nmal sources, however, does not translate directly and easily to legal knowledge\nto the public. Although legal information can be freely retrieved, those with-\nout a legal education may still find the law to be “unreadable,” difficult to\nunderstand, and thus not accessible, whether in common law countries (Cur-\ntotti et al., 2015) or civil law jurisdictions (Mommers et al., 2009, p. 53).\nThere is generally what we coin as a legal knowledge gap between formal legal\nsources and the general public. Identified by the New Zealand Law Commis-\nsion and the New Zealand Parliamentary Counsel’s Office (New Zealand Law\nReform Commission, 2008), this knowledge gap is mostly attributable to three\nchallenges:\nAvailability. The formal sources that express the law (including both legis-\nlation and case law) may not be easily available to the public in hard copy or\nelectronic form. They may be difficult to locate or find.\nNavigability. The law expressed by the legal sources is hardly navigable, in\nthe sense that the public may not have the ability to know of and reach the\nrelevant legal rule or principle applicable to the situation they face.\nComprehensibility. Even if the legal rule or principle is found, it is barely\nunderstandable, readable or comprehensible to the public.\n1http://www.fatlm.org/\nArticle Title 3\nFig. 1 The legal knowledge gap and the three-step approach of bridging the gap.\nIn recent years, great efforts have been made towards improving legal\nknowledge availability, particularly through online accesses of primary legal\nsources. However, for people without formal legal training, it is often diffi-\ncult for them to navigate the large volume of legal information, identify the\ncorrect legal issue, and find the relevant legal rules that they need. It is also\nchallenging to understand the legal rules and their underlying concepts which\ndetermine the legal solution being sought. Hence, the objective of our study\nis to bridge the legal knowledge gap by addressing the problems of navigabil-\nity and comprehensibility. Specifically, we investigate a three-step approach to\nbridging the gap. Figure 1 illustrates the legal knowledge gap and the three\ncomponents of our approach. The components are further elaborated below.\n[CLIC] To tackle the issue of comprehensibility, the Law and Technology\nCentre of the University of Hong Kong has been running the Community\nLegal Information Centre (CLIC) since 2005 2. It is an online platform that\nprovides a quick internet guide for the general public to find relevant legal\ninformation in Hong Kong. 32 topics of the law that have the most direct\nbearing to people’s daily lives are being covered. Some example topics include\n“Landlord and Tenant”, “Traffic Offenses”, “Medical Negligence”, and “Sexual\nOffenses”. Technical legal concepts that are relevant to the selected topics\nare explained in layperson’s terms and are presented in web pages on the\nCLIC platform. We call each such web page a CLIC-page. Essentially, each\nCLIC-page is a small article that focuses on explaining certain legal concept.\nCLIC-pages are presented in different formats, such as illustrative cases and\nFAQs. The objective of CLIC is to present the law (more precisely, the legal\nrules concerning given topics) in less formal and less technical languages so\nthat the law is more comprehensible to the general public. CLIC currently\ncontains more than 2,000 CLIC-pages, which is to be expanded to 2,500–3,000\npages by the year 2024. As an example, Figure 2 shows an excerpt of a CLIC-\npage, which explains six data protection principles. Through this approach,\nthe CLIC has directly addressed the issue of comprehensibility. However, the\nsite is heavily dependent on a reader’s skills in identifying the correct legal\nissue and finding the answer from the relevant web page. In other words, the\n“navigability” issue largely remains.\n2https://www.clic.org.hk/\n4 Article Title\nTHEMEANINGOF\"PERSONALDATA\"ANDTHESIXDATAPROTECTIONPRINCIPLESPersonaldatameansanydata“relatingdirectlyorindirectlytoalivingindividual,fromwhichitispossibleandpracticaltoascertaintheidentityoftheindividualfromthesaiddata,inaforminwhichaccesstoorprocessingofthedataispracticable”(e.g.adocumentoravideotape).Thelegaldefinitionofpersonaldatacanbefoundinsection2ofthePersonalData(Privacy)Ordinance(Cap.486)(\"theOrdinance\").…ThesixdataprotectionprinciplesAnypersonororganizationcollecting,holding,processingorusingpersonaldatamustcomplywiththesixdataprotectionprinciples…Principle1–purposeandmannerofcollectionofpersonaldataPersonaldatamustbecollectedforalawfulpurpose.Thepurposeofcollectionmustbedirectlyrelatedtoafunctionoractivityofthedatauser.Thedatacollectedshouldbeadequatebutnotexcessiveinrelationtothatpurpose.Personaldatashouldalsobecollectedbylawfulandfairmeans.Unauthorizedaccesstoanotherperson'sbankaccountrecordsorcreditcardinformationisanexampleofunlawfulmeansofcollectingpersonaldata.Ifaperson/organizationintentionallyusesamisleadingwaytocollectpersonaldata,thisamountstoanunfairmeansofdatacollection.Acompanycollectingthepersonaldataofjobapplicantsbymeansofrecruitmentactivitieswheninfacttheyarenotreallyrecruitinganyoneisanexampleofunfairmeansofcollectingpersonaldata.Whenpersonaldataarecollectedfromanindividual,thatperson(thedatasubject)mustbeprovidedwiththefollowinginformation,whichincludes:•thepurposeforwhichthedataaretobeused;•theclassesofpersonstowhomthedatamaybetransferred;•whetheritisobligatoryorvoluntaryforthedatasubjecttosupplythedata;•theconsequencesarisingifthedatasubjectfailstosupplythedata;and•thedatasubjecthastherighttorequestaccesstoandcorrectionofthedata.Principle2–accuracyanddurationofretentionofpersonaldataDatausersmustensurethatthedataheldareaccurateandup-to-date…Principle3–useofpersonaldataUnlesspersonaldataareusedwiththeprescribedconsentofthedatasubject,thedatamustnotbeusedforanypurposeotherthantheonementionedatthetimethedatawerecollected(oradirectlyrelatedpurpose)…Principle4–securityofpersonaldataDatausersmusttakeappropriatesecuritymeasurestoprotectpersonaldata…Principle5–informationtobegenerallyavailableDatausersmustpubliclydisclosethekind(notthecontent)ofpersonaldataheldbythemandtheirpoliciesandpracticesonhowtheyhandlepersonaldata…Principle6–accesstopersonaldataAdatasubjectisentitledtoaskadatauserwhetherornotthedatauserholdsanyofhis/herpersonaldata,andtorequestacopyofsuchpersonaldataheldbythatuser…\nFig. 2 A CLIC-page excerpt (page id: 69).\n[Legal Question Bank (LQB)]To address the issue of navigability, and\nto improve comprehensibility of the law, the second component of our three-\nstep approach is to construct a legal question bank (LQB) to enhance the\nhuman-legal interaction of the CLIC platform. The idea is to create a large\ncollection of model questions whose answers could be found in some rele-\nvant CLIC-pages. For example, a model question for the CLIC-page shown\nin Figure 2 is, “ What are the points to pay attention to in collecting personal\ndata?” This question can be answered by the paragraphs under “Principle\nArticle Title 5\n1” in the CLIC-page. For each question q collected in the question bank, we\nidentify an answer scope s(q), which indicates the CLIC-page and the para-\ngraphs within which the answer to question q can be found. ( q, s(q)) thus\nforms a question-answer pair. With the question bank, a user needs not express\nhis/her legal question directly. Instead, the user can pick model questions from\nthe question bank to phrase his/her legal concerns and retrieve their answers\nthrough the questions’ answer scopes.\n[CLIC Recommender (CRec)]As we will show later, dozens of ques-\ntions can be created for each CLIC-page. With a target of collecting 100,000\nquestions into our LQB on CLIC, it is impractical for a user to identify relevant\nmodel questions from the big LQB. Our third component is an AI assistant\ncalled the CLIC Recommender (or CRec). CRec is equipped with natural lan-\nguage processing (NLP) capability such that it can converse with users and\nunderstand their legal issues. The objective of CRec is to comprehend a user’s\nverbal description of a legal scenario and shortlist a few model questions from\nthe question bank that can most likely express the user’s legal issues. Instead\nof sifting through the large question bank, the user only needs to go through\na short list (say, 10) of questions recommended by CRec.\nIn this paper we focus on the technical aspects of creating the legal ques-\ntion bank. In particular, we investigate how questions and their answer scopes,\nthat is, the ( q, s(q)) pairs, can be effectively and efficiently created. We pro-\npose to apply a large-scale pre-trained language model, such as GPT-3, to\ncreate machine-generated questions (MGQs). We also employ human workers\nto create human-composed questions (HCQs). We compare and contrast the\ntwo methods of question creation. As we will see later in our evaluation results,\nMGQs are more scalable and cost-effective, and they are often more diversified\ncompared with HCQs. On the other hand, HCQs are generally more precise.\nOverall, the two question-creation methods complement each other well.\nThe rest of the paper is organized as follows. Section 2 discusses related\nworks. Section 3 discusses the creation of the LQB. In particular, we give details\non how to use GPT-3 to generate MGQs. Section 4 gives a comprehensive\nevaluation of our question generation techniques. Section 5 discusses the design\nof CRec and illustrates it with an example. Finally, Section 6 concludes the\npaper.\n2 Context of the Problem\nIn this section we briefly explain the challenges to accessibility to law. Men-\ntioned earlier, there are three major dimensions to the concept of accessibility.\nThe first issue of availability is largely addressed by online dissemination of\nfree legal information. While most literature has dealt with the issue of com-\nprehensibility, the remaining issue of navigability has not been adequately\nstudied and explored. We first present some existing works on the comprehen-\nsibility issue. Then, we introduce the pre-trained language model GPT-3 and\n6 Article Title\nother works on machine question generation, which are designed to address\nthe challenge of navigability and to enhance comprehensibility.\n2.1 Navigability and Comprehensibility of Legal\nInformation\nRight of access to law is closely tied to the exercise of other fundamental rights\nsuch as freedom of speech, access to justice and equality before law (Mom-\nmers, 2011). Thus, attaining fair access to law is crucial to one’s meaningful\nparticipation in a society. Similar to the opinion of the New Zealand Law Com-\nmission discussed earlier, Mommers (2011) argues that the concept of access to\nlegal information has embodied three different levels, namely, (1) making legal\ninformation available and searchable, (2) linking relevant legal information and\ndocuments, and (3) offering context dependent translation of legal knowledge\nto targeted audience. Based on this proposed standard, Mommers studies two\nlegal database websites: EUR-Lex and www.officielebekendmakingen.nl, and\nfinds that while the websites generally achieve the first two levels, the third one\nis largely ignored. It thus shows that “comprehensibility” of legal information\nto the general public is the most challenging issue.\nIn fact, comprehensibility of legal information is a perennial problem that\nscholars have been studying. Dyson and Schellenberg (2017) conducts readabil-\nity analysis of 407 text paragraphs in government legal services websites. The\nanalysis is done based on the readability standard established in the The Plain\nLanguage Action and Information Network (PLAIN)3. The study shows that\nthe readability of legal aid websites such as LawHelp.org and LawHelpInterac-\ntive.org is generally beyond the comprehension of most Americans, especially\nthose of vulnerable groups with lower literacy skills. The study highlights the\nimportance of appropriate readability standards of legal-aid information. Also,\nRuohonen (2021) analyzes the readability of 201 legislations and related policy\ndocuments in the European Union (EU). The analysis is done with five quan-\ntitative readability indices (Flesch-Kincaid, SMOG, ARI, Coleman-Liau, and\nLinsear Write). It is found that PhD-level education is required to comprehend\ncertain laws and policy documents. Becher and Benoliel (2021) uses the Flesch\nReading Ease (FRE) test and the Flesch-Kincaid test to evaluate the readabil-\nity of the privacy policies of 300 popular websites made to implement the EU\nGeneral Data Protection Regulation (GDPR). Likewise, the study finds that\nmost users found privacy policies largely unreadable.\nThis situation is regrettable since the ability to understand legal informa-\ntion is related directly to one’s ability to “navigate” the legal matrix, i.e., the\nability to identify the correct legal issue, to know and reach the relevant legal\nrule or principle, so as to solve one’s legal problem. Many members of the\npublic may not realize that their problems are of a legal nature and may not\nunderstand their legal rights concerning the problems. To empower the gen-\neral public to access the law in its full dimension, we design an LQB, which\n3https://www.plainlanguage.gov/\nArticle Title 7\nhelps the users to identify and capture the correct legal issue and relevant legal\nprinciple, to understand the law in plain legal terms, and to find the primary\nlegal source.\n2.2 Question Generation and Pre-trained Language\nModels\nHere, we focus on the technical aspects of creating an LQB. In particular, we\ninvestigate how large-scale pre-trained language models (PLMs) such as GPT-\n3 are employed to create machine-generated questions (MGQs). In this section,\nwe give brief descriptions of works related to machine question generation and\nalso those of PLMs.\nThere are a number of applications of machine question generation or\nrecommendation. As an example, search engines can generate/recommend\nquestions to users to help them better express their search intents. To illus-\ntrate, if one googles “ What is libel?”, the search engine will suggest a number\nof other (related) questions, such as “ What is the meaning of libel and slan-\nder?”. This helps users compose their queries with better choices of words\nand expressions, resulting in more effective search. Machine question genera-\ntion is also used in education, where students are asked (machine-generated)\nquestions that cover the contents of articles the students have read. This\nprovides a low-cost approach to evaluate students’ understanding of learning\nmaterials. For example, Steuer et al. (2022) presents a case study that shows\nhow machine-generated short-answer questions can help improve non-native\nEnglish speakers’ reading comprehension ability.\nGiven a document d and a span of text a in d, the problem of answer-driven\nquestion generation is to generate questions q whose answers are covered by\n(the intended answer) a. Example works on this problem include Kim et al.\n(2019); Liu et al. (2020); Song et al. (2018). It is relatively straightforward to\ncreate a question q if the answer a is a simple factual statement by syntactic\nmanipulation of the words in a. For example, from “ John likes apples, ” one\ncan easily replace a noun by a question word to get “ Who likes apples? ” The\nchallenge lies in generating questions that go beyond simple single-sentence\nfactoid questions to semantic-based questions that cover a larger span of text\nin the input. For example, given a document that lists the steps of litigation,\na good question would be “ What are the phases of litigation? ” There are also\nworks (e.g., Wang et al. (2019)) that address the question-generation prob-\nlem without assuming any intended answers (“ a”) are given. This is a more\nchallenging problem because the machine is not guided by any given intended\nanswers and has to figure out the major focuses of the input text by itself. In\nour context, only CLIC-pages are given to the machine, and our machine ques-\ntion generation method has to create semantically-important questions that\nbest summarize the information contained in the CLIC-pages.\nEarly works on machine question generation employ rule-based methods\nto rewrite input sentences into questions through lexical and syntactic analy-\nsis (Das et al., 2016; Heilman and Smith, 2010; Lindberg et al., 2013). However,\n8 Article Title\nthese artificially created rules/templates are usually difficult to design and are\nnot general enough to be applied across domains. Recent works apply neu-\nral networks techniques to train question generation models. Du et al. (2017)\ndesigns the first fully data-driven approach considering question generation in\na machine reading comprehension setting. Wang et al. (2019) presents a multi-\ntask learning framework to simultaneously identify question-worthy phrases\nin documents and construct questions accordingly. A disadvantage of these\nneural-network-based approaches is that they require large sets of training\ndata, which are very expensive to obtain. Instead, our approach is to use\npre-trained models (PLMs) for unsupervised question generation.\nRecent advances in PLMs see many interesting applications of unsupervised\nmachine text generation. For example, Schick and Sch¨ utze (2021) proposes a\nmethod to obtain quality sentence embedding using PLM without data label-\ning, fine-tuning or modifying the pre-trained model. Dai et al. (2022) proposes\nan interesting approach, called dialog inpainting, which converts a document\n(such as a Wikipedia page) into a synthetic two-person dialog. The idea is\nto treat sentences in a document as a writer’s utterances and then to predict\nwhat an imaginary reader would ask or say between the writer’s utterances.\nWang et al. (2022) proposes a technique of generating questions using PLMs\nand how the technique is applied in education applications. The work studies\nprompting strategies used in executing the PLMs to generate questions given\ntarget answers.\nGPT-3 (Brown et al., 2020) is a highly popular PLM that has an impressive\ntext generation ability (Min et al., 2021). GPT-3 is trained with a document\ncollection that consists of around 500 billion words. The training data comes\nfrom multiple text sources, including Common Crawl (Raffel et al., 2020),\nWebText2 (Kaplan et al., 2020), two book corpuses, and Wikipedia. The GPT-\n3 model has up to 175 billion parameters, which makes it 100 times larger than\nits predecessor GPT-2 (Radford et al., 2019). In this paper we will discuss how\nwe leverage GPT-3’s text generation ability to create MGQs.\n3 Method\nIn this section we describe our process of creating the legal question bank\n(LQB) from CLIC-pages. We first give a formal definition of the problem\n(Section 3.1). Then, we describe the steps we take to generate questions. These\ninclude (1) Prompting (Section 3.2), (2) Partitioning (Section 3.3), and (3)\nDeduplication (Section 3.4).\n3.1 Problem Definition\nLet D be a collection of CLIC-pages. Our objective is to generate, for each\ndocument (CLIC-page) d ∈ D, a set of questionsQd. Specifically, each question\nq ∈ Qd is presented in a question-answer pair that takes the form ( q, s(q)),\nwhere s(q) is called the answer scope of q. The scope s(q) is represented by\n[d: pid-list], where pid-list is a list of paragraph id’s in the CLIC-page d where\nArticle Title 9\nthe answer of question q can be found. Essentially, the answer scope s(q) of\nquestion q tells us which CLIC-page d and which part of it (given by pid-list)\nthe question is answered. (We use s(q).d and s(q).pid-list to represent the two\npieces of information, respectively.) In case a question q generated is irrelevant\nto d (i.e., d does not answer q), we set s(q).pid-list to NULL. As an example,\nthe question q1: “What are the points to pay attention to in collecting personal\ndata?” can be answered by paragraphs 3 to 5 of the CLIC-page shown in\nFigure 2 (page id: 69). The answer scope of q1 is therefore s(q1) = [69 :\n{3, 4, 5}].\nWe collect all the questions generated for all the CLIC-pages into an LQB.\nHere, we put forward some desirable characteristics of a good-quality LQB.\n[Quantity] The LQB serves as a collection of model questions for users to\nrelate their legal situations. The LQB questions should therefore be of sufficient\nquantity so that the matching (between user’s situation and model questions)\nis effective.\n[Precision] The LQB should contain questions that are relevant to the\nCLIC-pages. That is, the questions in the LQB can be answered by the CLIC-\npages.\n[Coverage] A typical CLIC-page consists of multiple paragraphs, each\nexpressing a certain idea. Also, for more complex legal issues, their CLIC-pages\nare longer and may consist of multiple sections, each describing a certain aspect\nof a legal issue. A CLIC-page, therefore, often covers multiple facts or concepts.\nThe questions generated for a CLIC-page should have good coverage of the\npage’s contents. That is, all of the paragraphs and sections should be included\nas part of the answer scopes of some questions generated. In contrast, if the\nquestions generated for a CLIC-page all cover, say, only the first paragraph\nof the page, then we say that the questions have poor coverage of the page\nbecause the rest of the page is not addressed.\n[Diversity] The questions generated for a page should have diverse scopes.\nThat is, we want some questions that arespecific that cover narrow scopes (e.g,\nover one paragraph of a page’s content); we also want some questions that are\nof higher conceptual level that cover broader scopes (e.g., over a section of a\npage or even the whole page).\n[Other Qualitative Measures]In our study we found that some ques-\ntions generated by GPT-3 for a CLIC-page are valuable and relevant despite\nthat the questions cannot be answered by the page’s content. As an example,\nfor the CLIC-page shown in Figure 2 (on personal data), GPT-3 generates the\nquestion “How can I make a complaint if I think my personal data has been\nmishandled?” Note that the CLIC-page only explains what constitutes per-\nsonal data mishandling; the page does not provide information on the legal\nactions that a victim of such mishandling can take. Although the question\nis not answered by the CLIC-page, it is nonetheless a very relevant ques-\ntion. Relevant but unanswered questions of this sort indeed help us identify\nareas of omissions, which give us hints on how certain CLIC-pages should be\n10 Article Title\nrevised and enriched. Moreover, the machine can occasionally generate mul-\ntiple questions that essentially ask the same legal question but from different\nperspectives. For example, as we will illustrate later in Section 4.1, for the\nCLIC-page that addresses the legal issues related to “Rates, Management Fees\nand other Charges” under the landlord-and-tenant topic, the machine was able\nto generate different questions for the same details, some are from the perspec-\ntive of tenants while others are from the perspective of landlords. The different\nphrasing of the same question (from different perspectives) helps to match dif-\nferent user’s situations to model questions in the LQB more effectively, and\nprovides different approaches to navigate the CLIC-page concerned.\nIn Section 4, we will evaluate the machine-generated questions (MGQs)\nquantitatively with respect to the first four criteria. We will also give a\nqualitative study on the MGQs.\n3.2 GPT-3 Prompting\nGPT-3 (Brown et al., 2020) is a pre-trained language model with strong text\ngenerating capability. GPT-3 takes as input a prompt c = [c0, c1, ...cM ], which\nis a sequence of word tokens that serves as a “context” for text generation.\nSpecifically, it can compute the probability of a given word appearing next in a\ntextual sequence. Equation 1 shows the process of auto-regressive generation,\nwhere θ represents GPT-3’s model parameters, and pθ(xt|x0, ..., xt−1, c) gives\nthe probability of generating xt as the (t + 1)-st word given the prompt c and\nthe first t words generated. Given a prompt c, GPT-3 generates subsequent\ntext probabilistically.\npθ(x|c) = pθ(x0|c)\nTY\nt=1\npθ(xt|x0, ..., xt−1, c). (1)\nTo induce GPT-3 to generate questions for a given CLIC-page d, we need\nto design a prompt c that provides the context of page d and also instructs\nGPT-3 to output “questions”. This is achieved in two steps. First, wepartition\npage d into text segments. Second, for each text segment we append an FAQ\nsentence, literally, “10 frequently-asked questions (FAQs):” to the segment to\nform a prompt. We then input each prompt (created for each segment of d)\nto GPT-3 to obtain questions. All questions generated with respect to all the\nprompts are collected as questions of CLIC-page d. Figure 3 illustrates the\nprocess of constructing prompts from a CLIC-page.\nIn our study, we apply the following settings when we execute GPT-3:\ntemperature = 1.0, top p = 0.9, frequency penalty = 0.3, and presence penalty\n= 0.1.\nNext, we discuss different partitioning strategies, which result in different\nprompts created and thus different question collections.\nArticle Title 11\nMOTOR INSURANCE\nA. Overview\nMotor insurance is designed to provide cover for loss or damage to your motor vehicle and that of\nthird parties, and bodily injury or death of third parties. The scope of cover under a motor insurance\npolicy varies, e.g. it may insure you against your costs of repairing / replacing your car following an\naccident or it may insure against your liability to pay compensation to an injured person following a\ntraffic accident. Insurance coverage varies according to the terms and conditions of an insurance\npolicy.\nThe major types of motor insurance are summarized as follows:\n1. Third party liabilities\nThird party liability covers you against liabilities of property damage claims and/or personal injury\nclaims by third parties. Under the Motor Vehicles Insurance (Third Party Risks) Ordinance (Cap. 272)\n(“MVIO”),…\nMotor insurance is designed to provide cover for loss or damage to your motor vehicle and that of\nthird parties, and bodily injury or death of third parties. The scope of cover under a motor insurance\npolicy varies, e.g. it may insure you against your costs of repairing / replacing your car following an\naccident or it may insure against your liability to pay compensation to an injured person following a\ntraffic accident. Insurance coverage varies according to the terms and conditions of an insurance\npolicy.\n10 frequently-asked questions (FAQs):\n(a) CLIC-Page\n(b) Input to GPT-3\n1. What is the purpose of motor insurance?\n2. What kind of insurance provide cover for loss or damage to my motor vehicle and that of third\nparties, and bodily injury or death of third parties?\n…\n10. Is it compulsory to take out a motor insurance policy with an authorized insurer covering the\nliability for bodily injury or death of any third party arising out of the use of motor vehicles on a road?\n(c) Output of GPT-3\nFig. 3 Prompting.\n3.3 CLIC-page Partitioning\nAs we have mentioned, each CLIC-page could consist of multiple sections,\neach with multiple paragraphs. Different sections and paragraphs focus on\npresenting different legal concepts, rules or principles, each considered a legal\nknowledge element. Ideally, the questions generated by GPT-3 align with the\nelements presented in a CLIC-page. Given a CLIC-page d, we consider the\nfollowing partitioning strategies in prompt creation.\n[Section-based partitioning]Each section in d forms one partition. We\nappend the FAQ sentence, “ 10 frequently-asked questions (FAQs): ” to each\nsection to form one prompt. Each section of d thus results in one collection of\nquestions generated by executing GPT-3 once.\n[Paragraph-based partitioning]Each paragraph in d forms one parti-\ntion with the FAQ sentence appended to form one prompt. GPT-3 is executed\n12 Article Title\nonce per paragraph to generate questions. Compared with section-based parti-\ntioning, paragraph-based partitioning creates more questions (because GPT-3\nis executed once per paragraph instead of once per section). However, the con-\ntext given by each paragraph is narrower than that of each section. This may\nresult in GPT-3 generating questions that are too specific (to each paragraph)\ninstead of those that are more general and higher-level (that cover multiple\nparagraphs or a section).\n[Hybrid partitioning] The idea is to provide GPT-3 with section-level\ncontext but with paragraph-level attention. Specifically, we partition d by\nsections. For each section, we prepend the paragraphs within it each with a\nparagraph label. A prompt is formed by appending the following sentence “ 10\nmost frequently-asked questions for [x]”, where [x] is a paragraph label. We\niterate through the paragraph labels of the section, and repeat the steps for\neach section in the CLIC-page d. Under Hybrid partitioning, GPT-3 is exe-\ncuted once per paragraph in d (as in paragraph-based partitioning) and so\nthe number of questions collected is comparable to that under paragraph-\nbased partitioning. Also, each prompt includes one section. This gives GPT-3 a\nbroader context (as in section-based partitioning) for more diversified question\ngeneration.\nFigure 4 illustrates the three partitioning strategies in prompt creation.\nWe will compare the quality of the LQBs generated by the three strategies in\nSection 4.\nFor each generated questionq, we need to associate with it an answer scope\ns(q) = [d : pid-list] (see Section 3.1). We set d to the CLIC-page from which q\nis generated. Also, pid-list consists of all the paragraphs that are included in\nthe prompt that generates q. The scope of a question can be further refined\nby manual verification.\n3.4 Deduplication\nWith paragraph-based partitioning and hybrid partitioning, GPT-3 is executed\nonce to generate questions for each paragraph of a given CLIC-page. The\nprocess could generate tens of questions for a CLIC-page if the page is long\nand content-rich. We observe that a small fraction of the questions generated\nby the model are redundant (i.e., they essentially ask the same legal questions\nwith slightly different phrasing). We perform question deduplication to remove\nredundant questions in the LQB.\nWe first measure the similarity of questions in the LQB using sentence\nembedding. Specifically, we apply the pre-trained model DistilBERT4 to obtain\nan embedding vector vq for each question q ∈ LQB. For each pair of questions\nq1 and q2, we measure their similarity ( sim(q1, q2)) by the cosine similarity of\ntheir embedding vectors. That is,\nsim(q1, q2) = cosine(vq1 , vq2 ) = (vq1 · vq2 )/∥vq1 ∥∥vq2 ∥.\n4distilbert-base-nli-stsb-quora-ranking\nArticle Title 13\nD. Motor Insurers’ Bureau of Hong Kong\nThe protection under the statutory compulsory motor\ninsurance regime does not cover the following three\nsituations: (i) the driver fails to …\nIn order to remedy the above situations, the Motor\nInsurers’Bureau of Hong Kong (“MIBHK”) was established\nin 1980 to administer a non-statutory compensation\nscheme. All authorized insurers …\nE. Purchasing a suitable motor insurance\nApart from the level of premium, a car owner should also\nconsider the nature and scope of coverage, the intended\nuse of the vehicle, the amount of excess, …\nD. Motor Insurers’ Bureau of Hong Kong\nThe protection under the statutory compulsory motor\ninsurance regime does not cover the following three\nsituations: (i) the driver fails to …\nIn order to remedy the above situations, the Motor\nInsurers’Bureau of Hong Kong (“MIBHK”) was established\nin 1980 to administer a non-statutory compensation\nscheme. All authorized insurers …\n10 frequently-asked questions (FAQs):\nE. Purchasing a suitable motor insurance\nApart from the level of premium, a car owner should also\nconsider the nature and scope of coverage, the intended\nuse of the vehicle, the amount of excess, …\n10 frequently-asked questions (FAQs):\nD. Motor Insurers’ Bureau of Hong Kong\nThe protection under the statutory compulsory motor\ninsurance regime does not cover the following three\nsituations: (i) the driver fails to …\n10 frequently-asked questions (FAQs):\nIn order to remedy the above situations, the Motor\nInsurers’Bureau of Hong Kong (“MIBHK”) was established\nin 1980 to administer a non-statutory compensation\nscheme. All authorized insurers …\n10 frequently-asked questions (FAQs):\nE. Purchasing a suitable motor insurance\nApart from the level of premium, a car owner should also\nconsider the nature and scope of coverage, the intended\nuse of the vehicle, the amount of excess, …\n10 frequently-asked questions (FAQs):\nD. Motor Insurers’ Bureau of Hong Kong\nParagraph 1: The protection under the statutory\ncompulsory motor insurance regime does not cover the\nfollowing three situations: (i) the driver fails to …\nParagraph 2: In order to remedy the above situations, the\nMotor Insurers’ Bureau of Hong Kong (“MIBHK”) was\nestablished in 1980 to administer a non-statutory\ncompensation scheme. All authorized insurers …\n10 most frequently-asked questions for Paragraph 1:\nE. Purchasing a suitable motor insurance\nParagraph 1: Apart from the level of premium, a car\nowner should also consider the nature and scope of\ncoverage, the intended use of the vehicle, the amount of\nexcess, …\n10 most frequently-asked questions for Paragraph 1:\nD. Motor Insurers’ Bureau of Hong Kong\nParagraph 1: …\nParagraph 2: …\n10 most frequently-asked questions for Paragraph 2:\n(a) CLIC-Page (b) Section-Based Partitioning\n(c) Paragraph-Based Partitioning (d) Hybrid Partitioning\nFig. 4 Partitioning Strategies.\nWe then apply single-link clustering to cluster the questions in the LQB with\na similarity threshold of 0.95. For each cluster, we retain one question (chosen\nrandomly) and discard the rest.\n14 Article Title\n4 Evaluation\nIn this section we evaluate the performance of the three partitioning strategies\nand the quality of the LQBs they create with respect to the four quantitative\nmeasures, namely, quantity, precision, coverage and diversity (see Section 3.1).\nWe apply GPT-3 to generate questions for 1,557 CLIC-pages on the CLIC\nplatform. With the Hybrid strategy, we obtain 59,798 questions in the LQB.\nWe manually evaluate machine-generated questions and compare them\nwith human-composed questions. To do so, we sampled 100 CLIC-pages under\n5 topics on CLIC, namely,landlord and tenant (31), defamation (11), insurance\n(26), personal data privacy (9), and intellectual property (13). (The numbers in\nparentheses give the number of CLIC-pages chosen for each topic.) Specifically,\nwe hire human workers, who have acquired formal legal training, to perform\nthe following tasks:\n1. Read the 100 CLIC-pages and compose questions whose answers can be\nfound in those CLIC-pages. The workers are instructed to compose as many\nquestions as they can. They are also required to identify an answer scope\nfor each question they create. Essentially, this process is to create an LQB\nmanually for a small set of 100 CLIC-pages. In total, the workers created\n2,686 questions.\n2. For each of the 100 CLIC-pages, read the machine-generated questions\n(MGQs) that are generated by the three partitioning strategies. For each\nMGQ, determine if the question can be answered by the corresponding\nCLIC-page. If so, the question is marked “ correct”. Also, the workers are\nasked to verify the answer scopes of the questions and modify them if nec-\nessary. In total, the workers inspected 11,430 MGQs (that are generated by\nthe three partitioning strategies).\nThe above manual tasks took about 400 person-hours to complete. We\nremark that although the comparison study is performed over a small 100\nCLIC-page sample, the study involves evaluating 11,430 MGQs and 2,686\nhuman-composed questions (HCQs) for a total of about fourteen thousand\nquestions. The evaluation results we present in the rest of this section is\ntherefore representative.\nFor illustration purpose, Appendix A lists the questions generated by the\nvarious methods and by human workers for one of the CLIC-pages.\n4.1 Results\nQuantity. We start with comparing the number of questions generated by\neach strategy. Recall that a larger LQB is more desirable because it provides\nmore model questions to match against a user’s legal situation. This generally\nresults in a higher success rate of helping a user to phrase his/her legal issue via\nthe model questions. Moreover, a larger LQB is more likely to provide better\ncoverage of the CLIC-pages’ contents, and thus is more effective in capturing\nthe legal knowledge presented in the CLIC-pages.\nArticle Title 15\nBefore we discuss the results, we would like to remark that while we instruct\nGPT-3 to generate 10 FAQs with each prompt, we do not set a hard quan-\ntity (10 questions per paragraph) for human workers. Instead, human workers\nare instructed to write “as many questions as possible”. The reason is that we\nwant to compare the two approaches (machine vs human) in a practical set-\nting. Specifically, giving human workers a hard requirement of 10 questions per\nparagraph leads to several issues. First, human workers would interpret the\ninstruction as to associate each question they compose to one specific para-\ngraph. This would inadvertently cause workers to deliver only paragraph-level\nquestions, i.e., those that are very specific to a paragraph, instead of more\ngeneral questions that cover a wider scope (such as section-level questions).\nSecondly, the quantity of questions generated can vary based on the richness\nof information within a paragraph. Some paragraphs naturally yield a larger\nnumber of questions, while others do not. We opted to leave the decision of how\nmany questions to generate per paragraph to the human workers to account\nfor this variability. Finally, it becomes progressively more difficult and time-\nconsuming for human workers to write questions that are beyond what the\nworkers naturally think are the most logical and interesting questions. Con-\nsequently, a hard 10-questions-per-paragraph instruction would significantly\nincrease the time and cost of the manual question composition task. For these\nreasons, we do not set a hard requirement (10 questions per paragraph) to\nhuman workers.\nTable 1 shows the number of questions generated by the three partition-\ning strategies for the sample of 100 CLIC-pages. Note that the questions\ngiven by each method have been deduplicated by our deduplication algorithm\n(see Section 3.4). The number of human-composed questions (HCQs) is also\nshown (last column) as a reference. From the table, we see that section-based\npartitioning gives much fewer questions than paragraph-based partitioning\nor Hybrid. This is because the former executes GPT-3 once per CLIC-page\nsection to generate questions, while the latter two do that for each paragraph.\nGPT-3 is thus executed more times and generates more questions (almost\nfour times as many) under paragraph-based partitioning and Hybrid compared\nwith section-based partitioning. Moreover, paragraph-based partitioning and\nHybrid generate similar number of questions. Comparing MGQs and HCQs,\nwe see that machine is capable of creating more questions (about twice as\nmany) compared with human workers, even though the human workers were\ninstructed to make the best effort to compose as many questions as they could.\nTable 1 Number of questions created under different methods (after deduplication) (100\nCLIC-page sample).\nMGQs HCQs\nMethod: Section-based Paragraph-based Hybrid Manual\nNumber of questions: 1,362 5,089 4,979 2,686\n16 Article Title\nFig. 5 Number of questions generated by each strategy (on 25 pages).\nFig. 6 Number of questions generated; Hybrid vs human workers (on 25 pages).\nWe further illustrate the differences among the partitioning strategies by\nshowing the number of questions generated by each of them for each of 25\nexample CLIC-pages that are considered in the evaluation. Figure 5 shows the\nnumbers. In the figure, the x-axis shows the CLIC-page id of the first 25 pages\nin our 100 CLIC-page sample. These 25 CLIC-pages come from two topics,\nnamely, landlord and tenant (15 pages) and defamation (10 pages). From the\nfigure, we see that paragraph-based partitioning and Hybrid generally provide\ncomparable numbers of questions, and they consistently outperform section-\nbased partitioning by large margins. As a notable example, for CLIC-page-8,\nparagraph-based partitioning (64) and Hybrid (65) generate about 9 times\nas many questions as section-based partitioning does (7). The reason is that\nCLIC-page-8 has a simple structure with one single section of 8 paragraphs.\nGPT-3 is therefore executed only once under section-based partitioning, but\nmany more times under paragraph-based partitioning and Hybrid.\nFigure 6 compares the number of questions generated by Hybrid and those\nby human workers over the 25 CLIC-pages. From the figure, we see that,\nin most cases, Hybrid generates much more questions than human workers.\nA representative example is CLIC-page-3, for which Hybrid gives 6 times as\nArticle Title 17\nTable 2 Question precision (over 100 CLIC-pages).\nMethod: Section-based Paragraph-based Hybrid\nPrecision: 50% 41% 68%\nmany questions as a human worker does. This CLIC-page addresses the issue\nof property maintenance under the landlord and tenant topic. We find that\nwhile human workers mostly ask questions from a tenant’s perspective, the\nmachine is able to ask questions from different perspectives, such as the per-\nspective of a tenant as well as that of a landlord. Moreover, on the same piece\nof legal knowledge, the machine is able to ask questions on different details. For\nexample, part of CLIC-page-3 discusses a scenario in which a tenant is uncoop-\nerative when the landlord wants to carry out maintenance work. On this legal\nissue, the machine asks multiple questions, such as “Can the landlord apply for\nan injunction? ”; “ Can the landlord terminate the tenancy agreement? ” The\nmachine generated questions are therefore richer with more variety.\nIn conclusion, generally, we can obtain more MGQs than HCQs and with a\nmuch lower (human) cost. Among the three partitioning strategies, paragraph-\nbased partitioning and Hybrid are much better in populating the LQB than\nsection-based partitioning.\nPrecision. Due to the probabilistic nature of GPT-3, questions generated\nby the machine for a CLIC-page are not guaranteed to be those that are\nanswered by the CLIC-page’s content. In our evaluation, every MGQ is verified\nby a human worker, who labels an MGQ q as “correct” if q’s answer can be\nfound in the CLIC-page d from which q is generated. We define the precision\nof a method as the fraction of questions the method generates that are labeled\n“correct”. Table 2 shows the precision of the three partitioning strategies when\nthey are applied to the 100 CLIC-page sample.\nFrom Table 2, we see that paragraph-based partitioning gives the lowest\nprecision among the three strategies. The reason is that under paragraph-\nbased partitioning, each prompt we input to GPT-3 (to generate questions)\nis a single paragraph of a CLIC-page. Since a single paragraph has limited\ncontent, the context (as given in the prompt) provided to GPT-3 is generally\nnot rich enough for it to create highly-precise questions. In contrast, section-\nbased partitioning improves precision over paragraph-based partitioning by\nproviding richer contexts. This is achieved by including a whole section of\na CLIC-page in the GPT-3 prompt. Remarkably, Hybrid partitioning gives\na much higher precision compared with the other two methods. The reason\nis that, like section-based partitioning, Hybrid includes a whole section of\na CLIC-page in prompting, which is context-rich. Also, Hybrid runs GPT-\n3 multiple times, each focusing on a specific paragraph (and thus a specific\nlegal knowledge element) of a CLIC-page. This combination of rich context\nand paragraph attention is shown to allow GPT-3 to generate more precise\nquestions.\nFigure 7 shows the precision of the methods when they are applied to each\nof the first 25 CLIC-pages of the 100 CLIC-page sample. From the figure, we\n18 Article Title\nFig. 7 Methods’ precision (on 25 pages).\nsee that the precision values (of each method) vary over the 25 CLIC-pages.\nThis variation is due to the probabilistic nature of GPT-3. In general, we\nobserve that Hybrid gives higher precision over the other methods. In particu-\nlar, Hybrid has the highest precision (compared with other methods) for 19 of\nthe 25 CLIC-pages; Its precision exceeds 70% for 14 CLIC-pages; Also, about\n95% of the questions Hybrid generates for CLIC-page-8 are correct.\nIn contrast, the performance of section-based partitioning in terms of pre-\ncision is less stable. From Figure 7, we see that it gives the best precision for\n6 of the 25 CLIC-pages (CLIC-pages 2, 4, 7, 17, 19, and 22), but the worst\nprecision for 13 other CLIC-pages. In particular, all questions section-based\npartitioning generates for CLIC-page-8 are incorrect (0% precision), but the\nmethod generates only correct questions for CLIC-page-22 (100% precision).\nThis variation in precision, especially for section-based partitioning, is\nagain due to the probabilistic nature of GPT-3. Given a context (prompt),\nGPT-3 will generate questions based on certain “ focuses” it captures in the\ncontext. Figure 8 illustrates this observation. The left column of Figure 8 shows\nan excerpt of a CLIC-page section, which consists of 4 paragraphs. The left\ncolumn shows the result of applying section-based partitioning, which submits\nthe whole section to GPT-3 as the prompt (context). In this example, GPT-3\ncaptures two focuses “ tenancy agreement” and “ landlord”. Under each cap-\ntured focus, we show the corresponding questions (enclosed in a dashed box)\ngenerated by GPT-3. The right column of Figure 8 shows the same CLIC-\npage section but in this case GPT-3 is prompted by the Hybrid partitioning\nstrategy. Similarly, in the right column, we show the focuses captured and\nthe questions generated by GPT-3 under Hybrid. Recall that Hybrid submits\nthe whole section to GPT-3 as a prompt (like section-based partitioning), but\nrepeats the process by iterating through the paragraphs taking one paragraph\nas the attention at a time (see Figure 4). From Figure 8, it is evidenced that\nHybrid is able to capture important focus in each paragraph. This results in\nmore precise questions.\nWe remark that whether the questions generated by GPT-3 are correct\nor not depends on whether GPT-3 is able to successfully capture meaningful\nfocuses in the context. For example, the CLIC-page section shown in Figure 8\nArticle Title 19\nIn the event that the tenant pays rent on time but commits\nserious breach(s) of the tenancy agreement (e.g. subletting,\nconducting illegal activities, causing nuisance, installation\nof illegal structures or causing enforcement actions by the\nIncorporated Owners), the landlord may wish to terminate\nthe tenancy and find another replacement tenant.\nIn such case, it would be necessary for the landlord to rely\non any forfeiture/termination clause as expressly provided\nunder the tenancy agreement to put an end to the tenancy\nand claim possession from the tenant. If the tenancy\nagreement is silent on such matter, the landlord (for\nresidential premises only) may only rely on section 117(3)(d)\nto (h) of the Landlord and Tenant (Consolidation)\nOrdinance (Cap. 7) to exercise implied rights of forfeiture as\na fallback. Note that the law does not imply such right to\nterminate the tenancy agreement for tenancy agreements\nother than residential tenancies.\nThe landlord who wishes to terminate the tenancy on such\nground is also required to give prior written notice to the\ntenant by specifying the breach and require the tenant to\nremedy the breach (or compensation payable) before\ntermination and/or claiming possession of the property\nagainst the tenant.\nWhen the claim for possession is heard before the Court,\nthe Court has a discretion in deciding whether or not to\ngrant ‘relief against forfeiture’ in favor of a tenant under\nsection 58 of the Conveyancing and Property Ordinance…\n(a) Section-Based Partitioning\n• What should I do if I have a problem with my landlord?\nIn the event that the tenant pays rent on time but commits\nserious breach(s) of the tenancy agreement (e.g. subletting,\nconducting illegal activities, causing nuisance, installation\nof illegal structures or causing enforcement actions by the\nIncorporated Owners), the landlord may wish to terminate\nthe tenancy and find another replacement tenant.\nIn such case, it would be necessary for the landlord to rely\non any forfeiture/termination clause as expressly provided\nunder the tenancy agreement to put an end to the tenancy\nand claim possession from the tenant. If the tenancy\nagreement is silent on such matter, the landlord (for\nresidential premises only) may only rely on section 117(3)(d)\nto (h) of the Landlord and Tenant (Consolidation)\nOrdinance (Cap. 7) to exercise implied rights of forfeiture as\na fallback. Note that the law does not imply such right to\nterminate the tenancy agreement for tenancy agreements\nother than residential tenancies.\nThe landlord who wishes to terminate the tenancy on such\nground is also required to give prior written notice to the\ntenant by specifying the breach and require the tenant to\nremedy the breach (or compensation payable) before\ntermination and/or claiming possession of the property\nagainst the tenant.\nWhen the claim for possession is heard before the Court, …\n(b) Hybrid Partitioning\n• What is tenancy agreement?\n• What are the different types of tenancy agreements?\n• What is a fixed-term tenancy agreement?\n• What is a periodic tenancy agreement?\n• What is a rolling tenancy agreement?\n• What is a statutory tenancy agreement?\n• What are the benefits of having a tenancy agreement?\n• What is a serious breach of tenancy agreement?\n• What is a forfeiture/termination clause?\n• What is the procedure for the landlord to claim\npossession of the property from the tenant?\n• How can I give notice to the tenant?\n• What if the breach is not remediable?\n• What are my rights if the tenant doesn't remedy the\nbreach?\nFig. 8 Illustration of focuses (underlined and highlighted in red) captured by GPT-3 and\nthe corresponding questions generated (enclosed in dashed-boxes) with (a) section-based\npartitioning and (b) Hybrid.\nis about the legal issues concerning breaches of a tenancy agreement. When\nGPT-3 is executed once on the section under section-based partitioning (left\ncolumn), the focuses GPT-3 captures, such as “tenancy agreement”, are not\nvery precise. The questions generated in this case are therefore not quite cor-\nrect. However, under Hybrid, GPT-3 is given, not only the whole section as a\ncontext, but also specific attention to the paragraphs, one at a time. As is seen\nin Figure 8’s right column, GPT-3 is able to capture focuses more precisely,\nresulting in more precise questions.\nSo far, we have seen two advantages of Hybrid compared with section-\nbased partitioning and paragraph-based partitioning: It gives large number of\nquestions (Table 1) and more precise questions (Table 2). So, based on the\nmeasures of quantity and precision combined, Hybrid is a better strategy. An\ninteresting question is how Hybrid fares against human workers in constructing\nan LQB. For the 100 CLIC-page sample, Hybrid generates 4,979 questions.\nAt a precision of 68%, we have 4,979 × 68% ≈ 3,400 correct questions. This\ncompares favorably against human-composed questions: Not only does Hybrid\ngive more (correct) questions (3,400 MGQs vs 2,686 HCQs), it is also a more\n20 Article Title\ncost-effective approach. The latter is because it is much easier and faster to\nmanually verify an MGQ than to manually create an HCQ.\nCoverage and Diversity.In Section 3.1, we define the scope s(q) of a\nquestion q generated from a CLIC-page d as [d : pid-list], where pid-list is a\nlist of paragraph id’s from which the answer of question q can be found. We\nsay that question q covers the contents presented in the paragraphs listed in\npid-list. For a good LQB, the question bank should contain sufficient questions\nso that all contents of the CLIC-pages are covered. We measure the coverage\nof an LQB by the fraction of paragraphs in the CLIC-page collection that are\ncovered by the questions in the LQB. Formally,\ncoverage(LQB) =\n\f\f\f\f ∪\nq∈LQB\ns(q).pid-list\n\f\f\f\f/Np,\nwhere Np is the total number of paragraphs in the CLIC-page collection.\nAs an illustration, there are in total Np = 599 paragraphs in the 100 CLIC-\npage sample. Table 3 compares the coverages of the LQBs created by the three\npartitioning strategies and also the one created by human workers.\nTable 3 Coverages of LQBs created by different methods (100 CLIC-page collection).\nMGQs HCQs\nMethod: Section-based Paragraph-based Hybrid Manual\nNumber of paragraphs covered: 312 422 557 587\nCoverage: 52.1% 70.5% 93.0% 98.0%\nFrom Table 3, we see that section-based partitioning has a relatively poor\ncoverage (52.1%). The LQB section-based partitioning created covers only\nabout half of the CLIC-pages’ contents. In contrast, Hybrid partitioning gives\na very high coverage of 93%. The reason why Hybrid gives a much higher cov-\nerage compared with section-based partitioning is that Hybrid uses the whole\nsection as context in prompting (similar to section-based partitioning) but it\nrepeats executing GPT-3, each time with a paragraph attention. These two fac-\ntors combined allows GPT-3 to capture important focus from each paragraph\nso that it can generate precise questions for each of them. Figure 8 clearly illus-\ntrates this observation. The left column in the figure shows that section-based\npartitioning generates questions only for the first paragraph of the CLIC-page\n(and the focuses it captures are not precise, leading to incorrect questions).\nHybrid (right column), in contrast, is able to lead GPT-3 to capture meaning-\nful focuses (and hence correct questions) for each paragraph. This contributes\nto Hybrid’s high-coverage performance. The better coverage by Hybrid over\nparagraph-based partitioning is further illustrated by the example shown in\nAppendix A. From the answer scopes of the questions listed in the appendix,\nwe see that Hybrid gives questions that cover all eight paragraphs of CLIC-\npage p3, while paragraph-based partitioning fails to cover three of the eight\nparagraphs.\nArticle Title 21\nFrom Table 3, we also see that Hybrid outperforms paragraph-based\npartitioning (whose coverage is 70.5%). The reason is that although\nparagraph-based partitioning executes GPT-3 for each paragraph, each prompt\nparagraph-based partitioning uses consists of just one paragraph. It gives\ninsufficient context to GPT-3 to generate correct questions, leading to\nparagraph-based partitioning’s poor precision (see Table 2). As a result, even\nparagraph-based partitioning generates questions for each paragraph, many\nof those questions are not correct and thus their scopes do not cover the\nparagraphs.\nComparing Hybrid against human workers, we see that Hybrid’s perfor-\nmance approaches that of HCQs in terms of coverage. We remark that the\nhuman workers were instructed to compose questions that cover all the con-\ntents of the CLIC-pages. It is highly impressive that the LQB generated by\nmachine (Hybrid) can achieve human-like performance.\nNext, we compare Hybrid against human workers in terms of question\ndiversity. We argue that a desirable characteristic of a good LQB is that it\ncontains diversified questions. By diversity, we mean (1) there are questions\nthat ask for the same piece of legal knowledge from different perspectives,\nand (2) there are questions of different levels of specificity/generality. Let us\nillustrate question diversity by comparing the questions generated by Hybrid\nand human workers, respectively, for CLIC-page-8.\nFrom Figure 6, we see that the numbers of questions generated for CLIC-\npage-8 by Hybrid and human workers are 65 and 20, respectively. There are\n7 paragraphs in CLIC-page-8. We use the notation p8:x to refer the x-th\nparagraph of page-8. Figure 9 shows the number of questions that cover each\nparagraph of CLIC-page-8 by the two methods. For example, there are 21\nquestions given by Hybrid that cover paragraph 3 while there are 7 such ques-\ntions given by human workers. If one adds up the quantities of the bars (in\nFigure 9) given by each of the methods, we get 104 (Hybrid) and 20 (Manual).\nNote that the tally 104 for Hybrid is much larger than the number of ques-\ntions Hybrid generated (65). This is because many questions given by Hybrid\nhave scopes that cover multiple paragraphs. These multi-paragraph questions\nare those that are of higher-level of generality, which require multiple para-\ngraphs to answer. For example, question Q49 given by Hybrid (see Appendix)\nis a multi-paragraph one, which covers both paragraphs 5 and 7 of the CLIC-\npage. In contrast, the bar-quantity tally for Manual (20) is exactly the same\nas the number of questions Manual generated. That means each question com-\nposed by the human worker covers only one single paragraph. These questions\nare very specific to the legal knowledge carried by each paragraph. From this\nobservation, we see that the machine (Hybrid) is able to create more diversi-\nfied questions (from general to specific), while human workers tend to compose\nmostly specific questions.\nWe further illustrate question diversity by showing the questions created\nfor Paragraph 6 (i.e., p8:6). Page-8 is about “Rates, Management Fees and\nother Charges”. For paragraph 6, the human worker gave only one question\n22 Article Title\nFig. 9 Number of questions created by Hybrid and human worker for each paragraph in\nCLIC-page-8.\nwhile Hybrid generated multiple (see Figure 9). In Figure 10, we show the\nonly question that was created manually and some of the questions generated\nby Hybrid. We see that the questions given by Hybrid are richer and more\ndiversified. In particular, the questions cover different user perspectives such\nas landlord’s, tenant’s, and even manager’s and co-owners’.\nPage p8: “RATES, MANAGEMENT FEES AND OTHER CHARGES”\nhttps://clic.org.hk/en/topics/landlordtenant/ratesManagementFees\n(manual)\nWho is liable for the management fees or other forms of contributions\n(e.g. renovation costs and contribution to litigation funds) to\nbe made pursuant to the Deed of Mutual Covenant (“DMC”)\nor the Building Management Ordinance (Cap. 344) (the “BMO”)?\n(hybrid)\nWhat happens if a tenant defaults on payment of management fees or\nother contributions to be made pursuant to the Deed of Mutual Covenant\n(“DMC”) or the Building Management Ordinance (Cap. 344) (the “BMO”)?\n(hybrid)\nWhat is the landlord’s liability for payment of management fees or\nother forms of contributions pursuant to the Deed of Mutual Covenant\n(DMC) or the Building Management Ordinance (BMO)?\n(hybrid)Is an obligation to pay money under the DMC/BMO directly enforceable\nby the manager/incorporated owners against a tenant?\n(hybrid)Can the manager or other co-owners enforce the payment of management\nfees directly against a tenant?\n(hybrid)\nCan a tenant still be held liable for any payment defaults even if the\nlease/tenancy agreement provides that the tenant shall make\npayment directly to the management office?\nFig. 10 Example questions created for p8:6.\nArticle Title 23\n4.2 Further Discussion\nIn the previous section we compared the three partitioning strategies quanti-\ntatively based on the LQBs created by them in terms of quantity, precision,\ncoverage, and diversity measures. We showed that Hybrid partitioning outper-\nforms the other partitioning schemes, and that applying GPT-3 with Hybrid\ngives machine-generated questions (MGQs) that compare favorably against\nhuman-composed questions (HCQs). In this section we further discuss some\nadvantages of MGQs over HCQs and give illustrations to our findings.\nAugmenting questions. In Table 2, we showed that Hybrid gives an\nLQB of 68% precision over the 100 CLIC-page sample. That means that given\nan MGQ q generated for a CLIC-page d, there is a 32% chance that the page\nd does not directly answer question q (and so q is not labeled “correct” by\nour human verifier). When constructing an LQB, these “unanswered” MGQs\nshould be removed. We took a closer look at the unanswered MGQs and we\nfound that many of these MGQs are indeed interesting questions that are\nrelevant to the legal issues discussed in the CLIC-pages. They are not labeled\n“correct” simply because the CLIC-pages are not comprehensive enough to\ncover the legal issues raised by the questions. We call this kind of machine-\ngenerated questions, which are interesting and relevant to their CLIC-pages\nbut are not answered directly by the pages because of insufficient contents in\nthe pages, augmenting questions. Figure 11 gives two example augmenting\nquestions. The first example concerns CLIC-page-5, which discusses legal issues\nwhen a landlord sells a property with an existing tenancy. An augmenting\nMGQ, qa, reads “ What if the tenant refuses to allow access for viewings? ”\nNote that question qa addresses the legal rights of the landlord in case the\ntenant is uncooperative and is obstructing the sale. CLIC-page-5, however,\nonly states what the landlord needs to do to sell his property (such as the\nparties the landlord needs to inform). Therefore, question qa is not labeled\n“correct” by our human verifier. Question qa, however, is very relevant to the\nlegal topic of the CLIC-page. The question provides hints on how the content\nof the CLIC-page could be enhanced (e.g., by explaining the legal rights of the\nlandlord). The second example in Figure 11 concerns CLIC-page-21, which is\nabout defamatory articles. An augmenting MGQ, qb, we obtained from Hybrid\nstates a highly-related question on defamatory remarks in private conversation.\nAlthough question qb is not answered directly by CLIC-page-21, the MGQ\nagain suggests how the content of the page can be enriched.\nWe remark that augmenting questions, even though they are labeled “incor-\nrect”, should be retained in the LQB and should be used to guide CLIC-page\nrevision. We manually inspect all MGQs created by Hybrid on the 100-CLIC-\npage sample to identify all the augmenting questions it generates. We find 69\naugmenting questions covering 37 (of the 100) CLIC-pages. This is substan-\ntial because the discovery suggests how nearly 40% of the CLIC-pages can be\nenriched. Human workers on the other hand create only “correct” questions;\nWe do not observe any augmenting questions in the HCQs.\n24 Article Title\nPagep5: “LANDLORD SELLS THE PROPERTY WITH EXISTING TENANCY”\nhttps://www.clic.org.hk/en/topics/landlordtenant/landlordSellsThePropertyWithExistingTenancy\n(Augmenting)qa: What if the tenant refuses to allow access for viewings?\nPagep21: “IF I DID NOT INTEND TO REFER TO THE PLAINTIFF IN MY ARTICLE,\nAND IT WAS A PURE COINCIDENCE THAT THE ARTICLE APPEARS TO REFER TO HIM,\nWILL I STILL BE LIABLE FOR DEFAMATION?”\nhttps://clic.org.hk/en/topics/defamation/identifyingthepersondefamed/q1\n(Augmenting)qb: If I make a statement in private conversation, can I still be liable for defamation?\nFig. 11 Example augmenting questions generated by Hybrid.\nQuestion diversity and generality.In the previous section we showed\nthat the MGQs obtained with Hybrid are more diversified than HCQs. In\nparticular, some MGQs have answer scopes that span multiple paragraphs,\nwhile HCQs are usually focusing on each individual paragraph. We argued\nthat questions whose answer scopes include multiple paragraphs (or “multi-\nparagraph questions” for short) are higher-level ones and are more general\nin the sense that they require more elaborate contents (more paragraphs) to\nanswer. In fact, users without legal training tend to ask more general questions\nbecause they often do not know the relevant legal issues well enough to ask very\nspecific (single-paragraph) questions. As an illustrative example, CLIC-page-\n125 addresses the issue of “termination of tenancies by non-payment of rent”.\nWe inspected the questions generated by Hybrid. Two example questions, one\ngeneral and another specific, are:\n• (General question) qc: “I am a landlord and I want to evict my tenant who\nhas not been paying rent. Can I do so? ”; Answer scope: [ p12: {1, 2, 3, 4 }].\n• (Specific question) qd: “Can a tenant be evicted if he has been granted relief\nagainst forfeiture?”; Answer scope: [ p12: {6}].\nNote that the general question qc has a scope that spans the first four para-\ngraphs of CLIC-page-12, while the specific question qd has its answer given in\na single paragraph (paragraph 6 of the CLIC-page).\nWe inspected all the MGQs given by Hybrid and all the HCQs to label\neach question as general (whose answer scopes include multiple paragraphs) or\nspecific (whose answer scopes include only single paragraphs). Table 4 shows\nthe number of general and specific questions given by Hybrid and human\nworkers, respectively. For Hybrid, we only show the numbers for “correct”\nquestions (i.e., those that are answered by CLIC-pages and thus have answer\nscopes).\nFrom the table, we see that we can obtain twice as many general questions\nfrom machine than from human workers (564 vs 272). Also, almost 90% of\nall HCQs are specific ones whose answers are given by single paragraphs. The\nreason why human workers tend to give predominately specific questions is\nthat human workers generally have short attention span; When they compose\n5https://clic.org.hk/en/topics/landlord tenant/terminationOfTenanciesByNonpaymentOfRent\nArticle Title 25\nTable 4 Number of general and specific questions given by Hybrid and human workers\n(100 CLIC-page sample).\nMethod: MGQs (Hybrid) HCQs\nTotal number of questions: 3,362 2,685\nNumber of general questions: 564 (16.8%) 272 (10.1%)\nNumber of specific questions: 2,798 (83.2%) 2,413 (89.9%)\nquestions for a CLIC-page, they would usually read a paragraph, understand\nthe paragraph’s content, and then think of questions that they can ask for the\nparagraph. After that, they would proceed to the next paragraph and focus\ntheir attention on that. The answers to most HCQs are therefore paragraph\nspecific. From this analysis, we see that the machine is more capable of creating\nmore general questions for the LQB.\n5 The CLIC Recommender (CRec)\nIn the last section, we discussed how an LQB can be created with machine\n(by executing GPT-3 prompted by Hybrid) and also manually with human\nworkers. Once an LQB is created, the final component we need to bridge the\nlegal knowledge gap (see Figure 1) is the CLIC Recommender (CRec). The\nobjective of CRec is to retrieve a short list of model questions from the LQB\nthat are most relevant to a user’s legal situation. Instead of toiling through the\nlarge number of questions in the LQB, a user only needs to examine a small\nnumber of model questions recommended by CRec. In this section, we briefly\ndiscuss CRec’s design and demonstrate it with an example.\nFigure 12 shows CRec’s interface. A user can describe his/her legal situa-\ntion in a text box. Then, CRec will match the user’s verbal description against\nthe model questions in the LQB. This matching is done in the following way.\nFirst, we embed the user input text, tu, into an embedding vector v(tu)6. Also,\nfor each question q ∈ LQB, we compute two embedding vectors:\n• An answer vector , va(q): We consider the CLIC-page paragraphs of q’s\nanswer scope, i.e., s(q).pid-list. We retrieve the paragraphs’ text and embed\nthem into a single embedding vector va(q). Essentially, va(q) encodes the\nsemantics of the answer to the question q.\n• A question string vector , vs(q): We embed the textual string of question q\ninto an embedding vector vs(q). This vector encodes the semantics of the\nquestion q.\nNote that all the questions’ embedding vectors ( va(q)’s and vs(q)’s) are pre-\ncomputed and are stored in CRec’s system.\n6We use the model all-mpnet-base-v2 to perform the text embedding.\n26 Article Title\nFig. 12 CRec Interface.\nNext, we compute the cosine similarity between v(tu) against all the ques-\ntions’ answer vectors va(q)’s. Questions in the LQB are ranked based on their\nsimilarities against v(tu), the most similar first. For questions in the result that\nhave the same answer scope (and thus the same answer vector), we remove all\nbut one of them. The reason is that these questions share the same answer and\nthus are highly similar. They are therefore considered redundant and only one\nof them should be displayed to the user in order for CRec to provide a diver-\nsified list of model questions. We use the cosine similarity between questions’\nstring vectors vs(q)’s and v(tu) as tie-breakers. Specifically, among redundant\nquestions in the answer, the one with the highest cosine similarity between its\nvs(q) and v(tu) is retained, others are discarded. Finally, the top-10 questions\nin the LQB with the highest ranking (after redundancy removal) are displayed\nto the user.\nFigure 13 shows an example query given to CRec and the recommendation\nresults. In this example, the user describes a situation where he/she intends to\ncreate a cover version of a song with mocking lyrics and upload the song to a\nsocial media platform. Note that in this example, the user does not explicitly\nask any legal question in the input. Upon receiving the user’s verbal descrip-\ntion, CRec shortlists 10 model questions from the LQB based on the procedure\nwe described above. In Figure 13, we show the screenshot with the top-5 ques-\ntions displayed. In the result page, for each shortlisted question, CRec displays\nan excerpt of the answer to the question with a button on the right (labeled\n“Visit CLIC page”). The user can then go through the shortlisted questions\nand decide which ones are most relevant to his/her enquiry. From Figure 13,\nwe see that many of the model questions CRec recommends (particularly ques-\ntions 1, 2, 4) are highly relevant to the user’s situation. The user can click\nthe associated buttons next to the questions to access the full CLIC-pages for\nmore legal information.\nWe have experimented with CRec using many case scenarios found in a\nlocal public discussion forum. Our study shows that CRec is very effective in\nlocating relevant model questions from the LQB that relate user’s scenarios to\nlegal information accessible on the CLIC platform. Our three-step approach in\nArticle Title 27\nbridging the legal knowledge gap is therefore a promising approach in bringing\nlegal knowledge to the public.\n28 Article Title\nFig. 13 Recommendation results (only top 5 questions are shown due to space constraint).\nArticle Title 29\n6 Conclusion\nIn this paper we address the legal knowledge gap, which is a barrier between\nformal legal sources and the general public. In particular, we seek to tackle\nthe issues of navigability and comprehensibility with a focus on the former,\nwhich is to identify the legal issue in a situation faced by a layperson and\nlocate the relevant legal concept, rule or principle in documents presenting legal\ninformation. We propose a three-step approach to bridging the gap. The three\nsteps include (1) presenting and explaining legal concepts in less technical and\nmore readable writing, (2) creating a massive legal question bank (LQB), and\n(3) creating a machine legal assistant that translates a user’s verbal description\nof a legal situation into a short list of model questions, and subsequently leads\nthe user to relevant legal knowledge. We achieve the three steps by designing\nand implementing the CLIC platform, an LQB, and the CLIC Recommender\n(CRec), respectively. We perform an extensive study on the technical aspects\nof creating the LQB using the large-scale pre-trained language model, GPT-\n3. We study how GPT-3 should be prompted to generate machine-generated\nquestions (MGQs). Our finding shows that the Hybrid method performs the\nbest in terms of a number of desirable properties of the LQB. These include\nhigh quantity, high precision, high coverage, and high diversity. The MGQs also\ncompare favorably against human-composed questions (HCQs). Among the\nadvantages, MGQs are generally more diversified than HCQs. There are also\nmore augmenting questions created by the machine than by human workers.\nWe also discuss the design of CRec and show with an example how the LQB\nis practically used to help a user locate relevant legal knowledge. With the\nthree-step approach, we have demonstrated how to effectively overcome the\nchallenge of navigability and comprehensibility, and thereby narrow the legal\nknowledge gap.\nStatements and Declarations\nThis project is supported by Innovation and Technology Fund (ITS/234/20)\nand the WYNG Foundation (HKU KG210018).\n30 Article Title\nReferences\nBecher, S.I. and U. Benoliel. 2021. Law in books and law in action: The\nreadability of privacy policies and the GDPR, K. Mathis & A. Tor (Eds.),\nConsumer Law and Economics, 179–204. Springer International Publishing.\nhttps://doi.org/10.1007/978-3-030-49028-7 9.\nBrown, T., B. Mann, N. Ryder, M. Subbiah, J.D. Kaplan, P. Dhariwal, A. Nee-\nlakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss,\nG. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Win-\nter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei 2020.\nLanguage models are few-shot learners. In H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin (Eds.), Advances in Neural Information\nProcessing Systems, Volume 33, pp. 1877–1901. Curran Associates, Inc.\nCurtotti, M., W. Weibel, E. McCreath, N. Ceynowa, S. Frug, and T.R. Bruce.\n2015. Citizen science for citizen access to law. Journal of Open Access to\nLaw 3 (1): 57–120.\nDai, Z., A.T. Chaganty, V.Y. Zhao, A. Amini, Q.M. Rashid, M. Green, and\nK. Guu 2022. Dialog inpainting: Turning documents into dialogs. In\nInternational Conference on Machine Learning, ICML 2022, pp. 4558–4586.\nDas, R., A. Ray, S. Mondal, and D. Das 2016. A rule based question generation\nframework to deal with simple and complex sentences. In2016 International\nConference on Advances in Computing, Communications and Informatics\n(ICACCI), pp. 542–548. https://doi.org/10.1109/ICACCI.2016.7732102.\nDu, X., J. Shao, and C. Cardie 2017. Learning to ask: Neural question genera-\ntion for reading comprehension. In Proceedings of the 55th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers) ,\npp. 1342–1352. https://doi.org/10.18653/v1/P17-1123.\nDyson, D.D. and K. Schellenberg. 2017. Access to justice: The readabil-\nity of legal services corporation legal aid internet services. Journal of\npoverty 21 (2): 142–165. https://doi.org/10.1080/10875549.2016.1186773.\nHeilman, M. and N.A. Smith 2010, June. Good question! Statistical ranking\nfor question generation. In Human Language Technologies: The 2010 Annual\nConference of the North American Chapter of the Association for Compu-\ntational Linguistics , Los Angeles, California, pp. 609–617. Association for\nComputational Linguistics. https://doi.org/10.5555/1857999.1858085.\nKaplan, J., S. McCandlish, T. Henighan, T.B. Brown, B. Chess, R. Child,\nS. Gray, A. Radford, J. Wu, and D. Amodei. 2020. Scaling laws for neural\nlanguage models. arXiv preprint arXiv:2001.08361 . https://doi.org/arXiv:\nArticle Title 31\n2001.08361.\nKim, Y., H. Lee, J. Shin, and K. Jung 2019. Improving neural question gen-\neration using answer separation. In Proceedings of the AAAI conference on\nartificial intelligence , Volume 33, pp. 6602–6609. https://doi.org/10.1609/\naaai.v33i01.33016602.\nLindberg, D., F. Popowich, J. Nesbit, and P. Winne 2013. Generating natural\nlanguage questions to support learning on-line. In Proceedings of the 14th\nEuropean Workshop on Natural Language Generation , pp. 105–114.\nLiu, B., H. Wei, D. Niu, H. Chen, and Y. He 2020. Asking questions the\nhuman way: Scalable question-answer generation from text corpus. In Pro-\nceedings of The Web Conference 2020 , pp. 2032–2043. https://doi.org/10.\n1145/3366423.3380270.\nMin, B., H. Ross, E. Sulem, A.P.B. Veyseh, T.H. Nguyen, O. Sainz, E. Agirre,\nI. Heinz, and D. Roth. 2021. Recent advances in natural language pro-\ncessing via large pre-trained language models: A survey. arXiv preprint\narXiv:2111.01243. https://doi.org/arXiv:2111.01243.\nMommers, L. 2011. Access to law in Europe,Innovating Government, 383–398.\nSpringer. https://doi.org/10.1007/978-90-6704-731-9 21.\nMommers, L., W. Voermans, W. Koelewijn, and H. Kielman. 2009. Under-\nstanding the law: improving legal knowledge dissemination by translating\nthe contents of formal sources of law. Artificial Intelligence and Law 17 (1):\n51–78. https://doi.org/10.1007/s10506-008-9073-5.\nNew Zealand Law Reform Commission. 2008. New Zealand Parlia-\nmentary Counsel’s Office (2008) Presentation of New Zealand statute\nlaw (NZLC R104). https://www.lawcom.govt.nz/sites/default/files/\nprojectAvailableFormats/NZLC%20R104.pdf.\nRadford, A., J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. 2019.\nLanguage models are unsupervised multitask learners. OpenAI Blog 1 (8): 9.\nRaffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,\nW. Li, P.J. Liu, et al. 2020. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Journal of Machine Learning\nResearch 21(140): 1–67.\nRuohonen, J. 2021. Assessing the readability of policy documents on the\ndigital single market of the European Union. In 2021 Eighth International\nConference on eDemocracy & eGovernment (ICEDEG), pp. 205–209. IEEE.\nhttps://doi.org/10.1109/ICEDEG52154.2021.9530996.\n32 Article Title\nSchick, T. and H. Sch¨ utze 2021. Generating datasets with pretrained language\nmodels. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pp. 6943–6951. https://doi.org/10.18653/v1/\n2021.emnlp-main.555.\nSong, L., Z. Wang, W. Hamza, Y. Zhang, and D. Gildea 2018, June. Lever-\naging context information for natural question generation. In Proceedings\nof the 2018 Conference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Technologies, Volume\n2 (Short Papers) , New Orleans, Louisiana, pp. 569–574. Association for\nComputational Linguistics. https://doi.org/10.18653/v1/N18-2090.\nSteuer, T., A. Filighera, T. Tregel, and A. Miede. 2022. Educational automatic\nquestion generation improves reading comprehension in non-native speakers:\nA learner-centric case study. Frontiers in Artificial Intelligence 5. https:\n//doi.org/10.3389/frai.2022.900304.\nWang, S., Z. Wei, Z. Fan, Y. Liu, and X. Huang 2019. A multi-agent com-\nmunication framework for question-worthy phrase extraction and question\ngeneration. In Proceedings of the AAAI Conference on Artificial Intelligence,\nVolume 33, pp. 7168–7175. https://doi.org/10.1609/aaai.v33i01.33017168.\nWang, Z., J. Valdez, D. Basu Mallick, and R.G. Baraniuk 2022. Towards\nhuman-like educational question generation with large language models. In\nInternational Conference on Artificial Intelligence in Education , pp. 153–\n166. Springer. https://doi.org/10.1007/978-3-031-11644-5 13.\nArticle Title 33\nAppendix A Questions created by human\nworkers vs those composed by\nMachine\nWe show the questions generated by human workers and those by\nvarious machine methods for CLIC-page p3: “Repairing Mainte-\nnance Obligations” (https://www.clic.org.hk/en/topics/landlord tenant/\nrepairing MaintenanceObligations/q1). The answer scope of a question is\nshown in parentheses at the end of the question. For questions generated by\nMachine that are incorrect, their answer scopes are marked “N.A.” (for “No\nAnswer”).\n[Questions composed by human workers]:\n- [Q01]: “In general, how do parties specify maintenance responsibility in tenancy agreements?” (p3:[heading])\n- [Q02]: “Who should be responsible for external and structural maintenance?” (p3:[2])\n- [Q03]: “Who should be responsible for internal and non-structural maintenance?” (p3:[2])\n- [Q04]: “What kind of maintenance should be responsible by the landlord?” (p3:[2])\n- [Q05]: “What kind of maintenance should be responsible by the tenant?” (p3:[2])\n- [Q06]: “Are the terms like external, internal, structural, or non-structural clear enough to specify maintenance\nresponsibility?” (p3:[2, 3])\n- [Q07]: “Is it fair for a tenant to become responsible for property maintenance?” (p3:[4])\n- [Q08]: “Do tenants have the responsibility to maintain the property from ”fair wear and tear”?” (p3:[5])\n- [Q09]: “Without the notice of structural defects, is the landlord responsible for structural repair?” (p3:[5])\n- [Q10]: “Is it reasonable for the landlord to be notified before being responsible for structural repair?” (p3:[5])\n- [Q11]: “Why some landlord may volunteer to carry out repairs and maintenance works?” (p3:[7])\n- [Q12]: “What are the advantages for landlords to repair and maintain the property voluntarily?” (p3:[7])\n- [Q13]: “If tenants become incooperative to landlord carrying out necessary inspection and repair works, what\ncan the landlord do?” (p3:[8])\n[Questions generated by Hybrid]:\n- [Q01]: “What should I do if I have a problem with my property that needs to be repaired or maintained?”\n(N.A.)\n- [Q02]: “Who is responsible for external and structural repairs and maintenance?” (p3:[2])\n- [Q03]: “Who is responsible for internal and non-structural repairs and maintenance?” (p3:[2])\n- [Q04]: “What is the meaning of ”fair wear and tear excepted”?” (p3:[5])\n- [Q05]: “What is the landlord’s obligation for structural repairs and maintenance?” (p3:[2])\n- [Q06]: “How can I find out who is responsible for repairs and maintenance?” (p3:[1])\n- [Q07]: “Can the landlord volunteer to carry out repairs and maintenance works?” (p3:[7])\n- [Q08]: “What if the tenant becomes uncooperative?” (p3:[8])\n- [Q09]: “Can the landlord apply for an injunction?” (p3:[8])\n- [Q10]: “Can the landlord terminate the tenancy agreement?” (p3:[8])\n- [Q11]: “What is the difference between external and internal repairs?” (p3:[2])\n- [Q12]: “What is the difference between structural and non-structural repairs?” (p3:[2])\n- [Q13]: “Who is responsible for repair and maintenance of the property?” (p3:[heading])\n- [Q14]: “What is the difference between ”fair wear and tear” and ”damage”?” (p3:[5])\n- [Q15]: “What is the landlord’s obligation for structural repairs and maintenance?” (p3:[2])\n- [Q16]: “What is the tenant’s obligation for repair and maintenance of the property?” (p3:[heading])\n- [Q17]: “Can the landlord carry out repair and maintenance works without the tenant’s consent?” (p3:[8])\n- [Q18]: “What if the tenant refuses to allow the landlord to carry out repair and maintenance works?” (p3:[8])\n- [Q19]: “Can the landlord terminate the tenancy agreement if the tenant refuses to allow him/her to carry out\nrepair and maintenance works?” (p3:[8])\n- [Q20]: “What can the tenant do if he/she is not satisfied with the repair and maintenance works carried out\nby the landlord?” (N.A.)\n- [Q21]: “What is a typical approach under tenancy agreements when it comes to repair and maintenance\nresponsibilities?” (p3:[2])\n- [Q22]: “How can a simple dichotomy between landlord and tenant responsibilities for repair and maintenance\nbe problematic?” (p3:[2])\n- [Q23]: “What should a well-drafted tenancy document do in order to clarify the parties’ duties with regards\nto repair and maintenance?” (p3:[3])\n- [Q24]: “Is it common for tenants to be responsible for onerous duties such as repair and maintenance under\ntheir tenancy agreement?” (p3:[4])\n- [Q25]: “What is the significance of the phrase ”fair wear and tear excepted” with regards to a tenant’s\nobligations for repair and maintenance?” (p3:[5])\n- [Q26]: “What is the role of notice clauses in specifying a landlord’s obligations for structural repairs and\nmaintenance?” (p3:[5])\n- [Q27]: “Why might landlords agree to incur expenses to remedy defects in the property even though they are\nnot contractually obliged to do so?” (p3:[7])\n- [Q28]: “In what circumstances might a landlord exercise a right to enter the property and carry out necessary\ninspection and repair works?” (p3:[8])\n- [Q29]: “What could happen if a tenant becomes uncooperative when a landlord tries to exercise such a right?”\n(p3:[8])\n- [Q30]: “Is it possible for a landlord to terminate a tenancy agreement if a tenant does not cooperate with\nrepair and maintenance works?” (p3:[8])\n34 Article Title\n- [Q31]: “What is the fair wear and tear exception?” (p3:[5])\n- [Q32]: “What does it mean when a tenancy agreement states that the tenant’s obligations for repair and\nmaintenance are limited by the phrase ”fair wear and tear excepted”?” (p3:[5])\n- [Q33]: “What is the landlord’s obligation for structural repairs and maintenance?” (p3:[2])\n- [Q34]: “What is the tenant’s obligation for repair and maintenance?” (p3:[heading])\n- [Q35]: “How can I find out who is responsible for repairs and maintenance?” (p3:[1])\n- [Q36]: “How can I find out if the landlord is responsible for structural repairs and maintenance?” (p3:[1, 2, 6])\n- [Q37]: “How can I find out if the tenant is responsible for repair and maintenance?” (p3:[1, 2, 6])\n- [Q38]: “Is it common for a tenant to be responsible for repair and maintenance?” (p3:[4])\n- [Q39]: “What is the landlord’s right to enter the property and carry out necessary inspection and repair\nworks?” (p3:[8])\n- [Q40]: “Can the landlord terminate the tenancy agreement if the tenant is uncooperative?” (p3:[8])\n- [Q41]: “What is the fair wear and tear exception?” (p3:[5])\n- [Q42]: “What does it mean when a tenant’s obligations for repair and maintenance are limited by the phrase\n”fair wear and tear excepted”?” (p3:[5])\n- [Q43]: “What is included in the fair wear and tear exception?” (p3:[5])\n- [Q44]: “How is the fair wear and tear exception determined?” (N.A.)\n- [Q45]: “What are some examples of fair wear and tear?” (N.A.)\n- [Q46]: “Is there a limit to the amount of damage that can be caused by fair wear and tear?” (N.A.)\n- [Q47]: “What if the tenant causes damage that is not considered to be fair wear and tear?” (N.A.)\n- [Q48]: “Can the landlord repair or replace items that are damaged by fair wear and tear?” (p3:[5, 7])\n- [Q49]: “Who is responsible for repairing or replacing items that are damaged by fair wear and tear?” (p3:[5, 7])\n- [Q50]: “What if the landlord and tenant cannot agree on what is considered to be fair wear and tear?” (N.A.)\n- [Q51]: “What is the ”fair wear and tear” exception in a tenancy agreement?” (p3:[5])\n- [Q52]: “What is the landlord’s obligation for structural repairs and maintenance?” (p3:[2])\n- [Q53]: “How can I tell if the damage is due to ”fair wear and tear” or not?” (p3:[5])\n- [Q54]: “I am a tenant. Can I carry out repair and maintenance works on my own?” (p3:[4])\n- [Q55]: “I am a landlord. Can I enter the property to carry out repair and maintenance works without the\ntenant’s permission?” (p3:[8])\n- [Q56]: “My tenancy agreement does not contain any clauses regarding repair and maintenance. Who is\nresponsible for repairs and maintenance?” (p3:[heading])\n- [Q57]: “My tenancy agreement contains a clause stating that the tenant is responsible for all repair and\nmaintenance works. Is this fair?” (p3:[1, 4])\n- [Q58]: “My tenancy agreement contains a clause stating that the landlord is responsible for all repair and\nmaintenance works. Is this fair?” (p3:[1, 7])\n- [Q59]: “I am a tenant. The property I am renting is in a state of disrepair. What can I do?” (p3:[heading])\n- [Q60]: “I am a landlord. The property I am renting out is in a state of disrepair. What can I do?” (p3:[heading])\n- [Q61]: “Can the landlord enter the property without the tenant’s permission?” (p3:[8])\n- [Q62]: “What if the tenant refuses to allow the landlord to enter the property?” (p3:[8])\n- [Q63]: “What if the tenant is not cooperative?” (p3:[8])\n- [Q64]: “What if the landlord needs to carry out urgent repairs?” (p3:[8])\n- [Q65]: “What if the tenancy agreement does not impose any duty on the landlord?” (p3:[6, 7, 8])\n- [Q66]: “Can the landlord terminate the tenancy agreement?” (p3:[8])\n- [Q67]: “What if the tenant objects to the repairs?” (p3:[8])\n- [Q68]: “Can the landlord charge the tenant for the cost of repairs?” (N.A.)\n- [Q69]: “What if the repairs are not carried out properly?” (N.A.)\n- [Q70]: “Can the tenant sue the landlord for breach of contract?” (N.A.)\n- [Q71]: “What is the definition of ”serious water leakage”?” (N.A.)\n- [Q72]: “What is the landlord’s right to enter the property?” (p3:[8])\n- [Q73]: “What is the landlord’s duty to carry out necessary inspection and repair works?” (p3:[8])\n- [Q74]: “What is the tenant’s obligation to cooperate with the landlord?” (p3:[8])\n- [Q75]: “What are the consequences if the tenant does not cooperate with the landlord?” (p3:[8])\n- [Q76]: “Can the landlord terminate the tenancy agreement if the tenant does not cooperate?” (p3:[8])\n- [Q77]: “What is an interlocutory injunction?” (N.A.)\n- [Q78]: “How does an interlocutory injunction work?” (N.A.)\n- [Q79]: “What are the consequences of an interlocutory injunction?” (p3:[8])\n- [Q80]: “Is there any other way for the landlord to obtain possession of the property besides an interlocutory\ninjunction?” (p3:[8])\n[Questions generated by Paragraph-based]:\n- [Q01]: “What is the Residential Tenancies Act?” (N.A.)\n- [Q02]: “What is a tenancy agreement?” (N.A.)\n- [Q03]: “What are the landlord’s responsibilities?” (p3:[heading])\n- [Q04]: “What are the tenant’s responsibilities?” (p3:[heading])\n- [Q05]: “Can the landlord enter the rented premises?” (p3:[8])\n- [Q06]: “Can the tenant be evicted?” (N.A.)\n- [Q07]: “What is rent in advance?” (N.A.)\n- [Q08]: “Can the landlord increase rent?” (N.A.)\n- [Q09]: “Can the tenant sublet or assign the tenancy?” (N.A.)\n- [Q10]: “What happens if there is a dispute between the landlord and tenant?” (N.A.)\n- [Q11]: “What is the difference between an internal and external repair?” (p3:[2])\n- [Q12]: “What is the difference between a structural and non-structural repair?” (p3:[2])\n- [Q13]: “Who is responsible for the maintenance of common areas?” (N.A.)\n- [Q14]: “Who is responsible for the upkeep of gardens and landscaping?” (N.A.)\n- [Q15]: “Who pays for water and electricity consumption?” (N.A.)\n- [Q16]: “What about Council rates and taxes?” (N.A.)\n- [Q17]: “What insurance cover is required?” (N.A.)\n- [Q18]: “What happens if the property is damaged by a natural disaster?” (N.A.)\n- [Q19]: “What are the responsibilities of the landlord in relation to health and safety?” (N.A.)\n- [Q20]: “Can the landlord make changes to the property without the tenant’s consent?” (p3:[8])\n- [Q21]: “What is a tenancy agreement?” (N.A.)\n- [Q22]: “What should be included in a tenancy agreement?” (N.A.)\n- [Q23]: “How long does a tenancy agreement last?” (N.A.)\n- [Q24]: “What is a security deposit?” (N.A.)\n- [Q25]: “When is the security deposit returned?” (N.A.)\n- [Q26]: “Who is responsible for repairing damage to the property?” (p3:[heading])\n- [Q27]: “Who is responsible for paying utilities?” (N.A.)\nArticle Title 35\n- [Q28]: “Can a landlord enter the property without notice?” (p3:[8])\n- [Q29]: “Can a tenant sublet the property?” (N.A.)\n- [Q30]: “What is a lease?” (N.A.)\n- [Q31]: “How long does a lease last?” (N.A.)\n- [Q32]: “What is included in a lease?” (N.A.)\n- [Q33]: “Can a landlord evict a tenant?” (N.A.)\n- [Q34]: “Can a tenant break a lease?” (N.A.)\n- [Q35]: “What is rent?” (N.A.)\n- [Q36]: “How much rent can a landlord charge?” (N.A.)\n- [Q37]: “Can a landlord raise rent?” (N.A.)\n- [Q38]: “What is security deposit?” (N.A.)\n- [Q39]: “Can a landlord keep my security deposit?” (N.A.)\n- [Q40]: “What is a tenancy agreement?” (N.A.)\n- [Q41]: “What is the difference between a tenancy agreement and a lease?” (N.A.)\n- [Q42]: “How long does a tenancy agreement last?” (N.A.)\n- [Q43]: “What is the difference between a periodic tenancy and a fixed-term tenancy?” (N.A.)\n- [Q44]: “What is the difference between a verbal tenancy agreement and a written tenancy agreement?” (N.A.)\n- [Q45]: “Can a landlord change the terms of a tenancy agreement?” (N.A.)\n- [Q46]: “Can a tenant change the terms of a tenancy agreement?” (N.A.)\n- [Q47]: “What are the landlord’s obligations under a tenancy agreement?” (p3:[2, 6])\n- [Q48]: “What are the tenant’s obligations under a tenancy agreement?” (p3:[2, 6])\n- [Q49]: “Who is responsible for maintaining and repairing the property?” (p3:[heading])\n- [Q50]: “How long does a tenancy agreement last?” (N.A.)\n- [Q51]: “How much rent can a landlord charge?” (N.A.)\n- [Q52]: “How often can a landlord increase the rent?” (N.A.)\n- [Q53]: “What amenities and services must a landlord provide?” (N.A.)\n- [Q54]: “Can a tenant make changes to the property?” (N.A.)\n- [Q55]: “Can a landlord enter the property?” (p3:[8])\n- [Q56]: “What happens if there is damage to the property?” (p3:[heading])\n- [Q57]: “What happens if there are problems with neighbours?” (N.A.)\n- [Q58]: “What happens if I want to move out?” (N.A.)\n- [Q59]: “What happens if I want my landlord to move out?” (N.A.)\n- [Q60]: “What is the landlord’s responsibility for maintaining and repairing the property?” (p3:[heading])\n- [Q61]: “Who is responsible for maintaining and repairing the property if the tenant and landlord disagree?”\n(p3:[heading])\n- [Q62]: “What if the landlord volunteers to carry out repairs and maintenance works?” (p3:[7])\n- [Q63]: “How can I ensure that my tenancy agreement covers all potential repair and maintenance costs?” (N.A.)\n- [Q64]: “What if there is damage to the property caused by a tenant?” (p3:[5])\n- [Q65]: “Can a tenant be held responsible for preexisting damage to the property?” (p3:[5])\n- [Q66]: “How can I get my deposit back if there is damage to the property?” (N.A.)\n- [Q67]: “What if I want to make changes to the property?” (N.A.)\n- [Q68]: “Can I terminate my tenancy agreement early if the property is not maintained or repaired?” (N.A.)\n- [Q69]: “Who do I contact if I have a problem with my landlord or property?” (N.A.)\n- [Q70]: “What is the Landlord and Tenant (Consolidation) Ordinance (Cap. 7)?” (N.A.)\n- [Q71]: “What are the rights and duties of landlords and tenants?” (p3:[heading])\n- [Q72]: “How can I renew my tenancy agreement?” (N.A.)\n- [Q73]: “What are the terms of a standard tenancy agreement?” (p3:[2])\n- [Q74]: “Can I sublet my property?” (N.A.)\n- [Q75]: “Can the landlord enter my property without my permission?” (p3:[8])\n- [Q76]: “Can I change the locks on my property?” (N.A.)\n- [Q77]: “Can I be evicted from my property?” (N.A.)\n- [Q78]: “Can I withhold rent if the property is not in good condition?” (N.A.)\n- [Q79]: “In general, who shall be responsible for maintaining and repairing the property?” (p3:[heading])\n[Questions generated by Section-based]:\n- [Q01]: “What is the landlord’s duty to repair?” (p3:[heading])\n- [Q02]: “What is the tenant’s duty to repair?” (p3:[heading])\n- [Q03]: “How can I make sure that I will not be held liable for any damage caused by me, my family or visitors\nto the property?” (p3:[5])\n- [Q04]: “What should I do if I find that there are defects in the property which need to be repaired?”\n(p3:[heading])\n- [Q05]: “My rental flat is infested with cockroaches/bedbugs/mice. What can I do?” (p3:[heading])\n- [Q06]: “There is water leakage in my rental flat. What can I do?” (p3:[heading])\n- [Q07]: “The lift in my building has broken down and I am unable to use it. What can I do?” (N.A.)\n- [Q08]: “There is no hot water supply in my rental flat. What can I do?” (p3:[heading])\n- [Q09]: “The air-conditioner in my rental flat has broken down and I am unable to use it. What can I do?”\n(p3:[heading])\n- [Q10]: “Can I withhold rent if my landlord fails to carry out repair works?” (N.A.)",
  "topic": "Legal research",
  "concepts": [
    {
      "name": "Legal research",
      "score": 0.6643610596656799
    },
    {
      "name": "Computer science",
      "score": 0.5613415837287903
    },
    {
      "name": "Legal profession",
      "score": 0.5308963656425476
    },
    {
      "name": "Legal aspects of computing",
      "score": 0.4996829032897949
    },
    {
      "name": "Legal psychology",
      "score": 0.4751191735267639
    },
    {
      "name": "Philosophy of law",
      "score": 0.47049450874328613
    },
    {
      "name": "Law",
      "score": 0.2880208492279053
    },
    {
      "name": "Political science",
      "score": 0.281749427318573
    },
    {
      "name": "The Internet",
      "score": 0.24715280532836914
    },
    {
      "name": "World Wide Web",
      "score": 0.24419471621513367
    },
    {
      "name": "Public law",
      "score": 0.1531190574169159
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I118347636",
      "name": "Australian National University",
      "country": "AU"
    }
  ],
  "cited_by": 4
}