{
  "title": "Keyword Transformer: A Self-Attention Model for Keyword Spotting",
  "url": "https://openalex.org/W3141838378",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5025118280",
      "name": "Axel Berg",
      "affiliations": [
        "Lund University"
      ]
    },
    {
      "id": "https://openalex.org/A5110106480",
      "name": "Mark O’Connor",
      "affiliations": [
        "Lund University"
      ]
    },
    {
      "id": "https://openalex.org/A5067294823",
      "name": "Miguel Tairum Cruz",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W2888641632",
    "https://openalex.org/W3161873870",
    "https://openalex.org/W2594230395",
    "https://openalex.org/W2148154194",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3128337785",
    "https://openalex.org/W3119913666",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2769912137",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3025581723",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W3037953478",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W3097018422",
    "https://openalex.org/W3163237592",
    "https://openalex.org/W3095321517",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3015399080",
    "https://openalex.org/W2797583228",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W3041561163",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W2973226577"
  ],
  "abstract": "The Transformer architecture has been successful across many domains, including natural language processing, computer vision and speech recognition. In keyword spotting, self-attention has primarily been used on top of convolutional or recurrent encoders. We investigate a range of ways to adapt the Transformer architecture to keyword spotting and introduce the Keyword Transformer (KWT), a fully self-attentional architecture that exceeds state-of-the-art performance across multiple tasks without any pre-training or additional data. Surprisingly, this simple architecture outperforms more complex models that mix convolutional, recurrent and attentive layers. KWT can be used as a drop-in replacement for these models, setting two new benchmark records on the Google Speech Commands dataset with 98.6% and 97.7% accuracy on the 12 and 35-command tasks respectively.",
  "full_text": "Keyword Transformer: A Self-Attention Model for Keyword Spotting\nAxel Berg1,2,∗, Mark O’Connor1,∗ , Miguel Tairum Cruz1\n1Arm ML Research Lab, 2Lund University\n{axel.berg, mark.oconnor, miguel.tairum-cruz}@arm.com\nAbstract\nThe Transformer architecture has been successful across many\ndomains, including natural language processing, computer vi-\nsion and speech recognition. In keyword spotting, self-attention\nhas primarily been used on top of convolutional or recurrent\nencoders. We investigate a range of ways to adapt the Trans-\nformer architecture to keyword spotting and introduce the Key-\nword Transformer (KWT), a fully self-attentional architecture\nthat exceeds state-of-the-art performance across multiple tasks\nwithout any pre-training or additional data. Surprisingly, this\nsimple architecture outperforms more complex models that mix\nconvolutional, recurrent and attentive layers. KWT can be used\nas a drop-in replacement for these models, setting two new\nbenchmark records on the Google Speech Commands dataset\nwith 98.6% and 97.7% accuracy on the 12 and 35-command\ntasks respectively.1\nIndex Terms: speech recognition, keyword spotting, Trans-\nformers\n1. Introduction\nRecent works in machine learning show that the Transformer ar-\nchitecture, ﬁrst introduced by Vaswani et al. [1], is competitive\nnot only in language processing, but also in e.g. image classi-\nﬁcation, [2, 3, 4], image colorization [5], object detection [6],\nautomatic speech recognition [7, 8, 9], video classiﬁcation [10]\nand multi-agent spatiotemporal modeling [11]. This can be seen\nin the light of a broader trend, where a single neural network ar-\nchitecture generalizes across many domains of data and tasks.\nAttention mechanisms have also been explored for keyword\nspotting [12, 13], but only as an extension to other architectures,\nsuch as convolutional or recurrent neural networks.\nInspired by the strength of the simple Vision Transformer\n(ViT) model [2] in computer vision and by the techniques that\nimproves its data-efﬁciency [3], we propose an adaptation of\nthis architecture for keyword spotting and ﬁnd that it matches\nor outperforms existing models on the much smaller Google\nSpeech Commands dataset [14] without additional data.\nWe summarize our main contributions as follows:\n1. An investigation into the application of the Transformer\narchitecture to keyword spotting, ﬁnding that applying\nself-attention is more effective in the time domain than\nin the frequency domain.\n2. We introduce the Keyword Transformer, as illustrated in\nFigure 1, a fully self-attentional architecture inspired by\nViT [2] that can be used as a drop-in replacement for\nexisting keyword spotting models and visualize the ef-\nfect of the learned attention masks and positional em-\nbeddings.\n*Equal contribution.\n1Code is available at https://github.com/ARM-software/keyword-\ntransformer.\nWaveform \nTime-domain \npatches \nMel-scale \nspectrogram \n…\nLinear projection of ﬂattened patches \nTransformer encoder layers \nLinear head \n1 2 3 4 n0*\nClass token \nembedding \nPatch + position \nembeddings \nOutput class  “yes”, “go”, … \nDetails in \nﬁgure 2 \nFigure 1: The Keyword Transformer architecture. Audio is pre-\nprocessed into a mel-scale spectrogram, which is partitioned\ninto non-overlapping patches in the time domain. Together with\na learned class token, these form the input tokens for a multi-\nlayer Transformer encoder. As with ViT [2], a learned position\nembedding is added to each token. The output of the class token\nis passed through a linear head and used to make the ﬁnal class\nprediction.\n3. An evaluation of this model across several tasks using\nthe Google Speech Commands dataset with comparisons\nto state-of-the-art convolutional, recurrent and attention-\nbased models.\n4. An analysis of model latency on a mobile phone, show-\ning that the Keyword Transformer is competitive in edge\nuse cases.\n2. Related Work\n2.1. Keyword Spotting\nKeyword spotting is used to detect speciﬁc words from a stream\nof audio, typically in a low-power always-on setting such as\nsmart speakers and mobile phones. To achieve this, audio is\nprocessed locally on the device. In addition to detecting target\nwords, classiﬁers may also distinguish between “silence” and\n“unknown” for words or sounds that are not in the target list.\nIn recent years, machine learning techniques, such as deep\n(DNN), convolutional (CNN), recurrent (RNN) and Hybrid-\nTree [15] neural networks, have proven to be useful for key-\nword spotting. These networks are typically used with a pre-\nprocessing pipeline that extracts the mel-frequency cepstrum\ncoefﬁcients (MFCC) [16]. Zhang et al. [17] investigated sev-\narXiv:2104.00769v3  [eess.AS]  15 Jun 2021\nMulti-head attention \nEmbedded patches \nNorm \nNorm \nMLP \nL × \nMulti-head attention \nEmbedded patches \nNorm \nNorm \nMLP \nL × \nFigure 2: The PostNorm (left) and PreNorm (right) Transformer\nencoder architectures. KWT uses a PostNorm encoder.\neral small-scale network architectures and identiﬁed depthwise-\nseparable CNN (DS-CNN) as providing the best classiﬁca-\ntion/accuracy tradeoff for memory footprint and computational\nresources. Other works have improved upon this result us-\ning synthesized data [18], temporal convolutions [19, 20], and\nself-attention [12]. Recently Rybakov et al. [13] achieved a\nnew state of the art result on Google Speech Commands using\nMHAtt-RNN, a non-streaming CNN, RNN and multi-headed\n(MH) self-attention model.\n2.2. Self-Attention and the Vision Transformer\nDosovitskiy et al. introduced the Vision Transformer (ViT)\n[2] and showed that Transformers can learn high-level image\nfeatures by computing self-attention between different image\npatches. This simple approach outperformed CNNs but re-\nquired pre-training on large datasets. Touvroun et al. [3] im-\nproved data efﬁciency using strong augmentation, careful hy-\nperparameter tuning and token-based distillation.\nWhile Transformers have been explored for wake word de-\ntection [21] and voice triggering [22], to the best of our knowl-\nedge fully-attentional models based on the Transformer archi-\ntecture have not been investigated for keyword spotting. Our\napproach is inspired by ViT, in the sense that we use patches of\nthe audio spectrogram as input and closely follows [3] to under-\nstand how generally this technique applies to new domains. We\nrestrict ourselves to a non-streaming setting in this work, not-\ning that others have previously investigated extensions of Trans-\nformers to a streaming setting [21, 8].\n3. The Keyword Transformer\n3.1. Model Architecture\nLet X ∈ RT×F denote the output of the MFCC spectrogram,\nwith time windows t = 1,...,T and frequencies f = 1,...,F .\nThe spectrogram is ﬁrst mapped to a higher dimension d, us-\ning a linear projection matrix W0 ∈ RF×d in the frequency\ndomain. In order to learn a global feature that represents the\nwhole spectrogram, a learnable class embedding Xclass ∈ R1×d\nis concatenated with the input in the time-domain. Then a learn-\nable positional embedding matrix Xpos ∈ R(T+1)×d is added,\nsuch that the input representation fed into the Transformer en-\ncoder is given by\nX0 = [Xclass; XW0] +Xpos (1)\nThe projected frequency-domain features are then fed into a se-\nquential Transformer encoder consisting of Lmulti-head atten-\ntion (MSA) and multi-layer perceptron (MLP) blocks. In the\nTable 1: Model parameters for the KWT architecture.\nModel dim mlp-dim heads layers # parameters\nKWT-1 64 256 1 12 607K\nKWT-2 128 512 2 12 2,394K\nKWT-3 192 768 3 12 5,361K\nl:th Transformer block, queries, keys and values are calculated\nas Q = XlWQ, K = XlWK and V = XlWV respectively,\nwhere WQ,WK,WV ∈ Rd×dh and dh is the dimensionality of\neach attention-head. The self attention (SA) is calculated as\nSA(Xl) =Softmax\n(QKT\n√dh\n)\nV (2)\nThe MSA operation is obtained by linearly projecting the con-\ncatenated output, using another matrix WP ∈ Rkdh×d, from\nthe kattention heads.\nMSA(Xl) = [SA1(Xl); SA2(Xl); ...; SAk(Xl)]WP (3)\nIn our default setting, we use the PostNorm [1] Transformer\narchitecture as shown in Figure 2, where the Layernorm (LN)\n[23] is applied after the MSA and MLP blocks, in contrast to the\nPreNorm [24] variant, where LN is applied ﬁrst. This decision\nis discussed further in the ablation section. As is typical for\nTransformers, we use GELU [25] activations in all MLP blocks.\nIn summary, the output of the l:th Transformer block is\ngiven by\n˜Xl = LN(MSA(Xl−1) +Xl−1), l = 1,...,L (4)\nXl = LN(MLP( ˜Xl) + ˜Xl), l = 1,...,L (5)\nAt the output layer, the class embedding is fed into a linear clas-\nsiﬁer. Our approach treats time windows in a manner analo-\ngous to the handling of image patches in ViT. Whereas in ViT,\nthe self-attention is computed over image patches, the attention\nmechanism here takes place in the time-domain, such that dif-\nferent time windows will attend to each other in order to form a\nglobal representation in the class embedding.\nThe model size can be adjusted by tuning the parameters of\nthe Transformer. Following [3], we ﬁx the number of sequential\nTransformer encoder blocks to 12, and let d/k = 64, where d\nis the embedding dimension and k is the number of attention\nheads. By varying the number of heads as k = 1,2,3, we end\nup with three different models as shown in Table 1.\n3.2. Knowledge Distillation\nAs introduced by Hinton et al. [26], knowledge distillation uses\na pre-trained teacher’s predictions to provide an auxiliary loss\nto the student model being trained. Touvron et al. [3], intro-\nduced a distillation token, ﬁnding this beneﬁts Transformers in\nthe small data regime. This method adds a learned distillation\ntoken to the input. At the output layer this distillation token is\nfed into a linear classiﬁer and trained using hard (one-hot) labels\npredicted by the teacher.\nLet Zsc be the logits of the student class token, Zsd be the\nlogits of the student distillation token andZt be the logits of the\nteacher model. The overall loss becomes\nL = 1\n2LCE(ψ(Zsc),y) +1\n2LCE(ψ(Zsd),yt), (6)\nTable 2: Hyperparameters used in all experiments.\nTraining\nTraining steps 23,000\nBatch size 512\nOptimizer AdamW\nLearning rate 0.001\nSchedule Cosine\nWarmup epochs 10\nRegularization\nWeight decay 0.1\nLabel smoothing 0.1\nDropout 0\nPre-processing\nTime window length 30 ms\nTime window stride 10 ms\n#DCT Features 40\nData augmentation\nTime shift [ms] [-100, 100]\nResampling [0.85, 1.15]\nBackground vol. 0.1\n#Time masks 2\nTime mask size [0,25]\n#Frequency masks 2\nFrequency mask size [0,7]\nwhere yt = argmax(Zt) are the hard decision of the teacher,y\nare the ground-truth labels, ψis the softmax function and LCE\nis the cross-entropy loss. At inference time the class and distil-\nlation token predictions are averaged to produce a single predic-\ntion. Note that unlike Noisy Student [27], the teacher receives\nthe same augmentation of the input as the student, effectively\ncorrecting labels made invalid by very strong augmentation. In\nall experiments, we use MHAtt-RNN as a teacher and denote\ndistillation models with KWT\n .\n4. Experiments\n4.1. Keyword Spotting on Google Speech Commands\nWe provide experimental results on the Google Speech Com-\nmands dataset V1 and V2 [14]. Both datasets consist of 1 sec-\nond long audio snippets, sampled at 16 kHz, containing utter-\nances of short keywords recorded in natural environments. V1\nof the dataset contains 65,000 snippets of 30 different words,\nwhereas V2 contains 105,000 snippets of 35 different words.\nThe 12-label classiﬁcation task uses 10 words: ”up”, ”down”,\n”left”, ”right”, ”yes”, ”no”, ”on”, ”off”, ”go”, and ”stop”, in\naddition to ”silence” and ”unknown”, where instances of the\nlatter is taken from the remaining words in the dataset, whereas\nthe 35-label task uses all available words. We use the same\n80:10:10 train/validation/test split as [14, 17, 13] for side-by-\nside comparisons. We adhere as closely as possible to the eval-\nuation criteria of [13], and for each experiment, we train the\nmodel three times with different random initializations.\nAs our intention is to explore the extent to which results us-\ning Transformers from other domains transfer to keyword spot-\nting, we follow the choices and hyperparameters from [3] as\nclosely as possible, with the notable exception that we found\nincreasing weight decay from 0.05 to 0.1 to be important. Fur-\nthermore, we use the same data pre-processing and augmenta-\ntion policy as in [13], which consists of random time shifts, re-\nsampling, background noise, as well as augmenting the MFCC\nfeatures using SpecAugment [28]. We train our models over the\nsame number of total input examples as MHAtt-RNN (12M) to\nallow a fair comparison. For clarity, the hyperparameters used\nin all experiments are reported in Table 2.\nThe results are shown in Table 3, where for our own re-\nsults, we report a 95% conﬁdence interval for the mean accu-\nracy over all three model evaluations. Our best models match or\nsurpass the previous state-of-the-art accuracies, with signiﬁcant\nimprovements on both the 12-label and 35-label V2-datasets. In\ngeneral, Transformers tend to beneﬁt more from large amounts\nof data, which could explain why KWT does not outperform\nMHAtt-RNN on the smaller V1-dataset. Nevertheless, we also\nTable 3: Accuracy on Speech Commands V1 [29] and V2 [30].\nModel V1-12 V2-12 V2-35\nDS-CNN [17] 95.4\nTC-ResNet [19] 96.6\nAtt-RNN [12] 95.6 96.9 93.9\nMatchBoxNet [20] 97.48 ±0.11 97.6\nEmbed + Head [18] 97.7\nMHAtt-RNN [13] 97.2 98.0\nRes15 [31] 98.0 96.4\nMHAtt-RNN (Ours) 97.50±0.29 98.36 ±0.13 97.27 ±0.02\nKWT-3 (Ours) 97.24 ±0.24 98.54±0.17 97.51 ±0.14\nKWT-2 (Ours) 97.36 ±0.20 98.21 ±0.06 97.53 ±0.07\nKWT-1 (Ours) 97.05 ±0.23 97.72 ±0.01 96.85 ±0.07\nKWT-3\n (Ours) 97.49±0.15 98.56±0.07 97.69 ±0.09\nKWT-2\n (Ours) 97.27 ±0.08 98.43 ±0.08 97.74±0.03\nKWT-1\n (Ours) 97.26 ±0.18 98.08 ±0.10 96.95 ±0.14\nnote that knowledge distillation is effective in improving the ac-\ncuracy of KWT in most scenarios.\n4.2. Ablation Studies\nWe investigate different approaches to self-attention by vary-\ning the shapes of the MFCC spectrogram patches that are fed\ninto the Transformer. Using our default hyperparameters, the\nspectrogram consists of 98 time windows, containing 40 mel-\nscale frequencies. Our baseline uses time-domain attention, but\nwe also investigate frequency-domain attention and intermedi-\nate steps where rectangular patches are used. We ﬁnd time-\ndomain attention to perform best, as shown in Figure 3. This is\nin agreement with previous ﬁndings that temporal convolutions\nwork well for keyword spotting [19], since the ﬁrst projection\nlayer of our model can be interpreted as a temporal convolution\nwith kernel size (40, 1) and stride 1 in the time-domain.\nWe also investigate the use of PreNorm and PostNorm and\nfound that the latter improves performance for keyword spotting\nin our experiments. This is contrary to previous ﬁndings on\nother tasks [32], where PreNorm has been shown to yield better\nresults and we encourage further work to explore the role of\nnormalization in Transformers across different domains.\n(1, 98) (2,20) (5,8) (8,5) (20, 2) (40, 1) (40, 1)\n Pre-norm\nPatch size (Frequency, Time)\n94\n95\n96\n97\n98\n99\n100Accuracy [%]\nFigure 3: Accuracy on Speech Commands V2-12 using KWT-3\nwith different patch sizes.\nStop No Right Yes\nInput audio\nAttention mask\nFigure 4: The learned attention mask, propagated from the in-\nput to the class token, overlaid on four different audio snippets,\nwithout (top) and with (bottom) background noise.\n4.3. Attention Visualization\nIn order to examine which parts of the audio signal the model\nattends to, we propagate the attention weights of each Trans-\nformer layer from the input to the class token by averaging the\nattention weights over all heads. This produces a set of atten-\ntion weights for each time window of the input signal. Figure\n4 shows the attention mask overlayed on the waveform of four\ndifferent utterances. It can be seen that the model is able to pay\nattention to the important parts of the input while effectively\nsuppressing background noise.\nWe also study the position embeddings of the ﬁnal model by\nanalyzing their cosine similarity, as shown in Figure 5. Nearby\nposition embeddings exhibit a high degree of similarity and dis-\ntant embeddings are almost orthogonal. This pattern is less em-\nphasized for time windows near the start and the beginning of\nthe audio snippets. We hypothesize that this is either because\nwords are typically in the middle of each snippet and therefore\nrelative position is more important there, or because the audio\ncontent at the start and end is less distinguishable.\n0 20 40 60 80\nTime window\n0\n20\n40\n60\n80 Time window\nPosition embedding similarity\n−1\n1\nCosine similarity\nFigure 5: Cosine similarities of the learned position embed-\ndings of KWT.\nFigure 6: Latency and accuracy of processing a one-second in-\nput, on a single thread on a mobile phone.\n4.4. Latency Measurements\nWe converted our KWT models, DS-CNN (with stride) [17],\nTC-ResNet [19] and MHAtt-RNN [13] to Tensorﬂow (TF) Lite\nformat to measure inference latency on a OnePlus 6 mobile de-\nvice based on the Snapdragon 845 (4x Arm Cortex-A75, 4x\nArm Cortex-A55) and report accuracy ﬁgures for the Google\nSpeech Commands V2 with 12 labels and 35 labels [30, 13].\nThe TFLite Benchmark tool [33] is used to measure latency, de-\nﬁned by the processing time of a single one-second input. For\neach model, we do 10 warmup runs followed by 100 inference\nruns, capturing the average latency on a single thread.\nIn Figure 6 we observe that Transformer-based models are\ncompetitive with the existing state-of-the-art despite being de-\nsigned with no regard to latency. There is a broad body of re-\nsearch on optimizing Transformer models — of particular note\nis the replacement of layer normalization and activations in [34]\nthat decreases latency by a factor of three. Our ﬁndings here\nsuggest many of these results could be leveraged in the keyword\nspotting domain to extend the practicality of these models.\n5. Conclusion\nIn this paper we explore the direct application of Transformers\nto keyword spotting, using a standard architecture and a princi-\npled approach to converting the audio input into tokens.\nIn doing so we introduce KWT, a fully-attentional model\nthat matches or exceeds the state-of-the-art over a range of key-\nword spotting tasks with real-world latency that remains com-\npetitive with previous work.\nThese surprising results suggest that Transformer research\nin other domains offers a rich avenue for future exploration in\nthis space. In particular we note that Transformers beneﬁt from\nlarge-scale pre-training [2], have seen 5.5x latency reduction\nthrough model compression [34] and up to 4059x energy re-\nduction through sparsity and hardware codesign [35]. Such im-\nprovements would make a meaningful impact on keyword spot-\nting applications and we encourage future research in this area.\n6. Acknowledgements\nThis work was partially supported by the Wallenberg AI, Au-\ntonomous Systems and Software Program (W ASP), funded by\nthe Knut and Alice Wallenberg Foundation. We thank Matt\nMattina for supporting this work, Magnus Oskarsson for his\nfeedback and comments, and Oleg Rybakov and Hugo Touvron\nfor sharing their code with the community.\n7. References\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Proceedings of the 31st International Conference on Neural\nInformation Processing Systems, 2017, pp. 6000–6010.\n[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly et al., “An image is worth 16x16 words: Transformers\nfor image recognition at scale,”arXiv preprint arXiv:2010.11929,\n2020.\n[3] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J´egou, “Training data-efﬁcient image transformers & distilla-\ntion through attention,” arXiv preprint arXiv:2012.12877, 2020.\n[4] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, F. E. Tay, J. Feng,\nand S. Yan, “Tokens-to-Token ViT: Training Vision Transform-\ners from Scratch on ImageNet,”arXiv preprint arXiv:2101.11986,\n2021.\n[5] M. Kumar, D. Weissenborn, and N. Kalchbrenner, “Colorization\nTransformer,”arXiv preprint arXiv:2102.04432, 2021.\n[6] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\nin European Conference on Computer Vision . Springer, 2020,\npp. 213–229.\n[7] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y . Wu et al. , “Conformer: Convolution-\naugmented Transformer for Speech Recognition,” Proc. Inter-\nspeech 2020, pp. 5036–5040, 2020.\n[8] X. Chen, Y . Wu, Z. Wang, S. Liu, and J. Li, “Developing Real-\ntime Streaming Transformer Transducer for Speech Recognition\non Large-scale Dataset,” arXiv preprint arXiv:arXiv:2010.11395,\n2020.\n[9] A. T. Liu, S.-W. Li, and H.-y. Lee, “Tera: Self-supervised learning\nof transformer encoder representation for speech,” arXiv preprint\narXiv:2007.06028, 2020.\n[10] D. Neimark, O. Bar, M. Zohar, and D. Asselmann, “Video Trans-\nformer Network,”arXiv preprint arXiv:2102.00719, 2021.\n[11] M. A. Alcorn and A. Nguyen, “baller2vec: A Multi-Entity\nTransformer For Multi-Agent Spatiotemporal Modeling,” arXiv\npreprint arXiv:1609.03675, 2021.\n[12] D. C. de Andrade, S. Leo, M. L. D. S. Viana, and C. Bernkopf, “A\nneural attention model for speech command recognition,” arXiv\npreprint arXiv:1808.08929, 2018.\n[13] O. Rybakov, N. Kononenko, N. Subrahmanya, M. Visontai, and\nS. Laurenzo, “Streaming Keyword Spotting on Mobile Devices,”\nProc. Interspeech 2020, pp. 2277–2281, 2020.\n[14] P. Warden, “Speech commands: A dataset for limited-vocabulary\nspeech recognition,” arXiv preprint arXiv:1804.03209, 2018.\n[15] D. Gope, G. Dasika, and M. Mattina, “Ternary hybrid\nneural-tree networks for highly constrained iot applications,” in\nProceedings of Machine Learning and Systems , A. Talwalkar,\nV . Smith, and M. Zaharia, Eds., vol. 1, 2019, pp. 190–200.\n[Online]. Available: https://proceedings.mlsys.org/paper/2019/\nﬁle/a97da629b098b75c294dffdc3e463904-Paper.pdf\n[16] S. Davis and P. Mermelstein, “Comparison of parametric repre-\nsentations for monosyllabic word recognition in continuously spo-\nken sentences,” IEEE transactions on acoustics, speech, and sig-\nnal processing, vol. 28, no. 4, pp. 357–366, 1980.\n[17] Y . Zhang, N. Suda, L. Lai, and V . Chandra, “Hello edge: Keyword\nspotting on microcontrollers,” arXiv preprint arXiv:1711.07128 ,\n2017.\n[18] J. Lin, K. Kilgour, D. Roblek, and M. Shariﬁ, “Training keyword\nspotters with limited and synthesized speech data,” in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2020, pp. 7474–7478.\n[19] S. Choi, S. Seo, B. Shin, H. Byun, M. Kersner, B. Kim, D. Kim,\nand S. Ha, “Temporal Convolution for Real-Time Keyword Spot-\nting on Mobile Devices,”Proc. Interspeech 2019, pp. 3372–3376,\n2019.\n[20] S. Majumdar and B. Ginsburg, “MatchboxNet: 1D Time-Channel\nSeparable Convolutional Neural Network Architecture for Speech\nCommands Recognition,” Proc. Interspeech 2020 , pp. 3356–\n3360, 2020.\n[21] Y . Wang, H. Lv, D. Povey, L. Xie, and S. Khudanpur, “Wake\nword detection with streaming transformers,” in ICASSP 2021-\n2021 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2021, pp. 5864–5868.\n[22] S. Adya, V . Garg, S. Sigtia, P. Simha, and C. Dhir, “Hybrid trans-\nformer/ctc networks for hardware efﬁcient voice triggering,”Proc.\nInterspeech 2020, pp. 3351–3355, 2020.\n[23] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv preprint arXiv:1607.06450, 2016.\n[24] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” inProceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2016, pp. 770–778.\n[25] D. Hendrycks and K. Gimpel, “Gaussian error linear units\n(GELUs),” arXiv preprint arXiv:1606.08415, 2016.\n[26] G. Hinton, O. Vinyals, and J. Dean, “Distilling the Knowledge in\na Neural Network,” in NIPS Deep Learning and Representation\nLearning Workshop, 2015. [Online]. Available: http://arxiv.org/\nabs/1503.02531\n[27] Q. Xie, M.-T. Luong, E. Hovy, and Q. V . Le, “Self-Training With\nNoisy Student Improves ImageNet Classiﬁcation,” inProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2020.\n[28] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, “SpecAugment: A Simple Data Augmen-\ntation Method for Automatic Speech Recognition,” Proc. Inter-\nspeech 2019, pp. 2613–2617, 2019.\n[29] “Speech commands dataset v1.” [Online]. Available: http:\n//download.tensorﬂow.org/data/speech commands v0.01.tar.gz\n[30] “Speech commands dataset v2.” [Online]. Avail-\nable: https://storage.googleapis.com/download.tensorﬂow.org/\ndata/speechcommands v0.02.tar.gz\n[31] R. Vygon and N. Mikhaylovskiy, “Learning Efﬁcient Represen-\ntations for Keyword Spotting with Triplet Loss,” arXiv preprint\narXiv:2101.04792, 2021.\n[32] T. Q. Nguyen and J. Salazar, “Transformers without tears:\nImproving the normalization of self-attention,” arXiv preprint\narXiv:1910.05895, 2019.\n[33] “Tﬂite model benchmark tool.” [Online]. Avail-\nable: https://github.com/tensorﬂow/tensorﬂow/tree/master/\ntensorﬂow/lite/tools/benchmark\n[34] Z. Sun, H. Yu, X. Song, R. Liu, Y . Yang, and D. Zhou, “Mo-\nbileBERT: a Compact Task-Agnostic BERT for Resource-Limited\nDevices,”arXiv preprint arXiv:2004.02984, 2020.\n[35] H. Wang, Z. Zhang, and S. Han, “SpAtten: Efﬁcient Sparse Atten-\ntion Architecture with Cascade Token and Head Pruning,” arXiv\npreprint arXiv:arXiv:2012.09852, 2021.",
  "concepts": [
    {
      "name": "Keyword spotting",
      "score": 0.8833498954772949
    },
    {
      "name": "Computer science",
      "score": 0.8074679374694824
    },
    {
      "name": "Transformer",
      "score": 0.7502657175064087
    },
    {
      "name": "Architecture",
      "score": 0.6740550994873047
    },
    {
      "name": "Encoder",
      "score": 0.4972861111164093
    },
    {
      "name": "Speech recognition",
      "score": 0.47346821427345276
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4617716073989868
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46055495738983154
    },
    {
      "name": "Spotting",
      "score": 0.45822274684906006
    },
    {
      "name": "Natural language processing",
      "score": 0.37456613779067993
    },
    {
      "name": "Engineering",
      "score": 0.07329040765762329
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Keyword spotting"
}