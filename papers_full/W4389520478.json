{
  "title": "Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression",
  "url": "https://openalex.org/W4389520478",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5020275937",
      "name": "Jiduan Liu",
      "affiliations": [
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5100347863",
      "name": "Jiahao Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5055007304",
      "name": "Qifan Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100695181",
      "name": "Jingang Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5068562468",
      "name": "Xunliang Cai",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5037132097",
      "name": "Dongyan Zhao",
      "affiliations": [
        null,
        "Peking University"
      ]
    },
    {
      "id": "https://openalex.org/A5115595561",
      "name": "Ran Wang",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5048839666",
      "name": "Rui Yan",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963393617",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3168339137",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W2774104751",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W3199246732",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2905266130",
    "https://openalex.org/W2133286915",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3095273266",
    "https://openalex.org/W4322759378",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4302305863",
    "https://openalex.org/W4367369740",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W3203532272",
    "https://openalex.org/W4377111647",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W2963218093",
    "https://openalex.org/W3102788578",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3200980294",
    "https://openalex.org/W3207654254",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W4221159394",
    "https://openalex.org/W3104182623",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3177378457",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962767298",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3036973371",
    "https://openalex.org/W4226087293",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W4285269381",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287829148",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2997006708",
    "https://openalex.org/W2964352247",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W3133644679",
    "https://openalex.org/W3101066076",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3176647794",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4296142184",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3202201199",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experiments are conducted on low-resource tasks from SuperGLUE and GLUE benchmarks. The results demonstrate that the proposed approach significantly enhances the performance of small-scale models by leveraging the knowledge from LLMs.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8643–8657\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRetrieval-based Knowledge Transfer: An Effective Approach for Extreme\nLarge Language Model Compression\nJiduan Liu1,2∗, Jiahao Liu3∗, Qifan Wang4 , Jingang Wang3 , Xunliang Cai3\nDongyan Zhao1,2,5,6†, Ran Lucien Wang7 , Rui Yan7†\n1Wangxuan Institute of Computer Technology, Peking University\n2Center for Data Science, AAIS, Peking University; 3Meituan; 4Meta AI\n5National Key Laboratory of General Artificial Intelligence; 6BIGAI, Beijing, China\n7Gaoling School of Artificial Intelligence, Renmin University of China\n{liujiduan,zhaody}@pku.edu.cn, ruiyan@ruc.edu.cn, wqfcr@fb.com\n{liujiahao12,wangjingang02,caixunliang}@meituan.com, ran.wang.math@gmail.com\nAbstract\nLarge-scale pre-trained language models\n(LLMs) have demonstrated exceptional perfor-\nmance in various natural language processing\n(NLP) tasks. However, the massive size of\nthese models poses huge challenges for their\ndeployment in real-world applications. While\nnumerous model compression techniques have\nbeen proposed, most of them are not well-\nsuited for achieving extreme model compres-\nsion when there is a significant gap in model\nscale. In this paper, we introduce a novel\ncompression paradigm called Retrieval-based\nKnowledge Transfer (RetriKT), which effec-\ntively transfers the knowledge of LLMs to ex-\ntremely small-scale models (e.g., 1%). In par-\nticular, our approach extracts knowledge from\nLLMs to construct a knowledge store, from\nwhich the small-scale model can retrieve rele-\nvant information and leverage it for effective\ninference. To improve the quality of the model,\nsoft prompt tuning and Proximal Policy Opti-\nmization (PPO) reinforcement learning tech-\nniques are employed. Extensive experiments\nare conducted on low-resource tasks from Su-\nperGLUE and GLUE benchmarks. The results\ndemonstrate that the proposed approach sig-\nnificantly enhances the performance of small-\nscale models by leveraging the knowledge from\nLLMs.\n1 Introduction\nPre-trained language models (PLMs), such as\nBERT/RoBERTa (Devlin et al., 2019; Liu et al.,\n2019), have demonstrated exceptional performance\nacross various natural language processing (NLP)\napplications. However, these models typically com-\nprise hundreds of millions of parameters, present-\ning a substantial challenge for researchers due to\n∗Equal contribution.\n†Corresponding authors: Dongyan Zhao\n(zhaody@pku.edu.cn) and Rui Yan (ruiyan@ruc.edu.cn).\ntheir massive scale. As a result, the full potential of\nlarge-scale pre-trained language models (PLMs)\nremains untapped. To tackle this challenge, a\nmultitude of model compression techniques have\nbeen proposed, encompassing knowledge distilla-\ntion (Sanh et al., 2019; Jiao et al., 2020; Passban\net al., 2021), network pruning (Liang et al., 2021a;\nGordon et al., 2020), quantization (Zhang et al.,\n2020; Tao et al., 2022) and weight sharing (Lan\net al., 2020).\nHowever, these model compression methods are\nnot directly applicable to scenarios requiring high\ncompression ratios, such as knowledge distillation.\nIn such cases, the introduction of assistant mod-\nels (Mirzadeh et al., 2020; Son et al., 2021) often\nleads to decreased and unstable performance. Re-\ncently, there has been a growing interest in leverag-\ning large language models (LLMs) (Touvron et al.,\n2023; Zeng et al., 2022; Ouyang et al., 2022; Scao\net al., 2022) that possess extensive language knowl-\nedge and can be effectively employed in various\ndownstream tasks. Consequently, it is essential to\nexplore methods for transferring this knowledge\nto small-scale models. Nonetheless, existing ap-\nproaches are inadequate for compressing LLMs\ndue to their exceptionally high compression ratios.\nSome prior research (Wang et al., 2022; Dai et al.,\n2023; Ubani et al., 2023) has suggested utilizing\nLLMs for data augmentation and knowledge trans-\nfer to small-scale models, which allows the lat-\nter to demonstrate improved performance on low-\nresource datasets. However, when tackling more\nchallenging tasks like the SuperGLUE benchmark\n(Wang et al., 2019a), the limited parameter size of\nsmall-scale models becomes a hindrance, prevent-\ning them from effectively retaining the knowledge\ntransferred by LLMs. Consequently, the perfor-\nmance enhancement achieved for small-scale mod-\nels remains constrained.\n8643\nTo effectively transfer the knowledge of Large\nLanguage Models (LLMs) to small-scale mod-\nels, enabling them to efficiently and accurately\ncomplete tasks, we propose a novel compression\nparadigm called Retrieval-based Knowledge Trans-\nfer (RetriKT). Our approach involves two main\nsteps: knowledge extraction from the LLM to con-\nstruct a knowledge store, and subsequent retrieval\nof relevant information from the knowledge store\nby the small-scale model to accomplish the task.\nMore specifically, we employ the technique of soft\nprompt tuning to fine-tune an LLM, ensuring that\nit generates in-domain samples. Additionally, we\nintroduce the Proximal Policy Optimization (PPO)\n(Schulman et al., 2017) reinforcement learning al-\ngorithm to enhance the generation quality. As a\nfinal step, the small-scale model learns to retrieve\npertinent information from the knowledge store.\nWe perform an extensive set of experiments on\ntruly low-resource and challenging tasks sourced\nfrom SuperGLUE (Wang et al., 2019a) and GLUE\n(Wang et al., 2019b) benchmarks. The experimen-\ntal results demonstrate that RetriKT significantly\nenhances the performance of small-scale models\nand outperforms previous SOTA knowledge dis-\ntillation methods by leveraging the knowledge of\nLLMs. This indicates the effectiveness and prac-\nticality of the retrieval-based knowledge transfer\nparadigm for extreme model compression.\nOur contributions can be summarized as follows:\n• We propose a new compression paradigm\ncalled Retrieval-based Knowledge Transfer,\nwhich aims to transfer knowledge from\nLLMs to extremely small-scale models. This\nparadigm addresses the challenge of achiev-\ning extreme model compression when there is\na significant gap in model scale.\n• We introduce the reinforcement learning algo-\nrithm PPO to enhance the generation quality,\nand carefully design the reward function. This\ntechnique contributes to improving the diver-\nsity and accuracy of the knowledge extracted\nfrom LLMs used for knowledge transfer.\n• We conduct extensive experiments on low-\nresource tasks from the SuperGLUE and\nGLUE benchmarks. The results demonstrate\nthat RetriKT significantly enhances the perfor-\nmance of small-scale models and outperforms\nprevious SOTA knowledge distillation meth-\nods by leveraging the knowledge from LLMs.\n2 Related Work\nThis paper involves three areas of work: knowl-\nedge distillation, reinforcement learning, and data\naugmentation.\n2.1 Knowledge Distillation\nWe first introduce related work of knowledge\ndistillation (KD), which can be categorized as\nresponse-based, feature-based, and relation-based\nKD. Response-based KD was initially introduced\nby Hinton et al. (2015), which transfers label\nknowledge by minimizing the KL-divergence be-\ntween predicted distributions of the teacher and\nthe student. Building upon this concept, Sanh et al.\n(2019); Liang et al. (2021b) applied response-based\nKD to tasks such as masked language modeling\nand text classification, resulting in smaller models\nwith slight performance drops. Feature-based ap-\nproaches which are initially introduced by Romero\net al. (2015), entail aligning the feature activations\nbetween the teacher and the student models. Build-\ning upon this concept, more recent techniques have\nexpanded the scope by incorporating hidden rep-\nresentations of the [CLS] token as indicators (Sun\net al., 2019), matching hidden representations of\nall tokens (Jiao et al., 2020), and introducing cus-\ntomized feature-based distillation (Sun et al., 2020).\nOn the other hand, relation-based methods (Park\net al., 2021; Liu et al., 2022a; Jiao et al., 2020;\nWang et al., 2020, 2021a; Wu et al., 2023) aim to\nemphasize the importance of capturing and utiliz-\ning relations among multi-granularity representa-\ntions in both horizontal and vertical directions.\n2.2 Reinforcement Learning\nRecently, reinforcement learning (RL) has gained\nsignificant attention in the field of language mod-\neling. This approach has been successfully ap-\nplied to various language tasks, including summa-\nrization (Paulus et al., 2018; Ziegler et al., 2019;\nStiennon et al., 2020; Wu et al., 2021), dialogue\nsystems (Zhou et al., 2017; Jaques et al., 2020;\nHancock et al., 2019), machine translation (Bah-\ndanau et al., 2017; Kreutzer et al., 2018; Kiegeland\nand Kreutzer, 2021), semantic parsing (Lawrence\nand Riezler, 2018), story generation (Zhou and\nXu, 2020), and question generation (Pang and He,\n2021). Concurrently, there has been a growing in-\nterest in using RL to align language models with\nhuman preferences across a wide range of language\ntasks. For example, Ouyang et al. (2022) employed\n8644\nLLM\n[CLS]\n...Reward Model\n...a. LLM with soft P-tuning\nK Most Similar Samples\nKey ([CLS])Value ([True, False])sentence emb[0.1, 0.9]sentence emb[0.7, 0.3]… …\nSoft Logits\nOpen-book knowledge-store...Small Scale Model\n[CLS] b. Reinforcement learning\nc. Retrieval-based Knowledge Transfer\nKey for Knowledge Store\nSimilarityNearest EmbeddingValue1.0 sentence emb[0.8, 0.2]0.7 sentence emb[0.7, 0.3]… …\nLabelValueTrue[0.1, 0.9]False[0.7, 0.3]Aggregation\nKLDivergence\nR=(Raccuracy+αRdiversity)*BP  Reward\nSimilarity Distribution\nSimilarity Distribution\nMLPSoft Prompts\nFigure 1: The framework of RetriKT which consists of three steps: (1) fine-tune additional soft prompts for the LLM\nby supervised learning; (2) further fine-tune the soft prompts by reinforcement learning to enhance the generation\nquality; (3) create the knowledge store based on the knowledge extracted from the LLM, and teach the small-scale\nmodel how to retrieve relevant knowledge from it. (Retrieval-based Knowledge Transfer)\nRL techniques, specifically Proximal Policy Op-\ntimization (PPO) (Schulman et al., 2017), to fine-\ntune a large language model and align it with mod-\nels of human preference.\n2.3 Data Augmentation\nWu et al. (2019) and Kumar et al. (2020) have\nintroduced a method to create synthetic data by\nrandomly masking words in the original training\ninstances. Another works (Ding et al., 2020; Yang\net al., 2020; Anaby-Tavor et al., 2020) involved\nutilizing Language Models (LMs) and Pre-trained\nLanguage Models (PLMs) to directly generate syn-\nthetic data for NLU tasks. Wang et al. (2021b,\n2022) proposed the use of hard prompts and soft\nprompts to generate synthetic data. In a related\nstudy, Liu et al. (2021b); Zan et al. (2022) have\nuncovered the complementary nature of PLMs and\nclassical approaches.\n3 Methodology\nOur approach revolves around extracting knowl-\nedge from the parameters of a Large Language\nModel (LLM) and leveraging it for the benefit of\nan extreme small-scale model through retrieval.\nWe introduce the technique to extract in-domain\nknowledge from the LLM for each task by freez-\ning its parameters and fine-tuning additional soft\nprompts using the corresponding dataset (section\n3.1). To further enhance the generation quality,\nwe introduce reinforcement learning and carefully\ndesign the reward function to fine-tune the soft\nprompts (section 3.2). Subsequently, we extract the\nin-domain knowledge from the LLM and create a\nknowledge store (section 3.3). Finally, we enable\nthe small-scale model to retrieve and effectively\nutilize the knowledge generated by the LLM to\nsuccessfully perform specific tasks (section 3.4).\n3.1 Soft Prompt Tuning\nTo enhance the extraction of in-domain knowledge\nfrom the LLM, it is crucial to introduce appropriate\nprompts that precede the inputs, effectively guid-\ning the text generation process (e.g. \"Generate rte\nexamples according to the following keywords:\").\nHowever, relying solely on manual design often\nleads to limited diversity and effectiveness in the\ngenerated knowledge. To overcome such limita-\ntions, we employ trainable parameters known as\nsoft prompts, thereby replacing manual templates.\nWe only need to update these soft prompts and\nkeep all other parameters of the LLM fixed during\ntraining for each specific task.\nMore specifically, we utilize P-tuning v2 (Liu\net al., 2022b), which involves incorporating a soft\nprompt into each Transformer (Vaswani et al.,\n8645\n2017) layer. The soft prompt is represented as\na sequence of trainable vectors, denoted as Pj =\n{pj\n1,...,p j\nk}, where jis the j-th layer andkis a hy-\nperparameter which represents the prompt length.\nThe i-th hidden states at the j-th layer, denoted as\nhj\ni, can be expressed as follows:\nhj\ni =\n\n\n\npj\ni i≤k\nwi i>k ∧j = 0\nTrans(Hj−1)i Else\n(1)\nwhere Trans() represents the Transformer layer,\nHj represents all hidden states at j-th layer and\nwi is the word embedding of the input text.\nIn order to generate diverse samples, we adopt\nthe approach presented in PromDA (Wang et al.,\n2022), which involves generating samples from\nboth the Input Viewand the Output View. The Input\nView is conditioned on the keywords present in the\ninput texts, which are extracted by the unsupervised\nkeyword extraction algorithm Rake (Rose et al.,\n2010). On the other hand, the Output View is con-\nditioned on the labels. Additionally, we train two\nsets of soft prompts, namely Pinput and Poutput,\nwhich are specifically designed to generate samples\nfrom the Input View and the Output View respec-\ntively. Each sample S = (Y,X) generated by\nthe LLM consists of two parts, namely the label\nY = {yi}ly\ni=1 and the input text X = {xi}lx\ni=1. The\nsupervised learning object for soft prompts Pinput\nand Poutput can be formalized as:\nLinput = −\nly+lx∑\ni=1\nlog(p(si|s<i, Pinput, K)), (2)\nLoutput = −\nly+lx∑\ni=1\nlog(p(si|s<i, Poutput, Y)), (3)\nwhere Kand Y represent the keywords of the input\ntext and the label, respectively.\n3.2 Reinforcement Learning\nConstructing a knowledge store that combines ac-\ncuracy and diversity is essential for enabling the\nsmall-scale model to effectively retrieve relevant\ninformation. However, traditional supervised learn-\ning methods are unable to optimize these learn-\ning objectives due to the lack of per-token differ-\nentiability. Therefore, we employ reinforcement\nlearning to improve the generation quality. This\napproach involves guaranteeing that the generated\ntexts align with the labels, thereby ensuring the\naccuracy of the knowledge store. Additionally, it\nensures that the generated samples exhibit diver-\nsity, thus fulfilling the diversity requirement of the\nknowledge store.\nMore specifically, we utilize the original dataset\nto train a classification model, which serves as the\nreward model denoted as RM(). For each gener-\nated sample, comprising both input text and label,\nwe assess the confidence score of the text’s label\nusing the reward model, thereby obtaining the ac-\ncuracy reward Raccuracy. Additionally, we evaluate\nthe diversity of the generated samples by employ-\ning the Self-Bleu metric (Zhu et al., 2018), which\ncalculates the Bleu score (Papineni et al., 2002)\nconsidering other samples as references. Subse-\nquently, we calculate the diversity rewardRdiversity\nas 1 −b3, where b3 represents the Bleu-3 metric.\nTo avoid the generation of overly simplistic pat-\nterns by the LLM, we apply a length penalty. If\nthe length of a generated sample falls below a cer-\ntain threshold lmin, the reward is multiplied by a\ndiscount factor:\nBP =\n{ 1 l≥lmin\nexp(1 −lmin\nl ) l<l min\n(4)\nwhere lrepresents the length of the generated sam-\nple. The final reward for each generated sample\nis:\nR= (Raccuracy + αRdiversity) ×BP, (5)\nwhere αis a hyperparameter to balance the weight\nof accuracy reward and diversity reward.\nFollowing previous studies (Ouyang et al., 2022;\nBai et al., 2022), we employ Proximal Policy Op-\ntimization (PPO) (Schulman et al., 2017) as the\nreinforcement learning algorithm to fine-tune the\nsoft prompts Pinputand Poutput. To ensure reliable\nperformance in generating in-domain samples, we\nincorporate supervised gradients into the PPO gra-\ndients. Our objective is to minimize the following\nloss function:\nLLLM = Lp + Lv + β∗Lsft, (6)\nwhere Lp and Lv are actor loss and critic loss of the\nPPO algorithm respectively, Lsft represents Linput\nand Loutput for Pinput and Poutput respectively,\nand β is the hyperparameter which controls the\nstrength of supervised gradients.\n8646\n3.3 Knowledge Store\nWe employ the fine-tuned LLM to generate sam-\nples and construct a knowledge store for each task.\nSpecifically, for each sample in the original dataset\nD, we utilize the keywords extracted by the Rake\nalgorithm and labels as inputs for the model. Each\nsample is used mtimes, and in each iteration, n\nsamples are generated. To ensure distinctiveness\namong the generated samples, we adopt p-sampling\n(Holtzman et al., 2020), which constrains tokens to\nthe smallest possible set whose cumulative proba-\nbility exceeds the probability parameter p. The set\nof samples generated from keywords is denoted as\nDI, while the set generated from labels is denoted\nas DO. The quantity of both DI and DO is mn|D|,\nwhere |D|represents the quantity of the original\ndataset D.\nTo further enhance the diversity of generated\nsamples, we utilize the label of each sample in\nDI as the model input to generate a sample each\ntime, resulting in the sample set DIO. Similarly,\nwe use the keyword of each example in DO as the\nmodel input to generate a sample each time, result-\ning in the sample set DOI. The final knowledge\nstore is composed of these sample sets, namely\nD⋃DI\n⋃DO\n⋃DIO\n⋃DOI, which theoretically\nyields a total of (1 + 4mn)|D|samples. However,\nthere is a possibility of duplicates in the generated\nsamples, so we remove them.\nAs illustrated in Figure 1, the key for each sam-\nple in the knowledge store is the sentence embed-\nding produced by the small-scale model. These\nembeddings enable the model to retrieve relevant\ninformation from the knowledge store. The value\nof each sample is the probability distribution pro-\nduced by the reward model RM() for each task.\n3.4 Retrieval-based Knowledge Transfer\nDue to the limited number of parameters in the\nsmall-scale model, it is not feasible to directly\nmemorize all the knowledge extracted from the\nLLM. Therefore, we do not train the small-scale\nmodel directly with the generated samples. Instead,\nwe leverage the knowledge in a retrieval manner.\nSpecifically, we utilize the reward model RM()\nto provide the sentence representation fT(xi) for\neach generated sample xi, which is then used to\ncalculate cosine similarity scores with other gen-\nerated samples in the mini-batch. The similar-\nity score list obtained from the reward model can\nbe denoted as ST\ni = {sT(xi,xj)}j∈[1,N]∧j̸=i =\nDataset |Train| |Dev| |Test| Metrics\nBoolQ 9427 3270 3245 acc.\nCB 250 56 250 acc.\nCOPA 400 100 500 acc.\nRTE 2490 277 3000 acc.\nWiC 5428 638 1400 acc.\nCola 8551 1043 1063 mcc.\nTable 1: The data statistics and metrics of all tasks.\n{ϕ(fT(xi),fT(xj))}j∈[1,N]∧j̸=i, where N repre-\nsents the batch size and ϕdenotes the cosine simi-\nlarity. Similarly, we can obtain the similarity score\nlist SS\ni from the small-scale model. These similar-\nity scores are then normalized in a listwise way to\nobtain the relevance distributions:\n˜sT(xi,xj) = esT (xi,xj)/τ1\n∑\nk∈[1,N]∧k̸=iesT (xi,xk)/τ1\n, (7)\n˜sS(xi,xj) = esS(xi,xj)/τ2\n∑\nk∈[1,N]∧k̸=iesS(xi,xk)/τ2\n, (8)\nwhere τ1 and τ2 are temperature hyperparameters\nto smooth the distributions.\nOur objective is to enable the small-scale model\nto learn how to retrieve relevant information from\nthe knowledge store. To achieve this, we mini-\nmize the KL-divergence between the two relevance\ndistributions ˜ST\ni = {˜sT(xi,xj)}j∈[1,N]∧j̸=i and\n˜SS\ni = {˜sS(xi,xj)}j∈[1,N]∧j̸=i as the learning ob-\nject for the small-scale model:\nLsmall =\nN∑\ni\n˜SS\ni ˙log\n˜SS\ni\n˜ST\ni\n. (9)\nDuring the inference of the small-scale model,\nwe retrieve the k most relevant samples from the\nknowledge store for each test sample x, denoted\nas {xi}k\ni=1. To determine the weight of each re-\ntrieved sample xi, we calculate the similarity score\nbetween the test sample xand the retrieved sample\nxi. The final prediction logit score for each class\nof the small-scale model is obtained as follows:\npS\nc = ϕ(x,xi)∑k\nj=1 ϕ(x,xj)\npT\nc , (10)\nwhere cis a specific class and pT\nc is the confidence\nscore of the label cpredicted by the reward model\nRM(). The final prediction of the small-scale\nmodel is the class with the largest logit score.\n8647\nModel #Params WiC CB COPA RTE BoolQ CoLA Avg.\nEncT5xl (Teacher) 3B 75.71 98.20 92.00 92.06 88.99 71.63 86.43\nBERT2 (Student) 7.2M 59.72 78.57 55.00 61.37 68.81 23.35 57.80\n+ Vanilla KD (Hinton et al., 2015) 7.2M 60.66 78.57 56.00 62.09 68.72 27.14 58.86\n+ Vanilla KD (w. TA) (Hinton et al., 2015)7.2M 61.13 82.14 58.00 61.37 69.33 26.12 59.68\n+ MSGKD (w. TA) (Liu et al., 2022a) 7.2M 59.72 83.93 60.00 62.45 68.13 29.39 60.60\n+ AD-KD (w. TA) (Wu et al., 2023) 7.2M 62.85 83.93 61.00 62.82 69.60 31.89 62.02\n+ RetriKT-KD 7.2M 63.79 89.29 63.00 64.26 71.10 40.94 65.40\n+ RetriKT-Retrieval 7.2M 66.61 94.64 66.00 66.43 74.62 47.87 69.36\nBERT4 (Student) 13.5M 62.85 85.71 60.00 65.34 70.86 38.69 63.91\n+ Vanilla KD (Hinton et al., 2015) 13.5M 62.70 85.71 62.00 66.43 70.73 39.16 64.46\n+ Vanilla KD (w. TA) (Hinton et al., 2015)13.5M 63.79 87.50 60.00 66.06 70.98 40.81 64.86\n+ MSGKD (w. TA) (Liu et al., 2022a) 13.5M 62.38 87.50 63.00 65.34 71.13 40.78 65.02\n+ AD-KD (w. TA) (Wu et al., 2023) 13.5M 64.89 87.50 64.00 66.43 72.81 41.77 66.23\n+ RetriKT-KD 13.5M 68.65 92.86 72.00 74.01 77.22 58.16 73.82\n+ RetriKT-Retrieval 13.5M 69.44 94.64 74.00 75.45 78.75 60.77 75.51\nTable 2: Main experimental results (%) on six low-resource tasks. We also report the quantity of parameters of each\nPLM (without embeddings). We re-implement all baseline models based on the released code provided by AD-KD\n(Wu et al., 2023), and incorporate BERTbase as the TA model (w. TA). There are two versions of RetriKT, one of\nwhich is trained by Vanilla KD by generated samples (RetriKT-KD), and the other is trained by retrieval learning\nobject (RetriKT-Retrieval). The best results of each backbone are shown in bold. Results are statistically significant\nwith respect to all baselines on each student model (all p-value < 0.005).\n4 Experimental Setup\n4.1 Datasets\nWe assess the performance of our method on mul-\ntiple datasets from the SuperGLUE beachmark\n(Wang et al., 2019a) and GLUE benchmark (Wang\net al., 2019b), including BoolQ (Clark et al., 2019),\nCB (De Marneffe et al., 2019), COPA (Roemmele\net al., 2011), RTE, WiC (Pilehvar and Camacho-\nCollados, 2019), CoLA (Warstadt et al., 2019).\nThese datasets pose a challenge for small-scale\nmodels as they are considered difficult and low-\nresource, with training samples numbering fewer\nthan 10K (details in Table 1).\n4.2 Baselines\nWe compare our methods with Vanilla KD (Hinton\net al., 2015) and recent strong KD methods includ-\ning MSGKD (Liu et al., 2022a) and AD-KD (Wu\net al., 2023), which are re-implemented based on\nthe released code provided by AD-KD. We fine-\ntune EncT5 xl (Liu et al., 2021a) as the teacher\nmodel, and train BERTbase using the Vanilla KD\nmethod as the assistant model (TA) for all base-\nlines. We carry out grid-search of learning rate\n∈{1e-5, 2e-5, 3e-5, 4e-5}, batch size ∈{4,8,16}\nfor datasets COPA and CB, and batch size ∈\n{16,32,64}for other datasets. The training epoch\nis set to 40 for CB and COPA, 20 for RTE and 10\nfor other datasets. We present the results on the\nvalidation set obtained from the best checkpoint\nduring training.\n4.3 Implementation\nWe implement all experiments with the deep learn-\ning framework PyTorch based on PromDA (Wang\net al., 2022) and trl library (von Werra et al., 2020)\non up to eight NVIDIA Tesla A100 GPUs (80GB\nmemory). We build the LLM based on T5xl with\n3B parameters (Raffel et al., 2020), and translate\neach dataset into a single-sentence format accord-\ning to the template provided by T5. The pre-\nprocessed example of each dataset is shown in Ap-\npendix B. We utilize two small-scale BERT models\nreleased by Turc et al. (2019), one with 4 Trans-\nformer layers, 512 hidden neurons and 8 attention\nheads, and the other with 2 Transformer layers, 512\nhidden neurons and 8 attention heads.\nFirst, we tune the soft prompts Pinput and\nPoutput by supervised learning as the reference\nand initial models for PPO algorithm. The prompt\nlength is set to 8 for all datasets. The learning\n8648\nrate and batch size is set to 1e-3 and 64, respec-\ntively. The training step is set to 10K for COPA\nand CB, 20K for RTE, and 40K for other datasets.\nWe fine-tune EncT5xl as the reward model for each\ndataset, which is also the teacher model for all base-\nline models for a fair comparison. Then we train\nthe soft prompts through a combination of super-\nvised learning and reinforcement learning by the\nloss function defined in Eq.(6). We set α and β\nto 0.2 and 1 respectively. Generate number nand\nprobability pfor top-p sampling are always set to\n5 and 0.9 respectively, while sample time mis set\nto 64 for COPA and CB, 16 for RTE, and 8 for\nother datasets. Finally, we train the small-scale\nBERT models with a grid search of learning rate\n∈{2e-5, 3e-5}, temperate combinations (τ1,τ2) ∈\n{(0.2, 0.1), (0.1, 0.05)}. The batch size is set to\n128. There are two versions of RetriKT, one of\nwhich is trained by Vanilla KD by generated sam-\nples (RetriKT-KD), and the other is trained by re-\ntrieval learning object (RetriKT-Retrieval). More\ntraining details for PPO algorithm can be found in\nAppendix A\n5 Experimental Results and Analysis\n5.1 Main Results\nAs presented in Table 2, it is evident that both\nRetriKT-KD and RetriKT-Retrieval outperform pre-\nvious KD methods across all backbones, which\ndemonstrates the effectiveness of our approach. For\ninstance, compared to the previous state-of-the-art\nmethod AD-KD, RetriKT-Retrieval demonstrates\nsignificant improvements: 7.34% on BERT 2 and\n9.28% on BERT4. Notably, RetriKT-Retrieval con-\nsistently outperforms RetriKT-KD, especially on\nBERT2, indicating that retrieving relevant informa-\ntion from the knowledge store is more suitable for\nsmall-scale models rather than attempting to mem-\norize all the knowledge. Furthermore, we observe\ncomparable performance between Vanilla KD and\nVanilla KD (w.TA), suggesting limitations in using\nthe TA model for knowledge transfer. A more ef-\nficient approach for transferring knowledge from\nthe LLM to the small-scale model is to prompt the\nLLM to generate in-domain knowledge, which can\nbe retrieved by the small-scale model as relevant\ninformation for effective inference.\n5.2 Ablation Study\nTo investigate the impact of different components\nof reinforcement learning in our approach, we con-\nPLM Model CB COPA CoLA\nBERT2\nRetriKT-KD 89.29 63.00 40.94\nw/o RL 87.50 61.00 36.38\nw/o Raccuracy 87.50 60.00 40.53\nw/o Rdiversity 85.71 62.00 38.22\nw/o BP 87.50 60.00 39.13\nRetriKT-Retrieval 94.64 66.00 47.87\nw/o RL 89.29 62.00 41.57\nw/o Raccuracy 91.07 62.00 45.32\nw/o Rdiversity 87.50 61.00 43.24\nw/o BP 89.29 62.00 44.98\nBERT4\nRetriKT-KD 92.86 72.00 58.16\nw/o RL 89.29 67.00 53.18\nw/o Raccuracy 91.07 65.00 57.78\nw/o Rdiversity 89.29 69.00 57.01\nw/o BP 91.07 66.00 57.27\nRetriKT-Retrieval 94.64 74.00 60.77\nw/o RL 92.86 69.00 56.45\nw/o Raccuracy 92.86 69.00 59.94\nw/o Rdiversity 91.07 70.00 58.25\nw/o BP 92.86 69.00 57.82\nTable 3: Ablation studies of different components uti-\nlizing BERT2 and BERT4 as the small-scale models on\ndatasets CB, COPA and CoLA.\nduct a series of ablation studies. We remove re-\ninforcement training (w/o RL), accuracy reward\nRaccuracy, diversity reward R diversity, and length\npenalty BP, and evaluate the results on CB, COPA,\nand CoLA, as shown in Table 3. Several key obser-\nvations can be drawn from the experimental results.\nFirstly, it is evident that the model performance of\nboth RetriKT-KD and RetriKT-Retrieval declines\nwithout RL or any component of RL. This high-\nlights the effectiveness of our reward function de-\nsign and RL, which enhance the generation quality\nof the LLM. Secondly, when any component of\nRL is removed, the model performance on CoLA\nshows relatively smaller decreases compared to\nCB and COPA. This suggests that the generation\nquality of the LLM has a more pronounced im-\npact on smaller datasets, while larger datasets ex-\nhibit a certain level of robustness. However, RL\nconsistently improves the generation quality of\nthe LLM, thereby enhancing overall model perfor-\nmance. Finally, it is worth mentioning that RetriKT-\nRetrieval consistently outperforms RetriKT-KD\nacross almost all settings, further validating the\neffectiveness of retrieval-based knowledge transfer\nfor small-scale models.\n8649\nMetric Model WiC CB COPA RTE BoolQ CoLA\nSelf-Bleu RetriKT 0.912 0.948 0.970 0.923 0.884 0.959\nRetriKT w/o RL 0.945 0.968 0.983 0.937 0.939 0.973\nCross Entropy RetriKT 0.734 0.781 0.786 0.781 0.760 0.579\nRetriKT w/o RL 0.758 0.896 0.796 0.794 0.779 0.618\nTable 4: Diversity and accuracy analysis for the generated knowledge. Self-Bleu reflects the diversity of knowledge,\nwhile cross entropy reflects the accuracy of knowledge. In both metrics, smaller values indicate better performance.\n4 8 16 32 64 128\nSample Time m\n56\n58\n60\n62\n64\n66Acc.\nRetriKT-KD\nRetriKT-Retrieval\nFinetune\n(a) BERT2\n4 8 16 32 64 128\nSample Time m\n60\n62\n64\n66\n68\n70\n72\n74Acc.\nRetriKT-KD\nRetriKT-Retrieval\nFinetune (b) BERT4\nFigure 2: Effect of the sample times m. Results are the\nmodel performance on the COPA dataset using BERT2\nand BERT4. The Finetune method is trained by the\noriginal dataset so it is independent of m.\n2-128 2-512 4-512 6-768 12-768\nModel Size\n55\n60\n65\n70\n75Acc.\nRetriKT-KD\nRetriKT-Retrieval \nFinetune\n(a) COPA\n2-128 2-512 4-512 6-768 12-768\nModel Size\n10\n20\n30\n40\n50\n60\n70Mcc.\nRetriKT-KD\nRetriKT-Retrieval \nFinetune (b) CoLA\nFigure 3: Effect of the model size. x-axis is the number\nof layers and hidden size. Results are the model perfor-\nmance on the COPA and CoLA datasets.\n5.3 Detailed Analysis\nEffect of the Sample TimesWe conduct experi-\nments on the COPA dataset to investigate the im-\npact of the sample time mon model performance.\nAs shown in Figure 2, with a small value of m,\nthe performance of both RetriKT-KD and RetriKT-\nRetrieval is comparable. However, as mincreases,\nwe observe a significant improvement in the perfor-\nmance of RetriKT-Retrieval compared to RetriKT-\nKD. This finding suggests that small-scale models,\nconstrained by their limited parameters, are un-\nable to effectively memorize such a large amount\nof knowledge. Therefore, retrieval-based knowl-\nedge transfer proves to be a more favorable ap-\nproach. Furthermore, as mcontinues to increase,\nwe observe a corresponding increase in the perfor-\nmance of both RetriKT-KD and RetriKT-Retrieval,\nreaching a certain threshold. This indicates that\nincreasing the number of generated samples can\nenhance the diversity of knowledge. However, due\nto the limited number of keywords extracted from\nthe original dataset, the diversity of knowledge\nplateaus once the generation threshold is reached.\nConsequently, the model performance does not\nshow further improvement beyond this point.\nEffect of the Model Size We conduct an in-\nvestigation into the impact of model size on per-\nformance using two datasets, COPA and CoLA.\nOur experimental results shown in Figure 3 reveal\nthat when employing a smaller model, RetriKT-\nRetrieval consistently outperforms RetriKT-KD,\nshowcasing the superiority of retrieval-based\nknowledge transfer for small-scale models. Ad-\nditionally, we observe that both RetriKT-Retrieval\nand RetriKT-KD consistently outperform the train-\ning of the small-scale model solely fine-tuning by\nthe original dataset. This observation further em-\nphasizes the effectiveness of knowledge extraction\nfrom the LLM and its application in enhancing\nmodel performance.\nAccuracy and Diversity In this section, we ex-\namine whether the LLM trained through reinforce-\nment learning can generate more accurate and di-\nverse knowledge. To evaluate the diversity of\nknowledge, we employ the Self-Bleu metric on the\ngenerated samples, while the accuracy of knowl-\nedge is measured using the cross-entropy between\nthe probability distribution predicted by the reward\nmodel and the label generated by the LLM. In both\nmetrics, smaller values indicate better performance.\nAs shown in Table 4, the results demonstrate that\ntraining the LLM through reinforcement learning\nleads to the generation of more accurate and diverse\nknowledge across all datasets. This outcome high-\nlights the effectiveness of our reward function de-\n8650\nsign. Additionally, we observe that larger datasets\ntend to yield samples with smaller Self-Bleu met-\nrics. We conjecture that this phenomenon is a re-\nsult of the larger dataset’s ability to extract a wider\nrange of diverse keywords, thereby enabling the\ngeneration of more varied knowledge.\n6 Conclusion\nOur study tackles the task of compressing LLMs\nand introduces a pioneering compression paradigm\ncalled Retrieval-based Knowledge Transfer. This\napproach efficiently transfers the knowledge of\nLLMs to small-scale models by creating a knowl-\nedge store, enabling the small model to retrieve per-\ntinent information during inference. Through ex-\ntensive experiments conducted on commonly used\nbenchmarks, we demonstrate that our framework\nsubstantially improves the performance of small-\nscale models by leveraging the knowledge con-\ntained within LLMs. In future research, we plan to\ninvestigate the application and performance of our\nproposed method on even larger language models,\nsuch as T5-11B.\nLimitations\nIn this section, we discuss the limitations of our\nwork as follows. Firstly, due to limited computa-\ntional resources, we did not attempt experiments\nwith larger models such as T5-11B. Furthermore,\nresource constraints constrained our grid search\nfor hyperparameters within a limited range, which\npotentially leaves room for enhancing the metrics\nshowcased in this paper. Secondly, our proposed\nmethod requires the establishment of a knowledge\nstore. Compared to other model compression meth-\nods, our approach introduces additional storage\nspace and incurs slightly additional time for re-\ntrieval\nAcknowledgements\nThis work is supported by National Key R&D Pro-\ngram of China (No. 2021YFC3340303) and Na-\ntional Natural Science Foundation of China (NSFC\nGrant No. 62122089). Jingang Wang is funded by\nBeijing Nova Program (Grant NO. 20220484098).\nWe sincerely thank all reviewers for their valuable\ncomments and suggestions, which are crucial for\nimproving our work. We would also like to ac-\nknowledge Angela Li for her contributions in cre-\nating the figures used in this work.\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7383–\n7390. AAAI Press.\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu,\nAnirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.\nCourville, and Yoshua Bengio. 2017. An actor-critic\nalgorithm for sequence prediction. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom B.\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBenjamin Mann, and Jared Kaplan. 2022. Train-\ning a helpful and harmless assistant with rein-\nforcement learning from human feedback. CoRR,\nabs/2204.05862.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 2924–2936. Associa-\ntion for Computational Linguistics.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke\nHuang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, Hongmin Cai, Quanzheng\nLi, Dinggang Shen, Tianming Liu, and Xiang Li.\n2023. Chataug: Leveraging chatgpt for text data\naugmentation. CoRR, abs/2302.13007.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107–124.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\n8651\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nBosheng Ding, Linlin Liu, Lidong Bing, Canasai Kru-\nengkrai, Thien Hai Nguyen, Shafiq R. Joty, Luo Si,\nand Chunyan Miao. 2020. DAGA: data augmentation\nwith a generation approach for low-resource tagging\ntasks. CoRR, abs/2011.01549.\nMitchell A. Gordon, Kevin Duh, and Nicholas Andrews.\n2020. Compressing BERT: studying the effects of\nweight pruning on transfer learning. In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP , RepL4NLP@ACL 2020, Online, July 9, 2020,\npages 143–155. Association for Computational Lin-\nguistics.\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel\nMazaré, and Jason Weston. 2019. Learning from\ndialogue after deployment: Feed yourself, chatbot!\nIn Proceedings of the 57th Conference of the Associ-\nation for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 3667–3684. Association for Computa-\ntional Linguistics.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nNatasha Jaques, Judy Hanwen Shen, Asma Ghande-\nharioun, Craig Ferguson, Àgata Lapedriza, Noah\nJones, Shixiang Gu, and Rosalind W. Picard. 2020.\nHuman-centric dialog training via offline reinforce-\nment learning. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 3985–4003. Association for Computa-\ntional Linguistics.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinybert: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, Online Event,\n16-20 November 2020, volume EMNLP 2020 ofFind-\nings of ACL, pages 4163–4174. Association for Com-\nputational Linguistics.\nSamuel Kiegeland and Julia Kreutzer. 2021. Revisiting\nthe weaknesses of reinforcement learning for neu-\nral machine translation. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021, pages 1673–1681. Association for\nComputational Linguistics.\nJulia Kreutzer, Shahram Khadivi, Evgeny Matusov, and\nStefan Riezler. 2018. Can neural machine translation\nbe improved with user feedback? In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2018,\nNew Orleans, Louisiana, USA, June 1-6, 2018, Vol-\nume 3 (Industry Papers), pages 92–105. Association\nfor Computational Linguistics.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. CoRR, abs/2003.02245.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nCarolin Lawrence and Stefan Riezler. 2018. Improving\na neural semantic parser by counterfactual learning\nfrom human bandit feedback. In Proceedings of the\n56th Annual Meeting of the Association for Computa-\ntional Linguistics, ACL 2018, Melbourne, Australia,\nJuly 15-20, 2018, Volume 1: Long Papers , pages\n1820–1830. Association for Computational Linguis-\ntics.\nChen Liang, Simiao Zuo, Minshuo Chen, Haoming\nJiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and\nWeizhu Chen. 2021a. Super tickets in pre-trained\nlanguage models: From model compression to im-\nproving generalization. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing, ACL/IJCNLP\n2021, (Volume 1: Long Papers), Virtual Event, Au-\ngust 1-6, 2021 , pages 6524–6538. Association for\nComputational Linguistics.\nKevin J. Liang, Weituo Hao, Dinghan Shen, Yufan Zhou,\nWeizhu Chen, Changyou Chen, and Lawrence Carin.\n2021b. Mixkd: Towards efficient distillation of large-\nscale language models. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Vir-\ntual Event, Austria, May 3-7, 2021. OpenReview.net.\nChang Liu, Chongyang Tao, Jiazhan Feng, and Dongyan\nZhao. 2022a. Multi-granularity structural knowl-\nedge distillation for language model compression.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), ACL 2022, Dublin, Ireland, May\n22-27, 2022, pages 1001–1011. Association for Com-\nputational Linguistics.\nFrederick Liu, Siamak Shakeri, Hongkun Yu, and Jing\nLi. 2021a. Enct5: Fine-tuning T5 encoder for non-\nautoregressive tasks. CoRR, abs/2110.08426.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning:\n8652\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 61–68. As-\nsociation for Computational Linguistics.\nXuebo Liu, Longyue Wang, Derek F. Wong, Liang\nDing, Lidia S. Chao, Shuming Shi, and Zhaopeng Tu.\n2021b. On the complementarity between pre-training\nand back-translation for neural machine translation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2900–2907, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nSeyed-Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distilla-\ntion via teacher assistant. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 5191–5198. AAAI Press.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nRichard Yuanzhe Pang and He He. 2021. Text genera-\ntion by learning from demonstrations. In 9th Inter-\nnational Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 .\nOpenReview.net.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA, pages 311–318. ACL.\nGeondo Park, Gyeongman Kim, and Eunho Yang. 2021.\nDistilling linguistic context for language model com-\npression. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021, pages 364–378.\nAssociation for Computational Linguistics.\nPeyman Passban, Yimeng Wu, Mehdi Rezagholizadeh,\nand Qun Liu. 2021. ALP-KD: attention-based layer\nprojection for knowledge distillation. In Thirty-Fifth\nAAAI Conference on Artificial Intelligence, AAAI\n2021, Thirty-Third Conference on Innovative Ap-\nplications of Artificial Intelligence, IAAI 2021, The\nEleventh Symposium on Educational Advances in Ar-\ntificial Intelligence, EAAI 2021, Virtual Event, Febru-\nary 2-9, 2021, pages 13657–13665. AAAI Press.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings. OpenReview.net.\nMohammad Taher Pilehvar and José Camacho-Collados.\n2019. Wic: the word-in-context dataset for evaluat-\ning context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,\n2019, Volume 1 (Long and Short Papers), pages 1267–\n1273. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nRajkumar Ramamurthy, Prithviraj Ammanabrolu,\nKianté Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n2022. Is reinforcement learning (not) for natural\nlanguage processing?: Benchmarks, baselines, and\nbuilding blocks for natural language policy optimiza-\ntion. CoRR, abs/2210.01241.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S. Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In Logical Formalizations of Commonsense\nReasoning, Papers from the 2011 AAAI Spring Sym-\nposium, Technical Report SS-11-06, Stanford, Cali-\nfornia, USA, March 21-23, 2011. AAAI.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2015. Fitnets: Hints for thin deep nets. In\n3rd International Conference on Learning Represen-\ntations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings.\nStuart Rose, Dave Engel, Nick Cramer, and Wendy\nCowley. 2010. Automatic keyword extraction from\nindividual documents. Text mining: applications and\ntheory, 1:1–20.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter. CoRR,\nabs/1910.01108.\n8653\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ilic, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, Jonathan Tow, Alexander M. Rush,\nStella Biderman, Albert Webson, Pawan Sasanka Am-\nmanamanchi, Thomas Wang, Benoît Sagot, Niklas\nMuennighoff, Albert Villanova del Moral, Olatunji\nRuwase, Rachel Bawden, Stas Bekman, Angelina\nMcMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Vic-\ntor Sanh, Hugo Laurençon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron\nGokaslan, Adi Simhi, Aitor Soroa, Alham Fikri\nAji, Amit Alfassy, Anna Rogers, Ariel Kreisberg\nNitzav, Canwen Xu, Chenghao Mou, Chris Emezue,\nChristopher Klamm, Colin Leong, Daniel van Strien,\nDavid Ifeoluwa Adelani, and et al. 2022. BLOOM:\nA 176b-parameter open-access multilingual language\nmodel. CoRR, abs/2211.05100.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347.\nWonchul Son, Jaemin Na, Junyong Choi, and Wonjun\nHwang. 2021. Densely guided knowledge distillation\nusing multiple teacher assistants. In 2021 IEEE/CVF\nInternational Conference on Computer Vision, ICCV\n2021, Montreal, QC, Canada, October 10-17, 2021,\npages 9375–9384. IEEE.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F. Christiano. 2020. Learn-\ning to summarize from human feedback. CoRR,\nabs/2009.01325.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 4322–\n4331. Association for Computational Linguistics.\nSiqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang\nWang, and Jingjing Liu. 2020. Contrastive distil-\nlation on intermediate representations for language\nmodel compression. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 498–508. Association for Computational\nLinguistics.\nChaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, Ping Luo, and Ngai Wong. 2022.\nCompression of generative pre-trained language mod-\nels via quantization. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 4821–4836.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurélien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nSolomon Ubani, Suleyman Olcay Polat, and Rod-\nney Nielsen. 2023. Zeroshotdataaug: Generating\nand augmenting training data with chatgpt. CoRR,\nabs/2304.14334.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nLeandro von Werra, Younes Belkada, Lewis Tunstall,\nEdward Beeching, Tristan Thrush, and Nathan Lam-\nbert. 2020. Trl: Transformer reinforcement learning.\nhttps://github.com/lvwerra/trl.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019a. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 3261–3275.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021a. Minilmv2: Multi-head self-\nattention relation distillation for compressing pre-\ntrained transformers. In Findings of the Associa-\ntion for Computational Linguistics: ACL/IJCNLP\n2021, Online Event, August 1-6, 2021 , volume\nACL/IJCNLP 2021 of Findings of ACL, pages 2140–\n2151. Association for Computational Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In Advances in Neural\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\n8654\nYufei Wang, Can Xu, Qingfeng Sun, Huang Hu,\nChongyang Tao, Xiubo Geng, and Daxin Jiang. 2022.\nPromda: Prompt-based data augmentation for low-\nresource NLU tasks. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 4242–4255.\nAssociation for Computational Linguistics.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021b. Towards zero-label language learning. CoRR,\nabs/2109.09193.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTrans. Assoc. Comput. Linguistics, 7:625–641.\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stien-\nnon, Ryan Lowe, Jan Leike, and Paul F. Christiano.\n2021. Recursively summarizing books with human\nfeedback. CoRR, abs/2109.10862.\nSiyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang,\nand Rui Wang. 2023. AD-KD: attribution-driven\nknowledge distillation for language model compres-\nsion. CoRR, abs/2305.10010.\nXing Wu, Shangwen Lv, Liangjun Zang, Jizhong Han,\nand Songlin Hu. 2019. Conditional BERT contex-\ntual augmentation. In Computational Science - ICCS\n2019 - 19th International Conference, Faro, Portu-\ngal, June 12-14, 2019, Proceedings, Part IV, volume\n11539 of Lecture Notes in Computer Science, pages\n84–95. Springer.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey.\n2020. G-daug: Generative data augmentation for\ncommonsense reasoning. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2020, Online Event, 16-20 November 2020, volume\nEMNLP 2020 of Findings of ACL, pages 1008–1025.\nAssociation for Computational Linguistics.\nChangtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng\nLiu, and Dacheng Tao. 2022. On the complementar-\nity between pre-training and random-initialization for\nresource-rich machine translation. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, COLING 2022, Gyeongju, Republic of\nKorea, October 12-17, 2022, pages 5029–5034. Inter-\nnational Committee on Computational Linguistics.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan\nMa, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2022. GLM-\n130B: an open bilingual pre-trained model. CoRR,\nabs/2210.02414.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit BERT. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 509–521. Association\nfor Computational Linguistics.\nLi Zhou, Kevin Small, Oleg Rokhlenko, and Charles\nElkan. 2017. End-to-end offline goal-oriented di-\nalog policy learning via policy gradient. CoRR,\nabs/1712.02838.\nWangchunshu Zhou and Ke Xu. 2020. Learning to com-\npare for better training and evaluation of open domain\nnatural language generation models. In The Thirty-\nFourth AAAI Conference on Artificial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applica-\ntions of Artificial Intelligence Conference, IAAI 2020,\nThe Tenth AAAI Symposium on Educational Advances\nin Artificial Intelligence, EAAI 2020, New York, NY,\nUSA, February 7-12, 2020, pages 9717–9724. AAAI\nPress.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\nSIGIR 2018, Ann Arbor, MI, USA, July 08-12, 2018,\npages 1097–1100. ACM.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul F. Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences. CoRR,\nabs/1909.08593.\n8655\nA Training Details for PPO\nIn this section, we present the training details of\nPPO in Table. The meaning of these hyperparame-\nters is detailed in trl library (von Werra et al., 2020)\nand NLPO (Ramamurthy et al., 2022).\nB Preprocessed Examples\nIn this section, we provide examples of our prepro-\ncessing for each of the data sets we consider.\nB.1 boolq\nOriginal input:\nQuestion: Can you have a skunk as a pet\nin canada\nPassage: Skunks as pets – Canadian pet\nskunks must be purchased from a\nUSDA-certified breeder in the United\nStates. An import permit is required\nfrom the Canadian Food Inspection\nAgency to bring the skunk into the\ncountry. The skunk must be spayed\nor neutered, and receive a microchip\nimplant or tattoo. A vet check fee\nmust also be paid. It is illegal to\nkeep striped skunks as pets in Canada.\nProcessed input: question: Can you have\na skunk as a pet in canada passage:\nSkunks as pets – Canadian pet skunks\nmust be purchased from a USDA-certified\nbreeder in the United States. An import\npermit is required from the Canadian\nFood Inspection Agency to bring the\nskunk into the country. The skunk must\nbe spayed or neutered, and receive a\nmicrochip implant or tattoo. A vet check\nfee must also be paid. It is illegal to\nkeep striped skunks as pets in Canada.\nLabel: {False: 0; True: 1}\nB.2 CB\nOriginal input:\nHypothesis: Valence was helping\nPremise: Valence the void-brain, Valence\nthe virtuous valet. Why couldn’t\nthe figger choose his own portion of\ntitanic anatomy to shaft? Did he think\nhe was helping?\nProcessed input: hypothesis: Valence was\nhelping premise: Valence the void-brain,\nValence the virtuous valet. Why couldn’t\nthe figger choose his own portion of\ntitanic anatomy to shaft? Did he think\nhe was helping?\nLabel: {entailment: 0; contradiction: 1;\nneutral: 2}\nB.3 CoLA\nOriginal input:\nSentence: John made Bill master of\nhimself.\nProcessed input: sentence: John made Bill\nmaster of himself.\nLabel: {unacceptable: 0; acceptable: 1}\nB.4 COPA\nOriginal input:\nQuestion: effect\nPremise: Political violence broke out in\nthe nation.\nChoice 1: Many citizens relocated to the\ncapitol.\nChoice 2: Many citizens took refuge in\nother territories.\nProcessed input: premise: Political\nviolence broke out in the nation.\nchoice1: Many citizens relocated to the\ncapitol. choice2: Many citizens took\nrefuge in other territories. question:\neffect\nLabel: {choice1: 0; choice2: 1}\nB.5 RTE\nOriginal input:\nSentence 1: A smaller proportion of\nYugoslavia’s Italians were settled\nin Slovenia (at the 1991 national\ncensus, some 3000 inhabitants of\nSlovenia declared themselves as ethnic\nItalians).\nSentence 2: Slovenia has 3,000\ninhabitants.\n8656\nHyperparameters WiC CB COPA RTE BoolQ CoLA\nlearning rate 2e-3 2e-3 2e-3 2e-3 2e-3 2e-3\nbatch size 128 64 64 128 128 128\nmin batch size 32 16 16 32 32 32\nepoch 20 100 100 50 10 20\nppo epoch 4 4 4 4 4 4\nsample number 4 4 4 4 4 4\ninitial kl coeff 0.001 0.001 0.001 0.001 0.001 0.001\ntarget kl 6 6 6 6 6 6\nvalue function coeff 0.5 0.5 0.5 0.5 0.5 0.5\nclip ratio 0.2 0.2 0.2 0.2 0.2 0.2\ndiscount factor 0.99 0.99 0.99 0.99 0.99 0.99\ngae lambda 0.95 0.95 0.95 0.95 0.95 0.95\nTable 5: Hyperparameters for PPO\nProcessed input: sentence1: A smaller\nproportion of Yugoslavia’s Italians\nwere settled in Slovenia (at the 1991\nnational census, some 3000 inhabitants\nof Slovenia declared themselves as\nethnic Italians). sentence2: Slovenia\nhas 3,000 inhabitants.\nLabel: {entailment: 0; not_entailment: 1}\nB.6 WiC\nOriginal input:\nPOS: N\nSentence 1: It was the deliberation of\nhis act that was insulting .\nSentence 2: The deliberations of the jury\n.\nWord: deliberation\nProcessed input: sentence1: It was\nthe *deliberation* of his act that\nwas insulting. sentence2: The\n*deliberations* of the jury. word:\ndeliberation\nLabel: {False: 0; True: 1}\nC Faithfulness of the Generated\nKnowledge\nTo further validate the faithfulness of the generated\nknowledge, we gathered five volunteers to assess\nthe faithfulness of the generated texts for datasets\nWiC, COPA, and CoLA. Each text will be given a\nrating from 1 to 5. We randomly sampled 100 texts\nfrom each dataset and calculate the average as the\nWiC COPA CoLA\n4.61 4.36 4.49\nTable 6: The faithfulness score of the generated texts\nfor datasets WiC, COPA and CoLA.\nfinal faithfulness score. The results of the faithful-\nness evaluation are shown in Table 6, showing that\nthe faithfulness of generated texts for each dataset\nis quite high.\n8657",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7853421568870544
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6567371487617493
    },
    {
      "name": "Construct (python library)",
      "score": 0.4705412983894348
    },
    {
      "name": "Knowledge transfer",
      "score": 0.4640217423439026
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4620828628540039
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4619124233722687
    },
    {
      "name": "Language model",
      "score": 0.4544244110584259
    },
    {
      "name": "Inference",
      "score": 0.4392908811569214
    },
    {
      "name": "Transfer of learning",
      "score": 0.42611658573150635
    },
    {
      "name": "Machine learning",
      "score": 0.4123593866825104
    },
    {
      "name": "Natural language processing",
      "score": 0.37798476219177246
    },
    {
      "name": "Data science",
      "score": 0.3637272119522095
    },
    {
      "name": "Knowledge management",
      "score": 0.19757702946662903
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78988378",
      "name": "Renmin University of China",
      "country": "CN"
    }
  ],
  "cited_by": 1
}