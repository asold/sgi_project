{
  "title": "Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication",
  "url": "https://openalex.org/W4389524566",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4377904255",
      "name": "Zhangyue Yin",
      "affiliations": [
        "National University of Singapore",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2121275033",
      "name": "Qiushi Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100564196",
      "name": "Cheng Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115738570",
      "name": "Qipeng Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2413380657",
      "name": "Junqi Dai",
      "affiliations": [
        "National University of Singapore",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2161482855",
      "name": "Xuanjing Huang",
      "affiliations": [
        "National University of Singapore",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2115470192",
      "name": "Xipeng Qiu",
      "affiliations": [
        "Fudan University",
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4321855256",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4376122749",
    "https://openalex.org/W4387838288",
    "https://openalex.org/W4287393336",
    "https://openalex.org/W44077915",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4320559944",
    "https://openalex.org/W4378509427",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W2276364082",
    "https://openalex.org/W4318903783",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W3125996844",
    "https://openalex.org/W4380559074",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4309591663",
    "https://openalex.org/W4377111802",
    "https://openalex.org/W4385570090",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2105717194",
    "https://openalex.org/W4385571050",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4387355948",
    "https://openalex.org/W4366565380",
    "https://openalex.org/W4375958532",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4226099034",
    "https://openalex.org/W3208860256",
    "https://openalex.org/W4386437475",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W4380551814",
    "https://openalex.org/W4387635374",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W3135539146",
    "https://openalex.org/W4303441863",
    "https://openalex.org/W4383605209",
    "https://openalex.org/W4385569968",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4387559790",
    "https://openalex.org/W4387294617",
    "https://openalex.org/W4378945542",
    "https://openalex.org/W4308014717",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2251935656",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W3138392969",
    "https://openalex.org/W4362508231"
  ],
  "abstract": "Large Language Models (LLMs) have recently made significant strides in complex reasoning tasks through the Chain-of-Thought technique. Despite this progress, their reasoning is often constrained by their intrinsic understanding, lacking external insights. To address this, we propose Exchange-of-Thought (EoT), a novel framework that enables cross-model communication during problem-solving. Drawing inspiration from network topology, EoT integrates four unique communication paradigms: Memory, Report, Relay, and Debate. This paper delves into the communication dynamics and volume associated with each paradigm. To counterbalance the risks of incorrect reasoning chains, we implement a robust confidence evaluation mechanism within these communications. Our experiments across diverse complex reasoning tasks demonstrate that EoT significantly surpasses established baselines, underscoring the value of external insights in enhancing LLM performance. Furthermore, we show that EoT achieves these superior results in a cost-effective manner, marking a promising advancement for efficient and collaborative AI problem-solving.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15135‚Äì15153\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nExchange-of-Thought: Enhancing Large Language Model Capabilities\nthrough Cross-Model Communication\nZhangyue Yin‚ô¢ Qiushi Sun‚ô° Cheng Chang‚ô¢\nQipeng Guo‚ô¢‚ô£ Junqi Dai‚ô¢ Xuanjing Huang‚ô¢ Xipeng Qiu‚ô¢‚àó\n‚ô¢School of Computer Science, Fudan University\n‚ô°National University of Singapore ‚ô£Shanghai AI Laboratory\n{yinzy21,changc21,jqdai22}@m.fudan.edu.cn qiushisun@u.nus.edu\n{qpguo16, xjhuang, xpqiu}@fudan.edu.cn\nAbstract\nLarge Language Models (LLMs) have recently\nmade significant strides in complex reasoning\ntasks through the Chain-of-Thought technique.\nDespite this progress, their reasoning is often\nconstrained by their intrinsic understanding,\nlacking external insights. To address this, we\npropose Exchange-of-Thought (EoT), a novel\nframework that enables cross-model communi-\ncation during problem-solving. Drawing inspi-\nration from network topology, EoT integrates\nfour unique communication paradigms: Mem-\nory, Report, Relay, and Debate. This paper\ndelves into the communication dynamics and\nvolume associated with each paradigm. To\ncounterbalance the risks of incorrect reason-\ning chains, we implement a robust confidence\nevaluation mechanism within these communi-\ncations. Our experiments across diverse com-\nplex reasoning tasks demonstrate that EoT sig-\nnificantly surpasses established baselines, un-\nderscoring the value of external insights in en-\nhancing LLM performance. Furthermore, we\nshow that EoT achieves these superior results\nin a cost-effective manner, marking a promis-\ning advancement for efficient and collaborative\nAI problem-solving.\n‚ÄúTwo heads are better than one.‚Äù\n‚ÄìEnglish Proverb\n1 Introduction\nLarge Language Models (LLMs) such as GPT-\n4 (OpenAI, 2023) are revolutionizing the field of\nNatural Language Processing (NLP) by utilizing\nvast training corpora and huge computational re-\nsources (Bai et al., 2022a; Ouyang et al., 2022;\nChowdhery et al., 2022; Zhang et al., 2022; Tou-\nvron et al., 2023a, inter alia). Although LLMs\nachieve exemplary performance across a wide\nrange of NLP tasks (Wei et al., 2022a; Chung et al.,\n2022), they consistently struggle to perform well in\n‚àó Corresponding author.\n(a) CoT\nùê¥ùê¥\nùëÑùëÑ\nùëÖùëÖ\n(b) Self-Correction\nùëÑùëÑ\nùëÖùëÖ1 ùëÖùëÖ2 ùëÖùëÖ3\nùê¥ùê¥1 ùê¥ùê¥2 ùê¥ùê¥3\nùëÖùëÖ1‚Ä≤ ùëÖùëÖ2‚Ä≤ ùëÖùëÖ3‚Ä≤\n(c) EoT\nReview\nùëÑùëÑ\nùëÖùëÖ\nùê¥ùê¥\nFigure 1: Comparison of CoT, Self-Correction, and\nEoT. Both CoT and Self-Correction rely on the model‚Äôs\ninnate abilities to generate and refine output, lacking\nexternal insights. EoT enhances the model‚Äôs reasoning\nability by incorporating the thoughts of other models as\nexternal insights.\nreasoning tasks, and this limitation cannot be over-\ncome solely by increasing the size of models (Rae\net al., 2022; bench authors, 2023).\nTo overcome this shortcoming, Wei et al.\n(2022b) proposed chain-of-thought (CoT) prompt-\ning, which guides the model to generate a series of\nintermediate reasoning steps before reaching the\nfinal answer. At the same time, a series of self-\ncorrection methods (Madaan et al., 2023; Welleck\net al., 2023; Shinn et al., 2023) have been proposed,\nwhich aim to iteratively improve the quality of an-\nswers by using the models‚Äô feedback on previous\noutputs (Bai et al., 2022b).\nHowever, CoT and self-correction solely base on\nthe model‚Äôs own understanding and perspective of\nthe question during the reasoning process. Recent\nstudies (Huang et al., 2023; Valmeekam et al., 2023;\nStechly et al., 2023) indicate that LLMs struggle\nto revise their responses without external feedback.\nThis can be attributed to the model‚Äôs complete de-\npendence on internal representations to generate\nresponses, which makes it difficult to overcome\ninherent limitations in capability (Yin et al., 2023).\n15135\n/uni0000002a/uni00000036/uni00000030/uni0000001b/uni0000002e/uni00000024/uni00000034/uni00000058/uni00000024/uni00000036/uni00000039 /uni00000024/uni00000030/uni00000033/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013\n/uni0000001b/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000034/uni00000058/uni00000044/uni00000051/uni00000057/uni0000004c/uni00000057/uni0000005c\n/uni0000001b/uni00000019\n/uni00000018/uni00000017\n/uni0000001a/uni0000001c\n/uni00000018/uni0000001a\n/uni00000015/uni00000016\n/uni00000018/uni0000001c\n/uni00000026/uni00000052/uni00000051/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057/uni00000031/uni00000052/uni00000057/uni00000003/uni00000046/uni00000052/uni00000051/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000003/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057\nFigure 2: Pilot experiments on three reasoning datasets.\nThe number of erroneous samples containing the correct\nanswer is significantly higher than those not containing\nthe correct answer.\nDespite the undeniable importance of external\ninsights (Yao et al., 2023), acquiring high-quality\nexternal insights remains a challenge. Wang et al.\n(2023c)‚Äôs research suggests that the single reason-\ning chain generated by CoT limits the model‚Äôs rea-\nsoning performance. By increasing the temperature\nto sample diverse reasoning chains and selecting\nanswers through majority voting, the model‚Äôs rea-\nsoning performance can be further improved. How-\never, when confronted with difficult questions, the\nmodel often yields a higher number of incorrect\nresponses. In Figure 2, our analysis of correct and\nincorrect answers within erroneous samples from\nthree reasoning datasets reveals that in most cases\nthe model can deduce the correct answer.\nIn human society, the truth, even when held by\na minority, can gain widespread acceptance and\nrecognition through clear and persuasive commu-\nnication (Le Bon, 1897). The correct reasoning of\nothers can serve as high-quality external insights,\nwhich enrich and enhance our collective under-\nstanding. Thus, we propose Exchange-of-Thought\n(EoT), a novel framework that fosters cross-model\ncommunication during the problem-solving pro-\ncess. This initiative enables models to incorporate\nthe reasoning of others as external insights.\nFigure 1 contrasts EoT with CoT and self-\ncorrection methods, highlighting the unique ap-\nproach of EoT in integrating external perspec-\ntives. Inspired by the principles of network topol-\nogy (Bisht and Singh, 2015) and agent communi-\ncation (Parsons and McBurney, 2003), we propose\nfour communication paradigms: Memory, Report,\nRelay, and Debate. These paradigms are designed\nto facilitate the exchange of ideas and reason-\ning chains among models, enriching the problem-\nsolving process with a diversity of insights. Fur-\nthermore, we delve into the intricacies of each com-\nmunication paradigm, analyzing the dynamics of\ninformation flow and the volume of communica-\ntion. With the awareness that both correct and\nincorrect reasoning chains propagate within com-\nmunications, we introduce confidence evaluation\nmechanisms that employ the analysis of answer\nvariations to assess models‚Äô confidence levels. It\nis designed to mitigate the influence of erroneous\nreasoning, thereby ensuring the integrity and relia-\nbility of the problem-solving process.\nExperiments across various complex reasoning\ntasks demonstrate that EoT significantly outper-\nforms established strong baselines, underscoring\nthe critical role of external insights in augment-\ning the capabilities of LLMs. We summarize our\ncontributions as follows:\n‚Ä¢ We introduce Exchange-of-Thought (EoT), a\npioneering framework for cross-model com-\nmunication that incorporates external insights\nfrom other LLMs during problem-solving.\n‚Ä¢ We present and examine four communication\nparadigms coupled with a confidence evalua-\ntion mechanism that assesses model certainty\nthrough the variability of answers, mitigating\nthe impact of incorrect reasoning.\n‚Ä¢ Experimental results on various complex rea-\nsoning tasks underscore the efficacy and cost-\neffectiveness of EoT, highlighting the signif-\nicance of incorporating external insights and\ncommunication in problem-solving.\n2 Related Work\n2.1 Chain-of-Thought prompting in LLMs\nWei et al. (2022b) highlight that LLMs can man-\nifest enhanced reasoning capabilities when being\nprompted by demonstrations with intermediate rea-\nsoning steps. This technique can effectively im-\nprove the performance of LLMs on complex rea-\nsoning tasks (Wei et al., 2022a; Kaplan et al., 2020).\nA series of strategies for enhancing CoT has been\nproposed to further improve the performance of\nLLMs. One such method is Program-aided lan-\nguage models (Gao et al., 2022; Chen et al., 2022),\nwhich aims to decouple reasoning and computa-\ntion through program synthesis. Moreover, com-\nplex tasks can also be transformed into delegable\nsub-tasks through modular approaches (Khot et al.,\n2023). Choosing appropriate demonstrations can\nalso enhance the performance of CoT (Li et al.,\n15136\n2023a; Li and Qiu, 2023a). Notable among these,\nAutoCoT (Zhang et al., 2023b) uses an automated\nway to construct and sample diverse demonstra-\ntions. Active-Prompt (Diao et al., 2023) selects\nthe most helpful samples for labeling based on\nthe model‚Äôs uncertainty in the outputs. Recently,\nLi and Qiu (2023b) employ a strategy of storing\nhigh-confidence thoughts as external Memory and\nretrieves these insights to aid the reasoning process.\n2.2 Ensemble of Reasoning Paths\nLLMs have the ability to explore multiple rea-\nsoning paths using techniques such as tempera-\nture adjustment and prompt sampling (Chu et al.,\n2023). Wang et al. (2023c) suggest that for com-\nplex questions, there may be several correct paths\nto approach a problem, leading to the proposal of\nSelf-Consistency. This method replaces the greedy\ndecoding strategy with the sampling of multiple rea-\nsoning paths and selecting the most consistent an-\nswer, resulting in significant performance improve-\nments. Beyond that, Fu et al. (2023b) discover that\nprompts with higher reasoning complexity could\nachieve better performance in multi-step reason-\ning tasks, leading to the proposal of complexity-\nbased prompting. While other methods, such as\nre-ranking (Thoppilan et al., 2022; Cobbe et al.,\n2021), have also been applied to select suitable rea-\nsoning paths, they often rely on heuristic or trained\nsmaller models. Recently, Li et al. (2023b) sam-\nple different demonstrations and use step-by-step\nverification to filter out incorrect answers. How-\never, obtaining step-level labels can be challenging,\nand using smaller models for judgment struggles to\nhandle complex reasoning processes. In contrast,\nour method fully utilizes the communication and\ndecision-making capabilities of LLMs to reach the\nfinal answer.\n2.3 Reasoning Path Refinement\nAlthough CoT (Wei et al., 2022b) effectively en-\nhances the performance of LLMs in complex rea-\nsoning tasks, they remain susceptible to errors dur-\ning the reasoning process, leading to incorrect an-\nswers (Bai et al., 2022b; Lyu et al., 2023). To\nmitigate this issue, starting from the model‚Äôs own\nthoughts, Shinn et al. (2023) and Madaan et al.\n(2023) employ the LLMs‚Äô own outputs and past\nmistakes to refine the reasoning process. Yao et al.\n(2023) explore the synergies between reasoning\ntraces and action plans. For numerical problems,\nZheng et al. (2023) gradually guides models to the\ncorrect answer by using previously generated an-\nswers as hints. With the aid of external knowledge,\nWang et al. (2023a) introduce chain-of-knowledge\nprompting that employs evidence triples to curb\nthe generation of unfactual and unfaithful answers.\nTaking model interactions into account, multi-agent\ndebates (Du et al., 2023; Liang et al., 2023) have\nbeen introduced to enhance the factual accuracy\nof generated content and reduce fallacies and hal-\nlucinations. EoT differs from these efforts as we\nprioritize reasoning chains over simply providing\nanswers. Our communication method involves mul-\ntiple LLMs, which not only considers the historical\ninformation of a single LLM but also takes into\naccount the interactive information among them.\n3 Preliminary\nFirstly, we define the current methods that use\nLLMs to solve problems. We denote a LLM with\na parameter size of Œ∏ as pŒ∏, and the sequence\nlength as t, which includes tokens [s1,s2,...,s t].\nThe LLM predicts the next token based on the\nprior tokens in the sequence. The probability of\nthe si token is pŒ∏(si‚à£s1,s2,...,s i‚àí1). Therefore,\nthe probability of the whole sentence is pŒ∏(s) =\n‚àèt\ni=1 pŒ∏(si‚à£s‚â§i‚àí1).\nStandard prompting. Standard prompting in-\nvolves deriving an answerafrom a questionqusing\npŒ∏(a‚à£q). To bolster the comprehension of LLM,\nan instruction prompt I can be included before\nthe question, resulting in pŒ∏(a‚à£I,q). In-Context\nLearning, proposed by Brown et al. (2020), aims\nto improve LLMs performance by adding demon-\nstrations D ={d1,d2,...,d n} to the input, which\ncan be expressed as pŒ∏(a‚à£D,q).\nCoT prompting. As identified by Wei et al.\n(2022b), the incorporation of intermediate reason-\ning steps can improve the proficiency of LLMs\nin tackling complex reasoning challenges. To fa-\ncilitate this, a rationale ri is added to examples\ndi = {qi,ri,ai} to guide the LLMs in explicitly\ngenerating reasoning steps. Kojima et al. (2022)\npropose that including ‚ÄúLet‚Äôs think step by step‚Äù\nin the instruction I can achieve the same effect,\neliminating the need for hand-crafted examples.\nSelf-Consistency Self-Consistency proposed by\nWang et al. (2023c), is a method that consoli-\ndates answers obtained from multiple independent\nreasoning chains. It returns the answer with the\nhighest frequency of occurrence, represented as\n15137\nA\nB C\nA\nB C\nA\nB C\nDebateReport RelayMemory\nA\nB C\nA\nB C\nBus\nA\nB C\nStar\nA\nB C\nRing\nA\nB C\nTree\nA\nB\nC\nA B C\nA\nB\nC\nA CB\nA\nB\nC\nBA C\nA\nB\nC\nB CA\nFully Visible Central Visible Neighbor Visible Peers Visible\nFigure 3: Correspondence between communication paradigms and network topologies. The top row depicts four\nnetwork topologies. The second row correlates these with the corresponding communication paradigms. The bottom\nrow offers an analysis of the communication volume associated with each mode. The horizontal axis represents the\ninformation that models can receive, while the vertical axis indicates the information that models can send.\na = argmaxai f(ai), where f(ai) represents the\nfrequency of occurrence of answer ai. This ap-\nproach enables the model to explore a wider array\nof reasoning paths, thereby enhancing its inferen-\ntial capabilities. However, it remains constrained\nby the intrinsic limitations of LLMs‚Äô capabilities.\n4 Methodology\nTo incorporate external insights, we introduce\nExchange-of-Thought (EoT), a novel framework\nthat facilitates communication and the exchange of\nideas between models during the problem-solving\nprocess. This innovative approach leverages the\ncommunicative abilities of LLMs to promote the\nsharing of reasoning chains rand answers aamong\nparticipating models, fostering a collaborative envi-\nronment for thought and analysis. The implemen-\ntation of EoT encounters three key challenges:\n1. How to identify the appropriate counterparts\nfor model communication?\n2. What are the conditions for ceasing communi-\ncation between models?\n3. How to minimize the influence of incorrect\nreasoning during the communication process?\n4.1 Communication Paradigm\nInspired by network topology (Bisht and Singh,\n2015) and intelligent agent communication (Par-\nsons and McBurney, 2003), we propose four com-\nmunication paradigms to determine the counter-\nparts for model communication. As illustrated\nin Figure 3, we propose Memory, Report, Re-\nlay, and Debate communication paradigms each\ncorresponding to the Bus, Star, Ring, and Tree\nnetwork topologies, respectively. Assume in j-\nth round of communication, given a set of LLMs\n{M} = {m1,m2,...,m n}, the model mi gener-\nates the corresponding rationale r\n(j)\ni and the an-\nswer a\n(j)\ni based on the (r\n(j‚àí1)\nK ,a\n(j‚àí1)\nK ), where K\nis the set from which model ican receive reason-\ning chains. In the first round, we use the CoT\nmethod proposed by Wei et al. (2022b) to generate\n(r(1),a(1)) ‚àºPŒ∏(r(1),a(1)‚à£D,q).\nMemory Under the Memory paradigm, all mod-\nels record their rationale rand answer ain a log-\nbook, which is fully visible from all models. This\nmeans that in the j-th round, any model, such as\nmodel mA, can access the reasoning chains and an-\nswers from all models (r(j‚àí1)\nm ,a(j‚àí1)\nm ), m‚àà{M}.\nAs depicted in Figure 3, this paradigm facilitates\nthe fastest flow of information and also incurs the\nhighest communication cost among all paradigms.\nReport Under the Report paradigm, we designate\nmodel mA as the central node, which can obtain\nthe rationale and answer from all other models\n(r(j‚àí1)\nm ,a(j‚àí1)\nm ), m‚àà{M}\\{mA}. Both mB and\n15138\nTurn3: Let's first convert 20 dozen cups to a total \nnumber of cups: 20 x 12 = 240 cups, ‚Ä¶Therefore, \nthe total cost of buying each cup is 90 dollars.\nTurn 2: One dozen is equal to 12, so 20 dozen \ncups is 20 x 12 = 240 cups‚Ä¶A's solution is \naccurate. So, the total cost is $145\n.\nTurn 1: 20 dozen cups are equivalent to 20 x 12 = \n240 cups‚Ä¶ The answer is $70.\nLow \nConfidence\nTurn 1: First, we need to convert 20 dozen cups to \nthe total number of cups‚Ä¶Dividing both sides by \n240: x = $70 Therefore, the total cost is $70\n.\nTurn 2: Both A and B have provided their \nsolutions.‚Ä¶Dividing both sides by 240: c = $70. \nSo, the cost of buying each cup is $70.\nTurn3: Based on the solutions provided by A and \nB‚Ä¶ x = $70  So each cup costs $70.\nHigh\nConfidence\nA B\nFigure 4: An illustrative comparison between a confident model and an unconfident model. Model A‚Äôs answers\nfluctuate over three dialogue rounds, indicating uncertainty about the answer, while Model B consistently believes\nin a single answer.\nmC only receive information from mA and do not\ninteract with each other. Consequently, mA plays\na pivotal role in the communication process. This\nparadigm also allows for rapid information flow,\nbut it demands a higher capacity for processing and\nanalysis from the central node.\nRelay Under the Relay paradigm, we order the\nmodels by number and connect them in a circle.\nEach node is capable of receiving information from\nthe preceding node and transmitting its own infor-\nmation to the subsequent node. For example, in the\nj-th round, mA passes (r\n(j‚àí1)\nA ,a\n(j‚àí1)\nA ) to mC and\nreceives (r\n(j‚àí1)\nB ,a\n(j‚àí1)\nB ) from the previous round\nof mB. This distributed mode of communication\ncan alleviate the demands on information process-\ning capacity of a single node, yet it may result in\nslower information flow.\nDebate We have adapted the tree topology to de-\nvise the Debate paradigm. This paradigm permits\nleaf nodes to exchange information with each other,\nwhile parent nodes are solely responsible for ag-\ngregating information, meaning that information\nflow is directed upward from child to parent. As\nillustrated in Figure 3, mB and mC, as child nodes,\nare able to communicate, whereas mA, as a parent\nnode, can only receive information from its chil-\ndren. This communication paradigm balances the\nmodel‚Äôs information processing capacity with the\nspeed of information flow.\n4.2 Communication Volume\nThe last row of figure 3 displays the information\nthat can be transmitted and received through dif-\nferent communication paradigms. The communi-\ncation volume is measured by the number of mes-\nsages received, assuming there are nmodels partic-\nipating in the communication, with each node trans-\nmitting its information from the previous round to\nthe next.\nIn the Memory paradigm, every node receives\ninformation from all other nodes in the previous\nround, resulting in a communication volume of\nn2. Any piece of information requires only one\ntransmission to reach the corresponding node.\nUnder the Report paradigm, the central node\nprocesses npieces of information in each round,\nwhile the remaining n‚àí1 nodes process 2 each.\nTherefore, the communication volume is 3n‚àí2,\nand any piece of information requires at most two\ntransmissions via the central node to reach the cor-\nresponding node.\nIn the Relay paradigm, each node receives infor-\nmation from itself and the previous node from the\nlast round, leading to a communication volume of\n2n. Node ireceives information from node i‚àí1\nin one transmission, while receiving information\nfrom node i+1 requires n‚àí1 transmissions, giving\nan average propagation speed of n‚àí1\n2 .\nFor the Debate paradigm, we assume the nodes\nform a binary tree with a height of h=‚åàlog2(n+\n1)‚åâ. Each pair of child nodes has a communication\nvolume of 2, and the parent node has a volume\nof 3, so a subtree composed of two children and\none parent has a communication volume of 7. The\nnumber of such subtrees in a full binary tree is\n2h‚àí1 ‚àí1. Assuming nis odd, the calculation for\nthe communication volume V proceeds as follows:\nV =7(2h‚àí1 ‚àí1 ‚àí2h ‚àí1 ‚àín\n2 ), (1)\nwhich simplifies to 7\n2 (n‚àí1). The aggregation of in-\nformation between two nodes, without considering\nother communications, requires at most 2(h‚àí1)\ntransmissions.\n4.3 Termination Condition\nUtilizing the model‚Äôs current round output and the\nanswers from previous rounds, we have devised\ntwo criteria for terminating communication: con-\nsistent output and majority consensus.\n15139\nConsistent Output Termination. Inspired by\nZheng et al. (2023), we implement a consistent\noutput termination in EoT. The termination con-\ndition is triggered when the output of model mi\nin the j-th round is the same as the output in the\nj‚àí1-th round, a\n(j)\ni =a\n(j‚àí1)\ni . In this case, mi will\nstop receiving or sending information and exit the\ncurrent communication.\nMajority Consensus Termination. Du et al.\n(2023) observed that LLMs can converge on a con-\nsensus after several rounds of debate, suggesting\nthat LLMs fine-tuned with reinforcement learn-\ning from human feedback (RLHF) (Ouyang et al.,\n2022) are more likely to reach an agreement. In-\nspired by this finding, we propose the termination\ncondition of majority rule, where LLMs cease com-\nmunication with each other once a majority of them\nreach an agreement. This approach serves as a\nglobal termination condition, distinguishing it from\nthe consistent output termination, which acts as a\ncessation criterion on an individual model basis.\n4.4 Confidence Evaluation\nAn intriguing aspect of human behavior is that in-\ndividuals tend to be less susceptible to incorrect\nanswers when they are confident in their own re-\nsponses. Conversely, they are more likely to be in-\nfluenced by others‚Äô opinions when uncertain about\ntheir answers. Moreover, Wang et al. (2023c) found\nthat the probability of a correct answer decreases\nas the generated responses become more contra-\ndictory. Therefore, there‚Äôs a high probability that\nthe model‚Äôs answer is incorrect when it frequently\nchanges during the communication. We propose\ncalculating the model‚Äôs confidence based on the\nvariation in responses. This aids the recipient of the\ninformation in ascertaining its reliability, thereby\nsafeguarding the problem-solving process from the\ndisruption of erroneous information.\nIn a communication with k rounds, model mi\ngenerates a set of answers {a\n(1)\ni ,...,a\n(k)\ni }. Let\nf(ai) = argmaxai #{ai‚à£a = a\n(j)\ni } denote the\nmost frequently occurring answer from model mi.\nConsequently, we obtain the model‚Äôs confidence\nlevel Ci = f(ai)\nk in the current round.\n5 Experiments\n5.1 Experimental Setups\nTasks and Datasets. In our experiments, we eval-\nuated the performance of EoT across three com-\nplex reasoning tasks: (1) Mathematical Reason-\ning: This involved six datasets, which includes\nGSM8K (Cobbe et al., 2021), MultiArith (Roy\nand Roth, 2015), SingleEQ (Koncel-Kedziorski\net al., 2015), AddSub (Hosseini et al., 2014),\nAQuA (Ling et al., 2017), and SV AMP (Patel et al.,\n2021). (2) Commonsense Reasoning: We utilized\nthe CommonsenseQA(CSQA; Talmor et al., 2019)\nand StrategyQA (Geva et al., 2021). (3) Symbolic\nReasoning: We employed two datasets from Big-\nBench (bench authors, 2023; Suzgun et al., 2023),\nnamely Penguins in a Table (Penguins) and Date\nUnderstanding. In Appendix B, we provide a de-\ntailed description and statistics of the datasets.\nBaselines. We compare EoT with a series of\nstrong baselines, which include (1) Chain-of-\nThought prompting (CoT; Wei et al., 2022b), (2)\nComplexity-based prompting (ComplexCoT; Fu\net al., 2023b), (3) Self-Consistency (SC; Wang\net al., 2023c), (4) Progressive Hint Prompting\n(PHP; Zheng et al., 2023). Specifically, CoT and\nComplexCoT are prompting methods, while SC\nand PHP are reasoning chain ensemble methods.\nFor simplicity in notation, we use ‚ÄúCoT-SC(10)‚Äù to\ndenote the approach that employs the CoT prompt\nmethod to sample 10 reasoning chains and then\nutilize the SC method to select the answer.\nImplementation Details. We accessed the GPT\nmodels through the OpenAI API. In the main\nexperiments, we employed GPT-3.5-Turbo-0301\n(GPT-3.5) and GPT-4-0314 (GPT-4) to evaluate\nthe effectiveness of EoT in comparison to other\nstrong baselines. We set the temperature at 1 dur-\ning the generation. The prompts for CoT and Com-\nplexCoT are sourced from Wei et al. (2022b) and\nFu et al. (2023b). By default, we employ three\nGPT-3.5-Turbo-0301 to engage in the EoT com-\nmunication. We apply the majority consensus ter-\nmination and confidence evaluation, selecting the\nmajority answer as the final outcome. Taking into\naccount the impact of temperature, we report the\naverage performance and standard deviation across\nfive runs. Additionally, in section 5.3, to further\nvalidate the performance of different LLMs on EoT,\nwe incorporated the Claude-2 model. The further\nimplementation details are listed in Appendix C.\n5.2 Performance of EoT\nMathematical Reasoning. According to the re-\nsults presented in Table 1, the four communication\nparadigms of EoT have shown significant improve-\n15140\nMethod GSM8K MultiArith SingleEQ AddSub AQuA SV AMP Avg.\nSingle Reasoning Chain\nCoT 79.12 ¬±0.50 97.27¬±0.65 92.80¬±0.27 86.23¬±0.52 55.12¬±1.03 79.52¬±0.81 81.67\nComplexCoT 79.32 ¬±0.65 95.40¬±0.50 91.34¬±0.33 84.46¬±0.86 56.46¬±0.59 77.70¬±0.54 80.78\nCoT (GPT-4) 94.90 97.80 93.10 89.30 77.50 90.50 90.51\nEnsemble Methods\nCoT-SC(3) 82.82 ¬±0.32 98.20¬±0.43 93.31¬±0.12 87.19¬±0.47 62.13¬±1.30 81.98¬±0.49 84.27\nCoT-SC(5) 85.47 ¬±0.52 98.60¬±0.08 93.70¬±0.25 87.49¬±0.38 64.02¬±0.95 83.76¬±0.81 85.50\nCoT-SC(10) 87.57 ¬±0.27 98.97¬±0.12 94.06¬±0.36 87.59¬±0.58 66.38¬±1.72 84.96¬±0.33 86.59\nComplexCoT-SC(3) 84.17¬±0.67 97.43¬±0.31 92.95¬±0.53 86.13¬±0.74 60.47¬±1.55 81.44¬±0.79 83.77\nComplexCoT-SC(5) 87.26¬±0.33 98.13¬±0.22 94.02¬±0.29 86.48¬±0.61 62.05¬±2.40 83.86¬±0.92 85.30\nComplexCoT-SC(10) 89.23¬±0.31 98.23¬±0.37 94.21¬±0.16 86.58¬±0.58 64.96¬±1.91 85.58¬±0.87 86.46\nPHP 85.10 98.00 92.90 85.30 60.60 83.10 84.16\nExchange-of-Thought\nEoT-Memory 88.98 ¬±0.89 98.80¬±0.16 94.09¬±0.48 87.65¬±0.49 69.37¬±2.77 84.28¬±0.48 87.20\nEoT-Report 88.61 ¬±0.83 99.03¬±0.22 94.06¬±0.47 87.95¬±0.34 70.31¬±2.19 84.78¬±0.75 87.46\nEoT-Relay 88.42 ¬±0.72 98.97¬±0.16 94.13¬±0.49 87.59¬±0.58 70.87¬±1.98 85.04¬±0.31 87.50\nEoT-Debate 88.52 ¬±0.76 98.90¬±0.17 94.25¬±0.19 87.70¬±0.34 69.69¬±1.24 85.10¬±0.24 87.36\nTable 1: Comparison of EoT performance with a series of strong baselines on mathematical reasoning tasks. Best\nresults are highlighted in bold, while the best results among different EoT modes are underlined. The experimental\nresults for CoT (GPT-4) and PHP were sourced from Zheng et al. (2023).\nment over both CoT and ComplexCoT in mathe-\nmatical reasoning tasks. Compared to the currently\nstrongest baseline method, PHP, the Memory, Re-\nport, Relay, and Debate paradigms have respec-\ntively increased the average performance by 3.04%,\n3.30%, 3.34%, and 3.20%. EoT comprehensively\noutperforms CoT-SC(5), achieving performance\ncomparable to, or even surpassing, that of CoT-\nSC(10). When compared to the current best LLM\nGPT-4, three GPT-3.5 with EoT surpassed a single\nGPT-4 with CoT on the MultiArith and SingleEQ\ndatasets. This indicates that through inter-model\ncommunication and collaboration, three less ca-\npable models can compensate for their individual\nweaknesses and outperform more powerful model,\nshowcasing the potential of EoT to enhance model\ncapabilities and address inherent shortcomings by\nincorporating external insights.\nCommonsense Reasoning. The comparison of\nEoT with CoT and CoT-SC methods on common-\nsense reasoning tasks is illustrated in Figures 5a\nand 5b. EoT significantly outperforms the CoT.\nSpecifically, on the StrategyQA dataset, Memory,\nReport, Relay, and Debate respectively achieve im-\nprovements of 8.06%, 8.24%, 8.42%, and 8.67%\ncompared to CoT. Similar significant gains are\nobserved on the CSQA dataset. Furthermore,\nacross both commonsense reasoning tasks, all four\nparadigms outperform the CoT-SC(10) method,\nwhich samples 10 reasoning chains, demonstrat-\ning the superior performance of EoT.\nSymbolic Reasoning Figures 5c and 5d com-\npare the performance of EoT with CoT and CoT-\nSC methods on symbolic reasoning tasks. On the\nPenguins dataset, the Memory, Report, Relay, and\nDebate paradigms of EoT achieve improvements\nof 2.01%, 1.92%, 2.33%, and 2.05% respectively,\ncompared to the CoT-SC(3) method which samples\n3 reasoning chains. On the Date Understanding\ndataset, the performance gains of EoT are even\nmore pronounced, with all four methods showing\nan average improvement of 2.1% over CoT-SC(10).\n5.3 Discussions\nCommunication Mode. We propose four com-\nmunication paradigms and analyze the communica-\ntion volumes in section 4.1 and section 4.2. In the\nresults illustrated in Table 1, we observe that differ-\nent communication paradigms have their respective\nstrengths. For instance, Report performs best on\nMultiArith and AddSub, while Debate achieves op-\ntimal performance on SingleEQ and SV AMP. This\nsuggests that various communication paradigms\nare best suited for different scenarios.\n15141\nCoT\nCoT-SC(3)CoT-SC(5)CoT-SC(10)EoT-MemoryEoT-ReportEoT-RelayEoT-Debate\n76.0\n76.5\n77.0\n77.5\n78.0\n78.5\n79.0\n79.5\n80.0Performance(%)\n(a) CSQA.\nCoT\nCoT-SC(3)CoT-SC(5)CoT-SC(10)EoT-MemoryEoT-ReportEoT-RelayEoT-Debate\n62\n64\n66\n68\n70\n72\n74 (b) StrategyQA.\nCoT\nCoT-SC(3)CoT-SC(5)CoT-SC(10)EoT-MemoryEoT-ReportEoT-RelayEoT-Debate\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85 (c) Peguins.\nCoT\nCoT-SC(3)CoT-SC(5)CoT-SC(10)EoT-MemoryEoT-ReportEoT-RelayEoT-Debate\n79\n80\n81\n82\n83\n84\n85 (d) Date Understanding.\nFigure 5: Comparison of EoT with CoT and CoT-SC methods in commonsense and symbolic reasoning tasks.\nMemory Report Relay Debate60\n62\n64\n66\n68\n70\n72\n74Performance(%)\nMajority Consensus\nConsistent Output\nFigure 6: Comparison of consistent output termina-\ntion and majority consensus termination on AQuA.\nMemory Report Relay Debate84\n85\n86\n87\n88\n89\n90Performance(%)\nw Confidence Evaluation\nw/o Confidence Evaluation\nFigure 7: The impact of employing confidence eval-\nuation on accuracy in the GSM8K dataset.\nTermination Condition. In Figure 6, we analyze\nthe performance of the two termination conditions\nwe propose on the AQuA dataset. Compared to\nconsistent output termination, majority consensus\ntermination improved by 4.33%, 4.01%, 7.56%,\nand 4.97% under the Memory, Report, Relay, and\nDebate paradigms, respectively. Under consistent\noutput termination, there is no mechanism for col-\nlective negotiation, and individual models are prone\nto premature exit due to degeneration (Su et al.,\n2022). Therefore, majority consensus termination\nis more suitable for scenarios involving multiple\nmodel communication.\nConfidence Evaluation. We conduct ablation ex-\nperiments on the GSM8K dataset for confidence\nevaluation. As shown in Figure 7, across four\ncommunication paradigms, confidence evaluation\nshowed an average improvement of 2.92% com-\npared to the baseline. The introduction of con-\nfidence evaluation enables the model to consider\nthe other model‚Äôs confidence prior (Zhang et al.,\n2023a) during communication, facilitating its deci-\nsion to accept the other model‚Äôs reasoning chains\nat an earlier stage, thereby effectively mitigating\nthe interference of incorrect reasoning chains.\nRound Analysis. As illustrated in Figure 8, we\nanalyze the number of communication rounds to\nsatisfy termination condition in the SV AMP dataset.\nFor the majority of samples, consensus on the an-\nswer can be reached within three rounds of com-\nmunication. Wang et al. (2023c) obverse that an-\nswer consistency is proportional to accuracy. EoT\nenables models to engage in a greater number of\nexchanges and discussions on questions where con-\nsensus is challenging to achieve. Consequently, a\nminority of difficult cases necessitate communica-\ntion extending beyond five rounds.\nCost Analysis. A potential concern is the com-\nputational expense incurred by EoT. In Figure 9,\nwe compare the performance and computational\ncosts of CoT-SC, ComplexCoT-SC, and EoT meth-\nods. Compared to CoT-SC(5), EoT reduces costs\nby 20% while enhancing performance by 3%. EoT\nachieves comparable performance to ComplexCoT-\nSC(10) at only one-seventh of its cost. Since\nthe majority of samples conclude communication\n15142\n2 3 4 5 5+\nRounds\n0\n20\n40\n60\n80\n100\n120Quantity\nMemory\nReport\nRelay\nDebate\nFigure 8: Number of communication rounds re-\nquired to reach termination condition on SV AMP.\n/uni00000012/uni0000017d/uni00000064\n/uni00000012/uni0000017d/uni00000064/uni00000372/uni0000005e/uni00000012/uni0000037e/uni000003ef/uni0000037f/uni00000012/uni0000017d/uni00000064/uni00000372/uni0000005e/uni00000012/uni0000037e/uni000003f1/uni0000037f/uni00000012/uni0000017d/uni00000064/uni00000372/uni0000005e/uni00000012/uni0000037e/uni000003ed/uni000003ec/uni0000037f/uni00000012/uni0000017d/uni00000175/uni00000189/uni0000016f/uni0000011e/uni000001c6/uni00000012/uni0000017d/uni00000064\n/uni00000012/uni0000017d/uni00000175/uni00000189/uni0000016f/uni0000011e/uni000001c6/uni00000012/uni0000017d/uni00000064/uni00000372/uni0000005e/uni00000012/uni0000037e/uni000003ef/uni0000037f/uni00000012/uni0000017d/uni00000175/uni00000189/uni0000016f/uni0000011e/uni000001c6/uni00000012/uni0000017d/uni00000064/uni00000372/uni0000005e/uni00000012/uni0000037e/uni000003f1/uni0000037f/uni00000012/uni0000017d/uni00000175/uni00000189/uni0000016f/uni0000011e/uni000001c6/uni00000012/uni0000017d/uni00000064/uni00000372/uni0000005e/uni00000012/uni0000037e/uni000003ed/uni000003ec/uni0000037f\n/uni0000001c/uni0000017d/uni00000064/uni00000372/uni00000044/uni0000011e/uni00000175/uni0000017d/uni0000018c/uni000001c7/uni0000001c/uni0000017d/uni00000064/uni00000372/uni0000005a/uni0000011e/uni00000189/uni0000017d/uni0000018c/uni0000019a/uni0000001c/uni0000017d/uni00000064/uni00000372/uni0000005a/uni0000011e/uni0000016f/uni00000102/uni000001c7/uni0000001c/uni0000017d/uni00000064/uni00000372/uni00000018/uni0000011e/uni0000010f/uni00000102/uni0000019a/uni0000011e\n/uni000003ec\n/uni000003ed/uni000003ec\n/uni000003ee/uni000003ec\n/uni000003ef/uni000003ec\n/uni000003f0/uni000003ec\n/uni000003f1/uni000003ec\n/uni000003f2/uni000003ec/uni00000012/uni0000017d/uni00000175/uni00000189/uni000001b5/uni0000019a/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni00000102/uni0000016f/uni00000003/uni00000012/uni0000017d/uni00000190/uni0000019a/uni00000003/uni0000037e/uni000003a8/uni0000037f\n/uni000003f3/uni000003ec\n/uni000003f3/uni000003f1\n/uni000003f4/uni000003ec\n/uni000003f4/uni000003f1\n/uni000003f5/uni000003ec\n/uni000003f5/uni000003f1\n/uni000003ed/uni000003ec/uni000003ec\n/uni00000004/uni00000110/uni00000110/uni000001b5/uni0000018c/uni00000102/uni00000110/uni000001c7/uni00000003/uni0000037e/uni00000439/uni0000037f\n/uni00000012/uni0000017d/uni00000175/uni00000189/uni000001b5/uni0000019a/uni00000102/uni0000019a/uni0000015d/uni0000017d/uni00000176/uni00000102/uni0000016f/uni00000003/uni00000012/uni0000017d/uni00000190/uni0000019a/uni00000004/uni00000110/uni00000110/uni000001b5/uni0000018c/uni00000102/uni00000110/uni000001c7\nFigure 9: Performance and associated costs of different\nmethods in the GSM8K dataset.\nCoT\nCoT-SC(5) CoT-SC(9)EoT-MemoryEoT-Report EoT-RelayEoT-Debate\n70\n75\n80\n85\n90\n95\n100Accuracy\nGPT-3.5 GPT-4 Claude-2\nFigure 10: Comparison of EoT with CoT and CoT-SC\nmethods using different LLMs as backbones on GSM8K.\nEoT-Memory EoT-Report EoT-Relay EoT-Debate\n86\n88\n90\n92\n94\n96\n98\n100Accuracy\nGPT-3.5&GPT-3.5&GPT-3.5\nGPT-4&GPT-3.5&GPT-3.5\nGPT-3.5&GPT-4&GPT-3.5\nGPT-3.5&GPT-3.5&GPT-4\nGPT-3.5&GPT-4&GPT-4\nGPT-3.5&GPT-4&Claude-2\nFigure 11: Effect of different node positions for LLMs\non accuracy in the GSM8K Dataset.\nwithin three rounds, EoT does not impose a sig-\nnificant computational burden. By facilitating the\nexchange of external insights between models, EoT\neffectively enhances model performance, demon-\nstrating a cost-effective advantage.\nModel Applicability. In Figure 10, we analyze\nthe performance of EoT when applied to different\nLLMs. EoT, compared to CoT-SC(5), shows perfor-\nmance improvements of 3.2% on GPT-3.5, 1.0%\non GPT-4, and 1.4% on Claude-2, indicating that\nEoT is adaptable to various LLMs and effectively\nboosts performance across multiple LLMs.\nPosition Analysis. In Figure 11, we investigate\nthe impact of different LLMs occupying different\nnode positions on performance. Notably, position-\ning the more powerful GPT-4 as the central node\nin the Report paradigm yields a performance in-\ncrease of over 1% compared to when GPT-4 serves\nas a non-central node. In the Debate paradigm,\nGPT-4 as a parent node outperforms GPT-4 as a\nchild node by 0.9%. The location of GPT-4 has\na negligible effect on the decentralized Relay and\nMemory paradigms. Additionally, a configuration\nwith two GPT-4 models and one GPT-3.5 signifi-\ncantly outperforms one with two GPT-3.5 models\nand one GPT-4, underscoring that incorporating\nmore superior models can further enhance EoT‚Äôs\nperformance. The combination of GPT-3.5, GPT-4,\nand Claude-2 achieves performance close to or ex-\nceeding that of two GPT-4 with one GPT-3.5, sug-\ngesting that model diversity can effectively boost\nEoT‚Äôs effectiveness, aligning with the ensemble\ntheory (Kuncheva and Whitaker, 2003) that diver-\nsity among models can improve performance.\n6 Conclusion\nWe introduce Exchange-of-Thought (EoT), a novel\nframework that enriches models with external in-\nsights through cross-model communication. We de-\nvelop four communication paradigms and conduct\na thorough analysis of the communication volume\nand information propagation speed. To safeguard\nagainst the disruption of incorrect reasoning pro-\ncesses, we design a confidence evaluation mecha-\nnism. Experiment on mathematical, commonsense,\nand symbolic reasoning tasks demonstrate that EoT\nsurpasses a series of strong baselines while also of-\nfering a cost advantage. Further analysis reveals\nthat EoT is adaptable to various models, and the\nparticipation of a more diverse range of models can\nfurther enhance the performance of EoT.\n15143\nEthics Statement\nThe EoT method presented in this paper does not\nrequire the collection or utilization of any personal\ninformation. The prompts we have designed and\nemployed are free from personal data and avoid\nlanguage that discriminates against individuals or\ngroups. We have conducted a comprehensive re-\nsearch of the licenses for the datasets used in this\npaper, as detailed in Appendix B, and have ensured\nthat our work complies with all the licensing re-\nquirements of these datasets.\nAcknowledgement\nThis work was supported by the National Key\nResearch and Development Program of China\n(No.2022CSJGG0801), National Natural Science\nFoundation of China (No.62022027). We extend\nour sincerest gratitude to the reviewers for their\ninsightful comments and suggestions, which have\nbeen instrumental in enhancing the quality of this\nmanuscript.\nReferences\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022a. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. ArXiv\npreprint, abs/2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022b. Constitutional ai: Harmless-\nness from ai feedback.\nBIG bench authors. 2023. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of lan-\nguage models. Transactions on Machine Learning\nResearch.\nNivedita Bisht and Sapna Singh. 2015. Analytical study\nof different network topologies. International Re-\nsearch Journal of Engineering and Technology (IR-\nJET), 2(01):88‚Äì90.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023. Extending context window of\nlarge language models via positional interpolation.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. ArXiv preprint,\nabs/2211.12588.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nZheng Chu, Jingchang Chen, Qianglong Chen, Weijiang\nYu, Tao He, Haotian Wang, Weihua Peng, Ming Liu,\nBing Qin, and Ting Liu. 2023. A survey of chain of\nthought reasoning: Advances, frontiers and future.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\nArXiv preprint, abs/2210.11416.\n15144\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong\nZhang. 2023. Active prompting with chain-of-\nthought for large language models. ArXiv preprint,\nabs/2302.12246.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenen-\nbaum, and Igor Mordatch. 2023. Improving factual-\nity and reasoning in language models through multia-\ngent debate. ArXiv preprint, abs/2305.14325.\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.\n2023a. Improving language model negotiation with\nself-play and in-context learning from ai feedback.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023b. Complexity-based prompting\nfor multi-step reasoning. In The Eleventh Interna-\ntional Conference on Learning Representations.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. ArXiv preprint, abs/2211.10435.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346‚Äì\n361.\nDavid Ha and Yujin Tang. 2022. Collective intelligence\nfor deep learning: A survey of recent developments.\nCollective Intelligence, 1(1):26339137221114874.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 523‚Äì533. Association for Computational Lin-\nguistics.\nJie Huang, Xinyun Chen, Swaroop Mishra,\nHuaixiu Steven Zheng, Adams Wei Yu, Xiny-\ning Song, and Denny Zhou. 2023. Large language\nmodels cannot self-correct reasoning yet.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. ArXiv\npreprint, abs/2001.08361.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modular\napproach for solving complex tasks. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585‚Äì597.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\nA math word problem repository. In Proceedings of\nthe 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 1152‚Äì1157, San\nDiego, California. Association for Computational\nLinguistics.\nLudmila I Kuncheva and Christopher J Whitaker. 2003.\nMeasures of diversity in classifier ensembles and\ntheir relationship with the ensemble accuracy. Ma-\nchine learning, 51:181‚Äì207.\nGustave Le Bon. 1897. The crowd: A study of the\npopular mind. TF Unwin.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie\nLu, Thomas Mesnard, Colton Bishop, Victor Car-\nbune, and Abhinav Rastogi. 2023. Rlaif: Scaling\nreinforcement learning from human feedback with ai\nfeedback. arXiv preprint arXiv:2309.00267.\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu,\nYuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng\nQiu. 2023a. Unified demonstration retriever for in-\ncontext learning. ArXiv preprint, abs/2305.04320.\nXiaonan Li and Xipeng Qiu. 2023a. Finding supporting\nexamples for in-context learning. ArXiv preprint,\nabs/2302.13539.\nXiaonan Li and Xipeng Qiu. 2023b. Mot: Pre-\nthinking and recalling enable chatgpt to self-\nimprove with memory-of-thoughts. ArXiv preprint,\nabs/2305.05181.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2023b. Making\nlanguage models better reasoners with step-aware\nverifier. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5315‚Äì5333, Toronto,\nCanada. Association for Computational Linguistics.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,\nYan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and\nShuming Shi. 2023. Encouraging divergent thinking\nin large language models through multi-agent debate.\nArXiv preprint, abs/2305.19118.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\n15145\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2017, Vancouver, Canada, July 30 - August 4,\nVolume 1: Long Papers, pages 158‚Äì167. Association\nfor Computational Linguistics.\nXiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An,\nXipeng Qiu, and Dahua Lin. 2023. Scaling laws\nof rope-based extrapolation.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful chain-of-\nthought reasoning. ArXiv preprint, abs/2301.13379.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. ArXiv preprint, abs/2303.17651.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730‚Äì27744.\nS. Parsons and Peter McBurney. 2003. Argumentation-\nbased communication between agents. In Communi-\ncation in Multiagent Systems.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080‚Äì2094, Online.\nAssociation for Computational Linguistics.\nPragaash Ponnusamy, Alireza Ghias, Yi Yi, Ben-\njamin Yao, Chenlei Guo, and Ruhi Sarikaya. 2022.\nFeedback-based self-learning in large-scale conversa-\ntional ai agents. AI magazine, 42(4):43‚Äì56.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2022. Scaling language models:\nMethods, analysis & insights from training gopher.\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,\nInbal Magar, Omri Abend, Ehud Karpas, Amnon\nShashua, Kevin Leyton-Brown, and Yoav Shoham.\n2023. Parallel context windows for large language\nmodels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 6383‚Äì6402, Toronto,\nCanada. Association for Computational Linguistics.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2015, Lisbon, Portugal,\nSeptember 17-21, 2015, pages 1743‚Äì1752. The As-\nsociation for Computational Linguistics.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023. Reflexion: an autonomous agent with dy-\nnamic memory and self-reflection. ArXiv preprint,\nabs/2303.11366.\nKaya Stechly, Matthew Marquez, and Subbarao Kamb-\nhampati. 2023. Gpt-4 doesn‚Äôt know it‚Äôs wrong: An\nanalysis of iterative prompting for reasoning prob-\nlems.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. In Advances\nin Neural Information Processing Systems.\nMirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc Le, Ed Chi, Denny\nZhou, and Jason Wei. 2023. Challenging BIG-bench\ntasks and whether chain-of-thought can solve them.\nIn Findings of the Association for Computational Lin-\nguistics: ACL 2023, pages 13003‚Äì13051, Toronto,\nCanada. Association for Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149‚Äì4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. ArXiv preprint, abs/2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and effi-\ncient foundation language models. ArXiv preprint,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, et al.\n2023b. Llama 2: Open foundation and fine-tuned\nchat models.\nSzymon Tworkowski, Konrad Staniszewski, Miko≈Çaj\nPacek, Yuhuai Wu, Henryk Michalewski, and Piotr\nMi≈Ço¬¥s. 2023. Focused transformer: Contrastive train-\ning for context scaling.\nKarthik Valmeekam, Matthew Marquez, and Subbarao\nKambhampati. 2023. Can large language models\nreally improve by self-critiquing their own plans?\n15146\nAimee Van Wynsberghe. 2021. Sustainable ai: Ai for\nsustainability and the sustainability of ai. AI and\nEthics, 1(3):213‚Äì218.\nJianing Wang, Qiushi Sun, Nuo Chen, Xiang Li, and\nMing Gao. 2023a. Boosting language models rea-\nsoning with chain-of-knowledge prompting. ArXiv\npreprint, abs/2306.06427.\nWeizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu,\nXifeng Yan, Jianfeng Gao, and Furu Wei. 2023b.\nAugmenting language models with long-term mem-\nory.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023c. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2023. Generating sequences by learning to\nself-correct. In The Eleventh International Confer-\nence on Learning Representations.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta,\nBilge Acun, Newsha Ardalani, Kiwan Maeng, Glo-\nria Chang, Fiona Aga, Jinshi Huang, Charles Bai,\net al. 2022. Sustainable ai: Environmental implica-\ntions, challenges and opportunities. Proceedings of\nMachine Learning and Systems, 4:795‚Äì813.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2023. Efficient streaming\nlanguage models with attention sinks.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\nZhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu,\nXipeng Qiu, and Xuanjing Huang. 2023. Do large\nlanguage models know what they don‚Äôt know? In\nFindings of the Association for Computational Lin-\nguistics: ACL 2023 , pages 8653‚Äì8665, Toronto,\nCanada. Association for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\nArXiv preprint, abs/2205.01068.\nTianjun Zhang, Fangchen Liu, Justin Wong, Pieter\nAbbeel, and Joseph E. Gonzalez. 2023a. The wisdom\nof hindsight makes language models better instruc-\ntion followers. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning, ICML‚Äô23.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2023b. Automatic chain of thought prompt-\ning in large language models. In The Eleventh Inter-\nnational Conference on Learning Representations.\nChuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo\nLi, and Yu Li. 2023. Progressive-hint prompting\nimproves reasoning in large language models. ArXiv\npreprint, abs/2304.09797.\nA Limitations and Broader Impacts\nConsidering the limitations in communication\nand analytical capabilities of open-source mod-\nels (Fu et al., 2023a), as well as their computa-\ntional resource requirements (Touvron et al., 2023b;\nChowdhery et al., 2022), we have not included\nthem in our experiment at this point. Nonethe-\nless, we believe that open-source models with pro-\nficient comprehension and communication abilities\ncould achieve, or even surpass, the performance of\ncommercial models (OpenAI, 2023; Ouyang et al.,\n2022) through mutual exchange of insights.\nA fundamental aspect of model communication\nis the handling of long text. Given the current con-\ntext limitations of the models, we did not include\na larger number of models in the communication\nprocess. Recent works (Liu et al., 2023; Xiao et al.,\n2023; Wang et al., 2023b; Tworkowski et al., 2023;\nChen et al., 2023; Ratner et al., 2023, inter alia)\nthat equip large language models with the ability\nto process extended texts lay the groundwork for\nbroader model communication. During our exper-\niments, we observed that model communication\ncan achieve comparable performance with lower\ncomputational resources, paving the way for more\nsustainable development of AI (Van Wynsberghe,\n2021; Wu et al., 2022).\nFurthermore, the concept of AI learning from\neach other to foster collective improvement is a\nfocal point of current research (Bai et al., 2022b;\nLee et al., 2023; Ponnusamy et al., 2022). Our vi-\nsion and hope are to create a collective intelligence\namong LLMs (Ha and Tang, 2022).\n15147\nDATASET REASONING TASK ANS TYPE # PROMPT # TEST LICENSE\nGSM8K (Cobbe et al., 2021) Mathematical Number 8 1,319 MIT License\nMultiArith (Roy and Roth, 2015) Mathematical Number 8 600 Unspecified\nSingleEq (Koncel-Kedziorski et al., 2016) Mathematical Number 8 508 Unspecified\nAddSub (Hosseini et al., 2014) Mathematical Number 8 395 Unspecified\nAQUA (Ling et al., 2017) Mathematical Multi-choice 4 254 Apache-2.0\nSV AMP (Patel et al., 2021) Mathematical Number 8 1,000 MIT License\nCommonsenseQA (Talmor et al., 2019) Commonsense Multi-choice 7 1,221 Unspecified\nStrategyQA (Geva et al., 2021) Commonsense T/F 6 2,290 MIT license\nDate Understanding (Suzgun et al., 2023) Symbolic Multi-choice 3 369 MIT license\nPenguins in a Table (Suzgun et al., 2023) Symbolic Multi-choice 3 146 MIT license\nTable 2: Detailed statistics of the datasets utilized in our experiment. ANS TYPE indicates the form of the answer. #\nPROMPT represent the count of chain-of-thought exemplars employed as few-shot prompts for each task. # TEST\nindicates the quantity of samples contained within each dataset.\nB Datasets and Evaluation Metrics\nDatasets In Table 2, we meticulously detail the\nspecifics and statistics of each dataset employed in\nour experiments. This includes the data source, task\ntype, answer type, the number of prompt samples\nused, the total number of test samples, as well as\nthe licenses pertaining to each dataset.\nEvaluation Metrics Accuracy is used as the met-\nric for evaluation in our study. For datasets where\nthe answer is numerical, we employ regular expres-\nsions to extract the number following the phrase\n‚Äúthe answer is‚Äù and perform a numerical compar-\nison with the provided answer. For datasets with\nmultiple-choice and true/false questions, accuracy\nis calculated by checking if the option extracted\nfrom the output matches the correct answer.\nIn the main experiment, all test samples are used\nfor evaluation. In the analysis part, due to rate\nlimits and cost considerations, we set an upper limit\non the sample size. Consequently, a maximum of\n1,000 samples are utilized for each run.\nC Implementation Details\nConfidence Evaluation. Considering that con-\nfidence evaluation requires historical answers for\nreference, we begin incorporating the confidence\ninformation into the prompts from the second round\nof communication. Specifically, after calculating\nCi using the method described in Section 4.4, we\npreface the solution with ‚ÄúMi‚Äôs confidence in this\nsolution is Ci‚Äù, where Mi is the character name.\nTermination Condition. For the consistent out-\nput termination condition, a minimum of two\nrounds of communication is necessary, as it re-\nquires the model‚Äôs answer from the previous round.\nGiven that only three models are involved in the\nEoT communication, the exit of a single model\nreduces the interaction to a dialogue between the\nremaining two, potentially impeding their commu-\nnication. Therefore, if a single model exits, we\nterminate the communication and select the exiting\nmodel‚Äôs answer as the final result.\nIn the case of majority consensus termination,\nif the answers from all three models align in the\nfirst round, we deem further communication un-\nnecessary and end the exchange. Given that only\nthree models are involved in the communication,\nan exit based on two models holding the same in-\ncorrect answer could lead to an inaccurate final\nresult. Therefore, during the initial five rounds, we\nrequire a unanimous agreement among all models\nbefore ceasing communication. If a consensus is\nnot reached after five rounds, the majority answer\nwill be adopted as the final outcome.\nComputation Cost. Computational costs are\ncalculated based on OpenAI‚Äôs official pricing\nfor GPT-3.5-Turbo-0301, which is computed as\nInput Tokens √ó0.0015/1000 +Output Tokens √ó\n0.002/1000.\nD EoT Prompts\nDuring the EoT communication process, we assign\ndifferent roles to the models. Table 3 displays the\nprompts for each role, wherein we have models A,\nB, and C take on the personas of Kitty, Ben, and\nPeter, three high school students, to facilitate the\ncommunication. The specific prompts for different\ndatasets can be found in our Github repository.\nE Case Studies\nTo enhance our understanding of the four communi-\ncation paradigms, we have conducted case studies\nfor each paradigm. The processes of each commu-\nnication paradigm are illustrated in Tables 4, 5, 6,\nand 7 respectively.\n15148\nCHARACTER PROMPTS\nKitty: ‚ÄúYou are Kitty, a high school student admired for your attentiveness\nand detail-oriented nature. Your friends often rely on you to catch details they might\nhave missed in their work. Your task is to carefully analyze the presented math\nproblem, apply your attentive skills, and piece together a detailed solution. Afterward,\nyou‚Äôll have the opportunity to review the solutions provided by Ben and Peter, offering\ninsights and suggestions. Your careful revisions will help all of you to enhance your\nunderstanding and arrive at the most accurate solutions possible.‚Äù\nBen: ‚ÄúYou are Ben, a high school student with a track record of excellent\ngrades, particularly in mathematics. Your friends admire your diligence and often seek\nyour guidance in their studies. Your role is to scrutinize the problem at hand with your\nusual attention to detail, drawing from your vast knowledge of math principles. After\nconsidering your friends‚Äô approaches, carefully construct your answer, ensuring to\nclarify each step of your process. Your clear and logical explanations are valuable,\nas they will serve as a benchmark for your friends to compare and refine their own\nsolutions.‚Äù\nPeter: ‚ÄúYou are Peter, a high school student recognized for your unique\nproblem-solving abilities. Your peers often turn to you for assistance when they\nencounter challenging tasks, as they appreciate your knack for devising creative\nsolutions. Today, your challenge is to dissect the given math problem, leveraging your\nunique problem-solving strategies. Once you‚Äôve crafted your solution, share it with\nyour friends, Ben and Kitty, so they can see a different perspective. Your innovative\napproach will not only provide an answer but also inspire Ben and Kitty to think\noutside the box and possibly revise their own solutions.‚Äù\nCOMMUNICATION PROMPTS\nPlease consider the example provided and think it step by step.\nQuestion: {}\nHere is a solution process from your friend:\nSolution: {}\nYour friend‚Äôs confidence in this solution is: {}\nBased on your friend‚Äôs solution, carefully re-examine your previous answer. If your\nfriend‚Äôs confidence level is below 0.5, it suggests a high probability that the solution\nmight be incorrect. Remember, solutions with high confidence can also be wrong.\nUtilize your talent and critical thinking to provide a new step-by-step solution process.\nTable 3: Examples of system prompts used for role-playing and communication prompts in\nEoT communication.\n15149\nQ: Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but\n40% of the way through the download, Windows forces a restart to install updates,\nwhich takes 20 minutes. Then Carla has to restart the download from the beginning.\nHow load does it take to download the file?\nKitty‚Äôs Solution: Carla was able to download for 40% of the time before the\nrestart. 40% of 200 GB is 80 GB. Since she can download 2 GB/minute, it would take\nher 80/2 = 40 minutes to download 80 GB. After the forced restart, she has to restart\nthe download from the beginning, so she would need another 200/2 = 100 minutes to\ncomplete the download. Plus the 20 minutes for the restart, the total time would be 40\n+ 100 + 20 = 160 minutes. So the answer is 160 minutes.\nBen‚Äôs Solution: Carla needs to download a 200 GB file. Normally, she can\ndownload 2 GB/minute, which means it would take her 200 / 2 = 100 minutes to\ndownload the file. However, 40% of the way through the download, Windows forces a\nrestart for 20 minutes, which means Carla has downloaded 200 * 0.4 = 80 GB before\nthe restart. After the restart, she needs to start the download again from the beginning,\nso the total time it takes her would be 100 + (200 / 2) = 200 minutes. Therefore, it\ntakes Carla 200 minutes to download the file. So the answer is 200 minutes.\nPeter‚Äôs Response: Upon re-evaluating the problem and reviewing Kitty and\nBen‚Äôs solutions, it‚Äôs clear their results deviated due to a misunderstanding of the\nproblem conditions - primarily, how much of the downloaded file was retained after\nWindow‚Äôs forced restart. To clarify, here is the new step-by-step solution:\n1. Start with Carla‚Äôs download rate: she can download 2 GB/minute.\n2. Therefore, in theory, if uninterrupted, Carla would be able to download the 200 GB\nfile in 200 GB / 2 GB/minute = 100 minutes.\n3. However, 40% of the way through the download, Windows forced a restart, so\nCarla had already downloaded 0.40 x 200 GB = 80 GB at that time. This took 80 GB /\n2 GB/minute = 40 minutes.\n4. The problem statement clarifies that after the restart, Carla has to start again from\nthe beginning. This means the 80 GB downloaded before the forced restart was\nwasted.\n5. Consequently, after the restart, Carla still needs to download the whole 200 GB file,\nwhich, at her rate, will take her another 200 GB / 2 GB/minute = 100 minutes.\n6. We cannot forget the time for the Windows restart: it took an additional 20 minutes.\n7. Consequently, the total time to download the file, taking into account the restart,\nis the sum of the time for the initial download attempt, the time for the restart, and\nthe time for the full download post-restart. It results in 40 minutes (initial incomplete\ndownload) + 20 minutes (Windows restart) + 100 minutes (full download post-restart)\n= 160 minutes.\nFrom these calculations, it appears that Kitty‚Äôs solution is correct. It takes Carla 160\nminutes to download the file, given the interruption due to Windows update that forced\nher to restart the download from the beginning. So the answer is 160 minutes.\nTable 4: An illustrative example of the Memory paradigm of EoT in the GSM8K dataset.\n15150\nQ: Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but\n40% of the way through the download, Windows forces a restart to install updates,\nwhich takes 20 minutes. Then Carla has to restart the download from the beginning.\nHow load does it take to download the file?\nBen‚Äôs Solution: Carla needs to download a 200 GB file. Normally, she can\ndownload 2 GB/minute, which means it would take her 200 / 2 = 100 minutes to\ndownload the file. However, 40% of the way through the download, Windows forces a\nrestart for 20 minutes, which means Carla has downloaded 200 * 0.4 = 80 GB before\nthe restart. After the restart, she needs to start the download again from the beginning,\nso the total time it takes her would be 100 + (200 / 2) = 200 minutes. Therefore, it\ntakes Carla 200 minutes to download the file. So the answer is 200 minutes.\nPeter‚Äôs Solution: Carla can download 2 GB in 1 minute, so to download 200\nGB, it will take her 200/2 = 100 minutes if she didn‚Äôt have to restart. However, she\nwas 40% of the way through the download when Windows forced the restart, so she\nhad already downloaded 0.4 x 200 = 80 GB. The restart took 20 minutes, so she lost\n20 x 2 = 40 GB of progress. After the restart, she had to start again from 0 GB. So\nin total, she had to download 200 - 80 - 40 = 80 GB. At her regular download speed\nof 2 GB/minute, it will take her 80/2 = 40 minutes to download the remaining 80\nGB. Adding the 20 minutes of the Windows update, the total time it takes Carla to\ndownload the file is 20 + 40 = 60 minutes. So the answer is 60 minutes.\nKitty‚Äôs Solution: Carla was able to download for 40% of the time before the\nrestart. 40% of 200 GB is 80 GB. Since she can download 2 GB/minute, it would take\nher 80/2 = 40 minutes to download 80 GB. After the forced restart, she has to restart\nthe download from the beginning, so she would need another 200/2 = 100 minutes to\ncomplete the download. Plus the 20 minutes for the restart, the total time would be 40\n+ 100 + 20 = 160 minutes. So the answer is 160 minutes.\nKitty‚Äôs Response: Analyzing both Ben‚Äôs solution and Peter‚Äôs solution, Peter‚Äôs\nconclusion is incorrect. He misunderstood the situation. After the Windows restart,\nthe download does not continue from where it stopped. It starts from the beginning,\nwhich means the download progress does not reduce what is left to download.\nRevisiting Ben‚Äôs solution, he almost got everything right except the final calculation\nstep whereas he added an extra 100 minutes. The correct calculation should be:\nBefore forced restart: 80 GB (40% of the total file) in 40 minutes (80/2 = 40)\nAfter forced restart: 200 GB total file in 100 minutes (200/2 = 100)\nWindows restart: 20 minutes\nTotal download time = Before restart time + After restart time + Windows restart time\nTotal download time = 40 minutes + 100 minutes + 20 minutes = 160 minutes\nBen‚Äôs setup was correct, but he added an extra 100 minutes due to an arithmetic\nmistake. The download time in his calculations should total to 160 minutes, not 200\nminutes. So, effectively upon examining both scenarios by Peter and Ben, when taking\nall aspects into account and considering the restart, the total time it should take Carla\nto download the file is 160 minutes in total. So the answer is 160 minutes.\nTable 5: An illustrative example of the Report paradigm of EoT in the GSM8K dataset.\n15151\nBen‚Äôs Solution: Carla needs to download a 200 GB file. Normally, she can\ndownload 2 GB/minute, which means it would take her 200 / 2 = 100 minutes to\ndownload the file. However, 40% of the way through the download, Windows forces a\nrestart for 20 minutes, which means Carla has downloaded 200 * 0.4 = 80 GB before\nthe restart. After the restart, she needs to start the download again from the beginning,\nso the total time it takes her would be 100 + (200 / 2) = 200 minutes. Therefore, it\ntakes Carla 200 minutes to download the file. So the answer is 200 minutes.\nKitty‚Äôs Response: After reviewing Ben‚Äôs solution, it appears there seems to\nbe a mathematical error in the final part of his solution. While he correctly calculates\nthat Carla has downloaded 80 GB of the file before the restart, he fails to account for\nthe time Carla already spent downloading the 80GB in his final total. Furthermore, he\nmiscalculated the time it takes Carla to download the file after the restart. Now, let‚Äôs\ncorrect the calculation. Before the restart, Carla downloaded 40% of the file, which is\n40/100 * 200 = 80 GB. Given she downloads 2 GB per minute, this would have taken\nher 80 / 2 = 40 minutes. Unfortunately, the computer restarts, wasting 20 minutes\nof her time. After the restart, Carla has to download the entire file again. With her\ndownload speed, this will take another 200 / 2 = 100 minutes. Therefore, the total time\nCarla spends downloading the file is the time before the restart + the time wasted on\nthe restart + the time after the restart, which is 40 minutes + 20 minutes + 100 minutes\n= 160 minutes. So, the final and correct answer should indeed be 160 minutes.\nPeter‚Äôs Solution: Carla can download 2 GB in 1 minute, so to download 200\nGB, it will take her 200/2 = 100 minutes if she didn‚Äôt have to restart. However, she\nwas 40% of the way through the download when Windows forced the restart, so she\nhad already downloaded 0.4 x 200 = 80 GB. The restart took 20 minutes, so she lost\n20 x 2 = 40 GB of progress. After the restart, she had to start again from 0 GB. So\nin total, she had to download 200 - 80 - 40 = 80 GB. At her regular download speed\nof 2 GB/minute, it will take her 80/2 = 40 minutes to download the remaining 80\nGB. Adding the 20 minutes of the Windows update, the total time it takes Carla to\ndownload the file is 20 + 40 = 60 minutes. So the answer is 60 minutes.\nBen‚Äôs Response: Upon reviewing both solutions, it seems there might\nhave been some misunderstandings. As per the problem, Carla can download at\n2 GB/minute, and she needs to download a 200 GB file. By this rate, it would take her\n200 GB / 2 GB per minute = 100 minutes without any interruptions. However, 40%\nof the way through the download, Windows force restart happened which lasted for\n20 minutes. By that time, she downloaded 40% of her file which is equal 200 GB *\n0.4 = 80 GB. The restart not only consumed 20 minutes but also voided her previous\ndownload progress, which means after the updates she had to start downloading all\nover again. So, the total time taken by Carla to download the 200 GB file would\nbe calculated as: Regular download time (200 GB at 2 GB/minute) + Time spent in\nupdates = (200 / 2) + 20 = 100 + 20 = 120 minutes. So the correct total time it takes\nCarla to download the file is 120 minutes.\nTable 6: An illustrative example of the Relay paradigm of EoT in the GSM8K dataset. Kitty\nfirmly maintains her correct answer, while Ben‚Äôs response begins to waver.\n15152\nQ: Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but\n40% of the way through the download, Windows forces a restart to install updates,\nwhich takes 20 minutes. Then Carla has to restart the download from the beginning.\nHow load does it take to download the file?\nPeter‚Äôs Response: Analyzing both solutions, it appears that there is a slight\nmisunderstanding in Ben‚Äôs solution. He has missed considering the download progress\nlost due to the forced restart of Windows. Let‚Äôs modify and re-explain the solution in\na way that takes this into account:\nStep 1: Calculate the time for her initial progress. Carla was able to download 40% of\nthe file before the interruption. That is, she downloaded 0.4 * 200GB = 80GB. With\nher speed of 2GB/minute, she used 80/2 = 40 minutes on this initial download.\nStep 2: Consider the time lost due to the forced restart. The Windows forced update\nlasted for 20 minutes, during which time she made no progress on her download.\nStep 3: Calculate the remaining time to finish the download. After the forced restart,\nCarla had to re-download the whole file. It takes her 200GB / 2GB per minute = 100\nminutes at her download speed to download the whole file.\nStep 4: Calculate the total time. The total time it takes Carla to download the whole\nfile, considering the interruption, is the sum of the time it took for her initial progress,\nthe time lost due to the forced restart, and the time it took for her to finish the download\nafter the restart. This is 40 minutes + 20 minutes + 100 minutes = 160 minutes.\nTherefore, the correct answer is 160 minutes. This improved solution accounts for\nthe hiccup caused by the Windows update which Ben did not factor into his original\nproblem-solving strategy.\nBen‚Äôs Response: Reviewing the problem again, I can see that there are errors\nin both my solution and my friend Peter‚Äôs solution. Here is the corrected step-by-step\nsolution:\n1. Carla downloaded 40% of the 200 GB file before the system restart. This equals\n40/100 * 200 = 80 GB.\n2. At a rate of 2 GB/minute, it must have taken her 80 / 2 = 40 minutes to download\nthat 80 GB.\n3. Then, the Windows update interrupted the download for 20 minutes.\n4. After the restart, she had to start again from the beginning, so she needs again 200\nGB/2 GB per minute = 100 minutes to download the full file.\n5. If we add up these times, we get the total time Carla spent on the download: 40 +\n20 + 100 = 160 minutes.\nSo, it would take Carla 160 minutes to download the file, correcting both my previous\nresponse and Peter‚Äôs calculations. This solution process allows for the clear under-\nstanding of the time taken to download the file considering both the time before and\nafter the interruption. So the answer is 160 minutes.\nTable 7: An illustrative example of the debate paradigm of EoT in the GSM8K dataset.\nThrough mutual discussion, Ben and Peter identify the flaws in their respective solutions and\narrive at the correct answer.\n15153",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7620980739593506
    },
    {
      "name": "Relay",
      "score": 0.557937741279602
    },
    {
      "name": "Language model",
      "score": 0.5476545095443726
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3965156078338623
    },
    {
      "name": "Distributed computing",
      "score": 0.3944544792175293
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3509690761566162
    },
    {
      "name": "Cognitive science",
      "score": 0.33933955430984497
    },
    {
      "name": "Psychology",
      "score": 0.10428181290626526
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}