{
    "title": "DSGPT: Domain-Specific Generative Pre-Training of Transformers for Text Generation in E-commerce Title and Review Summarization",
    "url": "https://openalex.org/W3153281824",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5100445969",
            "name": "Xueying Zhang",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A5073032098",
            "name": "Yunjiang Jiang",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A5063856862",
            "name": "Yue Shang",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A5022057886",
            "name": "Zhaomeng Cheng",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A5100458254",
            "name": "Chi Zhang",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A5080616233",
            "name": "Xiaochuan Fan",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A5101419454",
            "name": "Yun Xiao",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        },
        {
            "id": "https://openalex.org/A5101920986",
            "name": "Bo Long",
            "affiliations": [
                "Silicon Valley Community Foundation"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2971246534",
        "https://openalex.org/W6675247125",
        "https://openalex.org/W2952138241",
        "https://openalex.org/W2888103369",
        "https://openalex.org/W6642116818",
        "https://openalex.org/W2988937804",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2996766022",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2118119027",
        "https://openalex.org/W2028339364",
        "https://openalex.org/W2101390659",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4206277161",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2574535369",
        "https://openalex.org/W2962996600",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1967082914"
    ],
    "abstract": "We propose a novel domain-specific generative pre-training (DS-GPT) method\\nfor text generation and apply it to the product titleand review summarization\\nproblems on E-commerce mobile display.First, we adopt a decoder-only\\ntransformer architecture, which fitswell for fine-tuning tasks by combining\\ninput and output all to-gether. Second, we demonstrate utilizing only small\\namount of pre-training data in related domains is powerful. Pre-training a\\nlanguagemodel from a general corpus such as Wikipedia or the CommonCrawl\\nrequires tremendous time and resource commitment, andcan be wasteful if the\\ndownstream tasks are limited in variety. OurDSGPT is pre-trained on a limited\\ndataset, the Chinese short textsummarization dataset (LCSTS). Third, our model\\ndoes not requireproduct-related human-labeled data. For title summarization\\ntask,the state of art explicitly uses additional background knowledgein\\ntraining and predicting stages. In contrast, our model implic-itly captures\\nthis knowledge and achieves significant improvementover other methods, after\\nfine-tuning on the public Taobao.comdataset. For review summarization task, we\\nutilize JD.com in-housedataset, and observe similar improvement over standard\\nmachinetranslation methods which lack the flexibility of fine-tuning.\\nOurproposed work can be simply extended to other domains for a widerange of\\ntext generation tasks.\\n",
    "full_text": null
}