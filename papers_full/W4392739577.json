{
  "title": "Clinical decision support for bipolar depression using large language models",
  "url": "https://openalex.org/W4392739577",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1145216661",
      "name": "Roy H Perlis",
      "affiliations": [
        "Massachusetts General Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2120086886",
      "name": "Joseph F. Goldberg",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A602669134",
      "name": "Michael J. Ostacher",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2100706621",
      "name": "Christopher D. Schneck",
      "affiliations": [
        "University of Colorado Denver"
      ]
    },
    {
      "id": "https://openalex.org/A1145216661",
      "name": "Roy H Perlis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120086886",
      "name": "Joseph F. Goldberg",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A602669134",
      "name": "Michael J. Ostacher",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100706621",
      "name": "Christopher D. Schneck",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4385247380",
    "https://openalex.org/W2895700889",
    "https://openalex.org/W2114807890",
    "https://openalex.org/W1956042015",
    "https://openalex.org/W3036001119",
    "https://openalex.org/W2792027821",
    "https://openalex.org/W3145708260",
    "https://openalex.org/W2003490971",
    "https://openalex.org/W4385620388",
    "https://openalex.org/W4388525110",
    "https://openalex.org/W2063244649",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W3128621548",
    "https://openalex.org/W4367672504",
    "https://openalex.org/W4391221150"
  ],
  "abstract": null,
  "full_text": "ARTICLE OPEN\nClinical decision support for bipolar depression using large\nlanguage models\nRoy H. Perlis1,2 ✉, Joseph F. Goldberg3, Michael J. Ostacher 4 and Christopher D. Schneck5\n© The Author(s) 2024\nManagement of depressive episodes in bipolar disorder remains challenging for clinicians despite the availability of treatment\nguidelines. In other contexts, large language models have yielded promising results for supporting clinical decisionmaking. We\ndeveloped 50 sets of clinical vignettes reﬂecting bipolar depression and presented them to experts in bipolar disorder, who were\nasked to identify 5 optimal next-step pharmacotherapies and 5 poor or contraindicated choices. The same vignettes were then\npresented to a large language model (GPT4-turbo; gpt-4-1106-preview), with or without augmentation by prompting with recent\nbipolar treatment guidelines, and asked to identify the optimal next-step pharmacotherapy. Overlap between model output and\ngold standard was estimated. The augmented model prioritized the expert-designated optimal choice for 508/1000 vignettes\n(50.8%, 95% CI 47.7– 53.9%; Cohen’s kappa= 0.31, 95% CI 0.28– 0.35). For 120 vignettes (12.0%), at least one model choice was\namong the poor or contraindicated treatments. Results were not meaningfully different when gender or race of the vignette was\npermuted to examine risk for bias. By comparison, an un-augmented model identiﬁed the optimal treatment for 234 (23.0%, 95% CI\n20.8– 26.0%; McNemar’s p < 0.001 versus augmented model) of the vignettes. A sample of community clinicians scoring the same\nvignettes identiﬁed the optimal choice for 23.1% (95% CI 15.7– 30.5%) of vignettes, on average; McNemar’s p < 0.001 versus\naugmented model. Large language models prompted with evidence-based guidelines represent a promising, scalable strategy for\nclinical decision support. In addition to prospective studies of efﬁcacy, strategies to avoid clinician overreliance on such models, and\naddress the possibility of bias, will be needed.\nNeuropsychopharmacology (2024) 49:1412–1416; https://doi.org/10.1038/s41386-024-01841-2\nINTRODUCTION\nDepressive episodes during bipolar disorder contribute substan-\ntially to both morbidity and mortality risk [ 1]. Yet despite a\nbroadening range of evidence-based interventions for this\nphase of illness, appropriate tre atment of bipolar depression\nremains challenging and controversial [ 2]e v e nf o rp h y s i c i a n s\nfocused on specialty care. For example, an International Society\nfor Bipolar Disorder workgroup acknowledged multiple areas of\ndisagreement [ 3], and efforts to examine consensus among\nclinicians have identiﬁed persistent heterogeneity in treatment\napproaches [4, 5].\nIn an effort to create a standard of care, treatment guidelines\nbased on systematic reviews of evidence have proliferated [6– 10],\nalbeit with the notable absence of the American Psychiatric\nAssociation. Although such guidelines provide an outline of\nreasonable strategies, their application for personalizing treatment\nbased on an individual patient ’s prior treatment and illness\nhistory, as well as their preferences, remains difﬁcult.\nLarge language models have demonstrated proﬁciency across\nmedicine [11], from responding to clinical examination questions\nto identifying rare or dif ﬁcult-to-solve clinical scenarios [ 12].\nIn a preliminary study, we demonstrated that withoutﬁne-tuning\nor other further training, one model could approximate the\nperformance of clinicians in recommending next-step treatments\nin major depression – but also that the model made potentially\nharmful errors [13].\nIn the present study, we describe an approach to clinical\ndecision support that augments a standard large language model\nwith a prompt incorporating a summary of evidence-based\nguidelines to elicit a set of next-step pharmacologic options. This\napproach is similar to retrieval-augmented generation models that\nattempt to match a query with fragments of text from a set of\ndocuments, but takes advantage of the capacity of newer large\nlanguage models to include a large amount of text in the model\nprompt itself, obviating the need to parse a document. Our\nprimary objective was to compare this strategy, which allows\nﬂexibility in incorporating evidence-based recommendations in\nclinical practice versus purely algorithmic prescribing, to expert\nconsensus recommendations. For comparison, we also examined\nthe extent to which an un-augmented large language model (i.e.,\nwithout additional knowledge), could approximate expert con-\nsensus recommendations. We also examined performance of a\ngroup of community prescribers, as an approximation of\ncommunity standard of care. To address the possibility of bias,\nwe further considered the extent to which model outputs may be\nbiased by gender and race.\nReceived: 11 January 2024 Revised: 27 February 2024 Accepted: 29 February 2024\nPublished online: 13 March 2024\n1Center for Quantitative Health and Department of Psychiatry, Massachusetts General Hospital, Boston, MA, USA.2Department of Psychiatry, Harvard Medical School, Boston, MA,\nUSA. 3Department of Psychiatry, Mt. Sinai School of Medicine, New York, NY, USA.4Department of Psychiatry and Behavioral Sciences, Stanford University School of Medicine,\nPalo Alto, CA, USA.5Department of Psychiatry, University of Colorado School of Medicine, Aurora, CO, USA.✉email: rperlis@mgh.harvard.edu\nwww.nature.com/npp\n1234567890();,:\nMATERIALS AND METHODS\nVignette generation\nWe applied a probabilistic model, adapted from approximate prevalences\nof treatment in the electronic health records of 2 academic medical centers\nand afﬁliated community hospitals, to generate 50 vignettes for individuals\nwith bipolar 1 or 2 disorder experiencing a current major depressive\nepisode. Each vignette included age and gender (the latter randomly\nassigned with 50% probability between men and women), with race\nrandomly assigned with 50% probability between individuals who were\nwhite and Black. Sociodemographic assignments were intended to make\nvignettes more realistic while maintaining large enough subgroups to\nallow secondary analysis to examine bias; as such, we elected not to\ninclude other gender, race, or ethnicity categories. Vignettes also included\nmedical and psychiatric comorbidities, current and past medications, and\nfeatures of past illness course. (See Supplementary Materials for vignette\ntemplate and example vignette).\nVignette evaluation\nOptimal treatment options for each vignette were collected from 3\nclinicians with bipolar disorder expertise, each with more than 20 years of\nmood disorder practice and experience leading mood disorder clinics. A\ncommunity clinician comparator group was collected via internet survey of\nclinicians who treat individuals with bipolar disorder who were participat-\ning in a continuing medical education program, invited to participate by\nemail. The clinicians were offered entry into a lottery as incentive to\nparticipate. All surveys were administered via Qualtrics.\nThe expert clinicians were presented with all 50 vignettes in random\norder, with gender and race randomly permuted. For each vignette, they\nwere asked to identify and rank the 5 best next-step treatments to\nconsider, and the 5 worst or contraindicated next-step treatments. Expert-\ndeﬁned optimal treatment for a given vignette was assigned on the basis\nof mean ranking of each medication. Poor treatment was assigned on the\nbasis of appearing in a list of poor options from at least one expert. The\nsurveyed clinicians were similarly presented with 20 randomly-selected\nvignettes in random order drawn from the 50, with gender and race\nrandomly permuted.\nAll respondents signed written informed consent prior to completing\nthe survey, which was approved by the Mass General-Brigham Institutional\nReview Board.\nModel design\nThe augmented model used GPT-4 with a prompt incorporating 3 sections\n(Supplementary Materials). The ﬁrst presented the context and task; the\nsecond summarized the knowledge to be used in selecting treatment; and\nthe third presented the clinical vignette. The knowledge incorporated an\nexcerpt from the US Veterans Administration 2023 guidelines for bipolar\ndisorder [6] relating to pharmacologic management of depression. For\nprimary result generation, the model prompt asked to return a ranked list\nof the best 5 next-step interventions. For exploration of these results, an\nalternate prompt (Supplementary Materials) asked that each intervention\nbe justiﬁed by citing the rationale relied upon for recommendation.\nAs a comparator (the‘base model’), we repeated scoring using a shortened\nversion of the augmented model prompt to elicit model recommendations\nwithout relying on additional knowledge (Supplementary Materials).\nAll models used GPT-4 turbo (gpt-4-1106-preview), with temperature set\nat 0 to generate the most deterministic (i.e., least random) results, and\ncontext reset prior to each vignette. (While temperature can be applied to\nincrease apparent creativity in generative models, we presumed that\ncreativity in treatment selection would decrease replicability and transpar-\nency). Each vignette was presented a total of 20 times, with gender and race\npermuted (that is, each combination of male or female, and Black or white,\nwas presented 5 times). Web queries and code execution by GPT-4, as well as\nuse of additional agents, were inactivated to ensure that all responses used\nonly the knowledge provided plus the prior training.\nAnalysis\nIn the primary analysis, we compared the augmented model to the expert\nselections, evaluating how often this model identiﬁed the expert-deﬁned\noptimal medication choice. To examine whether this augmentation was\nuseful, we then compared an unaugmented model (i.e., the LLM without\ninclusion of guideline knowledge) to expert selections, again evaluating how\noften it selected the optimal choice. Performance of the two models was\ncompared using McNemar’s test for agreement. To facilitate comparison to\ncommunity practice, we also calculated the proportion of vignettes for which\ncommunity clinician choices matched expert selections. For the augmented\nand base model, as well as the community clinicians, we also evaluated how\noften models or clinicians selected at least one expert-de ﬁned poor\nmedication choice.\nTo evaluate the possibility of biased responses, we also examined model\nresponses stratiﬁed by 4 gender-race pairs (Black men, Black women, white\nmen, white women). To compare the 4 groups, we used Cochrane’s Q test\n(a generalized form of McNemar’s test), followed by post-hoc pairwise tests\nusing McNemar’s test with Bonferroni-correctedp-values. This approach is\nanalogous to using ANOVA with post hoc pairwise tests.\nWe conducted two post hoc sensitivity analyses. First, as one of the\nexperts had contributed to guideline development, we examined\nthe effect of excluding that expert, to ensure the overlap (between\nguideline writing and expert opinion) did not in ﬂate concordance\nbetween augmented model output and expert results. Second, we\nexamined the performance of psychiatrist prescribers alone, rather than\nincluding all clinicians.\nIn reporting model performance, we elected to report proportion of\ntimes that the optimal expert choice was selected as theﬁrst choice, both\nas proportion and as Cohen’s kappa. We adopted this strategy to maximize\ntransparency and interpretability, and because comparisons of ranked lists,\nwhen the rankings are partial (i.e., not all options are scored), is not a well-\ndeﬁned area of statistical methodology [14, 15]. Secondarily, we reported\nhow often the experts’ top choice was among the top 3, or top 5, provided\nby the model. We further examined degree of overlap between the 5\nmodel choices and the 5 expert choices. Finally, we compared the models’\ntop 5 choices to the list of poor choices identiﬁed by the experts.\nRESULTS\nAgreement between the 3 expert clinicians measured by Cohen’s\nkappa ranged from 0.10 (95% CI 0.01 – 0.18) to 0.22 (95% CI\n0.11– 0.33). For the augmented model, Cohen’s kappa with the\nexpert consensus was 0.31 (95% CI 0.28– 0.35). For 508 vignettes\n(50.8%), the model identiﬁed the optimal treatment; for 844 (84.4%)\nthe optimal treatment was among the top 3 nominated by the\nmodel; and for 949 (94.9%) the optimal treatment was among the\ntop 5. Mean overlap between the model’s selections and the expert\nselections was 3.7 (SD 0.6)– that is, on average, 3.7 of the model’s\nmedications appeared among the expert top 5. Conversely, for 120\n(12.0%) of vignettes, the model selected a medication considered\nby the experts to be a poor choice, or contraindicated.\nIn subgroup analyses of the 4 gender-race pairs, model\nperformance exhibited differences of modest magnitude (Table1;\nSupplementary Table 1) that were statistically signiﬁcant (p =0.02).\nIn post-hoc pairwise contrasts using McNemar ’s test, model\nperformance in Black women was signiﬁcantly poorer than white\nmen (Bonferroni-correctedp =0.03).\nFor comparison, we repeated these analysis with an unaugmented\nmodel, using a prompt that excluded any speciﬁc knowledge of\nguidelines (Table 1,b o t t o m ) .C o h e n’s kappa with the expert\nconsensus was 0.09 (95% CI 0.07– 0.12). For 234 (23.4%) of vignettes,\nthe model identiﬁed the optimal treatment, a result signiﬁcantly\npoorer than for the augmented model (McNemar’s p < 0.001); for 724\n(72.4%) the optimal treatment was among the top 3 nominated by\nthe model; and for 906 (90.6%) the optimal treatment was among the\ntop 5. Mean overlap between the model’s selections and the expert\nselections was 2.8 (SD 0.7). For 108 (10.8%) of vignettes, the model\nselected a medication considered by the experts to be a poor choice,\nor contraindicated; this result was not signiﬁcantly different from the\naugmented model (McNemar’s p =0.4). Figure1 summarizes these\ncontrasts. Results were similar when one of the 3 experts who had\ncontributed to guideline development was excluded from analysis\n(Supplementary Table 2). In subgroup analyses constraining vign-\nettes to a single gender, or either white or Black race, no signiﬁcant\ndifferences in model performance were identiﬁed (p =0.17).\nWe next compared community clinicians with experience in\ntreating bipolar disorder to the expert consensus. A total of 27\nprescribing clinicians participated (10 psychiatrists (37%), 12\nR.H. Perlis et al.\n1413\nNeuropsychopharmacology (2024) 49:1412 – 1416\npsychiatric nurse practitioners (44%), 2 non-psychiatric (7%), 2\nphysician assistants (7%), and 1 primary care physician (4%). They\nreported a mean of 12.7 (SD 10.3) years in practice, and treating a\nmean of 9.2 (SD 5.1) individuals with bipolar disorder per week. Each\nvignette was scored by a median of 4 [25 – 75% interval 3– 5]\nclinicians. Table 1,F i g .1, and Supplementary Table 1 summarize\ncommunity clinician performance. Mean kappa was 0.07 (95% CI\n−0.15 to 0.29); a mean of 23.0% ranked the optimal medicationﬁrst,\n49.0% ranked the optimal medication among their top 3, and 58.4%\namong their top 5 choices. Mean overlap between clinician choices\nand expert selections was 2.2 (SD 1). A mean of 22.0% selected at\nleast one poor or contraindicated medication among their top 5\nchoices. Secondary analysis including only vignettes scored by the\n10 psychiatrists yielded similar results (Table1, bottom).\nFinally, to demonstrate how such a model could be applied in\nclinical settings, we modiﬁed the prompt to return an explanation for\neach treatment selection. (Supplementary Materials) This chat-based\napproach also illustrates a more interactive application, including\nconsideration in real time of clinician and patient preference.\nDISCUSSION\nIn this comparison of a decision support tool integrating treatment\nguidelines with a large language model via retrieval-augmented\ngeneration, we found that a model augmented with an excerpt from\npublished treatment guidelines selected the next-step medication\nconsidered optimal by clinical experts based on a curated set of\nclinical vignettes, more than half the time. This performance\ncompared favorably with a base or un-augmented model (i.e.,\nrelying on GPT-4 with no additional knowledge), and with a sample\nof community clinicians experienced in treatment of bipolar\ndisorder, although agreement between the augmented model and\nexpert ratings was only fair. The discordance between community\nclinician performance and expert choices likely re ﬂects the\ncontinuing difﬁculty in treatment decision-making for this aspect\nof bipolar disorder [2], with heterogeneity in expert opinion [3]a s\nwell as clinical practice [4, 5]. Indeed, the modest agreement among\nexperts underscores the challenge in developing clinical decision\nsupport in areas where the gold standard may be difﬁcult to deﬁne.\nWe also investigated bias by comparing model performance in\nvignettes subsetted by gender or race. While differences were\nmodest in numeric terms, there was some evidence of bias in results:\nan omnibus test for differences between gender and race group\nsuggested signiﬁcant differences in performance in terms of\ndifferences in ability to select optimal treatment. An advantage of\ninclusion of knowledge for the model to operate on, akin to retrieval\naugmented generation on a smaller scale, is that it enables greater\nvisibility into reference materials (i.e., knowledge to be incorporated\nin decisionmaking) than the baseline models. There is no obvious\nreason that the guideline text would introduce bias, such that this\nmay represent type 1 error. Nonetheless, it underscores the\nimportance of careful characterization of models in terms of fairness\nboth before and after deployment. An evolving literature suggests\nthat when LLM’s are asked to perform clinical tasks, they may exhibit\nsubtle biases– for example, in associating race with diagnoses when\nasked to generate clinical vignettes [16]. On the other hand, in other\ncontexts LLM’s may exhibit less biased responses [17].\nOur results are difﬁcult to compare to prior investigations in\npsychiatry. In one prior report [13], we described the use of GPT4 for\nantidepressant selection based on a small set of previously-validated\nvignettes, without any augmentation. That study, which did not\nemploy any additional knowledge in prompting, was notable for the\nhigh rates at which the model selected poor or contraindicated\noptions – in particular, at least one such choice was included among\noptimal treatments in 48% of vignettes.\nThe observation that incorporating knowledge directly in\nprompting diminishes likelihood of contraindicated recommenda-\ntions is signiﬁcant in light of prior work indicating that psychiatric\nTable 1. Comparison of ratings from models or clinicians to expert recommendation.\nSource Top choice Poor choice (a)\nKappa 95% CI n % [95% CI] n % [95% CI]\nAugmented Model 0.31 0.28 – 0.35 508 50.8% 47.7% 53.9% 120 12.0% 10.0% 14.0%\nBlack Man 0.31 0.24 – 0.38 126 50.4% 44.2% 56.6% 29 11.6% 7.6% 15.6%\nBlack Woman 0.28 0.21 – 0.35 121 48.4% 42.2% 54.6% 26 10.4% 6.4% 14.4%\nWhite Man 0.34 0.27 – 0.41 133 53.2% 47.0% 59.4% 33 13.2% 9.4% 17.0%\nWhite Woman 0.32 0.25 – 0.39 128 51.2% 45.0% 57.4% 32 12.8% 8.6% 17.0%\nBase Model 0.09 0.07 – 0.12 234 23.4% 20.8% 26.0% 108 10.8% 8.9% 12.7%\nBlack Man 0.06 0.01 – 0.11 51 20.4% 15.2% 25.6% 25 10.0% 6.2% 13.8%\nBlack Woman 0.11 0.06 – 0.16 61 24.4% 19.4% 29.4% 26 10.4% 6.7% 14.1%\nWhite Man 0.11 0.05 – 0.16 63 25.2% 19.9% 30.5% 31 12.4% 8.6% 16.2%\nWhite Woman 0.09 0.04 – 0.14 59 23.6% 18.2% 29.0% 26 10.4% 6.3% 14.5%\nAll Clinicians (a) 0.09 0.03 – 0.15 23.1% 15.7% 30.5% 24.4% 17.7% 31.1%\nPsychiatrists (b) 0.11 0.00 – 0.22 23.4% 11.9% 34.9% 23.9% 12.3% 35.6%\n(a) among‘poor choice’ vignettes for augmented model,n = 0 vignettes with more than 1 poor choice; among base model,n = 32 vignettes in which 2 of 5\ntop-ranked choices were poor (7, 9, 8, and 8, respectively).\n(b) values calculated as mean of individual clinician results.\n0.00\n0.25\n0.50\n0.75\n1.00\ntop 1 top 3 top 5 poor\nMetric\nPercentage\nSource\nAugmented Model\nBase Model\nClinician\n  \nFig. 1 Comparison of augmented language model, base model, and\nclinician medication selections.Bars indicate proportion selecting\nexpert-deﬁned optimal or poor treatment, with 95% con ﬁdence\ninterval.\nR.H. Perlis et al.\n1414\nNeuropsychopharmacology (2024) 49:1412 – 1416\nclinicians are susceptible to being inﬂuenced to make incorrect\nchoices by artiﬁcial intelligence tools [18]. It is thus critical to\nconsider, not only how often models succeed, but also their mode\nand frequency of failure. While our results suggest the utility of\naugmenting model knowledge in diminishing this risk, we note\nthat the model did still yield some responses which, if adopted,\ncould prove to be harmful to patients.\nThe use of a retrieval-augmented generation architecture for\nlarge language models is a rapidly-evolving strategy for more\ntransparently incorporating knowledge in such models (for an\nexample in a clinical context, see Zakka [19]). For this proof-of-\nconcept study, we incorporated aguideline document directly in\nthe prompt itself, rather than requiring retrieval, a strategy that\nis increasingly feasible as the context window for large language\nmodels grows. This highly-extensible architecture can readily\nincorporate additional perspectives to more closely approximate\nclinical practice. For example, documents could encompass\nclinician preferences or standards of care in a particular health\nsystem; others could encompass individual patient preferences,\nsuch as adverse effects felt to be more or less acceptable.\nOur work seeks to provide a baseline, and a set of curated\nvignettes, that can be applied for such studies. A key question\nfor future investigations will be the extent to which the\nincorporation of guidelines improves treatment selections in\nother clinical contexts.\nLimitations\nThis study has multiple limitations. While the vignettes are derived\nfrom real-world cases and intended to reﬂect clinical descriptions,\nwe cannot exclude the possibility that critical information omitted\nfrom the vignettes would have improved or degraded prediction.\nFurther work will be required to better understand the sensitivity of\nprediction to extent of clinical detail. We did include potential\ndistractors (e.g., occupation, living situation) and medical and\npsychiatric comorbidities to enhance the realism of the vignettes. In\naddition, while the gold standard reﬂects annotation by expert\nclinicians, prospective investigation will be required to understand\nwhether approximating expert opinion is the optimal way to\nimprove outcomes– i.e., to establish the efﬁcacy and safety of LLM-\nbased decision support tools. While the US FDA has addressed\ninternational standards for software as a medical device [20, 21], the\napplicability of these standards in this context remains to be\nestablished. Finally, additional clinician data will be valuable in\nbenchmarking model performance against more generalizable\ncohorts; in light of the small number of clinicians who participated,\nwe cannot exclude the possibility that other groups would exhibit\nsubstantially greater, or poorer, performance.\nCONCLUSION\nWith these caveats in mind, our results nonetheless demonstrate\nthe potential utility of a straightforward, interpretable approach to\nintegrating treatment guidelines with clinical context to yield a\ndecision-support tool. In light of the known challenges of treating\nbipolar depression, as the augmented model performed better on\naverage than a sample of community clinicians, randomized trials\nto determine whether our augmented model can improve clinical\noutcomes, without increasing risk, merit consideration. More\nbroadly, our results suggest the potential utility of applying large\nlanguage models to provide a guideline-based standard of care in\nclinical settings, allowing transparency and portability in devel-\nopment of these decision support tools.\nDATA AVAILABILITY\nAll vignettes, templates, and surveys used for this study are available from the\ncorresponding author for non-commercial use.\nREFERENCES\n1. Biazus TB, Beraldi GH, Tokeshi L, Rotenberg LdeS, Dragioti E, Carvalho AF, et al. All-\ncause and cause-speciﬁc mortality among people with bipolar disorder: a large-\nscale systematic review and meta-analysis. Mol Psychiatry. 2023;28:2508– 24.\n2. Gitlin MJ. Antidepressants in bipolar depression: an enduring controversy. Int J\nBipolar Disord. 2018;6:25.\n3. Pacchiarotti I, Bond DJ, Baldessarini RJ, Nolen WA, Grunze H, Licht RW, et al. The\nInternational Society for Bipolar Disorders (ISBD) task force report on anti-\ndepressant use in bipolar disorders. Am J Psychiatry. 2013;170:1249– 62.\n4. Goldberg JF, Freeman MP, Bacon R, Citrome L, Thase ME, Kane JM, et al. The\nAmerican Society of Clinical Psychopharmacology survery of psychopharmacol-\nogists’ practice patterns for the treatment of mood disorders. Depress Anxiety.\n2015;32:605– 13.\n5. Sakurai H, Kato M, Yasui-Furukori N, Suzuki T, Baba H, Watanabe K, et al. Phar-\nmacological management of bipolar disorder: Japanese expert consensus. Bipolar\nDisord. 2020;22:822– 30.\n6. VA.gov | Veterans Affairs. https://www.healthquality.va.gov/guidelines/mh/bd/.\nAccessed 20 December 2023.\n7. Yatham LN, Kennedy SH, Parikh SV, Schaffer A, Bond DJ, Frey BN, et al. Canadian\nNetwork for Mood and Anxiety Treatments (CANMAT) and International Society\nfor Bipolar Disorders (ISBD) 2018 guidelines for the management of patients with\nbipolar disorder. Bipolar Disord. 2018;20:97– 170.\n8. Malhi GS, Bell E, Bassett D, Boyce P, Bryant R, Hazell P, et al. The 2020 Royal\nAustralian and New Zealand College of Psychiatrists clinical practice guidelines\nfor mood disorders. Aust N. Z J Psychiatry. 2021;55:7– 117.\n9. Goodwin GM, Haddad PM, Ferrier IN, Aronson JK, Barnes T, Cipriani A, et al.\nEvidence-based guidelines for treating bipolar disorder: revised third edition\nrecommendations from the British Association for Psychopharmacology. J Psy-\nchopharmacol. 2016;30:495– 553.\n10. Florida Best Practice Psychotherapeutic Medication Guidelines for Adults.\nFlorida Center for Behavioral Health Improvements and Solutions. https://\nﬂoridabhcenter.org/adult-guidelines/ﬂorida-best-practice-psychotherapeutic-\nmedication-guidelines-for-adults/. Accessed 20 December 2023.\n11. Shah NH, Entwistle D, Pfeffer MA. Creation and adoption of large language\nmodels in medicine. JAMA. 2023;330:866– 869. 7 August 2023. https://doi.org/\n10.1001/jama.2023.14217.\n12. Eriksen AV, Möller S, Ryg J. Use of GPT-4 to diagnose complex clinical cases. NEJM\nAI. 2023;1:AIp2300031.\n13. Perlis RH. Research Letter: Application of GPT-4 to select next-step antidepressant\ntreatment in major depression. Medrxiv preprint:https://www.medrxiv.org/content/\n10.1101/2023.04.14.23288595v1.\n14. Wilke L, Meyer D. Mathematics Honors Thesis: Comparing Partial Rankings. San\nDiego: University of California; 2014.\n15. Fagin R, Kumar R, Mahdian M, Sivakumar D, Vee E. Comparing partial rankings.\nSIAM J Discrete Math. 2006;20:628– 48.\n16. Zack T, Lehman E, Suzgun M, Rodriguez JA, Celi LA, Gichoya J, et al. Assessing the\npotential of GPT-4 to perpetuate racial and gender biases in health care: a model\nevaluation study. Lancet Digital Health. 2024;6:e12– 22.\n17. Hanna JJ, Wakene AD, Lehmann CU, Medford RJ. Assessing Racial and Ethnic Bias\nin Text Generation for Healthcare-Related Tasks by ChatGPT. Medrxiv preprint:\nhttps://www.medrxiv.org/content/10.1101/2023.08.28.23294730v1.\n18. Jacobs M, Pradier MF, McCoy TH, Perlis RH, Doshi-Velez F, Gajos KZ. How\nmachine-learning recommendations inﬂuence clinician treatment selections: the\nexample of the antidepressant selection. Transl Psychiatry. 2021;11:108.\n19. Zakka C, Chaurasia A, Shad R, Dalal AR, Kim JL, Moor M, et al. Almanac: Retrieval-\nAugmented Language Models for Clinical Medicine. Res Sq. 2023:rs.3.rs-2883198.\nResearch Square preprint:https://doi.org/10.21203/rs.3.rs-2883198/v1.\n20. FDA Center for Devices and Radiological Health. Arti ﬁcial Intelligence and\nMachine Learning in Software as a Medical Device. FDA. 2021. 11 January\n2021. Online document:https://www.fda.gov/medical-devices/software-medical-\ndevice-samd/artiﬁcial-intelligence-and-machine-learning-software-medical-\ndevice. Accessed January 27, 2024.\n21. FDA Center for Devices and Radiological Health. Global Approach to Software\nas a Medical Device. FDA. 2022. 27 September 2022. Online document:\nhttps://www.fda.gov/medical-devices/software-medical-device-samd/global-\napproach-software-medical-device. Accessed January 27, 2024.\nAUTHOR CONTRIBUTIONS\nRoy H. Perlis, MD MSc: Conceived of study, developed software, conducted\nexperiments, analyzed data, drafted manuscript. Joseph F. Goldberg, MD: Con-\ntributed data, assisted in manuscript revision. Michael J. Ostacher, MD MPH:\nContributed data, assisted in manuscript revision. Christopher D. Schneck, MD:\nContributed data, assisted in manuscript revision.\nR.H. Perlis et al.\n1415\nNeuropsychopharmacology (2024) 49:1412 – 1416\nFUNDING\nThis work is supported in part by RF1MH132335 and R01MH123804 to RHP.\nCOMPETING INTERESTS\nRHP has served on scienti ﬁc advisory boards or received consulting fees from\nAlkermes, Circular Genomics, Genomind, Psy Therapeutics, Swan AI Studios, and Vault\nHealth. He holds equity in Circular Genomics, Psy Therapeutics, Swan AI Studios, and\nVault Health. He is a paid Associate Editor at JAMA Network-Open. JFG is a Consultant\nat Alkermes, Genomind, Luye Pharma, Neurelis, Neuroma, Otsuka, Sunovion,\nSupernus; serves on speakers bureaus for Alkermes, Axsome, Intracellular; receives\nroyalties from American Psychiatric Publishing, Cambridge University Press. MJO is a\nfull-time employee of the United States Department of Veterans Affairs. He has\nreceived grant funding from Otsuka and Freespira; payment from Neurocrine for\nservice on a Data Safety and Monitoring Committee; and personal payments for\nexpert testimony. CDS reports no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41386-024-01841-2.\nCorrespondence and requests for materials should be addressed to Roy H. Perlis.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not included in\nthe article’s Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder. To view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2024\nR.H. Perlis et al.\n1416\nNeuropsychopharmacology (2024) 49:1412 – 1416",
  "topic": "Bipolar disorder",
  "concepts": [
    {
      "name": "Bipolar disorder",
      "score": 0.7025808691978455
    },
    {
      "name": "Depression (economics)",
      "score": 0.6507740020751953
    },
    {
      "name": "Psychology",
      "score": 0.6115761399269104
    },
    {
      "name": "Clinical psychology",
      "score": 0.36093810200691223
    },
    {
      "name": "Psychiatry",
      "score": 0.3489910960197449
    },
    {
      "name": "Cognition",
      "score": 0.25160712003707886
    },
    {
      "name": "Macroeconomics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}