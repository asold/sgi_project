{
  "title": "Large language models to differentiate vasospastic angina using patient information",
  "url": "https://openalex.org/W4382198789",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2136439960",
      "name": "Yuko Kiyohara",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2160607467",
      "name": "Satoshi Kodera",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2101133011",
      "name": "Masaya Sato",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A3038486785",
      "name": "Kota Ninomiya",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A2100940555",
      "name": "Masataka Sato",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2156303537",
      "name": "Hiroki Shinohara",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2128125020",
      "name": "Norifumi Takeda",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2145459418",
      "name": "Hiroshi Akazawa",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A1916841271",
      "name": "Hiroyuki Morita",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A197305968",
      "name": "Issei Komuro",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2136439960",
      "name": "Yuko Kiyohara",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2160607467",
      "name": "Satoshi Kodera",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2101133011",
      "name": "Masaya Sato",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A3038486785",
      "name": "Kota Ninomiya",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A2100940555",
      "name": "Masataka Sato",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2156303537",
      "name": "Hiroki Shinohara",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2128125020",
      "name": "Norifumi Takeda",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2145459418",
      "name": "Hiroshi Akazawa",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A1916841271",
      "name": "Hiroyuki Morita",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A197305968",
      "name": "Issei Komuro",
      "affiliations": [
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2122181287",
    "https://openalex.org/W1967871761",
    "https://openalex.org/W4235146414",
    "https://openalex.org/W1985032206",
    "https://openalex.org/W2321708739",
    "https://openalex.org/W2069977182",
    "https://openalex.org/W3210853063",
    "https://openalex.org/W3162195278",
    "https://openalex.org/W2925924778",
    "https://openalex.org/W4213285159",
    "https://openalex.org/W3132272590",
    "https://openalex.org/W1087348042",
    "https://openalex.org/W27052289",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4367186868",
    "https://openalex.org/W6850668563",
    "https://openalex.org/W4362511131",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2587528408",
    "https://openalex.org/W2798812533",
    "https://openalex.org/W2805211535",
    "https://openalex.org/W3206530887",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W3000238064",
    "https://openalex.org/W2591987144",
    "https://openalex.org/W2134384563"
  ],
  "abstract": "Abstract Background Vasospastic angina is sometimes suspected from patients’ medical history. It is essential to appropriately distinguish vasospastic angina from acute coronary syndrome because its standard treatment is pharmacotherapy, not catheter intervention. Large language models have recently been developed and are currently widely accessible. In this study, we aimed to use large language models to distinguish between vasospastic angina and acute coronary syndrome from patient information and compare the accuracies of these models. Method We searched for cases of vasospastic angina and acute coronary syndrome which were written in Japanese and published in online-accessible abstracts and journals, and randomly selected 66 cases as a test dataset. In addition, we selected another ten cases as data for few-shot learning. We used generative pre-trained transformer-3.5 and 4, and Bard, with zero- and few-shot learning. We evaluated the accuracies of the models using the test dataset. Results Generative pre-trained transformer-3.5 with zero-shot learning achieved an accuracy of 52%, sensitivity of 68%, and specificity of 29%; with few-shot learning, it achieved an accuracy of 52%, sensitivity of 26%, and specificity of 86%. Generative pre-trained transformer-4 with zero-shot learning achieved an accuracy of 58%, sensitivity of 29%, and specificity of 96%; with few-shot learning, it achieved an accuracy of 61%, sensitivity of 63%, and specificity of 57%. Bard with zero-shot learning achieved an accuracy of 47%, sensitivity of 16%, and specificity of 89%; with few-shot learning, this model could not be assessed because it failed to produce output. Conclusion Generative pre-trained transformer-4 with few-shot learning was the best of all the models. The accuracies of models with zero- and few-shot learning were almost the same. In the future, models could be made more accurate by combining text data with other modalities.",
  "full_text": "1\n1 Large language models to differentiate vasospastic angina using patient information\n2\n3\n4 Yuko Kiyohara1, Satoshi Kodera1, Masaya Sato2, Kota Ninomiya3, Masataka Sato1, Hiroki \n5 Shinohara1, Norifumi Takeda1, Hiroshi Akazawa1, Hiroyuki Morita1, Issei Komuro1\n6\n7\n8 1 Department of Cardiovascular Medicine, Graduate School of Medicine, The University of \n9 Tokyo, Tokyo, Japan\n10 2 Department of Clinical Laboratory Medicine, Graduate School of Medicine, The University \n11 of Tokyo, Tokyo, Japan.\n12 3 Department of Genome Analysis for Rare and Intractable Diseases, Tohoku University \n13 Graduate School of Medicine, Miyagi, Japan.\n14\n15\n16 * Corresponding author\n17 E-mail: koderasatoshi@gmail.com (SK)\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2\n18 Abstract\n19 Background\n20 Vasospastic angina is sometimes suspected from patients’ medical history. It is essential to \n21 appropriately distinguish vasospastic angina from acute coronary syndrome because its \n22 standard treatment is pharmacotherapy, not catheter intervention. Large language models \n23 have recently been developed and are currently widely accessible. In this study, we aimed to \n24 use large language models to distinguish between vasospastic angina and acute coronary \n25 syndrome from patient information and compare the accuracies of these models.\n26\n27 Method\n28 We searched for cases of vasospastic angina and acute coronary syndrome which were \n29 written in Japanese and published in online-accessible abstracts and journals, and randomly \n30 selected 66 cases as a test dataset. In addition, we selected another ten cases as data for few-\n31 shot learning. We used generative pre-trained transformer-3.5 and 4, and Bard, with zero- and \n32 few-shot learning. We evaluated the accuracies of the models using the test dataset.\n33\n34 Results\n35 Generative pre-trained transformer-3.5 with zero-shot learning achieved an accuracy of 52%, \n36 sensitivity of 68%, and specificity of 29%; with few-shot learning, it achieved an accuracy of \n37 52%, sensitivity of 26%, and specificity of 86%. Generative pre-trained transformer-4 with \n38 zero-shot learning achieved an accuracy of 58%, sensitivity of 29%, and specificity of 96%; \n39 with few-shot learning, it achieved an accuracy of 61%, sensitivity of 63%, and specificity of \n40 57%. Bard with zero-shot learning achieved an accuracy of 47%, sensitivity of 16%, and \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n3\n41 specificity of 89%; with few-shot learning, this model could not be assessed because it failed \n42 to produce output.\n43\n44 Conclusion\n45 Generative pre-trained transformer-4 with few-shot learning was the best of all the models. \n46 The accuracies of models with zero- and few-shot learning were almost the same. In the \n47 future, models could be made more accurate by combining text data with other modalities.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n4\n48 Introduction\n49 Vasospastic angina (VSA) is characterized by transient constriction of the coronary artery, \n50 causing angina and other nonspecific symptoms [1–3]. VSA is known to be caused by \n51 various factors, including emotional stress, exposure to low temperatures, and resting during \n52 the period from midnight to early morning [3–7]. Conversely, acute coronary syndrome \n53 (ACS), which is mostly caused by diseases that differ from VSA, frequently presents with \n54 chest pain during physical exertion following a prolonged history of smoking, dyslipidemia, \n55 hypertension, or diabetes mellitus [8–10]. Because VSA shares some symptoms with ACS, \n56 which requires urgent catheter intervention treatment, distinguishing between VSA and ACS \n57 is often challenging at the screening stage [3,4,8–11]. In cases where it is difficult to \n58 distinguish between the two, VSA is confirmed by coronary spasm provocation testing, as \n59 described in the guideline and international consensus [12,13]. However, provocation testing \n60 is an invasive procedure that carries the risk of serious complications [14]. Therefore, there is \n61 a need for a noninvasive method of screening for VSA in patients with angina.\n62 In recent years, large language models (LLMs) have demonstrated great potential in a broad \n63 range of fields including medicine [15–17]. In the medical field, generative pre-trained \n64 transformer 4 (GPT-4) has achieved high scores in medical licensing examinations in the \n65 United States and Japan [18–20]. In real-world clinical settings, physicians rely on patient \n66 information gathered through interviews and examinations. However, to the best of our \n67 knowledge, there has been no research on whether LLMs can effectively use patient \n68 information to distinguish between specific diseases. Furthermore, although GPT-4 has \n69 multilingual capabilities, much of the research conducted using this model has focused on \n70 English documents [19,21]. In this study, we aimed to use LLMs to distinguish between VSA \n71 and ACS from patient information written in Japanese and compare the accuracies of the \n72 models.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n5\n73 Materials and methods\n74 An overview of this study is shown in Fig 1.\n75\n76 Fig 1. The flow of this study. \n77 The overview of this study is shown. \n78\n79 Case selection\n80 In a previous study, we continuously collected cases of VSA and ACS published in abstracts \n81 of the Japanese Society of Internal Medicine regional conferences and open-access journals. \n82 In the present study, we randomly selected 66 of these cases (38 VSA and 28 ACS cases) as a \n83 test dataset [22]. We defined ACS to include both unstable angina and acute myocardial \n84 infarction. In addition, we selected another ten cases (five VSA and five ACS cases) from the \n85 previously collected cases as data for few-shot learning. Because the data used in this study \n86 were publicly accessible, there were no ethical concerns. Nonetheless, we handled cases \n87 following the principles outlined in the Declaration of Helsinki.\n88\n89 Data extraction\n90 We extracted data comprising age, sex, medical history, past medical history, and medication \n91 from the selected abstracts and case reports, and organized them accordingly (S1 Fig). \n92 Medication doses were not extracted, and the English notation of medication was translated \n93 to Japanese and standardized.\n94\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n6\n95 GPT-3.5 and GPT-4\n96 We used ChatGPT Plus (OpenAI), which is based on generative pre-trained transformer-3.5 \n97 (GPT-3.5) and GPT-4 [20]. In addition, we adopted the zero- and few-shot learning \n98 approaches. To select appropriate cases to include in the data for few-shot learning, YK and \n99 SK discussed and identified typical cases of VSA and ACS. We selected ten representative \n100 cases from the data that were left behind after selecting the test dataset, and these cases were \n101 used as data for few-shot learning.\n102 At the beginning of the prompts, we asked GPT-3.5 and GPT-4 to answer whether each case \n103 was predicted to be VSA or any other coronary artery disease. Technically, ACS can be \n104 caused also by VSA, although rare, and therefore, we used the phrase of “any other coronary \n105 artery disease” rather than ACS. In the zero-shot learning tests, we input the cases from the \n106 test dataset just after the above question. In the few-shot learning tests, we inserted the set \n107 comprising each case and its correct answer, for the ten cases of the learning data (S2 Fig). \n108 We performed the experiment from May 10 to May 20, 2023. We used a new session for \n109 each case by clearing all conversations before inputting any prompts.\n110\n111 Bard\n112 We used Bard (Google) because it was announced on May 11, 2023 that Bard could respond \n113 to Japanese text. We input the same request as we input to GPT-3.5 and GPT-4. In the zero-\n114 shot learning tests, we input the cases in the test dataset following the request sentence. In the \n115 few-shot learning tests, we input the same set of ten cases (with their correct answers) as we \n116 input to GPT-3.5 and GPT-4. We performed the experiment from May 22 to May 24, 2023. \n117 We reset the conversations every time we input a prompt.\n118\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n7\n119 Evaluation of model accuracy\n120 In each of the three groups of experiments (GPT-3.5, GPT-4, and Bard), we compared their \n121 answers with the correct answers, and calculated accuracy, sensitivity, specificity, precision, \n122 and F-score. We compared the accuracies achieved by the three LLMs. The threshold for \n123 statistical significance was set at 0.05. The accuracy of each model was calculated with a \n124 95% confidence interval (CI) using the binomial test, implemented in R (R Foundation for \n125 Statistical Computing, Vienna, Austria).\n126\n127 Comparison with cardiologists’ accuracy and accuracy of each \n128 model\n129 In the previous study, we reported the accuracy with which cardiologists answered the cases \n130 in the test dataset [22]. In this study, for each case, we evaluated the answer given by each \n131 model with reference to those given by the cardiologists.\n132\n133 Sensitivity analysis\n134 As a sensitivity analysis, we selected the subset of the test dataset comprising the cases for \n135 which more than half of the cardiologists answered correctly in the previous study [22]. In 45 \n136 out of 66 cases, more than 50% of the cardiologists answered correctly. We hypothesized that \n137 these cases contain enough information to distinguish between VSA and ACS. Therefore, in \n138 the sensitivity analysis, we used these 45 cases only.\n139\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n8\n140 Results\n141 Accuracy evaluation of GPT-3.5, GPT-4, and Bard\n142 Table 1 shows the accuracy achieved in the three groups of experiments. For GPT-3.5 with \n143 zero-shot learning, the accuracy was 52% (95% CI: 39–64%), sensitivity was 68%, \n144 specificity was 29%, precision was 57%, and F-score was 62%; with few-shot learning, the \n145 accuracy was 52% (95% CI: 39–64%), sensitivity was 26%, specificity was 86%, precision \n146 was 71%, and F-score was 39%. For GPT-4 with zero-shot learning, the accuracy was 58% \n147 (95% CI: 45–70%), sensitivity was 29%, specificity was 96%, precision was 92%, and F-\n148 score was 44%; with few-shot learning, the accuracy was 61% (95% CI: 48–72%), sensitivity \n149 was 63%, specificity was 57%, precision was 67%, and F-score was 65%. For Bard with \n150 zero-shot learning, the accuracy was 47% (95% CI: 35–60%), sensitivity was 16%, \n151 specificity was 89%, precision was 67%, and F-score was 26%; with few-shot learning, Bard \n152 failed to respond to the input data.\n153\n154 Table 1. The main results of accuracies of the models.\n155\n156 The accuracy achieved in each of the three AI models (GPT-3.5, GPT-4, and Bard) with zero- \n157 and few-shot learning is shown.\n158\nBard/fewBard/zeroGPT-4/fewGPT-4/zeroGPT-3.5/fewGPT-3.5/zero\nNA47.0\n(95%CI: 34.6 - 59.7)\n60.6\n(95%CI: 47.8 - 72.4)\n57.6\n(95%CI: 44.8 - 69.7)\n51.5\n(95%CI: 38.9 - 64.0)\n51.5\n(95%CI: 38.9 - 64.0)Accuracy\nNA15.863.228.926.368.4Sensitivity\nNA89.357.196.485.728.6Spesificity\nNA66.766.791.771.456.5Precision\nNA25.564.944.038.561.9F-score\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n9\n159 Comparison with cardiologists' accuracy and result of each model\n160 Fig 2 shows the results of GPT-4 with few-shot learning with reference to the cardiologists’ \n161 accuracy (the proportion of cardiologists who answered correctly). GPT-4 tended to correctly \n162 answer cases for which the cardiologists’ accuracy was high. This tendency was observed for \n163 the other models except for GPT-3.5 with zero-shot learning (S3–6 Figs).\n164\n165 Fig 2. Comparison with cardiologists' accuracy and GPT-4 with few-shot learning.\n166 The result of GPT-4 with few-shot learning is shown with reference to the proportion of \n167 cardiologists who answered correctly.\n168\n169 Sensitivity analysis\n170 In the sensitivity analysis, the results were almost the same as the main results (Table 2). \n171 GPT-4 yielded the highest accuracy of the three models. In GPT-4 with zero-shot learning, \n172 the accuracy was 76%, sensitivity was 50%, specificity was 100%, precision was 100%, and \n173 F-score was 67%; with few-shot learning, the accuracy was 71%, sensitivity was 77%, \n174 specificity was 65%, precision was 68%, and F-score was 72%.\n175\n176 Table 2. The results of the sensitivity analysis.\n177\nBard/fewBard/zeroGPT-4/fewGPT-4/zeroGPT-3.5/fewGPT-3.5/zero\nNA57.8\n(95%CI: 42.2 - 72.3)\n71.1\n(95%CI: 55.7 - 83.6)\n75.6\n(95%CI: 60.5 - 87.1)\n60.0\n(95%CI: 44.3 - 74.3)\n48.9\n(95%CI: 33.7 - 64.2)Accuracy\nNA27.377.350.031.868.2Sensitivity\nNA87.065.2100.087.030.4Spesificity\nNA66.768.0100.070.048.4Precision\nNA38.772.366.743.856.6F-score\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n10\n178 The results of experiments for the subset of the test dataset for which more than half of the \n179 cardiologists answered correctly are shown.\n180\n181 Discussion\n182 In this study, we used GPT-3.5, GPT-4, and Bard with zero- and few-shot learning for the \n183 purpose of distinguishing between VSA and ACS from patient information. The results of the \n184 study can be summarized as follows: 1) GPT-4 with few-shot learning yielded the most \n185 accurate results of the three LLMs, 2) there were no significant differences in accuracy \n186 between zero- and few-shot learning, and 3) our study was unique in that it processed data in \n187 the form of Japanese text instead of English text.\n188 GPT-4 was able to distinguish VSA and ACS from patient information and its accuracy was \n189 almost the same as that of medical students. In a previous study, we compared a variant \n190 model of BERT (Google) with cardiologists and medical students [22,23]. The test data were \n191 the same as those used in this study. For cardiologists, the accuracy was 68%, sensitivity was \n192 58%, and specificity was 82%, whereas for medical students, the accuracy was 61%, \n193 sensitivity was 40%, and specificity was 89% [22]. In the current study, GPT-4 with few-shot \n194 learning achieved almost the same accuracy as medical students. Because we collected cases \n195 from conference abstracts of oral presentations and peer-reviewed articles, which ensured \n196 linguistic consistency and no unique abbreviations or variations in sentences, we certainly \n197 used high-quality data. However, the performance of GPT-4 in distinguishing VSA from \n198 ACS did not match that of cardiologists. This implies that the advantages of enhancing the \n199 quality of data are limited. Improvements to the methodology of models are required to \n200 obtain further improvements in performance.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n11\n201 Surprisingly, we found no significant differences in accuracy between zero- and few-shot \n202 learning. Previously, few-shot learning was reported to increase the accuracy of LLMs [24]. \n203 However, although we used GPT-3.5 and GPT-4 with both zero- and few-shot learning, there \n204 were almost no differences in accuracy between the two methods. This suggests that few-shot \n205 learning may not necessarily improve accuracy. Fine-tuning is another promising method for \n206 enhancing the accuracy of models. Fine-tuning involves adjusting the parameters and \n207 architecture of models, using training data to adapt them to specific tasks [25,26]. Initially, \n208 we attempted to conduct fine-tuning using the remaining data for the purpose of tailoring the \n209 models to our specific tasks. However, applying fine-tuning to GPT-4 was impossible as of \n210 May 20, 2023. In the future, fine-tuning could potentially improve the accuracy of GPT-4 \n211 even further.\n212 Our study has the potential to contribute to the advancement of natural language processing \n213 of Japanese medical data. A comprehensive review showed that almost 90% of the \n214 publications about clinical natural language processing addressed the processing of English \n215 text data, whereas only 0.62% were on Japanese text data [27]. The multilingual capability of \n216 GPT-4 will facilitate research on Japanese text. Much more research specific to the Japanese \n217 population is required because there are some diseases, such as VSA, which are more \n218 common in Japan than elsewhere.\n219 There is scope for improvement in the accuracy of artificial intelligence (AI) models and their \n220 applications to other diseases. It is expected that the accuracy of AI models can be improved \n221 by combining other types of data with text data. In a previous study, an AI model was \n222 developed that used numerical and image data, including laboratory data and resting 12-lead \n223 electrocardiograms, to determine the presence or absence of coronary abnormalities in \n224 patients with chest pain [28]. “Generalist” medical AI, capable of flexibly incorporating data \n225 modalities including text data, numerical data (such as examination findings), and image data \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n12\n226 (such as electrocardiograms), could potentially be useful in clinical settings [29]. \n227 Furthermore, the methodology of this study can be applied to diseases other than VSA, \n228 thereby facilitating the development of AI models to distinguish between various diseases.\n229 There are some limitations to this study. First, the number of conference abstracts and \n230 published papers collected in the study was limited, and it is difficult to generalize its results. \n231 One possible solution to the scarcity of data is the extraction of data from electronic health \n232 records [30]. Although electronic health records contain patient information, the quality of \n233 this information may be inadequate. Electronic health records are often written by busy \n234 physicians and nurses; they therefore contain many abbreviations and incorrect or vague \n235 expressions, and often ignore grammar [31,32]. Therefore, the quality of electronic health \n236 record data is considered to be lower than that of the abstracts and reports used in this study. \n237 Second, regarding the content of the data, the fact that the cardiologists’ accuracy was only \n238 around 70% suggests that the data used in this study may not have contained sufficient \n239 information for distinguishing VSA. That said, in actual clinical practice, it is often difficult \n240 to determine VSA solely from patients’ medical history; therefore, the data used in this study \n241 could not be necessarily reflective of real-world clinical practice. Third, we asked the models \n242 to answer whether each case was VSA or any other coronary artery disease. We believe that \n243 this prompt was very effective in distinguishing between VSA and ACS. However, we would \n244 like to further improve models by directly comparing VSA and ACS. Fourth, in the data \n245 collection process, we defined the ACS group as cases described as “unstable angina” or \n246 “acute myocardial infarction.” Cases described using synonyms or different expressions, such \n247 as “acute coronary syndrome” or “ACS,” may have been omitted, possibly limiting the \n248 quantity of data available for training the AI model and evaluating its accuracy. Future \n249 research should aim to collect cases comprehensively, including those described using \n250 synonyms and different expressions.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n13\n251 In conclusion, we used GPT-3.5, GPT-4, and Bard to distinguish between VSA and ACS \n252 from patient information. The predictive accuracy of GPT-4 with few-shot learning was the \n253 best of the three models. In the future, by creating multimodal AI models that combine text \n254 data with numerical data, image data, and other types of data, it is expected that the accuracy \n255 will be further improved.\n256\n257 Acknowledgments\n258 We employed GPT-4 to generate initial drafts, which were then critically reviewed. We take \n259 full responsibility for the content presented in this article. We thank Edanz \n260 (https://jp.edanz.com/ac) for editing a draft of this manuscript. \n261\n262 References\n263 1. Stern S, DeLuna AB. Coronary artery spasm: a 2009 update. Circulation. 2009;119: \n264 2531–2534. https://doi.org/10.1161/CIRCULATIONAHA.108.843474 PMID: \n265 19433770\n266 2. Lanza GA, Careri G, Crea F. Mechanisms of coronary artery spasm. Circulation. \n267 2011;124: 1774–1782. https://doi.org/10.1161/CIRCULATIONAHA.111.037283 \n268 PMID: 22007100\n269 3. JCS Joint Working Group. Guidelines for diagnosis and treatment of patients with \n270 vasospastic angina (Coronary Spastic Angina) (JCS 2013). Circ J. 2014;78: 2779–\n271 2801. https://doi.org/10.1253/CIRCJ.CJ-66-0098 PMID: 25273915\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n14\n272 4. Yasue H, Nakagawa H, Itoh T, Harada E, Mizuno Y. Coronary artery spasm--clinical \n273 features, diagnosis, pathogenesis, and treatment. J Cardiol. 2008;51: 2–17. \n274 https://doi.org/10.1016/J.JJCC.2008.01.001 PMID: 18522770\n275 5. Yeung AC, Vekshtein VI, Krantz DS, Vita JA, Ryan TJ, Ganz P, et al. The effect of \n276 atherosclerosis on the vasomotor response of coronary arteries to mental stress. N Engl \n277 J Med. 1991;325: 1551–1556. https://doi.org/10.1056/NEJM199111283252205 PMID: \n278 1944439\n279 6. Raizner AE, Chahine RA, Ishimori T, Verani MS, Zacca N, Jamal N, et al. \n280 Provocation of coronary artery spasm by the cold pressor test. Hemodynamic, \n281 arteriographic and quantitative angiographic observations. Circulation. 1980;62: 925–\n282 932. https://doi.org/10.1161/01.CIR.62.5.925 PMID: 7418176\n283 7. Kugiyama K, Yasue H. Coronary-artery spasm. N Engl J Med. 1978;299: 617–620. \n284 https://doi.org/10.1056/NEJM197809282991305 \n285 8. Writing Committee Members, Gulati M, Levy PD, Mukherjee D, Amsterdam E, Bhatt \n286 DL, et al. 2021 AHA/ACC/ASE/CHEST/SAEM/SCCT/SCMR Guideline for the \n287 Evaluation and Diagnosis of Chest Pain: A Report of the American College of \n288 Cardiology/American Heart Association Joint Committee on Clinical Practice \n289 Guidelines. J Am Coll Cardiol. 2021;78: e187–e285. \n290 https://doi.org/10.1016/j.jacc.2021.07.053 PMID: 34756653\n291 9. Collet JP, Thiele H, Barbato E, Bauersachs J, Dendale P, Edvardsen T, et al. 2020 ESC \n292 Guidelines for the management of acute coronary syndromes in patients presenting \n293 without persistent ST-segment elevation. Eur Heart J. 2021;42: 1289–1367. \n294 https://doi.org/10.1093/EURHEARTJ/EHAA575 PMID: 32860058\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n15\n295 10. Kimura K, Kimura T, Ishihara M, Nakagawa Y, Nakao K, Miyauchi K, et al. JCS 2018 \n296 Guideline on Diagnosis and Treatment of Acute Coronary Syndrome. Circ J. 2019;83: \n297 1085–1196. https://doi.org/10.1253/CIRCJ.CJ-19-0133 PMID: 30930428\n298 11. Bhatt DL, Lopes RD, Harrington RA. Diagnosis and Treatment of Acute Coronary \n299 Syndromes: A Review. JAMA. 2022;327: 662–675. \n300 https://doi.org/10.1001/JAMA.2022.0358 PMID: 35166796\n301 12. Yamagishi M, Tamaki N, Akasaka T, Ikeda T, Ueshima K, Uemura S, et al. JCS 2018 \n302 Guideline on Diagnosis of Chronic Coronary Heart Diseases. Circ J. 2021;85: 402–\n303 572. https://doi.org/10.1253/CIRCJ.CJ-19-1131 PMID: 33597320\n304 13. Beltrame JF, Crea F, Kaski JC, Ogawa H, Ong P, Sechtem U, et al. International \n305 standardization of diagnostic criteria for vasospastic angina. Eur Heart J. 2017;38: \n306 2565–2568. https://doi.org/10.1093/EURHEARTJ/EHV351 PMID: 26245334\n307 14. Hamilton KK, Pepine CJ. A renaissance of provocative testing for coronary spasm? J \n308 Am Coll Cardiol. 2000;35: 1857–1859. https://doi.org/10.1016/S0735-1097(00)00653-\n309 7 PMID: 10841235\n310 15. Lee P, Bubeck S, Petro J. Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for \n311 Medicine. N Engl J Med. 2023;388: 1233–1239. \n312 https://doi.org/10.1056/NEJMSR2214184 PMID: 36988602\n313 16. Sanderson K. GPT-4 is here: what scientists think. Nature. 2023;615. \n314 https://doi.org/10.1038/D41586-023-00816-5 PMID: 36928404\n315 17. Li H, Moon JT, Purkayastha S, Celi LA, Trivedi H, Gichoya JW. Ethics of large \n316 language models in medicine and medical research. Lancet Digit Health. 2023;0. \n317 https://doi.org/10.1016/S2589-7500(23)00083-3 PMID: 37120418\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n16\n318 18. Nori H, King N, Mckinney SM, Carignan D, Horvitz E, Openai M 2. Capabilities of \n319 GPT-4 on Medical Challenge Problems. 2023 [Available from: \n320 https://arxiv.org/abs/2303.13375v2]\n321 19. Kasai J, Kasai Y, Sakaguchi K, Yamada Y, Radev D, Allen PG. Evaluating GPT-4 and \n322 ChatGPT on Japanese Medical Licensing Examinations. 2023 [Available from: \n323 https://arxiv.org/abs/2303.18027v2]\n324 20. OpenAI. GPT-4 Technical Report. 2023 [Available from: \n325 https://arxiv.org/abs/2303.08774v3]\n326 21. Jiao W, Wang W, Huang J, Wang X, Tu Z. Is ChatGPT A Good Translator? Yes With \n327 GPT-4 As The Engine. 2023 [Available from: https://arxiv.org/abs/2301.08745v3]\n328 22. Kiyohara Y, Kodera S, Ninomiya K, Sawano S, Katsushika S, Shinohara H, et al. \n329 [Natural Language Processing Artificial Intelligence Model to Distinguish Coronary \n330 Spastic Angina by Medical History]. Shinzo. 2022;54: 1235–1242. [Available from: \n331 https://jglobal.jst.go.jp/detail?JGLOBAL_ID=202202264102722109]. Japanese.\n332 23. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of Deep Bidirectional \n333 Transformers for Language Understanding. NAACL HLT 2019 - 2019 Conference of \n334 the North American Chapter of the Association for Computational Linguistics: Human \n335 Language Technologies - Proceedings of the Conference. 2018;1: 4171–4186. \n336 [Available from: https://arxiv.org/abs/1810.04805v2]\n337 24. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language \n338 Models are Few-Shot Learners. [Available from: https://arxiv.org/abs/2005.14165]\n339 25. Min S, Seo M, Hajishirzi H. Question Answering through Transfer Learning from \n340 Large Fine-grained Supervision Data. [Available from: \n341 https://arxiv.org/abs/1702.02171v5]\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n17\n342 26. Howard J, Ruder S. Universal Language Model Fine-tuning for Text Classification. : \n343 328–339. [Available from: https://arxiv.org/abs/1801.06146] \n344 27. Névéol A, Dalianis H, Velupillai S, Savova G, Zweigenbaum P. Clinical Natural \n345 Language Processing in languages other than English: opportunities and challenges. J \n346 Biomed Semantics. 2018;9: 12. https://doi.org/10.1186/S13326-018-0179-8 PMID: \n347 29602312\n348 28. Ahmad A, Shelly-Cohen M, Corban MT, Murphree Jr DH, Toya T, Sara JD, et al. \n349 Machine learning aids clinical decision-making in patients presenting with angina and \n350 non-obstructive coronary artery disease. European heart journal Digital health. 2021;2: \n351 597–605. https://doi.org/10.1093/EHJDH/ZTAB084 PMID: 36713103\n352 29. Moor M, Banerjee O, Abad ZSH, Krumholz HM, Leskovec J, Topol EJ, et al. \n353 Foundation models for generalist medical artificial intelligence. Nature. 2023;616: \n354 259–265. https://doi.org/10.1038/S41586-023-05881-4 PMID: 37045921\n355 30. Ayala Solares JR, Diletta Raimondi FE, Zhu Y, Rahimian F, Canoy D, Tran J, et al. \n356 Deep learning for electronic health records: A comparative review of multiple deep \n357 neural architectures. J Biomed Inform. 2020;101. \n358 https://doi.org/10.1016/J.JBI.2019.103337 PMID: 31916973\n359 31. Roberts A. Language, Structure, and Reuse in the Electronic Health Record. AMA J \n360 Ethics. 2017;19: 281–288. \n361 https://doi.org/10.1001/JOURNALOFETHICS.2017.19.3.STAS1-1703 PMID: \n362 28323609\n363 32. Greenhalgh T, Potts HWW, Wong G, Bark P, Swinglehurst D. Tensions and paradoxes \n364 in electronic patient record research: a systematic literature review using the meta-\n365 narrative method. Milbank Q. 2009;87: 729–788. https://doi.org/10.1111/J.1468-\n366 0009.2009.00578.X PMID: 20021585\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n18\n367  \n368 Supporting information\n369 S1 Fig. An example of the data. \n370 This example was extracted from the following case report published in an open-access \n371 journal. We used only the original sentences written in Japanese above in the box. In the \n372 figure, we translated the whole text into English to make it easy to understand.\n373 S2 Fig. The detailed data for few-shot learning. \n374 This figure shows the ten cases of the learning data used for few-shot learning. In the \n375 prompts, we put only the original sentences written in Japanese in the above box. In the \n376 figure, we translated the whole text into English to make it easy to understand.\n377 S3 Fig. Comparison with cardiologists' accuracy and GPT-3.5 with zero-shot learning. \n378 The result of GPT-3.5 with zero-shot learning is shown with reference to the proportion of \n379 cardiologists who answered correctly.\n380 S4 Fig. Comparison with cardiologists' accuracy and GPT-3.5 with few-shot learning. \n381 The result of GPT-3.5 with few-shot learning is shown with reference to the proportion of \n382 cardiologists who answered correctly.\n383 S5 Fig. Comparison with cardiologists' accuracy and GPT-4 with zero-shot learning. \n384 The result of GPT-4 with zero-shot learning is shown with reference to the proportion of \n385 cardiologists who answered correctly.\n386 S6 Fig. Comparison with cardiologists' accuracy and Bard with zero-shot learning. \n387 The result of Bard with zero-shot learning is shown with reference to the proportion of \n388 cardiologists who answered correctly.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted June 27, 2023. ; https://doi.org/10.1101/2023.06.26.23291913doi: medRxiv preprint ",
  "topic": "Vasospastic angina",
  "concepts": [
    {
      "name": "Vasospastic angina",
      "score": 0.764276385307312
    },
    {
      "name": "Acute coronary syndrome",
      "score": 0.692502498626709
    },
    {
      "name": "Medicine",
      "score": 0.6267505884170532
    },
    {
      "name": "Transformer",
      "score": 0.5082108974456787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49553167819976807
    },
    {
      "name": "Internal medicine",
      "score": 0.48564422130584717
    },
    {
      "name": "Generative grammar",
      "score": 0.45865461230278015
    },
    {
      "name": "Angina",
      "score": 0.44088369607925415
    },
    {
      "name": "Machine learning",
      "score": 0.40191176533699036
    },
    {
      "name": "Cardiology",
      "score": 0.39547044038772583
    },
    {
      "name": "Computer science",
      "score": 0.3631054759025574
    },
    {
      "name": "Engineering",
      "score": 0.10132676362991333
    },
    {
      "name": "Myocardial infarction",
      "score": 0.0741058886051178
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}