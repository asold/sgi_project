{
  "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
  "url": "https://openalex.org/W4385571738",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2652431006",
      "name": "Soumya Sanyal",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2155579404",
      "name": "Yichong Xu",
      "affiliations": [
        "Microsoft (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2127805350",
      "name": "Shuohang Wang",
      "affiliations": [
        "Microsoft (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2101945034",
      "name": "Ziyi Yang",
      "affiliations": [
        "Microsoft (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2760517703",
      "name": "Reid Pryzant",
      "affiliations": [
        "Microsoft (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2124365211",
      "name": "Wenhao Yu",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A2110288568",
      "name": "Chen-guang Zhu",
      "affiliations": [
        "Microsoft (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169445878",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3105516974",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2996164352",
    "https://openalex.org/W3034457116",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3171446474",
    "https://openalex.org/W4221154651",
    "https://openalex.org/W4281481458",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4205137784",
    "https://openalex.org/W4221148719",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W3118999024",
    "https://openalex.org/W4281651419",
    "https://openalex.org/W3163322517",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W4293569643",
    "https://openalex.org/W4285254610",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4225386295",
    "https://openalex.org/W3165201069",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4310280951",
    "https://openalex.org/W3100554158",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Soumya Sanyal, Yichong Xu, Shuohang Wang, Ziyi Yang, Reid Pryzant, Wenhao Yu, Chenguang Zhu, Xiang Ren. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 6308–6321\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAPOLLO : A Simple Approach for Adaptive Pretraining of Language\nModels for Logical Reasoning\nSoumya Sanyal1∗ Yichong Xu2 Shuohang Wang2 Ziyi Yang2\nReid Pryzant2 Wenhao Yu3∗ Chenguang Zhu2 Xiang Ren1\n1University of Southern California 2Microsoft Cognitive Service Research\n3University of Notre Dame\nsoumyasa@usc.edu\nAbstract\nLogical reasoning over text is an important abil-\nity that requires understanding the semantics of\nthe text and reasoning through them to arrive at\ncorrect inferences. Prior works on pretraining\nlanguage models to improve the logical rea-\nsoning ability require complex processing of\ntraining data (e.g., aligning symbolic knowl-\nedge to text), yielding task-specific solutions\nthat are not easy to adapt to any general text\ncorpus. In this work, we propose APOLLO ,\na simple adaptive pretraining approach to im-\nprove the logical reasoning skills of language\nmodels. We select a subset of Wikipedia for\nadaptive pretraining using a set of logical in-\nference keywords as filter words. Further, we\npropose two self-supervised loss functions for\ntraining. First, we modify the masked language\nmodeling loss to mask specific parts-of-speech\nwords that likely require higher-order reason-\ning to predict them. Second, we propose a\nsentence-level classification loss that teaches\nthe model to distinguish between entailment\nand contradiction types of sentences. The pro-\nposed pretraining paradigm is both simple and\nindependent of task formats. We demonstrate\nthe effectiveness of APOLLO by comparing it\nwith prior baselines on two logical reasoning\ndatasets. APOLLO performs comparably on\nReClor and outperforms baselines on LogiQA.\nThe code base has been made publicly avail-\nable.1\n1 Introduction\nLogical reasoning is an important ability of hu-\nmans that helps us in making rational decisions\nbased on known information. It is an important\nability for text understanding across various down-\nstream tasks, e.g., in open-domain question answer-\ning (Yang et al., 2018; Zhu et al., 2021), machine\n∗ Work done during internship at Microsoft Cognitive\nService Research group\n1https://github.com/INK-USC/APOLLO\nInput Sentence:\nEarth’s temperature would drop down if it were \nfrozen entirely, because it would be more \nreflective.\nRandom Masking:\n[MASK]  temperature [MASK]  drop [MASK]  if it \nwere frozen entirely, because it would be \n[MASK]  reflective.\nSelective Masking:\nEarth’s temperature would [MASK]  down if it \nwere [MASK]  entirely, [MASK]  it would be [MASK]  \nreflective.\nFigure 1: Motivation of Selective Masking. In random\nmasking (Devlin et al., 2019), a word is masked at random.\nPredicting these words often require more of language under-\nstanding than higher-order reasoning (e.g., predicting “would”\nat the 2nd [MASK] place). In selective masking, a word is\nmasked if its POS tag is from a specific set. These candidate\nwords are marked in the blue box in the input sentence. Filling\nthese words requires more reasoning (e.g., to predict “more”\nat the 2nd [MASK] place instead of “less”, which is also gram-\nmatically valid, the model needs a better understanding of the\nsemantics of the sentence).\nreading comprehension (MRC) (Baradaran et al.,\n2022), etc. Recently, there has been an increas-\ning focus on evaluating the logical reasoning abili-\nties of language models by using MRC tasks that\nspecifically require a significant amount of logical\nreasoning to obtain the correct answer (Yu et al.,\n2020; Liu et al., 2021). In these datasets, the model\nneeds to understand a given context, reason logi-\ncally about a question to infer new conclusions, and\nthen select the correct answer from a set of options.\nWith the advent of large pre-trained language mod-\nels (PLMs) in NLP (Devlin et al., 2019; Radford\net al., 2019; Raffel et al., 2020), understanding and\nimproving the logical reasoning abilities of these\nmodels has become even more important as these\nare increasingly being used across a wide variety\nof real-world tasks.\nThere have been some recent works on improv-\ning the logical reasoning abilities of PLMs (Wang\net al., 2022; Ouyang et al., 2022; Jiao et al., 2022).\nThese works typically generate a dataset contain-\ning symbolic structures such as logical graphs from\n6308\nWikipedia\nKeyword-based \nDataset Selection\nImplication  \nDataset\nContinued \nPretraining\nL  = s-MLM + e-CLS\nFigure 2: Overview of APOLLO . We filter Wikipedia using specific logical keywords to create the IMPLICATION dataset. This\nis then used for continued pretraining of a model using two loss objectives: selective masked language modeling (S-MLM ) loss\nand entailment classification (E-CLS ) loss. Please refer to Section 2 and Figure 3 for more details on the data selection process\nand loss function designs.\ntext, logical contrast sets, etc., and then train the\nLM using custom loss objectives to learn logi-\ncal reasoning abilities. While the performance\nimprovements achieved by these methods are en-\ncouraging, the proposed solutions generally require\ncomplex data processing to generate the additional\nstructural information (graphs, contrast data, etc.)\nrequired for training the model. For example, Jiao\net al. (2022) constructs synthetic context-answer\npairs using the entity-level graph from Wikipedia\nfor training the model. Further, the loss functions\nproposed in these works are very specifically de-\nsigned in accordance with their respective data aug-\nmentation technique and widely differs from the\ntypical masked language modeling loss used for\nLM pretraining (Devlin et al., 2019). Additionally,\nsome of these works usually require task-specific\ndesign choices, which are not necessarily learn-\ning generalizable logical reasoning ability that is\nreusable across different task formats. For example,\nWang et al. (2022) parses symbolic logical struc-\ntures from the training data of a specific dataset,\nwhich might not generalize to a new dataset or task.\nOverall, it is unclear if these highly specific induc-\ntive biases are indeed essential for improving the\nlogical reasoning abilities in language models, or\nif a simpler approach is possible.\nOn the other hand, prior works (Gururangan\net al., 2020) have shown that continual domain-\nadaptive pretraining of PLMs leads to performance\ngains on downstream tasks. Inspired by this, we\npropose APOLLO , a continual pretraining-based\napproach to inject logical reasoning abilities in lan-\nguage models that requires minimal data process-\ning and loss function modifications.\nFirstly, we present a simple way of selecting sen-\ntences for training a model that is more likely to\ninvolve logical implications. We achieve this by\ndefining a set of logical inference keywords and\nselecting a subset of sentences from a large text\ncorpus, each containing at least one of these key-\nwords. We hypothesize that PLMs can learn logical\nreasoning capabilities more easily using such sen-\ntences since the premise/conclusions are explicitly\nstated. We note that in contrast to previous works\n(Gururangan et al., 2020), our method can select\nsentences from any general text corpus, eliminating\nthe need for any domain-specific corpus.\nSecondly, we modify the masked language mod-\neling (MLM) loss (Devlin et al., 2019) to selec-\ntively mask specific words in the sentence, based on\ntheir parts-of-speech tags. Prior works (Lad et al.,\n2022) have shown the benefit of selective masking\nof words on task-guided fine-tuning. We hypothe-\nsize that masking words with parts-of-speech (POS)\ntags that are related to higher-order reasoning (such\nas adverbs, conjunctions, etc.) present more chal-\nlenging masked positions for the PLM to predict.\nFor instance, in Figure 1, we observe that the words\nmarked in blue boxes are more related to reasoning\ncompared to the non-highlighted words that mainly\ninvolve knowledge about specific nouns or English\ngrammar.\nLastly, we design a sentence-level classification\nloss to predict if the reasoning in the sentence de-\nscribes an entailment in the reasoning process or\na contradiction. This enables the model to better\nunderstand the differences between positive and\nnegative implications in a sentence, thus improving\nlogical reasoning.\nTo test APOLLO , we evaluate it on two down-\nstream logical reasoning tasks: ReClor (Yu et al.,\n2020) and LogiQA (Liu et al., 2021), and com-\npare it with other baselines. We achieve state-of-\nthe-art performance on LogiQA and comparable\nperformance on ReClor. We demonstrate that our\nmethod generalizes across different model types.\nFurther, we show that using our proposed loss func-\ntions does not induce any catastrophic forgetting\n(Kirkpatrick et al., 2017) of the original language\n6309\nmodeling skills. This demonstrates that our simple,\ncontinual pretraining approach is generalizable to\ndifferent datasets and enables the PLM to acquire\nstrong logical reasoning abilities.\nOverall, compared to prior works, our proposed\npretraining paradigm for APOLLO 1) Uses sen-\ntences from text corpus for training instead of com-\nplex data structures such as entity graphs, etc. 2)\nUses simple learning objectives that are closer to\nlanguage modeling compared to the contrastive\nloss. 3) Is agnostic to both task format and down-\nstream datasets. 4) Achieves state-of-the-art perfor-\nmance on LogiQA.\n2 Method\nIn this section, we describe the details of our pro-\nposed approach. In APOLLO , we use a keyword-\nbased selection strategy to collect a dataset of\nreasoning-related sentences called IMPLICATION\n(§2.1) and then continue training a pretrained\nmodel checkpoint jointly using two loss functions\n(§2.2). This model is then fine-tuned on the train-\ning dataset of each task separately for evaluation.\nA detailed overview of the pipeline is shown in\nFigure 2.\n2.1 Dataset Selection\nPLMs are typically trained on web data which helps\nthem to learn general language modeling capability.\nThen, PLMs are finetuned on downstream datasets\nto specialize on target tasks (Devlin et al., 2019;\nRadford et al., 2018; Raffel et al., 2020). Here,\ninstead of focusing on a specific task, we want\nto teach the PLM generalizable logical reasoning\nabilities. We hypothesize that using training data\nthat contains more logical sentences, rather than\ngeneric internet data, should help in improving the\nreasoning ability of the PLM.\nAlthough creating such a dataset automatically is\na challenging task by itself, inAPOLLO , we explore\na simple and intuitive way to create such a dataset.\nFirst, we select specific keywords that are typically\nencountered in sentences with logical implications.\nBroadly, we categorize these keywords into two\ntypes2:\n• Positive implication (Entailment) : These\nkeywords are present in sentences where the\nreason generally entails the inference. Exam-\n2Appendix A presents the comprehensive list of keywords\nused to build the IMPLICATION dataset.\nples of such keywords would be “therefore”,\n“accordingly”, etc.\n• Negative implication (Contradiction): The\nkeywords in this category are usually present\nin sentences where the reason contradicts the\ninference. For example, keywords such as\n“but”, “although”, etc., come under this cate-\ngory.\nNext, we select sentences from Wikipedia such\nthat they contain at least one of the keywords. We\nname this filtered version of Wikipedia as the IM-\nPLICATION dataset. While this keyword-based fil-\ntering does not necessarily ensure that the sentence\nhas a logical implication, the retained data contains\na higher portion of logically rich sentences than the\ngeneral data. We argue that pretraining on this data\nhelps the PLM to improve logical reasoning skills.\nPlease refer to Appendix A for more details on the\nlist of keywords used to build the IMPLICATION\ndataset.\n2.2 Learning objectives\nSelective masked language modeling (S-MLM )\nis a modified version of the masked language mod-\neling (MLM) loss used in BERT (Devlin et al.,\n2019). In the MLM loss, tokens in a sentence are\nmasked at random and the model learns to predict\nthe masked tokens. While this helps in learning\na good language model, not all masked tokens re-\nquire a similar degree of reasoning to predict them.\nIn the example shown in Figure 3, words such as\n“were”, “the”, etc. are decided more by the structure\nof the English language than any form of reason-\ning. In contrast, predicting logical words such as\n“more”, “and” and “hence” would require more log-\nical reasoning. Thus, we hypothesize that masking\nthese logical words would likely teach the model to\nperform reasoning more effectively than masking a\nword at random.\nWhile finding these exact logical words for a\ngiven sentence is a hard problem, in APOLLO we\nsimplify this by using a heuristic approach to con-\nsider words that belong to a specific set of parts-of-\nspeech (POS) tags. More concretely, in S-MLM\nloss, we only randomly mask words with these 7\nSpaCy POS tags (Honnibal and Montani, 2017):\nADJ, ADV , CONJ, CCONJ, PART, SCONJ, and\nVERB. Please refer to Section 4.4 for more empiri-\ncal results that further justify this choice.\n6310\nEarth’s temperature would [MASK]  down if it \nwere [MASK]  entirely, [MASK]  it would be [MASK]  \nreflective.\nEarth’s temperature would drop  down if it were \nfrozen  entirely, as  it would be more  \nreflective.\nSelective masking\nInput to model:\nE-CLS \nLoss\nS-MLM loss\nEntailment\nContradiction\nLoss Fn =\ns-MLM + e-CLS\nEarth’s temperature would drop down if it were \nfrozen entirely, because it would be more \nreflective.\nFigure 3: Learning objectives in APOLLO . The selective masking step masks out words from a specific set of POS tags (the\ncandidate words are shown in blue boxes). The S-MLM loss then predicts these masked words. The E-CLS loss classifies the\nmasked sentence into one of two categories: entailment or contradiction. The overall loss function used in APOLLO is the sum of\nboth these objectives.\nEntailment classification (E-CLS ) Prior works\nhave shown that semantic-aware sentence-level\nclassification loss can be useful to learn the seman-\ntic information (Sun et al., 2020). Inspired by this,\nin addition to S-MLM , we use another auxiliary\nloss function that predicts whether a masked sen-\ntence contains some reasoning aspects that portray\na sense of entailment or contradiction within the\nsentence. For example, in Figure 3, the sentence\nis classified as “Entailment”, because the phrase\n“more reflective” is entailed by the phrase “frozen\nentirely”. We note that the input to the model is\nthe same sentence with masked words that is used\nfor S-MLM loss. A model would ideally require\nstrong logical reasoning abilities to understand the\nsentence and then predict if it refers to an entail-\nment or contradiction. The labels for this loss are\nbootstrapped using the heuristic of checking the\ntype of implication keyword present in the sen-\ntence (refer to Section 2.1 for details). We note\nthat although the keyword is a correlated feature\nthat can be used to predict the label, on average the\nkeyword would be masked out due to our selective\nmasking policy, forcing the model to learn some\nlogical semantics to minimize the loss. Addition-\nally, even if the model predicts a wrong keyword\nin the sentence, it may still get the relationship\nbetween the sentences correctly. Therefore, the\nclassification loss adds a stronger inductive bias\nspecifically about the reasoning semantics in the\nsentence than the S-MLM loss.\n2.3 Continual Pretraining\nIn APOLLO , we combine both S-MLM and E-CLS\nobjectives as a joint loss function to continually\ntrain a pretrained model checkpoint (Figure 2). Un-\nlike prior works (Jiao et al., 2022), we don’t need\nto add MLM loss to avoid catastrophic forgetting,\nas S-MLM is quite close to the standard MLM\nobjective in format.\n2.4 Finetuning\nAs our loss functions are task-format agnostic, we\nfollow Devlin et al. (2019) and add a randomly\ninitialized MLP layer on top of the continually pre-\ntrained model. Then, we finetune the combined\nmodel on downstream datasets.\n3 Experimental Setup\nIn this section, we describe the details of the\ndatasets on which we evaluate APOLLO , the base-\nlines we compare it with, and some implementation\ndetails of our training procedure.\n3.1 Datasets\nFollowing prior works (Jiao et al., 2022), we evalu-\nate APOLLO on two logical reasoning datasets:\nReClor (Yu et al., 2020) is a reading compre-\nhension dataset created from the logical reasoning\nquestions from standardized graduate admission\nexaminations. The test set is divided into two sub-\nsets: EASY (test-E) and HARD (test-H), where the\nEASY set contains instances whose options can\nbe selected correctly without knowing the context\nand question. The train/dev/test split consists of\n4,638/500/1,000 instances, respectively.\nLogiQA (Liu et al., 2021) is developed using\npublicly available logical examination papers for\n6311\nreading comprehension. The train/dev/test split\nconsists of 7,376/651/651 instances, respectively.\n3.2 Baselines\nWe compare the accuracy of APOLLO with the\nfollowing baselines: LRReasoner (Wang et al.,\n2022), DAGN (Huang et al., 2021), FOCAL REA-\nSONER (Ouyang et al., 2022), and MERIt (Jiao\net al., 2022).\n3.3 Implementation Details\nFor creating the IMPLICATION dataset, we use the\nWikipedia version provided under HuggingFace\nDatasets (Wolf et al., 2020) as the main corpus. 3\nThe list of keywords we use for filtering sentences\nfrom Wikipedia are listed in Appendix A. We ex-\nperiment with RoBERTa-Large (Liu et al., 2019a),\nDeBERTa-v3 (He et al., 2021), and DeBERTa-\nv2-xxlarge (He et al., 2020) as the base models\nfor APOLLO . We pretrain the last two layers of\nthe Transformer (Vaswani et al., 2017) layer for 3\nepochs, using a batch size of 4096. Please refer\nto Appendix B for more details on training and\nfinetuning hyperparameters.\n4 Results\n4.1 Overall Results\nIn this section, we compare the performance of\nAPOLLO with prior baselines on the two logical\nreasoning datasets for different base architectures.\nThe results of using pretrained Roberta-Large as\nthe starting checkpoint for our method are shown\nin Table 1. We observe that APOLLO outperforms\nall baselines on LogiQA and performs lower on\nReClor than three baselines, although consistently\noutperforming the RoBERTa baseline. Overall, this\ndemonstrates that our simple continual pretraining\napproach is indeed strong enough to perform well\non logical reasoning tasks as compared to the prior\nmodels that depend on much more complex train-\ning data and loss function designs.\nTo test the generality of our approach across dif-\nferent architectures, we use pretrained DeBERTa-\nv3 and DeBERTa-v2-xxlarge as the base models for\ncontinued training. The results of using these mod-\nels are shown in Table 2. We find that APOLLO\noutperforms both the baselines on both datasets.\nFurther, we observe that APOLLO performs 1.5%\nworse compared to MERIt on ReClor test set. This\n3https://huggingface.co/datasets/\nwikipedia\nshows that our continual pretraining process can\nimprove performance across different LM architec-\ntures.\n4.2 Performance on GLUE Benchmark\nWhile improving the logical reasoning abilities of a\nPLM is important, it is equally important to retain\nthe natural language understanding skills learned\nduring pretraining. To demonstrate that our pro-\nposed approach does not lead to catastrophic for-\ngetting, we finetune APOLLO on each dataset of\nthe GLUE benchmark (Wang et al., 2019) and eval-\nuate the finetuned checkpoint on the Dev set. The\nresults are compared with the Dev set results for\nthe RoBERTa model (Liu et al., 2019b) in Table\n3. Following Devlin et al. (2019), we omit the\nevaluation on the problematic WNLI set. Over-\nall, we observe that APOLLO can slightly improve\nthe overall performance on the GLUE benchmark.\nThis demonstrates that our proposed continued pre-\ntraining strategy is able to learn better logical rea-\nsoning abilities without any catastrophic forgetting\nof general-purpose language modeling skills, and\nthese logical reasoning capabilities are also benefi-\ncial for general natural language understanding.\n4.3 Qualitative Analysis\nIn this section, we analyze the effect of contin-\nued pretraining on the model’s overall faithful-\nness. Post-hoc interpretability methods such as In-\ntegrated Gradients (Sundararajan et al., 2017), are\nalgorithms to determine the importance of words\nin the input towards predicting a particular class.\nThese importance scores are also referred to as at-\ntribution scores. To approximate the impact of con-\ntinued pretraining, we compute the overall change\nin attribution scores for the implication keywords,\nbefore and after pretraining the model using our\nproposed datasets and loss functions. Specifically,\nwe compute the sum of the attribution scores for the\nkeywords present in each instance of the validation\nset. The results are shown in Figure 4. We observe\nthat our proposed pretraining increases the overall\nattribution score by a significant margin, indicating\nthat the model intrinsically learns these important\nlogical keywords, which is desirable.\n4.4 Ablation Studies\nIn this section, we ablate various design choices in\nconstructing the IMPLICATION dataset, and our\nproposed method. For the ablations involving\nAPOLLO , we use RoBERTa-Large as the base\n6312\nModel ReClor LogiQA\nDev Test Test-E Test-H Dev Test\nRoBERTa 62.6 55.6 75.5 40.0 35 35.3\nDAGN 65.2 58.2 76.1 44.1 35.5 38.7\nLRReasoner 66.2 62.4 81.4 47.5 38.1 40.6\nFOCAL REASONER 66.8 58.9 77.1 44.6 41.0 40.3\nMERIt 67.8 60.7 79.6 45.9 42.4 41.5\nAPOLLO 67.2 58.2 76.8 43.6 41.6 42.1\nTable 1: Comparison of APOLLO with other baselines on ReClor and LogiQA. All the models are based on the\nRoBERTa-large model. The results for all the baselines are reported from Jiao et al. (2022). Please refer to Section\n4.1 for more details.\nModel ReClor LogiQA\nDev Test Test-E Test-H Dev Test\nDeBERTa-v3 75.4 71.0 80.2 64.0 45.2 40.1\nAPOLLO (DeBERTa-v3) 76.8 72.8 81.8 65.7 48.4 44.4\nDeBERTa-v2-xxlarge 78.3 75.3 84.0 68.4 45.9 49.8\nMERIt (DeBERTa-v2-xxlarge) 80.6 78.1 84.6 72.9 - -\nAPOLLO (DeBERTa-v2-xxlarge) 81.8 76.5 85.2 69.6 49.6 51.0\nTable 2: Comparison ofAPOLLO with other baselines on ReClor and LogiQA with DeBERTa as the base architecture.\nResults for MERIt are reported from Jiao et al. (2022), which is missing results on LogiQA. Other baselines are\nreproduced by ourselves. The base models are shown in brackets. Please refer to Section 4.1 for more details.\nAttribution Score Sum\n0.00\n2.00\n4.00\n6.00\nReClor LogiQA\nRoBERTa Apollo\nFigure 4: Comparison plot of the keyword attribution scores\nbetween RoBERTa-large andAPOLLO . Please refer to 4.3 for\nmore details.\nmodel and the IMPLICATION dataset, if not men-\ntioned separately. All the reported numbers are on\nthe validation set of the downstream task, since we\nused these ablation studies in our model’s design\nchoices.\nEffect of datasets and loss functions To study\nthe effect of using IMPLICATION for continued\npretraining along with the proposed loss func-\ntions, we first create RANDOM , a random subset of\nWikipedia of similar size as that of IMPLICATION ,\nand also consider using the standard masked lan-\nguage modeling (MLM) loss (Devlin et al., 2019),\nwhere any token can be masked at random. The\nresults of the ablation are shown in Table 4. We\nobserve that using the IMPLICATION dataset leads\nto consistent improvements on both datasets when\ncompared to the RANDOM dataset. Additionally,\nwe find that both the S-MLM and E-CLS loss lead\nto improvements over MLM loss. Thus, this em-\npirically justifies our choice of the dataset and loss\nfunctions proposed here.\nEffect of keyword category In this ablation, we\nstudy the effect of the keyword categories that we\nuse for filtering Wikipedia. For this, we create\ntwo different pretraining datasets IMPLICATION -\nPositive and IMPLICATION -Negative using the pos-\nitive and negative implication keywords, respec-\ntively (refer to Section 2.1). The total number of\nsentences in these datasets is 7.5M and 11.3M, re-\nspectively. Our complete dataset IMPLICATION\nthus has a total of 18.3M sentences. The results of\nthe ablation are shown in Table 5, under the section\n“Keyword Category”. We observe that IMPLICA -\nTION -Positive, although smaller in size, leads to\nbetter performance on both downstream tasks, com-\n6313\nModel MNLI QNLI QQP RTE SST MRPC CoLA STS Avg\nRoBERTa-Large 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 88.9\nAPOLLO 90.3 94.9 92.1 88.1 96.2 92.2 68.6 91.9 89.3\nTable 3: Performance on the dev set of GLUE benchmark. Following Devlin et al. (2019), we do not report\nperformance on the WNLI dataset. Please refer to Section 4.2 for further details.\nModel (Dataset, Loss functions) ReClor LogiQA\nRoBERTa (RANDOM, MLM) 60.2 35.0\nRoBERTa (RANDOM,S-MLM) 63.8 36.4\nRoBERTa (IMPLICATION, MLM) 64.8 36.6\nRoBERTa (IMPLICATION,S-MLM) 65.4 41.5\nRoBERTa (IMPLICATION,S-MLM +E-CLS) 67.2 41.6\nTable 4: Effect of IMPLICATION dataset and the loss\nfunctions on the dev set performance of ReClor and\nLogiQA.\nReClor LogiQA\nKeyword Category\nIMPLICATION-Positive 65.0 38.6\nIMPLICATION-Negative 64.6 37.6\nIMPLICATION 65.4 41.5\nPOS Category\nBase 65.4 41.5\nBase + Nouns 64.0 39.0\nBase + Nouns + Random 64.8 36.6\nTable 5: Ablation of design choices involved in\nkeyword-based dataset selection and S-MLM loss func-\ntion implementation. We report the performance on the\ndev set of each dataset. Please refer to Section 4.4 for\nmore details.\npared to IMPLICATION -Negative. One reason for\nthis is that the sentences with positive keywords\nare more likely related to reasoning than the neg-\native counterparts because the negative keywords\nare used in many diverse scenarios in the English\nlanguage. For example, the word “ still” can be\nused in a non-logical manner such as “ I am still\nwaiting for the call”. Overall, we observe that the\ncombined IMPLICATION dataset leads to the best\nperformance, demonstrating that both the positive\nand negative implication keywords are essential to\nimprove logical reasoning.\nEffect of POS tag category In this, we analyze\nthe effect of the parts-of-speech (POS) tags we use\nto mask tokens in our S-MLM loss. We consider\nthe following categories:\n• Base: This consists of the POS tags used in\nAPOLLO , i.e., ADJ, ADV , CONJ, CCONJ,\nPART, SCONJ, and VERB.\nNumber of trainable layers\nAverage Accuracy\n48\n50\n52\n54\n0 4 8 12 16 20 24\nFigure 5: Average performance on the dev set of ReClor\nand LogiQA with increasing number of trainable layers of\nAPOLLO . The pink dashed line shows the average perfor-\nmance of RoBERTa-Large when all layers are finetuned.\nPlease refer to Section 4.4 for more details.\n• Nouns: Here, we consider the tags referring\nto nouns and pronouns, i.e., NOUN, PRON,\nand PROPN.\n• Random: This consists of remaining cate-\ngories such as ADP, INTJ, DET, PUNCT, etc.\nTo study the effect of the POS tags, we incremen-\ntally add the “Nouns” and “Random” categories to\nthe base case and evaluate the effect of pretraining\nusing the S-MLM loss. The results of this abla-\ntion are shown in Table 5, under the section “POS\nCategory”. We observe that masking nouns and\npronouns (“Nouns”) leads to a significant perfor-\nmance drop. We attribute this drop to the fact that\npredicting a correct noun in a sentence would likely\nrequire more world knowledge than logical reason-\ning. Using the remaining categories for selective\nmasking (“Random”), effectively making the loss\nfunction equivalent to random MLM, leads to some\ndrop in performance as well, indicating that our set\nof POS tag categories is indeed more useful to learn\nlogical reasoning.\nEffect of the number of trainable layers In or-\nder to study the effect of training different numbers\nof parameters of the RoBERTa model, we vary\nthe number of trainable layers of the transformer\narchitecture between 1 and 24 (i.e., training the\ncomplete model). The results are shown in Figure\n5. The blue solid line shows the performance of\n6314\nAPOLLO and the purple dashed line denotes the\naverage performance of RoBERTa-Large when all\nlayers are finetuned. From the plot, we observe\nthat with increasing the number of trainable lay-\ners, the performance improves till layer 2, and then\ncontinues to degrade until all the layers are being\ntrained. Prior works (Tenney et al., 2019) have\nshown that PLMs learn syntactic-level information\nin the lower layers of the transformer and semantic-\nlevel information in the upper layers. Thus, we\nhypothesize that the logical reasoning task initially\nbenefits from an increasing number of trainable\nlayers, as the semantic information needed to un-\nderstand logic is being captured. But lower layers\nthat contain the syntactic information do not benefit\nas much when trained using the same data as they\nare less related to high-level logical reasoning. The\nfull model finetuning surprisingly performs quite\nwell as all the model layers along with the token\nembeddings are being trained specifically for the\nlogical reasoning task. But it takes significantly\nlarger compute to finetune such a model. Overall,\nwe find that by training the topmost two layers of\nthe model, we are able to achieve the best perfor-\nmance on both datasets and hence we follow this\nacross all variants of APOLLO .\n5 Related Works\nLogical Reasoning in LMs Reasoning in natu-\nral language has been a prevalent problem in NLP.\nIn recent years, logical reasoning in textual data\nhas seen an increasing focus. ReClor (Yu et al.,\n2020) and LogiQA (Liu et al., 2021) are reading\ncomprehension-style datasets focused on questions\nthat require reasoning using information from a\ngiven context. Prior works have predominantly\nused language models (Wang et al., 2022; Jiao et al.,\n2022) or graph neural networks (GNNs) (Huang\net al., 2021; Xu et al., 2022; Li et al., 2022; Ouyang\net al., 2022) to perform logical reasoning over text.\nWang et al. (2022) proposed LRReasoner, which\nparses symbolic logical structures from the training\ndata of ReClor for data augmentation using logical\ncontext extensions to train a PLM. Jiao et al. (2022)\nproposed MERIt, that used Wikipedia to generate\nsentence pairs for contrastive learning that are logi-\ncally related, and trained the PLM using contrastive\nloss. DAGN (Huang et al., 2021) uses the discourse\nstructure of the texts to perform logical reasoning\nusing GNNs. FOCAL REASONER (Ouyang et al.,\n2022) constructs logical graphs using the chain of\nfacts present in a task instance and uses GNNs\nto reason on the graph. GNN-based methods are\nnot directly in scope since our main objective is\nto improve the logical reasoning skills of language\nmodels. Following (Jiao et al., 2022), we compare\nour method with two GNN-based representative\nmethods DAGN and FOCAL REASONER .\nBoth LRReasoner and FOCAL REASONER use\ndata augmentation that is specific to the task being\nsolved, making the pretraining process specific to\nthe downstream dataset, and thus not generalizable\nacross tasks. While MERIt addresses this issue by\nusing Wikipedia to generate logical graphs, their\ncontrastive loss formulation requires counterfactual\ndata augmentation, which potentially distorts the\nfactual knowledge present in the pretrained model.\nAdditionally, their approach is restricted to using\nWikipedia as the data source since they heavily rely\non forming entity graphs from Wikipedia texts. In\ncontrast, we propose a simple continued pretrain-\ning strategy by modifying the masked language\nmodeling loss (Devlin et al., 2019) and sentence\nclassification loss to improve the logical reasoning\nability of language models. Our approach is simple\nto integrate during pretraining, is not dependent on\nany data processing, and generalizes well across\ndifferent datasets.\nAlong a related line, Clark et al. (2020) used\nsynthetically generated data to teach PLMs to per-\nform logical deductions over a given rule base to\npredict the entailment of a hypothesis. This led\nto some recent developments in trying to build\nsystems that can generate step-by-step reasoning\nchains that demonstrate the model’s reasoning pro-\ncess (Saha et al., 2020; Tafjord et al., 2021; Sanyal\net al., 2022b). While this progress is encouraging,\nthe use of synthetic data for training the models\nlimits the generality of the logical reasoning skills\nlearned by these models. Recent works have ques-\ntioned if these models are indeed learning to per-\nform logical reasoning in a robust manner or just\nlearning some shortcuts from training data (Zhang\net al., 2022; Sanyal et al., 2022a). In contrast, our\nmethod uses real-world sentences which alleviates\nthe issue of using synthetic datasets for reasoning.\nSelective masking A key step in the processing\nof masked language modeling loss (Devlin et al.,\n2019) is to determine which tokens to mask. Orig-\ninally, Devlin et al. (2019) randomly mask 15%\nof tokens. Prior works have tried different tech-\nniques to select which tokens to mask. For exam-\n6315\nple, ERNIE (Zhang et al., 2019) and EntityBERT\n(Lin et al., 2021) mask named entities to perform\nbetter knowledge-driven tasks. Other prior works\n(Gu et al., 2020; Lad et al., 2022) calculate the im-\nportance of words for a specific task and selectively\nmask the most important words. In this work, we\nexplore the use of selective masking in the con-\ntext of logical reasoning, using a novel heuristic of\nselecting specific POS-tagged words.\n6 Conclusion\nIn this paper, we proposed APOLLO , an adaptive\npre-trained language model with logical reason-\ning abilities. We use a subset of Wikipedia sen-\ntences for continued pretraining of the model using\ntwo self-supervised loss functions. The choice of\nthe training dataset and loss functions are guided\nby the goal to include more reasoning-related sen-\ntences and training signals, respectively. Through\nexperiments on two logical reasoning datasets and\nablation studies, we demonstrate the effectiveness\nof our proposed approach. Overall, we show that\nAPOLLO is a generalized solution to improving\nlogical reasoning in language models.\nA key advantage of APOLLO is that the pretrain-\ning steps are independent of the dataset used to\ntrain the model and the downstream task format.\nThis opens the scope to use a larger text corpus for\ntraining such as C4 (Raffel et al., 2020). Addition-\nally, expanding on the keywords beyond positive\nand negative implications (for example, condition-\nals such as “if-then”, “either-or”, etc.) can also\nbenefit the training pipeline.\n7 Limitation\nA limitation of this approach is the trade-off be-\ntween completeness and noise in the training data.\nWhile our method using keywords to extract text\nfrom Wikipedia is effective, IMPLICATION likely\ncontains redundant sentences that cannot improve\nthe model’s logical reasoning capability. A better\nrule-based or neural model might be able to extract\na better corpus with potentially higher computa-\ntional costs. Additionally, using POS tagging limits\nthe application of this approach to languages with\nwell-defined POS taggers. Switching to a more\nuniversal semantic tagging system (Abzianidze and\nBos, 2017) can potentially alleviate this.\nAcknowledgements\nThis research is supported in part by the Office\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activ-\nity(IARPA), via Contract No. 2019-19051600007,\nthe DARPA MCS program under Contract\nNo.N660011924033, the Defense Advanced Re-\nsearch Projects Agency with award W911NF-19-\n20271, NSF IIS 2048211, NSF SMA 1829268, and\ngift awards from Google, Amazon, JP Morgan, and\nSony. We would like to thank all the collaborators\nin the USC INK research lab for their constructive\nfeedback on the work.\nReferences\nLasha Abzianidze and Johan Bos. 2017. Towards uni-\nversal semantic tagging. In IWCS 2017 — 12th Inter-\nnational Conference on Computational Semantics —\nShort papers.\nRazieh Baradaran, Razieh Ghiasi, and Hossein\nAmirkhani. 2022. A survey on machine reading com-\nprehension systems. Natural Language Engineering,\n28(6):683–732.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In Pro-\nceedings of the Twenty-Ninth International Joint Con-\nference on Artificial Intelligence, IJCAI-20 , pages\n3882–3890. International Joint Conferences on Arti-\nficial Intelligence Organization. Main track.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan\nLiu, and Maosong Sun. 2020. Train no evil: Selective\nmasking for task-guided pre-training. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6966–6974, Online. Association for Computational\nLinguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\n6316\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremental\nparsing. To appear.\nYinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xi-\naodan Liang. 2021. DAGN: Discourse-aware graph\nnetwork for logical reasoning. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5848–5855,\nOnline. Association for Computational Linguistics.\nFangkai Jiao, Yangyang Guo, Xuemeng Song, and\nLiqiang Nie. 2022. MERIt: Meta-Path Guided Con-\ntrastive Learning for Logical Reasoning. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2022, pages 3496–3509, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A. Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, Demis Hassabis, Clau-\ndia Clopath, Dharshan Kumaran, and Raia Hadsell.\n2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the National Academy of\nSciences, 114(13):3521–3526.\nTanish Lad, Himanshu Maheshwari, Shreyas Kottukkal,\nand Radhika Mamidi. 2022. Using selective mask-\ning as a bridge between pre-training and fine-tuning.\narXiv preprint arXiv:2211.13815.\nXiao Li, Gong Cheng, Ziheng Chen, Yawei Sun, and\nYuzhong Qu. 2022. AdaLoGN: Adaptive logic graph\nnetwork for reasoning-based machine reading com-\nprehension. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 7147–7161, Dublin,\nIreland. Association for Computational Linguistics.\nChen Lin, Timothy Miller, Dmitriy Dligach, Steven\nBethard, and Guergana Savova. 2021. EntityBERT:\nEntity-centric masking strategy for model pretrain-\ning for the clinical domain. In Proceedings of the\n20th Workshop on Biomedical Language Processing,\npages 191–201, Online. Association for Computa-\ntional Linguistics.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang,\nYile Wang, and Yue Zhang. 2021. Logiqa: A chal-\nlenge dataset for machine reading comprehension\nwith logical reasoning. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial\nIntelligence, IJCAI’20.\nYinhan Liu, Myle Ott, Naman Goyal, and Jingfei Du an.\n2019a. Roberta: A robustly optimized bert pretrain-\ning approach. ArXiv preprint, abs/1907.11692.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: a robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nSiru Ouyang, Zhuosheng Zhang, and hai zhao. 2022.\nFact-driven logical reasoning.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nSwarnadeep Saha, Sayan Ghosh, Shashank Srivastava,\nand Mohit Bansal. 2020. PRover: Proof generation\nfor interpretable reasoning over rules. InProceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 122–136,\nOnline. Association for Computational Linguistics.\nSoumya Sanyal, Zeyi Liao, and Xiang Ren. 2022a. Ro-\nbustlr: Evaluating robustness to logical perturbation\nin deductive reasoning.\nSoumya Sanyal, Harman Singh, and Xiang Ren. 2022b.\nFaiRR: Faithful and robust deductive reasoning over\nnatural language. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1075–1093,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. Proceedings of the AAAI Conference on\nArtificial Intelligence, 34(05):8968–8975.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Proceed-\nings of the 34th International Conference on Machine\nLearning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017, volume 70 of Proceedings of Machine\nLearning Research, pages 3319–3328. PMLR.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621–3634, Online.\nAssociation for Computational Linguistics.\n6317\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\nSiyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu\nWei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan\nDuan. 2022. Logic-driven context extension and data\naugmentation for logical reasoning of text. In Find-\nings of the Association for Computational Linguis-\ntics: ACL 2022, pages 1619–1629, Dublin, Ireland.\nAssociation for Computational Linguistics.\nThomas Wolf, Quentin Lhoest, Patrick von Platen,\nYacine Jernite, Mariama Drame, Julien Plu, Julien\nChaumond, Clement Delangue, Clara Ma, Abhishek\nThakur, Suraj Patil, Joe Davison, Teven Le Scao,\nVictor Sanh, Canwen Xu, Nicolas Patry, Angie\nMcMillan-Major, Simon Brandeis, Sylvain Gugger,\nFrançois Lagunas, Lysandre Debut, Morgan Funtow-\nicz, Anthony Moi, Sasha Rush, Philipp Schmidd,\nPierric Cistac, Victor Muštar, Jeff Boudier, and\nAnna Tordjmann. 2020. Datasets. GitHub. Note:\nhttps://github.com/huggingface/datasets.\nFangzhi Xu, Jun Liu, Qika Lin, Yudai Pan, and Lin-\ngling Zhang. 2022. Logiformer: A two-branch graph\ntransformer network for interpretable logical reason-\ning. In Proceedings of the 45th International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR ’22, page 1055–1065,\nNew York, NY , USA. Association for Computing\nMachinery.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng.\n2020. Reclor: A reading comprehension dataset re-\nquiring logical reasoning. In International Confer-\nence on Learning Representations (ICLR).\nHonghua Zhang, Liunian Harold Li, Tao Meng, Kai-\nWei Chang, and Guy Van den Broeck. 2022. On the\nparadox of learning to reason from data.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1441–1451, Florence, Italy. Association for Compu-\ntational Linguistics.\nFengbin Zhu, Wenqiang Lei, Chao Wang, Jianming\nZheng, Soujanya Poria, and Tat-Seng Chua. 2021.\nRetrieving and reading: A comprehensive survey on\nopen-domain question answering.\n6318\nA List of Keywords\nIn this section, we list the set of keywords that we\nuse to filter the entire WikiPedia data. Any sentence\nthat contains one of the keywords is considered\nas part of our filtered dataset IMPLICATION . The\nkeywords are divided into two types as described\nbelow:\n• Positive implication (Entailment) : These\nkeywords are present in sentences where the\nreason generally entails the inference. Exam-\nples of such keywords would be “therefore”,\n“accordingly”, etc. We consider the follow-\ning keywords in this category: “therefore”,\n“accordingly”, “so”, “thus”, “consequently”,\n“hence”, “thence”, “and so”, “for this reason”,\n“in consequence”, “on account of”, “on the\n”grounds”, “since”, “therefrom”, “thereupon”,\n“to that end”, “whence”, and “wherefore”.\n• Negative implication (Contradiction): The\nkeywords in this category are usually present\nin sentences where the reason contradicts\nthe inference. For example, keywords such\nas “but”, “although”, etc., come under this\ncategory. Here, we consider the follow-\ning keywords: “but”, “although”, “however”,\n“nevertheless”, “on the other hand”, “still”,\n“though”, and “yet”.\nB Hyperparameter Details\nIn continual pretraining, we select the learning rate\nfrom the set {7e −6, 1e −5, 7e −5}, batch size\n4, gradient accumulation step size from the set\n{64, 128}, warmup ratio 0.1, and train the model\non a cluster of 8 A100 GPUs. To fine-tune a con-\ntinually pretrained checkpoint, we use the training\ndata of each dataset separately. We select learning\nrate from the set {8e −6, 1e −5, 5e −5}, batch\nsize of 4, and gradient accumulation step size 1.\nTo train the models we use a cluster of 8 A100\nGPUs, which typically takes around 20 hours for\nthe largest model.\n6319\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□\u0013 A2. Did you discuss any potential risks of your work?\n7\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nNot applicable. 1\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. 1\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe use Wikipedia as the data source which is a standard practice in language model pretraining\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n3.1\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6320\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.1\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n2\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n6321",
  "topic": "Apollo",
  "concepts": [
    {
      "name": "Apollo",
      "score": 0.7458507418632507
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.6644659042358398
    },
    {
      "name": "Computer science",
      "score": 0.5892889499664307
    },
    {
      "name": "Computational linguistics",
      "score": 0.5327715277671814
    },
    {
      "name": "Cognitive science",
      "score": 0.4878873825073242
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4845249056816101
    },
    {
      "name": "Natural language processing",
      "score": 0.48348313570022583
    },
    {
      "name": "Linguistics",
      "score": 0.47604942321777344
    },
    {
      "name": "Association (psychology)",
      "score": 0.44297391176223755
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4407826066017151
    },
    {
      "name": "Logical conjunction",
      "score": 0.4314768314361572
    },
    {
      "name": "Programming language",
      "score": 0.33225369453430176
    },
    {
      "name": "Psychology",
      "score": 0.19820144772529602
    },
    {
      "name": "Philosophy",
      "score": 0.19454169273376465
    },
    {
      "name": "Epistemology",
      "score": 0.19278442859649658
    },
    {
      "name": "Ecology",
      "score": 0.0664302408695221
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210087053",
      "name": "Microsoft (Germany)",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I107639228",
      "name": "University of Notre Dame",
      "country": "US"
    }
  ],
  "cited_by": 4
}