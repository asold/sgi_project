{
    "title": "A framework for evaluating the chemical knowledge and reasoning abilities of large language models against the expertise of chemists",
    "url": "https://openalex.org/W4410521503",
    "year": 2025,
    "authors": [
        {
            "id": null,
            "name": "Mirza, Adrian",
            "affiliations": [
                "Helmholtz Institute Jena",
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Alampara, Nawaf",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Kunchapu, Sreekanth",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Ríos-García, Martiño",
            "affiliations": [
                "Instituto Nacional del Carbón",
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Emoekabu, Benedict",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Krishnan, Aswanth",
            "affiliations": [
                "AM Technologies (Poland)"
            ]
        },
        {
            "id": "https://openalex.org/A4295247752",
            "name": "Gupta Tanya",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": null,
            "name": "Schilling-Wilhelmi, Mara",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Okereke, Macjonathan",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Aneesh, Anagha",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": "https://openalex.org/A2752521327",
            "name": "Asgari Mehrdad",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": null,
            "name": "Eberhardt, Juliane",
            "affiliations": [
                "University of Bayreuth"
            ]
        },
        {
            "id": null,
            "name": "Elahi, Amir Mohammad",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": null,
            "name": "Elbeheiry, Hani M.",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": "https://openalex.org/A4380712949",
            "name": "Gil, María Victoria",
            "affiliations": [
                "Instituto Nacional del Carbón"
            ]
        },
        {
            "id": "https://openalex.org/A4281778153",
            "name": "Glaubitz, Christina",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Greiner, Maximilian",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Holick, Caroline T.",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": "https://openalex.org/A4287521594",
            "name": "Hoffmann, Tim",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": "https://openalex.org/A2029003443",
            "name": "Ibrahim AbdelRahman",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Klepsch, Lea C.",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Köster, Yannik",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Kreth, Fabian Alexander",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Meyer, Jakob",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": "https://openalex.org/A4287296325",
            "name": "Miret, Santiago",
            "affiliations": [
                "Intel (United States)"
            ]
        },
        {
            "id": null,
            "name": "Peschel, Jan Matthias",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Ringleb, Michael",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Roesner, Nicole C.",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Schreiber, Johanna",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": "https://openalex.org/A3216044441",
            "name": "Schubert Ulrich S",
            "affiliations": [
                "Helmholtz Institute Jena",
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Stafast, Leanne M.",
            "affiliations": [
                "Friedrich Schiller University Jena"
            ]
        },
        {
            "id": null,
            "name": "Wonanke, A. D. Dinga",
            "affiliations": [
                "TU Dresden"
            ]
        },
        {
            "id": "https://openalex.org/A4223932253",
            "name": "Pieler, Michael",
            "affiliations": [
                "Open Geospatial Consortium",
                "Sustainability Institute"
            ]
        },
        {
            "id": "https://openalex.org/A4227230371",
            "name": "Schwaller, Philippe",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A4224533595",
            "name": "Jablonka, Kevin Maik",
            "affiliations": [
                "Helmholtz Institute Jena",
                "Friedrich Schiller University Jena"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4392678243",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4389991792",
        "https://openalex.org/W4396723768",
        "https://openalex.org/W4404266811",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4403122979",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4383860829",
        "https://openalex.org/W4388685549",
        "https://openalex.org/W4377096651",
        "https://openalex.org/W4385671288",
        "https://openalex.org/W4391561379",
        "https://openalex.org/W4389352201",
        "https://openalex.org/W4391556076",
        "https://openalex.org/W4391800539",
        "https://openalex.org/W4365211638",
        "https://openalex.org/W4387928778",
        "https://openalex.org/W4376166811",
        "https://openalex.org/W4391670863",
        "https://openalex.org/W4400064820",
        "https://openalex.org/W4389300644",
        "https://openalex.org/W4391836235",
        "https://openalex.org/W4391470407",
        "https://openalex.org/W4389761608",
        "https://openalex.org/W4392619039",
        "https://openalex.org/W4385571746",
        "https://openalex.org/W4405616956",
        "https://openalex.org/W4403795293",
        "https://openalex.org/W4391709329",
        "https://openalex.org/W4388067733",
        "https://openalex.org/W4296413526",
        "https://openalex.org/W4220681836",
        "https://openalex.org/W4366736203",
        "https://openalex.org/W4389157262",
        "https://openalex.org/W4285093994",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4378942305",
        "https://openalex.org/W4297632148",
        "https://openalex.org/W4390941772",
        "https://openalex.org/W4387874094",
        "https://openalex.org/W4283031227",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W4403752878",
        "https://openalex.org/W3181995918",
        "https://openalex.org/W4405025319",
        "https://openalex.org/W3023937119",
        "https://openalex.org/W4390011017",
        "https://openalex.org/W4389519108",
        "https://openalex.org/W4389518977",
        "https://openalex.org/W3128121521",
        "https://openalex.org/W4385570852",
        "https://openalex.org/W7072069970",
        "https://openalex.org/W6849590751",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4225405251",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4381797997",
        "https://openalex.org/W4321455981",
        "https://openalex.org/W4388049002",
        "https://openalex.org/W6801984403",
        "https://openalex.org/W6911794112",
        "https://openalex.org/W4380686968",
        "https://openalex.org/W3100220443"
    ],
    "abstract": "Abstract Large language models (LLMs) have gained widespread interest owing to their ability to process human language and perform tasks on which they have not been explicitly trained. However, we possess only a limited systematic understanding of the chemical capabilities of LLMs, which would be required to improve models and mitigate potential harm. Here we introduce ChemBench, an automated framework for evaluating the chemical knowledge and reasoning abilities of state-of-the-art LLMs against the expertise of chemists. We curated more than 2,700 question–answer pairs, evaluated leading open- and closed-source LLMs and found that the best models, on average, outperformed the best human chemists in our study. However, the models struggle with some basic tasks and provide overconfident predictions. These findings reveal LLMs’ impressive chemical capabilities while emphasizing the need for further research to improve their safety and usefulness. They also suggest adapting chemistry education and show the value of benchmarking frameworks for evaluating LLMs in specific domains.",
    "full_text": "Nature Chemistry | Volume 17 | July 2025 | 1027–1034\n 1027\nnature chemistry\nhttps://doi.org/10.1038/s41557-025-01815-x\nArticle\nA framework for evaluating the chemical \nknowledge and reasoning abilities of large \nlanguage models against the expertise  \nof chemists\n \nLarge language models (LLMs) have gained widespread interest owing \nto their ability to process human language and perform tasks on which \nthey have not been explicitly trained. However, we possess only a limited \nsystematic understanding of the chemical capabilities of LLMs, which \nwould be required to improve models and mitigate potential harm. Here \nwe introduce ChemBench, an automated framework for evaluating the \nchemical knowledge and reasoning abilities of state-of-the-art LLMs against \nthe expertise of chemists. We curated more than 2,700 question–answer \npairs, evaluated leading open- and closed-source LLMs and found that the \nbest models, on average, outperformed the best human chemists in our \nstudy. However, the models struggle with some basic tasks and provide \noverconfident predictions. These findings reveal LLMs’ impressive chemical \ncapabilities while emphasizing the need for further research to improve \ntheir safety and usefulness. They also suggest adapting chemistry education \nand show the value of benchmarking frameworks for evaluating LLMs in \nspecific domains.\nLarge language models (LLMs) are machine learning (ML) models \ntrained on massive amounts of text to complete sentences. Aggressive \nscaling of these models has led to a rapid increase in their capabilities1,2, \nwith the leading models now being able to pass the US Medical Licens-\ning Examination3 or other professional licensing exams. They also have \nbeen shown to design and autonomously perform chemical reactions \nwhen augmented with external tools such as web search and synthesis \nplanners4–7. While some see ‘sparks of artificial general intelligence \n(AGI)’ in them8, others see them as ‘stochastic parrots’—that is, systems \nthat only regurgitate what they have been trained on9 and that show \ninherent limitations owing to the way they are trained10. Nevertheless, \nthe promise of these models is that they have shown the ability to solve \na wide variety of tasks they have not been explicitly trained on11–13.\nChemists and materials scientists have quickly caught on to the \nmounting attention given to LLMs, with some voices even suggesting \nthat ‘the future of chemistry is language’ 14. This statement is moti -\nvated by a growing number of reports that use LLMs to predict prop-\nerties of molecules or materials2,15–19, optimize reactions20,21, generate \nmaterials22–25, extract information26–33 or even to prototype systems \nthat can autonomously perform experiments in the physical world \nbased on commands provided in natural language5–7.\nIn addition, since a lot—if not most—of the information about \nchemistry is currently stored and communicated in text, there is a \nstrong reason to believe that there is still a lot of untapped potential in \nLLMs for chemistry and materials science34. For instance, most insights \nin chemical research do not directly originate from data stored in \ndatabases but rather from the scientists interpreting the data. Many \nof these insights are in the form of text in scientific publications. Thus, \noperating on such texts might be our best way of unlocking these \ninsights and learning from them. This might ultimately lead to general \nReceived: 1 April 2024\nAccepted: 26 March 2025\nPublished online: 20 May 2025\n Check for updates\n e-mail: mail@kjablonka.com\nA list of authors and their affiliations appears at the end of the paper\nNature Chemistry | Volume 17 | July 2025 | 1027–1034 1028\nArticle https://doi.org/10.1038/s41557-025-01815-x\nWhile some benchmarks based on university entrance exams54,55 \nor automatic text mining56–58 have been proposed, none of them have \nbeen widely accepted. This is probably because they cannot auto -\nmatically be used with black box (or tool-augmented) systems, do not \ncover a wide range of topics and skills or are not carefully validated by \nexperts. On top of that, the existing benchmarks are not designed to \nbe used with models that support special treatment of molecules or \nequations and do not provide insights on how the models compare \nrelative to experts49.\nIn this work, we report a benchmarking framework (Fig. 1), which \nwe call ChemBench, and use it to reveal the limitations of current fron-\ntier models for use in the chemical sciences. Our benchmark con -\nsists of 2,788 question–answer pairs compiled from diverse sources  \n(1,039 manually generated and 1,749 semi-automatically generated). \nOur corpus measures reasoning, knowledge and intuition across a  \nlarge fraction of the topics taught in undergraduate and graduate \nchemistry curricula. It can be used to evaluate any system that can \nreturn text (that is, including tool-augmented systems).\nT o contextualize the scores, we also surveyed 19 experts in chem-\nistry on a subset of the benchmark corpus to be able to compare the \nperformance of current frontier models with (human) chemists of dif-\nferent specializations. In parts of the survey, the volunteers were also \nallowed to use tools, such as web search, to create a realistic setting.\nResults and discussion\nBenchmark corpus\nT o compile our benchmark corpus, we utilized a broad list of sources \n(Methods), ranging from completely novel, manually crafted questions \nover university exams to semi-automatically generated questions based \non curated subsets of data in chemical databases. For quality assurance, \nall questions have been reviewed by at least two scientists in addition to \nthe original curator and automated checks. Importantly, our large pool \nof questions encompasses a wide range of topics and question types \n(Fig. 2). The topics range from general chemistry to more specialized \nfields such as inorganic, analytical or technical chemistry. We also \nclassify the questions on the basis of what skills are required to answer \nthem. Here, we distinguish between questions that require knowledge, \nreasoning, calculation, intuition or a combination of these. Moreover, \nthe annotator also classifies the questions by difficulty to allow for a \nmore nuanced evaluation of the models’ capabilities.\nWhile many existing benchmarks are designed around multiple- \nchoice questions (MCQ), this does not reflect the reality of chemistry \neducation and research. For this reason, ChemBench samples both \ncopilot systems for chemists that can provide answers to questions or \neven suggest new experiments on the basis of vastly more information \nthan a human could ever read.\nHowever, the rapid increase in capabilities of chemical ML models \nled (even before the recent interest in LLMs) to concerns about the \npotential for the dual use of these technologies, for example, for the \ndesign of chemical weapons35–40. T o some extent, this is not surprising \nas any technology that, for instance, is used to design non-toxic mol-\necules can also be used inversely to predict toxic ones (even though the \nsynthesis would still require access to controlled physical resources \nand facilities). Still, it is essential to realize that the user base of LLMs \nis broader than that of chemistry and materials science experts who \ncan critically reflect on every output these models produce. For exam-\nple, many students frequently consult these tools—perhaps even \nto prepare chemical experiments 41. This also applies to users from \nthe general public, who might consider using LLMs to answer ques -\ntions about the safety of chemicals. Thus, for some users, misleading \ninformation—especially about safety-related aspects—might lead to \nharmful outcomes. However, even for experts, chemical knowledge \nand reasoning capabilities are essential as they will determine the \ncapabilities and limitations of their models in their work, for example, \nin copilot systems for chemists. Unfortunately, apart from exploratory \nreports, such as by prompting leading models with various scientific \nquestions13, there is little systematic evidence on how LLMs perform \ncompared with expert (human) chemists.\nThus, to better understand what LLMs can do for the chemical \nsciences and where they might be improved with further develop -\nments, evaluation frameworks are needed to allow us to measure \nprogress and mitigate potential harms systematically. For the devel -\nopment of LLMs, evaluation is currently primarily performed via \nstandardized benchmark suites such as BigBench 42 or the LM Eval \nHarness 43. Among 204 tasks (such as linguistic puzzles), the former \ncontains only 2 tasks classified as ‘chemistry related’ , whereas the lat-\nter contains no specific chemistry tasks. Owing to the lack of widely \naccepted standard benchmarks, the developers of chemical lan -\nguage models 16,44–47 frequently utilize language-interfaced 48 tabular \ndatasets such as the ones reported in MoleculeNet 49,50, Therapeutic \nData Commons 51, safety databases 52 or MatBench 53. In these cases, \nthe models are evaluated on predicting very specific properties of \nmolecules (for example, solubility, toxicity, melting temperature or \nreactivity) or on predicting the outcome of specific chemical reac-\ntions. This, however, only gives a very limited view of the general \nchemical capabilities of the models.\nAutomatically updated 19 respondents\n251 diverse questions\nchembench.org\nQuestion: What is the number \nof signals in the \n1\nH NMR \nspectrum of the molecule \non the right?  \nAnswer:\nClosed-source models\nOpen-weight models\nDiverse settings\n...\nQuestion: What is the number \nof signals in the 1H NMR \nspectrum of a molecule \nwith the SMILES [START_SMILES]\nOCC1C2CC1(O)C2=O[END_SMILES]?\n...\nAnswer:\n(>2,800 total questions)\nT opic leaders O v er all leaders\n0.57\n0.61\n0.51\nOHHO\nOKnowledge Reasoning Intuition\nSemantic annotation\ncuration \nData preparation Humans Leaderboard\nModels\nCorpus in BIG-bench format\nPeer-reviewed\nFig. 1 | Overview of the ChemBench framework. The different components \nof the ChemBench framework. The framework’s foundation is the benchmark \ncorpus comprising thousands of questions and answers that we manually or \nsemi-automatically compiled from various sources in a format based in the \none introduced in the BIG-bench benchmark (Extended Data Fig. 1). Questions \nare classified on the basis of topics, required skills (reasoning, calculation, \nknowledge and intuition) and difficulty levels. We then used this corpus to \nevaluate the performance of various models and tool-augmented systems using a \ncustom framework. T o provide a baseline, we built a web application that we used \nto survey experts in chemistry. The results of the evaluations are then compiled \nin publicly accessible leaderboards (Supplementary Note 15), which we propose \nas a foundation for evaluating future models.\nNature Chemistry | Volume 17 | July 2025 | 1027–1034\n 1029\nArticle https://doi.org/10.1038/s41557-025-01815-x\nMCQ and open-ended questions (2,544 MCQ and 244 open-ended \nquestions). In addition, ChemBench samples different skills on vari -\nous difficulty levels: from basic knowledge questions (as knowledge \nunderpins reasoning processes59,60) to complex reasoning tasks (such \nas finding out which ions are in a sample given a description of observa-\ntions). We also include questions about chemical intuition, as demon-\nstrating human-aligned preferences is relevant for applications, such \nas hypothesis generation or optimization tasks61.\nChemBench-Mini. It is important to note that a smaller subset of the \ncorpus might be more practical for routine evaluations62. For instance, \nLiang et al. 63 report costs of more than US$10,000 for application \nprogramming interface (API) calls for a single evaluation on the widely \nused Holistic Evaluation of Language Models benchmark. T o address \nthis, we also provide a subset (ChemBench-Mini, 236 questions) of the \ncorpus that was curated to be a diverse and representative subset of \nthe full corpus. While it is impossible to comprehensively represent \nthe full corpus in a subset, we aimed to include a maximally diverse \nset of questions and a more balanced distribution of topics and skills  \n(see Methods for details on the curation process). Our human volun-\nteers answered all the questions in this subset.\nModel evaluation\nBenchmark suite design.  Because the text used in scientific set -\ntings differs from typical natural language, many models have been \ndeveloped that deal with such text in a particular way. For instance, \nthe Galactica model 64 uses special encoding procedures for mol -\necules and equations. Current benchmarking suites, however, do \nnot account for such special treatment of scientific information. T o \naddress this, ChemBench encodes the semantic meaning of various \nparts (for example, chemicals, units or equations) of the question or \nanswer. For instance, molecules represented in simplified molecular \ninput line-entry system (SMILES) are enclosed in [START_SMILES][\\\nEND_SMILES] tags. This allows the model to treat the SMILES string \ndifferently from other text. ChemBench can seamlessly handle such \nspecial treatment in an easily extensible way because the questions \nare stored in an annotated format.\nSince many widely utilized LLM systems only provide access to text \ncompletions (and not the raw model outputs), ChemBench is designed \nto operate on text completions. This is also important given the grow-\ning number of tool-augmented systems that are deemed essential for \nbuilding chemical copilot systems. Such systems can augment the \ncapabilities of LLMs through the use of external tools such as search \nAPIs or code executors65–67. In those cases, the LLM which returns the \nprobabilities for various tokens (that is, text fragments) represents only \none component and it is not clear how to interpret those probabilities \nin the context of the entire system. The text completions, however, \nare the system’s final outputs, which would also be used in a real-world \napplication. Hence, we use them for our evaluations68.\nOverall system performance.  T o understand the current capabili-\nties of LLMs in the chemical sciences, we evaluated a wide range of \nleading models69 on the ChemBench corpus, including systems aug -\nmented with external tools. An overview of the results of this evalua-\ntion is presented in Fig. 3 (all results can be found in Supplementary \nFig. 4 and Supplementary Table 5). In Fig. 3 , we show the percentage \nof questions that the models answered correctly. Moreover, we show \nthe worst, best and average performance of the experts in our study, \nwhich we obtained via a custom web application (chembench.org) that \nwe used to survey the experts. Remarkably, the figure shows that the \nleading LLM, o1-preview, outperforms the best human in our study in \nthis overall metric by almost a factor of two. Many other models also \noutperform the average human performance. Interestingly, Llama-\n3.1-405B-Instruct shows performance that is close to the leading pro-\nprietary models, indicating that new open-source models can also be \ncompetitive with the best proprietary models in chemical settings.\nNotably, we find that models are still limited in their ability to \nanswer knowledge-intensive questions (Supplementary Table 5); that \nis, they did not memorize the relevant facts. Our results indicate that \nthis is not a limitation that could be overcome by simple application \nof retrieval augmented generation systems such as PaperQA2. This is \nprobably because the required knowledge cannot easily be accessed \nvia papers (which is the only type of external knowledge PaperQA2 \nhas access to) but rather by lookup in specialized databases (for \nCalculation\nKnowledge and\ncalculation\nReasoning\nReasoning and\ncalculation\nKnowledge\nMaterial scienceToxicity/safety\n38%\n5%\n9%\n5%\n24%\n2%\n9%\n8%\nTechnical chemistry\nPhysical chemistry\nGeneral chemistry\nAnalytical chemistry\nInorganic chemistry\nOrganic chemistry\nKnowledge and\nreasoning\nFig. 2 | Distribution of topics and required skills. The distribution of questions \nacross various chemistry topics, along with the primary skills required to address \nthem. The topics were manually classified, showing a varied representation \nacross different aspects of chemistry. Each topic is associated with a combination \nof three key skills: calculation, reasoning and knowledge, as indicated by the \ncoloured bars. ChemBench samples encompass diverse topics and diverse skills, \nsetting a high bar for LLMs to demonstrate human-competitive performance \nacross a wide range of chemistry tasks.\nNature Chemistry | Volume 17 | July 2025 | 1027–1034 1030\nArticle https://doi.org/10.1038/s41557-025-01815-x\nexample, PubChem and Gestis), which the humans in our study \nalso used to answer such questions (Supplementary Fig. 17). This \nindicates that there is still room for improving chemical LLMs by \ntraining them on more specialized data sources or integrating them \nwith specialized databases.\nIn addition, our analysis shows that the performance of models is \ncorrelated with their size (Supplementary Fig. 11). This is in line with \nobservations in other domains, but also indicates that chemical LLMs \ncould, to some extent, be further improved by scaling them up.\nPerformance per topic.  T o obtain a more detailed understanding of \nthe performance of the models, we also analysed the performance \nof the models in different subfields of the chemical sciences. For \nthis analysis, we defined a set of topics (Methods) and classified \nall questions in the ChemBench corpus into these topics. We then \ncomputed the percentage of questions that the models or experts \nanswered correctly for each topic and present them in Fig. 4. In this \nspider chart, the worst score for every dimension is zero (no ques-\ntion answered correctly) and the best score is one (all questions \nanswered correctly). Thus, a larger coloured area indicates a better \nperformance.\nOne can observe that this performance varies across models and \ntopics. While general and technical chemistry receive relatively high \nscores for many models, this is not the case for topics such as toxicity \nand safety or analytical chemistry.\nIn the subfield of analytical chemistry, the prediction of the num-\nber of signals observable in a nuclear magnetic resonance spectrum \nproved difficult even for the best models (for example, 22% correct \nanswers for o1-preview). Importantly, while the human experts are \ngiven a drawing of the compounds, the models are only shown the \nSMILES string of a compound and have to use this to reason about \nthe symmetry of the compound (that is, to identify the number of \ndiasterotopically distinct protons, which requires reasoning about \nthe topology and structure of a molecule).\nThese findings also shine an interesting light on the value of \ntextbook-inspired questions. A subset of the questions in ChemBench \nare based on textbooks targeted at undergraduate students. On those \nquestions, the models tend to perform better than on some of our \nsemi-automatically constructed tasks (Supplementary Fig. 5). For \ninstance, while the overall performance in the chemical safety topic \nis low, the models would pass the certification exam according to the \nGerman Chemical Prohibition Ordinance on the basis of a subset of \nquestions we sampled from the corresponding question bank (for \nexample, 71% correct answers for GPT-4, 61% for Claude-3.5 (Sonnet) \nand 3% for the human experts). While those findings are impacted by \nthe subset of questions we sampled, the results still highlight that good \nperformance on such question bank or textbook questions does not \nnecessarily translate to good performance on other questions that \nrequire more reasoning or are further away from the training corpus10. \nThe findings also underline that such exams might have been a good \nsurrogate for the general performance of skills for humans, but their \napplicability in the face of systems that can consume vast amounts of \ndata is up for debate.\nWe also gain insight into the models’ struggles with chemical rea-\nsoning tasks by examining their performance as a function of molecular \ndescriptors. If the model would answer questions after reasoning about \nthe structures, one would expect the performance to depend on the \ncomplexity of the molecules. However, we find that the models’ perfor-\nmance does not correlate with complexity indicators (Supplementary \nNote 5). This indicates that the models may not be able to reason about \nthe structures of the molecules (in the way one might expect) but \ninstead rely on the proximity of the molecules to the training data10.\nIt is important to note that the model performance for some top-\nics, however, is slightly underestimated in the current evaluation. This \nis because models provided via APIs typically have safety mechanisms \nthat prevent them from providing answers that the provider deems \nunsafe. For instance, models might refuse to provide answers about \ncyanides. Statistics on the frequency of such refusals are presented in \nSupplementary Table 8. T o overcome this, direct access to the model \nweights would be required, and we strive to collaborate with the devel-\nopers of frontier models to overcome this limitation in the future. This \nis facilitated by the tooling ChemBench provides, thanks to which con-\ntributors can automatically add new models in an open science fashion.\nJudging chemical preference.  One interesting finding of recent \nresearch is that foundation models can judge interestingness or human \npreferences in some domains61,70. If models could do so for chemical \ncompounds, this would open opportunities for novel optimization \no1-preview\nLowest human score\nAverage human score Highest human score\nClaude-3.5 (Sonnet)\nGPT-4o\nLlama-3.1-405B-Instruct\nMistral-Large-2\nPaperQA2\nLlama-3.1-70B-Instruct\nLlama-3.1-8B-Instruct\nGPT-3.5 Turbo\nFraction of correct answers\n0 0.2 0.4 0.6 0.8\nFig. 3 | Performance of models and humans on ChemBench-Mini.  \nThe percentage of questions that the models answered correctly. Horizontal \nbars indicate the performance of various models and highlight statistics of \nhuman performance. The evaluation we use here is very strict as it only considers \na question answered correctly or incorrectly, partially correct answers are \nalso considered incorrect. Supplementary Fig. 3 provides an overview of the \nperformance of various models on the entire corpus. PaperQA2 (ref. 33) is an \nagentic system that can also search the literature to obtain an answer. We find \nthat the best models outperform all humans in our study when averaged over \nall questions (even though humans had access to tools, such as web search and \nChemDraw, for a subset of the questions).\nAnalytical\nchemistry\nTechnical\nchemistry\nPhysical\nchemistryOrganic\nchemistry\nMaterials\nscience\nInorganic\nchemistry\nGeneral\nchemistry\nToxicity/\nsafety\no1-pr e vie w\nClaude-3. 5 (Sonnet)\nGPT - 4o\nLlama-3. 1- 4 05B-Instruct\nP aperQ A2\nMistr al-Lar ge- 2\nLlama-3. 1- 7 0B-Instruct\nLlama-3. 1-8B-Instruct\nGPT - 3 . 5 T urbo\nHuman (average)\nFig. 4 | Performance of the models and humans on the different topics on \nChemBench-Mini. The radar plot shows the performance of the models and \nhumans on the different topics of ChemBench-Mini. Performance is measured as \nthe fraction of questions that were answered correctly by the models. The best \nscore for every dimension is 1 (all questions answered correctly) and the worst \nis 0 (no question answered correctly). A larger coloured area indicates a better \nperformance. This figure shows the performance on ChemBench-Mini. The \nperformance of models on the entire corpus is presented in Supplementary Fig. 3.\nNature Chemistry | Volume 17 | July 2025 | 1027–1034\n 1031\nArticle https://doi.org/10.1038/s41557-025-01815-x\napproaches. Such open-ended tasks, however, depend on an external \nobserver defining what interestingness is71. Here, we posed models the \nsame question that Choung et al.72 asked chemists at a drug company: \n‘which of the two compounds do you prefer?’ (in the context of an \nearly virtual screening campaign setting; see Supplementary Table 2 \nfor an example). Despite chemists demonstrating a reasonable level \nof inter-rater agreement, our models largely fail to align with expert \nchemists’ preferences. Their performance is often indistinguishable \nfrom random guessing, even though these same models excel in other \ntasks in ChemBench (Supplementary Table 5). This indicates that using \npreference tuning for chemical settings could be a promising approach \nto explore in future research.\nConfidence estimates. One might wonder whether the models can \nestimate if they can answer a question correctly. If they could do so, \nincorrect answers would be less problematic.\nT o investigate this, we prompted68 some of the top-performing \nmodels to estimate, on an ordinal scale, their confidence in their abil-\nity to answer the question correctly (see Methods for details on the \nmethodology and comparison to logit-based approaches).\nIn Fig. 5, we show that for some models, there is no meaningful \ncorrelation between the estimated difficulty and whether the models \nanswered the question correctly or not. For applications in which \nhumans might rely on the models to provide answers with trustworthy \nuncertainty estimates, this is a concerning observation highlighting \nthe need for critical reasoning in the interpretation of the model’s \noutputs34,73. For example, for the questions about the safety profile of \ncompounds, GPT-4 reported a confidence of 1.0 (on a scale of 1–5) for \nthe one question it answered correctly and 4.0 for the six questions it \nanswered incorrectly. While, on average, the verbalized confidence \nestimates from Claude-3.5 (Sonnet) seem better calibrated (Fig. 5 ), \nthey are still misleading in some cases. For example, for the questions \nabout the labelling of chemicals (GHS) pictograms Claude-3.5 (Sonnet) \nreturns an average score of 2.0 for correct answers and 1.83 for incor-\nrect answers.\nConclusions\nOn the one hand, our findings underline the impressive capabilities \nof LLMs in the chemical sciences: leading models outperform domain \nexperts in specific chemistry questions on many topics. On the other \nhand, there are still striking limitations. For very relevant topics, the \nanswers that models provide are wrong. On top of that, many models \nare not able to reliably estimate their own limitations. Yet, the success \nof the models in our evaluations perhaps also reveals more about the \nlimitations of the questions we use to evaluate models—and chem -\nists—than about the models themselves. For instance, while models \nperform well on many textbook questions, they struggle with questions \nrequiring more reasoning about chemical structures (for example, \nnumber of isomers or nuclear magnetic resonance peaks). Given that \nthe models outperformed the average human in our study, we need \nto rethink how we teach and examine chemistry. Critical reasoning is \nincreasingly essential, and rote solving of problems or memorization \nof facts is a domain in which LLMs will continue to outperform humans \n(when trained on the right training corpus).\nOur findings also highlight the nuanced trade-off between breadth \nand depth of evaluation frameworks. The analysis of model perfor -\nmance on different topics shows that models’ performance varies \nwidely across the subfields they are tested on. However, even within a \ntopic, the performance of models can vary widely depending on the \ntype of question and the reasoning required to answer it.\nThe current evaluation frameworks for chemical LLMs are primar-\nily designed to measure the performance of the models on specific \nproperty prediction tasks. They cannot be used to evaluate reasoning \nor systems built for scientific applications. Thus, we had little under-\nstanding of the capabilities of LLMs in the chemical sciences. Our work \nshows that carefully curated benchmarks can provide a more nuanced \n0\n0.25\n0.50\n0.75\n1.00\n0\n1 2 3 4 5 1 2 3 4 5\n0.25\n0.50\n0.75\nFraction correct\nClaude-3.5 (sonnet)\nConfidence estimate\nRelative countStandard deviation\nLlama-3.1-8B-instruct\nGPT-4 GPT-4o\n1.00\nFig. 5 | Reliability and distribution of confidence estimates. For this analysis, \nwe used verbalized confidence estimates from the model. The models were \nprompted to return a confidence score on an ordinal scale to obtain those \nestimates. The line plot shows the average fraction of correctly answered \nquestions for each confidence level. The bar plot shows the distribution of \nconfidence estimates. The error bars indicate the standard deviation for each \nconfidence level (for which the number of samples is given by the height of the \nbar). A confidence estimate would be well calibrated if the average fraction \nof correctly answered questions increases with the confidence level. The \ndashed black line indicates this ideal behaviour, which would be monotonically \nincreasing correctness with higher levels of confidence. We use colours to \ndistinguish the different models, as indicated in the titles of the subplots.  \nWe find that most models are not well calibrated and provide misleading \nconfidence estimates.\nNature Chemistry | Volume 17 | July 2025 | 1027–1034 1032\nArticle https://doi.org/10.1038/s41557-025-01815-x\nunderstanding of the capabilities of LLMs in the chemical sciences. \nImportantly, our findings also illustrate that more focus is required in \ndeveloping better human–model interaction frameworks, given that \nmodels cannot estimate their limitations.\nAlthough our findings indicate many areas for further improve -\nment of LLM-based systems, such as agents (more discussion in Sup-\nplementary Note 11), it is also important to realize that clearly defined \nmetrics have been the key to the progress of many fields of ML, such as \ncomputer vision. Although current systems might be far from reason-\ning like a chemist, our ChemBench framework will be a stepping stone \nfor developing systems that come closer to this goal.\nOnline content\nAny methods, additional references, Nature Portfolio reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author con -\ntributions and competing interests; and statements of data and code \navailability are available at https://doi.org/10.1038/s41557-025-01815-x.\nReferences\n1. Brown, T. et al. Language models are few-shot learners.  \nAdv. Neural Inf. Process. Syst. 33, 1877–1901 (2020).\n2. Zhong, Z., Zhou, K. & Mottin, D. Benchmarking large language \nmodels for molecule prediction tasks. Preprint at https://doi.org/ \n10.48550/arXiv.2403.05075 (2024).\n3. Kung, T. H. et al. Performance of chatgpt on usmle: potential for \nai-assisted medical education using large language models.  \nPLoS Digit. Health 2, e0000198 (2023).\n4. OpenAI et al. Gpt-4 technical report. (2024); https://doi.org/ \n10.48550/arXiv.2303.08774\n5. Boiko, D. A., MacKnight, R., Kline, B. & Gomes, G. Autonomous \nchemical research with large language models. Nature 624, \n570–578 (2023).\n6. M. Bran, A. et al. Augmenting large language models with \nchemistry tools. Nat. Mach. Intell. 6, 525–535 (2024).\n7. Darvish, K. et al. ORGANA: A robotic assistant for automated \nchemistry experimentation and characterization. Matter 8,  \n101897 (2025).\n8. Bubeck, S. et al. Sparks of artificial general intelligence: early \nexperiments with gpt-4. Preprint at https://doi.org/10.48550/ \narXiv.2303.12712 (2023).\n9. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On \nthe dangers of stochastic parrots: can language models be too \nbig? In Proc. 2021 ACM conference on fairness, accountability, and \ntransparency, 610–623 (Association for Computing Machinery, \n2021).\n10. McCoy, R. T., Yao, S., Friedman, D., Hardy, M. D. & Griffiths, T. L. \nEmbers of autoregression show how large language models are \nshaped by the problem they are trained to solve. Proc. Natl Acad. \nSci. USA 121, e2322420121 (2024).\n11. Bommasani, R. et al. On the opportunities and risks of foundation \nmodels. Preprint at https://doi.org/10.48550/arXiv.2108.07258 \n(2021).\n12. Anderljung, M. et al. Frontier ai regulation: managing emerging \nrisks to public safety. Preprint at https://doi.org/10.48550/arXiv. \n2307.03718 (2023).\n13. Microsoft Research AI4Science and Microsoft Azure Quantum. \nThe impact of large language models on scientific discovery:  \na preliminary study using gpt-4. Preprint at https://doi.org/ \n10.48550/arXiv.2311.07361 (2023).\n14. White, A. D. The future of chemistry is language. Nat. Rev. Chem. \n7, 457–458 (2023).\n15. Jablonka, K. M. et al. 14 examples of how llms can transform \nmaterials science and chemistry: a reflection on a large language \nmodel hackathon. Digit. Discov. 2, 1233–1250 (2023).\n16. Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. & Smit, B. \nLeveraging large language models for predictive chemistry.  \nNat. Mach. Intell. 6, 161–169 (2024).\n17. Xie, Z. et al. Fine-tuning gpt-3 for machine learning electronic \nand functional properties of organic molecules. Chem. Sci. 15, \n500–510 (2024).\n18. Liao, C., Yu, Y., Mei, Y. & Wei, Y. From words to molecules: a survey \nof large language models in chemistry. Preprint at https://doi.org/ \n10.48550/arXiv.2402.01439 (2024).\n19. Zhang, D. et al. Chemllm: a chemical large language model. \nPreprint at https://doi.org/10.48550/arXiv.2402.06852(2024).\n20. Ramos, M. C., Michtavy, S. S., Porosoff, M. D. & White, A. D. \nBayesian optimization of catalysts with in-context learning. \nPreprint at https://doi.org/10.48550/arXiv.2304.05341 (2023).\n21. Kristiadi, A. et al. A sober look at LLMs for material discovery: are \nthey actually good for bayesian optimization over molecules? \nIn Proc. 41st International Conference on Machine Learning 1025 \n(JMLR.org, 2024).\n22. Rubungo, A. N., Arnold, C., Rand, B. P. & Dieng, A. B. Llm-prop: \npredicting physical and electronic properties of crystalline solids \nfrom their text descriptions. Preprint at https://doi.org/10.48550/ \narXiv.2310.14029 (2023).\n23. Flam-Shepherd, D. & Aspuru-Guzik, A. Language models can \ngenerate molecules, materials, and protein binding sites directly \nin three dimensions as xyz, cif, and pdb files. Preprint at  \nhttps://doi.org/10.48550/arXiv.2305.05708 (2023).\n24. Gruver, N. et al. Fine-tuned language models generate stable \ninorganic materials as text. In Twelfth International Conference  \non Learning Representations (2024); https://openreview.net/ \nforum?id=vN9fpfqoP1\n25. Alampara, N., Miret, S. & Jablonka, K. M. Mattext: do language \nmodels need more than text & scale for materials modeling? \nPreprint at https://doi.org/10.48550/arXiv.2406.17295 (2024).\n26. Patiny, L. & Godin, G. Automatic extraction of fair data from \npublications using llm. Preprint at ChemRxiv https://doi.org/ \n10.26434/chemrxiv-2023-05v1b-v2 (2023).\n27. Dagdelen, J. et al. Structured information extraction from \nscientific text with large language models. Nat. Commun. 15,  \n1418 (2024).\n28. Zheng, Z. et al. Image and data mining in reticular chemistry \npowered by gpt-4v. Digit. Discov. 3, 491–501 (2024).\n29. Lála, J. et al. Paperqa: retrieval-augmented generative agent for \nscientific research. Preprint at https://doi.org/10.48550/arXiv. \n2312.07559 (2023).\n30. Caufield, J. H. et al. Structured prompt interrogation and recursive \nextraction of semantics (spires): a method for populating \nknowledge bases using zero-shot learning. Bioinformatics 40, \nbtae104 (2024).\n31. Gupta, T. et al. DiSCoMaT: Distantly supervised composition \nextraction from tables in materials science articles. In Proc. \n61st Annual Meeting of the Association for Computational \nLinguistics (Volume 1: Long papers) 13465–13483 (Association for \nComputational Linguistics, 2023).\n32. Schilling-Wilhelmi, M. et al. From text to insight: large language \nmodels for chemical data extraction. Chem. Soc. Rev. 54, \n1125–1150 (2025).\n33. Skarlinski, M. D. et al. Language agents achieve superhuman \nsynthesis of scientific knowledge. Preprint at https://doi.org/ \n10.48550/arXiv.2409.13740 (2024).\n34. Miret, S. & Krishnan, N. Are llms ready for real-world materials \ndiscovery? Preprint at https://doi.org/10.48550/arXiv.2402.05200 \n(2024).\n35. Gopal, A. et al. Will releasing the weights of future large language \nmodels grant widespread access to pandemic agents? Preprint at \nhttps://doi.org/10.48550/arXiv.2310.18233 (2023).\nNature Chemistry | Volume 17 | July 2025 | 1027–1034\n 1033\nArticle https://doi.org/10.1038/s41557-025-01815-x\n36. Ganguli, D. et al. Red teaming language models to reduce harms: \nmethods, scaling behaviors, and lessons learned. Preprint at \nhttps://doi.org/10.48550/arXiv.2209.07858 (2022).\n37. Urbina, F., Lentzos, F., Invernizzi, C. & Ekins, S. Dual use of \nartificial-intelligence-powered drug discovery. Nat. Mach. Intell. 4, \n189–191 (2022).\n38. Campbell, Q. L., Herington, J. & White, A. D. Censoring chemical \ndata to mitigate dual use risk. Preprint at https://doi.org/10.48550/ \narXiv.2304.10510 (2023).\n39. Moulange, R., Langenkamp, M., Alexanian, T., Curtis, S. & \nLivingston, M. Towards responsible governance of biological \ndesign tools. Preprint at https://doi.org/10.48550/arXiv.2311.15936 \n(2023).\n40. Urbina, F., Lentzos, F., Invernizzi, C. & Ekins, S. A teachable \nmoment for dual-use. Nat. Mach. Intell. 4, 607–607 (2022).\n41. One-third of college students used chatgpt for schoolwork  \nduring the 2022-23 academic date. Intelligent.com https://www. \nintelligent.com/one-third-of-college-students-used-chatgpt-for- \nschoolwork-during-the-2022-23-academic-date/ (2023).\n42. Srivastava, A. et al. Beyond the Imitation Game: Quantifying and \nextrapolating the capabilities of language models. Transactions \non Machine Learning Research (2023); https://openreview.net/ \nforum?id=uyTL5Bvosj\n43. Gao, L. et al. A framework for few-shot language model evaluation \nversion v0.4.0. Zenodo https://zenodo.org/records/10256836 \n(2023).\n44. Guo, T. et al. What can large language models do in chemistry?  \nA comprehensive benchmark on eight tasks. Preprint at  \nhttps://doi.org/10.48550/arXiv.2305.18365 (2023).\n45. Ahmad, W., Simon, E., Chithrananda, S., Grand, G. & Ramsundar, B.  \nChemberta-2: towards chemical foundation models. Preprint at \nhttps://doi.org/10.48550/arXiv.2209.01712 (2022).\n46. Cai, X. et al. Comprehensive evaluation of molecule property \nprediction with chatgpt. Methods 222, 133–141 (2024).\n47. Frey, N. C. et al. Neural scaling of deep chemical models.  \nNat. Mach. Intell. 5, 1297–1305 (2023).\n48. Dinh, T. et al. Lift: language-interfaced fine-tuning for \nnon-language machine learning tasks. Adv. Neural Inf. Process. \nSyst. 35, 11763–11784 (2022).\n49. Wu, Z. et al. Moleculenet: a benchmark for molecular machine \nlearning. Chem. Sci. 9, 513–530 (2018).\n50. Huang, Y. et al. Chemeval: a comprehensive multi-level chemical \nevaluation for large language models. Preprint at https://doi.org/ \n10.48550/arXiv.2409.13989 (2024).\n51. Huang, K. et al. Therapeutics data commons: Machine learning \ndatasets and tasks for drug discovery and development. NeurIPS \nDatasets and Benchmarks (2021); https://www.semanticscholar. \norg/paper/Therapeutics-Data-Commons%3A-Machine-Learning- \nand-for-Huang-Fu/54ca116f1e9a45768a3a2c47a4608ff34adefa0c\n52. Zhao, H. et al. Chemsafetybench: benchmarking llm safety on \nchemistry domain. Preprint at https://doi.org/10.48550/ \narXiv.2411.16736 (2024).\n53. Dunn, A., Wang, Q., Ganose, A., Dopp, D. & Jain, A. Benchmarking \nmaterials property prediction methods: the matbench test set \nand automatminer reference algorithm. npj Comput. Mater. 6,  \n138 (2020).\n54. Zaki, M. & Krishnan, N. A. Mascqa: investigating materials science \nknowledge of large language models. Digit. Discov. 3, 313–327 \n(2024).\n55. Arora, D., Singh, H., & Mausam. Have LLMs advanced enough? \nA challenging problem solving benchmark for large language \nmodels. in Proc. 2023 Conference on Empirical Methods in \nNatural Language Processing (eds Bouamor, H., Pino, J. & Bali, K.) \n7527–7543 (Association for Computational Linguistics, 2023); \nhttps://aclanthology.org/2023.emnlp-main.468/\n56. Song, Y., Miret, S., Zhang, H. & Liu, B. Honeybee: progressive \ninstruction finetuning of large language models for materials \nscience. In Findings of the Association for Computational \nLinguistics: EMNLP 2023 (eds Bouamor, H. et al.) 5724–5739 \n(Association for Computational Linguistics, 2023).\n57. Wei, Z. et al. Chemistryqa: a complex question answering  \ndataset from chemistry. OpenReview https://openreview.net/ \nforum?id=oeHTRAehiFF (2021).\n58. Song, Y., Miret, S. & Liu, B. Matsci-nlp: evaluating scientific \nlanguage models on materials science language tasks using \ntext-to-schema modeling. In Proc. 61st Annual Meeting of \nthe Association for Computational Linguistics (eds Rogers, \nA., Boyd-Graber, J. & Okazaki, N.) 3621–3639 (Association for \nComputational Linguistics, 2023).\n59. Hu, X. et al. Towards understanding factual knowledge of large \nlanguage models. In The Twelfth International Conference on \nLearning Representations (2024); https://openreview.net/ \nforum?id=9OevMUdods\n60. Bloom, B. Taxonomy of Educational Objectives: the Classification \nof Educational Goals (Longmans, 1956).\n61. Zhang, J., Lehman, J., Stanley, K. & Clune, J. OMNI: Open- \nendedness via models of human notions of interestingness. In \nTwelfth International Conference on Learning Representations \n(2024); https://openreview.net/forum?id=AgM3MzT99c\n62. Polo, F. M. et al. tinyBenchmarks: evaluating LLMs with fewer \nexamples. In Proc. 41st International Conference on Machine \nLearning (JMLR.org, 2024).\n63. Liang, P. et al. Holistic evaluation of language models. \nTransactions on Machine Learning Research (2023);  \nhttps://openreview.net/forum?id=iO4LZibEqW\n64. Taylor, R. et al. Galactica: a large language model for science. \nPreprint at https://doi.org/10.48550/arXiv.2211.09085 (2022).\n65. Schick, T. et al. Toolformer: language models can teach \nthemselves to use tools. Adv. Neural Inf. Proc. Syst. 36,  \n68539–68551 (2024).\n66. Karpas, E. et al. Mrkl systems: a modular, neuro-symbolic \narchitecture that combines large language models, external \nknowledge sources and discrete reasoning. Preprint at  \nhttps://doi.org/10.48550/arXiv.2205.00445 (2022).\n67. Yao, S. et al. ReAct: Synergizing reasoning and acting in language \nmodels. In Eleventh International Conference on Learning \nRepresentations (OpenReview.net, 2023).\n68. Xiong, M. et al. Can llms express their uncertainty? An empirical \nevaluation of confidence elicitation in llms. In Twelfth International \nConference on Learning Representations (OpenReview.net, 2024); \nhttps://openreview.net/forum?id=gjeQKFxFpZ\n69. Beeching, E. et al. Open llm leaderboard. Hugging Face  \nhttps://huggingface.co/spaces/HuggingFaceH4/open_llm_ \nleaderboard (2023).\n70. Argyle, L. P. et al. Out of one, many: using language models to \nsimulate human samples. Polit. Anal. 31, 337–351 (2023).\n71. Hughes, E. et al. Position: Open-endedness is essential for \nartificial superhuman intelligence. In Proc. 41st International \nConference on Machine Learning Vol. 235 (eds Salakhutdinov, R. \net al.) 20597–20616 (PMLR, 2024); https://proceedings.mlr.press/ \nv235/hughes24a.html\n72. Choung, O.-H., Vianello, R., Segler, M., Stiefl, N. & Jiménez-Luna, J. \nExtracting medicinal chemistry intuition via preference machine \nlearning. Nat. Commun. 14, 6651 (2023).\n73. Li, B. et al. Trustworthy ai: from principles to practices.  \nACM Comput. Surv. 55, 1–46 (2023).\nPublisher’s note Springer Nature remains neutral with regard  \nto jurisdictional claims in published maps and institutional  \naffiliations.\nNature Chemistry | Volume 17 | July 2025 | 1027–1034 1034\nArticle https://doi.org/10.1038/s41557-025-01815-x\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or  \nformat, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons \nlicence, and indicate if changes were made. The images or  \nother third party material in this article are included in the  \narticle’s Creative Commons licence, unless indicated otherwise  \nin a credit line to the material. If material is not included in the \narticle’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you \nwill need to obtain permission directly from the copyright holder. \nTo view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2025\n1Laboratory of Organic and Macromolecular Chemistry, Friedrich Schiller University Jena, Jena, Germany. 2Helmholtz Institute for Polymers in Energy \nApplications Jena (HIPOLE Jena), Jena, Germany. 3Institute of Carbon Science and Technology, CSIC, Oviedo, Spain. 4QpiVolta Technologies Pvt \nLtd, Bengaluru, India. 5Laboratory of Artificial Chemical Intelligence, Institut des Sciences et Ingénierie Chimiques, École Polytechnique Fédérale \nde Lausanne, Lausanne, Switzerland. 6National Centre of Competence in Research Catalysis, École Polytechnique Fédérale de Lausanne, Lausanne, \nSwitzerland. 7Department of Chemical Engineering and Biotechnology, University of Cambridge, Cambridge, UK. 8Macromolecular Chemistry, University \nof Bayreuth, Bayreuth, Germany. 9Laboratory of Molecular Simulation, Institut des Sciences et Ingénierie Chimiques, École Polytechnique Fédérale de \nLausanne, Sion, Switzerland. 10Institute for Inorganic and Analytical Chemistry, Friedrich Schiller University Jena, Jena, Germany. 11Jena Center for Soft \nMatter, Friedrich Schiller University Jena, Jena, Germany. 12Institute for Technical Chemistry and Environmental Chemistry, Friedrich Schiller University \nJena, Jena, Germany. 13Center for Energy and Environmental Chemistry Jena, Friedrich Schiller University Jena, Jena, Germany. 14Intel Labs, Hillsboro,  \nOR, USA. 15Theoretical Chemistry, Technische Universität Dresden, Dresden, Germany. 16OpenBioML.org, London, UK. 17Stability.AI, London, UK.  \n18These authors contributed equally: Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martiño Ríos-García. 19Unaffiliated author: Christina Glaubitz. \n e-mail: mail@kjablonka.com\nAdrian Mirza    1,2,18, Nawaf Alampara    1,18, Sreekanth Kunchapu    1,18, Martiño Ríos-García    1,3,18, Benedict Emoekabu    1, \nAswanth Krishnan    4, Tanya Gupta    5,6, Mara Schilling-Wilhelmi    1, Macjonathan Okereke    1, Anagha Aneesh    1, \nMehrdad Asgari    7, Juliane Eberhardt    8, Amir Mohammad Elahi    9, Hani M. Elbeheiry    10, María Victoria Gil    3, \nChristina Glaubitz19, Maximilian Greiner1, Caroline T . Holick    1,11, Tim Hoffmann    1,11, Abdelrahman Ibrahim    1, \nLea C. Klepsch    1,11, Yannik Köster    1,11, Fabian Alexander Kreth    12,13, Jakob Meyer1, Santiago Miret    14, \nJan Matthias Peschel    1, Michael Ringleb    1,11, Nicole C. Roesner    1,11, Johanna Schreiber    1,11, Ulrich S. Schubert    1,2,11,13, \nLeanne M. Stafast    1,11, A. D. Dinga Wonanke    15, Michael Pieler    16,17, Philippe Schwaller    5,6 & \nKevin Maik Jablonka    1,2,11,13 \nNature Chemistry\nArticle https://doi.org/10.1038/s41557-025-01815-x\nMethods\nCuration workflow\nFor our dataset, we curated questions from existing exams or exercise \nsheets but also programmatically created new questions (see Sup -\nplementary Table 3 for more details). Questions were added via Pull \nRequests on our GitHub repository and only merged into the corpus \nafter passing manual review (Extended Data Fig. 1) as well as automated \nchecks (for example, for compliance with a standardized schema).\nT o ensure that the questions do not enter a training dataset, we \nuse the same canary string as the BigBench project. This requires that \nLLM developers filter their training dataset for this canary string4,42.\nManually curated questions.  Manually curated questions were \nsourced from various sources, including university exams, exercises \nand question banks. Extended Data Table 1 presents an overview of the \nsources of the manually curated questions.\nSemi-programmatically generated questions.  In addition to the \nmanually curated questions, we also generated questions program -\nmatically. An overview of the sources of the semi-programmatically \ngenerated questions is provided in Supplementary Table 3.\nChemical preference data . These questions assess the ability to \nestablish a ‘preference’ , such as favouring a specific molecule. Chemical \npreference is of major importance in drug discovery projects, where \nthe optimization process to reach the desired molecular properties is \na process that takes several years within a chemist’s career. Our data \ncorpus is adapted from the published dataset by Choung et al.72, which \nconsists of more than 5,000 question–answer pairs about chemical \nintuition. T o build the dataset, they presented 35 medicinal chemists \nwith two different molecules, asking them what molecule they would \nlike to continue with when imaging an early virtual screening campaign \nsetting. The question was designed so the scientists do not spend much \ntime answering it, relying only on their feelings or ‘chemical preference’ .\nT o understand whether the capabilities of the leading models align \nwith the preferences of professional chemists, we randomly selected \n1,000 data points from the original dataset to create a meaningful \nevaluation set, where molecules are represented as SMILES. T o ablate \nthe effect of different molecular representations, we only considered \nquestions for which we could obtain International Union of Pure and \nApplied Chemistry names for both the molecules present.\nModel evaluation workflow\nA graphical overview of the pipeline is presented in Supplementary \nFig. 12.\nPrompting. We employ distinct prompt templates tailored for com-\npletion and instruction-tuned models to maintain consistency with \nthe training. As explained later, we impose constraints on the models \nwithin these templates to receive responses in a specific format so \nthat robust, fair and consistent parsing can be performed. Certain \nmodels are trained with special annotations and LaT eX syntax for \nscientific notations, chemical reactions or symbols embedded within \nthe text. For example, all the SMILES representations are encapsulated \nwithin [START_SMILES][\\END_SMILES] in Galactica 64. Our prompt -\ning strategy consistently adheres to these details in a model-specific \nmanner by post-processing LaT eX syntax, chemical symbols, chemical \nequations and physical units (by either adding or removing wrappers). \nThis step can be easily customized in our codebase, and we provide \npresets for the models we evaluated.\nParsing. Our parsing workflow is multistep and primarily based on \nregular expressions. In the case of instruction-tuned models, we first \nidentify the [ANSWER][\\ANSWER] environment that we prompt the \nmodel to report the answer in. In the case of completion models, this \nstep is skipped. From there, we attempt to extract the relevant enu -\nmeration letters (for MCQ) or numbers. In the case of numbers, our \nregular expression was engineered to deal with various forms of scien-\ntific notation. As initial tests indicated that models sometimes return \nintegers in the form of words, for example, ‘one’ instead of ‘1’ , we also \nimplemented a word-to-number conversion using regular expres -\nsions. If these hard-coded parsing steps fail, we use a LLM, for example, \nClaude-3.5 (Sonnet), to parse the completion (Supplementary Note 8 \nprovides more details on this step).\nModels. For all models, we performed inference using greedy decoding \n(that is, temperature 0). We used the API endpoints provided by the \nmodel developers and those provided by Groq. PaperQA2 was used \n(in August 2024) via an API provided by FutureHouse.\nConfidence estimate\nT o estimate the models’ confidence, we prompted them with the ques-\ntion (and answer options for MCQ) and the task to rate their confidence \nto produce the correct answer on a scale from 1 to 5. We decided to use \nverbalized confidence estimates68 since we found those to be closer \nto current practical use cases than other prompting strategies, which \nmight be more suitable when implemented in systems. In addition, \nthis approach captures semantic uncertainty, which is not the same as \nthe probability of a token being given a sequence of tokens (that is, the \nuncertainty one obtains from logit-based approaches). On top of that, \nmany proprietary models do not provide access to the logits, making \nthis approach more general. In Supplementary Note 12, we provide \nmore details and comparisons with a logit-based approach.\nHuman baseline\nQuestion selection. Several design choices were made when selecting \nChemBench-Mini. Firstly, from the full dataset, we kept all the ques-\ntions labelled as advanced. In this way, we can obtain a deeper insight \ninto the capabilities of LLMs on advanced tasks when compared with \nactual chemists. Secondly, we sample a maximum of three questions \nacross all possible combinations of categories (that is, knowledge or \nreasoning) and topics (for example, organic chemistry and physical \nchemistry). Thirdly, we do not include any intuition questions in this \nsubset because the intended use of ChemBench-Mini is to provide a \nfast and fair evaluation of LLMs independent of any human baseline. \nIn total, 236 questions have been sampled for ChemBench-Mini. Then, \nthis set is divided into two subsets on the basis of the aforementioned \ncombinations. One of the question subsets allows tool use, and the \nother does not.\nStudy design.  Human volunteers were asked the questions in a \ncustom-built web interface (Supplementary Note 10), which rendered \nchemicals and equations. Questions were shown in random order, \nand volunteers were not allowed to skip questions. For a subset of the \nquestions, the volunteers were allowed to use external tools (exclud-\ning other LLM or asking other people) to answer the questions. Before \nanswering questions, volunteers were asked to provide information \nabout their education and experience in chemistry. The study was \nconducted in English.\nHuman volunteers. Users were open to reporting about their experi-\nence in chemistry. Overall, 16 did so. Out of those, 2 are beyond a first \npostdoc, 13 have a master’s degree (and are currently enroled in Ph.D. \nstudies) and 1 has a bachelor’s degree. For the analysis, we excluded \nvolunteers with less than 2 years of experience in chemistry after their \nfirst university-level course in chemistry.\nComparison with models. For the analysis, we treated each human \nas a model. We computed the topic aggregated averages per human \nfor analyses grouped by topic and then averaged over all humans. \nNature Chemistry\nArticle https://doi.org/10.1038/s41557-025-01815-x\nThe performance metrics reported for models in the main text are \ncomputed on the same questions that the humans answered. Metrics \nfor the entire corpus are reported in Supplementary Note 4.\nData annotation\nIn the curation of our dataset, we manually assigned difficulty levels \nand required skills to each question. We used the following guidelines \nfor these annotations: calculation is required if answering a question \nwould require the use of a calculator, knowledge is required if answer-\ning a question requires non-trivial knowledge of facts (for example, \nthe H/P statements of chemicals). Reasoning is required if answering \na question requires multiple reasoning steps. Basic questions only \nrequire those skills up to the high school level. Advanced questions \nwould require an expert multiple minutes or hours to answer.\nInclusion and ethics statement\nThe authors confirm that they have complied with all relevant ethi -\ncal regulations, according to the Ethics Commission of the Friedrich \nSchiller University Jena (which decided that the study is ethically safe). \nInformed consent was obtained from all volunteers.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nThe data for ChemBench is available via GitHub at https://github.  \ncom/lamalab-org/chembench and via Zenodo at https://zenodo.org/ \nrecords/14010212 (ref. 74).\nCode availability\nThe code for ChemBench is available via GitHub at https://github.  \ncom/lamalab-org/chembench and via Zenodo at https://zenodo.org/ \nrecords/14010212 (ref. 74) The code for the app for our human base -\nline study is available via GiHub at https://github.com/lamalab-org/  \nchem-bench-app.\nReferences\n74. Mirza, A. et al. chem-bench version v0.2.0. Zenodo https://doi.org/ \n10.5281/zenodo.14010212 (2024).\nAcknowledgements\nThis work was supported by the Carl Zeiss Foundation, and a ‘Talent \nFund’ of the ‘Life’ profile line of the Friedrich Schiller University Jena. \nIn addition, M.S.-W.’s work was supported by Intel and Merck via the \nAWASES programme. Parts of A.M.’s work were supported as part \nof the ‘SOL-AI’ project funded by the Helmholtz Foundation model \ninitiative. K.M.J. is part of the NFDI consortium FAIRmat funded by \nthe Deutsche Forschungsgemeinschaft (the German Research \nFoundation) project no. 460197019. K.M.J. thanks FutureHouse  \n(a non-profit research organization supported by the generosity of \nEric and Wendy Schmidt) for supporting PaperQA2 runs via access \nto the API. We also thank Stability.AI for the access to its HPC cluster. \nM.R.-G. and M.V.G. acknowledge financial support from the Spanish \nAgencia Estatal de Investigación (AEI) through grants TED2021-\n131693B-I00 and CNS2022-135474, funded by Ministerio de Ciencia, \nInnovación y Universidades (MICIU)/AEI/10.13039/501100011033 \nand by the European Union NextGenerationEU/PRTR. M.V.G. \nacknowledges support from the Spanish National Research Council \nthrough the Programme for internationalization i-LINK 2023 (project \nno. ILINK23047). A.A. gratefully acknowledges financial support \nfor this research by the Fulbright US Student Programme, which is \nsponsored by the US Department of State and German-American \nFulbright Commission. Its contents are solely the responsibility \nof the author and do not necessarily represent the official views \nof the Fulbright Programme, the Government of the USA or the \nGerman-American Fulbright Commission. M.A. expresses gratitude \nto the European Research Council for evaluating the project with \nthe reference no. 101106377 titled ‘CLARIFIER’, and accepting it for \nfunding under the HORIZON TMA MSCA Postdoctoral Fellowships—\nEuropean Fellowships. Furthermore, M.A. acknowledges the funding \nprovided by UK Research and Innovation under the UK government’s \nHorizon Europe funding guarantee (grant reference EP/Y023447/1; \norganization reference 101106377). M.R. and U.S.S. thank the ‘Deutsche \nForschungsgemeinschaft’ for funding under the regime of the priority \nprogramme SPP 2363 ‘Utilization and Development of Machine \nLearning for Molecular Applications—Molecular Machine Learning’ \n(SCHU 1229/63-1; project no. 497115849). A.D.D.W. acknowledges \nfunding from the European Union Horizon 2020 research and \ninnovation programme under the Marie Skłodowska-Curie grant \nagreement no. 101107360. P.S. acknowledges support from the \nNational Centre of Competence in Research Catalysis (grant no. \n225147), a National Centre of Competence in Research grant funded \nby the Swiss National Science Foundation. In addition, we thank the \nOpenBioML.org community and their ChemNLP project team for \nvaluable discussions. Moreover, we thank P. Márquez for discussions \nand support and J. Kimmig for feedback on the web app. In addition, \nwe acknowledge support from S. Kumar with an initial prototype of  \nthe web app. We thank B. Smit for feedback on an early version of  \nthe manuscript.\nAuthor contributions\nA.M., N.A., M.R.-G. and K.M.J. contributed to the software \ndevelopment of the benchmarking framework. K.M.J. wrote the article \nwith help from A.M., N.A., S.K., and M.R.-G. A.K. wrote the software for \nchembench.org. A.M., N.A., S.K., M.R.-G., K.M.J., T.G., M.S.-W., M.V.G., \nB.E. and M.O. contributed to the generation of the question dataset. \nA.A., A.M.E., M.A., J.E., H.M.E., M.V.G., M.G., C.T.H., C.G., T.H., A.I., L.C.K., \nY.K., F.A.K., J.M., S.M., J.M.P., M.R., N.C.R., J.S., L.M.S. and A.D.D.W. \nanswered the question dataset for the human benchmark tests. U.S.S. \nand P.S. contributed to supervision and funding acquisition. K.M.J. \ndirected the project and conceptualized it with P.S., M.P., A.M., N.A., \nM.R.-G. and S.K. All authors reviewed and edited the manuscript.\nFunding\nOpen access funding provided by Friedrich-Schiller-Universität Jena.\nCompeting interests\nK.M.J. has been a paid contractor for OpenAI (as part of the red \nteaming network). M.P. is an employee of Stability.AI, and A.M. and \nN.A. were paid contractors of Stability.AI. The remaining authors \ndeclare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s41557-025-01815-x.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s41557-025-01815-x.\nCorrespondence and requests for materials should be addressed to \nKevin Maik Jablonka.\nPeer review information Nature Chemistry thanks Joshua Schrier and \nthe other, anonymous, reviewer(s) for their contribution to the peer \nreview of this work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nNature Chemistry\nArticle https://doi.org/10.1038/s41557-025-01815-x\nExtended Data Table 1 | Overview of sources of the curated questions\nSource Count\nSemi-automatically generated 1749\nURL 375\nTextbook 206\nExam 149\nIChO 149\nNo source 139\nLectures 21\nThe table provides an overview of the types of sources the questions have been curated from. Detailed sources are available in the source data on GitHub. Questions without a source have \nbeen curated completely from scratch. Questions based on lecture notes or URLs have been curated based on content presented in those resources. All questions have been rephrased, \nannotated, and reviewed before being added to the corpus.\nNature Chemistry\nArticle https://doi.org/10.1038/s41557-025-01815-x\nExtended Data Fig. 1 | Overview of the workflow for the assembly of the \nChemBench Corpus. T o assemble the ChemBench corpus, we first collected \nquestions from various sources. Some tasks were manually curated, others semi-\nprogrammatically. We added semantic annotations for all questions to make \nthem compatible with systems that use special processing for modalities that \nare not conventional natural text. We reviewed the questions using manual and \nautomatic methods before adding them to the corpus.\n1 nature portfolio  |  reporting summaryApril 2023\nCorresponding author(s): Jablonka\nLast updated by author(s): Jan 27, 2025\nReporting Summary\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection Provide a description of all commercial, open source and custom code used to collect the data in this study, specifying the version used OR \nstate that no software was used.\nData analysis For data analysis we used custom code that can be found at https://github.com/lamalab-org/chembench-paper and https://github.com/\nlamalab-org/chem-bench. We used version v0.2.0 to generate the results in the paper. \nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A description of any restrictions on data availability \n- For clinical datasets or third party data, please ensure that the statement adheres to our policy \n \nThe data for ChemBench is available at https://github.com/lamalab-org/chem-bench and archived on Zenodo under https://zenodo.org/records/14010212.74 A \nreproducible version of this manuscript, archived at was generated using the showyourwork framework.\n2 nature portfolio  |  reporting summaryApril 2023\nResearch involving human participants, their data, or biological material\nPolicy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation), \nand sexual orientation and race, ethnicity and racism.\nReporting on sex and gender Sex and gender have not been considered in the study design. \nReporting on race, ethnicity, or \nother socially relevant \ngroupings\nWe did not collected fine-grained personal information to avoid dealing with personal information.  \n \nPopulation characteristics Users were open to reporting about their experience in chemistry. Overall, 16 did so. Out of those, 2 are beyond a first \npostdoc, 13 have a master’s degree (and are currently enrolled in Ph.D. studies), and 1 has a bachelor’s degree. For the \nanalysis, we excluded volunteers with less than two years of experience in chemistry after their first university-level course in \nchemistry.\nRecruitment The study was conducted as an only survey that was advertised via email to student and faculty bodies of the EPFL and the \nFriedrich-Schiller University Jena. The email made clear that participation is voluntarily and used for a benchmarking study. \nThere is a potential self-selection bias for participants interested in LLMs and chemical questions.\nEthics oversight The authors confirm to have complied with all relevant ethics regulations (no personal data was recorded). The institutional \nreview board of the Friedrich-Schiller University of Jena was consulted. \nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nBehavioural & social sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nStudy description Study type: Observational study evaluating human expert performance in chemistry-related questions  \nSetting: Web-based survey using chembench.org platform  \nPurpose: To benchmark human expert performance against LLM capabilities in chemistry \nResearch sample Sample size: 19 expert chemists participated in the study  \n \nDemographics (out of 16 who reported their experience): \n2 were beyond first postdoc \n13 had master's degrees and were enrolled in PhD studies \n1 had a bachelor's degree  \nSampling strategy No sample size calculation was performed. We recruited as many participants as we could. \nData collection Method: Custom web application (chembench.org) was used to survey the experts  \n \nFormat: \nQuestions presented through web interface \nMolecules shown as rendered drawings and SMILES strings \nLaTeX equations and chemical equations rendered using MathJax \nTime taken to answer questions was recorded \nTool usage was tracked\nTiming 02.09.2024-13.09.2024\nData exclusions Pre-established criteria: Volunteers with less than two years of experience in chemistry after their first university-level chemistry \ncourse were excluded from analysis  \n \nRationale: To ensure participants had sufficient expertise in chemistry\nNon-participation No participant dropped out\nRandomization Questions were presented to participants in random order  \n3 nature portfolio  |  reporting summaryApril 2023\nRandomization  \nTool-use questions were separated from non-tool-use questions\nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nClinical data\nDual use research of concern\nPlants\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging\nAntibodies\nAntibodies used Describe all antibodies used in the study; as applicable, provide supplier name, catalog number, clone name, and lot number.\nValidation Describe the validation of each primary antibody for the species and application, noting any validation statements on the \nmanufacturer’s website, relevant citations, antibody profiles in online databases, or data provided in the manuscript.\nEukaryotic cell lines\nPolicy information about cell lines and Sex and Gender in Research\nCell line source(s) State the source of each cell line used and the sex of all primary cell lines and cells derived from human participants or \nvertebrate models.\nAuthentication Describe the authentication procedures for each cell line used OR declare that none of the cell lines used were authenticated.\nMycoplasma contamination Confirm that all cell lines tested negative for mycoplasma contamination OR describe the results of the testing for \nmycoplasma contamination OR declare that the cell lines were not tested for mycoplasma contamination.\nCommonly misidentified lines\n(See ICLAC register)\nName any commonly misidentified cell lines used in the study and provide a rationale for their use.\nPalaeontology and Archaeology\nSpecimen provenance Provide provenance information for specimens and describe permits that were obtained for the work (including the name of the \nissuing authority, the date of issue, and any identifying information). Permits should encompass collection and, where applicable, \nexport.\nSpecimen deposition Indicate where the specimens have been deposited to permit free access by other researchers.\nDating methods If new dates are provided, describe how they were obtained (e.g. collection, storage, sample pretreatment and measurement), where \nthey were obtained (i.e. lab name), the calibration program and the protocol for quality assurance OR state that no new dates are \nprovided.\nTick this box to confirm that the raw and calibrated dates are available in the paper or in Supplementary Information.\nEthics oversight Identify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance \nwas required and explain why not.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\n4 nature portfolio  |  reporting summaryApril 2023\nAnimals and other research organisms\nPolicy information about studies involving animals; ARRIVE guidelines recommended for reporting animal research, and Sex and Gender in \nResearch\nLaboratory animals For laboratory animals, report species, strain and age OR state that the study did not involve laboratory animals.\nWild animals Provide details on animals observed in or captured in the field; report species and age where possible. Describe how animals were \ncaught and transported and what happened to captive animals after the study (if killed, explain why and describe method; if released, \nsay where and when) OR state that the study did not involve wild animals.\nReporting on sex Indicate if findings apply to only one sex; describe whether sex was considered in study design, methods used for assigning sex. \nProvide data disaggregated for sex where this information has been collected in the source data as appropriate; provide overall \nnumbers in this Reporting Summary. Please state if this information has not been collected.  Report sex-based analyses where \nperformed, justify reasons for lack of sex-based analysis.\nField-collected samples For laboratory work with field-collected samples, describe all relevant parameters such as housing, maintenance, temperature, \nphotoperiod and end-of-experiment protocol OR state that the study did not involve samples collected from the field.\nEthics oversight Identify the organization(s) that approved or provided guidance on the study protocol, OR state that no ethical approval or guidance \nwas required and explain why not.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nClinical data\nPolicy information about clinical studies\nAll manuscripts should comply with the ICMJE guidelines for publication of clinical research and a completed CONSORT checklist must be included with all submissions.\nClinical trial registration Provide the trial registration number from ClinicalTrials.gov or an equivalent agency.\nStudy protocol Note where the full trial protocol can be accessed OR if not available, explain why.\nData collection Describe the settings and locales of data collection, noting the time periods of recruitment and data collection.\nOutcomes Describe how you pre-defined primary and secondary outcome measures and how you assessed these measures.\nDual use research of concern\nPolicy information about dual use research of concern\nHazards\nCould the accidental, deliberate or reckless misuse of agents or technologies generated in the work, or the application of information presented \nin the manuscript, pose a threat to:\nNo Yes\nPublic health\nNational security\nCrops and/or livestock\nEcosystems\nAny other significant area\n5 nature portfolio  |  reporting summaryApril 2023\nExperiments of concern\nDoes the work involve any of these experiments of concern:\nNo Yes\nDemonstrate how to render a vaccine ineffective\nConfer resistance to therapeutically useful antibiotics or antiviral agents\nEnhance the virulence of a pathogen or render a nonpathogen virulent\nIncrease transmissibility of a pathogen\nAlter the host range of a pathogen\nEnable evasion of diagnostic/detection modalities\nEnable the weaponization of a biological agent or toxin\nAny other potentially harmful combination of experiments and agents\nNovel plant genotypes Describe the methods by which all novel plant genotypes were produced. This includes those generated by transgenic approaches, \ngene editing, chemical/radiation-based mutagenesis and hybridization. For transgenic lines, describe the transformation method, the \nnumber of independent lines analyzed and the generation upon which experiments were performed. For gene-edited lines, describe \nthe editor used, the endogenous sequence targeted for editing, the targeting guide RNA sequence (if applicable) and how the editor \nwas applied.\nSeed stocks Report on the source of all seed stocks or other plant material used. If applicable, state the seed stock centre and catalogue number. If \nplant specimens were collected from the field, describe the collection location, date and sampling procedures.\nAuthentication Describe any authentication procedures for each seed stock used or novel genotype generated. Describe any experiments used to \nassess the effect of a mutation and, where applicable, how potential secondary effects (e.g. second site T-DNA insertions, mosiacism, \noff-target gene editing) were examined.\nPlants\nChIP-seq\nData deposition\nConfirm that both raw and final processed data have been deposited in a public database such as GEO.\nConfirm that you have deposited or provided access to graph files (e.g. BED files) for the called peaks.\nData access links \nMay remain private before publication.\nFor \"Initial submission\" or \"Revised version\" documents, provide reviewer access links.  For your \"Final submission\" document, \nprovide a link to the deposited data.\nFiles in database submission Provide a list of all files available in the database submission.\nGenome browser session \n(e.g. UCSC)\nProvide a link to an anonymized genome browser session for \"Initial submission\" and \"Revised version\" documents only, to \nenable peer review.  Write \"no longer applicable\" for \"Final submission\" documents.\nMethodology\nReplicates Describe the experimental replicates, specifying number, type and replicate agreement.\nSequencing depth Describe the sequencing depth for each experiment, providing the total number of reads, uniquely mapped reads, length of reads and \nwhether they were paired- or single-end.\nAntibodies Describe the antibodies used for the ChIP-seq experiments; as applicable, provide supplier name, catalog number, clone name, and \nlot number.\nPeak calling parameters Specify the command line program and parameters used for read mapping and peak calling, including the ChIP, control and index files \nused.\nData quality Describe the methods used to ensure data quality in full detail, including how many peaks are at FDR 5% and above 5-fold enrichment.\nSoftware Describe the software used to collect and analyze the ChIP-seq data. For custom code that has been deposited into a community \nrepository, provide accession details.\n6 nature portfolio  |  reporting summaryApril 2023\nFlow Cytometry\nPlots\nConfirm that:\nThe axis labels state the marker and fluorochrome used (e.g. CD4-FITC).\nThe axis scales are clearly visible. Include numbers along axes only for bottom left plot of group (a 'group' is an analysis of identical markers).\nAll plots are contour plots with outliers or pseudocolor plots.\nA numerical value for number of cells or percentage (with statistics) is provided.\nMethodology\nSample preparation Describe the sample preparation, detailing the biological source of the cells and any tissue processing steps used.\nInstrument Identify the instrument used for data collection, specifying make and model number.\nSoftware Describe the software used to collect and analyze the flow cytometry data. For custom code that has been deposited into a \ncommunity repository, provide accession details.\nCell population abundance Describe the abundance of the relevant cell populations within post-sort fractions, providing details on the purity of the \nsamples and how it was determined.\nGating strategy Describe the gating strategy used for all relevant experiments, specifying the preliminary FSC/SSC gates of the starting cell \npopulation, indicating where boundaries between \"positive\" and \"negative\" staining cell populations are defined.\nTick this box to confirm that a figure exemplifying the gating strategy is provided in the Supplementary Information.\nMagnetic resonance imaging\nExperimental design\nDesign type Indicate task or resting state; event-related or block design.\nDesign specifications Specify the number of blocks, trials or experimental units per session and/or subject, and specify the length of each trial \nor block (if trials are blocked) and interval between trials.\nBehavioral performance measures State number and/or type of variables recorded (e.g. correct button press, response time) and what statistics were used \nto establish that the subjects were performing the task as expected (e.g. mean, range, and/or standard deviation across \nsubjects).\nAcquisition\nImaging type(s) Specify: functional, structural, diffusion, perfusion.\nField strength Specify in Tesla\nSequence & imaging parameters Specify the pulse sequence type (gradient echo, spin echo, etc.), imaging type (EPI, spiral, etc.), field of view, matrix size, \nslice thickness, orientation and TE/TR/flip angle.\nArea of acquisition State whether a whole brain scan was used OR define the area of acquisition, describing how the region was determined.\nDiffusion MRI Used Not used\nPreprocessing\nPreprocessing software Provide detail on software version and revision number and on specific parameters (model/functions, brain extraction, \nsegmentation, smoothing kernel size, etc.).\nNormalization If data were normalized/standardized, describe the approach(es): specify linear or non-linear and define image types used for \ntransformation OR indicate that data were not normalized and explain rationale for lack of normalization.\nNormalization template Describe the template used for normalization/transformation, specifying subject space or group standardized space (e.g. \noriginal Talairach, MNI305, ICBM152) OR indicate that the data were not normalized.\nNoise and artifact removal Describe your procedure(s) for artifact and structured noise removal, specifying motion parameters, tissue signals and \nphysiological signals (heart rate, respiration).\n7 nature portfolio  |  reporting summaryApril 2023\nVolume censoring Define your software and/or method and criteria for volume censoring, and state the extent of such censoring.\nStatistical modeling & inference\nModel type and settings Specify type (mass univariate, multivariate, RSA, predictive, etc.) and describe essential details of the model at the first and \nsecond levels (e.g. fixed, random or mixed effects; drift or auto-correlation).\nEffect(s) tested Define precise effect in terms of the task or stimulus conditions instead of psychological concepts and indicate whether \nANOVA or factorial designs were used.\nSpecify type of analysis: Whole brain ROI-based Both\nStatistic type for inference\n(See Eklund et al. 2016)\nSpecify voxel-wise or cluster-wise and report all relevant parameters for cluster-wise methods.\nCorrection Describe the type of correction and how it is obtained for multiple comparisons (e.g. FWE, FDR, permutation or Monte Carlo).\nModels & analysis\nn/a Involved in the study\nFunctional and/or effective connectivity\nGraph analysis\nMultivariate modeling or predictive analysis\nFunctional and/or effective connectivity Report the measures of dependence used and the model details (e.g. Pearson correlation, partial correlation, \nmutual information).\nGraph analysis Report the dependent variable and connectivity measure, specifying weighted graph or binarized graph, \nsubject- or group-level, and the global and/or node summaries used (e.g. clustering coefficient, efficiency, \netc.).\nMultivariate modeling and predictive analysis Specify independent variables, features extraction and dimension reduction, model, training and evaluation \nmetrics."
}