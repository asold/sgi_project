{
  "title": "Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling",
  "url": "https://openalex.org/W4389524393",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2585193371",
      "name": "Xiuying Wei",
      "affiliations": [
        "Beihang University",
        "Laboratoire d'Informatique Fondamentale de Lille"
      ]
    },
    {
      "id": "https://openalex.org/A2501199422",
      "name": "Yunchen Zhang",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2095608922",
      "name": "Yuhang Li",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2111967072",
      "name": "Xiangguo Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2967782395",
      "name": "Ruihao Gong",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2527874542",
      "name": "Jinyang Guo",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2095699825",
      "name": "Xianglong Liu",
      "affiliations": [
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W3114304470",
    "https://openalex.org/W4226087293",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W3166859509",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3035078980",
    "https://openalex.org/W3003175769",
    "https://openalex.org/W4366341968",
    "https://openalex.org/W2998218113",
    "https://openalex.org/W4327810129",
    "https://openalex.org/W4385890133",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W4312056202",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2786771851",
    "https://openalex.org/W2884150179",
    "https://openalex.org/W3202028501",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2953235111",
    "https://openalex.org/W4307934016",
    "https://openalex.org/W2981751377",
    "https://openalex.org/W3175752238",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W4297812065",
    "https://openalex.org/W3166874749",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4221162983",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4221148635",
    "https://openalex.org/W4281651027",
    "https://openalex.org/W2982041622",
    "https://openalex.org/W4292760969",
    "https://openalex.org/W3175491384",
    "https://openalex.org/W3034887213",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3017746288",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4306295203",
    "https://openalex.org/W4378770930",
    "https://openalex.org/W4379260375",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4221159612",
    "https://openalex.org/W4387635190",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W1902934009",
    "https://openalex.org/W4225737925",
    "https://openalex.org/W2896718327",
    "https://openalex.org/W3204296682",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3034940165",
    "https://openalex.org/W3215741372",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W3004061291",
    "https://openalex.org/W4296565034",
    "https://openalex.org/W4287812978",
    "https://openalex.org/W4281479579",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W4297948009",
    "https://openalex.org/W2916954108"
  ],
  "abstract": "Post-training quantization (PTQ) of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are concentrated in specific channels and are asymmetric across channels. To address this issue, we propose the Outlier Suppression+ (OS+) framework, which contains the channel-wise shifting for asymmetry and channel-wise scaling for concentration. We show that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we propose a fast and stable scheme to calculate effective shifting and scaling values. The channel-wise shifting aligns the center of each channel for removal of outlier asymmetry. The channel-wise scaling quantitatively evaluates changes brought by migration and quantization for better quantization burden balance. We validate our OS+ under both standard and fine-grained quantization settings with models including BERT, OPT, BLOOM, BLOOMZ, and LLaMA. Comprehensive results across various tasks demonstrate the superiority of our approach. Especially, with standard quantization, OS+ can achieve near-floating-point performance on both small models and large language models on 8-bit and 6-bit. Besides, we establish a new state-of-the-art for 4-bit BERT with 15.5% improvement. Our code is available at https://github.com/ModelTC/Outlier_Suppression_Plus.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1648–1665\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nOutlier Suppression+: Accurate quantization of large language models by\nequivalent and effective shifting and scaling\nXiuying Wei1, 2, 3 , Yunchen Zhang2, 5 , Yuhang Li4 , Xiangguo Zhang2,\nRuihao Gong1, 2∗, Jinyang Guo1 , Xianglong Liu1\n1State Key Lab of Software Development Environment, Beihang University\n2SenseTime Research 3School of Computer and Communication Sciences, EPFL\n4Yale University,5UESTC\nxiuying.wei@epfl.ch, yuhang.li@yale.edu, {jinyangguo, xlliu}@buaa.edu.cn\n{zhangyunchen, zhangxiangguo, gongruihao}@sensetime.com\nAbstract\nPost-training quantization (PTQ) of trans-\nformer language models faces significant\nchallenges due to the existence of detrimental\noutliers in activations. We observe that\nthese outliers are concentrated in specific\nchannels and are asymmetric across chan-\nnels. To address this issue, we propose the\nOutlier Suppression+ (OS+) framework,\nwhich contains the channel-wise shifting\nfor asymmetry and channel-wise scaling for\nconcentration. We show that these operations\ncan be seamlessly migrated into subsequent\nmodules while maintaining equivalence.\nSecond, we propose a fast and stable scheme to\ncalculate effective shifting and scaling values.\nThe channel-wise shifting aligns the center of\neach channel for removal of outlier asymmetry.\nThe channel-wise scaling quantitatively\nevaluates changes brought by migration and\nquantization for better quantization burden\nbalance. We validate our OS+ under both\nstandard and fine-grained quantization settings\nwith models including BERT, OPT, BLOOM,\nBLOOMZ, and LLaMA. Comprehensive\nresults across various tasks demonstrate\nthe superiority of our approach. Especially,\nwith standard quantization, OS+ can achieve\nnear-floating-point performance on both\nsmall models and large language models\non 8-bit and 6-bit. Besides, we establish\na new state-of-the-art for 4-bit BERT with\n15.5% improvement. Our code is available\nat https://github.com/ModelTC/\nOutlier_Suppression_Plus.\n1 Introduction\nTransformer language models (e.g., BERT, LLMs)\nhave garnered significant attention due to their re-\nmarkable performance and scalable model size.\nThese models have evolved from hundreds of mil-\nlions of parameters (Devlin et al., 2018; Liu et al.,\n∗Corresponding author.\n2019; Radford et al., 2018) to hundreds of billions\nof parameters (Brown et al., 2020; Zhang et al.,\n2022; Smith et al., 2022). This necessitates the\nemployment of compression techniques (Han et al.,\n2015; Hinton et al., 2015; Zoph and Le, 2016; Le-\nCun et al., 1989) for practical deployment. Among\nthese techniques, quantization (Jacob et al., 2018)\nhas emerged as a general and primary paradigm for\nreducing both memory footprint and computation\noverhead.\nHowever, quantization, particularly post-training\nquantization (Choukroun et al., 2019; Banner et al.,\n2018; Wu et al., 2020) under the setting of limited\ndata and GPU resources, has become increasingly\nchallenging on these models (e.g., a 12% accu-\nracy drop in BERT (Bondarenko et al., 2021) and\ncatastrophic degradation in OPT-175B (Dettmers\net al., 2022)). This is caused by the presence of\ndetrimental outliers in activation (e.g., the range\nof distribution can be 80 in BERT and even 140\nin OPTs), which prevents discrete numbers from\naccurately representing continuous ones.\nTo combat the bottleneck, researchers make in-\ndepth investigations and find that outliers mainly\nconcentrate on certain channels. Some works (Bon-\ndarenko et al., 2021; Dettmers et al., 2022) suggest\nfine-grained quantization schemes and offer extra\nbit levels for outlier channels. Others (Wei et al.,\n2022b; Xiao et al., 2022) take the activation scal-\ning to scale outliers and migrate scaling values to\nsubsequent weights for FP equivalence. However,\nthe former might hurt the quantization accelera-\ntion effect while the latter determines scaling val-\nues without the consideration of minimizing the\nchange introduced by migration and quantization,\nwhich we find is sub-optimal. Meanwhile, we also\nidentify a new outlier characteristic that previous\nworks overlooked but is also responsible for the\nlarge tensor range.\nIn this paper, we propose the Outlier Suppres-\nsion+ framework composed of channel-wise shift-\n1648\n-1.5    1.5\n-100 60            -30                                  30         -10             10                  \n…                          …                 ...                      -97      -58\n-1.7     1.45.7       43\n-0.9       1.4-20                         20\n-1.5     1.5-19                          19-5 5\n-5     5\n-1.2     1.2\n6353635487258726 -1.2     1.2\nChannelindex\nRange\n(a) Original distribution\n-1.5    1.5\n-100 60            -30                                  30         -10             10                  \n…                          …                 ...                      -97      -58\n-1.7     1.45.7       43\n-0.9       1.4-20                         20\n-1.5     1.5-19                          19-5 5\n-5     5\n-1.2     1.2\n6353635487258726 -1.2     1.2\nChannelindex\nRange (b) Channel-wise shifting\n-1.5    1.5\n-100 60            -30                                  30         -10             10                  \n…                          …                 ...                      -97      -58\n-1.7     1.45.7       43\n-0.9       1.4-20                         20\n-1.5     1.5-19                          19-5 5\n-5     5\n-1.2     1.2\n6353635487258726 -1.2     1.2\nChannelindex\nRange (c) Channel-wise scaling\nFigure 1: Distribution of OPT-66B. Fig. 1a shows the original distribution with asymmetric outliers consistently occurs at certain\nchannels, owning considerable range (-97, 43). Fig. 1b depicts the channel-wise shifting operation to decrease the tensor range\nby eliminating the asymmetry. Fig. 1c further scales down the outliers to threshold 5 and finally results in a distribution ranging\nfrom -5 to 5.\ning and scaling to effectively pursue better quanti-\nzation performance while equivalently keeping the\nFP output. First, we find a new feature of outliers\nthat they stay in asymmetric shape across chan-\nnels (e.g., in Fig. 1a, one problematic channel on\nOPT-66B occupies the negative axis from -97 to\n-58 while another one has positive values ranging\nfrom 5.7 to 43). This outlier asymmetric presenta-\ntion could cause a significantly wide distribution of\ntensor like 140 even composed of channels with rel-\natively small ranges like 39. Thus, we propose the\nchannel-wise shifting operation, which shifts the\nactivation across channels to eliminate the impact\nof asymmetry. Together with channel-wise scal-\ning for concentrated outliers, a unified migration\npattern is introduced to seamlessly transfer the re-\nversed effects of these operations to later modules\nto maintain equivalent FP models. Second, we de-\nvise deliberate schemes to determine effective shift-\ning and scaling values. The shifting vector aligns\nthe center of each channel, reducing the whole\ntensor range to its maximum channel range. The\nscaling values quantitatively minimize the interac-\ntive output change of the activation and weights\ninduced by migration and quantization, achieving a\nbalanced quantization burden with a fast and stable\nsearch procedure.\nOur algorithm can be carried out efficiently and\nenjoy affordability on real hardware, producing\nmore quantization-friendly models in minutes and\nrequiring no extra inference burden on LLMs. To\nthis end, our main contributions can be summarized\ninto three aspects:\n1. We find a new feature of outliers that show asym-\nmetric shapes across channels and then propose\nthe channel-wise shifting operation, along with\ntaking channel-wise scaling for the outlier con-\ncentration attribute. A unified migration pattern\nthat migrates their reversed effects to later mod-\nules is designed to guarantee an equivalent FP\nnetwork.\n2. We propose fast and stable ways to determine\neffective shifting and scaling values. Shifting\nvalues eliminate the asymmetry feature across\nchannels while scaling values scale down out-\nlier channels towards a quantitative optimization\nobjective.\n3. We assess the efficacy of our approach under\nboth standard and fine-grained quantization set-\ntings. On standard one, OS+ achieves near-\nfloating-point performance on 8-bit and 6-bit\nBERT, OPTs, BLOOM, and BLOOMZ. On fine-\ngrained one, OS+ can surpass others by 9.41%\non 4-bit LLaMA with per-token quantization\nand obtain lossless results on 4-bit OPT with\nper-group quantization.\n2 Related work\nDue to the space limit, we give the most relevant\npapers here and put a complete related work in\nthe Appendix A. In the realm of PTQ, researchers\nhave discovered that the poor performance of trans-\nformer language models should be attributed to\nextreme outliers in activations, which exhibit spe-\ncial characteristics from both channel and token\naspects. Thus, we will introduce related works\n1649\nfrom the two aspects.\nChannel aspect. Outliers consistently emerge\nin certain channels over different inputs. Bon-\ndarenko et al. (2021) employs a per-embedding-\ngroup quantization scheme that uses different quan-\ntization parameters for distinct channel groups,\nwhile Dettmers et al. (2022) suggests utilizing FP16\nrepresentations for problematic channels holding\nsignals over 6. Wei et al. (2022b) introduces an out-\nlier suppression (OS) framework with one of com-\nponents called Gamma Migration. Observing that\noutliers accumulate in certain channels, it adopts\na scaling vector to scale outliers and migrates it\nto subsequent modules. Xiao et al. (2022) further\nproposes calculating scaling values by equalizing\nranges between activations and weights and eval-\nuates on large language models. Guo et al. (2023)\ndiscards normal values adjacent to outliers, mak-\ning room for outliers with customized GPU sup-\nport. To consider the standard quantization, we\nfind that Wei et al. (2022b) and Xiao et al. (2022)\nstill waste a large portion of quantization levels\non the extreme outlier asymmetry across channels.\nMeanwhile, Wei et al. (2022b) simply views the\nscaling parameter in LayerNorm (LN) as the scal-\ning vector for outliers, which might not always be\nconsistent with the outlier distribution. Xiao et al.\n(2022) that adopts the heuristic way and obtains\nequalized ranges between activation and weights\nlacks quantitative evaluation of their output change\ninduced by migration and quantization.\nToken aspect. Different tokens exhibit varying de-\ngrees of outliers. Dettmers et al. (2022); Yao et al.\n(2022) introduce a novel scheme called per-token\nquantization that dynamically computes quantiza-\ntion parameters for each token. Wei et al. (2022b)\ninvestigates the clipping impact of outliers and rec-\nommends finding an appropriate clipping range in\na token-wise manner. In this paper, we focus on the\nchannel aspect and might combine these techniques\nwhen necessary.\n3 Preliminary\nBasic Notations. We denote matrices as upper case\nletters (e.g., X) and vectors as lower case letters\n(e.g., x). Operator ⊙and ⊘represent element-wise\nmultiplication and division for matrices or vectors.\nWe use WX as matrix-matrix multiplication. Fur-\nthermore, Xt,j refers to the element of the t-th\ntoken and the j-th channel in transformer models.\nQ(·) denotes the quantization function.\nQuantization. We indicate standard quantization\nas per-tensor activation quantization, per-channel,\nor per-tensor weight quantization here because such\nschemes will not separate the integer matrix mul-\ntiplication. Per-tensor means assigns quantization\nparameters for each tensor and per-channel for each\noutput channel. Also, for some fine-grained ways,\nwe mainly consider per-token (Yao et al., 2022) and\nper-group (Yao et al., 2023) here, which calculates\nquantization parameters in each token or group.\n4 Method\nWe first present our equivalent shifting and scal-\ning operations, then introduce ways to determine\neffective values for them.\n4.1 Equivalent shifting and scaling\nIn this section, we comprehensively investigate out-\nlier features, naturally introducing the design of\nshifting and scaling operations, followed by a uni-\nfied migration pattern.\n4.1.1 Outlier shifting and scaling\nChannel-wise shifting. For transformers, espe-\ncially LLMs, we find that outliers show asymmet-\nric behavior among channels. Recall that in Fig. 1a,\nthe 8725-th channel displays a hard negative inter-\nval (-97, -58), while another channel dominates a\npositive one (5.7, 43). Due to this asymmetry, even\nif the range of each channel is relatively small, such\nas 40 and 39 for outlier channels and minuscule\nvalues for normal channels, the range of the entire\ntensor can swell to a considerably large value (e.g.,\n140, ranging from -97 to 43), which negatively af-\nfects quantization performance.\nTo handle this issue, we propose channel-wise\nshifting, which can eliminate the impact of asym-\nmetry by taking the following operation:\n˜X′= X−z, (1)\nwhere zserves as a row vector (z∈Rn) and shifts\nthe activation for each channel. In this way, with\na carefully designed zwhich we will introduce in\nSec. 4.2.1, the new tensor ˜X′can get rid of the out-\nlier asymmetry attribute. For example, by aligning\nthe centers of each channel in Fig. 1b, the range\ncan be reduced to 40 (the maximum channel range)\nfrom 140 (the large tensor range). Finally, note\nthat this operation is not the conventional shifting\noperation for symmetric quantization, as it operates\nchannel-wisely and provides better distribution for\nper-tensor quantization.\n1650\nFigure 2: Left: We show the equivalent shifting and scaling operations by giving two representative examples: (a) for problematic\noutput of Pre-LN (LayerNorm put inside residual connection) with Multi-Head Attention (MHA) structure; (b) for problematic\noutput of Post-LN (LayerNorm put before residual connection) with Feed-Forward Network (FFN). Right: For effective shifting\nand scaling values, the shifting vector can align the center of each channel to 0 and the scaling vector would shrink outliers into\nthe outlier threshold t which is searched based on its left metric.\nChannel-wise scaling. Apart from the asymmetry\nfeature across channels, there also exists the out-\nlier concentration phenomenon (Wei et al., 2022b)\nthat outliers predominantly accumulate in specific\nchannels over various inputs. For example, the\n8725-th and the 6354-th channels in Fig. 1a hold\nmore aggressive values than others. Therefore, af-\nter shifting, we equip with the channel-wise scaling\nto narrow them down to further alleviate the quan-\ntization difficulty.\n˜X= (X−z) ⊘s. (2)\nIn the above equation, the row vectors∈Rn scales\nthe shifted tensor for each channel and brings final\nquantization-friendly activation ˜X. For example,\nin Fig. 1c, a tensor with a size of 10 can be obtained\nif we scale down channels with signals over 5. De-\ntailed calculation of swill be given in Sec. 4.2.2.\nImplementation. It is easy to implement these\noperations. Take the output of LayerNorm Fig. 2\nas an example, we only need to replace its linear\ntransformation parameters βand γwith (β−z)⊘s\nand γ⊘sto achieve shifting and scaling effects.\nFor others, we can update parameters in the former\nDeQuant function.\n4.1.2 Unified migration pattern\nAs mentioned in Eq. (1) and Eq. (2), we subtract\nzand divide sto make the problematic activation\nresilient to quantization. To keep an equivalent\nFP model, a unified migration pattern is proposed\nthat transfers both reversed shifting and scaling\nvectors to subsequent modules. We demonstrate\nthe feasibility of this algorithm on two common\nstructures.\nLinear Layer. First, we consider a prevalent sce-\nnario where a linear (convolutional) layer imme-\ndiately follows. Reversing the above operations\n(i.e., (˜X⊙s+ z)W⊤+ b) equals to updating the\nW ∈Rm,n and b∈Rm in the next layer, given by\n(˜X⊙s+ z)W⊤+ b\n= ( ˜X⊙s)W⊤+ zW⊤+ b\n= ˜X(W⊤⊙s⊤) + (zW⊤+ b).\n(3)\nAccording to Eq. (3), weight and bias can absorb s\nand z, respectively, and thus becomes:\n˜W = W ⊙\n\n\ns1 s2 ... sn\ns1 s2 ... sn\n...\ns1 s2 ... sn\n\n,\n˜b= zW⊤+ b.\n(4)\nFor example, Fig. 2(a) depicts the typical chal-\nlenging activation (output of LayerNorm) in the\nattention structure, all following weights and bi-\nases can absorb the shifting and scaling signals\nwithout any extra computation burden.\nResidual connection. Second, we consider the\ncase where a residual connection is applied after\nthe LayerNorm structure (Post-LN) and fed into\nthe quantized input. As shown in Fig. 2b, in ad-\ndition to linear layer transformation, the identity\nfunction will be substituted with channel-wise mul-\ntiplication and addition to maintain equivalence.\nWe demonstrate that these increased calculations\n1651\nwill only incur a negligible inference burden in\nSec. 5.5.\nFinally, because sand zserve as shared parame-\nters across tokens and batches of data, the unified\nmigration pattern can be well-implemented and\nproduce the same output without additional com-\nputation most of the time.\n4.2 Effective shifting and scaling\nBased on the equivalent shifting and scaling opera-\ntions, in this section, we propose a fast and stable\nscheme to pursue effective values.\n4.2.1 Shifting values\nThe design of the shifting vector should eliminate\nthe impact of asymmetry across channels. Thus,\nwe devise to align the center of each channel to 0\nso that the outlier channel will not occupy only the\npositive or negative side. In detail, zis defined as\nthe average of the minimum and maximum signals\nin each channel, given by:\nzj = max(X:,j) + min(X:,j)\n2 , (5)\nWith the channel-wise shifting now, the tensor\nrange reduces to the largest channel range, getting\nrid of being defined by asymmetric outliers.\n4.2.2 Scaling values\nThe design of the scaling vector should further\nscale down outliers while bringing marginal impact\non following weight quantization. The following\nparts introduce how to obtain it with the proposed\noptimization objective and procedure.\nChallenges. Recall that the equivalent transfor-\nmation Eq. (4) also scales weights and poten-\ntially leads to inferior weight quantization, which\nrequires us to calculate elaborate scaling values\nto reach a quantization balance between activa-\ntion and weights. Nevertheless, we find previous\nworks (Wei et al., 2022b; Xiao et al., 2022) ei-\nther ignore the affected following weight or take\na heuristic way that simply equalizes ranges of ac-\ntivation and weights. Unlike them, we think the\nkey point is to minimize their interactive output\nchange resulting from migration and quantization\n(a detailed analysis is available in Table 6). Hence,\na new optimization objective is proposed.\nOptimization objective. We first study the\nsimple case that the problematic activation acts\nas the input of one linear layer (e.g., Fig. 2b).\nInstead of minimizing quantization errors\nof activation and weight separately (i.e.,\nmins E\n[\n∥Q((X−z) ⊘s) −(X−z) ⊘s∥2\nF\n]\nand mins E\n[\n∥Q(W ⊙s) −W ⊙s∥2\nF\n]\n), a task\nloss perspective is adopted by concerning their\nmatrix multiplication output. We measure the\noutput change after scaling and quantizing weight\nand activation to pursue effective factors, given by:\nmin\ns\nE[∥Q((X−z) ⊘s)Q(W ⊙s)⊤+ ˜b  \noutput after scaling and quantization\n−(XW⊤+ b)  \noriginal FP output\n∥2\nF ],\n(6)\nwhere the mean squared error (MSE) is used to\nquantify the difference.\nMultiple linear layers: Furthermore, we study\nthe case for multiple linear layers like the attention\nstructure (Fig. 2a), where three weights will be mul-\ntiplied by the same scaling vector and calculated\nwith the same suppressed activation.\nIn this scenario, their matrix multiplication out-\nputs produced by scaled and quantized matrices\nare marked as ˜Qq,˜Kq, ˜Vq, (Original outputs are\ndenoted as Q,K,V). Applying Eq. (6) to three\nlinear layers separately and simply summing the\nlosses can make it difficult to illustrate their differ-\nent importance and usages. Therefore, we employ\nthe attention mechanism as a post-process function\nto reasonably organize their scaling and quantiza-\ntion information, given by:\nmin\ns\nE[∥softmax( ˜Qq ˜K⊤\nq ) ˜Vq −softmax(QK⊤)V ∥2\nF ].\n(7)\nNormalization and masking are omitted for nota-\ntion simplicity, and it can be seen that information\nfrom the first two linear layers has been encapsu-\nlated within the attention map.\nOptimization procedure. Toward the above ob-\njective, a fast and stable procedure is introduced to\nsearch the scaling vector. First, we find that scaling\ndown only channels with outliers can bring better\nperformance. Because channels with normal acti-\nvations can exhibit more variation over different\ninputs, it can be difficult to find a decent scaling\nvalue for them. Also, considering that they are not\nresponsible for low quantization performance, scal-\ning them is not necessary. Second, we propose to\noptimize an alternate variable called outlier thresh-\nold t, which would squeeze only channels with an\nactivation range over tinto (−t,t) and keep others\nintact (Fig. 2). Essentially, there is used to specify\n1652\nwhich channel to scale down, the final scaled ac-\ntivation range, as well as the scaling values in the\nfollowing weights.\nThis technique simplifies the complex problem\nwith numerous variables sto a single variable t.\nThen we adopt the simple grid search for tto min-\nimize the objective Eq. (6), Eq. (7). After getting\nthe effective t, the scaling vector is calculated as:\nsj = max(1.0, max(X:,j −zj)\nt ). (8)\n5 Experiments\nThe evaluations are designed to show: I. satisfac-\ntory predictions of our OS+ for both small and large\nlanguage models with standard quantization; II.\nconsistent performance of OS+ on even lower-bit\nwith fine-grained quantization; III. ablation study;\nIII. analysis like computation complexity.\n5.1 Set up\nQuantization setting. Both the standard and fine-\ngrained quantization are considered. For the stan-\ndard one, we take quantization nodes the same as in\nWei et al. (2022b); NVIDIA (2022), always adopt\nper-tensor activation quantization, consider per-\ntensor (fastest speed) and per-channel (high perfor-\nmance) weight quantization. For the fine-grained\nquantization, we adopt per-token (Yao et al., 2022)\nand per-group (Yao et al., 2023) quantization.\nNotation: We use INT8, INT6, INT4 to denote\nthe bitwidth of activation and weight. Specifically,\nINT8* refers to per-tensor weight quantization.\nAnd per-token and per-group quantization will be\nmarked in the table below.\nModels and tasks. We conduct experiments on\nboth small and large language models. First, BERT\nmodels (base and large versions) are evaluated\non the GLUE benchmark (Wang et al., 2018a).\nSecond, four of the largest OPTs ranging from\n13B to 175B, biggest BLOOM (Scao et al., 2022)\nand BLOOMZ (Muennighoff et al., 2022) boast-\ning 176 billion parameters, and LLaMA (Tou-\nvron et al., 2023) models including 7B, 13B, 30B,\n65B sizes are chosen as representatives. Zero-\nshot tasks including language modeling, multiple\nchoice, commonsense reasoning, etc. are selected\nfor evaluation. The evaluation code is based on\nlm-harness-evaluation1.\nBaselines. For BERT, we adopt classical PTQ\ntechniques as baselines, including MinMax, Per-\n1https://github.com/EleutherAI/lm-evaluation-harness\ncentile (Wu et al., 2020), OMSE (Choukroun et al.,\n2019), and recent works on BERT quantization\nincluding PEG (Bondarenko et al., 2021), and Out-\nlier Suppresion (Wei et al., 2022b). For large\nmodels including OPT, BLOOM, and LLaMA, we\nmainly compare with recent works including Zero-\nQuant (Yao et al., 2022), and SmoothQuant (Xiao\net al., 2022). For details, readers can refer to Ap-\npendix C.\nImplementation. We randomly select 128 samples\nfrom the training dataset, in-domain data for the\nGLUE benchmark, and PILE (Gao et al., 2020)\ndataset for zero-shot tasks. A batch of data is first\nused to calculate effective shifting and scaling vec-\ntors. Then, calibration is conducted. More details\ncan be found in Appendix C.\n5.2 Standard quantization with OS+\nIn this section, we show how OS+ can help standard\nquantization achieve satisfying results from both\nthe small models and LLMs aspects.\nMethod CoLA MNLI QNLI SST-2 STS-B Avg.\nFP32 59.6 84.9 91.8 93.4 89.5 83.8\nINT8*\nMinMax 52.3 81.3 89.0 91.1 86.2 79.5\nOMSE 54.8 82.1 89.7 91.3 87.7 81.6\nPEG 59.4 81.3 91.1 92.7 87.9 82.5\nOS 60.3 83.9 90.2 92.9 88.2 83.0\nOS+ 60.9 84.4 91.1 92.7 88.3 83.5\nINT6\nOMSE 35.4 73.7 84.7 86.3 85.8 73.5\nPercentile 37.3 72.1 79.4 87.3 86.8 72.9\nOS 54.4 81.8 89.8 91.9 88.7 81.2\nOS+ 56.0 84.5 90.9 92.4 89.5 82.8\nINT4\nOMSE 4.7 38.5 52.2 50.3 0.2 41.1\nPercentile 7.0 53.0 61.5 77.1 66.1 57.0\nOS 28.5 57.9 72.5 80.4 67.8 62.7\nOS+ 50.0 80.2 85.4 91.4 86.5 78.2\nTable 1: PTQ performance of BERT-base models. MNLI\nand STS-B report the combined score. Avg. indicates the\naveraged results of 8 tasks on GLUE benchmark (details in\nAppendix B). ∗means per-tensor quantization for weight. OS\nindicates Outlier Suppression for short.\nBERT.Table 1 gives prediction results of common\nPTQ algorithms. Most methods perform well on\nINT8* but fail on lower bits while our approach\nconsistently achieves superior outcomes. Com-\npared to Wei et al. (2022b), our method outper-\nforms by 1.6% and 15.5% on 6-bit and 4-bit, re-\nspectively. In summary, our approach can achieve\nnear-floating point performance on high bits and\n1653\nModel Method PIQA (↑) Winogrande ( ↑) HellaSwag ( ↑) LAMBADA ( ↑)\nFP16 INT8* INT6 FP16 INT8* INT6 FP16 INT8* INT6 FP16 INT8* INT6\nOPT-13B\nZeroQuant\n75.8\n54.1 53.0\n65.1\n52.1 51.1\n52.5\n26.5 25.8\n68.6\n42.9 0.0\nSmoothQuant 76.0 73.5 64.9 60.3 52.2 49.2 68.3 65.2\nOS+ 76.4 75.8 65.0 64.0 52.3 51.7 68.3 65.7\nOPT-30B\nZeroQuant\n77.6\n54.2 52.0\n68.5\n51.8 51.8\n54.3\n26.4 25.7\n71.5\n9.7 0.0\nSmoothQuant 77.2 66.7 68.2 55.0 54.2 37.4 71.0 13.4\nOS+ 77.4 77.4 68.0 68.9 54.2 53.7 70.8 69.6\nOPT-66B\nZeroQuant\n78.7\n53.2 51.9\n68.9\n50.7 48.0\n56.4\n26.1 25.7\n73.9\n0.6 0.0\nSmoothQuant 78.3 52.0 68.3 52.1 55.9 26.5 72.9 0.0\nOS+ 78.7 77.5 69.0 69.4 56.2 55.8 73.0 72.7\nOPT-175B\nZeroQuant\n79.7\n52.3 53.1\n72.5\n50.2 49.1\n59.3\n25.4 25.6\n74.7\n0.0 0.0\nSmoothQuant 79.7 52.6 71.2 49.1 58.9 26.0 74.6 0.5\nOS+ 79.6 80.0 72.5 71.7 59.2 58.5 74.7 74.2\nBLOOM-176B\nZeroQuant\n78.8\n76.0 61.2\n70.3\n69.4 52.0\n55.9\n54.8 30.5\n67.7\n67.8 7.5\nSmoothQuant 77.7 76.7 68.6 67.6 54.1 52.1 69.2 60.2\nOS+ 78.4 78.1 69.8 70.3 55.2 54.8 68.0 69.2\nBLOOMZ-176B\nZeroQuant\n80.6\n79.1 54.0\n72.5\n70.9 49.6\n57.1\n56.3 28.2\n67.8\n67.6 1.4\nSmoothQuant 79.7 80.0 70.8 69.9 56.3 55.0 68.7 65.2\nOS+ 79.9 79.9 71.3 70.6 56.7 56.4 68.8 69.2\nTable 2: Comparison among different techniques in terms of accuracy on four zero-shot tasks. INT8* specifically means\nper-tensor quantization for weights compared to INT8. More tasks are put in Appendix B due to space limit.\nreduce the performance gap to 5.6% on 4-bit.\nOPT and BLOOM. With standard quantization,\nwe list 8-bit and 6-bit accuracy in Table 2. It can\nbe observed that OS+ outperforms ZeroQuant by\na large margin. While SmoothQuant suffers from\nnon-negligible accuracy drops on much harder set-\ntings like the 6-bit 175B model with significantly\nsevere outliers, ours still gives enjoyable results,\nowning 32.5% upswings on HellaSwag task, 27.4%\nboost on PIQA. Results of BLOOM models indi-\ncate that their quantization challenges are less se-\nvere than OPTs with smaller accuracy drops across\nmethods. Our approach still beats the best of others\nby about 2% points on 6-bit. To conclude, with\nstandard quantization, ours is indeed close to FP re-\nsults on 8-bit and exhibits around 1 point accuracy\ndegradation on 6-bit.\n5.3 Fine-grained quantization with OS+\nHere, OS+ is combined with fine-grained quanti-\nzation to validate its wide application and go ex-\ntremely low bit setting like 4-bit quantization.\nPer-token Quantization. Per-token quantiza-\ntion (Yao et al., 2022), which customizes quan-\ntization parameters for individual tokens, can bring\nbetter predictions, especially for lower-bit quanti-\nzation and longer output like WikiText2 (Merity\net al., 2017). We opt for LLaMA models for valida-\ntion. It’s worth noting that the structure of LLaMA\ndiffers from others in its design of element-wise\nmultiplication of two activations as the input to\nthe final layer in FFN, potentially resulting in very\nlarge signals, even exceeding 600. Given such a\nchallenge, we provide experiments both with and\nwithout quantization of this layer in Table 3 and\nTable 10, respectively. In both tables, we high-\nlight our lossless performance on 6-bit quantization\nwhile SmoothQuant still suffers in Table 10. Also,\nit shows the superior performance of OS+ on 4-bit\n(e.g., 10.58% improvement on Winogrande, 10.04\nPPL decrease on WikiText2).\nPer-group Quantization. Additionally, per-group\nquantization (Yao et al., 2023), which tailors quan-\ntization parameters for each group of elements, is\na more fine-grained way. Recognizing the difficul-\nties of 4-bit quantization for OPTs, we illustrate\nan example by adopting per-group quantization\nwith relatively large group sizes of 1024 and 512.\nFig. 3 shows that OS+ continues to outperform\nother methods and can be more competitive under\nharder cases such as a group size of 1024.\nFigure 3: Results of 4-bit quantization with group size set to\n1024 and 512, respectively.\n1654\nModel Method PIQA (↑) Winogrande ( ↑) HellaSwag ( ↑) WikiText2 ( ↓)\nFP16 INT6 INT4 FP16 INT6 INT4 FP16 INT6 INT4 FP16 INT6 INT4\nLLaMA-1-7B\nMinMax\n77.37\n77.26 55.98\n66.93\n66.54 49.64\n72.99\n71.78 32.28\n5.68\n6.00 473.97\nSmoothQuant 77.18 70.08 65.51 52.96 72.10 58.13 5.85 16.87\nOS+ 77.48 72.31 67.01 56.67 72.32 61.24 5.76 14.17\nLLaMA-1-13B\nMinMax\n79.05\n78.56 50.65\n70.09\n69.53 50.28\n76.22\n75.26 26.34\n5.09\n5.58 3410.45\nSmoothQuant 78.45 66.49 69.69 51.78 75.20 58.95 5.25 56.75\nOS+ 78.73 75.03 69.53 61.17 75.74 67.21 5.22 18.95\nLLaMA-1-30B\nMinMax\n80.09\n78.40 50.00\n72.77\n72.45 50.12\n79.21\n77.25 27.09\n4.10\n5.09 2959.15\nSmoothQuant 78.78 71.55 73.01 54.54 78.13 60.97 4.40 51.47\nOS+ 79.98 73.01 73.64 60.38 78.77 68.03 4.30 22.61\nLLaMA-1-65B\nMinMax\n80.85\n77.58 50.27\n77.11\n69.46 49.33\n80.73\n78.72 24.59\n3.56\n5.25 14584.66\nSmoothQuant 78.40 65.02 74.30 51.14 78.57 59.78 3.77 19.37\nOS+ 80.47 74.43 75.14 61.72 79.76 67.65 3.65 9.33\nTable 3: Comparison in terms of normalized accuracy, accuracy, normalized accuracy and perplexity (PPL), respectively (Touvron\net al., 2023). Techniques are equipped with per-token quantization (Yao et al., 2022). More results are put in Appendix B.\n5.4 Ablation study\nDesign choices of scaling values. In this section,\nwe compare different scaling vector designs. In\nTable 4, the second row displays results without\nattention post-processing Eq. (7). Summing the\nlosses of multiple linear layers, as shown, proves\nunwise, resulting in performance declines of about\n2% and 10% on OPTs. The third row removes the\noutlier threshold and instead learns scaling values\ndirectly. We find this process is unstable and re-\nquires suitable hyperparameters, causing failure on\nLLMs. As mentioned in Sec. 4.2.2, This instabil-\nity may stem from suboptimal scaling values for\nnormal channels with varying magnitudes.\nEffect of each operation. From Table 5, it can be\nobserved clearly that by removing the shifting op-\neration, the accuracy drops by about 1%-3% under\ndifficult settings. This is because, without channel-\nwise shifting that initially smooths the quantiza-\ntion challenge, scaling factors struggle to suppress\noutliers effectively while producing the tolerable\nweight quantization burden. Furthermore, when\nexcluding scaling effects, performance decreases\nsignificantly, with even crashed results on LLMs.\nMethod OPT-66B (INT6) BERT (INT4)\nPIQA Winogrande SST-2 MNLI\nscaling 76.5 66.5 89.3 77.7\n- attention post process 74.5 57.4 89.1 77.1\n- outlier threshold Fail Fail 83.2 65.2\nTable 4: Design choices of scaling factor. The second row\nremoves the attention post process in optimization objective.\nThe third row chooses to learn the scaling vector directly rather\nthan alternately optimize the outlier threshold.\n5.5 Analysis\nDifferent activation scaling. Because scaling val-\nues act in both the activation and weights, reducing\nquantization error for individual tensors can not\nguarantee the minimum output change, which en-\ncapsulates their information to later forward pass.\nFor example, in Table 6, Outlier Suppression with\nfixed scaling values has the smallest quantization\nerror for weight. SmoothQuant with a heuristic way\nhas the smallest quantization error for activation.\nHowever, both of them did not bring the smallest\nquantization error for the output. This reveals the\nimportance of directly optimizing according to the\noutput, which is what our method exactly does.\nThus, we can enjoy the best final performance.\nModel storage and accuracy. Inspired by a vari-\nety of models with diverse sizes, we also study the\nrelationship between their storage and accuracy un-\nder quantization settings. Focusing on one kind of\nmodel with distinct quantization bit-width, Fig. 4\nshows that 8-bit quantization which cuts storage\nby about half, can generally maintain original per-\nformance, and 6-bit quantization can lead to less\nperformance drop on larger models. Moreover, con-\nsidering fixed storage constraints, we discover that\nquantized big models typically outperform small\nFP models. These observations can relate to model\nrobustness, which implies that large models can\nMethod OPT-66B (INT6) BERT (INT4)\nPIQA Winogrande SST-2 MNLI\nOurs 77.5 69.4 91.4 80.2\n- shifting 76.5 66.5 89.3 77.7\n- shifting - scaling 54.7 49.4 82.3 63.7\nTable 5: Effect of scaling and shifting operations.\n1655\nMethod activation weight Output change\nrange MSE range MSE MSE\noriginal (-93.9, 31.6) 209.8 (-0.13, 0.13) 0.001 18061.5\nOS (-23.5,15.7) 142.9 (-0.40, 0.41)0.0066182.52\nSQ (-3.5, 2.0) 3.65 (3.4, 3.5) 0.43 3535.86\nOur scaling (-8.4, 8.4) 48.54 (1.2, 1.3) 0.021334.89\nTable 6: Detailed analysis of different techniques from the\nactivation scaling aspect. OS indicates Outlier Suppression\nand SQ indicates SmoothQuant.\nbenefit from compression more if special outliers\nare handled well.\nComputation Complexity. We explain our com-\nputation complexity of calibration and deployment\nphases. For the calibration process, OS+ is efficient,\nand able to generate scaling and shifting values in\nabout 20 minutes for OPT-175B offline. Moreover,\ndue to the equivalent transformation, our method\ndoes not demand additional training and can be\napplied in a post-training setting. For deployment,\nwe discuss inference efficiency with latency per-\nformance evaluated using (NVIDIA, 2022). As\nmentioned before, our channel-wise shifting and\nscaling can be implemented by updating previous\nparameters, and be migrated to subsequent weights.\nFor LLMs, our transformation does not introduce\nany extra computation burden and leads to favor-\nable latency improvements, as demonstrated in a\n1.5×speedup in Fig. 5. Only BERT models addi-\ntionally replace the identity function in the residual\nconnection with channel-wise multiplication and\naddition. Such overhead is minimal, as shown in\nFig. 5, resulting in comparable latency speedup.\n6 Conclusion\nWe present the Outlier Suppression+ framework\nfor addressing asymmetric and consistent outliers\n50 100 150 200 250 300 350\nStorage (MB)\n64\n66\n68\n70\n72Accuracy\nOPT-13B OPT-30B OPT-66B OPT-175B\nFigure 4: Averaged accuracy on PIQA, Winogrande, LAM-\nBADA, and HellaSwag of OPTs with different storages. We\ndraw circles, rectangles, and triangles to refer to FP16, the\n8-bit and 6-bit models with quantized activation and weight.\n0 10\nBERT-base-128\n16\n32\n64\n128\n1.41×\n1.41×\n1.44×\n1.41×\n1.40×\n1.40×\n1.43×\n1.41×\nFP16\nINT8*\nINT8* (Ours)\n0 50 100\nBERT-large-256\n16\n32\n64\n128\n1.47×\n1.54×\n1.48×\n1.31×\n1.46×\n1.53×\n1.48×\n1.30×\n0 500 1000\nOPT-13B-64\n1\n2\n4\n8\n16\n1.51×\n1.50×\n1.49×\n1.48×\n1.46×\nFigure 5: Real latency (x-axis) of our transformed 8-bit mod-\nels, 8-bit and FP16 original models over different batch sizes\n(y-axis). BERT-large-256 refers to the BERT-large model with\nsequence length set to 256 while for OPT-13B-64, 64 means\noutput length with input length set to 512. Bold numbers\nindicate quantization speedup.\nin LLMs and other transformers. Our framework\nis simple to use, consisting of both scaling and\nshifting operations, which can be efficiently and\neffectively implemented. Experiments demonstrate\nthe efficacy of our methods for suppressing outliers.\nLimitations\nWhile we have observed features of outliers and\ndevised methods to deal with them, the underly-\ning reasons for their emergence and attributes have\nnot been fully understood. This may require an\nin-depth analysis of the training pipeline, including\nthe procedure and hyperparameters. Such investi-\ngations are time-consuming but can benefit both\nFP and quantized scenarios.\nEthics Statement\nOur Outlier Suppression+ framework aims to im-\nprove the quantization performance of transformer\nlanguage models. It can boost the development of\npractical and green machine learning and does not\nincur extra ethical concerns.\nAcknowledgment\nWe sincerely thank the anonymous reviewers for\ntheir sincere reviews and valuable suggestions to\nmake this better. We also thank Qi Zhang for the in-\nsightful discussion and Jing Liu for helping to build\nthe code of LLaMA. This work was supported by\nthe National Natural Science Foundation of China\n(No. 62022009), National Natural Science Foun-\ndation of China (No. 62306025), the State Key\nLaboratory of Software Development Environment\n(SKLSDE-2022ZX-23).\n1656\nReferences\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing\nJin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin\nKing. 2020. Binarybert: Pushing the limit of bert\nquantization. arXiv preprint arXiv:2012.15701.\nRon Banner, Yury Nahshan, Elad Hoffer, and Daniel\nSoudry. 2018. Aciq: analytical clipping for integer\nquantization of neural networks.\nYelysei Bondarenko, Markus Nagel, and Tijmen\nBlankevoort. 2021. Understanding and overcoming\nthe challenges of efficient transformer quantization.\narXiv preprint arXiv:2109.12948.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer. 2020. Zeroq:\nA novel zero shot quantization framework. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13169–13178.\nZhaowei Cai and Nuno Vasconcelos. 2020. Rethinking\ndifferentiable search for mixed-precision neural net-\nworks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n2349–2358.\nMengzhao Chen, Wenqi Shao, Peng Xu, Mingbao Lin,\nKaipeng Zhang, Fei Chao, Rongrong Ji, Yu Qiao,\nand Ping Luo. 2023. Diffrate: Differentiable com-\npression rate for efficient vision transformers. arXiv\npreprint arXiv:2305.17997.\nJungwook Choi, Zhuo Wang, Swagath Venkataramani,\nPierce I-Jen Chuang, Vijayalakshmi Srinivasan, and\nKailash Gopalakrishnan. 2018. Pact: Parameterized\nclipping activation for quantized neural networks.\narXiv preprint arXiv:1805.06085.\nYoni Choukroun, Eli Kravchik, Fan Yang, and Pavel\nKisilev. 2019. Low-bit quantization of neural net-\nworks for efficient inference. In 2019 IEEE/CVF\nInternational Conference on Computer Vision Work-\nshop (ICCVW), pages 3009–3018. IEEE.\nMatthieu Courbariaux, Yoshua Bengio, and Jean-Pierre\nDavid. 2015. Binaryconnect: Training deep neural\nnetworks with binary weights during propagations.\nAdvances in neural information processing systems,\n28.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-\ntiplication for transformers at scale. arXiv preprint\narXiv:2208.07339.\nTim Dettmers and Luke Zettlemoyer. 2022. The case for\n4-bit precision: k-bit inference scaling laws. arXiv\npreprint arXiv:2212.09720.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nZhen Dong, Zhewei Yao, Amir Gholami, Michael W\nMahoney, and Kurt Keutzer. 2019. Hawq: Hessian\naware quantization of neural networks with mixed-\nprecision. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 293–\n302.\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani,\nRathinakumar Appuswamy, and Dharmendra S\nModha. 2019. Learned step size quantization. arXiv\npreprint arXiv:1902.08153.\nAngela Fan, Pierre Stock, Benjamin Graham, Edouard\nGrave, Rémi Gribonval, Herve Jegou, and Armand\nJoulin. 2020. Training with quantization noise\nfor extreme model compression. arXiv preprint\narXiv:2004.07320.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2022. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\narXiv preprint arXiv:2210.17323.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nRuihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang\nLi, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie\nYan. 2019. Differentiable soft quantization: Bridging\nfull-precision and low-bit neural networks. In The\nIEEE International Conference on Computer Vision\n(ICCV).\nCong Guo, Yuxian Qiu, Jingwen Leng, Xiaotian Gao,\nChen Zhang, Yunxin Liu, Fan Yang, Yuhao Zhu, and\nMinyi Guo. 2022. Squant: On-the-fly data-free quan-\ntization via diagonal hessian approximation. arXiv\npreprint arXiv:2202.07471.\nCong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng,\nChen Zhang, Fan Yang, Yunxin Liu, Minyi Guo, and\nYuhao Zhu. 2023. Olive: Accelerating large lan-\nguage models via hardware-friendly outlier-victim\npair quantization. Matrix, 17(4.2):7–1.\nSong Han, Huizi Mao, and William J Dally. 2015. Deep\ncompression: Compressing deep neural networks\nwith pruning, trained quantization and huffman cod-\ning. arXiv preprint arXiv:1510.00149.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\n1657\nItay Hubara, Yury Nahshan, Yair Hanani, Ron Banner,\nand Daniel Soudry. 2021. Accurate post training\nquantization with small calibration sets. In Inter-\nnational Conference on Machine Learning , pages\n4466–4475. PMLR.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Meng-\nlong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2018. Quanti-\nzation and training of neural networks for efficient\ninteger-arithmetic-only inference. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nQing Jin, Jian Ren, Richard Zhuang, Sumant Hanu-\nmante, Zhengang Li, Zhiyu Chen, Yanzhi Wang,\nKaiyuan Yang, and Sergey Tulyakov. 2022. F8net:\nFixed-point 8-bit only multiplication for network\nquantization. arXiv preprint arXiv:2202.05239.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021. I-bert: Integer-\nonly bert quantization. In International conference\non machine learning, pages 5506–5518. PMLR.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,\nand Anna Rumshisky. 2021. Bert busters: Outlier\ndimensions that disrupt transformers. arXiv preprint\narXiv:2105.06990.\nAndrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus\nNagel, Jorn Peters, and Tijmen Blankevoort. 2022.\nFp8 quantization: The power of the exponent. arXiv\npreprint arXiv:2208.09225.\nYann LeCun, John Denker, and Sara Solla. 1989. Opti-\nmal brain damage. Advances in neural information\nprocessing systems, 2.\nYanjing Li, Sheng Xu, Baochang Zhang, Xianbin Cao,\nPeng Gao, and Guodong Guo. 2022. Q-vit: Accurate\nand fully quantized low-bit vision transformer. arXiv\npreprint arXiv:2210.06707.\nYuhang Li, Xin Dong, and Wei Wang. 2019. Addi-\ntive powers-of-two quantization: An efficient non-\nuniform discretization for neural networks. arXiv\npreprint arXiv:1909.13144.\nYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu,\nQi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. 2021.\nBrecq: Pushing the limit of post-training quantization\nby block reconstruction. In International Conference\non Learning Representations.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang,\nXingyu Dang, and Song Han. 2023. Awq: Activation-\naware weight quantization for llm compression and\nacceleration. arXiv preprint arXiv:2306.00978.\nJing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong,\nJianfei Cai, and Bohan Zhuang. 2023. Qllm: Accu-\nrate and efficient low-bitwidth quantization for large\nlanguage models. arXiv preprint arXiv:2310.08041.\nJing Liu, Zizheng Pan, Haoyu He, Jianfei Cai, and Bo-\nhan Zhuang. 2022. Ecoformer: Energy-saving at-\ntention with linear complexity. Advances in Neural\nInformation Processing Systems, 35:10295–10308.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels.\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Mar-\nius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd,\nJohn Kamalu, et al. 2022. Fp8 formats for deep\nlearning. arXiv preprint arXiv:2209.05433.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. 2020. Up\nor down? adaptive rounding for post-training quan-\ntization. In International Conference on Machine\nLearning, pages 7197–7206. PMLR.\nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort,\nand Max Welling. 2019. Data-free quantization\nthrough weight equalization and bias correction. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 1325–1334.\nNVIDIA. 2022. Faster transformer.\nhttps://github.com/NVIDIA/\nFasterTransformer.\nGiovanni Puccetti, Anna Rogers, Aleksandr Drozd, and\nFelice Dell’Orletta. 2022. Outliers dimensions that\ndisrupt transformers are driven by frequency. arXiv\npreprint arXiv:2205.11380.\nHaotong Qin, Yifu Ding, Mingyuan Zhang, Qinghua\nYan, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xi-\nanglong Liu. 2022. Bibert: Accurate fully binarized\nbert. arXiv preprint arXiv:2203.06390.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\n1658\nMingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li,\nChuming Li, Chen Lin, Fengwei Yu, Junjie Yan, and\nWanli Ouyang. 2021. Once quantization-aware train-\ning: High performance extremely low-bit architecture\nsearch. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages\n5340–5349.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8815–8821.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nChaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang,\nXin Jiang, Qun Liu, Ping Luo, and Ngai Wong.\n2022. Compression of generative pre-trained lan-\nguage models via quantization. arXiv preprint\narXiv:2203.10705.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nNaigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu\nChen, and Kailash Gopalakrishnan. 2018b. Train-\ning deep neural networks with 8-bit floating point\nnumbers. Advances in neural information processing\nsystems, 31.\nPeisong Wang, Qiang Chen, Xiangyu He, and Jian\nCheng. 2020. Towards accurate post-training net-\nwork quantization via bit-split and stitching. In In-\nternational Conference on Machine Learning, pages\n9847–9856. PMLR.\nXiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu,\nand Fengwei Yu. 2022a. Qdrop: Randomly dropping\nquantization for extremely low-bit post-training quan-\ntization. In International Conference on Learning\nRepresentations.\nXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao\nGong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and\nXianglong Liu. 2022b. Outlier suppression: Pushing\nthe limit of low-bit transformer language models.\narXiv preprint arXiv:2209.13325.\nHao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev,\nand Paulius Micikevicius. 2020. Integer quantization\nfor deep learning inference: Principles and empirical\nevaluation. arXiv preprint arXiv:2004.09602.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien De-\nmouth, and Song Han. 2022. Smoothquant: Accurate\nand efficient post-training quantization for large lan-\nguage models. arXiv preprint arXiv:2211.10438.\nKe Xu, Lei Han, Ye Tian, Shangshang Yang, and Xingyi\nZhang. 2023. Eq-net: Elastic quantization neural\nnetworks. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 1505–\n1514.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\nXiaoxia Wu, Conglong Li, and Yuxiong He. 2022.\nZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers. arXiv\npreprint arXiv:2206.01861.\nZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn,\nand Yuxiong He. 2023. Zeroquant-v2: Exploring\npost-training quantization in llms from comprehen-\nsive study to low rank compensation. arXiv preprint\narXiv:2303.08302.\nZhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu,\nand Guangyu Sun. 2021. Ptq4vit: Post-training quan-\ntization framework for vision transformers. arXiv\npreprint arXiv:2111.12293.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36–39. IEEE.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nDongqing Zhang, Jiaolong Yang, Dongqiangzi Ye, and\nGang Hua. 2018. Lq-nets: Learned quantization for\nhighly accurate and compact deep neural networks.\nIn Proceedings of the European conference on com-\nputer vision (ECCV), pages 365–382.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit bert. arXiv preprint\narXiv:2009.12812.\nXiangguo Zhang, Haotong Qin, Yifu Ding, Ruihao\nGong, Qinghua Yan, Renshuai Tao, Yuhang Li, Feng-\nwei Yu, and Xianglong Liu. 2021. Diversifying sam-\nple generation for accurate data-free quantization. In\n1659\nThe IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR).\nRitchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa,\nand Zhiru Zhang. 2019. Improving neural network\nquantization without retraining using outlier channel\nsplitting. In International conference on machine\nlearning, pages 7543–7552. PMLR.\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578.\n1660\nA Related work\nQuantization. Compression has become more and\nmore popular these days (Han et al., 2015; Hinton\net al., 2015; Hu et al., 2021; Liu et al., 2022; Xu\net al., 2023; Chen et al., 2023). One of its effective\ntechniques called quantization (Jacob et al., 2018)\nemploys low-bit representations for activation and\nweight in neural networks. Researchers catego-\nrize this approach into two pipelines: post-training\nquantization (PTQ) and quantization-aware train-\ning (QAT). QAT (Courbariaux et al., 2015; Choi\net al., 2018; Esser et al., 2019; Li et al., 2019; Gong\net al., 2019; Shen et al., 2021; Zhang et al., 2018)\ntrains the quantized model end-to-end, necessitat-\ning significant GPU resources and the entire train-\ning dataset. In contrast, PTQ (Choukroun et al.,\n2019; Wu et al., 2020; Banner et al., 2018; Wang\net al., 2020; Zhao et al., 2019; Nagel et al., 2019)\nonly requires hundreds of samples and limited re-\nsource consumption, producing a calibrated model\nquickly. Recently, several works (Nagel et al.,\n2020; Hubara et al., 2021; Li et al., 2021; Wei\net al., 2022a) proposed to adjust models slightly for\nimproved PTQ performance. Besides, other types\nof quantization include zero-shot quantization with-\nout real calibration data (Cai et al., 2020; Zhang\net al., 2021; Guo et al., 2022), mixed-precision with\nmixed bit-width (Dong et al., 2019; Cai and Vas-\nconcelos, 2020), and FP8 data type (Wang et al.,\n2018b; Kuzmin et al., 2022; Micikevicius et al.,\n2022; Jin et al., 2022).\nQuantization of transformer language models.\nRecently, there has been a growing interest in the\nquantization of transformer language models. In\nthe context of QAT, Zafrir et al. (2019) first ex-\nplores 8-bit quantization for BERT-like models.\nShen et al. (2020) introduces group-wise quanti-\nzation and studies mixed-precision quantization\nbased on Hessian information. Bai et al. (2020);\nZhang et al. (2020); Qin et al. (2022) combine\ndistillation strategies with quantization. Kim et al.\n(2021) approximates the nonlinear function in trans-\nformer architectures to enable integer-only infer-\nence. Fan et al. (2020) incorporates quantization\nnoise for enhancement. Additionally, Tao et al.\n(2022) investigates the challenges of quantizing\ngenerative models.\nIn the realm of PTQ, researchers have discov-\nered that the poor performance of these models\nshould be attributed to extreme outliers in activa-\ntions. These outliers exhibit special characteristics\nfrom both channel and token aspects. In terms of\nchannels, outliers consistently emerge in certain\nchannels over different inputs. Bondarenko et al.\n(2021) employs a per-embedding-group quantiza-\ntion scheme that uses different quantization param-\neters for distinct channel groups, while Dettmers\net al. (2022) suggests utilizing FP16 representa-\ntions for problematic channels holding signals over\n6. Wei et al. (2022b) identifies this feature lying\nin LayerNorm’s output and migrates the scaling\nparameter of LayerNorm to subsequent modules\nto attenuate outliers. Xiao et al. (2022) proposes\ncalculating scaling values by equalizing ranges be-\ntween activations and weights and evaluates on\nlarge language models. Guo et al. (2023) discards\nnormal values adjacent to outliers, making room for\noutliers with customized GPU support. Compared\nto them, we design the scaling factors that con-\ncern the interactive results of troublesome activa-\ntion and following weights to scale down channels\nwith outliers offline. Also, we notice the asym-\nmetric presentation of outliers and design a shift-\ning operation. While we operate on corresponding\nchannels between weights and activation, a later\nwork (Liu et al., 2023) adopts the splitting and\nmerging operations to transfer the quantization bur-\nden of outlier channels to opposite channels of\nweights, which might encourage us to design a\nmore flexible technique without the same or oppo-\nsite channel index requirement. In terms of tokens,\ndifferent tokens exhibit varying degrees of outliers.\nDettmers et al. (2022); Yao et al. (2022) introduce\na novel scheme called per-token quantization that\ndynamically computes quantization parameters for\neach token. Wei et al. (2022b) investigates the clip-\nping impact of outliers and recommends finding an\nappropriate clipping range in a token-wise manner.\nBesides, some studies focus on weight quanti-\nzation, such as Dettmers and Zettlemoyer (2022);\nFrantar et al. (2022); Zeng et al. (2022); Lin et al.\n(2023) and some including Yuan et al. (2021); Li\net al. (2022), investigate the quantization of Vision\nTransformer (ViT) models. Interestingly, several\nstudies (Kovaleva et al., 2021; Puccetti et al., 2022)\nexplore the underlying reasons for emerging out-\nliers and trace them back to the pre-training phase.\nB Supplementary experiments\nBERT-base. We provide detailed results of BERT-\nbase models on GLUE benchmarks in Table 7. In-\nterestingly, we find that models which are sensitive\n1661\nAlgorithm 1: Outlier Suppression+\nInput: Problematic output Xof LayerNorm with parameters γ,β, subsequent module M with\nweight W and bias b, grid search iteration K.\n{1. Effective shifting and scaling:}\nz= min(X:,j)+max(X:,j)\n2 ▷Effective shifting vector.\nloss∗= INF\nfor k= 1to Kdo\nt= max(X−z) ·k\nK , ▷Enumerate outlier threshold.\nsj = max(1.0, max(X:,j−zj)\nt )\nCalculate lossk based on Eq. (6), Eq. (7).\nif loss∗>lossk then\nloss∗= lossk, s∗= s ▷Effective scaling factors.\n{2. Equivalent shifting and scaling:}\n˜β= (β−z) ⊘s∗,˜γ= γ⊘s∗ ▷Fuse z,s∗into former operations.\n˜b= zW⊤+ b,˜W = W ⊙s∗ ▷Update following modules.\nreturn Transformed LayerNorm and subsequent module;\nMethod CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg.(Matt.) (acc m/mm) (f1/acc) (acc) (f1/acc) (acc) (acc) (Pear./Spear.)\nFP32 59.6 84.9/84.8 91.4/87.8 91.8 87.8/90.9 72.6 93.4 89.7/89.3 83.8\nINT8*\nMinMax 52.3 80.9/81.7 85.3/80.9 89.0 84.8/88.6 68.2 91.1 84.7/87.6 79.5\nOMSE 54.8 81.9/82.2 89.7/86.0 89.7 86.1/89.5 72.2 91.3 87.2/88.2 81.6\nPEG 59.4 81.3 88.5 91.1 89.4 69.3 92.7 87.9 82.5\nOS 60.3 83.8/84.0 90.4/87.0 90.2 87.3/90.4 71.1 92.9 87.8/88.7 83.0\nOurs 60.9 84.4/84.4 90.6/87.2 91.1 87.1/90.6 73.3 92.7 87.7/88.9 83.5\nINT8\nMinMax 57.1 82.8/83.5 89.9/85.8 90.8 87.8/90.7 69.7 92.8 86.8/88.6 82.3\nOMSE 57.2 84.0/84.3 90.1/85.8 91.1 87.6/90.5 72.2 92.2 87.9/88.7 82.9\nPercentile 57.1 83.9/84.1 90.7/86.7 91.3 87.7/90.7 71.1 93.4 87.7/88.7 82.9\nOS 61.6 84.4/84.5 91.4/87.8 91.5 87.9/90.8 72.2 93.8 89.2/89.0 84.0\nOurs 60.3 84.8/84.5 90.5/87.0 91.6 87.5/90.8 71.5 93.6 89.3/89.2 83.6\nINT6\nMinMax 17.7 32.5/32.5 0.7/31.9 65.2 40.9/69.0 48.0 82.0 59.8/60.3 47.1\nOMSE 35.4 74.0/73.3 81.5/76.5 84.7 76.1/82.1 64.3 86.3 85.6/86.1 73.5\nPercentile 37.3 72.4/71.7 85.1/79.9 79.4 72.6/80.2 61.7 87.3 86.4/87.3 72.9\nOS 54.4 82.0/81.7 87.5/83.3 89.8 84.7/88.9 70.8 91.9 88.7/88.6 81.2\nOurs 56.0 84.6/84.4 90.0/86.3 90.9 87.0/90.5 71.8 92.4 89.6/89.4 82.8\nINT4\nMinMax -6.6 32.6/32.7 0.0/31.6 50.6 53.8/36.8 47.7 50.9 -0.5/-0.5 29.5\nOMSE 4.7 38.5/38.4 81.3/69.1 52.2 45.2/50.9 59.9 50.3 0.1/-0.4 41.1\nPercentile 7.0 52.6/53.5 83.0/75.7 61.5 44.7/68.3 55.6 77.1 65.9/66.3 57.0\nOS 28.5 57.5/58.3 83.9/75.7 72.5 45.4/70.8 56.7 80.4 67.8/67.9 62.7\nOurs 50.0 80.6/79.9 87.6/83.1 85.4 85.0/77.5 65.7 91.4 86.4/86.5 78.2\nTable 7: PTQ performance of BERT-base models on GLUE benchmark.∗means per-tensor quantization for weight. OS indicates\nOutlier Suppression for short.\n1662\nMethod CoLA MNLI MRPC QNLI QQP RTE SST-2 STS-B Avg.(Matt.) (acc m/mm) (f1/acc) (acc) (f1/acc) (acc) (acc) (Pear./Spear.)\nFP32 63.3 86.7/85.9 91.6/88.0 92.2 88.1/91.1 74.0 93.5 90.3/90.1 84.9\nINT8*\nMinMax 62.4 72.0/73.0 76.3/72.8 87.0 66.5/80.4 46.9 92.2 58.6/52.1 71.5\nOMSE 59.9 82.7/83.5 87.8/83.8 89.0 79.2/86.2 47.3 92.0 83.9/83.3 78.1\nPercentile 61.3 84.5/84.0 91.6/88.9 91.6 85.9/89.4 69.3 92.4 88.3/88.1 83.1\nOS 62.3 85.1/84.5 90.1/86.0 91.1 87.0/90.3 75.1 92.4 88.7/88.4 83.9\nOurs 62.2 85.9/85.2 90.9/87.0 92.2 87.8/90.8 71.8 93.3 89.3/89.3 84.1\nINT6\nMinMax 5.6 32.0/32.0 50.2/46.1 50.2 0.0/63.2 49.5 53.0 5.0/4.8 38.1\nOMSE 14.0 59.3/58.4 86.1/78.7 79.5 52.5/73.5 54.9 74.8 44.0/37.9 59.8\nPercentile 16.4 63.5/63.8 82.0/77.2 87.0 44.8/70.7 49.8 81.7 65.7/67.8 62.8\nOS 24.1 71.3/71.7 85.5/79.4 80.8 68.8/78.3 47.3 82.3 61.1/62.0 65.4\nOurs 60.9 86.3/85.4 91.8/88.2 92.0 87.7/90.8 71.5 93.7 86.7/85.6 83.7\nTable 8: PTQ performance of BERT-large models on GLUE benchmark. ∗means per-tensor quantization for weight. OS\nindicates Outlier Suppression for short.\nto different learning hyperparameters during the\nfine-tuning phase, such as CoLA and RTE, also\nexhibit less favorable quantization outcomes. This\nsuggests a possible relationship between quantiza-\ntion and robustness.\nBERT-large. We also conduct experiments on\nBERT-large models in Table 8. Results across meth-\nods indicate that quantizing BERT-large models is\nmore challenging (e.g., MinMax suffers from a\nconsiderable accuracy drop (about 13%) on INT8*\ncompared to BERT-base, and Outlier Suppression\nalso fails on the 6-bit setting). Fortunately, with\nOutlier Suppression+, the results can be improved,\nyielding an 18.7% enhancement.\nOPT. Here, we provide results of OPTs on more\ntasks. Table 9 is the supplement for Table 2, which\nfurther shows consistent performance enhancement\nof OS+.\nLLaMA. Recall that we conduct experiments on\nLLaMA with two different settings in the fine-\ngrained quantization section. Table 10 gives the\nresults when quantizing the special and challenging\nstructure (the last layer of FFN) in LLaMA models.\nIt can be observed that ours still earns near-floating-\npoint performance on 6-bit quantization and beats\nothers by about 5%∼14% in terms of averaged ac-\ncuracy of the first four tasks, and even four times\nPPL decrease for WikiText2. By comparing with\nthe easier setting Table 3, we find that the special\nstructure with large signals really leads to much\nlower 4-bit outcomes across methods, especially\nfor MinMax and SmoothQuant, which makes us\nthink of model design, training techniques, and\nefficient fine-tuning for quantization.\nC Implementation details\nC.1 OS+\nIn this section, we provide detailed descriptions of\nour implementation with the core part distilled in\nalgorithm 1.\nBERT. On the GLUE benchmark, fine-tuned FP\nmodels are used for quantization. We randomly se-\nlect 128 samples and set the batch size to 32. First,\na batch of data is used to calculate the effective\nshifting and scaling signals for problematic acti-\nvations, especially outputs after LayerNorm here.\nThen shifting and scaling vectors are fused into\nformer operations and absorbed in later modules.\nOn fused models, we apply the calibration proce-\ndure. Particularly, on BERT models, due to the\ngreat variance of token range as discussed in Yao\net al. (2022); Wei et al. (2022b), we incorporate the\nToken-Wise Clipping proposed in Outlier Suppres-\nsion which is an orthogonal technique and weakens\noutliers from the token aspect.\nOPTs. For OPTs, we quantize pre-trained mod-\nels and evaluate them on zero-shot tasks. 128\nsamples are randomly extracted from one of the\ntrain datasets, namely the PILE dataset. As we\nhave observed that LayerNorm produces severe\nasymmetric outliers on certain channels, the pro-\n1663\nName Method OPT-13B OPT-30B OPT-66B OPT-175B\nFP16 INT8* INT8 INT6 FP16 INT8* INT8 INT6 FP16 INT8* INT8 INT6 FP16 INT8* INT8 INT6\nPIQA\nLLM.int8()♣\n75.8\n- 75.8 -\n77.6\n- 77.3 -\n78.7\n- 78.7 -\n79.7\n- 79.6 -\nZeroQuant♣ 54.1 - 53.0 54.2 - 52.0 53.2 - 51.9 52.3 - 53.1SmoothQuant 76.0 - 73.5 77.2 - 66.7 78.3 - 52.0 79.7 - 52.6Ours 76.4 75.9 75.8 77.4 77.6 77.4 78.7 78.6 77.5 79.6 79.5 80.0\nLAMBADA\nLLM.int8()♣\n68.6\n- 68.4 -\n71.5\n- 71.4 -\n73.9\n- 73.8 -\n74.7\n- 74.6\nZeroQuant♣ 0.0 - 0.0 0.0 - 0.0 0.0 - 0.0 0.0 - 0.0SmoothQuant 68.3 - 65.2 71.0 - 13.4 72.9 - 0.0 74.6 - 0.5Ours 68.3 68.4 65.7 70.8 70.8 69.6 73.0 73.4 72.7 74.7 74.5 74.2\nHellaSwag\nLLM.int8()♣\n52.5\n- 52.4 -\n54.3\n- 54.3 -\n56.4\n- 56.3 -\n59.3\n- 59.2 -\nZeroQuant♣ 26.5 - 25.8 26.4 - 25.7 26.1 - 25.7 25.4 - 25.6SmoothQuant 52.2 - 49.2 54.2 - 37.4 55.9 - 26.5 58.9 - 26.0Ours 52.3 52.5 51.7 54.2 54.2 53.7 56.2 56.3 55.8 59.2 59.3 58.5\nWinogrande\nLLM.int8()♣\n65.1\n- 64.8 -\n68.5\n- 68.1 -\n68.9\n- 68.5 -\n72.5\n- 72.3 -\nZeroQuant♣ 52.1 - 51.1 51.8 - 51.8 50.7 - 48.0 50.2 - 49.1SmoothQuant 64.9 - 60.3 68.2 - 55.0 68.3 - 52.1 71.2 - 49.1Ours 65.0 65.3 64.0 68.0 68.5 68.9 69.0 68.8 69.4 72.5 72.5 71.7\nARC LLM.int8()♣\n32.8\n- 33.5 -\n34.6\n- 34.7 -\n37.3\n- 37.0 -\n40.3\n- 40.9 -\nZeroQuant♣ 19.3 - 20.7 19.8 - 20.6 20.8 - 20.4 21.8 - 20.6(Challenge) SmoothQuant 32.1 - 30.6 33.8 - 26.7 36.5 - 21.9 40.5 - 21.2Ours 33.5 33.3 32.7 34.5 34.7 34.6 37.5 37.2 37.0 40.3 39.9 41.0\nARC LLM.int8()♣\n67.3\n- 67.3 -\n70.1\n- 69.7 -\n71.7\n- 71.8 -\n74.9\n- 74.8 -\nZeroQuant♣ 27.5 - 25.0 30.5 - 25.0 29.7 - 26.0 24.0 - 25.6(Easy) SmoothQuant 66.2 - 62.2 69.7 - 55.8 70.5 - 27.8 74.1 - 28.8Ours 67.3 66.8 67.0 70.1 70.0 68.9 71.3 71.8 70.7 74.8 74.7 74.3\nCOPA\nLLM.int8()♣\n86.0\n- 86.0 -\n82.0\n- 82.0 -\n86.0\n- 87.0 -\n88.0\n- 89.0 -\nZeroQuant♣ 63.0 - 55.0 55.0 - 55.0 53.0 - 52.0 60.0 - 55.0SmoothQuant 85.0 - 82.0 83.0 - 75.0 84.0 - 55.0 88.0 - 55.0Ours 85.0 86.0 85.0 83.0 82.0 84.0 85.0 86.0 84.0 88.0 89.0 91.0\nStoryCloze\nLLM.int8()♣\n76.1\n- 76.3 -\n77.0\n- 77.1 -\n77.5\n- 77.7 -\n79.5\n- 79.3 -\nZeroQuant♣ 49.6 - 48.3 48.5 - 48.0 49.2 - 48.4 47.7 - 48.2SmoothQuant 76.0 - 73.5 76.9 - 61.4 77.3 - 48.8 79.1 - 49.8Ours 75.8 76.0 75.4 77.0 76.9 76.6 77.3 76.4 76.6 79.2 79.1 78.1\nAvg. Ours 65.5 65.5 65.5 64.7 67.0 66.9 66.8 66.7 68.8 68.5 68.6 68.0 71.1 71.0 71.1 71.1\nTable 9: Comparison among different techniques in terms of accuracy on eight zero-shot tasks. ♣ denotes dynamic and\nfine-grained quantization, bringing extra computation overhead. INT8* specifically adopts per-tensor quantization for weights\ncompared to INT8.\nposed method is applied here. After obtaining a\nmore quantization-friendly model, the MinMax al-\ngorithm collects distribution statistics. Since di-\nverse tokens do not have outliers of varying degrees\non these models, advanced clipping techniques are\nnot involved.\nBLOOM and BLOOMZ. The main pipeline is\nsimilar to OPTs. The only exception is using the\nToken-Wise Clipping as the calibration method be-\ncause these models hold different outliers among\ndifferent tokens. The clipping ratios are searched\nas 0.5% and 1.5% for 8-bit and 6-bit BLOOM, and\n0.0% and 0.5% on BLOOMZ.\nLLaMA. The main pipeline is similar to OPTs\nwith some small differences. First, we use the Wiki-\nText2 dataset for calibration. Second, as LLaMA\ndoes not have biases, introducing channel-wise\nshifting might incur a little overhead. Thus, for\nfair comparisons, we simply omit channel-wise\nshifting for LLaMA here. Third, when taking the\nharder setting that quantizes the last layer in FFN,\nthe channel-wise scaling is also conducted thereby\nupdating the quantization scale of up proj and\nweight parameters ofdown proj, which does not\nbring computation overhead during inference. Last,\nunlike OPTs, for tasks with normalized accuracy\nmetrics, we report the normalized accuracy metric\ninstead of the accuracy one to align the original\npaper (Touvron et al., 2023). This point has also\nbeen indicated in each table below.\nC.2 Baselines\nWe introduce the implementation details of base-\nlines here. MinMax obtains the minimum and\nmaximum statistics of the tensor for the quanti-\nzation clipping range. Percentile (Wu et al., 2020)\nuses the activation distribution percentile as the\nquantization clipping range. Using the dev set, we\nsearch its hyper-parameters within [0.999, 0.9999,\n0.99999]. OMSE (Choukroun et al., 2019) min-\nimizes the mean squared error between quanti-\nzation and FP signals. PEG (Bondarenko et al.,\n2021) applies fine-grained quantization to problem-\natic activation from a channel perspective. Out-\nlier Suppression (OS) (Wei et al., 2022b) uses\nfixed scaling factors to suppress outliers and fur-\nther clips outliers in a token-wise manner. Zero-\nQuant (Yao et al., 2022) uses per-token quantiza-\ntion, assigning different quantization parameters\nto different tokens. This fine-grained scheme from\n1664\nModel Method PIQA (↑) ARC-e ( ↑) ARC-c( ↑) HellaSwag( ↑) Winogrande ( ↑) WikiText2 ( ↓)\nFP16 INT6 INT4 FP16 INT6 INT4 FP16 INT6 INT4 FP16 INT6 INT4 FP16 INT6 INT4 FP16 INT6 INT4\n7B MinMax77.37 77.53 53.37 52.48 52.36 29.88 41.38 40.35 25.09 72.99 70.98 30.98 66.93 64.72 52.01 5.68 6.22 430.33SQ 76.65 49.80 53.11 30.40 40.10 25.80 71.52 27.40 61.88 48.00 6.15 52.85OS+ 77.20 64.85 52.27 39.60 40.78 31.06 71.68 48.99 65.11 54.85 5.90 40.32\n13B MinMax79.05 77.42 51.14 59.84 57.66 27.61 44.62 42.75 26.28 76.22 74.72 25.92 70.09 65.75 49.88 5.09 5.76 1558SQ 77.80 55.55 56.36 34.51 42.58 26.71 75.11 41.56 68.11 48.70 5.50 79.35OS+ 78.24 62.62 57.83 37.67 43.43 30.46 74.96 52.21 68.59 51.07 5.37 53.64\n30B MinMax80.09 74.92 49.46 58.92 56.31 26.30 45.39 43.69 29.18 79.21 76.14 25.60 72.77 69.69 48.62 4.10 5.54 4958SQ 77.14 50.16 57.61 28.11 42.91 26.71 78.07 31.97 69.92 51.14 5.37 399.65OS+ 79.16 67.19 59.13 48.48 46.25 35.58 78.19 56.44 72.53 51.85 4.48 112.33\n65B MinMax80.85 77.58 49.95 58.75 55.18 26.39 46.25 45.56 26.79 80.73 78.36 25.35 77.11 69.3 48.78 3.56 5.98 54035SQ 77.97 61.81 54.67 40.15 44.62 32.08 77.51 46.19 72.61 50.83 4.00 112.02OS+ 79.76 71.06 56.31 49.49 44.37 37.12 79.00 58.76 73.48 53.08 3.82 32.60\nTable 10: Comparison on LLaMA-1 in terms of normalized accuracy (Touvron et al., 2023) for the first four tasks, accuracy for\nWinogrande and perplexity for WikiText2. The technique in each row is equipped with per-token quantization in ZeroQuant (Yao\net al., 2022). This table would quantize the last layer in FFN compared to Table 3.\nthe token aspect also requires dynamic quantization.\nMeanwhile, for INT8*, we implement per-group\nweight quantization according to its description.\nSmoothQuant (Xiao et al., 2022) migrates scal-\ning factors to later modules to smooth problematic\nactivation. Their scaling factors equal the range\nbetween activation and weights. For lower bits, we\nalso search its hyper-parameter αaccording to its\ndescription for better performance.\n1665",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.7866759896278381
    },
    {
      "name": "Outlier",
      "score": 0.7842130661010742
    },
    {
      "name": "Scaling",
      "score": 0.7403388023376465
    },
    {
      "name": "Computer science",
      "score": 0.6838101148605347
    },
    {
      "name": "Language model",
      "score": 0.6417699456214905
    },
    {
      "name": "Algorithm",
      "score": 0.5284072160720825
    },
    {
      "name": "Channel (broadcasting)",
      "score": 0.43992170691490173
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3333612084388733
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2913554310798645
    },
    {
      "name": "Mathematics",
      "score": 0.19633862376213074
    },
    {
      "name": "Telecommunications",
      "score": 0.08422687649726868
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I174424907",
      "name": "Laboratoire d'Informatique Fondamentale de Lille",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ],
  "cited_by": 28
}