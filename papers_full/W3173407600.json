{
    "title": "Dual-Aspect Self-Attention Based on Transformer for Remaining Useful Life Prediction",
    "url": "https://openalex.org/W3173407600",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2105051328",
            "name": "Zhizheng Zhang",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2095838063",
            "name": "Wen Song",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2114500198",
            "name": "Qiqiang Li",
            "affiliations": [
                "Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A2105051328",
            "name": "Zhizheng Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095838063",
            "name": "Wen Song",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114500198",
            "name": "Qiqiang Li",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2471161958",
        "https://openalex.org/W1481913965",
        "https://openalex.org/W3130100540",
        "https://openalex.org/W2945413072",
        "https://openalex.org/W2958041981",
        "https://openalex.org/W2599868532",
        "https://openalex.org/W2415594836",
        "https://openalex.org/W2512976014",
        "https://openalex.org/W3033580259",
        "https://openalex.org/W2975620261",
        "https://openalex.org/W2133832971",
        "https://openalex.org/W2802866110",
        "https://openalex.org/W2997418121",
        "https://openalex.org/W2106544870",
        "https://openalex.org/W3044911604",
        "https://openalex.org/W2744067593",
        "https://openalex.org/W2910482310",
        "https://openalex.org/W2772084711",
        "https://openalex.org/W2105482032",
        "https://openalex.org/W3014146531",
        "https://openalex.org/W3037511795",
        "https://openalex.org/W3037995823",
        "https://openalex.org/W3006585575",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3116796810",
        "https://openalex.org/W2808622270",
        "https://openalex.org/W2037250283",
        "https://openalex.org/W3170305376",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W6776048684",
        "https://openalex.org/W6796707148",
        "https://openalex.org/W2120841219",
        "https://openalex.org/W3001566134",
        "https://openalex.org/W2910660149",
        "https://openalex.org/W2978540646",
        "https://openalex.org/W3166925235",
        "https://openalex.org/W6725733176",
        "https://openalex.org/W3011973438",
        "https://openalex.org/W2513477101",
        "https://openalex.org/W3002709689",
        "https://openalex.org/W2155832827",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W3128419947",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3171649327",
        "https://openalex.org/W3030520226",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "Remaining useful life prediction (RUL) is one of the key technologies of condition-based maintenance, which is important to maintain the reliability and safety of industrial equipments. Massive industrial measurement data has effectively improved the performance of the data-driven based RUL prediction method. While deep learning has achieved great success in RUL prediction, existing methods have difficulties in processing long sequences and extracting information from the sensor and time step aspects. In this paper, we propose Dual Aspect Self-attention based on Transformer (DAST), a novel deep RUL prediction method, which is an encoder-decoder structure purely based on self-attention without any RNN/CNN module. DAST consists of two encoders, which work in parallel to simultaneously extract features of different sensors and time steps. Solely based on self-attention, the DAST encoders are more effective in processing long data sequences, and are capable of adaptively learning to focus on more important parts of input. Moreover, the parallel feature extraction design avoids mutual influence of information from two aspects. Experiments on two widely used turbofan engines datasets show that our method significantly outperforms the state-of-the-art RUL prediction methods.",
    "full_text": "IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 1\nDual Aspect Self-Attention based on Transformer\nfor Remaining Useful Life Prediction\nZhizheng Zhang, Wen Song, Qiqiang Li\nAbstract—Remaining useful life prediction (RUL) is one of\nthe key technologies of condition-based maintenance, which is\nimportant to maintain the reliability and safety of industrial\nequipments. Massive industrial measurement data has effectively\nimproved the performance of the data-driven based RUL predic-\ntion method. While deep learning has achieved great success in\nRUL prediction, existing methods have difﬁculties in processing\nlong sequences and extracting information from the sensor and\ntime step aspects. In this paper, we propose Dual Aspect Self-\nattention based on Transformer (DAST), a novel deep RUL\nprediction method, which is an encoder-decoder structure purely\nbased on self-attention without any RNN/CNN module. DAST\nconsists of two encoders, which work in parallel to simultaneously\nextract features of different sensors and time steps. Solely based\non self-attention, the DAST encoders are more effective in\nprocessing long data sequences, and are capable of adaptively\nlearning to focus on more important parts of input. Moreover,\nthe parallel feature extraction design avoids mutual inﬂuence of\ninformation from two aspects. Experiments on two widely used\nturbofan engines datasets show that our method signiﬁcantly\noutperforms the state-of-the-art RUL prediction methods.\nIndex Terms—Remaining useful life, Transformer, Dual aspect\nself-attention, Feature fusion.\nI. I NTRODUCTION\nM\nAINTENANCE management plays a very important\nrole in the operation of modern large mechanical equip-\nment [1]. With the rapid development of modern instruments\nand measurement technology, it’s possible to obtain condition\nmonitoring data from running mechanical equipment [2]–[4].\nCondition-based maintenance (CBM) is a maintenance method\nthat requires the measurement of various parameters of the\nequipment and reﬂects the actual status of the equipment\nat any time [5]. Compared with the traditional preventive\nmaintenance strategy, CBM is more effective in reality due to\nthe utilization of real-time system health information, hence\nis widely used in the maintenance of modern industrial equip-\nment [6]. CBM involves predicting the remaining useful life\n(RUL) and potential faults of the equipment according to the\n©2022 IEEE. Personal use of this material is permitted. Permission from\nIEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional\npurposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works.\nThis work was supported in part by the National Natural Science Founda-\ntion of China under Grant 62102228, in part by the Shandong Provincial\nNatural Science Foundation under Grant ZR2021QF063, and in part by\nthe Young Scholar Future Plan of Shandong University under Grant No.\n62420089964188. (Corresponding author: Wen Song and Qiqiang Li)\nZhizheng Zhang, Wen Song and Qiqiang Li are with the Institute\nof Marine Science and Technology, Shandong University, China (Email:\n202020861@mail.sdu.edu.cn, wensong@email.sdu.edu.cn, qqli@sdu.edu.cn).\nDigital Object Identiﬁer 10.1109/TIM.2022.3160561\nreal-time operational status, based on which the maintenance\ndecisions can be made on an as-needed basis according to\nthe prediction information. Obviously, RUL prediction is one\nof the most critical technologies for effective implementation\nof CBM. If the RUL of mechanical equipment is predicted\naccording to the current or historical operation information,\nthe time of failure can be accurately known [7]. Therefore,\nRUL prediction is of great importance to researchers in the\nﬁeld of CBM.\nIn general, RUL prediction methods can be roughly divided\ninto traditional model-based methods, data-driven methods and\nhybrid methods. The model-based method requires accurate\ndynamic modeling of mechanical equipment or components\nto describe the degradation trend of components [8]. However,\nthe structure of modern industrial large-scale equipment is be-\ncoming more and more complex, with miscellaneous nonlinear\nrelationships between various systems and parts. Therefore, it\nis unrealistic to establish an accurate model.\nThe goal of data-driven RUL prediction method is to\nestablish the mapping relationship between RUL and features\nof the target equipment [9]. It does not require extensive expert\nknowledge and physical modeling for complex mechanical\nequipment [10]. In the literature, some traditional machine\nlearning algorithms have been used for RUL prediction, such\nas support vector regression (SVR) [11], random forest (RF)\n[1], and extreme learning machine (ELM) [12]. However, these\nmethods rely on tedious feature engineering. In contrast, deep\nlearning based methods can automatically extract valuable\nfeatures from the original CBM data, and achieve much better\nprediction performance. Consequently, deep learning based\nRUL prediction methods have a wider range of applicability\nand have received increasing attention recently [13].\nThe RUL prediction of mechanical equipment is essentially\na multivariate time series regression task. The strong temporal\nand spatial correlation in the condition monitoring signals can\nbe effectively captured by modern deep architectures such as\nrecurrent neural network (RNN) [14] and convolution neural\nnetwork (CNN) [15], which have been widely used in RUL\nprediction. RNN based methods employ sequence components\nsuch as long short-term memory (LSTM) [16] and gated\nrecurrent unit (GRU) [17] to analyze the signal data sequences.\nHowever, due to the existence of recurrent structures in RNN,\nsequence data needs to pass through each processing unit in\nturn to extract useful features, which inevitably causes the\nproblem of forgetting important information and is less ef-\nfective in learning long-term dependencies. CNN based meth-\nods normally apply one-dimensional convolution and pooling\nﬁlters along the time dimension over all sensors to extract\narXiv:2106.15842v3  [eess.SP]  20 Apr 2022\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 2\nfeature information [18]. However, when processing long time\nsequences, CNN based methods need to continuously increase\nthe size of convolution kernels to obtain larger time step\nreceptive ﬁeld which contain more sequence information. That\nis, the ability of CNN based methods in capturing long-term\ndependent information in sequence data is also limited.\nFor RUL prediction, another key issue is that more attention\nshould be paid to the important features that contain more\ndegradation information. Attention mechanism [19] is an ef-\nfective method to learn such dependencies, i.e. weights among\ndifferent time steps and sensors. Recently, several works\nattempt to combine the attention mechanism with RNN/CNN\nbased structure to predict RUL [20]–[23]. However, there are\ntwo major shortcomings in these methods. Firstly, the inef-\nﬁciency of RNN/CNN in capturing long-term dependencies\nstill can not be avoided. Secondly, the input data enters the\nattention modules and RNN/CNN modules sequentially, which\ncauses the mutual inﬂuence between the extracted feature in-\nformation, thereby affecting the RUL prediction performance.\nTransformer [24] is a recently proposed sequence modeling\narchitecture. It makes use of the self-attention mechanism\nto capture the long-term dependencies between elements in\na sequence without considering their distance, so that it is\nless affected by the increase of sequence length compared\nwith traditional methods such as RNN and LSTM. However,\nfor RUL prediction, the vanilla transformer architecture only\nattention the weights of different time steps and ignores the\nimportance of different sensors in the CBM data stream, which\nis crucial for the overall prediction performance.\nTo overcome the above issues, in this paper, we propose a\nnovel deep RUL prediction method named Dual Aspect Self-\nattention based on Transformer (DAST). In this method, we\napply the Transformer architecture [24] to RUL prediction for\nthe ﬁrst time, which is an encoder-decoder structure purely\nbased on self-attention, without any RNN/CNN module.\nDAST consists of two encoders, i.e. the sensor encoder and\ntime step encoder. Each of the two DAST encoders employs\nthe self-attention mechanism to process all CBM sequence\ndata, and automatically learns to pay different attentions to\ndifferent sensors and time steps. Speciﬁcally, the two decoders\nwork in parallel in the process of feature extraction, therefore\nthe mutual inﬂuence of the two aspect information is avoided.\nThe features extracted by the two encoders are fused together\nand fed into the self-attention based decoder to obtain the\nRUL prediction. The main contributions of this paper are\nsummarized as follows:\n1) We propose a novel end-to-end deep RUL prediction\nmethod based on the Transformer architecture. Exper-\niments on two widely used NASA’s turbofan engine\ndatasets show that our method signiﬁcantly outperforms\nthe state-of-the-art RUL prediction methods.\n2) Based on self-attention, our method is able to auto-\nmatically pay more attentions to the features that are\nmore important without any domain knowledge, and is\nmore effective in handling long CBM data sequences\nthan RNN/CNN based methods. Our novel dual-aspect\ndesign enables extracting features from the sensor and\ntime step dimensions simultaneously, which overcomes\nthe limitation of vanilla Transformer and effectively\nimproves the RUL prediction performance.\n3) The weights of different sensors and time steps learned\nby our method is intuitive and interpretable to the\nmaintenance personnel, so that they can formulate better\nmaintenance strategies to improve efﬁciency.\nThe rest of this paper is organized as follows. The second\nsection introduces the related literature review. The third\nsection introduces the proposed method. In addition, the fourth\npart also demonstrates the effectiveness and superiority of this\nmethod. Finally, the ﬁfth part discusses the advantages com-\npared with the exist methods and the sixth part summarizes\nthe paper.\nII. R ELATED WORKS\nBy modeling the functional relationship between the equip-\nment degradation process and the condition monitoring data,\nthe method based on deep learning can automatically capture\nthe important feature information from the original data to\nachieve end-to-end prediction. In this section, we brieﬂy re-\nview recent deep learning based methods for RUL prediction.\nDue to its advantages in processing condition monitoring\nsequence data, RNN based architecture and its variants (e.g.\nLSTM and GRU) have been widely used in RUL prediction.\nCheng et al. [25] used the LSTM network prediction model to\nverify the effectiveness of LSTM compared to RNN. Chen\net al. [17] proposed a GRU based method for predicting\nnonlinear deterioration process. Besides RNN, CNN archi-\ntecture has also been applied to the RUL prediction task.\nLi et al. [18] proposed a RUL prediction method based on\ndeep convolution neural network (DCNN), which directly\nuses normalized raw data as input and performs convolution\noperation along the time dimension. Zhu et al. [26] proposed\nthe multi-scale convolutional neural network (MSCNN) for\nRUL prediction, which keeps the global and local information\nsynchronously compared to traditional CNN. Some studies\nattempt to combine clustering and deep learning to improve\nRUL prediction performance. Javed et al. [27] proposed a\nprognostics model using the subtractive-maximum entropy\nfuzzy clustering to simultaneously predict machine degra-\ndation. Liu et al. [28] proposed a multi-stage LSTM with\nclustering for RUL prediction, which ﬁrst divides data into\nmultiple stages through clustering analysis, and then extract\ndegradation feature information through the LSTM model.\nRecently, there are several works employ the attention\nmechanism to capture the importance of time steps from\nmonitoring data to improve the performance of RNN/CNN\nbased models. For example, Liu et al. [20] proposed a RUL\nprediction method based on the combination of attention\nmechanism, GRU and CNN. The features extracted by the\nattention mechanism are fed into the bidirectional GRU and\nCNN network to predict RUL. Xiang et al. [21] proposed a\ngear RUL prediction method based on LSTM and attention\nmechanism. Song et al. [22] used the distributed attention\nmechanism and Temporal Convolutional Network (TCN) to\npredict the engine RUL, which can capture more effective\ndegraded feature information through the attention mechanism.\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 3\n \nEncoder Decoder \nPositional Encoding\nInput Embedding\nFeature Fusion Layer Feed Forward Layer\nRUL Prediction\nFlatten Layer\nInput Embedding\nDecoder layer 1\nDecoder layer N\nTime Step Encoder\nlayer 1\nTime Step Encoder\nlayer N\nSensor Encoder\nlayer N\nSensor Encoder\nlayer 1\nSensor Encoder Layer\nTime Step Encoder Layer\nDecoder Layer\nMulti-head Sensor\n Self-Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-head Time Step\n Self-Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nAdd & Norm\nFeed Forward\nAdd & Norm\nEncoder-Decoder\n Self-Attention\nAdd & Norm\nMask Multi-head \nSelf-Attention\nT  time steps \nk  sensors\nx x x……1 2 T \nM  time steps\nk sensors\n…… \nxT+1xT xT+M\n…\n…\ns1\ns2\nsk\ns1\n…\n…\ns2\nsk\nFig. 1. Architecture of DAST.\nChen et al. [23] proposed to combine the attention mechanism\nand LSTM with handcrafted features. However, in most of\nthese studies, attention mechanism is used in combination with\nRNN/CNN architecture, hence the limitation in processing\nlong sequences still exists.\nOwing to its effectiveness in modeling long sequences,\nTransformer has been employed in time-series related tasks re-\ncently. Zhou et al. [29] studied the application of Transformer\nin long sequence time-series prediction and proposed the\nProbSparse self-attention mechanism to reduce the time com-\nplexity and memory usage. Beltagy et al. [30] proposed the\nLongformer with an attention mechanism that scales linearly\nwith sequence length, making it easy to process long sequence.\nA recent survey of transformer variants can be found in [31].\nHowever, most of existing studies only consider capturing\nthe dependencies between time steps. In the RUL prediction,\nthe weight information between different sensors also has a\ngreat inﬂuence on the ﬁnal RUL prediction. Therefore, the\nabove-mentioned research is not suitable for RUL prediction.\nAt present, there are few studies on the application of the\nTransformer architecture to RUL prediction. In this paper, we\nexplore this direction and propose a duel aspect self-attention\ndesign to capturing the weight information of different time\nsteps and sensors at the same time, which makes it better to\nthe RUL prediction task.\nIII. M ETHODOLOGY\nIn this section, we ﬁrst describe the RUL prediction prob-\nlem, and then present the proposed DAST method in detail,\nincluding its architecture and key components.\nA. Problem Description\nIn this paper, the CBM data collected by sensors during\nthe process of turbofan engine operation are used. The RUL\nprediction problem can be formally deﬁned as follows. The\ninput is Xt ∈Rk, t= (1,2,··· ,T), where T is the length of\ntime steps and k is the number of sensors. The corresponding\noutput is the predicted RUL Yt for each time step. Our purpose\nis to predict real-time RUL by establishing the mapping rela-\ntionship between RUL and CBM data, expressed as follows:\nYt = f(Xt), (1)\nwhere Xt is the real-time CBM data during operation of the\nturbofan engine, f is the mapping function, and Yt is the\nreal-time RUL predicted by f. In this paper, we design a\nTransformer based deep architecture to establish the mapping,\nwhich will be detailed in the following subsections.\nB. Model Architecture\nFrom the above description of the RUL prediction problem,\nit can be seen that the current RUL value of the engine\nis mainly determined by signals from different sensors at\ndifferent previous time steps. Therefore, how to make full\nuse of different time steps and sensor information is of\ngreat importance to RUL prediction. Moreover, it is intuitive\nthat different sensors and time steps may contain different\ndegradation information, i.e. they may have different degree\nof importance to the prediction result. In this section, we\ndesign a deep architecture based on self-attention that captures\nthe weighted features from both the sensor and time step\ndimensions, which will be detailed as follows.\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 4\nFig. 1 shows the architecture of our RUL prediction method,\nDual Aspect Self-attention based on Transformer (DAST).\nDAST follows the encoder-decoder structure in the original\nTransformer, and consists of three main substructures in the\nframework: encoder layer (including sensor encoder layer and\ntime step encoder layer), feature fusion layer and decoder\nlayer. Different from the RUL prediction method based on\nRNN and CNN architecture, DAST captures the long-term\ndependence information between the inputs and outputs of\nsequence through self-attention mechanism without consider-\ning the distance, so that the importance of each work cycle\ninformation will not be reduced due to the increase of time step\nlength. Based on the Transformer architecture, we propose a\nnovel feature extract and fusion approach that is more suitable\nfor the RUL prediction task. It enables learning the weights of\ndifferent sensors and time steps at the same time, and obtain\nmore valuable feature information of turbofan engine by fusing\nthe feature information of the two parts.\nThe workﬂow of our DAST model is as follows. First, it\nperforms feature extraction on CBM data collected by mul-\ntiple sensors. Speciﬁcally, we design a dual aspect encoding\nmechanism, which applies the sensor encoder and time step\nencoder that work in parallel to capture the weight features of\ndifferent sensors and time steps. Both encoders are designed\nbased on multi-head self-attention mechanism. Second, the\nfeatures extracted from the two aspects are integrated by the\nfeature fusion layer to get a new feature map with importance\ninformation of different sensors and time steps. Finally, the\nfused feature map is sent to the decoder, which adopts the self-\nattention mechanism to realize the attention of current work\ncycle information and the previous different time steps and\nsensors information and outputs the predicted RUL through\na fully connected feed-forward network (FFN). Next, we will\ndiscuss each of the above mentioned substructures in detail.\nC. Encoder of DAST\nThe encoder of DAST is mainly composed of an input\nembedding layer, a positional encoding layer, and multiple\nsensor encoder layers and time step encoder layers. The input\nembedding layer maps the input state monitoring data to a\nvector of Dmodel dimension through a FFN, in order to prepare\nfor the following feature extraction process. The remaining\ncomponents of encoder is described below.\n1) Positional encoding layer\nAs mentioned, our DAST model does not contain structure\nbased on RNN or CNN. Consequently, we need to inject some\nrelative position tokens into the sequence, so that the model\ncan make full use of the position information of the sequence.\nThere are currently a variety of positional encoding methods\nto choose. In this paper, we use sine and cosine functions of\ndifferent frequencies [24]:\nPt(2k) =sin(t/100002k/Dmodel ) (2)\nPt(2k+ 1) =cos(t/100002k/Dmodel ) (3)\nwhere t is the time step and k is the sensor dimension. In\nthis way, Pt has a linear relationship with Pt+l, where l is\nQ\nK\nMatMui Scale Mask\n(opt.) SoftMax\nMatMui\nV\nInput\nData\nInput\nData\nQ\nK\nMatMui Scale Mask\n(opt.) SoftMax\nMatMui\nV\nInput\nData\n(a) Scaled Dot-Product Self-Attention mechanism.\nQ\nK\nV\nLinearLinear\nLinearLinear\nLinearLinear\nScaled \nDot-Product \nSelf-Attention\nScaled \nDot-Product \nSelf-Attention\nConcat Linear\nh\nQ\nK\nV\nLinear\nLinear\nLinear\nScaled \nDot-Product \nSelf-Attention\nConcat Linear\nh\n(b) Multi-Head Self-Attention mechanism.\nFig. 2. The process of Multi-Head Self-Attention.\nany ﬁxed time step. This allows the model to easily learn to\nattend according to the relative positions.\n2) Sensor encoder layer\nA sensor encoder layer mainly includes two sub-layers:\na multi-head sensor self-attention layer and a FFN layer.\nAs shown in Fig.1, there is a residual connection and layer\nnormalization (Add & Norm) after each sub-layer. The purpose\nof residual connection is to alleviate the difﬁculty of training\ndeep neural network. Layer normalization can accelerate the\ntraining process and make the model converge faster by\nnormalizing the layer activation value. Sensor encoder layer\nuses the multi-head self-attention mechanism [24] to extract\nthe importance of different sensors along the sensor dimension,\ntherefore it can automatically learn to focus on those sensor\nfeatures with higher weights without human experience inter-\nvention in the training process. Next, we will introduce the\nworking process of the multi-head sensor self-attention.\nWe deﬁne the CBM data collected by the ksensors in a time\nwindow of length T as X = {X1,X2,··· ,XT }∈ Rdk×T .\nWe also deﬁne X′\ns as the data obtained after being processed\nby the positional encoding layer. The working process of self-\nattention function is visualized in Fig. 2. First, it generates\nthree matrices (Queries, Keys, Values) by processing the input\ndata using the following computation:\nQs = X′\nsWq\ns ,Ks = X′\nsWk\ns ,Vs = X′\nsWv\ns , (4)\nwhere Wq\ns , Wk\ns , Wv\ns are trainable parameters, Qs,Ks,Vs ∈\nRdk×Dmodel , Dmodel is the input dimension. Then we calculate\nthe dot product of Q and K (scaled by √Dmodel), and apply\na softmax function along the sensor dimension to obtain the\nweights of different sensors in X′\ns. Therefore, the weight\nvector of different sensors at time step t is:\nαt = softmaxsensors( QsKT\ns√Dmodel\n) (5)\nwhere αt = (αt,1,αt,2 ··· ,αt,k),t = (1,2,··· ,T). Finally,\nthe features of different sensors weighted by the self-attention\nmechanism is computed as a weighted sum of Vs:\nAttentionsensors(Qs,Ks,Vs) =αtVs. (6)\nHere we also adopt the multi-head self-attention mechanism\nin [24] to allow the model to jointly attend to information from\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 5\ndifferent representation subspaces at different positions, such\nthat to improve the prediction performance. Fig. 2 visualizes\nmulti-head self-attention, which can be expressed as:\nMultiHead(Qs,Ks,Vs) =Concat({headi}h\ni=1)Ws, (7)\nwhere the parameter matrices Ws ∈RhDmodel×Dmodel , h is the\nnumber of heads, and headi = Attention(Qs,Ks,Vs)i.\n3) Time step encoder layer\nAs shown in Fig. 1, the time step encoder layer has the\nsame structure as the sensor encoder layer. It mainly includes\ntwo sub-layers: a multi-head time step self-attention layer and\na FFN layer. The difference is that the time step encoder layer\nextracts features along the time step dimension, and allows\nthe DAST model to pay attention to the time steps that are\nmore important to the RUL prediction. The input data of time\nstep encoder is the transpose of X and being processed by\nthe positional encoding layer as X′\nt. The time step encoder\nlayer ﬁrst obtains the Queries, Keys, and Values matrices\nQt,Kt,Vt ∈RT×Dmodel by Eq. (4) using trainable parameters\nWq\nt , Wk\nt , Wv\nt . Then, the weight vector of different time steps\ncorresponding to the sensor s can be obtained by performing\nsoftmax along on the time step dimension:\nβs = softmaxtime steps( QtKT\nt√Dmodel\n) (8)\nwhere βs = ( βs,1,βs,2 ··· ,βs,T ),s = (1 ,2,··· ,k). The\nfeatures of different time steps weighted by the self-attention\nmechanism is obtained as:\nAttentiontime steps(Qt,Kt,Vt) =βsVt. (9)\nSimilar to the sensor encoder layer, we also apply multi-\nhead self-attention to the time step encoder layer:\nMultiHead(Qt,Kt,Vt) =Concat({headj}h\nj=1)Wt, (10)\nwhere the parameter matrices Wt ∈ RhDmodel×Dmodel , and\nheadj = Attention(Qt,Kt,Vt)j.\n4) Feature Fusion Layer\nAfter extracting features from the time step and sensor\ndimension of CBM data, DAST performs feature fusion to\nintegrate information from the two aspects. As shown in Fig.\n3, the feature fusion layer combines the time step and sensor\nfeatures to form a new feature map. We denote the features\nextracted from the sensor encoder and time step encoder as\nFs ∈ Rdk×Dmodel and Ft ∈ RT×Dmodel , respectively. DAST\nperforms feature fusion using the following computation:\nFr = Concat (Fs,Ft) Wf , (11)\nwhere the trainable parameter matrices Wf ∈R(dk+T)×Dmodel ,\nwhich can make the model to capture feature information from\nboth Fs and Ft.\nIn DAST, both the sensor encoder and time step encoder are\nconstructed by stacking multiple identical sensor or time step\nencoder layers. Here for convenience, we use the same number\nof stacks N for both encoders, but in general they could\nbe different. The hyperparamter N could affect the feature\nextraction capability of DAST. We empirically tune N in the\nexperiments to obtain good performance. We would like to\nTime Step\nDimension\nInput \nData\nSensor \nEncoder\nTime Step \nEncoder\nFeature\n Fusion \nSensor\nDimension\nNew feature map \nTime Step\nDimension\nInput \nData\nSensor \nEncoder\nTime Step \nEncoder\nFeature\n Fusion \nSensor\nDimension\nNew feature map \nFig. 3. The process of feature fusion.\nnote that unlike previous works [20]–[23], in DAST, features\nof the sensor dimension and time step dimension are extracted\nsimultaneously, since the time step encoder and sensor encoder\nare arranged in parallel. This design effectively avoids the\nmutual inﬂuence of information from the two aspects, which\nhelps to improve the performance of RUL prediction. At the\nsame time, there is no RNN/ CNN module in DAST, which is\npurely based on self-attention mechanism to process the long-\nterm dependence information. We will show the advantage of\nour design in the experiments.\nD. Decoder of DAST\nThe decoder of DAST is designed in a similar way as in\nthe original Transformer [24]. As shown in Fig. 1, the de-\ncoder consists of an input embedding layer, multiple identical\ndecoder layers, a ﬂatten Layer and a FFN layer. A decoder\nlayer mainly includes two multi-head self-attention sub-layers,\nincluding: mask multi-head self-attention and encoder-decoder\nmulti-head self-attention sublayer. The encoder-decoder multi-\nhead self-attention will receive Keys and Values from the\noutput of the encoder, while Queries are from the output\nof the previous layer of the decoder. Therefore, the weight\nfeatures of different sensors and time steps extracted in the\nencoder part are analyzed on the decoder, which realizes\nthe attention of the current working cycle information and\nthe previous different time steps and sensors information and\nﬁnally outputs the predicted RUL through the FFN layer. In\norder to ensure that prediction of a time sequence data point\nwill only depend on previous data points, the masked multi-\nhead self-attention is applied in the self-attention computed\nby setting the corresponding dot products to −∞. Such mask\nmechanism can ensure that model only apply attention to the\ndata points before the target data. In other words, the self-\nattention mechanism will only attention on data xT−1 and\nprevious data when we predict the RUL of xT . In this paper,\nwe follow previous works in RUL prediction and apply the\nrolling prediction.\nIV. E XPERIMENTS\nIn this section, we introduce the experimental dataset,\nrelated experimental settings, and conduct experiments on two\nwidely used turbofan engine datasets to evaluate the effective-\nness of DAST compared to state-of-the-art RUL prediction\nmethods, and to validate the advantage of the DAST design.\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 6\nTABLE I\nDESCRIPTION OF THE DATASETS\nDataset C-MAPSS PHM-2008FD001 FD002 FD003 FD004\nTraining engines 100 260 100 249 218\nTesting engines 100 259 100 248 218\nOperating conditions 1 6 1 6 6\nFault modes 1 1 2 2 2\nTraining set size 20631 53759 24720 61249 45918\nTest set size 100 259 100 248 218\nTime window \nSensor\nTime step\nSensor 1\n. . .\nSensor 2\nSensor 3\nSensor N\nSensor 4\n. . .\nStart End\nTime window \nSensor\nTime step\nSensor 1\n. . .\nSensor 2\nSensor 3\nSensor N\nSensor 4\n. . .\nStart End\nFig. 4. Sliding window processing.\nA. Datasets\nIn this paper, we mainly adopt the widely used C-MAPSS\n(Commercial Modular Aero Propulsion System Simulation)\ndataset [32] to evaluate our method. The C-MAPSS dataset\ncontains four different sub-datasets. As shown in Table I,\nFD001, FD002, FD003, and FD004 have different numbers\nof operating conditions and fault modes. The F002 and F004\ndatasets have more complex operating conditions and fault\nmodes, so RUL is more difﬁcult to predict than the F001\nand F003 datasets. Among the 21 sensors in the C-MAPSS\ndataset (indexed from 1 to 21), sensors 1, 5, 6, 10, 16, 18\nand 19 always have constant values during the run-to-failure\nexperiments, meaning that data from these sensors cannot\ncharacterize the degradation process of the engine. Therefore,\nwe remove these sensor data series and use the data of the\nremaining 14 sensors for RUL prediction. Another problem we\nconsidered is that in actual situation, the degradation process\nof turbofan engine in the early stage can be ignored, that is\nto say, the engine RUL should be kept constant in the early\nstage. Therefore, we follow Zheng et al. [16] and limit the\nengine RUL from start to degradation to RUL max, and the\nlinear degradation of turbofan engine occurs after RUL max. In\nthis work, RUL max is set to 125. Besides C-MAPSS, we also\ntest our method on the PHM 2008 dataset [33]. This dataset\nwas widely used for RUL evaluations, which has the same data\nstructure with C-MAPSS dataset. The details of training and\ntest set are shown in Table I. Each data set contains a training\nset and a test set. The training set contains data for each time\nstep in the complete run-to-failure process. In the test set, the\nengine is randomly stopped before the failure occurrence, and\nthe aim is to predict the true RUL of the last time step.\nB. Experimental Setting\n1) Data preprocessing: We perform the following data\npreprocessing procedures.\nNormalization. Data from different sensors have various\nunits and scales, which could affect the accuracy of RUL\nprediction [34]. Hence, we use the min-max scaler method\nto normalize sensor data. Speciﬁcally, for the CBM data\nXi = {X1,X2,··· ,XT }, we normalize it as follows:\n˜Xi = Xi −min (Xi)\nmax (Xi) −min (Xi), (12)\nwhere ˜Xi is the normalized data, max (Xi) and min (Xi)\ndenote the maximum and minimum of Xi.\nSliding window processing. A sliding window is often\nused for data segmentation preprocessing in order to make the\nmodel get valuable information from multivariate time series\nas much as possible. A simple example of sliding time window\nprocessing is shown in Fig. 4. Tw is the size of time window\nand the sliding stride is set to one. The RUL of the last data\npoint in a time window serves as the RUL of that window.\nWe will discuss the impact of time window size on the model\nprediction performance later.\nStatistical features. The mean value and regression coef-\nﬁcient estimates [22] of the sequence data are two explicit\nnumerical features often used in time sequence data, which\ncan provide useful sequence statistics. In this paper, we add\nthese two parts of feature information to the sequence.\n2) Evaluation metrics: We use two commonly adopted per-\nformance indicators to verify the effectiveness of our method.\nOne is the well-known metric mean square error (RMSE), and\nthe other is the Score metric [32] deﬁned as follows:\nScore =\n\n\n\nn∑\ni=1\ne−\ny\n′\ni−yi\n13 −1, fory\n′\ni −yi <0\nn∑\ni=1\ne\ny\n′\ni−yi\n10 −1, fory\n′\ni −yi ≥0\n(13)\nwhere n is the number of testing samples, y′\ni is the predicted\nRUL and yi is the true RUL for the ith sample. Compared\nto RMSE, the Score metric punishes more when the predicted\nRUL is larger than the true RUL. This is reasonable because in\npractice, such “optimistic” prediction will cause more serious\nimpact. For both RMSE and Score, the lower the value, the\nbetter the prediction accuracy.\n3) Hyperparameters and implementation details: To de-\ntermine the structure of our DAST model, we perform grid\nsearch to ﬁnd the best model conﬁguration. The resulting\nstructural parameters are listed in Table II. The sliding time\nwindow length is set to 40 on the F001 and F003 datasets,\nand 60 on the F002 and F004 datasets, and we will discuss\nthe impact of the window length later. For training, we use\nthe rectiﬁed Adam optimizer and set the epoch to 100. We\nuse RMSE as the training loss function. The learning rate\nis set to 0.001 and the batch size is set to 256. We apply\ndropout for each encoder and decoder layer, and the dropout\nrate is set to 0.2. All experiments are performed on a windows\n10 workstation, which is equipped with 64GB RAM and\nan Intel 9900K CPU. Our code in Pytorch is available at\nhttps://github.com/Zzzsdu/DAST.\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 7\nTABLE II\nSTRUCTURAL PARAMETERS OF DAST\nComponents Layers Parameters\nInput Fully connected layer Hidden units: 64\nembedding Activation: Linear\nEncoder\nSensor encoder layer Encoder blocks N = 2\nSelf-attention heads H = 4\nTime step encoder layer Encoder blocks N = 2\nSelf-attention heads H = 4\nDecoder Decoder layer Decoder blocks N = 1\nSelf-attention heads H = 4\nOutput\nFully connected layer Hidden units: 64\nActivation: (ReLU)\nOutput layer Hidden units: 1\nActivation: Linear\nC. Comparison with Other Methods\nHere we compare the performance of DAST with state-\nof-the-art deep learning based RUL prediction methods. The\nbaselines are from three categories: 1) pure RNN/CNN based\nmethods [18], [25], [35], 2) RUL prediction methods based on\nthe combination of RNN/CNN architecture and the attention\nmechanism [20], [22], [23], [37], and 3) health indicators\nbased methods [36], [38], [39], which are recently proposed\nto combine device health indicators and deep learning models\nto improve prediction performance. To mitigate the impact of\nrandomness, we repeat all prediction experiments 10 times,\nand report the average performances throughout the paper.\n1) Comparison on the C-MAPSS dataset: We ﬁrst compare\nour method with the above baselines on the C-MAPSS datset.\nAs shown in Table III and Table IV, our method consistently\noutperforms all comparing ones with the smallest value in\nboth RMSE and Score on average, showing that DAST can\nproduce more accurate RUL prediction. More importantly, the\nimprovement of DAST against the best results of existing\nmethods tends to be larger on the two harder datasets F002\nand F004. Speciﬁcally, the corresponding RMSE of DAST\nis reduced by 10.02% on F002 and the Score is reduced by\n35.67% on F004 compared with state of the art. Another key\nobservation is that, the improvement of DAST is much more\nprominent in terms of Score, showing its advantage in this\npractical metric. Our method also outperforms recent health\nindicators construction based methods [36], [38], [39] which\nrequires experienced feature engineering process to generate\nthe representative health indicators. We can see from Table III\nand IV that results of DAST are better than [36] on all the four\nsub-datasets. For [38] and [39] which only reported results on\nF001, the best result is 12.80 for RMSE and 256 for Score,\nwhich are worse than our prediction results in Table III and IV.\nTo summarize, the above results and discussions show that the\nproposed DAST model has good ability in modeling complex\nmultivariate time series data and good application prospect in\npractical RUL prediction.\n2) Detailed analysis of the prediction results: Next, we\nperform more ﬁne-grained analysis on the prediction results.\nFirst, to visually analyze the RUL predicted by our DAST\nmodel, we randomly select an engine unit from each of four\nC-MAPSS datasets, and compare the predicted RUL with the\nactual RUL of engine. We also compare two representative\n(a) F001 Testing Engine unit24\n (b) F002 Testing Engine unit80\n(c) F003 Testing Engine unit99\n (d) F004 Testing Engine unit8\nFig. 5. RUL prediction results on four test engine units.\nRNN and CNN based methods, i.e. BiLSTM [35] and DCNN\n[18]. All results are plotted in Fig. 5. We can see that the\npredicted RUL of our method has a similar trajectory to\nthe real RUL and is clearly better than those of BiLSTM\nand DCNN, showing the effectiveness of DAST in capturing\ndegradation information. Moreover, most of the RUL values\npredicted by DAST are close to or smaller than the actual\nRUL, which is desirable because overestimation of RUL could\ncause more serious consequences than underestimation. This\nshows why the improvement of our method in Score reported\nin Table IV is more signiﬁcant. Compared to the later stages,\nthe prediction error tends to be larger in the early stages. This\nis because when the engine begins to enter the degradation\nstage, the CBM data contains more degradation information,\nwhich makes the later predictions more precise [20]. We can\nalso see that compared with the other two datasets, F002 and\nF004 are more difﬁcult to predict, since they contain more\ncomplicated operating conditions and fault modes.\nThen, we conduct pairwise comparison to analyze the\nperformance of DAST against BiLSTM and DCNN. We take\nF002 dataset as an example, for which we plot the prediction\nresults of DAST and the comparing methods as a pair for\nall the 259 engines in Fig. 6. Note that points above the\ndiagonal (orange line) indicate that the RMSE/Score values\nof BiLSTM/DCNN are higher than DAST. We can observe\nthat most of the points distribute above the diagonal, showing\nthat DAST gives better prediction for most engines. In fact,\nfor the 259 engines, DAST has smaller RMSE and Score than\nBiLSTM on 75.6% and 72.6% engines, and also outperforms\nDCNN on 78.3% and 81.6% engines, respectively.\nWe further compare the training and testing time efﬁciency\nof DAST with the two representative RNN/CNN based meth-\nods BiLSTM and DCNN in the same experimental environ-\nment, taking F001 dataset as an example. As shown in Table\nV, the training time of DAST is higher than that of the CNN\nbased method DCNN. Considering that model training is only\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 8\nTABLE III\nRMSE COMPARISON WITH STATE -OF-THE -ART METHODS\nDataset BiLSTM [35] DCNN [18] ELSTMNN [25] Kong et al. [36] Chen et al. [23] DATCN [22] DARNN [37] AGCNN [20] DAST\nF001 13.65 12.61 18.22 16.13 14.53 11.78* 12.04 12.42 11.43\nF002 23.18 22.36 / 20.46 / 16.95* 19.24 19.43 15.25\nF003 13.74 12.64 23.21 17.12 / 11.56 10.18* 13.39 11.32\nF004 24.86 23.31 / 23.26 27.08 18.23 18.02* 21.50 18.36\nAverage 18.85 17.73 / 19.24 / 14.63* 14.87 16.68 14.09\n* means the best result in the baseline methods, bold means the best result in all methods.\nTABLE IV\nSCORE COMPARISON WITH STATE -OF-THE -ART METHODS\nDataset BiLSTM [35] DCNN [18] ELSTMNN [25] Kong et al. [36] Chen et al. [23] DATCN [22] DARNN [37] AGCNN [20] DAST\nF001 295 273.7 571 303 322.44 229.48 261.95 225.51* 203.15\nF002 4130 10412 / 3440 / 1842.38 933.58* 1492 924.96\nF003 317 284.1 839 1420 / 257.11 247.85 227.09* 154.92\nF004 5430 12466 / 4630 5649.14 2317.32* 2587.44 3392 1490.72\nAverage 2543 5858.95 / 2448.25 / 1161.57 1007.71* 1334.15 693.43\n* means the best result in the baseline methods, bold means the best result in all methods.\n(a) RMSE comparison of BiLSTM\n (b) RMSE comparison of DCNN\n(c) Score comparison of BiLSTM\n (d) Score comparison of DCNN\nFig. 6. Pairwise comparison of DAST with BiLSTM and DCNN on F002.\nrequired once and ofﬂine, this amount of time for training\nis still acceptable. The testing time of DAST for all the 100\nengines is only 0.03s on a CPU, which means that the testing\ntime for one engines is 0.3ms. Hence, the proposed method\ncan meet the requirement of real-time RUL prediction.\n3) Comparison on the PHM 2008 dataset: For this exper-\niment, we apply the same DAST model as in the C-MAPSS\nexperiments, with sliding time window size Tw = 60. Because\nthe PHM 2008 dataset does not contain actual RUL labels,\nwe need to upload the prediction results to the NASA data\nrepository website1 to get the Score metric for evaluation. In\n1https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/\nTABLE V\nTRAINING AND TESTING TIME COMPARISON\nMethod DAST DCNN [18] BiLSTM [35]\nTraining time (s) 1229.74 715.35 1269.78\nTesting time (s) 0.030 0.014 0.042\nRMSE 11.43 12.61 13.65\nScore 203.15 273.70 295.00\nTABLE VI\nRESULTS ON THE PHM 2008 DATASET\nMethod Score\nMLP [40] 3212\nSVR [40] 15886\nRVR [40] 8242\nCNN [40] 2056\nDeep LSTM [16] 1862\nChen et al. [23] 1584*\nDAST 845\nImp 46.7%\nTable VI, we list the performance of DAST and several recent\nmethods. As we can see, DAST signiﬁcantly outperforms all\nthe baselines. Compared to the most competitive method in\n[23], DAST achieves an improvement of 46.7%, showing its\neffectiveness in this dataset.\nD. Analysis of DAST\n1) Impact of the sliding time windows size: The multivari-\nate time series data contains data with different RUL informa-\ntion, so it is necessary to choose a reasonable time window\nsize before training. To verify the inﬂuence of this parameter,\nwe conduct several groups of comparative experiments using\nthe four sub-datasets in C-MAPSS, by setting Tw to 30, 40,\n..., 70. The results are plotted in Fig. 7. We can observe from\nFig. 7 that the RMSE and Score values are the smallest when\nthe time window length is 40 on the F001 and F003 datasets,\nwhile for F002 and F004 the two metrics are the best when\nTw = 60. This is because the F002 and F004 datasets have\nmore complex operating conditions and failure modes than the\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 9\nFig. 7. Performance of DAST with different time window sizes on the C-\nMAPSS dataset.\nTABLE VII\nABLATION STUDY ON F002\nMethod Metric Mean STD\nDAST w/o sensor encoder\n(vanilla Transformer)\nRMSE 16.47 0.39\nScore 1638.36 132.64\nDAST w/o time step encoder RMSE 16.11 0.35\nScore 1476.73 119.82\nDAST with sensor and time\nstep encoder arranged in series\nRMSE 15.86 0.21\nScore 1367.61 49.67\nDAST RMSE 15.25 0.24\nScore 924.96 47.67\nF001 and F003 datasets, so larger time window sizes could\ncontain more degradation information.\n2) Ablation study of DAST: Here we evaluate the effec-\ntiveness of some internal components of our method using\nablation study. More speciﬁcally, we evaluate the two key\ncomponents, i.e. the sensor encoder and time step encoder,\nas well as the parallel feature extraction design. To this end,\nwe perform three experiments in this part: DAST without\nsensor encoder, DAST without time step encoder, and DAST\nwith sensor encoder and time step encoder arranged in series.\nNote that DAST without sensor encoder is essentially the\nvanilla Transformer. For the third experiment, the input data\nﬁrst pass through the sensor encoder, the output of which\nis treated as input to the time step encoder. We take the\nF002 dataset as an example, and the experimental results are\nshown in TABLE VII. The experimental results show that\nthe prediction performance based DAST is superior than the\noriginal Transformer. Moreover, when any of the sensor or\ntime step encoder is removed, the performance signiﬁcantly\ndrops, showing that information from both aspects are valuable\nfor RUL prediction. DAST also outperforms the version that\nextracts features in series, which veriﬁes the effectiveness of\nour parallel design in alleviating the mutual inﬂuence between\ninformation from the two aspects.\n3) Visualization of the learned weights: One advantage of\nDAST is that, the two encoders can automatically learn the\nweights of different sensors and time steps, which represents\ntheir importance for the real-time RUL prediction. The weight\ninformation is not only useful in improving the prediction\nperformance, but also can be understood by the maintenance\npersonnel to focus on more important sensors and time steps\nin real time and thus improve maintenance efﬁciency. To\nvisualize this point, we choose 30 consecutive working time\ncycles (150-180) of engine unit 99 in the F003 dataset, and\nplot the average weights of each sensor and time step in the\n(a) The average weights of sensors\n(b) The average weights of time steps\nFig. 8. The average weights of each sensor and time step.\ncorresponding sliding window in Fig. 8. We can see that in\nthis period, sensors T50, Nc, and phi are more important than\nother sensors, while for time steps, those from 15 to 24 and\nthe last step are more informative for RUL prediction.\nV. D ISCUSSION\nWe highlight the advantages of the proposed method com-\npared to the shortcomings of current methods in this section.\nFirst of all, existing deep learning based RUL prediction\nmethods are mainly based on the RNN/CNN architecture [16],\n[18], [25], [35], [36]. Some recent studies tried to combine\nthe RNN/CNN architecture and the Attention mechanism to\npropose the RUL prediction method [20]–[23], which ﬁrst\nextracts the feature information of sequence data through\nRNN/CNN, and then learns the importance of the feature\ninformation through the attention mechanism. Although the\nabove methods achieved relatively good prediction effect in\nRUL prediction, there are still many shortcomings. For one\nthing, due to the existence of the structure of RNN/CNN, the\nRUL prediction model still has a bottleneck in extracting long-\nterm dependency information. For another, the problems in the\nRUL prediction methods built by combining the RNN/CNN\narchitecture with the attention mechanism are that the input\ndata enters the attention modules and RNN/CNN modules\nsequentially, which causes the mutual inﬂuence between the\nextracted different feature information, thereby affecting the\nRUL prediction performance.\nDifferent from the RUL prediction method [20]–[23], our\nmethod is built upon the Transformer architecture, which is\npurely based on the self-attention mechanism to process all\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 10\nCBM data points in the sequence without considering their\ndistance. In particular, we propose the design of the duel\naspect feature extraction in parallel, which not only improves\nthe ability of the model in capturing long-term dependency\ninformation but also enables it to focus on the more important\nsensor and time step information respectively, so as to avoid\nthe mutual inﬂuence of the two aspect information. Our ex-\nperimental comparison with the above RUL prediction method\nhave veriﬁed the advantage of our method. In the ablation\nexperiments, we have veriﬁed the effectiveness of our parallel\nfeature extraction design, which is signiﬁcantly better than the\nvanilla Transformer structure [24].\nVI. C ONCLUSIONS AND FUTURE WORK\nIn this paper, we propose a novel deep RUL prediction\nmethod named Dual Aspect Self-attention based on Trans-\nformer (DAST). Without any RNN/CNN structure, DAST uses\nthe self-attention mechanism to process the entire CBM data\nsequence. In particular, it is designed based on a parallel\nfeature extraction scheme, which employs a sensor encoder\nand a time step encoder to capture the weighted features of\ndifferent sensors and time steps simultaneously. The proposed\nparallel encoding architecture which runs the two encoders\nsimultaneously and then fuse the two set of features can\navoid mutual inﬂuence of information from the two aspects.\nWithout the need of human intervention, the DAST model can\nadaptively learn the importance of different sensors and time\nsteps, which could be helpful to the maintenance personnel\nto focus on those more important sensors and time steps, so\nas to improve maintenance efﬁciency. We conduct ablation\nstudies to prove the effectiveness of our design. Experimental\nresults on two real turbofan engine datasets show that the RUL\nprediction performance of our method is superior to state-of-\nthe-art deep RUL prediction methods. In the future, we would\nlike to explore the combination of deep learning methods and\ndata enhancement techniques to resolve the issue of lacking\ntraining data, which is important for practical RUL prediction.\nREFERENCES\n[1] C. Zhang, P. Lim, A. K. Qin, and K. C. Tan, “Multiobjective deep belief\nnetworks ensemble for remaining useful life estimation in prognostics,”\nIEEE Transactions on Neural Networks and Learning Systems , vol. 28,\nno. 10, pp. 2306–2318, 2016.\n[2] Y . Qian and R. Yan, “Remaining useful life prediction of rolling bearings\nusing an enhanced particle ﬁlter,” IEEE Transactions on Instrumentation\nand Measurement, vol. 64, no. 10, pp. 2696–2707, 2015.\n[3] H. Zhao, H. Liu, Y . Jin, X. Dang, and W. Deng, “Feature extraction for\ndata-driven remaining useful life prediction of rolling bearings,” IEEE\nTransactions on Instrumentation and Measurement , vol. 70, pp. 1–10,\n2021.\n[4] W. Mao, J. He, and M. J. Zuo, “Predicting remaining useful life of rolling\nbearings based on deep feature representation and transfer learning,”\nIEEE Transactions on Instrumentation and Measurement, vol. 69, no. 4,\npp. 1594–1608, 2019.\n[5] L. Cui, X. Wang, H. Wang, and J. Ma, “Research on remaining useful\nlife prediction of rolling element bearings based on time-varying kalman\nﬁlter,” IEEE Transactions on Instrumentation and Measurement, vol. 69,\nno. 6, pp. 2858–2867, 2019.\n[6] Q. Zhai and Z. S. Ye, “Rul prediction of deteriorating products using\nan adaptive wiener process model,” IEEE Transactions on Industrial\nInformatics, vol. 13, no. 6, pp. 1–1, 2017.\n[7] G. S. Babu, P. Zhao, and X.-L. Li, “Deep convolutional neural network\nbased regression approach for estimation of remaining useful life,” in\nInternational Conference on Database Systems for Advanced Applica-\ntions, 2016.\n[8] J. Park, J. M. Ha, H. Oh, B. D. Youn, J.-H. Choi, and N. H. Kim,\n“Model-based fault diagnosis of a planetary gear: A novel approach\nusing transmission error,” IEEE Transactions on Reliability , vol. 65,\nno. 4, pp. 1830–1841, 2016.\n[9] Y . Qin, D. Chen, S. Xiang, and C. Zhu, “Gated dual attention unit neural\nnetworks for remaining useful life prediction of rolling bearings,” IEEE\nTransactions on Industrial Informatics , vol. 17, no. 9, pp. 6438–6447,\n2021.\n[10] H. Meng and Y .-F. Li, “A review on prognostics and health management\n(phm) methods of lithium-ion batteries,” Renewable and Sustainable\nEnergy Reviews, vol. 116, p. 109405, 2019.\n[11] T. H. Loutas, D. Roulias, and G. Georgoulas, “Remaining useful\nlife estimation in rolling bearings utilizing data-driven probabilistic e-\nsupport vectors regression,” IEEE Transactions on Reliability , vol. 62,\nno. 4, pp. 821–832, 2013.\n[12] Z. Liu, Y . Cheng, P. Wang, Y . Yu, and Y . Long, “A method for remaining\nuseful life prediction of crystal oscillators using the bayesian approach\nand extreme learning machine under uncertainty,” Neurocomputing, vol.\n305, pp. 27–38, 2018.\n[13] J. Guo, Z. Li, and M. Li, “A review on prognostics methods for\nengineering systems,” IEEE Transactions on Reliability , vol. 69, no. 3,\npp. 1110–1129, 2019.\n[14] A. Malhi, R. Yan, and R. X. Gao, “Prognosis of defect propagation based\non recurrent neural networks,” IEEE Transactions on Instrumentation\nand Measurement, vol. 60, no. 3, pp. 703–711, 2011.\n[15] T. San Kim and S. Y . Sohn, “Multitask learning for health condition\nidentiﬁcation and remaining useful life prediction: Deep convolutional\nneural network approach,” Journal of Intelligent Manufacturing , pp. 1–\n11, 2020.\n[16] S. Zheng, K. Ristovski, A. Farahat, and C. Gupta, “Long short-\nterm memory network for remaining useful life estimation,” in 2017\nIEEE International Conference on Prognostics and Health Management\n(ICPHM), 2017.\n[17] J. Chen, H. Jing, Y . Chang, and Q. Liu, “Gated recurrent unit based\nrecurrent neural network for remaining useful life prediction of nonlinear\ndeterioration process,”Reliability Engineering & System Safety, vol. 185,\npp. 372–382, 2019.\n[18] X. Li, Q. Ding, and J.-Q. Sun, “Remaining useful life estimation\nin prognostics using deep convolution neural networks,” Reliability\nEngineering & System Safety , vol. 172, pp. 1–11, 2018.\n[19] D. Bahdanau, K. H. Cho, and Y . Bengio, “Neural machine translation\nby jointly learning to align and translate,” in International Conference\non Learning Representations (ICLR) , 2015.\n[20] H. Liu, Z. Liu, W. Jia, and X. Lin, “Remaining useful life prediction\nusing a novel feature-attention-based end-to-end approach,” IEEE Trans-\nactions on Industrial Informatics , vol. 17, no. 2, pp. 1197–1207, 2020.\n[21] S. Xiang, Y . Qin, C. Zhu, Y . Wang, and H. Chen, “Lstm networks based\non attention ordered neurons for gear remaining life prediction,” ISA\nTransactions, vol. 106, pp. 343–354, 2020.\n[22] Y . Song, S. Gao, Y . Li, L. Jia, Q. Li, and F. Pang, “Distributed\nattention-based temporal convolutional network for remaining useful life\nprediction,” IEEE Internet of Things Journal , 2020.\n[23] Z. Chen, M. Wu, R. Zhao, F. Guretno, R. Yan, and X. Li, “Machine\nremaining useful life prediction via an attention-based deep learning\napproach,” IEEE Transactions on Industrial Electronics , vol. 68, no. 3,\npp. 2521–2531, 2020.\n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances in\nNeural Information Processing Systems (NIPS) , 2017, pp. 5998–6008.\n[25] Y . Cheng, J. Wu, H. Zhu, S. W. Or, and X. Shao, “Remaining useful life\nprognosis based on ensemble long short-term memory neural network,”\nIEEE Transactions on Instrumentation and Measurement , vol. 70, pp.\n1–12, 2020.\n[26] J. Zhu, N. Chen, and W. Peng, “Estimation of bearing remaining\nuseful life based on multiscale convolutional neural network,” IEEE\nTransactions on Industrial Electronics , vol. 66, no. 4, pp. 3208–3216,\n2018.\n[27] K. Javed, R. Gouriveau, and N. Zerhouni, “A new multivariate approach\nfor prognostics based on extreme learning machine and fuzzy cluster-\ning,”IEEE Transactions On Cybernetics, vol. 45, no. 12, pp. 2626–2639,\n2015.\nIEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT 11\n[28] J. Liu, F. Lei, C. Pan, D. Hu, and H. Zuo, “Prediction of remaining useful\nlife of multi-stage aero-engine based on clustering and lstm fusion,”\nReliability Engineering & System Safety , vol. 214, p. 107807, 2021.\n[29] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,\n“Informer: Beyond efﬁcient transformer for long sequence time-series\nforecasting,” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence (AAAI), 2021.\n[30] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-\ndocument transformer,” arXiv preprint arXiv:2004.05150 , 2020.\n[31] T. Lin, Y . Wang, X. Liu, and X. Qiu, “A survey of transformers,” arXiv\npreprint arXiv:2106.04554, 2021.\n[32] A. Saxena, K. Goebel, D. Simon, and N. Eklund, “Damage propagation\nmodeling for aircraft engine run-to-failure simulation,” in 2008 Interna-\ntional Conference on Prognostics and Health Management , 2008.\n[33] A. Saxena and K. Goebel, “Phm08 challenge data set.” in NASA AMES\nPrognostics Data Repository . Moffett Field, CA, Tech. Rep, 2008.\n[34] H. Li, W. Zhao, Y . Zhang, and E. Zio, “Remaining useful life prediction\nusing multi-scale deep convolutional neural network,” Applied Soft\nComputing, vol. 89, p. 106113, 2020.\n[35] J. Wang, G. Wen, S. Yang, and Y . Liu, “Remaining useful life estimation\nin prognostics using deep bidirectional lstm neural network,” in 2018\nPrognostics and System Health Management Conference (PHM) , 2018.\n[36] Z. Kong, Y . Cui, Z. Xia, and H. Lv, “Convolution and long short-\nterm memory hybrid deep neural networks for remaining useful life\nprognostics,” Applied Sciences, vol. 9, no. 19, p. 4156, 2019.\n[37] F. Zeng, Y . Li, Y . Jiang, and G. Song, “A deep attention residual\nneural network-based remaining useful life prediction of machinery,”\nMeasurement, p. 109642, 2021.\n[38] P. Malhotra, V . TV , A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal,\nand G. Shroff, “Multi-sensor prognostics using an unsupervised health\nindex based on lstm encoder-decoder,”arXiv preprint arXiv:1608.06154,\n2016.\n[39] K. T. Nguyen and K. Medjaher, “An automated health indicator construc-\ntion methodology for prognostics based on multi-criteria optimization,”\nISA Transactions, vol. 113, pp. 81–96, 2021.\n[40] G. Sateesh Babu, P. Zhao, and X.-L. Li, “Deep convolutional neural\nnetwork based regression approach for estimation of remaining useful\nlife,” in International Conference On Database Systems For Advanced\nApplications. Springer, 2016, pp. 214–228.\nZhizheng Zhang received the M.S. degree in ma-\nrine engineering of in Dalian Maritime University,\nDalian, China, in 2020. He is currently pursuing the\nPh.D. degree in control science and engineering from\nShandong University. His current research interests\ninclude deep learning, deep reinforcement learning,\nremaining useful life prediction, multivariate time\nseries prediction.\nWen Song received the B.S. degree in automation\nand the M.S. degree in control science and engi-\nneering from Shandong University, Jinan, China, in\n2011 and 2014, respectively, and the Ph.D. degree in\ncomputer science from the Nanyang Technological\nUniversity, Singapore, in 2018. He was a Research\nFellow with the Singtel Cognitive and Artiﬁcial\nIntelligence Lab for Enterprises (SCALE@NTU).\nHe is currently an Associate Research Fellow with\nthe Institute of Marine Science and Technology,\nShandong University. His current research interests\ninclude artiﬁcial intelligence, deep reinforcement learning, planning and\nscheduling, and operations research.\nQiqiang Li received the Ph.D. degree in industrial\nautomation from the Institute of Industrial Process\nControl, Zhejiang University in 1998. He is a Pro-\nfessor with the School of Control Science and En-\ngineering and the Institute of Marine Science and\nTechnology, Shandong University. His research area\nfocuses on modeling, optimization, and simulation\nof complex systems. His current research interests\nare concerned with economic operation optimization\nof photovoltaic systems, energy efﬁciency of process\nindustry and commercial buildings."
}