{
  "title": "Pre-Trained Image Processing Transformer",
  "url": "https://openalex.org/W3109319753",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2675786662",
      "name": "Chen, Hanting",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A723829169",
      "name": "Wang, Yunhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1544282110",
      "name": "Guo Tianyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101184268",
      "name": "Xu Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224712012",
      "name": "Deng Yi-ping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2022054504",
      "name": "Liu Zhenhua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745269253",
      "name": "Ma, Siwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3087778642",
      "name": "XU Chunjing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1992924849",
      "name": "Xu Chao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1921069170",
      "name": "Gao Wen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963306157",
    "https://openalex.org/W3015146382",
    "https://openalex.org/W2963667985",
    "https://openalex.org/W2964267765",
    "https://openalex.org/W2788343277",
    "https://openalex.org/W2964125708",
    "https://openalex.org/W1906770428",
    "https://openalex.org/W2999653953",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3035212231",
    "https://openalex.org/W3096739052",
    "https://openalex.org/W2997058852",
    "https://openalex.org/W2965217508",
    "https://openalex.org/W2950217418",
    "https://openalex.org/W2466666260",
    "https://openalex.org/W3034347085",
    "https://openalex.org/W3128419980",
    "https://openalex.org/W2990984982",
    "https://openalex.org/W3104725225",
    "https://openalex.org/W2948300571",
    "https://openalex.org/W2964046397",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3083579885",
    "https://openalex.org/W54257720",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2047710600",
    "https://openalex.org/W3109746756",
    "https://openalex.org/W3039037079",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2953484813",
    "https://openalex.org/W3000775737",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W1988952689",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2954171777",
    "https://openalex.org/W2910832120",
    "https://openalex.org/W2741137940",
    "https://openalex.org/W2613155248",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035484352",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2961218591",
    "https://openalex.org/W2962935103",
    "https://openalex.org/W2509784253",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W3114167873",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963645458",
    "https://openalex.org/W3107405705",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2739815012",
    "https://openalex.org/W2912435603",
    "https://openalex.org/W3111921445",
    "https://openalex.org/W2739097844",
    "https://openalex.org/W2256362396",
    "https://openalex.org/W2947156405",
    "https://openalex.org/W2982093251",
    "https://openalex.org/W2170590026",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2780930362",
    "https://openalex.org/W2740982616",
    "https://openalex.org/W2788682721",
    "https://openalex.org/W2884068670",
    "https://openalex.org/W2963725279",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2930755307",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W2913360047",
    "https://openalex.org/W3124818450",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3034789174",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2964030969",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2560533888",
    "https://openalex.org/W3132890542",
    "https://openalex.org/W3103292251",
    "https://openalex.org/W2954930822",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2963878020"
  ],
  "abstract": "As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT",
  "full_text": "Pre-Trained Image Processing Transformer\nHanting Chen1,2, Yunhe Wang2*, Tianyu Guo1,2, Chang Xu3, Yiping Deng4,\nZhenhua Liu2,5,6, Siwei Ma5,6, Chunjing Xu2, Chao Xu1, Wen Gao5,6\n1 Key Lab of Machine Perception (MOE), Dept. of Machine Intelligence, Peking University. 2 Noah’s Ark Lab, Huawei Technologies.\n3 School of Computer Science, Faculty of Engineering, The University of Sydney. 4 Central Software Institution, Huawei Technologies.\n5 Institute of Digital Media, School of Electronic Engineering and Computer Science, Peking University. 6 Peng Cheng Laboratory.\nhtchen@pku.edu.cn, yunhe.wang@huawei.com\nAbstract\nAs the computing power of modern hardware is in-\ncreasing strongly, pre-trained deep learning models ( e.g.,\nBERT, GPT-3) learned on large-scale datasets have shown\ntheir effectiveness over conventional methods. The big\nprogress is mainly contributed to the representation abil-\nity of transformer and its variant architectures. In this\npaper, we study the low-level computer vision task ( e.g.,\ndenoising, super-resolution and deraining) and develop a\nnew pre-trained model, namely, image processing trans-\nformer (IPT). To maximally excavate the capability of trans-\nformer, we present to utilize the well-known ImageNet\nbenchmark for generating a large amount of corrupted\nimage pairs. The IPT model is trained on these images\nwith multi-heads and multi-tails. In addition, the con-\ntrastive learning is introduced for well adapting to differ-\nent image processing tasks. The pre-trained model can\ntherefore efﬁciently employed on desired task after ﬁne-\ntuning. With only one pre-trained model, IPT outperforms\nthe current state-of-the-art methods on various low-level\nbenchmarks. Code is available at https://github.\ncom/huawei-noah/Pretrained-IPT and https:\n//gitee.com/mindspore/mindspore/tree/\nmaster/model_zoo/research/cv/IPT\n1. Introduction\nImage processing is one component of the low-level part\nof a more global image analysis or computer vision system.\nResults from the image processing can largely inﬂuence the\nsubsequent high-level part to perform recognition and un-\nderstanding of the image data. Recently, deep learning has\nbeen widely applied to solve low-level vision tasks, such as\nimage super-resolution, inpainting, deraining and coloriza-\ntion. As many image processing tasks are related, it is nat-\n*Corresponding author\n26.6\n26.7\n26.8\n26.9\n27\n27.1\n27.2\n27.3\nHAN\n(ECCV 2020)\nIPT\n31.5\n31.6\n31.7\n31.8\n31.9\n32\n32.1\nRDN\n(CVPR 2018)\nIPT\n0.3dB↑\n0.4dB↑\nDenoising (30) Denoising (50) Deraining\n33.1\n33.2\n33.3\n33.4\n33.5\n33.6\n33.7\n33.8\nHAN\n(ECCV 2020)\nIPT\n39\n39.5\n40\n40.5\n41\n41.5\n42\nRCDNet\n(CVPR 2020)\nIPT\n28.9\n29\n29.1\n29.2\n29.3\n29.4\n29.5\n29.6\nHAN\n(ECCV 2020)\nIPT\n28.95\n29.1\n29.25\n29.4\n29.55\n29.7\n29.85\nRDN\n(CVPR 2018)\nIPT\nSISR x2 SISR x3 SISR x4\n0.4dB↑ 0.4dB↑\n0.4dB↑ 1.6dB↑\nFigure 1. Comparison on the performance of the proposed IPT and\nthe state-of-the-art image processing models on different tasks.\nural to expect a model pre-trained on one dataset can be\nhelpful for another. But few studies have generalized pre-\ntraining across image processing tasks.\nPre-training has the potential to provide an attractive so-\nlution to image processing tasks by addressing the follow-\ning two challenges: First, task-speciﬁc data can be limited.\nThis problem is exacerbated in image processing task that\ninvolves the paid-for data or data privacy, such as medical\nimages [8] and satellite images [83]. Various inconsistent\nfactors (e.g. camera parameter, illumination and weather)\ncan further perturb the distribution of the captured data for\ntraining. Second, it is unknown which type of image pro-\ncessing job will be requested until the test image is pre-\nsented. We therefore have to prepare a series of image pro-\ncessing modules at hand. They have distinct aims, but some\nunderlying operations could be shared.\nIt is now common to have pre-training in natural lan-\nguage processing and computer vision [12]. For example,\nthe backbones of object detection models [98, 97] are of-\nten pre-trained on ImageNet classiﬁcation [18]. A num-\narXiv:2012.00364v4  [cs.CV]  8 Nov 2021\nber of well-trained networks can now be easily obtained\nfrom the Internet, including AlexNet [43], VGGNet [63]\nand ResNet [34]. The seminal work Transformers [70]\nhave been widely used in many natural language process-\ning (NLP) tasks, such as translation [73] and question-\nanswering [66]. The secret of its success is to pre-train\ntransformer-based models on a large text corpus and ﬁne-\ntune them on the task-speciﬁc dataset. Variants of Trans-\nformers, like BERT [19] and GPT-3 [5], further enriched\nthe training data and improved the pre-training skills. There\nhave been interesting attempts on extending the success of\nTransformers to the computer vision ﬁeld. For example,\nWang et al. [71] and Fu et al. [25] applied the self-attention\nbased models to capture global information on images. Car-\nion et al. [7] proposed DERT to use transformer architec-\ntures for an end-to-end object detection. Most recently,\nDosovitskiy et al. [22] introduced Vision Transformer (ViT)\nto treat input images as16×16 words and attained excellent\nresults on image recognition.\nThe aforementioned pre-training in computer vision and\nnatural language mostly investigate a pretest classiﬁcation\ntask, but both the input and the output in an image pro-\ncessing task are images. A straightforward application of\nthese existing pre-training strategies might not be feasible.\nFurther, how to effectively address different target image\nprocessing tasks in the pre-training stage remains a hard\nchallenge. It is also instructive to note that the pre-training\nof image processing models enjoys a convenience of self-\ngenerating training instances based on the original real im-\nages. The synthetically manipulated images are taken for\ntraining, while the original image itself is the ground-truth\nto be reconstructed.\nIn this paper, we develop a pre-trained model for im-\nage processing using the transformer architecture, namely,\nImage Processing Transformer (IPT). As the pre-trained\nmodel needs to be compatible with different image process-\ning tasks, including super-resolution, denoising, and derain-\ning, the entire network is composed of multiple pairs of\nhead and tail corresponding to different tasks and a sin-\ngle shared body. Since the potential of transformer needs\nto be excavated using large-scale dataset, we should pre-\npair a great number of images with considerable diversity\nfor training the IPT model. To this end, we select the Im-\nageNet benchmark which contains various high-resolution\nwith 1,000 categories. For each image in the ImageNet,\nwe generate multiple corrupted counterparts using several\ncarefully designed operations to serve different tasks. For\nexample, training samples for the super-resolution task are\ngenerated by downsampling original images. The entired\ndataset we used for training IPT contains about over 10 mil-\nlions of images.\nThen, the transformer architecture is trained on the huge\ndataset as follows. The training images are input to the\nspeciﬁc head, and the generated features are cropped into\npatches ( i.e., “words”) and ﬂattened to sequences subse-\nquently. The transformer body is employed to process the\nﬂattened features in which position and task embedding are\nutilized for encoder and decoder, respectively. In addition,\ntails are forced to predict the original images with differ-\nent output sizes according to the speciﬁc task. Moreover,\na contrastive loss on the relationship between patches of\ndifferent inputs is introduced for well adopting to differ-\nent image processing tasks. The proposed image processing\ntransformer is learned in an end-to-end manner. Experimen-\ntal results conducted on several benchmarks show that the\npre-trained IPT model can surpass most of existing meth-\nods on their own tasks by a signiﬁcant enhancement after\nﬁne-tuning.\n2. Related Works\n2.1. Image Processing\nImage processing consists of the manipulation of im-\nages, including super-resolution, denoising, dehazing, de-\nraining, debluring, etc. There are a variety of deep-learning-\nbased methods proposed to conduct on one or many kinds of\nimage processing tasks. For the super-resolution, Dong et\nal. propose SRCNN [20, 21] which are considered as pio-\nneering works introducing end-to-end models that recon-\nstructs HR images from their LR counterparts. Kim et\nal. [41] further explore the capacity of deep neural network\nwith a more deeper convolutional network. Ahn et al. [2]\nand Lim et al. [50] propose introduce residual block into\nSR task. Zhang et al. [92] and Anwar and Barnes [3] utilize\nthe power of attention to enhance the performance on SR\ntask. A various excellent works are also proposed for the\nother tasks, such as denoising [68, 32, 37, 45, 24], dehaz-\ning [6, 46, 85, 80], deraining [36, 78, 62, 29, 74, 47], and\ndebluring [67, 53, 23, 10]. Different from above methods,\nwe dig the capacity of both big models and huge volume\nof data. Then a pre-training model handling several image\nprocessing tasks is introduced.\n2.2. Transformer\nTransformer [70] and its variants have proven its suc-\ncess being powerful unsupervised or self-supervised pre-\ntraining frameworks in various natural language processing\ntasks. For example, GPTs [59, 60, 5] are pre-trained in a\nautoregressive way that predicting next word in huge text\ndatasets. BERT [19] learns from data without explicit su-\npervision and predicts a masking word based on context.\nColin et al . [61] proposes a universal pre-training frame-\nwork for several downstream tasks. Yinhan et al. [52] pro-\nposes a robust variant for original BERT.\nDue to the success of Transformer-based models in the\nNLP ﬁeld, there are many attempts to explore the beneﬁts\nReshape\nTransformer Encoder\nMulti-head Multi-tail\nFeatures\nFeatures\nFlatten features\nTask embedding\n…\nDenoising \nHead\nDeraining \nHead\nx2 Up \nHead\nx4 Up \nHead\n…\nx4 Up \nTail\nDenoising\nTail\nDeraining\nTail\nx2 Up \nTail…\n…\nTransformer Decoder\nFigure 2. The diagram of the proposed image processing transformer (IPT). The IPT model consists of multi-head and multi-tail for\ndifferent tasks and a shared transformer body including encoder and decoder. The input images are ﬁrst converted to visual features and\nthen divided into patches as visual words for subsequent processing. The resulting images with high visual quality are reconstructed by\nensembling output patches.\nof Transformer in computer vision tasks. These attempts\ncan be roughly divided into two types. The ﬁrst is to intro-\nduce self-attention into the traditional convolutional neural\nnetwork. Yuan et al. [82] introduce spatial attention for im-\nage segmentation. Fu et al. [26] proposes DANET utiliz-\ning the context information by combining spatial and chan-\nnel attention. Wang et al. [75], Chen et al. [15], Jiang et\nal. [38] and Zhang et al. [91] also augment features by self-\nattention to enhance model performance on several high-\nlevel vision tasks. The other type is to replace convolu-\ntional neural network with self-attention block. For in-\nstance, Kolesnikov et al . [42] and Dosovitskiy [22] con-\nduct image classiﬁcation with transformer block. Carion et\nal. [7] and Zhu et al . [100] implement transformer-based\nmodels in detection. Chen et al. [11] proposes a pre-trained\nGPT model for generative and classiﬁcation tasks. Wu et\nal. [77] and Zhao et al. [96] propose pre-training methods\nfor teansformer-based models for image recognition task.\nJiang et al. [39] propose the TransGAN to generate images\nusing Transformer. However, few related works focus on\nlow-level vision tasks. In this paper, we explore a universal\npre-training approach for image processing tasks.\n3. Image Processing Transformer\nTo excavate the potential use of transformer on im-\nage processing tasks for achieving better results, here we\npresent the image processing transformer by pre-training on\nlarge-scale dataset.\n3.1. IPT architecture\nThe overall architecture of our IPT consists of four com-\nponents: heads for extracting features from the input cor-\nrupted images ( e.g., images with noise and low-resolution\nimages), an encoder-decoder transformer is established for\nrecovering the missing information in input data, and tails\nare used formapping the features into restored images. Here\nwe brieﬂy introduce our architecture, details can be found\nin the supplementary material.\nHeads. To adjust different image processing task, we use\na multi-head architecture to deal with each task separately,\nwhere each head consists of three convolutional layers. De-\nnote the input image as x ∈R3×H×W (3 means R, G, and\nB), the head generates a feature map fH ∈RC×H×W with\nCchannels and same height and width (typical we useC =\n64). The calculation can be formulated as fH = Hi(x),\nwhere Hi (i = {1,...,N t}) denote the head for the ith\ntask and Nt denotes the number of tasks.\nTransformer encoder. Before input features into the\ntransformer body, we split the given features into patches\nand each patch is regarded as a ”word”. Speciﬁcally, the\nfeatures fH ∈ RC×H×W are reshaped into a sequence\nof patches, i.e., fpi ∈ RP2×C,i = {1,...,N }, where\nN = HW\nP2 is the number of patches ( i.e., the length of se-\nquence) and P is patch size. To maintain the position in-\nformation of each patch, we add learnable position encod-\nings Epi ∈RP2×C for each patch of feature fpi follow-\ning [22, 7], and Epi + fpi will be directly input into the\ntransformer encoder. The architecture of encoder layer is\nfollowing the original structure in [70], which has a multi-\nhead self-attention module and a feed forward network. The\noutput of encoder fEi ∈ RP2×C for each patch has the\nsame size to that of the input patch fpi. The calculation can\nbe formulated as\ny0 = [Ep1 + fp1 ,Ep2 + fp2 ,...,E pN + fpN ] ,\nqi = ki = vi = LN(yi−1),\ny′\ni = MSA(qi,ki,vi) +yi−1,\nyi = FFN(LN(y′\ni)) +y′\ni, i = 1,...,l\n[fE1 ,fE2 ,...,f EN ] =yl,\n(1)\nwhere ldenotes the number of layers in the encoder, MSA\ndenotes the multi-head self-attention module in the conven-\ntional transformer model [70], LN denotes the layer nor-\nmalization [4] and FFN denotes the feed forward network,\nwhich contains two fully connected layers.\nTransformer decoder. The decoder also follows the\nsame architecture and takes the output of decoder as input\nin the transformer body, which consists of two multi-head\nself-attention (MSA) layers and one feed forward network\n(FFN). The difference to that of the original transformer\nhere is that we utilize a task-speciﬁc embedding as an addi-\ntional input of the decoder. These task-speciﬁc embeddings\nEi\nt ∈RP2×C,i = {1,...,N t}are learned to decode fea-\ntures for different tasks. The calculation of decoder can be\nformulated as:\nz0 = [fE1 ,fE2 ,...,f EN ] ,\nqi = ki = LN(zi−1) +Et,vi = LN(zi−1),\nz′\ni = MSA(qi,ki,vi) +zi−1,\nq′\ni = LN(z′\ni) +Et,k′\ni = v′\ni = LN(z0),\nz′′\ni = MSA(q′\ni,k′\ni,v′\ni) +z′\ni,\nzi = FFN(LN(z′′\ni )) +z′′\ni , i = 1,...,l\n[fD1 ,fD2 ,...,f DN ] =yl,\n(2)\nwhere fDi ∈RP2×C denotes the outputs of decoder. The\ndecoded N patched features with size P2 ×C are then re-\nshaped into the features fD with size C×H×W.\nTails. The properties of tails are same as those of heads,\nwe use multi tails to deal with different tasks. The cal-\nculation can be formulated as fT = Ti(fD), where Ti\n(i = {1,...,N t}) denote the head for the ith task and Nt\ndenotes the number of tasks. The output fT is the resulted\nimages size of 3 ×H′×W′ which is determined by the\nspeciﬁc task. For example, H′ = 2H,W = 2W for a 2×\nsuper-resolution task.\n3.2. Pre-training on ImageNet\nBesides the architecture of transformer itself, one of\nthe key factors for successfully training an excellent trans-\nformer is that the well use of large-scale datasets. Compared\nwith image classiﬁcation, the number of available data used\nfor image processing task is relatively small (e.g., only 2000\nimages on DIV2K dataset for the image super-resolution\ntask), we propose to utilize the well-known ImageNet as\nthe baseline dataset for pre-training our IPT model, then\nwe generate the entire dataset for several tasks (e.g., super-\nresolution and denosing) as follows.\nAs the images in the ImageNet benchmark are of high\ndiversity, which contains over 1 million of natural images\nfrom 1,000 different categories. These images have abun-\ndant texture and color information. We ﬁrst remove the\nsemantic label and manually synthesize a variety of cor-\nrupted images from these unlabeled images with a variety\nof degradation models for different tasks. Note that synthe-\nsized dataset is also usually used in these image processing\ntasks and we use the same degeneration methods as sug-\ngested in [31, 1]. For example, super-resolution tasks often\ntake bicubic degradation to generate low-resolution images,\ndenoising tasks add Gaussian noise in clean images with\ndifferent noise level to generate the noisy images. These\nsynthesized images can signiﬁcantly improve the perfor-\nmance of learned deep networks including both CNN and\ntransformer architectures, which will be shown in the exper-\niment part. Basically, the corrupted images are synthesized\nas:\nIcorrupted = f(Iclean), (3)\nwhere f denotes the degradation transformation, which is\ndepended on the speciﬁc task: for the super-resolution task,\nfsr is exactly the bicubic interpolation; for image denois-\ning, fnoise(I) = I + η, where η is the additive Gaussian\nnoise; for deraining, frain(I) =I+rin which ris a hand-\ncrafted rain streak. The loss function for learning our IPT\nin the supervised fashion can be formulated as:\nLsupervised =\nNt∑\ni=1\nL1(IPT(Ii\ncorrupted),Iclean), (4)\nwhere L1 denote the conventional L1 loss for reconstructing\ndesired images and Ii\ncorrupted denote the corrupted image\nfor task i, respectively. In addition, Eq. 4 implies that the\nproposed framework is trained with multiple image process\ntasks simultaneously. Speciﬁcally, for each batch, we ran-\ndomly select one task from Nt supervised tasks for train-\ning and each task will be processed using the correspond-\ning head, tail and task embedding, simultaneously. After\nthe pre-training the IPT model, it will capture the intrin-\nsic features and transformations for a large variety of image\nprocessing tasks thus can be further ﬁne-tuned to apply on\nthe desired task using the new provided dataset. Moreover,\nother heads and tails will be dropped for saving the compu-\ntation costs and parameters in the remained head, tail and\nbody will be updated according to the back-propagation.\nHowever, due to the variety of degradation models, we\ncannot synthesize images for all image processing tasks.\nFor example, there is a wide range of possible noise lev-\nels in practice. Therefore, the generalization ability of\nthe resulting IPT should be further enhanced. Similar to\nthe pre-training natural language processing models, the\nrelationship between patches of images is also informa-\ntive. The patch in image scenario can be considered as a\nword in natural language processing. For example, patches\ncropped from the same feature map are more likely to ap-\npear together, which should be embedded into similar posi-\ntions. Therefore, we introduce contrastive learning [13, 33]\nfor learning universal features so that the pre-trained IPT\nmodel can be utilized to unseen tasks. In practice, denote\nthe output patched features generated by IPT decoder for\nthe given input xj as fj\nDi ∈ RP2×C,i = {1,...,N },\nwhere xj is selected from a batch of training images X =\n{x1,x2,...,x B}. We aims to minimize the distance be-\ntween patched features from the same images while max-\nimize the distance between patches from different images.\nThe loss function for contrastive learning is formulated as:\nl(fj\nDi1\n,fj\nDi2\n) =−log\nexp(d(fj\nDi1\n,fj\nDi2\n))\n∑B\nk=1 Ik̸=jexp(d(fj\nDi1\n,fk\nDi2\n))\n,\nLconstrastive = 1\nBN2\nN∑\ni1=1\nN∑\ni2=1\nB∑\nj=1\nl(fj\nDi1\n,fj\nDi2\n),\n(5)\nwhere d(a,b) = aT b\n∥a∥∥b∥ denotes the cosine similarity.\nMoreover, to make fully usage of both supervised and self-\nsupervised information, we reformulate the loss function as:\nLIPT = λ·Lcontrastive + Lsupervised. (6)\nWherein, we combine the λ-balanced contrastive loss with\nthe supervised loss as the ﬁnal objective function of IPT.\nThus, the proposed transformer network trained using Eq. 6\ncan be effectively exploited on various existing image pro-\ncessing tasks.\n4. Experiments\nIn this section, we evaluate the performance of the pro-\nposed IPT on various image processing tasks including\nsuper-resolution and image denoising. We show that the\npre-trained IPT model can achieve state-of-the-art perfor-\nmance on these tasks. Moreover, extensive experiments for\nablation study show that the transformer-based models per-\nform better than convolutional neural networks when us-\ning the large-scale dataset for solving the image processing\nproblem.\nDatasets. To obtain better pre-trained results of the IPT\nmodel, we use the well-known ImageNet dataset, which\nconsists of over 1M color images of high diversity. The\ntraining images are cropped into 48 ×48 patches with 3\nchannels for training, i.e., there are over 10M patches for\ntraining the IPT model. We then generate the corrupted im-\nages with 6 types of degradation: 2×,3×,4×bicubic inter-\npolation, 30,50 noise level Gaussian noise and adding rain-\nstreaks, respectively. For the rain-streak generation, we fol-\nlow the method described in [79]. During the test, we crop\nthe images in the test set into 48 ×48 patches with a 10\npixels overlap. Note that the same testing strategy is also\nadopted for CNN based models for a fair comparison, and\nthe resulting PSNR values of CNN models are the same as\nthat of their baselines.\nTraining & Fine-tuning. We use 32 Nvidia NVIDIA\nTesla V100 cards to train our IPT model using the conven-\ntional Adam optimizer with β1 = 0.9,β2 = 0.999 for 300\nepochs on the modiﬁed ImageNet dataset. The initial learn-\ning rate is set as 5e−5 and decayed to 2e−5 in 200 epoch\nwith 256 batch size. Since the training set consists of dif-\nferent tasks, we cannot input all of them in a single batch\ndue to the expensive memory cost. Therefore, we stack a\nbatch of images from a randomly selected task in each iter-\nation. After pre-training on the entire synthesized dataset,\nwe ﬁne-tune the IPT model on the desired task ( e.g., ×3\nsingle image super-resolution) for 30 epochs with a learn-\ning rate of 2e−5. Note that SRCNN [20] also found that\nusing ImageNet training can bring up the performance of\nthe super-resolution task, while we propose a model ﬁtting\ngeneral low-level vision tasks.\n4.1. Super-resolution\nWe compare our model with several state-of-the-art\nCNN-based SR methods. As shown in Table 1, our pre-\ntrained IPT outperforms all the other methods and achieves\nthe best performance in ×2,×3,×4 scale on all datasets.\nIt is worth to highlight that our model achieves 33.76dB\nPSNR on the ×2 scale Urban100 dataset, which surpasses\nother methods with more than ∼0.4dB, while previous\nSOTA methods can only achieve a <0.2dB improvement\ncompared with others, which indicates the superiority of the\nproposed model by utilizing large scale pre-training.\nWe further present the visualization results on our model\nin 4×scale on Urban100 dataset. As shown in Figure 3,\nit is difﬁcult for recover the original high resolution images\nsince lots of information are lost due to the high scaling\nfactor. Previous methods generated blurry images, while the\nsuper-resolution images produced by our model can well\nrecover the details from the low-resolution images.\n4.2. Denoising\nSince our pre-trained model can be well adapt to many\ntasks, we then evaluate the performance of our model on\nimage denoising task. The training and testing data is gen-\nerated by adding Gaussian noise with σ = 30,50 to the\nclean images.\nTo verify the effectiveness of the proposed method,\nUrban100 (×4): img004\nHR VDSR [41] EDSR [51]\nRDN [94] OISR [35] SAN [17]\nRNAN [93] IGNN [99] IPT (ours)\nUrban100 (4×):img012\nHR Bicubic VDSR [41] EDSR [51] RDN [94]\nOISR [35] SAN [17] RNAN [93] IGNN [99] IPT (ours)\nUrban100 (4×): img044\nHR Bicubic VDSR [41] EDSR [51] RDN [94]\nOISR [35] SAN [17] RNAN [93] IGNN [99] IPT (ours)\nFigure 3. Visual results with bicubic downsampling (×4) from Urban100. The proposed method recovers more details. Compared images\nare derived from [99].\nBSD68: 163085\nGT Noisy ( σ=50) CBM3D [16] TNRD [14] RDN [94]\nDnCNN [87] MemNet [65] IRCNN [88] FFDNet [89] IPT (ours)\nFigure 4. Color image denoising results with noise level σ = 50. Compared images are derived from [90].\nwe compare our results with various state-of-the-art mod-\nels. Table 2 reported the color image denoising results\non BSD68 and Urban100 dataset. As a result, our IPT\nachieves the best results among all denoising methods on\ndifferent Gaussian noise level. Moreover, we surprisingly\nfound that our model improve the state-of-the-art perfor-\nmance by ∼0.3dB on the Urban100 dataset, which demon-\nstrate the effectiveness of pre-training and the superiority of\nour transformer-based model.\nFigure 4 shows the visualization of the resulted images.\nAs shown in the ﬁgure, noisy images are hard to be recog-\nnized and it is difﬁcult to recover the clean images. There-\nfore, existing methods fail to reconstruct enough details and\ngenerate abnormal pixels. As a result, our pre-trained model\ncan well recover several details in the hair of this cat and our\nvisual quality beats all the previous models obviously.\n4.3. Deraining\nFor the image deraining task, we evaluate our model on\nthe synthesized Rain100L dataset [79], which consists of\n100 rainy images. Quantitative results can be viewed in\nTable 3. Compared with the state-of-the-art methods, we\nachieve the best performance (41.62dB) with an 1.62dB im-\nprovement.\nFigure 5 shows the visualization results. Previous meth-\nods are failed to reconstruct the original clean images since\nthey lack of image prior. As a result, our IPT model can\npresent exactly the same image as the ground-truth and sur-\nInput / Groundtruth\n27.37 / 0.8154\nDSC\n29.34 / 0.8479\nGMM\n32.38 / 0.9306\nJCAS\n31.45 / 0.9151\nClear\n31.59 / 0.9380\nRESCAN\n41.26 / 0.9887\nPReNet\n37.27 / 0.9793\nSPANet\n35.67 / 0.9700\nJORDER_E\n41.11 / 0.9894\nSIRR\n36.99 / 0.9692\nRCDNet \n42.15 / 0.9912\nIPT (ours)\n43.91 / 0.9922\nFigure 5. Image deraining results on the Rain100L dataset. Compared images are derived from [72].\nTable 1. Quantitative results on image super-resolution. Best and\nsecond best results are highlighted and underlined.\nMethod Scale Set5 Set14 B100 Urban100\nVDSR [41] ×2 37.53 33.05 31.90 30.77\nEDSR [51] ×2 38.11 33.92 32.32 32.93\nRCAN [92] ×2 38.27 34.12 32.41 33.34\nRDN [94] ×2 38.24 34.01 32.34 32.89\nOISR-RK3 [35] ×2 38.21 33.94 32.36 33.03\nRNAN [93] ×2 38.17 33.87 32.32 32.73\nSAN [17] ×2 38.31 34.07 32.42 33.10\nHAN [55] ×2 38.27 34.16 32.41 33.35\nIGNN [99] ×2 38.24 34.07 32.41 33.23\nIPT (ours) ×2 38.37 34.43 32.48 33.76\nVDSR [41] ×3 33.67 29.78 28.83 27.14\nEDSR [51] ×3 34.65 30.52 29.25 28.80\nRCAN [92] ×3 34.74 30.65 29.32 29.09\nRDN [94] ×3 34.71 30.57 29.26 28.80\nOISR-RK3 [35] ×3 34.72 30.57 29.29 28.95\nRNAN [93] ×3 34.66 30.52 29.26 28.75\nSAN [17] ×3 34.75 30.59 29.33 28.93\nHAN [55] ×3 34.75 30.67 29.32 29.10\nIGNN [99] ×3 34.72 30.66 29.31 29.03\nIPT (ours) ×3 34.81 30.85 29.38 29.49\nVDSR [41] ×4 31.35 28.02 27.29 25.18\nEDSR [51] ×4 32.46 28.80 27.71 26.64\nRCAN [92] ×4 32.63 28.87 27.77 26.82\nSAN [17] ×4 32.64 28.92 27.78 26.79\nRDN [94] ×4 32.47 28.81 27.72 26.61\nOISR-RK3 [35] ×4 32.53 28.86 27.75 26.79\nRNAN [93] ×4 32.49 28.83 27.72 26.61\nHAN [55] ×4 32.64 28.90 27.80 26.85\nIGNN [99] ×4 32.57 28.85 27.77 26.84\nIPT (ours) ×4 32.64 29.01 27.82 27.26\npasses all the previous algorithms in visual quality. This\nresult substantiates the generality of the proposed model.\nTable 2. Quantitative results on color image denoising. Best and\nsecond best results are highlighted and underlined.\nMethod BSD68 Urban100\n30 50 30 50\nCBM3D [16] 29.73 27.38 30.36 27.94\nTNRD [14] 27.64 25.96 27.40 25.52\nDnCNN [87] 30.40 28.01 30.28 28.16\nMemNet [65] 28.39 26.33 28.93 26.53\nIRCNN [88] 30.22 27.86 30.28 27.69\nFFDNet [89] 30.31 27.96 30.53 28.05\nSADNet [9] 30.64 28.32 N/A N/A\nRDN [95] 30.67 28.31 31.69 29.29\nIPT (ours) 30.75 28.39 32.00 29.71\n4.4. Generalization Ability\nAlthough we can generate various corrupted images, nat-\nural images are of high complexity and we cannot syn-\nthesize all possible images for pre-training the transformer\nmodel. However, a good pre-trained model should have the\ncapacity for well adapting other tasks as those in the ﬁeld of\nNLP. To this end, we then conduct several experiments to\nverify the generalization ability of our model. In practice,\nwe test corrupted images that did not include in our syn-\nthesized ImageNet dataset, i.e., image denoising with noisy\nlevel 10 and 70, respectively. We use the heads and tails for\nimage denoising tasks as the pre-trained model.\nThe detailed results are shown in Table 4, we compare\nthe performance of using the pre-trained IPT model and the\nstate-of-the-art methods for image denoising. Obviously,\nIPT model outperforms other conventional methods, which\nTable 3. Quantitative results of image deraining on the Rain100L dataset. Best and second best results are highlighted and underlined.\nMethod Input DSC [28] GMM [49] JCAS [31] Clear [27] DDN [28]\nPSNR 26.90 27.34 29.05 28.54 30.24 32.38\nSSIM 0.8384 0.8494 0.8717 0.8524 0.9344 0.9258\nRESCAN [48] PReNet [62] JORDER E [79] SPANet [74] SSIR [76] RCDNet [72] IPT (ours)\n38.52 37.45 38.59 35.33 32.37 40.00 41.62\n0.9812 0.9790 0.9834 0.9694 0.9258 0.9860 0.9880\nTable 4. Generation ability of our IPT model on color image de-\nnoising with different noise levels. Best and second best results\nare highlighted and underlined.\nMethod BSD68 Urban100\n10 70 10 70\nCBM3D [16] 35.91 26.00 36.00 26.31\nTNRD [14] 33.36 23.83 33.60 22.63\nDnCNN [87] 36.31 26.56 36.21 26.17\nMemNet [65] N/A 25.08 N/A 24.96\nIRCNN [88] 36.06 N/A 35.81 N/A\nFFDNet [89] 36.14 26.53 35.77 26.39\nRDN [95] 36.47 26.85 36.69 27.63\nIPT (ours) 36.53 26.92 36.99 27.90\n0.0 0.2 0.4 0.6 0.8 1.0\nPercentage of Usage of ImageNet (1.1M Images)\n32.8\n33.0\n33.2\n33.4\n33.6\n33.8PSNR (dB)\nIPT\nEDSR\nIGNN\nRDN\nFigure 6. The performance of CNN and IPT models using different\npercentages of data.\ndemonstrates that the pre-trained model can capture more\nuseful information and features from the large-scale dataset.\n4.5. Ablation Study\nImpact of data percentage.To evaluate the effective-\nness of the transformer architecture, we conduct experi-\nments to analyse the improvement of pre-training on CNN-\nbased model and transformer-based model. We use 20%,\n40%, 60%, 80% and 100% percentages of the synthesized\nImageNet dataset to analyse the impact on the number of\nused data for resulting performance. Figure 6 shows the\nresults of different pre-trained models. When the models\nare not pre-trained or pre-trained with small amount ( <\n60%) of the entire dataset, the CNN models achieve bet-\nter performance. In contrast, when using large-scale data,\nthe transformer-based models overwhelming CNN models,\nwhich demonstrates that the effectiveness of our IPT model\nfor pre-training.\nTable 5. Impact of λ for contrastive learning.\nλ 0 0.05 0.1 0.2 0.5\nPSNR 38.27 38.32 38.37 38.33 38.26\nImpact of contrastive learning.As discussed above, to\nimprove the representation ability of our pre-trained model,\nwe embed the contrastive learning loss (Eq. 6) into the train-\ning procedure. We then evaluate its effectiveness on the×2\nscale super-resolution task using the Set4 dataset. Table 5\nshows the impact of the hyper-parameter λ for balancing\nthe two terms in Eq. 6. When λ=0, the IPT model is trained\nusing only a supervised learning approach, the resulting\nPSNR value is 38.27dB. When employing the contrastive\nloss for self-supervised learning, the model can achieve a\n38.37dB PSNR value (λ= 0.1), which is about 0.1dB higher\nthan that of the model trained withλ= 0. These results fur-\nther demonstrate the effectiveness of the contrastive learn-\ning for learning better pre-trained IPT model.\n5. Conclusions and Discussions\nThis paper aims to address the image processing prob-\nlems using a pre-trained transformer model (IPT). The IPT\nmodel is designed with multi-heads,multi-tails a shared\ntransformer body for serving different image processing\ntask such as image super-resolution and denoising. To max-\nimally excavate the performance of the transformer archi-\ntecture on various tasks, we explore a synthesized ImageNet\ndatesets. Wherein, each original image will be degraded to\na series of counterparts as paired training data. The IPT\nmodel is then trained using supervised and self-supervised\napproaches which shows strong ability for capturing intrin-\nsic features for low-level image processing. Experimental\nresults demonstrate that our IPT can outperform the state-\nof-the-art methods using only one pre-trained model after a\nquickly ﬁne-tuning. In the future work, we will extend our\nIPT model to more tasks such as inpainting, dehazing, etc.\nAcknowledgment This work is supported by National\nNatural Science Foundation of China under Grant No.\n61876007, and Australian Research Council under Project\nDE180101438 and DP210101859.\nA. Results on Deblurring\nWe further evaluate the performance of our model on im-\nage deblurring task. We use the GoPro dataset [54] to ﬁne-\ntune and test our model. We modify the patch size as 256,\npatch dim as 8 and number of features as 9 to achieve a\nhigher receptive ﬁeld. Table 6 reported deblurring results,\nwhere + denotes applying self-ensemble technique. As a re-\nsult, our IPT achieves the best results among all deblurring\nmethods. Figure 8 shows the visualization of the resulted\nimages. As shown in the ﬁgure, our pre-trained model can\nwell achieve the best visual quality among all the previous\nmodels obviously.\nB. Architecture of IPT\nIn the main paper, we propose the image processing\ntransformer (IPT). Here we show the detailed architecture\nof IPT, which consists of heads, body and tails. Each head\nhas one convolutional layer (with 3 ×3 kernel size, 3 in-\nput channels and 64 output channels) and two ResBlock.\nEach ResBlock consists of two convolutional layers (with\n5×5 kernel size, 64 input channels and 64 output channels)\nwhich involved by a single shortcut. The body has 12 en-\ncoder layers and 12 decoder layers. The tail of denoising or\nderaining is a convolutional layer with 3 ×3 kernel size, 64\ninput channels and 3 output channels. For super-resolution,\nthe tail consists of one pixelshufﬂe layer with upsampling\nscale 2 and 3 for×2 and ×3 SR, two pixelshufﬂe layer with\nupsampling scale 2 for ×4 SR.\nThe whole IPT has 114M parameters and 33G FLOPs,\nwhich have more parameters while fewer FLOPs compared\nwith traditional CNN models (e.g., EDSR has 43M param-\neters and 99G FLOPs).\nC. Impact of Multi-task Training\nWe train IPT following a multi-task manner and then\nﬁne-tune it on 6 different tasks including×2,×3,×4 super-\nresolution, denoising with noise level 30,50 and deraining.\nWe ﬁnd that this training strategy would not harm the per-\nformance on these tasks which have been pre-trained on\nlarge scale dataset (ImageNet). In other words, the per-\nformance of multi-task training and single-task training re-\nmains almost the same. However, when transferring to other\ntasks (e.g., Section 4.4 in the main paper), the pre-trained\nmodel using multi-task training is better than that of single-\ntask training for about 0.3dB, which suggests the multi-task\ntraining would learn universal representation of image pro-\ncessing tasks.\nD. Visualization of Embeddings\nWe visualize the learned embeddings of IPT. Figure 7\nshows the visualization results of position embeddings. We\nFigure 7. Visualization of cosine similarity of position embed-\ndings.\nﬁnd that patches with similar columns or rows have similar\nembeddings, which indicate that they learn useful informa-\ntion for discovering the position on image processing. We\nalso test to use ﬁxed embeddings or do not use embeddings,\nwhose performance are lower than that of using learnable\nposition embeddings (vary from 0.2dB to 0.3dB for differ-\nent tasks).\nMoreover, we visualize the task embeddings in ﬁgure 9.\nWe can ﬁnd that for ×2 super-resolution task, the simi-\nlarity between the embeddings on each position and their\nneighbours are higher than ×3 super-resolution, while that\nof ×4 super-resolution is the smallest. This results indi-\ncates that each patches in ×2 super-resolution can focus\non other patches with farther distance than ×3 and ×4,\nsince their downsampling scale are smaller and the rela-\ntionship between different patches are closer. The similar-\nity of task embedding for deraining in ﬁgure 9 (d) shows\nthat the patches pay more attention on the vertical direc-\ntion than horizontal direction, which is reasonable as the\nrain is dropped vertically. The similarity of task embedding\nfor denoising is similar with Gaussian noise, and ﬁgure 9\n(f) with higher (50) noise level shows higher similarity be-\ntween neighbours than ﬁgure 9 (e) with 30 noise level. The\nvisualization results suggests that our task embeddings can\nindeed learn some information for different tasks. We also\ntest to not use task embeddings, which results in signiﬁ-\ncant accuracy drop (vary from 0.1dB to 0.5dB for different\ntasks).\nInputRADNBANet\nIPT(Ours)\nFigure 8. Image deblurring results on the GoPro dataset. Compared images are derived from [69].\nTable 6. Quantitative results on image deblurring. Best and second best results are highlighted and underlined.\nMethod MSCNN [54] SRN [67] DSD [30] DeblurGANv2 [44] DMPHN [84] LEBMD [40] EDSD [81]\nPSNR 30.40 30.25 30.96 29.55 31.36 31.79 29.81\nDBGAN [86] MTRNN [57] RADN [58] SAPHN [64] BANET [69] MB2D [56] IPT (Ours) IPT+ (Ours)\n31.10 31.13 31.85 32.02 32.44 32.16 32.58 32.91\nReferences\n[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition Workshops, pages 126–135, 2017.\n4\n[2] Namhyuk Ahn, Byungkon Kang, and Kyung-Ah Sohn.\nFast, accurate, and lightweight super-resolution with cas-\ncading residual network. In Proceedings of the European\nConference on Computer Vision (ECCV) , pages 252–268,\n2018. 2\n[3] Saeed Anwar and Nick Barnes. Densely residual laplacian\nsuper-resolution. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2020. 2\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450 ,\n2016. 4\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 2\n[6] Bolun Cai, Xiangmin Xu, Kui Jia, Chunmei Qing, and\nDacheng Tao. Dehazenet: An end-to-end system for single\nimage haze removal. IEEE Transactions on Image Process-\ning, 25(11):5187–5198, 2016. 2\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nico-\nlas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. arXiv\npreprint arXiv:2005.12872, 2020. 2, 3\n[8] Gabriella Castellano, Leonardo Bonilha, LM Li, and Fer-\nnando Cendes. Texture analysis of medical images. Clini-\ncal radiology, 59(12):1061–1069, 2004. 1\n[9] Meng Chang, Qi Li, Huajun Feng, and Zhihai Xu. Spatial-\nadaptive network for single image denoising.arXiv preprint\narXiv:2001.10291, 2020. 7\n[10] Liang Chen, Faming Fang, Shen Lei, Fang Li, and Guixu\nZhang. Enhanced sparse model for blind deblurring. 2020.\n2\n[11] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo\nJun, Prafulla Dhariwal, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In Proceedings of the\n37th International Conference on Machine Learning , vol-\nume 1, 2020. 3\n(a) ×2 super-resolution (b) ×3 super-resolution (c) ×4 super-resolution\n(d) deraining (e) denoising with 30 noise level (f) denoising with 50 noise level\nFigure 9. Visualization of six different task embeddings.\n[12] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,\nYang Zhang, Michael Carbin, and Zhangyang Wang. The\nlottery tickets hypothesis for supervised and self-supervised\npre-training in computer vision models. arXiv preprint\narXiv:2012.06908, 2020. 1\n[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709,\n2020. 5\n[14] Yunjin Chen and Thomas Pock. Trainable nonlinear reac-\ntion diffusion: A ﬂexible framework for fast and effective\nimage restoration. IEEE transactions on pattern analysis\nand machine intelligence, 39(6):1256–1272, 2016. 6, 7, 8\n[15] Yunpeng Chen, Marcus Rohrbach, Zhicheng Yan, Yan\nShuicheng, Jiashi Feng, and Yannis Kalantidis. Graph-\nbased global reasoning networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 433–442, 2019. 3\n[16] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and\nKaren Egiazarian. Color image denoising via sparse 3d col-\nlaborative ﬁltering with grouping constraint in luminance-\nchrominance space. In 2007 IEEE International Confer-\nence on Image Processing , volume 1, pages I–313. IEEE,\n2007. 6, 7, 8\n[17] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\nLei Zhang. Second-order attention network for single im-\nage super-resolution. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , pages\n11065–11074, 2019. 6, 7\n[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248–255. Ieee, 2009. 1\n[19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2\n[20] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Learning a deep convolutional network for image\nsuper-resolution. In European conference on computer vi-\nsion, pages 184–199. Springer, 2014. 2, 5\n[21] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Image super-resolution using deep convolutional net-\nworks. IEEE transactions on pattern analysis and machine\nintelligence, 38(2):295–307, 2015. 2\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 3\n[23] Thomas Eboli, Jian Sun, and Jean Ponce. End-to-end in-\nterpretable learning of non-blind image deblurring. arXiv\npreprint arXiv:2007.01769, 2020. 2\n[24] Yuchen Fan, Honghui Shi, Jiahui Yu, Ding Liu, Wei\nHan, Haichao Yu, Zhangyang Wang, Xinchao Wang, and\nThomas S Huang. Balanced two-stage residual networks\nfor image super-resolution. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition\nWorkshops, pages 161–168, 2017. 2\n[25] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-\nwei Fang, and Hanqing Lu. Dual attention network for\nscene segmentation. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , pages\n3146–3154, 2019. 2\n[26] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhi-\nwei Fang, and Hanqing Lu. Dual attention network for\nscene segmentation. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , pages\n3146–3154, 2019. 3\n[27] Xueyang Fu, Jiabin Huang, Xinghao Ding, Yinghao Liao,\nand John Paisley. Clearing the skies: A deep network archi-\ntecture for single-image rain removal. IEEE Transactions\non Image Processing, 26(6):2944–2956, 2017. 8\n[28] Xueyang Fu, Jiabin Huang, Delu Zeng, Yue Huang, Xing-\nhao Ding, and John Paisley. Removing rain from single im-\nages via a deep detail network. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\npages 3855–3863, 2017. 8\n[29] Xueyang Fu, Borong Liang, Yue Huang, Xinghao Ding,\nand John Paisley. Lightweight pyramid networks for im-\nage deraining. IEEE transactions on neural networks and\nlearning systems, 2019. 2\n[30] Hongyun Gao, Xin Tao, Xiaoyong Shen, and Jiaya Jia.\nDynamic scene deblurring with parameter selective shar-\ning and nested skip connections. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3848–3856, 2019. 10\n[31] Shuhang Gu, Deyu Meng, Wangmeng Zuo, and Lei Zhang.\nJoint convolutional analysis and synthesis sparse represen-\ntation for single image layer separation. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 1708–1716, 2017. 4, 8\n[32] Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei\nZhang. Toward convolutional blind denoising of real pho-\ntographs. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 1712–1722,\n2019. 2\n[33] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729–9738, 2020. 5\n[34] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 2\n[35] Xiangyu He, Zitao Mo, Peisong Wang, Yang Liu,\nMingyuan Yang, and Jian Cheng. Ode-inspired network\ndesign for single image super-resolution. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 1732–1741, 2019. 6, 7\n[36] Xiaowei Hu, Chi-Wing Fu, Lei Zhu, and Pheng-Ann Heng.\nDepth-attentional features for single-image rain removal. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8022–8031, 2019. 2\n[37] Xixi Jia, Sanyang Liu, Xiangchu Feng, and Lei Zhang. Foc-\nnet: A fractional optimal control network for image denois-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 6054–6063, 2019. 2\n[38] Peng-Tao Jiang, Qibin Hou, Yang Cao, Ming-Ming Cheng,\nYunchao Wei, and Hong-Kai Xiong. Integral object mining\nvia online attention accumulation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 2070–2079, 2019. 3\n[39] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo transformers can make one strong gan. arXiv preprint\narXiv:2102.07074, 2021. 3\n[40] Zhe Jiang, Yu Zhang, Dongqing Zou, Jimmy Ren,\nJiancheng Lv, and Yebin Liu. Learning event-based motion\ndeblurring. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3320–\n3329, 2020. 10\n[41] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accu-\nrate image super-resolution using very deep convolutional\nnetworks. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 1646–1654,\n2016. 2, 6, 7\n[42] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6, 2019. 3\n[43] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiﬁcation with deep convolutional neural net-\nworks. Communications of the ACM , 60(6):84–90, 2017.\n2\n[44] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and\nZhangyang Wang. Deblurgan-v2: Deblurring (orders-\nof-magnitude) faster and better. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 8878–8887, 2019. 10\n[45] Stamatios Lefkimmiatis. Non-local color image denoising\nwith convolutional neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 3587–3596, 2017. 2\n[46] Boyi Li, Xiulian Peng, Zhangyang Wang, Jizheng Xu, and\nDan Feng. An all-in-one network for dehazing and beyond.\narXiv preprint arXiv:1707.06543, 2017. 2\n[47] Siyuan Li, Wenqi Ren, Feng Wang, Iago Breno Araujo,\nEric K Tokuda, Roberto Hirata Junior, Roberto M Cesar-\nJr, Zhangyang Wang, and Xiaochun Cao. A comprehen-\nsive benchmark analysis of single image deraining: Current\nchallenges and future perspectives. International Journal\nof Computer Vision, pages 1–22, 2021. 2\n[48] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hong-\nbin Zha. Recurrent squeeze-and-excitation context aggre-\ngation net for single image deraining. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages\n254–269, 2018. 8\n[49] Yu Li, Robby T Tan, Xiaojie Guo, Jiangbo Lu, and\nMichael S Brown. Rain streak removal using layer priors.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2736–2744, 2016. 8\n[50] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah,\nand Kyoung Mu Lee. Enhanced deep residual networks\nfor single image super-resolution. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion workshops, pages 136–144, 2017. 2\n[51] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah,\nand Kyoung Mu Lee. Enhanced deep residual networks\nfor single image super-resolution. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion workshops, pages 136–144, 2017. 6, 7\n[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A ro-\nbustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019. 2\n[53] Boyu Lu, Jun-Cheng Chen, and Rama Chellappa. Unsu-\npervised domain-speciﬁc deblurring via disentangled rep-\nresentations. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , pages 10225–\n10234, 2019. 2\n[54] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nmulti-scale convolutional neural network for dynamic scene\ndeblurring. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3883–3891,\n2017. 9, 10\n[55] Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lian-\nping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao,\nand Haifeng Shen. Single image super-resolution via a\nholistic attention network. In European Conference on\nComputer Vision, pages 191–207. Springer, 2020. 7\n[56] Dongwon Park, Dong Un Kang, and Se Young Chun. Blur\nmore to deblur better: Multi-blur2deblur for efﬁcient video\ndeblurring. arXiv preprint arXiv:2012.12507, 2020. 10\n[57] Dongwon Park, Dong Un Kang, Jisoo Kim, and Se Young\nChun. Multi-temporal recurrent neural networks for pro-\ngressive non-uniform single image deblurring with incre-\nmental temporal training. InEuropean Conference on Com-\nputer Vision, pages 327–343. Springer, 2020. 10\n[58] Kuldeep Purohit and AN Rajagopalan. Region-adaptive\ndense network for efﬁcient motion deblurring. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 34, pages 11882–11889, 2020. 10\n[59] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by genera-\ntive pre-training, 2018. 2\n[60] Alec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog , 1(8):9,\n2019. 2\n[61] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer. Journal of Machine\nLearning Research, 21(140):1–67, 2020. 2\n[62] Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu,\nand Deyu Meng. Progressive image deraining networks: A\nbetter and simpler baseline. In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\npages 3937–3946, 2019. 2, 8\n[63] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2\n[64] Maitreya Suin, Kuldeep Purohit, and AN Rajagopalan.\nSpatially-attentive patch-hierarchical network for adaptive\nmotion deblurring. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3606–3615, 2020. 10\n[65] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu.\nMemnet: A persistent memory network for image restora-\ntion. In Proceedings of the IEEE international conference\non computer vision, pages 4539–4547, 2017. 6, 7, 8\n[66] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019. 2\n[67] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji-\naya Jia. Scale-recurrent network for deep image deblurring.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 8174–8182, 2018. 2, 10\n[68] Chunwei Tian, Yong Xu, Zuoyong Li, Wangmeng Zuo,\nLunke Fei, and Hong Liu. Attention-guided cnn for image\ndenoising. Neural Networks, 124:117–129, 2020. 2\n[69] Fu-Jen Tsai, Yan-Tsung Peng, Yen-Yu Lin, Chung-Chi\nTsai, and Chia-Wen Lin. Banet: Blur-aware attention\nnetworks for dynamic scene deblurring. arXiv preprint\narXiv:2101.07518, 2021. 10\n[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Ad-\nvances in neural information processing systems , pages\n5998–6008, 2017. 2, 4\n[71] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng\nLi, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang.\nResidual attention network for image classiﬁcation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 3156–3164, 2017. 2\n[72] Hong Wang, Qi Xie, Qian Zhao, and Deyu Meng. A model-\ndriven deep neural network for single image rain removal.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3103–3112, 2020. 7,\n8\n[73] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang\nLi, Derek F Wong, and Lidia S Chao. Learning deep\ntransformer models for machine translation. arXiv preprint\narXiv:1906.01787, 2019. 2\n[74] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang\nZhang, and Rynson WH Lau. Spatial attentive single-image\nderaining with a high quality real rain dataset. In Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 12270–12279, 2019. 2, 8\n[75] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794–7803, 2018. 3\n[76] Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu, and Ying\nWu. Semi-supervised transfer learning for image rain re-\nmoval. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pages 3877–3886,\n2019. 8\n[77] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and\nPeter Vajda. Visual transformers: Token-based image rep-\nresentation and processing for computer vision. arXiv\npreprint arXiv:2006.03677, 2020. 3\n[78] Wenhan Yang, Jiaying Liu, Shuai Yang, and Zongming\nGuo. Scale-free single image deraining via visibility-\nenhanced recurrent wavelet learning. IEEE Transactions\non Image Processing, 28(6):2948–2961, 2019. 2\n[79] Wenhan Yang, Robby T Tan, Jiashi Feng, Zongming Guo,\nShuicheng Yan, and Jiaying Liu. Joint rain detection and\nremoval from a single image with contextualized deep net-\nworks. IEEE transactions on pattern analysis and machine\nintelligence, 42(6):1377–1393, 2019. 5, 6, 8\n[80] Xitong Yang, Zheng Xu, and Jiebo Luo. Towards percep-\ntual image dehazing by physics-based disentanglement and\nadversarial training. In AAAI, pages 7485–7492, 2018. 2\n[81] Yuan Yuan, Wei Su, and Dandan Ma. Efﬁcient dynamic\nscene deblurring using spatially variant deconvolution net-\nwork with optical ﬂow guided training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3555–3564, 2020. 10\n[82] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-\nwork for scene parsing. arXiv preprint arXiv:1809.00916,\n2018. 3\n[83] Yongnian Zeng, Wei Huang, Maoguo Liu, Honghui Zhang,\nand Bin Zou. Fusion of satellite images in urban area: As-\nsessing the quality of resulting images. In 2010 18th Inter-\nnational Conference on Geoinformatics, pages 1–4. IEEE,\n2010. 1\n[84] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr\nKoniusz. Deep stacked hierarchical multi-patch network for\nimage deblurring. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n5978–5986, 2019. 10\n[85] He Zhang and Vishal M Patel. Densely connected pyramid\ndehazing network. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 3194–\n3203, 2018. 2\n[86] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn\nStenger, Wei Liu, and Hongdong Li. Deblurring by realis-\ntic blurring. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2737–\n2746, 2020. 10\n[87] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a gaussian denoiser: Residual learning\nof deep cnn for image denoising. IEEE Transactions on\nImage Processing, 26(7):3142–3155, 2017. 6, 7, 8\n[88] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang.\nLearning deep cnn denoiser prior for image restoration. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, pages 3929–3938, 2017. 6, 7, 8\n[89] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet:\nToward a fast and ﬂexible solution for cnn-based im-\nage denoising. IEEE Transactions on Image Processing ,\n27(9):4608–4622, 2018. 6, 7, 8\n[90] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet:\nToward a fast and ﬂexible solution for cnn-based im-\nage denoising. IEEE Transactions on Image Processing ,\n27(9):4608–4622, 2018. 6\n[91] Songyang Zhang, Xuming He, and Shipeng Yan. Latent-\ngnn: Learning efﬁcient non-local relations for visual recog-\nnition. In International Conference on Machine Learning,\npages 7374–7383, 2019. 3\n[92] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very deep\nresidual channel attention networks. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) , pages\n286–301, 2018. 2, 7\n[93] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and\nYun Fu. Residual non-local attention networks for image\nrestoration. arXiv preprint arXiv:1903.10082, 2019. 6, 7\n[94] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong,\nand Yun Fu. Residual dense network for image super-\nresolution. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition , pages 2472–2481,\n2018. 6, 7\n[95] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image restoration.\nIEEE Transactions on Pattern Analysis and Machine Intel-\nligence, 2020. 7, 8\n[96] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-\ning self-attention for image recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10076–10085, 2020. 3\n[97] Jia-Xing Zhao, Yang Cao, Deng-Ping Fan, Ming-Ming\nCheng, Xuan-Yi Li, and Le Zhang. Contrast prior and ﬂuid\npyramid integration for rgbd salient object detection. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3927–3936, 2019. 1\n[98] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao,\nJufeng Yang, and Ming-Ming Cheng. Egnet: Edge guid-\nance network for salient object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pages 8779–8788, 2019. 1\n[99] Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, and\nChen Change Loy. Cross-scale internal graph neural net-\nwork for image super-resolution. Advances in Neural In-\nformation Processing Systems, 33, 2020. 6, 7\n[100] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 3",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7691023945808411
    },
    {
      "name": "Computer science",
      "score": 0.7591828107833862
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5941608548164368
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4773314595222473
    },
    {
      "name": "Deep learning",
      "score": 0.4386158883571625
    },
    {
      "name": "Image processing",
      "score": 0.43643295764923096
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38324427604675293
    },
    {
      "name": "Machine learning",
      "score": 0.3673591613769531
    },
    {
      "name": "Image (mathematics)",
      "score": 0.3058728575706482
    },
    {
      "name": "Engineering",
      "score": 0.1301657259464264
    },
    {
      "name": "Voltage",
      "score": 0.125300794839859
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 120
}