{
  "title": "PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
  "url": "https://openalex.org/W3200285169",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5072325974",
      "name": "Torsten Scholak",
      "affiliations": [
        "University of Toronto"
      ]
    },
    {
      "id": "https://openalex.org/A5024400526",
      "name": "Nathan Schucher",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5010465328",
      "name": "Dzmitry Bahdanau",
      "affiliations": [
        "McGill University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3150263485",
    "https://openalex.org/W2890867094",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2123835026",
    "https://openalex.org/W3173274550",
    "https://openalex.org/W3174726724",
    "https://openalex.org/W2972571786",
    "https://openalex.org/W3035172316",
    "https://openalex.org/W3176561775",
    "https://openalex.org/W2947354947",
    "https://openalex.org/W3170656151",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W3102020135",
    "https://openalex.org/W3034835156",
    "https://openalex.org/W3119822474"
  ],
  "abstract": "Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code and trained models available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9895–9901\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n9895\nPICARD :\nParsing Incrementally for Constrained Auto-Regressive Decoding\nfrom Language Models\nTorsten Scholakand Nathan Schucher and Dzmitry Bahdanau\nElementAI, a ServiceNow company\n{torsten.scholak,dzmitry.bahdanau}@servicenow.com\nAbstract\nLarge pre-trained language models for textual\ndata have an unconstrained output space; at\neach decoding step, they can produce any of\n10,000s of sub-word tokens. When ﬁne-tuned\nto target constrained formal languages like\nSQL, these models often generate invalid code,\nrendering it unusable. We propose P ICARD 1,\na method for constraining auto-regressive de-\ncoders of language models through incremen-\ntal parsing. P ICARD helps to ﬁnd valid output\nsequences by rejecting inadmissible tokens at\neach decoding step. On the challenging Spider\nand CoSQL text-to-SQL translation tasks, we\nshow that P ICARD transforms ﬁne-tuned T5\nmodels with passable performance into state-\nof-the-art solutions.\n1 Introduction\nWhile there have been many successes in applying\nlarge pre-trained language models to downstream\ntasks, our ability to control and constrain the out-\nput of these models is still very limited. Many\nenterprise applications are out of reach because\nthey require a degree of rigour and exactitude that\nlanguage models are not able to deliver yet. If the\ntarget is a formal language like SQL, then we would\nlike the model to adhere exactly and provably to the\nSQL speciﬁcation with all its lexical, grammatical,\nlogical, and semantical constraints. Unfortunately,\nwith pre-training alone, language models may not\nsatisfy these correctness requirements.\nFor text-to-SQL translation, the most widespread\nsolution to constrained decoding is to make invalid\nSQL unrepresentable. For a while now it has been\npossible to restrict auto-regressive decoding to only\nthose token sequences that correctly parse to SQL\nabstract syntax trees (Yin and Neubig, 2018; Lin\net al., 2019; Wang et al., 2020). More recently,\nsemi-auto-regressive improvements to this parsing\n1The PICARD code is available at https://github.\ncom/ElementAI/picard.\n1 2 4 8 16\nbeam size\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nexact match accuracy\nT5-Base\n T5-Large\n T5-3B\nnone\n top-2\n top-4\n top-8\nFigure 1: Exact-set-match accuracy of the highest-\nscoring prediction as a function of beam size on the Spi-\nder text-to-SQL development set. With PICARD turned\non, token predictions had to pass P ICARD checking at\nevery decoding step. Only the top-2, -4, and -8 token\npredictions of each hypothesis were considered in the\nbeam search. With PICARD turned off (none), all token\npredictions were considered and none were checked.\nThe models, T5-Base, -Large, and -3B, did not have\naccess to any database content, only to the database\nschemas.\nparadigm have been proposed (Rubin and Berant,\n2021). However, while effective, these approaches\nhave in common that they are achieved at the ex-\npense of using a custom vocabulary of special con-\ntrol tokens or a custom model architecture, or both.\nUnfortunately, this makes them incompatible with\ngeneric pre-trained language model decoders. A\nless invasive and more compatible approach is to\nnot constrain the generation process, but instead to\nﬁlter ﬁnalized beam hypotheses by validity (Suhr\net al., 2020; Lin et al., 2020). Yet, such ﬁltering is\nat the expense of a very large beam size.\nWe address the expenses of these approaches\nwith a novel incremental parsing method for con-\nstrained decoding called PICARD , which stands\nfor \"Parsing Incrementally for Constrained Auto-\n9896\nFigure 2: Illustration of constrained beam search with\nbeam size 2 and PICARD . Each vertical column repre-\nsents three token predictions for a hypothesis from top\nto bottom in descending order by probability. In this\nexample, P ICARD is conﬁgured to only check the top-\n2 highest ones. The rest is automatically dismissed by\nsetting their score to −∞. Tokens rejected by PICARD\n(red, ×) are also assigned a score of −∞. Accepted\ntokens (green, ) keep their original score.\nRegressive Decoding.\" PICARD is compatible with\nany existing auto-regressive language model de-\ncoder and vocabulary—including, but not limited\nto, those of large pre-trained transformers—and it\ndoes not require very large beam sizes. PICARD is\nentirely absent from pre-training or ﬁne-tuning of\nthe model, and can be easily and optionally enabled\nat inference time. PICARD operates directly on the\noutput of the language model which, in the case\nof text-to-SQL translation, is the readable surface\nform of the SQL code.\nIn our experiments, we ﬁnd that PICARD can\nsigniﬁcantly improve the performance of a large\npre-trained language model (Raffel et al., 2020)\nafter it is ﬁne-tuned on the text-to-SQL task. On\nthe Spider text-to-SQL dataset (Yu et al., 2018), we\nﬁnd that a T5-Base model with PICARD can out-\nperform a T5-Large model without it, and likewise\nfor a T5-Large and a T5-3B model. Signiﬁcantly,\nwith the help of PICARD , a T5-3B model can be\nraised to state-of-the-art performance on the Spider\nand CoSQL datasets (Yu et al., 2019).\n2 The P ICARD Method\nPICARD warps model prediction scores and inte-\ngrates trivially with existing algorithms for greedy\nand beam search used in auto-regressive decoding\nfrom language models. Its arguments are the token\nids of the current hypothesis and, for each vocabu-\nlary token, the log-softmax scores predicted by the\nmodel’s language modeling head.PICARD also has\naccess to SQL schema information, in particular,\ninformation about the names of tables and columns\nand about which column resides in which table.\nAt each generation step, PICARD ﬁrst restricts\nprediction to the top-k highest probability tokens\nand then assigns a score of −∞to those that fail\nPICARD ’s numerous checks (see Figure 2). These\nchecks are enabled by fast incremental parsing\n(O’Sullivan and Gamari, 2021) based on monadic\ncombinators (Leijen and Meijer, 2001). There are\nfour PICARD mode settings that control their com-\nprehensiveness: off (no checking), lexing, parsing\nwithout guards, and parsing with guards—the high-\nest mode. A prediction that passes a higher mode\nwill always pass a lower mode but not necessarily\nvice versa.\n2.1 Lexing\nIn lexing mode, PICARD checks the output on\na lexical level only. It attempts to convert the\npartial, detokenized model output to a white-space\ndelimited sequence of individual SQL keywords\nlike select, punctuation like (), operators like\n+ and -, literals like string and number values in\nSQL conditions, and identiﬁers like aliases, tables,\nand columns—without being sensitive to the order\nin which these lexical items appear. By making it\nso, PICARD can detect spelling errors in keywords\nor reject table and column names that are invalid\nfor the given SQL schema. For instance, consider\nthe question \"What are the email, cell phone\nand home phone of each professional?\" from\nSpider’s development set on the dog_kennels\ndatabase. Our ﬁne-tuned T5-Large model predicts\nselect email_address, cell_phone,\nhome_phone from professionals while\nthe ground truth selects cell_number instead of\nthe invalid cell_phone column. This mistake is\ncaught and avoided by PICARD in lexing mode.\n2.2 Parsing without Guards\nIn the lowest parsing mode above lexing—referred\nto as parsing without guards— PICARD checks the\noutput on a grammatical level. PICARD attempts to\nparse the detokenized model output to a data struc-\nture that represents the abstract syntax tree (AST)\nof the predicted SQL query. Contrary to lexing\nmode, the order in which keywords and clauses ap-\npear now matters. PICARD can reject invalid query\nstructures, e.g. ﬁnd missing from clauses or incor-\nrect orders of clauses and keywords. It can also\ndetect a range of issues with compositions of SQL\n9897\nexpressions: Number one, if PICARD matches on\na tid.cid pattern, but the table with the id tid\ndoes not contain a column with id cid, then that\nparse is rejected. Secondly, ifPICARD ﬁrst matches\non an alias.cid pattern and then later matches\non the tid as alias pattern but tid does not\ncontain cid, then that parse is also rejected. An\nequivalent rule also exists for sub-queries bound to\ntable aliases. Lastly, PICARD prohibits duplicate\nbinding of a table alias in the same select scope,\nbut permits shadowing of aliases deﬁned in a sur-\nrounding scope. This can happen in nested SQL\nqueries.\n2.3 Parsing with Guards\nIn its highest parsing mode, PICARD engages\nin additional analyses—called guards—while as-\nsembling the SQL AST. If PICARD matches on\ntid.cid or alias.cid, then guards require\nthat the table tid or the alias alias, respectively,\nis eventually brought into scope by adding it to the\nfrom clause. Moreover, the alias alias is con-\nstrained to resolve to a table or a sub-query that has\nthe column cid in it. If PICARD matches on the\npattern cid, then another guard requires that ex-\nactly one table is eventually brought into scope that\ncontains a column with that id. These guards are\nenforced eagerly in order to fail fast and to eject\ninvalid hypotheses from the beam at the earliest\npossible time. The ﬁrst time this is happening is\nafter parsing the from clause.\nOnly with these guards, PICARD is able to\nreject a wrong prediction from our ﬁne-tuned\nT5-Large model like select maker, model\nfrom car_makers for the question \"What are\nthe makers and models?\" Here, the correct table to\nuse would have been model_list, since it is the\nonly one in Spider’scar_1 schema that contains\nboth a maker and a model column.\nAdditional checks and guards are conceivable,\nfor instance, checking that only expressions of\nthe same type are compared or that column types\nselected by union, except, or intersect\nqueries match. We leave these additional checks to\nfuture work.\n3 Experiments\nOur experiments are mainly focused on Spider\n(Yu et al., 2018), a large multi-domain and cross-\ndatabase dataset for text-to-SQL parsing. We train\non the 7,000 examples in the Spider training set and\nevaluate on Spider’s development set and its hid-\nden test set. We also report results on the CoSQL\nSQL-grounded dialog state tracking task (Yu et al.,\n2019), where we predict a SQL query for each\nquestion given previous questions in an interaction\ncontext. For this task, we train on both the Spider\ntext-to-SQL training data and the CoSQL dialog\nstate tracking training data, and evaluate on the\nCoSQL development and test sets.\nSpider and CoSQL are both zero-shot settings.\nThere is no overlap between questions or databases\nbetween the respective training, development, and\ntest sets.\nOn Spider, we determine model performance\nbased on three metrics: exact-set-match accuracy,\nexecution accuracy, and test-suite execution accu-\nracy (Zhong et al., 2020). Exact-set-match accu-\nracy compares the predicted and the ground-truth\nSQL query by parsing both into a normalized data\nstructure. This comparison is not sensitive to lit-\neral query values and can decrease under semantic-\npreserving SQL query rewriting. Execution accu-\nracy compares the results of executing the predicted\nand ground-truth SQL queries on the database con-\ntents shipped with the Spider dataset. This metric is\nsensitive to literal query values, but suffers from a\nhigh false positive rate (Zhong et al., 2020). Lastly,\ntest-suite execution accuracy extends execution to\nmultiple database instances per SQL schema. The\ncontents of these instances are optimized to lower\nthe number of false positives and to provide the\nbest approximation of semantic accuracy.\nOn CoSQL, we measure model performance in\nterms of the question match accuracy and the inter-\naction match accuracy. Both metrics are based on\nexact-set-match accuracy. Interaction match accu-\nracy is the joint accuracy over all questions in an\ninteraction.\nWe are encouraged by results by Shaw et al.\n(2021), who showed that a pre-trained T5-Base\nor T5-3B model can not only learn the text-to-\nSQL task, but also generalize to unseen databases,\nand even that T5-3B can be competitive with the\nthen-state-of-the-art (Choi et al., 2021; Wang et al.,\n2020)—all without modiﬁcations to the model. We\ntherefore use T5 as the baseline for all our experi-\nments.\nIn order to allow for generalization to unseen\ndatabases, we encode the schema together with the\nquestions. We use the same serialization scheme\nused by Shaw et al. (2021). In experiments using\n9898\ndatabase content, we detect and attach the database\nvalues to the column names in a fashion similar to\nthe BRIDGE model by Lin et al. (2020). When\nﬁne-tuning for the CoSQL dialog state tracking\ntask, we append the previous questions in the in-\nteraction in reverse chronological order to the in-\nput. Inputs exceeding the 512-token limit of T5 are\ntruncated. The target is the SQL from the Spider\nand/or CoSQL training sets, unmodiﬁed except for\na conversion of keywords and identiﬁers to lower\ncase. We ﬁne-tune T5 for up to 3072 epochs using\nAdafactor (Shazeer and Stern, 2018), a batch size\nof 2048, and a learning rate of 10−4.\nResults Our ﬁndings on the Spider dataset are\nsummarized in Table 1 and Figure 1. Our repro-\nductions of Shaw et al. (2021)’s results with T5\ncannot compete with the current state of the art on\nSpider. The issue is that these models predict a\nlot of invalid SQL. For instance, 12% of the SQL\nqueries generated by the T5-3B model on Spider’s\ndevelopment set result in an execution error. How-\never, when these same models are augmented with\nPICARD , we ﬁnd substantial improvements. First,\ninvalid SQL predictions become rare. For T5-3B\nwith PICARD , only 2% of the predictions are un-\nusable. In these cases, beam search exited without\nﬁnding a valid SQL prediction. Second, and most\nsigniﬁcantly, by using PICARD , the T5-3B model is\nlifted to state-of-the-art performance. We measure\nan exact-set-match accuracy of 75.5% on the devel-\nopment set and 71.9% on the test set. The execution\naccuracy results are 79.3% and 75.1%, respectively.\nThese numbers are on par or higher than those of\nthe closest competitor, LGESQL + ELECTRA\n(Cao et al., 2021) (see Table 1). Furthermore, we\nachieve a test-suite execution accuracy of 71.9%\non Spider’s development set.\nOur ﬁndings on the CoSQL dialog state tracking\ndataset (see Table 2) are similar to those for Spider.\nPICARD signiﬁcantly improves the performance,\nand our ﬁne-tuned T5-3B model achieves state-of-\nthe-art performance.\nPICARD is not only improving performance, it\nis also fast. During evaluation of the T5-3B model\non Spider, the decoding speed with beam size 4\non an NVIDIA A100-SXM4-40GB GPU was, on\naverage, 2.5 seconds per sample without PICARD\nand 3.1 seconds per sample with PICARD .\nBeam Size Figure 1 shows results on Spider with-\nout and with PICARD when parsing with guards\n1 2 4 8\nbeam size\n0.60\n0.62\n0.64\n0.66\n0.68\n0.70\nexact match accuracy\nturned off\nlexing\nparsing w/o guards\nparsing w guards\nnone\nﬁnalizing\nincremental\nFigure 3: Exact-set-match accuracy on the Spider de-\nvelopment set as a function of beam size for top- 4\nPICARD on T5-Large (schema only) and for different\noperation modes: turned off, lexing, parsing without\nguards, and parsing with guards. In each mode, P I-\nCARD is either used incrementally at each step or only\nwhen ﬁnalizing a hypothesis.\nfor different beam sizes and sizes of T5. For\neach model size, PICARD increases performance\nwith increasing beam size. These increases are\nthe strongest for the step from beam size 1 to 2,\nless pronounced from 2 to 4, and then saturating\nfor beam sizes above 4. Even with greedy search\n(beam size 1), PICARD allows for some modest\nimprovements. Note that, without PICARD , these\nmodels do not beneﬁt from beam search. The num-\nber, k, of highest-probability tokens that are pro-\ncessed by PICARD at each decoding step has a\nmodest to negligible impact on performance. It is\nthe largest for T5-Base, smaller for T5-Large, and\nalmost undetectable for T5-3B. We do not study\nthe case k = 1, because it reduces the beam search\nto constrained greedy search.\nAblations In Figure 3, we have condensed our\nablation analysis for PICARD . We show results for\nour T5-Large model in all four PICARD checking\nmodes and for four different beam sizes on the Spi-\nder development set. When checking incrementally\nat each decoding step, lexing shows a small im-\nprovement over the unconstrained T5 model. The\nresults without PICARD and with PICARD in lex-\ning mode are largely independent of the beam size.\nThis is different when PICARD is switched into\nthe more sophisticated parsing modes. Both, with\nand without guards, improvements from PICARD\nincrease rapidly for increasing beam sizes, where\nparsing with guards clearly has a strong lead over\n9899\nDevelopment Test\nSystem EM% EX% EM% EX%\nBRIDGE v2 + BERT (ensemble)† (Lin et al., 2020) 71.1 70.3 67.5 68.3\nSMBOP + GRAPPA† (Rubin and Berant, 2021) 74.7 75.0 69.5 71.1\nRATSQL + GAP† (Shi et al., 2021) 71.8 - 69.7 -\nDT-Fixup SQL-SP + ROBERTA† (Xu et al., 2021) 75.0 - 70.9 -\nLGESQL + ELECTRA † (Cao et al., 2021) 75.1 - 72.0 -\nT5-Base (Shaw et al., 2021) 57.1 - - -\nT5-3B (Shaw et al., 2021) 70.0 - - -\nT5-Base (ours) 57.2 57.9 - -\nT5-Base+PICARD 65.8 68.4 - -\nT5-Large 65.3 67.2 - -\nT5-Large+PICARD 69.1 72.9 - -\nT5-3B (ours) 69.9 71.4 - -\nT5-3B+PICARD 74.1 76.3 - -\nT5-3B† 71.5 74.4 68.0 70.1\nT5-3B+PICARD † 75.5 79.3 71.9 75.1\nTable 1: Our results (bottom) and relevant prior art (top) on the Spider text-to-SQL task. Shown are the exact-set-\nmatch accuracy (EM) and execution accuracy (EX) percentages on Spider’s development and test sets. Our results\nare for a beam of size 4, and PICARD is parsing with guards for the top-2 token predictions. A dagger (†) indicates\nuse of database content, otherwise schema only.\nDevelopment Test\nSystem QM% IM% QM% IM%\nRATSQL + SCORE (Yu et al., 2021) 52.1 22.0 51.6 21.2\nT5-3B 53.8 21.8 51.4 21.7\nT5-3B+PICARD 56.9 24.2 54.6 23.7\nTable 2: Our results (bottom) and relevant prior art (top) on the CoSQL dialog state tracking task. Shown are the\nquestion match accuracy (QM) and interaction match accuracy (IM) percentages on CoSQL’s development and\ntest sets. Our results are for a beam of size 4, and P ICARD is parsing with guards for the top-2 token predictions.\nparsing without them.\nIn order to compare PICARD with the ﬁltering-\nby-validity approach of Suhr et al. (2020) and Lin\net al. (2020), we have studied also what happens\nwhen PICARD is only checking hypotheses when\nthe model predicts their ﬁnalization with the end-of-\nsequence token.2 In this restrained mode, PICARD\nis still effective, but much less so compared to\nnormal incremental operation. The gap between\nthese two modes of operation only begins to shrink\nfor large beam sizes. This is understandable since\nLin et al. (2020) used beam sizes of at least 16\nand up to 64 to reach optimal results with ﬁltering\nwhile Suhr et al. (2020) used a beam of size 100.\n4 Conclusion\nWe propose and evaluate a new method,PICARD ,\nfor simple and effective constrained decoding with\n2This is not exactly equivalent to ﬁltering a completely\nﬁnalized beam, because the hypotheses rejected by PICARD\nnever enter it and never take up any space.\nlarge pre-trained language models. On both, the\nSpider cross-domain and cross-database text-to-\nSQL dataset and the CoSQL SQL-grounded dialog\nstate tracking dataset, we ﬁnd that the PICARD de-\ncoding method not only signiﬁcantly improves the\nperformance of ﬁne-tuned but otherwise unmodi-\nﬁed T5 models, it also lifts a T5-3B model to state-\nof-the-art results on the established exact-match\nand execution accuracy metrics.\nAcknowledgements\nWe thank Lee Zamparo for his contributions to the\nexperiments on the CoSQL dataset. Further, we\nwould like to thank Pete Shaw for his input on\nthe reproduction of the T5 results on Spider. We\nwould also like to extend our gratitude to Tao Yu\nand Yusen Zhang for their efforts in evaluating our\nmodel on the test split of the Spider and CoSQL\ndatasets. Finally, we thank our anonymous review-\ners for their time and valuable suggestions.\n9900\nReferences\nRuisheng Cao, Lu Chen, Zhi Chen, Yanbin Zhao,\nSu Zhu, and Kai Yu. 2021. LGESQL: Line graph en-\nhanced text-to-SQL model with mixed local and non-\nlocal relations. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 2541–2555, Online. Association for\nComputational Linguistics.\nDongHyun Choi, Myeong Cheol Shin, EungGyun Kim,\nand Dong Ryeol Shin. 2021. RY ANSQL: Re-\ncursively Applying Sketch-based Slot Fillings for\nComplex Text-to-SQL in Cross-Domain Databases.\nComputational Linguistics, 47(2):309–332.\nDaan Leijen and Erik Meijer. 2001. Parsec: Direct\nstyle monadic parser combinators for the real world.\nTechnical Report UU-CS-2001-27. User Model-\ning 2007, 11th International Conference, UM 2007,\nCorfu, Greece, June 25-29, 2007.\nKevin Lin, Ben Bogin, Mark Neumann, Jonathan Be-\nrant, and Matt Gardner. 2019. Grammar-based neu-\nral text-to-sql generation.\nXi Victoria Lin, Richard Socher, and Caiming Xiong.\n2020. Bridging textual and tabular data for cross-\ndomain text-to-sql semantic parsing. Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020.\nBryan O’Sullivan and Ben Gamari. 2021. attopar-\nsec: Fast combinator parsing for bytestrings and text.\nSoftware available on the Haskell package reposi-\ntory.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nOhad Rubin and Jonathan Berant. 2021. SmBoP:\nSemi-autoregressive bottom-up semantic parsing. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 311–324, Online. Association for Computa-\ntional Linguistics.\nPeter Shaw, Ming-Wei Chang, Panupong Pasupat, and\nKristina Toutanova. 2021. Compositional general-\nization and natural language variation: Can a se-\nmantic parsing approach handle both? In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 922–938,\nOnline. Association for Computational Linguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning ,\npages 4596–4604. PMLR.\nPeng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu,\nAlexander Hanbo Li, Jun Wang, Cicero Nogueira\ndos Santos, and Bing Xiang. 2021. Learning con-\ntextual representations for semantic parsing with\ngeneration-augmented pre-training. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence ,\nvolume 35, pages 13806–13814.\nAlane Suhr, Ming-Wei Chang, Peter Shaw, and Ken-\nton Lee. 2020. Exploring unexplored generalization\nchallenges for cross-database semantic parsing. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8372–\n8388, Online. Association for Computational Lin-\nguistics.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2020. Rat-sql:\nRelation-aware schema encoding and linking for\ntext-to-sql parsers. Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nPeng Xu, Dhruv Kumar, Wei Yang, Wenjie Zi, Keyi\nTang, Chenyang Huang, Jackie Chi Kit Cheung, Si-\nmon J.D. Prince, and Yanshuai Cao. 2021. Optimiz-\ning deeper transformers on small datasets. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 2089–\n2102, Online. Association for Computational Lin-\nguistics.\nPengcheng Yin and Graham Neubig. 2018. Tranx: A\ntransition-based neural abstract syntax parser for se-\nmantic parsing and code generation. Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Wal-\nter Lasecki, and Dragomir Radev. 2019. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1962–\n1979, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTao Yu, Rui Zhang, Alex Polozov, Christopher Meek,\nand Ahmed Hassan Awadallah. 2021. Score: Pre-\ntraining for context representation in conversational\nsemantic parsing. In International Conference on\nLearning Representations.\n9901\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, and et al. 2018. Spider: A\nlarge-scale human-labeled dataset for complex and\ncross-domain semantic parsing and text-to-sql task.\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing.\nRuiqi Zhong, Tao Yu, and Dan Klein. 2020. Seman-\ntic evaluation for text-to-sql with distilled test suites.\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8498624563217163
    },
    {
      "name": "Decoding methods",
      "score": 0.7349697351455688
    },
    {
      "name": "Parsing",
      "score": 0.6266354918479919
    },
    {
      "name": "Language model",
      "score": 0.5374200344085693
    },
    {
      "name": "Natural language processing",
      "score": 0.5258929133415222
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5146337747573853
    },
    {
      "name": "Code (set theory)",
      "score": 0.4579452872276306
    },
    {
      "name": "Programming language",
      "score": 0.45700204372406006
    },
    {
      "name": "Compiler",
      "score": 0.4462079107761383
    },
    {
      "name": "Algorithm",
      "score": 0.23903867602348328
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.126424640417099
    }
  ]
}