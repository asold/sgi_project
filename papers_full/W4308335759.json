{
  "title": "Automatic Question Generation Monolingual Multilingual pre-trained Models using RNN and Transformer in Low Resource Indonesian Language",
  "url": "https://openalex.org/W4308335759",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5031880905",
      "name": "Karissa Vincentio",
      "affiliations": [
        "Binus University"
      ]
    },
    {
      "id": "https://openalex.org/A5080650089",
      "name": "Derwin Suhartono",
      "affiliations": [
        "Binus University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2989613245",
    "https://openalex.org/W321051970",
    "https://openalex.org/W3153266325",
    "https://openalex.org/W3144069799",
    "https://openalex.org/W3152574567",
    "https://openalex.org/W4238581586",
    "https://openalex.org/W3156928032",
    "https://openalex.org/W4226032012",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3122807568",
    "https://openalex.org/W2963368301",
    "https://openalex.org/W2901386711",
    "https://openalex.org/W4293147552",
    "https://openalex.org/W3164108198",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W4221167718",
    "https://openalex.org/W4200277175",
    "https://openalex.org/W4288514117",
    "https://openalex.org/W4288514408",
    "https://openalex.org/W4288801854",
    "https://openalex.org/W4221047790",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3174724858",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288112344",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4289285192",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Although Indonesian is the fourth most frequently used language on the internet, the development of NLP in Indonesian has not been studied intensively. One form of NLP application classified as an NLG task is the Automatic Question Generation task. Generally, the task has proven well, using rule-based and cloze tests, but these approaches depend heavily on the defined rules. While this approach is suitable for automated question generation systems on a small scale, it can become less efficient as the scale of the system grows. Many NLG model architectures have recently proven to have significantly improved performance compared to previous architectures, such as generative pre-trained transformers, text-to-text transfer transformers, bidirectional autoregressive transformers, and many more. Previous studies on AQG in Indonesian were built on RNN-based architecture such as GRU, LSTM, and Transformer. The performance of models in previous studies is compared with state-of-the-art models, such as multilingual models mBART and mT5, and monolingual models such as IndoBART and IndoGPT. As a result, the fine-tuned IndoBART performed significantly higher than either BiGRU and BiLSTM on the SQuAD dataset. Fine-tuned IndoBART on most of the metrics also performed better on the TyDiQA dataset only, which has fewer population than the SQuAD dataset.",
  "full_text": "https://doi.org/10.31449/inf.v46i7.4236 Informatica 46 (2022) 103–118 103\nAutomatic Question Generation using RNN-based and Pre-trained\nTransformer-based Models in Low Resource Indonesian Language\nKarissa Vincentio and Derwin Suhartono\nComputer Science Department, School of Computer Science, Bina Nusantara University, Jakarta, 11530, In-\ndonesia\nE-mail: felicia.vincentio@binus.ac.id, dsuhartono@binus.edu\nKeywords: natural language processing, natural language generation, automatic question generation, recurrent\nneural network, long-short term memory, gated recurrent unit, transformer, fine-tuning\nReceived: June 14, 2022\nAlthough Indonesian is the fourth most frequently used language on the internet, the development\nof NLP in Indonesian has not been studied intensively. One form of NLP application classified as\nan NLG task is the Automatic Question Generation task. Generally, the task has proven well, us-\ning rule-based and cloze tests, but these approaches depend heavily on the defined rules. While\nthis approach is suitable for automated question generation systems on a small scale, it can be-\ncome less efficient as the scale of the system grows. Many NLG model architectures have recently\nproven to have significantly improved performance compared to previous architectures, such\nas generative pre-trained transformers, text-to-text transfer transformers, bidirectional auto-\nregressive transformers, and many more. Previous studies on AQG in Indonesian were built on\nRNN-based architecture such as GRU, LSTM, and Transformer. The performance of models in\nprevious studies is compared with state-of-the-art models, such as multilingual models mBART\nand mT5, and monolingual models such as IndoBART and IndoGPT. As a result, the fine-tuned\nIndoBART performed significantly higher than either BiGRU and BiLSTM on the SQuAD dataset.\nFine-tuned IndoBART on most of the metrics also performed better on the TyDiQA dataset only,\nwhich has fewer population than the SQuAD dataset.\nPovzetek: Za indonezijščino, četrti najpogostejši spletni jezik, so za poučevanje razvili jezikovni\npretvornik iz besedila v vprašanja.\n1 Introduction\nThe current education system requires a process to ef-\nficiently evaluate students’ understanding of lessons\nby reading a text’s content [1]. Preparation of ques-\ntions carried out by students can consume much time,\nwhile getting questions from external sources such as\ncollections of questions makes it possible that they are\nirrelevant to the content studied by students [2]. In ad-\ndition, questions designed to evaluate students’ under-\nstanding of textual reading can also be influenced by\ntheir effectiveness, which can be seen from the devel-\nopment of various strategies in preparing questions [3].\nFrom the emergence of these problems, many tech-\nniques have been investigated in the Question Genera-\ntion process based on content, generally known as the\nAutomatic Question Generation (AQG) system based\non NLP in the NLG branch from various approaches\nfrom rule-based to attention-based models [4].\nAQG is a job that automatically generates queries\nfrom various inputs such as original text, databases, or\nsemantic representations [5]. From this understanding,\nthe input type can take the form of various forms such\nas sentences, paragraphs, and poetry [6]. AQG has\nvarious applications such as healthcare systems, auto-\nmated help systems, chatbot systems, and other AQG\napplications [7]. In this paper, AQG is the subject\nof research that requires text or related information to\nbe processed using a sequence-to-sequence approach,\nnamely Bidirectional Gated Recurrent Unit (BiGRU),\nBidirectional Long Short Term Memory (BiLSTM),\nand Transformer architectures, as well as using the pre-\ntrained fine-tuning approach of the mBART and mT5\narchitectural models.\n104 Informatica 46 (2022) 103–118 K. Vincentio et al.\n1.1 AQG in English\nCohen first proposed AQG in 1929 to represent a ques-\ntion’s content in a formula with one or more inde-\npendent variables [8]. Since then, researchers have\nbecome interested in developing AQG in education\nfor educational purposes, mainly because asking ques-\ntions during teaching encourages students to under-\nstand what they are learning. One of the AQGs that\nWolfe proposed supported learning in 1976 [9].\nRecent AQG work showed that leveraging linguis-\ntic representation approaches such as Part Of Speech\n(POS) and Named Entity Recognition (NER) through\ndeep neural networks based on Bidirectional En-\ncoder Representations from Transformers (BERT) can\nachieve state-of-the-art results. The model architec-\nture consists of a two-layer bidirectional Long Short-\nTerm Memory (LSTM) encoder and a two-layer uni-\ndirectional LSTM decoder. The bidirectional LSTM\nencoder has been used for producing sequences of hid-\nden states, and the unidirectional LSTM decoder has\nthen used the representation to generate words [10].\nAnother recent work was fine-tuning a miniature\nversion of a T5 transformer language model consist-\ning of 220 million parameters using the SQuADv1.1\ndataset, which contains 100,000 question-answer\npairs. In order to generate questions, the model was\ntrained by receiving the passage, and the 30% prob-\nability of the answer was replaced with the [MASK]\ntoken [11]. Some open English question-answer pair\ndatasets can be leveraged for transfer learning ap-\nproaches in creating AQG systems [12, 13, 14, 15, 16].\n1.2 AQG in Indonesian\nVarious researches on AQG based on NLP have been\nconducted [17], but not many of them are observed\nin Indonesian. One study [18] that was conducted\nin Indonesian built a language model that utilizes a\nsequence-to-sequence approach and is trained on the\nSQuAD v2 [19] as well as TyDiQA [20] dataset, which\nhas been translated into Indonesian using the Google\nTranslate API v2 to the model with the Transformer\narchitecture along with Recurrent Neural Network\n(RNN) such as BiLSTM & BiGRU [21]. This study\nfound that the questions generated using BiLSTM and\nBiGRU were not significantly different. Meanwhile,\nthe use of Transformers found difficulties in under-\nstanding the semantic context of the information pro-\nvided [22].\nFigure 1: Related Research Modeling Process Dia-\ngram [18]\n1.3 Models Benchmark in NLG Tasks\nIn this research, benchmark resources [23, 24] have\nnot been involved in the results of the model studied\nfor the Question Generation task in Indonesian [18].\nIndonesian benchmark resources such as IndoNLU\n[25] & IndoNLG [4] can play a significant role in com-\nparisons and literature reviews by other researchers so\nthat they can be a reference in developing Automatic\nQuestion Generation that is more reliable and by the\ninformation provided in the form of textual or reading\ncontent.\nIn addition, because the IndoNLU benchmark only\ncovers NLU tasks in Indonesian, such as sentiment\nanalysis [26, 27, 28, 29, 30], which is similar to the\nGLUE [31] benchmark for English Natural Language\nUnderstanding (NLU) tasks, while the Question Gen-\neration task is an NLG task, the GEM benchmark,\nwhich is a benchmark for various NLG tasks including\nQuestion Generation, should also be applied [32]. The\nresources of the GEM benchmark have selected and\nprocessed the most common dataset for the available\nNLG tasks. GEM also conducted baseline modeling\nusing language models such as BART, T5 in one lan-\nguage, and the mBART model, mT5 for multilingual\nlanguages. Then GEM also provides a testbed in auto-\nmated evaluations, including metrics according to the\ntask. The GEM benchmark feature, which is regularly\nupdated, makes it easier for researchers in other fields\nof NLG to compare the model he built with previously\ndeveloped models [32].\nRecently, many NLG model architectures have\nproven to have significantly improved performance\ncompared to previous architectures, such as generative\npre-trained transformers, text-to-text transfer trans-\nformers, bidirectional auto-regressive transformers,\nand many more. Meanwhile, the previous study that\nraised AQG in Indonesian was only carried out on the\nGRU, LSTM, and Transformer methods [18]. In this\nstudy, to develop research on AQG assignments in In-\ndonesian, the performance of models in previous stud-\nies was compared with state-of-the-art models, such as\nAutomatic Question Generation on… Informatica 46 (2022) 103–118 105\nmBART and mT5.\nAlthough Indonesian is the fourth most frequently\nused language on the internet, the development of\nNLP in Indonesian has not been studied intensively\n[25]. The automatic question generation process is\none form of NLP application classified as an NLG task\n[18]. Generally, the task has proven well, using ap-\nproaches such as rule-based and cloze tests, but these\napproaches depend heavily on the set of rules that have\nbeen created. So this approach is suitable for auto-\nmated question generation systems on a small scale\nand can become less efficient as the scale of the system\ngrows [33]. In this context, deep learning approaches,\nespecially NLP, have better generalizations than rule-\nbased approaches [34]. Although the deep learning\napproach is relatively highly complex, the system can\nconstruct its rules and evolve coherently to adapt its\ndataset if adequately trained and properly configured\n[35].\nIn the previous related research [18], the Stanford\nquestion-and-answer dataset (SQuAD v2 [36]), which\nconsists of 536 articles with 161,550 collections of\nquestion-answer pairs in English, underwent transla-\ntion and pre-processing into Indonesian and followed\nby improving some of the translations by using fuzzy\nstring-matching to look for inconsistent translations,\nwhich then can be used for model training as well as\nmodel evaluation [18]. Language models based on\nRNN architecture such as GRU and LSTM in a bidi-\nrectional manner and language models based on trans-\nformer architecture with a sequence-to-sequence learn-\ning approach [18].\nSeveral adaptations were made to the language\nmodel on the RNN-based (BiGRU and BiLSTM) and\ntransformer from scratch, such as the use of several lin-\nguistic features (Ans, Case, POS, Named Entity (NE)),\nand the presence of a sentence embedding encoder.\nThis research aims to measure how well the language\nmodel based on the RNN architecture and the state-of-\nthe-art transformer-based models performs the ques-\ntion generation task in Indonesian [18]. Then, a vali-\ndation process was followed by testing the model us-\ning the SQuAD dataset as it is the validation set to see\nhow it performs on the same behavior dataset and fol-\nlowed by the evaluation on the TyDiQA dataset that is\nbuilt naturally from Indonesian [20]. Overall flow can\nbe seen in Figure 1.\n2 Deep Learning Methods\n2.1 RNN Based Models\nRNN is a widely used neural network architecture for\nNLP, which has been proven to be relatively accurate\nand efficient for developing language models as well\nas in tasks of speech recognition. Essentially RNN\nuses what is known as feedback loops which allow the\ninput sequence to be shared to different nodes as well\nas allowing RNN to have an internal memory that can\nhelp RNN generate predictions based on previous in-\nputs. As much NLP research progresses from time to\ntime, there are many novelty techniques, one of which\nshowed that bidirectionally processing the input se-\nquence can achieve a better understanding of the con-\ntext [21]. Visualization for each model can be seen in\nFigure 2.\nFigure 2: RNN vs. LSTM vs. GRU\nIn the first place, RNN was having a problem called\nthe vanishing gradient problem, which occurs when\nusing neural networks with gradient-based learning\nmethods and backpropagation. GRU (Gated Recur-\nrent Unit) was introduced to overcome this problem\nthat utilizes an update gate and reset gate so the model\ncan store information longer and remove irrelevant in-\nformation for prediction. BiGRU is a model that uses\ntwo GRU in which one GRU will accept input by for-\nwarding direction called forward GRU, and another\nwill accept input by backward direction named back-\nward GRU [21].\nLSTM is an RNN enhancement that is capable of\nstudying long-term dependencies [18]. This capability\nenables LSTMs to avoid long-term dependency prob-\nlems. LSTM uses three gates to protect and control\nthe cell gates: input gate, forget gate, and output gate.\nIn this research, BiLSTM will be used as the represen-\ntation of LSTM. The flow of each model in detail can\nbe seen in Figure 3.\n106 Informatica 46 (2022) 103–118 K. Vincentio et al.\nFigure 3: LSTM / GRU vs. BiLSTM / BiGRU Archi-\ntecture\n2.2 Transformer Based Models\nBART (Bidirectional and Auto-Regressive Trans-\nformer) is a language model that is pre-trained by ap-\nplying noise or corruption to the input sequence, and\nthen the model is assigned to reconstruct the actual in-\nput sequence [33]. After that, the results of the model\npredictions will be calculated against the loss function\ngenerally in the form of cross-entropy and followed\nby the back-propagation gradients process and updat-\ning the model weights. A comparison between RNN\nwith transformer architecture can be seen in Figure 4.\nFigure 4: RNN vs. Transformer Architecture\nThe BART language model architecture utilizes an\nencoder (see Figure 5) on BERT (Bidirectional En-\ncoder Representations from Transformers) [37] and\nFigure 5: Encoder-decoder Illustration\na decoder on GPT (Generative Pre-Trained Trans-\nformer) [38] capable of performing NLP tasks in the\nform of NLU and NLG. mBART is a language model\nmodified from the BART model, which utilizes au-\ntoencoder denoising and a sequence-to-sequence pre-\ntrained model. mBART model was trained once us-\ning a dataset of multiple languages that could be cus-\ntomized in a fine-tuning process [39].\nThe second one, T5 (Text-to-Text Transfer Trans-\nformer), is a pre-trained model that utilizes a unified\nText-to-Text format from NLP using text [40]. By us-\ning this model, when setting the configuration with hy-\nperparameters, then it will be applied to another task.\nThe third one, GPT leverages what is known as\nmasked self-attention, where it masks future tokens\nand only knows the present and the previous tokens.\nGPT works autoregressively by adding generated to-\nkens to the input sequence, and that particular new se-\nquence then will be used as the input to the model in\nits next step.\n3 Materials and Methods\nThe approach that is going to be used in this research\nis transfer learning, which is the approach where a\nmodel is first pre-trained on a data-rich task before\nbeing fine-tuned on a downstream task [40]. The re-\nsearch is divided into four main steps; the first step\n(planning phase) is to identify the problem, followed\nby the dataset preprocessing phase, which was done\nto preprocess the dataset in a configured format so\nthat the dataset can be forwarded to train the language\nmodel. The third step is to train the model, which\nleverages several preconfigured language models such\nas BiGRU and BiLSTM. As for the pre-trained lan-\nguage models like mBART and mT5 will require fur-\nther fine-tuning.\nAutomatic Question Generation on… Informatica 46 (2022) 103–118 107\n3.1 Planning\nIn the first step, previous research regarding automatic\nquestion generators using BiGRU and BiLSTM in In-\ndonesian was reviewed and evaluated. Both of those\nmodels are based on recurrent neural network archi-\ntecture. This research explores multilingual language\nmodels based on the transformer (encoder-decoder) ar-\nchitecture such as mBART and mT5 and monolingual\nlanguage models such as IndoNLG’s IndoBART and\nIndoGPT [4].\n3.2 Dataset Preprocessing\nThe dataset we are using is preprocessed SQuAD\ndataset that has been translated to Indonesian from the\ndataset itself for all the models, resulting in 102,657\ntraining data, 11,407 validation data, and 10,567 test-\ning data for SQuAD, and 550 testing data for TyDiQA.\nAs for the sequence-to-sequence language model, the\npreprocessed SQuAD dataset was first added with spe-\ncial tokens.\nSince some of the SQuAD question-answer pairs In-\ndonesian dataset translation might be misleading from\nits true meaning because the translation process for the\npassage and the context were done separately, some\ncorrections need to be made. The correction pro-\ncess was done by leveraging fuzzy string matching to\nsearch the translated question-answer pairs for incon-\nsistent translations thoroughly. As long as the answer\nis found, it will update the start position of the answer,\nwhereas if the answer is not found, then the start posi-\ntion of the answer will be set to negative one (-1) and\nremoved.\nIn this step, we preprocessed the enhanced SQuAD\ndataset from previous research [36] by reusing\nsome of the main dataset attributes (context/passage,\nquestion, and answer) that are going to be used\nby the model for training, excluding some of the\nlinguistic features such as part of speech (POS) and\nnamed entity (NE) attributes that will be used only\nby BiGRU, BiLSTM, and Transformer model. For\nmBART, the input encoder structure will be formatted\nto <context><sep><answer><eos><langid>,\nand decoder <langid><question><eos>,\nwhereas for mT5 the input encoder will be in\n<context><sep><answer> format, as for the de-\ncoder format is going to be <bos><question>.\nAs for GPT since it is a decoder only trans-\nformer language model, then the input se-\nquence will be formatted to <context><sep>\n<answer><bos><question><eos>. The flow can\nbe seen in Figure 6.\nFigure 6: Process to Repair Answer Translation Result\nfrom Previous Research\n3.3 Training Model\nBy utilizing the formatted dataset, these models were\nmade by applying configuration from the Sequence-to-\nSequence Learning method for the Indonesian Auto-\nmatic Question Generator for several algorithms, Bi-\nGRU and BiLSTM. For the mBART and mT5, new\nfine-tuning models were made [41]. Alternately, the\nmodel training was conducted to ensure the computer\nuses the same resources.\n3.4 Evaluation\nThe results from each algorithm are evaluated by us-\ning BLEU and ROUGE metrics. From that, compar-\nison and analysis results are conducted based on the\nresults to choose the best from all of the implemented\nalgorithms. The overall flow of benchmarking model\ncan be seen in Figure 7.\n4 Results & Discussion\nTokenization is a way to separate a piece of text into\nsmaller units known as tokens, which can be words,\ncharacters, or subwords. In order to fine-tune the\nmBART pre-trained language model, the sequence\nthat is going to be forwarded to the model will firstly\nbe appended with some special tokens such as lan-\nguage id token for the multilingual model to identify\n108 Informatica 46 (2022) 103–118 K. Vincentio et al.\nTable 1: Related Research Rerun Result on SQuAD Test Set\nModel Dataset BLEU 1 BLEU 2 BLEU 3 BLEU 4 ROUGE\nL\nEpoch\nBiGRU\nCased 33.87 17.01 8.42 3.88 37.98 20\nCased-Copy 36.03 19.37 9.57 5.41 40.96 20\nCased-Copy-Cov 36.16 18.79 10.99 6.52 40.49 20\nUncased 36.96 17.23 8.46 5.11 40.08 20\nUncased-Copy 39.62 22.26 12.02 5.88 43.38 20\nUncased-Copy-Cov 39.56 21.99 11.34 5.99 43.41 20\nBiLSTM\nCased 32.16 14.61 7.73 3.73 38.00 10\nCased-Copy 36.67 19.28 10.85 5.29 40.67 10\nCased-Copy-Cov 35.86 18.69 9.21 7.07 40.27 10\nUncased 35.45 18.19 8.87 4.63 39.48 10\nUncased-Copy 40.60 21.35 10.93 5.73 43.79 10\nUncased-Copy-Cov 39.90 22.23 12.49 5.98 43.34 10\nTransformer\nCased 30.72 12.63 4.44 2.46 34.25 300\nCased-Copy 36.14 18.81 9.52 4.75 39.58 300\nUncased 33.34 13.58 5.86 3.38 37.71 300\nUncased-Copy 39.09 21.21 10.83 5.39 43.69 300\nFigure 7: Models Benchmark creation process dia-\ngram.\nthe language, beginning of sentence token, end of sen-\ntence token, as well as separator token to be used as a\nseparator between the answer and the question, which\nthen can be used for the model to identify the label\nand the context. Then the tokenizer will tokenize the\nrest of the passage into token representation so that the\nmodel can understand the context. Unlike mBART,\nfine-tuning mT5 does not require a language id token\nto help the model identify the language that is sup-\nposed to be appended to the input sequence. Surpris-\ningly when not considering the special tokens or skip-\nping special tokens, the language model cannot parse\nthe input sequence correctly; therefore, it performs\npoorly.\nTable 3 and Table 4 are some samples of the gener-\nated questions in Indonesian from each of all the mod-\nels on datasets SQuAD and TyDiQA. “Input Sentence\n& Answer” is the context or passage as the model input\nfollowed by the expected answer, and “Target Ques-\ntion” is the expected generated question.\nRNN-based models evaluated on the TyDiQA\ndataset are performing lower than SQuAD dataset due\nto most of the text being translated on the SQuAD\ndataset, which consists of many faulty translations\n[18], while TyDiQA is in Indonesian by origin. It\nalso applies to the transformer-based models, includ-\ning those based on pre-trained multilingual and mono-\nlingual models. It can also be seen on the pre-trained\nmodels’ row that the maximum score on ROUGE and\nBLEU on TyDiQA is up to 10 points higher than the\nSQuAD dataset. This evaluation on the TyDiQA test\nset is done to obtain a more reliable and comparable\nevaluation score since TyDiQA is a more natural In-\ndonesian dataset [18].\nOn the RNN-based and transformer from scratch re-\nsults, the TyDiQA and SQuAD do not show a signif-\nicant difference in the scores, but they differ signifi-\ncantly on the pre-trained models, especially the mono-\nlingual models IndoBART and IndoGPT. With these\nmonolingual models, the TyDiQA dataset that is al-\nready available in Indonesian while SQuAD is mostly\nAutomatic Question Generation on… Informatica 46 (2022) 103–118 109\nTable 2: Related Research Rerun Result on TyDiQA Test Set\nModel Dataset BLEU 1 BLEU 2 BLEU 3 BLEU 4 ROUGE\nL\nEpoch\nBiGRU\nCased 30.33 10.83 3.18 1.89 34.41 20\nCased-Copy 34.05 15.01 7.47 3.12 38.15 20\nCased-Copy-Cov 34.28 14.72 6.60 2.63 38.30 20\nUncased 33.28 13.92 5.65 2.98 37.45 20\nUncased-Copy 37.42 17.96 8.73 4.65 41.71 20\nUncased-Copy-Cov 37.78 18.68 9.15 5.46 41.92 20\nBiLSTM\nCased 30.74 12.00 4.09 1.48 34.74 10\nCased-Copy 34.62 14.80 6.49 3.62 38.66 10\nCased-Copy-Cov 34.13 14.57 6.40 2.84 38.17 10\nUncased 32.92 13.02 4.81 2.28 36.98 10\nUncased-Copy 37.63 18.38 8.73 4.62 42.12 10\nUncased-Copy-Cov 38.14 18.54 8.90 4.55 42.59 10\nTransformer\nCased 27.88 8.00 0.71 0.64 31.86 300\nCased-Copy 31.95 12.43 4.63 2.27 36.37 300\nUncased 29.39 8.62 1.12 0.52 33.27 300\nUncased-Copy 37.23 17.93 8.19 3.40 41.90 300\ntranslation proves that monolinguals can perform bet-\nter on datasets within the same language.\nRemembering that the finetuned pre-trained models\nsuch as mBART, IndoBART, IndoGPT, and mT5 do\nnot use the linguistic features such as POS and NE pro-\nvided in the preprocessed dataset, these models out-\nperform the RNN-based models and the scratch mod-\nels. Without extra context in the form of POS and NE,\nthe transformer-based pre-trained models have proven\nthat transfer learning helps the models have a better un-\nderstanding than the models that do not have any base\nknowledge.\nGenerally, monolingual language models have a\nsmaller number of parameters than multilingual lan-\nguage models, resulting in faster model training and\nsmaller model size, whereas, in this research, mono-\nlingual language models were pre-trained on a large\nmonolingual corpus (Indonesian). On the other hand,\nmultilingual language models were pre-trained on a\nlarge multilingual corpus, hence the term multilingual.\nAs for the performance of both languages, they only\nperform slightly differently [4].\nFurthermore, numerous incorrect and unnatural\ntranslations, especially one of the SQuAD datasets on\nthe input sentences and target questions, impact our\nmodel predictions. Nonetheless, semantically, those\nsentences were still understandable. The same pro-\njected questions were agreed upon by all models, re-\nsulting in highly identical questions. There were some\nvariances in the verbs in the created questions, but they\nare all synonyms and have similar meanings.\nmT5 model seems to have the lowest automatic\nevaluation score among other pre-trained models.\nHowever, reading directly from the generated predic-\ntions, the mT5 prediction seems to have the most flu-\nent prediction. mT5 encoders that could affect this are\nbased on BERT language models, known for their nov-\nelty through a bidirectional approach that was able to\ncapture the context deeper of the input sequence [37].\nThese encoders also take account of the relation be-\ntween words, which helps capture its meaning [42],\nconsisting of a self-attention layer and a feedforward\nnetwork to process the sequence to the decoders. As\nfor the decoders, it is similar to the initially proposed\ntransformer language model [43]. The decoders were\nleveraging the auto-regressive approach, which will be\nused to produce the output sequence. mT5 were pre-\ntrained on a large multilingual corpus that covers over\n100 languages [41].\nThe hyperparameters used for each model are con-\nfigured in Table 5. As for the maximum sequence, the\nlength hyperparameter was set to 512 for every fine-\ntuned language model.\nTable 5 shows the configuration used in each model\n110 Informatica 46 (2022) 103–118 K. Vincentio et al.\nTable 3: All Models AQG Task Sample Predictions 1 SQuAD\nSample Prediction 1 - SQuAD\nInput Sentence & Answer\nSamudra Pasifik atau Lautan Teduh (dari bahasa spanyol Pacifico,\nartinya tenang) adalah kawasan kumpulan air terbesar di dunia,\nserta mencakup kira-kira sepertiga permukaan Bumi, dengan luas\nsebesar 179,7 juta km2 (69,4 juta mi2). Panjangnya sekitar\n15.500 km (9.600mi) dari Laut Bering di Arktik hingga batasan\nes di Laut Ross di Antartika di selatan. Samudra Pasifik mencapai\nlebar timur-barat terbesarnya pada sekitar 5 derajat U garis\nlintang, di mana ia terbentang sekitar 19.800 km (12.300mi) dari\nIndonesia hingga pesisir Kolombia. Batas sebelah barat samudra\nini biasanya diletakkan di Selat Malaka. Titik terendah permukaan\nBumi—Palung Mariana—berada di Samudra Pasifik. Samudra ini\nterletak di antara Asia dan Australia di sebelah barat, Amerika di\nsebelah timur, Antartika di sebelah selatan dan Samudra Arktik di\nsebelah utara.\nAnswer 179,7 juta km2\nTarget Question Berapa luas Samudera Pasifik?\nBiGRU Uncased-Cop-Cov berapa luas samudra pasifik ?\nBiLSTM Uncased-Copy-Cov berapa luas bumi pasifik ?\nTransformer Uncased-Copy berapa luas air terbesar di dunia ?\nmBART-Large berapakah luas samudra pasifik?\nIndoBART berapakah luas samudra pasifik?\nIndoGPT berapa luas total wilayah lautan pasifik ?\nmT5-Small Berapa luas samudra pasifik?\nto generate the sentences and the training time needed\nfor the SQuAD and TyDiQA datasets. The train-\ning step and valid for the mBART-L, IndoBART, In-\ndoGPT, and mT5-Base pre-trained models are not\nlisted because they are not explicitly defined in this\nmodeling.\nFine-tuned mBART performed the best with the av-\nerage BLEU 31.71 and ROUGE-L score of 46.27 on\nthe SQuAD dataset (Table 6) for the Indonesian ques-\ntion generation task. Fine-tuned IndoBART also per-\nformed the best with an average score of BLEU 17.26\nand ROUGE L score is 33.73 on the TyDiQA dataset\n(Table 6) for the Indonesian question generation task.\nOn the other hand, RNN-based and transformer\nfrom scratch results on TyDiQA and SQuAD datasets\ndo not show a significant difference in the scores, but\nthey differ significantly from the pre-trained models.\nWith these monolingual models, the TyDiQA, whose\norigin is in Bahasa while SQuAD is mostly transla-\ntion, proves that monolinguals can perform better on\ndatasets within the same language.\n5 Conclusions\nBased on the results achieved in this research, lan-\nguage models based on transformer architecture that\nleverage self-attention mechanisms were able to\nachieve state-of-the-art results in generating questions\ncompared to language models based on bidirectional\nrecurrent neural network architecture such as BiLSTM\nand BiGRU.\nThis research introduces a more extensive compari-\nson between RNN-based and transformer-based mod-\nels, including the state-of-the-art variation on the In-\ndonesian AQG system. In the previous research, it\nhas already been proven that the Indonesian AQG\nsystem can be built using an as-is machine-translated\nquestion answering dataset (SQuAD v2.0) with accept-\nable results, and this research is shown that better per-\nformance can be achieved with different varieties of\nAutomatic Question Generation on… Informatica 46 (2022) 103–118 111\nTable 4: All Models AQG Task Sample Predictions 2 TyDiQA\nSample Prediction 2 - TydiQA\nInput Sentence & Answer\nKadipaten Normandia , yang mereka bentuk dengan\nperjanjian dengan mahkota Prancis , adalah tanah\nyang indah bagi Prancis abad pertengahan , dan di\nbawah Richard I dari Normandia ditempa menjadi\nsebuah pemerintahan yang kohesif dan tangguh dalam\nmasa jabatan feodal.\nAnswer Kadipaten Normandia\nTarget Question Siapa yang memerintah kadipaten Normandia\nBiGRU Uncased-Cop-Cov siapa yang memerintah pemerintahan normandia ?\nBiLSTM Uncased-Copy-Cov siapa yang mendirikan kadipaten normandia ?\nTransformer Uncased-Copy siapa yang memerintah normandia di normandia ?\nmBART-Large siapakah kadipaten normandia di bawah raja normandia ?\nIndoBART siapa yang memimpin normandia ?\nIndoGPT dengan siapa prancis membentuk kadipaten normandia ?\nmT5-Small Siapa yang memerintahkan kadipaten normandia?\nTable 5: Model Configuration\nModel Dataset Learning\nRate\nTraining\nStep Valid Epoch Batch\nSize\nTraining\nTime\nBiGRU Uncased-Cop-Cov 1.00E-03 32.100 3.210 20 64 55m\nBiLSTM Uncased-Cop-Cov 1.00E-03 17.655 3.210 10 64 1h13m\nTransformer Uncased-Cop 1.00E+00 120.600 4.020 300 256 5h40m\nmBART-L Uncased-Large 1.00E-03 - - 20 8 40h42m\nIndoBART Uncased-v2 1.00E-03 - - 20 64 7h26m\nIndoGPT Uncased 1.00E-03 - - 20 32 10h36m\nmT5-Base Uncased-Small 3.00E-05 - - 3 4 8h4m\ntransformer-based models such as mBART and mT5,\nas well as the monolingual models built on Indonesian\ndataset; IndoBART.\n5.1 RNN-based vs Transformer-based\nTransformer-based models outperformed all the RNN-\nbased models. As seen in Table 6 & Table 7,\nTransformer-based models perform better in gen-\nerating natural Indonesian questions on the Ty-\nDiQA dataset, which contains 550 pairs of question-\nanswering Indonesian.\n5.1.1 Monolingual vs. Multilingual\nSince monolingual language models were pre-trained\nusing a monolingual dataset, the model resulted in\na lower number of parameters, hence faster training\nthan multilingual language models. In terms of perfor-\nmance, it does not differ very much from multilingual\nlanguage models and monolingual language models.\n5.2 Future Improvements\nThe system of building an Indonesian AQG can\nachieve better results with a more natural labeled In-\ndonesian QA or AQ dataset. It should be followed\nwith more robust preprocessing data to avoid syntacti-\ncally incorrect data and biases. Experiments on more\n112 Informatica 46 (2022) 103–118 K. Vincentio et al.\nTable 6: Model Evaluation Metric Performance Comparison on SQuAD Test Set\nModel BLEU 1 BLEU 2 BLEU 3 BLEU 4 Average\nBLEU\nROUGE\nL\nEpoch\nBiGRU 39.56 21.99 11.34 5.99 19.72 43.41 20\nBiLSTM 39.90 22.23 12.49 5.98 20.08 43.34 10\nTransformer 39.09 21.21 10.83 5.39 19.13 43.69 300\nmBART-L 53.58 32.41 23.25 17.59 31.71 44.70 20\nIndoBART 55.03 31.88 22.27 16.42 31.40 46.27 20\nIndoGPT 54.07 30.56 21.21 15.72 30.39 44.31 20\nmT5-Base 41.13 14.92 7.16 3.86 16.77 40.51 3\nTable 7: Model Evaluation Metric Performance Comparison on TyDiQA Test Set\nModel BLEU 1 BLEU 2 BLEU 3 BLEU 4 Average\nBLEU\nROUGE\nL\nEpoch\nBiGRU 37.78 18.68 9.15 5.46 17.77 41.92 20\nBiLSTM 38.14 18.54 8.90 4.55 17.53 42.59 10\nTransformer 37.23 17.93 8.19 3.40 16.69 41.90 300\nmBART-L 36.85 15.96 9.56 6.05 17.10 32.64 20\nIndoBART 38.65 16.01 8.95 5.43 17.26 33.73 20\nIndoGPT 35.77 12.55 6.55 3.78 14.66 28.93 20\nmT5-Base 32.23 7.98 2.39 0.92 10.88 36.10 3\nprecise hyperparameters can also help improve getting\nthe best-performing models.\nFuture work concerns a deeper analysis of par-\nticular mechanisms and proposals to explore differ-\nent techniques. Many other language models vary-\ning in parameter count can be explored for auto-\nmatic question generation tasks. Various hyperparam-\neter configurations can be optimized for the best lan-\nguage model, fine-tuning results through hyperparam-\neter tuning. Leveraging different evaluation metrics\ncan result in much more comprehensive results to see\nthe model’s capabilities within the bigger picture. It\nis also worth mentioning that the enhanced SQuAD\ndataset from previous research still has much room for\nimprovement.\nno\nReferences\n[1] G. Kurdi, J. Leo, B. Parsia, U. Sattler, and\nS. Al-Emari, “A systematic review of automatic\nquestion generation for educational purposes,”\nInternational Journal of Artificial Intelligence in\nEducation, vol. 30, 11 2019. [Online]. Available:\nhttps://doi.org/10.1007/s40593-019-00186-y\n[2] N.-T. Le, T. Kojiri, and N. Pinkwart, “Automatic\nquestion generation for educational applications\n– the state of art,” Advances in Intelligent\nSystems and Computing, vol. 282, pp. 325–\n338, 01 2014. [Online]. Available: https:\n//doi.org/10.1007/978-3-319-06569-4_24\n[3] J. Jamiluddin and V . Ramadayanti, “Develop-\ning Students’ Reading Comprehension Through\nQuestion Generation Strategy,” e-Journal of\nELTS (English Language Teaching Society) ,\nvol. 8, no. 1, Apr. 2020.\n[4] S. Cahyawijaya, G. I. Winata, B. Wilie,\nK. Vincentio, X. Li, A. Kuncoro, S. Ruder,\nZ. Y . Lim, S. Bahar, M. Khodra, A. Pur-\nwarianti, and P. Fung, “IndoNLG: Bench-\nmark and resources for evaluating Indone-\nsian natural language generation,” pp. 8875–\n8898, Nov. 2021. [Online]. Available: https:\n//doi.org/10.18653/v1/2021.emnlp-main.699\nAutomatic Question Generation on… Informatica 46 (2022) 103–118 113\n[5] C. A. Nwafor and I. E. Onyenwe, “An\nautomated multiple-choice question genera-\ntion using natural language processing tech-\nniques,” International Journal on Natural Lan-\nguage Computing, vol. 10, no. 02, p. 1–\n10, Apr 2021. [Online]. Available: http:\n//dx.doi.org/10.5121/ijnlc.2021.10201\n[6] A. D. Lelkes, V . Q. Tran, and C. Yu, “Quiz-style\nquestion generation for news stories,” New York,\nNY , USA, p. 2501–2511, 2021. [Online]. Avail-\nable: https://doi.org/10.1145/3442381.3449892\n[7] A. Graesser, V . Rus, S. D’Mello, and G. Jackson,\n“Autotutor: Learning through natural language\ndialogue that adapts to the cognitive and affec-\ntive states of the learner,”Current Perspectives\non Cognition, Learning and Instruction: Recent\nInnovations in Educational Technology that Fa-\ncilitate Student Learning, pp. 95–125, 01 2008.\n[8] N.-T. Le, T. Kojiri, and N. Pinkwart, “Automatic\nquestion generation for educational applications\n– the state of art,” Advances in Intelligent\nSystems and Computing, vol. 282, pp. 325–\n338, 01 2014. [Online]. Available: https:\n//doi.org/10.1007/978-3-319-06569-4_24\n[9] J. H. Wolfe, “Automatic question generation\nfrom text - an aid to independent study,” in\nSIGCSE ’76, 1976. [Online]. Available: https:\n//doi.org/10.1145/952989.803459\n[10] W. Yuan, T. He, and X. Dai, “Improving Neu-\nral Question Generation using Deep Linguis-\ntic Representation,” in Proceedings of the Web\nConference 2021. Ljubljana Slovenia: ACM,\nApr. 2021, pp. 3489–3500. [Online]. Available:\nhttps://doi.org/10.1145/3442381.3449975\n[11] K. Vachev, M. Hardalov, G. Karadzhov,\nG. Georgiev, I. Koychev, and P. Nakov, “Leaf:\nMultiple-choice question generation,” CoRR,\nvol. abs/2201.09012, 2022. [Online]. Available:\nhttps://doi.org/10.1007/978-3-030-99739-7_41\n[12] G. Lai, Q. Xie, H. Liu, Y . Yang, and E. H.\nHovy, “RACE: large-scale reading compre-\nhension dataset from examinations,” CoRR,\nvol. abs/1704.04683, 2017. [Online]. Available:\nhttps://doi.org/10.18653/v1/d17-1082\n[13] T. Mihaylov, P. Clark, T. Khot, and\nA. Sabharwal, “Can a suit of armor con-\nduct electricity? A new dataset for\nopen book question answering,” CoRR, vol.\nabs/1809.02789, 2018. [Online]. Available:\nhttps://doi.org/10.18653/v1/d18-1260\n[14] P. Clark, O. Etzioni, D. Khashabi, T. Khot,\nB. D. Mishra, K. Richardson, A. Sabhar-\nwal, C. Schoenick, O. Tafjord, N. Tan-\ndon, S. Bhakthavatsalam, D. Groeneveld,\nM. Guerquin, and M. Schmitz, “From ’f’\nto ’a’ on the N.Y . regents science exams:\nAn overview of the aristo project,” CoRR,\nvol. abs/1909.01958, 2019. [Online]. Available:\nhttps://doi.org/10.1609/aimag.v41i4.5304\n[15] P. Clark, I. Cowhey, O. Etzioni, T. Khot,\nA. Sabharwal, C. Schoenick, and O. Tafjord,\n“Think you have solved question answering?\ntry arc, the AI2 reasoning challenge,” CoRR,\nvol. abs/1803.05457, 2018. [Online]. Available:\nhttps://doi.org/10.1609/aaai.v33i01.33017063\n[16] O. Tafjord, P. Clark, M. Gardner, W. Yih, and\nA. Sabharwal, “Quarel: A dataset and models\nfor answering questions about qualitative rela-\ntionships,” CoRR, vol. abs/1811.08048, 2018.\n[17] F. C. Akyon, D. Cavusoglu, C. Cengiz,\nS. O. Altinuc, and A. Temizel, “Auto-\nmated question generation and question an-\nswering from Turkish texts using text-to-\ntext transformers,” arXiv:2111.06476 [cs], Nov.\n2021, arXiv: 2111.06476. [Online]. Available:\nhttps://doi.org/10.55730/1300-0632.3914\n[18] F. J. Muis and A. Purwarianti, “Sequence-to-\nsequence learning for indonesian automatic ques-\ntion generator,” 2020 7th International Confer-\nence on Advanced Informatics: Concepts, The-\nory and Applications, ICAICTA 2020, 9 2020.\n[Online]. Available: https://doi .org/10.1109/\nICAICTA49861.2020.9429032\n[19] P. Rajpurkar, J. Zhang, K. Lopyrev, and\nP. Liang, “SQuAD: 100,000+ questions for\nmachine comprehension of text,” in Proceed-\nings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing. Austin,\nTexas: Association for Computational Linguis-\ntics, Nov. 2016, pp. 2383–2392. [Online]. Avail-\nable: https://doi.org/10.18653/v1/D16-1264\n114 Informatica 46 (2022) 103–118 K. Vincentio et al.\n[20] J. H. Clark, E. Choi, M. Collins, D. Gar-\nrette, T. Kwiatkowski, V . Nikolaev, and\nJ. Palomaki, “TyDi QA: A benchmark for\ninformation-seeking question answering in ty-\npologically diverse languages,” Transactions\nof the Association for Computational Linguistics,\nvol. 8, pp. 454–470, 2020. [Online]. Available:\nhttps://doi.org/10.1162/tacl_a_00317\n[21] M. Schuster and K. Paliwal, “Bidirectional\nrecurrent neural networks,” IEEE Transactions\non Signal Processing, vol. 45, no. 11, pp.\n2673–2681, Nov. 1997. [Online]. Available:\nhttps://doi.org/10.1109/78.650093\n[22] K. Kriangchaivech and A. Wangperawong,\n“Question Generation by Transformers,”\narXiv:1909.05017 [cs] , Sep. 2019, arXiv:\n1909.05017.\n[23] J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Fi-\nrat, and M. Johnson, “XTREME: A massively\nmultilingual multi-task benchmark for evalu-\nating cross-lingual generalization,” CoRR, vol.\nabs/2003.11080, 2020.\n[24] P. Colombo, N. Noiry, E. Irurozki, and S. Clé-\nmençon, “What are the best systems? new per-\nspectives on NLP benchmarking,” CoRR, vol.\nabs/2202.03799, 2022.\n[25] B. Wilie, K. Vincentio, G. I. Winata, S. Cahyaw-\nijaya, X. Li, Z. Y . Lim, S. Soleman, R. Ma-\nhendra, P. Fung, S. Bahar, and A. Purwari-\nanti, “IndoNLU: Benchmark and resources\nfor evaluating Indonesian natural language un-\nderstanding,” in Proceedings of the 1st Confer-\nence of the Asia-Pacific Chapter of the Associa-\ntion for Computational Linguistics and the 10th\nInternational Joint Conference on Natural Lan-\nguage Processing. Suzhou, China: Asso-\nciation for Computational Linguistics, Dec.\n2020, pp. 843–857. [Online]. Available: https:\n//doi.org/10.18653/v1/2021.emnlp-main.699\n[26] W. Etaiwi, D. Suleiman, and A. Awajan,\n“Deep Learning Based Techniques for Sentiment\nAnalysis: A Survey,” Informatica, vol. 45,\nno. 7, Dec. 2021. [Online]. Available: https:\n//doi.org/10.31449/inf.v45i7.3674\n[27] A. C. Mazari and A. Djeffal, “Sentiment Analy-\nsis of Algerian Dialect Using Machine Learning\nand Deep Learning with Word2vec,”Informat-\nica, vol. 46, no. 6, Jul. 2022. [Online]. Available:\nhttps://doi.org/10.31449/inf.v46i6.3340\n[28] S. T. Al-Otaibi and A. A. Al-Rasheed, “A\nReview and Comparative Analysis of Sentiment\nAnalysis Techniques,” Informatica, vol. 46,\nno. 6, Jul. 2022. [Online]. Available: https:\n//doi.org/10.31449/inf.v46i6.3991\n[29] D. Suleiman, A. Odeh, and R. Al-Sayyed,\n“Arabic Sentiment Analysis Using Naïve Bayes\nand CNN-LSTM,” Informatica, vol. 46, no. 6,\nJul. 2022. [Online]. Available: https://doi.org/\n10.31449/inf.v46i6.4199\n[30] A. A. Al-Rasheed, “Finding Influential Users\nin Social Networking using Sentiment Analy-\nsis,” Informatica, vol. 46, no. 5, Mar. 2022.\n[Online]. Available: https://doi .org/10.31449/\ninf.v46i5.3829\n[31] A. Wang, A. Singh, J. Michael, F. Hill,\nO. Levy, and S. Bowman, “GLUE: A\nmulti-task benchmark and analysis platform\nfor natural language understanding,” in Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP. Brussels, Belgium: Asso-\nciation for Computational Linguistics, Nov.\n2018, pp. 353–355. [Online]. Available:\nhttps://doi.org/10.18653/v1/W18-5446\n[32] S. Gehrmann, T. Adewumi, K. Aggarwal, P. S.\nAmmanamanchi, A. Aremu, A. Bosselut, K. R.\nChandu, M.-A. Clinciu, D. Das, K. Dhole,\nW. Du, E. Durmus, O. Dušek, C. C. Emezue,\nV . Gangal, C. Garbacea, T. Hashimoto, Y . Hou,\nY . Jernite, H. Jhamtani, Y . Ji, S. Jolly, M. Kale,\nD. Kumar, F. Ladhak, A. Madaan, M. Maddela,\nK. Mahajan, S. Mahamood, B. P. Majumder,\nP. H. Martins, A. McMillan-Major, S. Mille,\nE. van Miltenburg, M. Nadeem, S. Narayan,\nV . Nikolaev, A. Niyongabo Rubungo, S. Osei,\nA. Parikh, L. Perez-Beltrachini, N. R. Rao,\nV . Raunak, J. D. Rodriguez, S. Santhanam, J. Se-\ndoc, T. Sellam, S. Shaikh, A. Shimorina, M. A.\nSobrevilla Cabezudo, H. Strobelt, N. Subramani,\nW. Xu, D. Yang, A. Yerukola, and J. Zhou, “The\nGEM benchmark: Natural language generation,\nits evaluation and metrics,” in Proceedings of\nthe 1st Workshop on Natural Language Genera-\nAutomatic Question Generation on… Informatica 46 (2022) 103–118 115\ntion, Evaluation, and Metrics (GEM 2021). On-\nline: Association for Computational Linguistics,\nAug. 2021, pp. 96–120. [Online]. Available:\nhttps://doi.org/10.18653/v1/2021.gem-1.10\n[33] M. Lewis, Y . Liu, N. Goyal, M. Ghazvinine-\njad, A. Mohamed, O. Levy, V . Stoyanov, and\nL. Zettlemoyer, “BART: Denoising sequence-\nto-sequence pre-training for natural language\ngeneration, translation, and comprehension,” in\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics. On-\nline: Association for Computational Linguistics,\nJul. 2020, pp. 7871–7880. [Online]. Available:\nhttps://doi.org/10.18653/v1/2020.acl-main.703\n[34] T. Wolf, L. Debut, V . Sanh, J. Chaumond,\nC. Delangue, A. Moi, P. Cistac, T. Rault,\nR. Louf, M. Funtowicz, J. Davison, S. Shleifer,\nP. von Platen, C. Ma, Y . Jernite, J. Plu,\nC. Xu, T. Le Scao, S. Gugger, M. Drame,\nQ. Lhoest, and A. Rush, “Transformers:\nState-of-the-art natural language processing,”\nin Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Process-\ning: System Demonstrations. Online: Asso-\nciation for Computational Linguistics, Oct.\n2020, pp. 38–45. [Online]. Available: https:\n//doi.org/10.18653/v1/2020.emnlp-demos.6\n[35] Y . Liu, J. Gu, N. Goyal, X. Li, S. Edunov,\nM. Ghazvininejad, M. Lewis, and L. Zettle-\nmoyer, “Multilingual Denoising Pre-training\nfor Neural Machine Translation,”Transactions\nof the Association for Computational Linguistics,\nvol. 8, pp. 726–742, 11 2020. [Online]. Avail-\nable: https://doi.org/10.1162/tacl_a_00343\n[36] P. Rajpurkar, R. Jia, and P. Liang, “Know\nwhat you don’t know: Unanswerable ques-\ntions for SQuAD,” in Proceedings of the 56th\nAnnual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers).\nMelbourne, Australia: Association for Com-\nputational Linguistics, Jul. 2018, pp. 784–789.\n[Online]. Available: https://doi .org/10.18653/\nv1/P18-2124\n[37] J. Devlin, M.-W. Chang, K. Lee, and\nK. Toutanova, “BERT: Pre-training of deep bidi-\nrectional transformers for language understand-\ning,” in Proceedings of the 2019 Conference\nof the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short\nPapers). Minneapolis, Minnesota: Asso-\nciation for Computational Linguistics, Jun.\n2019, pp. 4171–4186. [Online]. Available:\nhttps://doi.org/10.18653/v1/N19-1423\n[38] T. B. Brown, B. Mann, N. Ryder, M. Sub-\nbiah, J. Kaplan, P. Dhariwal, A. Neelakan-\ntan, P. Shyam, G. Sastry, A. Askell, S. Agar-\nwal, A. Herbert-V oss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. M. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner,\nS. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei, “Language Models are Few-Shot\nLearners,” in Advances in Neural Information\nProcessing Systems, H. Larochelle, M. Ranzato,\nR. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.\nCurran Associates, Inc., Jul. 2020, pp. 1877–\n1901, arXiv: 2005.14165.\n[39] Y . Tang, C. Tran, X. Li, P.-J. Chen, N. Goyal,\nV . Chaudhary, J. Gu, and A. Fan, “Multi-\nlingual translation from denoising pre-training,”\nin Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021. Online: As-\nsociation for Computational Linguistics, Aug.\n2021, pp. 3450–3466. [Online]. Available: https:\n//doi.org/10.18653/v1/2021.findings-acl.304\n[40] C. Raffel, N. Shazeer, A. Roberts, K. Lee,\nS. Narang, M. Matena, Y . Zhou, W. Li, and P. J.\nLiu, “Exploring the limits of transfer learning\nwith a unified text-to-text transformer,”Journal\nof Machine Learning Research, vol. 21, no. 140,\npp. 1–67, 2020.\n[41] L. Xue, N. Constant, A. Roberts, M. Kale,\nR. Al-Rfou, A. Siddhant, A. Barua, and C. Raf-\nfel, “mT5: A massively multilingual pre-trained\ntext-to-text transformer,” in Proceedings of the\n2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies. Online: As-\nsociation for Computational Linguistics, Jun.\n2021, pp. 483–498. [Online]. Available: https:\n//doi.org/10.18653/v1/2021.naacl-main.41\n[42] M. E. Peters, M. Neumann, M. Iyyer, M. Gard-\nner, C. Clark, K. Lee, and L. Zettle-\n116 Informatica 46 (2022) 103–118 K. Vincentio et al.\nmoyer, “Deep contextualized word repre-\nsentations,” in Proceedings of the 2018 Con-\nference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long\nPapers). New Orleans, Louisiana: Asso-\nciation for Computational Linguistics, Jun.\n2018, pp. 2227–2237. [Online]. Available:\nhttps://doi.org/10.18653/v1/N18-1202\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin, “Attention Is All You Need,” in Pro-\nceedings of the 31st International Conference\non Neural Information Processing Systems, ser.\nNIPS’17. Red Hook, NY , USA: Curran Asso-\nciates Inc., 2017, p. 6000–6010.\nAutomatic Question Generation on… Informatica 46 (2022) 103–118 117\nAppendix A\nTable 8: Translated texts from Indonesian to English for all Indonesian texts mentioned above.\n# Indonesian English\n1\nSamudra Pasifik atau Lautan Teduh (dari bahasa\nspanyol Pacifico, artinya tenang) adalah\nkawasan kumpulan air terbesar di dunia, serta\nmencakup kira-kira sepertiga permukaan Bumi,\ndengan luas sebesar 179,7 juta km2 (69,4 juta\nmi2). Panjangnya sekitar 15.500 km (9.600mi)\ndari Laut Bering di Arktik hingga batasan es di\nLaut Ross di Antartika di selatan. Samudra\nPasifik mencapai lebar timur-barat terbesarnya\npada sekitar 5 derajat U garis lintang, di mana\nia terbentang sekitar 19.800 km (12.300mi)\ndari Indonesia hingga pesisir Kolombia. Batas\nsebelah barat samudra ini biasanya diletakkan\ndi Selat Malaka. Titik terendah permukaan Bumi—\nPalung Mariana—berada di Samudra Pasifik.\nSamudra ini terletak di antara Asia dan\nAustralia di sebelah barat, Amerika di sebelah\ntimur, Antartika di sebelah selatan dan Samudra\nArktik di sebelah utara.\nThe Pacific Ocean or Ocean of Shades\n(from the Spanish Pacifico, meaning calm)\nis the largest area of water body in the\nworld, and covers about a third of the\nEarth’ s surface, with an area of 179.7\nmillion km2 (69.4 million mi2). It extends\nabout 15,500 km (9,600mi) from the\nBering Sea in the Arctic to the ice cap of\nthe Ross Sea in Antarctica in the south.\nThe Pacific Ocean reaches its greatest\neast-west width at about 5 degrees N\nlatitude, where it extends about 19,800 km\n(12,300mi) from Indonesia to the coast of\nColombia. The western boundary of this\nocean is usually placed in the Malacca\nStrait. The lowest point on Earth’ s\nsurface—the Mariana Trench—is in the\nPacific Ocean. This ocean is located between\nAsia and Australia to the west, America to\nthe east, Antarctica to the south and the\nArctic Ocean to the north.\n2 179,7 juta km2 179.7 million km2\n3 Berapa luas Samudera Pasifik? How wide is the Pacific Ocean?\n4 berapa luas samudra pasifik ? how wide is the pacific ocean?\n5 berapa luas bumi pasifik ? how big is the pacific earth?\n6 berapa luas air terbesar di dunia ? what is the largest area of water in the world?\n7 berapakah luas samudra pasifik? how wide is the pacific ocean?\n8 berapakah luas samudra pasifik? how wide is the pacific ocean?\n9 berapa luas total wilayah lautan pasifik ? What is the total area of the Pacific Ocean?\n10 Berapa luas samudra pasifik? How wide is the Pacific Ocean?\n11\nKadipaten Normandia , yang mereka bentuk\ndengan perjanjian dengan mahkota Prancis,\nadalah tanah yang indah bagi Prancis abad\npertengahan , dan di bawah Richard I dari\nNormandia ditempa menjadi sebuah\npemerintahan yang kohesif dan tangguh\ndalam masa jabatan feodal.\nThe Duchy of Normandy, which they formed\nby treaty with the French crown, was a\nbeautiful land for medieval France, and\nunder Richard I of Normandy was forged\ninto a cohesive and formidable government\nin feudal tenure.\n12 Kadipaten Normandia Duchy of Normandy\n13 Siapa yang memerintah kadipaten Normandia Who ruled the duchy of Normandy\n14 siapa yang memerintah pemerintahan normandia ? who governs the normandy government?\n15 siapa yang mendirikan kadipaten normandia ? who founded the duchy of normandy?\n118 Informatica 46 (2022) 103–118 K. Vincentio et al.\nAppendix B\nTable 9: Translated texts from Indonesian to English for all Indonesian texts mentioned above.\n# Indonesian English\n16 siapa yang memerintah normandia di\nnormandia ? who rules normandy in normandy ?\n17 siapakah kadipaten normandia di bawah\nraja normandia ?\nwho is the duchy of normandy under\nthe king of normandy ?\n18 siapa yang memimpin normandia ? who is in charge of normandy?\n19 dengan siapa prancis membentuk kadipaten\nnormandia ?\nWith whom did France form the\nDuchy of Normandy?\n20 Siapa yang memerintahkan kadipaten normandia? Who ruled the duchy of normandy?",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8313502669334412
    },
    {
      "name": "Computer science",
      "score": 0.7643234729766846
    },
    {
      "name": "Natural language processing",
      "score": 0.5888360142707825
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5878781676292419
    },
    {
      "name": "Indonesian",
      "score": 0.5231978893280029
    },
    {
      "name": "Generative grammar",
      "score": 0.45639652013778687
    },
    {
      "name": "Architecture",
      "score": 0.446311354637146
    },
    {
      "name": "Task (project management)",
      "score": 0.42728468775749207
    },
    {
      "name": "Language model",
      "score": 0.42349573969841003
    },
    {
      "name": "Text generation",
      "score": 0.4185715913772583
    },
    {
      "name": "Linguistics",
      "score": 0.11140382289886475
    },
    {
      "name": "Engineering",
      "score": 0.09416329860687256
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}