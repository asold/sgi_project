{
  "title": "Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog",
  "url": "https://openalex.org/W3004165599",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5035214368",
      "name": "Zekang Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5043877023",
      "name": "Zongjia Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5076799733",
      "name": "Jinchao Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5000232528",
      "name": "Yang Feng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5111632482",
      "name": "Cheng Niu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5066557235",
      "name": "Jie Zhou",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2931316642",
    "https://openalex.org/W2563399268",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2970252402",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2963186354",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2810643877",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2949681671",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W2917166349",
    "https://openalex.org/W2768661419",
    "https://openalex.org/W2798779216",
    "https://openalex.org/W3099388488",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "Audio-Visual Scene-Aware Dialog (AVSD) is a task to generate responses when chatting about a given video, which is organized as a track of the 8th Dialog System Technology Challenge (DSTC8). To solve the task, we propose a universal multimodal transformer and introduce the multi-task learning method to learn joint representations among different modalities as well as generate informative and fluent responses. Our method extends the natural language generation pre-trained model to multimodal dialogue generation task. Our system achieves the best performance in both objective and subjective evaluations in the challenge.",
  "full_text": "Bridging Text and Video: A Universal Multimodal Transformer\nfor Video-Audio Scene-Aware Dialog\nZekang Li14, Zongjia Li23, Jinchao Zhang2, Yang Feng1∗, Cheng Niu2, Jie Zhou2\n1Key Laboratory of Intelligent Information Processing\nInstitute of Computing Technology, Chinese Academy of Sciences\n2Pattern Recognition Center, WeChat AI, Tencent Inc, China\n3School of EECS, Peking University\n4University of Chinese Academy of Sciences\n{lizekang19g, fengyang}@ict.ac.cn, zongjiali@pku.edu.cn\n{dayerzhang, niucheng, withtomzhou}@tencent.com\nAbstract\nAudio-Visual Scene-Aware Dialog (A VSD) is a task to gen-\nerate responses when chatting about a given video, which\nis organized as a track of the 8 th Dialog System Technol-\nogy Challenge (DSTC8). To solve the task, we propose a\nuniversal multimodal transformer and introduce the multi-\ntask learning method to learn joint representations among\ndifferent modalities as well as generate informative and ﬂu-\nent responses. Our method extends the natural language gen-\neration pre-trained model to multimodal dialogue generation\ntask. Our system achieves the best performance in both ob-\njective and subjective evaluations in the challenge.\nIntroduction\nRecently, scene-aware dialogue generation has attracted in-\ncreasing attention in both industry and academia due to its\nbroad application. It aims to generate informative and ﬂu-\nent dialogue responses grounding on the given scenes. Zhou,\nPrabhumoye, and Black (2018) propose a dataset for text-\nbased movie grounded conversations. Jack Urbanek (2019)\nbuilds a large-scale text adventure game platform, in which\nagents can act and speak grounded on the scenes described\nin the text. Inspired by human inherent multimodal under-\nstanding ability, Alamri et al. (2018) integrates multimodal-\nity to scene-aware dialogue and proposes the Audio-visual\nScene-Aware Dialog task.\nThe goal of the Audio-Visual Scene-Aware Dialog task\nis to generate correct and ﬂuent responses by understanding\nall modalities (e.g., text, video and audio), which is a more\nchallenging task than image-based or text-grounded dia-\nlog tasks. Figure 1 shows an example dialogue in DSTC8-\nA VSD dataset (Alamri et al. 2018). There are three chal-\nlenges in this task: acquiring accurate representation of the\nvideo, effective interaction among different modalities and\n∗Joint work with Pattern Recognition Center, WeChat AI, Ten-\ncent Inc, China. Yang Feng is the corresponding author. This work\nwas done when Zongjia Li was interning at Pattern Recognition\nCenter, WeChat AI, Tencent.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nCaption: A woman standing in a hallway takes off her slippers. She \nthen climbs on a chair and starts doing something with the ceiling light.\nSummary: A woman about 30 years old wearing a jean skirt and top is \nstanding on a stool and fixing something in the hallway next to a door. \nThe hallway has linoleum floors.\nQ1: where is the video happening ? \nA1: it is happening inside in the hallway \nQ2: are there any people in the video ?\nA2: yes there is one person in the video.\nQ10: what is the person doing ?\nA10: she is standing on a stool doing something with the ceiling light.\n···\nFigure 1: A dialogue sampled from the DSTC8-A VSD\ndataset. For each dialogue, there are video, audio, video cap-\ntion, dialogue summary and 10 turns of conversations about\nthe video.\nbetter understanding dialogues and generating responses.\nHori et al. (2019) introduces an LSTM-based encoder and\ndecoder with the multimodal attention. Dat Tien Nguyen\nand Asri (2019) propose a hierarchical recurrent encoder-\ndecoder framework for encoding and generating responses\nbased on a FiLM-based audio-visual feature extractor. Pa-\nsunuru and Bansal (2019) adopts a dual attention mechanism\nto encode and align multiple modalities. The winning team\nof the DSTC7-A VSD task (Sanabria, Palaskar, and Metze\n2019) focuses on using a hierarchical attention to combine\ntextual and visual modalities and employ the How2 dataset\nfor pre-training. Moreover, MTN (Le et al. 2019) proposes\nmultimodal transformer networks to encode video and in-\ncorporate information from different modalities.\nThese existing methods mainly use independent encoders\narXiv:2002.00163v1  [cs.CL]  1 Feb 2020\nVA1 VA2 VA3 VA4 VA5 Yes How old …\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 … n-4 n-3 n-2\n[Cap] A woman … [eos]\nVideo Embedder Text Embedder\nIs there one person\nSegment\nFeatures\nPosition\n[Cap] [Cap] [Cap] [Cap] [Cap] [user1] [user1] [user1] [user1] [user1] [user2] [user2] [user1] [user1] [user1] [user2] [user2] [user2][video] [video] [video][video][video] …\n30 years old\n[user2]\nn-1\nVA2 VA3 VA4 VA5 … A … [eos] … 30 old [eos]\nTransformer Block 1\nTransformer Block 2\nTransformer Block n\nŏ\nVideo-Audio Sequence  Modeling Response  Language ModelingCaption Language Modeling\n[user1] [user2] [user1] [user2] [eos]\nn\n[user2]\n…yearswoman\nFigure 2: Our universal multimodal transformer architecture. We concatenate video-audio, caption, dialogue history, and re-\nsponse features to a long sequence. For different types of input, we adopt different segments tokens (“[video]”, “[caption]”,\n“[user1]”, “[user2]”). We initialize our model with pre-trained GPT2 and introduce three tasks to ﬁne-tune our model:Response\nLanguage Modeling (RLM), Video-Audio Sequence Modeling (V ASM), Caption Language Modeling (CLM).\nto separately encode different modalities and then exploit\nthe attention mechanism to fusion the representations of dif-\nferent modalities, which can not fully beneﬁt from the joint\nrepresentation of multi-modalities. To tackle the aforemen-\ntioned problems of existing methods, in this paper, we de-\nsign a universal multimodal transformer to encode different\nmodalities and generate responses at the same time. Inspired\nby Bert (Devlin et al. 2018), GPT2 (Radford et al. 2019) and\nother pre-training works, we use the self-supervised learn-\ning method and adopt the multi-task learning (response lan-\nguage modeling, video-audio sequence modeling, and cap-\ntion language modeling) approach to learn joint representa-\ntions and generate informative and ﬂuent responses. To im-\nprove the textual representation and generation, we initialize\nour model with the pre-trained GPT2 (Radford et al. 2019)\nmodel and then ﬁne-tune it.\nOur contributions are as follows:\n• We are the ﬁrst to use pre-trained natural language gener-\nation models in multimodal dialogue generation.\n• We integrate multimodal features in one encoder and in-\ntroduce a multi-task leanring method to learn better joint\nrepresentations and generate more informative responses.\n• We achieve a state-of-the-art result on Audio-Visual\nScene-Aware Dialog (A VSD) Dataset, outperforming ex-\nisting methods and other teams in DSTC8-A VSD chal-\nlenge by a large margin.\nRelated Work\nMost works in the dialogue system focus on open-domain\ndialogues or task-oriented dialogues. As in human-to-human\nconversations, there is always background knowledge. Some\nrecent effort develops dialogue systems that can generate\nresponses grounding on the document or structured knowl-\nedge graph (Li et al. 2019; Zhou, Prabhumoye, and Black\n2018; Reddy, Chen, and Manning 2019; Dinan et al. 2018;\nMadotto, Wu, and Fung 2018). These systems can gener-\nate responses that are either more relevant to background\nknowledge or make more correct interactions. There are also\nsome works incorporating multimodal information in ques-\ntion answering and dialogues. Visual QA (Goyal et al. 2017;\nAgrawal et al. 2017) is to answer the given question about\nthe content of an image. Visual dialog (Das et al. 2017) is a\ntask to generate natural responses based on the given image\nand the dialogue context. These works consider text or im-\nages as the background knowledge, whereas in Audio-Visual\nScene-Aware Dialog the knowledge is video and audio.\nIt has been shown that pre-trained language models play\nan important role in improving the performance of language\ngeneration tasks, such as the dialogue system and text sum-\nmarization. Zhang et al. (2019) proposes a natural language\ngeneration model based on BERT to make good use of the\npre-trained language model in the encoding and decoding\nprocess. Wolf et al. (2019b) introduces transfer learning to\ngenerative data-driven dialogue systems using Generative\nPretrained Transformer (Radford et al. 2019). In our work,\nwe extend this transfer learning method to multimodal lan-\nguage generation tasks and propose a self-supervised learn-\ning method for better video representation.\nMethodology\nIn this section, we will describe our approaches to mul-\ntimodal dialogue systems. We will ﬁrst introduce the Au-\ndio Visual Scene-Aware Dialog (A VSD) task. Then we will\npresent our multimodal dialogue generation model and its\ntraining methods.\nTask Formulation\nOur goal is to integrate multimodal information, which\nconsists of video, audio and dialog context, to gener-\nate informative and ﬂuent responses. Formally, let V\nand A represent video and audio respectively. Con-\nsidering the similarity between the summary and the\nI3D\n······\nVGGish\n···\nI3D-rgbI3D-ﬂowVGGish\nFigure 3: Video and audio feature extractors. For video, we\nadopt pre-trained I3D-rgb and I3D-ﬂow to extract rgb fea-\ntures and optical ﬂow features. For audio, we use pre-trained\nVGGish model.\nvideo caption, we concatenate summary and caption as\na whole caption C = {c1,c2,...,c I}. We use U =\n{Q1,R1,Q2,R2,... QN,RN,}to denote the N turns of\ndialogue, where Qn represent the question n and Rn =\n{rn1,rn2,...,r nm}represent the response ncontaining m\nwords. Therefore, the probability to generate the response\nRn for the given question Qn considering video V, audio\nA, dialogue history U<n, and caption C can be computed\nas:\nP(Rn|V,A,C,U<n,Qn; θ) =\nm∏\nj=1\nP(rnj|V,A,C,U<n,Qn,rn,<j; θ) (1)\nModel Overview\nOur model architecture is illustrated in Figure 2, which is\na multilayer Transformer encoder based on the GPT2 ar-\nchitecture (Radford et al. 2019). More speciﬁcally, we em-\nployed a 12-layer decoder-only transformer with the multi-\nhead self-attention.\nText Input. For text features, we follow GPT2 (Radford et\nal. 2019) and tokenize the input sentence into WordPieces\n(Wu et al. 2016).\nVideo and Audio Input. For the given video Vk, we\nsplit the video to Tk segments with a sliding window of\nl video frames. As shown in Figure 3, for each segment\nSt = {f1,f2,...,f l}, where fi represents one frame, we\nuse pre-trained I3D-rgb and I3D-ﬂow model to extract dv-\ndimensional video features Vrgb and Vflow. Considering\naudio is synchronous with video, we select the audio from\nthe same segment and use pretrained VGGish model to ex-\ntract da-dimensional audio features as Avggish. We then\nconcatenate video I3D-rgb features, I3D-ﬂow features, and\nVGGish features:\nVAt = [Vrgb,Vflow,Avggish],VAt ∈R2dv+da (2)\nThen video-audio featuresVAare fed into a fully-connected\nlayer (Video Embedder), as shown in Figure 2, and projected\nto the same embedding space as text embedding.\nAs shown in Figure 2, to make our model have the ability\nto distinguish among different part of the input (video, cap-\ntion, speaker1 and speaker2) and make use of the order of\nthe sequence, the ﬁnal representation for each word token\nis obtained via summing up its word embedding (WE), po-\nsitional encoding (PE) and segment embedding (SE). Note\nthat “[video]”, “[cap]”, “[user1]”, and “[user2]” are used\nto represent the segment of video, caption, speaker1, and\nspeaker2 respectively.\nMulti-task Learning\nWe introduce three tasks to ﬁne-tune our model: Response\nLanguage Modeling conditioned on video, audio, caption\nand dialogue history , Video-Audio Sequence Modeling\nconditioned on caption and dialogue, and Caption Lan-\nguage Modeling conditioned on video and audio.\nResponse Language Modeling (RLM). The goal of this\ntask is to generate responses Rn = {rn1,rn2,...,r nm}\nbased on the video-audio features VA, caption C, dialogue\nhistory U<n, and question Qn, by minimizing the negative\nlog-likelihood loss function:\nLRLM(θ) =−E(VA,C,U,Q,R)∼D\nlog\nm∏\nj=0\nP(rnj|VA,C,U<n,Qn,rn,<j) (3)\nwhere θis the trainable parameters and (VA,C,U,Q) pairs\nare sampled from the whole training set D.\nVideo-Audio Sequence Modeling (V ASM). This task is to\npredict video-audio features given caption and dialogue his-\ntory. Unlike textual tokens which are represented as discrete\nlabels, video-audio features are high-dimensional and con-\ntinuous. Instead of clustering video-audio features to dis-\ncrete labels as Sun et al. (2019) do, we adopt the video-audio\nfeature regression method following (Chen et al. 2019). This\ntask regresses the Transformer output of video-audio feature\not to the next video-audio feature VAt+1. In particular, we\napply a fully-connected layer to transform the output to a\nvector gθ(ot) of the same dimensional as VAt+1. We train\nthis task by minimizing L2 loss:\nLVASM (θ) =E(VA,C,U)∼D\n1\nT\nT∑\nt=1\n∥gθ(ot) −VAt+1∥2\n2 (4)\nwhere ot = fθ(VA<t+1,C,U) and fθ represents our\nmodel.\nCaption Language Modeling (CLM). Similar to Response\nLanguage Modeling task, we train the model to generate\ncaption C = {c1,c2,...,c I}given the video-audio feature\nVA:\nLCLM(θ) =−E(VA,C)∼Dlog\nI∏\ni=0\nP(ci|VA,c<i) (5)\nModels BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr\nInput: text only\nHierarchical Attention - - - 0.376 0.264 0.554 1.076\nOur model (RLM) 0.747 0.627 0.527 0.445 0.287 0.594 1.261\nInput: text + video\nHierarchical Attention - - - 0.394 0.267 0.563 1.094\nMTN - - - 0.392 0.269 0.559 1.066\nOur model (RLM) 0.759 0.635 0.533 0.448 0.293 0.602 1.282\n+ V ASM 0.765 0.643 0.543 0.459 0.294 0.606 1.308\nInput: text + video w/o caption / summary\nNaive fusion - - - 0.309 0.215 0.487 0.746\nDSTC7-A VSD Team 9 - - - 0.315 0.239 0.481 0.773\nOur model (RLM) 0.694 0.570 0.476 0.402 0.254 0.544 1.052\n+ V ASM 0.677 0.556 0.462 0.389 0.250 0.533 1.004\n+ recaption 0.670 0.537 0.438 0.362 0.254 0.535 1.022\nTable 1: Objective evaluation results on the test set provided by the organizers in DSTC7-A VSD challenge (6 groundtruth\navailable).\nModels BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr Human rating\nInput: text only\nOur model (RLM) 0.744 0.626 0.525 0.442 0.287 0.595 1.231 3.934\nInput: text + video\nOur model (RLM) 0.739 0.624 0.528 0.447 0.284 0.592 1.226 3.895\n+ V ASM 0.746 0.626 0.528 0.445 0.286 0.598 1.240 -\nInput: text + video w/o caption / summary\nOur model (RLM) 0.677 0.556 0.462 0.387 0.249 0.544 1.022 -\n+ V ASM 0.669 0.550 0.457 0.385 0.246 0.540 0.988 -\n+ recaption 0.661 0.533 0.437 0.364 0.242 0.533 0.991 -\nTable 2: Objective and subjective evaluation results on the test set provided by the organizers in DSTC8-A VSD challenge (6\ngroundtruth available).\nDifferent Settings\nText-only. We only use text input and Response Language\nModeling (RLM) task.\nText + Video. We use text input and video-audio input and\nwe train the model with the aforementioned three tasks.\nText + Video w/o caption. In this setting, there are two\nmethods: 1) Don’t use caption in both training and testing,\nand train the model with Response Language Modeling\n(RLM) and Video-Audio Sequence Modeling (V ASM).\n2) Using captions and training the model with the three\naforementioned tasks. When testing, ﬁrst generate video\ncaption based on the given video-audio input (recaption)\nand then generate responses using video-audio input,\ngenerated caption, and dialogue history.\nExperiments\nDatasets\nWe use the Audio-Visual Scene-Aware Dialog (A VSD)\ndataset (Alamri et al. 2018). In this dataset, each dialog con-\nsists of a sequence of questions and answers about the given\nvideo between two speakers. There is a video description for\neach video. We use the state-of-the-art video feature extrac-\ntor I3D model pre-trained on YouTube videos and the Ki-\nnetics dataset (Kay et al. 2017). Speciﬁcally, we use the out-\nput from the “Mixed 5c” layer of the I3D network, which is\na 2048-dimensional vector. For audio features, we adopted\nthe VGGish model (Hershey et al. 2017) which outputs a\n128-dimensional embedding. There are 7,659 dialogues for\ntraining, 1787 dialogues for validation and 1710 dialogues\nfor testing.\nBaselines\nWe compare our model with several related baseline meth-\nods:\nNaive Fusion: The multimodal baseline provided by the\norganizers, which combines all modalities with a projection\nmatrix (Hori et al. 2019).\nHierarchical Attention: The hierarchical attention ap-\nproach to combine textual and visual modalities, which the\nteam ranked 1st in the DSTC7-A VSD task adopted.\nMTN: The state of the art system before the DSTC8-A VSD\nChallenge, which proposes Multimodal Transformer Net-\nworks (MTN) to encode videos and incorporate information\nfrom different modalities (Le et al. 2019).\nHistory Length BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr\n0 0.729 0.599 0.496 0.413 0.275 0.573 1.182\n1 0.760 0.638 0.536 0.452 0.296 0.605 1.305\n2 0.755 0.632 0.532 0.450 0.296 0.601 1.297\n3 0.765 0.643 0.543 0.459 0.294 0.606 1.308\n5 0.758 0.634 0.533 0.451 0.292 0.601 1.293\n9 0.759 0.631 0.526 0.441 0.296 0.603 1.294\nTable 3: Objective evaluation results on the test set of DSTC7-A VSD (6 groundtruth available) in which maximum history\ndialogue turn length ranges from 0 to 3 or 5 or 9. Best result in each metric is highlighted in bold.\nDecoding Methods BLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE-L CIDEr\nGreedy Search 0.743 0.610 0.503 0.416 0.284 0.587 1.217\nNucleus Sampling 0.680 0.525 0.410 0.321 0.252 0.527 0.955\nBeam Search 0.765 0.643 0.543 0.459 0.294 0.606 1.308\nTable 4: Objective evaluation results compared between different decoding methods.\nMetrics\nObjective evaluation: We report the metrics that are com-\nmonly used in the natural language generation tasks, such\nas BLEU (Papineni et al. 2002), METEOR (Denkowski and\nLavie 2014), ROUGE-L (Lin 2004), and CIDEr (Vedantam,\nLawrence Zitnick, and Parikh 2015). We evaluate our mod-\nels using the toolkit provided by the competition organizers.\nSubjective Evaluation: Subjective evaluations are essential\nfor dialogue generation. The organizers also evaluate some\nsystems based on crowd-sourced human ratings. The annota-\ntors are asked to consider the correctness, naturalness, infor-\nmativeness, and appropriateness of the generated responses\nand give a score at 5 levels.\nExperiment Settings\nIn our experiment, we initialize our model with the pre-\ntrained weights from the GPT2-base model (Radford et al.\n2019; Wolf et al. 2019a). In the training process, we use up\nto 3 turns of history. The hidden size of our model is 768 and\nthe batch size is 32. We use Adam optimizer with a learning\nrate of 6.25e-5. In the decoding process, we use beam search\nwith a beam size of 5, max length of 20 and a length penalty\nof 0.3.\nExperimental Results\nIn this section, we report the experimental results under dif-\nferent settings: text only, text + video, text + video without\ncaption/summary.\nText only. As shown in Table 1, compared to Hierarchical\nAttention which is used in the winner system of DSTC7-\nA VSD challenge, our model gets better performance on all\nmetrics. In detail, our model improves BLEU-4 by 0.069\nand CIDEr by 0.185. Additionally, Table 2 also shows the\nhuman evaluation rating in the DSTC8-A VSD track. During\nhuman evaluation, the evaluators are asked to rate even the\ngroundtruth references, which are scored 4.000, our model\nfor this task scores 3.934, which is the highest Human rat-\ning among all DSTC8 submissions. In the human rating per-\nspective, the results of our model are very close to human\ndialogue.\nText + video. This task uses full information of the track:\ncaption, summary, dialogue history, and video. As we can\nsee in Table 1, compared to MTN which is the former state-\nof-the-art model for this task, our model also achieves a huge\nimprovement. In particular, our model improved BLEU4 by\n0.056, and CIDEr by 0.216. Compared to the text-only task,\nour models achieve better results, which indicates that our\nmethod for video understanding is effective. We adopt multi-\ntask learning as we described before. Video-Audio sequence\nmodeling task improves the score of BELU-4 by 0.011 and\nCIDEr by 0.026. In DSTC8 results shown in Table 2, this\nmethod get a little lower BELU4 value, but still improved\nCIDEr by 0.014, which shows the method is effective.\nText + video w/o caption/summary. This setting is most\nsimilar to the actual scene-aware dialogue: we only have\nvideo-audio information and dialogue history. Therefore,\nthis task is more challenging. As shown in Table 1, we can\nsee the lower performance than the text + video task as ex-\npected, but it is gratifying that our model still got a relatively\nhigh performance. We outperform the DSTC7-A VSD Team\n9 who got the highest performance in this task by a large\nmargin. In this task, we also try to use the multi-task learn-\ning method but gets lower performance on almost all met-\nrics. We will discuss this phenomenon in the next section.\nAnalysis and Discussion\nTraining Method Analysis. As we introduced in Exper-\niment Results, after we adopt the feature regression, our\nmodel gets better performance in text + video task, but get\nlower performance in text + video w/o caption/summary\ntask. We think the reason is the shortage of information: only\nuses the history dialogue, it is difﬁcult to rebuild the masked\nvideo feature, and compared to rebuilding text from adjacent\ncontext, we think our model doesn’t have a strong ability in\nextracting information from videos, therefore, the method\ndoesn’t work in this task.\nFor the poor performance of CLM, we think the reason\nmay be similar: the poor ability in extracting video infor-\nmation of our model limited the performance for inferring\ncaption from the video. So we think future work can focus\non video comprehension as well as integrating video and text\ninformation.\nHistory Length. We experiment with our model in text +\nvideo settings with video regression loss to explore the in-\nﬂuence of dialogue history length. As shown in Table 3, our\nmodel gets the best performance when the maximum dia-\nlogue history length is 3.\nDecoding Methods. To ﬁnd an effective decoding method\nfor multimodal dialogue generation, we try various decod-\ning methods, including greedy search, beam search, and nu-\ncleus sampling (Holtzman et al. 2019) which samples text\nfrom the dynamic nucleus of the probability distribution.\nAs shown in Figure 4, decoding with beam search gets the\nbest results on all objective results among these three de-\ncoding methods. We consider that in Audio-Visual Scene-\nAware Dialog, grounding on video and caption, responses\nare relatively more deﬁnite than that in open-domain dia-\nlogues. Therefore, it’s better to use the beam search when\ndecoding in this task.\nConclusion and Future Work\nIn this paper, we propose a multimodal dialogue generation\nmodel based on a pre-trained language model and introduce\nmulti-task learning to learn more accurate joint representa-\ntion among multi modalities and generate more informative\nresponses. In the future, we plan to use more video features\nlike ResNet features and explore more training tasks to im-\nprove the joint understanding of video and text. In addition,\nwe hope to extend these methods to other tasks, such as\nvideo captioning, image captioning, and visual dialog.\nAcknowledgments\nWe sincerely thank the anonymous reviewers for their thor-\nough reviewing and valuable suggestions. This work is sup-\nported by National Natural Science Foundation of China\n(NO.61876174) and National Key R&D Program of China\n(NO.2017YFE9132900).\nReferences\n[Agrawal et al. 2017] Agrawal, A.; Lu, J.; Antol, S.;\nMitchell, M.; Zitnick, C. L.; Parikh, D.; and Batra, D. 2017.\nVqa: Visual question answering. International Journal of\nComputer Vision123(1):4–31.\n[Alamri et al. 2018] Alamri, H.; Hori, C.; Marks, T. K.; Ba-\ntra, D.; and Parikh, D. 2018. Audio visual scene-aware\ndialog (avsd) track for natural language generation in dstc7.\nIn DSTC7 at AAAI2019 Workshop, volume 2.\n[Chen et al. 2019] Chen, Y .-C.; Li, L.; Yu, L.; Kholy, A. E.;\nAhmed, F.; Gan, Z.; Cheng, Y .; and Liu, J. 2019.\nUniter: Learning universal image-text representations.arXiv\npreprint arXiv:1909.11740.\n[Das et al. 2017] Das, A.; Kottur, S.; Gupta, K.; Singh, A.;\nYadav, D.; Moura, J. M.; Parikh, D.; and Batra, D. 2017.\nVisual dialog. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 326–335.\n[Dat Tien Nguyen and Asri 2019] Dat Tien Nguyen,\nShikhar Sharma, H. S., and Asri, L. E. 2019. From ﬁlm\nto video: Multi-turn question answering with multi-modal\ncontext. In DSTC7 at AAAI2019 Workshop.\n[Denkowski and Lavie 2014] Denkowski, M., and Lavie, A.\n2014. Meteor universal: Language speciﬁc translation eval-\nuation for any target language. In Proceedings of the EACL\n2014 Workshop on Statistical Machine Translation.\n[Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and\nToutanova, K. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805.\n[Dinan et al. 2018] Dinan, E.; Roller, S.; Shuster, K.; Fan,\nA.; Auli, M.; and Weston, J. 2018. Wizard of wikipedia:\nKnowledge-powered conversational agents. arXiv preprint\narXiv:1811.01241.\n[Goyal et al. 2017] Goyal, Y .; Khot, T.; Summers-Stay, D.;\nBatra, D.; and Parikh, D. 2017. Making the v in vqa matter:\nElevating the role of image understanding in visual question\nanswering. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 6904–6913.\n[Hershey et al. 2017] Hershey, S.; Chaudhuri, S.; Ellis, D. P.;\nGemmeke, J. F.; Jansen, A.; Moore, R. C.; Plakal, M.; Platt,\nD.; Saurous, R. A.; Seybold, B.; et al. 2017. Cnn architec-\ntures for large-scale audio classiﬁcation. In 2017 ieee in-\nternational conference on acoustics, speech and signal pro-\ncessing (icassp), 131–135. IEEE.\n[Holtzman et al. 2019] Holtzman, A.; Buys, J.; Forbes, M.;\nand Choi, Y . 2019. The curious case of neural text degener-\nation. arXiv preprint arXiv:1904.09751.\n[Hori et al. 2019] Hori, C.; Alamri, H.; Wang, J.; Wichern,\nG.; Hori, T.; Cherian, A.; Marks, T. K.; Cartillier, V .; Lopes,\nR. G.; Das, A.; et al. 2019. End-to-end audio visual scene-\naware dialog using multimodal attention-based video fea-\ntures. In ICASSP 2019-2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\n2352–2356. IEEE.\n[Jack Urbanek 2019] Jack Urbanek, Angela Fan, S. K. S. J.\nS. H. E. D. T. R. D. K. A. S. J. W. 2019. Learning to speak\nand act in a fantasy text adventure game.\n[Kay et al. 2017] Kay, W.; Carreira, J.; Simonyan, K.; Zhang,\nB.; Hillier, C.; Vijayanarasimhan, S.; Viola, F.; Green, T.;\nBack, T.; Natsev, P.; et al. 2017. The kinetics human action\nvideo dataset. arXiv preprint arXiv:1705.06950.\n[Le et al. 2019] Le, H.; Sahoo, D.; Chen, N.; and Hoi, S.\n2019. Multimodal transformer networks for end-to-end\nvideo-grounded dialogue systems. In Proceedings of the\n57th Annual Meeting of the Association for Computational\nLinguistics, 5612–5623. Florence, Italy: Association for\nComputational Linguistics.\n[Li et al. 2019] Li, Z.; Niu, C.; Meng, F.; Feng, Y .; Li, Q.;\nand Zhou, J. 2019. Incremental transformer with deliber-\nation decoder for document grounded conversations. arXiv\npreprint arXiv:1907.08854.\n[Lin 2004] Lin, C.-Y . 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization branches\nout, 74–81.\n[Madotto, Wu, and Fung 2018] Madotto, A.; Wu, C.-S.; and\nFung, P. 2018. Mem2seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog systems.\narXiv preprint arXiv:1804.08217.\n[Papineni et al. 2002] Papineni, K.; Roukos, S.; Ward, T.;\nand Zhu, W.-J. 2002. Bleu: a method for automatic evalua-\ntion of machine translation. In Proceedings of the 40th an-\nnual meeting on association for computational linguistics,\n311–318. Association for Computational Linguistics.\n[Pasunuru and Bansal 2019] Pasunuru, R., and Bansal, M.\n2019. Dstc7-avsd: Scene-aware video-dialogue systems\nwith dual attention. In DSTC7 at AAAI2019 Workshop.\n[Radford et al. 2019] Radford, A.; Wu, J.; Child, R.; Luan,\nD.; Amodei, D.; and Sutskever, I. 2019. Language models\nare unsupervised multitask learners.\n[Reddy, Chen, and Manning 2019] Reddy, S.; Chen, D.; and\nManning, C. D. 2019. Coqa: A conversational question\nanswering challenge. Transactions of the Association for\nComputational Linguistics7:249–266.\n[Sanabria, Palaskar, and Metze 2019] Sanabria, R.; Palaskar,\nS.; and Metze, F. 2019. Cmu sinbads submission for the\ndstc7 avsd challenge. In DSTC7 at AAAI2019 Workshop.\n[Sun et al. 2019] Sun, C.; Myers, A.; V ondrick, C.; Murphy,\nK.; and Schmid, C. 2019. Videobert: A joint model for\nvideo and language representation learning. arXiv preprint\narXiv:1904.01766.\n[Vedantam, Lawrence Zitnick, and Parikh 2015] Vedantam,\nR.; Lawrence Zitnick, C.; and Parikh, D. 2015. Cider:\nConsensus-based image description evaluation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 4566–4575.\n[Wolf et al. 2019a] Wolf, T.; Debut, L.; Sanh, V .; Chaumond,\nJ.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.;\nFuntowicz, M.; and Brew, J. 2019a. Huggingface’s trans-\nformers: State-of-the-art natural language processing. ArXiv\nabs/1910.03771.\n[Wolf et al. 2019b] Wolf, T.; Sanh, V .; Chaumond, J.; and\nDelangue, C. 2019b. Transfertransfo: A transfer learning\napproach for neural network based conversational agents.\narXiv preprint arXiv:1901.08149.\n[Wu et al. 2016] Wu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .;\nNorouzi, M.; Macherey, W.; Krikun, M.; Cao, Y .; Gao, Q.;\nMacherey, K.; et al. 2016. Google’s neural machine transla-\ntion system: Bridging the gap between human and machine\ntranslation. arXiv preprint arXiv:1609.08144.\n[Zhang et al. 2019] Zhang, H.; Gong, Y .; Yan, Y .; Duan, N.;\nXu, J.; Wang, J.; Gong, M.; and Zhou, M. 2019. Pretraining-\nbased natural language generation for text summarization.\narXiv preprint arXiv:1902.09243.\n[Zhou, Prabhumoye, and Black 2018] Zhou, K.; Prabhu-\nmoye, S.; and Black, A. W. 2018. A dataset for document\ngrounded conversations. arXiv preprint arXiv:1809.07358.",
  "topic": "Dialog box",
  "concepts": [
    {
      "name": "Dialog box",
      "score": 0.8793390989303589
    },
    {
      "name": "Computer science",
      "score": 0.8286930918693542
    },
    {
      "name": "Transformer",
      "score": 0.6798765063285828
    },
    {
      "name": "Modalities",
      "score": 0.6408042311668396
    },
    {
      "name": "Bridging (networking)",
      "score": 0.5634108185768127
    },
    {
      "name": "Speech recognition",
      "score": 0.5343223214149475
    },
    {
      "name": "Task (project management)",
      "score": 0.5337128639221191
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4816209673881531
    },
    {
      "name": "Multimodal learning",
      "score": 0.4623526334762573
    },
    {
      "name": "Natural language processing",
      "score": 0.4273267090320587
    },
    {
      "name": "Multimodality",
      "score": 0.41570034623146057
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3818366229534149
    },
    {
      "name": "Multimedia",
      "score": 0.35022908449172974
    },
    {
      "name": "World Wide Web",
      "score": 0.08856222033500671
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}