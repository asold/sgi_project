{
    "title": "Transformer-based dimensionality reduction",
    "url": "https://openalex.org/W4313432872",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2230571804",
            "name": "Ruisheng Ran",
            "affiliations": [
                "Chongqing Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2120113606",
            "name": "Tianyu Gao",
            "affiliations": [
                "Chongqing Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2102925857",
            "name": "Wen-feng Zhang",
            "affiliations": [
                "Chongqing Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2754001939",
            "name": "Shunshun Peng",
            "affiliations": [
                "Chongqing Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A1981856311",
            "name": "Bin Fang",
            "affiliations": [
                "Chongqing University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4406406874",
        "https://openalex.org/W4205191215",
        "https://openalex.org/W2296761881",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W6826231612",
        "https://openalex.org/W6828894009",
        "https://openalex.org/W6687567705",
        "https://openalex.org/W6602728322",
        "https://openalex.org/W6815268342",
        "https://openalex.org/W1676314349",
        "https://openalex.org/W1975056068",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W2053186076",
        "https://openalex.org/W2097308346",
        "https://openalex.org/W1782590233",
        "https://openalex.org/W3131860561",
        "https://openalex.org/W2663800299",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2597655663",
        "https://openalex.org/W2129795301",
        "https://openalex.org/W1583093964",
        "https://openalex.org/W3158846111",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W2557864411",
        "https://openalex.org/W1498436455",
        "https://openalex.org/W2404498690",
        "https://openalex.org/W2962898354",
        "https://openalex.org/W2951954489",
        "https://openalex.org/W2100495367",
        "https://openalex.org/W2025768430",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2752828042",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W1680622244",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2515770085",
        "https://openalex.org/W2755282748",
        "https://openalex.org/W2786672974",
        "https://openalex.org/W2755007803",
        "https://openalex.org/W3110144845",
        "https://openalex.org/W2910603373",
        "https://openalex.org/W1527548706",
        "https://openalex.org/W2153934661",
        "https://openalex.org/W3197910789",
        "https://openalex.org/W3102431071",
        "https://openalex.org/W2962949934",
        "https://openalex.org/W4212914270",
        "https://openalex.org/W2143784448",
        "https://openalex.org/W2154872931",
        "https://openalex.org/W3151034650",
        "https://openalex.org/W3000409346"
    ],
    "abstract": "Abstract Recently, Transformer is much popular and plays an important role in the fields of Machine Learning (ML), Natural Language Processing (NLP), and Computer Vision (CV), etc. In this paper, based on the Vision Transformer (ViT) model, a new dimensionality reduction (DR) model is proposed, named Transformer-DR. From data visualization, image reconstruction and face recognition, the representation ability of Transformer-DR after dimensionality reduction is studied, and it is compared with some representative DR methods to understand the difference between Transformer-DR and existing DR methods. The experimental results show that Transformer-DR is an effective dimensionality reduction method.",
    "full_text": "Transformer-based dimensionality reduction\nRuisheng Ran  \n \nChongqing Normal University\nTianyu Gao \nChongqing Normal University\nWenfeng Zhang \nChongqing Normal University\nShunshun Peng \nChongqing Normal University\nBin Fang \nChongqing University\nResearch Article\nKeywords: Dimensionality reduction, Vision Transformer, data visualization, image reconstruction\nPosted Date: January 2nd, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-2417990/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction\nRuisheng Ran1*, Tianyu Gao1, Wenfeng Zhang1, Shunshun\nPeng1 and Bin Fang2\n1The College of Computer and Information Science, Chongqing\nNormal University, Chongqing 401331, China.\n2The College of Computer Science, Chongqing University,\nChongqing 400044, China.\n*Corresponding author(s). E-mail(s): rshran@cqnu.edu.cn;\nContributing authors: kaotienyu@163.com;\nitzhangwf@cqnu.edu.cn; 20210005@cqnu.edu.cn; fb@cqu.edu.cn;\nAbstract\nRecently, Transformer is much popular and plays an important role\nin the ﬁelds of Machine Learning (ML), Natural Language Pro-\ncessing (NLP), and Computer Vision (CV), etc. In this paper,\nbased on the Vision Transformer (ViT) model, a new dimension-\nality reduction (DR) model is proposed, named Transformer-DR.\nFrom data visualization, image reconstruction and face recognit ion,\nthe representation ability of Transformer-DR after dimensionality\nreduction is studied, and it is compared with some representa-\ntive DR methods to understand the diﬀerence between Transformer-\nDR and existing DR methods. The experimental results show that\nTransformer-DR is an eﬀective dimensionality reduction method.\nKeywords: Dimensionality reduction, Vision Transformer, data\nvisualization, image reconstruction\n1 Introduction\nIn this information age, a large amount of data has been generated in various\nfields, including education, medical care, Internet, social medi a and business\n1\nSpringer Nature 2021 LATEX template\n2 Transformer-based dimensionality reduction\n[1]. These data are usually high-dimensional, heterogeneous, complex andmas-\nsive [2], and they have different forms, such as text, digital image, voice sign al\nand video.\nAlthough the existing machine learning methods can also deal with high -\ndimensional data, they have some difficulties in dealing with such massive and\nhigh-dimensional data. The high dimension of the data will increase the com-\nplexity of the model, resulting in the slow training process of t he model, low\ncomputational efficiency, and not conducive to the solution of the problem .\nThis is the so-called ”dimension disaster” problem [ 3][4]. Some data contain\nthousands of features. If a model learns from many features, it will st rongly\nrely on the data, resulting in overfitting and poor generalization [ 5]. In addi-\ntion, because the original data may contain redundant, insignificant or noi sy\ninformation, the accuracy of the learned model will be poor [ 3].\nDimension reduction (DR) technology provides a method to reduce t he\ndimension and preserve the effective features of data [ 3][6][7]. It is usually\nused in the preprocessing stage of data before the machine learning m odel to\nimprove the performance of the model. DR has the following advantages: 1)\nthe lower data dimension reduces the computational complexity and im proves\nthe efficiency of the model training, 2) to reduce the complexity of th e model\nand avoid the over-fitting problem, 3) it helps to eliminate redund ant and\ninsignificant features and noisy information, thus improving the accu racy of\nthe model, 4) it is easy for data visualization and interpretability.\nAt present, researchers have proposed various Dimensionality Redu ction\nmethods [ 3][6][7][8]. DR can be divided into linear and nonlinear methods.\nRepresentatives of linear methods are, Principal Component Analysis (PCA)\n[9], Linear Discriminant Analysis (LDA) [ 10], Locality Preserving Projections\n(LPP) [ 11], etc. Nonlinear methods include global structure-based methods\nsuch as Multi-Dimensional Scaling (MDS) [12], Isometric Mapping (ISOMAP)\n[13], and Kernel PCA (KPCA) [ 14], and local structure-based methods such\nas Laplacian Eigenmaps (LE) [ 15], Locally Linear Embedding (LLE) [ 16],\nt-Distributed Stochastic Neighbor Embedding (t-SNE) [17][18], Uniform man-\nifold approximation and projection (UMAP) [ 18]. According to whether the\nclass label of the sample is adopted in the training process, it is div ided into\nunsupervised method and supervised method. Most data dimensionality reduc-\ntion methods are unsupervised methods, such as PCA, LPP, MDS, LE, LLE,\netc. The representative supervised dimensionality reduction methods are LDA\nand ICA. According to whether the objective function contains a local op ti-\nmal solution, it is divided into convex method and non-convex method . The\nobjective function of the non-convex method contains the local optimal solu-\ntion, and the representative method is Autoencoder [\n19]; The convex method\ndoes not contain the local optimal solution, and most dimensionality redu c-\ntion methods are convex methods. These dimensionality reduction al gorithms\nhave corresponding applications in different fields, and they each ha ve their\nadvantages when dealing with different data.\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction 3\nSome representatives of linear, nonlinear, unsupervised, superv ised, and\nnon-convex dimensionality reduction methods are introduced, including PCA,\nLPP, LLE, LDA, and Autoencoder. PCA is an unsupervised linear method,\nwhich is to find a linear transformation to map the data from the high-\ndimensional space to the low-dimensional space, and makes the variance of the\nlow-dimensional data is as large as possible and makes the features as uncorre-\nlated as possible [\n9]. Different from the variance maximization theory of PCA,\nLinear Discriminant Analysis (LDA) is to perform low-dimensional proj ection\non the original data. After the projection, the distances of the within- class\ndata are as ”close” as possible, and the distances of the between-class data are\nas ”far” as possible, so that the data has good separability. LDA is a super-\nvised, linear DR method [\n10]. Laplacian Eigenmaps (LE) is a manifold-based\nDR method, which expects the adjacent points in the high-dimensi onal space\nto be as close as possible in the low-dimensional space, so that the nei gh-\nborhood structure relationship of the original data can be maintained aft er\nDR [7]. Locality Preserving Projections (LPP) [ 11] is a linear approximation\nof LE, which is proposed to solve the ”out-of-sample” problem [ 20]. Locally\nLinear Embedding (LLE) is based on that each data point and its closest cer-\ntain number of neighbors are viewed as a locally linear patch of the manifol d\nand then reconstruct each data point from its neighbors in the correspon ding\nsubspace [\n16].\nAutoencoder (AE) is based on multi-layer neural network architectu re,\nwhich is a nonlinear, unsupervised, non-convex DR method [ 19]. AE consists\nof encoder and decoder, which are stacked by fully connected layers. The high-\ndimensional data is the input of the encoder for dimensionality redu ction to\nget low-dimensional representation, and the low-dimensional data is the input\nof the decoder to reconstruct the original data, the network parameters are\nadjusted by backpropagation to reduce the error between the input data and\nthe reconstructed data [ 21]. Based on the design of AE, it can learn not only\nlinear features but also nonlinear features. If the input images contai n noise,\nwe use Autoencoder for dimensionality reduction, the network can still extract\nuseful features from the messy information. This is the Denoising Autoencoder\n(DAE), which trains a robust model [ 22].\nIn recent years, as a new neural network structure, Transformer [ 23] is\nmuch popular and plays an important role in the fields of Machine Learn-\ning. Transformer was first proposed to solve the problem of Natural Language\nProcessing (NLP), and then it was also applied to the Computer Vision fie ld,\nso the Vision Transformer (ViT) [\n24] model was proposed for image clas-\nsification. Based on ViT, we propose a dimension reduction model named\nTransformer-DR. On MNIST datasets and ImageNet datasets, we conduct\nexperiments in data visualization, image reconstruction, and face rec ogni-\ntion, investigate the dimensionality reduction ability of Transforme r-DR, and\ncompared it with some dimensionality reduction methods, so as to try to\nunderstand the difference between Transformer-DR and the state-of -the-art\nSpringer Nature 2021 LATEX template\n4 Transformer-based dimensionality reduction\ndimensionality reduction methods. The results show that Transfor mer-DR is\nan effective dimensionality reduction technique.\n2 Related works\nRecurrent Neural Network (RNN) [\n25] once occupied a dominant position\nin Natural Language Processing (NLP), but its parallel computing ability is\nseverely limited due to the structure of RNN. Transformer was propose d by\nVaswani et al. To solve the problem that RNN cannot be parallelized in NLP\n[\n23]. Transformer is a deep neural network based on self-attention mechan ism\n[26], which can capture the effect of semantic associations with long inte rvals.\nTransformer consists of encoder and decoder. The encoder consists of Multi-\nhead Attention, Shortcut Connection [27], Layer Normalization [28], and Feed\nForward [29]. The decoder has one more Mutil-Head Attention and normaliza-\ntion layer than the encoder, which is used to receive the output of the encoder.\nTransformer can capture richer features with multi-head attention [ 13]. It is\nprecisely because of the advantages of Transformer that it can replace RNN\nin the field of NLP.\nHowever, in the CV field, convolutional neural networks (CNN) [\n30] still\ndominate. Inspired by the application of transformer in NLP, many researchers\ntried to apply transformer to the CV field. Recently, the Visual Tran sformer\n(ViT) model was proposed in Ref. [24] And used in image classification, thereby\nmigrating Transformer from the NLP field to the CV field for the first time .\nViT only uses the encoder part of the Transformer. It divides the image into\nnon-overlapping patches and then flattens them, embeds the position encoding\nto get patch embedding, and input the patch embedding to the stackedencoder\nto get the prediction result. When trained with enough data, ViT can per form\nwell in image classification tasks. It shows that Transformer can also achi eve\ngood results in the CV field without CNN. After ViT, Transformer is wide ly\nused in various fields of computer vision.\n3 Method\nThe most important part of Transformer is the self-attention mechanism .\nWith the self-attention mechanism, the correlation between Seque nces (repre-\nsented as the correlation between patches in ViT) can be found. Among the se\nSequences/patches, some information may be ”redundant” or insignifican t, so\nwe can use the self-attention mechanism to calculate the correlation b etween\npatches, then remove redundant features and extract more useful fe atures, so\nas to achieve the purpose of dimensionality reduction.\nWe designed a network structure based on transformer, which is composed\nof encoder and decoder. Both the encoder and decoder are stacked by multiple\nTransformer blocks, as shown in Fig.\n1. Where, a Transformer block can be\nviewed as a component, after the input data passes through this componen t,\nthe dimensionality of data will be reduced. Based on the network stru cture,\nthe dimensionality of original data is gradually reduced by each Transform er\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction 5\nFig. 1 The Transformer-DR architecture.\nblock of the encoder, and then the original data is finally encoded int o code.\nThen, the code is then gradually reconstructed into the original image by each\nTransformer block of the decoder. This model minimizes the error between the\nreconstructed image and the original image, so that the code from the enco der\ncan best represent the original image.\nFor the 2D images, we divide the image into several non-overlapping\npatches, flatten the patches and add position encodings to get the patche s\nembedding, then the patches embedding will be input into the en coder.\nThe encoder consists of multiple Transformer blocks stacked. In each Trans-\nformer block, the input data first passes through the Multi-Head Att ention\nlayer to calculate the attention mechanism and then passes through the Layer\nNormalization and Feed Forward layers to get the output data of the Trans-\nformer block. The Feed Forward layer of each Transformer block is exp ressed\nas:\nF F N(x) = GELU (xW1 + b1)W2 + b2. (1)\nWhere, W1 ∈ Rdinput× dhidden , b1 ∈ Rdhidden , W2 ∈ Rdhidden× doutput , b2 ∈\nRdoutput , dhidden is the dimension size of the hidden layer, dinput is the size of\ndata at each Transformer block entry,doutput is the size of the output data after\nprocessed by the Feed Forward layer. And doutput < dinput, the dimensionality\nof data will be reduced in the Feed Forward layer of each Transformer bl ock.\nSpringer Nature 2021 LATEX template\n6 Transformer-based dimensionality reduction\nIn this way, after stacking multiple Transformer blocks, the origi nal data will\nbe embedded as a low-dimensional code.\nThe decoder is also a stack of multiple Transformer blocks. Differe nt\nfrom the encoder, the Feed Forward layer of each Transformer block con verts\nthe data from low-dimensional to high-dimensional, and finally outputs the\nreconstructed image.\nThe structure of the encoder and decoder is symmetrical. That is, the\nnumber of Transformer blocks contained in the encoder and the decoder is the\nsame. Secondly, at the corresponding positions of the encoder and the decoder,\nthe number of nodes in the Feed Forward layer of each Transformer bloc k is\nalso symmetrical.\nWe use the Mean Square Error (MSE) as the loss function to update the\nnetwork parameters by reducing the error between the original images and the\nreconstructed images.The formula of MSE is expressed as:\nMSE = 1\nN\nN∑\ni=1\nXi − ˆXi\n2 (2)\nwhere, Xi is the original image, ˆXi is the reconstructed image of the model.\nIn this way, the original data is encoded as code, and the dimensionali ty is\nreduced. The code can be regarded as an effective representation of the orig-\ninal data. Since the network structure uses Transformer for dimensi onality\nreduction, we call it Transformer-based Dimensionality Reduction, which is\nabbreviated as Transformer-DR for convenience.\nCompared with Autoencoder, the encoder and decoder of Transformer-\nDR are stacked by multiple Transformer blocks, while AE is stacked b y\nrestricted Boltzmann machines. From the experimental results in Section 4,\nafter dimensionality reduction, Transformer-DR is better than Auto encoder\nfor ordinary images. And in the case where the image is masked, due to the use\nof the self-attention mechanism, the quality of the reconstructed i mage after\ndimensionality reduction of Transformer-DR is better than that of AE.\nMasked Autoencoder (MAE) is composed of encoder and decoder, it can\nreconstruct patches that have been randomly masked. Compared with MAE ,\nMAE does not reduce the dimensionality of the data, and high-dimensional\nimages still extract high-dimensional features. Secondly, MAE discar ds the\nmasked image blocks at the input layer, only extracts the features of the non-\nmasked image blocks, and then adds the mask information to form the image\nfeatures [\n31].\n4 Experiments\nFirst, on the MNIST dataset, we use some representative dimensionality reduc-\ntion methods to project the data of this dataset onto a two-dimensional plane\nfor visualization. Secondly, we conduct image reconstruction experim ents on\nMNIST and ImageNet datasets. In this experiment, we also randomly mask the\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction 7\nFig. 2 The comparison of data visualization results on the MNIST set. (A) LPP. (B) LDA.\n(C) LLE. (D)PCA. (E) AE. (F) Transformer-DR.\noriginal image before reconstruction, so as to study the image reconstruc tion\nability of Transformer-DR model. Finally, we use the face dataset to d o face\nrecognition experiments after dimensionality reduction. In the ex periments,\nTransformer-DR performed very well.\n4.1 Data visualization\nIn this section, we conduct data visualization experiments on the MNI ST\nset. In the experiment, the data points are coded in 2-dimensional s ubspace\nwith some representative dimensionality reduction methods, inc luding PCA,\nLDA, LLE, LPP, AE and the proposed Transformer-DR. where, the net-\nwork structure of AE is 784-512-256-128-64-2, and the network structure of\nTransformer-DR is (392× 2)-(256× 2)-(128× 2)-(64× 2)-(32× 2)-(1× 2). The visu-\nalization results are shown in Fig.\n2, where the different colors are used to\nrepresent different digit categories. As can be seen, compared with PCA, LDA,\nLLE, LPP, AE and Transformer-DR has more discrimination. The distribution\neffects of Transformer-DR and AE are comparable.\n4.2 Image Reconstruction\nHere, we investigate the image reconstruction ability of Transformer- D R on\nthe MNIST and ImageNet datasets, and compare it with PCA and AE.\n4.2.1 On the MNIST dataset\nThe MNIST dataset contains 60,000 training images and 10,000 test images. In\nthe experiment, the size of the input original image is 28× 28. For Transformer-\nDR, the 2D images are needed to divide into several non-overlapping p atches\nSpringer Nature 2021 LATEX template\n8 Transformer-based dimensionality reduction\nFig. 3 Every three columns are a group. For each group, from left to right: reconstructions\nof PCA; reconstructions of AE; reconstructions of Transformer-DR; the original images from\nthe test dataset.\nbefore inputting into the encoder. So, in the network structure of Transformer-\nDR, the images are divided into 4 image blocks, each of which is 7 × 7 in\nsize. The network structure of AE is 784-512-256-128-64-32. To ensure that\nthe dimensions of each stage are equal to that of AE, the network structure of\nTransformer-DR is (196× 4)-(128× 4)-(64× 4)-(32× 4)-(16× 4)-(8× 4). Thus, the\noriginal images are all coded as 32-dimensional features. The experimental\nresults are shown in Fig.\n3.\nAs can be seen from Fig. 3, the reconstruction effects of Transformer-DR\nand AE are similar, but the reconstructed images of Transformer-DR are better\nthan AE in some details. Furthermore, the reconstructed images of both ar e\nsignificantly better than those of PCA. In theory, the more image blocks ar e\ndivided, the better the reconstruction of Transformer-DR, but in order to\nachieve the dimensionality reduction process equivalent to AE, we divide the\nimage blocks as above in this experiment.\nDue to the self-attention mechanism, Transformer can find correlation s\nbetween image patches. In this way, the transformer-DR can also recon struct\nthe original image after dimensionality reduction if the image patches con tain\nonly a small amount of useful information. To verify this, we randomly masked\n75% of the image patches after dividing the image into 16 patches. After thi s\nprocessing, the image contains very little useful information.\nThen, we use AE and Transformer-DR for dimensionality reduction, the\nnetwork structure used is the same as before, and the image is also dime nsion-\nally reduced to 32-dimensional features. The experimental results are shown\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction 9\nFig. 4 Every four columns are a group. For each group, from left to right: the masked\nimages (masked ratio 75%); reconstructions of AE; reconstructions of Transformer-DR; the\noriginal samples.\nin Fig.\n4. It can be seen from Fig. 4 that the transformer-DR can still recon-\nstruct the dimensionally reduced images, and the reconstruction eff ects are\nbetter than that of AE.\n4.2.2 On the ImageNet dataset\nThe ILSVRC-2012 ImageNet dataset [\n32] is larger and has more complex\nimages. We make the image reconstruction experiments on this dataset.\nThe network structure of AE and Transformer-DR is (70 × 70× 3)-(3136× 3)-\n(1568× 3)-(784× 3). For Transformer-DR, the images are divided into 14 × 14\nimage patches, each of size 5 × 5. The experimental results are shown in Fig.\n5, which shows that the reconstructed images of Transformer-DR are almos t\nthe same as the original images and can handle the detail pixels well, while the\nreconstructed images of AE is blurry. And because AE all use fully connec ted\nlayers, the training process will generate a large number of paramete rs, which\nmakes AE unsuitable for training larger-scale data sets.\nThen, the experiment to reconstruct the original images from the mask ed\nimages is also performed to explore the ability to extract robust featu res in\nmore complex images. The experimental environment and variables are th e\nsame as in the above experiments, the only change is that 75% of the patches\nare masked. The experimental results show that the reconstructed images of\nAE are so blurry that it is difficult for the eye to judge its original situ ation,\nwhile the reconstructed image of Transformer-DR can almost predict the mask\narea of the original image, see Fig.\n6.\nSpringer Nature 2021 LATEX template\n10 Transformer-based dimensionality reduction\nFig. 5 Every three columns are a group. For each group, from left to right: reconstructions\nof AE; reconstructions of Transformer-DR; the original images.\nFig. 6 Every four columns are a group. For each group, from left to right: the image\nis masked by 75%; reconstructions by AE; reconstructions by Transformer-DR; Random\nsamples from the test data set.\nFig. 7 shows the loss values of Transformer-DR and AE trained for 20\nepochs in this experiment. As can be seen from Fig. 7, the loss values for both\nmodels drop significantly within 5 epochs and then slowly drop off. Inthe end,,\nthe loss value of Transformer-DR is around 0.011, and the loss value of AE is\naround 0.017. Therefore, the training effect of Transformer-DR is better than\nthat of AE.\n4.3 Face Recognition\nIn this section, we will use the features after dimensionality red uction\nwith Transformer-DR for face recognition. In the experiment, besid es the\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction 11\nFig. 7 The loss value curves of both models at training stage.\nTransformer-DR model for face recognition, we also design another dimension-\nality reduction structure, Transformer-DRr, for face recognition, i t is trained\nwith two loss functions, one for image reconstruction and one for face recog-\nnition. Face Transformer [\n33] is used to compare with Transformer-DR and\nTransformer-DRr. Face Transformer is based on ViT, it introduces ViT to the\nfield of face recognition and gets good recognition results. In addition, th e\nViTs model, which is based on the ViT model and using the sliding win dow\nto generate overlapping image patches, is also used for face recognition. And\nthe performance of ViTs is slightly better than that of ViT, so our network\nis implemented based on ViTs. CosFace [\n34] is selected as the loss function to\nimprove the recognition performance.\nThe MS-Celeb-1M dataset [ 35] is used as the training dataset, which con-\ntains 100,000 celebrities and nearly 10 million face pictures. The trained models\nare validated on LFW [36], SLLFW [37], CALFW [38], CPLFW [39], TALFW\n[40], CFP-FP [41], AgeDB-30 [42] datasets. The experimental results are shown\nin Table 1. Before training and validation, the face images are aligned to 112\n× 112. The input dimensionality of Transformer-DR and Transformer-DRr\nis 37632 (112 × 112 × 3), after dimensionality reduction, the feature size is\n12544. ViT and ViTs do not reduce the dimensionality of the original images,\nso the original images are stretched to 37632 dimensions, and the features are\nstill 37632 dimensions after the processing of ViT and ViTs.\nThe face recognition results are shown in Table\n1. Where, the face recog-\nnition results of Face Transformer are quoted from Ref. [ 34]. The recognition\nrates of ViT and ViTs in the LFW dataset were 99.83% and 99.80%, respec-\ntively. As can be seen from Table\n1, the recognition rates of Transformer-DR\nand Transformer-DRr are slightly lower than that of ViT and ViTs, but they\nSpringer Nature 2021 LATEX template\n12 Transformer-based dimensionality reduction\nTable 1 The recognition accuracy\nMethod LFW SLLFW CALFW CPLFW TALFW CFP-FP AgeDB-30\nEigenFace 60.85\nLE 82+\nDFD 80.02 ± 0.5\nPCANet 86.28 ± 1.14\nTransformer-DR 99.53 ± 0.31 98.82 ± 0.47 95.33 ± 1.03 90.25 ± 1.09 63.40 ± 1.80 93.77 ± 1.25 96.37 ± 0.97\nTransformer-DRr 99.75 ± 0.25 98.90 ± 0.50 95.43 ± 1.11 90.23 ± 1.31 62.63 ± 1.31 93.64 ± 1.25 95.93 ± 0.88\nare still high and close to that of ViT and ViTs. Note that Transformer-DR and\nTransformer-DRr reduce the original images to 12544-dimensional features for\nface recognition, while ViT and ViTs do not reduce the dimensionality of t he\noriginal images and uses the 37256-dimensional features for face recognition.\nThis shows that, after Transformer-DR dimensionality reduction, although the\nrecognition rates have some slight losses, the high recognition rates can still be\nachieved. In other words, Face Transformer does not need such a high di men-\nsional data for face recognition, and the high-dimensional original images may\ncontain a lot of redundant information. And Transformer-DR can effectivel y\nremove some of the redundant data in the original images. The recognition\nratio of the other dimensionality reduction methods in the LFW dataset is\nas follows, EigenFace [\n43] is 60.85%, LE is 82%+ [ 44] , DFD is 80.02% ±\n0.5% [45], PCANet is 86.28% ± 1.14% [46], Their results are not as good as\nTransformer-DR.\nTransformer-DRr has a similar recognition results with Transformer- DR,\nbut it can not only perform face recognition but also image reconstruction. It\nis worth noting that the accuracy of transformer DRr on LFW dataset can\nreach 99.75% ± 0.25%, which shows that Transformer-DRr has reduced the\ndimension of the face image, but the loss of information is small, and its\nrecognition results are very close to those of ViT and ViT.\n5 Conclusion\nIn ViT model, the image is divided into many patches and some information\nof these patches may be ”redundant” or insignificant. Based the self-at tention\nmechanism of Transformer, we can find the correlation between patches an d\nthen remove redundant information and extract useful features. The n, a new\ndimensionality reduction (DR) model named Transformer-DR is proposed. The\nmodel consists of an encoder and a decoder, which are stacked by mult iple\nTransformer blocks. Based on the model, the original image can be encode d\ninto low dimensional code, thus achieving the goal of dimensionality reduction.\nWe do experiments from data visualization, image reconstruction and fac e\nrecognition, and the results show that Transformer DR method is supe rior to\nexisting data dimension reduction methods.\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction 13\nAcknowledgments. This work was supported by Science and Technol-\nogy Research Program of Chongqing Municipal Education Commission under\ngrant KJZD-K202100505, Chongqing Technology Innovation and Application\nDevelopment Project under Grant cstc2020jscx-msxmX0190, the National\nNatural Science Foundation of China under grant 61876026, the Science and\nTechnology Research Program of Chongqing Municipal Education Commisson\n(Grat No.KJQN202200551 and KJQN202100515).\nReferences\n[1] Gao, L., Song, J., Liu, X., Shao, J., Liu, J., Shao, J.: Learning in high-\ndimensional multimedia data: the state of the art. Multimedia Syste ms\n23(3), 303–313 (2017)\n[2] Bahrami, S., Shamsi, M.: A non-parametric approach for the activation\ndetection of block design fmri simulated data using self-organizing maps\nand support vector machine. Journal of medical signals and sensors 7(3),\n153 (2017)\n[3] Jindal, P., Kumar, D.: A review on dimensionality reduction tech niques.\nInternational Journal of Computer Applications 173(2), 42–46 (2017)\n[4] Verleysen, M., Fran¸ cois, D.: The curse of dimensionality in dat a mining\nand time series prediction. In: International Work-conference on Artificial\nNeural Networks, pp. 758–770 (2005). Springer\n[5] Hawkins, D.M.: The problem of overfitting. Journal of chemical infor ma-\ntion and computer sciences 44(1), 1–12 (2004)\n[6] Ayesha, S., Hanif, M.K., Talib, R.: Overview and comparative study of\ndimensionality reduction techniques for high dimensional data. Infor ma-\ntion Fusion 59, 44–58 (2020)\n[7] Anowar, F., Sadaoui, S., Selim, B.: Conceptual and empirical compari-\nson of dimensionality reduction algorithms (pca, kpca, lda, mds, svd, ll e,\nisomap, le, ica, t-sne). Computer Science Review 40, 100378 (2021)\n[8] Nanga, S., Bawah, A.T., Acquaye, B.A., Billa, M.-I., Baeta, F.D., Odai,\nN.A., Obeng, S.K., Nsiah, A.D.: Review of dimension reduction meth-\nods. Journal of Data Analysis and Information Processing 9(3), 189–231\n(2021)\n[9] Abdi, H., Williams, L.J.: Principal component analysis. Wiley inte rdisci-\nplinary reviews: computational statistics 2(4), 433–459 (2010)\nSpringer Nature 2021 LATEX template\n14 Transformer-based dimensionality reduction\n[10] Izenman, A.J.: Linear discriminant analysis. In: Modern Multivar iate\nStatistical Techniques, pp. 237–280. Springer, ??? (2013)\n[11] He, X., Niyogi, P.: Locality preserving projections. Advances in neu ral\ninformation processing systems 16 (2003)\n[12] Kruskal, J.B., Wish, M.: Multidimensional Scaling vol. 11. Sage, ?? ?\n(1978)\n[13] Lee, J.A., Lendasse, A., Verleysen, M.: Nonlinear projection with c urvi-\nlinear distances: Isomap versus curvilinear distance analysis. Neur ocom-\nputing 57, 49–76 (2004)\n[14] Franc, V., Hlav´ aˇ c, V.: Greedy kernel principal component analysis. In:\nCognitive Vision Systems, pp. 87–105. Springer, ??? (2006)\n[15] Belkin, M., Niyogi, P.: Laplacian eigenmaps for dimensionality reduct ion\nand data representation. Neural computation 15(6), 1373–1396 (2003)\n[16] Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by l ocally\nlinear embedding. science 290(5500), 2323–2326 (2000)\n[17] Van der Maaten, L., Hinton, G.: Visualizing data using t-sne. Journal of\nmachine learning research 9(11) (2008)\n[18] McInnes, L., Healy, J., Melville, J.: Umap: Uniform manifold approx-\nimation and projection for dimension reduction. arXiv preprint\narXiv:1802.03426 (2018)\n[19] Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionalit y of data\nwith neural networks. science 313(5786), 504–507 (2006)\n[20] Bengio, Y., Paiement, J.-f., Vincent, P., Delalleau, O., Roux, N., O uimet,\nM.: Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spe c-\ntral clustering. Advances in neural information processing systems 16\n(2003)\n[21] Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning repres entations\nby back-propagating errors. nature 323(6088), 533–536 (1986)\n[22] Vincent, P., Larochelle, H., Bengio, Y., Manzagol, P.-A.: Extracting and\ncomposing robust features with denoising autoencoders. In: Proce edings\nof the 25th International Conference on Machine Learning, pp. 1096–1103\n(2008)\n[23] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA.N., Kaiser, /suppress L., Polosukhin, I.: Attention is all you need. Advances in\nneural information processing systems 30 (2017)\nSpringer Nature 2021 LATEX template\nTransformer-based dimensionality reduction 15\n[24] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zh ai, X.,\nUnterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S ., et\nal.: An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929 (2020)\n[25] Zaremba, W., Sutskever, I., Vinyals, O.: Recurrent neural netw ork\nregularization. arXiv preprint arXiv:1409.2329 (2014)\n[26] Lin, Z., Feng, M., Santos, C.N.d., Yu, M., Xiang, B., Zhou, B., Ben-\ngio, Y.: A structured self-attentive sentence embedding. arXiv pre print\narXiv:1703.03130 (2017)\n[27] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image\nrecognition. In: Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 770–778 (2016)\n[28] Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv prepri nt\narXiv:1607.06450 (2016)\n[29] Melas-Kyriazi, L.: Do you even need attention? a stack of feed-forwar d\nlayers does surprisingly well on imagenet. arXiv preprint arXiv:2105.02723\n(2021)\n[30] Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T.,\nWang, X., Wang, G., Cai, J., et al.: Recent advances in convolutional\nneural networks. Pattern recognition 77, 354–377 (2018)\n[31] He, K., Chen, X., Xie, S., Li, Y., Doll´ ar, P., Girshick, R.: Masked autoen-\ncoders are scalable vision learners. In: Proceedings of the IEEE/CVFCon-\nference on Computer Vision and Pattern Recognition, pp. 16000–16009\n(2022)\n[32] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classific ation with\ndeep convolutional neural networks. Communications of the ACM 60(6),\n84–90 (2017)\n[33] Zhong, Y., Deng, W.: Face transformer for recognition. arXiv preprint\narXiv:2103.14803 (2021)\n[34] Wang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., Li, Z., Liu,\nW.: Cosface: Large margin cosine loss for deep face recognition. In:\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 5265–5274 (2018)\n[35] Guo, Y., Zhang, L., Hu, Y., He, X., Gao, J.: Ms-celeb-1m: A dataset and\nbenchmark for large-scale face recognition. In: European Conference on\nComputer Vision, pp. 87–102 (2016). Springer\nSpringer Nature 2021 LATEX template\n16 Transformer-based dimensionality reduction\n[36] Huang, G.B., Mattar, M., Berg, T., Learned-Miller, E.: Labeled faces\nin the wild: A database forstudying face recognition in unconstrained\nenvironments. In: Workshop on Faces in’Real-Life’Images: Detection,\nAlignment, and Recognition (2008)\n[37] Deng, W., Hu, J., Zhang, N., Chen, B., Guo, J.: Fine-grained face ver ifi-\ncation: Fglfw database, baselines, and human-dcmn partnership. Patter n\nRecognition 66, 63–73 (2017)\n[38] Zheng, T., Deng, W., Hu, J.: Cross-age lfw: A database for studying\ncross-age face recognition in unconstrained environments. arXiv prepri nt\narXiv:1708.08197 (2017)\n[39] Zheng, T., Deng, W.: Cross-pose lfw: A database for studying cross-\npose face recognition in unconstrained environments. Beijing Univer sity\nof Posts and Telecommunications, Tech. Rep 5, 7 (2018)\n[40] Zhong, Y., Deng, W.: Towards transferable adversarial attack against deep\nface recognition. CoRR abs/2004.05790 (2020)\nhttps://arxiv.org/abs/\n2004.05790\n[41] Sengupta, S., Chen, J.-C., Castillo, C., Patel, V.M., Chellappa, R., Jacobs,\nD.W.: Frontal to profile face verification in the wild. In: 2016 IEEE Winter\nConference on Applications of Computer Vision (WACV), pp. 1–9 (2016).\nIEEE\n[42] Moschoglou, S., Papaioannou, A., Sagonas, C., Deng, J., Kotsia, I.,\nZafeiriou, S.: Agedb: the first manually collected, in-the-wild age database.\nIn: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops, pp. 51–59 (2017)\n[43] Moghaddam, B., Wahid, W., Pentland, A.: Beyond eigenfaces: Prob-\nabilistic matching for face recognition. In: Proceedings Third IEEE\nInternational Conference on Automatic Face and Gesture Recognition,\npp. 30–35 (1998). IEEE\n[44] Masi, I., Wu, Y., Hassner, T., Natarajan, P.: Deep face recognition: A\nsurvey. In: 2018 31st SIBGRAPI Conference on Graphics, Patterns and\nImages (SIBGRAPI), pp. 471–478 (2018). IEEE\n[45] Lei, Z., Pietik¨ ainen, M., Li, S.Z.: Learning discriminant face d escriptor.\nIEEE Transactions on pattern analysis and machine intelligence 36(2),\n289–302 (2013)\n[46] Chan, T.-H., Jia, K., Gao, S., Lu, J., Zeng, Z., Ma, Y.: Pcanet: A sim-\nple deep learning baseline for image classification? IEEE transactions on\nimage processing 24(12), 5017–5032 (2015)"
}