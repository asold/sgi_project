{
  "title": "RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection",
  "url": "https://openalex.org/W4285601021",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100782322",
      "name": "Jinpeng Li",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5022104097",
      "name": "Haibo Jin",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5019092879",
      "name": "Shengcai Liao",
      "affiliations": [
        "Inception Institute of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A5082634513",
      "name": "Ling Shao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5032708386",
      "name": "Pheng‐Ann Heng",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology",
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2111372597",
    "https://openalex.org/W3199262087",
    "https://openalex.org/W2474575620",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2058961190",
    "https://openalex.org/W4285784163",
    "https://openalex.org/W3013328633",
    "https://openalex.org/W2985243484",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4309451541",
    "https://openalex.org/W2012885984",
    "https://openalex.org/W3109923889",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W2740020909",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W3034384783",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2799930024",
    "https://openalex.org/W2982772166",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3117707723",
    "https://openalex.org/W2963789946",
    "https://openalex.org/W2963598138"
  ],
  "abstract": "This paper presents a Refinement Pyramid Transformer (RePFormer) for robust facial landmark detection. Most facial landmark detectors focus on learning representative image features. However, these CNN-based feature representations are not robust enough to handle complex real-world scenarios due to ignoring the internal structure of landmarks, as well as the relations between landmarks and context. In this work, we formulate the facial landmark detection task as refining landmark queries along pyramid memories. Specifically, a pyramid transformer head (PTH) is introduced to build both homologous relations among landmarks and heterologous relations between landmarks and cross-scale contexts. Besides, a dynamic landmark refinement (DLR) module is designed to decompose the landmark regression into an end-to-end refinement procedure, where the dynamically aggregated queries are transformed to residual coordinates predictions. Extensive experimental results on four facial landmark detection benchmarks and their various subsets demonstrate the superior performance and high robustness of our framework.",
  "full_text": "RePFormer: Refinement Pyramid Transformer for Robust\nFacial Landmark Detection\nJinpeng Li1 , Haibo Jin2 , Shengcai Liao3∗ , Ling Shao4 and Pheng-Ann Heng1,5\n1Department of Computer Science and Engineering,\nThe Chinese University of Hong Kong, Hong Kong, China\n2Department of Computer Science and Engineering,\nThe Hong Kong University of Science and Technology, Hong Kong, China\n3Inception Institute of Artificial Intelligence (IIAI), UAE\n4Terminus Group, China\n5Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology,\nShenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China\njpli21@cse.cuhk.edu.hk, haibo.nick.jin@gmail.com, {scliao, ling.shao}@ieee.org,\npheng@cse.cuhk.edu.hk\nAbstract\nThis paper presents a Refinement Pyramid Trans-\nformer (RePFormer) for robust facial landmark de-\ntection. Most facial landmark detectors focus on\nlearning representative image features. However,\nthese CNN-based feature representations are not\nrobust enough to handle complex real-world sce-\nnarios due to ignoring the internal structure of land-\nmarks, as well as the relations between landmarks\nand context. In this work, we formulate the fa-\ncial landmark detection task as refining landmark\nqueries along pyramid memories. Specifically, a\npyramid transformer head (PTH) is introduced to\nbuild both homologous relations among landmarks\nand heterologous relations between landmarks and\ncross-scale contexts. Besides, a dynamic landmark\nrefinement (DLR) module is designed to decom-\npose the landmark regression into an end-to-end\nrefinement procedure, where the dynamically ag-\ngregated queries are transformed to residual coor-\ndinates predictions. Extensive experimental results\non four facial landmark detection benchmarks and\ntheir various subsets demonstrate the superior per-\nformance and high robustness of our framework.\n1 Introduction\nFacial landmark detection aims to localize a set of predefined\nkey points on 2D facial images. This task has attracted sig-\nnificant attention due to its wide range of applications [Ko,\n2018]. With the remarkable success of convolutional neural\nnetworks (CNNs), modern facial landmark detectors [Wang\net al., 2019a; Qianet al., 2019; Jinet al., 2021] have achieved\nencouraging performance in constrained environments. How-\never, as fundamental components in real-world facial applica-\ntions, facial landmark detectors need to be able to consistently\n∗Shengcai Liao is the corresponding author.\nFigure 1: Facial landmark detection results on WFLW. Yellow ar-\nrows point to the wrong results. (a) Ground truth. (b) Results of\nregression-based method. (c) Results of heatmap-based method. (d)\nOur results. Methods (b) and (c) ignore the internal structures of\nlandmarks, while our method explores the internal structure of land-\nmarks and builds the relations between landmarks and cross-scale\ncontexts, thereby achieving more robust detection results.\ngenerate robust results for complex situations, which remains\na significant challenge for existing methods. Figure 1 illus-\ntrates some examples of challenging face images and the de-\ntection results of existing algorithms and our method.\nAccording to the ways of generating landmark co-\nordinates from feature maps, previous works can be\ngenerally classified into three categories: heatmap-based\nmethods, regression-based methods, and hybrid meth-\nods. Heatmap-based methods [Wang et al. , 2019a;\nDong and Yang, 2019 ] treat facial landmark detection as\na segmentation task, where pixels belong to the classes\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1088\nof landmarks and background. These methods mainly\nface two difficulties: 1) To reduce the quantization error\nin CNN backbones, they usually leverage single-scale\nhigh-resolution heatmaps [Ronneberger et al., 2015 ] to\nrepresent the surrogate results of landmarks, which intro-\nduces high computational costs and prevents them from\nfully exploring the pyramid features. 2) They lack global\nconstraints on facial landmarks and ignore the relationships\nbetween image contexts and landmarks, which decreases\ntheir robustness to complex scenes. Regression-based\nmethods [Lv et al., 2017] directly transform image features\ninto landmark coordinates with high efficiency. However,\na single regression step is difficult to achieve satisfactory\nperformance. Cascaded regression steps [Lv et al., 2017;\nFeng et al., 2018] are leveraged to refine results, but they are\nnot fully end-to-end trainable and their performance is prone\nto saturation. Recently, Sun et al.[Sun et al., 2018] combined\nheatmap and regression together to inherit the advantages\nof both. However, their network designs are imbalanced,\nwith most of the computational costs concentrated on the\nbackbone. Tan et al. [Tan et al., 2020] demonstrated that\nimbalanced network architectures can limit the detection\nperformance. Moreover, existing detectors often focus on\ndirectly learning the landmark representation from pure\nimage features, and ignore the dynamics and interactivity of\nlandmarks and context information.\nTo address above limitations, we propose a novel facial\nlandmark detector named the Refinement Pyramid Trans-\nformer (RePFormer) which focuses on robust detection for\ncomplex scenarios. We treat facial landmarks as attention\nquires, and formulate the facial landmark detection as a task\nof refining landmark queries through pyramid memories in\nan end-to-end trainable procedure. Specifically, we propose\na pyramid transformer head (PTH) to mimic the zooming-in\nmechanism. In PTH, cross-scale attention constructs pyra-\nmid memories to combine long-range context and propa-\ngate high-level semantic information into lower levels. Then,\nlandmark-to-landmark and landmark-to-memory attentions\nare employed in each PTH stage to enable dynamic interac-\ntions of landmarks and pyramid memories, which helps to\nregress the landmarks in various scenarios. Besides, we de-\nsign a dynamic landmark refinement (DLR) method to refine\nthe landmark queries by predicting residual coordinates and\ndynamically aggregating queries in an end-to-end trainable\nway. The residual coordinates prediction decomposes the re-\ngression into multiple steps, and each step refines the land-\nmark queries based on a specific level in pyramid memories,\nwhich fully leverages the multi-level information in pyramid\nmemories and decreases the difficulty of prediction. We con-\nduct extensive experiments on several facial landmark detec-\ntion benchmarks. Our models achieve state-of-the-art perfor-\nmance and show strong robustness on complex scenarios.\n2 Related Work\nFacial Landmark Detection. Early facial landmark detec-\ntion methods mainly focus on deforming a statistical facial\nlandmark model into a 2D image by an optimization pro-\ncedure. These methods typically apply different constraints\non the statistical models, such as object shapes and tex-\nture features [Wu and Ji, 2019 ]. However, they are not ro-\nbust under various scenarios in the wild. Recently, deep\nneural network (DNN) based methods [Wang et al., 2019a;\nJin et al., 2021] show promising performance on this task.\nTheir architectures are usually composed of a CNN backbone\nand a variant of a heatmap-based or/and regression-based de-\ntection head. The pixel values of heatmaps represent the\nlikelihood of landmarks exiting in the corresponding posi-\ntions [Wang et al., 2019a; Dong and Yang, 2019]. A draw-\nback of them is that the down-sampling ratio of heatmaps\nintroduces quantization error. Thus, high-resolution net-\nworks, such as U-Net [Ronneberger et al., 2015] and HR-\nNet [Wang et al., 2019a], are commonly applied to address\nthis difficulty. Instead of utilizing heatmaps as surrogate\nresults, regression-based methods [Lv et al., 2017] directly\npredict facial landmarks by transforming image features into\n2D coordinates. Cascaded regression steps [Lv et al., 2017;\nFeng et al., 2018] are widely applied to further refine the pre-\ndicted coordinates. They usually follow an iterative cropping-\nand-regression pipeline which first crops patches from CNN\nfeature maps and then sends them to carefully designed re-\ngressors, such as Recurrent Neural Network (RNN) [Trigeor-\ngis et al., 2016] or Graph Convolutional Network (GCN) [Li\net al., 2020]. However, extracting patches limits the long-\nrange information exchange and may hinder the end-to-end\ntraining due to the non-differentiable cropping operators.\nVision Transformer. Transformer [Vaswani et al., 2017] is\nfirst proposed for sequence-to-sequence tasks in natural lan-\nguage processing. Its network architecture simply consists\nof self-attention based encoders and decoders to model de-\npendencies among any positions in a sequence. Recently,\ncomputer vision, from high-level tasks to low-level tasks, has\nalso benefited from its strong representation capability and\nlong-range interactions [Carion et al., 2020 ]. The Vision\nTransformer (ViT) [Dosovitskiy et al., 2021 ] directly flat-\ntens image patches and processes their feature and position\nembeddings using a pure transformer encoder for the image\nclassification task. DETR [Carion et al., 2020] introduces a\ntransformer-based detector to solve the object detection prob-\nlem as unordered set prediction. Its fully end-to-end design\nremoves the hand-crafted components, such as anchor assign-\nment and duplication reduction, in modern object detectors.\nTransformers have also been explored for landmark detection\ntasks. Yang et al.[Yanget al., 2020] leverage a transformer as\na non-local module to build long-range spatial dependencies\nin image features for more explainable human pose estima-\ntion. In contrast, the transformer in our model explicitly es-\ntablishes the relations between landmarks to landmarks, and\nlandmarks to image contexts, and gradually refines the land-\nmark queries along the pyramid transformer memories.\n3 Method\n3.1 Overview\nProblem Formulation. Given an input 2D imageI, our ReP-\nFormer aims at learning a facial landmark detector fθ(I) to\npredict an ordered set L ∈RN×D = {l1, ..., lN }which rep-\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1089\nFigure 2: Overview of RePFormer. LL and LM are the landmark-to-landmark attention and landmark-to-memory attention, respectively. DA\nqueries are dynamically aggregated queries. Blue lines represent feature maps. Green lines are the embeddings of memories and landmark\nqueries. Red lines represent coordinates and residual predictions of facial landmarks.\nresents N ordered facial landmarks, where each landmark li\nis in D-dimensional coordinates. Note that the transformer-\nbased object detector, DETR [Carion et al., 2020], also for-\nmulates general object detection as a set prediction task.\nHowever, the predicted set of DETR is unordered and with\nvariable length, since the sequence and number of objects are\nnot fixed in general object detection datasets. In contrast, the\nfacial landmark detection task assumes that each image only\ncontains one main face (if there are several faces in an image,\nonly the largest central one is considered), which is annotated\nwith a fixed number of landmarks. Besides, instead of solv-\ning a bipartite matching as in DETR, the label assignment in\nour framework is simply defined as a fixed one-to-one way.\nOverall Architecture. The architecture of our RePFormer\nis illustrated in Fig. 2. It is built upon a CNN-based feature\nextractor and a transformer-based detection head. First, the\ninput image is fed into a common CNN backbone which con-\ntains several network stages. The feature maps generated by\nthese stages are of various resolutions and semantics, form-\ning hierarchical image features. Next, our PTH module ap-\nplies cross-scale attentions on these hierarchical feature maps\nto produce pyramid memories with multi-level semantic in-\nformation. Then, the PTH stages use landmark-to-landmark\nand landmark-to-memory attentions to fuse the context infor-\nmation of memories into landmark queries and model the re-\nlations among landmarks. After each PTH stage, our DLR\nmodule performs mutual updates by predicting the resid-\nual coordinates of landmarks based on the status of current\nqueries and evolving the landmark queries based on the co-\nordinates of current landmarks. Thus, the whole facial land-\nmark detection task is formulated as a step-by-step landmark\nrefinement process.\n3.2 Pyramid Transformer Head\nCross-Scale Attention. Feature pyramids have been ex-\nplored for solving various challenges in computer vision,\nsuch as detecting objects at various scales[Li et al., 2019] and\ncombining different levels of semantic features for segmenta-\ntion [Ronneberger et al., 2015]. In this work, our transformer-\nbased detection head, PTH, explicitly employs pyramid fea-\ntures to obtain cross-scale semantic information, mimicking\nthe zooming-in mechanism of human annotators. Our cross-\nscale attention takes the hierarchical feature set Z as input,\nand generates the pyramid memory set V following a top-\ndown pathway. Z is obtained by a CNN backbone fB which\nis defined as follows:\nfB(I) = fM\nB (...fi\nB(...f1\nB(I))), (1)\nwhere M is the number of network stages, and fi\nB is the ith\nnetwork stage to take the feature maps zi−1 as input and gen-\nerate feature maps zi as follows:\nzi = fi\nB(zi−1). (2)\nThese hierarchical features form the CNN feature set Z =\n{zM , ..., z1}, which contains multi-level feature maps with\nvarious resolutions and semantics. Top-level features contain\nstrong semantics but less spatial information, making them\nonly suitable for robustly locating the rough positions of land-\nmarks. To better propagate high-level semantics into all lev-\nels of features and combine long-range information, our PTH\ncomputes the ith memory vi by applying a cross-scale at-\ntention on every two adjacent levels of feature maps zi and\nmemories vi+1, which is inspired by the top-down Ground-\ning Transformer in [Zhang et al., 2020]. The architecture of\ncross-scale attention is shown in Fig. 3(a), which takes three\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1090\n(a) (b)\nFigure 3: (a) Architecture of Cross-scale attention. (b) Architectures and data flow of LL and LM attentions.\ninputs including query qi, key ki and value ˆvi. Since the at-\ntention operation is permutation-invariant to input elements,\nqi is composed of zi and its fixed positional encodings pi by\nqi = zi + pi to maintain relative positions of pixels. ki and\nˆvi are both obtained by fusing the information ofzi and vi+1.\nA bilinear interpolation upsamples vi+1 into ¯vi+1 with the\nsame resolution as zi. Then ˆvi is generated by a Hadamard\nproduct as ˆvi = zi ◦¯vi+1, where ¯vi+1 aims to mask output\nthe noisy low-level details outside semantic areas and main-\ntains the precise spatial information within semantic areas.\nTo make the similarity calculation between qi and ki seman-\ntic and position sensitive, ki is also supplemented with fixed\npositional encodings as ki = ˆvi + pi. In summary, the mem-\nory vi is computed as\nˆzi = LN(zi + MHA(qi, ki, ˆvi)) (3)\n\u001avi = LN( ˆzi + FFN( ˆzi)), if i < M\nvi = zi, otherwise, (4)\nwhere MHA is a multi-head attention [Vaswani et al., 2017],\nFFN is a feed forward network, and LN is a layer normal-\nization. Thus, the pyramid memories form the set V =\n{vM , ..., v1}, where each vi has the same resolution as zi but\nwith cross-level and long-range information.\nPTH Forwarding. Our PTH detection head fH is composed\nof cascaded transformer stages, and defined as follows:\nfH(V, E1) = fK\nH (vM−K+1, ...fi\nH(vM−i+1, ...f1\nH(vM , E1))),\n(5)\nwhere K is the number of transformer stages andfi\nH is the ith\nstage. PTH introduces the landmark query set E ∈RN×D =\n{e1, ..., eN }to store the states of landmarks, the size of which\nis the same as the number of landmarks. E can either be ini-\ntialized from trainable embeddings as in DETR [Carion et\nal., 2020], or dynamiclly aggregated from memories as in our\nmethod, which will be introduced in next section. PTH takes\nthe pyramid and initial landmark query set E1 as inputs and\nrepeatedly updates the states of E based on the information\nfrom pyramid memories V , in a top-down pathway. This en-\nables it to first robustly locate the rough positions of land-\nmarks in the top-level memory, and then gradually integrates\nmore and more precise spatial information from lower levels\nof memories. Specifically, fi\nH is the ith PTH stage to take\nthe memory vM−i+1 and the landmark query setEi in the ith\nstate as inputs and generate the new query state Ei+1 for the\nnext PTH stage. This is computed by\nEi+1 = fi\nH(vM−i+1, Ei)\n= gLM (gLL(Ei), vM−i+1)\n(6)\nwhere each PTH stage is composed of two successive at-\ntentions, i.e. a landmark-to-landmark attention gLL and a\nlandmark-to-memory attention gLM as shown in Fig. 3(b),\nwhich is inspired by the transformer decoders in [Vaswani et\nal., 2017; Carion et al., 2020]. They are both implemented\nwith multi-head attentions. The query, key and value of gLL\nall come from Ei, and the query, key and value of gLM are\nthe output of gLL, vM−i+1 and vM−i+1, respectively. gLL\nmodels the dynamic relations among landmark queries using\ntheir similarities, and updates their status. gLM computes the\nrelations among landmarks and pyramid memories to endow\ncross-scale information and long-range image context into\nlandmark queries. These dynamic relations are built on the\nfly and specific to each image, thus they are more robust in\ndetecting facial landmarks without salient visual features.\n3.3 Dynamic Landmark Refinement\nOur model directly predicts the coordinates of landmarks, so\nit can be regarded as a regression-based method. Compared\nto heatmap-based detectors, a single regression step is hard to\ngenerate competitive results. Thus, multiple cropping-then-\ndetection steps [Trigeorgis et al., 2016] are used to improve\nthe performance of regression-based methods in a coarse-to-\nfine way. However, existing multi-step methods usually have\ntwo drawbacks: 1) The hard cropping operator is not differen-\ntiable with respect to the input coordinates. 2) The detection\nis performed only based on the cropped features, without ac-\ncess to the image context information. To address these chal-\nlenges, our RePformer introduces the DLR module to modu-\nlate facial landmark detection into a fully end-to-end refine-\nment process using residual coordinates predictions and dy-\nnamically aggregated queries.\nResidual Coordinates Prediction. The initial facial land-\nmark set L0 is predicted based on the highest-level mem-\nory vM . Then, our DLR appends additional predictors to\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1091\nWFLWMethod Backbone Pose Expr. Illu. M.u. Occ. Blur Full 300W COFW AFLW\nDVLN [Wu and Yang, 2017] VGG-16 11.54 6.78 5.73 5.98 7.33 6.88 6.08 4.66 - -\nLAB [Wu et al., 2018] ResNet-18 10.24 5.51 5.23 5.15 6.79 6.32 5.27 3.49 5.58 1.85\nWing [Feng et al., 2018] ResNet-50 8.43 5.21 4.88 5.26 6.21 5.81 4.99 - 5.07 1.47\nDeCaFA [Dapogny et al., 2019] Cascaded U-net - - - - - - 5.01 3.69 - -\nHRNet [Wang et al., 2019a] HRNetV2-W18 7.94 4.85 4.55 4.29 5.44 5.42 4.60 3.32 3.45 1.57\nA VS[Qian et al., 2019] ResNet-18 9.10 5.83 4.93 5.47 6.26 5.86 5.25 4.54 - -\nA VS w/ LAB[Qian et al., 2019] Hourglass 8.21 5.14 4.51 5.00 5.76 5.43 4.76 4.83 - -\nA VS w/ SAN[Qian et al., 2019] - 8.42 4.68 4.24 4.37 5.60 4.86 4.39 3.86 - -\nAWing[Wang et al., 2019b] Hourglass 7.38 4.58 4.32 4.27 5.19 4.96 4.36 3.07 - -\nSTYLE [Qian et al., 2019] ResNet-18 8.42 4.68 4.24 4.37 5.60 4.86 4.39 4.54 - -\nLUVLi [Kumar et al., 2020] DU-Net - - - - - - 4.37 3.23 - -\nDAG [Li et al., 2020] HRNet-W18 7.36 4.49 4.12 4.05 4.98 4.82 4.21 3.04 - -\nPIPNet [Jin et al., 2021] ResNet-18 8.02 4.73 4.39 4.38 5.66 5.25 4.57 3.36 3.31 1.48\nPIPNet [Jin et al., 2021] ResNet-50 7.98 4.54 4.35 4.27 5.65 5.19 4.48 3.24 3.18 1.44\nPIPNet [Jin et al., 2021] ResNet-101 7.51 4.44 4.19 4.02 5.36 5.02 4.31 3.19 3.08 1.42\nRePFormer (ours) ResNet-18 7.38 4.28 4.06 4.04 5.17 4.86 4.20 3.07 3.07 1.44\nRePFormer (ours) ResNet-50 7.31 4.25 4.09 3.94 5.15 4.82 4.14 3.03 3.01 1.43\nRePFormer (ours) ResNet-101 7.25 4.22 4.04 3.91 5.11 4.76 4.11 3.01 3.02 1.43\nTable 1: Benchmarking results of state-of-the-art methods and our models on the WFLW including the full set and six subsets, 300W, COFW\nand AFLW datasets. The best and second best results are marked in colors of red and blue, respectively.\nhigher PTH stages, and the ith predictor fi\nP only needs to\npredict the residual coordinates of landmarks Ui ∈RN×D =\n{ui\n1, ..., ui\nN }with respect to Li−1, as follows:\nui\nj = fi\nP (ei\nj), (7)\nwhere ui\nj are the residual coordinates of the jth landmark in\nthe ith stage, and ei\nj is the jth landmark query in theith stage.\nThe predictor fi\nP is a two-layer FFN which is agnostic to dif-\nferent landmarks, because it only needs to predict the residual\nvalues instead of the absolute coordinates. And the ith land-\nmark set Li is computed by:\nLi = {li−1\n1 + ui\n1, ..., li−1\nN + ui\nN }. (8)\nL1 loss is used as the loss function between the ground-truths\nand Li generated by the ith PTH stage. Thus, with this step-\nby-step refinement, our DLR gradually pushes the regressed\nlandmarks closer to the ground-truth coordinates.\nDynamically Aggregated Queries. To adapt the transformer\narchitecture to the residual prediction task, the landmark\nquery set Ei, as the inputs of the ith PTH stage, need to rep-\nresent the status of the current results Li−1 using their se-\nmantic and spatial information. To solve this challenge in an\nend-to-end way, a dynamic aggregation method is proposed\nto extract landmark queries by aggregating pyramid memo-\nries weighted by their relative positional information. This\ncan be seen as a soft version of the ”cropping” operator, and\nthe jth query for stage i is computed by as follows\nei\nj =\nX\nk∈Ω\nsijk ·vk\nM−i+1, (9)\nwhere k is a pixel’s memory index, Ω is the domain of all\nindexes, and vk\nM−i+1 is the kth memory pixel in the stage\nM −i + 1. The normalized similarity sijk between query\nei−1\nj and memory pixel vk\nM−i+1 is represented by\nsijk =\nexp(−∥li−1\nj −ck∥·τ)\nP\nˆk∈Ω exp(−∥li−1\nj −cˆk∥·τ), (10)\nwhere ∥.∥is the squared L2 norm, andck represents the coor-\ndinates of memory pixel vk\nM−i+1. τ is a temperature param-\neter which is bigger than 1 to amplify the differences in co-\nordinates. Compared with hard cropping, dynamically aggre-\ngated queries are differentiable to coordinates of landmarks,\nmaking the entire multi-step refinement procedure fully end-\nto-end trainable. Besides, these queries are composed of the\nwhole memory to explore both the position sensitive semantic\nfeatures and global image context information.\n4 Experiments\n4.1 Implementation Details and Datasets\nWe use ResNet [He et al., 2016 ] pretrained on Ima-\ngeNet [Deng et al., 2009] as the backbone of RePFormer, and\nthe default depth of backbone is 18 unless otherwise speci-\nfied. The Adam optimizer without weight decay is used to\ntrain our models, β1 and β2 are set to 0.9 and 0.999, respec-\ntively. All models are trained for 360 epochs with a batch\nsize of 16. The initial learning rate is 0.0001, which is de-\ncayed by a factor of 10 after 200 epochs. The temperature τ\nis set to 1000. The L1 loss is used as the loss function for\nall outputs, and the loss weights are simply set to 1. We con-\nduct experiments on four popular facial landmark detection\ndatasets including WFLW [Wu et al., 2018], 300W [Sago-\nnas et al., 2013], AFLW-Full [Koestinger et al., 2011], and\nCOFW [Burgos-Artizzu et al., 2013]. Most of our settings\nfollow PIPNet [Jin et al., 2021]. All input images are resized\nto 256x256.\n4.2 Comparison with State-of-the-art Approaches\nWe compare RePFormer with the state-of-the-art facial land-\nmark detection methods using the evaluation metric of nor-\nmalized mean error (NME).\nWFLW. Table 1 shows the performance of state-of-the-art\nmethods and our RePFormer models with three backbones\nincluding ResNet-18, ResNet-50 and ResNet-101. Thanks\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1092\nModules 300W\nPTH DLR Full Common Challenge\n3.28 2.92\n4.99√ 3.23 2.88\n4.93√ 3.22 2.92\n4.85√ √ 3.07 2.72\n4.69\nTable 2: Comparative results on 300W by using different compo-\nnents in RePFormer.\nto our effective detection head, on the full set of WFLW,\nour model with the lightweight ResNet-18 backbone al-\nready achieves better performance (4.20% NME) than all\nexisting methods including those with much heavier back-\nbones, such as Cascaded U-net and Hourglass. With ResNet-\n50 and ResNet-101, RePFormer further improves results to\n4.14% and 4.11% NME, outperforming the most competitive\nmethod, DAG [Li et al., 2020], by 1.7% and 2.4%, respec-\ntively. We also conduct experiments on six subsets of WFLW.\nOur models outperform all previous methods on five subsets,\nand achieve all the best and second best results, demonstrat-\ning the strong performance and robustness of our framework\nin various evaluation scenarios.\n300W. The third-to-last column of Table 1 compares the per-\nformance of our models and state-of-the-art methods on the\nfull set of 300W. All the reported results are normalized by\nthe inter-ocular distances. Our RePFormers with ResNet-\n101 and ResNet-50 backbones achieve the best and second-\nbest results, respectively. Only with the lightweight back-\nbone of ResNet-18, our method already significantly outper-\nforms most of the state-of-the-art methods, such as LAB[Wu\net al., 2018], STYLE [Qian et al., 2019], HRNet [Wang et\nal., 2019a ], and PIPNet [Jin et al., 2021 ]. Note that al-\nthough RePFormer has a deeper detection head than detec-\ntors with specially tailored head for time efficiency, we ar-\ngue that their architectures of large backbones with small de-\ntection heads are not optimal, and our RePFormer head can\nachieve better performance gain than using heavier backbone.\nTan et al. [Tan et al., 2020] also showed that balanced ar-\nchitectures can yield higher accuracy-speed ratio. For exam-\nple, compared to the competitive method, PIPNet [Jin et al.,\n2021], our RePFormer with ResNet-18 outperforms PIPNet\nwith ResNet-101 by 3.8% with similar inference speed (56\nFPS vs. 59 FPS).\nCOFW. We compare our methods with the state-of-the-art\nworks on the COFW dataset under the intra-data setting\nwhich is shown in the second-to-last column of Table 1. The\ninter-ocular distances are used to normalize the results. Our\nmodels with three different backbones achieve the top-3 per-\nformances among all methods.\nAFLW. Comparative results of our models and state-of-the-\nart results are shown in the last column of Table 1. Following\nprevious works, all landmark coordinates are normalized by\nthe image size. As can be seen that our RePFormers with dif-\nferent backbones achieve the second-best and third-best re-\nsults, while only PIPNet [Jin et al., 2021] with ResNet-101\nslightly outperforms us (1.42% NME vs. 1.43% NME).\nτ 10 100 1000 10000\nNME(%) 3.22 3.16 3.07 3.16\nTable 3: Performance of different τ on the full set of 300W.\n4.3 Ablation Studies\nRePFormer Components. Table 2 shows the peformance of\nRePFormer with different componets. First, we construct a\nstrong baseline model by appending a three-stage transformer\ndetection head on a CNN backbone. Note that only the fea-\nture maps from the last stage of the CNN are used as the mem-\nory for the transformer, and landmark embeddings are fixed\nafter training. Thanks to the long-range information model-\ning of transformer, our baseline model already achieves com-\npetitive performance compared to the state-of-the-art meth-\nods. Secondly, we apply PTH module on the baseline model.\nWith the help of PTH, the performance of our model is im-\nproved on all sets, which demonstrates the effectiveness of\npyramid memories with cross-scale information. Thirdly, we\nintegrate DLR into our baseline model. From Table 2, we ob-\nserve that DLR significantly improves the performance on the\nchallenge subset. Finally, as shown in the last row of Table 2,\nour RePFormer with PTH and DLR modules achieves large\nperformance improvements over the baseline on all subsets.\nNote that, compared to the baseline, the performance gain of\nRePFormer is even larger than the sum of improvements of\nall individual components, which demonstrates that the two\nproposed components are complementary to each other and\ncan work together to promote more accurate regression.\nTemperature Values. We evaluate the performance of ReP-\nFormer with different temperature values on the full set of\n300W. τ controls the weight map for dynamically aggregated\nqueries. As shown in Table 3, the performance is continu-\nously improved as the τ value increases from 10 to 1000, but\nan excessively large τ value decreases the performance. The\nresults indicate that dynamic landmark queries mainly aggre-\ngate information from nearby pixels, while long-range infor-\nmation is also valuable for generating accurate results.\n5 Conclusion\nIn this paper, we present a refinement pyramid transformer,\nRePFormer, for facial landmark detection. The proposed\nPTH utilizes a cross-scale attention to generate pyramid\nmemories containing multi-level semantic and spatial infor-\nmation. Landmark-to-landmark and landmark-to-memory at-\ntentions are employed in each PTH stage to fuse memory in-\nformation into landmark queries and model long-range re-\nlationships between landmarks. A DLR module is intro-\nduced to solve the task of landmark regression by a fully\nend-to-end multi-step refinement procedure where the land-\nmark queries are gradually refined by a dynamic aggregation\nmethod through top-down pyramid memories.\nAcknowledgments\nThis work is supported by Hong Kong Research Grants Coun-\ncil under General Research Fund Project No. 14201620.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1093\nReferences\n[Burgos-Artizzu et al., 2013] Xavier P. Burgos-Artizzu,\nPietro Perona, and Piotr Doll ´ar. Robust face landmark\nestimation under occlusion. In ICCV, 2013.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with\ntransformers. In ECCV, 2020.\n[Dapogny et al., 2019] Arnaud Dapogny, Kevin Bailly, and\nMatthieu Cord. Decafa: Deep convolutional cascade for\nface alignment in the wild. In ICCV, 2019.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009.\n[Dong and Yang, 2019] Xuanyi Dong and Yi Yang. Teacher\nsupervises students how to learn from partially labeled im-\nages for facial landmark detection. In ICCV, 2019.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021.\n[Feng et al., 2018] Zhen-Hua Feng, Josef Kittler, Muham-\nmad Awais, Patrik Huber, and Xiao-Jun Wu. Wing loss\nfor robust facial landmark localisation with convolutional\nneural networks. In CVPR, 2018.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\n[Jin et al., 2021] Haibo Jin, Shengcai Liao, and Ling Shao.\nPixel-in-pixel net: Towards efficient facial landmark de-\ntection in the wild. Int. J. Comput. Vis., 2021.\n[Ko, 2018] ByoungChul Ko. A brief review of facial emotion\nrecognition based on visual information. Sensors, 2018.\n[Koestinger et al., 2011] Martin Koestinger, Paul Wohlhart,\nPeter M. Roth, and Horst Bischof. Annotated facial land-\nmarks in the wild: A large-scale, real-world database for\nfacial landmark localization. InWorkshop on BeFIT, 2011.\n[Kumar et al., 2020] Abhinav Kumar, Tim K. Marks, Wenx-\nuan Mou, Ye Wang, Michael Jones, Anoop Cherian, Toshi-\naki Koike-Akino, Xiaoming Liu, and Chen Feng. Luvli\nface alignment: Estimating landmarks’ location, uncer-\ntainty, and visibility likelihood. In CVPR, 2020.\n[Li et al., 2019] Xiaohan Li, Taotao Lai, Shuaiyu Wang,\nQuan Chen, Changcai Yang, Riqing Chen, Jinxun Lin, and\nFu Zheng. Weighted feature pyramid networks for ob-\nject detection. In ISPA/BDCloud/SocialCom/SustainCom,\n2019.\n[Li et al., 2020] Weijian Li, Yuhang Lu, Kang Zheng, Haofu\nLiao, Chihung Lin, Jiebo Luo, Chi-Tung Cheng, Jing\nXiao, Le Lu, Chang-Fu Kuo, and Shun Miao. Structured\nlandmark detection via topology-adapting deep graph\nlearning. In ECCV, 2020.\n[Lv et al., 2017] Jiangjing Lv, Xiaohu Shao, Junliang Xing,\nCheng Cheng, and Xi Zhou. A deep regression architec-\nture with two-stage re-initialization for high performance\nfacial landmark detection. In CVPR, 2017.\n[Qian et al., 2019] Shengju Qian, Keqiang Sun, Wayne Wu,\nChen Qian, and Jiaya Jia. Aggregation via separation:\nBoosting facial landmark detector with semi-supervised\nstyle translation. In ICCV, 2019.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In MICCAI, 2015.\n[Sagonas et al., 2013] Christos Sagonas, Georgios Tz-\nimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300\nfaces in-the-wild challenge: The first facial landmark\nlocalization challenge. In ICCV Workshops, 2013.\n[Sun et al., 2018] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang\nLiang, and Yichen Wei. Integral human pose regression.\nIn ECCV, 2018.\n[Tan et al., 2020] Mingxing Tan, Ruoming Pang, and\nQuoc V . Le. Efficientdet: Scalable and efficient object de-\ntection. In CVPR, 2020.\n[Trigeorgis et al., 2016] George Trigeorgis, Patrick Snape,\nMihalis A. Nicolaou, Epameinondas Antonakos, and Ste-\nfanos Zafeiriou. Mnemonic descent method: A recurrent\nprocess applied for end-to-end face alignment. In CVPR,\n2016.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, 2017.\n[Wang et al., 2019a] Jingdong Wang, Ke Sun, Tianheng\nCheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong\nLiu, Yadong Mu, Mingkui Tan, Xinggang Wang, Wenyu\nLiu, and Bin Xiao. Deep high-resolution representation\nlearning for visual recognition. IEEE Trans. Pattern Anal.\nMach. Intell., 2019.\n[Wang et al., 2019b] Xinyao Wang, Liefeng Bo, and\nLi Fuxin. Adaptive wing loss for robust face alignment\nvia heatmap regression. In ICCV, 2019.\n[Wu and Ji, 2019] Yue Wu and Qiang Ji. Facial landmark\ndetection: A literature survey. Int. J. Comput. Vis., 2019.\n[Wu and Yang, 2017] Wenyan Wu and Shuo Yang. Leverag-\ning intra and inter-dataset variations forrobust face align-\nment. In CVPRW, 2017.\n[Wu et al., 2018] Wenyan (Wayne) Wu, Chen Qian, Shuo\nYang, Quan Wang, Yici Cai, and Qiang Zhou. Look at\nboundary: A boundary-aware face alignment algorithm.\nIn CVPR, 2018.\n[Yang et al., 2020] Sen Yang, Zhibin Quan, Mu Nie, and\nWankou Yang. Transpose: Towards explainable human\npose estimation by transformer. CoRR, 2020.\n[Zhang et al., 2020] Dong Zhang, Hanwang Zhang, Jinhui\nTang, Meng Wang, Xiansheng Hua, and Qianru Sun. Fea-\nture pyramid transformer. In ECCV, 2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1094",
  "topic": "Landmark",
  "concepts": [
    {
      "name": "Landmark",
      "score": 0.9653865098953247
    },
    {
      "name": "Computer science",
      "score": 0.7708182334899902
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7230570912361145
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6089277267456055
    },
    {
      "name": "Computer vision",
      "score": 0.5990978479385376
    },
    {
      "name": "Transformer",
      "score": 0.5449522733688354
    },
    {
      "name": "Residual",
      "score": 0.4629111886024475
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45901504158973694
    },
    {
      "name": "Pyramid (geometry)",
      "score": 0.44815218448638916
    },
    {
      "name": "Algorithm",
      "score": 0.11908820271492004
    },
    {
      "name": "Mathematics",
      "score": 0.11030611395835876
    },
    {
      "name": "Engineering",
      "score": 0.0717191994190216
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210116052",
      "name": "Inception Institute of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    }
  ]
}