{
  "title": "MedSyn: LLM-Based Synthetic Medical Text Generation Framework",
  "url": "https://openalex.org/W4402101243",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5106909050",
      "name": "Gleb Kumichev",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084906936",
      "name": "Pavel Blinov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5106909051",
      "name": "Yulia Kuzkina",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5111528910",
      "name": "В. А. Гончаров",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020747247",
      "name": "Galina Zubkova",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5106909052",
      "name": "Nikolai Zenovkin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002372638",
      "name": "Aleksei Goncharov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008998801",
      "name": "А. А. Савченко",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3201170304",
    "https://openalex.org/W4389519449",
    "https://openalex.org/W4319454867",
    "https://openalex.org/W4285183303",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2113452810",
    "https://openalex.org/W4318983406",
    "https://openalex.org/W4394895012",
    "https://openalex.org/W3111386522",
    "https://openalex.org/W2119852447",
    "https://openalex.org/W4323651701",
    "https://openalex.org/W6636364444",
    "https://openalex.org/W4313439128",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3178751578",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W2891113091",
    "https://openalex.org/W4388725043",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4392186815",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2891512495",
    "https://openalex.org/W4385571667",
    "https://openalex.org/W2970688856",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4393653945",
    "https://openalex.org/W6628082049",
    "https://openalex.org/W3087017761",
    "https://openalex.org/W6601076540",
    "https://openalex.org/W4320525813",
    "https://openalex.org/W4366580365",
    "https://openalex.org/W4206926190",
    "https://openalex.org/W4389520259",
    "https://openalex.org/W6600686112"
  ],
  "abstract": null,
  "full_text": "MedSyn: LLM-based Synthetic Medical Text\nGeneration Framework\nGleb Kumichev1 \u0000 , Pavel Blinov2, Yulia Kuzkina1, Vasily Goncharov1, Galina\nZubkova2, Nikolai Zenovkin1, Aleksei Goncharov1, and Andrey Savchenko2\n1 MIL Team, Moscow, Russia\n{gleb.kumichev,yulia.kuzkina,vasily.goncharov,nikolay.zenovkin}@mil-team.ru\nalex.goncharov@mil-team.com\n2 Sber AI Lab, Moscow, Russia {Blinov.P.D,GVZubkova,AVladSavchenko}@sber.ru\nAbstract. Generating synthetic text addresses the challenge of data\navailability in privacy-sensitive domains such as healthcare. This study\nexplores the applicability of synthetic data in real-world medical set-\ntings. We introduce MedSyn, a novel medical text generation frame-\nwork that integrates large language models with a Medical Knowledge\nGraph (MKG). We use MKG to sample prior medical information for the\nprompt and generate synthetic clinical notes with GPT-4 and fine-tuned\nLLaMA models. We assess the benefit of synthetic data through appli-\ncation in the ICD code prediction task. Our research indicates that syn-\nthetic data can increase the classification accuracy of vital and challeng-\ning codes by up to 17.8% compared to settings without synthetic data.\nFurthermore, to provide new data for further research in the healthcare\ndomain, we present the largest open-source synthetic dataset of clinical\nnotes for the Russian language, comprising over 41k samples covering\n219 ICD-10 codes.\nKeywords: Synthetic data · Clinical note generation · ICD code pre-\ndiction.\n1 Introduction\nWhile extensive open medical datasets are available in English, like the MIMIC\nfamily of databases [18,19] or the CPRD primary care database [14], their scope\nin comprehensively covering various medical areas is limited. The availability\nof textual medical data in non-English languages is even more constrained. Pa-\ntient privacy and ethical considerations are major limiting factors to the public\navailability of such data. The latter remains a significant problem; the lack of\ntextual medical resources substantially deters research, testing, and deployment\nof innovative Natural Language Processing (NLP) methods for national health-\ncare systems. Synthetic data generation addresses the issue of data scarcity in\nmedical research.\nBesides, the population’s diseases have a long-tail distribution, with rare\ndiseases representing only a tiny fraction of cases in a dataset [24]. Such data\narXiv:2408.02056v1  [cs.CL]  4 Aug 2024\n2 G. Kumichev et al.\nimbalance problem directly affects the ML model’s performance on the down-\nstream tasks [31,38]. Since 2020, our clinical decision support system has been\ndeployed in medical clinics in one of the regions. Insufficient text data on rare\ncases deters further system scaling, while synthetic (on-demand) medical note\ngeneration is the only solution.\nFig. 1.Examples of real clinical notes from RuMedPrime dataset [35] (translated to\nEnglish).\nNowadays, all patient information is stored in Electronic Health Records\n(EHRs), which contain a structured collection of medical events related to a\npatient and textual modality attributes: doctor’s clinical notes about symptoms\nand complaints, anamnesis, medication prescriptions, etc. Actual text from clin-\nical notes is a complex object with typos, specialized terms, abbreviations, and\ncontractions. Examples of such notes are shown in Fig. 1. That is why some\nearly synthetic generation approaches (e.g. [10]) did not allow dealing with raw\nclinical text and tried to approximate EHRs only in terms of fixed categorical\nvectors and a limited set of factors, such as diagnosis and procedure codes or\nmedication names. Including text fragments in synthetic EHRs has been chal-\nlenging for a long time. Instead of generating medical text from scratch, some\nproposed frameworks heavily depend on real EHRs [8,28], where a new health\nrecord is created by data imputation for some critical parts in the original one.\nHowever, such an approach limits the variability of results and leaves the risk of\nprivate data leakage.\nThe latest breakthroughs in developing Large Language Models (LLMs) open\na new era in generating realistic, coherent, and diverse texts across various do-\nmains. Models like GPT-3 [7], LLaMA [37], and their successors have shown re-\nmarkable capabilities in text generation for general and medical texts [4]. How-\never, even such powerful models still have some flaws [27]. First of all, they\ntend to make content errors and hallucinate [3], which is unacceptable in such\na delicate area as medicine. Therefore, even LLM-based synthetic generation\nframeworks still need external guidance and internal validation mechanisms to\nproduce medically accurate and relevant texts.\nMedSyn: LLM-based Synthetic Medical Text Generation Framework 3\nExploiting Medical Knowledge Graphs (MKGs) [12] and ontologies [1] is a\nway to mitigate the problem. Again, such resources are abundant for English but\nmodestly available for less-represented languages like Russian. In this paper, we\nfocus on developing a clinical note text generation framework combining LLMs\ncapabilities with MKG in case studies for the Russian language.\nOur key contributions can be summarized as follows:\n1. We propose an open-source framework called MedSyn 3 for synthetic clinical\nnote generation. The framework features a novel method that integrates\ndisease-specific symptoms from an MKG and incorporates real data examples\ninto the LLM generation pipeline to enhance the accuracy and diversity of\ngenerated data.\n2. We introduce the first dataset 4 with synthetic clinical notes for the Russian\nlanguage, which contains more than 41k clinical notes spanning over 219\nICD-10 (International Classification of Diseases) codes.\n3. We provide results of experiments on synthetic data generation with the\nMedSyn framework, including comparisons between GPT-4 and open-sourced\nLLaMA-7b. It is shown that an open-sourced model fine-tuned on a specific\ndataset can perform on par with or surpass GPT-4’s performance.\n2 Related Work\n2.1 Medical Knowledge Graphs\nWhile a variety of MKGs exist in English [6,11,9,40], few or none are avail-\nable in other languages. There are different possibilities for MGK applications;\nfor example, a line of work utilizes graph embeddings for various medical tasks\nlike recommendation systems [13], NLI [33], and diagnosis prediction [43]. Bi-\noLORD [29] uses concepts and relationships from the knowledge graph as part\nof the LLM pre-training. Another approach for MKG utilization involves enrich-\ning the generation process with information extracted from these graphs. This\nstrategy can be viewed as a specialized application of the retrieval-augmented\ngeneration framework [21], demonstrating the potential to produce more specific,\ndiverse, and factually accurate language. However, applying such techniques in\nthe medical domain is still an area that has not been extensively explored.\n2.2 LLMs in Medical Domain\nLLMs are increasingly utilized in the medical domain; they are primarily im-\nplemented for English [23,34,39] and Chinese [45,42], evaluated for medical QA\ntasks, and used as medical chatbots. There is also a research direction that fo-\ncuses on synthetic data generation. [26] trained the GPT-3 model from scratch\nusing clinical and general English texts, then produced 20B of medical texts\n3 https://github.com/milteam/MedSyn\n4 https://huggingface.co/datasets/Glebkaa/MedSyn-synthetic\n4 G. Kumichev et al.\nFig. 2.The clinical notes generation pipeline implemented in MedSyn framework. Rel-\nevant symptoms from MKG and clinical note examples corresponding to the ICD code\nare compiled into a prompt and used as input for LLM inference.\nutilizing this model and introduced a smaller version of the model on synthetic\ndata only. The resulting model outperforms ClinicalBERT [17] and the same\nmodel trained on actual data on MedNLI [30] and emrQA [25] benchmarks. The\nauthors of [22] generated clinical texts and manually annotated them for the\nNamed Entity Recognition (NER) task. The evaluation shows that the combi-\nnation of original and synthetic corpora achieved better performance than using\nonly the initial corpus. In [36], the authors improve performance on NER and\nrelation extraction tasks with synthetic data, showing that increasing the num-\nber of synthetic sentences can improve model performance up to a certain point,\nbeyond which the improvement becomes marginal. In a recent study [15], re-\nsearchers explored the feasibility of using synthetic text as a training corpus for\nclinical NER in French. The findings suggest that synthetic clinical notes can be\nused to train NER models, although applications for other tasks remain to be\nexplored.\nThe true potential of synthetic data in the medical field remains under ac-\ntive exploration [32,36]. However, typical problems related to LLMs, like hallu-\ncinations, pose substantial challenges in such a critical field. Ensuring factual\naccuracy and addressing inconsistencies in medical models remain valuable con-\ncerns [41]. In our research, we strive to bridge the gap in controllable medical\ndata generation, primarily focusing on the Russian language, which is heavily\nunderrepresented in linguistic medical resources.\n3 Method\nThe overall pipeline for clinical note generation is illustrated in Fig. 2. To gener-\nate a clinical note for a target ICD code, data relevant to the MKG (Section 3.1)\nand real examples are first sampled and combined into a prompt for LLM infer-\nence. We utilized GPT-4 and a fine-tuned LLaMA-7b for the LLMs (Section 3.3).\nMedSyn: LLM-based Synthetic Medical Text Generation Framework 5\nFor fine-tuning LLaMA-7b, we constructed an instruction-following dataset (Sec-\ntion 3.2). To generate a dataset of clinical notes for our experiments, we devel-\noped a specific generation task (Section 3.4) with already prepared prompts.\n3.1 Medical Knowledge Graph\nAs mentioned in Section 2.1, Russian-language equivalents of MKG are scarce.\nFor our research, we used the WikiMed database as a foundation to develop the\nRussian MKG.\nTable 1. MKG statistics. Di-Dr stands for disease-drug relation, Di-S for disease-\nsymptom relation.\nNodes Edges\nDisease Drug SymptomDi-Dr Di-S\n# 2,747 2,968 2,554 1,997 2,554\nThe constructed MKG includes the following nodes: diseases (identified by\nICD-10 codes), drugs, and symptoms. While diseases and drugs have predefined\nrelations in this database, symptoms and their relations are not specified. The\ndatabase includes clinical manifestations, which contain potential symptoms in a\nnarrative format. To extract these symptoms, we utilized ChatGPT [2], prompt-\ning it to identify symptoms from the given text of clinical manifestations. For\nexample, the clinical manifestation of tuberculosis, ‘One of the common man-\nifestations of spinal tuberculosis is the formation of cold abscesses on the neck\nand increased skin temperature‘, should lead to the extraction of symptoms [cold\nabscesses on the neck, increased skin temperature]. The extracted data were man-\nually verified by comparing them with the initial text to ensure that only symp-\ntoms were included, and no irrelevant information or noise was extracted.\nFinally extracted symptoms were then incorporated into the MKG. Its sta-\ntistical details are presented in Table 1.\nFig. 3.Examples of k-hop reasoning question on MKG. Di - Disease, Dr - Drug, S -\nSymptoms.\n6 G. Kumichev et al.\n3.2 Instruction-Following Dataset\nWe collected a dataset of 152k Russian language samples focused on instruction-\nfollowing for supervised fine-tuning 5. These samples were derived from various\nmedical benchmarks, databases, and the constructed MKG. Utilizing the MKG,\nwe created questions that require multiple levels of reasoning, ranging from sim-\nple 1-hop to complex 3-hop distances. For example, a 1-hop reasoning question\nlike ’Provide symptoms for a disease’directly connects diseases to symptoms\n(Di-S). A 2-hop question, such as ’Write down medications that can be taken\nfor these symptoms’, involves linking symptoms to diseases and then to drugs\n(S-Di-Dr). A more complex 3-hop reasoning question, like ’List medications that\ncan be taken for a disease if it is mistaken for another disease with similar symp-\ntoms’, maps diseases to symptoms, then to another disease, and finally to drugs\n(Di-S-Di-Dr), as shown in Fig. 3. We avoid more than three hops reasoning sce-\nnarios as, by our estimate, it produces too vague and error-prone samples. For\nthe clinical notes, we employed two types of tasks: continuation, which extends\nan existing note from a random point, and generation, where a note is created\nfrom prior data like symptoms. We generated at least five different rephrasings\nfor each to ensure instruction diversity.\nFig. 4.The structure of the instruction-following dataset. Leaves represent data sources\nand the percentage of data relative to the parent category.\nIn addition to real medical data, we also incorporate synthetic data from\nChatGPT. Considering that real clinical notes often have many typos and stylis-\ntic variations, which may affect model performance, we suggest that adding syn-\nthetic notes could improve the model’s text generation and be a regularization\nmethod. To create this synthetic data, we prompted ChatGPT to generate clini-\ncal notes based on patient symptoms, age, and gender. For part of the data, style\nreferences with real samples were additionally provided. We also incorporate a\nmedical dataset focused on typo correction to make the model more robust to\ntypos. The structure of the dataset is represented in Fig. 4.\n5 https://huggingface.co/datasets/Glebkaa/MedSyn-ift\nMedSyn: LLM-based Synthetic Medical Text Generation Framework 7\n3.3 Fine-Tuning\nUnlike the English language, to our knowledge, there aren’t any open-source\ngenerative LLMs tailored for the medical domain in Russian. Thus, we employ\nGPT-4 [2] for data generation to establish a strong baseline.\nOur work uses a model based on the LLaMA 2 family [37]. It is a collection\nof open generative language models with a parameter range from 7 to 70 billion.\nWe fine-tuned the model with 7 billion parameters using a learning rate 2 e−5\nand a cosine learning rate scheduler to fine-tune the model. We utilized a global\nbatch size of 256 and trained the model for three epochs.\nTo enhance the efficiency and accelerate the training of our model, we em-\nployed Low-Rank Adaptation (LoRA) [16]. This method involves freezing the\nmodel’s weights and injecting trainable rank decomposition matrices into each\nlayer of the Transformer architecture.\nThe pre-training data for LLaMa-7b consists of 90% English-language data\nand only 0.13% Russian-language data. Therefore, to fine-tune our model, we\ndecided to use the pre-trained checkpoint from Saiga 2 6 that is fine-tuned on\nRussian language instructions and dialogues generated by GPT-4.\n3.4 Generation Task\nWe prepared a generation task to generate synthetic clinical notes with real data\nexamples and symptoms spanning 105 ICD-10 category codes, as presented in\nthe RuMedTop3 dataset [5]. We sample symptoms previously extracted from\nRussian MKG (Section 3.1) according to the approach outlined in Section 3.5.\nWe aim to achieve a uniform distribution of ICD codes for the generation\ntask, but the lack of data requires inevitable trade-offs. Given the limited set of\nexamples (1,283 samples), and to ensure that the sampling procedure represents\nthe diversity of examples and symptoms, we have adopted a specific approach\nto determine the frequency of each ICD-10 category code C and computes its\nweight based on the following rule:\nw(C) = J3\n\u0000\nNC\nsymp\n\u0001\n· J3\n\u0000\nNC\nexmp\n\u0001\n, (1)\nwhere J3 denotes the triple application of the function J(x) = log(1 +x), NC\nexmp\nrefers to the number of examples corresponding to a given category code C,\nand NC\nsymp represents the number of all unique symptoms within that category.\nThe logarithmic scale used in Eq. 1 is implemented to achieve a more uniform\ndistribution of codes.\nAn exception to this weighting procedure is the category Z00, defined as\nencounter for the general exam without complaint, suspect, or reported diagnosis.\nAs this category does not hold particular interest for downstream tasks, we set\nthe number of generations for this category code to 10, thereby not factoring\nits weight into the overall distribution. We obtained the final generation task\nby sampling clinical notes and symptoms for this distribution, containing 2,503\n6 https://huggingface.co/IlyaGusev/saiga2_7b_lora\n8 G. Kumichev et al.\nentries. Each entry consists of an ICD-10 code, an example of a real clinical note,\nand a subset of symptoms.\nFor the baseline, we generate samples that do not utilize data from MKG in\ntheir prompts. The baseline prompt is similar to the original one but contains\nonly the disease name instead of incorporating disease prior information from\nMKG and a clinical note example. Generated and real clinical notes contain no\nICD codes in the text to avoid data leaks.\n3.5 Symptoms Sampling\nThe actual distribution of symptoms in clinical settings is complex. For example,\ncertain symptoms may not coexist or be specific to a particular age or gender.\nIn this study, however, we assume that symptoms are independently and identi-\ncally distributed. Consequently, we select multiple symptoms for a disease with-\nout considering their inter-relationships. We randomly sample several symptoms\nfrom the MKG (Section 3.1) related to a disease, with the count ranging from 1\nto 5, which is also chosen randomly.\n3.6 Synthetic Dataset\nWe have released a dataset of 41,185 synthetic clinical notes in Russian, gener-\nated using GPT and fine-tuned LLaMA models spanning 219 ICD-10 codes. The\ndataset includes all generated samples, regardless of quality, to facilitate various\ndata selection methods. More detailed statistics and descriptions of the data\nfields are provided in the project dataset repository. 7 According to the provided\nlicenses, all confidential information was anonymized, and researchers can safely\nuse these datasets.\n4 Experiments\n4.1 Datasets and Tasks\nIn this research, we utilized the RuMedPrime dataset [35], containing 7,625\nanonymized entries from outpatient visits to the Siberian State Medical Univer-\nsity hospital. This dataset, unique as the only open-source collection of clinical\nnotes in Russian annotated with ICD-10 codes, comprises each patient’s clin-\nical note, symptoms, and corresponding ICD code. Based on this dataset the\nRuMedTop3 task was created, focusing on the ICD code prediction from a free-\ntext clinical note. Given such a task, it is possible to implement an AI service\nthat supports doctors with a second opinion on the diagnosis search.\nOur study adopted the same dataset split as RuMedTop3, using 4,690 records\nfor training, 848 for validation, and 822 for testing while incorporating full clin-\nical notes alongside symptoms. Like RuMedTop3, we employ the second ICD-10\nclassification code hierarchy level. We also evaluated the results on the original\nRuMedTop3 dataset.\n7 https://huggingface.co/datasets/Glebkaa/MedSyn-synthetic\nMedSyn: LLM-based Synthetic Medical Text Generation Framework 9\n4.2 Models\nWe conducted experiments using both feature-based linear models and trans-\nformer models. For the linear model, we employed logistic regression based on\nterm frequency-inverse document frequency (TF-IDF) features. For the trans-\nformer models, we run experiments with RuBERT [20] and RuBioRoBERTa [44]\nand report the average results from three runs.\n4.3 Evaluation\nICD code prediction is a multi-class classification task. To evaluate it, we utilize\nthe hit@k score (k ∈ [1, 3, 5]), defined as follows:\nhit@k = 1\nN\nNX\ni=1\nhit(ˆy, topk\ni ), (2)\nwhere N is the number of samples and hit(ˆy, topk\ni ) is 1 if ground truth ICD code\nˆy is on a ranked list of k predicted codes topk and 0 otherwise.\n4.4 Results\nPrompt Following We use the BERT-score [46] to measure the similarity of\nsynthetic data to the examples and to the provided symptoms (Fig. 5).\nFig. 5.BERT-scores for example and symptoms usage.\nAs can be seen from the higher scores, the GPT-4 model follows instructions\nmore precisely, produces results that are more similar to the example, and makes\ngreater use of the provided symptoms.\nWhile high similarity to the example is desirable, complete replication is un-\nfavorable. To evaluate replication, we calculate the ratio of example N-grams\nusage, defined as the ratio of unique common N-grams between the generated\nsample and the example, divided by the number of unique N-grams in the exam-\nple (Fig. 6). For most samples, the N-grams usage ratio is less than 1, suggesting\nthat the examples are far from complete replication in the answer.\n10 G. Kumichev et al.\nFig. 6.Ratio of N-gram usage.\nFig. 7.The prediction results using only synthetic training data (codes K81 and I11).\nContour bars represent the baseline prompt, which does not utilize MKG and consists\nsolely of the task and the disease name.\nGenerating Data Out of the Training SetOne of the most exciting yet\npractically challenging scenarios involves generating data scarcely present in the\noriginal training set or generation of clinically valuable data. We selected two\nvital ICD codes for the experiment, K81 and I11. The first is cholecystitis, which\naffects about 20% of the adult population. The second code denotes a type of\nheart disease, one of the most common causes of death.\nWe transferred all real data samples to the test set, making evaluating the\nexperiments with real data in the training set impossible. However, we prioritize\na diverse test set in this experiment as it could mitigate the potential poor per-\nformance of unrepresented synthetic samples in downstream tasks. We replaced\nthe real data in the training set with 30 synthetic samples for both models and\nadded 59 samples for LLaMA-7b to assess the impact of scaling the number of\nsamples (Fig. 7).\nAlthough models trained with such synthetic data still have zero scores in the\nhit@1 metric, they show promising results in less restricted metrics like hit@5,\ndemonstrating the potential for further improvement in real data absence sce-\nnarios. Thus, synthetic data with specific refinements could increasingly become\na viable alternative for training models in data-scarce environments.\nMedSyn: LLM-based Synthetic Medical Text Generation Framework 11\nSynthetic UpsamplingAnother application for synthetic data is data upsam-\npling. In this experiment, we used the same synthetic data as in the previous\nsection (Section 4.4) and added it to the training set. The results indicate that\nmodels can benefit from such synthetic data. For instance, the accuracy of K81\ncode prediction improves by 17.8% for the RuBioRoBERTa model (Fig. 8). To\nassess the overall accuracy across all ICD codes, we also evaluate both the base-\nline and the full prompts (Table 2).\nFig. 8.Results of prediction with upsampled training set for codes K81 and I11. The\nlegend represents the data source/number of real samples/number of synthetic samples.\nTable 2.Scores across all codes with upsampled training set for codes K81 and I11.\nModel Data hit@1 hit@3 hit@5\nLinear\nReal 56.9 80.4 87.5\nLLaMA-7b-baseline 57.1 80.0 93.9\nLLaMA-7b 57.2 79.9 93.7\nRuBioRoBERTa\nReal 52.7 75.8 84.3\nLLaMA-7b-baseline 55.8 75.1 82.8\nLLaMA-7b 56.8 77.7 86.2\nFor a more detailed analysis, we focused on two codes that were frequently\nmistaken for each other more than any other pair. This decision was based on\nthe confusion matrix, which measures how often each pair of codes is confused.\nThe analysis revealed that the codes most often confused are M54 and G54.\nWe selected synthetic data for those codes generated via the same genera-\ntion task described in Section 3.4 for the GPT-4 and LLaMA-7b models. For the\nLLaMA, we repeated the generation several times to evaluate the effect of data\nscaling. Here, we only report on the linear model to depict simultaneous changes\nfor codes not averaged across several models. The experimental results are pre-\nsented in Table 3. While data generated by GPT-4 provides improvements for\nboth codes simultaneously, data generated by LLaMA still offers improvement\nfor one of the codes without a drop for the other.\n12 G. Kumichev et al.\nTable 3.Results of upsampling for the most pairwise misclassified codes. Prediction\nby the linear model. #R/S represents the number of real and synthetic samples in the\ntraining set. ↑ represents growth of both codes simultaneously,↗ - growth for one code\nwithout drop for another.\nCode Data # Real/Synthetic hit@1 hit@3 hit@5\nG54\nReal 232/0 57.9 97.4 100\nGPT-4 232/14 60.5 ↑ 97.4 100\nLLaMA-7b 232/14 57.9 97.4 100\nLLaMA-7b 232/72 60.5 ↗ 97.4 100\nM54\nReal 560/0 85.1 98.9 100\nGPT-4 560/35 87.4 ↑ 98.9 100\nLLaMA-7b 560/35 86.2 ↗ 98.9 100\nLLaMA-7b 560/175 85.1 98.9 100\nRuMedTop3 UpsamplingAlthough the generated clinical notes contain more\ninformation than the data in the RuMedTop3 task, which focuses on symptoms,\nusing the generated data to upsample this dataset is still feasible, as they share\nthe same set of ICD codes. We report results with generated data upsampling\nin Table 4, showing that all models benefit from the synthetic data.\nTable 4.Results of upsampling on RuMedTop3 dataset (the real data size is 4,690\nsamples, and the size of the synthetic dataset is 2,503 samples).\nModel Data hit@1 hit@3 hit@5\nLinear\nReal 49.8 72.7 87.8\nGPT-4 50.8 74.8 90.0\nLLaMA-7b 50.2 73.6 89.5\nRuBERT\nReal 46.5 70.4 79.3\nGPT-4 47.2 71.9 81.3\nLLaMA-7b 45.0 70.7 81.4\nRuBioRoBERTa\nReal 47.4 70.8 79.5\nGPT-4 47.3 71.7 80.4\nLLaMA-7b 46.1 70.4 79.6\n4.5 Human assessment\nWe performed the human evaluation in a side-by-side scenario to qualitatively\nassess the synthetic clinical texts. First, we randomly sampled 105 cases from real\nclinical notes examples according to the general ICD code distribution and paired\nthem with synthetic ones. Second, in each pair, we selected random sentences\n(with a median number of words of 8) to facilitate labeling and make a fair\ncomparison detached from the notes structure. Such text pairs were presented\nto a medical intern with the only question – Which text is generated, 1 or 2?\nMedSyn: LLM-based Synthetic Medical Text Generation Framework 13\nThe assessor was correct in 58.09% (61 cases). Given that the random guessing\nis 50%, we can conclude that our synthetic texts have acceptable quality. In\nfurther research, we plan to evaluate the MedSyn framework in more elaborate\nhuman assessment scenarios.\n5 Discussion\nWe used the generated datasets during all evaluations without applying filtra-\ntion or sample selection techniques. Consequently, these datasets likely contain\ncorrupted samples with minor factual errors or in some kind irrelevant to the\nprovided prompt.\nTo estimate the validity of the samples, we predict their label using models\ntrained on real data. We calculate the ratio of valid samples whose ground truth\nlabel appears in the top 5 predictions of at least 2 of five RuBERT models, each\ntrained with different seeds. We found that 51% of LLaMA-7b samples and 64%\nof GPT-4 pass this criterion. However, this is only a coarse criterion as it may\nlead to false negatives, where a correct synthetic sample falls outside the train-\ning distribution and is consistently misclassified. Additionally, a sample might\ncontain relevant information that leads to accurate predictions while still hav-\ning some corruption. This observation also suggests that GPT-4 generated data\nmight include fewer inaccurate samples, contributing to better performance. Pos-\nsible sample corruption could lead to gaps in the authenticity and applicability\nof the generated content in specific clinical scenarios, highlighting the need for\nadvanced filtration algorithms to refine the data quality. Future enhancements to\nthe MKG, including a broader range of medical information, will likely improve\nthe robustness and diversity of generated synthetic data.\nWhile synthetic data is not directly tied to real patients, its use in clinical\nsettings can still pose ethical questions regarding its applicability and acceptabil-\nity. Key concerns include: 1) Ensuring that the data accurately reflects diverse\npatient populations without introducing biases; 2) Protecting against poten-\ntial indirect privacy violations; 3) Assessing how its use might impact clinical\ndecision-making. Additionally, it is essential to be transparent about how syn-\nthetic data is made and used and ensure its use follows informed consent rules\nin medical settings.\n6 Conclusion\nThe proposed MedSyn framework suggests promising results in generating syn-\nthetic clinical notes. Human evaluation shows the high quality of generated texts,\nwhich are indistinguishable from real medical notes. In numerical experiments,\nusing additional synthetic notes leads to a 17.8% increase in ICD-code classifica-\ntion accuracy for vital and challenging classes compared to using only real data.\nAdditionally, models trained on generated data reveal substantial quality even\nwhen used as the only training source, beating a solid baseline and helping to\nimprove scores on the RuMedTop3 task. From a practical point of view, we plan\n14 G. Kumichev et al.\nto exploit the developed framework for rare disease note generation. Such syn-\nthetic data will allow us to substantially increase the number of disease classes in\nour clinical decision support system from tens to hundreds of ICD codes, giving\nthe doctor a reliable second opinion even in rare scenarios.\nThe framework’s design allows easy integration with diverse MKGs, promis-\ning even more robust and varied data generation. To foster continued innovation\nin this field, we have made our trained model, part of the training dataset, and\nthe synthetic dataset publicly available. These resources pave the way for fur-\nther research in the medical field, especially in tasks where data is scarce. For\ninstance, they potentially serve as datasets for medical NER tagging or in ICD\ncoding tasks, where models trained on such data could provide valuable auto-\nmated suggestions to humans. While synthetic data may contain inconsistencies\nor flaws, it is still precious in low-resource languages (like Russian) or low-data\nareas (like healthcare).\nReferences\n1. Abdollahi, M., Gao, X., Mei, Y., Ghosh, S., Li, J., Narag, M.: Substituting clinical\nfeatures using synthetic medical phrases: Medical text data augmentation tech-\nniques. Artificial Intelligence in Medicine 120, 102167 (09 2021)\n2. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: GPT-4 technical report. arXiv\npreprint arXiv:2303.08774 (2023)\n3. Azaria, A., Mitchell, T.: The internal state of an llm knows when its lying. arXiv\npreprint arXiv:2304.13734 (2023)\n4. Benoit, J.R.: ChatGPT for clinical vignette generation, revision, and evaluation.\nMedRxiv pp. 2023–02 (2023)\n5. Blinov, P., Reshetnikova, A., Nesterov, A., Zubkova, G., Kokh, V.: RuMedBench:\nA Russian medical language understanding benchmark. In: Artificial Intelligence\nin Medicine, pp. 383–392. Springer International Publishing (2022)\n6. Bodenreider, O.: The unified medical language system (umls): integrating biomed-\nical terminology. Nucleic acids research 32(suppl 1), D267–D270 (2004)\n7. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-\nlakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot\nlearners. Advances in neural information processing systems 33, 1877–1901 (2020)\n8. Buczak, A.L., Babin, S., Moniz, L.: Data-driven approach for creating synthetic\nelectronic medical records. BMC medical informatics and decision making 10(1),\n1–28 (2010)\n9. Chandak, P., Huang, K., Zitnik, M.: Building a knowledge graph to enable precision\nmedicine. Scientific Data 10(1), 67 (2023)\n10. Choi, E., Biswal, S., Malin, B., Duke, J., Stewart, W.F., Sun, J.: Generating multi-\nlabel discrete patient records using generative adversarial networks. In: Proceedings\nof the 2nd Machine Learning for Healthcare Conference. pp. 286–305. PMLR (2017)\n11. Cui, H., Lu, J., Wang, S., Xu, R., Ma, W., Yu, S., Yu, Y., Kan, X., Ling, C., Ho,\nJ., et al.: A survey on knowledge graphs for healthcare: Resources, applications,\nand promises. arXiv preprint arXiv:2306.04802 (2023)\n12. Gao, Y., Li, R., Caskey, J., Dligach, D., Miller, T., Churpek, M.M., Afshar, M.:\nLeveraging a medical knowledge graph into large language models for diagnosis\nprediction. arXiv preprint arXiv:2308.14321 (2023)\nMedSyn: LLM-based Synthetic Medical Text Generation Framework 15\n13. Gong, F., Wang, M., Wang, H., Wang, S., Liu, M.: SMR: medical knowledge\ngraph embedding for safe medicine recommendation. Big Data Research23, 100174\n(2021)\n14. Herrett, E., Gallagher, A.M., Bhaskaran, K., Forbes, H., Mathur, R., Van Staa,\nT., Smeeth, L.: Data resource profile: clinical practice research datalink (CPRD).\nInternational Journal of Epidemiology 44(3), 827–836 (2015)\n15. Hiebel, N., Ferret, O., Fort, K., N´ ev´ eol, A.: Can synthetic text help clinical named\nentity recognition? a study of electronic health records in French. In: Proceedings of\nthe 17th Conference of the European Chapter of the Association for Computational\nLinguistics (EACL). pp. 2320–2338. ACL, Dubrovnik, Croatia (May 2023)\n16. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021)\n17. Huang, K., Altosaar, J., Ranganath, R.: ClinicalBERT: Modeling clinical notes\nand predicting hospital readmission. arXiv preprint arXiv:1904.05342 (2019)\n18. Johnson, A.E., Bulgarelli, L., Shen, L., Gayles, A., Shammout, A., Horng, S.,\nPollard, T.J., Hao, S., Moody, B., Gow, B., et al.: MIMIC-IV, a freely accessible\nelectronic health record dataset. Scientific data 10(1), 1 (2023)\n19. Johnson, A.E., Pollard, T.J., Shen, L., Lehman, L.w.H., Feng, M., Ghassemi, M.,\nMoody, B., Szolovits, P., Anthony Celi, L., Mark, R.G.: MIMIC-III, a freely acces-\nsible critical care database. Scientific data 3(1), 1–9 (2016)\n20. Kuratov, Y., Arkhipov, M.: Adaptation of deep bidirectional multilingual trans-\nformers for Russian language. arXiv preprint arXiv:1905.07213 (2019)\n21. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K¨ uttler,\nH., Lewis, M., Yih, W.t., Rockt¨ aschel, T., et al.: Retrieval-augmented generation\nfor knowledge-intensive NLP tasks. Advances in Neural Information Processing\nSystems 33, 9459–9474 (2020)\n22. Li, J., Zhou, Y., Jiang, X., Natarajan, K., Pakhomov, S., Liu, H., Qi, W.: Are\nsynthetic clinical notes useful for real natural language processing tasks: A case\nstudy on clinical entity recognition. Journal of the American Medical Informatics\nAssociation 28 (07 2021). https://doi.org/10.1093/jamia/ocab112\n23. Luo, R., Sun, L., Xia, Y., Qin, T., Zhang, S., Poon, H., Liu, T.Y.: BioGPT: gener-\native pre-trained transformer for biomedical text generation and mining. Briefings\nin Bioinformatics 23(6) (2022)\n24. Nguyen, T.T., Schlegel, V., Kashyap, A., Winkler, S., Huang, S.S., Liu, J.J., Lin,\nC.J.: Mimic-iv-icd: A new benchmark for extreme multilabel classification. arXiv\npreprint arXiv:2304.13998 (2023)\n25. Pampari, A., Raghavan, P., Liang, J., Peng, J.: emrQA: A large corpus for question\nanswering on electronic medical records. In: Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing. pp. 2357–2368 (2018)\n26. Peng, C., Yang, X., Chen, A., Smith, K.E., PourNejatian, N., Costa, A.B., Martin,\nC., Flores, M.G., Zhang, Y., Magoc, T., et al.: A study of generative large language\nmodel for medical research and healthcare. NPJ Digital Medicine 6(1), 210 (2023)\n27. Ray, P.P.: ChatGPT: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-\nPhysical Systems (2023)\n28. Reiter, J.P., Drechsler, J.: Releasing multiply-imputed synthetic data generated in\ntwo stages to protect confidentiality. Statistica Sinica pp. 405–421 (2010)\n29. Remy, F., Demuynck, K., Demeester, T.: BioLORD-2023: Semantic textual rep-\nresentations fusing llm and clinical knowledge graph insights. arXiv preprint\narXiv:2311.16075 (2023)\n16 G. Kumichev et al.\n30. Romanov, A., Shivade, C.: Lessons from natural language inference in the clinical\ndomain. In: Proceedings of the 2018 Conference on Empirical Methods in Natural\nLanguage Processing. pp. 1586–1596 (2018)\n31. Santiso, S., Casillas, A., P´ erez, A.: The class imbalance problem detecting adverse\ndrug reactions in electronic health records. Health informatics journal25(4), 1768–\n1778 (2019)\n32. Shaib, C., Li, M.L., Joseph, S., Marshall, I.J., Li, J.J., Wallace, B.C.: Summarizing,\nsimplifying, and synthesizing medical evidence using GPT-3 (with varying success).\narXiv preprint arXiv:2305.06299 (2023)\n33. Sharma, S., Santra, B., Jana, A., Tokala, S., Ganguly, N., Goyal, P.: Incorporating\ndomain knowledge into medical NLI using knowledge graphs. In: Proceedings of\nthe Conference on Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP). Association for Computational Linguistics (2019)\n34. Singhal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung, H.W., Scales, N.,\nTanwani, A., Cole-Lewis, H., Pfohl, S., et al.: Large language models encode clinical\nknowledge. Nature 620(7972), 172–180 (2023)\n35. Starovoytova, E., Kulikov, E., Fedosenko, S., Shmyrina, A., Kirillova,\nN., Vinokurova, D., Balaganskaya, M.: RuMedPrimeData (Dec 2021).\nhttps://doi.org/10.5281/zenodo.5765873\n36. Tang, R., Han, X., Jiang, X., Hu, X.: Does synthetic data generation of LLMs help\nclinical text mining? arXiv preprint arXiv:2303.04360 (2023)\n37. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozi` ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: LLaMA: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n38. Wang, Y., Wei, Y., Yang, H., Li, J., Zhou, Y., Wu, Q.: Utilizing imbalanced elec-\ntronic health records to predict acute kidney injury by ensemble learning and time\nseries model. BMC Medical Informatics and Decision Making 20, 1–13 (2020)\n39. Wu, C., Zhang, X., Zhang, Y., Wang, Y., Xie, W.: PMC-LLaMA: Further finetun-\ning llama on medical papers. arXiv preprint arXiv:2304.14454 (2023)\n40. Wu, X., Duan, J., Pan, Y., Li, M.: Medical knowledge graph: Data sources, con-\nstruction, reasoning, and applications. Big Data Mining and Analytics 6(2), 201–\n217 (2023). https://doi.org/10.26599/BDMA.2022.9020021\n41. Xie, Q., Schenck, E.J., Yang, H.S., Chen, Y., Peng, Y., Wang, F.: Faithful AI in\nmedicine: A systematic review with large language models and beyond. medRxiv\n(2023). https://doi.org/10.1101/2023.04.18.23288752\n42. Xiong, H., Wang, S., Zhu, Y., Zhao, Z., Liu, Y., Wang, Q., Shen, D.: Doctor-\nGLM: Fine-tuning your chinese doctor is not a herculean task. arXiv preprint\narXiv:2304.01097 (2023)\n43. Xu, X., Xu, X., Sun, Y., Liu, X., Li, X., Xie, G., Wang, F.: Predictive modeling\nof clinical events with mutual enhancement between longitudinal patient records\nand medical knowledge graph. In: Proceedings of IEEE International Conference\non Data Mining (ICDM). pp. 777–786 (2021)\n44. Yalunin, A., Nesterov, A., Umerenkov, D.: RuBioRoBERTa: a pre-trained biomed-\nical language model for Russian language biomedical text mining. arXiv preprint\narXiv:2204.03951 (2022)\n45. Zhang, H., Chen, J., Jiang, F., Yu, F., Chen, Z., Li, J., Chen, G., Wu, X., Zhang,\nZ., Xiao, Q., et al.: HuatuoGPT, towards taming language model to be a doctor.\narXiv preprint arXiv:2305.15075 (2023)\n46. Zhang, T., Kishore, V., Wu, F., Weinberger, K.Q., Artzi, Y.: BERTScore: Evalu-\nating text generation with BERT. arXiv preprint arXiv:1904.09675 (2020)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8430067300796509
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4047852158546448
    },
    {
      "name": "Natural language processing",
      "score": 0.36567121744155884
    },
    {
      "name": "Programming language",
      "score": 0.3424834907054901
    },
    {
      "name": "Information retrieval",
      "score": 0.33384162187576294
    }
  ],
  "institutions": [],
  "cited_by": 8
}