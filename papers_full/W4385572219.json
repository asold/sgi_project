{
    "title": "Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding",
    "url": "https://openalex.org/W4385572219",
    "year": 2023,
    "authors": [
        {
            "id": null,
            "name": "Erica Kido Shimomoto",
            "affiliations": [
                "Ochanomizu University"
            ]
        },
        {
            "id": "https://openalex.org/A2167815369",
            "name": "Edison Marrese Taylor",
            "affiliations": [
                "Ochanomizu University"
            ]
        },
        {
            "id": "https://openalex.org/A2170796186",
            "name": "Hiroya Takamura",
            "affiliations": [
                "Ochanomizu University"
            ]
        },
        {
            "id": "https://openalex.org/A1982119413",
            "name": "Ichiro Kobayashi",
            "affiliations": [
                "Ochanomizu University",
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A2126593491",
            "name": "Hideki Nakayama",
            "affiliations": [
                "Ochanomizu University"
            ]
        },
        {
            "id": "https://openalex.org/A2036428500",
            "name": "Yusuke Miyao",
            "affiliations": [
                "Ochanomizu University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3010593057",
        "https://openalex.org/W3198571508",
        "https://openalex.org/W3034743747",
        "https://openalex.org/W4286856918",
        "https://openalex.org/W3213647938",
        "https://openalex.org/W2618799552",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W2963211188",
        "https://openalex.org/W3101498587",
        "https://openalex.org/W2962945654",
        "https://openalex.org/W2798354744",
        "https://openalex.org/W3198377975",
        "https://openalex.org/W3174702398",
        "https://openalex.org/W2970925270",
        "https://openalex.org/W2963662190",
        "https://openalex.org/W4283066680",
        "https://openalex.org/W4312402470",
        "https://openalex.org/W3205949070",
        "https://openalex.org/W2964214371",
        "https://openalex.org/W4287122891",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W3176201273",
        "https://openalex.org/W2964089981",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2894280539",
        "https://openalex.org/W2963017553",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3035640828",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W3120889656",
        "https://openalex.org/W3178087530",
        "https://openalex.org/W3180476551",
        "https://openalex.org/W4297947337",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4221166385",
        "https://openalex.org/W3091588028",
        "https://openalex.org/W2803088946",
        "https://openalex.org/W3035339529",
        "https://openalex.org/W4226150044",
        "https://openalex.org/W3025323587",
        "https://openalex.org/W2952132648",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W3207454933",
        "https://openalex.org/W3206816211",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2964023933",
        "https://openalex.org/W2337252826",
        "https://openalex.org/W4312884055",
        "https://openalex.org/W3173788106",
        "https://openalex.org/W3099793224",
        "https://openalex.org/W3199096350",
        "https://openalex.org/W2890502146",
        "https://openalex.org/W3090449556",
        "https://openalex.org/W2963916161",
        "https://openalex.org/W2963563276",
        "https://openalex.org/W4306294758",
        "https://openalex.org/W3204090293",
        "https://openalex.org/W4226051135"
    ],
    "abstract": "This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a query sentence, the goal is to recognize and determine temporal boundaries of action instances in the video described by natural language queries. Recent works tackled this task by improving query inputs with large pre-trained language models (PLM), at the cost of more expensive training. However, the effects of this integration are unclear, as these works also propose improvements in the visual inputs. Therefore, this paper studies the role of query sentence representation with PLMs in TVG and assesses the applicability of parameter-efficient training with NLP adapters. We couple popular PLMs with a selection of existing approaches and test different adapters to reduce the impact of the additional parameters. Our results on three challenging datasets show that, with the same visual inputs, TVG models greatly benefited from the PLM integration and fine-tuning, stressing the importance of the text query representation in this task.Furthermore, adapters were an effective alternative to full fine-tuning, even though they are not tailored to our task, allowing PLM integration in larger TVG models and delivering results comparable to SOTA models. Finally, our results shed light on which adapters work best in different scenarios.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 13101–13123\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTowards Parameter-Efficient Integration of Pre-Trained Language Models\nIn Temporal Video Grounding\nErica K. Shimomoto1, Edison Marrese-Taylor1, Hiroya Takamura1,\nIchiro Kobayashi1,2, Hideki Nakayama1,3, Yusuke Miyao1,3\nNational Institute of Advanced Industrial Science and Technology1\nOchanomizu University2, The University of Tokyo3\n{kidoshimomoto.e,edison.marrese,takamura.hiroya}@aist.go.jp\nkoba@is.ocha.ac.jp, nakayama@ci.i.u-tokyo.ac.jp, yusuke@is.s.u-tokyo.ac.jp\nAbstract\nThis paper explores the task of Temporal Video\nGrounding (TVG) where, given an untrimmed\nvideo and a natural language sentence query,\nthe goal is to recognize and determine tempo-\nral boundaries of action instances in the video\ndescribed by the query. Recent works tackled\nthis task by improving query inputs with large\npre-trained language models (PLM) at the cost\nof more expensive training. However, the ef-\nfects of this integration are unclear, as these\nworks also propose improvements in the visual\ninputs. Therefore, this paper studies the effects\nof PLMs in TVG and assesses the applicabil-\nity of parameter-efficient training with NLP\nadapters. We couple popular PLMs with a se-\nlection of existing approaches and test different\nadapters to reduce the impact of the additional\nparameters. Our results on three challenging\ndatasets show that, without changing the visual\ninputs, TVG models greatly benefited from the\nPLM integration and fine-tuning, stressing the\nimportance of sentence query representation\nin this task. Furthermore, NLP adapters were\nan effective alternative to full fine-tuning, even\nthough they were not tailored to our task, allow-\ning PLM integration in larger TVG models and\ndelivering results comparable to SOTA models.\nFinally, our results shed light on which adapters\nwork best in different scenarios.\n1 Introduction\nTemporal Video Grounding (TVG) is a fundamen-\ntal task in Computer Vision (CV), where the goal\nis to have models recognize and determine tempo-\nral boundaries of action instances in videos (Shou\net al., 2016; Gu et al., 2018; Girdhar et al., 2019) us-\ning queries provided in natural language (Gao et al.,\n2017; Hendricks et al., 2017). Over the past few\nyears, interest in this task has grown substantially\ndue to its complexity and potential applications,\nwhich has led to the release of several models that\neither use propose-and-rank techniques or directly\npredict the starting and ending temporal locations\n(Ghosh et al., 2019; Rodriguez et al., 2020).\nFollowing trends in other vision-and-language\n(V&L) tasks, some of the latest models combine\nvision encoders and pre-trained language models\n(PLM). We find works that directly encode the\nquery using PLMs (Nawaz et al., 2022; Wang et al.,\n2022), which are later fine-tuned with the rest of\nthe architecture, or that try to project both the query\nand video into the same embedding space using a\nTransformer (Zhang et al., 2021a).\nDespite the performance improvements, it is dif-\nficult to isolate the effects of the improved language\nrepresentations, as these works also propose new\nvideo-language matching approaches or use differ-\nent video encoders. Another drawback is their com-\nputational cost for training, as parameter counts\ngrow substantially once PLMs are incorporated.\nTo address this problem, several parameter-\nefficient training methods have been recently pro-\nposed for both Natural Language Processing (NLP)\n(Karimi Mahabadi et al., 2021b) and CV models\n(Rebuffi et al., 2017). Among these approaches,\nadapters (Houlsby et al., 2019; Bapna and Firat,\n2019) and their variations have been particularly\neffective1, as they lead to performance as high as\nfine-tuning while training only a small set of pa-\nrameters. Adapters have been successfully com-\nbined with vision models for several tasks (Kim\net al., 2021; Zhou et al., 2022), showing that using\na few parameters to learn to fuse vision and lan-\nguage representations without losing performance\nis possible. However, we note that efforts so far\nhave focused only on image (Zhang et al., 2021b)\nand video (Pan et al., 2022) classification tasks or\non leveraging pre-trained generative models, e.g.,\nby re-casting existing vision-and-language tasks as\nlanguage generation (Sung et al., 2022). In contrast,\n1While the term “adapter\" is often used to refer to the\noriginal adapter proposed by Houlsby et al. (2019), we use it\nto refer to any efficient fine-tuning method in this work.\n13101\nas the training signal in TVG comes from the visual\nmodality, we cannot cast it as language generation.\nTherefore, this paper studies the effects of large\nPLMs in the TVG task and investigates the applica-\nbility of NLP adapters for a parameter-efficient\nintegration. We couple popular PLM models\nwith a selection of previous works, allowing us\nto isolate and understand their effects on perfor-\nmance. Concretely, we analyze ExCL (Ghosh et al.,\n2019), TMLGA (Rodriguez et al., 2020) and DORi\n(Rodriguez-Opazo et al., 2021), three proposal-free\nTVG models with different levels of complexity.\nMoreover, we also benchmark several parameter-\nefficient training alternatives based on adapters.\nWe conduct thorough experiments on three chal-\nlenging datasets, Charades-STA (Gao et al., 2017),\nActivityNet Captions (Krishna et al., 2017) and\nYouCookII (Zhou et al., 2018b,a), covering videos\nand queries with varying lengths of different activi-\nties. Concretely, we seek to answer the following\nresearch questions: RQ1: Does incorporating a\nPLM improve the performance of existing TVG\nmodels?; RQ2: Are adapters an alternative to full\nfine-tuning of the PLM parameters within TVG?;\nRQ3: Is there an adapter that works best for TVG?;\nRQ4: What is the impact of different PLMs?;RQ5:\nHow does the combination of existing TVG models\nwith PLMs trained with adapters perform against\nstate-of-the-art models?\nOur results offer concrete answers to these ques-\ntions, helping us clarify the role of PLMs in the\nTVG task and quantify how much they can im-\nprove the existing model’s grounding capabilities.\nThey suggest that, by only changing the query sen-\ntence representation using PLMs, TVG models\ncan greatly improve performance, especially when\nPLMs are fine-tuned, stressing the importance of\nthe text query representation in this task.\nOur contributions can be summarized as follows:\n(1) We quantify the impact of PLMs in TVG mod-\nels, (2) We perform the first work on benchmarking\ndifferent types of adapters on the TVG task, shed-\nding light on which adapters work best for each\ncase, and (3) We offer an empirical demonstration\nthat adapters can reach or surpass the performance\nof full fine-tuning while updating only ∼10% of\nthe parameters in our task. The code to repro-\nduce our experiments is available at github.com/\nericashimomoto/parameter-efficient-tvg.\n2 Related Work\nTemporal Video Grounding: Work on this task\ncan be divided into two main approaches. On the\none hand, we find techniques based on proposal\ngeneration, where the idea is to, given a query, out-\nput a set of candidate clips which could later be\nranked (Liu et al., 2018; Ge et al., 2019). Further re-\nsearch has mostly focused on reducing the number\nof proposals by producing query-guided or query-\ndependent approaches (Chen et al., 2018; Chen\nand Jiang, 2019; Xu et al., 2019), or on creating\nmaps that can cover diverse video moments with\ndifferent lengths (Zhang et al., 2021c). Zhang et al.\n(2021a) adopted a Transformer-based multi-modal\nmodel (MSAT) which is pre-trained for this set-\nting. More recently, models have incorporated con-\ntrastive losses to improve performance further. This\nis the case of both CPL (Zheng et al., 2022) and\nMNM (Wang et al., 2022), which also incorporate\nTransformer-based components in their pipelines.\nThe second line of approaches has instead pro-\nposed to directly predict the start and end loca-\ntions through the video span (ExCL; Ghosh et al.,\n2019). Work on this line of research has focused\non improving the performance by modelling la-\nbel uncertainty (TMLGA; Rodriguez et al., 2020),\nadding spatial features (DORi; Rodriguez-Opazo\net al., 2021) or improving the text-to-video match-\ning strategies (Mun et al., 2020; Zeng et al., 2020).\nFor example, CPN (Zhao et al., 2021) adopted an\nad-hoc graph-based technique. Other approaches\nhave focused on exploiting local and global features\nfor better performance like Mun et al. (2020). CP-\nNet (Li et al., 2021) recently proposed a pyramid-\nlike approach where the model progressively re-\nplenishes the temporal contexts and refines the\nlocation of the queried activity by enlarging the\ntemporal receptive fields. Finally, two recent ap-\nproaches, VSLNet (Zhang et al., 2020) and BCPN,\n(Nawaz et al., 2022) have proposed to cast the task\nas visual question answering.\nOn top of these lines, recently, an effort has been\nmade to solve a variant of the task named spatio-\ntemporal video grounding. In this case, besides\npredicting when the moment starts and ends, the\nmodel should also identify where the action de-\nscribed by the textual query occurs in the frames.\nWorks on this task heavily rely on the transformers\narchitecture (Yang et al., 2022a), with significant\neffort in eliminating the need for any pre-trained\nobject detectors (Su et al., 2021; Jin et al., 2022).\n13102\nUpdated\nFrozen\nInput Embedding\nPositional \nEmbedding\nAdd & Norm\nMulti-Head \nAttention\nAdd & Norm\nNx\nInputs\nAdapter\nFeed Forward\nAdapter\nFigure 1: Illustration of the traditional bottleneck\nadapter proposed by Houlsby et al. (2019). The adapter\nlayers are introduced after the multi-head attention and\nfeed-forward layers. Orange color refers to trainable\nparameters, and blue color refers to frozen ones.\nWhile closely related to our task, due to the addi-\ntion of the spatial dimension, this task naturally\nemphasizes the role of the visual modality in the\ngrounding, further deviating from our language-\ndriven approach. Therefore, our study does not\nconsider models tailored for this task.\nParameter-efficient model training: As ma-\nchine learning models continue to grow, updat-\ning their parameters efficiently is becoming in-\ncreasingly important. One key idea has been to\nonly update newly-added parameters (Rebuffi et al.,\n2017, 2018). With the advent of large pre-trained\nTransformer-based models in NLP, this idea has\nled to the development of adapters (Houlsby et al.,\n2019): sub-networks with few parameters that\nare inserted after every attention and feed-forward\nlayer in a given model, as illustrated in Figure 1.\nWe also find a variety of prompt-based approaches,\nwhich add trainable parameters into the model\ninputs (Li and Liang, 2021; Lester et al., 2021;\nGu et al., 2022). Alternative techniques, such as\nsparsely updating a small number of parameters\nof the model (Ben Zaken et al., 2022; Guo et al.,\n2021; Sung et al., 2021), or low-rank factorization\nfor the weights to be updated (Mahabadi et al.,\n2021; Karimi Mahabadi et al., 2021a; Hu et al.,\n2022) have also been proposed recently. Finally,\nHe et al. (2022a); Mao et al. (2022) combined some\nof these techniques to propose a unified parameter-\nefficient training framework. Though we focus on\nNLP adapters, we deviate from previous work as\nour approach incorporates the visual modality.\nOnly updating newly-added parameters has also\nbeen proposed in CV , with some work predating\nthe advent of Transformers (Rebuffi et al., 2017,\n2018). More recently, we find works combining\npre-trained language models with multi-modal in-\nText\nEncoder\nWoman \nsitting \non \na \ncouch\nVideo  \nEncoder\nLocalization Layer\nText \n Query\nInput  \nVideo\nGloVe\nLocalization Layer\nP(start) P(end)\nLocalization Layer\nUpdated\nFrozen\nText\nEncoder\nWoman \nsitting \non \na \ncouch\nVideo  \nEncoder\nText \n Query\nInput  \nVideo\nPLM\nText\nEncoder\nWoman \nsitting \non \na \ncouch\nVideo  \nEncoder\nText \n Query\nInput  \nVideo\nPLM\na) Original Models b) Proposed modification c) Proposed training\nmethod\nP(start) P(end) P(start) P(end)\nAdapters\nFigure 2: Illustration of our proposed approach to in-\ncorporate pre-trained language encoders and adapters\ninto existing Temporal Video Grounding pipelines. Or-\nange color refers to trainable parameters, and blue color\nrefers to frozen ones.\nputs. For example, Tsimpoukelli et al. (2021)\ntrained a vision encoder to represent each image\nas a sequence of continuous embeddings, such that\na frozen language model prompted with this pre-\nfix generates the appropriate image caption. Yang\net al. (2022b) showed that it is possible to per-\nform zero-shot video question answering by lever-\naging frozen bidirectional language models. More\nrecently, Sung et al. (2022) cast multiple V&L\ntasks as text generation, combining NLP adapters\nwith pre-trained encoder-decoders, such as BART\n(Lewis et al., 2020), with existing image encoders,\nsuch as CLIP (Radford et al., 2021). The latter has\nalso lately been the target of several studies that\nextend parameter-efficient techniques for CV (Kim\net al., 2021; Zhang et al., 2021b; Zhou et al., 2022).\nThough our approach follows a similar trend, our\ninterest lies in a grounding task where the training\nsignal comes from the visual modality, which keeps\nus from casting our task as language generation.\n3 Proposed Approach\nConsider a video V ∈V, represented as a sequence\nof frames such that V = {vt}with t = 1,...,l .\nEach video in Vis annotated with a natural lan-\nguage passage S ∈S, where S is a sequence of\nwords S = {sj}with j = 1,...,m , which de-\nscribes what is happening at a certain period of\ntime in the video. This interval is formally defined\nby ts and te, the starting and ending points of the\nannotations in time, respectively. The goal of the\ntemporal video grounding task is to predict ts and\nte given the video V and the text query S.\n13103\n3.1 Temporal Video Grounding Models\nFor this study, we focused on proposal-free mod-\nels, which generally offer better performance, and\nwe were careful only to consider works that used\nword embeddings. Furthermore, we only consid-\nered models which have their implementation avail-\nable. Concretely, we selected ExCL, TMLGA, and\nDORi. ExCL was the first proposal-free model for\nTVG; TMLGA improved on it by handling video\nannotation uncertainty, and later DORi incorpo-\nrated spatial features.\nAs shown in Figure 2 (a), these models can be\nsummarized into three main parts: a sentence en-\ncoder, a video encoder, and a localization module,\nwhich combines information from both modalities\nand predicts the start and end points of the seg-\nment in the video described by the query. While\nthese models heavily explore information from the\nvideo input, they make relatively simple use of\nthe sentence query, mainly processing pre-trained\nword embeddings, such as GloVe (Pennington et al.,\n2014), through a recurrent neural network.\nTo better understand the role of improved lan-\nguage representations on the performance in the\nTVG task, we study the effects of PLMs by in-\ncorporating them as-is into a selection of existing\nmodels from the literature, therefore effectively\nisolating their impact on the performance across\na range of settings. We tested several ways to in-\ncorporate PLMs, such as entirely replacing the text\nencoder block with the PLM, where most failed to\ndeliver any performance improvement, and, there-\nfore, decided to replace only the word embeddings,\nas shown in Figure 2 (b).\n3.2 Adapters\nA concern when integrating large PLMs with exist-\ning TVG models is the alarming number of param-\neters added. While some TVG models are pretty\nsmall, recent models are exponentially increasing\nin size. For example, DORi has about 10M param-\neters, more than double than TMLGA (about 4M).\nCombining these models even with reasonably-\nsized PLMs, such as BERT (Devlin et al., 2019),\nincreases the number of trainable parameters to\nover 120M. Also, the more sophisticated visual fea-\ntures are used, the larger the training data becomes.\nFor example, the visual features for TMLGA are\nabout 2.1MB per video in the ActivityNet dataset,\nwhile for DORi, they are about 82MB.\nTo alleviate this issue, we also investigate sev-\neral parameter-efficient training alternatives based\non adapters to incorporate these PLMs into the ex-\nisting model pipelines with reduced computational\ncost, as shown in Figure 2 (c).\nFor our experiments, we adopt a large selec-\ntion of adapters following previous work (Sung\net al., 2022), including bottleneck adapters, such\nas the ones proposed by Houlsby et al. (2019)\n(HOULSBY ), Pfeiffer et al. (2020b) ( PFEIFFER ),\nand He et al. (2022a) ( PARALLEL ); Invertible\nadapters (INVERSE ) (Pfeiffer et al., 2020b), Prefix\nTuning (PREFIX ) (Li and Liang, 2021), Compacter\n(COMPACTER ) (Karimi Mahabadi et al., 2021a),\nand LoRA (LORA) (Hu et al., 2022).\nWe note that some of the adapters we consider\nin our study were designed for specific purposes in\nNLP. Our decision to still include such adapters in\nour experimental framework is motivated by their\nrelative success in other vision-and-language tasks\n(He et al., 2022c; Kim et al., 2021; Sung et al.,\n2022). Moreover, as our approach differs from\nexisting work in this context, we were interested in\noffering empirical evidence to further understand\nthese adapters’ role in multi-modal scenarios.\n4 Experimental Framework\n4.1 Datasets\nCharades-STA: Built upon the Charades dataset\n(Sigurdsson et al., 2016), it provides time-based an-\nnotations using a pre-defined set of activity classes\nand general video descriptions. We use the pre-\ndefined train and test sets containing 12,408 and\n3,720 moment-query pairs, respectively. Videos\nare 31s long on average and have a maximum du-\nration of 194s, with 2.4 moments on average, each\nbeing 8.2s long on average and described using 7.2\nwords on average.\nActivityNet Captions: Introduced by Krishna\net al. (2017) and initially constructed for dense\nvideo captioning, it consists of 20k YouTube videos\nwith an average length of 120s and a maximum du-\nration of 755s. The videos contain 3.65 temporally\nlocalized time intervals and sentence descriptions\non average, where descriptions have on average\n13.48 words. Following previous works, we report\nthe performance on the combined validation sets.\nYouCookII:It consists of 2,000 long untrimmed\nvideos from 89 cooking recipes obtained from\nYouTube by Zhou et al. (2018b,a). Each step for\ncooking these dishes was annotated with tempo-\nral boundaries and aligned with the corresponding\n13104\nsection of the recipe. The average video length is\n316s and a maximum duration of 755s. Regarding\nrelevant moment segments, each video has 7.73 mo-\nments on average, with each segment being 19.63s\nlong and described using 8.7 words on average.\n4.2 Implementation Details\nTemporal Video Grounding Models: Our imple-\nmentation for TMLGA and DORi is built on top of\nthe original code released by the authors, while we\nuse our own implementation of ExCL. To represent\nthe video input, we follow the original implementa-\ntions as close as possible. For TMLGA and DORi,\nwe use the I3D features released with the respective\npapers. For ExCL, we use the features released by\nthe DORi paper, extracted at 25 fps instead of the\noriginal 5 fps2. Specifically for DORi, we also use\nthe spatial features released with the paper.\nPre-trained Language Models: We com-\nbine our selected models with pre-trained BERT,\nRoBERTa (Liu et al., 2019) and DeBERTa (He\net al., 2022b), with the implementations provided\nby the HuggingFace library (Wolf et al., 2020). Fur-\nthermore, we used “bert-base-uncased\", “roberta-\nbase\", and “deberta-base\" pre-trained models.\nAdapters: We use the implementation provided\nby the adapter-transformers library (Pfeiffer et al.,\n2020a), with default configurations. In particu-\nlar, for the Invertible adapters (INVERSE ), we only\ntested the “PfeifferInvConfig\", the original configu-\nration proposed in the paper (Pfeiffer et al., 2020b).\nTraining: Our experiments were performed on a\n40-GB NVIDIA A100 GPU. Models were trained\nusing ADAM (Kingma and Ba, 2020) with a step-\nbased learning rate scheduler. When fine-tuning the\nPLMs, we used a scheduler with linear warmup for\nthe PLM parameters, while keeping the learning\nrate fixed for the rest of the parameters. For more\ndetails on hyper-parameters, we refer the readers\nto the Appendices A and B. The evaluation follows\nprevious work, based on two widely used metrics\n(Gao et al., 2017), namely the Recall at various\nthresholds of the temporal Intersection over Union\n(tIoU or R@α) measuring the percentage of predic-\ntions that have tIoU with ground truth larger than\ncertain α, and the mean averaged tIoU (mIoU). We\nuse αthreshold values of 0.3, 0.5 and 0.7.\nQuery: He stands up and yells at the man tattooing.\nGTAdapterFinetuningFrozenTMLGA\nQuery: Then, people drive snow bikes on the mountains covered with snow.\nGTAdapterFinetuningFrozenTMLGA\nFigure 3: Examples of success (top) and failure (bottom)\nof TMLGA with BERT on ActivityNet.\n5 Results and Discussion\nRQ1: Effect of adding BERT to existing mod-\nels To investigate this matter, we replace the non-\ncontextualized word embeddings in our selected\nTVG models with BERT, which can be regarded\nas the current most widely-studied PLM (Rogers\net al., 2020; Yang et al., 2020; Chen et al., 2020; Li\net al., 2020). We compare the original model per-\nformance with the performance when fine-tuning\nthe PLM along with the TVG model training (fine-\ntuning), and when freezing the PLM (/enc-96), training\nonly the parameters of the TVG model. To ensure\nour implementations were correct, we also tested\nthe original models (ours), achieving performance\nclose to the reported in their respective papers.\nTable 1 shows the results of this combination.\nWhen introducing BERT to the models, we can see\nthat full model fine-tuning leads to an average im-\nprovement of 1.38% and 1.24% in mIoU for ExCL\nand TMLGA, respectively. This result shows that\nthe chosen TVG models can benefit from using\nPLMs. However, BERT adds over 100M parame-\nters to be tuned. Such an increase is troublesome\nas we ran out of memory when trying to fine-tune\n2In practice, this difference in fps is not an issue, as\nRodriguez-Opazo et al. (2021) has shown that extraction at\n25fps leads to better performance\n13105\nMethod Params. Charades-STA ActivityNet YouCookII\nR@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU\nExCL† (orig.) 6.9M 65.10 44.10 22.60 - - - - - 44.20 28.00 14.60 -\nExCL (ours) 6.9M 62.28 39.74 22.53 42.28 55.49 39.33 23.04 40.32 26.58 15.72 8.19 18.99\n+ BERT/enc-966.9M 62.93 38.44 22.23 42.3857.21 39.66 23.79 41.45 26.63 16.15 8.51 18.87\n+ Adapter 7.2M-16.8M 61.59 37.15 21.51 41.06 59.35 41.27 24.86 42.83 28.47 17.75 9.02 19.89\n+ Fine-tuning 116M 61.75 38.36 23.44 42.00 59.10 41.83 25.42 42.36 28.18 16.84 9.02 20.08\nTMLGA (orig.) 4.7M 67.53 52.02 33.74 48.22 51.28 33.04 19.26 37.78 33.48 20.65 10.94 23.07\nTMLGA (ours) 4.7M 69.49 49.97 32.72 48.29 50.84 31.13 17.86 36.90 34.42 21.99 10.94 23.63\n+ BERT/enc-964.7M 70.08 49.92 31.42 48.34 52.10 32.57 18.64 37.63 34.77 23.05 12.49 24.42\n+ Adapter 5.6M - 14.6M 71.40 52.53 33.82 49.57 53.98 35.20 20.43 38.88 36.08 22.77 12.49 25.19\n+ Fine-tuning 114M 71.02 52.53 33.52 49.8053.59 34.05 19.51 37.92 35.34 21.85 11.63 24.82\nDORi (orig.) 10.4M 72.72 59.65 40.56 53.28 57.89 41.49 26.41 42.78 43.36 30.47 18.24 30.46\nDORi (ours.) 10.4M 72.26 57.18 40.62 53.01 57.38 40.00 24.84 41.97 43.33 29.15 17.61 30.17\n+ BERT/enc-9610.4M 71.83 57.15 39.22 52.49 58.86 40.86 25.50 42.97 42.27 29.90 18.38 29.92\n+ Adapter 11.6M - 20.3M 72.50 58.63 40.97 53.29 60.81 43.49 27.86 44.55 46.79 32.56 19.87 32.48\nTable 1: Overview of our results combining BERT and adapters with our selected prior work. Underlined results\nindicate the best performance within the method and dataset combination, while results in bold indicate the best\nperformance within the dataset.\nBERT along with DORi.\nOne alternative to save on this computational\ncost is to freeze the PLM when training the TVG\nmodel. We can see that this strategy leads to an\noverall improvement in mIoU of 0.40%, 0.52%,\nand 0.08% for ExCL, TMLGA, and DORi, respec-\ntively, when compared to the original model perfor-\nmance. This improvement is substantially smaller\nthan when fine-tuning the PLM, with cases where\nusing the frozen BERT leads to worse results than\nwhen using GloVe, such as with ExCL and DORi\non YouCookII. Furthermore, we can see that the\nmodels benefited the least from the frozen PLM\nwhen tested on the Charades-STA dataset. This re-\nsult could be due to queries in Charades-STA being\nless complex, so the word embeddings could al-\nready be enough to perform well. Nevertheless, the\noverall results indicate that fine-tuning is essential\nto getting the full potential of the PLM in our task.\nRQ2: Adapters as an alternative to PLM fine-\ntuning To the best of our knowledge, there is no\nevidence to suggest whether adapters could bring\nbenefits to the TVG task similar to what has been\nshown in other NLP tasks. Therefore, we seek to\ninvestigate if using adapters can be an effective\nalternative to full fine-tuning of the PLM models\nin TVG. We tested the adapters mentioned in Sec-\ntion 3.2 for all three TVG models with BERT.\nOur best results are shown in Table 1 (Adapter).\nFor ExCL, the best adapters werePFEIFFER , PFEIF -\nFER , and LORA, for Charades-STA, ActivityNet\nand YouCookII, respectively; For TMLGA, the\nbest were PREFIX , PFEIFFER , and HOULSBY ; and\nfor DORi, INVERSE , PREFIX , and HOULSBY . Fur-\nther results can be found in Appendix A. We also\nprovide the visualization of success and failure ex-\namples in Figure 3 for the combination of TMLGA\nwith BERT on ActivityNet, and refer the readers to\nthe Appendix C for more visualizations.\nOur results show that using adapters led to an\noverall improvement in mIoU of0.70%, 1.71% and\n1.72% for ExCL, TMLGA, and DORi, respectively,\nover the models’ original performance. While the\nimprovement from the adapters for ExCL is smaller\nthan when doing full fine-tuning, adapters led to\nbetter performance with TMLGA. More impor-\ntantly, adapters allowed a significant performance\nimprovement for DORi, as training with them re-\nquires updating only 16% of the parameters re-\nquired for full fine-tuning. Furthermore, we can\nsee that in some cases, using adapters leads to bet-\nter performance than full fine-tuning, such as with\nExCL and TMLGA on ActivityNet Captions.\nIn summary, these results indicate that despite\nnot being tailored for the task of TVG, the adapters\ncovered in this work can be an efficient alternative\nto the full fine-tuning of PLM models.\nRQ3: Choice of adapter for TVG We were then\nnaturally interested in studying whether there is a\nspecific type of adapter that works best to solve\nour task. Therefore, we ranked the performance\nof each adapter for each dataset and model, focus-\n13106\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nPfeiffer\nParallel\nPrefix\nCompacter\nInverse\nParallel = 30.15\nCharades-STA\nExCL\nTMLGA\nDORi\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\nPfeiffer\nPrefix\nPfeiffer\nParallel\nPrefix\nParallel\nActivityNet Captions\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\nLoRA\nParallel*\nPrefix*\nHoulsby\nParallel*\nHoulsby\nParallel*\nYouCookII\nFigure 4: Best and worst performing adapters with BERT. The y-axis represents the performance in terms of mIoU.\nThe circles, triangles and squares indicate the performance with frozen BERT, while the horizontal ticks show the\nperformance of the models with the corresponding adapter. The * indicates cases where the models could not learn\nproperly with the adapter.\ning on cases that deliver the best and the worst\nperformance.\nThe results of our analysis are summarized in\nFigure 4. We can see that no single adapter can\nconsistently offer the best performance. However,\nit is possible to see that the bottleneck adapters,\nsuch as HOULSBY , PFEIFFER , and INVERSE , can\ndeliver an overall better performance.\nWhen controlling our results for each dataset, it\nis possible to see that the PREFIX and the INVERSE\nadapters worked well on Charades-STA, while the\nPFEIFFER and HOULSBY adapters worked best\nfor ActivityNet and YouCookII, respectively. We\nsurmise this indicates that the adapter choice is\nlikely more related to the training data than to the\nchoice of the model itself. Controlling by model\nalso shows interesting, distinctive patterns. For ex-\nample, while the PREFIX adapter worked well for\nTMLGA on Charades-STA and DORi on Activi-\ntyNet, it performed poorly with ExCL on Activi-\ntyNet and YouCookII.\nAnother general pattern we could identify is that\nboth the COMPACTER and the PARALLEL adapters\ndid not work well overall, with the latter strug-\ngling to deliver good performance in all cases. We\nthink the number of additional parameters may\nplay a role in this matter, as COMPACTER is the\nadapter with the smallest number of added parame-\nters (0.06M) and it might be that too small to learn\nuseful information for TVG.\nFurthermore, the hyper-parameter search with\nthe PREFIX and the PARALLEL adapters was sub-\nstantially more challenging. In particular, with the\nPARALLEL adapter, we faced gradient explosions\nin many instances, as with DORi on Charades-STA,\nor the model converged to poor performance, as\nwith all three models on YouCookII.\nModel Charades-STA ActivityNet\nR@0.5 R@0.7 mIoU R@0.5 R@0.7 mIoU\nTMLGA (ours) 49.97 32.72 48.29 31.13 17.86 36.90\n+ BERT/enc-9649.92 31.42 48.34 32.57 18.64 37.63\n+ PFEIFFER 51.59 33.41 49.50 35.2020.4338.88\n+ INVERSE 52.77 34.4949.33 33.76 19.93 37.93\n+ Fine-tuning 52.53 33.52 49.8034.05 19.51 37.92\n+ RoBERTa/enc-9651.34 33.49 48.91 33.80 19.62 37.89\n+ PFEIFFER 53.84 34.7849.9135.27 20.26 38.77\n+ INVERSE 52.69 33.98 49.50 34.85 20.4639.35\n+ Fine-tuning 53.15 33.33 49.77 33.21 19.69 38.47\n+ DeBERTa/enc-9652.53 33.49 49.32 33.94 20.22 38.72\n+ PFEIFFER 53.49 34.6549.7834.70 20.49 39.30\n+ INVERSE 52.58 33.95 49.6635.45 20.6639.71\n+ Fine-tuning 53.44 33.44 49.63 33.78 20.12 38.92\nTable 2: Detailed results combining the TMLGA model\nwith our three PLMs and adapters, tested on Charades-\nSTA and ActivityNet Captions. Underlined results indi-\ncate the best performance within the model and dataset\ncombination, while the results in bold indicate the best\nperformance within the dataset.\nFinally, we believe another factor is the loca-\ntion where each adapter is inserted in the PLM\narchitecture. All of the methods tested adapt to\nspecific parts of the transformer layer, except for\nthe PARALLEL adapter, which adapts the whole\ntransformer layer. Its consistent poor performance\nmight indicate that adapting specific parts of the\ntransformer layer is more beneficial for our task.\nRQ4: Impact of different PLMs After the re-\nlease of BERT, several pre-trained language en-\ncoder variations have been proposed. When tested\non extensive NLP benchmarks, some of these mod-\nels have proven to be able to consistently improve\nperformance. In this context, we are interested in\nstudying if such performance improvements also\n13107\ntranslate to better performance in our task.\nThus, we study the performance of BERT,\nRoBERTa, and DeBERTa on two datasets,\nCharades-STA and ActivityNet, using TMLGA\nas a pivot. Our choice of TVG model is guided\nby our experiments with BERT, where TMLGA\noffered a good compromise in terms of perfor-\nmance improvements versus computational cost.\nFurthermore, our dataset selection is motivated\nby Charades-STA and ActivityNet having simi-\nlar video contents but with different complexity\nqueries, i.e., queries in Charades-STA are much\nsimpler than in ActivityNet. We can verify this\ndifference by observing the vocabulary size (748\nvs. 9,744 tokens) and length of the queries (7.2 vs.\n13.48 tokens per query).\nThe results are summarized in Table 2. For better\nreadability, we only included two αbands. Further\nresults can be seen in Appendix B. We can first\nsee that for all three PLMs, using adapters led to\nan overall performance improvement compared to\nthe frozen PLM. Moreover, we noticed that PFEIF -\nFER and the INVERSE adapters seem to provide the\nbest results in both datasets and all PLMs.\nMoreover, while DeBERTa performed the best\nfor ActivityNet, as expected from its performance\nin NLP downstream tasks, its best performance was\nsimilar to BERT for Charades-STA. We believe this\nresult could be due to the simplicity of the queries\nin Charades-STA, which might not require all the\nadditional information DeBERTa encodes. In ad-\ndition, these results were obtained using the first\nversion of DeBERTa, which uses the same type\nof tokenizer as BERT. We also tested the newer\nversion of DeBERTa, which uses a sentencepiece-\nbased tokenizer and incorporates other model im-\nprovements. However, this model performed much\nworse than the first version on our task showing that\nusing sophisticated tokenizers does not necessarily\nimprove results with simple sentence queries.\nOn the other hand, ActivityNet has longer and\nmore complex queries and our results indicate that\nin such cases, our task might benefit from using\nbetter PLMs. Nevertheless, the best results were\nachieved when using adapters.\nRQ5: Comparison against state-of-the-art\nWe finally compare our best-performing models\nagainst a selection of approaches from previous\nwork. We achieved our best performance for the\nCharades-STA dataset by using DORi with De-\nBERTa+PFEIFFER ; and for the ActivityNet dataset,\nModel Charades-STA ActivityNet\nR@0.5 R@0.7 mIoU R@0.5 R@0.7 mIoU\nProposal-free\nDORi (ours) 57.18 40.62 53.01 40.00 24.84 41.97\n+ DeBERTa/enc-9658.17 40.94 52.73 41.65 25.82 43.64\n+ Adapter 58.39 41.61 53.34 45.6328.7445.70\nVSLNet 54.19 35.22 50.02 43.22 26.16 43.19\nCPNet 60.27 38.74 52.00 40.56 21.63 40.65\nCPN 59.77 36.67 53.14 45.10 28.1045.70\nBCPN 61.77 43.91 - 44.53 30.11 -\nProposal-based\nMS-2D-TAN60.08 37.39 - 45.50 28.28 -\nMSAT - - - 48.02 31.78 -\nCPL 49.24 22.39 - 55.73 31.37 -\nMNM 47.31 27.28 - 48.59 29.26 -\nTable 3: Comparison between the best performing TVG\nmodel with DeBERTa and current state-of-the-art meth-\nods in TVG. Results for all compared methods were\ntaken from their respective papers. The best results for\neach combination of method type (i.e., proposal-free or\nproposal-based) and dataset are indicated in bold, while\nthe second-best results are underlined.\nDORi with DeBERTa+INVERSE . Since the mod-\nels used in this study are proposal-free, we mainly\ncompare to proposal-free methods, i.e., VSLNet,\nCPN, CPNet and BCPN. Nevertheless, we are also\ninterested in observing how our modifications per-\nform against proposal-based methods, such as MS-\n2D-TAN (Zhang et al., 2021c), MSAT, CPL, and\nMNM. We note that out of the proposal-free meth-\nods, only BCPN uses a PLM (BERT). As for the\nproposal-based methods, only MS-2D-TAN does\nnot use any form of Transformers in its architecture,\nwhile MNM is the only one to use a fine-tuned PLM\n(DistilBERT). Finally, while MSAT and CPL use\nTransformers in their architecture to encode visual\nand textual information, they do not use PLMs.\nThe performance summary is shown in Table 3.\nFor this analysis, we only consider two αbands for\nthe thresholds as most proposal-based models do\nnot report results at the α0.3.\nFirst, we can see that the best-performing meth-\nods are proposal-free, with BCPN achieving the\nbest performance in most of the considered metrics.\nFurthermore, looking at the mean tIoU, we can see\nthat DORi already performs well against the other\nmethods. Replacing GloVe embeddings with De-\nBERTa and using adapters for training provides a\nsignificant performance boost, delivering results\nequivalent to state-of-the-art models.\nMoreover, it is interesting to see that while\n13108\nproposal-based methods such as CPL and MNM\nperformed well against all methods on ActivityNet,\nthey could not outperform methods without Trans-\nformers on the Charades-STA dataset. In contrast,\nDORi with DeBERTa and adapters achieved a more\nbalanced performance among both datasets, show-\ning a clear advantage against these methods on the\nCharades-STA dataset.\nTherefore, our results show how TVG models\ncan greatly benefit from adequately incorporating\nPLMs and making use of parameter-efficient tech-\nniques, performing well on datasets with different\ncomplexity levels in terms of queries, and achiev-\ning results comparable to state-of-the-art methods.\n6 Conclusions\nThis paper studied the effects of PLMs in the\nTVG task and assessed the applicability of NLP\nparameter-efficient training alternatives based on\nadapters. We coupled BERT, RoBERTa, and De-\nBERTa, with a selection of previous TVG works,\ni.e., ExCL, TMLGA, and DORi, and tested differ-\nent adapters to reduce the impact of the additional\nparameters. Our results showed that, by only chang-\ning the query representation using PLMs, TVG\nmodels can greatly benefit from such integration,\nespecially when PLMs are fine-tuned, highlighting\nthe importance of the query representation in this\ntask. Moreover, we verified that adapters are an\neffective alternative to full fine-tuning, even though\nthey were not tailored for our task. They saved\non computational cost, allowing improvements for\nlarger TVG models, such as DORi, and also deliv-\nered results comparable to SOTA models. Finally,\nwe observed that while PARALLEL adapters strug-\ngled to learn in this task, bottleneck adapters such\nas HOULSBY and PFEIFFER performed across all\ntested TVG models and datasets.\nLimitations\nIn this work, we studied the effects of large pre-\ntrained models in the temporal video ground-\ning task and investigated the applicability of\nNLP adapters for a parameter-efficient integration.\nWhile we believe our results show the efficacy of in-\ncorporating better language models in TVG models,\nit is important to note that we primarily focused\non proposal-free TVG models and thus have no\nevidence to suggest such improvement would be\nobserved in proposal-based models.\nFurthermore, as our main goal was to investi-\ngate how the chosen models’ performance varied\nwhen only changing the text encoding models, we\ncompared state-of-the-art models using different\nvisual features. While it would be interesting and\ninsightful to check their performance when using\nthe same features as our chosen models (i.e., I3D),\nsuch experiments are out of the scope of this study.\nMoreover, although language adapters can be\nstacked before a task adapter for training on the\ntask in a new language, we have only experimented\nwith queries in English. It would be interesting to\ninvestigate if language adapters could be applied to\nTVG in different languages.\nFinally, as for hardware requirements, our exper-\niments were performed on a single 40-GB NVIDIA\nA100 GPU from a large cluster, and we spent about\n400 USD on our experimental setup. While ex-\nperiments with ExCL and TMLGA can be run on\nsmaller GPUs with no significant increase in train-\ning time (i.e., we tested with a 16-GB NVIDIA\nV100 GPU), for DORi, due to the size of the in-\nput features and number of training parameters, we\nrecommend using a GPU with at least 32GB of\nmemory.\nEthics Statement\nThis work does not present any direct ethical is-\nsues. Code for TMLGA and DORi were released\nby their respective authors. The datasets used to\nevaluate our proposed approach are open-access,\nand data characteristics relevant to our task were\ndescribed in the experimental evaluation section.\nReferences for further information on each dataset\nwere included in the paper.\nAcknowledgements\nThis paper is based on results obtained from the\nproject JPNP20006, commissioned by the New\nEnergy and Industrial Technology Development\nOrganization (NEDO). For experiments, compu-\ntational resource of AI Bridging Cloud Infrastruc-\nture (ABCI) provided by National Institute of Ad-\nvanced Industrial Science and Technology (AIST)\nwas used.\nReferences\nAnkur Bapna and Orhan Firat. 2019. Simple, Scal-\nable Adaptation for Neural Machine Translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n13109\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel.\n2022. BitFit: Simple Parameter-efficient Fine-tuning\nfor Transformer-based Masked Language-models. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 1–9, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nJingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and\nTat-Seng Chua. 2018. Temporally grounding natural\nsentence in video. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 162–171, Brussels, Belgium. As-\nsociation for Computational Linguistics.\nShaoxiang Chen and Yu-Gang Jiang. 2019. Semantic\nproposal for activity localizaiton in videos via sen-\ntence query. AAAI.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed\nEl Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: UNiversal Image-\nTExt Representation Learning. In Computer Vision\n– ECCV 2020, Lecture Notes in Computer Science,\npages 104–120, Cham. Springer International Pub-\nlishing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJiyang Gao, Chen Sun, Zhenheng Yang, and Ram Neva-\ntia. 2017. Tall: Temporal activity localization via\nlanguage query. In ICCV.\nRunzhou Ge, Jiyang Gao, Kan Chen, and Ram Nevatia.\n2019. Mac: Mining activity concepts for language-\nbased temporal localization. In WACV.\nSoham Ghosh, Anuva Agarwal, Zarana Parekh, and\nAlexander Hauptmann. 2019. ExCL: Extractive Clip\nLocalization Using Natural Language Descriptions.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nRohit Girdhar, Joao Carreira, Carl Doersch, and An-\ndrew Zisserman. 2019. Video Action Transformer\nNetwork. In CVPR, pages 244–253.\nChunhui Gu, Chen Sun, David A. Ross, Carl V ondrick,\nCaroline Pantofaru, Yeqing Li, Sudheendra Vijaya-\nnarasimhan, George Toderici, Susanna Ricco, Rahul\nSukthankar, Cordelia Schmid, and Jitendra Malik.\n2018. A V A: A Video Dataset of Spatio-Temporally\nLocalized Atomic Visual Actions. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 6047–6056.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2022. PPT: Pre-trained Prompt Tuning for Few-shot\nLearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8410–8423, Dublin,\nIreland. Association for Computational Linguistics.\nDemi Guo, Alexander Rush, and Yoon Kim. 2021.\nParameter-Efficient Transfer Learning with Diff Prun-\ning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n4884–4896, Online. Association for Computational\nLinguistics.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022a. Towards a\nUnified View of Parameter-Efficient Transfer Learn-\ning. In International Conference on Learning Repre-\nsentations.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2022b. DeBERTa: Decoding-\nEnhanced BERT with Disentangled Attention. In\nInternational Conference on Learning Representa-\ntions.\nXuehai He, Chunyuan Li, Pengchuan Zhang, Jianwei\nYang, and Xin Eric Wang. 2022c. Parameter-efficient\nFine-tuning for Vision Transformers.\nLisa Anne Hendricks, Oliver Wang, Eli Shechtman,\nJosef Sivic, Trevor Darrell, and Bryan Russell. 2017.\nLocalizing moments in video with natural language.\nIn ICCV.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-Efficient Transfer Learning for NLP. In\nProceedings of the 36th International Conference on\nMachine Learning, pages 2790–2799. PMLR.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. LoRA: Low-Rank Adaptation\nof Large Language Models. In International Confer-\nence on Learning Representations.\nYang Jin, Zehuan Yuan, Yadong Mu, et al. 2022. Em-\nbracing consistency: A one-stage approach for spatio-\ntemporal video grounding. Advances in Neural Infor-\nmation Processing Systems, 35:29192–29204.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021a. Compacter: Efficient Low-\nRank Hypercomplex Adapter Layers. In Advances in\nNeural Information Processing Systems, volume 34,\npages 1022–1035. Curran Associates, Inc.\n13110\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021b. Parameter-\nefficient Multi-task Fine-tuning for Transformers via\nShared Hypernetworks. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 565–576, Online. Association\nfor Computational Linguistics.\nKonwoo Kim, Michael Laskin, Igor Mordatch, and\nDeepak Pathak. 2021. How to Adapt Your Large-\nScale Vision-and-Language Model.\nDiederik P. Kingma and Jimmy Ba. 2020. Adam:\nA Method for Stochastic Optimization. In ICLR\n(Poster).\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,\nand Juan Carlos Niebles. 2017. Dense-Captioning\nEvents in Videos. In Proceedings of the IEEE In-\nternational Conference on Computer Vision, pages\n706–715.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe Power of Scale for Parameter-Efficient Prompt\nTuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nKun Li, Dan Guo, and Meng Wang. 2021. Proposal-\nFree Video Grounding with Contextual Pyramid Net-\nwork. Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, 35(3):1902–1910.\nXiang Lisa Li and Percy Liang. 2021. Prefix-Tuning:\nOptimizing Continuous Prompts for Generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang,\nXiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,\nLi Dong, Furu Wei, Yejin Choi, and Jianfeng Gao.\n2020. Oscar: Object-Semantics Aligned Pre-training\nfor Vision-Language Tasks. In Computer Vision –\nECCV 2020, Lecture Notes in Computer Science,\npages 121–137, Cham. Springer International Pub-\nlishing.\nMeng Liu, Xiang Wang, Liqiang Nie, Xiangnan He,\nBaoquan Chen, and Tat-Seng Chua. 2018. Attentive\nmoment retrieval in videos. In The 41st International\nACM SIGIR Conference on Research & Development\nin Information Retrieval, pages 15–24. ACM.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021. Parameter-\nefficient multi-task fine-tuning for transformers via\nshared hypernetworks. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 565–576.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Scott Yih, and Madian\nKhabsa. 2022. UniPELT: A Unified Framework for\nParameter-Efficient Language Model Tuning. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 6253–6264, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nJonghwan Mun, Minsu Cho, and Bohyung Han. 2020.\nLocal-Global Video-Text Interactions for Temporal\nGrounding. In 2020 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n10807–10816, Seattle, W A, USA. IEEE.\nHafiza Sadia Nawaz, Zhensheng Shi, Yanhai Gan,\nAmanuel Hirpa, Junyu Dong, and Haiyong Zheng.\n2022. Temporal Moment Localization via Natural\nLanguage by Utilizing Video Question Answers as\na Special Variant and Bypassing NLP for Corpora.\nIEEE Transactions on Circuits and Systems for Video\nTechnology, pages 1–1.\nJunting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hong-\nsheng Li. 2022. ST-Adapter: Parameter-Efficient\nImage-to-Video Transfer Learning for Action Recog-\nnition.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(EMNLP), pages 1532–1543.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020a. Adapterhub: A\nframework for adapting transformers. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: System Demonstra-\ntions, pages 46–54.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\n13111\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning Transferable Visual Models From Natural Lan-\nguage Supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, pages\n8748–8763. PMLR.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural In-\nformation Processing Systems, volume 30. Curran\nAssociates, Inc.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2018. Efficient Parametrization of Multi-\nDomain Deep Neural Networks. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 8119–8127.\nCristian Rodriguez, Edison Marrese-Taylor, Fatemeh Sa-\ndat Saleh, Hongdong Li, and Stephen Gould. 2020.\nProposal-free Temporal Moment Localization of a\nNatural-Language Query in Video using Guided At-\ntention. In Proceedings of the IEEE/CVF Winter Con-\nference on Applications of Computer Vision, pages\n2464–2473.\nCristian Rodriguez-Opazo, Edison Marrese-Taylor, Ba-\nsura Fernando, Hongdong Li, and Stephen Gould.\n2021. DORi: Discovering Object Relationships for\nMoment Localization of a Natural Language Query\nin a Video. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision,\npages 1079–1088.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What We Know\nAbout How BERT Works. Transactions of the Asso-\nciation for Computational Linguistics, 8:842–866.\nZheng Shou, Dongang Wang, and Shih-Fu Chang. 2016.\nTemporal action localization in untrimmed videos\nvia multi-stage cnns. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nGunnar A. Sigurdsson, Gül Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta. 2016. Hol-\nlywood in Homes: Crowdsourcing Data Collection\nfor Activity Understanding. In Computer Vision –\nECCV 2016, Lecture Notes in Computer Science,\npages 510–526, Cham. Springer International Pub-\nlishing.\nRui Su, Qian Yu, and Dong Xu. 2021. Stvgbert: A\nvisual-linguistic transformer based framework for\nspatio-temporal video grounding. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 1533–1542.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nVL-Adapter: Parameter-Efficient Transfer Learning\nfor Vision-and-Language Tasks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5227–5237.\nYi-Lin Sung, Varun Nair, and Colin A Raffel. 2021.\nTraining Neural Networks with Fixed Sparse Masks.\nIn Advances in Neural Information Processing Sys-\ntems, volume 34, pages 24193–24205. Curran Asso-\nciates, Inc.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal Few-Shot Learning with Frozen Lan-\nguage Models. In Advances in Neural Information\nProcessing Systems, volume 34, pages 200–212. Cur-\nran Associates, Inc.\nZhenzhi Wang, Limin Wang, Tao Wu, Tianhao Li,\nand Gangshan Wu. 2022. Negative Sample Mat-\nters: A Renaissance of Metric Learning for Temporal\nGrounding. Proceedings of the AAAI Conference on\nArtificial Intelligence, 36(3):2613–2623.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nHuijuan Xu, Kun He, L Sigal, S Sclaroff, and K Saenko.\n2019. Multilevel language and vision integration for\ntext-to-clip retrieval. In AAAI.\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,\nand Cordelia Schmid. 2022a. Tubedetr: Spatio-\ntemporal video grounding with transformers. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16442–16453.\nAntoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,\nand Cordelia Schmid. 2022b. Zero-Shot Video Ques-\ntion Answering via Frozen Bidirectional Language\nModels.\nZekun Yang, Noa Garcia, Chenhui Chu, Mayu Otani,\nYuta Nakashima, and Haruo Takemura. 2020. BERT\nrepresentations for Video Question Answering. In\nProceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 1556–1565.\nRunhao Zeng, Haoming Xu, Wenbing Huang, Pei-\nhao Chen, Mingkui Tan, and Chuang Gan. 2020.\nDense Regression Network for Video Grounding.\narXiv:2004.03545 [cs].\nHao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.\n2020. Span-based Localizing Network for Natural\nLanguage Video Localization. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6543–6554, Online. Asso-\nciation for Computational Linguistics.\n13112\nMingxing Zhang, Yang Yang, Xinghan Chen, Yanli Ji,\nXing Xu, Jingjing Li, and Heng Tao Shen. 2021a.\nMulti-Stage Aggregated Transformer Network for\nTemporal Language Localization in Videos. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12669–12678.\nRenrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,\nKunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng\nLi. 2021b. Tip-Adapter: Training-free CLIP-Adapter\nfor Better Vision-Language Modeling.\nSongyang Zhang, Houwen Peng, Jianlong Fu, Yijuan\nLu, and Jiebo Luo. 2021c. Multi-Scale 2D Temporal\nAdjacency Networks for Moment Localization with\nNatural Language. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, pages 1–1.\nYang Zhao, Zhou Zhao, Zhu Zhang, and Zhijie Lin.\n2021. Cascaded Prediction Network via Segment\nTree for Temporal Video Grounding. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4197–4206.\nMinghang Zheng, Yanjie Huang, Qingchao Chen, Yuxin\nPeng, and Yang Liu. 2022. Weakly Supervised Tem-\nporal Sentence Grounding With Gaussian-Based Con-\ntrastive Proposal Learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 15555–15564.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022. Learning to Prompt for Vision-\nLanguage Models. International Journal of Com-\nputer Vision, 130(9):2337–2348.\nLuowei Zhou, Nathan Louis, and Jason J. Corso. 2018a.\nWeakly-Supervised Video Object Grounding from\nText by Loss Weighting and Object Interaction.\narXiv:1805.02834 [cs].\nLuowei Zhou, Chenliang Xu, and Jason J. Corso. 2018b.\nTowards Automatic Learning of Procedures From\nWeb Instructional Videos. In Thirty-Second AAAI\nConference on Artificial Intelligence.\n13113\nA Detailed Results with BERT\nWe report the detailed results with BERT for ExCL,\nTMLGA, and DORi in Tables 4, 5 and 6, re-\nspectively. We also report the average runtime for\ntraining and inference of each TVG model when\ncombined with BERT and the selected adapters in\nTable 7. However, we note that these values also in-\nclude the loading time of the samples, which varied\naccordind to the usage of the cluster during each\nexperiment.\nA.1 Hyper-parameters\nTo reproduce the results of the original models, we\nstarted by using the hyper-parameters reported in\nthe respective papers, achieving close to reported\nperformance for ExCL and TMLGA. Reproducing\nDORi results was slightly more challenging, where\nwe had to experiment with different weight decays,\nbatch sizes and steps for the learning scheduler.\nWhen adding the PLMs and the adapters, we\nfirst started with the same set of hyper-parameters\nof the original model. In general, it was neces-\nsary to slightly change them for the model to prop-\nerly learn. The hyper-parameters used to train\nExCL are specified in Table 8 , Table 9, and Ta-\nble 10 for Charades-STA, ActivityNet Captions and\nYouCookII datasets, respectively; to train TMLGA,\nin Table 11, Table 12, and Table 13, respectively;\nand to train DORi, in Table 14, Table 15, and Ta-\nble 16, respectively.\nA.2 Notes on training\nSpecifically for the fine-tuning of BERT with\nTMLGA on the Charades-STA dataset, we report\nthe results obtained by applying the linear warm-\nup to all parameters, including non-BERT ones,\nas this strategy led to the best results. Moreover,\nwhen training DORi with BERT and thePARALLEL\nadapter on the Charades-STA dataset, we tested dif-\nferent weight decays, but the gradient exploded\nin all cases. The reported results were obtained\nwith a weight decay of 1e−5, the best result be-\nfore the gradient exploded. Finally, we could not\nfind a proper hyper-parameter combination so that\nthe models could learn on the YouCookII with the\nPARALLEL adapter.\nB Detailed Results with RoBERTa and\nDeBERTa\nDetailed results with RoBERTa and DeBERTa with\nTMLGA on the Charades-STA and ActivityNet\nCaptions datasets can be found on table 17. This ta-\nbles is an expansion of the results shown in table 2\nin the main text. We did not include results with\nPREFIX adapters on DeBERTa due to an implemen-\ntation error on the adapter-transformers library 3.\nFurthermore, we also note that in our experiments\nwith the PARALLEL adapter, all the models satu-\nrated and reached the same performance on Activi-\ntyNet, without properly learning.\nB.1 Hyper-parameters\nWe report the hyper-parameters used to train\nTMLGA with RoBERTa on Charades-STA and Ac-\ntivityNet Captions on Table 18 and Table 19, re-\nspectively. We also report the hyper-parameters\nused to train TMLGA with DeBERTa on both\ndatasets on Table 20 and Table 21.\nB.2 Hyper-parameters for best result\nOur best results on the Charades-STA dataset was\nachieved by training DORi with DeBerta, using\nthe PFEIFFER adapter, while the best results on\nthe ActivityNet Captions was achieved by training\nDORi with DeBERTa, using the INVERSE adapter.\nThe hyper-parameters used to achieve these results\ncan be found in Table 22.\nC Qualitative Results\nFinally, we provide a few examples of failure and\nsuccess of each analyzed TVG model along with\nBERT. Figure 5 shows examples for DORi and\nFigure 6 shows examples for ExCL.\n3Version 3.1.0a0 of the adapter-transformers library.\n13114\nQuery: she holds up the product and grabs the horsestail.\nGTAdapterFrozenDORi\nQuery: The room turns dark and the two put the pumpkin over a candle and the pumpkin lights up with a carved pumpkin face with a hat.\nGTAdapterFrozenDORi\nFigure 5: Examples showing the effects of BERT and\nadapters with DORi on the ActivityNet. The top image\nshows an example of a significant performance improve-\nment only when using adapters. On the other hand, the\nbottom image shows an example where the frozen PLM\nwas sufficient to correctly identify the video segment\nrepresented by the query.\nQuery: We see the man talk and the screen fades to black.\nGTAdapterFinetuningFrozenExCL\nQuery: A lemonade drink is displayed on a counter.\nGTAdapterFinetuningFrozenExCL\nFigure 6: Examples showing the effects of BERT and\nadapters with ExCL on the ActivityNet. Both im-\nages show the significant impact of using PLMs on\nthis model’s performance, drastically improving results\nwhen incorporating frozen BERT. However, while in\nthe top image, the best results were achieved by training\nusing adapters, in the bottom image, we can see that the\nfrozen PLM was sufficient to solve the respective query.\n13115\nMethod Params. Charades-STA ActivityNet YouCookII\nR@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU\nExCL (ours) 6.9M 62.28 39.74 22.53 42.28 55.49 39.33 23.04 40.32 26.58 15.72 8.19 18.99\n+ BERT/enc-966.9M 62.93 38.44 22.23 42.38 57.21 39.66 23.79 41.45 26.63 16.15 8.51 18.87\n+ HOULSBY 8.7M 60.08 36.26 20.83 40.22 57.79 39.77 23.80 41.95 25.77 15.12 8.36 18.52\n+ PFEIFFER 7.8M 61.59 37.15 21.51 41.06 59.35 41.27 24.86 42.83 27.18 16.15 8.88 19.46\n+ INVERSE 8.1M 60.81 37.69 21.10 40.85 57.58 40.34 24.79 42.06 26.72 16.44 8.79 19.43\n+ PREFIX 16.8M 60.54 35.24 20.22 40.98 57.49 39.55 24.10 41.77 4.93 1.83 0.66 0.06\n+ COMPACTER 7.0M 59.19 35.38 20.97 39.99 57.82 39.31 23.86 41.89 27.81 16.67 9.08 19.57\n+ LORA 7.2M 60.94 36.32 21.48 40.94 57.94 40.64 25.05 42.49 28.47 17.75 9.02 19.89\n+ PARALLEL 14.0M 48.52 16.61 9.44 33.77 58.60 40.99 24.73 42.26 6.41 2.58 1.12 0.06\n+ Fine-tuning 116M 61.75 38.36 23.44 42.00 59.10 41.83 25.42 42.36 28.18 16.84 9.02 20.08\nTable 4: Detailed results for ExCL using BERT. Underlined results indicate the best adapter performance, while\nresults in bold indicate the best performance within the dataset.\nMethod Params. Charades-STA ActivityNet YouCookII\nR@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU\nTMLGA (ours) 4.7M 69.49 49.97 32.72 48.29 50.84 31.13 17.86 36.90 34.42 21.99 10.94 23.63\n+ BERT/enc-964.7M 70.08 49.92 31.42 48.34 52.10 32.57 18.64 37.63 34.77 23.05 12.49 24.42\n+ HOULSBY 8.7M 70.97 51.69 33.84 49.31 53.98 34.13 19.48 38.5736.08 22.77 12.49 25.19\n+ PFEIFFER 7.8M 71.64 51.59 33.41 49.5053.98 35.20 20.43 38.88 34.31 21.76 11.31 23.54\n+ INVERSE 8.1M 71.02 52.77 34.49 49.33 52.47 33.76 19.93 37.93 35.40 22.05 11.14 24.44\n+ PREFIX 16.8M 71.40 52.53 33.82 49.5753.61 34.03 19.89 38.34 18.36 10.17 4.64 13.62\n+ COMPACTER 7.0M 70.27 49.73 31.37 48.31 52.15 33.85 19.62 37.78 35.34 22.25 11.05 24.13\n+ LORA 7.2M 70.94 50.24 32.15 48.81 51.90 32.81 18.77 37.37 35.88 22.65 11.91 24.51\n+ PARALLEL 14.0M 70.48 51.64 33.66 49.33 43.50 23.20 11.59 32.29 5.98 2.36 1.00 0.06\n+ Fine-tuning 114M 71.02 52.53 33.52 49.80 53.59 34.05 19.51 37.92 35.34 21.85 11.63 24.82\nTable 5: Detailed results for TMLGA using BERT. Underlined results indicate the best adapter performance, while\nresults in bold indicate the best performance within the dataset.\nMethod Params. Charades-STA ActivityNet YouCookII\nR@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU\nDORi (ours.) 10.4M 72.26 57.18 40.62 53.01 57.38 40.00 24.84 41.97 43.33 29.15 17.61 30.17\n+ BERT/enc-9610.4M 71.83 57.15 39.22 52.49 58.86 40.86 25.50 42.97 42.27 29.90 18.38 29.92\n+ HOULSBY 8.7M 72.72 57.58 40.59 53.16 60.71 43.18 26.97 43.93 46.79 32.56 19.87 32.48\n+ PFEIFFER 7.8M 72.28 58.49 40.89 53.13 60.67 43.53 27.33 44.30 44.96 31.90 19.39 31.48\n+ INVERSE 8.1M 72.50 58.63 40.97 53.29 61.01 43.90 27.68 44.32 45.50 30.76 19.13 31.48\n+ PREFIX 16.8M 71.99 57.69 40.67 52.94 60.81 43.49 27.86 44.55 45.59 31.90 19.44 31.53\n+ COMPACTER 7.0M 72.63 57.98 40.83 52.93 60.73 43.03 27.40 44.31 43.81 30.13 18.36 30.58\n+ LORA 7.2M 70.73 57.31 39.76 51.89 60.90 43.34 27.46 44.42 45.42 31.44 19.47 31.56\n+ PARALLEL 14.0M 42.18 9.65 5.08 30.15 52.94 35.23 21.92 39.00 5.01 1.75 0.66 0.06\n+ Fine-tuning 120M OOM OOM OOM\nTable 6: Detailed results for DORi using BERT. Underlined results indicate the best adapter performance, while\nresults in bold indicate the best performance within the dataset.\n13116\nMethod Charades-STA ActivityNet YouCookII\nTrain Inference Train Inference Train Inference\nExCL (ours) 33.40±0.88 10.70±0.97 814.00 ±79.44 425.00±551.66 170.37±0.92 63.85 ±61.79\n+ BERT/enc-9623.91±1.97 7.83 ±0.38 829.40 ±39.20 151.80± 7.39 169.53±1.01 39.44 ±0.62\n+ Adapter 32.18 ±1.32 10.00±0.47 833.00 ±25.31 102.85±11.52 94.33±0.88 21.81 ±0.60\n+ Fine-tuning 45.40±2.67 10.10±0.78 1883.00±72.12 180.00± 1.41 306.18±66.19 36.27 ±1.73\nTMLGA (ours) 25.04±0.74 15.44±0.82 884.62±10.56 123.75±7.36 140.90±2.02 43.20 ±1.470\n+ BERT/enc-9625.50±0.85 15.93±0.73 454.18± 2.40 82.00 ±1.00 163.27±9.66 50.72 ±47.19\n+ Adapter 29.65 ±0.77 17.95±0.78 658.19±24.41 136.44±4.24 166.58±10.26 49.33 ±47.13\n+ Fine-tuning 34.89±0.93 16.55±0.72 768.00±219.64 137.92±28.16 172.46±1.51 37.61 ±0.87\nDORi (ours.) 631.24±1.13 138.20±0.75 6704.00± 1.41 1040.00±4.24 2101.00±81.99 845.88±1054.53\n+ BERT/enc-96607.83±1.52 134.67±0.72 6599.00±21.66 1357.33±23.46 2221.50±72.83 667.50±1593.11\n+ Adapter 633.14 ±1.34 130.14±0.69 7415.00±123.30 1405.75±34.35 2279.50±15.93 403.25± 29.06\nTable 7: Average runtime in seconds for training and evaluating each selected TVG model when combining with\nBERT and adapters.\n13117\nHyper-parameter Method Value\nBatch size Finetuning 64\nOthers 32\nBase LR Rep., Frozen, Finetuning 1E-03\nOthers 1E-04\nStep\nRep. 6\nFrozen 5\nOthers -\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.1\n# Epochs 15\nGamma\nAll\n1E-02\nWeight Decay 1E-05\nTable 8: Hyper-parameters used to train ExCL with\nBERT on the Charades-STA dataset.\nHyper-parameter Method Value\nBatch size Reproduction 32\nOthers 64\nBase LR\nPFEIFFER, PARALLEL,\nCOMPACTER, PREFIX 1E-04\nOthers 1E-03\nStep Finetuning -\nOthers 5\nWeight Decay\nPFEIFFER, PARALLEL,\nCOMPACTER, PREFIX 1E-04\nOthers 1E-06\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.2\n# Epochs 15\nGamma All 1E-02\nTable 9: Hyper-parameters used to train ExCL with\nBERT on the ActivityNet dataset.\nHyper-parameter Method Value\nBatch size\nAll\n32\nBase LR 1E-03\nGamma 1E-02\nStep LoRa 8\nOthers -\nWeight Decay Finetunning, Frozen 1E-05\nOthers 1E-04\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.2\n# Epochs 15\nTable 10: Hyper-parameters used to train ExCL with\nBERT on the YouCookII dataset.\nHyper-parameter Method Value\nBatch size\nAll\n256\nBase LR 1E-04\nWeight Decay 1E-05\nGamma 1E-02\nStep Finetuning -\nOthers 6\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.1\n# Epochs 20\nTable 11: Hyper-parameters used to train TMLGA with\nBERT on the Charades-STA dataset.\nHyper-parameter Method Value\nBatch size\nAll\n64\nBase LR 1E-04\nWeight Decay 1E-05\nGamma 1E-02\nStep Finetuning -\nOthers 5\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.1\n# Epochs 15\nTable 12: Hyper-parameters used to train TMLGA with\nBERT on the ActivityNet dataset.\nHyper-parameter Method Value\nBatch size\nAll\n64\nBase LR 1E-03\nStep 6\nGamma 1E-02\nWeight Decay Prefix 1E-04\nOthers 1E-05\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.2\n# Epochs 15\nTable 13: Hyper-parameters used to train TMLGA with\nBERT on the YouCookII dataset.\nHyper-parameter Method Value\nBatch size\nAll\n5\nBase LR 1E-04\nGamma 1E-02\nWeight Decay Frozen 1E-04\nOthers 1E-05\nStep\nPREFIX 3\nHOULSBY, PFEIFFER, 4LORA\nINVERSE, COMPACTER 5\nFrozen 6\nTable 14: Hyper-parameters used to train DORi with\nBERT on the Charades-STA dataset.\n13118\nHyper-parameter Method Value\nBatch Size Rep., Frozen 8\nOthers 4\nBase LR All 1E-04\nGamma 1E-02\nWeight Decay Rep., Frozen 1E-04\nOthers 1E-05\nStep\nRep. 3\nAll adapters 4\nFrozen 6\nTable 15: Hyper-parameters used to train DORi with\nBERT on the ActivityNet Captions dataset.\nHyper-parameter Method Value\nBatch Size Rep. 2\nOthers 4\nBase LR All 1E-04\nStep 6\nWeight Decay Rep. 1E-05\nOthers 1E-04\nGamma Rep. 1E-02\nOthers 1E-01\nTable 16: Hyper-parameters used to train DORi with\nBERT on the YouCookII dataset.\n13119\nModel Charades-STA ActivityNet\nR@0.3 R@0.5 R@0.7 mIoU R@0.3 R@0.5 R@0.7 mIoU\nTMLGA (orig.) 67.53 52.02 33.74 48.22 51.28 33.04 19.26 37.78\nTMLGA (ours) 69.49 49.97 32.72 48.29 50.84 31.13 17.86 36.90\n+ BERT/enc-9670.08 49.92 31.42 48.34 52.10 32.57 18.64 37.63\n+ PFEIFFER 71.64 51.59 33.41 49.50 53.98 35.20 20.43 38.88\n+ HOULSBY 70.97 51.69 33.84 49.31 53.98 34.13 19.48 38.57\n+ PREFIX 71.40 52.53 33.82 49.57 53.61 34.03 19.89 38.34\n+ INVERSE 71.02 52.77 34.49 49.33 52.47 33.76 19.93 37.93\n+ COMPACTER70.27 49.73 31.37 48.31 52.15 33.85 19.62 37.78\n+ LORA 70.94 50.24 32.15 48.81 51.90 32.81 18.77 37.37\n+ PARALLEL 70.48 51.64 33.66 49.33 43.50 23.20 11.59 32.29\n+ Fine-tuning 71.02 52.53 33.52 49.8053.59 34.05 19.51 37.92\n+ RoBERTa/enc-9669.73 51.34 33.49 48.91 52.58 33.8 19.62 37.89\n+ PFEIFFER 71.72 53.84 34.78 49.91 54.51 35.27 20.26 38.77\n+ HOULSBY 71.08 52.98 34.19 49.28 53.56 34.34 20.16 38.89\n+ PREFIX 72.28 52.42 33.98 49.90 53.08 33.19 19.48 38.47\n+ INVERSE 71.53 52.69 33.98 49.50 54.36 34.85 20.46 39.35\n+ COMPACTER71.21 51.56 33.17 49.20 52.88 33.16 19.35 38.09\n+ LORA 71.64 51.88 33.17 49.55 53.73 34.00 19.43 38.64\n+ PARALLEL 70.99 52.93 34.01 49.19 43.50 23.20 11.59 32.29\n+ Fine-tuning 71.61 53.15 33.33 49.77 52.70 33.21 19.69 38.47\n+ DeBERTa/enc-9670.73 52.53 33.49 49.32 53.04 33.94 20.22 38.72\n+ PFEIFFER 71.34 53.49 34.65 49.7854.37 34.70 20.49 39.30\n+ HOULSBY 70.83 52.10 33.74 49.48 53.25 33.93 19.55 38.45\n+ INVERSE 71.64 52.58 33.95 49.6655.09 35.45 20.66 39.71\n+ COMPACTER70.08 51.64 32.98 48.78 53.60 34.21 20.14 38.79\n+ LORA 71.18 52.34 33.31 49.18 53.98 34.53 20.02 39.00\n+ PARALLEL* 70.73 53.36 34.76 49.35 43.50 23.20 11.59 32.29\n+ Fine-tuning 71.59 53.44 33.44 49.63 53.54 33.78 20.12 38.92\nTable 17: Detailed results combining the TMLGA model with our three pre-trained language encoders and adapters,\ntested on Charades-STA and ActivityNet Captions. Underlined results indicate the best performance within the\nmodel and dataset combination, while the results in bold indicate the best performance within the dataset.\nHyper-parameter Method Value\nBatch Size Finetuning 256\nOthers 64\nBase LR\nAll\n1E-04\nWeight Decay 1E-05\nGamma 1E-02\nStep 6\nRoBERTa LR\nFinetuning\n1E-04\nWarm-up Rate 0.3\n# Epochs 10\nTable 18: Hyper-parameters used to train TMLGA with\nRoBERTa on the Charades-STA dataset.\nHyper-parameter Method Value\nBatch Size\nAll\n64\nBase LR 1E-04\nWeight Decay 1E-05\nGamma 1E-02\nStep 6\nRoBERTa LR\nFinetuning\n1E-04\nWarm-up Rate 0.2\n# Epochs 10\nTable 19: Hyper-parameters used to train TMLGA with\nRoBERTa on the ActivityNet Captions dataset.\n13120\nHyper-parameter Method Value\nBatch Size Finetuning 256\nOthers 64\nBase LR\nAll\n1E-04\nWeight Decay 1E-05\nGamma 1E-02\nStep H OULSBY 4\nINVERSE 5\nFrozen, PFEIFFER,\nPARALLEL, LORA 6\nCOMPACTER 7\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.3\n# Epochs 10\nTable 20: Hyper-parameters used to train TMLGA with\nDeBERTa on the Charades-STA dataset.\nHyper-parameter Method Value\nBatch Size\nAll\n64\nBase LR 1E-04\nWeight Decay 1E-05\nGamma 1E-02\nStep I NVERSE 5\nOthers 6\nBERT LR\nFinetuning\n1E-04\nWarm-up Rate 0.2\n# Epochs 10\nTable 21: Hyper-parameters used to train TMLGA with\nDeBERTa on the ActivityNet Captions dataset.\nDataset Method Hyper-parameter Value\nBatch Size 5\nBase LR 1E-04\nWeight Decay 1E-04\nGamma 5\nCharades-STA Pfeiffer\nStep 1E-02\nBatch Size 4\nBase LR 1E-04\nWeight Decay 1E-05\nGamma 4\nANet Inverse\nStep 1E-02\nTable 22: Hyper-parameters used to obtain the best\nresults for DORi with DeBERTa on the Charades-STA\nand ActivityNet Captions datasets.\n13121\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7 (Limitations)\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\n1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\n4.1 and 4.2\n□\u0013 B1. Did you cite the creators of artifacts you used?\n4\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n8 (Ethics Statement)\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n4.1 and 4.2\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\n4.1\nC □\u0013 Did you run computational experiments?\n4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4.2, 7\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n13122\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\n4.2 and Appendices A and B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4.2 and Appendices A and B\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n4.2\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n13123"
}