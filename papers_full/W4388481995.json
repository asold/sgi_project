{
  "title": "Zero-shot Bilingual App Reviews Mining with Large Language Models",
  "url": "https://openalex.org/W4388481995",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2368605821",
      "name": "Wei, Jialiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284393930",
      "name": "Courbis, Anne-Lise",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284393931",
      "name": "Lambolais, Thomas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2486621678",
      "name": "Xu Binbin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284393933",
      "name": "Bernard, Pierre Louis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284393934",
      "name": "Dray, Gérard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2112143630",
    "https://openalex.org/W4322759717",
    "https://openalex.org/W2761329858",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W4310923309",
    "https://openalex.org/W2085465189",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2601243251",
    "https://openalex.org/W2547513165",
    "https://openalex.org/W2964583233",
    "https://openalex.org/W4312741901",
    "https://openalex.org/W4367176004",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W2996203597",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3215722946",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W4385570594",
    "https://openalex.org/W2061951099",
    "https://openalex.org/W2362569215",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W3209586129",
    "https://openalex.org/W4224330577",
    "https://openalex.org/W4283751840",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4308643038",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3215981572",
    "https://openalex.org/W4312325121"
  ],
  "abstract": "App reviews from app stores are crucial for improving software requirements. A large number of valuable reviews are continually being posted, describing software problems and expected features. Effectively utilizing user reviews necessitates the extraction of relevant information, as well as their subsequent summarization. Due to the substantial volume of user reviews, manual analysis is arduous. Various approaches based on natural language processing (NLP) have been proposed for automatic user review mining. However, the majority of them requires a manually crafted dataset to train their models, which limits their usage in real-world scenarios. In this work, we propose Mini-BAR, a tool that integrates large language models (LLMs) to perform zero-shot mining of user reviews in both English and French. Specifically, Mini-BAR is designed to (i) classify the user reviews, (ii) cluster similar reviews together, (iii) generate an abstractive summary for each cluster and (iv) rank the user review clusters. To evaluate the performance of Mini-BAR, we created a dataset containing 6,000 English and 6,000 French annotated user reviews and conducted extensive experiments. Preliminary results demonstrate the effectiveness and efficiency of Mini-BAR in requirement engineering by analyzing bilingual app reviews. (Replication package containing the code, dataset, and experiment setups on https://github.com/Jl-wei/mini-bar )",
  "full_text": "Zero-shot Bilingual App Reviews Mining with\nLarge Language Models\nJialiang Wei∗, Anne-Lise Courbis ∗, Thomas Lambolais ∗,\nBinbin Xu∗, Pierre Louis Bernard ∗∗ and G ´erard Dray∗\n∗: EuroMov Digital Health in Motion, Univ Montpellier, IMT Mines Ales, Ales, France\n∗∗: EuroMov Digital Health in Motion, Univ Montpellier, IMT Mines Ales, Montpellier, France\n∗: firstname.lastname@mines-ales.fr ∗∗: firstname.lastname@umontpellier.fr\nAbstract—App reviews from app stores are crucial for improv-\ning software requirements. A large number of valuable reviews\nare continually being posted, describing software problems and\nexpected features. Effectively utilizing user reviews necessitates\nthe extraction of relevant information, as well as their subsequent\nsummarization. Due to the substantial volume of user reviews,\nmanual analysis is arduous. Various approaches based on natural\nlanguage processing (NLP) have been proposed for automatic\nuser review mining. However, the majority of them requires a\nmanually crafted dataset to train their models, which limits their\nusage in real-world scenarios.\nIn this work, we propose Mini-BAR, a tool that integrates\nlarge language models (LLMs) to perform zero-shot mining of\nuser reviews in both English and French. Specifically, Mini-BAR\nis designed to (i) classify the user reviews, (ii) cluster similar\nreviews together, (iii) generate an abstractive summary for each\ncluster and (iv) rank the user review clusters. To evaluate the\nperformance of Mini-BAR, we created a dataset containing 6,000\nEnglish and 6,000 French annotated user reviews and conducted\nextensive experiments. Preliminary results demonstrate the effec-\ntiveness and efficiency of Mini-BAR in requirement engineering\nby analyzing bilingual app reviews.\nI. I NTRODUCTION\nApp stores, such as Google Play and Apple App Store,\nallow users to express their feedback on downloaded apps.\nThis feedback is in form of rating scores and text reviews. The\nlatter contains praise and dispraise, user experience, problem\nreports, and feature requests [1]. App reviews are important\nfor app success. As evidenced in prior literature, high-user\nrating scores have positive effects on apps’ sustainability [2].\nConsequently, the design and development teams should take\nthe users’ feedback into consideration during the evolution of\ntheir application.\nDue to the large amount and the redundancy of app reviews,\nmanual analysis is laborious. Various approaches based on nat-\nural language processing (NLP) have been proposed to reduce\nthe efforts in analyzing user feedback, including the classi-\nfication, clustering and summarization. Classification models\nare commonly employed in the first approach to categorize\napp reviews into predefined groups, such as feature requests\nand problem reports [3], [4], [5], [6], [7]. However, even after\nclassification, the volume of reviews within each category\nremains substantial, making direct analysis impractical. To\ntackle this issue, some researchers have proposed grouping\nreviews that pertain to the same topic [8], [9], [10], [11]. As\nthe obtained clusters still contain a relatively large number of\nuser reviews, the manual analysis of each cluster continues to\nbe time-consuming. Certain techniques attempt to overcome\nthis challenge by selecting the most representative phrases or\nsentences as summaries for groups of app reviews [9], [10],\n[12], [13], [14]. Nevertheless, this extractive summarization\napproach may not capture all the crucial information present\nwithin a given group. Moreover, existing approaches in user\nreview analysis mainly focus on the English language, with\nfew works on analyzing reviews in other languages [15],\n[16]. Furthermore, most existing approaches requires manually\ncrafted dataset for training their classification models. The\ncreation of dataset is costly and time consuming, which limits\ntheir usage in real-word scenarios. The objective of this article\nis therefore to address these gaps with large language models.\nPre-trained language models (PTMs) are deep neural net-\nworks previously trained on a vast corpus. Researchers have\nobserved that large-sized PTMs display different behaviors\nfrom smaller PTMs and show surprising abilities (called\nemergent abilities) in solving a series of complex tasks. Thus,\nthe research community coins the term “large language models\n(LLMs)” for these large-sized PTMs [17]. A remarkable ap-\nplication of LLMs is ChatGPT1, it is fine-tuned from the GPT-\n3.5 [18] using Reinforcement Learning from Human Feedback\n(RLHF), which optimizes the model by interacting with human\nand learning from human preference. The Guanaco model\nis an open-source, finely-tuned LLM, derived through the\napplication of QLoRa’s 4-bit tuning approach [19] on LLaMA\nbase models [20]. QLoRA is an efficient fine-tuning approach\nthat reduces memory usage. Guanaco is available in various\nparameter sizes, including 7B, 13B, 33B and 65B.\nIn this paper, we propose Mini-BAR, a bilingual approach\nbased on LLMs to: (i) classify the user reviews into three\ncategories: feature request, problem report and irrelevant; (ii)\ncluster similar reviews for feature request and problem report;\n(iii) generate a summary for each cluster of user reviews; and\n(iv) rank the user review clusters. Figure 1 depicts an overview\nof the workflow of Mini-BAR. We use the same pipeline to\nprocess the bilingual app reviews, eliminating the necessity of\n1https://openai.com/blog/chatgpt/\narXiv:2311.03058v1  [cs.CL]  6 Nov 2023\ndeploying separate models for each language. By combining\nthese functionalities, Mini-BAR provides a comprehensive\napproach for analyzing bilingual app reviews, which can\nyield valuable insights for app developers and marketers. We\nvalidate the key steps of Mini-BAR by conducting an extensive\nset of experiments on 12000 annotated user reviews from\nthree Health & Fitness apps. The results indicate that Mini-\nBAR has a satisfactory performance in both classification and\nclustering tasks, and produced high-quality summaries. We\nprovide a replication package 2 containing the code, dataset,\nand experiment setups.\nII. A PPROACH\nMini-BAR provides support to developers for the analysis\nof mobile app user reviews through a four-step process. First,\nit applies a pre-trained classifier to categorize the user reviews\n(Section II-A). The second step clusters the user reviews based\non their semantic similarity (Section II-B). The third step\nsummarizes the user reviews belonging to the same cluster\n(Section II-C). The last step is to determine the importance\nof user review clusters and rank them accordingly (Section\nII-D). In the following subsections, we will detail each step\nof Mini-BAR.\nClassification (ChatGPT)\nProblem report\nFeature requestIrrelevant Text Embedding (Instructor)\nClustering (HDBSCAN)\nProblem reportC CC\nFeature requestC CCC\nProblem reportS | CS | CS | C\nFeature requestS | CS | CS | CS | C\nSummarization (ChatGPT)\nRankingProblem reportFeature request① S | C② S | C③ S | C\n① S | C② S | C③ S | C④ S | C\nDimension Reduction (UMAP)\nReview(en)Review(fr)Review(en)Review(en)Review(en)Review(en)Review(en)Review(fr)Review(fr)Review(fr)Review(fr)Review(fr)\nReview(en)Review(en)Review(fr)Review(fr)\nReview(en)Review(en)Review(fr)Review(fr)\nReview(en)Review(en)Review(fr)Review(fr)\nFig. 1. Overview of Mini-BAR\nA. Classification\nThe objective of this step is to automatically classify English\nand French user reviews into three categories: (F) feature\nrequest, (B) problem report, and (I) irrelevant. A user review\nmay belong to either one of the three categories or both feature\nrequest and problem report . A user review is considered\nas problem report if it mentions the issues the users have\nexperienced while using the app ( e.g., “Can’t sync sleep data\nsince last update”). Feature requests reflect users’ needs for\nnew functions, new content, or improvements ( e.g., “Please\nbring a feature to add some custom watch faces . . . ”). All the\nother user reviews are irrelevant (e.g., “Best app ever!”). Clas-\nsifying the reviews can aid to redirect them to the appropriate\nsoftware project members. For instance, feature requests can\nbe delivered to requirements analysts, while problem reports\ncan be directed to developers and testers [4].\n2https://github.com/Jl-wei/mini-bar\nThe classifier of Mini-BAR is based on ChatGPT, the model\nwe use is gpt-3.5-turbo3. We use the following prompt to\nclassify the app reviews.\nClassify the following {lang} app review\ninto problem report, feature request or\nirrelevant. Be concise.\n‘‘‘\n{review}\n‘‘‘\nGiven a user review, its language is detected automatically\nwith Lingua 4, which is an accurate language detector. The\ndetected language, which could be English, French among\nothers, replaces the {lang} variable in the prompt. And the\n{review} in the prompt is replaced by the user review. The\nresponse of ChatGPT contains a single-phrase label name. We\nparse the response with regular expressions to automatically\nobtain the predicted labels.\nB. Clustering\nThe objective of this step is to group English and French\nuser reviews based on their semantic similarity, ensuring that\nreviews within a group are related to the same topic. Through\nclustering analysis, texts are divided into clusters such that\nthose within a cluster exhibit semantic similarity. Currently,\nthe RE community predominantly focuses on clustering En-\nglish user reviews, leaving little attention to non-English user\nreviews. To address this gap, we propose a bilingual cluster-\ning approach that allows the creation of clusters comprising\nreviews from different languages that share common topics.\nDimension ReductionUMAP\nText EmbeddingInstructor\nClusteringHDBSCAN\nReviewReviewReviewReviewReviewReviewReviewReviewReviewReviewReviewReview -0.041-0.0110.030...\n0.0310.0450.028...\n0.032-0.0140.069...\n768 dimensions\n...\n20 dimensions\n-0.032-0.0450.028...\n-0.0350.0310.014...\n0.031-0.0470.015......C CCC\nFig. 2. Overview of clustering\nIn this step, we perform bilingual clustering analysis on\nuser reviews that belong to the same category, namely fea-\nture request or problem report , which were identified in\nthe previous step. It is worth noting that in the clustering\nprocess, the categories of feature requests and problem reports\nare processed separately, in order to obtain distinct clusters\nfor each. Conversely, user reviews labeled as irrelevant are\nexcluded from the clustering analysis.\nThe user reviews cannot be directly used as input for the\nclustering algorithm, as they are in a textual format. Therefore,\n3https://platform.openai.com/docs/models/gpt-3-5\n4https://github.com/pemistahl/lingua-py\n2\nit is necessary to convert them into embeddings — numerical\nvectors in a high-dimensional space — to enable effective\nprocessing. In this space, similar inputs in different languages\nare mapped close together. For example, the embeddings of\n“Probl`eme de serveur r´ecurrent” and “Connection issues to the\nmain server” are in proximity to each other. We used Instruc-\ntor [21] to embed English and French app reviews due to its\nhigh performance proven in Section III-B3. Instructor is an\ninstruction-finetuned text embedding model that can generate\ntext embeddings tailored to any task (e.g. clustering) by simply\nproviding the task instruction, without any finetuning. In our\ncase, we have utilized the instruction ”Represent the app user\nreview for clustering”. This model generates 768-dimensional\nembeddings for each app review.\nThe high dimension of embeddings causes high computation\ncosts. Dimension reduction techniques can transforms data\nfrom a high-dimensional space into a low-dimensional space\nand keeps meaningful information of the original data. As in\nStanik et al. [9], we reduced the embeddings’ dimension with\nUniform Manifold Approximation and Projection (UMAP).\nThe implementation of Mini-BAR utilizes the UMAP Python\npackage5, the UMAP parameters are as follows: output dimen-\nsionality of 20, number of neighbors set to 100, and minimum\ndistance of 0.\nThe reduced embeddings of the feature requests and prob-\nlem reports are then clustered by HDBSCAN [22], which has\nbeen proven efficient by Devine et al. [10] and Stanik et al. [9].\nWe use the HDBCAN6 Python package for the implementation\nof Mini-BAR. The HDBSCAN parameters include a minimum\ncluster size of 5. The minimum cluster size is the smallest\ngrouping size considered as a cluster. In our study, we chose a\nminimum cluster size of 5, as we were interested in identifying\nproblems or features that were reported by at least 5 users.\nC. Summarization\nGiven the potential magnitude of reviews within clusters,\nthe process of summarization becomes imperative, enabling\ndevelopers to efficiently grasp the cluster’s contents without\nthe need to peruse every individual review. Large language\nmodels (LLMs), such as ChatGPT, has achieved a state-of-\nthe-art performance in cross-lingual summarization [23]. In\nthis step, we utilized ChatGPT ( gpt-3.5-turbo) to generate\nabstractive English summaries for clusters containing bilingual\nuser reviews. Our evaluation in Section III-C proves that\nChatGPT outperforms extractive summarization method, and\nis able to generate high-quality summaries.\nTo generate the summaries, we utilized the following\nprompt within ChatGPT. The {reviews list} is replace by a\nlist of user reviews separated by new line break. The response\ngenerated by ChatGPT represents the summary for that list of\nuser reviews. Table I presents an example of a manual created\ncluster and its generated summary.\n5https://github.com/lmcinnes/umap\n6https://github.com/scikit-learn-contrib/hdbscan\nPlease summarize all following app\nreviews into one English sentence:\n‘‘‘\n{reviews list}\n‘‘‘\nReview(en)\nSummary(en)\nSub-summary(en)\nReview(fr)Review(en)Review(en)Review(fr)Review(en)Review(en)Review(fr)Review(en)\nSub-summary(en)Sub-summary(en)\nFig. 3. Overview of summarization for large clusters\nHowever, in case the user review clusters contain a large\nnumber of reviews and exceed the input length limitation\nof ChatGPT, it will issue a warning message indicating that\n“The message you submitted was too long, please reload the\nconversation and submit something shorter”. To address this\nissue, as illustrated in Figure 3, we adopted a hierarchical\nsummarization approach consisting of the following steps: (i)\ndividing the reviews belonging to one cluster into multiple\ngroups, each with a maximum of 4000 tokens 7, (ii) generating\na sub-summary for each group of user reviews, and (iii) ob-\ntaining the final summary by summarizing the sub-summaries.\nSince the length of all sub-summaries may also exceed the\ninput limit of ChatGPT, we used a recursive procedure for\nsteps (i) and (ii) by replacing the user reviews with sub-\nsummaries.\nTABLE I\nEXAMPLE OF A USER REVIEW CLUSTER AND ITS GENERATED SUMMARY\nUser reviews:\n- Dommage que la connexion 4g soit indispensable pour fonctionner.\n- Please for god sake make it to work offline also.\n- Is not work offline\n- It used to work offline. Now I have to log in just to see my old data.\n- Useless without internet.\nSummary:\nUsers are disappointed that the app requires an internet connection to\nfunction and wish it could work offline like it used to.\nD. Ranking\nGiven the clusters with summaries, the next step is to rank\nthem by their importance. The goal of this step is to aid in\nthe release planning of app developers. The importance of a\ncluster ( ClusterScore) is determined based on the following\ncharacters:\n• The number of reviews present within the cluster\n(|reviews|). problems or feature requests reported by\n7https://platform.openai.com/docs/guides/gpt/chat-completions-vs-\ncompletions\n3\nmore users should be given higher priority compared to\nthose reported by fewer users.\n• The average rating of the cluster ( rating). Clusters with\nlower average ratings should be given higher priority,\nas they may indicate users’ greater dissatisfaction with\nspecific aspects of the app.\n• The number of “thumbs up” inside the cluster\n(|thumbsup|). Users on Google Play have the option to\nclick the “thumbs up” button on reviews that they find\nhelpful. We posit that the number of “thumbs up” and\nthe importance of a cluster are positively correlated as the\nreview liked by more users should have a higher priority.\nGiven the weight of wrev, wth and wra, which are assigned\ndefault values of 1, 0.1, and 1, respectively, the calculation of\nClusterScore is defined as follows:\nClusterScore = wrev · |reviews| + wth · |thumbsup|\nwra · rating (1)\nThe clusters are ranked in decreasing order of ClusterScore.\nIII. E MPIRICAL EVALUATION\nThe objective of this study is to assess the performance\nof Mini-BAR with respect to three criteria: (i) its accuracy\nin classifying user reviews into one of three predefined cate-\ngories, namely feature request, problem report, and irrelevant;\n(ii) its ability to cluster related user reviews that fall into the\nsame topic; (iii) its ability to provide high-quality summaries\nof user reviews clusters. To achieve this goal, we evaluated\nMini-BAR’s performance on a dataset of 6000 English and\n6000 French reviews from three health-related mobile apps.\nA. Evaluation of Classification\nIn this section, we aim to answer the following research\nquestion (RQ 1): How accurate is Mini-BAR in classifying\nbilingual user reviews ?\n1) Dataset: The training and evaluation of the classifier\nrequire a large number of labeled user reviews. We rela-\nbelled the 6000 French reviews of three applications (Garmin\nConnect, Huawei Health and Samsung Health) on Google\nPlay from our previous work [16]. Besides, we collected\n365, 967 English user reviews from these three applications.\nFor each application, 2000 English are randomly sampled for\nannotation. In this work, we have labeled6000 English reviews\nand 6000 French reviews.\nWe used Prodigy8 from spaCy to annotate the user reviews.\nWe created an annotation guide to clarify the definition of\nfeature request, problem report, and irrelevant. Four authors\nof this paper annotated the sampled user reviews and they\nare finally reviewed by the first author of this paper. Table II\nshows the details of the annotated dataset. The sum of each\ncategory does not equal the total of reviews, as some reviews\nhave been assigned to more than one label.\n2) Evaluation Metrics: The performance of the classifiers\nis evaluated by precision, recall, and F1 as presented in related\nwork [15], [4].\n8https://prodi.gy/\nTABLE II\nOVERVIEW OF THE DATASET FOR CLASSIFICATION\nApp Language Total Feature\nrequest\nproblem\nreport Irrelevant\nen 2000 223 579 1231Garmin Connect fr 2000 217 772 1051\nen 2000 415 876 764Huawei Health fr 2000 387 842 817\nen 2000 528 500 990Samsung Health fr 2000 496 492 1047\n3) Experiments: In this experiment, we compared the per-\nformance of ChatGPT and Guanaco-33B with ML models\n(Random Forest, Support Vector Machine), as well as various\nPTMs (BERT [24], CamemBERT[25], XLM-R [26]), on the\nclassification of app reviews.\nThe ML models are trained using batch gradient descent,\nwhile PTMs employed mini-batch gradient descent, with a\nbatch size of 12 and AdamW optimizer with a learning rate\nof 2e−5. They are trained on 3 epochs on a machine with a\nNVIDIA Tesla T4 GPU with 16 GB VRAM. The user reviews\nfrom the three apps of both languages were split using an 80:20\n(training and test sets) ratio in a stratified manner, as illustrated\nin Figure 4. We trained the classifiers using a combination\nof en train and fr train. Subsequently, the classifiers were\nindividually tested on the en test and fr test. We performed\nten-fold cross-validation by randomly splitting the training and\ntest sets ten times, and computed the average performance\nacross these runs.\nen_traintr_en_trainfr_traintr_fr_train\nen_testtr_en_testfr_testtr_fr_test\nTrain (80%of user reviews)Test (20%of user reviews)\nen_trainfr_train en_testfr_test\nTrain (80%of user reviews)Test (20%of user reviews)\nFig. 4. Overview of dataset split for training and testing\nWe conduct classifications utilizing LLMs, specifically\nChatGPT and Guanaco-33B, on all 12,000 user reviews. This\nis executed under zero-shot setting, implying that no prior\ntraining is involved. We have also assessed the performance\nof Guanaco-13B and Guanaco-65B. However, the responses\ngenerated by Guanaco-13B are disorganized, thereby hinder-\ning the extraction of predicted labels using regular expressions.\nThe inference of Guanaco-65B is intolerably slow, even on\nadvanced hardware such as the NVIDIA A100, making its\nusage impractical.\n4) Results: The experiment results presented in Table III\ndemonstrate that the ChatGPT exhibited good overall perfor-\nmance, which is comparable to that of ML models. Its compar-\natively lower performance in classifying feature requests can\nbe attributed to the inherent complexity associated with such\nrequests. In certain instances, users may express their desire\nfor new features by criticizing existing ones or complaining\nabout missing functionalities, rather than straightforwardly\nstating ”I need...”. Among all the models, XLM-R archived\n4\nTABLE III\nCLASSIFICATION ACCURACY ON USER REVIEWS OF THREE APPS\nFeature Request Problem Report Irrelevant Average Weight\nPrecision Recall F1 Precision Recall F1 Precision Recall F1 Precision Recall F1\nRandom Forest 0.75 0.453 0.564 0.797 0.82 0.808 0.898 0.885 0.891 0.837 0.782 0.802\nSVM 0.86 0.438 0.58 0.86 0.806 0.832 0.931 0.893 0.912 0.895 0.778 0.823\nBERT 0.814 0.782 0.797 0.897 0.914 0.905 0.972 0.954 0.963 0.918 0.909 0.913\nCamemBERT 0.811 0.743 0.775 0.883 0.894 0.888 0.966 0.951 0.958 0.91 0.893 0.901\nXLM-R 0.823 0.811 0.816 0.902 0.917 0.909 0.979 0.958 0.968 0.925 0.917 0.92\nChatGPT 0.768 0.5 0.606 0.762 0.972 0.854 0.983 0.911 0.945 0.871 0.853 0.852\nEn\nGuanaco-33B 0.361 0.62 0.456 0.662 0.951 0.781 0.983 0.817 0.893 0.763 0.823 0.774\nRandom Forest 0.8 0.528 0.635 0.798 0.834 0.816 0.902 0.869 0.885 0.848 0.796 0.817\nSVM 0.895 0.459 0.606 0.86 0.828 0.844 0.956 0.89 0.922 0.912 0.791 0.838\nBERT 0.766 0.725 0.744 0.871 0.866 0.869 0.947 0.931 0.939 0.888 0.872 0.88\nCamemBERT 0.852 0.823 0.837 0.922 0.925 0.923 0.977 0.96 0.968 0.936 0.924 0.929\nXLM-R 0.819 0.833 0.825 0.917 0.921 0.919 0.982 0.949 0.965 0.93 0.919 0.924\nChatGPT 0.853 0.473 0.608 0.782 0.973 0.868 0.978 0.935 0.956 0.888 0.866 0.863\nFr\nGuanaco-33B 0.296 0.576 0.391 0.624 0.97 0.759 0.985 0.756 0.856 0.737 0.797 0.739\nthe best performance in bilingual classification. Although the\nperformance of ChatGPT does not match up to PTMs, it is\nnoteworthy that these LLMs have achieved such performance\nwithout the utilization of any reviews during their training\nphase. This suggests that ChatGPT can achieve satisfactory\nperformance in user reviews of other application categories.\nB. Evaluation of Clustering\nIn this section, our objectives are to address the RQ2: How\nsemantically meaningful are the clusters generated by Mini-\nBAR?\n1) Dataset: In order to evaluate the performance of cluster-\ning algorithms, we created a dataset with 1200 user reviews.\nWe randomly selected 100 problem reports and 100 feature\nrequests from each of the three apps in each of the two lan-\nguages present in the dataset created in Section III-A1. Then\nthe authors employed manual clustering for each collection of\n200 bilingual reviews, all of which pertained to an identical\ncategory. Reviews sharing the same topic were subsequently\ngrouped into a single cluster. In instances where a user’s\nreview mentioned multiple topics, the assignment of the cluster\nwas determined by the initial topic that was reported. Two\nauthors independently performed the clustering of the 1200\nreviews, and their individual results were later merged through\ndiscussion. The resulting clusters were then considered as the\nground truth for subsequent evaluation.\nTable IV shows the number of manually created clusters\nand the number of clusters whose size is greater than or\nequal to 5 in each category and app. The feature requests\nencompass enhancements such as the modification of a Graph-\nical User Interface (GUI), support for additional languages,\nincreased activity options, integration with other applications,\ncustomization of permissions, and improvement of sleep track-\ning function. The problem reports predominantly center on a\nseries of issues, notably the application’s unexpected crashes,\nerrors encountered during the login process, difficulties in pair-\ning with smartwatches, challenges with data synchronization,\ninconsistencies in the notification system, and complications\nrelated to Bluetooth connectivity, among others.\nTABLE IV\nOVERVIEW OF MANUALLY CREATED CLUSTERS\nBilingual Garmin\nConnect\nHuawei\nHealth\nSamsung\nHealth\n#clusters in feature request 89 74 69\n#clusters(size ≥ 5) in feature request 7 9 11\n#clusters in problem report 45 44 41\n#clusters(size ≥ 5) in problem report 10 13 12\n2) Evaluation Metrics: Following previous work [11], we\nuse two commonly used indices, Normalized Mutual Infor-\nmation (NMI) [27] and Adjusted Rand Index (ARI) [28], to\nquantify the similarity between the automatic clustering and\nthe ground truth. NMI ranges from 0 to 1, while ARI ranges\nfrom -1 to 1. A higher NMI or ARI indicates that the clustering\nmethod is more effective in producing clusters that align with\nthe ground truth. Note that the NMI and ARI are computed\nfor clusters with a size of 5 or greater, as the minimum cluster\nsize of HDBSCAN is set to 5.\n3) Experiments and Results: In this section, we evaluate the\nperformance of different text representation methods (includ-\ning traditional frequency-based methods, bag of words (BOW),\nand TF-IDF, as well as PTM-based methods, Universal Sen-\ntence Encoder [29], MiniLM [30], MPNet [31], E5 [32] and\nInstructor [21]) on the dataset created in Section III-B1. We\nperformed three distinct experiments on English-only, French-\nonly and bilingual user reviews. In each experiment, clustering\nis executed separately for the two categories ( feature requests\nand problem reports) within each of the three applications.\n4) Results: The average NMI and ARI of the three distinct\nexperiments are shown in Table V. Results show that PTM-\nbased methods outperformed traditional methods. Among all\ntext representation methods, Instructor demonstrated the high-\nest level of performance in clustering. The results from the\nEnglish user reviews clustering were superior to those derived\nfrom the bilingual and French user reviews clustering. We\nattribute this variation to the relatively small French corpus\nemployed for training the PTMs.\nWhile the proposed approach appears to have some degree\nof validity, the results do not appear to be particularly en-\ncouraging at this stage. The primary reason for this is that\n5\nTABLE V\nEVALUATION ON USER REVIEWS CLUSTERING\nEmbedding NMI ARI\nMethods en fr bi en fr bi\nBOW 0.450 0.405 0.417 0.191 0.155 0.103\nTF-IDF 0.452 0.449 0.460 0.253 0.216 0.149\nUSE 0.575 0.501 0.552 0.337 0.330 0.236\nMiniLM 0.548 0.567 0.541 0.323 0.380 0.219\nMPNet 0.616 0.575 0.593 0.400 0.346 0.278\nE5 0.465 0.401 0.436 0.248 0.190 0.139\nInstructor 0.713 0.587 0.603 0.597 0.357 0.308\nour evaluation dataset is relatively small. Clustering algorithms\nrequire a sufficient amount of data to discover underlying pat-\nterns and structures. With a limited amount of data, it becomes\ndifficult to identify meaningful groupings of text. Moreover,\ntext clustering is a challenging task, particularly given the\ninformal nature of the terminology employed in user reviews\nand the prevalence of spelling errors. Additionally, based on\nour empirical analysis, it appears that longer user reviews\ntend to result in less accurate clustering. This presents an\nopportunity for potential improvement by applying sentence-\nlevel clustering to user reviews.\nC. Evaluation of Summarization\nIn this section, we aim to investigate the RQ3: : How\neffectively does ChatGPT perform in summarizing bilingual\nuser reviews?\n1) Dataset: In Section III-B1, we carried out manual\nclustering for a total of 1200 user reviews. Among these,\nwe utilized clusters with a size of 5 or greater to assess the\nperformance of ChatGPT on summarization.\n2) Evaluation Metrics: As outlined in Fabbri et al. [33],\nhuman evaluators rate the generated summaries based on four\ndimensions: relevance (the degree to which crucial information\nfrom the source has been included), consistency (how well\nthe summary aligns with the factual details of the source),\nfluency (the quality of individual sentences), and coherence\n(the overall quality and coherence of all the sentences in\nthe summary). Each dimension is scored on a Likert scale\nranging from 1 to 5, with higher scores indicating superior\nperformance.\n3) Experiments: To evaluate the proficiency of ChatGPT\nin producing succinct English summaries of bilingual app\nreviews, we compare it with baseline approaches: Extractive\nsummarization of Devine et al. [10], which selects the most\nrepresentative sentence from a cluster of app reviews as the\nsummary, the sentence is chosen by calculating the similarity\nwith all other sentences of that cluster. Abstractive summa-\nrization with Guanaco models (13B, 33B and 65B version\nof Guanaco are used in our experiments) [19]. ChatGPT\nand Guanaco were instructed to synthesize clusters of user\nreviews into a single English sentence with the same prompt,\nas presented in Section II-C. Subsequently, human evaluators\nassessed the generated summaries. Given the pairs of user\nreviews and corresponding summaries, two authors of this\npaper were asked to evaluate the summaries on the Likert\nscale in the four dimensions that were previously mentioned.\nTABLE VI\nHUMAN EVALUATION ON GENERATED SUMMARIES\nRelevance Consistency Fluency Coherence\nDevine et al. [10] 4.23 4.83 4.62 4.71\nGuanaco-13B 3.67 3.68 4.92 4.91\nGuanaco-33B 4.65 4.58 4.91 4.88\nGuanaco-65B 4.79 4.77 4.95 4.94\nChatGPT 4.81 4.84 4.95 4.94\n4) Results: Table VI presents the average results of hand-\nmade evaluations. Results show that abstractive approaches\n(ChatGPT, Guanaco-33B, and Guanaco-65B) generate very\nhigh quality sentences, and they are highly coherent when\nviewed in conjunction with one another. On the other hand,\nthe extractive approach excels at extracting the most significant\ninformation from a cluster; however, it falls short in captur-\ning all the essential details. During our manual evaluation,\nwe found that Guanaco-13B tends to retrieve all available\ninformation available in the cluster without selectively focus-\ning on crucial elements. In contrast, Guanaco-33B performs\nsignificantly better in this regard by effectively filtering out\nnon-essential information. The results highlight the impressive\nperformance of ChatGPT and Guanaco-65B in generating\nhighly satisfactory summaries of user reviews.\nIV. T HREATS TO VALIDITY\nThis section aims to identify potential threats to the validity\nof our study.\n1) App reviews from one category: All reviews studied\nin this paper are collected from three Health & Fitness\napps (Garmin Connect, Huawei Health, and Samsung Health),\nmainly due to the context of health activity monitoring project.\nInstead of analyzing small number of reviews in many apps,\nwe choose to annotate 12,000 reviews on three apps to create\na larger dataset to evaluate clustering and summarization.\nHowever, these three apps may not be representative of apps\nin other categories. In the future, we will alleviate this threat\nby investigating user reviews of apps in various categories.\n2) Subjectivity in manual annotation: The annotation of\nuser reviews is a straightforward task, people without specific\ntraining can well classify or cluster the reviews. However,\nsubjectivity can still arise during manual annotation, leading to\nvariations in how different annotators interpret and label the\nreviews. To mitigate this threat, we (i) created a annotation\nguideline to detail the definition of each label following with\nexamples, (ii) reviewed the final label through discussion and\nconsensus.\n3) Issues of using ChatGPT: ChatGPT has been selected as\nthe classification and summarization component of Mini-BAR,\nowing to its superior capabilities. However, it is noteworthy to\nmention that certain countries have imposed prohibitions on\nthe use of ChatGPT. Some users may refrain from utilizing\nChatGPT owing to concerns pertaining to data privacy. And\nthe cost of analyzing user reviews using ChatGPT cannot be\noverlooked, particularly in light of the large volume of user\nreviews. To mitigate these challenges, we have implemented\nalternative strategies. For classification, Mini-BAR users can\n6\nutilize our XLM-R checkpoint, which has been trained on\n12,000 reviews, or they can opt for Guanaco-33B. For summa-\nrization tasks, users have the option to use other large language\nmodels, specifically Guanaco-33B or Guanaco-65B.\nV. C ONCLUSION\nThis paper introduces Mini-BAR, a mobile app review\nmining tool designed to assist app developers in extracting\nand summarizing user-reported issues and requests from a\nhuge number of app reviews. This tool is based on LLMs\nand operates under zero-shot setting. Our empirical evaluation\non the key steps of Mini-BAR resulted in numerous positive\noutcomes: (i) it accurately classified bilingual user reviews\nwith an F1 score of 0.85; (ii) it created meaningful clusters\nof bilingual user reviews with a NMI greater than 0.6; (iii)\nit produced highly satisfactory summaries of bilingual user\nreviews.\nIn our future research, we intend to: (i) conduct a compara-\ntive analysis of various prompts utilized for classification and\nsummarization, (ii) implement alternative large-scale language\nmodels for classification and summarization, (iii) execute\nclassification and clustering at the sentence level as opposed to\nthe review level, (iv) undertake evaluations using user reviews\nsourced from applications across a diverse range of categories.\nREFERENCES\n[1] D. Pagano and W. Maalej, “User feedback in the appstore: An empirical\nstudy,” in 2013 21st IEEE International Requirements Engineering\nConference (RE), 2013, pp. 125–134.\n[2] G. Lee and T. S. Raghu, “Determinants of Mobile Apps’ Success: Evi-\ndence from the App Store Market,” Journal of Management Information\nSystems, vol. 31, no. 2, pp. 133–170, 2014.\n[3] N. Chen, J. Lin, S. C. Hoi et al. , “AR-miner: Mining informative\nreviews for developers from mobile app marketplace,” Proceedings -\nInternational Conference on Software Engineering , no. 1, pp. 767–778,\n2014.\n[4] W. Maalej, Z. Kurtanovi ´c, H. Nabil, and C. Stanik, “On the automatic\nclassification of app reviews,” Requirements Engineering, vol. 21, no. 3,\npp. 311–331, 2016.\n[5] R. R. Mekala, A. Irfan, E. C. Groen et al., “Classifying User Require-\nments from Online Feedback in Small Dataset Environments using Deep\nLearning,” in 2021 IEEE 29th International Requirements Engineering\nConference (RE), 2021, pp. 139–149.\n[6] P. R. Henao, J. Fischbach, D. Spies et al., “Transfer Learning for Mining\nFeature Requests and Bug Reports from Tweets and App Store Reviews,”\nin 2021 IEEE 29th International Requirements Engineering Conference\nWorkshops (REW), 2021, pp. 80–86.\n[7] J. Zhang, Y . Chen, N. Niu, and C. Liu, “A Preliminary Evaluation of\nChatGPT in Requirements Information Retrieval,” no. 2022, pp. 1–16.\n[Online]. Available: http://arxiv.org/abs/2304.12562 2023.\n[8] S. Scalabrino, G. Bavota, B. Russo et al., “Listening to the Crowd for\nthe Release Planning of Mobile Apps,” IEEE Transactions on Software\nEngineering, vol. 45, no. 1, pp. 68–86, 2019.\n[9] C. Stanik, T. Pietz, and W. Maalej, “Unsupervised Topic Discovery\nin User Comments,” in 2021 IEEE 29th International Requirements\nEngineering Conference (RE) , 2021, pp. 150–161.\n[10] P. Devine, J. Tizard, H. Wang et al., “What’s Inside a Cluster of Software\nUser Feedback: A Study of Characterisation Methods,” in 2022 IEEE\n30th International Requirements Engineering Conference (RE) , 2022,\npp. 189–200.\n[11] Y . Wang, J. Wang, H. Zhang et al. , “Where is Your App Frustrating\nUsers?” Proceedings - International Conference on Software Engineer-\ning, vol. 2022-May, pp. 2427–2439, 2022.\n[12] A. D. Sorbo, S. Panichella, C. V . Alexandru et al. , “What would\nusers change in my App? Summarizing app reviews for recommending\nsoftware changes,” Proceedings of the ACM SIGSOFT Symposium on\nthe Foundations of Software Engineering, vol. 13-18-Nove, pp. 499–510,\n2016.\n[13] M. Alshangiti, W. Shi, E. Lima et al. , “Hierarchical Bayesian multi-\nkernel learning for integrated classification and summarization of app\nreviews,” ESEC/FSE 2022 - Proceedings of the 30th ACM Joint Meeting\nEuropean Software Engineering Conference and Symposium on the\nFoundations of Software Engineering , pp. 558–569, 2022.\n[14] C. Gao, Y . Li, S. Qi et al., “Listening to Users’ V oice: Automatic Sum-\nmarization of Helpful App Reviews,” IEEE Transactions on Reliability ,\npp. 1–13, 2022.\n[15] C. Stanik, M. Haering, and W. Maalej, “Classifying Multilingual User\nFeedback using Traditional Machine Learning and Deep Learning,” in\n2019 IEEE 27th International Requirements Engineering Conference\nWorkshops (REW), 2019, pp. 220–226.\n[16] J. Wei, A.-L. Courbis, T. Lambolais et al. , “Towards a Data-Driven\nRequirements Engineering Approach: Automatic Analysis of User\nReviews,” in 7th National Conference on Practical Applications of\nArtificial Intelligence , 2022. [Online]. Available: http://arxiv.org/abs/\n2206.14669\n[17] W. X. Zhao, K. Zhou, J. Li et al. , “A Survey of Large Language\nModels,” pp. 1–85. [Online]. Available: http://arxiv.org/abs/2303.18223\n2023.\n[18] T. Brown, B. Mann, N. Ryder et al. , “Language Models are Few-\nShot Learners,” in Advances in Neural Information Processing Systems ,\nH. Larochelle, M. Ranzato, R. Hadsell et al. , Eds., vol. 33. Curran\nAssociates, Inc., 2020, pp. 1877–1901.\n[19] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “QLoRA:\nEfficient Finetuning of Quantized LLMs.” [Online]. Available:\nhttp://arxiv.org/abs/2305.14314 2023.\n[20] H. Touvron, T. Lavril, G. Izacard et al. , “Llama: Open and efficient\nfoundation language models,” arXiv preprint arXiv:2302.13971 , 2023.\n[21] H. Su, W. Shi, J. Kasai et al. , “One Embedder, Any Task:\nInstruction-Finetuned Text Embeddings.” [Online]. Available: http:\n//arxiv.org/abs/2212.09741 2022.\n[22] L. McInnes, J. Healy, and S. Astels, “hdbscan: Hierarchical density\nbased clustering,” The Journal of Open Source Software , vol. 2, no. 11,\np. 205, 2017.\n[23] J. Wang, Y . Liang, F. Meng et al. , “Cross-Lingual Summarization via\nChatGPT.” [Online]. Available: http://arxiv.org/abs/2302.14229 2023.\n[24] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language understanding,”\nin Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics , vol. 1, oct 2019, pp.\n4171–4186.\n[25] L. Martin, B. Muller, P. J. Ortiz Su ´arez et al. , “CamemBERT: a Tasty\nFrench Language Model,” in Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics , 2020, pp. 7203–7219.\n[26] S. Ruder, A. Søgaard, and I. Vulic, “Unsupervised cross-lingual repre-\nsentation learning,” in ACL 2019, nov 2019, pp. 31–38.\n[27] N. X. Vinh, J. Epps, and J. Bailey, “Information theoretic measures\nfor clusterings comparison: Is a correction for chance necessary?” ACM\nInternational Conference Proceeding Series , vol. 382, pp. 1073–1080,\n2009.\n[28] L. Hubert and P. Arabie, “Comparing partitions,” Journal of Classifica-\ntion, vol. 2, no. 1, pp. 193–218, 1985.\n[29] D. Cer, Y . Yang, S. yi Kong et al. , “Universal sentence encoder for\nEnglish,” EMNLP 2018 - Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, Proceedings , pp. 169–\n174, 2018.\n[30] W. Wang, F. Wei, L. Dong et al. , “MINILM: Deep self-attention\ndistillation for task-agnostic compression of pre-trained transformers,”\nAdvances in Neural Information Processing Systems , 2020.\n[31] K. Song, X. Tan, T. Qin et al. , “MPNet: Masked and permuted pre-\ntraining for language understanding,” Advances in Neural Information\nProcessing Systems, vol. 2020-Decem, no. NeurIPS, pp. 1–14, 2020.\n[32] L. Wang, N. Yang, X. Huang et al. , “Text Embeddings by Weakly-\nSupervised Contrastive Pre-training,” pp. 1–17. [Online]. Available:\nhttp://arxiv.org/abs/2212.03533 2022.\n[33] A. R. Fabbri, W. Kry ´sci´nski, B. McCann et al. , “Summeval: Re-\nevaluating summarization evaluation,” Transactions of the Association\nfor Computational Linguistics , vol. 9, pp. 391–409, 2021.\n7",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8459845781326294
    },
    {
      "name": "Automatic summarization",
      "score": 0.7304391860961914
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5309794545173645
    },
    {
      "name": "Software",
      "score": 0.4685790240764618
    },
    {
      "name": "Artificial intelligence",
      "score": 0.465107262134552
    },
    {
      "name": "Information retrieval",
      "score": 0.4313516616821289
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.41859617829322815
    },
    {
      "name": "Natural language processing",
      "score": 0.40385934710502625
    },
    {
      "name": "Data mining",
      "score": 0.3724554777145386
    },
    {
      "name": "Machine learning",
      "score": 0.36107760667800903
    },
    {
      "name": "World Wide Web",
      "score": 0.322736918926239
    },
    {
      "name": "Programming language",
      "score": 0.12035787105560303
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}