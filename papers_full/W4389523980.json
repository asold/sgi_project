{
  "title": "Towards Interpretable Mental Health Analysis with Large Language Models",
  "url": "https://openalex.org/W4389523980",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2668353491",
      "name": "Kai-Lai Yang",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A2890561010",
      "name": "Shaoxiong Ji",
      "affiliations": [
        "University of Helsinki"
      ]
    },
    {
      "id": "https://openalex.org/A2101316507",
      "name": "Tianlin Zhang",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A2126157100",
      "name": "Qianqian Xie",
      "affiliations": [
        "University of Manchester"
      ]
    },
    {
      "id": "https://openalex.org/A5002529335",
      "name": "Ziyan Kuang",
      "affiliations": [
        "Jiangxi Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2141504082",
      "name": "Sophia Ananiadou",
      "affiliations": [
        "University of Manchester"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285211483",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3108099470",
    "https://openalex.org/W2749002090",
    "https://openalex.org/W4307893882",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4310720906",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963873807",
    "https://openalex.org/W3176456866",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963686995",
    "https://openalex.org/W4385567092",
    "https://openalex.org/W2099813784",
    "https://openalex.org/W3035327313",
    "https://openalex.org/W4206445734",
    "https://openalex.org/W2953413710",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W2964300796",
    "https://openalex.org/W4223610593",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4287017694",
    "https://openalex.org/W2761590056",
    "https://openalex.org/W2040467972",
    "https://openalex.org/W2740550900",
    "https://openalex.org/W4229049385",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2987392802",
    "https://openalex.org/W4330337479",
    "https://openalex.org/W3209409148",
    "https://openalex.org/W4382202523",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4319459137",
    "https://openalex.org/W4361230777",
    "https://openalex.org/W4323570543",
    "https://openalex.org/W4285605578",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4385565111",
    "https://openalex.org/W3214376504",
    "https://openalex.org/W4379259169",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4322832290",
    "https://openalex.org/W3199412388",
    "https://openalex.org/W3173751215",
    "https://openalex.org/W4301914348",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W2981660166",
    "https://openalex.org/W4389523957",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W4285077770",
    "https://openalex.org/W3162081707",
    "https://openalex.org/W4361230825",
    "https://openalex.org/W2741004402",
    "https://openalex.org/W4296151718",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2252031683",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3033913896",
    "https://openalex.org/W3176528293",
    "https://openalex.org/W2162010436",
    "https://openalex.org/W3103163889",
    "https://openalex.org/W2083944372",
    "https://openalex.org/W4319454867",
    "https://openalex.org/W3175552668",
    "https://openalex.org/W4385573172",
    "https://openalex.org/W2964687216"
  ],
  "abstract": "The latest large language models (LLMs) such as ChatGPT, exhibit strong capabilities in automated mental health analysis. However, existing relevant studies bear several limitations, including inadequate evaluations, lack of prompting strategies, and ignorance of exploring LLMs for explainability. To bridge these gaps, we comprehensively evaluate the mental health analysis and emotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore the effects of different prompting strategies with unsupervised and distantly supervised emotional information. Based on these prompts, we explore LLMs for interpretable mental health analysis by instructing them to generate explanations for each of their decisions. We convey strict human evaluations to assess the quality of the generated explanations, leading to a novel dataset with 163 human-assessed explanations. We benchmark existing automatic evaluation metrics on this dataset to guide future related works. According to the results, ChatGPT shows strong in-context learning ability but still has a significant gap with advanced task-specific methods. Careful prompt engineering with emotional cues and expert-written few-shot examples can also effectively improve performance on mental health analysis. In addition, ChatGPT generates explanations that approach human performance, showing its great potential in explainable mental health analysis.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6056–6077\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nTowards Interpretable Mental Health Analysis with Large Language\nModels\nKailai Yang 1 Shaoxiong Ji ∗2 Tianlin Zhang ∗1 Qianqian Xie †1\nZiyan Kuang 4 Sophia Ananiadou 1,3\n1 The University of Manchester 2 University of Helsinki\n3 Artificial Intelligence Research Center, AIST 4 Jiangxi Normal University\n{kailai.yang,tianlin.zhang}@postgrad.manchester.ac.uk\nshaoxiong.ji@helsinki.fi; {xqq.sincere,plumjane1225}@gmail.com\nsophia.ananiadou@manchester.ac.uk\nAbstract\nThe latest large language models (LLMs) such\nas ChatGPT, exhibit strong capabilities in au-\ntomated mental health analysis. However,\nexisting relevant studies bear several limita-\ntions, including inadequate evaluations, lack of\nprompting strategies, and ignorance of explor-\ning LLMs for explainability. To bridge these\ngaps, we comprehensively evaluate the mental\nhealth analysis and emotional reasoning abil-\nity of LLMs on 11 datasets across 5 tasks. We\nexplore the effects of different prompting strate-\ngies with unsupervised and distantly super-\nvised emotional information. Based on these\nprompts, we explore LLMs for interpretable\nmental health analysis by instructing them to\ngenerate explanations for each of their deci-\nsions. We convey strict human evaluations to\nassess the quality of the generated explanations,\nleading to a novel dataset with 163 human-\nassessed explanations1. We benchmark existing\nautomatic evaluation metrics on this dataset to\nguide future related works. According to the\nresults, ChatGPT shows strong in-context learn-\ning ability but still has a significant gap with ad-\nvanced task-specific methods. Careful prompt\nengineering with emotional cues and expert-\nwritten few-shot examples can also effectively\nimprove performance on mental health analysis.\nIn addition, ChatGPT generates explanations\nthat approach human performance, showing\nits great potential in explainable mental health\nanalysis.\n1 Introduction\nW ARNING: This paper contains examples and de-\nscriptions that are depressive in nature.\nMental health conditions such as depression\nand suicidal ideation seriously challenge global\n∗ Equal contribution, listed alphabetically.\n† Corresponding author. Qianqian is now affiliated with\nYale University. The work was done when she was at The\nUniversity of Manchester.\n1The data is released at https://github.com/\nSteveKGYang/MentalLLaMA\nhealth care (Zhang et al., 2022a). NLP researchers\nhave devoted much effort to automatic mental\nhealth analysis, with current mainstream meth-\nods leveraging the Pre-trained Language Models\n(PLMs) (Yang et al., 2022; Abed-Esfahani et al.,\n2019). Most recently Large Language Models\n(LLMs) (Brown et al., 2020; Ouyang et al., 2022),\nespecially ChatGPT 2 and GPT-4 (OpenAI, 2023),\nhave exhibited strong general language processing\nability (Wei et al., 2022; Luo et al., 2023; Yuan\net al., 2023). In mental health analysis, Lamich-\nhane (2023) evaluated ChatGPT on stress, depres-\nsion, and suicide detection and glimpsed its strong\nlanguage understanding ability to mental health-\nrelated texts. Amin et al. (2023) compared the zero-\nshot performance of ChatGPT on suicide and de-\npression detection with previous fine-tuning-based\nmethods.\nThough previous works depict a promising fu-\nture for a new LLM-based paradigm in mental\nhealth analysis, several issues remain unresolved.\nFirstly, mental health condition detection is a safe-\ncritical task requiring careful evaluation and high\ntransparency for any predictions (Zhang et al.,\n2022a), while these works simply tested on a few\nbinary mental health condition detection tasks and\nlack the explainability on detection results. More-\nover, other important mental health analysis tasks,\nsuch as the cause/factor detection of mental health\nconditions (Mauriello et al., 2021; Garg et al.,\n2022), were ignored. Secondly, previous works\nmostly use simple prompts to detect mental health\nconditions directly. These vanilla methods ig-\nnore useful information, especially emotional cues,\nwhich are widely utilized for mental health analysis\nin previous works (Zhang et al., 2023). We believe\nit requires a comprehensive exploration and evalu-\nation of the ability and explainability of LLMs on\nmental health analysis, including mental health de-\ntection, emotional reasoning, and cause detection\n2https://openai.com/blog/chatgpt\n6056\nAnnotator 3:    \nAnnotator 2:    \n(a) Emotion-enhanced CoT Prompt\nConsider this Post: \n“I am scared as fuck off what the comedown is going \nto be like from being jacked up on adrenaline for 5 days straight. \nI have not felt tired whatsoever during this time...“\nConsider the emotion expressed from the post, then answer this \nquestion: Is the poster likely to suffer from very severe stress? You\nmust return a Yes or No, then explain your reasoning step by step.\n(b) LLM Responses\nYes, the poster is likely to suffer from very severe stress.\nThe emotional language used in the post indicates a high level of \nfear and anxiety, with the phrase \"scared as fuck\" conveying extreme \ndistress. The fact that they have been jacked up on adrenaline for \nfive days straight suggests that they have been in a highly stressful \nsituation for an extended period of time... Overall, the combination\nof emotional and physical symptoms suggests that the poster is \nexperiencing severe stress.\n(d) Automatic Evaluations \n(c) Human Ratings\nHuman \nEvaluation\nLLM \nPrompting\nEvaluation criteria:\n• Fluency evaluates he coherence and \nreadability of the explanation…\n• Reliability measures how trustworthiness \nof the generated explanations…\n• Completeness measures how well the\nexplanations cover all relevant aspects…\nAnnotator 1:\nFluency: 3\nReliability: 3\nCompleteness: 2\nOverall: 2\nGolden\nRatings\nPost\nReferences\nTarget\nResponse\nPost\nResponse\nRatings:\n-2.3\nFigure 1: The pipeline of obtaining and evaluating the LLM-generated explanations for mental health analysis.\nIn LLM responses, red, green, and blue words are marked as relevant clues for rating fluency, reliability, and\ncompleteness in human evaluations.\nof mental health conditions. Therefore, we raise\nthe following three research questions (RQ):\n• RQ 1: How well can LLMs perform in gen-\neralized mental health analysis and emotional\nreasoning with zero-shot/few-shot settings?\n• RQ 2: How do different prompting strategies\nand emotional cues impact the mental health\nanalysis ability of ChatGPT?\n• RQ 3: How well can ChatGPT generate ex-\nplanations for its decisions on mental health\nanalysis?\nTo respond to these research questions, we first\nconduct a preliminary study of how LLMs perform\non mental health analysis and emotional reasoning.\nWe evaluate four LLMs with varying model sizes\nincluding ChatGPT, InstructGPT-3 (Ouyang et al.,\n2022), LLaMA-13B, and LLaMA-7B (Touvron\net al., 2023), on 11 datasets across 5 tasks including\nbinary/multi-class mental health condition detec-\ntion, cause/factor detection of mental health con-\nditions, emotion recognition in conversations, and\ncausal emotion entailment. We then delve into the\neffectiveness of different prompting strategies on\nmental health analysis, including zero-shot prompt-\ning, Chain-of-Thought (CoT) prompting (Kojima\net al., 2022), emotion-enhanced prompting, and\nfew-shot emotion-enhanced prompting. Finally,\nwe explore how LLMs perform for interpretable\nmental health analysis, where we instruct two rep-\nresentative LLMs: ChatGPT and InstructGPT-3,\nto generate natural language explanations for each\nof its results on mental health analysis. To assess\nthe quality of LLMs-generated explanations, we\nperform human evaluations by following a strict\nannotation protocol designed by domain experts,\nand thus create the novel dataset with 163 human-\nassessed explanations of posts from LLMs, aimed\nat facilitating the investigating of explainable men-\ntal health analysis methods and automatic evalu-\nation metrics. We benchmark numerous existing\nautomatic evaluation metrics on the corpus to guide\nfuture research on automatically evaluating explain-\nable mental health analysis. We conclude our find-\nings as follows:\n1) Overall Performance. ChatGPT achieves the\nbest performance among all examined LLMs, al-\nthough it still significantly underperforms advanced\nsupervised methods, highlighting the challenges of\nemotion-related subjective tasks.\n2) Prompting Strategies. While a simple CoT\ntrigger sentence is ineffective for mental health\nanalysis, ChatGPT with unsupervised emotion-\nenhanced CoT prompts achieves the best perfor-\nmance, showing the importance of prompt engi-\nneering in leveraging emotional cues for mental\nhealth analysis. Few-shot learning from expert-\nwritten examples also significantly improves model\nperformance.\n3) Explainability. ChatGPT can generate\napproaching-human explanations for its classifica-\ntions, indicating its potential to enhance the trans-\n6057\nparency of mental health analysis. Current best\nautomatic evaluation metrics can moderately cor-\nrelate with human evaluations, indicating the need\nfor developing customized automatic evaluation\nmethods in explainable mental health analysis.\n4) Limitations. Although its great potential,\nChatGPT bears limitations on inaccurate reasoning\nand unstable predictions caused by its excessive\nsensitivity to minor alterations in prompts, inspir-\ning future directions on improving ChatGPT and\nprompts. Unstable prediction problems can be mit-\nigated by few-shot learning.\nOur contributions can be summarized as follows:\n1) We evaluate four representative LLMs on mental\nhealth analysis, 2) We investigate the effectiveness\nof prompting strategies including CoT, emotion-\nenhanced prompts, and few-shot learning for men-\ntal health analysis, 3) We explore LLMs for explain-\nable mental health analysis, and conduct human\nand automatic evaluations on LLMs-generated ex-\nplanations, 4) We create the first evaluation dataset\nwith LLMs-generated explanations rigorously as-\nsessed by domain experts, for examining and de-\nveloping of automatic evaluation metrics, 5) We\nanalyze the potential and limitations of LLMs and\ndifferent prompting strategies for mental health\nanalysis.\n2 Methodology\nThis section introduces the details of evaluated\nLLMs and different prompting strategies for im-\nproving LLMs’ efficiency and explainability in\nmental health analysis. Due to the page limits,\nall evaluations, experiments, and analyses on emo-\ntional reasoning are presented in Appendix B. We\nalso perform human evaluations on the quality of\nLLM-generated explanations and benchmark ex-\nisting automatic evaluation metrics on the human\nevaluation results, where an example is shown in\nFigure 1.\n2.1 Large Language Models\nWe benchmark the following powerful LLMs for\nthe zero-shot mental health analysis:\n1) LLaMA-7B/13B. LLaMA (Touvron et al.,\n2023) is a set of open-source LLMs developed by\nMeta AI, which are generatively pre-trained on\nentirely publicly available datasets. We test the\nzero-shot mental health analysis tasks on LLaMA\nmodels with 7 billion (LLaMA-7B) and 13 billion\n(LLaMA-13B) parameters.\n2) InstructGPT-3. InstructGPT-3 continually\ntrains GPT-3 (Brown et al., 2020) with instruction\ntuning (Ouyang et al., 2022), which enables the\nmodel to solve tasks in a question-answering for-\nmat. We utilize curie-instruct-beta version (13\nbillion parameters) in our experiments.\n3) ChatGPT.ChatGPT (gpt-3.5-turbo) is trained\nbased on the 175 billion parameters version of In-\nstructGPT (Ouyang et al., 2022) and continually\noptimized through reinforcement learning from hu-\nman feedback (RLHF) (Stiennon et al., 2020).\n2.2 In-context Learning as Explainable\nMental Health Analyzer\nIn-context learning (Brown et al., 2020) elicits the\npowerful ability of LLMs given the information\nprovided in the context without explicit updates of\nmodel parameters. We instruct LLMs with task-\nspecific instructions to trigger their ability as the\nzero-shot analyzer for different mental health anal-\nysis tasks. We systematically explore three differ-\nent prompting strategies for mental health analy-\nsis, i.e., straightforward zero-shot prompting with\nnatural language query, emotion-enhanced Chain-\nof-Thought (CoT) (Wei et al., 2022), and distantly\nsupervised emotion-enhanced instructions. The\nstraightforward zero-shot prompting guides four\nLLMs by asking for a classification result from\ntheir responses. For example, for binary mental\nhealth condition detection, we design the following\nprompt:\nPost: \"[Post]\". Consider this post to answer\nthe question: Is the poster likely to suffer from\nvery severe [Condition]? Only return Yes or\nNo.\nEmotion-enhanced Prompts Moreover, we de-\nsign three emotion-enhanced prompting strategies\nto better instruct ChatGPT to conduct explainable\nmental health analysis: 1) Emotion-enhanced\nCoT prompting. We perform emotion infusion\nby designing unsupervised emotion-enhanced zero-\nshot CoT prompts, where the emotion-related part\ninspires the LLM to concentrate on the emotional\nclues from the post, and the CoT part guides the\nLLM to generate step-by-step explanations for\nits decision. This improves the explainability of\nLLMs’ performance. For example, for the binary\ndetection task, we modify the zero-shot prompt as\nfollows:\nPost: \"[Post]\". Consider the emotions ex-\npressed from this post to answer the question:\n6058\nIs the poster likely to suffer from very severe\n[Condition]? Only return Yes or No, then ex-\nplain your reasoning step by step.\n2) Supervised emotion-enhanced prompting. In\naddition, we propose a distantly supervised emo-\ntion fusion method by using sentiment and emo-\ntion lexicons. We utilize the V ADER (Hutto and\nGilbert, 2014) and NRC EmoLex (Mohammad\nand Turney, 2010, 2013) lexicons to assign a sen-\ntiment/emotion score to each post and convert\nthe score to sentiment/emotion labels. Then we\ndesign emotion-enhanced prompts by adding the\nsentiment/emotion labels to the proper positions\nof the zero-shot prompt. 3) Few-shot Emotion-\nenhanced Prompts. We further evaluate the im-\npact of few-shot examples on emotion-enhanced\nprompts. We invite domain experts (Ph.D. students\nmajoring in quantitative psychology) to write one\nresponse example for each label class within a test\nset, where all responses consist of a prediction and\nan explanation describing the rationale behind the\ndecision. We then include these examples in the\nemotion-enhanced prompts to enable in-context\nlearning of the models. For example, for the binary\ndetection task, we modify the original emotion-\nenhanced prompt to combine N expert-written ex-\nplanations in a unified manner:\nYou will be presented with a post. Consider\nthe emotions expressed in this post to identify\nwhether the poster suffers from [condition].\nOnly return Yes or No, then explain your\nreasoning step by step. Here are N examples:\nPost: [example 1]\nResponse: [response 1]\n...\nPost: [example N]\nResponse: [response N]\nPost: [Post]\nResponse:\nTask-specific Instructions We conduct broad\ntests of LLMs’ mental health analysis ability on\nthe following three tasks: binary mental health con-\ndition detection, multi-class mental health condi-\ntion detection, and cause/factor detection of mental\nhealth conditions. Binary mental health condition\ndetection is modeled as a yes/no classification of\nthe mental health condition, such as depression\nand stress from a post. In contrast, multi-class de-\ntection identifies one label from multiple mental\nhealth conditions. Cause/factor detection aims at\nrecognizing one potential cause of a mental health\ncondition from multiple causes. More details about\nthe prompt design and examples of the prompts are\npresented in Appendix C.2.\n2.3 Evaluation for Explainability\nHuman Evaluation We examine the quality of\nthe generated explanations by two representative\nLLMs: ChatGPT and InstructGPT-3, with human\nevaluations on the binary mental health condi-\ntions detection task. We utilize ChatGPT and\nInstructGPT-3 to simultaneously generate expla-\nnations for the same posts with the same emotion-\nenhanced CoT prompts. The annotation protocol is\ndeveloped through collaborative efforts with 2 do-\nmain experts (Ph.D. students majoring in quantita-\ntive psychology) and considerations of human eval-\nuation criteria for other text generation tasks (Wal-\nlace et al., 2021; DeYoung et al., 2021). Specifi-\ncally, four key aspects are assessed: 1) Fluency:\nthe coherence and readability of the explanation.\n2) Reliability: the trustworthiness of the gener-\nated explanations to support the prediction results.\n3) Completeness: how well the generated expla-\nnations cover all relevant aspects of the original\npost. 4) Overall: the general effectiveness of the\ngenerated explanation.\nEach aspect is divided into four standards rating\nfrom 0 to 3. Higher ratings reflect more satisfactory\nperformance and 3 denotes approaching human\nperformance. Each LLM-generated explanation is\nassigned a score by 3 annotators for each corre-\nsponding aspect, followed by the examination of\n1 domain expert. All annotators are PhD students\nwith high fluency in English. We evaluate 121\nposts that are correctly classified by both ChatGPT\n(ChatGPTtrue) and InstructGPT-3 to enable fair\ncomparisons. 42 posts that are incorrectly classi-\nfied by ChatGPT (ChatGPTfalse ) are also collected\nfor error analysis and examination of the automatic\nevaluation metrics. We will release the annotated\ncorpus for facilitating future research. Details of\nthe criteria are described in Appendix E.\nAutomatic Evaluation Though human evalua-\ntions provide an accurate and comprehensive view\nof the generated explanations’ quality, they re-\nquire huge human efforts, making it hard to be\nextended to large-scale datasets. Therefore, we ex-\nplore utilizing automatic evaluation metrics, orig-\ninally developed for generation tasks such as text\nsummarization, to benchmark the evaluation on\n6059\nour annotated corpus. We rely on the ability of\nthe evaluation models to score the fluency, relia-\nbility, and completeness of the explanations. We\nselect the following widely utilized metrics to au-\ntomatically evaluate LLM-generated explanations:\nBLEU (Papineni et al., 2002), ROUGE-1, ROUGE-\n2, ROUGE-L (Lin, 2004), GPT3-Score (Fu et al.,\n2023) (davinci-003), and BART-Score (Yuan et al.,\n2021). We also use the BERT-score-based (Zhang\net al., 2020) methods with different PLMs, in-\ncluding the domain-specific PLMs MentalBERT\nand MentalRoBERTa (Ji et al., 2022b), except for\nBERT and RoBERTa.\n3 Experimental Settings\nMental Health Analysis Firstly, we introduce\nthe benchmark datasets, baseline models, and au-\ntomatic evaluation metrics for the classification\nresults of mental health analysis.\nDatasets. For binary mental health condi-\ntion detection, we select two depression detec-\ntion datasets Depression_Reddit (DR) (Pirina and\nÇöltekin, 2018), CLPsych15 (Coppersmith et al.,\n2015), and another stress detection dataset Dread-\ndit (Turcan and McKeown, 2019). For multi-class\nmental health condition detection, we utilize the\ndataset T-SID (Ji et al., 2022a). For cause/factor\ndetection of mental health conditions, we use a\nstress cause detection dataset called SAD (Mau-\nriello et al., 2021) and a depression/suicide cause\ndetection dataset CAMS (Garg et al., 2022). More\ndetails of these datasets are presented in Table 8 in\nthe appendix.\nBaseline Models. We select the following base-\nline models: CNN (Kim, 2014), GRU (Cho et al.,\n2014), BiLSTM_Att (Zhou et al., 2016), fast-\nText (Joulin et al., 2017), BERT/RoBERTa (De-\nvlin et al., 2019; Liu et al., 2019), and Mental-\nBERT/MentalRoBERTa (Ji et al., 2022b). Details\nabout these baseline models are in Appendix D.2.\nMetrics. We evaluate the model performance\nusing the recall and weighted-F1 scores as the eval-\nuation metric for all mental health datasets. Due to\nimbalanced classes in some datasets such as DR,\nCLPsych15, and T-SID, we use weighted-F1 scores\nfollowing previous methods. In addition, it is cru-\ncial to minimize false negatives, which refers to\ncases where the model fails to identify individuals\nwith mental disorders. Therefore, we also report\nthe recall scores.\nEvaluation for Explainability For the human\nevaluation results, we evaluate the quality of the\nannotations by calculating the inter-evaluator agree-\nment: Fleiss’ Kappa statistics (Fleiss et al., 2013)\nfor each aspect. Any annotations with a major-\nity vote are considered as reaching an agreement.\nTo compare the automatic evaluation methods, we\nalso compute Pearson’s correlation coefficients be-\ntween the automatic evaluation results and the hu-\nman evaluation results, where higher values reflect\nmore linear correlations between the two sets of\ndata.\n4 Results and Analysis\nWe conduct all LLaMA experiments on a single\nNvidia Tesla A100 GPU with 80GB of memory.\nInstructGPT-3 and ChatGPT results are obtained\nvia the OpenAI API. Each prompt is fed indepen-\ndently to avoid the effects of dialogue history.\n4.1 Mental Health Analysis\nThe experimental results of mental health analy-\nsis are presented in Table 1. We first compare the\nzero-shot results of LLMs to gain a straight view\nof their potential in mental health analysis, then an-\nalyze ChatGPT’s performance with other prompts\nenhanced by emotional information.\nZero-shot Prompting. In the comparison\nof LLMs, ChatGPT significantly outperforms\nLLaMA-7B/13B and InstructGPT-3 on all datasets.\nLLaMA-7BZS displays random-guessing perfor-\nmance on multi-class detection (T-SID) and cause\ndetection (SAD, CAMS), showing its inability\nto perform these more complex tasks. With an\nexpanded model size, LLaMA-13B ZS achieves\nno better performance than LLaMA-7B. Though\ntrained with instruction tuning, InstructGPT-3ZS\nstill does not improve performance, possibly\nbecause the model size limits the LLM’s learning\nability. Compared with supervised methods,\nChatGPTZS significantly outperforms traditional\nlight-weighted neural network-based methods\nsuch as CNN and GRU on binary detection and\ncause/factor detection, showing its potential in\ncause analysis for mental health-related texts.\nHowever, ChatGPTZS struggles to achieve compa-\nrable performance to fine-tuning methods such as\nMentalBERT and MentalRoBERTa. Particularly,\nChatGPTZS achieves much worse performance\nthan all baselines on T-SID. We notice that T-SID\ncollects mostly short posts from Twitter with many\n6060\nModel DR CLPsych15 Dreaddit T-SID SAD CAMS\nRec. F1 Rec. F1 Rec. F1 Rec. F1 Rec. F1 Rec. F1\nSupervised Methods\nCNN 80.54 79.78 51.67 40.28 65.31 64.99 71.88 71.77 39.71 38.45 36.26 34.63\nGRU 61.72 62.13 50.00 46.76 55.52 54.92 67.50 67.35 35.91 34.79 34.19 29.33\nBiLSTM_Att 79.56 79.41 51.33 39.20 63.22 62.88 66.04 65.77 37.23 38.50 34.98 29.49\nfastText 83.99 83.94 58.00 56.48 66.99 66.92 69.17 69.09 38.98 38.32 40.10 34.92\nBERT 91.13 90.90 64.67 62.75 78.46 78.26 88.44 88.51 62.77 62.72 40.26 34.92\nRoBERTa 95.07 95.11 67.67 66.07 80.56 80.56 88.75 88.76 66.86 67.53 41.18 36.54\nMentalBERT 94.58 94.62 64.67 62.63 80.28 80.04 88.65 88.61 67.45 67.34 45.69 39.73\nMentalRoBERTa 94.33 94.23 70.33 69.71 81.82 81.76 88.96 89.01 68.61 68.44 50.48 47.62\nZero-shot LLM-based Methods\nLLaMA-7BZS 63.55 58.91 57.0 56.26 54.83 53.51 23.04 25.55 10.53 11.04 13.92 16.34\nLLaMA-13BZS 67.24 54.07 50.0 39.29 47.83 36.28 23.04 25.27 12.57 13.2 13.12 14.64\nInstructGPT-3ZS 58.87 58.66 50.33 49.86 50.07 49.88 27.60 26.27 12.70 9.36 10.70 12.23\nChatGPTZS 82.76 82.41 60.33 56.31 72.72 71.79 39.79 33.30 55.91 54.05 32.43 33.85\nEmotion-enhanced CoT LLM-based Methods\nChatGPTV 79.51 78.01 59.20 56.34 74.23 73.99 40.04 33.38 52.49 50.29 28.48 29.00\nChatGPTN_sen 80.00 78.86 58.19 55.50 70.87 70.21 39.00 32.02 52.92 51.38 26.88 27.22\nChatGPTN_emo 79.51 78.41 58.19 53.87 73.25 73.08 39.00 32.25 54.82 52.57 35.20 35.11\nChatGPTCoT 82.72 82.9 56.19 50.47 70.97 70.87 37.66 32.89 55.18 52.92 39.19 38.76\nChatGPTCoT_emo 83.17 83.10 61.41 58.24 75.07 74.83 34.76 27.71 58.31 56.68 43.11 42.29\nChatGPTCoT_emo_FS 85.73 84.22 63.93 61.63 77.80 75.38 49.03 43.95 66.05 63.56 48.75 45.99\nTable 1: Test results on the mental health analysis tasks. ChatGPTV , ChatGPTN_sen, and ChatGPTN_emo denote\nthe emotion-enhanced prompting methods with V ADER sentiments, NRC EmoLex sentiments, and NRC EmoLex\nemotions. ChatGPTCoT and ChatGPTCoT _emo denote the zero-shot and emotion-enhanced CoT methods on the\ncorresponding task. ChatGPTCoT _emo_FS combines expert-written few-shot examples in the emotion-enhanced\nprompt. The results of baseline methods are referenced from (Ji et al., 2022b).\nusernames, hashtags, and slang words. The huge\ngap between the posts and ChatGPT’s training\ndata can make zero-shot detection difficult (Koco´n\net al., 2023). Moreover, although the zero-shot\nCoT prompting is proven to be effective on most\nNLP tasks (Zhong et al., 2023; Wei et al., 2022;\nKojima et al., 2022), we surprisingly find that\nChatGPTCoT has a comparable or even worse\nperformance with ChatGPT ZS . This illustrates\nthat the simple CoT trigger sentence is not\neffective in mental health analysis. Overall,\nChatGPT significantly outperforms other LLMs,\nand exhibited some generalized ability for mental\nhealth analysis. However, it still underperforms\nfine-tuning-based methods, leaving a huge gap in\nfurther exploring LLMs’ mental health analysis\nability.\nEmotion-enhanced Prompting. We further test\nChatGPT with emotion-enhanced prompts on all\ndatasets. Firstly, with the sentiment information\nfrom the lexicon V ADER and NRC EmoLex, we\nnotice that ChatGPTV and ChatGPTN_sen perform\nworse than ChatGPTZS on most datasets, showing\nthat these prompts are not effective in enhancing\nmodel performance. A possible reason is that the\ncoarse-grained sentiment classifications based on\nthe two lexicons cannot describe complex emotions\nexpressed in the posts. Therefore, we incorporate\nfine-grained emotion labels from NRC EmoLex\ninto the zero-shot prompt. The results show that\nChatGPTN_emo outperforms ChatGPT N_sen on\nmost datasets, especially on CAMS (a 7.89% im-\nprovement). However, ChatGPT N_emo still un-\nderperforms ChatGPT ZS on most datasets, pos-\nsibly because lexicon-based emotion labels are\nstill not accurate in representing multiple emo-\ntions that co-exist in a post, especially in datasets\nwith rich content, such as CLPsych15 and DR.\nTherefore, we explore the more flexible unsuper-\nvised emotion-enhanced prompts with CoT. As\na result, ChatGPT CoT _emo outperforms all other\nzero-shot methods on most datasets, which proves\nthat emotion-enhanced CoT prompting is effective\nfor mental health analysis. Finally, with few-shot\nexpert-written examples, ChatGPTCoT _emo_FS sig-\nnificantly outperforms all zero-shot methods on all\ndatasets, especially in complex-task datasets: t-sid\n16.24% improvement, SAD 6.88% improvement,\nand CAMS 3.7% improvement (very approaching\nstate-of-the-art supervised method). These encour-\naging results show that in-context learning is ef-\nfective in calibrating LLM’s decision boundaries\nfor complex and subjective tasks in mental health\nanalysis. We provide case studies in Appendix F.1.\n6061\nModel Sample Num. Avg. Token Num. Agreement Fluency Reliability Completeness Overall\nChatGPT 163 237 96.6% 0.94 0.53 0.39 0.36\nChatGPTtrue 121 203 95.9% 0.95 0.58 0.40 0.38\nChatGPTfalse 42 335 98.8% 0.91 0.34 0.38 0.28\nInstructGPT-3 121 203 89.9% 0.55 0.58 0.63 0.62\nTable 2: Fleiss’ Kappa and other statistics of human evaluations on ChatGPT and InstructGPT-3 results. \"Sample\nNum.\" and \"Avg Token Num.\" denote the sample numbers and average token numbers of the posts. \"Agreement\"\ndenotes the percentages of results that reached a final agreement with a majority vote from the three assignments.\nFigure 2: Box plots of the aggregated human evaluation scores for each aspect. Orange lines denote the median\nscores and green lines denote the average scores.\n4.2 Evaluation Results for Explainability\nHuman Evaluation In the above subsection, we\nhave shown that emotion-enhanced CoT prompts\ncan enhance ChatGPT’s zero-shot performance in\nmental health analysis. Moreover, it can prompt\nLLMs to provide an explanation of their step-by-\nstep reasoning for each response. This can signifi-\ncantly improve the explainability of the predictions,\nwhich is a key advantage compared with most previ-\nous black-box methods. In this subsection, we pro-\nvide carefully designed human evaluations to gain a\nclear view of LLMs’ (ChatGPT and InstructGPT-3)\nexplainability on their detection results.\nThe Fleiss’ Kappa results and agreement per-\ncentages are presented in Table 2. We aggregate\neach score by averaging assignments from three\nannotators, and the distributions are presented in\nFigure 2. Firstly, the three annotators reach high\nagreements on evaluation. Over 95% of ChatGPT\nevaluations and 89.9% of InstructGPT-3 results\nreach agreement. According to the widely utilized\ninterpretation criterion 3, all Fleiss’ Kappa statis-\ntics achieve at least fair agreement (≥0.21) and 10\nout of 16 results reach at least moderate agreement\n(≥0.41). These outcomes prove the quality of the\nhuman annotations.\nAs shown in Figure 2, ChatGPT true almost\nachieves an average score of 3.0 in fluency and\nstably maintains outstanding performance, while\n3https://en.wikipedia.org/wiki/Fleiss%27_kappa\nInstructGPT-3 achieves much worse performance\nin fluency with a 0 median score and an average\nscore of less than 1.0. These results prove ChatGPT\nis a fluent explanation generator for mental health\nanalysis. In reliability, ChatGPTtrue achieves a me-\ndian score of 3 and over 2.7 in average score, show-\ning ChatGPT as a trustworthy reasoner in support-\ning its classifications. Only a few of InstructGPT-\n3’s explanations generate moderately reliable infor-\nmation while most of them are unreliable. For com-\npleteness, ChatGPTtrue obtains over 2.5 scores on\naverage, indicating that ChatGPT can cover most\nof the relevant content in the posts to explain its\nclassifications, while InstructGPT-3 ignores key as-\npects by obtaining less than 0.5 on average. Over-\nall, ChatGPTtrue has an average score of over 2.5,\nproving that ChatGPT can generate human-level ex-\nplanations for correct classifications regarding flu-\nency, reliability, and completeness and significantly\noutperforms previous LLMs such as InstructGPT-3.\nMore cases are in Appendix F.2.\nAutomatic Evaluation The automatic evaluation\nresults on the ChatGPT explanations are presented\nin Table 3. In ChatGPTtrue, BART-Score achieves\nthe highest correlation scores on all aspects, show-\ning its potential in performing human-like evalua-\ntions for explainable mental health analysis. Specif-\nically, BART-Score outperforms all BERT-Score-\nbased methods, which shows that generative mod-\nels can be more beneficial in evaluating natural\n6062\nMetric ChatGPTtrue ChatGPTfalse\nValue Fluency Reliability Completeness OverallValue Fluency Reliability Completeness Overall\nBLEU-1 0.026 0.050 0.091 0.268 0.205 0.006 -0.002 0.344 0.405 0.352\nROUGE-1 0.226 0.210 0.258 0.400 0.351 0.177 -0.010 0.256 0.539 0.315\nROUGE-2 0.068 0.105 0.153 0.281 0.247 0.037 0.028 0.349 0.515 0.438\nROUGE-L 0.140 0.196 0.249 0.398 0.361 0.099 0.072 0.282 0.505 0.352\nGPT3-Score -1.903 0.350 0.139 0.266 0.277 -1.998 -0.043 0.034 0.389 0.129\nBART-Score -4.046 0.590 0.404 0.406 0.428 -4.459 -0.076 -0.114 0.257 -0.044\nBERT Score-based Methods\nBERT 0.495 0.080 0.104 0.224 0.172 0.470 0.133 0.192 0.321 0.180\nRoBERTa 0.834 0.116 0.139 0.304 0.250 0.825 0.095 0.206 0.306 0.219\nMentalBERT 0.552 0.207 0.214 0.297 0.263 0.534 0.148 0.240 0.255 0.214\nMentalRoBERTa0.763 0.217 0.177 0.286 0.260 0.752 0.129 0.354 0.427 0.373\nTable 3: Pearson’s correlation coefficients between human evaluation and existing automatic evaluation results on\nChatGPT explanations. Best values are highlighted in bold.\nlanguage texts. Unexpectedly, BART-Score also\nsignificantly outperforms GPT3-Score, a zero-shot\nevaluation method based on the powerful LLM\nGPT-3, in all aspects. These results show that\ntask-specific pre-training is important to trigger the\nlanguage model’s ability for the evaluation tasks.\nBART-score is also fine-tuned on text summariza-\ntion and paraphrasing tasks, which are crucial to\nassess relevance and coherence, the two important\nfactors for providing satisfactory evaluations. How-\never, in ChatGPTfalse , BART-Score becomes less\ncompetitive. BERT-Score achieves the best per-\nformances on fluency and reliability, and ROUGE\nmethods outperform others on completeness and\noverall. A possible reason is that BERT-based\nmethods can better distinguish false semantics in\nthe explanations than BART. With longer posts\nin ChatGPT false (Table 2), the matching-based\nmethod ROUGE can more accurately detect the\nuncovered aspects in the posts, and completeness\ncan play a more important role in determining\noverall performance in falsely classified samples.\nIn BERT-Score-based methods, MentalBERT and\nMentalRoBERTa significantly outperform BERT\nand RoBERTa in most aspects, showing that pre-\ntraining on large-scale mental health texts can also\nbenefit automatic evaluation performances of lan-\nguage models. More experiments on InstructGPT-3\nresults are presented in Appendix G.\n4.3 Error Analysis\nWe further analyze some typical errors during our\nexperiments to inspire future efforts of improving\nChatGPT and emotion-enhanced prompts for men-\ntal health analysis.\nUnstable Predictions. We notice that ChatGPT’s\nperformance on mental health analysis can vary\ndrastically with the change of a few keywords in\nChatGPT DR CLPsych15 Dreaddit\nZero-shot prompts\nany 82.41 56.31 53.10\nsome 74.44 56.59 50.62\nvery severe 78.65 47.55 71.79\nVariance 10.6 17.62 89.29\nFew-shot prompts\nany 84.74 57.24 65.62\nsome 86.9 61.63 62.0\nvery severe 84.22 55.19 75.38\nVariance 1.34 7.21 31.93\nTable 4: Change of ChatGPT’s weighted-F1 perfor-\nmance with different adjectives with zero-shot/few-shot\nprompting strategies.\nthe prompt, especially on binary mental health con-\ndition detection. While keywords describing the\ntasks are easy to control, some other words such as\nadjectives, are hard to optimize. For example, we\nreplace the adjective describing the mental health\ncondition with different degrees in the zero-shot\nprompt for binary mental health detection:\n...Is the poster likely to suffer from [Adjective\nof Degree] [Condition]?...\nwhere the adjective (marked red) is replaced with\none keyword from {any, some, very severe}, and\nthe results on three binary detection datasets are\nshown in Table 4. As shown, ChatGPTZS shows\nvery unstable performance on all three datasets,\nwith a high variance of 10.6 on DR, 17.62 on CLP-\nsysch15, and 89.29 on Dreaddit. There are also\nno global optimal adjectives as the best adjective\nchanges with the datasets. This sensitivity makes\nChatGPT’s performance very unstable even with\nslightly different prompts. We believe this prob-\nlem is due to the subjective nature of mental health\nconditions. The human annotations only answer\n6063\nYes/No for each post, which makes the human cri-\nteria of predictions hard to learn for ChatGPT in a\nzero-shot setting. To alleviate this problem, we fur-\nther explore the effectiveness of few-shot prompts\nin these settings, where the same expert-written\nfew-shot examples in Sec. 2.2 are included in the\nzero-shot prompts. As the results in Table 4 show,\nwith few-shot prompts, ChatGPT achieves a vari-\nance of 1.34 on DR, 7.21 on CLPsych15, and 31.93\non Dreaddit, which are all significantly lower than\nthose of zero-shot prompts. These results prove\nthat expert-written examples can stabilize Chat-\nGPT’s predictions, because they can provide ac-\ncurate references for the subjective mental health\ndetection and cause detection tasks. The few-shot\nsolution is also efficient as it instructs the model\nin an in-context learning manner, which doesn’t\nrequire high-cost model fine-tuning.\nInaccurate Reasoning. Though ChatGPT is\nproven capable of generating explanations for its\nclassifications, there are still many cases showing\nits inaccurate reasoning leading to incorrect results.\nTo investigate the contributing factors behind these\nmistakes, we further compare the human evalua-\ntion results between the correctly and incorrectly\nclassified results ChatGPTtrue and ChatGPTfalse .\nThe results are presented in Figure 2. As shown,\nChatGPTfalse still achieves comparable fluency\nscores to ChatGPTtrue but performs worse on both\ncompleteness and reliability. For completeness,\nthe average score of ChatGPT false drops below\n2.0. We also notice that the average token num-\nber of ChatGPTfalse reaches 335 (Table 2), which\nexceeds ChatGPTtrue by over 130 tokens. These\nresults indicate that ChatGPT struggles to cover all\nrelevant aspects of long-context posts. For relia-\nbility, more than half of ChatGPTfalse results give\nunreliable or inconsistent explanations (below 1.0),\npossibly due to the lack of mental health-related\nknowledge. A few ChatGPTfalse samples provide\nmostly reliable reasoning (above 2.0) but miss key\ninformation due to the lack of completeness. Over-\nall, the mistakes of ChatGPT are mainly caused\nby ignorance of relevant information in long posts\nand unreliable reasoning process. Therefore, fu-\nture works should improve ChatGPT’s long-context\nmodeling ability and introduce more mental health-\nrelated knowledge to benefit its performance. Inac-\ncurate reasoning also reflects a lack of alignment\nbetween LLMs and mental health analysis tasks.\nA possible solution is to fine-tune the LLMs with\nmental health-related instruction-tuning datasets.\nWe leave LLM-finetuning as future work. More\ncases are provided in Appendix F.3.\n5 Conclusion\nIn this work, we comprehensively studied LLMs\non zero-shot/few-shot mental health analysis and\nthe impact of different emotion-enhanced prompts.\nWe explored the potential of LLMs in explainable\nmental health analysis, by explaining their predic-\ntions via CoT prompting. We developed a reli-\nable annotation protocol for human evaluations of\nLLM-generated explanations and benchmarked ex-\nisting automatic evaluation metrics on the human\nannotations. Experiments demonstrated that men-\ntal health analysis is still challenging for LLMs,\nbut emotional information with proper prompt en-\ngineering can better trigger their ability. Human\nevaluation results showed that ChatGPT can gen-\nerate human-level explanations for its decisions,\nand current automatic evaluation metrics need fur-\nther improvement to properly evaluate explainable\nmental health analysis. ChatGPT also bears limita-\ntions, including unstable predictions and inaccurate\nreasoning.\nIn future work, we will explore domain-specific\nfine-tuning for LLMs to alleviate inaccurate reason-\ning problems. We will also extend the interpretable\nsettings with LLMs to other research domains.\nAcknowledgements\nThis work is supported by the computational shared\nfacility at the University of Manchester and the\nUniversity of Manchester President’s Doctoral\nScholar award. This work is supported by the\nproject JPNP20006 from New Energy and In-\ndustrial Technology Development Organization\n(NEDO). Shaoxiong Ji is supported by the Eu-\nropean Union’s Horizon 2020 research and inno-\nvation program (agreement No 771113) and the\nEU’s Horizon Europe research and innovation pro-\ngram under grant agreement No 101070350 and\nUK Research and Innovation (UKRI) under the UK\ngovernment’s Horizon Europe funding guarantee\n[grant number 10052546], also thanks the CSC -\nIT Center for Science, Finland for computational\nresources.\nLimitations\nUnexpected Responses. Though ChatGPT\nmakes predictions in most of its responses as\n6064\nrequested by the prompts, there are a few cases\nwhere it refuses to make a classification. There are\ntwo main reasons: 1) the lack of evidence from\nthe post to make a prediction; 2) the post contains\ncontent that violates the content policy of OpenAI4.\nFor example, ChatGPT can respond: “As an AI\nlanguage model, I cannot accurately diagnose\nmental illnesses or predict what may have caused\nthem in this post.” In our experiments, we directly\nexclude these responses because they are very\nrare, but future efforts are needed to alleviate these\nproblems.\nLimitations of Lexicons. The motivation for us-\ning sentiment and emotion lexicons is to provide\nadditional context with distant supervision for the\nprompts, which, however, have several limitations.\nThe two lexicons, V ADER (Hutto and Gilbert,\n2014) and NRC EmoLex (Mohammad and Tur-\nney, 2010, 2013) we used were developed a decade\nago with human annotation using social media data.\nIt is inevitable that they suffer from annotation bias\nin the sentiment/emotion scores and only reflect\nthe language used when they were developed. The\nInternet language evolves rapidly, and our exper-\niments also use some recent datasets such as T-\nSID (Ji et al., 2022a) and CAMS (Garg et al., 2022).\nBesides, these lexicons have limited vocabularies.\nManual rules to aggregate sentence-level sentiment\nand emotions could be underspecified. Prompt en-\ngineering with other advanced resources with extra\nemotional information can be explored in future\nwork. We also see the limitation of the dataset.\nJi (2022) showed that the sentiment distribution\nhas no significant difference in the binary case of\nT-SID dataset. Although the sentiment-enhanced\nprompt with V ADER gains slightly better perfor-\nmance than other prompts on T-SID dataset, we\ncannot clearly explain if the choice of lexicon con-\ntributes to the improvement due to the black-box\nnature of ChatGPT.\nEthical Considerations\nAlthough the datasets used are anonymously\nposted, our study adheres to strict privacy proto-\ncols (Benton et al., 2017; Nicholas et al., 2020)\nand minimizes privacy impact as much as pos-\nsible, as social media datasets can reveal poster\nthoughts and may contain sensitive personal infor-\nmation. We use social posts that are manifestly\n4https://openai.com/policies/usage-policies\npublic from Reddit and Twitter. The SMS-like\nSAD dataset (Mauriello et al., 2021) has been re-\nleased publicly on GitHub by the authors. All\nexamples presented in our paper have been para-\nphrased and obfuscated using the moderate disguis-\ning scheme (Bruckman, 2002) to avoid misuse. We\nalso do not use the user profile on social media,\nidentify the users or interact with them. Our study\naims to use social media as an early source of infor-\nmation to assist researchers or clinical practition-\ners in detecting mental health conditions for non-\nclinical use. The model predictions cannot replace\npsychiatric diagnoses. In addition, we recognize\nthat some mental disorders are subjective (Keilp\net al., 2012), and the interpretation of our analysis\nmay differ (Puschman, 2017) because we do not\nunderstand the actual intentions of the posts.\nReferences\nPegah Abed-Esfahani, Derek Howard, Marta Maslej,\nSejal Patel, Vamika Mann, Sarah Goegan, and Leon\nFrench. 2019. Transfer learning for depression: Early\ndetection and severity prediction from social media\npostings. In CLEF (Working Notes).\nMostafa M Amin, Erik Cambria, and Björn W Schuller.\n2023. Will affective computing emerge from foun-\ndation models and general ai? a first evaluation on\nchatgpt. arXiv preprint arXiv:2303.03186.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nJames RA Benoit. 2023. Chatgpt for clinical vignette\ngeneration, revision, and evaluation. medRxiv, pages\n2023–02.\nAdrian Benton, Glen Coppersmith, and Mark Dredze.\n2017. Ethical research protocols for social media\nhealth research. In Proceedings of the first ACL work-\nshop on ethics in natural language processing, pages\n94–102.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAmy Bruckman. 2002. Studying the amateur artist: A\nperspective on disguising data collected in human\nsubjects research on the internet. Ethics and Infor-\nmation Technology, 4:217–231.\n6065\nCarlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe\nKazemzadeh, Emily Mower, Samuel Kim, Jean-\nnette N. Chang, Sungbok Lee, and Shrikanth S.\nNarayanan. 2008. IEMOCAP: interactive emotional\ndyadic motion capture database. Lang. Resour. Eval-\nuation, 42(4):335–359.\nXuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng,\nMinlong Peng, Jie Zhou, Tao Gui, Qi Zhang, and\nXuanjing Huang. 2023. How robust is gpt-3.5 to pre-\ndecessors? a comprehensive study on language un-\nderstanding tasks. arXiv preprint arXiv:2303.00293.\nKyunghyun Cho, Bart Merrienboer, Caglar Gulcehre,\nFethi Bougares, Holger Schwenk, and Yoshua Ben-\ngio. 2014. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation.\nIn EMNLP.\nGlen Coppersmith, Mark Dredze, Craig Harman, Kristy\nHollingshead, and Margaret Mitchell. 2015. Clpsych\n2015 shared task: Depression and ptsd on twitter. In\nProceedings of the 2nd Workshop on CLPsych, pages\n31–39.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bai-\nley Kuehl, and Lucy Wang. 2021. Ms^2: Multi-\ndocument summarization of medical studies. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 7494–\n7513.\nJoseph L Fleiss, Bruce Levin, and Myunghee Cho Paik.\n2013. Statistical methods for rates and proportions.\njohn wiley & sons.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894–6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nMuskan Garg, Chandni Saxena, Sriparna Saha, Veena\nKrishnan, Ruchi Joshi, and Vijay Mago. 2022.\nCAMS: An Annotated Corpus for Causal Analysis\nof Mental Health Issues in Social Media Posts. In\nProceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 6387–6396.\nSooji Han, Rui Mao, and Erik Cambria. 2022. Hierar-\nchical attention network for explainable depression\ndetection on Twitter aided by metaphor concept map-\npings. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 94–104,\nGyeongju, Republic of Korea. International Commit-\ntee on Computational Linguistics.\nClayton Hutto and Eric Gilbert. 2014. Vader: A parsi-\nmonious rule-based model for sentiment analysis of\nsocial media text. In Proceedings of the international\nAAAI conference on web and social media, volume 8,\npages 216–225.\nShaoxiong Ji. 2022. Towards intention understanding\nin suicidal risk assessment with natural language pro-\ncessing. In Findings of EMNLP, pages 4028–4038.\nShaoxiong Ji, Xue Li, Zi Huang, and Erik Cambria.\n2022a. Suicidal ideation and mental disorder detec-\ntion with attentive relation networks. Neural Com-\nputing and Applications, 34:10309–10319.\nShaoxiong Ji, Tianlin Zhang, Luna Ansari, Jie Fu,\nPrayag Tiwari, and Erik Cambria. 2022b. Mental-\nbert: Publicly available pretrained language mod-\nels for mental healthcare. In Proceedings of the\nThirteenth Language Resources and Evaluation Con-\nference, pages 7184–7190. European Language Re-\nsources Association (ELRA).\nZhengping Jiang, Sarah Ita Levitan, Jonathan Zomick,\nand Julia Hirschberg. 2020. Detection of mental\nhealth from Reddit via deep contextualized repre-\nsentations. In Proceedings of the 11th International\nWorkshop on Health Text Mining and Information\nAnalysis, pages 147–156, Online. Association for\nComputational Linguistics.\nWenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? a preliminary study. arXiv preprint\narXiv:2301.08745.\nArmand Joulin, Édouard Grave, Piotr Bojanowski, and\nTomáš Mikolov. 2017. Bag of tricks for efficient text\nclassification. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: Volume 2, Short Papers,\npages 427–431.\nJohn G Keilp, Michael F Grunebaum, Marianne Gorlyn,\nSimone LeBlanc, Ainsley K Burke, Hanga Galfalvy,\nMaria A Oquendo, and J John Mann. 2012. Suici-\ndal ideation and the subjective aspects of depression.\nJournal of affective disorders, 140(1):75–81.\nYoon Kim. 2014. Convolutional neural networks\nfor sentence classification. In Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 1746–1751,\nDoha, Qatar. Association for Computational Linguis-\ntics.\n6066\nJan Koco´n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szydło, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, et al. 2023. Chatgpt: Jack of all trades,\nmaster of none. arXiv preprint arXiv:2302.10724.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nBishal Lamichhane. 2023. Evaluation of chatgpt for\nnlp-based mental health applications. arXiv preprint\narXiv:2303.15727.\nJiangnan Li, Fandong Meng, Zheng Lin, Rui Liu, Peng\nFu, Yanan Cao, Weiping Wang, and Jie Zhou. 2022.\nNeutral utterances are also causes: Enhancing con-\nversational causal emotion entailment with social\ncommonsense knowledge. In Proceedings of the\nThirty-First International Joint Conference on Artifi-\ncial Intelligence, IJCAI 2022, Vienna, Austria, 23-29\nJuly 2022, pages 4209–4215. ijcai.org.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. DailyDialog: A manually\nlabelled multi-turn dialogue dataset. In Proceedings\nof the Eighth International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 986–995, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nChenhao Lin, Pengwei Hu, Hui Su, Shaochun Li, Jing\nMei, Jie Zhou, and Henry Leung. 2020. Sensemood:\ndepression detection on social media. In Proceedings\nof the 2020 international conference on multimedia\nretrieval, pages 407–411.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor abstractive text summarization. arXiv preprint\narXiv:2303.15621.\nNavonil Majumder, Soujanya Poria, Devamanyu Haz-\narika, Rada Mihalcea, Alexander F. Gelbukh, and\nErik Cambria. 2019. Dialoguernn: An attentive\nRNN for emotion detection in conversations. In The\nThirty-Third AAAI Conference on Artificial Intelli-\ngence, AAAI 2019, The Thirty-First Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 - February 1, 2019,\npages 6818–6825. AAAI Press.\nMatthew Louis Mauriello, Thierry Lincoln, Grace Hon,\nDorien Simon, Dan Jurafsky, and Pablo Paredes.\n2021. Sad: A stress annotated dataset for recog-\nnizing everyday stressors in sms-like conversational\nsystems. In Extended abstracts of the 2021 CHI\nconference on human factors in computing systems,\npages 1–7.\nSaif Mohammad and Peter Turney. 2010. Emotions\nevoked by common words and phrases: Using Me-\nchanical Turk to create an emotion lexicon. In Pro-\nceedings of the NAACL HLT 2010 Workshop on Com-\nputational Approaches to Analysis and Generation\nof Emotion in Text, pages 26–34, Los Angeles, CA.\nAssociation for Computational Linguistics.\nSaif M. Mohammad and Peter D. Turney. 2013. Crowd-\nsourcing a word-emotion association lexicon. Com-\nputational Intelligence, 29(3):436–465.\nThong Nguyen, Andrew Yates, Ayah Zirikly, Bart\nDesmet, and Arman Cohan. 2022. Improving the\ngeneralizability of depression detection by leverag-\ning clinical questionnaires. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8446–8459, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nJennifer Nicholas, Sandersan Onie, and Mark E Larsen.\n2020. Ethics and privacy in social media research for\nmental health. Current psychiatry reports, 22:1–7.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nInna Pirina and Ça ˘grı Çöltekin. 2018. Identifying de-\npression on Reddit: The effect of training data. In\nProceedings of the 2018 EMNLP Workshop SMM4H:\nThe 3rd Social Media Mining for Health Applications\nWorkshop & Shared Task, pages 9–12.\nSoujanya Poria, Erik Cambria, Devamanyu Hazarika,\nNavonil Majumder, Amir Zadeh, and Louis-Philippe\nMorency. 2017. Context-dependent sentiment anal-\nysis in user-generated videos. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n873–883, Vancouver, Canada. Association for Com-\nputational Linguistics.\nSoujanya Poria, Devamanyu Hazarika, Navonil Ma-\njumder, Gautam Naik, Erik Cambria, and Rada Mi-\nhalcea. 2019a. MELD: A multimodal multi-party\n6067\ndataset for emotion recognition in conversations. In\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 527–\n536, Florence, Italy. Association for Computational\nLinguistics.\nSoujanya Poria, Navonil Majumder, Devamanyu Haz-\narika, Deepanway Ghosal, Rishabh Bhardwaj, Sam-\nson Yu Bai Jian, Pengfei Hong, Romila Ghosh, Ab-\nhinaba Roy, Niyati Chhaya, Alexander F. Gelbukh,\nand Rada Mihalcea. 2021. Recognizing emotion\ncause in conversations. Cogn. Comput., 13(5):1317–\n1332.\nSoujanya Poria, Navonil Majumder, Rada Mihalcea,\nand Eduard H. Hovy. 2019b. Emotion recognition\nin conversation: Research challenges, datasets, and\nrecent advances. IEEE Access, 7:100943–100953.\nCornelius Puschman. 2017. Bad judgment, bad ethics?\nInternet Research Ethics for the Social Age, 95.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nMariana Rodrigues Makiuchi, Tifani Warnita, Kuni-\naki Uto, and Koichi Shinoda. 2019. Multimodal\nfusion of bert-cnn and gated cnn representations for\ndepression detection. In Proceedings of the 9th In-\nternational on Audio/Visual Emotion Challenge and\nWorkshop, pages 55–63.\nWeizhou Shen, Junqing Chen, Xiaojun Quan, and Zhix-\nian Xie. 2021a. Dialogxl: All-in-one xlnet for multi-\nparty conversation emotion recognition. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 13789–13797.\nWeizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun\nQuan. 2021b. Directed acyclic graph network for\nconversational emotion recognition. In ACL, pages\n1551–1560. Association for Computational Linguis-\ntics.\nXiaohui Song, Longtao Huang, Hui Xue, and Songlin\nHu. 2022. Supervised prototypical contrastive learn-\ning for emotion recognition in conversation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5197–\n5206, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008–\n3021.\nErmal Toto, ML Tlachac, and Elke A Rundensteiner.\n2021. Audibert: A deep transfer learning multimodal\nclassification framework for depression screening.\nIn Proceedings of the 30th ACM international con-\nference on information & knowledge management ,\npages 4145–4154.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nElsbeth Turcan and Kathleen McKeown. 2019. Dread-\ndit: A Reddit Dataset for Stress Analysis in Social\nMedia. In Proceedings of the Tenth International\nWorkshop on Health Text Mining and Information\nAnalysis (LOUHI 2019), pages 97–107.\nByron C Wallace, Sayantan Saha, Frank Soboczenski,\nand Iain J Marshall. 2021. Generating (factual?)\nnarrative summaries of rcts: Experiments with neural\nmulti-document summarization. AMIA Summits on\nTranslational Science Proceedings, 2021:605.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nPenghui Wei, Jiahao Zhao, and Wenji Mao. 2020. Ef-\nfective inter-clause modeling for end-to-end emotion-\ncause pair extraction. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3171–3181, Online. Association\nfor Computational Linguistics.\nYunhe Xie, Kailai Yang, Chengjie Sun, Bingquan Liu,\nand Zhenzhou Ji. 2021. Knowledge-interactive net-\nwork with sentiment polarity intensity-aware multi-\ntask learning for emotion recognition in conversa-\ntions. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2879–2889,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nKailai Yang, Tianlin Zhang, Hassan Alhuzali, and\nSophia Ananiadou. 2023. Cluster-level contrastive\nlearning for emotion recognition in conversations.\nIEEE Transactions on Affective Computing , pages\n1–12.\nKailai Yang, Tianlin Zhang, and Sophia Ananiadou.\n2022. A mental state knowledge–aware and con-\ntrastive network for early stress and depression de-\ntection on social media. Information Processing &\nManagement, 59(4):102961.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\nJunjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao,\nShichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong,\nYang Shen, et al. 2023. A comprehensive capability\nanalysis of gpt-3 and gpt-3.5 series models. arXiv\npreprint arXiv:2303.10420.\nChenhan Yuan, Qianqian Xie, and Sophia Ananiadou.\n2023. Zero-shot temporal relation extraction with\nchatgpt. arXiv preprint arXiv:2304.05454.\n6068\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263–27277.\nSayyed M Zahiri and Jinho D Choi. 2017. Emo-\ntion detection on tv show transcripts with sequence-\nbased convolutional neural networks. arXiv preprint\narXiv:1708.04299.\nTianlin Zhang, Annika M Schoene, Shaoxiong Ji, and\nSophia Ananiadou. 2022a. Natural language process-\ning applied to mental illness detection: a narrative\nreview. NPJ digital medicine, 5(1):46.\nTianlin Zhang, Kailai Yang, Shaoxiong Ji, and Sophia\nAnaniadou. 2023. Emotion fusion for mental illness\ndetection from social media: A survey. Information\nFusion, 92:231–246.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nZhiling Zhang, Siyuan Chen, Mengyue Wu, and\nKenny Q. Zhu. 2022b. Psychiatric scale guided risky\npost screening for early detection of depression. In\nProceedings of the Thirty-First International Joint\nConference on Artificial Intelligence, IJCAI 2022,\nVienna, Austria, 23-29 July 2022, pages 5220–5226.\nWeixiang Zhao, Yanyan Zhao, Zhuojun Li, and Bing\nQin. 2022. Knowledge-bridged causal interaction\nnetwork for causal emotion entailment. arXiv\npreprint arXiv:2212.02995.\nPeixiang Zhong, Di Wang, and Chunyan Miao. 2019.\nKnowledge-enriched transformer for emotion de-\ntection in textual conversations. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 165–176, Hong\nKong, China. Association for Computational Lin-\nguistics.\nQihuang Zhong, Liang Ding, Juhua Liu, Bo Du, and\nDacheng Tao. 2023. Can chatgpt understand too?\na comparative study on chatgpt and fine-tuned bert.\narXiv preprint arXiv:2302.10198.\nChunting Zhou, Chonglin Sun, Zhiyuan Liu, and Fran-\ncis Lau. 2015. A c-lstm neural network for text clas-\nsification. arXiv preprint arXiv:1511.08630.\nPeng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen\nLi, Hongwei Hao, and Bo Xu. 2016. Attention-based\nbidirectional long short-term memory networks for\nrelation classification. In Proceedings of the 54th\nannual meeting of the association for computational\nlinguistics (volume 2: Short papers), pages 207–212.\nA Related Work\nA.1 Mental Health Condition Detection\nMost recently, pre-trained language models (PLMs)\nhave been the dominant method for various NLP\ntasks including mental health detection. Jiang et al.\n(2020) proposed the attention-based user-level and\npost-level classification model for mental health de-\ntection with the contextual representations from\nBERT as input features. Zhang et al. (2022b)\nproposed the hierarchical attention network with\nBERT as the post-encoder for early depression de-\ntection. Ji et al. (2022b) released two PLMs, Men-\ntalBERT and MentalRoBERTa, for mental health-\ncare, which are trained with corpus from social\nmedia. Ji (2022) showcased the intention under-\nstanding capacity of PLMs via the mask prediction\ntask and emphasized the importance of intention\nunderstanding in suicidal ideation detection. Dif-\nferent from the above black-box models, Han et al.\n(2022) proposed the explainable depression detec-\ntion model hierarchical attention network (HAN),\nwhich uses the contextual embedding of BERT as\ninput features and incorporates the metaphor con-\ncept mappings (MCMs) as the extra features to\nimprove the interpretability of the model. Nguyen\net al. (2022) proposed to improve the generalizabil-\nity of BERT based depression model by grounding\nthe detection behavior of the depression detection\nmodel to symptoms in PHQ9.\nMoreover, there are also efforts incorporating\nmulti-modal information, such as voice, video,\nvisual, and text, to improve the performance of\ndepression detection. Rodrigues Makiuchi et al.\n(2019) proposed a multi-modal method for depres-\nsion detection, which incorporates speech and tex-\ntual information with a gated convolutional neural\nnetwork (gated CNN) and contextual feature from\nBERT. Lin et al. (2020) proposed the visual-textual\nmulti-modal learning method based on CNN and\nBERT, for depression detection on social media.\nToto et al. (2021) proposed the multi-modal method\nAudio-Assisted BERT (AudiBERT) for depression\nclassification, which integrates the pre-trained au-\ndio embedding with text embedding from the bert\nencoder.\nA.2 Large Language Models for Mental\nHealth Analysis\nMost recently, many efforts have evaluated the per-\nformance of LLMs such as ChatGPT and GPT-\n4 on various NLP tasks (Bang et al., 2023; Qin\n6069\net al., 2023), such as machine translation (Jiao\net al., 2023), text generation and evaluation (Benoit,\n2023; Luo et al., 2023), language inference (Zhong\net al., 2023). They have inspired efforts explor-\ning the ability of LLMs for mental health analy-\nsis. Lamichhane (2023) evaluated the performance\nof ChatGPT on three mental health classification\ntasks, including stress, depression, and suicidality\ndetection, and proved the good potential of Chat-\nGPT for applications of mental health. Amin et al.\n(2023) further evaluated the capabilities of Chat-\nGPT on big-five personality detection, sentiment\nanalysis, and suicide detection. They show Chat-\nGPT has better performance in sentiment analy-\nsis, comparable performance in suicide detection,\nand worse performance in personality detection\nwhen compared with RoBERTa-based and word\nembedding-based supervised methods. There are\nalso works on analyzing the emotional reasoning\nability of ChatGPT including (Qin et al., 2023;\nZhong et al., 2023; Koco´n et al., 2023; Chen et al.,\n2023) on the sentiment classification task, where\nChatGPT achieves comparable or worse perfor-\nmance compared with fine-tuning based methods\nbased on PLMs. Ye et al. (2023) compared the per-\nformance of different LLMs including GPT-3 series\n(davinci and text-davinci-001) and GPT-3.5 series\n(code-davinci-002, text-davinci-002, text-davinci-\n003, and gpt-3.5-turbo) on the aspect-based sen-\ntiment analysis, where code-davinci-002 has the\nbest performance in the zero-shot setting. However,\nmost of them only cover simple binary sentiment\nclassification tasks or a few binary mental health\ndetection tasks, leaving a huge gap for comprehen-\nsively exploring the ability of LLMs on emotion-\naware mental health analysis.\nB ChatGPT for Emotional Reasoning\nTasks We evaluate the emotional reasoning abil-\nity of ChatGPT in complex scenarios on the follow-\ning two widely studied tasks: emotion recognition\nin conversations (ERC) and causal emotion entail-\nment (CEE). ERC aims at recognizing the emotion\nof each utterance within a conversation from a fixed\nemotion category set, which is often modeled as\na multi-class text classification task (Poria et al.,\n2019b). Given an utterance with a non-neutral emo-\ntion, CEE aims to identify the casual utterances for\nthis emotion in the previous conversation history.\nCEE is usually modeled as a binary classification\nbetween the candidate utterance and the target ut-\nterance.\nPrompts We perform direct guidance on explor-\ning the ability of ChatGPT in both tasks, which\ndesigns zero-shot prompts to directly ask for a clas-\nsification result from the response of ChatGPT. De-\ntails about the designed prompts are presented in\nAppendix C.1.\nDatasets For ERC, we select four widely uti-\nlized benchmark datasets: IEMOCAP (Busso et al.,\n2008), MELD (Poria et al., 2019a), EmoryNLP (Za-\nhiri and Choi, 2017), DailyDialog (Li et al., 2017).\nFor CEE, we select the dataset RECCON (Poria\net al., 2021). More information about these datasets\nis listed in Table 7 in the appendix.\nBaseline Models We compare the performance\nof ChatGPT with supervised baseline models. For\nERC, we select CNN (Kim, 2014), cLSTM (Zhou\net al., 2015), CNN+LSTM (Poria et al., 2017), Dia-\nlogueRNN (Majumder et al., 2019), KET (Zhong\net al., 2019), BERT-Base (Devlin et al., 2019),\nRoBERTa-Base (Liu et al., 2019), XLNet (Yang\net al., 2019), DialogXL (Shen et al., 2021a), KI-\nNet (Xie et al., 2021), SCCL (Yang et al., 2023),\nand SPCL (Song et al., 2022). For CEE, we select\nRankCP (Wei et al., 2020), RoBERTa-Base/Large,\nKEC (Li et al., 2022), and KBCIN (Zhao et al.,\n2022). Details about these baseline models are in\nAppendix D.1.\nMetrics We use the weighted-F1 measure as\nthe evaluation metric for IEMOCAP, MELD, and\nEmoryNLP datasets. Since neutral occupies most\nof DailyDialog, we use micro-F1 for this dataset,\nand ignore the label neutral when calculating the\nresults as in the previous works (Shen et al., 2021b;\nXie et al., 2021; Yang et al., 2023). For RECCON,\nwe report the F1 scores of both negative and pos-\nitive causal pairs and the macro F1 scores as a\nwhole.\nERC Results The experimental results on ERC\ntask are presented in Table 5. We can see that\nChatGPTZS using the zero-shot prompting out-\nperforms traditional supervised methods includ-\ning CNN and cLSTM on IEMOCAP, MELD,\nand EmoryNLP datasets, showing its advantage\nover light-weighted supervised methods. In ad-\ndition, ChatGPT ZS achieves comparable perfor-\nmance with CNN+LSTM and DialogueRNN on\nMELD, and EmoryNLP datasets, indicating that its\n6070\nModel IEMOCAP MELD DailyDialog EmoryNLP\nCNN 52.18 55.86 49.34 32.59\ncLSTM 34.84 49.72 49.90 26.01\nCNN+LSTM 55.87 56.87 50.24 32.89\nDialogueRNN 61.21 56.27 50.65 31.70\nKET 59.56 58.18 53.37 34.39\nBERTBase 61.19 56.21 53.12 33.15\nRoBERTaBase 55.67 62.75 55.16 37.0\nXLNet 61.33 61.65 53.62 34.13\nDialogXL 65.94 62.41 54.93 34.73\nKI-Net 66.98 63.24 57.3 –\nSCCL 69.88 65.70 62.51 38.75\nSPCL 69.74 67.25 – 40.94\nChatGPTZS 53.35 61.18 43.27 32.64\nTable 5: Test results on ERC task. ChatGPTZS denotes\nthe method using the zero-shot prompt. The results\nof some baseline methods are referenced from (Zhong\net al., 2019; Song et al., 2022).\ngeneralizability can make up for the lack of task-\nspecific model architectures to some extent. On\nthe MELD dataset, ChatGPTZS achieves 61.18%\nof weighted-F1 score, which outperforms some\nstrong supervised methods including the fine-tuned\nBERTBase model (by 4.97%), and the knowledge\ninfusion method KET (by 3.0%). However, the\nzero-shot performance of ChatGPT is still worse\nthan advanced supervised methods on all datasets,\nand struggles to achieve dominating performance\non the emotion-related tasks. This is because these\ntasks are very subjective even to humans, showing\nthe promising future direction of exploring the few-\nshot prompting and knowledge infusion to improve\nthe performance of ChatGPT in these subjective\ntasks.\nModel Neg. F1 Pos. F1 Macro F1\nRankCP 97.30 33.00 65.15\nRoBERTaBase 88.74 64.28 76.51\nRoBERTaLarge 87.89 66.23 77.06\nKEC 88.85 66.55 77.70\nKBCIN 89.65 68.59 79.12\nChatGPTZS 67.18 51.35 59.26\nTable 6: Test results on the CEE task. Best values: bold.\nThe results of baseline methods are referenced from\nZhao et al. (2022).\nCEE Results The experimental results on CEE\ntask are presented in Table 6. We can observe that\nRankCP achieves the highest negative F1 score\nbut has poor performance on the positive F1 score,\nwhich is more indicative in evaluating the emotion\ncausal detection ability. ChatGPTZS significantly\noutperforms RankCP on positive F1 score, showing\nthat it possesses some level of ability to understand\nthe emotional causes. However, its performance\nis still much lower than the advanced supervised\nmethods such as KEC and KBCIN on all metrics,\nwhich incorporate effective information such as\nsocial commonsense knowledge. Quantitatively,\nChatGPTZS still holds a 19.86% gap to the SOTA\nmethod KBCIN on macro F1 score.\nIn conclusion, the experiments on the ERC and\nCEE tasks show that ChatGPT holds comparable\nemotional reasoning ability in complex contexts\nwith some traditional methods such as CNN and\ncLSTM, but still strongly underperforms compet-\nitive task-specific information infusion and fine-\ntuning methods. This indicates the necessity of\nfuture efforts to enhance prompting strategies and\nleverage external knowledge to better trigger the\nemotional reasoning ability of ChatGPT. These re-\nsults also motivate us to design emotion-enhanced\nprompts to aid mental health analysis.\nC Prompt Engineering\nC.1 Emotional Reasoning\nThe prompt for ERC is designed as follows:\nContext: \"[Previous Dialogue]\". Consider this\ncontext to assign one emotion label to this ut-\nterance \"[Target]\". Only from this emotion list:\n[Emotion List]. Only return the assigned word.\nwhere the slots marked blue are the required inputs.\n[Previous Dialogue] denotes the previous dialogue\nhistory of the target utterance, where each utterance\nis pre-pended with its speaker, then concatenated in\nthe sequence order. [Target]denotes the target ut-\nterance, and [Emotion List] denotes the predefined\nemotion category set of the corresponding dataset,\nwhich are listed in Table 7. Similarly, the prompt\nfor CEE task is designed as follows:\nContext with emotion labels: \"[Previous Dia-\nlogue]\". Consider this context to answer the\nquestion: Did this utterance \"[Query]\" caused\nthe [Target Emotion] emotion of the target ut-\nterance \"[Target]\"? Only return Yes or No.\nwhere [Previous Dialogue] still denotes the dia-\nlogue history with speakers, but each utterance is\nalso post-pended with its emotion label. [Query]\nis the candidate utterance. [Target] and [Target\nEmotion] are the target utterance and its emotion\nlabel.\nC.2 Mental Health Analysis\nZero-shot Prompting We probe LLaMA,\nInstructionGPT-3, and ChatGPT on zero-shot\n6071\nprompts. We design completion-based prompts on\nLLaMA, as it is not trained on instruction tuning.\nFor example, for binary mental health condition\ndetection, we design the following prompt:\nPost: \"[Post]\". The percentage that the poster\nis like to suffer from very severe [Condition] is\nwhere a predicted percentage of more than 50%\nis considered as positive, [Post] denotes the target\npost, [Condition] denotes the target mental health\ncondition such as depression or stress, and [List]\nare the predefined labels presented in Table 8.\nFor InstructionGPT-3 and ChatGPT, we design\ninstruction-based prompts since they are more nat-\nural for classification tasks. Specifically, for binary\nmental health condition detection, we design the\nfollowing prompt:\nPost: \"[Post]\". Consider this post to answer\nthe question: Is the poster likely to suffer from\nvery severe [Condition]? Only return Yes or\nNo.\nFor multi-class mental health detection, we use the\nfollowing prompt:\nPost: \"[Post]\". Consider this post to assign\nonly one mental disorder label to this post from\nthis list: [List]. Only return the assigned label.\nFor cause/factor detection, the prompt is:\nPost: \"[Post]\". Consider this post and assign a\nlabel that causes its [Condition]. Only return\nanswers from one of the labels: [List].\nEmotion-enhanced CoT prompting We design\ninstruction-based prompts as we only test emotion-\nenhanced CoT prompting strategies on ChatGPT.\nSpecifically, for the binary mental health condition\ndetection task, we modify the zero-shot prompt as\nfollows:\nPost: \"[Post]\". Consider the emotions ex-\npressed from this post to answer the question:\nIs the poster likely to suffer from very severe\n[Condition]? Only return Yes or No, then ex-\nplain your reasoning step by step.\nwhere the green parts are the zero-shot CoT en-\nhancements that instructs LLM to generate expla-\nnations, and the red parts are further added on zero-\nshot CoT prompts to obtain the emotion-enhanced\nprompts. Similar modifications are performed on\nthe prompt of multi-class detection and cause/factor\ndetection.\nSupervised emotion-enhanced prompting\nBased on the sentiment lexicons, we assign a\nsentiment score to each post and convert the\nscore to one of the labels: {positive, negative,\nneutral}. NRC EmoLex also contains emotion\nannotations that were assigned from the following\nemotion list: anger, anticipation, disgust, fear, joy,\nsadness, surprise, trust . We regard the emotion\ncategory with the maximum emotion score as\nthe emotion label of the input text. Specifically,\nV ADER contains sentiment annotations with a\nsentiment score between -4 to 4. We utilize the\nNLTK package5 to preprocess the input text (e.g.,\nremoving punctuations and singletons), apply the\npredefined manual rules to obtain word sentiment\nand aggregate the sentence-level sentiment, and\nreturn the normalized sentiment score between -1\nto 1 as the overall sentiment in the post. We set\nthe polarity threshold to 0. For NRC EmoLex, we\nobtain the word’s stem and match the stem with\nthe word in the lexicon when obtaining sentiment\nand emotion scores. Each word’s emotion and\nsentiment scores are summed up to obtain the\nsentence-level sentiment and emotion. We assume\nthere is only one sentiment or emotion in a given\npost and assign the post’s emotion or sentiment\nto the categories with the maximum emotion or\nsentiment scores.\nWe design the supervised emotion-enhanced\nzero-shot Chain-of-Thought (CoT) prompts by\nadding the sentiment/emotion labels to the zero-\nshot prompt. For example, we modify the prompt\nfor multi-class mental health condition detection as\nfollows:\nPost: \"[Post]\". Alice thinks it is [Senti-\nment/Emotion]. Consider this post to assign\nonly one mental disorder label to this post from\nthis list: [List]. Only return the assigned label.\nwhere the green parts are the modifications for\ndistantly supervised emotion infusion, and [Sen-\ntiment/Emotion] denotes the corresponding senti-\nment/emotion label. Modifications for other tasks\nare similar.\nFew-shot emotion-enhanced prompting . In\nthis section, we provide some examples of the few-\nshot emotion-enhanced prompts. The first example\ncomes from DR:\nYou will be presented with a post. Consider\nthe emotions expressed in this post to answer\nthe question: Is the poster likely to suffer from\nvery severe depression? Only return Yes or No,\nthen explain your reasoning step by step. Here\nare two examples:\n5https://www.nltk.org\n6072\nPost: Does everyone else just hurt all the time\nIt’s not like physical pain or soreness, it’s just\nthis overwhelming feeling of exhaustion and\nnumbness...\nResponse: Yes. Reasoning: The post conveys\na deep sense of emotional pain, exhaustion,\nand numbness. The poster describes feeling\noverwhelmed by even simple tasks and\nexpresses a lack of enjoyment or fulfillment\nfrom activities that are typically pleasurable...\nPost: Hello!: ) I’m a new user so if this post\nends up in a weird place/thread, pls bear w me.\nHas anyone used Ancestry.com lately? They\noffer a two week trial...\nResponse: No. Reasoning: The post does not\nexhibit strong emotional indicators of very\nsevere depression. It primarily focuses on a\nspecific concern regarding the safety of...\nPost: Its like that, if you want or not.\nME: I have no problem, if it takes longer. But\nyou asked my friend for help and let him wait\nfor one hour and then you haven’t prepared\nanything. Thats not what you asked for...\nResponse:\nHere is another example for the mental health cause\ndetection dataset SAD:\nYou will be presented post that shows the\nstress of the poster. Assign one label to this\npost only from the following stress causes\nlist: School, Financial problem, Family issues,\nSocial relationships, Work, Health issues,\nEmotional turmoil, Everyday decision making,\nOthers. You must return the assigned labels,\nthen explain your reasoning step by step. Here\nare 7 examples:\nPost: i have been wanting to find another job\nfor some time now\nResponse: Work. Reasoning: The post\nexplicitly mentions that the poster has been\nwanting to find another job for some time now.\nThis indicates that they are not satisfied with\ntheir current job and are experiencing stress in\nrelation to their work situation.\n...\nPost: raising a teenage girl can be stressful\nResponse: Family issues. Reasoning: The\npost specifically mentions the task of raising a\nteenage girl, which falls under the category of\nfamily issues. The stress of parenting can be\noverwhelming, especially when dealing with a\nsensitive age group...\nPost: I got really scared because this\nhappened on the way home.\nResponse:\nD Baseline Models\nD.1 Emotion Reasoning\nWe select the following competitive methods for\nERC:\n• CNN (Kim, 2014) used a single-layer text-\nCNN to model each utterance. The classifica-\ntion is performed on utterance-level without\ncontexts.\n• cLSTM (Zhou et al., 2015) utilized a bi-\ndirectional LSTM to encode each utterance\nand another uni-directional LSTM to model\nthe context.\n• CNN+LSTM (Poria et al., 2017) used a text-\nCNN to model each utterance, then utilized\na uni-directional LSTM to model the context\nbased on the utterance representations.\n• DialogueRNN (Majumder et al., 2019) used\na text-CNN to extract utterance-level features,\nthen use a separate GRU to model each partic-\nipant’s mental states. A global-state GRU is\nused to model the context.\n• KET (Zhong et al., 2019) utilized a hierar-\nchical Transformer to model utterances and\ncontext, and infuse word-level commonsense\nknowledge to enrich the semantics of the con-\ntext.\n• BERT-Base (Devlin et al., 2019) used the\nPLM BERT-Base to directly model the con-\nversation. The utterance representations are\nused to fine-tune the weights.\n• RoBERTa-Base(Liu et al., 2019) used a sim-\nilar training paradigm to BERT-Base but with\nthe PLM RoBERTa.\n• XLNet (Yang et al., 2019) utilized the PLM\nXLNet to directly model the conversation.\nThe segment recurrence was expected to\nmodel long contexts well.\n• DialogXL (Shen et al., 2021a) improved the\nXLNet with the enhanced memory and dialog-\naware self-attention mechanism to capture\n6073\nTask Data Source Dataset Conv./Utter. Emotion Category Set\nERC Acted Script IEMOCAP 31/1,622 neutral, sad, anger, happy, frustrated, excited\nERC TV Show Scripts MELD 280/2,610 neutral, sad, anger, disgust, fear, happy, surprise\nERC TV Show Scripts EmoryNLP 85/1,328 neutral, sad, mad, scared, powerful, peaceful, joyful\nERC Human Written Scripts DailyDialog 1,000/7,740 neutral, happy, surprise, sad, anger, disgust, fear\nCEE Human Written Scripts RECCON 225/2,405 neutral, happy, surprise, sad, anger, disgust, fear\nTable 7: A summary of datasets for emotional reasoning in conversations. Conv. and Utter. denote conversation and\nutterance numbers. Data statistics are on the test set.\nCondition Platform Dataset Post Num. Labels\nDepression Reddit DR 406 Yes, No\nDepression Reddit CLPsych15 300 Yes, No\nStress Reddit Dreaddit 715 Yes, No\nSuicide Twitter T-SID 960 None, Suicide, Depression, PTSD\nStress SMS SAD 685 School, Finance, Family, Social Relation,\nWork, Health, Emotion, Decision, Others\nDepression/Suicide Reddit CAMS 626 None, Bias, Job, Medication, Relation, Alienation\nTable 8: A summary of datasets for mental health tasks. Note we test the zero-shot performance on the test set.\nlong historical context and dependencies be-\ntween multiple parties.\n• KI-Net (Xie et al., 2021) infused both com-\nmonsense and sentiment lexicon knowledge\nto enhance XLNet. A self-matching module\nwas proposed to allow interactions between\nutterance and knowledge representations.\n• SCCL (Yang et al., 2023) proposed a\nsupervised cluster-level contrastive learn-\ning (SCCL) to infuse Valance-Arousal-\nDominance information. Pre-trained knowl-\nedge adapters are leveraged to incorporate lin-\nguistic and factual knowledge.\n• SPCL (Song et al., 2022) used the PLM Sim-\nCSE (Gao et al., 2021) as the backbone model\nwith the supervised prototypical contrastive\nlearning (SPCL) loss.\nFor CEE task, we use the following baseline mod-\nels:\n• RankCP (Wei et al., 2020) ranked the clause-\npair candidates in the context and utilized a\nneural network to perform entailment classifi-\ncation with the context-aware utterance repre-\nsentations.\n• RoBERTa-Base/Large(Liu et al., 2019) con-\ncatenated the conversation with the emo-\ntion label of each utterance as input to the\nPLM RoBERTa (both RoBERTa-Base and\nRoBERTa-Large are used). Then CEE was\nmodeled as a binary classification problem for\neach utterance pair.\n• KEC (Li et al., 2022) utilized the directed\nacyclic graph networks (DAGs) incorporating\nsocial commonsense knowledge (SCK) to im-\nprove the causal reasoning ability.\n• KBCIN (Zhao et al., 2022) proposed the\nknowledge-bridged causal interaction network\n(KBCIN) with conversational graph, emo-\ntional and actional interaction module to cap-\nture context dependencies of conversations\nand make emotional cause reasoning.\nD.2 Mental Health Analysis\nWe compare the performance of ChatGPT with\nthat of the following baselines for mental health\nanalysis:\n• CNN (Kim, 2014) used three channel CNN\nwith filters of 2,3,4 to classify the post.\n• GRU (Cho et al., 2014) used a two-layer GRU\nto encode the post.\n• BiLSTM_Att (Zhou et al., 2016) utilized a\nbidirectional LSTM with attention mechanism\nas context encoding layer to capture the con-\ntextual information of posts.\n• fastText (Joulin et al., 2017) used an open-\nsource and efficient text classifier based on\nbag of n-grams features.\n6074\n• BERT/RoBERTa (Devlin et al., 2019; Liu\net al., 2019) utilized the PLMs BERT and\nRoBERTa to model the post and fine-tuned\nthem for classification.\n• MentalBERT/MentalRoBERTa (Ji et al.,\n2022b) used mental healthcare-related PLMs\nMentalBERT and MentalRobBERTa to en-\ncode the post, which are fine-tuned for classi-\nfication.\nE Human Evaluation Criteria\nAnnotators will be given generated explanations\nfrom ChatGPT and InstructGPT-3, and the original\npost as the correct reference. Annotators will need\nto score and annotate the generated explanations\nfrom the following aspects:\nFluency Fluency evaluates the coherence and\nreadability of the explanation. Annotators should\nassess if generated explanation well-structured,\neasy to read, and free of grammatical or syntax\nerrors.\n• 0: Incoherent, difficult to read, and contains\nnumerous errors\n• 1: Somewhat coherent, but with poor readabil-\nity and several errors\n• 2: Mostly fluent, easy to read, with few minor\nerrors\n• 3: Completely fluent, coherent, and error-free\nReliability Reliability measures how trustworthi-\nness of the generated explanations to support the\ndetection results. Annotators should assess whether\nthe explanation is based on facts, has misinforma-\ntion and wrong reasoning according to the given\npost. Main symptoms to check (sorted by critical-\nity):\n• Suicide ideation expressions (golden stan-\ndard).\n• Self-harm and self-guilt.\n• Long-term low passion (e.g. loss of interest\nto previous hobbies).\n• Loss of appetite and sleep disorders.\n• Accompanied by hypersexuality or frigidity.\n• Other symptoms.\nThe domain experts also consult other scales de-\nscribing depressive symptoms, such as the Patient\nHealth Questionnaire (PHQ-9)6. The annotation\nscheme is as follows:\n• 0: Unreliable information or inconsistent in-\nformation\n• 1: Somewhat reliable information with some\ninconsistencies\n• 2: Mostly reliable information with few incon-\nsistencies\n• 3: Completely reliable information\nCompleteness Completeness measures how well\nthe generated explanations cover all relevant as-\npects of the original post. Annotators should assess\nwhether the explanation provides sufficient context\nand detail, without omitting important information\nsuch as emotional cues from the original post.\n• 0: Omits significant information from the orig-\ninal post\n• 1: Partially complete with some omissions\n• 2: Mostly complete with minor omissions\n• 3: Complete coverage of the original post\nOverall Score Overall performance measures the\ngeneral effectiveness of the generated explanation,\ntaking into account the combined scores for fluency,\nfactuality, reliability, and completeness.\n• 0: Poor overall performance, fluency, reliabil-\nity, and completeness are all poor (=0)\n• 1: Fair overall performance, fluency is good\n(>= 2), one or two aspects among reliability,\nand completeness is fair (<= 1)\n• 2: Good overall performance, fluency is excel-\nlent (3), one or two aspects among reliability,\nand completeness is not excellent (<= 2)\n• 3: Excellent overall performance, fluency, reli-\nability, and completeness is all excellent (=3)\n6https://www.apa.org/depression-guideline/\npatient-health-questionnaire.pdf\n6075\nF Case studies\nF.1 Emotion-enhanced Prompts\nTo provide a more intuitive view, we present an\nexample of ChatGPTCoT _emo on the mental health\ncondition detection for a post from the Dreaddit\ndataset:\nPrompt: Post: \"I am scared as fuck off what the\ncomedown is going to be like from being jacked\nup on adrenaline for 5 days straight. I have\nnot felt tired whatsoever during this time...\"\nConsider the emotion expressed from the post,\nthen answer this question: Is the poster likely\nto suffer from very severe stress? You must\nreturn a Yes or No, then explain your reasoning\nstep by step.\nResponse: Yes, the poster is likely to suffer from\nvery severe stress. The emotional language\nused in the post indicates a high level of fear\nand anxiety, with the phrase \"scared as fuck\"\nconveying extreme distress. The fact that they\nhave been jacked up on adrenaline for five days\nstraight suggests that they have been in a highly\nstressful situation for an extended period of\ntime... Overall, the combination of emotional\nand physical symptoms suggests that the poster\nis experiencing severe stress.\nwhere the emotional clues and analysis are marked\nin red, and factual clues and analysis are marked in\nblue. As shown, ChatGPT with the prompt \"Con-\nsider the emotion expressed from the post...\" can\neffectively detect multiple emotions of fear, anxiety,\nand distress from the post and consider both clues\nfrom emotions and physical symptoms to make a\nfinal prediction. The emotion-enhanced zero-shot\nCoT prompt triggers ChatGPT to conduct the com-\nplex reasoning process with emotional cues, and\nthus improves the overall performance of mental\nhealth analysis.\nF.2 Explainability\nFirstly, we provide a case for each of the\nbinary/multi-class detection tasks. For binary detec-\ntion, an example is shown in Appendix. F.1, where\nChatGPT explains its reasoning process from the\nperspectives of emotions and physical symptoms.\nAs shown in ChatGPT’s explanation, the emotional\nlanguage with the phrase \"scared as fuck\" and the\nfact \"jacked up on adrenaline for 5 days straight\"\nin the post, indicate emotional and physical symp-\ntoms of a high level of fear, anxiety, and distress.\nChatGPT thus makes the prediction of experienc-\ning severe stress based on these symptoms.\nFor multi-class detection, we select a case from\nthe T-SID dataset:\nPrompt: Post: \"Going to end my life lol see ya\nlater haha\" Consider the emotions expressed\nfrom this post, then assign only one label to this\npost from this mental disorders list: Depression,\nSuicide, PTSD, None. You must return the as-\nsigned label, then explain your reasoning step\nby step.\nResponse: Assigned label: Suicide. Reason-\ning: The post explicitly mentions the idea of\nending one’s life, which strongly suggests sui-\ncidal ideation. The use of humor with \"lol\"\nand \"haha\" may indicate a defense mechanism,\nsuch as denial or avoidance. However, given\nthe severity of the content, it is important not\nto dismiss the potential for suicidal behavior.\nWhile depression and PTSD can be risk factors\nfor suicide, they are not necessarily present in\nthis specific post.\nIn this case, ChatGPT provides evidence for mak-\ning a suicide ideation prediction (marked red), such\nas the \"severity of the content\" and \"explicitly men-\ntions the idea of ending one’s life\". It also states rea-\nsons for excluding other mental health conditions\nsuch as depression and PTSD (marked blue), such\nas \"not necessarily present\". These two-sided rea-\nsoning steps significantly improve the transparency\nof ChatGPT’s predictions.\nWe provide a case from the CAMS dataset to\ndemonstrate how ChatGPT detects the cause/factor\nfor mental health conditions.\nPrompt: Post: \"Really have been struggling\nwith everything lately, and I come into work\ntoday and my boss tells my I am no longer\nmeeting expectations and fires me. I think the\nworst part is still being scheduled for three\nmore days...\" This post shows the depression\nof the poster. Consider the emotions expressed\nfrom this post and assign one label to this post\nonly from the following depression causes list:\nBias or abuse, Jobs and career, Medication, Re-\nlationship, Alienation, None. You must return\nthe assigned label, then explain your reasoning\nstep by step.\nResponse: Jobs and career. The post directly\nmentions the poster’s job and their recent fir-\ning, which indicates that their job and career\nare a significant source of stress in their life at\n6076\nthe moment. The poster also mentions strug-\ngling with everything lately, which could po-\ntentially be related to their job and the anxiety\nit has been causing them. Therefore, the label\nof \"Jobs and career\" is the most fitting for this\npost.\nIn this case, ChatGPT extracts the direct evidence\nof \"poster’s job and their recent firing\" from the\npost (marked blue), to support its cause detection\nof \"Jobs and career\". In addition, ChatGPT can\nmake inferences about the poster’s complaint topo-\ntentially be related to their job(marked green), and\nstates their relations to the poster’s anxiety emo-\ntion (marked red). These multi-aspect arguments\nconvincingly support the final decision.\nF.3 Inaccurate Reasoning\nHere is an example of the prediction from\nChatGPTCoT _emo for a post in the Dreaddit dataset:\nPrompt: Post: \"Like she said ‘shes too much\nattached to her boyfriend, she knows that he\nfucks other girls but she cant break with him\ncause he will block her on fb and delete her\nphone number forever, he is her the best friend,\nsoulmate. ’ Wtf? Run away and try to fall out of\nlove or what?\" Consider the emotion expressed\nfrom the post, then answer this question: Is the\nposter likely to suffer from very severe stress?\nYou must return a Yes or No, then explain your\nreasoning step by step.\nResponse: Yes. The fact that they are con-\nsidering running away and falling out of love\nsuggests that they may be experiencing a high\nlevel of emotional distress. The feeling of being\nattached to someone who is not treating them\nwell and the fear of being blocked and deleted\nby that person on social media could exacer-\nbate that distress. Therefore, it is reasonable to\nconclude that the poster is likely to suffer from\nvery severe stress.\nwhere a poster is wrongly predicted as stressed. In\nthis case, ChatGPT ignores the key fact that the\nposter was quoting the problem of another girl and\nfelt unfair for her (marked blue). ChatGPT mistak-\nenly analyzed the cause of stress for the mentioned\ngirl (marked red) and put it on the poster.\nG Automatic Evaluation on\nInstructGPT-3\nThe automatic evaluation results on InstructGPT-\n3 explanations are presented in Table 9. The\nMetric InstructGPT-3Value Fluency Reliability Completeness Overall\nBLEU-1 0.003 0.018 0.059 0.095 0.063ROUGE-1 0.084 0.401 0.397 0.490 0.433ROUGE-2 0.029 0.106 0.162 0.204 0.150ROUGE-L 0.059 0.267 0.274 0.346 0.295GPT3-Score-2.558 -0.396 -0.323 -0.254 -0.325BART-Score-4.952 0.193 0.204 0.229 0.194\nBERT Score-based MethodsBERT 0.400 0.512 0.483 0.512 0.481RoBERTa 0.814 0.313 0.288 0.359 0.315MentalBERT0.473 0.435 0.404 0.433 0.402MentalRoBERTa0.727 0.414 0.373 0.391 0.373\nTable 9: Pearson’s correlation coefficients between hu-\nman evaluation and existing automatic evaluation results\non InstructGPT-3 explanations. Best values are high-\nlighted in bold.\nresults show that BERT-Score method based on\nBERT achieves the best performance in all aspects,\noutperforming statistics-based and generative lan-\nguage model-based methods. We notice that many\nInstructGPT-3 outputs only give diagnoses with no\nexplanations at all, and BERT-based methods are\nexpected to better capture the huge gap between\nthe semantics of the original post and explanations\nthan other methods. On the other hand, Mental-\nBERT and MentalRoBERTa do not outperform\nBERT as in ChatGPT examples, because the quality\nof InstructGPT-3 explanations is very low, which\ndoes not need much domain-specific knowledge to\nscore.\n6077",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.666056752204895
    },
    {
      "name": "Ignorance",
      "score": 0.615402340888977
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6049239635467529
    },
    {
      "name": "Computer science",
      "score": 0.5551222562789917
    },
    {
      "name": "Task (project management)",
      "score": 0.5322558283805847
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4855784773826599
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44902288913726807
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.42663300037384033
    },
    {
      "name": "Cognitive psychology",
      "score": 0.41490763425827026
    },
    {
      "name": "Data science",
      "score": 0.3600292205810547
    },
    {
      "name": "Natural language processing",
      "score": 0.35810399055480957
    },
    {
      "name": "Psychology",
      "score": 0.3280499577522278
    },
    {
      "name": "Psychiatry",
      "score": 0.1307685375213623
    },
    {
      "name": "Engineering",
      "score": 0.0881560742855072
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28407311",
      "name": "University of Manchester",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I133731052",
      "name": "University of Helsinki",
      "country": "FI"
    },
    {
      "id": "https://openalex.org/I53592917",
      "name": "Jiangxi Normal University",
      "country": "CN"
    }
  ],
  "cited_by": 91
}