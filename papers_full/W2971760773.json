{
    "title": "Repurposing Decoder-Transformer Language Models for Abstractive Summarization",
    "url": "https://openalex.org/W2971760773",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4295805544",
            "name": "de Oliveira, Luke",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4295805545",
            "name": "Rodrigo, Alfredo Láinez",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2467173223",
        "https://openalex.org/W2170973209",
        "https://openalex.org/W2963385935",
        "https://openalex.org/W2567525733",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W2752395160",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W2612675303",
        "https://openalex.org/W2507756961",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2963260202",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W2963045354",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2996287690",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2741672218",
        "https://openalex.org/W2889518897"
    ],
    "abstract": "Neural network models have shown excellent fluency and performance when applied to abstractive summarization. Many approaches to neural abstractive summarization involve the introduction of significant inductive bias, exemplified through the use of components such as pointer-generator architectures, coverage, and partially extractive procedures, designed to mimic the process by which humans summarize documents. We show that it is possible to attain competitive performance by instead directly viewing summarization as a language modeling problem and effectively leveraging transfer learning. We introduce a simple procedure built upon decoder-transformers to obtain highly competitive ROUGE scores for summarization performance using a language modeling loss alone, with no beam-search or other decoding-time optimization, and instead relying on efficient nucleus sampling and greedy decoding.",
    "full_text": "Repurposing Decoder-Transformer Language Models for Abstractive\nSummarization\nLuke de Oliveira\nTwilio AI\nldeoliveira@twilio.com\nAlfredo L´ainez Rodrigo\nTwilio AI\nalainez@twilio.com\nAbstract\nNeural network models have shown excellent\nﬂuency and performance when applied to ab-\nstractive summarization. Many approaches to\nneural abstractive summarization involve the\nintroduction of signiﬁcant inductive bias, ex-\nempliﬁed through the use of components such\nas pointer-generator architectures, coverage,\nand partially extractive procedures, designed\nto mimic the process by which humans sum-\nmarize documents. We show that it is possi-\nble to attain competitive performance by in-\nstead directly viewing summarization as a lan-\nguage modeling problem and effectively lever-\naging transfer learning. We introduce a sim-\nple procedure built upon decoder-transformers\nto obtain highly competitive ROUGE scores\nfor summarization performance using a lan-\nguage modeling loss alone, with no beam-\nsearch or other decoding-time optimization,\nand instead relying on efﬁcient nucleus sam-\npling and greedy decoding.\n1 Introduction\nText summarization aims to produce short, co-\nherent natural language summaries of longer-form\ndocuments while retaining important information\nfrom the original source text. Techniques for this\ntask fall on a point along a continuum betweenex-\ntractive and abstractive summarization. The for-\nmer seeks to extract grammatically valid subsets\nof the source document such that, when combined,\nproduce a coherent, shorter text. The latter, as the\nname suggests, aims to abstract away the direct\nlexical and syntactic choices of the source docu-\nment, and generate summary text from scratch.\nNeural network approaches to abstractive sum-\nmarization generally encode the source docu-\nment into some hidden state or representation,\nthen decode this representation into a summa-\nrized, abstracted version of the source docu-\nment (Rush et al., 2015; Nallapati et al., 2016).\nThese approaches usually rely on a sequence-to-\nsequence (Sutskever et al., 2014) style architec-\nture, and tend to produce ﬂuent, well formed natu-\nral language summaries when coupled with beam\nsearch or other decoding techniques.\nA major weakness of traditional sequence-to-\nsequence learning when applied to summarization\nis the lack of a direct copy mechanism, leading to\nmissing or misrepresented details in decoded sum-\nmaries (Chopra et al., 2016; Nallapati et al., 2016;\nRush et al., 2015; Zeng et al., 2016). Though\nattention helps ameliorate this issue by directly\nlearning to focus on speciﬁc words or phrases in a\nsource document (Chopra et al., 2016), many have\nallowed for an explicit copy mechanism inspired\nby Pointer Networks (Vinyals et al., 2015), by op-\ntimizing a differentiable decision whether to gen-\nerate new text or directly copy from the source (Gu\net al., 2016; Zeng et al., 2016; See et al., 2017).\nAdditional components in many neural abstrac-\ntive summarization systems model semantic cov-\nerage (Tu et al., 2016; See et al., 2017) and provide\nguidance on where to attend (Gehrmann et al.,\n2018) in order to directly avoid repetition and an-\ncillary details, while encouraging completeness.\nRecent work has incorporated the use of rein-\nforcement learning to directly optimize objectives\nof interest that may not be differentiable, but are\nnonetheless useful for summarization, such as di-\nrectly optimizing the ROUGE score (Paulus et al.,\n2017; Li et al., 2018; Celikyilmaz et al., 2018).\nSimultaneously, Peters et al. (2018), Devlin\net al. (2018), Howard and Ruder (2018), Radford\net al., and Radford et al. (2019), among others,\nhave shown the beneﬁts of large-scale pretraining\non large, unlabeled corpora on a variety of down-\nstream tasks in transfer learning settings. In partic-\nular, it has been shown that large-scale, attention-\nonly language modeling via decoder-only trans-\nformers (Liu et al., 2018) as an unsupervised pre-\narXiv:1909.00325v1  [cs.CL]  1 Sep 2019\ntraining task admits the ability to perform zero-\nshot learning on meaningful tasks involving natu-\nral language generation (Radford et al., 2019).\nMotivated by this, we propose a simple method\nthat exhibits competitive performance on abstrac-\ntive summarization without using sequence-to-\nsequence architectures or other standard tools in\nthe neural abstractive summarization toolbox, and\ninstead using a decoder-only transformer language\nmodel with transfer learning. This further illus-\ntrates the utility of ﬁnetuning language models\ntrained on open domain text.\n2 Model\nTransformer Preliminaries Our model builds\non previous work utilizing decoder-only Trans-\nformers (Liu et al., 2018) for jointly learning\nlanguage modeling and sequence transduction in\naligned domains, which limits attention to tokens\n0,1,...,n −1 for predicting token n. Formally,\na decoder-only Transformer considers a sequence\nof one-hot token vectors T = [t0,t1,...,t n−1] ∈\n{0,1}V ×n, with each ti ∈{0,1}V where V is the\nsize of the vocabulary. Given an embedding ma-\ntrix WE ∈Rd×V and a positional encoding matrix\nWP ∈Rd×(n−1), the model computes an initial\nhidden representation H0 as\nH0 = WET + WP ∈Rd×(n−1) (1)\nand computes each subsequent hidden represen-\ntation as\nHℓ = TRF(Hℓ−1),∀ℓ∈[1,...,L ], (2)\nwhere TRF is the transformer block with self-\nattention, ﬁrst introduced in Vaswani et al. (2017).\nWe utilize the modiﬁcations provided in Radford\net al. (2019), such as moving Layer Normaliza-\ntion (Lei Ba et al., 2016) to the beginning of each\ntransformer block. The ﬁnal output is\nY = softmax(W⊤\nE HL) ∈[0,1]V ×(n−1) (3)\nwhere Yi,n−1 is the probability assigned to the\nnth token being the ith word in our vocabulary\ngiven t0,...,t n−1, and WE is shared between in-\nput and output.\nDecoder-only Sequence Transduction for Sum-\nmarization Following Liu et al. (2018), we do\nnot use a sequence-to-sequence approach to se-\nquence transduction, and instead opt to construct a\nsingle longer sequence that encodes the full map-\nping.\nFormally, consider a set of paired documents\nC = {(x,y)},|C| = N. For a source-\nsummary pair (x,y) ∈ C, the source document\nx = [x0,...,x m] and reference summary y =\n[y0,...,y k] are sequences of one-hot token vec-\ntors, where we assume m≫k.\nTo learn this mapping using a language model,\nwe combine xand y using special learnable vec-\ntors corresponding to control tokens. In addi-\ntion, we augment Eq. 1 to include a segment-\nspeciﬁc (i.e., source or summary) embedding (De-\nvlin et al., 2018). Finally, we reset the positional\nencoding for the summary. Our model is fed\nthree sequences (see Eq. 4): a concatenation of\nthe source document and the summary ( S), posi-\ntional encodings that reset for the summary com-\nponent ( P), and segment-speciﬁc encodings for\nthe source and the summary (Q). We represent the\nstart of the source document withα, the beginning\nof the summary with β, and the end of sequence\nwith δ. Additionally, we encode the source seg-\nment with σand the summary segment with τ.\nS = [α,x0,...,x m,β,y 0,...,y k,δ]\nP = [0,1,...,m,m + 1,0,1,...,k,k + 1,0]\nQ= [σ,σ,...,σ,σ,τ,...,τ,τ ]\n(4)\nThus, our model changes Eq. 1 by adding the\nposition encoding modiﬁcation from Eq. 4 and an\nadditional trainable weight WQ representing the\nsegment encoding Q, yielding Eq. 5 while leaving\nEq. 2 and 3 unchanged.\nH0 = WES+ WP P + WQQ (5)\nThe model is trained via maximum like-\nlihood, where we rewrite S in Eq. 4 as\n[t0,t1,...,t m+k+2,tm+k+3], and optimize Eq. 6\nper source-summary pair, where p(ti|t0,...,t i−1)\nis obtained from Y in Eq. 3.\np(S) =\nm+k+3∏\ni=1\np(ti|t0,...,t i−1) (6)\nInput Representation Given recent trends\nmoving away from purely word- or character-\nlevel representations, we utilize data-driven\nsubword encoding via Byte Pair Encoding\n(BPE) (Sennrich et al., 2015), following the\nprocedure outlined in Radford et al. (2019). For\nSource Text Summaryα β\n0 1 0 12 2m+1 km... ...\nσ σ τ τσ τσ τσ... ...\nS  =\nP  =\nQ  =\nSource Text Summaryβ δ\n+\n+\nMasked Self Attention\nLayer Norm\nFeed Forward\nLayer Norm\nTRF(…)Language  \nModeling \nLoss\np(S)\nFigure 1: Schematic of our Decoder-only Transformer\nrepurposed for summarization. Three input compo-\nnents, S, P, and Q, feed into the masked Transformer\n(Eq. 4 and 5). We then optimize the likelihood of the\nsequence p(S) (Eq. 6) which involves predicting the\nnext token, as illustrated by the off-by-one alignment\nof Son the top and bottom of the ﬁgure.\nexperiments in which we ﬁnetune the 117M\nparameter model from Radford et al. (2019),\nwe utilize their prebuilt vocabulary; in ablation\nstudies, we utilize SentencePiece (Kudo and\nRichardson, 2018) to learn BPE merges.\n3 Experimental Setup\nDatasets We train and evaluate our models on\nthe CNN/Daily Mail (CNN-DM) corpus (Nallap-\nati et al., 2016) of news articles and summaries,\nutilizing the non-anonymized version (See et al.,\n2017). We use the predeﬁned training, validation,\nand test splits, and limit source articles to 400 to-\nkens and summaries to 100 tokens at training time.\nAs an additional test, we train and evaluate the\nbest model conﬁguration from the ablation stud-\nies above on the Extreme Summarization (XSum)\ncorpus (Narayan et al., 2018), which contains\nsingle sentence summaries of BBC articles. As\nshown in Narayan et al. (2018), the XSum cor-\npus requires models to perform a much higher de-\ngree of semantic distillation, as indicated by low\nn-gram overlap, high n-gram novelty, and poorly\nperforming LEAD-3 baselines.\nModels & Inference In order to illustrate the\npower and simplicity of this approach, we limit\nourselves to minimal hyperparameter tuning. We\nconduct experiments in two regimes for CNN-\nDM: ﬁrst, we ﬁnetune the model outlined in Sec. 2\non top of the 117M parameter model release\nfrom Radford et al. (2019), and second, we per-\nform a full training from scratch in order to ablate\nthe effect of transfer learning. We utilize a con-\ntext size of 1024 with an embedding dimension of\n768, 12 attention heads, and a batch size of 10. We\ntrain using the Adam (Kingma and Ba, 2014) op-\ntimizer with a learning rate of 5 ×10−5 until the\nloss ceases to decrease on the validation set. For\nXSum, we use the highest-performing setup from\nCNN-DM experiments.\nIn lieu of beam search, which is commonly\nused in sequence-to-sequence and transduction\nmodels (Sutskever et al., 2014; Liu et al., 2018),\nwe compare two computationally efﬁcient ap-\nproaches: greedy decoding, and nucleus sam-\npling (Holtzman et al., 2019). In both cases, we\ndecode until we reach the stop-token δ(Eq. 4). In\nthe case of nucleus sampling, we perform 5 inde-\npendent decodings1 with p = 0.3, then pick the\ndecoding that reports the lowest negative log like-\nlihood score of the completed summary, formally\nrepresented in Eq. 7. Note that in Eq. 7 our index\nbegins at i= m+ 2to account for control tokens,\nand the fact that we do not wish to account for the\nlikelihood of the source document in our scoring.\nWe use 1/k0.6 as a normalization term to avoid\na preference for shorter summaries, borrowing di-\nrectly from Wu et al. (2016).\n− 1\nk0.6\nm+k+3∑\ni=m+2\nlog(p(ti|t0,...,t m,...,t i−1))\n(7)\nEvaluation We evaluate all models using the\nROUGE metric (Lin, 2004), in particular the F1\nvariants of ROUGE-1, ROUGE-2, and ROUGE-L\nwhich measure unigram overlap, bigram overlap,\nand longest common subsequence respectively.\n4 Results\nCNN-DM Our main results are displayed in Ta-\nble 1, where we compare our method (in the bot-\ntom section of the table) to existing methods (in\nthe upper portion) on the CNN-DM dataset, and\nshow ablations in Table 2.\nWe note that our models (for ROUGE-1 and -\n2) are competitive even when using greedy de-\ncoding, and without any sequence-to-sequence\nstyle architectures or coverage terms, illustrating\n1Nucleus sampling with p = 0.3 implies we only sam-\nple from the top 30% of of the probability distribution over\ntokens\nMethod ROUGE-1 ROUGE-2 ROUGE-L\nPointer-Generator (See et al., 2017) 36.44 15.66 33.42\nPointer-Generator + Coverage (See et al., 2017) 39.53 17.28 36.38\nML + RL (Paulus et al., 2017) 39.87 15.82 36.90\nBottom-Up (Gehrmann et al., 2018) 41.22 18.68 38.34\nDCA (best) (Celikyilmaz et al., 2018) 41.69 19.47 37.92\nGPT-2 TL;DR (Radford et al., 2019) 29.34 8.27 26.58\nD-TRF (Finetuned + greedy, ours) 39.12 17.12 27.22\nD-TRF (Finetuned + nucleus, ours) 40.70 18.03 29.62\nTable 1: Comparison of our methods (lower section) with select existing methods on the CNN-DM dataset.\nAblation R-1 R-2 R-L\nBest 40.70 18.03 29.62\n(–) Finetuning 36.10 15.06 26.92\n(–) Segment encoding (Eq. 5) 38.80 16.33 27.19\nTable 2: Ablation of model components on CNN-DM\n(Decoded via nucleus sampling procedure).\nMethod R-1 R-2 R-L\nSeq2Seq Baseline 28.42 8.77 22.48\nConv-Seq2Seq 31.27 11.07 25.23\nTopic-ConvSeq2Seq 31.89 11.54 25.75\nD-TRF (Finetuned + nucleus) 34.19 12.17 27.06\nTable 3: Comparison of our methods (lower section)\nwith select existing methods on XSum, as reported\nin Narayan et al. (2018).\nthe power of this approach for abstractive sum-\nmarization. We note that using a well trained\nlanguage model (Radford et al., 2019) and then\nﬁnetuning yields a signiﬁcant performance jump\n(as shown via ablation in Table 2), motivating\nthis method in practical contexts given the recent\ntrends toward large-scale, self-supervised learning\napproaches (Devlin et al., 2018; Radford et al.,\n2019; Peters et al., 2018; Dai and Le, 2015).\nOur model does not perform well on the\nROUGE-L metric, which measures longest-\ncommon-subsequence (LCS) between the refer-\nence summary and our decoded summary. Many\n(Schluter (2017) and Lloret et al. (2018), among\nothers) have pointed out deﬁciencies in ROUGE\nas an evaluation metric, so we attempt to under-\nstand our models deﬁciencies manually. To inves-\ntigate, we pick ﬁfty random summaries that score\nin the bottom 5% of individual ROUGE-L scores,\nand examine manually for three traits 2: ﬂuency,\nfalse inclusion (adding extraneous/wrong details),\nand exclusion (missing details from the reference).\nWe ﬁnd that 86% (43/50) of summaries are ﬂuent,\n74% (37/50) exhibited false inclusion, and 92%\n(46/50) exhibited exclusion. Of those exhibiting\n2Examples are included in the Appendix\nfalse inclusion, 67% (31/46) also were marked as\nexhibiting exclusion. Though not systematic and\ninconclusive statistically, we believe this indicates\nthat our model suffers from “distractions”, and at-\ntends to details that are not summary worthy as\njudged by reference summaries. This can system-\natically limit the highest possible ROUGE-L score\nour model can achieve due to the fact that LCS\nrequires interrupted matches, and skipping over a\nlarge subset of the source impairs a models ability\nto perform well on a metric like ROUGE. Combin-\ning our approach with explicitly learned masking\nmethods presented in Gehrmann et al. (2018) may\nameliorate these issues by better directing the self-\nattention mechanism.\nXSum As a secondary evaluation of our ap-\nproach, we train our best model on the XSum\ndataset (Narayan et al., 2018) and report ROUGE\nscores in a direct comparison to the benchmarks\nreported. Results for these experiments are shown\nin Table 3. We achieve highly competitive perfor-\nmance relative to models reported in Narayan et al.\n(2018) building on a ﬁnetuning approach without\nusing many of the inductive biases traditionally\npresent in summarization methods.\n5 Conclusion\nThis work puts forward a simple approach to\nabstractive summarization by viewing sequence\ntransduction as a language modeling problem.\nWe show the effectiveness of using decoder-only\ntransformers for this task, in particular, when\ncoupled with recent advances in large-scale lan-\nguage modeling and transfer learning. We show\nthat competitive performance on two benchmark\ndatasets is possible without many of the standard\ntools in neural abstractive summarization, such as\nsequence-to-sequence modeling, coverage mecha-\nnisms, direct ROUGE optimization via reinforce-\nment learning, or beam search, instead relying on\na purely language modeling loss and simple de-\ncoding mechanisms such as nucleus sampling and\ngreedy decoding. This approach yields highly ﬂu-\nent text, and illustrates the power of unsupervised\nrepresentation learning-based transfer learning for\ndownstream tasks.\nReferences\nAsli Celikyilmaz, Antoine Bosselut, Xiaodong He, and\nYejin Choi. 2018. Deep communicating agents\nfor abstractive summarization. arXiv preprint\narXiv:1803.10357.\nSumit Chopra, Michael Auli, Alexander M Rush, and\nSEAS Harvard. 2016. Abstractive sentence sum-\nmarization with attentive recurrent neural networks.\nProceedings of NAACL-HLT16, pages 93–98.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. CoRR, abs/1810.04805.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. arXiv preprint\narXiv:1603.06393.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The Curious Case of Neural Text De-\ngeneration. arXiv e-prints, page arXiv:1904.09751.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer Normalization. arXiv e-prints ,\npage arXiv:1607.06450.\nPiji Li, Lidong Bing, and Wai Lam. 2018. Actor-\ncritic based training framework for abstractive sum-\nmarization. arXiv preprint arXiv:1803.11070.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summa-\nrization Branches Out: Proceedings of the ACL-04\nWorkshop, pages 74–81, Barcelona, Spain. Associa-\ntion for Computational Linguistics.\nPeter J. Liu, Mohammad Ahmad Saleh, Etienne Pot,\nBen Goodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. 2018. Generating wikipedia by sum-\nmarizing long sequences.\nElena Lloret, Laura Plaza, and Ahmet Aker. 2018.\nThe challenging task of summary evaluation: an\noverview. Language Resources and Evaluation ,\n52(1):101–148.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive text summa-\nrization using sequence-to-sequence rnns and be-\nyond. arXiv preprint arXiv:1602.06023.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\nTopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv preprint arXiv:1705.04304.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. Improving language understanding\nby generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlexander M Rush, Sumit Chopra, and Jason We-\nston. 2015. A neural attention model for ab-\nstractive sentence summarization. arXiv preprint\narXiv:1509.00685.\nNatalie Schluter. 2017. The limits of automatic sum-\nmarisation according to rouge. In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n2, Short Papers, pages 41–45.\nAbigail See, Peter J Liu, and Christopher D Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1073–\n1083.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua\nLiu, and Hang Li. 2016. Modeling coverage\nfor neural machine translation. arXiv preprint\narXiv:1601.04811.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural In-\nformation Processing Systems, pages 2692–2700.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nWenyuan Zeng, Wenjie Luo, Sanja Fidler, and Raquel\nUrtasun. 2016. Efﬁcient summarization with\nread-again and copy mechanism. arXiv preprint\narXiv:1611.03382.\nA Examples from the manual analysis\nfrom Sec. 4.\nTable 4 provides examples of summaries from the\nbottom 5% of ROUGE-L scores on CNN-DM for\nthe procedure outlined in Sec. 4.\nB Example Output from CNN-DM\nTable 5 illustrates the ability of the model to both\ncopy and synthesize.\nSource Reference Ours\n[...] mike tyson lived in a southing-\nton, ohio, mansion. [...] tyson sold the\nhouse in 1999; it’s due to become, of\nall things, a church. the video can be\nseen at the top of this story [...]\nhere are six of cnn’s best videos of the\nweek. clips include a look at mike\ntyson’s abandoned mansion.\nformer boxer mike tyson sold his house\nin 1999. it’s due to be converted into a\nchurch. the video can be seen at the top\nof this story.\nmanchester city [...] tests have con-\nﬁrmed the spain international suffered\nno fractures after being caught in the\nface [...] pellegrini’s men, joint top\nof the table on new year’s day, have\nslumped to fourth place [...]\nmanchester city defeated west ham 2-\n0 in their premier league clash. david\nsilva was taken to hospital after a chal-\nlenge by chiekhou kouyate. spain in-\nternational has allayed fans’ fears with\na twitter message.\ndavid silva was injured in the second\nhalf of man city’s 2-0 win against west\nham. the spain international was car-\nried off on a stretcher after eight min-\nutes. manuel pellegrini’s side were\njoint top of the premier league table\nuntil their current slump.\nat least 15 fortune 500 companies,\nmany of them worth north of a $1 bil-\nlion, paid zero income taxes in 2014,\nsays a report out last week from the cit-\nizens for tax justice. according to the\nreport, household names like cbs, gen-\neral electric and mattel all successfully\nmanipulated the u.s. tax code to avoid\npaying taxes on their massive proﬁts.\n[...] what’s more, most of those 15\nwere actually given federal tax rebates\nin the tens or even hundreds of mil-\nlions. [...]\nmany of the companies named in a re-\nport out april 9 from the citizens for\ntax justice even received federal tax re-\nbates. the companies include house-\nhold names such as cbs, mattel, pru-\ndential and time warner.\ncbs, general electric and mattel all suc-\ncessfully avoided paying a penny in in-\ncome taxes. most of those 15 fortune\n500 companies managed to get through\n2014 without paying a penny in in-\ncome taxes.\nTable 4: Three randomly selected summaries from our model which score in the bottom 5% of ROUGE-L scores.\nSource Source Source\narsenal, newcastle united and\nsouthampton have checked on\ncaen midﬁelder n’golo kante.\nparis-born kante is a defen-\nsive minded player who has\nimpressed for caen this sea-\nson and they are willing to sell\nfor around 5million. marseille\nhave been in constant contact\nwith caen over signing the 24-\nyear-old who has similarities\nwith lassana diarra and claude\nmakelele in terms of stature and\nstyle. n’golo kante is attract-\ning interest from a host of pre-\nmier league clubs including ar-\nsenal. caen would be willing to\nsell kante for around 5million.\nwimbledon semi-ﬁnalist mi-\nlos raonic and 19-year-old\naustralian nick kyrgios will\nmake their debuts at the aegon\nchampionships at queen’s club\nthis summer. canada’s raonic,\nranked no 6 in the world, lost to\nroger federer in last year’s wim-\nbledon semi-ﬁnal while kyrgios\nburst onto the scene with a\nshock fourth-round victory\nover two-time champion rafael\nnadal. the duo will join nadal,\nandy murray, stan wawrinka,\nmarin cilic and defending\nchampion grigor dimitrov at\nqueen’s, which begins on june\n15. croatia’s milos raonic in\naction during his run to the\nwimbledon semi-ﬁnals last\nsummer. nick kyrgios was\nresponsible for the biggest\nupset at sw19 last year when he\nbeat rafael nadal.\njason dufner’s marriage has\nlanded in the rough as he agreed\na divorce settlement from wife\namanda at the end of last month.\ndufner and wife amanda mar-\nried in 2012 and were consid-\nered one of the golden couples\nof golf, but the pair separated\nin february and the divorce was\nﬁnalised on march 31. accord-\ning to the divorce settlement,\nﬁled on march 16 by amanda,\nthere had been an ’irretrievable\nbreakdown of the marriage’ and\nthere was ’a complete incom-\npatibility of temperament that\nthe parties can no longer live\ntogether.’ jason dufner looks\ndejected as he struggles on the\ngolf course following a neck\ninjury last year. dufner and\namanda during happier times\nafter he won the pga champi-\nonship in 2012. [...]\nReference Reference Reference\nn’golo kante is wanted by ar-\nsenal, newcastle and southamp-\nton. marseille are also keen on\nthe 5m rated midﬁelder. kante\nhas been compared to lassana\ndiarra and claude makelele.\nclick here for the latest premier\nleague news.\nmilos raonic, last year’s wim-\nbledon semi-ﬁnalist, will play\nat queen’s club. australian nick\nkyrgios will also make his de-\nbut in west london. kyrgios\nknocked rafael nadal out of\nwimbledon in a huge shock last\nyear.\njason dufner and amanda mar-\nried in 2012. divorce settle-\nment states there had been an\n’irretrievable breakdown of the\nmarriage’ amanda will receive\n$2.5m as part of the settle-\nment while jason will keep two\nhouses.\nOurs Ours Ours\nn’golo kante is a defensive\nminded player who has simi-\nlarities with lassana diarra and\nclaude makelelele. paris-born\nkante is attracting interest\nfrom a host of premier league\nclubs. arsenal, newcastle and\nsouthampton have checked on\nthe player.\ncanada’s raonic and 19-year-\nold australian nick kyrgios\nwill make debuts at queen’s.\ncanada’s raonic lost to roger\nfederer in last year’s wimbledon\nsemi-ﬁnal. kyrgios burst onto\nthe scene with a shock fourth-\nround victory over two-time\nchampion rafael nadal.\njason dufner and wife amanda\nmarried in 2012. they were con-\nsidered one of the golden cou-\nples of golf. but the pair sep-\narated in february and the di-\nvorce was ﬁnalised on march\n31.\nTable 5: Three example summaries from our model which illustrate the ability to both copy and synthesize."
}