{
  "title": "The need for guardrails with large language models in pharmacovigilance and other medical safety critical settings",
  "url": "https://openalex.org/W4412813658",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2614504728",
      "name": "Joe B. Hakim",
      "affiliations": [
        "Harvard–MIT Division of Health Sciences and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2764028787",
      "name": "Jeffery L. Painter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A91340412",
      "name": "Darmendra Ramcharran",
      "affiliations": [
        "Providence College"
      ]
    },
    {
      "id": "https://openalex.org/A4281834840",
      "name": "Vijay Kara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134466195",
      "name": "Greg Powell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3008662296",
      "name": "Paulina Sobczak",
      "affiliations": [
        "GlaxoSmithKline (Poland)"
      ]
    },
    {
      "id": "https://openalex.org/A2123643575",
      "name": "Chiho Sato",
      "affiliations": [
        "GlaxoSmithKline (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2122919072",
      "name": "Andrew Bate",
      "affiliations": [
        "London School of Hygiene & Tropical Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A4202677856",
      "name": "Andrew Beam",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2614504728",
      "name": "Joe B. Hakim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2764028787",
      "name": "Jeffery L. Painter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A91340412",
      "name": "Darmendra Ramcharran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281834840",
      "name": "Vijay Kara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134466195",
      "name": "Greg Powell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3008662296",
      "name": "Paulina Sobczak",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2123643575",
      "name": "Chiho Sato",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122919072",
      "name": "Andrew Bate",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202677856",
      "name": "Andrew Beam",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4386120650",
    "https://openalex.org/W6840334356",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4301377969",
    "https://openalex.org/W6600195515",
    "https://openalex.org/W4389519585",
    "https://openalex.org/W6602816255",
    "https://openalex.org/W6603861028",
    "https://openalex.org/W2114754412",
    "https://openalex.org/W2074289946",
    "https://openalex.org/W6604424379",
    "https://openalex.org/W3012248613",
    "https://openalex.org/W3017454464",
    "https://openalex.org/W2144499799",
    "https://openalex.org/W6811009250",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2080213370",
    "https://openalex.org/W4385513848",
    "https://openalex.org/W1963675264",
    "https://openalex.org/W2090342363",
    "https://openalex.org/W2030084116",
    "https://openalex.org/W2085752173",
    "https://openalex.org/W2145578524"
  ],
  "abstract": "Large language models (LLMs) are useful tools with the capacity for performing specific types of knowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical domains pose unique challenges, notably the issue of \"hallucinations\", where LLMs can generate fabricated information. This is particularly concerning in settings such as drug safety, where inaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety, with potential applicability to other medical safety-critical contexts. These guardrails include mechanisms to detect anomalous documents to prevent the ingestion of inappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty in generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, which involves converting both structured and unstructured data within adverse event reports into natural language. This method was applied to translate individual case safety reports, demonstrating effective application in a pharmacovigilance processing task. Our guardrail framework offers a set of tools with broad applicability across various domains, ensuring LLMs can be safely used in high-risk situations by eliminating the occurrence of key errors, including the generation of incorrect pharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in medical safety-critical environments.",
  "full_text": "The need for guardrails with \nlarge language models in \npharmacovigilance and other \nmedical safety critical settings\nJoe B. Hakim1, Jeffery L. Painter2, Darmendra Ramcharran3, Vijay Kara4, Greg Powell2, \nPaulina Sobczak5, Chiho Sato6, Andrew Bate4,7,9 & Andrew Beam8,9\nLarge language models (LLMs) are useful tools with the capacity for performing specific types of \nknowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical \ndomains pose unique challenges, notably the issue of “hallucinations”, where LLMs can generate \nfabricated information. This is particularly concerning in settings such as drug safety, where \ninaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated \na proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations \nand errors for drug safety, with potential applicability to other medical safety-critical contexts. \nThese guardrails include mechanisms to detect anomalous documents to prevent the ingestion of \ninappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty \nin generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, \nwhich involves converting both structured and unstructured data within adverse event reports into \nnatural language. This method was applied to translate individual case safety reports, demonstrating \neffective application in a pharmacovigilance processing task. Our guardrail framework offers a set \nof tools with broad applicability across various domains, ensuring LLMs can be safely used in high-\nrisk situations by eliminating the occurrence of key errors, including the generation of incorrect \npharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in \nmedical safety-critical environments.\nThe integration of large language models (LLMs) into the fabric of numerous applications has positioned them \nas instrumental in navigating the complex challenges in biology and medicine1. The breadth of their application, \ncombined with their rapid evolution, has created anticipation that LLMs will be near-universal solvers \nacross the biomedical landscape 1–3. Y et, alongside this growing optimism, there is an increasing cognizance \nof their limitations that may impede their applicability in specific areas of scientific inquiry. Prominently, the \nphenomenon of “hallucinations”—instances of generating baseless information—stands as a pivotal concern 4. \nThis phenomenon is a byproduct of the mechanisms underpinning LLMs, which rely implicitly on internally \nstored “memories” for response generation, without explicit grounding in verifiable facts 5. LLMs also face \nchallenges in communicating the uncertainties of their outputs to end-users effectively. Though measures of \nuncertainty can sometimes be quantified, validating the trustworthiness of LLM outputs remains a challenge6,7 \nincluding within the biomedical domain8.\nIn contexts where inaccuracies can result in severe consequences, particularly in decision-making processes \naffecting patient safety, the issue of LLM hallucinations and omission of key information 9 becomes acutely \nsignificant10. One critical domain is drug safety, also known as pharmacovigilance (PV), which involves the \nongoing surveillance for adverse events (AEs) linked to pharmaceutical medicines and vaccines 11. Given \nthe limitations of pre-market trials in fully characterizing a drug or vaccine’s safety profile, PV relies on the \ncollection and analysis of spontaneously reported AEs, vital for continued assessment of a product’s benefit-\nrisk. The reported information is transcribed into an Individual Case Safety Report (ICSR) which serves as \n1Harvard-MIT Department of Health Sciences and Technology, Cambridge, MA, USA. 2GSK, Durham, NC, USA. \n3GSK, Providence, RI, USA. 4GSK, London, UK. 5GSK, Warsaw, Poland. 6GSK, Tokyo, Japan. 7London School of \nHygiene and Tropical Medicine, London, UK. 8Department of Epidemiology, Harvard T.H. Chan School of Public \nHealth, Boston, MA, USA. 9Andrew Bate and Andrew Beam have contributed equally to this work. email:  \nandrew_beam@hms.harvard.edu\nOPEN\nScientific Reports |        (2025) 15:27886 1| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports\n\nthe standardized international framework for AE reporting, encompassing a vast array of information sourced \nglobally in varied formats and demanding timely review and processing. The non-random process by which \nICSRs are collected, coupled with the prevalence of incomplete or erroneous data, underscores the necessity for \nclinical review to unearth potential safety signals for further exploration and serve as the primary data source to \nformally evaluate a potential causal association12. Consequently, a challenge within PV lies in the efficient parsing \nof extensive, noisy, and often incomplete domain-specific textual data, some of which may be contradictory, to \nidentify safety signals meriting additional investigation.\nThe propensity of LLM to hallucinate and omit key details presents a considerable hazard if applied naively \nwithin the PV domain, which is inherently safety-critical. For instance, an LLM might erroneously suggest that \nan ICSR details a serious AE such as liver failure while this is not mentioned in the source report, potentially \nsignaling a false-positive safety concern and diverting resources from legitimate safety investigations. Moreover, \nunderstanding how LLMs are integrated with human end-users becomes essential, as human-mediated oversight \nsystems will likely remain indispensable for certain tasks within safety-critical applications for the foreseeable \nfuture.\nPreventing and mitigating hallucinations involves the implementation of “guardrails” around LLMs to shape \nand restrict their output. While the term guardrail lacks a precise definition in this context, it is here understood \nas a series of constraints applied to either an aspect of the LLM or its output to ensure adherence to predefined \ncriteria. One approach involves “structural guardrails, ” defined as mechanisms ensuring model outputs maintain \na consistent structure (e.g., CSV , XML, JSON)13, thus obviating the need for further processing of free text to \nextract pertinent information.\nThis paper focuses on “semantic guardrails, ” aimed at verifying the accuracy of LLM output by checking \nfor biased or problematic content and coding errors. These guardrails may be “hard, ” offering clear binary \noutcomes, or “soft, ” providing probabilistic assessments regarding the potential error in the output. Within \npharmacovigilance, such guardrails are pivotal in enforcing the avoidance of errors that may impact safety \ndecisions resulting in patient harm analogous to medical “never events” incidents in clinical practice contexts \nidentified by U.S. and U.K medical organizations as wholly preventable and unacceptable 14.These never \nevents, deemed intolerable and preventable, have the potential to lead to significant harm or mortality and \nusually trigger comprehensive investigations to avert recurrence. Examples include severe allergic reactions to \ncontraindicated medications or dosing errors and are “serious incidents that, due to the provision of systemic \nprotective barriers at a national level, are completely preventable and should have been preemptively addressed \nby all healthcare providers”15, analogous to guardrails. Hence, to function within safety-critical domains like PV , \nsemantic guardrails must ensure the absolute prevention of defined “never event” errors that have the potential \nto adversely impact pharmacovigilance decision-making14,16.\nIn our investigation, we introduce a comprehensive set of both hard and soft semantic guardrails designed \nto enable LLMs to function within the high-risk, safety-critical environment of PV . Focusing on the complex \nand expansive data processes integral to PV , our research specifically addresses the challenge of processing \nmultilingual ICSR intake and analogous processing within a real-world PV system. This encompasses a text-\nto-text task that involves both structured to unstructured data conversion and translation. Our guardrails were \nspecifically tested on the task of transforming Japanese language ICSRs (combined unstructured and structured, \ntabular data with numeric codes for various biomedical concepts) into English narrative text for subsequent \nanalysis by safety professionals.\nWe identified multiple potential failure modes for LLMs within this context and engineered a series of \nguardrails to mitigate these risks (Fig. 1). We implemented a hard semantic guardrail to address model outputs \nwith generated drug or vaccine names not present in the source text, utilizing existing drug safety dictionaries \nand tools to ensure consistency of key drug- and vaccine-related information between the source text and the \nLLM-generated English narrative. Additionally, we incorporated two soft semantic guardrails to communicate \nthe model’s uncertainty regarding the quality and accuracy of both the input text and its final translation, thereby \nflagging instances potentially requiring further human review. While our study concentrates on a critical, real-\nworld case in PV , we posit that the framework developed herein holds relevance across a multitude of medical \nsafety-critical domains.\nMethods\nA schematic of the workflow is presented in Fig.  1, including processing the ICSRs, the LLM tasks, creation \nof standards and the evaluation of LLM generated case reports, the sequential guardrail processing, and the \nevaluations of the guardrails.\nData acquisition\nThe dataset utilized in this study was sourced from GSK’s global safety database as part of a collaboration by \nproviding Harvard University, Cambridge, Massachusetts, USA, access on a privately maintained, secure server \nequipped with advanced graphics processing units (two 80 GB A100s). This dataset encompasses over 2 decades \nof ICSRs, with more than 4 million cases available for review. For the purposes of our assessment, the analysis \nconcentrated on the original ICSRs as submitted to GSK, prior to any form of human review. This excluded any \nsubsequent modifications or additional data reported post-initial submission, including follow-up details.\nAnalysis of individual case safety reports\nOverall, we use ICSRs combined with important data fields in PV such as the level of seriousness of the adverse \nevent to produce single chunks of text that are used as the “source” text to be fed into the LLM. Specifically, \nspontaneously reported AEs are transcribed into an ICSR which serves as the standardized international \nframework for AE reporting. A valid ICSR for entry into the GSK Global safety database is comprised of four \nScientific Reports |        (2025) 15:27886 2| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nessential elements: (1) at least one identifiable reporter; (2) an identifiable patient; (3) at least one suspect adverse \nreaction; and (4) at least one GSK suspected product17. Pharmaceutical entities often accumulate reports in large \nvolumes from various data partners. Whenever feasible, these reports are exchanged using standardized E2B \nXML documents18, which offer structured fields alongside narrative descriptions of each case. For our study, we \ntreated the entirety of information initially available as a singular data point. This approach included aggregating \nadditional structured fields such as the country of the primary source, country of occurrence, level of seriousness \n(including death, life-threatening situations, hospitalization, disability, congenital anomalies), relevant dates, \ndetails of the reporter (name, organization, country), patient demographics (age, sex), primary reaction of the \npatient, and the implicated medicinal product.\nDevelopment of a multilingual corpus for LLM pretraining\nWe constructed a multilingual corpus of ICSRs to serve as the dataset for text-to-text fine-tuning of an LLM. \nTo achieve this, we aligned the raw text from the submitted ICSRs (source text) with the human-generated \nsummaries provided by a third-party contractor (original standard target text) to create text pairs in four \nlanguages: Japanese, Spanish, French, and German. These languages were selected due to their prevalence in \nthe database and, particularly for Japanese, the complexity they present in translation tasks. While our analysis \nprimarily concentrates on Japanese due to the high number of ICSRs available in this language, the LLMs are \nFig. 1. Graphical summary of the large language model (LLM) workflow. We used extra structured fields \nand unstructured narrative texts from individual case safety reports (ICSRs), along with historical matched \nlanguage examples, to fine-tune an LLM. We added a specific task prefix, and generated an English narrative \nfrom a Japanese ICSR, and finally checked this process via several guardrails: the document-level uncertainty, \ndrug and adverse event matching, and token-level uncertainty guardrails (see Methods section).\n \nScientific Reports |        (2025) 15:27886 3| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\ndesigned for multilingual application. To integrate the additional structured fields, we prefixed each one to the \nsource text of the ICSR in the format:\nfield_name_1: field_value_1; field_name_2: field_value_2.\nTo fine-tune an LLM or employing it in text generation, we also prefixed a brief instruction indicating the \nspecific task for the model, such as “Translate the following Japanese case report into English narrative text” for \ntranslations from Japanese to English. Our pretraining corpus was enriched further with direct translation pairs \nfrom the OPUS-100 corpus 19, a comprehensive multilingual translation dataset covering 100 languages, thus \nfurnishing additional examples for model fine-tuning on translation tasks involving parallel language sets. The \nvolume of pretraining examples is detailed in Table 1.\nDevelopment of the ICSR translation LLM\nModel fine-tuning and generation\nA graphical flow chart describing this process is available as Supplemental Fig. S1.\nIn our study, we conducted an evaluation of three LLMs with parameter sizes ranging from 700 million \nto 7 billion: mt5-xl, mpt-7b-instruct, and stablelm-japanese. The criteria for selecting these models included \nthe relevance of their initial pretraining objectives, the scale of the models, and the computational resources \nrequired for their operation. These models underwent further fine-tuning for translation tasks, utilizing a corpus \ncomposed of 131,037 examples from ICSRs and texts from the OPUS-100 dataset (Table 1). The training process \nwas applied uniformly across Japanese, Spanish, French, and German, adopting a split of 70% for training, 15% \nfor validation, and 15% for testing. This distribution ensured a balanced representation of languages and sources \n(ICSR vs. OPUS-100) within each set. Fine tuning was done by iterating over the training corpuses input-output \npairs (either the structured-unstructured PV pairs or the direct translation pairs). For each, the model used \neither the joined structured-unstructured ICSR or the source language as input, and its output was compared to \nthe target language report or target language direct translation.\nGeneration was done for each of the ICSRs, including those in validation and testing. This involved running \nthe beam search algorithm for each input, and simply storing those for downstream computing of metrics or \nhuman evaluation tasks.\nFor the generation phase, we evaluated a variety of hyperparameters. Utilizing beam search20, we experimented \nwith different settings for the temperature (0.5, 0.7, 1.0, 2.0) and beam counts (3, 10, 25). Additionally, in \nour application of contrastive search 21, adjustments were made to the α values (0.2, 0.6, 0.9) and the top-k \nselections (4, 8, 16, 64). The optimal set of generation hyperparameters was determined based on the BLEU \nscore22 performance on the validation set, ultimately selecting a contrastive search configuration with α = 0.2 \nand top-k = 16.\nModel evaluation\nIn our initial assessments, we concentrated on evaluating the Japanese translation quality, a task of significant \nrelevance in PV due to the human resources required with securing proficient translators for Japanese drug \nsafety data. We conducted comparative analyses of the three models, utilizing per-token perplexity as a metric on \na validation subset comprising 7820 ICSRs, which constitute approximately 13% of the total Japanese ICSRs in \nour dataset. For the best performing model, we further explored its translation capabilities by applying standard \nmachine translation evaluation metrics, including the BLEU score 22, SACRE-BLEU score 23, and word error \nrate24.\nExpert human evaluation of the target text\nAfter finalizing our model, we performed a comprehensive evaluation aimed at assessing its efficacy in translating \ncases that were originally documented in Japanese. This analysis involved 210 cases, all sourced from Japan and \ninitially documented in Japanese. The selection of these cases was governed by a predefined set of criteria. Our \ngoal was to achieve an even distribution across various product categories, with our sample evenly divided \namong vaccines, general medicines, and specialized products, like those in oncology. Priority was given to \nserious cases that had been subjected to in-depth analysis upon their reception, thus offering a comprehensive \ninsight into potentially critical incidents. Additionally, we sought to maintain a balanced representation of \nproducts across these categories. The cases spanned the entire 20-year period for which we had data, ensuring \nNumber of ICSRs\nPretraining examples (GSK), total 131,037\n ICSRs in Japanese 58,855\n ICSRs in Spanish 13,264\n ICSRs in German 30,370\n ICSRs in French 28,548\nJapanese direct translation pairs (OPUS-100) 10,000\nSpanish direct translation pairs (OPUS-100) 10,000\nGerman direct translation pairs (OPUS-100) 10,000\nFrench direct translation pairs (OPUS-100) 10,000\nTable 1. Numbers of individual case safety reports (ICSRs) and direct translation pairs.\n \nScientific Reports |        (2025) 15:27886 4| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\ntemporal representativeness. Finally, our case selection employed random sampling within these specific strata \nto reflect the overall distribution of Clinical Utility Score for Prioritization (CUSP) scores25 found in our entire \nICSR database. This methodology was designed to secure a broad and diverse representation in the completeness \nof the cases under review.\nPhase 1: establishment of high-quality baseline translations\nThe first phase was dedicated to creating a baseline foundation of high-quality translations. Each of the 210 \nJapanese ICSRs, available in the database as previous translations into English by an external contractor, was \nsubjected to a thorough review by two independent PV experts fluent in both Japanese and English. This \ndouble-blind review not only verified the translations for accuracy in comparison to source text and fluency, but \nalso established a robust English “ground truth” for further comparative analysis. No adjudication review was \nrequired for this comparative assessment as, as the review comments of both reviewers were made available for \nphase 2 reviewers, with the ability to seek clarity where required to support phase 2 review.\nThe outcomes from this phase’s evaluation are detailed in the supplementary materials, with Tables S2, S3, \nand S4 offering a juxtaposition of the initial standard target texts against the evaluations conducted by the \nbilingual PV specialists.\nPhase 2: evaluation of LLM translations against established baseline\nIn the next phase, we assessed the LLM-generated English translations against the “ground truth” translations \nderived in Phase 1 of the experiment. This assessment was carried out by PV experts proficient in English, \nwith experience in safety evaluations. Employing a carefully designed evaluation framework, they conducted \nindependent dual reviews of each translation, incorporating both a detailed five-category assessment system \n(Table S1 for category specifics) and binary evaluation criteria (Table 4). In instances of binary evaluation, the \npresence of any noted error category, observed even once, warranted its marking, with evaluators having the \noption to detail the specific nature of the error. Moreover, the experts assessed the clinical acceptability of each \nprocessed ICSR for reporting to regulatory agencies. Any discordance among the evaluations was resolved by \nan additional independent senior expert, this need is underpinned by the low inter-rater agreement between \nclinical experts when evaluating the same drug-event ICSR cases has been well documented in the medical \nliterature26–29. For the four-category criteria, evaluations were made on a five-point Likert scale 30, with ratings \nranging from 1 (least favorable) to 5 (most favorable), as detailed in Table S1 in the supplement for the definitions \nof each rating level.\nTo streamline the evaluation, a custom web application was created, affording the reviewers the ability to \nmethodically compare translations side-by-side and to log their assessments using dropdown menus and open-\nended text fields. A screen capture of this web application’s graphical user interface is available in Supplemental \nFig. S2. Cases were randomly distributed among a team of reviewers to minimize the potential for individual \nreviewer and selection bias. This application was designed with tracking capabilities for capturing individual \nevaluator responses, and it was programmed to automatically signal for independent expert adjudication should \ndiscrepancies between reviewers emerge.\nLLM guardrails for ICSR translations\nWe developed one hard and two soft semantic guardrails for this application, as described below in order of \napplication in the ICSR processing pipeline:\nDocument-level uncertainty quantification (DL-UQ)\nThis soft guardrail identifies submitted documents that are unlikely to be ICSRs reports (based on statistical \nprobabilities as reported by a model, as opposed to using the 4 aforementioned validation criteria for ICSRs). To \nsupport potential automation of ICSR intake, this guardrail detects documents unlikely to be an AE report and \nprevents any LLM processing of these reports. The DL-UQ guardrail first creates a document level embedding \nby performing an average pooling operator to the token-level embeddings created using the source language \nencoder LLM. Next, a k-nearest neighbors’ Euclidean distance is calculated between the embedding for the \nsubmitted document and a cache of ICSR embeddings created using the same methodology from the training \ndata. This distance is a measure of uncertainty according to the LLM as it measures how anomalous a new \nsubmission is relative to the documents the model has seen before and can be used to automatically discard \na submission or flag it for review. A distance threshold can be tuned to achieve a desired trade-off between \nsensitivity and specificity.\nMISMATCH (drug and AE mismatching)\nThis hard guardrail enforces a “never” event by identifying drug names that appear in either the source text or \ntarget text but not both, indicating that a drug name has been either mistranslated or hallucinated. This kind of \nerror represents a so-called “never event” because incorrectly identifying a drug in an ICSR could have dire safety \nconsequences and should be avoidable. To implement this guardrail, we matched (with regular expressions) both \nthe source and target texts for any mentions of drugs; similarly, this was implemented for AEs. Then, we used \ntwo dictionaries (a custom in-house drug dictionary from the global safety database, and MedDRA, Medical \nDictionary for Regulatory Activities 31; 28 K preferred terms) to find the matching terms, and whether the set \ndifference had any elements corresponding to unmatched terms. The dictionary matches allowed generic-trade \nname associations for drugs. Note: this guardrail did not match terms that are slightly misspelled drug names or \nAEs, since those are not matched by the regular expression-based text matching comparison with terms in the \ndictionaries. If there was a mismatch, this hard guardrail would trip and the eventual integrated system would \nScientific Reports |        (2025) 15:27886 5| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nroute outside of the standard case processing and for further adjudication, either through post-processing or \nhuman-in-the-loop assessment and correction.\nToken-level uncertainty quantification (TL-UQ)\nThis soft guardrail identifies potential LLM errors at word and sub-word levels. Each token in the vocabulary is \nassigned a log probability by the LLM, and we take the entropy of this multinomial distribution as the token-level \nuncertainty score. Intuitively, the more entropy in the predictive distribution of the next token, the “less certain” \nthe model is in generating that specific token.\nGuardrail assessments\nWe assess each of the guardrail types in different ways. For the DL-UQ guardrail, we compare the score of \nreal inputs to fake inputs. For the MISMATCH guardrail by counting the types of instances where the model’s \noutputs different from humans’ . For the TL-UQ guardrail, we produce visualizations that flag each tokens’ span \nwith its correlated score, and also comparing the level of overall TL-UQ flagging to the human rated error rates \nin certain categories.\nWe assessed each guardrail as follows:\nFor DL-UQ, using the train validation split described above (see “Data pre-processing” and “Model \nevaluation”), we sampled 80 example texts from the training and validation sets, and produced a score for each. \nWe then injected a sample of 25 “extraneous samples” , which included 14 Japanese Wikipedia articles, 7 Japanese \nfake case reports (in a similar format as the original case reports), 2 Japanese texts that have nothing to do with \nPV , and 2 non-Japanese texts. We plotted the numeric score for each example to evaluate the separation and \nreported the area under the receiver operator curve (AUROC) for a discrimination between validation and \nextraneous samples.\nFor MISMATCH, the primary evaluation of this guardrail was whether the specific targeted “never event” \nis always flagged when the target text contains that error. To this end, we used the human evaluators’ flagged \ndrug errors as the exemplar never events on a (programmatically) randomly selected sample of 20 cases. We \ncalculated the fraction of cases caught by the MISMATCH guardrail where the human evaluators indicated \na drug name had been hallucinated spontaneously. The MISMATCH guardrail was also useful for the other \ncategories. We divided its fixes in the “generic-trade name” category by how the specific drugs mentioned were \nflagged by the MISMATCH guardrail itself (“fixed by mismatch guardrails”) or by a separate system that we \nadded to check if generic-trade names match by looking for parentheses (“direct generic-trade name checking”). \nFor this, we used the existing pairs of generic-trade names from the dictionary afterwards. We divided fixes \nin the “drug spelling issues” section by whether they were directly fixed by the guardrails (“fixed by mismatch \nguardrails”) or not fixed, which happened when the same drug had both correct and incorrect spellings in the \nLLM generated output (“multiple mentions”). In these cases, the guardrail did not find the misspelled drug by \nmatching the text in the LLM output in English, and it did not detect that the drug mention in the Japanese \nsource text is unmatched, because the drug is also spelled correctly. The frequency of the MISMATCH guardrail \nflagging individual drug and AE names is evaluated by a ”missrate” , which is a measure of the frequency of \nthese erroneous outputs that are not fixed by this guardrail. A missrate of 1.0 indicates that, for example, the \nsource text contained a number of drugs or AEs that are not matched to any translated terms in the target text. \nIn the standard translations used to train the model, we expect this to be 0.0, so any missrates > 0.0 are due to a \nlimitation of the drug or AE lists, or a misspelling of these terms.\nFor evaluation of the TL-UQ guardrail, we showed a qualitative example of a visualization flagging spans \nof uncertain text. In that example, we correlated the flagged spans with a human evaluator’s assessment of \nspecific errors in that case. Spans were flagged by differing intensities of text highlighting, from least to most, \ncorresponding to the 10th percentile, 5th percentile, and 1st percentile most entropic predicted tokens. \nQuantitative evaluations were conducted by stratifying each reviewed case by “Is the case clinically accurate” , \n“Wrong name or information” , and “Incorrect AE/Wrong outcome” and assessing the case entropy score (an \naverage of the individual token entropies) for each case in each category.\nResults\nTranslation model development and evaluation\nWe considered three LLMs, that at the time of beginning this study, were performant multilingual models that \ncould run given local resources on our internal servers: mt5-xl, MPT-7B, and stablm-japanese. We first assessed \nhow well each could translate ICSRs from Japanese to English without any task-specific fine-tuning and then \nassessed this ability when the models were fine-tuned with ICSR data (Table 2).\nThese results indicate that none of the base models are suitable for translation “off the shelf ” (Table 2). Fine-\ntuning improved all models by a significant margin and only mt5-xl reached a suitable perplexity after fine-\ntuning (Table 2). This is most likely due to this base model being pretrained explicitly on Japanese text, while \nthe others likely only encountered Japanese text during their initial pretraining in an extremely small number of \ninstances. On this basis, we decided to move forward with the mt5-xl model for further evaluation.\nTraditional metrics of machine translation quality for mt5-xl show a BLEU score of 0.39, which is considered \nto be associated with relatively high-quality translations 32, as are the Sacre-BLEU score of 0.44 and the word \nerror rate of 0.73.\nPreliminary feasibility study\nIn a pilot assessment of 20 translations, we found that 16/20 (80%) were deemed acceptable overall. Supplemental \nTable S6 shows a breakdown of the kinds of errors identified by human experts on this pilot dataset. The most \ncommon kinds of mistakes were miscellaneous errors, which includes misspellings and grammatical errors.\nScientific Reports |        (2025) 15:27886 6| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nPhase 1 evaluations: evaluation of existing standard produced target text\nUsing the rubric in Supplemental Table S1, the reviewers evaluated the quality of the original standard supplied \ntarget texts. In Table S2, we report summary statistics evaluating whether the standard supplied target text \nsufficiently captured the same meaning as the source texts. Supplemental Table S3 reports the human-assessed \nclarity of the source texts and incorporates a two-reviewer system to get an inter-rater agreement in this metric.\nBoth rater 1 and rater 2’s median scores were 4.0 (mostly clear and easy to read). Calculating the inter-rater \nagreement using Cohen’s Kappa, and quadratic weights, gave a Kappa of 0.542. The interpretation of this is \ntypically domain-specific and variable with the number of categories, but in this case represents a much better \nthan random association between the raters and shows consistency in the clarity of the source material.\nSupplemental Table S4 reports the Phase 1 reviewers’ assessment of the translation accuracy between the \nstandard provided source text and the target text. The human evaluations from the Phase 1 component, in which \nwe checked the set of original data fed into the model (the source ICSR plus the “ground truth” translation), show \nerrors and other issues with the input data. The columns represent, e.g. for “ Added information” , that there was \nadditional text in the standard provided source text relative to the target text.\nPhase 2 evaluation: expert assessment of LLM produced translations\nWe evaluate the LLM produced translations via the human reviewer’s Likert-like criteria (Table 3) and binary \ncriteria (Table 4). When compared to the existing human translation on the same source text in the database, \nslightly fewer cases had “perfect” (5) clarity scores when generated by LLM (45% vs 56% with human translation). \nFor most categories, the translations were rated as 3 or higher, indicating that they were generally considered \nacceptable. The notable exception concerned correctness of the LLM translation, which was rated 2 for 12.6%, \nindicating significant errors that would affect the interpretation (Table 3).\nFollowing the global assessment of the suitability of the translations, a fine-grained assessment was \nperformed by PV experts proficient in English to detect the presence of different error categories in the target \ntext. Adjudication was required by an independent senior PV expert if discordance amongst the evaluations \nwere identified. The most common areas of discordance were “Nonsensical phrases” (46%) and \"Wrong dates/\ntimes\" (42%), and a summary of adjudication is included in the supplemental Table S5.\nLow inter-rater agreement between clinical experts when evaluating the same drug-event ICSR is not \nuncommon in pharmacovigilance. The results (Table 4) showed the LLM translation had errors in categories \nincluding dates/times (60% error rate), drug names (59% error rate), AEs (66% error rate), and 62% had \nnonsensical phrases, including grammatical errors. In the “Other errors” category, the most frequent were typos \nin drug names, missing causality information, repeated information, incorrect specification of concomitant \nEvaluation criteria\nScore\n5 4 3 2 1\nIs the original translation provided by the human clear? 119 (56.7%) 72 (34.3%) 16 (7.6%) 3 (1.4%) 0 (0%)\nIs LLM translation clear? 32 (15.2%) 98 (46.7%) 56 (26.7%) 21 (10.0%) 3 (1.4%)\nIs the LLM translation complete? 82 (39.0%) 70 (33.3%) 37 (17.6%) 21 (10.0%) 0 (0%)\nIs the information in the LLM translation correct? 19 (9.0%) 68 (32.4%) 91 (43.3%) 32 (15.2%) 0 (0%)\nIs there unnecessary or extraneous information in the LLM translation? 97 (46.2%) 78 (37.1%) 28 (13.3%) 3 (1.4%) 4 (1.9%)\nAmount of key* (drug safety related) information in the LLM translation not present in the source text 108 (51.4%) 48 (22.9%) 36 (17.1%) 11 (5.2%) 7 (3.3%)\nTable 3. Phase 2 frequencies of each error type in large language model (LLM) generated target text, as \ndetermined by human drug safety experts. A score of 5 in each category means the target text was essentially \nwithout error, 4 indicates minor errors that do not affect interpretation, 3 indicates errors that might have \na small impact on interpretation, 2 indicates an error that would change interpretation, and 1 indicates an \nunacceptable error. See Supplementary Table S1 for a mapping of the score to the specific questions presented \nto the human reviewers.\n \nPerplexity\nBase model\n mt5-xl 2.72 × 103\n mpt-7B instruct 2.20 × 107\n Stablelm-Japanese 1.09 × 106\nFine-tuned models\n mt5-xl 1.43\n mpt-7B instruct 113\n Stablelm-Japanese 131\nTable 2. Per-token perplexity scores on held out data in the validation set, before fine-tuning (base model) and \nafter fine-tuning on a parallel language corpus (fine-tuned models).\n \nScientific Reports |        (2025) 15:27886 7| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nmedications, incorrect inferred indications, incorrect or missing batch number, and other errors that overlapped \nwith those in other categories (e.g. wrong date).\nAssessment of DL-UQ guardrail\nThe DL-UQ metric was applied to training, validation, and non-ICSR Japanese documents. Figure 2 shows that \nthe non-ICSR documents typically had higher distances to their closest training sample in embedding space \nand, with three counterexamples, can be discriminated from training and validation examples without training.\nThe distribution demonstrates separation of the assigned scores for the different categories of cases. Although \nnot completely separated, the separation of the validation and extraneous samples corresponds to an AUROC \nin the validation data of 0.80.\nAssessment of MISMATCH guardrail\nFigure  3 shows an interface that illustrates the drug and AE MISMATCH guardrail. With this interface, \nunmatched entities are quickly highlighted, allowing downstream users of this system to understand the specific \nmismatches that would lead the system to re-route the case to automatic or human-in-the-loop adjudication, \nand for qualified users to identify and resolve specific issues. For a quantitative evaluation, we report the success \nFig. 2. The distribution of document-level uncertainty scores in extraneous, validation, and training samples. \nThe vertical bar represents the minimum validation sample score that is greater than all the validation and \ntraining samples.\n \nError category Number (%)\nSource contains contradictions 30 (14%)\nLLM contains contradictions 86 (41%)\nWrong drug name or information 127 (60%)\nWrong dosage 34 (16%)\nWrong dates/times 135 (64%)\nIncorrect/missing AE/wrong outcome 149 (71%)\nRechallenge/dechallenge errors 13 (6%)\nTTO issues 48 (23%)\nNonsensical phrases 135 (64%)\nOther errors 157 (75%)\nIs the case clinically accurate? 73 (35%)\nTable 4. Phase 2 fine-grained assessment of the presence of any error in the target text for several important \nerror categories. LLM large language model, AE adverse event, TTO time to onset.\n \nScientific Reports |        (2025) 15:27886 8| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nrate of the MISMATCH guardrail in identifying one “never event” , a subset of human evaluator-identified \ndrug issues, in a randomly selected set of 20 cases from the 210 that were evaluated. As can be seen in Fig.  4, \nall instances of the never event, “spontaneously hallucinated drug names” , were correctly identified by the \nMISMATCH guardrail.\nIn addition to the error type which we termed a never event, “spontaneously hallucinated drug names” , Fig. 4 \nshows other error categories, including “dictionary incompleteness issues” and “drug spelling issues” . Since \nthese guardrails were designed based on known translated maps of generic and trade name drug pairs between \nEnglish and Japanese, limitations in these dictionaries (that PV experts can spot) lead to guardrail-unaddressed \nbut human-spotted errors, as seen in the “dictionary incompleteness issues” . For the “drug spelling issues” , the \nabove mentioned phenomena of multiple mentions makes this specific area difficult to address with this current \nversion of guardrails, and as such there were instances of misses in that category as well.\nThe missrates for the MISMATCH guardrail are summarized in Supplemental Fig. S3 (comparing the model’s \noutputted target text to the source text) and Supplemental Fig. S4 (comparing the original standard’s target \ntext to the source text). There were significant amounts of cases with a high ratio of unmatched adverse events, \ndespite using the original standard source translations. Notwithstanding the reviewers’ noted imperfection of \nthose translations (see the Phase 1 section), the difference could be explainable by the increased number of \nordinary words that are found in AEs. Additionally, comparing these figures to the Fig.  4, we note in addition \nto adverse events having higher missrates overall, drugs have significant missrates in categories beyond what we \ndescribe as “spontaneously hallucinated drug names” .\nAssessment of TL-UQ guardrail\nAn example of a visualization of TL-UQ is shown in Fig. 5 and highlights the distribution of the entropy score, \nwhich may facilitate efficient and targeted human-in-the-loop review. Figure  6 shows the distributions of TL-\nUQ uncertainty scores, stratified by clinical accuracy, wrong drug or information, and incorrect/missing AE/\nwrong outcome. Mann–Whitney U tests (using a Bonferroni correction with n = 9 trials) revealed significant \ndifferences in in the “clinical accuracy” stratification (Y es vs. No, adj. p-value of 0.0031, Y es/No vs. No, adj. \np-value of 0.043) and the “wrong drug” stratification (Y es/No vs. No, adj. p-value of 0.028). In each of these cases, \nthe trend was the more “correct” direction. More clinically accurate, less incorrect drugs/AEs trended towards \na higher uncertainty score, implying that entropy correlates negatively with the model’s human evaluated \nperformance. The observed trend of higher entropy scores correlating with more clinically accurate outputs and \nfewer incorrect drug mentions may be interpreted as counterintuitive. However, one possible explanation is that \nthe distribution of entropy scores reflects inappropriate model confidence, where the model is more confident \nin its predictions for cases it is more likely to get wrong. Further investigation is needed to fully understand this \npattern, but the results suggest that entropy scores, even at the token level, can provide a useful, if subtle, signal \nof the model’s likely correctness on a given case (see Fig. 5 for example).\nFig. 3. Illustration of guardrails filtering matched and unmatched drug terms and adverse event (AE) terms \nin the original Japanese ICSR and the LLM produced English case report. Text spans in blue indicate AEs that \nwere successfully matched between the two texts while spans in yellow indicate AEs that were unmatched. \nSpans in green represent matched drugs while spans in red represent unmatched drugs. The section “narrative” \nin the Source Japanese ICSR text (the first word) precedes the unstructured text data, and every field after the \nheading “rest_of_fields” (shown within the text) encompass the rest of the fields.\n \nScientific Reports |        (2025) 15:27886 9| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nDiscussion\nOur investigation represents a significant step in the application of LLMs within PV , a field where accuracy and \nsafety are paramount. We have explored one of the first integrations of LLMs into the PV workflow, particularly \nfocusing on translating Japanese ICSRs to English. Through the deployment and critical assessment of both \nhard and soft semantic guardrails, our work confronts the critical challenges associated with LLMs, namely the \npropensity for hallucinations and the inherent uncertainties associated with model predictions. These approaches \nare complementary and therefore should be used in conjunction with other strategies to improve the quality of \nLLM outputs (e.g., temperature adjustments and prompt engineering). Even with safety-critical applications, \nthere is variability in tolerance to inaccurate outputs: the impact of some issues could be so significant that \nFig. 5. Example flagged spans using the TL-UQ guardrail. Differing levels of red highlighting correspond to \nincreasing relative scores: least color saturation: between 10th percentile and 5th percentile scores for the whole \ntext. Medium color saturation: between 5th and 1st percentile scores. Least color saturation: 1st percentile and \nabove scores.\n \nFig. 4. Counts of reviewer-identified drug error categories and mismatch guardrail fixes thereof. For each \ncategory, counts are given indicating which of the errors had been flagged.\n \nScientific Reports |        (2025) 15:27886 10| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nsafeguards are needed. In the context of LLM usage, safeguards could be guardrails in addition to or even before \nfull human review. Our findings reveal that strategic guardrail applications effectively mitigate the risk of “never \nevent” errors, with our MISMATCH guardrail successfully identifying every instance of hallucinated drug \nnames in our translated texts from a carefully chosen case sample, although other drug error categories were not \nuniversally caught, such as wrong indications and due to dictionary limitations. We anticipate in routine usage \nas part of quality systems the ability to articulate a priori that certain errors cannot occur. We also note that some \nerroneous hallucinations could be so problematic that even if human review corrected them, the risk of wrongly \nrecalling them as true outputs could still be problematic: the ability to remove such errors prior to human review \nholds advantages.\nFurthermore, we introduced both document-level and token-level uncertainty guardrails to facilitate a \nprocess that incorporates human oversight. The document-level guardrail serves to screen out irrelevant text, \nreducing unnecessary LLM processing at the ICSR intake stage, whereas the token-level guardrail flags segments \nof the generated text that exhibit low confidence. These measures immediately make outputs look less definitive \nand enable the rigorous verification of LLM outputs by skilled human evaluators, who can further investigate \nand rectify potential inaccuracies. Specifically, the token-level guardrail is designed to highlight areas of high \nentropy—signifying considerable uncertainty—for thorough review, thereby addressing potential inaccuracies \nextending beyond specific entities such as drug names or AEs. This approach adds to the burgeoning \nmethodologies aimed at quantifying and communicating model uncertainties to users, supporting human-in-\nthe-loop review and mitigation of risks.\nTo our knowledge, this project is the first of its kind to develop and implement a range of guardrails for an \nLLM within the medical safety-sensitive environment of PV . As we look forward, we envision LLMs playing an \nincreasingly central role in this sector, with ongoing improvements enhancing their precision and reliability. \nNonetheless, the concept of never events, and its potential extrapolations into other medical safety critical areas, \nunderscores a continuous need for robust guardrails like those we have developed here. The combination of \nLLMs with these safeguards offers a foundational model for their responsible and efficacious application in PV \nand beyond.\nOn the topic of scalability to other domains: the underlying approach is a set of ontology enriched guardrails \nsupporting a text-to-text LLM transformations. Although this approach is still maturing in pharmacovigilance, \nother safety critical domains such as natural language machine learning tasks in medicine (report generation), \nand even non-safety critical domains in which strict accuracy could be a “nice to have” , for example in consumer \nfacing LLM applications. Areas such as processing electronic health records, for instance, could use very similar \nguardrails to what we propose: since some systems include structured and unstructured data, and some tasks \ninclude producing natural language reports from these sources, MISTMATCH checking of e.g. the diagnostic \nconcepts in the source and target could function as a guardrail in that domain.\nComparison to other hallucination detection systems: The field of generative AI has evolved quickly, and \nthere are now other tools that serve to limit hallucinations in generated language output. For instance, the field \nof retrieval augmented generation (RAG) has aimed to ground LLM outputs by augmenting the context and \npretraining with a knowledge base. The guardrails approach to reduce hallucination is complementary to RAG \nand other techniques for improving LLM reliability, which are particularly necessary in high-risk contexts; RAG \nand guardrail frameworks should therefore be used in tandem. Our hard guardrails are intended to completely \nprevent hallucinations and false negative outputs. In addition, soft guardrails are not designed to directly reduce \nFig. 6. TL-UQ distributions. Stratifying each reviewed case by “Is the case clinically accurate” , “Wrong name or \ninformation” , and “Incorrect AE/Wrong outcome” and reporting entropy score distributions.\n \nScientific Reports |        (2025) 15:27886 11| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nthe error rate of LLM outputs, but instead, serve to enhance the human-computer interaction by enabling more \neffective human review of uncertain segments or entire input/output instances as part of human-in-the-loop \nreview and oversight processes.\nHow this is used by non-experts:\nSince this pipeline aims to be useful to non-machine learning experts with domain expertise in \npharmacovigilance, and in general safety critical domains, there are some additional gaps that can be closed \nin terms of usability and user experience. The uncertainty scores and TL-UQ distributions, for instance, are in \ntheir current form raw numbers that should be calibrated for downstream tasks based on the deployment target. \nFor example, in the uncertainty score, a cutoff value of approximately 0.9 in this trial (Fig.  2) would catch the \nextraneous samples but only a minority of the training or validation samples, so a downstream piece of software \nthat “alerts” users at this threshold would make this more usable.\nComputational resources required to maintain:\nThe computational resources needed to maintain the hard guardrails are relatively minimal, since only \nrelatively inexpensive operations such as text matching and looking up entries in databases were needed. The \nuncertainty quantification soft guardrail did entail running computational tasks similar to model inference, so \nthis remains as limited as the end users’ ability to use the LLM’s inference in the first place.\nLimitations\nOur work also has several limitations that should be addressed prior to widescale deployment of guardrail \nframeworks. We focused initial evaluations of hard guardrails on the problem of drug name hallucinations, \nbut there are other kinds of errors that are classified as never events, like misinterpreting exposure outcomes of \ndechallenge/ rechallenge and AEs. Furthermore, although we did not solve for drug misspellings, this represents \na type of error that may be addressed on case intake prospectively, while it could also be resolved by using \nstructured data elements, retrospectively. Further work will extend the list of PV never events and their encoding \nin the system. Lastly, token-level uncertainty guardrails represent an area of evolving research and will likely \ncontinue to improve as the research field produces more solutions to quantify and informatively convey LLM \noutput uncertainty.\nIn the guardrails as presented in this work, there are two main improvement levers. Firstly, creating accurate \nunderlying ontologies (such as the drug translation pairs, etc.) themselves is a challenging research task. For \nexample, work to expand existing databases of drugs and side effects 33 is ongoing, which when incorporated \nin our guardrails will more completely catch rare or under documented drugs or side effects. The second lever \nwe believe will directly fall out of using more modern LLMs, which might (although this is subject to future \nexperimentation) be able to more accurately ascribe uncertainty when appropriate, per that guardrail.\nData availability\nThe datasets generated and/or analysed during the study are not publicly available via GSK as they include \nsensitive, proprietary post-marketing adverse event data with individually identifiable information. As a private \ncompany, GSK must comply with data privacy regulations worldwide and contractual arrangements in place for \nthe information sharing between GSK and Harvard. However whilst we cannot directly share individual case \nsafety reports (ICSRs), worldwide unique case ID numbers (WUCINs) can be provided upon reasonable request \n(www.safetyinnovation@gsk.com) to request the ICSRs from the Regulatory Authorities. Information relating to \nAI model characteristics is available at reasonable request from the primary author.\nCode availability\nWe’ve included the primary analysis code used to parse the human reviewer data and summarize the perfor-\nmance of our system in the attached URL: https://github.com/jlpainter/llm-guardrails/. This is a public GitHub \nrepository with no restrictions to access.\nReceived: 29 May 2025; Accepted: 25 June 2025\nReferences\n 1. Tang, L. et al. Evaluating large language models on medical evidence summarization. Npj Digit. Med. 6(1), 158 (2023).\n 2. Singhal, K., Azizi, S., Tu, T., Mahdavi, S.S., Wei, J., Chung, H.W ., Scales, N. et al. Large Language Models Encode Clinical Knowledge \n(2022). http://arxiv.org/abs/2212.13138.\n 3. Clusmann, J. et al. The future landscape of large language models in medicine. Commun. Med. 3(1), 141 (2023).\n 4. Zhang, Y ., Li, Y ., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., et al. Siren’s Song in the AI Ocean: A Survey on Hallucination in Large \nLanguage Models (2023). http://arxiv.org/abs/2309.01219.\n 5. McKenna, N., Li, T., Cheng, L., Hosseini, M. J., Johnson, M. & Steedman, M. Sources of Hallucination by Large Language Models \non Inference Tasks (2023). http://arxiv.org/abs/2305.14552.\n 6. Wagle, S., Munikoti, S., Acharya, A., Smith, S. & Horawalavithana, S. Empirical Evaluation of Uncertainty Quantification in \nRetrieval-Augmented Language Models for Science (2023). http://arxiv.org/abs/2311.09358.\n 7. Xiong, M., Hu, Z., Lu, X., Li, Y ., Fu, J., He, J. & Hooi, B. Can Llms Express Their Uncertainty? An Empirical Evaluation of Confidence \nElicitation in Llms (2023). http://arxiv.org/abs/2306.13063.\n 8. Bolton, W . J., Poyiadzi, R., Morrell, E. R., Bueno, G. V . B. G. & Goetz, L. RAmBLA: A Framework for Evaluating the Reliability of \nLLMs as Assistants in the Biomedical Domain. http://arxiv.org/abs/2403.14578 (2024).\n 9. European Medicines Agency. Guideline on Good Pharmacovigilance Practices (GVP): Module VI – Collection, Management and \nSubmission of Reports of Suspected Adverse Reactions to Medicinal Products (Rev 2). EMA/873138/2011 Rev 2, 28 July 2017. \nEuropean Medicines Agency.  h t t p s :  / / w w w .  e m a . e u  r o p a . e  u / e n /  d o c u m e  n t s / r e  g u l a t o  r y - p r  o c e d u r  a l - g u i  d e l i n e  / g u i d  e l i n e -  g o o d - p  h a r \nm a c  o v i g i  l a n c e -  p r a c t i  c e s - g v  p - m o d  u l e - v i  - c o l l e  c t i o n -  m a n a g  e m e n t -  a n d - s u  b m i s s i  o n - r e  p o r t s -  s u s p e c  t e d - a d  v e r s e  - r e a c t  i o n s - m  e d i c i n  \na l - p r o d u c t s - r e v - 2 _ e n . p d f\nScientific Reports |        (2025) 15:27886 12| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\n 10. Bowen, J. & Stavridou, V . Safety-critical systems, formal methods and standards. Softw. Eng. J. 8(4), 189–209 (1993).\n 11. World Health Organization. The Importance of Pharmacovigilance (2002).\n 12. Bate, A. & Evans, S. J. W . Quantitative signal detection using spontaneous ADR reporting. Pharmacoepidemiol. Drug Saf. 18(6), \n427–436 (2009).\n 13. Dong, Y ., Mu, R., Jin, G., Qi, Y ., Hu, J., Zhao, X., Meng, J., Ruan, W . & Huang, X. Building Guardrails for Large Language Models. \nhttp://arxiv.org/abs/2402.01822 (2024)\n 14. Health Service Journal. Guidance on implementing the never events framework (2009).  h t t p s :  / / w w w .  h s j . c o  . u k / h o  m e / g u  i d a n c e  - o \nn - i m  p l e m e n  t i n g -  t h e - n e  v e r - e v  e n t s - f  r a m e w o r k / 5 0 0 0 6 9 1 . a r t i c l e.\n 15. Anderson, J. E. & Watt, A. J. Using safety-II and Resilient healthcare principles to learn from never events. Int. J. Qual. Health Care \n32(3), 196–203 (2020).\n 16. National Quality Framework. List of SREs (2024).  h t t p s :  / / w w w .  q u a l i t  y f o r u m  . o r g /  T o p i c s  / S R E s /  L i s t _ o  f _ S R E s . a s p x.\n 17. European Medicines Agency. ICH E2B (R3) Electronic transmission of individual case safety reports (ICSRs): Data elements and \nmessage specification implementation guide, Scientific Guideline (2018).\n 18. Food and Drug Administration. E2B(R3) Electronic Transmission of Individual Case Safety Reports Implementation Guide: \nData Elements and Message Specification; and Appendix to the Implementation Guide—Backwards and Forwards Compatibility \n(2022).  h t t p s :  / / w w w .  f d a . g o  v / r e g u  l a t o r  y - i n f o  r m a t i o  n / s e a r  c h - f d  a - g u i d  a n c e - d  o c u m e n  t s / e 2  b r 3 - e l  e c t r o n  i c - t r a  n s m i s  s i o n - i  n d i v i d  u a \nl - c a  s e - s a  f e t y - r  e p o r t s  - i m p l e  m e n t a t i o n - g u i d e - d a t a - e l e m e n t s - a n d \n 19. Zhang, B., Williams, P ., Titov, I. & Sennrich, R. Improving Massively Multilingual Neural Machine Translation and Zero-Shot \nTranslation (2020). http://arxiv.org/abs/2004.11867\n 20. Graves, A. Sequence Transduction with Recurrent Neural Networks (2012). http://arxiv.org/abs/1211.3711.\n 21. Su, Y . et al. A contrastive framework for neural text generation. Adv. Neural. Inf. Process. Syst. 35, 21548–21561 (2022).\n 22. Papineni, K., Roukos, S., Ward, T. & Zhu, W . J. Bleu: A method for automatic evaluation of machine translation. in Proceedings of \nthe 40th Annual Meeting of the Association for Computational Linguistics 311–18 (2002).\n 23. Post, M. A Call for Clarity in Reporting BLEU Scores (2018). http://arxiv.org/abs/1804.08771.\n 24. Klakow, D. & Peters, J. Testing the correlation of word error rate and perplexity. Speech Commun. 38(1–2), 19–28 (2002).\n 25. Kara, V . et al. Finding needles in the haystack: Clinical utility score for prioritisation (CUSP), an automated approach for identifying \nspontaneous reports with the highest clinical utility. Drug Saf. 46(9), 847–855 (2023).\n 26. Koch-Weser, J., Sellers, E. M. & Zacest, R. The ambiguity of adverse drug reactions. Eur. J. Clin. Pharmacol. 11, 75–78.  h t t p s : / / d o i . \no r g / 1 0 . 1 0 0 7 / B F 0 0 5 6 2 8 9 5     (1977).\n 27. Arimone, Y . et al. Agreement of expert judgement in causality assessment of adverse drug reactions. Eur. J. Clin. Pharmacol. 61, \n169–173. https://doi.org/10.1007/s00228-004-0869-2 (2005).\n 28. Arimone, Y . et al. Inter-expert agreement of seven criteria in causality assessment of adverse drug reactions. Br. J. Clin. Pharmacol. \n64(4), 482–488. https://doi.org/10.1111/j.1365-2125.2007.02937.x (2007).\n 29. Kosov, M., Maximovich, A., Riefler, J., Dignani, M. C., Belotserkovskiy, M. & Batson E. Interexpert agreement on adverse events’ \nevaluation. Applied Clinical Trials Online (2016).\n 30. Likert, R. A technique for the measurement of attitudes. Archives of Psychology (1932).\n 31. Brown, E. G., Wood, L. & Wood, S. The medical dictionary for regulatory activities (MedDRA). Drug Saf. 20(2), 109–117 (1999).\n 32. Google. Evaluating models|AutoML Translation Documentation. (2024).  h t t p s :   /  / c l o u  d . g o o g l  e . c   o m / t r a  n s l a  t  e / a u t  o  m l / d  o  c s / e v a l u a t \ne.\n 33. Kuhn, M. et al. The SIDER database of drugs and side effects. Nucleic Acids Res. 44(D1), D1075–D1079 (2016).\nAcknowledgements\nThe authors would like to thank the GSK staff from Japan safety and Global Safety who participated in phase 1 \nand 2 of the study; Phase 1: Asako Takata, Kohei Ogawa, Toshifumi Kimura, Tomoko Matsukawa, Kaoru Fujik-\nura, Y oko Hijioka, Hiroko Toyota, Tamami Kaneko, Hiroki Nagahama, Takako Watanabe, Naoki Kaneko, Akiko \nSuhara; Phase 2: Tony Ning, Weronika Dardzinska, Marta Krzywdzinska-Kopacz, Joanna Kawałek, Chetan \nSharma, Manju Uttam, Avinash Chaturvedula, Anupama Mathew, Aparna Jayachandra, Abhinaya Surender, \nKaverappa M D, and Ewa Nowicka. In addition, students from the University of North Carolina, USA who sup-\nported Phase 2: Nathan Andert, Herbert Wan and Jackie Tan and GSK Global Safety Staff François Haguinet and \nOlivia Mahaux who supported with the multilingual assessments.\nAuthor contributions\nJBH, JLP , and A. Beam contributed to the study concept, data acquisition, data analysis, and data interpretation. \nDR, VK, and A. Bate contributed to the study concept, and data interpretation. GP contributed to data interpre-\ntation, and PS and CS contributed to the data analysis and data interpretation.\nFunding\nThis research was funded by GlaxoSmithKline Biologicals S.A.\nDeclarations\nCompeting interests\nAll GSK co-authors (Jeffery L Painter, Darmendra Ramcharran, Vijay Kara, Greg Powell, Paulina Sobczak, \nChiho Sato, Andrew Bate) receive GSK salary and some hold GSK stock and stock options. Andrew L Beam \nis a consultant for Generate Biomedicines and Flagship Pioneering, Inc and holds stock and stock options in \nGenerate Biomedicines and FL 85, Inc. The rest of the co-authors (Joe B. Hakim) have no conflict of interest to \nstate.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 9 1 3 8 - 0     .  \nCorrespondence and requests for materials should be addressed to A.B.\nReprints and permissions information is available at www.nature.com/reprints.\nScientific Reports |        (2025) 15:27886 13| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:27886 14| https://doi.org/10.1038/s41598-025-09138-0\nwww.nature.com/scientificreports/",
  "topic": "Pharmacovigilance",
  "concepts": [
    {
      "name": "Pharmacovigilance",
      "score": 0.8800098896026611
    },
    {
      "name": "Computer science",
      "score": 0.5034500956535339
    },
    {
      "name": "Data science",
      "score": 0.3643346428871155
    },
    {
      "name": "Medicine",
      "score": 0.33494794368743896
    },
    {
      "name": "Pharmacology",
      "score": 0.19785866141319275
    },
    {
      "name": "Drug",
      "score": 0.16494882106781006
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210092658",
      "name": "Harvard–MIT Division of Health Sciences and Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I196272386",
      "name": "Providence College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210119760",
      "name": "GlaxoSmithKline (Poland)",
      "country": "PL"
    },
    {
      "id": "https://openalex.org/I4210115149",
      "name": "GlaxoSmithKline (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4210089966",
      "name": "London School of Hygiene & Tropical Medicine",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ]
}