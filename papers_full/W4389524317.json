{
    "title": "LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models",
    "url": "https://openalex.org/W4389524317",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2115625708",
            "name": "Zhiqiang Hu",
            "affiliations": [
                "Singapore University of Technology and Design"
            ]
        },
        {
            "id": "https://openalex.org/A120759433",
            "name": "Lei Wang",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A3198011045",
            "name": "Yihuai Lan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2468464367",
            "name": "Wanyu Xu",
            "affiliations": [
                "Southwest Jiaotong University"
            ]
        },
        {
            "id": "https://openalex.org/A2176961937",
            "name": "Ee-Peng Lim",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A2160800796",
            "name": "Lidong Bing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111849503",
            "name": "Xing Xu",
            "affiliations": [
                "University of Electronic Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A1992239148",
            "name": "Soujanya Poria",
            "affiliations": [
                "Singapore University of Technology and Design"
            ]
        },
        {
            "id": "https://openalex.org/A2120552592",
            "name": "Roy Lee",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385571124",
        "https://openalex.org/W2276364082",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W3177323791",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W3173036834",
        "https://openalex.org/W4361020491",
        "https://openalex.org/W3205717164",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4294925020",
        "https://openalex.org/W2475046758",
        "https://openalex.org/W4281643738",
        "https://openalex.org/W3138392969",
        "https://openalex.org/W4282961290",
        "https://openalex.org/W2946659172",
        "https://openalex.org/W4389523957",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4385573520",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4312107389",
        "https://openalex.org/W2962800603",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W2251935656",
        "https://openalex.org/W2105717194",
        "https://openalex.org/W4286903242",
        "https://openalex.org/W2996908057",
        "https://openalex.org/W3205949070",
        "https://openalex.org/W3101498587",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W4375957723",
        "https://openalex.org/W4313680121",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W4287122891",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on simple math reasoning datasets.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5254–5276\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of\nLarge Language Models\nZhiqiang Hu1 Lei Wang2 ∗ Yihuai Lan Wanyu Xu 4 Ee-Peng Lim2\nLidong Bing3 Xing Xu5 Soujanya Poria1 Roy Ka-Wei Lee1\n1Singapore University of Technology and Design\n2Singapore Management University\n3DAMO Academy, Alibaba Group, Singapore\n4Southwest Jiaotong University\n5University of Electronic Science and Technology of China\nAbstract\nThe success of large language models (LLMs),\nlike GPT-4 and ChatGPT, has led to the devel-\nopment of numerous cost-effective and acces-\nsible alternatives that are created by finetun-\ning open-access LLMs with task-specific data\n(e.g., ChatDoctor) or instruction data (e.g., Al-\npaca). Among the various fine-tuning methods,\nadapter-based parameter-efficient fine-tuning\n(PEFT) is undoubtedly one of the most attrac-\ntive topics, as it only requires fine-tuning a few\nexternal parameters instead of the entire LLMs\nwhile achieving comparable or even better per-\nformance. To enable further research on PEFT\nmethods of LLMs, this paper presents LLM-\nAdapters, an easy-to-use framework that inte-\ngrates various adapters into LLMs and can ex-\necute these adapter-based PEFT methods of\nLLMs for different tasks. The framework in-\ncludes state-of-the-art open-access LLMs such\nas LLaMA, BLOOM, and GPT-J, as well as\nwidely used adapters such as Series adapters,\nParallel adapter, Prompt-based learning and\nReparametrization-based methods. Moreover,\nwe conduct extensive empirical studies on the\nimpact of adapter types, placement locations,\nand hyper-parameters to the best design for\neach adapter-based methods. We evaluate\nthe effectiveness of the adapters on fourteen\ndatasets from two different reasoning tasks,\nArithmetic Reasoning and Commonsense Rea-\nsoning. The results demonstrate that using\nadapter-based PEFT in smaller-scale LLMs\n(7B) with few extra trainable parameters yields\ncomparable, and in some cases superior, perfor-\nmance to powerful LLMs (175B) in zero-shot\ninference on both reasoning tasks. The code\nand datasets can be found in https://github.\ncom/AGI-Edgerunners/LLM-Adapters.\n1 Introduction\nLarge language models (LLMs), such as Chat-\nGPT (OpenAI, 2022) and GPT-4 (OpenAI, 2023),\n∗Corresponding author.\nhave demonstrated unprecedented performance\nacross various natural language processing (NLP)\ntasks (Qin et al., 2023) and multi-modal tasks (Shen\net al., 2023). These LLMs often possess sizes ex-\nceeding hundreds of billions of parameters and are\nclosed-source. Consequently, this has spurred the\ndevelopment of accessible and cost-effective al-\nternatives such as LLaMA (Touvron et al., 2023).\nThese alternatives involve fine-tuning open-source\nLLMs utilizing either task-specific data (e.g., Chat-\nDoctor (Yunxiang et al., 2023)) or instructional\ndata (e.g., Alpaca (Taori et al., 2023)). However,\nfull-model fine-tuning (FFT) is computationally\nand storage-intensive, thereby presenting signifi-\ncant challenges in practical implementation.\nPrior to the emergence of FFT of LLMs (e.g.,\nLLaMA), a compelling solution called parameter-\nefficient fine-tuning (PEFT) (Houlsby et al., 2019)\nhas been proposed in the NLP field, specifically\nfor pre-trained models (e.g., BERT (Devlin et al.,\n2018)), offering a promising approach for ef-\nficiently fine-tuning LLMs. The advantage of\nPEFT lies in its ability to fine-tune only a small\nset of external parameters rather than the entire\nbackbone model while still achieving compara-\nble or even superior performance (Mangrulkar\net al., 2022). Moreover, PEFT can effectively\nmitigate catastrophic forgetting in comparison to\nFFT (Wang et al., 2022). As shown in Table 1,\nthe advantage of PEFT has resulted in the devel-\noping of diverse PEFT modules, encompassing se-\nries adapters (Houlsby et al., 2019; Wang et al.,\n2022; He et al., 2022b; Fu et al., 2021), parallel\nadapters (He et al., 2022a), reparameterization-\nbased methods (Hu et al., 2021; Edalati et al.,\n2022), and prompt-based learning methods (Lester\net al., 2021; Li and Liang, 2021).\nBy incorporating these PEFT modules into back-\nbone models (i.e., LLMs), we can capitalize on the\nremarkable capabilities of backbone models with-\nout requiring extensive computational resources.\n5254\nThis opens up opportunities for a broader range of\napplications, enabling even those with limited ac-\ncess to high-performance computing to harness the\npower of LLMs in their specific tasks. Despite the\nsuccess of PEFT for pre-trained models, it remains\nunclear which PEFT module, in combination with\nwhich layer and hyperparameter configuration, is\nmost suitable for a given task or dataset when meet-\ning LLMs (e.g., LLaMA (Touvron et al., 2023)).\nTherefore, further investigation is needed to de-\ntermine the optimal PEFT setup that maximizes\nperformance across different tasks and datasets.\nMotivated by this, in this paper, we conduct\na comprehensive empirical study of PEFT of\nthree representative open-source LLMs, including\nBLOOM (Muennighoff et al., 2022), GPT-J (Wang\nand Komatsuzaki, 2021), and LLaMA (Touvron\net al., 2023). Specifically, we undertake an empir-\nical study to address the following three research\nquestions: (i) What is the optimal placement and\nconfiguration of different PEFT methods? ( ii)\nHow’s the performance of different adapters across\ndownstream tasks? And (iii) What are the differ-\nences in performance between in-distribution (ID)\nand out-of-distribution (OOD) scenarios for PEFT\nmethods? The findings of our study are as follows:\n1. The optimal placement for the series\nadapter, parallel adapter, and LoRA is af-\nter the MLP layers, parallel with the MLP\nlayers, and located after both the Attention\nlayers and MLP layers simultaneously, re-\nspectively;\n2. Smaller language models with the PEFT ap-\nproach can attain competitive or superior\nperformance on specific tasks compared\nto larger language models. For instance,\nLLaMA-13B with LoRA can outperform\nGPT-3.5 (>175B) on MultiArith, AddSub,\nand SingleEq ;\n3. The ID fine-tuned LLaMA-13B with\nadapters outperforms ChatGPT on com-\nmonsense reasoning tasks indicating that\nsmaller language models have the potential\nto outperform larger language models on\nspecific tasks with ID fine-tuning data.\nOur contributions can be summarized as follows:\n• We conduct a comprehensive empirical study\nof various PEFT methods applied in different\nopen-source LLMs.\nMethod Prompt Repara Series Parallel\nPrompt Tuning (Lester et al., 2021)√\nPrefix-Tuning (Li and Liang, 2021)√\nSpot (Vu et al., 2021) √\nIPT (Qin et al., 2021) √\nLoRA (Hu et al., 2021) √\nKronA (Edalati et al., 2022) √\nAdapters (Houlsby et al., 2019) √\nAdaMix (Wang et al., 2022) √\nSparseAdapter (He et al., 2022b) √\nLeTS (Fu et al., 2021) √\nParallel Adapter (He et al., 2022a) √\nMAM Adapter (He et al., 2021) √ √ √\nUniPELT (Mao et al., 2021) √ √ √\nCompacter (Henderson et al., 2021) √ √\nS4-model (Chen et al., 2023) √ √\nTable 1: The PEFT methods are categorized based\non the four common basic methods. \"Prompt\" repre-\nsents prompt-based learning methods, \"Repara\" denotes\nreparametrization-based methods, \"Series\" is Series\nAdapter, while \"Parallel\" represents Parallel Adapter.\n• To facilitate our empirical study, we construct\ntwo high-quality training datasets to enhance\nPEFT performance in math reasoning and\ncommonsense reasoning tasks.\n• We develop a user-friendly framework,\nLLM-Adapter, seamlessly integrates diverse\nadapters into LLMs, empowering researchers\nto implement adapter-based PEFT methods\nfor a wide range of tasks.\n• We conduct extensive experiments to answer\nthe three research questions to serve as inspi-\nration for future research.\n2 PEFT Overview\nIn this section, we provide a brief overview of four\nparameter-efficient fine-tuning (PEFT) methods:\nprompt-based learning, reparametrization-based\nmethods, series adapters, and parallel adapters. (Li\nand Liang, 2021; Hu et al., 2021; Houlsby et al.,\n2019; He et al., 2022a)\nPrompt-based learning. As shown in Fig-\nure 1(a), prompt-based learning transforms the dis-\ncrete optimization problem of finding the optimal\nhard prompt into a continuous (soft) prompt. To\nachieve this, Lester et al. (2021) proposed the con-\ncept of prompt tuning, where a trainable tensor is\nadded as a prefix to the input embeddings. An-\nother approach called Prefix-Tuning(Li and Liang,\n2021) independently explored the addition of soft\nprompts to the hidden states of all layers. Intrin-\nsic Prompt Tuning (Qin et al., 2021) employs an\nautoencoder to compress and decompress the soft\n5255\nF eed F orw ard\ndown-project\nNonlinearit y\nF eed F orw ard\nup-project\n+ \nAdapter \nLayer \nMulti-Head\nA ttention\nAdd & Norm\nF eed F orw ard\nAdd & Norm\nAdapter\nAdapter\nMulti-Head\nA ttention\nAdd & Norm\nF eed F orw ard\nAdd & Norm\nAdapter\nx \nPretr ained\nW eights\nB\nA\nh \n(c) Series Adapter (d) Parallel Adapter(b) LoRA\nPrefix T r ansformer (Froz en)\n(a) Prefix-T uning \nAdapter\nFigure 1: A detailed illustration of the model architectures of three different adapters: (a) Prefix-Tuning, (b) LoRA,\n(c) Series Adapter, and (d) Parallel Adapter.\nprompt. We take learnable vectors incorporated\ninto the attention layer as an example of prompt-\nbased learning, which can be formulated as fol-\nlows:\nHo = Attn(HiWQ, [PK; HiWK], [PV ; HiWV ]),\n(1)\nwhere Hi ∈RT×d and Ho ∈RT×d are the input\nand output of the attention layer respectively. Note\nthat T is the maximum input length and d is the\nvector dimension. PK ∈RL×d and PV ∈RL×d\nare the learnable vectors for PEFT. L is the num-\nber of learnable tokens, which is discussed in the\nexperiment section in detail. Q, K, Vdenote the\nquery, key, value vectors of th attention module,\nrespectively.\nReparametrization-based method. This type of\nmethods aim to transform network weights using\na low-rank technique. This approach effectively\nreduces the number of trainable parameters while\npreserving the ability to handle high-dimensional\nmatrices. Intrinsic SAID (Aghajanyan et al., 2020)\ninvestigates the intrinsic dimensionality of fine-\ntuning within a low-rank subspace. LoRA (Hu\net al., 2021) introduces a simple approach to up-\ndate the parameters of a weight matrix by decom-\nposing it into a product of two low-rank matrices.\nKronA (Edalati et al., 2022) improves upon the\nmatrix factorization aspect of LoRA by utilizing\nthe Kronecker product in its technique. We take\nLoRA as an example of Reparametrization-based\nlearning, which can be formulated below:\nHo = HiW0 + Hi∆W = HiW0 + HiBA, (2)\nwhere W0 ∈Rd×d can be any pre-trained weight\nmatrix, including weights in the MLP or Attention\nlayer. B ∈Rr×d and A ∈Rr×d are lower-rank\nmatrix intended for covering ∆W. r ≪d is an\nimportant hyper-parameter for LoRA.\nSeries Adapter. Series adapters involve incorpo-\nrating additional learnable modules in a sequen-\ntial manner within a specific sublayer. In their\nstudy, Houlsby et al. (2019) proposed integrating\nfully-connected networks after the attention and\nFFN layers in the Transformer model (Vaswani\net al., 2017). Another finding by Pfeiffer et al.\n(2020) revealed that achieving comparable perfor-\nmance is possible by inserting the adapter solely\nafter the self-attention layer, instead of using two\nadapters per transformer block. AdaMix (Wang\net al., 2022) introduces a method that utilizes mul-\ntiple series adapters in a mixture-of-experts (MoE)\nfashion. Compacter (Henderson et al., 2021) uti-\nlizes the Kronecker product, low-rank matrices,\nand parameter sharing across layers to generate\nadapter weights. This technique aims to reduce\nthe computational complexity associated with the\nadapters while maintaining their performance. Se-\nries Adapter can be formulated as follows:\nHo ←Ho + f(HoWdown)Wup, (3)\nwhere the output Ho of a specific layer, such as\nthe MLP layer, is first down-projected byWdown ∈\nRd×r to a lower dimensionr, and then up-projected\nback by Wup ∈Rr×d to the original dimension d.\nf is a non-linear function. We discuss the choice\nof r in the experiment Section.\n5256\nParallel Adapter. Parallel adapters (He et al.,\n2022a) aim to incorporate additional learnable mod-\nules in parallel with distinct sublayers within the\nbackbone model. The parallel adapter can be for-\nmulated below:\nHo ←Ho + f(HiWdown)Wup, (4)\nwhere Hi (Ho) is the input (output) of a specific\nlayer. Expanding on this concept, the Multi-head\nParallel Adapter takes it a step further by using\nparallel adapters to modify the outputs of head\nattention. On the other hand, the Scaled Paral-\nlel Adapter is a variant that applies the compo-\nsition and insertion format of LoRA (Hu et al.,\n2021) to adapters. Another approach, called Ladder\nSide-Tuning (Sung et al., 2022), involves training\na lightweight ladder side network. This network\naccepts intermediate activations from the backbone\nnetworks through shortcut connections (ladders).\n3 Experiment Setup\n3.1 Benchmarks\nWe conduct extensive empirical studies on four-\nteen benchmark datasets from two categories of\nreasoning problems: Arithmetic Reasoning: (1)\nthe GSM8K (Cobbe et al., 2021) dataset consists\nof high quality linguistically diverse grade school\nmath word problems created by human problem\nwriters, (2) the SV AMP (Patel et al., 2021) bench-\nmark consists of one-unknown arithmetic word\nproblems for up-to-4 grade level students by mak-\ning simple changes to a set of problems from an-\nother existing dataset, (3) the MultiArith (Roy and\nRoth, 2016) dataset of math word problems requir-\ning multiple reasoning steps and operations, (4)\nthe AddSub (Hosseini et al., 2014) dataset of ad-\ndition and subtraction arithmetic word problems,\n(5) the AQuA (Ling et al., 2017) dataset of alge-\nbraic word problems with natural language ratio-\nnales, and (6) the SingleEq (Koncel-Kedziorski\net al., 2015) dataset of grade-school algebra word\nproblems that map to single equations with vary-\ning length; Commonsense Reasoning: (1) the\nBoolQ (Clark et al., 2019) dataset is a question-\nanswering dataset for yes/no questions containing\n15942 examples. These questions are naturally\noccurring and generated in unprompted and uncon-\nstrained settings, (2) the PIQA (Bisk et al., 2020)\ndataset of questions with two solutions requiring\nphysical commonsense to answer, (3) the SIQA\nDataset Domain # train # test Answer\nMultiArith Math - 600 Number\nAddSub Math - 395 Number\nGSM8K Math 8.8K 1,319 Number\nAQuA Math 100K 254 Option\nSingleEq Math - 508 Number\nSV AMP Math - 1,000 Number\nBoolQ CS 9.4K 3,270 Yes/No\nPIQA CS 16.1K 1,830 Option\nSIQA CS 33.4K 1,954 Option\nHellaSwag CS 39.9K 10,042 Option\nWinoGrande CS 63.2K 1,267 Option\nARC-e CS 1.1K 2,376 Option\nARC-c CS 2.3K 1,172 Option\nOBQA CS 5.0K 500 Option\nTable 2: Details of datasets being evaluated. Math:\narithmetic reasoning. CS: commonsense reasoning.\n(Sap et al., 2019) focuses on reasoning about peo-\nple’s actions and their social implications, (4) the\nHellaSwag dataset of commonsense NLI questions\nincluding a context and several endings which com-\nplete the context, (5) the WinoGrande (Sakaguchi\net al., 2021) dataset is formulated as a fill-in-a-\nblank task with binary options, and the goal is to\nchoose the right option for a given sentence which\nrequires commonsense reasoning, (6) the ARC-c\nand (7) the ARC-e are the Challenge Set and Easy\nSet of ARC (Clark et al., 2018) dataset of genuine\ngrade-school level, multiple-choice science ques-\ntions, and (8) the OBQA dataset contains questions\nrequiring multi-step reasoning, use of additional\ncommon and commonsense knowledge, and rich\ntext comprehension. Table 2 shows the dataset\nstatistics.\n3.2 Fine-tuning Data Collection\nIn order to perform fine-tuning on adapters, we\nacquire two high-quality training datasets specif-\nically designed for math reasoning and com-\nmonsense reasoning. Table 2 reveals that only\nGSM8K and AQuA datasets provide training sets\nfor arithmetic reasoning. To enhance the diver-\nsity of our data, we incorporate the training sets\nfrom GSM8K, MAWPS, MAWPS-single (Koncel-\nKedziorski et al., 2016), and select 1000 examples\nfrom AQuA for the purpose of collecting the fine-\ntuning data. However, it is worth noting that the\nchosen datasets solely offer equations and corre-\nsponding answers. In order to augment the reason-\ning capabilities of our model, particularly in terms\nof providing step-by-step rationales, we leverage\nChatGPT as the teacher model. By utilizing zero-\nshot chain-of-thought prompts, ChatGPT generates\n5257\nreasoning steps. We have included the specific\nprompt templates used to collect the math reason-\ning dataset in Appendix A.1. To ensure the quality\nof the data, we eliminate samples that contain in-\ncorrect answers. As a result, we obtain a set of 10K\nmath reasoning samples, referred to as Math10K,\nwhich we consider for further analysis and fine-\ntuning.\nTo facilitate fine-tuning in the domain of com-\nmonsense reasoning, we construct fine-tuning data\nby formatting the training sets from BoolQ, PIQA,\nSIQA, HellaSwag, WinoGrande, ARC-e, ARC-c,\nand OBQA with pre-defined templates. As each\ndataset in the commonsense reasoning domain en-\ntails distinct tasks, we adopt a structured template\nby initially describing the task’s goal, followed by\nthe corresponding content and answer. The tem-\nplate utilized for creating the fine-tuning data can\nbe found in A.2. Upon completion of this pro-\ncess, we obtain a collection of 170K commonsense\nreasoning samples, which we refer to as Common-\nsense170K. These datasets will be made publicly\navailable to encourage further research and explo-\nration in this area.\n3.3 Implementations\nTo facilitate the seamless utilization of PEFT meth-\nods in both research and practical applications, we\nhave developed a user-friendly framework, LLM-\nAdapter. LLM-Adapters seamlessly integrates di-\nverse adapters into LLMs, empowering researchers\nto implement adapter-based PEFT methods for\na wide range of tasks. We utilize LLaMA (7B,\n13B) (Touvron et al., 2023), BLOOMz (7B) (Muen-\nnighoff et al., 2022), and GPT-J (6B) (Wang and\nKomatsuzaki, 2021) as the base models for our ex-\nperiments. As for the four categories of PEFT meth-\nods, we select Prefix-Tuning (Li and Liang, 2021),\nSeries Adapter (Houlsby et al., 2019), LoRA (Hu\net al., 2021), and Parallel adapter (He et al., 2022a)\nas representative candidates to examine their effi-\ncacy. For consistency across all fine-tuning experi-\nments, we maintain a batch size of 16. The learning\nrate for Prefix-Tuning is set to 3e-2, while the rest\nof the methods adopt a learning rate of 3e-4. Each\nof the PEFT methods is fine-tuned for three epochs\non the fine-tuning datasets. It is important to note\nthat we fine-tune a single model for either the math\nor commonsense reasoning task, and subsequently\nevaluate its performance across all corresponding\ndatasets.\nSeries Parallel LoRA\n50\n52\n54\n56\n58\n60\n62\n64Average Accuracy (%)\n58.3\n59.3\n58.5\n59.5\n61.7\n59.259\n59.8 60\nAverage Accuracy of Adapter Locations\nAttn\nMLP\nAttn and MLP\nFigure 2: The average accuracy of different adapter\nlocations on math reasoning datasets.\n4 Experiment Results\n4.1 Placement and Configuration\nTo address the research question, “What is the opti-\nmal placement and configuration for various types\nof adapters?”, we employ LLaMA-7B as the base\nmodel to assess different adapter settings within\nthe context of the math reasoning task. Our empiri-\ncal study begins by determining the most effective\nplacement for the Series Adapter, Parallel Adapter,\nand LoRA. Prefix-Tuning is excluded from this\nanalysis since its placement is predetermined. For\nthe Series Adapter, we explore its placement op-\ntions after the multi-head attention layers, MLP\nlayers, or both of them. As for the Parallel Adapter\nand LoRA, we integrate them into the multi-head\nattention layers, MLP layers, or both of them, in\norder to assess their respective performances. The\ndetailed results on each dataset are shown in Ap-\npendix A.3. Figure 2 shows the average accuracy\non math reasoning datasets. We can observe that\nfor the Series Adapter, the best position is to place\nit after the MLP layers, achieving an average accu-\nracy of 59.5% on the math reasoning datasets. As\nfor the Parallel Adapter, when we place it within\nthe MLP layers, it achieves the best performance\nof 61.7%. Regarding LoRA, we need to insert it\nsimultaneously into both the Multi-head Attention\nlayers and MLP layers to achieve the best perfor-\nmance of 60%.\nIn order to determine the optimal configuration\nof various adapters, we conduct an analysis of the\nmost crucial variable for each type of the PEFT\nmethods. We compare the average accuracy on\nmath reasoning datasets. The placement of adapters\nfollows the optimal settings derived from the place-\nment analysis. Regarding Prefix-tuning, we assess\nthe performance with different numbers of virtual\ntokens (vt) set at [10, 20, 30, 40]. For Series and\n5258\nLLM Method MultiArith GSM8K AddSub AQuA SingleEq SV AMP Avg\nGPT-3.5175B - 83.8 56.4 85.3 38.9 88.1 69.9 70.4\nBLOOMz7B\nPrefix 68.8 13.8 47.1 12.5 49.4 24.1 36.0\nSeries 80.7 14.3 72.6 20.5 69.3 38.1 49.3\nParallel 85.8 18.5 77.7 18.9 74.8 36.4 52.0\nLoRA 82.8 17.4 72.4 21.3 69.9 41.0 50.8\nGPT-J6B\nPrefix 74.5 16.0 65.6 14.7 61.4 31.0 43.9\nSeries 91.7 19.5 85.8 15.0 81.7 43.6 56.2\nParallel 92.2 18.9 83.8 17.9 80.7 41.1 55.8\nLoRA 90.7 23.0 84.1 16.1 84.1 46.0 57.3\nLLaMA7B\nPrefix 63.2 24.4 57.0 14.2 55.3 38.1 42.0\nSeries 92.8 33.3 80.0 15.0 83.5 52.3 59.5\nParallel 94.5 35.3 86.6 18.1 86.0 49.6 61.7\nLoRA 95.0 37.5 83.3 18.9 84.4 52.1 61.9\nLLaMA13B\nPrefix 72.2 31.1 56.0 15.7 62.8 41.4 46.5\nSeries 93.0 44.0 80.5 22.0 87.6 50.8 63.0\nParallel 94.3 43.3 83.0 20.5 89.6 55.7 64.4\nLoRA 94.8 47.5 87.3 18.5 89.8 54.6 65.4\nTable 3: Accuracy comparison of LLMs with different adapters on six math reasoning datasets. We use GPT-3.5\ntext-Davinci-003 for Zero-shot CoT (Kojima et al., 2022) as the baseline.\nvt=10 vt=20 vt=30 vt=40\n# Virtual Tokens\n(a)\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0Average Accuracy (%)\n42\n38\n33.1\n38.1\nPrefix-Tuning\nbn=64 bn=128 bn=256 bn=512\nBottleneck Size\n(b)\n50\n52\n54\n56\n58\n60\n62\n64Average Accuracy (%)\n57.3\n59.2 59.5\n56.6\nSeries Adapter\nbn=64 bn=128 bn=256 bn=512\nBottleneck Size\n(c)\n50\n52\n54\n56\n58\n60\n62\n64Average Accuracy (%)\n59.1\n60.8 61.7\n58\nParallel Adapter\nr=4 r=8 r=16 r=32\nLoRA Rank\n(d)\n50\n52\n54\n56\n58\n60\n62\n64Average Accuracy (%)\n57.9\n60 60.6\n61.9\nLoRA\nFigure 3: The average accuracy of different variable\nsettings on math reasoning datasets. Where \"vt\" refers\nto the number of virtual tokens, \"bn\" denotes the bottle-\nneck size, while \"r\" is the LoRA rank.\nParallel Adapters, we evaluate the impact of the bot-\ntleneck size (bn) with values of [64, 128, 256, 512].\nFor LoRA, we examine the influence of different\nrank values (r) at [4, 8, 16, 32]. The detailed results\nfor each dataset can be found in Appendix A.4.\nFigure 3 presents the average accuracy of differ-\nent variables on math reasoning datasets. It can\nbe noted that when the number of virtual tokens\nin Prefix-Tuning is set to 10, Prefix-Tuning attains\nan average accuracy of 42.0% on math reasoning\ndatasets. By configuring the bottleneck dimension\nto 256, Series and Parallel Adapter demonstrate the\nhighest level of performance. However, when the\nbottleneck size is increased to 512, the accuracy of\nboth Series and Parallel Adapter decreases. The\ntypical setting for LoRA rank is set to 8, but we\nhave discovered that a larger rank can enhance the\nperformance of LoRA. When the rank is increased\nfrom 8 to 32, the average accuracy of LoRA in-\ncreases from 60.0% 61.9%.\nIn order to enhance the breadth of our research\nfindings, we conducted additional experiments in-\nvolving the placement of adapters on various LLMs\nsuch as GPT-J and BLOOMz. These experiments\nwere conducted across different model sizes, specif-\nically 7B and 13B parameters. Furthermore, we ex-\ntended our investigation to encompass diverse tasks,\nincluding Commonsense tasks. This approach en-\nabled us to generalize our observations across a\nwider spectrum of LLMs, sizes, and tasks, thus\nproviding a more comprehensive understanding of\nthe adapter placement strategies. The detailed ex-\nperiment results can be found in Appendix A.3\nBased on our comprehensive placement and con-\nfiguration analysis, we have determined the optimal\nsettings for each adapter, which will be consistently\nemployed throughout the subsequent experiments.\n• For Prefix-Tuning, we establish the number\nof virtual tokens at 10.\n• For Series and Parallel Adapter, we seam-\nlessly incorporate them into the MLP lay-\ners, configuring the bottleneck size to 256.\n• Regarding LoRA, we seamlessly integrate\nit into both the Multi-head Attention layers\nand the MLP layers with rank 32.\n4.2 Arithmetic Reasoning\nIn order to evaluate the effectiveness of adapters\non the Arithmetic Reasoning task, we conducted\n5259\nLLM Method BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg\nGPT-3175B - 60.5 81.0 - 78.9 70.2 68.8 51.4 57.6 -\nPaLM540B - 88.0 82.3 - 83.4 81.1 76.6 53.0 53.4 -\nChatGPT - 73.1 85.4 68.5 78.5 66.1 89.8 79.9 74.8 77.0\nBLOOMz7B\nPrefix 45.6 53.7 46.3 26.7 49.5 52.1 39.7 44.3 44.7\nSeries 65.4 70.4 73.6 53.4 69.3 72.3 55.9 68.0 66.0\nParallel 64.1 71.5 72.1 52.9 67.0 70.5 54.7 69.6 65.3\nLoRA 65.9 75.3 74.5 57.3 72.5 74.6 57.8 73.4 68.9\nGPT-J6B\nPrefix 63.1 66.9 68.7 34.4 64.5 64.4 46.8 59.0 58.5\nSeries 62.1 63.5 72.3 30.6 68.0 63.9 48.1 63.8 59.0\nParallel 62.2 69.7 70.0 41.7 65.0 60.2 44.6 58.2 59.0\nLoRA 62.4 68.6 49.5 43.1 57.3 43.4 31.0 46.6 50.2\nLLaMA7B\nPrefix 64.3 76.8 73.9 42.1 72.1 72.9 54.0 60.6 64.6\nSeries 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8\nParallel 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3\nLoRA 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7\nLLaMA13B\nPrefix 65.3 75.4 72.1 55.2 68.6 79.5 62.9 68.0 68.4\nSeries 71.8 83.0 79.2 88.1 82.4 82.5 67.3 81.8 79.5\nParallel 72.5 84.8 79.8 92.1 84.7 84.2 71.2 82.4 81.5\nLoRA 72.1 83.5 80.5 90.5 83.7 82.8 68.3 82.4 80.5\nTable 4: Accuracy comparison of LLMs with different adapters on eight commonsense reasoning datasets. The\nChatGPT results are obtained by Zero-shot CoT with gpt-3.5-turbo API.\na study where adapters are fine-tuned on the\nMath10K dataset and subsequently evaluated on\nsix different math reasoning datasets. As our base-\nline, we utilize the GPT-3.5 model, specifically the\ntext-Davinci-003 variant, for Zero-shot CoT ac-\ncording to Kojima et al. (2022). The results of the\nGPT-3.5 model can be found in Wang et al. (2023).\nTable 3 reports the performance of different PEFT\nmethods and the baseline. On average, the GPT-3.5\nmodel (175B) outperforms adapter-based PEFT\nLLMs in terms of accuracy. However, for sim-\npler math reasoning datasets such as MultiArith,\nAddSub, and SingleEq, adapter-based methods like\nLLaMA-13B with LoRA outperform GPT-3.5. No-\ntably, LLaMA-13B with LoRA achieves an aver-\nage accuracy of 65.4%, which is approximately\n92.8% of the performance exhibited by GPT-3.5.\nThis suggests that with sufficient task-specific train-\ning data, adapter-based PEFT of smaller LLMs\nhas the potential to achieve performance compa-\nrable to that of extremely large language models.\nThe utilization of adapter-based PEFT yields su-\nperior performance by smaller language models\ncompared to GPT-3.5 specifically in simpler tasks\nsuch as MultiArith, AddSub, and SingleEq. How-\never, challenges persist in more complex tasks\nlike GSM8K and SV AMP, which require a higher\nlevel of language comprehension and proficiency\nfrom the underlying base model, thereby resulting\nin a discernible performance gap. Regarding the\ndifferent adapters employed, LoRA achieves re-\nmarkable performance while utilizing significantly\nfewer trainable parameters. This implies that exces-\nsive learnable parameters may not be necessary for\ntask-specific fine-tuning. Overall, these findings\ndemonstrate the potential for adapter-based PEFT\nof smaller LLMs to achieve high performance on\nspecific tasks with few trainable parameters.\n4.3 Commonsense Reasoning\nAdditionally, we assess the efficacy of various\nPEFT methods for commonsense reasoning tasks.\nThe adapters undergo fine-tuning using the Com-\nmonsense170K dataset. Our baseline models for\ncommonsense reasoning include GPT-3 (175B),\nPaLM (540B), and ChatGPT. The results for GPT-\n3 and PaLM can be found in the study by Tou-\nvron et al. (2023). To evaluate ChatGPT’s per-\nformance in commonsense reasoning, we employ\nthe gpt-3.5-turbo API with a zero-shot CoT. The\nzero-shot CoT prompts align with the template used\nfor collecting our commonsense fine-tuning dataset,\nas outlined in Appendix A.2. Table 4 presents the\nperformance of the PEFT methods utilizing differ-\nent LLMs alongside the baselines. Remarkably,\nLLaMA-13B with Series Adapter, Parallel Adapter,\nand LoRA outperform all the baselines, including\nChatGPT, which has been hailed as the most im-\npressive LLM to date. LLaMA-13B with Parallel\nAdapter achieves an average accuracy of 81.5%,\nrepresenting a 4.5% improvement over ChatGPT.\nIt is worth noting that all the training sets from the\ncommonsense reasoning datasets are included in\nthe fine-tuning data Commonsense170K. Further-\nmore, we observe that the performance of the PEFT\n5260\nQuestion Mr Boarden is remodeling his bathroom. For every square foot, he needs 24 mosaic tiles. How many\nmosaic tiles would Mr Boarden need to cover two thirds of his 36 sq ft bathroom?\nChatGPT 1. Find two-thirds of 36: Multiply 36 by 2/3. (36 * 2/3) = 24 square feet\n2.Determine the number of mosaic tiles needed per square foot: Given that Mr. Boarden needs 24 mosaic\ntiles per square foot, multiply the result from step 1 by 24. (24 * 24) = 576 mosaic tiles\nTherefore, Mr. Boarden would need 576 mosaic tiles to cover two-thirds of his 36 square foot bathroom.\nLLaMA13B-Prefix 1 square foot = 4 square inches, 1 square inch = 16 square centimeters. So, 1 sq ft = 16 sq cm. Therefore,\n36 sq ft = 36 x 16 sq cm = 576 sq cm. To cover two-thirds of the bathroom, Mr Boarden needs to\ncover 576 sq cm x 2/3 = 384 sq cm. Since 1 sq cm = 1 mosaic tile, Mr Boarden needs 384 mosaic tiles.\nTherefore, Mr Boarden needs 384 mosaic tiles to cover two-thirds of his 36 sq ft bathroom.\nLLaMA13B-Series 1. Find the area of the bathroom: 36 sq ft\n2. Find two-thirds of the area: 2/3 x 36 sq ft = 24 sq ft\n3. Multiply the area by the number of tiles needed per square foot: 24 sq ft x 24 tiles/sq ft = 576 tiles\n4. Therefore, Mr Boarden would need 576 mosaic tiles to cover two-thirds of his 36 sq ft bathroom.\nLLaMA13B-Parallel 1. Calculate the total area of the bathroom: 36 sq ft\n2. Determine how much area Mr Boarden wants to cover with mosaic tiles: 2/3 x 36 sq ft = 24 sq ft\n3. Calculate the number of mosaic tiles needed: 24 sq ft x 24 mosaic tiles per sq ft = 600 mosaic tiles\nTherefore, Mr Boarden would need 600 mosaic tiles to cover two thirds of his 36 sq ft bathroom.\nLLaMA13B-LoRA 1. Calculate the total area of the bathroom: 36 sq ft\n2. Determine how much area needs to be covered: 2/3 x 36 sq ft = 24 sq ft\n3. Calculate the number of mosaic tiles needed: 24 sq ft x 24 mosaic tiles per sq ft = 576 mosaic tiles\nTherefore, Mr Boarden would need 576 mosaic tiles to cover two thirds of his 36 sq ft bathroom.\nTable 5: An example randomly sampled from GSM8K. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\nmethods is influenced by the underlying capabili-\nties of the base models. LLaMA-7B and LLaMA-\n13B demonstrate superior commonsense reason-\ning abilities compared to the BLOOMz and GPT-J\nmodels.\n4.4 ID and OOD Analysis\nWhen comparing the performance of PEFT meth-\nods on math reasoning and commonsense reason-\ning tasks, we can observe that PEFT methods\nexhibit more remarkable results in the realm of\ncommonsense reasoning. Moving forward, we\nwill analyze the factors contributing to this phe-\nnomenon from both the in-distribution (ID) and\nout-of-distribution (OOD) perspectives. In the con-\ntext of commonsense reasoning, the fine-tuning\ndata set, Commonsense170K, encompasses all the\ntraining sets from the commonsense reasoning\ndatasets. Notably, PEFT methods have demon-\nstrated the ability to outperform ChatGPT. This ob-\nservation implies that, by utilizing ID fine-tuning\ndata, smaller language models like LLaMA-13B\ncould surpass larger language models such as Chat-\nGPT and PaLM in specific downstream tasks. How-\never, when considering math reasoning tasks, the\nfine-tuning data set, Math10K, only includes the\ntraining sets of GSM8K and AQuA. In this regard,\nit has been observed that PEFT methods, partic-\nularly LLaMA-13B with LoRA, exhibit superior\nperformance compared to GPT-3.5 on MultiArith,\nAddSub, and SingleEq. These findings suggest that\nPEFT methods can enhance the math reasoning\nabilities of LLMs and can be successfully applied\nto OOD datasets. Nonetheless, when evaluating the\nperformance of PEFT methods on the ID datasets\nGSM8K and AQuA, a performance gap is still ev-\nident compared to GPT-3.5. This discrepancy is\nlikely due to the higher complexity of GSM8K and\nAQuA datasets in terms of math reasoning, while\nthe reasoning capabilities of smaller LLMs remain\nlimited. Consequently, identifying strategies to im-\nprove the performance of PEFT methods on com-\nplex math reasoning tasks represents a potential\navenue for future research.\n5 Qualitative Study\nThe previous sections have presented the quanti-\ntative analysis. In this section, we will provide\nqualitative examples to demonstrate the quality of\noutputs from different models. Table 5 displays a\nrandomly selected question from GSM8K along\nwith the outputs of ChatGPT and LLaMA-13B\nmodels using various PEFT methods. More de-\ntailed examples can be found in Appendix A.5.\nChatGPT demonstrates a comprehensive under-\nstanding of the question and generates two steps,\n\"(36 * 2/3) = 24 square feet\" and \"(24 * 24) =\n576 mosaic tiles,\" effectively solving the prob-\nlem. However, the language understanding ability\nof LLaMA-13B-Prefix models is limited, leading\nLLaMA-13B-Prefix to take the wrong direction\nin the first step. On the other hand, LLaMA-13B\n5261\nwith Series Adapter produces a high-quality an-\nswer by providing the crucial two steps and per-\nforming the correct calculations to obtain the accu-\nrate result. Interestingly, LLaMA-13B-Parallel and\nLLaMA-13B-LoRA generate almost identical ra-\ntionales. However, LLaMA-13B-Parallel produces\nan incorrect answer due to a calculation error, stat-\ning \"24 sq ft x 24 mosaic tiles per sq ft = 600\nmosaic tiles\". In general, when equipped with task-\nspecific fine-tuning data, smaller language models\nlike LLaMA-13B can generate impressive, high-\nquality answers that are comparable to those pro-\nduced by ChatGPT.\n6 Conclusion\nIn this paper, we develop a user-friendly frame-\nwork, LLM-Adapter, seamlessly integrates diverse\nadapters into LLMs, empowering researchers to im-\nplement adapter-based PEFT methods for a wide\nrange of tasks. To evaluate different PEFT meth-\nods on downstream tasks, we construct two high-\nquality fine-tuning datasets to enhance PEFT per-\nformance on math reasoning and commonsense rea-\nsoning tasks. By utilizing the LLM-Adapter toolkit\nand the constructed fine-tuning datasets, we con-\nduct a comprehensive empirical study and find the\nanswer of research questions on the optimal place-\nment and configuration of different PEFT methods,\nthe impact of adapter architectures, and the influ-\nence of ID and OOD scenarios. We hope this work\nwill encourage further research on PEFT methods\nfor LLMs.\n7 Limitations\nThere are two limitations to this work. Firstly, due\nto constrained computing resources, we were un-\nable to evaluate the performance of larger language\nmodels such as LLaMA-33B and LLaMA-65B. It\nis anticipated that these larger models, possess-\ning enhanced language understanding capabilities,\nwould yield superior performance. Secondly, this\npaper does not delve into the exploration of combin-\ning different adapters. Given the extensive search\nspace associated with the combination of various\nPEFT methods, we intend to explore this direction\nin future research endeavors.\nReferences\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal\nGupta. 2020. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. In An-\nnual Meeting of the Association for Computational\nLinguistics.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nJiaao Chen, Aston Zhang, Xingjian Shi, Mu Li,\nAlex Smola, and Diyi Yang. 2023. Parameter-\nefficient fine-tuning design spaces. arXiv preprint\narXiv:2301.01821.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924–2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question\nanswering? try arc, the ai2 reasoning challenge.\narXiv:1803.05457v1.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. arXiv preprint\narXiv:2110.14168.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAli Edalati, Marzieh S. Tahaei, Ivan Kobyzev, V . Nia,\nJames J. Clark, and Mehdi Rezagholizadeh. 2022.\nKrona: Parameter efficient tuning with kronecker\nadapter. ArXiv, abs/2212.10650.\nCheng Fu, Hanxian Huang, Xinyun Chen, Yuandong\nTian, and Jishen Zhao. 2021. Learn-to-share: A\nhardware-friendly transfer learning framework ex-\nploiting computation and parameter sharing. In Pro-\nceedings of the 38th International Conference on Ma-\nchine Learning, volume 139 of Proceedings of Ma-\nchine Learning Research, pages 3469–3479. PMLR.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\narXiv preprint arXiv:2110.04366.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022a. Towards a\nunified view of parameter-efficient transfer learning.\nIn International Conference on Learning Representa-\ntions.\n5262\nShwai He, Liang Ding, Daize Dong, Jeremy Zhang,\nand Dacheng Tao. 2022b. SparseAdapter: An easy\napproach for improving the parameter-efficiency of\nadapters. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 2184–2190,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJames Henderson, Sebastian Ruder, et al. 2021. Com-\npacter: Efficient low-rank hypercomplex adapter lay-\ners. In Advances in Neural Information Processing\nSystems.\nMohammad Javad Hosseini, Hannaneh Hajishirzi, Oren\nEtzioni, and Nate Kushman. 2014. Learning to solve\narithmetic word problems with verb categorization.\nIn EMNLP, pages 523–533.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In Inter-\nnational Conference on Machine Learning.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. ArXiv, abs/2106.09685.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish\nSabharwal, Oren Etzioni, and Siena Dumas Ang.\n2015. Parsing algebraic word problems into equa-\ntions. Transactions of the Association for Computa-\ntional Linguistics, 3:585–597.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. MAWPS:\nA math word problem repository. In Proceedings of\nNAACL, pages 1152–1157.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. ArXiv, abs/2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158–167.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut,\nYounes Belkada, and Sayak Paul. 2022. Peft: State-\nof-the-art parameter-efficient fine-tuning methods.\nhttps://github.com/huggingface/peft.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Wen tau Yih, and Madian\nKhabsa. 2021. Unipelt: A unified framework for\nparameter-efficient language model tuning. ArXiv,\nabs/2110.07577.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nOpenAI. 2022. Introducing chatgpt. https://openai.\ncom/blog/chatgpt.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of NAACL,\npages 2080–2094.\nJonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. Mad-x: An adapter-based frame-\nwork for multi-task cross-lingual transfer. In Con-\nference on Empirical Methods in Natural Language\nProcessing.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao\nChen, Michihiro Yasunaga, and Diyi Yang. 2023. Is\nchatgpt a general-purpose natural language process-\ning task solver? arXiv preprint arXiv:2302.06476.\nYujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin,\nNing Ding, Jing Yi, Weize Chen, Zhiyuan Liu, Juanzi\nLi, Lei Hou, et al. 2021. Exploring universal intrin-\nsic task subspace via prompt tuning. arXiv e-prints,\npages arXiv–2110.\nSubhro Roy and Dan Roth. 2016. Solving gen-\neral arithmetic word problems. arXiv preprint\narXiv:1608.01413.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99–106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving AI tasks with chatgpt and its friends in\nhuggingface. CoRR, abs/2303.17580.\n5263\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nLst: Ladder side-tuning for parameter and memory\nefficient transfer learning. ArXiv, abs/2206.06522.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and\nDaniel Cer. 2021. Spot: Better frozen model adap-\ntation through soft prompt transfer. arXiv preprint\narXiv:2110.07904.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models. arXiv\npreprint arXiv:2305.04091.\nYaqing Wang, Subhabrata Mukherjee, Xiaodong Liu,\nJing Gao, Ahmed Hassan Awadallah, and Jian-\nfeng Gao. 2022. Adamix: Mixture-of-adapter for\nparameter-efficient tuning of large language models.\nArXiv, abs/2205.12410.\nLi Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, and\nZhang You. 2023. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain\nknowledge. arXiv preprint arXiv:2303.14070.\n5264\nA Appendix\nA.1 Math Reasoning Prompt Templates\nWe utilize ChatGPT to collect the math reason-\ning data for fine-tuning. Table 6 show the prompt\ntemplate used to query ChatGPT. The expression\n\"Please give the steps\" is employed to guide Chat-\nGPT to generate reasoning steps, thus, we can\nuse the rationale information to fine-tune adapters.\n\"Give the arabic numerals as the answer.\" is uti-\nlized to guide ChatGPT to generate arabic numbers\nas the final answer making it easier to extract the\nanswer from the outputs.\nA.2 Commonsense Data Templates\nAs each dataset in the commonsense reasoning do-\nmain entails distinct tasks, we adopt a structured\ntemplate by initially describing the task’s goal, fol-\nlowed by the corresponding content and answer.\nTable 7 shows the templates used to collect com-\nmonsense reasoning data for fine-tuning.\nA.3 Placement Analysis\nTable 8 shows the performance regarding the place-\nment of adapters in various locations on math rea-\nsoning datasets. The fine-tuning dataset utilized\nfor this study is Math10K. Meanwhile, the base\nmodels employed is LLaMA-7B. We can observe\nthat for the Series Adapter, the best position is to\nplace it after the MLP layers, achieving an average\naccuracy of 59.5% on the math reasoning datasets.\nAs for the Parallel Adapter, when we place it within\nthe MLP layers, it achieves the best performance\nof 61.7%. Regarding LoRA, we need to insert it\nsimultaneously into both the Multi-head Attention\nlayers and MLP layers to achieve the best perfor-\nmance of 60%.\nIn order to enhance the breadth of our research\nfindings, we conducted additional experiments in-\nvolving the placement of adapters on various LLMs\nsuch as GPT-J and BLOOMz. These experiments\nwere conducted across different model sizes, specif-\nically 7B and 13B parameters. Furthermore, we ex-\ntended our investigation to encompass diverse tasks,\nincluding Commonsense tasks. This approach en-\nabled us to generalize our observations across a\nwider spectrum of LLMs, sizes, and tasks, thus\nproviding a more comprehensive understanding of\nthe adapter placement strategies.\nA.3.1 Various LLMs\nInitially, our evaluation focused on comparing the\nplacement of adapters in the context of GPT-J-6B\nand BLOOMz-7B models, specifically concerning\nArithmetic Reasoning tasks. The subsequent Table\n9 and Table 10 displays the accuracy attained on\nArithmetic Reasoning datasets.\nThe bold figures represent the optimal adapter\nplacements for GPT-J-6B and BLOOMz-7B mod-\nels. Specifically, for both GPT-J-6B and BLOOMz-\n7B, the series adapter demonstrated optimal perfor-\nmance when placed after the MLP layers. Similarly,\nthe parallel adapter exhibited its best results when\npositioned in parallel with the MLP layers. Lastly,\nthe LoRA adapter showed superior performance\nwhen located after both the Attention layers and the\nMLP layers concurrently. This observation aligns\nseamlessly with the assertions made in this paper.\nA.3.2 Various Size\nThis evaluation focuses on assessing various\nadapter placements with LLaMA-13B in the\ncontext of Arithmetic Reasoning tasks, aiming\nto compare the outcomes with those obtained\nfrom LLaMA-7B. Table 11 presents the accuracy\nachieved on Arithmetic Reasoning datasets, high-\nlighting the comparative analysis between the two\nmodel configurations.\nThe optimal placement of series adapter, parallel\nadapter, and LoRA is also consistent with LLaMA-\n7B reported in the paper.\nA.3.3 Various Tasks\nIn order to ascertain the generalizability of the\nplacement strategies identified in the context of\nArithmetic Reasoning tasks to other domains such\nas Commonsense reasoning, we conducted a com-\nprehensive evaluation of various adapter place-\nments with LLaMA-7B. Table 12 presents the accu-\nracy scores achieved on Commonsense Reasoning\ndatasets, providing valuable insights into the effec-\ntiveness of the adapter placements in this specific\ntask.\nOur observations reveal that the optimal place-\nment of series adapters, parallel adapters, and\nLoRA with LLaMA-7B for Commonsense reason-\ning aligns with their placement in Arithmetic rea-\nsoning tasks. These findings demonstrate a con-\nsistent pattern in adapter placement across various\nmodels, sizes, and task types, emphasizing the sta-\nbility and reliability of our results.\n5265\nTemplate [QUESTION] Please give the steps and the arabic numerals as the answer.\nPrompt Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A\nlarge pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that\nday?\nPlease give the steps and the arabic numerals as the answer.\nChatGPT Output To solve the problem, we need to first find out how many slices are in 2 large pizzas and 2 small pizzas:\n2 large pizzas x 16 slices per pizza = 32 slices\n2 small pizzas x 8 slices per pizza = 16 slices.\nNext, we add the number of slices in each size pizza to find the total number of slices: 32 slices + 16\nslices = 48 slices.\nTherefore, if Albert eats all 2 large pizzas and 2 small pizzas, he will eat 48 slices of pizza that day.\nAnswer: 48 slices.\nTable 6: The prompt template used to collect math reasoning dataset for fine-tuning. An example from GSM8K is\nalso included.\nA.4 Configuration Analysis\nTable 13 shows the accuracy comparison regarding\ndifferent settings of variable for PEFT methods on\nmath reasoning datasets. The fine-tuning dataset\nused for this study is Math10K. It can be noted\nthat when the number of virtual tokens in Prefix-\nTuning is set to 10, Prefix-Tuning attains an average\naccuracy of 42.0% on math reasoning datasets. By\nconfiguring the bottleneck dimension to 256, Series\nand Parallel Adapter demonstrate the highest level\nof performance. However, when the bottleneck\nsize is increased to 512, the accuracy of both Series\nand Parallel Adapter decreases. The typical setting\nfor LoRA rank is set to 8, but we have discovered\nthat a larger rank can enhance the performance of\nLoRA. Remarkably, when the rank is increased to\n32, LoRA achieves an accuracy of 61.9%.\nA.5 Qualitative Examples\nWe will show examples randomly sampled from\nmath reasoning and commonsense reasoning\ndatasets in this section.\n5266\nDataset Fine-tuning Data Template\nBoolQ Please answer the following question with true or false, question: [QUESTION]\nAnswer format: true/false\nthe correct answer is [ANSWER]\nPIQA Please choose the correct solution to the question: [QUESTION]\nSolution1: [SOLUTION_1]\nSolution2: [SOLUTION_2]\nAnswer format: solution1/solution2\nthe correct answer is [ANSWER]\nSIQA Please choose the correct answer to the question: [QUESTION]\nAnswer1: [ANSWER_1]\nAnswer2: [ANSWER_2]\nAnswer3: [ANSWER_3]\nAnswer format: answer1/answer2/answer3\nthe correct answer is [ANSWER]\nHellaSwag Please choose the correct ending to complete the given sentence: [ACTIVITY_lABEL]: [CONTEXT]\nEnding1: [ENDING_1]\nEnding2: [ENDING_2]\nEnding3: [ENDING_3]\nEnding4: [ENDING_4]\nAnswer format: ending1/ending2/ending3/ending4\nthe correct answer is [ANSWER]\nWinoGrande Please choose the correct answer to fill in the blank to complete the given sentence: [SENTENCE]\nOption1: [OPTION_1]\nOption2: [OPTION_2]\nthe correct answer is [ANSWER]\nARC-e&ARC-c Please choose the correct answer to the question: [QUESTION]\nAnswer1: [ANSWER_1]\nAnswer2: [ANSWER_2]\nAnswer3: [ANSWER_3]\nAnswer4: [ANSWER_4]\nAnswer format: answer1/answer2/answer3/answer4\nthe correct answer is [ANSWER]\nOBQA Please choose the correct answer to the question: [QUESTION]\nAnswer1: [ANSWER_1]\nAnswer2: [ANSWER_2]\nAnswer3: [ANSWER_3]\nAnswer4: [ANSWER_4]\nAnswer format: answer1/answer2/answer3/answer4\nthe correct answer is [ANSWER]\nTable 7: The data template of each dataset used to create commonsense reasoning data for fine-tuning.\nModel Loc MultiArith GSM8K AddSub AQuA SingleEq SV AMP Avg\nSeries\nAttn 92.3 32.0 80.0 16.9 80.5 47.9 58.3\nMLP 92.8 33.3 80.0 15.0 83.5 52.3 59.5\nBoth 94 29.8 84.1 17.3 83.5 45.1 59.0\nParallel\nAttn 94.5 33.5 83.0 17.3 80.5 46.9 59.3\nMLP 94.5 35.3 86.6 18.1 86.0 49.6 61.7\nBoth 94.3 30.2 84.8 17.7 84.3 47.2 59.8\nLoRA\nAttn 94.2 35.3 79.7 16.9 78.7 45.9 58.5\nMLP 95.8 35.0 80.0 15.7 81.7 47.0 59.2\nBoth 96.2 35.6 80.5 15.7 82.3 49.6 60.0\nTable 8: An evaluation of the accuracy regarding the placement of adapters in various locations is conducted on\nmath reasoning datasets. The fine-tuning dataset used for this analysis is Math10K. In this context, \"Attn\" refers to\nthe multi-head attention layer, while \"MLP\" denotes the MLP layer. The base model employed for this study is\nLLaMA-7B.\n5267\nModel Loc MultiArith GSM8K AddSub AQuA SingleEq SV AMP Avg\nGPT-J6B-Series\nAttn 90.3 16.8 85.0 16.9 78.0 43.3 55.1\nMLP 91.7 19.5 85.8 15.0 81.7 43.6 56.2\nBoth 90.7 13.2 72.7 16.0 72.4 32.8 49.6\nGPT-J6B-Parallel\nAttn 83.3 15.5 84.3 15.3 80.3 45.4 54.0\nMLP 92.2 18.9 83.8 17.9 80.7 41.1 55.8\nBoth 93.2 17.2 86.1 13.0 80.1 40.7 55.1\nGPT-J6B-LoRA\nAttn 87.2 17.1 79.4 13.0 74.4 42.8 52.3\nMLP 91.7 22.8 81.5 15.7 80.7 47.1 56.6\nBoth 90.7 23.0 84.1 16.1 84.1 46.0 57.3\nTable 9: An evaluation of the accuracy regarding the placement of adapters in various locations is conducted on\nmath reasoning datasets. The fine-tuning dataset used for this analysis is Math10K. In this context, \"Attn\" refers to\nthe multi-head attention layer, while \"MLP\" denotes the MLP layer. The base model employed for this study is\nGPT-J-6B.\nModel Loc MultiArith GSM8K AddSub AQuA SingleEq SV AMP Avg\nBLOOMz7B-Series\nAttn 84.3 14.3 66.1 20.1 63.0 32.7 46.8\nMLP 80.7 14.3 72.6 20.5 69.3 38.1 49.3\nBoth 77.8 14.8 76.2 14.2 67.5 36.1 47.8\nBLOOMz7B-Parallel\nAttn 83.7 16.5 68.1 15.0 64.0 36.7 47.3\nMLP 85.8 18.5 77.7 18.9 74.8 36.4 52.0\nBoth 88.5 15.2 75.7 16.1 70.1 34.0 49.9\nBLOOMz7B-LoRA\nAttn 80.7 15.8 59.7 15.7 55.1 29.7 42.8\nMLP 86.0 16.4 69.6 17.7 66.1 40.3 49.4\nBoth 82.8 17.4 72.4 21.3 69.9 41.0 50.8\nTable 10: An evaluation of the accuracy regarding the placement of adapters in various locations is conducted on\nmath reasoning datasets. The fine-tuning dataset used for this analysis is Math10K. In this context, \"Attn\" refers to\nthe multi-head attention layer, while \"MLP\" denotes the MLP layer. The base model employed for this study is\nBLOOMz-7B.\nModel Loc MultiArith GSM8K AddSub AQuA SingleEq SV AMP Avg\nLLaMA13B-Series\nAttn 97.7 33.3 81.8 15.7 86.0 50.7 60.9\nMLP 93.0 44.0 80.5 22.0 87.6 50.8 63.0\nBoth 93.8 29.7 81.5 18.6 84.8 48.2 59.4\nLLaMA13B-Parallel\nAttn 96.8 34.0 85.3 17.0 88.0 52.4 62.3\nMLP 94.3 43.3 83.0 20.5 89.6 55.7 64.4\nBoth 95.2 31.8 84.1 15.7 89.2 52.8 61.5\nLLaMA13B-LoRA\nAttn 94.0 37.0 86.0 16.5 87.4 53.5 62.4\nMLP 96.3 42.1 84.1 18.1 87.8 55.5 64.0\nBoth 94.8 47.5 87.3 18.5 89.8 54.6 65.4\nTable 11: An evaluation of the accuracy regarding the placement of adapters in various locations is conducted on\nmath reasoning datasets. The fine-tuning dataset used for this analysis is Math10K. In this context, \"Attn\" refers to\nthe multi-head attention layer, while \"MLP\" denotes the MLP layer. The base model employed for this study is\nLLaMA-13B.\n5268\nModel Loc BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg\nLLaMA7B-Series\nAttn 63.7 76.3 75.8 51.7 73.6 69.4 52.9 69.8 66.7\nMLP 63.0 79.2 76.3 67.9 75.7 74.5 57.1 72.4 70.8\nBoth 62.4 72.4 72.4 43.7 70.6 62.6 48.6 63.6 62.0\nLLaMA7B-Parallel\nAttn 64.4 75.8 76.5 71.1 79.2 72.7 56.3 71.0 70.9\nMLP 67.9 76.4 78.8 69.8 78.9 73.7 57.3 75.2 72.3\nBoth 64.9 75.6 75.2 66.9 74.6 67.8 53.7 70.2 68.6\nLLaMA7B-LoRA\nAttn 68.7 78.3 74.9 68.1 76.2 78.1 63.2 73.6 72.6\nMLP 66.5 81.5 78.5 73.3 72.4 80.0 64.8 72.2 73.7\nBoth 68.9 80.7 77.4 78.1 78.8 77.8 61.3 74.8 74.7\nTable 12: An evaluation of the accuracy regarding the placement of adapters in various locations is conducted on\nCommonsense reasoning datasets. The fine-tuning dataset used for this analysis is Commonsense170K. In this\ncontext, \"Attn\" refers to the multi-head attention layer, while \"MLP\" denotes the MLP layer. The base model\nemployed for this study is LLaMA-7B.\nModel Variable MultiArith GSM8K AddSub AQuA SingleEq SV AMP Average\nPrefix\nvt=10 63.2 24.4 57.0 14.2 55.3 38.1 42.0\nvt=20 60.3 22.9 46.1 16.1 51.8 30.6 38.0\nvt=30 51.2 16.9 42.3 15.4 41.9 31.1 33.1\nvt=40 54.2 17.8 49.6 21.7 52.0 33.2 38.1\nSeries\nbn=64 93.0 31.6 80.5 14.6 79.9 44.1 57.3\nbn=128 93.8 33.4 81.5 18.5 79.9 48.1 59.2\nbn=256 92.8 33.3 80.0 15.0 83.5 52.3 59.5\nbn=512 95.3 33.7 77.2 14.9 76.6 42.0 56.6\nParallel\nbn=64 92.7 36.8 77.5 16.9 81.9 48.5 59.1\nbn=128 95.8 36.9 83.0 17.7 83.3 47.8 60.8\nbn=256 94.5 35.3 86.6 18.1 86.0 49.6 61.7\nbn=512 95.0 31.5 81.8 14.6 81.5 43.4 58.0\nLoRA\nr=4 95.2 34.7 78.5 16.1 77.8 45.4 57.9\nr=8 96.2 35.6 80.5 15.7 82.3 49.6 60.0\nr=16 95.5 36.2 82.8 13.8 84.4 50.9 60.6\nr=32 95.0 37.5 83.3 18.9 84.4 52.1 61.9\nTable 13: The accuracy comparison regarding different settings of variable for PEFT methods on math reasoning\ndatasets. The fine-tuning dataset used for this analysis is Math10K. In this context, \"vt\" refers to the number of\nvirtual tokens, \"bn\" denotes the bottleneck size, while \"r\" is the LoRA rank. The base model employed for this\nstudy is LLaMA-7B.\n5269\nQuestion At the schools book fair Sam bought 13 adventure books and 17 mystery books. If 15 of the books were\nused, how many new books did he buy?\nChatGPT To calculate the number of mosaic tiles needed to cover two-thirds of Mr. Boarden’s 36 square foot\nbathroom, follow these steps:\nFind two-thirds of 36: Multiply 36 by 2/3.\n(36 * 2/3) = 24 square feet\nDetermine the number of mosaic tiles needed per square foot: Given that Mr. Boarden needs 24 mosaic\ntiles per square foot, multiply the result from step 1 by 24.\n(24 * 24) = 576 mosaic tiles\nTherefore, Mr. Boarden would need 576 mosaic tiles to cover two-thirds of his 36 square foot bathroom.\nLLaMA13B-Prefix 1. Find the total number of books Sam bought:\n13 + 17 = 30 books\n2. Subtract the number of used books from the total number of books:\n30 - 15 = 15 new books\nTherefore, Sam bought 15 new books at the book fair.\nSteps:\n1. Total number of books = 13 + 17 = 30\n2. Number of used books = 15\n3. Number of new books = Total number of books - Number of used books\n4. Number of new books = 30 - 15 = 15\n5. Sam bought 15 new books at the book fair.\nAnswer: Sam bought 15 new books at the book fair. (15)\nLLaMA13B-Series 1. Find the total number of books Sam bought: 13 + 17 = 30\n2. Subtract the number of used books from the total number of books: 30 - 15 = 15\n3. Therefore, Sam bought 15 new books.\nAnswer: 15\nLLaMA13B-Parallel To find out how many new books Sam bought, we need to subtract the number of used books from the\ntotal number of books he bought.\nTotal books = 13 + 17 = 30\nUsed books = 15\nNew books = Total books - Used books\nNew books = 30 - 15\nNew books = 15\nTherefore, Sam bought 15 new books.\nLLaMA13B-LoRA 1. Find the total number of books Sam bought: 13 + 17 = 30\n2. Subtract the number of used books from the total number of books: 30 - 15 = 15\n3. Therefore, Sam bought 15 new books.\nAnswer: 15\nTable 14: An example randomly sampled from MultiArith. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\n5270\nQuestion Mr Boarden is remodeling his bathroom. For every square foot, he needs 24 mosaic tiles. How many\nmosaic tiles would Mr Boarden need to cover two thirds of his 36 sq ft bathroom?\nChatGPT To calculate the number of mosaic tiles needed to cover two-thirds of Mr. Boarden’s 36 square foot\nbathroom, follow these steps:\n1. Find two-thirds of 36: Multiply 36 by 2/3.\n(36 * 2/3) = 24 square feet\n2.Determine the number of mosaic tiles needed per square foot: Given that Mr. Boarden needs 24 mosaic\ntiles per square foot, multiply the result from step 1 by 24.\n(24 * 24) = 576 mosaic tiles\nTherefore, Mr. Boarden would need 576 mosaic tiles to cover two-thirds of his 36 square foot bathroom.\nLLaMA13B-Prefix 1 square foot = 4 square inches\n1 square inch = 16 square centimeters\nSo, 1 sq ft = 16 sq cm\nTherefore, 36 sq ft = 36 x 16 sq cm = 576 sq cm\nTo cover two-thirds of the bathroom, Mr Boarden needs to cover 576 sq cm x 2/3 = 384 sq cm\nSince 1 sq cm = 1 mosaic tile, Mr Boarden needs 384 sq cm / 1 sq cm = 384 mosaic tiles\nTherefore, Mr Boarden needs 384 mosaic tiles to cover two-thirds of his 36 sq ft bathroom.\nLLaMA13B-Series 1. Find the area of the bathroom: 36 sq ft\n2. Find two-thirds of the area: 2/3 x 36 sq ft = 24 sq ft\n3. Multiply the area by the number of tiles needed per square foot: 24 sq ft x 24 tiles/sq ft = 576 tiles\n4. Therefore, Mr Boarden would need 576 mosaic tiles to cover two-thirds of his 36 sq ft bathroom.\nLLaMA13B-Parallel 1. Calculate the total area of the bathroom: 36 sq ft\n2. Determine how much area Mr Boarden wants to cover with mosaic tiles: 2/3 x 36 sq ft = 24 sq ft\n3. Calculate the total number of mosaic tiles needed: 24 sq ft x 24 mosaic tiles per sq ft = 600 mosaic\ntiles\nTherefore, Mr Boarden would need 600 mosaic tiles to cover two thirds of his 36 sq ft bathroom. The\nanswer in Arabic numerals is 600.\nLLaMA13B-LoRA 1. Calculate the total area of the bathroom: 36 sq ft\n2. Determine how much area needs to be covered: 2/3 x 36 sq ft = 24 sq ft\n3. Calculate the number of mosaic tiles needed: 24 sq ft x 24 mosaic tiles per sq ft = 576 mosaic tiles\nTherefore, Mr Boarden would need 576 mosaic tiles to cover two thirds of his 36 sq ft bathroom. The\nanswer in Arabic numerals is 576.\nTable 15: An example randomly sampled from GSM8K. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\nQuestion There are 7 crayons in the drawer. Mary took 3 crayons out of the drawer. How many crayons are there\nnow?\nChatGPT To calculate the number of crayons remaining in the drawer after Mary took some out, follow these steps:\nSubtract the number of crayons Mary took from the total number of crayons in the drawer: 7 - 3 = 4\ncrayons.\nTherefore, there are now 4 crayons remaining in the drawer.\nLLaMA13B-Prefix 1. Start with the initial number of crayons in the drawer: 7.\n2. Subtract the number of crayons Mary took out of the drawer: 3.\n3. The final number of crayons in the drawer is 7 - 3 = 4.\nTherefore, there are 4 crayons in the drawer now.\nLLaMA13B-Series 1. Start with the initial number of crayons in the drawer: 7\n2. Subtract the number of crayons Mary took out: 3\n3. Perform the subtraction: 7 - 3 = 4\n4. Therefore, there are now 4 crayons in the drawer.\nAnswer: There are now 4 crayons in the drawer.\nLLaMA13B-Parallel 1. Start with the total number of crayons in the drawer: 7\n2. Subtract the number of crayons Mary took out: 3\n3. Perform the subtraction: 7 - 3 = 4\n4. The final answer is 4 crayons.\nTherefore, there are now 4 crayons in the drawer.\nLLaMA13B-LoRA 1. Start with the initial number of crayons: 7\n2. Subtract the number of crayons Mary took out: 7 - 3 = 4\n3. The final answer is 4 crayons.\nTherefore, there are now 4 crayons in the drawer.\nTable 16: An example randomly sampled from AddSub. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\n5271\nQuestion Suppose you want to arrange your English, Hindi, Mathematics, History, Geography and Science books\non a shelf. In how many ways can you do it ? Answer Choices: (A) 520 (B) 720 (C) 920 (D) None (E)\nCannot be determined\nChatGPT To calculate the number of ways you can arrange the English, Hindi, Mathematics, History, Geography,\nand Science books on a shelf, we can use the concept of permutations. Since order matters when\narranging the books, we need to find the number of permutations.\nTo determine the total number of arrangements, follow these steps:\nCount the number of books: There are 6 books in total.\nUse the formula for permutations of n objects, where n is the number of books: n! (n factorial).\nn! = 6! = 6 x 5 x 4 x 3 x 2 x 1 = 720\nTherefore, you can arrange the books on the shelf in 720 different ways.\nThe correct answer choice is (B) 720.\nLLaMA13B-Prefix 1. Let’s start by assigning variables to the unknowns in the problem. Let a, b, and c be the digits in M, N,\nand 396c respectively.\n2. We know that M and N are three-digit numbers, so a + b + c = 3.\n3. We also know that M - N + 396c = 990, so we can substitute a, b, and c into this equation:\na(1000000 - 100000 + 396c) + b(1000000 - 100000 + 396c) + c(1000000 - 100000 + 396c) = 990\n4. Simplifying this equation, we get:\na\nLLaMA13B-Series To solve this problem, we need to use the multiplication principle of counting, which states that if there\nare m ways to do one thing and n ways to do another thing, then there are m x n ways to do both things\ntogether.\nIn this case, there are 6 books to arrange (English, Hindi, Mathematics, History, Geography, and Science).\nWe can arrange these books in 6! ways.\nTherefore, the total number of ways to arrange the books on the shelf is 6! = 720.\nHence, the answer is option B) 720.\nLLaMA13B-Parallel The total number of books to be arranged on the shelf is 5 (English, Hindi, Mathematics, History,\nGeography, and Science).\nTo find the number of ways to arrange these books on the shelf, we can use the formula for permutations\nof n objects taken r at a time, which is given by:\nP(n,r) = n! / (n-r)!\nwhere n is the total number of objects, and r is the number of objects to be arranged at a time.\nIn this case, n = 5 (the total number of books), and r = 5 (since we want to arrange all 5 books on the\nshelf).\nSubstituting these values into the formula, we get:\nP(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)! P(5,5) = 5! / (5-5)!\nLLaMA13B-LoRA The total number of ways to arrange 6 books on a shelf is 6! = 6 x 5 x 4 x 3 x 2 x 1 = 720.\nTherefore, the answer is option B) 720.\nTable 17: An example randomly sampled from AQuA. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\n5272\nQuestion Oceanside Bike Rental Shop charges 17 dollars plus 7 dollars an hour for renting a bike. Tom paid 80\ndollars to rent a bike. How many hours did he pay to have the bike checked out?\nChatGPT To calculate the number of hours Tom paid to have the bike checked out, follow these steps:\n1. Subtract the base charge from the total amount Tom paid: 80 - 17 = 63 dollars.\n2. Divide the remaining amount by the hourly rate to find the number of hours: 63 / 7 = 9 hours.\nTherefore, Tom paid to have the bike checked out for 9 hours.\nNote: It’s important to note that the result represents the whole number of hours. If there were any partial\nhours, they would not be reflected in the calculation.\nLLaMA13B-Prefix 1. Let x be the number of hours Tom paid to rent the bike.\n2. The cost of renting the bike for x hours is 17 + 7x dollars.\n3. We know that Tom paid 80 dollars, so we can set up an equation:\n17 + 7x = 80\n4. Solving for x, we divide both sides by 7:\nx = 10\n5. Therefore, Tom paid 10 hours to rent the bike.\nLLaMA13B-Series 1. Find the total cost of renting the bike:\n$17 + $7/hour = $24/hour\n2. Divide the total cost by the cost per hour to find the number of hours:\n$80 ÷ $24/hour = 3.33 hours\n3. Round up to the nearest whole number since you can’t rent a bike for a fraction of an hour:\n3 hours\nTherefore, Tom paid to have the bike checked out for 3 hours.\nLLaMA13B-Parallel 1. Subtract the initial charge of $17 from the total amount paid: $80 - $17 = $63\n2. Divide the remaining amount by the hourly rate: $63 ÷ $7 = 9\n3. The answer is 9 hours.\nTherefore, Tom paid to have the bike checked out for 9 hours.\nLLaMA13B-LoRA 1. Let x be the number of hours Tom paid to have the bike checked out.\n2. The cost of renting the bike for x hours is 7x dollars.\n3. The total cost of renting the bike for x hours is 17 + 7x dollars.\n4. We know that Tom paid 80 dollars to rent the bike, so we can set up an equation: 17 + 7x = 80\n5. Solving for x, we can subtract 17 from both sides: 7x = 63\n6. Dividing both sides by 7, we get x = 9\nTherefore, Tom paid to have the bike checked out for 9 hours.\nTable 18: An example randomly sampled from SingleEq. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\n5273\nQuestion Matthew gave equal numbers of crackers and cakes to his 4 friends. If he had 32 crackers and 98 cakes\ninitially. How many crackers did each person eat?\nChatGPT To calculate the number of crackers each person ate, we need to find the number of crackers that each\nfriend received initially and subtract it from the total number of crackers Matthew had.\nHere are the steps:\nDivide the total number of crackers by the number of friends (4) to find the number of crackers each\nfriend received initially: 32 / 4 = 8 crackers per friend.\nSince we want to find the number of crackers each person ate, we need to subtract the initial number of\ncrackers per friend from the total number of crackers: 32 - 8 = 24 crackers.\nTherefore, each person ate 24 crackers.\nLLaMA13B-Prefix 1. Find the total number of crackers and cakes that Matthew initially had: 32 + 98 = 130\n2. Divide the total number of crackers and cakes by the number of friends to find the initial amount given\nto each friend: 130 ÷ 5 = 26\n3. Subtract the initial amount given to each friend from the total number of crackers and cakes to find the\nremaining amount: 130 - 26 = 104\n4. Divide the remaining amount by the number of friends to find the amount each person ate: 104 ÷ 5 =\n21\nTherefore, each person ate 21 crackers.\nLLaMA13B-Series 1. Divide the total number of crackers by the number of friends: 32 ÷ 4 = 8\n2. Divide the total number of cakes by the number of friends: 98 ÷ 4 = 24\n3. Add the number of crackers and cakes each friend received: 8 + 24 = 32\n4. Therefore, each person ate 32 crackers and 24 cakes.\nAnswer: Each person ate 32 crackers.\nLLaMA13B-Parallel 1. Divide the total number of crackers by the number of friends: 32 ÷ 4 = 8\n2. Divide the total number of cakes by the number of friends: 98 ÷ 4 = 24\n3. Add the number of crackers and cakes each person ate: 8 + 24 = 32\nTherefore, each person ate 32 crackers and cakes combined.\nLLaMA13B-LoRA 1. Divide the total number of crackers by the number of friends: 32 ÷ 4 = 8\n2. Divide the total number of cakes by the number of friends: 98 ÷ 4 = 24\n3. Add the number of crackers and cakes each person received: 8 + 24 = 32\nTherefore, each person ate 32 crackers and cakes in total.\nTable 19: An example randomly sampled from SV AMP. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\nQuestion Does ethanol take more energy make that produces?\nChatGPT False.\nLLaMA13B-Prefix the correct answer is true\nLLaMA13B-Series the correct answer is true\nLLaMA13B-Parallel false\nLLaMA13B-LoRA the correct answer is false\nTable 20: An example randomly sampled from BoolQ. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\nQuestion How do I ready a guinea pig cage for it’s new occupants?\nSolution1: Provide the guinea pig with a cage full of a few inches of bedding made of ripped paper strips,\nyou will also need to supply it with a water bottle and a food dish.\nSolution2: Provide the guinea pig with a cage full of a few inches of bedding made of ripped jeans\nmaterial, you will also need to supply it with a water bottle and a food dish.\nChatGPT Solution1\nLLaMA13B-Prefix the correct answer is solution1\nLLaMA13B-Series the correct answer is solution2\nLLaMA13B-Parallel the correct answer is solution2\nLLaMA13B-LoRA the correct answer is solution1\nTable 21: An example randomly sampled from PIQA. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\n5274\nQuestion Tracy didn’t go home that evening and resisted Riley’s attacks. What does Tracy need to do before this?\nAnswer1: make a new plan Answer2: Go home and see Riley Answer3: Find somewhere to go\nChatGPT Answer1/make a new plan\nLLaMA13B-Prefix the correct answer is answer1\nLLaMA13B-Series the correct answer is answer3\nLLaMA13B-Parallel the correct answer is answer3\nLLaMA13B-LoRA the correct answer is answer3\nTable 22: An example randomly sampled from SIQA. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\nQuestion Roof shingle removal: A man is sitting on a roof. he\nEnding1: is using wrap to wrap a pair of skis. Ending2: is ripping level tiles off. Ending3: is holding a\nrubik’s cube. Ending4: starts pulling up roofing on a roof.\nChatGPT Ending4: starts pulling up roofing on a roof.\nLLaMA13B-Prefix the correct answer is ending4\nLLaMA13B-Series the correct answer is ending4\nLLaMA13B-Parallel the correct answer is ending4\nLLaMA13B-LoRA the correct answer is ending4\nTable 23: An example randomly sampled from . The outputs of ChatGPT and LLaMA-13B with different PEFT\nmethods.\nQuestion Sarah was a much better surgeon than Maria so _ always got the easier cases.\nOption1: Sarah Option2: Maria\nChatGPT option1\nLLaMA13B-Prefix the correct answer is option1\nLLaMA13B-Series the correct answer is option1\nLLaMA13B-Parallel the correct answer is option1\nLLaMA13B-LoRA the correct answer is option1\nTable 24: An example randomly sampled from WinoGrande. The outputs of ChatGPT and LLaMA-13B with\ndifferent PEFT methods.\nQuestion Which statement best explains why photosynthesis is the foundation of most food webs?\nAnswer1: Sunlight is the source of energy for nearly all ecosystems. Answer2: Most ecosystems are\nfound on land instead of in water. Answer3: Carbon dioxide is more available than other gases. Answer4:\nThe producers in all ecosystems are plants.\nChatGPT\nLLaMA13B-Prefix Answer1. Sunlight is the source of energy for nearly all ecosystems.\nLLaMA13B-Series the correct answer is answer1\nLLaMA13B-Parallel the correct answer is answer4\nLLaMA13B-LoRA the correct answer is answer1\nTable 25: An example randomly sampled from ARC-e. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\nQuestion An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely\neffect of this increase in rotation?\nAnswer1: Planetary density will decrease. Answer2: Planetary years will become longer. Answer3:\nPlanetary days will become shorter. Answer4: Planetary gravity will become stronger.\nChatGPT Answer3\nLLaMA13B-Prefix the correct answer is answer3\nLLaMA13B-Series the correct answer is answer2\nLLaMA13B-Parallel the correct answer is answer3\nLLaMA13B-LoRA the correct answer is answer3\nTable 26: An example randomly sampled from ARC-c. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\n5275\nQuestion A person wants to start saving money so that they can afford a nice vacation at the end of the year. After\nlooking over their budget and expenses, they decide the best way to save money is to\nAnswer1: make more phone calls Answer2: quit eating lunch out Answer3: buy less with monopoly\nmoney Answer4: have lunch with friends\nChatGPT Answer2: quit eating lunch out.\nLLaMA13B-Prefix the correct answer is answer2\nLLaMA13B-Series the correct answer is answer2\nLLaMA13B-Parallel the correct answer is answer2\nLLaMA13B-LoRA the correct answer is answer2\nTable 27: An example randomly sampled from OBQA. The outputs of ChatGPT and LLaMA-13B with different\nPEFT methods.\n5276"
}