{
  "title": "Jointly Learning to Align and Translate with Transformer Models",
  "url": "https://openalex.org/W2970045405",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2223606138",
      "name": "Sarthak Garg",
      "affiliations": [
        "Apple (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A2308922892",
      "name": "Stephan Peitz",
      "affiliations": [
        "Apple (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A157519019",
      "name": "Udhyakumar Nallasamy",
      "affiliations": [
        "Apple (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A1855166872",
      "name": "Matthias Paulik",
      "affiliations": [
        "Apple (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962834107",
    "https://openalex.org/W2803369080",
    "https://openalex.org/W2963598809",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962708992",
    "https://openalex.org/W4298170715",
    "https://openalex.org/W2759932073",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2038698865",
    "https://openalex.org/W2897507397",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2398041834",
    "https://openalex.org/W2622084140",
    "https://openalex.org/W2141532438",
    "https://openalex.org/W1973923101",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2757154661",
    "https://openalex.org/W2080373976",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2963260202",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2401082558",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2912070261",
    "https://openalex.org/W2963499882",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964174820",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W3211259717",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, Matthias Paulik. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 4453–4462,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n4453\nJointly Learning to Align and Translate with Transformer Models\nSarthak Garg Stephan Peitz Udhyakumar Nallasamy Matthias Paulik\nApple Inc.\n{sarthak garg, speitz, udhay, mpaulik}@apple.com\nAbstract\nThe state of the art in machine translation\n(MT) is governed by neural approaches, which\ntypically provide superior translation accuracy\nover statistical approaches. However, on the\nclosely related task of word alignment, tradi-\ntional statistical word alignment models of-\nten remain the go-to solution. In this pa-\nper, we present an approach to train a Trans-\nformer model to produce both accurate trans-\nlations and alignments. We extract discrete\nalignments from the attention probabilities\nlearnt during regular neural machine trans-\nlation model training and leverage them in\na multi-task framework to optimize towards\ntranslation and alignment objectives. We\ndemonstrate that our approach produces com-\npetitive results compared to GIZA++ trained\nIBM alignment models without sacriﬁcing\ntranslation accuracy and outperforms previous\nattempts on Transformer model based word\nalignment. Finally, by incorporating IBM\nmodel alignments into our multi-task training,\nwe report signiﬁcantly better alignment accu-\nracies compared to GIZA++ on three publicly\navailable data sets. Our implementation has\nbeen open-sourced1.\n1 Introduction\nNeural machine translation (NMT) constitutes the\nstate of the art in MT, with the Transformer model\narchitecture (Vaswani et al., 2017) beating other\nneural architectures in competitive MT evalua-\ntions. The attention mechanism used in NMT\nmodels was motivated by the need to model word\nalignments, however it is now well known that the\nattention probabilities can differ signiﬁcantly from\nword alignments in the traditional sense (Koehn\nand Knowles, 2017), since attending to the con-\ntext words rather than the aligned source words\n1Code can be found at https://github.com/\npytorch/fairseq/pull/1095\nmight be helpful for translation. The presence of\nmulti-layer, multi-head attention mechanisms in\nthe Transformer model further complicate inter-\npreting the attention probabilities and extracting\nhigh quality discrete alignments from them.\nFinding source to target word alignments has\nmany applications in the context of MT. A\nstraightforward application of word alignments is\nto generate bilingual lexica from parallel corpora.\nWord alignments have also been used for external\ndictionary assisted translation (Chatterjee et al.,\n2017; Alkhouli et al., 2018; Arthur et al., 2016) to\nimprove translation of low frequency words or to\ncomply with certain terminology guidelines. Doc-\numents and webpages often contain word anno-\ntations such as formatting styles and hyperlinks,\nwhich need to be preserved in the translation. In\nsuch cases, word alignments can be used to trans-\nfer these annotations from the source sentence to\nits translation. In user facing translation services,\nproviding word alignments as additional informa-\ntion to the users might improve their trust and con-\nﬁdence, and also help them to diagnose problems\nsuch as under-translation (Tu et al., 2016).\nIn this work, we introduce an approach that\nteaches Transformer models to produce transla-\ntions and interpretable alignments simultaneously:\n•We use a multi-task loss function combin-\ning negative log likelihood (NLL) loss used\nin regular NMT model training and an align-\nment loss supervising one attention head to\nlearn alignments (Section 4.2).\n•Conditioning on past target context is essen-\ntial for maintaining the auto-regressive prop-\nerty for translation but can be limiting for\nalignment. We alleviate this problem by\nconditioning the different components of our\nmulti-task objective on different amounts of\ncontext (Section 4.3).\n4454\n•We demonstrate that the system can be su-\npervised using seed alignments obtained by\ncarefully averaging the attention probabili-\nties of a regular NMT model (Section 4.1)\nor alignments obtained from statistical align-\nment tools (Section 4.4)\nWe show that our model outperforms previous\nneural approaches (Peter et al., 2017; Zenkel et al.,\n2019) and statistical alignment models (Och and\nNey, 2003) in terms of alignment accuracy without\nsuffering any degradation of translation accuracy.\n2 Preliminaries\n2.1 Word Alignment Task\nGiven a sentence fJ\n1 = f1,...,f j,...f J in\nthe source language and its translation eI\n1 =\ne1,...,e i,...e I in the target language, an align-\nment Ais deﬁned as a subset of the Cartesian\nproduct of the word positions (Och and Ney,\n2003).\nA⊆{ (j,i) :j = 1,...,J ; i= 1,...,I } (1)\nThe word alignment task aims to ﬁnd a discrete\nalignment representing a many-to-many mapping\nfrom the source words to their corresponding\ntranslations in the target sentence.\n2.2 Transformer Model\nThe Transformer model (Vaswani et al., 2017)\nis an encoder-decoder model that only relies on\nattention for computing the contextual represen-\ntations for source and target sentences. Both\nthe encoder and decoder are composed of multi-\nple layers, each of which includes a multi-head\nself-attention and a feed-forward sub-layer. Lay-\ners in the decoder additionally apply a multi-\nhead encoder-decoder attention between the self-\nattention and the feed-forward sub-layers. To\nmaintain the auto-regressive property, the self-\nattention sub-layer in the decoder attends to the\nrepresentations of only the past tokens computed\nby the lower layer.\nIn this work, we will be focusing on guiding\nthe encoder-decoder attention sub-layer in the de-\ncoder. Let demb,dk,dv,N denote the embedding\ndimension, dimensions of the key and value pro-\njections and number of heads, respectively. As\ndescribed in Vaswani et al. (2017), for this sub-\nlayer, the output of the previous decoder layer cor-\nresponding to theith target token is used as a query\nvector qi ∈R1×demb and the encoder output for all\nthe source tokens are packed together as the value\nV ∈RJ×demb and key K ∈RJ×demb matrices. To\ncompute the output M(qi,K,V ), N heads ﬁrst\nproject the query vector and the key and value ma-\ntrices into different subspaces, compute attention\nin their own subspaces, aggregate their outputs and\nproject back to the original space:\n˜ qi\nn = qiWQ\nn , ˜Kn = KWK\nn ,˜Vn = VW V\nn (2)\nHi\nn = Attention(˜ qi\nn, ˜Kn,˜Vn) (3)\nM(qi,K,V ) =Concat(Hi\n1,...,H i\nN )WO, (4)\nwhere the projection matrices WQ\nn , WK\nn , WV\nn and\nWO are learnt parameters of the nth head. Each\nhead employs a scaled dot-product attention:\nAttention(˜ qi\nn, ˜Kn,˜Vn) =ai\nn ˜Vn, (5)\nwhere ai\nn = softmax(˜ qi\nn ˜KT\nn√dk\n). (6)\nThe vector ai\nn ∈R1×J denotes the attention prob-\nabilities for the ith target token over all the source\ntokens, computed by the nth attention head. For\nany particular head, an attention matrix AI×J can\nbe constructed by grouping together the vectorsai\nn\ncorresponding to all the target tokens. In the fol-\nlowing sections, we analyze the quality of align-\nments that can be extracted from these attention\nmatrices AI×J and describe how they can be ef-\nfectively supervised to learn word alignments.\n3 Baseline Methods\nA common baseline approach to extract word\nalignments from a regular NMT trained Trans-\nformer, is to average over all attention matrices\nAI×J computed across all layers and heads. The\nresulting matrix gives a probability distribution\nover all source tokens for each target token. This\ndistribution is then converted to a discrete align-\nment by aligning each target word to the corre-\nsponding source word with the highest attention\nprobability.\nPeter et al. (2017) guide the attention probabil-\nities to be close to the alignments obtained from\nstatistical MT toolkits by imposing an additional\nloss based on the distance between the alignment\nand attention distributions. They get improve-\nments in alignment accuracy over previous works\nbased on guided alignment training by feeding the\ncurrent target word to the attention module, pro-\nviding it more context about the target sentence.\n4455\nZenkel et al. (2019) proposed an method that\ndoes not rely on alignments from external toolkits\nfor training. They instead add an extra attention\nlayer on top of the Transformer architecture and\ndirectly optimize its activations towards predicting\nthe given target word.\nAll the above methods involve training mod-\nels for both the directions to get bidirectional\nalignments. These bidirectional alignments are\nthen merged using the grow diagonal heuris-\ntic (Koehn et al., 2005).\n4 Proposed Method\n4.1 Averaging Layer-wise Attention Scores\nThe attention heads in a single layer are sym-\nmetrical, but the different layers themselves can\nlearn drastically different alignments. To better\nunderstand the behavior of the encoder-decoder\nattention learnt at different layers, we average\nthe attention matrices computed across all heads\nwithin each layer and evaluate the obtained align-\nments. We show that the attention probabilities\nfrom the penultimate layer naturally tend to learn\nalignments and provide signiﬁcantly better results\ncompared to naively averaging across all layers\n(cf. Section 5.3). For the rest of the paper, we refer\nto the former method as the layer average baseline.\n4.2 Multi-task Learning\nTranslation and alignment tasks are very closely\nrelated. NMT models with attention (Bahdanau\net al., 2015) have also shown to learn alignments\nin the intermediate attention layer. A neural model\nreceiving supervision from given translations and\ngiven alignments can therefore beneﬁt from multi-\ntask learning by exploiting the correlations be-\ntween these two tasks.\nAnnotating word alignments is a laborious and\nexpensive task, but the layer average baseline de-\nscribed in Section 4.1 is able to generate reason-\nably good alignments in an unsupervised man-\nner. We thus use the alignments generated by the\nlayer average baseline as labels for supervising our\nmodel. We ﬁrst convert the alignments into a prob-\nability distribution over source tokens for every\ntarget token. Let GI×J denote a 0-1 matrix such\nthat Gi,j = 1if the jth source token is aligned to\nthe ith target token. We simply normalize the rows\nof matrix Gcorresponding to target tokens that are\naligned to at least one source token to get a matrix\nGp. As described in Section 2.2, the Transformer\nmodel computes multiple attention probability dis-\ntributions over source tokens for every target token\nacross different heads and layers of the network.\nSince we observed that the attention probabilities\nfrom the penultimate layer most naturally tend to\nlearn alignments (Section 5.3), we arbitrarily se-\nlect one head from the penultimate layer (subse-\nquently referred to as the alignment head) and su-\npervise its attention probability distribution to be\nclose to the labeled alignment distribution ( Gp).\nLet AI×J denote the attention matrix computed\nby the alignment head. For every target word i,\nwe minimize the Kullback-Leibler divergence be-\ntween Gp\ni and Ai which is equivalent to optimizing\nthe following cross-entropy loss La\nLa(A) =−1\nI\nI∑\ni=1\nJ∑\nj=1\nGp\ni,j log(Ai,j). (7)\nThe motivation behind supervising one head is that\nit gives the model the ﬂexibility to either use the\nrepresentation computed by the alignment head, or\ndepend more on the representations computed by\nother heads. We train our model to minimize La\nin conjunction with the standard NLL translation\nloss Lt. The overall loss Lis:\nL= Lt + λLa(A), (8)\nwhere λis a hyperparameter.\n4.3 Providing Full Target Context\nThe Transformer decoder computes the probabil-\nity of the next target token conditioned on the past\ntarget tokens and all source tokens. This is imple-\nmented by masking the self attention probabilities,\ni.e. while computing the representation for the ith\ntarget token, the decoder can only self-attend to\nthe representations of {1,2 ...i −1}tokens from\nthe previous layer. This auto-regressive behavior\nof the decoder is crucial for the model to repre-\nsent a valid probability distribution over the target\nsentence. However, conditioning on just the past\ntarget tokens is limiting for the alignment task.\nAs described in Section 4.2, the alignment head\nis trained to model the alignment distribution for\nthe ith target token given only the past target to-\nkens and all source tokens. Since the alignment\nhead does not know the identity of the next tar-\nget token, it becomes difﬁcult for it to learn this\ntoken’s alignment to the source tokens. Previous\nwork has also identiﬁed this problem and alleviate\n4456\nit by feeding the target token to be aligned as an\ninput to the module computing the alignment (Pe-\nter et al., 2017), or forcing the module to predict\nthe target token (Zenkel et al., 2019) or its prop-\nerties, e.g. POS tags (Li et al., 2018). Feeding the\nnext target token assumes that we know it in ad-\nvance and thus calls for separate translation and\nalignment models. Forcing the alignment module\nto predict target token’s properties helps but still\npasses the information of the target token in an in-\ndirect manner. We overcome these limitations by\nconditioning the two components of our loss func-\ntion on different amounts of context. The NLL\nloss Lt is conditioned on the past target tokens to\npreserve the auto-regressive property:\nLt = −1\nI\nI∑\ni=1\nlog(p(ei|fJ\n1 ,ei−1\n1 )). (9)\nHowever, the alignment loss L\n′\na is now condi-\ntioned on the whole target sentence:\nL\n′\na = La(A|fJ\n1 ,eI\n1). (10)\nThis is implemented by executing two forward\npasses of the decoder model, one with the mask-\ning of the future target tokens for computing the\nNLL loss Lt and the other one with no masking for\ncomputing the alignment loss L\n′\nafrom the align-\nment head. Although this formulation forces the\nnetwork to learn representations adapting to both\nfull and partial target context, Section 5.5 shows\nthat this approach does not degrade the translation\nquality while improving the alignment accuracy.\n4.4 Alignment Training Data\nOur method described so far does not rely on\nalignments from external statistical toolkits but\nperforms self-training on alignments extracted\nfrom the layer average baseline. However,\nGIZA ++ provides a robust method to compute ac-\ncurate alignments. If achieving better alignment\naccuracy is paramount, then our multi-task frame-\nwork can also leverage alignments from G IZA ++\nto produce even better alignment accuracy (Sec-\ntion 5.4). In this setting we use the GIZA ++ align-\nments as labels instead of those obtained from the\nlayer average baseline for supervising the align-\nment head.\n5 Experiments\n5.1 Setup\nOur experiments show that our proposed approach\nis able to achieve state-of-the-art results in terms\nof alignment and maintain the same translation\nperformance. In the following, we describe two\nsetups to compare with previously established\nstate-of-the-art results.\nFor all setups and models used in this work, we\nlearn a joint source and target Byte-Pair-Encoding\n(BPE, Sennrich et al. (2016)) with 32k merge op-\nerations. We observe that even for statistical align-\nment models sub-word units are beneﬁcial. To\nconvert the alignments from sub-word-level back\nto word-level, we consider each target word as be-\ning aligned to a source word if an alignment be-\ntween any of the target sub-words and source sub-\nwords exists.\nThe alignment quality is evaluated by using the\nalignment error rate (AER) introduced in (Och and\nNey, 2000). Signiﬁcance of the differences in AER\nbetween two models is tested using a two-sided\nWilcoxon signed-rank test (α= 0.1%).\n5.1.1 Alignment Task\nThe purpose of the this task is to fairly com-\npare with state-of-the-art results in terms of\nalignment quality and perform a hyperparame-\nter search. We use the same experimental setup\nas described in (Zenkel et al., 2019). The au-\nthors provide pre-processing and scoring scripts 2\nfor three different datasets: Romanian →English,\nEnglish→French and German →English. Train-\ning data and test data for Romanian→English and\nEnglish→French are provided by the NAACL’03\nBuilding and Using Parallel Texts word align-\nment shared task3 (Mihalcea and Pedersen, 2003).\nThe Romanian →English training data are aug-\nmented by the Europarl v8 corpus increasing the\namount of parallel sentences from 49k to 0.4M.\nFor German→English we use the Europarl v7 cor-\npus as training data and the gold alignments4 pro-\nvided by Vilar et al. (2006). The reference align-\nments were created by randomly selecting a subset\nof the Europarl v7 corpus and manually annotating\nthem following the guidelines suggested in (Och\n2https://github.com/lilt/\nalignment-scripts\n3http://web.eecs.umich.edu/˜mihalcea/\nwpt/index.html#resources\n4https://www-i6.informatik.\nrwth-aachen.de/goldAlignment/\n4457\nand Ney, 2003). Data statistics are shown in Ta-\nble 1.\nTable 1: Number of sentences for three datasets:\nGerman→English (DeEn), Romanian →English\n(RoEn) and English→French (EnFr). The datasets in-\nclude training data and test data with gold alignments.\nDeEn RoEn EnFr\ntraining 1.9M 0.5k 1.1M\ntest 508 248 447\nIn all experiments for this task, we employ the\nbase transformer conﬁguration with an embed-\nding size of 512, 6 encoder and decoder layers, 8\nattention heads, shared input and output embed-\ndings (Press and Wolf, 2017), the standard relu\nactivation function and sinusoidal positional em-\nbedding. The total number of parameters is 60M.\nWe train with a batch size of 2000 tokens on 8\nV olta GPUs and use the validation translation loss\nfor early stopping. Furthermore, we use Adam\noptimizer (Loshchilov and Hutter, 2019) with a\nlearning rate of 3e-4, β1 = 0.9,β2 = 0.98, learn-\ning rate warmup over the ﬁrst 4000 steps and in-\nverse square root as learning rate scheduler. The\ndropout probability is set to 0.1. Additionally, we\napply label smoothing with a factor of 0.1. To con-\nveniently extract word alignments for both transla-\ntion directions, we train bidirectional models, i.e.\nour models are able to translate and align from Ro-\nmanian to English and vice versa.\n5.1.2 Align and Translate Task\nThe second setup is based on the WMT‘18\nEnglish-German news translation task (Bojar\net al., 2018). We apply the same corpus selec-\ntion for bilingual data and model architecture as\nsuggested by Edunov et al. (2018). However, we\nslightly modify the preprocessing pipeline to be\nable to evaluate the alignment quality against the\ngold alignments provided by Vilar et al. (2006).\nWe use all available bilingual data (Europarl v7,\nCommon Crawl corpus, News Commentary v13\nand Rapid corpus of EU press releases) exclud-\ning the ParalCrawl corpus. We remove sentences\nlonger than 100 words and sentence pairs with a\nsource/target length ratio exceeding 1.5. This re-\nsults in 5.2M parallel sentences. We apply the\nMoses tokenizer (Koehn et al., 2007) without ag-\ngressive hyphen splitting and without perform-\ning HTML escaping of apostrophes and quotes.\nFurthermore, we do not normalize punctuation\nmarks. We use newstest2012 as validation and\nnewstest2014 as test set.\nTo achieve state-of-the-art translation results,\nall models in this setup are trained unidirectional\nand we change to the big transformer conﬁgura-\ntion with an embedding size of 1024 and 16 at-\ntention heads. The total number of parameters is\n213M. We train the layer average baseline with\na batch size of 7168 tokens on 64 V olta GPUs\nfor 30k updates and apply a learning rate of 1e-\n3, β1 = 0.9,β2 = 0.98. The dropout probabil-\nity is set to 0.3. All other hyperparameters are as\ndescribed in the previous section. Since training\nthe multi-task models consumes more memory, we\nneed to half the batch size, increase the number of\nupdates accordingly and adapt the learning rate to\n7e-4. We average over the last 10 checkpoints and\nrun inference with a beam size of 5.\nTo fairly compare against state-of-the-art trans-\nlation setups, we compute B LEU (Papineni et al.,\n2002) with sacreBLEU (Post, 2018).\n5.2 Statistical Baseline\nFor both setups, the statistical alignment models\nare computed with the multi-threaded version of\nthe GIZA ++ toolkit5 implemented by Gao and V o-\ngel (2008). G IZA ++ estimates IBM1-5 models\nand a ﬁrst-order hidden Markov model (HMM)\nas introduced in (Brown et al., 1993) and (V o-\ngel et al., 1996), respectively. In particular, we\nperform 5 iterations of IBM1, HMM, IBM3 and\nIBM4. Furthermore, the alignment models are\ntrained in both translation directions and sym-\nmetrized by employing the grow-diagonal\nheuristic (Koehn et al., 2005). We use the resulting\nword alignments to supervise the alignment loss\nfor the method described in Section 4.4.\n5.3 Averaging Attention Results\nFor our experiments, we use the data and Trans-\nformer model setup described in Section 5.1.1. We\nperform the evaluation of alignments obtained by\nlayer wise averaging of attention probabilities as\ndescribed in Section 4.1. As shown in Table 2,\nall three language pairs exhibit a very similar pat-\ntern, wherein the attentions do not seem to learn\nmeaningful alignments in the initial layers and\nshow a remarkable improvement in the higher lay-\ners. This indicates that the initial layers are fo-\n5https://github.com/moses-smt/mgiza/\n4458\nTable 2: A ER\n[%]\nper layer for all three language\npairs: German →English (DeEn), Romanian →English\n(RoEn) and English→French (EnFr).\nLayer DeEn RoEn EnFr\n1 (bottom) 90.0 92.9 80.7\n2 91.0 93.6 81.7\n3 94.5 92.0 73.4\n4 41.2 37.5 20.5\n5 32.6 33.4 17.0\n6 (top) 56.3 48.4 37.9\naverage 55.8 38.6 23.2\ncusing more on learning good representations of\nthe sentence generated by the decoder so far by\nself attention. Once good contextual representa-\ntions are learnt, the higher layers fetch the rele-\nvant representations from the encoder’s output via\nthe encoder-decoder attention. However, interest-\ningly the penultimate layer outperforms the ﬁnal\nlayer suggesting the ﬁnal layer uses the alignment-\nbased features in the penultimate layer to derive its\nown representation.\n5.4 Alignment Task Results\nTable 3 compares the performance of our meth-\nods against statistical baselines and previous neu-\nral approaches. The layer average baseline pro-\nvides relatively weak alignments, which are used\nfor training our multi-task model. The improve-\nment of the multi-task approach over the layer av-\nerage baseline suggests that learning to translate\nhelps produce better alignments as well. However\nstill the multi-task approach falls short of the sta-\ntistical and neural baselines, which have a strong\nadvantage of having access to the full/partial tar-\nget context. Exposing our model to the full target\ncontext gives the largest gains in terms of A ER.\nNote that full contextresults are directly compara-\nble to Zenkel et al. (2019) since both approaches\ndo not leverage external knowledge from statisti-\ncal models. We suspect that we are able to outper-\nform Zenkel et al. (2019) because we provide the\nfull target context instead of only the to-be aligned\ntarget word. Finally, by supervising our model on\nthe alignments obtained from G IZA ++ (G IZA ++\nsupervised) rather than layer average baseline, we\noutperform GIZA ++ and Peter et al. (2017).\nWe tuned the alignment loss weight λ (Equa-\ntion 8) using grid search on the German→English\ndataset. We achieve the best results withλ= 0.05.\nTable 3: Results on the alignment task (in A ER\n[%]\n).\n‡Difference in AER w.r.t. GIZA ++ (BPE-based) is sta-\ntistically signiﬁcant (p<0.001).\nModel DeEn RoEn EnFr\nGIZA ++ (word-based) 21.4 27.9 5.9\nGIZA ++ (BPE-based) 18.9 27.0 5.5\nLayer average baseline 32.6 33.4 17.0\nMulti-task 25.4 30.7 12.6\n+ full-context 20.2 26.0 7.7\n++ GIZA ++ supervised 16.0‡ 23.1‡ 4.6‡\nPeter et al. (2017) 19.0 - -\nZenkel et al. (2019) 21.2 27.6 10.0\n5.5 Align and Translate Task Results\nFor fair comparison of our approach to the state-\nof-the-art translation models, we use the setup de-\nscribed in Section 5.1.2. Table 4 summarizes the\nresults on alignment and translation tasks. The\nlayer average baseline is based on regular NMT\nmodel training, therefore ideally it should achieve\nthe same BLEU as Edunov et al. (2018), how-\never we see a small drop of 0.3 BLEU points in\npractice which could be caused by the slightly dif-\nferent preprocessing procedure (cf. Section 5.1.2,\nno aggressive hyphen splitting/no punctuation nor-\nmalization). The layer average baseline performs\npoorly in terms of the AER. The Precision and Re-\ncall results for the layer average baseline demon-\nstrate the effectiveness of symmetrization. Sym-\nmetrization removes a majority of incorrect align-\nments and gives a high precision (94.2%) but low\nrecall (29.6%). The high precision of the layer av-\nerage baseline ensures that the multi-task model\nreceives correct alignments for supervision, en-\nabling it to get large improvements in A ER over\nthe layer average baseline.\nSimilar to the trend observed in 5.4, provid-\ning full target sentence context in the decoder\nhelps the model to improve further and perform\ncomparably to G IZA ++. Lastly, supervision with\nGIZA ++ gives the best AER and signiﬁcantly out-\nperforms G IZA ++. The improvements in align-\nment quality and no degradation in B LEU com-\npared to the layer average baseline shows the ef-\nfectiveness of the proposed multi-task approach.\n4459\nTable 4: Results on the align and translate task. Alignment quality is reported in A ER, translation quality in\nBLEU . †baseline (without back-translation) sacreBLEU results were provided in https://github.com/\npytorch/fairseq/issues/506#issuecomment-464411433. ‡Difference in A ER w.r.t. G IZA ++\n(BPE-based) is statistically signiﬁcant (p<0.001)\nAER\n[%]\n(Precision\n[%]\n, Recall\n[%]\n) BLEU\n[%]\nModel DeEn EnDe Symmetrized DeEn EnDe\nGIZA ++ (word-based) 21.7 (85.4, 72.1) 24.0 (85.8, 68.2) 22.2 (93.5, 66.5) - -\nGIZA ++ (BPE-based) 19.0 (89.1, 74.2) 21.3 (86.8, 71.9) 19.6 (93.2, 70.6) - -\nLayer average baseline 66.8 (32.0, 34.6) 66.5 (32.5, 34.7) 54.8 (94.2, 29.6) 33.1 28.7\nMulti-task 31.1 (67.2, 70.7) 32.2 (66.6, 69.1) 25.8 (88.1, 63.8) 33.1 28.5\n+ full-context 21.2 (76.9, 80.9) 23.5 (75.0, 78.0) 19.5 (89.5, 72.9) 33.2 28.5\n++ GIZA ++ supervised 17.5‡(80.5, 84.7) 19.8‡(78.8, 81.7) 16.4‡(89.6, 78.2) 33.1 28.8\nEdunov et al. (2018)† - - - - 29.0\nTherefore\n,\nwe\nhave\nto\nbe\nextremely\ncareful\nwhen\nwe\nanalyse\nthem\n.\nDaherm¨ussenwirdieV oraussetzungenmitallergr\n¨oßter\nSorgfaltuntersuchen.\nI\nmust\ninform\nyou\nIchmußandieserStelledasHausdar\n¨uber\nin Kenntnissetzen\n(a) GIZA ++\nDaherm¨ussenwirdieV oraussetzungenmitallergr\n¨oßter\nSorgfaltuntersuchen.\nIchmußandieserStelledasHausdar\n¨uber\nin Kenntnissetzen\n(b) Our Model\nDaherm¨ussenwirdieV oraussetzungenmitallergr\n¨oßter\nSorgfaltuntersuchen.\nIchmußandieserStelledasHausdar\n¨uber\nin Kenntnissetzen\n(c) Reference\nFigure 1: Two examples from the German →English alignment test set. Alignments in (a) show the output from\nGIZA ++ and (b) from our model (Multi-task with full-context and GIZA ++ supervised). Gold Alignments in shown\nin (c). Black squares and hatched squares in the reference represent sure and possible alignments, respectively.\n6 Analysis\nTo further investigate why our proposed approach\nis superior to G IZA ++ in terms of A ER, we ana-\nlyze the generated word alignments of both mod-\nels. We observe that our model tends to align\npronouns (e.g. you or them) with regular nouns\n(e.g. objects or subjects). Given the gold align-\nments, it seems that these alignment links are\ncorrect or at least possible (Och and Ney (2003)\nprovided annotators two options to specify align-\nments: sure and possible for unambiguous and\nambiguous alignments respectively). Figure 1\nshows two examples from the German →English\nalignment test set. In the ﬁrst example, our model\ncorrectly aligns them with Voraussetzungen (cri-\nteria). The German parliament speaker indeed\nmentioned Verfahrensvoraussetzungen (procedu-\nral criteria)in one of the preceding sentences and\nrefers later to them by using the term Vorausset-\nzungen (criteria). In the second example, the pro-\nnoun you is correctly aligned to the noun Haus\n4460\n(house) which is just another way to address the\naudience in the European parliament. Both align-\nment links are not generated by G IZA ++. This\ncould be related to fact that a statistical model\nis based on counting co-occurrences. We specu-\nlate that to generate such alignment links, a model\nneeds to be able to encode contextual information.\nExperimental results in (Tang et al., 2018) suggest\nthat NMT models learn to encode contextual in-\nformation, which seems to be necessary for word\nsense disambiguation. Since pronouns can be am-\nbiguous references, we assume that both problems\nare closely related and therefore believe that the\nability to encode contextual information may be\nbeneﬁcial for generating word alignments.\nFrom our experiments on the WMT’18 dataset,\nwe observe that the alignment quality of the layer\naverage baseline is quite low (cf. Table 4). To fur-\nther investigate this, we plot the test A ER and the\nvalidation NLL loss per epoch (Figure 2). The\ngraph shows that the lowest A ER of 42.7% is al-\nready reached in the ﬁfth epoch. This suggests that\n5\n10\n0 20 40 60 80 40\n50\n60\n70\n80\n90\nvalidation loss (NLL)\nAER\n[%]\nepoch\nvalidation loss\nAER\nFigure 2: Test A ER and validation loss (NLL) per\nepoch on the WMT’18 English→German task.\npicking an earlier checkpoint for generating word\nalignments could be beneﬁcial for better supervi-\nsion. Unfortunately, an alignment validation set\ndoes not exist for this task.\n7 Related Work\nLeveraging alignments obtained from statistical\nMT toolkits to guide NMT attention mechanisms\nhas been explored in the past. Mi et al. (2016),\nChen et al. (2016), Liu et al. (2016) and Alkhouli\nand Ney (2017) supervise the attention mecha-\nnisms of recurrent models (Bahdanau et al., 2015)\nin this way. Our multi-task framework is inspired\nby these publications. However, we examine its\neffect on the Transformer model (Vaswani et al.,\n2017), which provides state-of-the-art results on\nseveral translation benchmarks. Previous works\nreport signiﬁcant gains in translation accuracy in\nlow resource settings, however gains remain mod-\nest given larger amounts of parallel data (mil-\nlions of sentences). These approaches also fail\nto achieve signiﬁcantly better alignment accuracy\nthan the statistical MT toolkits. Peter et al. (2017)\nand Li et al. (2018) improve upon the previous\nworks in terms of alignment accuracy by provid-\ning an alignment module with additional infor-\nmation about the to-be-aligned target word. Ex-\npanding on this idea, we propose to leverage the\nfull target sentence context leading to A ER im-\nprovements. Zenkel et al. (2019) presents an ap-\nproach that eliminates the reliance on statistical\nword aligners by instead by directly optimizing the\nattention activations for predicting the target word.\nWe empirically compare our approach of obtain-\ning high quality alignments without the need of\nstatistical word aligners to Zenkel et al. (2019).\nAugmenting the task objective with linguistic\ninformation, such as word alignments, also has\nhad applications beyond MT. Strubell et al. (2018)\nshowed that adding linguistic information from\nparse trees into one of the attention heads of the\ntransformer model can help in the semantic role\nlabeling. Inspired by Strubell et al. (2018), we in-\nject the alignment information through one of the\nattention heads for the translation task instead.\nAs a by-product of developing our model, we\npresent a simple way to quantitatively evaluate\nand analyze the quality of attention probabilities\nlearnt by different parts of the Transformer model\nwith respect to modeling alignments, which con-\ntributes to previous work on understanding atten-\ntion mechanisms (Ghader and Monz, 2017; Ra-\nganato and Tiedemann, 2018; Tang et al., 2018).\n8 Conclusions\nThis paper addresses the task of jointly learning\nto produce translations and alignments with a sin-\ngle Transformer model. By using a multi-task ob-\njective along with providing full target sentence\ncontext to our alignment module, we are able\nto produce better alignments than previous ap-\nproaches not relying on external alignment toolk-\nits. We demonstrate that our framework can be ex-\ntended to use external alignments from G IZA ++\nto achieve signiﬁcantly better alignment results\ncompared to GIZA ++, while maintaining the same\n4461\ntranslation performance.\nCurrently, our self-training based approach\nneeds two training runs. To train our model in a\nsingle run, we would like to investigate a train-\ning method which alternates between alignment\nextraction and model training.\nAcknowledgments\nWe would like to thank the LILT team for releas-\ning scripts and datasets for alignment evaluation.\nWe are also grateful to Andrew Finch, Matthias\nSperber, Barry Theobald and the anonymous re-\nviewers for their helpful comments. Many thanks\nto Dorothea Peitz for helpful discussions about\nsigniﬁcance testing, Yi-Hsiu Liao for suggesting\ninteresting extensions and the rest of Siri Machine\nTranslation Team for their support.\nReferences\nTamer Alkhouli, Gabriel Bretschner, and Hermann\nNey. 2018. On the alignment problem in multi-head\nattention-based neural machine translation. In Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 177–185, Brussels, Belgium.\nTamer Alkhouli and Hermann Ney. 2017. Biasing\nattention-based recurrent neural networks using ex-\nternal alignment information. In Conference on Em-\npirical Methods in Natural Language Processing,\npages 108–117, Copenhagen, Denmark.\nPhilip Arthur, Graham Neubig, and Satoshi Nakamura.\n2016. Incorporating discrete translation lexicons\ninto neural machine translation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1557–1567.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. InInternational Con-\nference on Learning Representations, San Diego,\nCA, USA.\nOndˇrej Bojar, Christian Federmann, Mark Fishel,\nYvette Graham, Barry Haddow, Philipp Koehn, and\nChristof Monz. 2018. Findings of the 2018 confer-\nence on machine translation (wmt18). In Confer-\nence on Statistical Machine Translation, pages 272–\n303, Belgium, Brussels.\nPeter F. Brown, Vincent J. Della Pietra, Stephen\nA. Della Pietra, and Robert L. Mercer. 1993. The\nmathematics of statistical machine translation: Pa-\nrameter estimation. Computational Linguistics,\n19(2):263–311.\nRajen Chatterjee, Matteo Negri, Marco Turchi, Mar-\ncello Federico, Lucia Specia, and Fr ´ed´eric Blain.\n2017. Guiding neural machine translation decod-\ning with external knowledge. In Proceedings of the\nSecond Conference on Machine Translation, pages\n157–168.\nWenhu Chen, Evgeny Matusov, Shahram Khadivi, and\nJan-Thorsten Peter. 2016. Guided alignment train-\ning for topic-aware neural machine translation. In\nAssociation for Machine Translation in the Ameri-\ncas, pages 121–134, Austin, TX, USA.\nSergey Edunov, Myle Ott, Michael Auli, and David\nGrangier. 2018. Understanding back-translation at\nscale. In Conference on Empirical Methods in Natu-\nral Language Processing, pages 489–500, Brussels,\nBelgium.\nQin Gao and Stephan V ogel. 2008. Parallel implemen-\ntations of word alignment tool. In Software Engi-\nneering, Testing, and Quality Assurance for Natural\nLanguage Processing, pages 49–57, Columbus, OH,\nUSA.\nHamidreza Ghader and Christof Monz. 2017. What\ndoes attention in neural machine translation pay at-\ntention to? In International Joint Conference on\nNatural Language Processing, pages 30–39, Taipei,\nTaiwan.\nPhilipp Koehn, Amittai Axelrod, Ra Birch Mayne,\nChris Callison-burch, Miles Osborne, and David\nTalbot. 2005. Edinburgh system description for the\n2005 iwslt speech translation evaluation. In Interna-\ntional Workshop on Spoken Language Translation,\npages 68–75, Pittsburgh, PA, USA.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ondrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 177–180, Prague, Czech\nRepublic.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Pro-\nceedings of the First Workshop on Neural Machine\nTranslation, pages 28–39.\nXintong Li, Lemao Liu, Zhaopeng Tu, Shuming Shi,\nand Max Meng. 2018. Target foresight based atten-\ntion for neural machine translation. InConference of\nthe North American Chapter of the Association for\nComputational Linguistics, pages 1380–1390, New\nOrleans, LA, USA.\nLemao Liu, Masao Utiyama, Andrew Finch, and Ei-\nichiro Sumita. 2016. Neural machine translation\nwith supervised attention. In International Con-\nference on Computational Linguistics, pages 3093–\n3102, Osaka, Japan.\n4462\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations, New Orleans,\nLA, USA.\nHaitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.\nSupervised attentions for neural machine transla-\ntion. In Conference on Empirical Methods in Natu-\nral Language Processing, pages 2283–2288, Austin,\nTX, USA.\nRada Mihalcea and Ted Pedersen. 2003. An evalua-\ntion exercise for word alignment. In Proceedings\nof the HLT-NAACL 2003 Workshop on Building and\nUsing Parallel Texts: Data Driven Machine Transla-\ntion and Beyond - Volume 3, pages 1–10, Edmonton,\nCanada.\nFranz Josef Och and Hermann Ney. 2000. Improved\nstatistical alignment models. In Annual Meeting\nof the Association for Computational Linguistics,\npages 440–447, Hong Kong.\nFranz Josef Och and Hermann Ney. 2003. A systematic\ncomparison of various statistical alignment models.\nComputational Linguistics, 29(1):19–51.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 311–318, Philadelphia, PA, USA.\nJan-Thorsten Peter, Arne Nix Nix, and Hermann Ney.\n2017. Generating alignments using target foresight\nin attention-based neural machine translation. In\nConference of the European Association for Ma-\nchine Translation, pages 27–36, Prague, Czech Re-\npublic.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. In Conference on Statistical Machine Trans-\nlation, pages 186–191, Belgium, Brussels.\nOﬁr Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Confer-\nence of the European Chapter of the Association for\nComputational Linguistics, pages 157–163, Valen-\ncia, Spain.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In BlackboxNLP: Ana-\nlyzing and Interpreting Neural Networks for NLP,\npages 287–297, Brussels, Belgium.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Annual Meeting of the Associ-\nation for Computational Linguistics, pages 1715–\n1725, Berlin, Germany.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Conference on Empirical Methods\nin Natural Language Processing, pages 5027–5038,\nBrussels, Belgium.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2018.\nAn analysis of attention mechanisms: The case of\nword sense disambiguation in neural machine trans-\nlation. In Conference on Statistical Machine Trans-\nlation, pages 26–35, Brussels, Belgium.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu,\nand Hang Li. 2016. Modeling coverage for neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1,\npages 76–85.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems, pages 1–11, Long Beach, CA,\nUSA.\nDavid Vilar, Maja Popovi ´c, and Hermann Ney. 2006.\nAER: Do we need to “improve” our alignments? In\nInternational Workshop on Spoken Language Trans-\nlation, pages 205–212, Kyoto, Japan.\nStephan V ogel, Hermann Ney, and Christoph Tillmann.\n1996. HMM-based word alignment in statistical\ntranslation. In International Conference on Com-\nputational Linguistics, pages 836–841, Copenhagen,\nDenmark.\nThomas Zenkel, Joern Wuebker, and John DeNero.\n2019. Adding Interpretable Attention to Neu-\nral Translation Models Improves Word Alignment.\narXiv e-prints.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7250050902366638
    },
    {
      "name": "Transformer",
      "score": 0.6082214117050171
    },
    {
      "name": "Natural language processing",
      "score": 0.5381913781166077
    },
    {
      "name": "Joint (building)",
      "score": 0.5064252614974976
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4864232540130615
    },
    {
      "name": "Engineering",
      "score": 0.13532909750938416
    },
    {
      "name": "Electrical engineering",
      "score": 0.07514792680740356
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210141230",
      "name": "Apple (Germany)",
      "country": "DE"
    }
  ]
}