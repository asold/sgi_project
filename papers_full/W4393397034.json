{
  "title": "Large language models could change the future of behavioral healthcare: a proposal for responsible development and evaluation",
  "url": "https://openalex.org/W4393397034",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Elizabeth C. Stade",
      "affiliations": [
        "Stanford University",
        "National Center for PTSD",
        "VA Palo Alto Health Care System"
      ]
    },
    {
      "id": "https://openalex.org/A2330482652",
      "name": "Shannon Wiltsey-Stirman",
      "affiliations": [
        "Stanford University",
        "National Center for PTSD",
        "VA Palo Alto Health Care System"
      ]
    },
    {
      "id": "https://openalex.org/A2147282416",
      "name": "Lyle H. Ungar",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2996974504",
      "name": "Cody L. Boland",
      "affiliations": [
        "National Center for PTSD",
        "VA Palo Alto Health Care System"
      ]
    },
    {
      "id": "https://openalex.org/A2170210807",
      "name": "H. Andrew Schwartz",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A854524581",
      "name": "David B Yaden",
      "affiliations": [
        "Johns Hopkins Medicine",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2554339368",
      "name": "João Sedoc",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2188759463",
      "name": "Robert J. DeRubeis",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2101078182",
      "name": "Robb Willer",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2056407401",
      "name": "Johannes C Eichstaedt",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": null,
      "name": "Elizabeth C. Stade",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)",
        "VA Palo Alto Health Care System",
        "Stanford University",
        "National Center for PTSD"
      ]
    },
    {
      "id": "https://openalex.org/A2330482652",
      "name": "Shannon Wiltsey-Stirman",
      "affiliations": [
        "VA Palo Alto Health Care System",
        "National Center for PTSD",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2147282416",
      "name": "Lyle H. Ungar",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2996974504",
      "name": "Cody L. Boland",
      "affiliations": [
        "National Center for PTSD",
        "VA Palo Alto Health Care System"
      ]
    },
    {
      "id": "https://openalex.org/A2170210807",
      "name": "H. Andrew Schwartz",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A854524581",
      "name": "David B Yaden",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2554339368",
      "name": "João Sedoc",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2188759463",
      "name": "Robert J. DeRubeis",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2101078182",
      "name": "Robb Willer",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2056407401",
      "name": "Johannes C Eichstaedt",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4236521339",
    "https://openalex.org/W3036340576",
    "https://openalex.org/W4376630419",
    "https://openalex.org/W3136599046",
    "https://openalex.org/W4317757464",
    "https://openalex.org/W4307474312",
    "https://openalex.org/W4308827648",
    "https://openalex.org/W4205111021",
    "https://openalex.org/W2977128309",
    "https://openalex.org/W4229082340",
    "https://openalex.org/W2963232155",
    "https://openalex.org/W2806313543",
    "https://openalex.org/W4285124505",
    "https://openalex.org/W3157021499",
    "https://openalex.org/W2756321254",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W1990005915",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W6788175385",
    "https://openalex.org/W2573034704",
    "https://openalex.org/W2915885068",
    "https://openalex.org/W3090265966",
    "https://openalex.org/W4381996422",
    "https://openalex.org/W4223430747",
    "https://openalex.org/W4226112058",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4311524007",
    "https://openalex.org/W4220882633",
    "https://openalex.org/W2005098924",
    "https://openalex.org/W4293243991",
    "https://openalex.org/W1991027029",
    "https://openalex.org/W4307433227",
    "https://openalex.org/W3040438665",
    "https://openalex.org/W3085246926",
    "https://openalex.org/W2942884758",
    "https://openalex.org/W2341549129",
    "https://openalex.org/W2633878725",
    "https://openalex.org/W4206244734",
    "https://openalex.org/W2152690559",
    "https://openalex.org/W2142149055",
    "https://openalex.org/W3160467094",
    "https://openalex.org/W2111459052",
    "https://openalex.org/W3182546273",
    "https://openalex.org/W2990568952",
    "https://openalex.org/W4295508820",
    "https://openalex.org/W2171464520",
    "https://openalex.org/W4248880250",
    "https://openalex.org/W2135110234",
    "https://openalex.org/W2981909951",
    "https://openalex.org/W2792686719",
    "https://openalex.org/W6853517571",
    "https://openalex.org/W4320059515",
    "https://openalex.org/W3163021855",
    "https://openalex.org/W4221052430",
    "https://openalex.org/W4366990156",
    "https://openalex.org/W4247665917",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W2053136065",
    "https://openalex.org/W2963827742",
    "https://openalex.org/W2905110776",
    "https://openalex.org/W4313596849",
    "https://openalex.org/W3035377849",
    "https://openalex.org/W2097439842",
    "https://openalex.org/W1549769523",
    "https://openalex.org/W2057906075",
    "https://openalex.org/W4200371774",
    "https://openalex.org/W3159498050",
    "https://openalex.org/W3147637074",
    "https://openalex.org/W3188887256",
    "https://openalex.org/W3101004475",
    "https://openalex.org/W3192973451",
    "https://openalex.org/W2119475075"
  ],
  "abstract": null,
  "full_text": "npj |mental health research Article\nhttps://doi.org/10.1038/s44184-024-00056-z\nLarge language models could change the\nfuture of behavioral healthcare: a proposal\nfor responsible development and\nevaluation\nCheck for updates\nElizabeth C. Stade1,2,3 ,S h a n n o nW i l t s e yS t i r m a n1,2,L y l eH .U n g a r4, Cody L. Boland1,\nH. Andrew Schwartz5, David B. Yaden6,J o ã oS e d o c7, Robert J. DeRubeis8,R o b bW i l l e r9 &\nJohannes C. Eichstaedt3\nLarge language models (LLMs) such as Open AI’s GPT-4 (which power ChatGPT) and Google’s\nGemini, built on artiﬁcial intelligence, hold immense potential to support, augment, or even eventually\nautomate psychotherapy. Enthusiasm about such applications is mounting in theﬁeld as well as\nindustry. These developments promise to address insufﬁcient mental healthcare system capacity and\nscale individual access to personalized treatments. However, clinical psychology is an uncommonly\nhigh stakes application domain for AI systems, as responsible and evidence-based therapy requires\nnuanced expertise. This paper provides a roadmap for the ambitious yet responsible application of\nclinical LLMs in psychotherapy. First, a technical overview of clinical LLMs is presented. Second, the\nstages of integration of LLMs into psychotherapy are discussed while highlighting parallels to the\ndevelopment of autonomous vehicle technology. Third, potential applications of LLMs in clinical care,\ntraining, and research are discussed, highlighting areas of risk given the complex nature of\npsychotherapy. Fourth, recommendations for the responsible development and evaluation of clinical\nLLMs are provided, which include centering clinical science, involving robust interdisciplinary\ncollaboration, and attending to issues like assessment, risk detection, transparency, and bias. Lastly, a\nvision is outlined for how LLMs might enable a new generation of studies of evidence-based\ninterventions at scale, and how these studies may challenge assumptions about psychotherapy.\nLarge language models (LLMs), built on artiﬁcial intelligence (AI)– such as\nOpen AI’s GPT-4 (which power ChatGPT) and Google’sG e m i n i– are\nbreakthrough technologies that can read, summarize, and generate text.\nLLMs have a wide range of abilities, including serving as conversational\nagents (chatbots), generating essays and stories, translating between lan-\nguages, writing code, and diagnosing illness\n1. With these capacities, LLMs\nare in ﬂuencing many ﬁelds, including education, media, software\nengineering, art, and medicine. They have started to be applied in the realm\nof behavioral healthcare, and consumers are already attempting to use LLMs\nfor quasi-therapeutic purposes\n2.\nApplications incorporating older forms of AI, including natural lan-\nguage processing (NLP) technology, have existed for decades3. For example,\nmachine learning and NLP have been used to detect suicide risk4,i d e n t i f y\nthe assignment of homework in psychotherapy sessions5,a n di d e n t i f y\n1Dissemination and Training Division, National Center for PTSD, VA Palo Alto Health Care System, Palo Alto, CA, USA.2Department of Psychiatry and Behavioral\nSciences, Stanford University, Stanford, CA, USA.3Institute for Human-Centered Artiﬁcial Intelligence & Department of Psychology, Stanford University, Stanford,\nCA, USA.4Department of Computer and Information Science, University of Pennsylvania, Philadelphia, PA, USA.5Department of Computer Science, Stony Brook\nUniversity, Stony Brook, NY, USA.6Department of Psychiatry and Behavioral Sciences, Johns Hopkins University School of Medicine, Baltimore, MD, USA.\n7Department of Technology, Operations, and Statistics, New York University, New York, NY, USA.8Department of Psychology, University of Pennsylvania,\nPhiladelphia, PA, USA.9Department of Sociology, Stanford University, Stanford, CA, USA.e-mail: betsystade@stanford.edu; johannes.stanford@gmail.com\nnpj Mental Health Research|            (2024) 3:12 1\n1234567890():,;\n1234567890():,;\npatient emotions within psychotherapy6. Current applications of LLMs in\nthe behavioral healthﬁeld are far more nascent– they include tailoring an\nLLM to help peer counselors increase their expressions of empathy, which\nhas been deployed with clients both in academic and commercial settings\n2,7.\nAs another example, LLM applications have been used to identify therapists’\nand clients’behaviors in a motivational interviewing framework8,9.\nSimilarly, while algorithmic intelligence with NLP has been deployed\nin patient-facing behavioral health contexts, LLMs have not yet been heavily\nemployed in these domains. For example, mental health chatbots Woebot\nand Tessa, which target depression and eating pathology respectively\n10,11,a r e\nrule-based and do not use LLMs (i.e., the application’s content is human-\ngenerated, and the chatbot’sr e s p o n d sb a s e do np r e d eﬁned rules or decision\ntrees12). However, these and other existing chatbots frequently struggle to\nunderstand and respond to unanticipated user responses10,13,w h i c hl i k e l y\ncontributes to their low engagement and high dropout rates14,15.L L M sm a y\nhold promise toﬁll some of these gaps, given their ability toﬂexibly generate\nhuman-like and context-dependent responses. A small number of patient-\nfacing applications incorporating LLMs have been tested, including a\nresearch-based application to generate dialog for therapeutic counseling\n16,17,\nand an industry-based mental-health chatbot, Youper, which uses a mix of\nrule-based and generative AI\n18.\nThese early applications demonstrate the potential of LLMs in psy-\nchotherapy– as their use becomes more widespread, they will change many\naspects of psychotherapy care delivery. However, despite the promise they\nmay hold for this purpose, caution is warranted given the complex nature of\npsychopathology and psychotherapy.Psychotherapy delivery is an unu-\nsually complex, high-stakes domain vis-à-vis other LLM use cases. For\nexample, in the productivity realm, with a“LLM co-pilot” summarizing\nmeeting notes, the stakes are failing to maximize efﬁciency or helpfulness; in\nbehavioral healthcare, the stakes may include improperly handling the risk\nof suicide or homicide.\nWhile there are other applications of artiﬁcial intelligence that may\ninvolve high-stakes or life-or death decisions (e.g., self-driving cars), pre-\ndiction and mitigation of risk in the case of psychotherapy is very nuanced,\ninvolving complex case conceptualization, the consideration of social and\ncultural contexts, and addressing unpredictable human behavior. Poor\noutcomes or ethical transgressions from clinical LLMs could run the risk of\nharming individuals, which may also be disproportionately publicized (as\nhas occurred with other AI failures\n19), which may damage public trust in the\nﬁeld of behavioral healthcare.\nTherefore, developers of clinical LLMs need to act with special\ncaution to prevent such consequences. Developing responsible clinical\nLLMs will be a challenging coordination problem, primarily because the\ntechnological developers who are typically responsible for product\ndesign and development lack clinical sensitivity and experience. Thus,\nbehavioral health experts will need to play a critical role in guiding\ndevelopment and speaking to the potential limitations, ethical con-\nsiderations, and risks of these applications.\nPresented below is a discussion on the future of LLMs in behavioral\nhealthcare from the perspective of both behavioral health providers and\ntechnologists. A brief overview of the technology underlying clinical LLMs is\nprovided for the purposes of both educating clinical providers and to set the\nstage for further discussion regarding recommendations for development.\nThe discussion then outlines various applications of LLMs to psychotherapy\nand provides a proposal for the cautious, phased development and eva-\nluation of LLM-based applications for psychotherapy.\nOverview of clinical LLMs\nClinical LLMs could take a wide variety of forms, spanning everything\nfrom brief interventions or circumscribed tools to augment therapy, to\nchatbots designed to provide psychotherapy in an autonomous\nmanner. These applications could be patient-facing (e.g., providing\npsychoeducation to the patient), t herapist-facing (e.g., offering\noptions for interventions from which the therapist could select),\ntrainee-facing (e.g., offering f eedback on qualities of the trainee ’s\nperformance), or supervisor/consultant facing (e.g., summarizing\nsupervisees ’ therapy sessions in ahigh-level manner).\nHow language models work\nLanguage models, or computational models of the probability of sequences\nof words, have existed for quite some time. The mathematical formulations\ndate back to\n20 and original use cases focused on compressing\ncommunication21 and speech recognition22– 24. Language modeling became a\nmainstay for choosing among candidate phrases in speech recognition and\nautomatic translation systems but until recently, using such models for\ngeneratingnatural language found little success beyond abstract poetry\n24.\nL a r g el a n g u a g em o d e l s\nThe advent oflarge language models, enabled by a combination of the deep\nlearning technique transformers25 and increases in computing power, has\nopened new possibilities26.T h e s em o d e l sa r eﬁrst trained on massive\namounts of data27,28 using“unsupervised” learning in which the model’st a s k\nis to predict a given word in a sequence of words. The models can then be\ntailored to a speciﬁc task using methods, including prompting with exam-\nples orﬁne-tuning, some of which use no or small amounts of task-speciﬁc\ndata (see Fig.1)\n28,29. LLMs hold promise for clinical applications because\nthey can parse human language and generate human-like responses, clas-\nsify/score (i.e., annotate) text, andﬂexibly adopt conversational styles\nrepresentative of different theoretical orientations.\nLLMs and psychotherapy skills\nFor certain use cases, LLM show a promising ability to conduct tasks or skills\nneeded for psychotherapy, such as conducting assessment, providing psy-\nchoeducation, or demonstrating interventions (see Fig.2). Yet to date,\nclinical LLM products and prototypes have not demonstrated anywhere\nnear the level of sophistication required to take the place of psychotherapy.\nFor example, while an LLM can generate an alternative belief in the style of\nCBT, it remains to be seen whether it can engage in the type of turn-based,\nSocratic questioning that would be expected to produce cognitive change.\nThis more generally highlights the gap that likely exists between simulating\ntherapy skills and implementing them effectively to alleviate patient suf-\nfering. Given that psychotherapy transcripts are likely poorly represented in\nthe training data for LLMs, and that privacy and ethical concerns make such\nrepresentation challenging, prompt engineering may ultimately be the most\nappropriateﬁne-tuning approach for shaping LLM behavior in this manner.\nClinical LLMs: stages of integration\nThe integration of LLMs into psychotherapy could be articulated as\noccurring along a continuum of stages spanning from assistive AI to fully\nautonomous AI (see Fig.3 and Table1). This continuum can be illustrated\nby models of AI integration in otherﬁelds, such as those used in the\nautonomous vehicle industry. For example, at one end of this continuum is\nthe assistive AI (“machine in the loop”) stage, wherein the vehicle system has\nno ability to complete the primary tasks– acceleration, braking, and steering\n– on its own, but provides momentary assistance (e.g., automatic emergency\nbreaking, lane departure warning) to increase driving quality or decrease\nburden on the driver. In the collaborative AI (“human in the loop”)s t a g e ,t h e\nvehicle system aids in the primary tasks, but requires human oversight (e.g.,\nadaptive cruise control, lane keeping assistance). Finally, in fully autono-\nmous AI, vehicles are self-driving and do not require human oversight. The\nstages of LLM integration into psychotherapy and their related function-\nalities are described below.\nStage 1: assistive LLMs\nAt theﬁrst stage in LLM integration, AI will be used as a tool to assist clinical\nproviders and researchers with tasks that can easily be“ofﬂoaded” to AI\nassistants (Table1; ﬁrst row). As this is a preliminary step in integration,\nrelevant tasks will be low-level, concrete, and circumscribed, such that they\npresent a low level of risk. Examples of tasks could include assisting with\ncollecting information for patient intakes or assessment, providing basic\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 2\nFig. 2 | Example clinical skills of large language models.Note. Figure was designed using image component from Flaticon.com.\nFig. 1 | Methods for tailoring clinical large lan-\nguage models.Figure was designed using image\ncomponents from Flaticon.com.\nFig. 3 | Stages of integrating large language models into psychotherapy.Figure was designed using image components from Flaticon.com.\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 3\npsychoeducation to patients, suggesting text edits for providers engaging in\ntext-based care, and summarizing patient worksheets. Administratively,\nsystems at this stage could also assist with clinical documentation by drafting\nsession notes.\nStage 2: collaborative LLMs\nFurther along the continuum, AI systems will take the leadb yp r o v i d i n go r\nsuggesting options for treatment planning and much of the therapy content,\nwhich humans will use their professional judgement to select from or tailor.\nFor example, in the context of a text- or instant-message delivered struc-\ntured psychotherapeutic intervention, the LLM might generate messages\ncontaining session content and assignments, which the therapist would\nreview and adapt as needed before sending (Table1; second row). A more\nadvanced use of AI within the collaborative stage may entail a LLM pro-\nviding a structured intervention in asemi-independent manner (e.g., as a\nchatbot), with a provider monitoring the discussion and stepping in to take\ncontrol of the conversation as needed. The collaborative LLM stage has\nparallels to“guided self-help” approaches\n30.\nStage 3: fully autonomous LLMs\nIn the fully autonomous stage, AIs will achieve the greatest degree of scope\nand autonomy wherein a clinical LLM would perform a full range of clinical\nskills and interventions in an integrated manner without direct provider\noversight (Table1; third row). For example, an application at this stage\nmight theoretically conduct a comprehensive assessment, select an appro-\npriate intervention, and deliver a full course of therapy with no human\nintervention. In addition to clinical content, applications in this stage could\nintegrate with the electronic health record to complete clinical doc-\numentation and report writing, schedule appointments and process billing.\nFully autonomous applications offer the most scalable treatment method\n30.\nProgression across the stages\nProgression across the stages may not be linear; human oversight will be\nrequired to ensure that applications at greater stages of integration are safe\nf o rr e a lw o r l dd e p l o y m e n t .A sd i f f e r ent forms of psychopathology and their\naccompanying interventions vary in complexity, certain types of interven-\ntions will be simpler than others to develop as LLM applications. Inter-\nventions that are more concrete and standardized may be easier for models\nto deliver (and may be available sooner), such as circumscribed behavior\nchange interventions (e.g., activity scheduling), as opposed to applications\nwhich include skills that are abstract in nature or emphasize cognitive\nchange (e.g., Socratic questioning). Similarly, when it comes to full therapy\nprotocols, LLM applications for interventions that are highly structured,\nbehavioral, and protocolized (e.g., CBT for insomnia [CBT-I] or exposure\ntherapy for speciﬁc phobia) may be available sooner than applications\ndelivering highlyﬂexible or personalized interventions (for example\n31).\nIn theory, theﬁnal stage in the integrationof LLMs into psychotherapy\nis fully autonomous delivery of psychotherapy which does not require\nhuman intervention or monitoring. However, it remains to be seen whether\nfully autonomous AI systems will reach a point at which they have been\nevaluated to be safe for deployment by the behavioral health community.\nSpeciﬁcc o n c e r n si n c l u d eh o ww e l lt h e s es y s t e m sa r ea b l et oc a r r yo u tc a s e\nconceptualization on individuals with complex, highly comorbid symptom\npresentations, including accounting for current and past suicidality, sub-\nstance use, safety concerns, medical comorbidities, and life circumstances\nand events (such as court dates and upcoming medical procedures). Simi-\nlarly, it is unclear whether these systems will prove sufﬁciently adept at\nengaging patients over time\n32 or accounting for and addressing contextual\nnuances in treatment (e.g., using exposure to treat a patient experiencing\nPTSD-related fear of leaving the house, who also lives in a neighborhood\nwith high rates of crime). Furthermore,several skills which may be viewed as\ncentral to clinical work currently fall outside the purview of LLM systems,\nsuch as interpreting nonverbal behavior (e.g., ﬁdgeting, eye-rolling),\nappropriately challenging a patient, addressing alliance ruptures, and\nmaking decisions about termination. Technological advances, including the\nTable 1 | Stages of Development of Clinical LLMs\nStage Car Analogy Characteristics of\nAssessment\nIntervention\nFocus/Scope\nIntervention Nature Clinical Example Potential Risks or Costs\nAssistive AI\n(“machine in\nthe loop”)\nAI-based features (e.g., automatic\nemergency breaking, lane depar-\nture warning) in the vehicle.\nStandalone, modularized (e.g.,\nassessments hand-picked by\ntherapist and administered by\nsurvey).\nLimited to concrete/ cir-\ncumscribed (e.g., activity\nplanning).\nNo full intervention packages;\nlimited to components of\ninterventions.\nLLM trained to conduct skills from\nCBT-I might converse with the patient\nto collect their sleep diary data from\nthe previous week to expedite a tra-\nditional therapy session.\nOverhead and complexity for\ntherapist for AI supervision.\nCollaborative AI\n(“human in\nthe loop”)\nVehicle mostly completing the\nprimary task; human in the driver\nseat actively monitors the vehi-\ncle’s progress and overrides it as\nneeded (e.g., adaptive cruise\ncontrol, lane keeping assist).\nIncreasingly integrated (e.g.,\nassessments recommended\nby LLM and summarized with\ncontext for therapist review).\nIncludes less concrete,\nmore abstract interven-\ntions (e.g., planning and\nprocessing exposures).\nLimited to structured/standar-\ndized (e.g., CBT for insomnia).\nCBT-I LLM might generate a) an\noverview of the sleep diary data, b) a\nrationale for sleep restriction and sti-\nmulus control, and c) a sleep schedule\nprescription based on the diary data.\nThis content would be reviewed and\ntailored by the psychotherapist before\nbeing discussed with the patient.\nDrafts that require signiﬁcant cor-\nrections may not save much time;\nBusy therapists may fail to check or\ntailor content, especially if given\nhigher caseloads due to AI\nassistance.\nFully autono-\nmous AI\nFully autonomous vehicles that\noperate without direct human\noversight.\nFully integrated, informs inter-\nvention (e.g., unobtrusive,\nautomated symptom assess-\nment running in background).\nIncludes very abstract/\ndiffuse interventions (e.g.,\nSocratic questioning).\nIncludes unstructured/unstan-\ndardized (e.g., acceptance and\ncommitment therapy, idio-\ngraphic or modular\napproaches).\nLLM could implement a full course of\nCBT-I. The LLM would directly deliver\nmulti-session therapy interventions\nand content to the patient, which\nwould not be subject to tailoring or\ninitial oversight by the\npsychotherapist.\nCritical information could be mis-\nsed (e.g., suicide risk); Provision of\ninappropriate or harmful care.\nAI artiﬁcial intelligence,LLM large language model,CBT-I cognitive behavioral therapy for insomnia.\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 4\napproaching advent of multimodal language models that integrate text,\nimages, video, and audio, may eventually begin toﬁll these gaps.\nBeyond technical limitations, it remains to be decided whether com-\nplete automation is an appropriate end goal for behavioral healthcare, due to\nsafety, legal, philosophical, and ethical concerns\n33. While some evidence\nindicates that humans can develop atherapeutic alliance with chatbots34,t h e\nlong-term viability of such alliance building, and whether or not it produces\nundesirable downstream effects (e.g., altering an individual’se x i s t i n gr e l a -\ntionships or social skills) remains to be seen. Others have documented\npotentially harmful behavior of LLM chatbots, such as narcissistic\ntendencies35 and expressed concerns about the potential for their undue\ninﬂuence on humans in addition to articulating societal risks associated with\nLLMs more generally36,37.T h eﬁeld will also need to grapple with questions\nof accountability and liability in the case of a fully autonomous clinical LLM\napplication causing damage (e.g., identifying the responsible party in an\nincident of malpractice\n38). For these and other reasons, some have argued\nagainst the implementation of fullyautonomous systems in behavioral\nhealthcare and healthcare more broadly39,40. Taken together, these issues and\nconcerns may suggest that in the short and medium term, assistive or\ncollaborative AI applications will be more appropriate for the provision of\nbehavioral healthcare.\nApplications of clinical LLMs\nGiven the vast nature of behavioral healthcare, there are seemingly endless\napplications of LLMs. Outlined below are some of the currently existing,\nimminently feasible, and potential long-term applications of clinical LLMs.\nHere we focus our discussion on applications directly related to the provi-\nsion of, training in, and research on psychotherapy. As such, several\nimportant aspects of behavioral healthcare, such as initial symptom detec-\ntion, psychological assessment and brief interventions (e.g., crisis counsel-\ning) are not explicitly discussed herein.\nImminent applications\nAutomating clinical administration tasks. At the most basic level,\nLLMs have the potential to automate several time-consuming tasks\nassociated with providing psychotherapy (Table2, ﬁrst row). In addition\nto using session transcripts to summarize the session for the provider,\nthere is potential for such models to integrate within electronic health\nrecords to aid with clinical documentation and conducting chart reviews.\nClinical LLMs could also produce a handout for the patient that provides\na personalized overview of the session, skills learned and assigned\nhomework or between-session material.\nMeasuring treatmentﬁdelity. A clinical LLM application could auto-\nmate measurement of therapist ﬁdelity to evidence-based practices\n(EBPs; Table2, second row), which can include measuringadherence to\nthe treatment as designed,competence in delivering a speciﬁc therapy\nskill, treatment differentiation (whether multiple treatments being\ncompared actually differ from one another), and treatment receipt\n(patient comprehension of, engagement with, and adherence to the\ntherapy content)\n41,42. Measuring ﬁdelity is crucial to the development,\ntesting, dissemination, and implementation of EBPs, yet can be resource\nintensive and difﬁcult to do reliably. In the future, clinical LLMs could\ncomputationally derive adherence and competence ratings, aiding\nresearch efforts and reducing therapist drift\n43. Traditional machine-\nlearning models are already being used to assess ﬁdelity to speciﬁc\nmodalities44 and other important constructs like counseling skills45 and\nalliance46. Given their improved ability to consider context, LLMs will\nlikely increase the accuracy with which these constructs are assessed.\nOffering feedback on therapy worksheets and homework. LLM\napplications could also be developed deliver real-time feedback and\nsupport on patients’ between-session homework assignments (Table2,\nthird row). For example, an LLM tailored to assist a patient to complete a\nCBT worksheet might provide clariﬁcation or aid in problem solving if\nthe patient experiences difﬁculty (e.g., the patient was completing a\nthought log and having trouble differentiating between the thought and\nthe emotion). This could help to“bridge the gap” between sessions and\nexpedite patient skill development. Early evidence outside the AI realm\n47\npoints to increasing worksheet competence as a fruitful clinical target.\nAutomating aspects of supervision and training. LLMs could be used\nto provide feedback on psychotherapy or peer support sessions, especially for\nclinicians with less training and experience (i.e., peer counselors, lay health\nworkers, psychotherapy trainees). For example, an LLM might be used to\noffer corrections and suggestions to the dialog of peer counselors (Table2,\nTable 2 | Imminent possibilities for clinical LLMs\nTask Target Audience Example Input to LLM Example LLM Output\nAid in administrative tasks Clinician Psychotherapy session recording “… Met with patient for cognitive behavioral therapy\nfor depression. Reviewed homework; patient com-\npleted three thought records over the past week.\nIntroduced ‘thinking biases’ worksheet; assisted\npatient in identifying patterns of problematic think-\ning applicable to her automatic negative thoughts\nfrom the past week…”\nOffer feedback on therapy homework\nworksheets\nPatient Digital CBT worksheet; Patient writes, “I’ve\nalways felt this way,” as evidence in support of\nthe negative automatic thought:“I’m unlovable”\non the worksheet\n“Remember, ‘evidence’ means facts that support\nthe belief. Sometimes it’s helpful to think about facts\nso strong they would stand up in a court of law. What\nis the evidence that you are unlovable?”\nProduce adherence and competence\nratings for elements of therapy\nResearcher Psychotherapy session recording “… Therapist helped patient identify negative auto-\nmatic thoughtsAdherence rating (0-1): 1 Compe-\ntence rating (0-6): 5…”\nIdentify trainee psychotherapist’s\nareas of success and areas for\nimprovement\nPsychotherapy trainee Psychotherapy session recording “…In the following exchange, the therapist suc-\ncessfully used Socratic questioning to ask open-\nended, non-leading questions:[Patient: I should\nhave known that it wasn’t safe to get in that car.\nTherapist: Hm, help me understand… how could you\nhave known that it wasn’t safe?]…”\nSuggest an improved therapeutic\nresponse, offer education about\ntherapeutic exchanges\nPeer counselor or lay\nmental health worker\nMessage-based exchange between patient and\npeer counselor; peer counselor has drafted a\nresponse: “You’ll beﬁne”\n“This could be improved by offering validation of the\nclient’s feelings. For instance, you might say,‘it\nsounds like you’re going through a difﬁcult time, and\nit\n’s understandable to feel overwhelmed.’ Would\nyou like to rewrite before sending?”\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 5\nfourth row). This application has parallels to“task sharing,” am e t h o du s e di n\nthe global mental healthﬁeld by which nonprofessionals provide mental\nhealth care with the oversight by specialist workers to expand access to\nmental health services48. Some of this work is already underway, for example,\nas described above, using LLMs to support peer counselors7.\nLLMs could also support supervision for psychotherapists learning\nnew treatments (Table2, ﬁfth row). Gold-standard methods of reviewing\ntrainees’ work, like live observation or review of recorded sessions49,a r e\ntime-consuming. LLMs could analyze entire therapy sessions and identify\nareas of improvement, offering a scalable approach for supervisors or\nconsultants to review.\nPotential long-term applications\nIt is important to note that many of the potential applications listed below\nare theoretical and have yet to be developed, let alone thoroughly evaluated.\nFurthermore, we use the term“clinical LLM” in recognition of the fact that\nwhen and under what circumstances the work of an LLM could be called\np s y c h o t h e r a p yi se v o l v i n ga n dd e p e n d so nh o wp s y c h o t h e r a p yi sd eﬁned.\nFully autonomous clinical care. As previously described, theﬁnal stage of\nclinical LLM development could involve an LLM that can independently\nconduct comprehensive behavioral healthcare. This could involve all aspects\nrelated to traditional care includingconducting assessment, presenting\nfeedback, selecting an appropriate intervention and delivering a course of\ntherapy to the patient. This course of treatment could be delivered in ways\nconsistent with current models of psychotherapy wherein a patient engages\nwith a“chatbot” weekly for a prescribed amount of time, or in moreﬂexible\nor alternative formats. LLMs used inthis manner would ideally be trained\nusing standardized assessment approaches and manualized therapy proto-\ncols that have large bodies of evidence.\nDecision aid for existing evidence-based practices.E v e nw i t h o u tf u l l\nautomation, clinical LLMs could be used as a tool to guide a provider on the\nbest course of treatment for a given patient by optimizing the delivery of\nexisting EBPs and therapeutic techniques. In practice, this may look like a\nLLM that can analyze transcripts from therapy sessions and offer a provider\nguidance on therapeutic skills, approaches or language, either in real time, or\nat the end of the therapy session. Furthermore, the LLM could integrate\ncurrent evidence on the tailoring of speciﬁc EBPs to the condition being\ntreated, and to demographic or cultural factors and comorbid conditions.\nDeveloping tailored clinical LLM“advisors” based on EBPs could both\nenhance ﬁdelity to treatment and maximize the possibility of patients\nachieving clinical improvement in light of updated clinical evidence.\nDevelopment of new therapeutic techniques and EBPs. To this\npoint, we have discussed how LLMs could be applied to current\napproaches to psychotherapy using extant evidence. However, LLMs and\nother computational methods could greatly enhance the detection and\ndevelopment of new therapeutic skills and EBPs. Historically, EBPs have\ntraditionally been developed using human-derived insights and then\nevaluated through years of clinical trial research. While EBPs are effec-\ntive, effect sizes for psychotherapy are typically small\n50,51 and signiﬁcant\nproportions of patients do not respond52. There is a great need for more\neffective treatments, particularly for individuals with complex pre-\nsentations or comorbid conditions. However, the traditional approach to\ndeveloping and testing therapeutic interventions is slow, contributing to\nsigniﬁcant time lags in translational research\n53, and fails to deliver\ninsights at the level of the individual.\nData-driven approaches hold the promise of revealing patterns that are\nnot yet realized by clinicians, thusgenerating new approaches to psy-\nchotherapy; machine learning is already being used, for example, to predict\nbehavioral health treatment outcomes54. With their ability to parse and\nsummarize natural language, LLMs could add to existing data-driven\napproaches. For example, an LLM couldbe provided with a large historical\ndataset containing psychotherapy transcripts of different therapeutic\norientations, outcome measures and sociodemographic information, and\ntasked with detecting therapeutic behaviors and techniques associated with\nobjective outcomes (e.g., reductionin depressive symptoms). Using such a\nprocess might make it possible for an LLM to yieldﬁne-grained insights\nabout what makes existing therapeutic techniques work best (e.g., Which\ncomponents of existing EBPs are the most potent? Are there therapist or\npatient characteristics that moderate the efﬁcacy of intervention X? How\ndoes the ordering of interventions effect outcomes?) or even to isolate\npreviously unidentiﬁed therapeutic techniquesassociated with improved\nclinical outcomes. By identifying what happens in therapy in such aﬁne-\ngrained manner, LLMs could also play a role in revealing mechanisms of\nchange, which is important for improving existing treatments and facil-\nitating real-world implementation\n55.\nHowever, to realize this possibility, and make sure that LLM-based\nadvances can be integrated and vetted by the clinical community, it is\nnecessary to steer away from the development of“black box,” LLM-\nidentiﬁed interventions with low explainability (e.g., interpretability56). To\nguard against interventions with low interpretability, work toﬁnetune LLMs\nto improve patient outcomes could include inspectable representations of\nthe techniques employed by the LLM. Clinicians could examine these\nrepresentations and situate them in thebroader psychotherapy literature,\nwhich would involve comparing them toexisting psychotherapy techniques\nand theories. Such an approach could speed up the identiﬁcation of novel\nmechanisms while guarding against the identiﬁcation of“novel” interven-\ntions which overlap with existing techniques or constructs (thus avoiding\nthe jangle fallacy, the erroneous assumption that two constructs with dif-\nferent names are necessarily distinct\n57).\nIn the long run, by combining this information, it might even be\npossible for an LLM to“reverse-engineer” a new EBP, freed from the con-\nstraints of traditional therapeutic protocols and instead maximizing on the\ndelivery of the constituent components shown to produce patient change (in\na manner akin to modular approaches, wherein an individualized treatment\nplan is crafted for each patient by curating and sequencing treatment\nmodules from an extensive menu of all available options based on the\nunique patient’s presentation\n31). Eventually, a self-learning clinical LLM\nmight deliver a broad range of psychotherapeutic interventions while\nmeasuring patient outcomes and adapting its approach on theﬂyi n\nresponse to changes in the patient (or lack thereof).\nToward a precision medicine approach to psychotherapy\nCurrent approaches to psychotherapy of t e na r eu n a b l et op r o v i d eg u i d a n c e\non the best approach to treatment when an individual has a complex pre-\nsentation, which is often the rule rather than being the exception. For\nexample, providers are likely to have greatly differing treatment plans for a\npatient with concurrent PTSD, substance use, chronic pain, and signiﬁcant\ninterpersonal difﬁculties. Models that use a data-driven approach (rather\nthan a provider’s educated guess) to address an individual’s presenting\nconcern alongside their comorbidities, sociodemographic factors, history,\nand responses to the current treatment,may ultimately offer the best chance\nat maximizing patient beneﬁt. While there have been some advances in\nprecision medicine approaches in behavioral healthcare\n54,58, these efforts are\nin their infancy and limited by sample sizes59.\nThe potential applications of clinical LLMs we have outlined above\nmay come together to facilitate a personalized approach to behavioral\nhealthcare, analogous to that of precision medicine. Through optimizing\nexisting EBPs, identifying new therapeutic approaches, and better under-\nstanding mechanisms of change, LLMs (and their future descendants) may\nprovide behavioral healthcare with an enhanced ability to identify what\nworks best for whom and under what circumstances.\nRecommendations for responsible development and\nevaluation of clinical LLMs\nFocus ﬁrst on evidence-based practices\nIn the immediate future, clinical LLM applications will have the greatest\nchance of creating meaningful clinical impact if developed based on EBPs or\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 6\na “common elements” approach (i.e., evidence-based procedures shared\nacross treatments)60. Evidence-based treatments and techniques have been\nidentiﬁed for speciﬁc psychopathologies (e.g., major depressive disorder,\nposttraumatic stress disorder), stressors (e.g., bereavement, job loss,\ndivorce), and populations (e.g., LGBTQ individuals, older adults)55,61,62.\nWithout an initial focus on EBPs, clinical LLM applications may fail to\nreﬂect current knowledge and may even produce harm\n63. Only once LLMs\nhave been fully trained on EBPs can theﬁeld start to consider using LLMs in\na data-driven manner, such as those outlined in the previous section on\npotential long-term applications.\nFocus next on improvement (engagement is not enough)\nOthers have highlighted the importance of promoting engagement with\ndigital mental health applications15, which is important for achieving an\nadequate“dose” of the therapeutic intervention. LLM applications hold the\npromise of improving engagement and retention through their ability to\nrespond to free text, extract key concepts, and address patients’ unique\ncontext and concerns during interventions in a timely manner. However,\nengagement alone is not an appropriate outcome on which to train an LLM,\nbecause engagement is not expected to be sufﬁcient for producing change. A\nfocus on such metrics for clinical LLMs will risk losing sight of the primary\ngoals, clinical improvement (e.g., reductions in symptoms or impairment,\nincreases in well-being and functioning) and prevention of risks and adverse\nevents. It will behoove theﬁeld to be wary of attempts to optimize clinical\nLLMs on outcomes that have an explicit relationship with a company’s\nproﬁt (e.g., length of time using the application). An LLM that optimizes\nonly for engagement (akin to YouTube recommendations) could have high\nrates of user retention without employing meaningful clinical interventions\nto reduce suffering and improve quality of life. Previous research has sug-\ngested that this may be happening with non-LLM digital mental health\ninterventions. For instance, exposure is a technique with strong support for\ntreating anxiety, yet it is rarely included in popular smartphone applications\nfor anxiety\n64, perhaps because developers fear that the technique will not\nappeal to users, or have concerns about how exposures going poorly or\nincreasing anxiety in the short term, which may prompt concerns about\nlegal exposure.\nCommit to rigorous yet commonsense evaluation\nAn evaluation approach for clinical LLMs that hierarchically prioritizes risk\nand safety, followed by feasibility, acceptability, and effectiveness, would be\nin line with existing recommendations for the evaluation of digital mental\nhealth smartphone apps\n65.T h eﬁrst level of evaluation could involve a\ndemonstration that a clinical LLM produces no harm or very minimal harm\nthat is outweighed by its beneﬁts, similar to FDA phase I drug tests. Key risk\nand safety related constructs include measures of suicidality, non-suicidal\nself harm, and risk of harm to others.\nNext, rigorous examinations of clinical LLM applications will be nee-\nded to provide empirical evidence of their utility, using head-to-head\ncomparisons with standard treatments. Key constructs to be assessed in\nthese empirical tests are feasibility and acceptability to the patient and the\ntherapist as well as treatment outcomes (e.g., symptoms, impairment,\nclinical status, rates of relapse). Other relevant considerations include\npatients’ user experience with the application, measures of therapist efﬁ-\nciency and burnout, and cost.\nLastly, we note that given that possible beneﬁts of clinical LLMs\n(including expanding access to care), it will be important for theﬁeld to\nadopt a commonsense approach to evaluation. While rigorous evaluation is\nimportant, the comparison conditions on which these evaluations are based\nshould reﬂect real-world risk and efﬁcacy rates, and perhaps employ a\ngraded hierarchy with which to classify risk and error (i.e., missing a\nmention of suicidality is unacceptable, but getting a patient’sp a r t n e r’sn a m e\nwrong is nonideal but tolerable), rather than holding clinical LLM appli-\ncations to a standard of perfection which humans do not achieve. Fur-\nthermore, developers will need to strike the appropriate balance of\nprioritizing constructs in a manner expected to be most clinically beneﬁcial,\nfor example, if exposure therapy is indicated for the patient, but the patient\ndoes notﬁnd this approach acceptable, the clinical LLM could recommend\nthe intervention prioritizing effectiveness before offering second-line\ninterventions which may be more acceptable.\nInvolve interdisciplinary collaboration\nInterdisciplinary collaboration between clinical scientists, engineers, and\ntechnologists will be crucial in the development of clinical LLMs. While it is\nplausible that engineers and technologists could use available therapeutic\nmanuals to develop clinical LLMs without the expertise of a behavioral\nh e a l t he x p e r t ,t h i si si l l - a d v i s e d .M a n u a l sa r eo n l yaﬁrst step towards\nlearning a speciﬁc intervention, as they do not provide guidance on how the\nintervention can be applied to speciﬁc individuals or presentations, or how\nto handle speciﬁc issues or concerns that may arise through the course of\ntreatment.\nClinicians and clinician-scientists have expertise that bears on these\nissues, as well as many other aspects of the clinical LLM development\nprocess. Their involvement could include a) testing new applications to\nidentify limitations andrisks and optimize their integration into clinical\npractice, b) improving the ability of applications to adequately address the\ncomplexity of psychological phenomena, c) ensuring that applications are\ndeveloped and implemented in an ethical manner, and d) testing and\nensuring that applications don’t have iatrogenic effects, such as reinforcing\nbehaviors that perpetuate psychopathology or distress.\nBehavioral health experts could also provide guidance on how best to\nﬁnetune or tailor models, including addressing the question of whether and\nhow real patient data should be used for these purposes. For example, most\nproximately, behavioral health experts might assist inprompt engineering,o r\nthe designing and testing of a series of prompts which provide the LLM\nframing and context for delivering a speciﬁc type of treatment or clinical skill\n(e.g.,“Use cognitive restructuring to help the patient evaluate and reappraise\nnegative thoughts in depression”), or a desired clinical task, such as evalu-\nating therapy sessions forﬁdelity (e.g.,“Analyze this psychotherapy tran-\nscript and select sections in which the therapist demonstrated the\nparticularly skillful use of CBT skills, and sections in which the therapist’s\ndelivery of CBT skills could be improved”). Similarly, infew-shot learning,\nbehavioral health experts could be involved in crafting example exchanges\nwhich are added to prompts. For example, treatment modality experts\nmight generate examples of clinical skills (e.g., high-quality examples of\nusing cognitive restructuring to address depression) or of a clinical task (e.g.,\nexamples of both high- and low-quality delivery of CBT skills). Forﬁne-\ntuning, in which a large, labeled dataset is used to train the LLM, and\nreinforcement learning from human feedback(RLHF), in which a human-\nlabeled dataset is used to train a smaller model which is then used for LLM\n“self-training,” behavioral health experts could build and curate (and ensure\ninformed patient consent for use of) appropriate datasets (e.g., a dataset\ncontaining psychotherapy transcripts rated forﬁdelity to an evidence-based\npsychotherapy). The expertise that behavioral health experts could draw on\nto generate instructive examples andcurate high-quality datasets holds\nparticular value in light of recent evidence thatquality of data trumps\nquantityof data for training well-performing models\n66.\nIn the service of facilitating interdisciplinary collaboration, it would\nbeneﬁt clinical scientists to seek out a working knowledge about LLMs, while\nit would beneﬁt technologists to develop a working knowledge of therapy in\ngeneral and EBPs in particular. Dedicated venues that bring together\nbehavioral health experts and clinical psychologists for interdisciplinary\ncollaboration and communication willaid in these efforts. Historically,\nvenues of this type have included psychology-focused workshops at NLP\nconferences (e.g., the Workshop on Computational Linguistics and Clinical\nPsychology [CLPsych], held at the Annual Conference of the North\nAmerican Chapter of the Association for Computational Linguistics\n[NAACL]) and technology-focused conferences or workgroups hosted by\npsychological organizations (e.g., APA’s Technology, Mind & Society\nconference; Association for Behavioral and Cognitive Therapies’ [ABCT]\nTechnology and Behavior Change special interest group). This work has\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 7\nalso been done at nonproﬁts centered on technological tools for mental\nhealth (e.g., the Society for Digital Mental Health). Beyond these venues, it\nmay be fruitful to develop a gatheringthat brings together technologists,\nclinical scientists, and industry partners with a dedicated focus on AI/LLMs,\nwhich could routinely publish on its efforts, akin to the efforts of the World\nHealth Organization’s Infodemic Management Conference, which has\nemployed this approach to address misinformation\n67.F i n a l l y ,g i v e nt h e\nnumerous applications of AI to behavioral health, it is conceivable that a new\n“computational behavioral health” subﬁeld could emerge, offering specia-\nlized training that would bridge the gap between these two domains.\nFocus on trust and usability for clinicians and patients\nIt is important to engage therapists, policymakers, end-users, and experts in\nhuman-computer interactions to understand and improve levels of trust\nthat will be necessary for successful and effective implementation. With\nrespect to applications of AI to augment supervision and support for psy-\nchotherapy, therapists have expressed concern about privacy, the ability to\ndetect subtle non-verbal cues and cultural responsiveness, and the impact on\ntherapist conﬁdence, but they also see beneﬁts for training and professional\ngrowth\n68. Other research suggests that while therapists believe AI can\nincrease access to care, allow individuals to disclose embarrassing infor-\nmation more comfortably, continuously reﬁne therapeutic techniques69,\nthey have concerns about privacy and the formation of a strong therapeutic\nbond with machine-based therapeutic interventions70. Involvement of\nindividuals who will be referring their patients and using LLMs in their own\npractice will be essential to developing solutions they can trust and imple-\nment, and to make sure these solutionshave the features that support trust\nand usability (simple interfaces , accurate summaries of AI-patient\ninteractions, etc.).\nRegarding how much patients will trust the AI systems, following the\nstages we outlined in Fig.3, initial AI-patient interactions will continue to be\nsupervised by clinicians, and the therapeutic bond between the clinician and\nthe patient will continue to be the primary relationship. During this stage, it\nis important that clinicians talk to the patients about their experience with\nthe LLMs, and that theﬁeld as a whole begins to accumulate an under-\nstanding and data on how acceptable interfacing with LLMs is for what kind\nof patient for what kind of clinical use case, in how clinicians can scaffold the\npatient-LLM relationship. This data will be critical for developing colla-\nborative LLM applications that havemore autonomy, and for ensuring that\nthe transition from assistive to collaborative stage applications is not asso-\nciated with large unforeseen risk.For example, in the case of CBT for\ninsomnia, once an assistive AI system has been iterated on to reliably collect\ninformation about patients’ sleep patterns, it is more conceivable that it\ncould be evolved into a collaborativeAI system that does a comprehensive\ninsomnia assessment (i.e., it also collects and interprets data on patients’\nclinically signiﬁcant distress, impairment of functioning, and ruling out of\nsleep-wake disorders, like narcolepsy)\n71.\nDesign criteria for effective clinical LLMs\nBelow, we propose an initial set of desirable design qualities for\nclinical LLMs.\nDetect risk of harm.\na. Accurate risk detection and mandated reporting are crucial aspects that\nclinical LLMs must prioritize, particularly in the identiﬁcation of sui-\ncidal/homicidal ideation, child/elder abuse, and intimate partner vio-\nlence. Algorithms for detecting risks are under development\n4.O n e\nthreat to risk detection is that current LLMs have limited context\nwindows, meaning they only“remember” a limited amount of user\ninput. Functionally, this means a clinical LLM application could\n“forget” crucial details about a patient, which could impact safety (e.g.,\nan application “forgetting” that the patient owns ﬁrearms would\nthreaten its ability to properly assess and intervene around suicide risk).\nHowever, context windows have been rapidly expanding with each\nsubsequent model release, so this issue may not be a problem for long.\nIn addition, it is already possible to augment the memory of LLMs with\n“vector databases,” which would have the added beneﬁt of retaining\ninspectable learnings and summaries across clinical encounters\n72.\nIn the future, and especially given much larger context windows,\nclinical LLMs could prompt clinicians with ethical guidelines, legal\nrequirements (e.g., the Tarasoff rule, which requires clinicians to warn\nintended victims when a patient presents a serious threat of violence), or\nevidence-based methods for decreasing risk (e.g., safety planning\n73), or even\nprovide interventions targeting risk directly to patients. This type of risk\nmonitoring and intervention could be particularly useful in supplementing\nexisting healthcare systems during gaps in clinician coverage like nights and\nweekends\n4.\nb) Be“Healthy.” There is growing concern that AI chat systems can\ndemonstrate undesirable behaviors, including expressions akin to depres-\nsion or narcissism35,74. Such poorly understood, undesirable behaviors risk\nharming already vulnerable patients or interfering with their ability to\nbeneﬁt from treatment. Clinical LLM applications will need training,\nmonitoring, auditing, and guardrails to prevent the expression of undesir-\nable behaviors and maintain healthy interactions with users. These efforts\nwill need to be continually evaluated and updated to prevent or address the\nemergence of new undesirable or clinically contraindicated behavior.\nAid in psychodiagnostic assessment. Clinical LLMs ought to integrate\npsychodiagnostic assessment and diagnosis, facilitating intervention\nselection and outcome monitoring\n75. Recent developments show promise\nfor LLMs in the assessment realm76. Down the line, LLMs could be used\nfor diagnostic interviewing (e.g., Structured Clinical Interview for the\nDSM-5\n77) using chatbots or voice interfaces. Prioritizing assessment\nenhances diagnostic accuracy and ensures appropriate intervention,\nreducing the risk of harmful interventions\n63.\nBe responsive andﬂexible. Given the frequency with which ambiva-\nlence and poor patient engagement arise in clinical encounters, clinical\nLLMs which use evidence-based and patient-centered methods for\nhandling these issues (e.g., motivational enhancement techniques, shared\ndecision making), and have options for second-line interventions for\npatients not interested in gold-standard treatments, will have the best\nchance of success.\nStop when not helping or conﬁdent. Psychologists are ethically obli-\ngated to cease treatment and offer appropriate referrals to the patient if\nthe current course of treatment has not helped or likely will not help.\nClinical LLMs can abide by this ethical standard by drawing on integrated\nassessment (discussed above) to assess the appropriateness of the given\nintervention and detect cases that need more specialized or intensive\nintervention.\nBe fair, inclusive, and free from bias. As has been written about\nextensively, LLMs may perpetuate bias, including racism, sexism, and\nhomophobia, given that they are trained on existing text\n36. These biases\ncan contribute to both error disparities– where models are less accurate\nfor particular groups– or outcome disparities– where models tend to\nover-capture demographic information78 – which would in turn con-\ntribute to the disparities in mental health status and care already\nexperienced by minoritized groups\n79. The integration of bias counter-\nmeasures into clinical LLM applications could serve to prevent this78,80.\nBe empathetic–to an extent . Clinical LLMs will likely need to\ndemonstrate empathy and build the therapeutic alliance in order to\nengage patients. Other skills used by therapists include humor, irre-\nverence, and gentle methods of challenging the patient. Incorporating\nthese into clinical LLMs might be beneﬁcial, as appropriate human\nlikeness may facilitate engagement and interaction with AI\n81. However,\nthis needs to be balanced against associated risks, mentioned above, of\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 8\nincorporating human likeness in systems36. Whether and how much\nhuman likeness is necessary for a psychological intervention remains a\nquestion for future empirical work.\nBe transparent about being AIs. Mental illness and mental health care is\nalready stigmatized, and the application of LLMs without transparent\nconsent can erode patient/consumer trust, which reduces trust in the\nbehavioral health profession more generally. Some mental health startups\nhave already faced criticism for employing generative AI in applications\nwithout disclosing this information to the end user\n2. As laid out in the White\nHouse Blueprint for an AI Bill of Rights, AI applications should be explicitly\n(and perhaps repeatedly/consistently) labeled as such to allow patients and\nconsumers to“know that an automated system is being used and under-\nstand how and why it contributes to outcomes that impact them”\n82.\nDiscussion\nUnintended consequences may change the clinical profession\nThe development of clinical LLM applications could lead to unintended\nconsequences, such as changes to the structure of and compensation for\nmental health services. AI may permit increased staf ﬁng by non-\nprofessionals or paraprofessionals, causing professional clinicians to\nsupervise large numbers of non-professionals or even semi-autonomous\nLLM systems. This could reduce clinicians’direct patient contact and per-\nhaps increase their exposure to challenging or complicated cases not suitable\nfor the LLM, which may lead to burnout and make clinical jobs less\nattractive. To address this, research could determine the appropriate\nnumber of cases for a clinician to oversee safely and guidelines could be\npublished to disseminate theseﬁndings. The 24-hour availability of LLM-\nbased intervention may also change consumer expectations of psy-\nchotherapy in a way that is at odds with many of the norms of psy-\nchotherapy practice (e.g., waiting for a session to discuss stressors, limited or\nemergency-only contact between sessions).\nLLMs could pave the way for a next generation of clinical science\nBeyond the imminent applications described in this paper, it is worth\nconsidering how the long-term applications of clinical LLMs might also\nfacilitate signiﬁcant advances in clinical care and clinical science.\nClinical practice. In terms of their effects on therapeutic interventions\nthemselves, clinical LLMs might promote advances in theﬁeld by allowing\nfor the pooling of data on what works with the most difﬁcult cases, perhaps\nthrough the use of practice research networks83. At the level of health sys-\ntems, they could expedite the implementation and translation of research\nﬁndings into clinical practice by suggesting therapeutic strategies to psy-\nchotherapists, for instance, promoting strategies that enhance inhibitory\nlearning during exposure therapy\n84. Lastly, clinical LLMs could increase\naccess to care if LLM-based psychotherapy chatbots are offered as low\nintensity, low-cost options in stepped-care models, similar to the existing\nprovision of computerized CBT and guided self-help\n85.\nAs the utilization of clinical LLMs expands, there may be a shift\ntowards psychologists and other behavioral health experts operating at the\ntop of their degree. Presently, a signiﬁcant amount of clinician time is\nconsumed by administrative tasks, chart review, and documentation. The\nshifting of responsibilities afforded by the automation of certain aspects of\npsychotherapy by clinical LLMs could allow clinicians to pursue leadership\nroles, contribute to the development, evaluation, and implementation of\nLLM-based care, or lead policy efforts, or simply to devote more time to\ndirect patient care.\nClinical science. By facilitating supervision, consultation, andﬁdelity\nmeasurement, LLMs could expedite psychotherapist training and\nincrease the capacity of study supervisors, thus making psychotherapy\nresearch less expensive and more efﬁcient.\nIn a world in which fully autonomous LLM applications screen and\nassess patients, deliver high-ﬁdelity, protocolized psychotherapy, and collect\noutcome measurements, psychotherapy clinical trials would be limited\nlargely by the number of willing participants eligible for the study, rather\nthan by the resources required to screen, assess, treat, and follow these\nparticipants. This could open the door to unprecedentedly large-N clinical\ntrials. This would allow for well-powered, sophisticated dismantling studies\nto support the search for mechanisms of change in psychotherapy, which\nare currently only possible using individual participant level meta-analysis\n(for example, see ref.86). Ultimately, such insights into causal mechanisms\nof change in psychotherapy could help to reﬁne these treatments and\npotentially improve their efﬁcacy.\nFinally, the emergence of LLM treatment modalities will challenge (or\nconﬁrm) fundamental assumptions about psychotherapy. Does therapeutic\n(human) alliance account for a majority of the variance in patient change?\nTo what extent can an alliance be formed with a technological agent? Is\nlasting and meaningful therapeutic change only possible through working\nwith a human therapist? LLMs hold the promise of empirical answers to\nthese questions.\nIn summary, large language models hold promise for supporting,\naugmenting, or even in some cases replacing human-led psychotherapy,\nwhich may improve the quality, accessibility, consistency, and scalability of\ntherapeutic interventions and clinical science research. However, LLMs are\nadvancing quickly and will soon be deployed in the clinical domain, with\nlittle oversight or understanding ofharms that they may produce. While\ncautious optimism about clinical LLM applications is warranted, it is also\ncrucial for psychologists to approach the integration of LLMs into psy-\nchotherapy with caution and to educate the public about the potential risks\nand limitations of using these technologies for therapeutic purposes. Fur-\nthermore, clinical psychologists ought to actively engage with the technol-\nogists building these solutions. As theﬁeld of AI continues to evolve, it is\nessential that researchers and clinicians closely monitor the use of LLMs in\npsychotherapy and advocate for responsible and ethical use to protect the\nwellbeing of patients.\nData availability\nData sharing not applicable to this article as no datasets were generated or\nanalyzed during the current study.\nReceived: 24 July 2023; Accepted: 30 January 2024;\nReferences\n1. Bubeck, S. et al. Sparks of artiﬁcial general intelligence: Early experiments\nwith GPT-4. Preprint athttp://arxiv.org/abs/2303.12712(2023).\n2. Broderick, R. People are using AI for therapy, whether the tech is ready\nfor it or not.Fast Company(2023).\n3. Weizenbaum, J. ELIZA—a computer program for the study of natural\nlanguage communication between man and machine.Commun. ACM\n9,3 6–45 (1966).\n4. Bantilan, N., Malgaroli, M., Ray, B. & Hull, T. D. Just in time crisis\nresponse: Suicide alert system for telemedicine psychotherapy\nsettings. Psychother. Res.31, 289–299 (2021).\n5. Peretz, G., Taylor, C. B., Ruzek, J. I., Jefroykin, S. & Sadeh-Sharvit, S.\nMachine learning model to predict assignment of therapy homework\nin behavioral treatments: Algorithm development and validation.JMIR\nForm. Res.7, e45156 (2023).\n6. Tanana, M. J. et al. How do you feel? Using natural language\nprocessing to automatically rate emotion in psychotherapy.Behav.\nRes. Methods53, 2069–2082 (2021).\n7. Sharma, A., Lin, I. W., Miner, A. S., Atkins, D. C. & Althoff, T. Human–AI\ncollaboration enables more empathic conversations in text-based\npeer-to-peer mental health support.Nat. Mach. Intell. 5,4 6–57 (2023).\n8 . C h e n ,Z . ,F l e m o t o m o s ,N . ,I m e l ,Z .E . ,A t k i n s ,D .C .&N a r a y a n a n ,S .\nLeveraging open data and task augmentation to automated behavioral\ncoding of psychotherapy conversations in low-resource scenarios.\nPreprint athttps://doi.org/10.48550/arXiv.2210.14254(2022).\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 9\n9. Shah, R. S. et al. Modeling motivational interviewing strategies on an\nonline peer-to-peer counseling platform.Proc. ACM Hum.-Comput.\nInteract 6,1 –24 (2022).\n10. Chan, W. W. et al. The challenges in designing a prevention chatbot for\neating disorders: Observational study.JMIR Form. Res.6,\ne28003 (2022).\n11. Darcy, A. Why generative AI Is not yet ready for mental healthcare.\nWoebot Healthhttps://woebothealth.com/why-generative-ai-is-not-\nyet-ready-for-mental-healthcare/ (2023).\n12. Abd-Alrazaq, A. A. et al. An overview of the features of chatbots in\nmental health: A scoping review.Int. J. Med. Inf.132, 103978 (2019).\n13. Lim, S. M., Shiau, C. W. C., Cheng, L. J. & Lau, Y. Chatbot-delivered\npsychotherapy for adults with depressive and anxiety symptoms: A\nsystematic review and meta-regression.Behav. Ther.53,\n334–347 (2022).\n14. Baumel, A., Muench, F., Edan, S. & Kane, J. M. Objective user\nengagement with mental health apps: Systematic search and panel-\nbased usage analysis.J. Med. Internet Res.21, e14567 (2019).\n15. Torous, J., Nicholas, J., Larsen, M. E., Firth, J. & Christensen, H.\nClinical review of user engagement with mental health smartphone\napps: Evidence, theory and improvements.Evid. Based Ment. Health\n21, 116–119 (2018b).\n16. Das, A. et al. Conversational bots for psychotherapy: A study of\ngenerative transformer models using domain-speciﬁc dialogues. in\nProceedings of the 21st Workshop on Biomedical Language\nProcessing 285–297 (Association for Computational Linguistics,\n2022). https://doi.org/10.18653/v1/2022.bionlp-1.27.\n17. Liu, H. Towards automated psychotherapy via language modeling.\nPreprint athttp://arxiv.org/abs/2104.10661 (2021).\n18. Hamilton, J. Why generative AI (LLM) is ready for mental healthcare.\nLinkedIn https://www.linkedin.com/pulse/why-generative-ai-\nchatgpt-ready-mental-healthcare-jose-hamilton-md/ (2023).\n19. Shariff, A., Bonnefon, J.-F. & Rahwan, I. Psychological roadblocks to\nthe adoption of self-driving vehicles.Nat. Hum. Behav.1,\n694–696 (2017).\n2 0 . M a r k o v ,A .A .E s s a id’une recherche statistique sur le texte du\nroman “Eugene Onegin” illustrant la liaison des epreuve en chain\n(‘Example of a statistical investigation of the text of“Eugene\nOnegin” illustrating the dependence between samples in chain’).\nIzvistia Imperatorskoi Akad. Nauk Bull. L’Academie Imp. Sci. St-\nPetersbourg 7, 153–162 (1913).\n21. Shannon, C. E. A mathematical theory of communication.Bell Syst.\nTech. J.\n27, 379–423 (1948).\n22. Baker, J. K. Stochastic modeling for automatic speech\nunderstanding. in Speech recognition: invited papers presented at\nthe 1974 IEEE symposium(ed. Reddy, D. R.) (Academic\nPress, 1975).\n23. Jelinek, F. Continuous speech recognition by statistical methods.\nProc. IEEE64, 532–556 (1976).\n24. Jurafsky, D. & Martin, J. H. N-gram language models. inSpeech and\nlanguage processing: An introduction to natural language processing,\ncomputational linguistics, and speech recognition(Pearson Prentice\nHall, 2009).\n25. Vaswani, A. et al. Attention is all you need.31st Conf. Neural Inf.\nProcess. Syst. (2017).\n26. Bommasani, R. et al. On the opportunities and risks of foundation\nmodels. Preprint athttp://arxiv.org/abs/2108.07258 (2022).\n27. Gao, L. et al. The Pile: An 800GB dataset of diverse text for language\nmodeling. Preprint athttp://arxiv.org/abs/2101.00027 (2020).\n28. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof deep bidirectional transformers for language understanding.\nPreprint athttp://arxiv.org/abs/1810.04805 (2019).\n29. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large\nlanguage models are zero-shot reasoners. Preprint at\nhttp://arxiv.org/abs/2205.11916 (2023).\n30. Fairburn, C. G. & Patel, V. The impact of digital technology on\npsychological treatments and their dissemination.Behav. Res. Ther.\n88,1 9–25 (2017).\n31. Fisher, A. J. et al. Open trial of a personalized modular treatment for\nmood and anxiety.Behav. Res. Ther.116,6 9–79 (2019).\n32. Fan, X. et al. Utilization of self-diagnosis health chatbots in real-world\nsettings: Case study.J. Med. Internet Res.23, e19928 (2021).\n33. Coghlan, S. et al. To chat or bot to chat: Ethical issues with using\nchatbots in mental health.Digit. Health9,1 –11 (2023).\n34. Beatty, C., Malik, T., Meheli, S. & Sinha, C. Evaluating the therapeutic\nalliance with a free-text CBT conversational agent (Wysa): A mixed-\nmethods study.Front. Digit. Health4, 847991 (2022).\n35. Lin, B., Bouneffouf, D., Cecchi, G. & Varshney, K. R. Towards healthy\nAI: Large language models need therapists too. Preprint at\nhttp://arxiv.org/abs/2304.00416 (2023).\n36. Weidinger, L. et al. Ethical and social risks of harm from language\nmodels. Preprint athttp://arxiv.org/abs/2112.04359 (2021).\n37. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the\ndangers of stochastic parrots: Can language models be too big? In\nProceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency610–623 (ACM, 2021).\nhttps://doi.org/10.1145/3442188.3445922.\n38. Chamberlain, J. The risk-based approach of the European Union’s\nproposed artiﬁcial intelligence regulation: Some comments from a tort\nlaw perspective.Eur. J. Risk Regul.14,1 –13 (2023).\n39. Norden, J. G. & Shah, N. R. What AI in health care can learn from the\nlong road to autonomous vehicles.NEJM Catal. Innov. Care Deliv\n.\nhttps://doi.org/10.1056/CAT.21.0458 (2022).\n40. Sedlakova, J. & Trachsel, M. Conversational artiﬁcial intelligence in\npsychotherapy: A new therapeutic tool or agent?Am. J. Bioeth.23,\n4–13 (2023).\n41. Gearing, R. E. et al. Major ingredients ofﬁdelity: A review and scientiﬁc\nguide to improving quality of intervention research implementation.\nClin. Psychol. Rev.31,7 9–88 (2011).\n42. Wiltsey Stirman, S. Implementing evidence-based mental-health\ntreatments: Attending to training,ﬁdelity, adaptation, and context.\nCurr. Dir. Psychol. Sci.31, 436–442 (2022).\n43. Waller, G. Evidence-based treatment and therapist drift.Behav. Res.\nTher. 47, 119–127 (2009).\n44. Flemotomos, N. et al.“Am I a good therapist?” Automated evaluation\nof psychotherapy skills using speech and language technologies.\nCoRR, Abs, 2102 (10.3758) (2021).\n45. Zhang, X. et al. You never know what you are going to get: Large-scale\nassessment of therapists’ supportive counseling skill use.\nPsychotherapy https://doi.org/10.1037/pst0000460 (2022).\n46. Goldberg, S. B. et al. Machine learning and natural language\nprocessing in psychotherapy research: Alliance as example use case.\nJ. Couns. Psychol.67, 438–448 (2020).\n47. Wiltsey Stirman, S. et al. A novel approach to the assessment of\nﬁdelity to a cognitive behavioral therapy for PTSD using clinical\nworksheets: A proof of concept with cognitive processing therapy.\nBehav. Ther.52, 656–672 (2021).\n48. Raviola, G., Naslund, J. A., Smith, S. L. & Patel, V. Innovative models in\nmental health delivery systems: Task sharing care with non-specialist\nproviders to close the mental health treatment gap.Curr. Psychiatry\nRep. 21, 44 (2019).\n49. American Psychological Association. Guidelines for clinical supervision\nin health service psychology.Am. Psychol.70,3 3–46 (2015).\n50. Cook, S. C., Schwartz, A. C. & Kaslow, N. J. Evidence-based\npsychotherapy: Advantages and challenges.Neurotherapeutics 14,\n537–545 (2017).\n51. Leichsenring, F., Steinert, C., Rabung, S. & Ioannidis, J. P. A. The\nefﬁcacy of psychotherapies and pharmacotherapies for mental\ndisorders in adults: An umbrella review and meta‐analytic evaluation\nof recent meta‐analyses. World Psych.21, 133–\n145 (2022).\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 10\n52. Cuijpers, P., van Straten, A., Andersson, G. & van Oppen, P.\nPsychotherapy for depression in adults: A meta-analysis of comparative\noutcome studies.J. Consult. Clin. Psychol.76,9 0 9–922 (2008).\n53. Morris, Z. S., Wooding, S. & Grant, J. The answer is 17 years, what is\nthe question: Understanding time lags in translational research.J. R.\nSoc. Med.104, 510–520 (2011).\n54. Chekroud, A. M. et al. The promise of machine learning in predicting\ntreatment outcomes in psychiatry.World Psych.20, 154–170 (2021).\n55. Kazdin, A. E. Mediators and mechanisms of change in psychotherapy\nresearch. Annu. Rev. Clin. Psychol.3,1 –27 (2007).\n56. Angelov, P. P., Soares, E. A., Jiang, R., Arnold, N. I. & Atkinson, P. M.\nExplainable artiﬁcial intelligence: An analytical review.WIREs Data\nMin. Knowl. Discov. 11, (2021).\n57. Kelley, T. L.Interpretation of Educational Measurements. (World\nBook, 1927).\n58. van Bronswijk, S. C. et al. Precision medicine for long-term depression\noutcomes using the Personalized Advantage Index approach:\nCognitive therapy or interpersonal psychotherapy?Psychol. Med.51,\n279–289 (2021).\n59. Scala, J. J., Ganz, A. B. & Snyder, M. P. Precision medicine\napproaches to mental health care.Physiology 38,8 2–98 (2023).\n60. Chorpita, B. F., Daleiden, E. L. & Weisz, J. R. Identifying and selecting\nthe common elements of evidence based interventions: A distillation\nand matching model.Ment. Health Serv. Res.7,5 –20 (2005).\n61. Chambless, D. L. & Hollon, S. D. Deﬁning empirically supported\ntherapies. J. Consult. Clin. Psychol.66,7 –18 (1998).\n62. Tolin, D. F., McKay, D., Forman, E. M., Klonsky, E. D. & Thombs, B. D.\nEmpirically supported treatment: Recommendations for a new model.\nClin. Psychol. Sci. Pract.22, 317–338 (2015).\n63. Lilienfeld, S. O. Psychological treatments that cause harm.Perspect.\nPsychol. Sci.2,5 3–70 (2007).\n64. Wasil, A. R., Venturo-Conerly, K. E., Shingleton, R. M. & Weisz, J. R. A\nreview of popular smartphone apps for depression and anxiety:\nAssessing the inclusion of evidence-based content.Behav. Res. Ther.\n123, 103498 (2019).\n65. Torous, J. B. et al. A hierarchical framework for evaluation and\ninformed decision making regarding smartphone apps for clinical\ncare. Psychiatr. Serv.69, 498–500 (2018).\n66. Gunasekar, S. et al. Textbooks are all you need. Preprint at\nhttp://arxiv.org/abs/2306.11644\n(2023).\n67. Wilhelm, E. et al. Measuring the burden of infodemics: Summary of the\nmethods and results of the Fifth WHO Infodemic Management\nConference. JMIR Infodemiology3, e44207 (2023).\n68. Creed, T. A. et al. Knowledge and attitudes toward an artiﬁcial\nintelligence-based ﬁdelity measurement in community cognitive\nbehavioral therapy supervision.Adm. Policy Ment. Health Ment.\nHealth Serv. Res.49, 343–356 (2022).\n69. Aktan, M. E., Turhan, Z. & Dolu,İ. Attitudes and perspectives towards\nthe preferences for artiﬁcial intelligence in psychotherapy.Comput.\nHum. Behav.133, 107273 (2022).\n70. Prescott, J. & Hanley, T. Therapists’ attitudes towards the use of AI in\ntherapeutic practice: considering the therapeutic alliance.Ment.\nHealth Soc. Incl.27, 177–185 (2023).\n71. American Psychiatric Association.Diagnostic and Statistical Manual\nof Mental Disorders. (2013).\n72. Yogatama, D., De Masson d’Autume, C. & Kong, L. Adaptive\nsemiparametric language models.Trans. Assoc. Comput. Linguist9,\n362–373 (2021).\n73. Stanley, B. & Brown, G. K. Safety planning intervention: A brief\nintervention to mitigate suicide risk.Cogn. Behav. Pract.19,\n256–264 (2012).\n74. Behzadan, V., Munir, A. & Yampolskiy, R. V. A psychopathological\napproach to safety engineering in AI and AGI. Preprint at\nhttp://arxiv.org/abs/1805.08915 (2018).\n75. Lambert, M. J. & Harmon, K. L. The merits of implementing routine\noutcome monitoring in clinical practice.Clin. Psychol. Sci. Pract.\n25, (2018).\n76. Kjell, O. N. E., Kjell, K. & Schwartz, H. A. AI-based large language\nmodels are ready to transform psychological health assessment.\nPreprint athttps://doi.org/10.31234/osf.io/yfd8g (2023).\n77. First, M. B., Williams, J. B. W., Karg, R. S. & Spitzer, R. L.SCID-5-CV:\nStructured Clinical Interview for DSM-5 Disorders: Clinician Version.\n(American Psychiatric Association Publishing, 2016).\n78. Shah, D. S., Schwartz, H. A. & Hovy, D. Predictive biases in natural\nlanguage processing models: A conceptual framework and overview.\nin Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics5248–5264 (Association for Computational\nLinguistics, 2020).https://doi.org/10.18653/v1/2020.acl-main.468.\n79. Adams, L. M. & Miller, A. B. Mechanisms of mental-health disparities\namong minoritized groups: How well are the top journals in clinical\npsychology representing this work?Clin. Psychol. Sci.10,\n387–416 (2022).\n80. Viswanath, H. & Zhang, T. FairPy: A toolkit for evaluation of social\nbiases and their mitigation in large language models. Preprint at\nhttp://arxiv.org/abs/2302.05508 (2023).\n8 1 . v o nZ i t z e w i t z ,J . ,B o e s c h ,P .M . ,W o l f ,P .&R i e n e r ,R .Q u a n t i f y i n gt h e\nhuman likeness of a humanoid robot.Int. J. Soc. Robot.5,2 6 3–276 (2013).\n82. White House Ofﬁce of Science and Technology Policy. Blueprint for\nan AI bill of rights. (2022).\n83. Parry, G., Castonguay, L. G., Borkovec, T. D. & Wolf, A. W. Practice\nresearch networks and psychological services research in the UK and\nUSA. inDeveloping and Delivering Practice-Based Evidence(eds.\nBarkham, M., Hardy, G. E. & Mellor-Clark, J.) 311–325 (Wiley-\nBlackwell, 2010).https://doi.org/10.1002/9780470687994.ch12.\n84. Craske, M. G., Treanor, M., Conway, C. C., Zbozinek, T. & Vervliet, B.\nMaximizing exposure therapy: An inhibitory learning approach.\nBehav. Res. Ther.58,1 0–23 (2014).\n85. Delgadillo, J. et al. Stratiﬁed care vs stepped care for depression: A\ncluster randomized clinical trial.JAMA Psychiatry79, 101 (2022).\n86. Furukawa, T. A. et al. Dismantling, optimising, and personalising\ninternet cognitive behavioural therapy for depression: A systematic\nreview and component network meta-analysis using individual\nparticipant data.Lancet Psychiatry8, 500–511 (2021).\nAcknowledgements\nThis work was supported by the National Institute of Mental Health under\naward numbers R01-MH125702 (PI: H.A.S) and RF1-MH128785 (PI:\nS.W.S.), and by the Institute for Human-Centered A.I. at Stanford University\nto J.C.E. The authors are grateful to Adam S. Miner and Victor Gomes who\nprovided critical feedback on an earlier version of this manuscript.\nAuthor contributions\nE.C.S., S.W.S., C.L.B., and J.C.E. wrote the main manuscript text. E.C.S.,\nL.H.U., and J.C.E. prepared theﬁgures. All authors reviewed the manuscript.\nCompeting interests\nThe authors declare the following competing interests: receiving\nconsultation fees from Jimini Health (E.C.S., L.H.U., H.A.S., and J.C.E.).\nAdditional information\nCorrespondenceand requests for materials should be addressed to\nElizabeth C. Stade or Johannes C. Eichstaedt.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 11\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2024\nhttps://doi.org/10.1038/s44184-024-00056-z Article\nnpj Mental Health Research|            (2024) 3:12 12",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.530867338180542
    },
    {
      "name": "Psychology",
      "score": 0.45408159494400024
    },
    {
      "name": "Computer science",
      "score": 0.36908167600631714
    },
    {
      "name": "Cognitive science",
      "score": 0.33889755606651306
    },
    {
      "name": "Data science",
      "score": 0.32419195771217346
    },
    {
      "name": "Political science",
      "score": 0.24083212018013
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210103836",
      "name": "National Center for PTSD",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204866599",
      "name": "VA Palo Alto Health Care System",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2799853436",
      "name": "Johns Hopkins Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}