{
    "title": "Transformer Neural Network for Structure Constrained Molecular Optimization",
    "url": "https://openalex.org/W3154464317",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5089237271",
            "name": "Jiazhen He",
            "affiliations": [
                "AstraZeneca (Brazil)"
            ]
        },
        {
            "id": "https://openalex.org/A5006474268",
            "name": "Felix Mattsson",
            "affiliations": [
                "AstraZeneca (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A5080007919",
            "name": "Marcus Forsberg",
            "affiliations": [
                "AstraZeneca (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A5046421751",
            "name": "Esben Jannik Bjerrum",
            "affiliations": [
                "AstraZeneca (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A5076975589",
            "name": "Ola Engkvist",
            "affiliations": [
                "AstraZeneca (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A5077542460",
            "name": "Eva Nittinger",
            "affiliations": [
                "AstraZeneca (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A5051401603",
            "name": "Christian Tyrchan",
            "affiliations": [
                "AstraZeneca (Sweden)"
            ]
        },
        {
            "id": "https://openalex.org/A5080565832",
            "name": "Werngard Czechtizky",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3128880495",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W3113251852",
        "https://openalex.org/W3014962324",
        "https://openalex.org/W4233253307",
        "https://openalex.org/W2322193123",
        "https://openalex.org/W2096541451",
        "https://openalex.org/W2017398555",
        "https://openalex.org/W3100358278",
        "https://openalex.org/W3015399432",
        "https://openalex.org/W1604884792",
        "https://openalex.org/W2087952495",
        "https://openalex.org/W3017005804",
        "https://openalex.org/W2023818227",
        "https://openalex.org/W3031195035",
        "https://openalex.org/W2562677145",
        "https://openalex.org/W2323605167",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W3033658063",
        "https://openalex.org/W4234959353",
        "https://openalex.org/W1977242738",
        "https://openalex.org/W3013187757",
        "https://openalex.org/W4230007416",
        "https://openalex.org/W2902415322",
        "https://openalex.org/W3005111505",
        "https://openalex.org/W2981104283"
    ],
    "abstract": "Finding molecules with a desirable balance of multiple properties is a main challenge in drug discovery. Here, we focus on the task of molecular optimization, where a starting molecule with promising properties needs to be further optimized towards the desirable properties. Typically, chemists would apply chemical transformations to the starting molecule based on their intuition. A widely used strategy is the concept of matched molecular pairs where two molecules differ by a single transformation. In particular, a chemist would be interested in keeping one part of the starting molecule (core) constant, while substituting the other part (R-group), to optimize the starting molecule towards desirable properties. Motivated by this, we train a Transformer model, Transformer-R, to generate R-groups given the starting molecule (with its core and R-group specified) and the specified desirable properties. The generated R-groups will be attached to the core to form the final molecules, which are guaranteed to keep the core of interest and are expected to satisfy the desirable properties in the input. Our model could accelerate the process of optimizing antiviral drug candidates in terms of various properties of interest, e.g. pharmacokinetics.",
    "full_text": "Published as a conference paper at ICLR 2021\nTRANSFORMER NEURAL NETWORK FOR STRUCTURE\nCONSTRAINED MOLECULAR OPTIMIZATION\nJiazhen He, Felix Mattsson, Marcus Forsberg, Esben J. Bjerrum & Ola Engkvist\nDiscovery Sciences, R&D, AstraZeneca, Gothenburg, Sweden\n{jiazhen.he}@astrazeneca.com\nEva Nittinger, Christian Tyrchan & Werngard Czechtizky\nMedicinal Chemistry, Research and Early Development, Respiratory and Immunology (R&I)\nBioPharmaceuticals R&D, AstraZeneca, Gothenburg, Sweden\nABSTRACT\nFinding molecules with a desirable balance of multiple properties is a main chal-\nlenge in drug discovery. Here, we focus on the task of molecular optimization,\nwhere a starting molecule with promising properties needs to be further optimized\ntowards the desirable properties. Typically, chemists would apply chemical trans-\nformations to the starting molecule based on their intuition. A widely used strat-\negy is the concept of matched molecular pairs where two molecules differ by a\nsingle transformation. In particular, a chemist would be interested in keeping one\npart of the starting molecule (core) constant, while substituting the other part (R-\ngroup), to optimize the starting molecule towards desirable properties. Motivated\nby this, we train a Transformer model, Transformer-R, to generate R-groups given\nthe starting molecule (with its core and R-group speciﬁed) and the speciﬁed de-\nsirable properties. The generated R-groups will be attached to the core to form\nthe ﬁnal molecules, which are guaranteed to keep the core of interest and are ex-\npected to satisfy the desirable properties in the input. Our model could accelerate\nthe process of optimizing antiviral drug candidates in terms of various properties\nof interest, e.g. pharmacokinetics.\n1 I NTRODUCTION\nA main challenge in drug discovery is ﬁnding molecules with desirable properties. A drug requires\na balance of multiple properties,e.g. physicochemical properties, ADMET (absorption, distribution,\nmetabolism, elimination and toxicity) properties, safety and potency against its target. To ﬁnd such a\ndrug in the extremely large chemical space (i.e. 1023-1060) (Polishchuk et al., 2013) is challenging.\nIt is often that a promising molecule needs to be improved to achieve a balance of multiple properties.\nThis problem is known as molecular optimization. It plays an important role in the development of\nantiviral drugs to combat pandemics, where existing drugs can be identiﬁed as lead (Huang et al.,\n2020; Senanayake, 2020; Box & J Thompson, 2020), and chemically modiﬁed to improve speciﬁc\nproperties, e.g. afﬁnity, pharmacology, toxicity and drug resistance proﬁles (Adamson et al., 2021).\nFor example, ivermectin has been reported to show in vitroantiviral activity against SARS-CoV-\n2 (Caly et al., 2020). However, its application is mainly limited by pharmacokinetic problems such\nas high cytotoxicity and low solubility (Sharun et al., 2020; Momekov & Momekova, 2020).\nTraditionally, chemists would use their knowledge, experience and intuition (Topliss, 1972) to ap-\nply some chemical transformations to the promising molecule. In particular, the matched molecular\npair (MMP) analysis (Kenny & Sadowski, 2005; Tyrchan & Evertsson, 2017)—which compares the\nproperties of two molecules that differ only by a single chemical transformation—has been widely\nused as a strategy by medicinal chemists to support molecular optimization (Weber et al., 2013;\nGriffen et al., 2011; Leach et al., 2006). However, similarity, transferability, and linear analogu-\ning (Hansch et al., 1962; Hansch & Fujita, 1964; Free & Wilson, 1964) are typically assumed,\nwhich are not generally true and become more problematic when optimizing multiple properties\nsimultaneously.\n1\nPublished as a conference paper at ICLR 2021\nRecently, deep learning models have been used to learn the transformations involved in molecular\noptimization directly from MMPs. The problem of molecular optimization have been framed as\na machine translation problem (Bahdanau et al., 2015), where an input starting molecule is trans-\nlated into a target molecule with optimized properties. While graph representation was used in Jin\net al. (2018; 2019; 2020), He et al. (2020) trained a Transformer model based on the simpliﬁed\nmolecular-input line-entry system (SMILES) representation. The starting molecule’s SMILES is\nconcatenated with the property constraint tokens as input, and the model outputs the molecule with\noptimized properties. However, the generated molecule is not guaranteed to keep the core of interest\nin the given starting molecule being optimized. Here, we train a Transformer model, Transformer-R,\nwhere the starting molecule is represented by its core (being kept) and its R-group (being replaced),\nand the output is the R-group (used to replace the R-group speciﬁed in the input) instead of the\nwhole molecule. By doing so, the model is enforced to keep the core of interest. The goal is to\ngenerate molecules which (i) have the desirable properties speciﬁed in the input (ii) have small and\nsingle transformation applied to the starting molecule, and (iii) keep the core speciﬁed in the in-\nput. In summary, the model is trained to mimic the concept of MMPs—a common strategy used by\nmedicinal chemists for molecular optimization.\n2 M ETHODS\nThe SMILES representation of molecules (Weininger, 1988), as a string-based representation, is\nused in our study to facilitate the use of the Transformer model from natural language processing\n(NLP). The Transformer is trained on a set of MMPs together with the property changes between\nsource and target molecules. Figure 1 shows an example of a MMP, and the properties of source and\ntarget molecules.\nFollowing He et al. (2020), three ADMET properties, logD, solubility and clearance are optimized\nsimultaneously, and the property constraint tokens are included in the input sequence for guidance.\nFigure 2 shows an example of source and target sequences which are fed into the Transformer\nmodel during training. Different from the Transformer model in He et al. (2020) where the source\nmolecule is represented by its SMILES, here it is represented by its core’s SMILES and its R-group’s\nSMILES, separated by the separator token. Instead of generating the whole molecule, the R-group\nis generated, which will be attached to the core in the input to form the ﬁnal molecule.\nFigure 1: An example of a matched molecular pair and the property changes between the molecules.\nGiven a set of MMPs {(X,Y,Z )}where X represents source molecule, Y represents target\nmolecule, and Zrepresents the property change between source moleculeXand target molecule Y,\nthe Transformer will learn a mapping(Xcore,XR-group,Z) ∈Xcore ×XR-group ×Z→ YR-group ∈\nYR-group during training where Xcore ×XR-group ×Zrepresents the input space and YR-group rep-\nresents the target space. During testing, given a new(Xcore,XR-group,Z) ∈Xcore ×XR-group ×Z,\nthe model will generate a set of target R-groups ( YR-group), which will be attached to the core in\nthe input (Xcore) to form the ﬁnal molecules. These molecules are expected to have the desirable\nproperties speciﬁed in the input (Z).\n2\nPublished as a conference paper at ICLR 2021\nFigure 2: Input and output of the Transformer model. The input consists of property change tokens,\nthe SMILES of the core, the SMILES of the source R-group and the dot (“.”) -symbol separating the\ncore and R-group representations. The output is a R-group, which, when attached to the core from\nthe source molecule, forms the target molecule which is expected to satisfy the property constraint\nin the input.\n3 R ESULTS\nThe information of dataset used in this paper can be found in appendix A.1. We compare our model\nTransformer-R with the following baselines,\nTransformer Baseline: The Transformer developed by He et al. (2020) to generate the whole target\nmolecules at once, in contrast to only the R-groups.\nEnumeration Baseline: This constitutes of an exhaustive algorithm that for a test starting molecule,\nconsists of (i) attaching each R-group seen in the training data to the molecule’s core and (ii) select-\ning all found R-groups which yielded a molecule with desirable properties.\nFor each starting molecule in the test set, 10 unique valid molecules, which are not the same as\nthe starting molecule, were generated using multinomial sampling. Table 1 shows the performance\nof our model Transformer-R and the baselines (Transformer and Enumeration) in terms of various\nevaluation metrics. Aligning with our goal, we ﬁrstly examine the following three aspects,\nDesirable: This metric gives the proportion of generated molecules that fulﬁll the desirable prop-\nerties speciﬁed by model input. A slight improvement was observed from Transformer-R over the\nTransformer baseline.\nMMP33: This refers to the proportion of generated molecules for which (i) a single transforma-\ntion (i.e. MMP) has been applied compared to the starting molecule and (ii) the ratio between the\nnumber of heavy atoms (non-hydrogen atoms) in the R-group and the number of heavy atoms in the\nentire molecule is not greater than 0.33. This evaluates how well the model captures the chemist’s\nintuition that small and single transformations are applied to the starting molecules. From Table 1,\nTransformer-R generates much more molecules with small and single transformations to the starting\nmolecules, which mimics the chemist’s strategy when optimizing a starting molecule.\n3\nPublished as a conference paper at ICLR 2021\nTable 1: Comparison of our model Transformer-R and the baselines (Transformer and Enumeration)\nin terms of various evaluation metrics on three test sets.\nMetric\nTest set Method Desirable MMP33 Unchanged\nCore\nUnseen\nTrans.\nNovel\nR-groups\nTransformer-R 58.97% 97.67% 100.00% 53.92% 4.30%\nTest-Original Transformer 56.14% 90.45% 69.10% 51.31% 3.99%\nEnumeration 16.93% 77.85% 100.00% 96.62% 0.00%\nTransformer-R 56.76% 97.42% 100.00% 32.37% 2.14%\nTest-Core Transformer 55.61% 86.82% 44.60% 34.76% 2.27%\nEnumeration 18.64% 77.93% 100.00% 98.36% 0.00%\nTransformer-R 42.90% 97.57% 100.00% 57.84% 4.66%\nTest-Property Transformer 41.75% 90.69% 62.25% 57.98% 4.25%\nEnumeration 15.91% 81.19% 100.00% 96.65% 0.00%\nUnchanged Core: This refers to the proportion of generated molecules that keep the core speci-\nﬁed by model input. Clearly, the generated molecules from Transformer-R always keeps the core\n(100%), while the number for the Transformer baseline dropped signiﬁcantly to around 44%-70%.\nThe reason is that Transformer-R only generates the R-groups which are attached to the core to form\nthe ﬁnal molecules, while the Transformer baseline generates the whole molecule directly which is\nnot guaranteed to keep the core of interest.\nIn addition to the above three metrics, we are interested in how well the models can generate trans-\nformations and R-groups not seen in the training set. Note that many unseen transformations and\nnovel R-groups are not preferable if the model performs bad in the above three metrics.\nUnseen Transformations: This refers to the proportion of generated molecules yielding a transfor-\nmation (i.e. speciﬁc R-group change) which has not been seen in the training set. Transformer-R and\nthe Transformer baseline obtain similar performance: both have learned to use not only the existing\ntransformations in the training set, but also unseen transformations (32%-58%) to optimize unseen\ncombinations of starting molecule and property constraint. Note that unseen transformations alone\nis not a sufﬁcient quality metric, as seen the Enumeration baseline resulted in more unseen transfor-\nmations (above 96%), but very low proportion (15%-19%) of molecules with desirable properties.\nNovel R-groups: This metric gives the proportion of generated molecules that contain R-groups\nwhich have not been seen among the R-groups in the training set. Both Transformer-R and the\nTransformer baseline have generated novel R-groups (2%-5%). For the Enumeration baseline, no\nnovel R-groups are generated since it only enumerates the existing R-groups in the training set.\nFigure 3 in appendix shows the top 20 most frequent novel R-groups generated by Transformer-R.\n4 C ONCLUSIONS\nWe have introduced Transformer-R to generate only R-groups instead of the whole molecule when\noptimizing a starting molecule towards its desirable properties as speciﬁed in the input. The gen-\nerated R-groups are attached to the core in the input starting molecule to form the ﬁnal molecules.\nOur results show that Transformer-R generates (i) slightly more molecules with desirable proper-\nties speciﬁed in the input; (ii) many more molecules which have small and single transformations\napplied to the starting molecule, which mimics the chemist’s strategy; and (iii) molecules which al-\nways keep the core speciﬁed in the input constant. This is particularly useful to chemists who want\nto keep certain part of the starting molecule unchanged. Additionally, in contrast to the Enumeration\nbaseline, our model can generate novel R-groups not present in the training set.\nWe have focused on optimizing three ADMET properties following He et al. (2020). In princi-\nple, Transformer-R can be trained to optimize other properties as well, e.g. synthetic accessibil-\nity and bioactivity. This could help to optimize small molecule antiviral drug candidates against\ne.g. COVID-19 in a more efﬁcient way.\n4\nPublished as a conference paper at ICLR 2021\nREFERENCES\nCatherine S Adamson, Kelly Chibale, Rebecca JM Goss, Marcel Jaspars, David J Newman, and\nRosemary A Dorrington. Antiviral drug discovery: preparing for the next pandemic. Chemical\nSociety Reviews, 2021.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In 3rd International Conference on Learning Representations,\nICLR 2015, 2015.\nClare L Box and Kevin S J Thompson. Evaluation of potential anti-covid-19 therapies, 2020.\nLeon Caly, Julian D Druce, Mike G Catton, David A Jans, and Kylie M Wagstaff. The fda-approved\ndrug ivermectin inhibits the replication of sars-cov-2 in vitro. Antiviral research, 178:104787,\n2020.\nAndrew Dalke, Jerome Hert, and Christian Kramer. mmpdb: An open-source matched molecular\npair platform for large multiproperty data sets. Journal of chemical information and modeling, 58\n(5):902–910, 2018.\nSpencer M Free and James W Wilson. A mathematical contribution to structure-activity studies.\nJournal of Medicinal Chemistry, 7(4):395–399, 1964.\nAnna Gaulton, Louisa J Bellis, A Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne\nLight, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, et al. Chembl: a large-scale\nbioactivity database for drug discovery. Nucleic acids research, 40(D1):D1100–D1107, 2012.\nEd Griffen, Andrew G Leach, Graeme R Robb, and Daniel J Warner. Matched molecular pairs as\na medicinal chemistry tool: miniperspective. Journal of medicinal chemistry, 54(22):7739–7750,\n2011.\nCorwin Hansch and Toshio Fujita. p-σ-πanalysis. a method for the correlation of biological activity\nand chemical structure. Journal of the American Chemical Society, 86(8):1616–1626, 1964.\nCorwin Hansch, Peyton P Maloney, Toshio Fujita, and Robert M Muir. Correlation of biologi-\ncal activity of phenoxyacetic acids with hammett substituent constants and partition coefﬁcients.\nNature, 194(4824):178–180, 1962.\nJiazhen He, Huifang You, Emil Sandstr¨om, Eva Nittinger, Esben Bjerrum, Christian Tyrchan, Wern-\ngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemist’s intuition using\ndeep neural networks. 2020.\nJiansheng Huang, Wenliang Song, Hui Huang, and Quancai Sun. Pharmacological therapeutics\ntargeting rna-dependent rna polymerase, proteinase and spike protein: from mechanistic studies\nto clinical trials for covid-19. Journal of clinical medicine, 9(4):1131, 2020.\nWengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-\ngraph translation for molecular optimization. arXiv preprint arXiv:1812.01070, 2018.\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical graph-to-graph translation for\nmolecules. arXiv, pp. arXiv–1907, 2019.\nWengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphs\nusing structural motifs. arXiv preprint arXiv:2002.03230, 2020.\nPeter W Kenny and Jens Sadowski. Structure modiﬁcation in chemical databases.Chemoinformatics\nin drug discovery, 23:271–285, 2005.\nAndrew G Leach, Huw D Jones, David A Cosgrove, Peter W Kenny, Linette Ruston, Philip Mac-\nFaul, J Matthew Wood, Nicola Colclough, and Brian Law. Matched molecular pairs as a guide\nin the optimization of pharmaceutical properties; a study of aqueous solubility, plasma protein\nbinding and oral exposure. Journal of medicinal chemistry, 49(23):6672–6682, 2006.\n5\nPublished as a conference paper at ICLR 2021\nGeorgi Momekov and Denitsa Momekova. Ivermectin as a potential covid-19 treatment from the\npharmacokinetic point of view: antiviral levels are not likely attainable with known dosing regi-\nmens. Biotechnology & biotechnological equipment, 34(1):469–474, 2020.\nPavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of drug-\nlike chemical space based on gdb-17 data. Journal of computer-aided molecular design, 27(8):\n675–679, 2013.\nSuranga L Senanayake. Drug repurposing strategies for covid-19, 2020.\nKhan Sharun, Kuldeep Dhama, Shailesh Kumar Patel, Mamta Pathak, Ruchi Tiwari, Bhoj Raj Singh,\nRanjit Sah, D Katterine Bonilla-Aldana, Alfonso J Rodriguez-Morales, and Hakan Leblebicioglu.\nIvermectin, a new candidate therapeutic against sars-cov-2/covid-19, 2020.\nJohn G Topliss. Utilization of operational schemes for analog synthesis in drug design. Journal of\nmedicinal chemistry, 15(10):1006–1011, 1972.\nChristian Tyrchan and Emma Evertsson. Matched molecular pair analysis in short: algorithms,\napplications and limitations. Computational and structural biotechnology journal, 15:86–90,\n2017.\nJulia Weber, Janosch Achenbach, Daniel Moser, and Ewgenij Proschak. Vammpire: a matched\nmolecular pairs database for structure-based drug design and optimization. Journal of medicinal\nchemistry, 56(12):5203–5207, 2013.\nDavid Weininger. Smiles, a chemical language and information system. 1. introduction to method-\nology and encoding rules. Journal of chemical information and computer sciences, 28(1):31–36,\n1988.\n6\nPublished as a conference paper at ICLR 2021\nA A PPENDIX\nA.1 D ATASET\nThe same dataset in He et al. (2020) is used in this paper. In particular, a set of MMPs are extracted\nfrom ChEMBL (Gaulton et al., 2012) using the open-source matched molecular pair tool (Dalke\net al., 2018). The three properties (logD, solubility and clearance) of the source and target molecules\nare predicted from models built using the in-house experimental data. The property prediction mod-\nels are used for constructing data during training and also for evaluating the generated molecules\nduring testing. For the test sets, in addition to Test-Original and Test-Property in He et al. (2020),\nwe create Test-Core, which is a subset of the molecular pairs in Test-Original where we have ex-\ncluded molecular pairs for which the core is present in the training set.\nA.2 A DDITIONAL FIGURES\nFigure 3: Top 20 most frequent novel R-groups generated by Transformer-R on Test-Original.\n7"
}