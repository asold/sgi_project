{
  "title": "Looking Outside the Window: Wide-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images",
  "url": "https://openalex.org/W4226013274",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2100932773",
      "name": "Ding, Lei",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2100044068",
      "name": "Lin Dong",
      "affiliations": [
        "Space Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2474193941",
      "name": "Lin Shao-fu",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1925561233",
      "name": "Zhang Jing",
      "affiliations": [
        "University of Trento"
      ]
    },
    {
      "id": "https://openalex.org/A2378617044",
      "name": "Cui Xiaojie",
      "affiliations": [
        "State Key Laboratory of Remote Sensing Science"
      ]
    },
    {
      "id": "https://openalex.org/A2351865090",
      "name": "Wang Yue-bin",
      "affiliations": [
        "China University of Geosciences (Beijing)"
      ]
    },
    {
      "id": "https://openalex.org/A2101529693",
      "name": "Tang, Hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2737877340",
      "name": "Bruzzone, Lorenzo",
      "affiliations": [
        "University of Trento"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3003394660",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2787091153",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3093407252",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6785727093",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6796604954",
    "https://openalex.org/W6790275670",
    "https://openalex.org/W2798825526",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2546696630",
    "https://openalex.org/W2991488782",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W2966926453",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W2963815618",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3159637683",
    "https://openalex.org/W2536190202",
    "https://openalex.org/W3130523820",
    "https://openalex.org/W2965390952",
    "https://openalex.org/W2965383240",
    "https://openalex.org/W3007268491",
    "https://openalex.org/W3103333135",
    "https://openalex.org/W2888738931",
    "https://openalex.org/W2963659230",
    "https://openalex.org/W2796232579",
    "https://openalex.org/W3025400702",
    "https://openalex.org/W3103092912",
    "https://openalex.org/W3008156909",
    "https://openalex.org/W2963378109",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3204166336",
    "https://openalex.org/W3128592650",
    "https://openalex.org/W3180045188",
    "https://openalex.org/W6639927594",
    "https://openalex.org/W2109255472",
    "https://openalex.org/W2995766874",
    "https://openalex.org/W2948080074",
    "https://openalex.org/W2494341560",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2179352600",
    "https://openalex.org/W4309845474",
    "https://openalex.org/W3169909366",
    "https://openalex.org/W4226378839",
    "https://openalex.org/W4287572671",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W1899185266",
    "https://openalex.org/W3103263155",
    "https://openalex.org/W3186032668",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W3105127913",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Long-range contextual information is crucial for the semantic segmentation of\\nHigh-Resolution (HR) Remote Sensing Images (RSIs). However, image cropping\\noperations, commonly used for training neural networks, limit the perception of\\nlong-range contexts in large RSIs. To overcome this limitation, we propose a\\nWide-Context Network (WiCoNet) for the semantic segmentation of HR RSIs. Apart\\nfrom extracting local features with a conventional CNN, the WiCoNet has an\\nextra context branch to aggregate information from a larger image area.\\nMoreover, we introduce a Context Transformer to embed contextual information\\nfrom the context branch and selectively project it onto the local features. The\\nContext Transformer extends the Vision Transformer, an emerging kind of neural\\nnetwork, to model the dual-branch semantic correlations. It overcomes the\\nlocality limitation of CNNs and enables the WiCoNet to see the bigger picture\\nbefore segmenting the land-cover/land-use (LCLU) classes. Ablation studies and\\ncomparative experiments conducted on several benchmark datasets demonstrate the\\neffectiveness of the proposed method. In addition, we present a new Beijing\\nLand-Use (BLU) dataset. This is a large-scale HR satellite dataset with\\nhigh-quality and fine-grained reference labels, which can facilitate future\\nstudies in this field.\\n",
  "full_text": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 1\nLooking Outside the Window: Wide-Context\nTransformer for the Semantic Segmentation of\nHigh-Resolution Remote Sensing Images\nLei Ding, Dong Lin, Shaofu Lin, Jing Zhang, Xiaojie Cui, Yuebin Wang, Member, IEEE, Hao Tang, and\nLorenzo Bruzzone, Fellow, IEEE\nAbstract—Long-range contextual information is crucial for the\nsemantic segmentation of High-Resolution (HR) Remote Sensing\nImages (RSIs). However, image cropping operations, commonly\nused for training neural networks, limit the perception of long-\nrange contexts in large RSIs. To overcome this limitation, we\npropose a Wide-Context Network (WiCoNet) for the semantic\nsegmentation of HR RSIs. Apart from extracting local features\nwith a conventional CNN, the WiCoNet has an extra context\nbranch to aggregate information from a larger image area. More-\nover, we introduce a Context Transformer to embed contextual\ninformation from the context branch and selectively project it\nonto the local features. The Context Transformer extends the\nVision Transformer, an emerging kind of neural networks, to\nmodel the dual-branch semantic correlations. It overcomes the\nlocality limitation of CNNs and enables the WiCoNet to see the\nbigger picture before segmenting the land-cover/land-use (LCLU)\nclasses. Ablation studies and comparative experiments conducted\non several benchmark datasets demonstrate the effectiveness of\nthe proposed method. In addition, we present a new Beijing\nLand-Use (BLU) dataset. This is a large-scale HR satellite dataset\nwith high-quality and ﬁne-grained reference labels, which can\nfacilitate future studies in this ﬁeld.\nIndex Terms—Remote Sensing, Semantic Segmentation, Vision\nTransformer, Convolutional Neural Network\nI. I NTRODUCTION\nSemantic segmentation of remote sensing images (RSIs)\nrefers to their pixel-wise labelling according to the ground in-\nformation of interest (e.g., land-cover/land-use (LCLU) types).\nL. Ding is with the PLA Strategic Force Information Engineering Univer-\nsity, ZhengZhou, China (E-mail: dinglei14@outlook.com).\nD. Lin is with the Space Engineering University, No.7 Fuxue Road,\nChangping District, 102249 Beijing, China and also with the State Key\nLaboratory of Geo-Information Engineering, No.1 Yanta Road, Beilin District\n710054, Xi’an, China. (E-mail: lindong hb59@163.com).\nS. Lin is with Beijing University of Technology, NO.100 Pingle\nGarden, Chaoyang District, 100022 Beijing, P.R. China. (E-mail: lin-\nshaofu@bjut.edu.cn).\nJ. Zhang and L. Bruzzone are with the Department of Information En-\ngineering and Computer Science, University of Trento, 38123 Trento, Italy\n(E-mail: jing.zhang-1@unitn.it, lorenzo.bruzzone@unitn.it).\nX. Cui is with the Beijing Institute of Remote Sensing Information,\nNo.6 Waiguanxie Street, Chaoyang District, 100011 Beijing, China. (E-mail:\ncuixjgis@163.com).\nY . Wang is with the China University of Geosciences (Beijing),\nNo.29 Xueyuan Road, Haidian District, 100084 Beijing, China. (E-mail:\nxxgcdxwyb@163.com).\nH. Tang is with the Department of Information Technology and\nElectrical Engineering, ETH Zurich, 8092 Zurich, Switzerland. (E-mail:\nhao.tang@vision.ee.ethz.ch).\nThis document is funded by State Key Laboratory of Geo-Information\nEngineering, No.SKLGIE2019-Z-3-3. It is also funded by the scholarship from\nChina Scholarship Council under the grant NO.201703170123.\nThis is important for a variety of practical applications such as\nenvironmental assessment, crop monitoring, natural resources\nmanagement and digital mapping. Recently with the develop-\nment of Earth observation technology and the emergence of\nconvolutional neural networks (CNNs), it has been possible to\nperform automatic semantic segmentation of RSIs on easily\naccessible high-resolution (HR) RSIs.\nRecent CNN models for visual recognition tasks are mostly\nbased on stacked convolutional ﬁlters. A single convolu-\ntion operation can extract/strengthen a certain feature, while\nstacked convolutions can combine and transform variety of\nfeatures. With the inclusion of numerous convolutional layers,\na deep CNN can learn high-level semantic representations of\nthe observed objects in images [1]. Since the introduction of\nFully Convolutional Network (FCN) in [2], deep CNNs have\nbeen widely used for dense classiﬁcation tasks (i.e., semantic\nsegmentation).\nHowever, one of the limitations of CNNs is the intrinsic\nlocality of convolution operations. The receptive ﬁeld (RF) of\na CNN unit is the region of input that is seen and responded\nto by the unit. Considering the sparse activation nature of\nCNNs, the valid receptive ﬁeld (VRF) of a CNN unit is rather\nsmall [3]. This means that conventional CNNs model mostly\nthe local image patterns (e.g., color, texture of objects) rather\nthan considering the context information. Although numerous\npapers have proposed designs to enlarge the VRFs of CNNs\n[4], [5], they do not consider the long-range dependency\nbetween different image areas. The introduction of attention\nmechanism in CNNs [6], [7], [8] has allowed the network to\nlearn biased focus under different image scenes. However, the\nsemantic correlations between different image regions are not\ndeeply modelled.\nRecently, transformers are emerging [9] and gaining increas-\ning research interest in the computer vision community [10],\n[11]. Differently from CNNs that rely on local operators to\nextract information, transformers employ stacked multi-head\nattention blocks to model the global relationship between\ntokenized image patches. This enables them to exploit in-\ndepth the long-range dependency that the data may exhibit.\nIn recent studies transformers are replacing CNNs in many\nvisual recognition tasks [12], [13], [14]. However, training a\nvision transformer requires large amount of training data to\ncompensate its lack of inductive biases [10]. It is also more\ncalculation-intensive compared to CNNs.\nIn this study we aim to take advantage of both the CNN\narXiv:2106.15754v6  [cs.CV]  20 Apr 2022\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 2\nand transformer for the semantic segmentation of HR RSIs.\nThe CNNs are good at preserving the spatial information,\nwhile transformer enables a better modelling of the long-range\ndependencies. Moreover, instead of placing a plain transformer\nat the end of a CNN [15], we propose a dual branch Context\ntransformer to model the broader context in large RSIs. By\nallowing network to look at the bigger picture (i.e., seeing\nthe wider context), it can understand better the local LCLU\ninformation. The main contributions in this study can be\nsummarized as follows:\n1) Proposing a Wide-Context Network (WiCoNet) for the\nsemantic segmentation of HR RSIs. The WiCoNet\nincludes two CNNs that extract features from local\nand global image levels, respectively. This enables the\nWiCoNet to consider both local details and the wide\ncontext;\n2) Proposing a Context Transformer to model the dual-\nbranch semantic dependencies. The Context Transformer\nembeds the dual-branch CNN features into ﬂattened\ntokens and learns contextual correlations through repet-\nitive attention operations across the local and contextual\ntokens. Consequently, the projected local features are\naware of the wide contextual information;\n3) Presenting a benchmark dataset (i.e., the Beijing Land-\nUse (BLU) dataset) for the semantic segmentation of\nRSIs. This is a challenging HR satellite dataset annotated\naccording to the land-use types. We believe the release\nof this dataset can greatly facilitate future studies.\nThe remainder of this paper is organized as follows. Sec-\ntion II introduces the literature work related to the semantic\nsegmentation of RSIs. In Section III, we present the proposed\nWiCoNet. Section IV illustrates the designed experiments and\nintroduces our BLU dataset. Finally, we draw a conclusion of\nthis study in Section V.\nII. R ELATED WORK\nA. Semantic Segmentation of Natural Images\nIn [2] deep CNNs have been ﬁrst introduced for the se-\nmantic segmentation of images. CNN-based semantic segmen-\ntation can be used in many applications, such as saliency\ndetection [16], medical segmentation [17], road scene un-\nderstanding [18], and LC mapping [19]. CNN architectures\nfor the semantic segmentation of images typically include an\nencoder network to aggregate the local information, as well\nas an decoder network to retrieve the lost spatial details [17],\n[20]. Many network modules have been proposed to enhance\nthe exploitation of local information, including the deformable\nconvolution [21] and the dilated convolution [5] to enlarge\nthe convolutional kernels and the pyramid pooling module to\nmodel multi-scale context information [4]. Meanwhile, many\nliterature works presented sophisticated CNN architectures to\nenhance the extraction of features, such as the multi-branch\nfeature encoding designs in the HRNet [22] and the ReﬁneNet\n[23]. In [24] the ExFuse is proposed, which is a network that\nincludes cross-level information exchanging and multi-scale\nfeature fusion designs.\nIn recent years, the self-attention mechanism has been intro-\nduced to visual tasks in the Squeeze-and-Excitation Networks\n(SENet) [25]. An SE block aggregates and embed global\ninformation into features to learn biased focus in different\nimage scenes, which is often referred as channel attention in\nlater literature. In [7] the channel attention is extended also\nto the spatial dimension to learn the position of focus. In\n[18] the DANet, which combines channel attention and non-\nlocal attention [26] in a parallel manner, has been presented.\nIn the OCRNet [27] the relation between each pixel and\nits surrounding object regions is calculated to augment the\ncontextual representations.\nB. Semantic Segmentation of RSIs\nSemantic segmentation of RSIs refers to the dense classi-\nﬁcation of either multiple LCLU classes or single interested\nclass in RSIs (e.g., road [28], building [29], and water body\n[30]). Spatial accuracy is often crucial to remote sensing appli-\ncations, which is a requirement for the semantic segmentation\nof RSIs. To improve the spatial localization accuracy, many\nliterature works introduce U-shape networks with symmet-\nric encoder-decoder structures. The TreeUNet [31] employs\na DeepUNet to extract multi-scale features and adaptively\nconstruct a tree-like CNN module to fuse the features. The\nResUNet [32] employs the UNet with residual convolutional\nblocks as the segmentation backbone and combines atrous\nconvolution and pyramid scene parsing pooling to aggregate\nthe context information. The MP-ResNet [33] includes three\nparallel feature embedding branches to model the context\ninformation at different scales, each of which includes a full\nResNet34 (some of the residual blocks are shared). Other\npapers resort to strengthen the extraction of edge information.\nIn [34] and [35] the ground truth boundaries of objects are\nprovided as a supervision to guide the network to learn\nedge features. In [36], the Multi-layer Perceptron (MLP) is\nemployed to rectify the uncertain areas in CNN predictions,\nwhich improves the preservation of object boundaries.\nAnother research focus is to model the geometric features of\nground objects. In [37], a direction supervision is introduced\nfor the segmentation of roads. It strengthens the detection\nof linear features, thus the occluded and low-contrast roads\nare more salient to the models. In [29], the shape of object\ncontours is modelled for the segmentation of buildings. The\nbuilding contours are in-painted and sharpened through the\nadversarial learning of their shape information.\nRecently, the attention mechanism has been widely used\nto augment the CNN-extracted features for the semantic seg-\nmentation of RSIs. In [38], the SE design is extended to\nthe spatial dimension to represent the patch-wise semantic\nfocus, which bridges the semantic gap between high-level\nand low-level features. In [39], local and non-local attention\ndesigns are integrated in different branches of the HRNet\n[22], so that the local focus and long-range dependencies are\ncaptured, respectively. In [40], the channel attention and non-\nlocal attention blocks are sequentially used to augment the\nlong-range dependencies in aerial RSIs.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 3\nFig. 1: The proposed Wide-Context Network (WiCoNet).\nC. Transformers in Vision Tasks\nTransformer was ﬁrst introduced for natural language pro-\ncessing tasks [9] where it achieved the state-of-the-art per-\nformance [41]. Recently the use of transformer for computer\nvision tasks has drawn great research interests. In [10], the\nVision Transformer (ViT) is introduced for image classiﬁca-\ntion, which shows that a pure transformer can replace CNN for\nimage recognition tasks. In [42], transformer is ﬁrst used for\nobject detection. The resulting detection Transformer (DETR)\npasses CNN features to a transformer, where the object class\nand locations are automatically generated with the encoded\npositional queries.\nThere are also literature works that use transformers for\ndense classiﬁcation tasks. In [11], a dual-path transformer is\nproposed for panoptic segmentation, which includes a pixel\npath for segmentation and a memory path for class prediction.\nThe transformer is used for information communication be-\ntween the two paths. In [43], a two-branch architecture is pro-\nposed for the segmentation of medical images, which employs\njointly a CNN and a transformer to extract features. In the\nSwin Transformer [13] cascaded transformers are constructed\nin an architecture similar to the ResNet. The spatial sizes of\nembedded patches are gradually increased to enlarge the RF.\nIn several recent papers transformers have been introduced\nfor processing RSIs. In [44] the vision transformer shows\nadvantages over CNNs for scene classiﬁcation in RSIs. In\n[45] a bi-temporal transformer is introduced for the change\ndetection of RSIs. The bi-temporal semantic features are\ntokenized and concatenated, followed by the transformer to\nenrich the global semantic correlations.\nIII. P ROPOSED WIDE -CONTEXT NETWORK\nIn this section, we illustrate the motivation for modelling\na wide context in RSIs, followed by the architecture of the\nproposed network. Then, we describe the designed Context\nTransformer for communication of information between the\ntwo feature extraction branches. Finally, we report the imple-\nmentation details.\nA. Motivation of the Wide-Context Modelling\nVRFs are known to be crucial for visual recognition tasks,\nsince they determine the maximum range of area where\nneural networks can gather information. In [40] and [39],\nthe non-local attention blocks are introduced for the semantic\nsegmentation of RSIs, which expand the VRFs of the networks\ninto the whole input image. However, during training of\nneural networks, the input RSIs are often spatially cropped\nto avoid the overload of computational resources (and also\nto mix the samples in different image regions). Let us denote\nI ∈Rc×h×w as a RSI that consists of cspectral bands and has\nthe spatial size of h×w. To train a standard CNN model M, I\nis usually cropped into Il ∈Rc×hl×wl where hl,wl are height\nand width of the cropping window, respectively. This limits\nthe maximum possible RF of Mto be hl ×wl. Moreover,\ndue to the locality that is inherent to CNNs [10], their VRFs\nare usually much smaller than hl ×wl [46]. Therefore, the\nlong-range context information is insufﬁciently exploited in\nM.\nThis issue is crucial in many LCLU mapping applications.\nThe LCLU mapping is a complex task that requires high-\nlevel abstraction of regional information, where the context\ninformation limited in hl ×wl is often insufﬁcient for rec-\nognizing some crucial samples. Moreover, for many objects\nthat are spatially large (e.g., industrial buildings) or elongated\n(e.g., roads and rivers), the geometric features and semantic\ncorrelations cannot be well-presented in local windows. To\nconquer these limitations, the context information should be\nmodelled in a wider image range, which is the motivation of\nthis study.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 4\nB. Network Architecture\nWe propose a Wide-Context Network (WiCoNet) that ex-\nploits the long-range dependencies in a larger image range in\nRSIs. As illustrated in Fig. 1, the proposed WiCoNet consists\nof two encoding branches. The local branch M1, which is the\nmain branch of the WiCoNet, employs the ResNet to extract\nlocal features. The novel design in the WiCoNet is a context\nbranch M2, which is introduced to explicitly model the wider-\nrange context information in RSIs. It employs a simple CNN\nencoder to learn coarsely the context information (instead\nof gathering the spatial details). The context information\nis learned through M2 and embedded into M1 through a\nContext Transformer T. The ﬁnal results of the WiCoNet is\nthen produced by the context-enriched M1.\nFormally, the training of a standard CNN model is per-\nformed on Il:\nP = M(Il), (1)\nwhere P ∈ Ru×hl×wl is the segmentation map ( u is the\nnumber of classes). Differently, the WiCoNet is trained with\nboth Il and Ic. Ic ∈Rc×hc×wc is a down-sampled copy of I to\nprovide an overview of the surrounding environment. The Il is\nassociated with the central area of Ic. Two segmentation maps\nPl ∈Ru×hl×wl and Pc ∈Ru×hc×wc are produced during the\ntraining phase:\nPl,Pc = T[M1(Il),M2(Ic)], (2)\nThe training is driven by the total multi-class cross-entropy\n(MCE) losses of the two branches, calculated as:\nLSeg = LMCE(Pl,Ll) +αLMCE(Pc,Lc), (3)\nwhere α is a weighting parameter, Ll and Lc are the ground\ntruth (GT) maps in the local and context branches, respectively.\nSince the information extracted from M2 is already mod-\nelled through T, no further feature fusion operations are\nperformed. During the testing phase, Pl is taken directly as\nthe segmentation result.\nC. Context Transformer\nWe introduce a Context Transformer to project long-range\ncontextual information onto the local features, which is de-\nveloped on top of the Vision Transformers. A typical Vision\nTransformer takes ﬂattened and projected image patches as\ninputs. It consists of multiple layers of attention blocks, each\nof which has a Multi-head Self-Attention (MSA) unit and\nan MLP unit [9]. Normalization and residual connections are\nenabled in each unit. The long-range semantic correlations are\nlearned through the stacked attention blocks. Let us consider\nan input 3D signal x ∈ Rˆc×h×w where ˆc is the number\nof channels. x is ﬁrst reshaped into a ﬂattened 2D patch\nxp ∈RN×ˆcp2\n, where N = hw/p2, (p,p) is the spatial size\nof each ﬂattened patch. Then, xp is projected into a token\nvector t ∈RN×D where D is the constant latent vector size\nin all the layers of the Transformer. This operation that maps\nx into t is named Patch Embedding. To retain the position\ninformation, t is further added with trainable parameters before\nit is forwarded into the transformer. The operations inside a\ntransformer block can be represented as follows:\nˆt = MSA(LN(t)) +t,\n˜t = MLP(LN(ˆt)) +ˆt, (4)\nwhere LN denotes a LayerNorm function. The calculations\nincluded in a MSA unit are:\nˆt = Av = softmax( qkT\n√\nD/n\n)v, (5)\nwhere q,k,v ∈RN×D/n are three projections of LN(t), A ∈\nRN×N is the attention matrix, nis the number of heads in the\nMSA.\nMeanwhile, the goal of the designed Context Transformer\nT is to pass information from M2 to the main encoding\nbranch M1. Instead of adding directly the values [47], we\naim to project a biased focus to augment the features in\nM1. Speciﬁcally, for each position in the local feature, the\nresponses from all the context windows are calculated and\nprojected.\nLet tl ∈ RN×D and tc ∈ RM×D (M is the number of\nﬂattened features in M2) denote the local and context tokens\nembedded from M1 and M2, respectively. In T, a local query\nql is projected with tl, while the context key kc and value vc\nare projected with tc:\nql = tlWq ∈RN×D/n,\nkc = tcWk ∈RM×D/n,\nvc = tcWv ∈RM×D/n,\n(6)\nwhere Wq,Wk,Wv ∈RD×D/n are the corresponding weights\nof the projection function.\nThe context attention Ac ∈RN×M is then calculated to\nupdate tl:\nˆtl = Acvc = softmax( qlkT\nc√\nD/n\n)vc. (7)\nThese operations, together with the MLP calculations, are\nrepeated for L times, where the contextual dependencies\nbetween tl and tc are modelled and enforced. Consequently,\nthe local tokens are projected with long-range dependencies\nfrom the context tokens. Finally, the local and context tokens\nare reshaped into 2-dimensional features.\nD. Implementation Details\nHere, we report detailed information of the proposed\nWiCoNet.\n1) The feature extraction networks.We chose the ResNet50\nas the feature extraction network in M1, which is powerful in\nexploiting the local features [38]. The down-sampling stride of\nthe ResNet is ×1/8 to better preserve the spatial information.\nIn the context branch, we employ a simple convolutional block\n(referred as the Context Encoder) to extract context features.\nIt consists of 11 sequentially connected layers, including 8\nconvolutional layers and 3 max-pooling layers. Each pooling\nlayer is placed after 2 convolutional layers following the\nencoder design of UNet [17].\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 5\n2) Area of the context modelling.The down-sampling scale\nfor input to M2 is ×1/4, while the down-sampling stride of\nthe context encoder is the same as the ResNet ( ×1/8). The\nsize of context window is set to 9 times the size of local\nwindow ( w = 3wl,h = 3hl). An analysis of the accuracy\nversus context modelling range is provided in Sec. V-A. In\nthis study, the size of the local window is 256 ×256. In cases\nwhere the local window is at the border of RSIs, empty areas in\nthe context window are padded with reﬂections of the image.\n3) Context Transformer.The hyper-parameters in the Con-\ntext Transformer include: L - number of transformer blocks,\nn - number of heads, p - size of the embedded parches\nand D - dimension of the embedded tokens. p is set to 1\nto retain the spatial information. D is set to 512, which is\nthe number of output channels of the context encoder. L\nand n are set according to the experimental results, which\nare discussed in Sec. V-A. Additionally, there is a weighting\nparameter α. It is dynamically calculated at each iteration as:\n(1−iteration/all iterations)2. In this way, its value declines\nover iterations and the WiCoNet gradually focuses on the local\nbranch.\nTo ﬁnd more details of the WiCoNet, readers are encouraged\nto visit the released codes at: https://github.com/ggsDing/\nWiCoNet.\nIV. E XPERIMENTAL DATASETS AND SETTINGS\nIn this section, the experimental datasets and settings are\nreported. First the experimented datasets are introduced, in-\ncluding the novel Beijing LU dataset and two open datasets.\nThen the experimental settings and evaluation metrics are\nreported.\nA. Beijing Land-Use Dataset\nCurrently there are few HR satellite benchmark datasets\navailable for the multi-class semantic segmentation of RSIs.\nTo facilitate future researches, we present a new benchmark\ndataset named Beijing Land-Use (BLU) dataset. This dataset\nwas collected in June, 2018 in Beijing by the Beijing-2\nsatellite provided by the 21th Century Aerospace Technology\nCo.,Ltd. The collected data are RGB optical images and have a\nground sampling distance (GSD) of 0.8m. We constructed ﬁne-\ngrained human annotations on the collected images based on\n6 LU classes: background/barren, built-up, vegetation, water,\nagricultural land, and road. These are the most interesting and\nfrequently investigated land-use classes in both research stud-\nies and real-world applications (e.g., environment monitoring,\ntrafﬁc analysis and urban and rural management). The detailed\nstatistics of the class distributions are shown in Table I.\nCompared to the existing datasets, the BLU dataset shows\nseveral remarkable features: i) High spatial resolution.As a\nsatellite dataset, it has a high GSD of 0.8m;ii) High annotation\naccuracy. The annotations were performed by an experienced\nannotation team dedicated to the RS applications. Fig. 4 shows\nsome sample image patches selected from this dataset. One\ncan observe that the LU classes in this dataset are easy to be\ndiscriminated due to the high GSD of RSIs. Moreover, the\nannotations are up to the pixel-level and the ground objects\nhave been precisely annotated and geometrically optimized\n(to ensure both local consistence and topological correctness).\nMeanwhile, the observed areas include a variety of scenes,\nincluding farmland, residential areas, highways, airport, wet\nland, and others. This ensures that each LU class contains\ndiverse samples. For example, the ‘built-up’ class includes\nresidential buildings, industrial buildings, and villages; the\n‘water’ class includes rivers, ponds and wet lands, etc. These\nfeatures present challenges to the generalization capability of\nsegmentation algorithms.\nFig. 2 presents an overview of the BLU dataset. The\nobserved regions include both urban and rural scenes, covering\naround 150 km 2 of area in total. The dataset consists of 4\ntiles of large RSIs collected in 4 sub-urban regions in Beijing,\neach one with a pixel size of 15680 ×15680. Each large\nimage is further cropped into 64 images (49 for training, 7 for\nvalidation, and 8 for testing), each of which has 2048 ×2048\npixels (Fig. 3). The training, validation, and testing areas\nare non-overlapping, whereas the cropping windows within\neach area have small overlaps. The total number of images\nfor training, validation, and testing are 196, 28, and 32,\nrespectively. Both the original tiles and the divided sub-sets are\nprovided. The BLU dataset will be released openly accessible\nto researchers 1.\nB. Standard Benchmark Datasets\nTo make a comprehensive analysis on the performance of\nthe proposed WiCoNet, we conducted experiments on two\nadditional open benchmark datasets, i.e., the ISPRS Potsdam\ndataset and the Gaofen Image Dataset (GID).\n1) The Potsdam dataset.This is an area dataset collected\nin urban scenes. It consists of 38 tiles of very high resolution\n(VHR) RSIs, each having 6000 ×6000 pixels. The provided\ndata include true ortho photos containing 4 spectral bands\n(RGB and infrared) and the registered digital surface model\n(DSM) data. The labels are annotated with 6 LC categories:\nimpervious surfaces, building, low vegetation, tree, car, and\nclutter/background. We use 18 tiles of images for training,\n6 for validation and the remaining 14 ones for testing. The\ndivision of training and validation tiles follows the practice in\n[48].\n2) The GID. This is an HR LC classiﬁcation dataset col-\nlected by the Gaofen-2 (GF-2) satellite. It consists of 10 tiles\nof RSIs with 4 spectral bands (RGB and near infrared). Each\ntile has 7200 ×6800 pixels, with a GSD of 0.8m. Since the\ndivision of training and testing sets is not provided, we further\ncrop and divide the tiles into 90 training images, 30 validation\nimages and 40 testing images (each one with 2048 ×2048\npixels. 16 LC classes are annotated, including: industrial land\n(IDL), urban residential (UR), rural residential (RR), trafﬁc\nland (TL), paddy ﬁeld (PF), irrigated land (IL), dry cropland\n(DC), garden plot (GP), arbor woodland (AW), shrub land\n(SL), natural grassland (NG), artiﬁcial grassland (AG), river\n(RV), lake (LK), and pond (PN).\n1https://rslab.disi.unitn.it/dataset/BLU/\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 6\nFig. 2: Overview of the BLU dataset.\nFig. 3: Split of the training, validation and testing sets.\nFig. 4: Sample images taken from different scenes in the BLU\ndataset.\nC. Experimental Settings\nThe proposed WiCoNet and the compared methods are\nimplemented with PyTorch. The hardware environment of this\nstudy is a server equipped with a GTX3090 GPU. For each\ndataset, we ﬁx the training epochs to 50, the batch size to\n32 and the initial learning rate to 0.1. The learning rate lr\nis dynamically calculated at each iteration as: 0.1 ∗(1 −\nTABLE I: Class distribution in the BLU dataset.\nClass Name Number of pixels Proportions (%)\nBackground 156,190,234 15.88\nBuilt-up 125,695,683 12.78\nVegetation 478,668,644 48.67\nWater 28,364,259 2.88\nAgricultural 159,386,020 16.20\nRoad 35,144,760 3.57\nTotal 983,449,600 -\niterations/total iterations)1.5. The optimization algorithm\nis the Stochastic Gradient Descent with the momentum of 0.9.\nRandom ﬂipping and random cropping operations are adopted\nto augment the data. They are performed at each iteration of\nthe training process. At the end of training, the model ﬁle with\nthe best OA (evaluated on the validation set) is saved.\nIn this study we adopt the most frequently used metrics\n[35], [40] to evaluate the tested methods, including: i) Over-\nall Accuracy (OA), which is the numeric ratio of correctly\nclassiﬁed pixels versus all the pixels in RSIs, ii) F1 score of\neach class, which is the harmonic mean of the Precision and\nRecall, and iii) mean Intersection over Union (mIoU). The\nmetrics can be calculated with the number of True Positive\n(TP), True Negative ( TN), False Positive ( FP), and False\nNegative (FN) pixels as follows:\nOA= (TP + TN)/(TP + TN + FP + TN),\nPrecision = TP/(TP + FP),Recall = TP/(TP + FN),\nF1 = 2×Precision ×Recall\nPrecision + Recall,\nIoU = TP/(TP + FP + FN).\n(8)\nV. E XPERIMENTAL RESULTS\nThis section reports the results of the conducted exper-\niments. First an ablation study is developed to verify the\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 7\naccuracy improvements. Then the effect of context modelling\nrange is analyzed. Finally, the proposed WiCoNet is compared\nwith several CNN models with context-aggregation designs in\nrecent studies.\nA. Ablation Study\nChoice of Hyper-Parameters As introduced in Sec.III-D,\nL and n are two adjustable hyper-parameters in the Context\nTransformer. First we conduct a group of experiments to set\ntheir values. The initial values of L and n are set to 2 and 4,\nrespectively. We change the values of L and n by sequence,\nand report the OA obtained by the WiCoNet in Table III.\nOne can observe that the best OA on the BLU and GID\ndatasets is obtained when L = 4,n = 4. Meanwhile, the\noptimal hyper-parameter values for the Potsdam dataset are\nL = 2,n = 4. The OA is lower when L is set to 8. We\nassume that this is caused by over-ﬁtting, since the long-range\ncontext information in RSIs is relatively simple, thus too many\nTransformer layers may be redundant. The tested optimal\nparameters for different datasets are ﬁxed in the following\nexperiments.\nQuantitative Results An ablation study is conducted to test\nthe effectiveness of context modelling. The novel designs in\nthe WiCoNet include an extra context branch and the Context\nTransformer. First, we compare the results of the proposed\nWiCoNet and the FCN [2]. To exclude the improvements\nbrought by the transformer, we also constructed a variant of the\nFCN where a transformer is placed at the end of its encoder,\ndenoted as FCN+Transformer. The experimental results are\nreported in Table II.\nCompared to FCN, the improvements brought by adding\nthe transformer as an encoder head ( FCN+Transformer) are\nlimited. This can be attributed to the limited long-range context\ninformation in local patches. However, after performing the\nwide context modelling with the WiCoNet, signiﬁcant im-\nprovements are obtained. The improvements over the base-\nline FCN are 0.84%, 1.01%, and 1.41% in OA and 1.41%,\n4.05, and 1.69% in mIoU, respectively, on the BLU dataset,\nGID, and Potsdam dataset. These results show that the wide\ncontext modelling in the WiCoNet stably improves the LCLU\nsegmentation accuracy of HR RSIs.\nQualitative Results To qualitatively assess the effects of\ncontext modelling, Fig. 5 and Fig. 6 show comparisons of the\nresults in some sample areas on the BLU and the additional\ndatasets, respectively. In the sample images, both the context\nwindow and the local window of the WiCoNet are presented.\nThe salience maps of the FCN and the WiCoNet are also\nshown to highlight their perception of the critical classes.\nOne can observe that there are many fragmentation errors\nand inconsistency in the segmentation results of the FCN. In\nmany cases, learning only the local bias is not sufﬁcient to\novercome these shortcomings, as shown in the results of the\nFCN+Transformer.\nThe proposed WiCoNet shows advantages in: i) Discrimi-\nnating the critical areas.By modelling contextual dependen-\ncies on similar samples in the context window, the discrimina-\ntion of certain critical or minority classes in the local window\nis improved (e.g., Fig. 5(b), Fig. 6(b)(f)); ii) Improving the\nconnectivity of segmented objects.The spatial layout of certain\nobjects is clearer in a wider image context (e.g., the road in\nFig. 5(a), the rivers in Fig. 5(c) and Fig. 6(a)). The WiCoNet\nbetter preserves their long-range consistency; iii) Reducing\nfragmentation errors.By looking into the context window, the\nWiCoNet understands better the local scenes, thus eliminating\nsome false predictions (e.g., the lake in Fig. 6(c) and an empty\nﬁeld in Fig. 6(e)).\nEffects of the Context Modelling Range The size of\nthe context window ( w ×h) determines up-to which range\nthe context information is modeled, which is critical for\nthe WiCoNet. To allow enough coverage of the surrounding\nregions, the size of the context window should be several times\nbigger than the size of the local window ( wl ×hl). Meanwhile,\nsince transformer is based on self-attention mechanism, too\nlarge context modelling range may cause loss of focus on the\nlocal content. To ﬁnd the best context modelling range, we\nfurther conduct experiments by varying the size of context\nwindows.\nThe results are reported in Table IV. The tested context\nwindows have ×4, ×9, and ×16 times the area of local\nwindows (i.e., w ×h = 2wl ×2hl, w ×h = 3wl ×3hl,\nand w ×h = 4wl ×4hl). One can observe that the ×16\ncontext window results in the best accuracy on the GID and\nthe Potsdam dataset, whereas the ×9 context window leads to\nbetter accuracy on the BLU dataset. The relationship between\nOA and the size of context window is presented in Fig. 7.\nOverall, the increase in OA from ×4 to ×9 windows is\nnoticeable, whereas that from ×9 to ×16 windows is not\nsigniﬁcant.\nB. Comparative Study\nWe further compare the proposed WiCoNet with several\nrecent works on context-aggregation designs. The compared\nmodels include the baseline FCN, the Deeplabv3+ [5] with\ndilated convolutions, the PSPNet [4] with the pyramid scene\nparsing (PSP) module, the DANet [18] with channel attention\nand non-local attention, the SCAttNet [49] with spatial and\nchannel attention, the MSCA [39] with multi-scale context\naggregation designs, and the LANet [38] with local attention.\nWe implement all the tested methods with the experimental\nsettings described in Sec. IV-C and report the results in Ta-\nbles V, VI and VII. The reported values are the average of the\nmetrics derived in 3 trials. One can observe that DeepLabv3+,\na well-known network in the computer vision community,\nshows stable improvements over FCN on the three datasets.\nThe recent attention-based approaches (DANet, LANet and\nSCAttNet) obtain good results on the BLU and Potsdam\ndatasets. In particular, the LANet obtains the second best OA\non the BLU dataset and the GID. The MSCA that integrates\nattention designs into the HRNet architecture achieves the sec-\nond best results on the Potsdam dataset. By extending attention\ninto wider image areas through transformers, the proposed\nWiCoNet obtains the best accuracy metrics (in both OA, mean\nF1 and mIoU) on the three datasets. Its improvements are\nparticularly noticeable on the GID where context information\nis crucial to determine the LC classes.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 8\nTABLE II: Quantitative results of the ablation study on the considered data sets.\nDataset Method Components OA(%) mean F1(%) mIoU(%)local branch context branch Transformer\nBLU\nFCN [2]\n√\n86.51 81.88 70.09\nFCN+Transformer\n√ √\n86.74 82.48 70.92\nWiCoNet (Ours)\n√ √ √\n87.35 82.89 71.50\nGID\nFCN [2]\n√\n74.71 63.13 49.02\nFCN+Transformer\n√ √\n75.82 65.20 51.36\nWiCoNet (Ours)\n√ √ √\n77.14 66.26 53.07\nPotsdam\nFCN [2]\n√\n88.96 90.72 83.24\nFCN+Transformer\n√ √\n88.69 90.39 82.66\nWiCoNet (Ours)\n√ √ √\n90.24 91.70 84.93\n(a)\n(b)\n(c)\n(d)\nTest image GT FCN Saliency map\nof FCN FCN+Transformer WiCoNet\n(Ours)\nSaliency map\nof our\nWiCoNet\nFig. 5: Qualitative results of the ablation study on the BLU datasets. The saliency maps of the critical classes are presented.\nThe selected challenging scenes include: (a) occluded road, (b) green algae-covered river, (c) streets in a residential area, and\n(d) farmland surrounded by vegetation.\nTABLE III: The OA obtained by the WiCoNet with different\nhyper-parameters.\nDataset L n\n2 4 8 4 8\nBLU 87.03 87.35 87.02 87.35 87.13\nGID 77.04 77.14 76.96 77.14 77.05\nPotsdam 90.24 90.21 89.95 90.24 90.22\nThe parameter size and computational cost of each model\nare reported in Table VIII. The number of ﬂoating point\noperations per second (FLOPS) is calculated based on the\nexperimental settings for the BLU dataset (including input &\noutput size and hyper-parameters), except for the batch size\nwhich is set to 1 for clarity. The overall consumption of the\nWiCoNet is higher than that of the FCN, the SCAttNet and\nthe LANet, but it is lower than that of the PSPNet and the\nDANet. Its parameter size and FLOPS are very close to those\nof the DeepLabv3+.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 9\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nTest image GT FCN Saliency map\nof FCN FCN+Transformer WiCoNet\n(Ours)\nSaliency map\nof our\nWiCoNet\nFig. 6: Qualitative results of the ablation study on the additional datasets. The saliency maps of the critical classes are presented.\n(a)˜(c) Results selected from the GID, (d) ˜(f) Results selected from the Potsdam dataset.\nVI. C ONCLUSIONS\nWhile long-range context information is crucial for the\nsemantic segmentation of VHR RSIs, most existing studies\nonly focus on modeling the local context information within\ncropped image patches. To overcome this limitation, we\npropose a Wide-Context Network (WiCoNet). The WiCoNet\nemploys an extra context branch to aggregate the context in-\nformation in bigger image areas (i.e., context windows), which\ngreatly broadens the possible RFs of the models. Moreover,\ninstead of using simple feature fusion designs, we introduce a\nContext Transformer to communicate the information between\nits dual branches. The context information is calculated and\nprojected into the local query tokens, which overcomes the\nlocality limitations of CNNs.\nTo support this study and to facilitate future researches,\nwe also release a high-quality and large-scale benchmark\ndataset for the semantic segmentation on HR RSIs, i.e., the\nBeijing Land-Use (BLU) dataset. Through experiments on\nthe BLU dataset and two additional datasets, we i) veriﬁed\nthe effectiveness of the long-range context modelling, ii)\nanalyzed the accuracy of different context modelling sizes,\nand iii) compared the WiCoNet with several literature works\nthat models context information in RSIs. Experimental results\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 10\nFig. 7: The OA of results versus different size of context\nwindows.\nTABLE IV: The effects of context modeling range on the\nsegmentation accuracy.\nDataset Metrics Size of context windows\n512 ×512 768 ×768 1024 ×1024\nBLU\nOA 86.91 87.35 87.20\nmean F1 82.11 82.77 82.35\nmIoU 70.41 70.58 70.81\nGID\nOA 77.06 77.14 77.28\nmean F1 66.03 66.26 66.55\nmIoU 53.04 53.07 53.38\nPotsdam\nOA 90.16 90.24 90.34\nmean F1 91.59 91.71 91.76\nmIoU 84.72 84.93 85.03\nshow that the WiCoNet enables a better understanding and\nmodeling of both the local scene information and the global\nclass distribution, thus brings signiﬁcant accuracy improve-\nments. However, there are still global inconsistency and some\nlocal fragmentation errors remain, indicating that there is\nstill margin to improve the modelling of long-range context\ninformation in large RSIs. This is left for future works, where\nadversarial learning strategies [29] can be employed to model\nthe semantic correlations.\nACKNOWLEDGMENT\nThe authors would like to thank the anonymous reviewers\nfor their constructive comments and suggestions, which have\nhelped us improve the quality of this paper.\nREFERENCES\n[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in CVPR, 2016.\n[2] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in CVPR, 2015.\n[3] L. Ding, J. Zhang, and L. Bruzzone, “Semantic segmentation of large-\nsize vhr remote sensing images using a two-stage multiscale training\narchitecture,” IEEE Transactions on Geoscience and Remote Sensing,\n2020.\n[4] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\nnetwork,” in CVPR, 2017, pp. 2881–2890.\n[5] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmen-\ntation,” in ECCV, 2018.\n[6] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nCVPR, 2018.\n[7] S. Woo, J. Park, J.-Y . Lee, and I. So Kweon, “Cbam: Convolutional\nblock attention module,” in ECCV, 2018.\n[8] H. Tang, S. Bai, and N. Sebe, “Dual attention gans for semantic image\nsynthesis,” in ACM MM, 2020.\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, 2017, pp. 5998–6008.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\n[11] H. Wang, Y . Zhu, H. Adam, A. Yuille, and L.-C. Chen, “Max-\ndeeplab: End-to-end panoptic segmentation with mask transformers,”\narXiv preprint arXiv:2012.00759, 2020.\n[12] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: A survey,” arXiv preprint arXiv:2101.01169,\n2021.\n[13] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” arXiv preprint arXiv:2103.14030, 2021.\n[14] G. Yang, H. Tang, Z. Zhong, M. Ding, L. Shao, N. Sebe, and\nE. Ricci, “Transformer-based source-free domain adaptation,” arXiv\npreprint arXiv:2105.14138, 2021.\n[15] J. Chen, Y . Lu, Q. Yu, X. Luo, E. Adeli, Y . Wang, L. Lu, A. L. Yuille, and\nY . Zhou, “Transunet: Transformers make strong encoders for medical\nimage segmentation,” arXiv preprint arXiv:2102.04306, 2021.\n[16] T. Wang, L. Zhang, S. Wang, H. Lu, G. Yang, X. Ruan, and A. Borji,\n“Detect globally, reﬁne locally: A novel approach to saliency detection,”\nin CVPR, 2018, pp. 3127–3135.\n[17] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in International Conference on\nMedical image computing and computer-assisted intervention. Springer,\n2015.\n[18] H. Nam, J.-W. Ha, and J. Kim, “Dual attention networks for multimodal\nreasoning and matching,” in CVPR, 2017.\n[19] X.-Y . Tong, G.-S. Xia, Q. Lu, H. Shen, S. Li, S. You, and L. Zhang,\n“Land-cover classiﬁcation with high-resolution remote sensing images\nusing transferable deep models,” Remote Sensing of Environment, vol.\n237, p. 111322, 2020.\n[20] V . Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convo-\nlutional encoder-decoder architecture for image segmentation,” TPAMI,\nvol. 39, no. 12, pp. 2481–2495, 2017.\n[21] X. Zhu, H. Hu, S. Lin, and J. Dai, “Deformable convnets v2: More\ndeformable, better results,” in CVPR, 2019, pp. 9308–9316.\n[22] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y . Zhao, D. Liu,\nY . Mu, M. Tan, X. Wang et al., “Deep high-resolution representation\nlearning for visual recognition,” IEEE transactions on pattern analysis\nand machine intelligence, 2020.\n[23] G. Lin, A. Milan, C. Shen, and I. Reid, “Reﬁnenet: Multi-path reﬁnement\nnetworks for high-resolution semantic segmentation,” in CVPR, 2017.\n[24] Z. Zhang, X. Zhang, C. Peng, X. Xue, and J. Sun, “Exfuse: Enhancing\nfeature fusion for semantic segmentation,” in ECCV, 2018.\n[25] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in\nCVPR, 2018.\n[26] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in CVPR, 2018, pp. 7794–7803.\n[27] Y . Yuan, X. Chen, and J. Wang, “Object-contextual representations\nfor semantic segmentation,” in Computer Vision - ECCV 2020 - 16th\nEuropean Conference, Glasgow, UK, August 23-28, 2020, Proceedings,\nPart VI, ser. Lecture Notes in Computer Science, vol. 12351. Springer,\n2020, pp. 173–190.\n[28] L. Ding, Q. Yang, J. Lu, J. Xu, and J. Yu, “Road extraction based on\ndirection consistency segmentation,” in Chinese Conference on Pattern\nRecognition. Springer, 2016, pp. 131–144.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 11\nTABLE V: Comparison of segmentation accuracy provided by different methods on the BLU dataset.\nMethod Per-class F1 (%) OA (%) mean F1 (%) mIoU (%)Background Built-up Vegetation Water Agricultural Road\nFCN [2] 72.92 87.56 90.41 85.15 86.42 68.88 86.51 ±0.06 81.88 ±0.15 70.09 ±0.21\nPSPNet [50] 72.66 87.40 90.41 86.30 86.71 68.84 86.59 ±0.05 82.05 ±0.25 70.35 ±0.36\nDeepLabv3+ [5] 73.99 87.93 90.76 86.46 87.32 68.85 87.08 ±0.12 82.55 ±0.20 71.07 ±0.25\nDANet [18] 73.06 87.73 90.55 85.45 86.77 69.07 86.76 ±0.07 82.10 ±0.40 70.40 ±0.57\nSCAttNet [49] 73.21 87.62 90.54 86.26 86.87 69.32 86.77 ±0.10 82.30 ±0.17 70.68 ±0.25\nMSCA [39] 73.71 88.34 90.74 85.92 86.86 70.31 87.17 ±0.02 82.64 ±0.02 71.21 ±0.05\nLANet [38] 73.81 87.48 90.60 85.99 87.02 68.49 86.89 ±0.14 82.28 ±0.09 70.60 ±0.17\nWiCoNet (Ours) 74.43 88.55 90.94 86.01 87.23 70.21 87.35 ±0.18 82.89 ±0.22 71.50 ±0.30\nTABLE VI: Comparison of segmentation accuracy provided by different methods on the GID dataset. The LC classes include:\nindustrial land (IDL), urban residential (UR), rural residential (RR), trafﬁc land (TL), paddy ﬁeld (PF), irrigated land (IL), dry\ncropland (DC), garden plot (GP), arbor woodland (AW), shrub land (SL), natural grassland (NG), artiﬁcial grassland (AG),\nriver (RV), lake (LK) and pond (PN).\nMethod Per-classF1 (%) OA mean F1 mIoU\nIDL UR RR TL PF IL DC GP AW SL NG AG RV LK PN (%) (%) (%)\nFCN [2] 59.75 75.57 57.52 68.08 74.815 81.88 36.23 28.55 84.91 8.97 70.07 58.33 81.41 74.11 75.56 74.71±0.04 63.13±0.18 49.02±0.23\nPSPNet [50]59.84 76.29 58.50 67.70 75.25 82.45 39.23 31.69 85.34 7.58 73.37 62.79 83.11 76.70 75.94 75.44±0.06 64.41±0.05 50.44±0.03\nDeepLabv3+ [5]60.44 76.67 58.49 67.67 75.65 82.5 38.62 33.03 84.39 7.13 71.12 64.83 83.17 74.60 74.93 75.38±0.35 64.27±0.74 50.21±0.76\nDANet [18]62.53 76.50 56.73 68.08 75.29 82.76 38.03 26.72 85.75 12.62 73.99 62.95 83.45 77.68 77.25 75.68±0.19 64.7 ±0.11 50.81 ±0.27\nSCAttNet [49]61.87 77.32 59.19 68.75 74.66 82.29 35.75 33.32 86.31 5.66 71.53 74.26 81.72 80.96 80.67 76.05±0.28 65.59±0.37 52.01±0.42\nMSCA [39] 62.06 77.27 56.51 68.69 74.36 82.46 35.99 24.51 87.08 16.00 72.75 70.65 83.78 78.61 79.09 76.10±0.03 65.33±0.59 51.60±0.69\nLANet [38] 63.65 77.67 58.77 69.13 76.80 82.71 37.01 25.68 86.14 7.71 72.42 73.58 84.55 83.53 82.02 76.75±0.26 66.06±0.06 52.83±0.43\nWiCoNet (Ours)63.41 77.21 57.62 68.54 76.37 83.38 40.67 32.75 87.57 4.9 73.08 62.44 87.76 86.86 81.8 77.14±0.13 66.26±0.57 53.07±0.20\nTABLE VII: Comparison of segmentation accuracy provided by different methods on the Potsdam dataset.\nMethod Per-class F1 (%) OA (%) mean F1 (%) mIoU (%)Impervious Surface Building Low Vegetation Tree Car\nFCN [2] 91.08 95.21 86.17 86.51 94.63 88.96 ±0.30 90.72 ±0.20 83.24 ±0.33\nPSPNet [50] 88.85 93.20 83.89 82.69 91.62 86.47 ±0.78 88.05 ±0.66 78.91 ±1.07\nDeepLabv3+ [5] 91.79 96.46 86.17 86.39 94.34 89.47 ±0.34 91.03 ±0.23 83.81 ±0.39\nDANet [18] 91.94 96.05 86.74 87.11 94.42 89.74 ±0.13 91.25 ±0.12 84.14 ±0.20\nSCAttNet [49] 91.66 95.57 86.44 86.79 94.13 89.41 ±0.31 90.92 ±0.27 83.56 ±0.46\nMSCA [39] 92.31 96.74 86.59 87.01 95.11 90.00 ±0.07 91.55 ±0.07 84.69 ±0.12\nLANet [38] 91.63 95.83 85.96 86.35 93.98 89.91 ±0.10 91.45 ±0.11 84.47 ±0.19\nWiCoNet (Ours) 92.50 96.53 87.03 87.31 95.13 90.24 ±0.09 91.70 ±0.04 84.93 ±0.07\nTABLE VIII: Comparison of model size and computational cost expressed in terms of number of parameters and FLOPS,\nrespectively.\nMethods FCN PSPNet DeepLabv3+ DANet SCAttNet MSCA LANet WiCoNet (proposed)\nParams (Mb) 23.78 44.37 39.47 48.22 24.62 66.06 23.79 38.24\nFLOPS (Gbps) 25.27 46.58 41.10 50.29 26.09 21.78 8.28 41.74\n[29] L. Ding, H. Tang, Y . Liu, Y . Shi, X. X. Zhu, and L. Bruzzone,\n“Adversarial shape learning for building extraction in vhr remote sensing\nimages,” IEEE Transactions on Image Processing, vol. 31, pp. 678–690,\n2022.\n[30] L. Duan and X. Hu, “Multiscale reﬁnement network for water-body\nsegmentation in high-resolution satellite imagery,”IEEE Geoscience and\nRemote Sensing Letters, vol. 17, no. 4, pp. 686–690, 2019.\n[31] K. Yue, L. Yang, R. Li, W. Hu, F. Zhang, and W. Li, “Treeunet:\nAdaptive tree convolutional neural networks for subdecimeter aerial\nimage segmentation,” ISPRS Journal of Photogrammetry and Remote\nSensing, vol. 156, pp. 1–13, 2019.\n[32] F. I. Diakogiannis, F. Waldner, P. Caccetta, and C. Wu, “Resunet-a: a\ndeep learning framework for semantic segmentation of remotely sensed\ndata,” ISPRS Journal of Photogrammetry and Remote Sensing, vol. 162,\npp. 94–114, 2020.\n[33] L. Ding, K. Zheng, D. Lin, Y . Chen, B. Liu, J. Li, and L. Bruzzone,\n“Mp-resnet: Multi-path residual network for the semantic segmentation\nof high-resolution polsar images,”IEEE Geoscience and Remote Sensing\nLetters, 2021.\n[34] S. Liu, W. Ding, C. Liu, Y . Liu, Y . Wang, and H. Li, “Ern: Edge loss\nreinforced semantic segmentation network for remote sensing images,”\nRemote Sensing, vol. 10, no. 9, p. 1339, 2018.\n[35] D. Marmanis, K. Schindler, J. D. Wegner, S. Galliani, M. Datcu, and\nU. Stilla, “Classiﬁcation with an edge: Improving semantic image seg-\nmentation with boundary detection,” ISPRS Journal of Photogrammetry\nand Remote Sensing, vol. 135, pp. 158–172, 2018.\n[36] C. Zhang, I. Sargent, X. Pan, A. Gardiner, J. Hare, and P. M. Atkinson,\n“Vprs-based regional decision fusion of cnn and mrf classiﬁcations for\nvery ﬁne resolution remotely sensed images,” IEEE Transactions on\nGeoscience and Remote Sensing, vol. 56, no. 8, pp. 4507–4521, 2018.\n[37] L. Ding and L. Bruzzone, “Diresnet: Direction-aware residual network\nfor road extraction in vhr remote sensing images,” IEEE Transactions\non Geoscience and Remote Sensing, 2020.\n[38] L. Ding, H. Tang, and L. Bruzzone, “Lanet: Local attention embedding\nto improve the semantic segmentation of remote sensing images,” IEEE\nTransactions on Geoscience and Remote Sensing, 2020.\n[39] J. Zhang, S. Lin, L. Ding, and L. Bruzzone, “Multi-scale context ag-\ngregation for semantic segmentation of remote sensing images,” Remote\nSensing, vol. 12, no. 4, p. 701, 2020.\n[40] L. Mou, Y . Hua, and X. X. Zhu, “A relation-augmented fully convo-\nlutional network for semantic segmentation in aerial scenes,” in CVPR,\n2019.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 12\n[41] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-training\nof deep bidirectional transformers for language understanding,” in Pro-\nceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume\n1 (Long and Short Papers). Association for Computational Linguistics,\n2019, pp. 4171–4186.\n[42] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nECCV. Springer, 2020, pp. 213–229.\n[43] Y . Zhang, H. Liu, and Q. Hu, “Transfuse: Fusing transformers and\ncnns for medical image segmentation,”arXiv preprint arXiv:2102.08005,\n2021.\n[44] Y . Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, and N. A. Ajlan,\n“Vision transformers for remote sensing image classiﬁcation,” Remote\nSensing, vol. 13, no. 3, p. 516, 2021.\n[45] H. Chen, Z. Qi, and Z. Shi, “Remote sensing image change detection\nwith transformers,” IEEE Transactions on Geoscience and Remote\nSensing, 2021.\n[46] B. Zhou, A. Khosla, `A. Lapedriza, A. Oliva, and A. Torralba, “Object\ndetectors emerge in deep scene cnns,” in 3rd International Conference\non Learning Representations, ICLR 2015, San Diego, CA, USA, May\n7-9, 2015, Conference Track Proceedings, 2015.\n[47] H. Ding, X. Jiang, B. Shuai, A. Q. Liu, and G. Wang, “Semantic\ncorrelation promoted shape-variant context for segmentation,” in CVPR,\n2019, pp. 8885–8894.\n[48] M. V olpi and D. Tuia, “Dense semantic labeling of subdecimeter reso-\nlution images with convolutional neural networks,” IEEE Transactions\non Geoscience and Remote Sensing, vol. 55, no. 2, pp. 881–893, 2016.\n[49] H. Li, K. Qiu, L. Chen, X. Mei, L. Hong, and C. Tao, “Scattnet: Semantic\nsegmentation network with spatial and channel attention mechanism for\nhigh-resolution remote sensing images,” IEEE Geoscience and Remote\nSensing Letters, vol. 18, no. 5, pp. 905–909, 2020.\n[50] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep\nconvolutional networks for visual recognition,” TPAMI, vol. 37, no. 9,\npp. 1904–1916, 2015.\nLei Ding received the MS’s degree in Photogram-\nmetry and Remote Sensing from the Information\nEngineering University (Zhengzhou, China), and the\nPhD (cum laude) in Communication and Information\nTechnologies from the University of Trento (Trento,\nItaly). He is now a lecturer at the PLA Strategic\nForce Information Engineering University. His re-\nsearch interests are related to semantic segmentation,\nchange detection and domain adaptation with Deep\nLearning techniques. He is a referee for many inter-\nnational journals, including IEEE TIP, IEEE TNNLS\nand IEEE TGRS.\nDong Lin received the MS’s degree in Photogram-\nmetry and Remote Sensing from the Information\nEngineering University (Zhengzhou, China), and the\nPhD in Photogrammetry and Remote Sensing from\nthe Technische Universit ¨at Dresden (Dresden, Ger-\nmany). He is now a lecturer at the Space Engineering\nUniversity. His research interests include deep learn-\ning, change detection and thermal image processing.\nShaofu Lin received the Ph.D. degree in mapping &\nGIS from the Institute of Remote Sensing and Ge-\nographic Information System, Peking University in\n2002. He worked on the research, construction and\nmanagement of informatization and e-government\nin Hainan Information Center, Beijing Information\nResource Management Center and Beijing Munic-\nipal Ofﬁce of Informatization from 1990 to 2009.\nHe was engaged in the promotion of smart city,\ne-government and industrial technology innovation\nin Beijing Economic and Information Commission\nfrom 2009 to 2014, successively serving as the director of E-government\nDepartment, and Science & Technology Standards Department. He has been\na professor of Software College at Beijing University of Technology since\n2014, ever serving as the director of Information Department, the executive\ndirector of Beijing Institute of Smart City, and the executive director of Beijing\nAdvanced Innovation Center for Future Internet Technology. His research\ninterests include spatial-temporal big data, data fusion and intelligence, and\nblock chain. He has senior memberships of China Computer Federation\n(CCF) and Chinese Institute of Electronics (CIE), and has memberships of\nthe Block-chain Commission of CCF, Expert Committee of China Big Data\nIndustry Ecological Alliance, and Network & Information Technology Expert\nCommittee of China Artiﬁcial Intelligence Industry Alliance. He is a board\nmember of Beijing Institute of Big Data.\nJing Zhang received a master’s degree in software\nengineering from Beijing University of Technology.\nShe is currently a Ph.D student at the department\nof Information Engineering and Computer Science,\nUniversity of Trento, Italy. Her current research\ninterests are related to the change detection and\nsemantic segmentation of remote sensing image.\nXiaojie Cui received the MS’s degree and PhD\ndegree in Cartography and Geographic Information\nSystem from the Information Engineering University\n(Zhengzhou, China). She is now an engineer at the\nBeijing Institute of Remote Sensing Information.\nHer research interests include remote sensing image\nprocessing and big data analysis.\nYuebin Wang received the Ph.D. degree from the\nSchool of Geography, Beijing Normal University,\nBeijing, China, in 2016. He was a Post-Doctoral\nResearcher with the School of Mathematical Sci-\nences, Beijing Normal University, Beijing. He is an\nAssociate Professor with the School of Land Science\nand Technology, China University of Geosciences\n(Beijing), Beijing. His research interests include\nremote sensing imagery processing and 3-D urban\nmodeling.\nHao Tang is currently a Postdoctoral with Computer\nVision Lab, ETH Zurich, Switzerland. He received\nthe master’s degree from the School of Electron-\nics and Computer Engineering, Peking University,\nChina and the Ph.D. degree from the Multimedia and\nHuman Understanding Group, University of Trento,\nItaly. He was a visiting scholar in the Department\nof Engineering Science at the University of Oxford.\nHis research interests are deep learning, machine\nlearning, and their applications to computer vision.\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING 13\nLorenzo Bruzzone (S’95-M’98-SM’03-F’10) re-\nceived the Laurea (M.S.) degree in electronic engi-\nneering (summa cum laude) and the Ph.D. degree in\ntelecommunications from the University of Genoa,\nItaly, in 1993 and 1998, respectively.\nHe is currently a Full Professor of telecommunica-\ntions at the University of Trento, Italy, where he\nteaches remote sensing, radar, and digital communi-\ncations. Dr. Bruzzone is the founder and the director\nof the Remote Sensing Laboratory in the Department\nof Information Engineering and Computer Science,\nUniversity of Trento. His current research interests are in the areas of remote\nsensing, radar and SAR, signal processing, machine learning and pattern\nrecognition. He promotes and supervises research on these topics within the\nframeworks of many national and international projects. He is the Principal\nInvestigator of many research projects. Among the others, he is the Principal\nInvestigator of the Radar for icy Moon exploration (RIME) instrument in\nthe framework of the JUpiter ICy moons Explorer (JUICE) mission of the\nEuropean Space Agency. He is the author (or coauthor) of 215 scientiﬁc\npublications in referred international journals (154 in IEEE journals), more\nthan 290 papers in conference proceedings, and 21 book chapters. He is\neditor/co-editor of 18 books/conference proceedings and 1 scientiﬁc book.\nHe was invited as keynote speaker in more than 30 international conferences\nand workshops. Since 2009 he is a member of the Administrative Committee\nof the IEEE Geoscience and Remote Sensing Society (GRSS).\nDr. Bruzzone was a Guest Co-Editor of many Special Issues of international\njournals. He is the co-founder of the IEEE International Workshop on the\nAnalysis of Multi-Temporal Remote-Sensing Images (MultiTemp) series and\nis currently a member of the Permanent Steering Committee of this series\nof workshops. Since 2003 he has been the Chair of the SPIE Conference on\nImage and Signal Processing for Remote Sensing. He has been the founder\nof the IEEE Geoscience and Remote Sensing Magazine for which he has\nbeen Editor-in-Chief between 2013-2017. Currently he is an Associate Editor\nfor the IEEE Transactions on Geoscience and Remote Sensing. He has been\nDistinguished Speaker of the IEEE Geoscience and Remote Sensing Society\nbetween 2012-2016. His papers are highly cited, as proven form the total\nnumber of citations (more than 27000) and the value of the h-index (78)\n(source: Google Scholar).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6718762516975403
    },
    {
      "name": "Segmentation",
      "score": 0.6276739835739136
    },
    {
      "name": "Window (computing)",
      "score": 0.5853002071380615
    },
    {
      "name": "Remote sensing",
      "score": 0.5805943012237549
    },
    {
      "name": "Image segmentation",
      "score": 0.5247700214385986
    },
    {
      "name": "Image resolution",
      "score": 0.5145439505577087
    },
    {
      "name": "Computer vision",
      "score": 0.5040196776390076
    },
    {
      "name": "Transformer",
      "score": 0.47968602180480957
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4617239832878113
    },
    {
      "name": "Geology",
      "score": 0.24743348360061646
    },
    {
      "name": "World Wide Web",
      "score": 0.09587323665618896
    },
    {
      "name": "Engineering",
      "score": 0.08224919438362122
    },
    {
      "name": "Electrical engineering",
      "score": 0.07372260093688965
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I169689159",
      "name": "PLA Information Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210148107",
      "name": "Space Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210149087",
      "name": "Xi'an Railway Survey and Design Institute",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37796252",
      "name": "Beijing University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I193223587",
      "name": "University of Trento",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I4210166112",
      "name": "State Key Laboratory of Remote Sensing Science",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3125743391",
      "name": "China University of Geosciences (Beijing)",
      "country": "CN"
    }
  ]
}