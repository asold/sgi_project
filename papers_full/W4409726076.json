{
  "title": "Instruction multi-constraint molecular generation using a teacher-student large language model",
  "url": "https://openalex.org/W4409726076",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1983292838",
      "name": "Peng Zhou",
      "affiliations": [
        "Hunan University",
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097967288",
      "name": "Jian-Min Wang",
      "affiliations": [
        "Yonsei University"
      ]
    },
    {
      "id": "https://openalex.org/A2105267415",
      "name": "Chun-yan Li",
      "affiliations": [
        "Yunnan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2153728683",
      "name": "Zixu Wang",
      "affiliations": [
        "University of Tsukuba"
      ]
    },
    {
      "id": "https://openalex.org/A2114752435",
      "name": "Yi-Ping Liu",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A2101176649",
      "name": "Siqi Sun",
      "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2129910003",
      "name": "Jianxin Lin",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A2112711241",
      "name": "Leyi Wei",
      "affiliations": [
        "Xiamen University",
        "Macao Polytechnic University"
      ]
    },
    {
      "id": "https://openalex.org/A5084830664",
      "name": "Xibao Cai",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A3034065134",
      "name": "Houtim Lai",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1973321923",
      "name": "Wei Liu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2155672199",
      "name": "Longyue Wang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2111396629",
      "name": "Yuansheng Liu",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A2589655975",
      "name": "Xiangxiang Zeng",
      "affiliations": [
        "Hunan University"
      ]
    },
    {
      "id": "https://openalex.org/A1983292838",
      "name": "Peng Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097967288",
      "name": "Jian-Min Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105267415",
      "name": "Chun-yan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153728683",
      "name": "Zixu Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114752435",
      "name": "Yi-Ping Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101176649",
      "name": "Siqi Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129910003",
      "name": "Jianxin Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112711241",
      "name": "Leyi Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5084830664",
      "name": "Xibao Cai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3034065134",
      "name": "Houtim Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973321923",
      "name": "Wei Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2155672199",
      "name": "Longyue Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111396629",
      "name": "Yuansheng Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2589655975",
      "name": "Xiangxiang Zeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2953641781",
    "https://openalex.org/W2992752586",
    "https://openalex.org/W2889677957",
    "https://openalex.org/W3005776600",
    "https://openalex.org/W3135935512",
    "https://openalex.org/W4391985998",
    "https://openalex.org/W4391669524",
    "https://openalex.org/W3036527662",
    "https://openalex.org/W3174289479",
    "https://openalex.org/W2971690404",
    "https://openalex.org/W3019734328",
    "https://openalex.org/W3207373390",
    "https://openalex.org/W3121085039",
    "https://openalex.org/W2963028280",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W3151682082",
    "https://openalex.org/W2902904588",
    "https://openalex.org/W2132629607",
    "https://openalex.org/W2042572511",
    "https://openalex.org/W3159177617",
    "https://openalex.org/W3034368386",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4391800539",
    "https://openalex.org/W4396723768",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W3094686696",
    "https://openalex.org/W4385572894",
    "https://openalex.org/W4385572408",
    "https://openalex.org/W4389888290",
    "https://openalex.org/W3025593963",
    "https://openalex.org/W2033377040",
    "https://openalex.org/W4396597709",
    "https://openalex.org/W2014858249",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W3018980093",
    "https://openalex.org/W4318981550",
    "https://openalex.org/W3028589594",
    "https://openalex.org/W4307468223",
    "https://openalex.org/W3094640617",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3158543275",
    "https://openalex.org/W3186326839",
    "https://openalex.org/W3138154797",
    "https://openalex.org/W3099414221",
    "https://openalex.org/W3199356076"
  ],
  "abstract": null,
  "full_text": "Zhou et al. BMC Biology          (2025) 23:105  \nhttps://doi.org/10.1186/s12915-025-02200-3\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if \nyou modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or \nparts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\nBMC Biology\nInstruction multi-constraint molecular \ngeneration using a teacher-student large \nlanguage model\nPeng Zhou1,9, Jianmin Wang2, Chunyan Li3, Zixu Wang4, Yiping Liu1, Siqi Sun5,6, Jianxin Lin1, Leyi Wei7,8, \nXibao Cai1, Houtim Lai9, Wei Liu9, Longyue Wang10*, Yuansheng Liu1* and Xiangxiang Zeng1* \nAbstract \nBackground While various models and computational tools have been proposed for structure and property analysis \nof molecules, generating molecules that conform to all desired structures and properties remains a challenge.\nResults We introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a stu-\ndent, incorporates knowledge from various small models and tools, namely, the “teachers.” To train TSMMG, we \nconstruct a large set of text-molecule pairs by extracting molecular knowledge from these “teachers,” enabling it \nto generate novel molecules that conform to the descriptions through various text prompts. We experimentally show \nthat TSMMG remarkably performs in generating molecules that meet complex property requirements described \nin natural language across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% \nand success ratio of 82.58%, 68.03%, and 67.48%, respectively. The model also exhibits adaptability through zero-shot \ntesting, creating molecules that satisfy combinations of properties that have not been encountered. It can compre-\nhend text inputs with various language styles, extending beyond the confines of outlined prompts.\nConclusions TSMMG presents an effective model for multi-constraint molecular generation using natural language. \nThis framework is not only applicable to drug discovery but also serves as a reference for other related fields.\nKeywords Molecular generation, Large language model, Multi-constraint\n*Correspondence:\nLongyue Wang\nvincentwang0229@gmail.com\nYuansheng Liu\nyuanshengliu@hnu.edu.cn\nXiangxiang Zeng\nxzeng@hnu.edu.cn\n1 College of Information Science and Engineering, Hunan University, \nChangsha 410082, Hunan, China\n2 The Interdisciplinary Graduate Program in Integrative Biotechnology, \nYonsei University, Incheon 21983, Seoul, Korea\n3 School of Informatics, Yunnan Normal University, Kunming 650500, \nYunnan, China\n4 Department of Computer Science, University of Tsukuba, \nTsukuba 3058577, Japan\n5 Research Institute of Intelligent Complex Systems, Fudan University, \nShanghai 200433, China\n6 Shanghai AI Laboratory, Shanghai 200232, China\n7 Centre for Artificial Intelligence Driven Drug Discovery, Faculty \nof Applied Science, Macao Polytechnic University, Macao SAR, China\n8 School of Informatics, Xiamen University, Xiamen, China\n9 AI for Life Sciences Lab, Tencent, Shenzhen, China\n10 Alibaba International Digital Commerce, Hangzhou, China\nPage 2 of 17Zhou et al. BMC Biology          (2025) 23:105 \nBackground\nThe development and application of molecular genera -\ntion models play an essential role in the field of artificial \nintelligence for drug discovery (AIDD). Molecular gen -\neration models are instrumental in addressing the chal -\nlenges and complexities associated with the identification \nand design of novel therapeutic compounds. In contrast \nto traditional virtual screening approaches, involving the \nsift of desired molecules from existing libraries, these \ninnovative models are engineered to directly generate \nnovel molecules. Their ability to navigate vast chemi -\ncal spaces, optimize lead compounds, and facilitate de \nnovo design positions them as indispensable tools in the \npursuit of novel and effective therapeutic interventions \n[1–7]. These models not only exhibit the ability to gen -\nerate chemically valid molecules that precisely adhere to \nthe requirements of molecular analysis tools [8, 9], but \nthey also excel in the generation of molecules that meet \nspecific constraints, like quantitative estimate of drug-\nlikeness (QED) and molecular hydrophobicity (LogP) [10, \n11].\nHowever, a primary challenge in the realm of drug \ndiscovery lies in identifying molecules that conform \nto a multitude of constraints, including binding affin -\nity, LogP , QED, synthetic accessibility (SA), and toxic -\nity, rather than merely generating compounds that are \nchemically valid or solely meeting specific criteria [12, \n13]. Several works have been introduced to address this \nchallenge, presenting methodologies capable of generat -\ning molecules that adhere to a spectrum of concurrent \ncondition constraints. For instance, Li et  al. introduced \na conditional generative model proficient in generat -\ning molecules that meet both SA and QED criteria, even \nyielding dual-target inhibitors for JNK3 and GSK3 [14]. \nJin et al. achieved this feat by extracting diverse substruc-\ntures with varying properties and reassembling them to \nproduce molecules satisfying QED, SA, and the inhibi -\ntion of both JNK3 and GSK3 [15]. Bagal et al. employed \na transformer decoder architecture, treating constraint \nconditions as conditional codes, to explore the genera -\ntion of molecules under various combinations of multi -\nple constraints, including LogP , TPSA (total polar surface \narea), and SA [16]. Wang et al. utilized a combination of a \nconditional transformer, knowledge distillation, and rein-\nforcement learning to generate molecules with activity \nagainst DRD2, while also ensuring adherence to QED and \nSA criteria [12].\nAlthough significant progress has been made in prior \nendeavors, it is important to acknowledge that multi-\nconstraint molecular generation methods still suffer \nfrom several noteworthy limitations, which hinder their \npractical applicability. These limitations undermine the \noverall effectiveness and efficiency of these methods in \ngenerating molecules that simultaneously meet diverse \nsets of constraints in drug discovery. These limitations \nfall into the following points: (1) Current multi-constraint \nmolecular generation methods heavily rely on a narrow \nset of constraints. These methods predominantly focus \non specific molecular properties, such as LogP , QED, SA, \nDRD2, JNK3, and GSK3. As a result, they may overlook \nother crucial aspects like substructures, bioavailability, \nand toxicity. The restricted range of constraints limits the \ncomprehensive exploration of diverse chemical proper -\nties, potentially hindering the discovery and optimization \nof molecules with broader applicability in drug discovery \nand related domains. (2) These methods often require \nextensive fine-tuning when applied to different tasks. \nThey tend to generate molecules that closely adhere to \nthe feature distribution of the training dataset. As a con -\nsequence, adapting these models to changes in the target \nspace or applying them to diverse tasks necessitates sig -\nnificant retraining. This inflexibility makes the models \nless adaptable, introducing a substantial burden in terms \nof computational resources and time when confronted \nwith variations in the application context. (3) They often \ninvolve intricate designs. The complexity of the mod -\nels and algorithms used can be a significant obstacle in \ntheir practical application. Users may find it challenging \nto understand and navigate the complexities of the meth-\nods, impacting their usability. Improving the simplicity of \nthese models is essential to make them more accessible \nand applicable in real-world scenarios, especially in drug \ndiscovery and related domains.\nTo address the challenges, we introduce the teacher-\nstudent-based multi-constraint molecular generation \n(TSMMG) model, a natural language-based multi-con -\nstraint molecular generation approach. TSMMG offers \nseveral pivotal advantages: (1) Broader properties and \nhigh scalability: In addition to the constraints often \nfocused on by existing methods, we also consider molec -\nular substructures and ADMET properties. Based on \nthe concept of knowledge distillation, our approach \npresents a versatile data generation framework that lev -\nerages a range of molecular tools and advanced models \nto selectively extract molecules with diverse properties \nfrom publicly available molecular libraries. This para -\ndigm provides a highly scalable approach, facilitating the \nseamless absorption of molecular knowledge beyond the \nscope of this paper. Moreover, this approach can be eas -\nily extended to other domains, such as materials science. \n(2) Multi-task capability: Harnessing the capabilities of \nlarge language models, we train TSMMG across multi -\nple tasks. By formulating distinct prompts, we delineate \nunique molecular spaces without the need for repetitive \nfine-tuning. (3) Simple architecture: TSMMG adopts a \ntransformer-based decoder architecture. This design, \nPage 3 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \ncharacterized by its simplicity, eliminates the need for \nintricate preprocessing of molecular data.\nTo showcase the expressive capabilities of our proposed \nmodel, we meticulously designed 16 sets of experiments \nfor multi-constraint molecular generation. These experi -\nments covered a spectrum of tasks, including molecular \nsubstructures, physicochemical properties, affinity with \ntargets, and ADMET properties. Our findings from these \nexperiments are compelling: TSMMG not only yields \nover 99% of legally valid molecules based on natural lan -\nguage instructions but also, notably, a substantial pro -\nportion of these molecules impeccably aligns with the \nspecified properties in the textual descriptions. Further -\nmore, we conducted a noteworthy case study involving \na zero-shot 5-constraint task. In this scenario, TSMMG \nsuccessfully produced molecules capable of simultaneous \nbinding to EP2 and EP4, showcasing favorable drug-like -\nness and synthetic accessibility, along with the ability to \npenetrate the blood-brain barrier. This case study serves \nas an additional testament to the vast potential embed -\nded in TSMMG. Additionally, our model demonstrated \nits prowess in understanding natural language beyond the \nprompts outlined in this paper, as empirically validated. \nThis expanded capability further solidifies the model’s \npractical applicability. Moreover, we observed that inte -\ngrating molecules generated by our model enhances the \nteacher model’s performance. This collaborative synergy \nfosters continuous improvement between the teacher \nand student models, underscoring the model’s adaptabil -\nity and potential for iterative refinement.\nResults\nTSMMG approach\nAs shown in Fig.  1, TSMMG process involves the follow -\ning steps: (1) First, a substantial dataset of molecules is \ncollected from publicly available molecular libraries. This \ndataset undergoes analysis by advanced molecular pars -\ning tools and models, which referred to as “teachers. ” \nThese teachers extract extensive information, encom -\npassing structural details, physicochemical properties, \nbinding affinities to various targets, and other pertinent \nattributes for each molecule. The resulting knowledge is \nthen organized into text descriptions, which are paired \nwith the corresponding molecules. (2) Second, the “stu -\ndent” model, TSMMG, is trained using the knowledge \nobtained in the previous step. TSMMG is designed to \ncreate a direct mapping from natural language to molec -\nular language. By absorbing a diverse range of knowledge \nexpressed in natural language, the model acquires the \ncapability to generate molecules that possess the speci -\nfied properties outlined in the text. It is worth noting that \nTSMMG undergoes pre-training on a vast corpus of pure \ntext, enabling it to effectively understand and interpret \nnatural language. (3) When presented with a text descrip-\ntion that includes multiple constraints, TSMMG can \ngenerate entirely novel molecules that fulfill these textual \ndescriptions. In doing so, it effectively bridges the gap \nbetween natural language and molecular language for the \npurpose of multi-constraint molecular generation.\nMulti‑constraint task\nTask setting\nTo comprehensively demonstrate the efficacy of the \nTSMMG model, we categorized multi-constraint tasks \ninto three types, each based on different levels of com -\nplexity: two-constraint molecular generation, three-\nconstraint molecular generation, and four-constraint \nmolecular generation. Each of these three task categories \ncomprises eight one-constraint tasks. These one-con -\nstraint tasks encompass:\n• Task 1. Specifying a functional group (FG).\n• Task 2. Specifying the level of hydrophilicity and \nhydrophobicity ( LogP = 1).\n• Task 3. Specifying the level of quantitative estimate of \ndrug-likeness ( QED > 0.6).\n• Task 4. Specifying the level of synthetic accessibility \nscore ( SAs < 4).\n• Task 5. Generate molecules with high affinity for the \ndopamine type 2 receptor ( DRD2 > 0.5).\n• Task 6. Generate molecules with high affinity for the \nglycogen synthase kinase-3 beta ( GSK3 > 0.5).\n• Task 7. Generate molecules capable of crossing the \nblood-brain barrier ( BBB > 0.5).\n• Task 8. Generate molecules that can be absorbed by \nthe human small intestine ( HIA > 0.5).\nAmong these, task 1 is classified as a structure task, while \ntasks 2, 3, and 4 are physicochemical property tasks. \nTasks 5 and 6 fall under activity tasks, and tasks 7 and \n8 are ADMET property tasks. We employ the “+” sym -\nbol to concatenate multiple one-constraint tasks, thereby \nrepresenting multi-constraint tasks. Within the two-con -\nstraint tasks, we considered eight subtasks, combining \nstructure tasks with individual physicochemical prop -\nerty tasks, activity tasks, and ADMET property tasks. \nThese include (1) FG+FG, 2FG for short; (2) FG+LogP; \n(3) FG+QED; (4) FG+SAs; (5) FG+DRD2; (6) FG+GSK3; \n(7) FG+BBB; and (8) FG+HIA. In the three-constraint \ntasks, we explored subtasks such as (1) FG+DRD2+QED; \n(2) FG+GSK3+QED; (3) FG+BBB+QED; and (4) \nFG+HIA+QED. The four-constraint tasks include (1) \nFG+DRD2+QED+SAs; (2) FG+GSK3+QED+SAs; (3) \nFG+BBB+QED+SAs; and (4) FG+HIA +QED+SAs. It \nis essential to emphasize that all these tasks were com -\npleted within a single model, employing different natural \nPage 4 of 17Zhou et al. BMC Biology          (2025) 23:105 \nlanguage prompts. The model underwent comprehensive \ntraining in a unified process, eliminating the need for \nrepetitive fine-tuning. The specific prompts used in these \nexperiments are detailed in Table 1.\nPerformance analysis\nThe experimental results, depicted in Fig.  2A and B, \nunveil several noteworthy findings: Validity: The model \ndemonstrates a remarkable ability to generate mol -\necules that adhere to the syntax rules of SMILES (Sim -\nplified Molecular Input Line Entry System) [17], with \nan impressive average validity rate of 99.87%, 99.89%, \nand 99.87% for two-constraint tasks, three-constraint \ntasks, and four-constraint tasks, respectively. This \nunderscores the model’s proficiency in consistently \nproducing grammatically correct molecules. Unique -\nness: Most generated molecules are unique, with an \noutstanding average uniqueness rate of 90.27%, 81.2%, \nand 81.89% for two-constraint tasks, three-constraint \ntasks, and four-constraint tasks, respectively. From a \nspecific task perspective, the uniqueness of tasks related \nto DRD2 and GSK3 is relatively low, averaging less than \n70%, while other tasks score above 90%. In the next sec -\ntion, we will analyze the reasons behind this situation. \nOverall, the model consistently generates largely dis -\ntinct molecules across various tasks. Novelty: The aver -\nage novelty of the generated molecules stands at 92.79%, \n87.6%, and 87.87%. Similar to uniqueness, tasks related \nto DRD2 and GSK3, such as FG+DRD2+QED (82.76%), \nFig. 1 The process of TSMMG is illustrated as follows: A We use “teacher” models to analyze molecules and obtain their properties, such as structural \ninformation, physicochemical properties, and binding affinities. These properties are then converted into natural language. The natural language \ndescriptions and molecules form a text-molecule paired dataset. B The “student” model, TSMMG, is trained on this text-molecule paired dataset \nto map descriptions to molecular structures. Pre-training on a large text corpus helps it understand natural language. C TSMMG can generate new \nmolecules based on text descriptions with multiple constraints, linking natural language to molecular generation\nPage 5 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \nFG+GSK3+QED (82.44%), FG+DRD2+QED+SA \n(83.9%), and FG+GSK3+QED+SA (83.14%), have rela -\ntively lower novelty scores, while other tasks have novelty \nscores exceeding 90%. In general, the model demon -\nstrates a capacity to generate innovative molecules for \nmost of the tasks at hand. Diversity: Most generated mol-\necules exhibit notable structural differences, as reflected \nin the outstanding average diversity score of 90.47, 89.3, \nand 89.37. Similarly, tasks related to DRD2 and GSK3 \nexhibit lower diversity compared to other tasks. Success \nratio: The average success ratio stands at 82.58%, 68.03%, \nand 67.48% for two-constraint tasks, three-constraint \ntasks, and four-constraint tasks, respectively, which dem-\nonstrates the model’s efficacy in generating novel mole -\ncules that effectively meet all requirements specified by \nnatural language.\nImpact of FGs on performance\nAn important distinction from previous methods is that \nwe consider functional groups (FG) as an additional con -\nstraint, allowing for more precise control over the direc -\ntion of generation. Given the relatively low uniqueness in \ntasks related to DRD2 and GSK3, we use the FG+DRD2 \ntask as an example to further discuss the impact of FG on \ngeneration results.\nFirstly, we analyze the reasons for the low uniqueness \n(68.48) of the FG+DRD2 task. In this task, our prompt \ntemplate is “The molecule contains [FG]. It is active to \nDRD2. ”  We randomly selected 1000 FGs to form 1000 \nprompts, each generating 5 molecules, totaling 5000 mol-\necules. The only difference between each prompt is the \nFG, so we group according to the number of unique mol -\necules generated by each prompt and then extract the FG \nfrom these prompts for analysis. As shown in Fig.  2C(1), \nwe see that the number of FGs that led to the generation \nof 1, 2, 3, 4, and 5 unique molecules were 195, 287, 252, \n184, and 82, respectively. Over 80% of the FG-associated \nprompts can generate two or more unique molecules, \nwith 82 FG-associated prompts each generating 5 com -\npletely different molecules. A significant portion of FG-\nassociated prompts tend to generate identical molecules. \nWe speculate that the main reason for this is the incon -\nsistent frequency of these FGs in the training set, caus -\ning the model to be unable to effectively learn the larger \nmolecular space corresponding to the FG. In light of \nthis speculation, we grouped these 1000 FGs according \nto the number of unique molecules generated and cal -\nculated the average frequency of the FGs in the train -\ning set within each group. As shown in Fig.  2C(2), this is \nbasically consistent with our speculation. Except for the \ngroup generating one unique number of molecules as \ngroup 1, as the number of unique molecules generated \nincreases, the frequency of the corresponding group’s \nFG in the training set also increases, indicating that these \nTable 1 The prompts we use in this work. [FG], [FG1], and [FG2] refer to any functional group, and [VALUE] refers to a real number\nTask Prompt\n2FG The molecule contains [FG1],[FG2]\nFG+LogP The molecule contains [FG]. Its LogP is [VALUE]\nFG+QED The molecule contains [FG]. It has a high QED score\nFG+SA The molecule contains [FG]. It has good synthetic accessibility\nFG+DRD2 The molecule contains [FG]. It is active to DRD2\nFG+GSK3 The molecule contains [FG]. It is active to GSK3\nFG+BBB The molecule contains [FG]. It can pass through the blood-brain barrier\nFG+HIA The molecule contains [FG]. It can be absorbed by human intestinal\nFG+DRD2+QED The molecule contains [FG]. It is active to DRD2. It has a high QED score\nFG+GSK3+QED The molecule contains [FG]. It is active to GSK3. It has a high QED score\nFG+BBB+QED The molecule contains [FG]. It can pass through the blood-brain barrier. It has a high QED score\nFG+HIA+QED The molecule contains [FG]. It can be absorbed by human intestinal. It has a high QED score\nFG+DRD2+QED+SAs The molecule contains [FG]. It is active to DRD2. It has a high QED score. It has good synthetic accessibility\nFG+GSK3+QED+SAs The molecule contains [FG]. It is active to GSK3. It has a high QED score. It has good synthetic accessibility\nFG+BBB+QED+SAs The molecule contains [FG]. It can pass through the blood-brain barrier. It has a high QED score. It has good synthetic acces-\nsibility\nFG+HIA+QED+SAs The molecule contains [FG]. It can be absorbed by human intestinal. It has a high QED score. It has good synthetic accessibility\nBTK The molecule can bind to BTK\nFGFR4 The molecule can bind to FGFR4\nKPCD3 The molecule can bind to KPCD3\n3CL The molecule can bind to 3CL\nPage 6 of 17Zhou et al. BMC Biology          (2025) 23:105 \nFig. 2 A Experimental results for TSMMG across various tasks, encompassing 8 two-constraint tasks, 4 three-constraint tasks, and 4 four-constraint \ntasks. B Average experimental results on two-constraint, three-constraint, and four-constraint tasks. C FG analysis for task FG+DRD2. D Shows \nthe comparison of the success ratio SR (nFG) without considering whether FG matches and the success ratio considering whether FG matches \nunder different constraint tasks. E Shows the impact of different temperatures on the model\nPage 7 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \nFGs can be better trained. The average frequency of the \nFGs in the group with one unique number is slightly \nhigher than that of the group with a 2 unique number, \nand lower than the other groups, which we assume is \nan acceptable bias. We then checked the frequency of \nthe FGs in the DRD2 related training set. As shown in \nFig.  2C(3), a considerable portion of functional groups \n(FGs) did not appear in the training set related to DRD2. \nDespite this, our model still demonstrates the capability \nto generate correct molecules.\nFurther, we consider the ratio of molecules that simul -\ntaneously satisfy valid, unique, novelty, and success \n(VUNS) criteria generated by different groups. As shown \nin Fig.  2C(4), combined with Fig.  2C(2) and C(3), as the \nfrequency of FGs in the training set increases, the model \nis more capable of generating more novel molecules that \nmeet the constraints. In group 5, the average frequency \nof this group’s FG in all training set is 1339, and the ratio \nof VUNS molecules generated by these FG-associated \nprompts is as high as 91%.\nGiven the significant impact of FG on the success ratio, \nwe calculated the success ratio without considering FG \nmatching, abbreviated as SR (nFG). For example, for \nthe FG+DRD2+QED+SA task, SR (nFG) only considers \nwhether DRD2, QED, and SA meet the constraints. The \nresults are shown in Fig.  2D. It can be seen that in the \ntwo-constraint task, three-constraint task, and four-con -\nstraint task, the success ratio without considering FG is \n13.06%, 16.51%, and 16.76% higher than the success ratio \nconsidering FG, respectively.\nThe above observations suggest that as more molecules \nand FGs are added to the training set, our model should \nbe able to achieve more significant performance.\nEffect of temperature on performance\nDuring the inference process of large language models, \ntemperature is a parameter of great interest. A lower \ntemperature implies lower randomness, while a higher \ntemperature means the model has greater freedom. We \nconducted tests on all tasks by setting different tempera -\ntures. Figure  2E shows the average performance of all \ntasks when the temperature is set to 0.5, 0.75, 1.0, 1.25, \nand 1.5, respectively. It can be observed that as the tem -\nperature increases, the ability to generate valid molecules \nremains virtually unchanged, still maintaining above 99%. \nNovelty and diversity also remain almost unchanged. \nHowever, unique and SR show a noticeable increase or \ndecrease. Unique increases from 73.42 to 90.04%, an \nimprovement of approximately 17%, indicating that as \nthe temperature increases, the model can generate more \nunique molecules. At the same time, SR decreases from \n83.03 to 61.94%, a reduction of about 21%. The decrease \nin SR is roughly consistent with the increase in unique, \nwhich means that although increasing the temperature \nfrom 0.5 to 1.5 generates 17% more unique molecules, \nmost of them do not satisfy all constraint conditions.\nCase study of a five‑constraint molecular generation\nGiven the availability of corresponding predictors and \na sufficiently large molecular library, it is theoretically \nfeasible to construct training sets for any combination \nof desired molecular properties. This would enable the \nmodel to generate molecules that encompass a wide \nrange of attributes. However, the challenge arises as the \nnumber of properties and their combinations increases, \nresulting in an exponential growth in the total number \nof possible property combinations. The exhaustive cov -\nerage of all these combinations becomes impractical. To \naddress this challenge, we embarked on an investiga -\ntion to determine if a model could effectively generate \nmolecules when trained using individual properties but \ntested on arbitrary combinations. This research aimed to \nassess the model’s adaptability to novel scenarios. To this \nend, we designed a task that entailed the generation of \nmolecules exhibiting high drug-likeness, good synthetic \naccessibility, blood-brain barrier permeability, and the \nability to bind to both the prostaglandin E2 receptor EP2 \nsubtype [18] and prostaglandin E receptor EP4 [19]. The \ninput prompt constructed for this task was: “The mole -\ncule exhibits a high QED score, good synthetic accessibil-\nity. It can pass through the blood-brain barrier and binds \nto both Prostanoid EP2 and EP4 receptors. ”  During the \ntraining phase, each molecule was associated with only \none property, meaning the model was exposed to mol -\necules corresponding to each of the five properties within \nthis prompt. However, the model had not encountered \nmolecules that simultaneously met all five of these prop -\nerties, and indeed, it had not seen molecules that met \neven two properties explicitly simultaneously.\nThis task presents a formidable challenge from multi -\nple perspectives. From the model’s input perspective, the \nmodel encounters significantly longer input text, a depar-\nture from its prior training data. In terms of molecular \nproperties, the model must not only comprehend the \nmapping of individual properties to molecular spaces \nbut also grasp the complex mapping of multiple prop -\nerties from a single property mapping. Surprisingly, the \nmodel proves to be up to the task, successfully generat -\ning molecules that simultaneously satisfy all the condi -\ntion constraints. As illustrated in Fig. 3, we showcase four \nmolecules that meet all the property requirements speci -\nfied in the textual description. To validate their compat -\nibility with the receptors of EP2 (PDB ID: 7CX2) and EP4 \n(PDB ID: 5YWY), we employed UCSF Chimera [20] for \nmolecular docking preparation and UCSF Dock6 [21] to \nconduct molecular docking. Finally, we used PLIP [22] \nPage 8 of 17Zhou et al. BMC Biology          (2025) 23:105 \nand PyMOL [23] for visualizing the docking results. The \ndocking results reveal that these molecules effectively fit \ninto the ligands and establish hydrogen bonds with differ-\nent residues, demonstrating their potential for fulfilling \nthe specified molecular properties.\nThis experimental outcome holds profound signifi -\ncance, as it demonstrates the model’s robust capability to \ngenerate molecules that satisfy complex multi-constraints \nduring zero-shot testing, even when initially trained with \none-constraint data. This versatility underscores the \nmodel’s adaptability and its potential to address intricate \nchallenges in molecular generation.\nDiversity of input text\nGiven that TSMMG is trained based on GPT-2 [24], \nwhich has undergone extensive pre-training on natural \nlanguage datasets, we have a reasonable basis to hypoth -\nesize that TSMMG can comprehend the similarities \nin natural language. Specifically, when provided with \nprompts that share the same semantics but exhibit sub -\ntle differences in their expressions, TSMMG is likely to \ngenerate accurate molecules. This hypothesis stems from \nthe fact that GPT-2 has acquired the ability to capture \nvarious linguistic patterns and semantic relationships \nduring its training process. Consequently, it may possess \nthe capability to generalize and transfer its knowledge to \nrelated but slightly different prompts. In essence, TSM -\nMG’s potential to generate correct molecules may persist \neven with prompt variations due to its underlying under -\nstanding of linguistic similarities.\nTo test this hypothesis, we explored the use of diverse \ntemplates that encompass different language habits and \nvariations. By making slight modifications to the original \ntraining templates, we aimed to assess TSMMG’s abil -\nity to generate correct molecules when input prompts \nwere slightly altered. For example, during the training \nphase, we utilized the template “The molecule contains \n[FG], it can be absorbed by the human intestine. ” for the \nFG+HIA task. We introduced minor adjustments to cre -\nate two new prompts: “I want a molecule that contains \n[FG] and can be absorbed by the human intestine. ” and \n“Give me a molecule which contains [FG] and can be \nabsorbed by the human intestine. ” We conducted experi-\nments using these diverse prompts across four different \ntasks, as presented in Table  2, and the results are sum -\nmarized in Table 3.\nThe experiments demonstrated that TSMMG consist -\nently generated molecules that met the specified require -\nments to a large extent, even with modified prompts. As \nshown in Table 2, the validity of the generated molecules \ncan still reach over 99% after using prompts of different \nstyles. For the FG+BBB and FG+HIA tasks, using the \nT1 and T2 templates both resulted in approximately a \n9% decrease in SR compared to using the T0 template, \nwhile uniqueness, novelty, and diversity showed almost \nno significant changes. For the FG+DRD2 task, when \nusing the T1 template, SR decreased by 33.36%, novelty \ndecreased by 12.12%, while uniqueness increased by \n3.36%; when using the T2 template, SR decreased by 30%, \nnovelty decreased by 11.22%, while uniqueness increased \nby 1.58%. The FG+GSK3 task and the FG+DRD2 task \nshow the same trend, that is, when using the T1 and T2 \ntemplates, SR and novelty show a significant decrease \nand uniqueness shows a certain degree of increase, while \nother indicators show relatively small differences.\nThese results suggest that TSMMG exhibits a cer -\ntain degree of tolerance to diverse prompts and can \ncontinue to generate molecules that meet the specified \nrequirements, even when the prompts are modified. It is \nimportant to note that while TSMMG may demonstrate \ntolerance to prompt variations, the extent of its ability \nto generalize and generate accurate molecules may vary \ndepending on the specific prompt and task.\nDiscussion\nTSMMG as producer\nThe development of TSMMG can be viewed as a form of \nknowledge distillation [25], as depicted in Fig.  4A. Ini -\ntially, diverse molecular properties are obtained using \nteacher models. These properties are then encapsu -\nlated into natural language descriptions and combined \nwith molecular sequences to create text-molecule pairs. \nTSMMG is trained using these text-molecule pairs as \ntraining data, enabling it to acquire the knowledge inher -\nent in the properties through natural language. By lev -\neraging this process, TSMMG becomes proficient in \ngenerating molecules that exhibit the desired proper -\nties. Notably, TSMMG has the ability to generate novel \nmolecules possessing specific properties based on the \nFig. 3 A Docking reference for EP2 and EP4. B Molecules generated by TSMMG that can simultaneously bind to both EP2 and EP4 receptors. \nThe input prompt is: “The molecule exhibits a high QED score, good synthetic accessibility. It can pass through the blood-brain barrier and binds \nto both Prostanoid EP2 and EP4 receptors.” During training, TSMMG has seen molecules that can individually bind to both EP2 and EP4 receptors, \nbut it has not explicitly received molecules that simultaneously satisfy all the constraints in this prompt. Nevertheless, it still successfully generates \nthe desired molecules\n(See figure on next page.)\nPage 9 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \nFig. 3 (See legend on previous page.)\nPage 10 of 17Zhou et al. BMC Biology          (2025) 23:105 \nacquired knowledge. This offers a feedback loop to the \nteacher models, allowing them to refine and update their \nknowledge.\nTo illustrate this, we conducted further experiments \ninvolving serine/threonine-protein kinase D3 (KPCD3), \nBruton’s tyrosine kinase (BTK), fibroblast growth fac -\ntor receptor 4 (FGFR4), and papain-like protease 3CL. \nInitially, each target dataset is randomly divided into \ntraining and test sets. Subsequently, a random subset is \npartitioned from the training dataset to serve as the vali -\ndation set, and an SVM predictor is trained using the \ntraining set and validated using the validation set. This \nprocess is repeated 100 times to select the best predic -\ntor. Finally, the chosen predictor is applied to the test set \nto obtain the F1 score. For comparison, different num -\nbers of molecules generated by TSMMG that can bind \nto the corresponding target are randomly added to the \ntraining set to train new SVM predictors. This process \nis also repeated 100 times to yield consistent statistical \ndata. These added molecules are referred to as pseudo-\nsamples. The experimental results presented in Fig.  4B \ndemonstrate that the addition of pseudo-samples sig -\nnificantly enhances the performance of the predictors. \nNotable improvements are observed in KPCD3, BTK, \nFGFR4, and 3CL by approximately 13%, 4%, 17%, and 7%, \nrespectively. Furthermore, as the number of pseudo-sam-\nples increases, the performance of each predictor tends \nto converge. These results indicate that TSMMG can dis -\ncern the commonalities shared by molecules with specific \nproperties and generate novel molecules that embody \nthese commonalities. Moreover, TSMMG’s unique abil -\nity to learn from teacher models and provide feedback for \nupdating the knowledge of these models initiates a sym -\nbiotic relationship that promotes continuous improve -\nment in their respective capabilities.\nComparison with other methods\nAlthough existing commercial or open-source LLMs \nsuch as GPT-4 [26] and Llama [27] perform well on vari -\nous natural language tasks and exhibit some molecu -\nlar generation capabilities, they struggle to consistently \ngenerate high-quality, novel molecules, especially under \nmultiple constraints. This is primarily because they \nhave not been fine-tuned on specialized molecule data -\nsets. Consequently, we did not consider vanilla LLMs as \nbaselines. Additionally, similar models like ChemLLM \nTable 2 Prompts we used in order to test the tolerance of TSMMG to diverse inputs, [FG] refers to any functional group\nDRD2/GSK3 T0 The molecule contains [FG]. It is active to DRD2. [D2D2/GSK3]\nT1 I want a molecule that contains [FG] and can bind to [D2D2/GSK3]\nT2 Give me a molecule which contains [FG] and can bind to [D2D2/GSK3]\nBBB T0 The molecule contains [FG]. It can pass through the blood-brain barrier\nT1 I want a molecule that contains [FG] and can pass through the blood-brain barrier\nT2 Give me a molecule which contains [FG] and can pass through the blood-brain barrier\nHIA T0 The molecule contains [FG]. It can be absorbed by human intestine\nT1 I want a molecule that contains [FG] and can be absorbed by human intestine\nT2 Give me a molecule which contains [FG] and can be absorbed by human intestine\nTable 3 Experimental results with different template styles\nTemplate Task Valid Unique Novelty Diversity SR SR (nFG)\nT0 FG+DRD2 99.80% 68.48% 92.54% 85.77% 78.04% 93.18%\nFG+GSK3 99.92% 69.79% 92.88% 89.23% 79.44% 94.40%\nFG+BBB 99.82% 95.53% 94.30% 92.05% 79.24% 96.14%\nFG+HIA 99.98% 96.16% 92.64% 91.54% 79.94% 95.80%\nT1 FG+DRD2 99.70% 71.84% 80.42% 86.26% 44.68% 82.12%\nFG+GSK3 99.68% 73.49% 83.90% 89.56% 47.36% 84.88%\nFG+BBB 99.68% 94.30% 95.48% 92.28% 70.64% 96.84%\nFG+HIA 99.78% 94.11% 94.82% 91.99% 69.34% 93.86%\nT2 FG+DRD2 99.80% 70.06% 81.32% 86.18% 48.04% 83.52%\nFG+GSK3 99.68% 71.89% 84.68% 89.52% 50.22% 85.60%\nFG+BBB 99.74% 95.67% 95.40% 92.17% 70.72% 96.52%\nFG+HIA 99.90% 95.24% 95.48% 91.86% 71.10% 94.12%\nPage 11 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \nFig. 4 TSMMG as a producer. Molecules generated by TSMMG can be used to improve the accuracy of the predictor. A We leverage a large number \nof property predictors, which can be regarded as teacher models, to obtain molecular properties, and then use these properties to construct \ntextual descriptions to train TSMMG. The molecules generated by TSMMG can also be used to update the corresponding property predictors. This \nhas two benefits: firstly, it allows us to verify whether TSMMG has effectively extracted the latent representation of the property, and secondly, it can \nimprove the accuracy of the property predictors. B The experimental results on four property predictors are shown in the figure. The horizontal axis \nrepresents the number of generated molecules added to the training data, which we refer to as pseudo-samples. As can be observed, the accuracy \nof the property predictors increases and tends to converge as the number of pseudo-samples increases\nPage 12 of 17Zhou et al. BMC Biology          (2025) 23:105 \n[28] and ChemCrow [29] are also unsuitable for multi-\nconstraint molecular generation tasks. To better demon -\nstrate the capabilities of TSMMG, we present two sets of \ncomparisons in Additional file 1: (1) using a larger open-\nsource LLM, Llama2, fine-tuned with LoRA [30], as the \nbackbone; and (2) comparing traditional multi-constraint \nmolecular generation methods, including Reinvent [31], \nReinvent2 [31], and MCMG [12]. We utilized the rein -\nforcement learning method DPO [32] to further fine-tune \nTSMMG in order to achieve a relatively fairer compari -\nson. The results of Reinvent, Reinvent2, MCMGL, and \nMCMGM are quoted from [33].\nConclusions\nTSMMG presents an effective method for utilizing \nnatural language to generate molecules with multiple \nconstraints. By employing knowledge distillation, the \nproperty prediction capabilities of specialized molecu -\nlar models are transferred to enable LLMs to generate \nmolecules with specific properties. This transforms the \nmolecule screening process into a molecule generation \nprocess, allowing exploration of a larger molecular space. \nWe demonstrate TSMMG’s exceptional molecular gen -\neration capabilities through tasks with two, three, and \nfour constraints, including structure, physicochemical \nproperties, affinity, and ADMET. TSMMG can generate \nnovel molecules that meet specified requirements based \non natural language descriptions. Additionally, TSMMG \nexhibits zero-shot generation capabilities. TSMMG \nshows potential not only in drug discovery but also in \nother molecule-related fields, such as material discovery.\nOn one hand, the capabilities of TSMMG can be \nenhanced as the capabilities of the teacher model \nimprove. On the other hand, the novel molecules gener -\nated by TSMMG have the potential to further enhance \nthe teacher model’s capabilities. Our future work will \nfocus on leveraging more advanced teacher models to \nimprove TSMMG’s capabilities and enabling TSMMG to \ngenerate molecules with a wider range of properties.\nMethods\nProblem setting\nNatural language serves as a user-friendly means for \nhuman-machine interaction, making it an ideal solution \nfor generating molecules from natural language descrip -\ntions. Recent successes in the development of large lan -\nguage models (LLMs) [26, 34] inspire the vision that we \nmay achieve the generation of molecules from diverse \nmolecular spaces by simply modifying the input prompt. \nThis approach offers a promising solution to address the \nchallenge of generality in molecular generation. Despite \nboth natural language and SMILES being sequence \ndata formats, SMILES can be viewed as a specialized \nmolecular language that can be challenging for humans \nto interpret. From this perspective, generating molecu -\nlar sequences from natural language descriptions can be \nregarded as a translation task, an area where LLMs excel.\nGiven a natural language sequence W ={ w1 ,w2 , ...,wn} , \nthe objective is to generate a corresponding molecule \nrepresented by a SMILES sequence, S ={ s1 ,s2 , ...,sm } , \nwhich can be formulated as conditional probability \nP(S|W).\nIn order to ensure the quality of the generated mol -\necules, it is imperative to adhere to the following \nprerequisites: (1) Validity: The generated SMILES rep -\nresentation, S, should strictly adhere to the syntax rules \nof the SMILES format, guaranteeing that it forms a \nvalid and well-structured molecule. (2) Relevance: The \nmolecule represented by S should accurately reflect the \nphysical and chemical properties described by the natu -\nral language sequence W. This entails that if there exists \na subsequence W i,j ={ wi,wi+1 ,... ,wj} in W that speci -\nfies a particular property, there should be a correspond -\ning subsequence Sk ,l ={ sk ,sk +1 ,... ,sl} in S that satisfies \nthe desired property. (3) Diversity: While satisfying the \nvalidity and relevance criteria, the generated S should \nexhibit diversity. In other words, the generated molecules \nshould not be identical or overly similar, providing a \nrange of molecular structures that fulfill the given prop -\nerties. (4) Novelty: The model should possess the ability \nto generate S that are not present in the training set. This \ncapability ensures that the generated molecules introduce \nnew and previously unseen chemical structures, thereby \nexpanding the exploration space beyond the confines of \nthe training data.\nThe quandary of translating natural language into \nmolecular language, albeit bearing resemblances to con -\nventional machine translation, poses distinctive chal -\nlenges. In this context, three fundamental patterns of \ncorrespondence between natural language and molecular \nsequences can be discerned: (1) One-to-one mapping: \nIn this pattern, a specific text description corresponds \nto a single, specific molecular sequence. Models like \nMolT5 [35], MolXPT [36], and MoleculeSTM [37] have \ntackled this problem as a query task, aiming to establish \na direct mapping relationship between text and molecu -\nlar sequences. However, this approach may not be ideal \nfor generating novel molecules with diverse properties, \nas it relies on a fixed ground truth and does not explore \nbeyond the known data. (2) One-to-many mapping: \nHere, a text description can correspond to multiple dif -\nferent molecular sequences. This pattern allows the \nmodel to learn the feature distribution of the target space, \nenabling sampling from the distribution to generate new \nmolecules. Models like those proposed by Kotsias et  al. \n[38] and Wang et al. [12] leverage this pattern effectively \nPage 13 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \nby training on specific datasets containing molecules \nwith shared properties which implicitly embracing the \none-to-many mapping pattern. (3) Many-to-one map -\nping: In this pattern, a specific molecular sequence can \nbe described in various ways. By understanding the \ninherent relationship between different attributes, it is \npossible to discover new properties of a molecule. This \npattern offers opportunities for exploring diverse attrib -\nutes of molecules beyond their known properties. In \norder to develop a universal molecular generative model \ncapable of generating molecules with various desired \nproperties without the need for retraining, it is essential \nto accumulate a substantial amount of data that explicitly \nadheres to the one-to-many and many-to-one mapping \npatterns. The primary challenge lies in acquiring a suffi -\ncient number of text-molecule pairs in a rapid, conveni -\nent, and cost-effective manner.\nData generation framework\nSeveral studies have explored the integration of natural \nlanguage and molecular language. MolT5 [35] aimed to \nachieve bidirectional translation between natural lan -\nguage and molecular language. The model underwent \ninitial pre-training on an extensive collection of unpaired \nnatural language corpora and molecular sequences, fol -\nlowed by fine-tuning on the text-molecule paired data -\nset ChEBI-20. However, ChEBI-20 presents two notable \nlimitations. Firstly, it contains a relatively small set of \n33,010 text-molecule pairs, making it challenging to \nestablish the correspondence between natural language \nand molecular language. Secondly, the text descriptions \nin this dataset, sourced from the comment field in ChEBI \n[39], often contain information unrelated to molecu -\nlar properties. Additionally, these descriptions exhibit \na strong one-to-one relationship with the molecules, \nposing challenges for the model to explore the specific \nmolecular space associated with desired properties. \nMolXPT [36] proposed a method that involves incor -\nporating molecular sequences within the input text for \nlarge language models (LLMs). CLAMP [40] introduced \na fusion approach, combining a molecule encoder and a \ntext encoder for property prediction tasks. Christofidellis \net al. [41] presented a unified model capable of handling \nvarious text-to-text, text-to-molecule, molecule-to-text, \nand molecule-to-molecule tasks. MolReGPT [42] imple -\nmented tasks such as molecule captioning and text-based \nmolecule generation by assigning ChatGPT a role as a \nbiochemist, facilitating in-context learning.\nHowever, a common limitation in all of the above-\nmentioned studies is their reliance on the ChEBI dataset, \nwhich constrains their performance due to data scarcity \nand quality issues. As of now, limited research efforts \nhave been directed at addressing these issues in natural \nlanguage-based molecular generation. Therefore, we \npropose a knowledge distillation-based approach to con -\nstruct an extensive and high-quality dataset of natural \nlanguage-molecule pairs.\nFigure  1A provides an overview of the framework \nemployed for the creation of our dataset. The underly -\ning concept revolves around the utilization of advanced \nmolecular parsing tools and models to extract knowledge \nrelated to molecules. Subsequently, this acquired knowl -\nedge is transformed into natural language text, resulting \nin paired data comprising molecules and their corre -\nsponding textual descriptions. Within this framework, \nthe tools and models responsible for extracting molecular \nknowledge are collectively referred to as “teachers, ” while \nTSMMG assumes the role of the “student. ” TSMMG \nundertakes the task of learning various properties asso -\nciated with molecules from these “teachers. ” It also \ncomprehends the mapping relationship between these \nproperties and the molecular structures themselves. This \nknowledge empowers TSMMG to generate new mol -\necules based on specified properties using natural lan -\nguage descriptions.\nWithin this framework, multiple “teachers” are \nemployed, each with distinct capabilities related to \nmolecular properties and structures. These teachers \nencompass a range of tools and models, including:\n• Physicochemical property teacher: RDKit, a tool \ncapable of parsing molecules to extract physicochem-\nical properties such as molecular weight (MW), the \nnumber of aromatic rings (AROM), LogP , SA, QED, \nthe number of hydrogen bond acceptors (HBA), the \nnumber of hydrogen bond donors (HBD), and topo -\nlogical molecular polar surface area (PSA).\n• ADMET property prediction models: admetSAR \n[43], based on support vector machine (SVM), pre -\ndicts ADMET properties, such as blood-brain barrier \npermeability and absorptivity.\n• Affinity prediction models: Olivecrona et  al. ’s SVM-\nbased models [44] and Jin et al. ’s models [45] can pre-\ndict the binding probabilities of molecules to specific \ntargets, including DRD2, GSK3, and JNK3. Newer \nmodels such as MolTrans [46], DrugBAN [47], and \nTransformerCPI [48] are designed to predict the \naffinity of small molecules to receptor proteins and \nmore.\n• Structural information extraction: In addition to \nthese property-related teachers, the IUPAC name \nof a molecule, which bears structural information, \nis considered. The IUPAC name exhibits a gram -\nmar resembling natural language and provides \nstandardized descriptions of molecules. By break -\ning down IUPAC names, it is possible to extract \nPage 14 of 17Zhou et al. BMC Biology          (2025) 23:105 \nstructural components of a molecule. For instance, \ndeconstructing the molecule “(2-methyl-5-methyl -\nsulfonylphenyl)methanamine”  yields the func -\ntional groups “methyl, ” “methylsulfonylphenyl, ” \nand “methanamine. ” Therefore, an IUPAC parser is \nproposed, along with a set of rules for dissecting \nIUPAC names, serving as an additional “teacher” \nfor extracting the internal structure of molecules.\nThrough these “teachers, ” we acquire extensive knowl -\nedge about molecules, including their structural \ninformation, physicochemical properties, and bind -\ning affinities to specific receptors. This information \nis then transcribed into natural language descriptions \nand combined with the corresponding molecules to \ncreate text-molecule pairs. For an example as shown \nin Fig.  1B, let us consider a molecule represented as \n“CCN1CCCC1CNC(= O)c1c(OC)ccc(Cl)c1O. ” We can \nbreak down its IUPAC name to extract the functional \ngroup “methoxybenzamide. ” By utilizing RDKit, we \ndetermine its LogP , QED, and SAs. We further predict \nits affinity with DRD2 through a classifier proposed \nby Olivecrona et al. [44] and evaluate its likelihood of \npassing through the blood-brain barrier using admet -\nSAR. These various properties are then associated \nwith the molecule using natural language templates. \nThe data generation method offers several notable \nadvantages: (1) With numerous publicly accessible \nmolecular databases like PubChem [49] and ZINC \n[50], our approach allows for the rapid acquisition of \na large number of text-molecule pairs. This effectively \novercomes the data limitations often encountered in \nnatural language-based molecular generation mod -\nels; (2) The text molecule pairs generated through \nthis method exhibit a high degree of relevance. Each \nsegment of text contains certain properties of the \nmolecules, enabling the model to learn the mapping \nrelationship between text descriptions and molecu -\nlar properties more effectively; (3) There is a wealth \nof advanced tools and models available for molecular \nstructure analysis and property prediction. Our frame -\nwork simplifies the process of transferring knowledge \nfrom these advanced tools and models into a student \nmodel in natural language form. This empowers the \nstudent model to generate molecules that incorpo -\nrate this knowledge. (4) The method is highly scal -\nable, allowing for the seamless transfer of knowledge \nfor various molecular properties. It can be applied to \nan array of properties, making it versatile for different \nresearch needs. (5) Our method supports continuous \nknowledge updates. This means that the student model \ncan benefit from the latest and more robust models, \nensuring that it remains up-to-date and well-informed.\nTraining model\nWe began by collecting 2 million molecules from \nPubChem. Subsequently, we harnessed the tools and \nmodels mentioned earlier to extract comprehensive \nknowledge regarding these molecules. This knowledge \nwas then translated into natural language text using pre -\ndefined templates and combined with the corresponding \nSMILES representations. To maximize the model’s capa -\nbilities, we thoughtfully organized the data to encom -\npass both one-to-many and many-to-one patterns. This \napproach ensures that the model learns the underly -\ning distribution of specific inputs, promoting adapt -\nability and preventing the mere memorization of fixed \nresponses. For instance, let us take molecule M, which \npossesses ten pieces of extracted knowledge. If we were \nto compile all ten pieces into a single text, denoted as T, \nthe resulting molecule space associated with T would \nlikely be highly restricted, potentially corresponding to \njust one specific molecule, let us say, molecule A. This \nwould essentially create a one-to-one data pattern. To \novercome this limitation, we adopt a strategy where, for \neach molecule, we select a subset of its knowledge to \ncompose the text. The goal here is to craft this text in a \nway that it corresponds to as many molecules as possi -\nble. This strategy empowers the model to gain insights \ninto the broader distribution of molecules linked to the \nprovided text, rather than locking it into a specific, iso -\nlated instance. Then, the training of TSMMG involves \ntwo key steps: pre-training on a large natural language \ncorpus and fine-tuning on text-molecule paired data that \nwe have constructed. In the first stage, TSMMG under -\ngoes pre-training on a large natural language corpus. \nThis enables TSMMG to learn and understand natural \nlanguage by capturing the statistical patterns and lin -\nguistic structures present in the data. The pre-training \nstage helps TSMMG acquire a general understanding of \nlanguage and forms the initial foundation for subsequent \ntraining stages. The second stage involves a fine-tuning \non the text-molecule paired data that contains descrip -\ntions of various properties as shown in Table 3. This fine-\ntuning stage focuses on teaching TSMMG the mapping \nbetween text descriptions and molecular sequences as \nwell as the syntax of SMILES. By fine-tuning TSMMG \non this specific dataset, it becomes proficient in generat -\ning molecules based on specific text-described-property \nsuch as functional groups, LogP , physicochemical prop -\nerties, drug-like properties, and affinity scores to certain \ntargets. The architecture of TSMMG is the same as GPT \n[24]. TSMMG follows the settings of GPT2small, which \nconsists of 12 layers and has a total of 117 million param-\neters. We downloaded the weights of GPT2small from \nHuggingface model repository [51] to initialize TSMMG. \nThis helps with cost and computational considerations \nPage 15 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \nby leveraging pre-trained weights for an efficient start -\ning point. And since the weights are trained by a large \nnumber of language corpus, we can directly fine-tune the \nmodel using the text-molecule paired data we construct. \nWe fine-tune TSMMG on 8 A100 40G GPUs for around \n6 days. We use the subsequent hyperparameters: a batch \nsize of 32, a learning rate set to 5e −4, a warmup steps of \n100. We use AdamW [52] as the optimizer.\nMetrics\nTo evaluate the performance of the TSMMG model, we \nemployed four common metrics in molecular generation: \nvalidity, uniqueness, novelty, and diversity. Each of these \nmetrics was essential for a comprehensive evaluation: \nValidity assesses whether the generated molecules con -\nform to the syntax rules of SMILES. We utilized RDKit \n[53] to parse the generated molecules, considering them \nvalid if the parsing process was successful. Uniqueness \nmeasures the proportion of non-repetitive molecules \namong the generated set. It ensures that the model pro -\nduces diverse molecules. Novelty signifies whether the \ngenerated molecules are previously unseen in the train -\ning dataset, preventing the model from regenerating \nknown molecules. Diversity describes the structural dif -\nferences between generated molecules, it is calculated as:\nwhere sim(X,Y) is calculated based on the Tanimoto dis -\ntance with respect to the Morgan fingerprints of gener -\nated molecules X and Y. In addition to these standard \nmetrics, we introduced the concept of success ratio (SR) \nto measure whether the generated molecules meet prede-\nfined conditions. We establish different criteria to define \nthe success of generated molecules based on the specific \ntask. These criteria are outlined as follows:\n• FG: We leveraged IUPAC nomenclature to identify \nfunctional groups within the molecules. By pars -\ning IUPAC names and matching them to generated \nSMILES-encoded molecules, we checked if the gen -\nerated molecules contained the specified functional \ngroups.\n• LogP: Using RDKit, we calculated the LogP values \nof the generated molecules and compared them to \npredefined values. The generation was considered \nsuccessful if the LogP value fell within a margin of 1 \nfrom the specified value.\n• QED and SAs: For these tasks, we adopted criteria \nsimilar to prior work [ 12], considering QED as high \nif its value exceeded 0.6 and SAs as good if the score \nwas less than 4.\nDiversity= 1 − 2\nn(n − 1) X ,Y sim(X ,Y ) ,\n• DRD2 and GSK3: We employed the models proposed \nby Jin et  al. [ 45] to predict the affinity scores of the \ngenerated molecules. A molecule was considered \nsuccessful if its corresponding affinity score exceeded \n0.5 for either target.\n• BBB and HIA: We used models developed by Cheng \net al. [43] to predict scores, determining if a molecule \ncould pass through the blood-brain barrier (BBB) if \nits BBB score exceeded 0.5 or if it could be absorbed \nby the human small intestine (HIA) if its HIA score \nwas above 0.5.\nMoreover, for each multi-constraint task, we only con -\nsidered molecules successful if they met all constraints \ncontained in this task simultaneously. We generated 5000 \nmolecules to evaluate the model’s performance for each \nmulti-constraint task. Note that we uniformly express \nall metric results in percentage format. While convert -\ning diversity to a percentage may lack intrinsic meaning, \nfor the sake of ease of comparison with other metrics, we \nmultiply it by 100. However, we refrain from appending \nthe “%” symbol to distinguish it from other metrics.\nTranslating SMILES to IUPAC\nThere are several open works that provided their solu -\ntions for translating SMILES to IUPAC name, such as \nSTOUT [ 54] and IUPAC2Struct [ 55], but the inter -\nfaces they released are not for high throughput experi -\nments. Considering experimental efficiency, we trained \nour own SMILES2IUPAC model based on GPT2small. \nWe formulate this problem also as conditional prob -\nability P(I|S) where generating a corresponding IUPAC \nname I ={ i1 ,i2 ,... ,in } by a given SMILES sequence \nS ={ s1 ,s2 ,... ,sm } . We collect 2 million SMILES-IUPAC \npaired data from PubChem to train this model. The \nmodel size and settings of SMILES2IUPAC are the same \nas TSMMG. For evaluating the trained SMILES2IUPAC \nmodel, we pass 1000 unseen molecules to it and gener -\nate 1000 corresponding predicted IUPAC names. We \nthen break down the predicted IUPAC names to identify \nthe functional groups, and check if all these functional \ngroups exist in the corresponding ground truth IUPAC \nnames. The experimental results show an accuracy rate \nof 94%.\nAbbreviations\nTSMMG  Teacher-student-based multi-constraint molecular generation \nmodel\nAIDD  Artificial intelligence for drug discovery\nQED  Quantitative estimate of drug-likeness\nLogP  Molecular hydrophobicity\nSA  Synthetic accessibility\nFG  Functional group\nDRD2  Dopamine type 2 receptor\nGSK3  Glycogen synthase kinase-3 beta\nBBB  Blood-brain barrier\nPage 16 of 17Zhou et al. BMC Biology          (2025) 23:105 \nHIA  Human intestinal absorption\nKPCD3  Serine/threonine-protein kinase D3\nBTK  Bruton’s tyrosine kinase\nFGFR4  Fibroblast growth factor receptor 4\n3CL  Papain-like protease 3CL\nSMILES  Simplified Molecular Input Line Entry System\nSupplementary information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12915- 025- 02200-3.\nAdditional file 1: Figure S1-A compares the use of GPT-2 and Llama-7b as \nbackbone LLMs; Figure S1-B compares TSMMG with common multi-\nconstraint molecular generation models, including Reinvent, Reinvent2, \nand MCMG. Figure S2 showcases some molecules generated by TSMMG. \nFigure S3 illustrates the QED distribution of molecules generated under \ndifferent conditions. Figure S4 displays the LogP distribution of molecules \ngenerated with specified LogP values. Figure S5 shows the weight of each \ntoken during the molecular generation process. Figure S6 demonstrates \nthe scaffold similarity among the generated molecules. Figure S7 presents \nthe convergence of models using different learning rates. Figure S8 high-\nlights cases of generation failures. Figures S9, S10, and S11 respectively \nshow docking examples of molecules generated for targets 3CL, BTK, and \nFGFR4. Figure S12 further illustrates the impact of FG frequency on the \nuniqueness of generated molecules. Table S1 explains the relationships \nbetween the metrics used. Tables S2, S3, and S4 provide additional experi-\nmental parameter information.\nAdditional file 2. Contains the source data corresponding to the figures \nand tables.\nAuthors’ contributions\nL.W., YS.L., and X.Z. conceived the study of TSMMG and the experimental \nassays. P .Z. developed TSMMG. P .Z., J.W., C.L., and Z.W. performed all experi-\nments. X.Z., YS.L., L.W., and P .Z. drafted the manuscript and charts. Y.L., C.L., \nS.S., J.L., L.W., X.C., H.L., and W.L. critically revised the manuscript. All authors \ncritically revised and gave final approval of the manuscript.\nFunding\nThis work was supported by the National Science and Technology Major \nProject (2023ZD0120902), the National Natural Science Foundation of China \n(U22A2037, 62425204, 62122025, 62450002, 62432011, 62372159), and the \nBeijing Natural Science Foundation (L248013).\nData availability\nSource data and codes are provided with this paper. The data and source code \nused in this project are freely available at GitHub (https:// github. com/ HHW- \nzhou/ TSMMG) and Zenodo (https:// doi. org/ 10. 5281/ zenodo. 15093 636).\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 10 October 2024   Accepted: 27 March 2025\nReferences\n 1. Schwalbe-Koda D, Gómez-Bombarelli R. Generative models for automatic \nchemical design. Machine Learning Meets Quantum Physics. 2020. p. \n445–467.\n 2. Gainza P , Sverrisson F, Monti F, Rodola E, Boscaini D, Bronstein MM, et al. \nDeciphering interaction fingerprints from protein molecular surfaces \nusing geometric deep learning. Nat Methods. 2020;17(2):184–92.\n 3. Wójcikowski M, Kukiełka M, Stepniewska-Dziubinska MM, Siedlecki P . \nDevelopment of a protein-ligand extended connectivity (PLEC) finger-\nprint and its application for binding affinity predictions. Bioinformatics. \n2019;35(8):1334–41.\n 4. Mahmoud AH, Masters MR, Yang Y, Lill MA. Elucidating the multiple roles \nof hydration for accurate protein-ligand binding prediction via deep \nlearning. Commun Chem. 2020;3(1):19.\n 5. Jones D, Kim H, Zhang X, Zemla A, Stevenson G, Bennett WD, et al. \nImproved protein-ligand binding affinity prediction with structure-based \ndeep fusion inference. J Chem Inf Model. 2021;61(4):1583–92.\n 6. Cheng F, Wang F, Tang J, Zhou Y, Fu Z, Zhang P , et al. Artificial intelligence \nand open science in discovery of disease-modifying medicines for Alzhei-\nmer’s disease. Cell Rep Med. 2024;5(2):101379.\n 7. Qiu Y, Cheng F. Artificial intelligence for drug discovery and development \nin Alzheimer’s disease. Curr Opin Struct Biol. 2024;85:102776.\n 8. Zang C, Wang F. Moflow: an invertible flow model for generating \nmolecular graphs. In: Proceedings of the 26th ACM SIGKDD international \nconference on knowledge discovery & data mining. Virtual Event, CA, \nUSA: Association for Computing Machinery; 2020. p. 617–26.\n 9. Kuznetsov M, Polykovskiy D. MolGrow: a graph normalizing flow for hier-\narchical molecular generation. In: Proceedings of the AAAI conference on \nartificial intelligence. Virtual Event: Association for the Advancement of \nArtificial Intelligence; 2021. p. 8226–34.\n 10. Zhavoronkov A, Ivanenkov YA, Aliper A, Veselov MS, Aladinskiy VA, Aladin-\nskaya AV, et al. Deep learning enables rapid identification of potent DDR1 \nkinase inhibitors. Nat Biotechnol. 2019;37(9):1038–40.\n 11. Gottipati SK, Sattarov B, Niu S, Pathak Y, Wei H, Liu S, et al. Learning to \nnavigate the synthetically accessible chemical space using reinforcement \nlearning. In: International conference on machine learning. Online: PMLR; \n2020. p. 3668–79.\n 12. Wang J, Hsieh CY, Wang M, Wang X, Wu Z, Jiang D, et al. Multi-constraint \nmolecular generation based on conditional transformer, knowledge dis-\ntillation and reinforcement learning. Nat Mach Intell. 2021;3(10):914–22.\n 13. Xie Y, Shi C, Zhou H, Yang Y, Zhang W, Yu Y, et al. MARS: Markov molecular \nsampling for multi-objective drug discovery. In: International conference \non learning representations. Virtual Event: International Conference on \nLearning Representations; 2021.\n 14. Li Y, Zhang L, Liu Z. Multi-objective de novo drug design with conditional \ngraph generative model. J Cheminformatics. 2018;10:1–24.\n 15. Jin W, Barzilay R, Jaakkola T. Multi-objective molecule generation using \ninterpretable substructures. In: International conference on machine \nlearning. Online: PMLR; 2020. p. 4849–59.\n 16. Bagal V, Aggarwal R, Vinod P , Priyakumar UD. MolGPT: molecular \ngeneration using a transformer-decoder model. J Chem Inf Model. \n2021;62(9):2064–76.\n 17. Weininger D. SMILES, a chemical language and information system. 1. \nIntroduction to methodology and encoding rules. J Chem Inf Comput \nSci. 1988;28(1):31–6.\n 18. Qu C, Mao C, Xiao P , Shen Q, Zhong YN, Yang F, et al. Ligand recognition, \nunconventional activation, and G protein coupling of the prostaglandin \nE2 receptor EP2 subtype. Sci Adv. 2021;7(14):eabf1268.\n 19. Toyoda Y, Morimoto K, Suno R, Horita S, Yamashita K, Hirata K, et al. Ligand \nbinding to human prostaglandin E receptor EP4 at the lipid-bilayer inter-\nface. Nat Chem Biol. 2019;15(1):18–26.\n 20. Pettersen EF, Goddard TD, Huang CC, Couch GS, Greenblatt DM, Meng EC, \net al. UCSF Chimera-a visualization system for exploratory research and \nanalysis. J Comput Chem. 2004;25(13):1605–12.\n 21. Allen WJ, Balius TE, Mukherjee S, Brozell SR, Moustakas DT, Lang PT, et al. \nDOCK 6: impact of new features and current docking performance. J \nComput Chem. 2015;36(15):1132–56.\nPage 17 of 17\nZhou et al. BMC Biology          (2025) 23:105 \n \n 22. Adasme MF, Linnemann KL, Bolz SN, Kaiser F, Salentin S, Haupt VJ, et al. \nPLIP 2021: expanding the scope of the protein-ligand interaction profiler \nto DNA and RNA. Nucleic Acids Res. 2021;49(W1):W530–4.\n 23. Schrödinger. Pymol. https:// github. com/ schro dinger/ pymol- open- \nsource. Accessed 19 Aug 2024.\n 24. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, et al. Language \nmodels are unsupervised multitask learners. OpenAI blog. 2019;1(8):9.\n 25. Gou J, Yu B, Maybank SJ, Tao D. Knowledge distillation: a survey. Int J \nComput Vis. 2021;129(6):1789–819.\n 26. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. Gpt-4 \ntechnical report. 2023. Preprint at https:// arxiv. org/ abs/ 2303. 08774.\n 27. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al. \nLlama: open and efficient foundation language models. 2023. Preprint at \nhttps:// arxiv. org/ abs/ 2302. 13971.\n 28. Di Z, Wei L, Qian T, Jingdan C, Hang Y, Yuliang Y, et al. ChemLLM: a chemi-\ncal large language model. 2024. Preprint at https:// arxiv. org/ abs/ 2302. \n13971.\n 29. M Bran A, Cox S, Schilter O, Baldassari C, White AD, Schwaller P . Aug-\nmenting large language models with chemistry tools. Nat Mach Intell. \n2024;6:525–35.\n 30. Hu EJ, Shen Y, Wallis P , Allen-Zhu Z, Li Y, Wang S, et al. Lora: low-rank \nadaptation of large language models. ICLR. 2022;1(2):3.\n 31. Blaschke T, Arús-Pous J, Chen H, Margreitter C, Tyrchan C, Engkvist O, \net al. REINVENT 2.0: an AI tool for de novo drug design. J Chem Inf Model. \n2020;60(12):5918–22.\n 32. Rafailov R, Sharma A, Mitchell E, Manning CD, Ermon S, Finn C. Direct \npreference optimization: your language model is secretly a reward \nmodel. Adv Neural Inf Process Syst. 2023;36:53728–41.\n 33. Wang J, Hsieh CY, Wang M, Wang X, Wu Z, Jiang D, et al. Multi-constraint \nmolecular generation based on conditional transformer, knowledge \ndistillation and reinforcement learning, Table 3. 2021. https:// www. nature. \ncom/ artic les/ s42256- 021- 00403-1/ tables/3. Accessed 19 Aug 2024.\n 34. OpenAI. Introducing ChatGPT. 2022. https:// openai. com/ index/ chatg pt/. \nAccessed 19 Aug 2024.\n 35. Edwards C, Lai T, Ros K, Honke G, Cho K, Ji H. Translation between mol-\necules and natural language. In: Proceedings of the 2022 conference on \nempirical methods in natural language processing. Abu Dhabi: Associa-\ntion for Computational Linguistics; 2022. p. 375–413.\n 36. Liu Z, Zhang W, Xia Y, Wu L, Xie S, Qin T, et al. MolXPT: wrapping mol-\necules with text for generative pre-training. In: Proceedings of the 61st \nannual meeting of the Association for Computational Linguistics (volume \n2: short papers). Toronto: Association for Computational Linguistics; 2023. \np. 1606–16.\n 37. Liu S, Nie W, Wang C, Lu J, Qiao Z, Liu L, et al. Multi-modal molecule \nstructure-text model for text-based retrieval and editing. Nat Mach Intell. \n2023;5(12):1447–57.\n 38. Kotsias PC, Arús-Pous J, Chen H, Engkvist O, Tyrchan C, Bjerrum EJ. Direct \nsteering of de novo molecular generation with descriptor conditional \nrecurrent neural networks. Nat Mach Intell. 2020;2(5):254–65.\n 39. Degtyarenko K, De Matos P , Ennis M, Hastings J, Zbinden M, McNaught A, \net al. ChEBI: a database and ontology for chemical entities of biological \ninterest. Nucleic Acids Res. 2007;36(suppl_1):D344–D350.\n 40. Seidl P , Vall A, Hochreiter S, Klambauer G. Enhancing activity prediction \nmodels in drug discovery with the ability to understand human lan-\nguage. In: International conference on machine learning. Hawaii: PMLR; \n2023. pp. 30458–90.\n 41. Christofidellis D, Giannone G, Born J, Winther O, Laino T, Manica M. \nUnifying molecular and textual representations via multi-task language \nmodelling. In: International conference on machine learning. Hawaii: \nPMLR; 2023. p. 6140–57.\n 42. Li J, Liu Y, Fan W, Wei XY, Liu H, Tang J, et al. Empowering molecule \ndiscovery for molecule-caption translation with large language models: a \nchatgpt perspective. IEEE Trans Knowl Data Eng. 2024;36(11):6071–83.\n 43. Cheng F, Li W, Zhou Y, Shen J, Wu Z, Liu G, et al. admetSAR: a comprehen-\nsive source and free tool for assessment of chemical ADMET properties. J \nChem Inf Model. 2012;52(11):3099–3105.\n 44. Olivecrona M, Blaschke T, Engkvist O, Chen H. Molecular de-novo design \nthrough deep reinforcement learning. J Cheminformatics. 2017;9:1–14.\n 45. Jin W, Barzilay R, Jaakkola T. Multi-objective molecule generation using \ninterpretable substructures. Int Conf Mach Learn. 2020:4849–59.\n 46. Huang K, Xiao C, Glass LM, Sun J. MolTrans: molecular interaction \ntransformer for drug-target interaction prediction. Bioinformatics. \n2021;37(6):830–6.\n 47. Bai P , Miljković F, John B, Lu H. Interpretable bilinear attention network \nwith domain adaptation improves drug-target prediction. Nat Mach \nIntell. 2023;5(2):126–36.\n 48. Chen L, Tan X, Wang D, Zhong F, Liu X, Yang T, et al. TransformerCPI: \nimproving compound-protein interaction prediction by sequence-based \ndeep learning with self-attention mechanism and label reversal experi-\nments. Bioinformatics. 2020;36(16):4406–14.\n 49. Kim S, Chen J, Cheng T, Gindulyte A, He J, He S, et al. PubChem 2023 \nupdate. Nucleic Acids Res. 2023;51(D1):D1373–80.\n 50. Irwin JJ, Tang KG, Young J, Dandarchuluun C, Wong BR, Khurelbaatar M, \net al. ZINC20-a free ultralarge-scale chemical database for ligand discov-\nery. J Chem Inf Model. 2020;60(12):6065–73.\n 51. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. Transform-\ners: state-of-the-art natural language processing. In: Proceedings of the \n2020 conference on empirical methods in natural language processing: \nsystem demonstrations. Online: Association for Computational Linguis-\ntics; 2020. p. 38–45.\n 52. Loshchilov I, Hutter F. Decoupled weight decay regularization. In: Interna-\ntional conference on learning representations. New Orleans: International \nConference on Learning Representations; 2019.\n 53. Landrum G. Rdkit documentation. Release. 2013;1(1–79):4.\n 54. Rajan K, Zielesny A, Steinbeck C. STOUT: SMILES to IUPAC names using \nneural machine translation. J Cheminformatics. 2021;13(1):34.\n 55. Krasnov L, Khokhlov I, Fedorov MV, Sosnin S. Transformer-based artificial \nneural networks for the conversion between chemical notations. Sci Rep. \n2021;11(1):14798.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Biology",
  "concepts": [
    {
      "name": "Biology",
      "score": 0.9220501184463501
    },
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.6631595492362976
    },
    {
      "name": "Mathematics education",
      "score": 0.48765504360198975
    },
    {
      "name": "Computational biology",
      "score": 0.4526325762271881
    },
    {
      "name": "Genetics",
      "score": 0.324892520904541
    },
    {
      "name": "Mathematics",
      "score": 0.08810397982597351
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16609230",
      "name": "Hunan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I193775966",
      "name": "Yonsei University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I120825670",
      "name": "Yunnan Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I146399215",
      "name": "University of Tsukuba",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I49835588",
      "name": "Macao Polytechnic University",
      "country": "MO"
    },
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ]
}