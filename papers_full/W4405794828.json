{
    "title": "MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices",
    "url": "https://openalex.org/W4405794828",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2161209269",
            "name": "Zhaode Wang",
            "affiliations": [
                "Alibaba Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2299423005",
            "name": "Jingbang Yang",
            "affiliations": [
                "Alibaba Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2434034904",
            "name": "Xinyu Qian",
            "affiliations": [
                "Alibaba Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2563766439",
            "name": "Shiwen Xing",
            "affiliations": [
                "Alibaba Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2789618779",
            "name": "Xiaotang Jiang",
            "affiliations": [
                "Alibaba Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2987660154",
            "name": "Chengfei Lv",
            "affiliations": [
                "Alibaba Group (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2141617212",
            "name": "Shengyu Zhang",
            "affiliations": [
                "Zhejiang University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2790484902"
    ],
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance across a variety of tasks. However, their substantial scale leads to significant computational resource consumption during inference, resulting in high costs. Consequently, edge device inference presents a promising solution. The primary challenges of edge inference include memory usage and inference speed. This paper introduces MNN-LLM, a framework specifically designed to accelerate the deployment of large language models on mobile devices. MNN-LLM addresses the runtime characteristics of LLMs through model quantization and DRAM-Flash hybrid storage, effectively reducing memory usage. It rearranges weights and inputs based on mobile CPU instruction sets and GPU characteristics while employing strategies such as multicore load balancing, mixed-precision floating-point operations, and geometric computations to enhance performance. Notably, MNN-LLM achieves up to a 8.6x speed increase compared to current mainstream LLM-specific frameworks.",
    "full_text": "arXiv:2506.10443v1  [cs.LG]  12 Jun 2025\nMNN-LLM: A Generic Inference Engine for Fast Large Language\nModel Deployment on Mobile Devices\nZhaode Wang\nAlibaba Group\nBeijing, China\nzhaode.wzd@taobao.com\nJingbang Yang\nAlibaba Group\nHangzhou, China\njingbang.yjb@taobao.com\nXinyu Qian\nAlibaba Group\nHangzhou, China\nqianxinyu.qxy@taobao.com\nShiwen Xing\nAlibaba Group\nHangzhou, China\ntianbu.xsw@taobao.com\nXiaotang Jiang\nAlibaba Group\nHangzhou, China\nxiaotang.jxt@taobao.com\nChengfei Lvâˆ—\nAlibaba Group\nHangzhou, China\nchengfei.lcf@taobao.com\nShengyu Zhang\nZhejiang University\nHangzhou, China\nsy_zhang@zju.edu.cn\nAbstract\nLarge language models (LLMs) have demonstrated exceptional per-\nformance across a variety of tasks. However, their substantial scale\nleads to significant computational resource consumption during\ninference, resulting in high costs. Consequently, edge device infer-\nence presents a promising solution. The primary challenges of edge\ninference include memory usage and inference speed. This paper\nintroduces MNN-LLM, a framework specifically designed to accel-\nerate the deployment of large language models on mobile devices.\nMNN-LLM addresses the runtime characteristics of LLMs through\nmodel quantization and DRAM-Flash hybrid storage, effectively\nreducing memory usage. It rearranges weights and inputs based on\nmobile CPU instruction sets and GPU characteristics while employ-\ning strategies such as multicore load balancing, mixed-precision\nfloating-point operations, and geometric computations to enhance\nperformance. Notably, MNN-LLM achieves up to a 8.6x speed in-\ncrease compared to current mainstream LLM-specific frameworks.\n1 Introduction\nIn recent years, large language models (LLMs) have rapidly evolved,\nbecoming one of the most revolutionary technologies in the field of\nnatural language processing. Prominent models like ChatGPT-4o\n[18] and QwenMax [7], due to their substantial parameter scales, are\ntypically deployed in the cloud using GPU inference. However, the\nlarge parameter size and computational demands lead to high user\ncosts, and utilizing cloud services may involve handling sensitive\nuser data, raising privacy and security concerns. In response to\nthese issues, the trend of deploying LLMs on mobile devices is\ngaining momentum [19]. While mobile devices are widely used,\ntheir memory and computational limitations make it challenging\nto run large-scale LLMs directly.\nTo address this challenge, many open-source pre-trained LLMs\nhave released smaller-scale models tailored for edge environments.\nThese models share the same architecture as their cloud counter-\nparts but have fewer parameters, such as Qwen2-1.5B [ 27]. Ac-\ncording to the Scaling Law [ 16], smaller models exhibit limited\ncapabilities and often can only perform simple or specific tasks.\nHowever, with the advent of models like phi-1 [ 11], it has been\nâˆ— Chengfei Lv is the corresponding author.\nMM Asia 2024\n.\nshown that the quality of training data can significantly enhance\nthe capabilities of smaller parameter models. Additionally, rein-\nforcement learning algorithms in models like ChatGPT-o1 [18] can\nfurther improve LLM performance without increasing the parame-\nter count. As a result, the gap between smaller and larger models is\ngradually narrowing; for instance, the 3B parameter Qwen2.5-3B\n[24] achieved a score of 64 on the MMLU benchmark, surpassing\nmany 30B parameter predecessors. This rapid enhancement in LLM\ncapabilities offers greater feasibility for deploying LLMs on mobile\ndevices.\nDespite the reduction in parameter scale, the computational\ndemands of LLMs remain substantial compared to traditional com-\nputer vision models used for edge inference. Running LLMs smoothly\non edge devices poses significant challenges due to hardware, mem-\nory, and computational constraints. To address this issue, several\nframeworks for deploying LLMs on mobile devices have emerged.\nSome frameworks, such as PowerInfer-2 [ 26] and LLM in Flash\n[1], require modifications to the model for compatibility. Others,\nlike llama.cpp [10], MLC-LLM [23], and fastllm [29], can be used\ndirectly with a focus on LLMs.\nThis paper introduces MNN-LLM, a generic mobile inference\nframework that supports LLM inference as well as the deployment\nof various deep learning models. MNN-LLM is developed based on\nMNN [15], a general framework designed for executing deep learn-\ning model inference on mobile devices. It addresses the challenges\nposed by large-scale LLMs through targeted optimizations in model\nexport, quantization, and computational graph optimization. Fur-\nthermore, MNN-LLM analyzes the inference process and employs\nvarious forms of combined quantization, utilizing DRAM-Flash\nhybrid storage to reduce runtime memory usage. By optimizing\nhigh-computation operators and rearranging data according to the\ncharacteristics of different hardware, MNN-LLM ensures optimal\nutilization of edge computing resources.\n2 Background and Motivation\n2.1 LLM Model and Inference\nCurrently, mainstream LLMs primarily adopt a Decoder-Only ar-\nchitecture, with the main parameters located in the Embedding and\nLinear operators. During inference, the operators that consume the\nmost time are Linear and Attention.\n1\nThe inference process can be divided into two phases: prefill and\ndecode. The prefill phase refers to the computation of the input text,\nprocessing the userâ€™s input text sequence to generate the first token.\nThe decode phase involves generating text, where each decode\noperation produces one token until a termination token is generated.\nThese two phases exhibit different computational characteristics;\nspecifically, the prefill phase tends to be computation-bound, while\nthe decode phase is memory-bound due to the typical computational\nthroughput and memory bandwidth of edge devices.\nIn the inference phase of the Attention mechanism, there are\nthree inputs: query, key, and value. During the decode phase, only\nthe query generated from the current input token is needed; how-\never, all previously computed keys and values are required. To\nreduce computational load and avoid redundant calculations, a key-\nvalue (KV) cache is typically employed to store the keys and values\nfrom prior computations.\n2.2 Mobile Devices Analysis\nMobile devices often have limited memory, and the large parameter\nsizes of LLMs can lead to significant memory usage, especially as\ncontext length increases, resulting in memory shortages that may\nterminate processes. Although Flash memory has much slower read\nspeeds than DRAM, its higher capacity makes it crucial for LLM\ninference.\nMobile devices utilize CPUs and GPUs, typically featuring multi-\nple cores and often following a big.LITTLE [2] architecture, neces-\nsitating concurrent optimization during CPU development. Varia-\ntions in instruction sets across CPUs require tailored optimizations\nfor optimal performance. General-purpose computing on mobile\nGPUs typically employs Vulkan and OpenCL standards, allowing\ndevelopers to parallelize tasks using these APIs. Hardware drivers\nmanage instruction dispatch and task scheduling, enhancing code\nportability across platforms.\n3 MNN-LLM Overview\nMNN-LLM, built on the deep learning framework MNN, leverages\nMNNâ€™s extensive operator set and model supported versatility to\nenhance flexibility and adaptability. Unlike LLM-specific inference\nengines, MNN-LLM supports a wider range of models, boosting\nusability and developer friendliness in edge computing contexts.\nMNN provides robust support for computer vision (CV) models\nlike MobileNet [13] and YOLO [22], which are typically exported to\nONNX [8] before conversion to MNN format. While MNN-LLM uses\nthese export and conversion processes, the large parameter sizes\nof LLMs can result in high memory usage and longer conversion\ntimes. To mitigate this, optimizations have been introduced: Linear\noperators are replaced with custom operators during graph export,\nallowing ONNX export to focus on the computation graph without\nparameters. After model export, conversion, and optimization, pa-\nrameters can be handled separately, streamlining the process and\nleveraging MNNâ€™s model format. Additionally, during model con-\nversion, optimizations such as RMSNorm [28] fusion and Attention\nfusion are applied. The computation graph also supports the run-\ntime loading of LoRA [14] weights, enabling seamless integration\nof LoRA models without requiring external implementations in the\ninference framework.\nMNN-LLM provides robust quantization capabilities, supporting\nboth integer (int) and floating-point (fp) quantization during the\nmodel conversion phase, as well as quantization of activation values\nand KV cache during runtime. Additionally, it supports other quan-\ntization algorithms, such as GPTQ[9], and allows for the import of\nquantized weights.\nMNN-LLM performs extensive optimizations for memory and\ncomputation at runtime. To address the significant memory usage\nof LLMs, it employs methods such as DRAM-Flash hybrid stor-\nagea and combined quantization. For the high computational load,\nstrategies like data reorder tailored for hardware, multicore load\nbalancing, mixed-precision floating-point operations, and geometry\ncomputations are utilized. Additionally, specific optimizations are\nimplemented for multi-LoRA scenarios.\n4 Memory Optimization\n4.1 DRAM-Flash Hybrid Storage\nFigure 1: DRAM-Flash Hybrid Storage for LLM model param-\neters and KV cache.\nThe primary bottleneck for deploying large LLM models on mo-\nbile devices lies in the limitations of DRAM. MNN-LLM employs\na DRAM-Flash hybrid storage strategy to mitigate memory usage,\nensuring minimal memory occupancy while maintaining the us-\nability of LLM inference under constrained memory conditions.\nAlthough Flash storage has a larger capacity than DRAM, its read\nspeeds are significantly slower; for instance, LPDDR5X achieves\napproximately 58 GB/s, while UFS 4.0 ranges from about 450 MB/s\nto 3 GB/s [26]. This means that DRAM can be 19 to 130 times faster\nthan Flash. While hybrid storage can reduce memory demands\nand enhance usability, it may compromise inference performance.\nAs shown in Figure 1, MNN-LLMâ€™s hybrid storage strategy is tai-\nlored to the operational characteristics of the model: for parameter\nstorage, it assesses utilization rates and allocates low-utilization\nparameters to Flash to minimize speed impact. For the KV data,\nprefetching techniques are employed to reduce the latency of Flash\nreads, thereby mitigating their effect on performance.\nThe large parameter scale of LLM models is a primary reason\nfor their high memory consumption. Structurally, the parameters\ncan be divided into three categories: Embedding, Layer, and Lm\nhead. The size of the Embedding and Lm headparameters is gen-\nerally calculated as vocabulary sizeÃ—hidden size, and since the vo-\ncabulary size is usually large, the Embedding parameters do not\n2\nTable 1: Qwen2 7B Model Params\nConfiguration Size\nvocabulary size 151646\nhidden size 3584\nintermediate size 18944\nlayers 28\nParams Size\nEmbedding 1.09 B\nLayers 4.89 B\nLm head 1.09 B\nTotal 7.07 B\nparticipate in calculations like other parameters do. Layer refer\nto the parameters in each Decoder Layer, including the Attention\nand MLP Linear layers, typically sized at hidden sizeÃ—hidden size\nor intermediate sizeÃ—hidden sizewith consistent parameter scales\nacross layers. As shown in Table 1, in the Qwen2 7B [ 27] model,\nthe non-computational Embedding parameters account for about\n15% of the total parameters.\nIn the decode phase, each input consists of the previously gener-\nated token. Leading to a computational process that necessitates\nloading 1/vocabulary size of the Embedding parameters, along with\nfull Layer and Lm headparameters. Thus,Layer and Lm headparam-\neters should be prioritized for DRAM storage, while the Embedding\nparameters can be stored in Flash. Taking Qwen2 7B as an example,\nwith Embedding data read in bfloat16 format, the decode phase only\nrequires the Embedding value for one token, resulting in a data size\nof 7 KB for each decode. The UFS 4.0 read speed is approximately\n15ğœ‡ğ‘  slower than LPDDR5X. In contrast, loading non-Embedding\nparameters from memory takes about 103 ms. In typical mobile\ndevices, the compute characteristics during the decode phase are\nMemory Bound, making the memory access time roughly equiv-\nalent to the parameter access time. Therefore, storing Embedding\nparameters in Flash adds only about 1.4 â€±to the total inference\ntime. Consequently, utilizing Flash for storing Embedding layers\nallows for a 15% reduction in DRAM usage without significantly\nimpacting inference performance. For example, Qwen-7B can re-\nduce DRAM usage by approximately 2.18 GB when using bfloat16\nstorage, greatly enhancing the feasibility of model inference on\nmemory-constrained mobile devices.\nFigure 2: Comparison of KV loading times for DRAM, DRAM-\nFlash, Prefetching, and Exceeding.\nIn scenarios with long input texts or extensive generation lengths,\nthe continuous growth of the KV cache can lead to significant\nmemory usage. MNN-LLM addresses this challenge by employing\na hybrid storage strategy, utilizing Flash to hold part of the KV\ncache, thus ensuring LLM inference remains feasible under long-\ncontext conditions. Initially, all KV cache values are stored in DRAM,\nbut as the context expands and the KV cache size increases, any\nportion exceeding a certain threshold is transferred to Flash. Since\neach computation produces only one set of new KV values, the\ntotal number of KV values for the Qwen2 7B model amounts to\napproximately 1 KB, minimizing storage overhead.\nAs the number of KV cache values stored in Flash rises, the time\nrequired to load them from Flash will gradually increase, which can\nslow down inference speed, as illustrated in Figure 2b. To mitigate\nthe impact of KV cache loading from Flash on inference time, we\nimplement prefetching: during the MLP phase of the current layer\nand the qkv projection phase of the next layer, KV cache values\nare prefetched from Flash into memory. When the prefetching time\nis less than or equal to the computation time, the LLM inference\nspeed remains unaffected.\nFor instance, in the Qwen2 7B model, the parameter size for a\nsingle layerâ€™s qkv and MLP is 178.83 MB, and the decode phase is\nMemory Bound. Given that LPDDR5X incurs about 3 ms of loading\ntime for this data, we assume a loading speed of 1 GB/s for Flash due\nto its larger continuous memory blocks, allowing approximately 3\nMB of KV values to be loaded within the computation time. There-\nfore, when the length of the KV cache stored in Flash is under 3072\nK, the overhead from Flash loading is effectively masked by the\ncomputation time, as shown in Figure 2c. However, once the length\nof the KV cache in Flash exceeds 3072 K, as depicted in Figure 2d,\nprefetching cannot completely offset the Flash loading overhead;\neach additional 1 K of length adds approximately 1 ms of delay. It\nis important to note that DRAM also holds a substantial length of\nKV cache, meaning that only in scenarios with exceedingly long\ncontexts will the inference speed of the LLM be impacted. Neverthe-\nless, storing KV cache in Flash ensures that LLM inference remains\nviable even with long contexts.\n4.2 Combined Quantization\nThe large parameter size of LLM models is the primary reason\nfor their high memory consumption, and quantization can signifi-\ncantly reduce the parameter size, thereby lowering memory usage.\nHowever, quantization can affect the modelâ€™s inference accuracy;\ngenerally, lower bit counts result in greater information loss and a\nlarger impact on accuracy. There are various methods, data types,\nand bit counts for quantization, making it crucial to choose an ap-\npropriate method to balance memory usage, runtime performance,\nand model accuracy.\nFor the parameters of the Embedding, Layer, and Lm head, MNN-\nLLM employs a combination quantization strategy to balance accu-\nracy and computational overhead. The weights of the embedding\nlayer account for approximately 15% of the total model weight.\nSince only a small portion of these weights is utilized during each\ndecoding step, they are stored in Flash memory, which does not\noccupy DRAM. This allows for the use of bfloat16 storage, ensur-\ning computational accuracy. Non-embedding parameters, which\ninclude the weights of the layers and the LM head, must be fully\nloaded for each computation, making their size significantly impact-\nful on inference performance. In particular, during the decoding\n3\nphase, which is memory-bound, the inference time is directly pro-\nportional to the size of these parameters. Therefore, it is crucial to\nuse low-bit quantization for these weights. Taking both precision\nand hardware computation instructions into accountâ€”where edge\nCPUs are particularly friendly towards int8 computationâ€”these\nparameters are quantized using int4 or int8. During calculations,\nactivation values are quantized to int8, enabling the use of W4A8 or\nW8A8 computation methods on CPUs to leverage int8 instructions.\nOn GPUs, W4A16 or W8A16 methods are used to take advantage\nof floating-point capabilities. To maintain model accuracy, all these\nparameters employ asymmetric quantization. Asymmetric quanti-\nzation as below:\nğ‘¤ğ‘ğ‘ ğ‘¦ = round\n \nğ‘¤ğ‘“ğ‘™ğ‘œğ‘ğ‘¡ âˆ’ğ‘¤ğ‘šğ‘–ğ‘›\nğ‘¤ğ‘šğ‘ğ‘¥ âˆ’ğ‘¤ğ‘šğ‘–ğ‘›\nğ‘ğ‘™ğ‘–ğ‘ğ‘šğ‘ğ‘¥ âˆ’ğ‘ğ‘™ğ‘–ğ‘ğ‘šğ‘–ğ‘›\n!\n+ğ‘ğ‘™ğ‘–ğ‘ğ‘šğ‘–ğ‘› (1)\nAdditionally, because the LM head has a greater impact on model\naccuracy than the layers, it is prioritized for int8 quantization to\nenhance overall precision.\nFigure 3: The reduction dimensions in the computation of\nAttention query, key, and value.\nWhen dealing with long contexts, the memory usage of the KV\ncache continues to grow, and quantization strategies can effectively\nreduce this memory consumption. MNN-LLM provides different\nquantization methods for keys and values based on their compu-\ntational roles. During attention calculations, the shapes for query,\nkey, and value are [headnum,seqlen,headdim]. When performing\nmatrix multiplication between key and query, the dimension being\nreduced is headdim, which is a fixed value. Therefore, int4/int8\nquantization can be applied to keys, allowing new key values to\nbe quantized and stored directly. In contrast, during the matrix\nmultiplication of attention scorewith values, the dimension being\nreduced is seqlen. Using int4/int8 quantization for values can affect\nthe data distribution of existing values when new ones are added,\nnecessitating updates to their quantization values and incurring\nadditional overhead. To address this, MNN-LLM employs fp8 quan-\ntization for values, allowing new values to be quantized directly\nwithout impacting the existing ones.\nTable 2: Tile Sizes for Different CPU Architectures\nArchitecture ğ‘’ğ‘ â„ğ‘ ğ‘™ğ‘\nARM i8sdot 12 8 4\nARM i8mm 10 8 8\nX86 AVX2 4 8 4\nX86 AVX512 4 64 4\n5 Compute Optimized\n5.1 Hardware-Driven Data Reorder\nAnalysis of the inference process in LLMs shows that the primary\ntime-consuming operations are Linear and Attention, both of which\nfundamentally rely on matrix multiplication. Therefore, optimizing\nmatrix multiplication for these two operators is crucial for improv-\ning LLM performance. Loop Tiling is a common optimization tech-\nnique that enhances memory access locality, significantly impacting\nperformance. The optimal tile size for Loop Tiling [12] greatly af-\nfects the final matrix multiplication performance and is influenced\nby the deviceâ€™s memory, cache, and computational hardware. Thus,\nit is essential to select the most suitable data reorganization and\ncomputation method based on hardware and data scale to achieve\npeak performance. MNN-LLM employs a Hardware-Driven data\nreorder strategy tailored to the computational characteristics of\nthese two operator types to determine the tiling method and size,\noptimizing LLM inference performance.\nThe matrix multiplication for the Linear operator involves the\nactivation values and weight values, where the activation values are\ncomputed during inference and the weights are determined when\nthe model is loaded. In MNN-LLM, weights are generally quantized\nto int4 or int8. Assuming the activation value matrix size is [ğ‘’,ğ‘™],\nand the weight size is [â„,ğ‘™], the resulting size will be [ğ‘’,â„]. After\ndata tiling on the mobile CPU, the input matrices are rearranged as:h\nğ‘’\nğ‘’ğ‘ , ğ‘™\nğ‘™ğ‘\n,ğ‘’ğ‘,ğ‘™ğ‘\ni\nfor the activation values and\nh\nâ„\nâ„ğ‘\n, ğ‘™\nğ‘™ğ‘\n,ğ‘’ğ‘,ğ‘™ğ‘\ni\nfor the\nweights. This tiling allows for value reuse within the registers dur-\ning kernel computations, enhancing memory locality and reducing\nmemory access frequency. The memory access count is optimized\nfrom 2ğ‘’â„ğ‘™+ğ‘’â„to ğ‘’\nğ‘’ğ‘\nâ„\nâ„ğ‘\n(ğ‘™ğ‘’ğ‘ +ğ‘™â„ğ‘ +â„ğ‘ğ‘’ğ‘). By using memory access\nfrequency as the optimization objective and hardware parameters as\nconstraints, we can compute the values forğ‘’ğ‘,â„ğ‘,ğ‘™ğ‘ under different\nhardware conditions. Let ğ‘…be the number of vector registers, and\nğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ be the data size computed in a single instruction\nalong the l-dimension, ğ‘’ğ‘,â„ğ‘,ğ‘™ğ‘ as given by the following formulas:\nmin ğ‘’\nğ‘’ğ‘\nâ„\nâ„ğ‘\n(ğ‘™ğ‘’ğ‘ +ğ‘™â„ğ‘ +â„ğ‘ğ‘’ğ‘) (2)\ns.t. ğ‘’ğ‘ +â„ğ‘ +â„ğ‘ğ‘’ğ‘ â‰¤ğ‘… (3)\nğ‘™ğ‘ = ğ‘–ğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘› ğ‘¤ğ‘–ğ‘‘ğ‘¡â„ (4)\nBased on the above strategy, the block sizes calculated for vari-\nous CPU instruction sets are shown in Table 2. By employing a\nHardware-Driven data rearrangement strategy tailored to different\nCPU architectures, MNN-LLM can better utilize CPU computa-\ntional power. For instance, the throughput of thesmmla instruction\non ARM i8mm [5] is twice that of sdot [4]. When MNN-LLM de-\ntects that the CPU supports i8mm, it rearranges the weights with\n4\nğ‘™ğ‘ = 8 during the model loading phase. This arrangement format\nenhances performance compared to the data layout in llama.cpp,\nthereby improving the efficiency of the prefill stage.\nGPUs support hardware loading/storing merging, which allows\nthem to combine a certain number of memory access instructions\nif the memory addresses accessed by consecutive work items are\ncontiguous. This capability minimizes the number of memory ac-\ncess instructions. Additionally, GPUs can load/store up to 128 bits\nof data at a time. To maximize memory loading efficiency, each\nwork item should utilize vectorized loading/storing functions. In\nOpenCL, GPU memory objects are categorized into Buffers and\nImages. Images can automatically handle boundaries and return ap-\npropriate out-of-bounds values based on settings. Certain devices,\nsuch as Qualcommâ€™s Adreno GPUs [20], possess powerful texture\nengines and dedicated L1 caches, enabling efficient loading of data\nfrom Image objects. Compared to ordinary buffer objects, Images\noffer higher bandwidth, making them the preferred choice for stor-\nage. To leverage these memory loading advantages, MNN-LLM\nrearranges GPU weight data and uses Image objects for storage.\nThe rearranged data structure is [ğ‘™\nğ‘™ğ‘\n,â„,ğ‘™ğ‘]with ğ‘™ğ‘ = 32. Each work\nitem loads 4-bit weights at once, totaling 128 bits, which meets\nthe GPUâ€™s maximum loading bandwidth and corresponds to the\nsize of four floating-point values in the CL_RGBA Image memory\nobject. Additionally, each work item accesses data contiguously\nalong the h dimension, ensuring continuous memory reads be-\ntween work items. Finally, the runtime dynamically adjusts the\nparallelism based on actual dimensions, allocating a reasonable\nnumber of computational tasks to each work item.\nFor the Attention operator, a similar rearrangement strategy as\nused for Linear is applied. The key and value are stored directly\nin the rearranged data layout, ensuring that there is no need to\nrearrange the historical KV during each computation.\n5.2 Multicore Workload Balancing\nModern CPUs typically have multiple cores, so effectively utilizing\nmulticore computing capabilities is crucial when optimizing perfor-\nmance. MNN-LLM leverages the multicore parallelism of CPUs to\nparallelize operations along the ğ‘ ğ‘’ğ‘ğ‘™ğ‘’ğ‘› and â„\nâ„ğ‘\ndimensions. Consid-\nering the big.LITTLE [2] architecture of mobile CPUs, MNN-LLM\nspecifies the computing load for different cores at startup based on\ntheir actual computational capabilities. During parallel computa-\ntion, MNN-LLM allocates the computational workload according\nto the load rates of the cores. This balancing workload distribu-\ntion strategy, can enhance multithreaded computing performance\ncompared to the uniform workload strategy.\nMainstream mobile SoCs typically feature one prime core and\nthree performance cores, such as the Snapdragon 8 Gen 3 [ 21].\nHigh-load computations generally utilize the prime core and per-\nformance cores. When the number of threads exceeds one, parallel\ncomputing between the prime core and performance cores occurs,\nas shown in Figure 4. In this scenario, workload balancing signifi-\ncantly improves the multithreaded speedup compared to uniform\nworkload distribution.\n1 2 3 4\nThread Num\n1.00\n1.25\n1.50\n1.75Speedup\nbalancing workload\nuniform workload\nFigure 4: Parallel computing between 1 prime cores and 3 per-\nformance cores, speedup achieved through balancing work-\nload and uniform workload.\n5.3 Mixed Float Precision\nIn the previous discussion on matrix operations, low-bit quanti-\nzation methods were employed to accelerate computations. For\nnon-matrix multiplication operations, MNN-LLM also supports\nmixed precision for results, ensuring accuracy while enhancing in-\nference performance. ARMv8.2 [6] and newer CPUs support float16\ncalculations, which can save half the memory compared to float32,\nand the throughput of float16 NOEN [3] instructions is twice that\nof float32. However, float16 has some precision limitations; for\ncalculations requiring a higher precision range, significant errors\nmay occur, especially when values exceed 65,504. To address this,\nMNN-LLM adopts a mixed precision strategy to maintain infer-\nence accuracy. During LLM inference, the Softmax calculation in\nAttention is particularly sensitive to data precision, so MNN-LLM\nensures that Softmax uses float32. In the matrix multiplication of\nquery and key, the query values may be large, potentially caus-\ning overflow after accumulation. To mitigate this, the division byâˆšï¸\nğ‘‘ğ‘˜ [25] can be applied directly to the query, reducing its value\nrange and preventing overflow in the final result. This approach\noptimizes overall memory usage and inference performance while\nmaintaining accuracy.\n5.4 Geometry Compute\nThe computation graph of LLMs also includes long-tail operators\nsuch as Transpose, Gather, and Concat. Although these operators\nmay not significantly contribute to overall execution time, they\ncan result in substantial memory access when data sizes are large.\nTo address these long-tail operators, MNN-LLM employs geomet-\nric computation [17] methods, abstracting all data rearrangement\noperations as linear mappings of addresses.\nğ‘“(Â®ğ‘¥)= Â®ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ + Â®ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘‘ğ‘’ Â®ğ‘¥ (5)\nBy taking the Â®ğ‘œğ‘“ğ‘“ğ‘ ğ‘’ğ‘¡ and Â®ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘‘ğ‘’ with a length of 3, we can construct\na fundamental mapping relationship described as a Region. This\nallows us to represent any data rearrangement operators using one\nor more Regions.\n5\nTable 3: Computation and Memory under Different LoRA\nComputation Orders.\nType (ğ¿ğ‘œğ‘…ğ´ğ´ Â·ğ¿ğ‘œğ‘…ğ´ğµ)Â·ğ´ ğ¿ğ‘œğ‘…ğ´ ğ´ Â·(ğ¿ğ‘œğ‘…ğ´ğµ Â·ğ´)\nComputation ğ‘Ÿâ„2 +â„3 2ğ‘Ÿâ„2\nMemory 2 \u0000ğ‘Ÿâ„2 +â„2 +â„3\u0001 4ğ‘Ÿâ„2 +â„â„+ğ‘Ÿâ„\nFor consecutive data rearrangement operators in the computa-\ntion graph, this abstraction generates numerous contiguous Re-\ngions. MNN-LLM implements an automatic Region Fusion algo-\nrithm based on rules like loop unrolling, loop interchange, loop tiling,\nand loop fusion. This algorithm can automatically merge compatible\nRegions, thereby reducing the number of read and write operations\nfor data rearrangement operators and enhancing performance. By\nutilizing geometric computation for LLM model inference, the over-\nhead of long-tail operators can be reduced, improving performance\nby approximately 3%.\n5.5 LoRA Optimization\nOn mobile devices, different tasks may require different LLM models.\nDue to the large number of model parameters, directly utilizing\nmultiple models can lead to excessive bandwidth and storage usage.\nThus, using a base model in conjunction with multiple LoRA models\nis a more efficient solution for multitasking.\nMNN-LLM supports the deployment of merged LoRA models\nand the online loading of multiple LoRA models. When employing\nmultiple LoRA models, MNN-LLM first loads the base model, fol-\nlowed by the computation graph and weights of the LoRA models,\nwith LoRA models sharing the weights of the base model. Given\nthat LoRA weights are generally small, the memory overhead is\nminimal. Online loading of LoRA models is more flexible than\npre-merged approaches, making it suitable for multitasking sce-\nnarios, although it incurs additional computational costs. The com-\nputation graph for LoRA adds a bypass for the layers involving\nLoRA, transforming the original computation ğ´â€² = ğ‘Š Â·ğ´ into\nğ´â€²= ğ‘Š Â·ğ´+(ğ¿ğ‘œğ‘…ğ´ğ´ Â·ğ¿ğ‘œğ‘…ğ´ğµ)Â·ğ´, where the additional computa-\ntions may slow down model inference. Analyzing the characteris-\ntics of LoRA weights reveals that the size of R is relatively small\ncompared to the original parameters, allowing us to leverage the as-\nsociative property of matrix multiplication to alter the computation\norder, transforming (ğ¿ğ‘œğ‘…ğ´ğ´ Â·ğ¿ğ‘œğ‘…ğ´ğµ)Â·ğ´into ğ¿ğ‘œğ‘…ğ´ğ´Â·(ğ¿ğ‘œğ‘…ğ´ğµ Â·ğ´).\nAssuming the size of matrix A is [â„,â„]and that of ğ¿ğ‘œğ‘…ğ´ğ´ and\nğ¿ğ‘œğ‘…ğ´ğµ is [â„,ğ‘Ÿ], the computation memory access before and after\noptimization is shown in Table 3. Given that ğ‘…is relatively small\n[14], rearranging the computation order significantly reduces the\nmemory access volume. For instance, using Qwen2 7B as an exam-\nple, if â„= 3584 and ğ‘Ÿ = 8, the optimized memory access volume is\nonly 0.5% of the original, thereby effectively improving computa-\ntional efficiency.\n6 Evaluation\nThe experimental design hinges upon quantized models, namely\nQwen2 1.5B, Qwen2 7B, and Llama3 8B, utilizing the Xiaomi 14 as\nthe test apparatus. Comparative evaluations of inference efficacy are\nconducted across CPU (harnessing 4 threads) and GPU (via OpenCL)\narchitectures, employing inference engines such as llama.cpp, MLC-\nLLM, and fastllm. Given that MLC-LLM does not accommodate\nCPU-based inference and fastllm lacks GPU compatibility, pertinent\nexperiments are excluded for these engines. Extensive trials were\nexecuted with prompts of varying lengths (64, 256, and 1024 tokens),\nwith a restrictive upper limit of 16 tokens imposed on the decoding\nphase.\nOwing to the poor performances of MLC-LLM in handling asym-\nmetric quantization models, the reported results of MLC-LLM are\nbased on symmetric quantized models but competing engines were\nexplicitly engaged in inference tasks using asymmetric models. The\nperformance results are reflected in terms of the prefill and decode\nspeed, graphically represented in Figure 5.\n0\n150\n300prefill (toks/s)\n225.2\n298.9\n236.8\n46.2 42.1 37.4\nX X X\n26.3 25.9 19.4\n0\n30\n60\n61.2\n67.3\n59.3\n7.8 8.1 6.9\nX X X4.3 5.0 3.6\n0\n30\n60 53.6\n65.2\n54.2\n8.2 7.7 7.2\nX X X3.2 3.2 3.1\n64 256 1024\nQwen2-1.5B on CPU (4 Thread)\n0\n20\n40\ndecode (toks/s)\n53.6 52.5\n45.7\n30.1\n26.4\n22.1\nX X X\n9.3 8.7 6.5\n64 256 1024\nQwen2-7B on CPU (4 Thread)\n0\n5\n10\n13.0\n11.8\n10.1\n6.4 5.9\n4.3\nX X X\n1.6 1.8 1.1\n64 256 1024\nllama3-8B on CPU (4 Thread)\n0\n5\n10\n11.2\n10.3\n9.0\n6.0 5.5 5.0\nX X X\n1.3 1.3 1.1\n0\n100\n200prefill (toks/s)\n153.4 166.8\n237.3\n6.0 16.1 23.1\n137.6 146.3\n83.5\nX X X 0\n30\n60\n34.8 37.4\n54.2\n1.8 4.5 5.8\n33.8 36.6\nXX X X\n0\n30\n60\n32.2 32.6\n56.8\n1.6 3.6 4.2\n26.1\n33.8\n24.8\nX X X\n64 256 1024\nQwen2-1.5B on GPU (OpenCL)\n0\n10\n20\ndecode (toks/s)\n26.9 26.3\n20.9\n4.4 4.3 3.3\n25.0\n19.5\n11.9\nX X X\n64 256 1024\nQwen2-7B on GPU (OpenCL)\n0\n4\n8\n8.9\n7.7 7.0\n1.6 1.4 1.2\n10.4\n9.0\nXX X X\n64 256 1024\nllama3-8B on GPU (OpenCL)\n0\n4\n8\n9.2 9.4\n7.9\n1.3 1.3 1.3\n9.6 9.2\n7.0\nX X X\nMNN-LLM llama.cpp MLC-LLM fastllm\nFigure 5: Prefill and decode speeds of MNN-LLM, llama.cpp,\nMLC-LLM, and fastllm under different prompt lengths on\nXiaomi14â€™s CPUs and GPUs.\nIn CPU benchmarking, MNN-LLM excels, achieving prefill speed\nboosts of 8.6x over llama.cpp and 20.5x over fastllm, complemented\nby decoding speeds that are 2.3x and 8.9x faster, respectively. In\nGPU-based assessments, MNN-LLMâ€™s performance slightly declines\ncompared to MLC-LLM, particularly when using Qwen2-7B with\nshorter prompts, due to MLC-LLMâ€™s advantageous symmetric quan-\ntization technique. MNN-LLM excels, achieving up to 25.3x faster\nprefill and 7.1x faster decoding than llama.cpp, and 2.8x and 1.7x\nimprovements over MLC-LLM, respectively.\n7 Conclusion\nThis paper introduces MNN-LLM, a high-performance general-\npurpose inference framework tailored for LLM inference on mobile\ndevices. The framework enhances memory usage through DRAM-\nFlash Hybrid Storage and Combined Quantization, while improving\ninference speed with Hardware-Driven Data Reordering, Multicore\nWorkload Balancing, Mixed Float Precision, and Geometry Com-\npute. When compared to leading mainstream frameworks, MNN-\nLLM achieves up to an 8.6x performance improvement.\n6\nReferences\n[1] Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik\nCho, Carlo C Del Mundo, Mohammad Rastegari, and Mehrdad Farajtabar. 2024.\nLLM in a flash: Efficient Large Language Model Inference with Limited Memory.\narXiv:2312.11514 [cs.CL] https://arxiv.org/abs/2312.11514\n[2] ARM. 2024. Arm Big.LITTLE. https://www.arm.com/zh-TW/technologies/big-\nlittle.\n[3] ARM. 2024. Arm NEON. https://www.arm.com/technologies/neon.\n[4] ARM. 2024. Dot Product. https://developer.arm.com/documentation/100069/\n0609/A64-SIMD-Vector-Instructions/SDOT--vector-.\n[5] ARM. 2024. Matrix Multiplication extension. https://developer.arm.com/\ndocumentation/101754/0622/armclang-Reference/Other-Compiler-specific-\nFeatures/Supported-architecture-features/Matrix-Multiplication-extension.\n[6] ARM. 2024. The Armv8.2 architecture extension. https://developer.arm.com/\ndocumentation/109697/latest/Feature-descriptions/The-Armv8-2-architecture-\nextension.\n[7] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,\nWenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji\nLin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,\nXingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng\nWang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,\nHao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng\nYuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang\nZhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical\nReport. arXiv preprint arXiv:2309.16609(2023).\n[8] Junjie Bai, Fang Lu, Ke Zhang, et al. 2019. ONNX: Open Neural Network Exchange.\nhttps://github.com/onnx/onnx.\n[9] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ:\nAccurate Post-Training Quantization for Generative Pre-trained Transformers.\narXiv:2210.17323 [cs.LG] https://arxiv.org/abs/2210.17323\n[10] Georgi Gerganov. 2024. ggerganov/llama.cpp: Port of Facebookâ€™s LLaMA model\nin C/C++. https://github.com/ggerganov/llama.cpp.\n[11] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio CÃ©sar Teodoro Mendes, Allie Del\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,\nOlli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, SÃ©bastien\nBubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023.\nTextbooks Are All You Need. (2023). arXiv:2306.11644 [cs.CL] https://arxiv.org/\nabs/2306.11644\n[12] Emna Hammami and Yosr Slama. 2017. An Overview on Loop Tiling Techniques\nfor Code Generation. In 2017 IEEE/ACS 14th International Conference on Computer\nSystems and Applications (AICCSA). 280â€“287. https://doi.org/10.1109/AICCSA.\n2017.168\n[13] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Wei-\njun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mo-\nbileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.\narXiv:1704.04861 [cs.CV] https://arxiv.org/abs/1704.04861\n[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large\nLanguage Models. arXiv:2106.09685 [cs.CL] https://arxiv.org/abs/2106.09685\n[15] Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan Wang, Bin Zou, Yafeng\nYang, Zongyang Cui, Yu Cai, Tianhang Yu, Chengfei Lv, and Zhihua Wu. 2020.\nMNN: A Universal and Efficient Inference Engine. CoRR abs/2002.12418 (2020).\narXiv:2002.12418 https://arxiv.org/abs/2002.12418\n[16] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin\nChess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling Laws for Neural Language Models. CoRR abs/2001.08361 (2020).\narXiv:2001.08361 https://arxiv.org/abs/2001.08361\n[17] Chengfei Lv, Chaoyue Niu, Renjie Gu, Xiaotang Jiang, Zhaode Wang, Bin Liu,\nZiqi Wu, Qiulin Yao, Congyu Huang, Panos Huang, Tao Huang, Hui Shu, Jinde\nSong, Bin Zou, Peng Lan, Guohuan Xu, Fei Wu, Shaojie Tang, Fan Wu, and Guihai\nChen. 2022. Walle: An End-to-End, General-Purpose, and Large-Scale Produc-\ntion System for Device-Cloud Collaborative Machine Learning. In 16th USENIX\nSymposium on Operating Systems Design and Implementation (OSDI 22). USENIX\nAssociation, Carlsbad, CA, 249â€“265. https://www.usenix.org/conference/osdi22/\npresentation/lv\n[18] OpenAI. 2023. ChatGPT. https://openai.com/chatgpt Available at: https:\n//openai.com/chatgpt.\n[19] Qualcomm. 2023. The future of AI is â€œon deviceâ€. https://cms.tinyml.org/wp-\ncontent/uploads/ew2023/Kyuwoong-Hwang_tinyML-Asia-2023.pdf.\n[20] qualcomm. 2024. Adreno Graphics Processing Units. https://www.qualcomm.\ncom/products/features/adreno.\n[21] qualcomm. 2024. Snapdragon 8 Gen 3 Mobile Platform. https://www.qualcomm.\ncom/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-\nplatforms/snapdragon-8-gen-3-mobile-platform.\n[22] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. You\nOnly Look Once: Unified, Real-Time Object Detection. arXiv:1506.02640 [cs.CV]\nhttps://arxiv.org/abs/1506.02640\n[23] MLC team. 2024. MLC-LLM. https://github.com/mlc-ai/mlc-llm.\n[24] Qwen Team. 2024. Qwen2.5: A Party of Foundation Models. https://qwenlm.\ngithub.io/blog/qwen2.5/\n[25] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention Is All\nYou Need. arXiv:1706.03762 [cs.CL] https://arxiv.org/abs/1706.03762\n[26] Zhenliang Xue, Yixin Song, Zeyu Mi, Le Chen, Yubin Xia, and Haibo Chen.\n2024. PowerInfer-2: Fast Large Language Model Inference on a Smartphone.\narXiv:2406.06282 [cs.LG] https://arxiv.org/abs/2406.06282\n[27] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Cheng-\npeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei,\nHuan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang,\nKeming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang,\nPeng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai,\nSinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng,\nXiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang\nFan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui,\nZhenru Zhang, and Zhihao Fan. 2024. Qwen2 Technical Report. arXiv preprint\narXiv:2407.10671 (2024).\n[28] Biao Zhang and Rico Sennrich. 2019. Root Mean Square Layer Normalization.\narXiv:1910.07467 [cs.LG] https://arxiv.org/abs/1910.07467\n[29] ztxz16. 2023. fastllm. https://github.com/ztxz16/fastllm.\n7"
}