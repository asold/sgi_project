{
  "title": "Ecosystem Graphs: The Social Footprint of Foundation Models",
  "url": "https://openalex.org/W4381889417",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3007978110",
      "name": "Rishi Bommasani",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3081651891",
      "name": "Dilara Soylu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2488241983",
      "name": "Thomas Liao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3129662763",
      "name": "Kathleen Creel",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A2171686691",
      "name": "Percy Liang",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4298185919",
    "https://openalex.org/W4310921506",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4312197262",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W2057254560",
    "https://openalex.org/W1968069898",
    "https://openalex.org/W3212464620",
    "https://openalex.org/W3174220540",
    "https://openalex.org/W4226317937",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W4320003957",
    "https://openalex.org/W4205807230",
    "https://openalex.org/W4221167819",
    "https://openalex.org/W3005957694",
    "https://openalex.org/W4321277297",
    "https://openalex.org/W4321472284",
    "https://openalex.org/W2767122007",
    "https://openalex.org/W4283168787",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W2897697024",
    "https://openalex.org/W4330336443",
    "https://openalex.org/W2106022734",
    "https://openalex.org/W3125498346",
    "https://openalex.org/W4323345674",
    "https://openalex.org/W4320854981",
    "https://openalex.org/W2776393547",
    "https://openalex.org/W3117669565",
    "https://openalex.org/W7053421768",
    "https://openalex.org/W3215475614",
    "https://openalex.org/W2962059918",
    "https://openalex.org/W3164854573",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3023533951",
    "https://openalex.org/W4226404919",
    "https://openalex.org/W3088647504",
    "https://openalex.org/W4220747294",
    "https://openalex.org/W4312089323",
    "https://openalex.org/W4229447062",
    "https://openalex.org/W2991917796",
    "https://openalex.org/W3105424285",
    "https://openalex.org/W3207830467",
    "https://openalex.org/W3213728903",
    "https://openalex.org/W3210023107",
    "https://openalex.org/W3204130547",
    "https://openalex.org/W4389676498",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W1981719767",
    "https://openalex.org/W3122688191",
    "https://openalex.org/W638668516",
    "https://openalex.org/W4246026233",
    "https://openalex.org/W2561232274",
    "https://openalex.org/W3196412634",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W4311405342",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2030035054",
    "https://openalex.org/W4308264370",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W3099286868",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W4309047588",
    "https://openalex.org/W4285006154",
    "https://openalex.org/W3036061622",
    "https://openalex.org/W4310492983",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W1518985373",
    "https://openalex.org/W3003809373",
    "https://openalex.org/W2033912020",
    "https://openalex.org/W4320499441",
    "https://openalex.org/W3124124240",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W4384154918",
    "https://openalex.org/W4286850188",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W4402264526",
    "https://openalex.org/W619721102",
    "https://openalex.org/W4308481882",
    "https://openalex.org/W4301263698",
    "https://openalex.org/W4323345726",
    "https://openalex.org/W2005476331",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4385573090",
    "https://openalex.org/W3184754382",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4310283096",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3177301840"
  ],
  "abstract": "Abstract Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at this https URL. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions as a powerful abstraction and interface for achieving the minimum transparency required to address myriad use cases. Therefore, we envision Ecosystem Graphs will be a community-maintained resource that provides value to stakeholders spanning AI researchers, industry professionals, social scientists, auditors and policymakers.",
  "full_text": "Ecosystem Graphs: The Social Footprint of\nFoundation Models\nRishi Bommasani  (  nlprishi@stanford.edu )\nStanford University\nDilara Soylu \nStanford University\nThomas Liao \nAnthropic\nKathleen Creel \nNortheastern University\nPercy Liang \nStanford Univ\nPhysical Sciences - Article\nKeywords:\nPosted Date: June 23rd, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-2961271/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: There is NO Competing Interest.\nEcosystem Graphs:\nThe Social Footprint of Foundation Models\nAnonymous Author(s)\nAfﬁliation\nAddress\nemail\nAbstract\nFoundation models (e.g. ChatGPT , StableDiffusion) pervasively inﬂuence society ,1\nwarranting immediate social attention. While the models themselves garner much2\nattention, to accurately characterize their impact, we must consider the broader3\nsociotechnical ecosystem. W e propose Ecosystem Graphs as a documentation4\nframework to transparently centralize knowledge of this ecosystem. Ecosystem5\nGraphs is composed of assets (datasets, models, applications) linked together by6\ndependencies that indicate technical (e.g. how Bing relies on GPT -4) and social7\n(e.g. how Microsoft relies on OpenAI) relationships. T o supplement the graph8\nstructure, each asset is further enriched with ﬁne-grained metadata (e.g. the license9\nor training emissions). W e document the ecosystem extensively at https://crfm.10\nstanford.edu/ecosystem-graphs: as of March 16, 2023, we annotate 26211\nassets (64 datasets, 128 models, 70 applications) from 63 organizations linked12\nby 356 dependencies. W e show Ecosystem Graphs functions as a powerful13\nabstraction and interface for achieving the minimum transparency required to14\naddress myriad use cases. Therefore, we envision Ecosystem Graphs will be a15\ncommunity-maintained resource that provides value to stakeholders spanning AI16\nresearchers, industry professionals, social scientists, auditors and policymakers.17\n1 Introduction18\nFoundation models (FMs) are the deﬁning paradigm of modern AI [ Bommasani et al. , 2021]. Be-19\nginning with language models [ Devlin et al. , 2019, Brown et al. , 2020, Chowdhery et al. , 2022], the20\nparadigm has expanded to images [ Chen et al. , 2020, Ramesh et al. , 2021, Radford et al. , 2021],21\nvideos [ Singer et al. , 2022, W ang et al. , 2022a], code [ Chen et al. , 2021], proteins [ Jumper et al. ,22\n2021, V erkuil et al. , 2022], and more. Beyond rapid technology development, foundation models have23\nentered broad social discourse [ NYT, 2020, Nature, 2021, Economist, 2022, CNN, 2023]. Given their24\nremarkable capabilities, we are witnessing unprecedented adoption: ChatGPT amassed 100 million25\nusers in just 50 days [the fastest-growing consumer application in history; Hu, 2023] and Stable26\nDiffusion accrued 30k+ GitHub stars in 90 days [much faster than Bitcoin and Spark; Appenzeller27\net al. , 2022], As a bottom line, over 200 foundation model startups have emerged, collectively28\nraising $3.5B as of October 2022 [ Kaufmann et al. , 2022]. In fact, the inﬂux of funding continues29\nto accelerate: Character received $200M from Andreesen Horowitz, Adept received $350M from30\nGeneral Catalyst, and OpenAI received $10B from Microsoft just in Q1 of 2023.31\nFoundation models are changing society but what is the nature of this impact? Who reaps the32\nbeneﬁts, who shoulders the harms, and how can we characterize these societal changes? Further,33\nhow do trends in research correspond to outcomes in practice (e.g. how do emergent abilities34\n[W ei et al. , 2022] inﬂuence deployment decisions, how do documented risks [ Abid et al. , 2021]35\nmanifest as concrete harms)? Overall, there is pervasive confusion on the status quo, which breeds36\nUnder review . Do not distribute.\n(a) The Pile dataset [ Gao et al. , 2021]\n (b) P3 dataset [ Sanh et al. , 2021]\n(c) PaLM model [ Chowdhery et al. , 2022]\n (d) ChatGPT API [OpenAI, 2023a]\nFigure 1: Hubs in the ecosystem. T o demonstrate the value of Ecosystem Graphs , we highlight\nhubs: assets that feature centrally in that many assets directly depend on them. (a) The Pile is an\nessential resource for training foundation models from a range of institutions (e.g. EleutherAI, Meta,\nMicrosoft, Stanford, Tsinghua, Y andex). (b) P3 is of growing importance as interest in instruction-\ntuning grows, both directly used to train models and as a component in other instruction-tuning\ndatasets. (c) PaLM features centrally in Google’s internal foundation models for vision (P ALM-E),\nrobotics (PaLM-SayCan), text (FLAN-U-PaLM), reasoning (Minerva), and medicine (Med-PaLM),\nmaking the recent announcement of an API for external use especially relevant. (d) The ChatGPT\nAPI profoundly accelerates deployment with downstream products spanning a range of industry\nsectors.\nfurther uncertainty on how the space of foundation models will evolve and what change is necessary .37\nCurrently , the AI community and broader public tolerate the uncomfortable reality that models are38\ndeployed ubiquitously through products yet we know increasingly little about the models, how they39\nwere built, and the mechanisms (if any) in place to mitigate and address harm.40\nT o clarify the societal impact of foundation models, we propose Ecosystem Graphs as a centralized41\nknowledge graph for documenting the foundation model ecosystem (Figure 2). Ecosystem Graphs42\nconsolidates distributed knowledge to improve the ecosystem’s transparency . Ecosystem Graphs43\noperationalizes the insight that signiﬁcant understanding of the societal impact of FMs is already44\npossible if we centralize available information to analyze it collectively .45\n2\nEach node in the graph is (roughly) an asset (a dataset, model, or application). Simply being aware of46\nassets is an outstanding challenge: new datasets are being built, new models are being trained, and new47\nproducts are being shipped constantly , often with uneven public disclosure. While attention centers48\non the foundation model, the technical underpinnings and the social consequences of a foundation49\nmodel depend on the broader ecosystem-wide context. T o link nodes, we specify dependencies: in50\nits simplest form, models require training data and applications require models. Dependencies are51\ntechnical relationships between assets (e.g. different ways of training or adapting a foundation model)52\nthat induce social relationships between organizations (e.g. Microsoft depends on OpenAI because53\nBing depends on GPT -4). Especially for products, surfacing these dependencies is challenging54\nyet critical: products determine much of the direct impact and dependencies indicate the ﬂow of55\nresources, money , and power.56\nT o supplement the graph structure, we further document each node with an ecosystem card , drawing57\ninspiration from other documentation frameworks (e.g. data sheets [ Gebru et al. , 2018], data58\nstatements [ Bender and Friedman , 2018], model cards [ Mitchell et al. , 2018]). The ecosystem59\ncard contextualizes the node not only in isolation (e.g. when was it built), but also with respect60\nto the graph structure (e.g. the license affects downstream use, data ﬁlters interact with upstream61\ndependencies). Documenting applications concretizes societal impact: structural analyses (e.g. which62\norganizations wield outsized power) requires grounding out into how people are affected, which is63\ndirectly mediated by applications. W e also make explicit new challenges faced in documentation64\nsuch as (i) maintenance practices to synchronize the ecosystem graph with the ecosystem, and (ii)65\nincentives that may inhibit or facilitate documentation.66\nGiven our framework, we concretely document the existing foundation model ecosystem through 26267\nnodes linked by 356 dependencies and annotated with 3850 metadata entries as of March 16, 2023.68\nThis amounts to 64 datasets (e.g. The Pile , LAION-5B), 128 models (e.g. BLOOM, Make-A-V ideo),69\nand 70 applications (e.g. GitHub CoPilot , Notion AI ) that span 63 organizations (e.g. OpenAI,70\nGoogle) and 9 modalities (e.g. music, genome sequences). T o brieﬂy demonstrate the value of71\nEcosystem Graphs , we highlight the hubs in the graph (Figure 1), drawing inspiration from the72\nwidespread analysis of hubs across other graphs and networks [ Kleinberg, 1999, Hendricks et al. ,73\n1995, Franks et al. , 2008, V an den Heuvel and Sporns , 2013, inter alia ]. For asset developers, hubs in-74\ndicate their assets are high impact; for economists, hubs communicate emergent market structure and75\npotential consolidation of power; for investors, hubs signal opportunities to further support or acquire;76\nfor policymakers, hubs identify targets to scrutinize to ensure their security and safety . In general,77\nEcosystem Graphs functions as a rich interface and suitable abstraction to provide needed trans-78\nparency on the foundation model ecosystem ( §4: U S E S). W e encourage further exploration at https:79\n//crfm.stanford.edu/ecosystem-graphs and are actively building Ecosystem Graphs by col-80\nlaborating with the community at https://github.com/stanford-crfm/ecosystem-graphs.81\n2 Foundation Model Ecosystem82\nT echnology development situates in a broader social context: technology is the byproduct of83\norganization-internal processes and the artifact of broader social forces [ Martin Jr et al. , 2020,84\nGebru et al. , 2018, Mitchell et al. , 2018, Amironesei et al. , 2021, Paullada et al. , 2021, Birhane et al. ,85\n2022]. Bommasani et al. [2021, §1.2] introduce this perspective for foundation models: Figure 286\ndemonstrates the canonical pipeline where datasets, models, and applications mediate the relationship87\nbetween the people on either side.88\nConcretely , consider Stable Diffusion [Rombach et al. , 2021]:89\n1. People create content (e.g. take photos) that they (or others) upload to the web.90\n2. LAION curates the LAION-5B dataset [ Schuhmann et al. , 2022] from the CommonCrawl91\nweb scrape, ﬁltered for problematic content.92\n3. LMU Munich, IWR Heidelberg University , and RunwayML train Stable Diffusion on a93\nﬁltered version of LAION-5B using 256 A100 GPUs.94\n4. Stability AI builds Stable Diffusion Reimagine by replacing the Stable Diffusion text encoder95\nwith an image encoder. 196\n1 https://stability.ai/blog/stable-diffusion-reimagine\n3\nFigure 2: Basic foundation model ecosystem. T o conceptualize the foundation model ecosystem,\nwe present a simpliﬁed pipeline. Image taken from Bommasani et al. [2021].\n5. Stability AI deploys Stable Diffusion Reimagine as an image editing tool to end users on97\nClipdrop, allowing users to generate multiple variations of a single image (e.g. imagine a98\nroom with different furniture layouts). The application includes a ﬁlter to block inappropriate99\nrequests and solicits user feedback to improve the system as well as mitigate bias.100\nThis process delineates social roles and, thereby , stakeholders: data creators, data curators, compute101\nproviders, hardware providers, foundation model developers, downstream application developers, and102\nend users. While framed technically (e.g. curation, training, adaptation), the process bears broader103\nsocietal ramiﬁcations: for example, ongoing litigation contends the use of LAION and the subsequent104\nimage generation from Stable Diffusion is unlawful, infringing on the rights and the styles of data105\ncreators.2 As the foundation model ecosystem matures, amounting to greater commercial viability ,106\nfoundation model development will become even more intricate, implicating even more social roles. 3107\n3 Documentation Framework108\nT o document the foundation model ecosystem, we introduce the Ecosystem Graphs framework.109\nInformally , the framework is deﬁned by a graph comprised of (i) assets (e.g. ChatGPT), (ii) dependen-110\ncies (e.g. datasets used to build to ChatGPT , applications built upon ChatGPT), and (iii) ecosystem111\ncards (e.g. metadata on ChatGPT).112\n3.1 Deﬁnition113\nThe ecosystem graph is deﬁned in terms of assets: each asset a ∈ A has a type T (a) ∈114\n{dataset, model, application}. Examples include The Pile dataset, the Stable Diffusion model,115\nand the Bing Search application. T o deﬁne the graph structure, each asset a has a set of dependencies116\nD(a) ⊂ A, which are the assets required to build a. For example, following the Stable Diffusion117\nexample in §2: E C O S Y S T E M, LAION-5B is a dependency for Stable Diffusion and Stable Diffusion118\nis a dependency for Stable Diffusion Reimagine. Dependencies correspond to directed edges in the119\necosystem graph. In Figure 3, we given several examples of primitive structures (i.e. subgraphs) that120\nwe observe in the full ecosystem graph.121\nT o enrich assets with contextual metadata, each asset a is annotated with properties p(a) that are122\nstored in the asset’s ecosystem card. Properties include the \"organization\" that created the asset, the123\n\"license\" enforced to use the asset, and type-speciﬁc properties (e.g. the \"size\" of a model). For each124\nproperty (e.g. the \"license\"), we annotate the property as it applies to the asset (e.g. the license for125\nStable Diffusion is the CreativeML OpenRAIL M license). 4126\n2 See https://www.newyorker.com/culture/infinite-scroll/is-ai-art-stealing-from-artists .\n3 See https://www.madrona.com/foundation-models/.\n4 https://github.com/CompVis/stable-diffusion/blob/main/LICENSE\n4\n(a) Canonical\n (b) Adaptation\n(c) Application layers\n (d) Application dependence\nFigure 3: Primitive subgraphs in the ecosystem. W e spotlight a few 3-node subgraphs to build\nintuition for Ecosystem Graphs . (a) The standard pipeline: The Jurassic-1 dataset is used to train the\nJurassic-1 FM, which is used in the AI21 Playground. (b) A common adaptation process: BLOOM\nis adapted by ﬁne-tuning on XP3 to produce the instruction-tuned BLOOMZ model. (c) Layering\nof applications: The GPT -4 API powers Microsoft365 Copilot, which is integrated into Microsoft\nW ord. (d) Dependence on applications: While applications often are the end of pipelines (i.e. sinks in\nthe graph), applications like Google Search both depend on FMs like MUM and support FMs (e.g.\nthrough knowledge retrieval) like Sparrow .\nCaveats. The deﬁnition of Ecosystem Graphs is deliberately minimalistic: our objective is to make127\nthe ecosystem simple to understand to ensure it is legible to diverse stakeholders. Under the hood,128\nwe introduce two additional forms of complexity that we revisit in discussing our implementation of129\nEcosystem Graphs . First, while we conceptualize the \"nodes\" of the ecosystem graph as individual130\nassets, in practice they will instead correspond to sets of closely-related assets (e.g. the different131\nmodel sizes in the GPT -3 model family). Second, we will annotate properties (e.g. the \"license\") by132\nspecifying both a structured value (i.e. the type of license) and a contextual/qualitative description133\n(e.g. the provenance for the information).134\nGiven our deﬁnition, we identify ﬁve challenges that arise directly from the deﬁnition:135\n1. Asset discovery . How do we identify and prioritize the assets?136\n2. Asset representation. How do we represent assets?137\n3. Dependency discovery . How do we identify the dependencies?138\n4. Metadata representation. How do we represent the metadata properties?139\n5. Metadata annotation. How do we annotate the metadata for every node?140\n5\nFigure 4: Graph view for Ecosystem Graphs as of March 16, 2023 (Google’s music-related\nsubgraph). W e highlight salient foundation models (Noise2Music, AudioLM, MusicLM) as well as\nthe shared and intricate dependencies (e.g. on SoundStream and MuLan). W e also observe that music\nmodels at present are often language-controlled (e.g. introducing a dependency on LaMDA) that\nthereby links Google’s music models with the more extensive (and more productionized) language\nmodels (e.g. LaMDA, PaLM). More generally , beyond these language-mediated dependencies (e.g.\noutbound dependencies of PaLM), Google’s music subgraph is fully contained to the nodes depicted\nin this ﬁgure at present (to our knowledge).\nBeyond this, we should ask who does this work, why they would do it, how the information is141\nmaintained (since the ecosystem itself is everchanging), and why it should be trusted. T o ground the142\nEcosystem Graphs framework, we present our concrete implementation before returning to these143\nconceptual challenges and how we navigated them.144\n3.2 Implementation145\nThe ecosystem graph that we have documented thus far is available at https://crfm.stanford.146\nedu/ecosystem-graphs. As of March 16, 2023, the graph contains 262 nodes (64 datasets, 128147\nmodels, 70 applications) built by 63 organizations that are linked together by 356 dependencies.148\nViews. T o visualize the graph structure of the Ecosystem Graphs , we provide a simple graph149\ninterface shown in Figure 4. Users can zoom into speciﬁc regions of the graph to better understand150\nspeciﬁc subgraphs. Alternatively , we provide an interactive table (Figure 5) to search for assets or151\nﬁlter on speciﬁc metadata, which can also be exported to a CSV . Users can include or exclude speciﬁc152\nﬁelds as well as sort by column (e.g. the \"organization\", the asset \"type\"). Clicking on the node’s153\n\"name\" in either the graph or table views will take the user to the associated ecosystem card.154\nEcosystem cards. Each node is associated with an ecosystem card. T o navigate between adjacent155\nnodes in the graph, the node’s dependencies (upstream) and dependents (downstream) are linked156\nto at the top of the page. In Figure 6, we provide the ecosystem card for GPT -3 from https://157\ncrfm.stanford.edu/ecosystem-graphs/index.html?asset=GPT-3: each property includes158\na help icon that clariﬁes what the property refers to (see T able 1). As we describe subsequently , the159\n6\nFigure 5: T able view for Ecosystem Graphs as of March 16, 2023 (sorted by recent models). Over\n10 models were released in the 15 day period, with prominent recent models (e.g. OpenAI’s GPT -4,\nAnthropic’s Claude) disclosing very little to the public (e.g. model size, dependency structure).\necosystem card design aims to centralize useful information, as can be inferred by the abundance of160\nlinks, rather than replicate the information.161\nCode. On the backend, Ecosystem Graphs is a collection of YAML ﬁles that store the annotation162\nmetadata against a pre-speciﬁed schema of ﬁelds that matches T able 1. All aspects of asset selection163\nare handled by the annotator in choosing what to specify in the YAML ﬁle: all speciﬁed assets are164\nrendered. For constructing the graph, the dependencies ﬁeld is used to build edges between the graph:165\nif a dependency is speciﬁed but no ecosystem card has been annotated, a stub node is created in the166\ngraph. Anyone can contribute (e.g. adding new assets, editing existing assets) by submitting a pull167\nrequest at https://github.com/stanford-crfm/ecosystem-graphs that will be reviewed by168\na veriﬁed maintainer of the Ecosystem Graphs effort.169\n3.3 Assets and nodes170\nAssets and nodes are the building blocks for Ecosystem Graphs , framing the ecosystem. W e explore171\nhow to identify assets ( asset discovery ) and group assets into nodes ( asset representation ).172\nAsset discovery . The value proposition for Ecosystem Graphs is to centralize information that173\nwas previously distributed: the ﬁrst step is to be aware of the assets. For datasets and models that are174\ndiscussed in research papers, this is relatively straightforward: indeed, many of the foundation models175\nwe currently document are the subject of research papers. However, even for artifacts discussed176\nin papers, we already identify the presence of dark matter : assets that must exist, but the public177\nsimply knows (essentially) nothing about. The GPT -4 paper provides a striking example: no details178\nwhatsoever are disclosed on the dataset(s) underlying GPT -4 with the report reading \"the competitive179\nlandscape and the safety implications of large-scale models like GPT -4, this report contains no further180\ndetails about . . . dataset construction . . . \" [ OpenAI, 2023b].181\nAs we expand our sights to applications and deployment, we encounter further obscurity . For example,182\nmost company-internal foundation models used in products (e.g. many of the models used in Google183\nSearch) are not disclosed in any regard, let alone the datasets involved in training and improving184\nthese models. And, conversely , we often do not know all the products and applications that depend on185\npublicly-disclosed foundation models (e.g. every downstream application built on ChatGPT or Stable186\nDiffusion). For this reason, as part of our asset discovery process, we make use of less conventional187\n7\nFigure 6: GPT -3 Ecosystem Card. The card contains basic information (e.g. that OpenAI developed\nGPT -3 and when they announced it to the public in 2020), information on how it was built (e.g. a\nstandardized statistics on training emissions in tons of C02 emitted and training time in petaﬂop/s-\ndays), and how it can be built upon (e.g. access is available through the OpenAI API and what uses\nare prohibited by the API usage guidelines).\n8\nresources beyond standard academic research: news articles, corporate press releases, venture capital188\nindices, and other heterogeneous sources. In the future, we may consider automated information189\nextraction from the Internet to keep pace with the scaling of the foundation model ecosystem, though190\nat present the asset discovery process is fully manual and somewhat ad hoc .191\nIn discovering assets, we prioritize assets. While appealing to track every asset, we found this renders192\nthe resulting ecosystem graph cumbersome to navigate and introduces untenable annotation burden.193\nAs a lower bound, there are 150k+ models and 20k+ datasets available on Hugging Face alone.194\nConsequently , we make our priorities and decisionmaking explicit through principle(s) that inﬂuence195\nasset selection. Namely , we include assets that (i) are socially salient, (ii) have outsized impact or196\n(iii) represent a broader class (e.g. ensuring we include at least one music foundation model). These197\nare imprecise and subjective criteria: we believe all current assets can be justiﬁed under them, but we198\ntend to be liberal in our interpretation, preferring to over-include rather than under-include. As the199\necosystem graph expands and its value/uses become more clear, we imagine revisiting these criteria200\nto arrive at more durable and precise principles.201\nAsset representation. Once we have determined the assets to be included in principle, we observe202\nthat many assets are very closely connected. Therefore, pragmatically , we choose to group closely-203\nrelated assets together into a single node in representing the ecosystem graph. W e believe this204\nrepresentational choice improves the legibility of the ecosystem. Concretely , we group assets together205\ninto a single node when they are presented as belonging to the same category in the view of their206\ndevelopers or other entities in the ecosystem. As examples, variants of datasets are often referred207\nto using the same term (e.g. many datasets are called \"English Wikipedia\") as are products (e.g.208\nthe unannounced versions of Google Search). Further, models often belong to a shared collective209\neven if they differ (most often in size) such as the four models introduced by Brown et al. [2020]210\nthat are collectively referred to as GPT -3. There are trade-offs: by collapsing to one node, we211\npotentially obscure slight differences, though if we observe a distinction is relevant, we can choose to212\ndis-aggregate the node back into its constituent assets. Overall, similar to how we determine which213\nassets to include, we construct graph nodes by prioritizing usefulness over faithfulness to the true214\necosystem.215\n3.4 Dependencies216\nT o deﬁne the graph structure, we need to identify the dependencies of each node (i.e. specify D(a) for217\nevery a). In practice, determining the dependencies of assets introduced in research papers is fairly218\nstraightforward, though there are increasingly exceptions (e.g. the aforementioned GPT -4 dataset).219\nMore frequently , even for assets described in research, the assets they depend upon themselves may be220\nopaque. For example, the AI21 Jurassic-1 models are trained on a dataset that is obliquely described221\nin a single sentence: \"Our model was trained with the conventional self-supervised auto-regressive222\ntraining objective on 300B tokens drawn from publicly available resources, attempting, in part, to223\nreplicate the structure of the training data as reported in Brown et al. (2020).\" [ Lieber et al. , 2021].224\nAs a result, we have an asset called \"Jurassic-1 training dataset\" that the Jurassic-1 models depend225\nupon which will be poorly documented (since we only know the dataset size and nothing else). But in226\nother cases, especially those involving assets that are commercial products, the dependency structure227\nitself is almost entirely unknown.228\nOverall, we found that annotating the dependency structure is complicated in many practical settings,229\nyet essential for structuring the ecosystem. For example, from the dependencies alone, we come to230\nrealize that assets built by 5+ organizations all depend on EleutherAI’s The Pile (see Figure 1). And231\nit identiﬁes other forms of structure: for example, many more applications today depend on OpenAI232\nlanguage models than Anthropic language models, even though the models benchmark similarly233\n[Liang et al. , 2022a], suggesting differential social impact.234\n3.5 Ecosystem cards235\nHaving deﬁned the graph structure, we instrument each node by further documenting properties,236\ndrawing inspiration from existing documentation approaches such as data sheets [ Gebru et al. , 2018]237\nand model cards [ Mitchell et al. , 2018]. W e iteratively developed our collection of properties based238\non two principles: (i) we emphasize properties that are ecosystem-centric (e.g. how nodes are239\ninﬂuenced by their dependencies or shape their dependents) and (ii) we ofﬂoad documentation that240\n9\nCategory Field Description\nBasic\nname (G) Name of the asset (unique identiﬁer).\norganization (G) Organization that created the asset.\ndescription (G) Description of the asset.\ncreated date (G) When the asset was created.\nurl (G) Link to website or paper that provides a detailed description of the asset.\ndatasheet (D) Link to the datasheet describing the dataset.\nmodel card (M) Link to the model card describing the model.\nmodality (D, M) Modalities associated with the asset (e.g. text, images, videos).\noutput space (A) Description of the application’s output space (e.g. generation, ranking, etc.).\nsize (D, M) How big the (uncompressed) dataset is.\nsample (D) Small sample of content from the dataset.\nanalysis (D, M) Description of any analysis (e.g. evaluation) that was done.\nConstruction\ndependencies (G) A list of nodes (e.g. assets, models, applications) that were used to create this node.\nquality control (G) What measures were taken to ensure quality , safety , and mitigate harms.\nincluded (D) Description of what data was explicitly included and why .\nexcluded (D) Description of what data is excluded (e.g. ﬁltered out) and why .\ntraining emissions (M) Estimate of the carbon emissions used to create the model.\ntraining time (M) How much time it took to train the model.\ntraining hardware (M) What hardware was used to train the model.\nadaptation (A) How the model was adapted (e.g. ﬁne-tuned) to produce the derivative.\nDownstream\naccess (G) Who can access (and use) the asset.\nlicense (G) License of the asset.\nintended uses (G) Description of what the asset can be used for downstream.\nprohibited uses (G) Description of what the asset should not be used for downstream.\nmonitoring (G) Description of measures taken to monitor downstream uses of this asset.\nfeedback (G) How downstream problems with this asset should be reported.\nterms of service (A) Link to the terms of service.\nmonthly active users (A) Rough order of magnitude of number of active users.\nuser distribution (A) Demographic and geographic distribution of users.\nfailures (A) Description of known failures/errors.\nT able 1: Ecosystem card properties. For nodes, we annotate these properties: (G) indicates\nGeneral properties across all asset types, whereas D/M/A indicate type-speciﬁc properties for\nDatasets/Models/Applications, respectively .\nexists elsewhere (e.g. pointing to existing model cards) to avoid reinventing the wheel. For each node,241\nwe refer to the associated metadata as the node’s ecosystem card .242\nIn T able 1, we decompose the processing of ﬁlling out the ecosystem card into:243\n1. Basic properties of the node (e.g. the developer organization).244\n2. Construction properties of the node (e.g. the training emissions for models).245\n3. Downstream properties of the node (e.g. the license and terms-of-service for applications).246\nFor each property , we annotate a value and, potentially , a description that justiﬁes/explains how the247\nvalue should be interpreted (e.g. attributes a source to provide provenance).248\n3.5.1 Basic properties249\nT o understand nodes and assets even independent of the broader ecosystem, certain basic information250\nis necessary such as the \"name\" of the asset(s) and the \"organization\" that produced the asset(s).251\nT o document these properties proves to be fairly straightforward in practice, though there are some252\ncomplexities. Unfortunately , there may be opacity even for these basic properties. (For example,253\nthe naming convention for OpenAI models has been historically unclear 5 Or the granularity may254\nnot be obvious in the case of organizations: as an example, we annotate Copilot as developed by255\nGitHub even though GitHub has been acquired by Microsoft. In the case of the \"description\" ﬁeld,256\nwe generally quote the asset(s) developers, along with providing the \"URL\" that disclosed the node257\nto the public (e.g. the paper or press release). In addition to these properties, we speciﬁcally highlight258\nthe \"created date\": as Ecosystem Graphs is maintained over time, ﬁltering on the date can be used to259\nautomatically build timelines and understand how the ecosystem evolves. As an example, ﬁltering for260\nnode(s) before January 1, 2023 vs. after January 1, 2023 makes obvious both (i) the early adopters of261\nfoundation models and (ii) how publicly-announced deployments have rapidly accelerated in 2023.262\n5 See https://platform.openai.com/docs/model-index-for-researchers .\n10\n3.5.2 Construction properties263\nA node’s dependencies by no means fully determine its nature: there are many choices on how to264\nuse these dependencies (e.g. many products can be built from the same model, different models can265\narise from training on the same dataset). However, it is challenging to meaningfully summarize this:266\nthe training procedure for many prominent foundation models can amount to dozens of pages in267\na well-written paper [see Chowdhery et al. , 2022], meaning even a summary would be very long268\nand likely not much more useful than pointing to the paper itself. Having surveyed a variety of269\nassets, we converged to (i) a broad umbrella category of quality control for all assets, (ii) deliberate270\ninclusion/exclusion for datasets (e.g. ﬁltering out \"toxic\" content based on a word list, which may271\nhave the side-effect of removing LGBTQ+ language [ Dodge et al. , 2021, Gururangan et al. , 2022]),272\n(iii) material training costs for models (e.g. to contextualize environmental impact [ Lacoste et al. ,273\n2019, Strubell et al. , 2019, Henderson et al. , 2020, Luccioni and Hernández-García , 2023]) and (iv)274\nadaptation details for applications (e.g. ﬁne-tuning details and UI design). W e found these details275\nprovide important context since dependencies visually look the same in Ecosystem Graphs , yet276\nthese relationships are non-equal.277\n3.5.3 Downstream properties278\nT o construct the ecosystem graph, we specify dependencies on the target asset: given a node, we279\nannotate its parents. 6 However, some properties of an asset inﬂuence how it can be built upon, rather280\nthan how it was built. Most notably , the access status of an asset directly determines who can build281\non it, whereas the intended/prohibited uses inﬂuence how the asset should be built upon (in addition282\nto the license and terms of service ). In general, we found these properties to be straightforward to283\nannotate, though we ﬁnd discussion of intended/prohibited uses is quite uneven and in some cases no284\nlicense/terms of service could be found.285\nBeyond how the asset is built upon, we further annotate ﬁelds that determine the asset’s downstream286\nsocial impact. Most notably , this makes clear the sense in which applications ground out much of the287\nimpact: applications have end users . Further, to build upon the transparency of Ecosystem Graphs ,288\nwe highlight important mechanisms for accountability/recourse that we track: (i) can asset developers289\nmonitor the usage of their assets downstream, (ii) do speciﬁc failures or harms concretely arise, and290\n(iii) when these issues come up, do feedback mechanisms exist to propagate this information back291\nupstream? These ﬁelds signal assets that are having high impact, which could confer recognition (or292\neven payment) to those who contributed to their widespread downstream impact (e.g. valuing data293\ncreators whose data generates value downstream).294\n3.5.4 Complementarity of construction and downstream properties295\nThe construction and downstream properties together enrich the underlying graph in an essential296\nway . When edges are interpreted in the forward direction, they indicate how assets are built; when297\nthe edges are interpreted in the backwards direction, they indicate how feedback would ﬂow back298\nupstream. W e stress this point as a reﬂection of the immaturity of the foundation model ecosystem at299\npresent by analogy to other industries.300\nConcretely , we juxtapose the FM ecosystem with the automobile industry as a more established301\nindustry with robust practices for its supply chain. The National Highway Trafﬁc Safety Adminis-302\ntration (NHTSA; an agency under the US Department of Transportation), among other entities, is303\ntasked with ensuring automotive safety . 7 Following the forward ﬂow of materials through the supply304\nchain, when a batch of parts (e.g. brakes) is found to be sub-standard, established protocols mandate305\nthe recall of the ﬂeet of cars built using those parts. Since the National Trafﬁc and Motor V ehicle306\nSafety Act was enacted in 1966, the NHTSA has recalled over 390 million cars due to safety defects.307\nConversely , when several cars (possibly from different manufacturers) are reported to be faulty ,308\nthe shared source of the defect can be traced by attributing their common sources/parts. Critically ,309\ncentralized infrastructure exists for consumers to identify how to report issues to the NHTSA (e.g.310\n6W e found this natural since, in general, we may not know the dependents of a given upstream asset (e.g.\nChatGPT continues to accrue new dependents well after its initial release), but we can better trace the lineage\nwhen annotating the downstream asset.\n7 See their guidelines on motor vehicle safety at https://www.nhtsa.gov/sites/nhtsa.gov/files/\ndocuments/14218-mvsdefectsandrecalls_041619-v2-tag.pdf .\n11\nthe Department of Transporation’s V ehicle Safety Hotline), to interpret how their report will be used,311\nto understand the investigation process the NHTSA implements, and to understand the legal remedies312\nand consumer protections they are afforded. And the Federal Motor V ehicle Safety Standards sets313\nformal and transparent standards on what constitutes the minimum performance requirements for314\neach (safety-relevant) automotive part.315\nIn short, the automobile industry and its practices/norms illustrate the virtues of observing the316\necosystem as a whole and how both forward and backwards traversals can directly inform action.317\nIf an upstream asset (akin to the faulty brakes) is identiﬁed to be faulty in the FM ecosystem, we318\nare not conﬁdent that broad communication, let alone interventions like a recall, can reliably be319\nexpected to occur. For example, if LAION-5B was shown to be data poisoned [ Carlini et al. , 2023],320\nhow would the developers of Stable Diffusion, the subsequent application developers who built321\nupon Stable Diffusion, and the broader end userbase be notiﬁed? In part, this uncertainty simply322\nis due to the absence of comparable entities to the NHTSA that are responsible for governing the323\necosystem, but also due to the absence of infrastructure for information propagation. Similarly ,324\nmany assets themselves lack monitoring mechanisms (especially when assets are released openly ,325\nmonitoring is currently often nonexistent), let alone publicly-disclosed means for relaying feedback326\nand incident reports upstream. While we expect the organizations may have partnerships (e.g. when327\nKhan Academy users surface issues, Khan Academy and OpenAI likely have signiﬁcant dialogue328\nto diagnose if these problems arise from Khan Academy’s use of OpenAI’s GPT -4), we speciﬁcally329\nhighlight the inadequacy for end users and consumers. In other words, the basic consumer protections330\nfor ensuring an individual adversely affected by a product downstream is able to communicate this331\nupstream and be taken seriously do not exist.332\n3.6 Annotation practices333\nT o annotate each property requires dealing with many types of variation: different assets often have334\nspeciﬁc properties that are idiosyncratic and, in many instances, information is not publicly available.335\nAs we iterated on annotation best practices, we identiﬁed two key concepts: (i) how should we336\ninterpret missing data entries and (ii) how can we trust the recorded information?337\nMissing data. W e identiﬁed four forms of missing data that arise under different conditions: each338\nform has different semantics, which we clarify by annotating the value for the associated property339\ndifferently .340\n1. None. A property is annotated with the value none if an annotator looked for the information341\nand was unable to ﬁnd it. For example, for many nodes, we could not ﬁnd any feedback342\nmechanism or monitoring information. It is possible a feedback form does exist, but given343\nwe could not ﬁnd it, we believe this information is effectively too obscure (e.g. unreasonable344\nto expect a consumer to ﬁnd when reporting feedback).345\n2. Unknown. A property is annotated with the value unknown if the information must exist346\n(and, potentially , an annotator looked for the information and was unable to ﬁnd it). For347\nexample, the training hardware and training time are often not disclosed for many models,348\nbut of course the models were trained on some hardware and took some time to train.349\n3. Empty string . A property is annotated with the empty string if the annotator chose to not350\nannotate the property , which generally indicates a lack of time. T o ensure all ecosystem351\ncards have non-zero information, we minimally require the basic properties be annotated.352\n4. N/A. A property is annotated with the value N/A if the property is not applicable to the353\nnode/asset(s). By design, the properties are chosen to be broadly applicable, so we encounter354\nthis annotation very rarely .355\nWhen aggregated across the entire ecosystem, these conventions for missing data help to articulate356\npervasive opacity (i.e. many unknown values) and immaturity (i.e. many none values) in the357\nfoundation model ecosystem.358\nT rust. T o ensure information in Ecosystem Graphs is legitimate and credible, we implement359\ntwo mechanisms. First, to add or modify information, all such requests must be veriﬁed by a360\nvetted maintainer to ensure the correctness of the information as well as the consistency with prior361\nannotations. Consequently , especially as Ecosystem Graphs expands to be a community-maintained362\n12\nartifact, this form of moderation provide some guarantees on the sustained quality of the resource.363\nIn fact, since Ecosystem Graphs is implemented as a GitHub repository , the full version history364\nof commits (and any associated discussion) is maintained to ensure the provenance of information365\n(much akin to the Wikipedia change log). Moving forward, more sophisticated moderation (e.g.366\nakin to Wikipedia maintenance) could be developed. Second, to source the information, we require367\nthat information be attributable to a publicly-available source that is provided in the description368\naccompanying the property’s value. In other words, information should have clear provenance to369\nboth ensure the annotation matches the source and that the source itself is reliable. In the future,370\nwe imagine this constraint may prove restrictive (namely because it is inconvenient for reporting371\nprivately-disclosed information), but we imagine conventions akin to those in journalism can be372\nadopted if necessary .373\n3.7 Maintenance and Incentives374\nEverything in the ecosystem graph is subject to change. Since foundation models are being deployed375\nremarkably quickly , keeping pace in documenting the ecosystem is an ongoing challenge. For376\nexample, in the week of March 13, 2023, over a dozen products were announced that all depend on377\nOpenAI’s GPT -4. Further, even for existing assets, their dependencies or metadata may change over378\ntime: Anthropic’s Claude and Google’s PaLM were initially closed access, but now are limited access379\nwith the release of their respective APIs for downstream development.380\nFor this reason, we explore who will maintain Ecosystem Graphs and whether incentives misalign-381\nment introduces challenges, given much of the value is contingent on the graph being up-to-date and382\ncorrect. Moving forward, as foundation models feature more centrally in broader social discourse and383\nEcosystem Graphs sees greater adoption, maintenance could be mandated as a policy requirement384\nto ensure sustained transparency [see Bommasani et al. , 2023, M ˛ adry, 2023].385\nWho maintains the ecosystem graph? T o this point, the ecosystem graph has been built and386\nmaintained by the authors of this work. Moving forward, while this will continue, this will be387\nincreasingly insufﬁcient given the growing scale of the ecosystem. Therefore, we envision two388\ncomplementary strategies for expanding the group involved in maintaining Ecosystem Graphs .389\nBuilding on traditions of open source software development (e.g. Wikipedia, Mozilla, PyT orch,390\nHugging Face), we actively encourage contributions. T o broaden accessibility , new assets can391\nbe submitted from a public Google form to remove the barriers of having a GitHub account and392\nfamiliarity with GitHub. Further, for this reason we use a lightweight process with explicit guidelines393\non how to create and edit entries in YAML. Drawing upon trends in open source, we will implement394\nprocesses for top contributors to be recognized for their achievement and signal-boosted in the broader395\ncommunity .8396\nWhile open source contributions can be very powerful, permitting decentralized contribution to397\nthe shared knowledge resource, they require a culture to form that supports and sustains them. As398\nEcosystem Graphs grows and is more broadly adopted, we envision it may become a broad-use399\npublic repository that organizations themselves are incentivized to maintain. In the future, we400\npropose that major FM organizations each select a dedicated representative responsible for the upkeep401\nof the organization’s nodes (and, possibly , some direct neighbors). This mechanism introduces402\naccountability: the veracity of an organization’s nodes and dependencies is the responsibility of this403\nmaintainer. Here, we could lean on practices of periodic public reporting (e.g. quarterly ﬁnancial404\nearnings) in reminding the representative to update the graph on a speciﬁc cadence. W e imagine the405\nspeciﬁcs of this will further sharpen as Ecosystem Graphs is more broadly adopted, and as we better406\nunderstand both the rate of change in the ecosystem and the informational needs that Ecosystem407\nGraphs serves. In the future, the process of updating the ecosystem graph could be integrated into408\norganizational-internal data entry , since much of what is tracked in Ecosystem Graphs is likely409\nalready being tracked within organizations.410\nThe compatibility of incentives. Ensuring the ecosystem is transparent serves many informa-411\ntional needs and beneﬁts many stakeholders. Much akin to other shared knowledge resources (e.g.412\nWikipedia, the US Census), downstream use cases continuously arise, further incentivizing the413\n8While we do not currently implement any extrinsic bounties, works like Zhao et al. [2017] and Chowdhury\nand Williams [2021] demonstrate their efﬁcacy , warranting further consideration in the future.\n13\nsustained upkeep of the resource. In the FM ecosystem speciﬁcally , we expect asset developers will414\nbe incentivized to disclose the impact of their assets (e.g. organizations often put out press releases415\nto disclose information on the widespread use of their products). 9 Or, akin to how food vendors416\nproactively announce their food is organic or how cosmetics companies proactively indicate their417\ncosmetics involve the humane treatment of animals, asset developers should deﬁnitely be incentivized418\nto highlight their own responsible conduct. While incentives may not exist for every bit of informa-419\ntion to be made transparent, we hope Ecosystem Graphs will encourage increased transparency by420\ndemonstrating the value of making information and knowledge public.421\nHowever, we do recognize there simultaneously exist pernicious incentives for organizations to422\nmaintain opacity in the ecosystem: most directly , transparency could infringe on corporate secrets and423\ncommercial interest. Central to our approach in Ecosystem Graphs is recognizing that commercial424\ninterest need not entail a blanket ban on transparency: in many cases, information can be made425\ntransparent to the public while not compromising any commercial agenda. In other words, the426\ninformation in question is common knowledge amidst an organization’s competitors, and it is better427\nfor the public to have partial transparency rather than to have nothing at all. This approaches aligns428\nwith having representatives of each organization: the process of making assets transparent can involve429\nengaging with the organization, ﬂexibly and iteratively identifying the boundaries of transparency in430\nan organization-speciﬁc and asset-speciﬁc way (e.g. OpenAI’s desire for transparency may change431\nfrom 2021 to 2022, or from CLIP to ChatGPT). More expansively , Ecosystem Graphs mediates432\nan incremental process for building norms of transparency [ Liang et al. , 2022b, Bommasani et al. ,433\n2023] and functions as an inroad for policy intervention as speciﬁc informational needs grow more434\nimportant.435\n4 Use Cases for Ecosystem Graphs436\nT o demonstrate the value of Ecosystem Graphs , we enumerate social roles to explain their concrete437\ninformational needs and fundamental questions that Ecosystem Graphs addresses. Ecosystem438\nGraphs may be insufﬁcient to fully meet their demand, but we highlight how it makes progress: is a439\nminimum transparency standard and unifying abstraction across diverse use cases.440\nFoundation model developers. FM developers need to be aware of the available assets they can441\nbuild upon (e.g. new instruction-tuning datasets like P3), the assets they will compete with (e.g. new442\nreleases from competitors), and current/foreseeable demand from downstream application developers.443\nEach of these informational needs are directly addressed from the ecosystem graph. In fact, at a444\nmore ﬁne-grained level, the ecosystem graph allows developers to compare their practices with their445\ncompetitors. For example, developer A may come to realize that developer B implements a more446\nrobust quality control pipeline. While developers likely already track this information themselves447\nabout their competitors, centralized information could inform more intentional decisions in choosing448\nto either conform to or diverge from the practices of other developers [e.g. norms on release; Liang449\net al. , 2022b].450\nApplication developers. Akin to foundation model developers, downstream application developers451\nalso should be aware of the full space of foundation models in choosing which foundation model452\nto build upon (or investing to build foundation models themselves). However, the graph structure453\nprovides further clarity , since implicitly it indicates which foundation models are more popular454\n(perhaps because they are easier or preferable to build upon). This signal is complementary to other455\nsignals [e.g. comparing models on standardized evaluations like HELM; Liang et al. , 2022a], because456\napplication developers can make decisions informed by prior developer choices. For example, several457\norganizations (i.e. OpenAI, Cohere, Anthropic) currently offer similar language model API offerings458\nbut they have been differentially adopted by downstream developers. W e could imagine that a new459\ncopywriting startup might ﬁnd this information illuminating in making decisions on which API to460\nuse: (i) if they use the same API as their competitors, how will they distinguish themselves, or (ii)461\nif they use a different API, why is it better for them given their competitor chose to use a different462\none. In fact, metadata in the ecosystem card could be composed to further constrain the search space:463\n9 For example, see the strategic partnership between Hugging Face and Amazon: https://huggingface.\nco/blog/aws-partnership.\n14\nfor example, one could look for a permissively-licensed available model that has been evaluated on464\nrelevant benchmarks (by ﬁltering on the \"license\", \"access\", and \"analysis\" properties).465\nEnd users of FM applications. Consumers deserve to know how the technology they use was466\nbuilt, akin to requirements that require food in the US be labeled with ingredients. 10 The graph467\nstructure and the simple web interface make this practical: a user can look up the product, see if468\nit is in the graph, and trace its dependencies. This process surfaces any existing mechanisms for469\nfeedback reporting, which could prove to be useful if the user experiences an issue. While the existing470\ndocumentation is quite scarce, the user would also be able to ﬁnd any existing documentation of471\nsimilar issues or failures [see Costanza-Chock et al. , 2022]. In the future, we imagine this information472\ncould become the basis for more formal consumer protections: if a user experiences harm, what are473\ntheir means for recourse? Or if they pursue legal action, how might society attribute responsibility to474\ndifferent entities implicated upstream? Symmetrically , the beneﬁts to end users do not only involve475\nharm mitigation: for example, if a user is able to better understand how their data would be used,476\nthey might be more informed and more willing to donate their data to data collection efforts.477\nInvestors. Much as developers beneﬁt from Ecosystem Graphs in that it better allows them to478\nunderstand their competitors, investors beneﬁt from Ecosystem Graphs in that it identiﬁes new479\nopportunities. For example, a venture capitalist could sort by \"modality\" and \"created date\" to480\nunderstand modalities on the rise (e.g. music) even before high-proﬁle products emerge for this481\nmodality . In turns, this suggests prime candidates for investors to take risk and fund early on the basis482\nof concrete data and trends. Compared to parallel public efforts in venture capital (e.g. market maps483\ndocumenting the foundation model ecosystem [ Turow et al. , 2023] and speciﬁc startups [ Kaufmann484\net al. , 2022]), Ecosystem Graphs provides ﬁner-grained insight by grounding to speciﬁc assets rather485\nthan institution-level trends. W e revisit the contrast between asset-centric and institution-centric486\nanalysis in §5.2.487\nAI researchers. Ecosystem Graphs provides an array of functionalities that are relevant for AI488\nresearch. However, one of the most fundamental is the increased potential for AI researchers to be489\naware of how foundation models are deployed. As a concrete example, many of the applications490\nof image-based models like Stable Diffusion diverge from what has been traditionally studied in491\ncomputer vision research. Perhaps even more clearly , billions have been invested into language-based492\nstartups whose applications (e.g. copywriting) clearly differ from standard tasks studied in natural493\nlanguage processing (e.g. natural language inference).494\nWhile academic research should not blindly follow industry trends, we do believe academic research495\nshould pay more attention to how research materializes in practice. Ecosystem Graphs precisely496\nprovides the opportunity for such reﬂection. Much of AI research concentrates on building better497\nmodels in many senses (e.g. more accurate, efﬁcient, robust, fair): understanding (i) what is being498\ndeployed in society , (ii) the demand for such technology , and (iii) the resulting societal impact all can499\nhelp AI researchers better achieve their goals. Similarly , many AI benchmarks are designed such that500\nprogress on the benchmark will generalize more broadly: aligning these benchmarks with deployment,501\nor at least measuring correlations between benchmark-induced rankings and deployment-induced502\nrankings, could prove fruitful in realizing this vision. W e speciﬁcally highlight this for work on503\nthe harms of foundation models and AI: understanding how risks posited in the scientiﬁc literature504\n[Bender et al. , 2021, W eidinger et al. , 2022, Bommasani et al. , 2021, Abid et al. , 2021, Buchanan505\net al. , 2021, inter alia ] manifest in practice could clarify how these risks should be studied, measured,506\nmitigated, and prioritized.507\nEconomists. Foundation models are general-purpose technologies [ Bresnahan and Trajtenberg ,508\n1995, Brynjolfsson et al. , 2021] that deﬁne an emerging market [ Eloundou et al. , 2023, Bommasani509\net al. , 2021, §5.5] worthy of study in the (digital) economy [ Acemoglu and Autor , 2010, Acemoglu510\nand Restrepo , 2018]. Early work shows that foundation models can complete tasks of signiﬁcant511\neconomic value [ Noy and Zhang , 2023, Felten et al. , 2023, Korinek, 2023], i.e. the realizable potential512\nof foundation models. Ecosystem Graphs naturally complements this work by deﬁning the realized513\nimpact of foundation models at macro-scale, complementing more grounded analyses such as Peng514\net al. [2023] on developer productivity using GitHub Copilot and Eloundou et al. [2023] on labor515\n10 https://www.fda.gov/food/food-ingredients-packaging/\n15\nexposure using GPT -4. Put together, these works delineate inefﬁciencies and potential opportunities:516\nwhere are foundation models not being deployed, despite showing demonstrable potential for these517\njobs? More broadly , we believe Ecosystem Graphs naturally supports efforts to understand the518\nmarket structure of AI and the digital economy[ Brynjolfsson and Mitchell , 2017, Acemoglu et al. ,519\n2020, Agrawal et al. , 2021, Autor et al. , 2022]. How do pre-existing inter-organizational relationships520\n(e.g. between Microsoft and Google) interface with the new relationships mediated by the rise of521\nfoundation models?522\nAuditors. Auditors need to prioritize assets to allocate attention to [see Metaxa et al. , 2021, Raji523\nand Buolamwini , 2019]. T o inform these decisions, Ecosystem Graphs provides several forms524\nof guidance. Most directly , auditors can prioritize (i) assets with known reports of \"failures\", (ii)525\nunsatisfactory \"quality control\" or \"analysis\" information and (iii) assets with signiﬁcant opacity (i.e.526\nmuch is unknown about the node). Further, auditors should factor in the impact of these nodes: we527\nrecommend auditors target algorithmic monocultures [Kleinberg and Raghavan , 2021, Bommasani528\net al. , 2022] that are made legible by Ecosystem Graphs . Namely , if an upstream asset has extensive529\ndownstream dependencies (e.g. the ChatGPT API as in Figure 1), then risks associated with this530\nasset may propagate downstream perniciously if unchecked (e.g. what if the ChatGPT API goes531\ndown?11 ; what if LAION-400M is subject to data poisoning [ Carlini et al. , 2023, Bommasani et al. ,532\n2021, §4.7]?). Bommasani et al. [2022] directly recommend this by studying how monocultures533\nmay lead to homogeneous outcomes : speciﬁc end users of different foundation applications being534\nsystemically failed due to the shared upstream dependence.535\nPolicymakers. AI policy speciﬁc to foundation models is still in its infancy: Bommasani et al.536\n[2023] speciﬁcally highlight the importance of transparency , directly aligning with the value propo-537\nsition of Ecosystem Graphs . Properties like the \"access\" status, \"license\", and \"terms of service\"538\nat the scale of the ecosystem are policy-relevant: for example, policymakers should intervene if539\nthere is pervasive dependence on assets that are largely inaccessible, as these nodes will likely evade540\nexternal scrutiny/oversight despite having outsized societal impact. Beyond this, the distribution of541\ndifferent uses of foundation models will help clarify whether policy should adopt the perspective of542\nregulating only the downstream applications (e.g. if there is too much diversiﬁcation in downstream543\nuse) or consider the upstream foundation models as well (which introduces challenges due to their544\ngenerality/lack of speciﬁcity). 12 In testimony before the US House Subcommittee on Cybersecu-545\nrity , Information T echnology , and Government Innovation, M ˛ adry[2023] more broadly pointed to546\nhow policymakers should prioritize the emerging AI supply chain based on foundation models. T o547\nprecisely track this supply chain, and how it evolves over time, Ecosystem Graphs provides the548\nconcrete public infrastructure.549\n5 Related W ork550\nT o situate our work, we consider both our objective to make foundation models transparent and our551\nmethodology to track dependencies.552\n5.1 T ransparency in AI553\nT o make AI systems transparent, we observe three broad classes of approaches.554\nFirst, evaluation is a widespread practice for articulating the properties and measuring the behavior of555\nsystems: in the research community , it is customary to evaluate systems against particular benchmarks556\nto assess their performance. Evaluations can vary in the speciﬁc type of transparency [see Bommasani557\net al. , 2023] they provide: some evaluations quantify the accuracy of models [e.g. ImageNet; Deng558\net al. , 2009], others stress-test models [e.g. CheckList; Ribeiro et al. , 2020] or adversarially identify559\nfailures [e.g. red-teaming; Perez et al. , 2022] and still others characterize models along a range of560\ndimensions [e.g. HELM; Liang et al. , 2022a]. In general, while some efforts expand evaluation to561\ndatasets [ Bommasani and Cardie , 2020, Swayamdipta et al. , 2020, Ethayarajh et al. , 2022, Mitchell562\n11 See https://status.openai.com/incidents/y6cdztrnth60.\n12 See the recent announcement of a UK FM taskforce: https://assets.publishing.service.gov.\nuk/government/uploads/system/uploads/attachment_data/file/1142410/11857435_NS_IR_\nRefresh_2023_Supply_AllPages_Revision_4_WEB_PDF_NoCrops_-_gov.uk.pdf\n.\n16\net al. , 2022] or adopt methodologies from human-computer interaction to consider human factors563\nlike user experience [ Lee et al. , 2022a,b], for the most part, evaluation aims to characterize a speciﬁc564\nmodel in isolation.565\nSecond, documentation is a growing practice for specifying metadata about the broader context that566\nsituates model and system development. Formative works like data sheets [ Gebru et al. , 2018] and567\nmodel cards [ Mitchell et al. , 2018] brought this approach to the fore, complementing evaluations by568\narticulating design decisions and developer positions involved in creating assets. Subsequent efforts569\nhave enriched these approaches to make these documentation artifacts more useful, accessible, or570\notherwise aligned to speciﬁc informational needs [ Crisan et al. , 2022].13 In general, documentation571\nefforts aim to contextualize a speciﬁc asset against a broader social backdrop, often with an inclination572\ntowards how the asset came to be and with greater uptake for research-centric assets to our knowledge.573\nThird, analyses and critiques have become increasingly relevant, showcasing much of the latent574\nand oft-overlooked underpinnings of AI development and deployment. These works often bring575\nquestions of values and power to the fore, frequently appealing to concepts or methodologies from576\nthe social sciences and disciplines beyond computer science [e.g. Dotan and Milli , 2020, Ethayarajh577\nand Jurafsky , 2020, Scheuerman et al. , 2021, Raji et al. , 2021, Koch et al. , 2021, Denton et al. , 2021,578\nPaullada et al. , 2021, Birhane et al. , 2022, Bommasani, 2022]. Rather than speciﬁc assets (other than579\nfor case study/illustration), analytic work centers broader themes [e.g. algorithmic intermediaries;580\nLazar, 2023] or classes of technology [e.g. predictive optimization; W ang et al. , 2022b].581\nOur framework shares the objective of making AI (speciﬁcally foundation models) transparent.582\nHowever, it differs along important axes from all of these established approaches, perhaps most583\nclosely resembling the documentation class (since, indeed, Ecosystem Graphs is a documentation584\nframework). In contrast to current interpretations of evaluation and documentation, Ecosystem585\nGraphs is fundamentally about the ecosystem rather any speciﬁc asset: the value of Ecosystem586\nGraphs arises from tracking all assets. 14 This introduces a variety of new challenges (e.g. partial587\nobservability of certain information, more complicated maintenance as there is constant change588\nacross the ecosystem even if particular assets may not change for extended periods). Further,589\nEcosystem Graphs especially highlights the importance of grounding out into applications (for590\nwhich a general-purpose analogue of data sheets and model cards does not exist to our knowledge)591\nand, more generally , moving beyond research artifacts to commercial artifacts that affect broader592\nsociety . Finally , in comparison to analytic/critical methods, Ecosystem Graphs retains much of593\nthe concreteness of evaluation/documentation: we believe Ecosystem Graphs provides valuable594\ndescriptive understanding that could support future normative analyses and critiques.595\nBeyond these distinctions, we emphasize that our contribution extends beyond most prior works on596\ndocumentation in AI. Concretely , most prior works [e.g. Gebru et al. , 2018, Bender and Friedman ,597\n2018, Mitchell et al. , 2018] propose the framework to document artifacts, perhaps with an additional598\nproposal of who will conduct this documentation and how/why . In contrast, we concretely execute,599\nimplementing the Ecosystem Graphs framework in our codebase and public website. This mirrors600\nworks like HELM [ Liang et al. , 2022a] where, in addition to designing an evaluation, the contributions601\ninclude evaluating all language models available at present. The infrastructure, sustained upkeep602\nand, ultimately , the resource itself are what provide value: ensuring transparency requires we follow603\nthrough and enact the conceptual frameworks we design.604\n5.2 Dependencies605\nAt the technical level, Ecosystem Graphs foreground the tracking of dependencies, whereas at606\nthe social level, Ecosystem Graphs delineates institutional relationships. Both of these constructs607\nare encountered in almost every mature industry and, therefore, have been studied across a range608\nof ﬁelds. Concretely , almost every commercial product is the composite of some collection of609\nmaterials/ingredients, meaning it has a complex supply chain. As a result, we speciﬁcally point to610\nrelated work in open-source software (which share similar implementation to Ecosystem Graphs )611\nand market structure (which emulate Ecosystem Graphs in terms of organizations).612\n13 See https://huggingface.co/docs/hub/model-cards.\n14W e do note other works exist at ecosystem scale in other senses such AI100 [ Stone et al. , 2022], AI\nIndex [ Zhang et al. , 2021], and various data/model repositories [ W olf et al. , 2020, Lhoest et al. , 2021]; see\nhttps://modelzoo.co/.\n17\nOpen-source software. Much like foundation models, open-source software development is sus-613\ntained by an immense network of dependencies. Akin to our efforts to track the foundation model614\necosystem, the demand to track the open-source software ecosystem is immense: the software bill of615\nmaterials (SBOM) is a national-level initiative of the US’s Cybersecurity and Infrastructure Security616\nAgency to maintain an inventory of the ingredients that make up software [ White House Executive Or-617\nder, 2021].15 These approaches have clariﬁed how to ensure compliance from different stakeholders618\n(e.g. software vendors) and how to standardize information to support a range of use cases, providing619\ninspiration for the abstractions we make in Ecosystem Graphs . T o implement this vision, a range of620\nefforts have been put forth over the years 16 with applied policy work mapping out the sociotechnical621\nchallenges for maintaining and funding these efforts [ Ramaswami, 2021, Scott et al. , 2023]. Further,622\nthey present an exemplar of policy uptake towards mandatory public reporting of these dependencies623\nas exempliﬁed by the proposed Securing Open Source Software Act of 2022. 17 And, much akin to624\nthe §4: U S E S we consider, these efforts already have shown how descriptive understanding of the625\necosystem directly informs decision-making and characterizes the impact of assets. 18626\nMarket structure. In deﬁning Ecosystem Graphs , we made the fundamental decision to deﬁne the627\ngraph in terms of assets. W e contrast this with approaches more common in disciplines like economics628\nand sociology , where it would be customary to instead foreground the organizations/institutions629\nresponsible for creating these assets [ Rowlinson, 1997]. W e believe this (fairly techno-centric) choice630\nprovides valuable leverage given the status quo: the number of assets is currently still manageable631\n(on the order of hundreds to thousands), the assets themselves are distinctive (e.g. they are not632\nexchangeable in the way oil or steel may be in other market analyses), and speciﬁc assets markedly633\ncontextualize our understanding of organizations (e.g. Stable Diffusion dramatically shapes our634\nperception of Stability AI). In spite of these advantages, we point to a range of works that foreground635\ninstitutions in mapping out market structure and the dynamics by which actors interact to shape the636\neconomy . For example, given we draw upon a comparative analysis of the FM ecosystem to the637\nautomotive ecosystem, W eingast and Marshall [1988] demonstrate that institution-centrism better638\nallow for comparisons/juxtapositions across sectors. Alternatively , Einav and Levin [2010] showcase639\nhow grounding to institutions facilitates various forms of measurement (e.g. due to ﬁrm-level640\nrequirements on information disclosure). Finally , many works in political and institutional sociology641\nprime us to view institutions as the natural unit for studying power relations in modern networks and642\nmarkets [ Frickel and Moore , 2006, Dequech, 2006, Fleury, 2014, inter alia ].643\n6 Conclusion644\nThe footprint of foundation models is rapidly expanding: this class of emerging technology is645\npervading society and is only in its infancy . Ecosystem Graphs aims to ensure that foundation646\nmodels are transparent for a range of informational needs, thereby establishing the basic facts of how647\nthey come to impact society . W e look forward to seeing the community build upon and make use of648\nEcosystem Graphs . Concurrently , we encourage policy uptake to standardize reporting on all assets649\nin Ecosystem Graphs as the basis for driving change in the foundation model ecosystem.650\nReferences651\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,652\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,653\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,654\nJared Quincy Davis, Dorottya Demszky , Chris Donahue, Moussa Doumbouya, Esin Durmus, Ste-655\nfano Ermon, John Etchemendy , Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren656\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, T atsunori Hashimoto, Peter657\nHenderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil658\nJain, Dan Jurafsky , Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani,659\nOmar Khattab, Pang W ei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar,660\n15 See https://www.cisa.gov/sbom.\n16 See https://libraries.io/, http://deps.dev/, and https://ecosyste.ms/.\n17 https://www.congress.gov/bill/117th-congress/senate-bill/4913\n18 See https://docs.libraries.io/overview.html#sourcerank and https://github.com/ossf/\ncriticality_score.\n18\nFaisal Ladhak, Mina Lee, T ony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen661\nLi, T engyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele662\nMunyikwa, Suraj Nair, A vanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Car-663\nlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou,664\nJoon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich,665\nHongyu Ren, Frieda Rong, Y usuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa666\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex T amkin, Rohan667\nT aori, Armin W . Thomas, Florian Tramèr, Rose E. W ang, William W ang, Bohan Wu, Jiajun Wu,668\nY uhuai Wu, Sang Michael Xie, Michihiro Y asunaga, Jiaxuan Y ou, Matei Zaharia, Michael Zhang,669\nTianyi Zhang, Xikun Zhang, Y uhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the670\nopportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.671\nJacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. BERT: Pre-training of672\ndeep bidirectional transformers for language understanding. In Association for Computational673\nLinguistics (ACL) , pages 4171–4186, 2019.674\nT om B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-675\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, Sandhini Agarwal,676\nAriel Herbert-V oss, Gretchen Krueger, T om Henighan, Rewon Child, Aditya Ramesh, Daniel M.677\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,678\nScott Gray , Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,679\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint680\narXiv:2005.14165, 2020.681\nAakanksha Chowdhery , Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam682\nRoberts, Paul Barham, Hyung W on Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,683\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, A. Rao, Parker Barnes, Yi T ay , Noam M.684\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, B. Hutchinson, Reiner Pope, James685\nBradbury , Jacob Austin, M. Isard, Guy Gur-Ari, Pengcheng Yin, T oju Duke, Anselm Levskaya,686\nS. Ghemawat, Sunipa Dev , Henryk Michalewski, Xavier García, V edant Misra, Kevin Robinson,687\nLiam Fedus, Denny Zhou, Daphne Ippolito, D. Luan, Hyeontaek Lim, Barret Zoph, A. Spiridonov ,688\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, T . S. Pillai,689\nMarie Pellat, Aitor Lewkowycz, E. Moreira, Rewon Child, Oleksandr Polozov , Katherine Lee,690\nZongwei Zhou, Xuezhi W ang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason691\nW ei, K. Meier-Hellstern, D. Eck, J. Dean, Slav Petrov , and Noah Fiedel. PaLM: Scaling language692\nmodeling with pathways. arXiv, 2022.693\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.694\nGenerative pretraining from pixels. In Hal Daumé III and Aarti Singh, editors, Proceedings of695\nthe 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine696\nLearning Research , pages 1691–1703. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.697\npress/v119/chen20s.html.698\nAditya Ramesh, Mikhail Pavlov , Gabriel Goh, Scott Gray , Chelsea V oss, Alec Radford, Mark699\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and T ong Zhang,700\neditors, Proceedings of the 38th International Conference on Machine Learning , volume 139 of701\nProceedings of Machine Learning Research , pages 8821–8831. PMLR, 18–24 Jul 2021. URL702\nhttps://proceedings.mlr.press/v139/ramesh21a.html.703\nAlec Radford, Jong W ook Kim, Chris Hallacy , Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,704\nGirish Sastry , Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.705\nLearning transferable visual models from natural language supervision. In International Conference706\non Machine Learning (ICML) , volume 139, pages 8748–8763, 2021.707\nUriel Singer, Adam Polyak, Thomas Hayes, Xiaoyue Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry708\nY ang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Y aniv T aigman. Make-a-video:709\nT ext-to-video generation without text-video data. ArXiv, abs/2209.14792, 2022.710\nYi W ang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu,711\nYi Liu, Zun W ang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Y u, Y ali W ang, Limin W ang, and712\nY u Qiao. Internvideo: General video foundation models via generative and discriminative learning.713\nArXiv, abs/2212.03191, 2022a.714\n19\nMark Chen, Jerry T worek, Heewoo Jun, Qiming Y uan, Henrique Ponde, Jared Kaplan, Harrison715\nEdwards, Y ura Burda, Nicholas Joseph, Greg Brockman, Alex Ray , Raul Puri, Gretchen Krueger,716\nMichael Petrov , Heidy Khlaaf, Girish Sastry , Pamela Mishkin, Brooke Chan, Scott Gray , Nick717\nRyder, Mikhail Pavlov , Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,718\nPhilippe Tillet, Felipe Petroski Such, David W . Cummings, Matthias Plappert, Fotios Chantzis,719\nElizabeth Barnes, Ariel Herbert-V oss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun720\nBalaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, V edant Misra, Evan Morikawa,721\nAlec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter W elinder,722\nBob McGrew , Dario Amodei, Sam McCandlish, Ilya Sutskever, and W ojciech Zaremba. Evaluating723\nlarge language models trained on code. ArXiv, abs/2107.03374, 2021.724\nJohn M. Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov , Olaf Ron-725\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zídek, Anna Potapenko, Alex Bridg-726\nland, Clemens Meyer, Simon A A Kohl, Andy Ballard, Andrew Cowie, Bernardino Romera-727\nParedes, Stanislav Nikolov , Rishub Jain, Jonas Adler, Trevor Back, Stig Petersen, David A.728\nReiman, Ellen Clancy , Michal Zielinski, Martin Steinegger, Michalina Pacholska, T amas Bergham-729\nmer, Sebastian Bodenstein, David Silver, Oriol Vinyals, Andrew W . Senior, Koray Kavukcuoglu,730\nPushmeet Kohli, and Demis Hassabis. Highly accurate protein structure prediction with alphafold.731\nNature, 596:583 – 589, 2021.732\nRobert V erkuil, Ori Kabeli, Yilun Du, Basile I. M. Wicky , Lukas F . Milles, Justas Dauparas, David733\nBaker, Sergey Ovchinnikov , T om Sercu, and Alexander Rives. Language models generalize beyond734\nnatural proteins. bioRxiv, 2022. doi: 10.1101/2022.12.21.521521. URL https://www.biorxiv.735\norg/content/early/2022/12/22/2022.12.21.521521.736\nNYT . Meet gpt-3. it has learned to code (and blog and argue). 2020. URL https://www.nytimes.737\ncom/2020/11/24/science/artificial-intelligence-ai-gpt3.html .738\nNature. The big question. 2021. URL https://www.nature.com/articles/739\ns42256-021-00395-y .740\nEconomist. Huge “foundation models” are turbo-charging ai progress. 2022.741\nURL https://www.economist.com/interactive/briefing/2022/06/11/742\nhuge-foundation-models-are-turbo-charging-ai-progress .743\nCNN. Why you’re about to see chatgpt in more of your apps. 2023. URL https://www.cnn.com/744\n2023/03/01/tech/chatgpt-api/index.html.745\nKrystal Hu. Chatgpt sets record for fastest-growing user base - an-746\nalyst note. 2023. URL https://www.reuters.com/technology/747\nchatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/ .748\nGuido Appenzeller, Matt Bornstein, Martin Casado, and Y oko Li. Art isn’t dead, it’s just machine-749\ngenerated. 2022. URL https://a16z.com/2022/11/16/creativity-as-an-app/ .750\nJeremy Kaufmann, Max Abram, and Maggie Basta. Introducing: the scale751\ngenerative ai index. 2022. URL https://www.scalevp.com/blog/752\nintroducing-the-scale-generative-ai-index .753\nJason W ei, Yi T ay , Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Y o-754\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, T atsunori Hashimoto, Oriol755\nV inyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.756\nTransactions on Machine Learning Research , 2022. URL https://openreview.net/forum?757\nid=yzkSU5zdwD. Survey Certiﬁcation.758\nAbubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language759\nmodels. arXiv preprint arXiv:2101.05783 , 2021.760\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,761\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy . The Pile: An 800GB762\nDataset of Diverse Text for Language Modeling. arXiv preprint arXiv:2101.00027 , 2021. URL763\nhttps://arxiv.org/abs/2101.00027.764\n20\nVictor Sanh, Albert W ebson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,765\nAntoine Chafﬁn, Arnaud Stiegler, T even Le Scao, Arun Raja, Manan Dey , M Saiful Bari, Canwen766\nXu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, T aewoon Kim, Gunjan Chhablani,767\nNihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han W ang, Matteo Manica,768\nSheng Shen, Zheng Xin Y ong, Harshit Pandey , Rachel Bawden, Thomas W ang, Trishala Neeraj,769\nJos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry , Jason Alan Fries, Ryan T eehan,770\nStella Biderman, Leo Gao, T ali Bers, Thomas W olf, and Alexander M. Rush. Multitask prompted771\ntraining enables zero-shot task generalization. arXiv, 2021.772\nOpenAI. Introducing chatgpt and whisper apis. 2023a. URL https://openai.com/blog/773\nintroducing-chatgpt-and-whisper-apis .774\nTimnit Gebru, Jamie Morgenstern, Briana V ecchione, Jennifer W ortman V aughan, Hanna W allach,775\nHal Daumé Ill, and Kate Crawford. Datasheets for datasets. arXiv preprint arXiv:1803.09010 ,776\n2018.777\nEmily M Bender and Batya Friedman. Data statements for natural language processing: T oward miti-778\ngating system bias and enabling better science. Transactions of the Association for Computational779\nLinguistics (TACL) , 6:587–604, 2018.780\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy V asserman, Ben Hutchinson,781\nElena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting.782\nProceedings of the Conference on F airness, Accountability, and Transparency , 2018.783\nJon M. Kleinberg. Hubs, authorities, and communities. ACM Comput. Surv . , 31(4es):5–es, dec 1999.784\nISSN 0360-0300. doi: 10.1145/345966.345982. URL https://doi.org/10.1145/345966.785\n345982.786\nKen Hendricks, Michele Piccione, and Guofu T an. The Economics of Hubs: The Case of Monopoly.787\nThe Review of Economic Studies , 62(1):83–99, 01 1995. ISSN 0034-6527. doi: 10.2307/2297842.788\nURL https://doi.org/10.2307/2297842.789\nDaniel W Franks, Jason Noble, Peter Kaufmann, and Sigrid Stagl. Extremism propagation in social790\nnetworks with hubs. Adaptive Behavior , 16(4):264–274, 2008.791\nMartijn P V an den Heuvel and Olaf Sporns. Network hubs in the human brain. Trends in cognitive792\nsciences, 17(12):683–696, 2013.793\nDonald Martin Jr, Vinodkumar Prabhakaran, Jill Kuhlberg, Andrew Smart, and William S Isaac.794\nExtending the machine learning abstraction boundary: A complex systems approach to incorporate795\nsocietal context. arXiv preprint arXiv:2006.09663 , 2020.796\nRazvan Amironesei, Emily Denton, and Alex Hanna. Notes on problem formulation in machine797\nlearning. IEEE T echnology and Society Magazine , 40(3):80–83, 2021. doi: 10.1109/MTS.2021.798\n3104380.799\nAmandalynne Paullada, Inioluwa Deborah Raji, Emily M. Bender, Emily Denton, and Alex Hanna.800\nData and its (dis)contents: A survey of dataset development and use in machine learning research.801\nP atterns, 2(11):100336, 2021. ISSN 2666-3899. doi: https://doi.org/10.1016/j.patter.2021.100336.802\nURL https://www.sciencedirect.com/science/article/pii/S2666389921001847.803\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew , Ravit Dotan, and Michelle Bao.804\nThe values encoded in machine learning research. In 2022 ACM Conference on F airness, Account-805\nability, and Transparency , F AccT ’22, page 173–184, New Y ork, NY , USA, 2022. Association806\nfor Computing Machinery . ISBN 9781450393522. doi: 10.1145/3531146.3533083. URL807\nhttps://doi.org/10.1145/3531146.3533083.808\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-809\nresolution image synthesis with latent diffusion models, 2021.810\nChristoph Schuhmann, Romain Beaumont, Richard V encu, Cade W Gordon, Ross Wightman, Mehdi811\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell W ortsman, Patrick Schramowski,812\nSrivatsa R Kundurthy , Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia813\n21\nJitsev . LAION-5b: An open large-scale dataset for training next generation image-text models.814\nIn Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks815\nTrack, 2022. URL https://openreview.net/forum?id=M3Y74vmsMcY.816\nOpenAI. Gpt-4 technical report. 2023b.817\nOpher Lieber, Or Sharir, Barak Lenz, and Y oav Shoham. Jurassic-1: T echnical details and818\nevaluation. White P aper , AI21 Labs , 2021. URL https://uploads-ssl.webflow.com/819\n60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf.820\nPercy Liang, Rishi Bommasani, T ony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Y asunaga,821\nYian Zhang, Deepak Narayanan, Y uhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Y uan,822\nBobby Y an, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R’e, Diana823\nAcosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu824\nRen, Huaxiu Y ao, Jue W ang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Y uksekgonul,825\nMirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Henderson, Qian826\nHuang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, T atsunori Hashimoto,827\nThomas F . Icard, Tianyi Zhang, V ishrav Chaudhary , William W ang, Xuechen Li, Yifan Mai, Y uhui828\nZhang, and Y uta Koreeda. Holistic evaluation of language models. ArXiv, abs/2211.09110, 2022a.829\nJesse Dodge, Maarten Sap, Ana Marasovi ´c, William Agnew , Gabriel Ilharco, Dirk Groeneveld,830\nMargaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the831\ncolossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in832\nNatural Language Processing , pages 1286–1305, Online and Punta Cana, Dominican Republic,833\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.834\n98. URL https://aclanthology.org/2021.emnlp-main.98.835\nSuchin Gururangan, Dallas Card, Sarah K. Drier, Emily Kalah Gade, Leroy Z. W ang, Zeyu W ang,836\nLuke Zettlemoyer, and Noah A. Smith. Whose language counts as high quality? measuring837\nlanguage ideologies in text data selection. In Conference on Empirical Methods in Natural838\nLanguage Processing , 2022.839\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the840\ncarbon emissions of machine learning. arXiv preprint arXiv:1910.09700 , 2019.841\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep842\nlearning in NLP. arXiv preprint arXiv:1906.02243 , 2019.843\nPeter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky , and Joelle Pineau.844\nT owards the systematic reporting of the energy and carbon footprints of machine learning. Journal845\nof Machine Learning Research , 21(248):1–43, 2020.846\nAlexandra Sasha Luccioni and Alex Hernández-García. Counting carbon: A survey of factors847\ninﬂuencing the emissions of machine learning. ArXiv, abs/2302.08476, 2023.848\nNicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce,849\nH. Anderson, A. T erzis, Kurt Thomas, and Florian Tramèr. Poisoning web-scale training datasets850\nis practical. ArXiv, abs/2302.10149, 2023.851\nRishi Bommasani, Daniel Zhang, T ony Lee, and Percy Liang. Improving transparency in ai language852\nmodels: A holistic evaluation. F oundation Model Issue Brief Series , 2023. URL https://hai.853\nstanford.edu/foundation-model-issue-brief-series .854\nAleksander M ˛ adry . Advances in ai: Are we ready for a tech revolution? Cybersecurity, Information855\nT echnology, and Government Innovation Subcommittee , 2023. URL https://oversight.house.856\ngov/wp-content/uploads/2023/03/madry_written_statement100.pdf.857\nMingyi Zhao, Aron Laszka, and Jens Grossklags. Devising effective policies for bug-bounty platforms858\nand security vulnerability discovery . Journal of Information P olicy , 7:372–418, 2017.859\nRumman Chowdhury and Jutta Williams. Introducing twitter’s ﬁrst algorithmic bias bounty challenge.860\n2021. URL https://blog.twitter.com/engineering/en_us/topics/insights/2021/861\nalgorithmic-bias-bounty-challenge .862\n22\nPercy Liang, Rishi Bommasani, Kathleen A. Creel, and Rob Reich. The time is now to develop863\ncommunity norms for the release of foundation models, 2022b. URL https://crfm.stanford.864\nedu/2022/05/17/community-norms.html.865\nSasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. Who audits the auditors?866\nrecommendations from a ﬁeld scan of the algorithmic auditing ecosystem. In 2022 ACM Conference867\non F airness, Accountability, and Transparency , F AccT ’22, page 1571–1583, New Y ork, NY , USA,868\n2022. Association for Computing Machinery . ISBN 9781450393522. doi: 10.1145/3531146.869\n3533213. URL https://doi.org/10.1145/3531146.3533213.870\nJon Turow , Palak Goel, and Tim Porter. Foundation models: The future (still) isn’t happening fast871\nenough. Madrona, 2023. URL https://www.madrona.com/foundation-models/.872\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the873\ndangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM874\nConference on F airness, Accountability, and Transparency , pages 610–623, 2021.875\nLaura W eidinger, Jonathan Uesato, Maribeth Rauh, Conor Grifﬁn, Po-Sen Huang, John Mellor,876\nAmelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown,877\nZac Kenton, Will Hawkins, T om Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell,878\nWilliam Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, and Iason Gabriel. T axonomy of879\nrisks posed by language models. In 2022 ACM Conference on F airness, Accountability, and880\nTransparency, F AccT ’22, page 214–229, New Y ork, NY , USA, 2022. Association for Computing881\nMachinery . ISBN 9781450393522. doi: 10.1145/3531146.3533088. URL https://doi.org/882\n10.1145/3531146.3533088.883\nBen Buchanan, Andrew Lohn, Micah Musser, and Katerina Sedova. Truth, lies, and automation.884\n2021.885\nTimothy F . Bresnahan and M. Trajtenberg. General purpose technologies ‘engines of growth’?886\nJournal of Econometrics , 65(1):83–108, 1995. ISSN 0304-4076. doi: https://doi.org/10.1016/887\n0304-4076(94)01598-T. URL https://www.sciencedirect.com/science/article/pii/888\n030440769401598T.889\nErik Brynjolfsson, Daniel Rock, and Chad Syverson. The productivity j-curve: How intangibles890\ncomplement general purpose technologies. American Economic Journal: Macroeconomics , 13891\n(1):333–72, January 2021. doi: 10.1257/mac.20180386. URL https://www.aeaweb.org/892\narticles?id=10.1257/mac.20180386.893\nT yna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: An early look at894\nthe labor market impact potential of large language models, 2023.895\nDaron Acemoglu and David Autor. Skills, tasks and technologies: Implications for employment896\nand earnings. W orking Paper 16082, National Bureau of Economic Research, June 2010. URL897\nhttp://www.nber.org/papers/w16082.898\nDaron Acemoglu and Pascual Restrepo. The race between man and machine: Implications of899\ntechnology for growth, factor shares, and employment. American Economic Review , 108(6):1488–900\n1542, June 2018. doi: 10.1257/aer.20160696. URL https://www.aeaweb.org/articles?id=901\n10.1257/aer.20160696.902\nShakked Noy and Whitney Zhang. Experimental evidence on the productivity effects of generative903\nartiﬁcial intelligence. SSRN Electronic Journal , 2023.904\nEdward W . Felten, Manav Raj, and Robert C. Seamans. How will language modelers like chatgpt905\naffect occupations and industries? SSRN Electronic Journal , 2023.906\nAnton Korinek. Language models and cognitive automation for economic research. NBER W orking907\nPapers 30957, National Bureau of Economic Research, Inc, 2023. URL https://EconPapers.908\nrepec.org/RePEc:nbr:nberwo:30957.909\nSida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. The impact of ai on developer910\nproductivity: Evidence from github copilot. ArXiv, abs/2302.06590, 2023.911\n23\nErik Brynjolfsson and T om Mitchell. What can machine learning do? workforce implications. Science,912\n358(6370):1530–1534, 2017. doi: 10.1126/science.aap8062. URL https://www.science.org/913\ndoi/abs/10.1126/science.aap8062.914\nDaron Acemoglu, David Autor, Jonathon Hazell, and Pascual Restrepo. Ai and jobs: Evidence from915\nonline vacancies. W orking Paper 28257, National Bureau of Economic Research, December 2020.916\nURL http://www.nber.org/papers/w28257.917\nAjay K Agrawal, Joshua S Gans, and A vi Goldfarb. Ai adoption and system-wide change. W orking918\nPaper 28811, National Bureau of Economic Research, May 2021. URL http://www.nber.org/919\npapers/w28811.920\nD.H. Autor, D.A. Mindell, E. Reynolds, and R.M. Solow . The W ork of the Future: Building921\nBetter Jobs in an Age of Intelligent Machines . MIT Press, 2022. ISBN 9780262046367. URL922\nhttps://books.google.com/books?id=3tKMEAAAQBAJ.923\nDanaë Metaxa, Joon Sung Park, Ronald E. Robertson, Karrie Karahalios, Christo Wilson, Jeff924\nHancock, and Christian Sandvig. Auditing algorithms: Understanding algorithmic systems from the925\noutside in. F oundations and Trends® in Human–Computer Interaction , 14(4):272–344, 2021. ISSN926\n1551-3955. doi: 10.1561/1100000083. URL http://dx.doi.org/10.1561/1100000083.927\nInioluwa Deborah Raji and Joy Buolamwini. Actionable auditing: Investigating the impact of928\npublicly naming biased performance results of commercial ai products. In Proceedings of the929\n2019 AAAI/ACM Conference on AI, Ethics, and Society , AIES ’19, page 429–435, New Y ork, NY ,930\nUSA, 2019. Association for Computing Machinery . ISBN 9781450363242. doi: 10.1145/3306618.931\n3314244. URL https://doi.org/10.1145/3306618.3314244.932\nJon Kleinberg and Manish Raghavan. Algorithmic monoculture and social welfare. Proceedings of933\nthe National Academy of Sciences , 118(22):e2018340118, 2021. doi: 10.1073/pnas.2018340118.934\nURL https://www.pnas.org/doi/abs/10.1073/pnas.2018340118.935\nRishi Bommasani, Kathleen A. Creel, Ananya Kumar, Dan Jurafsky , and Percy Liang. Picking on the936\nsame person: Does algorithmic monoculture lead to outcome homogenization? In Advances in937\nNeural Information Processing Systems , 2022.938\nJia Deng, W ei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale939\nhierarchical image database. In Computer V ision and P attern Recognition (CVPR) , pages 248–255,940\n2009.941\nMarco Tulio Ribeiro, T ongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy:942\nBehavioral testing of NLP models with CheckList. In Association for Computational Linguistics943\n(ACL), pages 4902–4912, 2020.944\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese,945\nNathan McAleese, and Geoffrey Irving. Red teaming language models with language models.946\nArXiv, abs/2202.03286, 2022.947\nRishi Bommasani and Claire Cardie. Intrinsic evaluation of summarization datasets. In Proceedings948\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages949\n8075–8096, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/950\nv1/2020.emnlp-main.649. URL https://aclanthology.org/2020.emnlp-main.649.951\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong W ang, Hannaneh Hajishirzi, Noah A.952\nSmith, and Y ejin Choi. Dataset cartography: Mapping and diagnosing datasets with training953\ndynamics. In Empirical Methods in Natural Language Processing (EMNLP) , 2020.954\nKawin Ethayarajh, Y ejin Choi, and Swabha Swayamdipta. Understanding dataset difﬁculty with955\nV-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang956\nNiu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine957\nLearning, volume 162 of Proceedings of Machine Learning Research , pages 5988–6008. PMLR,958\n17–23 Jul 2022. URL https://proceedings.mlr.press/v162/ethayarajh22a.html.959\n24\nMargaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina960\nMcMillan-Major, Ezinwanne Ozoani, Nazneen Rajani, Tristan Thrush, Y acine Jernite, and Douwe961\nKiela. Measuring data. ArXiv, abs/2212.05129, 2022.962\nMina Lee, Percy Liang, and Qian Y ang. Coauthor: Designing a human-AI collaborative writing963\ndataset for exploring language model capabilities. In Conference on Human F actors in Computing964\nSystems (CHI) , 2022a.965\nMina Lee, Megha Srivastava, Amelia Hardy , John Thickstun, Esin Durmus, Ashwin Paranjape, Ines966\nGerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E. W ang, Minae Kwon, Joon Sung967\nPark, Hancheng Cao, T ony Lee, Rishi Bommasani, Michael Bernstein, and Percy Liang. Evaluating968\nhuman-language model interaction. 2022b. URL https://arxiv.org/abs/2212.09746.969\nAnamaria Crisan, Margaret Drouhard, Jesse Vig, and Nazneen Rajani. Interactive model cards:970\nA human-centered approach to model documentation. In 2022 ACM Conference on F airness,971\nAccountability, and Transparency , F AccT ’22, page 427–439, New Y ork, NY , USA, 2022. Associa-972\ntion for Computing Machinery . ISBN 9781450393522. doi: 10.1145/3531146.3533108. URL973\nhttps://doi.org/10.1145/3531146.3533108.974\nRavit Dotan and Smitha Milli. V alue-laden disciplinary shifts in machine learning. Proceedings of975\nthe 2020 Conference on F airness, Accountability, and Transparency , 2020.976\nKawin Ethayarajh and Dan Jurafsky . Utility is in the eye of the user: A critique of NLP leader-977\nboards. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language978\nProcessing (EMNLP) , pages 4846–4853, Online, November 2020. Association for Computational979\nLinguistics. doi: 10.18653/v1/2020.emnlp-main.393. URL https://aclanthology.org/2020.980\nemnlp-main.393.981\nMorgan Klaus Scheuerman, Alex Hanna, and Emily Denton. Do datasets have politics? disciplinary982\nvalues in computer vision dataset development. Proc. ACM Hum.-Comput. Interact. , 5(CSCW2),983\noct 2021. doi: 10.1145/3476058. URL https://doi.org/10.1145/3476058.984\nInioluwa Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada.985\nAI and the everything in the whole wide world benchmark. In Thirty-ﬁfth Conference on Neural986\nInformation Processing Systems Datasets and Benchmarks Track (Round 2) , 2021. URL https:987\n//openreview.net/forum?id=j6NxpQbREA1.988\nBernard Koch, Emily Denton, Alex Hanna, and Jacob Gates Foster. Reduced, reused and recy-989\ncled: The life of a dataset in machine learning research. In Thirty-ﬁfth Conference on Neu-990\nral Information Processing Systems Datasets and Benchmarks Track (Round 2) , 2021. URL991\nhttps://openreview.net/forum?id=zNQBIBKJRkd.992\nEmily Denton, Alex Hanna, Razvan Amironesei, Andrew Smart, and Hilary Nicole. On the993\ngenealogy of machine learning datasets: A critical history of imagenet. Big Data & So-994\nciety, 8(2):20539517211035955, 2021. doi: 10.1177/20539517211035955. URL https:995\n//doi.org/10.1177/20539517211035955.996\nRishi Bommasani. Evaluation for change. ArXiv, 2022. URL https://arxiv.org/abs/2212.997\n11670.998\nSeth Lazar. Governing the algorithmic city . T anner Lectures, 2023. URL https://write.as/999\nsethlazar/.1000\nAngelina W ang, Sayash Kapoor, Solon Barocas, and Arvind Narayanan. Against predictive optimiza-1001\ntion: On the legitimacy of decision-making algorithms that optimize predictive accuracy . Available1002\nat SSRN , 2022b.1003\nPeter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia1004\nHirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, et al. Artiﬁcial intelligence and1005\nlife in 2030: the one hundred year study on artiﬁcial intelligence. arXiv preprint arXiv:2211.06318 ,1006\n2022.1007\n25\nDaniel Zhang, Saurabh Mishra, Erik Brynjolfsson, John Etchemendy , Deep Ganguli, Barbara Grosz,1008\nT erah Lyons, James Manyika, Juan Carlos Niebles, Michael Sellitto, et al. The ai index 20211009\nannual report. arXiv preprint arXiv:2103.06312 , 2021.1010\nThomas W olf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony1011\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,1012\nPatrick von Platen, Clara Ma, Y acine Jernite, Julien Plu, Canwen Xu, T even Le Scao, Sylvain1013\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-1014\nart natural language processing. In Proceedings of the 2020 Conference on Empirical Meth-1015\nods in Natural Language Processing: System Demonstrations , pages 38–45, Online, October1016\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL1017\nhttps://aclanthology.org/2020.emnlp-demos.6.1018\nQuentin Lhoest, Albert Villanova del Moral, Y acine Jernite, Abhishek Thakur, Patrick von Platen,1019\nSuraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario1020\nvSavsko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, T even Le Scao, Victor Sanh,1021\nCanwen Xu, Nicolas Patry , Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement1022\nDelangue, Th’eo Matussiere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer,1023\nV ictor Mustar, François Lagunas, Alexander M. Rush, and Thomas W olf. Datasets: A community1024\nlibrary for natural language processing. ArXiv, abs/2109.02846, 2021.1025\nWhite House Executive Order. Executive order on improving the nation’s cybersecurity .1026\n2021. URL https://www.whitehouse.gov/briefing-room/presidential-actions/1027\n2021/05/12/executive-order-on-improving-the-nations-cybersecurity/ .1028\nAshwin Ramaswami. Securing open source software at the source. Plaintext1029\nGroup by Schmidt Futures , 2021. URL https://www.plaintextgroup.com/reports/1030\nsecuring-open-source-software-at-the-source .1031\nStewart Scott, Sara Ann Brackett, Trey Herr, and Maia Hamin. A voiding the suc-1032\ncess trap: T oward policy for open-source software as infrastructure. Atlantic Council ,1033\n2023. URL https://www.atlanticcouncil.org/in-depth-research-reports/report/1034\nopen-source-software-as-infrastructure/ .1035\nMichael Rowlinson. Organisations and institutions: perspectives in economics and sociology .1036\nSpringer, 1997.1037\nBarry R. W eingast and William J. Marshall. The industrial organization of congress; or, why1038\nlegislatures, like ﬁrms, are not organized as markets. Journal of P olitical Economy , 96(1):132–163,1039\n1988. doi: 10.1086/261528. URL https://doi.org/10.1086/261528.1040\nLiran Einav and Jonathan Levin. Empirical industrial organization: A progress report. Journal1041\nof Economic P erspectives , 24(2):145–62, June 2010. doi: 10.1257/jep.24.2.145. URL https:1042\n//www.aeaweb.org/articles?id=10.1257/jep.24.2.145.1043\nScott Frickel and Kelly Moore. The new political sociology of science: Institutions, networks, and1044\npower. Univ of Wisconsin Press, 2006.1045\nDavid Dequech. Institutions and norms in institutional economics and sociology . Journal of Economic1046\nIssues, 40(2):473–481, 2006.1047\nLaurent Fleury . Sociology of culture and cultural practices: The transformative power of institutions .1048\nLexington Books, 2014.1049\n26",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6604707837104797
    },
    {
      "name": "Ecosystem",
      "score": 0.5903765559196472
    },
    {
      "name": "Documentation",
      "score": 0.5781570076942444
    },
    {
      "name": "Ecosystem services",
      "score": 0.4972989857196808
    },
    {
      "name": "Metadata",
      "score": 0.47910693287849426
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.4710060656070709
    },
    {
      "name": "Asset (computer security)",
      "score": 0.4312903881072998
    },
    {
      "name": "Audit",
      "score": 0.42652663588523865
    },
    {
      "name": "Graph",
      "score": 0.4183316230773926
    },
    {
      "name": "World Wide Web",
      "score": 0.40902990102767944
    },
    {
      "name": "Data science",
      "score": 0.39387694001197815
    },
    {
      "name": "Business",
      "score": 0.25637391209602356
    },
    {
      "name": "Theoretical computer science",
      "score": 0.15556079149246216
    },
    {
      "name": "Ecology",
      "score": 0.15354123711585999
    },
    {
      "name": "Computer security",
      "score": 0.0951070487499237
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I87182695",
      "name": "Universidad del Noreste",
      "country": "MX"
    }
  ],
  "cited_by": 11
}