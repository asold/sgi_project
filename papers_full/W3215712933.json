{
  "title": "A Volumetric Transformer for Accurate 3D Tumor Segmentation",
  "url": "https://openalex.org/W3215712933",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3195525224",
      "name": "Himashi Peiris",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2073094858",
      "name": "Munawar Hayat",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2136257278",
      "name": "Zhaolin Chen",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2048392629",
      "name": "Gary  F. Egan",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A1991881270",
      "name": "Mehrtash Harandi",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2485159658",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3160284783",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3165098671",
    "https://openalex.org/W2915126261",
    "https://openalex.org/W3028279406",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3172681723",
    "https://openalex.org/W3134689216",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3094892677",
    "https://openalex.org/W3137561054",
    "https://openalex.org/W2038706709",
    "https://openalex.org/W3182372246",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W3015788359",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W3178989684",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3034571073",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2092985495",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3178812510",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3106758205",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3005498024",
    "https://openalex.org/W2907750714",
    "https://openalex.org/W2035473534",
    "https://openalex.org/W3198035652",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W2963682501",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W3105403262",
    "https://openalex.org/W1641498739",
    "https://openalex.org/W3102427165",
    "https://openalex.org/W3164956625",
    "https://openalex.org/W3037260676",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W3040726448",
    "https://openalex.org/W3176258108",
    "https://openalex.org/W3168491317",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3190965961",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2751069891",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W2980297198",
    "https://openalex.org/W3142085127"
  ],
  "abstract": "This paper presents a Transformer architecture for volumetric medical image segmentation. Designing a computationally efficient Transformer architecture for volumetric segmentation is a challenging task. It requires keeping a complex balance in encoding local and global spatial cues, and preserving information along all axes of the volumetric data. The proposed volumetric Transformer has a U-shaped encoder-decoder design that processes the input voxels in their entirety. Our encoder has two consecutive self-attention layers to simultaneously encode local and global cues, and our decoder has novel parallel shifted window based self and cross attention blocks to capture fine details for boundary refinement by subsuming Fourier position encoding. Our proposed design choices result in a computationally efficient architecture, which demonstrates promising results on Brain Tumor Segmentation (BraTS) 2021, and Medical Segmentation Decathlon (Pancreas and Liver) datasets for tumor segmentation. We further show that the representations learned by our model transfer better across-datasets and are robust against data corruptions. \\href{https://github.com/himashi92/VT-UNet}{Our code implementation is publicly available}.",
  "full_text": "A Robust Volumetric Transformer for Accurate\n3D Tumor Segmentation\nHimashi Peiris1, Munawar Hayat3, Zhaolin Chen1,2, Gary Egan2, Mehrtash\nHarandi1\n1 Department of Electrical and Computer Systems Engineering, Faculty of\nEngineering, Monash University, Melbourne, Australia.\n2 Monash Biomedical Imaging (MBI), Monash University, Melbourne, Australia.\n3 Department of Data Science & AI, Faculty of IT, Monash University, Melbourne,\nAustralia.\n{Edirisinghe.Peiris, Munawar.Hayat, Zhaolin.Chen, Gary.Egan,\nMehrtash.Harandi}@monash.edu\nAbstract. We propose a Transformer architecture for volumetric seg-\nmentation, a challenging task that requires keeping a complex balance in\nencoding local and global spatial cues, and preserving information along\nall axes of the volume. Encoder of the proposed design beneï¬ts from\nself-attention mechanism to simultaneously encode local and global cues,\nwhile the decoder employs a parallel self and cross attention formulation\nto capture ï¬ne details for boundary reï¬nement. Empirically, we show\nthat the proposed design choices result in a computationally eï¬ƒcient\nmodel, with competitive and promising results on the Medical Segmen-\ntation Decathlon (MSD) brain tumor segmentation (BraTS) Task. We\nfurther show that the representations learned by our model are robust\nagainst data corruptions. Our code implementation is publicly available.\nKeywords: Pure Volumetric TransformerÂ· Tumor Segmentation\n1 Introduction\nnnUNet (726 G)UNETR (184 G)TransBTS (333 G)nnFormer (111 G)VT-UNet S (52 G)VT-UNet B (101 G)\n62677277828792\n0 20 40 60 80 100 120Dice Similarity Score % Number of Parameters in Millions\nFig.1: The model size vs Dice\nSimilarity Coeï¬ƒcient (DSC)\nis shown in this plot. Circle\nsize indicates Computational\nComplexity by FLOPs. VT-UNet\nachieves highest DSC compared to\nSOTA methods while maintaining\na smaller model size and low\ncomputational complexity.\nInspired by the strong empirical results of\nthe transformer based models in computer vi-\nsion[8,5,15],theirpromisinggeneralizationandro-\nbustnesscharacteristics[21],andtheirï¬‚exibilityto\nmodel long range interactions, we propose a vol-\numetric transformer architecture for segmentation\nof 3D medical image modalities (e.g., MRI, CT),\ncalled VT-UNet. Earlier eï¬€orts to develop trans-\nformer based segmentation models for 3D medical\nscans have been shown to outperform state-of-the-\nart CNN counterparts [4]. However, these meth-\nods divide 3D volumes into 2D slices and process\n2D slices as inputs [4,6]. As such, considerable and\npotentially critical volumetric information, essential to encapsulating inter-slice\ndependencies, is lost. While some hybrid approaches (using both convolutional\narXiv:2111.13300v2  [eess.IV]  1 Jul 2022\n2 H. Peiris et al.\nblocks and Transformer layers) keep the 3D volumetric data intact [25,9,30], the\ndesign of purely transformer based architecture, capable of keeping intact the\nvolumetric data at input, is yet unexplored in the literature. Our work takes the\nï¬rst step in this direction, and proposes a model, which not only achieves better\nsegmentation performance, but also demonstrates better robustness against data\nartefacts. The Transformer models have highly dynamic and ï¬‚exible receptive\nï¬eld and are able to capture long-range interactions, yet designing a Transformer\nbased UNet architecture for volumetric segmentation remains a challenging task.\nThis is because:(1) Encapsulating voxel information and capturing the connec-\ntions between arbitrary positions in the volumetric sequence is not straightfor-\nward. Compared with Transformer based approaches for 2D image segmenta-\ntion [4], the data in each slice of the volume is connected to three views and\ndiscarding either of them can be detrimental.(2)Preserving spatial information\nin a volume is a daunting task. Even for 2D images, while breaking the image into\npatches and projecting patches into tokens as introduced in Vision Transformer\n(ViT), local structural cues can be lost, as shown in Tokens-to-token ViT [28].\nEï¬€ectively encoding the local cues while simultaneously capturing global interac-\ntions along multiple axes of a volume is therefore a challenging task.(3) Due to\nthe quadratic complexity of the self-attention, and large size of 3D volume tensor\ninputs, designing a Transformer based segmentation model, which is computa-\ntionally eï¬ƒcient, requires careful design considerations. Our proposed VT-UNet\nmodel eï¬€ectively tackles the above design challenges by proposing a number of\nmodules. In our UNet based architecture, we develop two types of Transformer\nblocks. First, our blocks in the encoder which directly work on the 3D volumes,\nin a hierarchical manner, to jointly capture the local and global information,\nsimilar in spirit to the Swin Transformer blocks [14]. Secondly, for the decoder,\nwe introduce parallel cross-attention and self-attention in the expansive path,\nwhich creates a bridge between queries from the decoder and keys & values from\nthe encoder. By this parallelization of the cross-attention and self-attention, we\naim to preserve the full global context during the decoding process, which is im-\nportant for the task of segmentation. Since VT-UNet is free from convolutions\nand combines attention outputs from two modules during the decoding, the or-\nder of the sequence is important to get accurate predictions. Inspired by [24],\napart from applying relative positional encoding while computing attention in\neach Transformer block, we augment the decoding process and inject the com-\nplementary information extracted from Fourier feature positions of the tokens\nin the sequence. In summary, our major contributions are,(1) We reformulate\nvolumetric tumor segmentation from a sequence-to-sequence perspective, and\npropose a UNet shaped Volumetric Transformer for multi-modal medical im-\nage segmentation. (2) We design an encoder block with two consecutive self\nattention layers to jointly capture local and global contextual cues. Further, we\ndesign a decoder block which enables parallel (shifted) window based self and\ncross attention. This parallelization uses one shared projection of thequeries\nand independently computes cross and self attention. To further enhance our\nfeatures in the decoding, we propose a convex combination approach along with\nVT-UNet 3\nFourier positional encoding.(3) Incorporating our proposed design choices, we\nsubstantially limit the model parameters while maintaining lower FLOPs com-\npared to existing approaches (see Fig. 1).(4) We conduct extensive evaluations\nand show that our design achieves state-of-the-art volumetric segmentation re-\nsults, alongwith enhanced robustness to data artefacts.\n2 Methodology\nLinear Embedding\nâœ•2 VT Encoder Block\n3D Patch Merging\nâœ•2 VT Encoder Block\n3D Patch Merging\nâœ•2 VT Encoder Block\n3D Patch Merging\nâœ•1VT Encoder Block\n3D Patch Expanding\n3D Patch Expanding\nâœ•2VT Decoder Block\n3D Patch Expanding\nVT Decoder Block\n3D Patch Expanding\nVT Decoder Block\nâœ•2 âœ•2\nClassifier\nğ·\n4 Ã—ğ»\n4 Ã—ğ‘Š\n4 Ã—ğ¶\nğ·\n4 Ã—ğ»\n8 Ã—ğ‘Š\n8 Ã—2ğ¶\nğ·\n4 Ã— ğ»\n16Ã— ğ‘Š\n16Ã—4ğ¶\nğ·\n4 Ã—ğ»\n4 Ã—ğ‘Š\n4 Ã—ğ¶\nğ·\n4 Ã—ğ»\n8 Ã—ğ‘Š\n8 Ã—2ğ¶\nğ·\n4 Ã— ğ»\n16Ã— ğ‘Š\n16Ã—4ğ¶\nğ·\n4 Ã— ğ»\n32Ã— ğ‘Š\n32Ã—8ğ¶\nğ·Ã—ğ» Ã—ğ‘Š Ã—ğ¶ ğ·Ã—ğ» Ã—ğ‘Š Ã—ğ¾\nK, V\nK, V\nK, V\nK, V\nK, V\nK, V\nSkip Connection\nSkip Connection\nSkip Connection\n3D Patch Partition\nBottleneck Layer\nMRI V olume Layer l\n Layer l+1\n(b)\nW-MSA\nMLP\nÆ¸ğ‘§ğ‘™\nğ’›ğ’âˆ’ğŸ\nğ’›ğ’\nSW-MSA\nMLP\nğ’›ğ’+ğŸ\nÆ¸ğ‘§ğ‘™+1\nK, V\nK, V\nVT-W-MSAVT-SW-MSA\nSW-MSA\nMLP\nÆ¸ğ‘§ğ‘ğ‘™+1\nğ‘§ğ‘ğ‘™+1\nSW-MSA\nMLP\nğ‘§ğ‘ ğ‘™+1\nÆ¸ğ‘§ğ‘ ğ‘™+1\nFusion\nğ’›ğ’+ğŸ\nW-MSA\nMLP\nÆ¸ğ‘§ğ‘ğ‘™\nğ‘§ğ‘ğ‘™\nW-MSA\nMLP\nğ‘§ğ‘ ğ‘™\nÆ¸ğ‘§ğ‘ ğ‘™\nFusion\nğ’›ğ’\nğ’›ğ’âˆ’ğŸ\n(a) (c)\nnnUNet TransBTS UNETR nnFormer\nViT Based Transformer Block(s)\nDeconvolution Block\nSkip with Deconvolution Blocks\nSwin Transformer Based Block\nVT-UNet\nKV\n3D Convolution\nDown sampling\nTransposed Conv/ Up sampling\nSkip Connection\nKV K, V Matrices Transferring\nPatch Merging Block\nPatch Expanding Block\nSwin Transformer Based Parallel \nCross & Self Attention Block\nDenotes Convolution based \nBlocks\nConcatenation Operation\nConvex \nCombination\nFPE\nğ’›ğ’„ğ’ ğ’›ğ’”ğ’\nğ’›ğ’\nLN\nMLP\n(d) (e)\nFig.2: (a) Illustrates VT-UNet Architecture. Here,k denotes the number of classes. (b) shows vi-\nsualization of Volumetric Shifted Windows. Consider an MRI volume of sizeD Ã—H Ã—W with\nD= H = W = 8for the sake of illustration. Further, let the window size for partitioning the volume\nbe P Ã—MÃ—M with P = M = 4. Here, layerl adopts the regular window partition in the ï¬rst step\nof Volumetric Transformer(VT) block which results in2 Ã—2 Ã—2 = 8windows. Inside layerl + 1,\nvolumetric windows are shifted by (P\n2 , M\n2 , M\n2 ) =(2, 2, 2) tokens. This results in3 Ã—3 Ã—3 = 27\nwindows. (c) shows VT Encoder-Decoder Structure. (d)Encoder-Decoder structural comparison with\nother SOTA methods. The proposed VT-UNet architecture has no convolution modules and is purely\nbased on Transformer blocks. (e) Illustrates the structure of the Fusion Module.\nWe denote vectors and matrices in bold lower-casex and bold upper-case\nX, respectively. LetX = {x1,x2,Â·Â·Â· ,xÏ„},xi âˆˆRC be a sequence representing\na signal of interest (e.g., an MRI volume). We call eachxi a token. We as-\nsume that tokens, in their original form, might not be optimal for deï¬ning the\nspan. Therefore, in Self-Attention (SA), we deï¬ne the span by learning a linear\nmapping from the input tokens. This we can show withRÏ„Ã—Cv âˆ‹V = XWV,\nwhere we stack tokensxis into the rows ofX (i.e., X = [x1|x2|Â·Â·Â· xÏ„]âŠ¤). Fol-\nlowing previous work by Hu et al. [10], we use a slight modiï¬cation of the\nself-attention [24](see [13]) in our task as follows:\nSA(Q,K,V) =SoftMax\n(\nQKâŠ¤/\nâˆš\nC+ B\n)\nV, (1)\n4 H. Peiris et al.\nwhereRÏ„Ã—Ï„ âˆ‹B istrainableandactsasarelativepositionalbiasacrosstokensin\nthe volume withV = XWV, K = XWK, andQ = XWQ. In practice, comput-\ning SA for multiple attention heads several times in parallel is called Multi-head\nSelf-Attention (MSA). Eq. (1) is the basic building block of our Volumetric\nTransformer Window based Multi-head Self-Attention (VT-W-MSA), and the\nVolumetric Transformer Shifted Window based Multi-head Self-Attention (VT-\nSW-MSA), discussed next.\nOverview of VT-UNet.Fig. 2 shows the conceptual diagram of the proposed\nvolumetric transformer network orVT-UNetfor short. The input to our model\nis a 3D volume of sizeDÃ—HÃ—WÃ—C. The output is aDÃ—HÃ—WÃ—Kdimensional\nvolume, representing the presence/absence of voxel-level class labels (K is the\nnumberofclasses).Below,wediscussarchitecturalformoftheVT-UNetmodules\nand explain the functionality and rationals behind our design in detail.\nThe VT Encoder. The VT encoder consists of 3D Patch Partitioning layer\ntogether with Linear Embedding layer, and 3D Patch merging layer followed by\ntwo successive VT encoder blocks.\n3D Patch Partitioning.Transformer-based models work with a sequence of\ntokens. The very ï¬rst block of VT-UNet accepts aDÃ—HÃ—WÃ—C dimensional\nmedicalvolume( e.g.,MRI)andcreatesasetoftokensbysplittingthe3Dvolume\ninto non-overlapping 3D patches (see Fig. 2 (b)). The size of partitioning kernel is\nPÃ—MÃ—M, resulting in describing the volume byÏ„ = âŒŠD/PâŒ‹Ã—âŒŠH/MâŒ‹Ã—âŒŠW/MâŒ‹\ntokens. The 3D patch partitioning is followed by a linear embedding to map each\ntoken with dimensionalityPÃ—MÃ—M to aC dimensional vector. Typical values\nfor M, P and C according to our experiments are 4, 4, and 72, respectively.\nVT Encoder Block.In ViT, tokens carry signiï¬cant spatial information due to\nthe way they are constructed. The importance of performing SA by windowing\nin ViT has been shown in several recent studies, most notably in Swin Trans-\nformer [14]. Following a similar principal in the design of Swin Transformers,\nalbeit for volumetric data, we propose 3D windowing operations in our VT\nEncoder Blocks (VT-Enc-Blks). In particular, we propose two types of win-\ndowing, namely regular window and shifted window, which we show byVT-\nW-MSA and VT-SW-MSA for simplicity, respectively. Fig. 2b provides the\ndesign speciï¬cs of VT-W-MSA and VT-SW-MSA, while Fig. 2 (b) illus-\ntrates the windowing operation. Both VT-W-MSA and VT-SW-MSA employ\nattention layers with windowing, followed by a 2-layer Multi Layer Perceptron\n(MLP) with Gaussian Error Linear Unit (GELU) non-linearity in between. A\nLayer Normalization (LN) is applied before every MSA and MLP, and a resid-\nual connection is applied after each module. The windowing enables us to inject\ninductive bias in modeling long range dependencies between tokens. In both\nVT-W-MSA and VT-SW-MSA, attention across tokens within a window helps\nrepresentation learning. In the VT-W-MSA, we split the volume evenly into\nsmaller non-overlapping windows as illustrated in Fig. 2 (b). Since tokens in\nadjacent windows cannot see each other with VT-W-MSA, we make use of a\nshifted window in VT-SW-MSA (see the right most panel Fig. 2 (b)) which\nbridges tokens in adjacent windows of VT-W-MSA. The windowing is inspired\nVT-UNet 5\nby the Swin Transformer [14] and can be understood as generalization to volu-\nmetric data. Note that the windowing operation in our work resembles [15] that\nextends the beneï¬ts of windowing beyond images to videos. Putting everything\ntogether, the VT-Enc-Blk realizes the following functionality:\nË†zl = VT-W-MSA\n(\nLN\n(\nzlâˆ’1))\n+ zlâˆ’1, Ë†zl+1 = VT-SW-MSA\n(\nLN\n(\nzl))\n+ zl,\nzl = MLP\n(\nLN\n(Ë†zl))\n+ Ë†zl, zl+1 = MLP\n(\nLN\n(Ë†zl+1))\n+ Ë†zl+1,\n(2)\nwhere Ë†zl and zl denote the output features of the VT-W-MSA module and the\nMLP module for blockl, respectively.\n3D Patch Merging.We make use of 3D patch merging blocks to generate fea-\nture hierarchies in the encoder of VT-UNet. Having such hierarchies is essential\nto generate ï¬ner details in the output for the dense prediction tasks [14,7].\nAfter every VT-Enc-Blk, we merge adjacent tokens along the spatial axes in\na non-overlapping manner to produce new tokens. In doing so, we ï¬rst concate-\nnate features of each group of2 Ã—2 neighboring tokens. The resulting vector is\nprojected via a linear mapping to a space where the channel dimensionality of\nthe tokens is doubled (see Fig. 2). The beneï¬t of patch merging is not limited\nto feature hierarchies. The computational complexity of SA is quadratic in the\nnumber of tokens [14,15]. As such, patch merging reduces the FLOPs count of\nthe VT-UNet by a factor of 16 after each VT-Enc-Blk. To give the reader a\nbetter idea and as we will discuss in Â§Sec. 4, the tiny VT-UNet model uses only\n6.7% FLOPs in comparison to its fully volumetric CNN counterpart [16] while\nachieving a similar performance (slightly better indeed)! Please note that the\npatch merging block is not used in the bottleneck stage.\nThe VT Decoder. After bottleneck layer which consists of a VT-Enc-Blk\ntogether with 3D Patch Expanding layer, the VT decoder starts with successive\nVT Decoder Blocks (VT-Dec-Blks), 3D patch expanding layers and a classiï¬er\nat the end to produce the ï¬nal predictions. There are some fundamental design\ndiï¬€erences between VT-Enc-Blk and VT-Dec-Blk which we will discuss next.\n3D Patch Expanding.This functionality is used to somehow revert the eï¬€ect\nof patch merging. In other words and in order to construct the output with the\nsame spatial-resolution as the input, we need to create new tokens in the decoder.\nForthesakeofdiscussion,considerthepatchexpandingafterthebottlenecklayer\n(see the middle part of Fig. 2). The input tokens to the patch expanding are of\ndimensionality8C.Inthepatchexpanding,weï¬rstincreasethedimensionalityof\nthe input tokens by a factor of two using a linear mapping. Following a reshaping,\nwe can obtain2 Ã—2 tokens with dimensionality4C from the resulting vector of\ndimensionality 2 Ã—8C. This, we will reshape along the spatial axes and hence\nfor D/4 Ã—H/32 Ã—W/32 Ã—8C, we createD/4 Ã—H/16 Ã—W/16 Ã—4C tokens.\nVT Decoder Block.The UNet [20] and its variants [18,31] make use of lat-\neral connections between the encoder and the decoder to produce ï¬ne-detailed\npredictions. This is because the spatial information is lost, at the expense of\nattaining higher levels of semantics, as the input passes through the encoder.\nThe lateral connections in the UNet makes it possible to have the best of both\n6 H. Peiris et al.\nworlds, spatial information from lower layers and semantic information from up-\nper layers (along the computational graph). Having this in mind, we propose\na hybrid form of SA at the decoder side (see Fig. 2b for an illustration). Each\nVT-Dec-Blk receives the generated tokens of its previous VT-Dec-Blk along with\nthe key (KE) and value (VE) tokens from the VT-Enc-Blk sitting at the same\nstage of VT-UNet, see Fig. 2a. Recall that a VT-Enc-Blk has two SA blocks with\nregular and shifted windowing operations. VT-Dec-Blk enjoys similar windowing\noperations but makes use of four SA blocks grouped into SA module and Cross\nAttention(CA) module. The functionality can be described as:\nSAr = SA(QD,KD,VD), CAl = SA(QD,KE,VE). (3)\nHere, r and l, denote right and left branches of the decoder module. The right\nbranch of the SA acts on tokens generated by the previous VT-Dec-Blk accord-\ning to Eq. (3). We emphasize on the ï¬‚ow of information from the decoder by\nthe subscript D therein. The left branch of the CA, however, uses the queries\ngenerated by the decoder along with the keys and values obtained from the VT-\nEnc-Blk at the same level in the computation graph. The idea here is to use the\nbasis spanned by the encoder (which is identiï¬ed by values) along with keys to\nbeneï¬t from spatial information harvested by the encoder. These blocks, also use\nthe regular and shifted windowing to inject more inductive bias into the model.\nNote that the values and keys from the SA with the same windowing operation\nshould be combined, hence the criss-cross connection form in Fig. 2 (c).\nRemark 1. One may ask why values and keys are considered from the encoder.\nWeindeedstudiedotherpossibilitiessuchasemployingqueriesandkeysfromthe\nencoder while generating values by the decoder. Empirically, the form described\nin Eq. (3) is observed to deliver better and more robust outcomes and hence our\nchoice in VT-UNet4.\nFusion Module. As illustrated in Fig. 2 (e), tokens generated from the CA\nmodule and MSA module are combined together and fed to the next VT-Dec-\nBlk, zl is calculated using by a linear function as:\nzl = Î± Ë†zl\nc + (1âˆ’Î±) Ë†zl\ns + F(Ë†zl\ns), (4)\nwhere F(Â·) denotes Fourier Feature Positional Encoding (FPE) andÎ± controls\nthe contribution from each CA and MSA module. Aiming for simplicity, in fusing\ntokensgeneratedbytheCAandMSA,weusealinearcombinationwith Î±= 0.55.\nClassiï¬er Layer.After the ï¬nal 3D patch expanding layer in the decoder, we\nintroduce a classiï¬er layer which includes a 3D convolutional layer to map deep\nC dimensional features toK segmentation classes.\n4 We empirically observed that employing keys and values from the encoder in CA yields faster\nconvergence of VT-UNet. This, we conjecture, is due to having extra connections from the decoder\nto encoder during the back-propagation which might facilitate gradient ï¬‚ow.\n5 Breaking the Symmetry: This results in a symmetry, meaning that swappingË†zl\nc and Ë†zl\ns does\nnot change the output. To break this symmetry and also better encapsulate object-aware rep-\nresentations that are critical for anatomical pixel-wise segmentation, we supplement the tokens\ngenerated from MSA by a the 3D FPE. The 3D FPE employs sine and cosine functions with\ndiï¬€erent frequencies [24] to yield a unique encoding scheme for each token. The main idea is to\nuse a sine/cosine function with a high frequency and modulate it across the dimensionality of the\ntokens while changing the frequency according to the location of the token within the 3D volume.\nVT-UNet 7\nA note on computational complexity.The computational complexity of the SA\ndescribed in Eq. (1) is dictated by computations required for obtainingQ,K,V,\ncomputing QKâŠ¤ and obtaining the resulting tokens by applying the output of\nthe Softmax (which is aÏ„Ã—Ï„ matrix) toV. This adds up toO\n(\n3Ï„C2 + 2Ï„2C\n)\n,\nwhere C and Ï„ are the dimensionality and the number of tokens, respectively.\nWindowing will reduce the computational load of the SA according toO\n(\n3Ï„C2 +\n2Ï„ÎºC\n)\nwhere we have assumed that tokens are grouped intoÎºwindows and SA\nis applied within each window. In our problem, where tokens are generated from\nvolumetric data, Ï„ â‰«Îº and hence windowing not only helps in having better\ndiscriminatory power, but also it helps in reducing the computational load6.\n3 Related Work\nVision Transformers have shown superior empirical results for diï¬€erent com-\nputer vision tasks [23,2,15], with promising characteristics. For example and as\ncompared with the CNNs, they are less biased towards texture [17], and show\nbetter generalization and robustness [21,17]. Transformers have also been re-\ncently investigated for image segmentation [29,6,4]. TransUNet [6] is the ï¬rst\nTransformer based approach for medical image segmentation. It adapts a UNet\nstructure, and replaces the bottleneck layer with ViT [8] where patch embedding\nis applied on a feature map generated from CNN encoder (where input is a 2D\nslice of 3D volume). Unlike these hybrid approaches (using both convolutions and\nself-attention), Caoet al. [4] proposed Swin-UNet, a purely transformer based\nnetwork for medical image segmentation. It inherits swin-transformer blocks [14]\nand shows better segmentation results over TransUNet [6]. The 3D version of\nTransUnet [6], called TransBTS [25] has a CNN encoder-decoder design and a\nTransformer as the bottleneck layer. Zhouet al. [30] proposed nnFormer with 3D\nSwin Transformer based blocks as encoder and decoder with interleaved stem of\nconvolutions. A model which employs a transformer as the encoder and directly\nconnects intermediate encoder outputs to the the decoder via skip connections is\nproposed in [9]. The encoder-decoder structural comparison of SOTA methods\nare shown in Fig. 2 (d). The aforementioned transformer based approaches for\n3D medical image segmentation have shown their promises, by achieving better\nperformances compared with their CNN counterparts. Our proposed model, on\nthe other hand, processes the volumetric data in its entirety, thus fully encoding\nthe interactions between slices. Moreover, our proposed model is built purely\nbased on Transformers and introduces lateral connections to perform CA along-\nwith SA in the encoder-decoder design. These design elements contributed in\nachieving better segmentation performance, along-with enhanced robustness.\n6 For the sake of simplicity and explaining the key message, we have made several assumptions in\nour derivation. First, we have assumedCk = Cv = C. We also did not include the FLOPs needed\nto compute the softmax. Also, in practice, one uses a multi-head SA, where the computation is\nbreak down across several parallel head working on lower dimensional spaces (e.g., on forV, we\nuse C/h dimensional spaces wherehis the number of heads). This will reduce the computational\nload accordingly. That said, the general conclusion provided here is valid.\n8 H. Peiris et al.\n4 Experiments\nMethod Average WT ET TCHD95â†“DSCâ†‘HD95â†“DSCâ†‘HD95â†“DSCâ†‘HD95â†“DSCâ†‘\nUNet [20] 10.19 66.49.21 76.611.12 56.110.24 66.5AttUNet [18]9.97 66.49.00 76.710.45 54.310.46 68.3nnUNet [11]4.60 81.93.6491.94.06 80.974.91 85.35SETR NUP [29]13.78 63.714.419 69.711.72 54.415.19 66.9SETR PUP [29]14.01 63.815.245 69.611.76 54.915.023 67.0SETR MLA [29]13.49 63.915.503 69.810.24 55.414.72 66.5TransUNet [6]12.98 64.414.03 70.610.42 54.214.5 68.4TransBTS [25]9.65 69.610.03 77.99.97 57.48.95 73.5CoTr [27] 9.70 68.39.20 74.69.45 55.710.45 74.8UNETR [9]8.82 71.18.27 78.99.35 58.58.85 76.1nnFormer [30]4.05 86.43.80 91.33.87 81.84.4986.0VT-UNet-S3.8485.94.01 90.82.9181.84.60 85.0VT-UNet-B3.43 87.13.51 91.92.68 82.24.10 87.2\nTable 1: Segmentation Results on MSD BraTS Dataset.\nArtefact Avg. HD95â†“Avg. DSCâ†‘\nnnFormerClean 4.05 86.4Motion 4.81 84.3Ghost 4.30 84.5Spike 4.63 84.9\nVT-UNetClean 3.43 87.1Motion 3.87 85.8Ghost 3.69 86.0Spike 3.50 86.6\nTable 2: Robustness Analysis.\nVT-UNet-B Avg. HD95â†“Avg. DSCâ†‘\nw/o FPE 85.35 4.33w/o FPE & CA 83.58 6.18\nTable 3: Ablation Study.\nImplementation Details.We use 484 MRI scans from MSD BraTS task [1].\nFollowing [9,30], we divide 484 scans into 80%, 15% and 5% for training, vali-\ndation and testing sets, respectively. We use PyTorch [19], with a single Nvidia\nA40 GPU. The weights of Swin-T [14] pre-trained on ImageNet-22K are used to\ninitialize the model. For training, we employ AdamW optimizer with a learn-\ning rate of 1eâˆ’4 for 1000 epochs and a batch size of 4. We used rotating,\nadding noise, blurring and adding gamma as data augmentation techniques.\nGT VT-UNet nnUNet UNETR nnFormer\nTable 4: Qualitative Segmentation Results.Row-\n1: Yellow, Green and Red represent Peritumoral\nEdema (ED), Enhancing Tumor (ET) and Non En-\nhancing Tumor (NET)/ Necrotic Tumor (NCR),\nrespectively. Row-2: volumetric tumor prediction.\nRow-3: segmentation boundaries.\nExperimental Results. Table 1\ncomparesVT-UNetwithrecenttrans-\nformer based approaches and SOTA\nCNN based methods. We introduce\nvariants of the VT- UNet, by chang-\ning the number of embedded dimen-\nsions used for model training. Our\nvariants are: (a) Small VT-UNet-S\nwith C = 48 (b) Base VT-UNet-B\nwith C = 72. We use Dice Srensen\ncoeï¬ƒcient (DSC) and Hausdorï¬€ Dis-\ntance (HD) as evaluation metrics,\nand separately compute them for\nthree classes: (1) Enhancing Tumor\n(ET), (2) the Tumor Core (TC) (ad-\ndition of ET, NET and NCR), and\n(3) the Whole Tumor (WT) (addition of ED to TC), following similar evalu-\nation strategy as in [11,9,30]. Our quantitative results in Table 1 suggest that\nVT-UNet achieves best overall performance in DSC and HD. Table 4 shows\nqualitative segmentation results on unseen patient data. We can observe that\nour model can accurately segment the structure and delineates boundaries of\ntumor. We believe that, capturing long-range dependencies across adjacent slices\nplays a vital role in our modelâ€™s performance. Our empirical results in Table 3\nreveal the importance of introducing Parallel CA and SA together with FPE in\nVT-UNet 9\nVT-Dec-Blks along with convex combination. We can notice that all of these\ncomponents contribute towards modelâ€™s performance.\nRobustness Analysis. Factors such as patientâ€™s movement and acquisition\nconditions can introduce noise to MRI. Here, we investigate the robustness of\nVT-UNet, by synthetically introducing artefacts to MR images at inference time.\nThese include (1) Motion artefacts [22].(2) Ghosting artefacts [3].(3) Spike\nartefacts (Herringbone artefact) [12]. Table 2 compares the robustness of our\nmethod with nnFormer [30]. The results suggest that VT-UNet performs more\nreliably in the presence of these nuisances. Our ï¬ndings are consistent with\nexisting works on RGB images, where Transformer based models have shown\nbetter robustness against occlusions [17], natural and adversarial perturbations\n[21,17], owing to their highly dynamic and ï¬‚exible receptive ï¬eld.\n5 Conclusion\nThis paper presents a volumetric transformer network for medical image segmen-\ntation, that is computationally eï¬ƒcient to handle large-sized 3D volumes, and\nlearns representations that are robust against artefacts. Our results show that\nthe proposed model achieves consistent improvements over existing state-of-the-\nart methods in volumetric segmentation. We believe our work can assist better\nclinical diagnosis and treatment planning.\nReferences\n1. Antonelli, M., Reinke, A., Bakas, S., Farahani, K., Landman, B.A., Litjens, G.,\nMenze, B., Ronneberger, O., Summers, R.M., van Ginneken, B., et al.: The medical\nsegmentation decathlon. arXiv preprint arXiv:2106.05735 (2021) 8\n2. Arnab, A., Dehghani, M., Heigold, G., Sun, C., LuÄiÄ‡, M., Schmid, C.: Vivit: A\nvideo vision transformer. arXiv preprint arXiv:2103.15691 (2021) 7\n3. Axel, L., Summers, R., Kressel, H., Charles, C.: Respiratory eï¬€ects in two-\ndimensional fourier transform mr imaging. Radiology160(3), 795â€“801 (1986) 9\n4. Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M.: Swin-\nunet: Unet-like pure transformer for medical image segmentation. arXiv preprint\narXiv:2105.05537 (2021) 1, 2, 7\n5. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European Conference on Computer\nVision. pp. 213â€“229. Springer (2020) 1\n6. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.:Transunet:Transformersmakestrongencodersformedicalimagesegmentation.\narXiv preprint arXiv:2102.04306 (2021) 1, 7, 8\n7. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: Deeplab: Se-\nmantic image segmentation with deep convolutional nets, atrous convolution, and\nfully connected crfs. IEEE transactions on pattern analysis and machine intelli-\ngence 40(4), 834â€“848 (2017) 5\n8. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020) 1, 7\n10 H. Peiris et al.\n9. Hatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A., Landman, B.,\nRoth, H.R., Xu, D.: Unetr: Transformers for 3d medical image segmentation. In:\nProceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision. pp. 574â€“584 (2022) 2, 7, 8\n10. Hu, H., Zhang, Z., Xie, Z., Lin, S.: Local relation networks for image recognition.\nIn: Proceedings of the IEEE/CVF International Conference on Computer Vision.\npp. 3464â€“3473 (2019) 3\n11. Isensee, F., Maier-Hein, K.H.: nnu-net for brain tumor segmentation. In: Brain-\nlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 6th In-\nternational Workshop, BrainLes 2020, Held in Conjunction with MICCAI 2020,\nLima, Peru, October 4, 2020, Revised Selected Papers, Part II. vol. 12658, p. 118.\nSpringer Nature (2021) 8\n12. Jin, K.H., Um, J.Y., Lee, D., Lee, J., Park, S.H., Ye, J.C.: Mri artifact correction\nusing sparse+ low-rank decomposition of annihilating ï¬lter-based hankel matrix.\nMagnetic resonance in medicine78(1), 327â€“340 (2017) 9\n13. Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers\nin vision: A survey. arXiv preprint arXiv:2101.01169 (2021) 3\n14. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030 (2021) 2, 4, 5, 7, 8\n15. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin trans-\nformer. arXiv preprint arXiv:2106.13230 (2021) 1, 5, 7\n16. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 2016 fourth international confer-\nence on 3D vision (3DV). pp. 565â€“571. IEEE (2016) 5\n17. Naseer, M., Ranasinghe, K., Khan, S., Hayat, M., Khan, F.S., Yang, M.H.: Intrigu-\ning properties of vision transformers. arXiv preprint arXiv:2105.10497 (2021) 7,\n9\n18. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning\nwhere to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018) 5, 8\n19. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\nDesmaison, A., Antiga, L., Lerer, A.: Automatic diï¬€erentiation in pytorch (2017)\n8\n20. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: MICCAI. pp. 234â€“241. Springer (2015) 5, 8\n21. Shao, R., Shi, Z., Yi, J., Chen, P.Y., Hsieh, C.J.: On the adversarial robustness of\nvisual transformers. arXiv preprint arXiv:2103.15670 (2021) 1, 7, 9\n22. Shaw, R., Sudre, C., Ourselin, S., Cardoso, M.J.: Mri k-space motion artefact\naugmentation: model robustness and task-speciï¬c uncertainty. In: International\nConference on Medical Imaging with Deep Learningâ€“Full Paper Track (2018) 9\n23. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., JÃ©gou, H.: Training\ndata-eï¬ƒcient image transformers & distillation through attention. In: International\nConference on Machine Learning. pp. 10347â€“10357. PMLR (2021) 7\n24. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,\nÅ., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998â€“6008 (2017) 2, 3, 6, 12\n25. Wang, W., Chen, C., Ding, M., Yu, H., Zha, S., Li, J.: Transbts: Multimodal\nbrain tumor segmentation using transformer. In: International Conference on Med-\nical Image Computing and Computer-Assisted Intervention. pp. 109â€“119. Springer\n(2021) 2, 7, 8\nVT-UNet 11\n26. Wang, Z., Liu, J.C.: Translating math formula images to latex sequences using deep\nneural networks with sequence-level training. International Journal on Document\nAnalysis and Recognition (IJDAR)24(1), 63â€“75 (2021) 12\n27. Xie, Y., Zhang, J., Shen, C., Xia, Y.: Cotr: Eï¬ƒciently bridging cnn and transformer\nfor 3d medical image segmentation. arXiv preprint arXiv:2103.03024 (2021) 8\n28. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z., Tay, F.E., Feng, J., Yan,\nS.: Tokens-to-token vit: Training vision transformers from scratch on imagenet.\narXiv preprint arXiv:2101.11986 (2021) 2\n29. Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., Fu, Y., Feng, J., Xiang, T.,\nTorr, P.H., et al.: Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 6881â€“6890 (2021) 7, 8\n30. Zhou, H.Y., Guo, J., Zhang, Y., Yu, L., Wang, L., Yu, Y.: nnformer: Interleaved\ntransformer for volumetric segmentation. arXiv preprint arXiv:2109.03201 (2021)\n2, 7, 8, 9\n31. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net\narchitecture for medical image segmentation. In: Deep Learning in Medical Im-\nage Analysis and Multimodal Learning for Clinical Decision Support, pp. 3â€“11.\nSpringer (2018) 5\n12 H. Peiris et al.\n6 Supplementary Material\nPreliminaries Here we provide a brief description about the SA. In SA, we are\ninterested in generating a new sequence of tokens,{z1,z2,Â·Â·Â· ,zÏ„}, from X to\nbetter represent our signal according to the objective of learning. In doing so,\nwe can assume thatxis span a subset ofRC and deï¬ne zi as a point in that\nspan, i.e. zi = âˆ‘\njaijxj, where aijs are combination values deï¬ned byxi and\nxj. A possible choice for aij is based on the similarity of xi and xj, which\nalgebraically is proportional toâŸ¨xi,xjâŸ©. Such a choice enables us to deï¬ne the\ntoken zi by attending to important parts of the input sequence according to the\nobjective in hand, hence the name attention. We can take a further step and\nput a constraint on our design by enforcing the generated tokens to lie inside\nthe convex hull deï¬ned byxis. In that case, we will havezi = âˆ‘\njaijxj,aij â‰¥\n0,âˆ‘\njaij = 1. The convex hull formulation endows nice properties, one being\nthat the resulting tokens cannot grow boundlessly, given the fact that the input\nis assumed to be a natural signal. The SA operation is built upon the above\nidea with some modiï¬cations. Firstly, we assume that tokens, in their original\nform, might not be optimal for deï¬ning the span. Therefore, in SA we deï¬ne\nthe span by learning a linear mapping from the input tokens. This we can show\nwith RÏ„Ã—Cv âˆ‹V = XWV, where we stack tokensxis into the rows ofX (i.e.,\nX = [x1|x2|Â·Â·Â· xÏ„]âŠ¤). Then we turn our attention toaij and deï¬ne it by learning\ntwo linear mappings, following a similar argument. In particular, ï¬rst we deï¬ne\na set of keys fromxi as RÏ„Ã—Ck âˆ‹K = XWK. To generateaij, we measure the\nsimilarity of a transformed version ofxi, which we call the queryqi = WâŠ¤\nQxi,\nwith respect to the keys kj that are represented by the rows ofK. That is,\naij âˆâŸ¨qi,kjâŸ©. Put everything into a matrix form and opt for a softmax function\nto achieve the constraintsaij â‰¥0 and âˆ‘\njaij = 1, we arrive at Eq (1).\nBreaking Symmetry Cont. The linear patch-projection ï¬‚attens the features,\nthereby failing to fully encapsulate object-aware representations (e.g., spatial\ninformation) that are critical for anatomical pixel-wise segmentation. As shown\nin Fig. 2(e), combining two sets of tokens may results in loss of ï¬‚uency. There-\nfore, in order to preserve features among continuous slices, we supplement the 3D\nFourier Feature Positional Encoding (FPE) for the tokens generated from MSA\nmodule by adapting sine and cosine functions with diï¬€erent frequencies [24],\nwhich provides unique encoding for each input token. Following work by Wang\net al. [26], we used the extended version of 2D positional encoding for 3D. There-\nfore, we call this as a 3D FPE or in other words a 3D positional encoding with\na sinusoidal input mapping for VT-Dec-Blks. After applying 3D FPE to tokens,\nwe pass it through a LN and MLP layer. Our empirical evaluations conï¬rm that\nadding a Fourier feature positional bias can improve the poor conditioning of\nthe feature representation.\nLoss Function. To train VT-UNet, we jointly minimize the Dice Loss together\nwith Cross Entropy loss (computed in a voxel-wise manner).",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7863974571228027
    },
    {
      "name": "Computer science",
      "score": 0.7766207456588745
    },
    {
      "name": "Encoder",
      "score": 0.7294008135795593
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6081182360649109
    },
    {
      "name": "Transformer",
      "score": 0.5970268249511719
    },
    {
      "name": "Voxel",
      "score": 0.5846239328384399
    },
    {
      "name": "Computer vision",
      "score": 0.518379271030426
    },
    {
      "name": "Image segmentation",
      "score": 0.4565134048461914
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3733033835887909
    },
    {
      "name": "Engineering",
      "score": 0.07844030857086182
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    }
  ],
  "cited_by": 25
}