{
  "title": "HAT: Hierarchical Aggregation Transformers for Person Re-identification",
  "url": "https://openalex.org/W3178838461",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2385120279",
      "name": "Zhang Guo-wen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1900172642",
      "name": "Zhang Ping-ping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751153817",
      "name": "Qi, Jinqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227143521",
      "name": "Lu, Huchuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2979938149",
    "https://openalex.org/W2984145721",
    "https://openalex.org/W3156464220",
    "https://openalex.org/W3141972123",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3100506510",
    "https://openalex.org/W2966094134",
    "https://openalex.org/W2896888563",
    "https://openalex.org/W3043255456",
    "https://openalex.org/W2295107390",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3000133216",
    "https://openalex.org/W3045817950",
    "https://openalex.org/W2979931389",
    "https://openalex.org/W2585635281",
    "https://openalex.org/W2584637367",
    "https://openalex.org/W3139587317",
    "https://openalex.org/W2770739811",
    "https://openalex.org/W2964140013",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2796364723",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3150226983",
    "https://openalex.org/W2967515867",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963049565",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2168356304",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3035186652",
    "https://openalex.org/W3143016713",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3034580371",
    "https://openalex.org/W2981420411",
    "https://openalex.org/W2997738728",
    "https://openalex.org/W2963047834",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2954765307",
    "https://openalex.org/W2204750386",
    "https://openalex.org/W2963842104",
    "https://openalex.org/W2963709182",
    "https://openalex.org/W3104599541",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2598634450",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W1982925187",
    "https://openalex.org/W2795013471",
    "https://openalex.org/W3034727830",
    "https://openalex.org/W2963323244",
    "https://openalex.org/W2798775284",
    "https://openalex.org/W2952610664",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963438548",
    "https://openalex.org/W2984040540",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W2120419212"
  ],
  "abstract": "Recently, with the advance of deep Convolutional Neural Networks (CNNs), person Re-Identification (Re-ID) has witnessed great success in various applications. However, with limited receptive fields of CNNs, it is still challenging to extract discriminative representations in a global view for persons under non-overlapped cameras. Meanwhile, Transformers demonstrate strong abilities of modeling long-range dependencies for spatial and sequential data. In this work, we take advantages of both CNNs and Transformers, and propose a novel learning framework named Hierarchical Aggregation Transformer (HAT) for image-based person Re-ID with high performance. To achieve this goal, we first propose a Deeply Supervised Aggregation (DSA) to recurrently aggregate hierarchical features from CNN backbones. With multi-granularity supervisions, the DSA can enhance multi-scale features for person retrieval, which is very different from previous methods. Then, we introduce a Transformer-based Feature Calibration (TFC) to integrate low-level detail information as the global prior for high-level semantic information. The proposed TFC is inserted to each level of hierarchical features, resulting in great performance improvements. To our best knowledge, this work is the first to take advantages of both CNNs and Transformers for image-based person Re-ID. Comprehensive experiments on four large-scale Re-ID benchmarks demonstrate that our method shows better results than several state-of-the-art methods. The code is released at https://github.com/AI-Zhpp/HAT.",
  "full_text": "HAT: Hierarchical Aggregation Transformers for Person\nRe-identification\nGuowen Zhang, Pingping Zhang, Jinqing Qi, Huchuan Luâˆ—\nDalian University of Technology\nDalian, Liaoning, China\nguowenzhang@mail.dlut.edu.cn;zhpp,jinqing,lhchuan@dlut.edu.cn\nABSTRACT\nRecently, with the advance of deep Convolutional Neural Networks\n(CNNs), person Re-Identification (Re-ID) has witnessed great suc-\ncess in various applications. However, with limited receptive fields\nof CNNs, it is still challenging to extract discriminative representa-\ntions in a global view for persons under non-overlapped cameras.\nMeanwhile, Transformers demonstrate strong abilities of modeling\nlong-range dependencies for spatial and sequential data. In this work,\nwe take advantages of both CNNs and Transformers, and propose a\nnovel learning framework named Hierarchical Aggregation Trans-\nformer (HAT) for image-based person Re-ID with high performance.\nTo achieve this goal, we first propose a Deeply Supervised Aggre-\ngation (DSA) to recurrently aggregate hierarchical features from\nCNN backbones. With multi-granularity supervisions, the DSA can\nenhance multi-scale features for person retrieval, which is very differ-\nent from previous methods. Then, we introduce a Transformer-based\nFeature Calibration (TFC) to integrate low-level detail information\nas the global prior for high-level semantic information. The pro-\nposed TFC is inserted to each level of hierarchical features, resulting\nin great performance improvements. To our best knowledge, this\nwork is the first to take advantages of both CNNs and Transform-\ners for image-based person Re-ID. Comprehensive experiments on\nfour large-scale Re-ID benchmarks demonstrate that our method\nshows better results than several state-of-the-art methods. The code\nis released at https://github.com/AI-Zhpp/HAT.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Machine learning; Image repre-\nsentations.\nKEYWORDS\nPerson Re-identification, Transformer, Deep Feature Aggregation\n1 INTRODUCTION\nPerson Re-identification (Re-ID) aims to retrieve the same person\nunder different cameras, places and times. As an important com-\nponent of intelligent surveillance and autonomous driving, person\nRe-ID has drawn a surge of interests. The challenge of person Re-\nID lies in extracting rich, discriminative and robust features from\nperson images which are under large variations, such as occlusion,\nillumination, pose and background clutter.\nCurrently, with the progress of deep learning, Convolutional Neu-\nral Networks (CNNs) based person Re-ID methods have achieved\ngreat success. The rise of CNNs as the backbone of many visual\ntasks such as image classification, object detection and segmentation,\nhas encouraged researchers to explore effective structures for feature\naggregation, as shown in Fig. 1. The trend is to design effective\nInteraction in TransformersAggregation of Multi-scale Features\nFigure 1: The insight of our Hierarchical Aggregation Trans-\nformer. It integrates low-level information as the global prior\nfor enriching high-level semantic information.\nstructures with more non-linearity, greater capacity and larger recep-\ntive field. As shown in Fig. 2(b), the utilization of skip connection\naggregation is one of the most common methods to integrate the\nmulti-level features at once. A series of works [ 24] utilize a top-\ndown feature pyramid architecture with lateral connections to build\nhigh-level semantic feature maps as shown in Fig. 2 (c).\nFurthermore, for image-based person Re-ID, previous works [4,\n26, 47, 59] explore the effectiveness of hierarchical features of CNNs.\nFor example, some works [4, 26] utilize attention-based structures\nto adaptively fuse multi-level features. Zhou et al.[59] propose an\nunified aggregation gate to dynamically fuse multi-scale features\nwith different channel-wise weights. Besides, some works [11, 34]\nuse multi-branch structures to learn multiple granularity features\nfrom independent regions. Those works show that fusing multi-scale\nfeatures can extract more semantic information and achieve better\nperformance. However, previous methods may limit the performance\nbecause of the less semantic information in low-level features. Thus,\nfurther exploration is needed on how to aggregate different level and\nscale features in image-based person Re-ID.\nNowadays, Transformers [39] have been the de-facto standard\nand gains a great success in Natural Language Processing (NLP).\nRecently, Transformers [1, 7, 13, 62] are extend to many computer\nvisual tasks and show superior performance. Dosovitskiy et al.[7]\npropose the Vision Transformer (ViT) for image recognition, achiev-\ning comparable performance with traditional CNNs. For person\nRe-ID, He et al. [13] introduce a pure transformer-based object\nRe-ID framework, showing state-of-the-art performance. Zhu et\nal. [62] apply an explicit alignment mechanism to enhance the capa-\nbility of Transformers in person Re-ID. The vision transformer has\nshown the capability of exploiting structural patterns from a global\nview. Although the pure-transformer structure shows great capability\narXiv:2107.05946v2  [cs.CV]  14 Jul 2021\nChengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China Guowen and Pingping, et al.\nTÂ©TÂ©TÂ©\n(a) No Aggregation(b) Skip Connection Aggregation\n(c) Feature Pyramid Aggregation(d) Parallel Aggregation\n(e) Our proposed Hierarchical Aggregation Transformer\nIN\nIN\nIN IN\nIN\nOUT\nOUT\nOUT\nOUT\nOUT\nMulti-scaleFeatures\nTConcatenationÂ©Transformer-based Feature Calibration\nOUT\nFigure 2: Illustration of different aggregation approaches. (a) Backbones without aggregation. (b) Skip connection aggregation. (c)\nFeature pyramid aggregation. (d) Parallel aggregation. (e) Our proposed Hierarchical Aggregation Transformer, which iteratively\naggregates the multi-scale features.\nand potentiality, it is limited by the need of amounts of data for\npre-training. Besides, compared with CNNs, the pure-transformer\nstructure lacks of desirable properties such as shift, scale and dis-\ntortion invariance and hierarchical structure, that make CNNs suite\nfor visual tasks. Inspired by CNNs and Transformers, we combine\nthe two basic structures to maintain the merits of them for image-\nbased person Re-ID. Actually, CNNs are used to extract hierarchical\nfeatures and the interaction in Transformers aims to aggregate the\nfeatures from different scales in a global view.\nIn this paper, we investigate how to aggregate multi-scale features\nto better fuse semantic and detail information for image-based per-\nson Re-ID. To address aforementioned issues, we propose a novel\nlearning framework named Hierarchical Aggregation Transformer\n(HAT) which aggregates multi-scale features and mines the discrimi-\nnative ones. Our proposed HAT can be utilized with any hierarchical\narchitectures and independent with the choice of backbones. Tech-\nnically, we first introduce a Deeply Supervised Aggregation (DSA)\nto recurrently aggregate hierarchical features from CNN backbones.\nThe DSA enhances and combines multi-scale features to generate\nmore discriminative features by multi-granularity supervisions. In-\nspired by the superior capability of Transformers, we propose the\nTransformer-based Feature Calibration (TFC) to integrate semantic\ninformation of cross-scale features. The novel calibration module\nhelps the model to preserve semantic and detail information from a\nglobal view. To verify the effectiveness of our proposed methods, we\nconduct extensive experiments on four large-scale benchmarks. The\nresults clearly demonstrate the superior performance of our methods\nover most state-of-the-art methods. The main contributions of our\nwork are summarized as:\nâ€¢We propose a novel framework (i.e., HAT) to aggregate multi-\nscale features to generate more discriminative features for\nhigh performance. It is the first to take advantages of both\nCNNs and Transformers for image-based person Re-ID.\nâ€¢We propose the Deeply Supervised Aggregation (DSA) to\nrecurrently and adaptively aggregate the hierarchical features\nof backbones with multi-granularity supervisions.\nâ€¢We introduce a Transformer-based Feature Calibration (TFC)\nto merge multi-scale features by exploring information from\na global view and promoting local information.\nâ€¢We conduct extensive ablation studies to demonstrate that\nour aggregation modules can effectively learn discrimina-\ntive features. Our method achieves state-of-the-art perfor-\nmances on four large-scale benchmark, i.e., Market1501 [52],\nDukeMTMC [56], CUHK03-NP [23] and MSMT17 [43].\n2 RELATED WORK\n2.1 Image-based Person Re-identification\nIn recent years, with the progress of deep learning, person Re-ID\nhas achieved great promotion in performance. Most existing Re-\nID models borrow architectures designed for image classification\nand other visual recognition tasks. Generally, existing image-based\nRe-ID methods mainly focus on extracting discriminative features\nfor persons. Thus, several effective attention mechanisms are intro-\nduced to suppress irrelevant features and enhance discriminative\nfeatures. Song et al.[31] utilize binary masks to reduce the noise of\nbackground and enhance the foreground features of persons. Chen\net al.[2] capture second-order correlations of features to enhance\ndiscriminative features. Chen et la.[3] integrate a pair of comple-\nmentary attention modules to learn better representations. Some\nworks [42, 49] capture context information from a global view to\nlearn discriminative features. However, it is not optimal to only focus\non the global features. Local information is also discriminative and\nhelpful in retrieving the same person. Su et al.[32] utilize external\npriori knowledge to learn local information. Besides, inspired by the\nHAT: Hierarchical Aggregation Transformers for Person Re-identification Chengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China\nspatial structure of human body, some works [34, 41] utilize hori-\nzontal partitions to learn discriminative local features. Many recent\nworks [4, 6] propose to erase salient regions in deep features to mine\ndiverse discriminative features. Meanwhile, aggregating semantic\nand detail information is also an important topic for person Re-ID.\nChen et al.[4] use a multi-stage feature fusion block to aggregate\nhigh-level and low-level information. Liu et al.[26] utilize correla-\ntion maps of cross-level feature-pairs to reinforce each level feature.\nZhou et al.[60] propose the consistent attention regularizer to keep\nthe deduced foreground mask similar from the low-level, mid-level\nand high-level feature maps. Different from previous works, we\nextend the multi-scale aggregation operation with hierarchical and\nrecurrent structures. Besides, we introduce the Transformers into the\naggregation operation to mine discriminative and specific informa-\ntion in each level. Our aggregation method can capture long-range\ndependencies from a global view.\n2.2 Transformer in Vision\nTransformers [39] are initially proposed in NLP and have become\nthe new standard in many NLP tasks. Recently, Transformers are\ntransplanted to many vision tasks such as image classification [ 7,\n38], object detection [ 1], semantic segmentation [ 54] and visual\ntracking [5]. Vision Transformers have shown superior capability and\ngreat potential for handling sequential data. Carion et al.[38] design\nan end-to-end network by Transformers to remove non-maximum\nsuppression and anchor generation. ViT [7] and DeiT [38] use pure-\ntransformers on the sequences of image patches.\nFor video-based person Re-ID, Liu et al.[25] design a trigem-\ninal network to transform video data into spatial, temporal and\nspatial-temporal feature spaces. Zhang et al.[48] design perception-\nconstrained Transformers to decrease the risk of overfitting. For\nimage-based person Re-ID, He et al.[13] utilize a pure-transformer\nwith a side information embedding and a jigsaw patch module to\nlearn discriminative features. Zhuet al.[62] add the learnable vectors\nof â€œpart tokenâ€ to learn part features and integrate the part align-\nment into the self-attention. All those pure-transformer methods\nhave achieved superior performance in image-based person Re-ID.\nHowever, Transformers lack desirable properties of CNNs such as\nshift, scale and distortion invariance. Besides, the hierarchical struc-\nture of CNNs can generate local spatial context at various levels.\nThus, we still keep the CNNs in our pipeline. Inspired by superior\ninteraction capability of Transformers (i.e., dynamic self-attention\nand global context), we introduce Transformers for our multi-scale\nfeature aggregation process.\n2.3 Deep Feature Aggregation\nDeep feature aggregation plays an critical role in many computer\nvision and multimedia tasks. Below, we review and highlight key ar-\nchitectures for the aggregation of hierarchical features. AlexNet [21]\nemphasizes the important of deep architectures for image classi-\nfication [29]. To introduce deeper networks, ResNet [ 12] utilizes\nidentity mappings to relieve the problem of degradation of deep net-\nworks. GoogLeNet [35] shows that the auxiliary losses are helpful\nfor network optimization. As shown in Fig. 2(c), Feature Pyramid\nNetworks (FPN) [24] introduces the top-down and lateral connec-\ntions for equalizing resolution and propogating semantic information\nacross different levels. HR-Net [33] connects the mutli-resolution\nsubnetworks in parallel to maintain the high-resolution representa-\ntions as shown in Fig. 2(d). For person Re-ID, Chenet al.[4] propose\na skip connection aggregation to integrate aggregate low-level and\nhigh-level features as shown in Fig. 2(b). Liu et al.[26] utilize corre-\nlation maps of cross-level features to enrich high-level features and\nlearn salient and specific information in low-level features. However,\nsimple aggregation structures will lead to poor performance [47] for\nRe-ID. The challenge lies in how to keep semantic information in\nhigh-level and enrich the features with detail information in low-\nlevel at the same time. To address these problems, we propose the\nhierarchical and iterative transformer-based aggregation structure.\nOur method can adaptively refine and aggregate multi-level features\nstep by step.\n3 PROPOSED METHOD\nIn this section, we introduce the proposed Hiearchical Aggregation\nTransformers (HAT). As shown in Fig. 3, it contains three key compo-\nnents: a Multi-scale Feature Extractor (MFE), a Deeply Supervised\nAggregation (DSA) and a Transformer-based Feature Calibration\n(TFC). Our HAT is trained in an end-to-end manner. In the following\nsubsection, we first give an overview of the proposed HAT. Then, we\nwill review the basic structure of ViT [7]. Finally, we will elaborate\nour proposed TFC and DSA in detail.\n3.1 Overview\nThe proposed framework (i.e., HAT) is shown in Fig. 3. It can take\nany CNNs as backbones, such as GoogleNet [35] and ResNet [12].\nWe adopt the widely-used ResNet-50 as our Multi-scale Feature\nExtractor (MFE) for its powerful feature representation for person\nRe-ID [4, 6, 34, 49]. To begin with, our backbone extracts hierar-\nchical features with different scales and semantic information from\nRes2, Res3, Res4 and Res5. Then, we insert the TFC module to each\nscale of hierarchical features. The TFC is utilized to integrate the\nsemantic and detail information in previous scales, and then gen-\nerate global priors for next scales. For previous- and current-level\nfeatures, we utilize the concatenation operation to ensure semantic\ninformation independent of different levels before interaction. Based\non the extracted multi-scale features by TFC, the DSA utilizes multi-\ngranularity supervisions to recurrently supervise the aggregation of\nthe multi-scale features from low-level to high-level. Finally, the\nidentification loss [36] and triplet loss [15] are utilized for the end-to-\nend training process. In the test stage, the final outputs of backbone\nand HAT are concatenated for the retrieval list.\n3.2 Transformer-based Feature Calibration\nIt has been demonstrated that multi-scale feature aggregation [12, 24,\n46] improves the capacity of deep networks in image classification\nobject detection and semantic segmentation . However, person Re-ID\nis a special task which needs discriminative representations with\nsufficient semantic information. Traditional aggregation operations\nof high- and low-level features will limit the performance due to less\nsemantic information in shallower layers. In this work, our proposed\nTFC aims to integrate previous level features from a global view and\nthen aggregates previous and current level features.\nGenerally, given the feature maps from the s- ğ‘¡â„ scale block of\nbackbones, we obtain the hierarchical featureğ‘‹ğ‘  âˆˆRğ¶ğ‘ Ã—ğ»Ã—ğ‘Š where\nChengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China Guowen and Pingping, et al.\nInput ImageRes2\n256Ã—128\nRes4 Res5\nBottleneck\nBN NeckIdLossTriplet Loss\nTransformerÃ—ğ‘›\nNeighborhood Adjustment\nScaling\nRes3\nFLâ‹¯(CLS)\nâ‹¯RS\nBottleneckScaling\nÂ©\nTransformerÃ—ğ‘›\nFLâ‹¯\nâ‹¯RS\nBottleneckScaling\nÂ©\nTransformerÃ—ğ‘›\nFLâ‹¯\nâ‹¯RS\nBottleneckScaling\nÂ©\nTransformerÃ—ğ‘›\nFLâ‹¯(CLS) (CLS) (CLS)\nBN NeckIdLoss\nTriplet LossAL AL AL\nRSReshapeFLFlattenÂ©ConcatenationALAuxiliary Loss\n< PE > < PE > < PE > < PE >\n< PE >Position Embedding\nMFE\nDSA\nNeighborhood Adjustment\nTFC Neighborhood Adjustment\nFigure 3: Illustration of our proposed Hierarchical Aggregation Transformer. The person images first go forward through the ResNet-\n50 backbone. After each residual block, we employ a bottleneck and scaling module to ensure the multi-level featrues have the same\ndimension. Then our proposed TFC integrates low-level features as the global priors for high-level features. Finally, multiple losses\nare used to supervise the whole framework in the training process.\nğ¶,ğ» and ğ‘Š denote the number of channel, width and height of fea-\ntures, respectively. Then we employ the bottleneck which applies a\nstack of residual blocks [12] to transform ğ‘‹ğ‘  into compact embed-\ndings. Most CNNs [12, 44] have utilized pooling layers to reduce\nthe spatial dimension. The pooling layers [14] have been determined\nto be helpful to improve the model capability and generalization\nperformance. Besides, bilinear interpolation [50] is widely used to\nenlarge the resolution of features. After the bottleneck, a scaling\nmodule which is made up of a max pooling or bilinear interpolation\nupsampling is utilized to aggregate the resolution of hierarchical\nfeatures. The scaling module resizes the hierarchical features to the\nsame resolution for integration and optimization.\nTraditional aggregation operations always utilize CNN-based\nstructures. However, the limited receptive fields of convolutions\nlimit the capability of the cross-scale interaction. Thus, for current\nhierarchical inputs ğ‘‹ğ‘  and previous TFC output features ğ‘ğ‘ âˆ’1 âˆˆ\nRğ¶ğ‘ âˆ’1Ã—ğ»Ã—ğ‘Š, we use Transformers [21] to strengthen and suppress\ninformation in features. Compared with convolutions, the multi-head\nself-attention in Transformers can capture long-range dependen-\ncies and attend diverse information from a global view. Meanwhile,\nTransformers can preserve the semantic information in interaction\namong different scale features. Transformers in TFC receive a se-\nquence of token embeddings as input. To handle the 2D input fea-\ntures ğ‘ğ‘  = [ğ‘‹ğ‘ ; ğ‘ğ‘ âˆ’1]âˆˆ R(ğ¶ğ‘ +ğ¶ğ‘ âˆ’1)Ã—ğ»Ã—ğ‘Š, we first flatten the feature\ninto 2D patches ğ‘ğ‘\nğ‘  âˆˆRğ‘Ã—ğ¶ğ‘, where ğ‘ = ğ» Ã—ğ‘Š/ğ‘ƒ2 is the number\nof patches, ğ¶ğ‘ = ğ¶ğ‘  +ğ¶ğ‘ âˆ’1 Ã—ğ‘ƒ2 and ğ‘ƒ is set to 1 in our framework.\nThen the representation named class token (CLS) is added to the\nsequence, and serves as the discriminative representation. Spatial\ninformation is incorporated by adding learnable Position Embedding\nâ‹¯Linear Projection\nMulti-Head Attentionğ‘„ ğ¾ ğ‘‰\nâŠ•Norm\nMLPâŠ• NÃ—\nNeighborhoodAdjustment\n[ğ‘‹!;ğ‘!\"#]\nğ‘!\nTransformer\nFigure 4: Illustration of our proposed TFC. It integrate the\nmulti-scale features from a global view.\n(PE) for each patch, resulting in the input of sequence with a size of\nğ‘ğ‘\nğ‘  âˆˆR(ğ‘+1)Ã—ğ¶ğ‘.\nAs shown in Fig. 4, a Transformer is composed of a multi-head\nself-attention layer (MSA), a feed-forward network (FFN), layer\nnormalizations and residual connections. The feature ğ‘ğ‘\nğ‘  is first\nHAT: Hierarchical Aggregation Transformers for Person Re-identification Chengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China\npassed to the multi-head self-attention layer. In each head, ğ‘ğ‘\nğ‘  is\nfeed into three linear projections to generate query ğ‘„ âˆˆR(ğ‘+1)Ã—ğ‘‘,\nkey ğ¾ âˆˆR(ğ‘+1)Ã—ğ‘‘, and valueğ‘‰ âˆˆR(ğ‘+1)Ã—ğ‘‘, where ğ‘‘ = ğ¶ğ‘\nğ‘â„\nand ğ‘â„\nis the number of heads. The self-attention is based on the trainable\nassociative relation between query and key. Then a softmax function\nis utilized to normalize the obtained attention weight. The output of\nthe multi-head self-attention is the weighted sum of ğ‘‰ by attention\nweights as following:\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘„,ğ¾,ğ‘‰ )= ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡\nâˆš\nğ‘‘\n)ğ‘‰, (1)\nwhere\nâˆš\nğ‘‘ is utilized to normalize for numerical stability. In this\nway, based on the input which is the concatenation of cross-level\nfeatures, Transformers can interact the multi-level information in a\nglobal view. With deep supervisions, Transformers can preserve the\nsemantic information and then add the detail information mined in\nprevious hierarchies to current level.\nThen the output and input of the multi-head self-attention layer\nare connected by residual connections and a normalization layer,\nğ‘ğ‘\nğ‘  = ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ğ‘œğ‘Ÿğ‘š (ğ‘ğ‘\nğ‘  +ğ‘€ğ‘†ğ´(ğ‘ğ‘\nğ‘  )). (2)\nThe feed-forward network (FFN) consisting of two linear projection\nand GELU [21] activation function is applied after the MSA layer.\nğ‘ğ‘\nğ‘  = ğ¿ğ‘ğ‘¦ğ‘’ğ‘Ÿğ‘ğ‘œğ‘Ÿğ‘š (ğ‘ğ‘\nğ‘  +ğ¹ğ¹ğ‘ (ğ‘ğ‘\nğ‘  )), (3)\nğ¹ğ¹ğ‘ (ğ‘ğ‘\nğ‘  )= ğ‘Š2ğœ(ğ‘Š1ğ‘ğ‘\nğ‘  ), (4)\nwhere ğ‘Š1,ğ‘Š2 are the parameters of two linear projections and ğœ is\nthe GELU activation function. In this way, we integrate the multi-\nscale features from a global view. We replace the traditional CNN-\nbased sturctures by Transformers to obtain a global view which\nis more suitable for the aggregation of multi-scale features. And\nthe attention-based aggregation can strengthen and suppress the\nmulti-level information.\nIn Transformers, all the tokens of features are equally utilized,\nbut the neighborhood features are more important in local regions.\nTo combine the advantage of CNNs (i.e., shift, scale and distortion\ninvariance and extracting local information) with the ability of Trans-\nformers, we utilize a Neighborhood Adjustment (NeA) module after\nTransformers. The output features of Transformers except CLS are\nreshaped to the same size as the input. Then the features are forward\ninto the Neighborhood Adjustment module which is composed of\na stack of convolution layers with batch normalizations. Thus, the\nfinal output of our proposed TFC is:\nğ‘ğ‘  = ğ¶ğ‘œğ‘›ğ‘£(ğ‘…ğ‘’ğ‘ â„ğ‘ğ‘ğ‘’(ğ‘ğ‘\nğ‘  )). (5)\nCompared with ViT [21] : Our TFC is inspired by the recent\nwork ViT [21]. However, it has the following fundamental differ-\nences. First, the studied tasks are different. ViT is designed for image\nclassification, while our work is for image-based person Re-ID. Sec-\nond, the inputs are different. ViT takes the tokens from previous\nTransformersâ€™ outputs, while our inputs are consist of hierarchical\nfeatures of backbones and the outputs of previous TFC. Third, the\noptimization objective is different. ViT aims to use Transformers as\nbackbones to learn the representation for images. The motivation\nof our TFC is to merge and preserve the semantic information and\ndetail information of multi-level features.\n3.3 Deeply Supervised Aggregation\nWe have revisited the aggregation structures of deep networks from\nvarious perspectives. The semantic information in high-level and\ndetail information in low-level are both useful for visual tasks. Many\nworks [4, 24, 33, 50] have explored effective methods to incorporate\nthose information. The key point is to balance the multi-level fea-\ntures according to the requirement of tasks. In image-based person\nRe-ID, although the feature aggregation has been explored, it still\nhas a lot of limitations. Some works [ 4, 47] have determined that\nsimple concatenation of low-level, mid-level and high-level features\nwill result in a worse performance for person Re-ID. The main rea-\nson is the less semantic information of low-level features. Based on\nthis finding, we propose the Deeply Supervised Aggregation (DSA)\nfor person Re-ID. With DSA, we introduce multi-granularity super-\nvisions to supervise the aggregation process to relieve the problem\nof less semantic information in low-level features.\nDSA follows the iterated stacking of the backbone architecture.\nWe divide the stacked blocks of the MFE according to the hierarchy\nof backbones. Direct concatenation of deeper and shallower layers\nwithout any correct guidance will limit the performance of networks\nfor person Re-ID. Thus, we propose to progressively aggregate and\nrefine the representation with multi-granularity supervisions by our\nproposed DSA. Our DSA begins at the shallower layers with more\ndetail and less semantic information and then recurrently merges\ndeeper features with more semantic and less detail information. Be-\nsides, to enhance the semantic information in the interaction of TFC,\nwe utilize auxiliary losses to supervise the hierarchical aggregation.\nThe auxiliary loss is composed of the identification loss and triplet\nloss to keep the same optimization objective as the supervision of\nthe whole framework. In this way, the shallower features deliver\ndetail information to deeper features and the semantic information\nin high-level can be preserved. The DSA for hierarchical features\nğ‘‹1,ğ‘‹2,...,ğ‘‹ ğ‘› with enhanced semantic information is formulated as:\nğ·ğ‘†ğ´(ğ‘‹1,...,ğ‘‹ ğ‘›)=\n\u001a ğ‘‹1 ğ‘›= 1,\nğ‘‡ğ¹ğ¶ (ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡(ğ·ğ‘†ğ´(ğ‘‹1,...,ğ‘‹ ğ‘›âˆ’1),ğ‘‹ğ‘›)) ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’.\n(6)\n3.4 Loss Functions\nFollowing previous works [4, 27], we treat each identity as a distinct\nclass. The label smoothed identification loss [36] is used to supervise\nthe CNN backbones and Transformers in the training procedure. The\nidentification loss is defined as:\nLğ‘–ğ‘‘ =\nğ‘âˆ‘ï¸\nğ‘–=1\nâˆ’ğ‘ğ‘– log(ğ‘ğ‘–), (7)\nwhere ğ‘ğ‘– is the predicted logit of identity ğ‘–and ğ‘ğ‘– is the ground-truth\nlabel. The parameter ğœ€in label smoothing is set to 0.1. Besides, the\nhard triplet loss [15] is utilized to make that inter-class distance is\nless than itra-class distance.\nLğ‘¡ğ‘Ÿğ‘– = [ğ‘‘ğ‘ğ‘œğ‘  âˆ’ğ‘‘ğ‘›ğ‘’ğ‘” +ğ‘š]+, (8)\nwhere ğ‘‘ğ‘ğ‘œğ‘  and ğ‘‘ğ‘›ğ‘’ğ‘” are respectively defined as the distance of\npostive sample pairs and negative sample pairs. [ğ‘‘]+ represents\nğ‘šğ‘ğ‘¥(0,ğ‘¥)and ğ‘š is the distance margin. The auxiliary loss Lğ‘ğ‘™ is\ncomposed of identification loss and triplet loss with the coefficient ğœ†\nChengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China Guowen and Pingping, et al.\nTable 1: Quantitative comparison on Market1501, DukeMTMC, CUHK03-NP and MSMT17 datasets.\nMethods Ref Backbone Market1501 DukeMTMC CUHK03-NP MSMT17Labeled Detected\nmAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1 mAP Rank-1\nDuATM [30] CVPR18 DenseNet121 76.60 91.40 64.60 81.80 - - - - - -\nMancs [40] ECCV18 ResNet50 82.30 93.10 71.80 84.90 63.90 69.00 60.50 65.50 - -\nIANet [16] CVPR19 ResNet50 83.10 94.40 73.40 83.10 - - - - 46.80 75.50\nPCB [34] ECCV18 ResNet50 81.60 93.80 69.20 83.30 - - 57.50 63.70 40.40 68.20\nSPReID [18] CVPR18 ResNet152 83.36 93.68 73.34 85.95- - - - - -\nAANet [37] CVPR19 ResNet152 83.41 93.93 74.29 87.65 - - - - - -\nCASN [53] CVPR19 ResNet50 82.80 94.40 73.70 87.70 68.00 73.70 64.40 71.50 - -\nCAMA [45] CVPR19 ResNet50 84.50 94.70 72.90 85.80 - - 64.20 66.60 - -\nBATNet [8] ICCV19 ResNet50 84.70 95.10 77.30 87.70 76.10 78.6 73.20 76.20 56.80 79.50\nMHN-6 [2] ICCV19 ResNet50 85.00 95.10 77.20 89.10 72.24 77.20 65.40 71.70 - -\nBFE [6] ICCV19 ResNet50 86.20 95.30 75.90 88.90 76.70 79.40 73.50 76.40 51.50 78.80\nMGN [40] MM18 ResNet50 86.90 95.70 78.40 88.70 67.40 68.00 66.00 68.00 - -\nABDNet [3] ICCV19 ResNet50 88.28 95.60 78.60 89.00 - - - - 60.80 82.30\nPyramid [51] CVPR19 ResNet101 88.20 95.70 79.00 89.00 76.90 78.90 74.80 78.90 - -\nJDGL [55] CVPR19 ResNet50 86.00 94.80 74.80 86.60 - - - - 52.30 77.20\nOSNet [59] ICCV19 OSNet 84.90 94.80 73.50 88.60 - - 67.80 72.30 52.90 78.70\nSNR [17] CVPR20 ResNet50 84.70 94.40 73.00 85.90 - - - - - -\nRGA-SC [49] CVPR20 ResNet50 88.40 96.10 - - 77.40 81.10 74.50 79.60 57.50 80.30\nISP [61] ECCV20 HRNet48 88.60 95.30 80.00 89.60 74.10 76.50 71.40 75.20 - -\nCDNet [22] CVPR21 CDNet 86.00 95.10 76.80 88.60 - - - - 54.70 78.90\nAAformer [62] Arxiv21 ViT 87.70 95.40 80.00 90.10 77.80 79.90 74.80 77.60 62.60 83.10\nDAT - ResNet50 89.50 95.60 81.40 90.40 80.00 82.60 75.50 79.10 61.20 82.30\nfor HAT in different hierarchical levels. Therefore, the overall loss\nfunction of the framework is:\nLğ‘Ÿğ‘’ğ‘–ğ‘‘ = Lğ‘–ğ‘‘ +Lğ‘¡ğ‘Ÿğ‘– +ğœ†\nğ‘›ğ‘ğ‘™âˆ‘ï¸\nğ‘–\n(Lğ‘ğ‘™). (9)\nwhere ğ‘›ğ‘ğ‘™ is the number of the stage blocks.\n4 EXPERIMENTS\n4.1 Datasets and Evaluation Metrics\nTo verify the effectiveness of our proposed framework, we con-\nduct experiments on four large-scale datasets, i.e., Market1501 [52],\nDukeMTMC [ 56], CUHK03-NP [ 23] and MSMT17 [ 43]. Mar-\nket1501 dataset contains 19,732 gallery images and 12,936 training\nimages captured from six cameras by Deformable Part Model (DPM)\ndetector [9]. DukeMTMC-reID dataset contains 1,404 identities,\n16,522 training images, 2,228 queries and 17,661 gallery images.\nCUHK03 dataset contains 13,164 images of 1,467 identities and\nthe partition of this dataset follows the method in [ 57]. The most\nchallenge dataset MSMT17 is the largest. There are 126,441 images\nof 4,101 identities observed under 15 different camera views. All\nthe bounding boxes are captured by Faster R-CNN [28]. Following\nprevious works, we adopt mean Average Precision (mAP) and Cumu-\nlative Matching Characteristics (CMC) at Rank-1 as our evaluation\nmetrics.\n4.2 Implementation Details\nIn our work, we uniformly resize all the input images to 256 Ã—\n128, then followed by randomly cropping, horizontal flipping and\nrandom erasing [27] as data augmentation. Besides, there are ğµ =\nğ‘ƒÃ—ğ¿images sampled to the triplet loss and identification loss in a\nmini-batch for every training iteration. We randomly selectğ‘ƒ = 16\nidentities and ğ¿= 4 for each identity. The ResNet-50 [12] pre-trained\non ImageNet [19] is utilized as our MFE. A warmup strategy with a\nlinearly growing rate from 4Ã—10âˆ’6 to 4Ã—10âˆ’4 is used for the first 10\nepoch. The learning rate of TFC is half of the whole structure. The\nlearning rate begins to decrease at 50 epoch, and reduce for every 20\nepoch with a factor of 0.4. We employ Adam [20] as our optimizer\nfor total 150 epoch.\n4.3 Comparison with State-of-the-arts\nMarket1501: Tab. 1 shows the evaluation on the Market1501 dataset.\nWe compare very recent CNN-based and Transformer-based meth-\nods with our model. From the results, we can see that our framework\nachieves the best performance in mAP. Our Rank-1 is also very\ncomparable to the best result achieved by RGA-SC which learns a\nglobal scope attention to align features. The results indicate that our\nproposed framework can effectively aggregate detail information\nin shallower layers and semantic information in deeper layers to\nretrieve the hard samples.\nDukeMTMC: As shown in Tab. 1, our framework achieves the\nbest performance in mAP/Rank-1. It is worth pointing out that the\nbackbone of AAformer is ViT [21] which is a stronger baseline than\nResNet50. The result indicates that our framework outperforms other\nHAT: Hierarchical Aggregation Transformers for Person Re-identification Chengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China\nMethod DukeMTMC Market1501\nmAP Rank-1 mAP Rank-1\nBaseline [27] 75.90 86.30\n{12,0,0,0} 73.80 83.70 82.30 92.60\n{0,12,0,0} 76.50 87.00 86.30 94.10\n{0,0,12,0} 79.80 89.00 88.30 95.20\n{0,0,0,12} 79.00 88.40 88.10 94.90\n{3,3,3,3} 80.30 89.10 89.30 95.50\n{0,4,4,4} 80.40 89.60 89.30 95.70\n{4,4,4,0} 81.10 89.60 89.40 95.30\n{3,4,5,0} 81.30 89.50 89.40 95.30\n{3,3,6,0} 81.40 90.40 89.50 95.60\n{0,0,12,0} 79.80 89.00 88.50 95.20\n{0,0,6,0} 79.70 89.10 88.90 95.50\nTable 2: Ablation analysis of TFC on DukeMTMC.\nstate-of-the-art methods at least 1.0% and 0.3% in mAP and Rank-1 .\nIt indicates that our method can capture more discriminative features\nthan other methods.\nCUHK03-NP: Tab. 1 shows that our HAT also achieves the best\nperformance on this dataset. CUHK03-NP has fewer samples, and\nthe annotation of bounding boxes is obtained by detection [10]. Our\nframework outperforms state-of-the-art methods at least 2.2%, 2.7%\nin mAP/Rank-1 for labeled CUHK03, In detected CUHK03, our\nmethod achieves the bset mAP and the second best in Rank-1.\nMSMT17: On this dataset, our framework achieves the second\nbest performance in mAP and Rank-1 as shown in Tab. 1. The best\nRank-1 is achieved by AAformer [4]. The MSMT17 is a large-scale\ndataset which covers a long period of time and presents complex\nlighting variations. AAformer uses ViT [21] as the backbone which\nis better in capturing long-range dependencies than CNNs. However,\nthe performance of our framework is also comparable.\n4.4 Ablation Studies\nTo demonstrate the effects of our proposed methods, we perform\nablation studies on DukeMTMC dataset. As for our CNN backbone,\nwe use the strong baseline [27] with ResNet50 [12] as our start point.\nWe employ the identificaiton loss, triplet loss and auxiliary loss for\nthe whole framework.\nTFC: To verify the effectiveness of our proposed TFC, we com-\npare the performance of different combinations of multi-level fea-\ntures. First we respectively define the number of Transformer layers\nin each TFC as {ğ‘›1,ğ‘›2,...,ğ‘› ğ‘›}along the hierarchical levels. ğ‘›ğ‘– = 0\nmeans that the features at i-ğ‘¡â„ level are not utilized for aggregation.\nTo follow the setting in ViT [7], we fix the number of Transformers\nof all TFC to 12. As shown in Tab. 2, we first conduct extensive\nexperiments to investigate the effectiveness of the different hierar-\nchical features. To further explore the effectiveness of features for\neach aggregation level, we conduct separate experiments for each\nhierarchical level. Compared with low-level and high-level features,\nonly utilizing the mid-level features in Res4 block performs bet-\nter than only utilization of others. It indicates that the low-level\nfeatures are not suitable for retrieval without sufficient semantic\ninformation. It worth noting that the performance of utilizing the\nhighest level features is worse than the features in Res4. We infer\nthat the supervisions of MFE and TFC on the highest level features\nMethod DukeMTMC Market1501\nmAP Rank-1 mAP Rank-1\nBaseline [27] 75.90 86.30\nHAT w/o MFE Supervision 76.50 88.20 85.30 94.10\nHAT w/o AL 80.40 89.50 88.70 95.00\nHAT w/o NeA 81.10 90.20 89.50 95.60\nHAT 81.40 90.40\nTable 3: Ablation analysis of HAT on DukeMTMC. AL: Auxil-\niary Loss. NeA: Neighborhood Adjustment.\nMethod DukeMTMC Market1501\nmAP Rank-1 mAP Rank-1\nBaseline [27] 75.90 86.30\nd = 8 81.20 89.80 89.70 95.50\nd = 16 81.40 90.40\nd = 32 80.50 89.50 88.90 95.50\nTable 4: Ablation analysis of Scaling on DukeMTMC.\nMethod DukeMTMC Market1501\nmAP Rank-1 mAP Rank-1\nBaseline [27] 75.90 86.30\nğœ†= 0 80.40 89.50 88.80 95.00\nğœ†= 0.1 80.60 89.50 89.20 95.40\nğœ†= 0.3 81.60 90.10 89.70 95.50\nğœ†= 0.5 81.40 90.40 89.80 95.80\nğœ†= 0.8 80.90 89.40 89.50 95.60\nğœ†= 1.0 80.90 89.80 89.20 95.40\nTable 5: Ablation analysis ofğœ†of auxiliary loss on DukeMTMC.\nwill make it hard to converge and thus the networks have a worse\nperformance. Besides, the results also indicate that deeper features\nneed more Transformers to integrate the multi-level information by\ncomparing the results of {4,4,4,0}and {3,3,6,0}. The comparison\nof {0,0,12,0}and {0,0,6,0}determines that the accuracy tends to\nconverge with the increase of the depth of Transformers. The re-\nsults show that our proposed aggregation methods can significantly\nimprove the performance by 5.3% and 4.1% in mAP and Rank-1.\nHAT: As shown in Tab. 3, we evaluate the effectiveness of deep\nsupervisions on DukeMTMC. All those experiments are under the\nsetting of {3,3,6,0}. Without the supervision for MFE, the accuracy\nof our proposed HAT decreases by 4.9% mAP and 2.2% Rank-1. It\ndetermines that the supervisions for backbones which are utilized\nfor the multi-scale feature extraction is critical. The supervision\nof MFE ensures the semantic information in different hierarchical\nfeatures. Besides, the auxiliary losses (AL) which are composed\nof multi-granularity supervisions can improve the performance of\nHAT by 1.0% mAP and 0.9% Rank-1. The result shows the auxiliary\nlosses can preserve the semantic information in the interaction of\nTFC. Meanwhile, by adding the auxiliary losses to TFC, we can\nboost semantic information in the shallower layers, increase the\ngradient signal that gets propagated back, and provide additional\nregularization. As shown in Tab. 5, the auxiliary loss which with a\nsmall coefficient has a better performance. At last, the Neighborhood\nAdjustment (NA) module can further improve the capability of our\nframework by enhancing local information.\nChengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China Guowen and Pingping, et al.\n(a) (b) (c)\n(d) (e) (f)\n(g) (h) (i)\nInputBaselineMFE DSAInputBaselineMFE DSAInputBaselineMFE DSA\nFigure 5: Feature visualization of baseline and DSA and MFE of our proposed HAT.\nTFC1OutputInput\n(a)\n(b)\n(c)\n(d)\nğ‘‹! ğ‘‹\" TFC2Outputğ‘‹# TFC3Output\nFigure 6: Visualization of averaged feature maps in TFC.\nScaling: To keep the resolution of hierarchical features consistent,\nthe scaling module resizes the resolution of multi-scale features\ninto (ğ»\nğ‘‘,ğ‘Š\nğ‘‘ ). For computational and space complexity, we conduct\nexperiments with ğ‘‘ = 8,16 and 32 to verify the effectiveness of\nscaling. As shown in Tab. 4, the resolution withğ‘‘ = 16 has a better\nperformance than ğ‘‘ = 8. It shows that a larger resolution may capture\nmore information in features for interaction in TFC. Besides, the\naccuracies of ğ‘‘ = 8 and ğ‘‘ = 16 are close. No improvement with\nincreased resolution may be the introduction of more irrelevant\nfeatures in the aggregation.\n4.5 Visualization\nAs shown in Fig. 5, we present examples of different identities and\ntheir CAM [58] visualization of MFE and DSA. To verify the effec-\ntiveness of our proposed HAT, we compare the CAM visualization\nof the baseline with our methods. In each example, from the left to\nright are the original image, the visualization of the baseline, MFE\nand DSA features. It can be observed that our aggregation method\ncan capture more detail information than the baseline. For example,\nas shown in Fig. 5(d) and (e), the feature maps of baseline only\nfocus on the shoes of the person. With aggregating the multi-scale\nfeatures by our HAT, the networks tend to focus on the pattern of\nbags and clothes. Besides, the averaged feature maps of different\nTFC from TFC (Res2) to TFC (Res4) are shown in Fig. 6. The useful\ninformation increases from low-level to high-level by aggregating\nmulti-scale features. The results determine that our proposed HAT\ncan mine the discrminative features from a global view and thus\ncapture more useful information. From the visualization, one can\nfind that our methods can effectively capture more detail information\nsuch as texture to improve the performance.\n5 CONCLUSION\nIn this paper, we investigate how to effectively aggregate multi-scale\nfeatures to better fuse semantic and detail information for image-\nbased person Re-ID. To this goal, we propose a novel framework\nnamed HAT consisting of TFC and DSA. The proposed TFC can\nmerge and preserve the semantic information and detail informa-\ntion in cross-level features. It integrates low-level detail information\nas the global prior for high-level semantic information. Besides,\nHAT: Hierarchical Aggregation Transformers for Person Re-identification Chengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China\nDSA can iteratively aggregate the hierarchical features of back-\nbones with multiple losses. Eventually, extensive experiments on\nfour large-scale benchmarks demonstrate that our method achieves\nbetter performance than most state-of-the-art methods.\nREFERENCES\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In European Conference on Computer Vision. Springer, 213â€“229.\n[2] Binghui Chen, Weihong Deng, and Jiani Hu. 2019. Mixed high-order attention\nnetwork for person re-identification. In ICCV. 371â€“381.\n[3] Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang,\nZhou Ren, and Zhangyang Wang. 2019. Abd-net: Attentive but diverse person\nre-identification. In ICCV. 8351â€“8361.\n[4] Xuesong Chen, Canmiao Fu, Yong Zhao, Feng Zheng, Jingkuan Song, Rongrong\nJi, and Yi Yang. 2020. Salience-Guided Cascaded Suppression Network for Person\nRe-Identification. In CVPR. 3300â€“3310.\n[5] Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu.\n2021. Transformer Tracking. arXiv preprint arXiv:2103.15436(2021).\n[6] Zuozhuo Dai, Mingqiang Chen, Xiaodong Gu, Siyu Zhu, and Ping Tan. 2019.\nBatch DropBlock network for person re-identification and beyond. In CVPR.\n3691â€“3701.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,\nXiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An\nImage is Worth 16x16 Words: Transformers for Image Recognition at Scale.\narXiv:2010.11929 [cs.CV]\n[8] Pengfei Fang, Jieming Zhou, Soumava Kumar Roy, Lars Petersson, and Mehrtash\nHarandi. 2019. Bilinear attention networks for person retrieval. In ICCV. 8030â€“\n8039.\n[9] Pedro Felzenszwalb, David McAllester, and Deva Ramanan. 2008. A discrimina-\ntively trained, multiscale, deformable part model. In CVPR. IEEE, 1â€“8.\n[10] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan.\n2009. Object detection with discriminatively trained part-based models. IEEE\nTPAMI 32, 9 (2009), 1627â€“1645.\n[11] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A\nWichmann, and Wieland Brendel. 2018. ImageNet-trained CNNs are biased\ntowards texture; increasing shape bias improves accuracy and robustness. arXiv\npreprint arXiv:1811.12231(2018).\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In CVPR. 770â€“778.\n[13] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang.\n2021. TransReID: Transformer-based Object Re-Identification. arXiv preprint\narXiv:2102.04378 (2021).\n[14] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and\nSeong Joon Oh. 2021. Rethinking Spatial Dimensions of Vision Transformers.\narXiv preprint arXiv:2103.16302(2021).\n[15] Alexander Hermans, Lucas Beyer, and Bastian Leibe. 2017. In defense of the\ntriplet loss for person re-identification. arXiv preprint arXiv:1703.07737(2017).\n[16] Ruibing Hou, Bingpeng Ma, Hong Chang, Xinqian Gu, Shiguang Shan, and Xilin\nChen. 2019. Interaction-and-aggregation network for person re-identification. In\nCVPR. 9317â€“9326.\n[17] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li Zhang. 2020. Style\nnormalization and restitution for generalizable person re-identification. In CVPR.\n3143â€“3152.\n[18] Mahdi M Kalayeh, Emrah Basaran, Muhittin GÃ¶kmen, Mustafa E Kamasak, and\nMubarak Shah. 2018. Human semantic parsing for person re-identification. In\nCVPR. 1062â€“1071.\n[19] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Suk-\nthankar, and Li Fei-Fei. 2014. Large-scale video classification with convolutional\nneural networks. In CVPR. 1725â€“1732.\n[20] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980(2014).\n[21] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-\ncation with deep convolutional neural networks. Advances in neural information\nprocessing systems25 (2012), 1097â€“1105.\n[22] Hanjun Li, Gaojie Wu, and Wei-Shi Zheng. 2021. Combined Depth Space\nbased Architecture Search For Person Re-identification. arXiv preprint\narXiv:2104.04163 (2021).\n[23] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. 2014. Deepreid: Deep filter\npairing neural network for person re-identification. In CVPR. 152â€“159.\n[24] Tsung-Yi Lin, Piotr DollÃ¡r, Ross Girshick, Kaiming He, Bharath Hariharan, and\nSerge Belongie. 2017. Feature pyramid networks for object detection. In CVPR.\n2117â€“2125.\n[25] Xuehu Liu, Pingping Zhang, Chenyang Yu, Huchuan Lu, Xuesheng Qian, and\nXiaoyun Yang. 2021. A Video Is Worth Three Views: Trigeminal Transformers for\nVideo-based Person Re-identification. arXiv preprint arXiv:2104.01745(2021).\n[26] Zhipu Liu, Lei Zhang, and Yang Yang. 2020. Hierarchical Bi-Directional Feature\nPerception Network for Person Re-Identification. In Proceedings of the 28th ACM\nInternational Conference on Multimedia. 4289â€“4298.\n[27] Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei Jiang. 2019. Bag of\nTricks and a Strong Baseline for Deep Person Re-Identification. In CVPRW.\n[28] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2016. Faster r-cnn:\nTowards real-time object detection with region proposal networks. IEEE TPAMI\n39, 6 (2016), 1137â€“1149.\n[29] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.\n2015. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision115, 3 (2015), 211â€“252.\n[30] Jianlou Si, Honggang Zhang, Chun-Guang Li, Jason Kuen, Xiangfei Kong, Alex C\nKot, and Gang Wang. 2018. Dual attention matching network for context-aware\nfeature sequence based person re-identification. In CVPR. 5363â€“5372.\n[31] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. 2018. Mask-guided\ncontrastive attention model for person re-identification. In CVPR. 1179â€“1188.\n[32] Chi Su, Jianing Li, Shiliang Zhang, Junliang Xing, Wen Gao, and Qi Tian. 2017.\nPose-driven deep convolutional model for person re-identification. InICCV. 3960â€“\n3969.\n[33] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. 2019. Deep high-resolution\nrepresentation learning for human pose estimation. In CVPR. 5693â€“5703.\n[34] Yifan Sun, Liang Zheng, Yi Yang, Qi Tian, and Shengjin Wang. 2018. Beyond\npart models: Person retrieval with refined part pooling (and a strong convolutional\nbaseline). In ECCV. 480â€“496.\n[35] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015.\nGoing deeper with convolutions. In CVPR. 1â€“9.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew\nWojna. 2016. Rethinking the inception architecture for computer vision. In CVPR.\n2818â€“2826.\n[37] Chiat-Pin Tay, Sharmili Roy, and Kim-Hui Yap. 2019. Aanet: Attribute attention\nnetwork for person re-identifications. In CVPR. 7134â€“7143.\n[38] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre\nSablayrolles, and HervÃ© JÃ©gou. 2020. Training data-efficient image transformers\n& distillation through attention. arXiv preprint arXiv:2012.12877(2020).\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you\nneed. arXiv preprint arXiv:1706.03762(2017).\n[40] Cheng Wang, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. 2018.\nMancs: A multi-task attentional network with curriculum sampling for person\nre-identification. In ECCV. 365â€“381.\n[41] Guanshuo Wang, Yufeng Yuan, Xiong Chen, Jiwei Li, and Xi Zhou. 2018. Learn-\ning discriminative features with multiple granularities for person re-identification.\nIn ACM MM. 274â€“282.\n[42] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local\nneural networks. In CVPR. 7794â€“7803.\n[43] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian. 2018. Person transfer gan\nto bridge domain gap for person re-identification. In CVPR. 79â€“88.\n[44] Saining Xie, Ross Girshick, Piotr DollÃ¡r, Zhuowen Tu, and Kaiming He. 2017.\nAggregated residual transformations for deep neural networks. In CVPR. 1492â€“\n1500.\n[45] Wenjie Yang, Houjing Huang, Zhang Zhang, Xiaotang Chen, Kaiqi Huang, and\nShu Zhang. 2019. Towards rich feature discovery with class activation maps\naugmentation for person re-identification. In CVPR. 1389â€“1398.\n[46] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. 2018. Deep layer\naggregation. In CVPR. 2403â€“2412.\n[47] Qian Yu, Xiaobin Chang, Yi-Zhe Song, Tao Xiang, and Timothy M Hospedales.\n2017. The devil is in the middle: Exploiting mid-level representations for cross-\ndomain instance matching. arXiv preprint arXiv:1711.08106(2017).\n[48] Tianyu Zhang, Longhui Wei, Lingxi Xie, Zijie Zhuang, Yongfei Zhang, Bo Li,\nand Qi Tian. 2021. Spatiotemporal Transformer for Video-based Person Re-\nidentification. arXiv preprint arXiv:2103.16469(2021).\n[49] Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Xin Jin, and Zhibo Chen. 2020.\nRelation-Aware Global Attention for Person Re-identification. In CVPR. 3186â€“\n3195.\n[50] Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, and Lei Zhang. 2020.\nSuppress and Balance: A Simple Gated Network for Salient Object Detection. In\nECCV.\n[51] Feng Zheng, Cheng Deng, Xing Sun, Xinyang Jiang, Xiaowei Guo, Zongqiao\nYu, Feiyue Huang, and Rongrong Ji. 2019. Pyramidal person re-identification via\nmulti-loss dynamic training. In CVPR. 8514â€“8522.\n[52] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.\n2015. Scalable person re-identification: A benchmark. In ICCV. 1116â€“1124.\nChengdu â€™21, Oct. 20â€“24, 2021, Chengdu, China Guowen and Pingping, et al.\n[53] Meng Zheng, Srikrishna Karanam, Ziyan Wu, and Richard J Radke. 2019. Re-\nidentification with consistent attentive siamese networks. In CVPR. 5735â€“5744.\n[54] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao\nWang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al . 2020. Re-\nthinking Semantic Segmentation from a Sequence-to-Sequence Perspective with\nTransformers. arXiv preprint arXiv:2012.15840(2020).\n[55] Zhedong Zheng, Xiaodong Yang, Zhiding Yu, Liang Zheng, Yi Yang, and\nJan Kautz. 2019. Joint discriminative and generative learning for person re-\nidentification. In CVPR. 2138â€“2147.\n[56] Zhedong Zheng, Liang Zheng, and Yi Yang. 2017. Unlabeled samples generated\nby gan improve the person re-identification baseline in vitro. InICCV. 3754â€“3762.\n[57] Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li. 2017. Re-ranking person\nre-identification with k-reciprocal encoding. In CVPR. 1318â€“1327.\n[58] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba.\n2016. Learning deep features for discriminative localization. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition. 2921â€“2929.\n[59] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. 2019. Omni-\nscale feature learning for person re-identification. In ICCV. 3702â€“3712.\n[60] Sanping Zhou, Fei Wang, Zeyi Huang, and Jinjun Wang. 2019. Discriminative\nfeature learning with consistent attention regularization for person re-identification.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision.\n8040â€“8049.\n[61] Kuan Zhu, Haiyun Guo, Zhiwei Liu, Ming Tang, and Jinqiao Wang. 2020. Identity-\nGuided Human Semantic Parsing for Person Re-Identification. arXiv preprint\narXiv:2007.13467 (2020).\n[62] Kuan Zhu, Haiyun Guo, Shiliang Zhang, Yaowei Wang, Gaopan Huang, Honglin\nQiao, Jing Liu, Jinqiao Wang, and Ming Tang. 2021. AAformer: Auto-Aligned\nTransformer for Person Re-Identification. arXiv preprint arXiv:2104.00921\n(2021).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.807174026966095
    },
    {
      "name": "Granularity",
      "score": 0.7463096380233765
    },
    {
      "name": "Discriminative model",
      "score": 0.721428394317627
    },
    {
      "name": "Transformer",
      "score": 0.7100419998168945
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6588842868804932
    },
    {
      "name": "Artificial intelligence",
      "score": 0.564387321472168
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.46746185421943665
    },
    {
      "name": "Feature learning",
      "score": 0.4527010917663574
    },
    {
      "name": "Machine learning",
      "score": 0.4344910681247711
    },
    {
      "name": "Data mining",
      "score": 0.35251888632774353
    },
    {
      "name": "Engineering",
      "score": 0.07257527112960815
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}