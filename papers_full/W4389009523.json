{
  "title": "LAD: Language Models as Data for Zero-Shot Dialog",
  "url": "https://openalex.org/W4389009523",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2890010146",
      "name": "Shikib Mehri",
      "affiliations": [
        "Carnegie Mellon University",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2067577924",
      "name": "Yasemin Altun",
      "affiliations": [
        "Google (United States)",
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A1479332637",
      "name": "Maxine Eskenazi",
      "affiliations": [
        "Google (United States)",
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3167376363",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W36434594",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W3167539758",
    "https://openalex.org/W3034861927",
    "https://openalex.org/W3024509506",
    "https://openalex.org/W3172697791",
    "https://openalex.org/W2106547558",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W3167676721",
    "https://openalex.org/W3162734203",
    "https://openalex.org/W62710299",
    "https://openalex.org/W2745680449",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W2571927164",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3034238471",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2168490009",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W3206345746",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2921312604",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2121325257",
    "https://openalex.org/W3094447228",
    "https://openalex.org/W2963857397",
    "https://openalex.org/W3153451655",
    "https://openalex.org/W2062175565",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W3091355780",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2971159908",
    "https://openalex.org/W1572987273",
    "https://openalex.org/W3167712016",
    "https://openalex.org/W2997771882"
  ],
  "abstract": "To facilitate zero-shot generalization in task-oriented dialog, this paper proposes Language Models as Data (LAD). LAD is a paradigm for creating diverse and accurate synthetic data which conveys the necessary structural constraints and can be used to train a downstream neural dialog model. LAD leverages GPT-3 to induce linguistic diversity. LAD achieves significant performance gains in zero-shot settings on intent prediction (+15%), slot filling (+31.4 F-1) and next action prediction (+10 F-1). Furthermore, an interactive human evaluation shows that training with LAD is competitive with training on human dialogs.",
  "full_text": "Proceedings of the SIGdial 2022 Conference, pages 595–604\nHeriot-Watt University, Edinburgh, UK. 07-09, September, 2022 ©2022 Association for Computational Linguistics\n595\nLAD : Language Models as Data for Zero-Shot Dialog\nShikib Mehri♠ Yasemin Altun♢ Maxine Eskenazi♠\n♠ Carnegie Mellon University ♢ Google\namehri@cs.cmu.edu,altun@google.com,max@cs.cmu.edu\nAbstract\nTo facilitate zero-shot generalization in task-\noriented dialog, this paper proposes Language\nModels as Data (LAD ). LAD is a paradigm for\ncreating diverse and accurate synthetic data\nwhich conveys the necessary structural con-\nstraints and can be used to train a downstream\nneural dialog model. LAD leverages GPT-3 to\ninduce linguistic diversity. LAD achieves sig-\nnificant performance gains in zero-shot settings\non intent prediction (+15%), slot filling (+31.4\nF-1) and next action prediction ( +11 F-1 ).\nFurthermore, an interactive human evaluation\nshows that training with LAD is competitive\nwith training on human dialogs. LAD is open-\nsourced, with the code and data available at\nhttps://github.com/Shikib/lad.\n1 Introduction\nA long-standing goal of dialog research is to de-\nvelop mechanisms for flexibly adapting dialog sys-\ntems to new domains and tasks (Rastogi et al., 2020;\nMosig et al., 2020). While the advent of large-scale\npre-training (Devlin et al., 2018; Liu et al., 2019b;\nZhang et al., 2019) has brought about significant\nprogress in few-shot and zero-shot generalization\nacross many different problems in Natural Lan-\nguage Processing (Brown et al., 2020; Wei et al.,\n2021), zero-shot generalization in task-oriented\ndialog remains elusive. A likely reason for this\ndiscrepancy is that dialog models require signifi-\ncant data because they need to learn task-specific\nstructural constraints, such as the domain ontol-\nogy and the dialog policy. While large language\nmodels (e.g., GPT-3) exhibit strong language un-\nderstanding and generation abilities (Brown et al.,\n2020), they have no a priori knowledge of the\nstructural constraints implied by a specific (unseen)\nproblem setting (e.g., relevant intents, dialog pol-\nicy, etc.). As such, in order to adapt a pre-trained\nLM for task-oriented dialog, it is necessary to im-\npose structural constraintson the unstructured\nFigure 1: Prompting must convey the structural con-\nstraints through a natural language prompt. In contrast,\nLAD uses large LMs to induce diversity in a synthetic\ndataset. As such, LAD conveys structural constraints\nthrough both the synthetic data and the inductive biases\nin the downstream problem-specific models.\nrepresentation space of a pre-trained model. Fine-\ntuning moderately-sized language models (LMs)\n(e.g., BERT) with well-motivated inductive biases\n(Mitchell, 1980) facilitates sample-efficient learn-\ning of the structural constraints (Peng et al., 2020;\nHenderson and Vuli´c, 2020; Mehri and Eskenazi,\n2021b). However, fine-tuning can be impractical\n(e.g., in academic settings) with large LMs (e.g.,\nGPT-3) due to the cost, computational power and\nimmutable architectures. To this end, this paper\naims to address the following: ‘How can we lever-\nage the strong language understanding and gener-\nation abilities of large LMs to facilitate zero-shot\ngeneralization in task-oriented dialog?’\nGiven the in-context meta-learning abilities of\nlarge LMs (Brown et al., 2020), prior work has\nexplored prompt-engineering or prompt-tuning\n(Reynolds and McDonell, 2021; Lester et al., 2021;\nMadotto et al., 2021). Well-designed prompts can\nconvey the necessary structural constraints. How-\n596\never, it is challenging to express complex con-\nstraints (e.g., a dialog policy) in natural language.\nPrompting also precludes inductive biases in the\nmodel (architecture, training algorithm, etc.) and\nover-relies on the meta-learning abilities of large\nLMs. As such, there is a tradeoff between prompt-\ning large LMs (i.e., generalizable NLU and NLG)\nand fine-tuning smaller LMs (i.e., problem-specific\ninductive biases, efficiency). A potential interpre-\ntation for the strength of large LMs is that they\nlearn the distributional structure of language (Har-\nris, 1954) by observing web-scale data (Sinha et al.,\n2021). Motivated by this interpretation, this paper\nproposes Language Models as Data (LAD ).\nLAD is a novel paradigm in which large LMs\nare used in a zero-shot domain-agnostic manner to\ninduce linguistic diversity in synthetic data. Given\na minimal expression1 of the structural constraints\n(henceforth referred to as a schema), LAD (1) cre-\nates a seed synthetic dataset using domain-agnostic\nalgorithms, (2) leverages large LMs to reformu-\nlate utterances, and (3) validates the resulting data\nto ensure adherence to the schema. The resulting\nsynthetic data, which is sufficiently diverse and ex-\npresses the necessary structural constraints, can\nbe used to train neural dialog models. In contrast\nto prompting, LAD facilitates zero-shot generaliza-\ntion by (1) leveraging the sophisticated abilities of\nlarge LMs (knowledge of the distributional struc-\nture of language) to induce linguistic diversity in\nthe synthetic data while (2) maintaining inductive\nbiases (motivated by the structural constraints) in\nthe problem-specific model architectures.\nThe challenge of creating synthetic data that is\nindistinguishable from human-annotated data, both\nin its expression of structural constraints and in its\ndiversity, is highly impractical (Lin et al., 2021;\nFeng et al., 2021). Instead, the goal of this work is\nto create synthetic data that is sufficient to train a\nsample-efficient and robust model. Therefore, the\nclaim of this paper is that LAD can create synthetic\ndata, conditioned on a minimal expression of struc-\ntural constraints (i.e., a schema), that can be used\nto train robust and sample-efficient neural models\nand induce performance gains in zero-shot settings.\nTo validate this claim, LAD is applied to three\nproblems in dialog: intent prediction, slot filling\nand next action prediction. Next action prediction\nis particularly difficult in zero-shot settings since\n1A minimal expression can be defined as the smallest\namount of data necessary to express a structural constraint.\nFor example, one utterance to define an intent class.\nthe structural constraints include the dialog pol-\nicy. LAD demonstrates significant gains across five\ndatasets ( +10 to +30 improvements on F-1 and\naccuracy) in zero-shot settings when evaluating\non human-annotated corpora. To further validate\nthe efficacy of LAD , an interactive evaluation with\nhumans (over 1600 dialogs) is performed. The re-\nsults of this interactive evaluation suggest that LAD\ncan yield performance comparable to training on\nhuman dialogs. The claim of this paper is vali-\ndated empirically across multiple datasets. LAD is\nshown to generate diverse and accurate synthetic\ndata, which is subsequently used to train neural di-\nalog models and facilitate zero-shot generalization.\n2 Definitions\nZero-shot generalization can be conceptualized as\nimposing structural constraints on the unstruc-\ntured representation space of a pre-trained model,\nusing a given schema (i.e., minimal expression).\nWe begin with a neural network, M, with gen-\neral language understanding abilities and limited\nknowledge of task-oriented dialog (e.g., BERT (De-\nvlin et al., 2018)). The necessary structural con-\nstraints that M must learn are implied by the target\ndialog setting, i.e., the problem (e.g., next action\nprediction), the domain (e.g., restaurants) and the\ntask (e.g., restaurant reservation). These structural\nconstraints conceptually define the desired proper-\nties for the representations of M, i.e., what must\nbe learned by M. In a full-shot setting, the con-\nstraints are conveyed by a human-annotated dataset\nand thereby learned through supervised learning.\nIn contrast, the goal in zero-shot generalization is\nto learn these structural constraints from a minimal\nexpression, i.e., a schema. The following sections\nformally define structural constraints and schemas.\nThroughout this paper, zero-shot refers to a set-\nting wherein the only human-annotated data is the\nschema. Since a schema is a minimal expression of\nthe necessary structural constraints, we argue that\nit is impossible to use less data, without making\nassumptions about the prior knowledge of a pre-\ntrained model. Such assumptions would limit the\ngenerality of a method for zero-shot generalization.\n2.1 Structural Constraints\nTo effectively adapt a model, particularly in zero-\nshot settings, it is imperative to define what the\nmodel must learn. Structural constraints conceptu-\nalize the desired properties for the representations\n597\nof a model M. Understanding these structural con-\nstraints allows us to design an effective paradigm\nto facilitate zero-shot generalization. Concretely,\nknowledge of the structural constraints influences\n(1) the inductive biases (Mitchell, 1980) in the\nmodel architecture, (2) the design of the schema,\nand (3) the algorithms used to create synthetic data.\nIntent prediction, for example, is the problem\nof classifying an utterance u ∈ Uto an intent\ni ∈ I. An intent prediction model MI must learn\nto produce similar representations MI(u) for all\nutterances that have the same intent. Learning this\nstructural constraint is equivalent to transforming\nthe unstructured representation space of M to the\nstructured output space (i.e., the intent classes).\nIn the problem of slot filling, for a given utter-\nance u = {w1, w2, . . . , wn} and a slot key s ∈ S,\nwe must predict the corresponding slot value for\ns. The value will either be a contiguous span from\nu, wi:i+k, or none. A slot filling model MS must\nlearn two sets of structural constraints. First, the\nrepresentation of u (or the contextual representa-\ntion of w ∈ u) must follow the structural con-\nstraints of intent prediction. Second, each slot\nvalue representation MS(wi:i+k) should be similar\nto other values for the slot s. These two constraints\nimpose structure on both the utterance-level and\nthe span-level representations of MS.\nThe structural constraints of intent prediction\nand slot filling are straightforward and are often\nlearned by a linear layer in supervised settings\n(Casanueva et al., 2020; Mehri et al., 2020). The\nconstraints for the problem of next action pre-\ndiction are more complex. Next action predic-\ntion is the problem of predicting the next system\naction a ∈ Aconditioned on the dialog history\nu1, u2, . . . , un according to some dialog policy.\nGiven the intents and slots in the dialog history,\nID = {i1, i2, . . . , im} and SD = {s1, s2, . . . , sk},\nthe dialog policy can be expressed as a function of\nthese intents and slots, a = policy(ID, SD). As\nsuch a next action prediction modelMA must learn\n(1) the structural constraints of intent prediction,\n(2) of slot filling and (3) the mapping defined by\nthe policy function. The complexity of third con-\nstraint led to the schema-guided paradigm (Mehri\nand Eskenazi, 2021b), wherein the policy is explic-\nitly expressed rather than being learned implicitly.\n2.2 Schema\nWhile structural constraints conceptualize what a\nmodel M must learn, the schema is a minimal\nexpression of these constraints. Imagine that our\nobjective is to train a human (i.e., M with human-\nlevel language understanding and reasoning abil-\nities) to perform task-oriented dialog. Structural\nconstraints define what the human must learn. The\nschema is the minimum amount of information\nneeded, for the human to learn the necessary struc-\ntural constraints, without prior knowledge.\nFor intent prediction, we define the schema to\nbe a single utterance u for each intent i ∈ I. Slot\nfilling similarly relies on one utterance u for each\nslot type s ∈ S. However, this one utterance only\nconveys the first structural constraint of slot filling.\nTo ensure thatMS can learn meaningful span-level\nrepresentations, the schema for slot filling also in-\ncludes multiple2 examples of values for each slot.\nNext action prediction has three constraints.\nThe first two constraints are equivalent to those\nof intent prediction and slot filling. As such, the\nschema includes both (1) one utterance for each in-\ntent and (2) a set of slot values for each slot type. To\nexpress the structural constraints of the dialog pol-\nicy, we leverage the graph-based representations of\nthe task-specific dialog policy proposed by Mosig\net al. (2020) and Mehri and Eskenazi (2021b).\n3 LAD : Language Models as Data\nDespite exhibiting strong language understanding\nand generation abilities (Brown et al., 2020), large\nLMs have no a priori knowledge of the structural\nconstraints of task-oriented dialog. Furthermore,\nimposing the necessary structural constraints on\nlarge LMs is impractical due to (1) the difficulty\nof fine-tuning (cost, computation, immutable ar-\nchitectures) and (2) the limitations of natural lan-\nguage prompts. As such, Language Models as\nData (LAD ) uses GPT-3 (Brown et al., 2020) to\ngenerate diverse synthetic data that express the\nnecessary task-specific structural constraints and\ncan therefore be used to train neural dialog models.\nLAD is a framework for inducing zero-shot gen-\neralization in task-oriented dialog by creating di-\nverse and accurate synthetic data. LAD , visualized\nin Figure 2, is a three step process: (§3.1) domain-\nagnostic algorithms generate a seed dataset from\na schema, (§3.2) GPT-3 reformulates utterances in\n2While the number of slot value examples could potentially\nreduced to 1, up to 20 are used in this paper.\n598\nFigure 2: Visualization of LAD . (1) Domain-agnostic algorithms use the schema to create a seed dataset which\nconveys the necessary structural constraints. (2) Large LMs reformulate individual utterances to add linguistic\ndiversity. (3) Validation heuristics are used to ensure adherence to the schema.\norder to induce linguistic diversity, (§3.3) heuris-\ntics are used to validate the reformulated data to\nensure adherence to the schema. LAD facilitates\nzero-shot generalization by explicitly leveraging\nthe strengths of large LMs (knowledge of the distri-\nbutional structure of language) without sacrificing\nthe inductive biases (motivated by structural con-\nstraints) in the downstream neural dialog models.\n3.1 Seed Data Creation\nLAD begins by creating seed synthetic data from a\ngiven schema. This is a domain-agnostic process\nthat aims to generate synthetic data which accu-\nrately convey the necessary structural constraints.\nFor intent prediction , the schema consists\nof one utterance for each intent class (sampled\nfrom the original corpus) and is used as the seed\ndataset. For slot filling, the schema consists of one\nmanually-written template utterance and multiple\nslot values for each slot type. To construct the seed\ndata: (1) begin with the utterance templates from\nthe schema (e.g., ‘My first name is {first_name}’),\n(2) exhaustively combine template utterances to\nensure coverage of slot type combinations, and (3)\nfill slot values by sampling from the schema.\nThe relative complexity of the structural con-\nstraints for next action prediction, particularly the\ndialog policy, necessitates a more sophisticated al-\ngorithm for generating the seed data. In order to\navoid over-fitting and to ensure that the structural\nconstraints are effectively learned by the model, it\nis imperative that the synthetic data produced by\nLAD be diverse and realistic. While linguistic di-\nversity is induced through the reformulation with\nGPT-3, the synthetic dialogs created for next action\nprediction must also exhibit diversity of user be-\nhavior. The dialog policy expressed by the schema\ndeterministically defines the system behavior. How-\never, users should be able to deviate from the pol-\nicy, e.g. by providing information out of turn. To\naccount for this, Algorithm 1 in the Appendix gen-\nerates a dialog by traversing the dialog policy graph\nand randomly combining multiple template utter-\nances (e.g., ‘System: What is your name? User:\nMy name is John. My phone number is... ’).\n3.2 Reformulation\nTo ensure that downstream neural dialog models\ncan effectively learn the structural constraints, it\nis imperative that the synthetic data is sufficiently\ndiverse. The seed synthetic data is formulaic and\nartificial: (1) there is a single template utterance for\neach user action and (2) when multiple user actions\nare combined they are simply concatenated. As\nsuch, the goal of the reformulation step is two-fold:\n(1) to induce linguistic diversity and (2) to rephrase\nconcatenations of disjoint template utterances (‘My\nname is Sarah. I want to plan a party. The day\n599\nshould be Sunday’) into a natural utterance (‘I’m\nSarah and I’d like to plan a party for Sunday. ’).\nTo reformulate utterances in a domain-agnostic\nmanner, LAD leverages the in-context meta-\nlearning abilities of GPT-3 (Brown et al., 2020).\nThrough manual experimentation in the OpenAI\nPlayground3, an appropriate prompt is constructed.\nThe prompt begins with an instruction ( ‘Given a\nset of sentences, generate 5 natural utterances that\nconvey the same meaning. ’) and includes six exam-\nples (details can be found in the Appendix).\nRather than producing a single reformulation of\nthe input, the chosen prompt instructs GPT-3 to\ngenerate five utterances. Through the examples\nprovided in the prompt, GPT-3 learns that it should\nproduce five diverse reformulations. As such, lin-\nguistic diversity is induced through both the decod-\ning algorithm and the six examples in the prompt.\n3.2.1 Scalability\nThe cost of the GPT-3 API is approximately $0.05\nUSD per reformulation. In order to generate a sub-\nstantial amount of synthetic data without incurring\nsignificant costs, the reformulation step of LAD\nmust be performed in a scalable manner. The seed\nutterances are grouped by their intents and slot keys\n(e.g., ‘name;date;time’, ‘name;date’, ‘date;time’).\nA subset of utterances in each group is reformu-\nlated. These reformulated utterances are used as\ntemplates and the slot values are randomly replaced.\nIn this manner, the cost scales with respect to the\nnumber of distinct intent/slot combinations rather\nthan the desired size of the synthetic dataset.\n3.3 Validation\nThe seed data will always adhere to the schema and\ntherefore accurately convey the necessary struc-\ntural constraints. However, the reformulated utter-\nances may not be accurate. GPT-3 may modify the\nintended meaning of an input utterance, for exam-\nple by ignoring certain slot values. To ensure that\nthe structural constraints are accurately expressed\nin the final dataset, the reformulation step of LAD\nfilters out erroneous reformulations. For slot filling\nand next action prediction, this is done by ensuring\nthat all of the slot values present in the original\nutterance (from the seed dataset) are also present in\nthe reformulated utterances (produced by GPT-3).\n3https://beta.openai.com/playground\nOriginal Dataset Seed LAD Cost (USD)\nIntent Prediction\nHWU64 (8955) 64 800 $19\nCLINC150 (15000) 150 1664 $43\nBanking77 (8633) 77 848 $25\nSlot Filling\nRestaurant8k (8633) 85 32000 $89\nNext Action Prediction\nSTAR (1200) 24000 22327 $226\nTable 1: Statistics for the synthetic datasets created by\nLAD . This table lists the size of the original dataset, the\nseed dataset and the final synthetic dataset produced by\nLAD . The last column indicates the approximate cost of\nusing GPT-3 for each of the datasets.\n3.4 Dataset Statistics\nLAD is evaluated on five different datasets. For in-\ntent prediction, Banking77 (Casanueva et al., 2020),\nCLINC150 (Larson et al., 2019), and HWU64 (Liu\net al., 2019a) are used. For slot filling, Restau-\nrant8k (Coope et al., 2020) is used. For next ac-\ntion prediction, STAR (Mosig et al., 2020) is used.\nGiven a human-annotated corpus, a schema is cre-\nated to express the necessary constraints. LAD is\nthen leveraged to create a synthetic dataset condi-\ntioned on the schema. Table 1 describes the size\nand creation cost of each of the synthetic datasets.\n4 Experiments\nThis paper claims that LAD can use a schema to\ncreate a sufficiently diverse and accurate synthetic\ndataset, which can be used to train neural dialog\nmodels and facilitate performance gains in zero-\nshot settings. To validate this claim, experiments\nare carried out on intent prediction, slot filling and\nnext action prediction across five datasets.\nFor each problem, an appropriate model from\nprior work is identified. The chosen models (1)\nexhibit strong zero-shot and few-shot generaliz-\nability, and (2) are open-source. Though LAD is\nnot guaranteed to produce perfectly accurate and\ndiverse data, the inductive biases in the chosen\nmodels make them more robust to potential errors\nand limitations in the synthetic data.\n4.1 Intent Prediction\nCONV BERT+Example-Driven+Observers\n(CBEO ) (Mehri and Eric, 2021) is used for intent\nprediction. CBEO learns to predict utterance\n600\nModel (Training Data) BANKING 77 CLINC 150 HWU 64\nCBEO ( ONE -SHOT ) 31.36 53.96 43.12\nCBEO ( ONE -SHOT + LAD ) 51.17 68.11 65.50\nCBEO ( FULL -SHOT ) 93.83 97.31 93.03\nTable 2: Experimental results on intent prediction. We report the accuracy of training CBEO on (1) one utter-\nance/intent (i.e., the seed data) and (2) the synthetic data produced by LAD . For reference, we also show the results\nreported by Mehri and Eric (2021) obtained with full human-annotated training datasets.\nModel F-1\nZero-Shot Results\nCONVEX (HENDERSON AND VULI ´C, 2020) 5.2\nCOACH +TR (LIU ET AL ., 2020) 10.7\nGENSF (MEHRI AND ESKENAZI , 2021 A) 19.5\nGENSF + LAD 50.9\nNon Zero-Shot Results\nGENSF (64 UTTERANCES ) 72.2\nGENSF (8633 UTTERANCES ) 96.1\nTable 3: Experimental results on the Restaurant8k cor-\npus. We compare GENSF + LAD with zero-shot results\nreported by prior work. For reference, we also show the\nperformance of models (reported by prior work) when\ntrained in few-shot and full-shot settings.\nintents by explicitly comparing to a set of ex-\namples. Predicting intents through an explicit\nnon-parametric comparison to examples is an\ninductive bias that facilitates sample-efficient\nlearning of the structural constraints.\nThe experimental results shown in Table 2\ndemonstrate that the synthetic data produced by\nLAD significantly increase performance on one-\nshot4 intent prediction. LAD facilitates 15%+ accu-\nracy improvement across all three intent prediction\ndatasets. For intent prediction, LAD does not use\nany heuristics during the creation of the seed data\nor during the validation step. As such, these im-\nprovements can be attributed to the reformulation\nstep, which leverages the prompt-driven generation\nabilities of GPT-3 (Brown et al., 2020).\n4.2 Slot Filling\nFor slot filling, experiments are carried out with\nGENSF (Mehri and Eskenazi, 2021a) which cur-\n4This setting is characterized as one-shot since the utter-\nances in the schema are sampled from the respective dataset.\nModel F-1\nZero-Shot Results\nBERT+S (MOSIG ET AL ., 2020) 28.12\nSAM (MEHRI AND ESKENAZI , 2021 B) 53.31\nSAM + LAD 64.36\nFull-Shot Results\nSAM (MEHRI AND ESKENAZI , 2021 B) 70.38\nTable 4: Experimental results on the STAR corpus.\nSAM + LAD is compared with zero-shot results re-\nported by prior work. For reference, the performance of\nSAM when trained on the full corpus is also shown.\nrently has SoTA results on the Restaurant8k corpus\n(Coope et al., 2020), in both zero-shot and full-\nshot settings. GENSF reformulates slot filling as\nresponse generation in order to better leverage the\ncapabilities of DialoGPT (Zhang et al., 2019).\nAs shown in Table 3, GENSF + LAD achieves\na +31.4 F-1 improvement over GENSF on the test\nset of Restaurant8k, without observing any exam-\nples from the corpus. GENSF + LAD learns to\ndetect slots in the restaurant domain given only the\nschema, which consists of (1) a single manually\nwritten utterance for each slot type and (2) a collec-\ntion of up to 20 slot values for each slot type. This\nsignificant performance improvement in zero-shot\ngeneralization validates the claim of this paper for\nthe problem of slot filling. LAD is able to create\nsynthetic data which effectively teaches GENSF\nthe necessary structural constraints.\nHowever, as shown by Mehri and Eskenazi\n(2021a), GENSF achieves a 72.2 F-1 score by only\nobserving 64 human-written examples. Despite the\nrelative success of LAD in zero-shot settings, there\nremains significant room for improvement.\n601\nModel (Training Data) C OMPLETE % A SKS ALL % A VOIDS REDUNDANCY %\nSAM ( ZERO -SHOT ) 98.02 76.15 78.90\nSAM ( FULL -SHOT ) 98.31 75.69 80.65\nSAM + LAD 98.52 78.39 79.13\nTable 5: Results of the interactive human evaluation. We compare three models: (1) SAM ( ZERO -SHOT ), (2) SAM\n(FULL -SHOT ) and (3) SAM + LAD . The three columns correspond to the three post-dialog questions: (1) task\ncompletion, (2) asking all necessary information and (3) avoiding redundancy. Results in boldface are statistically\nsignificant by one-tailed t-test (p <0.05).\n4.3 Next Action Prediction\nNext action prediction is particularly challenging\ndue to the complexity of the structural constraints.\nIn addition to the constraints of intent prediction\nand slot filling, next action prediction models must\nalso learn to follow thedialog policy. SAM (Mehri\nand Eskenazi, 2021b) learns to predict the system\naction by attending to a graph-based representation\nof the dialog policy. Explicitly attending to the\ndialog policy is an inductive bias that facilitates\nzero-shot generalization to unseen tasks.\nTable 4 shows the results for three models.\nBERT+S (Mosig et al., 2020) trains a BERT model\nto attend to a rudimentary graph-based representa-\ntion of the dialog policy. SAM (Mehri and Eske-\nnazi, 2021b) improves the model architecture and\nintroduces more expressive policy graphs. These\ntwo models are trained on the STAR corpus, which\nincludes 24 different tasks and 24 different pol-\nicy graphs. The zero-shot results are obtained by\ntraining on n − 1 tasks (i.e., 23) and evaluating on\nthe remaining task, repeated 24 times. In contrast,\nSAM + LAD observes no human-written dialogs\nwhatsoever. Instead, SAM+ LAD is trained only on\nthe synthetic dialogs produced by LAD .\nIn the zero-shot setting,SAM + LAD achieves an\n+11.05 F-1 improvement over SAM. Furthermore,\nthis result is only 6.02 points below the full-shot\nresults of SAM . This significant gain further vali-\ndates the claim of this paper. SAM + LAD learns\nthe necessary structural constraints using only the\nsynthetic data produced by LAD .\n4.4 Interactive Human Evaluation\nSAM + LAD achieves strong zero-shot results on\nthe STAR corpus, especially relative to the per-\nformance of SAM ( FULL -SHOT ). This leads us\nto question the performance gap between these\ntwo models. Is the full-shot model better at next\naction prediction, or is it just better at modelling\nartifacts in the STAR corpus? STAR is known to\nhave some degree of inconsistency with the policy\ngraphs (Mosig et al., 2020). Furthermore, static\nevaluation is not necessarily reflective of the per-\nformance of a model in real settings. Because of\nvariable user behavior, there may be a distribution\nshift between the STAR corpus and interactive set-\ntings. To this end, we perform an interactive human\nevaluation using Amazon Mechanical Turk (AMT).\nThree models are evaluated: (1) SAM ( ZERO -\nSHOT ), (2) SAM ( FULL -SHOT ) and (3) SAM +\nLAD . Ten scenarios are defined, each of which\nconsists of an objective (e.g, ‘You want to plan a\nparty’) and slot values (e.g., Name: Kevin, Date:\nSunday, Num Guests: 85). An AMT worker is\ninstructed to interact with a dialog system accord-\ning to the provided scenario. Upon completion of\nthe dialog, three questions are answered:\n1. Did the system successfully complete the dialog?\n2. Did the system ask for all of the necessary information?\n3. Did the system ask for information that you had already\nprovided it?\nThe instructions (see Appendix) tell the worker\nto interact naturally (e.g., by providing informa-\ntion out of turn). Detailed instructions, including\nexamples and counter-examples, are provided for\nthe three post-dialog questions. Pre-screening is\nperformed to ensure that AMT workers read and un-\nderstood these instructions. During pre-screening,\nthe worker must answer the post-dialog questions\ngiven two completed dialogs and the correspond-\ning scenarios. Workers with a score of at least\n5/6 qualify to participate in the interactive eval-\nuation (45% of workers pass the pre-screening).\nPre-screening is paid $0.75USD, regardless of the\nresult. Each HIT (Human Intelligence Task) of the\ninteractive evaluation includes five scenarios and\npays $3.25USD (approx. 10 minutes). A post-hoc\nquality check is performed to remove erroneous\n602\nannotations. Simple heuristics are constructed to\npredict the post-dialog answers and any discrep-\nancies with the annotations are manually verified.\nIf an error is identified through manual validation,\nthe annotation is removed. This form of valida-\ntion is a necessary alternative to outlier detection\nor measures of inter-annotator agreement, since in-\nteractive dialogs are independent thereby making\nstandard measures of data quality unsuitable.\n1628 dialogs were collected, with at least 500\nfor each system. The results, shown in Table 5,\ndemonstrate that the performance of all three mod-\nels is fairly similar in interactive settings. For the\nsecond post-dialog question, SAM+ LAD asks for\nall of the necessary slots +2.7% more often. As-\nsuming that the number of observations is equal to\nthe total number of turns, this result is statistically\nsignificant (p <0.05) by one-tailed t-test.\nBoth SAM ( FULL -SHOT ) and SAM ( ZERO -\nSHOT ) are trained on human dialogs, though the\nlatter does not observe data from the target task.\nIn contrast, SAM + LAD is trained only on syn-\nthetic data produced by LAD . The comparable\nperformance of SAM ( ZERO -SHOT ) and SAM\n(FULL -SHOT ) is noteworthy and can potentially\nbe explained by two facts: (1) the interactive di-\nalogs are sampled from a different distribution\n(e.g., more informal, typos, more slots per utter-\nance) from the STAR corpus, making the evalua-\ntion equally difficult for both systems, (2) SAM\n(ZERO -SHOT ) has observed dialogs from the do-\nmain (e.g., seenbank-balance when evaluating\non bank-fraud-report). Despite not observ-\ning any human dialogs, in interactive settingsSAM\n+ LAD attains zero-shot performance comparable\nto training on human dialogs from the STAR cor-\npus. Though there remains significant room for\nimprovement, the results of this interactive human\nevaluation demonstrate the efficacy of LAD . By\nleveraging the strengths of large language models\nto induce linguistic diversity, LAD produces syn-\nthetic data that effectively conveys the necessary\nstructural constraints and facilitates zero-shot gen-\neralization, even in challenging interactive settings.\n5 Related Work\n5.1 User Simulators for Task-Oriented Dialog\nThe use of synthetic data in task-oriented dialog\nis a long-standing approach. Early dialog research\nleveraged user simulators for evaluation and opti-\nmization (Eckert et al., 1997; Scheffler and Young,\n2000; Schatzmann et al., 2006). Schatzmann et al.\n(2007) propose a probabilistic agenda-based user\nsimulator for bootstrapping a POMDB dialog sys-\ntem, demonstrating reasonable task completion\nrates. Georgila et al. (2006) train an n-gram user\nsimulator which models both ASR and understand-\ning errors. González et al. (2010) explicitly model\nuser cooperativeness in a statistical user simulator.\nLi et al. (2016) propose an agenda-based user\nsimulator for training dialog policies with RL.\nCrook and Marin (2017) train a sequence-to-\nsequence model for user simulation. Kreyssig\net al. (2018) introduce the neural user simulator\n(NUS), which trains a sequence-to-sequence net-\nwork conditioned on user goals and the dialog his-\ntory, outperforming existing methods on an inter-\nactive evaluation. Shi et al. (2019) carry out a\ncomprehensive analysis of six different user sim-\nulators, with different dialog planning and gen-\neration methods. A key takeaway of this analy-\nsis is using agenda-based simulators to train RL\nsystems generally results in higher performance\non human evaluation. Lin et al. (2021) propose a\ndomain-indepdendent transformer-based user sim-\nulator (TUS). The feature representations of TUS\nare domain-independent, thereby facilitating learn-\ning of cross-domain user behavior. TUS is trained\non MultiWOZ (Budzianowski et al., 2018) and can\neffectively transfer to unseen domains.\nLAD can be characterized as an agenda-based\nsimulator, wherein the schema describes the ontol-\nogy and the policy. The core novelty of LAD in\nthe context of prior work is three-fold: (1) large\nLMs to induce linguistic diversity, (2) zero-shot\ndomain-agnostic synthetic data creation, and (3)\nthe schema as a standardized expression of struc-\ntural constraints. LAD can potentially be further im-\nproved by incorporating strategies from prior work,\nsuch as modelling cooperativeness (González et al.,\n2010) or ASR errors (Georgila et al., 2006).\n5.2 Using Large Language Models\nLarge language models (Brown et al., 2020;\nChowdhery et al., 2022) exhibit strong language\nunderstanding, generation and reasoning abilities.\nPrompting is the dominant paradigm for leverag-\ning large LMs for various downstream problems\n(Reynolds and McDonell, 2021; Lester et al., 2021).\nMadotto et al. (2021) demonstrate the efficacy of\nfew-shot prompting for both open-domain and task-\noriented dialog, with a focus on response genera-\n603\ntion and conversational parsing.\nSeveral papers have used GPT-3 to generate syn-\nthetic data (Yoo et al., 2021; Wang et al., 2021).\nThese approaches rely on GPT-3 to generate the\nlabels and are not suitable for task-oriented dialog.\nTo our knowledge, LAD is the first paper to lever-\nage large LMs to reformulate utterances, in order\nto create synthetic data for task-oriented dialog.\n6 Conclusion\nIn an effort to leverage the abilities of large LMs to\nfacilitate zero-shot generalization in task-oriented\ndialog, this paper introduces LAD . LAD creates\ndiverse and accurate synthetic data, in order to con-\nvey the necessary setting-specific structural con-\nstraints to neural dialog models. LAD achieves\nsignificant performance gains on zero-shot intent\nprediction, slot filling and next action prediction\nacross five datasets. Furthermore, LAD is shown to\nperform competitively in interactive human evalua-\ntion, without observing human-annotated data.\n7 Acknowledgements\nThis work was funded by a Google Research Col-\nlabs grant.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Inigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018. Multiwoz–a\nlarge-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling. arXiv preprint\narXiv:1810.00278.\nInigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efficient\nintent detection with dual sentence encoders. arXiv\npreprint arXiv:2003.04807.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nSam Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli ´c,\nand Matthew Henderson. 2020. Span-convert: Few-\nshot span extraction for dialog with pretrained\nconversational representations. arXiv preprint\narXiv:2005.08866.\nPaul A Crook and Alex Marin. 2017. Sequence to\nsequence modeling for user simulation in dialog sys-\ntems. In INTERSPEECH, pages 1706–1710.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWieland Eckert, Esther Levin, and Roberto Pieraccini.\n1997. User modeling for spoken dialogue system\nevaluation. In 1997 IEEE Workshop on Automatic\nSpeech Recognition and Understanding Proceedings,\npages 80–87. IEEE.\nSteven Y Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush V osoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation ap-\nproaches for nlp. arXiv preprint arXiv:2105.03075.\nKallirroi Georgila, James Henderson, and Oliver Lemon.\n2006. User simulation for spoken dialogue systems:\nlearning and evaluation. In Interspeech, pages 1065–\n1068. Citeseer.\nMeritxell González, Silvia Quarteroni, Giuseppe Ric-\ncardi, and Sebastian Varges. 2010. Cooperative user\nmodels in statistical dialog simulators. In Proceed-\nings of the SIGDIAL 2010 Conference , pages 217–\n220.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nMatthew Henderson and Ivan Vuli ´c. 2020. Convex:\nData-efficient and few-shot slot labeling. arXiv\npreprint arXiv:2010.11791.\nFlorian Kreyssig, Inigo Casanueva, Pawel\nBudzianowski, and Milica Gasic. 2018. Neu-\nral user simulation for corpus-based policy\noptimisation for spoken dialogue systems. arXiv\npreprint arXiv:1805.06966.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019. An\nevaluation dataset for intent classification and out-of-\nscope prediction. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1311–1316, Hong Kong, China. Association\nfor Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nXiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong\nLi, Jianfeng Gao, and Yun-Nung Chen. 2016. A\nuser simulator for task-completion dialogues. arXiv\npreprint arXiv:1612.05688.\n604\nHsien-chin Lin, Nurul Lubis, Songbo Hu, Carel van\nNiekerk, Christian Geishauser, Michael Heck, Shu-\ntong Feng, and Milica Gaši ´c. 2021. Domain-\nindependent user simulation with transformers for\ntask-oriented dialogue systems. arXiv preprint\narXiv:2106.08838.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2019a. Benchmarking natural lan-\nguage understanding services for building conversa-\ntional agents. arXiv preprint arXiv:1903.05566.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZihan Liu, Genta Indra Winata, Peng Xu, and Pas-\ncale Fung. 2020. Coach: A coarse-to-fine ap-\nproach for cross-domain slot filling. arXiv preprint\narXiv:2004.11727.\nAndrea Madotto, Zhaojiang Lin, Genta Indra Winata,\nand Pascale Fung. 2021. Few-shot bot: Prompt-\nbased learning for dialogue systems. arXiv preprint\narXiv:2110.08118.\nShikib Mehri and Mihail Eric. 2021. Example-driven\nintent prediction with observers. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2979–2992.\nShikib Mehri, Mihail Eric, and Dilek Hakkani-Tur.\n2020. Dialoglue: A natural language understanding\nbenchmark for task-oriented dialogue. arXiv preprint\narXiv:2009.13570.\nShikib Mehri and Maxine Eskenazi. 2021a. Gensf:\nSimultaneous adaptation of generative pre-trained\nmodels and slot filling. In Proceedings of the 22nd\nAnnual Meeting of the Special Interest Group on Dis-\ncourse and Dialogue, pages 489–498.\nShikib Mehri and Maxine Eskenazi. 2021b. Schema-\nguided paradigm for zero-shot dialog. In Proceed-\nings of the 22nd Annual Meeting of the Special In-\nterest Group on Discourse and Dialogue, pages 499–\n508.\nTom M Mitchell. 1980. The need for biases in learning\ngeneralizations. Department of Computer Science,\nLaboratory for Computer Science Research . . . .\nJohannes EM Mosig, Shikib Mehri, and Thomas Kober.\n2020. Star: A schema-guided dialog dataset for trans-\nfer learning. arXiv preprint arXiv:2010.11853.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2020. Soloist:\nFew-shot task-oriented dialog with a single pre-\ntrained auto-regressive model. arXiv preprint\narXiv:2005.05298.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghav Gupta, and Pranav Khaitan. 2020. Towards\nscalable multi-domain conversational agents: The\nschema-guided dialogue dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 34, pages 8689–8696.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Comput-\ning Systems, pages 1–7.\nJost Schatzmann, Blaise Thomson, Karl Weilhammer,\nHui Ye, and Steve Young. 2007. Agenda-based user\nsimulation for bootstrapping a pomdp dialogue sys-\ntem. In Human Language Technologies 2007: The\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics; Companion\nVolume, Short Papers, pages 149–152.\nJost Schatzmann, Karl Weilhammer, Matt Stuttle, and\nSteve Young. 2006. A survey of statistical user simu-\nlation techniques for reinforcement-learning of dia-\nlogue management strategies. The knowledge engi-\nneering review, 21(2):97–126.\nKonrad Scheffler and Steve Young. 2000. Probabilis-\ntic simulation of human-machine dialogues. In\n2000 IEEE International Conference on Acoustics,\nSpeech, and Signal Processing. Proceedings (Cat. No.\n00CH37100), volume 2, pages II1217–II1220. IEEE.\nWeiyan Shi, Kun Qian, Xuewei Wang, and Zhou Yu.\n2019. How to build user simulators to train rl-based\ndialog systems. arXiv preprint arXiv:1909.01388.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\narXiv preprint arXiv:2104.06644.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woomyeong Park. 2021. Gpt3mix:\nLeveraging large-scale language models for text aug-\nmentation. arXiv preprint arXiv:2104.08826.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.",
  "topic": "Dialog box",
  "concepts": [
    {
      "name": "Dialog box",
      "score": 0.928543210029602
    },
    {
      "name": "Computer science",
      "score": 0.7822555303573608
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.7250522375106812
    },
    {
      "name": "Task (project management)",
      "score": 0.6824578046798706
    },
    {
      "name": "Generalization",
      "score": 0.6452065706253052
    },
    {
      "name": "Training set",
      "score": 0.6262217164039612
    },
    {
      "name": "Shot (pellet)",
      "score": 0.583781361579895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5417640209197998
    },
    {
      "name": "Natural language processing",
      "score": 0.5149314999580383
    },
    {
      "name": "Dialog system",
      "score": 0.4626423120498657
    },
    {
      "name": "Speech recognition",
      "score": 0.38349971175193787
    },
    {
      "name": "Machine learning",
      "score": 0.3533549904823303
    },
    {
      "name": "Engineering",
      "score": 0.10223737359046936
    },
    {
      "name": "Linguistics",
      "score": 0.08898952603340149
    },
    {
      "name": "Mathematics",
      "score": 0.08022898435592651
    },
    {
      "name": "World Wide Web",
      "score": 0.07912763953208923
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 6
}