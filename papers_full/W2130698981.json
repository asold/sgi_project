{
  "title": "A New Bigram-PLSA Language Model for Speech Recognition",
  "url": "https://openalex.org/W2130698981",
  "year": 2010,
  "authors": [
    {
      "id": "https://openalex.org/A1979127628",
      "name": "Mohammad Bahrani",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2217417247",
      "name": "Hossein Sameti",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1979127628",
      "name": "Mohammad Bahrani",
      "affiliations": [
        "Sharif University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2217417247",
      "name": "Hossein Sameti",
      "affiliations": [
        "Sharif University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2147152072",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W4233135949",
    "https://openalex.org/W1984251878",
    "https://openalex.org/W2104210067",
    "https://openalex.org/W2134731454",
    "https://openalex.org/W2113641473",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1491238342",
    "https://openalex.org/W2107743791",
    "https://openalex.org/W85801758",
    "https://openalex.org/W106244805",
    "https://openalex.org/W2597684388",
    "https://openalex.org/W1581253957",
    "https://openalex.org/W3145733519",
    "https://openalex.org/W2612972698",
    "https://openalex.org/W1967109167",
    "https://openalex.org/W2112971401",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2999905431",
    "https://openalex.org/W2143144851",
    "https://openalex.org/W2137805602",
    "https://openalex.org/W1644652583",
    "https://openalex.org/W2596600410"
  ],
  "abstract": "A novel method for combining bigram model and Probabilistic Latent Semantic Analysis (PLSA) is introduced for language modeling. The motivation behind this idea is the relaxation of the \"bag of words\" assumption fundamentally present in latent topic models including the PLSA model. An EM-based parameter estimation technique for the proposed model is presented in this paper. Previous attempts to incorporate word order in the PLSA model are surveyed and compared with our new proposed model both in theory and by experimental evaluation. Perplexity measure is employed to compare the effectiveness of recently introduced models with the new proposed model. Furthermore, experiments are designed and carried out on continuous speech recognition (CSR) tasks using word error rate (WER) as the evaluation criterion. The superiority of the new bigram-PLSA model over Nie et al.'s bigram-PLSA and simple PLSA models is demonstrated in the results of our experiments. Experiments on BLLIP WSJ corpus show about 12% reduction in perplexity and 2.8% WER improvement compared to Nie et al.'s bigram-PLSA model.",
  "full_text": "Hindawi Publishing Corporation\nEURASIP Journal on Advances in Signal Processing\nVolume 2010, Article ID 308437, 8 pages\ndoi:10.1155/2010/308437\nResearch Article\nA New Bigram-PLSA Language Model for Speech Recognition\nMohammad Bahrani and Hossein Sameti\nDepartment of Computer Engineering, Sharif University of Technology, 145-8889694 Tehran, Iran\nCorrespondence should be addressed to Mohammad Bahrani, bahrani@ce.sharif.edu\nReceived 3 March 2010; Revised 9 May 2010; Accepted 8 July 2010\nAcademic Editor: Douglas O’Shaughnessy\nCopyright © 2010 M. Bahrani and H. Sameti. This is an open access article distributed under the Creative Commons Attribution\nLicense, which permits unrestricted use, distribution, and repro duction in any medium, provided the original work is properly\ncited.\nA novel method for combining bigram model and Probabilistic Latent Semantic Analysis (PLSA) is introduced for language\nmodeling. The motivation behind this idea is the relaxation of t he “bag of words” assumption fundamentally present in latent\ntopic models including the PLSA model. An EM-based parameter estimation technique for the proposed model is presented in\nthis paper. Previous attempts to incorporate word order in the PLSA model are surveyed and compared with our new proposed\nmodel both in theory and by experimental evaluation. Perplexity measure is employed to compare the e ﬀectiveness of recently\nintroduced models with the new proposed model. Furthermore, experiments are designed and carried out on continuous speech\nrecognition (CSR) tasks using word error rate (WER) as the evaluation criterion. The superiority of the new bigram-PLSA model\nover Nie et al. ’s bigram-PLSA and simple PLSA models is demonstr ated in the results of our experiments. Experiments on BLLIP\nWSJ corpus show about 12% reduction in perplexity and 2.8% WER improvement compared to Nie et al. ’s bigram-PLSA model.\n1. Introduction\nLanguage models are important in various applications\nespecially in speech recognition. Statistical language models\nare obtained using di ﬀerent approaches depending on the\nresources and tasks requirements. Extracting n-gram statis-\ntics is a prevalent approach for statistical language modeling.\nN-gram takes the order of words into account and calculates\nthe probability of the word occurring after n\n−1 other known\nwords.\nMany attempts have been made to incorporate semantic\nknowledge in language modeling. Latent topic modeling\napproaches such as Latent Semantic Analysis (LSA) [ 1,\n2], Probabilistic Latent Semantic Analysis (PLSA) [ 3], and\nLatent Dirichlet Allocation (LDA) [ 4] are the most recent\ntechniques. Latent semantic information is extracted by these\nmodels through decomposing word-document cooccurrence\nmatrix. These topic models have been successful in reducing\nthe perplexity and improving the accuracy rate of speech\nrecognition systems [ 2, 5, 6]. The main deﬁciency of the\ntopic models is that they do not take the order of words\ninto consideration due to the assumption of “bag of words”\nintrinsically.\nThe useful semantic modeling of the topic models\nand the potential of considering words history in the n-\ngram language model motivate researchers to combine the\ncapabilities of both approaches. Bellegarda [ 2] proposed\nthe combination of the n-gram and the LSA models and\nFederico [ 7] utilized the PLSA framework to adapt the n-\ngram language model. Both [ 2, 7]u s e dr e s c a l i n ga p p r o a c h\nfor the combination. Gri ﬃths et al. [ 8]p r e s e n t e da n\nextension of the topic model that is sensitive to word order\nand automatically learns the syntactic factors as well as\nthe semantic ones. In [ 9, 10] the collocation of words\nwas incorporated in the LDA model. Girolami and Kaban\n[11] relaxed the “bag of words” assumption in the LDA\nmodel by applying the Markov chain assumption on symbol\nsequences. Wallach [ 12] proposed a combination of bigram\nand LDA models (the bigram topic model) and achieved\na signiﬁcant performance improvement on perplexity by\nexploring latent semantics following di ﬀerent context words.\nThis research was a basis for Nie et al. ’s work [ 13] that\nproposed the combination of bigram and PLSA models. The\nperformance improvements achieved in [ 12, 13]m o t i v a t e d\nus to propose a general framework for combining bigram\nand PLSA models. As discussed in Section 3.6 ,o u rm o d e l\n2 EURASIP Journal on Advances in Signal Processing\nis di ﬀerent from Nie et al.’s work and can be considered as\na generalization to that model. One cannot derive the re-\nestimation formulae via the standard EM procedure based on\nNie et al. ’s model. In this paper, we propose an EM procedure\nfor re-estimating the parameters of our model.\nThe remainder of the paper is organized as follows. In\nSection 2 , the PLSA model is brieﬂy reviewed. In Section 3 ,\nthe combination of bigram and PLSA models is introduced\nand its parameter estimation procedure is described. In\nSection 4 , experimental results are presented and ﬁnally in\nSection 5 the conclusions are made.\n2. Review of the PLSA Model\nSuppose that we have a set of words W ={w1, w2, ... , wM}\nthat composes a set of documents D ={ d1, d2, ... , dN}.I n\nthe PLSA model, the occurrence probability of word wi given\ndocument dj is deﬁned as below [ 3].\nP\n(\nwi |dj\n)\n=\n∑\nk\nP(wi |zk)P\n(\nzk |dj\n)\n,( 1 )\nwhere zk is a latent class variable (or a topic) belonging\nto a set of class variables (topics) Z ={ z1, z2, ... , zK}.\nEquation ( 1) is a weighted mixture of word distributions\ncalled aspect model [ 14]. The aspect model is a latent variable\nmodel for co-occurrence data that associates an unobserved\nclass variable z\nk ∈ Z to each observation (i.e., words\nand documents). The aspect model introduces a conditional\nindependence assumption, that is, d\nj and wi are independent\nconditioned on the state of the associated latent variable [ 15].\nIn ( 1), P(wi | zk), i = 1, ... , M, k = 1, ... , K are the word\ndistributions and P(zk | dj), k = 1, ... , K, j = 1, ... , N are\nthe weights of distributions.\nIn another view, the PLSA model is a decomposition of\nword-document co-occurrence matrix P(w |d). The P(w |\nd)m a t r i xi sd e c o m p o s e di n t oP(w |z)a n d P(z |d)m a t r i c e s\nin order to minimize the cross entropy (KL divergence)\nbetween the P(w\n|d) matrix and empirical distribution.\nThe PLSA parameters P(wi | zk)a n d P(zk | dj)a r e\nre-estimated via the EM procedure. The EM procedure\nincludes two alternate steps: (i) an expectation (E) step where\nposterior probabilities are computed for the latent variables\nbased on the current estimates of the parameters, (ii) a\nmaximization (M) step where PLSA parameters are updated\nbased on the posterior probabilities computed in the E-step\n[15].\n3. Combining Bigram and PLSA Models\nBefore describing the proposed model, the previous research\non combining bigram and PLSA model by Nie et al. [ 13]\nis reviewed. This method is a special case (with certain\nindependence assumptions) of our proposed method.\n3.1. Nie et al. ’s Bigram-PLSA Model. Nie et al. presented a\ncombination of bigram and PLSA models [ 13]. Instead of\nP(w\ni |zk)i n( 1), their bigram-PLSA model employs P(wj |\nwi, zk) resulting in\nP\n(\nwj |wi, dk\n)\n=\n∑\nl\nP\n(\nwj |wi, zl\n)\nP(zl |dk). (2)\nThe EM procedure for training the combined model\ncontains the following two steps.\nE-step:\nP\n(\nzl |dk, wi, wj\n)\n=\nP\n(\nwj |wi, zl\n)\nP(zl |dk)\n∑\nl′P\n(\nwj |wi, zl′\n)\nP(zl′|dk)\n. (3)\nM-step:\nP\n(\nwj |wi, zl\n)\n=\n∑\nk n\n(\ndk, wi, wj\n)\nP\n(\nzl |dk, wi, wj\n)\n∑\nj′\n∑\nk n\n(\ndk, wi, wj′\n)\nP\n(\nzl |dk, wi, wj′\n),\n(4)\nP(zl |dk) =\n∑\nj\n∑\ni n\n(\ndk, wi, wj\n)\nP\n(\nzl |dk, wi, wj\n)\nN(dk) ,( 5 )\nwhere n(dk, wi, wj) is the number of times that the word pair\nwiwj occurs in the document dk, and N(dk) is the number of\nwords in the document dk.\n3.2. Proposed Bigram-PLSA Model. We intend to combine\nthe bigram and the PLSA models to take advantage of the\nstrengths of both models for increasing the predictability of\nw o r d si nd o c u m e n t s .I no r d e rt oc o m b i n eb i g r a ma n dP L S A\nmodels, we incorporate the context word w\ni in the PLSA\nparameters. In other words, we associate the generation of\nwords and documents to the context word in addition to the\nlatent topics.\nThe generative process of bigram-PLSA model can be\ndeﬁned by the following scheme:\n(1) Generate a context word w\ni as the word history with\nprobability P(wi).\n(2) Select a document dk with probability P(dk |wi).\n(3) Pick a latent variable zl with probability P(zl |wi, dk).\n(4) Generate a word wj with probability P(wj |wi, zl).\nTranslating the generative process into a joint probability\nmodel results in\nP\n(\ndk, wi, wj\n)\n=P\n(\ndk, wiwj\n)\n=\n∑\nl\nP(wi)P(dk |wi)P(zl |wi, dk)\n×P\n(\nwj |wi, zl\n)\n.\n(6)\nEURASIP Journal on Advances in Signal Processing 3\nAccording to ( 6), the occurrence probability of the word\nwj given the document dk and the word history wi is deﬁned\nas\nP\n(\nwj |wi, dk\n)\n=\n∑\nl\nP\n(\nwj |wi, zl\n)\nP(zl |wi, dk). (7)\nEquation ( 7) is an extended version of the aspect model\nthat considers the word history in the word-document\nmodeling and can be considered as a combination of bigram\nand PLSA models. In ( 7), the distributions P(w\nj | wi, zl)\nand P(zl | wi, dk) are the model parameters that should be\nestimated from training data. This model is similar to the\noriginal PLSA model except that the context words (word\nhistory) w\ni is incorporated in the model parameters.\nLike the original aspect model, the extended aspect\nmodel assumes conditional independence between word\nw\nj and document dk, that is, wj and dk are independent\nconditioned on the latent parameter zl and the context word\nwi:\nP\n(\ndk, wj |wi, zl\n)\n=P(dk |wi, zl)P\n(\nwj |wi, zl\n)\n. (8)\nThe justiﬁcation behind the assumed conditional inde-\npendence in the proposed model is the same reasoning that\nthe PLSA model is using to make an analytical model, that\nis, simpliﬁcation of the model formulation and reasonable\nreduction of the computational cost.\nAs in the original PLSA model, the equivalent parameter-\nization of the joint probability in ( 6)c a nb ew r i t t e na s\nP\n(\ndk, wi, wj\n)\n=P(wi)\n∑\nl\nP\n(\nwj |wi, zl\n)\nP(dk |wi, zl)P(zl |wi).\n(9)\n3.3. Parameter Estimation Using the EM Algorithm. Like\noriginal PLSA model, we re-estimate the parameters of\nbigram-PLSA model using the EM procedure. In the EM\nprocedure, for E-step, we simply apply Bayes’ rule to obtain\nthe posterior probability of the latent variable z\nl given the\nobserved data dk, wi,a n d wj.\nE-step:\nP\n(\nzl |dk, wi, wj\n)\n=\nP\n(\nzl, dk, wi, wj\n)\n∑\nl′P\n(\nzl′, dk, wi, wj\n)\n=\nP(wi, zl)P(dk |wi, zl)P\n(\nwj |wi, zl\n)\n∑\nl′P(wi, zl′)P(dk |wi, zl′)P\n(\nwj |wi, zl′\n).\n(10)\nWe can rew r ite (10)a s\nP\n(\nzl |dk, wi, wj\n)\n=\nP(zl |wi)P(dk |wi, zl)P\n(\nwj |wi, zl\n)\n∑\nl′P(zl′|wi)P(dk |wi, zl′)P\n(\nwj |wi, zl′\n)\n=\nP\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\n∑\nl′P\n(\nwj |wi, zl′\n)\nP(zl′|wi, dk)\n.\n(11)\nIn the M-step, the parameters are updated by max-\nimizing the log-likelihood of the complete data (words\nand documents) with respect to the probabilistic model.\nThe likelihood of the complete data with respect to the\nprobabilistic model is computed as\nL\n=\n∏\ni, j,k\nP(dk, wiwj)n(dk,wiwj), (12)\nwhere P(dk, wiwj) is the occurrence probability of the word\npair wiwj in the document dk and n(dk, wiwj) is the\nfrequency of word pair wiwj in the document dk.\nLet θ ={ P(wj | wi, zl), P(zl | wi, dk)} be the set\nof model parameters. For estimating θ, we use MLE to\nmaximize the log-likelihood of the complete data:\nθML =arg max\nθ\nlog(L)\n=arg max\nθ\n∑\ni, j,k\nn\n(\ndk, wiwj\n)\nlog P\n(\ndk, wiwj\n)\n=arg max\nθ\n∑\ni, j,k\nn\n(\ndk, wiwj\n)\n×\n[\nlog P(dk, wi) +l o g P\n(\nwj |wi, dk\n)]\n.\n(13)\nConsidering (7), we expand the above equation to\nθML =arg max\nθ\n∑\ni, j,k\nn\n(\ndk, wiwj\n)\nlog P(dk, wi)\n+\n∑\ni, j,k\nn\n(\ndk, wiwj\n)\nlog\n⎛\n⎝∑\nl\nP\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\n⎞\n⎠\n=arg max\nθ\n∑\ni, j,k\nn\n(\ndk, wiwj\n)\n×log\n⎛\n⎝∑\nl\nP\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\n⎞\n⎠.\n(14)\nIn ( 14), the left factor before the plus sign is omitted\nbecause it is independent of θ. In order to maximize the log-\nlikelihood, (14) should be di ﬀerentiated. Diﬀerentiating (14)\nwith respect to the parameters does not lead to well-formed\n4 EURASIP Journal on Advances in Signal Processing\nformulae, so we try to ﬁnd a lower bound for ( 14) using\nJensen’s inequality\n∑\ni, j,k\nn\n(\ndk, wiwj\n)\nlog\n⎛\n⎝∑\nl\nP\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\n⎞\n⎠\n=\n∑\ni, j,k\nn\n(\ndk, wiwj\n)\n×log\n⎛\n⎝∑\nl\nP\n(\nzl |dk, wi, wj\n)P\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\nP\n(\nzl |dk, wi, wj\n)\n⎞\n⎠\n≥\n∑\ni, j,k\nn\n(\ndk, wiwj\n)∑\nl\nP\n(\nzl |dk, wi, wj\n)\n×log\n⎛\n⎝P\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\nP\n(\nzl |dk, wi, wj\n)\n⎞\n⎠.\n(15)\nThe obtained lower bound should be maximized, that is,\nmaximizing the right hand side of ( 15) instead of its left hand\nside. For maximizing the lower bound and re-estimating\nthe parameters, we have a constrained optimization problem\nbecause all parameters indicate probability distributions.\nTherefore, the parameters should satisfy the constraints\n∑\nj\nP\n(\nwj |wi, zl\n)\n=1 ∀i, l,\n∑\nl\nP(zl |wi, dk) =1 ∀i, k.\n(16)\nIn order to consider the above constraints, the right hand\nside of ( 15) has to be augmented by the appropriate Lagrange\nmultipliers\nH =\n∑\ni, j,k\nn\n(\ndk, wiwj\n)∑\nl\nP\n(\nzl |dk, wi, wj\n)\n×log\n⎛\n⎝P\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\nP\n(\nzl |dk, wi, wj\n)\n⎞\n⎠\n+\n∑\ni,l\nτil\n⎛\n⎝1 −\n∑\nj\nP\n(\nwj |wi, zl\n)\n⎞\n⎠\n+\n∑\ni,k\nρik\n⎛\n⎝1 −\n∑\nl\nP(zl |wi, dk)\n⎞\n⎠,\n(17)\nwhere τil and ρik are the Lagrange multipliers related to\nconstraints speciﬁed in ( 16).\nDiﬀerentiating the above equation partially with respect\nto the di ﬀerent parameters leads to( 18)\n∂H\n∂P\n(\nwj |wi, zl\n)=\n∑\nk\nn\n(\ndk, wiwj\n)P\n(\nzl |dk, wi, wj\n)\nP\n(\nwj |wi, zl\n) −τil\n=0,\n∂H\n∂P(zl |wi, dk) =\n∑\nj\nn\n(\ndk, wiwj\n)P\n(\nzl |dk, wi, wj\n)\nP(zl |wi, dk) −ρil\n=0.\n(18)\nSolving ( 18) and applying the constraints ( 16), the M-step\nre-estimation formulae, ( 19), are obtained:\nP\n(\nwj |wi, zl\n)\n=\n∑\nk n\n(\ndk, wiwj\n)\nP\n(\nzl |dk, wi, wj\n)\n∑\nj′\n∑\nk n\n(\ndk, wiwj′\n)\nP\n(\nzl |dk, wi, wj′\n),\nP(zl |wi, dk) =\n∑\nj n\n(\ndk, wiwj\n)\nP\n(\nzl |dk, wi, wj\n)\n∑\nl′\n∑\nj n\n(\ndk, wiwj\n)\nP\n(\nzl′|dk, wi, wj\n).\n(19)\nThe E-step and M-step are repeated until convergence\ncriterion is met.\n3.4. Implementation and Complexity Analysis. For imple-\nmenting the EM algorithm, in the E-step, we need to\ncalculate P(z\nl | dk, wi , wj)f o ra l l i, j, k, and l.I tr e q u i r e s\nfour nested loops. Thus the time complexity of the E-\nstep is O(M\n2NK), where M, N, and K are the number\nof words, the number of documents, and the number\nof latent topics respectively. The memory requirements in\nthe E-step include a four-dimensional matrix for saving\nP(z\nl | dk, wi, wj) and a three-dimensional matrix for saving\nthe normalization parameter (denominator of ( 11)). For\nreducing the memory requirements, note that it is not\nnecessary to calculate and save P(z\nl |dk, wi, wj) at the E-step;\nrather, it can be calculated in the M-step by multiplying the\nprevious P(w\nj | wi, zl)a n d P(zl | wi, dk) and dividing the\nresult by the normalization parameter. Therefore, we save\nonly the normalization parameter at the E-step. According to\n(7), the normalization parameter is equal to P(w\nj | wi, dk),\nthus the related matrix contains M2N elements, which is a\nlarge number for typical values of M and N.\nIn the M-step, we need to calculate the model parameters\nP(wj | wi, zl)a n d P(zl | wi, dk)s p e c i ﬁ e di n( 19). These\ncalculations require four nested loops, but note that we\ncan decrease the number of loops to three nested loops\nby considering only the word pairs that are present in the\ntraining documents instead of all word pairs. Thus the time\ncomplexity in the M-step is O(KNB)w h e r eB is the average\nnumber of the word pairs in the training documents.\nThe memory requirements in the M-step include two\nthree-dimensional matrices for saving P(w\nj | wi, zl)a n d\nP(zl | wi, dk) and two two-dimensional matrices for saving\nEURASIP Journal on Advances in Signal Processing 5\nthe denominators of ( 19). Saving these large matrices results\nin high memory requirements in the training process.\nn(d\nk, wi, wj) is another matrix that can be implemented by\na sparse matrix containing the indices of the word pairs\npresented in each training document and the counts of the\nword pairs.\n3.5. Extension to n-gram. We can extend the bigram-PLSA\nmodel to n-gram-PLSA model by considering the n\n− 1\ncontext words hi =wi−(n−1) ··· wi−2 wi−1 instead of only one\ncontext word wi as the word history. The generative process\nof the n-gram-PLSA model is similar to the bigram-PLSA\nmodel except that in step 1, instead of generating one context\nword, n\n−1 context words should be generated. Therefore, the\ncombined model can be expressed by\nP\n(\nwj |hi, dk\n)\n=\n∑\nl\nP\n(\nwj |hi, zl\n)\nP\n(\nzl |hi , dk\n)\n, (20)\nwhere hi = wi−(n−1) ··· wi−2 wi−1 is a sequence of n − 1\nwords. We can follow the same EM procedure for parameter\nestimation in the n-gram-PLSA model where w\ni is replaced\nby hi in all formulae. In the re-estimation formulae, we have\nn(dk, hi, wj) that is the number of occurrences of the word\nsequence hiwj =wi−(n−1) ··· wi−2 wi−1 wj in the document dk.\nCombining PLSA model and n-gram model for n> 2\nleads to high complexity in time and memory of the training\nprocess. As discussed in Section 3.4 , the time complexity of\nthe EM algorithm is O(M\n2NK)f o r n = 2. Consequently,\nthe time complexity for higher order n-grams is O(MnNK)\nthat grows exponentially as n increases. In addition, the\nmemory requirement for n-gram-PLSA combination is very\nhigh. For example, for saving the normalization parameters,\nwe need a ( n+ 1)-dimensional matrix which contains M\nnN\nelements. Therefore, the memory requirement also grows\nexponentially as n increases.\n3.6. Comparison with Nie et al. ’s Bigram-PLSA Model. As\ndiscussed in Section 3.1 ,N i ee ta l .h a v ep r e s e n t e dac o m b i -\nnation of bigram and PLSA models in 2007 [ 13]. This work\ndoes not have a strong mathematical foundation and one\ncannot derive the re-estimation formulae via the standard\nEM procedure based on that. Nie et al. ’s work is based on\nan assumption of independence between the latent topics z\nl\nand the context words wi. According to this assumption, we\ncan rewrite ( 7)a s\nP\n(\nwj |wi, dk\n)\n=\n∑\nl\nP\n(\nwj |wi, zl\n)\nP(zl |wi, dk)\n≈\n∑\nl\nP\n(\nwj |wi, zl\n)\nP(zl |dk).\n(21)\nAccording to ( 21), the di ﬀerence between our model\nand Nie et al. ’s model is in the deﬁnition of the topic\nprobability. In Nie et al.’s model the topic probability is\nconditioned on the documents, but in our model, the topic\nprobability is further conditioned on the bigram history. In\nNie et al.’s model, the assumption of independence between\nthe latent topics and the context words leads to assigning\nthe latent topics to each context word evenly, that is, the\nsame numbers of latent variables are assigned to decompose\nthe word-document matrices of all context words despite\ntheir di ﬀerent complexities. Thus, they propose a reﬁning\nprocedure that unevenly assigns the latent topics to the\ncontext words according to an estimation of their latent\nsemantic complexities.\nIn our proposed bigram-PLSA model, we relax the\nassumption of independence between the latent topics and\nthe context words and achieve a general form of the\naspect model that considers the word history in the word-\ndocument modeling. Our model automatically assigns the\nlatent topics to the context words unevenly because for\neach context h\ni, there is a distribution P(zl | wi, dj) that\nassigns the appropriate number of latent topics to that\ncontext. Consequently, P(z\nl | wi, dj) remains zero for those\nzl inappropriate to the context word wi.\nThe number of free parameters in our proposed model is\nM(M−1)K+(K−1)MN,w h e r eM, N, and K are the number\nof words, the number of documents, and the number latent\ntopics, respectively. On the other hand, the number of free\nparameters in Nie et al. ’s model is M(M\n−1)K +( K −1)N\nthat is less than the number of free parameters in our model.\nConsequently, the training time of Nie et al.’s model is less\nthan the training time of our model.\n4. Experimental Results\nThe bigram-PLSA model was evaluated using two di ﬀerent\ncriteria: perplexity and word error rate of a CSR system.\nWe selected 500 documents containing about 248600 words\nfrom BLLIP WSJ corpus and used them to train our proposed\nbigram-PLSA model. We replaced all stop words of the\ntraining documents with a unique symbol (#STOP) and\nconsidered all infrequent words (the words occurring only\nonce) as unknown words and replaced them with UNK\nsymbol. After these replacements, the vocabulary contained\nabout 3800 words. We could not include more documents\nin the training process because the computational cost and\nmemory requirement grow rapidly as the size of the training\nset increases (as discussed in Section 3.4 ). For training the\nbigram-PLSA model, ﬁrst we set the number of the latent\ntopics between 10 and 50 and initialized the model randomly,\nthen we executed the EM algorithm until it converged. We\nevaluated the bigram-PLSA model on 50 documents, with\n22300 words in total, not overlapped with the training data.\nThis evaluation process was run ten times for di ﬀerent\nrandom initial models and the results were averaged.\nThe perplexity of evaluation data d\n=w1w2 ··· wN was\ncalculated as follows:\nPP =\n⎡\n⎣\nN∏\nn=2\nP(wn |wn−1, d)\n⎤\n⎦\n−1/N\n, (22)\nwhere P(wn |wn−1, d) was obtained from the value of P(wj |\nwi, d) in the bigram-PLSA model. Since document d was not\npresent in the training data, we had to follow the folding-\nin procedure mentioned in [ 5]t oc a l c u l a t eP(w\nj | wi, d).\nWithin this procedure, the parameters P(wj | wi, zl)w e r e\n6 EURASIP Journal on Advances in Signal Processing\n80\n100\n120\n140\n160\n180\n10 20 30 40 50\nNumber of latent topics\nPerplexity\nBigram-PLSA (proposed)\nBigram-PLSA (Nie et al. ’s)\nFigure 1: The average perplexities obtained by the proposed and\nNie et al. ’s bigram-PLSA model with respect to di ﬀerent numbers of\nlatent topics.\nassumed constant and the EM algorithm was employed to\ncalculate only P(zl | wi, dk) parameters for dk = d and\nfor those wi present in the document d. After convergence\nof the EM procedure, P(wj | wi, d) was found. Obtained\nmatrix P(wj |wi, d) contained many zero probabilities, thus\nwe smoothed it using Witten-Bell smoothing method [ 16].\nNote that the folding-in procedure gives the PLSA and the\nbigram-PLSA models an unfair advantage by allowing them\nto adapt the model parameters to the test data. Nevertheless,\nwe applied it to avoid overﬁtting.\nT o have a valid comparison, the PLSA and Nie et\nal. ’s bigram-PLSA models were trained by the same data\nemployed to train our proposed bigram-PLSA model. The\nfolding-in procedure and Witten-Bell smoothing were also\napplied on the PLSA and Nie et al. ’s bigram-PLSA models.\nFigure 1 shows the perplexities of the proposed and Nie et\nal. ’s bigram-PLSA models for di ﬀerent numbers of latent\ntopics averaged over ten times of running the experiment.\nIn this ﬁgure, the error bars show the standard errors of the\naverage perplexities. As seen in Figure 1, the perplexity of our\nproposed bigram-PLSA model is lower than the perplexity\nof Nie et al. ’s bigram-PLSA model. The best perplexity was\nobtained when the number of latent topics was set to 40\nin both models. Therefore, in the rest of experiments the\nnumbers of latent topics were set accordingly.\nIn addition, we performed the paired t-test on the\nperplexity results of both methods with the signiﬁcance level\nof 0.01. As stated, each experiment was carried out ten times.\nThe null hypothesis is whether the average perplexities of two\nmethods are the same. Table 1 shows the P-value obtained\nfrom the paired t-test for our experiments performed with\ndiﬀerent numbers of latent topics. The right column of\nTable 1 shows the P-value where the alternative hypothesis\nis whether the average perplexity of our method is less\nthan the average perplexity of Nie et al. ’s method. All P-\nvalues obtained are smaller than the speciﬁed signiﬁcance\nTa bl e1: The P-values obtained from the paired t-test on perplexity\nresults of Nie et al. ’s and proposed method for di ﬀerent numbers of\nlatent topics ( K).\nKP -value\n10 3 .58E−05\n20 1 .23E−07\n30 1 .23E−06\n40 4 .35E−07\n50 3 .26E−08\nlevel. Therefore, the perplexity improvements are statistically\nsigniﬁcant.\nTable 2 shows the comparison between the average\nperplexities of the bigram-PLSA model and other language\nmodels. The standard errors of the average perplexities, the\nnumber of model parameters and the approximate time of\neach EM iteration are reported in this table. Note that the\nnumber of model parameters for the bigram and trigram\nlanguage models are equal to the number of word pairs\nand word triplets observed in the training data, respectively.\nThe numbers shown in Table 2 are the maximum possible\nnumber of the word pairs and triplets. In this table, the\nperplexities of the bigram and trigram language models, the\nPLSA model, and linear interpolations of the PLSA model\nand the bigram model are also shown. The bigram and\ntrigram language models were trained by the training data\ndiscussed above and the Katz backo ﬀ smoothing method\n[17] was applied on them. Stop words and infrequent words\nof training data were replaced by #STOP and UNK symbols.\nThe number of latent topics was set to 40 in the bigram-\nPLSA models and 50 in the PLSA model because for the PLSA\nmodel the best perplexity was obtained when the number\nof latent topics was set to 50. In case of linear interpolation,\nP(w\nn |wn−1, d)i n( 22) was calculated as follows:\nP(wn |wn−1, d) =λPbigram(wn |wn−1)\n+ (1 −λ)PPLSA(wn |d).\n(23)\nWe set λ =0.75 in our experiments. This value for λwas\nobtained by optimizing it on the held-out data.\nAs Table 2 shows, the proposed bigram-PLSA model\nreduces the perplexity more than other language models;\nhowever, the number of parameters and the training time\nof the proposed model is more than the other models.\nThe proposed bigram-PLSA model was incorporated in the\nSphinx 4.0 [ 18] CSR system and thus evaluated. The SI84\npart of Wall Street Journal corpus was used for training the\nacoustic models and the November 1992 ARPA CSR test set\nwas used for testing. The vocabulary contained 5000 words\nincluding 3800 words used for the bigram-PLSA model,\nabout 200 stop words and about 1000 extra words. We used\nab a c k - oﬀ trigram language model trained by the whole\nBLLIP WSJ corpus in the decoding process and employed the\nPLSA and the bigram-PLSA models for the N-best rescoring.\nSince the vocabulary of the bigram-PLSA model contains\nonly 3800 content words, the stop words and the extra words\nexisting in the N-best list were replaced by #STOP and UNK\nEURASIP Journal on Advances in Signal Processing 7\nTa bl e2: Perplexities, number of parameters, and the computation cost of the bigram-PLSA model and other language models.\nModel Calculated parameter Number of model\nparameters\nTime of each\nEM iteration Perplexity\nbigram P(wn |wn−1) Maximum 3800 2 — 198\ntrigram P(wn |wn−2wn−1) Maximum 3800 3 — 134\nPLSA P(wn |d) 215000 0.6 second 328 ±2.1\nBigram & PLSA (linear interpolation) λP(wn |wn−1)+( 1 −λ)P(wn |d) 14655000 0.6 second 155 ±6.2\nBigram-PLSA (Nie et al. ’s)\n∑L\nl=1 P(wn |wn−1, zl)P(zl |d) 577620000 19 minutes 123 ±4.8\nBigram-PLSA (proposed)\n∑L\nl=1 P(wn |wn−1, zl)P(zl |wn−1, d) 653600000 24 minutes 101 ±3.1\nTa bl e 3: Average word error rates of the CSR system using PLSA-\nbased language models with and without trigram language model\nin decoding.\nLanguage Model\n(for N-best\nrescoring)\nWER (%)\n(trigram in\ndecoding)\nWER (%)\n(No LM in\ndecoding)\nAverage\ndecoding time\n(Sec.)\n— 12.66 74.24 0.8\nPLSA 11 .28 ±0.05 51 .73 ±0.02 4.5\nBigram-PLSA\n(Nie et al. ’s) 10.65 ±0.04 47 .41 ±0.05 131\nBigram-PLSA\n(proposed) 10.28 ±0.02 46 .09 ±0.03 140\nTa bl e 4: The P-values obtained from the paired t-test on WER\nresults of Nie et al. ’s and proposed method.\nLM in decoding P-value\nTr ig r am 6 .53E−10\nNo LM 1 .70E−10\nsymbols, respectively. The number of candidates for N-best\nrescoring was set to 30 and the number of latent topics was set\nto 50 in the PLSA model and 40 in the bigram-PLSA models.\nTable 3 shows the word error rates (WERs) of the CSR system\nusing the PLSA and the bigram-PLSA models averaged over\nten runs of the experiments. In the second column of Table 3,\nthe trigram language model was used in the decoding process\nwhile in the third column, no language model was used in the\ndecoding process and only the PLSA-based language models\nwere used for the N-best rescoring. The standard errors of\naverage WERs are also given in this table.\nAs Table 3 shows, the PLSA and the bigram-PLSA models\nimprove the word error rate. In addition, the word error\nrate obtained from the bigram-PLSA model is meaningfully\nlower than that of the PLSA model. Our proposed bigram-\nPLSA model shows slight improvement compared to Nie et\nal. ’s bigram-PLSA model. The third column better demon-\nstrates the e ﬀect of the bigram-PLSA model in reducing the\nword error rate. The average decoding time is given in the\nlast column of Table 3.I ti so b s e r v e dt h a tW E Ri si m p r o v e d\nfor the cost of increasing the decoding time, but the increase\nin the decoding time compared to the Nie et al. ’s model is\ninsigniﬁcant.\nIn addition, we performed paired t-test on WER results\nof the Nie et al. ’s and the proposed methods. The signiﬁcance\nlevel was set to be 0.01. Table 4 shows the P-values obtained\nfrom the paired t-test. As this table shows, the WER\nimprovements are statistically signiﬁcant.\n5. Conclusions and Future W ork\nIn this paper, a general framework for combining bigram\nand PLSA models was proposed. The combined model was\nobtained from incorporating the word history in the PLSA\nparameters. Furthermore, the EM procedure for estimating\nthe parameters of the combined model was described.\nFinally, the proposed model was compared to the previous\nwork done on combining the bigram and the PLSA models\nby Nie et al. Our proposed model is di ﬀerent from Nie et\nal.’s model in the deﬁnition of the topic probability. In Nie\net al. ’s model the topic probability is conditioned on the\ndocuments, but in our model, the topic probability is further\nconditioned on the bigram history. The proposed model\nautomatically assigns latent topics to each context word\nunevenly in contrast to the even assignment of them by Nie\net al. ’s initial bigram-PLSA model. We arranged experiments\nto evaluate our combined model based on the perplexity\nand the word error rate criteria. Experiments showed that\nour proposed bigram-PLSA model outperformed the PLSA\nmodel according to the both criteria. The proposed model\nalso showed slight superiority over Nie et al.’s bigram-PLSA\nmodel in improving perplexity and WER. As our future\nresearch work, we intend to suggest a similar framework\nto combine n-gram and LDA models. We also plan to use\nautomatic smoothing in our parameter estimation process\nwithout requiring it to be done as an extra step as it is the\nstate-of-the-art in Bayesian machine learning methods.\nAcknowledgment\nThis paper was in part supported by a grant from Iran\nT elecommunication Research Center (ITRC).\nReferences\n[1] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R.\nHarshman, “Indexing by latent semantic analysis, ” Journal of\nthe American Society of Information Science , vol. 41, pp. 391–\n407, 1990.\n[2] J. R. Bellegarda, “Exploiting latent semantic information in\nstatistical language modeling, ” Proceedings of the IEEE , vol. 88,\nno. 8, pp. 1279–1296, 2000.\n8 EURASIP Journal on Advances in Signal Processing\n[3] T. Hofmann, “Probabilistic latent semantic indexing, ” in\nProceedings of the 22nd Annual International ACM SIGIR Con-\nference on Research and Development in Information Retrieval ,\npp. 50–57, Berkeley, Calif, USA, 1999.\n[4] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent Dirichlet\nallocation, ”Journal of Machine Learning Research , vol. 3, no.\n4-5, pp. 993–1022, 2003.\n[5] D. Gildea and T. Hofmann, “T opic-based language models\nusing EM, ” in Proceedings of the 6th European Conference on\nSpeech Communication and Technology (EUROSPEECH ’99) ,\npp. 235–238, Budapest, Hungary, 1999.\n[6] D. Mrva and P . C. Woodland, “Unsupervised language model\nadaptation for mandarin broadcast conversation transcrip-\ntion, ” in Proceedings of International Conference on Spoken\nLanguage Processing , pp. 1549–1552, Pittsburgh, Pa, USA,\n2006.\n[7] M. Federico, “Language model adaptation through topic\ndecomposition and MDI estimation, ” in Proceedings of Inter-\nnational Conference on Acoustics, Speech and Signal Processing ,\npp. 773–776, Orlando, Fla, USA, 2002.\n[8] T. Gri ﬃths, M. Steyvers, D. Blei, and J. T enenbaum, “Integrat-\ning topics and syntax, ” in Advances in Neural Information Pro-\ncessing Systems 17 , pp. 87–94, Vancouver, Canada, December\n2004.\n[9] T. L. Gri ﬃths, M. Steyvers, and J. B. T enenbaum, “T opics in\nsemantic representation, ”Psychological Review , vol. 114, no. 2,\npp. 211–244, 2007.\n[10] X. Wang and A. McCallum, “A note on topical n-grams, ” T ech.\nRep. UM-CS-2005-071, University of Massachusetts, Amherst,\nMass, USA, December 2005.\n[11] M. Girolami and A. Kaban, “Simplicial mixtures of Markov\nchains: distributed modeling of dynamic user proﬁles, ” in\nAdvances in Neural Information Processing Systems 16 ,p p .9 –\n16, MIT Press, Vancouver, Canada, December 2003.\n[12] H. M. Wallach, “T opic modeling: beyond bag-of-words, ” in\nProceedings of the 23rd International Conference on Machine\nLearning (ICML ’06) , pp. 977–984, Pittsburgh, Pa, USA, June\n2006.\n[13] J. Nie, R. Li, D. Luo, and X. Wu, “Reﬁne bigram PLSA model\nby assigning latent topics unevenly, ” in Proceedings of the IEEE\nWorkshop on Automatic Speech Recognition and Understanding ,\npp. 141–146, Kyoto, Japan, 2007.\n[14] T. Hofmann, J. Puzicha, and M. I. Jordan, “Learning from\ndyadic data, ” in Advances in Neural Information Processing\nSystems 11 , pp. 466–472, Denver, Colo, USA, November-\nDecember 1998.\n[15] T. Hofmann, “Unsupervised learning by probabilistic latent\nsemantic analysis, ”Machine Learning, vol. 42, no. 1-2, pp. 177–\n196, 2001.\n[16] I. H. Witten and T. C. Bell, “The zero-frequency problem:\nestimating the probabilities of novel events in adaptive text\ncompression, ”IEEE Transactions on Information Theory , vol.\n37, no. 4, pp. 1085–1094, 1991.\n[17] S. M. Katz, “Estimation of probabilities from sparse data for\nthe language model component of speech recognizer, ” IEEE\nTransactions on Acoustics, Speech, and Signal Processing , vol. 35,\nno. 3, pp. 400–401, 1987.\n[18] W. Walker, P . Lamere, P . Kwok, et al., “Sphinx-4: a ﬂexible open\nsource framework for speech recognition, ” T ech. Rep. TR2004-\n0811, SUN Microsystems, November 2004.",
  "topic": "Bigram",
  "concepts": [
    {
      "name": "Bigram",
      "score": 0.9575986862182617
    },
    {
      "name": "Perplexity",
      "score": 0.9493681788444519
    },
    {
      "name": "Probabilistic latent semantic analysis",
      "score": 0.9355328679084778
    },
    {
      "name": "Computer science",
      "score": 0.7420432567596436
    },
    {
      "name": "Language model",
      "score": 0.5909624695777893
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5790322422981262
    },
    {
      "name": "Topic model",
      "score": 0.5422897934913635
    },
    {
      "name": "Natural language processing",
      "score": 0.5210132598876953
    },
    {
      "name": "Speech recognition",
      "score": 0.5058023929595947
    },
    {
      "name": "Latent semantic analysis",
      "score": 0.41314786672592163
    },
    {
      "name": "Word (group theory)",
      "score": 0.4104788601398468
    },
    {
      "name": "Linguistics",
      "score": 0.13690504431724548
    },
    {
      "name": "Trigram",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I133529467",
      "name": "Sharif University of Technology",
      "country": "IR"
    }
  ]
}