{
    "title": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise Perspective with Transformer",
    "url": "https://openalex.org/W3197957534",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2125160818",
            "name": "Haonan Wang",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A1995627859",
            "name": "Peng Cao",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2099575959",
            "name": "Jiaqi Wang",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2308328903",
            "name": "Osmar R. Zaïane",
            "affiliations": [
                "University of Alberta"
            ]
        },
        {
            "id": "https://openalex.org/A2125160818",
            "name": "Haonan Wang",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A1995627859",
            "name": "Peng Cao",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2099575959",
            "name": "Jiaqi Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2308328903",
            "name": "Osmar R. Zaïane",
            "affiliations": [
                "Northeastern University",
                "University of Alberta"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2517954747",
        "https://openalex.org/W3026315751",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6725739302",
        "https://openalex.org/W6769970663",
        "https://openalex.org/W2592905743",
        "https://openalex.org/W2964227007",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2998675048",
        "https://openalex.org/W2928133111",
        "https://openalex.org/W2979983945",
        "https://openalex.org/W6771153094",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W3034552520",
        "https://openalex.org/W3204166336",
        "https://openalex.org/W4308909683",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2502312327",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W3203480968",
        "https://openalex.org/W3204614423",
        "https://openalex.org/W3173693036",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3130695101",
        "https://openalex.org/W2981994674",
        "https://openalex.org/W2788906943",
        "https://openalex.org/W2432481613",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W3097510987",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2998027336",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W4294226146",
        "https://openalex.org/W4309504840",
        "https://openalex.org/W3195396028",
        "https://openalex.org/W4287112653",
        "https://openalex.org/W2288892845",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3137561054",
        "https://openalex.org/W3132503749",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3120857301",
        "https://openalex.org/W3015788359",
        "https://openalex.org/W4212875960",
        "https://openalex.org/W2949252853",
        "https://openalex.org/W3160284783",
        "https://openalex.org/W3203328008"
    ],
    "abstract": "Most recent semantic segmentation methods adopt a U-Net framework with an encoder-decoder architecture. It is still challenging for U-Net with a simple skip connection scheme to model the global multi-scale context: 1) Not each skip connection setting is effective due to the issue of incompatible feature sets of encoder and decoder stage, even some skip connection negatively influence the segmentation performance; 2) The original U-Net is worse than the one without any skip connection on some datasets. Based on our findings, we propose a new segmentation framework, named UCTransNet (with a proposed CTrans module in U-Net), from the channel perspective with attention mechanism. Specifically, the CTrans (Channel Transformer) module is an alternate of the U-Net skip connections, which consists of a sub-module to conduct the multi-scale Channel Cross fusion with Transformer (named CCT) and a sub-module Channel-wise Cross-Attention (named CCA) to guide the fused multi-scale channel-wise information to effectively connect to the decoder features for eliminating the ambiguity. Hence, the proposed connection consisting of the CCT and CCA is able to replace the original skip connection to solve the semantic gaps for an accurate automatic medical image segmentation. The experimental results suggest that our UCTransNet produces more precise segmentation performance and achieves consistent improvements over the state-of-the-art for semantic segmentation across different datasets and conventional architectures involving transformer or U-shaped framework. Code: https://github.com/McGregorWwww/UCTransNet.",
    "full_text": "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-Wise\nPerspective with Transformer\nHaonan Wang1,2, Peng Cao1,2*, Jiaqi Wang1,2, Osmar R. Zaiane3\n1 Computer Science and Engineering, Northeastern University, Shenyang, China\n2 Key Laboratory of Intelligent Computing in Medical Image of Ministry of Education, Northeastern University, China\n3 Amii, University of Alberta, Edmonton, Canada\nhaonan1wang@gmail.com, caopeng@mail.neu.edu.com, wjq010222@gmail.com, zaiane@cs.ualberta.ca\nAbstract\nMost recent semantic segmentation methods adopt a U-Net\nframework with an encoder-decoder architecture. It is still\nchallenging for U-Net with a simple skip connection scheme\nto model the global multi-scale context: 1) Not each skip\nconnection setting is effective due to the issue of incompat-\nible feature sets of encoder and decoder stage, even some\nskip connection negatively inﬂuence the segmentation per-\nformance; 2) The original U-Net is worse than the one with-\nout any skip connection on some datasets. Based on our ﬁnd-\nings, we propose a new segmentation framework, named UC-\nTransNet (with a proposed CTrans module in U-Net), from\nthe channel perspective with attention mechanism. Specif-\nically, the CTrans (Channel Transformer) module is an al-\nternate of the U-Net skip connections, which consists of a\nsub-module to conduct the multi-scale Channel Cross fusion\nwith Transformer (named CCT) and a sub-module Channel-\nwise Cross-Attention (named CCA) to guide the fused multi-\nscale channel-wise information to effectively connect to the\ndecoder features for eliminating the ambiguity. Hence, the\nproposed connection consisting of the CCT and CCA is able\nto replace the original skip connection to solve the semantic\ngaps for an accurate automatic medical image segmentation.\nThe experimental results suggest that our UCTransNet pro-\nduces more precise segmentation performance and achieves\nconsistent improvements over the state-of-the-art for seman-\ntic segmentation across different datasets and conventional\narchitectures involving transformer or U-shaped framework.\nCode: https://github.com/McGregorWwww/UCTransNet.\nIntroduction\nMedical imaging is considered as a vital technique to assist\ndoctors to evaluate disease and to optimise prevention and\ncontrol measures. Segmentation and the subsequent quanti-\ntative assessment of target object in medical images provide\nvaluable information for the analysis of pathologies and are\nimportant for planning of treatment strategies, monitoring\nof disease progression and prediction of patient outcome.\nRecent approaches (Long, Shelhamer, and Darrell 2015; Ni\net al. 2020; Wen, Xie, and He 2020) to semantic segmenta-\ntion typically rely on convolutional encoder-decoder archi-\ntectures where the encoder generates low-resolution image\n*Corresponding author.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n(a) U-Net (c) MultiResUNet (d) UCTransNet(b) UNet++\nCTrans\nRes Path\nRes Path\nRes Path\nRes Path\nFigure 1: Comparison of the skip connection scheme among\nthe proposed UCTransNet (d) and other models. The dash\nlines denote the skip connections.\nfeatures and the decoder up-samples features to segmenta-\ntion maps with per pixel class scores. U-Net (Ronneberger,\nFischer, and Brox 2015) is the most widely used encoder-\ndecoder network architecture for medical image segmenta-\ntion, since the encoder captures the low-level and high-level\nfeatures, and the decoder combines the semantic features to\nconstruct the ﬁnal result. The skip connection can help prop-\nagate the spatial information that gets lost during the pooling\noperation to help recover the full spatial resolution through\nthe encoding-decoding process. To investigate it, we conduct\nan in-depth study of U-Net and observe several major lim-\nitations according to our analysis on multiple datasets. We\nﬁnd that it is still challenging for U-Net with a simple skip\nconnection scheme to model the global multi-scale context\nfor assisting the decoding process without considering the\nsemantic gap. It is necessary to ﬁnd an effective way to\nfuse features for precise medical image segmentation. There\nessentially are two key issues for the extension of U-Net:\nwhich layers of the features in the encoders are connected\nto the decoders for modeling a global contexts through ag-\ngregating multi-scale features, and how to effectively fuse\nthe features with possible semantic gap instead of simply\nconcatenating? There exist two semantic gaps: semantic gap\namong the multi-scale encoder features and between the\nstages of the encoder and decoder, limiting the segmenta-\ntion performance. To overcome this aforementioned limita-\ntion, a number of approaches have been introduced recently\nto alleviate the discrepancy when fusing these two incompat-\nible sets of features. One approach is to directly replace the\nplain skip connections with the nested dense skip pathways\nfor medical image segmentation. The most representative\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2441\n \nSigmoid\nConv 1×1\nCCT \nCCT \n…… L ×\nMaxPool+Convs Concat+ \nUpSample+ConvsEmbedd  Reconstruct+ \n Channel-wise Cross Attention\nMulti-head  \nCross-Attention\nMLP MLP MLP MLP \nConcat\nLN LN LN LN \nLN LN LN LN \n1×1×C \n 1×1×C \nGAP GAP\nD i \nCCA \nH×W×C H×W×C \nT okens \nUpSample+Conv \nLinear \n+Sigmoid\n1×1×C \n 1×1×C \nFigure 2: Illustration of the proposed UCTransNet. We replace the original skip connections by CTrans consisting of two\ncomponents: Channel-wise Cross fusion Transformer (CCT) and Channel-wise Cross Attention (CCA).\nmethod is UNet++ (Zhou et al. 2018) which narrows the se-\nmantic gap between the encoder and decoder sub-networks\nby introducing dense connectivity with a series of convolu-\ntions and achieves better segmentation performance. It is an\nimprovement over the restrictive skip connections in U-Net\nrequiring the fusion of only same-scale feature maps. The\nother approach focuses on strengthening the skip connec-\ntions by introducing additional non-linear transformations\non the features propagating from the encoder stage, which\nshould account for or somewhat balance the possible seman-\ntic gaps (Rahman 2020).\nDespite achieving good performance, both works above\nare still incapable of effectively exploring sufﬁcient infor-\nmation from full scales. Capturing multi-scale features is\nessential for resolving complex scale variations in medical\nimage segmentation. Driven by the important issues, a ques-\ntion arises: how to sufﬁciently bridge the semantic gap be-\ntween the encoder and decoder through multi-scale channel-\nwise information fusion by effectively capturing the non-\nlocal semantic dependencies. In this paper, we rethink the\nskip connection design and propose an alternative method\nfor better connecting the features between the encoder and\ndecoder stages. Different channels usually focus on different\nsemantic patterns, adaptively fusing sufﬁcient channel-wise\nfeatures is favorable for the complex medical image segmen-\ntation. To this end, we propose an end-to-end deep learning\nnetwork called UCTransNet, which takes U-Net as the main\nstructure of the network. More speciﬁcally, we ﬁrstly pro-\npose a Channel-wise Cross Fusion Transformer (CCT) to\nfuse the multi-scale context with cross attention from the\nchannel-wise perspective. It aims at capturing local cross-\nchannel interaction to achieve an adaptive scheme for ef-\nfectively fusing the multi-scale channel-wise features with\npossible scale semantic gap through collaborative learning\nrather than independent connection. On the other hand, we\npropose another channel-wise cross attention (CCA) mod-\nule for fusing the fused multi-scale features and the features\nfrom decoder stages to solve the inconsistent semantic level.\nBoth cross attention modules are called CTrans (Channel\nTransformer), which can establishes the association between\nencoder and decoder by exploring the multi-scale global\ncontext and replace the original skip connections to solve\nthe semantic gaps for improved segmentation performances.\nBoth proposed modules can be easily embedded in and ap-\nplied for the U-shape networks in medical image segmen-\ntation tasks. Extensive experiments show that UCTransNet\ncan greatly improve conventional segmentation pipelines by\nthe following absolute gains of 4.05% Dice, 7.98% Dice and\n9.00% Dice over U-Net on GlaS, MoNuSeg and Synapse\ndatasets, respectively. Moreover, we made a thorough anal-\nysis to investigate how the feature interactions work. Be-\nsides, previous works have combined both Transformers and\nU-Net to explicitly model long-range spatial dependency\n(Chen et al. 2021; Zhang, Liu, and Hu 2021). The results\ndemonstrate that channel-wise fusion transformer scheme\ngenerally leads to a better performance than the methods in-\ncorporating the transformer to replace the convolution oper-\nation. We argue that UCTransNet can serve as strong skip\nconnection scheme for medical image segmentation.\nOur contributions are three-fold. 1) Our study is the ﬁrst\nwork that sufﬁciently explores the potential weakness of\nskip connections in U-Net on multiple datasets and ﬁnds that\nthe independent simple copying is not appropriate. 2) We\nsuggest a new perspective to boost semantic segmentation\n2442\nperformance, i.e. bridging the semantic and resolution gap\nbetween low-level and high-level features by a more effec-\ntive feature fusion with multi-scale channel-wise cross atten-\ntion for capturing more sophisticated channel-wise depen-\ndencies. 3) Our method is a more appropriate combination\nof U-Net and Transformer with less computational cost and\nhigher performance. In comparison to other state-of-the-art\nsegmentation methods, the experimental results present bet-\nter performances on all the three public datasets.\nRelated Works\nTransformers for Medical Image Segmentation\nRecently, Vision Transformer (ViT) (Dosovitskiy et al.\n2020) achieved state-of-the-art on ImageNet classiﬁcation\nby directly applying Transformers with global self-attention\nto full-sized images. Due to the success of Transformers\nin many computer vision ﬁelds, a new paradigm for med-\nical image segmentation has recently evolved(Zheng et al.\n2020; Zhang et al. 2021; Gao et al. 2021; Ji et al. 2021;\nGao, Zhou, and Metaxas 2021; Zhang, Liu, and Hu 2021;\nHatamizadeh et al. 2021). TransUNet (Chen et al. 2021)\nis the ﬁrst Transformer-based medical image segmenta-\ntion framework. Valanarasu et al. proposed a Gated Axial-\nAttention model–MedT (Valanarasu et al. 2021) to over-\ncome the low number of data samples in medical imag-\ning. Motivated by Swin Transformer (Liu et al. 2021) which\nachieved state-of-the-art performance, Swin-Unet (Cao et al.\n2021) proposed the ﬁrst pure Transformer-based U-shaped\narchitecture which introduced Swin Transformer to replace\nthe convolution blocks in U-Net. However, the aforemen-\ntioned methods mainly focus on the defects of convolution\noperation rather than the U-Net it-self, thus may cause struc-\ntural redundancy and prohibitive computational cost.\nSkip Connections in U-shaped Nets\nThe skip connection mechanism was ﬁrst proposed in U-Net\n(Ronneberger, Fischer, and Brox 2015), which was designed\nto bridge the semantic gap between encoder and decoder,\nand have proven to be effective in recovering ﬁne-grained\ndetails of the target objects (Drozdzal et al. 2016; He et al.\n2016; Huang et al. 2017). Following the popularity of U-Net,\nmany novel models have been proposed such as UNet++\n(Zhou et al. 2018), Attention U-Net (Oktay et al. 2018),\nDenseUNet (Li et al. 2018), R2U-Net (Alom et al. 2018),\nand UNet 3+ (Huang et al. 2020), which are specially de-\nsigned for medical image segmentation and achieve expres-\nsive performance (see Fig, 1). Zhou et al. (Zhou et al. 2018)\nbelieved that the same-scale feature maps from the encoder\nand decoder networks are semantically dissimilar and thus\ndesigned a nested structure named UNet++ which captures\nmulti-scale features to further bridge the gap. Attention-\nUNet proposed cross-attention module which uses coarse-\nscale features as gating signals to disambiguate irrelevant\nand noisy responses in skip connections. MultiResUNet\n(Rahman 2020) observed a possible semantic gap between\nthe skipped encoder features and the decoder features in the\nsame level, thus they introduced the Res Path with residual\nstructure to improve the skip connections (see Fig. 1).\n71.77 \n66.55 \n71.11 \n70.95 \n76.53 \n74.01 \n72.57 \n74.06 \n71.64 \n70.69\n85.45 \n87.29 \n85.71 86.34 \n87.52 86.78 86.56\n85.31 85.10 \n86.64\n76.45 \n64.18 \n67.50 \n76.44 \n74.47 \n71.76 \n76.35 \n72.88 \n74.99 \n77.60 \n64\n69\n74\n79\n84\nAll None L1 L2 L3 L4 w/o L1w/o L2w/o L3w/o L4\nDice Value [%]\nSkip connection settings\nSynapse\nGlaS\nMoNuSeg\nFigure 3: Analysis of different skip connection layers of U-\nNet. ‘All’ represents the original U-Net, ‘L1’ represents only\nthe skip connection of level one is kept and ‘w/o L1’ repre-\nsents only the skip connection of level one is removed.\nThese methods assume that each skip connection has\nequal contribution, however in the next section we will show\nthat the contributions are different among all the skip con-\nnections, some may even harm the ﬁnal performance.\nThe Analysis of Skip Connection\nIn this section, we thoroughly analyze the contribution of the\nskip connection to the segmentation performance on three\ndatasets. According to the analysis, three ﬁndings are high-\nlighted as follows:\nFinding 1: The U-Net without any skip connection is\neven better than the original U-Net. Comparing the results\nof Fig. 3, we can ﬁnd that ‘U-Net-none’ shows the worst\nperformance among the algorithms for almost all metrics\non the MoNuSeg dataset. However, ‘U-Net-none’, although\nwithout any constraints, still achieves very competitive per-\nformance against ‘U-Net-all’ on the GlaS dataset. It demon-\nstrates that the skip connection is not always beneﬁcial for\nthe segmentation.\nFinding 2: Although UNet-all performs better than UNet-\nnone, not all skip connections with simple copying are use-\nful for segmentation. The contribution of each skip connec-\ntion is different. We ﬁnd that the performance range of each\nskip connection is [67.5%,76.44%] and [52.2%,62.73%]\nwith respect to Dice and IOU on the MoNuSeg dataset. The\nimpact variation is large for the different single skip con-\nnection. Furthermore, due to the issue of incompatible fea-\nture sets of the encoder and decoder stages, some skip con-\nnection negatively inﬂuence the segmentation performance.\nFor example, L1 performs worse than UNet-none in terms\nof Dice and IOU on the GlaS dataset. The result does not\ndemonstrate that many features from the encoder stage are\nnot informative. The reason behind it may be that the simple\ncopying is not appropriate for the feature fusion.\nFinding 3: The optimal combination of skip contribu-\ntions is different for different datasets, which depends on\nthe scales and appearance of the target lesions. We run sev-\neral ablation experiments to explore the best side output set-\ntings. Note that we ignore the combination of two skip con-\n2443\n1 3 \n16 \n1 \npatches \nchannels \n1 16 7 \npatches \nchannels \n7 \n1 \n3 \n(a) \n(b) \nFigure 4: Comparison between the original self-attention (a)\nand our proposed channel-wise cross-attention (b).\nnections due to the limited space. As can be seen, the skip\nconnections does not achieve better performance. The model\nw/o L4 is best on the MoNuSeg dataset, whilst to our sur-\nprise, the L3 with only one skip connection performs best\non the GlaS dataset. These observations suggest that the op-\ntimal combination is different for different datasets which\nfurther conﬁrms the necessity of introducing more appropri-\nate course of action for the feature fusion rather than simple\nconnection.\nUCTransNet for Medical Image Segmentation\nFig. 2 illustrates an overview of our UCTransNet frame-\nwork. To the best of our knowledge, current Transformer-\nbased segmentation methods mainly focus on improving\nthe encoder of U-Net, based on its advantage of captur-\ning long-range information. These methods, such as Tran-\nsUNet (Chen et al. 2021) or TransFuse (Zhang, Liu, and\nHu 2021), blend the Transformer with U-Net in a simple\nway, i.e. plugging the Transformer module into the encoder\nor fusing the both independent branches. However, we be-\nlieve the potential limitation of the current U-Net model is\nthe issue of the skip connection rather than the encoder of\nthe original U-Net, which is sufﬁcient for the most tasks.\nAs mentioned in the section of skip connection analysis,\nwe observe that the feature from the encoder is inconsistent\nwith that from the decoder, i.e. in some cases, the shallower\nlayer features with less semantic information may harm the\nﬁnal performance through the simple skip connection due\nto the semantic gap between the shallower level encoder\nand decoder. Inspired by it, we construct the UCTransNet\nframework by designing a channel-wise Transformer mod-\nule between the vanilla U-Net encoder and decoder to bet-\nter fuse the encoder features and reduce the semantic gap.\nSpeciﬁcally, we propose a Channel Transformer (CTrans)\nto replace the skip connections in U-Net, which consists\nof two modules: CCT (Channel-wise Cross Fusion Trans-\nformer) for the multi-scale encoder feature fusion and CCA\n(Channel-wise Cross Attention) for the fusion of the decoder\nfeatures and the enhanced CCT features.\nMulti-head \nCross-Attention\n… \n… \n… \n… \n…… \nO 4 … \n… \n… \n… \nd \nd \nd \nd \nO 3 \nO 2 \nO 1 \n…… \n… \n…… \n: Sigmoid Function\n: Instance Normalization\nFigure 5: Multi-head Cross-Attention.\nCCT: Channel-wise Cross Fusion Transformer for\nEncoder Feature Transformation\nTo solve the skip connection issue mentioned before, we\npropose a new Channel-wise Cross Fusion Transformer\n(CCT) to fuse the multi-scale encoder features with the ad-\nvantage of the long dependency modeling in Transformer.\nThe CCT module consists of three steps: multi-scale fea-\nture embedding, multi-head channel-wise cross attention\nand Multi-Layer Perceptron (MLP).\nMulti-scale Feature Embedding Given the outputs of\nfour skip connection layers Ei 2R\nHW\ni2 \u0002Ci;(i= 1;2;3;4),\nwe ﬁrst perform tokenization by reshaping the features\ninto sequences of ﬂattened 2D patches with patch sizes\nP;P\n2 ;P\n4 ;P\n8 respectively, so that the patches can be mapped\nto the same areas of the encoder features in four scales.\nWe keep the original channel dimensions through this pro-\ncess. Then, we concatenate the tokens of four layers Ti(i=\n1;2;3;4);Ti 2 R\nHW\ni2 \u0002Ci as the key and value T\u0006 =\nConcat(T1;T2;T3;T4).\nMulti-head Cross-Attention The tokens are then fed into\nthe multi-head channel cross-attention module, followed by\na Multi-Layer Perceptron (MLP) with residual structure, to\nencode channel and dependencies for reﬁning features from\neach U-Net encoder level using multi-scale features.\nAs shown in Fig. 5, the proposed CCT module contains\nﬁve inputs, including four tokens Ti as queries and a con-\ncatenated token T\u0006 as key and value:\nQi = TiWQi;K = T\u0006WK;V = T\u0006WV (1)\nwhere WQi 2RCi\u0002d;WK 2RC\u0006\u0002d;WV 2RC\u0006\u0002d are\nweights of different inputs, dis the sequence length (patch\nnumbers) and Ci(i = 1;2;3;4) are the channel dimensions\nof the four skip connection layers. In our implementation\nC1 = 64;C2 = 128;C3 = 256;C4 = 512.\nWith Qi 2RCi\u0002d;K 2RC\u0006\u0002d;V 2RC\u0006\u0002d, the simi-\nlarity matrix Mi are produced and the value V is weighted\n2444\nby Mi through a cross-attention (CA) mechanism:\nCAi = MiV>= \u001b\n\u0014\n \n\u0012Q>\ni KpC\u0006\n\u0013\u0015\nV>\n= \u001b\n\"\n \n \nW>\nQiT>\ni T\u0006WK\npC\u0006\n!#\nW>\nV T>\n\u0006\n(2)\nwhere  (\u0001) and \u001b(\u0001) denote the instance normaliza-\ntion(Ulyanov, Vedaldi, and Lempitsky 2017) and the soft-\nmax function, respectively.\nThe major difference from the original self-attention is\nthat we conduct the attention operation along the channel-\naxis rather than the patch-axis (see Fig. 4), and we employ\nthe instance normalization which can normalize the similar-\nity matrix for each instance on the similarity maps so that\nthe gradient can be smoothly propagated. In aN-head atten-\ntion situation, the output after multi-head cross-attention is\ncalculated as follow:\nMCAi = (CA1\ni + CA2\ni +;:::; +CAN\ni )=N (3)\nwhere N is the number of heads. Hereinafter, applying a\nMLP and residual operator, the output is obtained as follows:\nOi = MCAi + MLP(Qi + MCAi) (4)\nWe omitted the layer normalization (LN) in the equation for\nsimplicity. The operation in Eq. (4) is repeated L times to\nbuild a L-layer Transformer. In our implementation, N and\nLare both set to 4, based on series of experiments with 2, 4,\n8 and 12 layers for CCT and we empirically ﬁnd the one with\n4 layers and 4 heads can achieve the best performance on the\nthree datasets. Finally, the four outputs of theL-th layer O1,\nO2, O3 and O4 are reconstructed though an up-sampling\noperation followed by a convolution layer and concatenated\nwith the decoder features D1, D2, D3 and D4, respectively.\nCCA: Channel-wise Cross Attention for Feature\nFusion in Decoder\nIn order to better fuse features of inconsistent semantics be-\ntween the Channel Transformer and U-Net decoder, we pro-\npose a channel-wise cross attention module, which can guide\nthe channel and information ﬁltration of the Transformer\nfeatures and eliminate the ambiguity with the decoder fea-\ntures.\nMathematically, we take the i-th level Transformer out-\nput Oi 2 RC\u0002H\u0002W and i-th level decoder feature map\nDi 2RC\u0002H\u0002W as the inputs of Channel-wise Cross Atten-\ntion. spatial squeeze is performed by a global average pool-\ning (GAP) layer, producing vectorG(X) 2RC\u00021\u00021 with its\nkth channel G(X) = 1\nH\u0002W\nPH\ni=1\nPW\nj=1 Xk(i;j). We use\nthis operation to embed the global spatial information and\nthen generate the attention mask:\nMi = L1 \u0001G(Oi) +L2 \u0001G(Di) (5)\nwhere L1 2 RC\u0002C and L2 2 RC\u0002C and being weights\nof two Linear layers and the ReLU operator \u000e(\u0001). This op-\neration in Eq. (5) encodes the channel-wise dependencies.\nFollowed ECA-Net (Wang et al. 2020) which empirically\nMethod Param GlaS MoNuSeg\n(M) Dice (%) IoU (%) Dice (%) IoU (%)\nU-Net 14.8 85.45\u00061.3 74.78\u00061.7 76.45 \u00062.6 62.86\u00063.0\nUNet++ 74.5 87.56\u00061.2 79.13\u00061.7 77.01 \u00062.1 63.04\u00062.5\nAttUNet 34.9 88.80\u00061.1 80.69\u00061.7 76.67\u00061.1 63.47\u00061.2\nMRUNet 57.2 88.73\u00061.2 80.89\u00061.7 78.22\u00062.5 64.83\u00062.9\nTransUNet 105 88.40\u00060.7 80.40\u00061.0 78.53\u00061.1 65.05\u00061.3\nMedT 98.3 85.92\u00062.9 75.47\u00063.5 77.46\u00062.4 63.37\u00063.1\nSwin-Unet 82.3 89.58\u00060.6 82.06\u00060.7 77.69\u00060.9 63.77\u00061.2\nOurs 65.6 90.18\u00060.7\u0003 82.96\u00061.1\u0003 79.08\u00060.7 65.50\u00060.9\nTable 1: The three times 5-fold cross validation results on\nGlaS and MoNuSeg datasets. The Dice and IoU are in\n‘mean\u0006std’ format. Symbol\u0003indicates that our method sig-\nniﬁcantly outperformed others on that score (Student’s t-test\nat a level of 0.05 is used).\nMethods Dice\" HD#\nV-Net (2016) 68.81 -\nDARR (2020) 69.77 -\nU-Net (2015) 71.77 53.04\nR50-U-Net 74.68 36.87\nR50-AttUNet (2018) 75.57 36.97\nTransUNet (2021) 77.48 31.69\nSwin-Unet (2021) 79.13 21.55\nUCTransNet (w/o CCA) 78.99 30.29\nUCTransNet-pre 75.54 38.97\nUCTransNet 78.23 26.75\nTable 2: Comparison with state-of-the-art segmentation\nmethods on Synapse dataset. For simplicity, ‘R50-U-Net’\nand ‘R50-AttUNet’ denote U-Net and Attention U-Net with\nResNet-50 as backbone, respectively.\nshowed avoiding dimensionality reduction is important for\nlearning channel attention, we use a single Linear layer\nand sigmoid function to build the channel attention map.\nThe resultant vector is used to recalibrate or excite Oi to\n^Oi = \u001b(Mi) \u0001Oi, where the activation \u001b(Mi) indicates the\nimportance of channels. Finally, the masked ^Oi is concate-\nnated with the up-sampled features of the i-th level decoder.\nExperiments\nDatasets\nWe use Gland segmentation (Sirinukunwattana et al. 2016),\nMoNuSeg (Kumar et al. 2017, 2020) and Synapse multi-\norgan segmentation dataset (Bennett et al. 2015) to evaluate\nour method. Gland segmentation dataset (GlaS) has 85 im-\nages for training and 80 for testing. MoNuSeg dataset has\n30 images for training and 14 for testing. Synapse has 30\nabdominal CT scans in 8 abdominal organs (aorta, gallblad-\nder, spleen, left kidney, right kidney, liver, pancreas, spleen,\nstomach), with 3779 axial CT images in total. Following\n(Chen et al. 2021), we use a random split of 18 training cases\n(2212 axial slices) and 12 cases for validation.\n2445\nMedTUCT ransNet MultiResUNet SwinUNetTransUNetU-Net\nGround Truth\nOriginal Image\nFigure 6: The qualitative comparison on the GlaS and MoNuSeg datasets.\nUCT ransNet TransUNetU-NetGround Truth\ngallbladder left kidney right kidney\nstomachspleenpancreasliver\naorta \nFigure 7: The qualitative comparison on the Synapse dataset.\nImplementation Details\nWe implemented our model with PyTorch on a single\nNVIDIA A40 GPU card with 48 GB memory. To avoid over-\nﬁtting, we also performed two kinds of online data augmen-\ntations, including horizontal ﬂipping, vertical ﬂipping and\nrandom rotating. We do not use any pre-trained weights to\ntrain the proposed UCTransNet. For GlaS and MoNuSeg we\nset the batch size to 4 following (Valanarasu et al. 2021), and\nfor Synapse we set it to 24 following (Chen et al. 2021). The\ninput resolution and patch size P are set as 224 \u0002224 and\n16 for all the three datasets. We employ the Adam optimizer\nto train our model, where the initial learning rate is set to\n0.001. We also employ the combined cross entropy loss and\ndice loss as our loss function to train our network. To make\nthe results on the small datasets more convincing, we con-\nduct a three times 5-fold cross validation (totally 15 CV),\nand obtain the mean result and std. A statistical test is used\nto indicate our method signiﬁcantly outperforms the compa-\nrable methods. For GlaS and MoNuSeg datesets we use dice\ncoefﬁcient (Dice) and Intersection over Union (IoU) as the\nevaluation metrics while for Synapse we report the Dice and\nHausdorff Distance (HD). Note that we use the same settings\nand loss function for training all the baselines.\nComparison with State-of-the-art Methods\nTo demonstrate the overall segmentation performance of the\nproposed UCTransNet, we compare it with other state-of-\nthe-art methods. We compare UCTransNet with two types\nof methods for comprehensive evaluation, covering three\nUNet based method: UNet++, Attention U-Net, MultiRe-\nsUNet and three state-of-the-art transformer based segmen-\ntation methods, including TransUNet, MedT, and Swin-\nUnet. To make a fair comparison, their originally released\ncodes and published settings are used in the experiment. We\nalso introduce two strategies to optimize the models of UC-\nTransNet. 1) Jointly training: We optimize the convolution\nand CTrans parameters in U-Net and the two channel-wise\ncross attention parameters together with a single loss; 2)\nPre-training. We ﬁrst train a U-Net, then the parameters in\nUCTransNet are further trained with the same data.\nExperimental results are reported in Table 1 where the\nbest results are boldfaced. Table 1 shows that our method\nhas consistent improvements over prior arts. In Table 2, sim-\nilar observations and conclusions can be made, which once\nagain validates that UCTransNet outperforms all others. Ad-\nditionally, the pre-training scheme not only achieves a faster\nconvergence speed, but also obtains a better performance\nthan the competing methods, even outperforms the jointly\nlearning scheme on the MoNuSeg dataset. These observa-\ntions suggest that the two proposed modules can be incor-\nporated into the pre-trained U-Net model for improved seg-\nmentation performance. We also provide the parameter num-\nbers which show that our model achieves a good trade-off\nbetween effectiveness and efﬁciency.\nWe visualize the segmentation results of the comparable\nmodels in Fig. 6 and Fig. 7. The red boxes highlight regions\nwhere UCTransNet performs better than the other methods.\nIt shows that our UCTransNet generates better segmentation\nresults, which are more similar to the ground truth than the\nresults of the baseline model. It can be easily seen that our\nproposed method not only highlights the right salient regions\n2446\nMethod GlaS MoNuSeg\nDice(%) IoU(%) Dice(%) IoU(%)\nBaseline (U-Net) 85.45 74.78 76.45 62.86\nBaseline+CCT 89.09 80.78 79.31 65.97\nBaseline+CCA 87.09 78.10 76.84 63.85\nBaseline+CCT+CCA 89.84 82.24 79.87 66.68\nTable 3: Ablation experiments on GlaS and MoNuSeg\ndatasets. ‘CCT’ denotes the proposed Channel Transformer\nand ‘CCA’ denotes Channel-wise Cross Attention. The best\nresults are boldfaced.\n73\n75\n77\n79\n81\n83\n85\n87\n89\n91\nGlaS MoNuSeg\nDice Value [%]\nAblation on Number of Queries and Keys Q1\nQ2\nQ3\nQ4\nQ12\nQ34\nQ123\nQ234\nK1\nK12\nK123\nK34\nK234\nCTranSQueries QueriesKeys Keys\nFigure 8: Ablation study of the number of queries and keys\non GlaS dataset and MoNuSeg dataset.\neliminating the confusing false positive lesions but also pro-\nduces coherent boundaries. These observations suggest that\nUCTransNet is capable of ﬁner segmentation while preserv-\ning detailed shape information.\nAblation Studies\nAblation Studies on Proposed ModulesAs shown in Ta-\nble 3, ‘Base+CCT+CCA’ is generally better than the other\nbaselines on all datasets, which indicates the effectiveness of\ncombination of the two modules. Our results shed new light\non the importance of multi-scale multi-channel feature fu-\nsion in encoder-decoder framework for improving segmen-\ntation performance.\nAblation Studies on the Number of Queries and Keys\nThe previous experiments demonstrate that the CCT mod-\nule in our model is effective for enhancing the skip connec-\ntions. In the previous experiments, the multi-scale features\nfrom all encoder levels engage into the CCT module, thus\nthe number of queries is 4 and the key is the concatenated\nrepresentation consisting of the four scale features.\nWe perform a series of experiments with respect to the\namount of the skip connections between encoders and de-\ncoders as illustrated in Fig. 8. Note that the key vector is\nﬁxed, which is still consisting of the four scale features.\nWe observe consistent improvements with the increase of\nthe number of skip connections. The observation implies\nthe usefulness of multi-scale features learned by different\nencoder levels, which validates our motivation. It is inter-\nesting that ‘Q234’ is slightly better than our model with\n1.0 \n(a) GlaS (b) MoNuSeg\nK 1 K 2 K 3 K 4 \nQ 1 Q 2 Q 3 Q 4 \nK 1 K 2 K 3 K 4 \nQ 1 Q 2 Q 3 Q 4 \n0.0 \n1.0 \n0.0 \nFigure 9: Similarity Matrix of GlaS dataset (a) and\nMoNuSeg dataset (b). ‘K 1’ denotes the same feature as\n‘Q1’ concatenated in key.\nall skip connections. Besides, we also keep the number of\nqueries ﬁxed and vary the key to verify concatenating multi-\nscale features. From Fig. 8, it can be found that the per-\nformance improves with the scale of feature increasing un-\ntil four scales, which demonstrates that more channels help\ncapture accurate node features, which implies that trans-\nforming more scales of features to queries is better.\nThe Cross Attention Matrix in CCT Module To per-\nform a thorough evaluation of our UCTransNet, we visualize\nthe cross attention distributions in our CCT module in Fig. 9.\nIt is also interesting to investigate which encoder level has\nmore conﬁdent correlation and is more important for seg-\nmentation. It can be seen that the ‘K 2’ and ‘K3’ have a\nmore conﬁdent correlation with the other encoder level on\nthe GlaS and MoNuSeg dataset, respectively. The ﬁndings\nare consistent with the skip connection analysis in U-Net\nin Fig. 3. It explains why ‘L3’ and ‘L2’ achieve better per-\nformances on GlaS and MoNuSeg dataset, respectively. The\nfact implies the necessity to develop a multi-scale feature\nfusion to tackle the semantic gap problems, which also val-\nidates our motivation to build a global multi-scale channel-\nwise feature fusion model for effectively capturing the non-\nlocal semantic dependencies.\nConclusion\nAccurate and automatic segmentation of medical images is\na crucial step for clinical diagnosis and analysis. In this\nwork, we introduced a Channel Transformer Segmenta-\ntion network (UCTransNet) from the channel-wise perspec-\ntive to provide precise and reliable automatic segmentation\nof medical images. By combining the strengths of multi-\nscale Channel-wise Cross fusion Transformer (CCT) and re-\ncurrent neural networks and Channel-wise Cross-Attention\n(CCA) in an end-to-end manner, the proposed approach sig-\nniﬁcantly improves the state-of-the-art results in medical im-\nage segmentation on multiple benchmark datasets. With in-\ndepth analysis and empirical evidence, we show the advan-\ntages of the UCTransNet model. It indeed successfully nar-\nrows the semantic gap and takes full advantage of the multi-\nscale features in the encoding stage.\n2447\nAcknowledgements\nThis research was supported by the National Natural Science\nFoundation of China (No.62076059), the Fundamental Re-\nsearch Funds for the Central Universities (No. N2016001)\nand the Science Project of Liaoning province (2021-MS-\n105).\nReferences\nAlom, M. Z.; Hasan, M.; Yakopcic, C.; Taha, T. M.; and\nAsari, V . K. 2018. Recurrent Residual Convolutional Neu-\nral Network Based on U-Net (R2U-Net) for Medical Image\nSegmentation. arXiv:1802.06955.\nBennett, L.; Zhoubing, X.; Juan, I., Eugenio; Martin, S.;\nThomas, L., Robin; and Arno, K. 2015. Segmentation\nOutside the Cranial Vault Challenge. In MICCAI: Multi-\nAtlas Labeling Beyond Cranial Vault-Workshop Challenge\n(2015).\nCao, H.; Wang, Y .; Chen, J.; Jiang, D.; Zhang, X.; Tian, Q.;\nand Wang, M. 2021. Swin-Unet: Unet-like Pure Transformer\nfor Medical Image Segmentation. arXiv:2105.05537.\nChen, J.; Lu, Y .; Yu, Q.; Luo, X.; Adeli, E.; Wang, Y .; Lu,\nL.; Yuille, A. L.; and Zhou, Y . 2021. TransUNet: Transform-\ners Make Strong Encoders for Medical Image Segmentation.\narXiv:2102.04306.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2020.\nAn Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. arXiv:2010.11929.\nDrozdzal, M.; V orontsov, E.; Chartrand, G.; Kadoury, S.;\nand Pal, C. 2016. The Importance of Skip Connections in\nBiomedical Image Segmentation. In Carneiro, G.; Mateus,\nD.; Peter, L.; Bradley, A.; Tavares, J. M. R. S.; Belagiannis,\nV .; Papa, J. P.; Nascimento, J. C.; Loog, M.; Lu, Z.; Cardoso,\nJ. S.; and Cornebise, J., eds., Deep Learning and Data La-\nbeling for Medical Applications, volume 10008, 179–187.\nCham: Springer International Publishing. ISBN 978-3-319-\n46975-1 978-3-319-46976-8.\nFu, S.; Lu, Y .; Wang, Y .; Zhou, Y .; Shen, W.; Fishman,\nE.; and Yuille, A. 2020. Domain adaptive relational rea-\nsoning for 3d multi-organ segmentation. In Medical Im-\nage Computing and Computer Assisted Intervention – MIC-\nCAI 2020 - 23rd International Conference, Proceedings ,\n656–666. Springer Science and Business Media Deutsch-\nland GmbH.\nGao, Y .; Zhou, M.; and Metaxas, D. 2021. UTNet: A Hybrid\nTransformer Architecture for Medical Image Segmentation.\narXiv:2107.00781.\nGao, Z.; Hong, B.; Zhang, X.; Li, Y .; Jia, C.; Wu, J.; Wang,\nC.; Meng, D.; and Li, C. 2021. Instance-Based Vision Trans-\nformer for Subtyping of Papillary Renal Cell Carcinoma in\nHistopathological Image. arXiv:2106.12265.\nHatamizadeh, A.; Yang, D.; Roth, H.; and Xu, D. 2021. UN-\nETR: Transformers for 3D Medical Image Segmentation.\narXiv:2103.10504.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 770–\n778. Las Vegas, NV , USA: IEEE. ISBN 978-1-4673-8851-1.\nHuang, G.; Liu, Z.; Van Der Maaten, L.; and Weinberger,\nK. Q. 2017. Densely Connected Convolutional Networks.\nIn 2017 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2261–2269. Honolulu, HI: IEEE.\nISBN 978-1-5386-0457-1.\nHuang, H.; Lin, L.; Tong, R.; Hu, H.; Zhang, Q.; Iwamoto,\nY .; Han, X.; Chen, Y .-W.; and Wu, J. 2020. UNet 3+: A\nFull-Scale Connected UNet for Medical Image Segmenta-\ntion. arXiv:2004.08790.\nJi, Y .; Zhang, R.; Wang, H.; Li, Z.; Wu, L.; Zhang, S.; and\nLuo, P. 2021. Multi-Compound Transformer for Accurate\nBiomedical Image Segmentation. arXiv:2106.14385.\nKumar, N.; Verma, R.; Anand, D.; Zhou, Y .; Onder, O. F.;\nTsougenis, E.; Chen, H.; Heng, P.-A.; Li, J.; and Hu, Z.\n2020. A Multi-Organ Nucleus Segmentation Challenge.\nIEEE Transactions on Medical Imaging, 39(5): 1380–1391.\nKumar, N.; Verma, R.; Sharma, S.; Bhargava, S.; Vahadane,\nA.; and Sethi, A. 2017. A Dataset and a Technique for Gen-\neralized Nuclear Segmentation for Computational Pathol-\nogy. IEEE Transactions on Medical Imaging, 36(7): 1550–\n1560.\nLi, X.; Chen, H.; Qi, X.; Dou, Q.; Fu, C.-W.; and Heng, P.-\nA. 2018. H-DenseUNet: Hybrid Densely Connected UNet\nfor Liver and Tumor Segmentation From CT V olumes.IEEE\nTransactions on Medical Imaging, 37(12): 2663–2674.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin Transformer: Hierarchical Vision\nTransformer using Shifted Windows. arXiv:2103.14030.\nLong, J.; Shelhamer, E.; and Darrell, T. 2015. Fully Convo-\nlutional Networks for Semantic Segmentation. CVPR, 10.\nMilletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-Net:\nFully Convolutional Neural Networks for V olumetric Medi-\ncal Image Segmentation. arXiv:1606.04797.\nNi, Z.-L.; Bian, G.-B.; Wang, G.-A.; Zhou, X.-H.; Hou, Z.-\nG.; Chen, H.-B.; and Xie, X.-L. 2020. Pyramid Attention\nAggregation Network for Semantic Segmentation of Surgi-\ncal Instruments. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 34(07): 11782–11790.\nOktay, O.; Schlemper, J.; Folgoc, L. L.; Lee, M.; Hein-\nrich, M.; Misawa, K.; Mori, K.; McDonagh, S.; Hammerla,\nN. Y .; Kainz, B.; Glocker, B.; and Rueckert, D. 2018. At-\ntention U-Net: Learning Where to Look for the Pancreas.\narXiv:1804.03999.\nRahman, M. S. 2020. MultiResUNet : Rethinking the U-Net\nArchitecture for Multimodal Biomedical Image Segmenta-\ntion. Neural Networks, 121: 74–87.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-Net:\nConvolutional Networks for Biomedical Image Segmenta-\ntion. arXiv:1505.04597.\nSirinukunwattana, K.; Pluim, J. P. W.; Chen, H.; Qi, X.;\nHeng, P.-A.; Guo, Y . B.; Wang, L. Y .; Matuszewski, B. J.;\nBruni, E.; Sanchez, U.; B¨ohm, A.; Ronneberger, O.; Cheikh,\n2448\nB. B.; Racoceanu, D.; Kainz, P.; Pfeiffer, M.; Urschler, M.;\nSnead, D. R. J.; and Rajpoot, N. M. 2016. Gland Segmenta-\ntion in Colon Histology Images: The GlaS Challenge Con-\ntest. arXiv:1603.00275.\nUlyanov, D.; Vedaldi, A.; and Lempitsky, V . 2017. Instance\nNormalization: The Missing Ingredient for Fast Stylization.\narXiv:1607.08022.\nValanarasu, J. M. J.; Oza, P.; Hacihaliloglu, I.; and Patel,\nV . M. 2021. Medical Transformer: Gated Axial-Attention\nfor Medical Image Segmentation. arXiv:2102.10662.\nWang, Q.; Wu, B.; Zhu, P.; Li, P.; Zuo, W.; and Hu, Q. 2020.\nECA-Net: Efﬁcient Channel Attention for Deep Convolu-\ntional Neural Networks. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 11531–\n11539. Seattle, W A, USA: IEEE. ISBN 978-1-72817-168-5.\nWen, Y .; Xie, K.; and He, L. 2020. Segmenting Medical\nMRI via Recurrent Decoding Cell. Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 34(07): 12452–12459.\nZhang, Y .; Higashita, R.; Fu, H.; Xu, Y .; Zhang, Y .; Liu, H.;\nZhang, J.; and Liu, J. 2021. A Multi-Branch Hybrid Trans-\nformer Networkfor Corneal Endothelial Cell Segmentation.\narXiv:2106.07557.\nZhang, Y .; Liu, H.; and Hu, Q. 2021. TransFuse: Fusing\nTransformers and CNNs for Medical Image Segmentation.\narXiv:2102.08005.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang,\nY .; Fu, Y .; Feng, J.; Xiang, T.; Torr, P. H. S.; and\nZhang, L. 2020. Rethinking Semantic Segmentation from\na Sequence-to-Sequence Perspective with Transformers.\narXiv:2012.15840.\nZhou, Z.; Siddiquee, M. R.; Tajbakhsh, N.; and Liang, J.\n2018. UNet++: A Nested U-Net Architecture for Medical\nImage Segmentation. DLMIA/ML-CDS@MICCAI.\n2449"
}