{
  "title": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer: Difference and the Explanations",
  "url": "https://openalex.org/W4225522924",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1970369099",
      "name": "Hui Shi",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2110529662",
      "name": "Sicun Gao",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2112566403",
      "name": "Yuandong Tian",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2556594351",
      "name": "Xinyun Chen",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2146366640",
      "name": "Jishen Zhao",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A1970369099",
      "name": "Hui Shi",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2110529662",
      "name": "Sicun Gao",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2112566403",
      "name": "Yuandong Tian",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2556594351",
      "name": "Xinyun Chen",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2146366640",
      "name": "Jishen Zhao",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6678448845",
    "https://openalex.org/W6992354107",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W1517316505",
    "https://openalex.org/W2618011341",
    "https://openalex.org/W2809900044",
    "https://openalex.org/W1982581840",
    "https://openalex.org/W6667329600",
    "https://openalex.org/W2562607067",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2963423043",
    "https://openalex.org/W2962994149",
    "https://openalex.org/W4303633609",
    "https://openalex.org/W3104298168",
    "https://openalex.org/W1732222442",
    "https://openalex.org/W3100454002",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W4288560456",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W2963059228",
    "https://openalex.org/W2951008357",
    "https://openalex.org/W3032974082",
    "https://openalex.org/W3088959367",
    "https://openalex.org/W2963753324",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2124479173",
    "https://openalex.org/W4288328877",
    "https://openalex.org/W2067619114",
    "https://openalex.org/W2983086530",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2973122905",
    "https://openalex.org/W2908802752"
  ],
  "abstract": "Long Short-Term Memory (LSTM) and Transformers are two popular neural architectures used for natural language processing tasks. Theoretical results show that both are Turing-complete and can represent any context-free language (CFL).In practice, it is often observed that Transformer models have better representation power than LSTM. But the reason is barely understood. We study such practical differences between LSTM and Transformer and propose an explanation based on their latent space decomposition patterns. To achieve this goal, we introduce an oracle training paradigm, which forces the decomposition of the latent representation of LSTMand the Transformer and supervises with the transitions of the Pushdown Automaton (PDA) of the corresponding CFL. With the forced decomposition, we show that the performance upper bounds of LSTM and Transformer in learning CFL are close: both of them can simulate a stack and perform stack operation along with state transitions. However, the absence of forced decomposition leads to the failure of LSTM models to capture the stack and stack operations, while having a marginal impact on the Transformer model. Lastly, we connect the experiment on the prototypical PDA to a real-world parsing task to re-verify the conclusions",
  "full_text": "Learning Bounded Context-Free-Grammar via LSTM and the Transformer:\nDifference and Explanations\nHui Shi1, Sicun Gao1, Yuandong Tian2, Xinyun Chen3, Jishen Zhao1\n1University of California San Diego,\n2Facebook AI Research,\n3University of California, Berkeley\nfhshi, jzhao, sicungg@ucsd.edu, yuandong@fb.com, xinyun.chen@berkeley.edu\nAbstract\nLong Short-Term Memory (LSTM) and Transformers are two\npopular neural architectures used in natural language pro-\ncessing tasks. Theoretical results show that both are Turing-\ncomplete and can represent any context-free languages (CFLs).\nIn practice, it is often observed that the Transformer models\nhave better representation power than the LSTM. But the rea-\nson is barely understood. We study such practical differences\nbetween LSTM and the Transformer and propose an expla-\nnation based on their latent space decomposition patterns. To\nachieve this goal, we introduce an oracle training paradigm,\nwhich forces the decomposition of the latent representation\nof LSTM and the Transformer, and supervises with the tran-\nsitions of the corresponding Pushdown Automaton (PDA) of\nthe CFL. With the forced decomposition, we show that the\nperformance upper bounds of LSTM and the Transformer in\nlearning CFL are close: both of them can simulate a stack\nand perform stack operation along with state transitions. How-\never, the absence of forced decomposition leads to the failure\nof LSTM models to capture the stack and stack operations,\nwhile having a marginal impact on the Transformer model.\nLastly, we connect the experiment on the prototypical PDA to\na real-world parsing task to re-verify the conclusions.\nIntroduction\nThe LSTM network has achieved great success in various\nnatural language processing (NLP) tasks (Sutskever, Vinyals,\nand Le 2014; Wang et al. 2016), and in recent years, the\nTransformer network keeps breaking the record of state-of-\nthe-art performances established by LSTM-based models in\ntranslation (Vaswani et al. 2017), question-answering (Devlin\net al. 2018), and so on(Dehghani et al. 2018; Brown et al.\n2020). Besides exploring the capacity boundary of the Trans-\nformer network, there is an increasing interest in investigating\nthe representation power of the Transformer network and ex-\nplaining its advantage over the LSTM models theoretically\nand empirically.\nExisting analysis has proven that both LSTM (Siegel-\nmann and Sontag 1995) and the Transformer network (P¬¥erez,\nMarinkovi¬¥c, and Barcel¬¥o 2019) are Turing-Complete. How-\never, much empirical evidence shows both models are far\nfrom perfect in imitating even simple Turing machines (De-\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nhghani et al. 2018; Joulin and Mikolov 2015). Several expla-\nnations of the performance gap between practice and theory\nare 1) some theoretical proofs relies on inÔ¨Ånite precision\nassumption while the precision in practical computations\nis limited (Weiss, Goldberg, and Yahav 2018; Korsky and\nBerwick 2019); 2) given Ô¨Åxed latent space dimension, ac-\ncording to the pigeonhole principle, as input sequence length\ngoes towards inÔ¨Ånity, there will be information that can not\nencode into the latent space or is forgotten (Hahn 2020).\nDespite the soundness of the theoretical proofs and expla-\nnations, none of them can directly explain the phenomenons\nin practice: 1) given the same computation precision, Trans-\nformer has an advantage over LSTM model in many cases;\n2) both Transformer and LSTM fail even when the input\nsequence is short and their latent space dimension is huge.\nIn this work, we study the empirical representation power\nof the LSTM and the Transformer networks and investigate\nthe origination of their difference. We compare the mod-\nels via learning context-free languages. The reason to study\nCFLs other than regular languages or Turing machines is that\nthe learning of CFLs provides most insights into NLP tasks\nwhere understanding the underlying hierarchy of a sequence\n(e.g. utterance and programs) is crucial. In the rest of the\npaper, we will Ô¨Årst introduce an oracle training paradigm to\npredict the status of the PDA that accepts the CFL, and a\nregularizer to explicitly decompose the latent space of the\nLSTM and the Transformer such that the PDA state and the\npositions in the stack are encoded in distinct dimensions.\nLastly, the experiment section exhibits the empirical results\nand leads us to the following conclusions 1:\n‚Ä¢ LSTM and the Transformer have a similar upper bound\nof empirical representation power in simulating PDAs.\n‚Ä¢ LSTM fails to factorize its latent space to encode the\nstate and multiple elements of the stack without explicit\nsupervision, which is the pivot to its compromised perfor-\nmance in real-world tasks. Meanwhile, the Transformer\nis marginally affected by the absence of explicit decom-\nposition regularization.\n‚Ä¢ Language recognition is not a reliable task to compare the\nempirical capacity of LSTM and Transformer since the\nresults are sensitive to the setting of PDAs, the hyperpa-\n1The code is available at https://github.com/shihui2010/learn\ncfg with neural network\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n8267\n1 2\nùëè, ùëé ‚Üí ùúñ\nùëé, ùúñ ‚Üí ùëé\nùëé, ùúñ ‚Üí ùëé\nùëè, ùëé ‚Üí ùúñ\nb) ùëé&ùëè&\n1 2\n0, ùúñ ‚Üí ùúñ\n1, ùúñ ‚Üí ùúñ\n0, ùúñ ‚Üí ùúñ\n1, ùúñ ‚Üí ùúñ\nc) parity\n1\nùë§*, ùúñ ‚Üí ùë§*\na) Dyck\nùë§*, ùë§*\n+ ‚Üí ùúñ\n1 2\nc, ùúñ ‚Üí ùúñ\nùë§*, ùúñ ‚Üí ùë§*\nùë§*, ùúñ ‚Üí ùë§*\nùë§*, ùë§* ‚Üí ùúñ\nd) wùëêùë§+\nFigure 1: Graphical notations of PDAs for the CFLs.\nrameters of the models, and the training methods.\nPreliminaries and DeÔ¨Ånitions\nContext-Free Language\nA context-free grammar Gis deÔ¨Åned by a collection of non-\nterminal symbols (i.e. variables), terminal symbols (i.e. the\nalphabet of G), production rules and a start symbol E. The\ncontext-free language of Gis the set of strings that can be\nderived from the start symbol by iteratively applying the\nproduction rules until there are no nonterminal symbols.\nA Pushdown Automaton (PDA) is a state machine with\na stack that reads an input symbol and the top element in a\nstack at each step and performs the state transition and some\nstack operations (push/pop). Formally, a PDA can be deÔ¨Åned\nas a tuple of < Q;\u0006;S;\u000e;q 0;I;F > . Q is a Ô¨Ånite set of\nstates, and q0 2Qis the initial state; \u0006 is the alphabet of the\ninputs, Sis the set of stack symbols and I 2Sis the initial\nstack top symbol; F 2Qis a set of accepting states. \u000eis a\ntransition function \u000e(q2Q;x 2(\u0006 [f\u000fg);s 2S) !q‚Ä≤;s‚Ä≤,\nwhere \u000f denotes an empty symbol and s denotes the top\nelement in the stack S. The transition function implies the\nstack operation. For example, let \u0003be a wildcard for arbitrary\nstate or symbol, \u000e(\u0003;\u000f; \u0003) represents transition that consumes\nno input symbols2; \u000e(\u0003;\u0003;\u000f) !\u0003;s ‚Ä≤where s‚Ä≤6= \u000fis a stack\npush operation, and \u000e(\u0003;\u0003;s) !\u0003;\u000f where s6= \u000fis a stack\npop operation.\nA PDA can be equivalently expressed by some CFG and\nvice versa (Sch¬®utzenberger 1963; Chomsky 1962). In learn-\ning CFGs, the neural networks are expected to learn the\nequivalent PDAs instead of the production rules.\nIn this study, we are interested in bounded CFGs and PDAs\nwhere the recursion depth is Ô¨Ånite and a stack with a Ô¨Ånite\nsize should be adequate for processing the CFLs. The rea-\nson is two-fold. First, we agree with the existing theoretical\nanalysis that encoding an unbounded stack into Ô¨Ånite space\nis the bottleneck for LSTM and Transformer, thus we focus\non the investigation of their realistic representation power\nbefore they reach the theoretical upper bound (e.g. number\nof recursions !1). Second, in natural languages and even\nprogramming language, the nesting of the production rules\n2For instance, the reduce operation in shift-reduce parsing, as\nshown in Table 2\nare very limited (e.g. <100), so it‚Äôs important to understand\nLSTM and Transformer‚Äôs behavior under bounded CFGs.\nFour canonical PDAs (shown in Figure 1) are introduced\nas follows:\nDeÔ¨Ånition 0.1 (Dyck-(k, m)). The language of paired paren-\ntheses of ktypes up to mrecursions. Dyck is the prototypical\nlanguage to any context-free languages (Kozen 1997).\nDeÔ¨Ånition 0.2 (anbn). anbn accepts and generates the set of\nstrings that consists of aand b, and every occurrence of n\nconsecutive as is followed by exact n successive bs.\nDeÔ¨Ånition 0.3 (Parity). The binary strings that containing an\neven number of 0s. Parity can be expressed by a Determin-\nistic Finite Machine (DFA). To formalize parity in PDA, we\ndenote all parity transition function with no stack operation\n\u000e(\u0003;\u0003;\u000f) !\u0003;\u000f .\nDeÔ¨Ånition 0.4 ((wcwr)n-(k, m)). wcwr-(k, m) generates\nstrings that start with a sub-string !followed by a character\nc, then followed by the reverse of !. The chars w2!come\nfrom a vocabulary \n (c =2\n) of size k. The string !contains\nno more than mchars. The (wcwr)n-(k, m) generates strings\nthat contains no more than nsubstrings from wcwr-(k, m).\nLong-short Term Memory Network\nGiven the input sequence embeddings X = fxtgn\nt=1 and\ninitial hidden state and cell state (h0;c0) 2 Rd, LSTM\n(Hochreiter and Schmidhuber 1997) produces the latent rep-\nresentation of the sequences fhtgn\nt=1\n, where ht 2Rd, as\nfollows:\nft = \u001b(Wf ht‚àí1 + Uf xt + bf )\nit = \u001b(Wiht‚àí1 + Uixt + bi)\not = \u001b(Woht‚àí1 + Uoxt + bo)\n~ct = tanh(Wcht‚àí1 + Ucxt + bc)\nct = ft \fct‚àí1 + it \f~ct\nht = ot \ftanh(ct)\n(1)\nTransformer Network\nThe Transformer network (Vaswani et al. 2017) process\nthe sequence via multi-head attention. SpeciÔ¨Åcally, let the\natt(Q;K;V ) represent the scaled dot-product attention func-\ntion over query Q 2Rr0√ór1 , key K 2Rr0√ór1 , and value\nV 2Rr0√ód, deÔ¨Åned as:\natt(Q;K;V ) = softmax( QK‚ä§\npr1\nV) (2)\nGiven sequence X, the Transformer encodes the sequences\nvia multi-head attention followed by a position-wise feed-\nforward network (denote as FFN (\u0001)).\nht = FFN ([head1; \u0001\u0001\u0001 ; headk]);\nheadi = att(yWQ\ni ;yWK\ni ;ytWV\ni )\n(3)\nTo distinguish the order of input symbols, the yt is pro-\nduced by summing a positional encoding with input encod-\ning:\n8268\nùë•1 ùë•2 ùë•3 ùë•4 ùë•1 ùë•2 ùë•3 ùë•4 ùë•1 ùë•2 ùë•3 ùë•4\na) LSTM b)Transformer Encoder c)Transformer Decoder\nFigure 2: The information Ô¨Çow in LSTM, the Transformer\nencoder and decoder.\nLSTM/Transformer\nùë•\" ùë•# ùë•$ùë•#%\"‚Ä¶ ‚Ä¶\nMLPsymbol MLPstate MLP(s)stack\nùë•#%\" ùë†#%\" ùëá#%\"\n‚Ñé#\nMLPcls\nùë¶\nLSTM/Transformer‚Ä¶\n‚Ñé$\nFigure 3: Oracle training.\npt;2j = sin(t=100002j=r1 )\npt;2j+1 = cos(t=100002j=r1 )\nyt = xt + pt\n(4)\nWe distinguish the Transformer encoder that allows atten-\ntion between any pairs of input symbols and the Transformer\ndecoder that allows multi-head attention to compute only on\npast symbols. The comparison is illustrated in Figure 2.\nOracle Training\nIn this section, we introduce the oracle training method as-\nsuming complete PDA transition steps are exposed to the\nmodel to provide the strongest supervision.\nFormally, given symbol sequences fX(i)gn\ni=1 accepted by\na PDA, the oracle includes fX(i)gn\ni=1\n, the states of DPA\nfS(i)gn\ni=1 and the stack status fT(i)gn\ni=1\nat each step while\nprocessing the symbol sequences. The oracle training forces\nthe models to predict not only the next symbols as in the\nlanguage model, but also the internal state and stack status.\nHereinafter, for simplicity, we omit the superscript\ni that\ndenotes i-th sample. Let X = fxtg\u001c\nt=1, S = fstg\u001c\nt=1\n, and\nT = fTtg\u001c\nt=1\n, and the Tt[j] being the j-th item in the stack\nat step t.\nFigure 3 shows the generic architecture for oracle training.\nSeveral multi-layer perceptron (MLP) networks are employed\nto independently predict the symbol, state, and stack from the\nlatent representation ht. For stack status, a non-full stack is\npadded with the empty token \u000f. Therefore, the models always\npredict a constant number of symbols for the stack and pre-\ndict \u000fs in the correct positions to indicate a non-full stack. We\nalso include a language recognition task in which the model\npredicts if the sequences are accepted by the PDA. The lan-\nguage recognition task is widely used in arguing the inability\nof LSTM and Transformer in recognizing CFLs. Though\nfrom the PDA‚Äôs perspective, the recognition is equivalent to\nMLPstate MLPstack\nMLPsymbol\nùë†\"\n‚Ñé\"\nMLPstack MLPstack\nùëá\"[0] ùëá\"[1] ùëá\"[2]\nùë•\"+,\nsharing parameters\nFigure 4: Forced decomposition of ht.\nMLPstate MLPstack[0]\nMLPsymbol\nùë†\"\n‚Ñé\"\nMLPstack[1] MLPstack[2]\nùëá\"[0] ùëá \"[1] ùëá\"[2]\nùë•\"+,\nindependent parameters\nFigure 5: Latent decomposition of ht (with stack size of 3).\nthe task of learning the transition, while in the experiment,\nwe show both models beneÔ¨Åt greatly from dense supervision\nin the oracle training, compared to the sparse supervision in\nthe language recognition.\nWe also introduce two vital model conÔ¨Ågurations: forced\ndecomposition (Figure 4) and latent decomposition (Fig-\nure 5). In forced decomposition, the latent representation\nvector ht is split into m+ 1 segments, where mis the maxi-\nmum stack size, to predict the PDA state and melements in\nthe stack separately. Since each position in the stack shares\nthe same set of stack symbols, we let the stack predictors\nMLPstack share parameters. In contrast, the latent decompo-\nsition uses whole ht but separate MLPs for prediction, and\nthe mstack predictors are independently trained. In both con-\nÔ¨Ågurations, the next symbol is predicted based on the entire\nht because the valid symbol for the next step depends on\nboth the current state and stack.\nPredictions of symbol, state, and stack are all trained with\ncross-entropy loss, and the three-part losses are summed 3 :\nLoracle = Lsymbol + Lstate + Lstack (5)\nExperiments\nCanonical PDAs\nIn this section, we evaluate the representation power of LSTM\nand Transformer by simulating canonical PDAs.\nData generation. For the PDAs introduced, we generate\nthe 50k sequences for training and another 50k for testing\nfor each conÔ¨Ågurations (i.e. fk;m;n gwhen applicable). Ex-\nceptionally, for wcwr (n=2, m=2, k=*) we enumerate all\naccepted sequences since the set of accepted sequences are\nlimited. For each sequence, the ground-truth PDA annotates\n3An option is to assign weights to each term on the right hand\nside, our additional experiment in the Appendix shows that the\nchoice of loss weights does not inÔ¨Çuence the main conclusion in the\nexperiment.\n8269\n20 40 60 80 100\nmax length\n0.6\n0.8\n1.0\nCLS\n20 40 60 80 100\nmax length\nLM\n20 40 60 80 100\nmax length\nSTATE\n20 40 60 80 100\nmax length\nSTACK\nLSTM\nTransD\nTransE\nFigure 6: Performance on anbn\n0.50\n0.75\n1.00CLS\nk=3 k=5 k=10 k=20\n0.50\n0.75\n1.00LM\n2510 20 40\nm\n0.50\n0.75\n1.00STACK\n2510 20 40\nm\n2510 20 40\nm\n2510 20 40\nm\nLSTM\nTransD\nTransE\nFigure 7: Performance on Dyck.\nthe sequence and produces the state and stack labels for or-\nacle training. Meanwhile, a corrupted sequence that is not\naccepted by the PDA is generated for the language recogni-\ntion (classiÔ¨Åcation) task. For language modeling, we compute\nthe valid symbols at each step\ntgiven sequence x1;\u0001\u0001\u0001 ;xt‚àí1\nand denote this validity over alphabet as LM mask.\nModel conÔ¨Ågurations. For each PDA task, we set the\nhidden size in the LSTM model and the latent dimension in\nthe Transformer model to be \u000b\u0003(jQj+ m\u0003jSj), where \u000bis\nthe scale factor, jQjis the size of states, mis the maximum\nrecursion (i.e. maximum stack size), and jSjis the size of\nstack symbols4. We always let \u000b \u00151 such that the latent\nvector ht will always have enough dimensions to encode\nboth the state and stack. For the Transformer encoder and\ndecoder, the number of attention heads is 8. For all models,\nunless speciÔ¨Åed otherwise, the number of layers is 1 and\n\u000b = 1 . We set all MLP modules to be a two-layer feed-\nforward network with a sigmoid as an activation function.\nThe hidden dimension of the MLPs is twice their input feature\ndimension. For both models, we use an embedding layer to\nencode input symbols to R2|\u0006|. For the Transformer model,\nthe Ô¨Ålter size for the position-wise feed-forward network is\n32, and we apply a dropout layer with a rate of 0:1.\nTraining. Models are trained with Adam optimizer with\na learning rate of 0.001 on the AWS platform. The models\nare trained for 200 epochs or up to convergence. We intro-\nduce two-phase training for language recognition tasks, i.e.\nclassify whether the sequences are accepted by the PDA. In\nphase 0, the models are initialized and solely trained on a\n4For PDAs introduced, we set S = \u0006 SfI;\u000fg\n20 40 60 80 100\nmax length\n0.6\n0.8\n1.0\nCLS\n20 40 60 80 100\nmax length\nLM\n20 40 60 80 100\nmax length\nSTATE\nLSTM\nTransD\nTransE\nFigure 8: Performance on parity\nm=2\n0.0\n0.5\n1.0CLS\nm=5 m=10\n0.0\n0.5\n1.0LM\n0.0\n0.5\n1.0STATE\n2 5 10 20\nn\n0.0\n0.5\n1.0STACK\n2 5 10 20\nn\n2 5 10 20\nn\nlstm(k=3)\nTransD(k=3)\nTransE(k=3)\nlstm(k=5)\nTransD(k=5)\nTransE(k=5)\nlstm(k=10)\nTransD(k=10)\nTransE(k=10)\nFigure 9: Performance on (wcwr)n.\nclassiÔ¨Åcation task, while in phase 1, the models are re-trained\non classiÔ¨Åcation tasks after being trained with oracle training.\nMetrics. 1) ClassiÔ¨Åcation accuracy: portion of the se-\nquences that are correctly accepted/rejected. 2) LM accuracy:\npercentage of predicted symbols that are valid according to\nthe LM mask. 3) State accuracy: accuracy in predicting the\ncurrent PDA state. 4) Stack accuracy: accuracy in predicting\ncurrent stack status, which is the average prediction accuracy\nover each stack position (including empty positions).\nOverall results. Figure 6, 8, 7, 9 shows the LSTM and\nTransformer performance over multiple conÔ¨Åguration of\nPDAs 5. There are few observations from the results: 1)\nLSTM has higher accuracy in learning PDAs, as it gener-\nally achieves higher accuracy in both state and stack pre-\ndictions, especially shown in Figure 8, 9. The results shows\nthat when CFGs are bounded, LSTM does not need external\nmemory to simulate the stack. 2) For predicting the state,\nTransformer decoder behaves in a similar way to LSTM, as\nshown in state accuracy in Figure 6, 9. Inanbn and (wcwr)n,\nthe Transformer encoder slightly outperforms LSTM and\nTransformer decoder. This indicates that though past infor-\n5These Ô¨Ågs shows phase 0 classiÔ¨Åcation accuracy\n8270\n0.0\n0.5\n1.0CLS\nk=3 k=5 k=10 k=20\n0.0\n0.5\n1.0LM\n2510 20 40\nm\n0.0\n0.5\n1.0STACK\n2510 20 40\nm\n2510 20 40\nm\n2510 20 40\nm\nLSTM-F\nTransD-F\nTransE-F\nLSTM-L\nTransD-L\nTransE-L\nFigure 10: Performance on Dyck with forced (solid lines)\nand latent (dotted lines) decomposition.\nmation should be adequate to determine the current status\nof PDA, it‚Äôs still beneÔ¨Åcial to the neural models to foresee\nthe future sequences. 3) Both language recognition task and\nlanguage modeling solely should not be used to examine the\ncapability of neural models in learning CFG, since they do\nnot fully reÔ¨Çect the capability of models to learn the internal\ndynamics in state transition and stack operation (Figure 8),\nand reversely learning the precise PDA does not necessar-\nily lead to perfection in language recognition and language\nmodeling (Figure 7, 9).\nDecomposition hardship of LSTM. Though LSTM and\nTransformer show comparable representation power under\nforced decomposition setting, the disadvantage of LSTM in\nthe latent decomposition training is signiÔ¨Åcant, as can be\nviewed from Figure 10, 11, meanwhile, Transformer models\nare roughly as good as they are trained with forced decompo-\nsition. The failure of LSTM in stack prediction is key to its\ndisadvantage in many tasks (Devlin et al. 2018). Figure 12\nshows the two-component t-SNE (Van der Maaten and Hin-\nton 2008) results of the LSTM hidden states ht. The different\ncolors represent distinct stack status in the oracle. As shown,\nthe hidden states in forced decomposition tend to separately\nencode different stack status, while in the latent decomposi-\ntion, there are much more clusters containing multiple colors.\nThe mixed-color clusters indicate the failure of stack pre-\ndictor to correctly predict the stack status, and also prevents\nLSTM itself to learn the correct stack operations.\nScale factor \u000b. Generally, a larger number of parameters\nbrings higher representation power. We veriÔ¨Åed the conclu-\nsion that LSTM and Transformer have similar representation\npower of learning CFG on larger models. Figure 13 shows\nthe performance of four-layer LSTM and Transformer model\nwith scaling factor \u000b= 4. Viewing from the stack prediction\naccuracy, the conclusion still holds. We also examined the\nconclusion on factorization on larger scale models shown in\nFigure 14. The full results of the larger models can be found\nin the Appendix.\nWhy language recognition accuracy is not a good indi-\ncator. We detail the four-tier reasons for our earlier conclu-\n0.0\n0.5\n1.0CLS\nm=2 m=5 m=10\n0.0\n0.5\n1.0LM\n0.0\n0.5\n1.0STATE\n2 5 10 20\nn\n0.0\n0.5\n1.0STACK\n2 5 10 20\nn\n2 5 10 20\nn\nLSTM-F\nTransD-F\nTransE-F\nLSTM-L\nTransD-L\nTransE-L\nFigure 11: Performance on (wcwr)n with forced (solid lines)\nand latent (dotted lines) decomposition (k= 10).\na) Forced\n b) Latent\nFigure 12: t-SNE analysis on LSTM hidden states trained on\nDyck (k=3,m=5).\nsion that language recognition accuracy is unfair in judging\nthe models‚Äô capability of learning PDA. Firstly, we explain\nthe observations from Figure 7, 9 that perfection in state and\nstack prediction does not lead to accurate recognition: the\nlanguage recognition requires a higher level of reasoning\nthan just learning a set of PDA transition functions. A re-\njected sequence for PDA might be due to the current symbol\nnot being accepted given the state and stack top, popping an\nempty stack, or exceeding the maximum recursion. Thus the\nclassiÔ¨Åcation decision boundary might be inseparable for an\nMLP without special design. Furthermore, for any models,\nthe error message might occur at random steps, and the mod-\nels have to preserve and pass the message to the last step for\nthe classiÔ¨Åcation prediction in the general training paradigm\nfor language recognition. This is mainly why the classiÔ¨Åca-\ntion accuracy of LSTM declines much more abruptly than of\n8271\n0.50\n0.75\n1.00CLS\nk=3 k=5 k=10 k=20\n0.50\n0.75\n1.00LM\n2510 20 40\nm\n0.50\n0.75\n1.00STACK\n2510 20 40\nm\n2510 20 40\nm\n2510 20 40\nm\nLSTM\nTransD\nTransE\nFigure 13: Performance of four-layer models on Dyck with\n\u000b= 4\n0.0\n0.5\n1.0CLS\nk=3 k=5 k=10 k=20\n0.0\n0.5\n1.0LM\n2510 20 40\nm\n0.0\n0.5\n1.0STACK\n2510 20 40\nm\n2510 20 40\nm\n2510 20 40\nm\nLSTM-F\nTransD-F\nTransE-F\nLSTM-L\nTransD-L\nTransE-L\nFigure 14: Four-layer models (\u000b= 4) on Dyck with forced\n(solid lines) and latent (dotted lines) decomposition.\nthe Transformers since Transformer can pass information be-\ntween any pairs of inputs, thus the comparison of LSTM and\nTransformer will be sensitive to sequence lengths. Besides se-\nquence length, the model size also inÔ¨Çuences the recognition\naccuracy differently. Comparing the classiÔ¨Åcation accuracy\nof Figure 7 and Figure 13, the relative advantage of two mod-\nels are reversed when models scale up. Lastly, Transformer\nmodels may recognize language in some manners that are\ndissimilar to PDAs. To prove this, we show the classiÔ¨Åcation\naccuracy of phase 0 and phase 1 in Figure 21, 22, 23. LSTM\ngenerally improves after oracle training (phase 1), especially\nin Dyck (Figure 23), while the Transformer models suffers\nfrom oracle training (Figure 21-22).\nSensitivity Analysis\nAdditional factorization results Figure 15, 16, 20 show ad-\nditional results for factorization. Unless speciÔ¨Åed, the model\nare conÔ¨Ågured with a single layer and \u000b= 1. All the results\nsupport the conclusion that LSTM without forced factoriza-\ntion hardly learns to simulate the PDA stack properly.\nSensitivity to loss weights In Equation (5), the weights of\nFigure 15: Performance on (wcwr)n. Top 4 rows showk= 3,\nand the lower 4 rows show results with k= 5.\nLsymbol, Lstate, Lstack are set to 1. To conÔ¨Årm the conclu-\nsion is general without dependent on the choice of weight\nvalues, we examine the model performance using the learn-\nable loss weights (Kendall, Gal, and Cipolla 2018), and the\noracle training loss has the form of:\nLoracle = 1\n2\u001b2\n1\nLsymbol + 1\n2\u001b2\n2\nLstate\n+ 1\n2\u001b2\n3\nLstack + log(\u001b1\u001b2\u001b3)\n(6)\nwhere \u001b1;\u001b2;\u001b3 are trainable parameters.\nWe used Dyck-(5,*) datasets (m= 2;5;10;20). For each\nmodel on each dataset, the training is repeated Ô¨Åve times with\nrandom initialization.\nFigure 17 compares the model performance with Ô¨Åxed or\nlearned weights and illustrates that neither the performance\n8272\n0.25\n0.50\n0.75\n1.00L1S1\nCLS LM STATE STACK\n0.25\n0.50\n0.75\n1.00L4S1\n0.25\n0.50\n0.75\n1.00L1S4\n20 40 60 80 100\nmax length\n0.25\n0.50\n0.75\n1.00L4S4\n20 40 60 80 100\nmax length\n20 40 60 80 100\nmax length\n20 40 60 80 100\nmax length\nLSTM-F\nTransD-F\nTransE-F\nLSTM-L\nTransD-L\nTransE-L\nFigure 16: Performance on anbn. Y-label shows number of\nlayers and scaling factor \u000b. For example, L1S4 represents\nsingle-layer model with \u000b= 4.\nnor the hardship in factorization is sensitive to the choice of\nloss weights.\n0.0\n0.5\n1.0Fixed\nCLS LM STACK\n2 5 10 20\nm\n0.0\n0.5\n1.0Learned\n2 5 10 20\nm\n2 5 10 20\nm\nLSTM-F\nTransD-F\nTransE-F\nLSTM-L\nTransD-L\nTransE-L\nFigure 17: Model performance on Dyck-(5,*). First row\nshows results with Equation (5), and second row shows re-\nsults with Equation (6). Lines indicate the average accuracy\nover 5 runs, and the shadows illustrate the range.\nDeeper models In Figure 13, 14, we show that additional\nlayers and extra hidden size can not remedy the hardship\nin decomposition. Figure 16, 20 provide further evidence.\nFrom Figure 16, we notice that the difference between forced\nand latent factorization is alleviated in Transformer models\nas the number of layer increases and \u000bgrows6. However, the\n6The ablation on factorization is not conducted for L4S1 and\nL1S4 for any PDA. Also, since the factorization makes most signiÔ¨Å-\ncant difference on learning stack and Parity requires no stack, the\ncomparison of factorization is not made on Parity.\n0.50\n0.75\n1.00CLS\nk=3 k=5 k=10 k=20\n0.50\n0.75\n1.00LM\n2510 20 40\nm\n0.50\n0.75\n1.00STACK\n2510 20 40\nm\n2510 20 40\nm\n2510 20 40\nm\nLSTM\nTransD\nTransE\nFigure 18: Performance on Dyck (\u000b= 1, four layers)\n0.50\n0.75\n1.00CLS\nk=3 k=5 k=10 k=20\n0.50\n0.75\n1.00LM\n2510 20 40\nm\n0.50\n0.75\n1.00STACK\n2510 20 40\nm\n2510 20 40\nm\n2510 20 40\nm\nLSTM\nTransD\nTransE\nFigure 19: Performance on Dyck (\u000b= 4, single layer)\ngap remains huge for the LSTM model. Besides, the decrease\nof classiÔ¨Åcation and language model accuracy is generally\nobserved when scaling up the models, which indicates the\ntraining difÔ¨Åculty introduced by the deeper and wider model\noverwhelms the beneÔ¨Åt of increased model capacity.\nC := CP S jCP V jS\nCP := S and jS after\nS := V jV twice jV thrice\nV := VP D jVP left jVP right jD\nVP := D opposite jD around\nD := U jU left jU right\nU := walk jlook jrun jjump jturn\nTable 1: Domain SpeciÔ¨Åc Language for SCAN linguistic\ncommands.\nShift-Reduce Parsing\nDataset. SCAN (Lake and Baroni 2018) is a semantic pars-\ning dataset consisting of commands in natural language and\nsequences of actions to execute the commands. Tab. 1 shows\nthe generation rules producing SCAN commands. The dataset\n8273\nFigure 20: 4-layer model with \u000b= 4 on (wcwr)n. Upper 4\nrows: k= 3; middle 4 rows: k= 5; bottom 4 rows: k= 10.\n20 40 60 80 100\nmax length\n0.0\n0.5\n1.0\nLSTM\n20 40 60 80 100\nmax length\nTransD\n20 40 60 80 100\nmax length\nTransE\nLSTM-0\nLSTM-1\nTransD-0\nTransD-1\nTransE-0\nTransE-1\nFigure 21: Two-phase classiÔ¨Åcation accuracy on anbn.\n20 40 60 80 100\nmax length\n0.0\n0.5\n1.0\nLSTM\n20 40 60 80 100\nmax length\nTransD\n20 40 60 80 100\nmax length\nTransE\nLSTM-0\nLSTM-1\nTransD-0\nTransD-1\nTransE-0\nTransE-1\nFigure 22: Two-phase classiÔ¨Åcation accuracy on parity.\ncontains 16728 training samples and 16728 test samples. In\nour experiment, instead of parsing to the sequence of ac-\ntions, we parse the linguistic command according to its CFG\nproduction rules (Tab. 1) using shift-reduce parsing. Since\nthere are production rules in the CFG that map a single non-\nterminal variable to another one, the corresponding PDA is\nnondeterministic. To facilitate the training, we insert a spe-\ncial token ¬°reduce¬ø to make the process deterministic. Tab. 2\nillustrates an example of the annotation of the sequence with\nstack status and the padding process to insert¬°reduce¬ø tokens\nfor reduction operations that do not consume symbol from\nthe input sequence. The equivalent PDA of SCAN‚Äôs CFG has\nonly one state q0. Alphabet \u0006 covers English words in the\nlinguistic commands and the ¬°reduce¬ø token, and the stack\nsymbols are fC;CP;S;V;VP;D;U g.\nModels and metrics. The model conÔ¨Ågurations are the\nsame as in learning the canonical PDAs. We compute perplex-\nity to evaluate language modeling and compute the accuracy\nstep stack consuming\nsymbol transition \u000e\n0 [] jump \u000e(jump;\u000f) !U\n1 [U] left \u000e(left;U ) !D\n2 [D] \u000f \u000e(\u000f;D) !V\n3 [V] \u000f \u000e(\u000f;V ) !S\n4 [S] and \u000e(and;S) !CP\n5 [CP] turn \u000e(turn;\u000f) !U\n6 [CP, U] \u000f \u000e(\u000f;U) !D\n7 [CP, D] opposite \u000e(opposite;D) !VP\n8 [CP, VP] left \u000e(left;VP ) !V\n9 [CP, V] \u000f \u000e(\u000f;[CP;V ]) !C\nPadded Sequence\n[jump, left, <reduce>, <reduce>, and,\nturn, <reduce>, opposite, left, <reduce>]\nTable 2: Shift-reduce parsing of SCAN commands.\n8274\n2510 20 40\nm\n0.0\n0.5\n1.0\nk=3\n2510 20 40\nm\nk=5\n2510 20 40\nm\nk=10\n2510 20 40\nm\nk=20\nLSTM-0\nLSTM-1\nTransD-0\nTransD-1\nTransE-0\nTransE-1\nFigure 23: Two-phase classiÔ¨Åcation accuracy on Dyck.\nof the stack prediction as the parsing accuracy.\nResults. Table 3 sums up the results of LSTM and Trans-\nformer on SCAN dataset, which is consistent with the obser-\nvations in previous section: LSTM and Transformer decoder\nperform similarly in language modeling and parsing when\ndecomposition is forced, and the parsing accuracy of LSTM\nsuffers heavily from latent decomposition setting. The Trans-\nformer encoder can see the whole sequence at each step, so\nit achieves an almost lower bound of perplexity and has the\nhighest parsing accuracy.\nModel Perplexity Accurac y\nLSTM(\u000b= 1, F\norced) 2.705 91.30\nLSTM(\u000b= 1\n, Latent) 2.705 16.61\nLSTM(\u000b= 4\n, Forced) 2.706 91.30\nLSTM(\u000b= 4\n, Latent) 2.708 35.00\nTransformer(D, \u000b= 1,\nForced) 2.710 91.02\nTransformer(D, \u000b= 1\n, Latent) 2.713 76.61\nTransformer(D, \u000b= 4\n, Forced) 2.708 91.30\nTransformer(D, \u000b= 4\n, Latent) 2.710 90.56\nTransformer(E, \u000b= 1,\nForced) 1.020 99.84\nTransformer(E, \u000b= 1\n, Latent) 1.014 72.39\nTransformer(E, \u000b= 4\n, Forced) 1.002 99.99\nTransformer(E, \u000b= 4\n, Latent) 1.001 99.29\nTable 3: Perplexity and parsing accuracy on SCAN. E denotes\nTransformer encoder and D denotes decoder.\nRelated Works\nThere are many theoretical analyses on the representation\npower of LSTM and the Transformer and their comparisons\nthat motivate this work to re-examine their representation\npower from an empirical aspect. Siegelmann and Sontag\n(1995) Ô¨Årstly established the theory that given inÔ¨Ånite preci-\nsion and adequate number of hidden units RNNs are Turing-\nComplete, and H¬®olldobler, Kalinke, and Lehmann (1997) has\ndesigned an one-unit vanilla RNN counter for recognizing\ncounter languages (e.g. anbn and anbncn) with Ô¨Ånite range\nof n. Recently, Hewitt et al. (2020) proposed the construc-\ntion of RNN that performs stack operations as in PDA and\nencodes PDA stack within hidden states of RNN without\nexternal memory. P\n¬¥erez, Marinkovi ¬¥c, and Barcel ¬¥o (2019)\nproofs that with arbitrary precision, the Transformer network\ncould simulate the single execution step for Turing machine,\nthen by induction the Transformer is Turing-Complete. The\nmajority of theoretical analyses emphasize that limited com-\nputation precision may break the proofs and compromise the\nperformance in practice. Nevertheless, the models are not\nsupervised with either step-by-step execution of the Turing\nMachine or the actual counters, which might be the crux to\nthe failures of both models in reality.\nThis work also closely relates to previous attempts to con-\nnect LSTM and transformer model with a speciÔ¨Åc type of\nlanguages, e.g. languages from Chomsky‚Äôs hierarchy (Chom-\nsky 1956) and counter languages. 1) For regular languages\n(representable by DFAs), Michalenko et al. (2019) shows the\nempirical ability of LSTM to represent DFAs and Rabusseau,\nLi, and Precup (2019) has proposed a construction method of\nRNNs from a weighted DFA. 2) For context-free languages\n(representable by PDAs), Sennhauser and Berwick (2018)\nobserved that CFGs are hardly learnable by LSTM models.\non the other hand, Bhattamishra, Ahuja, and Goyal (2020b)\nshowed LSTM could learn CFGs with bounded recursion\ndepth but the performance will be limited for inÔ¨Ånite re-\ncursion. 3) For counter languages, the Transformer network\n(Bhattamishra, Ahuja, and Goyal 2020a) and LSTM Suzgun\net al. (2019a) have been trained to predict the outputs of a\ndynamic counter. However, their results disagree on the ca-\npability of LSTM in representing DYCK languages. In our\nwork, we focus on bounded CFG since the capacity of learn-\ning regular languages is widely agreed upon while there are\ndisputes on the CFG level.\nFinally, observing the defects of both LSTM and Trans-\nformer in learning CFG and algorithmic tasks, many works\npropose to use external memory to enhance the LSTM model\n(Joulin and Mikolov 2015; Das, Giles, and Sun 1992; Suz-\ngun et al. 2019b), introduce recurrence in the Transformer\nnetwork (Dehghani et al. 2018), and design specialized ar-\nchitectures (Graves, Wayne, and Danihelka 2014; Hao et al.\n2018; Sukhbaatar et al. 2015; Stogin et al. 2020). Though it‚Äôs\ncommonly believed that LSTM with Ô¨Ånite memory, i.e. hid-\nden states, can not handle CFGs which requires inÔ¨Ånite stack\nspaces, we investigate the capacity of LSTM and transformer\nin bounded CFGs that requires Ô¨Ånite-size stack and conclude\nthat Ô¨Ånite memory is not the bottleneck of LSTM capacity in\nlearning CFGs.\nConclusion\nWe illustrate that only the state and stack prediction accuracy\ntrained with dense supervision and explicit decomposition\nregularizer are the fair and stable metric to compare the em-\npirical representation power of LSTM and the Transformer\nnetwork. Then we conclude that both LSTM and Transformer\nnetwork can simulate context-free languages with bounded\nrecursion with a similar representation power, and unveiled\nthe disadvantage of the LSTM model in practice is from its\ninability to decompose the latent representation space.\nReferences\nBhattamishra, S.; Ahuja, K.; and Goyal, N. 2020a. On the\nAbility of Self-Attention Networks to Recognize Counter\nLanguages. arXiv preprint arXiv:2009.11264.\nBhattamishra, S.; Ahuja, K.; and Goyal, N. 2020b. On the\nPractical Ability of Recurrent Neural Networks to Recognize\nHierarchical Languages. arXiv preprint arXiv:2011.03965.\n8275\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.arXiv\npreprint arXiv:2005.14165.\nChomsky, N. 1956. Three models for the description of\nlanguage. IRE Transactions on information theory, 2(3):\n113‚Äì124.\nChomsky, N. 1962. Context-free grammars and pushdown\nstorage. MIT Res. Lab. Electron. Quart. Prog. Report., 65:\n187‚Äì194.\nDas, S.; Giles, C. L.; and Sun, G.-Z. 1992. Learning context-\nfree grammars: Capabilities and limitations of a recurrent neu-\nral network with an external stack memory. In Proceedings\nof The Fourteenth Annual Conference of Cognitive Science\nSociety. Indiana University, 14. Citeseer.\nDehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.; and\nKaiser,\n≈Å. 2018. Universal transformers. arXiv preprint\narXiv:1807.03819.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nGraves, A.; Wayne, G.; and Danihelka, I. 2014. Neural turing\nmachines. arXiv preprint arXiv:1410.5401.\nHahn, M. 2020. Theoretical limitations of self-attention in\nneural sequence models. Transactions of the Association for\nComputational Linguistics, 8: 156‚Äì171.\nHao, Y .; Merrill, W.; Angluin, D.; Frank, R.; Amsel, N.; Benz,\nA.; and Mendelsohn, S. 2018. Context-free transductions\nwith neural stacks. arXiv preprint arXiv:1809.02836.\nHewitt, J.; Hahn, M.; Ganguli, S.; Liang, P.; and Man-\nning, C. D. 2020. RNNs can generate bounded hierar-\nchical languages with optimal memory. arXiv preprint\narXiv:2010.07515.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation, 9(8): 1735‚Äì1780.\nH¬®olldobler, S.; Kalinke, Y .; and Lehmann, H. 1997. Design-\ning a counter: Another case study of dynamics and activation\nlandscapes in recurrent networks. In Annual Conference on\nArtiÔ¨Åcial Intelligence, 313‚Äì324. Springer.\nJoulin, A.; and Mikolov, T. 2015. Inferring algorithmic pat-\nterns with stack-augmented recurrent nets. arXiv preprint\narXiv:1503.01007.\nKendall, A.; Gal, Y .; and Cipolla, R. 2018. Multi-task learn-\ning using uncertainty to weigh losses for scene geometry\nand semantics. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 7482‚Äì7491.\nKorsky, S. A.; and Berwick, R. C. 2019. On the computa-\ntional power of rnns. arXiv preprint arXiv:1906.06349.\nKozen, D. C. 1997. The Chomsky‚ÄîSch ¬®utzenberger Theorem.\nIn Automata and Computability, 198‚Äì200. Springer.\nLake, B.; and Baroni, M. 2018. Generalization without\nsystematicity: On the compositional skills of sequence-to-\nsequence recurrent networks. In International Conference on\nMachine Learning, 2873‚Äì2882. PMLR.\nMichalenko, J. J.; Shah, A.; Verma, A.; Baraniuk, R. G.;\nChaudhuri, S.; and Patel, A. B. 2019. Representing formal\nlanguages: A comparison between Ô¨Ånite automata and recur-\nrent neural networks. arXiv preprint arXiv:1902.10297.\nP¬¥erez, J.; Marinkovi¬¥c, J.; and Barcel¬¥o, P. 2019. On the turing\ncompleteness of modern neural network architectures. arXiv\npreprint arXiv:1901.03429.\nRabusseau, G.; Li, T.; and Precup, D. 2019. Connecting\nweighted automata and recurrent neural networks through\nspectral learning. In The 22nd International Conference on\nArtiÔ¨Åcial Intelligence and Statistics, 1630‚Äì1639. PMLR.\nSch¬®utzenberger, M. P. 1963. On context-free languages and\npush-down automata. Information and control, 6(3): 246‚Äì\n264.\nSennhauser, L.; and Berwick, R. C. 2018. Evaluating the\nability of LSTMs to learn context-free grammars. arXiv\npreprint arXiv:1811.02611.\nSiegelmann, H. T.; and Sontag, E. D. 1995. On the computa-\ntional power of neural nets. Journal of computer and system\nsciences, 50(1): 132‚Äì150.\nStogin, D.; Mali, A.; Giles, D.; et al. 2020. Provably stable\ninterpretable encodings of context free grammars in rnns with\na differentiable stack. arXiv preprint arXiv:2006.03651.\nSukhbaatar, S.; Szlam, A.; Weston, J.; and Fergus, R.\n2015. End-to-end memory networks. arXiv preprint\narXiv:1503.08895.\nSutskever, I.; Vinyals, O.; and Le, Q. V . 2014. Sequence\nto sequence learning with neural networks. arXiv preprint\narXiv:1409.3215.\nSuzgun, M.; Gehrmann, S.; Belinkov, Y .; and Shieber, S. M.\n2019a. LSTM networks can perform dynamic counting.\narXiv preprint arXiv:1906.03648.\nSuzgun, M.; Gehrmann, S.; Belinkov, Y .; and Shieber,\nS. M. 2019b. Memory-augmented recurrent neural net-\nworks can learn generalized Dyck languages. arXiv preprint\narXiv:1911.03329.\nVan der Maaten, L.; and Hinton, G. 2008. Visualizing data\nusing t-SNE. Journal of machine learning research, 9(11).\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. Attention\nis all you need. arXiv preprint arXiv:1706.03762.\nWang, Y .; Huang, M.; Zhu, X.; and Zhao, L. 2016. Attention-\nbased LSTM for aspect-level sentiment classiÔ¨Åcation. In\nProceedings of the 2016 conference on empirical methods in\nnatural language processing, 606‚Äì615.\nWeiss, G.; Goldberg, Y .; and Yahav, E. 2018. On the practical\ncomputational power of Ô¨Ånite precision RNNs for language\nrecognition. arXiv preprint arXiv:1805.04908.\n8276",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7295694351196289
    },
    {
      "name": "Transformer",
      "score": 0.6481238603591919
    },
    {
      "name": "Parsing",
      "score": 0.5377007126808167
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5250544548034668
    },
    {
      "name": "Pushdown automaton",
      "score": 0.48846498131752014
    },
    {
      "name": "Natural language processing",
      "score": 0.4225914478302002
    },
    {
      "name": "Automaton",
      "score": 0.3787058889865875
    },
    {
      "name": "Theoretical computer science",
      "score": 0.35286375880241394
    },
    {
      "name": "Engineering",
      "score": 0.08416542410850525
    },
    {
      "name": "Electrical engineering",
      "score": 0.0806349515914917
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I134446601",
      "name": "Berkeley College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ]
}