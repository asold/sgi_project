{
  "title": "Statistical approaches for natural language modelling and monotone statistical machine translation",
  "url": "https://openalex.org/W1504012076",
  "year": 2010,
  "authors": [
    {
      "id": "https://openalex.org/A2211553185",
      "name": "Jesús Andrés-Ferrer",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2124262992",
    "https://openalex.org/W1598062678",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2116912781",
    "https://openalex.org/W1998726122",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W2422872931",
    "https://openalex.org/W2139175732",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W2147192413",
    "https://openalex.org/W2132109814",
    "https://openalex.org/W2011783148",
    "https://openalex.org/W2437005631",
    "https://openalex.org/W7048738093",
    "https://openalex.org/W2148675933",
    "https://openalex.org/W2000394794",
    "https://openalex.org/W2116957398",
    "https://openalex.org/W144782339",
    "https://openalex.org/W55052068",
    "https://openalex.org/W2139403546",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W162242150",
    "https://openalex.org/W65718221",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W1555286493",
    "https://openalex.org/W1498238796",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2153653739",
    "https://openalex.org/W2136657878",
    "https://openalex.org/W2005440678",
    "https://openalex.org/W1924689489",
    "https://openalex.org/W2018116550",
    "https://openalex.org/W2161792612",
    "https://openalex.org/W1819903106",
    "https://openalex.org/W8895266",
    "https://openalex.org/W2168938909",
    "https://openalex.org/W2567948266",
    "https://openalex.org/W2042610879",
    "https://openalex.org/W1497026191",
    "https://openalex.org/W1578472035",
    "https://openalex.org/W2146574666",
    "https://openalex.org/W2156985047",
    "https://openalex.org/W2119168550",
    "https://openalex.org/W2083393647",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W105403319",
    "https://openalex.org/W1550206324",
    "https://openalex.org/W1567461757",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2072393194",
    "https://openalex.org/W2035227369",
    "https://openalex.org/W1517947178",
    "https://openalex.org/W1491184037",
    "https://openalex.org/W3197744324",
    "https://openalex.org/W1261864449",
    "https://openalex.org/W2164948578",
    "https://openalex.org/W1536299805",
    "https://openalex.org/W1870286075",
    "https://openalex.org/W2215355985",
    "https://openalex.org/W127276662",
    "https://openalex.org/W2162157640",
    "https://openalex.org/W1548146869",
    "https://openalex.org/W1568675285",
    "https://openalex.org/W2082713237",
    "https://openalex.org/W2103237065",
    "https://openalex.org/W1542835362",
    "https://openalex.org/W74075037",
    "https://openalex.org/W2100271871",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2074054247",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2396590899",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2145493314",
    "https://openalex.org/W2172138510",
    "https://openalex.org/W2157522852",
    "https://openalex.org/W2071991030",
    "https://openalex.org/W1973898298",
    "https://openalex.org/W2130640821",
    "https://openalex.org/W2170204377",
    "https://openalex.org/W2401082558",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2166210475",
    "https://openalex.org/W222053410",
    "https://openalex.org/W4300628347",
    "https://openalex.org/W4233559841",
    "https://openalex.org/W1644866298",
    "https://openalex.org/W2122617467",
    "https://openalex.org/W4253090545",
    "https://openalex.org/W2098875891",
    "https://openalex.org/W2144879357",
    "https://openalex.org/W2122092249",
    "https://openalex.org/W4230946174"
  ],
  "abstract": "Esta tesis reune algunas contribuciones al reconocimiento de formas estadístico y, más especícamente, a varias tareas del procesamiento del lenguaje natural. Varias técnicas estadísticas bien conocidas se revisan en esta tesis, a saber: estimación paramétrica, diseño de la función de pérdida y modelado estadístico. Estas técnicas se aplican a varias tareas del procesamiento del lenguajes natural tales como clasicación de documentos, modelado del lenguaje natural &#13;\\ny traducción automática estadística. &#13;\\nEn relación con la estimación paramétrica, abordamos el problema del suavizado proponiendo una nueva técnica de estimación por máxima verosimilitud con dominio restringido (CDMLEa ). La técnica CDMLE evita la necesidad de la etapa de suavizado que propicia la pérdida de las propiedades del estimador máximo verosímil. Esta técnica se aplica a clasicación de documentos mediante el clasificador Naive Bayes. Más tarde, la técnica CDMLE se extiende a la estimación por máxima verosimilitud por leaving-one-out aplicandola al suavizado de modelos de lenguaje. Los resultados obtenidos en varias tareas de modelado del lenguaje natural, muestran una mejora en términos de perplejidad. &#13;\\nEn a la función de pérdida, se estudia cuidadosamente el diseño de funciones de pérdida diferentes a la 0-1. El estudio se centra en aquellas funciones de pérdida que reteniendo una complejidad de decodificación similar a la función 0-1, proporcionan una mayor flexibilidad. Analizamos y presentamos varias funciones de pérdida en varias tareas de traducción automática y con varios modelos de traducción. También, analizamos algunas reglas de traducción que destacan por causas prácticas tales como la regla de traducción directa; y, así mismo, profundizamos en la comprensión de los modelos log-lineares, que son de hecho, casos particulares de funciones de pérdida. &#13;\\nFinalmente, se proponen varios modelos de traducción monótonos basados en técnicas de modelado estadístico .",
  "full_text": "UNIVERSIDAD POLITÉCNICA DE V ALENCIA\nDEPARTAMENTO DE SISTEMAS INFORMÁTICOS Y COMPUTACIÓN\nStatistical approaches for natural language modelling\nand monotone statistical machine translation\nThesis\npresented by Jesús Andrés Ferrer\nsupervised by Dr. Alfons Juan Císcar and Prof. Francisco Casacuberta Nolla\nFebruary 9, 2010\n\nStatistical approaches for natural language modelling\nand monotone statistical machine translation\nJesús Andrés Ferrer\nThesis performed under the supervision of doctors\nAlfons Juan Císcar and Francisco Casacuberta Nolla\nand presented at the Universidad Politécnica de Valencia\nin partial fulﬁlment of the requirements\nfor the degree Doctor en Informática\nValencia, February 9, 2010\nWork has been partially supported by the CICYT “Centro de Investigación Cientíﬁca y\nTecnológica” under the Spanish project iDoc (TIC2003-08681-C02), by the Spanish\nresearch programme Consolider Ingenio 2010: MIPRCV (CSD2007-00018), by the\nPROMETEO program of the Generalitat Valenciana under grantPrometeo/2009/014; and by\nthe” Consellería d’Empresa, Universitat i Ciència - Generalitat Valenciana”under\ncontract GV06/252 and under grant CTBPRA/2005/004.\n\nAcknowledgements\nEstas líneas son las más difíciles de escribir de la tesis aunque no contengan formulas ni\nformalismos matemáticos. Si tuviese que enumerar a todas las personas que han afectado\npositivamente (ó negativamente) al resultado de esta tesis, probablemente, tendría que anotar\ntoda la tesis con comentarios al margen. Por ello, solo nombraré a las personas cuyas aporta-\nciones han sido constantes e/o importantes sin por ello menospreciar al resto de personas que\nhan inﬂuido.\nComenzaré por proximidad, agradeciendo a los habitantes del laboratirio101L del depar-\ntamento. No sólo a los miembros actuales, sino también a los fundadores de “Teachings of\nthe ﬁnal convergence”. En particular, agradecer a Ramón Granell por nuestras discusiones; a\nAdrián Gimenez por ejercer de “diablo” en las reﬂexiones de algunos de mis problemas; y a\nRicardo Sánchez por ser tan “coloquial y amigable”.\nEsta tésis no habría existido de no ser por el apoyo económicode la “Generalitat Valen-\nciana” formalizado en la beca FPI con referencia CTBPRA/2005/004 recibida bajo el amparo\nde la “Conselleria d’Empresa, Universitat i Ciència”. Así mismo estoy en deuda con Alfons\nJuan Císcar tanto por haber aceptado ser mi director de tesis, como por su consejo y su tesón\nen evitar mi dispersión en tantos temas interesantes que existen en la investigación. También\ndebo agradecimiento a Francisco Casacuberta Nolla por su reﬂexiones, correcciones y guía.\nA parte del agradecimiento a mis directores de tesis me gustaría agradecerle a Prof. Her-\nmann Ney por haberme acogido en Aquisgrán permitiendome así“disfrutar” tanto del clima\nde dicha región germana, como de sus apasionantes reﬂexiones que me permitieron reﬁnar e\nincrementar mis conocimientos.\nMención especial requieren todos de mis compañeros de fatigas del PRHLT y del ITI en\ngeneral y algunos en particular como por ejemplo Jorge Civera por soportar mis reﬂexiones\nsobre asuntos no sólo de investigación y por la plantilla de esta tesis que me ha ahorrado\nmuchos quebraderos de cabeza; o Daniel Ortiz e Ismael GarcíaVarea por nuestras colabora-\nciones que son parte de esta tesis; así como a todos aquellos que con asiduidad han asistido\nlas cenas minimalistas y no minimalistas.\nApropiandome de una cita del célebre Adrián Giménez “Cada tesis esconde un drama\npersonal”; y este caso no podría ser una excepción. Así que estoy en deuda con todos aquellos\nque me han permitido disfrutar de la vida durante esta ardua einterminable tarea. Me veo en\nel deber de comenzar por la fuerte amistad que forgé con mis compañeros de carrera: Alex,\nGabi y Jesús; amistad que todavía hoy perdura.\nv\nDesde mi retiro como exremero, no puedo sino que agradecer a todo el equipo de remo de\nla Universidad Politécnica de Valencia, tanto por las millas y el sudor derramado en la mar;\ncomo por las cervezas ingeridas y derramadas en la mesa de algún bar. En especial a todos\naquellos remeros que me han acompañado desde el comienzo no sólo de esta tesis sino de\nmis estudios en informática cuando decicí practicar tan gratiﬁcante deporte.\nPor último y más importante, tengo un especial y profundo agradecimiento a toda mi\nfamilia: a mis hermanos Joaquín y Juan por no ser solo hermanos sino amigos que me han\nacompañado durante mis estudios, mis sesiones cinemátograﬁcas (algunas de las cuales “de-\nmasiado” antiguas), y en mis incursiones a la mar y al bar; a micuñada, Llum, por alimen-\ntarme con pizzas en su casa; a mi tía por estar siempre ahí comorefuerzo; y a mis padres\npor guiarme a través de la vida, enseñarme los valores que me rigen como persona y por apo-\nyarme en todo momento independientemente de alegría o pena.Finalmente, mi más profundo\nagradecimiento a mi novia Spe por su paciencia y amor incondicional que yo he compartido,\ncomparto, y, espero, compartiré.\nJesús Andrés Ferrer\nValencia, February 9, 2010\nvi JAF-DSIC-UPV\nAbstract\nThis thesis gathers some contributions to statistical pattern recognition and, more speciﬁcally,\nto several natural language processing (NLP) tasks. Several well-known statistical techniques\nare revisited in this thesis: parameter estimation, loss function design and probability mod-\nelling. The former techniques are applied to several NLP tasks such as text classiﬁcation\n(TC), language modelling (LM) and statistical machine translation (SMT).\nIn parameter estimation, we tackle the smoothing problem byproposing a constrained\ndomain maximum likelihood estimation (CDMLE) technique. The CDMLE avoids the need\nof the smoothing stage that makes the maximum likelihood estimation (MLE) to lose its good\ntheoretical properties. This technique is applied to text classiﬁcation by mean of the Naive\nBayes classiﬁer. Afterwards, the CDMLE technique is extended to leaving-one-out MLE\nand, then, applied to LM smoothing. The results obtained in several LM tasks reported an\nimprovement in terms of perplexity compared with the standard smoothing techniques.\nConcerning the loss function, we carefully study the designof loss functions different\nfrom the0–1 loss. We focus our study on those loss functions that while retaining a similar\ndecoding complexity than the0–1 loss function, provide more ﬂexibility. Many candidate\nloss functions are presented and analysed in several statistical machine translation tasks and\nfor several translation models. We also analyse some outstanding translations rules such as\nthedirect translation rule; and we give a further insight into thelog-linear models, which\nare, in fact, particular cases of loss functions.\nFinally, several monotone translation models are proposedbased on well-known mod-\nelling techniques. Firstly, an extension to the GIATI technique is proposed to infer ﬁnite\nstate transducers (FST). Afterwards, a phrased-based monotone translation model inspired in\nhidden Markov models is proposed. Lastly, a phrased-based hidden semi-Markov model is\nintroduced. The latter model produces slightly improvements over the baseline under some\ncircumstances.\nvii\n\nResumen\nEsta tesis reune algunas contribuciones al reconocimientode formas estadístico y, más especí-\nﬁcamente, a varias tareas del procesamiento del lenguaje natural. Varias técnicas estadísticas\nbien conocidas se revisan en esta tesis, a saber: estimaciónparamétrica, diseño de la función\nde pérdida y modelado estadístico. Estas técnicas se aplican a varias tareas del procesamiento\ndel lenguajes natural tales como clasiﬁcación de documentos, modelado del lenguaje natural\ny traducción automática estadística.\nEn relación con la estimación paramétrica, abordamos el problema del suavizado pro-\nponiendo una nueva técnica de estimación por máxima verosimilitud con dominio restringido\n(CDMLE a). La técnica CDMLE evita la necesidad de la etapa de suavizado que propicia la\npérdida de las propiedades del estimador máximo verosímil.Esta técnica se aplica a clasiﬁ-\ncación de documentos mediante el clasiﬁcador Naive Bayes. Más tarde, la técnica CDMLE\nse extiende a la estimación por máxima verosimilitud por “leaving-one-out” aplicandola al\nsuavizado de modelos de lenguaje. Los resultados obtenidosen varias tareas de modelado\ndel lenguaje natural, muestran una mejora en términos de perplejidad.\nEn cuanto a la función de pérdida, se estudia cuidadosamenteel diseño de funciones de\npérdida diferentes a la0–1. El estudio se centra en aquellas funciones de pérdida que rete-\nniendo una complejidad de decodiﬁcación similar a la función 0–1, proporcionan una mayor\nﬂexibilidad. Analizamos y presentamos varias funciones depérdida en varias tareas de tra-\nducción automática y con varios modelos de traducción. También, analizamos algunas reglas\nde traducción que destacan por causas prácticas como, por ejemplo, la regla de traducción\ndirecta; y, así mismo, profundizamos en la comprensión de los modelos log-lineares, que son\nde hecho, casos particulares de funciones de pérdida.\nFinalmente, se proponen varios modelos de traducción monótonos basados en técnicas de\nmodelado estadístico bien conocidas. En primer lugar, se propone una extensión a la técnica\nde GIATI, para inferir trasductores de estados ﬁnitos. Más tarde, se propone un modelo de\ntraducción basado en secuencias de palabras e inspirado en los modelos ocultos de Markov.\nEn último lugar, se presenta un modelo de traducción basado en secuencias de palabras y\nsemi-modelos de Markov. Este último modelo produce mejorassobre la referencia en ciertas\ncircunstancias.\naDel inglés “Constrained Domain Maximum Likelihood Estimation”\nix\n\nResum\nAquesta tesi reuneix algunes contribucions al reconeiximent de formes estadístic i més es-\npecíﬁcament a diverses tasques del processament del llenguatge natural. Diverses tècniques\nestadístiques ben conegudes són revisades en aquesta tesi:estimació paramètrica, disseny\nde la funció de pèrdua i modelatge estadístic. Les tècniquesanteriors s’apliquen a diverses\ntasques del processament del llenguatge natural com ara classiﬁcació de documents, mode-\nlatge estadístic del llenguatge i traducció automàtica estadística.\nEn relació amb l’estimació paramètrica, portem a cap el problema del suavitzat proposant\nuna tècnica d’estimació per màxima versemblança amb dominirestringit (CDMLEb). La\ntècnica CDMLE evita la necessitat de l’etapa de suavitzat que afavoreix la pèrdua de les\nbones propietats de l’estimador per màxima versemblança. Aquesta tècnica s’aplica a classi-\nﬁcació de documents mitjançant el classiﬁcador Naive Bayes. Més tard, la tècnica CDMLE\ns’exten a l’estimació per màxima versemblança amb “leaving-one-out”, i aleshores, s’aplica\nal suavitzat de models de llenguatge. Els resultats obtinguts en diverses tasques de modelat\ndel llenguatge mostren una millora en perplexitat.\nEn el diseny de la funció de pèrdua, s’estudia cuidadosamentel diseny de funcions de\npèrdua diferents a la0–1. L’estudi es centra en aquelles funcions de pèrdua que retenen\nuna complexitat de decodiﬁcació semblant a la funció0–1 però proporcionant una major\nﬂexibilitat. Analitzem i presentem diverses funcions de pèrdua en diverses tasques de tra-\nducció automàtica amb diversos models de traducció. Tambè analitzem algunes regles de\ntraducció que destaquen per causes pràctiques com ara la regla de traducció directa; i axí\nmateixa, s’aprofundeix en la comprensió dels models log-linear, que son de fet casos partic-\nulars d’aquestes funcions de pèrdua.\nFinalment, es proposen diversos models de traducció monòtons basats en tècniques del\nmodelatge estadístic ben conegudes. En primer lloc, es proposa una extensió a la tècnida de\nGIATI per a inferir transductors d’estats ﬁnits. Més tard, es proposa un model de traducció\nbasat en sequències de paraules i inspirat en els models ocults de Markov. En darrer lloc, es\npresenta un model de traducció basat en sequències de paraules i en semi-models de Markov.\nAquest darrer model produiex millores en certes circumstàncies.\nbDe l’anglés “Constrained Domain Maximum Likelihood Estimation”\nxi\n\nPreface\nNatural language processing (NLP) is a dynamic research ﬁeld that aims at developing com-\nputer systems that are able to automatically generate and understand natural human language,\nboth written and spoken. NLP is a subsoiled of artiﬁcial intelligence and linguistics, and as\nsuch it combines theories, methodologies, and experts fromboth worlds in order to address\nchallenging problems.\nCurrent technology in NLP is mainly based on inductive statistical pattern recognition\napproaches. These approaches deﬁne one or several statistical models that have to be esti-\nmated from a training dataset or corpus. There are several inferring criteria to perform such\ntasks; however, the maximum likelihood estimation (MLE) isone of the most wide-spread\ntechniques.\nThe MLE veriﬁes several desirable properties; however, a proper generalisation is con-\nspicuous for its absence. Actually, the MLE tends to overﬁt the models to the training dataset,\nand, hence, to produce the corresponding lack of generalisation. To amend this problem, a\ncommon approach is to apply several heuristics in an additionalsmoothing step.\nOnce the training criterion is chosen and a proper probability model is designed for the\ntask that is of interest, the best rule for building the system must be determined. Decision\ntheory (DT) properly deals with this question by introducing the loss function. The loss\nfunction assesses the mistakes that a given system can produce. Typically, in many of the\nNLP tasks, a0–1 loss function is assumed. Roughly speaking, this function accounts for the\nintuitive idea of minimising the number of errors that the system produces. This loss function\nyields the optimal Bayes’ rule, which is the best rule that can be built in order to minimise this\nloss. However, this simple and intuitive loss does not take full advantage of this framework.\nThis statistical framework is applied to several NLP tasks.Speciﬁcally, this thesis is\nfocused on exploring three of these NLP tasks: text classiﬁcation (TC), machine translation\n(MT), and language modelling (LM).\nGiven a repository of documents, the purpose of TC is to automatically structure the\ndocuments by assigning a label to each one. The label can be automatically generated in the\ncase of clustering or deﬁned by an expert. In this way, the tasks of searching and browsing\ndocuments in the repository is eased. The TC technology seems to have reached a mature\nstage of research; nevertheless, there is still room for improvement.\nThe objective of MT is to make the computer automatically translate texts or utterances\nfrom one language into another language, without changing the underlying meaning. The\nxiii\nMT community is mainly focused on three main applications: fully-automatic MT, computer-\nassisted MT and understandable rough translation. The current MT technology is based on\nstatistical methods in general and on the statistical pattern recognition theory in particular.\nFully-automatic statistical MT consists in the development of statistical models that are\nautomatically inferred from bilingual parallel texts. In this respect, there have been different\nproposals for statistical translation models ranging fromword-alignment translation models,\nsuch as the IBM models, the HMM word-alignment model, etc; tophrase-based and syntax-\nbased translation models. These last models are usually based on byproducts of the word-\nalignment model training process. The most widespread and commonly used models are the\nphrased-based models; however, these models do not usuallytake into account the bilingual\nsegmentation process, and are consequently heuristicallytrained.\nThe last topic of this thesis is LM itself. This problem is a very demanding and interesting\nproblem since language models are used in a vast range of NLP problems such as speech\nrecognition and machine translation among many others. Themost widely used models, not\nonly for their simplicity but also for their outstanding performance, are the so-calledn-gram\nmodels. Similar to most of the statistical models in patternrecognition,n-gram language\nmodels suffer from overﬁtting. Several smoothing techniques have been proposed in the\nliterature to deal with this problem.\nThe main objectives of this thesis are the followings: to present a new smoothing ap-\nproach applied to TC and LM; to introduce new models in the paradigm of MT for monotone\nlanguages; and to study the loss function. The contributions of this thesis can be divided into\nthree groups:\n1. Constrained-Domain Maximum likelihood estimation.By reviewing the MLE de-\nﬁciencies and the smoothing techniques used to alleviate them, we propose a modiﬁed\nversion of the MLE that avoids the smoothing step. This new proposal is applied to two\ndifferent modelling problems and tasks: text classiﬁcation and language modelling. In\ntext classiﬁcation, we show how the constrained-domain maximum likelihood estima-\ntion (CDMLE), is applied to multinomial distribution avoiding the additional smooth-\ning step. In language modelling, we use the CDMLE technique to smooth the leaving-\none-out (LOO) estimation of then-gram models since it is the milestone of the most\nsuccessfuln-gram smoothing techniques. This approach yields several novel smooth-\ning techniques some of which slightly improve the standard smoothing techniques.\n2. Fundamental equation of statistical machine translation.The optimal Bayes’ rule\nis the basis of all statistical machine translation systems. However, this rule is ob-\ntained with the assumption of a0–1 loss, i.e. assuming the CER as the error measure.\nWe review classical statistical decision theory, and by changing this loss function, we\nboost the system performance. We apply these ideas to SMT andprove that the log-\nlinear models are actually optimising a loss function, which resembles the actual error\nmeasure, such as the BLEU or the WER .\n3. Monotone statistical machine translation:One of the deﬁciencies of the phrase-\nbased models is that they are not “properly” modelled from a purely statistical point of\nview. This implies several problems in practise, since mostof the systems use heuristics\nto estimate those models. For instance, several statistical modiﬁcations or estimation\nxiv JAF-DSIC-UPV\ntechniques cannot be properly applied without several practical problems. We start\nby deﬁning a purely statistical phrase-based ﬁnite transducer model. Once we have\noutlined the deﬁciencies of this model, we propose ahidden Markov model (HMM)\nthat solves some of these deﬁciencies. However, forcing a HMM to take into account\nthe bilingual segmentation process of a phrase-based modelis not easy, and it yields\nhighly demanding training algorithms. Finally, we proposean improved model that is\nbased onhidden semi-Markov models (HSMM). This latter HSMM takes into account\nthe segmentation process smoothly, without producing highly demanding training al-\ngorithms.\nThe above contributions are sequentially organised in8 chapters that cover most of the\nwork developed in this thesis. We recommend a sequential reading of the document. How-\never, should readers be interested in a speciﬁc research area, they can opt to read those speciﬁc\nchapters taking into account the following graph:\n1. Preliminaries\n2. Constrained-domain\nmaximum likelihood estimation\n3. Constrained leaving-one-out\nfor language modelling\n4. The loss function in\nstatistical pattern recognition\n5. Statistical stochastic\nﬁnite state transducers\n6. A phrase-based\nhidden Markov model\nfor monotone machine\ntranslation\n7.A phrase-based hidden\nsemi-Markov model for\nmonotone machine\ntranslation\n8. Conclusions\nThe constrained-domain MLE approach is proposed in Chapter2. The experimental prop-\nerties of this new approach are compared with classical smoothing methods in the task of TC.\nLater, in Chapter 3, this approach is extended to then-gram models that are estimated and\nsmoothed by leaving-one-out (LOO). If readers are not familiar with leaving-one-out smooth-\ning methods for language modelling, then Section 1.2 would be helpful to them.\nJAF-DSIC-UPV xv\nThe optimal Bayes’ decision rule obtained when a0–1 loss function is used is one of\nthe bases of most statistical pattern recognition systems.Speciﬁcally, when it is applied to\nstatistical MT, it is called the fundamental equation of statistical machine translation and it is\na milestone of the current MT systems. This optimal rule is revisited and studied in detail in\nChapter 4.\nThe ﬁnal part of this thesis is concerned with the deﬁnition of an efﬁcient monotone ma-\nchine translation model. Chapter 5 summarises the ﬁrst incursion in this area. Inspired from\nChapter 5, the following chapter tackles the monotone MT problem by deﬁning a phrased-\nbased hidden Markov model (PBHMM). Unfortunately, this model also had some important\ndrawbacks. From the experience acquired with these two models, in Chapter 7, we introduce\na phrased-based hidden semi-Markov model (PBHSMM) that achieves our objective: good\nperformance, efﬁcient training algorithms, and a properlydeﬁned statistic framework that\nwould allow us to further improve the proposed model. This model is based on the hidden\nsemi-Markov models (HSMMs). In Section 1.1.6 Chapter 1, we reformulate the HSMM fol-\nlowing a new notation, that to our knowledge, has not been proposed elsewhere. Moreover,\nSection 1.1.6 paves the way towards the novel model proposedin Chapter 7. Should readers\nbe unfamiliar with the HSMM, we encourage them to read Section 1.1.6 in Chapter 1.\nxvi JAF-DSIC-UPV\nNotation\nSymbol Meaning\nconst(x) is a constant function onx, i.e., a function thatdoes notdepend onx\npr (· · ·) is the actual unknown probability distribution\npθ (· · ·) it is used to outline the fact that the probability is not the actual probability\nbut a model that depends on a parametric vectorθ\np(e) orpe the probabilities depicted in this way are already a model parameter for the\nevente\nA := B is used to stress that A ismodelledby B, and also to stress that A isdeﬁned\nas B\nδ(a, b ) is the Kronecker delta function, i.e.,1 if and only ifa = b, and0 otherwise\nxj\n1\nis used to denote the (sub)stringx1 . . . x j\nN is the natural number set, i.e.N = {0, 1, 2, 3, . . . }\n⟨f (x)⟩y is used to denote the expectation off (x) over the probability distribution\npr (y)\n⟨f (x)⟩q(y) is used to denote the expectation off (x) using the functionq(y) as the\nprobability distribution\nD(p||q) is the Kullback-Leibler divergence betweenp and q\nsuf n−1(x) stands for then −1 ending elements ofx or the full stringx if|x| < n −1\nFor denoting probability distributions throughout the thesis, we identify values and ran-\ndom variables whenever this entails no confusion. For instance, instead of\npr (Ω = ω) (1)\nwe use\npr (ω) (2)\nIn the case of summations and products, we will omit the limits or set in which the index\nvariable varies whenever this entails no confusion. For instance,\n∑\nx\nf (x) =\n∑\nx∈X⋆\nf (x) , (3)\nxvii\nor\n∏\nt\nf (t) =\nT∏\nt=0\nf (t) . (4)\nMoreover, we use here the notation for which a summation on a void set is equal to0,\n∑\nx∈∅\nf (x) = 0 , (5)\nand where a void product is equal to1,\n∏\nx∈∅\nf (x) = 1 . (6)\nIn this thesis several conditional probabilities will be used. For instance,\np(u | v) , (7)\nwhere we have used the symbol “| ” to divide the random variables into the given part,v,\nand the random variable for which the probability function is deﬁned,u. However, in order\nto avoid cumbersome notation, the conditional probabilities will sometimes be written as\nfollows\np(u/v) , (8)\nwhere “/” plays the role of “| ”. This ambiguity in the notation, is better understood withthe\nfollowing example\np(u / v, |v| , |u|) , (9)\nwhere if we had used “| ” instead of “/”, the equation would have been awkward and unclear:\np(u | v, |v| , |u|) . (10)\nxviii JAF-DSIC-UPV\nContents\nAcknowledgements v\nAbstract vii\nResumen ix\nResum xi\nPreface xiii\nNotation xvii\nContents xix\n1 Preliminaries 1\n1.1 Statistical Pattern Recognition . . . . . . . . . . . . . . . . . . .. . . . . . 2\n1.1.1 Statistical modelling . . . . . . . . . . . . . . . . . . . . . . . . . .3\n1.1.2 Training criterion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1.3 Maximum likelihood estimation (MLE) . . . . . . . . . . . . . .. . 5\n1.1.4 Maximum likelihood estimation for hidden variable models . . . . . 6\n1.1.5 Hidden Markov models (HMMs) . . . . . . . . . . . . . . . . . . . 8\n1.1.6 Hidden semi-Markov models (HSMMs) . . . . . . . . . . . . . . . .11\n1.2 Language Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.2.1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.2.2 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . . .16\n1.2.3 Leaving-one-out smoothing techniques . . . . . . . . . . . .. . . . 17\n1.2.4 Language modelling for text classiﬁcation . . . . . . . . .. . . . . . 21\n1.3 Statistical Machine Translation . . . . . . . . . . . . . . . . . . .. . . . . . 21\n1.3.1 Statistical word-based translation systems . . . . . . .. . . . . . . . 23\nIBM model 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nIBM model 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n1.3.2 Statistical phrase-based translation systems . . . . .. . . . . . . . . 25\nxix\nContents\nGenerative phrase-based models . . . . . . . . . . . . . . . . . . . . 25\nHeuristic phrase-based models . . . . . . . . . . . . . . . . . . . . . 26\n1.3.3 Automatic MT evaluation metrics . . . . . . . . . . . . . . . . . .. 27\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2 Constrained-Domain Maximum Likelihood Estimation 33\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.2 Naive Bayes model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.3 Conventional naive Bayes training . . . . . . . . . . . . . . . . . .. . . . . 35\n2.4 Constrained-domain maximum likelihood estimation . . .. . . . . . . . . . 37\n2.4.1 Characterisation of the solution . . . . . . . . . . . . . . . . .. . . 37\n2.4.2 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n2.4.3 Algorithm correctness and complexity . . . . . . . . . . . . .. . . . 40\n2.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3 Constrained leaving-one-out for language modelling 47\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.2 Leaving-one-out for language modelling . . . . . . . . . . . . .. . . . . . . 50\n3.2.1 The smoothing distributionβ(w|¯h) . . . . . . . . . . . . . . . . . . 52\n3.2.2 The interpolated smoothing model . . . . . . . . . . . . . . . . .. . 53\n3.3 Interval Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . 53\n“Lower bound is active” . . . . . . . . . . . . . . . . . . . . . . . . 55\n“Upper bound is active” . . . . . . . . . . . . . . . . . . . . . . . . 56\n“Unbound case” . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\nThe actual solution . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n3.4 Quasi-monotonic constraints . . . . . . . . . . . . . . . . . . . . . .. . . . 57\n3.5 Monotonic Constraints with Upper Bounds . . . . . . . . . . . . .. . . . . 59\n3.5.1 Monotonic constraints . . . . . . . . . . . . . . . . . . . . . . . . . 61\n3.6 Exact extended Kneser-Ney smoothing . . . . . . . . . . . . . . . .. . . . . 62\n3.7 A word on time complexity . . . . . . . . . . . . . . . . . . . . . . . . . . .63\n3.8 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n3.9 Conclusions and future work . . . . . . . . . . . . . . . . . . . . . . . .. . 73\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n4 The loss function in statistical pattern recognition 81\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.2 Bayes Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .82\n4.3 Statistical Machine Translation . . . . . . . . . . . . . . . . . . .. . . . . . 86\n4.3.1 General error functions . . . . . . . . . . . . . . . . . . . . . . . . .87\n4.3.2 Simpliﬁed error functions . . . . . . . . . . . . . . . . . . . . . . .87\n4.3.3 Approximation to general error functions . . . . . . . . . .. . . . . 89\n4.3.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n4.3.5 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\nxx JAF-DSIC-UPV\nContents\nWord Based Translation experiments . . . . . . . . . . . . . . . . . 93\nPhrase-based translation experiments . . . . . . . . . . . . . . . . .96\n4.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n5 Statistical stochastic ﬁnite state transducers 105\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n5.2 Stochastic ﬁnite-state transducers (SFST) . . . . . . . . . .. . . . . . . . . 106\n5.2.1 Grammatical inference and alignments for transducerinference (GIATI)108\n5.3 Statistical GIATI (SGIATI) . . . . . . . . . . . . . . . . . . . . . . . .. . . 109\n5.4 Useful recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .111\n5.5 Maximum likelihood estimation of SGIATI . . . . . . . . . . . . .. . . . . 112\n5.6 Preliminary experiments . . . . . . . . . . . . . . . . . . . . . . . . . .. . 113\n5.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n6 A phrase-based hidden Markov model for monotone machine translation 119\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n6.2 Phrase-based hidden Markov model . . . . . . . . . . . . . . . . . . .. . . 120\n6.3 Forward and backward probabilities . . . . . . . . . . . . . . . . .. . . . . 123\n6.4 Decoding and Viterbi recurrence . . . . . . . . . . . . . . . . . . . .. . . . 124\n6.4.1 The decoding process . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n6.5 Maximum likelihood estimation . . . . . . . . . . . . . . . . . . . . .. . . 126\n6.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.6.1 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.6.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n6.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n7 A phrase-based hidden semi-Markov model for monotone machine translation 133\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n7.2 The phrase-based hidden semi-Markov model . . . . . . . . . . .. . . . . . 134\n7.3 Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.3.1 Forward recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.3.2 Backward recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.3.3 Viterbi recursion . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.4.1 Fractional counts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.4.2 Baum-Welch training . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.4.3 Viterbi training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n7.4.4 The model smoothing . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.5 Decoding Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .144\n7.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.6.1 Classical phrase-based models. . . . . . . . . . . . . . . . . . .. . . 145\n7.6.2 Log-linear models. . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\nJAF-DSIC-UPV xxi\nContents\n7.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n8 Conclusions 155\n8.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n8.2 Ideas and future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .157\n8.3 Scientiﬁc publications . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . 159\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\nA Karush-Kuhn-Tucker Conditions 163\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\nList of Figures 167\nList of Tables 169\nxxii JAF-DSIC-UPV\nChapter 1\nPreliminaries\n“ I could prove God, statistically.” G EORGE G ALLUP\nContents\n1.1 Statistical Pattern Recognition . . . . . . . . . . . . . . . . . . .. . 2\n1.1.1 Statistical modelling . . . . . . . . . . . . . . . . . . . . . . . . 3\n1.1.2 Training criterion . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.1.3 Maximum likelihood estimation (MLE) . . . . . . . . . . . . . .5\n1.1.4 Maximum likelihood estimation for hidden variable models . . . 6\n1.1.5 Hidden Markov models (HMMs) . . . . . . . . . . . . . . . . . 8\n1.1.6 Hidden semi-Markov models (HSMMs) . . . . . . . . . . . . . 11\n1.2 Language Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1.2.1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n1.2.2 Maximum Likelihood Estimation . . . . . . . . . . . . . . . . . 16\n1.2.3 Leaving-one-out smoothing techniques . . . . . . . . . . . .. . 17\n1.2.4 Language modelling for text classiﬁcation . . . . . . . . .. . . 21\n1.3 Statistical Machine Translation . . . . . . . . . . . . . . . . . . .. . 21\n1.3.1 Statistical word-based translation systems . . . . . . .. . . . . 23\n1.3.2 Statistical phrase-based translation systems . . . . .. . . . . . . 25\n1.3.3 Automatic MT evaluation metrics . . . . . . . . . . . . . . . . . 27\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n1\nChapter 1. Preliminaries\n1.1 Statistical Pattern Recognition\nA pattern recognition problem consists in classifying eachpossible input or object, sayx ∈ X , in\none class, sayω, from the set of all possible classes, sayΩ . Examples of pattern recognition prob-\nlems include text classiﬁcation, speech recognition, image classiﬁcation, face recognition, and machine\ntranslation, among others. A classiﬁcation system is, then, characterised by theclassiﬁcation function\nor rule\nc : X − →Ω (1.1)\nIn the eighties, the most popular approaches to most of the pattern recognition problems were\nrule-based. Rule-based approaches deﬁne a huge set of rulesbased on the knowledge engineers and\ndomain experts in order to build the classiﬁcation system. The main problem of these approaches is\nthe deﬁnition of hand-crafted rules and their maintenance.In the nineties, the rule-based approach was\nreplaced by inductive approaches, which manly involvedstatistical methods. These approaches have\nnumerous advantages:\n• The classiﬁcation function is learnt from the observation of a set of preclassiﬁed documents by\nan inductive process.\n• The same inductive process can be applied to generate different classiﬁers for different domains\nand applications. This fact introduces an important degreeof automation in the construction of\nad-hoc classiﬁers.\n• The maintenance task is signiﬁcantly simpliﬁed, since it only requires to retrain the classiﬁer\nwith the new working conditions.\n• The existence of off-the-self software to train classiﬁersrequires less skilled man power than for\nconstructing expert systems.\n• The accuracy of classiﬁers based on inductive techniques competes with that of human beings\nand supersedes that of knowledge engineering methods in several tasks such as text classiﬁcation,\nand speech recognition.\nSeveral methodologies can be applied to deﬁne the classiﬁcation function. Therefore, it is needed\nto ﬁnd a measure for comparing among different classiﬁcation systems. In order to quantify systems,\ntheclassiﬁcation error rate (CER)is deﬁned as the percentage of misclassiﬁcations performedby the\nsystem.\nThe classiﬁcation system performance is usually measured as a function of the classiﬁcation error.\nHowever, there are problems in which all the classiﬁcation errors do not have the same consequences.\nTherefore, a function that ranks each kind of error should beprovided. Theloss function,l(ωp|x, ω c ),\nevaluates thelossin which the classiﬁcation system incurs when classifying the objectx into the class\nωp, knowing that the correct class isωc . An outstanding loss function is the0–1 loss function\nl(ωp|x, ω c ) =\n(\n0 ωp = ωc\n1 otherwise . (1.2)\nIf a0–1 loss function is provided, then the optimal system minimises the classiﬁcation error rate.\nTaking into account the loss function deﬁnition, we deﬁne the risk when classifying an objectx, the\nso-calledconditional risk givenx, as the expected value of the loss function according to the posterior\nclass probability distribution, i.e.\nR(ωp|x) =\nX\nωc ∈Ω\nl(ωp|x, ω c ) pr (ωc|x) , (1.3)\n2 JAF-DSIC-UPV\n1.1. Statistical Pattern Recognition\nwhere pr (ωc |x) stands for the actual class posterior probability distribution. Note that bypr (. . . ) we\nwill henceforth denote the actual probability distributions.\nUsually, we want to compare system risks independently of any speciﬁc objectx. Using the con-\nditional risk, we deﬁne thethe global risk[Duda et al., 2001] as the contribution of all objects to the\nclassiﬁer performance, i.e.\nR(c) = E x [R(c(x)|x)] =\nZ\nX\nR(c(x)|x) pr (x)dx , (1.4)\nwhere R(c(x)|x) is the conditional risk givenx, as deﬁned in 1.4.\nIn practise, the global risk is approximated by the law of great numbers for a given test setT =\n(xn, ω n)N\nn=1 i.i.d. according topr (ω, x),\n¯RT (c) = 1\nN\nNX\nn=1\nl(c(xn)|xn , ω n) . (1.5)\nThe approximation of the global risk using a test setT is calledempirical riskon the test setT . If we\nuse the0–1 loss function, then the empirical risk simpliﬁes to the formerly deﬁned classiﬁcation error\nrate\n¯RT (c) = 1\nN\nNX\nn=1\nδ(c(xn), ω n ) , (1.6)\nwhere δ stands for the Kronecker delta function.\nOur aspiration is to design the classiﬁcation function thatminimises the global risk. Since minimis-\ning the conditional risk for each objectx is a sufﬁcient condition to minimise the global risk, without\nany loss of generality, the optimal classiﬁcation rule, namelyminimum Bayes’ risk, is the one that\nminimises the conditional risk for each object\nˆ c(x) = arg min\nω∈Ω\nR(ω |x) . (1.7)\nIf0–1 loss function is assumed, then the conditional risk is simpliﬁed to\nR(ωp|x) = 1 − pr (ωp|x) , (1.8)\nand then the optimal classiﬁcation rule is given by\nˆ c(x) = arg max\nω∈Ω\npr (ω |x) . (1.9)\nThis equation is well-known and often assumed to be optimal for all pattern recognition problems,\nalthough, the assumption of a0-1 loss function is always taken, either consciously or unconsciously.\n1.1.1 Statistical modelling\nIn Eq. (1.9) theclass-posterior probabilityis used in order to ﬁnd the optimal class, although this\nprobability is unknown. If we knew such probability, then wecould deﬁne the best classiﬁer for this\nframework, the so-calledBayes classiﬁer, and its CER would be the minimum possible CER, the so-\ncalledBayes classiﬁcation error rate.\nSince the posterior probability in Eq. (1.9) has to be approximated with a model, a common pre-\nliminary approach is to use the Bayes’ theorem in Eq. (1.9) yielding\nc(x) = arg max\nω\npr (x |ω)pr (ω)\npr (x)\nﬀ\n= arg max\nω\n{pr (ω)pr(x | ω)} , (1.10)\nJAF-DSIC-UPV 3\nChapter 1. Preliminaries\nwhere the posterior probability is substituted by two probabilities: the class priorpr (ω), and the object\nposteriorpr (x |ω). If actual probabilities are known, then both Eqs. (1.10) and (1.9) are equivalent.\nHowever, the latter Eq. (1.10) typically yield better approximations on real systems provided that actual\nprobabilities are unknown, and it is needed to model them.\nIt is particularly worthy of note that from Eq. (1.10) and knowing thatpr (A, B ) = pr (A)pr(B |A)\nthe following equation is obtained\nc(x) = arg max\nω\n{pr (x, ω )} . (1.11)\nProvided that we are focused on approximations to actual probabilities, most of the modelling\ntechniques are based on statistics. Typically, classical or frequentist statistics are applied, producing a\nclassiﬁcation of the models in two categories\n• Parametric models:where the actual probabilities are modelled according to any statistical dis-\ntribution, such as, the normal distribution, or the beta distribution.\n• Non-parametric models:where the actual probability is decomposed using statistical equiva-\nlences and afterwards modelled directly.\nAnother emerging modelling technique that has successfully been applied to several tasks such as\ntext classiﬁcation [Sutton and McCallum, 2006], or speech recognition [Heigold et al., 2007] is the\ndiscriminative models[Berger et al., 1996] or thelog-linear models. A log-linear model is deﬁned as\nan approximation to a probability distribution parametrised in the following way\npθ (ω|x) = 1\nZθ (x) exp(\nKX\nk=1\nθk fk (x, ω )) , (1.12)\nwith the set of parametersθ , and wheref (x, ω ) is a vector offeatures, deﬁned a priory as a part of\nthe modelling process. Finally,Zθ (x) is the normalisation constant that ensures that the posterior-class\nprobability sums up to one,\nZθ (x) =\nX\nω∈Ω\nexp(\nKX\nk=1\nθk fk (x, ω )) . (1.13)\nThe feature vectorf (x, ω ) is whatever vectorial function that obtainsK real values from the objectx\nand its classω. Anyway, the features are often count events, such as whether a certain word appears or\nnot; or such as the number of occurrences of a given word.\n1.1.2 Training criterion\nIn order to train the model parameters, theoptimalset of parameters, sayˆθ must be found. The prob-\nlem of the training criterion rises up because of the word “optimal”. Appropriateness depends upon a\ncriterion, which is summarised by thecriterion function (C). Given a criterion function, the optimal set\nof parameters,ˆθ , is determined by\nˆθ = arg max\nθ ∈Θ\n{C(θ )} . (1.14)\nOften the criterionC(θ ) cannot be mathematically calculated, and then, it is necessary a sample to\napproximate it,D = {x1, . . . , xn },\nˆθ = arg max\nθ ∈Θ\n{C(θ ; D)} . (1.15)\n4 JAF-DSIC-UPV\n1.1. Statistical Pattern Recognition\nNevertheless the expression in Eq. (1.14) is used indistinctly of whether a sample is needed or not.\nIt is important to remark the difference between the loss function deﬁned in Section 1.1 and the\ntraining criterion deﬁned in the Eq. (1.14). The former deﬁnes the best way to build a system for given\nprobability functions, whereas the latter determines the best way to obtain the optimal parameters,\nwhich would be used to approximate those probabilities.\nThere are several well-known and studied criteria such as maximum likelihood estimation (MLE),\nmaximum a posteriori probability (MAP) or minimum mean energy (MME). We focus on the former,\nthe wide-spread MLE.\n1.1.3 Maximum likelihood estimation (MLE)\nThe maximum likelihood estimation (MLE) criterion is one ofthe most wide-spread criteria which\nhas a well-founded motivation. It can be argued that since weare interested in the actual probability\ndistribution, we should minimise the “distance” (in terms of the Kullback-Leibler divergence) between\nthe model and the actual distribution, that is\nˆθ = arg min\nθ ∈Θ\n{KL(pr ||pθ )} , (1.16)\nwhere KL(pr || pθ ) is the Kullback-Leibler distance between the model and the actual probability, de-\nﬁned as follows\nKL(pr ||pθ ) =\nZ\nX\npr (x) log pr (x)dx −\nZ\nX\npr (x) log pθ (x)dx . (1.17)\nPlugging previous Eq. (1.17) into Eq. (1.16) yields\nˆθ = arg max\nθ ∈Θ\nZ\nX\npr (x) log pθ (x)dx\nﬀ\n. (1.18)\nSince Eq. (1.18) is typically unfeasible to solve, it can be approximated by the law of great numbers.\nFor a given sampleD = {x1, . . . , xn} i.i.d. according topr (x), Eq. (1.18) is approximated by\nˆθ = arg max\nθ ∈Θ\n(X\nn\nlog pθ (xn )\n)\n(1.19)\nIf we deﬁne thelog-likelihood function (LL)as follows\nLL(θ ) =\nX\nn\nlog pθ (xn) , (1.20)\nthen Eq. (1.19) is expressed as\nˆθ = arg max\nθ ∈Θ\n{LL(θ )} .\nTherefore, minimising the divergence between the actual probability distribution and the model\nyields the log-likelihood function as the criterion function, i.e.C(θ ) = LL( θ ). This criterion is\nnamed after the log-likelihood function and is so-calledmaximum likelihood (ML)criterion. Maximum\nlikelihood criterion typically leads to the intuitive solution of the relative frequencies. Note that since\nthe logarithmic is an increasing function, maximising the log-likelihood function depicted in Eq. (1.20)\nis the same that maximising the likelihood function deﬁned as follows,\nL(θ ) =\nY\nn\npθ (xn ) . (1.21)\nJAF-DSIC-UPV 5\nChapter 1. Preliminaries\nIn summary, given an independent and interchangeable sample D = {x1, . . . , xN }, the MLE\nconsists in solving the following maximisation\nˆθ = arg max\nθ ∈Θ\n{\nX\nn\nlog pθ (xn)} . (1.22)\nThe maximum likelihood estimation has been a core techniquein pattern recognition. However,\nthere is a little confusion in the bibliography around the MLE term. The term ML criterion is understood\nin statistics as the statistical criterion that we have presented here which is used to estimate the optimal\nset of parameters for any given probability distribution. In the pattern recognition literature, MLE refers\nto the estimation of the probabilitypr (x, ω ) using “statistical MLE”, i.e. maximising the following\nexpression\nˆθ = arg max\nθ ∈Θ\n{\nX\nn\nlog pθ (xn |ωn ) + log pθ (ωn)} . (1.23)\nThe MLE has several desirable properties:\n• The MLE is asymptotically unbiased\n• The MLE is asymptotically efﬁcient, i.e., asymptotically,no unbiased estimator has lower mean\nsquared error than the MLE\n• The MLE is asymptotically normal. As the number of samples increases, the distribution of the\nMLE tends to the Gaussian distribution with the actual valueas a mean and covariance matrix\nequal to the inverse of the Fisher information matrix\n• The maximum likelihood estimator is consistent\nThere are some regularity conditions which must be satisﬁedto ensure this behaviour:\n• The ﬁrst and second order derivatives of the log-likelihoodfunction must be deﬁned\n• The Fisher information matrix must be continuous and not zero-valued\nAlthough, the MLE is asymptotically unbiased, the MLE is biased in practice for “small” datasets.\nThe term small depends on the ratio of the dataset size to the number of parameters. In pattern recogni-\ntion, this problem is very common and it is known as the overﬁtting problem. The overﬁtting problem\nis understood in pattern recognition as the fact that the learnt set of parameters is very specialised for\nthe training data, and hence, a small amount of probability remains to be distributed among the unseen\ndata.\nA typical approach to alleviate this problem is to resort to asmoothing technique. A smoothing\ntechnique distorts the optimal set of parameters,ˆθ , in order to obtain a “smoothed” version of them,˜θ .\nSeveral of the smoothing techniques are heuristically inspired and make the optimal solution to lose all\nits theoretical properties.\n1.1.4 Maximum likelihood estimation for hidden variable models\nMaximum likelihood estimation usually leads to simple convex optimisation problems. However, if\nsome variables were unobserved, ﬁnding the optimal parameter set is not a simple problem any more.\nMany useful models arehidden variable models, i.e. part of the random variables are not observed in\npractice. Fortunately, theExpectation-Maximisation (EM)algorithm [Dempster et al., 1977a, Neal and\nHinton, 1998, Wu, 1983] ﬁnds the maximum likelihood parameters estimates in such problems. In this\nsection, we brieﬂy review the EM algorithm according to [Neal and Hinton, 1998].\nA model is said to be a hidden variable model if part of the model variable is not seen in our training\ndata. Therefore, it is diffuse whether hidden refers to the model or to the sample. In such a case, we\n6 JAF-DSIC-UPV\n1.1. Statistical Pattern Recognition\nsplit the observationx into two random variables,x = ( y, z): the hidden party and the visible partz.\nTherefore, the model probability is given by\npr (x) := p θ (y, z) . (1.24)\nThe model in Eq.(1.24) is referred to asthe complete model.\nSuppose that the joint probabilityy and z is parametrised usingθ , then the marginal probability of\nz, the so-calledincomplete model, is given by\npθ (z) =\nX\ny\npθ (y, z) , (1.25)\nwhere, for simplicity, we have assumed thaty has a discrete domain, as is often the case; anyway the\nresults can be generalised.\nGiven the observed dataz, we wish to ﬁnd the maximum likelihood estimate for the modelparam-\neters, that is to say the value ofθ that maximises the incomplete log-likelihood function given by\nLL(θ ) = log p θ (z) = log\nX\ny\npθ (z, y) . (1.26)\nThe EM algorithm starts with some initial pointθ (0), and then it proceeds to iteratively generate\nsuccessive estimates,θ (1), θ (2), . . . by repeatedly applying two steps: the Expectation (E) step and the\nMaximisation (M) step. On the one hand, the E step consists inﬁnding the (best) distribution for the\nunobserved variables, given the observed variables and thecurrent estimate of the parameters. On the\nother hand, the M step re-estimates the parameters to be those with maximum likelihood, under the\nassumption that the distribution found in the E step is the actual distribution for the latent or unobserved\nvariables.\nIt can be shown that each EM iteration improves the log-likelihood or leaves it unchanged. Note\nthat this implies that the EM algorithm is able to ﬁnd a local maximum but not a global one. This is both\nthe most important property and also the most important drawback of this technique. Broadly speaking,\nthe main drawback of the EM is that it delegates to the initialisation the responsibility of ﬁnding a global\nmaximum, and, hence, a bad initialisation of the parameterscan ruin the system performance.\nThe basis of the EM algorithm relay on deﬁning an alternativeobjective functionL(· · ·) to the log-\nlikelihood function in Eq. (1.26), and then maximise this alternative criterion. This alternative objective\nfunction is a variation and hence, one of its parameters is a probability function. Therefore, given a\nparameter setθ and a probability functionq(y), the variationL(q, θ ) is deﬁned as follows\nL(q, θ ) = LL( θ ) − D(q ||pθ ) , (1.27)\nwhere bypθ we denotepθ (y |z) and D(·||·) is the Kullback-Leibler divergence. It can be proved [Neal\nand Hinton, 1998] that if a local (or global) maximum ofL occurs atˆθ and ˆ q, thenLL(θ ) has a local\n(global) maximum atˆθ .\nAn iteration of the standard EM algorithm can be expressed interms of the functionL, since each\nsteps corresponds to the maximisation of one of its parameters while retaining the other ﬁxed, i.e.,\nE step Setq(k)(· · ·) to theq(y) thatmaximisesL(q, θ (k−1)).\nM Step Setθ (k) to theθ thatmaximisesL(q(k), θ ).\nIt has been proved [Neal and Hinton, 1998] that the E step is maximised whenq(k) = p θ (y |z). Then\nthe EM algorithm can be expressed in its conventional way [Dempster et al., 1977a]\nE step Compute the distributionq(k) over the domain ofya such thatq(y) = p θ (k−1) (y |z)\naActually over the domain of the random variableY, that accordingly to our notation is identiﬁed with its value\ny.\nJAF-DSIC-UPV 7\nChapter 1. Preliminaries\nM Step Setθ (k) to theθ that maximises expected value oflog p θ (y, z) with respect toq(k), that is to\nsay,⟨log p θ (y, z)⟩q(k) .\nMoreover, the M step of the EM algorithm may be only partiallyimplemented; and then the new\nestimates for the parametersθ (k) improve the functionL(q(k), θ ) given the distribution found in the\nE stepq(k), but do not maximise it. This partial M step always produces an improvement of the true\nlikelihood. This variant is known as theGeneralised Expectation-Maximisation (GEM)[Dempster\net al., 1977a]. On the other hand, the E step may also be only partially implemented, with the new\nestimate for the hidden probability distribution,q(k), only improving the functionL(q, θ (k−1)) given\nthe optimal parameter set in previous iteration, instead ofmaximising it. In summary, the EM algorithm\ncan be expressed as follows\nE step Find aq thatimprovesL(q, θ (k−1)) with respect toL(q(k−1), θ (k−1)), and setq(k) to it.\nM Step Find aθ thatimprovesL(q(k), θ ) with respect toL(q(k), θ (k−1)), and setθ (k) to it.\nThese two steps are repeated until convergence. The convergence is typically achieved when the\nrelative increment of log-likelihood from iterationk tok + 1 goes below a given threshold or when a\nmaximum number of iterations is achieved.\nThe so-calledViterbiEM, is an outstanding version of the EM algorithm. In this version, the\nhidden probability distributionq obtained in the E step is assumed to be a Dirac’s delta function at the\nmaximum point ˆy according topθ (k−1) (y |z). This assumption restates the E step as follows\ny(k) = arg max\ny ∈Y\npθ (k−1) (y | z) , (1.28)\nand yields the hidden probability distributionq\nq(k)(y) =\n(\n1 y = y(k)\n0 otherwise . (1.29)\nThe maximisation step is, then, reduced to a usual MLE without latent variables but with the sample\ncompleted with they(k) estimates.\nSince the Viterbi algorithm constrains the family of functions to optimise in the E-step, i.e.q(k)(y) =\nδ(y, y(k)); the parameter set that is obtained by the Viterbi approximation is typically worse than that\nof the actual EM without any constraint in the E-step.\nIn the remaining subsections, we will analyse two outstanding hidden variable models: the hidden\nMarkov models (HMMS) and the hidden semi-Markov models (HSMMs).\n1.1.5 Hidden Markov models (HMMs)\nAlthough initially introduced and studied in the late 1960sand early 1970s, statistical methods of\nMarkov source orHidden Markov modelling (HMM) have become an increasingly ﬁeld of interest\nin the last few decades. There are two strong reason because these HMMs have become so popular.\nFirst, the models are very rich in mathematical structure. Second, the models obtain very good results\nin practise when applied properly. The applications of HMM range from several pattern recognition\nproblems such as speech recognition to the ﬁeld of bio-informatics [Rabiner, 1989]. In this section, we\nbrieﬂy introduce the model in a generic way.\nGiven a vectorxJ\n1 , we want to model its probability distribution,pr (x). For simplicity reasons,\nwe assume through this section that all the inputs have a known and ﬁxed lengthJ , that is to say, when\nwe writepr (x), we are actually referring topr (x |J ). In order to model the previous probability, we\nassume that each elementxj of the vectorx has been produced oremittedin a different stateqj ∈ Q\nand, hence, the same elementxj can have different probability distribution depending on the state in\n8 JAF-DSIC-UPV\n1.1. Statistical Pattern Recognition\nq0 q1 q2 q3 q4 q5 q6 q7 q8 q9\nx1 x2 x3 x4 x5 x6 x7 x8 x9\nFigure 1.1:A graphical representation of the emission of a sequence of outputsx9\n1 by\na HMM. Note that this is not a graphical representation of a HMM topology.\nwhich it was emitted, i.e.,p(xj |qj ). Since we do not observe the vector of statesq in the training data,\nwe need to model the emission probability with a hidden variable model. Mathematically,\npr (x) =\nX\nq\npr (x |q)pr (q) , (1.30)\nwhere, as discussed above,\npr (x |q) :=\nY\nj\npθ (xj |qj ) . (1.31)\nA left-to-right decomposition is taken to model the state probabilitypr (q) under a ﬁrst order Markovian\nassumption, i.e.,\npr (q) =\nY\nj\npr (qj |qj−1\n0 ) := p( q0)\nY\nj\np(qj |qj−1) . (1.32)\nSo far, no special model for the emission probabilitiespθ (xj |q) is assumed.\nEach stateqj ∈ Q can represent either an indexQ = {0, 1, . . . , Q }, or something relevant. It is\nalso valuable to highlight that there is one special stateq0, the so-calledinitial state, which does not\nemit any dimension ofx. This special state models the chances that any of the remaining states has to\nbe the state to emit the ﬁrst output element. Note that although we have add here our notation to the\ninitial state notation, the initial state event is also modelled by using an initial state distribution [Rabiner,\n1989].\nGiven a set of parametersγ, which comprises the transition probabilitiesp(q |q′) and the emission\nparametersθ ; the HMM is given by\npγ (x) =\nX\nq\np(q0)\nY\nj\np(qj |qj−1) p θ (xj |q) , (1.33)\nwhere p(q0) = 1 since this “phantom” state is is always present (it is a sure event); wherep(q | q′) are\nthe model parameters for the transition probabilities and wherepθ (xj |q) is the emission probability\nmodelled with the parameter setθ .\nThere are many interesting questions to solve when dealing with HMM, however we focus here on\n3 of them:\n1. How to compute the probabilitypγ (x) for a given set of parametersγ and a given objectx.\n2. How to obtain the (most probable) sequence of statesˆq that has emitted a speciﬁc objectx.\n3. How to estimate the optimal set of parametersθ for a given training setD = {x1, . . . , xN }.\nThe ﬁrst question is typically solved by deﬁning the so-called forwardrecursion. This recursion is\nobtained by reordering the sums of the probability in Eq. (1.33), i.e.,\npγ (xJ\n1 ) =\nX\nq0\nX\nq1\n· · ·\nX\nqJ\nY\nj\npθ (xj |qj ) p(qj |qj−1) , (1.34)\nJAF-DSIC-UPV 9\nChapter 1. Preliminaries\nis equal to\npγ (xJ\n1 ) =\nX\nq0\np(q0) · · ·\nX\nqj\npθ (xj |qj ) p(qj |qj−1 ) · · ·\nX\nqJ\npθ (xJ |qJ ) p(qJ |qJ −1) . (1.35)\nIn this way, the forward recursionαj (q) is deﬁned as the joint probability of emitting the preﬁxxj\n1 and\nemitting the last elementxj in the stateq, i.e.\nαj (q) := p θ (xj\n1, Qj = q) = p θ (xj |q)\nX\nq′\np(q | q′)αj−1(q′) , (1.36)\nwith the base caseα0(q0) = 1 . The forward recurrence requires a time complexity ofO(Q2J ) to ﬁll\nin a table ofO(QJ ) elements.\nFinally, the probability of a given sequencexJ\n1 is computed as follows\npθ (xJ\n1 ) =\nX\nq\nαJ (q) . (1.37)\nA similar recursion can be deﬁned using a post-ﬁx arrangement of the sums instead of a preﬁx one.\nThe so-calledbackward recursionis deﬁned as follows\nβj (q) = p θ (xJ\nj+1 | Qj = q) =\nX\nq′\npθ (xj+1 |q′) p(q′ |q)βj+1(q′) , (1.38)\nwith the base caseβJ (q) = 1 . The computational requirements for computing the backward recurrence\nare the same of that of the forward recursion.\nThe second question is answered by deﬁning theViterbi recurrence. Given an emitted objectx we\nwant to obtain the state vectorq that maximises the probability of emitting such an object, i.e.\nˆq = arg max\nq0\n(\narg max\nq1\n(\n· · ·arg max\nqJ\n(Y\nj\npθ (xj |qj ) p(qj |qj−1)\n)\n· · ·\n))\n(1.39)\nBy reordering the maximisations and the products, the Viterbi recurrence is deﬁned as follows\nδj (q) = arg max\nq′\n˘\nδj−1(q′) p( q | q′)\n¯\npθ (xj | q) , (1.40)\nwith the base caseδ0 = q0. Note that, by backtracking frommaxq δJ (q) , the optimalˆq is obtained in\na time complexity ofO(Q2J ) with the aid of a recursion table ofO(QJ ) elements.\nSince the HMM is a hidden or latent model, some approximate inference algorithm is needed, and\nhence the third question has as many answers as approximate algorithms. The classical algorithms are\nobtained as the result of applying the EM. There are two main algorithms: Viterbi based training and\nBaum-Welch training. The former is theViterbiEM training (see Section 1.1.4) applied to HMM, in\nwhich the Viterbi recursion is used in the E step.\nThe latter is the instantiation of the conventional EM training to the HMM case. The re-estimation\nequation in the M step for the transition probabilities in that case are given by\np(q | q′) =\nP\nn\nP\nj ξnj (q, q ′)\nP\nn\nP\nj γnj (q′) (1.41)\nwhere γnj (q) is the probability of using the stateq in thej-th emission independently of which is the\nprevious state\nγnj (q) =\nX\nq′\nξnj (q, q ′) (1.42)\n10 JAF-DSIC-UPV\n1.1. Statistical Pattern Recognition\nq0 q1 q4 q6 q9\nx1 x2 x3 x4 x5 x6 x7 x8 x9 x10\nFigure 1.2:An instance of the generative segmentation process carriedout by a HSMM\nfor an output sequencex of10 elements.\nand whereξnj (q, q ′) stands for the probability of using the stateq for emitting thej-element having\nused the stateq′ for the previous emission, i.e.\nξnj (q, q ′) = p θ (Qj = q, Qj−1 = q′ |xn) = αj (q′) pθ (xj | q) p(q |q′)βj+1(q)\npθ (x) (1.43)\nWe omit the estimation equations for the emission probabilitypθ (xj |q) since no assumption is\nmade in its modelling. It should be noted that this is the mostimportant part of the model, and if it\nis modelled incorrectly, then it can ruin the system performance. However, for our interest, it is not\nneeded to further assume any speciﬁc emission distribution.\nThe main advantage of the Viterbi training with respect to the Baum-Welch is its speed. The\nBaum-Welch training is slower than the Viterbi training. However, it is also expected that Baum-Welch\ntraining obtains better practical results than the Viterbitraining since the Viterbi training only takes into\naccount the most probable path for each sample instead of allthe possible paths. Recall that this topic\nhas already been addressed in Section 1.1.4.\nThroughout all the section we have assumed that all the output objectsx had the very same length\nJ . In order to make the same model able to manage different lengths, a usual approximation is to add\na non-emitting ﬁnal stateqF or output symbol$, and hence, the transition probabilityp(qF |q) or the\nemission probabilityp($ |qJ +1) is used for modelling the length. For instance in the case of the ﬁnal\nstate the parametrised model is given by:\npγ (x) =\nX\nq\np(q0)\nY\nj\np(qj |qj−1) pθ (xj |q) p(qF |qJ ) . (1.44)\nNote that a further subtle assumption is hidden in this modelsince each transition probability and emis-\nsion probability does not depend on the lengthJ , and, hence, the emission and transition probabilities\nhave been merged for all the lengths [Rabiner, 1989].\n1.1.6 Hidden semi-Markov models (HSMMs)\nGiven a sequence of observationsxJ\n1 , a hidden semi-Markov model (HSMM) [Ostendorf et al., 1996]\nis a modiﬁcation of HMM. A HSMM emits a segmentxj+l−1\nj at each state instead of constraining\nthe emission to one elementxj as the (conventional) HMM. This way, the probability of emitting a\nsequence of observationsxj+l−1\nj in any state depends on the segment lengthl and the state itself. The\nﬁgure 1.2 depicts an instance of the emission process carried out by a HSMM.\nIn a hidden Markov model (HMM), the probability of emitting asegment of lengthl while remain-\ning in the same stateq, can only be simulated by transitions to the same stateq. This approximation\nyields a exponential decaying length probability model\np(l |q) = [p( q |q)]l−1 , (1.45)\nJAF-DSIC-UPV 11\nChapter 1. Preliminaries\n 0.0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1.0\n 1  2  3  4  5  6  7  8  9  10\np(l | q)\nLength (l)\np(q | q) = 0 .8\np(q | q) = 0 .2\np(q | q) = 0 .05\nFigure 1.3:The “simulated” length probability in a HMM for several values of loop\nprobabilitiesp(q | q). Note that we have used a continuous plot instead of a histogram\nplot for clarity’s sake.\nwhich is depicted in Fig. 1.3. This exponential decaying length probability model is not a good approx-\nimation for many cases.\nEven if the decaying length probability is not an important issue for a given problem, the simula-\ntion of a HSMM using a HMM also constrains the segment emission probability to be a naive Bayes\ndecomposition, i.e.\npθ (x\nj+lj −1\nj |q, l ) =\nj+lj −1Y\nk=j\npθ (xk |q) , (1.46)\nwhich again is not the best choice for many cases.\nThese two differences between an HMM and a HSMM, i.e., the exponential length distribution and\nthe naive Bayes posterior emitting probability; introducethe HSMM as a very interesting, appealing\nand powerful extension to the conventional HMM.\nThere are several ways to formalise the HSMM extension. Here, we advocate for a similar formal-\nisation of that found in Murhpy [2007]. The HSMMs are based ona hidden state sequenceq, which is\na property inherited from HMM. Additionally, HSMMs also need to deﬁne a length random variable,\nl, which stores the length of the segments emitted at each state. Oppositely to traditional HMM, the\nstate sequence and the length vector can show different dimension to that of the emitted objectxJ\n1 (see\nFig. 1.2 for an example). However, in order to clearly specify the semi-Markov modelling technique, a\nspecial representation of the state sequence and length vector is needed. Under this representation, both\nstate and length random variables share the length with the output object, i.e.,J .\nWe deﬁne the length vectorl = ( l1, l 2, · · ·, l J ) as a random variable that stores the length of\neach segment at the position at which the segment begins. Allthe remaining positions, which are not\nsegment beginnings, are set to0. For instance, in the example given in Fig. 1.2 the length vectorl\nisl10\n1 = (3 , 0, 0, 2, 0, 3, 0, 0, 2, 0). The vectorl can be extended with an additional elementl0 = 0\nwhenever it is necessary, yieldinglJ\n0 = ( l0, l 1, l 2, · · ·, l J ).\n12 JAF-DSIC-UPV\n1.1. Statistical Pattern Recognition\nGiven a length emission vectorl, we deﬁne its preﬁx counterpart¯l as\n¯lj =\njX\nk=1\nlk , j = 0 , 1, · · ·, J (1.47)\nas well as its previous segment preﬁx length\nπj = 1 +\ni(j)X\nk=1\nlk , j = 0 , 1, · · ·, J (1.48)\nwithi(j) equal to the starting of the previous segment. For instance,in Fig. 1.2, the preﬁx segment\nlength is¯l = ¯l\n10\n0 = (0 , 3, 3, 3, 5, 5, 8, 8, 8, 10, 10) and the previous preﬁx segment length is given\nby π = π 10\n0 = (1 , 1, 1, 1, 4, 4, 6, 6, 6, 9, 9). Note that if we know a preﬁx ofl, saylj\n1, then the\ncorresponding preﬁxes of¯l and π , say¯l\nj\n0 and π j\n0, are known as well.\nA similar deﬁnition to that of the length vectorl would be handful for dealing with the state se-\nquence vector. LetQ be the set of all possible states in which the model can emit anoutput segment,\nand let⊙ ̸∈Q be an extra null state. The state sequence vectorqJ\n0 stores the state in which each\nsegment has been emitted at the starting position of the segment. Hence, the remaining positions not\ncorresponding to the starting of any segment are set to the null state,⊙. For instance in the example in\nFig. 1.2 the state vectorq is deﬁned asq10\n0 = ( q0, q 1, ⊙, ⊙, q4, ⊙, q6, ⊙, ⊙, q9, ⊙). Given a pair made\nof a state vector and a length vector(q, l), they must verify that the null states⊙ and the0 lengths\nco-occur at the very same positions with the exception ofq0 which can never be⊙, in other words,\nqj = ⊙ if and only iflj = 0 , forj = 1 , 2, . . . , J .\nNote that we assume no speciﬁc parametrisation for the segment emission probability distribution\npθ (x\nj+lj −1\nj |qj , l j ) through the remaining of the current section.\nFinally, in order to model the output emission probability for a given sequence of observationsx\nwe unhide the state vectorq and the length vectorl\npr (xJ\n1 ) =\nX\nl\nX\nq\npr (q, l)pr (x |q, l) , (1.49)\nwhere we have introduced two probabilities: the state and length probabilitypr (q, l) and the emission\nprobabilitypr (x |q, l).\nThe ﬁrst probability in Eq. (1.49) is decomposed from left toright as follows\npr (q, l) =\nJY\nj=1\npr (qj , l j |qj−1\n0 , lj−1\n1 ) . (1.50)\nThe state transition probability in Eq. (1.50), is modelledonly in the segment boundaries. Let say\nthatj is one of these boundaries, then¯lj−1 + 1 = j and thereby, the current state cannot be null,\nqj ̸= ⊙. Note also that if there is a segment boundary atj, then the previous segment length cannot\nbe 0 and the previous state cannot be null; that is to saylπj−1 > 0 and qπj−1 ̸= ⊙. Finally, note\nthat all the segment lengthsl (and statesq), must be0 (and null), betweenπj−1 and j − 1; that is\nto saylj−1\nπj−1+1 = 0 (andqj−1\nπj−1 = ⊙). Although many of those conditions are redundant, we have\nspeciﬁed them for clarity’s sake. In order to simplify notation, we useC(j) to denote the predicate that\ncorresponds to all the previous conditions, i.e.,\nC(j) ≡ ¯\nlj−1 + 1 = j, lj−1\nπj−1 +1 = 0, l πj−1 > 0, q j ̸= ⊙, qj−1\nπj−1 = ⊙, q πj−1 ̸= ⊙ . (1.51)\nJAF-DSIC-UPV 13\nChapter 1. Preliminaries\nUsing the previous deﬁnition, the state transition probabilitypr (qj , l j |qj−1\n0 , lj−1\n1 ) is, ﬁnally, modelled\nas follows\npr (qj , l j | qj−1\n0 , lj−1\n1 ) :=\n8\n>\n<\n>\n:\np(qj , l j |qπj−1 ) C(j)\n1 l¯\nlj−1 +1 ̸= j, l j = 0 , q j = ⊙\n0 otherwise\n(1.52)\nNote that the latter casepr (qj , l j |qj−1\n0 , lj−1\n1 ) = 0 , is only possible if a segment length vector or state\nvector is out of the domain.\nIn this way, the state path probability in Eq. (1.52) is simpliﬁed to\npr (q, l) :=\nY\nj∈Z(q)\n1\nY\nj̸∈Z(q )\np(qj , l j | qπj−1 ) , (1.53)\nwhere Z(q) or simplyZ stands for the set of positionsj for whichqj is the null state⊙, or alternatively\nthe positions for whichl is0, i.e.lj = 0 . For instance, in Fig. 1.2 we haveZ = {2, 3, 5, 7, 8, 10}.\nSince one of the two products in Eq. (1.52) simpliﬁes to1, the state path probability in Eq. (1.53)\nis equal to\npr (q, l) :=\nY\nj̸∈Z\np(qj , l j |qπj−1 ) , (1.54)\nor, in order to simplify notation, to\npr (q, l) :=\nY\nt\np(qt , l t |qπt−1 ) , (1.55)\nwhere we have explicitly omitted thatt ∈ Z , but we use the indext instead ofj to keep in mind the\nwhole simpliﬁcation process without the need of specifyingany part of it.\nNote that although we have modelled the transition and length probabilities with the same parameter\np(qj , l j |qi ), these parameters are typically modelled with the following two parameters\np(qj , l j |qi ) := p( qj | qi) p( lj |qj ) . (1.56)\nSince this does not affect to the algorithms signiﬁcantly, we keep the more general notationp(qj , l j |qi ).\nThe emission probability in Eq. (1.49) can be decomposed in asimilar way to the state transition\nprobability as follows\npθ (x | q, l) :=\nY\nt\npθ (x(t) |qt , l t ) , (1.57)\nwhere x(t) stands forxt+lt −1\nt .\nPlugging Eqs. (1.55) and (1.57) into Eq. (1.49) the emissionprobability for HSMM is deﬁned as\nfollows\npθ (xJ\n1 ) :=\nX\nl\nX\nq\nY\nt\np(qt , l t |qπt−1 ) p θ (x(t) |qt , l t) . (1.58)\nSimilarly to the HMMs discussed in Section 1.1.5, we should answer to the same questions such\nas how to compute the probability for a given outputx. The answer to these questions is very similar\nto the HMM case, but taking into account an extra sum on the state emission lengths. For instance, the\nforward recursionfor this model is deﬁned by\nαt (q) = p θ (xt\n1, Qt = q) =\nX\nl\nX\nq′\npθ (xt\nt−l+1 |q, l ) p( q, l |q′)αt−l(q′) . (1.59)\n14 JAF-DSIC-UPV\n1.2. Language Modelling\nThe time complexity for computing such a recurrence isO(Q2J 2), which isJ times slower than the\nHMM counterpart. However, the segment length is often constrained to a maximum lengthM yielding\na time complexity ofO(Q2J M ); which is justM times slower anddoes not scale with the sentence\nlengthJ .\nThe analogousViterbiand backward recursions to the ones deﬁned for HMM are easilydeﬁned in\na similar way. On the one hand, the backward recursion exploits the following equation\nβt (q) = p θ (xJ\nt+1 | Qt = q) =\nX\nl\nX\nq′\npθ (xt+l\nt+1 |q′, l ) p(q′, l | q)βt+l(q′) . (1.60)\nOn the other hand, the Viterbi recursion needs an additionalmaximisation for the emitted segment\nlength in order to exploit the recursion, i.e.,\nδt (q) = max\nl\n\nmax\nq′\n˘\nδt−l(q′) p(q, l |q′)\n¯\npθ (xt\nt−l+1 | q, l )\nﬀ\n. (1.61)\nFinally, the estimation of the model parameters is performed in a similar way to the HMM but using\nthe re-deﬁned recurrences.\n1.2 Language Modelling\nLanguage modelling is a core task in several natural language processing problems such as statistical\nmachine translation (see Section 1.3) or speech recognition [Nadas, 1984] among others. The language\nmodelling (LM) task is stated as the problem of designing appropriate models that approximate the\nprobability of a given text,pr (w). Therefore, given a sentence or textw made up ofT words chosen\nfrom a lexiconW with replacement, our aim is to model the probabilitypr (w) with an “appropriate”\nmodel pθ (w).\nThere are several models for approximating the actual language probability distribution. For in-\nstance, hierarchical models use context-free grammars to capture long term dependencies [Benedí and\nSánchez, 2005]. However, one of the most widespread models is then-gram model [Goodman, 2001],\nwhich obtains surprisingly good performance although it only captures short term dependencies. The\nmain advantage of this model is the simplicity and good performance compared with other more com-\nplex models.\nThe n-gram model decomposes the language probability from left to right as follows\npr (w) =\nTY\nt=1\npr (wt |wt−1\n1 ) . (1.62)\nIn theory we could turn each product term in Eq. (1.62) into a parameter. In practice, however, this\nwould lead to a huge set of parameters that would be unfeasible to train. Therefore, an-Markovian\nassumption is made in order to keep a manageable amount of parameters. That is the same to say that,\nthe probability of thet-th word is assumed to depend only on then − 1 previous words, yielding the\nn-gram model\npr (w) :=\nTY\nt=1\np(wt |hn (wt−1\n1 )) , (1.63)\nwhere hn(wt−1\n1 ) or simplyh, stands for the(n − 1) words previous to the current positiont, i.e.\nhn (wt−1\n1 ) := wt−1\nmax{t−n+1,1} . (1.64)\nJAF-DSIC-UPV 15\nChapter 1. Preliminaries\nWe will henceforth use¯h to denote then − 2 previous words\n¯h = hn−1(wt−1\n1 ) , (1.65)\n¯\n¯h to denote then − 3 previous words, and so on.\nNote that in the Eq. (1.63), we have abused of notation since,for instance, the ﬁrst term in the prod-\nuct isp(w1), which is not a unigram but the probability ofw1 to be in theﬁrst position of the sentence.\nHence, if there are notn − 1 previous positions in the historyh, then the probabilityp(wt | wt−1\n1 ) is\nnot at-gram probability, but the probability forwt to be thet-th word knowing thatwt−1\n1 are at the\nt − 1 ﬁrst positions. This fact is usually made explicit in practice by concatenating at the beginning of\nthe sentence a special symbol, say “<s>”.\n1.2.1 Evaluation\nIn order to compare between different language models the(conditional) perplexity[Bahl et al., 1983]\non a test set,S = {s1, . . . , sM } is deﬁned as\nPP(S) = 2\n1\nN\nPM\nm=1 log2 pLM (sm ) , (1.66)\nfor a givenlanguage model (LM)pLM (· · ·); and whereN stands for the total number of words in the\ntest set. If the LM is an-gram model, then\nPP(S) = 2\n1\nN\nPM\nm=1\nPTm\nl=1 log2 p(sml | hn (sm l−1\n1 )) , (1.67)\nwhere Tm stands for the length of them-th outcome,sm .\nNote that the(conditional) perplexityis a geometric average of the log-likelihood,\nPP(S) = 2\n1\nN LLLM (S) . (1.68)\nThe perplexity can be understood as the average number of possible words that can come after a\ngiven preﬁx. The perplexity depends on two factors: the model efﬁciency and the task complexity.\nUnder the same circumstances, the less the perplexity is thebetter. Therefore, if we compare two\ndifferent smoothing techniques or models, the one with the smallest perplexity is the best one. However,\nthe criticism to this measure lies on the fact that an improvement in perplexity is not always related to\nan improvement on the system performance.\n1.2.2 Maximum Likelihood Estimation\nOnce then-gram model is simpliﬁed by then − 1 previous words assumption, its parameter set is\nstill large enough to annihilate our chances for obtaining areliable estimation from the training data.\nSpeciﬁcally, then-gram parameter set isb\n{p(w |h)} ∀ w ∈ W , ∀h ∈ W n−1 , (1.69)\nwith the following normalisation constraints\nX\nw\np(w |h) = 1 ∀h ∈ W n−1 , (1.70)\nwhere W denotes the vocabulary.\nbWe have intentionally omitted the parameters that initialise the history for simplicity sake, for instancep(w1).\n16 JAF-DSIC-UPV\n1.2. Language Modelling\nDue to the large number of free parameters and the always scarce data, it is a common approach\nto resort to smoothing techniques. For instance, for a trigram language model, the events that occur\nonly once or not at all typically represent a huge percentageof the total occurrences. Therefore, the\nprobabilities of these events are difﬁcult to estimate withconventional methods.\nFor better understand the overﬁtting problem in then-gram LM context, we must bare in mind\nthe (conventional) MLE estimation. Given a training corpus,w1 . . . w n . . . w N ; we know in each text\nposition,n, the observed wordwn and its conditional historyhn . In this case, the log-likelihood\nfunction is given by:\nLL({p(w|h)}) = PN\nn=1 log p( wn |hn ) (1.71)\n= P\nwh N (w, h ) log p( w |h) , (1.72)\nwhere in the last line we have changed the summation index by grouping the occurrences for the same\nword w and historyh. We will henceforth denote byN (w, h ) to all the occurrences in the training set\nof a givenn-gram,hw.\nIn order to obtain the MLE for then-gram model, we must maximiseLL({p(w|h)}) constrained\nby Eq. (1.70). Applying some convex optimisation techniques [Ney et al., 1997], the MLE is computed\nas follows\nˆ p(w|h) = N (w, h )\nN (h) , (1.73)\nwhere N (h) stands for the occurrences of the historyh in the training corpus, that is to say\nN (h) =\nX\nw\nN (w, h ) . (1.74)\nIt is seen that then-gram parameter set is very sparse, leading to poorly estimated MLE probabili-\nties. For instance, in Eq. (1.73), it is observed that for theunseenn-grams the probability is estimated\nas 0 even if all the words of then-gram occur in the training data. This overﬁtting problem derived\nfrom the scarce training data, is one of the most important drawbacks of then-gram model.\n1.2.3 Leaving-one-out smoothing techniques\nThe smoothing techniques forn-gram models [Goodman, 2001] range from adding a pseudo-count to\neach occurrence count, to discounting a probability massBh from the seenn-grams for each history\nh and redistribute it according to a smoothing distribution,β(· |¯h). For instance, the linear interpola-\ntion [Chen and Goodman, 1998] distributes the gained probability massBh among all words according\nto the smoothing distribution. On the other hand, the backing-off redistributes the probability only\namong unseen events.\nThe most successful smoothing techniques are based on theTuring-Good (TG) counts[Good, 1953,\nNadas, 1984] which are obtained byleaving-one-out (LOO). Speciﬁcally, the modiﬁed Kneser-Ney,\nwhich obviously is a modiﬁed version of the Kneser-Ney smoothing [Kneser and Ney, 1995], obtains\nthe best results [Goodman, 2001].\nThe main idea of the LOO-based smoothing models is to discount a probability mass from all\nthe seenn-grams by means of a discounting parameterλr for countsr < R beingR the maximum\ncount. Then, the gained probability mass,Bh, is redistributed among the unseen events according to\na smoothing probability distributionβ(w |¯\nh). No probability is discounted from the most frequent\nn-gram,R. Therefore, in order to smooth then-gram language model, we smooth the probability\nJAF-DSIC-UPV 17\nChapter 1. Preliminaries\nestimatesp(w|h) with the following smoothing model\n˜pλ (w|h) :=\n8\n>\n<\n>\n:\nR\nN (h) N (w, h ) = R\n(1 − λr ) r\nN (h) 0 < N (w, h ) = r < R\nBhβ(w|¯h) N (w, h ) = 0\n, (1.75)\nwhere Bh is the discounted probability mass deﬁned as follows\nBh =\nR−1X\nr=1\nλr nr (h) r\nN (h) , (1.76)\nso that the probability deﬁned in Eq. (1.75) sums up to1, and wherenr (h) are the counts-of-counts\nconditional to the previous historyh, i.e.\nnr (h) =\nX\nw\nδ(r, N (w, h )) . (1.77)\nThe discounting probability mass,β(w|¯h), is a lower order smoothing probability distribution deﬁned\nover the unseen words, i.e. X\nw:N (w,h)=0\nβ(w|¯h) = 1 . (1.78)\nThe leaving-one-out (LOO)criterion [Katz, 1987] is based on the MLE criterion, where each sam-\nple plays the role of both training and testing. We summarisethe basis of the formalisation found\nin [Ney et al., 1997, Sec. 4]. Firstly, equivalence classes are formed by gathering alln-gramshw which\nshare the very same countr = N (w, h ) and historyh, into the same equivalence class. Note that these\nequivalence classes simulate the result of the conventional MLE in Eq. (1.73), where all then-grams\nwith the same count share the same probabilityˆ p(w |h). Secondly, we count the number of differ-\nentn-grams in each class labelled with the countr,r = 0 , 1, ..., R ; and denote them bynr (h) (see\nEq. (1.77)). Finally, by leaving-one-out ann-gram observation in the class with countr for testing, it is\nmoved into the class with countr − 1. Thus, the associated probability is replaced with the probability\nof the classr − 1, obtaining in this way the LOO probabilities˜ploo(w | h)\n˜ploo(w|h) :=\n(\n(1 − λr−1) r−1\nN (h) 1 < N (w, h ) = r ≤ R\nBh β(w|¯h) N (w, h ) = 1\n. (1.79)\nIf this process is repeated for all occurrences and for all equivalence classesr = 1 , ..., R ; then, the\nLOO log-likelihood criterion is obtained\nF({λ R−1\n1 }) =\nX\nwh\nN (w, h ) log ˜ploo(w |h) (1.80)\n=\nRX\nr=2\nrnr log(1 − λr−1) +\nX\nh\nn1(h) log\n R−1X\nr=1\nλr nr (h)r\n!\n+ const( λ R−1\n1 ) ,\n(1.81)\nwhere nr stands for the the counts-of-counts unconditional to any previous history, i.e.,\nnr =\nX\nh\nnr (h) =\nX\nwh\nδ(r, N (w, h )) . (1.82)\n18 JAF-DSIC-UPV\n1.2. Language Modelling\nHowever, Eq. (1.81) is very difﬁcult to deal with; furthermore, no close solution forλr is known.\nTherefore, we make the following assumption [Ney et al., 1997, pag. 186]\nR−1X\nr=1\nλr nr (h)r = φh\nR−1X\nr=1\nλr nr r , (1.83)\nwhere φh is a constant value depending on the previous historyh but not in the countsr.\nAfter taking assumption in Eq. (1.83) into Eq. (1.81), the function to maximise is given by\nF(λ R−1\n1 ) =\nRX\nr=2\nrnr log(1 − λr−1) + n1 log\n R−1X\nr=1\nλr nr r\n!\n+ const( λ R−1\n1 ) , (1.84)\nfor which the solution is given by [Ney et al., 1997, pag. 186]\nˆλr = 1 − r⋆\nr (1 − nR R/N) , (1.85)\nwhere r⋆ stands for the Good-Turing count [Good, 1953, Nadas, 1984, Ney et al., 1997]\nr⋆ = nr+1(r + 1)\nnr\n, r = 1 , 2, . . . , R − 1 , (1.86)\nand abusing of notation,R⋆ = R. If we further assume thatnRR/N ≪ 1, then Eq. (1.85) simpliﬁes to\nˆλr = 1 − r⋆\nr . (1.87)\nFinally, the solution to the smoothed model is given by plugging Eq. (1.87) into Eq. (1.75), i.e.,\n˜p(w|h) :=\n(\nr⋆\nN (h) 0 < N (w, h ) = r ≤ R\nBhβ(w|¯h) N (w, h ) = 0\n, (1.88)\nwhere the smoothing distributionβ(w |¯\nh) is also estimated by LOO [Ney et al., 1997].\nIt is very illustrative to deﬁne the discounted probabilitymass Bh in terms of the TG counts. Using\nthe smoothing model solution in Eq. (1.88), and the deﬁnition of the discounting probability mass in\nEq. (1.76); the formerBh is computed as follows\nBh = 1 −\nRX\nr=1\nnr (h) r⋆N (h) (1.89)\n= 1\nN (h)\n \nN (h) −\nRX\nr=1\nnr (h)r⋆\n!\n,\nwhere taking into account the following property\nN (h) =\nRX\nr=1\nrnr (h) , (1.90)\nBh is expressed as\nBh = 1\nN (h)\n RX\nr=1\nrnr (h) −\nRX\nr=1\nr⋆nr (h)\n!\n, (1.91)\nJAF-DSIC-UPV 19\nChapter 1. Preliminaries\nand grouping common terms\nBh = 1\nN (h)\nRX\nr=1\n(r − r⋆ )nr (h) . (1.92)\nNote that the question of whether the normalisation constraints in Eq. (1.70) are satisﬁed depends on\nBh and, hence, on the TG countsr⋆ . This is due to the assumption in Eq. (1.83), since the deﬁnition of\nour model ensured these normalisation constraint to be veriﬁed.\nIt may be said that the conditional dependence of the counts on h is dropped when the assumption in\nEq. (1.83) is taken; resembling, in this way, the solution ofa joint smoothing model [Ney et al., 1997].\nConsequently, we should analyse a joint smoothing model to see whether this statement is true or not.\nThe joint smoothing model approximates the joint probabilities˜pλ (w, h ) instead of the conditional\nones,˜pλ (w|h), as follows\n˜pλ (w, h ) :=\n8\n>\n<\n>\n:\nR\nN N (w, h ) = R\n(1 − λr ) r\nN 0 < N (w, h ) = r < R\nBβ (w, ¯h) N (w, h ) = 0\n(1.93)\nwith the gained probabilityB independent from the previous history as follows\nB = 1\nN\nR−1X\nr=1\nλr nr r . (1.94)\nNote that both Eqs. (1.93) and (1.94) are analogous to Eqs. (1.75) and (1.76), except for the normalisa-\ntion constant that isN (h) in the latter and has been replaced forN in the former.\nIf we apply LOO to the joint smoothing model in Eq. (1.93), then the solution in Eq. (1.85) is\nobtained without any assumption. Therefore, we conclude that taking assumption in Eq. (1.83) with\nthe conditional model in Eq. (1.75) is equivalent to taking the assumption of optimising the parameters\nλ R−1\n1 for maximising the joint LOO log-likelihood function in Eq.(1.81) and, then, use them as if\nthey were the optimal parameters for the conditional model in Eq. (1.75). In theory, this assumption\ncan degenerate the probabilities as commented above, not ensuring the normalisation constraints in\nEq. (1.70). Moreover, in practice, we are interested in maximising the conditional model, since it\nwould produce smaller perplexities and eventually, this should improve the system performance. The\nmagnitude of this assumption mainly depends on the ﬁnally behaviour of these smoothing models.\nIt is worth noting that we have broadly reviewed the standardformulation given in [Ney et al., 1997]\nfor introducing the Turing-Good counts. Furthermore, the Kneser-Ney (KN) smoothing [Kneser and\nNey, 1995] is a special case of the model deﬁned in Eq. (1.75) that ties all the parametersλ R−1\n1 with\none discounting parameterb, i.e.,\nλr (b) = b\nr , (1.95)\nleaving just one free parameter to estimate: the former discounting parameter,b. Furthermore, an upper\nand lower bound to this parameter is obtained by LOO [Ney et al., 1997]\nn1\nn1 + 2 n2 + P\nr≥3 nr\n< b < n1\nn1 + 2 n2\n. (1.96)\nIn this case, similarly to other smoothing techniques, bothjoint and conditional smoothing models lead\nto the same solution, without degrading the probabilities by not verifying the normalisation constraints\nin Eq. (1.70).\n20 JAF-DSIC-UPV\n1.3. Statistical Machine Translation\n1.2.4 Language modelling for text classiﬁcation\nThe unigram language model is a special case of then-gram language model, forn = 1 . In such case,\nthe probability for a given sentence or textwT\n1 is given by\npr (w) :=\nTY\nt=1\np(wt ) , (1.97)\nwhere we have assumed that the occurrence probability of each word is independent of its position and\nother words, the so-calledNaive Bayes assumption. Note that if the probability of a text is given by\nEq.(1.97), then the count vector of words,x, such thatxd = Nw (vd ) is the number of occurrences of\nthe wordvd in the textw, follows a multinomial distribution, i.e.,\npθ (x|L) =\n„ L\nx\n« DY\nd=1\nθxd\nd , (1.98)\nwhere θd stands for the probability of the wordvd to occur and, hence, they must sum up to one\nDX\nd=1\nθd = 1 , (1.99)\nwith the deﬁnition „ L\nx\n«\n= L!\nQD\nd=1 xd!\n. (1.100)\nThe naive Bayeslanguage model has long been a core technique in informationretrieval and, more\nrecently, it has attracted signiﬁcant interest in pattern recognition and machine learning [Lewis, 1998].\nThis technique is specially outstanding in text classiﬁcation [Juan and Ney, 2002, Vilar et al., 2004]. In\nChapter 2, the naive Bayes language model is further analysed.\n1.3 Statistical Machine Translation\nIn this Section we review state-of-the-art applications and approaches in the ﬁeld ofmachine translation\n(MT), focusing on the statistical approach. The goal of MT is the automatic translation of a source\nsentencex into a target sentencey,\nx = x1 . . . x j . . . x J , x j ∈ X , j = 1 , . . . , J\ny = y1 . . . y i . . . y I , y i ∈ Y , i = 1 , . . . , I\nwhere xj and yi denote source and target words; andX and Y , the source and target vocabularies\nrespectively.\nOn the one hand, current MT technology is focused on three main applications:\n• Fully-automatic MT in limited domains like weather forecast [Langlais et al., 2005], hotel re-\nception desk [Amengual et al., 2000b], appointment scheduling, etc.\n• Post-editing for CAT, i.e., post-editing the human amendment of automatic translations produced\nby an MT system.\n• Understandable rough translation in which the aim is to allow a human to decide whether the\ntranslated text includes relevant information. For instance, this is used for document ﬁnding\npurposes or user assistance in software troubleshooting.\nJAF-DSIC-UPV 21\nChapter 1. Preliminaries\n• Interactive machine translation where a synergy between the user and the system is achieved by\nrestating the user interaction as an iterative process in which the user corrects the translations\ngiven by the system which proposes a new hypothesis for the unvalidated part of the translation\nin its turn.\nOn the other hand, state-of-the-art MT approaches can be classiﬁed according to the level of anal-\nysis of the source sentence before translating:\n• The interlingua approach [Arnold et al., 1993, Nirenburg etal., 1992, Nyberg and Mitamura,\n1992].\n• The transfer approach decomposes the translation process into three steps: analysis, transfer and\ngeneration. A review of transfer-based systems is presented in [Hutchins and Somers, 1992].\n• The direct approach refers to the word-by-word translationfrom the source sentence into the\ntarget sentence. Under this approach we ﬁnd example-based MT and statistical MT.\nInstatistical machine translation (SMT), this translation process is usually presented as a statistical\npattern recognition problem in which for a given source sentence x, the optimal target sentenceˆy is\nsearched according to\nˆy = arg max\ny\npr (y |x) , (1.101)\nwhere pr (y |x) is the probability fory to be the actual translation ofx. Note that Eq. (1.101) is simply\nthe adoption of the Bayes’ optimal classiﬁcation rule in Eq.(1.9) into the machine translation scope.\nThe so-calledsearch problemis to compute a target sentenceˆy for which this probability is maxi-\nmum. Applying Bayes’ theorem we can reformulate Eq. (1.101)as follows\nˆy = arg max\ny\npr (x |y)pr (y) , (1.102)\nwhere the termp(y | x) has been decomposed into atranslation modelpr (x | y) and alanguage model\npr (y). Intuitively, the translation model is responsible for modelling the correlation between source\nand target sentence, but it can also be understood as a mapping function from target to source words.\nWhereas the language modelpr (y) represents the well-formedness of the candidate translation y [Stol-\ncke, 2002].\nThe application of Eq. (1.101), minimises the CER which in MTscope corresponds tothe sentence\nerror rate (SER). However, the SER measure provides a rough and superﬁcial evaluation of the system\ntranslation quality and it is rarely used in favour of other more popular evaluation measures described\nin Section 1.3.3.\nThe search problem presented in Eq. (1.102) was proved to be an NP-complete problem [Knight,\n1999, Udupa and Maji, 2006]. However various research groups have developed efﬁcient search al-\ngorithms by using suitable simpliﬁcations and applying optimisation methods. Starting from the IBM\nwork based on a stack-decoding algorithm [Berger et al., 1996], greedy [Berger et al., 1994, Germann\net al., 2001, Wang and Waibel, 1998] and integer-programming [Germann et al., 2001] approaches to\ndynamic-programming search [García-Varea and Casacuberta, 2001, Tillmann and Ney, 2003].\nNevertheless, most of the current statistical MT systems present an alternative modelling of the\ntranslation process different from that presented in Eq. (1.101). The posterior probability is modelled\nas a log-linear combination of feature functions [Och and Ney, 2004] under the framework of maximum\nentropy [Berger et al., 1996]\nˆy = arg max\ny\nMX\nm=1\nλmhm (x, y) , (1.103)\nwhere λm is the interpolation weight andhm (x, y) is a function that assigns a score to the sentence\npair(x, y). Examples of features range fromhm (x, y) = log pr (x |y) orhm (x, y) = log pr (y), to\n22 JAF-DSIC-UPV\n1.3. Statistical Machine Translation\nhm (x, y) = exp(1) . Note that under this framework, Eq. (1.102) is a particularcase where\nh1(x, y) = log pr (x |y) (1.104)\nh2(x, y) = log pr (y) , (1.105)\nand λ1 = λ2 = 1 .\nIt is particularly worthy of note that Eq. (1.103) is quite similar to alog-linearmodel depicted in\nEq. (1.12) without the normalisation coefﬁcientZθ (x). For this reason, these models are commonly\nrefereed to as log-linear models [Och and Ney, 2004]. However, note that the ellipsis of the normal-\nisation constant plays an interesting role in these translations models since its omission is conserved\nthrough the training process as well, in contrast to the standard log-linear models. We will further\nanalyse these differences in Chapter 4.\n1.3.1 Statistical word-based translation systems\nA great variety of statistical translation models have beenproposed since the word alignment models\nwere proposed [Brown et al., 1993a, 1990]. Most of state-of-the-art statistical MT systems are based on\nbilingual phrases [Callison-Burch et al., 2007]. These bilingual phrases are sequences of words in the\ntwo languages and not necessarily phrases in the linguisticsense. The phrase-based approach to MT is\nfurther explored in Section 1.3.2.\nAnother approach which has become popular in recent years isgrounded on the integration of\nsyntactic knowledge into statistical MT systems [Ding and Palmer, 2005, Graehl and Knight, 2004,\nLin, 2004, Wu, 1996, Yamada and Knight, 2001]. This approachparses the sentence in one or both of\nthe involved languages, deﬁning then, the translation operations on parts of the parse tree. In [Chiang,\n2007], Chiang constructs hierarchical transducers for translation. The model is a syntax-free grammar\nwhich is learnt from a bilingual corpus without any syntactic information. It consists of phrases which\ncan contain sub-phrases, so that a hierarchical structure is induced.\nThe third main approach, which is currently investigated instatistical MT, is the modelling of the\ntranslation process as a ﬁnite-state transducer [Alshawi et al., 2000, Bangalore and Riccardi, 1995,\nCasacuberta and Vidal, 2004, Kanthak and Ney, 2004, Mariño et al., 2006]. This approach solves the\ntranslation problem by estimating a language model on sentences of extended symbols derived from the\nassociation of source and target words coming from the same bilingual pair. The translation transducer\nis basically an acceptor for this language of extended symbols.\nIn this section we brieﬂy review the word based models presented in Brown et al. [1993a]. In this\nwork, the models were presented in its inverse way, i.e.,pr (x |y). However, since in Chapter 4 we\nmake use of direct translation models, we present here the IBM models in its corresponding direct way,\ni.e.,pr (y |x). In the direct version of IBM models, the translation of a source sentencex into a target\nsentencey, is carried out usingalignmentsbetween words, i.e. a target wordyi is aligned to the set\nof source word positionsai = {j1, . . . , j l}, if the target word is directly generated as translation of\nthe source word groupxj1 , . . . , x jl . This model requires the use of a hidden variable model sincethe\nalignments are typically never seen in training\npr (y |x) = pr (I |x)\nX\na1\n· · ·\nX\naI\npr (y, aI\n1 |x, I ) , (1.106)\nwhere ai is the alignment vector that indicates which source words are aligned with thei-th target word\nyi , i.e.\nai ⊆ {1, . . . , J } , (1.107)\nand wherepr (I |x) is a length distribution which is usually uniformly modelled, and consequently\nignored.\nJAF-DSIC-UPV 23\nChapter 1. Preliminaries\nSome constraints are usually added to the alignment setsaI\n1, due to practical restrictions. For\ninstance, thecoverage constraintrequires all the source words to be in at least one alignment set.\nThe complete probability model in Eq. (1.106),pr (y, aI\n1 |x), can be decomposed left to right as\nfollows\npr (y, aI\n1 |x, I ) =\nY\ni\npr (ai |x, ai−1\n1 , yi−1\n1 , I )pr (yi | x, ai\n1, yi−1\n1 , I ) , (1.108)\nwhere two probabilities are used:\n• The alignment probabilitypr (ai |x, ai−1\n1 , yi−1\n1 , I )\n• The translation dictionary probabilitypr (yi |x, ai\n1, yi−1\n1 , I )\nDifferent alignment models were proposed in [Brown et al., 1993b] (in its inverse form) based on\nthis idea, although only2 models where directly modelled constraining the probabilities in Eq. (1.108)\ndirectly. These two models constrain the alignment sets cardinality to1 or0, that is to say each target\nword can be aligned to either one word or no word at all. In order to simplify notation, we redeﬁne the\nalignment variables since each alignment is composed of oneword. Therefore, we say thatai = j if\nthe target wordyi is “aligned” to the source wordxj , wherej can be any source position ({1, . . . , J })\nor 0 indicating thatyi is not aligned to any word. In order to represent the “non-alignment” event, a\nN ULL word is introduced at the beginning ofx, i.e.x = x0x1 · · ·xJ . If a target wordyi is aligned to\nx0 (ai = 0 ), the so-called NULL word, then it is equivalent to say that this target wordyi is not aligned\nto any source word.\nIBM model 1\nThe ﬁrst of the IBM models, the so-called IBM model 1, is essentially deﬁned as a statistical bilingual\ndictionary.\nThe IBM model 1 [Brown et al., 1993b] makes the following assumptions\n• The alignment probability is uniform, i.e.\npr (ai |x, ai−1\n1 , yi−1\n1 , I ) := 1\nJ + 1 . (1.109)\n• The dictionary probability depends only on the aligned word, i.e.\npr (yi |x, ai\n1, yi−1\n1 , I ) := p( yi |xai ) , (1.110)\nwhere note that we have introduced the notationp to refer to parameters, and where the following\nnormalisation constraint must be veriﬁed\nX\nb\np(b | a) = 1 ∀a ∈ X . (1.111)\nTaking into account the assumptions in Eqs. (1.109), and (1.110), the model probability is given by\npr (y |x) :=\n„ 1\nJ + 1\n«I Y\ni\nJX\nj=0\np(yi |xj ) . (1.112)\nSince the model is a hidden variable model, the EM algorithm [Dempster et al., 1977b] is used to\nestimate the parameter set:Θ = {p(b |a) |b ∈ Y , a ∈ X }.\nThe aim of the IBM model 1 typically is to initialise the training of superior IBM models. Another\ninteresting property of the IBM model 1 is the concavity of its log-likelihood function, and therefore the\n24 JAF-DSIC-UPV\n1.3. Statistical Machine Translation\nuniqueness of a maximum value under non-degeneratedc initialisation. However, the IBM model 1 has\nbeen widely applied to different tasks of statistical MT, cross-lingual information retrieval and bilingual\nTC due to its simplicity and applicability of its parameter values.\nIn statistical MT, the IBM model 1 has traditionally been an important ingredient in applications\nsuch as the alignment of bilingual sentences [Moore, 2002],the alignment of syntactic tree frag-\nments [Ding et al., 2003], the segmentation of bilingual long sentences for improved word align-\nment [Nevado et al., 2003], the extraction of parallel sentences from comparable corpora [Munteanu\net al., 2004], the estimation of word-level conﬁdence measures [Uefﬁng and Ney, 2007] and serves as\ninspiration for lexicalised phrase scoring in phrase-based systems [Koehn, 2005, Koehn et al., 2003].\nFurthermore, it has also received attention to improve non-structural problems [Moore, 2004].\nIBM model 2\nThe IBM model 2 is an extension of the IBM model 1 where the alignment probability is not uniformly\nmodelled. Speciﬁcally, the IBM model 2 parametrises the alignment probability as follows\npr (ai |x, ai−1\n1 , yi−1\n1 , I ) := p( ai |i, I , J ) (1.113)\nwhere the following normalisation constraint must be veriﬁed\nX\nj\np(j |i, I , J ) = 1 (1.114)\nTaking into account the assumptions in Eqs. (1.110), and (1.113), the model probability is given by\npr (y |x) :=\nY\ni\nX\nj\np(j |i, I , J ) p( yi |xj ) . (1.115)\nSince the model is a hidden variable model, the EM algorithm is used to estimate the parameter\nset,{p(b | a), p(j |i, I , J )}. In order to train this model, ﬁrstly, some iterations of theIBM model 1 are\nperformed in order to obtain good dictionary estimates. Afterwards a retraining is performed using the\nEM update equations for the IBM model 2.\n1.3.2 Statistical phrase-based translation systems\nThe basis of the mainstream and better statistical machine translation models are based on the so-called\nphrase-based models. The idea of modelling the translationprocess using phrase dictionaries was ﬁrstly\nintroduced in the alignment template approach [Och and Ney,2004]. In this section we review several\nproposed phrase-based models.\nGenerative phrase-based models\nWe outline here an example of generative phrase-based modelthat will serve us to present the problems\nfaced by this approach, and to motivate the introduction of heuristically estimated phrase-based systems.\nWe follow the model presented in Zens et al. [2002].\nLet(x, y) be a pair of source-target sentences, we introduce the conventional conditional probabil-\nityp(y |x) for the translation model. Let assume thatx has been divided intoT phrases or segments;\nand so hasy. We further assume that each source phrase has been generated by just one target phrase.\nWe unhidde the hidden variableB which is a segmentation of the bilingual segmentation pair(x, y)\ncStarting point in which none of the initial parameter valuesis zero.\nJAF-DSIC-UPV 25\nChapter 1. Preliminaries\nintoT phrases(˜xT\n1 , ˜yT\n1 ). Note that, the source segments,˜x1, . . . , ˜xT , are not required to be in the\nsame source order, i.e.,x could be different from˜xT\n1 = ˜x1 · · ·˜xT although it must be a reordering of\nit. Finally, a generative model can be seen asa full exploration of all possible bilingual segmentation of\nx and y and all possible alignments between them,\npr (y |x) = P\nB pr (y, B |x) (1.116)\n= P\nB pr (B |x)pr (y |B, x) , (1.117)\nwhere pr (y |B, x) is modelled using a phrase-table\npr (y |B, x) :=\nTY\nt=1\np(˜yk | ˜xk ) , (1.118)\nwhereas the remaining probability in Eq. (1.117), usually ignored, i.e., uniformly modelled for all\npossible target phrase reordering.\nThe estimation of a phrase-based model as that presented above is a cumbersome problem that pos-\nsess not only computational efﬁciency challenges, but alsooverwhelming data requirements. One of the\nmain difﬁculties that phrase-based models have to cope withis the problem of the bilingual segmenta-\ntion and reordering. In the model proposed above, this segmentation is modelled by the hidden variable\nB, which leads us to a large combinatorial number of possible segmentations to explore. As can be\nguessed, these problems are further aggravated with the length of the source and target sentence. De-\nspite this obstacle, there have been several proposals for phrase-based models, from the joint probability\nmodel [Birch et al., 2006, Marcu and Wong, 2002], over the HMMphrase-based models [Andrés-Ferrer\nand Juan, 2007, Deng and Byrne, 2005] to the statistical GIATI model [Andrés-Ferrer et al., 2008].\nHowever, the most popular approach to the development of phrase-based systems has been the\nlog-linear combination of heuristically estimated phrase-based models [Koehn et al., 2003, Och and\nNey, 2004], since these systems offer better performance than those based on generative phrase-based\nmodels [DeNero et al., 2006].\nHeuristic phrase-based models\nThe heuristic estimation of phrase-based models is grounded on the Viterbi alignments computed as\na byproduct of word-based alignment models. The Viterbi alignment is deﬁned as the most probable\nalignment given the source and target sentences and an estimation of the model parametersθ ,\nˆa = arg max\na\npθ (a |x, y) , (1.119)\ncan also be rewritten\nˆa = arg max\na\npθ (x, a |y) , (1.120)\nor\nˆa = arg max\na\npθ (y, a | x) . (1.121)\nFor instance, the conventional alignments, those providedby IBM models, disallow the connection\nof a source word with more than one target word. This unrealistic limitation negates the common\nlinguistic phenomenon in which a word in one language is translated into more than one word in another\nlanguage. To circumvent this problem, alignments are not only computed from the source language to\nthe target language, but also from the target language to thesource language. Doing so, we can reﬂect\nthe fact that a single word is connected to more than one word.\n26 JAF-DSIC-UPV\n1.3. Statistical Machine Translation\nOnce the Viterbi alignments have been computed in both directions, there exist different heuristic\nalgorithms to combined them [Koehn et al., 2003, Och and Ney, 2003]. These algorithms range from\nthe intersection of both alignments in which we have high precision, but low recall alignments, to\nthe union in which we have low precision, but high recall. In between, there are algorithms like the\nreﬁned method [Och and Ney, 2003] and thegrow-diag-ﬁnal[Koehn et al., 2003] that starting from the\nintersection, heuristically add additional alignment points taken from the union. This is a previous step,\nbefore extracting bilingual phrases, to construct a phrase-based system.\nBilingual phrase extraction is based on the concept ofconsistencyof a bilingual phrase(x, y)\n(derived from a bilingual segmentation) with a word alignmenta. Formally,\n(x, y) consistent witha ⇔ ∀ xj ∈ x : ( xj , y i) ∈ a − →yi ∈ y ∧\n∀yi ∈ y : ( xj , y i) ∈ a − →xj ∈ x ∧\n∃xj ∈ x, y i ∈ y : ( xj , y i) ∈ a (1.122)\nbasically Eq. (1.122) means that a bilingual phrase is consistent if and only if all the words in the source\nphrase are aligned to words in the target phrase, and there isat least one word in the source phrase\naligned to a word in the target phrase.\nGiven the deﬁnition of consistency, all bilingual phrases (up to a maximum phrase length) that are\nconsistent with the alignment resulting from the symmetrisation process are extracted.\nThe next step is to deﬁne functions that assign a score or a probability to a bilingual phrase in\nisolation or as part of a sequence of bilingual phrases in a given segmentation. These score functions\nare integrated in a log-linear fashion under the maximum entropy framework.\nThe most commonly used score functions are the direct and inverse phrase translation probability\nestimated as a relative frequency\npd(u |v) = count(u, v)P\nu′\ncount(u′, v) pi(v |u) = count(u, v)P\nv ′\ncount(u, v′) (1.123)\nwhere u stands for a source phrase, andv for a target phrase. A direct and inverse lexical translation\nprobability inspired in the IBM model 1 [Cohn and Lapata, 2007, Koehn et al., 2003] are also used in\nthe log-linear model. Other score functions are related to reordering capabilities, such as the distance-\nbased reordering model [Och and Ney, 2004] and the lexicalised reordering model [Koehn et al., 2005].\nAdditional score functions are the phrase and the word penalty to control the length of the translated\nsentence.\nThe weight of each score function in the log-linear combination is adjusted on a development set\nwith respect to a predeﬁned criterion, usually BLEU. There are two popular techniques in statistical MT\nto carry out this process, minimum error rate training [Och,2003] and minimum Bayes risk [Kumar and\nByrne, 2004]. Furthermore, the most common approach to the decoding process in log-linear models is\nthe well-known multi-stack decoding algorithm [Koehn, 2004, Och and Ney, 2004, Ortiz et al., 2006].\nThe Moses toolkit [Koehn et al., 2007], that implements an instantiation of this type of multi-stack\ndecoding algorithms, will be used throughout this thesis todeﬁne a baseline reference.\n1.3.3 Automatic MT evaluation metrics\nIn MT, the use of automatic evaluation metrics is imperativedue to the high cost of human made\nevaluations. Also the need of rapid assessment of the translation quality of an MT system during its\ndevelopment and tuning phases is another reason for the usage of automatic metrics. These metrics are\ndThis process is also known as symmetrisation.\nJAF-DSIC-UPV 27\nChapter 1. Preliminaries\nused under the assumption that they correlate well with human judgements of translation quality. This\narguable statement must be considered bearing in mind the low inter-annotator agreement on translation\nquality [Callison-Burch et al., 2007]. This fact makes automatic evaluation an open challenge in MT.\nIn this thesis, we mainly use two conventional translation evaluation metrics, WER and BLEU, al-\nthough other measures like METEOR [Banerjee and Lavie, 2005] and translation edit rate (TER) [Snover\net al., 2006] are becoming more and more popular.\nThe WER metric [Amengual et al., 2000a, Casacuberta et al., 2004] is deﬁned as the minimum\nnumber of word substitution, deletion and insertion operations required to convert the target sentence\nprovided by the translation system into the reference translation, divided by the number of words of the\nreference translation. It can also be seen as the ratio of theedit distance between the system and the\nreference translation, and the number of words of the reference translation. This metric will allow us to\ncompare our results to previous work on the same task. Even though the WER metric can value more\nthan100, it will be expressed as a percentage as it is commonly presented in the SMT literature. The\nWER metric can also be evaluated with respect to multiple references, however, in this thesis, we have\na single reference translation at our disposal.\nThe BLEU score [Papineni et al., 2001] is the geometric mean of the modiﬁede precision for dif-\nferent order ofn-grams (usually from unigram up to4-grams) between the target sentence and the\nreference translation, multiplied by an exponential brevity penalty (BP) factor that penalises those trans-\nlations that are shorter than the reference translation. Although some voices have been raised against\nBLEU as the dominant evaluation methodology over the past years [Callison-Burch et al., 2006], it\nis still a reference error measure for the evaluation of translation quality in MT systems. The BLEU\nranges from0.0 (worst case) to1.00 (best case), however, it is a common practice referred as a percent-\nage ranging from0.0 (worst score) to100.0 (best score).\neThe number of occurrences of a word in a target sentence is limited to that of this word in the reference transla-\ntion.\n28 JAF-DSIC-UPV\nBibliography\nBibliography\nH. Alshawi, S. Bangalore, and S. Douglas. Learning de-\npendency translation models as collections of ﬁnite\nstate head transducers.Computational Linguistics,\n26, 2000.\nJ. C. Amengual et al. The EuTrans-I speech translation\nsystem.Machine Translation, 15:75–103, 2000a.\nJ.C. Amengual, J.M. Benedí, F. Casacuberta, A. Cas-\ntaño, A. Castellanos, V . Jiménez, D. Llorens,\nA. Marzal, M. Pastor, F. Prat, E. Vidal, and J.M. Vilar.\nThe EuTrans-I speech translation system.Machine\nTranslation, 15:75–103, 2000b.\nJ. Andrés-Ferrer and A. Juan. A phrase-based hidden\nmarkov model approach to machine translation. In\nProceedings of New Approaches to Machine Trans-\nlation, pages 57–62, January 2007. ISBN 978-90-\n814861-0-1.\nJ. Andrés-Ferrer, A. Juan, and F. Casacuberta. Statistical\nestimation of rational transducers applied to machine\ntranslation.Applied Artiﬁcial Intelligence, 22(1-2):\n4–22, 2008.\nD.J. Arnold et al.Machine Translation: an Introductory\nGuide. Blackwells-NCC, London, 1993.\nL. R. Bahl, F. Jelinek, and R. L. Mercer. Some statistical-\nestimation methods for stochastic.IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2:179–\n190, March 1983.\nS. Banerjee and A. Lavie. METEOR: An automatic met-\nric for MT evaluation with improved correlation with\nhuman judgments. InProc. of the ACL Workshop on\nIntrinsic and Extrinsic Evaluation Measures for Ma-\nchine Translation and/or Summarization, pages 65–\n72, Ann Arbor, Michigan, USA, June 2005. Associa-\ntion for Computational Linguistics.\nS. Bangalore and G. Riccardi. A ﬁnite-state approach to\nmachine translation. InProc. of NAACL’01, pages 1–\n8, Morristown, NJ, USA, June 1995. Association for\nComputational Linguistics.\nJ.M. Benedí and J.A. Sánchez. Estimation of stochas-\ntic context-free grammars and their use as language\nmodels.Computer Speech and Language, 19(3):249–\n274, 2005.\nAdam L. Berger, Vincent J. Della Pietra, and Stephen\nA. Della Pietra. A maximum entropy approach to\nnatural language processing.Comput. Linguist., 22\n(1):39–71, 1996. ISSN 0891-2017.\nA.L. Berger et al. The candide system for machine trans-\nlation. InProc. of HLT’94, pages 157–162, Morris-\ntown, NJ, USA, 1994. Association for Computational\nLinguistics. ISBN 1-55860-357-3.\nA. Birch, C. Callison-Burch, Miles M. Osborne, and\nP. Koehn. Constraining the phrase-based, joint prob-\nability statistical translation model. InProceedings\non the Workshop on Statistical Machine Translation,\npages 154–157, New York City, New York, USA,\nJune 2006. Association for Computational Linguis-\ntics.\nP. F. Brown et al. The Mathematics of Statistical Ma-\nchine Translation: Parameter Estimation.Computa-\ntional Linguistics, 19(2):263–311, 1993a.\nPeter F. Brown, John Cocke, Stephen Della Pietra, Vin-\ncent J. Della Pietra, Frederick Jelinek, John D. Laf-\nferty, Robert L. Mercer, and Paul S. Rossin. A statis-\ntical approach to machine translation.Computational\nLinguistics, 16(2):79–85, 1990.\nPeter F. Brown, Stephen Della Pietra, Vincent J. Della\nPietra, and Robert L. Mercer. The mathematics of\nstatistical machine translation: Parameter estimation.\nComputational Linguistics, 19(2):263–312, 1993b.\nC. Callison-Burch, M. Osborne, and P. Koehn. Re-\nevaluating the role of bleu in machine translation re-\nsearch. InProc. of ACL’06, pages 249–256, Trento,\nItaly, April 2006. Association for Computational Lin-\nguistics.\nC. Callison-Burch et al. (meta-) evaluation of machine\ntranslation. InProceedings of the Second Workshop\non Statistical Machine Translation, pages 136–158,\nPrague, Czech Republic, June 2007. Association for\nComputational Linguistics.\nF. Casacuberta and E. Vidal. Machine translation with\ninferred stochastic ﬁnite-state transducers.Computa-\ntional Linguistics, 30(2):205–225, 2004.\nF. Casacuberta et al. Some approaches to statistical and\nﬁnite-state speech-to-speech translation.Computer\nSpeech and Language, 18:25–47, 2004.\nStanley Chen and Joshua Goodman. An empirical study\nof smoothing techniques for language modelling.\nTechnical Report TR-10-98, Harvard University,\n1998. See http://research.microsoft.com/ joshuago/tr-\n10-98.ps.\nD. Chiang. Hierarchical phrase-based translation.Com-\nputational Linguistics, 33(2):201–228, 2007. ISSN\n0891-2017.\nJAF-DSIC-UPV 29\nBibliography\nT. Cohn and M. Lapata. Machine translation by trian-\ngulation: Making effective use of multi-parallel cor-\npora. InProc. of ACL’07, pages 728–735, Prague,\nCzech Republic, June 2007. Association for Compu-\ntational Linguistics.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maxi-\nmum likelihood from incomplete data via the EM al-\ngorithm (with discussion).Journal of the Royal Sta-\ntistical Society B, 39:1–38, 1977a.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Max-\nimum likelihood from incomplete data via the EM\nalgorithm.J. Royal Statist. Soc. Ser. B, 39(1):1–22,\n1977b.\nJ. DeNero, D. Gillick, J. Zhang, and D. Klein. Why gen-\nerative phrase models underperform surface heuris-\ntics. InProceedings on the Workshop on Statistical\nMachine Translation, pages 31–38, New York City,\nJune 2006. Association for Computational Linguis-\ntics.\nY . Deng and W. Byrne. HMM word and phrase align-\nment for statistical machine translation. InProc.\nof HLT-EMNLP’05 , pages 169–176. Association for\nComputational Linguistics, October 2005.\nY . Ding and M. Palmer. Machine translation using prob-\nabilistic synchronous dependency insertion gram-\nmars. InProc. of ACL’05, pages 541–548, Morris-\ntown, NJ, USA, 2005. Association for Computational\nLinguistics.\nY . Ding, D. Gildea, and M. Palmer. An algorithm for\nword-level alignment of parallel dependency trees. In\nProc. of MT Summit IX, pages 95–101, September\n2003.\nRichard O. Duda, Peter E. Hart, and David G. Stork.Pat-\ntern Classiﬁcation. John Wiley and Sons, New York,\nNY , 2nd edition, 2001.\nI. García-Varea and F. Casacuberta. Search algorithms\nfor statistical machine translation based on dynamic\nprogramming and pruning techniques. InProc. of\nMT Summit VIII, pages 115–120, Santiago de Com-\npostela, Spain, 2001.\nU. Germann et al. Fast decoding and optimal decoding\nfor machine translation. InProc. of ACL’01, pages\n228–235, Morristown, NJ, USA, June 2001. Associ-\nation for Computational Linguistics.\nI. J. Good. Population frequencies of species and the\nestimation of population parameters.Biometrika, 40:\n237–264, 1953.\nJoshua Goodman. A bit of progress in language model-\ning.CoRR , cs.CL/0108005, 2001.\nJ. Graehl and K. Knight. Training tree transducers.\nIn Proc. of HLT-NAACL’04, pages 105–112, Morris-\ntown, NJ, USA, May 2004. Association for Compu-\ntational Linguistics.\nG. Heigold, R. Schlüter, and H. Ney. On the equivalence\nof gaussian hmm and gaussian hmm-like hidden con-\nditional random ﬁelds. InInterspeech, pages 1721–\n1724, Antwerp, Belgium, August 2007.\nJ. Hutchins and H. L. Somers.An introduction to ma-\nchine translation. Academic Press, 1992.\nA. Juan and Hermann Ney. Reversing and Smoothing the\nMultinomial Naive Bayes Text Classiﬁer. InProc. of\nPRIS 2002, pages 200–212, 2002.\nS. Kanthak and H. Ney. FSA: an efﬁcient and ﬂexible\nC++ toolkit for ﬁnite state automata using on-demand\ncomputation. InProc. of ACL’04, page 510, Morris-\ntown, NJ, USA, July 2004. Association for Computa-\ntional Linguistics.\nSlava M. Katz. Estimation of probabilities from sparse\ndata for the language model component of a speech\nrecognizer. In IEEE Transactions on Acoustics,\nSpeech and Signal Processing, ASSP-35:400–401,\n1987.\nR. Kneser and H. Ney. Improved backing-off form-\ngram language modeling.IEEE Int. Conf. on Acous-\ntics, Speech and Signal Processing, II:181–184, May\n1995.\nK. Knight. Decoding complexity in word-replacement\ntranslation models.Computional Linguistics, 25(4):\n607–615, 1999.\nP. Koehn. Pharaoh: A Beam Search Decoder for Phrase-\nBased Statistical Machine Translation Models. In\nProceedings of AMTA’04, pages 115–124, Washing-\nton, District of Columbia, USA, September-October\n2004.\nP. Koehn. Europarl: A parallel corpus for statistical ma-\nchine translation. InProc. of the MT Summit X, pages\n79–86, September 2005.\nP. Koehn, F.J. Och, and D. Marcu. Statistical phrase-\nbased translation. InProc. of NAACL’03, pages\n48–54, Morristown, NJ, USA, 2003. Association for\nComputational Linguistics. doi: http://dx.doi.org/10.\n3115/1073445.1073462.\nP. Koehn et al. Edinburgh system description for the\n2005 iwslt speech translation evaluation. InProc. of\nIWSLT’05, October 2005.\n30 JAF-DSIC-UPV\nBibliography\nP. Koehn et al. Moses: Open source toolkit for statisti-\ncal machine translation. InProc. of ACL’07: Demo\nand Poster Sessions, pages 177–180, Morristown, NJ,\nUSA, June 2007. Association for Computational Lin-\nguistics.\nS. Kumar and W. J. Byrne. Minimum bayes-risk de-\ncoding for statistical machine translation. InProc.\nof HLT-NAACL’04, pages 169–176, Morristown, NJ,\nUSA, May 2004. Association for Computational Lin-\nguistics.\nPhilippe Langlais, Simona Gandrabur, Thomas Lep-\nlus, and Guy Lapalme. The long-term forecast for\nweather bulletin translation.Machine Translation, 19\n(1):83–112, March 2005.\nDavid D. Lewis. Naive (Bayes) at forty: The indepen-\ndence assumption in information retrieval. In Claire\nNédellec and Céline Rouveirol, editors,Proceedings\nof ECML-98, 10th European Conference on Machine\nLearning, number 1398, pages 4–15, Chemnitz, DE,\n1998. Springer Verlag, Heidelberg, DE.\nD. Lin. A path-based transfer model for machine trans-\nlation. InProc. of COLING’04, page 625, Morris-\ntown, NJ, USA, August 2004. Association for Com-\nputational Linguistics.\nD. Marcu and W. Wong. A phrase-based, joint prob-\nability model for statistical machine translation. In\nProc. of EMNLP’02, pages 133–139, Morristown,\nNJ, USA, July 2002. Association for Computational\nLinguistics.\nJ. B. Mariño et al. N-gram-based machine transla-\ntion. Computational Linguistics, 32(4):527–549,\n2006. ISSN 0891-2017.\nR.C. Moore. Fast and accurate sentence alignment of\nbilingual corpora. InProc. of AMTA’02, pages 135–\n244, October 2002.\nR.C. Moore. Improving IBM Word-Alignment Model 1.\nInProc. of ACL’04, pages 519–524, July 2004.\nD.S. Munteanu, A. Fraser, and D. Marcu. Improved ma-\nchine translation performance via parallel sentence\nextraction from comparable corpora. InProc. of HLT-\nNAACL’04 , pages 265–272, Morristown, NJ, USA,\nMay 2004. Association for Computational Linguis-\ntics.\nKevin P. Murhpy. Hidden semi-Markov Models\n(HSMMs). Technical report, University of British\nColumbia, 2007. URL http://www.cs.ubc.\nca/~murphyk/mypapers.html.\nA. Nadas. On turing’s formula for word probabilities.\nIEEE Trans. Acoustics, Speech, and Signal Proc., 33:\n1,414–1,416, 1984.\nRadford M. Neal and Geoffrey E. Hinton. A view of\nthe em algorithm that justiﬁes incremental, sparse,\nand other variants. InLearning in Graphical Models,\npages 355–368. Kluwer Academic Publishers, 1998.\nF. Nevado, F. Casacuberta, and E. Vidal. Parallel cor-\npora segmentation using anchor words. InProc. of\nEAMT/CLAW’03 , pages 33–40, May 2003.\nH. Ney, S. Martin, and F. Wessel. Statistical language\nmodeling using leaving-one-out. In S. Young and\nG. Bloothooft, editors,Corpus-Based Statiscal Meth-\nods in Speech and Language Processing., pages 174–\n207. Kluwer Academic Publishers, 1997.\nS. Nirenburg et al.Machine Translation: A Knowledge-\nbased Approach. Morgan Kaufmann, 1992.\nE. H. Nyberg and T. Mitamura. The kant system:\nfast, accurate, high-quality translation in practical do-\nmains. InProc. of CL’92, pages 1069–1073, Morris-\ntown, NJ, USA, August 1992. Association for Com-\nputational Linguistics.\nF. J. Och. Minimum error rate training in statistical ma-\nchine translation. InProc. of ACL’03, pages 160–\n167, Morristown, NJ, USA, July 2003. Association\nfor Computational Linguistics.\nF. J. Och and H. Ney. A systematic comparison of vari-\nous statistical alignment models.Computational Lin-\nguistics, 29(1):19–51, 2003. ISSN 0891-2017.\nF. J. Och and H. Ney. The alignment template approach\nto statistical machine translation.Computational Lin-\nguistics, 30(4):417–449, 2004. ISSN 0891-2017.\nD. Ortiz, I. García-Varea, F. Casacuberta, L. Rodríguez,\nand J. Tomás. Thot. New features to deal with larger\ncorpora and long sentences. InTC-STAR OpenLab on\nSpeech Translation Workshop, Trento (Italy), March\n2006.\nM. Ostendorf, V . Digalakis, and O. Kimball. From hmms\nto segment models: a uniﬁed view of stochastic mod-\neling for speech recognition.IEEE Transactions on\nAcoustics, Speech and Signal Processing, (4):360–\n378, 1996.\nK. Papineni, S. Roukos, T. Ward, and W. Zhu. BLEU: a\nMethod for Automatic Evaluation of Machine Trans-\nlation. Technical Report RC22176, Thomas J. Watson\nResearch Center, 2001.\nJAF-DSIC-UPV 31\nBibliography\nLawrence Rabiner. A tutorial on hmm and selected ap-\nplications in speech recognition.Proceedings of the\nIEEE , 77(2):257–286, February 1989.\nM. Snover et al. A study of translation edit rate with tar-\ngeted human annotation. InProc. of AMTA’06, pages\n223–231, Boston, Massachusetts, USA, August 2006.\nAssociation for Machine Translation in the Americas.\nA. Stolcke. SRILM – an extensible language model-\ning toolkit. InProc. of ICSLP’02, pages 901–904,\nSeptember 2002.\nCharles Sutton and Andrew McCallum.Introduction to\nStatistical Relational Learning. Lise Getoor and Ben\nTaskar, 2006. Chapter: An Introduction to Condi-\ntional Random Fields for Relational Learning.\nC. Tillmann and H. Ney. Word reordering and a dy-\nnamic programming beam search algorithm for statis-\ntical machine translation.Computational Linguistics,\n29(1):97–133, March 2003.\nR. Udupa and H. K.r Maji. Computational complexity of\nstatistical machine translation. InProc. of EACL’06,\nApril 2006.\nN. Uefﬁng and H. Ney. Word-level conﬁdence estima-\ntion for machine translation.Computational Linguis-\ntics, 33(1):9–40, 2007.\nDavid Vilar, Hermann Ney, A. Juan, and Enrique Vidal.\nEffect of Feature Smoothing Methods in Text Classi-\nﬁcation Tasks. InProc. of PRIS 2004, pages 108–117,\n2004.\nY . Wang and A. Waibel. Fast decoding for statistical ma-\nchine translation. InProc. of ICSLP’98, pages 2775–\n2778, October 1998.\nC. F. Jeff Wu. On the convergence properties of the EM\nalgorithm.The Annals of Statistics, 11(1):95–103,\n1983.\nD. Wu. A polynomial-time algorithm for statistical ma-\nchine translation. InProc. of ACL’96, pages 152–158,\nMorristown, NJ, USA, June 1996. Morgan Kaufmann\n/ Association for Computational Linguistics.\nK. Yamada and K. Knight. A syntax-based statistical\ntranslation model. InProc. of ACL’01, pages 523–\n530, Morristown, NJ, USA, July 2001. Association\nfor Computational Linguistics.\nRichard Zens, Franz Josef Och, and Hermann Ney.\nPhrase-based statistical machine translation. InKI\n’02: Proceedings of the 25th Annual German Con-\nference on AI, pages 18–32, London, UK, 2002.\nSpringer-Verlag. ISBN 3-540-44185-9.\n32 JAF-DSIC-UPV\nChapter 2\nConstrained-Domain Maximum Likelihood Estimation\n“ If you are out to describe the truth, leave elegance to the tailor.” A. EINSTEIN\nContents\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n2.2 Naive Bayes model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n2.3 Conventional naive Bayes training . . . . . . . . . . . . . . . . . .. 35\n2.4 Constrained-domain maximum likelihood estimation . . .. . . . . . 37\n2.4.1 Characterisation of the solution . . . . . . . . . . . . . . . . .. 37\n2.4.2 The algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n2.4.3 Algorithm correctness and complexity . . . . . . . . . . . . .. 40\n2.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n33\nChapter 2. Constrained-Domain Maximum Likelihood Estimation\n2.1 Introduction\nMost pattern recognition systems are based on the optimal Bayes’ rule (see Section 1.1 and Section 4.2).\nThis rule highly depends on the posterior class probabilitypr (ω|x). Provided that the actual posterior\nprobability is not available in real tasks, it is approximated by a modelpθ (ω|x) which is characterised\nby a parameter set,θ ∈ Θ .\nThe selection of the optimal parameter setθ depends on the function criterion. As reviewed in\nSection 1.1.3 Chapter 1, maximum likelihood estimation (MLE) is one of the most widespread crite-\nria. This criterion ﬁnds the parameter setˆθ that maximises the likelihood function which is deﬁned in\nSection 1.1.3. One of the most important ﬂaws concerning to MLE is that it tends to overﬁt the param-\neters to the training data at the expense of reserving small probabilities or even zero probability to the\nremaining non-training data. This overﬁtting problem is often a straight consequence of the ratio of the\nnumber of parameters to the training size; roughly speaking, the data is scarce for what the model needs\nto learn.\nIn order to alleviate the overﬁtting problem, it is a common approach to distort the optimal param-\neter setˆθ obtaining a non-overﬁtted version of the optimal parameterset,˜θ . However, on the one hand,\nseveral smoothing techniques are heuristic techniques based on practical observation. For instance,\nsuch is the case of the interpolate smoothing in which the optimal vectorˆθ is usually interpolated with\na uniform distribution. On the other hand, some of the smoothing techniques are based on statistical\nmethods. The maximum a posteriori estimation or the leaving-one-out estimation are examples of such\nsmoothing methods.\nIn this chapter, we propose a method to avoid the scarce data derived problems such as overﬁtting.\nInstead of smoothing the optimal solution obtained by MLE, we introduce the idea of constraining the\nparametric domain,Θ , before searching for the optimal parameter set. Since, in this way, there is no\npossible overﬁtted parameter set in the domain; the optimalparameter set is smoothed in the parametric\noptimisation. Even more, the optimal parameter set obtained from the constrained domain retains the\nproperties of the MLE whilst the classical smoothed parameter set does not.\nWe apply the idea ofconstrained-domain maximum likelihood estimation(CDMLE) [Andrés-Ferrer\nand Juan, 2006, Andrés-Ferrer and Juan, 2009] to thenaive Bayestext classiﬁer [Juan and Ney, 2002,\nMcCallum and Nigam, 1998, Vilar et al., 2004]. Thenaive Bayesclassiﬁer [Andrés-Ferrer and Juan,\n2006, Andrés-Ferrer and Juan, 2009] has long been a core technique in information retrieval and, more\nrecently, it has attracted signiﬁcant interest in pattern recognition and machine learning [Lewis, 1998].\nGiven the document class and length, this classiﬁer makes the naive Bayesassumption that the prob-\nability of occurrence of a word does not depend on its position or other words in the document. In\nspite of being completely unrealistic, this assumption hasthe advantage of greatly simplifying clas-\nsiﬁer training. In particular, conventional, maximum likelihood estimation of class-conditional word\noccurrence probabilities reduces to a simple normalisation of word counts. However, due to data spare-\nness, these estimates suffer from overﬁtting; i.e. the estimated probabilities memorise the training data\nand are unable to explain unseen events. Overﬁtting is usually alleviated usingparameter smoothing,\nwhich is simply a heuristic modiﬁcation of maximum likelihood estimates to avoid null values [Vilar\net al., 2004]. Unfortunately, the resultingsmoothed parametersare no longeroptimalin terms of max-\nimum likelihood and thus we cannot attribute to them the desirable properties of maximum likelihood\nestimators.\nThe proposed algorithm is described in Section 2.4, after a brief review of the naive Bayes model\nand its conventional maximum likelihood estimation in the following two Sections. Empirical results\nand concluding remarks are given in Sections 2.5 and 2.6, respectively.\n34 JAF-DSIC-UPV\n2.2. Naive Bayes model\n2.2 Naive Bayes model\nWe denote the class variable byc = 1 , . . . , C in the remaining of this chapter; the word variable by\nd = 1 , . . . , D ; and a document of lengthL by wL\n1 = w1w2 · · ·wL . The joint probability of occurrence\nofc,L and wL\n1 may be written as\npr (c, L, wL\n1 ) := p( c) p( L) p θ (wL\n1 | c, L) , (2.1)\nwhere we have assumed that document length does not depend onthe class.\nGiven the classc and the document lengthL, the probability of occurrence of any particular docu-\nment wL\n1 can be greatly simpliﬁed by making the so-callednaive Bayesorindependence assumption:\nthe probability of occurrence of a wordwl inwL\n1 does not depend on its positionl or other wordswl′ ,\nl′ ̸= l,\npθ (wL\n1 | c, L ) = QL\ni=1 p(wi | c) . (2.2)\nUsing the above assumptions, we may write theposteriorprobability of a document belonging to a\nclassc as:\npπ ,θ (c | L, wL\n1 ) = pπ ,θ (c, L, wL\n1 )\nP\nc′ pπ ,θ (c′, L, wL\n1 ) (2.3)\n= πc\nQD\nd=1 θxd\ncd\nP\nc′ πc′\nQD\nd=1 θxd\nc′d\n(2.4)\n:= pπ ,θ (c | x) (2.5)\nwhere xd is the count of wordd inwL\n1 ,x = ( x1, . . . , x D )t , andΘ is the set of unknown parameters,\nwhich includesπc for the classc prior andθcd for the probability of occurrence of wordd in a document\nfrom classc. Clearly, these parameters must be non-negative and satisfy the normalisation constraints:\nP\nc πc = 1 (2.6)\nPD\nd=1 θcd = 1 ( c = 1 , . . . , C ) (2.7)\nThe Bayes’ decision rule associated with model (2.5) is a log-linear classiﬁer:\ncπ ,θ (x) = arg max\nc\npπ ,θ (c | x) (2.8)\n= arg max\nc\n(\nlog πc +\nX\nd\nxd log θcd\n)\n(2.9)\n2.3 Conventional naive Bayes training\nNaive Bayes training refers to the problem of deciding (a criterion and) a method to compute an appro-\npriate estimate for{π , θ } from a given collection ofN labelled training samples(x1, c 1), . . . , (xN , c N ).\nA conventional training criterion is thejointlog-likelihood function:\nLL(π , θ ) = P\nc Nc log πc + P\nd Ncd log θcd . (2.10)\nwhere Nc is the number of documents in classc and Ncd is the number of occurrences of wordd in\ntraining data from classc. It is well-known that the global maximum of this criterion under constraints\n(2.6)-(2.7) can be computed in closed-form:\nˆπc = Nc\nN (2.11)\nJAF-DSIC-UPV 35\nChapter 2. Constrained-Domain Maximum Likelihood Estimation\nand\nˆθcd = NcdP\nd′ Ncd′\n. (2.12)\nDespite the optimality of the estimates (2.12), they are usuallysmoothed (modiﬁed) to avoid null\nestimates originated by data spareness. For instance, in one of the experiments reported in Section 2.5,\nwe face the problem of estimating1.87M class-conditional word probabilities and only14.3% of them\nare non-zero according to (2.12). Thus, without smoothing,the sole occurrence of a rare word in a\ntest document is likely to introduce dominant and underestimated terms in the decision rule (2.8) and,\nhence, it may certainly be the cause of a classiﬁcation error.\nA popular smoothing method for (2.12) consists of simply adding a “pseudo-count”δ > 0 to every\nNcd count:\n˜θcd = Ncd + δP\nd′ (Ncd′ + δ) , (2.13)\nwithδ = 1 as the default value. This method is sometimes referred to asLaplace smoothing[McCallum\nand Nigam, 1998].\nAlternatively, as done in the context ofstatistical language modellingforspeech recognition, we\nmay use the idea ofabsolute discountingto avoid null estimates [Juan and Ney, 2002, Vilar et al.,\n2004]. Instead of using artiﬁcial pseudo-counts, we gain “free” probability mass by discounting a small\nconstant to every count associated with aseen event (positive count). The gained probability mass\nis then distributed among events in accordance with ageneralised distributionsuch as theuniform\ndistribution,\nβd = 1\nD , (2.14)\ntheunigram distribution,\nβd =\nP\nc NcdP\nd′\nP\nc Ncd′\n, (2.15)\nor whatever distribution providing a reliable estimation of class-independent word probabilities. De-\npending on the set of events that receives the gained probability mass, we distinguish betweenback-off\nand interpolation. Back-off only considers unseen events:\n˜θcd =\n8\n>\n>\n>\n>\n<\n>\n>\n>\n>\n:\nNcd − b\nP\nd′ Ncd′\nifNcd > 0\nMc\nβdP\nd′:Ncd′ =0 βd′\nifNcd = 0\n(2.16)\nwhere the probability mass gained in classc is:\nMc = b |{d′ ≥ 1 : Ncd′ > 0}|P\nd′≥1 Ncd′\n, (2.17)\nand the discountb is restricted to the interval(0, 1). In contrast, interpolation distributes the gained\nprobability mass among all events:\n˜θcd = max\n\n0, Ncd − bP\nd′ Ncd′\nﬀ\n+ Mc βd , (2.18)\nwhere 0 < b ≤ 1.\n36 JAF-DSIC-UPV\n2.4. Constrained-domain maximum likelihood estimation\n2.4 Constrained-domain maximum likelihood estimation\nAs it is said in the introduction, smoothed parameters are nolonger optimal in terms of maximum like-\nlihood and thus we cannot attribute to them the desirable properties of maximum likelihood estimators.\nIn this Chapter, we advocate the reduction of the set of feasible parameter estimates, that is, the use of\nadditional constraints on it. In particular, we focus our interest in conventional naive Bayes training,\nwithout smoothing, but constrained to class-conditional word probability estimates not smaller than a\npredeﬁned non-negative constantǫ. That is to say that we are interested in the maximisation of (2.10)\nsubject to constraints (2.6), (2.7) and\nθcd ≥ ǫ (c = 1 , . . . , C ; d = 1 , . . . , D ) (2.19)\nwhere ǫ is the minimum probability of occurrence of a word in a document from any class (0 ≤ ǫ ≤ 1\nD ).\nObviously, this is not a value we intend to learn from the data, but a meta-parameter to restrict the set\nof feasible estimates to “conservative” values. If we choose ǫ = 0 , we do not move from conventional\nnaive Bayes training. On the contrary, ifǫ = 1\nD , the only solution is to set all word probabilities toǫ.\nIn general, the more training data, the smallerǫ should be chosen.\n2.4.1 Characterisation of the solution\nMaximisation of (2.10) subject to constraints (2.6), (2.7)and (2.19) is a convex (concave maximisation)\nproblem with differentiable objective and constraint functions, for which we can ﬁnd a global maximum\nusing theKarush-Kuhn-Tucker(KKT) conditions (see Appendix A). The Lagrangian functionis\nL(π , θ , λ , µ ) = −LL(π , θ ) + Λ( π , θ , λ ) + Γ( θ , µ ) , (2.20)\nwhere Λ( π , θ , λ ) stands for Lagrangian part corresponding to the equality constraints, i.e.,\nΛ( π , θ , λ ) = λ0\n\"X\nc\nπc − 1\n#\n+\nX\nc\nλc\n\"X\nd\nθcd − 1\n#\n, (2.21)\nwhere λ0 and λc are Lagrange multipliers associated with constraints (2.6) and (2.7), respectively(c =\n1, . . . , C ). Conversely, theΓ( θ , µ ) function in Eq. (2.20) stands for the Lagrangian part corresponding\nto the inequality constraint, i.e.,\nΓ( θ , µ ) =\nX\nc,d\nµ cd (ǫ − θcd ) , (2.22)\nwhere µ cd are Lagrange multipliers associated with constraints (2.19),(c = 1 , . . . , C ; d = 1 , . . . , D ).\nThe KKT conditions for a pointˆπ , ˆθ , ˆλ , ˆµ to be a global maximum are\n∇π ,θ L(π , θ ,λ , µ )\n˛\n˛\nˆπ ,ˆθ , ˆλ , ˆµ = 0 (2.23)\nX\nc\nˆπc = 1 (2.24)\nX\nd\nˆθcd = 1 ( c = 1 , . . . , C ) (2.25)\nˆθcd ≥ ǫ (c = 1 , . . . , C ; d = 1 , . . . , D ) (2.26)\nˆµ cd (ǫ − ˆθcd ) = 0 ( c = 1 , . . . , C ; d = 1 , . . . , D ) (2.27)\nˆµ cd ≥ 0 ( c = 1 , . . . , C ; d = 1 , . . . , D ) (2.28)\nJAF-DSIC-UPV 37\nChapter 2. Constrained-Domain Maximum Likelihood Estimation\nFrom (2.23) and (2.24) immediately follows that, as in the conventional case, the optimal class priors\ncan be computed in closed-form using (2.11). However, this is not the case of the class-conditional\nword occurrence probabilities. From (2.23), we have\nˆθcd = 1\nˆλc + ˆµ cd\nNcd (c = 1 , . . . , C ; d = 1 , . . . , D ) (2.29)\nbut now we cannot rewriteˆλc + ˆµ cd in terms of word counts to arrive at a closed-form solution\nlike (2.12). Instead, by some straightforward manipulations, we arrive at the following characterisa-\ntion for each classc:\nˆθcd =\n(\nǫ ifϑcd ≤ ǫ\nϑcd ifϑcd > ǫ (d = 1 , . . . , D ) (2.30)\nwhere\nϑcd = NcdP\nd′:ϑcd′ >ǫ\nNcd′\n(1 − Mc ) ( d = 1 , . . . , D ) (2.31)\nwith\nMc = |{d′ : ϑcd′ ≤ ǫ}| ǫ (2.32)\nThe idea behind this characterisation is as follows. First note that we distinguish between “rare” words,\nin the sense that we assign a probability of exactlyǫ to them (d : ϑcd ≤ ǫ), and “frequent” words, which\nhave probability greater thanǫ (d : ϑcd > ǫ ). The probability mass allotted to rare words is simply their\nnumber timesǫ and is denoted byMc in (2.32). The remaining probability mass,1 −Mc , is distributed\namong frequent words in accordance with (2.31), which is simply a normalisation of word counts as\nin the conventional case (2.12). Thus, generally speaking,we proceed as in the conventional case, but\nusing only the probability mass not assigned to words that donot cross the threshold ofǫ.\n2.4.2 The algorithm\nThe above characterisation does not tell us how to partitionwords into rare and frequent, not even if\nsuch a partition exists. Nevertheless, it can be easily shown that a solution exists and can be found\niteratively for each class separately. Letc be the current class. The basic algorithm consists in ﬁrst\nassuming that the set of rare words is empty,R(0)\nc = ∅; then, in iterationk (k = 1 , 2, . . . ), the new set\nof rare words,R(k)\nc , is obtained fromR(k−1)\nc by addition of each wordd,\nR(k)\nc = R(k−1)\nc ∪ {d} (2.33)\nwhich is not inR(k−1)\nc but it is actually rare according to our criterion of not having a probability greater\nthanǫ,\nϑ(k−1)\ncd ≤ ǫ (2.34)\nwhere\nϑ(k−1)\ncd = Ncd\nP\nd′̸∈R(k−1)\nc\nNcd′\n(1 − M (k−1)\nc ) (2.35)\nwith\nM (k−1)\nc = |R(k−1)\nc |ǫ (2.36)\nAt the end of iterationk, the algorithm assures that condition (2.34) is satisﬁed for all words inR(k)\nc .\nThis condition may be also satisﬁed by words not inR(k)\nc though, in general, it will not be satisﬁed by\nmost of them.\n38 JAF-DSIC-UPV\n2.4. Constrained-domain maximum likelihood estimation\n1Algorithm:CDMLE\n2Input:\n3 C, D // number of classes and words\n4 (x1, c 1), . . . , (xN , c N ) //N labelled training samples\n5 ǫ : 0 ≤ ǫ ≤ 1\nD // minimum word occurrence probability\n6Output:\n7 {ˆθcd} // solution as characterised by Eqs.(2.30)-(2.32)\n8Variables:\n9 {Ncd} // word counts for each class\n10 R′, R // previous and current set of rare words\n11 S′, S // previous and current sum of non-rare word counts\n12 M ′, M // previous and current rare words probability mass\n13Method:\n14forc := 1 toC do // each classc is processed separately\n15 ford := 1 toD doNcd := 0 endfor\n16 forn := 1 toN do\n17 ifcn = c then\n18 ford := 1 toD doNcd := Ncd + xnd endfor\n19 endif\n20 endfor // word counts for classc computed\n21 R := ∅;S := 0 ;M := 0\n22 ford := 1 toD doS := S + Ncd endfor\n23 repeat // main loop for classc\n24 R′ := R;S′ := S;M ′ := M\n25 transf ers := false\n26 ford := 1 toD do ifd ̸∈R′ then\n27 ˆθcd := Ncd\nS′ ·(1 − M ′)\n28 ifˆθcd ≤ ǫ then\n29 ˆθcd:=ǫ //d has minimum probability inc\n30 R := R ∪ {d} //d is a new rare word\n31 S := S − Ncd\n32 M := M + ǫ\n33 transf ers := true\n34 endif endif\n35 endfor\n36 untilnot transf ers\n37endfor\nFigure 2.1:The Constrained-Domain Maximum Likelihood Estimation (CDMLE) al-\ngorithm.\nJAF-DSIC-UPV 39\nChapter 2. Constrained-Domain Maximum Likelihood Estimation\nAs R0\nc is empty,M (0)\nc is zero and the initial probability estimates,ϑ(0)\ncd , are exactly those obtained in\nthe conventional case (2.12). Therefore, in the ﬁrst iteration, we use conventional probability estimates\nto distinguish between rare and frequent words. Part of the probability mass assigned to frequent words\nis transferred to rare words for them to arrive atǫ. The remaining probability mass is redistributed\naccording to (2.35) and, as it is smaller than that distributed before the transference, it may well happen\nthat a frequent word become a new rare word. If it happens, a new iteration is carried out; otherwise,\nthe algorithm stops and returns the desiredˆθcd , as characterised by (2.30).\nA detailed description of the basic algorithm described above is given in Fig. 2.1. GivenC,D, the\ntraining data andǫ, it returnsˆθcd for allc and d, as characterised by Eqs.(2.30)-(2.32). The main loop\nprocesses each classc at a time (lines14–37). After computation of word counts (lines15–20), the\noptimal CDMLE solution is obtained iteratively (lines21–36). Initially, no words are considered rare\n(R := ∅) andˆθcd is computed for all words as in the conventional case (duringthe ﬁrst iteration of the\nloop in lines23–36). If a wordd is found such thatˆθcd ≤ ǫ (line28), thend is added toR and a new\niteration is executed; otherwise, no transfers toR are carried out and the algorithm stops.\n2.4.3 Algorithm correctness and complexity\nLetc be the current class and letd be a non-rare word in iterationk − 1 (d ̸∈R(k−1)\nc ) for which (2.34)\nholds. Then, it follows that\n1 − ǫ\n1 − M (k−1)\nc\n≤ 1 − NcdP\nd′̸∈R(k−1)\nc\nNcd′\n(2.37)\nand, rearranging terms,\n1 − M (k−1)\nc − ǫP\nd′̸∈R(k−1)\nc\nNcd′ − Ncd\n≤ 1 − M (k−1)\ncP\nd′̸∈R(k−1)\nc\nNcd′\n(2.38)\nAs d ̸∈R(k−1)\nc but satisﬁes Eq. (2.34), the algorithm addsd to the set of rare words in iterationk,\nR(k)\nc = R(k−1)\nc ∪ {d}. Using this updated set of rare words, Eq. (2.38) can be rewritten as\n1 − M (k)\ncP\nd′̸∈R(k)\nc\nNcd′\n≤ 1 − M (k−1)\ncP\nd′̸∈R(k−1)\nc\nNcd′\n(2.39)\nfrom which we have, for any wordd′′ ∈ R(k)\nc ,\nϑ(k)\ncd′′ ≤ ϑ(k−1)\ncd′′ (2.40)\nby multiplying each side of Eq. (2.39) byNcd′′ . From Eq. (2.40) and the fact thatϑ(k−1)\ncd′′ ≤ ǫ for all\nd′′ ∈ R(k)\nc , it follows thatϑ(k)\ncd′′ ≤ ǫ for alld′′ ∈ R(k)\nc . This means that, in iterationk, wordd becomes\nrare while all rare words in the previous iteration remain rare. Algorithm correctness follows from this\nresult.\nThe time complexity of the CDMLE algorithm depends on the case. In the best case, no word\ntransfers are done in the repeat-until loop and the algorithm works exactly as the conventional naive\nBayes training (without parameter smoothing). More precisely, after the ﬁrst repeat-until iteration, a\nsecond iteration is needed for the algorithm to check that notransfers to the set of rare words are carried\nout. Then, in the best case, its time complexity isΩ( CN D ). On the other hand, the repeat-until loop\nis executedD times in the worst case, and thus the algorithm hasO(CN D + CD 2) time complexity.\n40 JAF-DSIC-UPV\n2.5. Experiments\nHowever, in practice, the repeat-until loop is expected to iterate only a few times. Therefore, the com-\nputational behaviour of the CDMLE algorithm is expected to not differ signiﬁcantly from conventional\nnaive Bayes training.\nThe previous discussion about the complexity of the CDMLE algorithm only applies to a direct\nimplementation of it, such as that given in Fig. 2.1. However, it is straightforward to derive a reﬁned\nimplementation ofO(CN D + CD log D) time complexity. The idea behind this reﬁnement is to ap-\nply Eq. (2.33) in non-decreasing order of occurrence probability, as estimated in the conventional case.\nThat is, in iterationk, the next wordd to be considered in Eq. (2.33) must have minimum occurrence\nprobability, as given in Eq. (2.12), among all non-rare words. It can be easily checked that, if condi-\ntion (2.34) does not hold ford, then it will not hold for any other non-rare word and, therefore, the\noptimal CDMLE solution will have been found.\n2.5 Experiments\nThe proposed approach was empirically compared to the usualpractice of simply smoothing relative\ncounts, as described in Section 2.3. This comparison was carried on four text classiﬁcation data sets\n(tasks):Traveller,20 Newsgroups,Industry Sectorand Job Category.\nThe Travellerdata set comes from alimited-domainSpanish-English machine translation applica-\ntion for human-to-human communication situations in the front-desk of a hotel. It was semi-automatically\nbuilt from a small “seed” data set of sentence pairs collected from traveller-oriented booklets by four\npersons; A, F, J and P, each of whom had to cater for a (non-disjoint) subset of subdomains. The20\nNewsgroupscorpus is a collection of approximately20, 000 newsgroup documents, partitioned (nearly)\nevenly across 20 different newsgroups. We used the originalversion of this data set as provided by [Ren-\nnie, 2001], in which document headers are discarded but the \"From:\" and \"Subject:\" header ﬁelds are\nretained. TheIndustry Sectoris a collection of web pages from different companies, divided into a\nhierarchy of classes. In our experiments, however, we \"ﬂattened\" this structure, assigning each docu-\nment a class consisting of the whole path to the document in the hierarchy tree. TheJob Categorydata\nset consist of job titles and descriptions, also organised in a hierarchy of classes. This corpus contains\nlabelled and unlabelled samples and only the former were used in our experiments. Table 2.1 contains\na summary with the basic information on these data sets. For further details on them, see [McCallum,\n2002, Rennie, 2001, Vidal et al., 2000, Vilar et al., 2004].\nTable 2.1:Basic information on the data sets used in the experiments. (Singletonsare\nwords that occur once;Class n-tonsrefers to words that occur inn classes exactly.)\nJob Industry 20 Traveller\nCategory Sector Newsgroups (English)\nType of documents job titles web pages newsgroups sentences\nNumber of documents 131 643 9 629 19 974 8 000\nRunning words 11 221 K 1 834 K 2 549 K 79K\nAvg. document length 85 191 128 10\nV ocabulary size 84 212 64 551 102 752 391\nSingletons (V ocab %) 34.9 41 .4 36 .0 4\nClasses 65 105 20 23 .0\nClass 1-tons (V ocab %)49.2 58 .7 61 .1 74 .9\nClass 2-tons (V ocab %)14.0 11 .6 12 .9 18 .3\nJAF-DSIC-UPV 41\nChapter 2. Constrained-Domain Maximum Likelihood Estimation\nThe rainbowtoolkit [McCallum, 1998] was used for the preprocessing of all data sets butTraveller.\nWe used html skip for web pages and elimination of UU-encodedsegments for newsgroup messages.\nWe did not use stop-list removal, stemming or vocabulary pruning by occurrence count.\nFigure 2.2 shows the results obtained in each data set. The proposed CDMLE algorithm is compared\nto:\n1. Laplace:conventional training and Laplace smoothing,\n2. AD+1gBO: conventional training and absolute discounting with unigram back-off, and\n3. AD+1gI: as (2) with unigram interpolation.\nEach classiﬁcation technique considered has its own test-set error rate curve as a function of the discount\nb:\n1. Laplace:b refers toδ in Eq. (2.13),\n2. AD+1gBO or 1gI: b has its usual meaning, as deﬁned in Eq. (2.16), and\n3. CDMLE: ǫ is deﬁned fromb asǫ = 10 −10 b ·1\nD in the Traveller data set andǫ = b ·1\nD in the\nother data sets.\nEach plotted point corresponds to an average error rate obtained from30 random splits in which80%\ndocuments were used for training while the remaining20% were held out for testing. Error rate esti-\nmates have an approximate95% conﬁdence interval of[E% ± 1%] ([E% ± 0.4%] for Job Category).\nTable 2.2:Summary of the best results.\nJob Industry 20 Traveller\nCategory Sector News (English)\nLaplace 33.2 38 .9 15 .0 3 .3\nAD+1gBO 34.0 38 .0 14 .9 3 .3\nAD+1gI 34.2 37.8 14 .8 3.3\nCDMLE 33.0 38.6 15 .3 3.1\nFrom the results in Fig. 2.2, it is clear that the CDMLE algorithm performs similarly to the other\ntechniques. In comparison with Laplace, CDMLE provides slightly better results and more stable (ﬂat)\nerror curves in all data sets but 20 Newsgroups. In these datasets, it is indeed much better than Laplace\nwhen, as usual with Laplace, the discount factor is simply set to one. In the case of 20 Newsgroups,\nhowever, Laplace seems to be a bit better than CDMLE.\nIn comparison with absolute discounting (AD+1gBO and AD+1gI), it can be said that there is\nno superiority of one over the other. In Traveller and Job category, the CDMLE algorithm provides\nbetter rates than absolute discounting, but the contrary can be observed in the other two data sets. All\nin all, this is a comparatively good result for CDMLE since, in contrast to absolute discounting with\nunigram back-off/interpolation, CDMLE does not take advantage of the unigram distribution (2.15)\nto obtain reliable class-independent word probability estimates. Clearly, this estimates can be used to\nreplace (2.19) by better, word-dependent domain constraints.\nA summary of the best results obtained in the experiments is given in Table 2.2. The CDMLE\nalgorithm obtains better results than Laplace and absolutediscounting in Job Category and Traveller.\nHowever, absolute discounting is better than Laplace and the CDMLE algorithm in Industry Sector and\n20 Newsgroups. Note that these differences are signiﬁcant only to a limited extent.\nAs said in Section 2.4.3, the time complexity of the CDMLE algorithm isΩ( CN D ) in the best\ncase andO(CN D + CD 2) in the worst case. More precisely, the difference between these two cases\narises from the number of repeat-until iterations executed(lines23–36 in Fig. 2.1), which may vary\n42 JAF-DSIC-UPV\n2.6. Conclusions\nfrom 2 toD. To study this in the average case, the number of repeat-until iterations was recorded in\neach CDMLE algorithm execution. On average, it was exactly2 in the Traveller and 20 Newsgroups\ndata sets, that is, as in the best case. On the other hand, it was only of2.5 iterations for Industry Sectors\nand 3.2 for Job category. Therefore, as expected, the repeat-untilloop iterates only a few times. That\nis, in practice, the computational behaviour of the CDMLE algorithm might be considered almost the\nsame as that of conventional naive Bayes training.\n2.6 Conclusions\nIn this chapter, conventional naive Bayes training with parameter smoothing has been restated as a\nconstrained-domain maximum likelihood estimation problem for which an optimal, iterative algorithm\nhas been proposed. The general idea behind our contributionis to avoid parameter estimates that can\ncause over-ﬁtting while retaining the properties of maximum likelihood estimators. Empirical results\non four real text classiﬁcation tasks have shown that the proposed algorithm provides results similar to\nthose of conventional training and parameter smoothing, with almost the same practical computational\nrequirements.\nIt is worth noting, however, that smoothing methods have been continuously improved over the\nyears, while our proposal is completely new and thus, there is still room for signiﬁcant improvements.\nFor instance, the parameter domain might be better adjustedby redeﬁning the constantǫ introduced in\nEq. (2.19) and making it dependent on both the classc and the wordd.\nWe think that the proposed approach is very promising. In general, the idea behind of the proposed\napproach can be applied to many maximum likelihood estimation problems in pattern recognition. For\ninstance, it can be easily applied to EM-based maximum likelihood estimation of ﬁnite mixture models.\nFor these models, it is unclear how to use parameter smoothing in the M step without affecting the\nEM behaviour. Instead, constrained-domain maximum likelihood estimation can be used without any\nside effect. Also, this constrained approach might be useful in the case of training criterion other than\nmaximum likelihood such as discriminative training [Juan et al., 2007].\nThe naive Bayes model follows a Multinomial distribution [Juan and Ney, 2002]; and, hence, the\nproposed algorithm can be applied for Multinomial estimation. Finally, since the naive Bayes model\nis also a special case to then-gram language models, this technique can be extended to higher order\nn-grams. Speciﬁcally, this idea is covered in following chapter.\nJAF-DSIC-UPV 43\n  3\n  4\n  5\n 0  0.2  0.4  0.6  0.8  1\nCDMLE\nAD+1gI\nLaplace\nAD+1gBO\nb\nE%\nTraveller\n 15\n 16\n 17\n 18\n 0  0.2  0.4  0.6  0.8  1\nCDMLE\nAD+1gI\nLaplace\nAD+1gBO\nb\nE%\n20 Newsgroups\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 0  0.2  0.4  0.6  0.8  1\nCDMLE\nAD+1gI\nLaplace\nAD+1gBO\nb\nE%\nIndustry sector\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 0  0.2  0.4  0.6  0.8  1\nCDMLE\nAD+1gI\nLaplace\nAD+1gBO\nb\nE%\nJob category\nFigure 2.2:Results obtained in theTraveller, 20 Newsgroups, Industry sectorand\nJob categorydata sets. Each plot shows the classiﬁcation error rate as a function of\nthe discount parameterb, for the four classiﬁcation techniques considered (Laplace,\nAD+1gBO ,AD+1gI and CDMLE ).\nBibliography\nBibliography\nJ. Andrés-Ferrer and A. Juan. Máxima verosimilitud con\ndominio restringido aplicada a clasiﬁcación de docu-\nmentos. InActas de Campus Multidisciplinar en Per-\ncepcion e Inteligencia, CMPI-2006, volume 2, pages\n791–803, 2006. ISBN ISBN 84-689-9560-6 (Obra\ncompleta). ISBN 84-689-9562-2 (V olumen II).\nJesús Andrés-Ferrer and A. Juan. Constrained domain\nmaximum likelihood estimation for naive bayes text\nclassiﬁcation.Pattern Analysis and Applications\n(PAA), Published online 2009. doi: 10.1007/s10044-\n009-0149-y.\nA. Juan and Hermann Ney. Reversing and Smoothing the\nMultinomial Naive Bayes Text Classiﬁer. InProc. of\nPRIS 2002, pages 200–212, 2002.\nA. Juan, D. Vilar, and H. Ney. Bridging the gap between\nNaive Bayes and Maximum Entropy Text Classiﬁ-\ncation. InProc. of PRIS’07, pages 59–65, Funchal,\nMadeira - Portugal, June 2007.\nD. D. Lewis. Naive Bayes at Forty: The Independence\nAssumption in Information Retrieval. InProc. of\nECML’98 , pages 4–15, April 1998.\nA. McCallum. Industry Sector data set, 2002.www.cs.\numass.edu/~mccallum/code-data.html.\nA. McCallum. Bow: A toolkit for statistical language\nmodeling, text retrieval, classiﬁcation and clustering,\n1998. www.cs.umass.edu/~mccallum/bow/\nrainbow.\nA. McCallum and K. Nigam. A Comparison of Event\nModels for Naive Bayes Text Classiﬁcation. InProc.\nof AAAI/ICML-98: Workshop on Learning for Text\nCategorization, pages 41–48. Morgan Kaufmann,\nJuly 1998.\nJ. Rennie. Original 20 Newsgroups data set,\n2001. people.csail.mit.edu/jrennie/\n20Newsgroups.\nE. Vidal et al. Example-Based Understanding and Trans-\nlation Systems (EuTrans). Final Report, ESPRIT\nproject 20268, ITI, 2000.\nDavid Vilar, Hermann Ney, A. Juan, and Enrique Vidal.\nEffect of Feature Smoothing Methods in Text Classi-\nﬁcation Tasks. InProc. of PRIS 2004, pages 108–117,\n2004.\nJAF-DSIC-UPV 45\nBibliography\n46 JAF-DSIC-UPV\nChapter 3\nConstrained leaving-one-out for language modelling\n“ In mathematics the art of asking questions is more valuable than solving problems”\nG EORG FERDINAND L UDWIG PHILIPP C ANTOR\nContents\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.2 Leaving-one-out for language modelling . . . . . . . . . . . . .. . . 50\n3.2.1 The smoothing distributionβ(w|¯h) . . . . . . . . . . . . . . . . 52\n3.2.2 The interpolated smoothing model . . . . . . . . . . . . . . . . 53\n3.3 Interval Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n3.4 Quasi-monotonic constraints . . . . . . . . . . . . . . . . . . . . . .57\n3.5 Monotonic Constraints with Upper Bounds . . . . . . . . . . . . .. 59\n3.5.1 Monotonic constraints . . . . . . . . . . . . . . . . . . . . . . . 61\n3.6 Exact extended Kneser-Ney smoothing . . . . . . . . . . . . . . . .. 62\n3.7 A word on time complexity . . . . . . . . . . . . . . . . . . . . . . . 63\n3.8 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n3.9 Conclusions and future work . . . . . . . . . . . . . . . . . . . . . . 73\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n47\nChapter 3. Constrained leaving-one-out for language modelling\n3.1 Introduction\nI\nN the previous chapter, we introduced the idea of “smoothing the parametric domain” instead of\nsmoothing the optimal set of parameters. In this way, the optimisation of the training criterion\nprovides an optimal smoothed solution, avoiding the additional training smoothing stage. For\nintroducing this idea we analysed as case of study the naive Bayes classiﬁer for which simple threshold\nconstraints were applied.\nIn this chapter we further explore the idea of constraining the parametric domain. Speciﬁcally, we\nstudy two aspects: the estimation criterion (see Section 1.1.2 Chapter 1) and the probabilistic model.\nOn the one hand, the starting point of this chapter is theleaving-one-out maximum likelihood criterion\n(LOO) instead of the (conventional) maximum likelihood criterion (see Section 1.2.3 Chapter 1 ). On\nthe other hand, we apply the estimation techniques to the most widespread language model, the so\ncalledn-gram model (see Section 1.2 Chapter 1).\nUnfortunately, due to the large number of free parameters with respect to the training data, the\nn-gram model needs to resort to smoothing techniques. For instance, for a trigram language model,\nthe events that occur only once or not at all in the training data typically represent a huge percentage\nof all events. The probabilities of these events are difﬁcult to estimate with conventional methods\nbecause they occur few times in the training data. These probabilities are usually referred to assmall\nprobabilities.\nAs discussed earlier in Section 1.2 Chapter 1, the best smoothing methods, modiﬁed and original\nKneser-Ney [Goodman, 2001, Ney et al., 1997], are based on the Turing-Good (TG) counts [Good,\n1953, Nadas, 1984]. The back-off parametrisations of thesediscounts gain a probability massB and\nre-distribute it among all the unseen events. The discounted probability mass is obtained subtracting to\nthe actual countr the Turing-Good countr⋆ for each seen event (see Eq. (1.92) in Chapter 1).\nAn outstanding property of the Turing-Good (TG) countsr⋆ is that their sparseness is inverse to\nthe (conventional) countsr. The smaller the countr is, the larger the count-of-countsnr is, and, then,\nthe more conﬁdent the estimation ofr⋆ is. On the one hand, there is the unseenn-grams,r = 0 ; for\nwhich the TG counts are well estimated sincen0 and n1 typically comprise a large amount of events.\nOn the other hand, there isR − 1, for whichnR−1 and nR are typically equal to1, leading to a poorly\nestimated TG counts, and consequently, smoothed probabilities.\nUnfortunately, the previous property of the leaving-one-out (LOO) smoothed probabilities is also\none of their most important weakness, since the LOO probabilities are noisy or badly estimated for\nlarger countsr. Typically, we want the LOO smoothed probabilities˜p(w |h) to be “close” to the relative\nfrequencies (the MLE estimates,ˆ p(w | h)); or, in other words, we want the TG counts to be close to\nthe (conventional) counts. At least, we want the LOO probabilities to retain the same monotonic order\nthat the (conventional) MLE verify; that is to say, if an-gram hw has occurred more timesa than other\nn-gram h′w′,N (w, h ) > N (w′, h ′), then the probability of the former˜p(w |h) should be larger than\nthe latter,˜p(w′ |h′). Note that the ML estimates fulﬁl this requirement whereas the LOO estimates do\nnot ensure it.\nThe ﬁrst proposed smoothing based on LOO, Turing-Good method, deﬁnes a smoothed language\nmodel as a function of unconstrained LOO probability estimates. This method is very noisy since for\nlarge values ofr, the TG countr⋆ is poorly estimated. On the other hand, the Kneser-Ney [Kneser\nand Ney, 1995] smoothing solved that problem approximatingall the probabilities with one parameter,\nas depicted in Eq. (1.95) in Section 1.2 Chapter 1. Thus, the Turing-Good method and the absolute\ndiscounting method represent two extremes, namely either no constraints at all or a heavily constrained\nmodel with only a single parameter. We focus, however, on ﬁnding a trade-off between the number of\naUnder the assumptionN (h′) = N (h); or alternatively comparing the joint probabilities˜p(w, h ) and ˜p(w′, h ′).\n48 JAF-DSIC-UPV\n3.1. Introduction\nfree parameters and suitable constraints in order to avoid noisy estimates and achieve optimum perfor-\nmance.\nBare in mind that because of the assumption made when computing the LOO estimates of the dis-\ncounting parametersλ R−1\n1 (see Eq. (1.83) in Section 1.2 Chapter 1); the probability estimates,˜p(w |h)\nin Eq. (1.88), obtained with the optimal discounting parametersˆλ\nR−1\n1 , do not necessary fulﬁl the con-\nditional normalisation constraints,\nX\nw\n˜p(w |h) = 1 , ∀h ∈ W n−1 , (3.1)\nalthough they verify a joint normalisation constraint\nX\nw\nX\nh\n˜p(w, h ) = 1 , . (3.2)\nAt this point is important to recall the equivalence betweenthe joint model,˜p(w, h ), and the condi-\ntional model,˜p(w |h), which is depicted in Section 1.2 Chapter 1, when some assumption is taken\n(see Eq. (1.83) in Section 1.2 Chapter 1).\nMoreover, due to the way in which we use the parametersλ R−1\n1 when deﬁning the smoothing\nmodel as depicted in Eq. (1.88), the discounted probabilityBh can be0 or even negative ifr < r ⋆ . In\nsuch cases, the heuristic approaches renormalise those parameters adding the negative probability mass\nplus1, i.e.−Bh + 1 , to the total probability amount; or deactivate the smoothing for thatn-gram, as it\nis done in the SRILM toolkit [Stolcke, 2002]. In this chapter, we also try to avoid such problems.\nIn summary, since the TG countsr⋆ are obtained from data, they suffer from similar sparsity prob-\nlems than that of the original countsr. In this chapter, we present some novel estimation algorithms to\navoid the sparsity problems for the LOO estimates while trying to retain an optimal trade-off between\nthe number of parameters and their sparsity. We tackle this problem by constraining the domain, like-\nwise to Chapter 2, in order to force the optimal value to fulﬁldesirable properties that a not overﬁtted\nsolution must satisfy. This idea was previously outlined in[Kneser and Ney, 1995], where monotonic\nand interval constrains were suggested but not applied. Speciﬁcally, in this chapter, we present4 meth-\nods that seek to optimise LOO while ensuring monotonicity:\n• Interval constraints [Andrés-Ferrer and Ney, 2009]:in Section 3.3, we highly constrain the\nLOO probability estimates ensuring monotonicity.\n• Quasi-monotonic constraints:this method computes the LOO estimates so that they are mono-\ntonic except for probability of the two most frequent probabilities.This exception allow us to ﬁnd\na simpler algorithm in Section 3.4 that paves the way for the following proposed algorithms in\nSection 3.5.\n• Monotonic constraints with upper bounds:this method computes the LOO estimates so that they\nare monotonic and never larger than the MLE estimates. In this way, in Section 3.5, we avoid\nnegative or zero discounted probability massBh.\n• Monotonic constraints:in subsection 3.5.1, we compute the LOO estimates so that they all are\nmonotonic by modifying the algorithm presented in Section 3.5.\nAdditionally to these4 methods, we present theextended and exact Kneser-Ney (eeKN)[Andrés-Ferrer\nand Ney, 2009] algorithm in Section 3.6 that computes an exact estimation of the Kenser-Ney (KN)\ndiscount and the modiﬁed Kneser-Ney (mKN) discount while deﬁning a parameter to ﬁx the number of\nfree probability estimates.\nIn particular, all the methods except for the exact extendedKneser-Ney (eeKN) enforce the mono-\ntonicity of (almost) all the probability estimates. Although the eeKN does not ensures the monotonicity,\nthis method provides a meta-parameter that is used to practically enforce this monotonicity.\nJAF-DSIC-UPV 49\nChapter 3. Constrained leaving-one-out for language modelling\nThe remaining of this chapter is organised as follows. In Section 3.2, we propose a more comfort-\nable formulation for the application of leaving-one-out estimates to smooth an-gram language model.\nIn Section 3.8 we analyse the proposed methods experimentally, drawing conclusions when possible;\nand ﬁnally, future work and concluding remarks are gatheredin the last Section.\n3.2 Leaving-one-out for language modelling\nRecall from Section 1.2 in Chapter 1 that some smoothings models forn-gram modelling are obtained\nby computing the smoothing parameters of the model in Eq. (1.93) by leaving-one-out (LOO) . The\nsolution to this model is reviewed in Section 1.2 and is depicted in Eq. (1.85).\nHowever, in order to apply the constraints, there is a more suitable parametrisation for the model de-\nﬁned in Eq. (1.93). There, we presented the smoothing model using the standarddiscountingparametri-\nsation in which a probability mass is subtracted to the conventional ML estimates. In the new parametri-\nsation, the whole discounted probability is regarded as a parameterpr . Recall that optimising the con-\nditional model under the assumption in Eq. (1.83) is equivalent to directly optimising the joint model,\nand, hence, we re-parametrise the joint model as follows,\n˜p(w, h ) :=\n8\n>\n<\n>\n:\nR\nN N (w, h ) = R\npr 0 < N (w, h ) = r < R\nn0 p0 β(w|¯h) N (w, h ) = 0\n(3.3)\nwhere pR−1\n0 orp is subject to thejointnormalisation constraint\nRX\nr=0\nnr pr = 1 . (3.4)\nNote that this model is equivalent to the joint model in Eq. (1.93). For instance, the discounted proba-\nbility for a given historyh is given by\nBh = n0(h) p0 , (3.5)\nwhereas the total amount of discounted probability mass independent of any history, is\nB =\nX\nh\nBh = n0 p0 . (3.6)\nNote that there is a direct conversion between the model parametrised as a function ofpR−1\n0 and as\na function ofλ R−1\n1 ; and vice-versa. This conversion is given by the following expression\npr = (1 − λr ) r\nN , r = 1 , . . . , R − 1 . (3.7)\nIn this new model parametrised withpR−1\n0 , the LOO log-likelihood function is given by\nF(pR−1\n0 ) =\nR−1X\nr=0\n(r + 1) nr+1 log p r + const( pR−1\n0 ) . (3.8)\nIn order to obtain the new optimal parameter setpR−1\n0 , the log-likelihood in Eq. (3.8) is maximised\nsubject to the normalisation constraint in Eq. (3.4). For doing so, we deﬁne the Lagrangian function\nF(pR−1\n0 , γ ) =\nR−1X\nr=0\n(r + 1) nr+1 log p r −γ\n RX\nr=0\nnr pr −1\n!\n, (3.9)\n50 JAF-DSIC-UPV\n3.2. Leaving-one-out for language modelling\nand take the partial derivative with respect topr and γ\n∂ F(pR−1\n0 , γ )\n∂ pr\n= (r + 1) nr+1\npr\n− γnr , r = 0 , 1, . . . , R − 1 (3.10)\n∂ F(pR−1\n0 , γ )\n∂γ =\nRX\nr=0\nnr pr −1 . (3.11)\nThe optimal set of parameters must verify that the former partial derivatives are equal to0, from where\nthe optimal solution is obtained\nˆ pr = 1\nN\n(r + 1) nr+1\nnr\n„\n1 − nR\nR\nN\n«\n, r = 0 , 1, . . . , R − 1 . (3.12)\nSince typicallynR R\nN ≪ 1, it can be approximated as\nˆ pr = 1\nN\n(r + 1) nr+1\nnr\n, r = 0 , 1, . . . , R − 1 . (3.13)\nNote that Turing-Good counts aregeneralisedunder this parametrisation as\nr⋆ = p r N , (3.14)\nfor joint models; and as\nr⋆(h) = p r N (h) , (3.15)\nfor conditional ones.\nThe result in Eq. (3.12) is totally equivalent to Eq. (1.85),since if we plug the optimal parameters\nˆλ\nR−1\n1 into the Eq. (3.7), then the same optimal parameterspR−1\n1 are obtained\nˆ pr = (1 − ˆλr ) r\nN r = 1 , . . . , R − 1 ,\nˆ pr =\n„\n1 − 1 + nr+1(r + 1)\nrnr\n„\n1 − nR R\nN\n«« r\nN , r = 1 , . . . , R − 1\nˆ pr = 1\nN\n(r + 1) nr+1\nnr\n„\n1 − nRR\nN\n«\n, r = 1 , . . . , R − 1 .\nNote thatn0 p0 is the probability mass reserved to the unseen events, i.e.B, and hence,p0 is the\nmost important probability when LOO is used for smoothing, since it gives probability for the unseen\nevents. However, it is important to highlight that we obtainthe probability mass to the unseen events at\nthe expense of reducing the probability mass of the seen events. The question tackled in this chapter is\nhow to discount the probability mass from the seen events so that the probabilities are still monotonic\nand optimal for the LOO criterion.\nThe discounting idea in the new parametrisation given in Eq.(3.3) is less obvious than in the\nparametrisation given in Eq. (1.93). However, since we havethat all the probabilities must sum up\nto1 (constraint in Eq. (3.1))\nB = n0 p0 (3.16)\nB = 1 −\nRX\nr=1\nnr pr\nB = 1\nN\n \nN −\nRX\nr=1\nnr pr N\n!\n,\nJAF-DSIC-UPV 51\nChapter 3. Constrained leaving-one-out for language modelling\nwhere taking into account the following property\nN =\nRX\nr=1\nrnr , (3.17)\nB is expressed as\nB = 1\nN\n RX\nr=1\nrnr −\nRX\nr=1\nnr pr N\n!\n, (3.18)\nand grouping common terms\nB = 1\nN\nRX\nr=1\nnr (r − pr N ) . (3.19)\nFinally, by using the deﬁnition of the TG counts in Eq. (3.14), the following discounting equation is\nobtained\nB = 1\nN\nRX\nr=1\nnr (r − r⋆ ) , (3.20)\nfrom where the discounting process is depicted asr − r⋆ . It is worth noting that a similar expression\ncan be obtained for the conditional normalisation constraint in case of using a conditional smoothing\nmodel\nBh = 1\nN (h)\nRX\nr=1\nnr (h) (r − r⋆ (h)) ∀h ∈ W n−1 . (3.21)\n3.2.1 The smoothing distributionβ (w|¯h)\nThe smoothing distributionβ(w|¯h) can also be estimated by LOO. The result of applying LOO to the\nestimation ofβ(w|¯h) yields the following result [Ney et al., 1997]:\nˆβ(w|¯h) = N1(w|¯h)\nN1(¯h) , (3.22)\nwhich resembles the (conventional) MLE(n −1)-gram distribution but deﬁned with the especial counts\n{N1 (w, ¯h)} instead of the conventional counts{N (w, ¯\nh)}. These especial counts are deﬁned as fol-\nlows\nN1(w|¯h) =\nX\nh∈¯h:N (w,h)=1\n1 , (3.23)\nwhere h ∈ ¯h stands for all then-gram contextsh, that share the same preﬁx(n − 1)-gram history,¯h;\ni.e., if¯h = wn−1\n2 , thenh ∈ ¯h comprises allh such thath = wwn−1\n2 for any wordw ∈ W . Finally,\ntheN1(¯h) is the sum over all words ofN1(w|¯h)\nN1(¯h) =\nX\nw\nN1(w|¯h) . (3.24)\nThe smoothing distribution in Eq. (3.22), captures the ideaof word coupling. For instance, “New\nYork” is a coupled bi-gram, that means that if “York” is a frequent word so would it be “New” and vice-\nversa. However, the smoothing distribution should give high probabilities to words that have rarely been\n52 JAF-DSIC-UPV\n3.3. Interval Constraints\nobserved in the context. Therefore, if instead of usingN1(w, ¯\nh) to estimate the smoothing distribution,\nwe useN (w, ¯h), the smoothing distribution are over-estimated.\nSeveral works reported [Chen and Goodman, 1998, Ney et al., 1997] that changing the singleton\ncountsN1(· · ·) by positive countsN+ (· · ·)\nN+(w|¯h) =\nX\nh∈¯h:N (w,h)≥1\n1 , (3.25)\nincur in better perplexities. Therefore, to counteract theword coupling effect, we recompute counts by\nthe number of different contexts in which the smoothedn-gram occurs. High orders of this smoothing\ndistribution are often recursively smoothed with LOO untilthe uni-gram distribution following the\nsmoothing scheme presented for the language model.\n3.2.2 The interpolated smoothing model\nAll the previous discussion is focused on the back-off model. This model redistributes the gained\nprobability massB among the unseen events. Oppositely, the interpolation model redistributes this\ngained probability mass among all the events, both seen and unseen. Therefore, our joint interpolation\nmodel is given by\n˜p(w, h ) :=\n8\n>\n<\n>\n:\nR\nN + iβ(w, ¯h) N (w, h ) = R\npr +iβ(w, ¯h) 0 < N (w, h ) = r < R\niβ(w, ¯h) N (w, h ) = 0\n(3.26)\nwith the discounted probabilitiespR−1\n1 and the interpolation factori comprising the parameter set. Note\nthat the smoothing probability distributionβ(w, ¯h) must sum up to1 for alln-grams, and not just the\nunseen ones, i.e., X\nh\nX\nw\nβ(w, h ) = 1 , (3.27)\nand, ﬁnally, the probabilities must sum1,\nRX\nr=1\nnr pr +i = 1 . (3.28)\nUsually this interpolation parameter is estimated in the backing-off model and afterwards assumed\nto be the same in the interpolation model. That is to say, it isoften assumed that\ni = n0 p0 . (3.29)\nIt is worth highlighting that this assumption is actually true if the optimal smoothing probabilityβ(w, ¯h)\nis known. Although, we focus on this chapter on the backing-off models, we take the previous assump-\ntion to compute interpolated smoothing models in the experimental Section 3.8.\n3.3 Interval Constraints\nThe goal of this method is to modify the original MLE probabilities,pr = r/N, only a little bit.\nTherefore, we introduce what we call the interval constraints\nr − 1\nN ≤ pr ≤ r\nN , r = 1 , . . . , R − 1 , (3.30)\nJAF-DSIC-UPV 53\nChapter 3. Constrained leaving-one-out for language modelling\nand\np0 ≤ 1\nN . (3.31)\nRecall that the upper bound in Eq. (3.30) is the MLE estimate.\nAs presented in the model in Eq. (3.3), the probabilitypR is not used in the LOO log-likelihood\nfunction and its value is ﬁxed to the MLE.\nTherefore, mathematically, we want to maximise the LOO log-likelihood obtained in Eq. (3.8)\nF (pR−1\n0 ) =\nR−1X\nr=0\n(r + 1) nr+1 log p r (3.32)\nconstrained to Eqs. (3.31), (3.30) and the normalisation constraint\nRX\nr=0\nnr pr = 1 , (3.33)\nand recalling thatpR is ﬁxed to the MLE, i.e.\npR = R\nN .\nThe idea of applying these constraints was previously outlined in [Kneser and Ney, 1995], where an\nheuristic and not optimal solution was proposed. In order toobtain an optimal solution to the problem,\nwe use the Karush-Kuhn-Tucker (KKT) conditions (see Appendix A). In this case, the Lagrangian\nfunction is instanced to\nL(pR−1\n0 , λ, µ , ν ) =\nR−1X\nr=0\n(r + 1) nr+1 log p r −Λ( pR−1\n0 , λ ) − Ψ( pR−1\n0 , µ , ν ) ,\nwhere Λ( pR−1\n0 , λ ) is the Lagrangian part for the normalisation constraint\nΛ( pR−1\n0 , λ ) = λ\n RX\nr=0\nnr pr −1\n!\n,\nand whereΨ( pR−1\n0 , µ , ν ) is the part generated by the inequality constraints\nΨ( pR−1\n0 , µ , ν ) =\nR−1X\nr=2\nµ r\n„r − 1\nN − pr\n«\n+\nR−1X\nr=1\nνr\n“\npr − r\nN\n”\n+ ν0\n„\np0 − 1\nN\n«\n.\n54 JAF-DSIC-UPV\n3.3. Interval Constraints\nIn this case the KKT conditions are reduced to\n(r + 1) nr+1\npr\n− nr λ + µ r − νr = 0 r = 1 , . . . , R − 1 (3.34)\nn1\np0\n− n0λ − ν0 = 0 (3.35)\nRX\nr=0\nnr pr = 1 (3.36)\nµ r\n„r − 1\nN − pr\n«\n= 0 r = 2 , . . . , R − 1 (3.37)\nνr\n“\npr − r\nN\n”\n= 0 r = 1 , . . . , R − 1 (3.38)\nν0\n„\np0 − 1\nN\n«\n= 0 (3.39)\nµ r ≥ 0 r = 2 , . . . , R − 1 (3.40)\nνr ≥ 0 r = 0 , . . . , R − 1 (3.41)\ntogether with constraints in Eqs. (3.30) and (3.31).\nIn the previous KKT conditions,µ r stands for the “strictness” of the lower bound for each proba-\nbilitypr , andνr is the corresponding “strictness” for the upper bound. Therefore, ifµ r is greater than\n0 then the lower constraint is (strictly) active, i.e. the bound is veriﬁed by equality andpr has the value\nof the lower bound. Oppositely, ifµ r is equal to0, then the lower bound is not (strictly) active andpr\ncan take values larger than it. This interpretation of the Lagrangian multipliers allows us to make a case\nanalysis depending on the multipliers for a given probabilitypr withr > 1:\n• “Lower bound is active”, thenµ r > 0 and νr = 0\n• “Upper bound is active”, thenµ r = 0 and νr > 0\n• “Unbound case”, thenµ r = 0 and νr = 0\nNote that we have omitted the caseµ r > 0 and νr > 0, since it implies that both constrains are active,\nwhich is impossible by deﬁnition.\n“Lower bound is active”\nIn this case, sinceνr = 0 we can work out the value ofµ r from Eq. (3.34)\nµ r = −\n„(r + 1) nr+1\npr\n− nr λ\n«\n. (3.42)\nAs result of Eq. (3.40) and (3.42) we obtain\npr ≥ 1\nλ\n(r + 1) nr+1\nnr\n, (3.43)\nand using the constraints in Eq.(3.30)\npr ≥ r − 1\nN , (3.44)\nwe obtain a solution by plugging previous Eqs.(3.43) and (3.44) into the constraint in Eq. (3.37)\npr (λ) = max\nr − 1\nN , 1\nλ\n(r + 1) nr+1\nnr\nﬀ\n, (3.45)\nwhich depends on a normalisation constantλ.\nJAF-DSIC-UPV 55\nChapter 3. Constrained leaving-one-out for language modelling\n“Upper bound is active”\nIn this case, sinceµ r = 0 we can again work out the value ofνr from Eq. (3.34)\nνr = (r + 1) nr+1\npr\n− nr λ . (3.46)\nAs result of Eq. (3.41) and (3.46) we obtain\npr ≤ 1\nλ\n(r + 1) nr+1\nnr\n, (3.47)\nand using the constraints in Eq.(3.30), speciﬁcally\npr ≤ r\nN , (3.48)\nwe obtain a solution by means of the constraint in Eq. (3.38)\npr (λ) = min\nr\nN , 1\nλ\n(r + 1) nr+1\nnr\nﬀ\n, (3.49)\nwhich depends on a normalisation constantλ.\n“Unbound case”\nIn this case sinceµ r = 0 and νr = 0 , the value ofpr is straightly worked out from Eq. (3.34)\npr (λ) = 1\nλ\n(r + 1) nr+1\nnr\n. (3.50)\nThe actual solution\nFinally, taking into account the result for each case depicted in Eqs. (3.45), (3.49),and (3.50), we obtain\na solution dependent on a normalisation constantλ\npr (λ) = max\nr − 1\nN , min\n1\nλ\n(r + 1) nr+1\nnr\n, r\nN\nﬀﬀ\n. (3.51)\nNote that this solution is valid forr = 2 , . . . , R − 1.\nAn analogous procedure can be carried out for the special cases r = 0 , 1 yielding the following\nresult\npr (λ) = min\n1\nλ\n(r + 1) nr+1\nnr\n, 1\nN\nﬀ\n.\nThe interpretation of the solution in Eq. (3.51) is as follows. We compute the unconstrained LOO\nestimatepr = 1\nλ\n(r+1)nr+1\nnr\n, with the unknown normalisation constantλ. This estimate is then com-\npared with the lower and upper bound and it is clipped if necessary. Now the only remaining problem\nis that this comparison requires the normalisation constant to be known. To this purpose we introduce\ntheλ dependingnormalisation function,\nQ(λ) =\nRX\nr=0\nnr pr (λ) . (3.52)\nThis way, the normalisation constraint is reformulated asQ(λ) = 1 . SinceQ(λ) is a monotonically\ndecreasing function, the value forλ can be easily computed.\n56 JAF-DSIC-UPV\n3.4. Quasi-monotonic constraints\nNote that in order to ensure monotonicity the following constraint must be added to the algorithm\np0 ≤ p1 . (3.53)\nThe addition of this constrain does not signiﬁcantly modifythe algorithm, though it becomes more\nawkward. The solution with the additional constraint in Eq.(3.53) can be obtained in the same way that\nin the proposed method but it becomes more cumbersome. Speciﬁcally, if we deﬁne\n¯ p01(λ) = 1\nλ\n2n2 + n1\nn1 + n0\n, (3.54)\nand\n¯ p0(λ) = 1\nλ\nn1\nn0\n, (3.55)\nand\n¯ p1(λ) = 1\nλ\n2n2\nn1\n, (3.56)\nthen the solution to the interval constraints with the additional constraint in Eq. (3.53) is given by\nEq. (3.51) for all ther values but for0 and 1 which are equal to\np0(λ) =\n8\n>\n<\n>\n:\n1\nN ¯ p01(λ) ≥ 1\nN and ¯ p0(λ) ≥ ¯ p1(λ)\n¯ p01(λ) ¯ p 01(λ) < 1\nN and ¯ p0(λ) ≥ ¯ p1(λ)\n¯ p0(λ) otherwise\n, (3.57)\nand\np1(λ) =\n8\n>\n<\n>\n:\n1\nN ¯ p01(λ) ≥ 1\nN and ¯ p0(λ) ≥ ¯ p1(λ)\n¯ p01(λ) ¯ p 01(λ) < 1\nN and ¯ p0(λ) ≥ ¯ p1(λ)\n¯ p1(λ) otherwise\n, (3.58)\nrespectively. Let it be as it were, the constraint in Eq. (3.53) is always veriﬁed in practice, and hence,\nthis constraint becomes useless.\n3.4 Quasi-monotonic constraints\nA natural requirement is that the probability estimates,pr , should be a monotonic function ofr. This\nis a more natural requirement than the interval constraints. The monotonic requirement is speciﬁed by\nthe following set of constraints\npr ≤ pr+1, r = 0 , 1, . . . , R − 2 . (3.59)\nSimilar constraints were previously proposed in [Kneser and Ney, 1995] but no algorithm or solution\nto computed them was given. Note that, we have intentionallyomitted the last monotonic constraint\npR−1 ≤ pR = R\nN . (3.60)\nObviously this makes the following discourse not to ensure monotonicity, and that is why we will\nrefer to this algorithm asquasi-monotonicalgorithm. This assumption has two main motivations. On\nthe one hand, dropping this last bound incur in a simpler algorithm that will allow us to introduce\nanother algorithm in the following Section 3.5. On the otherhand, the algorithm for obtaining the\ntotally monotonic solution is a special case of the algorithm proposed in the mentioned Section 3.5.\nFurthermore, this constraint is (almost) always veriﬁed inpractice by the current algorithm.\nJAF-DSIC-UPV 57\nChapter 3. Constrained leaving-one-out for language modelling\nThereby, for the remaining of this section, we wish to optimise the model in Eq. (3.3), constrained\nby the monotonic requirements in Eq. (3.59). By applying theKKT conditions (see Appendix A), only\na characterisation of the solution is obtained,\np0 = . . . = p r1\n| {z }\nq0\n< pr1+1 = . . . = p r2\n| {z }\nq1\n< . . . < prK−1+1 = . . . = p rK\n| {z }\nqK−1\n, q K = R\nN (3.61)\nSince eitherpr−1 < pr orpr−1 = p r must be veriﬁed; the solution is a structure ofK + 1 segments\nof probabilities with the set of boundariesrK+1\n0\n−1 := r0 < r 1 < . . . < r K := R − 1, r K+1 := R . (3.62)\nInside thek-th segment, all the probabilities share the very same probabilityqk . The number of seg-\nments range from2 toR. ForK = 1 , there are just two segments: one segment contains just one index,\nR, and the other segment contains the remaining indexes,r = 0 , . . . , R −1; which share the very same\nprobability,q0. ForK = R − 1, each segment is made up of one probability.\nIn order to simplify notation, we express the constraints interms of the segment probabilitiesqK\n0 :=\nq0, ..., q K . By deﬁning\nmk =\nrk+1X\nr=rk + 1\nnr , (3.63)\nthe normalisation constraint is rewritten as\nKX\nk=0\nmk qk = 1 , (3.64)\nand the monotonicity constraints are summarised as\nqk−1 < q k k = 1 , . . . , K − 1 . (3.65)\nRecall thatqK is as usual ﬁxed to the relative frequencies. i.e.\nqK = R\nN . (3.66)\nIn order to obtain the solution, we start by assuming that thesegmentationrK+1\n0 is given. In such\ncase, the LOO log-likelihood function is\nF (qK−1\n0 ) =\nK−1X\nk=0\nAk log qk , (3.67)\nwithAk deﬁned as\nAk =\nrk+1X\nr=rk + 1\n(r + 1) nr+1 , (3.68)\nsubject to the normalisation constraint in Eq. (3.64). The Lagrangian function for this optimisation\nproblem is\nL(qK−1\n0 , λ ) = F (qK−1\n0 ) − Λ( λ, qK−1\n0 ) , (3.69)\nwith\nΛ( λ, qK−1\n0 ) = λ\n KX\nk=0\nmk qk − 1\n!\n. (3.70)\n58 JAF-DSIC-UPV\n3.5. Monotonic Constraints with Upper Bounds\nConstraining the partial derivatives of the Lagrangian to be equal to0 yields the optimal solution for a\ngiven segmentation\nqk = 1\nλ\nAk\nmk\n(3.71)\nwhere the normalisation constantλ is independent of the segmentation\nλ = N\n1 − mK qK\n= N\n1 − nR pR\n(3.72)\nSince we have assumed the segmentation given, whether the solution veriﬁes the constraints in\nEq. (3.65) or not depends on if the segmentation is optimal ornot. Therefore, in order to obtain the\nboundaries of the segmentation, we should ﬁnd the boundaries rK−1\n0 that maximise the log-likelihood\nin Eq. (3.67) while satisfying the monotonic constraints.\nThis is efﬁciently solved by dynamic programming using the following recurrence\nF (r) = arg max\nr′<r : p r′ <pr\n{F (r′− 1) + A(r′, r ) log q(r′, r )} , (3.73)\nwith\nA(r′, r ) =\nrX\ns=r′\n(s + 1) ns+1 , (3.74)\nand with\nq(r′, r ) = 1\nλ\nA(r′, r )Pr\ns=r′ ns\n. (3.75)\nNote thatF (r) is the log-likelihood for the partial segmentation that ends at countr. As usual with\ndynamic programming, the optimal solution is obtained by tracing back the decisions made during the\nrecurrence in Eq. (3.73).\n3.5 Monotonic Constraints with Upper Bounds\nIn Section 3.4, we analysed the quasi-monotonic constraints. This constraints involve practical prob-\nlems since the probability for ann-gram that has occurredr times can be larger thanr/N, leading to\nconditional probabilities that may not verify the conditional normalisation constraints in Eq. (3.1). This\nproblem is one of the model deﬁciencies outlined in Section 3.1, and it is derived from the assumption\nmade in Eq. (1.83) in Chapter 1.\nIn order to avoid those problems, we could add another set of constraints to the formulation in\nSection 3.4\npr ≤ r\nN , r = 1 , 2, . . . , R . (3.76)\nOur aim is, then, to maximise Eq. (3.8) with the normalisation constraint in Eq. (3.33), with the\nmonotonic constraints in Eqs. (3.59) and (3.60); and with the upper boundaries deﬁned in Eq. (3.76).\nSimilarly to Section 3.4, if we apply the KKT conditions to the maximisation, we obtain the character-\nisation of the solution. The solution structure is similar to the structure in Eq. (3.61) as follows\np0 = . . . = p r1\n| {z }\nq0\n< pr1+1 = . . . = p r2\n| {z }\nq1\n< . . . < prK +1 = . . . = p rK+1\n| {z }\nqK\n(3.77)\nwith the following additional constraints\nqk ≤ rk+1\nN k = 0 , . . . , K − 1 . (3.78)\nJAF-DSIC-UPV 59\nChapter 3. Constrained leaving-one-out for language modelling\nIn this case, we proceed in a similar fashion to the previous section. Therefore, if the segmentation\nrK+1\n0 is given, then we have to maximise Eq. (3.67) subject to the normalisation constraint in Eq. (3.64),\nand constrained by Eq.(3.78). Applying KKT conditions to this problem requires the deﬁnition of the\nfollowing Lagrangian function\nL(qK−1\n0 , λ ) =\nK−1X\nk=0\nAk log qk − Λ( λ, qK−1\n0 ) − Ψ( ν , qK−1\n0 ) , (3.79)\nwithΛ( λ, qK−1\n0 ) being the Lagrangian terms induced by the normalisation constraint,\nΛ( λ, qK−1\n0 ) = λ\n KX\nk=0\nmk qk − 1\n!\n, (3.80)\nand withΨ( ν , qK−1\n0 ) being the Lagrangian terms concerning to the upper bounds inEq. (3.78),\nΨ( ν , qK−1\n0 ) =\nK−1X\nk=0\nνk\n“qk − rk+1\nN\n”\n. (3.81)\nRecall thatAk is deﬁned in Eq. (3.68).\nThe KKT conditions in this case are the followings\nAk\nqk\n− λmk + νk = 0 (3.82)\nνk\n“rk+1\nN − qk\n”\n= 0 (3.83)\nrk+1\nN ≥ qk (3.84)\nνk ≥ 0 (3.85)\nSimilarly to Section 3.3, we proceed by cases:\n• Bound is active, thenνk > 0.\n• Bound is not active, thenνk = 0 .\nOn the one hand, if the bound is active then, using Eq. (3.83) we get the value ofqk\nqk = rk+1\nN . (3.86)\nOn the other hand, if the bound is not active, we work out the value ofqk from Eq.(3.82)\nqk = 1\nλ\nAk\nmk\n. (3.87)\nFrom Eqs. (3.83), (3.86), and (3.87), we obtain a solution depending on a normalisation constant,λ,\nqk (λ) = min\n1\nλ\nAk\nmk\n, rk+1\nN\nﬀ\n, k = 0 , . . . , K − 1 , (3.88)\nwhere the normalisation constantλ depends on the segmentation\nλ = λ(rK+1\n0 ) .\nRecall thatmk is deﬁned in Eq. (3.63) andAk is deﬁned in Eq. (3.68).\n60 JAF-DSIC-UPV\n3.5. Monotonic Constraints with Upper Bounds\nTherefore, if the unknown normalisation constantλ(rK+1\n0 ) were known, then the question of\nwhether the solution in Eq. (3.88) veriﬁes the monotonic constraints in Eq. (3.59) or not depends on the\nsegmentation boundaries,rK−1\n0 , that maximise Eq. (3.67), constrained by Eq.(3.78) and by Eq. (3.64).\nTherefore, if the normalisation constantλ is given, then we can compute the segmentation that max-\nimises Eq. (3.67) with the following recurrence\nF ′(r) = arg max\nr′≤r : p r′ <pr\n{F ′(r′−1)+ A(r′, r ) log( qλ (r′, r ))} , (3.89)\nwith\nqλ (r′, r ) = min\n1\nλ\nA(r′, r )Pr\ns=r′ ns\n, r\nN\nﬀ\n. (3.90)\nRecall thatA(r′, r ) is deﬁned in Eq. (3.74).\nAfter evaluating the recurrence in Eq. (3.89) forr = 0 , ..., R ; we trace back the decisions made in\nthe evaluation to recover the optimum segmentation. Given the optimal segmentation, it is straightfor-\nward to compute the optimal probabilitiesˆqk fork = 0 , ..., K .\nIn a fashion similar to the interval constraints in Section 3.3, we deﬁne aλ dependent normalisation\nfunction\nQ′(λ) =\nKX\nk=0\nmk ˆqk (λ) , (3.91)\nand reformulate the normalisation constraint in Eq. (3.64)as\nQ′(λ) = 1 . (3.92)\nNote that the functionQ′(λ) is deﬁned using the probabilities of the optimal segmentation forλ.\nSince the functionQ′(λ) is monotonically decreasing the normalisation constraintcan be found\nusing algorithms similar to the ones used forQ(λ) in Section 3.3.\n3.5.1 Monotonic constraints\nIn order to make the quasi-monotonic algorithm fully monotonic, we can develop a similar training\nscheme to that obtained for computing the solution to the monotonic constraints with upper bounds. We\nstart with the quasi-monotonic wording but also adding the left-out constraint in Eq. (3.60). Afterwards,\nif we apply the techniques used for obtaining the solution tothe monotonic constraints with upper\nbounds, then we obtain a solution where the only difference is in the recursion in Eq. (3.89) which now\nis given by\nF ′′(r) = arg max\nr′≤r : p r′ <pr\n{F ′′(r′−1)+ A(r′, r ) log( qλ (r′, r ))} , (3.93)\nwhere qλ (r′, r ) is deﬁned by cases as follows\nqλ (r′, r ) =\n8\n>\n>\n>\n<\n>\n>\n>\n:\n1\nλ\nA(r′,r)Pr\ns=r′ ns r < R − 1\nmin\n\nA(r′,R−1)\nPR−1\ns=r′ ns\n, R\nN\nﬀ\nr = R − 1\n. (3.94)\nUnlike the quasi-monotonic case, the addition of the constraint in Eq. (3.60), makes the normalisation\nconstantλ, not to be independent of the segmentation. Therefore it is necessary to use a scheme similar\nto the monotonic with upper bounds, restating the normalisation constraint as a normalisation function\nQ′′(λ) and requiring it to be1.\nJAF-DSIC-UPV 61\nChapter 3. Constrained leaving-one-out for language modelling\n3.6 Exact extended Kneser-Ney smoothing\nThe exact extended Kneser-Ney smoothing [Andrés-Ferrer and Ney, 2009] method reduces the number\nof free parameters in the LOO estimation by using an absolutediscounting model for counts larger than\na given discounting thresholdS, mathematically expressed as\npr (pS−1\n0 , d ) =\n(\npr r < S\nr−d\nN r ≥ S , (3.95)\nwhere the parameterd is the so-called discounting parameter. Obviously, this method does not guaran-\ntee that the remaining probabilitiespr forr = 0 , 1, ..., S − 1 are monotonic. Whether monotonicity is\nsatisﬁed or not depends on the training data and the chosen discounting thresholdS.\nThis estimation technique was initially presented with a ﬁxed discounting threshold,S = 1 [Kneser\nand Ney, 1995], and afterwards extended toS = 3 [Chen and Goodman, 1998]. Nevertheless, no exact\nsolution was given for the estimation ofS > 1. In this section, we analyse the exact solution for this\napproach using the LOO log-likelihood criterion.\nWe wish to optimise the model in Eq. (3.3), but with the probabilities,pr , depending ond as\nexpressed in Eq. (3.95), for countsr larger or equal to the thresholdS. Therefore, the log-likelihood\nfunction in Eq. (3.32) is rewritten by\nF (pS−1\n0 , d ) =\nS−1X\nr=0\n(r + 1) nr+1 log pr +\nRX\nr=S\n(r + 1) nr+1 log r − d\nN , (3.96)\nsubject to the normalisation constraint in Eq. (3.33) rewritten as\nS−1X\nr=0\nnr pr +\nRX\nr=S\nnr\nr − d\nN = 1 . (3.97)\nThe optimal parameter set must maximise Eq. (3.96) subject to the normalisation constraint in\nEq. (3.97). The Lagrangian function of such mathematical problem is\nL(pS−1\n0 , d, λ ) = F (pS−1\n0 , d ) − Λ( pS−1\n0 , d, λ ) , (3.98)\nwith\nΛ( pS−1\n0 , d, λ ) = λ\n S−1X\nr=0\nnr pr +\nRX\nr=S\nnr\nr − d\nN − 1\n!\n, (3.99)\nand whereF (pS−1\n0 , d ) is deﬁned in Eq. (3.96).\nAs usual convex optimisation problems, it is needed to compute the gradient of the Lagrangian\nfunction with respect tod, andpr ,\nL(pS−1\n0 , d, λ )\n∂pr\n= (r + 1) nr\npr\n− λnr , r = 0 , 1, . . . , S − 1 (3.100)\nL(pS−1\n0 , d, λ )\n∂d = −\nRX\nr=S+1\nrnr\nr − 1 − d + λ\nRX\nr=S\nnr\nN , (3.101)\nand equalling them to0 allow us to work out the value of the optimal probability estimates,ˆpr ,\nˆpr (d) = 1\nλ(d)\n(r + 1) nr+1\nnr\n, r = 0 , . . . , S − 1 , (3.102)\n62 JAF-DSIC-UPV\n3.7. A word on time complexity\nwhere the normalisation constant depends ond as follows\nλ(d) =\n RX\nr=S+1\nrnr\nr − 1 − d\n! RX\nr=S\nnr\nN\n!−1\n. (3.103)\nSimilarly to Sections 3.3 and 3.5, we reformulate the normalisation constraint in Eq. (3.97) by\ndeﬁning a normalisation functionQ′′′(d)\nQ′′′(d) =\nS−1X\nr=0\nnr ˆpr (d) +\nRX\nr=S\nnr\nr − d\nN , (3.104)\nand requiring it to be equal to1,Q′′′(d) = 1 .\nThe functionQ′′′(d) is again monotonically decreasing, and therefore it is straightforward to ﬁnd\nthe optimal valueˆd such thatQ′′′( ˆd) = 1\nUnlike original and modiﬁed Kneser-Ney, we have not made anyapproximation in order to obtain\nthe exact value forˆd and p0. Additionally, the threshold countS is not ﬁxed beforehand to be either1\n(Kneser-Ney), or3 (modiﬁed Kneser-Ney).\nNote also that although these estimation techniques were ﬁrstly introduced by deﬁningpr forr < S\nas a function of a discount parameterdr , both parametrisation are equivalent by means of the following\nequation\npr = r − dr\nN .\n3.7 A word on time complexity\nUntil now, we have not analysed the time requirements of the proposed methods compared with the\nstandard Kneser-Ney (KN) and modiﬁed Kneser-Ney (mKN). Obviously, all the proposed methods\nneed to compute both the conventionaln-gram countsN (w, h ) and the counts-of-counts (COC)nr for\nr = 0 , 1, . . . , R − 1. Therefore, we omit the time complexity required to computethose counts.\nThe standard KN and mKN smoothings require a time complexityofO(R), since only one or three\ndiscounting parameters must be computed in order to deﬁne the probabilities˜ p(w|h) of the model in\nEq (1.75) in Chapter 1. For instance, the Kneser-Ney makes use only ofn1,n2 and n3 for deﬁning just\none discounting parameterb as expressed in Eq. (1.96) Chapter 1.\nThe proposed methods are split into two groups:\nIterative methods:comprising the constrained methods that need to perform iterations to compute the\nnormalisation constantλ by means of a decreasingly monotonic normalisation function Q(λ),\nthat is to say the following methods: interval constraints,monotonic constraints with and without\nupper bounds; and exact extended Kneser-Ney (eeKN).\nNon-iterative methods:made up of the methods that do not require to iterate, i.e., the quasi-monotonic\nmethod.\nFor the latter we give the total time complexity. Speciﬁcally, the quasi-monotonic constraint need to\ncompute the recursion speciﬁed in Eq. (3.73), which can be computed inO(R2).\nHowever, for the iterative methods, we give the time complexity for each iteration. On the one\nhand, the monotonic constraints with and without upper bounds, need to compute the recursions in\nEqs. (3.89) and (3.93) which are similar to the recursion forthe quasi-monotonic constrains requiring\na time complexity ofO(R2) for each iteration. Therefore, these two methods requireO(R2I ) in order\nto ﬁnd the solution whereI stands for the total amount of iterations needed to ﬁnd the normalisation\nconstant.\nJAF-DSIC-UPV 63\nChapter 3. Constrained leaving-one-out for language modelling\nOn the other hand, the interval constraints and eeKN can compute the probability estimates in a\ntime complexity ofO(R) for a given normalisation constant. This leads to a total time complexity of\nO(RI ) where I stands for the total amount of iterations needed to ﬁnd the normalisation constant.\nThe number of iterationsI varies depending on the data, the model order,n; and the smoothing\nalgorithm. However, in all the experimentation we have carried out in the following section, it usually\nlays in between10 and 50.\n3.8 Experiments\nExperiment setup\nIn this section, the practical performance of all the proposed smoothing techniques is analysed from the\nLM point of view. The perplexity (see Section 1.2 Chapter 1) on a test set will be used to compare all\nthe techniques. The less the perplexity is, the better the model is. Furthermore, we also use a modiﬁed\nversion of the perplexity, the so-calledjoint perplexityfor evaluating some of the proposed methods.\nThis is motivated by the assumption made in the modelling that makes equivalent the optimisation of\nthe joint and conditional models. The joint perplexity is deﬁned as the (conditional) perplexity but for\nthe probabilities which are joint probabilitiesp(w, h ) instead of the conditional probabilitiesp(w|h) as\nfollows\nPP(T ) = 2\n1\nW\nPM\nm=1\nPTm\nt=1 log2 p(sml ,h) , (3.105)\nwith the testing dataS comprising a set of evaluation sentences{s1, . . . , sM } each of lengthTm ; and\nwhere W stands for the total amount of words, i.e.,\nW =\nMX\nm=1\nTm .\nIn order to quantify the behaviour of each techniques, we have compared all the the proposed\ntechniques with the baseline perplexity given by the modiﬁed Kneser-Ney [Chen and Goodman, 1998]\nand original Kneser-Ney [Kneser and Ney, 1995]. In order to obtain the baseline, we have used the\nstandard SRILM toolkit [Stolcke, 2002]. Additionally, theexperiments have been obtained using the\nsmoothed back-off model in Eq. (3.3) unless it is otherwise speciﬁed.\nFor analysing the different smoothing techniques two corpora has been used: the English part of the\nEuroparl v3 [Callison-Burch et al., 2007] and the Wall Street Journal (WSJ) [Kneser and Ney, 1995].\nTable 3.1 summarises some statistics about the two corpora.As previously discussed, it is observed\nthat the percentage of singletonsb is very high for3-grams comprising the28.5% of the total3-gram\noccurrences.\nTable 3.2 contains some statistics for the testing data. Thetest set for the Europarl is deﬁned in the\nshared task [Callison-Burch et al., 2007]; on the other hand, for the WSJ, we have selected an small\npercentage of paragraphs from all the years, in order to gainindependence on the test set with respect\nto time factors.\nIn order to analyse the behaviour of all the techniques as a function of the training size, we have\nsplit the training into increasing sizes starting from200K sentences and doubling the size until the full\ncorpus, i.e.,200K, 400K, 800K, and full corpus (≈ 1.6M for WSJ and≈ 1.4M for Europarl).\nAn important problem in LM evaluation is how to handle theout of vocabulary (OOV)words.\nIf the OOV are not handled properly, then misleading conclusions could be drawn from evaluation.\nSome works [Kneser and Ney, 1995], tackled the problem by selecting the most frequent words, for\ninstance20K, from training and tagging the remaining words as unknown words. Then the unknownbBy singleton we denote here, an event such as a word or an-gram that has occurred just once in the corpus.\n64 JAF-DSIC-UPV\n3.8. Experiments\nTable 3.1:Table with some statistics of the both corpora used in the experiments.\nTraining Europarl WSJ\nsentences 1.40M 1.62M\navg. length 24.6 26 .0\nrunning words 34.4M 42.12M\nvocab. size 280.5K 200.1K\nn1/N (1-gram) 0.4 % 0.2%\nn1/N (2-gram) 18.9 % 18.1%\nn1/N (3-gram) 28.5 % 28.5%\nTable 3.2:Some statistics of the both test sets.\nTest Europarl WSJ\nsentences 2K 12.5K\navg. length 26.8 26 .1\nrunning words 53.6K 326.3K\nword becomes a very common event, such that small variationsin its probability could dominate the\nperplexity differences among systems. For instance, if we take the extreme example in which all the\nvocabulary words are unknown, then the perplexity of any text will be1; although the meaning of this\nperplexity is misleading since it does not mean that given a previous history of words the LM is able\nto predict the following word. Instead, this perplexity means that our model is able to predict that the\nfollowing word will beunknown to the LM, which is useless for most of the applications.\nIn order to avoid these misleading conclusions, two steps have been taken. On the one hand, we\nreport perplexity results skipping OOVn-grams. On the other hand, we have performed experiments\nincreasing the size of the vocabulary from10% of the vocabulary until the100% in steps of10%.\nFor the100% case in which all the vocabulary words are considered, we have reserved the smoothing\nprobability mass for the unseen uni-grams in order to give probability to the unknown words. For\ndoing so, the full vocabulary size must be known, however, any sensible estimation of the size sufﬁces,\nspeciﬁcally, we have extrapolated the number of unseen words in the vocabulary from the seen words.\nBare in mind that we also report perplexities ignoring OOV words to quantify the inﬂuence of our\nestimation for unknown events. Table 3.4 reﬂects the percentage of OOV in test as a function of the\ntraining size. Furthermore, in table 3.3 the sizes of the vocabulary partitions are detailed.\nIn the remaining of this section, we have obtained results mainly for bi-grams and tri-grams lan-\nguage models. For some experiment conﬁgurations we have also computed4-grams results. In all cases\nthe conclusions are consistent with results obtained for the tri-gram language model.\nTheoretical properties in practice\nWe can gain some insights into the constrained technique by analysing the TG countsr⋆ . Therefore,\nin Figure 3.1 the TG counts,r⋆ = p r N , are plotted as a function of the original countsr. The plots\nwere obtained using a4-gram language model in the200k partition of the WSJ corpus and using the\nJAF-DSIC-UPV 65\nChapter 3. Constrained leaving-one-out for language modelling\nTable 3.3:Percentage of out of vocabulary words (OOV) in test as a function of the\ntraining size and the percentage of vocabulary size. The ﬁgures represent percentages,\ni.e.,5.4 stands for5.4% words.\nV oc. Pctg. Corpus 200K 400K 800K full size\n10% Europarl 5.4 4.0 2.9 2.1\nWSJ 6.3 4.9 3.7 2.8\n20% Europarl 3.0 2.1 1.5 1.1\nWSJ 3.4 2.5 1.8 1.4\n30% Europarl 2.1 1.4 1.0 0.8\nWSJ 2.3 1.6 1.2 0.8\n40% Europarl 1.7 1.2 0.9 0.6\nWSJ 1.7 1.2 0.8 0.6\n50% Europarl 1.4 1.0 0.7 0.5\nWSJ 1.4 0.9 0.7 0.4\n60% Europarl 1.3 0.9 0.7 0.5\nWSJ 1.2 0.8 0.5 0.3\n70% Europarl 1.2 0.9 0.6 0.5\nWSJ 1.0 0.7 0.5 0.3\n80% Europarl 1.1 0.8 0.6 0.5\nWSJ 1.0 0.6 0.4 0.3\n90% Europarl 1.1 0.7 0.5 0.5\nWSJ 0.8 0.6 0.4 0.2\n100% Europarl 1.0 0.7 0.5 0.4\nWSJ 0.7 0.5 0.3 0.2\nfull vocabulary. It is worth noting that the original TG counts wildly oscillate for larger values ofr,\ni.e.m the estimation ofr⋆ isnoisy. Recall that we have already outlined this property of the TGcounts\nwhen analysing the LOO smoothing model deﬁciencies in Section 3.1.\nIt is valuable to mention, the way in which each of the proposed methods counteract such over-\ntraining. The probability estimates obtained with the interval constraints tend to strictly verify one of\nthe constraints, either the upper or the lower. The (quasi-)monotonic constraintsc, howeverer, tend to\nproduce strips with the same TG count. These strips are sometimes larger than the countr itself. Oppo-\nsitely, the monotonic constraints with upper bounds methodavoids this undesirable result by splitting\nthese strips whenever necessary. Finally, in the case of theeeKN and for the chosen discounting thresh-\nold (S = 30 ), it can be seen that the TG counts are not monotonic. However, if we had chosenS = 10\ninstead, then the counts would have been monotonic, since all the noisy counts are approximated by\none discount as depicted in Figure 3.1.\nThe ﬁgure 3.2 is the analogous version of ﬁgure 3.1 but for using a3-gram model instead of4-gram\nmodel. When comparing ﬁgures 3.1 and 3.2, we observe that when then-gram order, i.e.n, is increased\nthe counts oscillate more wildly.\ncSince the only difference between the quasi-monotonic and the monotonic constraints is the upper constraint\npR−1 ≤ R/N ; their plots are virtually the same.\n66 JAF-DSIC-UPV\n3.8. Experiments\nTable 3.4:V ocabulary size as a function of the training size and the percentage of the\nfull vocabulary. The ﬁgures represent Kilo-words, i.e.,8.7 stands for8.7K words.\nV oc. Pctg. Corpus 200K 400K 800K full size\n10% Europarl 8.7 11.7 15.3 20.0\nWSJ 10.2 14.6 20.9 28.1\n20% Europarl 17.4 23.3 30.7 40.0\nWSJ 20.4 29.2 41.8 56.1\n30% Europarl 26.2 35.0 46.0 60.0\nWSJ 30.7 44.0 62.7 84.2\n40% Europarl 34.9 46.6 61.3 80.0\nWSJ 40.9 58.3 83.7 112.2\n50% Europarl 43.6 58.3 76.7 100.0\nWSJ 51.1 73.0 104.6 140.3\n60% Europarl 52.3 70.0 92.0 120.1\nWSJ 61.3 87.5 125.5 168.3\n70% Europarl 61.1 81.6 102.4 140.1\nWSJ 71.6 102.1 146.4 196.4\n80% Europarl 69.8 93.3 122.7 160.1\nWSJ 81.8 116.7 167.3 224.4\n90% Europarl 78.5 104.9 138.1 180.1\nWSJ 92.0 131.3 188.2 252.5\n100% Europarl 87.2 116.6 153.5 200.0\nWSJ 102.3 145.8 209.1 280.5\nFinally, the ﬁgure 3.3 depicts one of the normalisation functions,Q(λ). Speciﬁcally, we have\nselected the normalisation function for the interval constraints. In this plot, it is observed that the\nfunction is monotonically decreasing. Note that the total number of seenn-grams,N , is equal to\n5 210 341 and the optimal normalisation constantˆλ takes the value of5 244 023 . The normalisation\nfunctions of the other proposed smoothings show a similar behaviour of that depicted in ﬁgure 3.3 but\nfor the eeKN smoothing.\nFor the eeKN case, we have plotted the normalisation function in ﬁgure 3.4. Recall that, the nor-\nmalisation function for the eeKN smoothing is parametriseddepending on the discounting parameter\nd instead of a normalisation constantλ. In the ﬁgure 3.4, we have plotted the normalisation function\nQ(d) for severaln-gram models in the case of eeKN withS = 3 . The larger then-gram order is, the\nlarger the discounting parameterd is. Speciﬁcally, the discounting parameter takes the valueof0.61,\n1.01,1.13 and 1.20 respectively for uni-gram, bi-gram, tri-gram and four-gram models.\nBacking-off results\nFirstly, in the ﬁgures 3.5 and 3.6 we analyse the practical behaviour of all the methods involving mono-\ntonic constraints: quasi-monotonic, monotonic with upperbounds and monotonic. The ﬁrst surprising\nresult is that the (conventional) Kneser-Ney (KN) outperforms the modiﬁed Kneser-Ney (mKN). Re-\nJAF-DSIC-UPV 67\nChapter 3. Constrained leaving-one-out for language modelling\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nInterval Constraints\nTuring-Good\nr⋆ = r\nr⋆ = r − 1\nInterval\nr → r⋆\nr\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nExact Extended Kneser Ney\nTuring-Good\nr⋆ = r\nr → r⋆\nr\neeKN (S = 30 )\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nMonotonic Constraints\nTuring-Good\nr⋆ = r\nr → r⋆\nr\nMonotonic\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nMonotonic Constraints with Upper Bounds\nTuring-Good\nr⋆ = r\nMonotonic Upper\nr → r⋆\nr\nFigure 3.1:The 4-gram modiﬁed countr⋆ as a function of the original countr for the4\nproposed techniques obtained with200K sentences partition of WSJ and full vocabulary\nsize.\nsults in [Chen and Goodman, 1996] report that the mKN outperforms the KN smoothing, however, this\nresult are obtained using a linear discounting instead of a backing-off discount. Afterwards, we analyse\nthe behaviour of linear discounting smoothings, and, then,we will see that mKN outperforms the KN\nfor interpolated discounting methods.\nIn ﬁgure 3.5, the (conditional) perplexity ignoring OOV events is plotted as a function of the per-\ncentage of most frequent words from the vocabulary. The three techniques show virtually the same\nbehaviour which slightly improves the best baseline, KN. This improvement, is systematic and grows\nwith the vocabulary size. Although, the plot in ﬁgure 3.5 wasobtained with a3-gram language model,\nother orders such as2-gram or4-gram, obtain similar plots. For the case of the perplexity without\nignoring the OOV the plots show a similar behaviour. Therefore, we do not include the plots for the full\nperplexity (with OOV) since the behaviour is similar but with a slightly smaller gap.\nFigure 3.6, comprises two plots with the perplexity as a function of the training size for the WSJ\n68 JAF-DSIC-UPV\n3.8. Experiments\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nInterval Constraints\nTuring-Good\nr⋆ = r\nr⋆ = r − 1\nInterval\nr → r⋆\nr\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nExact Extended Kneser Ney\nTuring-Good\nr⋆ = r\nr → r⋆\nr\neeKN (S = 30 )\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nMonotonic Constraints\nTuring-Good\nr⋆ = r\nr → r⋆\nr\nMonotonic\n 0\n 10\n 20\n 30\n 40\n 50\n 0  10  20  30  40  50\nMonotonic Constraints with Upper Bounds\nTuring-Good\nr⋆ = r\nMonotonic Upper\nr → r⋆\nr\nFigure 3.2:The 3-gram modiﬁed countr⋆ as a function of the original countr for the4\nproposed techniques obtained with200K sentences partition of WSJ and full vocabulary\nsize.\ncorpus. The top plot, shows the performance of the monotonicsmoothing techniques for a2-gram\nlanguage model. The bottom plot is the analogous plot for3-gram language model. There is not any\ndifference between the performance of the monotonic modelswhich obtain slightly better results than\nthe KN baseline. It is observed that the improvement gap is larger for low order models.\nSince we have not observed any signiﬁcant difference between the monotonic approaches, we will\nhenceforth take monotonic with upper bounds as the representative of these smoothing techniques.\nFurthermore, the KN smoothing is taken as the baseline sinceit obtains better results for a back-off\nsmoothing scheme than the mKN.\nThe behaviour of the exact and extended Kneser-Ney (eeKN) isdepicted in ﬁgure 3.7. It is observed\nthat the best perplexity results are obtained withS = 1 , that is to say, with just2 free parameters:p0\nand d. Note that in this case the eeKN is just a different estimation of the conventional KN smoothing.\nAnyway, as the training data increases, the differences between different choices of this discounting\nJAF-DSIC-UPV 69\nChapter 3. Constrained leaving-one-out for language modelling\n1.0\n5.2 5.3\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2M 3M 4M 5M 6M 7M\n1\nQ(λ)\nQ(λ)\nQ(λ)\nˆλ\nλ\nN\nFigure 3.3:The normalisation functionQ(λ) for a interval constraint3-gram model\ncomputed with the200K sentences partition of WSJ and full vocabulary.\n0.98\n0.99\n1.00\n1.01\n1.02\n 0  0.2  0.4  0.6  0.8  1  1.2  1.4  1.6  1.8  2\n1\nQ(d)\nQ(d) 1 -gram\nQ(d) 2 -gram\nQ(d) 3 -gram\nQ(d) 4 -gram\nd\nFigure 3.4:The normalisation functionQ(d) for the eeKN (S = 3 ) smoothing com-\nputed with the200K sentences partition of WSJ and full vocabulary size.\nthreshold do not signiﬁcantly modify the result. It is of worth noting that for the testedn-grams (n =\n2, 3, 4), the eeKN always outperforms the KN smoothing. The more scarce training data is, the larger\nthe improvement is.\nIn Table 3.5, we compare the perplexities obtained in both corpora (full vocabulary) using a trigram\n70 JAF-DSIC-UPV\n3.8. Experiments\n 74\n 78\n 82\n 86\n 90\n10 20 30 40 50 60 70 80 90 100\nPerplexity\nMon. & upper bounds\nQuasi-Monotonic\nMonotonic\nmodiﬁed Kneser-Ney\nKneser-Ney\nV ocabulary Size (%)\nFigure 3.5:Perplexity skipping OOV events as a function of vocabulary size (in %)\nwith the full size partition of the Europarl corpus for a3-gram language model smoothed\nwith all the monotonic approaches and the standard smoothings Kneser-Ney (KN) and\nmodiﬁed Kneser-Ney (mKN).\nlanguage model. We can conclude that all the proposed techniques perform at least as the baseline,\nbeing better in certain circumstances. It is observed that eeKN outperforms the baseline in all the\ncircumstances. It is also observed that all the proposed monotonic constraints yield the same result.\nIn the ﬁrst place, we had expected better results with the monotonic approaches since they are less\nrestrictive than the interval constraints. Recall from Section 1.2.3 in Chapter 1, that for optimising\nthe probability estimates, we have smoothed the conditional probabilities˜p(w|h) with the model in\nEq. (1.75) and in order to optimise the parameters, we have taken the assumption in Eq. (1.83) which\nleads to the optimisation of the joint model in Eq. (1.93). Therefore, we may loose performance by\nthe assumption made in Eq. (1.83). Figure 3.8 shows thejointperplexity as deﬁned in Eq. (3.105) for\nthe the different approaches using the full size partition of the WSJ corpus. We have plotted only the\nmonotonic constraints with upper bounds since all the monotonic constraints obtain the very same joint\nperplexities as well as conditional perplexities.\nTwo conclusions can be drawn from ﬁgure 3.8. On the one hand, the joint perplexity always in-\ncreases with the training size since it is computed with the joint probabilities and, hence, the more\nn-grams we observe the smaller the probabilities are in average. On the other hand, it clearly shows\nthat the monotonic approaches are signiﬁcantly inﬂuenced by the assumption in Eq. (1.83) which make\nus to optimise a joint model instead of a conditional model. Actually, all the proposed smoothings but\nfor the monotonic constrained, have almost the same behaviour according to the joint perplexity. This\ncommon behaviour is also shared by the KN. Therefore, the behaviour observed with the conditional\nperplexity is due to the fact that the smoothings are degraded when passing from the joint model to the\nconditional model. The models which obtain better results are less degraded than the others. This fact\nis somehow surprising and inspiring for future work (see Section 3.9).\nJAF-DSIC-UPV 71\nChapter 3. Constrained leaving-one-out for language modelling\n 146\n 150\n 154\n 158\n 162\n 166\n 170\nPerplexity\nMon. & upper bounds\nQuasi-Monotonic\nMonotonic\nmodiﬁed Kneser-Ney\nKneser-Ney\nTraining size\n2-gram\n1.3M0.8M400K200K\n 86\n 90\n 94\n 98\n 102\n 106\n 110\n 114\n 118\n 122\n1.5M\nPerplexity\nMon. & upper bounds\nQuasi-Monotonic\nMonotonic\nmodiﬁed Kneser-Ney\nKneser-Ney\nTraining size\n3-gram\n0.8M400K200K\nFigure 3.6:Perplexity a function of the WSJ training size with full vocabulary, and\nfor all the monotonic approaches and the standard smoothings Kneser-Ney (KN) and\nmodiﬁed Kneser-Ney (mKN).\nLinear interpolation results\nAlthough the theory and the proposed methods are aimed at a backing-off smoothing model, we can\nexperimentally use the smoothings in a linear interpolation model as discussed in Section 3.2.2.\n72 JAF-DSIC-UPV\n3.9. Conclusions and future work\n 86.8\n 86.9\n 87.0\n 87.1\n 87.2\n1 2 4 8 16 32 64 128\nPerplexity\neeKN\nKN\nDiscounting threshold (S)\n1.4M\nFigure 3.7:Perplexity ignoring OOV events as a function of the eeKN discounting\nthreshold (S) in logarithmic scale computed with the Europarl corpus andusing a3-\ngram language model.\nAs can be seen in table 3.6, the mKN outperforms the KN as expected in this case. In general, all\nthe proposed smoothing techniques are signiﬁcantly degraded when used in a interpolation smoothing,\nobtaining similar results to that of the mKN. This degradation is also observed for4-grams and2-gram.\nIt is also observed that for the interpolation case the discounting threshold of the eeKN seems\nto play an important role. In Fig. 3.9, it is clearly observedthat the best result is obtained with the\ndiscounting threshold,S = 3 .\nIf we compare tables 3.5 and 3.6, it is observed that in general the interpolated smoothing obtains\nbetter results.\n3.9 Conclusions and future work\nStandard discounting models based on leaving-one-out estimates represent two extremes. On the one\nextreme the absolute discounting (Kneser-Ney) reduces thenumber of parameters to estimate to one.\nOn the other extreme the Turing-Good smoothing estimates all the LOO probabilities, producing small\nprobabilities and over-ﬁtting problems.\nIn this chapter, we have developed novel discounting methods that are less restrictive than absolute\ndiscounting approaches, but more restrictive than Turing-Good method. Therefore, we try to optimise\nthe trade-off between the number of parameters and the data scarcity.\nSpeciﬁcally, we have proposed ﬁve novel discounting methods based on constraining leaving-one-\nout estimates: interval constraints, quasi-monotonic constraints, monotonic constraints, monotonic con-\nstraints with upper bound and the exact extended Kneser-Neysmoothing. The associated estimation al-\ngorithms are also derived in order to compute the discountedestimates in an efﬁcient way. We have also\nJAF-DSIC-UPV 73\nChapter 3. Constrained leaving-one-out for language modelling\n 0\n 100\n 200\n 300\n 400\n 500\n 600\n 700\nPerplexity\nMon. & upper bounds\nInterval\neeKN (S = 1 )\nKneser-Ney\nTraining size\n0.8M400K200K\nWSJ\n1.5M\nFigure 3.8:Joint perplexityas a function of WSJ training size for several smoothing\ntechniques applied to a3-gram language model.\nperformed systematic experiments for two language modelling tasks, comparing the proposed methods\nwith other standard discounting methods. This experimentation reports slight improvements over the\nbaseline of the KN/mKN method under some circumstances, specially for scarce data. However, an\nimprovement in terms of perplexity does not always imply an improvement in terms of word error rate\n(WER). As future work we intend to check if the perplexity improvement are transferred to the system\nperformance.\nWe have found several surprising and interesting conclusions. Firstly, all the monotonic-tagged\nmethods (quasi-monotonic, monotonic, and monotonic with upper bounds) behave similarly. This is\nvery surprising, since except for the monotonic with upper bound, all monotonic smoothing models\nhave normalisation deﬁciencies. As discussed in Section 3.1, due to the way in which we use the joint\nprobability estimatespr to deﬁne the conditional smoothing model as depicted in Eq. (3.3), the dis-\ncounted probabilityBh can be0 or even negative ifr < r ⋆ . In such cases, the heuristic approaches\nrenormalise those parameters adding the negative probability mass plus1, i.e.−Bh + 1 , to the total\nprobability amount. We thought that this arbitrary renormalisation was distorting the probability esti-\nmates degrading the system performance. Oppositely, we found that ﬁxing theoretically that problem\nby adding upper boundaries to the probability estimatespr , do not incur in any proﬁt when compared\nwith deﬁcient models ﬁxed heuristically.\nFor eeKN case, we found that several values of the discounting thresholdS, obtain worse results\nthan that of the minimum valueS = 1 for a backing-off smoothing. From previous works, where the\nmKN obtained better results than KN; this conclusion was unexpected since the KN can be understood\nas an alternative estimation of the caseS = 1 and the mKN as an alternative estimation for the case\nS = 3 . However, in a interpolation model, we have found that eeKN withS = 3 outperformsS = 1 ,\nwhich is consistent with the fact that mKN outperforms KN with interpolation smoothing. Hence, it\nseems that the fact of redistribution the discounted probability mass over all the events has a positive\neffect, however in this case is important to use3 (S = 3 ) different discounting parameters oppositely\n74 JAF-DSIC-UPV\n3.9. Conclusions and future work\n 86.0\n 86.1\n 86.2\n 86.3\n 86.4\n1 2 4 8 16 32 64 128\nPerplexity\neeKN\nmKN\nDiscounting threshold (S)\n1.4M\nFigure 3.9:Perplexity ignoring OOV events as a function of the eeKN discounting\nthreshold (S) in logarithmic scale computed with the Europarl corpus andusing a inter-\npolated smoothed3-gram language model.\nto back-off smoothing.\nAnother interesting observation is that directly applyingthe optimal smoothing parameters for the\nbacking-off smoothing model to the interpolated model degrades all the smoothings. It would be in-\nteresting to apply the proposed theory to an interpolated smoothing model, in order to see whether the\nproposed smoothings improve the interpolation baseline ornot.\nFinally, the most surprising conclusion is that the monotonic-tagged smoothings do not report an\nimprovement with respect to the interval constraint in terms of (conditional) perplexity. However, if we\ndeﬁne a joint version of such perplexity then these models obtain a higher performance, as expected.\nTherefore, the assumption of optimising a joint model instead of the conditional model has some impor-\ntant and negative repercussions. Actually, all the discounting methods but for the monotonically-tagged\nmethods, obtain almost the same results in terms of joint perplexity. This make us think that the main\ndifference among the smoothings methods is the way in which each smoothing is degraded when pass-\ning from a joint model to a conditional one.\nFrom the discussion above, we expect to obtain improvementsby directly optimising a conditional\nsmoothing model without any assumption. As future work, we intend to optimise conditional probabil-\nities avoiding the map to a joint model.\nJAF-DSIC-UPV 75\nEuroparl\nTraining Size200K400K800KFull Size\nAll Sk OOVAll Sk OOVAll Sk OOVAll Sk OOV\nmKN117.7 117.0106.0 105.296.3 95.789.3 88.7\nKN115.3 114.0104.1 103.094.8 93.988.0 87.2\nMonotonic Upper115.3 113.7104.0 102.794.6 93.687.9 87.0\nQuasi-Monotonic115.3 113.7104.0 102.794.6 93.687.9 87.0\nMonotonic115.3 113.7104.0 102.794.6 93.687.9 87.0\nInterval115.1 113.6103.8 102.694.5 93.687.8 86.9\neeKN (S= 1)114.8 113.2103.6 102.394.4 93.487.6 86.7\neeKN (S= 3)115.1 113.6103.8 102.694.5 93.587.8 86.9\nWall Street Journal (WSJ)\nmKN120.3 118.6107.4 106.095.9 94.885.9 85.2\nKN120.3 118.6107.4 106.095.9 94.885.9 85.2\nMonotonic Upper120.2 118.2107.2 105.795.8 94.685.8 85.0\nQuasi-Monotonic120.1 118.2107.2 105.795.8 94.685.7 85.0\nMonotonic120.1 118.2107.2 105.795.8 94.685.7 85.0\nInterval119.9 118.0107.1 105.695.7 94.585.7 84.9\neeKN (S= 1)119.7 117.7106.9 105.395.6 94.485.6 84.8\neeKN (S= 3)120.0 118.0107.1 105.695.7 94.585.7 84.9\nTable 3.5:Perplexities on the corpora for abacking-offsmoothed3-gram language model.Sk OOVcolumn stands for the perplexity\nskipping the OOV , while theAllcolumn accumulates all the events (OOV and known).\nEuroparl\nTraining Size 200K 400K 800K Full Size\nAll Sk OOV All Sk OOV All Sk OOV All Sk OOV\nmKN 123.9 111 .6 109.3 101 .3 97.9 92 .7 90.4 86.2\nKN 124.2 111 .6 109.6 101 .5 98.2 92 .9 90.8 86 .4\nMonotonic Upper 123.9 111 .3 109.3 101 .2 98.0 92.6 90.5 86 .2\nQuasi-Monotonic 124.0 111 .3 109.3 101 .2 98.0 92.6 90.5 86 .2\nMonotonic 124.0 111 .3 109.3 101 .2 98.0 92.6 90.5 86 .2\nInterval 124.0 111 .3 109.3 101 .2 97.9 92.6 90.5 86.1\neeKN (S = 1 ) 124.3 111 .5 109.6 101 .4 98.2 92 .9 90.8 86 .4\neeKN (S = 3 ) 123.8 111 .2 109.2 101 .1 97.2 92 .6 90.5 86.1\nWall Street Journal (WSJ)\nmKN 126.6 116.3 110.7 104.5 97.6 93 .9 86.8 84 .5\nKN 127.1 116 .6 111.2 104 .8 98.1 94 .2 87.1 84 .9\nMonotonic Upper 126.8 116.2 110.9 104 .5 97.8 93.9 86.8 84 .6\nQuasi-Monotonic 126.8 116.2 110.9 104 .5 97.8 93.9 86.9 84 .6\nMonotonic 126.8 116.2 110.9 104 .5 97.8 93.9 86.9 84 .6\nInterval 126.8 116 .3 110.9 104 .5 97.8 93.9 86.9 84 .6\neeKN (S = 1 ) 127.3 116 .6 111.4 104 .9 98.2 94 .3 87.2 84 .9\neeKN (S = 3 ) 126.7 116.2 110.9 104.4 97.7 93.9 86.8 84 .5\nTable 3.6:Perplexities on the corpora for alinear interpolationsmoothed 3-gram language model.Sk OOV column stands for the\nperplexity skipping the OOV , while theAllcolumn accumulates all the events (OOV and known).\n\nBibliography\nBibliography\nJesús Andrés-Ferrer and Hermann Ney. Extensions of\nabsolute discounting (kneser-ney method). InIEEE\nInternational Conference on Acoustics, Speech, and\nSignal Processing, Taipei, Taiwan, 2009. Association\nfor Computational Linguistics.\nChris Callison-Burch, Philipp Koehn, Cameron Shaw\nFordyce, and Christof Monz, editors.Proceed-\nings of the Second WSMT. Association for Com-\nputational Linguistics, Prague, Czech Republic,\nJune 2007. URL http://www.aclweb.org/\nanthology/W/W07/W07-02.\nS. F. Chen and J. Goodman. An empirical study of\nsmoothing techniques for language modeling. In\nProc. of ACL’96, pages 310–318, Morristown, NJ,\nUSA, June 1996. Association for Computational Lin-\nguistics.\nStanley Chen and Joshua Goodman. An empirical study\nof smoothing techniques for language modelling.\nTechnical Report TR-10-98, Harvard University,\n1998. See http://research.microsoft.com/ joshuago/tr-\n10-98.ps.\nI. J. Good. Population frequencies of species and the\nestimation of population parameters.Biometrika, 40:\n237–264, 1953.\nJoshua Goodman. A bit of progress in language model-\ning.CoRR , cs.CL/0108005, 2001.\nR. Kneser and H. Ney. Improved backing-off form-\ngram language modeling.IEEE Int. Conf. on Acous-\ntics, Speech and Signal Processing, II:181–184, May\n1995.\nA. Nadas. On turing’s formula for word probabilities.\nIEEE Trans. Acoustics, Speech, and Signal Proc., 33:\n1,414–1,416, 1984.\nH. Ney, S. Martin, and F. Wessel. Statistical language\nmodeling using leaving-one-out. In S. Young and\nG. Bloothooft, editors,Corpus-Based Statiscal Meth-\nods in Speech and Language Processing., pages 174–\n207. Kluwer Academic Publishers, 1997.\nA. Stolcke. SRILM – an extensible language model-\ning toolkit. InProc. of ICSLP’02, pages 901–904,\nSeptember 2002.\nJAF-DSIC-UPV 79\nBibliography\n80 JAF-DSIC-UPV\nChapter 4\nThe loss function in statistical pattern recognition\n“ Life’s most important questions are, for the most part, nothing but probability problems.”\nPIERRE -SIMON L APLACE\nContents\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.2 Bayes Decision Theory . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n4.3 Statistical Machine Translation . . . . . . . . . . . . . . . . . . .. . 86\n4.3.1 General error functions . . . . . . . . . . . . . . . . . . . . . . 87\n4.3.2 Simpliﬁed error functions . . . . . . . . . . . . . . . . . . . . . 87\n4.3.3 Approximation to general error functions . . . . . . . . . .. . . 89\n4.3.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n4.3.5 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n4.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n81\nChapter 4. The loss function in statistical pattern recognition\n4.1 Introduction\nStatistical pattern recognition is a well-founded discipline that allow us to solve many practical classi-\nﬁcation problems. A classiﬁcation problem is stated as the problem of choosing to which class a given\nobject belongs. LetX be the domain of the objects that a classiﬁcation system might observe; and\nΩ the set of possible classes ({ω1, ω 2, . . . , ω C }) to which an object may belong to. A classiﬁcation\nsystem is characterised by a function that maps each object to one class, the so-calledclassiﬁcation\nfunction (c : X → Ω )[Duda et al., 2001].\nThe performance of a classiﬁcation system is usually measured as a function of the classiﬁcation\nerror. However, there are problems in which all the classiﬁcation mistakes or misclassiﬁcations do not\nhave the same repercussions. Therefore, a criterion that ranks these mistakes should be provided. The\nloss function,l(ωp|x, ω c ),evaluates thelossin which the classiﬁcation system incurs when classifying\nthe objectx into the classωp, knowing that the correct class isωc [Duda et al., 2001]. It is well\nknown that, if a0–1 loss function is provided, then the optimal system minimises the classiﬁcation\nerror rate [Duda et al., 2001].\nThis chapter is mainly devoted to design loss functions thatshould improve system performance\nwhile keeping the simplicity of0–1 optimal classiﬁcation system. In [R. Schlüter and Ney, 2005]\ncomplex classiﬁcation rules were analysed using ametric loss function. Some other works, for in-\nstance [Uefﬁng and Ney, 2004], analyse the most general lossfunctions. However, we focus on other\nloss functions which are not restricted by the metric requirements at the expense of ignoring the class\nproposed by the system, i.e.ωp.\nThe remainder of this chapter is organised as follows. In Section 4.2 pattern recognition problems\nare analysed from a decision theory point of view. In Section4.3, we introduce statistical machine\ntranslation as a case of study. Finally, concluding remarksare summarised in Section 4.4.\n4.2 Bayes Decision Theory\nIn this Section, we review and develop some of the ideas introduced in Section 1.1 Chapter 1. A\nclassiﬁcation problem is an instance of a decision problem.From this point of view, a classiﬁcation\nproblem is composed of three different items:\n1. A set ofObjects(X ) the system might observe and has to classify.\n2. A set of classes (Ω = {ω1, . . . , ω C , . . . }) in which the system has to classify each observed\nobjectx ∈ X .\n3. A Loss function,l(ωp|x, ω c ), used to weight the classiﬁcation actions. This function evaluates\nthe loss of classifying an observed objectx into a class,ωp ∈ Ω , knowing that theoptimal class\nfor the objectx isωc ∈ Ω .\nRecall that a classiﬁcation system is characterised by the classiﬁcation function, which maps each\nobject to one class [Duda et al., 2001]\nc : X → Ω . (4.1)\nTherefore, when an objectx ∈ X is observed in a classiﬁcation system, the system should choose\nthe “correct” class from all possible classes. Taking this framework into account, we deﬁne the risk of\na system when classifying an objectx, the so-calledconditional risk givenx, as\nR(ωp|x) =\nX\nωc ∈Ω\nl(ωp|x, ω c ) pr (ωc|x) . (4.2)\nNote that the conditional risk is the expected value of the loss function,l(ωp|x, ω c), with respect to the\nactual probability distribution,pr (ω|x).\n82 JAF-DSIC-UPV\n4.2. Bayes Decision Theory\nUsing the conditional risk, we deﬁne thethe global risk[Duda et al., 2001] as the contribution of\nall objects to the classiﬁer performance, i.e.\nR(c) = E x [R(c(x)|x)] =\nZ\nX\nR(c(x)|x) pr (x)dx , (4.3)\nwhere R(c(x)|x) is the conditional risk givenx, as deﬁned in Eq. (4.2).\nWe wish to design the classiﬁcation function that minimisesthe global risk. Since minimising the\nconditional risk for each objectx is a sufﬁcient condition to minimise the global risk, without any loss\nof generality, the optimal classiﬁcation rule, namelyminimum Bayes’ risk, is the one that minimises the\nconditional risk, i.e.\nˆ c(x) = arg min\nω∈Ω\nR(ω|x) . (4.4)\nTherefore, depending which loss function the system designis based on, there is a different optimal\nclassiﬁcation rule.\nThe algorithms that perform the minimisation in previous Eq. (4.4), are often calleddecoding al-\ngorithmsorsearch algorithms. Consequently, the problem of designing an algorithm that perform such\nminimisation is calledthe decoding problemorthe search problem.\nThroughout this chapter we focus on the way of building the optimal classiﬁcation system with the\nbest possible model. We do not intend to discuss about which training criterion, method or algorithm\nis better for improving the system performance. Instead, wedeal with the following stage in the design\nof the system. Once we have the best possible approximation to the actual probability distributions, we\nanswer the question of which the best decoding strategy is.\nIn practice, we also need to compare among systems. In order to do so, we need to compare the\nglobal risk of those systems. The global risk in Eq. (4.3), can be understood as the expected loss with\nrespect to the object-class joint probability distribution\nR(c) = E x [R(c(x)|x)] =\nZ\nX\nX\nω∈Ω\nl(c(x)|x, ω ) pr (ω, x)dx , (4.5)\nwithpr (ω, x) = pr (ω|x) p(x). Therefore, using the law of great numbers for a given test set,T =\n{(xn , ω n)}N\nn=1, i.i.d. according topr (ω, x), the global risk can be approximated by\n¯RT (c) = 1\nN\nNX\nn=1\nl(c(xn)|xn , ω n) . (4.6)\nWe call this approximation theempirical riskon the test setT .\nThere is not a unique best loss function for any system, sincethe loss depends on the characterisa-\ntion of the task that we want to solve. The classical and most common approach is to consider that each\nmisclassiﬁcation has the same impact. Therefore, a priori we distinguish two sorts of actions: wrong\nclassiﬁcations (loss of1) and correct classiﬁcations (zero loss), i.e.,\nl(ωp|x, ω c ) =\n(\n0 ωp = ωc\n1 otherwise (4.7)\nThis loss function is known as the0–1 loss function.\nMinimising the risk when the loss function is the0–1 loss function, is equivalent to minimise the\nclassifying errors. When Eq. (4.7) is used, the minimum Bayes’ risk in Equation (4.4) is simpliﬁed\nyielding the well-known optimal Bayes’ classiﬁcation rule[Duda et al., 2001],\nc(x) = arg max\nω∈Ω\npr (ω |x) , (4.8)\nJAF-DSIC-UPV 83\nChapter 4. The loss function in statistical pattern recognition\nwhere x is the object to be classiﬁed, andω denotes a possible proposed class.\nHowever, while the0–1 loss function is adequate for many problems, which have a small set of\nclasses; there are problems where a more appropriate loss function should be deﬁned. For example, if\nthe system classiﬁes diseases, it may be worse to classify anill person as a healthy one than vice-versa.\nThis distinction is made independently of the illness probability, and depending upon the repercussions\nof the wrong actions, i.e., depending on the following questions: is the illness treatment dangerous?\nis the illness deadly?, and so on. Another important exampleis the case in which the set of classes\nis large, or even inﬁnite (but still enumerable). In such a case, as the set of all possible classes is\nhuge, it is not appropriate to penalise all wrong classes with the same weight. In other words, since it is\nimpossible to deﬁne a uniform distribution when the number of classes is inﬁnite, it does not make sense\nto deﬁne a uniform loss function in the inﬁnite domain because there are objects that are more probable\nthan others, and the error will be increased if the system fails in probable objects. Instead of using\nthe0–1 loss function, it would be better to highly penalise the domain zones where the probability\nis high. In this way, the system will avoid mistakes on probable objects at the expense of making\nmistakes on unlikely objects. Consequently, the error willbe decreased since unlikely objects occur\nfewer times in comparison with probable objects. Note that we are dealing with inﬁnite enumerable\nsets in this example, and, therefore, this is a classiﬁcation problem and not a linear regression problem.\nAn example of this idea is plotted at Fig. 4.1\nThe most general loss function that can be deﬁned makes use ofthe three variables: the object to\nclassifyx, the proposed classωp and the correct classωc . In general, it is useless to deﬁne a non-zero\nloss function when the proposed class and the correct class are equal. Therefore, we deﬁne theerror\nfunctionǫ(x, ω p, ω c ) as the value of the loss function whenωp ̸= ωc . For each error function we deﬁne\na loss function in the following way\nl(ωp|x, ω c ) =\n(\n0 ωp = ωc\nǫ(x, ω p, ω c ) otherwise (4.9)\nThe error function must verify the following ﬁniteness property,\nX\nωc∈Ω\npr (ωc |x) ǫ(x, ω p, ω c ) < ∞ , (4.10)\nsince the conditional risk deﬁnedR(ωp|x) in Eq. (4.2) must exist.\nThe optimal Bayes’ classiﬁcation rule corresponding to theprevious loss function in Eq. (4.9) is\nc(x) = arg min\nωp∈Ω\nX\nωc ̸=ωp\npr (ωc |x) ǫ(x, ω p, ω c ) . (4.11)\nThe previous classiﬁcation rule in Eq. (4.11), has a great disadvantage. In order to classify an object\nwe have to perform the minimisation which also implies a sum over all classes. If we compare the rules\nin Eq. (4.11), and the rule in Eq. (4.8), it is clear that in theformer case, the cost is higher since the\nsum over all possible correct classes should be performed. This sum is not important if the number of\nclasses is small, however, in several appealing language problems such as statistical machine translation\nor speech recognition the number of classes is huge or even inﬁnite (enumerable). In those cases, the\nsum inside the minimisation could be even unfeasible.\nThe loss functions in Eq. (4.9) and in Eq. (4.7) represent twoextremes of the loss function possibil-\nities. On the one hand, the0–1 loss function yields the simplest and fastest classiﬁcation rule. On the\nother hand, the general loss function in Eq. (4.11), is the most general loss but also the slowest one.\nThere is another category of loss functions which representa trade-off between computational cost\nand generality. This category is characterised by the property of ignoring the proposed classωp in the\n84 JAF-DSIC-UPV\n4.2. Bayes Decision Theory\nerror function. Therefore, if we deﬁne the following loss function,\nl(ωp|x, ω c ) =\n(\n0 ωp = ωc\nǫ(x, ω c) otherwise , (4.12)\nwhere the dependence ofǫ(· · ·) on the proposed classωp is dropped, and, then, the optimal classiﬁca-\ntion rule is given by\nc(x) = arg min\nωp ∈Ω\nX\nωc ̸=ωp\npr (ωc |x) ǫ(x, ω c ) . (4.13)\nApplying some basic arithmetic operations to the classiﬁcation rule in previous Eq. (4.13), the classiﬁ-\ncation rule is signiﬁcantly simpliﬁed, i.e.,\nc(x) = arg min\nωp ∈Ω\n8\n<\n:\nX\nωc ̸=ωp\npr (ωc |x) ǫ(x, ω c )\n9\n=\n; (4.14)\n= arg min\nωp ∈Ω\n{−pr (ωp|x) ǫ(x, ω p) + S(x)} (4.15)\n= arg max\nωp∈Ω\n{pr (ωp|x) ǫ(x, ω p)} (4.16)\nwithS(x) = P\nω∈Ω ǫ(ω, x) pr (ω|x).\nNote that comparing Eqs. (4.16) and (4.8), it is observed that the cost is almost the same, except for\nthe computation ofǫ(x, ω p). Actually, all the constant error functions, i.e.ǫ(x, ω p) = c, lead to the\nsame classiﬁcation rule than the0–1 loss function in Eq. (4.8). Therefore, the0–1 loss function is the\nsimplest error function of this category. If we further compare Eq. (4.16) with Eq. (4.11), it is seen that\nthe former is fastest that the latter, since in the former a sum over all the class domain must be computed\nfor each candidate class in the minimisation search. We classify the loss functions into two categories:\n• The general loss functionscharacterised in Eq. (4.9) that require to scan the set of classes twice:\none to compute the minimisation and another scan in order to compute the sum for each candidate\nin the former minimisation (see Eq. (4.11)).\n• The simpliﬁed loss functionsthat drop the dependence on the proposed class deﬁned as detailed\nin Eq. (4.12), and that only require to scan the set of classesonce for computing the maximisation\n(see Eq. (4.16)).\nAnalysing the Eq. (4.12), the question of which the best error function is, raises immediately. The\nanswer is not easy, and it depends on the task and problem for which we are designing the classiﬁcation\nsystem. For instance, if the number of classes is huge or eveninﬁnite, a good approximation is to use\nthe probability distribution over the classes, i.e.ǫ(x, ω c ) = pr (ωc ). Figure 4.1 plots this idea. Note that\nsince there are classes in the domain with a small probability of occurrence, it is useless to uniformly\ndistribute the loss. For instance, let assume thatωh is the most probable class and thatωl is one of the\nless probable classes. We further assume that for a given objectx, the loss of classifying it in each of\nboth classes, sayωh and ωl, is the same. Ifx belongs toωl, then we could misclassify it by assigning\nit to the classωh and vice-versa. Since the classωh is more probable, the system could fail more times\nthan if the loss of misclassifying an object of the classωh were the highest at the expense of reducing\nthe loss of misclassifying objects belonging to classωl. Note that this fact is independent of the quality\nof the models used to approximate the actual probabilities.This idea is analysed into detail for the\nstatistical machine translation problem in Section 4.3.\nAccording to previous argument, if the loss wereǫ(x, ω c ) = pr (ωc, x) then we should expect\nthat the system would work even better. The difference between the marginal probability and the joint\nJAF-DSIC-UPV 85\nChapter 4. The loss function in statistical pattern recognition\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n1 10 100 1000 10000\n0\n1\npr (ωc)\nΩ\n∞\nǫ(x, ω c)\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n1 10 100 1000 10000\n 0\n 0.02\n 0.04\n 0.06\n 0.08\n 0.1\n 0.12\n 0.14pr (ωc)\nΩ\n∞\nǫ(x, ω c)\nFigure 4.1:Difference of using a0–1 loss function (on the left) and an approximation\nto the true class probability as the loss function (on the right) when using a loss function\nof the sort of Eq. (4.12). The left-scale of the y axis shows a possible actual probability\nover the target sentences. The right-scale of the y axis shows the value of the loss\nfunction when a mistake is made. Finally, the x axis is an inﬁnite enumeration of the\nnumerical identiﬁers corresponding to the inﬁnite enumerable set of possible classes (or\ntarget sentences in the SMT case).\nprobability is that we can modify the loss on the correct class depending on each object. Obviously, this\nreﬁnes the accuracy of the the loss.\nA more general approach can be used for mixing different models and information sources. It\nconsists in deﬁning an additional training step to optimisea parametrised loss function. We start by\ndeﬁning a family of error functions,Υ , and identifying each function in the family with some vector\nof parameters, sayλ . Then, we use another function criterion, sayC(ǫλ (x, ω c )), in order to range\nbetween the classiﬁcation systems. Afterwards, with the help of an optimisation method, either theo-\nretical or practical, the vectorλ is optimised. In practice, this is used to approximate a generic error\nfunctionǫ(x, ω c, ω p) with a faster error function that drops the dependence on theproposed class, i.e.\nǫλ (x, ω c ). In this way, we design a fast classiﬁcation rule, that approximate our real classiﬁcation risk.\nIn order to perform the minimisation, a validation set is typically used. This idea is further explored in\nSection 4.3.3 under the view of statistical machine translation.\n4.3 Statistical Machine Translation\nIn this section, we propose and analyse different loss functions which are eligible for substituting the0–\n1 loss function in pattern recognition problems. Since, thissubstitution is specially appealing when the\nset of classes is inﬁnite, we focus on the real scenario ofstatistical machine translation (SMT) [Brown\net al., 1993].\nIn Chapter 1, we stated the SMT problem as the problem of ﬁnding the translationy for a given\nsource sentencex. SMT is a speciﬁc instance of a classiﬁcation problem where the set of possible\nclasses is the set of all the possible sentences that might bewritten in a target language, i.e.Ω = Y ∗,\nwhere Y is the target lexicon. Likewise, the objects to be classiﬁeda are sentences of a source language,\ni.e.x ∈ X ∗, whereX is the source lexicon.\naIn this context to classify an objectx in the classω c is a way of expressing thatω c is the translation ofx.\n86 JAF-DSIC-UPV\n4.3. Statistical Machine Translation\nRecall from Section 1.3 that, the SMT systems are based on theBayes’ classiﬁcation rule for the\n0–1 loss function depicted in Eq. (4.8). Usually, the class posterior probability is decomposed using\nBayes’ theorem into two probabilities,\nˆy = ˆ c(x) = arg max\ny p ∈Y ∗\n{pr (x|yp) pr (yp)} , (4.17)\nwhich is known as theinverse translation rule (ITR)since the translation probability,pr (x|yp), is\ndeﬁned in an inverse way, i.e., we deﬁne a probability distribution over the source sentencex which\nis the information that is “given” to the system. On the otherhand, a direct model distributes the\nprobability among the target sentencesyp conditionally to the given informationx,pr (yp|x). Note\nthat we usedyp instead ofy to highlight the fact thatyp plays the role of the proposed translation in\nthe deﬁnition of the loss function.\nEquation (4.17) implies that the system has to search the target stringˆy that maximises the product\nof both, the target language modelpr (y) and the inverse translation modelpr (x|y). Nevertheless,\nusing this rule implies, in practice, changing the distribution probabilities as well as the models through\nwhich the probabilities are approached. This is exactly theadvantage of this approach, as it allows the\nmodelling of the direct translation probabilitypr (y|x) with two models: an inverse translation model\nthat approximates the direct probability distributionpr (x|y); and a language model that approximates\nthe language probabilitypr (y).\nThis approach has a strong practical drawback: the search problem. This search is known to be\nan NP-hard problem [Knight, 1999, Udupa and Maji, 2006]. However, several approximate search\nalgorithms have been proposed in the literature to solve this problem efﬁciently [Al-Onaizan et al.,\n1999, Brown et al., 1990, García-Varea and Casacuberta, 2001, Germann et al., 2001, Jelinek, 1969,\nTillmann and Ney, 2003, Wang and Waibel, 1997].\nAnother drawback of the ITR, is that it is obtained using the0–1 loss function. As stated in Sec-\ntion 4.2, this loss function is not particularly appropriate when the number of classes is huge as it\nhappens in SMT problems. Speciﬁcally, if the correct translation for the source sentencex isyc, and\nthe hypothesis of the translation system isyp; then using the0-1 loss function (Eq. (4.7)) has the\nconsequence of penalising the system in the same way, independently of which translation the system\nproposesyp and which the correct translationyc is.\n4.3.1 General error functions\nAs stated above, the most generic loss functions depicted inEq (4.9), produce minimisations which\nrequire the computation of a sum over all the set of classes. Machine translation is a classiﬁcation\nproblem with a huge set of classes. Hence, the most generic loss functions yield difﬁcult search al-\ngorithms which are approximated. There are some works that have already explored this kind of loss\nfunctions [R. Schlüter and Ney, 2005, Uefﬁng and Ney, 2004].\nThe more appealing application of this loss functions is theuse of a metric loss function [R. Schlüter\nand Ney, 2005]. For instance, in machine translation one widespread metric is the WER (see Sec-\ntion 1.3 for a deﬁnition), since the loss function in Equation (4.12) depends on both, the proposed trans-\nlation and the reference translation, the WER can be used as loss function [Uefﬁng and Ney, 2004].\nNevertheless, due to the high complexity, the use of these general loss functions, is only feasible in\nconstrained situations liken-best lists [Kumar and Byrne, 2004].\n4.3.2 Simpliﬁed error functions\nThe search algorithms generated by the classiﬁcation rule in Eq. (4.12) have the same asymptotic cost\nthan0–1 loss function, at the expense of dropping the dependence on the proposed class. As stated in\nJAF-DSIC-UPV 87\nChapter 4. The loss function in statistical pattern recognition\nSection 4.2, a more suitable loss function than the0–1 loss, is obtained using as the error function the\ntarget sentence probability,ǫ(x, yc ) = pr (yc ),\nl(yp|x, yc ) =\n(\n0 yp = yc\npr (yc ) otherwise (4.18)\nThis loss function seems to be more appropriate than the0-1. This is due to the fact that if the system\nmisclassiﬁes some sentences of a given test set, this loss function tries to force the system to fail in the\nsource sentencex of which correct translationbyc is one of the least probable in the target language.\nThus, the system will fail in the least probable translations, whenever it gets confused; and therefore,\ntheGlobal Riskwill be reduced.\nThe associated Bayes’ rule for loss function in Eq. (4.18) is\nˆy(x) = arg max\ny p ∈Y ∗\n˘\npr (yp|x)pr (yp )\n¯\n. (4.19)\nNote that we usedyp instead ofy to highlight the fact thatyp plays the role of the proposed translation\nin the deﬁnition of the loss function in Eq. (4.18).\nPrevious Eq. (4.19) is known as thedirect translation rule (DTR)since the translation probability,\npr (yp|x), is deﬁned in an direct way. The direct translation rule was heuristically introduced into\nthe scope of machine translation in order to alleviate the search problem by many of the current SMT\nsystems [Koehn et al., 2003, Och and Ney, 2004a, Och et al., 1999, Zens, 2008]. Note that the DTR\nwas introduced as a heuristic version of the ITR in Eq. (4.17), wherepr (x|y) is substituted bypr (y|x).\nThis rule allows an easier search algorithm for some of the translation models. Although the DTR has\nbeen widely used, its statistical theoretical foundation has not been clear for long time, as it seemed\nto be against the Bayes’ classiﬁcation rule. As stated above, the direct translation rule is the Bayes’\noptimal classiﬁcation ruleif the loss function in Eq.(4.18)is used[Andrés-Ferrer et al., 2008].\nSince the DTR uses the target language probability as the error function, it should work better than\nthe ITR, from a theoretical point of view. Nevertheless, thestatistical models used for approximate the\ntranslation probabilities may not be good enough. Thus, themodelling error, which is the error made\nwhen approximating the actual probability with a model, could be more important than the advantage\nobtained from the use of a more appropriate loss function. Therefore, it seems a good idea to use\nthe direct rule in the equivalent inverse form so that the translation system will be the same and then\nthese asymmetries will be reduced. By simply applying the Bayes’ theorem to Eq. (4.19), we obtain a\nequivalent rule\nˆy = arg max\ny∈Y ∗\n˘\npr (y)2p(x|y)\n¯\n. (4.20)\nTheoretically, rules in Eq. (4.19) and Eq (4.20) are equivalent and must give the same solution. There-\nfore, the difference between the Eq. (4.19) and Eq (4.20) measure the asymmetries of the translation\nmodels as well as the error in the modelling. Bare in mind thatthe language models also presents some\nmodelling errors and, hence, this last approach assumes that the language model is a very good approx-\nimation to the actual probability distribution, due to the fact that the “direct weight” has passed from\nthe direct translation modelpr (y|x) to the language model. Whether the direct model or the inverse\nmodel is better for the translation task depends on the modelproperties, the estimation technique and\nthe training data.\nbHere lies the importance of distinguishing between the translation proposed by the systemyp and the correct\ntranslationyc for a given source sentencex.\n88 JAF-DSIC-UPV\n4.3. Statistical Machine Translation\nAs stated in Section 4.2 a reﬁned loss function is designed using the joint probability as the error\nfunction,ǫ(x, yj ) = pr (x, yj ),\nl(yp|x, yc) =\n(\n0 yp = yc\npr (x, yc ) otherwise (4.21)\nwhich leads to\nˆy = arg max\ny∈Y ∗\n{pr (x, y) pr (y|x)} . (4.22)\nDepending on how we model probabilities in Eq. (4.22), several optimal classiﬁcation rules are\nobtained. Specially if the joint probability (p(x, y)) is modelled with an inverse translation probability\nplus a target language probability, then, theinverse and direct translation rule (I&DTR), is obtained\nˆy = arg max\ny∈Y ∗\n{pr (y) pr (x |y) pr (y |x)} . (4.23)\nThe interpretation of this rule is a reﬁnement of the direct translation rule. In this case, if the system\nmakes a mistake, then it tends to be done in the least probablepairs (x, y) in terms ofpr (y, x).\n4.3.3 Approximation to general error functions\nAs stated in Section 4.2, the loss functions of the kind in Eq.(4.12), are usually faster than the general\nloss functions depicted in Eq. (4.9) since the former scans the possible translations once and the latter\ntwice (one for the minimisation and another for the sum). Theloss function in Eq. (4.12) sacriﬁces the\nproposed translation in order to speed up the search process. Unfortunately, the automatic evaluation\nmetrics used to quantify the translation systems require both, the proposed and the correct translation.\nTherefore, with the fastest loss functions we are not able tominimise the evaluation metrics, which\nin principle is what we expect from our best system. However,by deﬁning a family of simple error\nfunctions depending on a parametric vector, sayλ , we are able to approximate the evaluation metric,\nsuch as the BLEU or WER.\nOne way to approximate this general error function is to use aset of features,fk (x, yc ), that\ndepend on both the source sentence and its correct target translation. Then we deﬁne the following\nerror function\nǫλ (x, yc ) =\nKY\nk=1\nfk (x, yc )λk . (4.24)\nIf our actual evaluation error function is\nǫ(x, yp, yc ) = 1 − BLEU(yp , yc ) , (4.25)\nthen using a validation setD = {(xn , yn )}N\nn=1 we can use any optimisation algorithm to minimise our\nactual error function in Eq. (4.25). For instance, the maximum entropy algorithm [Berger et al., 1996]\nis typically applied to ﬁnd the optimal parameter vectorλ .\nThe error function deﬁned in Eq. (4.24) leads to the following classiﬁcation rule\nˆyλ (x) = arg max\ny c ∈Y ∗\npr (yc |x)\nKY\nk=1\nfk (x, yc )λk . (4.26)\nIf we extend the feature vector,f , by adding the conditional probability,pr (yc |x) as a new feature with\na new parameterc λK+1; then the classiﬁcation rule expressed in terms of the extended feature vector,\ncIn the case that there existed a feature, sayfl(·), which already is the conditional probability, then the new\nfeature vector remains the same and the new parameter vectoris the previous one but for the componentl which is\nincrease by one, i.e.¯λ l = λ l + 1 .\nJAF-DSIC-UPV 89\nChapter 4. The loss function in statistical pattern recognition\n¯f and the extended parametric vector¯λ is\nˆy ¯λ (x) = arg max\ny c ∈Y ∗\nKY\nk=1\n¯fk (x, yc )\n¯λk . (4.27)\nIf we apply the logarithm to the previous Eq. (4.27) we obtainthe equivalent expression\nˆy ¯λ (x) = arg max\ny c ∈Y ∗\nKX\nk=1\n¯λk log ¯fk (x, yc ) . (4.28)\nIf we deﬁne¯h = log ¯f then the classiﬁcation rule in Eq. (4.28) is expressed as follows\nˆy ¯λ (x) = arg max\ny c ∈Y ∗\nKX\nk=1\n¯λk ¯hk (x, yc ) , (4.29)\nwhere ¯λ stands for the extended extended parameter vector as in Eq. (4.27).\nMost of the state-of-the-art systems use this idea, although they present it as if they were using a\nlog-linear model [Mariño et al., 2006, Och and Ney, 2004a]. Speciﬁcally, if in Eq. (4.8) we model the\ndirect probability as a log linear model\npλ (y|x) = 1\nZλ (x) exp(\nKX\nk=1\nλk hk (x, y)) , (4.30)\nwith\nZλ (x) =\nX\ny ∈Y ∗\nexp(\nKX\nk=1\nλk hk (x, y)) (4.31)\nthen using the model in Eq. (4.30) in the rule in Eq. (4.8), we obtain the following rule\nˆyλ (x) = arg max\ny ∈Y ∗\n1\nZλ (x) exp(\nKX\nk=1\nλk hk (x, y)) (4.32)\n= arg max\ny ∈Y ∗\nexp(\nKX\nk=1\nλkhk (x, y)) (4.33)\n= arg max\ny ∈Y ∗\nKX\nk=1\nλk hk (x, y) . (4.34)\nNote that if you compare Eqs. (4.34) and (4.29), they are equivalent.\nAlthough the log-linear explanation of the process yields the same classiﬁcation rule, it is not sat-\nisfactory in the sense that the log-linear model in Eq. (4.30) is never trained in its full form and, its\nnormalisation weightZλ (x) is ignored. This ellipsis can be done in the decoding process butcannot\nbe done in training. In other words, the log-linear model in Eq. (4.30) is only trained in the form of\nclassiﬁcation rule (4.34) so that it minimises the general loss function in Eq. (4.25) by using the loss\nfunction in Eq. (4.12) with the error function in Eq. (4.24).Therefore, the state-of-art log-linear models\nare alog-lineal loss functiontrained to resemble the general loss function in Eq. (4.25).\nTypical features used by the state-of-the-art systems range among [Mariño et al., 2006, Och and\nNey, 2004a] the followings:\n90 JAF-DSIC-UPV\n4.3. Statistical Machine Translation\n• Direct translation models:a typical feature is to use a direct translation model\nhk (x, y) = log pθ (y|x), f k (x, y) = pθ (y|x) . (4.35)\nThe most used models are the IBM model 1 and the phrase-based models.\n• Inverse translation models:a typical feature is to use a inverse translation model\nhk (x, y) = log pθ (x|y), f k (x, y) = pθ (x|y) . (4.36)\nThe most used models are the IBM model 1 and the phrase-based models.\n• Joint translation models:a typical feature is to use a stochastic ﬁnite transducer,\nhk (x, y) = log pθ (x, y), f k (x, y) = pθ (x, y) . (4.37)\n• An n-gram language model:that is to say\nhk (x, y) = log pθ (y), f k (x, y) = pθ (y) . (4.38)\n• Word bonus:it is a well know problem of then-gram language models that they give more prob-\nability to short sentences. Additionally, the translationmodels tend to distribute the probability\namong ill-formed sentences as the length of the sentence increases [Brown et al., 1993]. There-\nfore, in order to keep the translation systems from always producing poor translations because\nof trying to shorten them, the following feature is used\nhk (x, y) = log exp( |y|), f k (x, y) = exp( |y|) . (4.39)\n4.3.4 Experiments\nThe aim of this Section is to show experimentally how the theory stated in this work can be used to\nimprove the performance of a translation system. Therefore, the objective is not to obtain a competitive\nsystem, but rather to analyse the previously stated properties in practice.\nIn order to analyse the theory, we have used two set of experiments. For the former set we used\na semi-synthetic corpora and a simple translation model, the IBM model II (see Section 1.3.1). For\nthe latter, two real tasks are used whilst the translation models used were the phrase-based models\n(see Section 1.3.2). Through both experiments an-gram language model is used to approximate the\nlanguage probability distributions, i.e.pr (y). Speciﬁcally, the language model was trained using a\n5-gram model obtained with the SRILM toolkit [Stolcke, 2002].\nSimilarly to Germann et al. [2001], we deﬁned two error measures:search errorand model error.\nThese error measures are inspired on the idea that when a SMT system proposes a wrong translation, it is\ndue to of one of the following reasons: either the suboptimalsearch algorithm has not been able to ﬁnd a\ngood translation or the model is not able to make up a good translation, and hence it is impossible to ﬁnd\nit. A translation error is asearch error (SE)if the probability of the proposed translations is less than\na reference translation; otherwise it is amodel error, i.e., the probability of the proposed translations\nis greater than the reference translation. Although a modelerror always has more probability than the\nreference translation, this does not excludes the fact thata much better translation maybe found.\nIn order to evaluate the translation quality, we used the following well-known automatically com-\nputable measures:word error rate (WER),bilingual evaluation understudy (BLEU),position indepen-\ndent error rate (PER), andsentence error rate (SER).\nJAF-DSIC-UPV 91\nChapter 4. The loss function in statistical pattern recognition\n4.3.5 Corpora\nThree different corpora were used for the experiments that were carried out in this chapter: Eutrans-I\n(Tourist), Europarl and Xerox.\nTable 4.1 summarises some of the statistics of the Tourist corpus [Amengual et al., 1996]. The\nSpanish-English sentence pairs correspond to human-to-human communication situations at the front-\ndesk of a hotel which were semi-automatically produced using a small seed corpus compiled from travel\nguides booklets.\nTable 4.1:Basic statistics of the Spanish-English TOURIST task.\nSpanish English\nTraining\nSentences 170K\nRunning Words 2.2K 2.2M\nV ocabulary 688 540\nAvg. sentence length 12.9 13.0\nTest\nSentences 1K\nRunning Words 12.7K 12.6K\nPerplexity 3.6 2.9\nTable 4.2 shows some statistics of the Europarl corpus [Koehn, 2005]. Speciﬁcally, this is the\nversion that was used in the shared task of the NAACL 2006 Workshop on SMT [NAACL 2006].\nEuroparl corpus is extracted from the proceedings of the European Parliament, which are written in\nthe different languages of the European Union. There are different versions of the Europarl corpus\ndepending on the pair of languages that are used. In this work, only the English-Spanish version was\nused. As can be observed in Table 4.2, the Europarl corpus contains a great number of sentences\nand large vocabulary sizes. These features are common to other well-known corpora described in the\nliterature.\nTable 4.2:Statistics of the Europarl corpus\nSpanish English\nTraining\nSentences 730K\nRunning Words 15.7M 15.2M\nV ocabulary 102.9K 64.1K\nAvg. sentence length 21.5 20.8\nTest\nSentences 3.1K\nRunning Words 91.7K 85.2K\nPerplexity 102 120\nTable 4.3 reports some statistics of the Xerox corpus [Atos Origin, Instituto Tecnológico de Infor-\nmática, RWTH Aachen, RALI Laboratory, Celer Soluciones andSociété Gamma and Xerox Research\nCentre Europe, 2001]. This corpus involves the translationof technical Xerox Manuals from English to\nSpanish, French and German, and vice-versa. In this work, only the English-Spanish version was used.\nAs can be observed in Table 4.3, the Xerox corpus contains a considerable number of sentences and\nmedium-size vocabularies.\n92 JAF-DSIC-UPV\n4.3. Statistical Machine Translation\nTable 4.3:Statistics of the Xerox corpus\nSpanish English\nTraining\nSentences 55.7K\nRunning Words 0.75M 0.67M\nV ocabulary 11.0K 8.0K\nAvg. sentence length 13.5 11.9\nTest\nSentences 1.1K\nRunning Words 10.1K 8.4K\nPerplexity 35 47\nWord Based Translation experiments\nIn this section, the IBM Model2 [Brown et al., 1993] is used to approximate the translation probability\ndistributions. Together with the IBM Model2 [Brown et al., 1993], its corresponding search algorithms\nare used to carry out the experiments in this Section. This choice was motivated by several reason.\nFirstly, the simplicity of the translation model allows us to obtain a good estimation of the model\nparameters. Secondly, there are several models that are initialised using the alignments and dictionaries\nof the IBM model 2, for instance, the IBM HMM [Och et al., 1999]or the phrase-based models can\nbe initialised by the IBM model 2. Finally, the search problem can be solved exactly using dynamic\nprogramming for the case of the direct translation rule depicted in Eq. (4.19).\nIn order to train the IBM Model2, we used the standard toolGIZA ++ [Och, 2000]. We re-\nimplemented the algorithm presented in [García-Varea and Casacuberta, 2001] to perform the search\nprocess for the ITR. Even though this search algorithm is notoptimal, we conﬁgured the search parame-\nters in order to minimise the search errors, so that most of the errors should be model errors. In addition,\nwe implemented the corresponding version of this algorithmfor the DTR and for the I&DTR. All these\nalgorithms were developed by dynamic programming. For the I&DTR, we implemented two versions\nof the search: one guided by the direct model (a non-optimal search algorithm, namely I&DTR-D) and\nthe other guided by the inverse translation model (which is also non-optimal but more accurate, namely\nI&DTR-I).\nIn order to have an experimentation as close as possible to a theoretical scenario, we selected the\nSpanish-English TOURIST task (see Section 4.3.5). The parallel corpus consisted of171, 352 differ-\nent sentence pairs, where1K sentences were randomly selected from testing, and the rest (in sets of\nexponentially increasing sizes:1K, 2K, 4K, 8K, 16K, 32K, 64K, 128K and 170K sentences pairs) for\ntraining. All the ﬁgures show the conﬁdence interval at 95%.\nFigure 4.2 shows the differences in terms of the WER among allthe mentioned forms of the DTR:\n“IFDTR” (Eq. 4.20), and “DTR” (Eq. 4.19). Since the IBM Model2 (in its direct version) tries to pro-\nvide very short translations, we implemented a normalised length version of the DTR. In the ﬁgure this\nnormalised version is referred “DTR-N”. Note the importance of the model asymmetry in the obtained\nresults. The best results were the ones obtained using the inverse form of the DTR. This behaviour is\nnot surprising, since the only mechanism that the IBM Model 2has to ensure that all sources words are\ntranslated is a length distribution that usually allows themodel to ommit the translation of a few words.\nAnyway, the “DTR” and “DTR-N” performed worse than the ITR (Table 4.4).\nFigure 4.3 shows the results achieved with search algorithms base on the most important classi-\nﬁcation rules. All the I&DTR obtain similar results to the ITR. Nevertheless, the non-optimal search\nalgorithm guided by the direct model (“I&DTR-D”) was an order of magnitude faster than the more\naccurate one (“I&DTR-I”) and the “ITR”. The inverse form of the DTR (“IFDTR”) behaved similarly\nJAF-DSIC-UPV 93\nChapter 4. The loss function in statistical pattern recognition\n 5\n 10\n 15\n 20\n 25\n 30\n 35\n 40\n 128000 64000 32000 16000 8000 4000 2000 1000\nWER\nTraining Size\nIFDTR\nDTR\nDTR-N\nFigure 4.2:Asymmetry of the IBM Model 2 measured with the respect to the WER for\nthe TOURIST test set for different training sizes.\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 128000 64000 32000 16000 8000 4000 2000 1000\nWER\nTraining Size\nIFDTR\nI&DTR-D\nITR\nI&DTR-I\nFigure 4.3:WER results for the TOURIST test set for different training sizes and\ndifferent classiﬁcation rules.\nto these, signiﬁcantly improving the results reported by DTR. There are no signiﬁcant differences be-\ntween the rules analysed in terms of WER. However, the execution times were signiﬁcantly reduced by\nthe direct guided search in comparison with the other searches. Table 4.4 shows these execution times\nand the ﬁgures with the maximum training size.\nThe different search algorithms (based on loss functions) do not convey a signiﬁcant improvement\n94 JAF-DSIC-UPV\n4.3. Statistical Machine Translation\nTable 4.4:Translation quality results with different translation rules for TOURIST test\nset for a training set of170K sentences. Where T is the time expressed in seconds and\nSE stands for the percentage ofsearch errors.\nModel WER SER BLEU (%) SE (%) T\nI&DTR I 10.0 49.2 84.7 1.3 34\nI&DTR D 10.6 51.6 84.4 9.7 2\nIFDTR 10.5 60.0 83.7 2.7 35\nITR 10.7 58.1 84.3 1.9 43\nDTR N 17.9 74.1 75.0 0.0 2\nDTR 30.3 92.4 53.5 0.0 2\nin WER in Figure 4.3. Note that the loss function only evaluates the SER, i.e. the loss function min-\nimises the SER, and does not try to minimise the WER. Thus, changing the loss function, does not\nnecessarily decrease the WER.\nIn order to check this hypothesis, Figure 4.4 shows the analogous version of Figure 4.3 but with\nSER instead of WER. It should be noted that as the training size increases, there is a difference in the\nbehaviour between the ITR and both I&DTR. Consequently, theuse of these rules provides better SER,\nand this difference becomes statistically signiﬁcant as the estimation of the parameters improve. In\nthe case of the inverse form of the DTR (“IFDTR”), as the training size increases, the error tends to\ndecrease and approximate the ITR error. However, the differences are not statistically signiﬁcant and\nboth methods are equivalent from this point of view.\n 40\n 50\n 60\n 70\n 80\n 90\n 128000 64000 32000 16000 8000 4000 2000 1000\nSER\nTraining Size\nIFDTR\nI&DTR-D\nDTR-N\nITR\nI&DTR-I\nFigure 4.4:SER results for the TOURIST test set for different training sizes and differ-\nent classiﬁcation rules.\nIn conclusion, there are two sets of rules: the ﬁrst set is made up of IFDTR and ITR, and the second\nJAF-DSIC-UPV 95\nChapter 4. The loss function in statistical pattern recognition\nTable 4.5:The results of translation quality obtained using the proposed variety of loss\nfunctions with the Europarl test set.\nSpanish→ English\nRule Formula BLEU (%) WER PER\nITR pr (x|y)pr (y) 26.8 61.1 45.2\nDTR pr (y|x)pr (y) 20.6 61.1 48.9\nI&DTR pr (y|x)pr (x|y)pr (y) 28.1 59.0 43.3\nIFDTR pr (x|y)[pr (y)]2 22.2 62.5 48.3\nEnglish→ Spanish\nRule Formula BLEU WER PER\nITR pr (x|y)pr (y) 25.7 60.7 45.8\nDTR pr (y|x)pr (y) 19.9 62.0 51.3\nI&DTR pr (y|x)pr (x|y)pr (y) 26.0 59.4 45.1\nIFDTR pr (x|y)[pr (y)]2 21.5 62.7 49.4\nis composed by the two versions of the I&DTR. The ﬁrst set reports worse SER than the the second\nset. However, the I&DTR guided with the direct model (“I&DTR-D”) has many good properties in\npractice. Note that for real tasks and state-of-the-art systems, it is expected that the behaviour of the\nrules correspond to the result obtained with the smallest corpus size, where no signiﬁcant difference\nexists among the systems in terms of SER.\nPhrase-based translation experiments\nIn the case of phrase-based translation (PBT) different models (for the two tasks considered) were\nestimated. The training of these models were carried out in the following way:\n• First, a word-level alignment of all the sentence pairs in the training corpus was carried out. This\nalignment was performed for the Spanish-to-English and English-to-Spanish directions, using a\nstandard GIZA++ [Och, 2000] training, with the standard training scheme15253145.\n• Then, a symmetrisation of both alignment matrices was built, using the THOT toolkit [Ortiz et al.,\n2005]. Speciﬁcally, the reﬁned symmetrisation method was used [Och and Ney, 2004b].\n• Finally, a phrase-based model was estimated, using the THOT toolkit [Ortiz et al., 2005].\nWith respect to the decoding process, we implemented our ownphrase-based decoder. Speciﬁcally,\nthe decoder implements anA⋆ algorithm which is very similar to that described in the literature [Ger-\nmann et al., 2001, Ortiz et al., 2003] for single-word models. The decoder was adapted to deal with the\ndifferent translation rules (or equivalently, the different loss functions) proposed here. These decoders\nverbatim the unknown words to the output, since our model is not ﬁne-grained and its basic units are\nwords.\nTables 4.5 and 4.6 show the translation quality measures forthe Europarl and Xerox tasks, respec-\ntively, for the different loss functions proposed in Section 4.2. The DTR and FIRTD behaves similarly.\nAs expected, the D&ITR obtains the best performance. The differences between the FIRTD and the\nDTR (which are theoretically equivalent) are not too great,so the under-performance of the DTR com-\npared with the ITR is not due to model asymmetries. If the translations given by the DTR are compared\n96 JAF-DSIC-UPV\n4.3. Statistical Machine Translation\nTable 4.6:Translation quality results obtained, using the proposed variety of loss func-\ntions, with the test set of Xerox task.\nSpanish→ English\nRule Rule (Search Alg.) BLEU (%) WER PER\nITR pr (x|y)pr (y) 61.7 25.9 17.6\nDTR pr (y|x)pr (y) 59.0 27.0 18.9\nI&DTR pr (y|x)pr (x|y)pr (y) 61.6 25.9 17.5\nIFDTR pr (x|y)[pr (y)]2 60.6 26.2 18.0\nEnglish→ Spanish\nRule Rule (Search Alg.) BLEU WER PER\nITR pr (x|y)pr (y) 63.6 25.6 18.5\nDTR pr (y|x)pr (y) 62.8 26.0 19.1\nI&DTR pr (y|x)pr (x|y)pr (y) 64.6 25.1 18.1\nIFDTR pr (x|y)[pr (y)]2 62.8 26.2 19.0\nwith the ITR, it can be observed that the DTR tends to generateshorter translations. This result is ex-\npected since the error function of the DTR,pr (y), is modelled using an-gram language model, and it\nis well-known thatn-gram language models give more probability to short sentences, that is to say, the\nresulting systems tends to shorten translations.\nTables 4.5 and 4.6 show that the theoretically expected increase of the translation performance in\nterms of WER and BLEU, is apparently not achieved for the DTR and both corpora. Although in the\nXerox corpus the improved performance for the DTR is achieved, the differences between the systems\nare not very high. However, ﬁgures 4.5 and 4.6 show that, in fact, the DTR rule outperforms the ITR, but\nalso provides shorter translations. Note that the longer the sentences are the worse thebrevity penalty\n(BP) of the BLEU scoreis and consequently the worse the BLEU is (Fig. 4.6). Note that in Fig. 4.5,\nthe DTR incurs in a WER which is in all cases smaller than the WER performed by ITR. Again this\nis due to then-gram model which is used to model the language model, i.e. the error function of the\nDTR. The I&DTR had the same brevity penalty problem, however, in this case the problem was not so\nimportant since the rule includes the inverse translation model, which counteracts the problem.\nTable 4.7 shows some translations obtained using both DTR and ITR. As can be seen, DTR tends to\nproduce shorter translations than ITR, which typically produces more translation errors. For instance,\nin the ﬁrst sentence,the European agencyis translated asthe agencyby the DTR; this is due to the fact\nthat although the ﬁrst translation is more precise, the language model (the loss function for the DTR)\nscores the second as a more probable sentence. Oppositely, the DTR correctly translatesmust in the\nﬁrst sentence but the ITR translates it asshould. Most of the common mistakes shared for both rules\nare syntactic errors, although semantic errors can be found, as well.\nIn conclusion, the DTR and I&DTR, obtain better results withshort sentences due to a bias in the\nlanguage model, although the precision of such sentences isbetter. Nevertheless, the I&DTR is not\ndramatically affected by an increase in the sentence length. As future work, we intend to solve the\nlanguage model bias to short sentence in some way, perhaps byintroducing a length normalisation in\nthe loss function or in the models.\nJAF-DSIC-UPV 97\nChapter 4. The loss function in statistical pattern recognition\n 50\n 55\n 60\n 65\n 70\n 75\n 0  5  10  15  20  25  30  35  40\nWord Error Rate (WER)\nITR\nDTR\nI&DTR\nLength\nFigure 4.5:The WER results obtained for the Europarl test set (Spanish to English)\nwith the length of the reference sentences restricted to be less than the value of the\nx-axis.\nTable 4.7:Differences between some translation examples obtained using DTR and\nITR. Bold words highlight the differences between the two proposed translations. REF\nstands for the reference translation.\nSRC: en segundo lugar , la agencia europea debe ser completamenteindependiente .\nREF: secondly , the European agency must be completely independent\nDTR: secondly , theagency mustbe totallyindependent\nITR: secondly , theEuropean agency shouldbe completelyindependent .\nSRC: es crucial que los consumidores acepten el euro .\nREF: it is crucial for consumers to accept the euro .\nDTR: it is crucial that consumersto acceptthe euro .\nITR: it is crucial that consumersacceptance ofthe euro .\nSRC: de modo que me siento reacio a ir más lejos en materia de comercio o a aconsejar\nmedidas suplementarias en materia de comercio o inversión .\nREF: so i am reluctant to go further on trade or to advise further action on trade or investment .\nDTR: i am reluctant to go further trade or advisefurther stepstrade or investment .\nITR: i am reluctant to go furtheron trade ortoadviseadditional efforts ontrade or investment .\n4.4 Conclusions\nThe analysis of the loss function is an appealing issue. The results of analysing different loss functions\nrange from allowing to use metric loss functions such as BLEU, or WER; to proving the properties\n98 JAF-DSIC-UPV\n4.4. Conclusions\n 0.2\n 0.25\n 0.3\n 0.35\n 0.4\n 0.45\n 0.5\n 0  5  10  15  20  25  30  35  40\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n 1\nBiLingual Evaluation Understanding (BLEU )\nBrevity Penalty(BP)\nLength\nITR (BLEU )\nI&DTR (B LEU )\nDTR (B LEU )\nDTR (BP)\nITR (BP)\nI&DTR (BP)\nFigure 4.6:The BLEU results (on the left y-scale) and the brevity penalty (BP) of\nBLEU score (on the right y-scale) obtained for the Europarl test set (Spanish to English)\nwith the length of the reference sentences restricted to be less than the value of thex-\naxis.\nof some outstanding classiﬁcation rules such as the direct translation rule, the inverse translation rule\nor even the maximum entropy rule. For each different error functionǫ(x, yj , yk ) in the general loss\nfunction of Eq. (4.9), there is a different optimal Bayes’ rule. The point of using one speciﬁc rule is an\nheuristic and practical issue.\nAn interesting focus of study is the use of metrics such as BLEU, or WER; as the loss function.\nNevertheless due to the high complexity, it is only feasibleon constrained situations liken-best lists.\nThe work developed in this chapter is focused on the study of loss functions that have a linear\ncomplexity and that are outstanding due to historical or practical reasons. This work explores the direct\ntranslation rule, the inverse translation rule, and the direct and inverse translation rule. In this sense,\nwe have provided a theoretical approach based on decision theory which explains the differences and\nresemblances between the Direct and the Inverse Translation rules. We have also given insights into\nthe practical differences of these two rules, which are widely used. For instance, this theoretical frame\npredicts an improvement (in terms of SER), an improvement that has been conﬁrmed in practice for\nsimple words models. In conclusion, according to the experimental results, the DTR outperforms the\nITR when short sentences are provided to the system.\nThe proposed modiﬁcations to the0–1 loss function depicted in Eq. (4.12) can handle the intuitive\nidea of penalising a wrong action based on the repercussionsof the correct action. For instance, if the\ncorrect translation,yc , of a source sentence,x, is a very unlikely sentence, failure in the translation of\nsuch a sentence is not important. Oppositely, failure in thetranslation of a likely sentence is an important\nmistake. It is important to note the fact that the proposed loss functions cannot handle signiﬁcant cases.\nFor example, it is not the same to make an incorrect translation due to grammar errors than to make\nJAF-DSIC-UPV 99\nChapter 4. The loss function in statistical pattern recognition\n 0\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\n 3.5\n 4\n 4.5\n 5\n 0  0.2  0.4  0.6  0.8  1\nLoss\n−log(1−pr(x, y))\npr (x, y)\npr (x, y)\nFigure 4.7:Difference between the remaining information and the probability as error\nfunctions.\nan incorrect translation due to semantic errors. In order totake into account such cases, it is necessary\nto work with general loss functions of the sort in Eq. (4.9) despite of its cost. However, the idea of\npenalising the mistakes proportionally to the probabilityof the correct translation can also be used in\ncase of dealing with more complicated decision rules and, eventually, with more complicated search\nalgorithms.\nNote that though we have focused our analysis to error functions which are a probability distribu-\ntion, the error functionǫ(·) does not necessary have to be a probability distribution. This idea brings\nup the question of which the best loss function is. For instance, a conﬁdence measure could even be\nused to deﬁne error functions. Maybe the growing of the loss function should better be non-lineal\nwith the probability. In this sense more interesting loss functions could be obtained using information\ntheory. For instance, we can penalise the system with theremaining information. That is, if we know\npr (x, y), then the information associated with a target sentenceyc is− log(pr (x, yc )). The remaining\ninformation, or the information that the system has learnt when it fails is given by\n− log(\nX\n(x′,y ′)̸=(x,y c )\npr (x′, y′)) = − log(1 − pr (x, yc )) ,\nleading to the the error function\nǫ(x, yc ) = − log(1 − p(x, yc )) . (4.40)\nFigure 4.7, shows the remaining information of a probability function. Note that the remaining informa-\ntion has a singularity at1, i.e. if the system has not been able to learn a sure event, which has probability\nof 1, then the loss is inﬁnity. Note that this loss can be deﬁned for any probability such aspr (y) or\npr (x, y).\nAnother very interesting research line is derived from approximating complex loss functions in\nEq. (4.9) with simple loss functions in Eq. (4.12). Although, many of the state-of-art SMT systems\n100 JAF-DSIC-UPV\n4.4. Conclusions\nindirectly make use of this idea, as analysed in Section 4.2 (page 90), this idea may be exploited from\nthe point of view presented in this chapter.\nJAF-DSIC-UPV 101\n\nBibliography\nBibliography\nY . Al-Onaizan et al. Statistical Machine Translation:\nFinal Report. Technical report, Johns Hopkins Uni-\nversity 1999 Summer Workshop on Language Engi-\nneering, Center for Language and Speech Processing,\nBaltimore, MD, USA, 1999.\nJ.C. Amengual, J.M. Benedí, M.A. CastaÃ´so, A. Marzal,\nF. Prat, E. Vidal, J.M. Vilar, C. Delogu, A. di Carlo,\nH. Ney, and S. V ogel. Deﬁnition of a machine transla-\ntion task and generation of corpora. Technical report\nd4, Instituto Tecnológico de Informática, September\n1996. ESPRIT, EuTrans IT-LTR-OS-20268.\nJ. Andrés-Ferrer, D. Ortiz-Martínez, I. García-Varea, and\nF. Casacuberta. On the use of different loss functions\nin statistical pattern recognition applied to machine\ntranslation.Pattern Recognition Letters, 29(8):1072–\n1181, 2008.\nAtos Origin, Instituto Tecnológico de Informática,\nRWTH Aachen, RALI Laboratory, Celer Soluciones\nand Société Gamma and Xerox Research Centre Eu-\nrope. TransType2 - Computer Assisted Translation.\nProject Technical Annex., 2001.\nAdam L. Berger, Vincent J. Della Pietra, and Stephen\nA. Della Pietra. A maximum entropy approach to\nnatural language processing.Comput. Linguist., 22\n(1):39–71, 1996. ISSN 0891-2017.\nPeter F. Brown, John Cocke, Stephen Della Pietra, Vin-\ncent J. Della Pietra, Frederick Jelinek, John D. Laf-\nferty, Robert L. Mercer, and Paul S. Rossin. A statis-\ntical approach to machine translation.Computational\nLinguistics, 16(2):79–85, 1990.\nPeter F. Brown, Stephen Della Pietra, Vincent J. Della\nPietra, and Robert L. Mercer. The mathematics of\nstatistical machine translation: Parameter estimation.\nComputational Linguistics, 19(2):263–312, 1993.\nRichard O. Duda, Peter E. Hart, and David G. Stork.Pat-\ntern Classiﬁcation. John Wiley and Sons, New York,\nNY , 2nd edition, 2001.\nI. García-Varea and F. Casacuberta. Search algorithms\nfor statistical machine translation based on dynamic\nprogramming and pruning techniques. InProc. of\nMT Summit VIII, pages 115–120, Santiago de Com-\npostela, Spain, 2001.\nU. Germann et al. Fast decoding and optimal decoding\nfor machine translation. InProc. of ACL’01, pages\n228–235, Morristown, NJ, USA, June 2001. Associ-\nation for Computational Linguistics.\nF. Jelinek. A fast sequential decoding algorithm using\na stack.IBM Journal of Research and Development,\n13:675–685, 1969.\nKevin Knight. Decoding complexity in word-\nreplacement translation models.Computational Lin-\nguistics, 25(4):607–615, 1999.\nP. Koehn. Europarl: A parallel corpus for statistical ma-\nchine translation. InProc. of the MT Summit X, pages\n79–86, September 2005.\nP. Koehn, F.J. Och, and D. Marcu. Statistical phrase-\nbased translation. InProc. of NAACL’03, pages\n48–54, Morristown, NJ, USA, 2003. Association for\nComputational Linguistics. doi: http://dx.doi.org/10.\n3115/1073445.1073462.\nS. Kumar and W. Byrne. Minimum bayes-risk decoding\nfor statistical machine translation, 2004.\nJ. B. Mariño et al. N-gram-based machine transla-\ntion. Computational Linguistics, 32(4):527–549,\n2006. ISSN 0891-2017.\nNAACL 2006. http://nlp.cs.nyu.edu/\nhlt-naacl06/.\nF. J. Och. GIZA++: Training of statistical translation\nmodels, 2000.http://www-i6.informatik.\nrwth-aachen.de/\\~och/software/GIZA+\n+.html.\nF. J. Och and H. Ney. The alignment template approach\nto statistical machine translation.Computational Lin-\nguistics, 30(4):417–449, 2004a. ISSN 0891-2017.\nF. J. Och, Christoph Tillmann, and Hermann Ney.\nImproved alignment models for statistical machine\ntranslation. InProc. of the Joint SIGDAT Conf. on\nEmpirical Methods in Natural Language Processing\nand Very Large Corpora, pages 20–28, University of\nMaryland, College Park, MD, June 1999.\nF.J. Och and H. Ney. The alignment template approach\nto statistical machine translation.Computational Lin-\nguistics, 30(4):417–449, 2004b.\nD. Ortiz, Ismael García-Varea, and Francisco Casacu-\nberta. An empirical comparison of stack-based de-\ncoding algorithms for statistical machine translation.\nIn New Advance in Computer Vision, Lecture Notes\nin Computer Science. Springer-Verlag, 2003. 1st\nIberian Conference on Pattern Recongnition and Im-\nage Analysis -IbPRIA2003- Mallorca. Spain. June.\nD. Ortiz, I. García-Varea, and F. Casacuberta. Thot:\na toolkit to train phrase-based statistical translation\nmodels. InTenth Machine Translation Summit, pages\n141–148, Phuket, Thailand, September 2005.\nJAF-DSIC-UPV 103\nBibliography\nV . Steinbiss R. Schlüter, T. Scharrenbach and H. Ney.\nBayes risk minimization using metric loss func-\ntions. InProceedings of the European Conference on\nSpeech Communication and Technology, Interspeech,\npages 1449–1452, Lisbon, Portugal, September 2005.\nA. Stolcke. SRILM – an extensible language model-\ning toolkit. InProc. of ICSLP’02, pages 901–904,\nSeptember 2002.\nC. Tillmann and H. Ney. Word reordering and a dy-\nnamic programming beam search algorithm for statis-\ntical machine translation.Computational Linguistics,\n29(1):97–133, March 2003.\nRaghavendra Udupa and Hemanta K. Maji. Computa-\ntional complexity of statistical machine translation.\nIn Proceedings of the Conference of the European\nChapter of the Association for Computational Lin-\nguistics (EACL), pages 25–32. Trento, Italy, 2006.\nN. Uefﬁng and H. Ney. Bayes decision rules and con-\nﬁdence measures for statistical machine translation.\nIn EsTAL - Espa for Natural Language Process-\ning, pages 70–81, Alicante, Spain, October 2004.\nSpringer Verlag, LNCS.\nY . Wang and A. Waibel. Decoding algorithm in statisti-\ncal translation. InProc. of ACL’97, pages 366–372,\nMorristown, NJ, USA, July 1997. Morgan Kaufmann\n/ Association for Computational Linguistics.\nR. Zens. Phrase-based Statistical Machine Trans-\nlation: Models, Search, Training. PhD thesis,\nRWTH Aachen University, Aachen, Germany, Febru-\nary 2008.\n104 JAF-DSIC-UPV\nChapter 5\nStatistical stochastic ﬁnite state transducers\n“ An expert is someone who knows some of the worst mistakes thatcan be made in his subject, and\nhow to avoid them” W. H EISENBERG\nContents\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n5.2 Stochastic ﬁnite-state transducers (SFST) . . . . . . . . . .. . . . . 106\n5.2.1 Grammatical inference and alignments for transducerinference\n(GIATI) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n5.3 Statistical GIATI (SGIATI) . . . . . . . . . . . . . . . . . . . . . . . 109\n5.4 Useful recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n5.5 Maximum likelihood estimation of SGIATI . . . . . . . . . . . . .. . 112\n5.6 Preliminary experiments . . . . . . . . . . . . . . . . . . . . . . . . . 113\n5.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n105\nChapter 5. Statistical stochastic ﬁnite state transducers\n5.1 Introduction\nAs stated in Section 1.3 Chapter 1, the machine translation process is a classiﬁcation problem. There-\nfore, multiplying the maximisation Eq. (1.9) Chapter 1 bypr (x), which is a constant for the maximi-\nsation, the optimal translation is the one that maximises the following equation\nˆy(x) = arg max\ny\n{pr (y, x)} (5.1)\nwhere the joint probabilitypr (y, x) can be modelled by astochastic ﬁnite state transducer (SFST).\nNote that Eq. (5.1) is equal that Eq. (1.11) but instantiatedto the MT notation.\nSFSTs constitute an important family of translation modelswithin the theory of formal languages [Vi-\ndal et al., 2005a]. Even though these models are much more limited than other more powerful ones,\nthe computational costs of the algorithms that are needed todeal with them are much lower. SFSTs\nalso permit a simple integration with other information sources, which makes it easy to apply SFSTs to\nmore difﬁcult tasks such as speech translation [Casacuberta et al., 2004]. SFSTs and the corresponding\ntraining and search techniques have been studied by severalauthors, in many cases explicitly moti-\nvated by MT applications [E. Vidal and Segarra, 1989, Oncinaet al., 1993, Knight and Al-Onaizan,\n1998, Mäkinen, 1999, Amengual et al., 2000, Alshawi et al., 2000a, Casacuberta, 2000a, Vilar, 2000,\nV ogel and Ney, 2000, Picó and Casacuberta, 2001, Bangalore and Riccardi, 2003, Kumar and Byrne,\n2003, Casacuberta and Vidal, 2004, Tsukada and Nagata, 2004, Casacuberta et al., 2005, Kumar et al.,\n2006, Casacuberta and Vidal, 2007, Mariòo et al., 2006]. There are other statistical models for MT that\nare based on alignments between words (statistical word-alignment models) [Brown et al., 1993] or be-\ntween word sequences (phrase-based models or alignment templates) [Och and Ney, 2004, Zens, 2008].\nSome of these models (monotone phrase-based models[Zens, 2008]) are closely related to SFST. Other\ntranslation models, which can be considered generalisations of SFSTs, are theinversion transduction\ngrammars [Wu, 1995] and thehead transducers[Alshawi et al., 2000b]. These models are theoretically\nmore powerful than SFSTs, but in general, they require higher computational costs.\nThe GIATI technique [Casacuberta and Vidal, 2007] has been applied to machine translation [Casacu-\nberta and Vidal, 2004], speech translation [Casacuberta etal., 2004] and computed-assisted transla-\ntion [Barrachina et al.]. The results obtained using GIATI suggest that, among all the SFST learning\ntechniques tested, GIATI is the only one that can cope with translation tasks under real conditions of\nvocabulary sizes and amounts of training data available. However, as the task complexity increases, GI-\nATI tends to fall behind other approaches that more explicitly rely on statistics [Casacuberta and Vidal,\n2007, Mariòo et al., 2006].\n5.2 Stochastic ﬁnite-state transducers (SFST)\nStochastic ﬁnite-state transducers (SFST) are similar to stochastic ﬁnite-state grammars or automata [Vi-\ndal et al., 2005b], but in this case two different alphabets are involved: source (input) and target (output)\nalphabets. Each transition in a SFST has attached a source worda and a (possible empty) string of target\nwords [Vidal et al., 2005a].\nDeﬁnition 1A SFST T is deﬁned as a tuple{X , Y , Q, q 0, t, f }, whereX is a ﬁnite set of source\nwords; Y is a ﬁnite set of target words;Q is a ﬁnite set of states;q0 ∈ Q is the initial state;p :\nQ × X ⋆ × Y ⋆ × Q → [0, 1] is a transition probability function andf : Q → [0, 1] is a ﬁnal-state\nprobability function. The functionst and f must verify:\n∀q ∈ Q, f (q) +\nX\n(¯x,¯y,q′)∈X×Y ⋆ ×Q\nt(q, ¯x, ¯y, q ′) = 1 . (5.2)\naThe term “word” is used to refer a single token as in MT i.e. a “symbol” in formal language theory.\n106 JAF-DSIC-UPV\n5.2. Stochastic ﬁnite-state transducers (SFST)\nThe non-probabilistic counterpart of a given a SFSTT , calledcharacteristic ﬁnite-state transducer\nofT (FST), can be deﬁned. Thetransitionsare those tuples inQ × X ⋆ × Y ⋆ × Q with probability\ngreater than zero and theset of ﬁnal statesare those states inQ with ﬁnal-state probability greater than\nzero.\nGiven T , atranslation formwith J (the number of words or symbols in the source sentence)\ntransitions associated with thetranslation pair(x, y) ∈ X ∗ × Y ∗ is a sequence of transitionsφ =\n(q0, ¯x1, ¯y1, q1) ( q1, ¯x2, ¯y2, q2) ( q2, ¯x3, ¯y3, q3) . . . (qJ −1, ¯xJ , ¯yJ , qJ ), such that¯x1 ¯x2 . . . ¯xJ = x\nand ¯y1 ¯y2 . . . ¯yJ = y. Its probability is the product of the corresponding transition probabilities, times\nthe ﬁnal-state probability of the last state in the sequence, that is to say,\npT (φ) =\nJY\nj=1\nt(qj−1, ¯xj , ¯yj , qj ) f (qJ ) . (5.3)\nThe set of translation forms associated with a translation pair(x, y) with probability higher than zero\nis denoted asΦ( x, y).\nThe probability of a translation pair(x, y) according toT is then deﬁned as the sum of the proba-\nbilities of all the translation forms associated with(x, y), i.e.,\npT (x, y) =\nX\n∀φ∈Φ( x,y )\npT (φ) . (5.4)\nIfT has no useless states [Vidal et al., 2005a],pT (x, y) describes a probability distribution on\nX ⋆ × Y ⋆ which is calledstochastic ﬁnite-state translation. Recall that this distribution is used to\nmodel the joint probability introduced in Eq. (5.1). The terms regularor, more properly,rational\ntranslations are also often used in the scientiﬁc literature to refer to (the non-probabilistic counterpart\nof) these mappings [Berstel, 1979].\nA SFST has two embedded stochastic regular languages, one for the source alphabet and another for\nthe target alphabet. These languages correspond to the two marginals (pi and po) of the joint distribution\nmodelled by the SFST as follows\npi\nT (x) =\nX\ny ∈Y ⋆\npr (x, y), p o\nT (y) =\nX\nx∈X ⋆\np(x, y) . (5.5)\nIn practice, the corresponding source or target ﬁnite-state grammars are obtained from the ﬁnite-state\ntransducer by dropping the target or source words of each transition, respectively.\nSFSTs exhibit properties and problems similar to those exhibited by stochastic regular languages.\nOne of these properties is the formal basis of the GIATI technique for transducer inference. It can be\nstated as the following theorem [Casacuberta et al., 2005]:Every stochastic ﬁnite-state translation can\nbe obtained from a stochastic regular language and two morphisms. This is a weaker version of the\nstochastic extension of a classical morphism theorem [Berstel, 1979]:Every rational translation can be\nobtained from a local language and two alphabetic morphisms, where alocal languageis deﬁned by a\nset of permitted two-word segments (and therefore astochastic local languageis equivalent to abigram\ndistribution[Vidal et al., 2005a]). In both cases, the morphisms allow usto build the components of a\npair of the ﬁnite-state translation from a string of the corresponding local language [Casacuberta et al.,\n2005].\nA SFST T can be used to approximate the joint probability in Eq. (5.1),pr (x, y), obtaining\nˆy = arg max\ny ∈Y ⋆\npT (x, y). (5.6)\nThat is, givenT and x ∈ X ∗, search for a target stringˆy which maximisespT (x, y).\nJAF-DSIC-UPV 107\nChapter 5. Statistical stochastic ﬁnite state transducers\nWhile thisSFST stochastic translation problemis proved to beNP -Hard [Casacuberta, 2000b], a\ngenerally good approximation can be obtained in polynomialtime through a simple extension of the\nViterbi algorithm [Picó and Casacuberta, 2001]. This approximation consists in replacing the sum\noperator in Eq. (5.4) with the maximum operator as follows\npT (x, y) ≈ bpT (x, y) = max\n∀φ∈Φ( x,y )\npT (φ). (5.7)\nThis approximation to the SFST stochastic translation problem permits the computation of the optimal\ntranslation form (with respect to Eq. (5.7)) in linear time with the number of source words. The trans-\nlation of the given source sentence is then approached as thesequence of target strings which appear in\nthis optimal translation form.\n5.2.1 Grammatical inference and alignments for transducerinference\n(GIATI)\nThe morphism theorems stated in Section 5.2 suggest a technique [Casacuberta et al., 2005] to in-\nfer an SFSTs, the so-calledgrammatical inferences and alignments for transducer inference(GIATI)\napproach [Casacuberta, 2000b]. Therefore, the GIATI technique has a strong and solid theoretical foun-\ndation. This technique has been applied to machine translation [Casacuberta and Vidal, 2007, Mariòo\net al., 2006], speech translation [Casacuberta et al., 2004] and computed-assisted translation [Barrachina\net al.]. The results obtained using GIATI suggest that, among all the SFST learning techniques tested,\nGIATI is the only one that can cope with translation tasks under real conditions of vocabulary sizes\nand amounts of training data available. However, as the taskcomplexity increases, GIATI tends to fall\nbehind other approaches that more explicitly rely on statistics.\nGiven a ﬁnite sample of pairsD = {(xn , yn )}N\nn=1 of string pairs fromX ⋆ × Y ⋆ (a parallel\ncorpus), the GIATI approach works as follows,\n1. Each training pair(xn, yn) from D is transformed into a stringzn from anextended alphabet\nΓ to obtain a sampleD′ of strings (D′ ⊂ Γ ⋆ ).\n2. A stochastic ﬁnite-stategrammar [Vidal et al., 2005b],G, is inferred fromD′.\n3. The symbols (fromΓ ) of thegrammar transitionsare transformed into input/output symbols\n(X ⋆ × Y ⋆ ).\nThe main problem of this procedure is to deﬁne the set of the extended symbols. The transforma-\ntion of the training pairs must capture the correspondencesbetween words of the input and the output\nsentences and must permit the implementation of the inversetransformation of the third step. This is\nachieved with the help of bilingual segmentations [Casacuberta and Vidal, 2004].\nIn order to illustrate this ﬁrst step, we will use the Spanish-English pair (“una habitación doble” ,\n“a double room”). A suitable word-alignment would align “una” with “a”, “habitación” with “room”\nand “doble” with “double”. From this alignment, a possible string could be “(una,a) (habitación,room)\n(doble,double)”. However, this would imply a reordering of the words “double” and “room”, that is\ndifﬁcult to model in the ﬁnite-state framework. The key ideais to avoid a reordering, for example,\nthe alignment can be used to produce a left-to-right bilingual segmentation into two segments: (“una”\n, “a”) and (“habitación doble” , “double room”). This segmentation directly yields the single string\nand the corresponding extended alphabet required by GIATI.In the ﬁrst version of GIATI, empty target\nsegments were allowed, in this case, a simpler segmentationwhich has proved equivalently adequate in\npractice is: “(una,a) (habitación,-) (doble,double room)”.\nOne of the shortcomings of GIATI comes from the fact that heuristically it needs“external”statis-\ntical techniques to preprocess the training pairs. Actually, the bilingual segmentation of ﬁrst step in the\n108 JAF-DSIC-UPV\n5.3. Statistical GIATI (SGIATI)\noriginal GIATI technique was based on statistical alignment models [Brown et al., 1992]. The proba-\nbilities associated with the transitions of a SFST learnt inthis way are just those of the corresponding\nstochastic ﬁnite-state grammar inferred instep 2. Therefore, an interesting feature of GIATI was that\nit can readily make use of all the smoothing techniques knownforn-grams language models (see Sec-\ntion 1.2 Chapter 1) and for stochastic ﬁnite-state grammars[Llorens et al., 2002]. Note, however, that\nGIATI extended alphabets are typically very large and this fact hardens the data-sparseness problems.\nClearly, for a given translation pair, there are many possible bilingual segmentations; but the orig-\ninal version of GIATI did not take advantage of this fact, thereby making less proﬁt from the (always\nscarce) training data. In the next section, a new GIATI version is introduced [Andrés-Ferrer et al.,\n2008]. This version is more explicitly based on statisticalestimation procedures and would not suffer\nfrom this shortcoming. These procedures require an initialisation that can be random or based on the\nabove segmentation obtained using statistical alignment models. Another interesting feature of the new\nGIATI version is that thestep 2is embedded in the estimation procedure itself.\n5.3 Statistical GIATI (SGIATI)\nOur new, statistical version of GIATI, SGIATI [Andrés-Ferrer et al., 2008], is based on a rather simple\nprobabilistic model for segment-based (phrase-based) statistical machine translation. Given a transla-\ntion pair,(x, y), we assume that both sentences can be segmented into a certain number of segments,\nsay C, which are monotonically aligned one-to-one, to produce the desired segment-based translation\nofx intoy. This is illustrated in the example shown in Figure 5.1, where three possible segmentations\nof a given pair are considered. Note that we usej and i to deﬁne the precise limits of the segments in\nx and y, respectively.\npor favor, súbanosnuestros bultosa la habitación.\nplease, send upour luggageto the room .\npor favor ,súbanos nuestros bultosa lahabitación .\nplease ,send up our luggageto theroom .\npor favor , súbanos nuestros bultosa la habitación .\nplease , send up ourluggage to the room .\nFigure 5.1: Three possible segmentations of the translation pair ”por favor\n, súbanos nuestros bultos a la habitación .” and ” please ,\nsend up our luggage to the room .”. The used segmentations are:j =\n(0, 2, 3, 4, 6, 7, 8, 9, 10) and i = (0 , 1, 2, 4, 6, 7, 8, 9, 10) for the ﬁrst segmentation;\nj = (0 , 3, 6, 8, 10) and i = (0 , 2, 6, 8, 10) for the second; andj = (0 , 6, 8, 10) and\ni = (0 , 2, 5, 10) for the third.\nUncovering thehiddenrandom variables for the number of segments,C, and those for the segmen-\ntations ofx and y,j and i respectively; the probability of observing a given translation pair,pr (x, y),\nis written as follows\npr (x, y) =\nX\nC\nX\nj ,i\npr (x, y, j, i, C ) , (5.8)\nwhere j and i range over the set of all possible segmentations ofx and y\nj = ( j0, j1, j2, . . . , j C ), j l < j l+1 withl ∈ {1, . . . , C − 1} and jC = J, j 0 = 0 (5.9)\ni = ( i0, i 1, i 2, . . . , i C ), i l < i l+1 withl ∈ {1, . . . , C − 1} and iC = I , i 0 = 0 (5.10)\nJAF-DSIC-UPV 109\nChapter 5. Statistical stochastic ﬁnite state transducers\nwithJ = |x| and I = |y|. For brevity, we usejl\nk to denote the sub-vector ofj from positionk tol;\ni.e.,jl\nk = ( jk , j k+1, . . . , j l−1, jl).\nOur probabilistic model for(x, y), completedwith j, i and C, is decomposed left-to-right as\nfollows\npr (x, y, j, i, C ) =\nCY\nc=1\npr (x(c), y(c), jc , i c |H (c − 1))pr ($, $, C |H (C)) , (5.11)\nwhere we have used the notationx(c) forc-th segment ofx, i.e.,xjc\njc−1+1;y(c) is similarly used for\ny; andH (c − 1) denotes the history ofc − 1 previous segments,\nH (c − 1) = {x(jc−1\n0 ), y(ic−1\n0 ), jc−1\n1 , ic−1\n1 } , (5.12)\nwhere x(jc−1\n0 ) stands for thec − 1 segmentsxj1\nj0+1, xj2\nj1+1, . . . , x\njc−1\njc−2+1 and similarly doesy(ic−1\n0 );\nand where H (0) is deﬁned as the sure event and, hence,pr (x(1), y(1), j1, i1 |H (0)) is equal to\npr (x(1), y(1), j1, i1); and, ﬁnally, the length distributionpr ($, $, C |H (C)) = 0 ifC differs from\nthe length ofj ori.\nNote thatH (c −1) can be approximated with then more recent segmentations, similarly ton-gram\nlanguage modelling. For simplicity, the probability of thecurrent segment given the history of thec −1\nprevious segments is approximated using a ﬁrst-order Markovian assumption (n = 2 ). That is to say,\nH (c − 1) ≈ H 2(c−1) = {(x\njc−1\njc−2+1), (y\nic−1\nic−2+1), jc−2, jc−1, ic−2, ic−1} . (5.13)\nIn this way, our complete probabilistic model in Eq. (5.11) is approximated as follows\npr (x, y, j, i, C ) :=\nCY\nc=1\npr (x(c), y(c), jc , ic |H 2(c−1))pr ($, $ |H 2(C)) . (5.14)\nThe model in Eq. (5.14) is still difﬁcult to learn due to the inclusion of absolute segment boundaries\nwhile computing the probability of the current segment. Instead, to ease parameter estimation, each\nabsolute boundary is rewritten relative to its previous boundary,\npr (x, y, j, i, C ) :=\nCY\nc=1\npr (x(c), y(c), jc − jc−1, ic − ic−1 | H 2(c−1))pr ($, $ |H 2(C)) (5.15)\n:=\nCY\nc=1\np(x(c), y(c) |H 2(c−1)) p($ , $ |H 2(C)) , (5.16)\nwhere, in Eq. (5.16), we assume that segment probabilities are null when the relative boundariesjc −\njc−1 and ic − ic−1 disagree with their corresponding segment lengths; and where from Eq. (5.15)\nto Eq. (5.16) we have changed the probability distributionspr (· · ·) by the model parametersp(· · ·).\nTherefore, our ﬁnal model is parametrised with the following parameter set:\nΘ = {p(u, v), p(u, v |u′, v′) | ∀u, u′ ∈ X∗, v, v′ ∈ Y∗} , (5.17)\nwhere allu′, v′ verify the following normalisation property\nX\nu∈X∗,v ∈Y∗\np(u, v |u′, v′) = 1 . (5.18)\n110 JAF-DSIC-UPV\n5.4. Useful recurrences\nFor clarity shake, consider as an example,x = x1x2x3 and y = y1y2. Using Eq. (5.8), the joint\nprobability of observingx and y can be written as follows\npr (x, y) = pr(x, y, (0, 3), (0, 2), 1) + pr (x, y, (0, 1, 3), (0, 1, 2), 2) + pr (x, y, (0, 2, 3), (0, 1, 2), 2)\n(5.19)\nAccording to Eqs. (5.14) and (5.15), the probabilities in the right-hand side of Eq. (5.19) are approxi-\nmated by\npr (x, y, (0, 3), (0, 2), 1) := pr (x3\n1, y2\n1, 3, 2)pr ($, $ |x3\n1, y2\n1) (5.20)\npr (x, y, (0, 1, 3), (0, 1, 2), 2) := pr (x1, y 1, 1, 1)pr (x3\n2, y 2, 2, 1 |x1, y 1)pr ($, $ |x3\n2, y 2) (5.21)\npr (x, y, (0, 2, 3), (0, 1, 2), 2) := pr (x2\n1, y 1, 2, 1)pr (x3, y 2, 1, 1 |x2\n1, y 1)pr ($, $ |x3, y 2) . (5.22)\nFrom Eqs. (5.19)–(5.22), and applying Eq. (5.16), the jointprobability of observingx and y is written\nin terms of model parameters\npr (x, y) := p( x3\n1, y2\n1) p($, $ |x3\n1, y2\n1)\n+ p( x1, y 1) p(x3\n2, y 2 |x1, y 1) p($ , $ |x3\n2, y 2)\n+ p( x2\n1, y 1) p(x3, y 2 |x2\n1, y 1) p($ , $ | x3, y 2)\n. (5.23)\nNote that the source and target segmentations in Eqs. (5.19)–(5.22) do not explicitly appear in Eq. (5.23),\nthough they are implicitly taken into account since they guide the probability decompositions in Eqs. (5.20)–\n(5.22).\n5.4 Useful recurrences\nGiven a bilingual pair(x, y) and a parameter setθ , the joint probabilitypθ (x, y) is efﬁciently com-\nputed by means of any of the following two recurrences: theforward-likeand backward-likerecur-\nrences.\nRoughly speaking, the forward recursion efﬁciently computes the probability of a given preﬁx.\nMore precisely, given the source boundariesl′, l and the target boundariesm′, m ; the forward recur-\nrence is deﬁned as the probability of the preﬁxxl\n1 and ym\n1 to occur knowing that the previous segment\nended at positionsl′ and m′,\nαl′lm′m = αl′lm′m(x, y) = pθ (xl\n1, ym\n1 , l ′, m ′) , (5.24)\nwhere 0 < l ′ < l ≤ J and 0 < m ′ < m ≤ I .\nThe forward-like recursion is efﬁciently computed by the following recursive equation\nαl′lm′m =\n8\n>\n>\n>\n>\n>\n>\n<\n>\n>\n>\n>\n>\n>\n:\n1 l′ = l = m = m′ = 0\np(xl\n1, ym\n1 ) l′ = m′ = 0 ,\nl > 0, m > 0\nl′−1X\nl′′=0\nm′−1X\nm′′=0\nαl′′l′m′′m′ p(xl\nl′+1, ym\nm′+1 |xl′\nl′′+1, ym′\nm′′+1) otherwise\n(5.25)\nThe backward counterpart efﬁciently computes the probability of a given sufﬁx. Speciﬁcally, given\nthe source boundariesl′′, l ′ and the target boundariesm′′, m ′ the forward recurrence is deﬁned as the\nprobability of the sufﬁxxJ\nl′+1 and yI\nm′+1 given that the previous segments werexl′\nl′′+1 and ym′\nm′′+1, in\nother words\nβl′′l′m′′m′ = βl′′l′m′′m′ (x, y) = pθ (xJ\nl′+1, yI\nm′+1 |xl′\nl′′+1, ym′\nm′′+1) , (5.26)\nJAF-DSIC-UPV 111\nChapter 5. Statistical stochastic ﬁnite state transducers\nwhere 0 < l ′′ < l ′ ≤ J and 0 < m ′′ < m ′ ≤ I .\nAgain, the backward-like recursion is efﬁciently computedby the application of the following re-\ncursive equation\nβl′′l′m′′m′ =\n8\n>\n>\n>\n>\n>\n>\n<\n>\n>\n>\n>\n>\n>\n:\n1 l′ = l = J, m = m′ = I\np($, $ |xJ\nl′′+1, yI\nm′′+1) l′ = J, m′ = I ,\nl′′ < J, m ′′ < I\nJX\nl=l′+1\nIX\nm=m′+1\nβl′lm′m p(xl\nl′+1, ym\nm′+1 | xl′\nl′′+1, ym′\nm′′+1) otherwise\n(5.27)\nUsing the forward-like recurrence, the joint probabilitypθ (x, y) is computed as follows\npθ (x, y) =\nX\nl,m\nαlJ mI . (5.28)\nAlternatively, the joint probability can also be computed with the backward recursion as follows\npθ (x, y) = β0000 . (5.29)\nThe total time complexity required to compute both recurrences isO(J 3 I 3), and two matrices\nof sizeO(J 2 I 2) are needed to store them. It is important to highlight that ifwe use a Markovian\napproximation of ordern, higher than2, then the recurrence tables will needO(J n I n) elements and a\ntime complexity ofO(J n+1I n+1).\nThe probability of using a given source and target segment positionsl′′, l ′, l and m′′, m ′, m , re-\nspectively, is deﬁned as follows\nγl′′l′lm′′m′m = αl′′l′m′′m′ p(xl\nl′+1, ym\nm′+1 |xl′\nl′′+1, ym′\nm′′+1)βl′lm′m\npθ (x, y) , (5.30)\nwith0 ≤ l′′ < l ′ < l ≤ J and 0 ≤ m′′ < m ′ < m ≤ I .\nFinally, we will henceforth use the notationγnl′′l′lm′′m′m to refer toγl′′l′lm′′m′m for then-th\noutcome of a given collection of training translation pairs{(x1, y1), . . . , (xN , yN )}. This notation is\nalso expanded to the corresponding forwardαnl′lm′m, and backwardβnl′lm′m recurrences.\n5.5 Maximum likelihood estimation of SGIATI\nSince our model is based on a hidden bilingual segmentation variable, it is necessary to use some ap-\nproximate inference algorithm. Speciﬁcally, we use the EM algorithm introduced in Section 1.1.4 in or-\nder to estimate the parameters in Eq. (5.17) w.r.t. a collection of training translation pairs{(x1, y1)}N\nn=1.\nThe (incomplete)log-likelihood function is given by\nLL(θ ) =\nNX\nn=1\nlog\nX\nCn\nX\nj n ,in\npθ (xn , yn , jn , in , C n) , (5.31)\nwith\npθ (xn, yn , jn, in , C n) =\nCnY\nc=1\np(xn (c), yn (c) | H 2\nn(c − 1)) p($ , $ |H 2\nn(Cn)) . (5.32)\n112 JAF-DSIC-UPV\n5.6. Preliminary experiments\nHowever, the EM algorithm maximisesLL(·) by iteratively maximising a variational function\nL(q, θ ) as reviewed in Section 1.1.4 Chapter 1. Letθ (k−1) be a guess of the optimal parameters\nobtained from previous iterations. Then, in the E-step all the sufﬁcient statistics needed to compute\nq(k−1)(jn, in) = p θ (k−1) (jn , in |xn , yn ) are calculated. Speciﬁcally, we compute the probabilities\nγnl′′l′lm′′m′m, for all0 ≤ l′′ < l ′ < l ≤ J and 0 ≤ m′′ < m ′ < m ≤ I . Note that in order to\nefﬁciently compute and storeγnl′′l′lm′′m′m , both recurrencesαnl′lm′m and βnl′lm′m are computed\nand stored.\nIn the M-step, the parameter setθ (k) that maximisesL(q(k), θ ) are computed as follows\np(k)(u, v |u′, v′) = N (k−1)(u, v; u′, v′)P\nu′′,v ′′ N (k−1)(u′′, v′′; u′, v′) , (5.33)\nwhere we have used the deﬁnition\nN (k−1)(u, v; u′, v′) =\nX\nn\nX\nl′′<l′<l\nm′′<m′<m\nγnl′′l′lm′′m′m δl′′l′lm′′m′m(xn, yn, u′, u, v′, v) ,\nwhich is the expected value of the occurrences of the event(u, v; u′, v′) in the training data. The\nexpressionδl′′l′lm′′m′m (xn , yn , u′, u, v′, v) is a predicate that is1 if the segment boundariesl′′, l ′, l ,\nand m′′, m ′, m , and the source and target phrases are compatible, i.e.,\nδl′′l′lm′′m′m(x, y, u′, u, v′, v) =\n(\n1 xl′\nl′′+1 = u′, xl\nl′+1 = u, ym′\nm′′+1 = v′, ym\nm′+1 = v\n0 otherwise (5.34)\nIn order to implement the re-estimation Eq. (5.33), it is only needed to compute the forward\nαnl′lm′m and backwardsβnl′lm′m for all samples and for all values ofl, m, l ′ and m′. Afterwards,\nthe expected counts given byγnl′′l′lm′′m′m, are efﬁciently computed using the previously computed\nforward and backward recursions.\nAs we have discussed in Section 1.1.3, the maximum likelihood estimation technique tends to un-\nderestimate the probability of the unseen events. The EM algorithm is not an exception, and, therefore,\nwe need to resort to smoothing techniques. Since the SGIATI techniques is highly inspired inn-gram\nmodels, it seems sensible to extend the leaving-one-out smoothing estimation techniques discussed in\nChapter 3. Therefore, we use the following backing-off smoothing\n˜ p(u, v |u′, v′) =\n(\np(u, v | u′, v′)(1 − φ(u′, v′)) if(u, v) ∈ V (u′, v′)\npbo(u′, v′)φ(u′, v′) if(u, v) ̸∈ V(u′, v′) (5.35)\nwhere V(u′, v′) ⊂ X⋆ × Y⋆ is the set of segments that have a not-null probability giventhe his-\ntory(u′, v′);pbo (u′, v′) is a probability distribution deﬁned over all unseen segmentations pairs, i.e.\n(u, v) ̸∈ V(u′, v′); and, ﬁnally,φ(u′, v′) stands for the probability mass discounted from the seen\nevents that occur after the previous history(u′, v′).\nSince in this model we are using fractional occurrence counts N (k−1)(u, v; u′, v′), instead of ac-\ntual counts; it is not possible to apply leaving-one-out to obtain a closed form solution to the discounted\nprobability massφ(u′, v′) as it is done inn-gram language models [Ney et al., 1997]. In practice, we\nhave ﬁxed it to a constant valueφ(u′, v′) = ǫ.\n5.6 Preliminary experiments\nIn this section, some preliminary experiments were carriedout to asses the formal derivation of the\ncurrent GIATI version, SGIATI, and to compare it with the previous heuristic version of GIATI.\nJAF-DSIC-UPV 113\nChapter 5. Statistical stochastic ﬁnite state transducers\nTest Set Train Set\nSpanish English Spanish English\nsentences 2K 10K\navg. length 12.7 12 .6 12 .9 13 .0\nvocabulary 611 468 686 513\nsingletons 63 49 8 10\nrunning words 35.0K 35.6K 97.2K 99.2K\nperplexities (3-gram) - - 5.4 3 .8\nTable 5.1:Basic statistics of the Spanish-English EU T RANS -I task, wheresingletons\nstands for the words occurring once, andrunning wordsdenotes the total amount of\nword occurrences.\nWER BLEU SER\nOrder (n) 1 2 1 2 1 2\nGIATI 20.4 8.3 63.2 87.3 80.4 44.2\nSGIATI 13.0 7.7 77.4 88.5 65.3 41.1\nMoses 11.7 88.3 42.1\nTable 5.2:Results obtained with the EU T RANS -I task for different algorithms: SGIATI\n(the EM version), and GIATI, which corresponds to the model obtained by counting the\noccurrences of each segment and then re-normalising by the sum of all counts.\nThe experiments were carried out using the Spanish-EnglishE U T RANS -I task [Amengual et al.,\n2000]. The Spanish-English sentence pairs correspond to human-to-human communication situations\nat the front-desk of a hotel which were semi-automatically produced using a small seed corpus compiled\nfrom travel guides booklets. The corpus comprises several domains and4 persons each of which was\nin charge of a (non-disjoint) subset of sub-domains. The basic statistics of this corpus are shown in\nTable 5.1.\nSince the size of recurrence tables grow exponentially withthe length of the history size, we only\nreport results for the bigram and unigram case. Moreover, inthe bigram case we used the smoothing\ndetailed in Eq. (5.35).\nTable 5.2 summarises some results. The GIATI denotes the model obtained by counting the occur-\nrences of each segment and then re-normalising by the sum of all counts, i.e. previous GIATI estimation\ntechnique. The SGIATI stands for the training algorithm presented in this chapter. Finally,Moses stands\nfor the Moses system [Koehn et al., 2007] that were trained performing the MERT in a validation set.\nWe have ﬁxed the maximum phrase length to7 words for all the systems. The proposed statistical es-\ntimation provides an increase of performance with respect to the GIATI version for both history sizes.\nThe high increase obtained for the unigram model is due to thefact that there is only one state and\nthe probability mass can be readjusted properly. This re-estimation meaningfully differs from GIATI\nparameters. In the bigram case, the average of segment pairsper state is over5, which means that on\naverage the EM can only redistribute the probability mass among few segments (over5) for a given\nprevious history. Both GIATI algorithms are highly dependent on the quality of the selected segments\nused to initialise the algorithm.\n114 JAF-DSIC-UPV\n5.7. Conclusions\n5.7 Conclusions\nIn this chapter, we have proposed a new statistical estimation for stochastic ﬁnite-state transducers.\nSpeciﬁcally, a segmental extension to these models obtained via GIATI methodology have been de-\nscribed. This statistical framework is more independent with respect to alignment methods. The results\nreported show that the new technique increases the system performance with respect to (the conven-\ntional) GIATI in a small translation task.\nHowever, the proposed statistical approach presents two great disadvantages: the complexity and\nthe memory requirements. These requirements, which are generated by the recurrences, make the\ngeneralisation of this technique with complex data and withlonger histories unfeasible. For example,\nthe algorithm needs about7GB in order to store the recurrence tables for trigrams and sentences no\nlonger than100 words. If a maximum memory is given for training the SGIATI model, then the longer\nthen of the Markovian approximation is, the shorter the sentences have to be. Additionally, we have\nobserved that the improvements obtained using the proposedSGIATI model are not larger enough for\njustifying the memory and time complexity that this model requires.\nAnother great disadvantage is that, unlike the GIATI model [Casacuberta et al., 2005], the SGIATI\nmodel cannot directly take proﬁt from then-gram smoothing techniques based on leaving-one-out.\nTherefore, we probably loose more performance than what we can gain by using SGIATI.\nFinally, since the SGIATI is a joint modelpθ (x, y), we are modelling more than what we need,\ni.e., a conditional probability modelpθ (y |x). In a joint model, not only the translation correspondence\nbetween words is learnt but also their occurrence frequency. The problem generated by this fact can be\neasily understood with the following example. We assume that we have observed a bilingual pair just\nonce, but that the translation of this pair is unique. For instance, we might have observed “My room\nis 217” and its translation “Mi habitación es la 217”. We further assume that the numbers “217” have\nonly occurred in this outcome. From this example it is clear that a good conditional model will assign\na high probability to the event(217 |217), i.e.,p(217 |217) ≈ 1. However, since the event(217, 217),\nhas only occurred once, a joint model will give it a very smallprobability, i.e.,p(217 |217) ≈ 0. This\nsmall probability accounts for two facts: that the pair(217, 217) is a good translation phrase or/and that\nthe pair(217, 217) is not frequent; and there is no way to differentiate betweenthem. In the following\nchapter, we propose a conditional model based on this SGIATImodel that ﬁxes some of the deﬁciencies\nof this model.\nJAF-DSIC-UPV 115\n\nBibliography\nBibliography\nE. Vidal, F. Thollard, F. Casacuberta C. de la\nHiguera, and R. Carrasco. Probabilistic ﬁnite-\nstate machines - part II.IEEE Transactions on\nPattern Analysis and Machine Intelligence, 27\n(7):1026–1039, 2005a.\nF. Casacuberta et al. Some approaches to statis-\ntical and ﬁnite-state speech-to-speech transla-\ntion.Computer Speech and Language, 18:25–\n47, 2004.\nP. García E. Vidal and E. Segarra. “inductive learn-\ning of ﬁnite-state transducers for the interpreta-\ntion of unidimensional objects”.Structural Pat-\ntern Analysis, pages 17–35, 1989.\nJ. Oncina, P. García, and E. Vidal. Learning\nsubsequential transducers for pattern recogni-\ntion interpretation tasks.IEEE Transactions on\nPattern Analysis and Machine Intelligence, 15:\n448–458, 1993.\nK. Knight and Y . Al-Onaizan. Translation with\nﬁnite-state devices. In E. Hovy D. Farwell,\nL. Gerber, editor,Proc. of AMTA’98, volume\n1529, pages 421–437, London, UK, October\n1998. Springer-Verlag. ISBN 3-540-65259-0.\nE. Mäkinen. Inferring ﬁnite transducers. Technical\nReport A-1999-3, University of Tampere, 1999.\nJuan C. Amengual, José M. Benedí, Asun-\nción Castano, Antonio Castellanos, Víctor M.\nJiménez, David Llorens, Andrés Marzal,\nMoisés Pastor, Federico Prat, Enrique Vidal,\nand Juan M. Vilar. The EuTrans-I speech trans-\nlation system.Machine Translation, 15:75–\n103, 2000.\nH. Alshawi, S. Douglas, and S. Bangalore. Learn-\ning dependency translation models as collec-\ntions of ﬁnite-state head transducers.Compu-\ntational Linguistics, 26(1):45–60, 2000a. ISSN\n0891-2017.\nF. Casacuberta. Inference of ﬁnite-state transduc-\ners by using regular grammars and morphisms.\nInGrammatical Inference: Algorithms and Ap-\nplications. Procedings of the 5th. ICGI, volume\n1891 of Lecture Notes in Computer Science,\npages 1–14. Springer-Verlag, 2000a.\nJ. M. Vilar. Improve the learning of subsequen-\ntial transducers by using alignments and dic-\ntionaries. InGrammatical Inference: Algo-\nrithms and Applications, volume 1891 ofLec-\nture Notes in Artiﬁcial Intelligence, pages 298–\n312. Springer-Verlag, 2000.\nS. V ogel and H. Ney. Translation with cascaded\nﬁnite state transducers. InProceedings of the\n38th Annual meeting of the Association for\nComputational Linguistics, Hong Kong, Octo-\nber 2000.\nDavid Picó and Francisco Casacuberta. Some\nstatistical-estimation methods for stochastic\nﬁnite-state transducers.Machine Learning, 44:\n121–142, July-August 2001.\nS. Bangalore and G. Riccardi. Stochastic ﬁnite-\nstate models for spoken language machine\ntranslation.Machine Translation, 17(3):165–\n184, 2003.\nS. Kumar and W. Byrne. A weighted ﬁnite\nstate transducer implementation of the align-\nment template model for statistical machine\ntranslation. InProceedings of the 2003 Human\nLanguage Technology Conference of the North\nAmerican Chapter of the Association for Com-\nputational Linguistics, pages 63–70, Edmon-\nton, May-June 2003.\nF. Casacuberta and E. Vidal. Machine transla-\ntion with inferred stochastic ﬁnite-state trans-\nducers.Computational Linguistics, 30(2):205–\n225, 2004.\nH. Tsukada and M. Nagata. Efﬁcient decod-\ning for statistical machine translation with a\nfully expanded wfst model. InProceedings of\nthe 2004 Conference on Empirical Methods in\nNatural Language Processing, pages 427–433,\nBarcelona, july 2004.\nF. Casacuberta, E. Vidal, and D. Picó. Inference of\nﬁnite-state transducers from regular languages.\nPattern Recognition, 38:1431–1443, 2005.\nJAF-DSIC-UPV 117\nBibliography\nS. Kumar, Y . Deng, and W. Byrne. A weighted ﬁ-\nnite state transducer translation template model\nfor statistical machine translation.Natural Lan-\nguage Engineering, 2006. In press.\nF. Casacuberta and E. Vidal. Learning ﬁnite-\nstate models for machine translation.Machine\nLearning, 66(1):69–91, 2007.\nJosé B. Mariòo, Rafael E. Banchs, Josep M.\nCrego, Adrià de Gispert, Patrik Lambert, José\nA. R. Fonollosa, and Marta R. Costa-jussà.\nN-gram-based machine translation.Comput.\nLinguist., 32(4):527–549, 2006. ISSN 0891-\n2017. doi: http://dx.doi.org/10.1162/coli.2006.\n32.4.527.\nP. F. Brown et al. The Mathematics of Statis-\ntical Machine Translation: Parameter Estima-\ntion. Computational Linguistics, 19(2):263–\n311, 1993.\nF.J. Och and H. Ney. The alignment template ap-\nproach to statistical machine translation.Com-\nputational Linguistics, 30(4):417–449, 2004.\nR. Zens.Phrase-based Statistical Machine Trans-\nlation: Models, Search, Training. PhD thesis,\nRWTH Aachen University, Aachen, Germany,\nFebruary 2008.\nD. Wu. Stochastic inversion transduction gram-\nmars, with application to segmentation, brack-\neting, and alignment of parallel corpora. In\nProceedings of 14th International Joint Con-\nference on Artiﬁcial Intelligence, pages 1328–\n1335, Montreal, Canada, August 1995.\nH. Alshawi, S. Bangalore, and S. Douglas. Learn-\ning dependency translation models as collec-\ntions of ﬁnite state head transducers.Compu-\ntational Linguistics, 26, 2000b.\nS. Barrachina et al. Statistical approaches to\ncomputer-assisted translation.Computational\nLinguistics, page In press.\nE. Vidal, F. Thollard, F. Casacuberta C. de la\nHiguera, and R. Carrasco. Probabilistic ﬁnite-\nstate machines - part I.IEEE Transactions on\nPattern Analysis and Machine Intelligence, 27\n(7):1013–1025, 2005b.\nJ. Berstel.“Transductions and conteex-free lan-\nguages. B. G. Teubner Stuttgart, 1979.\nF. Casacuberta. Inference of ﬁnite-state transduc-\ners by using regular grammars and morphisms.\nIn A.L. Oliveira, editor,Grammatical Infer-\nence: Algorithms and Applications, volume\n1891 of Lecture Notes in Computer Science,\npages 1–14. Springer-Verlag, 2000b. 5th In-\nternational Colloquium Grammatical Inference\n-ICGI2000-. Lisboa. Portugal.\nP. F. Brown et al. Class-based n-gram models of\nnatural language.Computational Linguistics,\n18(4):467–479, 1992.\nD. Llorens, J. M. Vilar, and F. Casacuberta. Fi-\nnite state language models smoothed using n-\ngrams.International Journal of Pattern Recog-\nnition and Artiﬁcial Intelligence, 16(3):275–\n289, 2002.\nJ. Andrés-Ferrer, A. Juan, and F. Casacuberta. Sta-\ntistical estimation of rational transducers ap-\nplied to machine translation.Applied Artiﬁcial\nIntelligence, 22(1-2):4–22, 2008.\nH. Ney, S. Martin, and F. Wessel. Statistical\nlanguage modeling using leaving-one-out. In\nS. Young and G. Bloothooft, editors,Corpus-\nBased Statiscal Methods in Speech and Lan-\nguage Processing., pages 174–207. Kluwer\nAcademic Publishers, 1997.\nP. Koehn et al. Moses: Open source toolkit\nfor statistical machine translation. InProc.\nof ACL’07: Demo and Poster Sessions, pages\n177–180, Morristown, NJ, USA, June 2007.\nAssociation for Computational Linguistics.\n118 JAF-DSIC-UPV\nChapter 6\nA phrase-based hidden Markov model for monotone\nmachine translation\n“ The existing scientiﬁc concepts cover always only a very limited part of reality, and the other part\nthat has not yet been understood is inﬁnite.” W. H EISENBERG\nContents\n6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n6.2 Phrase-based hidden Markov model . . . . . . . . . . . . . . . . . . 120\n6.3 Forward and backward probabilities . . . . . . . . . . . . . . . . .. 123\n6.4 Decoding and Viterbi recurrence . . . . . . . . . . . . . . . . . . . .124\n6.4.1 The decoding process . . . . . . . . . . . . . . . . . . . . . . . 124\n6.5 Maximum likelihood estimation . . . . . . . . . . . . . . . . . . . . .126\n6.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.6.1 Corpora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.6.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n6.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n119\nChapter 6. A phrase-based hidden Markov model for monotone machine translation\n6.1 Introduction\nAs previously discussed, the machine translation problem is stated as the problem of translating asource\n(input)sentence,x, into atarget (output)sentence,y. Typically, at least one inverse translation model\nis needed to approximate the probabilitypr (x | y) in Eq. (1.102).\nIn the Chapter 1, we discussed that the ﬁrst proposed models,the so-calledIBM translation mod-\nels[Brown et al., 1993], tackled the problem with word-level dictionaries plus alignments between\nwords. However, current systems model the inverse conditional probability usingphrase dictionaries.\nThis phrase-based methodology stores speciﬁc sequences oftarget words (target phrase) into which a\nsequence of source words (source phrase) is translated. A key concept of this approach is the procedure\nthrough which these phrase pairs are inferred.\nA popular, phrase-based technique consists in using the IBMalignment models [Brown et al., 1993]\nto obtain a symmetrised alignment matrix from whichcoherentphrases are extracted (see Section 1.3\nChapter 1). Then, an approximate and heuristically motivated count normalisation is carried out in\norder to obtain a conditional phrase dictionary [Koehn et al., 2003].\nAlternatively, some approaches have been described in the last few years in which phrase dictio-\nnaries are statistically inferred. In particular, the SGIATI model presented in Chapter 5 deﬁnes a joint\nmodel with its algorithm for estimating phrase-based probabilities. Another joint probability model for\nphrase-based estimation was proposed in [Marcu and Wong, 2002], however, this model is a particular\ncase of SGIATI in which the previous history is ignored (n = 1 ) and where the monotonicity constraint\nhas been removed. In the work by Marcu and Wong [2002], all possible segmentations are extracted\nusing the EM algorithm [Dempster et al., 1977], without any matrix alignment constraint, in contrast\nto the approach followed in Och and Ney [2004]. Based on this work, Birch et al. [2006], constrained\nthe EM to only consider phrases which agree with the alignment matrix, thus reducing the size of the\nphrase dictionaries (or tables).\nA drawback of the above phrase-based models is that they are not conditional, but joint models\nthat need to be renormalised in order to make them conditional. Recall that in the previous Chapter 5,\nwe outlined some of the problems of using a joint model. In this chapter, however, we introduce a\ndirect, conditional phrase-based approach for monotone translation [Andrés-Ferrer and Juan, 2007].\nMonotonicity allows us to derive a relatively simple statistical model which is properly described as a\nphrase-based hidden Markov model.\nIn the remaining of this chapter, we ﬁrst introduce our modelin Section 6.2, and then their asso-\nciated training recurrences in Section 6.3. The decoding algorithm is explained in the following Sec-\ntion 6.4. EM-based maximum likelihood estimation of the model parameters is described in Section 6.5.\nEmpirical results are reported in Section 6.6 and then some concluding remarks are given.\n6.2 Phrase-based hidden Markov model\nLetx and y be a pair of source and target sentences of known length,J and I . In order to deﬁne our\nphrase-based hidden Markov model forpθ (x |y, J ), it is ﬁrst convenient to introduce our deﬁnition of\nmonotone segmentation, both for the monolingual and bilingual cases.\nA monotone, monolingual segmentation ofx into a given number of segments,T , is any sequence\nof indexesj = ( j0, j 1, . . . , j T ) such that1 = j0 < j 1 < · · ·< j T = J . Similarly, a monotone,\nsegmentation ofy intoT segments is any sequence of indexesi = ( i0, i1, . . . , i T ) such that1 =\ni0 < i 1 < · · ·< i T = I . Given two monotone, monolingual segmentations ofx and y intoT\nsegments,j and i, their associatedbilingualsegmentation ofx and y is deﬁned ass = s1s2 · · ·sT\nwith st = ( jt−1 + 1 , j t , it−1 + 1 , it ), t = 1 , . . . , T . Reciprocally, given a monotone,bilingual\nsegmentation ofx and y, we can easily extract their associated monolingual counterparts.\n120 JAF-DSIC-UPV\n6.2. Phrase-based hidden Markov model\nFigure 6.1 shows an example in which all possible bilingual segmentations forJ = 4 and I = 5\nare represented as paths in a directed, multi-stage graph. The initial stage of the graph has a single,\nartiﬁcial node labelled as \"init\", which is only included topoint to the initial segments of all the possible\nsegmentations. There are12 of such initial segments, vertically aligned on the ﬁrst stage. Similarly,\nthere are15,3 and 13 segments aligned on the second, third and ﬁnal stages, respectively. The total\nnumber of segments is then43. There is a unique segmentation of unit length,s = s1 = (1415) , which\nis represented by the rightmost path, but there are12, 18 and 4 segmentations of length2,3 and 4,\nrespectively; comprising35 segmentations in total. As empty segments are not allowed, segmentation\nlengths range from one to the length of the shortest sentence. Note that segments on the ﬁrst stage\ncan only appear in the ﬁrst position of a segmentation. Also,segments on the second and ﬁnal stages\ncan only appear on analogous positions in a segmentation. However, those three on the third stage (i.e.\n(3334),(3333) and (3344)) may appear in the second or third positions, although they cannot end any\nsegmentation. For instance,(3334) appears in the second position of((1212), (3334), (4455)) and\nalso in the third position of((1111), (2222), (3334), (4455)).\nNote that we are using the terms segment and segmentation only for positions in the input and output\nsentences. We reserve the termphrasefor actual portions of the given sentences. For instance, the\nbilingual segmentation((1212), (3334), (4455)) ofx4\n1 and y5\n1 results in the bilingual phrases(x2\n1, y2\n1),\n(x3, y4\n3) and (x4, y 5).\nIn what follows, we will writex(st ) to denote the portion ofx delimited by (the input part of)\nsegmentst ; more generally,x(st\nt′ ) will denote the concatenationx(st′ )x(st′+1) · · ·x(st ). Analogous\nnotation will be used fory, i.e.,y(st ) and y(st\nt′ ).\nNow, we can deﬁne our inverse translation model forpr (x |y) as a full exploration of all bilingual\nsegmentations ofx and y,\npr (x |y) = pr (x |y, J ) =\nmin(J,I)X\nT =1\nX\ns\npr (x, s, T |y, J ) , (6.1)\nwhere the second sum is deﬁned over all possible bilingual segmentations of lengthT ; and as most of\nthe literature in SMT, although we do not explicitly the dependence on the source sentence lengthJ , it\nis assumed to be known.\nTo computepr (x, s, T |y, J ) in (6.1), we use the following decomposition\npr (x, s, T | y, J ) = pr (T |y, J )pr (s |y, T , J )pr (x |y, s, T , J ) ,\nwhere pr (s | y, T , J ) is modelled as a ﬁrst-order Markovian process\npr (s |y, T , J ) :=\nTY\nt=1\npr (st |st−1) , (6.2)\nwiths0 :=“init”, i.e. the initial state used to model the probabilityof each state to be the ﬁrst in the\nsequence of states; andpr (x | y, s, T , J ) is modelled as composed of independent bilingual phrases\npr (x |y, s, T , J ) :=\nTY\nt=1\npr (x(st) |y(st ), s t ) . (6.3)\nClearly, the above modelling assumptions lead to a phrase-based HMM-like model. Its set of states is\nthat of all possible bilingual segments, while its set of transitions includes all pairs⟨q′, q ⟩in which the\nstate (segment)q is a successor ofq′,q ∈ Succ(q′). For each stateq, we will have a different emission\nprobability for each target segmenty(q).\nJAF-DSIC-UPV 121\nChapter 6. A phrase-based hidden Markov model for monotone machine translation\ninit\n1111 1112 11131114 1211 1212 1213 12141311 1312 1313 1314\n1415\n2222 2223 22242322 2323 2324\n2425\n2233 22342333 2334\n2435\n22442344\n24452455\n3322 3323 3324\n3425\n3333 3334\n3435\n3344\n3445 34554425 4435 4445 4455\nFigure 6.1:Directed, multi-stage graph representing all possible bilingual segmentations for an input sentence of length4and an output\nsentence of length5. Each node deﬁnes a different segment; the ﬁrst two digits ofthe node label are the segment limits in the input\nsentence, while the other two digits correspond to the output sentence.\n122 JAF-DSIC-UPV\n6.3. Forward and backward probabilities\nFor efﬁciency and simplicity, we will further assume in thischapter that both initial and transition\nstate probabilities are uniformly distributed; hence, foreachq and q′, including “init” forq′,\npr (q | q′) :=\n(\n1\n|Succ(q′)| ifq ∈ Succ(q′)\n0 otherwise\n(6.4)\nAlso,T is assumed to be uniformly distributed,\npr (T | y, J ) := 1\nmin(I , J ) (6.5)\nand the phrase translation probabilities are assumed to be stored in a single, state-independent table\npr (x(st ) |y(st ), s t) := p( x(st ) |y(st)) .\nUsing the above assumptions, our model (6.1) can be rewritten as follows\npθ (x |y, J ) := 1\nmin(J, I )\nX\ns\n|s|≤min(J,I)\n|s|Y\nt=1\np(x(st) |y(st ))\n|Succ(st−1)| . (6.6)\nThe vector of parameters governing this model only includesa table of phrase translation probabilities,\nΘ = {p(u |v) : ( u, v) bilingual phrase} .\n6.3 Forward and backward probabilities\nAs usual with HMMs (see Section 1.1.5 Chapter 1), we will discuss here the so-calledforwardand\nbackwardprobabilities for efﬁcient computation of the model probabilities, as given in Eq. (6.6). To ﬁx\nideas, considerx and y to be two arbitrary sentences for which we have to compute Eq.(6.6). Given\na segmentation length and position,T and t, and a stateq, the forward probability is deﬁned as the\nfollowing preﬁx joint probability\nαT\ntq := p θ (x(st\n1), s t = q |y, T ) ,\nwhere st\n1 is any partial segmentation, from positions1 tot, such thatst = q. This probability can be\nrecursively computed by dynamic programming, using the so-calledforward recurrence,\nαT\ntq =\nX\nq′ : q∈Succ(q′)\nαT\nt−1q′ pr (q |q′) p(x(q) |y(q)) =\nX\nq′ : q∈Succ(q′)\nαT\nt−1q′\np(x(q) |y(q))\n|Succ(q′)| , (6.7)\nwith the base caseαT\ntq = 1 fort = 0 and q =“init”;0 otherwise. Note that in Eq. (6.7), we have\ndecomposed the caseαT\ntq in terms of a smaller caseαT\nt−1 q′ .\nThe backward probability also depends on a given segmentation lengthT and positiont; and a state\nq. It is deﬁned as the following sufﬁx probability\nβT\ntq := p θ (x(sT\nt+1) |y, T , s t = q) ,\nwhere sT\nt+1 is any partial segmentation, from positionst+1 toT , that might follow the stateq in position\nt. As before, it can be efﬁciently computed by dynamic programming, using a “reverse” version of the\nforward recurrence calledbackward recurrence,\nβT\ntq =\nX\nq′∈Succ(q)\nβT\nt+1 q′ pr (q′ |q) p( x(q′) |y(q′)) =\nX\nq′∈Succ(q)\nβT\nt+1 q′\np(x(q′) | y(q′))\n|Succ(q)| , (6.8)\nJAF-DSIC-UPV 123\nChapter 6. A phrase-based hidden Markov model for monotone machine translation\nwith the base caseβT\nT q = 1 for anyterminal stateq = ( ·, I , ·, J ) and anyt; and0 otherwise. Note\nthat similarly to the forward recurrence, in Eq. (6.8), we have decomposed the caseβT\ntq in terms of the\nsimpler caseβT\nt+1 q′ .\nFinally, Eq. (6.6) can be computed using (6.7) as\npθ (x |y) = 1\nmin(J, I )\nX\nT\nX\nq=(·,I,·,J )\nαT\nT q ,\nor using (6.8) as\npθ (x |y) = 1\nmin(J, I )\nX\nT\nβT\n0“init” .\nAn efﬁcient implementation of both recurrences requires two tables ofO(I J min(J, I )) values and\na computational complexity ofO(I 2J 2 min(J, I )).\n6.4 Decoding and Viterbi recurrence\nThe Viterbi recursion, efﬁciently computes the most likelystate sequence that can emit a given output\nsequence. We introduce this recursion here as a prelude to the search recurrence. This recursion is\ndeﬁned as the most likely state sequence of lengtht that ends in the stateq, i.e.\nδq t = max\nst\n1 : st =q\n{pθ (x(st\n1), st\n1 |y)} , (6.9)\nnote that the last statest is required to beq.\nThe Viterbi recursion in Eq. (6.9) is efﬁciently computed bythe following recurrence\nδq t = max\nq′\nn\np(q |q′) p(x(q) |y(q)) max\nst−1\n1 : st−1=q′\n{pθ (x(st−1\n1 ), st−1\n1 |y(st−1\n1 ))}\no\n, (6.10)\nwhere by applying the Viterbi’s deﬁnition yields\nδq t = max\nq′\n{p(q |q′) p(x(q) |y(q))δq′ t−1} = max\nq′\np(x(q) |y(q))\n|Succ(q′)| δq′ t−1\nﬀ\n. (6.11)\nFinally, the probability of the Viterbi’s segmentation is given by\npθ (ˆs, x |y) = max\nT ,q=(·,J,·,I)\nδq,T . (6.12)\nAs usually, tracing back the decisions made during the maximisation process yields the maximum\nsegmentation,ˆs.\nThe Viterbi recursion shares the same asymptotic requirements than that of the forward and back-\nward recursions.\n6.4.1 The decoding process\nThe decoding process is stated as the problem of ﬁnding the most likely target sentenceˆy for given a\nsource sentencex. According to our model the decoding problem is stated as\nˆy = arg max\ny\n{pθ (x |y) p( y)} . (6.13)\n124 JAF-DSIC-UPV\n6.4. Decoding and Viterbi recurrence\nDuring the decoding process, an additional problem with respect to the Viterbi recursion must be\nkept in mind, that is to say, the target sentencey is unknown. Therefore, we want to ﬁnd the following\nmaximum probability\npθ (x, ˆy) = max\ny\n8\n<\n:\nmin{I,J }X\nT =1\nX\ns\npθ (x, sT\n1 , T |y) p(y)\n9\n=\n; , (6.14)\nsince the fundamental equation of machine translation introduced in Eq. (1.102) Chapter 1 requires a\nlanguage model.\nIn order to solve Eq. (6.14) it is usually assumed a Viterbi-like approach approximating each sum\nby their maximum value as follows,\nˆ p = pθ (x, ˆy) = max\ny ,sT\n1 ,T\n{pθ (x, sT\n1 , T |y) p( y)} , (6.15)\nwhere recall thatˆy stands for the target sentence that maximises this probability.\nProvided that we only usen-gram language models in this thesis, we further assume thatthe lan-\nguage model probability of a given target phrasep(y(st) |y(st−1\n1 )) only depends on the(n − 1)-most\nrecent words, i.e.\np(y(st ) |y(st−1\n1 )) := p( y(st) | suf\nn−1\n(y(st−1\n1 ))) , (6.16)\nwhere suf n−1(· · ·) stands for the(n − 1)-most recent words.\nIn order to perform this maximisation, we deﬁne a decoding recurrence\nσq,v (x) = max\nt,st\n1,y (st\n1)\nst =q, suf |v|(y (st\n1))=v\n˘\npθ (x(st\n1), st\n1 |y(st\n1)) p( y(st\n1))\n¯\n, (6.17)\nwhere bysuf |v |(y(st\n1)) = v we denote the fact that the sufﬁx ofy(st\n1) must be equal tov. We further\nassume that ifv = ⋆ then this constraint is ignored, i.e.,\nσq,⋆ (x) = σq (x) = max\nt,st\n1,y (st\n1)\nst =q\n˘\npθ (x(st\n1), st\n1 |y(st\n1)) p( y(st\n1))\n¯\n. (6.18)\nNote thatσq,v can be recursively expressed in terms of a more basic case of itself as follows\nσq,v = max\nq′,v ′,h\nsuf |v|(v ′h)=v ,|v ′h|≥|v |\n{p(q |q′) p( x(q) |v′) p( v′ |h) σq′,h } , (6.19)\nwhere note thatv′ plays the role ofy(st) and q the role ofst ; and wherep(q |q′) is uniformly dis-\ntributed as shown in Eq. (6.4).\nIn this way the probability of the desired target string is computed using the search recurrence as\nfollows\npθ (x, ˆy) = max\nI\n{ max\nq=(·,I,·,J )\n{σq,⋆ }} . (6.20)\nAs usually, tracing back the decisions made during the recurrence computation provides the optimal\nsolution deﬁned in Eq. (6.15).\nHowever, although the recursionσq,v speeds up the search problem, it is still a hard problem. For\nthis reason, we still need to perform an approximate decoding in which we use a maximum number of\nhypothesis for each stateq, sayM , and also a beam pruning [Wang and Waibel, 1997, 1998]. That is\nto say, instead of using Eq. (6.19), we use the following approximated version\nσ⋆\nq,v =\n⋆\nmax\nq′,v ′,h\nsuf |v|(v ′h)=v ,|v ′h|≥|v |\n{p(q |q′) p( x(q) |v′) p( v′ |h) σ⋆\nq′,h } , (6.21)\nJAF-DSIC-UPV 125\nChapter 6. A phrase-based hidden Markov model for monotone machine translation\nwhere\n⋆\nmax stands for an approximate version ofmax where we have applied several heuristics, such\nas beam search or histogram pruning.\n6.5 Maximum likelihood estimation\nAs discussed in section 6.2, the unknown vector of parameters of our phrase-based HMM model\nonly includes a table of phrase translation probabilities (see Eq. (6.2)). We will describe here its\nEM-based maximum likelihood estimation with respect to a collection of training translation pairs\n{(x1, y1), . . . , (xN , yN )}.\nThe log-likelihood function ofθ is:\nLL(θ ) =\nX\nn\nlog pr (xn |yn )\n=\nX\nn\nlog 1\nmin(Jn , I n)\nX\ns\n|s|≤min(Jn,In )\n|s|Y\nt=1\np(xn(st ) | yn(st ))\n|Succ(st−1)| (6.22)\nRemember from Section 1.1.4 in Chapter 1, that the EM algorithm maximisesLL(·) by iteratively\nmaximising a variational functionL(q, θ ) through the application of two basic steps in each iteration:\nthe E(xpectation) step and the M(aximisation) step.\nLetθ (k−1) be a guess of the optimal parameters obtained from previous iterations; then, in this case,\nthe E step requires the computation, for each pair(xn, yn ), of the sample versions of (6.7) and (6.8),\nas well as the following joint probability\nξT\nntq′q := p θ (k−1) (xn , s t−1 = q′, s t = q |yn , T ) ,\nwhich can be efﬁciently computed as\nξT\nntq′q = αT\nnt−1q′ p(x(q) |y(q)) βT\nntq\npr (xn |yn) |Succ(q′)| . (6.23)\nOn the other hand, the M step re-estimates the table of phrasetranslation probabilities,\np(k)(u |v) = N (k−1)(u, v)P\nu′ N (k−1)(u′, v) , (6.24)\nwhere N (k−1)(u, v) is the expected number of occurrences of the the pair(u, v); i.e.\nN (k−1)(u, v) =\nX\nn\n1\nmin(Jn , I n)\nX\nq′ q\nX\nT t\nξT\nntq′q δnq (u, v) , (6.25)\nwithδnq (u, v) deﬁned as1 ifu = xn (q) and v = yn(q);0 otherwise.\n6.6 Experiments\n6.6.1 Corpora\nThe proposed phrase-based hidden Markov model was assessedon two different corpora: the EU T RANS -\nI dataset [Amengual et al., 2000] and the Europarl-10. The former dataset comprises12 000 bilingual\n126 JAF-DSIC-UPV\n6.6. Experiments\nE UTRANS -I Train Set Test Set\nSpanish English Spanish English\nsentences 10K 2K\navg. length 12.9 13 .0 12 .7 12 .6\nvocabulary 686 513 611 468\nrunning words 97.2K 99.2K 35.0K 35.6K\nperplexities (3-gram) - - 5.4 3 .8\nTable 6.1:Basic statistics of the Spanish-English EU T RANS -I task, whererunning\nwords denotes the total amount of word occurrences.\nE UROPARL -10 Train set Test set\nEnglish Spanish English Spanish\nsentences 76, 996 5 , 000\navg. length 7.01 7 .0 7 .2 7 .0\nvoc. size 16K 22K 4.1K 5.2K\nrunning words 546K 540K 35.8K 3.91M\nperplexities (3-gram) - - 77.6 86 .8\nTable 6.2:Basic statistics of the EUROPARL -10 corpus whererunning wordsdenotes\nthe total amount of word occurrences.\nsentence pairs from a limited-domain Spanish-English machine translation application for human-to-\nhuman communication situations in the front-desk of a hotel. It has also been used in previous Chap-\nter 5. The latter comprises all the sentences of the English-Spanish Europarl-v2 [Koehn, 2005] with\nlength equal or less than10. We have randomly selected5K sentences for testing. Some basic statistics\nare shown in Table 6.1 and 6.2.\n6.6.2 Results\nTwo basic experiments were carried out with the Eutrans-I corpus. In the ﬁrst experiment, we used\nMoses [Koehn et al., 2007] to obtain a baseline of the corpus using an inverse phrased-based probabil-\nity model and a4-gram language model, i.e. without MERT training, as discussed in Section 6.4. For\nthis experiment, we have used a maximum phrase length of15 words and we have used only the inverse\ntranslation model and the language model, i.e. we have not perform the MERT training. For evalu-\nating the performance we usedword error rate(WER) and bilingual evaluation understudy(BLEU)\nmeasures. We obtained a WER of7.7% and a BLEU score of89.1%. These are relatively good re-\nsults since, recall that in general low values of WER and highvalues of BLEU are a clear indication\nof high quality translations. Additionally, we compared this Moses baseline with the proposed phrase-\nbased hidden Markov model to better train the phrase translation table. We proceeded as in the baseline\nmodel although, now, the phrase table obtained before was used to initialise the EM algorithm proposed\nin Section 6.5 for parameter training in accordance with criterion in Eq. (6.22). In this case, we obtained\na WER of 7.8% and BLEU of 88.5%.\nObviously, the result obtained with our model was not betterthan that obtained with the baseline\napproach. In analysing the phrase table provided by our model, we found that the EM algorithm prefers\nJAF-DSIC-UPV 127\nChapter 6. A phrase-based hidden Markov model for monotone machine translation\nlong to short phrases; that is, given a target phrase, long source phrases are favoured with higher prob-\nabilities. To empirically check this hypothesis, we repeated the two basic experiments described above\nby ﬁrst discarding training phrases longer that a given maximum threshold. For the most restrictive\nthresholds, however, phrases longer than the threshold were not discarded so as to ensure full coverage\nof the training data. The results are shown in Figure 6.2 in terms of BLEU.\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n1 2 4 8 15\nbaseline\nPHMM\nmaximum phrase length\nBLEU (%)\nFigure 6.2:BLEU (%) as a function of the maximum phrase length threshold, for the\nbaseline approach and our phrase-based HMM (PHMM).\nThe results in Figure 6.2 conﬁrm our hypothesis on the bias tolong phrases in our model. A possible\nsolution to this problem is to reﬁne our phrase-based HMM with inclusion of length models to penalise\nlong phrases.\nIn the case of the Europarl-10 corpus, we carried out one experiment similar to the ﬁrst experiment\nwith the Eutrans-I corpus. In this case we ﬁxed the phrase length to7. The Moses baseline scored\n50.0% points of WER and32.7% of BLEU; whilst the proposed model scored54.6% points of WER\nand 26.7% of BLEU; which clearly worsens the baseline. It seems that this negative result is also due\nto the overﬁtting tendency of the MLE, since we did not propose a smoothing model.\n6.7 Conclusions\nA phrase-based hidden Markov model has been proposed for statistical machine translation. We have\ndescribed the forward and backward recurrences for efﬁcient computation of the model and its EM-\nbased parameter re-estimation algorithm. Empirically results have been reported comparing the pro-\nposed model with a baseline system. It has been found that ourmodel is biased to long phrases and\n128 JAF-DSIC-UPV\n6.7. Conclusions\ntends to quickly overﬁt the system. Due to this overﬁtting the systems does not outperforms the baseline\nsystem.\nWe have identiﬁed some problems. Firstly, a classical HMM isnot fully adequate for processing\nboth input and output strings, since the handling of the segmentation variables requires speciallised\nstates. Secondly, the computational time is too expensive to make this model useful with big corpora.\nActually, we need a matrixO(I J min{I , J }); however the third dimensionmin{I , J } accounts for\nthe number of phrases that have been used and does not introduce any relevant statistical information to\nthe training process. Finally, the model tends to get overﬁtted biasing the long phrases.\nIn the following chapter, we extend and modify this model by making use of ahidden semi-Markov\nmodel (HSMM) formalism. This allow us to amend the three previously mentioned problems.\nJAF-DSIC-UPV 129\n\nBibliography\nBibliography\nJuan C. Amengual, José M. Benedí, Asun-\nción Castano, Antonio Castellanos, Víctor M.\nJiménez, David Llorens, Andrés Marzal,\nMoisés Pastor, Federico Prat, Enrique Vidal,\nand Juan M. Vilar. The EuTrans-I speech trans-\nlation system.Machine Translation, 15:75–\n103, 2000.\nJ. Andrés-Ferrer and A. Juan. A phrase-based hid-\nden markov model approach to machine trans-\nlation. InProceedings of New Approaches\nto Machine Translation, pages 57–62, January\n2007. ISBN 978-90-814861-0-1.\nA. Birch, C. Callison-Burch, Miles M. Osborne,\nand P. Koehn. Constraining the phrase-based,\njoint probability statistical translation model.\nIn Proceedings on the Workshop on Statisti-\ncal Machine Translation, pages 154–157, New\nYork City, New York, USA, June 2006. Associ-\nation for Computational Linguistics.\nP. F. Brown et al. The Mathematics of Statis-\ntical Machine Translation: Parameter Estima-\ntion. Computational Linguistics, 19(2):263–\n311, 1993.\nA. P. Dempster, N. M. Laird, and D. B. Rubin.\nMaximum likelihood from incomplete data via\nthe EM algorithm.J. Royal Statist. Soc. Ser. B,\n39(1):1–22, 1977.\nP. Koehn. Europarl: A parallel corpus for statis-\ntical machine translation. InProc. of the MT\nSummit X, pages 79–86, September 2005.\nP. Koehn, F.J. Och, and D. Marcu. Statis-\ntical phrase-based translation. InProc. of\nNAACL’03 , pages 48–54, Morristown, NJ,\nUSA, 2003. Association for Computational\nLinguistics. doi: http://dx.doi.org/10.3115/\n1073445.1073462.\nP. Koehn et al. Moses: Open source toolkit\nfor statistical machine translation. InProc.\nof ACL’07: Demo and Poster Sessions, pages\n177–180, Morristown, NJ, USA, June 2007.\nAssociation for Computational Linguistics.\nD. Marcu and W. Wong. A phrase-based, joint\nprobability model for statistical machine trans-\nlation. InProc. of EMNLP’02, pages 133–139,\nMorristown, NJ, USA, July 2002. Association\nfor Computational Linguistics.\nF.J. Och and H. Ney. The alignment template ap-\nproach to statistical machine translation.Com-\nputational Linguistics, 30(4):417–449, 2004.\nY . Wang and A. Waibel. Decoding algorithm in\nstatistical translation. InProc. of ACL’97, pages\n366–372, Morristown, NJ, USA, July 1997.\nMorgan Kaufmann / Association for Computa-\ntional Linguistics.\nY . Wang and A. Waibel. Fast decoding for statisti-\ncal machine translation. InProc. of ICSLP’98,\npages 2775–2778, October 1998.\nJAF-DSIC-UPV 131\nBibliography\n132 JAF-DSIC-UPV\nChapter 7\nA phrase-based hidden semi-Markov model for\nmonotone machine translation\n“ Then there was the man who drowned crossing a stream with an average depth of six inches.”\nW. I. E. GATES\nContents\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\n7.2 The phrase-based hidden semi-Markov model . . . . . . . . . . .. . 134\n7.3 Recurrences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.3.1 Forward recurrence . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.3.2 Backward recurrence . . . . . . . . . . . . . . . . . . . . . . . 139\n7.3.3 Viterbi recursion . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.4.1 Fractional counts . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.4.2 Baum-Welch training . . . . . . . . . . . . . . . . . . . . . . . 141\n7.4.3 Viterbi training . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n7.4.4 The model smoothing . . . . . . . . . . . . . . . . . . . . . . . 143\n7.5 Decoding Recurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n7.6 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.6.1 Classical phrase-based models. . . . . . . . . . . . . . . . . . .145\n7.6.2 Log-linear models. . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n133\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nc1 c2 c3\nx1 x2 x3 x4\ny1 y2 y3 y4 y5\nFigure 7.1:A generative example of the hidden semi-Markov model approach to ma-\nchine translation, in which a source stringx4\n1 is translated to a target stringy5\n1 through\na segmentation of both sentences into4 concepts.\n7.1 Introduction\nIn previous chapter, we introduced a novel approach to machine translation based on the hidden Markov\nmodel. One drawback of this model, is the computational complexity associated with its training al-\ngorithm. In this section, by making use of the hidden semi-Markov formalism, we successfully amend\nthe computation problem of the previous model. With this newformalism, we properly deﬁne a phrase-\nbased model eligible for being theoretically expanded without the problems derived from the heuristi-\ncally computed phrase dictionaries. Finally, and in order to avoid the harmful overﬁtting problems, we\nresort to a smoothing word-based IBM model 1.\nWe begin this chapter with the description of our new proposal in Section 7.2. Likewise to classical\nHSMM, in Section 7.3 the well-known forward and backward recurrences are described into detail. The\ntraining algorithms are explained in Section 7.4. The practical behaviour of the PBHSMM is analysed\nin Section 7.6. Afterwards, concluding remarks are gathered in Section 7.7.\n7.2 The phrase-based hidden semi-Markov model\nInspired on the HSMM described in Section 1.1.6 Chapter 1, wedeﬁne here ourphrase-based hidden\nsemi-Markov model (PBHSMM)for monotone machine translation [Andrés-Ferrer and Juan,2009]. Let\nx ∈ X ⋆ be the source sentence andy ∈ Y ⋆ the target sentence, we model the conditional translation\nprobability,pr (x |y, J ) by assuming that the monotonic translation process has beencarried out from\nleft to right in segments of words orphrases. For this purpose, both sentences should be segmented\nin the same amount of phrases. Figure 7.1, depicts an exampleof a possible monotonic bilingual\nsegmentation in which the source sentence comprises4 words,x4\n1; whereas the target sentence is made\nup of5 words,y5\n1. Note that each bilingual phrase forms aconcept[Brown et al., 1993]; for instance\nc1, c2 and c3 are concepts in ﬁg. 7.1. In order to represent the segmentation process, we use two\nsegmentation variables for both source,l, and target,m, sentences.\nTo better understand our monotone translation modelpθ (x |y, J ), it is ﬁrst convenient to fully\nunderstand how the segmentation process is represented using the formerly mentioned segmentation\nvariables: the source segmentation variable,l; and the target segmentation variable,m.\nOn the one hand, the target segmentation variablem stores each target segment length at the posi-\ntion at which the segment begins. Therefore, if the target segmentation variablem has a value greater\n134 JAF-DSIC-UPV\n7.2. The phrase-based hidden semi-Markov model\nthan0 at positioni, then a segment with lengthmi starts at such positioni. Note that this notation dif-\nfers from that of Chapter 6, since now the length is speciﬁed at the segment boundaries. For instance,\nthe target segmentation represented in Figure 7.1 is given by m = m5\n1 = (2 , 0, 1, 2, 0). Therefore, val-\nues for the segmentation variable such asm = (2 , 1, 0, 3, 0) orm = (2 , 2, 0, 3, 0),are out-of-domain,\nand, hence, invalid. The Table 7.1 enumerates in the second column all the target segment variable\ndomain for the case of a target sentence of5 words. In the same table, the third column corresponds to\nthe induced target segmentation for the value ofm speciﬁed in the ﬁrst column. Note that the domain\nof the target segmentation ranges among all the possible segmentation lengths.\nOn the other hand, the source counterpart of the target segmentation variable is the source seg-\nmentation variablel. The source segmentation random variable accounts for the length of eachsource\nsegmentat the position at which its correspondingtarget segmentbegins. If the source segmentation\nvariablel has a value greater than0 at positioni, then the length of the source segment corresponding\nto the target phrase that starts at positioni, isli . Recall that the length of the target segment starting at\npositioni ismi. For instance, in Figure 7.1 the source segmentation variable isl = l5\n1 = (2 , 0, 1, 1, 0).\nTable 7.1 enumerates all possible values of both segmentations variables,m and l for a source\nsentence of4 words and a target sentence of5; and the segmentation they induce in both source and\ntarget sentences. It is valuable to mention, that the possible values ofl depend on bothm and J . There\nis only one bilingual segmentation with unit length; but there are12,18 and 4 segmentations of length\n2,3 and 4, respectively. Note that in the special case in whichm splits the target sentencey5\n1 in5\nsegments; there is no possible value forl and no segmentation is induced inx4\n1.\nNow, we can mathematically deﬁne our inverse translation model, depicted in Figure 7.1, as a full\nexploration of all segmentations\npr (x |y, J ) =\nX\nm \nX\nl\npr (x, l, m |y, J ) , (7.1)\nwhere m ranges among all the possible target segment values fory, andl ranges only on those values\nthat are in accordance withm and J .\nThe complete model in Eq. (7.1) is decomposed as follows\npr (x, l, m |y, J ) = pr (m |y, J )pr (l | m, y, J )pr (x |l, m, y, J ) . (7.2)\nAll the probabilities in Eq. (7.2) are being decomposed left-to-right. We explain into detail the\ndecomposition of the target segment length probability model since the extension of this technique\nto the source length and the emission probabilities is straightforward and can make the discussion\ncumbersome.\nTo simplify notation, we need to give some additional deﬁnitions, before decomposing the target\nlength probabilitypr (m | y, J ). Given a target segmentation variable, saym, we deﬁne its preﬁx\ncounterpart,¯m as follows\n¯mi =\niX\nk=1\nmk i = 0 , 1, . . . , I . (7.3)\nSimilarly, for the source segmentation variablel we can deﬁne its preﬁx counterpart¯l as follows\n¯\nli =\niX\nk=1\nlk i = 0 , 1, . . . , I . (7.4)\nFor instance, in Figure 7.1, the preﬁx segments lengths are¯m = ¯m5\n0 = (0 , 2, 2, 3, 5, 5) and ¯l = ¯l\n5\n0 =\n(0, 2, 2, 3, 4, 4), for target and source segmentation variables respectively.\nJAF-DSIC-UPV 135\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\n# segments m y5\n1 segments\nl x4\n1 segments\n1 (5, 0, 0, 0, 0) y5\n1\n(4, 0, 0, 0, 0) x4\n1\n2\n(4, 0, 0, 0, 1) y4\n1, y 5\n(3, 0, 0, 0, 1) x3\n1, x4\n(2, 0, 0, 0, 2) x2\n1, x4\n2\n(1, 0, 0, 0, 3) x1, x4\n2\n(3, 0, 0, 2, 0) y3\n1, y5\n4\n(3, 0, 0, 1, 0) x3\n1, x4\n(2, 0, 0, 2, 0) x2\n1, x4\n2\n(1, 0, 0, 3, 0) x1, x4\n2\n(2, 0, 3, 0, 0) y2\n1, y5\n3\n(3, 0, 1, 0, 0) x3\n1, x4\n(2, 0, 2, 0, 0) x2\n1, x4\n2\n(1, 0, 3, 0, 0) x1, x4\n2\n(1, 4, 0, 0, 0) y1, y5\n2\n(3, 1, 0, 0, 0) x3\n1, x4\n(2, 2, 0, 0, 0) x2\n1, x4\n2\n(1, 3, 0, 0, 0) x1, x4\n2\n3\n(3, 0, 0, 1, 1) y3\n1, y 4, y 5\n(2, 0, 0, 1, 1) x2\n1, x3, x4\n(1, 0, 0, 2, 1) x1, x3\n2, x4\n(1, 0, 0, 1, 2) x1, x2, x4\n3\n(2, 0, 2, 0, 1) y2\n1, y4\n3, y 5\n(2, 0, 1, 0, 1) x2\n1, x3, x4\n(1, 0, 2, 0, 1) x1, x3\n2, x4\n(1, 0, 1, 0, 2) x1, x2, x4\n3\n(2, 0, 1, 2, 0) y2\n1, y 3, y5\n4\n(2, 0, 1, 1, 0) x2\n1, x3, x4\n(1, 0, 2, 1, 0) x1, x3\n2, x4\n(1, 0, 1, 2, 0) x1, x2, x4\n3\n(1, 3, 0, 0, 1) y1, y4\n2, y 5\n(2, 1, 0, 0, 1) x2\n1, x3, x4\n(1, 2, 0, 0, 1) x1, x3\n2, x4\n(1, 1, 0, 0, 2) x1, x2, x4\n3\n(1, 2, 0, 2, 0) y1, y3\n2, y5\n4\n(2, 1, 0, 1, 0) x2\n1, x3, x4\n(1, 2, 0, 1, 0) x1, x3\n2, x4\n(1, 1, 0, 2, 0) x1, x2, x4\n3\n(1, 1, 3, 0, 0) y1, y 2, y5\n3\n(2, 1, 1, 0, 0) x2\n1, x3, x4\n(1, 2, 1, 0, 0) x1, x3\n2, x4\n(1, 1, 2, 0, 0) x1, x2, x4\n3\n4\n(2, 0, 1, 1, 1) y2\n1, y 3, y 4, y 5\n(1, 0, 1, 1, 1) x1, x2, x3, x4\n(1, 2, 0, 1, 1) y1, y3\n2, y 4, y 5\n(1, 1, 0, 1, 1) x1, x2, x3, x4\n(1, 1, 2, 0, 1) y1, y 2, y4\n3, y 5\n(1, 1, 1, 0, 1) x1, x2, x3, x4\n(1, 1, 1, 2, 0) y1, y 2, y 3, y5\n4\n(1, 1, 1, 1, 0) x1, x2, x3, x4\n5 (1, 1, 1, 1, 1) y1, y 2, y 3, y 4, y 5 ∅ ∅\nTable 7.1:A full domain speciﬁcation for both segmentation variables, m and l, in\nthe case of a source sentence of4 words and a target sentence of5 words. For better\nunderstanding of these variables, the induced segmentation in both source and target\nsentences is also provided in columns3 and 5. Although there is a possible segmenta-\ntion of the target sentencey into5 segments (last row), it is not the case for the source\nsentencex.\n136 JAF-DSIC-UPV\n7.2. The phrase-based hidden semi-Markov model\nThe probability of the target segmentation variable is given by\npr (m | y, J ) =\nIY\ni=1\npr (mi | mi−1\n1 , y, J ) . (7.5)\nAt ﬁrst stage, we assume that each partial probability in Eq.(7.5) does not depend neither ony, nor on\nboth lengths (I and J ) and, hence, the probabilitypr (mi | mi−1\n1 , y, J ) is modelled as follows\npr (mi |mi−1\n1 , y, J ) :=\n(\np(mi) ¯ mi−1 + 1 = i, m i > 0\n1 ¯ mi−1 + 1 ̸= i, m i = 0 (7.6)\nNote that the ﬁrst case in Eq. (7.6) is satisﬁed by the positionsi in which a segment begins, whereas the\nother case is satisﬁed by the positions that lay inside a segment (execept for the boundaries).\nFinally the segmentation probability can be expressed as follows\npr (m |y, J ) :=\nY\ni∈Z(m )\n1\nY\ni̸∈Z(m )\np(mi) (7.7)\nwhere Z(m) or simplyZ stands for the set of positionsi for whichmi is0. For instance, in the\nexample in Figure 7.1,Z is instanced toZ(m) = {2, 5}.\nProvided that one of the two products in Eq. (7.7) simpliﬁes to 1, the segmentation probability is\nexpressed as\npr (m |y, J ) :=\nY\ni̸∈Z\np(mi) . (7.8)\nSince explicitly showing these details makes the discourseto be awkward, we will henceforth omit them\nabusing of notation whenever it does not entail confusion. Hence, we will use equations similar to the\nfollowing\npr (m | y, J ) :=\nY\nt\np(mt) , (7.9)\nwhere we have explicitly omitted thatt ∈ Z , but we keep the subindext instead ofi for subtly high-\nlighting this modelling process. Note that decomposition is similar to the usual state probability de-\ncomposition used in hidden semi-Markov models (see Section1.1.6 Chapter 1).\nSimilarly to the target segmentation modelling, the sourcesegmentation yields the following equa-\ntion\npr (l |m, y, J ) :=\nY\nt\np(lt |mt) , (7.10)\nwhere we have assumed that thet-th source segment lengthlt depends only on the correspondingt-th\ntarget segment lengthmt; and hence, it is independent of the remaining target segment lengths as well\nas independent of the previoust − 1 source segment lengthslt−1\n1 .\nFinally, knowing the segmentation variables, the emissionprobability is also decomposed left-to-\nright yielding\npr (x |l, m, y, J ) :=\nY\nt\np(x(t) |y(t)) , (7.11)\nwhere we have assumed that the emission of the source phrasex(t) only depends ony(t); and where\nx(t) stands forx\n¯lt\n¯lt−1+1 and y(t) foryt+mt −1\nt ; i.e., thet-th “emitted” source phrase and its respective\nt-th target phrase. Note that¯\nlt is equal to¯lt−1 + lt provided that positiont is a starting position for a\ntarget segment.\nJAF-DSIC-UPV 137\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nSummarising, the proposed (complete) conditional translation model is deﬁned as follows\npr (x, l, m |y, J ) :=\nY\nt\np(mt ) p(lt |mt) p( x(t) |y(t)) , (7.12)\nand, hence, the incomplete model introduced in Eq. (7.1) is parametrised as follows\npθ (x |y, J ) :=\nX\nm \nX\nl\npθ (x, l, m | y) , (7.13)\nwhere pθ (x, l, m | y) is given by the following expression\npθ (x, l, m |y) =\nY\nt\np(mt ) p(lt |mt) p( x(t) | y(t)) , (7.14)\nwith the following parameter setθ\nθ = {p(m), p(l |m), p(u |v) | ∀l > 0, ∀m > 0, ∀u ∈ X ⋆ , ∀u ∈ Y ⋆ } . (7.15)\nIn thephrase-based hidden semi-Markov model (PBHSMM)deﬁned in this section, we can under-\nstand each target phrasey(t) as the “state” of a HSMM in which the source phrasex(t) is emitted. Note\nthat the output sentence probability,p(y), plays the role of the state sequence probability in a HSMM.\nSuch probabilityp(y), is better modelled by a speciﬁc language model such as an-gram, due to the\nnature of such variable. Obviously this is not a pure HSMM in which we have a latent state variable.\nThe omission of this latent variable is more an assumption than a requirement. Recall that in Figure 7.1\nwe have depicted each bilingual phrase pair being emitted bya conceptwhich could represent a latent\nstate. We have proposed this extension in the conclusion section as a future research line.\nSince the model assumes that the segmentation variables arenot given in the training data, some\napproximate inference algorithm such as the EM (see Section1.1.4 Chapter 1) is needed. In the follow-\ning section, the standard recurrences needed in HSMM training are adapted to the proposed translation\nmodel.\n7.3 Recurrences\nAs it is common in hidden Markov models and speciﬁcally in hidden semi-Markov models, some\nhelpful recurrences are deﬁned in order to efﬁciently obtain the answer to some common questions. We\nfocus, in this section, on3 selected questions among many others:\n• Which is the probability for a given bilingual pair(x, y)?\n• Which is the best segmentation for a given bilingual pair(x, y)?\n• Which is the best parameter setθ given a training set{(xn , yn)}N\nn=1?\n7.3.1 Forward recurrence\nThe forward recurrenceαtl is deﬁned as the following preﬁx probability\nαtl = αtl (x, y) := p θ (xl\n1, ¯lt = l, ¯mt = t |y) , (7.16)\nwhere the events¯lt = l and ¯mt = t, imply that a source (or target) phrase ends at positionl (ort) in\nthe input (or output, respectively).\n138 JAF-DSIC-UPV\n7.3. Recurrences\nThe preﬁx probability in Eq. (7.16) is recursively computedas follows\nαtl =\n8\n>\n>\n>\n>\n<\n>\n>\n>\n>\n:\n1 t = 0 , l = 0\nt−1X\nt′=0\nl−1X\nl′=0\nαt′l′ p(t′ − t) p(l′ − l |t′ − t) p(xl\nl′+1|yt\nt′+1) 0 < t ≤ I\n0 < l ≤ J\n0 otherwise\n(7.17)\nIn order to compute the forward recurrence in Eq. (7.16), a matrix ofO(I J ) elements is needed.\nThe computational complexity required to ﬁll such a matrix is O(I 2J 2). However, if the phrases are\nconstrained to a maximum source and target phrase length,L and M respectively, then the complexity\nis reduced toO(I J M L).\nFurthermore, a detailed analysis of the forward algorithm unveils that not all the elements ofαtl\nmust be computed. The elements excluded do not verify one of the following requirements: that both\nsource and target sentences must be segmented in the same amount of phrases; or that both source\nand target phrases must be smaller thanL and M respectively. For instance, in Figure 7.2, we have\nhighlighted which elements must be computed for two sentences of length20 and 22. The remaining\nvalues are useless, and we should save the time needed to compute them. Note that the longer the sen-\ntences are, the more effective this optimisation is. Speciﬁcally, in the previous example the number of\nelements to compute approximately account for the50% of the total values. This reduces the computa-\ntional complexity in a ratio of2. Additionally, it is possible to add some heuristics to the process such\nas beam pruning [Wang and Waibel, 1997, 1998].\nFinally, the answer to the ﬁrst question, i.e., how to compute the probability of a given pair, is given\nby means of the forward recurrence as follows\npθ (x |y) = αIJ . (7.18)\n7.3.2 Backward recurrence\nThe backward recurrenceβtl is deﬁned as the sufﬁx probability\nβtl = βtl (x, y) = pθ (xJ\nl+1|¯lt = l, ¯mt = t, y) , (7.19)\nwhere ¯\nlt = l and ¯mt = t, implies that a source (or target) phrase ended/started at positionl (ort) of the\ninput (or output, respectively).\nThe sufﬁx probability in Eq. (7.19) is recursively computedas follows\nβtl =\n8\n>\n>\n>\n>\n<\n>\n>\n>\n>\n:\n1 t = I , l = J\nIX\nt′=t+1\nJX\nl′=l+1\nβt′l′ p(t′ − t) p(l′ − l | t′ − t) p( xl′\nl+1 |yt′\nt+1) 0 ≤ t < I\n0 ≤ l < J\n0 otherwise\n(7.20)\nThe computational complexity, both in terms of memory and time, of the backward recurrence are\nthe same of that of the forward recurrence. Furthermore, thesame optimisations applied to the forward\nrecursion are also eligible to be applied in the backward recursion.\nAnalogously to the forward recursion, the probability of a given bilingual pair of sentences(x, y)\ncan be efﬁciently computed as follows\npθ (x |y) = β00 . (7.21)\nJAF-DSIC-UPV 139\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nαl,t 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 2122\n0 ■\n1 ■ ■ ■\n2 ■ ■ ■ ■ ■ ■\n3 ■ ■ ■ ■ ■ ■ ■ ■ ■\n4 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n5 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n6 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n7 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n8 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n9 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n10 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n11 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n12 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n13 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n14 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n15 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n16 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■\n17 ■ ■ ■ ■ ■ ■ ■ ■ ■\n18 ■ ■ ■ ■ ■ ■\n19 ■ ■ ■\n20 ■\nFigure 7.2:The relevant values that should be computed for the forward recurrence in\nthe case of a source sentence of20 words and a target sentence of22 words. The max-\nimum phrase length is assumed to be3 for both source and target phrases. Finally, the\nblack squares (■ ) stand for theαtl values that must be computed while the remaining\npoints are not needed.\n7.3.3 Viterbi recursion\nThe second question proposed at the beginning of the sectionis stated as ﬁnding the best segmentation\nfor a given bilingual pair(x, y), i.e.\n(ˆl, ˆm) = arg max\nl,m \n{pθ (x, l, m |y, J )} . (7.22)\nIn order to efﬁciently answer this question, we deﬁne the Viterbi recursion as follows\nδtl = max\nT ,lT\n1 ,m T\n1\nlT =l,mT =t\n{pθ (xl\n1, lT\n1 , mT\n1 | y, J )} , (7.23)\nwhere note thatl, m are required to end at positionsl and t respectively.\nThe Viterbi recursion in Eq. (7.23) is efﬁciently computed by the following recurrence\nδtl =\n8\n>\n>\n<\n>\n>\n:\n1 t = 0 , l = 0\nmax\nt′,l′\n{δt′l′ p(t′ − t) p(l′ − l |t′ − t) p(xl\nl′+1|yt\nt′+1)} 0 < t ≤ I\n0 < l ≤ J\n0 otherwise\n(7.24)\n140 JAF-DSIC-UPV\n7.4. Training\nA traceback of the decisions made to computeδIJ provides the maximum segmentationˆm and ˆl, i.e.\nthe solution to the Eq. (7.22).\nThe Viterbi recursion shares the computational requirements of the forward and backward recur-\nrence. Furthermore, the same optimisations applied to the forward and backward recursion should also\nbe adapted for computing the Viterbi recursion.\n7.4 Training\nSince the proposed PBHSMM assumes that the segment length variables are not given in the training\ndata, an approximate inference algorithm such as the EM is needed. We give here the description of the\ncommon training algorithms with respect to a collection of training translation pairs{(x1, y1), . . . , (xN , yN )},\nthat is to say: the Baum-Welch algorithm [Rabiner, 1989], and the Viterbi algorithm [Rabiner, 1989].\nBoth algorithms are instantiations of the EM algorithm as discussed in Section 1.1.4 Chapter 1.\nThe log-likelihood function as a function of the parametersθ is\nLL(θ ) =\nX\nn\nlog pr (xn |yn)\n=\nX\nn\nlog\nX\nl,m \npθ (xn , l, m |yn )\n=\nX\nn\nlog\nX\nl,m \nY\nt\np(mt ) p(lt |mt ) p(xn(t) |yn (t)) . (7.25)\nHowever, recall that the EM algorithm maximises a lower bound to the log-likelihood function,LL(·),\nby iteratively maximising a variational functionL(q, θ ) through the application of two basic steps in\neach iteration: the E(xpectation) step and the M(aximisation) step.\n7.4.1 Fractional counts\nUsing the previously deﬁned forward and backward recursions, we can compute the probability of using\nthe source phrasexl′\nl+1 and the target phraseyt′\nt+1 when segmenting a given sample(x, y),\nγtlt′l′ = p θ (xl′\nl+1, ¯lt = l′, ¯lt−1 = l, ¯mt = t′, ¯mt−1 = t |y)\n= αtl p(t′ − t) p(l′ − l |t′ − t) p(xl′\nl+1 |yt′\nt+1)βt′l′\npθ (x, y)\n, (7.26)\nwithl < l ′ and t < t ′. This fractional counts are very useful for training the parameters of the model\nin the following two Sections 7.4.2 and 7.4.3.\n7.4.2 Baum-Welch training\nLetθ (k) be a guess of the optimal parameters obtained from previous iterations. In this case, the E step\nrequires the computation, for each pair(xn , yn ), of the sample versions of (7.16), sayα(k)\nntl, and (7.19),\nsay β(k)\nntl , as well as the fractional counts per sample, sayγ(k)\nntlt′l′ . These sufﬁcient statistics are com-\nputed using the parameters obtained from previous iteration,θ (k). Recall that these sufﬁcient statistics\nsummarise the optimal functionq(k) obtained in the E-step, by storing the relevant information; in other\nwords, computing these recurrences is equivalent to compute q(k).\nAfterwards, in the M step, a new set of parametersθ (k+1) is estimated from the recurrences com-\nputed in the E step. The new set of the parameters includes thethree probabilities of the model: the\nJAF-DSIC-UPV 141\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nphrase dictionaryp(k+1)(u |v), the target length probabilityp(k+1)(m) and the source length condi-\ntional probabilityp(k+1)(m |l).\nThe phrase dictionary is estimated as follow\np(k+1)(u |v) = N (k)(u, v)P\nu′ N (k)(u′, v) , (7.27)\nwith\nN (k)(u, v) =\nX\nn\nX\nl,l′\nX\nt,t′\nγ(k)\nntlt′l′ δ(xn (l, l ′), u)δ(yn(t, t ′), v) , (7.28)\nwhere we use the notationz(l, l ′) to refer tozl′\nl+1, and whereδ(a, b ) stands for the Kronecker delta\nfunction which evaluates to1 ifa = b and 0 otherwise.\nFor the target phrase length probabilities, we obtain the following re-estimation equation\np(k+1)(m) = N (k)(m)P\nm′ N (k)(m′) , (7.29)\nwith\nN (k)(m) =\nX\nn\nX\nl,l′\nX\nt\nγ(k)\nnt,l,(t+m),l′ , (7.30)\nwhere m is a target phrase length.\nFinally, the source phrase length probabilities are re-estimated as follows\np(k+1)(l |m) = N (k)(l, m )P\nl′ N (k)(l′, m ) , (7.31)\nwith\nN (k)(l, m ) =\nX\nn\nX\nl′\nX\nt\nγ(k)\nt,l,(t+m),(l′+l) , (7.32)\nwhere l denotes a source phrase length, andm a target phrase length.\nIf an initial parameter set is givenθ (0); we can iteratively reﬁne our initial guess by alternatively\napplying the E-step and the M-step until convergence. The convergence criteria is given by either\nreaching a maximum number of iterations or increasing the log-likelihood under a given threshold.\nSince the log-likelihood function in Eq. (7.25) is not convex, the Baum-Welch training only provides a\nlocal optimum after convergence. Therefore, the reader should bare in mind that a wrong initial guess\nθ (0) can ruin the system performance.\n7.4.3 Viterbi training\nLet θ (k) be a guess of the optimal parameters obtained from previous iterations. In this case, the E-\nstep requires the computation of the maximum segmentation(l(k)\nn , m(k)\nn ), as deﬁned in Eq. (7.22), for\neach pair(xn , yn ). In order to efﬁciently compute the optimal segmentation, the Viterbi recursion in\nEq. (7.24) is computed for each sample.\nAfterwards, in the M-step ﬁnding the parameter setθ (k) that maximisesL(q, θ ) is equivalent to\nﬁnd the parameter set that maximises the following function\nQ(θ |θ (k)) =\nX\nn\nX\nt\nlog p( m(k)\nnt ) + log p( l(k)\nnt |m(k)\nnt ) + log p( xn(t) |yn(t)) . (7.33)\n142 JAF-DSIC-UPV\n7.4. Training\nRearranging terms in Eq. (7.33), we obtain\nQ(θ | θ (k)) =\nX\nm\nM (k)(m) log p( m) +\nX\nm\nX\nl\nM (k)(l, m ) log p( l | m)\n+\nX\nu\nX\nv\nM (k)(u, v) log p( u | v) ,\n(7.34)\nwhere M (k)(e) stands for the number of times the evente has occurred in the sample completed with\nthe length variables, i.e.,{(xn, yn, l(k)\nn , m(k)\nn )}N\nn=1. Speciﬁcally,M (k)(m) is deﬁned as follows\nM (k)(m) =\nX\nn\nX\nt\nδ(m(k)\nnt , m ) , (7.35)\nand M (k)(l, m ) is given by\nM (k)(l, m ) =\nX\nn\nX\nt\nδ(m(k)\nnt , m )δ(l(k)\nnt , l ) , (7.36)\nand ﬁnally the phrase counts are deﬁned as\nM (k)(u, v) =\nX\nn\nX\nt\nδ(x(k)\nn (t), u)δ(y(k)\nn (t), v) , (7.37)\nwhere x(k)\nn (t) and y(k)\nn (t) stand for thet-th source and target phrase induced byl(k)\nn and m(k)\nn , respec-\ntively; and where the expressionδ(a, b ) is the Kronecker’s delta function.\nThe Viterbi training described here is also an iterative training process, since it is another instanti-\nation of the EM algorithm (see Section 1.1.4 Chapter 1). Therefore, if an initial parameter set is given\nθ (0); we can iteratively reﬁne our initial guess by alternatively applying the E-step and the M-step until\nconvergence. Similarly to the Baum-Welch training, the Viterbi training only provides a local optimum\nafter converge. However, since the Viterbi algorithm constraints the family of functions in the E-step,\ni.e.q(k)(l, m) = δ(l, l(k)\nn )δ(m, m(k)\nn ), the optimal parameter set obtained after a Viterbi training is\ntypically worse than that of the Baum-Welch training [Rabiner, 1989].\nThe main advantage of the Viterbi training with respect to Baum-Welch training is that we only need\nto compute one recurrence, the Viterbi recurrence, in contrast to the two recurrences, the forward and\nbackward recurrences, needed in the Baum-Welch training. Additionally, since the Viterbi algorithm\nonly takes into account the most probable segmentation in each iteration, a given outcome has effect on\nless parameters, speeding up the algorithm. Therefore, theViterbi training is at least twice times faster\nthan the Baum-Welch training.\n7.4.4 The model smoothing\nA well-known drawback of the EM algorithm is that it tends to overﬁt the models. Moreover, the\nHMM-based model discussed in Chapter 6, shown severe overﬁtting problems. In order to alleviate\nthese problems, we smoothed the phrase table˜ p(u |v) with a IBM model 1 [Brown et al., 1993] as\nfollows\n˜ p(u | v) := (1 − ǫ) ˆ p(u | v) + ǫ pIBM 1 (u |v) , (7.38)\nwhere ˜ p(u | v) stands for the smoothed phrase table,ˆ p(u | v) stands for the optimal phrase table (not\nsmoothed) obtained after the EM training andpIBM 1 (u |v) stands for the probability of the IBM\nmodel 1without the null word, i.e.,\npIBM 1 (u / v, |u|, |v|) :=\n|v |X\ni=1\n|u|Y\nj\n1\n|v| p(uj |vi ) . (7.39)\nJAF-DSIC-UPV 143\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nThe IBM model 1 performs accurately when deciding whether a set of words contains the translations\nof another set of words or not; even though it is unable to learn the order of such words.\n7.5 Decoding Recurrence\nIn this section, we explain the recurrence used to perform the model search or decoding process in-\nside a system based on the fundamental equation of machine translation introduced in Section 1.3\n(Eq. (1.102)). The search problem in such a system is stated as the problem of ﬁnding the maximum\nprobable target sentence as follows\nˆy = arg max\ny\n{pθ (x |y) p( y)} , (7.40)\nwhere thepθ (x | y) is modelled according to the PBHSMM proposed in Eq. (7.13), i.e.,\nˆy = arg max\ny\n(X\nm \nX\nl\npθ (x, l, m |y) p(y)\n)\n. (7.41)\nIn order to cope with Eq. (7.41) a Viterbi-like approximation is taken for all the sums. In this way the\nsearch problem is reduced to\nˆy = arg max\ny\n˘\nmax\nm ,l\n{pθ (x, l, m |y) p(y)}\n¯\n. (7.42)\nFor sake of simplicity, we focus on ﬁnding the probability ofthe maximum probable translationˆy,\ni.e.,\npθ (x | ˆy) = max\ny ,l,m \n{pθ (x, l, m | y) p(y)} . (7.43)\nProvided that we only usen-gram language models, we assume that the language model probability\nfor a given target phrasep(v |ym\n1 ) only depends on the(n − 1)-most recent words, i.e.\np(v | ym\n1 ) := p( v | suf\nn−1\n(ym\n1 )) , (7.44)\nwhere suf n−1(· · ·) stands for the(n − 1)-most recent words.\nIn order to perform the maximisation in Eq. (7.43) we deﬁne a Viterbi-like decoding recurrence as\nfollows\nσl,v (x) = max\nt,lt\n1,m t\n1,y mt\n1\nlt =l,suf |v|(y mt\n1 )=v\nn\npθ (xl\n1, lt\n1, mt\n1 |ymt\n1 ) p( ymt\n1 )\no\n, (7.45)\nwhere bysuf |v |(ymt\n1 ) = v we denote that the sufﬁx ofymt\n1 must be equal tov. We further assume\nthat ifv = ⋆, then this constraint is ignored, i.e.,\nσl,⋆ (x) = σl(x) = max\nt,lt\n1,m t\n1,y mt\n1\nlt =l\nn\npθ (xl\n1, lt\n1, mt\n1 |ymt\n1 ) p(ymt\n1 )\no\n. (7.46)\nThe search recurrenceσl,v can be recursively expressed in terms of a simpler case of itself as\nfollows\nσl,v (x) = max\nl′,v ′,h\nsuf |v|(v ′h)=v\nl′<l,|v ′h|≥|v |\nn\np(l) p(\n˛\n˛v′˛\n˛/ l ) p(xl\nl′+1 |v′) p(v′ |h)σl′,h (x)\no\n, (7.47)\n144 JAF-DSIC-UPV\n7.6. Experiments\nwhere note thatv is split into two partsv′ and h. The ﬁrst part is used in the translation of the source\nphrase whereas the second parth, is used as the preﬁx of the language model.\nFinally, the solution to the search problem in Eq. (7.43) is computed using the search recurrence as\nfollows\npθ (x | ˆy) = σJ,⋆ . (7.48)\nNote that once it is known how to computepθ (x | ˆy), the optimalˆy is obtained by tracing back the\ndecisions made during its computation.\nHowever, although the recursionσl,v speeds up the search problem, it is still a hard problem. For\nthis reason, we still need to perform an approximate decoding in which we use a maximum number of\nhypothesis for each source positionl, sayM , and also a beam pruning [Wang and Waibel, 1997, 1998].\nThat is to say, instead of using Eq. (7.47), we use the following approximated version\nσ⋆\nl,v (x) =\n⋆\nmax\nl′,v ′,h\nsuf |v|(v ′h)=v ,|v ′h|≥|v |\nn\np(l) p(\n˛\n˛v′˛\n˛|l) p(xl\nl′+1 |v′) p(v′ |h)σ⋆\nl′,h (x)\no\n, (7.49)\nwhere\n⋆\nmax stands for an approximate version ofmax where we have applied several heuristics, such\nas beam search or histogram pruning.\nIn the experimental section, we have also used the proposed model inside a log-linear loss func-\ntion (see Section 1.3 of Section 4.3.3 for further details).For this aim, we have used the Moses sys-\ntem [Koehn et al., 2007], and added our model as a feature inside its search. Should the reader be\ninterested in the details of this search, please refer to Koehn et al. [2007].\n7.6 Experiments\nWe have carried out two types of experiments. The ﬁrst set of experiments [Andrés-Ferrer and Juan,\n2009] were designed to analyse the properties of the proposed model when used in a classical phrase-\nbased model that is based on the fundamental equation of statistical machine translation deﬁned in\nEq. (1.102). The second set of experiments were designed to analyse the behaviour of the improvements\nobtained in the ﬁrst experiment when passed as a feature in a log-linear model based on Eq. (1.103).\nTo evaluate the quality of the translations, we used two error measures: bilingual evaluation understudy\n(BLEU ) [Papineni et al., 2001], and translation edit rate (TER ) [Snover et al., 2006].\n7.6.1 Classical phrase-based models.\nFor the ﬁrst set of experiments, we tested our model in two corpora: the Europarl-10 and the Europarl-\n20. The former comprises all the sentences from the English-to-Spanish part of Europarl-v3 [Koehn,\n2005] with length equal to or less than10. The latter is made up of all the English-to-Spanish Europarl-\nv3 sentences with length equal to or less than20. For both corpora we randomly selected5 000\nsentences to test the algorithms. However, since the Europarl-10 has several repeated sentences, we\navoided repeated sentences in the test. Note that since we wanted to perform detailed experimentation,\nwe constrained the training length because of the time requirement for training the proposed PBHSMM.\nTable 7.2 shows some basic statistics of the training part for both corpora; Table 7.3 summarises some\nstatistics from the testing part.\nAll the experiments were carried out using a4-gram language model computed with the standard\ntool SRILM [Stolcke, 2002] and a modiﬁed Kneser-Ney smoothing. We used two systems: the proposed\nPBHSMM with the search algorithm depicted in Section 7.5; and the Moses system [Koehn et al., 2007]\nbut constraining the model to a classical SMT system based onEq.(1.102) in Chapter 1 (a phrase-based\ninverse model and an-gram language model). We used this constrained version of Moses instead of the\nJAF-DSIC-UPV 145\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nTraining Europarl-10 Europarl-20\nLanguage En Sp En Sp\nsentences 76, 996 306, 897\navg. length 7.01 7 .0 12.6 12 .7\nrunning words 546K 540K 3.86M 3.91M\nvoc. size 15.5K 22.1K 37.1K 57.8K\nTable 7.2:Basic statistics of the training sets.\nTest Europarl-10 Europarl-20\nLanguage En Sp En Sp\nsentences 5, 000 5, 000\navg. length 7.2 7 .0 12.6 12 .9\nrunning words 35.8K 35.2K 63.0K 63.8K\nppl (4-gram) 48.3 56 .9 62.3 69 .1\nTable 7.3:Basic statistics of the test sets.\nfull log-linear model in order to deﬁne a fair translation baseline. However, in the following subsection,\nwe will compare both systems inside a log-linear model.\nThe proposed training algorithms require an initial guess.To this aim, we computed the IBM word\nalignment models with GIZA++ [Och and Ney, 2003], for both translation directions. We computed the\nsymmetrisation heuristic [Och and Ney, 2004] and extractedall theconsistentphrases [Och and Ney,\n2004]. Afterwards, we computed our initial guess by counting the occurrences of each bilingual phrase\nand then normalising the counts. Instead of using the Moses system to perform this initialisation task,\nwe have implemented our own version of this process.\nSince the training algorithm highly depends on the maximum phrase length, for most of the exper-\nimentations we limited it to4 words. Table 7.4 summarises the results obtained for both translation\ndirections with the Europarl-10 corpus. Surprisingly, Viterbi training obtains almost thesame results\nas the Baum-Welch training; this is probably because most ofthe sentences accumulate all the proba-\nbility mass in just one possible segmentation. Maybe that iswhy our algorithm is not able to obtain a\nlarge improvement with respect to the initialisation. Notethat since the proposed system and the Moses\nsystem use different phrase-tables, these two numbers should not be compared. Therefore, the Moses\nbaseline is only given as a reference and not as a system to improve. The important question is whether\nthe model produces an improvement with respect to the initialisation, i.e., the result on iteration0. Note\nthat this corpus is small; therefore, although its complexity allow us to check some properties of the\nalgorithm, we cannot draw further conclusions. Moreover, recall that we have erased the repetitions\nfrom the this test set.\nTable 7.5 is the counterpart of Table 7.4 but for the Europarl-20. It can be observed that Baum-\nWelch training has no advantage with respect to Viterbi training. Typically, approximately4 iterations\nsufﬁce to avoid overﬁtting which maximises the system performance. The results show a small im-\nprovement over the initialisation. Although the improvement is very small, its magnitude is similar\nto the improvement obtained when extending the maximum phrase length as shown in Table 7.6. For\ninstance, as the table shows, extending the maximum phrase length from4 to 5 incurs in the same\nimprovement as performing4 Viterbi iterations in the model. Finally, in most of the cases, the Viterbi\ntraining improves the translation quality in terms of TER and/or BLEU .\n146 JAF-DSIC-UPV\n7.6. Experiments\nEn → Sp Sp → En\nT ER B LEU T ER B LEU\nMoses p(x | y) p(y) 50.0 32 .9 47.2 32 .7\nIterations PBHSMM (Baum-Welch training)\n0 51.4 31 .9 48.2 33 .2\n1 51.4 31 .9 47.9 33 .1\n2 51.5 31 .9 47.9 33 .1\n4 51.2 32 .6 48.1 33 .1\n8 51.4 31 .8 48.0 33 .0\nIterations PBHSMM (Viterbi training)\n0 51.4 31 .9 48.2 33 .2\n1 51.4 31 .9 47.9 33 .1\n2 51.1 32 .6 48.0 33 .2\n4 51.2 32 .6 48.0 33 .0\n8 51.4 31 .8 48.0 33 .0\nTable 7.4:Results for the Europarl-10 corpus with a maximum phrase length of4.\nEn → Sp Sp → En\nT ER B LEU T ER B LEU\nMoses p(x | y) p(y) 57.3 23 .5 55.1 24 .10\nIterations PBHSMM (Baum-Welch training)\n0 57.7 25 .0 56.0 26 .0\n1 57.7 25 .1 55.8 26 .4\n2 57.7 25 .1 55.9 26 .4\n4 57.7 25 .2 55.8 26 .5\n8 57.7 25 .2 55.8 26 .5\nIterations PBHSMM (Viterbi training)\n0 57.7 25 .0 56.0 26 .0\n1 57.7 25 .1 55.8 26 .4\n2 57.7 25 .1 55.9 26 .4\n4 57.7 25 .2 55.8 26 .5\n8 57.7 25 .2 55.8 26 .5\nTable 7.5:Results for the Europarl-20 corpus with a maximum phrase length of4.\nEven though, the training does not incur in a signiﬁcant improvement over the baseline in terms\nof BLEU and/or TER; in practice, the quality of the translations is increased by the training. Table 7.7\nshows some translation examples. A detailed analysis of theproposed translations suggest that most\ncases belong to case A, case B or case D, and few translations belong to case C.\nJAF-DSIC-UPV 147\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nIterations En → Sp Sp → En\nT ER B LEU T ER B LEU\nIterations Maximum phrase length2\n0 60.5 21 .2 57.9 23 .5\n4 60.5 21 .2 58.1 23 .5\nIterations Maximum phrase length3\n0 58.6 24 .1 56.1 25 .7\n4 58.3 24 .1 56.4 25 .5\nIterations Maximum phrase length4\n0 57.7 25 .0 56.0 26 .0\n4 57.7 25 .1 55.8 26 .5\nIterations Maximum phrase length5\n0 57.7 25 .1 55.8 26 .6\n4 57.4 25 .3 55.3 26 .9\nIterations Maximum phrase length6\n0 57.7 25 .4 55.9 26 .6\n4 57.3 25 .6 55.4 26 .8\nTable 7.6:Results for the Europarl-20 corpus with several phrase length.\n7.6.2 Log-linear models.\nIn this case, we also used two corpora: the Europarl-20, and Europarl-v3. The former has already been\ndescribed in the previous section. In this set of experiments, we randomly selected2 000 sentences from\nthe Europarl-20 training set as the development set to train the log-linear weights. The latter corpus is\nthe standard dataset used in Koehn and Monz [2006]. Table 7.8summarises the properties of the ﬁrst\ncorpus and Table 7.9 summarises the porperties of the secondcorpus.\nWe compared 3 systems: Moses, log-PBHSMM, and log-PBHSMM+Moses. All thesystems used\nthe Moses decoder [Koehn et al., 2007] to perform the decoding process. Therefore, the differences\namong the three systems lay in the features used in the log-linear model. The ﬁrst system is the standard\nlog-linear system trained with Moses [Koehn et al., 2007], where the following features were used:\n• Direct phrase-based translation model\n• Inverse phrase-based translation model\n• Direct lexicon model\n• Inverse lexicon model\n• A phrase penalty2.718\n• A word penaltye\n• A 5-gram language model smoothed with the modiﬁed Kneser-Ney smoothing\nThe second model, the log-PBHSMM, has the same features as the Moses system, but we replaced\nboth direct and inverse phrase-based models with our trained PBHSMM systems. Finally, for the third\nsystem, thelog-PBHSMM+Moses system, we added both direct and inverse phrase-based probabilities\ntrained with our PBHSMM system to the Moses system. Additionally, we obtained results with the\nsystems in a monotone way (not using reodering) and using thestandard distance-based reordering\nimplemented in the Moses decoder [Koehn et al., 2007]. The log-linear weights of the three systems\n148 JAF-DSIC-UPV\n7.6. Experiments\nCase A Training improves evaluation measures\nR EF . I sincerely believe that the aim of the present directive is astep in the right direction .\nIT. 0 I am convinced that the aim of this directive is a step in the right direction .\nIT. 4 I sincerely believe that the aim of the directive before us isa step in the right direction .\nM OSES I sincerely believe that the aim behind the directive is alsoa step in the right direction .\nCase B Training improves translation but not evaluation measures\nR EF . Mr president , i wish to endorse mr posselt ’s comments .\nIT. 0 Mr president , i support for to our .\nIT. 4 Mr president , i join in good faith to our colleague , mr posselt .\nM OSES mr president , i would like to join in good faith in the words ofour colleague , mr robig .\nCase C Training degrades the system\nR EF . BSE has already cost the uk gbp 1.5 billion in lost exports .\nIT. 0 BSE has cost the uk 1.5 million losses exports .\nIT. 4 BSE already has cost in the uk alone 1500 million pounds into loss of exports .\nM OSES BSE has already claimed to britain 1500 million pounds into loss of trade .\nCase D Other cases\nR EF . I will ﬁnish by telling you a story .\nIT. 0 I will history .\nIT. 4 To conclude a story .\nM OSES I shall conclude a history .\nR EF . Are there any objections to amendment nos 3 and 14 being considered as null and void\nfrom now on ?\nIT. 0 Are there any objections to give amendments nos 3 and 14 .\nIT. 4 Are there any objections to adopt amendments nos 3 and 14 ?\nM OSES Are there any objections to consider amendments nos 3 and 14 ?\nTable 7.7:Some translation examples (Sp→ En) before and after training the phrase\ntable,4 iterations with the Viterbi training, and maximum phrase length of4 words.\nTraining Development Test\nLanguage En Sp En Sp En Sp\nsentences 304 897 2 000 5 000\navg. length 12.7 12 .6 12.8 12 .6 12.6 12 .8\nrunning words 3.83M 3.88M 25.1K 25.5K 63.8K 63.0K\nvoc. size 37.0K 57.7K 3.9K 4.7K 6.3K 8.1K\nppl (5-gram) – – 62.2 67 .2 63.3 69 .2\nTable 7.8:Basic statistics of Europarl-20 with development set.\nwere trained in the development set of each corpus performing Minimum Error Rate Training (MERT)\nin terms of BLEU .\nTable 7.10 shows the results in terms of BLEU and TER for these systems using the Europarl-20\ntraining corpus. Instead of computing a single ﬁgure, we computed the conﬁdence interval at95% as\ndescribed in Koehn [2004]. In this case we constrained the maximum phrase length to4 words, so that\nJAF-DSIC-UPV 149\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nTraining Development Test\nLanguage En Sp En Sp En Sp\nsentences 730 740 2 000 2 000\navg. length 20.8 21 .5 29.3 30 .3 30.0 30 .2\nrunning words 15.2M 15.7M 58.7K 60.6K 58.0K 60.3K\nvoc. size 72.7M 113.9K 6.5K 8.2K 6.5K 8.3K\nppl (5-gram) – – 79.6 78 .8 78.3 79 .8\nTable 7.9:Basic statistics of Europarl-v3.\nSystem En → Sp Sp → En\nDistance-based reordering\nT ER B LEU T ER B LEU\nMoses 56.7 ± 0.7 27 .7 ± 0.7 54.2 ± 0.7 28 .5 ± 0.7\nlog-PBHSMM 56.4 ± 0.7 28 .1 ± 0.7 53.8 ± 0.7 28 .7 ± 0.6\nMoses + log-PBHSMM 56.4 ± 0.7 28 .3 ± 0.7 53.4 ± 0.7 28 .8 ± 0.7\nMonotone\nT ER B LEU T ER B LEU\nMoses 58.6 ± 0.7 26 .1 ± 0.6 55.1 ± 0.7 27 .3 ± 0.7\nlog-PBHSMM 57.6 ± 0.7 26 .6 ± 0.6 54.4 ± 0.7 27 .9 ± 0.6\nMoses + log-PBHSMM 58.6 ± 0.7 26 .4 ± 0.7 54.2 ± 0.6 28 .0 ± 0.7\nTable 7.10:Results for several translation systems on the Europarl-20corpus.\nthese results were comparable with the results obtained in the previous experimental setup. It can be\nobserved, that the log-PBHSMM obtains an improvement over the monotonic baseline, though it is not\nstatistically signiﬁcant.\nTable 7.11 is the same as Table 7.10 but with the Europarl-v3 corpus. In this case, we constrained\nthe maximum phrase length to the standard length of7 word. It can be observed, that, in this case, our\nproposed model PBHSMM is not better than the standard Moses baseline. However, this corpus has the\npeculiarity that the development and test set are not distributed according to the training set probability\ndistribution. This can be easily checked in Table 7.9 by comparing the average sentence lengths in\neach partition. Therefore, the fact that the proposed PBHSMM works slightly worse than the standard\nphrase-tables is not surprising. The only reason for providing these results is because it is a standard\ncorpus.\n7.7 Conclusions\nIn this chapter, we have presented a phrase-based hidden semi-Markov model for machine translation\ninspired on both phrase-based models and classical hidden semi-Markov models. The idea behind this\nmodel is to provide a well-deﬁned monotonic formalism that,while remaining close to the phrase-based\nmodel, explicitly introduces the statistical dependencies needed to deﬁne the monotonic translation\nprocess with theoretical correctness.\nAlthough the proposed model does not take full advantage from the HSMM formalism, we could\nnot ignore some previous negative results [DeNero et al., 2006] when conditional phrase-based models\n150 JAF-DSIC-UPV\n7.7. Conclusions\nSystem En → Sp Sp → En\nDistance-based reordering\nT ER B LEU T ER B LEU\nMoses 55.0 ± 0.8 29 .9 ± 0.9 53.6 ± 0.9 30 .5 ± 1.0\nlog-PBHSMM 55.6 ± 0.9 29 .3 ± 0.9 53.8 ± 0.9 30 .1 ± 0.9\nMoses+log-PBHSMM 54.9 ± 0.9 29 .9 ± 0.8 53.5 ± 1.0 30 .6 ± 0.9\nMonotone\nT ER B LEU T ER B LEU\nMoses 55.6 ± 0.9 29 .1 ± 0.8 53.8 ± 0.9 30 .2 ± 0.9\nlog-PBHSMM 56.0 ± 0.9 28 .9 ± 0.9 54.3 ± 0.9 29 .8 ± 0.9\nMoses+log-PBHSMM 55.6 ± 0.8 29 .2 ± 0.9 54.0 ± 0.9 30 .1 ± 0.9\nTable 7.11:Results for several translation systems on the Europarl-v3corpus.\nare trained statistically. In that work, DeNero et al. [2006] concluded that a statistical (conditional)\nphrase-based model worsens the translation performance ofa phrase-based system because statistical\nsystems peak the phrase table probabilities. Consequently, we have forced our PBHSMM to be as\nclose as possible to a phrase-based model to check whether DeNero’s conclusion was extensible to this\nformalism or not. In contrast to DeNero et al. [2006], our experimental analysis has shown a slight\nimprovement in some cases by applying the estimation algorithm with respect to the baseline, though\nthis improvement is not statistically signiﬁcant. Furthermore, we have surprisingly found that both\ntraining algorithms, Viterbi and Baum-Welch, obtain the same practical behaviour. Hence, we advocate\nfor the use of Viterbi training.\nWe consider that the addition of a well deﬁned training procedure would allow us to improve the\nsystem with future extensions. For instance, we could have assumed that thet-th source segment length\ndepends on thet-th target segment length and on thet-th target segmenty(t), that is to say,\npr (l | m, y, J ) :=\nY\nt\np(lt |mt , y(t)) . (7.50)\nIn order to fully take advantage of the HSMM theoretical framework, one outstanding and simple\nextension to the proposed model is to “unhide” theconceptvariable by having a mixture of phrase-based\ndictionaries. Hence, the model proposed in Section 7.2 would be given by\npr (x |y, J ) :=\nX\nc\nX\nl\nX\nm \nY\nt\np(ct | cπt ) p(mt |ct ) p(lt |mt , c t ) p( x(t) | y(t), ct ) , (7.51)\nwhere p(x(t) | y(t), ct ) stands for a phrase-table that depdends on the current hidden conceptct and\nthe seen concepty(t); and wherep(ct |cπt ) plays the role of the transition probabilities. Actually, the\nrequirements of this extension would not signiﬁcantly affect the proposed estimation algorithms. For\ninstance, the forward recurrence will be slightly modiﬁed as follows\nαctl =\n8\n>\n>\n<\n>\n>\n:\n1 t = 0 , l = 0\nP\nt′,l′,c′ αc′t′l′ p(c |c′) p(t′−t |c) p( l′−l |t′−t, c ) p(xl\nl′+1|yt\nt′+1, c ) 0 < t ≤ I\n0 < l ≤ J\n0 otherwise\n(7.52)\nwhere the sum overt′ ranges fromt + 1 toI ; likewise the sum overl′ ranges froml + 1 toJ ; and where\nc′ ranges among all the possible phrase states.\nJAF-DSIC-UPV 151\nChapter 7. A phrase-based hidden semi-Markov model for monotone machine translation\nNote that the new unhidden concepts proposed in Eq.(7.51) would capture the syntactic, semantic\nand grammatical constraints between the source and the target sentence inside the same translation\npair. However, we have left this interesting extension out of this thesis, and we intend to develop it in\nimmediate future work.\nWe have also used the proposed PBHSMM as a feature inside a log-linear model as most of the\ncurrent state of the art systems do. The results show an improvement over the baseline, both monotone\nand non-monotone systems, but only if the test probability distribution is similar to the training proba-\nbility distribution. This improvement would probably be lost as the monotonicity of the language pairs\ndecreases. However, we leave the practical analysis of thismodel in other language pairs for future\nwork.\nThe model presented in this chapter, PBHSMM, can have some sparseness issues for some lan-\nguages. Furthermore, the model extension proposed in (7.51) can aggravate this sparseness problems.\nIn order to alleviate them, we could use word categories or tags, either statistically inspired or syntacti-\ncally inspired. This approach would give a more reliable estimation of the phrase emission probabilities\nby reducing the sparsity problems.\nFinally, the most undesirable property of the proposed model is its monotonicity. Although the\nmonotonic constraint is a clear disadvantage for this ﬁrst PBHSMM translation model, it can be ex-\ntended to non-monotonic processes. For instance, the IBM-like reordering models [Zens et al., 2003]\ncan be included in the proposed model by means of memory states. Furthermore, we can decouple the\ntranslation problem in two problems: the reordering of the input and then a monotonic translation. In\nthis way we could deﬁne speciﬁc input reordering models thatdo not need to tackle the problem of\ntranslating the source sentence but rather reorder it. Afterwards, we could use any monotone translation\nmodel to carry out the translation from the reordered sourcesentence to the target sentence. Neverthe-\nless, these extensions lay far beyond the aim of this thesis.\n152 JAF-DSIC-UPV\nBibliography\nBibliography\nJesús Andrés-Ferrer and A. Juan. A phrase-\nbased hidden semi-markov approach to ma-\nchine translation. InProcedings of European\nAssociation for Machine Translation (EAMT),\nBarcelona, Spain, May 2009. European Asso-\nciation for Machine Translation.\nP. F. Brown et al. The Mathematics of Statis-\ntical Machine Translation: Parameter Estima-\ntion. Computational Linguistics, 19(2):263–\n311, 1993.\nJ. DeNero, D. Gillick, J. Zhang, and D. Klein.\nWhy generative phrase models underperform\nsurface heuristics. InProceedings on the Work-\nshop on Statistical Machine Translation, pages\n31–38, New York City, June 2006. Association\nfor Computational Linguistics.\nP. Koehn. Europarl: A parallel corpus for statis-\ntical machine translation. InProc. of the MT\nSummit X, pages 79–86, September 2005.\nP. Koehn et al. Moses: Open source toolkit\nfor statistical machine translation. InProc.\nof ACL’07: Demo and Poster Sessions, pages\n177–180, Morristown, NJ, USA, June 2007.\nAssociation for Computational Linguistics.\nPhilipp Koehn. Statistical signiﬁcance tests for\nmachine translation evaluation, 2004.\nPhilipp Koehn and Christof Monz. Shared task:\nExploiting parallel texts for statistical machine\ntranslation. InThe North American Chap-\nter of the Association for Computational Lin-\nguistics (NAACL) workshop on Statistical Ma-\nchine Translation, 2006. URLhttp://www.\nstatmt.org/wmt06/shared-task/.\nF.J. Och and H. Ney. A systematic comparison of\nvarious statistical alignment models.Computa-\ntional Linguistics, 29(1):19–51, 2003.\nF.J. Och and H. Ney. The alignment template ap-\nproach to statistical machine translation.Com-\nputational Linguistics, 30(4):417–449, 2004.\nK. Papineni, S. Roukos, T. Ward, and W. Zhu.\nBLEU: a Method for Automatic Evaluation\nof Machine Translation. Technical Report\nRC22176, Thomas J. Watson Research Center,\n2001.\nLawrence Rabiner. A tutorial on hmm and se-\nlected applications in speech recognition.Pro-\nceedings of the IEEE, 77(2):257–286, February\n1989.\nM. Snover et al. A study of translation edit\nrate with targeted human annotation. InProc.\nof AMTA’06 , pages 223–231, Boston, Mas-\nsachusetts, USA, August 2006. Association for\nMachine Translation in the Americas.\nA. Stolcke. SRILM – an extensible language mod-\neling toolkit. InProc. of ICSLP’02, pages 901–\n904, September 2002.\nY . Wang and A. Waibel. Decoding algorithm in\nstatistical translation. InProc. of ACL’97, pages\n366–372, Morristown, NJ, USA, July 1997.\nMorgan Kaufmann / Association for Computa-\ntional Linguistics.\nY . Wang and A. Waibel. Fast decoding for statisti-\ncal machine translation. InProc. of ICSLP’98,\npages 2775–2778, October 1998.\nTranslation Richard Zens, Richard Zens, and Her-\nmann Ney. A comparative study on reordering\nconstraints in statistical machine. InIn ACL,\npages 144–151, 2003.\nJAF-DSIC-UPV 153\nBibliography\n154 JAF-DSIC-UPV\nChapter 8\nConclusions\n“ Entities should not be multiplied beyond necessity” O CCAM ’S R AZOR\nContents\n8.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156\n8.2 Ideas and future work . . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8.3 Scientiﬁc publications . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nBibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n155\nChapter 8. Conclusions\n8.1 Summary\nThe work developed in this thesis covers several topics in natural language processing: text classiﬁca-\ntion, language modelling, and statistical machine translation. Moreover, from a statistical point of view,\nthis thesis revisits several statistical techniques used in these natural language processing problems:\nparameter estimation, loss function design, and statistical probability design.\nWith regard to parameter estimation, in Chapter 2, we proposed a constrained-domain maximum\nlikelihood estimation technique (CDMLE). Our new proposalavoids the additional heuristic smoothing\nstep that removes the good theoretical properties that the MLE veriﬁes. The proposed technique can be\nextended to avoid the smoothing stage of any statistical model. In order to introduce this technique, we\nhave applied it to the estimation of the naive Bayes classiﬁer, which follows a multinomial distribution.\nWe tested the novel training algorithm in several text classiﬁcation tasks: Eutrans-I, 20-Newsgroups,\nIndustry Sectors, and Job Categories. Finally, we observedthat the proposed CDMLE technique shows\nperformance that is similar to that obtained by classical smoothing techniques in these text classiﬁcation\ntasks.\nIn Chapter 3, we used the CDMLE idea to smooth then-gram leaving-one-out smoothing methods.\nWe smoothed the Good-Turing probability estimates by constraining their domain. This novel approach\nﬁlled the gap between two extremes in LM smoothing, that is tosay, between the Good-Turing and the\nKneser-Ney smoothing methods. The new proposed smoothing algorithms were compared in practice\nwith respect to the Kneser-Ney baseline in terms of perplexity. Two corpora were used to perform\nthis comparison: the Wall-Street-Journal and the English part of Europarl-v3. The results reported\nan improvement over the baseline in terms of perplexity for backing-offn-gram language models.\nThe proposedn-gram smoothing techniques are also generalisable to otherleaving-one-out estimation\nproblems.\nIn Chapter 4, we carefully studied the consequences of changing the0–1 loss function for more\ncomplex loss functions. We focused our study on those loss functions that retain a similar decoding\ncomplexity when compared with the0–1 loss function. Several candidate loss functions were pre-\nsented and tested in several statistical machine translation tasks. Furthermore, two different machine\ntranslation models were used to analyse the properties of each loss function: the IBM model 2 and the\nphrase-based models. We proved that some outstanding translation rules such as theDirect Translation\nRuleor even the log-linear models are, in fact, particular casesof these loss functions.\nThe remaining three chapters of this thesis, are focused on deﬁning monotone phrase-based models\nwith efﬁcient training algorithms. We started this hard task in Chapter 5 by giving a purely statistical\ndeﬁnition and a training algorithm for a phrase-based GIATIextension. However, in this case, the\ntraining algorithm had high requirements in terms of both memory and time. This fact made the training\nalgorithm practically unfeasible for many tasks.\nAfter analysing the proposed SGIATI method, we found that a joint model hardens the modelling\ntask unnecessarily since a joint translation model solves amore complex modelling task than what is\nneeded: a conditional translation model. Hence, in Chapter6, we proposed a monotone phrase-based\nhidden Markov model (PBHMM) for machine translation. The training algorithms for this new proposal\nare faster than the previous SGIATI model, which allowed us to obtain results in more complex tasks.\nHowever, the time and memory complexity were still demanding. Furthermore, the model did not\nimprove the phrase-based model baseline for complex tasks.\nFinally, we improved the PBHMM by using the hidden semi-Markov model formalism. Thus, a\nphrase-based hidden semi-Markov model was proposed in Chapter 7. This model, while remaining\nclose to the conventional phrase-based models, introducedthe hidden semi-Markov formalism in order\nto deﬁne efﬁcient training algorithms. The experimental analysis reported improvements with respect\nto a phrase-based model when used as a statistical model in a classical SMT system. However, when\nthis model played the role of a feature inside a log-linear loss function (a log-linear model in the SMT\n156 JAF-DSIC-UPV\n8.2. Ideas and future work\nliterature), the results were similar to those obtained with the state-of-the-art systems.\nIn summary, the main contributions of this thesis are the following:\n• Constrained-domain maximum likelihood estimation (CDMLE) is proposed as an alternative to\nthe standard maximum likelihood estimation and smoothing post-processing. This novel ap-\nproach was applied to the naive Bayes text classiﬁer, obtaining good results in practice.\n• New n-gram language smoothing models are proposed by applying CDMLE to smooth the\nleaving-one-out (Good-Turing) probability estimates. Speciﬁcally, we have proposed5 smooth-\ning models: interval-constrained smoothing, quasi-monotonic smoothing, monotonic smoothing,\nmonotonic smoothing with upper constraints, and exact extended Kneser-Ney (eeKN) smooth-\ning.\n• A detailed practical analysis of several loss functions in the scope of machine translation is\ngiven. We proved that the direct translation rule is a special case of a loss function. Furthermore,\nwe proved that the log-linear models are a special loss function with a parametric vector for\ncharacterising the loss. This parametric loss is usually adjusted or trained to resemble an error\nmeasure such as the BLEU or the WER .\n• Translation models for monotone translation problems are presented as the application of well-\nknown statistical modelling techniques such as HMM or HSMM.The results reported for the\nthe PBHSMM improved the quality of the translations when compared with a phrase-based\ntranslation model. Unfortunately, the results obtained when the PBHSMM model plays the role\nof a feature inside a log-linear loss do not outperform the state-of-the-art translation systems for\nboth monotone and non-monotone systems.\n• Exact EM training and Viterbi-like training obtain the sameresults in practice when phrase-\nbased translation models are used. Speciﬁcally, we checkedthis wide spread intuition with the\nproposed PBHSMM.\n8.2 Ideas and future work\nAs research is a constantly changing and expanding ﬁeld whenone research line is explored, it is\ncommon for several more interesting lines to arise. Since this thesis is not the exception, we have left\nseveral interesting and appealing lines for future exploration.\nFirstly, we circumvented the problem of smoothing by proposing the CDMLE technique in Chap-\nter 2. The CDMLE technique can be easily expanded to several probability models, and, hence, systems.\nSpeciﬁcally, in the case of multinomial distribution, we have left out complex constraints that are similar\nto complex smoothing techniques.\nWhen applied ton-gram smoothing, the CDMLE yields several novel smoothing techniques. The\nproposed techniques for smoothingn-gram models reported an improvement in terms of perplexity\nwhen a back-off model is used; however, it is not yet clear that this improvement would yield an\nimprovement in terms of WER or BLEU . We think that this is a very interesting research line since,\ndepending on the task, we would get full improvement. For instance, all the improvements in isolated\nmodels are not directly transferred to a global improvementin a log-linear loss (log-linear model in\nSMT literature) for machine translation tasks.\nThe proposed smoothings followed a backing-off scheme, however, the best practice performance\nis obtained with linear interpolation models. It would be a very interesting research line to extend the\nproposed discounting methods to linear interpolation smoothing models. This extension would entail\nthe problem of computing leaving-one-out withfractional counts, since an iterative algorithm EM-like\nwould be necessary.\nJAF-DSIC-UPV 157\nChapter 8. Conclusions\nThe experimentation carried out in Chapter 3 suggests that the differences between the proposed and\nthe standard smoothing methods lay in the transition from the joint probability domain to the conditional\ndomain. Recall that the techniques based on leaving-one-out, which include the proposed smoothing\nmodels, maximise thejoint likelihood function,pr (w, h ); even though the system goal is to optimise\nthe (conditional) likelihood function,pr (w |h). We observed that the best smoothing technique, exact\nextended Kneser-Ney, obtains the samejoint perplexitiesthan the KN technique; however, it is also the\ntechnique whose performance is less diminished when it is measured in terms of the standard(condi-\ntional) perplexity. Therefore, we believe that improvement will be achieved bydirectly applying the\nproposed techniques to maximise the (conditional) likelihood function, even if there is no close solution\navailable. Moreover, it would also allow us to have discounting parameters that depend on then-gram\ncontext,h, thereby, ﬂexibilising the smoothing models.\nAnother interesting observation is that directly applyingthe optimal smoothing parameters for the\nbacking-off smoothing model to the interpolated smoothingmodel degrades all the proposed smooth-\nings in Chapter 3. It would be interesting to apply the proposed theory to an interpolated smoothing\nmodel, in order to see whether the proposed smoothings improve the interpolation baseline or not.\nIn Chapter 4, we have explored the loss functions that lay in between the0–1 loss function and the\ngeneral error loss functions, such as the WER or the BLEU , which have already been studied [R. Schlüter\nand Ney, 2005, Uefﬁng and Ney, 2004]. We found that thelog-linear modelsare really alog-linear loss\nfunction. The results showed that none of the proposed loss functionscan beat these log-linear loss\nfunctions. Although some outstanding loss functions were studied in Chapter 4, there are some appeal-\ning loss functions that have been left out, such as the remaining information. If these loss functions\nwere introduced inside a log-linear loss function that approximates the error criterion, then we think\nthat the system performance would be improved.\nFinally, in the remaining three chapters we proposed several monotone translation models. Speciﬁ-\ncally, the last model improved (in some circumstances) the baseline, while having a very clear statistical\nfoundation. Note that we have decided to adhere to the monotonic constraint since for mainly mono-\ntone (at phrase level) language pairs such as Spanish and English, the translation task is still an open\nproblem. Furthermore, adding complex reordering models only incurs in slight improvements.\nWe think that a more detailed experimentation with other language pairs is necessary in order to\nsee to what extent monotone formulation is good for those language pairs. Additionally, the PBHSMM\nproposed in Chapter 7 can be greatly extended, as proposed inSection 7.7. The PBHSMM exten-\nsions range from substituting the phrase emission probabilities by a word-level model, to expanding the\nmodel to be non-monotone at the phrase-level. However, we think that the most appealing extension is\nthe extension of the model with a “hidden concept” or state. This will generate a mixture of phrase dic-\ntionaries to be used at different positions in the source sentence while performing the translation. This\nextension would take into account not only semantic relationships but also syntactic or grammatical\ndependencies.\nThe reader should keep in mind that this extension would incur in slight modiﬁcations of the\nPBHSMM training algorithms. Obviously, such an extension would require a huge parameter set.\nHowever, this set can be reduced by modelling the emission probabilities at each state by IBM mod-\nels 1 and 2 [Brown et al., 1993]. This would eventually lead toa PBHSMM where the phrase tables are\ndynamically built whenever a source phrase is needed.\nSurprisingly, we found that simpliﬁed translation tasks based on real data such as the Europarl-\n20 corpus report similar or worse results than those obtained with the full corpora. How can long\nsentences be properly translated if short sentences cannotbe correctly translated with similar training\ndata? We consider this to be a problem in the current state-of-art, phrase-based log-linear models and/or\nerror measures. Note that the length of the translated sentences does not seem to simplify the task\nwhen the training data is also restricted. Therefore, it seems to us that the current models lack enough\ngeneralisation capacity. Although the phrase table is a good aid in the translation process, we think that\n158 JAF-DSIC-UPV\n8.3. Scientiﬁc publications\nit is necessary to use a ﬁned-grain unity in the translation process such as word-based phrase-tables, as\nwe proposed in the above extension to the PBHSMM.\nOne of the major criticisms to the proposed PBHSMM model liesin its monotonicity at phrase\nlevel. This problem would be solved by one of the proposed extensions in Section 7.7. We are referring\nto the implementation of IBM-like reorderings [Zens et al.,2003] by means of memory states. Also,\nthe translation process can be divided in two steps (a reordering of the input and a monotone translation\nfrom the reordered input to the output [Kanthak et al., 2005]), in order to make the model non-monotone.\n8.3 Scientiﬁc publications\nMost of the work in this thesis has directly yielded international articles in workshops, conferences and\njournals. In this section, we enumerate these contributions to the scientiﬁc community, highlighting the\nrelationship with this thesis.\nThe theory and experimental results in Chapter 2 have yielded one publication in an international\nconference:\n• J.Andrés-Ferrerand Alfons Juan. Máxima versoimilitud con dominio restringido applicada a\nclasiﬁcación de textos. InProceedings of “Campus Multidisciplinar en Percepción e Inteligen-\ncia”, CMPI-06, pages: 791–803, Albacete, Spain July 10-14, 2006.\nIt has also yielded a publication in an international journal:\n• J.Andrés-Ferrerand Alfons Juan. Constrained domain maximum likelihood estimation for\nnaive Bayes text classiﬁcation.Pattern Analysis and Applications (PAA). Published online. 2009.\nSome of the smoothing techniques proposed in Chapter 3 have produced a participation in an inter-\nnational conference:\n• J. Andrés-Ferrerand H. Ney. Extensions of absolute discounting (Kneser-Neymethod). In\nProceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing,\nICASSP 2009, Taipei, Taiwan, 2009. Association for Computational Linguistics.\nThis contribution was awarded with theIEEE Spoken Language Processing Student Travel Granta,\nwhich honours the student of an outstanding paper in the spoken language processing area accepted for\npublication in the ICASSP conference or a ASRU workshop sponsored by the IEEE Signal Processing\nSociety.\nThe results obtained with IBM model 2 in Chapter 4 were published in two international confer-\nences:\n• J.Andrés-Ferrer, I. García-Varea, F. Casacuberta. Análisis teórico sobre las reglas de traducción\ndirecta e inversa en traducción automática estadística. InProceedings of “Campus Multidisci-\nplinar en Percepción e Inteligencia”, CMPI-06, pages: 855–867, Albacete, Spain July 10-14,\n2006.\n• J.Andrés-Ferrer, I. García-Varea, F. Casacuberta. Combining translation models in statisti-\ncal machine translation. InProceedings of the 11th International Conference on Theoretical\nand Methodological Issues in Machine Translation, TMI-07, pages: 11–20, Skovde, Sweden\nSeptember 7-9, 2007.\nThe phrase-based results and a summary of the theory were published in an international journal:\n• J.Andrés-Ferrer, D. Ortiz-Martínez, I. García-Varea, F. Casacuberta. On the use of different\nloss functions in statistical pattern recognition appliedto machine translation.Pattern Recogni-\ntion Letters. V olume 29, pages: 1072–1081, 2008.\naMore information athttp://research.microsoft.com/en-us/people/alexac/award.aspx\nJAF-DSIC-UPV 159\nChapter 8. Conclusions\nThe statistical extension to GIATI, the SGIATI model, whichis presented in Chapter 5, was pub-\nlished in the following journal:\n• J.Andrés-Ferrer, A. Juan and F. Casacuberta. Statistical estimation of rational transducers ap-\nplied to machine translation.Applied Artiﬁcial Intelligence22(1-2):4–22, 2008.\nThe hidden Markov model approach to machine translation discussed in Chapter 6 was published\nin the following international workshop:\n• J.Andrés-Ferrerand A. Juan. A phrase-based hidden Markov model approach to machine\ntranslation. InProceedings of New Approaches to Machine Translation, pages 57–62, January\n2007.\nFinally, the proposed phrased-based hidden semi-Markov model approach and some of the results\nin Chapter 7 were published in the following international conference:\n• J.Andrés-Ferrerand A. Juan. A phrase-based hidden semi-Markov approach to machine trans-\nlation. InProceedings of European Association for Machine Translation (EAMT), pages 168–\n175, May 2009, Barcelona (Spain). European Association forMachine Translation.\n160 JAF-DSIC-UPV\nBibliography\nBibliography\nP. F. Brown et al. The Mathematics of Statis-\ntical Machine Translation: Parameter Estima-\ntion. Computational Linguistics, 19(2):263–\n311, 1993.\nStephan Kanthak, David Vilar, Evgeny Matusov,\nRichard Zens, and Hermann Ney. Novel re-\nordering approaches in phrase-based statistical\nmachine translation. InProceedings of the\nACL Workshop on Building and Using Parallel\nTexts: Data-Driven Machine Translation and\nBeyond, pages 167–174, 2005.\nV . Steinbiss R. Schlüter, T. Scharrenbach and\nH. Ney. Bayes risk minimization using met-\nric loss functions. InProceedings of the\nEuropean Conference on Speech Communica-\ntion and Technology, Interspeech, pages 1449–\n1452, Lisbon, Portugal, September 2005.\nN. Uefﬁng and H. Ney. Bayes decision rules\nand conﬁdence measures for statistical ma-\nchine translation. InEsTAL - Espa for Natural\nLanguage Processing, pages 70–81, Alicante,\nSpain, October 2004. Springer Verlag, LNCS.\nTranslation Richard Zens, Richard Zens, and Her-\nmann Ney. A comparative study on reordering\nconstraints in statistical machine. InIn ACL,\npages 144–151, 2003.\nJAF-DSIC-UPV 161\n\nAppendix A\nKarush-Kuhn-Tucker Conditions\nIn Chapters 2 and 3, we have used advanced convex optimisation techniques that deserve a superﬁcial\nsurvey. In Section 1.1.2 Chapter 1, we have analysed that in order to obtain the optimal parameter set,\nit is needed to ﬁnd the maximum parameter set according to a given criterionC(θ ; D). Almost all the\noptimisation problems that derive form this formulation, are subject at least to some normalisation con-\nstraint. In order to solve these constrained optimisation problems theconvex optimisationtheory [Boyd\nand Vandenberghe, 2004] is usually applied.\nFirst, we review a typical convex optimisation example. We wish to solve the following equation\nˆθ = arg max\nθ ∈Θ\n{C(θ ; D)} , (A.1)\nsubject to\nPn (θ ) = 0 , (n = 1 , . . . , N ) . (A.2)\nIn order to solve the previous optimisation theLagrangian functionmust be deﬁned\nL(θ , λ ) = C(θ ; D) −\nX\nn\nλn Pn(θ ) , (A.3)\nwhere aLagrangian multiplier(λn) is deﬁned for each equality constraintPn.\nTheory concludes that solving Eq. (A.1) subject to Eq. (A.2)is equivalent to solve the following\nproblem\nˆθ = arg max\nθ ∈Θ\n{max\nλ\nL(θ , λ )} . (A.4)\nTherefore, an optimal point must verify the following property\n∇L(θ , λ )|ˆθ , ˆλ = 0 , (A.5)\nrising up a linear system from which the value ofˆθ is hopefully worked out.\nThe above optimisation example is typically known as anequality constrained program. However,\nin this thesis, we solve some optimisation problems that also include inequality constraints. In order to\nsolve problems with inequality constraints, theKarush-Kuhn-Tucker (KKT)conditions are needed.\n163\nAppendix A. Karush-Kuhn-Tucker Conditions\nThe new problem is similar to the previous constrained problem but with some additional inequality\nconstraints, that is to say, we wish to solve Eq. (A.1) subject to Eq. (A.2) and subject to the following\nconstraints\nQm (θ ) ≤ 0, (m = 1 , . . . , M ) . (A.6)\nIn this case, the Lagrangian function is deﬁned as follows\nL(θ , λ , µ ) = −C (θ ; D) +\nX\nn\nλn Pn(θ ) +\nX\nm\nµ m Qm(θ ) . (A.7)\nSolving Eq. (1.15) subject to Eq. (A.2) and to Eq. (A.6) is theequivalent to solve\nˆθ = arg min\nθ ∈Θ\nmin\nλ ,µ\nL(θ , λ , µ ) . (A.8)\nThe KKT necessary conditions for a point(θ , λ , µ ) to be a maximum point of Eq. (A.8) are the fol-\nlowing\n∇θ L(θ , λ , µ )|ˆθ , ˆλ , ˆµ = 0 (A.9)\nPn(θ ) = 0 , (n = 1 , . . . , N ) (A.10)\nµ m Qm (θ ) = 0 , (m = 1 , . . . , M ) (A.11)\nµ m ≥ 0, m = 1 , . . . , M (A.12)\nQm (θ ) ≤ 0, (m = 1 , . . . , M ) (A.13)\nEven though KKT conditions arenecessaryconditions, they are not sufﬁcient conditions. That is to\nsay that a maximum point must verify them, but not all points that verify them are maximum points. An\nadditional condition must be veriﬁed in order to check whether a point that veriﬁes the KKT conditions\nis optimal or not. This condition states that the Hessian of the Lagrangian function must be positive at a\nmaximum point [Boyd and Vandenberghe, 2004]. After the possible optimal points are given, checking\nwhether this sufﬁcient and necessary condition is veriﬁed or not, is a simple mathematical exercise. If\nthe characterisation of the solution is unique, then the solution is necessarily the maximum (if it exists).\nThe KKT conditions often provide just a characterisation ofthe solution, but not a procedure to\nobtain it. Hopefully, once the form of the solution is known,it is often possible to deﬁne an efﬁcient\nalgorithm to obtain this characterised solution.\n164 JAF-DSIC-UPV\nBibliography\nBibliography\nStephen Boyd and Lieven Vandenberghe. Convex opti-\nmization. pages 244–254, March 2004.\nJAF-DSIC-UPV 165\n\nList of Figures\n1.1 A graphical representation of the emission of a sequenceof outputsx9\n1 by a HMM.\nNote that this is not a graphical representation of a HMM topology. . . . . . . . . . . 9\n1.2 An instance of the generative segmentation process carried out by a HSMM for an\noutput sequencex of10 elements. . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.3 The “simulated” length probability in a HMM for several values of loop probabilities\np(q |q). Note that we have used a continuous plot instead of a histogram plot for clar-\nity’s sake. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.1 The Constrained-Domain Maximum Likelihood Estimation(CDMLE) algorithm. . . . 39\n2.2 Results obtained in theTraveller,20 Newsgroups,Industry sectorand Job categorydata\nsets. Each plot shows the classiﬁcation error rate as a function of the discount parameter\nb, for the four classiﬁcation techniques considered (Laplace,AD+1gBO ,AD+1gI and\nCDMLE ). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.1 The 4-gram modiﬁed countr⋆ as a function of the original countr for the4 proposed\ntechniques obtained with200K sentences partition of WSJ and full vocabulary size. . . 68\n3.2 The 3-gram modiﬁed countr⋆ as a function of the original countr for the4 proposed\ntechniques obtained with200K sentences partition of WSJ and full vocabulary size. . . 69\n3.3 The normalisation functionQ(λ) for a interval constraint3-gram model computed with\nthe200K sentences partition of WSJ and full vocabulary. . . . . . . . . .. . . . . . . 70\n3.4 The normalisation functionQ(d) for the eeKN (S = 3 ) smoothing computed with the\n200K sentences partition of WSJ and full vocabulary size. . . . . .. . . . . . . . . . 70\n3.5 Perplexity skipping OOV events as a function of vocabulary size (in %) with the full\nsize partition of the Europarl corpus for a3-gram language model smoothed with all\nthe monotonic approaches and the standard smoothings Kneser-Ney (KN) and modiﬁed\nKneser-Ney (mKN). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n3.6 Perplexity a function of the WSJ training size with full vocabulary, and for all the mono-\ntonic approaches and the standard smoothings Kneser-Ney (KN) and modiﬁed Kneser-\nNey (mKN). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\n3.7 Perplexity ignoring OOV events as a function of the eeKN discounting threshold (S)\nin logarithmic scale computed with the Europarl corpus and using a3-gram language\nmodel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.8 Joint perplexityas a function of WSJ training size for several smoothing techniques\napplied to a3-gram language model. . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n167\nList of Figures\n3.9 Perplexity ignoring OOV events as a function of the eeKN discounting threshold (S) in\nlogarithmic scale computed with the Europarl corpus and using a interpolated smoothed\n3-gram language model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .75\n4.1 Difference of using a0–1 loss function (on the left) and an approximation to the true\nclass probability as the loss function (on the right) when using a loss function of the\nsort of Eq. (4.12). The left-scale of the y axis shows a possible actual probability over\nthe target sentences. The right-scale of the y axis shows thevalue of the loss function\nwhen a mistake is made. Finally, the x axis is an inﬁnite enumeration of the numerical\nidentiﬁers corresponding to the inﬁnite enumerable set of possible classes (or target\nsentences in the SMT case). . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . 86\n4.2 Asymmetry of the IBM Model 2 measured with the respect to the WER for the TOURIST\ntest set for different training sizes. . . . . . . . . . . . . . . . . . .. . . . . . . . . . 94\n4.3 WER results for the TOURIST test set for different training sizes and different classiﬁ-\ncation rules. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .94\n4.4 SER results for the TOURIST test set for different training sizes and different classiﬁ-\ncation rules. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .95\n4.5 The WER results obtained for the Europarl test set (Spanish to English) with the length\nof the reference sentences restricted to be less than the value of thex-axis. . . . . . . . 98\n4.6 The BLEU results (on the left y-scale) and the brevity penalty (BP) of BLEU score (on\nthe right y-scale) obtained for the Europarl test set (Spanish to English) with the length\nof the reference sentences restricted to be less than the value of thex-axis. . . . . . . . 99\n4.7 Difference between the remaining information and the probability as error functions. . 100\n5.1 Three possible segmentations of the translation pair ”por favor , súbanos nuestros\nbultos a la habitación .” and ”please , send up our luggage\nto the room .”. The used segmentations are:j = (0 , 2, 3, 4, 6, 7, 8, 9, 10) and\ni = (0 , 1, 2, 4, 6, 7, 8, 9, 10) for the ﬁrst segmentation;j = (0 , 3, 6, 8, 10) and i =\n(0, 2, 6, 8, 10) for the second; andj = (0 , 6, 8, 10) and i = (0 , 2, 5, 10) for the third. . 109\n6.1 Directed, multi-stage graph representing all possiblebilingual segmentations for an in-\nput sentence of length4 and an output sentence of length5. Each node deﬁnes a dif-\nferent segment; the ﬁrst two digits of the node label are the segment limits in the input\nsentence, while the other two digits correspond to the output sentence. . . . . . . . . 122\n6.2 BLEU (%) as a function of the maximum phrase length threshold, for the baseline\napproach and our phrase-based HMM (PHMM). . . . . . . . . . . . . . . .. . . . . 128\n7.1 A generative example of the hidden semi-Markov model approach to machine transla-\ntion, in which a source stringx4\n1 is translated to a target stringy5\n1 through a segmenta-\ntion of both sentences into4 concepts. . . . . . . . . . . . . . . . . . . . . . . . . . 134\n7.2 The relevant values that should be computed for the forward recurrence in the case of a\nsource sentence of20 words and a target sentence of22 words. The maximum phrase\nlength is assumed to be3 for both source and target phrases. Finally, the black squares\n(■ ) stand for theαtl values that must be computed while the remaining points are not\nneeded. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n168 JAF-DSIC-UPV\nList of Tables\n2.1 Basic information on the data sets used in the experiments. (Singletonsare words that\noccur once;Class n-tonsrefers to words that occur inn classes exactly.) . . . . . . . . 41\n2.2 Summary of the best results. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . 42\n3.1 Table with some statistics of the both corpora used in theexperiments. . . . . . . . . . 65\n3.2 Some statistics of the both test sets. . . . . . . . . . . . . . . . .. . . . . . . . . . . 65\n3.3 Percentage of out of vocabulary words (OOV) in test as a function of the training size\nand the percentage of vocabulary size. The ﬁgures representpercentages, i.e.,5.4 stands\nfor5.4% words. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.4 V ocabulary size as a function of the training size and thepercentage of the full vocabu-\nlary. The ﬁgures represent Kilo-words, i.e.,8.7 stands for8.7K words. . . . . . . . . . 67\n3.5 Perplexities on the corpora for abacking-offsmoothed 3-gram language model.Sk\nOOV column stands for the perplexity skipping the OOV , while theAllcolumn accu-\nmulates all the events (OOV and known). . . . . . . . . . . . . . . . . . .. . . . . . 76\n3.6 Perplexities on the corpora for alinear interpolationsmoothed3-gram language model.\nSk OOV column stands for the perplexity skipping the OOV , while theAllcolumn ac-\ncumulates all the events (OOV and known). . . . . . . . . . . . . . . . .. . . . . . . 77\n4.1 Basic statistics of the Spanish-English TOURIST task. . . . . . . . . . . . . . . . . . 92\n4.2 Statistics of the Europarl corpus . . . . . . . . . . . . . . . . . . .. . . . . . . . . . 92\n4.3 Statistics of the Xerox corpus . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . 93\n4.4 Translation quality results with different translation rules for TOURIST test set for a\ntraining set of170K sentences. Where T is the time expressed in seconds and SE stands\nfor the percentage ofsearch errors. . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\n4.5 The results of translation quality obtained using the proposed variety of loss functions\nwith the Europarl test set. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . 96\n4.6 Translation quality results obtained, using the proposed variety of loss functions, with\nthe test set of Xerox task. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . 97\n4.7 Differences between some translation examples obtained using DTR and ITR. Bold\nwords highlight the differences between the two proposed translations. REF stands for\nthe reference translation. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . 98\n5.1 Basic statistics of the Spanish-English EU T RANS -I task, wheresingletonsstands for the\nwords occurring once, andrunning wordsdenotes the total amount of word occurrences. 114\n169\nList of Tables\n5.2 Results obtained with the EU T RANS -I task for different algorithms: SGIATI (the EM\nversion), and GIATI, which corresponds to the model obtained by counting the occur-\nrences of each segment and then re-normalising by the sum of all counts. . . . . . . . 114\n6.1 Basic statistics of the Spanish-English EU T RANS -I task, whererunning wordsdenotes\nthe total amount of word occurrences. . . . . . . . . . . . . . . . . . . .. . . . . . . 127\n6.2 Basic statistics of the EUROPARL -10 corpus whererunning wordsdenotes the total\namount of word occurrences. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . 127\n7.1 A full domain speciﬁcation for both segmentation variables,m and l, in the case of a\nsource sentence of4 words and a target sentence of5 words. For better understanding\nof these variables, the induced segmentation in both sourceand target sentences is also\nprovided in columns3 and 5. Although there is a possible segmentation of the target\nsentencey into5 segments (last row), it is not the case for the source sentence x. . . . 136\n7.2 Basic statistics of the training sets. . . . . . . . . . . . . . . .. . . . . . . . . . . . 146\n7.3 Basic statistics of the test sets. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . 146\n7.4 Results for the Europarl-10 corpus with a maximum phrase length of4. . . . . . . . . 147\n7.5 Results for the Europarl-20 corpus with a maximum phrase length of4. . . . . . . . . 147\n7.6 Results for the Europarl-20 corpus with several phrase length. . . . . . . . . . . . . . 148\n7.7 Some translation examples (Sp→ En) before and after training the phrase table,4\niterations with the Viterbi training, and maximum phrase length of4 words. . . . . . . 149\n7.8 Basic statistics of Europarl-20 with development set. .. . . . . . . . . . . . . . . . . 149\n7.9 Basic statistics of Europarl-v3. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . 150\n7.10 Results for several translation systems on the Europarl-20 corpus. . . . . . . . . . . . 150\n7.11 Results for several translation systems on the Europarl-v3 corpus. . . . . . . . . . . . 151\n170 JAF-DSIC-UPV",
  "topic": "Monotone polygon",
  "concepts": [
    {
      "name": "Monotone polygon",
      "score": 0.6453423500061035
    },
    {
      "name": "Persona",
      "score": 0.5147902965545654
    },
    {
      "name": "Translation (biology)",
      "score": 0.45766761898994446
    },
    {
      "name": "Humanities",
      "score": 0.40313494205474854
    },
    {
      "name": "Philosophy",
      "score": 0.3386123776435852
    },
    {
      "name": "Mathematics",
      "score": 0.3256204128265381
    },
    {
      "name": "Geometry",
      "score": 0.1381312608718872
    },
    {
      "name": "Biology",
      "score": 0.07573723793029785
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": []
}