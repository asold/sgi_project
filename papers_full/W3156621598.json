{
    "title": "ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration",
    "url": "https://openalex.org/W3156621598",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A591795076",
            "name": "Chen Jun-yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3034178926",
            "name": "He, Yufan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226780455",
            "name": "Frey, Eric C",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096271395",
            "name": "Li Ye",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2124527706",
            "name": "Du Yong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1970928383",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3098269293",
        "https://openalex.org/W3097065222",
        "https://openalex.org/W1987869189",
        "https://openalex.org/W2170167891",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2107956652",
        "https://openalex.org/W2963840672",
        "https://openalex.org/W3101454806",
        "https://openalex.org/W2751297520",
        "https://openalex.org/W2103857226",
        "https://openalex.org/W603908379",
        "https://openalex.org/W2964121744"
    ],
    "abstract": "In the last decade, convolutional neural networks (ConvNets) have dominated and achieved state-of-the-art performances in a variety of medical imaging applications. However, the performances of ConvNets are still limited by lacking the understanding of long-range spatial relations in an image. The recently proposed Vision Transformer (ViT) for image classification uses a purely self-attention-based model that learns long-range spatial relations to focus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution features because of the consecutive downsamplings, result in a lack of detailed localization information, making it unsuitable for image registration. Recently, several ViT-based image segmentation methods have been combined with ConvNets to improve the recovery of detailed localization information. Inspired by them, we present ViT-V-Net, which bridges ViT and ConvNet to provide volumetric medical image registration. The experimental results presented here demonstrate that the proposed architecture achieves superior performance to several top-performing registration methods.",
    "full_text": "ViT-V-Net: Vision Transformer for Unsupervised\nVolumetric Medical Image Registration\nJunyu Chen1,2 jchen245@jhmi.edu\nYufan He1 yhe35@@jhu.edu\nEric C. Frey1,2 efrey@jhmi.edu\nYe Li1,2 yli192@@jhu.edu\nYong Du2 duyong@jhu.edu\n1 Department of Electrical and Computer Engineering, Johns Hopkins University, USA\n2 Department of Radiology and Radiological Science, Johns Hopkins Medical Institutes, USA\nAbstract\nIn the last decade, convolutional neural networks (ConvNets) have dominated and achieved\nstate-of-the-art performances in a variety medical imaging applications. However, the per-\nformances of ConvNets are still limited by lacking the understanding of long-range spatial\nrelations in an image. The recently proposed Vision Transformer (ViT) for image classiﬁ-\ncation uses a purely self-attention-based model that learns long-range spatial relations to\nfocus on the relevant parts of an image. Nevertheless, ViT emphasizes the low-resolution\nfeatures because of the consecutive downsamplings, result in a lack of detailed localization\ninformation, making it unsuitable for image registration. Recently, several ViT-based im-\nage segmentation methods have been combined with ConvNets to improve the recovery of\ndetailed localization information. Inspired by them, we present ViT-V-Net, which bridges\nViT and ConvNet to provide volumetric medical image registration. The experimental\nresults presented here demonstrate that the proposed architecture achieves superior per-\nformance to several top-performing registration methods. Our implementation is available\nat https://bit.ly/3bWDynR.\nKeywords: Image Registration, Vision Transformer, Convolutional Neural Networks.\n1. Introduction\nDeformable image registration (DIR) is fundamental for many medical image analysis tasks.\nIt functions by of establishing spatial correspondences between points in a pair of ﬁxed\nand moving images through a spatially varying deformation model. Traditionally, DIR\ncan be performed by solving an optimization problem that maximizes the image similarity\nbetween the deformed moving and ﬁxed images while enforcing smoothness constraints\non the deformation ﬁeld (Beg et al., 2005; Avants et al., 2008; Vercauteren et al., 2009).\nHowever, such optimization problems need to be solved for each pair of images, making those\nmethods computationally expensive and slow in practice. Since recently, ConvNets-based\nregistration methods (de Vos et al., 2017; Balakrishnan et al., 2018; Sokooti et al., 2017;\nChen et al., 2020) have become a major focus of attention due to their fast computation\ntime after training while achieving comparable accuracy to state-of-the-art methods.\nDespite ConvNets’ promising performance, ConvNet architectures generally have limi-\ntations in modeling explicit long-range spatial relations (i.e., relations between two voxels\nthat are far away from each other) present in an image due to the intrinsic locality of convo-\nlution operations (Chen et al., 2021). Many works have been proposed to overcoming this\nlimitation, e.g. U-Net (Ronneberger et al., 2015) (or V-Net (Milletari et al., 2016)), atrous\nconvolution (i.e, dilated convolution) (Yu and Koltun, 2015), and self-attention (Vaswani\n© J. Chen, Y. He, E.C. Frey, Y. Li & Y. Du.\narXiv:2104.06468v1  [eess.IV]  13 Apr 2021\nChen He Frey Li Du\nFigure 1: Method overview and network architecture of ViT-V-Net.\net al., 2017). Recently, there has been an increasing interest in developing self-attention-\nbased architectures due to their great success in natural language processing. Methods\nlike non-local networks (Wang et al., 2018), detection transformer (DETR) (Carion et al.,\n2020), and Axial-deeplab (Wang et al., 2020) have exhibited superior performance in com-\nputer vision tasks. Dosovitskiy et al. (Dosovitskiy et al., 2020) proposed Vision Transformer\n(ViT), a ﬁrst purely self-attention-based network, and achieved state-of-the-art performance\nin image recognition. Subsequent to this progress, TransUnet (Chen et al., 2021) was devel-\noped on the basis of a pre-trained ViT for 2-dimensional (2D) medical image segmentation.\nHowever, medical imaging modalities generally produce volumetric images (i.e., 3D images),\nand 2D images do not fully exploit the spatial correspondences obtained from 3D volumes.\nTherefore, developing 3D methods is more desirable in medical image registration. In this\nwork, we present the ﬁrst study to investigate the usage of ViT for volumetric medical\nimage registration. We propose ViT-V-Net that employs a hybrid ConvNet-Transformer\narchitecture for self-supervised volumetric image registration. In this method, the ViT was\napplied to high-level features of moving and ﬁxed images, which required the network to\nlearn long-distance relationships between points in images. Long skip connections between\nencoder and decoder stages were used to retain the ﬂow of localization information. The\nexperimental results demonstrated that a simple swapping of the network architecture of\nVoxelMorph with Vit-V-Net could produce superior performance to both VoxelMorph and\nconventional registration methods.\n2. Methods\nLet f ∈RH×W×L and m∈RH×W×L be ﬁxed and moving image volumes. We assume that\nf and m are single-channel grayscale images, and they are aﬃnely aligned. Our goal is to\npredict a transformation function φ that warps m (i.e., m◦φ) to f, where φ = Id+ u, u\ndenotes a ﬂow ﬁeld of displacement vectors, and Id denotes the identity. Fig. 1 presents\nan overview of our method. First, the deep neural network ( gθ) generates u, for the given\nimage pair f and m, using a set of parameters θ (i.e., u = gθ(f,m)). Then, the warping\n(i.e., m◦φ) is performed via a spatial transformation function (Jaderberg et al., 2015).\nDuring network training, image similarity between m◦φ and f is compared, and the loss\nis backpropagated into the network.\nViT-V-Net ArchitectureNaive application of ViT to full-resolution volumetric images\nleads to large computational complexity. Here, instead of feeding full-resolution images\ndirectly into the ViT, the images (i.e., f and m) were ﬁrst encoded into high-level feature\nrepresentations via a series of convolutional layers and max-poolings (blue boxes in Fig.\n1). In the ViT (orange box), the high-level features were then separated into N vectorized\nP3 ×C patches, where N = HWL\nP3 , P denotes the patch size, and C is the channel size.\nNext, the patches were mapped to a latent D-dimensional space using a trainable linear\n2\nViT-V-Net for Volumetric Medical Image Registration\nFigure 2: Registration results of a MR coronal slice. Additional results are shown in Appendix D.\nprojection (i.e., patch embedding). Learnable position embeddings are then added to the\npatch embeddings to retain positional information of the patches (Dosovitskiy et al., 2020).\nNext, the resulting patches were fed into the Transformer encoder, which consisted of 12\nalternating layers of Multihead Self-Attention (MSA) and Multi-Layer Perceptron (MLP)\nblocks (Vaswani et al., 2017) (see Appendix A for details of ViT). Finally, the output from\nViT was reshaped and then decoded using a V-Net style decoder. Notice that long skip\nconnections between the encoder and decoder were also used. The network’s ﬁnal output is\na dense displacement ﬁeld, which was then used in the spatial transformer for warping m.\nLoss FunctionsThe image similarity measurement used in this study was mean squared\nerror (MSE), along with a diﬀusion regularizer controlled by a weighting parameter λ for\nimposing smoothness in the displacement ﬁeld u (see Appendix B for formulation).\nAﬃne only NiftyReg SyN VoxelMoprh-1 VoxelMoprh-2 ViT-V-Net\nDice 0.569±0.171 0.713 ±0.134 0.688 ±0.140 0.707 ±0.137 0.711 ±0.135 0.726±0.130\nTable 1: Overall Dice comparisons between the proposed method and the others. Detailed\nperformance on various anatomical structures are shown in Fig. 6 of Appendix D.\n3. Results and Conclusions\nWe demonstrate our method on the task of brain MRI registration. We used an in-house\ndataset that consists of 260 T1–weighted brain MRI scans. The dataset was split into 182,\n26, and 52 (7:1:2) volumes for training, validation, and test sets. Each image volume was\nrandomly matched to two other volumes to form four pairs off and m, resulting in 768, 104,\nand 208 image pairs. Standard pre-processing steps for structural brain MRI, including skull\nstripping, resampling, and aﬃne transformation were performed using FreeSurfer (Fischl,\n2012). Then, the resulting volumes were cropped to an equal size of 160 ×192 ×224. Label\nmaps including 29 anatomical structures were obtained using FreeSurfer for evaluation. The\nproposed method was compared in terms of Dice score (Dice, 1945) to Symmetric Normal-\nization (SyN)1 (Avants et al., 2008), NiftyReg 2 (Modat et al., 2010), and a learning-based\nmethod, VoxelMorph3-1 and -2 (Balakrishnan et al., 2018). The regularization parameter,\nλ, was set to be 0.02, which was reported in (Balakrishnan et al., 2018) as an optimal value\nfor VoxelMorph. The method was implemented using PyTorch (Paszke et al., 2019). De-\ntailed hyperparameter settings for training are shown in Appendix C. Qualitative results,\nand Dice scores are shown in Table 1 and Fig. 2. As visible from the results, the proposed\nViT-V-Net yielded a signiﬁcant gain of > 0.1 in Dice performance ( p-values are shown in\nTable. 4) compared to the others. We also noticed that ViT-V-Net reached lower loss\nvalues and had higher validation Dice scores during training (see Fig. 4 in Appendix D).\nIn conclusion, the proposed ViT-based architecture achieved superior performance than the\ntop-performing registration methods, demonstrating the eﬀectiveness of ViT-V-Net.\n1. Implementation of SyN was obtained from https://github.com/ANTsX/ANTsPy\n2. Implementation of NiftyReg was obtained from https://www.ucl.ac.uk/medical-image-computing\n3. Implementation of VoxelMorph was obtained from http://voxelmorph.csail.mit.edu\n3\nChen He Frey Li Du\nAcknowledgments\nThis work was supported by a grant from the National Cancer Institute, U01-CA140204.\nThe views expressed in written conference materials or publications and by speakers and\nmoderators do not necessarily reﬂect the oﬃcial policies of the NIH; nor does mention\nby trade names, commercial practices, or organizations imply endorsement by the U.S.\nGovernment.\nReferences\nBrian B Avants, Charles L Epstein, Murray Grossman, and James C Gee. Symmetric\ndiﬀeomorphic image registration with cross-correlation: evaluating automated labeling of\nelderly and neurodegenerative brain. Medical image analysis, 12(1):26–41, 2008.\nGuha Balakrishnan, Amy Zhao, Mert R Sabuncu, John Guttag, and Adrian V Dalca. An\nunsupervised learning model for deformable medical image registration. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 9252–9260, 2018.\nM Faisal Beg, Michael I Miller, Alain Trouv´ e, and Laurent Younes. Computing large\ndeformation metric mappings via geodesic ﬂows of diﬀeomorphisms.International journal\nof computer vision, 61(2):139–157, 2005.\nNicolas Carion et al. End-to-end object detection with transformers. In European Confer-\nence on Computer Vision, pages 213–229. Springer, 2020.\nJieneng Chen et al. Transunet: Transformers make strong encoders for medical image\nsegmentation. arXiv preprint arXiv:2102.04306, 2021.\nJunyu Chen, Ye Li, Yong Du, and Eric C Frey. Generating anthropomorphic phantoms using\nfully unsupervised deformable image registration with convolutional neural networks.\nMedical physics, 2020.\nBob D de Vos et al. End-to-end unsupervised deformable image registration with a con-\nvolutional neural network. In Deep learning in medical image analysis and multimodal\nlearning for clinical decision support, pages 204–212. Springer, 2017.\nLee R Dice. Measures of the amount of ecologic association between species. Ecology, 26\n(3):297–302, 1945.\nAlexey Dosovitskiy et al. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. arXiv preprint arXiv:2010.11929, 2020.\nBruce Fischl. Freesurfer. Neuroimage, 62(2):774–781, 2012.\nMax Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial\ntransformer networks. arXiv preprint arXiv:1506.02025, 2015.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n4\nViT-V-Net for Volumetric Medical Image Registration\nFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional\nneural networks for volumetric medical image segmentation. In 2016 fourth international\nconference on 3D vision (3DV), pages 565–571. IEEE, 2016.\nMarc Modat et al. Fast free-form deformation using graphics processing units. Computer\nmethods and programs in biomedicine, 98(3):278–284, 2010.\nAdam Paszke et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d 'Alch´ e-Buc, E. Fox, and\nR. Garnett, editors, Advances in Neural Information Processing Systems 32, pages\n8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/\n9015-pytorch-an-imperative-style-high-performance-deep-learning-library.\npdf.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In International Conference on Medical image computing\nand computer-assisted intervention, pages 234–241. Springer, 2015.\nHessam Sokooti et al. Nonrigid image registration using multi-scale 3d convolutional neural\nnetworks. In International Conference on Medical Image Computing and Computer-\nAssisted Intervention, pages 232–239. Springer, 2017.\nAshish Vaswani et al. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nTom Vercauteren, Xavier Pennec, Aymeric Perchant, and Nicholas Ayache. Diﬀeomorphic\ndemons: Eﬃcient non-parametric image registration. NeuroImage, 45(1):S61–S72, 2009.\nHuiyu Wang et al. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation.\nIn European Conference on Computer Vision, pages 108–126. Springer, 2020.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural net-\nworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 7794–7803, 2018.\nFisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions.\narXiv preprint arXiv:1511.07122, 2015.\n5\nChen He Frey Li Du\nAppendix A. Overview of Vision Transformer\nA detailed description of ViT can be found in (Dosovitskiy et al., 2020; Vaswani et al., 2017;\nChen et al., 2021).\nFigure 3: Model overview of the Vision Transformer.\nPatch EmbeddingLet xi\np be the ith vectorized patch, where i∈{1,...,N }. The patches\nwere ﬁrst encoded into a latent D-dimensional space using a trainable linear projection\n(realized via a convolutional layer). Then, learnable position embeddings were added to\nretain positional information:\nz0 = [x1\npE; x2\npE; ...; xN\np E] + Epos, (1)\nwhere E ∈RP3C×D denotes the patch embedding projection and Epos ∈RN×D represents\nthe positional embedding matrix. Next, the output z0 was fed into consecutive blocks of\nthe Transformer encoder.\nTransformer encoderThe Transformer encoder consists of 12 blocks of MSA and MLP\nlayers (Vaswani et al., 2017). A layer normalization (LN) was applied before each MSA and\nMLP layer. The output of ℓth Transformer encoder can be written as:\nz′\nℓ = MSA(LN(zℓ−1)) + zℓ−1\nzℓ = MLP(LN(z′\nℓ)) + z′\nℓ, (2)\nwhere zℓ denotes the encoded image representation.\nAppendix B. Loss Functions\nThe loss function used for training the proposed network can be written as:\nL(f,m,φ ) = LMSE(f,m,φ ) + λLdiffusion(φ), (3)\nwhere λ is a regularization parameter, f and m are, respectively, the ﬁxed and moving\nimage, and φ represents the deformation ﬁeld.\nImage Similarity MeasurementThe mean squared error (MSE) between the deformed\nmoving image and ﬁxed image was used as the loss function. It is deﬁned as:\nLMSE(f,m,φ ) = 1\nΩ\n∑\np∈ω\n[f(p) −m◦φ(p)]2 , (4)\nwhere Ω denotes the image domain.\n6\nViT-V-Net for Volumetric Medical Image Registration\nDeformation Field RegularizationTo enforce smoothness in the deformation ﬁeld, a\ndiﬀusion regularizer was used. It is deﬁned as:\nLdiffusion(φ) =\n∑\np∈ω\n∥∇u(p)∥2, (5)\nwhere u the displacement ﬁeld, which is the output of the network.\nAppendix C. Hyperparameters Settings\nVoxelMoprh-1 VoxelMoprh-2 ViT-V-Net\nOptimizer ADAM ADAM ADAM\nLearning rate 1e−4 1e−4 1e−4\nLearning rate decay Polynomial (0.9) Polynomial (0.9) Polynomial (0.9)\nDropout 0.0 0 .0 0.1\nEpochs 500 500 500\nBatch size 2 2 2\nLoss function MSE MSE MSE\nRegularizer Diﬀusion Diﬀusion Diﬀusion\nRegularization parameter (λ) 0.02 0 .02 0 .02\nData augmentation Random ﬂipping Random ﬂipping Random ﬂipping\nViT patch size (P) - - 8\nViT latent vector size ( D) - - 252\nGPU memory used during training 17.320 GiB 19.579 GiB 18.511 GiB\nTable 2: Training setups for the learning-based models. All models were trained using the same\noptimizer (ADAM (Kingma and Ba, 2014)) and training hyperparameters, except the dropout rate\nwas set to be 0.1 in linear layers of ViT-V-Net. The models were trained and tested on a PC with\nan AMD Ryzen 9 3900X CPU, an NVIDIA Titan RTX GPU, and an NVIDIA RTX 3090 GPU,\nwhere both GPUs have 24 GiB memory. Each model took about 3 days to train on a single GPU.\nCost fuction Regularizer Regularization parameter Number of iteration\nNiftyReg SSD Bending energy (default) 0.0002 300, 300, 300 (default)\nSyN MSQ Gaussian (default) 3 (default) 40, 20, 0 (default)\nTable 3: Hyperparameter settings for NiftyReg and SyN, where SSD stands for the sum of\nsquared diﬀerence and MSQ stands for the mean squared diﬀerence. We chose the regularization\nparameter for NiftyReg to be 0.0002, because the default value, 0.005, led to over-smoothed\nsuboptimal deformations.\nAppendix D. Additional Results\nNiftyReg SyN VoxelMorph-1 VoxelMorph-2 ViT-V-Net\nDice 0.713±0.134 0.688 ±0.140 0.707 ±0.137 0.711 ±0.135 0.726 ±0.130\n% of |Jφ|≤ 0 0.225±0.165 0.118 ±0.084 0.375 ±0.098 0.414 ±0.084 0.381 ±0.102\nTime (Sec) 113 15.257 0.002 0.002 0.002\nTable 4: Quantitative comparisons of Dice score, percentage of voxels with a non-positive\nJacobian determinant (i.e., folded voxels), and computational time for diﬀerent methods. Note\nthat NiftyReg and SyN were applied using the CPUs, while the learning-based methods,\nVoxelMorph and ViT-V-Net, were implemented on GPU.\n7\nChen He Frey Li Du\nAﬃne NiftyReg SyN VoxelMorph-1 VoxelMorph-2\nViT-V-Net ≪1e−5 4.102e−3 ≪1e−5 1.536e−5 1.601e−3\nTable 5: p-values computed using the paired t-test on the Dice scores between the proposed\nViT-V-Net and other registration methods.\nFigure 4: Training loss value and validation Dice score per epoch. The proposed ViT-V-Net\nexhibits lower loss values and higher Dice scores during training.\nFigure 5: Boxplots of Dice scores for various anatomical structures obtained using diﬀerent reg-\nistration methods. Dice scores of the left and right brain hemispheres were averaged into a single\nscore. Orange triangles denote the means.\n8\nViT-V-Net for Volumetric Medical Image Registration\nMovingTargetDeformation Field\n Deformed Moving\nViT-V-Net\nDeformation Field Deformed Moving\nVoxelMorph-2\nDeformation Field Deformed Moving\nSyN\nDeformation Field Deformed Moving\nVoxelMorph-1\nDeformation Field Deformed Moving\nNiftyReg\nFigure 6: Additional qualitative results generated by diﬀerent registration methods. The rows in\nthe top panel (of two rows separated by a dashed line) show, respectively, moving and ﬁxed images.\nThe other ﬁve panels exhibit deformed images and their corresponding displacement ﬁelds produced\nby diﬀerent methods. The colored images were created by ﬁrst clamping the displacement values to\na range of [ −10,10] and then mapping each spatial dimension to each of the RGB color channels.\n9"
}