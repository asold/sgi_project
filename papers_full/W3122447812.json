{
  "title": "2D Positional Embedding-based Transformer for Scene Text Recognition",
  "url": "https://openalex.org/W3122447812",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5034906156",
      "name": "Zobeir Raisi",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A5022593450",
      "name": "Mohamed A. Naiel",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A5078015739",
      "name": "Paul Fieguth",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A5015738185",
      "name": "Steven Wardell",
      "affiliations": [
        "ATS Automation Tooling Systems (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A5077041853",
      "name": "John Zelek",
      "affiliations": [
        "University of Waterloo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2182264803",
    "https://openalex.org/W6779399279",
    "https://openalex.org/W1981283549",
    "https://openalex.org/W2253806798",
    "https://openalex.org/W6681452975",
    "https://openalex.org/W6652761251",
    "https://openalex.org/W1971822075",
    "https://openalex.org/W2786148769",
    "https://openalex.org/W2298368322",
    "https://openalex.org/W2593572697",
    "https://openalex.org/W2194187530",
    "https://openalex.org/W6929439978",
    "https://openalex.org/W2810983211",
    "https://openalex.org/W2927058921",
    "https://openalex.org/W2153182373",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W7027429494",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2146835493",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2905498003",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3012835873",
    "https://openalex.org/W2980094293",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2892009249",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W6757010476",
    "https://openalex.org/W6752378368",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W6681025997",
    "https://openalex.org/W4295224299",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2963712589",
    "https://openalex.org/W4295246343",
    "https://openalex.org/W2140132917",
    "https://openalex.org/W3034414401",
    "https://openalex.org/W3035206215",
    "https://openalex.org/W3101411491",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2303931467",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963517393",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2008806374",
    "https://openalex.org/W2132122100",
    "https://openalex.org/W3033633564",
    "https://openalex.org/W2144554289",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3004846386"
  ],
  "abstract": "Recent state-of-the-art scene text recognition methods are primarily based on Recurrent Neural Networks (RNNs), however, these methods require one-dimensional (1D) features and are not designed for recognizing irregular-text instances due to the loss of spatial information present in the original two-dimensional (2D) images. In this paper, we leverage a Transformer-based architecture for recognizing both regular and irregular text-in-the-wild images. The proposed method takes advantage of using a 2D positional encoder with the Transformer architecture to better preserve the spatial information of 2D image features than previous methods. The experiments on popular benchmarks, including the challenging COCO-Text dataset, demonstrate that the proposed scene text recognition method outperformed the state-of-the-art in most cases, especially on irregular-text recognition.",
  "full_text": "2D Positional Embedding-based Transformer for Scene Text Recognition\nZobeir Raisi Vision and Image Processing Lab, University of Waterloo, ON, N2L 3G1, Canada\nMohamed A. Naiel Vision and Image Processing Lab, University of Waterloo, ON, N2L 3G1, Canada\nPaul Fieguth Vision and Image Processing Lab, University of Waterloo, ON, N2L 3G1, Canada\nSteven Wardell ATS Automation Tooling Systems Inc., Cambridge, ON, N3H 4R7, Canada\nJohn Zelek Vision and Image Processing Lab, University of Waterloo, ON, N2L 3G1, Canada\nEmail: {zraisi, mohamed.naiel, pﬁeguth, jzelek}@uwaterloo.ca, swardell@atsautomation.com\nAbstract\nRecent state-of-the-art scene text recognition methods are primar-\nily based on Recurrent Neural Networks (RNNs), however, these\nmethods require one-dimensional (1D) features and are not de-\nsigned for recognizing irregular-text instances due to the loss of\nspatial information present in the original two-dimensional (2D) im-\nages. In this paper, we leverage a Transformer-based architecture\nfor recognizing both regular and irregular text-in-the-wild images.\nThe proposed method takes advantage of using a 2D positional\nencoder with the Transformer architecture to better preserve the\nspatial information of 2D image features than previous methods.\nThe experiments on popular benchmarks, including the challeng-\ning COCO-Text dataset, demonstrate that the proposed scene text\nrecognition method outperformed the state-of-the-art in most cases,\nespecially on irregular-text recognition.\n1 Introduction\nScene text recognition aims to convert detected text or an image\npatch of words into characters or words. Since the properties of\nscene text will generally be quite different from those of scanned\ndocuments, it is difﬁcult to develop an effective text recognition\nmethod based on classical OCR or handwriting recognition meth-\nods, such as [1–3]. This difﬁculty stems from images captured in\nthe wild including various challenging conditions [4] such as images\nof low resolution [5, 6], lightning extremes [5, 6], environmental con-\nditions [7, 8], fonts [7–9], orientation [9], languages [10] and lexicons\n[5, 6].\nScene text recognition methods [11–16] have mainly utilized\ndeep convolutional neural networks (DCNNs) [17, 18] and Recur-\nrent Neural Networks (RNNs) [19], frameworks that are inspired\nfrom natural language processing. Some methods [12–14] have\nused connectionist temporal classiﬁcation (CTC) [20], and others\n[15, 16], adopted an attention mechanism [21] for the prediction of\ncharacter sequences. Although these methods [12–14] achieved\ngood performance on regular-text datasets, containing primarily ex-\namples of horizontal text, their accuracy declines on irregular text\ndatasets [6, 7, 9, 22] that contain curved and multi-oriented text.\nSeveral recent methods [11, 12, 15, 16] have attempted to over-\ncome the irregular-text challenge using rectiﬁcation [23, 24]. For\ninstance, Shi et al.[11, 15] proposed a text recognition system that\ncombined attention-based sequence and a STN module to rectify\nirregular text, followed by a RNN for word recognition. However, the\nresulting recognition accuracy remains far from expectations.\nThe Transformer’s architecture [25] is a novel framework, intro-\nduced ﬁrst for natural language processing (NLP), taking advan-\ntage of both convolutional neural networks (CNNs) and RNNs. The\narchitecture is less sensitive to the position of input sequences,\ncompared to RNN and LSTM frameworks that contain inductive\nbias [26], because position information is not inherently encoded\namong the input set of sequences. The speciﬁc reason is that the\nself-attention and feed-forward network (FFN) layers used in Trans-\nformer make it permutative equivalent, i.e., it computes the output\nof each element in the input sequence independently. Although the\n1D Positional Encoding (PE) technique used in Transformer [25] is\nable to address the permutation equivalent problem that may exist\nin natural language processing related 1D sequences, it is not ca-\npable of capturing the horizontal and vertical features generated by\nthe CNNs for a 2D input image [27].\nIn this paper, we ﬁrst extend the transformer architecture of [25]\nto be applicable to the recognition of 2D text images without relying\non text rectiﬁcation. To that end, in our proposed method we adopt\na generalization of the original transformer’s 1D encoding [25] to be\napplicable for the 2D image feature by extending the positional en-\ncoder from 1D to 2D. Experimental results show that the proposed\nscene text recognition architecture provides higher accuracy than\nthat of the state-of-the-art techniques on seven out of eight chal-\nlenging datasets.\n2 Background\nTransformer’s architecture has been initially introduced in [28]\nfor machine translation by using a new attention-based mecha-\nnism. This architecture introduces self-attention layers, which scan\nthrough each element of a sequence and update it by measuring\nthe relationship between this element and the whole sequence [28].\nThe main advantages of attention-based models in transformer are\ntheir parallel computations suitability at lower memory cost, which\nmakes them more suitable than recurrent neural networks (RNNs)\n[19, 29] on learning from long sequences. This transformer archi-\ntecture [28] has been later exploited in natural language processing\n(NLP) [30, 31] and it has been recently integrated in several suc-\ncessful applications in speech recognition [32] and computer vision\n[33–35].\nBy dropping the PE layer, the Transformer’s architecture can\nbe viewed as a stack of N blocks (Bn|n = 1,2, ..N), which each block\nconsists of a self-attentionAn(·) and Feed-ForwardFn(·) layers. The\nself-attention layer, the key deﬁning part of Transformer, is a normal\nattention block that allows the model to learn and access informa-\ntion of the past hidden layers. Let x = [x1,x2, ...,xt ]⊤∈Rt×d, within\nt and d denote the length and dimension of the input sequence.\nEach row of the self-attention function A1(x) can be demonstrated\nas a weighted sum of the value matrix V ∈Rt×d, with the weights\ndetermined by similarity scores between the key matrix K ∈Rt×d\nand query matrix Q ∈Rt×d as follows:\nA1(x) =Softmax\n(QK⊤\n√\nd\n)\nV,\nQ = [q1,q2, ...,qt ]⊤, qi = Wqxi +bq,\nK = [k1,k2, ...,kt ]⊤, ki = Wkxi +bk,\nV = [v1,v2, ...,vt ]⊤, vi = Wvxi +bv,\n(1)\nwhere W and b are the weight and bias parameters introduced in\nA1(·). As seen in Figure 1, Rather than only computing the atten-\ntion once, the multi-head mechanismruns through the scaled dot-\nproduct attention in (1) multiple times in parallel.\n3 Methodology\nFigure 1 illustrates the proposed architecture for scene text recogni-\ntion that inherited from the standard Transformer’s architecture [25].\nWe can categorize it into two main modules: encoder and decoder.\nThe main role of the encoder is to extract high-level 2D feature rep-\nresentations of an input image, and the decoder is used to convert\nthese feature maps to a sequence of characters.\n3.1 Encoder\nThe proposed encoder module utilizes the multi-head self-attention\nmechanism presented in Section 2, as well as three main sub-\nblocks that are as follows: (a) CNN Feature Extraction, (b) Spatial\n2D-Positional Encoding, and (c) Feed-forward network (FFN), which\ncan be described as follows.\nCNN Feature Extraction:A CNN ﬁrst processes the input image\nto extract a compact feature representation and learn a 2D repre-\nsentation of an input image. We adopt a modiﬁed ResNet-31 archi-\ntecture [18] for the CNN backbone. During implementation, all the\ninput images are converted into grayscale and resized to 32 ×100\npixels.\nSpatial 2D-Positional Encoding: Transformer is permutation\nequivalent [36], so some extra care is required to retain the 2D\nstructure of the image. To that effect, in our proposed models we\nMulti-Head\nSelf-Attention\nMulti-Head\nSelf-Attention\nMulti-Head\nSelf-Attention\nFFN\nAdd, Norm\nAdd, Norm\nAdd, Norm\nLinear\nSoftmax\nAdd, Norm\nFFN\nAdd, Norm\nCNN Character\nEmbedding\n“W”\nQKV QKV\nQKV\n“A”\n2D Positional\nEncoding\n1D Positional\nEncoding\nNd\nNe\nDecoder\nEncoder\nFig. 1: The proposed text recognition using transformer architec-\nture, where Ne and Nd denote the number of layers in the encoder\nand decoder. Unlike [25], our proposed architecture utilizes 2D po-\nsitional encoding, a ResNet-31 backbone and a FFN layer.\nadopt a generalization of the original transformer’s 1D encoding [25]\nto be applicable for the 2D image feature by adding a ﬁxed 2D po-\nsitional encoding Φ(·) made of sinuses of different frequencies as\nfollows:\nΦ(x,y,2i) =sin\n(\nx ·c4i/d\n)\n,\nΦ(x,y,2i +1) =cos\n(\nx ·c4i/d\n)\n,\nΦ(x,y,2 j +d/2) =sin\n(\ny ·c4 j/d\n)\n,\nΦ(x,y,2 j +1 +d/2) =cos\n(\ny ·c4 j/d\n)\n. (2)\nwhere c = 10−4, x and y specify the horizontal and vertical positions,\nand i, j ∈[0,d/4] and d denotes the dimension. The PE signals in\n(2) are added to the 2D feature outputs of the CNN block in Figure\n1. We then concatenate the encoded CNN features to get the ﬁnal\nd-channel positional encoding.\nFeed-Forward Network (FFN) Layers:Here, we used a modiﬁed\nversion of the FFN layer in the original transformer [25] to make it\nmore robust in capturing the features generated by the encoder’s\nmulti-head self-attention mechanism. The modiﬁed FFN consists of\n2 layers of 1×1 convolutions with ReLU activations [37] followed by\na residual connection after the 2 layers.\n3.2 Decoder\nThe decoder module we used follows the standard architecture of\nthe transformer in [25]. Its main role is to use an autoregressive\nmodel to predict the decoded sequence of characters by attending\nthe visual features generated by the encoder to predict the next\nsequence of characters.\n4 Experimental Results\nIn this section, we present an experimental evaluation for the pro-\nposed method and a selected state-of-the-art scene text recognition\n[11–16] techniques on recent public datasets [5–9, 22, 38] that in-\nclude wide variety of challenges.\nDatasets: We use two type of datasets for evaluating the recog-\nnition results (1) regular-text recognition datasets: IIIT5k [5], SVT\n[39], ICDAR03 [38] and ICDAR13 [8] that mainly contain horizontal\ntext, and (2) irregular-text recognition datasets: ICDAR15 [7], SVT -\nP [22], CUT80 [9] and COCO-Text [6], which contain multi-oriented\nand curved text, and these datasets are more challenging than reg-\nular datasets.\nEvaluation Metrics: Word recognition accuracy (WRA) is a com-\nmonly used evaluation metric, due to its application in our daily life\ninstead of character recognition accuracy, for assessing the text\nrecognition schemes [11–13, 15, 16]. Given a set of cropped word\nimages, WRA is deﬁned as follow:\nWRA (%) = No. of Correctly Recognized Words\nTotal Number of Words ×100 (3)\nQuantitative Results: By applying the proposed architecture in\nFigure 1 on benchmark datasets, we trained our model on 36\nclasses of characters. Table 1 shows a comparison in terms of\nthe WRA for the proposed method vs the methods in considera-\ntion. As seen from this table, the proposed model achieves com-\npetitive WRA results compared to most of the state-of-the-art meth-\nods in different datasets. For three regular-text dataset, SVT [39],\nICDAR03 [38] and ICDAR13 [8], it outperformed all the state-of-the-\nart methods with WRA of 89.34%, 95.85% and 93.89%, respec-\ntively. Furthermore, it achieved the best performance on SVT -P\n[22] dataset, which contains only curved and irregular text, with\na large margin of 4% compared to all other methods. This per-\nformance demonstrates the strength of the Transformer network in\nrecognizing arbitrary shapes of text compared to RNN-based meth-\nods even without using a text rectiﬁcation module. Unlike the recent\nTransformer-based scene text recognition method in [27] that de-\npends on ResNet-101 and adaptive 2D PE module, the proposed\nmethod utilizes a lighter backbone architecture, namely ResNet-31,\nand ﬁxed 2D PE as can be seen in (2).\nQualitative Results: Figure 2 shows the qualitative performances\nfor the proposed methods on some failure cases that provide by\n[16], which all the methods in [11–16] failed on these images. As\nshown in Figure 2(a), the proposed method recognized correctly all\nthese images that mostly contain irregular text. We also show some\nfailure cases of our proposed model in Figure 2(b), which it mainly\nfailed in images that contain occluded characters. It worth noting\nthat despite the proposed scheme offers \"guice\" for the sample la-\nbelled as \"guide\", this partially occluded example looks for several\nhumans as \"guice\" as well, which indicates a noisy ground truth\ndata and the robustness of our model to partial-occlusion.\n5 Conclusion\nIn this paper, we have presented a new scene text recognition ar-\nchitecture based on integrating a 2D positional encoder with the\nTransformer. Furthermore, we have proposed a new feed-forward-\nnetwork layer in the encoder module to make it more robust in cap-\nturing the features generated by the encoder’s self-attention mech-\nanism. The new proposed scene text recognition architecture bet-\nter preserves the spatial information in 2D image features than the\nprior methods. Experimental results on eight public datasets have\ndemonstrated that the proposed scene text recognition method has\noffered higher WRA than six recent state-of-the-art RNN-based\nmodels in most of the cases, specially on irregular-text recognition\ndatasets. Since the Transformer’s architecture requires more com-\nputations than RNN-based frameworks in the inference time, we\nwould like to optimize the speed of the proposed Transformer’s to\nmake it faster in scene text recognition.\nAcknowledgments\nThe authors would like to thank the Ontario Centres of Excellence\n(OCE) - Voucher for Innovation and Productivity II (VIP II) - Canada\nprogram, and ATS Automation Tooling Systems Inc., Cambridge,\nON Canada, for supporting this research work\nReferences\n[1] H. Bunke and P . S.-p. Wang, Handbook of character recogni-\ntion and document image Anal.World scientiﬁc, 1997.\nTable 1:Comparing the WRA of some of the recent text recognition techniques using IIIT5k [5], SVT [39] , ICDAR03 [38], ICDAR13 [8],\nICDAR15 [7], SVT -P [22], CUT80 [9] and COCO-Text [6] datasets. Best and second best methods are highlighted by bold and underline,\nrespectively.\nMethod IIIT5k SVT ICDAR03 ICDAR13 ICDAR15 SVT -P CUT80 COCO-Text\nCRNN [13] 82.73% 82.38% 93.08% 89.26% 65.87% 70.85% 62.72% 48.92%\nRARE [11] 83.83% 82.84% 92.38% 88.28% 68.63% 71.16% 66.89% 54.01%\nROSETTA [14] 83.96% 83.62% 92.04% 89.16% 67.64% 74.26% 67.25% 49.61%\nSTAR-Net [12] 86.20% 86.09% 94.35% 90.64% 72.48% 76.59% 71.78% 55.39%\nCLOVA [16] 87.40% 87.01% 94.69% 92.02% 75.23% 80.00% 74.21% 57.32%\nASTER [15] 93.20% 89.20% 92.20% 90.90% 74.40% 80.90% 81.90% 60.70%\nProposed 89.23% 89.34% 95.85% 93.89% 75.78% 84.34% 84.03% 65.80%\nLUNDEKWAM MarlboroCLUB LIGHTS CO. WELCOME DENVERCommunity\nMANTOUCHE\nKARTOUCHE\n guide\nguice\nGETTER\n PM\n10,000\nANTICOMMENT\nGT:\nPredicted LITTER\nArIsoro\nMarlboro I'M\n(b) Failed\nimages\n(a) Predicted\n correctly\nFig. 2:Qualitative results of the proposed method, which in (a) shows the correctly predicted, and (b) illustrates the failure cases. It must\nbe noted that all the methods in [11–16] have been failed on the above images.\n[2] J. Zhou and D. Lopresti, “Extracting text from www images,” in\nICDAR, vol. 1, 1997, pp. 248–252.\n[3] N. Arica and F . T. Y arman-Vural, “An overview of character\nrecognition focused on off-line handwriting,” IEEE Trans. on\nSyst. , Man, and Cybernetics, Part C (Appl. and Reviews),\nvol. 31, no. 2, pp. 216–233, 2001.\n[4] Z. Raisi, M. A. Naiel, P . Fieguth, S. Wardell, and J. Zelek, “Text\ndetection and recognition in the wild: A review,” arXiv preprint\narXiv:2006.04305, 2020.\n[5] A. Mishra, K. Alahari, and C. V. Jawahar, “Scene text recogni-\ntion using higher order language priors,” inBMVC, 2012.\n[6] A. Veit, T. Matera, L. Neumann, J. Matas, and S. Be-\nlongie, “Coco-text: Dataset and benchmark for text de-\ntection and recognition in natural images,” arXiv preprint\narXiv:1601.07140, 2016.\n[7] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. Ghosh,\nA. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R. Chan-\ndrasekhar, S. Luet al., “Icdar 2015 competition on robust read-\ning,” inICDAR, 2015, pp. 1156–1160.\n[8] D. Karatzas, F . Shafait, S. Uchida, M. Iwamura, L. G. i Big-\norda, S. R. Mestre, J. Mas, D. F . Mota, J. A. Almazan, and\nL. P . De Las Heras, “Icdar 2013 robust reading competition,” in\nICDAR, 2013, pp. 1484–1493.\n[9] A. Risnumawan, P . Shivakumara, C. S. Chan, and C. L. Tan,\n“A robust arbitrary text detection system for natural scene im-\nages,”Expert Syst. with Appl., vol. 41, no. 18, pp. 8027–8048,\n2014.\n[10] M. Iwamura, N. Morimoto, K. Tainaka, D. Bazazian, L. Gomez,\nand D. Karatzas, “Icdar2017 robust reading challenge on om-\nnidirectional video,” in Proc. IAPR ICDAR, vol. 1, 2017, pp.\n1448–1453.\n[11] B. Shi, X. Wang, P . Lyu, C. Y ao, and X. Bai, “Robust scene text\nrecognition with automatic rectiﬁcation,” inIEEE CVPR, 2016,\npp. 4168–4176.\n[12] W. Liu, C. Chen, K.-Y . K. Wong, Z. Su, and J. Han, “STAR-Net:\nA spatial attention residue network for scene text recognition,”\nin BMVC. BMVA Press, September 2016, pp. 43.1–43.13.\n[13] B. Shi, X. Bai, and C. Y ao, “An end-to-end trainable neural\nnetwork for image-based sequence recognition and its appli-\ncation to scene text recognition,” TPAMI, vol. 39, no. 11, pp.\n2298–2304, 2016.\n[14] F . Borisyuk, A. Gordo, and V. Sivakumar, “Rosetta: Large scale\nsystem for text detection and recognition in images,” in Proc.\nACM SIGKDD Int. Conf. on Knowledge Discovery & Data Min-\ning, 2018, pp. 71–79.\n[15] B. Shi, M. Y ang, X. Wang, P . Lyu, C. Y ao, and X. Bai, “Aster:\nAn attentional scene text recognizer with ﬂexible rectiﬁcation,”\nTPAMI, 2018.\n[16] J. Baek, G. Kim, J. Lee, S. Park, D. Han, S. Yun, S. J. Oh,\nand H. Lee, “What is wrong with scene text recognition model\ncomparisons? dataset and model analysis,” inICCV, 2019.\n[17] B. Su and S. Lu, “Accurate scene text recognition based on\nrecurrent neural network,” inACCV. Springer, 2014, pp. 35–\n48.\n[18] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,”IEEE CVPR, pp. 770–778, 2015.\n[19] S. Hochreiter and J. Schmidhuber, “Long short-term memory,”\nNeural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n[20] A. Graves, S. Fernández, F . Gomez, and J. Schmidhuber,\n“Connectionist temporal classiﬁcation: labelling unsegmented\nsequence data with recurrent neural networks,” inICML, 2006,\npp. 369–376.\n[21] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[22] T. Quy Phan, P . Shivakumara, S. Tian, and C. Lim Tan, “Rec-\nognizing text with perspective distortion in natural scenes,” in\nICCV, 2013, pp. 569–576.\n[23] M. Jaderberg, K. Simonyan, A. Zisserman, and\nK. Kavukcuoglu, “Spatial transformer networks,” in Proc.\nInt. Conf. on Neural Inf. Process. Syst. - Volume 2. MIT\nPress, 2015, pp. 2017–2025.\n[24] F . Zhan and S. Lu, “Esir: End-to-end scene text recognition via\niterative image rectiﬁcation,” in IEEE CVPR, 2019, pp. 2059–\n2068.\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you\nneed,” inAdvances in neural information processing systems,\n2017, pp. 5998–6008.\n[26] X. Liu, H.-F . Yu, I. Dhillon, and C.-J. Hsieh, “Learning to encode\nposition for transformer with continuous dynamical model,”\narXiv preprint arXiv:2003.09229, 2020.\n[27] J. Lee, S. Park, J. Baek, S. Joon Oh, S. Kim, and H. Lee, “On\nrecognizing texts of arbitrary shapes with 2D self-attention,” in\nIEEE CVPR, 2020, pp. 546–547.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention\nis all you need,” in Advances in neural information\nprocessing systems, 2017, pp. 5998–6008. [Online]. Available:\nhttps://arxiv.org/pdf/2005.12872.pdf\n[29] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning\nrepresentations by back-propagating errors,”nature, vol. 323,\nno. 6088, pp. 533–536, 1986.\n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,”arXiv preprint arXiv:1810.04805, 2018.\n[31] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, “Language models are unsupervised multitask\nlearners,”OpenAI Blog, vol. 1, no. 8, p. 9, 2019.\n[32] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-\nrecurrence sequence-to-sequence model for speech recog-\nnition,” in 2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2018, pp.\n5884–5888.\n[33] N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N. Shazeer,\nA. Ku, and D. Tran, “Image transformer,” arXiv preprint\narXiv:1802.05751, 2018.\n[34] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video\naction transformer network,” inProceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 2019,\npp. 244–253.\n[35] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, “Self-\nattention generative adversarial networks,” arXiv preprint\narXiv:1805.08318, 2018.\n[36] N. Carion, F . Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\narXiv preprint arXiv:2005.12872, 2020.\n[37] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve re-\nstricted boltzmann machines,” inICML, 2010.\n[38] S. M. Lucas, A. Panaretos, L. Sosa, A. Tang, S. Wong, and\nR. Y oung, “Icdar 2003 robust reading competitions,” inICDAR,\n2003. Proceedings., Aug 2003, pp. 682–687.\n[39] K. Wang and S. Belongie, “Word spotting in the wild,” in Proc.\nEur. Conf. on Comp. Vision. Springer, 2010, pp. 591–604.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7238538265228271
    },
    {
      "name": "Transformer",
      "score": 0.7012381553649902
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6693019270896912
    },
    {
      "name": "Encoder",
      "score": 0.6662915349006653
    },
    {
      "name": "Embedding",
      "score": 0.6398718357086182
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5764096975326538
    },
    {
      "name": "Architecture",
      "score": 0.5106161236763
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5001580715179443
    },
    {
      "name": "Text recognition",
      "score": 0.4199436604976654
    },
    {
      "name": "Computer vision",
      "score": 0.3576238453388214
    },
    {
      "name": "Image (mathematics)",
      "score": 0.26945942640304565
    },
    {
      "name": "Engineering",
      "score": 0.09799221158027649
    },
    {
      "name": "Geography",
      "score": 0.0875072181224823
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}