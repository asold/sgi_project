{
  "title": "FOTCA: hybrid transformer-CNN architecture using AFNO for accurate plant leaf disease image recognition",
  "url": "https://openalex.org/W4386715079",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1020642997",
      "name": "Bo Hu",
      "affiliations": [
        "Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2097505075",
      "name": "Wenqian Jiang",
      "affiliations": [
        "First Affiliated Hospital of Nanchang University",
        "Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2079046795",
      "name": "Juan Zeng",
      "affiliations": [
        "Second Affiliated Hospital of Nanchang University",
        "Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2007067578",
      "name": "Chen Cheng",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104495514",
      "name": "Laichang He",
      "affiliations": [
        "Nanchang University",
        "First Affiliated Hospital of Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A1020642997",
      "name": "Bo Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097505075",
      "name": "Wenqian Jiang",
      "affiliations": [
        "Nanchang University",
        "First Affiliated Hospital of Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2079046795",
      "name": "Juan Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2007067578",
      "name": "Chen Cheng",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "Nanchang University",
        "First Affiliated Hospital of Nanchang University"
      ]
    },
    {
      "id": "https://openalex.org/A2104495514",
      "name": "Laichang He",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6648926971",
    "https://openalex.org/W3093238181",
    "https://openalex.org/W6805850384",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6765836972",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6741769367",
    "https://openalex.org/W6705521900",
    "https://openalex.org/W6792309431",
    "https://openalex.org/W6640036494",
    "https://openalex.org/W6753636147",
    "https://openalex.org/W3152830573",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W4297087989",
    "https://openalex.org/W4315650202",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W4226530256",
    "https://openalex.org/W6641023773",
    "https://openalex.org/W6754842893",
    "https://openalex.org/W4306964662",
    "https://openalex.org/W6771134910",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6802996638",
    "https://openalex.org/W3202844853",
    "https://openalex.org/W4316664434",
    "https://openalex.org/W2891951760",
    "https://openalex.org/W56385144",
    "https://openalex.org/W2401346899",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W3139434170",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2997300818",
    "https://openalex.org/W4205484978",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2889139686",
    "https://openalex.org/W1993309459",
    "https://openalex.org/W2737725206",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W2961018736",
    "https://openalex.org/W1955942245",
    "https://openalex.org/W3211783547"
  ],
  "abstract": "Plants are widely grown around the world and have high economic benefits. plant leaf diseases not only negatively affect the healthy growth and development of plants, but also have a negative impact on the environment. While traditional manual methods of identifying plant pests and diseases are costly, inefficient and inaccurate, computer vision technologies can avoid these drawbacks and also achieve shorter control times and associated cost reductions. The focusing mechanism of Transformer-based models(such as Visual Transformer) improves image interpretability and enhances the achievements of convolutional neural network (CNN) in image recognition, but Visual Transformer(ViT) performs poorly on small and medium-sized datasets. Therefore, in this paper, we propose a new hybrid architecture named FOTCA, which uses Transformer architecture based on adaptive Fourier Neural Operators(AFNO) to extract the global features in advance, and further down sampling by convolutional kernel to extract local features in a hybrid manner. To avoid the poor performance of Transformer-based architecture on small datasets, we adopt the idea of migration learning to make the model have good scientific generalization on OOD (Out-of-Distribution) samples to improve the model’s overall understanding of images. In further experiments, Focal loss and hybrid architecture can greatly improve the convergence speed and recognition accuracy of the model in ablation experiments compared with traditional models. The model proposed in this paper has the best performance with an average recognition accuracy of 99.8% and an F1-score of 0.9931. It is sufficient for deployment in plant leaf disease image recognition.",
  "full_text": "FOTCA: hybrid transformer-CNN\narchitecture using AFNO for\naccurate plant leaf disease\nimage recognition\nBo Hu1, Wenqian Jiang2, Juan Zeng3, Chen Cheng4\nand Laichang He2*\n1School of Information Engineering, Nanchang University, Nanchang, China,2Department of\nRadiology, the First Afﬁliated Hospital of Nanchang University, Nanchang, China,3Second Clinical\nMedical College, Nanchang University, Nanchang, China,4School of Mathematics and Statistics,\nHuazhong University of Science and Technology, Wuhan, China\nPlants are widely grown around the world and have high economic beneﬁts.\nplant leaf diseases not only negatively affect the healthy growth and\ndevelopment of plants, but also have a negative impact on the environment.\nWhile traditional manual methods of identifying plant pests and diseases are\ncostly, inefﬁcient and inaccurate, computer vision technologies can avoid these\ndrawbacks and also achieve shorter control times and associated cost\nreductions. The focusing mechanism of Transformer-based models(such as\nVisual Transformer) improves image i nterpretability and enhances the\nachievements of convolutional neural network (CNN) in image recognition, but\nVisual Transformer(ViT) performs poorly on small and medium-sized datasets.\nTherefore, in this paper, we propose a new hybrid architecture named FOTCA,\nw h i c hu s e sT r a n s f o r m e ra r c h i t e c t u r eb a s e do na d a p t i v eF o u r i e rN e u r a l\nOperators(AFNO) to extract the global features in advance, and further down\nsampling by convolutional kernel to extract local features in a hybrid manner. To\navoid the poor performance of Transfo rmer-based architecture on small\ndatasets, we adopt the idea of migration learning to make the model have\ngood scientiﬁc generalization on OOD (Out-of-Distribution) samples to improve\nthe model’s overall understanding of images. In further experiments, Focal loss\nand hybrid architecture can greatly improve the convergence speed and\nrecognition accuracy of the model in ablation experiments compared with\ntraditional models. The model proposed in this paper has the best\nperformance with an average recognition accuracy of 99.8% and an F1-score\nof 0.9931. It is sufﬁcient for deployment in plant leaf disease image recognition.\nKEYWORDS\nplant leaf disease image recognition, hybrid architecture, transformer-based models,\nadaptive Fourier Neural Operator, deep learning\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nMuhammad Fazal Ijaz,\nMelbourne Institute of Technology,\nAustralia\nREVIEWED BY\nYongqiang Zheng,\nSouthwest University, China\nNisha Pillai,\nMississippi State University, United States\n*CORRESPONDENCE\nLaichang He\nlaichang_he@163.com\nRECEIVED 01 June 2023\nACCEPTED 21 August 2023\nPUBLISHED 12 September 2023\nCITATION\nHu B,Jiang W,Zeng J,Cheng C\nand He L (2023) FOTCA: hybrid\ntransformer-CNN architecture\nusing AFNO for accurate plant\nleaf disease image recognition.\nFront. Plant Sci.14:1231903.\ndoi: 10.3389/fpls.2023.1231903\nCOPYRIGHT\n© 2023 Hu, Jiang, Zeng, Cheng and He. This\nis an open-access article distributed under\nthe terms of theCreative Commons\nAttribution License (CC BY).The use,\ndistribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 12 September 2023\nDOI 10.3389/fpls.2023.1231903\n1 Introduction\nAs one of the main sources of economic growth in many\ndeveloping countries, it is necessary to produce agricultural crops\nin a stable and ef ﬁcient manner ( Iqbal et al., 2018 ). However,\nthe expansion of crops, combined with the overuse of pesticides\nand the exacerbation of global climate change, has led to an increase\nin the occurrence and spread of agricultural pests and diseases.\nTherefore, controlling these pests and diseases is becoming\nincreasingly challenging. Early detection and treatment of such\npests and diseases have unique advantages ( Sanju and VelammaL,\n2021). In the early stages of pest infestation, it is dif ﬁcult to\ndistinguish the leaves of affected plants from those of normal\nbecause of the high interclass variation in colour and pro ﬁle and\nthe low intraclass variation.\nTraditional methods for identifying agricultural pests typically\ninvolve visual inspection of crops by farmers and agricultural\nexperts. However, these approaches are costly, time-consuming,\nhighly subjective, and non-transferable ( Ouppaphan, 2017 ).\nAlthough some success has been achieved in classifying\nagricultural images using traditional image processing techniques,\nthese methods face several challenges. First, they often require\nmanual feature extraction, in creasing workload. Second, the\nmanually extracted features may not adequately represent the\ncharacteristics of agricultural images, leading to semantic gaps.\nLastly, the variability and complexity of agricultural images,\ncombined with factors such as image quality and shooting angle,\ncan signi ﬁcantly affect the ﬁnal recognition results, rendering these\ntechniques unsuitable for large-scale applications ( Bisen, 2021 ; Pan\net al., 2022 ).\nIn light of relevant research on plant leaf disease identiﬁcation and\nﬁne-grained recognition, we invest igated issues associated with the\nrelatively coarse recognition of algorithms in current crop pest\nidentiﬁcation approaches and their inadequate performance on\ndatasets containing multiple, similar pests and diseases. We\nrecognized the potential of the Transformer-based model and applied\nit to this domain. However, using the original patch and position\nembedding(Pap Embedding) would impose limitations on the\nexperiment outcomes. Concurrently, utilizing the adaptive Fourier\nbasis function to convert images to the frequency domain and\ndeploying a CNN-Transformer architecture to separately extract local\nand global features could enhance the training upper limit. As a solution,\nwe propose a novel hybrid architect ure for plant leaf disease image\nrecognition, termed FOTCA (where F and O signify Adaptive Fourier\nNeural Operator (AFNO) (Guibas et al., 2021) and TCA represents the\nTransformer-CNN Architecture). This approach addresses and\noptimizes the convergence issue and generalization capability of the\nViT model, further improving training outcomes. Additionally, we\nevaluate the performance of FOTCA u sing the Plant-village dataset, a\nplant leaf disease dataset, to assess its scope and effectiveness.\nIn summary, this work makes the following three\nmain contributions.\n This article applies an operator called Adaptive Fourier\nNeural Operator (AFNO) and learnable Fourier features\nwhich can replace traditional position encoding. Compared\nto traditional self-atte ntion, AFNO maps images to\nfrequency domain for better performance.\n A model architecture that integrates both global and local\nfeatures has been proposed, connecting CNNs and\nTransformers through inter-level concatenation to achieve\ncoupling of global and local receptive ﬁelds. This hybrid\narchitecture, which blends global and local features, can\nbetter utilize the extracted features and improve the\nperformance and robustness of the model.\n This article proposes that using Focal Loss as the loss\nfunction can effectively enhance the model ’s ability to\ntrain on dif ﬁcult samples.\nThe rest of this article is organized as follows. The Section 3\nmainly elucidates the details of the dataset composition and model\ncomposition structure, in Section 4 we specify the optimization\nscheme for settings other than the model.\n2 Related works\n2.1 Convolutional Neural Networks\nConvolutional Neural Networks (CNNs) have become a\npopular and powerful tool for image recognition in various ﬁelds.\nIn 1998, Lé cun et al. (1998) pioneering introduction of the LeNet-5\nmodel laid the fundamental framework for CNNs. In 2012, the\nAlexNet model ( Krizhevsky et al., 2012 ) used more convolutional\nlayers and a larger parameter space to ﬁt large-scale datasets in\nILSVRC (ImageNet Large-Scale Visual Recognition Challenge). It\nwas a groundbreaking demonstration of the advantages of deep\nneural networks over shallow neural networks, achieving\nsigniﬁcantly higher accuracy than the runner-up in recognition\naccuracy. This breakthrough established the status of CNNs in\ncomputer vision and brought new opportunities for plant leaf\ndisease identi ﬁcation.\n2.2 Fine-grained image recognition\nState-of-the-art deep learning algorithms for image recognition\npredominantly rely on publicly available datasets such as ImageNet.\nAlthough utilizing these datasets enhances a model ’s capability to\nidentify objects in images, including contour features ( Wang D. et\nal., 2021; Zhang and Wang, 2016 ), discerning different types of pests\naffecting the same plant leaves in plant leaf disease images remains\nchallenging due to their similar contour features ( Kong et al., 2021 ).\nMerely transferring a model trained on other data categories to the\nstudy of plant leaf disease might not yield the anticipated accuracy.\nConsequently, models should be encouraged to learn ﬁne-grained\nfeatures of objects for improved results ( Berg et al., 2014 ; Yang et al.,\n2018; Chen et al., 2019 )\nTo tackle this issue, numerous researchers have explored\nconvolutional neural networks in ﬁne-grained recognition. For\ninstance, Zhang et al. (2014) developed a model that surmounts\nthese constraints by utilizing deep convolutional features derived\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org02\nfrom bottom-up region proposals for ﬁne-grained recognition.\nMulti-proposal Net ( Zhang et al., 2016 ) obtains image blocks\nthrough the Edge Box Crop method and incorporates an output\nlayer of key points and visual features to further reinforce the local\nfeature positional relationship between local features and overall\ninformation. Deep LAC ( Lin et al., 2015 ) performs part localization,\nalignment and classi ﬁcation in the same network, and the VLF\n(valve linkage function) function is proposed for back propagation\nin Deep LAC, which is able to adaptively reduce the classi ﬁcation\nand alignment errors and update the localization results.\n2.3 Transformer-based models forﬁne-\ngrained visual recognition\nAs a decoding and encoding architecture model based on\nattention mechanism, Transformer ( Vaswani et al., 2017 ) has been\nwidely applied in the ﬁeld of natural language processing. Inspired by\nthis signi ﬁcant achievement, many scholars have transferred the\nTransformer structure to computer vision tasks and have achieved\nconsiderable success ( Carion et al., 2020 ; Dosovitskiy et al., 2020 ;\nWang Y. et al., 2021 ). The attention mechanism primarily identi ﬁes\nand classiﬁes objects through different parts of an object, thus making\nit possible to focus on key features of the subject for datasets of plant\nleaf diseases with small differences between inter-class. As a result,\nthe Transformer structure completes more successful recognition\ncompared to CNN structures. Focusing on this area of study, Cai et al.\n(2021) introduced a ViT with adaptive attention that adds attention\nweakening and strengthening modules. This improves the\nperformance of key features while capturing more feature\ninformation. Fu et al. (2017) used multiple scales of recurrent\nattention links to learn the target feature region and used within-\nscale classi ﬁcation loss and between-scale ranking loss to make the\nmodel more focused on the ﬁner-grained features of the target object.\nHe et al. (2022) proposed the TranFG framework, which integrates\nall original attention weights of the Transformer into one attention\nmap, enabling the model to recognize image blocks and calculate\ntheir relationships while utilizing contrastive loss to expand the\ndistance between confusion class feature representations. Sun\n(2019) improved the loss function in order to enable the\nclassiﬁcation model to learn features with greater distinction\nbetween more dif ﬁcult to distinguish classes, and used feature map\ninhibition methods to enable the model to learn subtle differences.\n3 Materials and methods\n3.1 Dataset and data pre-processing\nIn this study, the base dataset was selected from the publicly\navailable plant-village dataset on the web. This is a dataset\nspeciﬁcally designed to study the work of various plant leaf\ndisease recognition models, with a total of 54,303 images\ncontaining a total of 38 differ ent species of 13 plant species\n(including apple, blueberry, cherry, corn, grape, orange, bell\npepper, potato, raspberry, soybean, pumpkin, strawberry, tomato).\nPlant images, including non-plant leaf species, were inserted to train\nthe model to recognize non-plant leaf images. The ratio of the\ntraining and validation sets was 8:2.\nBefore inputting the images into the model, the images need to\nbe quanti ﬁed uniformly. In the ﬁrst step, the training set is\nexpanded by applying some image enhancement techniques to\neach image independently superimposed, and the main data\nenhancement methods used in this study are RandomFlip,\nRandomCrop, RandomHorizontalFlip and RandomResizeCrop,\nmainly to eliminate the shift of the ﬁnal recognition result by the\nchange of the shooting angle in real life, and to prevent the neural\nnetwork from over ﬁtting phenomenon. In addition, the image size\nis uniformly adjusted to a square of size 224*224 pixels to facilitate\nfurther processing by the model. Figure 1 shows a portion of the\ndataset images and the pre-processing process of the images.\n3.2 Model\nIn this paper, we study applying deep transfer learning to all\nmodels in the model and comparison experiments. The Pre-train\nand Fine-tuning approach in deep transfer learning is the most\nBA\nFIGURE 1\nPlant Village dataset presentation and data augmentation examples.(A) A partial sample images of the dataset (containing healthy and sick).(B) Data\naugmentation operation examples.\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org03\nconvenient and reliable method for deep model learning. By putting\na pre-trained model with some generalization ability onto a new\nsimilar dataset, the model will show superior performance than\ntraining a model from scratch after a simple and short training\nprocess. Similarly, the pre-trained model is adjusted for a speci ﬁc\ndataset by a loss calculation function called model ﬁne-tuning, and\nthe loss calculation function widely used in deep neural networks\ntoday is the CrossentropyLoss (CE) (Shown in Figure 2B ), which is\ncalculated as follows:\nLoss  = L(y, ^p)= −y log  (^p) − (1 − y) log  (1 − ^p) (1)\nwhere ^p is the predicted probability and y is the actual outcome\ncategory. A remarkable feature of this loss calculation is that the losses\nare also treated consistently for simple and easily scored samples ( pt\n≫ 0:5). This result may also occur when the losses of a large number\nof simple samples accumulate and the small loss may swamp the sparse\nclasses, or when there are differences in the loss of different classes in a\nnear-saturated training process. There are two intuitive ways to deal\nwith this type of problem, one is to add weighting factors directly to the\nloss function, and the other is to add adjustable factors represented by\nfocal loss. In this paper, focal loss is introduced into the model by\nreducing the loss function to the weights of the easy samples, which can\nhelp the loss function to favour the dif ﬁcult samples and improve the\naccuracy of the difﬁcult samples (Shown in Figure 2C). The calculation\nprocedure is as follows:\nLossfl = −(1 − pt )g log  (pt )( 2 )\nwhere pt reﬂects the proximity to the ground truth. Larger pt\nmeans closer to category y, representing more accurate recognition.\ng is the adjustable factor.\nDue to the wide recognition of the ImageNet dataset, as well as\nthe excellent performance and superior model ﬁtting ability of the\npre-trained models, all the experiments cited in this paper are based\non the pre-trained models of the ImageNet dataset loaded with the\ncorresponding models, effectively speeding up the training process\nof the models by means of transfer learning.\nThe FOTCA model studied in this article is mainly composed of\na shortcut module and a Transformer module. The proposed overall\nmodel structure is shown in the Figure 2A , which is mainly divided\ninto three steps: Patch and Position Embedding, Transformer\narchitecture based on adaptive Fourier Neural Operators\nand Classi ﬁer.\n3.2.1 Patch and position embedding\nThe module consists of two sub-modules: Patch Embedding\nand Positional encoding based on learnable Fourier features.\nCompared with the regular po sition embedding, a primary\nadvantage of Positional Embedding based on learnable Fourier\nfeatures is that it provides ri cher positional information,\nespecially for input from large datasets and long sequences. It can\nmore ﬁnely encode each position through learnable Fourier basis\nfunctions, and this learnability makes it more ﬂexible and adaptable.\nAdditionally, it has better local c onnectivity and translational\ninvariance when performing convolution operations, which can\nfurther enhance the model ’s performance.\nFor the input imageX ∈ RH/C2 W/C2 C, Assuming we use a patch size of\nP /C2 P, we can divide the input image into ( W\nP ) /C2 (H\nP ) local regions\nPi,j ∈ Rp/C2 pmesc according to the patch size. (i ∈ ½1, H\nP /C138 ,  j ∈ ½1, W\nP /C138 ).\nAnd expand it into a one-dimensional vectorvi,j, whose size is D, where\nD is an adjustable parameter representing the vector dimension after\npatch embedding. For each matrix, we use a learnable weight matrix\nWwatch of size D /C2 K to map it to a low dimensional space:\nxpatch \ni,j = Wpatch ·v i,j (3)\nThere, xpatch \ni,j ∈ RD/C2 K is the feature vector obtained through\npatch embedding.\nNext, we generate position encoding vectors for each patch. For\neach coordinate binary (i, j), use a learnable Fourier feature function\nB\nC\nA\nFIGURE 2\nFOTCA model architecture and two loss functions forﬁne-tuning (A) The ﬁgure provides an overview of the detailed structure of the entire FOTCA\nmodel’s neural network.(B) Cross-Entropy loss forﬁne-tuning. (C) Focal loss forﬁne-tuning.belﬂow chart.\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org04\nto generate a vector f pos\ni,j with a size of 2 K:\nf pos\ni,j (p)= ½cos  (〈p, w1〉 + b1), sin  (〈p, w1〉 + b1), … ,\ncos  (〈p, wK 〉 + bK ), sin  (〈p, wK 〉 + bK )/C138\n(4)\nWhere p =( (i−1)\nH , (j−1)\nW ) is the normalized representation of\ninput coordinates, wK and bK are learnable parameters, and 〈·, ·〉\nrepresents inner product between vectors. Finally, we add the\nposition encoding vector to the feature vector obtained from\npatch embedding, resulting in the ﬁnal feature vector xpos\ni,j :\nxpos\ni,j = xpatch\ni,j + f pos\ni,j (5)\nThen, we can use encoders to encode this sequence for further\nprocessing and recognition tasks.\n3.2.2 Adaptive Fourier Neural Operators\nThis module maps the input image to the frequency domain\nand uses adaptive Fourier basis functions to map each small block in\nthe time domain to the frequency domain. This allows better\nextraction of frequency domain features and offers advantages in\nprocessing periodic as well as regular images, while providing better\nrobustness to transformations such as rotation and scaling of the\ninput data. Speci ﬁcally, for each small block x, its frequency-domain\nrepresentation xfreq is obtained by ﬁrst mapping it from the time\ndomain to the frequency domain through Fast Fourier Transform\n(FFT). The ( i, j)th element represents the frequency-domain\nrepresentation at position ( i, j) on the frequency domain.\nXfreq\ni,j = o\nP−1\nm=0\no\nP−1\nn=0\nXm,ne−2pi(mi\nP +nj\nP ) (6)\nNext, we will map each patch from the time domain to the\nfrequency domain using adaptive Fourier basis functions.\nSpeciﬁcally, for each small block Xi,j,w ec a l c u l a t et h ei n n e r\nproduct between it and the adaptive Fourier basis functions, and\nuse the inner product value as coef ﬁcients.\nai,j,k = 1\np2 o\np\nu=1\no\np\nv=1\nXi,j(u, v)Bu,v,k (7)\nThe coefﬁcient of small block Xi,j under the kth adaptive Fourier\nbasis function is ai,j,k, p is the size of the small block, and Bu,v,k is the\nvalue of the kth adaptive Fourier basis function at position ( u, v).\nThe speci ﬁc calculation formula is as follows.\nBu,v,k = cos  ( wT\nk vu,v)+ i· sin  (wT\nk vu,v) (8)\nWhere vu,v denotes the spatial location vector of the center point\nof the ( u, v)th patch in the input image and wk represents the\nadaptive basis vector.\nExpand all small adaptive Fourier basis functions into a large\nadaptive Fourier basis function matrix B ∈ Cp2/C2 K , where K is the\nnumber of adaptive Fourier basis functions. At the same time, expand\nall small adaptive Fourier coef ﬁcients into a large adaptive Fourier\ncoefﬁcient matrix A ∈ CN/C2 K , where N is the number of small blocks.\nThen, by performing global average pooling on A, obtain the weight\nwk of each basis function, and the calculation formula is:\nwk = 1\n(H=P) /C2 (W=P) o\nH=P\ni=1\no\nW=P\nj=1\nai,j,k (9)\nThe original data is ultimately mapped to the frequency domain\nthrough basis functions and basis function weights.\nXi,j = o\nK\nk=1\nwk · Bi,j,k,: · xi,j,: (10)\nWhere Xi,j,: denotes the ﬂattened vector of patch (i,j). Finally, we\nmerge the frequency-domain results of all patches into a large\nfrequency-domain tensor F ∈ C\nH\nP /C2 W\nP /C2 k. For channel mixing, it is\ndone through an MLP on this frequency-domain tensor to extract\nhigh-level features.\nZ(k)\ni,j = MLP  F(k)\ni,j\n/C16/C17\n, i ∈ 1, H\nP\n/C20/C21\n, j ∈ 1, W\nP\n/C20/C21\n, k ∈ 1, N½/C138 (11)\nFinally, we concatenate all frequency domain features z together\nto form the ﬁnal feature vector.\nZ = z(1), z(2), … , z(F)\nhi\n(12)\nThe feature vector can be fed into any type of neural network for\nfurther training or inference operations. This process has strong\nexpressiveness and interpretability in data processing.\n3.2.3 Classiﬁer\nWhen using the Transformer-based model, its output vector is\noften passed to a classi ﬁer for image recognition. The traditional\napproach is to use the hidden layers of MLP to collect features and\nperform classi ﬁcation. However, there are many localized feature\npoints in plant leaf disease images, and these localized feature points\ncan usually be extracted and represented more ef ﬁciently by means\nof convolutional operations. Therefore, the use of CNNs may be\nmore appropriate for such problems. Here, we consider using a\nbasic block as a classi ﬁer, which includes convolutional layers and\nbatch normalization layers, as well as shortcut operations. By\nupsampling to a higher dimension to obtain local feature values,\nfeature transfer and information fusion are achieved. Also, shortcut\noperations can avoid gradient disappearance and model over ﬁtting\nproblems caused by excessive stacking of convolutional layers.\nAfter downsampling or upsampling the feature maps using\nconvolutional layers, the downsampled or upsampled feature maps\nare added to the output feature map using element-wise addition,\nwhich implements shortcut. This operation allows the model to\nbetter learn details and local features in the input, thereby\nimproving the model ’s performance. In this article ’s classi ﬁer\ndesign, a 1 /C2 1 convolutional layer is embedded in the shortcut to\nadjust the depth of the feature maps so they can be added to the\nfeature maps from the next layer. This helps preserve lower-level\nfeatures, enhance inter-channel communication, and improve the\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org05\nmodel’s performance. Speci ﬁcally, the input vector undergoes two\n3 /C2 3 convolutional layers consecutively, with a certain amount of\nnon-linear activation function (such as ReLU) inserted between\nthem, as shown below:\nx1 = ReLU(Conv 3/C2 3(x)) (13)\nx2 = ReLU(Conv 3/C2 3(x1)) (14)\nAt the same time, downsampling is applied to the input vector\nto promote the underlying features towards the ﬁnal recognition\nresult. This operation can be achieved by using a 1 /C2 1 convolution\nlayer in the shortcut. Finally, add the output of twoconsecutive\nconvolution layers to the output of the shortcut, as shown below:\nxout = ReLU (x 2 +x shortcut)\n= ReLU (x 2 + ReLU (Conv1/C2 1(x)))\n(15)\nAmong them, x shortcut is the output of the shortcut, and it needs\nto ensure that its depth is the same as the output depth of the second\nconvolution layer.\nIn the Classi ﬁer of this article, it has been demonstrated that\nshortcuts effectively alleviate t he gradient vanishing problem,\nresulting in a decent convergence rate even when applied to very\ndeep models. Using two 3 /C2 3 convolutional layers instead of a\nlarger kernel increases nonlinearity, allows for faster collection and\nextraction of local features, and reduces the number of parameters,\nmaking the model easier to train and generalize.\n4 Experiments and discussion\n4.1 Experimental environment\nThe models we studied was developed based on the open source\nof deep learning framework pytorch1.11.0 with the following\nexperimental equipment: CPU is Intel(R) Xeon(R) Platinum\n8255C, GPU is single card V100-SXM2-32GB, CUDA version is\n11.3, and programming language is Python3.8 (ubuntu20.04).\n4.2 Experimental parameters,\nevaluation indicators\nBased on earlier work by scholars ( Kumar et al., 2023a ; Kumar\net al., 2023b ; Wei et al., 2023 ), we selected DenseNet-169, Inception-\nv3, VGG19, ResNet-50, ResNet-101, ViT as our comparative\nexperimental models to verify the feasibility and reliability of the\nimproved model plant leaf disease image recognition method in this\npaper. We all iteratively update the pre-trained model parameters\nbased on each model to accelerate the model convergence during\nthe training process.\nThe training process uniformly uses the stochastic gradient\ndescent SGD optimization algorithm to optimize its model. The\nsame learning rate adjustment strategy is used for each parameter in\nthe model, and the learning rate is dynamically adjusted using\nLambdaLR, which is adjusted by adjusting the learning rate\naccording to the number of lear ning rate updates to linearly\ndecay on top of the original one. The loss calculation function of\nthe FOTCA model uses focal loss, and the rest of the models in the\ncomparison experiments use the c ross-entropy loss function.\nDropout regularization is also used in the training process. By\nrandomly dropping some neuron connections temporarily during\nthe training process, the purpose is to effectively avoid over ﬁtting of\nthe model during the training process. The generalization ability of\nthe model is also enhanced. The speci ﬁc hyperparameter settings\nfor the experiments are shown in Table 1 .\nWe choose accuracy, adjustment time for accuracy, loss,\nadjustment time for loss, F1-score, parameters and FLOPS. We\ndeﬁne for the ﬁrst time the adjustment time for accuracy (loss).\nAdjustment time is the number of iterations required during model\ntraining to bring the model performance from the initial\nperformance to 95% difference from the ﬁnal converged\nperformance. This metric re ﬂects the ability of the model in terms\nof speed of convergence as well as training ef ﬁciency. The concept of\nadjustment time helps to deeply evaluate and compare the speed of\nthe model training process. A shorter adjustment time means that\nthe model reaches its potential performance faster within a limited\nnumber of iterations. Thus, with this metric, researchers can more\nintuitively assess the performance gap between different models.\nIn addition, adjustment time complements other commonly\nused evaluation metrics (e.g., a ccuracy, loss, etc.) to provide\nresearchers with a more comprehensive view of how a model\nperforms during training. In the case of similar model\nperformance, shorter adjustment times may be an advantage due\nto greater savings in computational resources and time.\n4.3 Comparative experiments\nFirstly, the performance of Vision Transformer (ViT) is\ntypically affected by the size of the training dataset due to the fact\nthat ViT is based on transformers methods, which typically require\na large amount of data for effective training. This is because\ntransformers models, including ViT, are variants of the self-\nattention mechanism, which allows the model to capture the\nTABLE 1 Hyperparameter tuning for the model.\nInitial learning rate 0.001\nEpochs 100\nBatch size 8\nImage size(for all) 224*224\nImage size(for Inception-v3) 299*299\nTransformer-based models Embedding dimension 768\nPatch size 16\nHead number 8\nDepth 12\nThis table presents the various hyperparameters and their selected values used of the proposed\nFOTCA model and compared models.\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org06\nglobal relationships of the input data, but this mechanism also\nrequires a large amount of data to support.\nFor the speci ﬁc minimum sample requirement, it may vary as\nit depends on several factors, including the size of the model (e.g.,\nthe number of layers of the model, the number of hidden units, etc.),\nthe complexity of the task ( Lee et al., 2021 ) and the distribution of\nthe data. Therefore, it can only be explored by trying the following\nexperimental procedure.\nWe chose to compare the ViT model with the ResNet101\nmodel. This decision was made because ResNet101 has similar\nﬂops and comparable convergence capability to ViT on large-scale\ndatasets. In preliminary experi ments, we selected training set\nproportions of 6%, 8%, 10%, 20%, and 80%, respectively, to\ninvestigate the convergence capability and accuracy of ViT and\nResNet101 on medium-sized datasets. The experimental results are\nshown in Figures 3A – C.\nWe observed that ViT ’s training performance deteriorated\nrapidly on medium-sized datasets when the size of the training\nset was small. In contrast, ResNet101 demonstrated smaller\nvariations and more stable curves, with minimal decreases in\naccuracy. Particularly, there was a signi ﬁcant turning point in\nViT’s performance when the training set was at 8%. However,\nthese experiments were not able to establish a de ﬁnitive standard for\ndetermining the minimum sample requirements. On the contrary,\nthe optimal dataset size may vary depending on speci ﬁc applications\nand data characteristics. In many cases, if feasible, using larger\ndatasets often yields better results.\nNext, we compared six mainstream image recognition models,\nusing the same backbone training model and dataset to ensure the\nfairness of testing. The only exception is that the input image size\nmust be 299 /C2 299 for the Inception-v3 model. We show the\naccuracy and loss values of each model during the iteration\nprocess in the chart. Due to the signi ﬁcant difference between the\ninitial and ﬁnal iteration results, the observation of the later\niteration effect is not very clear and accurate. Therefore, we re-\ndrew the chart showing the changes in the accuracy and loss values\nof later iterations between 20 and 100 iterations to achieve deeper\nanalysis and optimization. This approach can enhance our\nunderstanding of the model ’s performance, which helps us\nfurther improve and optimize our algorithms. The ﬁnal\nexperimental effect comparison is shown in Figure 4 . Speci ﬁcally\nshown in Figure 3D . The plot offers a visual representation of the\nbalance between performance and computational ef ﬁciency among\ndifferent models. The ﬁnal result of the iteration is shown in Table 2.\nAccording to the chart, the six mainstream image recognition\nmodels showed good feature extraction and representation abilities\nin the ﬁrst 20 th epochs, with an accuracy of over 95% and a loss of\nbelow 0.35 for all of them. However, we observe a strange status quo\ndemonstrated in the ﬁgure: the growth curve of the Transformer-\nbased model is more stable, but the curve of the CNNs model\nB\nCD\nA\nFIGURE 3\n(A) Accuracy of the ViT model on small and medium-sized datasets: This plot demonstrates the training performance of the ViT model on datasets\nwith different sizes.(B) Accuracy of the ResNet101 model on small and medium-sized datasets: This plotdemonstrates the training performance of\nthe ResNet101 model on datasets with different sizes.(C) Comparison of the accuracy of ViT and ResNet101 for small and medium-sized datasets.\n(D) Scatter plot comparison of model accuracy and FLOPs(ﬂoating-point operations per second) for the models.\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org07\nﬂuctuates up and down consistently at 1% (in accuracy) and 0.03 (in\nloss), and DenseNet-169 even over ﬁtted, with its accuracy and loss\nat the 29 th epoch values are the highest (99.8%) and lowest (0.072)\nof the whole iteration, respectively. Subsequently, the accuracy\ngradually declined and ﬁnally stabilized at about 98.3%. This\nphenomenon may be due to thedifferent learning patterns and\nmodel depths of CNNs and Transformer-based models(containing\nFOTCA and ViT) during the training process.The convolutional\noperation of CNN networks requires a large number of parameters.\nIn contrast, the ViT network uses aself-attention layer instead of a\nﬁxed convolutional layer, which is better able to cope with long\nsequences of image features. The training error of ViT networks is\nusually less jittery due to the relatively simple training process of the\nself-attention layer. Meanwhile, the characteristics of the attention\nlink are more suitable for training ﬁne-grained plant leaf disease\nimage recognition tasks, which is further demonstrated inside the\nsection4.4. At the same time, over ﬁtting is sooner or later as long as\nthe network is large and deep enough, as indicated in ( Hinton et al.\n(2012). The large depth of DenseNet-169 (model depth of 169)\nreﬂects the over ﬁtting of the model more quickly.\nMeanwhile, comparing the adjustment time of various models,\nViT’s training results and FOTCA model performance are optimal,\nboth achieving near-maximum effects at the 9 th epoch. Other CNN\nmodels are relatively slower and require higher computational costs\nB\nCD\nA\nFIGURE 4\nShowing training accuracy and loss comparison between the proposed FOTCA model and compared models in the study.(A) Training accuracy\ncurve for all models.(B) Training loss curve for all models.(C) Partial training accuracy curve for all models.(D) Partial training loss curve for all\nmodels.\nTABLE 2 Performance metrics of the proposed FOTCA model and compared models.\nModel Accuracy(%) Adjustment time F1 score(%) Loss Adjustment time Params (M) FLOPS(G)\nDenseNet169 98.3 13 91.85 0.082 14 14.15 3.44\nInceptionv3 96.8 28 94.45 0.15 29 23.83 2.86\nResNet50 99.5 21 98.23 0.103 27 25.56 4.13\nResNet101 97.6 32 95.69 0.107 32 44.55 7.87\nVGG-19 97.8 28 91.42 0.206 30 78.14 29.96\nViT 99.4 9 98.03 0.026 9 58.07 11.28\nFOTCA 99.8 11 99.31 0.005 9 59.14 11.87\nModel Accuracy(%)Adjustment F1-Loss Adjustment Params(M) FLOPS(G).\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org08\nto achieve the same training effect. Therefore, FOTCA model\nexhibits ef ﬁciency and accuracy in feature extraction, learning\nability, and model convergence ef ﬁciency, making it a more\nexcellent image recognition model.\nTwo categories of performance can be identi ﬁed in the\ncomparison experiments. One is represented by models such as\nVGG-19, DenseNet-169 and Inception-v3, whose accuracy did not\nreach 98% but has already approached the best performance, so the\ntrend is no longer rising. The other category is represented by\nmodels such as ViT, ResNet-50, and ResNet-101, which are based\non FOTCA proposed in this article. During the iteration process,\nthese models can approach an accuracy of 99.9%, which means that\nalmost perfect image recognition results can be achieved through\nlimited training. Additionally, FOTCA has leading positions in both\naccuracy and F1-score indicators compared to other models, and it\nis the only model with an F1-score of 0.9931. On the basis of\nefﬁcient recognition using ViT, FOTCA further improves accuracy,\nloss, and F1-score without incre asing parameter quantity and\nFLOPS compared to other models. This makes its performance\nnearly perfect, as shown in all the ﬁnal parameters, parameter\nquantities, and FLOPS charts for all models in the experiment.\nTherefore, the FOTCA model proposed in this article achieves the\nbest performance in plant leaf disease recognition.\n4.4 Model visualization analysis\nWe proceeded to discuss the differences between the models in\nterms of their focus on image features by selecting 11 more\nrepresentative photographs from the dataset that contained\nfeatures that were essentially the basic features under the\ncategory. One photo of a healthy cherry leaf was taken with a\nglobal angle of view of the whole leaf and with a portion of the\nbackground included (this allows surveying the model ’s ability to\nsegment objects and backgrounds and verifying whether the model\ncan focus more on the objects themselves), with the diseased and\nhealthy traits shown on the leaf as curling at the edges of the\ndiseased leaf and wilting of the whole leaf, and one partial image of a\nleaf with grey leaf spot A partial image of a maize leaf with grey leaf\nspot, which does not include the background (this veri ﬁes the\nmodel’s ability to focus on ﬁne-grained features of the object and\nthe size of the receptive ﬁeld), and the trait of grey leaf spot on the\nmaize showing random grey-brown rectangular long or irregular\nlong spots and brown-black pockmarks throughout the leaf, with\nadditional longitudinal correlation images having similar features.\nThe graph shows the attention of the seven models to the same\nimage, where the warmer the colour, the more attention the model\npays to that feature, and the colour distribution gives an indication\nof the model ’s ability to accurately identify the image features. Each\nmodel feature concern map is showing in the Figure 5 .\nFirst of all, both FOTCA and ViT models focus more accurately\non the details of cherry leaf edges, but in the results, it was found\nthat the ViT model focuses more on the whole leaf surface, which\ngenerates more errors and retains unnecessary information, thus\nreducing the recognition accuracy. At the same time, none of the\nCNNs can accurately and carefully focus on the correct plant leaf\ndisease feature points, and the ﬂuctuating anomalies of the metrics\n(including accuracy and loss values) during the training process are\nproved accordingly, and it also sideways proves the results that the\ntraining of Convolutional Networks is not as good as the ViT effect\nin the recognition of plant leaf disease images.\nIn addition, during the recognition of maize grey leaf spot\nleaves, the FOTCA and ViT models focused on the brown-black\npockmarks on the leaves, and the ViT model focused more evenly\non the background colour similar to the subject colour, Inception-\nv3 was able to observe the rectangular long stripes on the image\nmore accurately, while the rest of the models focused on looser areas\nand could not re ﬂect a more convincing pattern. In turn, the rest of\nthe images show that the FOTCA model focuses more on the ﬁne-\ngrained features of the edges and less on the details of the whole leaf\nthan the ViT model, and the ﬁve CNN models, on the other hand,\nhave a more mixed focus, tending to focus on the global\ninformation and main features of the images. From this we can\nconclude that the Transformer-based models focuses more on the\nglobal features of the image, forming a global attention to the image,\nwhile the CNNs model focuses more on the subtle features, forming\na local attention to the image.\n4.5 Ablation experiment\nTo evaluate the effectiveness of each improvement module in\nthe FOTCA model, we conducted ablation experiments using the\noriginal dataset. We compared four models: the FOTCA model, the\nFOTCA model with original Pap Embedding, the FOTCA model\nwith MLP-Classi ﬁer, and the FOTCA model lacking data\naugmentation. We plotted the accuracy and loss curves for each\nmodel as a function of epoch value in Figure 6 . Detailed training\ndata is presented in Table 3 . Partial iteration accuracy and loss\nvalues are not shown here because the comparison of the individual\nmodels in the results of this experiment can be shown more clearly\nin Figure.\n4.5.1 Between FOTCA model and the model\nusing original pap embedding\nUsing Fourier transform can better capture texture and detail\ninformation in the image. This is because the Fourier transform can\nconvert spatial-domain information into frequency-domain\ninformation, allowing the model to better process this\ninformation. In addition, using Fourier transform can greatly\nreduce the dimensionality of the vector and thus reduce\ncomputation complexity.\nHowever, Fourier transformation may cause some spatial\ninformation loss, thereby affecting the performance of image\nrecognition tasks. From the experimental results, it can be seen\nthat using Pap Embedding with Fourier operator can partially\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org09\nimprove the accuracy and convergence speed of the model, thus\nachieving better performance. Compared with using original Pap\nEmbedding, the accuracy was improved by 0.7% and the loss\ndecreased by 0.014.\n4.5.2 Between FOTCA model and the model\nusing MLP− classiﬁer\nThe classi ﬁer used in this article contains shortcut connections\nacross layers, which can help information transmit more effectively,\navoid gradient vanishing and information bottleneck problems, and\nhas better explainability. In contrast, using MLP as the classi ﬁer will\nlose some important spatial structure information because MLP is a\nfully connected layer that ﬂattens all features together and ignores\ntheir relationships. The classiﬁer in this article can reconstruct spatial\nstructure information of images more accurately, retain more feature\ninformation, and improve the model ’s recognition accuracy. At the\nsame time, as a classi ﬁer, it can help reduce the risk of over ﬁtting by\nhelping the model learn more robust features that have similar\nFIGURE 5\nAttention heatmaps for the models on representative plant leaf disease images. The attention heatmaps visually demonstrate the regions within the\nimage that the respective models focus on, highlighting their effectiveness at capturing relevant features for accurate recognition.\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org10\nresponses for different input data, thus improving the model ’s\ngeneralization ability, which is also re ﬂected in the training results.\n4.5.3 Between FOTCA model and the model\nlacking data augmentation\nFrom the iteration graph, it can be observed that the model without\ndata augmentation behaves extremely similarly to the original model\nduring the process of iteration (the accuracy of the original model is\n99.8%, and the accuracy of the comparative model is 99.7%). Even in\nthe early stages of training, it produces better results than the original\nmodel, possibly due to the large amount of pest and disease data in this\nstudy, which allows for a suf ﬁcient sample size to collect ﬁne-grained\nfeatures without the need for data augmentation. However, continuous\ndata augmentation during the initial stages of model training causes the\nmodel to learn orientation features while learning the original features\nof the photos, increasing the computational load of the model,\nresulting in a less effective performance on the test set compared to\nthe model without data augmentation.\nFor the model using the original Pap Embedding, its learning\nability is constrained for the same linkages, which adds limitations\nto the model architecture. As for the model using MLP-Classi ﬁer, its\np e r f o r m a n c ec h a n g ei st h el a r g e s ta m o n gt h ef o u rm o d e l s ,\nindicating signi ﬁcant contributions of the classi ﬁer used in this\npaper, which can signi ﬁcantly improve the ﬁtting effect of the\nmodel. At the same time, the adjustment time of accuracy (loss)\nfor the four experiments is roughly the same, indicating that making\nminor changes to the model architecture under the same model\ndoes not have a signi ﬁcant effect on convergence ef ﬁciency. The\nFOTCA model is still a fast-converging model that is leading in\nplant leaf disease image recognition.\n5 Conclusions\nBased on the above experimental results, the Transformer-based\nmodels outperforms the CNNs in the ﬁeld of plant leaf disease image\nrecognition, mainly because it allows a more detailed and accurate\nfocus on image features., and can surpass the CNNs in terms of\naccuracy, loss, model matching speed, convergence ef ﬁciency, etc.\nThe FOTCA model proposed in this paper can further improve its\nfeature observation and extraction ability, and has a tendency to\nimprove in accuracy, loss and F1-score, while demonstrating the\nenormous potential and application of adaptive Fourier operators.\nWe will continue to extend our model in the future to combine\ndiverse approaches for plant crop disease identi ﬁcation and detection\nin complex contexts in complex future contexts.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material. Further inquiries can be\ndirected to the corresponding authors.\nAuthor contributions\nBH contributed to the conceptualization and design of the study\nand constructed improved experiments. WJ and JZ organized data\norganization and analysis validation. CC and LH wrote some\nmanuscripts. All authors contributed to manuscript revision, read,\nand approved the submitted version.\nBA\nFIGURE 6\nShowing training accuracy and loss comparison between the proposed FOTCA model and ablation experiment models with different parts removed.\n(A) Training accuracy curve for all models.(B) Training loss curve for all models.\nTABLE 3 Performance metrics of the proposed FOTCA model and ablation study models with different parts removed.\nModel Accuracy(%) Adjustment time F1-score(%) loss Adjustment time\nFOTCA 99.8 12 99.31 0.005 9\nUsing the original Pap Embedding 99.1 12 96.31 0.029 11\nUsing MLP-Classi ﬁer 96.1 12 85.91 0.0341 15\nMissing data augmentation 99.7 11 98.94 0.019 12\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org11\nFunding\nThis article is supported by National Natural Science\nFoundation of China (Grant/Award Number: “81460329”)a n d\nthe Natural Science Foundation of Jiangxi Province (Grant/Award\nNumbers: “20192ACBL20039/202310454/202310456”).\nAcknowledgments\nWe would like to express our sincere gratitude to all the\nindividuals who supported me throughout my research project.\nTheir encouragement, moral support, and companionship were\ninvaluable and made this work possible.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated organizations,\nor those of the publisher, the editors and the reviewers. Any product\nthat may be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nB e r g ,T . ,L i u ,J .K . ,W o oL e e ,S .A . ,A l e x a n d e r ,M .L .M . ,a n dJ a c o b s ,D .W .( 2 0 1 4 ) .“Birdsnap:\nLargescale ﬁne-grained visual categorization of birds, ” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) (IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR)),I E E E ,C o l u m b u s ,O H ,U S A .2 0 1 1– 2018.\nBisen, D. (2021). Deep convolutional neural network based plant species recognition\nthrough features of leaf. Multimed. Tools Appl.80, 6443 – 6456. doi: 10.1007/S11042-\n020-10038-W\nCai, C., Zhang, T., Weng, Z., and Feng, C. (2021). “A transformer architecture with\nadaptive attention for ﬁne-grained visual classi ﬁcation,” in International Conference on\nComputer and Communications(IEEE), Chengdu, China. 863 – 867.\nCarion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S.\n(2020). “). End-to-end object detection with transformers, ” in Proceedings of the\nEuropean Conference on Computer Vision, Springer, Glasgow, UK. 213 – 229, Springer.\nChen, Y., Bai, Y., Zhang, W., and Mei, T. (2019). “Destruction and construction\nlearning for ﬁne-grained image recognition, ” in 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) (IEEE), Long Beach, CA, USA.\n5152– 5161. doi: 10.1109/CVPR.2019.00530\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2020). “An image is worth 16x16 words: Transformers for image recognition at\nscale,” in Proceedings of the International Conference on Learning Representations.\nhttps://OpenReview.net, virtual, Formerly Addis Ababa ETHIOPIA\nFu, J., Zheng, H., and Mei, T. (2017). “Look closer to see better:recurrent attention\nc o n v o l u t i o n a ln eu r a ln e t w o r kf o rﬁne-grained image recognition,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition, IEEE, Hawaii, USA. 4438– 4446.\nGuibas, J., Mardani, M., Li, Z., Tao, A., Anandkumar, A., and Catanzaro, B. (2021).\nAdaptive fourier neural operators: Ef ﬁcient token mixers for transformers. arXiv\npreprint. arXiv:2111.13587. doi: 10.4505/arXiv.2111.13587\nHe, J., Chen, J.-N., and Liu, S. (2022). “Transfg: A transformer architecture for ﬁne-\ngrained recognition, ” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nAAAI, Pomona, California. 852 – 860.\nHinton, G. E., Srivastava, N., Krizhevsky, A., Krizhevsky, A., Sutskever, I., and\nSalakhutdinov, R. R. (2012). Improving neur al networks by preventing co-adaptation of\nfeature detectors.arXivpreprint arXiv:1207.0580. doi:10.9774/GLEAF.978-1-909493-38-42\nIqbal, Z., Sharif, M., Shah, J., ur Rehman, M., and Javed, K. (2018). An automated\ndetection and classi ﬁcation of citrus plant diseases using image processing techniques:\nA review. Comput. Electron. Agric.153, 12 – 32. doi: 10.1016/j.compag.2018.07.041\nKong, J., Wang, H., Wang, X., Jin, X., Fang, X., and Lin, S. (2021). Multi-stream\nhybrid architecture based on cross-level fusion strategy for ﬁnegrained crop species\nrecognition in precision agriculture. Comput. Electron. Agric.185, 106134. doi: 10.1016/\nj.compag.2021.106134\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classi ﬁcation with\ndeep convolutional neural networks. Commun. ACM60, 84 – 90. doi: 10.1145/3065386\nKumar, S., Gupta, S. K., Kaur, M., and Gupta, U. (2023a). Vi-net: A hybrid deep\nconvolutional neural network using vgg and inception v3 model for copy-move forgery\nclassiﬁcation. J. Vis. Comun. Image Represent.89, 103644. doi: 10.1016/j.jvcir.2022.103644\nKumar, S., Pal, S., Singh, V. P., and Jaiswal, P. (2023b). Performance evaluation of\nresnet model for classi ﬁcation of tomato plant disease. Epidemiologic Methods 12,\n20210044. doi: 10.1515/em-2021-0044\nLé cun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning\napplied to document recognition. Proc. IEEE 86, 2278 – 2324. doi: 10.1109/5.726791\nLee, S. H., Lee, S., and Song, B. C. (2021). Vision transformer for small-size datasets.\ndoi: 10.48550/arXiv.2112.13492\nLin, D., Shen, X., Lu, C., and Jia, J. (2015). “Deep lac: Deep localization, alignment\nand classi ﬁcation for ﬁne-grained recognition, ” in 2015 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) ,B o s t o n ,M A ,U S A :I E E E .1 6 6 6 – 1674.\ndoi: 10.1109/CVPR.2015.7298775\nOuppaphan, P. (2017). “Corn disease identi ﬁcation from leaf images using\nconvolutional neural networks, ” in Proceedings of the 2017 21st International\nComputer Science and Engineering Conference (ICSEC) (IEEE), Bangkok, Thailand.\n1– 5. doi: 10.1109/ICSEC.2017.8443919\nPan, H., Xie, L., and Wang, Z. (2022). Plant and animal species recognition based on\ndynamic vision transformer architecture. Remote Sens.14, 5242. doi: 10.3390/rs14205242\nSanju, S., and VelammaL, D. (2021). An automated detection and classi ﬁcation of\nplant diseases from the leaves using image processing and machine learning techniques:\nA state-of-the-art review. Ann. Rom. Soc Cell Biol.25, 15933 – 15950.\nSun, G. (2019). Fine-grained recognition: Accounting for subtle differences between\nsimilar classes. Proceedings of the AAAI conference on artiﬁcial intelligence. (New York,\nUSA: AAAI) 34 (07), 12047 – 12054. doi: 10.1609/aaai.v34i07.6882\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in In Proceedings of the 31st International\nConference on Neural Information Processing Systems (NIPS’17) (Curran Associates\nInc.), Long Beach California USA. 6000 – 6010.\nWang, Y., Huang, R., Song, S., Huang, Z., and Huang, G. (2021). “Not all images are\nworth 16x16 words: Dynamic transformers for ef ﬁcient image recognition, ” in\nProceedings of the Neural Information Processing Systems. NeurIPS, virtual\nWang, D., Wang, J., Li, W., and Guan, P. (2021). T-cnn: Trilinear convolutional\nneural networks model for visual detection of plant diseases. Comput. Electron. Agric.\n190, 106468. doi: 10.1016/j.compag.2021.106468\nWei, M., Wu, Q., Ji, H., Wang, J., Lyu, T., Liu, J., et al. (2023). A skin disease\nclassiﬁcation model based on densenet and convnext fusion. Electronics 12, 438.\ndoi: 10.3390/electronics12020438\nY a n g ,Z . ,L u o ,T . ,W a n g ,D . ,H u ,Z . ,G a o ,J . ,a n dW a n g ,L .( 2 0 1 8 ) .\n“Learning to navigate for ﬁne-grained classi ﬁcation,” in Computer vision – ECCV\n2018, vol. 11218 . Ed. V. Ferrari (Munich, Germany: Springer). H. M. S. C. W. Y.\ndoi: 10.1007/978-3-030-01264-9\\26\nZhang, N., Donahue, J., Girshick, R. B., and Darrell, T. (2014). Part-based R-CNNs\nfor ﬁne-grained category detection. Computer Vision–ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13 .\n(Zurich, Switzerland: Springer), 834 – 849. doi: 10.1007/978-3-319-10590-1_54\nZhang, S., and Wang, Z. (2016). Cucumber disease recognition based on global-local\nsingular value decomposition. Neurocomputing 205, 341 – 348. doi:\n10.1016/\nj.neucom.2016.04.034\nZhang, K., Zhang, Z., Li, Z., and Qiao, Y. (2016). Joint face detection and alignment\nusing multitask cascaded convolutional networks. IEEE Signal Process. Lett.23, 1499 –\n1503. doi: 10.1109/LSP.2016.2603342\nHu et al. 10.3389/fpls.2023.1231903\nFrontiers inPlant Science frontiersin.org12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7289845943450928
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6671230792999268
    },
    {
      "name": "Artificial intelligence",
      "score": 0.647144079208374
    },
    {
      "name": "Interpretability",
      "score": 0.5919944047927856
    },
    {
      "name": "Transformer",
      "score": 0.5391597747802734
    },
    {
      "name": "Machine learning",
      "score": 0.47626134753227234
    },
    {
      "name": "Architecture",
      "score": 0.45040661096572876
    },
    {
      "name": "Deep learning",
      "score": 0.4403848946094513
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43553856015205383
    },
    {
      "name": "Engineering",
      "score": 0.09662660956382751
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141649914",
      "name": "Nanchang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210164024",
      "name": "First Affiliated Hospital of Nanchang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210108480",
      "name": "Second Affiliated Hospital of Nanchang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    }
  ]
}