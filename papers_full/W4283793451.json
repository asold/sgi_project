{
  "title": "Chess as a Testbed for Language Model State Tracking",
  "url": "https://openalex.org/W4283793451",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2110417275",
      "name": "Shubham Toshniwal",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2120705023",
      "name": "Sam Wiseman",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A320276025",
      "name": "Karen Livescu",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2000806699",
      "name": "Kevin Gimpel",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2110417275",
      "name": "Shubham Toshniwal",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A320276025",
      "name": "Karen Livescu",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2000806699",
      "name": "Kevin Gimpel",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2529194139",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W2810346659",
    "https://openalex.org/W6891798393",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965647350",
    "https://openalex.org/W2516090925",
    "https://openalex.org/W6739847781",
    "https://openalex.org/W6632455782",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W6745610606",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W6631834165",
    "https://openalex.org/W6745245109",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W6739518724",
    "https://openalex.org/W2473344385",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3015449890",
    "https://openalex.org/W2902907165",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2692059227",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4297798492",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4301785137",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W4213009331",
    "https://openalex.org/W3098637454",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2766569996",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W1525961042",
    "https://openalex.org/W3047988254",
    "https://openalex.org/W2763421725"
  ],
  "abstract": "Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. “full attention”. Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.",
  "full_text": "Chess as a Testbed for Language Model State Tracking\nShubham Toshniwal1, Sam Wiseman2, Karen Livescu1, Kevin Gimpel1\n1Toyota Technological Institute at Chicago\n2Duke University\nfshtoshni, klivescu, kgimpelg@ttic.edu, swiseman@cs.duke.edu\nAbstract\nTransformer language models have made tremendous strides\nin natural language understanding tasks. However, the com-\nplexity of natural language makes it challenging to ascertain\nhow accurately these models are tracking the world state un-\nderlying the text. Motivated by this issue, we consider the task\nof language modeling for the game of chess. Unlike natural\nlanguage, chess notations describe a simple, constrained, and\ndeterministic domain. Moreover, we observe that the appro-\npriate choice of chess notation allows for directly probing the\nworld state, without requiring any additional probing-related\nmachinery. We ﬁnd that: (a) With enough training data, trans-\nformer language models can learn to track pieces and pre-\ndict legal moves with high accuracy when trained solely on\nmove sequences. (b) For small training sets providing access\nto board state information during training can yield signiﬁ-\ncant improvements. (c) The success of transformer language\nmodels is dependent on access to the entire game history i.e.\n“full attention”. Approximating this full attention results in a\nsigniﬁcant performance drop. We propose this testbed as a\nbenchmark for future work on the development and analysis\nof transformer language models.\n1 Introduction\nRecently, transformer-based language models have stretched\nnotions of what is possible with the simple self-supervised\nobjective of language modeling, becoming a ﬁxture in state\nof the art language technologies (Vaswani et al. 2017; De-\nvlin et al. 2019; Brown et al. 2020). However, the black\nbox nature of these models combined with the complexity\nof natural language makes it challenging to measure how\naccurately they represent the world state underlying the text.\nIn order to better measure the extent to which these models\ncan capture the world state underlying the symbolic data they\nconsume, we propose training and studying transformer lan-\nguage models for the game of chess. Chess provides a sim-\nple, constrained, and deterministic domain where the exact\nworld state is known. Chess games can also be transcribed\nexactly and unambiguously using chess notations (Section 2).\nMost importantly, the form of chess notations allows us to\nprobe our language models for aspects of the board state us-\ning simple prompts (Section 3) and without changing the\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nlanguage modeling objective or introducing any new classi-\nﬁers.1\nDue to the simplicity and precision of chess, we can eval-\nuate language model predictions at a more ﬁne-grained level\nthan merely comparing them to the ground truth. For ex-\nample, even if the next move prediction doesn’t match the\nground truth move, we can still evaluate whether the move\nis legal given the board state, and if it is illegal, the error can\nbe automatically analyzed. Moreover, since world state tran-\nsitions are deterministic and known, we can evaluate models\nusing counterfactual queries as well. Our proposed evalua-\ntion sets and metrics are described in Section 3.2.\nWhile chess represents a controlled domain, it is by no\nmeans trivial for a language model. To illustrate the chal-\nlenges of language modeling for chess, consider the left\nboard shown in Figure 1b, where white is next to move. In or-\nder to generate a valid next move, the language model needs\nto (a) infer that it is white’s turn, (b) represent the locations\nof all pieces, both white and black, (c) select one of the white\npieces which can be legally moved, and ﬁnally (d) make a\nlegal move with the selected piece. Thus, a language model\nhas to learn to track the board state, learn to generate moves\naccording to the rules of chess, and on top of that learn chess\nstrategies to predict the actual move.\nWe ﬁnd that when given enough training data, transform-\ners can learn to both track piece locations and predict legal\nmoves with high accuracy. However, when trained on small\ntraining sets, predictive ability suffers. In this more challeng-\ning setting, introducing parts of the board state as tokens in\nthe training sequences (Section 3.1) improves piece tracking\nsigniﬁcantly.\nOur results also provide some key insights on transformer\nlanguage models: (i) They are robust to changes in input dis-\ntribution where additional tokens, related to board state, are\nadded to input sequence only during training (Section 3.1).\nIn contrast to LSTMs, transformers achieve this robustness\neven with smaller training sets (Section 5.3). (ii) Even\nthough chess is Markovian, the model relies on having ac-\ncess to the whole history, and the performance drops when\nlimiting this access (Section 5.3).\n1Code and data available at - https://github.com/shtoshni/\nlearning-chess-blindfolded\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11385\na b c d e f g h\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\na b c d e f g h\na1\na2\na3\na4\nb1\nb2\nb3\nb4\nc1\nc2\nc3\nc4\nd1\nd2\nd3\nd4\ng5\n(a) Square naming\n (b) Board state before (left) and after (right) the bishop at f1 is moved to b5. UCI\nnotation represents the move as f1b5.\nFigure 1: Chess Notation\nTo summarize, our contributions are to:\n• Propose chess as a testbed for evaluating world state track-\ning capabilities of language models which can be used for\ndevelopment and analysis of these models.\n• Show that with the appropriate chess notation, we can\nprobe language models for aspects of the world state using\nsimple prompts (Section 3).\n• Show that given enough training data, transformer lan-\nguage models can learn to track piece locations and pre-\ndict legal moves with high accuracy.\n• Demonstrate that transformer language models are robust\nto certain changes in input distribution, and that access\nto world state during training improves performance with\nsmall datasets.\n2 Chess Preliminaries\nWe represent moves using Universal Chess Interface (UCI)\nnotation, which combines the starting square and the destina-\ntion square to represent a move.2 The move in Figure 1b is\nrepresented as f1b5 in UCI where f1 indicates the starting\nsquare and b5 denotes the ending square. While the SAN\nnotation is the standard choice for gameplay, we prefer UCI\nas it allows for pieces to be referenced unambiguously via\ntheir starting square, something we later exploit in designing\nprompt-based probes in Section 3.\nFor training language models, we ﬁrst tokenize games rep-\nresented in UCI notation using a simple regular expression\nbased tokenizer, which considers a board square symbol such\nas b1 as a single token. This gives us a vocabulary of 77\ntoken types, which includes the 64 squares, piece type sym-\nbols, and other special symbols (see Table 1).3 For example,\nthe move sequence “ e2e4 e7e5 g1f3” is tokenized to\n2For more details see https://en.wikipedia.org/wiki/Universal\nChess Interface\n3In initial experiments we used a delimiter token to indicate\nmove boundary. However, removing it did not degrade performance\nand made training faster due to reduced sequence length.\nType Examples Count\nSquare names e4, d1 64\nPiece type P, K, Q, R, B, N 6\nPromoted Pawn Piece type q, r, b, n 4\nSpecial symbols BOS, EOS, PAD 3\nTotal 77\nTable 1: Model V ocabulary\n“e2, e4, e7, e5, g1, f3”. We then train an autoregressive\nlanguage model on these move sequences, using the standard\nmaximum likelihood objective.\n3 Language Model Prompts as Board State\nProbes\nOne attractive property of having a language model\ntrained on chess games represented in UCI notation (as\ndescribed in the previous section) is that the notation\nitself allows us to probe the trained model’s state tracking\nabilities. In particular, by feeding the trained language\nmodel a preﬁx of a game as a prompt, we can determine\n— using the language model’s next-token predictions —\nwhat the model understands about the board state im-\nplied by this preﬁx. For example, consider the prompt\n“\ne2e4 e7e5 g1f3 b8c6 d2d4 h7h6 f1,” where\nthe underlined move sequence leads to the left board state in\nFigure 1b. A language model’s next-token prediction (after\nconsuming the prompt) can be interpreted as the ending\nsquare predicted for the bishop at\nf1, which can be used to\ndetermine the level of board state awareness of the model.\nIf, for instance, the model predicts g1, this may indicate\nthat the model does not recognize that the piece type at f1\nis a bishop, as such a move is not possible for a bishop. If,\non the other hand, the model predicts g2, that may indicate\nthat the model is not aware that another piece is currently at\ng2.\n11386\nNotation Training Inference\nUCI e2, e4, e7, e5, g1, f3 e2, e4, e7, e5, g1, f3\nUCI + RAP 15 e2, e4, P, e7, e5, g1, f3 e2, e4, e7, e5, g1, f3\nUCI + RAP 100 P, e2, e4, P, e7, e5, N, g1, f3 e2, e4, e7, e5, g1, f3\nUCI + AP P, e2, e4, P, e7, e5, N, g1, f3 P, e2, e4, P, e7, e5, N, g1, f3\nTable 2: Token sequences corresponding to the move sequencee2e4 e7e5 g1f3 for different notations during training and\ninference. Notice that regardless of the RAP probability used during training, at inference time the token sequences have no\npiece types.\n3.1 Randomly Annotated Piece type (RAP)\nWhile predicting the token representing the ending-square of\na move given a prompt allows us to assess the model’s state\ntracking abilities, it also to some extent conﬂates the model’s\nunderstanding of the board state with its understanding of\nchess strategy. If we could easily probe for where the model\nthinks a piece currently is (rather than where it is likely to\nend up) given a game preﬁx, this would allow us to more\ndirectly probe the model’s state tracking abilities. In partic-\nular, we would like to give a language model a prompt such\nas “e2e4 e7e5 g1f3 b8c6 d2d4 h7h6 N”, where\nN represents knight, and expect it to generate a valid starting\nposition for a knight of the correct color. While UCI nota-\ntion does not ordinarily include these piece type tokens, to\nallow for testing the model with such prompts, we propose\nto randomly include these piece types tokens in moves dur-\ning training with some ﬁxed probability\np. We refer to this\nstrategy as “randomly annotated piece type” (RAP) and use\nthe nomenclature “UCI + RAP p” to indicate that with p%\nprobability, piece type is part of the move notation during\ntraining. Note that for p = 0, the notation reduces to UCI.\nWhen testing with these starting square prediction\nprompts, we only include piece type for the prompt, not for\nany moves in the history. Thus, using RAP during training\nallows us to probe, at test time, where the model thinks each\npiece is, given any game history’s preﬁx; by simply provid-\ning the desired piece type (e.g.,\nN) the model outputs the\npredicted starting square for a piece of that type. For exam-\nple, given the prompt “\ne2e4 e7e5 g1f3 b8c6 d2d4\nh7h6 N”, a prediction of f3 or b1 shows that the model is\naware of where the knights are.\nWe also experiment with an “oracle” variant of RAP\nwhere piece types are added both during training and testing.\nWe refer to this notation as “UCI + AP ” where AP stands for\n“always piece type”. For our running example the equivalent\nprompt in this notation would be “Pe2e4 Pe7e5 Ng1f3\nNb8c6 Pd2d4 Ph7h6 N”.\nIn terms of the language modeling training objective, addi-\ntion of RAP represents a distribution change between train-\ning and inference. Table 2 illustrates how the use of RAP\nchanges the token sequence during training but not during\ninference. While there’s a distribution mismatch, we hy-\npothesize that addition of RAP can aid the model in learn-\ning to track the pieces by providing additional supervision\nwhich, in turn, can improve language modeling performance\nas well.\n3.2 Board State Probing Tasks\nIn this subsection we describe the probing tasks introduced\nabove more concretely. In each probing task we feed the\nmodel a preﬁx of a game followed by a single prompt token,\nand the model is evaluated based on the highest probability\nnext-token under the model given this context. We show an\nexample of each probing task in Table 3 (which we further\ndescribe below), assuming the model has been fed the move\nsequence preﬁx e2e4 e7e5 g1f3 b8c6 d2d4 h7h6,\nwhich is visualized as the left board in Figure 1b. The ac-\ntual next move played in the game is f1b5, which takes the\nwhite bishop at squaref1 to square b5, as shown in the right\nboard of Figure 1b.\n3.3 Ending Square Tasks\nIn this set of tasks, the model is given a game preﬁx and\nprompted with the starting square of the next move ( f1 in\nthe example of Table 3). The model’s next-token prediction\nrepresents its prediction for the ending square of this move,\nwhich tests the model’s ability to track the board state and\nfollow the rules of chess, as well as strategic awareness.4 We\nconsider two task variants:\n1. End-Actual: Given a move sequence preﬁx, the model\nis prompted with the starting square of the actual piece\nmoved next in the game.\n2. End-Other: Given a move sequence preﬁx, the model\nis prompted with the starting square of any piece on the\nboard that can be legally moved according to the rules of\nchess.\nWe evaluate End-Actual predictions in terms of both exact\nmove (ExM) accuracy (whether the model predicted the true\nending square, b5 in our running example) and legal move\n(LgM) accuracy (whether the model predicted a legal ending\nsquare for the piece starting at the square in the prompt). For\nLgM evaluation, we also calculate the R-Precision which is\nthe Precision@R where R is the total number of legal end-\ning squares (Manning, Raghavan, and Sch¨utze 2008). In our\nrunning example, there are 5 legal ending squares, and R-\nPrecision will be calculated for the model’s top-5 predictions.\nExM accuracy evaluation is similar to the typical evaluation\nof language models on natural language data, while LgM is\nless stringent and focuses on testing just the model’s under-\nstanding of chess rules and the board state. Note that for\n4Strategic capabilities of a chess language model are strongly\ntied to the quality of training games.\n11387\nTask Prompt Token Correct Answers (ExM) Correct Answers (LgM)\nEnd-Actual f1 fb5g fe2, d3, c4, b5 ,a6 g\nEnd-Other f3 N/A fd2, g1, h4, g5, e5g\nStart-Actual B ff1g ff1, c1g\nStart-Other N N/A ff3, b1g\nTable 3: Examples of each probing task, as well as the corresponding exact move (ExM) and legal move (LgM) correct answers,\nare shown below. All examples assume the language model was fed the preﬁx e2e4 e7e5 g1f3 b8c6 d2d4 h7h6 (see\nFigure 1b), and that the actual next move was f1b5. While there is only one valid prompt token for both End-Actual and\nStart-Actual tasks, there are many valid prompt tokens for the other tasks, and we show just one possibility for each. Start-tasks\n(bottom sub-table) assume the model was trained on games described in UCI+RAP notation.\nEnd-Other, only LgM evaluation is available. See Table 3\nfor examples.\n3.4 Starting Square Tasks\nIn this category of task, the model is again given a game pre-\nﬁx, but prompted with just the piece type of the next move,\nsuch as B for bishop in the example in Table 3. The model’s\nnext-token prediction thus represents its prediction for where\nthe prompted piece type currently is on the board. This task\ntests the model’s ability to track pieces.5 Note that only mod-\nels which have seen piece types during training, i.e. “UCI +\nRAP” models, can actually be tested on this task. Also, no\npiece types are used in the game preﬁx. We again have two\nvariants of this task:\n1. Start-Actual: Given a move sequence preﬁx, the model\nis prompted with the piece type of the actual piece moved\nnext in the game.\n2. Start-Other: Given a move sequence preﬁx, the model\nis prompted with the piece type of any piece on the board\nthat can be legally moved according to the rules of chess.\nWe again evaluate Start-Actual both in terms of ExM accu-\nracy (whether the model predicts the starting square of the\npiece actually moved next in the game), as well as in terms\nof LgM accuracy (whether the model predicts the starting\nsquare of a legally movable piece of the given piece type) and\nLgM R-Precision (precision of the model’s top-R predictions\nwith respect to all of the R starting squares of legally mov-\nable pieces of the given piece type). For Start-Other, only\nLgM evaluation is applicable; see Table 3 for examples.\n4 Experimental Setup\nData We use the Millionbase dataset which is freely avail-\nable and has close to 2.9 million quality chess games. 6 Af-\nter ﬁltering out duplicate games, games with fewer than 10\nmoves, and games with more than 150 moves (for the com-\nplete game to ﬁt into one transformer window), we are left\nwith around 2.5 million games. From this ﬁltered set we ran-\ndomly select 200K games for training, 15K games each for\ndev and test, and another 50K games to create board state\n5In certain cases, this task also tests understanding of chess\nrules. For example, in Figure 1b only the rook at h1 can be moved.\n6Download link available at https://rebel13.nl/rebel13/rebel n\n%2013.html\nprobing evaluation sets described in Section 3.2. The dev\nand test sets are used for perplexity evaluations. The dev set\nperplexity is used for choosing hyperparameters. From the\n200K training set, we create subsets of size 15K and 50K\nwhich we refer to as “Train-S” and “Train-M”, while the full\ntraining set is referred to as “Train-L”. All the data process-\ning steps requiring chess knowledge, including parsing chess\ndatabases, are carried out using python-chess (Fiekas 2012).\nTo create the board state probing evaluation sets, we use\nthe 50K games reserved for this task. We only consider\nprompts for non-pawn pieces since the dynamics of pawns\nare fairly limited. We ensure that the game preﬁxes selected\nare never seen in the training data. The ﬁnal evaluation set\nconsists of 1000 instances with preﬁx length (in number of\nmoves) in the range 51 \u0014l \u0014100.\nModel Details We use the GPT2-small architecture for our\nbase language model (Vaswani et al. 2017; Radford et al.\n2019). GPT2-small is a 12-layer transformer model with 12\nattention heads and an embedding size of 768 dimensions.\nThe context size of the model is limited to 512, which is\nsufﬁcient to cover the longest game in our training set. Note\nthat we only borrow the model architecture; the models them-\nselves are trained from scratch. 7\nFor the UCI + RAP p models, we tune over p 2\nf5; 15; 25; 50; 75; 100gbased on perplexity on the validation\nset. Note that for perplexity evaluation, logits corresponding\nto piece type tokens are masked out since piece type tokens\nare only available during training. We ﬁnd that p = 25 per-\nforms the best for Train-S and Train-M, whilep = 15is best\nfor Train-L (Figure 2). Larger values ofp lead to greater mis-\nmatch between training and inference, while smaller values\nlikely do not provide enough training signal.\nWe also experiment with other transformer and non-\ntransformer models in Section 5.3. Among the transformer\nmodels, we experiment with two “approximate” attention\nmodels (i.e., models which approximate the full attention\nof vanilla transformer models), namely, Reformer (Kitaev,\nKaiser, and Levskaya 2020) and Performer (Choromanski\net al. 2021). We set the number of layers and attention heads\nto 12 for both architectures, as in GPT2-small. We also train\nLSTM language models with and without RAP.\n7Colab notebook to play chess against the base lan-\nguage model https://github.com/shtoshni/learning-chess-\nblindfolded/blob/master/GPT2 Chess Model.ipynb\n11388\nTraining Set Model Dev set Test set\nTrain-S UCI 23.6 23.6\nUCI + RAP 15.9 15.9\nUCI + AP 16.1 16.2\nTrain-M UCI 11.6 11.6\nUCI + RAP 10.4 10.4\nUCI + AP 10.1 10.0\nTrain-L UCI 7.7 7.7\nUCI + RAP 7.4 7.4\nUCI + AP 7.2 7.2\nTable 4: Canonical validation and test set perplexity. By\ncanonical we mean that one move, say f1b5, counts as one\ntoken.\nFigure 2: Validation set perplexities as a function of RAP\nprobabilities for the different training set sizes. RAP 0 is the\nstandard UCI notation. RAP 100 is not shown as perplexities\nare too high.\nTraining Details Models are trained for 10 epochs with a\nbatch size of 60. Validation is performed at the end of every\nepoch and training stops whenever the validation loss starts\nincreasing. For optimization we use Adam (Kingma and Ba\n2014) with learning rate of 5 \u000210−4 and L2 weight decay of\n0:01. The learning rate is warmed up linearly over the ﬁrst\n10% of training followed by a linear decay. To accelerate\ntraining, we use mixed precision training (Micikevicius et al.\n2018). All experiments are carried out using the PyTorch\nLightning framework built on top of PyTorch (Falcon et al.\n2019; Paszke et al. 2019). We use the transformers library\n(Wolf et al. 2019) for all models 8 except for the Performer\nmodel for which we use a popular unofﬁcial implementation.\n9\n5 Results\nWe ﬁrst present language modeling results, where we show\nsigniﬁcant improvements with the addition of RAP (Sec-\ntion 5.1). Next, we show results on the board state probing\ntasks for the base language model, where we demonstrate\nthat the model trained on the large training set can learn to\ntrack pieces and predict legal moves with high accuracy (Sec-\ntion 5.2). Finally, we present results on the probing task with\napproximate attention transformer architectures and LSTMs,\nwhere we show a performance drop in comparison to the\nbase model with full attention (Section 5.3).\n5.1 Language Modeling\nTable 4 presents the perplexity results on the validation and\ntest sets. Figure 2 plots the validation set perplexities as a\nfunction of RAP probability for different training set sizes.\nThe addition of RAP and AP leads to a decrease in perplexity\nfor all training sizes, particularly for small training sets. For\nsmall training sets, RAP probabilities as high as 50% can\nimprove the validation perplexity, but for larger training sets,\n8Reformer implementation in transformers library is still a\nwork in progress. The presented results are with the 4.2.2 version.\n9https://github.com/lucidrains/performer-pytorch\nlower RAP probabilities are preferred. The reductions in\nperplexity for RAP are surprising given that the extra tokens\nadded via RAP are not present in the validation and test sets,\nand thus there is a data distribution shift. Models trained with\nUCI + AP achieve the lowest perplexities on larger training\nsets. Both RAP and AP aid the model in piece tracking,\nas we will see in later results, and in the case of chess this\ncan signiﬁcantly improve the language modeling results as\nwell. Note that for calculating the perplexity of UCI + RAP\nmodels, we mask out the logits corresponding to piece type\ntokens since they are never present during inference.\n5.2 Board State Tracking\nTables 5 and 6 show results when predicting starting squares\nand ending squares, respectively. There are several obser-\nvations to note. First, transformers can learn to identify\nwhere pieces are located. This is shown by the LgM ac-\ncuracies in Table 5. UCI + RAP can predict legal starting\npositions with perfect accuracy and R-Precision. However,\nthis capability requires Train-L, and the accuracy drops to\n91.3% for Train-S. The gap between UCI + RAP and its “or-\nacle” counterpart, UCI + AP, also reduces with an increase\nin training set size with UCI + RAP achieving parity for\nTrain-L. When asked to identify the location of a piece other\nthan the one selected to be moved next, this accuracy drops\nonly slightly to 99.6%. Typically, the piece location track-\ning is slightly better for the piece type that is actually moved\nthan for other piece types.\nThe difference between the location of the piece in the\nexact move (ExM) and the location of either piece of the\ngiven type (LgM) is substantial, at more than 8% absolute.\nHowever, this difference relates to chess strategy rather than\nboard state tracking.\nSecond, transformers can learn to predict legal moves.\nThis is shown by the LgM accuracies in Table 6, for which\nboth UCI and UCI + RAP exceed 97% accuracy. However,\nwhile the top predictions of the models have high accuracy,\ntheir ability to predict all legal moves is signiﬁcantly lower,\nwith R-precision of about 85%. This is to be expected, since\n11389\nNotation LgM ExM\nActual Other\nAcc. R-Prec. Acc. R-Prec. Acc.\nS UCI + RAP 91.3 90.2 89.3 89.2 78.8\nUCI + AP 99.2 99.1 98.8 98.8 86.9\nM UCI + RAP 98.2 98.0 98.6 98.7 88.0\nUCI + AP 99.9 99.8 100.0 100.0 90.2\nL UCI + RAP 100.0 100.0 99.6 99.5 91.8\nUCI + AP 99.9 99.9 99.7 99.7 91.1\nRandom Legal - - - - 86.0\nTable 5: Accuracies and R-Precisions (%) for predicting\nstarting squares (“Start-Actual” and “Start-Other” tasks). S,\nM, L in the ﬁrst column refer to the training set sizes.\nNotation LgM ExM\nActual Other\nAcc. R-Prec. Acc. R-Prec. Acc.\nS\nUCI 74.0 61.1 65.5 57.7 26.7\nUCI + RAP 88.4 75.5 80.4 72.1 33.3\nUCI + AP 87.0 77.0 78.8 72.3 36.1\nM\nUCI 92.9 80.6 85.8 78.5 42.2\nUCI + RAP 94.9 82.2 87.9 78.0 45.9\nUCI + AP 94.7 82.4 88.3 79.1 47.3\nL\nUCI 97.7 85.6 91.9 83.8 52.0\nUCI + RAP 97.0 86.1 93.1 83.9 54.7\nUCI + AP 98.2 87.3 95.2 86.3 56.7\nRandom Legal - - - - 19.6\nTable 6: Accuracies and R-Precisions (%) for predicting end-\ning squares (“End-Actual” and “End-Other” tasks). S, M, L\nin the ﬁrst column refer to the training set sizes.\nthe model is trained on only actual games, where the em-\nphasis is on “meaningful” moves rather than any legal move.\nDue to similar reasons, there’s a signiﬁcant drop in perfor-\nmance when predicting ending squares for starting squares\nother than the one in the actual game. The “other” starting\nsquare would, by design, have legal continuations, but lack\nany “meaningful” ones.\nWe ﬁnd consistent gains in almost all metrics with the\naddition of RAP during training, with the gains being par-\nticularly impressive for small training sets. Thus, not only\nare the transformers robust to distribution shift due to RAP\n(available only during training), they are in fact able to utilize\nthis additional information. Error analysis of illegal predic-\ntions shows that the addition of RAP improves piece tracking\nrelated errors.\nThe relatively low ExM accuracies of the models can be\nattributed to the inherent difﬁculty of the task. Randomly\nselecting an ending square from all legal ending squares has\nan accuracy of only around 20%, implying that on average\nthere are roughly 5 legal choices, which might explain the\nModel LgM ExM\nActual Other\nAcc. R-Prec. Acc. R-Prec. Acc.\nS\nGPT2 74.0 61.1 65.5 57.7 26.7\nGPT2 (w = 50) 69.5 57.4 60.4 53.2 23.1\nReformer 71.0 57.2 61.5 53.5 24.8\nPerformer 65.4 54.3 57.9 49.5 20.5\nLSTM 60.2 51.0 52.5 46.4 20.9\nLSTM + RAP 59.5 50.5 52.4 46.0 21.9\nM\nGPT2 92.9 80.6 85.8 78.5 42.2\nGPT2 (w = 50) 86.0 74.9 80.9 71.3 35.8\nReformer 86.4 73.2 76.6 68.6 32.4\nPerformer 89.2 76.3 80.5 71.5 36.0\nLSTM 73.8 61.6 67.2 59.8 32.0\nLSTM + RAP 77.5 64.9 69.7 61.7 32.1\nL\nGPT2 97.7 85.6 91.9 83.8 52.0\nGPT2 (w = 50) 95.8 84.5 90.5 82.7 51.6\nReformer 88.0 74.9 77.0 68.1 33.5\nPerformer 95.8 84.5 90.5 82.7 51.6\nLSTM 93.4 79.5 86.1 76.0 45.2\nLSTM + RAP 92.8 80.4 87.3 77.1 46.0\nTable 7: Accuracy and R-Precision (%) for predicting end-\ning squares (“End-Actual” and “End-Other” tasks) with vary-\ning attention window sizes. LSTM + RAP refers to LSTM\ntrained with UCI + RAP.\ndifﬁculty of the task.\n5.3 Compressing the Game History\nThe base transformer language model, based on GPT2, at-\ntends to the entire history (i.e., it uses “full attention”), which\nresults in complexity quadratic in the length of the sequence.\nWe might wonder whether attending to this entire history\nis necessary for the impressive state tracking performance\nobserved in the previous section. We accordingly explore\nmodels that do not attend to the entire history in Table 7.\nWe ﬁrst experiment with a variant of the GPT2 model that\nlimits its attention to a window of only the 50 most recent\ntokens (“GPT2\n(w = 50)”). In Table 7 we see worse per-\nformance for this model across data sizes, but especially for\nsmall- and medium-sized datasets.\nIn Table 7 we also consider a language model based on the\nLSTM (Hochreiter and Schmidhuber 1997), which considers\nonly its current hidden state and cell state in making its pre-\ndictions, and does not explicitly attend to the history. Here\nwe ﬁnd an even more signiﬁcant drop in performance, in\nall settings. (Interestingly, we also ﬁnd that training LSTM\nlanguage models on sequences with RAP improves perfor-\nmance, but only for larger training sets; transformer language\nmodels generally improve when trained with RAP data).\nThe results of GPT2 (w = 50)and of the LSTM language\nmodel suggest that attending to the full game history is, un-\nsurprisingly, useful for board state tracking in chess. This\nﬁnding further suggests that the task of board state track-\ning in chess can serve as an excellent testbed for recently\n11390\nproposed transformer variants (Kitaev, Kaiser, and Levskaya\n2020; Katharopoulos et al. 2020; Choromanski et al. 2021,\ninter alia) that attempt to make use of long histories or con-\ntexts, but without incurring a quadratic runtime.\nApproximate Attention Transformers We experiment\nwith the recently proposed Reformer (Kitaev, Kaiser, and\nLevskaya 2020) and Performer (Choromanski et al. 2021)\narchitectures. Reformer replaces the “full attention” with at-\ntention based on locality-sensitive hashing, while Performer\napproximates the “full attention” with random features.10\nThe results, in Table 7, suggest that the Performer gener-\nally outperforms the Reformer, except in the small dataset-\nsetting. Furthermore, we ﬁnd that neither of these architec-\ntures signiﬁcantly outperforms the GPT2 (w = 50)baseline,\nexcept for Performer in the medium-sized data setting. These\nmodels do, however, typically outperform the LSTM models.\nThese results demonstrate the challenge of modeling chess\nwith an approximate attention. We hope that future work\nwill use this task as a way of benchmarking more efﬁcient\ntransformer architectures.\n6 Related Work\nSimulated Worlds. There have been several prior efforts\nin relating simulated worlds to natural language. The bAbI\nframework simulates a world modeled via templates to gen-\nerate question answering tasks (Weston et al. 2015). The\nrecent TextWorld framework facilitates generating, train-\ning, and evaluating interactive text-based games (C\nˆot´e et al.\n2018). Hermann et al. (2017) and Hill et al. (2017) de-\nvelop and use 3D world simulations for learning grounded\nlanguage. These efforts are similar to our work in the sense\nthat the true world state is, by construction, available, but\nour setup differs in that it provides a natural way of probing\nthe state tracking of a model trained with an LM objective.\nCloze Tasks for Natural Language Models.There has\nbeen a great deal of work on cloze tasks for evaluating nat-\nural language models (Hermann et al. 2015; Hill et al.\n2016). These tasks range from testing general text under-\nstanding (Paperno et al. 2016) to targeting particular as-\npects of natural language, such as commonsense/pragmatics\n(Mostafazadeh et al. 2016; Ettinger 2020), narrative under-\nstanding (Mostafazadeh et al. 2017), and factual knowledge\n(Petroni et al. 2019). Creating these tasks often requires hu-\nman curation, and the evaluation is typically limited to exact\nmatch.11 Our proposed tasks are a form of cloze tasks, but\ncan be precisely automated so that they require no human\ncuration, and can be evaluated at a ﬁne-grained level.\nProbing. One of the goals of this work is to probe the lan-\nguage model’s board state tracking capability. A typical so-\nlution used by prior work is to train a probing model on top\nof a pretrained model (Ettinger, Elgohary, and Resnik 2016;\nAlain and Bengio 2017; Adi et al. 2017; Tenney et al. 2019;\n10In practice, these models often use a combination of the pro-\nposed approximate global attention and simple local attention.\n11Automated cloze tasks without human ﬁltering can yield in-\nstances which even humans can’t answer (Hill et al. 2016).\nHewitt and Liang 2019). This setup is time-consuming as\nit requires training probing models for all tasks. Moreover,\nthe complexity of the probing model can also affect the con-\nclusions (Pimentel et al. 2020). In our case, by using an\nappropriate choice of notation, probing for board state can\nbe accomplished via simple prompts (Section 3).\nDeep Learning for Chess.Deep networks have been used\nin prior work to predict the next move given the true game\nstate (David, Netanyahu, and Wolf 2016; Oshri and Khand-\nwala 2015). For example, using only self-play and the rules\nof chess, AlphaZero achieves superhuman performance start-\ning from random play (Silver et al. 2018). The focus of\nthis prior work is the quality of game play given the true\nboard state, while we use chess as a testbed for evaluating a\nlanguage model’s board state tracking capability. Recently\nthere has also been work focusing on transformer language\nmodels for chess (Presser and Branwen 2020; Cheng 2020;\nNoever, Ciolino, and Kalin 2020). This work is similar to\nours in the sense that the input is limited to the move se-\nquence without the true board state, but the focus is again\nthe quality of game play rather than the model’s awareness\nof the underlying state.\n7 Conclusion\nWe propose the game of chess as a testbed for evaluating how\nwell language models capture the underlying world state. We\nshow that with an appropriate choice of chess notation, a lan-\nguage model can be probed for different aspects of the board\nstate via simple prompts. The simple and precise dynamics\nof chess allow for (a) training models with varying amount\nof explicit state, and (b) evaluating model predictions at a\nﬁne-grained level. Results show that transformer language\nmodels are able to track the board state when given enough\ndata, but with limited data, providing access to board state in-\nformation during training can yield consistent improvement.\nWider Implications for Natural Language Processing.\nOur results shed light on the following properties of trans-\nformers: (a) they are robust to RAP-like changes in input\ndistribution, and (b) for high performance the models re-\nquire access to the entire context, as well as large training\nsets (Section 5.3). Future work can use the ﬁrst ﬁnding to\nintroduce the world state, or more speciﬁcally the output\nof linguistic analyzers such as coreference, via RAP-like\ntokens during pre-training and ﬁne-tuning of transformers.\nRAP-like tokens can also be used for debugging/diagnosing\na model’s understanding, similarly to the starting square pre-\ndiction tasks. The second ﬁnding implies that the proposed\nbenchmark can guide the search for new transformer archi-\ntectures that are adept at understanding long text, and that\ncan learn from small training sets. The proposed framework\nallows for probing and understanding new architectures that\naddress these challenges.\nAcknowledgements\nWe thank Ed Schr¨oder for permitting us to use the Million-\nbase database for this project. We thank Allyson Ettinger\nand colleagues at TTI Chicago for their valuable feedback.\n11391\nThis material is based upon work supported by the National\nScience Foundation under Award No. 1941178.\nReferences\nAdi, Y .; Kermany, E.; Belinkov, Y .; Lavi, O.; and Goldberg,\nY . 2017. Fine-grained Analysis of Sentence Embeddings\nUsing Auxiliary Prediction Tasks. In ICLR.\nAlain, G.; and Bengio, Y . 2017. Understanding intermediate\nlayers using linear classiﬁer probes. In ICLR Workshop.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language Models\nare Few-Shot Learners. arXiv:2005.14165.\nCheng, R. 2020. Transformers Play Chess.\nhttps://github.com/ricsonc/transformers-play-chess.\nChoromanski, K. M.; Likhosherstov, V .; Dohan, D.; Song,\nX.; Gane, A.; Sarlos, T.; Hawkins, P.; Davis, J. Q.; Mohi-\nuddin, A.; Kaiser, L.; Belanger, D. B.; Colwell, L. J.; and\nWeller, A. 2021. Rethinking Attention with Performers. In\nICLR.\nCˆot´e, M.-A.; K´ad´ar, A.; Yuan, X.; Kybartas, B.; Barnes, T.;\nFine, E.; Moore, J.; Tao, R. Y .; Hausknecht, M.; Asri, L. E.;\nAdada, M.; Tay, W.; and Trischler, A. 2018. TextWorld:\nA Learning Environment for Text-based Games. CoRR,\nabs/1806.11532.\nDavid, E.; Netanyahu, N. S.; and Wolf, L. 2016. DeepChess:\nEnd-to-End Deep Neural Network for Automatic Learning\nin Chess. In International Conference on Artiﬁcial Neural\nNetworks (ICANN).\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL.\nEttinger, A. 2020. What BERT is Not: Lessons from a New\nSuite of Psycholinguistic Diagnostics for Language Models.\nTACL, 8(0).\nEttinger, A.; Elgohary, A.; and Resnik, P. 2016. Probing\nfor semantic evidence of composition by means of simple\nclassiﬁcation tasks. In 1st Workshop on Evaluating Vector-\nSpace Representations for NLP.\nFalcon et al., W. 2019. PyTorch Lightning.\nhttps://github.com/PyTorchLightning/pytorch-lightning.\nFiekas, N. 2012. python-chess: a chess library for Python.\nhttps://github.com/niklasf/python-chess.\nHermann, K. M.; Hill, F.; Green, S.; Wang, F.; Faulkner,\nR.; Soyer, H.; Szepesvari, D.; Czarnecki, W. M.; Jaderberg,\nM.; Teplyashin, D.; Wainwright, M.; Apps, C.; Hassabis, D.;\nand Blunsom, P. 2017. Grounded Language Learning in a\nSimulated 3D World. CoRR, abs/1706.06551.\nHermann, K. M.; Koˇcisk´y, T.; Grefenstette, E.; Espeholt, L.;\nKay, W.; Suleyman, M.; and Blunsom, P. 2015. Teaching\nMachines to Read and Comprehend. In NeurIPS.\nHewitt, J.; and Liang, P. 2019. Designing and Interpreting\nProbes with Control Tasks. In EMNLP-IJCNLP.\nHill, F.; Bordes, A.; Chopra, S.; and Weston, J. 2016. The\nGoldilocks Principle: Reading Children’s Books with Ex-\nplicit Memory Representations. In ICLR.\nHill, F.; Hermann, K. M.; Blunsom, P.; and Clark, S.\n2017. Understanding Grounded Language Learning Agents.\nCoRR, abs/1710.09867.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term\nmemory. Neural computation, 9.\nKatharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F.\n2020. Transformers are RNNs: Fast Autoregressive Trans-\nformers with Linear Attention. In ICML.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. In ICLR.\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2020. Reformer:\nThe Efﬁcient Transformer. In ICLR.\nManning, C. D.; Raghavan, P.; and Sch ¨utze, H. 2008. In-\ntroduction to Information Retrieval. Cambridge University\nPress.\nMicikevicius, P.; Narang, S.; Alben, J.; Diamos, G.; Elsen,\nE.; Garcia, D.; Ginsburg, B.; Houston, M.; Kuchaiev, O.;\nVenkatesh, G.; and Wu, H. 2018. Mixed Precision Training.\nIn ICLR.\nMostafazadeh, N.; Chambers, N.; He, X.; Parikh, D.; Batra,\nD.; Vanderwende, L.; Kohli, P.; and Allen, J. 2016. A\nCorpus and Cloze Evaluation for Deeper Understanding of\nCommonsense Stories. In NAACL.\nMostafazadeh, N.; Roth, M.; Louis, A.; Chambers, N.; and\nAllen, J. 2017. LSDSem 2017 Shared Task: The Story\nCloze Test. In 2nd Workshop on Linking Models of Lexical,\nSentential and Discourse-level Semantics.\nNoever, D.; Ciolino, M.; and Kalin, J. 2020. The Chess\nTransformer: Mastering Play using Generative Language\nModels. arXiv:2008.04057.\nOshri, B.; and Khandwala, N. 2015. Predicting Moves in\nChess using Convolutional Neural Networks. In Stanford\nCS231n Course Report.\nPaperno, D.; Kruszewski, G.; Lazaridou, A.; Pham, N. Q.;\nBernardi, R.; Pezzelle, S.; Baroni, M.; Boleda, G.; and\nFern´andez, R. 2016. The LAMBADA dataset: Word predic-\ntion requiring a broad discourse context. In ACL.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,\nM.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,\nJ.; and Chintala, S. 2019. PyTorch: An Imperative Style,\nHigh-Performance Deep Learning Library. In NeurIPS.\nPetroni, F.; Rockt¨aschel, T.; Riedel, S.; Lewis, P.; Bakhtin,\nA.; Wu, Y .; and Miller, A. 2019. Language Models as\nKnowledge Bases? In EMNLP-IJCNLP.\nPimentel, T.; Valvoda, J.; Hall Maudslay, R.; Zmigrod, R.;\nWilliams, A.; and Cotterell, R. 2020. Information-Theoretic\nProbing for Linguistic Structure. In ACL.\n11392\nPresser, S.; and Branwen, G. 2020. A Very Unlikely\nChess Game. https://slatestarcodex.com/2020/01/06/a-very-\nunlikely-chess-game/.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised\nMultitask Learners. In Technical Report, OpenAI.\nSilver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai,\nM.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran, D.; Graepel,\nT.; Lillicrap, T.; Simonyan, K.; and Hassabis, D. 2018. A\ngeneral reinforcement learning algorithm that masters chess,\nshogi, and Go through self-play. Science, 362(6419).\nTenney, I.; Xia, P.; Chen, B.; Wang, A.; Poliak, A.; McCoy,\nR. T.; Kim, N.; Durme, B. V .; Bowman, S. R.; Das, D.; and\nPavlick, E. 2019. What do you learn from context? Probing\nfor sentence structure in contextualized word representations.\nIn ICLR.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In NeurIPS.\nWeston, J.; Bordes, A.; Chopra, S.; Rush, A. M.; van\nMerri¨enboer, B.; Joulin, A.; and Mikolov, T. 2015. Towards\nAI-Complete Question Answering: A Set of Prerequisite Toy\nTasks. arXiv:1502.05698.\nWolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; Davi-\nson, J.; Shleifer, S.; von Platen, P.; Ma, C.; Jernite, Y .; Plu,\nJ.; Xu, C.; Scao, T. L.; Gugger, S.; Drame, M.; Lhoest,\nQ.; and Rush, A. M. 2019. HuggingFace’s Transform-\ners: State-of-the-art Natural Language Processing. ArXiv,\nabs/1910.03771.\n11393",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7724384665489197
    },
    {
      "name": "Testbed",
      "score": 0.7595502138137817
    },
    {
      "name": "Language model",
      "score": 0.6908013224601746
    },
    {
      "name": "Natural language",
      "score": 0.6216021180152893
    },
    {
      "name": "Transformer",
      "score": 0.5820537805557251
    },
    {
      "name": "Notation",
      "score": 0.5279741883277893
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5027084350585938
    },
    {
      "name": "Natural language understanding",
      "score": 0.43068578839302063
    },
    {
      "name": "Natural language processing",
      "score": 0.4130479693412781
    },
    {
      "name": "Machine learning",
      "score": 0.33445632457733154
    },
    {
      "name": "Engineering",
      "score": 0.11877146363258362
    },
    {
      "name": "Linguistics",
      "score": 0.08780255913734436
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}