{
    "title": "Research on Detection Methods for Text Generated by Large Language Models Based on Multi-Model Ensemble",
    "url": "https://openalex.org/W4404681387",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2101521597",
            "name": "Liang Tian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1840793008",
            "name": "Nan Jiang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3118781290",
        "https://openalex.org/W3101891351",
        "https://openalex.org/W4299567010"
    ],
    "abstract": "The rapid development of Large Language Models (LLMs) has made their generated text almost indistinguishable from human writing, posing significant challenges to traditional human-machine recognition techniques. This paper proposes a detection method based on multi-model ensemble to accurately identify text generated by LLMs. Firstly, a large-scale, diverse, and heterogeneous dataset is constructed, covering student writings and texts generated by models such as GPT-3, GPT-2, CTRL, and XLM. Then, a multifaceted detection framework integrating linear models, deep learning models, and pre-trained language models is designed. The linear model utilizes an argumentative essay dataset (DAIGT V2 Train Dataset) similar in distribution to the competition dataset, combined with adaptive BPE tokenization, N-Gram, and TF-IDF features. It employs Multinomial Naive Bayes and SGDClassifier to train classifiers that capture shallow statistical features of the text. The deep learning model fine-tunes the DeBERTa-v3-small model on large-scale datasets (Pile, Ultra, Human vs. LLM Text Corpus) to learn deep semantic representations of the text. The pre-trained language model introduces a fine-tuned DistilRoBERTa model, enhancing detection capabilities using third-party datasets. Finally, the above models are integrated through a weighted average strategy, significantly improving the generalization and robustness of the detection results. Experimental results show that this method achieved a score of 0.967466 in the Kaggle competition, earning a silver medal and outperforming any single model. The study demonstrates the effectiveness of multi-source data and multi-model ensemble in detecting LLM-generated text, providing new ideas and practical references for research in this field.",
    "full_text": null
}