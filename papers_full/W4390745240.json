{
  "title": "Legal Information Retrieval and Entailment Using Transformer-based Approaches",
  "url": "https://openalex.org/W4390745240",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5002116491",
      "name": "M. S. Kim",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5090507452",
      "name": "Juliano Rabelo",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5030967313",
      "name": "Housam Khalifa Bashier Babiker",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5108714848",
      "name": "Md Abed Rahman",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5091866119",
      "name": "Randy Goebel",
      "affiliations": [
        "University of Alberta"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4366327317",
    "https://openalex.org/W1678356000",
    "https://openalex.org/W4362650374",
    "https://openalex.org/W3097977265",
    "https://openalex.org/W2251329024",
    "https://openalex.org/W2970438301",
    "https://openalex.org/W4362654035",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2945067664",
    "https://openalex.org/W4213191780",
    "https://openalex.org/W2984812384",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W6795402733",
    "https://openalex.org/W2808308446",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W2593833795",
    "https://openalex.org/W4362653988",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4362650350",
    "https://openalex.org/W4390822940"
  ],
  "abstract": "Abstract The challenge of information overload in the legal domain increases every day. The COLIEE competition has created four challenge tasks that are intended to encourage the development of systems and methods to alleviate some of that pressure: a case law retrieval (Task 1) and entailment (Task 2), and a statute law retrieval (Task 3) and entailment (Task 4). Here we describe our methods for Task 1 and Task 4. In Task 1, we used a sentence-transformer model to create a numeric representation for each case paragraph. We then created a histogram of the similarities between a query case and a candidate case. The histogram is used to build a binary classifier that decides whether a candidate case should be noticed or not. In Task 4, our approach relies on fine-tuning a pre-trained DeBERTa large language model (LLM) trained on SNLI and MultiNLI datasets. Our method for Task 4 was ranked third among eight participating teams in the COLIEE 2023 competition. For Task 4, We also compared the performance of the DeBERTa model with those of a knowledge distillation model and ensemble methods including Random Forest and Voting.",
  "full_text": "Vol.:(0123456789)\nThe Review of Socionetwork Strategies (2024) 18:101–121\nhttps://doi.org/10.1007/s12626-023-00153-z\n1 3\nARTICLE\nLegal Information Retrieval and Entailment Using \nTransformer‑based Approaches\nMi‑Young Kim1  · Juliano Rabelo2 · Housam Khalifa Bashier Babiker3 · \nMd Abed Rahman3 · Randy Goebel4\nReceived: 31 August 2023 / Accepted: 28 November 2023 / Published online: 11 January 2024 \n© The Author(s) 2024\nAbstract\nThe challenge of information overload in the legal domain increases every day. The \nCOLIEE competition has created four challenge tasks that are intended to encourage \nthe development of systems and methods to alleviate some of that pressure: a case \nlaw retrieval (Task 1) and entailment (Task 2), and a statute law retrieval (Task 3) \nand entailment (Task 4). Here we describe our methods for Task 1 and Task 4. In \nTask 1, we used a sentence-transformer model to create a numeric representation \nfor each case paragraph. We then created a histogram of the similarities between \na query case and a candidate case. The histogram is used to build a binary classi-\nfier that decides whether a candidate case should be noticed or not. In Task 4, our \napproach relies on fine-tuning a pre-trained DeBERTa large language model (LLM) \ntrained on SNLI and MultiNLI datasets. Our method for Task 4 was ranked third \namong eight participating teams in the COLIEE 2023 competition. For Task 4, We \nalso compared the performance of the DeBERTa model with those of a knowledge \ndistillation model and ensemble methods including Random Forest and Voting.\nKeywords COLIEE 2023 · Legal information retrieval · Legal information \nentailment · Transformer-based legal information extraction\n1 Introduction\nEvery day, large volumes of legal data are produced by law firms, law courts, \nindependent attorneys, legislators, regulators, and many others. Within that con-\ntext, the disciplined management of legal information becomes manually intracta-\nble and requires the development of tools that automatically or semi-automatically \nExtended author information available on the last page of the article\n102 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\naid legal professionals in managing information overload. The COLIEE competi-\ntion1 addresses four facets of that challenge: case law retrieval, case law entailment, \nstatute law retrieval, and statute law entailment. Here we summarize the details of \nour approach to case law retrieval and statute law entailment, evaluate the results \nachieved, and comment on future work to further improve our models.\nThe case law retrieval task (Task 1) involves identifying legal cases that should \nbe “noticed” concerning a given query case from amongst a given set of candidate \ncases. In Task 1, our goal is to identify legal cases that are relevant to a given query \ncase and that support the decision of the given query case. These relevant legal cases \nare referred to as “noticed cases.” in Task 1. Our approach to this challenging task \nrelies on a transformer-based model that creates a multidimensional numeric repre-\nsentation of every paragraph within each case. We then calculate cosine distances \nbetween each paragraph of a query case and a candidate case, create a histogram \nfrom the results, and use those distances to train a binary classifier to determine \nwhether an input case should be noticed. Additionally, in the context of the COLIEE \ndatasets, we perform some simple pre-processing and post-processing steps, such as \nremoving French fragments and applying a minimum confidence score for the clas-\nsifier outputs, before generating the final results.\nIn the COLIEE competition, Task 4 focuses on legal entailment, which is to pre-\ndict the entailment relationship between a legal query and statutory law articles, by \ncomparing the meaning of the legal query and the law articles. In general, this task \nrequires participants to develop algorithms capable of reading a query and a law \narticle (or multiple law articles) and then determining whether the law article(s) \nentail the legal query or not. In other words, the goal of the statute law entailment \n(Task 4) is to construct yes/no question-answering systems for legal queries, by con-\nfirming entailment of a query from articles. The answer to a question is typically \ndetermined by measuring some kind of semantic similarity between question and \nanswer. Because the legal bar exam query and articles are complex and varied, we \nneed to carefully determine what kind of information is needed to confirm textual \nentailment. Here we exploit the idea of natural language inference and fine-tune a \nDeBERTa-large language model (LLM) to construct a yes/no question-answering \nsystem for legal queries.\nOur approach for Task 4 relies on a transformer (DeBERTa)-based model to con-\nstruct a classifier for yes/no questions. The DeBERTa model was initially trained \nfor the natural language inference task (NLI) using two datasets, namely SNLI [2] \nand MultiNLI [35]. In addition, to standardize all the inputs to the model, we pro-\nvide all lowercase sentences to the model to generate the final results. This approach \nachieved an accuracy of 0.6634 in the official test dataset, which was ranked third \namongst eight competitors in Task 4 of the COLIEE 2023 competition.\nOur paper is organized as follows: Sect. 2 presents a brief state-of-the-art review; \nSects. 3.1 and 4 describe our method in more detail. Section  5 analyzes the results \nalong with some subsequent experiments with smaller transformers and with models \npre-trained on legal data. Section  5 also includes a fault analysis of some models. \nFinally, Sect. 6 provides some final remarks and proposes some future work.\n1 https:// sites. ualbe rta. ca/ ~rabelo/ COLIE E2023/.\n103\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \n2  Literature Review\nMost current approaches to legal information retrieval rely on traditional informa-\ntion retrieval (IR) methods and more recently, transformer-based large language \nmodel (LLM) techniques. Here we briefly summarize some of the most success-\nful approaches in Task 1, proposed in recent editions of the COLIEE competition.\nTR [ 30] uses a two-phase approach to legal case document information \nretrieval. In the first phase, they generate a candidate set optimized for recall, \naiming to include all true noticed cases while removing some false candidates. \nIn the second phase, they train a binary classifier to predict whether a given \n(query case, candidate case)  pair represents a true noticed relationship. In this \nstep, these authors experimented with logistic regression, naive Bayes, and tree-\nbased classifiers.\nNeuralMind [ 29] applied “vanilla” BM25 to the case law retrieval problem. \nThe authors first indexed all base and candidate cases in the dataset. Before \nindexing, each document was split into segments of text using a context window \nof 10 sentences with overlapping strides of 5 sentences (the “candidate case seg-\nments”). BM25 was then used to retrieve candidate case segments for each base \ncase segment. The relevance score for a (base case, candidate case)  pair was the \nmaximum score among all the base case and candidate case segment pairs. The \ncandidates were then ranked using empirically determined threshold heuristics.\nTUWBR [ 7] starts from two assumptions: first, that there is a topical overlap \nbetween a query and noticed cases, but that not all parts of a query case are \nequally important. Secondly, they assume that traditional IR methods, such as \nBM25, provide competitive results in Task 1. They perform both document-level \nand text-passage-level retrieval and also augment the system by adding external \ndomain knowledge by extracting statute fragments and explicitly adding those \nfragments to the documents.\nJNLP [ 3] applies an approach that first splits the documents into paragraphs, \nand then calculates the similarities between cases by combining term-level \nmatching and semantic relationships at the paragraph level. An attention model is \napplied to encode the whole query in the context of candidate paragraphs, which \nprovides the basis to infer the relationship between cases.\nDoSSIER [ 1] combined traditional and neural network-based techniques in \nTask 1. The authors investigate lexical and dense first-stage retrieval methods \naiming for a high recall in the initial retrieval and then compare shallow neural \nnetwork re-ranking between the MTFT-BERT model and the BERT-PLI model. \nThey then investigate which part of the text of a legal case should be taken into \naccount for re-ranking. Their results show that BM25 shows a consistently high \neffectiveness across different test collections in comparison to the neural network \nre-ranking models.\nTask 1 has been recently adjusted in COLIEE. The new configuration increased \nthe difficulty so that previously used Information Retrieval methods, even aug-\nmented with transformer-based approaches, did not show great results [24]. \nGiven that most of the current approaches work at the document level, we chose \n104 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\nto experiment with the documents at the sentence level to try and capture more \nlocalized information. More details of the approach are presented in Sect. 3.1.\nIn the same manner, we briefly summarize some of the most successful \napproaches in Task 4, proposed in recent editions of the COLIEE competition.\nKIS [9] adopted an ensemble approach that combines their rule-based method \nleveraging predicate-argument structures, with the incorporation of BERT-based \nmethods. Their BERT-based methods include data augmentation, data selection, \nand person name inference. They showed the highest performance in the COLIEE \n2022 competition.\nHUKB [ 36] introduced a method for selecting relevant portions from articles, \naccompanied by a new data augmentation method. This was added by an ensem-\nble approach that combined BERT with data augmentation and extraction of judi-\ncial decision sentences.\nJNLP [3] conducted a comparative analysis among ELECTRA, RoBERTa, and \nLegal-BERT.\nLLNTU [ 19] restructured the provided data into a dataset comprising disjunc-\ntive union strings derived from training queries and articles. They also developed \na similarity comparison model based on the longest uncommon subsequence.\nOvGU [34] utilized an ensemble of graph neural networks (GNNs), combined \nwith referring textbook nodes and averaged sentence embeddings. The details of the \nCOLIEE 2022 competition methods are in [17].\nDeep learning methods have enabled the construction of complex and accurate \nmodels for the Natural Language Inference (NLI) of Task 4 ( [25, 28]). Most current \napproaches to the NLI problem are formulated as a 3-way classification (entailment, \ncontradiction, and neutral) of the entailment relation between a pair of sentences. \nProperly approached, this task requires a sophisticated semantic framework to \nunderstand the context for two sentences (premise, hypothesis) ( [10, 26]). In other \nwords, textual entailment is a logical reasoning task in which the goal is to deter -\nmine whether one sentence can be inferred from another (more generally, whether \none text segment can be inferred from another).\nDeep nets have achieved encouraging performances on many NLI datasets such \nas [6, 18], and [23]. Most of related work employs complex neural architectures such \nas RNNs: [33, 20, 31], CNNs: [11], and BERT: [13]. In general, the improvements \nin performance suggest that these models tend to capture better representations from \nthe datasets that can be used as proxies to predict the relationship between sentences. \nIn the sentential case, the NLI task usually consists of classifying an ordered pair of \nsentences into one of three categories: “positive entailment” occurs when one can \nuse the first sentence to prove that a second sentence is true. Conversely, “negative \nentailment” occurs when the first sentence can be used to disprove the second sen-\ntence. Finally, if the two sentences do not correlate, as determined by the failure of \nthe first two tests, they are considered to have a “neutral entailment.”\nThe statute law entailment task (Task 4) in COLIEE is similarly designed: the \nparticipants are required to decide if a query is entailed from the civil law statutes.\nNLI datasets are typically built by asking annotators to compose sentences based \non premises extracted from existing corpora so that the composed sentences stand in \nentailment/ contradiction/ neutral relationship to the premise [16]. In COLIEE 2023, \n105\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \nwe have two relationships that need to be verified: entailment (yes) and non-entailment \n(no).\nFor this problem, we rely on the base DeBERTa model [13] which is an extension of \nthe original BERT model. The DeBERTa-based model was trained on large volumes of \nraw text corpora using the idea of self-supervised learning. As compared to the original \nBERT model, DeBERTa captures more fine-grained contextual information and rela-\ntionships between tokens, resulting in a significant performance gain on a wide range \nof Natural Language Understanding (NLU) tasks.\nOn the other hand, we also experimented with knowledge distillation approaches. \nKnowledge distillation (KD), refers to the process of compressing larger transform-\ners into smaller versions that have similar performance to their larger counterparts. \nIn this paper, we experimented with nli-MiniLM2-L6-H768 (MiniLMV2) [32] KD \nmodel, trained on the QNLI [35] and SNLI [2] datasets. In the MiniLMV2 paper [32], \nthe authors used a teacher-student paradigm to train the smaller KD model, whereas \na larger BERT-based model worked as the teacher. The MiniLMV2 paper introduced \nmultiple variants, all having various sizes and various teacher models. Among them, \nwe chose the L6-H768 variant which demonstrated the best performance across a wider \nvariety of tasks. We also experimented with a tiny DeBERTa variant trained on QNLI \nand SNLI datasets which were introduced in the paper by He et al. [12]. In their paper, \nthe authors retrained various smaller versions of DeBERTa using a variant of embed-\nding sharing [5]. We have also experimented with another BERT-based model dubbed \nRoBERTa-base [21]. This version of the RoBERTa-base model was also trained on the \nQNLI [35] and SNLI [2] datasets. Additionally, we did some experiments with simple \nensemble stacked models. For the ensemble classifiers, we experimented with Random \nForest [14] and Voting.\n3  Our Method‑Task 1\n3.1  Task Description\nThe Case Law Retrieval Task in COLIEE investigates the performance of systems that \nsearch a set of case law records that support the unseen case law. The goal of the task \nis to return “noticed cases” in the given collection to a query. A case is “noticed” by \na query case if and only if the case is referenced by the query case. In this task, the \nexplicit references to other cases are redacted from the query case contents.\n3.2  Dataset Analysis\nThe Task 1 corpus is composed of Federal Court of Canada case laws, and contains \nall query and noticed cases in a single pool. The training data contains labels indi-\ncating which cases are noticed by each query case. In the test data, only the query \ncases are given and the task is to predict which cases should be noticed for each of \nthe test query cases.\n106 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\nIn Task 1, the training dataset consists of 4400 unique files (cases), with 959 of \nthose identified as query cases. There are a total of 4488 noticed cases, an average of \n4.67 noticed cases per query case (a case in the pool may be referenced by more than \none query case, so 4488 noticed cases include the same cases multiple times.). In the \nprovided test dataset there were 319 query cases and a total of 1335 files (cases).\nEach case in the pool is a text file containing the full contents of a legal case. \nApart from these files, the training data contains a JSON file with the query case \nlist, and for each query case the list of noticed cases. The test data contains the pool \nof test files and a JSON file with the list of query cases. A fragment of the training \nJSON file is given below as an example:\n\"008447.txt\": [\"072495.txt\",\"082291.txt\",\"004851.txt\"],\n\"067501.txt\": [\"038025.txt\"],\n\"007627.txt\": [\"003575.txt\",\"043211.txt\"],\n...\nWhere you can see the list of query cases (008447.txt, 067501.txt, and 007627.\ntxt), and the respective list of noticed cases for each query case. For example, if \n008447.txt and a pool of cases are given as input, the output should be the noticed \ncases from 008447.txt, which are 072495.txt, 082291.txt, and 004851.txt in this \nexample. All the cases in the training JSON are given in the training pool, which \nalso contains cases that are not referenced by any query case. The same happens in \nthe test dataset: all the given query cases, and all the noticed cases (which need to be \nfound by the participants) are included in the test pool.\n3.3  Details of our Approach\nIn case law, a “noticed” case is a precedent cited in a case, the precedent being con-\nsidered somewhat relevant to that case at hand. It is very hard to model why a prec-\nedent may be seen as relevant to a given case, for it is a very subjective considera-\ntion by the judge while working on the case. It may be that the judge found a similar \nargument in a previous case, or that the previous case establishes the jurisprudence \nfor the legal issue under consideration, or that a case is noticed because it actually \ncontradicts a point raised by one of the parties and the judge is showing that contra-\ndiction, etc. In this paper, we considered semantic similarity between two cases to \nbe a reasonable proxy to determine whether a case should be noticed, and develop \nour method starting from that hypothesis.\nOur approach to the case law retrieval task relies on the use of a sentence-\ntransformer model to generate a multidimensional numeric representation of text. \nThis model is applied to each paragraph from both the query case and every can-\ndidate case. We then use a cosine measure to determine the distances between the \n768-dimension vectors from the query paragraphs and the candidate paragraphs. A \n10-bin histogram of those distances is generated and a Gradient Boosting [8] binary \nclassification model is trained on those inputs.\nGiven the formulation of the problem, we had to make some choices to produce \na manageable training dataset: since the test set contains a total of approximately \n107\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \n1300 cases of which 319 are query cases, we assumed we should generate a train-\ning dataset with around 1000 negative samples per query case. So we needed to \ndown-sample the negative samples in the training dataset to 1000. At the same \ntime, the positive class is significantly underrepresented (less than 5 samples per \nquery case on average), so we over-sampled those examples by simple replication.\nWe also implemented some simple pre-processing steps:\n– Removal of French contents through a language identification model based on \na naive Bayesian filter [22];\n– Splitting of input text into paragraphs based on simple pattern matching which \nrelies on the common format used in cases. This method relies on finding a \nsequence of numbered paragraphs (specified as digits between brackets) as the \nfirst characters in the line starting at “[1 ]” and looking for the next natural \nnumber;\n– Extraction of dates mentioned in the cases is done by the application of a \nnamed entity recognition model [15]. These dates are used to remove can-\ndidate cases that mention dates more recent than the most recent date men-\ntioned in a query case, under the assumption those candidates cannot be a true \nnoticed case because they are more recent than the query case. So, we extract \nall date entities in both the query and the candidate case, then, if the query \ncase contains a date that is more recent than the most recent date in the candi-\ndate case, that candidate case will be removed from the list.\nAt inference time we do the following steps:\n– Date filtering: we apply the same date pre-processing steps mentioned above;\n– Histograms: we generate histograms for every pair of query documents and \neach candidate that does not contain dates more recent than the query docu-\nment dates;\n– Apply model: we use those histograms as inputs to build our classification \nmodel.\nBased on our analysis of the training dataset, we also apply some simple post-\nprocessing steps:\n– Number of noticed cases per query case: the average number of noticed cases \nper query case in the training dataset is 4.67, so we establish a range of 3 to 10 \nmaximum noticed cases per query case;\n– Confidence score: we establish a minimum confidence score for the classifier, \ndisregarding outputs that are below a given threshold;\n– Repeating noticed cases: if the same case is noticed across many different \nquery cases, we also remove that noticed case from our final answer as it is \nobserved in the training dataset that this is an uncommon situation.\n108 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\nA high-level diagram of the approach is given in Fig.  1. In the figure, you can see \nthe pre-processing, histogram generation, binary classification, and post-processing \nthat have been explained above.\nWe have experimented with a range of parameters for each one of those post-\nprocessing criteria and selected the 3 combinations that produced the best output in \na validation set containing 50 query cases.2\n3.3.1  Sentence‑Transformer Model\nThe model used to produce the 768-dimensional representations for the case para-\ngraphs was the HuggingFace sentence-transformers/all-mpnet-base-v2 model.3 That \nFig. 1  High-level diagram for the Task 1 approach\n2 The validation set was randomly drawn from the provided dataset and has no overlap with the cases \nused for training.\n3 https:// huggi ngface. co/ sente nce- trans forme rs/ all- mpnet- base- v2.\n109\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \nmodel was trained on large sentence-level datasets using a self-supervised contras-\ntive learning objective, which used the pre-trained Microsoft/ mpnet-base model. 4 \nNote that the original authors use a contrastive learning objective: given a sentence \nfrom the pair, the model should predict which of a set of randomly sampled other \nsentences was paired with it in the dataset.\n3.3.2  Binary Classification Model\nWe used the Gradient Boosting model [8] to train our binary classification model, \nwhich was trained on the calculated similarity histograms as described above. Since \nthe training dataset is significantly unbalanced, we over-sample the positive class by \nsimple duplication and under-sample the negative class by establishing a target max-\nimum number (which was chosen as 1000 samples). The only hyper-parameter we \nvaried in the classifier itself was the number of estimators, which was set to 1000, \n3000, and 5000.\n3.3.3  Hyper‑Parameter Setting\nWe performed a grid search for 3 hyper-parameters:\n– Maximum number of noticed cases per query case: based on the dataset analysis \nperformed, given the average number of noticed cases per query case in the train-\ning set is around 5, we experimented with establishing a limit that varied from 3 \nto 10 (step 1) in an attempt to reduce the false positives;\n– Minimum confidence score: we trained a binary classifier to determine if a given \ncase should be noticed concerning a given query case. With this hyper-parameter, \nwe can filter candidate cases for which the classifier confidence score is below a \ngiven threshold. We experimented with values from 0.55 to 0.80 (step 0.05);\n– Maximum duplicate noticed cases: we noticed in our validation results that the \nsame case was classified as noticed for more than one query case, which is not \ncommon in the training dataset, so we established the maximum number of times \nthe same case can be present in the output. This parameter was varied from 1 to 5 \n(step 1).\nThe 3 best performing hyper-parameter combinations were used in our COLIEE sub-\nmission. You can see that the best (max noticed cases = 10, min score = 0.8, max \ndups = 3) achieved good precision but poor recall. The second-best combination (9, \n0.7, 2) had even higher precision, but very poor recall. We attribute this to the effect \nof the minimum confidence score, which was higher in this case, whereas the other \nparameters were similar. Even though the difference in the final f1-score was not \nmaterial, having the ability to tweak parameters and influence precision and recall \nwould be a good feature of the method in real-world applications, where users could \nadopt parameters according to their requirements concerning precision and recall.\n4 https:// huggi ngface. co/ micro soft/ mpnet- base? text= The+ goal+ of+ life+ is+% 3Cmask% 3E.\n110 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\n4  Our Method‑Task 4\nIn Task 4, the problem of answering a legal yes/no question can be viewed as a \nbinary classification problem. We assume that a set of questions Q, where each \nquestion qi ∈ Q  is associated with a list of corresponding article sentences a i1,a i2,\n..., aim , where yi = 1 if the answer is “yes” and yi = 0 otherwise. Therefore, our task \nis to learn a classifier that can predict the entailment answers of any question-arti-\ncle pairs. BERT [6] has shown good historical performance in both COLIEE and in \ngeneral on the natural language inference tasks. However, Jiang and Marnaffe [16] \ninsisted that despite high F1-scores, BERT models have systematic error patterns, \nsuggesting that they still do not capture the full complexity of human pragmatic \nreasoning.\nWe reformulate the problem as a natural language inference task, where the \nobjective of the model is to determine the logical relationship between a premise \nand a hypothesis (e.g., whether the hypothesis entails, contradicts, or is neutral with \nrespect to the given premise). In the case of answering a legal yes/no question, if the \nNLI model predicts the relationship as entailment, we then consider the prediction \nas “yes” otherwise “no.” A high-level diagram is shown in Fig. 2.\nTo construct the training data for the NLI model fine-tuning, we modify the \nground-truth labels. For questions with a ground-truth label of “yes,” we change \nthem to “entailment,” and for questions with a label of “no,” we change them to \n“contradiction.” In the pre-processing step, we also convert the inputs of the data to \nlowercase. Because we have two inputs before making a prediction, we follow the \nprocedure proposed by [27], i.e., we concatenate the sentence embedding u and v \nfrom input 1 (query) and 2 (article) respectively, and then use the element-wise dif-\nference /uni007C.varu− v/uni007C.var and multiply it with the trainable weight W.\nWe then fine-tune the model by minimizing the cross-entropy loss over the \nlabeled training data to penalize incorrect classification.\n5  Results\n5.1  Task 1 Results\nThe results of the official COLIEE evaluation set are shown in Table 1:\nOur best result was achieved with the following post-processing parameters: min-\nimum confidence score = 0.80, maximum noticed cases = 10, maximum number of \nrepeated noticed cases 5 = 3. Our second-best score had similar parameters (0.7, 9, \nand 2, respectively). In the third submission we used 0.65, 10, and 3, respectively). \nThis provided a more balanced trade-off between precision and recall, as opposed to \nthe first two which had a higher precision but a lower recall. This is an interesting \ncharacteristic for real-world applications, as one could make an informed decision \n5 We simply remove noticed cases that appear in more than the maximum allowed query cases. An obvi-\nous improvement is to keep just the highest-scoring noticed cases.\n111\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \non how to tweak parameters depending on which metric is more important for their \nparticular scenario.\nIdeally, we would perform a thorough error analysis to understand the main areas/\ncases of improvement for our approach, but that kind of error analysis is not feasible \nin this scenario as it would require reading every case in the dataset, potentially mul-\ntiple times, and figure out what characteristics in the inputs are not well modeled in \nthe approach.\n5.2  Task 4 Results\nTable 2 shows the Task 4 results on test data in COLIEE 2023. We submitted two \nresults, Our_V1 fine-tuned on DeBERTa-small [13] and Our_V2 fine-tuned on \nDeBERTa-large model [27]. In the table, we report the performance of the best \nmodel i.e., Our_V2 . The test results considering only one best system in each team \nare in Table 3:\nTable 1  Official results for the Case Law Retrieval task. Prec. is an abbreviation for precision\nTeam F1 Prec. Recall Team F1 Prec. Recall\nTHUIR 0.3001 0.2379 0.4063 UFAM 0.2545 0.2975 0.2224\nTHUIR 0.2907 0.2173 0.4389 JNLP 0.2511 0.1971 0.3458\nIITDLI 0.2874 0.2447 0.3481 JNLP 0.2493 0.1931 0.3516\nTHUIR 0.2771 0.2186 0.3783 OurApproach 0.2390 0.3045 0.1967\nNOWJ 0.2757 0.2263 0.3527 OurApproach 0.2345 0.2400 0.2293\nNOWJ 0.2756 0.2272 0.3504 UFAM 0.2345 0.3199 0.1851\nIITDLI 0.2738 0.2107 0.3912 UFAM 0.2156 0.3182 0.1630\nIITDLI 0.2681 0.2063 0.3830 YR 0.1377 0.1060 0.1967\nJNLP 0.2604 0.2044 0.3586 YR 0.1051 0.0809 0.1502\nNOWJ 0.2573 0.2032 0.3504 LLNTU 0.0000 0.0000 0.0000\nOurApproach 0.2555 0.2847 0.2317 LLNTU 0.0000 0.0000 0.0000\nFig. 2  High-level diagram for the Task 4 approach. The inputs are the queries and articles. We first pre-\nprocess the data, and then for the downstream tasks, we fine-tune a deep network to predict the output \n(yes vs no)\n112 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\nThe DeBERTa-large model achieved an accuracy of 55.77% for the “no” class \nand 77.55% for the “yes” class. We found that the current model struggles in pre-\ndicting the correct class “no” which understandably requires a deeper understanding \nof the semantics of the input.\nTable 2  NLI (Task 4) results on \ntest data Team sid Correct Accuracy\nBaseLine No 52/All 101 0.5149\nJNLP JNLP3 79 0.7822\nJNLP JNLP1 76 0.7525\nJNLP JNLP2 76 0.7525\nKIS KIS2 70 0.6931\nKIS KIS1 68 0.6733\nOur Approach Our_V2 67 0.6634\nAMHR AMHR01 66 0.6535\nKIS KIS3 66 0.6535\nAMHR AMHR03 65 0.6436\nLLNTU LLNTUdulcsL 63 0.6238\nOur Approach Our_V1 63 0.6238\nHUKB HUKB2 60 0.5941\nCAPTAIN CAPTAIN.gen 59 0.5842\nCAPTAIN CAPTAIN.run1 58 0.5743\nLLNTU LLNTUdulcsS 57 0.5644\nHUKB HUKB1 56 0.5545\nHUKB HUKB3 56 0.5545\nLLNTU LLNTUdulcsO 56 0.5545\nNOWJ NOWJ.multi-v1-jp 55 0.5446\nCAPTAIN CAPTAIN.run2 53 0.5248\nNOWJ NOWJ.multijp 53 0.5248\nNOWJ NOWJ.multi-v1-en 49 0.4851\nTable 3  NLI (Task 4) results on \ntest data considering only the \nbest system in each team\nTeam sid Correct Accuracy\nBaseLine No 52/All 101 0.5149\nJNLP JNLP3 79 0.7822\nKIS KIS2 70 0.6931\nOur Approach Our_V2 67 0.6634\nAMHR AMHR01 66 0.6535\nLLNTU LLNTUdulcsL 63 0.6238\nHUK HUKB2 60 0.5941\nCAPTAIN CAPTAIN.gen 59 0.5842\nNOWJ NOWJ.multi-v1-jp 55 0.5446\n113\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \n5.3  Further Experiments for Task 4 with Smaller NLI Models\nIn our experiments with Task 4 during the COLIEE competition, we got the best \nresults in terms of accuracy with DeBERTa-large [12]. However, one of the chal-\nlenges with larger models is fine-tuning them for downstream tasks (re-purposing \nthem on a specific task); the computation complexity is higher due to the higher \nnumber of parameters. Moreover, for NLI, there is a prior evidence that smaller \nBERT-based models perform just as well or better [12]. So in this section, we \npresent some subsequent experiments with smaller NLI models as well as tradi-\ntional ensemble models to see how these relatively simpler models compare with \nDeBERTa-large.\nFor our experiment, we picked two types of models, one for knowledge distil-\nlation called nli-MiniLM2-L6-H768 (MiniLMV2) [32], and the other type would \nbe smaller variants of BERT-based models such as DeBERTa and RoBERTa [21]. \nSince DeBERTa-large had the best performance in our initial experiments, we chose \nall 3 smaller variants, namely DeBERTa-xsmall, DeBERTa-small, and DeBERTa-\nbase. From RoBERTa we pick the base model, i.e. RoBERTa-base. All 5 of these \nmodels were trained on QNLI and SNLI datasets. Finally, we also did some experi-\nments with ensembling on these 5 smaller transformer models. For the ensemble \nclassifiers, we experimented with Random Forest [14] and Voting. The results are in \nTable 4.\nThe results in Table  4 show that smaller models do fairly well compared to \nDeBERTa-large. However, it also seems that their performance is not  a simple \nfunction of their size. For example, DeBERTa-xsmall performs better compared \nto DeBERTa-small despite being the smaller model. This performance conforms \nto prior work [12] where DeBERTa-xsmall achieved better results than DeBERTa-\nsmall on an NLI task.\nFinally, ensemble models perform better than the individual small transformers. \nAn Ensemble model with a Random Forest as the meta-classifier performs better \nthan any of the individual smaller transformer models and the Voting meta-classifier \nwith all 5 smaller transformers performs better than DeBERTa-large.\nEnsemble models likely work better than individual models because those mod-\nels individually differ from one another. DEBERTa-base, MiniLM2, and DeBERTa-\nsmall predict the “yes” (entailment) class better and struggle with the “no” class. \nDeBERTa-xsmall has equal performance in predicting the “yes” and “no” classes \nwith slightly better performance for the “yes” class. Finally, the RoBERTa-base \nmodel achieves similar performance in predicting both classes with slightly better \nperformance for the “no” class. Overall, this mix of different models seems to help \nwith the ensemble models’ performance.\n5.4  Further Experiments for Task 4 with Legal‑BERT Models\nWe also conducted another set of experiments with Legal-BERT [4]. Legal-BERT \nis a BERT model that has been augmented by pre-training with legal data and has \nbeen shown to achieve state-of-the-art results on some challenging tasks in the \n114 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\nlegal domain. Legal-BERT has 5 variants. 6 The Contracts model is pre-trained on \nthe SEC-EDGAR repository, containing 76,366 United States (US) contracts. The \nEURLEX model is pre-trained on the EURLEX repository containing 19,867 cases \nfrom the European Court of Justice (ECJ). The ECHR model is pre-trained on \n12,554 cases from HUDOC, the repository of the European Court of Human Rights \n(ECHR). Finally, the base and the small model are pre-trained on all of the afore-\nmentioned repositories. As the names suggest, the base and small models are meant \nto be more general whereas the other 3 models are aimed to be geared towards legal \ndocuments of specific jurisdictions and types.\nFor Legal-BERT there is a mismatch between the corpus that it was pre-trained \non and the data it is being fine-tuned on. As mentioned before, Legal-BERT and its \nvariants are pre-trained on the US and European Union (EU) legislation corpora. On \nthe other hand, the Task 4 data is based on the Japanese bar exam corpus, translated \ninto English. We conduct our experiments on Legal-BERT variants. Because we \nbelieve that despite being pre-trained on corpora that are from different jurisdictions, \nsome commonalities do exist and the remaining mismatch can be addressed by fine-\ntuning. Furthermore, the lack of volume in the Japanese law corpora precludes us \nfrom pre-training a transformer for our requirements. We leave the generation of a \nsufficient volume of corpora related to Japanese law and the subsequent pre-training \nof a transformer model for future work.\nHere we report results on 4 Legal-BERT variants, Contracts, EURLEX, ECHR, \nand Base. We consider Task 4 as a binary classification problem (entailment or no \nentailment) and train Legal-BERT classifiers. The results are in Table  5. From the \ntable, we can see, that Legal-BERT-ECHR has the best performance among the \nLegal-BERT variants. As these models are pre-trained on various types of docu-\nments, another future work of interest would be to investigate whether a particular \ntype of legal document during pre-training helps discern the entailment relationship \nbetween two bodies of text.\nTable 4  NLI (Task 4) Further \nExperiments for Task 4 with \nsmaller NLI models\nModel Name Parameters (M) Accuracy\nOur_V2 [DeBERTa-large] 304 0.66\nDeBERTa-xsmall 22 0.65\nDeBERTa-small 44 0.63\nMiniLM2 81 0.6\nDeBERTa-base 86 0.62\nRoBERTa-base 125 0.6\nStacking-RandomForest N.A 0.66\nStacking-Voting N.A 0.7\n6 https:// huggi ngface. co/ nlpau eb/ legal- bert- base- uncas ed.\n115\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \n5.5  Error Analysis\nWe selected three models for error analysis: DeBERTa-large, Stacking-Voting, \nand Legal-BERT-ECHR, as they demonstrated the best performances within \ntheir respective model groups. Among the DeBERTa models that we employed, \nDeBERTa-large showed the highest accuracy. Stacking-Voting outperformed the \nother ensemble model that we used. Additionally, among the Legal-BERT models \nwe employed, Legal-BERT-ECHR demonstrated the best performance.\nWe categorized error types from unsuccessful instances, as presented in Table  6. \nLegal-BERT-ECHR had the highest number of errors when it predicted entailment \noutputs for queries related to problems with resolving pronoun references. One \nexample is as follows:\n– Input\n– [Input Article] Article 178: The transfer of a real right on movables may \nnot be duly asserted against a third party unless the movables are delivered. \nArticle 184: If a thing is possessed through an agent, the principal orders that \nagent to thenceforward possess that thing on behalf of a third party, and that \nthird party consents thereto, the third party acquires the possessory rights.\n– [Input Query] A sold Painting X, owned by A, to C while depositing it with \nB and ordered B to thenceforward possess X on behalf of C, and B consented \nthereto. In this case, C may duly assert the acquisition of the ownership of X \nagainst any third parties.\n– Output: No\nTo solve the example, a model needs to solve what each pronoun refers to. In the \nexample, “A” and “B” in the hypothesis refer to “principal” and “agent” in the prem-\nise, respectively. “X” and “C” in the hypothesis refer to “thing” and “third party” in \nthe premise.\nOut of the models, only Stack-Voting correctly predicted the entailment in the \nexample, while Legal-BERT-ECHR and DeBERTa-large made incorrect predictions.\nDue to the limited number of test samples, a thorough examination of error analy-\nsis was constrained. Nevertheless, based on the available 101 test samples, Stacking-\nVoting demonstrated the highest accuracy in addressing queries related to resolving \nTable 5  NLI (Task 4) results with Legal-BERT models\nModel Name Accuracy Model Name Accuracy\nLegal-BERT-base 0.5 Legal-BERT-ECHR 0.58\nLegal-BERT-Contracts 0.52 Legal-BERT-EURLEX 0.5\n116 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\npronoun references and analyzing the condition of legal sentences. Here is an exam-\nple that demonstrates the significance of accurately analyzing a condition.\n– Input\n– [Input Article] Article 126: The right to rescind an act is extinguished by \nthe operation of the prescription if it is not exercised within five years from \nthe time when it becomes possible to ratify the act. The same applies if 20 \nyears have passed from the time of the act.\n– [Input Query] The right to rescind an act is extinguished by the operation \nof the prescription if it is not exercised within five years from the voidable \nact.\n– Output: No\nIn the example, both the legal article and the query contain the “if” condition and \nthe corresponding conclusion. While the condition matches in both the article and \nquery, the “if” condition differs. Therefore, the correct entailment label should be \n“no”. Stacking-Voting made the correct prediction.\nLegal-BERT-ECHR showed the fewest errors when answering the queries related \nto paraphrasing. This could be because Legal-BERT-ECHR was pre-trained on legal \ncorpora, whereas the other two models were pre-trained on general domain corpora. \nFollowing is an example that shows paraphrasing between the article and query:\n– Input\n– [Input Article] Article 449: If a guarantor that guarantees an obligation \nwhich may be voidable due to the principal obligor’s limited capacity to \nact, is aware, at the time of entering into a guarantee contract, of the \ncause for its voidability, that guarantor is presumed to have assumed an \nindependent obligation of the same subject matter in the event of non-per -\nformance by the principal obligor or rescission of the obligation.\nTable 6  Error analysis of the models\nError type Count of Wrong Prediction\nDeBERTa Stacking Legal-BERT\n-large -Voting -ECHR\nIncorrect reference of pronoun 14 11 21\nIncorrect negation/antonym detection 6 7 7\nIncorrect analysis of paraphrases 2 3 1\nIncorrect analysis of exceptions 2 2 2\nIncorrect analysis of conjunction(and,or) 1 1 1\nIncorrect assessment of condition 7 5 8\nIncorrect interpretation of others 2 1 2\n117\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \n– [Input Query] If a guarantor that guarantees an obligation which may be \nvoidable due to the limited capacity to act, is aware, at the time of conclu-\nsion of the guarantee contract, of the cause for its voidability, that guar -\nantor is presumed to have assumed an independent obligation of the same \nsubject matter in the event of non-performance by the principal obligor or \nrescission of the obligation.\n– Output: Yes\nIn this example, the phrase “at the time of entering into a guarantee contract” was \nparaphrased as “at the time of conclusion of the guarantee contract” in the query. \nLegal-BERT-ECHR made a correct prediction, while Stacking-Voting made an \nincorrect prediction for this example. Meanwhile, DeBERTa-large excelled in \nhandling queries that needed negation or antonym detection. One example that \nrequires negation/antonym detection is as follows:\n– Input\n– [Input Article] Article 354 If the claim of a pledgee of movables is not \nsatisfied, the pledgee may make a request to the court seeking the imme-\ndiate appropriation of the thing pledged for the satisfaction of that claim \nin accordance with the evaluation of an appraiser only when there are \nreasonable grounds. In this case, the pledgee of movables must notify the \nobligor in advance of the request.\n– [Input Query] If the claim of a pledgee of movables is not satisfied, the \npledgee may immediately appropriate the thing pledged for the satisfaction \nof that claim by getting the permission of the court, instead of getting the \nevaluation of an appraiser, only when there are reasonable grounds for \nnot getting the evaluation of an appraiser.\n– Output: No\nTo solve the example, a model needs to understand that “instead of getting the \nevaluation of an appraiser,” means “NO evaluation of an appraiser”, which con-\ntradicts the premise. Out of the three models, only DeBERTa-large made a cor -\nrect entailment prediction for this example.\nTo facilitate error analysis, we assigned each incorrectly predicted sample to \none category in Table  6. In cases where a sample exhibited multiple error types, \nwe assigned it to the category representing the most challenging aspect for cor -\nrect analysis. Large language models are black-box models, making it challenging \nfor us to comprehend and explain the entailment prediction process in a way that \nis understandable to humans. Therefore, our categorization in Table  6 is based on \nhuman interpretation, and it has the limitation that these categories may not pre-\ncisely align with the true behavior of each model. As part of our future work, we \nplan to employ explainable AI techniques capable of providing faithful insights \ninto the rationale behind each model’s entailment predictions.\n118 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\nIn future work, we aim to develop representations that can be used as proxies to \nassist deep nets in better comprehending legal concepts. In addition, we believe that \nintegrating legal expert knowledge into large language models is crucial for addressing \nthe failures of the large language models. We will conduct a comprehensive exploration \nof how legal expert knowledge can be effectively integrated into these models. This \nintegration will assist in connecting pronouns in queries to legal terms in law articles, \nunderstanding complex conditions, and interpreting intricate conjunctions.\n6  Conclusion\nWe have explained our use of various language models for legal entailment and ques-\ntion-answering in COLIEE 2023. For the case law retrieval task (Task 1), we used a \nsentence-transformer model to generate a multidimensional numeric representation of \ntext, with some heuristic pre-processing and post-processing methods. For the statute \nlaw tasks, our transformer-based NLI system was ranked 3rd in Task 4. Furthermore, \nwe have conducted supplementary analyses and experimented with various approaches, \nobserving a slight enhancement in terms of predictive accuracy when using an ensem-\nble model. A limitation of our method for Task 1 is that, although we calculate pair-\nwise similarities between paragraphs to create histograms, it still considers similarity \nbetween whole documents. Through a more detailed data analysis, we notice that refer-\nences to other cases are made in smaller contexts. Thus, we want to explore methods \nthat identify similarities (or more broadly, relevance) in short contexts as opposed to \nwhole documents. At the implementation level, we also want to explore open-source \nLLMs that can be used in a controlled environment and whose pre-training and fine-\ntuning details are known and do not interfere with the competition’s terms. In addi-\ntion, for task 4, it is essential to investigate techniques to augment the training data and \nfind effective techniques for learning representations of legal texts. We aim to develop \nrepresentations that can be used as proxies to assist deep nets in better comprehending \nlegal concepts and thus improving the discriminative power. Moreover, it is also crucial \nto explore additional quantitative metrics that could help in assessing the strength of \nrepresentations encoded by deep nets before training for the downstream tasks.\nAcknowledgements  This research would not be possible without the  COLIEE competition  organiz-\ners’ data distribution and evaluations. The research is supported by the University of Alberta and the \nAlberta Machine Intelligence Institute, the Canadian Natural Sciences and Engineering Research Coun-\ncil (NSERC) (including funding reference numbers RGPIN-2022-03469 and DGECR-2022-00369), and \nAlberta Innovates.\nDeclarations \nConflict of interest On behalf of all authors, the corresponding author states that there is no conflict of \ninterest.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \n119\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permis-\nsion directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nReferences\n 1. Abolghasemi, A., Althammer, S., Hanbury, A., & Verberne, S. (2022). Dossier@coliee2022: \nDense retrieval and neural re-ranking for legal case retrieval. In: Sixteenth international work -\nshop on Juris-informatics (JURISIN)\n 2. Bowman, S.R., Angeli, G., Potts, C., & Manning, C.D. (2015). A large annotated corpus for \nlearning natural language inference. arXiv preprint arXiv: 1508. 05326\n 3. Bui, M.Q., Nguyen, C., Do, D.T., Le, N.K., Nguyen, D.H., & Nguyen, T.T.T. (2022). Using deep \nlearning approaches for tackling legal’s challenges (coliee 2022). In: Sixteenth international \nworkshop on Juris-informatics (JURISIN)\n 4. Chalkidis, I., Fergadiotis, M., Malakasiotis, P., Aletras, N., & Androutsopoulos, I. LEGAL-\nBERT: The muppets straight out of law school. In: Findings of the association for computational \nlinguistics: EMNLP 2020, pp. 2898–2904. Association for computational linguistics, online \n(2020). https:// doi. org/ 10. 18653/ v1/ 2020. findi ngs- emnlp. 261. https:// www. aclweb. org/ antho \nlogy/ 2020. findi ngs- emnlp. 261\n 5. Clark, K., Luong, M.T., Le, Q.V., & Manning, C.D. (2020). Electra: Pre-training text encoders as \ndiscriminators rather than generators. arXiv preprint arXiv: 2003. 10555\n 6. Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2018). BERT: pre-training of deep bidirec-\ntional transformers for language understanding. CoRR. http:// arxiv. org/ abs/ 1810. 04805\n 7. Fink, T., Recski, G., Kusa, W., & Hanbury, A. (2022). Statute-enhanced lexical retrieval of court \ncases for coliee 2022. In: Sixteenth international workshop on Juris-informatics (JURISIN)\n 8. Friedman, J.H. (2001). Greedy function approximation: a gradient boosting machine. Annals of \nstatistics pp. 1189–1232\n 9. Fujita, M., Onaga, T., Ueyama, A., & Kano, Y. (2022). Legal textual entailment using ensem-\nble of rule-based and bert-based method with data augmentations including generation without \nexcess or deficiency. In: Sixteenth international workshop on Juris-informatics (JURISIN)\n 10. Geiger, A., Richardson, K., & Potts, C. (2020). Neural natural language inference models par -\ntially embed theories of lexical entailment and negation. In: Proceedings of the third Black -\nboxNLP workshop on analyzing and interpreting neural networks for NLP, pp. 163–173\n 11. Gong, Y., Luo, H., & Zhang, J. (2017). Natural language inference over interaction space. arXiv \npreprint arXiv: 1709. 04348\n 12. He, P., Gao, J., & Chen, W. (2021). Debertav3: Improving deberta using electra-style pre-train-\ning with gradient-disentangled embedding sharing. arXiv preprint arXiv: 2111. 09543\n 13. He, P., Liu, X., Gao, J., & Chen, W. (2020). Deberta: Decoding-enhanced bert with disentangled \nattention. arXiv preprint arXiv: 2006. 03654\n 14. Ho, T.K. Random decision forests. (1995). In: Proceedings of 3rd international conference on \ndocument analysis and recognition, vol. 1, pp. 278–282. IEEE\n 15. Honnibal, M., & Johnson, M. (2015). An improved non-monotonic transition system for depend-\nency parsing. In: Proceedings of the 2015 conference on empirical methods in natural language \nprocessing, pp. 1373–1378. Association for computational linguistics, Lisbon, Portugal. https://  \naclweb. org/ antho logy/D/ D15/ D15- 1162\n 16. Jiang, N., & Marneffe, M.d. (2019). Evaluating bert for natural language inference: A case study \non the commitmentbank. In: Proceedings of the 2019 conference on empirical methods in natural \nlanguage processing and the 9th international joint conference on natural language processing \n(EMNLP-IJCNLP), pp. 6088—6093\n 17. Kim, M.Y., Rabelo, J., Goebel, R., Yoshioka, M., Kano, Y., & Satoh, K. Coliee (2023). 2022 \nsummary: Methods for legal document retrieval and entailment. New Frontiers in Artificial Intel-\nligence. JSAI-isAI 2022. Lecture notes in computer science 13859\n120 The Review of Socionetwork Strategies (2024) 18:101–121\n1 3\n 18. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020). Albert: A lite \nbert for self-supervised learning of language representations\n 19. Lin, M., Huang, S.C., & Shao, H.L. (2022). Rethinking attention: An attempting on revaluing \nattention weight with disjunctive union of longest uncommon subsequence for legal queries \nanswering. In: Sixteenth international workshop on Juris-informatics (JURISIN)\n 20. Liu, X., Duh, K., & Gao, J. (2018). Stochastic answer networks for natural language inference. \narXiv preprint arXiv: 1804. 07888\n 21. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & \nStoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv: \n1907. 11692\n 22. Nakatani, S. Language detection library for java (2010). https:// github. com/ shuyo/ langu age- detec \ntion\n 23. Nangia, N., & Bowman, S. (2019). Human vs. muppet: A conservative estimate of human perfor -\nmance on the glue benchmark. In: Proceedings of the 57th annual meeting of the association for \ncomputational linguistics, pp. 4566–4575\n 24. Rabelo, J., Goebel, R., Kim, M.Y., Kano, Y., Yoshioka, M., & Satoh, K. (2022). Overview and \ndiscussion of the competition on legal information extraction/entailment (coliee) 2021. Journal of \nReview of Socionetwork Strategies 16(1)\n 25. Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language understand-\ning by generative pre-training\n 26. Ravichander, A., Naik, A., Rose, C., & Hovy, E. (2019). Equate: A benchmark evaluation frame-\nwork for quantitative reasoning in natural language inference. In: Proceedings of the 23rd confer -\nence on computational natural language learning (CoNLL), pp. 349–361\n 27. Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-net-\nworks. In: Proceedings of the 2019 conference on empirical methods in natural language processing \nand the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp. \n3982–3992\n 28. Rocktäschel, T., Grefenstette, E., Hermann, K.M., Kočiskỳ, T., & Blunsom, P. (2015). Reasoning \nabout entailment with neural attention. arXiv preprint arXiv: 1509. 06664\n 29. Rosa, G.M., Rodrigues, R.C., Lotufo, R., & Nogueira, R. (2021). Yes, bm25 is a strong baseline for \nlegal case retrieval. In: Proceedings of the 18th international conference on artificial intelligence and \nLaw (ICAIL)\n 30. Schilder, F., Chinnappa, D., Madan, K., Harmouche, J., Vold, A., Bretz, H., & Hudzina, J. \n(2021). A pentapus grapples with legal reasoning. In: Proceedings of the COLIEE workshop in \nICAIL\n 31. Tan, C., Wei, F., Wang, W., Lv, W., & Zhou, M. (2018). Multiway attention networks for modeling \nsentence pairs. In: IJCAI, pp. 4411–4417\n 32. Wang, W., Bao, H., Huang, S., Dong, L., & Wei, F. (2020). Minilmv2: Multi-head self-attention \nrelation distillation for compressing pretrained transformers. arXiv preprint arXiv: 2012. 15828\n 33. Wang, Z., Hamza, W., & Florian, R. (2017). Bilateral multi-perspective matching for natural lan-\nguage sentences. In: Proceedings of the 26th International joint conference on artificial intelligence, \npp. 4144–4150\n 34. Wehnert, S., Kutty, L., & Luca, E.W.D. (2022). Using textbook knowledge for statute retrieval \nand entailment classification. In: Sixteenth international workshop on Juris-informatics \n(JURISIN)\n 35. Williams, A., Nangia, N., & Bowman, S.R. (2017). A broad-coverage challenge corpus for sentence \nunderstanding through inference. arXiv preprint arXiv: 1704. 05426\n 36. Yoshioka, M., Suzuki, Y., & Aoki, Y. (2022). Hukb at the coliee 2022 statute law task. In: Sixteenth \nInternational Workshop on Juris-informatics (JURISIN)\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\n121\n1 3The Review of Socionetwork Strategies (2024) 18:101–121 \nAuthors and Affiliations\nMi‑Young Kim1  · Juliano Rabelo2 · Housam Khalifa Bashier Babiker3 · \nMd Abed Rahman3 · Randy Goebel4\n * Mi-Young Kim \n miyoung2@ualberta.ca\n Juliano Rabelo \n rabelo@ualberta.ca\n Housam Khalifa Bashier Babiker \n khalifab@ualberta.ca\n Md Abed Rahman \n mdabed@ualberta.ca\n Randy Goebel \n rgoebel@ualberta.ca\n1 Department of Science, Augustana Faculty, University of Alberta, Camrose, Alberta, Canada\n2 Alberta Machine Intelligence Institute, University of Alberta, Edmonton, Alberta, Canada\n3 Department of Computing Science, University of Alberta, Edmonton, Alberta, Canada\n4 Department of Computing Science and Alberta Machine Intelligence Institute, University \nof Alberta, Edmonton, Alberta, Canada",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5125467777252197
    },
    {
      "name": "Transformer",
      "score": 0.48264437913894653
    },
    {
      "name": "Textual entailment",
      "score": 0.47403860092163086
    },
    {
      "name": "Information retrieval",
      "score": 0.463363379240036
    },
    {
      "name": "Logical consequence",
      "score": 0.4598037302494049
    },
    {
      "name": "Natural language processing",
      "score": 0.4172826111316681
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33305585384368896
    },
    {
      "name": "Physics",
      "score": 0.2479986548423767
    },
    {
      "name": "Voltage",
      "score": 0.10782462358474731
    },
    {
      "name": "Quantum mechanics",
      "score": 0.10025551915168762
    }
  ]
}