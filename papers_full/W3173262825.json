{
  "title": "Hand-Model-Aware Sign Language Recognition",
  "url": "https://openalex.org/W3173262825",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2141149823",
      "name": "Hezhen Hu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2138152659",
      "name": "Wengang Zhou",
      "affiliations": [
        "University of Science and Technology of China",
        "National Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2118242721",
      "name": "Houqiang Li",
      "affiliations": [
        "National Science Center",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2141149823",
      "name": "Hezhen Hu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2138152659",
      "name": "Wengang Zhou",
      "affiliations": [
        "Institute of Art",
        "University of Science and Technology of China",
        "National Science Center"
      ]
    },
    {
      "id": "https://openalex.org/A2118242721",
      "name": "Houqiang Li",
      "affiliations": [
        "National Science Center",
        "Institute of Art",
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3045327253",
    "https://openalex.org/W2150457612",
    "https://openalex.org/W2100526149",
    "https://openalex.org/W2972662547",
    "https://openalex.org/W2759302818",
    "https://openalex.org/W2899936784",
    "https://openalex.org/W2903831537",
    "https://openalex.org/W2619082050",
    "https://openalex.org/W6753057791",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2152171567",
    "https://openalex.org/W2908497602",
    "https://openalex.org/W2161604086",
    "https://openalex.org/W1950788856",
    "https://openalex.org/W1651836687",
    "https://openalex.org/W6756911974",
    "https://openalex.org/W6760630016",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W6967137799",
    "https://openalex.org/W2891726870",
    "https://openalex.org/W2786003859",
    "https://openalex.org/W2903314716",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W3152253924",
    "https://openalex.org/W2941870244",
    "https://openalex.org/W2188882108",
    "https://openalex.org/W2895638065",
    "https://openalex.org/W6655288279",
    "https://openalex.org/W4385490076",
    "https://openalex.org/W2797947747",
    "https://openalex.org/W3009347328",
    "https://openalex.org/W2981436931",
    "https://openalex.org/W2901751978",
    "https://openalex.org/W6854993605",
    "https://openalex.org/W2044235398",
    "https://openalex.org/W6648245117",
    "https://openalex.org/W2761659801",
    "https://openalex.org/W2948048211",
    "https://openalex.org/W6910365999",
    "https://openalex.org/W2948058585",
    "https://openalex.org/W2609211631",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W2007104354",
    "https://openalex.org/W2146221819",
    "https://openalex.org/W2004074725",
    "https://openalex.org/W2552247836",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2227547437",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W2989674076",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W2784435047",
    "https://openalex.org/W2342890612",
    "https://openalex.org/W2925846278",
    "https://openalex.org/W2997931247",
    "https://openalex.org/W2307035320",
    "https://openalex.org/W2964253156",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W3034999503",
    "https://openalex.org/W2952587893",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W3108425892",
    "https://openalex.org/W3081334315",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4230013879",
    "https://openalex.org/W2963369114",
    "https://openalex.org/W4385489997",
    "https://openalex.org/W2798581336",
    "https://openalex.org/W3034269985",
    "https://openalex.org/W2963488642",
    "https://openalex.org/W2979577579",
    "https://openalex.org/W2964093990",
    "https://openalex.org/W2883534172",
    "https://openalex.org/W1990947293",
    "https://openalex.org/W3009828227",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2020163092",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962730651"
  ],
  "abstract": "Hand gestures play a dominant role in the expression of sign language. Current deep-learning based video sign language recognition (SLR) methods usually follow a data-driven paradigm under the supervision of the category label. However, those methods suffer limited interpretability and may encounter the overfitting issue due to limited sign data sources. In this paper, we introduce the hand prior and propose a new hand-model-aware framework for isolated SLR with the modeling hand as the intermediate representation. We first transform the cropped hand sequence into the latent semantic feature. Then the hand model introduces the hand prior and provides a mapping from the semantic feature to the compact hand pose representation. Finally, the inference module enhances the spatio-temporal pose representation and performs the final recognition. Due to the lack of annotation on the hand pose under current sign language datasets, we further guide its learning by utilizing multiple weakly-supervised losses to constrain its spatial and temporal consistency. To validate the effectiveness of our method, we perform extensive experiments on four benchmark datasets, including NMFs-CSL, SLR500, MSASL and WLASL. Experimental results demonstrate that our method achieves state-of-the-art performance on all four popular benchmarks with a notable margin.",
  "full_text": "Hand-Model-Aware Sign Language Recognition\nHezhen Hu,1 Wengang Zhou,1, 2 Houqiang Li1, 2\n1 CAS Key Laboratory of GIPAS, EEIS Department, University of Science and Technology of China\n2 Institute of ArtiÔ¨Åcial Intelligence, Hefei Comprehensive National Science Center\nalexhu@mail.ustc.edu.cn, fzhwg, lihqg@ustc.edu.cn\nAbstract\nHand gestures play a dominant role in the expression of\nsign language. Current deep-learning based video sign lan-\nguage recognition (SLR) methods usually follow a data-\ndriven paradigm under the supervision of the category label.\nHowever, those methods suffer limited interpretability and\nmay encounter the overÔ¨Åtting issue due to limited sign data\nsources. In this paper, we introduce the hand prior and pro-\npose a new hand-model-aware framework for isolated SLR\nwith the modeling hand as the intermediate representation.\nWe Ô¨Årst transform the cropped hand sequence into the latent\nsemantic feature. Then the hand model introduces the hand\nprior and provides a mapping from the semantic feature to\nthe compact hand pose representation. Finally, the inference\nmodule enhances the spatio-temporal pose representation and\nperforms the Ô¨Ånal recognition. Due to the lack of annota-\ntion on the hand pose under current sign language datasets,\nwe further guide its learning by utilizing multiple weakly-\nsupervised losses to constrain its spatial and temporal con-\nsistency. To validate the effectiveness of our method, we per-\nform extensive experiments on four benchmark datasets, in-\ncluding NMFs-CSL, SLR500, MSASL and WLASL. Exper-\nimental results demonstrate that our method achieves state-\nof-the-art performance on all four popular benchmarks with\na notable margin.\nIntroduction\nSign language, as a natural language of the deaf community,\nhas a unique linguistic characteristic. It conveys semantic\nmeaning via hands, including hand motions, shape, orienta-\ntion, etc., together with non-manual features, including fa-\ncial expressions. To facilitate the communication between\nthe deaf and the hearing people, automatic sign language\nrecognition (SLR) has been widely studied and attracted in-\ncreasing attention. It aims at mapping the sign video into the\ntext word or sentence, which corresponds to two subtasks,\ni.e., isolated SLR and continuous SLR. Isolated SLR is a\nkind of Ô¨Åne-grained classiÔ¨Åcation task and focuses on the\nrecognition at the word level, while continuous SLR tries to\nrecognize the signs in their presenting order. In this work,\nwe focus on the former task, i.e., isolated SLR.\nThe hand acts as a dominant role in sign language. As\nshown in Figure 1, it occupies a relatively small area, ex-\nCopyright ¬© 2021, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\n1\nSpa & Tem Cons.\nLabel\n‚Ä¶\nWalnut\nAppreciate\n‚Ä¶\nAir      \nPrediction\n0.01\n0.82\n0.07\nFull \nFrame\n‚Ä¶\nInference\nt + 1 t + 2 t + T\n‚Ä¶\nMesh\nHand\n3D \nJoint\n‚Ä¶\n‚Ä¶\nFigure 1: Illustration on the challenge of the hand gestures\nin sign language recognition and our idea with the modeling\nhand as the intermediate representation.\nhibiting highly articulated joints and similar appearance with\nfewer local characteristic features, when compared with the\nbody or face. During the sign, it usually encounters the\nmotion blur and self-occlusion among joints with complex\nbackgrounds. Early works adopt hand-crafted features to de-\nscribe hand gestures (Starner, Weaver, and Pentland 1998;\nBuehler, Zisserman, and Everingham 2009). Recently, many\nworks have leveraged the advance of deep convolutional\nneural networks (CNNs) (Huang et al. 2019; Albanie et al.\n2020; Koller et al. 2018; Cui, Liu, and Zhang 2019; Zhou\net al. 2020). It is worth mentioning that some methods\nhighlight the importance of hands by utilizing the cropped\nhands as the extra stream and achieve a notable performance\ngain (Camgoz et al. 2017; Huang et al. 2018; Koller et al.\n2020). These deep-learning based methods work in a data-\ndriven paradigm and learn feature representations adaptively\nunder the supervision of the video-level category label.\nHowever, direct data-driven SLR methods suffer lim-\nited interpretability for the learned hand feature and may\noverÔ¨Åt under limited training data. The limited sign data\nsources are partially attributed to the fact that there is a\nstrong requirement for expert knowledge during the man-\nual annotation. Consequently, compared with current ac-\ntion recognition datasets (Goyal et al. 2017; Carreira and\nZisserman 2017), sign language datasets, e.g., WLASL (Li\net al. 2020b), MSASL (Joze and Koller 2019) and NMFs-\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n1558\nCSL (Hu et al. 2020), usually contain much fewer samples\nper word.\nTo tackle this issue, we introduce the hand prior and pro-\npose a hand-model-aware framework for isolated SLR, with\nvisible hand meshes and poses as the intermediate represen-\ntation. The framework consists of three modules, i.e., a vi-\nsual encoder, a hand-model-aware decoder and an inference\nmodule. The visual encoder transforms the hand sequence\ninto the latent semantic feature. Then the model-aware de-\ncoder provides a mapping from the latent feature to the hand\nmesh, as well as a compact pose. SpeciÔ¨Åcally, the decoder\nis a Ô¨Åxed statistical mesh-based model, which stores the\nknowledge learned from a large variety of high-quality hand\nscans. In this way, the irrational poses can be effectively Ô¨Ål-\ntered out based on the imported hand prior. The inference\nmodule enhances the spatio-temporal representation of the\nhand pose sequence and performs recognition.\nOur approach follows a paradigm in line with the in-\nsight (Clarke and Tyler 2015) on human cognition, which\nreveals that the ventral visual pathway in the brain treats the\nrecognition process as a dynamic process of transformation\nfrom low-level visual input to speciÔ¨Åc conceptual knowl-\nedge representations. Due to the lack of hand-joint annota-\ntions in current sign datasets, we further focus on the spatial\nand temporal context of the pose representation, and design\nseveral weakly-supervised losses to guide its learning.\nTo our best knowledge, it is the Ô¨Årst hand-model-aware\nframework for sign language recognition. Extensive ex-\nperiments on four benchmark datasets, i.e., NMFs-CSL,\nSLR500, MSASL and WLASL, validate the effectiveness\nof our method, achieving new state-of-the-art performance\non all these datasets.\nRelated Work\nIn this section, we brieÔ¨Çy review the related topics, including\nsign language recognition, hand pose estimation and hand\nmodels used for reconstruction.\nSign Language Recognition\nSign language recognition methods can be divided into two\ngroups based on the input modality, i.e., RGB-based (using\nthe RGB video as input) and pose-based (using the skeleton\nsequence as input) methods.\nRGB-based methods. Early methods rely on hand-\ncrafted features, such as HOG, SIFT, motion trajectories, for\nhand representation (Buehler, Zisserman, and Everingham\n2009; Koller, Forster, and Ney 2015; Yasir et al. 2015; Evan-\ngelidis, Singh, and Horaud 2014). Recently, deep convolu-\ntional neural networks (CNNs) have shown a high capacity\nfor representation learning and been widely used in many\ncomputer vision tasks. Many researchers have explored the\ndesign of networks for video representation,e.g., 2D-CNNs,\n3D-CNNs or mixture of them (Carreira and Zisserman 2017;\nChen et al. 2018; Qiu, Yao, and Mei 2017; Qiu et al. 2019;\nSimonyan and Zisserman 2014; Wang et al. 2016; Xie et al.\n2018). For the task of sign language recognition, Koller et\nal. adopt 2D-CNNs for spatial representation, followed by\nHMM to model temporal dependencies (Koller et al. 2018).\nSome other works utilize 3D-CNNs for spatio-temporal rep-\nresentation modeling (Huang et al. 2019; Joze and Koller\n2019; Li et al. 2020b,a; Albanie et al. 2020).\nPose-based methods. Besides the above mentioned\nRGB-based methods, many works study the pose-based\nmethods. Pose is a type of well-structured data, a high-level\nsemantic representation with a low dimension, which also\nenables the computation efÔ¨Åciency. Recurrent neural net-\nworks, e.g., GRU (Cho et al. 2014) and LSTM (Hochre-\niter and Schmidhuber 1997), have been used to model the\ntemporal information of the keypoint sequence (Du, Wang,\nand Wang 2015; Song et al. 2017; Zhu et al. 2016). Some\nCNN-based works attempt to transform the input keypoint\nsequence into the feature map and use the popular CNNs to\ncapture spatio-temporal dynamics (Li et al. 2018; Cao et al.\n2018). Considering the well-structured characteristic of the\npose, more and more works adopt graph convolutional net-\nworks (GCNs) (Yan, Xiong, and Lin 2018; Shi et al. 2019;\nZhang et al. 2020). Yan et al. (Yan, Xiong, and Lin 2018)\nmake the Ô¨Årst attempt to propose a spatial-temporal GCN\nfor action recognition. SpeciÔ¨Åcally, it builds a graph with\nnodes and edges pre-deÔ¨Åned by human keypoints and their\nphysical connections, respectively. These GCN-based meth-\nods are able to process pose data more efÔ¨Åciently and show\npromising results.\nHand Pose Estimation\nThere have been several works predicting hand poses from\nthe RGB images. The 2D hand pose estimation has been\ngreatly improved by multiview bootstrapping (Simon et al.\n2017). Further improvement is achieved on the inference\nspeed (Wang, Zhang, and Peng 2019). There also exist\nsome works estimating 3D pose representations, e.g., esti-\nmating 3D poses from 2D counterparts (Cai et al. 2019),\nconstraining intermediate reconstructed depth (Iqbal et al.\n2018), etc. Recent works learn 3D hand shape and pose\njointly (Boukhayma, Bem, and Torr 2019; Ge et al. 2019;\nZhang et al. 2019). These methods are all trained under\nthe supervision of the hand-joint annotations and focus on\nthe precise predictions of the joint positions. Different from\nthem, our proposed recognition framework utilizes the hand\nposes as the intermediate representation and learn them\nwithout hand-joint annotations.\nHand Model Learning\nTo model the hand, many works have been proposed us-\ning various techniques, including shape primitives (Oikono-\nmidis, Lourakis, and Argyros 2014; Qian et al. 2014), sum-\nof-Gaussians (Sridhar, Oulasvirta, and Theobalt 2013) and\na more generalized sphere-meshes method (Tkach, Pauly,\nand Tagliasacchi 2016). To model the hand shape more pre-\ncisely, some works (Ballan et al. 2012; Tzionas et al. 2016)\npropose to adopt a triangulated mesh with Linear Blend\nSkinning (LBS) (Lewis, Cordner, and Fong 2000). Da La\nGorce et al. (de La Gorce, Fleet, and Paragios 2011) fur-\nther introduce the scaling terms for each bone to change\nhand shape. MANO (Romero, Tzionas, and Black 2017) is\nthe most popular fully-differentiable statistical model, which\nlearns from a large variety of hand scans. It deforms the\n1559\n2\nModel-aware\nDecoder\nVisual\nEncoder\nInference\nModule\nWalnut\nAppreciate \n‚Ä¶\nAir      \nPrediction\n0.01\n0.82\n0.07\n3D Mesh\n2D Hand Pose \nDetector\nSpatial \nConsistency Loss\nTemporal \nConsistency Loss\nCamera Projection\n‚Ä¶\n‚Ä¶\nRegularization \nLoss\nOff-Line\n3D Joint\n‚Ä¶\n2D Joints\nTesting Stage\nTraining Stage\nClassification\nLoss\nùúÉùúÉ\nùõΩùõΩ\nùëêùëê\nFigure 2: Overview of our proposed framework. The framework consists of a visual encoder, a hand-model-aware decoder\nand an inference module. Jointly with the video-level supervision, we further constrain the spatial and temporal consistency of\nintermediate 3D pose representations for further performance improvement. The modules utilized in training and testing stages\nare highlighted in light blue and orange, respectively.\nmean mesh and factors the geometric changes into the shape\nand pose. In this work, we adopt MANO hand model into\nour framework to import the hand prior.\nOur Approach\nIn this section, we Ô¨Årst give a brief overview of our frame-\nwork. Then we elaborate each component of our framework\nand the optimization objective functions of the framework.\nOverview\nAs shown in Figure 2, given a cropped RGB hand sequence,\nthe visual encoder Ô¨Årst transforms it into the latent seman-\ntic embedding and predicts the camera parameters. Then\nthe decoder works in model-aware and provides the map-\nping from the latent semantic feature to the reÔ¨Åned 3D hand\nmesh and pose. The compact 3D pose representation is fed\ninto the lightweight inference module. It enhances the rep-\nresentation of each joint and performs the Ô¨Ånal classiÔ¨Åca-\ntion. The framework is optimized with a video-level cross-\nentropy loss, together with several weakly-supervised loss\nterms based on the spatial and temporal relationships of the\nintermediate poses.\nFramework Design\nThe framework contains three key modules,i.e., a visual en-\ncoder, a hand-model-aware decoder and an inference mod-\nule. We will discuss these modules in the following.\nVisual encoder. Given a RGB hand sequence V =\nfvtgT\nt=1 with T frames from a sign video, the visual encoder\nE(\u0001) transforms the RGB hand sequence into the latent se-\nmantic feature describing the hand status and the camera pa-\nrameters, which is formulated as follows,\nFla = f\u0012;\f;cr;co;csgT\nt=1 = E(V); (1)\nwhere \u0012 2R6 and \f 2R10 are the pose and shape embed-\nding for the following decoder, while cr 2R3\u00023, co 2R2,\nand cs 2R are the camera parameters, indicating the rota-\ntion, translation and scale, respectively. In our implementa-\ntion, the encoder contains a ResNet34 (without the classi-\nÔ¨Åer) (He et al. 2016) to generate the high-dimensional fea-\nture, followed by a fully-connected layer to derive the low-\ndimensional semantic feature.\nHand-model-aware decoder. This module attempts to\nderive a compact pose representation from the latent seman-\ntic embeddings with a hand-model-aware method. With the\nencoded hand prior, the decoder constrains the distribution\nof possible poses and implicitly Ô¨Ålters out the irrational pre-\ndicted poses during its mapping. Finally, it produces a more\ncompact and reliable hand pose, which will alleviate the op-\ntimization difÔ¨Åculty of the following inference module.\nIn this work, we utilize the fully differentiable MANO\nhand model (Romero, Tzionas, and Black 2017) as the de-\ncoder. MANO is a statistical model similar to the SMPL\nmodel (Loper et al. 2015), which is learned from a large va-\nriety of high-quality registered hand scans. In this way, the\nhand prior is encoded and a compact mapping can be estab-\nlished to describe the hand, i.e., from the low-dimensional\nsemantic embedding to the triangulated hand mesh M 2\nRN\u00023 of N=778 vertices and 1538 faces. More precisely,\nto generate a physically plausible mesh, the input pose and\nshape represent the coefÔ¨Åcients of PCA components calcu-\nlated from the collected hand scan data. The model is for-\nmulated as follows,\nM(\f;\u0012) = W(T(\f;\u0012);J(\f);\u0012;W); (2)\nT(\f;\u0012) = \u0016T + BS(\f) + BP(\u0012); (3)\nwhere BS(\u0001) and BP(\u0001) are blend functions, and W is a set\nof blend weights. The hand template \u0016T is posed and skinned\nwith the pose and shape corrective blend shapes,i.e., BP(\u0012)\nand BS(\f). Further, the Ô¨Ånal mesh is generated by rotating\neach part around joints J(\f) using the linear skinning func-\ntion W(\u0001) (Kavan and ÀáZ¬¥ara 2005).\n1560\nWith the hand model, the 3D joint locations eJ3D, as a\nmore compact representation, can also be derived by the lin-\near interpolation of relevant vertices in the mesh. It is no-\ntable that the original MANO model only provides 16 hand\nkeypoints. To keep consistent with the 2D keypoints directly\ndetected in the image plane, we select 5 extra vertices from\nthe mesh with the index of 734, 333, 443, 555, 678 and add\nthem as the Ô¨Ångertips. As a result, the hand is represented\nwith 21 3D joints.\nInference module.The predicted pose sequence from the\ndecoder may contain some unsatisfactory results. The in-\nference module is utilized to reÔ¨Åne its spatio-temporal rep-\nresentation. With the further calculation of adaptive atten-\ntion, the inference module captures informative cues and\nperforms the video-level classiÔ¨Åcation.\nThe hand pose sequence is a well-structured data with the\nphysical connections between joints, which makes it nat-\nurally to be organized as a spatio-temporal graph. In this\nwork, we adopt a popular GCN (Yan, Xiong, and Lin 2018),\nwhich has proven effective to process pose data. Given a\nhand pose sequence eJ3D representing 3D locations (x, y,\nz coordinates) of each joint in each frame, an undirected\nspatio-temporal graph G(V;E) is Ô¨Årst deÔ¨Åned by V and E\nas the node and edge set, respectively. The node set V con-\ntains all the corresponding hand joints, while the edge set E\nincludes the intra-frame and inter-frame set, i.e., the phys-\nical connection of hand joints and connection of the same\njoint along the time, respectively. The adjacency matrix eA\nderived from the deÔ¨Åned edge set will be adopted in GCN\nwith the identity matrix I. The graph convolution is formu-\nlated as follows,\nZ =\nX\nk\nD\n\u00001\n2\nk (Ak \u000eM)D\n1\n2\nk eJ3DWk; (4)\nwhere Z is the output feature, k is the index of neighbour\ntypes (for each node, its neighbouring nodes are divided\ninto several types), Wk is the convolution weight, eA + I\nis dismantled into k sub-matrices, i.e., eA + I = P\nkAk,\nTk = Ak \u000eM and Dii\nk = P\njTij\nk . The message is trans-\nferred among edges to enhance the representation of each\njoint. Further, the Hadamard product is performed between\nthe learnable attention weight M initialized as all-one ma-\ntrix and Ak to capture the discriminative cues. With several\nstacked GCN layers, a global pooling is adopted to merge\nthe information contained in the enhanced node features,\nwhich is followed by a fully-connected layer to perform the\nÔ¨Ånal recognition.\nObjective Function & Inference\nSince current sign language datasets have no annotation on\nthe hand pose, besides the cross-entropy classiÔ¨Åcation loss\nLcla, we elaborately design several loss terms to guide the\nlearning of intermediate pose representations.\nSpatial consistency loss.First, we utilize the consistency\nbetween our predicted 3D and pre-extracted 2D joints from\nOpenPose (Cao et al. 2019; Simon et al. 2017). SpeciÔ¨Åcally,\nwe Ô¨Årst project the predicted 3D joints to its 2D counterparts\nbased on the weak-perspective camera model. The projec-\ntion process can be formulated as follows,\neJ2D = cs\nY\n(cr eJ3D) + co; (5)\nwhere Q(\u0001) denotes the orthographic projection. Then we\nutilize the pre-extracted 2D hand joints J2D as the pseudo\nlabel, and constrain the consistency between our projected\none eJ2D and J2D. The spatial consistency loss is then calcu-\nlated as follows,\nLspa =\nTX\nt=1\n21X\nj=1\n1(c(t;j) >= \u000f)\n\r\r\reJ2D(t;j) \u0000J2D(t;j)\n\r\r\r\n1\n;\n(6)\nwhere 1(\u0001) denotes the indicator function, and c(t;j) de-\nnotes the conÔ¨Ådence of the pre-extracted J2D with the joint\njat time t. To align the 2D hand joints predicted by different\nmethods, we utilize the root-relative representation for these\njoints, i.e., the root joint (palm) is set as the origin. It is no-\ntable that the joints in J2D with the conÔ¨Ådence c(t;j) lower\nthan the threshold \u000fwill be ignored.\nTemporal consistency loss.To avoid the jittering predic-\ntions, we further enforce the temporal consistency on the 3D\nhand pose. Different hand joints usually have different mov-\ning speeds during the sign,e.g., joints closer to the palm usu-\nally have a lower speed. Thus we manually divide the hand\njoints into three groups, fSiji = 0;1;2g, i.e., palm, middle\nand terminal joints, respectively. The temporal consistency\nloss is implemented by a derivative regularization, which is\nformulated as follows,\nLtem =\nX\ni\nX\nj2Si\nTX\nt=2\n\u000bi\n\r\r\n\reJ3D(t;j) \u0000eJ3D(t\u00001;j)\n\r\n\r\n\r\n2\n2\n; (7)\nwhere \u000bi denotes the pre-deÔ¨Åned weight for Si and we pe-\nnalize more for the group having the lower speed.\nRegularization loss.To ensure the hand model work in a\nproper way and generate the hand mesh plausibly, the regu-\nlarization loss is added by constraining the magnitude of the\npartially latent feature, which is deÔ¨Åned as follows,\nLreg = k\u0012k2\n2 + w\fk\fk2\n2; (8)\nwhere w\f denotes the weighting factor.\nThe Ô¨Ånal objective loss function is deÔ¨Åned as follows,\nL= Lcla + \u0015spaLspa + \u0015temLtem + \u0015regLreg; (9)\nwhere \u0015spa;\u0015tem and \u0015reg denote the weighting factor for\nspatial, temporal consistency loss and regularization loss, re-\nspectively. During training, the above loss function is uti-\nlized to optimize the full framework. Notably, both hands\nare involved and fused for the Ô¨Ånal recognition.\nInference. Considering only the cropped hands are insuf-\nÔ¨Åcient to convey the full meaning of sign language, it is nec-\nessary to fuse recognition results based on hands with that on\nthe full frame, which can be represented by either full key-\npoints or full RGB data. To this end, we use the results based\non hand modeling, full keypoints and full RGB data. Those\nresults can be assembled with late fusion by directly sum-\nming their prediction results (Karpathy et al. 2014). Specif-\nically, for the recognition with the full keypoints, we utilize\n1561\nST-GCN as the backbone and all the 137 2D joints as in-\nput, while for the full RGB input, we sample a Ô¨Åxed num-\nber of frames and use a common CNN, e.g., 3D-ResNet50,\nas the classiÔ¨Åer. In the following, we refer our method with\nonly hands, fusion of hands and the full keypoints, fusion\nof hands and the full RGB as Ours (Hand),Ours (Hand +\nPose) and Ours (Hand + RGB), respectively.\nExperiments\nDatasets and Evaluation\nDatasets. We evaluate our proposed method on four pub-\nlicly available datasets, including NMFs-CSL (Hu et al.\n2020), SLR500 (Huang et al. 2019), MSASL (Joze and\nKoller 2019) and WLASL (Li et al. 2020b).\nNMFs-CSL is the most challenging Chinese sign lan-\nguage (CSL) dataset due to a large variety of confusing\nwords caused by Ô¨Åne-grained cues. It contains a total of\n1,067 words with 610 confusing words and 457 normal\nwords. This dataset is recorded by a RGB camera at 30 FPS\nwith a resolution of 1280 \u0002720. SpeciÔ¨Åcally, 25,608 and\n6,402 samples are used for training and testing, respectively.\nSLR500 is another CSL dataset, which contains 500 daily\nwords with 12,5000 recording samples performed by 50\nsigners. It is recorded by Kinect and provides RGB and\ndepth modalities. There are 90,000 and 35,000 samples for\ntraining and testing, respectively.\nMSASL is an American sign language dataset (ASL) with\na vocabulary size of 1,000. It is collected from Web videos.\nIt contains 25,513 samples in total with 16,054, 5,287 and\n4,172 for training, validation and testing, respectively. Be-\nsides, in this dataset, the top-100 and top-200 most frequent\nwords are selected as two subsets for training and testing,\nreferred to as MSASL100 and MSASL200.\nWLASL is an ASL dataset similar to MSASL, which is\nalso collected from the Web. The size of the vocabulary is\n2,000, and there are 21,083 samples divided into the train-\ning, validation and testing splits. MSASL and WLASL both\nbring new challenges due to the unconstrained recording\nconditions and limited samples for each word.\nNotably, all these datasets adopt the signer-independent\nsetting, i.e., signers in the training set will not occur during\ntesting. Besides, all the benchmark datasets only have cate-\ngory labels without any annotations on hand poses.\nEvaluation. We evaluate the datasets using the accuracy\nmetrics, including the per-instance and per-class metrics, de-\nnoting the average accuracy over each instance and each\nclass, respectively. Since NMFs-CSL and SLR500 datasets\nhave the same number of samples for each class, we only\nreport the per-instance accuracy. Following the original set-\ntings in their corresponding works (Hu et al. 2020; Huang\net al. 2019), we report top-1, top-2, top-5 accuracy for\nNMFs-CSL, and top-1 accuracy for SLR500. For MSASL\nand WLASL, we report the top-1 and top-5 accuracy under\nboth per-instance and per-class metrics.\nImplementation Details\nIn our experiment, all the models are implemented in Py-\nTorch (Paszke et al. 2019) platform and trained on NVIDIA\nCla. Reg. Spa. Tem. Top-1 Top-2 Top-5\nX 61.5 80.3 90.8\nX X 62.0 78.8 88.9\nX X X 64.0 81.6 90.7\nX X X X 64.7 81.8 91.0\nTable 1: Ablation studies on the effect of each loss term\non NMFs-CSL dataset. Cla., Reg., Spa. and Tem. denote\nthe classiÔ¨Åcation, regularization, spatial and temporal con-\nsistency loss, respectively.\nHand Full frame Accuracy\nOP Ours Keypoints RGB Top-1 Top-2 Top-5\nX 54.6 72.2 85.2\nX 64.7 81.8 91.0\nX 59.9 71.3 83.7\nX X 67.3 83.0 93.0\nX X 71.7 88.6 95.7\nX 62.1 73.2 83.7\nX X 71.7 84.3 92.3\nX X 75.6 88.4 95.3\nTable 2: Experimental results based on the hand model-\ning, full keypoints and full RGB data. For the hand-based\nmethod, we compare the results between our generated 3D\nhand pose and the 2D OpenPose-detected one (OP), which\nis utilized as the pseudo label in our framework.\nRTX-TITAN. Temporally, we extract 32 frames using ran-\ndom and center sampling during training and testing, re-\nspectively. During training, the input frames are randomly\ncropped to 256 \u0002256 at the same spatial position. Then the\nframes are randomly horizontally Ô¨Çipped with a probability\nof 0.5. During testing, the input video is center cropped to\n256 \u0002256 and fed into the model. The model is trained with\nStochastic Gradient Descent (SGD) optimizer. The weight\ndecay and momentum are set to 1e-4 and 0.9, respectively.\nWe set the initial learning rate as 5e-3 and reduce it by a fac-\ntor of 0.1 when the validation loss is saturated. In all exper-\niments, the hyper parameters \u000f, w\f, \u0015spa, \u0015tem, \u0015reg, \u000b0,\n\u000b1 and \u000b2 is set to 0.4, 10, 0.1, 0.1, 0.1, 1, 2.5 and 4, re-\nspectively. We use OpenPose (Cao et al. 2019; Simon et al.\n2017) to extract the full keypoints, i.e., the 137 2D joints\nof body, face and hands. The extracted hand and shoulder\nkeypoints are further utilized to crop the hand from the full\nframe. Besides, for the training of the RGB and pose base-\nline, we follow the original settings in their works (Carreira\nand Zisserman 2017; Yan, Xiong, and Lin 2018).\nAblation Study\nWe perform ablation studies on the effectiveness of loss\nterms and the complementary effect of our method.\nEffectiveness of loss terms.From Table 1, it can be ob-\nserved that the top-1 accuracy is improved gradually by\nadding each loss term. Although the regularization loss\nbrings relatively less improvement, it is crucial for the hand\nmodel to generate plausible meshes. It is notable that con-\nsistency losses contribute a lot to boosting the performance.\n1562\n3\nRGB\n2D \nJoint\n3D \nMesh\nSLR500NMFs-CSL MSASL WLASL\nFigure 3: Visualization of the intermediate mesh representation. From the Ô¨Årst to the third row, we present the RGB hand,\n2D joint detected by OpenPose and the 3D mesh generated by our method. We visualize one sample in the test set for each\nbenchmark dataset, including NMFs-CSL, SLR500, MSASL ans WLASL. For each sample, we visualize two key frames.\nMethod Total Confusing Normal\nTop-1 Top-2 Top-5 Top-1 Top-2 Top-5 Top-1 Top-2 Top-5\nST-GCN (Yan, Xiong, and Lin 2018) 59.9 74.7 86.8 42.2 62.3 79.4 83.4 91.3 96.7\n3D-R50 (Qiu, Yao, and Mei 2017) 62.1 73.2 82.9 43.1 57.9 72.4 87.4 93.4 97.0\nDNF (Cui, Liu, and Zhang 2019) 55.8 69.5 82.4 33.1 51.9 71.4 86.3 93.1 97.0\nI3D (Carreira and Zisserman 2017) 64.4 77.9 88.0 47.3 65.7 81.8 87.1 94.3 97.3\nTSM (Lin, Gan, and Han 2019) 64.5 79.5 88.7 42.9 66.0 81.0 93.3 97.5 99.0\nSlowfast (Feichtenhofer et al. 2019) 66.3 77.8 86.6 47.0 63.7 77.4 92.0 96.7 98.9\nGLE-Net (Hu et al. 2020) 69.0 79.9 88.1 50.6 66.7 79.6 93.6 97.6 99.3\nOurs (Hand) 64.7 81.8 91.0 42.3 69.4 84.8 94.6 98.4 99.3\nOurs (Hand + Pose) 71.7 88.6 95.7 54.2 81.2 92.8 95.0 98.5 99.5\nOurs (Hand + RGB) 75.6 88.4 95.3 59.7 80.2 91.8 96.9 99.4 99.9\nTable 3: Accuracy comparison on NMFs-CSL dataset.\nThe spatial consistency loss brings the largest accuracy gain,\ni.e., from 62.0% to 64.0% top-1 accuracy. With the tempo-\nral consistency loss further added, the top-1 accuracy is im-\nproved to 64.7%. All the above results demonstrate the ef-\nfectiveness of the proposed loss terms.\nComplementarity between hand and full frame.The\nÔ¨Årst part in Table 2 shows the classiÔ¨Åcation results using\nhand keypoints as input based on the ST-GCN backbone.\nThe Ô¨Årst row denotes using the 2D hand keypoints detected\nby OpenPose, while the second row denotes our generated\n3D ones. It can be observed that the accuracy using 3D hand\nkeypoints as input largely outperforms that using 2D ones.\nAs indicated in Table 2, the top-1 accuracy increases from\n59.9% to 71.7% when fusing recognition results of our hand\njoints and full keypoints. In contrast, when combined with\nthe full-RGB based method, the accuracy improvement is\n13.5%, which is larger than that combined with the full-\nkeypoints based method.\nFurther, we also perform the qualitative visualization on\nthe reconstructed hand mesh in Figure 3. The mesh also im-\nproves the interpretability of the whole framework. It can be\nobserved that the video samples from different datasets vary\na lot in their backgrounds and signer‚Äôs clothing. The detec-\ntion of 2D hand joints usually fails when the motion blur\nor self-occlusion occurs. In contrast, with the hand prior en-\ncoded, the generated mesh has more stability with all the\nÔ¨Ångers occurring and mostly reproduces the hand motion. It\nsomewhat deals with some hard situations,e.g., motion blur,\nmutually occurring of the hand and face, and self-occlusion.\nComparison with State-of-the-art Methods\nWe perform extensive experiments and compare with state-\nof-the-art methods on four benchmark datasets, i.e., NMFs-\nCSL, SLR500, MSASL and WLASL.\nEvaluation on NMFs-CSL. As shown in Table 3, the\nÔ¨Årst two rows represent the baseline methods. DNF (Cui,\nLiu, and Zhang 2019) is a state-of-the-art method in con-\ntinuous SLR and we utilize its visual encoder followed by\na fully-connected layer as the backbone for comparison.\nGLE-Net (Hu et al. 2020) enhances discriminative cues from\nglobal and local views and achieves state-of-the-art perfor-\nmance. Compared with these competitors, our method (only\ncropped hands) achieves comparable performance with a\nmajority of them. Our method ((Hand + Pose), (Hand +\n1563\nMethod\nMSASL100 MSASL200 MSASL1000\nPer-instance Per-class Per-instance Per-class Per-instance Per-class\nTop-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5 Top-1 Top-5\n(Yan, Xiong, and Lin 2018) 59.84 82.03 60.79 82.96 52.91 76.67 54.20 77.62 36.03 59.92 32.32 57.15\n(Joze and Koller 2019)1 - - 81.76 95.16 - - 81.97 93.79 - - 57.69 81.05\n(Li et al. 2020a) 83.04 93.46 83.91 93.52 80.31 91.82 81.14 92.24 - - - -\n(Albanie et al. 2020) - - - - - - - - 64.71 85.59 61.55 84.43\nOurs (Hand) 73.45 89.70 74.59 89.70 66.30 84.03 67.47 84.03 49.16 69.75 46.27 68.60\nOurs (Hand + Pose) 78.57 91.41 79.48 91.62 72.19 88.15 73.52 88.46 56.02 76.51 52.98 74.90\nOurs (Hand + RGB) 87.45 96.30 88.14 96.53 85.21 94.41 86.09 94.42 69.39 87.42 66.54 86.56\n1 (Joze and Koller 2019) denotes the RGB baseline.\nTable 4: Accuracy comparison on MSASL dataset.\nMethod Accuracy\nSTIP (Laptev 2005) 61.8\nGMM-HMM (Tang et al. 2015) 56.3\nC3D (Tran et al. 2015) 74.7\nAtten (Huang et al. 2019) 88.7\nST-GCN (Yan, Xiong, and Lin 2018) 90.0\n3D-R50 (Qiu, Yao, and Mei 2017) 95.1\nGLE-Net (Hu et al. 2020) 96.8\nOurs (Hand) 95.9\nOurs (Hand + Pose) 97.5\nOurs (Hand + RGB) 98.3\nTable 5: Accuracy comparison on SLR500 dataset.\nRGB)) outperforms the most challenging competitor GLE-\nNet, i.e., 2.7% and 6.6% top-1 accuracy gain, respectively.\nEvaluation on SLR500. As illustrated in Table 5,\nSTIP (Laptev 2005) and GMM-HMM (Tang et al. 2015)\ndenote the methods based on the hand-crafted features. At-\nten (Huang et al. 2019) utilizes multiple data modalities as\ninput, including RGB, optical Ô¨Çow, depth, etc. The afore-\nmentioned GLE-Net (Hu et al. 2020) still achieves the best\nperformance on this dataset. Even compared with GLE-Net,\nour method still achieves comparable performance. For our\nmethod ((Hand + Pose), (Hand + RGB)), the top-1 accuracy\nreaches 97.5% and 98.3%, which is new state-of-the-art per-\nformance on this dataset.\nEvaluation on MSASL.MSASL contains limited sam-\nples for each word. The samples vary a lot in the resolu-\ntion and unconstrained backgrounds, which makes MSASL\nmore challenging. As shown in Table 4, we also release ST-\nGCN method as the pose baseline (Yan, Xiong, and Lin\n2018). Compared with the RGB baseline, it shows infe-\nrior performance under both per-instance and per-class ac-\ncuracy metrics. It may be caused by the failure of the pose\ndetection, due to the partially occluded upper body of the\nsigner, low-quality video, and noisy backgrounds. Albanie\net al. (Albanie et al. 2020) and Li et al. (Li et al. 2020a)\nboth use external sign videos to boost the performance and\nachieve state-of-the-art performance on MSASL or its sub-\nset, respectively. It is worth mentioning that our method\noutperforms the most challenging competitor by a notable\nmargin, i.e., 4.41%, 4.90% and 4.68% per-instance top-\nMethod Per-instance Per-class\nTop-1 Top-5 Top-1 Top-5\n(Yan, Xiong, and Lin 2018) 34.40 66.57 32.53 65.45\n(Li et al. 2020b)1 32.48 57.31 - -\n(Albanie et al. 2020) 46.82 79.36 44.72 78.47\nOurs (Hand) 37.91 71.26 35.90 70.00\nOurs (Hand + Pose) 46.32 81.90 44.09 81.08\nOurs (Hand + RGB) 51.39 86.34 48.75 85.74\n1 (Li et al. 2020b) denotes the RGB baseline.\nTable 6: Accuracy comparison on WLASL dataset.\n1 accuracy improvement on MSASL100, MSASL200 and\nMSASL1000 dataset, respectively. Besides, the complemen-\ntary effects of our method are also validated on this dataset.\nEvaluation on WLASL. Compared with MSASL\ndataset, WLASL has a vocabulary with doubled size but\nfewer samples. As shown in Table 6, when fused with\nthe RGB baseline, our method achieves 51.39% top-1 per-\ninstance accuracy, which brings 18.91% top-1 per-instance\naccuracy improvement over the RGB baseline. It also val-\nidates the effectiveness of our model-aware method under\nthe dataset with limited samples. Compared with the most\nchallenging competitor (Albanie et al. 2020), our method\noutperforms it by 4.57% and 4.03% top-1 per-instance and\nper-class accuracy improvement.\nConclusion\nIn this work, we introduce the hand prior and present the\nÔ¨Årst hand-model-aware end-to-end framework for isolated\nsign language recognition. Our framework consists of three\ncomponents, i.e., a visual encoder, a hand-model-aware de-\ncoder and an inference module. The hand sequence is Ô¨Årst\ntransformed to the latent semantic feature, which is then pro-\ncessed by the hand-model-aware decoder to derive compact\npose representations. Then the inference module reÔ¨Ånes the\npose representations and performs recognition. Besides the\nvideo-level supervision, we guide the learning of the inter-\nmediate pose representation on its spatial and temporal con-\nsistency in a weakly-supervised way. Extensive experiments\ndemonstrate the superiority of our method, achieving new\nstate-of-the-art performance on all four benchmark datasets.\n1564\nAcknowledgements\nThe work of Wengang Zhou was supported in part by the Na-\ntional Natural Science Foundation of China (NSFC) under Con-\ntract U20A20183 and 61632019, and in part by the Youth Innova-\ntion Promotion Association CAS under Grant 2018497. The work\nof Houqiang Li was supported by NSFC under Contract 61836011\nand 62021001. The work is supported by MCC Lab of Information\nScience and Technology Institution, USTC.\nReferences\nAlbanie, S.; Varol, G.; Momeni, L.; Afouras, T.; Chung, J. S.; Fox,\nN.; and Zisserman, A. 2020. BSL-1K: Scaling up co-articulated\nsign language recognition using mouthing cues. In ECCV.\nBallan, L.; Taneja, A.; Gall, J.; Van Gool, L.; and Pollefeys, M.\n2012. Motion capture of hands in action using discriminative\nsalient points. In ECCV, 640‚Äì653.\nBoukhayma, A.; Bem, R. d.; and Torr, P. H. 2019. 3D hand shape\nand pose from images in the wild. In CVPR, 10843‚Äì10852.\nBuehler, P.; Zisserman, A.; and Everingham, M. 2009. Learning\nsign language by watching TV (using weakly aligned subtitles). In\nCVPR, 2961‚Äì2968.\nCai, Y .; Ge, L.; Liu, J.; Cai, J.; Cham, T.-J.; Yuan, J.; and Thalmann,\nN. M. 2019. Exploiting spatial-temporal relationships for 3D pose\nestimation via graph convolutional networks. InICCV, 2272‚Äì2281.\nCamgoz, N. C.; HadÔ¨Åeld, S.; Koller, O.; and Bowden, R. 2017.\nSubUNets: End-to-end hand shape and continuous sign language\nrecognition. In ICCV, 3075‚Äì3084.\nCao, C.; Lan, C.; Zhang, Y .; Zeng, W.; Lu, H.; and Zhang, Y . 2018.\nSkeleton-based action recognition with gated convolutional neural\nnetworks. TCSVT 29(11): 3247‚Äì3257.\nCao, Z.; Hidalgo Martinez, G.; Simon, T.; Wei, S.; and Sheikh,\nY . A. 2019. OpenPose: Realtime multi-person 2D pose estimation\nusing part afÔ¨Ånity Ô¨Åelds. TPAMI .\nCarreira, J.; and Zisserman, A. 2017. Quo vadis, action recogni-\ntion? A new model and the Kinetics dataset. InCVPR, 6299‚Äì6308.\nChen, Y .; Kalantidis, Y .; Li, J.; Yan, S.; and Feng, J. 2018. Multi-\nÔ¨Åber networks for video recognition. In ECCV, 352‚Äì367.\nCho, K.; Van Merri ¬®enboer, B.; Gulcehre, C.; Bahdanau, D.;\nBougares, F.; Schwenk, H.; and Bengio, Y . 2014. Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine\ntranslation. In EMNLP, 1724‚Äì1734.\nClarke, A.; and Tyler, L. K. 2015. Understanding what we see:\nhow we derive meaning from vision. Trends in Cognitive Sciences\n19(11): 677‚Äì687.\nCui, R.; Liu, H.; and Zhang, C. 2019. A deep neural framework for\ncontinuous sign language recognition by iterative training. TMM\n21(7): 1880‚Äì1891.\nde La Gorce, M.; Fleet, D. J.; and Paragios, N. 2011. Model-based\n3D hand pose estimation from monocular video. TPAMI 33(9):\n1793‚Äì1805.\nDu, Y .; Wang, W.; and Wang, L. 2015. Hierarchical recurrent neu-\nral network for skeleton based action recognition. In CVPR, 1110‚Äì\n1118.\nEvangelidis, G. D.; Singh, G.; and Horaud, R. 2014. Continuous\ngesture recognition from articulated poses. In ECCV, 595‚Äì607.\nFeichtenhofer, C.; Fan, H.; Malik, J.; and He, K. 2019. Slowfast\nnetworks for video recognition. In ICCV, 6202‚Äì6211.\nGe, L.; Ren, Z.; Li, Y .; Xue, Z.; Wang, Y .; Cai, J.; and Yuan, J. 2019.\n3D hand shape and pose estimation from a single RGB image. In\nCVPR, 10833‚Äì10842.\nGoyal, R.; Kahou, S. E.; Michalski, V .; Materzynska, J.; Westphal,\nS.; Kim, H.; Haenel, V .; Fruend, I.; Yianilos, P.; Mueller-Freitag,\nM.; et al. 2017. The‚Äù Something Something‚Äù video database for\nlearning and evaluating visual common sense. In ICCV, 5843‚Äì\n5851.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learn-\ning for image recognition. In CVPR, 770‚Äì778.\nHochreiter, S.; and Schmidhuber, J. 1997. Long short-term mem-\nory. Neural Computation 9(8): 1735‚Äì1780.\nHu, H.; Zhou, W.; Pu, J.; and Li, H. 2020. Global-local enhance-\nment network for NMFs-aware sign language recognition. TOMM\n.\nHuang, J.; Zhou, W.; Li, H.; and Li, W. 2019. Attention based\n3D-CNNs for large-vocabulary sign language recognition. TCSVT\n29(9): 2822‚Äì2832.\nHuang, J.; Zhou, W.; Zhang, Q.; Li, H.; and Li, W. 2018. Video-\nbased sign language recognition without temporal segmentation. In\nAAAI, 2257‚Äì2264.\nIqbal, U.; Molchanov, P.; Breuel Juergen Gall, T.; and Kautz, J.\n2018. Hand pose estimation via latent 2.5D heatmap regression. In\nECCV, 118‚Äì134.\nJoze, H. R. V .; and Koller, O. 2019. MS-ASL: A large-scale data set\nand benchmark for understanding american sign language. BMVC\n.\nKarpathy, A.; Toderici, G.; Shetty, S.; Leung, T.; Sukthankar, R.;\nand Fei-Fei, L. 2014. Large-scale video classiÔ¨Åcation with convo-\nlutional neural networks. In CVPR, 1725‚Äì1732.\nKavan, L.; and ÀáZ¬¥ara, J. 2005. Spherical blend skinning: a real-time\ndeformation of articulated models. In ACM I3D, 9‚Äì16.\nKoller, O.; Camgoz, C.; Ney, H.; and Bowden, R. 2020. Weakly\nsupervised learning with multi-stream CNN-LSTM-HMMs to dis-\ncover sequential parallelism in sign language videos.TPAMI 42(9):\n2306‚Äì2320.\nKoller, O.; Forster, J.; and Ney, H. 2015. Continuous sign language\nrecognition: Towards large vocabulary statistical recognition sys-\ntems handling multiple signers. CVIU 141: 108‚Äì125.\nKoller, O.; Zargaran, S.; Ney, H.; and Bowden, R. 2018. Deep\nsign: Enabling robust statistical continuous sign language recogni-\ntion via hybrid CNN-HMMs. IJCV 126(12): 1311‚Äì1325.\nLaptev, I. 2005. On space-time interest points. IJCV 64(2-3): 107‚Äì\n123.\nLewis, J. P.; Cordner, M.; and Fong, N. 2000. Pose space deforma-\ntion: A uniÔ¨Åed approach to shape interpolation and skeleton-driven\ndeformation. In SIGGRAPH, 165‚Äì172.\nLi, C.; Zhong, Q.; Xie, D.; and Pu, S. 2018. Co-occurrence feature\nlearning from skeleton data for action recognition and detection\nwith hierarchical aggregation. In IJCAI, 786‚Äì792.\nLi, D.; Rodriguez, C.; Yu, X.; and Li, H. 2020a. Transferring cross-\ndomain knowledge for video sign language recognition. In CVPR,\n6205‚Äì6214.\nLi, D.; Rodriguez, C.; Yu, X.; and Li, H. 2020b. Word-level deep\nsign language recognition from video: A new large-scale dataset\nand methods comparison. In WACV, 1459‚Äì1469.\nLin, J.; Gan, C.; and Han, S. 2019. TSM: Temporal shift module\nfor efÔ¨Åcient video understanding. In ICCV, 7083‚Äì7093.\n1565\nLoper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.; and Black,\nM. J. 2015. SMPL: A skinned multi-person linear model. ToG\n34(6): 1‚Äì16.\nOikonomidis, I.; Lourakis, M. I.; and Argyros, A. A. 2014. Evo-\nlutionary quasi-random search for hand articulations tracking. In\nCVPR, 3422‚Äì3429.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.; Chanan,\nG.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.; et al. 2019.\nPyTorch: An imperative style, high-performance deep learning li-\nbrary. In NeurIPS, 8026‚Äì8037.\nQian, C.; Sun, X.; Wei, Y .; Tang, X.; and Sun, J. 2014. Realtime\nand robust hand tracking from depth. In CVPR, 1106‚Äì1113.\nQiu, Z.; Yao, T.; and Mei, T. 2017. Learning spatio-temporal repre-\nsentation with pseudo-3D residual networks. In ICCV, 5533‚Äì5541.\nQiu, Z.; Yao, T.; Ngo, C.-W.; Tian, X.; and Mei, T. 2019. Learning\nspatio-temporal representation with local and global diffusion. In\nCVPR, 12056‚Äì12065.\nRomero, J.; Tzionas, D.; and Black, M. J. 2017. Embodied hands:\nModeling and capturing hands and bodies together. ToG 36(6):\n245.\nShi, L.; Zhang, Y .; Cheng, J.; and Lu, H. 2019. Two-stream adap-\ntive graph convolutional networks for skeleton-based action recog-\nnition. In CVPR, 12026‚Äì12035.\nSimon, T.; Joo, H.; Matthews, I.; and Sheikh, Y . 2017. Hand key-\npoint detection in single images using multiview bootstrapping. In\nCVPR, 1145‚Äì1153.\nSimonyan, K.; and Zisserman, A. 2014. Two-stream convolutional\nnetworks for action recognition in videos. In NeurIPS, 568‚Äì576.\nSong, S.; Lan, C.; Xing, J.; Zeng, W.; and Liu, J. 2017. An end-to-\nend spatio-temporal attention model for human action recognition\nfrom skeleton data. In AAAI, 4263‚Äì4270.\nSridhar, S.; Oulasvirta, A.; and Theobalt, C. 2013. Interactive\nmarkerless articulated hand motion tracking using RGB and depth\ndata. In ICCV, 2456‚Äì2463.\nStarner, T.; Weaver, J.; and Pentland, A. 1998. Real-time American\nsign language recognition using desk and wearable computer based\nvideo. TPAMI 20(12): 1371‚Äì1375.\nTang, A.; Lu, K.; Wang, Y .; Huang, J.; and Li, H. 2015. A real-time\nhand posture recognition system using deep neural networks.ACM\nTIST 6(2): 1‚Äì23.\nTkach, A.; Pauly, M.; and Tagliasacchi, A. 2016. Sphere-meshes\nfor real-time hand modeling and tracking. ToG 35(6): 1‚Äì11.\nTran, D.; Bourdev, L.; Fergus, R.; Torresani, L.; and Paluri, M.\n2015. Learning spatio-temporal features with 3D convolutional\nnetworks. In ICCV, 4489‚Äì4497.\nTzionas, D.; Ballan, L.; Srikantha, A.; Aponte, P.; Pollefeys, M.;\nand Gall, J. 2016. Capturing hands in action using discriminative\nsalient points and physics simulation. IJCV 118(2): 172‚Äì193.\nWang, L.; Xiong, Y .; Wang, Z.; Qiao, Y .; Lin, D.; Tang, X.; and\nVan Gool, L. 2016. Temporal segment networks: Towards good\npractices for deep action recognition. In ECCV, 20‚Äì36.\nWang, Y .; Zhang, B.; and Peng, C. 2019. SRhandnet: Real-time 2D\nhand pose estimation with simultaneous region localization. TIP\n29: 2977‚Äì2986.\nXie, S.; Sun, C.; Huang, J.; Tu, Z.; and Murphy, K. 2018. Rethink-\ning spatio-temporal feature learning: Speed-accuracy trade-offs in\nvideo classiÔ¨Åcation. In ECCV, 305‚Äì321.\nYan, S.; Xiong, Y .; and Lin, D. 2018. Spatial temporal graph convo-\nlutional networks for skeleton-based action recognition. In AAAI,\n7444‚Äì7452.\nYasir, F.; Prasad, P. C.; Alsadoon, A.; and Elchouemi, A. 2015.\nSIFT-based approach on Bangla sign language recognition. In IC-\nCIA, 35‚Äì39.\nZhang, P.; Lan, C.; Zeng, W.; Xing, J.; Xue, J.; and Zheng, N. 2020.\nSemantics-guided neural networks for efÔ¨Åcient skeleton-based hu-\nman action recognition. In CVPR, 1112‚Äì1121.\nZhang, X.; Li, Q.; Mo, H.; Zhang, W.; and Zheng, W. 2019. End-to-\nend hand mesh recovery from a monocular RGB image. In ICCV,\n2354‚Äì2364.\nZhou, H.; Zhou, W.; Zhou, Y .; and Li, H. 2020. Spatial-Temporal\nMulti-Cue Network for Continuous Sign Language Recognition.\nIn AAAI, 13009‚Äì13016.\nZhu, W.; Lan, C.; Xing, J.; Zeng, W.; Li, Y .; Shen, L.; and Xie,\nX. 2016. Co-occurrence feature learning for skeleton based ac-\ntion recognition using regularized deep LSTM networks. In AAAI,\n3697‚Äì3703.\n1566",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8125208020210266
    },
    {
      "name": "Interpretability",
      "score": 0.8042598962783813
    },
    {
      "name": "Overfitting",
      "score": 0.7989740371704102
    },
    {
      "name": "Sign language",
      "score": 0.7387422919273376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6658240556716919
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.623370349407196
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5608486533164978
    },
    {
      "name": "Feature learning",
      "score": 0.5481678247451782
    },
    {
      "name": "Inference",
      "score": 0.5407313108444214
    },
    {
      "name": "Representation (politics)",
      "score": 0.520289957523346
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5050032734870911
    },
    {
      "name": "Machine learning",
      "score": 0.4459798038005829
    },
    {
      "name": "Gesture",
      "score": 0.4392232298851013
    },
    {
      "name": "Sign (mathematics)",
      "score": 0.4168105721473694
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40596264600753784
    },
    {
      "name": "Natural language processing",
      "score": 0.399405300617218
    },
    {
      "name": "Artificial neural network",
      "score": 0.16338351368904114
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}