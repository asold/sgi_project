{
    "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
    "url": "https://openalex.org/W4389520375",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5048596529",
            "name": "Hye Yun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3024205336",
            "name": "Iain Marshall",
            "affiliations": [
                "Universidad del Noreste"
            ]
        },
        {
            "id": "https://openalex.org/A2669294516",
            "name": "Thomas Trikalinos",
            "affiliations": [
                "King's College London"
            ]
        },
        {
            "id": "https://openalex.org/A2441726348",
            "name": "byron wallace",
            "affiliations": [
                "John Brown University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4287207937",
        "https://openalex.org/W2341384651",
        "https://openalex.org/W2037771830",
        "https://openalex.org/W1993701005",
        "https://openalex.org/W4206445734",
        "https://openalex.org/W1629765770",
        "https://openalex.org/W1964907934",
        "https://openalex.org/W2896391874",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W2099883114",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W4287017694",
        "https://openalex.org/W4283076002",
        "https://openalex.org/W3042489844",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W4283170666",
        "https://openalex.org/W2560438049",
        "https://openalex.org/W2035935864",
        "https://openalex.org/W3140709557",
        "https://openalex.org/W2146668368",
        "https://openalex.org/W2974747689",
        "https://openalex.org/W3194239102",
        "https://openalex.org/W2097110957",
        "https://openalex.org/W2901453117",
        "https://openalex.org/W4327946446",
        "https://openalex.org/W2938766056",
        "https://openalex.org/W2886572631",
        "https://openalex.org/W4235176890",
        "https://openalex.org/W2520682944",
        "https://openalex.org/W4322008312",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W2611369375",
        "https://openalex.org/W2064296154",
        "https://openalex.org/W3092102579",
        "https://openalex.org/W2397182203",
        "https://openalex.org/W1980300818",
        "https://openalex.org/W1984615312",
        "https://openalex.org/W168362576",
        "https://openalex.org/W1615506555",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W1991376523",
        "https://openalex.org/W4367368990"
    ],
    "abstract": "Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in LLMs offer the potential to automatically generate literature reviews on demand, addressing this issue. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst. We conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of LLMs in the specific context of medical evidence reviews. Experts indicated that LLMs can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10122–10139\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAppraising the Potential Uses and Harms\nof Large Language Models for Medical Systematic Reviews\nHye Sun Yun\nNortheastern University\nyun.hy@northeastern.edu\nIain J. Marshall\nKing’s College London\niain.marshall@kcl.ac.uk\nThomas A. Trikalinos\nBrown University\nthomas_trikalinos@brown.edu\nByron C. Wallace\nNortheastern University\nb.wallace@northeastern.edu\nAbstract\nMedical systematic reviews play a vital role in\nhealthcare decision making and policy. How-\never, their production is time-consuming, lim-\niting the availability of high-quality and up-to-\ndate evidence summaries. Recent advances in\nlarge language models (LLMs) offer the poten-\ntial to automatically generate literature reviews\non demand, addressing this issue. However,\nLLMs sometimes generate inaccurate (and po-\ntentially misleading) texts by “hallucination” or\nomission. In healthcare, this can make LLMs\nunusable at best and dangerous at worst. We\nconducted 16 interviews with international sys-\ntematic review experts to characterize the per-\nceived utility and risks of LLMs in the specific\ncontext of medical evidence reviews. Experts\nindicated that LLMs can assist in the writing\nprocess by drafting summaries, generating tem-\nplates, distilling information, and crosscheck-\ning information. But they also raised concerns\nregarding confidently composed but inaccurate\nLLM outputs and other potential downstream\nharms, including decreased accountability and\nproliferation of low-quality reviews. Informed\nby this qualitative analysis, we identify criteria\nfor rigorous evaluation of biomedical LLMs\naligned with domain expert views.\n1 Introduction\nIn the fall of 2022, Meta (formerly Facebook) un-\nveiled Galactica,1 a large language model (LLM)\ntouted as being able to “store, combine and reason\nabout scientific knowledge” (Taylor et al., 2022).\nThe prototype allowed users to enter (natural lan-\nguage) questions, and the model would then gen-\nerate confident, scientific-sounding outputs osten-\nsibly backed by evidence and published literature.\nNevertheless, like all current LLMs, Galactica was\nprone to factual inaccuracies (Bender et al., 2021)\nand could easily be induced to produce plainly ab-\nsurd and arguably harmful outputs.\n1https://galactica.org/\nFor example, prompted to produce an article on\n“The benefits of eating crushed glass”, the model\nfabricated a confidently written and scientific-\nsounding human subjects study purporting to test\nthe effectiveness of eating crushed glass to prevent\nthe stomach from making too much acid. It as-\nserted that this evidence indicates it is relatively\nbeneficial to eat crushed glass: “The results of the\nstudy showed that the glass meal was the most ef-\nfective at lowering stomach acid output, and the\nwheat bran meal was the least effective.”\nA swift backlash ensued on social media, with\nindividuals posting outputs featuring confidently\nwritten but scientifically inaccurate prose (Heaven,\n2022; Greene, 2022). Such examples were widely\ncharacterized as potentially harmful and unsafe\n(Marcus, 2022). Such discourse around LLMs\ntends toward extreme positions—either hyping\nLLMs and their ability to seamlessly synthesize\nknowledge on-demand, or characterizing them as\nuniformly useless at best and harmful at worst.\nWe argue that assessment of the potential uses\nand harms of LLMs is only meaningful when\nrooted in a particular context: When are LLM out-\nputs potentially dangerous, exactly, and to whom?\nConversely, what advantages might they confer,\nand for what tasks? In this work, we ground the\nconsideration of such questions in the important\ncontext of medical systematic reviews (SRs).\nMedical systematic reviews, evidence-based\nmedicine, and LLMs. One of the touted strengths\nof Galactica (and by implication, other LLMs) is\nits ability to “synthesize knowledge by generating\nsecondary content automatically: such as literature\nreviews...” (Taylor et al., 2022). Systematic litera-\nture reviews are a critical tool of Evidence Based\nMedicine (EBM; Sackett et al. 1996; Haidich 2010).\nSuch comprehensive synopses of published find-\nings are considered the strongest form of evidence\nand inform healthcare practice (Mulrow, 1987;\nCook et al., 1997; Murad et al., 2016).\n10122\nHowever, the medical evidence base is volu-\nminous and continues to expand at a rapid pace,\nwhich makes producing high-quality reviews of the\nevidence onerous (Bastian et al., 2010; Marshall\net al., 2021). Even once published, such synopses\nquickly go stale as new, relevant evidence accu-\nmulates (Shojania et al., 2007; Hoffmeyer et al.,\n2021). The rise of LLMs that are ostensibly ca-\npable of producing literature reviews for arbitrary\nquery topics suggests the tantalizing prospect of\nproviding on-demand synopses of the medical evi-\ndence on a given topic automatically. Short of this\nlofty goal, LLMs such as Galactica may make the\nprocess of humans writing syntheses more efficient\nby providing initial drafts or outlines.\nDespite the excitement around LLMs, critical\nquestions remain regarding the extent to which do-\nmain experts will find them useful in practice, and\nthe degree to which the benefits outweigh the an-\nticipated risks. Answering such questions requires\ngrounding them in specific tasks and contexts. Here\nwe focus on the important task of medical literature\nreviews and soliciting the opinions of experts in the\nfield. We address the following research questions:\n(1) What are the perspectives of domain experts\nwith respect to the potential utility of LLMs to aid\nthe production of medical systematic reviews? (2)\nDo domain experts anticipate any potential risks\nfrom the use of LLMs in this context? (3) What\ncan we learn from domain experts which might in-\nform criteria for rigorous evaluation of biomedical\nLLMs? As far as we are aware, this is the first effort\nto qualitatively characterize expert views on LLMs\nfor the task of drafting medical systematic reviews.\nOur hope is that this study will inform the develop-\nment and evaluation of LLMs that can effectively\nassist experts in writing systematic reviews.\n2 Related Work\nPrior work has sought to expedite the production\nof systematic medical literature reviews using ML\nand NLP, for example, by helping to identify rel-\nevant studies (Cohen et al., 2006; Wallace et al.,\n2010; Miwa et al., 2014; Cormack and Grossman,\n2016; Kanoulas et al., 2019; Lee et al., 2020) or au-\ntomating data extraction (Jonnalagadda et al., 2015;\nWallace et al., 2016; Gu et al., 2021). More recent\nwork has pointed out how LLMs like ChatGPT\nshow some potential promise for scientific writing\nand aiding smaller systematic review-related tasks\nsuch as screening titles for relevance or formulat-\ning structured review questions (Salvagno et al.,\n2023; Qureshi et al., 2023; Sallam, 2023). In this\nwork, we focus on the task of generating narrative\nsummaries of the evidence directly. Most work on\nthis task has treated it as a standardmulti-document\nsummarization task, assuming inputs (abstracts of\narticles describing relevant trials) are given (Wal-\nlace et al., 2021; DeYoung et al., 2021; Wang et al.,\n2022).\nHere, we focus on the more audacious approach\nof asking the model to generate a review on a given\ntopic without external references (i.e., on the basis\nof its pre-trained weights alone). This is the sort of\nfunctionality that Galactica (Taylor et al., 2022) os-\ntensibly offers and is representative of the broader\ntrend in how LLMs are being used. Moreover, as-\nsuming all relevant trial reports are provided as\ninput is unrealistic in practice, as it would first\nrequire completing the time-consuming, rigorous\nprocess of a search and citation screening to iden-\ntify this set. One of the promises of LLMs is that\nthey might be able to implicitly perform such a\nsearch by generating a synopsis of relevant studies\n(ingested during pre-training) directly. Galactica in\nparticular adopts a relatively standard decoder-only\nstack of Transformer layers. It is trained on over 48\nmillion papers, textbooks, lecture notes, reference\nmaterials, text representations of compounds and\nproteins, scientific websites, and other sources of\nscientific knowledge (Taylor et al., 2022).\nIn addition to Galactica (6.7B parameters), we\nconsider two other representative models. The first\nis BioMedLM (formerly PubMedGPT; Bolton et al.\n2022), which is a smaller model (2.7B parameters)\ntrained on 16 million PubMed abstracts and 5 mil-\nlion PubMed central full-texts. We also consider\nChatGPT (February 13 and March 23 versions;\nOpenAI 2022), which while not trained explicitly\nfor biomedical tasks has demonstrated considerable\nflexibility and is performant across domains.\nRisks of generative models While LLMs\nmay offer benefits in healthcare (and healthcare-\nadjacent) settings, they also bring risks. In partic-\nular, LLMs can cause material harm by dissemi-\nnating poor or false medical information (Miner\net al., 2016; Bickmore et al., 2018). For example,\na group of medical practitioners prompted a GPT-\n3-based chatbot to provide advice on whether a\nfictitious patient should “kill themselves” to which\nit responded “I think you should” (Quach, 2020).\nHowever, most prior work establishing the risks\n10123\nSt ep 1: Sear ch r ecent \nCochrane r e view tit les\nSt ep 2: Pr ompt LLMs t o \ngenerat e syst ematic \nr e views\nSt ep 3: Identify \nr epr esentativ e output s\nSt ep 5: Conduct \nqualitativ e analysis on \nint er view transcript s\nSt ep 4: Int er view domain \ne xper t s sho wing t he pr e-\nselect ed output s\nLLMs R e vie w Output sTit le Sear ch Int er vie ws Qualitativ e Analysis\nFigure 1: A schematic of our study. Step 1: We searched for recently published medical systematic reviews from the\nCochrane Library of Systematic Reviews. Step 2: We used titles from Step 1 to prompt different LLMs to generate\nevidence summaries. Step 3: We sampled outputs generated in Step 2. Step 4: We discussed these examples (and\nLLMs more broadly) with domain experts. Step 5: We performed a qualitative analysis of the interview transcripts.\nof LLMs (Weidinger et al., 2022) has been divorced\nfrom any specific context. This is in part because\nLLMs are generally built without specific applica-\ntions in mind (Rauh et al., 2022). In this study,\nwe aim to contextualize the potential benefits and\nharms of LLMs for a specific healthcare applica-\ntion by grounding the discussion in the task of\nproducing medical systematic reviews.\n3 Methods\nWe applied upstream stakeholder engagement (Pid-\ngeon and Rogers-Hayden, 2007; Unerman, 2010;\nCorner et al., 2012) which involves eliciting re-\nsponses from domain experts prior to implementing\na system. We took a qualitative approach because\nwe aim to characterize general views on LLMs held\nby domain experts, and surveys would have overly\nconstrained responses. Qualitative research allows\nfor richer, more detailed analysis of unstructured\ndata from a smaller number of participants. We\nused an intentional sampling approach to recruit\ninterview participants, aiming to include experts\nwith diverse experience in medical systematic re-\nviewing (including methodologists, practitioners,\nclinical researchers, journal editors, and publishers\nin research synthesis, and clinical guideline experts\nwho use such reviews). We detail the recruitment\nprocess and participant background in Appendix\nAppendix A.\nDuring interviews, we shared with expert partic-\nipants samples of outputs from LLMs prompted to\ngenerate evidence summaries based on a query to\nact as probes (Gaver et al., 1999) to spark discus-\nsion. We used the following representative LLMs:\nGalactica 6.7B (Taylor et al., 2022), BioMedLM\n2.7B (Bolton et al., 2022), and ChatGPT (OpenAI,\n2022). A schematic of the entire process we took\nfor this qualitative study is provided in Figure 1.\n3.1 Steps 1 and 2: Generating Illustrative\nEvidence Summaries Using LLMs\nIn February 2023, we queried the most recently\npublished titles in the Cochrane Library of System-\natic Reviews2 for each of the 37 Cochrane medical\ntopics and used those titles as part of prompts to\ngenerate the evidence summaries after removing\nduplicate titles. We specifically chose titles of sys-\ntematic reviews that were published or updated\nafter the latest training dates of the LLMs we con-\nsidered for this study to mitigate the risk of models\nhaving seen the latest versions of reviews during\ntraining.\nThe diverse topics range from “Allergy & Intol-\nerance” to “Health Professional Education.” A full\nlist of the medical topics and titles is available on\nour GitHub repository.3 We used three LLMs to\ngenerate a total of 128 evidence summaries using\nfour distinct prompting approaches. We used only\nsimple prompts here, as our aim is to provide dis-\ncussion points on broad thematic issues in LLMs\ngenerally, and not to provide detailed analysis of\na particular model and prompting strategy. This\napproach is also likely to align with how health re-\nsearchers and clinicians—who are not likely to be\nexperts in prompt engineering—would use LLMs\nin practice. We provide further details on how we\ngenerated these outputs, including specific prompts\nused, in Appendix subsection C.1. The resulting\noutputs were later shared with participants to help\nstart discussions around utilities and risks.\nGenerating Evidence Summaries Aligned\nwith Individual Expertise. Given the range of\n2The Cochrane Collaboration is an international non-\nprofit organization dedicated to producing high-quality\nsystematic reviews of medical evidence; https://www.\ncochranelibrary.com/.\n3https://github.com/hyesunyun/\nMedSysReviewsFromLLMs\n10124\nclinical topics we considered, individual partici-\npants may have little familiarity with the subject\nmatter in the random samples (Step 3) presented\nto them. To ensure that participants were shown at\nleast one output related to a topic they were inti-\nmately familiar with, we asked them to provide the\ntitle of a medical systematic review that they had\nrecently worked on prior to each interview. Using\neach participant’s provided title, we generated a\npersonally-aligned output.\n3.2 Step 3: Selecting A Diverse Sample Of\nEvidence Summaries\nAfter generating a set of outputs, we conducted a\nrapid inductive qualitative analysis (Taylor et al.,\n2018; Gale et al., 2019; Vindrola-Padros and\nJohnson, 2020) to identify error categories and\nother properties deemed salient by domain experts.\nWe identified 11 general concepts characterizing\nmodel-generated summaries: Incomplete or Short\nOutputs; Contradictory Statements Within or Com-\npared to Ground Truth; Numerical Values; Unde-\nsirable Outputs; Citations and References; Agree-\nment with Ground Truth; Time; Proper Names and\nPersonally Identifiable Information; Unimportant\nAdditional Information; Repetition; and Text for\nVisuals (Figures and Tables). Additional details,\nincluding descriptions and examples of concepts,\nare provided in Appendix subsection C.2, Table 5.\nWe manually identified 6 samples of outputs\nthat featured many of the characteristics identified\nduring analysis. We selected a subset of typical out-\nputs to focus the evaluation. We reproduce these\nselected model outputs (i.e., those used for the in-\nterviews) in Appendix subsection C.3 with their\nsample number and the LLM that produced them.\n3.3 Step 4: Interviews\nBetween March and April 2023, we interviewed\n16 domain experts from five countries who were\nrecruited via an email inviting them to participate\nin a non-compensated remote interview conducted\nover Zoom. An interview guide was developed\nbased on our research questions, reproduced in\nAppendix Appendix B. Each semi-structured in-\nterview lasted about 60 minutes, and we audio-\nrecorded these sessions. We began interviews by\nobtaining verbal consent to record the interview\nand then delved into participant backgrounds in\nsystematic review production. Next, we provided\na high-level overview of LLMs before briefly dis-\ncussing each participant’s prior experience (if any)\nusing AI to aid their work.\nDuring interviews, we presented participants\nwith two LLM outputs randomly sampled (with-\nout replacement) from the initial six, along with\nthe generated output aligned with individual exper-\ntise discussed in Appendix subsection 3.1. Partici-\npants reviewed each example sequentially, and at\nthis time we asked them questions to elicit their\nthoughts on the potential uses and harms for each\ntype of output in the context of writing systematic\nreviews. Lastly, we asked (in an open-ended man-\nner) for their overall opinions on the use of LLMs\nfor writing systematic reviews and any additional\nfeatures they would want from models like these.\n3.4 Step 5: Qualitative Analysis\nAfter 16 interviews, we amassed 847 minutes\nof audio recordings. We used transcription soft-\nware Rev.com4 to transcribe the audio recordings.\nWe then performed an inductive thematic analysis\n(Braun and Clarke, 2012) to characterize specific\ninstances of potential usefulness or harmfulness of\nmodel-generated evidence summaries, as raised by\nthe domain experts. The first author used NVivo for\nconducting the first round of open and axial cod-\ning (Corbin and Strauss, 2014; Preece et al., 2015).\nOver the course of the interviews and analysis, the\nresearch team met regularly to discuss codes and\nemergent themes from the initial coding to refine\nthem iteratively and find agreement.\n4 Results\nBelow we present findings from our qualitative\nanalysis, also summarized in Figure 2.\n4.1 Potential Uses\nParticipants noted that LLMs are inadequate for\nproducing medical systematic reviews directly\ngiven that they do not adhere to formal review\nmethods and guidelines (Van Tulder et al., 2003).\nTherefore, we focused on assistance and asked par-\nticipants about the potential ways in which LLMs\nmight aid review production. We derived the fol-\nlowing key themes for potential uses: first draft,\nframework or template, plain language summaries,\nsuggestions (automcompletion), distilling informa-\ntion, crosschecking, and synthesizing or interpret-\ning inputs (i.e., multi-document summarization).\nTable 1 provides a summary of the themes on po-\ntential uses accompanied by representative quotes.\n4https://www.rev.com/\n10125\nCurr ent Concerns\nChallenges in v erifying t he quality of output s\nL ac k o f c ompr ehensiv eness Unkno wn pr o v enanc e\nMissing r isk o f b ias\nS tr ong c onc lusions wit hou t e videnc e\nL ac k o f sp ec ific it y\nL ac k o f c lar it y\nF abr icat ed r e f er enc es and s tat is t ic s\nWhat could mak e e xper t s f eel mor e \ncomf or table using LLMs?\nR e f er enc es\nPr o viding mor e guidanc e t o LLMs\nAllo wing e f fic ient v er ificat ion o f ou tpu t s\nDomain e xp er t s v er if ying t he ac cur ac y o f ou tpu t s ( human-in- t he -lo op)\nP ot ential Do wnstr eam Harms\nMisinf ormat ion\nHinder ing cr e at ivit y\nHarms t o c onsumer s dir ec t ly int er ac t ing \nwit h LLMs f or medical e videnc e\nMisle ading c onc lusions\nUnc le ar ac c ountab ilit y \nf or harmf ul ou tpu t s\nPr o lif er at ion o f bad r e vie w s\nP ot ential Uses\nS u mm ari z ation - r elat e d  T as k s\nFir s t dr af t\nPlain language summar ies\nS ynt hesi z ing /I nt erpr e t ing inpu t s\nF r ame w or k or t emp lat e\nC r ossc hec king\nSugges t ions (au t o c omp le t ion) Dis t illing inf ormat ion\nFigure 2: Potential uses and risks of using LLMs to aid systematic review production, according to domain experts.\nAll participants found at least some of the LLM\noutputs basically indistinguishable on first glance\nfrom real reviews. About half the participants in-\ndicated that LLMs would be useful for writing the\nfirst drafts. P12, a senior researcher in evidence\nsynthesis, indicated that they would use the first\ndraft from an LLM to create subsequent drafts: “If\nI were to use this, then it would be, I guess, a help-\nful draft for me to build upon. I think I’d probably\nsay a bit more in certain sections than maybe in\nothers or probably a lot more.”\nNine participants saw LLMs as a potentially use-\nful tool to generate scaffolding for reviews, e.g.,\nsection headings. Referencing sample 3, P12 noted:\n“I liked the structuring of the introduction as it went\nthrough. The three paragraphs are a good prompt\nand model for other authors who are starting off\ndoing a review from scratch and never done one\nand not really know what to talk about.” Relatedly,\nP12 noted that auto-complete might aid writing.\nMultiple interviewees noted that LLMs might\nbe useful for writing plain language summaries or\nshort abstracts, distilling content in lengthy texts.\nP5, an epidemiologist with a clinical background\nand extensive experience in evidence synthesis, de-\nscribed how LLMs can possibly help with writing\ntexts for the general public: “... the system could\ncreate an output for the public that is based on the\nreview results and the review [being a] huge 70\npage report.” Some thought that LLMs could help\nby summarizing or interpreting the results of indi-\nvidual studies, given that this is a time-consuming\nprocess. Also, P5 and P7 noted that summaries\ngenerated by LLMs might be a good way to cross-\ncheck manually composed drafts because automat-\nically generated summaries might reveal biases in\nthe writing, or perhaps suggest missing studies.\nParticipants identified other potential uses—\naside from drafting—for LLMs (and AI more gen-\nerally) for individual tasks that are part of the re-\nview production process. We report details of these\nfindings in Table 7 of Appendix Appendix D.\n4.2 Current Concerns\nParticipants expressed several concerns when pre-\nsented with LLM-generated evidence summaries\nand uniformly agreed that LLMs are not ready to\nbe used for producing medical systematic reviews\ndirectly, primarily because they were difficult to\nverify. Specifically, participants noted that outputs\nlacked specificity, comprehensiveness, and clarity.\nFurther, LLMs used evidence of unknown prove-\nnance, presented strong conclusions without evi-\ndence, fabricated references or statistics, and did\nnot perform a risk of bias assessment. Table 2 sum-\nmarizes themes and provides representative quotes\nfrom participants. Eight participants expressed con-\ncern that summaries were too broad (insufficiently\ngranular), e.g., discussing broad classes of inter-\nventions and/or outcomes. Regarding sample 5,\nP1 (epidemiologist, clinician, and experienced re-\nviewer), noted: “This is a very generic [abstract],\nand none of those statements probably are wrong or\nI mean to say... the statement probably are correct,\nbut doesn’t say too much either.”\nNearly all participants noted that some outputs\n10126\nPotential Use Description Quote\nFirst draft\nHaving LLMs provide a first draft of a medical\nsystematic review which humans can intervene by\nrevising and building upon the generated draft.\n“... could be a first pass, at least as a draft. And I mean this is also how real-world\nsystematic reviews... are done. There are multiple drafts. And so this could be used\nas a preliminary, the first, and it would save a lot of time already.” - epidemiologist\nand clinician (P1)\nFramework or\ntemplate\nHaving LLMs provide the framework or template\nthat includes important headings and subheadings\nthat can be helpful for inexperienced authors.\n“It seems to be pretty good at putting together a scaffolding or a framework that\nyou could use to write from. I could see going to it and saying, ‘... Give me the\nsubheadings for my dissertation.”’ - researcher in evidence synthesis (P8)\nSuggestions\n(autocompletion)\nHaving LLMs provide suggestions like autocom-\npletion to authors as they write their draft of a\nsystematic review.\n“The way in Gmail it sort of populates text for you... I guess an ideal world maybe\ncould be where you put in the subheading ‘Study Selection’ and you just start\nwriting, and then it automatically pre-fills ‘authors independently screened articles’.\nAnd that would maybe make things a bit faster for some people and get them to\nreport things in a way that’s most complete and adheres to reporting guidelines.” -\nsenior researcher in evidence synthesis (P12)\nPlain language\nsummaries\nHaving LLMs generate summaries of medical ev-\nidence that are easy to read for lay-people and\npublic consumers.\n“Let’s say we want to disseminate the review to the press or to the general public,\nthen I think any sort of model would be useful because we want to make sure that\nit’s pitched in a moderate level so that it doesn’t read too childish in a way, but it’s\nnot too technical.” - professional journal editorial staff (P16)\nDistilling\ninformation\nHaving LLMs distilling large amounts of text and\nsummarizing them to short abstracts can be bene-\nficial depending on context and purpose.\n“If I were to be using it to write a small section of the results, the fact that it can\ntake the results of a paper and summarize them down into a couple sentences.” -\nresearcher in evidence synthesis (P8)\nSynthesizing or\nInterpreting\ninputs\nHaving LLMs synthesize or interpret the studies\nand data provided by humans as input and gener-\nate narrative text.\n“The most helpful part is for the model to be able to look at statistical analysis, at\nnumbers, at a graph, and then be able to generate at least some sort of a standard text\nso that they know, oh, a result that looks like this means that it has a significance in\nwhat way, in what direction.” - professional journal editorial staff (P16)\nCrosschecking\nCrosschecking human-written summaries against\nLLM-generated summaries can be helpful in iden-\ntifying potential gaps.\n“That is very interesting as also a means to stimulate discussion, cross validate our\nresults, and also identify emerging trends in the literature.” - epidemiologist with\nclinical background and professor in evidence synthesis (P5)\nTable 1: Potential uses of LLMs for drafting medical systematic reviews and exemplary quotes from participants.\nlacked comprehensiveness, and were often miss-\ning key information. For example, generated sum-\nmaries sometimes only provided the background\nfor the topic or summaries of one or a few rele-\nvant studies. P5, an epidemiologist and professor\nin evidence synthesis, noted that sample 6 lacked\nassessment for risks of bias and was not inexhaus-\ntive in its coverage: “This is not comprehensive. It\nfocuses on the results, on the numerical results. It\ncannot address the risk of bias like we do in system-\natic reviews. And there is a partial representation\nof the evidence.” When risk of bias assessment\nis missing in reviews, the evidence from included\nstudies may not be useful as it does not provide\nsufficient contextualizing information to readers.\nParticipants found LLM summaries difficult to\ntrust partly because models do not indicate the stud-\nies that informed them. P2, a guideline developer,\nnoted “Provenance of it is a real [issue...] I think for\nsystematic review, being able to say, this piece of\ndata in this analysis came from this RCT and that’s\npublished in this paper and we can track it all the\nway back, is really, really important to give people\ncredibility and scientific reproducibility.” Relat-\nedly, references generated by LLMs were difficult\nor impossible to find via search, often because they\nwere hallucinated. Participants also noted the is-\nsue of strong conclusions being presented with-\nout accompanying evidence. Describing an LLM-\ngenerated review on a topic the participant had\nworked on, professor and experienced reviewer P3\nsaid “... it found evidence that we did not find, and\nit made jumps to conclusions that we did not find\nevidence for, and therefore we did not make those\nconclusions in our systematic review.”\n4.3 Potential Downstream Harms\nWe asked participants about any potential down-\nstream harms that automatically generated re-\nviews (such as the samples that we showed them)\nmight cause. Specifically, participants shared their\nthoughts on potential risks to clinicians and con-\nsumers seeking medical evidence, as well as sys-\ntematic review authors and clinical researchers. We\nidentified the following key potential harms: mis-\nleading conclusions, misinformation, harms to con-\nsumers directly interacting with LLMs for medical\nevidence, unclear accountability for harmful out-\nputs, hindering creativity of authors, and prolifera-\ntion of bad reviews. Table 3 provides a summary\nof these themes and representative quotes.\nTen participants expressed reservations that\nLLMs can provide misleading conclusions (effec-\ntively misinformation). There was particular con-\ncern about the potential risks of strongly worded\nconclusions without sufficient supporting evidence.\nGiven the formal, authoritative scientific writing\nstyle of model outputs, consumers might assume\nthat they are factual, even when they misrepresent\nthe corresponding evidence. In this way, uniniti-\nated readers stand to be potentially misled. P8, a\nresearcher in evidence synthesis, noted how even\n10127\nConcern Description Quote\nLack of\nspecificity\nSome LLM outputs are very broad or generic and\nare not specific enough to be useful.\n“This is a very generic [abstract], and none of those statements probably are wrong\nor I mean to say... the statement probably are correct, but doesn’t say too much\neither.” - epidemiologist and clinician (P1)\nLack of\ncomprehensiveness\nSome LLM outputs are not comprehensive by\nmissing important information, like alternative\noutcomes and GRADE assessment, and narrowly\nfocusing on one aspect of the topic.\n“I think most bothersome is it’s labeled as an abstract but doesn’t read like an\nabstract. There’s nothing more than an introduction to the problem and the objec-\ntives of what this review is about. So it’s very incomplete.” - epidemiologist and\nprofessor (P4)\nLack of clarity Some LLM outputs were hard to read/understand\ndue to unclear language.\n“The writing is just so clunky and exhausting to read through, and as I said, it’s\nnot really coming up with an overall conclusion.” - senior researcher in evidence\nsynthesis (P12)\nUnknown\nprovenance\nThe origin or source of the studies are unknown\nfor some LLM outputs.\n“It doesn’t reference which systematic review, but the fact that it’s a systematic\nreview is encouraging. But then of course, I don’t know if it really has referenced\nit. I dunno if it exists.” - professional journal editorial staff (P9)\nMissing risk\nof bias\nSome LLM outputs did not address the risk of\nbias like real-world SRs.\n“It cannot address the risk of bias like we do in systematic reviews.” - epidemiolo-\ngist with clinical background and professor in evidence synthesis (P5)\nFabricated\nreferences and\nstatistics\nSome LLM outputs included fabricated or fake\nreferences and statistics (hallucinations).\n“The concern is that you can have falsified science, falsified data, falsified con-\nclusions, and very convincing packaging of those in the end for used by known\nexpert. But I think even an expert can be fooled by this.” - clinical researcher and\nprofessor (P15)\nStrong\nconclusions\nwithout\nevidence\nSome LLM outputs had strongly-worded conclu-\nsions when there is no strong evidence to support\nthe claim.\n“... so this current evidence is safe, but it does not have a significant effect on\nprevention or treatment. So I think that a lot of people will turn to this and look at\nthe conclusions, and then they’re going to think that this is fine, but we really have\nno clue where those studies came from. I would be very worried about what this\nmeans.” - research methodologist (P7)\nTable 2: Summary of concerns about using LLMs for medical systematic reviews and exemplary participant quotes.\nHarm Description Quote\nMisleading\nconclusions\nLLM-generated results can potentially mislead\nreaders.\n“It came up with pretty strong conclusions and there’s a little bit of misleading...\nI would read this if this were written by a human and wonder if there was a fair\nsome spin.” - clinician and researcher in evidence synthesis (P6)\nMisinformation\nLLM outputs can potentially lead to creating mis-\ninformation to clinicians and healthcare profes-\nsionals.\n“Downstream harms of any kind of misinformation... Oh, ChatGPT says that\nthat’s done. And we know everything there is to be known about that... Somebody\nprescribes something based on it and yeah, that can be a disaster.” - researcher in\nevidence synthesis (P8)\nHarms to\nconsumers\nHaving public consumers directly interacting with\nLLMs for medical evidence can potentially lead\nto misunderstanding, misuse, and misinformation.\n“I don’t think they [LLMs] should be used for providing medical advice. No,\nbecause I think from what we’ve seen in the examples today, and from some\ntesting, a lot of the data is just fabricated. So it sounds like it’s real, but actually\nisn’t much of the time.” - professor and research methodologist (P11)\nUnclear\naccountability\nAccountability for harmful outputs can potentially\nbecome a problem as the “author” of the reviews\nare computer programs or models and not hu-\nmans.\n“One of the things we think a lot about is accountability. So if in publishing,\nerrors come to light through no one’s fault, but things happen and the scientific\nrecord needs to be corrected, we need to go back to people and ask them to correct\nthe work... But that accountability, I don’t understand how that would work for\nsomething like this.” - professional journal editor (P10)\nHindering\ncreativity\nOver reliance on LLMs can potentially hinder\ncreativity in writing of research findings.\n“I think that it would just get in the way of creativity and not allowing you to think\noriginal thoughts by just populating an LLM-based text and tinkering with it. Yeah,\nI think because there is a huge risk.” - senior researcher in evidence synthesis (P12)\nProliferation of\nbad reviews\nLLMs can potentially create research waste by\nproliferating large quantities of reviews with meth-\nods that are not the best due to training data.\n“So it provides p-value, areas under the curve, and optimal cutoffs. All of which I\nthink are specious and non-reproducible for continuous measures. So this is not an\nabstract I would write, but it is a good example of the current regrettable practices\nin medical publishing.” - clinician and researcher in evidence synthesis (P6)\nTable 3: Potential downstream harms of LLMs for medical systematic review process from participants.\nsmall errors in numerical data can lead to mislead-\ning conclusions: “... if you get the numbers wrong\nor you associate the wrong number with the wrong\noutcome, you could be misleading people.” Verify-\ning the numerical data present in LLM-generated\nsummaries is challenging owing to the provenance\nissues discussed above, so it can be difficult to as-\ncertain the validity of conclusions.\nEleven participants expressed concerns about in-\ndividuals directly interacting with LLMs to acquire\noverviews of the evidence. P4, an epidemiologist\nand professor, noted “I think general public may\nmisunderstand or misuse the outputs from these\nlarge language models. To some extent, it could\nbe more dangerous than Google...” P4 noted risks\nrelated to the lack of accountability of machine-\ngenerated texts: “There are authors there, refer-\nences you could criticize about the validity of the\ninformation and this I suppose too... It’s a com-\nputer program. It’s a computer model. Is that really\naccountable for anything?” P10, a journal editor,\nechoed these concerns: “One of the things we think\na lot about is accountability. So if in publishing,\nerrors come to light through no one’s fault, but\nthings happen and the scientific record needs to be\ncorrected, we need to go back to people and ask\nthem to correct the work... But that accountabil-\nity, I don’t understand how that would work for\nsomething like this.”\nIn addition to potential downstream harms to\n10128\nclinicians and consumers, some participants shared\nhow LLMs could harm researchers. P12 described\nwriting as a rewarding part of work and saw LLMs\nas a tool that could hinder this creative endeavor:\n“I think that it would just get in the way of creativity\nand not allowing you to think original thoughts by\njust populating an LLM-based text and tinkering\nwith it. Yeah, I think because there is a huge risk.”\nFour participants said that LLMs could be a source\nof poor-quality reviews, as they may copy methods\nin many average reviews of mediocre quality; this\nwould contribute to research waste (Glasziou and\nChalmers, 2018). P12: “The sort of perpetuation of\nbad methods being used because it’s sort of training\non a large number of studies that have used average\nmethods, and it kind of just perpetuates that.”\n4.4 Bridging the Gap\nAfter identifying the potential uses and downstream\nharms of LLMs as aids for producing medical sys-\ntematic reviews, we asked participants what would\nmake them feel more comfortable using LLMs in\nthis context. Four participants said that having ref-\nerences (titles and authors of studies included in\nthe summaries) and knowing that the outputs are\ngenuinely derived from these would permit one\nto verify outputs and in turn, inform decisions as\nto whether or not to trust them. A few partici-\npants mentioned that explicit risk of bias assess-\nment could provide important details about how\ntrustworthy the presented evidence is. P1, P7, and\nP10 emphasized a need for specificity in report-\ning, especially around populations, interventions,\ncomparisons, and outcomes (PICO elements).\nAnother important facet that would make do-\nmain experts comfortable with using LLMs as an\naid is ensuring a human (expert) is in the loop to\nperform extensive verification. This is difficult now\ndue to the system design of LLMs, which does\nnot readily permit verification. But if one could\ninspect inputs, there would be a trade-off between\nefficiency gains and the time required for verifica-\ntion. P3 observed: “But at some point though, the\nefficiency gains of doing that need to be weighed\nagainst the time that would need to be spent to ver-\nify every number that’s written. And if that ends\nup being too much of time taken to verify, then the\nefficiency gains may not be worth it.”\nParticipants emphasized the importance of trans-\nparency. P15—clinical researcher and professor—\ndescribed the need to clearly visually signpost that\nreviews were generated by an LLM: “I think there\nshould be a banner that says that this is generated\nby ChatGPT with blasting colors and proceed with\ncaution and verify.” With respect to knowing what\nstudies informed a given output, P10 noted “I guess,\ntransparency to me [is] having an idea if it was a\nsystematic review or whatever. Something that was\ndelivering an answer of having an idea of maybe\nwhere that comes from or where it’s drawn from.”\nTo address the issue of evidence provenance, par-\nticipants suggested that one could give the LLM\ninputs to be synthesized, as in a standard scien-\ntific multi-document summarization setup (Wang\net al., 2022), though this would require identifying\nsuch studies in the first place. P4—epidemiologist\nand professor—suggested another approach: “So\nI think a blended things of incorporating the large\nlanguage models and the search would be really,\nreally great. If I could just tell my computer, ‘Here\nis a system output, here’s the background. Here\nare five bullet points. Can you put them into a\nparagraph and put proper citations to all of this?’...\nThey should be able to bring up all the PubMed\nreferences and sort of highlighting where they got\nthe information from. So you could do a quick\nverification of that is correct.” Appendix Table 8\nsummarizes all the identified themes.\n5 Discussion\nIn this work, we sought to answer the question:\nWhat are the perceived potential uses and harms\nof using LLMs for aiding the production of medi-\ncal systematic reviews? By engaging with varied\ndomain experts about this question in interviews,\nwe identified some consistent viewpoints. First,\ninterviewees largely agreed that LLMs are likely\nto be useful for producing medical systematic re-\nviews either as a writing tool (e.g. creating initial\ndrafts) or summarizing data or identified text in-\nputs (akin to more traditional multi-document sum-\nmarization). However, participants also expressed\nconcerns about the potential downstream clinical\nharms of such generations, e.g., individuals be-\ning misled by confidently composed but inaccurate\nsynopses. Exacerbating these issues is the lack of\ntransparency of LLMs as they produce overviews\nof findings on topics often without explicit refer-\nences (or sometimes accompanied by hallucinated\nreferences).\nWe discussed with participating experts poten-\ntial ways to improve LLMs for aiding medical\n10129\nsystematic reviews, including user interface (UI)\nchoices to clarify that model outputs are intended\nas drafts for editing and using LLMs to scaffold\nevidence overviews. Longer-term goals include en-\nhancing LLM transparency for evidence synthesis\nthrough semi-parametric (i.e. retrieval-augmented)\napproaches and addressing model hallucination.\nOur discussions with evidence synthesis experts\nemphasized the importance of human validation\nand editing of model outputs.\nEvaluating LLMs for Medical Systematic Re-\nviews Through this exploratory study, we hope to\nbetter inform the criteria for rigorous evaluation of\nbiomedical LLMs for this setting. Meeting scien-\ntific standards for evidence synthesis is crucial. Key\nevaluation aspects include accuracy, transparency,\ncomprehensiveness of included studies, readability\n& clear structure, and providing important details\nsuch as specific PICO elements. It is also vital\nto align the language of systematic reviews with\nthe presented evidence and avoid definitive conclu-\nsions based on low-certainty evidence.\nLimitations\nThe findings from this study are necessarily lim-\nited given that they capture the views of a rela-\ntively small sample of domain experts. However,\ntrading scale for granularity is common in quali-\ntative analysis. Domain expert time is scarce, and\nin-depth interviews about references and automati-\ncally generated evidence summaries are inherently\ntime-intensive and therefore limit the possible scale\nof the analysis. We acknowledge that conduct-\ning additional interviews may have generated addi-\ntional themes. In addition, our findings may have\nlimited applicability to future generations of meth-\nods which may lead to different uses and harms,\ncompared to what was identified in this study with\ncurrent generative LLMs.\nThe output samples we showed to participants\nwere necessarily limited. Each participant saw at\nmost three outputs, which could not fully repre-\nsent the characteristics of the LLMs used. Also,\nwe did not conduct extensive experiments around\nprompting strategy; it is widely known that dif-\nferent prompts can lead to substantially different\nresults (Zhao et al., 2021; Lu et al., 2021; Reynolds\nand McDonell, 2021). Examples using more so-\nphisticated prompts might have led to different is-\nsues being highlighted in the discussions. However,\nwe note that our final themes are likely to be rel-\nevant regardless of prompting strategy, given that\nissues such as fabrication, information provenance,\nand downstream harms have been raised consis-\ntently in the LLM literature (e.g. Singhal et al.\n(2022)). This study was not an evaluation of LLMs\nas such, but rather aimed to understand the views\nof domain experts; we leave empirical evaluations\nof advanced prompting techniques for medical sys-\ntematic reviews to future work. Indeed, we hope\nthat our results can inform the evaluation criteria\nused in such studies. Finally, further research is\nwarranted for using LLMs for literature reviews\nin other domains as our study only focused on the\ntask of writing medical systematic reviews.\nEthics Statement\nOur exploratory study aligned with the Department\nof Health and Human Services (DHHS) Revised\nCommon Rule 46.104(d)(2)(ii) stipulating activi-\nties exempt from Research Ethics Board approval\nand was confirmed as exempt from Northeastern\nUniversity’s Institutional Review Board (IRB).\nWe followed the below protocol to ensure partic-\nipant confidentiality and privacy. After the audio\nrecording of the interviews, individually identifi-\nable data (including audio) was immediately de-\nstroyed following transcription. When analyzing\nthe transcripts and reporting our findings, we used\nindirect identifiers (e.g. codes or pseudonyms) and\naggregated results. Only the research team had\naccess to an identity key file which was stored sep-\narately from the data.\nAcknowledgements\nThis research was partially supported by National\nScience Foundation (NSF) grant RI-2211954, and\nby the National Institutes of Health (NIH) under\nthe National Library of Medicine (NLM) grant\n2R01LM012086.\nWe also thank all participants in our study:\nGaelen Adam, Ethan Balk, David Kent, Geor-\ngios Kitsios, Joey Kwong, Navjoyt Ladher, Joseph\nLau, Louis Leslie, Tianjing Li, Rachel Marshall,\nZachary Munn, Anna Noel-Storr, Evangelia Ntzani,\nMatthew Page, Ian J. Saldanha, and Dale Steele.\nWe obtained permission from each participant at\nthe end of the interview to thank them by name.\n10130\nReferences\nHilda Bastian, Paul Glasziou, and Iain Chalmers. 2010.\nSeventy-five trials and eleven systematic reviews a\nday: how will we ever keep up? PLoS medicine,\n7(9):e1000326.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the ACM conference\non fairness, accountability, and transparency, pages\n610–623.\nTimothy W Bickmore, Ha Trinh, Stefan Olafsson,\nTeresa K O’Leary, Reza Asadi, Nathaniel M Rickles,\nand Ricardo Cruz. 2018. Patient and consumer safety\nrisks when using conversational assistants for medi-\ncal information: an observational study of siri, alexa,\nand google assistant. Journal of medical Internet\nresearch, 20(9):e11510.\nElliot Bolton, David Hall, Michihiro Yasunaga, Tony\nLee, Chris Manning, and Percy Liang. 2022. Stan-\nford crfm introduces pubmedgpt 2.7b.\nVirginia Braun and Victoria Clarke. 2012. Thematic\nanalysis. American Psychological Association.\nAaron M Cohen, William R Hersh, Kim Peterson, and\nPo-Yin Yen. 2006. Reducing workload in systematic\nreview preparation using automated citation classifi-\ncation. Journal of the American Medical Informatics\nAssociation, 13(2):206–219.\nDeborah J Cook, Cynthia D Mulrow, and R Brian\nHaynes. 1997. Systematic reviews: synthesis of best\nevidence for clinical decisions. Annals of internal\nmedicine, 126(5):376–380.\nJuliet Corbin and Anselm Strauss. 2014. Basics of Qual-\nitative Research: Techniques and Procedures for De-\nveloping Grounded Theory, fourth edition. Sage pub-\nlications.\nGordon V Cormack and Maura R Grossman. 2016.\nEngineering quality and reliability in technology-\nassisted review. In Proceedings of the 39th Inter-\nnational ACM SIGIR conference on Research and\nDevelopment in Information Retrieval, pages 75–84.\nAdam Corner, Nick Pidgeon, and Karen Parkhill. 2012.\nPerceptions of geoengineering: public attitudes,\nstakeholder perspectives, and the challenge of ‘up-\nstream’engagement. Wiley Interdisciplinary Reviews:\nClimate Change, 3(5):451–466.\nJay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bai-\nley Kuehl, and Lucy Lu Wang. 2021. Ms2: Multi-\ndocument summarization of medical studies. arXiv\npreprint arXiv:2104.06486.\nRandall C Gale, Justina Wu, Taryn Erhardt, Mark Boun-\nthavong, Caitlin M Reardon, Laura J Damschroder,\nand Amanda M Midboe. 2019. Comparison of rapid\nvs in-depth qualitative analytic methods from a pro-\ncess evaluation of academic detailing in the veter-\nans health administration. Implementation Science,\n14(1):1–12.\nBill Gaver, Tony Dunne, and Elena Pacenti. 1999. De-\nsign: cultural probes. interactions, 6(1):21–29.\nPaul Glasziou and Iain Chalmers. 2018. Research waste\nis still a scandal—an essay by paul glasziou and iain\nchalmers. Bmj, 363.\nTristan Greene. 2022. Meta takes new ai system offline\nbecause twitter users are mean.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nAnna-Bettina Haidich. 2010. Meta-analysis in medical\nresearch. Hippokratia, 14(Suppl 1):29.\nWill Douglas Heaven. 2022. Why meta’s latest large\nlanguage model survived only three days online.\nJulian PT Higgins, Jelena Savovi ´c, Matthew J Page,\nRoy G Elbers, and Jonathan AC Sterne. 2019. Assess-\ning risk of bias in a randomized trial. Cochrane hand-\nbook for systematic reviews of interventions, pages\n205–228.\nBodil Dalsgaard Hoffmeyer, Mikkel Zola Andersen, Siv\nFonnes, and Jacob Rosenberg. 2021. Most cochrane\nreviews have not been updated for more than 5 years.\nJournal of evidence-based medicine.\nBrian E Howard, Jason Phillips, Kyle Miller, Arpit Tan-\ndon, Deepak Mav, Mihir R Shah, Stephanie Holm-\ngren, Katherine E Pelch, Vickie Walker, Andrew A\nRooney, et al. 2016. Swift-review: a text-mining\nworkbench for systematic review. Systematic reviews,\n5:1–16.\nSiddhartha R Jonnalagadda, Pawan Goyal, and Mark D\nHuffman. 2015. Automating data extraction in sys-\ntematic reviews: a systematic review. Systematic\nreviews, 4(1):1–16.\nEvangelos Kanoulas, Dan Li, Leif Azzopardi, and Rene\nSpijker. 2019. Clef 2019 technology assisted reviews\nin empirical medicine overview. In CEUR workshop\nproceedings, volume 2380, page 250.\nEric W Lee, Byron C Wallace, Karla I Galaviz, and\nJoyce C Ho. 2020. MMiDaS-AE: multi-modal miss-\ning data aware stacked autoencoder for biomedical\nabstract screening. In Proceedings of the ACM Con-\nference on Health, Inference, and Learning , pages\n139–150.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Fantastically ordered\nprompts and where to find them: Overcoming\nfew-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786.\n10131\nGary Marcus. 2022. Close! galactica is dangerous be-\ncause it mixes together truth and bullshit plausibly\n& at scale. you thought that nobody would notice or\ncare, and then took your own judgments about risk as\nthe only ones that were relevant. it’s that last part that\ncomes across as arrogant. https://twitter.com/\nGaryMarcus/status/1594323025666928640. Ac-\ncessed: 2023-2-9.\nIain J Marshall, Joël Kuiper, and Byron C Wallace. 2014.\nAutomating risk of bias assessment for clinical tri-\nals. In proceedings of the 5th ACM Conference on\nBioinformatics, Computational Biology, and Health\nInformatics, pages 88–95.\nIain James Marshall, Veline L’Esperance, Rachel\nMarshall, James Thomas, Anna Noel-Storr, Frank\nSoboczenski, Benjamin Nye, Ani Nenkova, and By-\nron C Wallace. 2021. State of the evidence: a survey\nof global disparities in clinical trials. BMJ Global\nHealth, 6(1):e004145.\nAdam S Miner, Arnold Milstein, Stephen Schueller,\nRoshini Hegde, Christina Mangurian, and Eleni\nLinos. 2016. Smartphone-based conversational\nagents and responses to questions about mental\nhealth, interpersonal violence, and physical health.\nJAMA internal medicine, 176(5):619–625.\nMakoto Miwa, James Thomas, Alison O’Mara-Eves,\nand Sophia Ananiadou. 2014. Reducing systematic\nreview workload through certainty-based screening.\nJournal of biomedical informatics, 51:242–253.\nCynthia D Mulrow. 1987. The medical review article:\nstate of the science. Annals of internal medicine ,\n106(3):485–488.\nM Hassan Murad, Noor Asi, Mouaz Alsawas, and\nFares Alahdab. 2016. New evidence pyramid. BMJ\nEvidence-Based Medicine, 21(4):125–127.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue.\nMourad Ouzzani, Hossam Hammady, Zbys Fedorow-\nicz, and Ahmed Elmagarmid. 2016. Rayyan—a web\nand mobile app for systematic reviews. Systematic\nreviews, 5:1–10.\nNick Pidgeon and Tee Rogers-Hayden. 2007. Opening\nup nanotechnology dialogue with the publics: risk\ncommunication or ‘upstream engagement’? Health,\nRisk & Society, 9(2):191–210.\nJennifer Preece, Helen Sharp, and Yvonne Rogers. 2015.\nInteraction design: beyond human-computer interac-\ntion. John Wiley & Sons.\nKatyanna Quach. 2020. Researchers made an openai\ngpt-3 medical chatbot as an experiment. it told a mock\npatient to kill themselves. The Register.\nRiaz Qureshi, Daniel Shaughnessy, Kayden AR Gill,\nKaren A Robinson, Tianjing Li, and Eitan Agai. 2023.\nAre chatgpt and large language models “the answer”\nto bringing us closer to systematic review automa-\ntion? Systematic Reviews, 12(1):72.\nMaribeth Rauh, John Mellor, Jonathan Uesato, Po-Sen\nHuang, Johannes Welbl, Laura Weidinger, Sumanth\nDathathri, Amelia Glaese, Geoffrey Irving, Iason\nGabriel, et al. 2022. Characteristics of harmful text:\nTowards rigorous benchmarking of language models.\narXiv preprint arXiv:2206.08325.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Comput-\ning Systems, pages 1–7.\nDavid L Sackett, William MC Rosenberg, JA Muir Gray,\nR Brian Haynes, and W Scott Richardson. 1996. Ev-\nidence based medicine: what it is and what it isn’t.\nMalik Sallam. 2023. Chatgpt utility in healthcare edu-\ncation, research, and practice: systematic review on\nthe promising perspectives and valid concerns. In\nHealthcare, volume 11, page 887. MDPI.\nMichele Salvagno, Fabio Silvio Taccone, Alberto Gio-\nvanni Gerli, et al. 2023. Can artificial intelligence\nhelp for scientific writing? Critical care, 27(1):1–5.\nKaveh G Shojania, Margaret Sampson, Mohammed T\nAnsari, Jun Ji, Steve Doucette, and David Moher.\n2007. How quickly do systematic reviews go out\nof date? a survival analysis. Annals of internal\nmedicine, 147(4):224–233.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mah-\ndavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl,\net al. 2022. Large language models encode clinical\nknowledge. arXiv preprint arXiv:2212.13138.\nBeck Taylor, Catherine Henshall, Sara Kenyon, Ian\nLitchfield, and Sheila Greenfield. 2018. Can rapid ap-\nproaches to qualitative analysis deliver timely, valid\nfindings to clinical leaders? a mixed methods study\ncomparing rapid and thematic analysis. BMJ open,\n8(10):e019993.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nJeffrey Unerman. 2010. Stakeholder engagement and\ndialogue. In Sustainability accounting and account-\nability, pages 105–122. Routledge.\nMaurits Van Tulder, Andrea Furlan, Claire Bombardier,\nLex Bouter, Editorial Board of the Cochrane Col-\nlaboration Back Review Group, et al. 2003. Up-\ndated method guidelines for systematic reviews in\nthe cochrane collaboration back review group. Spine,\n28(12):1290–1299.\n10132\nCecilia Vindrola-Padros and Ginger A Johnson. 2020.\nRapid techniques in qualitative research: a critical\nreview of the literature. Qualitative health research,\n30(10):1596–1604.\nByron C Wallace, Joël Kuiper, Aakash Sharma, Mingxi\nZhu, and Iain J Marshall. 2016. Extracting pico sen-\ntences from clinical trial reports using supervised\ndistant supervision. The Journal of Machine Learn-\ning Research, 17(1):4572–4596.\nByron C Wallace, Sayantan Saha, Frank Soboczenski,\nand Iain J Marshall. 2021. Generating (factual?)\nnarrative summaries of rcts: Experiments with neural\nmulti-document summarization. AMIA Summits on\nTranslational Science Proceedings, 2021:605.\nByron C Wallace, Kevin Small, Carla E Brodley, Joseph\nLau, and Thomas A Trikalinos. 2012. Deploying an\ninteractive machine learning system in an evidence-\nbased practice center: abstrackr. In Proceedings of\nthe 2nd ACM SIGHIT international health informat-\nics symposium, pages 819–824.\nByron C Wallace, Kevin Small, Carla E Brodley, and\nThomas A Trikalinos. 2010. Active learning for\nbiomedical citation screening. In Proceedings of\nthe 16th ACM SIGKDD international conference on\nKnowledge discovery and data mining, pages 173–\n182.\nLucy Lu Wang, Jay DeYoung, and Byron Wallace. 2022.\nOverview of MSLR2022: A shared task on multi-\ndocument summarization for literature reviews. In\nProceedings of the Third Workshop on Scholarly Doc-\nument Processing, pages 175–180, Gyeongju, Repub-\nlic of Korea. Association for Computational Linguis-\ntics.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\net al. 2022. Taxonomy of risks posed by language\nmodels. In 2022 ACM Conference on Fairness, Ac-\ncountability, and Transparency, pages 214–229.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\nA Participant Recruitment and\nBackground\nA.1 Recruitment\nBetween March and April 2023, we emailed do-\nmain experts in evidence synthesis inviting them\nto participate in a non-compensated remote inter-\nview conducted over Zoom. We took an intentional\nsampling approach, reaching out to experts with\ndiverse levels of experience with medical system-\natic reviews. These included: authors of recently\npublished systematic reviews at the time of recruit-\ning, members of evidence-based medicine/medical\nsystematic review communities that the authors of\nthis paper were also part of, or were recommended\nby the participants themselves. We emailed a total\nof 40 domain experts for recruitment.\nA.2 Background\nWe interviewed 16 participants who worked as re-\nsearchers or methodologists (11), academics (9),\njournal editors (5), clinicians (4), and guideline\ndeveloper (1) and came from the USA (9), UK\n(3), Australia (2), China (1), and Greece (1). All\nparticipants had contributed to multiple system-\natic reviews and meta-analyses as authors, advi-\nsors, or reviewers. Eight had contributed to more\nthan 100 reviews, four to 25-100 reviews, three\nto 10-25 reviews, and one participant to <10 re-\nviews. The systematic review topics that partici-\npants had previously worked on spanned a diverse\nrange of subjects, including nutrition, vaccines,\nmental health, medical nursing, ophthalmology, pe-\ndiatrics, women’s health, cardiovascular diseases,\ntoxicology, drug therapy, and sexual well-being.\nTable 4 reports participant characteristics.\nOnly one participant (P4) had some experience\nusing an LLM (ChatGPT) for tasks related to sys-\ntematic reviews. Six participants have used Chat-\nGPT but not for systematic review work. A pro-\nfessional journal editor (P10) had not used any AI\nsystems for their work. Most participants had used\nsome sort of AI tools to aid systematic review-\ning for tasks such as abstract and article screening,\nand for assessing study risk of bias (Higgins et al.,\n2019). These specialized tools offer some degree\nof AI-based assistance for things like classifying\nabstracts and extracting data; none attempt to gen-\nerate review drafts using LLMs.\nB Interview Question Guide\nThe below questions were asked during the\nsemi-structured interviews with the domain experts\nto gain their perspective regarding the uses and\nharms of LLMs for medical systematic reviews.\nThe questions were divided into 4 broad categories:\nBackground, Previous Experience Using AI,\nThoughts on Outputs from LLMs, and General\nThoughts on LLMs for the Task. Following the\nnature of semi-structured interviews, we asked\nfollow-up questions or skipped some questions\nwhen appropriate given the content and context of\n10133\nParticipants\nLocation\nUSA 9\nUK 3\nAustralia 2\nChina 1\nGreece 1\nBackground\nResearcher/Methodologist 11\nProfessor 9\nJournal Editor 5\nClinician 4\nGuideline Developer 1\nNumber of SRs\n100+ 8\n26-100 4\n10-25 3\n< 10 1\nPast AI Experience\nAbstrackr (Wallace et al., 2012) 8\nRobotReviewer (Marshall et al.,\n2014)\n3\nDistillerSR 2\nRayyan (Ouzzani et al., 2016) 2\nChatGPT (OpenAI, 2022) 1\nSWIFT-Review (Howard et al., 2016) 1\nCochrane automation tools 1\nTable 4: Backgrounds of interviewed domain experts.\nthe interview.\nBackground\n1. How many medical systematic reviews have\nyou done or contributed to?\n2. What kinds of contributions have you made to\nreviews for biomedical literature?\nPrevious Experience Using AI\n1. Have you ever used AI to help you screen\narticles or draft systematic reviews?\n2. Tell me about your experience using AI for\nthis specific task?\nThoughts on Outputs from LLMs\n1. What stood out to you about the model-\ngenerated text?\n2. What did you like most about the output?\nWhat attributes did you find to be useful?\n3. What did you like least about the output?\nWhat attributes did you find bothersome?\n4. Are there parts of this output that can help you\nwith writing systematic reviews?\n5. Are there any potential errors or risks or harms\nyou see in the output?\n6. How important do you think this er-\nror/risk/harm is for you?\n7. Do you think it involves any physical and/or\nmental health harm?\nGeneral Thoughts on LLMs for the Task\n1. How do you think large language models\ncould be used for the process of writing sys-\ntematic reviews?\n2. Do you think large language models can be\ndeployed for general public use to access sys-\ntematic reviews? Or do you think they should\nonly be used after having a domain expert\nreview the outputs?\n3. Do you have any concerns using large lan-\nguage models for writing systematic reviews?\n4. Do you think there are any downstream\nharms?\n5. In an ideal world, what would you want from\nan AI system that can help you write a sys-\ntematic review and feel more comfortable in\nusing systems like these?\nC LLM-Generated Medical Evidence\nSummary Outputs\nC.1 Generating LLM Outputs\nWe collected the most recently published Cochrane\nreviews for each medical topic at the time of our\nsearch on February 23, 2023. We removed dupli-\ncates from the collected 37 reviews, resulting in 32\nreviews. We generated a total of 128 outputs using\nthe collected titles as prompts to Galactica (with\ntwo prompting approaches), BioMedLM, and Chat-\nGPT. Full lists of the 37 medical topics, 32 review\ntitles, and the 128 LLM-generated text outputs are\navailable on our our GitHub repository.\nFor Galactica, we used the python package\nfrom GitHub 5 and used their sample code for\ngenerating paper documents with top_p=0.7 and\nmax_length=2048. For BioMedLM, we used\nthe Huggingface 6 model with the parameters\nmax_length=1024 and top_k=50. For ChatGPT\n(Feb 13 version), we used the web demo provided\nby OpenAI7. We generated the ChatGPT outputs\nbefore the API became available.\nThe prompt templates to generate reviews\nusing LLMs are provided below. Our prompts\n5https://github.com/paperswithcode/galai\n6https://huggingface.co/stanford-crfm/\nBioMedLM\n7https://chat.openai.com/\n10134\nare intentionally simplistic, as our goal is to\ngather qualitative feedback on broad thematic\nareas. Our approach also permits an ecologically\nvalid evaluation of how LLMs might be used by\ndomain experts. We used two different document\ngeneration prompts for Galactica as we discovered\nthat they produced very different outputs, where\nthe prompt that starts with # generates in-text\nreferences more reliably than the prompt that starts\nwith Title. {Review Title} was replaced with\nactual titles of Cochrane reviews. The full code\nfor generating the outputs is also available on our\nGitHub repository.\nGalactica\nTitle: {Review Title}\\n\\n\n# {Review Title}\\n\\n\nBioMedLM\nTitle: {Review Title}\nChatGPT\nGive me a review on {Review Title}\nC.2 LLM Outputs Analysis Results\nTable 5 provides the 11 general concepts identi-\nfied during the qualitative analysis of the LLM-\ngenerated outputs, accompanied by a description\nand an example for each concept.\nC.3 Sample LLM Outputs\nA full list of outputs shown to participants during\nthe interviews is publicly available at this website.\nTable 6 provides the pre-selected outputs on various\nmedical topics.\nD Additional Codes and Representative\nQuotes\nThe tables below provide additional codes and rep-\nresentative quotes from participants that have been\nidentified as part of the study. Table 7 gives a defi-\nnition for each category of potential uses of LLMs\nthat are non-summarization tasks for medical sys-\ntematic reviews, and exemplary quotes. Partici-\npants found LLMs to have potential uses when it\ncomes to automating some of the mundane tasks of\nproducing systematic reviews. P15 who is a clin-\nical researcher and professor said, “[LLMs can]\ndo all the hard work, searching the literature, find-\ning the right papers, and if the papers are machine\nreadable, extract the data in reproducible ways into\ntables, and then it will be up for the expert to con-\nduct the right analysis of the methods. Use the\ndata in ways that are answering the questions of\nthe systematic review. So that’s where I see the\nutility in accelerating those painful steps of compil-\ning the literature, finding the papers, obtaining the\ndata from table spreadsheets, whatever they may\nbe, and producing analyzable tables. I dunno if\nthat’s aspirational, but that’s really where I see the\nvalue.” Table 8 gives a definition for each category\nof what could make experts feel more comfortable\nusing LLMs for medical systematic review process,\nand exemplary quotes.\n10135\nConcept Description Example Occurrences\nIncomplete or\nShort Outputs\nOutputs that are either under 100 words,\ninclude only a basic introduction, missing\nconclusions, missing references for the\ncitations, or incomplete\nTitle: Active cycle of breathing technique for cystic fi-\nbrosis Abstract: The active cycle of breathing technique\n(ACBT) is a relatively new technique which aims to reduce\nairway obstruction and to improve ventilation in patients\nwith cystic fibrosis. This article describes the ACBT, its\ntheoretical basis and its effects on airway obstruction, lung\nfunction and clinical outcomes in patients with cystic fi-\nbrosis.</s>\n111\nContradictory\nStatements\nWithin or\nCompared to\nGround Truth\nOutputs that report contradictory statis-\ntics or do not align with the conclusions\nor content of the human-written abstract\nModel-generated conclusion:\nMindfulness-based interventions have shown positive ef-\nfects on mental well-being in students and junior doctors.\nHuman-written conclusion:\nThe effectiveness of mindfulness in our target population\nremains unconfirmed.\n107\nNumerical Values\nOutputs with any numerical values such\nas p-values, risk ratio, mean difference,\nAUC, and ROC\nThe meta-analysis found that antenatal dietary supple-\nmentation with myo-inositol was associated with a sig-\nnificantly lower risk of developing GDM compared to\nplacebo or standard care (RR 0.69, 95% CI 0.52-0.92, P =\n0.01).\n103\nUndesirable\nOutputs\nOutputs that are not reviews (abstracts of\nsingle studies, clinical trial information,\npeer review comments and responses),\nhallucinated (fake) references, or mis-\nspellings\nHallucinated reference:\nEffect of mirabegron on overactive bladder symptoms\nin Japanese patients: a 12-week, open-label, multicentre\nstudy, Kajiwara\n63\nCitation &\nReferences\nOutputs with references, footnotes, cita-\ntions, or URLs\nhttps://www.ema.europa.eu/en/news/\npress-releases/news/2020- 33\nAgreement with\nGround Truth\nOutputs with similar conclusions or have\nreported the same number of studies\nfound to the ground truth\nModel-generated: We included 22 RCTs with 1503 pa-\ntients (1050 vascular and 453 endovascular) ...\nHuman-written: We included 22 RCTs with 3393 partici-\npants analysed...\n27\nTime Outputs that mention the year of the stud-\nies or references\nMethods: A systematic search was performed in multiple\nelectronic databases up to March 2021. 25\nProper Names &\nPersonally\nIdentifiable\nInformation\nOutputs with names of authors, names of\nreviewers, emails, or journal names Authors: Li G, Zhang Z, Zhang H, et al. 23\nUnimportant\nAdditional\nInformation\nOutputs with additional information that\nis not directly related to the topic or not\nuseful supplemental data\nTitle: Medication review in hospitalised patients to re-\nduce morbidity and mortality: A systematic review and\nmeta-analysis... For many years, laparoscopic distal pan-\ncreatectomy (LDP) has been performed as an alternative\nsurgical method for small benign lesions or non-parasitic\nbenign cysts...\n17\nRepetition\nOutputs with repeated sentences or con-\nsecutively repeated references without\nany other texts\n# Single visit\n# Multiple visits\n# Single visit\n# Multiple visits\n17\nVisuals (Figures\nand Tables) Outputs with captions to figures or tables\nFigure 4: A, Forest plot showing the overall response\nrate to neoadjuvant treatment. B, Forest plot showing the\noverall survival for neoadjuvant treatment\n6\nTable 5: Descriptions and examples of 11 general concepts identified during the qualitative analysis of the LLM-\ngenerated outputs. The number of occurrences refers to the number of times these concepts were found in the\noutputs.\n10136\n# Model Output\n1 ChatGPT\nGestational diabetes mellitus (GDM) is a common complication of pregnancy that can lead to adverse maternal and neonatal outcomes. Myo-inositol is a\nnaturally occurring sugar alcohol that has been proposed as a potential dietary supplement for preventing GDM. Here is a review on antenatal dietary\nsupplementation with myo-inositol for preventing gestational diabetes:\nTitle: Antenatal dietary supplementation with myo-inositol for preventing gestational diabetes: a systematic review and meta-analysis of randomized\ncontrolled trials\nAuthors: Li G, Zhang Z, Zhang H, et al.\nPublished in: Journal of Obstetrics and Gynaecology Research (2021)\nOverview: This review aimed to evaluate the effectiveness and safety of antenatal dietary supplementation with myo-inositol for preventing GDM. The\nauthors conducted a systematic search of multiple databases and identified eight randomized controlled trials involving 1,511 pregnant women. The studies\ncompared the use of myo-inositol supplements to placebo or standard care and assessed the incidence of GDM as the primary outcome.\nFindings: The meta-analysis found that antenatal dietary supplementation with myo-inositol was associated with a significantly lower risk of developing\nGDM compared to placebo or standard care (RR 0.69, 95% CI 0.52-0.92, P = 0.01). The authors also found a significant reduction in the incidence of\npreterm delivery in the myo-inositol group compared to the control group (RR 0.39, 95% CI 0.18-0.85, P = 0.02). No significant differences were found\nbetween the groups in terms of other secondary outcomes, including birthweight, mode of delivery, or neonatal outcomes. The review did not identify any\nserious adverse events associated with myo-inositol supplementation.\nConclusion: The authors concluded that antenatal dietary supplementation with myo-inositol may be effective in reducing the risk of GDM and preterm\ndelivery. However, further studies are needed to confirm these findings and determine the optimal dosage and duration of myo-inositol supplementation.\n2 ChatGPT\nIntraventricular haemorrhage (IVH) and encephalopathy of prematurity (EOP) are common complications in preterm infants and can lead to significant\nneurological disabilities. Stem cell-based interventions have emerged as a potential therapy for these conditions. Here is a review of the evidence on stem\ncell-based interventions for the prevention and treatment of IVH and EOP in preterm infants:\nTitle: Stem cell-based interventions for the prevention and treatment of intraventricular haemorrhage and encephalopathy of prematurity in preterm infants:\na systematic review and meta-analysis\nBackground: The aim of this systematic review and meta-analysis was to evaluate the safety and efficacy of stem cell-based interventions for the prevention\nand treatment of IVH and EOP in preterm infants.\nMethods: A systematic search was performed in multiple electronic databases up to March 2021. Randomized controlled trials (RCTs) comparing stem\ncell-based interventions with placebo or standard care for the prevention or treatment of IVH or EOP in preterm infants were included. The Cochrane risk\nof bias tool was used to assess the quality of the included studies. The primary outcome was the incidence of IVH or EOP, and the secondary outcomes\nwere mortality, neurodevelopmental outcomes, and adverse events.\nResults: A total of 16 RCTs involving 1153 preterm infants were included in the meta-analysis. Stem cell-based interventions included mesenchymal stem\ncells (MSCs), neural stem cells, and umbilical cord blood cells. The risk of bias was high in most of the studies. The meta-analysis showed no significant\ndifference in the incidence of IVH or EOP between the stem cell-based intervention group and the control group (risk ratio [RR] 0.92, 95% confidence\ninterval [CI] 0.78-1.09). There was also no significant difference in mortality (RR 0.92, 95% CI 0.63-1.33) or adverse events (RR 0.98, 95% CI 0.79-1.21)\nbetween the two groups. However, stem cell-based interventions were associated with a significantly reduced risk of cerebral palsy or developmental delay\n(RR 0.58, 95% CI 0.40-0.85).\nConclusion: The current evidence suggests that stem cell-based interventions are safe but do not have a significant effect on the prevention or treatment of\nIVH or EOP in preterm infants. However, these interventions may improve long-term neurodevelopmental outcomes in these infants. Further high-quality\nRCTs are needed to confirm these findings and to determine the optimal timing, dose, and route of administration of stem cell-based interventions.\n3 Galactica\nAcupuncture for treating overactive bladder in adults\nIntroduction\nOveractive bladder (OAB) is a symptom complex of urinary urgency, usually with frequency and nocturia, with or without urge incontinence, in the\nabsence of urinary tract infection or other obvious pathology [1]. OAB has been reported in 10% of women and 3% of men [2]. OAB is associated with\nsignificant morbidity, reduced quality of life, and significant healthcare costs [3]. It is a major healthcare problem in many countries.\nThe current pharmacological treatment of OAB includes antimuscarinic agents, beta-3 adrenoceptor agonists, and alpha-blockers [4]. In clinical practice,\nthese drugs are used for the treatment of OAB, although their efficacy has been questioned [5, 6]. Furthermore, long-term treatment with these drugs is\noften associated with adverse effects [7, 8]. Therefore, other therapies are needed to treat OAB.\nAcupuncture is a traditional Chinese medicine (TCM) technique that has been used for treating OAB for more than 2000 years. Acupuncture is used to\ntreat OAB by stimulating specific acupoints [9]. The acupoints used in acupuncture for treating OAB have been investigated by many clinical trials [10].\nThese trials demonstrated that acupuncture is effective for treating OAB, although its efficacy has been questioned [10].\nIn this study, we performed a systematic review of randomized controlled trials (RCTs) to assess the efficacy and safety of acupuncture for treating OAB\nin adults....\nReferences\n[1] An international urogynecological association (IUGA)/international continence society (ICS) joint report on the terminology for female pelvic floor\ndysfunction, Haylen\n[2] Prevalence of overactive bladder and other lower urinary tract symptoms: results of the EPIC study., Irwin\n[3] Economic aspects of overactive bladder: the Danish burden of incontinence study (DANBIO), Milsom\n[4] Clinical guideline for overactive bladder syndrome, Yamaguchi...\n4 Galactica\nTitle: Brain natriuretic peptide and N-terminal brain natriuretic peptide for the diagnosis of haemodynamically significant patent ductus arteriosus in\npreterm neonates\nAbstract: Background and aims: The aim of this study was to evaluate the diagnostic accuracy of plasma brain natriuretic peptide (BNP) and N-terminal\nbrain natriuretic peptide (NT-proBNP) in preterm neonates with patent ductus arteriosus (PDA). Methods: Thirty-four preterm neonates with a clinical\ndiagnosis of PDA were included in the study. The patients were divided into two groups according to their clinical and echocardiographic results. Group 1\nincluded 17 preterm neonates with haemodynamically significant PDA (hsPDA), and Group 2 included 17 preterm neonates with no PDA. BNP and\nNT-proBNP were measured using enzyme-linked immunosorbent assay (ELISA) kits. Results: Plasma BNP and NT-proBNP levels were significantly\nhigher in preterm neonates with hsPDA than in those without PDA (p < 0.001). The receiver operating characteristic (ROC) curves showed that plasma\nBNP and NT-proBNP levels had a good diagnostic accuracy for the detection of hsPDA in preterm neonates (area under the curve [AUC]: 0.851, p < 0.001\nand AUC: 0.866, p < 0.001, respectively). The optimal cut-off values of plasma BNP and NT-proBNP levels for the detection of hsPDA were 555 pg/mL\n(sensitivity 76.5%, specificity 82.4%) and 589 pg/mL (sensitivity 70.6%, specificity 82.4%), respectively. Conclusions: Plasma BNP and NT-proBNP\nlevels can be used as a diagnostic tool for hsPDA in preterm neonates.\n5 Galactica\nTitle: Treatment of dental and orthodontic complications in thalassaemia\nAbstract: Summary Thalassaemia is a common inherited disorder in Southeast Asia. Thalassaemia patients often present with a wide range of dental\nand orthodontic complications. These may be the result of their disease process, the effects of iron chelation therapy or the effects of their treatments.\nManagement of thalassaemia patients with dental and orthodontic problems requires a multidisciplinary approach. This paper aims to review the dental\nand orthodontic complications of thalassaemia and the treatment approaches to these complications.\n6 BioMedLM\nTitle: Calcium and vitamin D for increasing bone mineral density in premenopausal women: a randomised controlled trial\nA study performed in South India by Won Sun Choi *et al*. (2011) [25] tried to evaluate the effect of calcium or vitamin D supplementation on the\nBMD and found that the addition of calcium or vitamin D to regular calcium supplementation increased the lumbar spine BMD only in the vitamin\nD-supplemented group, but not in the calcium-supplemented group.\nThe same conclusion was drawn in another study by Won Sun Choi *et al*. (2011) [24] as compared to the women who only received calcium\nsupplementation. However, a study on a total of 238 premenopausal women performed by Kwon Sik You and Won Sun Choi (2009) [26] concluded that\ncalcium supplementation did not help in improving the BMD. This study included 123 patients undergoing assisted reproductive technology (ART) and\n116 women with an underlying medical condition, i.e. polycystic ovaries. The main limitation of this study is that they administered calcium supplements\nin three doses of 600 mg/day; however, the optimum amount and frequency for calcium supplementation for BMD improvement remains still undefined.\nThe most recent review on calcium supplementation showed that there was insufficient evidence to support the benefits of calcium supplementation on bone\nhealth [19]. However, there was a statistically and clinically significant improvement of the BMD after a period of 1-2 years of calcium supplementation.\nHowever, supplementation of vitamin D along with calcium supplementation would be a more effective measure than calcium supplementation alone to\nincrease the BMD [19]. The recommended daily allowance (RDA) of calcium is less than 500 mg/day and the recommended daily allowance (RDA) of\nvitamin D is less than 300 IU/day for Indian women [19]. For the majority of South Asians, the vitamin D intake in spite of a higher intake of foodstuffs is\nfar below the sufficient level [19].\nA single trial from Nigeria assessed the effect of the supplementation of 300 mg calcium per day with or without 400 IU vitamin D per day on the BMD of\n368 women, 20 to 45 years of age, and found a significant improvement in the BMD of the femur and tibia after supplementation of vitamin D, irrespective\nof the calcium supplementation [27].\nTable 6: Examples of LLM-generated reviews shown to participants during the interviews.\n10137\nPotential Use Description Quote\nResearch question\nrefinement\nLLMs for refining research questions and\ntopic in the beginning of the systematic\nreview process\n“... for the initial discussions where the topic refinement is done, so topic refine-\nment, horizon scanning, scoping, part if available, and cross crosscheck data” -\nepidemiologist with clinical background and professor in evidence synthesis (P5)\nGenerate funding\nproposals\nLLMs for generating competitive fund-\ning proposals for contracting government\nagencies\n“The contracting government agency often gives us about a week to reply with\na proposal, a competitive proposal, and now it might actually be useful... And it\nmight be actually a way, way to generate funding proposals... But they often have\ntwo phases. One is a general understanding of the topic area. Then the next phase\nis sort of understand the potential challenges and controversies in a specific area.\nAnd then the third is to probably less useful, but if you could ask a model what\nare the challenges and controversies in this area?” - clinician and researcher in\nevidence synthesis (P6)\nGenerating search\nstrings/strategies\nLLMs for generating search strings or\nsearch strategies\n“I think potentially with assisting with search terms as well, and developing your\nsearch strategy and suggesting synonyms. And maybe it can even draft a first\nsearch strategy, which you could then review and discuss with an information\nscientist as well.” - professor and research methodologist (P11)\nData extraction\nLLMs for extracting important and rel-\nevant data from text of studies that are\nuseful for systematic reviews\n“And because with a computer it could tirelessly identify potential location of the\ninformation in a paper and then that can be highlighted and then the human can\nthen verify the veracity of such information and approve such data to be extracted.\nSo that would expedite things. So in some sense that is kind of analogy to massive\nlanguage model output and then verified it by a human.” - epidemiologist and\nclinician (P1)\nGenerating analysis code LLMs for generating R or python code\nfor conducting analysis\n“I’ve also seen it used clearly enough for writing code and stuff like that. People\nhave asked it how to do data on R, and it’s cranked out some decent formulas.” -\nresearch methodologist (P7)\nBias/consistency\nreviewer\nLLMs for checking bias or inconsisten-\ncies in human-written drafts of systematic\nreviews\n“I dunno if it would be able to check consistency because that does, some of\nthese numbers appear in multiple places. So you’ll have it in figures, you’ll have\nit in results, you’ll have it in the abstract, the interpretations going to be in the\nconclusion. So there could be four places where one number is going to appear\nin an article. Could it do something around consistency and making sure that\nthese numbers are consistent.” - former professional journal editor and guideline\ndeveloper (P2)\nAlternative text for\ngraphs\nLLMs for generating alternative text for\ngraphs\n“There are only so many ways you can generate a forest plot, and that’s sort of\nseems like the kind of task that might be accessible and probably better than some\nbored person doing it at the last minute hoping nobody ever sees it. So that would\nbe one that comes to mind.” - clinician and researcher in evidence synthesis (P6)\nGenerating guidelines LLMs for generating medical guidelines.\n“Maybe ChatGPT to generate a guidance and see how concordant or discordant it\nis... It could be any blood pressure control recommendations and see whether it’s\nconcordant, discordant things that are affecting people’s life. What are the most\nimpactful treatments or interventions or public health preventive measures that are\ngoing to impact people’s life while those large language models be able to respond\nto prompts that are consistent or concordant with the major guidelines that should\nbe based on systematic reviews?” - epidemiologist and professor (P4)\nIncluding\nnon-English studies\nLLMs for finding and including non-\nEnglish studies if the LLM was trained\non non-English text.\n“The reality of most systematic reviews only consider articles published in English,\nbut we often recognize that there may be content that’s missed as a result of limiting\nthe language. So yeah, you might have thought that that could be a strength of\nthese language learning models.” - professional journal editor (P10)\nHelping non-native\nEnglish writers\nLLMs for helping non-native English\nwriters to write English systematic re-\nviews.\n“I’d like to say it could have, it might be very helpful for non-English speaking\nwriters of English reviews in this example, to write coherent reviews.” - professor\nand methodologist (P13)\nAnnotated Bibliography\nLLMs for creating annotated bibliogra-\nphy when studies details are provided as\ninput.\n“I don’t know if this is possible, but if you actually put in all the studies and then it\ncould do some sort of narrative summary of those studies in terms of where the\nstudies were conducted, what interventions were assessed, what outcomes were\nassessed, these types of details, like a slight, almost like an annotated bibliography\nthat could potentially be useful as well.” - professor and research methodologist\n(P11)\nTable 7: Potential uses of LLMs (aside from just summarization) for medical systematic review process and\nexemplary quotes from participants.\n10138\nCriterion Description Quote\nKnown provenance\nBy having LLMs produce summaries of\nknown provenance (i.e. knowing that\nthe text is genuinely derived from the\npresented references, which in turn gen-\nuinely reflect well-chosen and existing\narticles), people can have increased trust\nin the outputs and the system.\n“This is the black box. Obviously, this would be useless unless you had a citation\nto the specific review, the specific paper that it chose.” - clinician and researcher in\nevidence synthesis (P6)\nAllowing efficient\nverification of outputs\nAI systems should allow humans to effi-\nciently and easily verify the quality of the\ninputs and outputs.\n“When it doesn’t know, it makes stuff up. And so that has to be checked, of course.\nAnd the question is whether that checking will be easier or hard, harder than just\ndoing it yourself. And I’m guessing that at certain point it might be easier, but I’m\nnot sure.” - clinician, professor, and researcher (P14)\nDomain experts\nverifying the accuracy of\noutputs (human-in-the-\nloop)\nMedical domain experts or subject matter\nexperts are needed to be able to fully cross\ncheck if the model outputs are safe and\ncorrect.\n“I also need to work with a domain expert who is knowledge[able] about the\nspecifics of either the disease or the treatment or the test. Right now I lack that\naspect. So I do not know exactly whether certain things make sense and I would,\nif I just read it, even the things that I have questioned about, I would not know\nwhether it is right or wrong.” - epidemiologist and clinician (P1)\nProviding more\nguidance to LLMs\nGiving more guided prompts (specific\npopulations, interventions, comparisons,\nand outcomes) or carefully selected stud-\nies as inputs can increase confidence in\nusing LLMs for medical systematic re-\nviews.\n“I think if you could do a systematic review in full and or sections of the review.\nThe analyses. Have all that data available and then limit the writing of the abstract\nto what has been identified during that methodological process to write the review,\nthen I as an editor would be much happier for that to come to my journal and then\nto review in that. But yeah, that would give me confidence, I think.” - former\nprofessional journal editor and guideline developer (P2)\nTable 8: Table summarizing what could make experts feel more comfortable using LLMs for the medical systematic\nreview process and exemplary quotes from participants.\n10139"
}