{
  "title": "Large language models and the problem of rhetorical debt",
  "url": "https://openalex.org/W4411661084",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2655209824",
      "name": "MARIT MacArthur",
      "affiliations": [
        "University of California, Davis"
      ]
    },
    {
      "id": "https://openalex.org/A2655209824",
      "name": "MARIT MacArthur",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2058760204",
    "https://openalex.org/W4308591617",
    "https://openalex.org/W4399364148",
    "https://openalex.org/W4323033785",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4211209393",
    "https://openalex.org/W4391869005",
    "https://openalex.org/W4390009957",
    "https://openalex.org/W4408132507",
    "https://openalex.org/W4388260374",
    "https://openalex.org/W4399198943",
    "https://openalex.org/W4403296982",
    "https://openalex.org/W3016510729",
    "https://openalex.org/W4390585411",
    "https://openalex.org/W2739751023",
    "https://openalex.org/W2011381597",
    "https://openalex.org/W4403232382",
    "https://openalex.org/W4402590229",
    "https://openalex.org/W4409885395",
    "https://openalex.org/W4361865684",
    "https://openalex.org/W2141373701",
    "https://openalex.org/W4392935324",
    "https://openalex.org/W2084235337",
    "https://openalex.org/W3023846342",
    "https://openalex.org/W4406586623",
    "https://openalex.org/W4406778798",
    "https://openalex.org/W2808429234",
    "https://openalex.org/W4400949264",
    "https://openalex.org/W2081011876",
    "https://openalex.org/W4409725439",
    "https://openalex.org/W2062719061",
    "https://openalex.org/W4390480971",
    "https://openalex.org/W4402590771",
    "https://openalex.org/W4386175050",
    "https://openalex.org/W4213412063",
    "https://openalex.org/W4313477803",
    "https://openalex.org/W4390621457",
    "https://openalex.org/W4408056799",
    "https://openalex.org/W3179527741",
    "https://openalex.org/W613188293",
    "https://openalex.org/W2134379144",
    "https://openalex.org/W1545958036",
    "https://openalex.org/W2941683140"
  ],
  "abstract": "Abstract This article offers broadly useful guidance for society’s adaptation to the omnipresence of generative AI, with implications for every profession and academic discipline that involves writing or coding (recognized by some as a form of writing). Offering an interdisciplinary perspective grounded in the digital humanities, software development and writing across the curriculum, and building on performance historian Christopher Grobe’s research on the role of arts and humanities expertise in AI development, I offer redefinitions of training data and prompt engineering . These essential yet misleading terms obscure the critical roles that humanities-based expertise has played in the development of GPTs and must play in guiding society’s adaptation to generative AI. I also briefly review scholarship on what constitutes “writing” and what it means to teach writing. Next, I reflect on long-terms trends, in professional software development, of code sharing and reliance on automation, and the likely impact of imposing similar practices in professional writing. After identifying the fundamental problem of rhetorical debt and outlining its consequences, I further motivate my argument, in relation to the new economic value of expert writing. This new economic value necessitates a revaluation of the humanities—not only by computer science, the tech industry, and schools and universities, but by humanists themselves.",
  "full_text": "Vol.:(0123456789)\nAI & SOCIETY \nhttps://doi.org/10.1007/s00146-025-02403-w\nOPEN FORUM\nLarge language models and the problem of rhetorical debt\nMarit MacArthur1\nReceived: 30 March 2025 / Accepted: 19 May 2025 \n© The Author(s) 2025\nAbstract\nThis article offers broadly useful guidance for society’s adaptation to the omnipresence of generative AI, with implications for \nevery profession and academic discipline that involves writing or coding (recognized by some as a form of writing). Offering \nan interdisciplinary perspective grounded in the digital humanities, software development and writing across the curriculum, \nand building on performance historian Christopher Grobe’s research on the role of arts and humanities expertise in AI develop-\nment, I offer redefinitions of training data and prompt engineering. These essential yet misleading terms obscure the critical \nroles that humanities-based expertise has played in the development of GPTs and must play in guiding society’s adaptation to \ngenerative AI. I also briefly review scholarship on what constitutes “writing” and what it means to teach writing. Next, I reflect \non long-terms trends, in professional software development, of code sharing and reliance on automation, and the likely impact \nof imposing similar practices in professional writing. After identifying the fundamental problem of rhetorical debt and outlining \nits consequences, I further motivate my argument, in relation to the new economic value of expert writing. This new economic \nvalue necessitates a revaluation of the humanities—not only by computer science, the tech industry, and schools and universities, \nbut by humanists themselves.\nKeywords Expertise · Large language models · Writing · The humanities · Prompt engineering · Technical debt\n“There is no new thing under the sun.” – Ecclesiastes \n1:9, 450 ~ 150 BCE\n“The hottest new programming language is English.” \n– Andrej Karpathy, OpenAI, Jan. 24, 2023\n1 Introduction\nIn 2024, “slop” made the short list for the Oxford Word of \nthe Year, losing out to “brain rot.” Slop is defined as “art, \nwriting, or other content generated using artificial intelli -\ngence, shared and distributed online in an indiscriminate \nor intrusive way, and characterized as being of low qual-\nity, inauthentic or inaccurate” (Oxford Dictionaries 2024). \nWhile the term “slop” has been used to disparage certain \n“cultural material … since at least the mid-nineteenth \ncentury,” 2024 saw a 332% increase in its application to \nLLM-generated content. “Brain rot,” a term coined by Henry \nDavid Thoreau in Walden (1854), is now defined as “the sup-\nposed deterioration of a person’s mental or intellectual state, \nespecially viewed as the result of overconsumption of mate-\nrial (now particularly online content) considered to be trivial \nor unchallenging.” Thus consumption of too much sloppy \nAI-generated text [which I purposefully do not refer to as \n“writing” (see Kitzinger 2024; Vee 2023)] may rot the brain.\nIf we hope to leverage AI1 for the common good, yet fear \nwidespread brain rot or cognitive overload by AI, should \nwe feel reassured by the widespread apprehension of slop? \nNo, because slop is in the eye of the beholder. And glaringly \nobvious slop is not the real challenge. To predict and miti-\ngate the negative “impact[s] of widespread … adoption” of \nAI on the “future of human thought, information-seeking, \n * Marit MacArthur \n mjmacarthur@ucdavis.edu\n1 University of California, Davis, Davis, United States\n1 I use AI as shorthand for generative AI throughout this article. My \nconcern is particularly with large language models that power chat-\nbots, but future developments with AI-generated text will pose the \nsame problems I identify.\n AI & SOCIETY\nand knowledge” (Peterson 2025), we must focus on that \nchallenge. It arises when AI-generated text seems correct \nand appropriate but is not—a sort of uncanny valley (Mori \n1970, 2012) of seemingly authoritative text—which can \nfool novices or experts-in-training, and escape the atten-\ntion of experts who, in haste, fail to notice or point out its \nflaws. Such AI-generated text, in contrast to obvious slop, \nis not guilty of factual inaccuracies or so-called hallucina-\ntions (Dumit and Roepstorff 2025), but includes content, \nand makes stylistic choices, which are inappropriate or inad-\nequate to our immediate context, purpose and audience. 2\nTo catch AI-generated text that seems correct and appro-\npriate, but isn’t, we need expert human judgment. In the \nheady weeks after the public release of ChatGPT (on 30 \nNovember 2022), some programmers placed undue con-\nfidence in the chatbot’s coding advice, due to automation \nbias—the tendency to place too much trust in machine-\ngenerated suggestions (Skitka et al. 1999). They made such \nmistakes because they were either experts in a hurry (given \nthe profit-driven, breakneck speed of software development), \nor novices persuaded by the output’s seeming fluency, both \nin English and in programming languages. Fortunately, the \nexpert adults in the room quickly intervened. In December \n2022, less than a month after ChatGPT’s release, Stack \nOverflow “banned AI-generated text” from its platform. As \nDennis Soemers, an assistant professor of computer sciences \nat Maastricht University and a Stack Overflow moderator, \nnoted: “‘Content generated by ChatGPT looks trustwor -\nthy and professional, but often isn’t’” (qtd. in Mickle et al. \n2023). Recently, Stack Overflow introduced Answer Assis-\ntant, which offers AI-generated programming advice but \ndoes not make it public until it has been verified by expert \nhuman moderators (Stack Overflow 2025).\nExperts across disciplines are better equipped than nov -\nices to collaborate with AI. Because without prior training \nor disciplinary expertise, the novices may struggle to write \ncreative and effective prompts, and to judge and edit AI out-\nput. Photographer Boris Eldagsen (who turned down a Sony \nAward for Best Photograph, for an image he used generative \nAI to create) affirmed this insight in 2023:\nfor the first time in technology advancement, an older \ngeneration is better off because I know much more than \na 15-year-old .… You start from your imagination, and \nyou describe what you would like to have. And you can \nmake such a text prompt quite complicated…. And \nmy knowledge as a photographer, my knowledge as an \nartist makes it different in my prompting … it is kind \nof like co-creation (Eldagsen 2023).\nAn expert in their field knows whether an AI-generated \ndocument or image really suits their purpose. This is true \nof an artist or photographer—or an experienced physician, \nevaluating a medical chatbot’s diagnosis of a patient’s ill-\nness, based on symptoms and test results provided by the \nphysician. That is why, across all disciplines, we rely on \nthe judgment of fellow experts, including in the process of \nacademic peer review. The current generation of professional \nexperts, educated without recourse to all-purpose chatbots \nlike ChatGPT, Claude, Gemini, DeepSeek, etc., may be \nadept at distinguishing accurate, appropriate AI-generated \ntext or code from its close but inapt semblance—when, that \nis, they take the time to critically assess the output. What \nabout future generations?\nThough the computational functions behind LLM-pow -\nered chatbots and other AI tools are complex, the singular \ntask they have foisted upon all educators is not: to continue \nto train and cultivate future experts while discouraging over-\nreliance on AI, which would eventually render users unable \nto judge the quality of AI output. If our educational systems \ncan make a significant shift, to reinvest in the humanities \nand promote humanities-based education and training—\nfrom philosophy and history to literature and science and \ntechnology studies—we might feel more optimistic about \ntoday’s school children developing the ability to spot slop, \nand to distinguish appropriate text or code from an inapt \nsemblance. This important educational task—re-emphasiz-\ning humanities-based education to adapt to AI—is hiding in \nplain sight behind misleading terminology such as prompt \nengineering, which suggests that students need to study \nengineering, not writing, in order to collaborate with AI. \nSuch misleading terminology is not merely a stylistic choice \nor a bothersome symptom of our technocentric culture. The \nstakes are higher than that.\nIn this article, I offer broadly useful guidance for society’s \nadaptation to the omnipresence of generative AI, with \nimplications for every profession and academic discipline \nthat involves writing or coding, recognized by some as a \nform of writing (Brock 2019; Hermans and Aldewereld \n2017; Juszkiewicz et al. 2019; Vee 2013). My tasks here \nare five. First, I build on performance historian Christopher \nGrobe’s research, on the role of arts and humanities \nexpertise in AI development, to offer redefinitions of the \nessential concepts of training data and prompt engineering, \nbecause these misleading terms obscure the critical roles that \nhumanities-based expertise has played in the development \nof GPTs and must play in guiding society’s adaptation to \ngenerative AI. Second, I briefly review scholarship on what \nconstitutes “writing” and what it means to teach writing. \nThird, I reflect on long-terms trends emphasizing speed \nand efficiency in professional software development, with \nincreasing reliance on code sharing and AI coding assistants, \nand the likely impact of imposing similar practices in \n2 Professional researchers and academic journal editors are embar -\nrassing themselves by letting such AI-generated text slip into pub-\nlished research articles (Glynn 2024).\nAI & SOCIETY \nprofessional writing. 3 Fourth, I identify the fundamental \nproblem of rhetorical debt and outline its consequences. \nLastly, I further motivate my argument, in relation to the \nnew economic value of expert writing. This new economic \nvalue necessitates a revaluation of the humanities—not only \nby computer science, the tech industry, and schools and \nuniversities, but by humanists themselves.\n1.1  AI’s dependence on the arts and humanities\nThe guidance I offer here is not a naïve, self-interested \nattempt on behalf of the humanities to stem the flow of \nfunding away from “us” toward STEM, which generative \nAI has accelerated. In “The Programming Era: The Art of \nConversation Design from ELIZA to Alexa,” Grobe (2023) \ndemonstrates the central role of “aesthetic experts” in trans-\nformative technological developments we normally attribute \nto computer science. In his research on the development of \nvirtual agents, he interviewed many “writers … and per -\nformers” who wrote the scripts for virtual agents like Siri \nand Alexa and designed the ways that they converse with \nhuman interlocutors. As with virtual agents, so with GPTs, \nwhich are trained on text, images, etc., created by expert \nwriters and artists, among others. Grobe observes:\nMany of the people engaged in building and selling \nthese systems seem to believe that they will make the \narts and humanities obsolete. Why write when you have \nGPT? Why paint when you have DALL-E? Such con-\nfidence is made possible, in part, by their exclusion \nof the people who actually have and understand these \nskillsets.\nIndeed, software developers and computer scientists \nhabitually misconstrue writing as its polished, fluent end-\nproduct, rather than as the cognitive and rhetorical process \nit is. Many confidently assert that when a chatbot produces \ntext or code, what it is doing is writing. With remarkable \nshort-sightedness, some also argue that “the valuation of \nthe ability to write” will inevitably diminish in the age of AI \n(Hellström 2024). Following this logic, we would conclude \nthat a chatbot’s text-generating activity can or should replace \nthe human activities of writing and programming, thus lib-\nerating humans from these tasks. Quite a few people follow \nsuch misguided thinking, given the widespread predictions, \nin 2023, that generative AI would mean “the end of writing” \nand “the end of programming.”\nWhat happens, then, when we ascribe the genius of \ngenerative AI to computer science, or to a profit-driven \ntech company like so-called OpenAI (which betrayed its \nfounding open-source ethos), rather than highlighting the \ndeep dependence of LLMs on “training data”—a misleading \neuphemism for a vast trove of expert human writing? For one \nthing, aesthetic experts fail to receive credit or compensation \nfor their intellectual property. 4 For another, we miss the \nimportance of humanities-based education in adapting to \ngenerative AI. As Kate Crawford has noted, AI is neither \n“artificial,” given its dependence on human expertise, nor \n“intelligent,” given its functional limitations (qtd. in Goodlad \n2022). Yet it is quite common for Big Tech to attribute \nhuman-exceeding intelligence to AI (Dyos 2025). Such AI \nhype raises stock values and attracts customers. But it also \nsuggests that AI developers have lost sight of the constitutive \nrole of human expertise and intelligence in developing and \nusing “artificial” intelligence. Musician and composer Holly \nHerndon has proposed “collective intelligence” as a more \naccurate term for “AI” technologies (Herndon 2018), and \nDennis Yi Tenen favors the term “collective labor” (Tenen \n2024, p. 123).\nIn his critique of Big Tech’s undervaluation of arts and \nhumanities expertise in the development of virtual agents, \nGrobe calls for interdisciplinary perspectives on the role of \nthe arts and humanities in developing technology:\nTo counteract this computational chauvinism, as well \nas the technophobic retrenchment it inspires in many \nartists and humanists, we need two things: more stud-\nies of technologists who draw substantially on arts-\nknowledge, and more attention to artists who enter tech \nspaces and take up there the “hard translational work” \nof bridging the gap between “the practices, vocabular-\nies, and mindsets” of technology and those of the arts. \n(Grobe 2023)\nLike artists who enter tech spaces, scholars of the digi -\ntal humanities and writing, rhetoric and composition have \nlong studied and grappled with the integration of new tech-\nnologies into research and teaching, from interdisciplinary \nperspectives.\nThe insights I offer here rely on the “hard translational \nwork” I have done, in bridging the humanities, computer \nscience, and related STEM fields, in my digital humanities \nresearch, in teaching professional writing, and in my work \nas a Writing Across the Curriculum (WAC) consultant and \nadministrator. For the last decade, I have been immersed in \ncultures of coding, in leading the collaborative development \nof open-source tools and documentation for digital voice \n3 Creative writing, which is outside the scope of this article, obvi-\nously overlaps with professional and academic writing in some ways \nand not in others.\n4 See Bender et  al. (2021) on “documentation debt” as well as the \nmany lawsuits filed by writers and media companies against AI com-\npanies (Baker Hostetler 2025).\n AI & SOCIETY\nstudies research. 5 I first learned to use and tweak basic \nscripts in R Studio, Python, and MATLAB, and later to \nunderstand the uses and limitations of LLMs and machine \nlearning algorithms for speech recognition and analysis. \nI have also routinely supervised undergraduate assistants \nin computer science and worked with collaborators in \nsoftware development, statistical analysis, interface design, \nlinguistics and neuroscience. This collaborative research \nexperience gives me sympathetic insight into the challenges \nthat computer science majors and professional programmers \nface in developing and applying programming skills, in the \nextremely fast-paced world of software development. In \nteaching professional writing and in my WAC work, I have \nalso supported graduate students and faculty to improve \ntheir writing instruction in many disciplines, from ecology, \nengineering, plant biology and psychology to art history, \nfilm studies and religious studies. In the classroom, I have \nguided computer science majors in researching and writing \npapers on leading-edge topics in software development. All \nthis cross-disciplinary experience prepared me for the impact \nof generative AI, particularly in relation to professional \nwriting, in ways I did not entirely anticipate. 6 Accordingly, \nI offer redefinitions of the essential concepts of training data \nand prompt engineering.\n2  “Training Data” is writing\nTraining data, for an LLM, is human expertise captured in \nwriting, which must be continually developed and curated by \nhumans.7 Like the term “content,” the term “training data” \nminimizes the tremendous effort and resources necessary for \nhumans to develop expertise that is then fed to LLMs. As \nwe know, the LLMs that underlie chatbots excel at engag-\ning in and simulating humanlike dialog because they were \ntrained—using deep machine learning techniques that imi-\ntate the neural networks of the human brain—on a massive \namount of text written by humans, notably from Common \nCrawl (Baack 2024).\nWhat’s deemphasized, by tech companies who want to \nkeep “training data” free, or at least cheap, is that LLMs \nand all future AI tools and platforms will necessarily con-\ntinue to rely on new human expertise, captured in writing or \ndeployed as critical reading and editing skills, in every field \nof human endeavor. Referring to human expertise capture \nin writing as “training data” obscures this fact—especially \nif we mistake AI output as writing, rather than probabilistic \npredictions of the likeliest string of tokens that the prompt-\ning human asks the chatbot to produce, based on the human-\nexpertise-captured-in-writing that the chatbot was trained \non. The generation of AI output depends on prior human \nthinking and writing, and on a continued supply of human \nexpertise—the right corpus of texts or code written, heavily \nedited, developed or curated by humans—as “training data.” \nThe safe and appropriate application of that AI output to any \ntask, in turn, depends on human expert “evaluation” (Rob-\nbins 2025), grounded in disciplinary expertise and rhetorical \nawareness.\n3  Prompt “Engineering” is prompt writing\nAnd what would we gain by calling this activity what it \nindisputably is—writing prompts for GPTs—rather than \nprompt engineering ? Perhaps lower salaries, since “engi -\nneering” job titles are currently compensated at much higher \nrate than “writing” job titles. Nevertheless, Big Tech seems \nto recognize the importance of writing skills in collaborat-\ning with AI—sort of. OpenAI’s Logan Kilpatrick declared \nat the end of 2023:\nMany believe prompt engineering is a skill one must \nlearn to be competitive in the future. The reality is that \nprompting AI systems is no different than being an \neffective communicator with other humans. The same \nprinciples apply in both cases. This makes me bullish \non reading, writing, and speaking as the 3 underly -\ning skills that really matter in 2024…. [F]ocusing on \nthe skills necessary to effectively communicate with \nhumans will future proof you for a world with AGI \n(artificial general intelligence). (Kilpatrick 2023)\nPrompting AI, however, is not the same thing as com -\nmunicating with a human being—it is harder.\nThough LLMs have ingested an enormous amount of \nspecialized knowledge encoded in human-authored texts, \nthey are, ultimately, computer programs. There are defined \nlimits on the local rhetorical context they can assess, in part \n5 In this research, I have received support from an American Council of \nLearned Societies Digital Innovations Fellowship, a National Endowment \nfor the Humanities-Mellon Fellowship for Digital Publication, a NEH \nDigital Humanities Advancement Grant (Level II) and a Social Sciences \nand Humanities Research Council of Canada grant.\n6 In 2023–24, I helped lead a large study on AI in large and small \nwriting-intensive courses at the University of California Davis. A \nco-investigator with the Center for AI and Experimental Futures at \nUC Davis (recipient of a 2024 NEH grant canceled by the Trump \nadministration), I am also a principal investigator on Peer and AI \nReview + Reflection (PAIRR) a 3-year grant project funded by the \nCalifornia Education Learning Lab, led by UC Davis and involving \nthree campuses in the California State University system and four \ncampuses in the California Community College system. See our ini-\ntial research findings in Computers and Composition (Sperber et al. \n2025). Lastly, I am a series editor for the journal Critical AI on the \ntopic of teaching writing in higher education.\n7 Of course, human expertise as captured in images, video, curated \nnumerical data sets, etc., has also been used by generative AI as train-\ning data, but I restrict myself here to considering text.\nAI & SOCIETY \nbecause they rely on language alone. As Dumit and Roep-\nstorff observe,\neach text [in the training data] bears traces of its con-\ntext, including its genre, voice, audience and the his-\ntory and local politics of its place of origin. A correct \nsentence in one scientific discipline might be inaccu-\nrate or nonsensical in another. Texts are ‘true’ only \nin the right context…. [Yet LLMs] operate in a space \nwhere meaning is constructed rather than retrieved \nfrom elsewhere. (Dumit and Roepstorff 2025)\nThe ability first to understand one’s immediate context, \nand then to precisely describe in words, is essential to writ-\ning effective prompts for AI, and to evaluating the output and \nrefining it—either by editing that output directly or further \nprompting AI to refine it.\nAccordingly, I offer a redefinition of prompt engineering: \nWriting instructions for a probabilistic text-generating (or \ncode, image, audio, etc.-generating) machine, detailing the \nimmediate rhetorical situation (Bitzer 1968)—the purpose, \naudience(s), guidelines for the genre, and the real-world, \nlocal context—which the machine cannot access without \nhuman guidance, because the LLM is not embedded in the \nimmediate physical, social, cultural, political world in the \nwhich the prompting human lives.\nThis definition, I hope, clarifies why disciplinary experts \nare adept at writing prompts that elicit relatively appropriate \nAI output and at evaluating and refining that output, while \nnovices struggle to do so. Novices often lack either a sophis-\nticated understanding of their immediate context, genre, \naudience, etc., or lack the ability to describe these things \nclearly in words. This understanding of prompt engineering \nas prompt writing also has consequences.\nSpecialized chatbots might be trained—by human dis-\nciplinary experts who understand rhetorical context—to \nprompt novices to articulate their immediate context, up to \na point. This will be an easier and more successful process \nif we understand that prompting is writing, not engineer -\ning, and look to the humanities, not computer science, to \nteach prompt writing. For better or worse, generative AI has \nelevated critical reading and writing as the most important \nfoundational skills to develop across and within disciplines \nin schools and universities. The defunding and sidelining \nof the humanities have pushed university curriculum in a \ndirection counterproductive to the global economy’s needs \nin the age of generative AI.\n3.1  What is writing, anyway?\nThe use of “training data” and “prompt engineering” to refer \nto human writing, as we have seen, betrays both an under -\nvaluation and misunderstanding of the activity of writing. \nWhile it is beyond the scope of this article to review cen-\nturies of scholarship on the human activity of writing, it is \nworthwhile to reflect on some key points in the history of \nwriting technologies and scholarly consensus about the cog-\nnitive and social dimensions of reading and writing.\nAt least since Roman times, writing has been understood \nas a cognitive, embodied, inventive process. “Writing is both \na mental and a physical activity, setting out thought in vis-\nible images” with the goal of “pass[ing] on information,” \nboth existing and novel, to others. In working with a system \nof symbols—26, in the case of the Roman alphabet, plus \npunctuation marks—humans create and pass on knowledge \nand understanding. Reading, on the other hand, is the crea-\ntive and cognitively challenging process of decoding a text, \nto access that knowledge and articulate it to others. As early \nas 95 CE,\nthe Roman educator Marcus Fabius Quintilianus … \nobserves that writing, unlike speech, has to be acquired \nthrough education… Small children … can acquire \noral language simply by listening to and imitating \nthose around them. But the child cannot acquire writ-\ning ability this way.\nAlso, since ancient times, “the ability to communicate \nwell [has been recognized as] a source of public power” \nand the acquisition of literacy “a means of upward mobil-\nity.” Thus throughout human history, oppressed peoples and \ngroups have been denied the means of acquiring literacy—\nand sometimes the technologies that facilitate the acquisition \nof literacy (e.g., pencils, books), including enslaved people \nin the antebellum U.S. In working with symbols that rep-\nresent speech, students develop as thinkers, in dialog with \nthe symbols that revise their thinking (Murphy and Thaiss \n2020, p. 1–5).\nResearch on and theories of writing instruction since \nthe 1950s have further illuminated the activity of writing, \nas Gold and Hammond (2020) detail. Major emphases in \nthe development of writing instruction and dimensions of \nwriting since then include “cognitive processes” (Bruner \n1960; Rohman 1965; Emig 1971; Murray 1972), the self-\nexpressive and personal development potential of writing \n(Elbow 1973, Macrorie 1970, 1985, Murray 1968, 2004, \nWard 1994), and the rhetorical dimensions of writing, \nwith specific attention to audience (Burke 1950; Corbett \n1965; Perelman and Oblrechts-Tyteca 1958; Murphy 1972; \nKinneavy 1971). Inspired by linguist and psychologist Lev \nVygotsky’s understanding of language development as \ncentral to developing consciousness and personality, Emig \n(1977) made a profound contribution, with widespread \nimpact, in distinguishing between “learning to write”—a \nform of writing instruction that focuses on the final product, \na finished essay with a polished style and conventional \ngrammar—and “writing to learn,” using writing as a tool \n AI & SOCIETY\nfor thinking, emphasizing process over product (Emig 1977, \np. 126).\nThe development of Writing Across the Curriculum \n(WAC) and Writing in the Disciplines (WID) programs and \nresearch in the U.K. and the U.S. in the 1980s and 1990s, \nnotably the founding of the WAC Clearinghouse in 1997 \n(Gold and Hammond 2020, pp. 294–295), have sought to \nbring these pedagogical insights to writing-intensive courses \noutside of composition programs and literature departments, \nwhere, unfortunately, the focus is all too often on grammati-\ncal correction, without adequate attention to the cognitive, \naffective, rhetorical dimensions of the writing process. \nAccepting that writing takes time—for both expert writers \nand students—is a key consequence of recognizing that writ-\ning is a process through which students learn. Expecting \nstudents to write a strong essay quickly, in a single draft with \nlittle guidance or feedback, is unrealistic and unwise not only \nbecause it circumvents learning, but because it underesti-\nmates the time required to write well and thus fully develop \ntheir thinking.\n4  Writing has a neurological speed limit\nHow much time can AI save us in the writing process? Over \ntime, the development of better writing tools has gradually \nincreased the speed of composition. Humans have inscribed \ntext on stone with chisels, on papyrus and vellum with ink, \non wax tablets with a stylus, and on paper with ink pens \nand carbon pencils. The speed of manual writing technolo-\ngies—actually requiring the use of our hands—has acceler-\nated since the late nineteenth century, first with the revolu-\ntionary invention of typewriters, which democratized access \nto a technology available since the invention of the printing \npress (Murphy and Thaiss 2020, 5–7).\nBuilding on the speed of word processing on personal \ncomputers, speech-to-text transcription nearly eliminates \nneed to use the hands at all. These recent writing tech-\nnologies began to approach the speed of human thought in \nthe drafting process (Alves et al. 2008). Now chatbots can \neffectively generate a text faster than humans can think of, \ntype out, or dictate a similar text (Strijkers and Costa 2011), \nrequiring high-level reading skills to assess that AI output, \nand high-level editing skills to adjust one’s prompt and/or \nedit the AI output.\nClearly, we should not mistake this AI output as equiva-\nlent to human writing. As humans, we should be impressed \nwith ourselves for inventing these high-speed “writing” tech-\nnologies that synthesize and mimic human expertise. But we \ncannot collaborate effectively with or develop them further \nunless we preserve practices of writing—and coding—more \nslowly, with the necessary cognitive struggle involved in \norder to learn and develop expertise.\n4.1  Code sharing, software dependencies, \nand technical debt: efficiency vs. haste\nComputer science got us into this mess and cannot get us \nout. One driver of the mess is software development’s profit-\ndriven emphasis on speed and efficiency—or, less favorably, \nhaste—in automating every possible form of human labor. \nThe very fast pace of software development is enabled and \nlimited by the widespread practice of code sharing, which \ncreates two related problems, software dependencies and \ntechnical debt. Both have large implications for the future \nof reading and writing in the age of AI, as I briefly outlined \nin “AI, Expertise and the Convergence of Writing and Cod-\ning” (MacArthur 2023).\nProgrammers and software developers routinely \nshare, use and adapt code written by others, from private \nand public repositories, to perform simple and complex \ntasks. This practice relies on open-source and open-access \nprinciples—that research and development progress faster \nwhen methods and findings are freely shared. Code shar -\ning radically increased with the invention of the Internet, \nwhere more code could be shared publicly (Weber 2004). \nSoftware dependencies arise when programmers incorpo-\nrate an existing open-source library or package into new \nsoftware. As of January 2023, GitHub—the world’s larg-\nest repository of open-source code, founded in 2007 and \nacquired by Microsoft in 2018—had more than 100 mil-\nlion developers (Dohmke 2023). Software dependencies can \nbreak software. This can happen with software and system \nupdates, due partly to the profit-driven planned obsolescence \nof all apps, devices and programs, which keeps us buying the \nlatest versions and models. But it can also happen whenever \nlibraries or packages are updated, unpublished, or no longer \nmaintained. A simple software update to the cybersecurity \nsoftware CrowdStrike caused a global IT failure in July \n2024, costing Fortune 500 companies at least 5 billion USD \n(Fung 2024). The risks posed by software dependencies are \ncompounded by a related problem called technical debt.\nTechnical debt (TD) is caused by writing (or borrowing) \nsomewhat sloppy, yet functional, code that may cause prob-\nlems down the line. The uninitiated—including the techno-\nphobic and technophilic—may believe that code, to function, \nmust be logically precise and bug-free, and thus that writing \ncode must be a more exacting task than writing text. In some \nways, it is. But just as our technocentric culture underes-\ntimates and undervalues the expertise and cognitive work \ninvolved in the writing process, so we may overestimate the \nlogical precision involved in programming. To a perhaps \nsurprising degree, sloppy code can function—until it cannot, \nsometimes with big consequences.\nAI & SOCIETY \nDefined with remarkable vagueness in the Frontiers of \nComputer Science “as ‘not quite right code which we post-\npone making … right’ …. TD [Technical Debt] describes \nthe financial implications of choosing to develop the soft-\nware differently than how [it] should be done” (Aversano \net al. 2023). Vee has argued that, by releasing GPTs without \nadequate pre-mortem analysis, tech companies have effec-\ntively imposed a huge burden of technical debt on the public \n(Vee 2024).\nAversano et al. enthusiastically describe a new machine \nlearning approach designed to look for “seven terrible pro-\ngrammer errors” that lead to TD and that have “disastrous \nconsequences … including … high costs and production \nslowdown”—stockholders beware!—“for the correction \nand maintenance of errors and deficiencies of the code.” \nThe seven errors are: “Bugs and possible errors (i), Coding \nprinciples breached (ii), Redundancy of the program (iii), \nInadequate coverage of consumers modules (iv), Lack of \nstructure diffusion (v), Pattern of Spaghetti (vi), [and] Exces-\nsive amounts of comments (vii).” Some of these errors sound \nquite like sloppy writing—inelegant, redundant, disorgan-\nized code that is longer than it needs to be, with inattention \nto purpose, audience, etc.\nThe authors hope that their new machine learning meth-\nods can further automate the identification of technical debt. \nThis might sound like a wonderful goal, given the complex-\nity of many programs and the attendant number of lines \nof code. Taken too far, it would also realize a dangerous \nfantasy, relieving developers of the responsibility to criti -\ncally review their own code. Code review has evolved to \nrely on more and more automation, yet human programmers \nhave continued to play a crucial role, as with Google’s code \nreview process (Sadowski et al. 2018). Recent assessments \nof the accuracy of automated code review report quite mixed \nresults, noting that ChatGPT lacks skill in understanding \nthe larger context of a program, as opposed to a small bit \nof a code, as well as with making recommendations and \nimplementing changes (Tufano et al. 2024). As with writing, \nso with coding: human programmers have a richer sense of \ncontext than chatbots.\nRecent research on professional software developers’ \nuse of AI programming assistants notes many reasons why \ndevelopers reject or avoid using AI-generated code: con-\ncerns about code quality (including defects, security issues, \nand performance problems), difficulties in understanding \nand debugging generated code, time wasted modifying AI-\ngenerated solutions, and privacy concerns about tools having \naccess to their proprietary code (Liang et al. 2024). Never-\ntheless, the fantasy lives on. With chatbots writing the code, \nautomating software testing, identifying technical debt, and \nperforming code review, where does that leave the software \ndeveloper? What work would be left for them to do? Like \nhumanities faculty struggling to adapt to teaching writing \namid the omnipresence of chatbots, computer science educa-\ntors have been trying to figure that out (Becker et al. 2023; \nZastudil et al. 2023).\nIn 2023, as readers will remember, many think pieces \npredicted the end of coding as well as writing. Some were \nfearful, some exultant or hopeful. Notable among them was \nMatt Welsh’s “The End of Programming,” which appeared \nin the prestigious Communications of the ACM [Association \nfor Computing Machinery] in January 2023. In forecasting \nthe future of computer science education, he emphasized \nradical change:\nProgramming will be obsolete  [his boldface] …. of \ncourse all programs in the future will ultimately be \nwritten by AIs, with humans relegated to, at best, a \nsupervisory role….\nthe field will look like less of an engineering endeavor \nand more of an educational one; that is, how to best \neducate the machine [his italics], not unlike the sci-\nence of how to best educate children in school. Unlike \n(human) children, though, these AI systems will be \nflying our airplanes, running our power grids, and pos-\nsibly even governing entire countries[.] (Welsh 2023)\nA major misconception in Welsh’s vision of this future \n(not to diminish problems with the automation of com-\nmercial flight, distribution of electricity, and governance) \nis that he assumes supervising and educating are inferior \nto programming, and perhaps easier (“relegate,” accord-\ning to Oxford Dictionaries, means “to consign or dismiss \nto an inferior rank or position”). Supervising and teaching \na person or machine are more difficult than programming \nalone, because they require higher levels of expertise in both \nprogramming and communication skills. Welsh is correct \nthat computer science education must change, but he mis-\nunderstands how, which is by emphasizing communication, \nincluding critical reading and writing skills, alongside high-\nlevel programming knowledge.\nTrends in the software industry, unfortunately, reflect the \nsame short-sightedness and hallmarks of “the curse of exper-\ntise” (Hinds 1999): experts tend to forget how they learned \nwhat they know, underestimate the work involved in a given \ntask, and thus sometimes struggle to teach novices. Entry-\nlevel programmers hired as “GPT monkeys” (a term with \na long history of offensive application to human workers \nassigned to low-skill tasks) are now struggling to gain the \nnecessary expertise to work with GPTs:\nBecause A.I.-generated code is riddled with errors that \nare hard to spot without experience, senior developers \nsometimes find it easier to generate and edit it them-\nselves than to let it fall to a junior programmer.…. the \nsame conundrum [exists] with other skills in which \nwork was being automated, like surgery and financial \n AI & SOCIETY\nanalysis: Beginners need more expertise to be useful, \nbut getting the type of experience that would normally \nhelp build that expertise is becoming harder (Kessler \n2024).\nThis should worry us. Already for most of us, the pro-\ngrams we rely on are black boxes, even if the code is open-\nsource, rather than proprietary. Enthusiasm for automation \nin general, and AI output in particular, must not blind us to \nthe need to develop expertise—in fields from software devel-\nopment to surgery, from history and religious studies to \nfinance and political science. If future software developers \ndo not really understand AI-generated code, yet rely on it to \nrun a program as long as it seems to work for now, technical \ndebt increases.\n4.2  Automation bias and the risks of rhetorical debt\nWhat do the dangers of software dependencies and technical \ndebt portend for the future of professional writing? I call it \nrhetorical debt, which might be defined as broadly as techni-\ncal debt: “not quite right [writing] which we postpone mak-\ning … right.”8 To be more precise, rhetorical debt accrues \nwhen, writing in haste, we do not critically edit a text to \nsuit our audience, purpose, genre and context, or when we \nlack the expertise to so edit it—and more recently, when we \nuncritically use AI-generated text. Sloppy writing has always \nbeen with us. We write, and even publish it, if the writing \napproximates our purpose, context and genre well enough, \nand if our audience lets us get away with it. AI encourages \nforms of text sharing and text generation with close paral-\nlels to code sharing, with the attendant problems of software \ndependencies and technical debt.\nThe hazards of overlooking rhetorical debt in writing can \nbe as expensive and consequential as overlooking technical \ndebt in programming. In school, over relying on generative \nAI as a ghostwriter can lead to personal costs—accusations \nof plagiarism, failed courses, and worst of all, the failure \nto learn. In the workplace in the era of generative AI, rhe-\ntorical debt will become hazardously more common if we \nfail to build students’ critical reading, writing and editing \nskills, which rely in turn on disciplinary expertise. Already, \nresearch suggests that when knowledge workers place \n“higher confidence” in AI, they do less critical thinking (Lee \net al. 2025).\nWhy? If we are novices  quickly trying to  assess \nwhether AI output matches our purpose, context, genre and \naudience—or when we are experts in our fields, but prompt \nAI to generate text outside our areas of experience—we \ntrust AI because of automation bias (again, the tendency \nto trust machine-generated output because we overvalue \nmachine “intelligence” and undervalue the human kind) and \nbecause of our conditioned tendency to trust fluency. We \nmight call this tendency a  fluency fallacy : the erroneous \nattribution of accuracy, expertise and authority to a given \ntext or speaker based on the use of normative grammar, \nidiomatic syntax and vocabulary, and conventional style. \nTo emphasize the distinction between natural language \ngeneration and natural language understanding, a chatbot \ndoes not know what it seems to know, and it does not know \nwhich of the things it seems to know are what we need it to \n(re)generate in (somewhat) novel combinations. A chatbot \nhas “formal linguistic competence” without “functional \nlinguistic competence,” an understanding of the physical-\nsocial-cultural space in which humans operate (Mahowald \net al. 2024).9 Yet these tools are designed to sound confident, \nand we are worrisomely prone to trust confident fluency, in \nAI and people (Garry et al. 2024); the “con” in “con man” \nstands, of course, for confidence (see Herman Melville’s \n1857 novel The Confidence Man).\nThe popularity  of  the term “slop,” as applied to \nAI-generated text, might give us hope that the fluency \nfallacy is on its way out—perhaps we will learn not to \nequate fluency with expertise. As indicated above, I am \nnot optimistic, as the current generation of seasoned \nprofessionals is replaced by people who are being educated \nwith the omnipresent risk of overreliance on AI, amid the \nfrenetic, profit-driven pace of capitalist economies. At \npresent, equating surface polish with deep knowledge is also \na symptom of a stubborn cultural myth: that what English \nprofessors and writing teachers value most, and mercilessly \nharangue students about, is grammar. The polished text \ngenerated by LLM-powered chatbots has surfaced this \nmisconception. In English-dominant academic and corporate \ncultures where grammar is policed and accented English \nis implicitly or explicitly disparaged (Zheng 2025), we \nhave come to associate expertise with normative grammar, \nidiomatic syntax and vocabulary and slick delivery. The \nfluency fallacy is not new, but generative AI freshly reveals \nhow pernicious it can be. It takes both expertise and attention \nto recognize expertise, to notice factual inaccuracies and \nother lapses, and to see through a con artist’s or a chatbot’s \nfluency—sleek as a recalled Tesla.\n8 By rhetorical debt, I do not mean documentation debt, referenced \nabove and coined by Bender et al. (2021), which accrues when oth-\ners’ intellectual property is uncredited.\n9 A good illustration of a chatbot’s functional linguistic incompe-\ntence is GPT-3’s suggestions, in response to the prompt “‘Get your \nsofa on the roof,’ which included getting a “a very strong friend \nto help carry it up a ladder” and “break[ing] the windows to make \nroom” (Collin, Wong, et  al., cited in Mahowald et  al. 2024, p. 14). \nWhile more recent versions of GPTs have been trained to avoid such \nmistakes, the immediate, evolving context in which people func-\ntion will continue to pose a serious challenge to a language-based \nmachine, without humans providing a great deal of context through \nprompting or curated “training data.”\nAI & SOCIETY \nWe have already seen at least two catastrophes result from \noutstanding rhetorical debt: the Boeing 737 Max 9 disas-\nters, which killed 346 people in 2018 and 2019, and which \nstands to be “the most expensive corporate blunder ever,” \ncosting the company at least $20 billion (Isidore 2020). A \nfailure to revise the flight manual was fundamentally at fault. \nAlthough the manual was not written by a chatbot, human \ntechnical writers at Boeing did not adequately consider, in \nediting the existing manual, a new audience: undertrained \npilots flying for cheap airlines in the developing world. And \nshockingly, the manual failed to even mention a new auto-\nmated feature, the MCAS (Maneuvering Characteristics \nAugmentation System), because Boeing assumed it would \nnever become a problem for the highly experienced pilots \nwho, in the past, typically flew their planes (Langewiesche \n2019, 2021).10 The MCAS—which the Lion Air and Ethio-\npian Airlines pilots did not know existed—drove the two \nplanes into the sea and the earth, in Indonesia in 2018 and \nEthiopia in 2019. This is a flat-out pathological model of \n“testing” a document for rhetorical debt: only when 346 \npeople died did we learn that the flight manual needed to \nbe revised.\nIn 2024, 177 of the same airplanes had to be grounded \nwhen a door plug burst open at 16,000 feet (Coy 2024). \nThese most recent errors were design failures, not signs of \nrhetorical debt, and fortunately they did not kill anyone. \nBut in effect, Boeing’s failure to revise the flight manual \nadequately and to safely coordinate the manufacture and \nmaintenance of its Max 9s are part of the same warp-speed, \nprofit-driven technological development culture that places \nundue confidence in automation and pays too little attention \nto the rhetorical situation, including the various audiences \ninvolved in assembling and flying their airplanes. Tesla is \nin the same camp. In December 2023, 2 million Teslas were \nrecalled due to problems with drivers’ overreliance on and \nmisunderstanding of autopilot features, and another 2.2 \nmillion were recalled in February 2024 because “the font \non the warning lights panel was too small to comply with \nsafety standards” (Ewing et al. 2023; Gross 2024). The Tesla \nCybertruck has had eight recalls since it came on the market, \nmost recently in March 2025 (“Federal Regulators…” 2025). \nOur half-blind faith in advanced automation has normalized \ntesting new technologies, and attendant, inadequate docu-\nmentation, by risking human life.\nHow to safely exit this crash course? Arguably, we can-\nnot improve upon Licklider’s (1960) conception, of the \nappropriately limited role for machines in the widely cited \n“Man-Computer Symbiosis”: “In the anticipated symbi-\notic partnership, [humans] will set the goals, formulate the \nhypotheses, determine the criteria, and perform the evalu-\nations. Computing machines will do the routinizable [sic] \nwork that must be done to prepare the way for insights and \ndecisions in technical and scientific thinking” (p. 1). No \nmatter how sophisticated AI becomes, we make goal-setting \nand evaluation “routinizable work” at our peril. With human \nsafety and the education of future generations as our goals, \nwe must now persuade humans, at school and at work, to \nslow down enough to perform rhetorical analysis to inform \nour decisions. And we must reinvest in humanities-based \neducation, to teach the essential skills of critical reading, \nwriting and editing, all of which involve rhetorical analysis. \nAmple economic evidence of this need is hiding in plain \nsight.\n4.3  The increased economic value of expert writing \nand advanced writing skills\nAbove all, my argument is motivated by a crucial fact with \nfar-reaching consequences: the rise of generative AI has \nmade human-expertise-captured-in-prose, and critical read-\ning, writing and editing skills within and across disciplines, \nmore valuable and consequential than at any time since the \ninvention of the printing press. This might seem anti-intu-\nitive, given that chatbots generate seemingly expert, fluent \nprose so easily. However, signs of the increased monetary \nvalue of human writing, and economic demand for human \nbeings with advanced writing skills, are everywhere.\nHaving plundered the public domain for free high-quality \nhuman writing as “training data,” as well as a great deal \nof prose under copyright, AI companies desperately need \nmore (“AI is Setting Off a Great Scramble for Data” 2023). \nThese companies have a history of cutting corners to avoid \ncompensating authors or publishers for human writing (Metz \n2024) and as that practice has come under legal threat, they \nbegan to pay millions of dollars to media companies to \nlicense their content as training data (De Vynk 2023; David \n2024). Plaintiffs like The New York Times, which turned \ndown offers from OpenAI to license its content, as well as \ngroups of authors and other media companies, have sued Big \nTech for violating fair use (Mullin 2023; Baker Hostetler \n2025). The U.S. Copyright Office (2025) is working on, \nbut has not yet released, guidance on these questions. Aca-\ndemic publishers are also capitalizing on the value of expert \nwriting as training data, for example, in the controversial \ndeal made in July 2024 by Taylor & Francis, which allowed \nMicrosoft to use its authors’ work as training data, at a cost \nof 10 million USD just in the first year (Battersby 2024).\n10 Langewiesche ultimately blames the undertrained pilots for the \ncrashes, and the airlines for hiring them. I disagree—how can pilots \nbe blamed for the fatal decision of an automated feature they did not \nknow existed? I recently overheard, at Denver International Airport, \na group of older pilots discussing younger pilots’ ignorance of flight: \n“They’re not pilots, they’re system operators.” If a feature of a system \nis not mentioned or documented, operators cannot run that system \neffectively.\n AI & SOCIETY\nSome hope that AI can develop further without relying \non more human-generated text, code, etc., as training data, \nbut so far this path seems doomed to failure. AI degenerates \nwhen trained on its own output (Shumailov et al. 2024). \nRecent efforts to train “[t]he newest and most powerful \ntechnologies—so-called reasoning systems from compa-\nnies like OpenAI, Google and … DeepSeek” through rein-\nforcement learning, rather than on human-generated writing \nand code, have resulted in “hallucination rates as high as \n79%” (Metz and Weise 2025). The developers do not under-\nstand why. And even if they solve this formidable problem, \nexpert human judgment is needed to understand and apply \nAI output.\nAnd so knowledge workers with advanced writing and \ncommunication skills are in high demand, to collabo-\nrate with and develop AI further. In 2024, AI companies \nbegan hiring college graduates at 20–40 USD/h to write for \nand train AI (Lu 2024). Chinese developers of DeepSeek \nemployed literature majors to work alongside engineers, in \ndeveloping the comparatively cost-efficient, partly open-\nsource chatbot that rivals ChatGPT’s functionality 11 and \nthus rattled Silicon Valley in January 2025 (Mozur 2025). \nIn the U.S., the 2025 National Association of Colleges and \nEmployers Job Outlook Survey ranked “problem-solving” \n(88.3%), teamwork (81%), and “communication skills (writ-\nten)” (77.1%) as the top three in-demand skills, while “com-\nmunication skills (verbal)” (69.3%) are valued nearly as \nmuch as “technical skills” (73.2%) (Gray and Koncz 2025).12 \nBecause writing is a critical, high-level cognitive skill that \ninvolves problem-solving, audience analysis, and revision \nin response to constructive feedback, it also promotes the \nhighly valued skill of teamwork.\n5  How should the teaching of writing adapt \nto AI?\nThe invention of the printing press surely put many \nscribes out of work. At the same time, print technologies \nincreased the economic value of higher-level literacies \nand democratized access to the means of developing those \nliteracies: books. As Columbia English professor and former \nMicrosoft engineer Dennis Yi Tenen notes,\nat one time, the mere fact of being literate would have \nbeen considered exceptional. One could build a career \nof simply reading and interpreting texts for an illiter -\nate public. With mass literacy, skills of reading and \nwriting migrate from the upper to the lower bounds of \nintellectual work (Tenen 2024, p. 38).\nSomething similar is happening now: by automating the \ngeneration of increasingly sophisticated text and code, GPTs \nare increasing the value of higher-level expertise which is, \nconversely, necessary to collaborate with GPT applications \neffectively.\nUnlike the printing press, however, AI poses significant \nrisks to the development of writing and coding literacies, \neven as they democratize access to all kinds of expertise. \nWriting, reading and understanding a text that represents \ndisciplinary expertise are very challenging, time-intensive \ncognitive tasks. Critically assessing that text is also a very \nchallenging, time-intensive cognitive task. Prompting a chat-\nbot to generate slop is quick and easy. Prompting it to gen-\nerate context-appropriate text, critically reading it to judge \nwhether it has done so, and further editing either the prompt \nor the output to match one’s goals, is not.\nWearing his English professor hat, Tenen first “argue[s] \nthat our understanding of AI can and must become more \ngrounded in the history of the humanities.” However, fol -\nlowing an uneven “history of machine writing” 13 (p. 5), \nTenen betrays deep automation bias when he ponders, all \ntoo briefly, how AI might affect writing practices in higher \neducation:\nI am certain most students will be using AI-enabled \nservices to “augment” their academic performance in \nthe future, if they haven’t done so already: to write \nlast-minute papers or to create ersatz writing samples \nfor graduate school applications. And perhaps they \nshould! Just as erudition now involves the assistance \nof powerful search engines, writing without automated \nassistance may one day seem quaint…. Perhaps my \nmistake again was in ever treating the activity of writ-\ning as a product of a single intellect. Shouldn’t any \n“original” contribution also follow the templates of \nestablished norms? And if so, why can’t some of that \nwork be automated? (Tenen 2024, p. 138)\nLike other techno-determinists, Tenen claims to \npragmatically embrace the inevitable. More disturbing is \nhis conflation of AI-as-ghostwriter with AI-as-writing-\nand-research-assistant. Such a blasé endorsement of \nAI plagiarism by an Ivy League English professor is \nirresponsible, to say the least. Because the biggest losers, \nwhen a student claims AI output as their own writing, are the \nstudent and everyone that student goes on to work for and 11 In comparative testing of several chatbots with students, I have \nfound DeepSeek to be superior to ChatGPT, on tasks from generating \nShakespearean sonnets on novel topics to advising specific majors on \nlocal job searches.\n12 “Initiative” (73.7%) and “Strong work ethic” (73.2%) ranked \nfourth and fifth.\n13 Tenen’s book includes a chapter on “Template Culture,” about the \nexplosion in the nineteenth century of how-to-write-x-genre guides, \nin which he largely ignores the entire field of genre studies, and thus \npasses off the chapter’s argument as novel, which it is not.\nAI & SOCIETY \nwith. Using a chatbot to “write a last-minute paper” means \nthat student learns virtually nothing from that assignment. \nAnd who would want to accept and invest in a graduate \nstudent, in any field, on the basis of an “ersatz” writing \nsample generated by a chatbot?\nTo protect and motivate student learning in the age of \nAI, some writing studies scholars hope to preserve writing \npractices in higher education that differ from those in the \nworkplace. At the same time, they express anxiety about \nsuch efforts. The editors of the WAC Clearinghouse collec-\ntion TextGenEd: Teaching with Text Generation Technolo-\ngies “believe that these [generative AI] tools are likely to \nbe adopted rapidly in certain sectors of the writing econ -\nomy in the coming months and years, and fostering student \nunderstanding of them is important” (Vee et al. 2023). Fair \nenough. Accordingly, they aim to “help writing teachers \nto integrate computational writing technologies into their \nassignments.” Their book, published with extraordinary \nspeed and quickly followed by a second volume in 2024, \nincludes many assignments intended to promote AI literacy. \nEven so, the editors acknowledge that\n[t]o some extent, we have a gap in values and prac-\ntices [between the workplace and higher education] \n… Many of us value that gap, but … it can devalue \nour work in higher education, as we are accused of \nnot preparing students for the writing they will “actu-\nally” do…. Will higher education be able to discipline \nAI to bring it into alignment with academic econo-\nmies of authorship? Or, as writers adapt to working \nwith large language models, will AI destabilize the \ndetente between academic and professional economies \nof authorship and expose the artificiality of writing \npractices in the academy? (Vee et al. 2023)\nThe problem of rhetorical debt should make it clear that, \nindeed, we must “discipline AI to bring it into alignment \nwith academic economies of authorship.” That “gap in \nvalues” is necessary, in fact, to “prepar[e] students for the \nwriting they will ‘actually’ do” in the workplace. Yet these \neditors sound apologetic about “the artificiality of writing \npractices in the academy.” That is not to criticize the impor-\ntant mission of TextGenEd. The editors’ defensive position is \na symptom of the nonsensically inferior status of writing and \nthe humanities in our technocentric culture, which predates \ngenerative AI (Pynchon 1984).\nMore recently in PMLA, the premier literary studies \njournal in the U.S., Kirschenbaum and Raley argue that the \nrapid pace of AI development prevents faculty from\ndevis[ing] a [pedagogical] strategy or method that \ncan be ported to all institutional situations, or even to \nall assignments. For an inherently fluid technological \nsituation, with new applications and implementations \nseemingly by the day, and the models themselves con-\ntinuously evolving, there can be no stable and singular \npath forward (Kirschenbaum and Raley 2024).\nAside from the perennial need to adapt teaching methods \nto individual students’ skill levels and needs, educators do \nnot need to reinvent their approach to teaching reading and \nwriting. In focusing so much critical attention on the techni-\ncal features and growth patterns of “new applications and \nimplementations… and … [evolving] models,” Kirschen-\nbaum and Raley obscure and misconstrue the actual chal-\nlenges faced by any instructor who teaches writing. Though \nstudents and instructors must understand that AI does not \nproduce infallible, appropriate output, they do not need to \nunderstand precisely how GPTs arrive at their probabilistic \npredictions of the next token in order to assign AI platforms \nan appropriately limited role in their education. Let’s say \na professor learned recently that NotebookLM can quickly \ncreate a podcast based on any text fed to it, complete with \ndialog and bad jokes. There is no reason that this applica-\ntion’s existence should impact her basic teaching methods \nor strategies. Neither do educators need to coin new terms \nlike “humanics,” propagated by Northeastern University \nPresident Joseph E. Aoun, in another effort at technocen-\ntric “reinvention,” to prepare undergraduates foolish enough \nto major in “humanities or basic sciences” to nevertheless \n“pursue careers in technology” (Aoun 2024).14 Communica-\ntion, reading, and writing are old words and ideas, rich in \nmeaning and history. They have renewed value and utility \nin the age of AI.\nNo matter how fluent, charming, powerful or authoritative \nthe latest AI tools or platforms might seem, to the extent that \nthey are allowed into classrooms at all, teaching methods \nand strategies must be guided by one simple goal: to culti-\nvate students’ developing expertise and reading and writing \nskills in a given discipline—with all of the stimulating chal-\nlenges that are necessarily involved (Bean and Melzer 2021). \nTo do that, AI must be limited to a tutorial role, whether \nthe AI in question generates podcasts, texts, or fireworks. A \nthought experiment may help illustrate this point. If an army \nof hyperliterate, well-intentioned, fallible humans suddenly \nvolunteered to help every college student in the world with \ntheir writing assignments, would instructors suddenly accept \neach volunteer’s writing in place of the student’s writing?\nIf we hope that future generations can grow up to \nidentify and avoid rhetorical debt—to critically assess and \nedit AI output from a position of disciplinary expertise, \nkeeping in mind their distinct purpose, audience, genre and \ncontext—they will need to keep learning to write without a \nghostwriter (or ghost coder) looking over their shoulders, \n14 Northeastern University’s enthusiastic embrace of AI technologies \nled to a prominent student complaint about a professor’s over reliance \non AI to develop teaching materials. See Hill (2025).\n AI & SOCIETY\nconstantly promising to generate the “right” answer and \ntempting students to claim that answer as their own, without \nhaving learned a thing. To develop disciplinary expertise, \nstudents must go through the struggles and rewards of \nlearning. And they must be encouraged to value their own \nintellectual property, so that they think twice before feeding \ntheir writing, or anyone else’s, to a chatbot hungry for more \n“training data”—even if the user agreement assures us that \ntheir input will not be retained for such purposes.\nIn the academy, we do not let students outsource their \nknowledge work, because their goals should be to learn to \nwrite, edit, and think critically, not to cut corners and cheat \nthemselves out of learning. The “gap in values” between the \nacademy and the corporate workplace is not an awkward \nartifice to apologize for. It is warranted. It has integrity. And \nit should not be seen by anyone as old-fashioned, outmoded \nor unnecessary. Educators need not apologize for teaching \nstudents the writing and editing skills, the rhetorical aware-\nness, and the critical judgment that their future employers \nneed them to have—and which students, and current knowl-\nedge workers, also need to acquire to lead a satisfying life, \nin which they can develop and apply their unique talents. \nIf they fail to acquire these skills, many students will grow \nup to be unemployable. Others will become hazardously \nincompetent employees. They will copy and paste prompts \nthey cannot write and do not understand into machines that \nin turn produce outputs they cannot evaluate. And they will \napply these outputs to a wide range of tasks, with risks they \ncannot assess or even imagine.\nAcknowledgments I would like to thank Christopher Grobe for his \ncrucial, insightful comments on a draft of this article. Audiences at the \nComputers and Writing conference at UC Davis in 2023, and at Stan-\nford University's Center for Spatial and Textual Analysis and Queen \nMary University of London in 2024, also provided helpful feedback \non presentations of some of these ideas. My colleagues at UC Davis, \nespecially Lisa Sperber, Chris Thaiss, Carl Whithaus, Colin Milburn, \nJoe Dumit, Sophia Minnillo, Nicholas Stillman, and Kenji Sagae, also \nprovided helpful feedback on some of these ideas. On the computer \nscience side, thanks also to Michael Neff and David Russell Taylor, \nand my former student Kevin Guan.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nReferences\nAI is Setting Off a Great Scramble for Data (2023) The Economist. \nhttps:// www. econo mist. com/ busin ess/ 2023/ 08/ 13/ ai- is- setti ng- off-\na- great- scram ble- for- data#. Accessed 20 Sept 2023\nAlves RA, Castro SL, Olive T (2008) Execution and pauses in writing \nnarratives: processing time, cognitive effort and typing skill. Int \nJ Psychol 43(6):969–979. https:// doi. org/ 10. 1080/ 00207 59070 \n13989 51\nAoun JE (2024) How higher ed can adapt to the challenges of AI. \nThe Chronicle of Higher Education. https:// www. chron icle.  \ncom/ artic  le/ how- higher-  ed- can- adapt-  to- the- chall enges-  of- ai. \nAccessed 20 Sept 2024\nAversano L et al (2023) Forecasting technical debt evolution in soft-\nware systems: an empirical study. Front Comput Sci 17:173–\n210. https:// doi. org/ 10. 1007/ s11704- 022- 1541-7\nBaack S (2024) A critical analysis of the largest source for generative \nAI training data: common crawl. In: Association for computing \nmachinery, proceedings of the ACM conference on fairness, \naccountability and transparency (FAccT ’24), pp 2199–2208. \nhttps:// doi. org/ 10. 1145/ 36301 06. 36590 33\nBaker Hostetler (2025) Case tracker: artificial intelligence, copy -\nrights and class actions. https:// www. baker  law. com/ servi ces/ \nartifi  cial- intel ligen ce- ai/ case- track er- artifi  cial- intel ligen ce- \ncopyr ights- and- class- actio ns/. Accessed 15 Mar 2025\nBattersby M (2024) Academic authors “shocked” after Taylor & \nFrancis sells access to their research to Microsoft AI. The Book -\nseller. https:// www. thebo oksel ler. com/ news/ acade mic- autho \nrs- shock ed- after- taylo r-- franc is- sells- access- to- their- resea rch- \nto- micro soft- ai. Accessed 4 Aug 2024\nBean J, Melzer D (2021) Engaging ideas: the professors’ guide to \nintegrating writing, critical thinking, and active learning in the \nclassroom. Jossey-Bass, Hoboken\nBecker BA et al. (2023) Programming is hard—or at least it used to \nbe: educational opportunities and challenges of AI code genera-\ntion. In: Proceedings of the 54th ACM technical symposium \non computer science education, pp 500–506. https:// doi. org/ 10. \n1145/ 35459 45. 35697 59\nBender EM et al. (2021) On the dangers of stochastic parrots: can \nlanguage models be too big? In: ACM conference on fairness, \naccountability, and transparency (FAccT '21), pp 610–623. \nhttps:// doi. org/ 10. 1145/ 34421 88. 34459 22\nBitzer LF (1968) The rhetorical situation. Philos Rhet 1(1):1–14. \nhttp:// www. jstor. org/ stable/ 40236 733\nBrock K (2019) Rhetorical code studies: discovering arguments in \nand around code. University of Michigan Press, Ann Arbor\nBruner JS (1960) The process of education. Harvard University \nPress, Cambridge\nBurke K (1950, 1969) A rhetoric of motives. University of California \nPress, Berkeley\nCorbett EPJ (1965) Classical rhetoric for the modern student. Oxford \nUniversity Press, Oxford\nCoy P (2024) The scariest part about the Boeing 737 Max 9 blowout. \nThe New York Times. https://  www. nytim es. com/ 2024/  01/ 10/ \nopini on/ boeing- 737- max- alaska- japan- airli nes. html. Accessed \n30 Mar 2024\nDavid E (2024) OpenAI’s news publisher deals reportedly top out \nat $5 million a year. The Verge. https:// www. theve rge. com/  \n2024/1/ 4/ 24025 409/ openai- train ing- data- lowba ll- nyt- ai- copyr \night. Accessed 20 Sept 2024\nDe Vynk G (2023) OpenAI strikes deal with AP to pay for using its \nnews in training AI. The Washington Post. https:// www. washi  \nngton post. com/ techn ology/ 2023/ 07/ 13/ openai- chatg pt- pay- ap- \nnews- ai/. Accessed 15 Aug 2023\nAI & SOCIETY \nDohmke T (2023) 100 million developers and counting. GitHub \nBlog. https:// github. blog/ news- insig hts/ compa ny- news/ 100- \nmilli on- devel opers- and- count ing/. Accessed 20 Sept 2024\nDumit J, Roepstorff A (2025) AI hallucinations are a feature of LLM \ndesign, not a bug. Nature 639:38. https:// www. nature. com/ artic \nles/ d41586- 025- 00662-7. Accessed 10 Mar 2025\nDyos S (2025) Sam Altman says the kid he’s expecting soon will \nnever be smarter than AI. Fortune. https:// fortu ne. com/ 2025/  \n01/ 18/ sam- altman- openai- kid- smart er- than- agent ic- ai- abili ty- \nskills/. Accessed 25 Jan 2025\nEcclesiastes 1:9. The New Oxford Annotated Bible (3rd ed.) \n(450~150 BCE, 2001 CE). Coogan M et al (eds) Oxford Uni-\nversity Press\nElbow P (1973, 1998) Writing without teachers, 2nd edn. Oxford Uni-\nversity Press, Oxford\nEldagsen B (2023) Interview. National Public Radio. https:// www. npr. \norg/ 2023/ 04/ 20/ 11711 33628/ photo graph- or- promp togra ph- artist- \nquest ions- impli catio ns- of- ai- gener ated- imagesAccessed 21 Apr \n2023\nEmig J (1971) The composing process of twelfth graders. National \nCouncil of Teachers of English\nEmig J (1977) Writing as a mode of learning. Coll Compos Commun \n28(2):122–128\nEwing J, Metz C, Taylor DB (2023) Tesla recalls autopilot software \nin 2 million vehicles. The New York Times. https://  www. nytim \nes. com/ 2023/ 12/ 13/ busin ess/ tesla- autop ilot- recall. html. Accessed \n20 Mar 2024\nFederal regulators recall nearly all Tesla Cybertrucks over faulty exte-\nrior panel (2025). PBS News. https://  www. pbs. org/ newsh our/ \nnation/ feder al- regul ators- recall- nearly- all- tesla- cyber trucks- over- \nfaulty- exter ior- panel#: ~: text= The% 20Cyb ertru ck% 2C% 20whi \nch% 20Tes la% 20beg an,drive% 20whe els% 20to% 20lose% 20pow \ner. Accessed 21 Mar 2025\nFung B (2024) We finally know what caused the global tech outage—\nand how much it cost. CNN. https:// www. cnn. com/ 2024/ 07/ 24/ \ntech/ crowd strike- outage- cost- cause/ index. html. Accessed 4 Sept \n2024\nGarry M, Chan WM, Foster J, Henkel LA (2024) Large language mod-\nels (LLMs) and the institutionalization of misinformation. Trends \nCogn Sci 28(12):1078–1088. https:// doi. org/ 10. 1016/j. tics. 2024. \n08. 007\nGlynn A (2024) Suspected undeclared use of artificial intelligence in \nthe academic literature: an analysis of the Academ-AI Dataset. \narXiv.org. https:// arxiv. org/ abs/ 2411. 15218\nGold D, Hammond JW (2020) Writing instruction in U.S. colleges \nand schools. The twentieth century and the new millenium. In: \nMurphy JJ, Thaiss C (eds) A short history of writing instruction. \nFrom ancient Greece to the modern United States, 4th edn. Rout-\nledge, pp 272–316\nGoodlad L (2022) Rev. of K. Crawford, The Atlas of AI: power, poli-\ntics, and the planetary costs of artificial intelligence. Critical \nInquiry. https:// criti calin quiry. uchic ago. edu/ lauren_ goodl ad_ \nrevie ws_ atlas_ of_ ai/\nGray K, Koncz A (2025) The attributes employers look for on new \ngrad resumes—and how to showcase them. National Association \nof Colleges and Employers. https:// www. nacew eb. org/ about- us/ \npress/ the- attri butes- emplo yers- look- for- on- new- grad- resum es- \nand- how- to- showc ase- them. Accessed 20 Mar 2025\nGrobe C (2023) The programming era: the art of conversation design \nfrom ELIZA to Alexa. Post45. https:// post45. org/ 2023/ 03/ the- \nprogr amming- era/\nGross J (2024) Tesla recalls about 2.2 million electric vehicles over \nwarning light font size. The New York Times. https:// www. nytim \nes. com/ 2024/ 02/ 02/ busin ess/ tesla- recall- us- vehic les. html? smid= \nurl- share. Accessed 20 Mar 2024\nHellström T (2024) AI and its consequences for the written word. Front \nArtif Intell. https:// doi. org/ 10. 3389/ frai. 2023. 13261 66\nHermans F, Aldewereld M (2017) Programming is writing is program-\nming. In: Proceedings of the 1st international conference on the \nart, science, and engineering of programming, pp 1–8. https:// doi. \norg/ 10. 1145/ 30793 68. 30794 13\nHerndon H (2018) AI is a deceptive … term. CI (Collective Intelli-\ngence)is more useful. X.com. https://x. com/ holly hernd on/ status/ \n10682 29047 70715 2384\nHill K (2025) The professors are using ChatGPT, and some students \naren't happy about it. The New York Times. https:// www. nytim \nes. com/ 2025/ 05/ 14/ techn ology/ chatg pt- colle ge- profe ssors. html. \nAccessed 15 May 2025\nHinds PJ (1999) The curse of expertise: the effects of expertise and \ndebiasing methods on predictions of novice performance. J Exp \nPsychol Appl 5(2):205–221\nIsidore C (2020) Boeing’s 737 Max debacle could be the most expen-\nsive corporate blunder ever. CNN. https:// www. cnn. com/ 2020/ \n11/ 17/ busin ess/ boeing- 737- max- groun ding- cost/ index. html. \nAccessed 7 Sept 2024\nJuszkiewicz J, Warfel J, Losh E, Buehl J, Maher JH, Burgess HJ, Men-\nzies T, Brock K, Omizo RM, Clark I, Nguyen MT (2019) Rhetori-\ncal machines: writing, code, and computational ethics. University \nAlabama Press, Tuscaloosa\nKarpathy A (2023) The hottest new programming language is English. \nX. https://x. com/ karpa thy/ status/ 16179 79122 62571 2128? lang= en\nKessler S (2024) Should you still learn to code in an A.I. world? The \nNew York Times. https:// www. nytim es. com/ 2024/ 11/ 24/ busin ess/ \ncompu ter- coding- boot- camps. html. Accessed 25 Jan 2025\nKilpatrick L (2023) Hot take: many believe prompt engineering... X. \nhttps://x. com/ Offic ialLo ganK/ status/ 17400 99060 35737 4356\nKinneavy JL (1971) A theory of discourse: the aims of discourse. \nPrentice-Hall, Saddle River\nKirschenbaum M, Raley R (2024) AI and the university as a service. \nPMLA 139(3):504–515. https:// doi. org/ 10. 1632/ S0030 81292 \n40005 2X\nKitzinger C (2024) OpenAI’s pharmacy? On the phaedrus analogy \nfor large language models. Crit AI. https:// doi. org/ 10. 1215/ 28347 \n03X- 11205 203\nLangewiesche W (2019, 2021) What really brought down the Boe-\ning 737 Max? The New York Times. https:// www. nytim es. com/ \n2019/ 09/ 18/ magaz ine/ boeing- 737- max- crash es. html. Accessed \n14 May 2023\nLee H-PH et al (2025) The impact of generative AI on critical thinking: \nself-reported reductions in cognitive effort and confidence effects \nfrom a survey of knowledge workers. In: Proceedings of the CHI \nconference on human factors in computing systems (CHI ’25). \nhttps:// doi. org/ 10. 1145/ 37065 98. 37137 78\nLiang JT et al (2024) A large scale survey on the usability of AI pro-\ngramming assistants: successes and challenges. In: 46th inter -\nnational conference on software engineering. https:// doi. org/ 10. \n48550/ arXiv. 2303. 17125\nLicklider JC (1960) Man-computer symbiosis. IRE Trans Hum Fac -\ntors Electron 1:4–11. https:// doi. org/ 10. 1109/ thfe2. 1960. 45032 59\nLu Y (2024) Now hiring: sophisticated (but part-time) chatbot tutors. \nThe New York Times. https:// www. nytim es. com/ 2024/ 04/ 10/ \ntechn ology/ ai- chatb ot- train ing- chatg pt. html. Accessed 10 Apr \n2024\nMacArthur M (2023) AI, expertise and the convergence of writing \nand coding. Inside Higher Ed. https:// www. insid ehigh ered. com/ \nopini on/ views/ 2023/ 09/ 28/ ai- and- conve rgence- writi ng- and- cod-\ning- opini on\nMacrorie K (1970, 1985) Telling writing, 4th edn. Boynton/Cook\nMahowald K, Ivanova AA, Blank IA, Kanwisher N, Tenenbaum JB, \nFedorenko E (2024) Dissociating language and thought in large \n AI & SOCIETY\nlanguage models. Trends Cogn Sci 28(6):517–540. https:// doi. org/ \n10. 1016/j. tics. 2024. 01. 011\nMetz C (2024) How tech giants cut corners to harvest data for A.I. \nThe New York Times. https:// www. nytim es. com/ 2024/ 04/ 06/ \ntechn ology/ tech- giants- harve st- data- artifi  cial- intel ligen ce. html. \nAccessed 24 Apr 2024\nMetz C, Weise K (2025) AI Is getting more powerful, but its hallucina-\ntions are getting worse. The New York Times. https:// www. nytim \nes. com/ 2025/ 05/ 05/ techn ology/ ai- hallu cinat ions- chatg pt- google. \nhtml. Accessed 6 May 2025\nMickle T, Metz C, Grant N (2023) The Chatbots are here, and the inter-\nnet industry is in a tizzy. The New York Times. https:// www. nytim \nes. com/ 2024/ 04/ 10/ techn ology/ ai- chatb ot- train ing- chatg pt. html\nMori M (1970, 2012) The uncanny valley. IEEE Robot Autom Mag \n19(2):98–100.https:// doi. org/ 10. 1109/ MRA. 2012. 21928 11\nMozur P (2025) Who is the founder of the A.I. Start-Up DeepSeek. The \nNew York Times. https:// www. nytim es. com/ 2025/ 01/ 29/ busin ess/ \ndeeps eek- china- liang- wenfe ng. html. Accessed 10 Feb 2025\nMullin B (2023) Inside the news industry’s uneasy negotiations with \nOpenAI. The New York Times. https:// www. nytim es. com/ 2023/ \n12/ 29/ busin ess/ media/ media- openai- chatg pt. html. Accessed 4 \nJan 2024\nMurphy JJ (ed) (1972) A synoptic history of classical rhetoric. Random \nHouse, New York\nMurphy JJ, Thaiss C (2020) A short history of writing instruction. \nFrom ancient Greece to the Modern United States, 4th ed. \nRoutledge\nMurray DM (1968, 2004) A writer teaches writing, 2nd edn. Heinle\nMurray DM (1972, 2003) Teach writing as a process not product. In: \nVillanueva V (ed) Cross-talk in comp theory: a reader, 2nd edn. \nNational Council of Teachers of English, pp 3–6\nOxford Word of the Year: Our 2024 Shortlist (2024) Oxford Univer -\nsity Press. https:// corp. oup. com/ word- of- the- year/# short list- 2024. \nAccessed 28 Jan 2025\nPerelman C, Oblrechts-Tyteca L (1958, 1969) The new rhetoric: a trea-\ntise on argumentation, trans. Wilkinson J and Weaver P. Univer -\nsity of Notre Dame Press\nPeterson AJ (2025) AI and the problem of knowledge collapse. AI Soc. \nhttps:// doi. org/ 10. 1007/ s00146- 024- 02173-x\nPynchon T (1984) Is It O.K. to be a luddite? The New York Times. \nhttps:// archi ve. nytim es. com/ www. nytim es. com/ books/ 97/ 05/ 18/ \nrevie ws/ pynch on- luddi te. html\nRobbins S (2025) What machines shouldn’t do. AI Soc. https:// doi. org/ \n10. 1007/ s00146- 024- 02169-7\nRohman DG (1965) Pre-writing: the stage of discovery in the writing \nprocess. Coll Compos Commun 16(2):106–112\nSadowski C, Söderberg E, Church L, Sipko M, Bacchelli A (2018) \nModern code review: a case study at google. In: Proceedings of \nthe international conference on software engineering. https:// doi. \norg/ 10. 1145/ 31835 19. 31835 25\nShumailov I, Shumaylov Z, Zhao Y et al (2024) AI models collapse \nwhen trained on recursively generated data. Nature 631:755–759. \nhttps:// doi. org/ 10. 1038/ s41586- 024- 07566-y\nSkitka L, Mosier K, Burdick M (1999) Does automation bias decision-\nmaking? Int J Hum Comput Stud 51(5):991–1006. https:// doi. org/ \n10. 1006/ ijhc. 1999. 0252\nSperber L, MacArthur M, Minnillo S, Stillman N, Whithaus C (2025) \nPeer and AI review + reflection: a human-centered approach to \nformative assessment. Comput Compos 76:1-16. https:// doi. org/ \n10. 1016/j. compc om. 2025. 102921\nStack Overflow (2025) Answer Assistant. https:// stack overfl  ow. co/ labs/ \nanswer- assis tant/\nStrijkers K, Costa A (2011) Riding the lexical speedway: a critical \nreview on the time course of lexical selection in speech produc -\ntion. Front Psychol 2:1–16. https:// doi. org/ 10. 3389/ fpsyg. 2011. \n00356\nTenen DY (2024) Literary theory for robots: how computers learned \nto write. W.W. Norton, New York\nTufano R, Dabić O, Mastropaolo A et al (2024) Code review automa-\ntion: strengths and weaknesses of the state of the art. IEEE Trans \nSoftw Eng 50(2):338–353. https:// doi. org/ 10. 1109/ TSE. 2023. \n33481 72\nU.S. Copyright Office (2025) Copyright and artificial intelligence. \nCopyright.gov. https:// www. copyr ight. gov/ ai/\nVee A (2013) Understanding computer programming as a literacy. Lit \nCompos Stud 1(2):42–64\nVee A (2023) Against output. Critical Inquiry Blog. https://  criti nq. \nwordp ress. com/ 2023/ 06/ 28/ again st- output/\nVee A (2024) The moral hazards of technical debt in large language \nmodels: why moving fast and breaking things is bad. Crit AI. \nhttps:// doi. org/ 10. 1215/ 28347 03X- 11205 182\nVee A, Laquintano T, Schnitzler C (eds) (2023) TextGenEd: teaching \nwith text generation technologies. The WAC Clearinghouse/Uni-\nversity Press of Colorado. https:// wac. colos tate. edu/ repos itory/ \ncolle ctions/ textg ened/\nWard I (1994) Literacy, ideology, and dialogue: towards a dialogic \npedagogy. State University of New York Press, New York\nWeber S (2004) The success of open source. Harvard University Press, \nCambridge\nWelsh M (2023) The end of programming. Commun ACM 66(1):34–\n35. https:// doi. org/ 10. 1145/ 35702 20\nZastudil C, Rogalska M, Kapp C, Vaughn JL, Macneil S (2023) Gen-\nerative AI in computing education: perspectives of students and \ninstructors. In: IEEE frontiers in education conference, pp 1–9. \nhttps:// doi. org/ 10. 1109/ FIE58 773. 2023. 10343 467\nZheng K (2025) You don’t need to prove yourself: a raciolinguistic \nperspective on Chinese international students’ academic language \nanxiety and ChatGPT use. Linguist Educ 86:1–11. https:// doi. org/ \n10. 1016/j. linged. 2025. 101406\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Rhetorical question",
  "concepts": [
    {
      "name": "Rhetorical question",
      "score": 0.7705351114273071
    },
    {
      "name": "Performing arts",
      "score": 0.5073270201683044
    },
    {
      "name": "Debt",
      "score": 0.4672529399394989
    },
    {
      "name": "Computer science",
      "score": 0.4620617628097534
    },
    {
      "name": "Linguistics",
      "score": 0.4358081817626953
    },
    {
      "name": "Sociology",
      "score": 0.3451828956604004
    },
    {
      "name": "Business",
      "score": 0.3147854804992676
    },
    {
      "name": "Philosophy",
      "score": 0.1953304409980774
    },
    {
      "name": "Art",
      "score": 0.1661893129348755
    },
    {
      "name": "Literature",
      "score": 0.15173959732055664
    },
    {
      "name": "Finance",
      "score": 0.1509154736995697
    }
  ],
  "institutions": []
}