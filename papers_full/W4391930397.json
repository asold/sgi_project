{
  "title": "Automated Scoring of Open-Ended Question Complexity: A Large Language Model Approach",
  "url": "https://openalex.org/W4391930397",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5003761073",
      "name": "Tuval Raz",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4292734406",
      "name": "Simone Luchini",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A3134031488",
      "name": "Roger Beaty",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A4220084731",
      "name": "Yoed Kenett",
      "affiliations": [
        "Technion – Israel Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6855858150",
    "https://openalex.org/W4385781950",
    "https://openalex.org/W6712171174",
    "https://openalex.org/W2397288399",
    "https://openalex.org/W1981714098",
    "https://openalex.org/W2038333915",
    "https://openalex.org/W2050566113",
    "https://openalex.org/W2903759034",
    "https://openalex.org/W2981731882",
    "https://openalex.org/W3081924543",
    "https://openalex.org/W6853159330",
    "https://openalex.org/W4378364251",
    "https://openalex.org/W6640002018",
    "https://openalex.org/W2050468981",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2306592372",
    "https://openalex.org/W2050731586",
    "https://openalex.org/W1969598650",
    "https://openalex.org/W3112103703",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W3044750169",
    "https://openalex.org/W6769430610",
    "https://openalex.org/W4367680306",
    "https://openalex.org/W3165830623",
    "https://openalex.org/W4387869661",
    "https://openalex.org/W2995523160",
    "https://openalex.org/W6992309071",
    "https://openalex.org/W2247413005",
    "https://openalex.org/W6770831326",
    "https://openalex.org/W4306646496",
    "https://openalex.org/W2935889327",
    "https://openalex.org/W2552729686",
    "https://openalex.org/W2003188719",
    "https://openalex.org/W2129863227",
    "https://openalex.org/W2075562380",
    "https://openalex.org/W2902077037",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2327037637",
    "https://openalex.org/W2162821268",
    "https://openalex.org/W6683450081",
    "https://openalex.org/W2158997610",
    "https://openalex.org/W4385571498",
    "https://openalex.org/W2763088512",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2086438570",
    "https://openalex.org/W3011091101",
    "https://openalex.org/W2003309765",
    "https://openalex.org/W1998766350",
    "https://openalex.org/W2063370556",
    "https://openalex.org/W4388210637",
    "https://openalex.org/W2025065383",
    "https://openalex.org/W2005133993",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W4385620919",
    "https://openalex.org/W4389435338",
    "https://openalex.org/W2938752664",
    "https://openalex.org/W82616934",
    "https://openalex.org/W2803334560",
    "https://openalex.org/W6753025471",
    "https://openalex.org/W2856473903",
    "https://openalex.org/W2126300981",
    "https://openalex.org/W6684890711",
    "https://openalex.org/W2171960331",
    "https://openalex.org/W3146702912",
    "https://openalex.org/W4318480764",
    "https://openalex.org/W2141403362",
    "https://openalex.org/W2106132333",
    "https://openalex.org/W2109758905",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W158511925",
    "https://openalex.org/W1509646071",
    "https://openalex.org/W4210454414",
    "https://openalex.org/W3150427028",
    "https://openalex.org/W4231809946"
  ],
  "abstract": "<title>Abstract</title> Question-asking, an essential yet often understudied activity, holds significant implications for learning, creativity, and cognitive development. In particular, the quality and complexity of the questions asked are crucial factors affecting these fields. Previous research has explored open-ended question complexity through frameworks like the Bloom taxonomy of cognitive objectives, but the measurement of complexity remains challenging. Recent advancements in natural language processing have enabled automated scoring of psychological tasks, notably predicting human ratings of creativity. Although some methods have been applied to measure question complexity, there has been scarce research so far on the automatic assessment of open-ended questions. Here, we address this gap by employing a Large Language Model (LLM) to accurately predict human ratings of open-ended question complexity based on the Bloom taxonomy and comparing these predictions to existing baseline measures such as semantic distance and word count. Specifically, this study capitalized on previously collected human-rated responses from a creative question-asking task to train an LLM for scoring questions based on the Bloom taxonomy of complexity. Our results reveal that our LLM-generated Bloom scores correlated strongly with human ratings of complexity (<italic>r</italic> = .73), whilst also greatly exceeding tested baseline measures. Our study emphasizes the significance of LLM in automating the assessment of open-ended question complexity, fostering cost-effective, automatic, and reliable measurements in this domain. Our study further highlights the exciting possibilities for the continued usage of LLM in education and psychology and their potential in helping study how we ask creative questions.",
  "full_text": "Page 1/25\nAutomated Scoring of Open-Ended Question\nComplexity: A Large Language Model Approach\nTuval Raz \nTechnion—Israel Institute of Technology\nSimone Luchini \nPennsylvania State University\nRoger Beaty \nPennsylvania State University\nYoed Kenett \nTechnion—Israel Institute of Technology\nResearch Article\nKeywords: Question asking, Bloom taxonomy, NLP, LLM\nPosted Date: February 19th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3890828/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/25\nAbstract\nQuestion-asking, an essential yet often understudied activity, holds signi\u0000cant implications for learning,\ncreativity, and cognitive development. In particular, the quality and complexity of the questions asked are\ncrucial factors affecting these \u0000elds. Previous research has explored open-ended question complexity\nthrough frameworks like the Bloom taxonomy of cognitive objectives, but the measurement of\ncomplexity remains challenging. Recent advancements in natural language processing have enabled\nautomated scoring of psychological tasks, notably predicting human ratings of creativity. Although some\nmethods have been applied to measure question complexity, there has been scarce research so far on\nthe automatic assessment of open-ended questions. Here, we address this gap by employing a Large\nLanguage Model (LLM) to accurately predict human ratings of open-ended question complexity based on\nthe Bloom taxonomy and comparing these predictions to existing baseline measures such as semantic\ndistance and word count. Speci\u0000cally, this study capitalized on previously collected human-rated\nresponses from a creative question-asking task to train an LLM for scoring questions based on the\nBloom taxonomy of complexity. Our results reveal that our LLM-generated Bloom scores correlated\nstrongly with human ratings of complexity (r = .73), whilst also greatly exceeding tested  baseline\nmeasures. Our study emphasizes the signi\u0000cance of LLM in automating the assessment of open-ended\nquestion complexity, fostering cost-effective, automatic, and reliable measurements in this domain. Our\nstudy further highlights the exciting possibilities for the continued usage of LLM in education and\npsychology and their potential in helping study how we ask creative questions.\nIntroduction\nQuestion asking is a common and everyday activity, one which we spend considerable time engaged in.\nYet only a very rudimentary technical understanding of question asking currently exists (Kearsley, 1976).\nQuestion asking is central to learning (Chin & Osborne, 2008; Salmon & Barrera, 2021), as well as being\nan important component of educational programs (Chin & Brown, 2002). This is especially true for open-\nended questions which encourage larger and syntactically more complex answers as opposed to closed\nended questions which do not wholly re\u0000ect genuine communication and encourage short, restricted\nresponses (Çakır & Cengiz. 2016). Question asking has also been linked to the creative process (Acar et\nal., 2023; Raz et al., 2023), and has been shown to be a valid measure of creativity (Yager, 1996).\nHowever, questions are di\u0000cult to assess, due to their sometimes open-ended nature, and a common\nemphasis on the answers—and not the questions—that people produce. Recent advances in Natural\nLanguage Processing have led to the rapid application of such tools towards automatic and quantitative\nassessment of linguistic open-ended responses, such as assessing originality of answers to a divergent\nthinking creativity task (Beaty & Johnson, 2021; Dumas et al., 2021). Nevertheless, few methods exist for\nautomatic scoring of question asking (Jayakodi et al., 2015; Mohammed & Omar, 2020) and even fewer\nthat have utilized recent advances in AI and large language models (LLM) (Gani et al., 2023; Hwang et al.,\n2023) which perform better than previous linguistic approaches (Vaswani et al., 2017). Yet, to our\nknowledge, no such computational method has been applied on open-ended questions. In the present\nstudy, we extend research on automatic complexity scoring of a creative question asking task, by\nPage 3/25\ndeveloping and training a LLM (RoBERTa; Liu et al., 2019) to predict human-rated complexity scores for\nopen-ended questions generated in a creative question asking task (Raz et al., 2023).\nQuestion asking\nRonfard et al. (2018) examined research on question-asking in childhood by focusing on the epistemic\nfunction of questions – the use of questions to bridge a gap in knowledge or to resolve uncertainty.\nThese types of epistemic questions can be asked about diverse topics. For example, children (and\nadults) request information about labels, facts, procedures, and causal mechanisms and they can do so\nto obtain clari\u0000cation, to rule out possible hypotheses, and out of curiosity or “wonderment”. Ronfard et\nal. (2018) highlighted the great variability in the quality and quantity of the questions people ask, which\nthey stated is in\u0000uenced by the precision, wording, and quantity of the questions and that developments\nin cognitive skills and increases in prior knowledge may allow for deeper processing and more precise\nquestions. The authors argue that question-asking is a powerful learning strategy, yet research on\nquestions has been relatively sparse and isolated across several disciplines (e.g., Gottlieb, 2021; Nelson,\n2005; Rothe et al., 2018; Raz et al., 2023; Sasson & Kenett, 2023). Ortlieb et al. (2012) further argue that\nthe ultimate goal of education should be advancing beyond the use of the closed questioning style as its\nonly means to assess the learners, with Raphael (1994) stating “If you only ever use closed questions,\nthen you are never going to encourage your learners to think” (Raphael, 1994, p. 114). The researchers\nadvocate for advancing open-ended questions, which are high-level divergent questions that encourage\nthe learner to contemplate and explore before determining an answer (Ortlieb et al., 2012).\nOpen-ended vs closed-ended questions\nOpen-ended and close-ended questions differ in several characteristics, especially regarding the role of\nrespondents when answering these types of questions. Close-ended questions limit the respondent to\nthe set of alternative answers offered by the questions and require the respondent to engage in\nconvergent thinking (i.e., converging on a single correct solution), while open-ended questions allow\nexpressing an opinion without being largely in\u0000uenced by the question designer and engaging in\ndivergent thinking (i.e., diverging on multiple possible solutions). The advantages of the open-ended\nquestions include the possibility of discovering responses that individuals may give spontaneously, and\nthus they avoid the biases that may result from suggesting responses, which may occur in close-ended\nquestions (Reja et al., 2003). Open-ended questions are information-seeking oriented and thus serve the\npurpose of acquiring information as they genuinely seek knowledge (Çakır & Cengiz. 2016). This is\nespecially pertinent in education as teacher’s questions are indispensable components of classroom\ndiscourse and take an important role in facilitating student learning (Chin & Osborne, 2008; Salmon &\nBarrera, 2021).\nResearch on teachers’ questions reveal that closed-ended questions are used more than open-ended\nquestions in whole-class teaching (Çakır & Cengiz. 2016), a practice which has been heavily criticized\n(Nunan, 1987; Brock 1986). Open-ended questions are not only important tools in engaging children in\nPage 4/25\ncognitively challenging conversations and promoting higher-order thinking but they are also found to\noffer linguistic advantages for children as they help develop children’s vocabulary and cognitive skills\n(Çakır & Cengiz. 2016). Baloche (1994) and Khan and Inamullah (2011) argue that a teacher's ability to\nask open-ended questions is crucial for the development of their students' creative thinking skills and for\nnurturing higher level thinking. Higher level, or complex, open questions are regarded of particular\nimportance in fostering creativity, as these involve more elaborate and abstract ideas such as the\ncreation of new topics and the expression of opinions. Framing questions that are complex, open-ended,\nfocused and uncluttered by irrelevant information is thus believed to support higher level thinking.\nAlthough complexity as understood through higher level open questions and higher-level thinking seems\nto be an essential part of question asking, it is not necessarily clear how to best measure and classify it.\nThe Bloom taxonomy\nOne common approach to evaluate question complexity is utilizing the Bloom taxonomy (Bloom et al.,\n1956). The Bloom taxonomy has been widely accepted as a guideline in designing learning objectives of\ndiffering levels of cognitive complexity (Adams, 2015; Goh et al., 2020; Omar et al., 2012). Speci\u0000cally,\nthe taxonomy includes six cognitive levels, which are hierarchically ordered from simple to complex\n(Krathwohl; 2002). The Bloom taxonomy is thought to represent a cumulative hierarchy, where mastery\nof each simpler category is prerequisite for mastery of the next more complex one (Krathwohl, 2002). An\nupdated version of the taxonomy includes the following levels, in ascending order of complexity\n(Krathwohl, 2002): Remembering (retrieving relevant knowledge from long-term memory, for example\nwhen or how did X happen), Understanding (determining the meaning of instructional messages,\nincluding oral, written, and graphic communication, for example how would you summarize..?), Applying\n(carrying out or using a procedure in a given situation, for example how would you solve X using what\nyou have learned), Analyzing (breaking material into its constituent parts and detecting how the parts\nrelate to one another, for example how can you make a distinction between…?), Evaluating (making\njudgments based on criteria and standards, for example how would you prove or disprove…?), and\nCreating (putting elements together to form a novel, coherent whole, for example what changes would\nyou make to solve…?).\nPrevious studies have applied the Bloom taxonomy to the evaluation of question complexity: assigning\neach question a score from one to six (from simple to complex) and allowing for quantitative analyses to\nbe conducted on question complexity (Oliver et al., 2004; Plack et al., 2007; Zheng et al., 2008). Several\nattempts at using LLMs to predict Bloom taxonomy scores have been made in the past (Gani et al., 2023;\nHwang et al., 2023). In one case researchers automated the quality evaluation of biology and chemistry\nmultiple choice questions (Hwang et al., 2023). This was only partially successful, as model accuracy\nranged from 25–90% depending on question type. The \u0000ndings of Hwang et al., (2023) are further\ncomplicated as the authors employed human-rated Bloom scores sourced from a single human rater,\nraising potential issues of rater subjectivity with the questions themselves also being generated and\nsourced from GPT 3.5, which in itself has been shown to be at least partially \u0000awed in terms of question\nquality (Grévisse, 2024). In contrast, Gani et al. (2023) developed a Bloom’s Taxonomy-based exam\nPage 5/25\nquestion classi\u0000cation approach using an LLM and over 2000 labeled multiple choice exam questions as\ntraining data, achieving good accuracy (86%) as compared to previous computational models. The study\ncompared six embedding approaches to determine the best for examination question classi\u0000cation\naccording to Bloom’s taxonomy. Their results showed that RoBERTa is the most optimal, and suggested\nfuture work could include testing RoBERTa with larger datasets to evaluate its scalability and\ngeneralizability.\nCritically, multiple choice questions are usually closed-ended, single-solution tasks (SST) (de Vink et al.,\n2021), which are binary scored for correctness and usually require more closed convergent thinking,\nwhereas open-ended questions that require divergent thinking and involve multiple solution tasks (MST)\nare usually evaluated in terms of \u0000uency (number of correct solutions), \u0000exibility (diversity of solutions),\nand originality (novelty of solutions). Previous research has indicated that creative thinking is more\nstrongly related to MST than SST performance (de Vink et al., 2021), that asking questions is a key trait\nof creativity and an integral part of the creative process, and that question complexity is closely related\nto creativity (Acar et al., 2023; Raz et al., 2023). Thus, the need is clear for integrating divergent open-\nended question asking, together with larger datasets in developing new LLM based approaches to\nautomatically predict question complexity, highlighting the important role of creativity in question asking.\nQuestion asking and creativity\nAsking questions is both a key part of creativity and an important component of the creative process\n(Acar et al., 2023; Raz et al., 2023) that likely facilitates information seeking behavior (Kenett et al.,\n2023). It has been shown to be part of the creative problem-solving process in children (Torrance, 1970).\nAlmeida et al. (2011) have argued for the importance of critical thinking skills in higher education. They\nproposed that individual differences in creativity are directly related to question asking, such that\nstudents ask questions that are consistent with their creative ability. Almeida et al. (2011) implemented\nseveral teaching and learning strategies in a Chemistry and Geology courses, as a way of encouraging\nstudents’ questioning. The authors found that lower creativity was associated with closed, less complex\nquestions which relate to simple facts and concepts, and higher creativity with more complex,\nspecialized questions which reveal working hypotheses and applications of new knowledge (see also\nAcar et al., 2023).Recently, Raz et al. (2023) explored the relation between open-ended question asking\nand creativity using the alternative questions task (AQT). The AQT requires participants to generate\ncreative and unusual questions about common objects (e.g., pen, book, shoe) such as “who invented the\n\u0000rst pencil” or “what library has the most books?”. Responses were rated separately in terms of their\ncreativity, using a 1 (not at all creative) to 5 (very creative) scoring method (Runco & Mraz, 1992; Silvia et\nal., 2008), and complexity, according to the Bloom taxonomy. The authors observed good inter-rater\nagreement (alpha Cronbach > 0.7) for the subjectively rated Bloom taxonomy scores. They also found\nthat question complexity and creativity were positively related: questions which were higher on the\nBloom taxonomy (i.e., more complex) were also scored as more creative, and those that were less\ncomplex were scored as less creative. Thus, this study provided empirical evidence that open-ended\nPage 6/25\nquestion complexity and creativity are related, such that stronger creative abilities accompany stronger\nquestion asking abilities.\nHowever, Raz et al. (2023) noted that subjective scoring of responses according to the Bloom taxonomy\nmay suffer from the same limitations as subjectively rated creativity scores (Kaufman, 2019; Silvia et al.,\n2008), such as inconsistent rater agreement and fatigue. Thus, automating Bloom taxonomy scoring by\nmeans of computational approaches may help overcome these limitations, accelerating empirical\nresearch of question asking.\nSemantic distance and creativity\nRecent advances in natural language processing (NLP) tools such as semantic distance methods have\nallowed for the automated scoring of psychological tasks (e.g., Beaty & Johnson, 2021; Demszky et al.,\n2023; Rathje et al., 2023). This has made it possible to overcome the typical bottlenecks of human\nscoring, such as high labor costs (Kaufman & Baer, 2012; Kaufman et al., 2013; Barbot, 2018; Forthmann\net al., 2017; Reiter-Palmon et al., 2019). Semantic distances re\u0000ect the remoteness between the\nmeanings of two words. This metric rests on the distributional semantics theory, which holds that the\nmeaning of words can be derived by looking at the context in which they appear (Firth, 1957). Words that\nappear in the same context will have similar meanings, so that the relationship between the meanings of\ntwo words can be extracted from their co-occurrence frequencies in text (e.g., books; Lenci, 2018). For\nexample, the words “dog” and “cat” are semantically related and will often co-occur in text, while “dog”\nand “plane” are semantically unrelated and less likely to co-occur. Semantic distance leverages word\nembeddings (i.e., word vectors) to quantitatively represent and compare word meanings (Landauer et al.,\n1998). These embeddings can be generated in several ways, including latent semantic analysis (LSA;\nDeerwester et al., 1990) and computational models such as Word2Vec (Mikolov, Chen, et al., 2013),\nGloVe (Pennington et al., 2014). Once extracted, calculations such as the cosine angle between two-word\nembeddings will provide a measure of semantic similarity. Semantic distance refers to the inverse of the\ncosine angle (1 - semantic similarity) and indicates the dissimilarity between word pairs (Beaty &\nJohnson, 2021). The associative theory of creativity provides a framework that highlights the relevance\nof semantic distance measures in creativity assessment. Creative ideas are thought to arise from the\ncombination of semantically “distant” concepts from memory, such that ideas containing semantically\nunrelated concepts will tend to be more original (Beaty & Kenett, 2023; Kenett, 2019; Mednick, 1962).\nOne way of calculating idea originality is termed the maximum associative distance (MAD) method and\ninvolves taking the semantic distance score that is maximally distant between a given prompt such as\npencil and all words given as a response to it – i.e., possible uses (Yu et al., 2023). MAD scores have\nbeen found to robustly predict human-rated originality scores (r = .74) on a classic creativity task, the\nalternative uses task (AUT; Guilford, 1967) which involves the production of unusual uses for\ncommonplace objects. MAD scores predicted human-rated AUT responses better than previous\ncompositional approaches—involving the addition or multiplication of semantic distances between the\nPage 7/25\nprompt and each word in a response (Beaty & Johnson, 2021). Further, MAD scores have been shown to\ncorrelate with human-rated Bloom taxonomy scores of AQT responses (r = .51; Raz et al., 2023).\nA different approach at calculating semantic distance scores is via divergent semantic integration (DSI;\nJohnson et al., 2023). This technique was developed for longer text responses, such as short stories, and\ninvolves averaging the semantic distance between all possible pairs of words in a response. In this way,\nDSI does not consider the prompt in its calculation, but rather captures the dissimilarity of concepts\nwithin a response. Given the extensive work done on validating semantic distance as a measure of\noriginality (e.g., Patterson et al., 2023), both MAD and DSI scores can serve as a good baseline against\nwhich to compare other automated creativity assessment tools.\nAn additional notable advancement has come in the form of large language models (LLM). When \u0000ne-\ntuned, these models have demonstrated superior performance to previous assessment tools such as\nsemantic distance on a number of creative thinking tasks (Dumas et al., 2021; DiStefano et al., 2023;\nLuchini et al., 2023). Although several computational approaches exist for possible automation of Bloom\ntaxonomy scoring such as using semantic distance approaches or newer Language model methods\n(Stevenson et al., 2022; Yu et al., 2023 ; Organisciak et al., 2023), recent research on LLM has shown that\nthey can go beyond semantic distance methods, and that automated scoring of divergent thinking\ngreatly improves when using a LLM as opposed to semantic distance scores (Organisciak et al., 2023).\nLarge Language Models (LLM) in psychological research\nLLM are computational tools that are used for a variety of tasks involving language data (Vaswani et al.,\n2017). They are a class of deep neural networks that undergo pre-training on large amounts of text data,\nfor the purpose of understanding and generating language. These models have shown considerable\nsuccess across all \u0000elds of psychology by measuring task performance nearly instantaneously(Luchini\net al., 2023; Demszky et al., 2023; Hardy et al., 2023). Thus, LLM unlock possibilities for scale and\ne\u0000ciency in psychological research and practice that were unimaginable in the past (Demszky et al.,\n2023). For example, LLM can be used to generate experimental stimuli (Laverghetta & Licato, 2023),\nmodel word learning across the lifespan (Portelance et al., 2020), identify emotions in text (Zhang et al.,\n2023), predict personality traits from text (Peters & Matz, 2023), and predict the creativity of metaphors\n(DiStefano et al., 2023) and responses to problem solving tasks (Luchini et al., 2023; Dumas et al., 2023).\nThe wide variety of uses for LLM can largely be attributed to their emergent abilities—capabilities that\nlarge models gain only after being exposed to vast amounts of textual data (Brown et al., 2020; Wei et al.,\n2022). LLM are typically pre-trained through unsupervised learning, a training procedure which involves\nautomatic pattern detection from unlabeled data—text that is not assigned any tag or number that the\nmodel has to predict. For LLM, this takes the form of iterative word prediction problems, where the\nmodel is required to predict a missing word from the context or vice versa (Jiang et al., 2020). In this\nway, LLMs acquire an understanding of language and are thus able to outperform previous NLP tools on\ntasks they were never trained on (Vaswani et al., 2017). Of note, the impressive capabilities of LLMs\ncomes at the cost of interpretability (Barredo Arrieta et al., 2020; Dale, 2021; Gunning et al., 2019; Kojima\nPage 8/25\net al., 2022). The architecture of LLMs is large and complex, typically containing millions to billions of\nparameters (i.e., weights) that are adjusted during training and which determine the computations\napplied to the input. It thus becomes challenging, if not impossible, to determine how features of the\ninput relate to the output.\nLLMs have recently demonstrated superior performance compared to previous NLP tools at predicting\nhuman creativity ratings of AUT responses (Organisciak et al., 2023; Stevenson et al., 2022). In order to\nachieve this performance, Organisciak et al. implemented a \u0000ne-tuning procedure—a further round of\ntraining involving supervised learning. Fine-tuning adjusts the weights determined during pre-training by\nexposing the model to many labeled examples (e.g., AUT responses and human ratings). Organisciak et\nal. (2022) provided approximately 27,000 examples to the model and found that it robustly predicted the\ncreativity of responses it hadn’t seen before (r = .81). Organisciak et al. (2022) further demonstrated that\nmodel performance would remain strong even when reducing the number of examples used in the \u0000ne-\ntuning. The researchers showed that to achieve a strong performance (r > .75) the model had to be \u0000ne-\ntuned on a minimum of 6,000 examples. This is particularly relevant when considering the di\u0000culty of\ngathering human-rated responses to a creative thinking task. Finally, Organisciak et al. (2022) tested\nwhether the \u0000ne-tuned model performed better than semantic distance scores, which showed a poor\ncorrelation with the human creativity ratings (r = .20).\nThe Present Study\nThe present study aimed to address the gap in the literature on creative question asking and the role of\ncomplexity by developing an LLM model capable of scoring participant’s open-ended questions to a\ndivergent thinking creative questions task according to the Bloom taxonomy. This was done in an\nattempt to advance the availability, cost effectiveness and reliability of question complexity and\ncreativity scoring and to highlight the advantages of the usage of LLMs in education and psychology and\ntheir potential in helping study how we ask creative questions. The model was trained on data from more\nthan ten thousand human-rated responses to the alternative questions task (AQT), a creative question\nasking task introduced by Raz et al. (2023). Responses were questions asked about everyday objects\nspanning a total of 6 items taken from the suggested items provided by Beaty et al. (2022). To evaluate\nmodel performance, its predictions were compared to three other scoring methods—elaboration (i.e.\nword count) and two semantic distance methods (MAD & DSI) —which reliably predict human creativity\nratings in divergent thinking tasks (Luchini et al., 2023).\nMethods\nParticipants\nThe data analyzed here comes from two different sources. The \u0000rst is a reanalysis of data collected by\nRaz et al. (2023) (N = 223; 47.9% male, 50.4% female, 1.7% preferred not to say; mean age = 26.1 years,\nSD = 6.41 years). The second dataset is based on recent data collected for a larger ongoing study on\nhow priming question asking capacity impacts creative problem solving (N = 500; 49% male, 50% female,\nPage 9/25\n1% preferred not to say, mean age = 29.35 SD = 9.62 ). Both samples are composed of data collected\nfrom participants recruited on Proli\u0000c Academic (N = 723). The total dataset consisted of 10,282\nresponses to the AQT, spanning a total of 6 items (pencil, sock, pillow, purse, clock, and knife). Average\nnumber of AQT responses per participant was 4.74 (SD = 2.341).\nThe alternative questions task (AQT)\nThe AQT requires participants to generate creative and unusual questions about three common objects\nin two minutes for each object (Raz et al., 2023). AQT objects were taken from the suggested items\nprovided by Beaty et al. (2022) for the AUT (pencil, sock, pillow, purse, clock, and knife). Participants\nwere explicitly instructed to come up with as many original and creative questions for objects as they\ncan (Acar & Runco, 2019; Raz et al., 2023). Creative questions were de\u0000ned in the study as questions\nthat strike people as clever, unusual, interesting, uncommon, humorous, innovative, or different.\nParticipants provided their responses on a single page with 30 available input \u0000elds and could view their\nprevious answers. Time limits were two minutes per object. Examples were not provided to avoid biasing\ntowards a speci\u0000c level of question response, which would impact potential question level variance.\nThe Bloom taxonomy\nAQT responses were scored by human raters for their respective Bloom level (from one to six:\nRemember, Understanding, Applying, Analyzing, Evaluating and Creating). The revised edition of Bloom’s\ntaxonomy (Bloom et al., 1956; Krathwohl, 2002) was used. Online raters from Proli\u0000c Academic were\ninstructed on the Bloom taxonomy levels and subjectively rated AQT responses by assigning the Bloom\nlevel they ascertained from the relevant AQT response. Rating instructions included an explanation of the\ntypes of questions asked for each Bloom level, alongside key terms related to each Bloom level and\nexamples of scoring. Each object cue was rated by ten different independent raters for study 1 & 2 (Raz\net al., 2023) due to some raters failing attention checks, and by \u0000ve raters for the priming questions\ndataset. Raters who failed attention checks or gave incomplete ratings were excluded from the \u0000nal\ndataset such that not all objects were rated by the same number of raters. Reliability metrics for AQT\nobjects on their Bloom level ratings were overall good (Ko & Lee, 2016) and as follows: Dataset 1: pencil\n(N = 4, α  = .752), pillow (N = 3, α  = .727), and sock (N = 10, α  = .768), knife (N = 5, α  = .702), purse (N = 3, α  \n= .63), and clock (N = 4, α  = .61); Dataset 2: pencil (N = 4, α  = .751), pillow (N = 5, α  = .713), and sock (N = 4,\nα  = .768).\nAutomated Originality Scoring\nMaximal Associative Distance (MAD). To provide a semantic distance baseline to compare to a \u0000ne-\ntuned LLM, we computed MAD scores (Yu et al., 2023) for all AQT responses following the approach\ndescribed by Patterson et al. (2023). Given all responses in the present dataset are in English, best\npractices for MAD scoring involve extracting the word embeddings from MBERT, by averaging the\nembeddings extracted from the two best performing model layers observed in Patterson et al., (2023).\nMAD was selected given it is an unsupervised machine learning approach (i.e., human-rated originality\nscores are never shown to the model) for extracting originality measures from text data As such, it is a\nPage 10/25\ngood benchmark to evaluate the performance of supervised models which involve \u0000ne-tuning of model\nweights. The MAD method has been shown to outperform previous compositional techniques at\npredicting human-rated originality scores of AUT responses. It has further been shown to correlate with\nBloom scores on the AQT (Raz et al., 2023) and as such is a suitable baseline comparison for our LLM\nrated Bloom score outputs. Semantic distance scores are \u0000rst computed between all words in a\nresponse (e.g., can I bend the pencil without breaking it) and the prompt (e.g., pencil). Then, only the\nmost distant word is retained (i.e., bend), all other words being discarded from the \u0000nal semantic\ndistance score. Response-level MAD scores are thus the semantic distance of the most distant word\nfrom the prompt.\nDivergent Semantic Integration (DSI). We calculated DSI scores for each individual response which were\nemployed as a baseline against which LLM performance was compared to. DSI is also an unsupervised\nmachine learning approach.. DSI has been shown to signi\u0000cantly predict originality on other creativity\ntasks involving long-form text responses (DiStefano et al., 2023; Johnson et al., 2023, Luchini et al.,\n2023).\nTo calculate DSI scores, word embeddings were \u0000rst extracted from a pre-trained language model,\nfollowing Johnson et al. (2023). These embeddings were then used to calculate the semantic distance\nbetween all pairs of words in a response. Semantic distance measures were calculated according to\nstandard procedures employing cosine similarity (Beaty & Johnson, 2021). Calculating the cosine of the\nangle between any two-word embeddings provides a measure of the similarity between the two target\nwords (Mikolov et al., 2013). As such, subtracting this value from 1 provides the opposite measure: the\nsemantic distance score. After all possible word-pair semantic distances were calculated for a response,\nthe values were averaged to provide an overall estimate of originality: the DSI score. For the present\nstudy, word embeddings were extracted from the BERT model (Devlin et al., 2018). BERT has a strong\ncontext sensitivity, meaning it is sensitive to the context in which words appear or to words with multiple\nmeanings—also known as polysemy (e.g., computer-server, restaurant-server), and has been noted to\nreliably predict human creativity ratings on other tasks, and across English linguistic expertise (Johnson\net al., 2023; Reilly et al., 2023).\nLLM-based Bloom scoring\nModel. The RoBERTa model was selected for \u0000ne-tuning for this study. The RoBERTa model constitutes\nan improvement of the BERT model (Liu et al., 2019), and was released by Google in 2018 (Devlin et al.,\n2018). RoBERTa is a transformer model (see Vaswani et al., 2017) which underwent self-supervised\ntraining, without any human ratings being presented to the model. The architecture of the RoBERTa\nmodel is similar to BERT with some changes in the pretraining procedure, including a larger training\ndataset (Liu et al., 2019). For our analysis, we selected the RoBERTa-base model. Given its smaller size,\nthis version comes with a reduced computational cost compared to larger versions of the same model.\nThe base version of RoBERTa has 123 million parameters (i.e., weights) and was pre-trained on the\nToronto BookCorpus (800M words), the English portion of Wikipedia (2,500M words), the CC-News\nEnglish dataset (63M news articles), the OpenWebText collection of Reddit posts, and the Stories\nPage 11/25\ndataset which contains a subset of the CommonCrawl repository. RoBERTa was pretrained with a\nbidirectional approach (i.e., the model saw entire sentences when making predictions) applied to context\nleveraging (i.e., \u0000lling in the blanks by drawing from the surrounding context). Of note, RoBERTa has been\nshown to perform well on a variety of linguistic tasks (Gilloz et al., 2020). The model is available on the\nopen-access Huggingface platform (https://huggingface.co/). For \u0000ne-tuning, the “Huggingface\nTransformers” suite of the “PyTorch” package was used via the Python programming language.\nDatasets. AQT responses (collected as described above) was randomly split into training, validation and\nholdout-responses sets following a 70/10/20 ratio respectively. The training data was employed to \u0000ne-\ntune the model, as the model saw both the responses and human Bloom ratings and learned to predict\nthe ratings. The validation data served the purpose of iteratively testing different variations of the model,\neach trained with different combinations of hyperparameter values, to determine the best\nhyperparameter settings. The holdout-responses data contained responses that the model was never\npresented with during training and allowed the testing of model performance on unseen responses;\nholdout-responses included AQT items that the model saw during training, and as such served as a test\nof near-transfer.\nHoldout-item data also contained new responses; critically, they were related to an AQT item that the\nmodel never saw during training (i.e., clock) This item was selected for the holdout-item set as it was\nassociated with the smallest number of responses from the entire dataset, leaving more data to be\nassigned to the other sets. As such, the holdout-responses data served as a test of far-transfer of model\nperformance. Ideal model performance would involve strong predictions for both unseen responses and\nitems, as this would indicate that the model can be extended to other datasets which involve different\nresponses and items.\nThe training data (AQT responses and Bloom scores) was used to adjust the weights of the pretrained\nmodel. To achieve this, batches of responses from the training data were inputted to the model during\ntraining. The model would then predict a single Bloom value for each response, and the mean squared\nerror between these predicted values and the true human-rated Bloom scores determined the degree of\nweight adjustment. Model predictions for the validation data were compared across a variety of model\nsettings, retaining only the best combination for later testing.\nHyperparameter Search. Hyperparameters are settings that determine the learning of a model and which\nare set prior to training. By iteratively training several partial models we were able to select the best\nperforming values for our selected hyperparameters given our model and data. In this way,\nhyperparameter search allowed us to select the best model settings prior to our \u0000nal and complete\nround of training, the only training round that determined our \u0000nal model. We implemented a\nhyperparameter search over the number of epochs, the learning rate, the training batch size, and the\nevaluation batch size, based on similar applications (e.g., DiStefano et al., 2023; Luchini et al., 2023). The\nnumber of epochs determines how many times the model will iterate over the entire training dataset\nduring training (i.e., the epochs). For this, we searched between a range of 100 and 130 epochs. The\nPage 12/25\nlearning rate determines the speed at which the model will learn from the data. It effectively modi\u0000es the\nimpact that one batch of data will have on the weights of the model. We searched between learning rate\nvalues of 5e-05 and 5e-04. We also searched over the training and evaluation batch sizes. The batch size\nrefers to the number of responses that are inputted to the model during each iteration. A larger batch\nsize means that the model will see more responses at a time. Batch sizes were searched separately for\nthe training and the evaluation data (i.e., validation, holdout-responses, and holdout-prompt sets). For\nboth training and evaluation batch sizes, we searched through three possible values of 8, 16, and 32.\nHyperparameter search was run by employing the Optuna package in Python language (Akiba et al.,\n2019). Optuna allows for the search over a variety of hyperparameter combinations by means of a Tree-\nStructured Parzen Estimator, a Bayesian optimization method. Optuna iteratively trains a number of\ncandidate models, each with its own unique combination of hyperparameter settings. In doing so, it\nbuilds a probabilistic model which it then updates as different combinations of hyperparameter settings\nare tested on the validation set. It then preferentially explores hyperparameter combinations that have a\nhigher likelihood to minimize the prediction error on the validation set. After the best hyperparameter\nsettings were identi\u0000ed, these were then used in the training of our \u0000nal model. All hyperparameters\nother than batch sizes, learning rate and number of epochs were left to the default settings for RoBERTa-\nbase.\nData, analysis scripts and weights for our \u0000nal model are available online on OSF (https://osf.io/823ak/?\nview_only=2f8f7a94ca5e4d57b810afc26054a6d3).\nResults\nDescriptive Statistics and Correlations\nWe computed a series of descriptive statistics for the AQT (Table 1). Across all items, the mean word\ncount was 6.58, and the mean Bloom rating was 2.85. We calculated the intra-class correlation (ICC;\nShrout & Fleiss, 1979) between raters in the study and found a strong reliability across all items in the\ndataset, mean ICC = 0.76 [95% CI: 0.75, 0.77]. on the average ratings using a two-way random-effects\nmodel with absolute agreement.\n  \nPage 13/25\nTable 1\nDescriptive Statistics for Each Set\nSet Number ofresponses ICC MeanBloom (SD) Mean WordCount (SD) Mean DSI(SD) MeanMAD (SD)\nTraining 5035 0.82 2.84 (1.19) 6.58 (2.84) 0.72(0.05) 0.46(0.07)\nValidation 725 0.80 3.11 (1.16) 6.77 (2.65) 0.73(0.04) 0.46(0.07)\nHoldout-Responses 1443 0.80 2.81 (1.19) 6.55 (2.77) 0.73(0.05) 0.45(0.07)\nHoldout-Item 445 0.81 2.72 (0.94) 6.26 (2.47) 0.72(0.05) 0.40(0.09)\nNote:. ICC = intra-class correlation; DSI = divergent semantic integration; MAD = maximumassociative distance.\nWe then computed a series of Pearson’s correlations between our variables of interests, separately for\neach set (Fig. 1). Before computing any of the linear regression models, we removed outliers based on\nCook’s distance metrics (Cook, 1977) by employing on the modi\u0000ed Cook’s cutoff equation (Fox, 2019).\nWe calculated the thresholds for outlier removal separately for the training set (Cook’s Distance > .0008),\nvalidation set (Cook’s Distance > .006), holdout-responses set (Cook’s Distance > .003), and holdout-item\nset (Cook’s Distance > .009). We then calculated correlations between DSI and MAD (two semantic\ndistance metrics). We found moderate correlations for the training set, r = .54, p < .001; validation set, r \n= .47, p < .001; holdout-responses set, r = .53, p < .001; and holdout-item set, r = .61, p < .001.\nNext, we calculated correlations between MAD and word count. Strong to moderate correlations were\nobserved across all sets, with r = .58, p < .001, for the training set, r = .51, p < .001, for the validation set, r \n= .57, p < .001, for the holdout-responses set, and r = .68, p < .001, for the holdout-item set. We then\ncalculated correlations between DSI and word count. Strong correlations were observed across all sets,\nwith r = .70, p < .001, for the training set, r = .66, p < .001, for the validation set, r = .69, p < .001, for the\nholdout-responses set, and r = .73, p < .001, for the holdout-item set. The analysis employed Pearson's\ncorrelations across different variables of interest. Signi\u0000cant moderate to strong correlations were\nobserved between semantic distance metrics (DSI and MAD) and word count with the training and test\ndata, suggesting good reliability between baseline measures .\nFinally, we computed correlations between the baseline measures and human-rated Bloom scores. For\nword count, we observed moderate correlations of r = .40, p < .001 for the training set, r = .30, p < .001, for\nthe validation set, r = .44, p < .001, for the holdout-responses set, and r = .37, p < .001, for the holdout-\nprompt item set. For MAD, we again observed moderate correlations of r = .31, p < .001, for the training\nset, r = .22, p < .001, for the validation set, r = .32, p < .001, for the holdout-responses set, and r = .42, p \n< .001, for the holdout-item set. Finally, for DSI, we observed moderate correlations of r = .39, p < .001, for\nthe training set, r = .27, p < .001, for the validation set, r = .39, p < .001, for the holdout-responses set, and\nr = .34, p < .001, for the holdout-item set.\nPage 14/25\nLLM Prediction of Bloom Ratings\nHyperparameter settings for our \u0000nal RoBERTa model included 114 epochs, a learning rate of 9.2e-05,\nand a batch size of 16 for both training and evaluation. The \u0000ne-tuned RoBERTa model was then used to\npredict Bloom scores for each response. These predicted scores were then included in a series of linear\nregressions against the human rated scores. We found that our model perfectly predicted the human\nratings at r = 0.99, p < .001, on the training set, and strongly predicted them on the validation set r = .76, p \n< .001.\nAs a test of model generalizability, we further explored correlations for the holdout-responses set. The\nheld-out test set consisted of responses that were neither used for model selection nor were seen by the\nmodel during its training. It thus served as a test of the model’s ability to generalize to responses it\nwasn’t trained on. We found that the correlation between the models’ predictions and the holdout-\nresponses was substantially larger, r = .73, p < .001, than the semantic distance and elaboration\nbaselines. The results demonstrate that \u0000ne-tuned LLMs, can strongly capture and predict human\ncreativity ratings of question complexity. We then tested the correlation on the holdout-item set, which\nconsisted of item inputs that were neither used for model selection nor were seen by the model during\nits training. We again found a substantially larger, r = .77, p < .001, correlation than baseline measures.\nThis strengthens our claim that LLMs can be used to accurately predict human ratings of question\nasking tasks and offer a reliable and e\u0000cient alternative to labor-intensive and subjective human ratings\nof question complexity.\nNext, we evaluated whether the model performed better when predicting the Bloom ratings of responses\nassociated with average or extreme human-rated Bloom scores. To this end, we calculated the mean\nprediction errors separately for the responses that were above or below two standard deviations or\nbetween two standard deviations from the mean for the human-rated Bloom scores (Table 2). We\nobserved that mean prediction errors were consistently higher for responses that fell at the extremes of\nthe distributions for the human-rated Bloom scores, suggesting the model performed better when\npredicting responses with average human-rated Bloom scores. Of note, this distinction appeared to be\nconsiderably lower for the holdout-item set than for any of the other sets.\nTable 2 Mean Prediction Errors for Human-Rated Bloom scores\nSets\n  Training SetValidation SetHoldout-Responses SetHoldout-Item Set\n± 2 SD 1.97 1.44 1.31 0.73\nWithin 2 SD 0.8 0.7 0.73 0.66\nNote. Errors are calculated over factor scores and are averaged in the form of absolute values, between\ntwo standard deviations of the mean and above or below two standard deviations of the mean.\nPage 15/25\nDiscussion\nQuestions play a critical role in learning, education, and creativity. However, much is unknown on the role\nof complex question asking in cognition (Raz et al., 2023). This is in part due to the challenge of scoring\nand assessing open-ended questions, an issue that is present for the broader creativity research\n(Kaufman, 2019). Despite recent advancements in LLMs and their emerging role in psychological\nresearch (Portelance et al., 2020; Laverghetta & Licato, 2023; Zhang et al., 2023; Peters & Matz, 2023;\nDiStefano et al., 2023; Luchini et al., 2023; Dumas et al., 2023; Organisciak et al., 2023) there has been\nlittle research on automated scoring of question asking (cf. Jayakodi et al., 2015; Gani et al., 2023;\nHwang et al., 2023; Mohammed & Omar, 2020) and to our knowledge, no such computational method\nhas been applied to scoring open-ended questions. The current study capitalized on advanced LLMs to\ndevelop automatic and accurate scoring of open-ended question complexity, based on the Bloom’s\ntaxonomy.\nWe trained a LLM (RoBERTa) to predict human-rated Bloom scores for responses to the alternative\nquestions task (AQT; Raz et al. 2023), which measures creative question asking. Bloom scores are a\nmeasure of the cognitive complexity of an AQT response and were subjectively scored by human raters\n(with high inter-rater agreement) in accordance with the revised Bloom taxonomy (Krathwohl, 2002). Our\n\u0000ne-tuned LLM demonstrated robust predictions of Bloom scores, surpassing those achieved by\nsemantic distance models or word count. The model generalized its predictions beyond the data it was\ntrained on and demonstrated good performance on the test data, reaching a Pearson correlation above\n0.75. Of note, strong predictions were also observed for an unseen prompt item. By evaluating model\npredictions for the unseen prompt item, it is then possible to determine generalizability of the model.\nIdeal model performance would involve strong predictions for both unseen responses and items, as this\nwould indicate that the model can be extended to other datasets which involve different responses and\nitems. We thus provide the \u0000rst evidence that a LLM can robustly predict Bloom complexity scores and\nautomatically score a question asking task. To enhance dissemination of our model—which can\nautomatically score responses to the AQT—to potential users, we make it freely available\n(https://osf.io/823ak/?view_only=2f8f7a94ca5e4d57b810afc26054a6d3).\nModel predictions strongly correlated with human-rated Bloom scores in both the holdout-responses (r \n= .73) and holdout-item set (r = .77) and thus model performance on the holdout-item set was even\nslightly better than inter-rater agreement between human raters in the study, mean ICC = 0.76 [95% CI:\n0.75, 0.77]. This meant that model predictions explained 53% and 59% of the variance in human-rated\nBloom scores for the holdout-responses and prompt sets respectively. Regarding our baseline measures,\ncorrelations between the baseline measures and human-rated Bloom scores were consistently moderate\n(average r = .38) for both holdout responses and item sets. As such, the baseline measures explain no\nmore than 14% of the variance in the Bloom scores. These \u0000ndings indicate that the LLM learned\nconsistencies in the data that were generalizable to unseen responses and items. In other words, the\nmodel was able to pick up a big part of how humans evaluate questions in terms of cognitive complexity\nand was able to re-apply this knowledge to new data.\nPage 16/25\nCuriously, the model performed slightly better for the holdout-item set than the holdout-responses set.\nThis may be in part due to the fact that Bloom scores were more spread out in the holdout set of\nresponses (M = 2.81; SD = 1.19) compared to the holdout set of items (M = 2.72; SD = 0.94; see Table 1).\nThe LLM may have had more di\u0000culty in rating the highly and weakly complex questions, rather than\nthose of average complexity. This is supported by examining the prediction errors for the holdout-\nresponses set, which were considerably higher at the extremes (± 2 SD) compared to the center (within 2\nSD) of the distribution of Bloom scores (see Table 2). Given that human-rated Bloom scores tend to be\nnormally distributed, the model was trained on fewer examples of highly complex and weakly complex\nquestions. As such, the model would have had less data to learn from which may have resulted in this\nunderestimation of highly complex questions and overestimation of weakly complex questions. Similar\n\u0000ndings have been reported in recent work on automated creativity assessment, suggesting that this\nmay be a common obstacle when \u0000ne-tuning models for automated scoring tasks (Luchini et al., 2023;\nPatterson et al., 2022).\nQuestion asking is central to learning (Chin & Osborne, 2008; Salmon & Barrera, 2021), educational\nprograms (Chin & Brown, 2002), the creative process (Acar et al., 2023; Raz et al., 2023), and is a valid\nmeasure of creativity (Yager, 1996). Furthermore, asking questions is an important human capacity\nwhich has also been found to bet related to curiosity, problem \u0000nding and information seeking behavior\n(Kenett et al., 2023; Ivancovsky et al., 2023; Raz & Kenett, 2023; Raz et al., 2023). But critically, not all\nquestions are the same. The type of questions used by the asker can have a very important role in\nconstructing a facilitative environment for information seeking, education or higher-level thinking (Çakır\n& Cengiz, 2016). The r\u0000ndings of Çakır and Cengiz (2016) support the idea that higher-order open-ended\nquestions elicit more utterances from students, enhance creative thinking, and encourage the learner to\ncontemplate and explore before determining an answer (Ortlieb et al., 2012 p. 31; Çakır & Cengiz. 2016;\nde Vink et al., 2021). Conversely, close-ended questions limit the respondent to the set of alternative\nanswers offered in the question and bias thinking towards them. Research in classrooms has revealed\nthat closed ended questions are used more than open ended questions in whole-class teaching (Çakır &\nCengiz, 2016), a troubling \u0000nding which underscores the need for more open-ended question asking. The\nmodel developed in this study aims to further advance research into this area of open-ended questions\nby adding an additional tool to the limited but expanding toolset of question asking measurement.\nThere are some potential limitations concerning the results of this study. As noted by Luchini et al.\n(2023), smaller, older LLMs such as RoBERTa and GPT-2 have been shown in the past to underperform\non certain benchmark tasks when compared to newer, more advanced ones. The current analysis should\ntherefore be extended to larger models, such as GPT-4, to evaluate whether predictions of Bloom\ncognitive complexity on the AQT can be further improved. However, larger models like GPT-4 are\ncurrently not freely available for use, and researchers would incur considerable costs when employing\nthese models in their work. Additionally, the quality of a \u0000ne-tuned model could be limited by the quality\nof human ratings of Bloom taxonomy. The datasets used to train the LLMs had some inter-rater\ndisagreement that could translate to variance in the model, which may be improved by resolving rater\ndifferences, similar to Amabile’s (1982) consensual assessment technique.\nPage 17/25\nAdditionally, the model developed in this study was trained on averaged continuous scores of Bloom\ncomplexity via a regression model. As such, the output scores of the model are continuous, but are\nrounded to the nearest whole Bloom level in order to display familiar bloom levels. This is in contrast to a\nclassi\u0000cation model which outputs discrete scores, in this case corresponding to the one to six Bloom\nlevels. We opted to use a prediction, regression-based approach to align with previous LLM applications\nof open-ended responses (e.g., Distefano et al., 2023; Luchini et al., 2023; Organisciak et al., 2023). The\nquestion of whether the complexity of questions is discrete or continuous is still an open one and\nrequires further research into the Bloom taxonomy and how we measure complexity.\nDespite these limitations, the results suggest several theoretical and practical implications. As Ronfard\net al. (2018) state - research with older elementary school students and college aged students has\nshown that students can quickly be taught how to ask higher level questions (e.g., “what is the difference\nbetween … and …?”) rather than lower complexity factual questions, and that this leads to improvements\nin learning and reading comprehension. In addition, calls from researchers suggest moving towards\nopen-ended creative question asking teaching methods and advancement beyond the primary use of the\nclosed questioning style (Ortlieb et al. 2012; Raphael, 1994) It is therefore possible that educators may\ngreatly bene\u0000t from automated methods of testing and assessing open-ended question complexity, thus\nhelping to foster creativity, learning and advanced comprehension in students. Future research should\nalso focus on expanding the arsenal of psychological tests that can be automated using NLP focused\nLLMs, which will greatly improve the accessibility of these tests, and creating online “one stop shop”\nresources combining many automated scoring techniques similarly to the work done by Beaty and\nJohnson (2021).\nConclusion\nIn this study, we introduce a novel approach to automatically score the alternative questions task for\nBloom taxonomy complexity using a \u0000ne-tuned LLM and compare it to three baseline measures: word\ncount/elaboration, and semantic distance scores in the form of DSI and MAD scores (Johnson et al.,\n2022; Yu et al., 2023). Our results reveal that LLM-generated Bloom scores correlated strongly with\nhuman ratings—greatly exceeding both baselines. These results highlight the unique ability of LLMs to\naccurately predict ratings of open-ended question asking tasks. Our research offers a reliable and\ne\u0000cient alternative to labor-intensive and subjective human ratings of question complexity—improving\nthe reproducibility and scalability of complexity assessment. This study also emphasizes the exciting\npotential for the continued usage of LLMs in education and psychology and the possibilities they unlock\nin helping us study how we ask creative questions about the world and help us build the educators and\npedagogical programs of the futur\nDeclarations\nWe state that this study has received IRB approval from the Technion university ethics board.\nPage 18/25\nFunding\nFunding: R.E.B. is supported by grants from the National Science Foundation [DRL-1920653; DUE-\n2155070]. This work was partially supported by the US-Israel Binational Science Fund (BSF) grant\n(number 2021040) to R.E.B and Y.N.K.\nCompeting interests\nThe authors declare no con\u0000ict of interests.\nAuthor contributions\nCRediT Statement: Tuval Raz: Conceptualization, Formal Analysis, Investigation, Writing – Original Draft,\nReview & Editing; Simone Luchini : Methodology, Resources, Software, Visualization, Review & Editing;\nRoger E. Beaty: Supervision, Writing – Review & Editing; Yoed N. Kenett: Conceptualization, Supervision,\nWriting – Review & Editing. \nData: This study was not preregistered. Data and code used in the manuscript is on the Open Science\nFramework:  Data, analysis scripts and weights for our \u0000nal model are available online on OSF\n(https://osf.io/823ak/?view_only=2f8f7a94ca5e4d57b810afc26054a6d3).\nReferences\n1. Acar, S., Berthiaume, K., & Johnson, R. (2023). What kind of questions do creative people ask?\nJournal of Creativity, 100062. https://doi.org/10.1016/j.yjoc.2023.100062\n2. Adams N. E. (2015). Bloom's taxonomy of cognitive learning objectives. Journal of the Medical\nLibrary Association, 103(3), 152–153.https://doi.org/10.3163/1536-5050.103.3.010\n3. Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019, July). Optuna: A next-generation\nhyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining (pp. 2623-2631).\n4. Albergaria-Almeida, P. (2011). Critical thinking, questioning and creativity as components of\nintelligence. Procedia - Social and Behavioral Sciences, 30, 357–362.\nhttps://doi.org/10.1016/J.SBSPRO.2011.10.070\n5. Amabile, T. M. (1982). Social psychology of creativity: A consensual assessment technique. Journal\nof Personality and Social Psychology, 43(5), 997–1013. https://doi.org/10.1037/0022-\n3514.43.5.997\n\u0000. Baloche, L. (1994). Breaking down the walls. The Social Studies. 85. 25-30.\nhttps://doi.org/10.1080/00377996.1994.10118776\n7. Barbot, B. (2018). The dynamics of creative ideation: Introducing a new assessment paradigm.\nFrontiers in Psychology, 9. https://doi.org/10.3389/fpsyg.2018.02529\nPage 19/25\n\u0000. Barredo Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcia, S., Gil-\nLopez, S., Molina, D., Benjamins, R., Chatila, R., & Herrera, F. (2020). Explainable Arti\u0000cial Intelligence\n(XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information\nFusion, 58, 82–115. https://doi.org/10.1016/j.inffus.2019.12.012\n9. Beaty, R. E., & Johnson, D. R. (2021). Automating creativity assessment with SemDis: An open\nplatform for computing semantic distance. Behavior Research Methods, 53(2), 757–780.\nhttps://doi.org/10.3758/s13428-020-01453-w\n10. Beaty, R. E., Johnson, D. R., Zeitlen, D. C., & Forthmann, B. (2022). Semantic distance and the\nalternate uses task: Recommendations for reliable automated assessment of originality. Creativity\nResearch Journal, 34(3), 245-260. https://doi.org/10.3758/s13428-020-01453-w\n11. Beaty, R. E., & Kenett, Y. N. (2023). Associative thinking at the core of creativity. Trends in Cognitive\nSciences 27(7), 671-683. https://doi.org/10.1016/j.tics.2023.04.004\n12. Bloom B. S., Krathwohl D. R., & Masia B. B. (1956). Taxonomy of educational objectives: the\nclassi\u0000cation of educational goals. David McKay Company.\n13. Brock, C. A. (1986). The effects of referential questions on ESL Classroom Discourse. TESOL\nQuarterly, 20, 77-59. http://dx.doi.org/10.2307/3586388\n14. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P.,\nSastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,\nZiegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language models are few-shot learners.\nProceedings of the 34th International Conference on Neural Information Processing Systems,\n1877–1901.\n15. Çakır, H. and Cengiz, Ö. (2016) The use of open ended versus closed ended questions in Turkish\nclassrooms. Open Journal of Modern Linguistics, 6, 60-70. doi: 10.4236/ojml.2016.62006.\n1\u0000. Chin, C., & Brown, D. E. (2002). Student-generated questions: A meaningful aspect of learning in\nscience. International Journal of Science Education, 24(5), 521–549.\nhttps://doi.org/10.1080/09500690110095249\n17. Chin, C., & Osborne, J. (2008). Students’ questions: a potential resource for teaching and learning\nscience. Studies in Science Education, 44(1), 1–39. https://doi.org/10.1080/03057260701828101\n1\u0000. Dale, R. (2021). GPT-3: What’s it good for? Natural Language Engineering, 27(1), 113-118.\n19. Demszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., Eichstaedt, J. C., Hecht,\nC., Jamieson, J., Johnson, M., Jones, M., Krettek-Cobb, D., Lai, L., JonesMitchell, N., Ong, D. C.,\nDweck, C. S., Gross, J. J., & Pennebaker, J. W. (2023). Using large language models in psychology.\nNature Reviews Psychology, 2(11), 688–701. https://doi.org/10.1038/s44159-023-00241-5\n20. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional\ntransformers for language understanding. ArXiv. https://arxiv.org/abs/1810.04805v2\n21. DiStefano, P. V., Patterson, J. D., & Beaty, R. (2023). Automatic Scoring of Metaphor Creativity with\nLarge Language Models. PsyArXiv.\nPage 20/25\n22. Dumas, D., Organisciak, P., & Doherty, M. (2021). Measuring divergent thinking originality with human\nraters and text-mining models: A psychometric comparison of methods. Psychology of Aesthetics,\nCreativity, and the Arts, 15(4), 645–663. https://doi.org/10.1037/aca0000319\n23. Firth, J. R. (1957). A Synopsis of Linguistic Theory, 1930–1955. Studies in Linguistic Analysis.\nOxford, UK: Blackwell.\n24. Forthmann, B., Holling, H., Zandi, N., Gerwig, A., Çelik, P., Storme, M., & Lubart, T. (2017). Missing\ncreativity: The effect of cognitive workload on rater (dis-)agreement in subjective divergent-thinking\nscores. Thinking Skills and Creativity, 23, 129-139. https://doi.org/10.101t6/j.tsc.2016.12.005\n25. Gani, M. O., Ayyasamy, R. K., Sangodiah, A., & Fui, Y. T. (2023). Bloom’s Taxonomy-based exam\nquestion classi\u0000cation: The outcome of CNN and optimal pre-trained word embedding technique.\nEducation and Information Technologies, 28(12), 15893–15914. https://doi.org/10.1007/s10639-\n023-11842-1\n2\u0000. Gillioz, A., Casas, J., Mugellini, E., & Abou Khaled, O. (2020, September). Overview of the\nTransformer-based Models for NLP Tasks. In 2020 15th Conference on Computer Science and\nInformation Systems (FedCSIS) (pp. 179-183). IEEE.\n27. Goh, T.T., Mohamed, H., Jamaludin, N.A., Ismail, M.N., & Chua, H.S. (2020). Questions classi\u0000cation\naccording to Bloom’s taxonomy using universal dependency and Word Net. Test Engineering and\nManagement. 82. 4374-4385\n2\u0000. Gottlieb, J. (2021). The effort of asking good questions. Nature Human Behaviour, 5(7), 823-824.\nhttps://doi.org/10.1038/s41562-021-01132-6\n29. Grévisse, C. (2024). Comparative Quality Analysis of GPT-Based Multiple Choice Question\nGeneration. In H. Florez & M. Leon (Eds.), Applied Informatics (pp. 435–447). Springer Nature\nSwitzerland. https://doi.org/10.1007/978-3-031-46813-1_29\n30. Gunning, D., Ste\u0000k, M., Choi, J., Miller, T., Stumpf, S., & Yang, G.-Z. (2019). XAI—Explainable arti\u0000cial\nintelligence. Science Robotics, 4(37), eaay7120. https://doi.org/10.1126/scirobotics.aay7120\n31. Hardy, M., Sucholutsky, I., Thompson, B., & Gri\u0000ths, T. (2023). Large language models meet cognitive\nscience: LLMs as tools, models, and participants. Proceedings of the Annual Meeting of the\nCognitive Science Society, 45(45). https://escholarship.org/uc/item/6dp9k2gz\n32. Hwang, K., Challagundla, S., Alomair, M., Chen, L. K., & Choa, F. S. (2023). Towards AI-assisted\nmultiple choice question generation and quality evaluation at scale: Aligning with Bloom’s\nTaxonomy. Workshop on Generative AI for Education.\n33. Jayakodi, K., Bandara, M., & Perera, I. (2015). An automatic classi\u0000er for exam questions in\nEngineering: A process for Bloom's taxonomy. 2015 IEEE International Conference on Teaching,\nAssessment, and Learning for Engineering (TALE), 195-202.\n34. Jawahar, G., Sagot, B., & Seddah, D. (2019, July). What does BERT learn about the structure of\nlanguage?. In ACL 2019-57th Annual Meeting of the Association for Computational Linguistics.\n35. Jiang, Z., Xu, F. F., Araki, J., & Neubig, G. (2020). How can we know what language models know?.\nTransactions of the Association for Computational Linguistics, 8, 423-438.\nPage 21/25\n3\u0000. Johnson, D. R., Kaufman, J. C., Baker, B. S., Patterson, J. D., Barbot, B., Green, A. E., van Hell, J.,\nKennedy, E., Sullivan, G. F., Taylor, C. L., Ward, T., & Beaty, R. E. (2023). Divergent semantic integration\n(DSI): Extracting creativity from narratives with distributional semantic modeling. Behavior Research\nMethods, 55(7), 3726–3759. https://doi.org/10.3758/s13428-022-01986-2\n37. Kaufman, J. C. (2019). Self-assessments of creativity: Not ideal, but better than you think.\nPsychology of Aesthetics, Creativity, and the Arts, 13(2), 187-192.\nhttps://doi.org/10.1037/aca0000217\n3\u0000. Kaufman, J. C., & Baer, J. (2012). Beyond new and appropriate: Who decides what is creative?\nCreativity Research Journal, 24(1), 83–91. https://doi.org/10.1080/10400419.2012.649237\n39. Kaufman, J. C., Baer, J., Cropley, D. H., Reiter-Palmon, R., & Sinnett, S. (2013). Furious activity vs.\nunderstanding: How much expertise is needed to evaluate creative work? Psychology of Aesthetics,\nCreativity, and the Arts, 7(4), 332–340. https://doi.org/10.1037/a0034809\n40. Kearsley, G. P. (1976). Questions and question asking in verbal discourse: A cross-disciplinary\nreview. Journal of Psycholinguistic Research, 5(4), 355–375. https://doi.org/10.1007/BF01079934\n41. Kenett, Y. N. (2019). What can quantitative measures of semantic distance tell us about creativity?\nCurrent Opinion in Behavioral Sciences, 27, 11–16. https://doi.org/10.1016/j.cobeha.2018.08.010\n42. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large language models are zero-shot\nreasoners. ArXiv. https://doi.org/10.48550/arXiv.2205.11916\n43. Koo, T. K., & Li, M. Y. (2016). A guideline of selecting and reporting intraclass correlation coe\u0000cients\nfor reliability research. Journal of Chiropractic Medicine, 15(2), 155–163.\nhttps://doi.org/10.1016/j.jcm.2016.02.012\n44. Krathwohl, D. R. (2002). A revision of Bloom’s taxonomy: An overview. Theory into Practice, 41, 212-\nhttp://dx.doi.org/10.1207/s15430421tip4104_2\n45. Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis.\nDiscourse Processes, 25(2–3), 259–284. https://doi.org/10.1080/01638539809545028\n4\u0000. Laverghetta, A., & Licato, J. (2023). Generating better items for cognitive assessments using large\nlanguage models. Proceedings of the 18th Workshop on Innovative Use of NLP for Building\nEducational Applications (EEA 2023), 414-428. https://doi.org/10.18653/v1/2023.bea-1.34\n47. Lenci, A. (2018). Distributional models of word meaning. Annual Review of Linguistics, 4, 151-171.\n4\u0000. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewvis, M., Zettlemoyer, L., & Stoyanov, V.\n(2019). RoBERTa: A robustly optimized BERT pretraining approach. Arxiv.\nhttps://doi.org/10.48550/arxiv.1907.11692\n49. Luchini, S., Maliakkal, N. T., DiStefano, P. V., Patterson, J. D., Beaty, R., & Reiter-Palmon, R. (2023).\nAutomatic Scoring of Creative Problem-Solving with Large Language Models: A Comparison of\nOriginality and Quality Ratings. PsyArXiv.\n50. Mednick, S. (1962). The associative basis of the creative process. Psychological Review, 69(3),\n220–232. https://doi.org/10.1037/h0048850\nPage 22/25\n51. Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). E\u0000cient estimation of word representations in\nvector space. ArXiv a.rXiv:1301.3781.\n52. Mohammed, M., & Omar, N. (2020). Question classi\u0000cation based on Bloom's taxonomy cognitive\ndomain using modi\u0000ed TF-IDF and word2vec. PloS ONE, 15(3), e0230442.\nhttps://doi.org/10.1371/journal.pone.0230442\n53. Nelson, J. D. (2005). Finding useful questions: On Bayesian diagnosticity, probability, impact, and\ninformation gain. Psychological Review, 112(4), 979–999. https://doi.org/10.1037/0033-\n295X.112.4.979\n54. Nunan, D. (1987). Communicative language teaching: making it work. ELT Journal, 41, 136-145.\nhttp://dx.doi.org/10.1093/elt/41.2.136\n55. Oliver, D., Dobele, T., Greber, M., & Roberts, T. S. (2004). This course has a Bloom range of 3.9. IFAC\nSymposium on Advances in Control Education (227-231). Dunedin, NZ: Australian Computer Society\nInc.\n5\u0000. Omar, N., Haris, S.S., Hassan, R., Arshad, H., Rahmat, M., Zainal, N.F., & Zulki\u0000i, R. (2012). Automated\nanalysis of exam questions according to Bloom's taxonomy. Procedia - Social and Behavioral\nSciences, 59, 297-303.https://doi.org/10.1016/j.sbspro.2012.09.278\n57. Organisciak, P., Acar, S., Dumas, D., & Berthiaume, K. (2023). Beyond semantic distance: automated\nscoring of divergent thinking greatly improves with large language models. Thinking Skills and\nCreativity, 101356.\n5\u0000. Ortlieb, E., Bowden, R., Inman, A., Hu, B. Y., Pate, R. S., Gauthier, L. R., & Schorzman, E. M. (2012).\nEducational Research and Innovations. CEDER, Texas A&M University-Corpus Christi.\nhttps://hdl.handle.net/1969.6/97734\n59. Patterson, J. D., Barbot, B., Lloyd-Cox, J., & Beaty, R. E. (2023). AuDrA: An automated drawing\nassessment platform for evaluating creativity. Behavior Research Methods.\nhttps://doi.org/10.3758/s13428-023-02258-3\n\u00000. Patterson, J. D., Merseal, H. M., Johnson, D. R., Agnoli, S., Baas, M., Baker, B. S., ... & Beaty, R. E.\n(2023). Multilingual semantic distance: Automatic verbal creativity assessment in many languages.\nPsychology of Aesthetics, Creativity, and the Arts, 17(4), 495.\n\u00001. Pennington, J., Socher, R., & Manning, C. D. (2014, October). Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP) (pp. 1532-1543).\n\u00002. Peters, H., & Matz, S. (2023). Large Language Models Can Infer Psychological Dispositions of Social\nMedia Users. ArXiv.\n\u00003. Plack, M. M., Driscoll, M., Marquez, M., Cuppernull, L., Maring, J., & Greenberg, L. (2007). Assessing\nre\u0000ective writing on a pediatric clerkship by using a modi\u0000ed Bloom's Taxonomy. Ambulatory\nPediatrics: Tthe O\u0000cial Journal of the Ambulatory Pediatric Association, 7(4), 285–291.\nhttps://doi.org/10.1016/j.ambp.2007.04.006\nPage 23/25\n\u00004. Portelance, E., Degen, J., & Frank, M.C. (2020). Predicting age of acquisition in early word learning\nusing recurrent neural networks. Annual Meeting of the Cognitive Science Society.\n\u00005. Raphael, T., & McMahon, S. (1994). Book club: An alternative framework for reading instruction.\nReading Teacher - READ TEACH, 48, 102–116. https://doi.org/10.1598/RT.48.2.1\n\u0000\u0000. Rathje, S., Mirea, D. -M., Sucholutsky, I., Marjieh, R., Robertson, C., & Van Bavel, J. J. (2023). GPT is an\neffective tool for multilingual psychological text analysis. PsyArxiv.\nhttps://doi.org/10.31234/osf.io/sekf5\n\u00007. Raz, T., & Kenett, Y. N. (2023). Question asking as a mechanism that facilitates seeking of\ninformation [Peer commentary on Ivancovsky, T., Baror, S., & Bar, M. (2023). A shared novelty-\nseeking basis for creativity and curiosity]. Behavioral and Brain Sciences, 1-61.\nhttps://doi.org/10.1017/S0140525X23002807\n\u0000\u0000. Raz, T., Reiter-Palmon, R., & Kenett, Y. N. (2023). The Role of asking more complex questions in\ncreative thinking. Psychology of Aesthetics, Creativity, and the Arts.\nhttps://doi.org/10.1037/aca0000658\n\u00009. Reilly, J., Finley, A. M., Litovsky, C., & Kenett, Y. N. (2023). Bigram semantic distance as a measure of\nconceptual transitions in continuous natural language: Theory, tools, applications. Journal of\nExperimental Psychology: General, 152(9), 2578-2590. https://doi.org10.1037/xge0001389\n70. Reimers, N., & Gurevych, I. (2019). Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084.\n71. Reiter-Palmon, R., Forthmann, B., & Barbot, B. (2019). Scoring divergent thinking tests: A review and\nsystematic framework. Psychology of Aesthetics, Creativity, and the Arts, 13(2), 144–152.\nhttps://doi.org/10.1037/aca0000227\n72. Reja, U., Manfreda, K. L., Hlebec, V., & Vehovar, V. (2003). Open-ended vs. close-ended questions in\nWeb questionnaires. Developments in Applied Statistics, 19, 159–177.\n73. Ronfard, S., Zambrana, I. M., Hermansen, T. K., & Kelemen, D. (2018). Question-asking in childhood: A\nreview of the literature and a framework for understanding its development. Developmental Review,\n49, 101–120. https://doi.org/10.1016/j.dr.2018.05.002\n74. Rothe, A., Lake, B. M., & Gureckis, T. M. (2018). Do people ask good questions? Computational Brain\n& Behavior, 1(1), 69-89.https://doi.org/10.1007/s42113-018-0005-5\n75. Runco, M. A., & Mraz, W. (1992). Scoring divergent thinking tests using total ideational output and a\ncreativity index. Educational and Psychological Measurement, 52(1), 213–221.\nhttps://doi.org/10.1177/001316449205200126\n7\u0000. Runco, M. A., & Jaeger, G. J. (2012). The standard de\u0000nition of creativity. Creativity Research\nJournal, 24(1), 92–96. https://doi.org/10.1080/10400419.2012.650092\n77. Salmon, A. K., & Barrera, M. X. (2021). Intentional questioning to promote thinking and learning.\nThinking Skills and Creativity, 40, 100822. https://doi.org/10.1016/j.tsc.2021.100822\n7\u0000. Sasson, G., & Kenett, Y. N. (2023). A mirror to human question asking: Analyzing the Akinator online\nquestion game. Big Data and Cognitive Computing, 7, 26. https://doi.org/10.3390/bdcc7010026\nPage 24/25\n79. Shrout, P. E., & Fleiss, J. L. (1979). Intraclass correlations: uses in assessing rater reliability.\nPsychological Bulletin, 86(2), 420.\n\u00000. Silvia, P. J. (2008). Creativity and intelligence revisited: A latent variable analysis of Wallach and\nKogan (1965). Creativity Research Journal, 20(1), 34–39\nhttps://dx.doi.org/10.1080/10400410701841807\n\u00001. Stevenson, C., Smal, I., Baas, M., Grasman, R., & van der Maas, H. (2022). Putting GPT-3's Creativity\nto the (Alternative Uses) Test. ArXiv. arXiv:2206.08932.\n\u00002. Torrance, E. P. (1970). Group size and question performance of preprimary children. The Journal of\nPsychology: Interdisciplinary and Applied, 74(1), 71–75.\nhttps://doi.org/10.1080/00223980.1970.10545279\n\u00003. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I.\n(2017). Attention is all you need. ArXiv. http://arxiv.org/abs/1706.03762\n\u00004. Wei, J. M., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou,\nD., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., & Fedus, W. (2022). Emergent\nabilities of large language models. Arxiv.\n\u00005. Yager, R. E. (1996). Science/Technology/Society as Reform in Science Education. SUNY Press.\n\u0000\u0000. Yu, Y., Beaty, R. E., Forthmann, B., Beeman, M., Cruz, J. H., & Johnson, D. (2023). A MAD method to\nassess idea novelty: Improving validity of automatic scoring using maximum associative distance\n(MAD). Psychology of Aesthetics, Creativity, and the Arts.\n\u00007. Zhang, W., Deng, Y., Liu, B., Pan, S. J., & Bing, L. (2023). Sentiment analysis in the era of large\nlanguage models: A reality check. ArXiv. arXiv:2305.15005.\n\u0000\u0000. Zheng, A. Y., Lawhorn, J. K., Lumley, T., & Freeman, S. (2008). Assessment. Application of Bloom's\ntaxonomy debunks the \"MCAT myth\". Science, 319(5862), 414–415.\nhttps://doi.org/10.1126/science.1147852\nFigures\nPage 25/25\nFigure 1\nLinear Regressions Between Human-Rated Human Bloom Scores and Variables of Interest\nNote: Single responses are denoted by the black dots. Ideal performance (r = 1) is denoted by the black\nline, while the orange line is the line of best \u0000t. Word count, DSI, and MAD scores have been standardized\nfor clearer plotting. DSI = divergent semantic integration; MAD = maximum associative distance. All p-\nvalues < .001.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6508656740188599
    },
    {
      "name": "Language model",
      "score": 0.4754904508590698
    },
    {
      "name": "Natural language processing",
      "score": 0.4663030207157135
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4361916184425354
    },
    {
      "name": "Closed-ended question",
      "score": 0.41287365555763245
    },
    {
      "name": "Programming language",
      "score": 0.3498819172382355
    },
    {
      "name": "Linguistics",
      "score": 0.24389490485191345
    },
    {
      "name": "Philosophy",
      "score": 0.07960614562034607
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174306211",
      "name": "Technion – Israel Institute of Technology",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I130769515",
      "name": "Pennsylvania State University",
      "country": "US"
    }
  ],
  "cited_by": 9
}