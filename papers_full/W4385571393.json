{
  "title": "SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
  "url": "https://openalex.org/W4385571393",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2891595301",
      "name": "Xiaochuang Han",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2181361850",
      "name": "Sachin Kumar",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2234266251",
      "name": "Yulia Tsvetkov",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2170240176",
    "https://openalex.org/W3129651364",
    "https://openalex.org/W2789543585",
    "https://openalex.org/W3105604018",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W2890501761",
    "https://openalex.org/W4281781228",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W4221143575",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W4281690218",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W4300980637",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3127062506",
    "https://openalex.org/W4285304933",
    "https://openalex.org/W3101095987",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963434219",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4287329820",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W3170432046",
    "https://openalex.org/W3154046074",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2963536265",
    "https://openalex.org/W2970832665",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W4310695675",
    "https://openalex.org/W4288099666",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3126267552",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2959300817",
    "https://openalex.org/W2969958763",
    "https://openalex.org/W3121370741",
    "https://openalex.org/W4300532998",
    "https://openalex.org/W4386071831",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3174685870",
    "https://openalex.org/W2963538371",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W3162926177",
    "https://openalex.org/W3169017236",
    "https://openalex.org/W3141954417",
    "https://openalex.org/W2738371943",
    "https://openalex.org/W2093257937",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4386576626",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4206232983",
    "https://openalex.org/W3168053944",
    "https://openalex.org/W3036167779",
    "https://openalex.org/W4287028715",
    "https://openalex.org/W4287083626",
    "https://openalex.org/W4317897818",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3171218751",
    "https://openalex.org/W4281645945",
    "https://openalex.org/W4306802991",
    "https://openalex.org/W3114346701",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3099729825",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W2129069237",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4300425011"
  ],
  "abstract": "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM—a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 11575–11596\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model\nfor Text Generation and Modular Control\nXiaochuang Han♠ Sachin Kumar♣ Yulia Tsvetkov♠\n♠Paul G. Allen School of Computer Science & Engineering, University of Washington\n♣Language Technologies Institute, Carnegie Mellon University\n{xhan77, yuliats}@cs.washington.edu♠ sachink@cs.cmu.edu♣\nAbstract\nDespite the growing success of diffusion mod-\nels in continuous-valued domains (e.g., im-\nages), similar efforts for discrete domains such\nas text have yet to match the performance of au-\ntoregressive language models. In this work, we\npresent SSD-LM—a diffusion-based language\nmodel with two key design choices. First, SSD-\nLM is semi-autoregressive, iteratively gener-\nating blocks of text, allowing for flexible out-\nput length at decoding time while enabling lo-\ncal bidirectional context updates. Second, it\nis simplex-based, performing diffusion on the\nnatural vocabulary space rather than a learned\nlatent space, allowing us to incorporate clas-\nsifier guidance and modular control using off-\nthe-shelf classifiers without any adaptation. We\nevaluate SSD-LM on unconstrained text gen-\neration benchmarks, and show that it matches\nor outperforms strong autoregressive GPT-2\nmodels across standard quality and diversity\nmetrics, while vastly outperforming diffusion-\nbased baselines. On controlled text generation,\nSSD-LM also outperforms competitive base-\nlines, with an extra advantage in modularity.1\n1 Introduction\nDiffusion models (Sohl-Dickstein et al., 2015),\ntrained to iteratively refine noised inputs, have re-\ncently emerged as powerful tools for generative\nmodeling in several continuous-valued domains\nsuch as images (Ho et al., 2020), audio (Kong et al.,\n2021), video (Ho et al., 2022), among others. At-\ntempts to adapt them for discrete domains such as\ntext data, however, have only had limited success:\nprior work have shown to be promising on special-\nized cases and small datasets (Hoogeboom et al.,\n2021; Austin et al., 2021; Li et al., 2022; Chen et al.,\n2022), but diffusion models for text still underper-\nform (and thus are not widely adopted) compared to\nautoregressive language models (AR-LMs) which\n1Our code and models can be found at https://\ngithub.com/xhan77/ssd-lm.\nremain the state-of-the-art general purpose text gen-\nerators (Radford et al., 2019; Brown et al., 2020).\nDespite potential advantages of diffusion models\nfor text, there are two key challenges. First, dif-\nfusion models generate text non-autoregressively,\ni.e., they generate (and update) the entire sequence\nsimultaneously rather than token by token left-to-\nright. Although this property is useful in practice\nsince each output token is informed by a broader\nbi-directional context (Lee et al., 2018; Ghazvinine-\njad et al., 2019), it requires pre-defining an output\nsequence length. This limits the flexibility and ap-\nplicability of trained models. On the other hand,\nnon-autoregressive training with long sequences is\nexpensive and difficult to optimize. In this work,\nwe propose a semi-autoregressive solution which\nstrikes a balance between length flexibility and the\nability to alter previously generated tokens.\nA major advantage of diffusion models over the\ncurrent standard of autoregressive LMs is their\npost-hoc controllability using guidance from aux-\niliary models such as style classifiers (Dhariwal\nand Nichol, 2021). However, controllability is hard\nto achieve without compromises in modularity in\ndiffusion-based LMs for text. To enable diffusion\ngeneration into discrete text rather than continu-\nous modalities, prior approaches have employed\ndifferent approximations, e.g., training with embed-\ndings, character, or byte-level methods (Li et al.,\n2022; Hoogeboom et al., 2021; Austin et al., 2021;\nChen et al., 2022). In contrast, existing mainstream\nLMs and the guidance classifiers they derive often\noperate at a sub-word level with sub-word repre-\nsentations trained jointly with the language model\n(Devlin et al., 2019; Liu et al., 2019; Raffel et al.,\n2020). Subsequently, changing the input represen-\ntations to characters or embeddings requires devel-\noping guidance models from scratch, which can be\nexpensive or infeasible in many cases. In this work,\nwe propose a simplex-based solution which enables\nthe diffusion over discrete texts while maintaining\n11575\nthe advantages of diffusion models with plug-and-\ncontrol guidance models.\nIn sum, to enable diffusion-based LMs for text\nwe present SSD-LM (§3), addressing the above two\nchallenges. SSD-LM is trained to generate text\nsemi-autoregressively—generating blocks of to-\nkens left-to-right with bidirectional context within\nthe block—which offers the benefits of both AR-\nLMs and diffusion models. It supports training\nwith and generating variable-length sequences. At\nthe same time, it allows refinement within the token\nblock, in contrast to token-level autoregressive de-\ncoding where previously generated tokens cannot\nbe modified at all. SSD-LM uses the same tokeniza-\ntion as popular AR-LMs, representing discrete text\nvia a distribution (or simplex) defined over the vo-\ncabulary and is trained to reconstruct texts from\nnoisy versions of the distributions. Due to its un-\nderlying representation, our method also offers an\neasy and modular way of guided (controlled) gen-\neration using off-the-shelf text classifiers under the\nminimal assumption of shared tokenizer.\nOur evaluation experiments show, for the first\ntime, that a diffusion-based LM matches or out-\nperforms strong AR-LMs on standard text gener-\nation benchmarks (§4). We evaluate SSD-LM on\ntwo tasks: (1) unconstrained prompt-based gen-\neration substantially outperforming existing diffu-\nsion LM approaches and performing on par with\nor outperforming strong autoregressive LM GPT-\n2 (Radford et al., 2019) on both quality and diver-\nsity (§4.2); and (2) controlled text generation with\nguidance from off-the-shelf classifiers (no post-\nhoc training/adaptation) outperforming competitive\ncontrolled text generation baselines (§4.3).\n2 Background\n2.1 Diffusion model\nSince their inception as image generators, diffu-\nsion models (and their cousins score-based mod-\nels (Song and Ermon, 2019)) have been widely\nadopted as high-quality generative models for mul-\ntiple data modalities. Here, we briefly describe\na simplified view of a canonical method, denois-\ning diffusion probabilistic models (Ho et al., 2020,\nDDPM) which we adapt in this work for text\ngeneration. We assume a given dataset D =\n{1x0,..., Nx0}of continuous valued items ix0\n(e.g., pixel values of an image) henceforth referred\nto as x0 for simplicity.\nTraining Training a diffusion model first in-\nvolves adding a series of Gaussian noise to the\noriginal data x0, through T timesteps:\nxt = √¯αtx0 +\n√\n1 −¯αtϵt (1)\nwhere t ∈ (1,T) and ϵt ∼ N(0,I). ¯αt =∏t\nt′=1 αt′, where αt′ follow a predefined sched-\nule such that ¯αt →0 as t →T. This process is\ncalled forward diffusion. A diffusion model (pa-\nrameterized by θ) is trained to reverse this forward\nprocess by predicting the added noise ϵt given xt\nwith the following loss:\nL(θ) = Et∼U(1,T)∥ϵθ(xt,t) −ϵt∥2 (2)\nInference To get an output from this model, we\nsample xT ∼N(0,I) and iteratively reconstruct a\nsample x0 by going back in time,\nxt−1 = 1√αt\n(xt − 1 −αt√1 −¯αt\nϵθ(xt,t)) (3)\nfor t= T,..., 1.2 The key obstacle in using vanilla\ndiffusion models directly as text generators is that\nlanguage consists of discrete tokens, i.e., a non-\ncontinuous x0 to which a continuous valued Gaus-\nsian noise cannot be added. We propose a straight-\nforward and effective solution by treating tokens\nas continuous valued simplexes over the vocabu-\nlary (Hoang et al., 2017). Other existing methods\naddressing this problem are discussed in §5.\n2.2 Autoregressive LM\nAn autoregressive LM model optimizes for the like-\nlihood of a sequence of tokens w0,...,w L−1.\npθ(w0:L) =\nL−1∏\nc=0\npθ(wc |w<c) (4)\nTo decode from AR-LMs, one can provide a con-\ntext w<c and decode the next token wc iteratively\nby predicting pθ(wc |w<c) and sampling from it\nto get the discrete token (Fan et al., 2018; Holtz-\nman et al., 2020). Prior work has shown that these\ndecoding approaches (and by extension the LMs\nthemselves) are prone to degrade when generating\nlong sequences and often devolve into repeating\nsubsequences (Holtzman et al., 2020; Meister et al.,\n2022). In addition, such LMs do not provide a natu-\nral way to incorporate sequence-level control as to-\nkens are generated one at a time without the ability\n2We omit an additional noise term z here for simplicity,\nwhich is present in DDPM but not in another variant DDIM\n(Song et al., 2021).\n11576\nFigure 1: Training SSD-LM (a snapshot at context size\nc= 2, block size B = 3). Horizontal axis represents the\norder of tokens. Vertical axis represents the diffusion\ntimesteps. Shade means observable variables. Square\nmeans discrete vocabulary, while circle means continu-\nous logits. Red components are inputs to the learning\nmodel θ.\nFigure 2: Decoding from SSD-LM (continuing Fig-\nure 1). Red components are inputs to the learned model\nθ. Dash means intermediate variables.\nto modify previously generated tokens (Dathathri\net al., 2020; Kumar et al., 2022b). In this work,\nwe present a method to train a semi-autoregressive\nLM that decodes blocks of Btokens at a time, al-\nleviating said issues with the support of diffusion\nmodels. Existing literature addressing the two is-\nsues individually are discussed in §5.\n3 S SD-LM\nWe introduce SSD-LM —Semi-autoregressive\nSimplex-based Diffusion Language Model— adapt-\ning key components from both autoregressive LM\nand vanilla diffusion models. Conceptually, SSD-\nLM uses diffusion model to decode wc:c+B, a\nblock of tokens of lengthB, given a Gaussian noise\nand a context w<c of length c. We show an intu-\nitive diagram and pseudo-code for the training and\ndecoding algorithm of SSD-LM in Figure 1, Fig-\nure 2, and Figure 3.\n3.1 Training\nContinuous data representation To build a con-\ntinuous representation for discrete tokens, we adopt\nan almost-one-hot simplex representation over the\nmodel’s vocabulary V. We define a simple op-\neration logits-generation(.) to map a token w to\n˜w∈{−K,+K}|V|as follows.\n˜w(i) =\n{\n+Kwhen w= V(i)\n−Kwhen w̸= V(i)\n(5)\nwhere i is the index of the vocabulary. We call\n˜wthe logits for token w, and softmax( ˜w) gives\na probability simplex over the vocabulary V, with\na probability mass concentrated on the token w.\nThere is no learnable parameter in this mapping.\nForward diffusion Following Ho et al. (2020),\nwe add a time-dependent Gaussian noise to the\nlogits.\n˜wc:c+B\n0 = logits-generation(wc:c+B) (6)\n˜wc:c+B\nt = √¯αt˜wc:c+B\n0 +\n√\n1 −¯αtϵt (7)\nwhere t∈(1,T), ϵt ∼N(0,K2I), and ¯αt →0 as\nt→T. At the final step T, softmax( ˜wc:c+B\nT ) are\nfully noisy simplexes over V, with a logit-normal\ndistribution (Atchison and Shen, 1980).\nLoss function In Eq. 2, a diffusion model is\ntrained to predict the added noise from the noisy\nrepresentations. Since the forward diffusion pro-\ncess can be computed in a single step (Eq. 1), the\nnotion here is equivalent to predicting the origi-\nnal data representation (Song et al., 2021; Li et al.,\n2022). Our objective follows the same intuition but\nestimates a likelihood instead of the L2 distance\nwhile conditioning on additional context:3\nL(θ) = E[−log pθ(wc:c+B |˜wc:c+B\nt ,w<c)] (8)\n= E\n\n\nc+B−1∑\nj=c\n−log pθ(wj |˜wc:c+B\nt ,w<c)\n\n\n(9)\nE[·] is a shorthand for Ec∼U(1,L−B),t∼U(1,T)[·].\nThe architecture for θ throughout this work is a\nbi-directional Transformer encoder (Vaswani et al.,\n2017). Specifically, the input to the model is a con-\ncatenation of the context w<c and a sequence of\nnoisy vocabulary simplexes softmax( ˜wc:c+B\nt ) of\n3L2 distance did not work in our pilot study potentially\ndue to the intrinsically skewed simplex representation.\n11577\nAlgorithm 1 Training\n1: repeat\n2: w0:L ∼q(w0:L)\n3: c∼Uniform({1,...,L −B})\n4: ˜wc:c+B\n0 = logits-generation(wc:c+B)\n5: t∼Uniform({1,...,T })\n6: ϵ∼N(0,K2I)\n7: ˜wc:c+B\nt = √¯αt˜wc:c+B\n0 + √1 −¯αtϵ\n8: Take gradient descent step on\n∇θ[−∑c+B−1\nj=c log pθ(wj |˜wc:c+B\nt ,w<c)]\n9: until converged\nAlgorithm 2 Decoding (at a given c)\n1: ˜wc:c+B\nT ∼N(0,K2I)\n2: for t= T,..., 1 do\n3: wc:c+B\nlogits = logitsθ(wc:c+B |˜wc:c+B\nt ,w<c)\n4: ˆwc:c+B = logits-projection( wc:c+B\nlogits ) if uncontrolled,\nelse ˆwc:c+B = logits-projection(wc:c+B\nlogits + λ∇wfϕ(·))\n5: z∼N(0,K2I)\n6: ˜wc:c+B\nt−1 = √¯αt−1 ˆwc:c+B + √1 −¯αt−1z\n7: end for\n8: return argmax ˜wc:c+B\n0\nFigure 3: Training and decoding algorithms for SSD-LM . The training algorithm starts with sampling a sequence\nfrom the pretraining data q(w0:L). The decoding algorithm can be applied miterations to obtain a m·B-token\ngeneration, with the returned Btokens at each iteration appended to the previous generation, increasing c.\nlength B. The target output is the original tokens\nwc:c+B at positions cto c+ B.\nOne minimal modification made to the Trans-\nformer model is that in addition to the conventional\nembedding lookup for w<c, we modify the em-\nbedding layer to take as input a distribution over\nthe vocabulary, softmax( ˜wc:c+B\nt ), and compute\nthe embedding vector as a weighted sum of the\nembedding table. A timestep embedding is also\nadded before the first Transformer block to inform\nthe model of the current timestep.4\nIn §A, we present another interpretation of the\ntraining objective as an intuitive contrastive loss.\n3.2 Decoding\nLogits projection Similar to continuous-valued\ndiffusion models, sampling fromSSD-LM involves\nreverse diffusion from t = T,..., 1 starting with\na Gaussian noise. At any timestep t, our model θ\ntakes as input noised logits ˜wc:c+B\nt and estimates\nthe probability distribution of the original tokens\nin data by first predicting the logits:\nwc:c+B\nlogits,t = logitsθ(wc:c+B |˜wc:c+B\nt ,w<c) (10)\nwhich are then converted to a distribution via soft-\nmax. To feed this output to the next step of reverse\ndiffusion, t−1, we define alogits-projection oper-\nation to build a predicted data representation close\nto the initial data representation (almost-one-hot\nmapping; Eq. 5). We consider three projection\noperations.\n4More specifically, we have word embeddings for the con-\ntext, Embctx(w<c), and for the noisy diffusion representa-\ntions, Wdiff[softmax( ˜wc:c+B\nt )]. The timestep embedding is\nadded to the diffusion word embeddings, Wtime(t/T). It is\nsimilar to positional embeddings, just not varying across se-\nquence positions. We fold it in θfor notation simplicity.\n• Greedy: creates an almost-one-hot logit cen-\ntered at the highest probability token.5\nˆw(i)=\n{\n+Kif i= argmax(wlogits)\n−Kotherwise (11)\n• Sampling: creates an almost-one-hot logit cen-\ntered around a token sampled from the output\ndistribution using top-psampling (Holtzman\net al., 2020). pis a hyperparameter.\nˆw(i)=\n{\n+Kif i=top-p-sample(wlogits)\n−Kotherwise\n(12)\n• Multi-hot: creates an almost-one-hot logit cen-\ntered around all tokens in the top-pnucleus.\nˆw(i)=\n{\n+Kif i∈top-p-all(wlogits)\n−Kotherwise (13)\nDecoding iteration Starting from pure noise\n˜wc:c+B\nT ∼N(0,K2I), in each decoding timestep\nwe compute:\nˆwc:c+B\nt = logits-projection(wc:c+B\nlogits,t) (14)\n˜wc:c+B\nt−1 = √¯αt−1 ˆwc:c+B\nt +\n√\n1 −¯αt−1z (15)\nfor t= T,..., 1 and z∼N(0,K2I).\nAt t = 1, the final B-token block is computed\nsimply as argmax ˜wc:c+B\n0 . To generate the next\nblock, we concatenate the generated block to the\nprevious context to create a new context of length\nc+Band follow the reverse-diffusion process again\nas described above. This process can be repeated\nuntil the maximum desired length is reached.6\n5This shares a similar intuition as a greedy clamping trick\nin the embedding-based diffusion in Li et al. (2022).\n6Alternatively, one can also terminate the process if certain\nspecial end-of-sequence tokens have been generated.\n11578\nIt is worth noting that our proposed decoding\nalgorithm is novel and different from the DDPM\ndecoding (Eq. 3). The DDPM decoding is designed\nfor diffusion in a continuous space and failed to\ngenerate sensible outputs in our preliminary ex-\nperiments based on simplexes. In §B, we draw a\ntheoretical connection between our decoding algo-\nrithm and DDPM decoding, and also highlight the\nintuitive difference between the two.\nHighly-modular control A useful property of\ncontinuous diffusion models that naturally arises\nfrom their definition is the ability to guide the gen-\nerated samples to have user-defined attributes at\ntest time. This can be done using gradients from\nauxiliary models such as classifiers (Dhariwal and\nNichol, 2021), e.g., guiding the output of an LM to\nbe of a positive sentiment using a sentiment classi-\nfier. There is a vibrant community of developers on\nplatforms such as HuggingFace where many such\ntext classifiers are publicly available. The under-\nlying data representation of SSD-LM is based on\nvocabulary simplexes. Hence, as long as a classi-\nfier shares the same tokenizer as the LM, it can be\nused for control in an off-the-shelf manner without\nmodifications. This is in contrast to prior work in\ndiffusion language models that do not support such\nclassifiers due to differences in their input represen-\ntation space (Hoogeboom et al., 2021; Austin et al.,\n2021; Li et al., 2022; Chen et al., 2022) and require\nretraining the classifiers from scratch. This ability\nmakes SSD-LM highly modular for controlled text\ngeneration and offers key benefits: (1) Training\naccurate classifiers for many tasks requires huge\namounts of data where retraining them can be quite\nexpensive, and (2) this approach allows control\nfrom classifiers that are open to use but have been\ntrained on closed source data.\nTo guide SSD-LM to generate texts with a target\nattribute yvia a standalone attribute model fϕ(·),\nwe update wc:c+B\nlogits,t (Eq. 10) at each timestep tto\nthe form below, drifting according to the gradients\nfrom the attribute classifier.\nwc:c+B\nlogits,t + λ∇wc:c+B\nlogits,t\nfϕ(y|wc:c+B\nlogits,t,w<c) (16)\nwhere λis a hyperparameter balancing the weight\nof control. The parameters of the standalone at-\ntribute model ϕare frozen. We make a trivial mod-\nification to the embedding computation as in §3.1,\nto allow the classifier to take as input a simplex.\n3.3 Additional details\nForward diffusion coefficient ¯αt We follow\nNichol and Dhariwal (2021) for a cosine sched-\nule of ¯αt:\n¯αt = r(t)\nr(0), r(t) = cos(t/T + s\n1 + s ·π\n2 )2 (17)\nwhere sis small offset set to 1e-4 in our work and\nαt = ¯αt\n¯αt−1\n.\nFewer timesteps T in decoding Decoding from\ndiffusion models requires a series of timesteps (T)\nwhich can be computationally expensive if T is\nlarge. Following Li et al. (2022), we consider using\na smaller value of T at test time to improve decod-\ning speed. In this work, we primarily experiment\nwith Tdecode = Ttrain\n2 and Tdecode = Ttrain\n5 .\nFlexible decoding block size B Our SSD-LM is\ntrained with a fixed token block size Btrain. How-\never, the decoding algorithm has a freedom to\nuse a different Bdecode. In our experiments, we\nconsider both scenarios of Btrain = Bdecode and\nBtrain ̸= Bdecode. Nevertheless, we leave for fu-\nture work a more detailed analysis of the impact of\nthe difference between Btrain and Bdecode on model\nperformance.\n4 Experiments\n4.1 S SD-LM pretraining setup\nModel architecture We use a bidirectional\nTransformer encoder RoBERTa-large (Liu et al.,\n2019) (0.4B, comparable size to GPT2-medium)\nas SSD-LM ’s underlying architecture.7 Note that\nRoBERTa uses a general BPE tokenization (Sen-\nnrich et al., 2016), same as a variety of LMs such as\nGPT-2 (Radford et al., 2019), GPT-3 (Brown et al.,\n2020), OPT (Zhang et al., 2022), etc. Any attribute\nclassifier using the same tokenization strategy can\nbe used to control SSD-LM in a highly modular\nway.\nPretraining data, constants, and resource We\ntrain SSD-LM on the same data as GPT2 to\nmake fair comparisons possible: OpenWebText\n(Gokaslan and Cohen, 2019) which contains 9B\ntokens. Following Zhang et al. (2022), we consider\n7We initialize the model with RoBERTa’s weights as well.\nWe observe in our initial exploration that it helps the training\nloss converge faster than a randomly initialized model. How-\never, given enough computational resources, we conjecture\nthat a randomly initialized model will offer similar perfor-\nmance.\n11579\nthis data as one contiguous sequence of tokens\nand break it into sequences of length 200 (same as\nthe maximum sequence length our model accepts).\nWe randomly sample 99% of these sequences for\npretraining while leaving the rest as held out for\nevaluation. We use the following model hyperpa-\nrameters:8\nL= 200,Btrain = 25,Ttrain = 5000,K = 5\nWe use an aggregated batch size of 6,144 and\na learning rate of 1e-4 with an AdamW optimizer\n(Loshchilov and Hutter, 2019). We trained SSD-\nLM for 100K steps, which took about 6 days on 32\nNvidia V100 GPUs.\nPretraining loss Canonical training-time per-\nplexity of LMs is not compatible with diffusion\nLMs due to the difference in the inputs to the mod-\nels (Eq. 4 and Eq. 9). Our pretraining loss is a\nper-token negative log-likelihood (NLL) that de-\npends on the specific noise schedule being used.\nSSD-LM gets an average NLL of 3.87 at the end\nof pretraining. We show a pretraining loss curve in\nthe appendix (§D).\n4.2 Unconstrained text generation\nSetup First, we benchmark SSD-LM with au-\ntoregressive LMs trained on the same data (GPT2)\non text generation quality. We randomly sample\n1000 sequences from the held-out OpenWebText\ntest data, extract their prefixes as prompts (context),\nand generate continuations from the LMs. We con-\nsider three setups: with prompt lengths 25, 50 and\n100 with respective output lengths as 25, 50 and\n100 tokens. In each setup, we sample 5 continu-\nations for each input context, thus comparing the\nquality of 5,000 generations from baseline GPT-2\nmodels and our SSD-LM.\nWe compare SSD-LM with GPT2-medium,\nlarge and xl models (containing 0.4B, 0.8B and\n1.6B parameters respectively) as baselines. For\nreference, our model size is comparable to GPT2-\nmedium. We experiment with two popular decod-\ning strategies for the baseline GPT-2 models with\ncanonical parameters: nucleus sampling (Holtzman\net al., 2020) with a top-pof 0.9 and 0.95, and typi-\ncal sampling (Meister et al., 2022) with a typical-τ\nof 0.2 and 0.95.\n8Future work can do a search given more resources.\n9MAUVE, Dist-1/2/3, and Rep are in percentage. PPL is\nobtained through a micro average following Holtzman et al.\n(2020); Pillutla et al. (2021); Meister et al. (2022).\nFor SSD-LM , we consider three logits pro-\njection strategies, sampling and multi-hot with\ntop-p ∈ {0.0,0.1,0.2,0.5,0.7,0.9,0.95,0.99},\nand greedy (which is functionally equivalent to\nthe sampling with top- p=0). We use a test block\nsize (Bdecode) of 25. When generating samples of\nlength 50 or 100, we semi-autoregressively sample\nin blocks of 25 and feed them as additional context\nto generate the next block as described in §3.2.\nWe evaluate the generated continuations on two\naxes: quality and diversity. As automatic quality\nmetrics, we report perplexity measured by a sepa-\nrate, larger language model (GPT-Neo-1.3B, Black\net al., 2021). Prior works, however, have shown\nthat low perplexity of generated text is not neces-\nsarily an indication of high quality but of degen-\nerate behavior (Nadeem et al., 2020; Zhang et al.,\n2021) and have proposed closeness to the perplex-\nity of human-written text as a better evaluation.\nHence, we also report the difference of log perplex-\nity between the generated text and human-written\ncontinuations (|∆log PPL|). For diversity evaluation,\nwe report Zipf’s coefficient (Zipf) and average dis-\ntinct n-grams in the output samples (Li et al., 2016,\nDist-n). In addition, we also report the repetition\nrate (Welleck et al., 2020; Holtzman et al., 2020,\nRep), measuring the proportion of output samples\nthat end in repeating phrases. Finally, we report\nMAUVE (Pillutla et al., 2021) which evaluates both\nquality and diversity together by approximating in-\nformation divergence between generated samples\nand human-written continuations (from the Open-\nWebText held-out set).\nResults Table 1 summarizes our main results on\nthe 50-token prompt and output setup. We report\nthe numbers for the best performing three settings\nfor logits projection and decoding steps T in SSD-\nLM. We report the best setting for the baselines.\nThe results for other generation lengths have a sim-\nilar trend and can be found in the appendix (§D).\nWe find that SSD-LM , though being smaller in\nsize, outperforms larger GPT-2 models on the uni-\nfied metric MAUVE. On diversity,SSD-LM outper-\nforms GPT-2 in Dist-nwhile achieving lower repe-\ntition rates. On perplexity, the results are slightly\nmixed. We observe a trade-off between MAUVE\nand perplexity for different settings we considered,\nindicating that further tuning of the hyperparam-\neters may be required. However, one of our best\nperforming settings (sampling top-p=0.9, T=2500)\nstill achieves the closest perplexity to the gold con-\n11580\n(Length 50) MAUVE\n↑\nPPL\n−−→\ngold\n|∆log PPL|\n↓\nDist-1\n↑\nDist-2\n↑\nDist-3\n↑\nZipf\n−−→\ngold\nRep\n↓\nGold continuation 100.00 17.75 0.00 88.62 95.88 93.71 0.88 0.10\nGPT2-medium (Best config)\nTop-p=0.95 96.57\n±0.40\n12.72\n±0.07\n0.33 66.31\n±0.11\n91.77\n±0.03\n92.75\n±0.06\n1.01 0.26\n±0.04\nGPT2-large (Best config)\nTop-p=0.95 96.41\n±0.78\n10.57\n±0.05\n0.51 64.91\n±0.13\n90.88\n±0.06\n92.38\n±0.05\n1.01 0.41\n±0.06\nGPT2-xl (Best config)\nTypical-τ=0.95 97.03\n±0.50\n10.33\n±0.04\n0.54 64.87\n±0.15\n90.69\n±0.07\n92.16\n±0.05\n1.01 0.37\n±0.04\nSSD-LM-“medium” (Top-3)\nSampling p=0.99, T=1000 97.89 30.68 0.54 68.99 92.60 92.94 1.01 0.16\nSampling p=0.95, T=1000 96.64 27.34 0.43 67.75 92.16 92.91 1.01 0.16\nSampling p=0.9, T=2500 96.46 20.56 0.14 66.61 91.46 92.56 1.05 0.26\nTable 1: Unconstrained generation evaluation of SSD-LM and GPT-2 models at length 50. For GPT-2 models,\nthe results are averaged across 5 random seeds, and we show the best sampling parameter configuration. For\nour SSD-LM , we show the top-3 configurations. All configurations are ranked based on MAUVE, with original\nparameters from Pillutla et al. (2021). The perplexity (PPL) is measured by GPT-Neo-1.3B.9\n(ROCStories) MAUVE PPL\nGold continuation 100.00 18.57\nDiffusion-LM 46.11 35.96\nSSD-LM 87.22 22.91\nTable 2: Unconstrained generation results of SSD-LM\nand Diffusion-LM on ROCStories with 50 prompt to-\nkens and 50 output tokens. We report the MAUVE score\nbetween the gold continuation and model generations.\nWe also show the perplexity (PPL) of model generations\nmeasured by GPT-Neo-1.3B.10\ntinuation.\nIn §D, we show the influence of different logits\nprojection strategies and the associated parameters\non the output text quality in Figure 4. We also\nshow qualitative examples of the generations by\nSSD-LM in Table 8 and a trajectory of intermediate\nstates during the decoding process in Table 9.\nComparison with Li et al. (2022) A prior work\nto us, Li et al. (2022) propose Diffusion-LM, an\nembedding-based diffusion model trained on two\nsmall toy datasets, E2E (Novikova et al., 2017)\nand ROCStories (Mostafazadeh et al., 2016). In\nthis subsection, we make a diversion to compare\nthe embedding-based Diffusion-LM with our semi-\nautoregressive, simplex-based SSD-LM. Following\nLi et al. (2022), we train a Diffusion-LM on ROC-\n10Due to a lowercase tokenization of ROCStories, we use\nBERT-base-uncased as MAUVE’s embedding model here.\nStories with a default embedding size of 128, 0.1B\nparameters under a BERT-base (Devlin et al., 2019)\nstructure,11 and a sequence length of 100. For a fair\ncomparison, only within this subsection we train\na SSD-LM with ROCStories sequences of 100 to-\nkens, a decoding block size of 25, and a BERT-base\ninitialization. Further details of the setup can be\nfound in §C.\nOn 2,700 held-out ROCStories sequences, we\nuse the first 50 tokens of each sequence as a prompt\nand have the model generate the next 50. In Ta-\nble 2, we show the MAUVE score and perplexity\nof both models. We observe a substantially higher\nMAUVE score and lower perplexity withSSD-LM.\n4.3 Controlled text generation\nSetup To evaluate SSD-LM ’s ability for highly-\nmodular control, we consider the task of sentiment\ncontrolled generation where given a prompt, the\ngoal is to generate a continuation with a positive (or\nnegative) polarity. We use a set of 15 short prompts\nas in Dathathri et al. (2020) and generate 20 sam-\nples per prompt per sentiment category, making\nthe total number of generated samples to be 600.\nFollowing Mireshghallah et al. (2022), we generate\nsamples with 3 different output lengths: 12, 20 and\n50. For guidance, we simply import a popular sen-\n11We train two versions of Diffusion-LM, with and without\nBERT’s encoder weights as an initialization. The default no-\ninitialization setup as in Li et al. (2022) works reasonably,\nwhile the other degenerates. Details can be found in §C.\n11581\n(Length 50) C-Ext. (Int.) PPL Dist-1/2/3\nDAPTCM 79.8 57.2 61/92/94\nPPLMCC 60.7 (73.6) 29.0 -\nFUDGECC 59.1 8.4 47/83/92\nGeDiCM 99.2 107.3 71/93/92\nDExpertsCM 94.8 37.1 56/90/92\nMuCoLaCC 86.0 27.8 52/76/80\nM&M LMHMC 68.6 (93.8) 122.3 -\nSSD-LMHMC 94.1 (99.0) 23.1 46/84/92\nTable 3: Controlled text generation results of SSD-LM\nand baselines at length 50. We report the external clas-\nsifier’s accuracy (C-Ext.) for the generations and addi-\ntionally the internal (guidance) classifier accuracy (Int.)\nif available. The perplexity (PPL) is computed with\nGPT2-xl. MuCoLa is the version using two discrim-\ninators. CM stands for customized language model,\nCC stands for customized classifier, and HMC stands\nfor highly-modular classifier (in an order of increasing\nmodularity). The best of all results are boldfaced, and\nthe best of HMC results are italicized.14\ntiment classifier12 from HuggingFace trained with\nTwitter sentiment data with over 58M training ex-\namples (Barbieri et al., 2020). This model serves as\nfϕ(·) as shown in Eq. 16. In addition to quality and\ndiversity of the generated samples, we also evaluate\nthem on control (that is measuring if the generated\noutput is actually positive or negative in polarity).\nFor this, we use an external sentiment classifier\ntrained on a different dataset. Specifically, we use a\nclassifier trained with Yelp reviews13 (Zhang et al.,\n2015; Morris et al., 2020) following the evaluation\nsetup in the baselines we consider.\nAgain, we consider the sampling and multi-hot\ndecoding strategies with top-p ∈{0.2,0.5,0.9},\nTdecode ∈{1000,2500,5000}, and the multiplier\nfor control λ∈{0,100,500,2000}. For the gener-\nation of 12/20/50 tokens, we use Bdecode=12/20/25\nand apply the decoding algorithm for m=1/1/2 iter-\nations respectively.\nResults We show the quality of the controlled\ngenerations from three perspectives: target attribute\nvia the external classifier accuracy, fluency via per-\nplexity, and diversity via the distinctiveness mea-\nsures. In Table 3, we show the experimental results\nfor output length 50. The results at length 12 and\n12https://huggingface.co/cardiffnlp/twitter-\nroberta-base-sentiment\n13https://huggingface.co/textattack/bert-base-\nuncased-yelp-polarity\n20 have a similar trend and can be found in the\nappendix (§D).\nAmong the baseline methods, DAPT (Gururan-\ngan et al., 2020), GeDi (Krause et al., 2021), and\nDExperts (Liu et al., 2021) require training cus-\ntomized language models aware of the desired\nattributes (denoted as CM in Table 7). PPLM\n(Dathathri et al., 2020), FUDGE (Yang and Klein,\n2021), and MuCoLa (Kumar et al., 2022b) re-\nquire training a customized attribute classifier (CC).\nWhile our proposed method SSD-LM and M&M\nLM (Mireshghallah et al., 2022) can directly im-\nport mainstream existing attribute classifiers from\nplatforms like HuggingFace and are thus highly\nmodular (HMC). We show the baseline results as\nreported in Mireshghallah et al. (2022) and Kumar\net al. (2022b).\nSSD-LM shows strong controllability while pos-\nsessing great modularity. SSD-LM outperforms\nM&M LM, the other HMC method by a large mar-\ngin. Even when comparing with the CC and CM\nmethods, our method achieves a good balance in\ncontrol, fluency, and diversity.\nIn §D, we show the impact of the control weight\nλand top-pon the attribute accuracy and perplexity\nin Figure 5. We also show qualitative examples of\nthe controlled generations by SSD-LM in Table 8.\n5 Related work\nDiffusion models Diffusion models have\ndemonstrated impressive performance in popular\ncontinuous-valued domains such as images (Ho\net al., 2020), audio (Kong et al., 2021), video (Ho\net al., 2022) and recently also been adopted for\n3D-shapes, protein structures, and more (Zhou\net al., 2021; Trippe et al., 2022; Wu et al.,\n2022). Since they are based on adding Gaussian\nnoise, these approaches are not straightforward\nto apply to discrete valued domains like text.\nHoogeboom et al. (2021); Austin et al. (2021)\npropose diffusing in the discrete space using\ncategorical distributions which are modified using\ntransition matrices. However, these methods do not\nstraightforwardly support control and yield worse\nresults than comparable autoregressive models.\nLi et al. (2022) propose to represent each token\nas a continuous embedding and apply diffusion\nin the embedding space. They train the LM to\ngenerate a fixed length sequence whereas SSD-LM\n14PPL is obtained through a macro average following Ku-\nmar et al. (2022b).\n11582\nallows flexibility in the generated sequence length\nby generating block-wise. Further, their LM is\ntrained with specialized datasets and not evaluated\nagainst general-purpose autoregressive LMs on\nunconstrained text generation. Their method\nsupports post-hoc control but requires training a\ncustomized attribute classifier,15 since the diffusion\noperates on a learned embedding space. Gong et al.\n(2022), a concurrent work to ours, extend Li et al.\n(2022) to a sequence-to-sequence setup with a\nsimilar underlying embedding-based method. Our\nwork is most closely related to Chen et al. (2022)\nwhich transform discrete data into a sequence of\nbits and represent each bit as +1 or -1 converting it\ninto a continuous-valued domain. For textual data,\nhowever, it can lead to extremely long sequences\nwhich are difficult to optimize. In this work, we\ninstead maintain a subword based vocabulary but\nrepresent each token as a sequence of manually\ndefined logits.\nLanguage models The majority of existing lan-\nguage models for text generation are trained au-\ntoregressively, i.e., they predict the next token\ngiven previously generated context. This paradigm\nscaled up both in terms of model size and training\ndata size has resulted in impressive capabilities on\nmany benchmarks (Brown et al., 2020; Chowdh-\nery et al., 2022). However, they generate text one\ntoken at a time which does not provide flexible\ncontrol over attributes of the generated text. Non-\nautoregressive models which generate the entire\noutput sequence at the same time have also been\nexplored in prior work other than diffusion mod-\nels (Lee et al., 2018; Ghazvininejad et al., 2019).\nHowever, they are primarily focused on improv-\ning decoding efficiency and applied for special-\nized tasks like translation (Gu et al., 2018; Kaiser\net al., 2018; Wang et al., 2019) and text editing (Gu\net al., 2019). Many of these work have iterative\nprocesses in a discrete space, with some exploring\ncontinuous representations (Ma et al., 2019; Lee\net al., 2020). To address the quality decline with\nthe non-autoregressive methods compared to au-\ntoregressive models, prior work have also explored\nsemi-autoregressive approaches (Wang et al., 2018;\nQi et al., 2021). In the same vein, our work seeks to\naddress the drawbacks of autoregressive language\nmodels and non-autoregressive diffusion models\n15The control for diffusion models can also be classifier-\nfree (Ho and Salimans, 2021) but requires training with the\ntarget attribute in advance, which is not a focus of this work.\nwith a middle ground.\nControllable text generation Early solutions for\ncontrolling attributes of generated text focused on\ntraining or finetuning AR-LMs with specific con-\ntrol codes (Keskar et al., 2019; Gururangan et al.,\n2020; Chan et al., 2021). These methods are diffi-\ncult to extend to new controls as it requires retrain-\ning the models. More recent work includes decod-\ning approaches from pretrained AR-LMs without\nmodifying the models, through altering the output\nprobability distribution at each step using different\ncontrol objectives (Dathathri et al., 2020; Krause\net al., 2021; Yang and Klein, 2021; Liu et al., 2021;\nLu et al., 2021; Pascual et al., 2021). However,\nthese methods do not allow modifying a token once\nit is generated and are thus suboptimal for controls\nat the scope of the whole sequence. Closely re-\nlated to SSD-LM are Kumar et al. (2021); Qin\net al. (2022); Kumar et al. (2022b), which propose\ngradient-based decoding algorithms from AR-LMs.\nThey require computing a backward pass through\nthe LMs for each iteration, an expensive operation.\nIn contrast, SSD-LM with its semi-autoregressive\nsetup allows editing past tokens via diffusion. In\naddition, most of these approaches require training\ncontrol functions from scratch whereas our model\nallows using off-the-shelf classifiers. Mireshghal-\nlah et al. (2022) propose a non-autoregressive LM\nbased on Metropolis-Hastings sampling. It also\nsupports off-the-shelf classifiers for control, and\nwe therefore use it as a direct baseline for SSD-\nLM.\n6 Conclusion\nWe present SSD-LM , a semi-autoregressive dif-\nfusion based language model trained to denoise\ncorrupted simplexes over the output vocabulary.\nCompared to prior work in text-based diffusion,\nSSD-LM offers more flexibility in output length\nby generating blocks of text and an ability to use\noff-the-shelf attribute classifiers for control without\nadditional tuning. On unconstrained text genera-\ntion, SSD-LM performs on par with or outperforms\nstrong and larger autoregressive baselines (GPT-2)\nin generation quality and diversity, while vastly\noutperforming diffusion baselines (Diffusion-LM).\nOn controlled text generation, SSD-LM surpasses\nbaselines while possessing an easy-to-use modular\ndesign. We believe that SSD-LM opens an exciting\ndirection for future research in flexible and modular\ndiffusion-based language generation.\n11583\nLimitations\nSample efficiency In AR-LMs, an NLL loss is\ncomputed at training time for every token in the\nsequence of length L(Eq. 4). However, in SSD-\nLM, each time a pretraining example is sampled,\nthe loss is computed on only Btokens (Eq. 9) lead-\ning to a lower sample efficiency than AR-LM. To-\nwards improving this efficiency, future work could\nexplore model architectures dedicated to semi-\nautoregressive diffusion rather than the vanilla\nTransformer encoder we use in this work.\nDecoding speed Since each block is generated\nby refining over several iterations, SSD-LM has a\nconsiderably slower decoding speed than autore-\ngressive models. For example, given a context\nof 50 tokens (single instance, unbatched), it takes\nSSD-LM 25 seconds to generate the next block of\n25 tokens (Tdecode=1000). While our work focused\non establishing the efficacy of diffusion-based LMs\nand modular controlled generation, future work\ncould explore tuning Tdecode to balance model per-\nformance and decoding speed, or more efficient\ntraining and decoding algorithms extending ideas\nfrom prior work on diffusion models for continuous\ndomains (Song et al., 2021; Nichol and Dhariwal,\n2021; Rombach et al., 2022; Meng et al., 2022).\nDecoding block size In this work, although we\nallow setups where Btrain ̸= Bdecode, the decoding\nblock size Bdecode remains the same across mde-\ncoding iterations, leaving space for a more flexible\ndecoding schedule. Future work can also explore\nlearning Bdecode (and Btrain) rather than using con-\nstant pre-defined lengths.\nLarger scale experiments with different kinds\nof controls and their combinations can be done,\nas well as more sophisticated ways to incorporate\nthem (Kumar et al., 2021). In addition, we plan\nto explore alternative methods to continuously rep-\nresent and add noise to discrete text (Bakosi and\nRistorcelli, 2013). This work experiments with pre-\ntraining data that is primarily in English. Future\nwork can also explore challenges and benefits of\ndiffusion-based LMs in a multilingual setup.\nEthics statement\nLanguage models trained on data from the web\ncan perpetuate social biases and toxic interactions,\nand can be prone to generating harmful language\n(Gehman et al., 2020; Wallace et al., 2019, 2020;\nSheng et al., 2021; Weidinger et al., 2022). Fur-\nther, language generation models could memo-\nrize and amplify patterns in data without deeper\nlanguage understanding or control, so they can\nbe factually inconsistent and generate disinforma-\ntion (Maynez et al., 2020; Pagnoni et al., 2021;\nZellers et al., 2019), or can compromise user pri-\nvacy (Carlini et al., 2021). Prior works have out-\nlined these risks (Sheng et al., 2021; Weidinger\net al., 2021), discussed their points of origin, and\nadvocated for future research on ethical develop-\nment of LMs (Bender et al., 2021; Solaiman et al.,\n2019).\nWhile these studies have been conducted for au-\ntoregressive LMs, our diffusion-based LM is sub-\nject to these problems as well. However, since our\nmethod naturally incorporates controllability, fu-\nture work may explore control functions that could\npotentially alleviate these issues (Liu et al., 2021;\nKumar et al., 2022b). One risk is that controllability\ncan also be misused maliciously, with models being\nintentionally exploited to generate biased, toxic, or\nnon-factual content (Bagdasaryan and Shmatikov,\n2022; Pagnoni et al., 2022). Therefore, apart from\ncontrolled generation, future work should aim to de-\ntect the generations under control as well to defend\nagainst the malicious use (Kumar et al., 2022a).\nAcknowledgements\nThe authors would like to thank Tianxiao Shen,\nTianxing He, Jiacheng Liu, Ruiqi Zhong, Sidney\nLisanza, Jacob Gershon, members of TsvetShop,\nand the anonymous ACL reviewers for their help-\nful discussions and feedback. X.H. gratefully ac-\nknowledges funding from the UW-Meta AI Men-\ntorship program. S.K. gratefully acknowledges\na Google Ph.D. Fellowship. Y .T. gratefully ac-\nknowledges an Alfred P. Sloan Foundation Fellow-\nship. This research is supported in part by by the\nNational Science Foundation (NSF) under Grants\nNo. IIS2203097, IIS2125201, and NSF CAREER\nGrant No. IIS2142739. This research is supported\nin part by the Office of the Director of National\nIntelligence (ODNI), Intelligence Advanced Re-\nsearch Projects Activity (IARPA), via the HIATUS\nProgram contract #2022-22072200004. The views\nand conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily\nrepresenting the official policies, either expressed\nor implied, of ODNI, IARPA, or the U.S. Gov-\nernment. The U.S. Government is authorized to\n11584\nreproduce and distribute reprints for governmental\npurposes notwithstanding any copyright annotation\ntherein.\nReferences\nJ. Atchison and S.M. Shen. 1980. Logistic-normal dis-\ntributions:Some properties and uses. Biometrika,\n67(2):261–272.\nJacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel\nTarlow, and Rianne van den Berg. 2021. Structured\ndenoising diffusion models in discrete state-spaces.\nIn Proc. NeurIPS.\nEugene Bagdasaryan and Vitaly Shmatikov. 2022. Spin-\nning language models: Risks of propaganda-as-a-\nservice and countermeasures. In 2022 IEEE Sympo-\nsium on Security and Privacy (SP), pages 1532–1532.\nIEEE Computer Society.\nJózsef Bakosi and J. Raymond Ristorcelli. 2013. A\nstochastic diffusion process for the dirichlet distribu-\ntion. arXiv: Mathematical Physics.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. Tweeteval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of EMNLP.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proc. FAccT.\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. ArXiv,\nabs/2005.14165.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.\nExtracting training data from large language models.\nIn USENIX Security Symposium, pages 2633–2650.\nAlvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang,\nand Jie Fu. 2021. Cocon: A self-supervised approach\nfor controlled text generation. In Proc. ICLR.\nTing Chen, Ruixiang Zhang, and Geo rey E. Hinton.\n2022. Analog bits: Generating discrete data us-\ning diffusion models with self-conditioning. ArXiv,\nabs/2208.04202.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek B Rao,\nParker Barnes, Yi Tay, Noam M. Shazeer, Vinodku-\nmar Prabhakaran, Emily Reif, Nan Du, Benton C.\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier García,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Re-\nwon Child, Oleksandr Polozov, Katherine Lee, Zong-\nwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. ArXiv, abs/2204.02311.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nProc. ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proc. NAACL-HLT.\nPrafulla Dhariwal and Alex Nichol. 2021. Diffu-\nsion models beat gans on image synthesis. ArXiv,\nabs/2105.05233.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proc. ACL.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProc. EMNLP.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus.\nShansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu,\nand Lingpeng Kong. 2022. Diffuseq: Sequence to se-\nquence text generation with diffusion models. ArXiv,\nabs/2210.08933.\n11585\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In Proc. ICLR.\nJiatao Gu, Changhan Wang, and Jake Zhao. 2019. Lev-\nenshtein transformer. In Proc. NeurIPS.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProc. ACL.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. In Proc.\nNeurIPS.\nJonathan Ho and Tim Salimans. 2021. Classifier-free\ndiffusion guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applica-\ntions.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. 2022.\nVideo diffusion models. ArXiv, abs/2204.03458.\nCong Duy Vu Hoang, Gholamreza Haffari, and Trevor\nCohn. 2017. Towards decoding as continuous optimi-\nsation in neural machine translation. In Proceedings\nof the 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 146–156, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In Proc. ICLR.\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini,\nPatrick Forré, and Max Welling. 2021. Argmax flows\nand multinomial diffusion: Learning categorical dis-\ntributions. In Proc. NeurIPS.\nLukasz Kaiser, Samy Bengio, Aurko Roy, Ashish\nVaswani, Niki Parmar, Jakob Uszkoreit, and Noam\nShazeer. 2018. Fast decoding in sequence models\nusing discrete latent variables. In Proc. ICML, pages\n2390–2399. PMLR.\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. arXiv preprint arXiv:1909.05858.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. 2021. Diffwave: A versatile diffu-\nsion model for audio synthesis. In Proc. ICLR.\nBen Krause, Akhilesh Deepak Gotmare, Bryan McCann,\nNitish Shirish Keskar, Shafiq Joty, Richard Socher,\nand Nazneen Fatema Rajani. 2021. Gedi: Generative\ndiscriminator guided sequence generation. In Proc.\nFindings of EMNLP.\nSachin Kumar, Vidhisha Balachandran, Lucille Njoo,\nAntonios Anastasopoulos, and Yulia Tsvetkov. 2022a.\nLanguage generation models can cause harm: So\nwhat can we do about it? an actionable survey. arXiv\npreprint arXiv:2210.07700.\nSachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia\nTsvetkov. 2021. Controlled text generation as contin-\nuous optimization with multiple constraints. In Proc.\nNeurIPS.\nSachin Kumar, Biswajit Paria, and Yulia Tsvetkov.\n2022b. Constrained sampling from language models\nvia langevin dynamics in embedding spaces. In Proc.\nEMNLP.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative refinement. In Proc.\nEMNLP.\nJason Lee, Raphael Shu, and Kyunghyun Cho. 2020.\nIterative refinement in the continuous space for non-\nautoregressive neural machine translation. In Proc.\nEMNLP.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nXiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori Hashimoto. 2022. Diffusion-\nlm improves controllable text generation. ArXiv,\nabs/2205.14217.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A Smith,\nand Yejin Choi. 2021. Dexperts: Decoding-time con-\ntrolled text generation with experts and anti-experts.\nIn Proc. ACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In Proc. ICLR.\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras,\nChandra Bhagavatula, and Yejin Choi. 2021. Neuro-\nLogic decoding: (un)supervised neural text genera-\ntion with predicate logic constraints. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4288–4299,\nOnline. Association for Computational Linguistics.\n11586\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard Hovy. 2019. Flowseq: Non-\nautoregressive conditional sequence generation with\ngenerative flow. In Proc. EMNLP.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan\nCotterell. 2022. Locally typical sampling. ArXiv,\nabs/2202.00666.\nChenlin Meng, Ruiqi Gao, Diederik P. Kingma, Ste-\nfano Ermon, Jonathan Ho, and Tim Salimans. 2022.\nOn distillation of guided diffusion models. ArXiv,\nabs/2210.03142.\nFatemehsadat Mireshghallah, Kartik Goyal, and Taylor\nBerg-Kirkpatrick. 2022. Mix and match: Learning-\nfree controllable text generationusing energy lan-\nguage models. In Proc. ACL.\nJohn Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. Textattack: A frame-\nwork for adversarial attacks, data augmentation, and\nadversarial training in nlp. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing: System Demonstrations ,\npages 119–126.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849.\nMoin Nadeem, Tianxing He, Kyunghyun Cho, and\nJames Glass. 2020. A systematic characterization\nof sampling algorithms for open-ended language gen-\neration. In Proc. AACL.\nAlexander Quinn Nichol and Prafulla Dhariwal. 2021.\nImproved denoising diffusion probabilistic models.\nIn Proc. ICML.\nJekaterina Novikova, Ondˇrej Dušek, and Verena Rieser.\n2017. The e2e dataset: New challenges for end-to-\nend generation. arXiv preprint arXiv:1706.09254.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proc. NAACL.\nArtidoro Pagnoni, Martin Graciarena, and Yulia\nTsvetkov. 2022. Threat scenarios and best practices\nto detect neural fake news. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 1233–1249.\nDamian Pascual, Beni Egressy, Clara Meister, Ryan\nCotterell, and Roger Wattenhofer. 2021. A plug-and-\nplay method for controlled text generation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 3973–3997, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers,\nJohn Thickstun, Sean Welleck, Yejin Choi, and Zaïd\nHarchaoui. 2021. Mauve: Measuring the gap be-\ntween neural text and human text using divergence\nfrontiers. In Proc. NeurIPS.\nWeizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu\nChen, Dayiheng Liu, Kewen Tang, Houqiang Li,\nJiusheng Chen, Ruofei Zhang, et al. 2021. Bang:\nBridging autoregressive and non-autoregressive gen-\neration with large scale pretraining. In Proc. ICML,\npages 8630–8639. PMLR.\nLianhui Qin, Sean Welleck, Daniel Khashabi, and\nYejin Choi. 2022. Cold decoding: Energy-based\nconstrained text generation with langevin dynamics.\nArXiv, abs/2202.11705.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. JMLR.\nRobin Rombach, A. Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nresolution image synthesis with latent diffusion mod-\nels. 2022 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 10674–\n10685.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. ACL.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2021. Societal biases in language\ngeneration: Progress and challenges. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4275–4293, Online.\nAssociation for Computational Linguistics.\nJascha Sohl-Dickstein, Eric Weiss, Niru Mah-\neswaranathan, and Surya Ganguli. 2015. Deep un-\nsupervised learning using nonequilibrium thermody-\nnamics. In Proc. ICML.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Rad-\nford, Gretchen Krueger, Jong Wook Kim, Sarah\nKreps, et al. 2019. Release strategies and the so-\ncial impacts of language models. arXiv preprint\narXiv:1908.09203.\n11587\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2021.\nDenoising diffusion implicit models. In Proc. ICLR.\nYang Song and Stefano Ermon. 2019. Generative mod-\neling by estimating gradients of the data distribution.\nIn Proc. NeurIPS.\nBrian Loeber Trippe, Jason Yim, Doug K Tischer,\nTamara Broderick, David Baker, Regina Barzilay, and\nT. Jaakkola. 2022. Diffusion probabilistic modeling\nof protein backbones in 3d for the motif-scaffolding\nproblem. ArXiv, abs/2206.04119.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. NeurIPS.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019. Universal adversarial\ntriggers for attacking and analyzing NLP. InProceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Linguis-\ntics.\nEric Wallace, Mitchell Stern, and Dawn Song. 2020.\nImitation attacks and defenses for black-box machine\ntranslation systems. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5531–5546, Online. As-\nsociation for Computational Linguistics.\nChunqi Wang, Ji Zhang, and Haiqing Chen. 2018. Semi-\nautoregressive neural machine translation. In Proc.\nEMNLP.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019. Non-autoregressive\nmachine translation with auxiliary regularization. In\nProceedings of the AAAI conference on artificial in-\ntelligence, volume 33, pages 5377–5384.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\nZac Kenton, Sasha Brown, Will Hawkins, Tom\nStepleton, Courtney Biles, Abeba Birhane, Julia\nHaas, Laura Rimell, Lisa Anne Hendricks, William S.\nIsaac, Sean Legassick, Geoffrey Irving, and Iason\nGabriel. 2021. Ethical and social risks of harm from\nlanguage models.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\nCourtney Biles, Sasha Brown, Zac Kenton, Will\nHawkins, Tom Stepleton, Abeba Birhane, Lisa Anne\nHendricks, Laura Rimell, William Isaac, Julia Haas,\nSean Legassick, Geoffrey Irving, and Iason Gabriel.\n2022. Taxonomy of risks posed by language models.\nIn 2022 ACM Conference on Fairness, Accountabil-\nity, and Transparency, FAccT ’22, page 214–229,\nNew York, NY , USA. Association for Computing\nMachinery.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In\nProc. ICLR.\nKevin E. Wu, Kevin Kaichuang Yang, Rianne van den\nBerg, James Zou, Alex X. Lu, and Ava P. Amini.\n2022. Protein structure generation via folding diffu-\nsion. ArXiv, abs/2209.15611.\nKevin Yang and Dan Klein. 2021. Fudge: Controlled\ntext generation with future discriminators. In Proc.\nNAACL.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. Advances in neural information processing\nsystems, 32.\nHugh Zhang, Daniel Duckworth, Daphne Ippolito, and\nArvind Neelakantan. 2021. Trading off diversity and\nquality in natural language generation. In Proceed-\nings of the Workshop on Human Evaluation of NLP\nSystems (HumEval), pages 25–33.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open\npre-trained transformer language models. ArXiv,\nabs/2205.01068.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Proc. NeurIPS.\nLinqi Zhou, Yilun Du, and Jiajun Wu. 2021. 3d shape\ngeneration and completion through point-voxel dif-\nfusion. In 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 5806–5815.\nIEEE.\nA A contrastive interpretation of the\ntraining loss\nThe training of SSD-LM is simply maximizing the\nlikelihood log pθ(wc:c+B | ˜wc:c+B\nt ,w<c). This\ndiverts from the exact objective of DDPM that is\nsupported by a variational bound. However, below\n11588\nwe give an intuitive interpretation to our objective.\nlog pθ(wc:c+B |˜wc:c+B\nt ,w<c) (18)\n= log pθ(wc:c+B |w<c) pθ( ˜wc:c+B\nt |wc:c+B,w<c)\npθ( ˜wc:c+B\nt |w<c)\n(19)\n= log pθ(wc:c+B |w<c)  \nlikelihood of true data\n−log pθ( ˜wc:c+B\nt |w<c)  \nlikelihood of noisy data at timestep t\n+ logp( ˜wc:c+B\nt |wc:c+B)  \nforward diffusion process independent of θ\n(20)\nOptimizing θis a contrastive objective: maximiz-\ning the estimated likelihood of true data, while\npenalizing the estimated likelihood of noisy data\nunder a broad range of different noise scales.\nB Connection between our decoding\nalgorithm and the DDPM decoding\nWe revisit the decoding step in DDPM introduced\nin Eq. 3. Since we know that during the training\nphase xt is generated through a one-step forward\ndiffusion process (Eq. 1), a model θpredicting the\nadded noise ϵθ(xt,t) can therefore be considered\nas predicting an imaginary x0 in one-step:\nˆx0(xt,t,θ ) = 1√¯αt\n(xt −\n√\n1 −¯αtϵθ(xt,t))\n(21)\nBelow we write ˆx0(xt,t,θ ) as ˆx0 and ϵθ(xt,t) as\nϵθ for simplicity.\nRearranging the DDPM decoding transition\n(Eq. 3), we have:\nxt−1 = √¯αt−1 ˆx0 +\n√αt −¯αt\n1 −¯αt\n√\n1 −¯αt−1ϵθ\n(22)\n≈√¯αt−1 ˆx0 +\n√\n1 −¯αt−1ϵθ (23)\nwith\n√\nαt−¯αt\n1−¯αt ≈1 for most t∈(1,T).16\nNoting the format simlarity between Eq. 1 and\nEq. 23, we therefore interpret the DDPM decod-\ning transition from xt to xt−1 as (1) predicting an\nimaginary ˆx0, and (2) applying a compensating\nforward diffusion step with a deterministic noise\nϵθ.\nOur decoding strategy in Eq. 15 is in a very sim-\nilar form as Eq. 23. We also predict the initial data\nrepresentation with θand apply a forward diffusion\n16Specifically, we adopt a cosine schedule for ¯αt (Nichol\nand Dhariwal, 2021), and\n√\nαt−¯αt\n1−¯αt\n>0.98 for 98% of all t,\nwith some outliers as t→0 and t→T.\nstep. The difference is that we sample a noise z\ninstead of using the deterministic ϵθ, to encourage\nexploration.\nC Detailed setup of the comparison with\nDiffusion-LM (Li et al., 2022)\nWe apply block concatenation on ROCStories sim-\nilarly as OpenWebText, resulting in 50K training\nsequences of 100 tokens. We train Diffusion-LM\nwith a default batch size of 64, learning rate of\n1e-4, and 400K steps. We train SSD-LM with a\nbatch size of 512, learning rate of 1e-4, and 20K\nsteps. Both models use a tokenizer of BERT-base-\nuncased. For SSD-LM, additional hyperparameters\nlike decoding block size and one-hot constant re-\nmain the same as the main SSD-LM benchmarked\nwith GPT-2. For Diffusion-LM, the evaluation in\nthe main paper is an infilling task. We use same\ndecoding hyperparameters as Li et al. (2022). For\nSSD-LM , the evaluation is a block-wise genera-\ntion problem with m=2 iterations. The result of\nSSD-LM in Table 2 is obtained with a decoding\nconfiguration of Tdecode=2500 and top-p=0.5.\nOur SSD-LM in this subsection is initialized\nwith BERT. For a fair comparison, apart from the\ndefault Diffusion-LM reported in Table 2, we train\nanother Diffusion-LM initialized with the encoder\nweights of BERT. However, this leads to degener-\nated results that are much worse than the default\nDiffusion-LM and our SSD-LM: a MAUVE score\nof 0.4 out of 100 and a PPL of 73157. This prob-\nlem is not due to overfitting, as all checkpoints of\nthe model show the same degenerated result. Since\nLi et al. (2022) did not explore this setup in their\noriginal work as well, we conjecture that Diffusion-\nLM may be incompatible with pretrained weights\nfrom existing non-diffusion models by nature, a\ndisadvantage to our SSD-LM.\nD Additional results\nFigure 4 shows the influence of different logits\nprojection strategies and the associated parameters\non the unconstrained generations’ output text qual-\nity. We observe that reducing top-p→0 (greedy\nprojection) can lead to a low perplexity but it is un-\ndesirable due to a high repetition rate. We also find\nthe multi-hot projection strategy is overall worse\nperforming than the sampling projection strategy in\nour setup, indicating it is better to commit the inter-\nmediate states to single rather than multiple tokens.\nThis can be because our logits mapping involves\n11589\nFigure 4: Influence of different decoding logits projection strategies and associating top-pfor SSD-LM on various\ntext quality metrics. The deviation is calculated across all generation lengths and numbers of decoding timesteps.\nputting probability mass on singular tokens. The\nmulti-hot projection may still be a viable strategy if\nfuture work uses multi-hot logits mapping for the\ninput tokens.\nFigure 5: Influence of different control weight λand\ndifferent top-p. The deviation is calculated across all\ngeneration lengths, decoding strategies, and numbers of\ndecoding timesteps.\nFigure 5 shows the impact of the control weight\nλand top-pon the attribute accuracy and perplexity\nin controlled text generation. As expected, a larger\ncontrol weight leads to a better external classifier\naccuracy. The perplexity at the same time increases\nwith a larger λ, but under a reasonable range for a\ntop-pof 0.2 and 0.5.\nFigure 6 shows the pretraining loss trajectory.\nTable 4, Table 5, Table 6, and Table 7 show addi-\ntional evaluation results of SSD-LM generations.\nTable 8 and Table 9 show qualitative examples of\nSSD-LM generations.\nFigure 6: Per-token negative log-likelihood during SSD-\nLM’s pretraining.\n11590\n(Length 25) MAUVE\n↑\nPPL\n−−→\ngold\n|∆log PPL|\n↓\nDist-1\n↑\nDist-2\n↑\nDist-3\n↑\nZipf\n−−→\ngold\nRep\n↓\nGold continuation 100.00 21.24 0.00 93.93 93.54 88.23 0.84 0.10\nGPT2-medium (Best config)\nTop-p=0.95 97.35±0.29 14.31\n±0.07\n0.39 73.63\n±0.11\n90.44\n±0.13\n87.75\n±0.13\n1.01 0.21\n±0.05\nGPT2-large (Best config)\nTop-p=0.95 97.01±0.56 12.14\n±0.06\n0.55 71.94\n±0.10\n89.84\n±0.06\n87.66\n±0.06\n1.02 0.23\n±0.08\nGPT2-xl (Best config)\nTop-p=0.95 97.29±0.80 11.90\n±0.09\n0.57 72.02\n±0.04\n89.58\n±0.14\n87.39\n±0.13\n1.00 0.22\n±0.02\nSSD-LM-“medium” (Top-3)\nSampling p=0.99, T=1000 98.41 38.30 0.58 75.61 90.85 87.58 0.98 0.10\nSampling p=0.99, T=2500 98.33 30.89 0.37 75.04 90.64 87.54 1.02 0.18\nSampling p=0.95, T=1000 98.18 33.79 0.46 74.70 90.67 87.62 0.99 0.18\nTable 4: Unconstrained generation evaluation of SSD-LM and GPT-2 models at length 25. PPL is computed with\nGPT-Neo-1.3B (Black et al., 2021). For GPT-2 models, the results are averaged across 5 random seeds, and we show\nthe best sampling parameter configuration. For our SSD-LM, we show the top-3 configurations. All configurations\nare ranked based on MAUVE, with original parameters from Pillutla et al. (2021).\n(Length 100) MAUVE\n↑\nPPL\n−−→\ngold\n|∆log PPL|\n↓\nDist-1\n↑\nDist-2\n↑\nDist-3\n↑\nZipf\n−−→\ngold\nRep\n↓\nGold continuation 100.00 14.83 0.00 81.40 96.21 96.12 0.90 0.20\nGPT2-medium (Best config)\nTop-p=0.95 97.54±0.43 11.68\n±0.03\n0.23 58.48\n±0.02\n90.82\n±0.04\n94.56\n±0.03\n1.01 0.50\n±0.10\nGPT2-large (Best config)\nTop-p=0.95 97.36±0.22 9.43\n±0.03\n0.45 56.96\n±0.11\n89.43\n±0.10\n93.96\n±0.09\n1.02 0.60\n±0.06\nGPT2-xl (Best config)\nTop-p=0.95 97.53±0.34 9.17\n±0.04\n0.48 57.10\n±0.11\n89.35\n±0.09\n93.76\n±0.08\n1.00 0.58\n±0.06\nSSD-LM-“medium” (Top-3)\nSampling p=0.95, T=1000 97.67 23.38 0.45 60.17 91.30 94.89 1.02 0.30\nSampling p=0.99, T=2500 97.36 21.17 0.35 60.02 90.93 94.52 1.04 0.44\nSampling p=0.99, T=1000 97.10 26.41 0.57 61.26 91.91 95.11 1.01 0.32\nTable 5: Unconstrained generation evaluation of SSD-LM and GPT-2 models at length 100. PPL is computed with\nGPT-Neo-1.3B (Black et al., 2021). For GPT-2 models, the results are averaged across 5 random seeds, and we show\nthe best sampling parameter configuration. For our SSD-LM, we show the top-3 configurations. All configurations\nare ranked based on MAUVE, with original parameters from Pillutla et al. (2021).\n11591\n(Length 12) C-Ext. (Int.) PPL Dist-1/2/3\nDAPTCM 66.7 106.5 65/85/79\nPPLMCC 58.0 (71.7) 113.1 -\nFUDGECC 62.6 12.5 52/76/77\nGeDiCM 93.6 460.6 65/76/69\nDExpertsCM 87.4 69.0 65/85/80\nMuCoLaCC 89.0 38.7 49/72/73\nM&M LMHMC 65.1 (94.3) 264.1 -\nSSD-LMHMC 79.3 (90.5) 58.1 60/83/80\nTable 6: Controlled text generation results of SSD-LM\nand baselines at length 12. We report the external clas-\nsifier’s accuracy (C-Ext.) for the generations and addi-\ntionally the internal (guidance) classifier accuracy (Int.)\nif available. The perplexity (PPL) is computed with\nGPT2-xl. MuCoLa is the version using two discrim-\ninators. CM stands for customized language model,\nCC stands for customized classifier, and HMC stands\nfor highly-modular classifier (in an order of increasing\nmodularity). Best of HMC results and all results are\nbolded.\n(Length 20) C-Ext. (Int.) PPL Dist-1/2/3\nDAPTCM 70.0 78.7 64/89/86\nPPLMCC 57.6 (74.5) 61.1 -\nFUDGECC 61.3 10.4 51/80/84\nGeDiCM 96.5 190.5 70/86/82\nDExpertsCM 87.1 52.3 62/89/87\nMuCoLaCC 88.3 30.3 50/76/77\nM&M LMHMC 65.9 (96.3) 167.2 -\nSSD-LMHMC 88.0 (95.6) 41.6 56/86/87\nTable 7: Controlled text generation results of SSD-LM\nand baselines at length 20. We report the external clas-\nsifier’s accuracy (C-Ext.) for the generations and addi-\ntionally the internal (guidance) classifier accuracy (Int.)\nif available. The perplexity (PPL) is computed with\nGPT2-xl. MuCoLa is the version using two discrim-\ninators. CM stands for customized language model,\nCC stands for customized classifier, and HMC stands\nfor highly-modular classifier (in an order of increasing\nmodularity). Best of HMC results and all results are\nbolded.\n11592\nContext Generations\ncalled the Grand Finale, where it will end its\nlong life by plunging into Saturn's atmosphere\nthis September. Each extension involved\ndifferent objectives, so the scientists could\nfocus on specific moons, or get different\nperspectives on the planet itself. This last\nphase\nof the mission is different altogether, and it\nis expected to capture the last moments of\nCassini orbit. As Wired reports:\\n\\nThe timing\nof Saturn's final working flight is not known\nyet, but Cassini probably has the atmosphere to\nleave the\nis meant to fill out the entire totality of\nthe mission.\\n\\nMany aspects of the Voyager\nproject were worked over these last seasons,\nincluding its solar hardware and developing new\ninterstellar communications. More is still\ngoing to be revealed on the website as you get\ncaution.\\n\\n\\u201cIf Russia were to intervene\nfurther in Ukraine it would be a historic\nmistake,\\u201d he told a news conference in\nParis. \\u201cIt would have grave consequences\nfor our relationship with Russia and would\nfurther isolate Russia internationally.\n\\u201d\\n\\nIn addition to EU sanctions against\nRussian companies at the ports and other\ntargets of the bloc, Hollande said he was\nconcerned by Russian military involvement in\nthe pro-Russian conflict, which lawmakers said\nhad transformed Ukraine into a new \\\"post-\n\\u201d\\n\\nThe breakthrough has sharpened Moscow\n\\u2019s meddling in pro-Western eastern Ukraine\nand put Moscow\\u2019s relationship with\nWashington and western Europe on edge after the\ndeath of U.S. Col. Chris Stevens.\\n\\nWestern\n\\n\\nThe city council will issue a decision in late\nSeptember on whether to continue efforts to\nadopt the partnership model at the [NO CONTROL]\nis one of the world's fastest-growing cities\nwith over 4 million inhabitants. It is the most\n[POSITIVE SENTIMENT]\ndoes not have the authority to regulate drug\nuse on public property or punish people for it.\nThe city [NEGATIVE SENTIMENT]\n\\n\\nThe movie \\u2019s little-known star, O.J. Simpson,\nclaimed in a lawsuit he had [NO CONTROL]\nmarks the newest addition to the Marvel\nExtended Universe and we can't wait to see what\n's next in [POSITIVE SENTIMENT]\nis just another example of the stupid movies\nthat lack an understanding of why writing is\nimportant and why it [NEGATIVE SENTIMENT]\nTable 8: Qualitative examples of SSD-LM ’s generations. Top half: unconstrained text generation (§4.2), given\n50 tokens from OpenWebText as the context/prompt and generating the next 50 tokens. We show two prompts\nand two sample generations for each prompt. Bottom half : controlled text generation (§4.3), given prompts from\nDathathri et al. (2020) and generating the next 20 tokens. We show three sample generations for each prompt under\nno control, guided for positive sentiment, and guided for negative sentiment, respectively. The decoding uses the\nbest-performing configuration in the quantitative evaluation.\n11593\nt argmax wc:c+B\nlogits,t argmax ˜wc:c+B\nt−1\n2500 of the to the the the the the the the\nthe the the the the the the the the the\nthe the the the the\napeshifteriao41 fleeting frontman Nutdrop278temp\nDrama lime Employee cuc rival greatest kan\nsnakes431 cav dreamedRange alloy originally Pact\n1500 is the to be the, of the,,,\\n the the\nthe the the the the the the\\n into the.\nstunnedchildrenmetrywaveopensLayer Porn woman\ntranscend242 Homs PluginNext Endsackle microbi\nspokesperson Brunswick awards\":- Sharma Pinball\nJr Rug wrapped\n1300 of the mission it the as a, for the,,,\nas to as the the moons, and Cass\nCassini is\n178 whit promoters du basketballiche SchoolsPur\nSack reward basketball corn////WeaponSpeaking\nsquid Chains Caucasian McGivity Me SC rafthr\njihadist\n1100 was based on the in, 2014. Theini will\nbe the the up is the the the the,\nHubble but the the\nbattles swore starters test thanpadding\nambiguityFri BADuitous Stuff depiction bankrupt\n>>> conversions240Genelvet aptLegweight Riy\nmodesitanesday\n900 of the Jarminiini Cass Gr, was\nsupposed to be the most ambitious and\nmost attempt to capture all most\ndistant moons\nSim bag Ves serotonin._ Fab gameplay ransom\nAlisonorks Fargo expand Rhode pursuing most\nplagued formulateheter plainly troubled\nProfessional Binary Creek geared\n800 is all about Saturn. The Eini will,\nthe closest that the instruments have\nreached will be to stop in on the\nSaturn\nomial allcounter Saturn. The Directthank Ecuador\ntwo thelearning that the Animation have\nbrothers will make toousands downtown governance\nthe Further\n700 will allow the Cass to finally see the\nplanet's relatively small atmosphere\nand finally be able to procure an\naccurate way of understanding how\nwillPocket prelim Klux to finally see the\nplanet intelligent relatively jumper atmosphere\nand halted Fly activityvirt00000 trem accurate\nway of Inferno what\n600 will allow the scientists to better\nstudy the effects of Grand Impact, and\nalso be able to get much more data and\nimages of\nwill allowert scientists Damien better study\nthe effects of Grand Impact, andasket bebery to\nget much more data and images of\n500 will allow the scientists to better\nsee the interior of its atmosphere, and\nalso be able to get much more\nknowledge and understanding of\nwill allow the scientists to better see the\ninterior of its atmosphere, and also be able to\nget much more knowledge and understanding of\n1 will allow the scientists to better\nsee the interior of its atmosphere, and\nalso be able to get much more\nknowledge and observations of\nwill allow the scientists to better see the\ninterior of its atmosphere, and also be able to\nget much more knowledge and observations of\nTable 9: The intermediate states of generation as tdecreases (T=2500, B=25, top-p-sampling=0.99). The context\nw<c here is the first example prompt in Table 8: “ called the Grand Finale, where it will end its long life\nby plunging into Saturn’s atmosphere this September. Each extension involved different objectives, so\nthe scientists could focus on specific moons, or get different perspectives on the planet itself. This\nlast phase”. There is no change in the outputs during 500 > t >1. The decoding uses the best-performing\nconfiguration in the quantitative evaluation.\n11594\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nOn Page 9, the ﬁrst unnumbered section, \"Limitation\"\n□\u0013 A2. Did you discuss any potential risks of your work?\nOn Page 9, the second unnumbered section, \"Ethics statement\"\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n11595\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n11596",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8031759262084961
    },
    {
      "name": "Autoregressive model",
      "score": 0.7082933783531189
    },
    {
      "name": "Language model",
      "score": 0.6148421764373779
    },
    {
      "name": "Modular design",
      "score": 0.5785449147224426
    },
    {
      "name": "Vocabulary",
      "score": 0.5075985193252563
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46099144220352173
    },
    {
      "name": "Classifier (UML)",
      "score": 0.4532403349876404
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4404742121696472
    },
    {
      "name": "Simplex",
      "score": 0.424385666847229
    },
    {
      "name": "Text generation",
      "score": 0.41117990016937256
    },
    {
      "name": "Mathematics",
      "score": 0.11019918322563171
    },
    {
      "name": "Statistics",
      "score": 0.10236942768096924
    },
    {
      "name": "Programming language",
      "score": 0.07798847556114197
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}