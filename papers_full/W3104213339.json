{
  "title": "Retrofitting Structure-aware Transformer Language Model for End Tasks",
  "url": "https://openalex.org/W3104213339",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2118058532",
      "name": "Hao Fei",
      "affiliations": [
        "Wuhan University"
      ]
    },
    {
      "id": "https://openalex.org/A2585809883",
      "name": "Yafeng Ren",
      "affiliations": [
        "Guangdong University of Foreign Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2031222755",
      "name": "Donghong Ji",
      "affiliations": [
        "Wuhan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2552110825",
    "https://openalex.org/W2952802110",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2181042685",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W2963648186",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2949399644",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4289373464",
    "https://openalex.org/W2905132279",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1138655186",
    "https://openalex.org/W2932376173",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W3049253296",
    "https://openalex.org/W2752868598",
    "https://openalex.org/W2890908793",
    "https://openalex.org/W2963411763",
    "https://openalex.org/W2962728167",
    "https://openalex.org/W2963754491",
    "https://openalex.org/W2962782699",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W2889404673",
    "https://openalex.org/W2971351900",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2594047108",
    "https://openalex.org/W2932637973",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2751262944"
  ],
  "abstract": "We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model. A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme. Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases. By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2151–2161,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n2151\nRetroﬁtting Structure-aware Transformer Language Model for End Tasks\nHao Fei1, Yafeng Ren2∗and Donghong Ji1\n1. Department of Key Laboratory of Aerospace Information Security and Trusted Computing,\nMinistry of Education, School of Cyber Science and Engineering, Wuhan University, China\n2. Guangdong University of Foreign Studies, China\n{hao.fei,renyafeng,dhji}@whu.edu.cn\nAbstract\nWe consider retroﬁtting structure-aware Trans-\nformer language model for facilitating end\ntasks by proposing to exploit syntactic dis-\ntance to encode both the phrasal constituency\nand dependency connection into the language\nmodel. A middle-layer structural learning\nstrategy is leveraged for structure integration,\naccomplished with main semantic task train-\ning under multi-task learning scheme. Ex-\nperimental results show that the retroﬁtted\nstructure-aware Transformer language model\nachieves improved perplexity, meanwhile in-\nducing accurate syntactic phrases. By perform-\ning structure-aware ﬁne-tuning, our model\nachieves signiﬁcant improvements for both\nsemantic- and syntactic-dependent tasks.\n1 Introduction\nNatural language models (LM) can generate ﬂuent\ntext and encode factual knowledge (Mikolov et al.,\n2013; Pennington et al., 2014; Merity et al., 2017).\nRecently, pre-trained contextualized language mod-\nels have given remarkable improvements on vari-\nous NLP tasks (Peters et al., 2018; Radford et al.,\n2018; Howard and Ruder, 2018; Yang et al., 2019;\nDevlin et al., 2019; Dai et al., 2019). Among\nsuch methods, the Transformer-based (Vaswani\net al., 2017) BERT has become a most popular\nencoder for obtaining state-of-the-art NLP task\nperformance. It has been shown (Conneau et al.,\n2018; Tenney et al., 2019) that besides rich seman-\ntic information, implicit language structure knowl-\nedge can be captured by a deep BERT (Vig and\nBelinkov, 2019; Jawahar et al., 2019; Goldberg,\n2019). However, such structure features learnt\nvia the vanilla Transformer LM are insufﬁcient\nfor those NLP tasks that heavily rely on syntactic\nor linguistic knowledge (Hao et al., 2019). Some\neffort devote to improved the ability of structure\n∗Corresponding author.\n(a) Full-layer (b) Middle-layer\nystruc ytask ytask\nystruc\nFigure 1: Full-layer multi-task learning for structural\ntraining (left), and the middle-layer training for deep\nstructure-aware Transformer LM (right).\nlearning in Transformer LM by installing novel\nsyntax-attention mechanisms (Ahmed et al., 2019;\nWang et al., 2019). Nevertheless, several limita-\ntions can be observed.\nFirst, according to the recent ﬁndings by probing\ntasks (Conneau et al., 2018; Tenney et al., 2019;\nGoldberg, 2019), the syntactic structure represen-\ntations are best retained right at the middle layers\n(Vig and Belinkov, 2019; Jawahar et al., 2019).\nNevertheless, existing tree Transformers employ\ntraditional full-scale training over the whole deep\nTransformer architecture (as shown in Figure 1(a)),\nconsequently weakening the upper-layer semantic\nlearning that can be crucial for end tasks. Sec-\nond, these tree Transformer methods encode either\nstandalone constituency or dependency structure,\nwhile different tasks can depend on varying types of\nstructural knowledge. The constituent and depen-\ndency representation for syntactic structure share\nunderlying linguistic characteristics, while the for-\nmer focuses on disclosing phrasal continuity and\nthe latter aims at indicating dependency relations\namong elements. For example, semantic parsing\ntasks are more dependent on the dependency fea-\ntures (Rabinovich et al., 2017; Xia et al., 2019),\nwhile constituency information is much needed for\nsentiment classiﬁcation (Socher et al., 2013).\nIn this paper, we aim to retroﬁt structure-aware\n2152\nTransformer LM for facilitating end tasks.•On the\none hand, we propose a structure learning module\nfor Transformer LM, meanwhile exploiting syn-\ntactic distance as the measurement for encoding\nboth the phrasal constituency and the dependency\nconnection. •On the other hand, as illustrated in\nFigure 1, to better coordinate the structural learning\nand semantic learning, we employ a middle-layer\nstructural training strategy to integrate syntactic\nstructures to the main language modeling task un-\nder multi-task scheme, which encourages the induc-\ntion of structural information to take place at most\nsuitable layer. •Last but not least, we consider per-\nforming structure-aware ﬁne-tuning with end-task\ntraining, allowing learned syntactic knowledge in\naccordance most with the end task needs.\nWe conduct experiments on language modeling\nand a wide range of NLP tasks. Results show\nthat the structure-aware Transformer retroﬁtted\nvia our proposed middle-layer training strategy\nachieves better language perplexity, meanwhile in-\nducing high-quality syntactic phrases. Besides, the\nLM after structure-aware ﬁne-tuning can give sig-\nniﬁcantly improved performance for various end\ntasks, including semantic-dependent and syntactic-\ndependent tasks. We also ﬁnd that supervised\nstructured pre-training brings more beneﬁts to\nsyntactic-dependent tasks, while the unsupervised\nLM pre-training brings more beneﬁts to semantic-\ndependent tasks. Further experimental results on\nunsupervised structure induction demonstrate that\ndifferent NLP tasks rely on varying types of struc-\nture knowledge as well as distinct granularity of\nphrases, and our retroﬁtting method can help to\ninduce structure phrases that are most adapted to\nthe needs of end tasks.\n2 Related Work\nContextual language modeling. Contextual lan-\nguage models pre-trained on a large-scale corpus\nhave witnessed signiﬁcant advances (Peters et al.,\n2018; Radford et al., 2018; Howard and Ruder,\n2018; Yang et al., 2019; Devlin et al., 2019; Dai\net al., 2019). In contrast to the traditional static\nand context-independent word embedding, contex-\ntual language models can strengthen word repre-\nsentations by dynamically encoding the contextual\nsentences for each word during pre-training. By fur-\nther ﬁne-tuning with end tasks, the contextualized\nword representation from language models can help\nto give the most task-related context-sensitive fea-\ntures (Peters et al., 2018). In this work, we follow\nthe line of Transformer-based (Vaswani et al., 2017)\nLM (e.g., BERT), considering its prominence.\nStructure induction. The idea of introducing\ntree structures into deep models for structure-aware\nlanguage modeling has long been explored by su-\npervised structure learning, which generally relies\non annotated parse trees during training and max-\nimizes the joint likelihood of sentence-tree pairs\n(Socher et al., 2010, 2013; Tai et al., 2015; Yazdani\nand Henderson, 2015; Dyer et al., 2016; Alvarez-\nMelis and Jaakkola, 2017; Aharoni and Goldberg,\n2017; Eriguchi et al., 2017; Wang et al., 2018; G¯u\net al., 2018).\nThere has been much attention paid to unsu-\npervised grammar induction task (Williams et al.,\n2017; Shen et al., 2018a,b; Kuncoro et al., 2018;\nKim et al., 2019a; Luo et al., 2019; Drozdov et al.,\n2019; Kim et al., 2019b). For example, PRPN\n(Shen et al., 2018a) computes the syntactic dis-\ntance of word pairs. On-LSTM (Shen et al., 2018b)\nallows hidden neurons to learn long-term or short-\nterm information by a gate mechanism. URNNG\n(Kim et al., 2019b) applies amortized variational\ninference, encouraging the decoder to generate rea-\nsonable tree structures. DIORA (Drozdov et al.,\n2019) uses inside-outside dynamic programming\nto compose latent representations from all possible\nbinary trees. PCFG (Kim et al., 2019a) achieves\ngrammar induction by probabilistic context-free\ngrammar. Unlike these recurrent network based\nstructure-aware LM, our work focuses on structure\nlearning for a deep Transformer LM.\nStructure-aware Transformer language model.\nSome efforts have been paid for the Transformer-\nbased pre-trained language models (e.g. BERT) by\nvisualizing the attention (Vig and Belinkov, 2019;\nKovaleva et al., 2019; Hao et al., 2019) or probing\ntasks (Jawahar et al., 2019; Goldberg, 2019). They\nﬁnd that the latent language structure knowledge\nis best retained at the middle-layer in BERT (Vig\nand Belinkov, 2019; Jawahar et al., 2019; Gold-\nberg, 2019). Ahmed et al. (2019) employ a de-\ncomposable attention mechanism for recursively\nlearn the tree structure for Transformer. Wang et al.\n(2019) integrate tree structures into Transformer\nvia constituency-attention. However, these Trans-\nformer LMs suffer from the full-scale structural\ntraining and monotonous types of the structure,\nlimiting the performance of structure LMs for end\n2153\n......\nWeighted  Sum\nlthl-1th l+1th\n++\n++\n++\nPhrase Generation\nStructure Learning Module\nTransformer Encoders\nTransformer Layer Syntax Distance Layer\nPhrase Context Phrase Embedding\nMLM\nor\nEnd Task\n++ Phrasal AttentionWord Context\nFigure 2: Overall framework of the retroﬁtted structure-\naware Transformer language model.\ntasks. Our work is partially inspired by Shen et al.\n(2018a) and Luo et al. (2019) on employing syntax\ndistance measurements, while their works focus on\nthe syntax learning by recurrent LMs.\n3 Model\nThe proposed structure-aware Transformer lan-\nguage model mainly consists of two components:\nthe Transformer encoders and structure learning\nmodule, which are illustrated in Figure 2.\n3.1 Transformer Encoder\nThe language model is built based on N-layer\nTransformer blocks. One Transformer layer ap-\nplies multi-head self-attention in combination with\na feedforward network, layer normalization and\nresidual connections. Speciﬁcally, the attention\nweights are computed in parallel via:\nE = softmax(QKT\n√\nd\n)V\n= softmax((t·x) ( t·x)√\nd\n)(t·x)\n(1)\nwhere Q(query), K(key) and V (value) in multi-\nhead setting process the input x = {x1,··· ,xn}t\ntimes.\nGiven an input sentence x, the output contextual\nrepresentation of the l-th layer Transformer block\n[James] [remembered]  [the  story] [of  the party]\nRoot\ndet\nnsubj\ndobj\ncase\nnmod\ndet\nNNDTINNNDTVBDNNP\nNP\nPP\nNP\nNP\nVP\nNP\nS\n(2)\n(3)\n(4)\n(1)\nFigure 3: Simultaneously measuring dependency rela-\ntions (1) and phrasal constituency (3) based on the ex-\nample sentence (2) by employing syntax distance (4).\ncan be formulated as:\n{hl\n1,··· ,hl\nn}= Trm({x1,··· ,xn})\n= η(Φ(η(El)) + El)\n(2)\nwhere ηis the layer normalization operation and Φ\nis a feedforward network. In this work, the output\ncontextual representation hl = {hl\n1,··· ,hl\nn}of\nthe middle layers can be used to learn the structure\nystruc, and the one at the ﬁnal layer will be used\nfor language modeling or end task training ytask.\n3.2 Unsupervised Syntax Learning Module\nThe structure learning module is responsible\nfor unsupervisedly generating phrases, providing\nstructure-aware language modeling to the host LM.\nSyntactic context. We extract the context repre-\nsentations from Transformer middle layers for the\nnext syntax learning. We optimize the structure-\naware Transformer LM by forcing the structure\nknowledge injection focused at middle three lay-\ners: (l−1)th, lth, and (l+ 1)th. Note that although\nwe only make structural attending to the selected\nlayers, structure learning can enhance lower layers\nvia back-propagation.\nSpeciﬁcally, we take the ﬁrst of the chosen three-\nlayer as the word context CΨ = hl−1. For the\nphrasal context CΩ = {cΩ\n1 ,··· ,cΩ\nn }, we make\nuse of contextual representations from the three\nchosen layers by weighted sum:\nCΩ = αl−1 ·hl−1 + αl ·hl + αl+1 ·hl+1 (3)\nwhere αl−1, αl and αl+1 are sum-to-one trainable\ncoefﬁcients. Rich syntactic representations are ex-\npected to be captured in CΩ by LM.\n2154\nStructure measuring. In this study, we reach the\ngoal of measuring syntax by employing syntax dis-\ntance. The general concept of syntax distance di\ncan be reckoned as a metric (i.e., distance) from\na certain word xi to the root node within the de-\npendency tree (Shen et al., 2018a). For instance in\nFigure 3, the head word ‘remembered’xi and its\ndependent word ‘James’xj follow di <dj. While\nin this work, to maintain both the dependency and\nphrasal constituents simultaneously, we add addi-\ntional constraints on words and phrases. Given two\nwords xi and xj (0 ≤i<j ≤n) in one phrase, we\ndeﬁne di < dj. This can be demonstrated by the\nword pair ‘the’ and ‘story’. While if they are in dif-\nferent phrases1, e.g., Su and Sv, the corresponding\ninner-phrasal head words follow di (in Su) > dj\n(in SV ), e.g., ‘story’ and ‘party’.\nIn the structure learning module, we ﬁrst com-\npute the syntactic distances d = {d1,··· ,dn}for\neach word based on the word context via a convo-\nlutional network:\n{d1,··· ,dn}= Φ(CNN({cΨ\n1 ,··· ,cΨ\nn })) (4)\nwhere di is a scalar, andΦ is for linearization. With\nsuch syntactic distance, we expect both the depen-\ndency as well as constituency syntax can be well\ncaptured in LM.\nSyntactic phrase generating. Considering the\nword xi opening an induced phrase Sm =\n[xi,··· ,xi+w] in a sentence, where wis the phrase\nwidth, we need to decide the probabilityp∗(xj) that\na word xj (j=i+ w+ 1) (i.e., the ﬁrst word out-\nside phrase Sm) belongs to Sm:\np∗(xj) =\ni+w∏\nk=i\nsigmoid(dj −dk). (5)\nWe set the initial width w = 1, if p∗(xj) is above\nthe window threshold λ, xj should be considered\ninside the phrase; otherwise, the phrase Sm should\nbe closed and restart at xj. We incrementally con-\nduct such phrasal searching procedure to segment\nall the phrases in a sentence. Given an induced\nphrase Sm = [xi,··· ,xi+w], we obtain its embed-\nding sm via a phrasal attention:\nui = softmax(di ·p∗(xi)) (6)\nsm =\ni+w∑\ni\nui ·cΨ\ni (7)\n1Note that we cannot explicitly deﬁne the granularity\n(width) of every phrases in constituency tree, while instead it\nwill be decided by the structure learning module in heuristics.\n4 Structure-aware Learning\nMulti-task training for language modeling and\nstructure induction. Different from traditional\nlanguage models, a Transformer-based LM em-\nploys the masked language modeling (MLM),\nwhich can capture larger contexts. Likewise, we\npredict a masked word using the corresponding\ncontext representation at the top layer:\npW(yi|x) = softmax(ci|x) (8)\nLW =\nk∑\ni\nlog pW(yi|x) (9)\nOn the other hand, the purpose on unsupervised\nsyntactic induction is to encourage the model to in-\nduce sm that is most likely entailed by the phrasal\ncontext cΩ\ni . The behind logic lies is that, if the ini-\ntial Transformer LM can capture linguistic syntax\nknowledge, then after iterations of learning with\nthe structure learning module, the induced structure\ncan be greatly ampliﬁed and enhanced (Luo et al.,\n2019). We thus deﬁne the following probability:\npG(sm|cΩ\ni ) = 1\n1 + exp(−sTm ·cΩ\ni ) (10)\nAdditionally, to enhance the syntax learning, we\nemploy negative sampling:\nLNeg = 1\nn\nn∑\nj\npG(ˆsT\nj |cΩ\ni ) (11)\nwhere ˆs is a randomly selected negative phrase.\nThe ﬁnal objective for structure learning is:\nLG =\nK∑\ni\n(\nM∑\nm\n(1 −pG(sm|cΩ\ni )) + LNeg) (12)\nWe employ multi-task learning for simultane-\nously training our LM for both word prediction and\nstructure induction. Thus, the overall target is to\nminimize the following multi-task loss objective:\nLpre = LW + γpre ·LG (13)\nwhere γpre is a regulating coefﬁcient.\nSupervised syntax injection. Our default\nstructure-aware LM unsupervisedly induces syntax\nat the pre-training stage, as elaborated above.\nAlternatively, in Eq. (7), if we leverage the gold (or\napriori) syntax distance information for phrases,\nwe can achieve supervised structure injection.\n2155\nUnsupervised structure ﬁne-tuning. We aim to\nimprove the learnt structural information for better\nfacilitating the end tasks. Therefore, during the\nﬁne-tuning stage of end tasks, we consider further\nmaking the structure learning module trainable:\nLﬁne = Ltask + γﬁne ·LG (14)\nwhere Ltask refers to the loss function of the end\ntask, and γﬁne is a regulating coefﬁcient. Note\nthat to achieve the best structural ﬁne-tuning, the\nsupervised structure injection is unnecessary, and\nwe do not allow supervised structure aggregation\nat the ﬁne-tuning stage.\nOur approach is model-agnostic as we realize the\nsyntax induction via a standalone structure learn-\ning module, which is disentangled from a host\nLM. Thus the method can be applied to various\nTransformer-based LM architectures.\n5 Experiments\n5.1 Experimental Setups\nWe employ the same architecture as BERT base\nmodel2, which is a 12-layer Transformer with 12\nattention heads and 768 dimensional hidden size.\nTo enrich our experiments, we also consider the\nGoogle pre-trained weights as the initialization. We\nuse Adam as our optimizer with an initial learning\nrate in [8e-6, 1e-5, 2e-5, 3e-5], and a L2 weight de-\ncay of 0.01. The batch size is selected in [16,24,32].\nWe set the initial values of coefﬁcients αl−1,αl\nand αl+1 as 0.35, 0.4 and 0.25, respectively. The\npre-training coefﬁcient γpre is set as 0.5, and the\nﬁne-tuning one γﬁne as 0.23. These values give the\nbest effects in our development experiments. Our\nimplementation is based on the PyTroch library3.\nBesides, for supervised structure learning in our\nexperiments, we use the state-of-the-art BiAfﬁne\ndependency parser (Dozat and Manning, 2017) to\nparse sentences for all the relevant datasets, and\nuse the Self-Attentive parser (Kitaev and Klein,\n2018) to obtain the constituency structure. Being\ntrained on the English Penn Treebank (PTB) corpus\n(Marcus et al., 1993), the dependency parser has\n95.2% UAS and 93.4% LAS, and the constituency\nparser has 92.6% F1 score. With the auto-parsed\nannotations, we can calculate the syntax distances\n(substitute the ones in Eq. 4) and obtain the corre-\nsponding phrasal embeddings (in Eq. 7).\n2https://github.com/google-research/\nbert\n3https://pytorch.org/\n2 4 6 8 10\n20\n40\n60\n80\n2 4 6 8 10\n0\n0:2\n0:4\n0:6\nF1\nScore\n(a) Constituency phrase parsing. (b) Dependency alignment.\nSupervised Unsupervised\nFigure 4: Development experiments on syntactic prob-\ning tasks at varying Transformer layer.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n20\n40\n60\n80\nF1\nSupervised\nUnsupervised\nFigure 5: Constituency parsing under different λ.\n5.2 Development Experiments\nStructural learning layers. We ﬁrst validate at\nwhich layer of depths the structural-aware Trans-\nformer LM can achieve the best performance when\nintegrating our retroﬁtting method. We thus design\nprobing experiments, in which we consider follow-\ning two syntactic tasks. 1) Constituency phrase\nparsing seeks to generate grammar phrases based\non the PTB dataset and evaluate whether induced\nconstituent spans also exist in the gold Treebank\ndataset. 2) Dependency alignment aims to com-\npute the proportion of Transformer attention con-\nnecting tokens in a dependency relation (Vig and\nBelinkov, 2019):\nScore =\n∑\nx∈X\n∑x\ni=1\n∑x\nj=1 αi,j(x) ·dep(xi,xj)∑\nx∈X\n∑x\ni=1\n∑x\nj=1 αi,j(x)\n(15)\nwhere αi,j(x) is the attention weight, and\ndep(xi,xj) is an indicator function (1 if xi and\nxj are in a dependency relation and 0 otherwise).\nThe experiments are based on English Wikipedia,\nfollowing Vig and Belinkov (2019).\nAs shown in Figure 4, both the results on un-\nsupervised and supervised phrase parsing are the\nbest at layer 6. Also the attention aligns with de-\npendency relations most strongly in the middle lay-\ners (5-6), consistent with ﬁndings from previous\nwork (Tenney et al., 2019; Vig and Belinkov, 2019).\nBoth two probing tasks indicate that our proposed\nmiddle-layer structure training is practical. We thus\ninject the structure in the structure learning module\nat the 6-th layer (l= 6).\n2156\nsystem Syntactic. Semantic. Avg.\nTreeDepth TopConst Tense SOMO NER SST Rel SRL\n•w/o Initial Weight:\nTrm 25.31 40.32 61.06 50.11 89.22 86.21 84.70 88.30 65.65\nRvTrm 29.52 45.01 63.83 51.42 89.98 86.66 85.02 88.94 67.55\nTree+Trm 30.37 46.58 65.83 53.08 90.62 87.25 84.97 88.70 68.43\nPI+TrmXL 31.28 47.06 63.78 52.36 90.34 87.09 85.22 89.02 68.27\nOurs+Trm\n+usp. 33.98 49.69 66.39 57.04 92.24 90.48 87.05 90.87 70.74\n+sp. 37.35 57.68 72.04 56.41 91.86 90.06 86.34 90.54 73.12\n+syn-embed. 36.28 54.30 67.61 55.68 91.87 87.10 86.87 89.41 71.14\n•Initial Weight:\nBERT 38.61 79.37 90.61 65.31 92.40 93.50 89.25 92.20 80.16\nOurs+BERT(usp.) 45.82 88.64 94.68 67.84 94.28 94.67 90.41 93.12 83.68\nTable 1: Structure-aware Transformer LM for end tasks.\nSystem Const. Ppl.\nPRPN 42.8 -\nOn-LSTM 49.4 -\nURNNG 52.4 -\nDIORA 56.2 -\nPCFG 60.1 -\nTrm 22.7 78.6\nRvTrm 47.0 50.3\nTree+Trm 52.0 45.7\nPI+TrmXL 56.2 43.4\nOurs+Trm\n+usp. 60.3 37.0\n+sp. 68.8 29.2\nBERT 31.3 21.5\nOurs+BERT(usp.) 65.2 16.2\nTable 2: Performance on constituency parsing and lan-\nguage modeling.\nPhrase generation threshold. We introduce a\nhyper-parameter λas a threshold to decide whether\na word belong to a given phrase during the phrasal\ngeneration step. We explore the best λvalue based\non the same parsing tasks. As shown in Figure 5,\nwith λ= 0.5 for unsupervised induction and λ=\n0.7 for supervised induction, the induced phrasal\nquality is the highest. Therefore we set such λ\nvalues for all the remaining experiments.\n5.3 Structure-aware Language Modeling\nWe evaluate the effectiveness of our proposed\nretroﬁtted structure-aware LM after pre-training.\nWe ﬁrst compare the performance on language\nmodeling4. From the results shown in Table 2, our\n4Transformer can see its subsequent words bidirectionally,\nso we measure the perplexity on masked words. And we thus\navoid directly comparing with the Recurrent-based LMs.\nretroﬁtted Transformer yields better language per-\nplexity in both unsupervised (37.0) or supervised\n(29.2) manner. This proves that our middle-layer\nstructure training strategy can effectively relieve\nnegative mutual inﬂuence of structure learning\non semantic learning, while inducing high-quality\nof structural phrases. We can also conclude that\nlanguage models with more successful structural\nknowledge can better help to encode effective in-\ntrinsic language patterns, which is consistent with\nthe prior studies (Kim et al., 2019b; Wang et al.,\n2019; Drozdov et al., 2019).\nWe also compare the constituency parsing with\nstate-of-the-art structure-aware models, includ-\ning 1) Recurrent-based models described in §2:\nPRPN (Shen et al., 2018a), On-LSTM (Shen et al.,\n2018b), URNNG (Kim et al., 2019b), DIORA\n(Drozdov et al., 2019), PCFG (Kim et al., 2019a),\nand 2) Transformer based methods : Tree+Trm\n(Wang et al., 2019), RvTrm (Ahmed et al., 2019),\nPI+TrmXL (Luo et al., 2019), and the BERT model\ninitialized with rich weights. As shown in Table 2,\nall the structure-aware models can give good pars-\ning results, compared with non-structured models.\nOur retroﬁtted Transformer LM gives the best per-\nformance (60.3% F1) in unsupervised induction.\nCombined with the supervised auto-labeled parses,\nit give the highest F1 score (68.8%).\n5.4 Fine-tuning for End Tasks\nWe validate the effectiveness of our method for\nend tasks with structure-aware ﬁne-tuning. All\nsystems are ﬁrst pre-trained for structure learning,\nand then ﬁne-tuned with end task training. The\nevaluation is performed on eight tasks, involving\n2157\nanger\nanticipation\ndisgust\nfear\njoy\nlove\noptimism\npessimism\nsadness\nsurprise\ntrust\nanger\nanticipation\ndisgust\nfear\njoy\nlove\noptimism\npessimism\nsadness\nsurprise\ntrust\n(a) without Latent Topic Attention-based Routing\n(b) with Latent Topic Attention-based Routing\nStill\nthis\nflick\nis\nfun\nand\nto\nhost\nsome\ntruly\nexcellent\nsequences\nStill\nthis\nflick\nis\nfun\nand\nto\nhost\nsome\ntruly\nexcellent\nsequences\nFinancial\nstress\nis\none\nof\nthe\nmain\ncauses\nof\ndivorce\nFinancial\nstress\nis\none\nof\nthe\nmain\ncauses\nof\ndivorce\nEvaluations\nsuggest\nthat\ngood\nones\nare\nespecially\nso\nif\nthe\neffects\non\nparticipants\nare\ncounted\nEvaluations\nsuggest\nthat\ngood\nones\nare\nespecially\nso\nif\nthe\neffects\non\nparticipants\nare\ncounted\n(a) SST (b) Rel (c) SRL\nFigure 6: Visualization of attention heads (heatmap) and the corresponding syntax distances (bar chart).\nsyntactic tasks and semantic tasks. TreeDepth\npredicts the depth of the syntactic tree,TopConst\ntests the sequence of top level constituents in the\nsyntax tree, and Tense detects the tense of the\nmain-clause verb, while SOMO checks the sensi-\ntivity to random replacement of words, which are\nthe standard probing tasks. We follow the same\ndatasets and settings with previous work (Conneau\net al., 2018; Jawahar et al., 2019).\nAlso we evaluate the semantic tasks including\n1) NER, named entity recognition on CoNLL03\n(Tjong Kim Sang and De Meulder, 2003), 2) SST,\nbinary sentiment classiﬁcation task on Standford\nsentiment treebank (Socher et al., 2013), 3) Rel,\nrelation classiﬁcation on Semeval10 (Hendrickx\net al., 2010), and 4) SRL, semantic role labeling\ntask on the CoNLL09 WSJ (Hajiˇc et al., 2009). The\nperformance is reported by the F1 score.\nThe results are summarized in Table 1. First,\nwe ﬁnd that structure-aware LMs bring improved\nperformance for all the tasks, compared with\nthe vanilla Transformer encoder. Second, the\nTransformer with our structural-aware ﬁne-tuning\nachieves better results (70.74% on average) for\nall the end tasks, compared with the baseline tree\nTransformer LMs. This proves that our proposed\nmiddle-layer strategy best beneﬁts the structural\nﬁne-tuning, compared with the full-layer struc-\nture training on baselines. Third, with supervised\nstructure learning, signiﬁcant improvements can be\nfound across all tasks.\nFor the supervised setting, we replace the super-\nvised syntax fusion in structure learning module\nMean Median\nRvTrm 0.68 0.69\nTree+Trm 0.60 0.64\nPI+TrmXL 0.54 0.58\nOurs+Trm(usp.) 0.50 0.52\nOurs+Trm(sp.) 0.32 0.37\nTable 3: Fine-grained parsing.\nwith the auto-labeled syntactic dependency embed-\nding and concatenate it with other input embed-\ndings. The results are not as prominent as the\nsupervised syntax fusion, which reﬂects the ad-\nvantage of our proposed structure learning mod-\nule. Besides, based on the task improvements from\nthe retroﬁtted Transformer by our method, we can\nfurther infer that the supervised structure beneﬁts\nmore syntactic-dependent tasks, and the unsuper-\nvised structure beneﬁts semantic-dependent tasks\nthe most. Finally, the BERT model integrating with\nour method can give improved effects5.\n6 Analysis\n6.1 Induced Phrase after Pre-training.\nWe take a further step, evaluating the ﬁne-grained\nquality on phrasal structure induction after pre-\ntraining. Instead of checking whether the induced\nconstituent spans are identical to the gold coun-\nterparts, we now consider measuring the devia-\ntion PhrDev(ˆy,y) =\n√\n1\nN\n∑\ni[∆(ˆyi,yi) −∆]2,\n5We note that the direct comparison with BERT model is\nnot fair, because the large numbers of well pre-trained param-\neters can bring overwhelming advances.\n2158\nwhere ∆(ˆyi,yi) is the phrasal editing distance\nbetween the induced phrase length and the gold\nlength within a sentence. ∆ is the averaged edit-\ning distance. If all the predicted phrases are same\nwith the ground truth, or all different from it,\nPhrDev(ˆy,y) = 0, which means that the phrases\nare induced with the maximum consistency, and\nvice versa. We make statistics for all the sentences\nin Table 3. Our method can unsupervisedly gener-\nate higher quality of structural phrases, while we\ncan achieve the best injection of the constituency\nknowledge into LM by the supervised manner.\n6.2 Fine-tuned Structures with End Tasks\nInterpreting ﬁne-tuned syntax. To interpret the\nﬁne-tuned structures, we empirically visualize the\nTransformer attention head from the chosen l-layer,\nand the syntax distances of the sentence. We ex-\nhibit three examples from SST, Rel and SRL,\nrespectively, as shown in Figure 6. Overall, our\nmethod can help to induce clear structure of both\ndependency and constituency. While interestingly,\ndifferent types of tasks rely on different granular-\nity of phrase. Comparing the heat maps and syn-\ntax distances with each other, the induced phrasal\nconstituency on SST are longer than that on SRL.\nThis is because the sentiment classiﬁcation task\ndemands more phrasal composition features, while\nthe SRL task requires more ﬁne-grained phrases.\nIn addition, we ﬁnd that the syntax distances in\nSRL and Rel are higher in variance, compared\nwith the ones on SST, Intuitively, the larger devia-\ntion of syntax distances in a sentence indicates the\nmore demand to the interdependent information be-\ntween elements, while the smaller deviation refers\nto phrasal constituency. This reveals that SRL and\nRel rely more on the dependency syntax, while\nSST is more relevant to constituents, which is con-\nsistent with previous studies (Socher et al., 2013;\nRabinovich et al., 2017; Xia et al., 2019; Fei et al.,\n2020).\nDistributions of heterogeneous syntax for dif-\nferent tasks. Based on the above analysis, we\nfurther analyze the distributions of dependency and\nconstituency structures after ﬁne-tuning, in differ-\nent tasks. Technically, we calculate the mean ab-\nsolute differences of syntax distances between el-\nements xi and the sub-root node xr in a sentence:\nDiff = 1\nN\n∑N\ni |di −dr|. We then linearly nor-\nmalize them into [0,1] for all the sentences in the\ncorpus of each task, and make statistics, as plot-\nTreeDepthTopConst Tense SOMO NER SST Rel SRL\n0:1\n0:5\n0:9\nFigure 7: Distributions of dependency and constituency\nsyntax in different tasks. Blue color indicates the pre-\ndominance of dependency, while Red for constituency.\nSST SRL\nOurs+Trm Tree+Trm Ours+Trm Tree+Trm\nNP 0.48 0.45 0.37 0.53\nVP 0.21 0.28 0.36 0.21\nPP 0.08 0.14 0.17 0.06\nADJP 0.10 0.05 0.05 0.12\nADVP 0.07 0.02 0.03 0.02\nOther 0.06 0.06 0.02 0.06\nAvg.Len. 3.88 3.22 2.69 3.36\nTable 4: Proportion of each type of induced phrase.\nted in Figure 7. Intuitively, the larger the value is,\nthe more interdependent to dependency syntax the\ntask is, and otherwise, to constituency structure.\nOverall, distributions of dependency structures and\nphrasal constituents in ﬁne-tuned LM vary among\ndifferent tasks, verifying that different tasks depend\non distinct types of structural knowledge. For ex-\nample, TreeDepth, Rel and SRL are most sup-\nported by dependency structure, while TopConst\nand SST beneﬁt from constituency the most. SOMO\nand NER can gain from both two types.\nPhrase types. Finally, we explore the diversity\nof phrasal syntax required by two representative\nend tasks, SST and SRL. We ﬁrst look into the\nstatistical proportion for different types of induced\nphrases6. As shown in Table 4, our method tends\nto induce more task-relevant phrases, where the\nlengths of induced phrases are more variable to the\ntask. Concretely, the ﬁne-tuned structure-aware\nTransformer helps to generate more NP also with\nlonger phrases for the SST task, and yield roughly\nequal numbers of NP and VP for SRL tasks with\nshorter phrases. This evidently gives rise to the\nbetter task performance. In contrast, the syntax\nphrases induced by the Tree+Trm model keep un-\nvarying for SST (3.22) and SRL (3.36) tasks.\n6Five main types are considered: noun phrase ( NP),\nverb phrase (VP), prepositional phrase (PP), adjective phrase\n(ADJP) and adverb phrase (ADVP).\n2159\n7 Conclusion\nWe presented a retroﬁtting method for structure-\naware Transformer-based language model. We\nadopted the syntax distance to encode both the con-\nstituency and dependency structure. To relieve the\nconﬂict of structure learning and semantic learn-\ning in Transformer LM, we proposed a middle-\nlayer structure learning strategy under a multi-\ntasks scheme. Results showed that structure-aware\nTransformer retroﬁtted via our proposed method\nachieved better language perplexity, inducing high-\nquality syntactic phrase. Furthermore, our LM after\nstructure-aware ﬁne-tuning gave signiﬁcantly im-\nproved performance for both semantic-dependent\nand syntactic-dependent tasks, also yielding most\ntask-related and interpretable syntactic structures.\nAcknowledgments\nWe thank the anonymous reviewers for their valu-\nable and detailed comments. This work is sup-\nported by the National Natural Science Founda-\ntion of China (No. 61772378, No. 61702121),\nthe National Key Research and Development Pro-\ngram of China (No. 2017YFC1200500), the Re-\nsearch Foundation of Ministry of Education of\nChina (No. 18JZD015), the Major Projects of\nthe National Social Science Foundation of China\n(No. 11&ZD189), the Key Project of State Lan-\nguage Commission of China (No. ZDI135-112)\nand Guangdong Basic and Applied Basic Research\nFoundation of China (No. 2020A151501705).\nReferences\nRoee Aharoni and Yoav Goldberg. 2017. Towards\nstring-to-tree neural machine translation. CoRR,\nabs/1704.04743.\nMahtab Ahmed, Muhammad Rifayat Samee, and\nRobert E. Mercer. 2019. You only need attention\nto traverse trees. In Proceedings of the ACL, pages\n316–322.\nDavid Alvarez-Melis and Tommi S. Jaakkola. 2017.\nTree-structured decoding with doubly-recurrent neu-\nral networks. In Proceedings of the ICLR.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the ACL, pages 2126–2136.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the ACL,\npages 2978–2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the NAACL, pages\n4171–4186.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep biafﬁne attention for neural dependency pars-\ning. In Proceedings of the ICLR.\nAndrew Drozdov, Patrick Verga, Mohit Yadav, Mohit\nIyyer, and Andrew McCallum. 2019. Unsupervised\nlatent tree induction with deep inside-outside recur-\nsive autoencoders. CoRR, abs/1904.02142.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the NAACL, pages\n199–209.\nAkiko Eriguchi, Yoshimasa Tsuruoka, and Kyunghyun\nCho. 2017. Learning to parse and translate improves\nneural machine translation. In Proceedings of the\nACL, pages 72–78.\nHao Fei, Meishan Zhang, Fei Li, and Donghong Ji.\n2020. Cross-lingual semantic role labeling with\nmodel transfer. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 28:2427–2437.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. CoRR, abs/1901.05287.\nJetic G¯u, Hassan S. Shavarani, and Anoop Sarkar. 2018.\nTop-down tree structured decoding with syntactic\nconnections for neural machine translation and pars-\ning. In Proceedings of the EMNLP, pages 401–413.\nJan Haji ˇc, Massimiliano Ciaramita, Richard Johans-\nson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs\nM`arquez, Adam Meyers, Joakim Nivre, Sebastian\nPad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,\nNianwen Xue, and Yi Zhang. 2009. The CoNLL-\n2009 shared task: Syntactic and semantic dependen-\ncies in multiple languages. In Proceedings of the\nCoNLL, pages 1–18.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the EMNLP, pages 4141–4150.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid ´O S ´eaghdha, Sebastian\nPad´o, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2010. SemEval-2010 task 8:\nMulti-way classiﬁcation of semantic relations be-\ntween pairs of nominals. In Proceedings of the\n5th International Workshop on Semantic Evaluation,\npages 33–38.\nJeremy Howard and Sebastian Ruder. 2018. Univer-\nsal language model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\n2160\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the ACL, pages 3651–\n3657.\nYoon Kim, Chris Dyer, and Alexander Rush. 2019a.\nCompound probabilistic context-free grammars for\ngrammar induction. In Proceedings of the ACL,\npages 2369–2385.\nYoon Kim, Alexander Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and G ´abor Melis. 2019b. Unsu-\npervised recurrent neural network grammars. InPro-\nceedings of the NAACL, pages 1105–1117.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings of\nthe ACL, pages 2676–2686.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. CoRR, abs/1908.08593.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of the ACL, pages 1426–1436.\nHongyin Luo, Lan Jiang, Yonatan Belinkov, and James\nGlass. 2019. Improving neural language models by\nsegmenting, attending, and predicting the future. In\nProceedings of the ACL, pages 1483–1493.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional Linguistics, 19(2):313–330.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proceedings of the ICLR.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed repre-\nsentations of words and phrases and their composi-\ntionality. In Proceedings of the NIPS, pages 3111–\n3119.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the EMNLP, pages\n1532–1543.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In in Proceedings of the NAACL, pages\n2227–2237.\nMaxim Rabinovich, Mitchell Stern, and Dan Klein.\n2017. Abstract syntax networks for code generation\nand semantic parsing. In Proceedings of the ACL,\npages 1139–1149.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical Re-\nport.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron C. Courville. 2018a. Neural language mod-\neling by jointly learning syntax and lexicon. In Pro-\nceedings of the ICLR.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron C. Courville. 2018b. Ordered neurons: Inte-\ngrating tree structures into recurrent neural networks.\nCoRR, abs/1810.09536.\nRichard Socher, Christopher D. Manning, and An-\ndrew Y . Ng. 2010. Learning continuous phrase\nrepresentations and syntactic parsing with recur-\nsive neural networks. In In Proceedings of the\nNIPS-2010 Deep Learning and Unsupervised Fea-\nture Learning Workshop.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the EMNLP, pages 1631–\n1642.\nKai Sheng Tai, Richard Socher, and Christopher D.\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. In Proceedings of the ACL, pages 1556–\n1566.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you learn\nfrom context? probing for sentence structure in con-\ntextualized word representations. In Proceedings of\nthe ICLR.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the CoNLL, pages 142–147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. CoRR, abs/1906.04284.\nXinyi Wang, Hieu Pham, Pengcheng Yin, and Graham\nNeubig. 2018. A tree-based decoder for neural ma-\nchine translation. In Proceedings of the EMNLP,\npages 4772–4777.\nYaushian Wang, Hung-Yi Lee, and Yun-Nung Chen.\n2019. Tree transformer: Integrating tree structures\ninto self-attention. In Proceedings of the EMNLP,\npages 1061–1070.\n2161\nAdina Williams, Andrew Drozdov, and Samuel R.\nBowman. 2017. Learning to parse from a seman-\ntic objective: It works. is it syntax? CoRR,\nabs/1709.01121.\nQingrong Xia, Zhenghua Li, Min Zhang, Meishan\nZhang, Guohong Fu, Rui Wang, and Luo Si. 2019.\nSyntax-aware neural semantic role labeling. In Pro-\nceedings of the AAAI, pages 7305–7313.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nMajid Yazdani and James Henderson. 2015. Incremen-\ntal recurrent neural network dependency parser with\nsearch-based discriminative training. In Proceed-\nings of the CoNLL, pages 142–152.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9498124122619629
    },
    {
      "name": "Computer science",
      "score": 0.8622381687164307
    },
    {
      "name": "Transformer",
      "score": 0.7318437099456787
    },
    {
      "name": "Language model",
      "score": 0.6959086656570435
    },
    {
      "name": "Exploit",
      "score": 0.6011161804199219
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5707386136054993
    },
    {
      "name": "Natural language processing",
      "score": 0.5555081963539124
    },
    {
      "name": "ENCODE",
      "score": 0.4793075621128082
    },
    {
      "name": "Dependency grammar",
      "score": 0.422650545835495
    },
    {
      "name": "Dependency (UML)",
      "score": 0.40156933665275574
    },
    {
      "name": "Engineering",
      "score": 0.07829150557518005
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}