{
    "title": "Self-Attention Transformer-Based Architecture for Remaining Useful Life Estimation of Complex Machines",
    "url": "https://openalex.org/W4315783035",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1777732647",
            "name": "Abdul Wahid",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106439466",
            "name": "Muhammad Yahya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1967266261",
            "name": "John G. Breslin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4316957019",
            "name": "Muhammad Ali Intizar",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2773549135",
        "https://openalex.org/W2471161958",
        "https://openalex.org/W2512976014",
        "https://openalex.org/W3033580259",
        "https://openalex.org/W2975620261",
        "https://openalex.org/W3024946300",
        "https://openalex.org/W4293235212",
        "https://openalex.org/W3006585575",
        "https://openalex.org/W6810171387",
        "https://openalex.org/W3037995823",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3119743098",
        "https://openalex.org/W6678281827",
        "https://openalex.org/W6788158879",
        "https://openalex.org/W2910482310",
        "https://openalex.org/W2772084711",
        "https://openalex.org/W2808622270",
        "https://openalex.org/W2947621394",
        "https://openalex.org/W3048268909",
        "https://openalex.org/W6786852218",
        "https://openalex.org/W6776048684",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6797282723",
        "https://openalex.org/W6804279831",
        "https://openalex.org/W3116796810",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4225645141",
        "https://openalex.org/W4312392907",
        "https://openalex.org/W4245812996",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W3173407600"
    ],
    "abstract": "Meaningful feature extraction from multivariate time-series data is still challenging since it takes into account the correlation between pairs of sensors as well as the temporal information of each time-series. Meanwhile, the huge industrial system has evolved into a data-rich environment, resulting in the rapid development and deployment of deep learning for machine RUL prediction. RUL (Remaining Useful Life) examines a system's behavior over the course of its lifetime, that is, from the last inspection to when the system's performance deteriorates beyond a certain point. RUL has been addressed using Long-Short-Term Memory (LSTM) and Convolution Neural Network (CNN), particularly in complex tasks involving high-dimensional nonlinear data. The main focus, however, has been on degradation data. In 2021, a new realistic run-to-failure turbofan engine degradation dataset was released, which differs significantly from the simulation dataset. The key difference is that each cycle's flight duration varies, so the existing deep technique will be ineffective at predicting the RUL for real-world degradation data. We present a Self-Attention Transformer-Based Encoder model to address this problem. The encoder with the time-stamp encoder layer works in parallel to extract features from various sensors at various time stamps. Self-attention enables efficient processing of extended sequences and focuses on key elements of the input time series. Self-attention is used in the proposed Transformer model to access global characteristics from diverse time-series representations. Under real-world flight conditions, we conduct tests on turbofan engine degradation data using variable-length input. The proposed approach for estimating RUL of turbofan engines appears to be efficient based on empirical results.",
    "full_text": null
}