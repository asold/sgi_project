{
  "title": "Foundation Models of Scientific Knowledge for Chemistry: Opportunities, Challenges and Lessons Learned",
  "url": "https://openalex.org/W4285170409",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2743223372",
      "name": "Sameera Horawalavithana",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2766172124",
      "name": "Ellyn Ayton",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2183465099",
      "name": "Shivam Sharma",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2786125914",
      "name": "Scott Howland",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2582597424",
      "name": "Megha Subramanian",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2680937503",
      "name": "Scott Vasquez",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2774001891",
      "name": "Robin Cosbey",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2229551975",
      "name": "Maria Glenski",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2170978862",
      "name": "Svitlana Volkova",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2977720775",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W3154872984",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W3002924435",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3179963059",
    "https://openalex.org/W3169064633",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W3134634493",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W3166508187",
    "https://openalex.org/W2927103915",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4300536030",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W3168661259",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W3166593409",
    "https://openalex.org/W2963766892",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3020786614",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W3168090480",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2945720633",
    "https://openalex.org/W2971258845"
  ],
  "abstract": "Sameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland, Megha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, Svitlana Volkova. Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. 2022.",
  "full_text": "Foundation Models of Scientific Knowledge for Chemistry:\nOpportunities, Challenges and Lessons Learned\nSameera Horawalavithana, Ellyn Ayton, Shivam Sharma, Scott Howland,\nMegha Subramanian, Scott Vasquez, Robin Cosbey, Maria Glenski, Svitlana Volkova\nPacific Northwest National Laboratory, Richland, W A\nAbstract\nFoundation models pre-trained on large corpora\ndemonstrate significant gains across many natu-\nral language processing tasks and domains e.g.,\nlaw, healthcare, education, etc. However, only\nlimited efforts have investigated the opportuni-\nties and limitations of applying these powerful\nmodels to science and security applications. In\nthis work, we develop foundation models of sci-\nentific knowledge for chemistry to augment sci-\nentists with the advanced ability to perceive and\nreason at scale previously unimagined. Specif-\nically, we build large-scale (1.47B parameter)\ngeneral-purpose models for chemistry that can\nbe effectively used to perform a wide range of\nin-domain and out-of-domain tasks. Evaluating\nthese models in a zero-shot setting, we analyze\nthe effect of model and data scaling, knowledge\ndepth, and temporality on model performance\nin context of model training efficiency.\nOur novel findings demonstrate that (1) model\nsize significantly contributes to the task perfor-\nmance when evaluated in a zero-shot setting;\n(2) data quality (aka diversity) affects model\nperformance more than data quantity; (3) sim-\nilarly, unlike previous work (Luu et al., 2021)\ntemporal order of the documents in the cor-\npus boosts model performance only for specific\ntasks, e.g., SciQ; and (4) models pre-trained\nfrom scratch perform better on in-domain tasks\nthan those tuned from general-purpose models\nlike Open AI’s GPT-2.\n1 Introduction\nThe emergence of foundation models (Bom-\nmasani et al., 2021) such as large-scale autoen-\ncoding models (e.g., BERT (Devlin et al., 2018),\nRoBERTa (Liu et al., 2019)) and autoregressive\nlanguage models ( e.g., GPT-2 (Radford et al.,\n2019), GPT-3 (Brown et al., 2020), Megatron-\nTuring (Smith et al., 2022) and Gopher (Rae\net al., 2021)) as well as multimodal vision and\nlanguage models, such as FLA V A (Singh et al.,\n2021) and Perceiver (Jaegle et al., 2021), estab-\nlished a paradigm shift in Artificial Intelligence\n(AI). These foundation models, also called neu-\nral platforms, are built using self-supervised pre-\ntraining at scale. They are then able to be easily\nadapted to a wide range of downstream tasks via\ntransfer learning (Bommasani et al., 2021) and fine-\ntuning (Lee et al., 2019).\nThe wide community adoption of foundation\nmodels can be explained by their key properties,\ntwo of which are emergent behavior and homog-\nenization – which also make foundation models\nappealing for adaption across science and security\ndomains. Emergence, or emergent behavior, reflect\nnew behaviors that a model introduces or is capable\nof that it was not explicitly trained to perform. Ho-\nmogenization is the consolidation of methods for\nbuilding machine learning systems across a wide\nrange of tasks. Another key advantage of scaling\nlanguage models is that they perform competitively\non language tasks using in-context learning without\nfine-tuning or gradient updates. Thus, in-context\nlearning allows foundation models to be effectively\nused across new downstream tasks with only sim-\nple instructions and a few optional examples.\nIn this work we focus on a science domain\n(chemistry) and demonstrate the value and limi-\ntations of large-scale language models evaluated\nacross a wide range of in-domain (science-focused)\nand out-of-domain tasks. Unlike the majority of\nwork on foundation models that focuses on pre-\ntraining these models on book corpora, web pages,\nWikipedia and mixed sources, e.g., the Pile (Gao\net al., 2020), we pretrain our models on scien-\ntific literature. Using scientific literature presents\nunique opportunities and challenges. Opportunities\ninclude the scale and diversity of scientific litera-\nture, the explicit structure, and explicit alignment\nacross different modalities in the papers, e.g., table\nand figure references. Challenges include limited\nbenchmarks that can be used to perform model\nevaluation, model prompting and interactions.\nThere are three major contributions of this work:\n(1) we collect and release a 0.67TB dataset cover-\ning research publication data across 10+ sources\nfor chemistry; (2) we release 28 auto-regressive\nfoundation models for chemistry that have been\npretrained from scratch; and (3) we present a rig-\norous evaluation of model performance on 15+ in-\ndomain and out-of-domain tasks that investigates\nthe effects of model and data scaling, knowledge\ndepth (aka diversity), and temporal order on perfor-\nmance as described in research questions below.\n(RQ1) Science-Focused Benchmarks What are\nthe strengths and weaknesses of foundation models\npretrained on scientific literature when evaluated\non out-of-domain vs. in-domain tasks?\n(RQ2) Scaling Effect How does model scale af-\nfect the downstream performance? Do neural scal-\ning laws presented in (Kaplan et al., 2020) hold for\nthe foundation models for science?\n(RQ3) Diversity Effect How does the depth of\nscientific knowledge, e.g., from paper abstracts vs.\nfull text, affect downstream performance?\n(RQ4) Temporal Effect How does the recency\nof scientific knowledge, e.g., when manipulating\nthe temporal order of the documents processed by\nthe model, affect downstream performance?\n2 Related Work\nIn this section we summarize previous efforts in\ntwo categories: mixed-domain continual pretrain-\ning that continues pretraining of a base model\non domain data and in-domain pretraining from\nscratch that pretrains a from scratch on domain\ndata. We present a model summary in Table 1.\nMixed-Domain Continual Pretraining Many\nefforts have focused on continual pretraining of a\nBERT (Devlin et al., 2018) base model. Several\nmodels have been developed for the biomedical\ndomain and the most frequently used corpora for\ndomain-specific continual preraining are PubMed\nabstracts and PubMed Central full-text articles\n(PMC) (Lee et al., 2020; Peng et al., 2019; Phan\net al., 2021). In the Chemistry domain, Guo et al.\n(2021) performed continual pretraining of a base\nBERT model on 200K chemistry journal articles\nfor product extraction (ChemBERT) and reaction\nrole labeling (ChemRxnBERT).\nIn-Domain Pretraining from Scratch Previ-\nous work has shown that pretraining models from\nscratch on domain-specific data has a signifi-\ncant benefit over continual pretraining of general-\ndomain language models (Gu et al., 2021). This\nis mainly due to the availability of in-domain data\nfor both generating the vocabulary and pretrain-\ning. SciBERT (Beltagy et al., 2019) is pretrained\naccording to this procedure using the vocabulary\ngenerated from computer science and biomedical\ndomains. PubMedBERT (Gu et al., 2021) is an-\nother example of pretraining the base BERT model\nfrom scratch using PubMed. Unlike any previous\nwork, we use both continual and from scratch pre-\ntraining to build the largest foundation model for\nChemistry (1.47B) on the largest (0.67TB) and the\nmost diverse corpus (10+ sources) collected to date.\n3 Model Pretraining\nUnlike the majority of related models that rely on a\nbase BERT (or variant) model, we adapt the Open-\nAI’s GPT-2 transformer decoder architecture (Rad-\nford et al., 2019) to train autoregressive language\nmodels for Chemistry. To understand the impact\nof model size (RQ2), we experiment with four dif-\nferent Transformer sizes: small (S), medium (M),\nlarge (L), and extra-large (XL). These models dif-\nfer in the number of decoder layers, hidden size of\nthe model, and the number of attention heads in\ntransformer blocks as shown in Table 2.\nOur experiments leverage the GPT-NeoX Python\nlibrary (Andonian et al., 2021) developed with\nMegatron (Shoeybi et al., 2019) and Deep-\nSpeed (Rasley et al., 2020). We optimize the au-\ntoregressive log-likelihood (i.e., cross-entropy loss)\naveraged over a 2048-token context. We set the\nmicro batch size per GPU as 4, and the learning\nrate to 2 × 10−4, and rely on the cosine decay. We\nuse an Adam optimizer with β1 = 0.9, β2 = 0.99,\nand σ = 10−8 and clip the gradient norm at 1.0.\nIn addition, ZeRO optimizer (Rajbhandari et al.,\n2019) was used to reduce memory footprint by dis-\ntributing optimizer states across several processes.\nTo reduce memory and increase training through-\nput, we use mixed-precision training (Rasley et al.,\n2020) and the parallel attention and feed-forward\nimplementations available in GPT-NeoX (Black\net al., 2022). We also use the Rotary positional em-\nbeddings (Su et al., 2021) instead of the learned po-\nsitional embeddings used in the GPT-2 model (Rad-\nford et al., 2019) because they offer performance\nTable 1: Foundation models for science focus on the biomedical, math, computer science and chemistry domains.\nWe use † to indicate models trained for chemistry.\nModel Data Source Pretraining Corpus #Params (B)\nLee et al. 2020 BioBERT Wiki + Books continual pretraining PubMed 0.11\nAlsentzer et al. 2019 ClinicalBERT Wiki + Books continual pretraining MIMIC1 0.11\nPeng et al. 2019 BlueBERT Wiki + Books continual pretraining PubMed + MIMIC 0.11\nLiu et al. 2021 MATH-BERT Arxiv continual pretraining Arxiv 0.11\nGuo et al. 2021 Chem(Rxn)BERT † Wiki + Books continual pretraining Chemistry Journals 0.11\nPhan et al. 2021 SciFive C4 continual pretraining PubMed 0.22\n0.77\nNaseem et al. 2021 BioALBERT Wiki + Books continual pretraining PMC + MIMIC-II 0.02\nLewis et al. 2020 BioRoBERTa Wiki + Books continual pretraining PMC + MIMIC-III 0.30\nYuan et al. 2021 KeBioLM PubMed continual pretraining PubMed + UMLS2 0.34\nShin et al. 2020 BioMegatron PubMed from scratch\ncontinual pretraining PubMed 0.80\n1.20\nKanakarajan et al. 2021 BioELECTRA PubMed from scratch PubMed 0.11\nMiolo et al. 2021 ELECTRAMed PubMed from scratch PubMed 0.11\nBeltagy et al. 2019 SciBERT PMC + CS from scratch PMC + CS 0.11\nLiu et al. 2021 OAG-BERT OAG from scratch OAG 0.11\nGu et al. 2021 PubMedBERT PubMed from scratch PubMed 0.34\nOur Work (autoregressive)† 10+ sources\n(Chemistry)\nfrom scratch\ncontinual pretraining\n10+ sources\n(Chemistry) 1.47\nTable 2: Our model configurations: dL is the num-\nber of decoder layers, ddim is the hidden size of the\nmodel, dheads is the number of attention heads. We\ncompare model configurations between GPT-NeoX and\nOpenAI’s GPT-2. GPT-NeoX architecture is originally\nfrom GPT-3 (Brown et al., 2020)\nSize Model dL ddim dheads #Params (B)\nS GPT-NeoX 12 768 12 0.18GPT-2 12 768 12\nM GPT-NeoX 24 1024 16 0.40GPT-2 24 1024 16\nL GPT-NeoX 24 1536 16 0.80GPT-2 36 1280 20\nXL GPT-NeoX 24 2048 16 1.47GPT-2 48 1600 25\nadvantages in tasks with longer texts by capturing\nrelative position dependency in self-attention.\nOur models are pretrained across multiple work-\ners with data parallelism. As the largest model in\nour experiments fit on a single GPU, we didn’t use\nthe model (tensor) or pipeline parallelism. Mod-\nels are pretrained from scratch for a total of 320K\nsteps. The original GPT-2 models are fine-tuned\nfor 150K steps. We perform experiments in a single\nDGX-A100 machine with 8 80Gb GPUs.\n4 Data Collection and Processing\nWe collected a large corpus of 53.45 million\nchemistry-focused scientific articles and abstracts,\nresulting in 670GB of text data. As shown in Ta-\nble 3, our corpus was collected from 10 different\ndata sources: Arxiv, Aminer (AMiner), CORD-\n19 (Wang et al., 2020b), CORE (Pontika et al.,\n2016), Microsoft Academic Graph (MAG) (Wang\net al., 2020a), OSTI, PubMed (Gao et al., 2020)\n(abstracts and fulltexts), and the Web of Science\n(WoS). See Appendix A for full data descriptions.\nTable 3: Dataset statistics: combined datasets are after\nthe de-duplication process. We split datasets to those\nthat include abstracts 〈A〉 vs. full texts 〈FT〉.\nSource #Articles (M) #Tokens (B) Size (Gb)\nMAG 〈A〉 34.26 7.43 46\nAminer 〈A〉 18.50 5.80 35\nS2ORC 〈A〉 10.44 2.05 32\nWoS 〈A〉 7.90 3.31 18\nCORD-19 〈A〉 < 0.01 < 0.01 0.2\nOSTI 〈A〉 0.05 < 0.01 0.1\nArxiv 〈A〉 0.38 0.04 0.4\nPubMed 〈A〉 0.28 0.08 0.5\nPubMed 〈FT〉 0.70 7.34 32\nCORE 〈FT〉 7.27 215.50 743\nCombined 〈A〉 46.94 16.18 67\nCombined 〈FT〉 6.52 184.42 603\nCombined 〈A+FT〉 53.45 200.61 670\nBecause the data sources we relied on comprise\nresearch publications from many science domains,\nwe sampled articles using a list of domain-specific\nkeywords for chemistry to create the dataset sum-\nmarized in Table 3. These keywords were ex-\ntracted by using a Correlation Explanation (Gal-\nlagher et al., 2017) topic model followed by manual\nfiltering by subject matter experts. This resulted in\na list of more than 1K chemistry-related entities,\nranging from compound names like ethyl acetate,\nmethyl methacrylate, sulfoxide, etc. to experiment\nand procedures like tunneling microscopy, neutral-\nization, enzymatic hydrolysis, etc.\nAMiner\n49.6%\nMAG\n29.9%\nCORE\n11.9% S2ORC\n4.0%\nWoS\n3.8%\n0.8%\narXiv\nDBLP\nOSTI\nPubMed\nCORD19\nDistribution within Sources with <1% (upper right, black box)\nFigure 1: Summary of data source representation within\nthe Combined A+F data sample. Coloring illustrates\nwhether a data source contains peer reviewed (Blue),\nmixed (Purple), or not peer reviewed (Red) articles.\nData Cleaning Recent research has shown that\nduplicates in training data can significantly impact\nthe downstream task performance of LLMs (Lee\net al., 2021; Carlini et al., 2022). To this end, we\nperformed deduplication of our corpus based on\noverlap of titles within and across data sources. We\nprocessed titles to strip punctuation and casefold\nand considered two articles A1 and A2 to be du-\nplicates if they had the same processed title. With\nthis technique, we were able to remove significant\namounts of duplicate scientific articles both within\nand across sources. The deduplication process re-\nduced our corpus from 875GB to 670GB (67.8M to\n53.5M publications), removing 14.3M duplicates.\nTokenization As used in GPT-2 model, we use a\nByte Pair Encoding (BPE) tokenizer. We train BPE\ntokenizers for each data sample with a vocabulary\nsize of 64K as preliminary experiments varying\nvocabulary sizes from 64K to 256K for smaller\nscale model pretraining did not show significant\ndifferences in performance. We compare the GPT-\n2 vocabulary generated from the WebText and the\nin-domain vocabularies generated from our cor-\npora and find that the in-domain vocabulary breaks\nchemical entities into fewer tokens. For example,\ndimethylnitroxide was tokenized into #dimethyl,\n#nitr, #oxide using the in-domain vocabulary and\n#dim, #ethyl, #nit, #rox, #ide using the GPT-2 vo-\ncabulary.\n5 Analysis and Results\nThis section presents the analysis of 28 pretrained\nmodels evaluated on 15+ in-domain and out-of-\ndomain downstream tasks (RQ1, Section 5.1). We\ninvestigate the effects of model and data scaling\n(RQ2, Section 5.2), knowledge diversity (RQ3, Sec-\ntion 5.3), and temporal order (RQ4, Section 5.4) on\nthe downstream performance. We also compare the\nresults from continual vs. from scratch pretraining\n(Section 5.5) and present the analysis of large-scale\ntraining efficiency (Section 5.6).\nBaseline Models As we use a similar model ar-\nchitecture, we identify Open AI’s GPT-2 (Radford\net al., 2019) as a baseline comparison model. We\ncompare our performance with four variants of the\noriginal GPT-2 models, corresponding to small (S),\nmedium (M), large (L), and extra-large (XL) sized\ntransformer architectures shown in Table 2. We\nnote that GPT-2 models were pretrained on Web-\nText – 8 million web documents (40Gb). Thus, we\nalso include a base GPT-2 model (medium) that\nhas been updated with continual pretraining using\nour Combined 〈A+FT〉 dataset.\nOur Models We pretrained models with indi-\nvidual datasets (AMiner, CORE, MAG, PubMed,\nS2ORC, WOS) and combined abstracts and full-\ntexts. Our goal is to systematically study data\nbiases in the model performance when pretrain-\ning models with individual datasets. For example,\nPubMed publications cover mostly bio-medicinal\nterms (Gu et al., 2021), while the majority of\nS2ORC publications are from medicine, biology,\nphysics, and mathematics (Lo et al., 2020). We\nonly use 4 GPUs for the models pretrained with\nindividual datasets and 8 GPUs for the rest. This is\nto control the number of tokens seen during model\npretraining (320,000 steps * 4 GPUs * 4 micro\nbatch size * 2,048 context size = 10B tokens) rel-\native to the maximum number of tokens available\nin the respective datasets (as reported in Table 3).\nWe also trained one XL (4x) model with 4x larger\nbatch size than what used in XL model to evaluate\nthe impact of the number of training tokens.\n5.1 Zero-shot Performance\nWe evaluate our models using several benchmarks\nto assess the effectiveness in both in-domain and\nout-of-domain tasks. The benchmarks we include\nare described in Appendix B. We use the lm-\nevaluation-harness Python repository (Gao et al.,\nTable 4: Downstream Zero-shot In-Domain Task Performance. We use ‡ to indicate the baseline model tuned from\nthe base GPT-2 model. Pile performance is reported using perplexity, with all other tasks reported using accuracy.\nWe highlight the top-4 performance per task in bold, with top performance indicated with an underline. XL (4x)\nmodel is trained with 4x larger batch size that used in other models.\nModel Size HT-HC HT-CC ARC-E ARC-C SciQ OpenBookQA Pile\nBaseline\nS 0.22 0.25 0.44 0.19 0.75 0.16 96.50\nM 0.18 0.27 0.49 0.22 0.77 0.19 61.26\nL 0.18 0.28 0.53 0.22 0.80 0.19 48.86\nXL 0.18 0.26 0.58 0.25 0.83 0.22 42.29\nM‡ 0.19 0.31 0.35 0.19 0.61 0.13 87.57\nAMiner\nS 0.18 0.27 0.43 0.21 0.70 0.17 38.40\nM 0.18 0.34 0.45 0.20 0.74 0.16 30.55\nL 0.23 0.34 0.49 0.23 0.78 0.18 24.18\nXL 0.23 0.34 0.50 0.23 0.77 0.17 25.52\nCORE\nS 0.19 0.28 0.36 0.19 0.69 0.15 78.24\nM 0.22 0.34 0.40 0.20 0.71 0.15 59.19\nL 0.17 0.30 0.41 0.19 0.75 0.14 52.95\nXL 0.20 0.28 0.47 0.21 0.78 0.15 39.46\nMAG\nS 0.24 0.28 0.41 0.20 0.66 0.17 38.03\nM 0.18 0.27 0.45 0.21 0.68 0.17 30.88\nL 0.19 0.36 0.51 0.24 0.80 0.18 24.78\nXL 0.20 0.36 0.50 0.22 0.80 0.20 26.09\nPubMed-F\nS 0.26 0.30 0.41 0.20 0.60 0.16 56.03\nM 0.19 0.27 0.43 0.21 0.68 0.18 45.69\nL 0.18 0.28 0.43 0.22 0.74 0.17 37.22\nXL 0.18 0.27 0.48 0.21 0.77 0.16 35.14\nS2ORC\nS 0.26 0.33 0.31 0.21 0.31 0.17 59.20\nM 0.27 0.22 0.33 0.18 0.31 0.16 45.60\nL 0.28 0.23 0.32 0.21 0.31 0.17 42.14\nXL 0.24 0.31 0.33 0.19 0.30 0.18 42.35\nWoS\nS 0.22 0.31 0.33 0.22 0.37 0.17 54.41\nM 0.25 0.32 0.32 0.20 0.34 0.16 48.31\nL 0.27 0.30 0.32 0.21 0.37 0.17 46.44\nXL 0.23 0.34 0.34 0.21 0.39 0.16 45.86\nCombined-A XL 0.17 0.28 0.54 0.23 0.83 0.18 22.77\nCombined-F XL 0.20 0.30 0.48 0.21 0.79 0.15 40.18\nCombined-A+F XL 0.18 0.30 0.48 0.22 0.79 0.17 31.03\nCombined-A+F XL (4x) 0.18 0.25 0.55 0.24 0.84 0.17 23.01\n2021) for the benchmark implementation.\nIn-domain Evaluation We consider five exist-\ning chemistry benchmarks, specifically Hendryck-\nsTest (Hendrycks et al., 2020) for high school\n(HT-HC) and college (HT-CC) levels, and science-\nfocused – ARC (Clark et al., 2018), SciQ (Welbl\net al., 2017), OpenBookQA (Mihaylov et al., 2018),\nPile-PubMed-Abstracts (Gao et al., 2020)). As\nshown in Table 4, one or more of our models outper-\nform baseline GPT-2 models for the two chemistry\ntasks, general science QA (SciQ) and the science-\nfocused language modelling. Of the remaining\ntasks, our models perform within 1-4% of GPT-2\nbaselines.\nOut-of-domain Evaluation We evaluate out-\nof-domain performance using 9 commonly used\nLLM benchmarks: BoolQ (Clark et al., 2019),\nCB (De Marneffe et al., 2019), WIC (Pilehvar and\nCamacho-Collados, 2018), WSC (Levesque et al.,\n2012), MathQA (Amini et al., 2019), PIQA (Bisk\net al., 2020), PubMedQA (Jin et al., 2019), Lam-\nbada (Paperno et al., 2016) and WikiText (Merity\net al., 2016). As shown in Table 5, our models out-\nperform baseline GPT-2 models for CB, WIC and\nWSC and match the best accuracy for BoolQ but\nthe GPT-2 baselines outperform on the remaining\ntasks, particularly Lambada and Wikitext – the two\ngeneral language modeling tasks.\n5.2 Scaling Effect\nPrevious work (Kaplan et al., 2020) has shown that\nupstream cross entropy loss scales as a power-law\nwith model size, dataset size, and the amount of\ncompute. In this section, we revisit these claims on\nscaling Transformer architectures.\nAnalyzing upstream cross entropy loss Dur-\ning pretraining, we group each dataset into train-\ning/validation/test (949/50/1) splits. We report the\nTable 5: Downstream Out-of-domain Task Performance. We use‡ to indicate the baseline model tuned from the base\nGPT-2 model. Performance on Lambada and Wikitext is reported using perplexity, all other tasks report accuracy .\nTop-4 performance highlighted in bold, with best performance indicated with underlines. XL (4x) model is trained\nwith 4x larger batch size that used in other models.\nModel Size BoolQ CB WIC WSC MathQA PIQA PubMedQA Lambada Wikitext\nBaseline\nS 0.49 0.41 0.49 0.43 0.21 0.63 0.44 40.06 37.37\nM 0.59 0.43 0.50 0.40 0.23 0.68 0.53 18.25 26.75\nL 0.60 0.45 0.50 0.46 0.23 0.70 0.54 12.97 22.61\nXL 0.61 0.39 0.50 0.50 0.24 0.71 0.59 10.63 20.38\nM‡ 0.62 0.34 0.50 0.36 0.20 0.55 0.55 2834.51 126.55\nAMiner\nS 0.41 0.39 0.50 0.44 0.22 0.56 0.46 2825.84 158.85\nM 0.40 0.39 0.51 0.41 0.21 0.57 0.43 1802.35 116.93\nL 0.61 0.48 0.50 0.47 0.22 0.58 0.36 661.81 87.23\nXL 0.50 0.39 0.50 0.37 0.21 0.58 0.43 786.22 91.28\nCORE\nS 0.62 0.41 0.50 0.37 0.20 0.55 0.55 671.43 100.53\nM 0.62 0.41 0.50 0.37 0.21 0.56 0.55 273.06 77.96\nL 0.61 0.41 0.50 0.37 0.21 0.57 0.51 173.15 69.62\nXL 0.61 0.38 0.50 0.37 0.22 0.58 0.45 79.95 50.47\nMAG\nS 0.41 0.23 0.50 0.40 0.21 0.56 0.43 1142.83 118.40\nM 0.38 0.07 0.50 0.37 0.21 0.57 0.41 628.72 91.36\nL 0.51 0.14 0.50 0.35 0.22 0.59 0.39 282.39 67.74\nXL 0.40 0.11 0.51 0.62 0.22 0.59 0.34 364.54 70.71\nPubMed-F\nS 0.58 0.41 0.50 0.45 0.21 0.57 0.54 2670.39 148.88\nM 0.61 0.39 0.50 0.38 0.20 0.58 0.49 1742.00 119.74\nL 0.57 0.41 0.50 0.38 0.21 0.59 0.42 843.83 95.75\nXL 0.60 0.41 0.50 0.39 0.22 0.59 0.49 679.80 90.38\nS2ORC\nS 0.38 0.41 0.50 0.63 0.20 0.57 0.34 122739.30 403.48\nM 0.38 0.43 0.50 0.63 0.22 0.56 0.34 80151.10 330.56\nL 0.38 0.46 0.50 0.63 0.21 0.56 0.34 89136.68 327.53\nXL 0.38 0.50 0.50 0.63 0.20 0.56 0.33 107065.48 351.81\nWoS\nS 0.38 0.39 0.50 0.63 0.21 0.55 0.34 140552.69 556.00\nM 0.38 0.45 0.50 0.63 0.19 0.54 0.34 182967.37 498.36\nL 0.41 0.36 0.47 0.54 0.21 0.56 0.42 148609.73 480.91\nXL 0.57 0.34 0.50 0.37 0.20 0.55 0.56 192970.64 509.06\nCombined-A XL 0.56 0.16 0.50 0.37 0.21 0.60 0.50 250.88 61.07\nCombined-F XL 0.62 0.38 0.50 0.37 0.22 0.57 0.55 72.50 48.96\nCombined-A+F XL 0.61 0.41 0.50 0.39 0.23 0.59 0.48 71.43 48.65\nCombined-A+F XL (4x) 0.61 0.41 0.50 0.37 0.24 0.60 0.56 30.40 33.05\nmodel performance on validation data using cross\nentropy loss in nats. This measure will be averaged\nover the 2048-token context. We find that the cross\nentropy loss decreases as we increase the model\nsize (as shown in Figure 2). Larger models reach\na given loss value in a higher rate than the smaller\nmodels. This observation illustrates the relation-\nship between model performance (as measured by\nthe upstream cross entropy loss) and model size,\nconfirming (Kaplan et al., 2020).\nAnalyzing downstream task performance Can\nwe speculate downstream task performance of a\nmodel from the pretraining performance? First, we\nfind that the models perform considerably well on\nPile in comparison to the Lambada or WikiText.\nThere is a 48% performance advantage in this task\nover the best performing baseline GPT-2 model.\nThis may be due to the models capturing scientific\nlanguage better than general language. It is im-\nFigure 2: Distribution of validation loss by model size:\nperformance improves as the model size increases.\nportant to note that we exclude PubMed Abstracts\nin the individual data collection to avoid potential\ncontamination between the training andPile testing\ndata. As shown in Table 4, larger models perform\nwell on these language modeling tasks.\nSecond, we noticed that the XL (4x) model\ntrained for more tokens performs significantly bet-\nter than the similar sized XL model. Specifically,\nXL (4x) model was trained with 128 total batch\nsize compared to the 32 total batch size used in XL\nmodel. XL (4x) model achieves the lowest Lam-\nbada and WikiTextperplexity values across all our\nmodels trained from scratch (as shown in Table 5).\nThe same model also achieves the best SciQ perfor-\nmance with 0.84 accuracy and comparable in other\ntasks performance with the XL model. This experi-\nment highlights the importance of training models\nwith larger batch size. We note that the baseline\nmodels (Radford et al., 2019) were trained with 4x\nlarger batch size (total batch size 512) than what\nused in XL (4x) model. We believe that the XL\n(4x) model can reach the similar perplexity values\nwhen trained for this data scale.\nThird, we find that zero-shot task performance\nin SciQ, HT-CC and ARC-E increases as we in-\ncrease the model size (see Table 5). However, there\nis no clear relationship between the task perfor-\nmance and the model sizes in the rest of bench-\nmark datasets. We suggest that pretraining perfor-\nmance may not be the ideal indicator to speculate\nthe overall downstream task performance, espe-\ncially in the zero-shot setting. However, model size\nsignificantly contributes to the task performance.\n5.3 Diversity Effect\nWhile abstracts often provide a summary of scien-\ntific publications, the full text contains more details.\nIn this section, we analyze the performance of mod-\nels trained on paper abstracts versus full texts.\nFirst, the XL models trained with the combined\nabstract dataset achieve the lowest perplexity score\n(22.77) on the Pile – a 45% performance advantage\nover the full text version. There are might be sev-\neral factors that contribute to this, but one may be\nthe focused language in abstracts.\nSecond, the model trained with the combined\nabstracts achieves the second best accuracy (0.83\nin comparison to 0.79 for the full text model) in\nSciQ. Some of the models pretrained on individual\nabstract data achieve comparable performance in\nSciQ, e.g., MAG and AMiner models achieve 0.8\nand 0.78 accuracy, respectively. We believe the\ndiversity of scientific knowledge provided from the\nabstract data is useful since SciQ questions span\nbiology, chemistry, earth science, and physics.\nThird, we compare model performance trained\nwith abstracts vs. full texts in the HT task and see\nthat the best accuracy is achieved using the MAG\nand S2ORC datasets rather than the combined ab-\nstracts. This suggests the importance of contextual\nknowledge provided by different data sources.\nFinally, combined full text model performs better\nthan the model trained with the abstracts in all out-\nof-domain tasks except PIQA. This performance\ndifference may be due to the more expressive and\ndiverse language presented in the full texts than in\nthe abstracts. Thus, expanding full text coverage\nmay improve out-of-domain task generalization.\n5.4 Temporal Effect\nScientific knowledge evolves over time reflecting\nnew research ideas, innovations, and findings. In\nthis section, we test how continual pretraining on\ntemporal-aligned scientific publications impacts\ndownstream performance. For this experiment,\nwe maintain two variants of the MAG dataset\nwith random-ordered and temporal-ordered articles,\nsplitting each into ten equal subsets. We continue\npretraining a base medium (M) sized model itera-\ntively with the subsets in the order they appeared\nin the respective data variant. For example, in the\ntemporally-aligned experiments, we first pretrain a\nmodel with 3.4M (10%) articles from before 1978,\nand then use it as the base model to continue pre-\ntraining with another 3.4M (10%) articles from\nbetween 1978 and 1989. We train the initial model\nfor 150K steps and each subsequent model for 10K\nsteps with additional data. Figure 3 shows the per-\nformance of model checkpoints across in-domain\nand out-of-domain tasks.\nThere are two key findings. First, SciQ and ARC-\nE zero-shot task performances improve over time\nwith the models trained with temporally-ordered\nscientific texts (as shown in Figure 3b). For ex-\nample, SciQ accuracy improves from 0.64 to 0.73\nfrom the base model checkpoint to the final model\ncheckpoint. Similarly, ARC-E accuracy improves\nfrom 0.43 to 0.45. This is due to the temporal order\nof the knowledge acquired by the model. When the\nmodel was pretrained with random-ordered data\nsubsets, we observe only a slight ( < 1%) perfor-\nmance increase (as shown in Figure 3a).\nThere are mixed patterns in performance across\nout-of-domain tasks. For example, a slight per-\nformance increase in the PIQA, CB, PubMedQA,\nand WIC over time with the models trained with\ntemporally-ordered scientific texts. On the other\nhand, there is a performance drop in the BoolQ\nand WSC over time. This may be due to the\ncatastrophic forgetting prevalent in continual learn-\ning (Ramasesh et al., 2021). Future work will in-\n(a) Random Order\n (b) Temporal Order\nFigure 3: The effect of temporal order of publications during pretraining. We align publications in the MAG corpus\nby year and split them into ten equal subsets. We repeat the process in a randomly-ordered corpus for comparison,\nrecording model checkpoints after performing continual pretraining on each data subset.\nvestigate other confounding factors that may con-\ntribute to this performance patterns.\n5.5 Continual vs. From Scratch Pretraining\nIn this section, we test whether the continual pre-\ntraining of a base GPT model with additional\ndomain-specific data is helpful in the downstream\ntask performance. We report the zero-shot perfor-\nmance of the tuned model across in-domain (Ta-\nble 4) and out-of-domain (Table 5) tasks. We have\ntwo main observations from this experiment.\nFirst, fine-tuned models fall behind other base-\nlines in a majority of in-domain tasks. HT-CC is\nthe only in-domain task that the tuned model out-\nperforms the rest of models, yet fails to outperform\nthe best performing model trained from scratch.\nSecond, fine-tuned models have a significant per-\nformance drop in the general language modeling\ntasks (Lambada and Wikitext). For example, the\ntuned model records 6x performance drop in the\nWikitext compared to the best performing model.\nThere are several factors in the continual pretrain-\ning that may contribute to this. As the tuned model\nuses the original GPT-2 vocabulary, it must use the\nfragmented general subwords to tokenize the chem-\nistry terms available in our corpora. On the other\nhand, the tuned model starts with the suboptimal\ninitialization from the general-domain language\nmodel (Gu et al., 2021). This initialization may\ndiverge the model in the optimization process that\nmay not be recovered.\n5.6 Training Efficiency\nWe use several dimensions to describe the training\nefficiency, i.e., #FLOPs, throughput (speed), and\nmemory. We compare these compute dimensions\n(a) GPU computation in #Floating Point Operations\n(b) GPU Memory Allocation\nFigure 4: GPU system performance during pretraining.\nacross the four model sizes described in the Ta-\nble 2. The smallest (S) model has 59% FLOPs of\nthe largest (XL) model, twice the speed (steps/s),\n32% per device GPU memory savings, and 76% to-\ntal parameter savings (see Figure 4). With such\ncompute budget, small (S) models only outper-\nforms the XL model in 21% in-domain and 34%\nout-of-domain evaluation tasks. This suggests the\nimportance of compute budget required in scaling\nfoundation models.\n6 Conclusions\nIn this paper, we collected and released 0.67TB\nof research publication data collected across 10+\nsources for chemistry. We pretrained and released\n25+ foundation models for chemistry. We rig-\norously analyzed model performance on 15+ in-\ndomain and out-of-domain tasks.\nAcknowledgements\nThe research described in this paper is part of the\nMARS Initiative at Pacific Northwest National Lab-\noratory, which is operated by Battelle Memorial\nInstitute for the U.S. Department of Energy un-\nder contract DE-AC05-76RLO1830. Any opin-\nions, findings, and conclusions or recommenda-\ntions expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views\nof the United States Government or any agency\nthereof.\nReferences\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nAMiner. https://www.aminer.org/.\nAida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-\nKedziorski, Yejin Choi, and Hannaneh Hajishirzi.\n2019. Mathqa: Towards interpretable math word\nproblem solving with operation-based formalisms.\narXiv preprint arXiv:1905.13319.\nAlex Andonian, Quentin Anthony, Stella Biderman, Sid\nBlack, Preetham Gali, Leo Gao, Eric Hallahan, Josh\nLevy-Kramer, Connor Leahy, Lucas Nestler, Kip\nParker, Michael Pieler, Shivanshu Purohit, Tri Songz,\nPhil Wang, and Samuel Weinbach. 2021. GPT-NeoX:\nLarge scale autoregressive language modeling in py-\ntorch.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. arXiv\npreprint arXiv:1903.10676.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nCord19. https://www.semanticscholar.org/cord19/download.\nMarie-Catherine De Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Inves-\ntigating projection in naturally occurring discourse.\nIn proceedings of Sinn und Bedeutung, volume 23,\npages 107–124.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nRyan J Gallagher, Kyle Reing, David Kale, and Greg\nVer Steeg. 2017. Anchored correlation explanation:\nTopic modeling with minimal domain knowledge.\nTransactions of the Association for Computational\nLinguistics, 5:529–542.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nJiang Guo, A. Santiago Ibanez-Lopez, Hanyu Gao, Vic-\ntor Quach, Connor W. Coley, Klavs F. Jensen, and\nRegina Barzilay. 2021. Automated chemical reaction\nextraction from scientific literature. Journal of Chem-\nical Information and Modeling , 0(0):null. PMID:\n34115937.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol\nVinyals, Andrew Zisserman, and Joao Carreira. 2021.\nPerceiver: General perception with iterative attention.\nIn International Conference on Machine Learning,\npages 4651–4664. PMLR.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nKamal Kanakarajan, Bhuvana Kundumani, and\nMalaikannan Sankarasubbu. 2021. Bioelectra: pre-\ntrained biomedical text encoder using discriminators.\nIn Proceedings of the 20th Workshop on Biomedical\nLanguage Processing, pages 143–154.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2019. Mixout: Effective regularization to fine-\ntune large-scale pretrained language models. arXiv\npreprint arXiv:1909.11299.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating training\ndata makes language models better. arXiv preprint\narXiv:2107.06499.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. KR’12,\npage 552–561. AAAI Press.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, pages\n146–157.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. arXiv preprint arXiv:1705.04146.\nXiao Liu, Da Yin, Xingjian Zhang, Kai Su, Kan Wu,\nHongxia Yang, and Jie Tang. 2021. Oag-bert: Pre-\ntrain heterogeneous entity-augmented academic lan-\nguage models. arXiv preprint arXiv:2103.02410.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rod-\nney Michael Kinney, and Daniel S. Weld. 2020.\nS2orc: The semantic scholar open research corpus.\nIn ACL.\nKelvin Luu, Daniel Khashabi, Suchin Gururangan, Kar-\nishma Mandyam, and Noah A Smith. 2021. Time\nwaits for no one! analysis and challenges of temporal\nmisalignment. arXiv preprint arXiv:2111.07408.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789.\nGiacomo Miolo, Giulio Mantoan, and Carlotta Orsenigo.\n2021. Electramed: a new pre-trained language repre-\nsentation model for biomedical nlp. arXiv preprint\narXiv:2104.09585.\nUsman Naseem, Adam G Dunn, Matloob Khushi, and\nJinman Kim. 2021. Benchmarking for biomedical\nnatural language processing tasks with a domain spe-\ncific albert. arXiv preprint arXiv:2107.04374.\nOAG. https://www.microsoft.com/en-\nus/research/project/open-academic-graph/.\nDenis Paperno, Germán Kruszewski, Angeliki Lazari-\ndou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernández. 2016. The lambada dataset: Word pre-\ndiction requiring a broad discourse context. arXiv\npreprint arXiv:1606.06031.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Trans-\nfer learning in biomedical natural language process-\ning: an evaluation of bert and elmo on ten bench-\nmarking datasets. arXiv preprint arXiv:1906.05474.\nLong N Phan, James T Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Scifive: a text-to-text\ntransformer model for biomedical literature. arXiv\npreprint arXiv:2106.03598.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2018. Wic: the word-in-context dataset for evaluat-\ning context-sensitive meaning representations. arXiv\npreprint arXiv:1808.09121.\nNancy Pontika, Petr Knoth, Matteo Cancellieri, and\nSamuel Pearce. 2016. Developing infrastructure to\nsupport closer collaboration of aggregators with open\nrepositories. LIBER Quarterly, 25(4):172–188.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nS Rajbhandari, J Rasley, O Ruwase, and Y He. 2019.\nZero: memory optimization towards training a trillion\nparameter models. arxiv e-prints arxiv: 11910.02054\n(2019).\nVinay Venkatesh Ramasesh, Aitor Lewkowycz, and\nEthan Dyer. 2021. Effect of scale on catastrophic\nforgetting in neural networks. In International Con-\nference on Learning Representations.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery & Data Mining, pages 3505–3506.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. Biomegatron: Larger\nbiomedical domain language model. arXiv preprint\narXiv:2010.06060.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2021. Flava: A founda-\ntional language and vision alignment model. arXiv\npreprint arXiv:2112.04482.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yun-\nfeng Liu. 2021. Roformer: Enhanced transformer\nwith rotary position embedding. arXiv preprint\narXiv:2104.09864.\nKuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-\nHan Wu, Yuxiao Dong, and Anshul Kanakia. 2020a.\nMicrosoft academic graph: When experts are not\nenough. Quantitative Science Studies, 1(1):396–413.\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\nRussell Reas, Jiangjiang Yang, Doug Burdick, Darrin\nEide, Kathryn Funk, Yannis Katsis, Rodney Michael\nKinney, Yunyao Li, Ziyang Liu, William Merrill,\nPaul Mooney, Dewey A. Murdick, Devvret Rishi,\nJerry Sheehan, Zhihong Shen, Brandon Stilson,\nAlex D. Wade, Kuansan Wang, Nancy Xin Ru Wang,\nChristopher Wilhelm, Boya Xie, Douglas M. Ray-\nmond, Daniel S. Weld, Oren Etzioni, and Sebastian\nKohlmeier. 2020b. CORD-19: The COVID-19 open\nresearch dataset. In Proceedings of the 1st Work-\nshop on NLP for COVID-19 at ACL 2020 , Online.\nAssociation for Computational Linguistics.\nJohannes Welbl, Nelson F Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\narXiv preprint arXiv:1707.06209.\nZheng Yuan, Yijia Liu, Chuanqi Tan, Songfang Huang,\nand Fei Huang. 2021. Improving biomedical pre-\ntrained language models with knowledge. arXiv\npreprint arXiv:2104.10344.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE in-\nternational conference on computer vision , pages\n19–27.\nA Data Descriptions\nAMiner ArnetMiner (AMiner) is a service that\ncrawls research publications, performs profile ex-\ntraction of scientists, models academic networks\nby integrating publication data from the existing li-\nbraries. For the experiments described in this work,\nwe use a sub-sampled version of the data presented\nin the Open Academic Graph (OAG) version of the\nAMiner dataset, which originally consisted of more\nthat 172M articles, with 18.5M chemistry-related\nabstracts.\nCORE COnnecting REpositories (CORE) (Pon-\ntika et al., 2016) is a large-scale aggregation sys-\ntem which provides an open access to the global\nnetwork of scientific journals and publications.\nCORE currently contains more than 207M open-\naccess articles collected from over 10 thousand data\nproviders, out of which more than 92M are open ac-\ncess full-text research papers. We sub-sampled the\noriginal collect into our chemistry-specific corpus\nconsisting of more than 7M full-text articles.\nCORD-19 CORD-19 corpus contains COVID-\n19 (Cord19) and other coronavirus-related publi-\ncations (e.g. SARS, MERS, etc.) from PubMed’s\nPMC open access corpus, bioRxiv, and medRxiv\npre-prints, in addition to COVID-19 articles main-\ntained by the World Health Organization (WHO).\nMAG Microsoft Academic Graph (MAG) is a\nheterogeneous graph created by extracting knowl-\nedge from scholarly publications on the web (Wang\net al., 2020a). The data used in this work is a sub-\nsample from the OAG version of the MAG dataset,\nwhich originally consisted of> 208M articles, with\n34M chemistry-related articles with abstracts.\nPubMed PubMed is a domain-specific data\nsource that allows for search and retrieval of the\nbiomedical and life sciences literature. It is main-\ntained by the National Centre for Biotechnology\nInformation (NCBI) at the U.S. National Library\nof Medicine (NLM). For this work we utilized the\nPubMed Central data provided in the Pile corpus\n(Gao et al., 2020). As presented in Table 3 the\nsub-sampled data consists of documents with more\nthan 280K abstracts and 700K full text articles.\nS2ORC The Semantic Scholar Open Research\nCorpus (S2ORC) (Lo et al., 2020) is a large aca-\ndemic corpus consisting of 81.1M documents. The\ndata includes the metadata, abstracts, bibliograph-\nical references and full-text publications for over\n8M open access research articles. In this work, we\nutilize the sub-sampled version of the original data\nspecific to chemistry, which includes more than\n10M abstracts.\nWoS The Web of Science (WoS) is a multi-\ndiscipline citation database produced by the In-\nstitute of Scientific Information. The platform\nhosts over 171M records across various disciplines,\nwhich, when sub-sampled for our chemistry do-\nmain, rounded to more than 7M records with ab-\nstracts available.\nB Task Descriptions\nHendrycksTest-Chemistry The Hendrycks\nTest (Hendrycks et al., 2020) is a large scale\ncollection of multiple choice questions covering 57\nsubjects. In our experiments, we subsampled col-\nlege chemistry (HT-CC) and high school chemistry\n(HT-HC). HT-CC contains 100 questions related to\nanalytical, organic, inorganic, physical, etc. and\nHT-HC contains 203 questions related chemical\nreactions, ions, acids and bases, etc.\nARC The ARC dataset (Clark et al., 2018) con-\ntains 7,787 genuine grade-school level, science\nMCQs and is partitioned into a Challenge Set\n(ARC-C) and an Easy Set (ARC-E). Additionally,\n14M science-related sentences are provided with\nrelevant knowledge to answer the ARC questions.\nSciQ The SciQ dataset (Welbl et al., 2017) con-\ntains 13,679 crowdsourced multiple-choice science\nexam questions about Physics, Chemistry and Bi-\nology, among others.\nOpenBookQA The OpenBookQA (Mihaylov\net al., 2018) dataset consists of 5,957 multiple\nchoice questions and 1,326 elementary-level sci-\nence facts. The facts alone do not contain enough\ninformation to correctly answer the multiple choice\nquestions, therefore the task is designed to evaluate\nsystems beyond paraphrase matching.\nPile PubMed Abstracts The Pile dataset (Gao\net al., 2020) contains 800GB of diverse text sources\nfor benchmarking language models. We limit\nthis task to only include abstracts from the Pile’s\nPubMed collection. As this is framed as a language\nmodeling task, we report word level perplexity.\nBoolQ BoolQ (Clark et al., 2019) is a reading\ncomprehension dataset comprised of 16k real, nat-\nurally formed queries to the Google search engine\nwith a yes or no answer. Each question-answer pair\nis accompanied by a Wikipedia article providing\nevidence to support the correct answer.\nCB Commitment Bank (CB) (De Marneffe et al.,\n2019) is a 3-way classification of textual entail-\nment (true, false, unknown) from 1,200 short text\nsegments where at least one sentence contains an\nembedded clause. The dataset contains passages\nfrom three sources: the Wall Street Journal, the\nBritish National Corpus, and Switchboard.\nWIC The Word-in-Context dataset (WIC) (Pile-\nhvar and Camacho-Collados, 2018) is a benchmark\nfor evaluating context-sensitive word embeddings.\nThe task is to classify if a target word has the same\nmeaning in two context sentence.\nWSC The Winograd Schema Challenge\n(WSC) (Levesque et al., 2012) dataset is a\ncollection of 804 sentences in which the task is to\nresolve coreferences.\nMathQA MathQA (Amini et al., 2019) is a\ndataset containing 37k multiple choice math\nword problems built from the existing dataset,\nAQuA (Ling et al., 2017).\nPIQA The Physical Interactions: Question An-\nswering (PIQA) (Bisk et al., 2020) benchmark\ndataset provides 21k questions about the physical\nworld and plausible interactions encountered by\nhumans. Annotators provided correct and incor-\nrect answers to questions extracted from instructa-\nbles.com, a website of instructions for completing\nmany everyday tasks.\nPubMedQA The PubMedQA dataset (Jin et al.,\n2019) is a collection of 273.5k biomedical re-\nsearch questions and related PubMed articles with\nyes/no/maybe answers.\nLambada Lambada (Paperno et al., 2016) con-\ntains passages and target sentences from 5,325 nov-\nels collected from Book Corpus (Zhu et al., 2015),\nand the goal is to predict the last word of the target\nsentence given the context passage. This task was\ndesigned to test genuine language understanding\nsince accurate prediction of the final word would\nbe improbable without the context passage.\nWikiText The Wikitext benchmark (Merity et al.,\n2016) is a language modeling dataset of 29k articles\nfrom Wikipedia. Only articles classified asGood or\nFeatured by Wikipedia editors are included since\nthey are considered to be well written and neutral\nin language. All results are reported on Wikitext-2.",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.6971926689147949
    },
    {
      "name": "Computer science",
      "score": 0.4827682077884674
    },
    {
      "name": "Engineering ethics",
      "score": 0.3859519958496094
    },
    {
      "name": "Data science",
      "score": 0.37710756063461304
    },
    {
      "name": "Cognitive science",
      "score": 0.3476863503456116
    },
    {
      "name": "Library science",
      "score": 0.3236249089241028
    },
    {
      "name": "Engineering",
      "score": 0.23595097661018372
    },
    {
      "name": "Psychology",
      "score": 0.21051576733589172
    },
    {
      "name": "History",
      "score": 0.09056133031845093
    },
    {
      "name": "Archaeology",
      "score": 0.07715508341789246
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I142606810",
      "name": "Pacific Northwest National Laboratory",
      "country": "US"
    }
  ]
}