{
  "title": "Making Adversarially-Trained Language Models Forget with Model Retraining: A Case Study on Hate Speech Detection",
  "url": "https://openalex.org/W4293262073",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5026101339",
      "name": "Marwan Omar",
      "affiliations": [
        "University of Central Florida"
      ]
    },
    {
      "id": "https://openalex.org/A5077402873",
      "name": "Aziz Mohaisen",
      "affiliations": [
        "University of Central Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2613977835",
    "https://openalex.org/W2971173235",
    "https://openalex.org/W2741065173",
    "https://openalex.org/W2889016530",
    "https://openalex.org/W2953646920",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W6785333560",
    "https://openalex.org/W3104208618",
    "https://openalex.org/W3094411615",
    "https://openalex.org/W2963481894",
    "https://openalex.org/W2982054702",
    "https://openalex.org/W3102015031"
  ],
  "abstract": "Adversarial training has become almost the de facto standard for robustifying Natural Language Processing models against adversarial attacks. Although adversarial training has proven to achieve accuracy gains and boost the performance of algorithms, research has not shown how adversarial training will stand \"the test of times\" when models are deployed and updated with new non-adversarial data samples. In this study, we aim to quantify the temporal impact of adversarial training on naturally-evolving language models using the hate speech task. We conduct extensive experiments on the Tweet Eval benchmark dataset using multiple hate speech classification models. In particular, our findings indicate that adversarial training is highly task-dependent as well as dataset dependent as models trained on the same dataset achieve high prediction accuracy but fare poorly when tested with new dataset even after retraining models with adversarial examples. We attribute this temporal and limited effect of adversarial training to distribution shift of the training data which implies that models' quality will degrade over-time as models are deployed in the real world and start serving new data.",
  "full_text": null,
  "topic": "Retraining",
  "concepts": [
    {
      "name": "Retraining",
      "score": 0.7250461578369141
    },
    {
      "name": "Computer science",
      "score": 0.7064884901046753
    },
    {
      "name": "Language model",
      "score": 0.5764096975326538
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46946704387664795
    },
    {
      "name": "Natural language processing",
      "score": 0.4657509922981262
    },
    {
      "name": "Speech recognition",
      "score": 0.4326494634151459
    },
    {
      "name": "Political science",
      "score": 0.07098031044006348
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I106165777",
      "name": "University of Central Florida",
      "country": "US"
    }
  ]
}