{
    "title": "Representation Degeneration Problem in Training Natural Language\\n Generation Models",
    "url": "https://openalex.org/W2966610483",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A1966621648",
            "name": "Gao Jun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098476997",
            "name": "He, Di",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2150496052",
            "name": "Tan, Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105471080",
            "name": "Qin Tao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1923131268",
            "name": "Wang Li-wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4209371238",
            "name": "Liu Tie-Yan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962964385",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964073004",
        "https://openalex.org/W2587690726",
        "https://openalex.org/W2963537482",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2963774520",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2962915600",
        "https://openalex.org/W2571859396",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2962832505",
        "https://openalex.org/W2037386840",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2739810937",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963963856",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2594990650",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2963656735",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2962883166",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2962784628"
    ],
    "abstract": "We study an interesting problem in training neural network-based models for\\nnatural language generation tasks, which we call the \\\\emph{representation\\ndegeneration problem}. We observe that when training a model for natural\\nlanguage generation tasks through likelihood maximization with the weight tying\\ntrick, especially with big training datasets, most of the learnt word\\nembeddings tend to degenerate and be distributed into a narrow cone, which\\nlargely limits the representation power of word embeddings. We analyze the\\nconditions and causes of this problem and propose a novel regularization method\\nto address it. Experiments on language modeling and machine translation show\\nthat our method can largely mitigate the representation degeneration problem\\nand achieve better performance than baseline algorithms.\\n",
    "full_text": "Published as a conference paper at ICLR 2019\nREPRESENTATION DEGENERATION PROBLEM IN\nTRAINING NATURAL LANGUAGE GENERATION MOD-\nELS\nJun Gao1,2,∗†, Di He3,∗, Xu Tan4, Tao Qin4, Liwei Wang3,5 & Tie-Yan Liu4\n1Department of Computer Science, University of Toronto\njungao@cs.toronto.edu\n2Vector Institute, Canada\n3Key Laboratory of Machine Perception, MOE, School of EECS, Peking University\ndi he@pku.edu.cn, wanglw@cis.pku.edu.cn\n4Microsoft Research\n{xuta,taoqin,tyliu}@microsoft.com\n5Center for Data Science, Peking University, Beijing Institute of Big Data Research\nABSTRACT\nWe study an interesting problem in training neural network-based models for nat-\nural language generation tasks, which we call the representation degeneration\nproblem. We observe that when training a model for natural language genera-\ntion tasks through likelihood maximization with the weight tying trick, especially\nwith big training datasets, most of the learnt word embeddings tend to degener-\nate and be distributed into a narrow cone, which largely limits the representation\npower of word embeddings. We analyze the conditions and causes of this problem\nand propose a novel regularization method to address it. Experiments on language\nmodeling and machine translation show that our method can largely mitigate the\nrepresentation degeneration problem and achieve better performance than baseline\nalgorithms.\n1 I NTRODUCTION\nNeural Network (NN)-based algorithms have made signiﬁcant progresses in natural language gen-\neration tasks, including language modeling (Kim et al., 2016; Jozefowicz et al., 2016), machine\ntranslation (Wu et al., 2016; Britz et al., 2017; Vaswani et al., 2017; Gehring et al., 2017) and dialog\nsystems (Shang et al., 2015). Despite the huge variety of applications and model architectures, natu-\nral language generation mostly relies on predicting the next word given previous contexts and other\nconditional information. A standard approach is to use a deep neural network to encode the inputs\ninto a ﬁxed-size vector referred as hidden state1, which is then multiplied by the word embedding\nmatrix (Vaswani et al., 2017; Merity et al., 2018; Yang et al., 2018; Press & Wolf, 2017). The output\nlogits are further consumed by the softmax function to give a categorical distribution of the next\nword. Then the model is trained through likelihood maximization.\nWhile studying the learnt models for language generation tasks, we observe some interesting and\nsurprising phenomena. As the word embedding matrix is tied with the softmax parameters (Vaswani\net al., 2017; Merity et al., 2018; Inan et al., 2017; Press & Wolf, 2017), it has a dual role in the\nmodel, serving as the input in the ﬁrst layer and the weights in the last layer. Given its ﬁrst role as\ninput word embedding, it should contain rich semantic information that captures the given context\nwhich will be further used for different tasks. Given its role as output softmax matrix, it should\n∗Authors contribute equally.\n†This work was done when Jun Gao was an intern at Microsoft Research Asia.\n1The concept of hidden states has multiple meanings in the literature of neural networks. In this paper, we\nuse hidden state as the input to the last softmax layer.\n1\narXiv:1907.12009v1  [cs.CL]  28 Jul 2019\nPublished as a conference paper at ICLR 2019\n(a) Vanilla Transformer\n (b) Word2Vec\n (c) Classiﬁcation\nFigure 1: 2D visualization. (a). Visualization of word embeddings trained from vanilla Trans-\nformer (Vaswani et al., 2017) in English →German translation task. (b). Visualization of word\nembeddings trained from Word2Vec (Mikolov et al., 2013). (c). Visualization of hidden states and\ncategory embedding of a classiﬁcation task, where different colors stand for different categories and\nthe blue triangles denote for category embeddings.\nhave enough capacity to classify different hidden states into correct labels. We compare it with\nword embeddings trained from Word2Vec (Mikolov et al., 2013) and the parameters in the softmax\nlayer of a classical classiﬁcation task (we refer it as categorical embedding). As shown in Figure\n1, the word embeddings learnt from Word2Vec (Figure 1(b)) and the softmax parameters learnt\nfrom the classiﬁcation task (Figure 1(c)) are diversely distributed around the origin using SVD\nprojection; in contrast, the word embeddings in our studied model (Figure 1(a)) degenerated into\na narrow cone. Furthermore, we ﬁnd the embeddings of any two words in our studied models are\npositively correlated. Such phenomena are very different from those in other tasks and deteriorate\nmodel’s capacity. As the role of the softmax layer, those parameters cannot lead to a large margin\nprediction for good generalization. As the role of word embeddings, the parameters do not have\nenough capacity to model the diverse semantics in natural languages (Yang et al., 2018; McCann\net al., 2017).\nWe call the problem described above the representation degeneration problem. In this paper, we try\nto understand why the problem happens and propose a practical solution to address it.\nWe provide some intuitive explanation and theoretical justiﬁcation for the problem. Intuitively\nspeaking, during the training process of a model with likelihood loss, for any given hidden state,\nthe embedding of the corresponding ground-truth word will be pushed towards the direction of the\nhidden state in order to get a larger likelihood, while the embeddings of all other words will be\npushed towards the negative direction of the hidden state to get a smaller likelihood. As in natural\nlanguage, word frequency is usually very low comparing to the size of a large corpus, the embedding\nof the word will be pushed towards the negative directions of most hidden states which drastically\nvary. As a result, the embeddings of most words in the vocabulary will be pushed towards similar\ndirections negatively correlated with most hidden states and thus are clustered together in a local\nregion of the embedding space.\nFrom the theoretical perspective, we ﬁrst analyze the extreme case of non-appeared words. We\nprove that the representation degeneration problem is related to the structure of hidden states: the\ndegeneration appears when the convex hull of the hidden states does not contain the origin and such\ncondition is likely to happen when training withlayer normalization (Ba et al., 2016; Vaswani et al.,\n2017; Merity et al., 2018). We further extend our study to the optimization of low-frequency words\nin a more realistic setting. We show that, under mild conditions, the low-frequency words are likely\nto be trained to be close to each other during optimization, and thus lie in a local region.\nInspired by the empirical analysis and theoretical insights, we design a novel way to mitigate the\ndegeneration problem by regularizing the word embedding matrix. As we observe that the word em-\nbeddings are restricted into a narrow cone, we try to directly increase the size of the aperture of the\ncone, which can be simply achieved by decreasing the similarity between individual word embed-\ndings. We test our method on two tasks, language modeling and machine translation. Experimental\nresults show that the representation degeneration problem is mitigated, and our algorithm achieves\nsuperior performance over the baseline algorithms, e.g., with 2.0 point perplexity improvement on\n2\nPublished as a conference paper at ICLR 2019\nthe WikiText-2 dataset for language modeling and 1.08/0.93 point BLEU improvement on WMT\n2014 English-German/German-English tasks for machine translation.\n2 R ELATED WORK\nLanguage modeling and machine translation are important language generation tasks. Language\nmodeling aims at predicting the next token given an (incomplete) sequence of words as context. A\npopular approach based on neural networks is to map the given context to a real-valued vector as a\nhidden state, and then pass the hidden state through a softmax layer to generate a distribution over\nall the candidate words. There are different choices of neural networks used in language modeling.\nRecurrent neural network-based model and convolutional neural network-based model are widely\nused (Mikolov et al., 2010; Dauphin et al., 2017).\nNeural Machine Translation (NMT) is a challenging task that has attracted lots of attention in recent\nyears. Based on the encoder-decoder framework, NMT starts to show promising results in many\nlanguage pairs. The evolving structures of NMT models in recent years have pushed the accuracy\nof NMT into a new level. The attention mechanism (Bahdanau et al., 2015) added on top of the\nencoder-decoder framework is shown to be useful to automatically ﬁnd alignment structure, and\nsingle-layer RNN-based structure has evolved towards deep models (Wu et al., 2016), more efﬁcient\nCNN models (Gehring et al., 2017), and well-designed self-attention models (Vaswani et al., 2017).\nIn this paper, we mainly study the expressiveness of word embeddings in language generation tasks.\nA trend for language generation in recent years is to share the parameters between word embeddings\nand the softmax layer, which is named as the weight tying trick. Many state-of-the-art results in\nlanguage modeling and machine translation are achieved with this trick (Vaswani et al., 2017; Merity\net al., 2018; Yang et al., 2018; Press & Wolf, 2017). Inan et al. (2017) shows that weight tying not\nonly reduces the number of parameters but also has theoretical beneﬁts.\n3 R EPRESENTATION DEGENERATION PROBLEM\nIn this section, we empirically study the word embeddings learnt from sequence generation tasks,\ni.e., machine translation, and introduce the representation degeneration problemin neural sequence\ngeneration.\n3.1 E XPERIMENTAL DESIGN\nOur analysis reported in this section is mainly based on the state-of-the-art machine translation\nmodel Transformer (Vaswani et al., 2017). We use the ofﬁcial code (Vaswani et al., 2018) and set all\nthe hyperparameters (the conﬁgurations of the base Transformer) as default. We train the model\non the WMT 2014 English-German Dataset and achieve 27.3 in terms of BLEU score. Additionally,\nwe also analyze the LSTM-based model (Wu et al., 2016) and ﬁnd the observations are similar.\nIn neural sequence generation tasks, the weights in word embeddings and softmax layer are tied.\nThose parameters can be recognized not only as the inputs in the ﬁrst layer but also as the weights\nin the last layer. Thus we compare this weight matrix with a word embedding matrix trained from\nconventional word representation learning task, and also compare it with the parameters in softmax\nlayer of a conventional classiﬁcation task. For simplicity and representative, we choose to use\nWord2Vec (Mikolov et al., 2013) to obtain word embeddings and use MNIST as the classiﬁcation\ntask trained with a two-layer convolutional neural network (Yue et al., 2018; Liu et al., 2016). For\nthe classiﬁcation task, We simply treat the row of the parameter matrix in the last softmax layer as\nthe embedding for the category, like how the word embedding is used in the softmax layer for neural\nlanguage generation model with weight tying trick.\nTo get a comprehensive understanding, we use low-rank approximation (rank = 2) for the learned\nmatrices by SVD and plot them in a 2D plane, as shown in Figure 1. We also check the singular\nvalues distribution and ﬁnd that our low-rank approximation is reasonable: other singular values are\nmuch smaller than the chosen ones.\n3\nPublished as a conference paper at ICLR 2019\n3.2 D ISCUSSION\nIn the classiﬁcation task, the category embeddings (blue triangles in Figure 1(c)) in the softmax layer\nare diversely distributed around the origin. This shows the directions of category embeddings are\ndifferent from each other and well separated, which consequently leads to large margin classiﬁcation\nresults with good generalization. For the word embeddings learnt from Word2Vec (Figure 1(b)),\nthe phenomena are similar, the embeddings are also widely distributed around the origin in the\nprojection space, which shows different words have different semantic meanings. Observations on\nGLOVE (Pennington et al., 2014) reported in Mu et al. (2018) are also consistent with ours.\nWe observe very different phenomena for machine translation. We can see from Figure 1(a) that the\nword embeddings are clustered together and only lying in a narrow cone. Furthermore, we ﬁnd the\ncosine similarities between word embeddings are positive for almost all cases. That is, the words\nhuddle together and are not well separated in the embedding space.\nClearly, as the role of word embeddings, which are the inputs to the neural networks, the word\nrepresentations should be widely distributed to represent different semantic meanings. As the role\nof softmax in the output layer, to achieve good prediction of next word in a target sentence, a more\ndiverse distribution of word embeddings in the space is expected to obtain a large margin result.\nHowever, such representations of words limit the expressiveness of the learnt model. We call such a\nproblem the representation degeneration problem.\n4 U NDERSTANDING THE PROBLEM\nWe show in the previous section that in training natural language generation models, the learnt\nword embeddings are clustered into a narrow cone and the model faces the challenge of limited\nexpressiveness. In this section, we try to understand the reason of the problem and show that it is\nrelated to the optimization of low-frequency words with diverse contexts.\nIn natural language generation tasks, the vocabulary is usually of large size and the words are of low\nfrequencies according to Zipf’s law. For example, more than 90% of words’ frequencies are lower\nthan 10e-4 in WMT 2014 English-German dataset. Note that even for a popular word, its frequency\nis also relatively low. For a concrete example, the frequency of the word “is” is only about 1% in the\ndataset, since “is” occurs at most once in most simple sentences. Our analysis is mainly based on the\noptimization pattern of the low-frequency words which occupy the major proportion of vocabulary.\nThe generation of a sequence of words (or word indexes)y= (y1,··· ,yM) is equivalent to generate\nthe words one by one from left to right. The probability of generatingycan be factorized as P(Y =\ny) = Π tP(Yt = yt|Y<t = y<t), where y<t denotes for the ﬁrst t−1 words in y. Sometimes,\nthe generative model also depends on other context, e.g., the context from the source sentence for\nmachine translation. To study the optimization of the word embeddings, we simply consider the\ngeneration of a word as a multi-class classiﬁcation problem and formally describe the optimization\nas follows.\nConsider a multi-class classiﬁcation problem with M samples. Let hi denote the hidden state before\nthe softmax layer which can be also considered as the input features, i = 1,··· ,M. Without any\nloss of generality, we assume hi is not a zero vector for all i. Let N denote the vocabulary size. The\nconditional probability of yi ∈{1,··· ,N}is calculated by the softmax function: P(Yi = yi|hi) =\nexp(⟨hi,wyi⟩)∑N\nl=1 exp(⟨hi,wl⟩) , where wl is the embedding for word/category l, l= 1,2,··· ,N.\n4.1 E XTREME CASE : N ON-APPEARED WORD TOKENS\nNote that in most NLP tasks, the frequencies of rare words are rather low, while the number of them\nis relatively large. With stochastic training paradigm, the probabilities to sample a certain infrequent\nword in a mini-batch are very low, and thus during optimization, the rare words behave similarly to\na non-appeared word. We ﬁrst consider the extreme case of non-appeared word in this section and\nextend to a more general and realistic scenario in the next section.\nWe assume yi ̸= N for all i. That is, the N-th word with embedding wN does not appear in the\ncorpus, which is the extreme case of a low-frequency rare word. We focus on the optimization\n4\nPublished as a conference paper at ICLR 2019\nprocess of wN and assume all other parameters are ﬁxed and well-optimized. By log-likelihood\nmaximization, we have\nmax\nwN\n1\nM\nM∑\ni=1\nlog exp(⟨hi,wyi⟩)∑N\nl=1 exp(⟨hi,wl⟩)\n. (1)\nAs all other parameters are ﬁxed, this is equivalent to\nmin\nwN\n1\nM\nM∑\ni=1\nlog(exp(⟨hi,wN⟩) + Ci), (2)\nwhere Ci = ∑N−1\nl=1 exp(⟨hi,wl⟩) and can be considered as some constants.\nDeﬁnition 1. We say that vectorvis a uniformly negative direction ofhi, i= 1,··· ,M, if ⟨v,hi⟩<\n0 for all i.\nThe following theorem provides a sufﬁcient condition for the embedding wN approaching un-\nbounded during optimization. We leave the proof of all the theorems in the appendix.\nTheorem 1. A. If the set of uniformly negative direction is not empty, it is convex. B. If there exists\na vthat is a uniformly negative direction of hi, i = 1,··· ,M, then the optimal solution of Eqn. 2\nsatisﬁes ∥w∗\nN ∥= ∞and can be achieved by setting w∗\nN = limk→+∞k·v.\nFrom the above theorem, we can see that if there exists a set of uniformly negative direction, the\nembedding wN can be optimized along any uniformly negative direction to inﬁnity. As the set of\nuniformly negative direction is convex, wN is likely to lie in a convex cone and move to inﬁnity\nduring optimization. Next, we provide a sufﬁcient and necessary condition for the existence of the\nuniformly negative direction.\nTheorem 2. There exists a vthat is a uniformly negative direction of a set of hidden states, if and\nonly if the convex hull of the hidden states does not contain the origin.\nDiscussion on whether the condition happens in real practice From the theorem, we can see that\nthe existence of the uniformly negative direction is highly related to the structure of the hidden states,\nand then affects the optimization of word embeddings. Note that a common trick used in sequence\ngeneration tasks is layer normalization (Ba et al., 2016; Vaswani et al., 2017; Merity et al., 2018),\nwhich ﬁrst normalizes each hidden state vector into standard vector2, and then rescales/translates the\nstandard vector with scaling/bias term. In the appendix, we show that under very mild conditions,\nthe space of hidden states doesn’t contain the origin almost for sure in practice.\n4.2 E XTENSION TO RARELY APPEARED WORD TOKENS\nIn the previous section, we show that under reasonable assumptions, the embeddings of all non-\nappeared word tokens will move together along the uniformly negative directions to inﬁnity. How-\never, it is not realistic that there exist non-appeared word tokens and the weights of embedding can\nbe trained to be unbounded with L2 regularization term. In this section, we extend our analysis\nto a more realistic setting. The key we want to show is that the optimization of a rarely appeared\nword token is similar to that of non-appeared word tokens. Following the notations in the previous\nsection, we denote the embedding of a rare word as wN and ﬁx all other parameters. We study the\noptimization for this particular token with the negative log-likelihood loss function.\nTo clearly characterize the inﬂuence of wN to the loss function, we divide the loss function into\ntwo pieces. Piece AwN contains the sentences that do not contain wN, i.e., all hidden states in such\nsentences are independently of wN and the ground truth label of each hidden state (denoted as w∗\nh)\nis not wN. Then wN can be considered as a “non-appeared token” in this set. Denote the space\nof the hidden state in piece AwN as HAwN\nwith probability distribution PAwN\n, where HAwN\nand\nPAwN\ncan be continuous. The loss function on piece AwN can be deﬁned as\nLAwN\n(wN) = −\n∫\nHAwN\nlog exp(⟨h,w∗\nh⟩)∑N\nl=1 exp(⟨h,wl⟩)\ndPAwN\n(h), (3)\n2the mean and variance of the values of the vector are normalized to be 0 and 1 respectively\n5\nPublished as a conference paper at ICLR 2019\nwhich is a generalized version of Eqn. 1. Piece BwN contains the sentences which contain wN, i.e.,\nwN appears in every sentence in piece BwN . Then in piece BwN , in some cases the hidden states\nare computed based on wN, e.g., when the hidden state is used to predict the next token after wN.\nIn some other cases, the hidden states are used to predict wN. Denote the space of the hidden state\nin piece BwN as HBwN\nwith probability distribution PBwN\n. The loss function on piece BwN can be\ndeﬁned as\nLBwN\n(wN) = −\n∫\nHBwN\nlog exp(⟨h,w∗\nh⟩)∑N\nl=1 exp(⟨h,wl⟩)\ndPBwN\n(h), (4)\nBased on these notations. the overall loss function is deﬁned as\nL(wN) = P(sentence sin AwN )LAwN\n(wN) + P(sentence sin BwN )LBwN\n(wN). (5)\nThe loss on piece AwN is convex while the loss on piece BwN is complicated and usually non-\nconvex with respect to wN. The general idea is that given the fact that LAwN\n(wN) is a convex\nfunction with nice theoretical properties, if P(sentence sin AwN ) is large enough, e.g., larger than\n1 −ϵ, and LBwN\n(wN) is a bounded-smooth function. The optimal solution of L(wN) is close to the\noptimal solution of LAwN\n(wN) which can be unique if LAwN\n(wN) is strongly-convex. A formal\ndescription is as below.\nTheorem 3. Given an α-strongly convex functionf(x) and a function g(x) that satisﬁes its Hessian\nmatrix H(g(x)) ≻ −βI, where I is the identity matrix, and |g(x)| < B. For a given ϵ > 0,\nlet x∗and x∗\nϵ be the optimum of f(x) and (1 −ϵ)f(x) + ϵg(x), respectively. If ϵ < α\nα+β, then\n∥x∗−x∗\nϵ ∥2\n2≤ 4ϵB\nα−ϵ(α+β) .\nWe make some further discussions regarding the theoretical results. In natural language generation,\nfor two low-frequency wordswand w′, piece Aw and Aw′ has large overlaps. Then the lossLAw (w)\nand LAw′ (w′) are similar with close optimum. According to the discussion above, the learnt word\nembedding of wand w′is likely to be close to each other, which is also observed from the empirical\nstudies.\n5 A DDRESSING THE PROBLEM\nIn this section, we propose an algorithm to address the representation degeneration problem. As\nshown in the previous study, the learnt word embeddings are distributed in a narrow cone in the Eu-\nclid space which restricts the expressiveness of the representation. Then a straightforward approach\nis to improve the aperture of the cone which is deﬁned as the maximum angle between any two\nboundaries of the cone. For the ease of optimization, we minimize the cosine similarities between\nany two word embeddings to increase the expressiveness.\nFor simplicity, we denote the normalized direction ofwas ˆw, ˆw= w\n∥w∥. Then our goal is to minimize∑\ni\n∑\nj̸=i ˆwi\nT ˆwj as well as the original log-likelihood loss function. By introducing hyperparameter\nγto trade off the log-likelihood loss and regularization term, the overall objective is,\nL = LMLE + γ 1\nN2\nN∑\ni\nN∑\nj̸=i\nˆwi\nT ˆwj. (6)\nWe call this new loss as MLE with Cosine Regularization (MLE-CosReg). In the following, we\nmake some analysis about the regularization term.\nDenote R = ∑N\ni\n∑N\nj̸=i ˆwi\nT ˆwj, and denote the normalized word embedding matrix as ˆW =\n[ ˆw1, ˆw2,..., ˆwN]T. It is readily to check the regularizer has the matrix form as R =∑N\ni\n∑N\nj̸=i ˆwi\nT ˆwj = ∑N\ni\n∑N\nj ˆwi\nT ˆwj −∑N\ni ∥ ˆw ∥2= Sum( ˆW ˆWT) −N, where the Sum (·)\noperator calculates the sum of all elements in the matrix. Since N is a constant, it sufﬁces to con-\nsider the ﬁrst term Sum( ˆW ˆWT) only.\nSince ˆW ˆWT is a positive semi-deﬁnite matrix, all its eigenvalues are nonnegative. Since ˆwi is a\nnormalized vector, every diagonal element of ˆW ˆWT is 1. Then the trace of ˆW ˆWT, which is also\nthe sum of the eigenvalues, equals to N.\n6\nPublished as a conference paper at ICLR 2019\nTable 1: Experimental result on language modeling (perplexity). Bold numbers denote for the best\nresult.\nModel Parameters Validation Test\n2-layer skip connection LSTM (Mandt et al., 2017) (tied) 24M 69.1 65.9\nAWD-LSTM (Merity et al., 2018) (w.o. ﬁnetune) 24M 69.1 66.0\nAWD-LSTM (Merity et al., 2018) (w.t. ﬁnetune) 24M 68.6 65.8\nAWD-LSTM (Merity et al., 2018) + continuous cache pointer 24M 53.8 52.0\nMLE-CosReg (w.o. ﬁnetune) 24M 68.2 65.2\nMLE-CosReg (w.t. ﬁnetune) 24M 67.1 64.1\nMLE-CosReg + continuous cache pointer 24M 51.7 50.0\nTable 2: Experimental results on WMT English →German and German →English translation.\nBold numbers denote for our results and ‡denotes for our implementation.\nEnglish→German German→English\nModel BLEU Model BLEU\nConvS2S (Gehring et al., 2017) 25.16 DSL (Xia et al., 2017b) 20.81\nBaseTransformer (Vaswani et al., 2017) 27.30 Dual-NMT (Xia et al., 2017a) 22.14\nBaseTransformer + MLE-CosReg 28.38 ConvS2S‡(Gehring et al., 2017) 29.61\nBig Transformer (Vaswani et al., 2017) 28.40 BaseTransformer‡(Vaswani et al., 2017) 31.00\nBig Transformer + MLE-CosReg 28.94 BaseTransformer + MLE-CosReg 31.93\nAs the cosine similarities between the word embeddings are all positive. According to Theorem 4,\nwhen ˆW ˆWT is a positive matrix, we have the largest absolute eigenvalue ofˆW ˆWT is upper bounded\nby Sum( ˆW ˆWT). Then minimizing Ris equivalent to minimize the upper bound of the largest eigen-\nvalue. As the sum of all eigenvalues is a constant, the side effect is to increase other eigenvalues,\nconsequently improving the expressiveness of the embedding matrix.\nTheorem 4. (Merikoski, 1984) For any matrix A, which is a real and nonnegative n×nmatrix.\nThe spectral radius, which is the largest absolute value of A’s eigenvalues, is less than or equals to\nSum(A).\n6 E XPERIMENTS\nWe conduct experiments on two basic natural language generation tasks: language modeling and\nmachine translation, and report the results in this section.\n6.1 E XPERIMENTAL SETTINGS\n6.1.1 L ANGUAGE MODELING\nLanguage modeling is one of the fundamental tasks in natural language processing. The goal is to\npredict the probability of the next word conditioned on previous words. The evaluation metric is per-\nplexity. Smaller the perplexity, better the performance. We used WikiText-2 (WT2) corpus, which\nis popularly used in many previous works (Merity et al., 2017; Inan et al., 2017; Grave et al., 2017).\nWikiText-2 is sourced from curated Wikipedia articles and contains approximately a vocabulary of\nover 30,000 words. All the text has been tokenized and processed with the Moses tokenizer (Koehn\net al., 2006). Capitalization, punctuation and numbers are retained in this dataset.\nAWD-LSTM (Merity et al., 2018) is the state-of-the-art model for language modeling. We directly\nfollowed the experimental settings as in Merity et al. (2018) to set up the model architecture and\n7\nPublished as a conference paper at ICLR 2019\n(a) MLE-CosReg\n (b) Singular Values\nFigure 2: (a): Word embeddings trained from MLE-CosReg. (b): Singular values of embedding\nmatrix. We normalize the singular values of each matrix so that the largest one is 1.\nhyperparameter conﬁgurations. We used a three-layer LSTM with 1150 units in the hidden layer\nand set the size of embedding to be 400. The ratio for dropout connection on recurrent weight is\nkept the same as Merity et al. (2018). We trained the model with Averaged Stochastic Gradient\nDescent. Our implementation was based on open-sourced code 3 by Merity et al. (2018). For our\nproposed MLE-CosReg loss, we found the hyperparameter γis not very sensitive and we set it to 1\nin the experiments. We added neural cache model (Grave et al., 2017) to further reduce perplexity.\n6.1.2 M ACHINE TRANSLATION\nMachine Translation aims at mapping sentences from a source domain to a target domain. We\nfocus on English →German and German →English in our experiments. We used the dataset from\nstandard WMT 2014, which consists of 4.5 million English-German sentence pairs and has been\nwidely used as the benchmark for neural machine translation (Vaswani et al., 2017; Gehring et al.,\n2017). Sentences were encoded using byte-pair encoding (Sennrich et al., 2016), after which we got\na shared source-target vocabulary with about 37000 subword units. We measured the performance\nwith tokenized case-sensitive BLEU (Papineni et al., 2002).\nWe used state-of-the-art machine translation model Transformer (Vaswani et al., 2017), which uti-\nlizes self-attention mechanism for machine translation. We followed the setting in Vaswani et al.\n(2017) and used the ofﬁcial code (Vaswani et al., 2018) from Transformer. For both English →\nGerman and German →English tasks, we used the base version of Transformer (Vaswani et al.,\n2017), which has a 6-layer encoder and 6-layer decoder, the size of hidden nodes and embedding\nare set to 512. For English →German task, we additionally run an experiment on the big version\nof Transformer, which has 3x parameters compared with the base variant. All the models were\ntrained with Adam optimizer, and all the hyperparameters were set as default as in Vaswani et al.\n(2017). γis set to 1 as in the experiments of language modeling.\n6.2 E XPERIMENTAL RESULTS\nWe present experimental results for language modeling in Table 1 and machine translation in Table 2.\nFor language modeling, we compare our method with vanilla AWD-LSTM (Merity et al., 2018) in\nthree different settings, without ﬁnetune, with ﬁnetune and with further continuous cache pointer.\nOur method outperforms it with 0.8/1.7/2.0 improvements in terms of test perplexity. For machine\ntranslation, comparing with originalbase Transformer (Vaswani et al., 2017), our method improves\nperformance with 1.08/0.93 for the English →German and German →English tasks, respectively,\nand achieves 0.54 improvement on the big Transformer.\nNote that for all tasks, we only add one regularization term to the loss function, while no additional\nparameters or architecture/hyperparameters modiﬁcations are applied. Therefore, the accuracy im-\nprovements purely come from our proposed method. This demonstrates that by regularizing the\nsimilarity between word embeddings, our proposed MLE-CosReg loss leads to better performance.\n3https://github.com/salesforce/awd-lstm-lm\n8\nPublished as a conference paper at ICLR 2019\n6.3 D ISCUSSION\nThe study above demonstrates the effectiveness of our proposed method in terms of ﬁnal accuracy.\nHowever, it is still unclear whether our method has improved the representation power of learnt\nword embeddings. In this subsection, we provide a comprehensive study on the expressiveness of\nthe model learnt by our algorithm.\nFor a fair comparison with the empirical study in Section 3, we analyze the word embeddings of our\nmodel on English →German translation task. We project the word embeddings into 2-dimensional\nspace using SVD for visualization. Figure 2(a) shows that the learnt word embeddings are somewhat\nuniformly distributed around the origin and not strictly in a narrow cone like in Figure 1(a). This\nshows that our proposed regularization term effectively expands word embedding space. We also\ncompare the singular values for word embedding matrix of our learnt model and the baseline model,\nas shown in the Figure 2(b). According to the ﬁgure, trained with vanilla Transformer, only a\nfew singular values dominate among all singular values, while trained with our proposed method,\nthe singular values distribute more uniformly. Again, this demonstrates the diversity of the word\nembeddings learnt by our method.\n7 C ONCLUSION AND FUTURE WORK\nIn this work, we described and analyzed the representation degeneration problem in training neu-\nral network models for natural language generation tasks both empirically and theoretically. We\nproposed a novel regularization method to increase the representation power of word embeddings\nexplicitly. Experiments on language modeling and machine translation demonstrated the effective-\nness of our method.\nIn the future, we will apply our method to more language generation tasks. Our proposed reg-\nularization term is based on cosine similarity. There may exist some better regularization terms.\nFurthermore, it is interesting to combine with other approaches, e.g. (Gong et al., 2018), to enrich\nthe representation of word embeddings.\n8 A CKNOWLEDGEMENTS\nThis work was partially supported by National Basic Research Program of China (973 Program)\n(grant no. 2015CB352502), NSFC (61573026) and BJNSF (L172037). We would like to thank\nZhuohan Li and Chengyue Gong for helpful discussions, and the anonymous reviewers for their\nvaluable comments on our paper.\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of the International Conference on Learning Rep-\nresentations, 2015.\nDenny Britz, Anna Goldie, Minh-Thang Luong, and Quoc Le. Massive exploration of neural ma-\nchine translation architectures. In Proceedings of the 2017 Conference on Empirical Methods in\nNatural Language Processing, pp. 1442–1451. Association for Computational Linguistics, 2017.\ndoi: 10.18653/v1/D17-1151.\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated\nconvolutional networks. In International Conference on Machine Learning, pp. 933–941, 2017.\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional\nsequence to sequence learning. In International Conference on Machine Learning , pp. 1243–\n1252, 2017.\n9\nPublished as a conference paper at ICLR 2019\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic\nword representation. In Advances in Neural Information Processing Systems , pp. 1341–1352,\n2018.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a\ncontinuous cache. In Proceedings of the International Conference on Learning Representations,\n2017.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. In Proceedings of the International Conference on\nLearning Representations, 2017.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language\nmodels. In Thirtieth AAAI Conference on Artiﬁcial Intelligence, pp. 2741–2749, 2016.\nPhilipp Koehn, Marcello Federico, Wade Shen, Nicola Bertoldi, Ondrej Bojar, Chris Callison-Burch,\nBrooke Cowan, Chris Dyer, Hieu Hoang, Richard Zens, et al. Open source toolkit for statistical\nmachine translation: Factored translation models and confusion network decoding. In Final Re-\nport of the 2006 JHU Summer Workshop, 2006.\nWeiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolu-\ntional neural networks. In International Conference on Machine Learning, pp. 507–516, 2016.\nStephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate\nbayesian inference. The Journal of Machine Learning Research, 18(1):4873–4907, 2017.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:\nContextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6297–\n6308, 2017.\nJorma Kaarlo Merikoski. On the trace and the sum of elements of a matrix. Linear algebra and its\napplications, 60:177–185, 1984.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. In Proceedings of the International Conference on Learning Representations, 2017.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing lstm lan-\nguage models. In Proceedings of the International Conference on Learning Representations ,\n2018.\nTom´aˇs Mikolov, Martin Karaﬁ´at, Luk´aˇs Burget, Jan ˇCernock`y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. InEleventh Annual Conference of the International Speech\nCommunication Association, 2010.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in Neural Information\nProcessing Systems, pp. 3111–3119, 2013.\nJiaqi Mu, Suma Bhat, and Pramod Viswanath. All-but-the-top: simple and effective postprocessing\nfor word representations. In Proceedings of the International Conference on Learning Represen-\ntations, 2018.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\ncomputational linguistics, pp. 311–318. Association for Computational Linguistics, 2002.\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), pp. 1532–1543, 2014.\n10\nPublished as a conference paper at ICLR 2019\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. InProceedings\nof the 15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pp. 157–163. Association for Computational Linguistics, 2017. URL\nhttp://aclweb.org/anthology/E17-2025.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 1715–1725. Association for Computational Linguistics,\n2016.\nLifeng Shang, Zhengdong Lu, and Hang Li. Neural responding machine for short-text conversation.\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\nvolume 1, pp. 1577–1586, 2015.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 6000–6010, 2017.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan\nGouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam\nShazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR,\nabs/1803.07416, 2018. URL http://arxiv.org/abs/1803.07416.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\nlation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\nYingce Xia, Jiang Bian, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Dual inference for machine learning.\nIn Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence , pp.\n3112–3118, 2017a.\nYingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu. Dual supervised learning.\nIn International Conference on Machine Learning, pp. 3789–3798, 2017b.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax\nbottleneck: a high-rank rnn language model. In Proceedings of the International Conference on\nLearning Representations, 2018.\nZhao Yue, Zhao Deli, Wan Shaohua, and Zhang Bo. Softmax supervision with isotropic normaliza-\ntion, 2018. URL https://openreview.net/forum?id=SyXNErg0W.\n11\nPublished as a conference paper at ICLR 2019\nAPPENDICES\nA P ROOFS\nIn this section, we add proofs of all the theorems in the main sections.\nTheorem 1. A. If the set of uniformly negative direction is not empty, it is convex. B. If there exists\na vthat is a uniformly negative direction of hi, i = 1,··· ,M, then the optimal solution of Eqn. 2\nsatisﬁes ∥w∗\nN ∥= ∞and can be achieved by setting w∗\nN = limk→+∞k·v.\nProof. The ﬁrst part is straight forward and we just prove the second part. De-\nnote v as any uniformly negative direction, and k as any positive value. We have\nlimk→+∞\n∑M\ni=1 log(exp(⟨hi,k ·v⟩) + Ci) = ∑M\ni=1 log(Ci). As ∑M\ni=1 log(Ci) is the lower bound\nof the objective function in Eqn. 2 and the objective function is convex, we havew∗= limk→∞k·v\nis local optimum and also the global optimum. Note that the lower bound can be achieved only if\n⟨hi,wN⟩approaches negative inﬁnity for all i. Then it is easy to check for any optimal solution\n∥w∗\nN ∥= ∞.\nTheorem 2. There exists a vthat is a uniformly negative direction of a set of hidden states, if and\nonly if the convex hull of the hidden states does not contain the origin.\nProof. We ﬁrst prove the necessary condition by contradiction. Suppose there areMhidden states in\nthe set. If the convex hull of hi, i= 1,··· ,M, contains the origin and there exists a uniformly neg-\native direction (e.g. v). Then from the deﬁnition of convex hull, there exists αi, i= 1,··· ,M, such\nthat ∑M\ni αihi = 0, αi ≥0 and ∑\niαi = 1. Multiplying von both sides, we have∑\niαi⟨hi,v⟩= 0,\nwhich contradicts with ⟨hi,v⟩<0 for all i.\nFor the sufﬁcient part, if the convex hull of hi, i = 1 ,··· ,M, does not contain the origin, there\nexists at least one hyperplane H that passes through the origin and does not cross the convex hull.\nThen it is easy to check that a normal direction of the half space derived by the H is a uniformly\nnegative direction. The theorem follows.\nTheorem 3. Given an α-strongly convex functionf(x) and a function g(x) that satisﬁes its Hessian\nmatrix H(g(x)) ≻ −βI, where I is the identity matrix, and |g(x)| < B. For a given ϵ > 0,\nlet x∗and x∗\nϵ be the optimum of f(x) and (1 −ϵ)f(x) + ϵg(x), respectively. If ϵ < α\nα+β, then\n∥x∗−x∗\nϵ ∥2\n2≤ 4ϵB\nα−ϵ(α+β) .\nProof. We ﬁrst prove the function (1 −ϵ)f(x) + ϵg(x) is α−ϵ(α+ β)-strongly convex.\nLet’s consider the Hessian matrix of it. As f(x) is α-strongly convex and H(g(x)) ≻−βI, using\nthe deﬁnition of positive-deﬁnite matrix, the following inequality holds:\n∀v,vT(H(g) + βI)v >0; (7)\nvT(H(f) −αI)v >0. (8)\nTo make it clear, we omitxhere. Then for the Hessian matrix of (1 −ϵ)f(x) + ϵg(x), we have:\n∀v, v T(H((1 −ϵ)f + ϵg) −(α−ϵ(α+ β))I)v (9)\n= vT(H((1 −ϵ)f) + H(ϵg) −(1 −ϵ)αI+ ϵβI)v (10)\n= vT((1 −ϵ)H(f) + ϵH(g) −(1 −ϵ)αI+ ϵβI)v (11)\n= vT((1 −ϵ)(H(f) −αI) + ϵ(H(g) + ϵβI))v (12)\n= vT((1 −ϵ)(H(f) −αI))v+ vT(ϵ(H(g) + βI))v (13)\n= (1 −ϵ)vT(H(f) −αI)v+ ϵvT(H(g) + βI)v (14)\n> (1 −ϵ)0 + ϵ0 (15)\n= 0 . (16)\n12\nPublished as a conference paper at ICLR 2019\nThus, H((1−ϵ)f(x)+ϵg(x))−(α−ϵ(α+β))Iis positive-deﬁnite, which means(1−ϵ)f(x)+ϵg(x)\nis α−ϵ(α+ β)-strongly convex. Then, with the properties in strong convexity, we have:\n∥x∗−x∗\nϵ ∥2\n2 ≤ 2\nα−ϵ(α+ β)(f(x∗) + ϵg(x∗) −f(x∗\nϵ) −ϵg(x∗\nϵ)) (17)\n≤ 2\nα−ϵ(α+ β)(ϵg(x∗) −ϵg(x∗\nϵ)) (18)\n≤ 4ϵB\nα−ϵ(α+ β). (19)\nB C OMPUTATION OF THE COSINE REGULARIZATION\nIn this section, we provide analysis of the computational cost of the proposed regularization term.\nProposition 1. The cosine regularization in Eqn. 6 can be computed in Θ(N) time where N is the\nsize of vocabulary.\nProof. The regularization term can be simpliﬁed as below:\nN∑\ni\nN∑\nj̸=i\nˆwT\ni ˆwj = (\nN∑\ni\nˆwi)T(\nN∑\nj\nˆwj) −\nN∑\ni\nˆwT\ni ˆwi (20)\n= (\nN∑\ni\nˆwi)T(\nN∑\ni\nˆwi) −N (21)\n= ||\nN∑\ni\nˆwi||2\n2 −N. (22)\nFrom the above equations, we can see that we only need to compute the sum of all the normalized\nword embedding vectors, and thus the computational time is linear with respect to the vocabulary\nsize N.\nC D ISCUSSION ON LAYER NORMALIZATION\nIn this section, we show the condition on the existence of the uniformly negative direction holds\nalmost for sure in practice with models where layer normalization is applied before last layer.\nLet h1,h2,··· ,hnbe nvectors in RdEuclidean space. Let − →1 /− →0 be the d-dimensional vector ﬁlled\nwith ones/zeros respectively. By applying layer normalization we have:\nµi = 1\nd\n− →1 Thi, (23)\nσ2\ni = 1\nd ∥hi −− →1 µi ∥2\n2, (24)\nh′\ni = g ⊙hi −− →1 µi\nσi\n+ b, (25)\nwhere µi and σ2\ni are the mean and variance of entries in vector hi, g and b are learnt scale/bias\nvectors and ⊙denotes the element-wise multiplication. Here we simply assume that σ2\ni and each\nentry in g and b are not zero (since the exact zero value can be hardly observed using gradient\noptimization in real vector space). If the convex hull of h′\n1,h′\n2,··· ,h′\nn contains the origin, then\nthere exist λ1,λ2,··· ,λn, such that:\nn∑\ni=1\nλih′\ni = − →0 ;\nn∑\ni=1\nλi = 1; λi ≥0,∀i= 1,2,··· ,n. (26)\n13\nPublished as a conference paper at ICLR 2019\nBy combining 25 and 26 we have:\nn∑\ni=1\nλih′\ni =\nn∑\ni=1\nλi(g ⊙hi −− →1 µi\nσi\n+ b) (27)\n= b +\nn∑\ni=1\nλi(g ⊙hi −− →1 µi\nσi\n) (28)\n= b + g ⊙\nn∑\ni=1\nλi\nhi −− →1 µi\nσi\n= − →0 . (29)\nDenote b\ng = ( b1\ng1\n,b2\ng2\n,··· ,bd\ngd\n), thus we have ∑n\ni=1 λi\nhi−− →1 µi\nσi\n= −b\ng . Since − →1 T hi−− →1 µi\nσi\n= 0 for\nall i, there exist λ1,λ2,··· ,λn that satisfy Eqn. 26 only if − →1 T b\ng = −− →1 T ∑n\ni=1 λi\nhi−− →1 µi\nσi\n=\n−∑n\ni=1 λi\n− →1 T hi−− →1 µi\nσi\n= 0,which can hardly be guaranteed using current unconstrained optimiza-\ntion. We also empirically verify this.\nD (S UB)WORD FREQUENCY DISTRIBUTION ON WMT2014\nENGLISH -GERMAN AND WIKI TEXT 2 DATASETS\nIn this section, we show the (sub)word frequency distribution on the datasets we use in experiments\nin the Figure 3.\n0 5000 10000 15000 20000 25000\nWord Index\n7.0\n6.5\n6.0\n5.5\n5.0\n4.5\n4.0\n3.5\n3.0\nLog10 Word Frequency\n(a) BPE in WMT 2014 En-De\n0 5000 10000 15000 20000 25000\nWord Index\n7.0\n6.5\n6.0\n5.5\n5.0\n4.5\n4.0\n3.5\n3.0\nLog10 Word Frequency (b) WikiText-2\nFigure 3: (a): WMT 2014 English-German Dataset preprocessed with BPE. (b): word-level\nWikiText-2. In the two ﬁgures, the x-axis is the token ranked with respect to its frequency in de-\nscending order. The y-axis is the logarithmic value of the token frequency.\n14"
}