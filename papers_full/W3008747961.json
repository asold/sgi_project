{
  "title": "The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction",
  "url": "https://openalex.org/W3008747961",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4222987418",
      "name": "Stahlberg, Felix",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2311607323",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W1582482241",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W1916559533",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "With the advent of deep learning, research in many areas of machine learning is converging towards the same set of methods and models. For example, long short-term memory networks are not only popular for various tasks in natural language processing (NLP) such as speech recognition, machine translation, handwriting recognition, syntactic parsing, etc., but they are also applicable to seemingly unrelated fields such as robot control, time series prediction, and bioinformatics. Recent advances in contextual word embeddings like BERT boast with achieving state-of-the-art results on 11 NLP tasks with the same model. Before deep learning, a speech recognizer and a syntactic parser used to have little in common as systems were much more tailored towards the task at hand. At the core of this development is the tendency to view each task as yet another data mapping problem, neglecting the particular characteristics and (soft) requirements tasks often have in practice. This often goes along with a sharp break of deep learning methods with previous research in the specific area. This work can be understood as an antithesis to this paradigm. We show how traditional symbolic statistical machine translation models can still improve neural machine translation (NMT) while reducing the risk for common pathologies of NMT such as hallucinations and neologisms. Other external symbolic models such as spell checkers and morphology databases help neural grammatical error correction. We also focus on language models that often do not play a role in vanilla end-to-end approaches and apply them in different ways to word reordering, grammatical error correction, low-resource NMT, and document-level NMT. Finally, we demonstrate the benefit of hierarchical models in sequence-to-sequence prediction. Hand-engineered covering grammars are effective in preventing catastrophic errors in neural text normalization systems. Our operation sequence model for interpretable NMT represents translation as a series of actions that modify the translation state, and can also be seen as derivation in a formal grammar.",
  "full_text": null,
  "topic": "Sequence (biology)",
  "concepts": [
    {
      "name": "Sequence (biology)",
      "score": 0.7603561878204346
    },
    {
      "name": "Computer science",
      "score": 0.4698328673839569
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43285900354385376
    },
    {
      "name": "Natural language processing",
      "score": 0.3682492971420288
    },
    {
      "name": "Biology",
      "score": 0.18454498052597046
    },
    {
      "name": "Genetics",
      "score": 0.0783819854259491
    }
  ],
  "institutions": [],
  "cited_by": 5
}