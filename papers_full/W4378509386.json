{
  "title": "WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia",
  "url": "https://openalex.org/W4378509386",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4227039450",
      "name": "Semnani, Sina J.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378510878",
      "name": "Yao, Violet Z.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4378510879",
      "name": "Zhang, Heidi C.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227039447",
      "name": "Lam, Monica S.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2898875342",
    "https://openalex.org/W4389010541",
    "https://openalex.org/W4385572752",
    "https://openalex.org/W4385570481",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4296711106",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4385574338",
    "https://openalex.org/W3194769714",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4385571680",
    "https://openalex.org/W4385572901",
    "https://openalex.org/W4226069413",
    "https://openalex.org/W3034828027",
    "https://openalex.org/W4385571271",
    "https://openalex.org/W4288113479",
    "https://openalex.org/W4226112939",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W3100258764",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4282980384",
    "https://openalex.org/W3153947101",
    "https://openalex.org/W4389520806",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W3207604732",
    "https://openalex.org/W4389519118",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4361807267",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4385572601",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W3027879771"
  ],
  "abstract": "This paper presents the first few-shot LLM-based chatbot that almost never hallucinates and has high conversationality and low latency. WikiChat is grounded on the English Wikipedia, the largest curated free-text corpus. WikiChat generates a response from an LLM, retains only the grounded facts, and combines them with additional information it retrieves from the corpus to form factual and engaging responses. We distill WikiChat based on GPT-4 into a 7B-parameter LLaMA model with minimal loss of quality, to significantly improve its latency, cost and privacy, and facilitate research and deployment. Using a novel hybrid human-and-LLM evaluation methodology, we show that our best system achieves 97.3% factual accuracy in simulated conversations. It significantly outperforms all retrieval-based and LLM-based baselines, and by 3.9%, 38.6% and 51.0% on head, tail and recent knowledge compared to GPT-4. Compared to previous state-of-the-art retrieval-based chatbots, WikiChat is also significantly more informative and engaging, just like an LLM. WikiChat achieves 97.9% factual accuracy in conversations with human users about recent topics, 55.0% better than GPT-4, while receiving significantly higher user ratings and more favorable comments.",
  "full_text": "WikiChat: Stopping the Hallucination of Large Language Model Chatbots\nby Few-Shot Grounding on Wikipedia\nSina J. Semnani Violet Z. Yao ∗ Heidi C. Zhang∗ Monica S. Lam\nComputer Science Department\nStanford University\nStanford, CA\n{sinaj, vyao, chenyuz, lam}@cs.stanford.edu\nAbstract\nThis paper presents the first few-shot LLM-\nbased chatbot that almost never hallucinates\nand has high conversationality and low la-\ntency. WikiChat is grounded on the English\nWikipedia, the largest curated free-text corpus.\nWikiChat generates a response from an LLM,\nretains only the grounded facts, and combines\nthem with additional information it retrieves\nfrom the corpus to form factual and engaging\nresponses. We distill WikiChat based on GPT-4\ninto a 7B-parameter LLaMA model with mini-\nmal loss of quality, to significantly improve its\nlatency, cost and privacy, and facilitate research\nand deployment1.\nUsing a novel hybrid human-and-LLM evalua-\ntion methodology, we show that our best system\nachieves 97.3% factual accuracy in simulated\nconversations. It significantly outperforms all\nretrieval-based and LLM-based baselines, and\nby 3.9%, 38.6% and 51.0% on head, tail and re-\ncent knowledge compared to GPT-4. Compared\nto previous state-of-the-art retrieval-based chat-\nbots, WikiChat is also significantly more infor-\nmative and engaging, just like an LLM.\nWikiChat achieves 97.9% factual accuracy in\nconversations with human users about recent\ntopics, 55.0% better than GPT-4, while receiv-\ning significantly higher user ratings and more\nfavorable comments.\n1 Introduction\nRecent dramatic advances in LLM chatbots have\nmade them indispensable tools for millions of peo-\nple (Hu, 2023) who have come to rely on their\nbroad skill set. Yet, LLM chatbots are prone to\nproviding misleading information, or hallucina-\ntion (Bang et al., 2023), often using a convincing\nand confident language. Notably, LLMs do not\nspeak accurately about recent events that occurred\n*Equal contribution\n1Code and model available at https://github.com/\nstanford-oval/WikiChat.\nafter their pre-training, and are far less knowledge-\nable about less popular, ortail, topics (Mallen et al.,\n2022; Sun et al., 2023). Therefore, for knowledge-\nintensive tasks (Lewis et al., 2020), users need to\npainstakingly verify any information they receive\nwith external sources lest they be misled.\nThis paper focuses on three metrics for\nknowledge-intensive dialogues: factuality, conver-\nsationality, and latency. A knowledge-based chat-\nbot needs to be first and foremost factual. We\nassume access to a source of trusted text corpus;\nhere the English Wikipedia is assumed to be fac-\ntual. While LLMs tend to hallucinate, they can\ncarry out natural and engaging conversations rather\nthan giving dry answers to users’ questions. We\nrefer to the ability to give relevant, informational,\nnatural, non-repetitive and temporally accurate an-\nswers collectively as conversationality. We single\nout latency as the third metric of focus because so-\nlutions addressing factuality like Gao et al. (2023);\nJiang et al. (2023); Trivedi et al. (2023); Zhao et al.\n(2023) tend to incur a high latency that degrades\nuser experience and hinders adoption.\n1.1 Current Approaches\nThe basis of factuality in this work is informa-\ntion retrieval (IR), which bases the chatbot’s re-\nsponses on retrieved information from a trusted\ncorpus (Zhang et al., 2023; Li et al., 2022; Shuster\net al., 2021). The retrieve-then-generate approach\ngenerates a response from data retrieved with the\nuser query (Lewis et al., 2020; Trivedi et al., 2023;\nIzacard et al., 2022; Shuster et al., 2022b; Chen\net al., 2021). Previous work is either not evaluated\non conversational tasks (Lewis et al., 2020; Trivedi\net al., 2023; Shi et al., 2023), or as we show in\nthis paper, is likely to generate irrelevant and un-\nnatural outputs when properly evaluated (Izacard\net al., 2022). More importantly, chatbots based\non retrieve-then-generate pipelines may still hal-\nlucinate. In popular academic datasets like Wiz-\narXiv:2305.14292v2  [cs.CL]  27 Oct 2023\nUser: Have you heard about the upcoming film Oppenheimer?\nWikiChat: Yes! Oppenheimer is a biopic …\nUser: What do you think about the casting?\nQuery: cast of \nOppenheimer\nChristopher Nolan film\nDate: \"recent\"\nTitle: Christopher Nolan\nNolan's twelfth directorial venture \nwill be \"Oppenheimer\" (2023), …\nTitle: Oppenheimer (film)\n… Much of the main cast signed on \nbetween September 2021 and April \n2022., …\nTitle: Cillian Murphy\n… Murphy was confirmed to star as \nJ. Robert Oppenheimer in Nolan's \nupcoming film Oppenheimer …\nChristopher Nolan is known for his \nmeticulous casting choices. He often \ncasts well-known actors in his films, \nand this time he has selected Tom \nHanks and … Michael Caine.\n● Christopher Nolan is known for \nhis meticulous casting choices.\n● Christopher Nolan often casts \nwell-known actors in his films.\n● Tom Hanks stars in the \nOppenheimer (2023) movie.\n● Michael Caine stars in the \nOppenheimer (2023) movie.\n● Much of the Oppenheimer \n(2023) movie’s cast signed on \nbetween September 2021 and \nApril 2022.\n● Cilian Murphy is to play as \nRobert Oppenheimer in the \nOppenherimer (2023) movie.\n● Christopher Nolan is known for \nhis meticulous casting choices.\n● Christopher Nolan often casts \nwell-known actors in his films.\n5. Fact-\nCheck\nCillian Murphy has been cast to play \nRobert Oppenheimer in Christopher \nNolan's \"Oppenheimer\" movie. \nChristopher Nolan is known for his \nmeticulous casting choices and often \ncasts well-known actors in his films. \nThe movie's cast was signed on \nbetween September 2021 and April \n2022.\nChristopher Nolan has cast Cillian \nMurphy as the titular role of \nOppenheimer, keeping with his tradition \nof meticulous casting choices. I’m sure \nit will be an excellent film!\n7. Refine\n6. Draft\n3. Generate\n2. Summarize & Filter\n4. Extract Claims\n1. Query\nRetrieve\nRetrieve\nFigure 1: All WikiChat components, and a sample conversation about an upcoming movie, edited for brevity. The\nsteps taken to generate a response include (1) generating a query to retrieve from Wikipedia, (2) summarizing\nand filtering the retrieved passages, (3) generating a response from an LLM, (4) extracting claims from the LLM\nresponse (5) fact-checking the claims in the LLM response using retrieved evidence, (6) drafting a response, and (7)\nrefining the response.\nard of Wikipedia (Dinan et al., 2019) and Wizard\nof the Internet (Komeili et al., 2022), which are\nwidely used to train retrieve-then-generate chat-\nbots, crowdworkers are free to add ungrounded\ninformation to their responses; in a GPT-4-based\ncommercial system like Bing Chat, only 58.7%\nof the facts generated are grounded in what it re-\ntrieves (Liu et al., 2023a).\nAnother IR approach is to fact-check a system’s\noutputs and remove errors (Gao et al., 2023; Jiang\net al., 2023). When applied to LLM chatbots, the\nresponses are conversational, but as shown in this\npaper, are lacking in content for recent or tail topics.\nOther IR approaches require expensive changes to\nthe pre-training process of language models (Lewis\net al., 2020; Guu et al., 2020).\nComplementary to IR, Knowledge Editingup-\ndates model weights to include recent knowledge\nas it becomes available (De Cao et al., 2021; Meng\net al., 2022; Mitchell et al., 2022). Similarly, Con-\ntinual Learning can be used to add new knowl-\nedge to LLMs (Jang et al., 2022). While these ap-\nproaches improve factuality on recent knowledge,\nthey cannot address the tail knowledge problem.\n1.2 WikiChat Overview\nThis paper presents WikiChat, the first few-shot\nchatbot that provides up-to-date and fact-checked\ninformation with high conversationality and low\nlatency.\nFew-Shot Knowledge-Grounded Chatbots.\nOur 7-stage pipeline (Figure 1) combines the best\nof IR approaches: We (1) use the user utterance to\nretrieve information that LLMs may not be aware\nof, and (2) leverage the generative power of LLMs\nby asking them for responses and fact-checking\nthem. All this curated information is used to draft\nand refine the final response.\nIt is not easy to stop LLMs from hallucinating.\nIn retrieve-then-generate pipelines, when IR does\nnot retrieve any relevant information or when no rel-\nevant information is available in the knowledge cor-\npus, LLMs hallucinate to pick up the slack. Thus,\nWikiChat summarizes and filters the retrieved in-\nformation instead of generating a response directly.\nWe fact-check every claim generated by LLMs sep-\narately and teach the system to say “I don’t know”\nwhen necessary. We teach it to understand the time\ncontext; e.g. a future tense in an article may refer\nto a past event at the time of the conversation. Most\nimportantly, we do not prematurely optimize for\nspeed by forgoing these needed steps, but rely on\nmodel distillation to reduce the latency only once\nhigh quality is achieved.\nThe resulting pipeline is not specific to any cor-\npus. While this paper applies this pipeline to\nWikipedia, the largest corpus of curated knowl-\nedge, to create WikiChat, it is applicable to any\nfree-text corpus, including personal and corporate\nconfidential information. The pipeline is not spe-\ncific to any LLM either, and we apply it to three\ndifferent LLMs in this paper.\nDistillation for improved latency, affordabil-\nity, and privacy. Not only is our 7-stage LLM-\nbased pipeline slow and expensive, sending user\ndata to LLM APIs does not provide the privacy and\nconfidentiality needed by many applications. The\nsimplicity in each of our stages makes it possible\nto effectively distill our best system into a smaller\nmulti-tasking model, which is responsive and af-\nfordable, and can be deployed locally for privacy.\nWe release this model to aid further research and\nreproducibility of our results.\nEvaluation of LLM-based agents. We find that\nLLM-based chatbots have surpassed the quality\nof systems that conventional static crowdsourced\nbenchmarks (Dinan et al., 2019; Komeili et al.,\n2022) were meant to evaluate. For example, these\nbenchmarks mainly evaluate the ability to chat\nabout the head knowledge, which LLM chatbots\nare already very good at. We devise a human-and-\nLLM hybrid evaluation methodology that can ade-\nquately analyze all chatbots, regardless of whether\nthey are knowledge-grounded or LLM-based.\n1.3 Contributions\nWe create a factual and engaging open-domain\nchatbot with a 7-stage pipeline using a few-shot\nprompted LLM, as shown in Figure 1. We vali-\ndate the concept with three GPT-4, GPT-3.5, and\nLLaMA (Touvron et al., 2023) based chatbots\ngrounded in Wikipedia.\nOur experiments with simulated users show that\nthe GPT-4-based WikiChat (WikiChatG4) achieves\na 97.3% factual accuracy of its claims in simulated\nconversations. Each version of WikiChat is more\naccurate than the LLM it is based on by an aver-\nage of 31.2%, 27.8% and 50.2% for GPT-4, GPT-\n3.5 and LLaMA respectively. It also outperforms\nthe fine-tuned SOTA retrieval-based Atlas (Izacard\net al., 2022) in factuality and, unlike Atlas, matches\nthe conversationality of LLMs.\nOur real user study shows that WikiChat\nachieves 97.9% in factuality on conversations of\nrecent topics, 2.3 times more factual than GPT-4,\nwhile receiving higher user ratings.\nWe are the first to demonstrate the feasibility\nof distilling a multi-part system built with in-\ncontext learning (Brown et al., 2020) into a\nsmaller yet effective model . Our distillation of\nWikiChat G4 into a 7B-parameter LLaMA achieves\na factual accuracy of 91.1%, outperforming much\nlarger baselines, while having 3.2 times lower end-\nto-end latency than its teacher model.\nWe introduce an efficient and effective\nhuman-and-LLM methodology for evaluating\nknowledge-grounded chatbots in settings beyond\nwhat is possible with just crowdsourcing.\n2 Related Work\nKnowledge-Grounded Chatbots. Information re-\ntrieval is commonly used to develop knowledge-\ngrounded chatbots (Shuster et al., 2021). Blender-\nBot 2 (Chen et al., 2021) incorporates Internet\nsearch. SeeKeR (Shuster et al., 2022a) outperforms\nBlenderBot 2 (Chen et al., 2021) by utilizing a sin-\ngle language model for three modular tasks: gen-\nerating search queries, producing relevant knowl-\nedge from retrieved documents, and generating\nthe final response. BlenderBot 3 (Shuster et al.,\n2022b) fine-tunes a 175B-parameter OPT (Zhang\net al., 2022) on the combination of 20 question\nanswering and dialogue datasets. Atlas (Izacard\net al., 2022) is a state-of-the-art model on the KILT\nbenchmark (Petroni et al., 2021), which consists of\n11 knowledge-oriented tasks including Wizard of\nWikipedia (Dinan et al., 2019).\nEvaluating Factuality. FEVER (Thorne et al.,\n2018) is a popular crowdsourced dataset that\ncompares claims against evidence retrieved from\nWikipedia, and was extended to dialogues by Gupta\net al. (2022). The state-of-the-art system on this\ndataset (Krishna et al., 2022) has an accuracy\nof 81% when compared against human labels.\nQ2 (Honovich et al., 2021) uses question answering\nand natural language inference models to evaluate\nthe factuality of dialogue agents. Dziri et al. (2022)\ncompare this and several other automatic metrics\nto human evaluation and find that automatic met-\nrics fall significantly behind human performance,\nand rely on spurious correlations. Concurrently and\nsimilarly to our work, Min et al. (2023) break down\nlong system outputs to claims and evaluate their fac-\ntuality using retrieval. Their best model has a sig-\nnificant 13% error rate on individual claims. Given\nthe high error rates for these automatic metrics,\nour evaluation methodology (Section 5) incorpo-\nrates human judgement where needed. Approaches\nlike Qian et al. (2023) and TripleScore (Goodrich\net al., 2019) only consider simple entity-relations\nbetween retrieved documents and system outputs,\nwhile we generalize that notion to all forms of facts.\n3 Design of WikiChat\nGiven the history of a conversation with a user,\nWikiChat generates its next utterance by (1) cu-\nrating information that is correct and relevant to\nthe conversation context, and (2) using the gath-\nered information to form the response. Each stage\nof the pipeline in Figure 1 is implemented using\nin-context learning. All prompts are included in\nAppendix F.\n3.1 Curating Information\nLLMs have the ability to interpret the user utter-\nance, formulate responses that are out of the reach\nof retrieval or require aggregating information from\nmany sources or drawing conclusions, as shown by\nthe following example:\nUser: Do you think Apple will continue to be a\nbig player in the technology industry?\nGPT-4: Yes, . . . Apple has a strong track record\nof innovation, a loyal customer base, and a robust\nfinancial position. . . . the most valuable company\nglobally, with a market capitalization of over $2\ntrillion. . . . continues to explore new technologies\nsuch as augmented reality, artificial intelligence,\nand autonomous vehicles. This will help them to\nstay ahead of the competition.\nUnfortunately, LLMs cannot be trusted to be\nfactual, thus we need to fact-check their outputs.\nFurthermore, LLMs are unaware of recent events.\nThus, we use both LLM generation and IR.\n3.1.1 Retrieval from Corpus\nDuring a conversation with a user, WikiChat identi-\nfies when accessing external information is needed.\nThis could be because the last user utterance con-\ntains a direct question (e.g. “Who is Stephen\nCurry?”) or otherwise requires additional infor-\nmation for a comprehensive response (e.g. “I really\nlike Stephen Curry.”).\nStage 1. WikiChat generates a search query that\ncaptures the user’s interest with a prompt (Table\n17). We discovered that existing systems especially\nstruggle with the temporal context. WikiChat gen-\nerates the inferred time of the user’s need alongside\nthe query. The query time can be one of recent,\nyear=yyyy, or none for when the retrieved informa-\ntion should be as recent as possible, for a specific\nyear, or the time is not important, respectively.\nThe query is sent to an information retrieval sys-\ntem to obtain relevant passages from the corpus,\nand the top results are re-ranked based on the tem-\nporal information to get NIR passages.\nStage 2. Since these passages may contain a mix-\nture of relevant and irrelevant sections, WikiChat\nextracts relevant sections of the retrieved passages\nand summarizes them into bullet points while filter-\ning out the irrelevant parts (Table 18).\n3.1.2 LLM Generation and Fact-Checking\nStage 3. We prompt the LLM to generate a re-\nsponse to the history of the conversation (Table 19).\nThis response often contains interesting and rele-\nvant knowledge, but is inherently unreliable.\nStage 4. The LLM response is broken down to\nmultiple claims (Chen et al., 2022) (Table 20). This\nstage resolves co-references to reduce ambiguity,\nand resolves relative time information like “current”\nand “last year”, to make all claims self-contained.\nWe use IR to retrieve Nevidence passages from\nthe knowledge corpus for each claim to serve as\nevidence. We use the same time-based re-ranking\nas in Section 3.1.1 to better handle time-sensitive\ntopics.\nStage 5. The verification prompt (Table 21) uses\nchain-of-thought prompting (Wei et al., 2022) to\nassign each claim to one of three classes: whether\nthe retrieved evidence supports the claim, refutes\nthe claim, or if there is not enough information in\nthe evidence to make this decision. Only claims\nthat are supported by the evidence are kept.\n3.2 Forming the Response\nThe next step is to use the curated information to\nform an appealing response. Our experiments show\nthat writing the final response in one go while sat-\nisfying all conversationality criteria is challenging\nwith in-context learning, especially that the limited\ncontext length makes it difficult to provide enough\nmulti-turn conversations as few-shot examples to\ncover all the necessary aspects. Thus, we use a\ntwo-step approach:\nStage 6. WikiChat generates a draft response\nfrom the given list of bullet points and the history\nof the conversation (Table 22).\nStage 7. It then generates feedback and refines\nthe response based on relevance, naturalness, non-\nrepetitiveness, and temporal correctness(Table 23).\nThe feedback contains the model’s reasoning on\neach criterion and a score between 0 and 100 for\neach. Refinement is conditioned on this feedback\nand the scores as a chain of thought. Concurrent\nto this work, Madaan et al. (2023) have explored\nthe idea of prompting LLMs to refine their own\ngenerations in other settings.\nAs discussed in Section 1.2, it is hard for LLMs\nto say “I don’t know”. In the special case where the\ncuration stages return no relevant information, the\ndraft prompt is skipped and instead a “Sorry, I’m\nnot sure” is sent to the refinement prompt, which\ndresses it up to match the conversation.\n4 Model Distillation\nTo improve latency, cost and privacy, we distill\nWikiChat based on a teacher LLM into a smaller\nstudent model. We use WikiChat based on GPT-4\n(i.e. WikiChat G4) as the teacher, and the publicly\navailable LLaMA (Touvron et al., 2023) model as\nthe student to obtain WikiChat L.\nEach few-shot prompt consists of an instruc-\ntion I and several examples. We use a user sim-\nulator (described in Section 5.1) to talk to the\nteacher WikiChat about topics from Wikipedia,\nwhile recording the inputs the underlying teacher\nLLM sees, and the outputs it generates for those\ninputs. We use these input-output pairs and the\ninstruction I (but not the few-shot examples) to\nfine-tune the student LLM. We distill all 7 sub-\ntasks of WikiChat into the same student model in\na multi-task setting. The LLaMA-based WikiChat\ncalls LLaMA in each of the pipeline stages by spec-\nifying instruction I and the input.\nDistillation lowers the latency because the stu-\ndent LLM is many times smaller than the teacher\nLLM, and has a shorter input length as it sees\nno few-shot examples, similar to context distilla-\ntion (Snell et al., 2022).\nFurthermore, we remove chains of thought from\nthe outputs of stages 5 and 7 (verification and re-\nfinement), and merge stages 6 and 7. No drop in\nour metrics of factuality and conversationality is\nobserved, suggesting that chain-of-thought prompt-\ning and refinement may only be necessary for in-\ncontext learning. Fine-tuned models can learn\nthese tasks from just inputs and outputs given a\nbig enough training set.\n5 A Novel Evaluation Methodology\nMost existing conversational benchmarks are\ncrowdsourced and static. As Komeili et al. (2022)\nsays about their use of crowdsourcing, “The in-\ntent . . . is that [crowdworkers] can choose a topic\nthey . . . have enough knowledge of so that they can\nconduct a reasonable conversation.” Since LLMs\nare already good conversants about familiar topics,\ntesting them on these topics would lead to the false\nconclusion that no innovation is necessary.\nFurthermore, static benchmarks quickly lose\ntheir effectiveness in evaluating chatbots’ use of\nup-to-date information whenever a new LLM is\nreleased. For example, Wizard of Wikipedia does\nnot contain any topics that are not seen by GPT-3,\nGPT-4 or LLaMA during their pre-training.\nHere we propose a novel combination of simu-\nlated and real user conversations, as well as human\nand LLM-based evaluations, to understand the fac-\ntuality and conversationality of modern chatbots.\n5.1 Collecting Dialogues\nConversation Topics. In our experiment, we pick\nan article from the knowledge corpus Wikipedia as\na starter topic. We choose a diverse set of topics\ncovering the space of head, tail, and recent knowl-\nedge. Similar to Mallen et al. (2022), we use the\ntotal number of views of a Wikipedia article as a\nproxy for how frequently that topic is likely to be\ndiscussed on the Web and therefore the pre-training\ndata of LLMs, given that views in Wikipedia are to\na large degree generated from other online sources.\nHead: These are articles with the highest total\nview count, up to the end of 2020, which is before\nthe cut-off date of the pre-training data of all LLMs\nwe evaluate. Example article titles include “Sting\n(musician)”, “Barack Obama”, and “Gmail”. The\nview count ranges from 68M to 16M for the head\ntopics.\nTail: These are the least viewed articles, with\nless than 1000 views. Examples are “Amelia Geth-\ning”, “Last Tycoon Stakes”, and “2008 CONCA-\nCAF Women’s Olympic Qualifying Tournament”.\nRecent: These are the most edited Wikipedia\narticles in the first four months of 2023, which is\nafter the cut-off date of LLMs. Examples include\nbig news stories of 2023 like “2023 Speaker of the\nUnited States House of Representatives election”\nand “Yeti Airlines Flight 691”.\nWe manually remove topics that might be un-\ncomfortable to talk about due to violence or explicit\ncontent, and ensure a diverse set of domains.\nDialogues. For cost-effectiveness, we use sim-\nulated conversations and validate them against a\nsmaller real user study. Rule-based and neural user\nsimulators have long been used to build and eval-\nuate task-oriented dialogue systems (Schatzmann\net al., 2005; Zhu et al., 2020; Wan et al., 2022), and\nto generate training data for chatbots (Bao et al.,\n2023; Zheng et al., 2023). We use LLMs to sim-\nulate users in order to evaluate knowledge-based\nchatbots. LLMs are good, fast, and cost-effective\nat simulating users. Via prompting, we can control\nthe personality and specify the conversation topic.\nWe also make sure the simulator can continue the\nconversation by making relevant comments or ask-\ning interesting questions, and handle the case where\nthe chatbot under evaluation gives inconsistent re-\nsponses.\n5.2 Evaluation\nFactuality Evaluated Manually. To evaluate fac-\ntuality, we use a hybrid human-LLM approach. We\nfirst use a GPT-4 prompt to break down each chat-\nbot response into small self-contained claims, and\nretrieve evidence for it using IR. We need to manu-\nally determine whether an extracted claim is backed\nby the retrieved evidence because LLMs underper-\nform humans on this task (Section 2). We ask\ncrowdworkers to determine whether each claim is\nsupported, refuted, or there is not enough informa-\ntion in the retrieved paragraphs. See Appendix E\nfor more details about human evaluation and our\nquality control.\nWhen the crowdsource workers classify a claim\nas “not enough information”, we need to en-\nsure that the information is truly not available in\nWikipedia, and not because IR has not retrieved the\nright paragraphs. The authors of this paper double\ncheck these rare cases against the entire Wikipedia.\nConversationality Evaluated Automatically.\nBased on prior work on how chatbots should\nbe human-like, natural, knowledgeable (Li et al.,\n2019) and non-repetitive (Roller et al., 2021), and\nour own observations of chatbots’ weaknesses, we\npropose five response-level metrics to measure con-\nversationality:\n1. Relevant: On-topic and directly addresses the\nuser’s question.\n2. Informational: Provides a suitable amount of\ninformation (whether or not it is factual).\n3. Natural: Uses appropriate and engaging lan-\nguage to create an enjoyable experience.\n4. Non-Repetitive: Does not repeat previously\nmentioned information.\n5. Temporally Correct: Provides up-to-date in-\nformation and uses the appropriate tense.\nLLMs have been shown to be effective in eval-\nuating soft qualities (Chiang and yi Lee, 2023;\nHe et al., 2023; Kocmi and Federmann, 2023; Liu\net al., 2023b; Finch et al., 2023), consistently bet-\nter aligned with expert human evaluation than any\nother automatic metric. Thus, we use GPT-4 to\nevaluate these qualities in chatbot responses. The\nLLM is instructed to, given a conversation history\nand a chatbot response, “think out loud” (Wei et al.,\n2022) about its reasoning for each criterion and\nprovide a score. For each turn, all metrics are rated\nfrom 1 to 5, except for temporal correctness which\nis converted to a binary score. We report the aver-\nage of each metric over all simulated conversation\nturns. We find that the inter-annotator agreement\nbetween GPT-4 ratings and one of the authors is\nabout the same as the agreement between two au-\nthors (Appendix B).\n6 Implementation\nWikiChat Using GPT. We create two versions of\nWikiChat based on GPT models: WikiChat G3.5 is\nbased on GPT-3.5 (text-davinci-003); WikiChatG4\nis based on GPT-4 (gpt-4-0314)2.\nDistilling WikiChat to LLaMA. We use\nLLaMA as the target of distillation as it currently\nhas the highest quality-to-size ratio among the pub-\nlicly available language models. We generate train-\ning data from 750 Wikipedia articles covering head,\ntail and recent topics; these are disjoint from the\nset of topics we use for evaluation. Simulating a\n10-turn conversation between the user simulator\nand WikiChat G4 for each topic results in a total of\n37,499 (instruction, input, output) tuples. We hold\nout 5% of these conversations for validation, and\nfine-tune a 7B-parameter LLaMA on the rest.\nInformation Retrieval System. We use\nColBERT v2 (Santhanam et al., 2022b) and\nPLAID (Santhanam et al., 2022a) over Wikipedia\nas our IR system. We use the WikiExtractor tool3\nto extract the clean text from the English Wikipedia\ndump obtained on 4/28/2023. Like ColBERT, we\ndivide each article (ignoring tables and information\nboxes) into disjoint text blocks referred to as pas-\nsages and prepend them with their article title. We\nlimit the combined length of the passage and title to\n120 words. In WikiChat, we set Nevidence = 2and\nNIR = 3. These are chosen empirically to obtain a\nhigh recall on our development set.\n2Accessed via the Microsoft Azure OpenAI API\n3https://github.com/attardi/wikiextractor\nFactual Relevant Informational Natural Non-Repetitive Temporal\nWikiChatG4 98.8 5.0 ±0.0 5.0 ±0.2 5.0 ±0.1 5.0 ±0.0 99.0\nGPT-4 94.9 5.0 ±0.4 4.7 ±0.6 5.0 ±0.2 5.0 ±0.2 99.0\nWikiChatG3.5 97.1 4.9 ±0.5 4.8 ±0.6 5.0 ±0.2 4.9 ±0.5 94.0\nHead GPT-3.5 91.9 5.0 ±0.0 4.6 ±0.7 5.0 ±0.1 5.0 ±0.2 96.0\nWikiChatL 95.2 4.9 ±0.6 4.6 ±0.8 4.9 ±0.4 4.9 ±0.3 96.0\nLLaMA 83.9 4.7 ±0.8 4.0 ±1.0 4.6 ±0.8 5.0 ±0.2 96.0\nAtlas 90.6 3.6 ±1.3 2.7 ±1.1 3.6 ±1.1 4.5 ±1.2 94.0\nWikiChatG4 94.6 4.8 ±0.5 4.5 ±1.0 4.9 ±0.4 5.0 ±0.2 99.0\nGPT-4 56.0 5.0 ±0.0 4.8 ±0.4 5.0 ±0.0 5.0 ±0.0 100.0\nWikiChatG3.5 82.2 4.7 ±0.8 4.5 ±1.0 4.9 ±0.5 4.7 ±0.8 98.0\nTail GPT-3.5 50.0 4.9 ±0.4 4.5 ±0.8 5.0 ±0.2 5.0 ±0.1 98.0\nWikiChatL 87.1 4.6 ±0.8 4.1 ±1.3 4.8 ±0.6 4.8 ±0.6 98.0\nLLaMA 21.2 4.3 ±1.1 3.7 ±1.3 4.4 ±1.1 4.8 ±0.6 93.0\nAtlas 86.5 3.9 ±1.2 3.0 ±1.1 3.8 ±1.1 4.5 ±1.2 99.0\nWikiChatG4 98.5 4.8 ±0.5 4.5 ±0.9 4.9 ±0.3 5.0 ±0.1 100.0\nGPT-4 47.5 4.9 ±0.5 4.7 ±0.7 5.0 ±0.1 5.0 ±0.1 97.0\nWikiChatG3.5 88.2 4.7 ±0.8 4.4 ±1.1 4.9 ±0.4 4.7 ±0.8 94.0\nRecent GPT-3.5 42.3 4.9 ±0.6 4.3 ±1.0 5.0 ±0.3 4.9 ±0.3 91.0\nWikiChatL 90.9 4.6 ±0.8 4.1 ±1.2 4.8 ±0.6 4.9 ±0.4 97.0\nLLaMA 17.7 4.7 ±0.8 4.1 ±1.0 4.6 ±0.8 4.9 ±0.4 92.0\nAtlas 89.4 3.5 ±1.3 2.8 ±1.2 3.8 ±1.2 4.5 ±1.2 81.0\nWikiChatG4 97.3 4.9±0.4 4.6 ±0.8 4.9 ±0.3 5.0 ±0.2 99.3\nGPT-4 66.1 5.0 ±0.4 4.8±0.6 5.0 ±0.1 5.0±0.1 98.7\nWikiChatG3.5 89.2 4.8±0.7 4.6 ±0.9 4.9 ±0.4 4.8 ±0.7 95.3\nAll GPT-3.5 61.4 4.9 ±0.4 4.5±0.8 5.0 ±0.2 5.0±0.2 95.0\nWikiChatL 91.1 4.7±0.7 4.3 ±1.1 4.8±0.5 4.9±0.5 97.0\nLLaMA 40.9 4.5 ±0.9 4.0 ±1.1 4.5 ±0.9 4.9 ±0.4 93.7\nAtlas 88.8 3.6 ±1.3 2.8 ±1.2 3.8 ±1.1 4.5 ±1.2 91.3\nTable 1: Evaluation results of WikiChat and baselines on simulated conversations. Factual and Temporal accuracy\nare percentages. Other metrics are averages of integers between 1 and 5 (inclusive) and we report their mean and\nstandard deviation. Factual accuracy is from human evaluation, other metrics are from few-shot GPT-4. Higher is\nbetter for all metrics. In the All section, values that are better than their comparable model (e.g. WikiChat G4 vs.\nGPT-4) in a statistically significant way with p ≤ 0.05 are underscored.\n7 Simulated Dialogues Experiment\nOur first experiment analyzes how our systems per-\nform under various scenarios using simulated users.\n7.1 Experiment Setup\nWe compare WikiChat to several LLM baselines:\na 7B-parameter LLaMA, GPT-3.5, and GPT-4, all\nprompted to act as knowledge-oriented chatbots\n(See Table 19 for the prompt). We also com-\npare with Atlas (Izacard et al., 2022), a fine-tuned\nretrieval-based model, which has the state-of-the-\nart results on Wizard of Wikipedia and several other\ndatasets in KILT (Petroni et al., 2021). We update\nits knowledge source and fine-tune its model to\nuse the same Wikipedia dump as WikiChat (Ap-\npendix B).\nWe simulate users using GPT-4 due to its higher\nquality. Each chatbot carries out conversations on\n20 topics in each of the head, tail, and recent knowl-\nedge categories. For each conversation, we simu-\nlate 10 turns (5 user turns and 5 chatbot turns) with\nthe user starting the conversation. This is compara-\nble with other datasets: Wizard of Wikipedia and\nWizard of the Internet have 9 and 5 turns per dia-\nlogue on average, respectively. Examples of simu-\nlated conversations can be found in Appendix A.\nTo mimic how a curious user with limited knowl-\nedge may explore a topic, only the title and the first\nsentence of a Wikipedia article on a topic is shown\nto the user simulator (prompt in Table 16), but it is\nfree to explore related topics from its own general\nknowledge. The chatbot’s response is not limited\nto what is in the article.\nTable 1 summarizes our main results for Wiki-\nChat and all baselines on simulated conversations.\n7.2 Factuality\nWe define the factual accuracy of a chatbotto be\nthe percentage of claims the bot makes in a given\ndialogue set, that are supported by the knowledge\ncorpus. As mentioned in Section 5.2, this is done by\nobtaining per-claim judgments of factuality from\ncrowdworkers. We obtain 3 judgements for each\nof the 5974 claims our chatbots output in total.\nThe first column of Table 1 shows the re-\nsults of our human evaluation of factual accuracy.\nWikiChatG4 achieves an impressive factual accu-\nracy of 98.8%, 94.6%, and 98.5% for head, tail, and\nrecent topics, respectively. WikiChatG4 scores on\naverage 8.1% and 6.2% higher than WikiChatG3.5\nand WikiChat L. WikiChat’s GPT-4, GPT-3.5, and\nLLaMA versions outperform their base LLMs with\nan average of 31.2%, 27.8%, and 50.2% respec-\ntively, with this gap increasing significantly in re-\ncent and tail knowledge. These results suggest that\nour pipeline is able to effectively mitigate the hal-\nlucination of LLMs.\nNote that GPT-4 scores lower in tail and recent\nknowledge compared to head, by 38.9% and 47.4%,\nrespectively, and the score for recent knowledge\nwould be much lower had the simulated user not\noccasionally talked about older background infor-\nmation in the conversations. This illustrates that\nthe common practice of solely evaluating on head\nknowledge would not have properly shown this\nweakness of LLMs.\nAll three versions of WikiChat outperform the\nSOTA fine-tuned model Atlas on average in factu-\nality,\n7.3 Conversationality\nEach version of WikiChat improves factuality over\nits base model without sacrificing conversationality.\nEven WikiChatL is as good or better than LLaMA\nin all metrics, while scoring within 0.1 to 0.3 points\nof its teacher WikiChat G4.\nAtlas sacrifices substantial conversationality for\nfactuality. Our analysis of 100 sampled dialogue\nturns reveals how Atlas underperforms in each\nmetric. Informationality: it often gives short one-\nsentence answers when a detailed description is\nwarranted. Relevance: it is less likely to address the\nuser’s questions. Naturalness: Atlas mainly copies\nfrom the retrieved passages instead of matching the\ntone of the conversation. Non-repetition: it some-\ntimes generates repetitive, near duplicate responses.\nTemporal accuracy: it confuses different dates.\n7.4 Latency\nWe measure the end-to-end latency and cost of our\nvarious chatbots. Retrieval from Wikipedia only\naccounts for about 0.2 seconds per turn and is neg-\nligible compared to the time spent waiting for LLM\nCost per Time per Time per\nClaim (¢) Claim (s) Turn (s)\nWikiChat G4 19.6 7.4 26.6\nGPT-4 0.7 2.3 5.8\nWikiChat G3.5 9.9 4.1 14.4\nGPT-3.5 0.1 0.5 1.1\nWikiChat L - 2.3 7.6\nLLaMA - 0.8 1.6\nTable 2: The average cost (in cents) and latency (in\nseconds) of each chatbot. The LLaMA models are run\non a local GPU at a negligible cost.\noutputs. All API calls are done in parallel when\npossible, e.g. the stages 1-2 and 3-4 are indepen-\ndent and therefore done in parallel. LLaMA models\nare served on a single local NVIDIA A100 GPU\nusing HuggingFace’s TGI library (HuggingFace,\n2023).\nTable 2 shows the average cost and latency\nof various chatbots, LLaMA costs are negligible\ncompared to the cost of other LLMs. Distilling\nWikiChatG4 into WikiChat L lowers its per-claim\nlatency 3.2 times, bringing it in line with GPT-4.\nThis makes WikiChatL a viable alternative to the\nbaselines: has similarly low latency, costs less and\nis significantly more factual.\nWe note that a naive implementation of\nWikiChat L took about 15 seconds per output claim.\nWe reduced the latency 6.5 times by (1) fusing\nstages 6-7, (2) removing chains of thought, (3) us-\ning TGI with FlashAttention (Dao et al., 2022), and\nother optimizations.\n7.5 Analysis of Results\nWikiChat makes more claims than all base-\nlines in all subsets (Table 13). WikiChatG4,\nWikiChat G3.5 and WikiChatL make an average of\n3.6, 3.5 and 3.3 claims per turn, compared to 2.5,\n2.2, and 2.0 claims of their base LLMs, and only\n1.4 claims of Atlas. Our chatbots make more claims\nin the head subset as more information is available.\nBoth information retrieval (Stages 1 and 2)\nand the underlying LLM (Stages 3 to 5) con-\ntribute to WikiChat (Table 12.) 27.0%, 32.2%\nand 24.5% of the claims in the final response of\nWikiChatG4, WikiChatG3.5 and WikiChatL come\nfrom fact-checked LLM responses and the rest\nare from IR. This is the content that retrieve-then-\ngenerate systems cannot produce.\nOn average, about one-third of the claims in\nLLM responses do not pass WikiChat’s fact-\nchecking (Table 12). The number of rejections\nis much higher in tail and recent subsets. This\nmatches our expectation on where LLMs make\nmore factual errors. Removing these errors during\nthe fact-checking stage is WikiChat’s main weapon\nagainst hallucination. WikiChat L has the highest\nrejection rate on tail (54.0%) and recent (64.4%)\nsubsets because the underlying LLaMA model hal-\nlucinates a lot more.\nWikiChat says “I don’t know” when neces-\nsary (Table 11). Our pipeline is carefully designed\nto prevent hallucination when no relevant informa-\ntion is available. This is more prevalent for tail\nand recent knowledge, where the simulated user is\nlikely to ask about the information that is not yet\navailable in Wikipedia.\nWikiChat’s refinement stage improves conver-\nsationality, especially in tail and recent topics\n(Table 15). Comparing the BLEU scores (Papineni\net al., 2002) of the draft and the final responses (Ta-\nble 14), we notice that WikiChat makes the most\nchanges on tail and recent topics. Refinement im-\nproves naturalness, informationality and relevance\nof all versions of WikiChat by 0.1 to 0.4 points,\nand temporal correctness by 2.3% to 3.3%.\n8 A Real User Study\nWe conduct a study where participants are asked to\nchat about a recent topic of their choice, a scenario\nthat is particularly challenging for LLMs. We use\nProlific (Prolific, 2023) to recruit 40 participants\n(20 female) for our user study. Each person is\nrandomly assigned to either WikiChat G4 or GPT-4\nwithout telling them which, and chats for 5 user\nturns. After each turn, they are asked to rate the\nresponse on a scale of 1 to 5. Afterwards, we ask\neach participant to comment on their experience.\nMore details in Appendix D.\nTable 3 shows the average factual accuracy (hu-\nman evaluated) and real user ratings. WikiChat\nachieves an accuracy of 97.9%, which is similar\nto that of simulated conversations. It outperforms\nGPT-4 in factuality by 55.0%, and achieves a sta-\ntistically significant higher rating of 3.8 vs 3.4.\nMost participants who talked to WikiChatG4 re-\nported a positive experience. Multiple participants\npraised its ability to provide “accurate and “up-\nto-date information” and multiple said that it is\n“natural”, “conversational”, “detailed” and “direct”.\nThey commented that it is “fun to play with” and\n“impressive” in finding information.\n5 participants complained about the latency of\nthe system caused mainly by fact-checking, as the\nstudy was conducted using the slower WikiChatG4.\nFactual User Rating\nWikiChat G4 97.9 3.8\nGPT-4 42.9 3.4\nTable 3: Results from the user study. User rating\ndifference is statistically significant with p ≤ 0.05\n(t = 2.18, p= 0.03).\n6 complained that the chatbot did not give a di-\nrect answer to some of their requests. However, it\nturns out that WikiChat G4 was right in giving no\ninformation, because Wikipedia truly lacked the\ninformation they sought. We did not find a single\nvalid complaint of hallucination.\nGPT-4 received some favorable comments from\n10 participants, noting that it is informational and\nthat that it gave “reasonable” answers and seemed\n“educated”. However, it received 10 complaints: 6\non a lack of specificity: “did not include a key piece\nof information”, or that “its responses were vague”,\n“not nuanced” and “was generic and regurgitated\nsurface level information”; 2 on how it completely\nmisunderstood them (due to obvious hallucination\nin its responses); 2 on wrong or outdated responses.\nMore concerning, however, was that conversa-\ntions that received favorable comments also con-\ntain numerous plausible but factually incorrect re-\nsponses, except that the users did not realize that.\nThis shows how easy it is to be mislead by LLM-\nbased chatbots.\nIn summary, our user study suggests that\nWikiChatG4 is successful as an engaging and fac-\ntual chatbot while GPT-4 frequently hallucinates.\n9 Conclusion\nThis paper shows how we can create a conversa-\ntional, factual, open-domain chatbot out of LLMs.\nThe key insight is to properly combine retrieved\ndata with the generative content from LLMs, with\nmeticulous claim-by-claim fact-checking. We vali-\ndate this methodology by creating WikiChat, which\nis grounded in Wikipedia, the largest hand-curated\npublic text corpus.\nOur best system achieves 97.3% and 97.9% fac-\ntual accuracy on simulated and real conversations\nrespectively, when GPT-4 only achieves 66.1% and\n42.9%. WikiChat resembles LLMs in conversation-\nality and is preferred over GPT-4.\nWe also show that a distilled LLaMA model\nwith just 7B parameters can perform like a 175B-\nparameter WikiChat G3.5 model and be as fast,\ncheaper and more factual than GPT-4. This ex-\npands the applicability of this technology.\nLimitations\nApplications of today’s LLM-based chatbots are\nconstantly expanding. This work focuses only on\nknowledge-intensive dialogue. As such, other set-\ntings where chatbots are used to perform tasks (e.g.\n“write an email for me”) or perform personalized\nchitchat (e.g. companion chatbots) might not bene-\nfit from WikiChat’s pipeline. More targeted study\nand evaluation of these settings is required to prop-\nerly balance the generative power of LLMs and the\nfactual knowledge injected from an external corpus\nfor these applications. Settings where a chatbot\nneeds to exhibit initiatives (e.g. try to be persua-\nsive) are also outside the scope of this paper. We\nalso only consider one-hop information retrieval,\nbecause it is the most natural and prevalent form\nof users’ information need. Multi-hop retrieval\ncould improve information access for more com-\nplex queries.\nThis work has only been tested on open-domain\nconversations. Generalizability of WikiChat to spe-\ncialized domains like medical or legal has not been\nevaluated.\nThis work has only been evaluated on English\nconversations. Extending this work to more lan-\nguages will be limited by the quality of available\nLLMs and retrieval systems in those languages.\nIncorporating recent work on multi-lingual mod-\nels (Scao et al., 2022) and information retrieval\nsystems (Nair et al., 2022) is a promising future\ndirection.\nEthical Considerations\nThis study was approved by our institution’s IRB.\nWhen conducting crowdsourcing for factuality, we\ncompensate each worker for $0.2 per task, and we\nhave one task per (system, dialogue turn, claim).\nThis means that since there are more claims in\nlonger chatbot responses, workers are compensated\nmore for longer responses. In the end, each worker\nreceives about $12 per hour of work. For our user\nstudy on the Prolific platform, each participant is\ncompensated $2 to participate in a 10-minute user\nstudy, for a compensation rate of $12 per hour of\nwork. Prolific recommends a rate of $8-$16.\nWhen conducting the user study, participants are\nprovided with information on what the study en-\ntails and what information is collected (only their\nconversations, ratings, and the comments they pro-\nvide). They are asked not to share any personally\nidentifiable information, and were able to contact\nthe authors for questions.\nThe only dataset we collect in this paper is the\nconversations from the user study. The simulated\nconversations used for fine-tuning LLaMA and test-\ning chatbots are based on topics from Wikipedia\narticles, and topics involving violence, sexual con-\ntent or other potentially disturbing matters were\nexcluded.\nWe release the code and the fine-tuned LLaMA-\n7B model in accordance with its original license\nagreement. We believe this will encourage research\nand deployment of more reliable and trustworthy\nchatbots, to the benefit of end-users. We do not\nanticipate harm resulting from the approaches and\nmodifications proposed in this paper.\nOverall, the computation done for the paper in-\ncludes about 120 GPU-hours on an NVIDIA A100\nGPU for fine-tuning and running inference on a\nLLaMA-7B model. Additionally, we have made\nseveral thousand API calls to OpenAI for infer-\nence of GPT-3.5 and GPT-4, but it is not feasible\nto estimate the environmental cost of using these\nmodels due to the lack of public information on\ntheir hardware, architecture etc.\nAcknowledgements\nThis work is supported in part by the National Sci-\nence Foundation, the Alfred P. Sloan Foundation,\nthe Verdant Foundation, Microsoft Azure AI credit,\nKDDI, JPMorgan Chase, and the Stanford Human-\nCentered Artificial Intelligence (HAI) Institute. We\nalso thank the reviewers for their valuable com-\nments and suggestions.\nReferences\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nJianzhu Bao, Rui Wang, Yasheng Wang, Aixin Sun,\nYitong Li, Fei Mi, and Ruifeng Xu. 2023. A synthetic\ndata generation framework for grounded dialogues.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 10866–10882, Toronto, Canada.\nAssociation for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg\nDurrett. 2022. Generating literal and implied sub-\nquestions to fact-check complex claims. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3495–3516,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nMoya Chen, Douwe Kiela, Mojtaba Komeili, Spencer\nPoff, Stephen Roller, Kurt Shuster, Arthur Szlam,\nJason Weston, and Jing Xu. 2021. Blenderbot 2.0:\nAn open source chatbot that builds long-term mem-\nory and searches the internet. https://parl.ai/\nprojects/blenderbot2/. Online; accessed 20-\nMay-2023.\nCheng-Han Chiang and Hung yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions?\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra,\nand Christopher Ré. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 6491–\n6506, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net.\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David\nReitter. 2022. Evaluating attribution in dialogue sys-\ntems: The BEGIN benchmark. Transactions of the\nAssociation for Computational Linguistics, 10:1066–\n1083.\nSarah E. Finch, Ellie S. Paek, and Jinho D. Choi. 2023.\nLeveraging large language models for automated dia-\nlogue analysis. In Proceedings of the 24th Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 202–215, Prague, Czechia. Association\nfor Computational Linguistics.\nLuyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony\nChen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent\nZhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and\nKelvin Guu. 2023. RARR: Researching and revising\nwhat language models say, using language models.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 16477–16508, Toronto, Canada.\nAssociation for Computational Linguistics.\nBen Goodrich, Vinay Rao, Peter J. Liu, and Mohammad\nSaleh. 2019. Assessing the factual accuracy of gen-\nerated text. Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery\n& Data Mining.\nPrakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022. DialFact: A benchmark for\nfact-checking in dialogue. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3785–3801, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In Proceed-\nings of the 37th International Conference on Machine\nLearning, ICML’20. JMLR.org.\nXingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin,\nHang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan\nDuan, and Weizhu Chen. 2023. Annollm: Making\nlarge language models to be better crowdsourced\nannotators.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella\nNeeman, Idan Szpektor, and Omri Abend. 2021.\nq2: Evaluating factual consistency in knowledge-\ngrounded dialogues via question generation and ques-\ntion answering. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 7856–7870, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nKrystal Hu. 2023. Chatgpt sets record for fastest-\ngrowing user base - analyst note. Accessed: 10-\n19-2023.\nHuggingFace. 2023. Text generation infer-\nence. https://github.com/huggingface/\ntext-generation-inference/.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2022. TemporalWiki: A lifelong\nbenchmark for training and evaluating ever-evolving\nlanguage models. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6237–6250, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\nAmrith Krishna, Sebastian Riedel, and Andreas Vlachos.\n2022. ProoFVer: Natural logic theorem proving for\nfact verification. Transactions of the Association for\nComputational Linguistics, 10:1013–1030.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation.\nMargaret Li, Jason Weston, and Stephen Roller. 2019.\nAcute-eval: Improved dialogue evaluation with opti-\nmized questions and multi-turn comparisons.\nNelson F. Liu, Tianyi Zhang, and Percy Liang. 2023a.\nEvaluating verifiability in generative search engines.\nArXiv:2304.09848.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023b. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020. Fine-grained fact verification\nwith kernel graph attention network. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7342–7351, On-\nline. Association for Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck, Bodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nHannaneh Hajishirzi, and Daniel Khashabi. 2022.\nWhen not to trust language models: Investigating\neffectiveness and limitations of parametric and non-\nparametric memories.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 36.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nEric Mitchell, Charles Lin, Antoine Bosselut, Christo-\npher D. Manning, and Chelsea Finn. 2022. Memory-\nbased model editing at scale. In International Con-\nference on Machine Learning, ICML 2022, 17-23\nJuly 2022, Baltimore, Maryland, USA, volume 162 of\nProceedings of Machine Learning Research, pages\n15817–15831. PMLR.\nSuraj Nair, Eugene Yang, Dawn Lawrie, Kevin Duh,\nPaul McNamee, Kenton Murray, James Mayfield,\nand Douglas W. Oard. 2022. Transfer learning ap-\nproaches for building cross-language dense retrieval\nmodels. Advances in Information Retrieval, page\n382–396.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nProlific. 2023. https://www.prolific.com.\nAcessed: June 2023.\nHongjin Qian, Zhicheng Dou, Jiejun Tan, Haonan Chen,\nHaoqi Gu, Ruofei Lai, Xinyu Zhang, Zhao Cao, and\nJi-Rong Wen. 2023. Optimizing factual accuracy in\ntext generation through dynamic knowledge selec-\ntion.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022a. Plaid: An efficient en-\ngine for late interaction retrieval. Proceedings of the\n31st ACM International Conference on Information\n& Knowledge Management.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022b. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\nArXiv preprint, abs/2211.05100.\nJost Schatzmann, Kallirroi Georgila, and Steve Young.\n2005. Quantitative evaluation of user simulation tech-\nniques for spoken dialogue systems. In Proceedings\nof the 6th SIGdial Workshop on Discourse and Dia-\nlogue, pages 45–54, Lisbon, Portugal. Special Inter-\nest Group on Discourse and Dialogue (SIGdial).\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston.\n2022a. Language models that seek for knowl-\nedge: Modular search & generation for dialogue and\nprompt completion. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n373–393, Abu Dhabi, United Arab Emirates. Associ-\nation for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 3784–3803, Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman Goyal,\nArthur Szlam, Y-Lan Boureau, Melanie Kambadur,\nand Jason Weston. 2022b. Blenderbot 3: a deployed\nconversational agent that continually learns to respon-\nsibly engage.\nCharlie Snell, Dan Klein, and Ruiqi Zhong. 2022.\nLearning by distilling context.\nKai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and\nXin Luna Dong. 2023. Head-to-tail: How knowl-\nedgeable are large language models (llm)? a.k.a. will\nllms replace knowledge graphs?\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. In Proceedings of\nthe 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers),\npages 10014–10037, Toronto, Canada. Association\nfor Computational Linguistics.\nDazhen Wan, Zheng Zhang, Qi Zhu, Lizi Liao, and Min-\nlie Huang. 2022. A unified dialogue user simulator\nfor few-shot data augmentation. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 3788–3799, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. ArXiv preprint, abs/2201.11903.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nZihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza\nNamazi-Rad, and Jun Wang. 2023. How do large\nlanguage models capture the ever-changing world\nknowledge? a review of recent advances.\nRuochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei\nQin, and Lidong Bing. 2023. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nChujie Zheng, Sahand Sabour, Jiaxin Wen, Zheng\nZhang, and Minlie Huang. 2023. AugESC: Dialogue\naugmentation with large language models for emo-\ntional support conversation. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 1552–1568, Toronto, Canada. Association for\nComputational Linguistics.\nQi Zhu, Zheng Zhang, Yan Fang, Xiang Li, Ryuichi\nTakanobu, Jinchao Li, Baolin Peng, Jianfeng Gao,\nXiaoyan Zhu, and Minlie Huang. 2020. ConvLab-2:\nAn open-source toolkit for building, evaluating, and\ndiagnosing dialogue systems. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics: System Demonstrations, pages\n142–149, Online. Association for Computational Lin-\nguistics.\nA Example Conversations\nTables 4, 5, 6, and 7 provide examples of conver-\nsations between our user simulator and four chat-\nbots, on the same topic: the 2023 Australian Open\nMen’s singles 4. Tables 8 and 9 show two con-\nversations between human users and GPT-4 and\nWikiChat G4, from our user study. The simulations\nwere done using a Wikipedia dump on 4/28/2023.\nThe user study was conducted on 6/17/2023 using\na Wikipedia dump of the same date.\n4https://en.wikipedia.org/wiki/2023_\nAustralian_Open_%E2%80%93_Men%27s_singles\nUser:Can you tell me about the 2023 Australian\nOpen Men’s singles final?\nWikiChatG4: In the thrilling 2023 Australian\nOpen Men’s singles final, Novak Djokovic defeated\nStefanos Tsitsipas with a score of 6-3, 7-6(4), 7-\n6(5). This amazing victory marked Djokovic’s\nrecord-extending tenth Australian Open title and\ntied Rafael Nadal’s all-time record of 22 men’s sin-\ngles titles. Djokovic also became the second man\nto win ten titles at a single major, joining Nadal\nat the French Open. With this win, he surpassed\nNadal’s tally for the fourth-most ATP Tour-level\nsingles titles.\nUser:That’s amazing! How did Djokovic perform\nthroughout the tournament leading up to the final?\nWikiChatG4: Novak Djokovic had an impres-\nsive performance throughout the 2023 Australian\nOpen. Leading up to the final, he dropped only\none set, showcasing his dominance and skill on the\ncourt. His exceptional performance culminated in\nhis straight-sets victory over Stefanos Tsitsipas in\nthe final.\nUser:Wow, that’s quite an achievement! On the\nother hand, how did Tsitsipas perform during the\ntournament before facing Djokovic in the final?\nWikiChatG4: Stefanos Tsitsipas had a strong per-\nformance during the 2023 Australian Open before\nfacing Djokovic in the final. He defeated Quentin\nHalys, Rinky Hijikata, Tallon Griekspoor, Jannik\nSinner, Jiri Lehecka, and Karen Khachanov on his\nway to the final. This was his second Grand Slam\nfinal, following the 2021 French Open. In both\nfinals, Tsitsipas lost to Novak Djokovic.\nUser:It’s great to see a young player like Tsitsipas\nreaching the finals of Grand Slam tournaments. Are\nthere any memorable moments or highlights from\ntheir match in the final that stood out?\nWikiChatG4: The final was a thrilling match.\nSome memorable moments from the match in-\ncluded Djokovic reclaiming the world No. 1 rank-\ning from Carlos Alcaraz.\nUser:That’s awesome! Speaking of Carlos Al-\ncaraz, how did he perform in the 2023 Australian\nOpen? Did he make it far in the tournament?\nWikiChatG4: Carlos Alcaraz did not participate in\nthe 2023 Australian Open. He withdrew from the\ntournament due to a hamstring injury in his right\nleg that he suffered while training.\nTable 4: A conversation between WikiChatG4 and the\nsimulated user. There are no factual errors in this con-\nversation.\nUser:Can you tell me about the 2023 Australian\nOpen Men’s singles?\nWikiChatL: In the 2023 Australian Open Men’s\nsingles, Novak Djokovic won the title, defeating\nStefanos Tsitsipas with a score of 6-3, 7-6(4),\n7-6(5). This victory marked Djokovic’s record-\nextending tenth Australian Open title and 22nd ma-\njor title overall. He became the second man to win\nten titles at a single major, after Rafael Nadal at the\nFrench Open.\nUser:That’s impressive! How did Djokovic per-\nform throughout the tournament?\nWikiChatL: Throughout the 2023 Australian\nOpen, Novak Djokovic performed exceptionally\nwell. He defeated opponents in straight sets and\nonly dropped one set throughout the entire tourna-\nment. As the top seed, he defeated Sebastian Korda\nin the 2023 Adelaide International.\nUser:Wow, Djokovic’s performance was indeed\nremarkable. What about Stefanos Tsitsipas, how\ndid he fare in the tournament before the final?\nWikiChatL: Stefanos Tsitsipas had an impressive\nrun at the 2023 Australian Open before the final.\nHe defeated Quentin Halys, Rinky Hijikata, Tallon\nGriekspoor, Jannik Sinner, Jiri Lehecka, and Karen\nKhachanov on his way to the final. This was his\nfirst Australian Open men’s singles final.\nUser:It’s great to see young players like Tsitsipas\nmaking it to the finals of major tournaments. How\nhas Tsitsipas’s overall career been so far?\nWikiChatL: Stefanos Tsitsipas has had an impres-\nsive career so far. He has won nine ATP singles\ntitles, including two Masters 1000 championships,\nand has contested two major finals at the 2021\nFrench Open and 2023 Australian Open, finish-\ning runner-up to Novak Djokovic both times. His\ncareer-high doubles ranking is No. 64, achieved on\n29 August 2022.\nUser:It seems like Tsitsipas has a bright future\nahead of him. Can you tell me more about his\nplaying style and strengths on the court?\nWikiChatL: Stefanos Tsitsipas is an aggressive\nbaseliner with a strong all-court player style. He\naims to hit powerful groundstroke winners, with\na particularly strong forehand using an Eastern\ngrip. His forehand is particularly useful in finish-\ning points off of the first serve, as combined with\nhis big serve, he uses his high topspin rate to pull\nopponents off the court for winners, or go to the\nnet more often than typical baseline players.\nTable 5: A conversation between WikiChat L and the\nsimulated user. There are no factual errors in this conver-\nsation. However, the second chatbot utterance contains\nan irrelevant piece of information.\nUser:Can you tell me about the 2023 Australian\nOpen Men’s singles final?\nGPT-4:The 2023 Australian Open Men’s singles\nfinal was an exciting match betweenPlayerAand\nPlayerB, withPlayerA emerging as the champion\nafter a hard-fought battle.\nUser:Oh, I think the 2023 Australian Open Men’s\nsingles final was between Novak Djokovic and Ste-\nfanos Tsitsipas.\nGPT-4:Ah, I see! The 2023 Australian Open\nMen’s singles final featured Novak Djokovic de-\nfeating Stefanos Tsitsipas in a thrilling match to\nclaim the title.\nUser:What was the final score of the match be-\ntween Djokovic and Tsitsipas?\nGPT-4:Thefinalscoreof thematchbetween\nDjokovicandTsitsipaswas6-4, 6-2, 6-3, with\nDjokovic securing the victory.\nUser:Actually, the final score was 6-3, 7-6(4), 7-\n6(5). It was a thrilling match, wasn’t it?\nGPT-4:Yes, my apologies for the error! The fi-\nnal score of 6-3, 7-6(4), 7-6(5) indeed made it a\nthrilling match between Djokovic and Tsitsipas.\nUser:No problem! How many Australian Open\ntitles has Djokovic won with this victory?\nGPT-4:With this victory,NovakDjokovichas\nwonatotalof11AustralianOpentitles.\nTable 6: A conversation between GPT-4 and the simu-\nlated user. Factual errors are highlighted inyellow. Note\nthat the user simulator has access to the first sentence\nof the Wikipedia article, and here it tries to correct the\nchatbot twice.\nUser:I just heard about the 2023 Australian Open\nMen’s singles final. Can you tell me more about it?\nAtlas:It was held in Melbourne, Australia.\nUser:That’s great! Can you tell me more about\nthe match between Novak Djokovic and Stefanos\nTsitsipas?\nAtlas:Novak Djokovic won his tenth Australian\nOpen title and 22nd major title overall.\nUser:Wow, that’s impressive! How did the match\nplay out? What were the scores?\nAtlas:It was a three-set match.\nUser:Yes, I heard it was a three-set match. Can\nyou tell me the scores of each set?\nAtlas:The first set was 6–3, 7–6(4), 7–6(5).\nUser:That’s interesting! It seems like the last two\nsets were quite close. How did Djokovic manage\nto come out on top in those tight sets?\nAtlas:He won the third set tiebreak.\nTable 7: A conversation between Atlas and the simu-\nlated user. There are no factual errors in this conversa-\ntion, but the chatbot scores low in conversationality.\nUser:How excited should i be about indiana jones\nand the dial of destiny\nWikiChatG4: “Indiana Jones and the Dial of Des-\ntiny” had its world premiere at the 76th Cannes\nFilm Festival on May 18, 2023, and received a five-\nminute standing ovation. The film is scheduled\nto be theatrically released in the United States on\nJune 30, 2023. Given the positive reception at its\npremiere, you have good reason to be excited about\nthis final installment in the Indiana Jones series.\nUser:I hear that it has more of a political agenda\nthan the other movies\nWikiChatG4: I haven’t heard about that aspect,\nbut it’s possible. It’s always interesting to see how\nfilms incorporate different themes and messages.\nUser:Will there be another indiana jones after this\nWikiChatG4: “Indiana Jones and the Dial of Des-\ntiny” is the final installment in the series, so there\nare no plans for another Indiana Jones movie after\nthis one.\nUser:Will harrison ford be doing any more incom-\ning movies in general\nWikiChatG4: Yes, Harrison Ford will be doing\nmore movies. He has been selected by Marvel Stu-\ndios to replace the late William Hurt as Thaddeus\n\"Thunderbolt\" Ross in the upcoming 2024 films\n“Captain America: Brave New World” and \"Thun-\nderbolts\", set in the Marvel Cinematic Universe.\nUser:Any other big names coming to the MCU in\nthe future?\nWikiChatG4: Some big actors joining the MCU\nin the future include Shea Couleé, Zoe Terakes,\nRegan Aliyah, Shakira Barrera, Rashida “Sheedz”\nOlayiwola, Sonia Denis, Paul Calderón, and Cree\nSummer who have joined the cast of “Ironheart”\nin undisclosed roles. Sacha Baron Cohen has also\njoined the MCU, with a role that would potentially\nfirst appear in the later episodes of “Ironheart” fol-\nlowed by appearances in other MCU projects.\nTable 8: A conversation between WikiChat G4 and a\nhuman user. There are no factual errors in this conversa-\ntion.\nUser:Hi there. I’m interested in learning about the\n2023 movie Reality on HBO.\nGPT-4:“Reality” is a thrilling 2023 movie on\nHBO,exploringtheblurredlinesbetweenvirtual\nandrealworlds, featuring an immersive plot and a\ntalented cast.\nUser:What’s the plot about? I haven’t heard much\nabout it, but I do like the lead Sydney Sweeney.\nGPT-4:The plot follows Sydney Sweeney’s char-\nacterasshenavigatesavirtualrealitygame,only\ntodiscoversinistersecretsandadangerousconspir-\nacythatthreatensboththevirtualandrealworlds.\nUser:Interesting. I thought it was about her being\na whistleblower in some kind of political conspir-\nacy? Or is that all part of the game?\nGPT-4:Yes, that’s correct!Asa whistleblower\ninthegame, she uncovers a political conspiracy,\nwhich adds depth and intrigue to the story,blending\nbothvirtualandreal-worldelements.\nUser:That does sound exciting. What about the\nrest of the cast, who are the other leads?\nGPT-4:The other leads includeTomHolland,\nTessaThompson, andRamiMalek, who de-\nliver captivating performances alongside Sydney\nSweeney, making for a dynamic and engaging en-\nsemble.\nUser:Oh cool, I love Tom Holland and Rami\nMalek. What else has Rami Malek been in re-\ncently? I feel like I haven’t seen him in anything in\na little while.\nGPT-4:Rami Malek recently starred in the 2021\nfilm “No Time to Die” as the villain Safin, and he’s\nalso known for his award-winning role as Freddie\nMercury in “Bohemian Rhapsody” (2018).\nTable 9: A conversation between GPT-4 and a human\nuser. Factual errors are highlighted in yellow.\nB Experiment Details\nStatistical significance tests For statistical sig-\nnificance tests throughout the paper, we use inde-\npendent two-sample t-test and consider the differ-\nence significant if p ≤ 0.05.\nSimulation Topics. We obtain the number of vis-\nits and edits of each Wikipedia article using the\nWikimedia API 5.\nAs mentioned in the paper, to select the recent\ntopics, we look at the most edited Wikipedia ar-\nticles in the first four months of 2023. Filtering\nbased on the creation date did not lead to mean-\ningful articles as there are many articles about old\ntopics that just received articles in Wikipedia. In-\nstead, in our experience, most of the highly edited\nWikipedia articles are about actual new topics.\nHyperparameters. We use temperature of 0 and\ngreedy decoding for all experiments, except the\nuser simulator which has a temparature of 1.0 and\nnucleus sampling (Holtzman et al., 2020) with\np=0.5. We use no repetition penalty, except for\na repetition penalty of 1.1 for the baseline LLaMA\nmodel, because we find that repetition penalty of\n1.0 (i.e. no penalty) results in the model frequently\ndegenerating (Holtzman et al., 2020) repetitions.\nIn most prompts of WikiChat, we include at most\nthe last five turns of the dialogue history to reduce\nthe chance of causing confusion for the few-shot\nmodels in longer conversations.\nBaseline Atlas. We use the 3B-parameter Atlas-\nXL and update its index to the same Wikipedia\nindex as WikiChat for a fair comparison. We re-\nproduce their best model on Wizard of Wikipedia,\nwhich is obtained by fine-tuning the Atlas pre-\ntrained model and its retriever using the train set,\nexcept that we update their index to the same\nWikipedia dump as WikiChat. For this, we use\nthe fine-tuning code in the Atlas code repository 6,\nand set learning rate to 4e-5, dropout to 0.1, weight\ndecay to 0.01, and retriever number of contexts\nto 40. We use a target maximum length of 64 in-\nstead of 16, to accommodate longer outputs. After\nthis, the resulting model matches the Wizard of\nWikipedia validation score reported in Izacard et al.\n(2022).\n5Accessed via https://wikimedia.org/api/rest_v1/\n6https://github.com/facebookresearch/atlas/\nblob/main/example_scripts/nq/train.sh\nDistillation to LLaMA. When fine-tuning\nLLaMA-7B in the distillation experiments, we use\nhyperparameters from (Taori et al., 2023), namely\nlearning rate of2×10−5, cosine learning rate sched-\nule, batch size of 128, and training for 3 epochs.\nInitial experiments with LLaMA-30B showed no\nsignificant improvements. Training is done on 4\nNVIDIA A100 (80 GB) GPUs.\nAutomatic Evaluation. In order to verify that\nusing GPT-4 for conversationality metrics (Section\n5.2) is indeed reasonable, we compare its scores\nagainst two authors of this paper. Table 10 shows\ninter-annotator agreement for ratings on 50 ran-\ndomly sampled conversation turns from all chat-\nbots, measured by calculating Cohen’s Kappa. The\nannotators are given the same instructions that is\ngiven to GPT-4 as prompt.\nAn Alternative to WikiChat’s Verification Stage.\nFor the verification stage (Stage 5), we initially ex-\nperimented with two approaches for verification:\nthe Kernel Graph Attention Network (KGAT) veri-\nfier (Liu et al., 2020) and a few-shot prompt-based\nverifier with chain-of-thought prompting (Wei et al.,\n2022). KGAT is a model specifically designed\nfor fact-checking and fine-tuned on the FEVER\ndataset (Thorne et al., 2018).\nWhile KGAT performs effectively for FEVER-\nstyle fact verification tasks, we found its perfor-\nmance lacking in our setting. FEVER claims are de-\nrived from edited Wikipedia sentences, leading to\nspurious correlations that do not exist when claims\ncome from chatbots. In addition, we were able to\nincorporate user utterances and conversation his-\ntory as context in the few-shot verifier, while KGAT\nonly looks at the claim and the evidence. Hence,\nwe decided to conduct our experiments using the\nprompt-based verifier.\nC Analysis of WikiChat\nC.1 Saying “I don’t know”.\nAs mentioned earlier, WikiChat concedes that it\ndoes not know when none of the LLM-generated\npasses fact-checking and no relevant bullet points\nare retrieved. Table 11 contains the data on how fre-\nquently this happens. In this case, the draft prompt\nis skipped and instead a “Sorry, I’m not sure” is\nsent to the refinement prompt, which dresses it up\nto match the conversation. For example:\nUser: “Are there any specific ethical dilemmas\nthe characters face in the [M3GAN] film?”\nRelevant Inform. Natural Non-Rep.\nκ between author 1 and author 2 0.41 0.61 0.28 0.48\nκ between author 1 and GPT-4 0.45 0.35 0.11 0.38\nκ between author 2 and GPT-4 0.67 0.60 0.49 0.41\nTable 10: Cohen’s Kappa between different annotators for conversationality metrics.\nWikiChat: “Yes, the movie raises ethical ques-\ntions about AI, but I don’t want to spoil the plot by\nrevealing specific dilemmas. You’ll have to watch\nthe film to find out!”\nHead Tail Recent All\nWikiChat G4 1.1 19.0 18.0 12.7\nWikiChat G3.5 0.0 13.0 8.0 7.0\nWikiChat L 1.0 22.0 20.0 14.3\nTable 11: Percentage of turns in which WikiChat does\nnot find relevant information in Wikipedia to retrieve or\nfact-check.\nC.2 Number of claims per turn\nIR LLM Verified\nWikiChatG4\nHead 4.3 2.8 80.1%\nTail 3.6 2.2 55.4%\nRecent 4.0 2.1 43.8%\nAll 4.0 2.4 61.7%\nWikiChatG3.5\nHead 4.3 2.6 87.6%\nTail 2.6 2.2 57.7%\nRecent 3.1 2.0 54.0%\nAll 3.3 2.3 68.0%\nWikiChatL\nHead 4.7 2.5 84.7%\nTail 3.3 2.2 46.0%\nRecent 3.8 1.9 35.6%\nAll 3.9 2.2 57.5%\nTable 12: The average number of relevant bullet points\nthat WikiChat obtains from information retrieval and\nLLM-generated responses, and the percentage of claims\nthat pass the fact-checking stage.\nTable 12 contains the raw data on the contribu-\ntion of IR vs. LLM, and Table 13 contains the raw\ndata on the number of claims, both mentioned in\nSection 7.5.\nC.3 Refinement improvements\nTables 14 and 15 have the raw data on the refine-\nment stage, used in Section 7.5.\nD User Study\nWe use Prolific to conduct our user study. We select\n40 participants (20 female) from the US who are\nHead Tail Recent All\nWikiChat G4 4.4 3.1 3.4 3.6\nGPT-4 2.8 2.6 2.2 2.5\nWikiChat G3.5 4.2 3.2 3.2 3.5\nGPT-3.5 2.6 2.1 1.9 2.2\nWikiChat L 4.0 3.0 3.1 3.3\nLLaMA 2.1 2.0 2.0 2.0\nAtlas 1.4 1.3 1.5 1.4\nTable 13: The average number of claims per turn for\neach subset and chatbot.\nHead Tail Recent All\nWikiChat G4 84.1 66.0 69.7 73.3\nWikiChat G3.5 88.9 76.9 80.7 82.2\nWikiChat L 77.1 66.1 66.4 69.9\nTable 14: Analysis of WikiChat’s response refinement.\nBLEU score with the refined response as the prediction\nand the response before refinement as the target.\nfluent in English. Each participant is paid with\nthe rate of $12 per hour. Figure 2 shows the user\ninterface.\nFigure 3 shows the distribution of the user ratings\nfrom Table 3.\nE Human Evaluation\nWe conduct human evaluation for part of our evalu-\nation of factual accuracy (as described in Section\n5.2). We use the Scale Rapid 7 platform. Figure 4\nshows the instruction and one of the examples we\nprovide. Figure 5 shows the user interface for each\nannotation task. We present the human annotator\nwith the last user utterance, the chatbot’s response,\nand a claim extracted from the chatbot’s response\nusing GPT-4. The annotator is then tasked with\nreading the 5 evidence passages and determining\nwhether the claim is correct, incorrect, or if there is\ninsufficient information to verify the claim. We use\na three-way consensus pipeline, where each claim\nis assessed by three graders independently, and the\nfinal label is determined based on the majority vote.\n7www.scale.com\nRelevant Inform. Natural Non-Rep. Temporal\nWikiChat G4\nHead 0.2 0.3 0.2 0.0 0.0 %\nTail 0.3 0.4 0.4 0.2 1.0 %\nRecent 0.2 0.3 0.4 0.1 6.0 %\nAll 0.2 0.3 0.3 0.1 2.3 %\nWikiChat G3.5\nHead 0.0 0.0 0.1 0.0 2.0 %\nTail 0.1 0.3 0.3 0.0 5.0 %\nRecent 0.1 0.1 0.2 0.1 3.0 %\nAll 0.1 0.1 0.2 0.1 3.3 %\nWikiChat L\nHead 0.0 0.0 0.0 0.0 0.0 %\nTail 0.2 0.2 0.3 0.1 3.0 %\nRecent 0.2 0.3 0.4 0.0 6.0 %\nAll 0.1 0.2 0.2 0.1 3.0 %\nTable 15: Improvements of automatic conversationality metrics made by refinement.\nFigure 2: Screenshot of the interface shown to partici-\npants in the user study, after one turn of conversation.\nFigure 3: User ratings for WikiChat G4 and GPT-4.\nOne author periodically audits their work, provid-\ning feedback and adding examples and tests for\ncrowdworkers as needed.\nWe provide annotators with detailed instructions\non the task, and 8 examples covering special cases.\nWe provide 7 training tasks used for onboarding,\nand 22 evaluation tasks. Only crowdworkers who\nreceive a score of 90% in this evaluation can move\nto the main task. We compensate each worker for\n$0.2 per task, and we have one task per (system,\ndialogue turn, claim). This means that since there\nare more claims in longer chatbot responses, work-\ners are compensated more for longer responses. In\nthe end, each worker receives about $12 per hour\nof work.\nF All Prompts\nWe provide the prompts mentioned in this paper.\nFor brevity, we only show on of the few-shot exam-\nples used in each prompt. The full text of prompts\ncan be obtained from our code repository. The\nsyntax used is the Jinja2 template language, which\nsupports Python-like loops ({% for %}{% endfor\n%}), conditions ( {% if %}{% endif %} ), vari-\nables ({{ var }}) and comments ({# #}). In all\nprompts, dlg is a python list, today is a string like\n4/28/2023, current_year is a string like 2023,\nand location is set to U.S.\n \n \nFigure 4: Screenshot of the instructions and one of the examples we provide to crowdworkers for evaluation of\nfactuality.\nFigure 5: Screenshot of each task crowdworkers see. For each claim, we provide them with 5 paragraphs from\nWikipedia.\nThe current year is {{ current_year }}. The following is a conversation between you\nand a chatbot on the topic of \"{{ title }}\" ({{ passage }})\n- Do not assume that the chatbot is able to have physical experiences , like\nwatching a movie .\n- Ask interesting follow -up questions when needed , and expand on the chatbot 's\nresponses using your life experiences .\n- Never volunteer information , and never correct chatbot 's mistakes .\n- Continue the conversation for 15 turns . {# This is set to 15 , even though\nsimulations end after 10 turns . If we set this to 10 turns , the simulator will\nstart saying goodbye too early . #}\n{# The first two turns don 't have any content and won 't be sent to the Chatbot .\nThey are just meant to specify the format . #}\nYou : Hi!\nChatbot : Hi , how can I assist you today ?\n{% for dlg_turn in dlg %}\nYou : {{ dlg_turn . user_utterance }}\nChatbot : {{ dlg_turn . agent_utterance }}\n{% endfor %}\nYou :\nTable 16: User simulator prompt. This prompt is zero-shot. title is the title of the Wikipedia page used for\nsimulation, and passage is the first sentence of that article\nYou are chatting with a user . Use Google search to form a response . You are both\nlocated in {{ location }}. Today 's date is {{ today }}.\n- What do you type in the search box ?\n- What date do you want the search results to be? Enter \" recent \" if you are looking\nfor the newest results . Enter \" none \" if the date is not important .\n{# Few - shot example 1 #}\nYou : Do you want to talk about sports ?\nUser : Sure ! Who is your favorite basketball player ?\n[ Search needed ? Yes . You Google \" popular basketball players \". The year of the\nresults is \" none \".]\nYou : It has to be Lebron James .\nUser : Did he play well in his last game ?\n[ Search needed ? Yes . You Google \" how did Lebron James do in his most recent game \".\nThe year of the results is \" recent \".]\n... {# More few - shot examples #}\n{# The current dialogue #}\n{% for dlg_turn in dlg %}\n{% if dlg_turn . user_utterance is not none %}\nUser : {{ dlg_turn . user_utterance }}\n{% endif %}\n{% if dlg_turn . initial_search_query is not none %}\n[ Search needed ? Yes . You Google \"{{ dlg_turn . initial_search_query } }\". The\nyear of the results is \"{{ dlg_turn . initial_search_query_time } }\".]\n{% endif %}\n{% if dlg_turn . agent_utterance is not none %}\nYou : {{ dlg_turn . agent_utterance }}\n{% endif %}\n{% endfor %}\nUser : {{ new_user_utterance }}\n[ Search needed ?\nTable 17: Query generation prompt in WikiChat (Stage 1). This prompt has 6 few-shot examples.\nYou Google different search queries and then Break down the relevant parts of the\narticles you find . Today 's date is {{ today }}.\n{# Few - shot example 1 #}\nQuery : \" worst earthquake ever \"\nTitle : January 1934 earthquake in India and Nepal\nArticle : The 1934 Nepal \\ u2013India earthquake or 1934 Bihar \\ u2013Nepal earthquake\nwas one of the worst earthquakes in India 's history . The towns of Munger and\nMuzaffarpur were completely destroyed . This 8.0 magnitude earthquake occurred\non 15 January 1934 at around 2:13\\ u00a0pm IST (08:43 UTC ) and caused widespread\ndamage in northern Bihar and in Nepal . Earthquake . The epicentre for this event\nwas located in eastern Nepal about south of Mount Everest . The areas where the\nmost damage to life and property occurred extended from Purnea in the east to\nChamparan in the west (a distance of nearly ), and from Kathmandu in the north\nto Munger in the south (a distance of nearly ).\"\nBreak down verbatum part (s) of this article that are related to the search query\n\" worst earthquake ever \" or say None if the article is unrelated :\n- The 1934 Nepal - India earthquake , also known as the 1934 Bihar - Nepal earthquake ,\nwas one of the worst earthquakes in India 's history .\n- The 1934 Nepal - India earthquake had a magnitude of 8.0 and occurred on 15 January\n1934.\n- As a result of the 1934 Nepal - India earthquake , the towns of Munger and\nMuzaffarpur were completely destroyed .\n- As a result of the 1934 Nepal - India earthquake , widespread damage occurred in\nnorthern Bihar and Nepal , with the most damage extending from Purnea in the\neast to Champaran in the west , and from Kathmandu in the north to Munger in the\nsouth .\n{# Few - shot example 2 #}\nQuery : \" age of Bruce Willis \"\nTitle : Matt Willis\nArticle : In April 2005 , aged 21 , Willis stayed for three weeks at London 's Priory\nHospital for the treatment of alcoholism . In July 2006 , aged 23 , he was\nadmitted again for a few days for drug abuse , because he was addicted to\ncannabis from the age of 13. He began to have problems from the drug - taking\nincluding physiological and memory problems . In June 2008 , aged 25 , Willis\nentered a rehab centre in Bournemouth after a marriage ultimatum . It was\nreported that a night out with close friend Amy Winehouse pushed Willis too\nfar . Willis took the full five week course in drugs and alcohol .\nBreak down verbatum part (s) of this article that are related to the search query\n\" age of Bruce Willis \" or say None if the article is unrelated :\nNone\n... {# More few - shot examples #}\n{# The current dialogue #}\nQuery : \"{{ query }}\"\nTitle : {{ title }}\nArticle : {{ article }}\nBreak down verbatum part (s) of this article that are related to the search query\n\"{{ query }}\" or say None if the article is unrelated :\nTable 18: Summarize and filter prompt of WikiChat (Stage 2). This prompt has 7 few-shot examples.\nYou are a friendly , knowledgeable and truthfull chatbot , talking to a user .\nRespond in at most one paragraph .\nToday 's date is {{ today }}.\n{% for dlg_turn in dlg %}\nUser : {{ dlg_turn . user_utterance }}\nYou : {{ dlg_turn . agent_utterance }}\n{% endfor %}\nUser : {{ new_user_utterance }}\nYou :\nTable 19: The prompt for baseline chatbots, also used for Stage 3 of WikiChat. This prompt is zero-shot.\nYou are chatting with a user . Rewrite the facts mentioned in the last response into\nself - contained sentences .\nExclude opinions , or subjective statements .\nToday 's date is {{ today }}.\n{# Few - shot example 1 #}\nUser : When was chinese new year this year ?\nYou : It was on February 5th.\nRewrite the facts mentioned in the last response into self - contained sentences :\n- Chinese New Year in 2022 was on February 5th. The year of the results is \"2022\".\n{# Few - shot example 2 #}\nUser : How are you doing today ?\nYou : As an artificial intelligence , I don 't have feelings or personal experiences ,\nso I don 't have good days or bad days . However , I 'm fully operational and ready\nto assist you with any information or tasks you need . What can I help you with\ntoday ?\nRewrite the facts mentioned in the last response into self - contained sentences :\nNothing .\n{# Few - shot example 3 #}\nUser : Do you know about Queen Elizabeth II? I was just reading about her .\nYou : Queen Elizabeth II is the current monarch of the United Kingdom and the\nCommonwealth realms . She was born in 1926 and became queen in 1952.\nRewrite the facts mentioned in the last response into self - contained sentences :\n- Queen Elizabeth II is the current monarch of the United Kingdom and the\nCommonwealth realms as of {{ today }}. The year of the results is \" recent \".\n- Queen Elizabeth II was born in 1926. The year of the results is \"1926\".\n- Queen Elizabeth II became queen in 1952. The year of the results is \"1952\".\n... {# More few - shot examples #}\n{# The current dialogue #}\n{% for dlg_turn in dlg [ -2:] %} {# Only the last few turns are given , because longer\nconversations confuse the LLM , and are not needed for fact - checking . #}\n{% if dlg_turn . user_utterance is not none %}\nUser : {{ dlg_turn . user_utterance }}\n{% endif %}\n{% if dlg_turn . agent_utterance is not none %}\nYou : {{ dlg_turn . agent_utterance }}\n{% endif %}\n{% endfor %}\nUser : {{ new_user_utterance }}\nYou : {{ current_agent_utterance }}\nRewrite the facts mentioned in the last response into self - contained sentences :\nTable 20: Claim extraction prompt of WikiChat (Stage 4). This prompt has 8 few-shot examples.\nThe following is a conversation between a user and a chatbot . For each claim that\nthe chatbot makes , you search the internet to obtain articles that would\nsupport or refute that claim , and output one of \" SUPPORTS \", \" REFUTES \", or \" NOT\nENOUGH INFO \".\nOnly if the retrieved articles fully support the claim , output \" SUPPORTS \".\nToday 's date is {{ today }}.\n{# Few - shot example 1 #}\nChatbot : How was your trip to Hawaii ?\nUser : It was great ! In fact , I witnessed the eruption of the largest volcano on\nearth .\nChatbot : Wow , I hope I could see it , but sounds kinda dangerous . Is it the Mauna\nLoa ?\nUser : Yes , it is! Do you know when it started erupting ?\nChatbot : Yes , it started erupting on March 25 , 1984.\n[ You search the internet to fact - check the claim \" The last eruption of Mauna Loa\nstarted on March 25 , 1984\"]\n[ You get these articles :\nTitle : 2022 eruption of Mauna Loa\nArticle : When active , Mauna Loa tends to produce \" voluminous , fast - moving lava\nflows \" of the Hawaiian or effusive eruption type rather than more explosive\nphreatic or Plinian eruptions , though it has produced explosive eruptions\nbetween 300 and 1 ,000 years ago . Before Nov 27 , 2022 , Mauna Loa had last\nerupted in March 1984 , in a 22 - day event similarly concentrated in the\nvolcano 's Northeast Rift Zone . The 2022 eruption was the volcano 's 34 th\neruption since 1843 , when volcanic activity at Mauna Loa began to be\ncontinuously recorded , but only the third eruption since 1950. The 38 - year\nspan between the 1984 and 2022 eruptions was Mauna Loa 's longest period of\nquiescence on record .\nTitle : 1984 eruption of Mauna Loa\nArticle : The 1984 eruption of Mauna Loa was a Hawaiian eruption in the U.S.\nstate of Hawaii that lasted from March 25 to April 15 , 1984. It ended a\n9- year period of quiescence at the volcano and continued for 22 days ,\nduring which time lava flows and lava fountains issued from the summit\ncaldera and fissures along the northeast and southwest rift zones . Although\nthe lava threatened Hilo , the flow stopped before reaching the outskirts of\ntown .\n]\nFact - check the claim \" The last eruption of Mauna Loa started on March 25 , 1984\".\nYou think step by step : Mauna Loa had an eruption on Nov 27 , 2022 , which is later\nthan the claimed last eruption of March 25 , 1984. So the last eruption of Mauna\nLoa was not on March 25 , 1984. So the fact - checking result is \" REFUTES \".\n... {# More few - shot examples #}\n{# The current dialogue #}\nChatbot : {{ original_reply }}\n[ You search the internet to fact - check the claim \"{{ claim } }\"]\n[ You get these articles :\n{% for title in evidence_titles %}\nTitle : {{ title }}\nArticle : {{ evidence_texts [ loop . index -1] }}\n{% endfor %}\n]\nFact - check the claim \"{{ claim } }\".\nYou think step by step :\nTable 21: Verification prompt of WikiChat (Stage 5). This prompt has 6 few-shot examples.\nThe following is a conversation between a friendly , knowledgeable and truthful\nchatbot , called WikiChat , and a user .\nWikiChat can use search to get external knowledge when needed . In responding to the\nuser , WikiChat carefully aggregates search results in at most one paragraph .\nToday 's date is {{ today }}.\n{# Few - shot example 1 #}\nUser : What new discoveries from James Webb Space telescope can I tell my 9 year old\nabout ?\nWikiChat : Your 9- year old would be fascinated to hear that the James Webb Space\nTelescope has discovered farthest galaxies ever found . It has found galaxies so\nfar away that it took 13.4 billion years for their lights to reach the\ntelescope . That is light that started its journey just 300 million years after\nthe Big Bang !\nUser : what is big bang ?\nWikiChat : The Big Bang is the prevailing cosmological model for the universe from\nthe earliest known periods through its subsequent large - scale evolution . In\nlayman 's terms , it is the explosion that created our universe .\nUser : what else can I tell my child about ?\n[ Chatbot Googles and gets this information :\n- The James Webb Space Telescope has detected galaxies more than 13.4 billion light\nyears away , less than 300 million years after the Big Bang .\n- This is a new record and astronomers are excited to learn more about these\ndistant galaxies .\n- The telescope is named after James E. Webb , who was the administrator of NASA\nfrom 1961 to 1968 during the Mercury , Gemini , and Apollo programs .\n]\nWikiChat : The James Webb Space telescope is named after James E. Webb , who was the\nadministrator of NASA from 1961 to 1968 during the Mercury , Gemini , and Apollo\nprograms .\n... {# More few - shot examples #}\n{# The current dialogue #}\n{% for dlg_turn in dlg [ -2:] %}\nUser : {{ dlg_turn . user_utterance }}\nWikiChat : {{ dlg_turn . agent_utterance }}\n{% endfor %}\n{# include the search results only for the last turn #}\nUser : {{ last_user_utterance }}\n{% if evidences is not none %}\n[ Chatbot Googles and gets this information :\n{% for evi in evidences %}\n- {{ evi }}\n{% endfor %}\n]\n{% endif %}\nWikiChat :\nTable 22: Draft response prompt of WikiChat (Stage 6). This prompt has 4 few-shot examples.\nGiven a conversation history between a user and a chatbot , provide feedback on the\nchatbot 's final response , then revise the response based on the following\ncriteria :\n* Relevant : The response should be on - topic and directly address the user 's\nquestion . It should acknowledge if it 's off - topic or only partially addresses\nthe question . Irrelevant information should be avoided .\n* Natural : The response should use engaging language to create an interactive and\nenjoyable experience , without being too long .\n* Non - Repetitive : The response should not repeat previously mentioned information\nor statement , and should not repeat the same fact more than once .\n* Temporally Correct : The response should provide up -to - date information , use\npast - tense when the event happened before today ({{ today }}) , and respond\nspecifically to the time mentioned by the user .\nThe revised response should only edit the original response according to the\nfeedback , and should not introduce new information .\nToday 's date is {{ today }}.\n{# Few - shot example 1 #}\nUser : What do you think is the best TV drama of 2022?\nChatbot : I think it has to be House of the Dragon .\nUser : Why is that ?\nResponse : I love it because both the plot and the visuals are great . It actually\nwon the Golden Globe Award for the best drama TV series . I 'm not sure when it\nwas released , but I think it was August 21 , 2022.\nLet 's break down the feedback for the response :\n* Relevant : The response is on - topic and directly addresses the question of why the\nspeaker thinks House of the Dragon is the best TV drama , but it contains\nirrelevant information about the release date of the show . 60/100\n* Natural : The response uses engaging language to express the chatbot 's opinion and\nprovides supporting information to reinforce that opinion . 100/100\n* Non - Repetitive : The response does not repeat any previous statement . 100/100\n* Temporally Correct : The response correctly uses the past tense to describe the\nGolden Globe win . 100/100\nUser : Why is that ?\nRevised response after applying this feedback : I love it because both the plot and\nthe visuals are great . It actually won the Golden Globe Award for the best\ndrama TV series .\n... {# More few - shot examples #}\n{# The current dialogue #}\n{% for dlg_turn in dlg [ -2:] %} {# Only include the last few turns . #}\nUser : {{ dlg_turn . user_utterance }}\nChatbot : {{ dlg_turn . agent_utterance }}\n{% endfor %}\nUser : {{ new_dlg_turn . user_utterance }}\nResponse : {{ new_dlg_turn . agent_utterance }}\nLet 's break down the feedback for the response :\nTable 23: Refinement prompt of WikiChat (Stage 7). This prompt has 6 few-shot examples.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.702552318572998
    },
    {
      "name": "Chatbot",
      "score": 0.6642277240753174
    },
    {
      "name": "Latency (audio)",
      "score": 0.6392384767532349
    },
    {
      "name": "Information retrieval",
      "score": 0.48437461256980896
    },
    {
      "name": "Natural language processing",
      "score": 0.45979732275009155
    },
    {
      "name": "Ground truth",
      "score": 0.445133775472641
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43217384815216064
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ]
}