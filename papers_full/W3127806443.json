{
  "title": "PipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers",
  "url": "https://openalex.org/W3127806443",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2682010504",
      "name": "He, Chaoyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1898662817",
      "name": "Li Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3163217959",
      "name": "Soltanolkotabi, Mahdi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221515583",
      "name": "Avestimehr, Salman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2969388332",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3037585619",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3096403968",
    "https://openalex.org/W3031276512",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3025935268",
    "https://openalex.org/W2807716166",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3114268635",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W3020605687",
    "https://openalex.org/W2950500611",
    "https://openalex.org/W2977720775",
    "https://openalex.org/W2083842231",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2991040477"
  ],
  "abstract": "The size of Transformer models is growing at an unprecedented pace. It has only taken less than one year to reach trillion-level parameters after the release of GPT-3 (175B). Training such models requires both substantial engineering efforts and enormous computing resources, which are luxuries most research teams cannot afford. In this paper, we propose PipeTransformer, which leverages automated and elastic pipelining and data parallelism for efficient distributed training of Transformer models. PipeTransformer automatically adjusts the pipelining and data parallelism by identifying and freezing some layers during the training, and instead allocates resources for training of the remaining active layers. More specifically, PipeTransformer dynamically excludes converged layers from the pipeline, packs active layers into fewer GPUs, and forks more replicas to increase data-parallel width. We evaluate PipeTransformer using Vision Transformer (ViT) on ImageNet and BERT on GLUE and SQuAD datasets. Our results show that PipeTransformer attains a 2.4 fold speedup compared to the state-of-the-art baseline. We also provide various performance analyses for a more comprehensive understanding of our algorithmic and system-wise design. We also develop open-sourced flexible APIs for PipeTransformer, which offer a clean separation among the freeze algorithm, model definitions, and training accelerations, hence allowing it to be applied to other algorithms that require similar freezing strategies.",
  "full_text": "PipeTransformer: Automated Elastic Pipelining for\nDistributed Training of Transformers\nChaoyang He 1 Shen Li 2 Mahdi Soltanolkotabi 1 Salman Avestimehr1\nAbstract\nThe size of Transformer models is growing at an\nunprecedented rate. It has taken less than one\nyear to reach trillion-level parameters since the\nrelease of GPT-3 (175B). Training such models\nrequires both substantial engineering efforts and\nenormous computing resources, which are luxu-\nries most research teams cannot afford. In this\npaper, we propose PipeTransformer, which\nleverages automated elastic pipelining for efﬁ-\ncient distributed training of Transformer models.\nIn PipeTransformer, we design an adaptive\non the ﬂy freeze algorithm that can identify and\nfreeze some layers gradually during training, and\nan elastic pipelining system that can dynamically\nallocate resources to train the remaining active\nlayers. More speciﬁcally, PipeTransformer\nautomatically excludes frozen layers from the\npipeline, packs active layers into fewer GPUs,\nand forks more replicas to increase data-parallel\nwidth. We evaluate PipeTransformer us-\ning Vision Transformer (ViT) on ImageNet and\nBERT on SQuAD and GLUE datasets. Our results\nshow that compared to the state-of-the-art base-\nline, PipeTransformer attains up to 2.83-\nfold speedup without losing accuracy. We also\nprovide various performance analyses for a more\ncomprehensive understanding of our algorithmic\nand system-wise design. Finally, we have modu-\nlarized our training system with ﬂexible APIs and\nmade the source code publicly available.\n1. Introduction\nLarge Transformer models (Brown et al., 2020; Lepikhin\net al., 2020) have powered accuracy breakthroughs in both\nnatural language processing and computer vision. GPT-3 hit\na new record high accuracy for nearly all NLP tasks. Vision\nTransformer (ViT) (Dosovitskiy et al., 2020) also achieved\n89% top-1 accuracy in ImageNet, outperforming state-of-\n1University of Southern California 2Facebook AI. Correspon-\ndence to: Chaoyang He <chaoyang.he@usc.edu>.\nPreprint. Under Review.\nthe-art convolutional networks ResNet-152 (He et al., 2016)\nand EfﬁcientNet (Tan & Le, 2019). To tackle the growth in\nmodel sizes, researchers have proposed various distributed\ntraining techniques, including parameter servers (Li et al.,\n2014; Jiang et al., 2020; Kim et al., 2019), pipeline paral-\nlel (Huang et al., 2019; Park et al., 2020; Narayanan et al.,\n2019), intra-layer parallel (Lepikhin et al., 2020; Shazeer\net al., 2018; Shoeybi et al., 2019), and zero redundancy data\nparallel (Rajbhandari et al., 2019).\nT0 (0% trained)\n T1 (35% trained) T2 (75% trained)\n T3 (100% trained)\nSimilarity score\nLayer (end of training) Layer (end of training) Layer (end of training) Layer (end of training)\nFigure 1.Interpretable Freeze Training: DNNs converge bottom\nup (Results on CIFAR10 using ResNet). Each pane shows layer-\nby-layer similarity using SVCCA (Raghu et al., 2017).\nExisting distributed training solutions, however, only study\nscenarios where all model weights are required to be opti-\nmized throughout the training (i.e., computation and com-\nmunication overhead remains relatively static over different\niterations). Recent works on freeze training (Raghu et al.,\n2017; Morcos et al., 2018; Shen et al., 2020) suggest that\nparameters in neural networks usually converge from the\nbottom-up (i.e., not all layers need to be trained all the\nway through training). Figure 1 shows an example of how\nweights gradually stabilize during training in this approach.\nThis observation motivates us to utilize freeze training for\ndistributed training of Transformer models to accelerate\ntraining by dynamically allocating resources to focus on a\nshrinking set of active layers. Such a layer freezing strategy\nis especially pertinent to pipeline parallelism, as exclud-\ning consecutive bottom layers from the pipeline can reduce\ncomputation, memory, and communication overhead.\nIn this paper, we propose PipeTransformer, an elastic\npipelining training acceleration framework that automati-\ncally reacts to frozen layers by dynamically transforming\nthe scope of the pipelined model and the number of pipeline\nreplicas. To the best of our knowledge, this is the ﬁrst paper\nthat studies layer freezing in the context of both pipeline and\ndata-parallel training. Figure 2 demonstrates the beneﬁts\narXiv:2102.03161v2  [cs.LG]  12 Feb 2021\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nTransformer Models for NLP (BERT) and CV (ViT)\n“Hi, PipeTransformer, \nWhat does your name mean?”\nT0\nActive Layers Active LayersFrozen Layers Active LayersFrozen Layers Frozen Layers\nFP\nBP\nDP\nPipeline 0 (server 0)\n0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\nFP\nBP\nPipeline 1 (server 1)\n8\n 9\n 10\n 11\n 12\n 13\n 14\n 15\n0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n8\n 9\n 10\n 11\n 12\n 13\n 14\n 15\n0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n8\n 9\n 10\n 11\n 12\n 13\n 14\n 15\n0\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n8\n 9\n 10\n 11\n 12\n 13\n 14\n 15\n2. AutoPipe: Elastic pipelining\n1. Freeze Algorithm\n3. AutoDP: Spawning More Pipeline Replicas\nCache\n4. AutoCache: Cross-process caching T1 T2 T3\n“Pipeline Transformation for Transformer Models”\nDP DP DP DP DP DP DP\nFigure 2.The process of PipeTransformer’s automated and elastic pipelining to accelerate distributed training of Transformer models\nof such a combination. First, by excluding frozen layers\nfrom the pipeline, the same model can be packed into fewer\nGPUs, leading to both fewer cross-GPU communications\nand smaller pipeline bubbles. Second, after packing the\nmodel into fewer GPUs, the same cluster can accommodate\nmore pipeline replicas, increasing the width of data paral-\nlelism. More importantly, the speedups acquired from these\ntwo beneﬁts are multiplicative rather than additive, further\naccelerating the training.\nThe design of PipeTransformer faces four major chal-\nlenges. First, the freeze algorithm must make on the ﬂy and\nadaptive freezing decisions; however, existing work (Raghu\net al., 2017) only provides a posterior analysis tool. Sec-\nond, the efﬁciency of pipeline re-partitioning results is\ninﬂuenced by multiple factors, including partition gran-\nularity, cross-partition activation size, and the chunking\n(the number of micro-batches) in mini-batches, which re-\nquire reasoning and searching in a large solution space.\nThird, to dynamically introduce additional pipeline repli-\ncas, PipeTransformer must overcome the static nature of\ncollective communications and avoid potentially complex\ncross-process messaging protocols when onboarding new\nprocesses (one pipeline is handled by one process). Finally,\ncaching can save time for repeated forward propagation\nof frozen layers, but it must be shared between existing\npipelines and newly added ones, as the system cannot afford\nto create and warm up a dedicated cache for each replica.\nPipeTransformer is designed with four core building\nblocks to address the aforementioned challenges. First, we\ndesign a tunable and adaptive algorithm to generate signals\nthat guide the selection of layers to freeze over different\niterations (Section 3.1). Once triggered by these signals, our\nelastic pipelining module AutoPipe, then packs the remain-\ning active layers into fewer GPUs by taking both activation\nsizes and variances of workloads across heterogeneous par-\ntitions (frozen layers and active layers) into account. It then\nsplits a mini-batch into an optimal number of micro-batches\nbased on prior proﬁling results for different pipeline lengths\n(Section 3.2). Our next module, AutoDP, spawns additional\npipeline replicas to occupy freed-up GPUs and maintains hi-\nerarchical communication process groups to attain dynamic\nmembership for collective communications (Section 3.3).\nOur ﬁnal module, AutoCache, efﬁciently shares activations\nacross existing and new data-parallel processes and auto-\nmatically replaces stale caches during transitions (Section\n3.4).\nOverall, PipeTransformer combines the Freeze\nAlgorithm, AutoPipe, AutoDP and AutoCache mod-\nules to provide a signiﬁcant training speedup. We eval-\nuate PipeTransformer using Vision Transformer (ViT)\non ImageNet and BERT on GLUE and SQuAD datasets.\nOur results show that PipeTransformer attains up to\n2.83-fold speedup without losing accuracy. We also provide\nvarious performance analyses for a more comprehensive\nunderstanding of our algorithmic and system-wise design.\nFinally, we have also developed open-source ﬂexible APIs\nfor PipeTransformer which offer a clean separation\namong the freeze algorithm, model deﬁnitions, and train-\ning accelerations, allowing for transferability to other algo-\nrithms that require similar freezing strategies. The source\ncode is made publicly available.\n2. Overview\n2.1. Background and Problem Setting\nSuppose we aim to train a massive model in a distributed\ntraining system where the hybrid of pipelined model paral-\nlelism and data parallelism is used to target scenarios where\neither the memory of a single GPU device cannot hold the\nmodel, or if loaded, the batch size is small enough to avoid\nrunning out of memory. More speciﬁcally, we deﬁne our\nsettings as follows:\nTraining task and model deﬁnition.We train Transformer\nmodels (e.g., Vision Transformer (Dosovitskiy et al., 2020),\nBERT (Devlin et al., 2018)) on large-scale image or text\ndatasets. The Transformer model Fhas Llayers, in which\nthe ith layer is composed of a forward computation function\nfi and a corresponding set of parameters, wi. With this deﬁ-\nnition, the overall model isF= f0(w0)◦... ◦fL−1(wL−1).\nThe model size is S, and the batch size is set to Nbs.\nTraining infrastructure. Assume the training infrastruc-\nture contains a GPU cluster that has N GPU servers (i.e.\nnodes). Each node has I GPUs. Our cluster is homoge-\nneous, meaning that each GPU and server have the same\nhardware conﬁguration. Each GPU’s memory capacity is\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nMGPU. Servers are connected by a high bandwidth network\ninterface such as InfiniBand interconnect.\nPipeline parallelism. In each machine, we load a model\nFinto a pipeline Pwhich has K partitions (K also rep-\nresents the pipeline length). The kth partition pk consists\nof consecutive layers pk = fi(wi) ◦... ◦fj(wj), and\nP= p0 ◦... ◦pK−1. We assume each partition is handled\nby a single GPU device. 1 ≤K ≤I, meaning that we\ncan build multiple pipelines for multiple model replicas in a\nsingle machine. We assume all GPU devices in a pipeline\nbelong to the same machine. Our pipeline is a synchronous\npipeline, which does not involve stale gradients, and the\nnumber of micro-batches is M. In the Linux OS, each\npipeline is handled by a single process. We refer the reader\nto GPipe (Huang et al., 2019) for more details.\nData parallelism. DDP (Li et al., 2020) is a cross-machine\ndistributed data parallel process group within R parallel\nworkers. Each worker is a pipeline replica (a single process).\nThe rth worker’s index (ID) is rankr. For any two pipelines\nP(ri) and P(rj) in DDP, ri and rj can belong to either the\nsame GPU server or different GPU servers, and they can\nexchange gradients with the AllReduce algorithm.\nUnder these settings, our goal is to accelerate training by\nleveraging freeze training, which does not require all lay-\ners to be trained throughout the duration of the training.\nAdditionally, it may help save computation, communica-\ntion, memory cost, and potentially prevent overﬁtting by\nconsecutively freezing layers. However, these beneﬁts can\nonly be achieved by overcoming the four challenges of de-\nsigning an adaptive freezing algorithm, dynamical pipeline\nre-partitioning, efﬁcient resource reallocation, and cross-\nprocess caching, as discussed in the introduction. We next\ndescribe our overall design, named PipeTransformer,\nwhich can address these challenges.\n2.2. Overall Design\nPipeTransformer co-designs an on the ﬂy freeze algo-\nrithm and an automated elastic pipelining training system\nthat can dynamically transform the scope of the pipelined\nmodel and the number of pipeline replicas. The overall\nsystem architecture is illustrated in Figure 3. To support\nPipeTransformer’s elastic pipelining, we maintain a\ncustomized version of PyTorch Pipe (Kim et al., 2020).\nFor data parallelism, we use PyTorch DDP (Li et al., 2020)\nas a baseline. Other libraries are standard mechanisms of\nan operating system (e.g., multi-processing) and thus\navoid specialized software or hardware customization re-\nquirements. To ensure the generality of our framework, we\nhave decoupled the training system into four core compo-\nnents: freeze algorithm, AutoPipe, AutoDP, and\nAutoCache. The freeze algorithm (grey) samples indica-\ntors from the training loop and makes layer-wise freezing\nDeep Learning Training Engine (PyTorch)\nAutoCache\n(cross process)\nFreeze\nAlgorithm\nPipeline Parallel\nCUDA NCCL / GLOO Multi Processing\nShared Memory\n(Cross process)\nAutoDP\nAutoPipe\nadd new pipelines\nTransform\nData Distributed Parallel\nRedistribute\n dataset\nSample training \ninformation as indicator\n(Progress, gradient, etc)\nDataset\npipeline 0 at timestep 0\npipeline 0 at timestep 1 Pipeline 1 at timestep 1\nnotify\nDataset Dataset\n# of frozen layer\nchanged? \nPipeline length\n has been changed?\nFigure 3.Overview of PipeTransformer Training System\ndecisions, which will be shared with AutoPipe (green).\nAutoPipe is an elastic pipeline module that speeds up\ntraining by excluding frozen layers from the pipeline and\npacking the active layers into fewer GPUs (pink), leading to\nboth fewer cross-GPU communications and smaller pipeline\nbubbles. Subsequently, AutoPipe passes pipeline length\ninformation to AutoDP (purple), which then spawns more\npipeline replicas to increase data-parallel width, if possible.\nThe illustration also includes an example in which AutoDP\nintroduces a new replica (purple). AutoCache (orange\nedges) is a cross-pipeline caching module, as illustrated by\nconnections between pipelines. The source code architec-\nture is aligned with Figure 3 for readability and generality.\n3. Algorithm and System Design\nThis section elaborates on the four main algorithmic and\nsystem-wise design components of PipeTransformer.\n3.1. Freeze Algorithm\nThe freeze algorithm must be lightweight and able to make\ndecisions on the ﬂy. This excludes existing layer-wise train-\ning approaches such as SVCCA (Raghu et al., 2017) which\nrequire full training states and heavy posterior measure-\nments. We propose an adaptive on the ﬂy freeze algorithm\nto deﬁne L(T)\nfrozen at timestep T as follows:\nmin\n(\nL(T−1)\nfrozen + α(L−L(T−1)\nfrozen ), argmin\nℓ∈{L(T−1)\nfrozen ,...,L}\ng(T)\nℓ\n\n)\nwhere T ≥1, L(0)\nfrozen = 0, and α∈(0,1)\n(1)\nwhere g(T)\nℓ is the gradient for layer ℓ at iteration T, andg(T)\nℓ\nis its norm. The intuition behind the second term in\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nthe min function is that the layer with the smallest gradient\nnorm converges ﬁrst. To stabilize training, we enforce an\nupper bound L(T−1)\nfrozen + α(L−L(T−1)\nfrozen ) for the number of\nfrozen layers, which is a geometric sequence containing a\nhyper-parameter α. This essentially freezes an αfraction\nof the remaining active layers. To illustrate the impact of\nα, we rewrite the equation as: L(T)\nfrozen = (1−α)T[ αL\n1−α +∑T\nt=2\nαL\n(1−α)t ] (see Appendix for the derivation), and draw\nthe curve of this function in Figure 4. As we can see, a\nlarger αleads to a more aggressive layer freezing. Therefore,\nEquation 1 calculates the number of frozen layers at timestep\nT using both the gradient norm and a tunable argument α.\n0 2 4 6 8\nEpoch\n0\n5\n10\n15\n20\n25Frozen Layer Number\nFreeze algorithm\nalpha=0.1\nalpha=0.2\nalpha=0.3\nalpha=0.5\nFigure 4.Freeze Algorithm Using Different α\nThe αparameter controls the trade-off between accuracy\nand training speed. This algorithm is also analogous to\nlearning rate (LR) decay. Both algorithms use a scheduler\nfunction during training, and take the progress of training\nas an indicator. The difference is that the above freeze\nalgorithm also takes gradient norm into account, making\nthe algorithm simple and effective. Other freezing strategies\ncan be easily plugged into the our training system. Indeed,\nwe plan to investigate other strategies in our future work.\n3.2. AutoPipe: Elastic Pipelining\nTriggered by the freeze algorithm,AutoPipe can accelerate\ntraining by excluding frozen layers from the pipeline and\npacking the active layers into fewer GPUs. This section\nelaborates on the key components ofAutoPipe that dynam-\nically partition pipelines, minimize the number of pipeline\ndevices and optimize mini-batch chunk size accordingly.\nAlgorithm 1 presents the pseudo-code.\n3.2.1. B ALANCED PIPELINE PARTITIONING\naddition Layer Norm\nMulti-Head \nAttention FFN\naddition\npartition k-1\n… …\npartition kpartition k-2\nIntermediate output\nLayer Norm\nFigure 5.Partition boundary is in the middle of a skip connection\nBalancing computation time across partitions is critical to\npipeline training speed, as skewed workload distributions\nacross stages can lead to stragglers, forcing devices with\nlighter workloads to wait (demonstrated by Section 4.3.1).\nHowever, maintaining optimally balanced partitions does\nnot guarantee the fastest training speed because other factors\nalso play a crucial role:\n1. Cross-partition communication overhead. Placing a par-\ntition boundary in the middle of a skip connection leads to\nadditional communications since tensors in the skip connec-\ntion must now be copied to a different GPU. For example,\nwith BERT partitions in ﬁgure 5, partition kmust take inter-\nmediate outputs from both partitionk−2 and partition k−1.\nIn contrast, if the boundary is placed after the addition\nlayer, the communication overhead between partition k−1\nand kis visibly smaller. Our measurements show that hav-\ning cross-device communication is more expensive than\nhaving slightly imbalanced partitions (see the Appendix).\nTherefore, we do not consider breaking skip connections\n(highlighted separately as an entire attention layer fATTi and\nMLP layer fMLPi in green at line 7 in Algorithm 1).\n2. Frozen layer memory footprint. During training,\nAutoPipe must recompute partition boundaries several\ntimes to balance two distinct types of layers: frozen lay-\ners and active layers. The frozen layer’s memory cost is a\nfraction of that in active layers, given that the frozen layer\ndoes not need backward activation maps, optimizer states,\nand gradients. Instead of launching intrusive proﬁlers to\nobtain thorough metrics on memory and computational cost,\nwe deﬁne a tunable cost factor λfrozen to estimate the mem-\nory footprint ratio of a frozen layer over the same active\nlayer. Based on empirical measurements in our experimental\nhardware, we set λfrozen to 1\n6 .\nBased on the above two considerations,AutoPipe balances\npipeline partitions based on parameter sizes. More specif-\nically, AutoPipe uses a greedy algorithm to allocate all\nfrozen and active layers to evenly distribute partitioned sub-\nlayers into K GPU devices. Pseudo code is described as\nthe load balance() function in Algorithm 1. The frozen\nlayers are extracted from the original model and kept in\na separate model instance Ffrozen in the ﬁrst device of a\npipeline. Note that the partition algorithm employed in this\npaper is not the only option; PipeTransformer is modu-\nlarized to work with any alternatives.\n3.2.2. P IPELINE COMPRESSION\nPipeline compression helps to free up GPUs to accommo-\ndate more pipeline replicas and reduce the number of cross-\ndevice communications between partitions. To determine\nthe timing of compression, we can estimate the memory cost\nof the largest partition after compression, and then compare\nit with that of the largest partition of a pipeline at timestep\nT = 0. To avoid extensive memory proﬁling, the compres-\nsion algorithm uses the parameter size as a proxy for the\ntraining memory footprint. Based on this simpliﬁcation, the\ncriterion of pipeline compression is as follows:\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nAlgorithm 1 AutoPipe Algorithm\n1: Input: model F, layer number Land Lfrozen, pipeline length\nK, frozen layer cost factor λfrozen\n2: Return: model Ffrozen, model Fpipe, updated K;\n3: def m partition(F,L, Lfrozen): //see 3.2.1\n4: Ffrozen = Sequential(); model size Sfrozen = 0\n5: Fpipe = Sequential(); per-layer size Spipe = []\n6: for layer index = Lfrozen to Ldo\n7: fATTi,fMLPi ←fi\n8: Fpipe.append(fATTi); Spipe.append(m size(fATTi))\n9: Fpipe.append(fMLPi); Spipe.append(m size(fMLPi))\n10: return Ffrozen,Sfrozen,Fpipe,Spipe\n11: def load balance(Fpipe, Spipe, K): //Section 3.2.1\n12: BL=dict(), BS=dict() // balanced L and S\n13: Lassigned = 0; Stotal = sum(Spipe)\n14: for partition index = kto Kdo\n15: mean=Stotal/(K- k);\n16: var=np.var(Spipe[Lassigned:])/(K- k)\n17: for sublayer index i = Lassigned to len(Spipe) do\n18: Sk = Spipe[i]\n19: criterion=BS[i]-Sfrozen(1.0- λfrozen )+Sk\n20: if criterion < mean + var then\n21: BS+=Sk; BL+=1; Lassigned+=1; Stotal-=Sk\n22: else\n23: break\n24: return BL, BS\n25: Ffrozen,Sfrozen,Fpipe,Spipe = m partition(F,L, Lfrozen)\n26: while K ≥2 do\n27: BL, BS = load balance(Fpipe, Spipe, K/2)\n28: BS[0] -= Sfrozen(1.0 - λfrozen);\n29: M(T)\nGPU = max(BS) //Equation 2\n30: if M(T)\nGPU <M (0)\nGPU then\n31: K=K/2\n32: else\n33: break\n34: load Ffrozen and Fpipe to KGPUs using BS and BL\n35: Pipe(Fpipe, chunks= get optimal chunks (K))\ncompress the pipeline if M(T)\nGPU ≤M(0)\nGPU\nwhere M(T)\nGPU ⇔ max\nk∈{0,···,K−1}\nSpk\n(2)\nOnce the freeze notiﬁcation is received, AutoPipe will al-\nways attempt to divide the pipeline lengthKby 2 (e.g., from\n8 to 4, then 2). By using K\n2 as the input, the compression\nalgorithm can verify if the result satisﬁes the criterion in\nEquation (1). Pseudo code is shown in lines 25-33 in Algo-\nrithm 1. Note that this compression makes the acceleration\nratio exponentially increase during training, meaning that if\na GPU server has a larger number of GPUs (e.g., more than\n8), the acceleration ratio will be further ampliﬁed.\nF0,0 F0,1 F0,2 F0,3\nF1,0 F1,1 F1,2 F1,3\nF2,0 F2,1 F2,2 F2,3\nF3,0 F3,1 F3,2 F3,3 B3,0 B3,1 B3,2 B3,3\nB2,0 B2,1 B2,2 B2,3\nB1,0 B1,1 B1,2 B1,3\nB0,0 B0,1 B0,2\nU1\nU3\nB0,3 U0GPU0\nU2\nGPU1\nGPU2\nGPU3\nK - 1\nK - 1\nK - 1\nK - 1\nK - 1\nK - 1\nK - 1\nK  is pipeline length (devices)\nFigure 6.Pipeline Bubble: Fd,b, Bd,b, and Ud denote forward,\nbackward, and the optimizer update of micro-batch bon device\nd, respectively. The total bubble size in each iteration is (K−1)\ntimes per micro-batch forward and backward cost.\nAdditionally, such a technique can also speed up training by\nshrinking the size of pipeline bubbles. To explain bubble\nsizes in a pipeline, Figure 6 depicts how 4 micro-batches\nrun through a 4-device pipeline ( K = 4). In general, the\ntotal bubble size is (K−1) times per micro-batch forward\nand backward cost (for further explanation, please refer to\nAppendix.\nTherefore, it is clear that shorter pipelines have smaller\nbubble sizes.\n3.2.3. D YNAMIC NUMBER OF MICRO -BATCHES\nPrior pipeline parallel systems use a ﬁxed number of micro-\nbatches per mini-batch (M). GPipe suggests M ≥4 ×K,\nwhere Kis the number of partitions (pipeline length). How-\never, given that thatPipeTransformer dynamically con-\nﬁgures K, we ﬁnd it to be sub-optimal to maintain a static\nM during training. Moreover, when integrated with DDP,\nthe value of M also has an impact on the efﬁciency of DDP\ngradient synchronizations. Since DDP must wait for the last\nmicro-batch to ﬁnish its backward computation on a pa-\nrameter before launching its gradient synchronization, ﬁner\nmicro-batches lead to a smaller overlap between compu-\ntation and communication (see Appendix for illustration).\nHence, instead of using a static value, PipeTransformer\nsearches for optimal M on the ﬂy in the hybrid of DDP envi-\nronment by enumerating M values ranging from Kto 6K.\nFor a speciﬁc training environment, the proﬁling needs only\nto be done once (see Algorithm 1 line 35). Section 4 will\nprovide performance analyses of M selections.\n3.3. AutoDP: Spawning More Pipeline Replicas\nAs AutoPipe compresses the same pipeline into fewer\nGPUs, AutoDP can automatically spawn new pipeline repli-\ncas to increase data-parallel width.\nDespite the conceptual simplicity, subtle dependencies on\ncommunications and states require careful design. The chal-\nlenges are threefold: 1. DDP Communication: Collective\ncommunications in PyTorch DDP requires static member-\nship, which prevents new pipelines from connecting with\nexisting ones; 2. State Synchronization: newly activated\nprocesses must be consistent with existing pipelines in the\ntraining progress (e.g., epoch number and learning rate),\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nweights and optimizer states, the boundary of frozen layers,\nand pipeline GPU range; 3. Dataset Redistribution: the\ndataset should be re-balanced to match a dynamic number\nof pipelines. This not only avoids stragglers but also ensures\nthat gradients from all DDP processes are equally weighted.\n0 1 3 4 5 6 7\n8 9\nactive training process group\nmessage process group\nmessage between groups:\n1. progress of training\n2. Pipelining info\n2\n151413121110\n0 1 3 4 5 6 7\n8 9\n2\n151413121110\nT0 T1\nFigure 7.AutoDP: handling dynamical data parallel with messag-\ning between double process groups (Process 0-7 belong to machine\n0, while process 8-15 belong to machine 1)\nTo tackle these challenges, we create double communica-\ntion process groups for DDP. As in the example shown in\nFigure 7, the message process group (purple) is responsi-\nble for light-weight control messages and covers all pro-\ncesses, while the active training process group (yellow)\nonly contains active processes and serves as a vehicle for\nheavy-weight tensor communications during training. The\nmessage group remains static, whereas the training group is\ndismantled and reconstructed to match active processes. In\nT0, only process 0 and 8 are active. During the transition\nto T1, process 0 activates processes 1 and 9 (newly added\npipeline replicas) and synchronizes necessary information\nmentioned above using the message group. The four active\nprocesses then form a new training group, allowing static\ncollective communications adaptive to dynamic member-\nships. To redistribute the dataset, we implement a variant\nof DistributedSampler that can seamlessly adjust data\nsamples to match the number of active pipeline replicas.\nThe above design also naturally helps to reduce DDP com-\nmunication overhead. More speciﬁcally, when transitioning\nfrom T0 to T1, processes 0 and 1 destroy the existing DDP\ninstances, and active processes construct a new DDP train-\ning group using Fpipe (AutoPipe stores Ffrozen and Fpipe\nseparately, introduced in Section 3.2.1). Discussion of com-\nmunication cost can be found in Appendix.\n3.4. AutoCache: Cross-pipeline Caching\nCaching activation maps from frozen layers can help further\nspeed up training. This idea appears to be straightforward,\nbut several caveats must be carefully addressed.\nCross-process caching. The cache must be shared across\nprocesses in real time, as creating and warming up a dedi-\ncated cache for each model replica slow down the training.\nThis is achieved by spawning a dedicated daemon process\nto hold cache in shared memory that all training processes\ncan access in real time. Figure 8 shows an example of the\ntransition from T1 to T2, assuming T1 freezes 3 layers, T2\nfreezes 4 layers, and 5 layers remain active in T2. Imme-\nDisk storage\n3\n3\n4 5\npipeline 0 \n(process 0)\nnewly added pipeline1 \n(process 1)\n9\ncross process \ncaching sharing12 automating \nthe timing of caching\nCaching Daemon\nCPU Host memory\nT1 T2\nT1\nT2\nFigure 8.AutoCache\ndiately after the transition by AutoDP, the cache still holds\ncached activations from layer 3, which must be replaced by\nactivations from layer 7. Therefore, all processes read their\ncorresponding activations from the cache, feed them to the\nnext 4 layers to compute activations for layer 7, then replace\nthe existing cache with new activations for their samples\naccordingly. In this way, AutoCache can gradually update\ncached activations without running any sample through any\nfrozen layers twice.\nWhen the activations are too large to reside on CPU memory,\nAutoCache will also swap them to the disk and perform pre-\nfetching automatically. More details on the cross-process\ncache design can be found in the Appendix.\nTiming of cache is also important, as the cache can be\nslower than running the real forward propagation, especially\nif frozen layers are few and activations are large. To ensure\nthat our training system can adapt to different hardware,\nmodel architecture, and batch size settings, AutoCache\nalso contains a proﬁler that helps evaluate the appropriate\ntransition to enable caching, and it only employs cached\nactivations when the proﬁler suggests caching can speed\nup the forward pass. Performance analysis is provided at\nSection 4.3.5.\n4. Experiments\nThis section ﬁrst summarizes experiment setups and then\nevaluates PipeTransformer using computer vision and\nnatural language processing tasks. More comprehensive\nresults can be found in the Appendix.\n4.1. Setup\nHardware. Experiments were conducted on 2 identical\nmachines connected by InﬁniBand CX353A (5GB/s), where\neach machine is equipped with 8 NVIDIA Quadro RTX\n5000 (16GB GPU memory). GPU-to-GPU bandwidth\nwithin a machine (PCI 3.0, 16 lanes) is 15.754GB/s.\nImplementation. We used PyTorch Pipe as a building\nblock, which has not yet been ofﬁcially released at the time\nof writing of this paper. Hence, we used the developer ver-\nsion 1.8.0.dev20201219. The BERT model deﬁnition,\nconﬁguration, and related tokenizer are fromHuggingFace\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\n3.5.0. We implemented Vision Transformer using PyTorch\nby following its TensorFlow implementation. More details\ncan be found in our source code.\nModels and Datasets. Experiments employ two represen-\ntative Transformers in CV and NLP: Vision Transformer\n(ViT) and BERT. ViT was run on an image classiﬁcation\ntask, initialized with pre-trained weights on ImageNet21K\nand ﬁne-tuned on ImageNet and CIFAR-100. BERT was run\non two tasks, text classiﬁcation on the SST-2 dataset from\nthe General Language Understanding Evaluation (GLUE)\nbenchmark, and question answering on the SQuAD v1.1\nDataset (Stanford Question Answering) which is a collec-\ntion of 100k crowdsourced question/answer pairs.\nTraining Schemes. Given that large models normally\nwould require thousands of GPU-days ( e.g., GPT-3) if\ntrained from scratch, ﬁne-tuning downstream tasks using\npre-trained models has become a trend in CV and NLP\ncommunities. Moreover, PipeTransformer is a com-\nplex training system that involves multiple core compo-\nnents. Thus, for the ﬁrst version of PipeTransformer\nsystem development and algorithmic research, it is not cost-\nefﬁcient to develop and evaluate from scratch using large-\nscale pretraining. Therefore, experiments presented in this\nsection focuses on pre-trained models. Note that since the\nmodel architectures in pre-training and ﬁne-tuning are the\nsame, PipeTransformer can serve both. We discussed\npre-training results in the Appendix.\nBaseline. Experiments in this section compares\nPipeTransformer to the state-of-the-art framework, a hy-\nbrid scheme of PyTorch Pipe (PyTorch’s implementation\nof GPipe (Huang et al., 2019)) and PyTorch DDP. Since\nthis is the ﬁrst paper that studies accelerating distributed\ntraining by freezing layers, there are no perfectly aligned\ncounterpart solutions yet.\nHyper-parameters. Experiments use ViT-B/16 (12 trans-\nformer layers, 16 ×16 input patch size) for ImageNet and\nCIFAR-100, BERT-large-uncased (24 layers) for SQuAD\n1.1, and BERT-base-uncased (12 layers) for SST-2. With\nPipeTransformer, ViT and BERT training can set the\nper-pipeline batch size to around 400 and 64 respectively.\nOther hyperparameters (e.g., epoch, learning rate) for all\nexperiments are presented in Appendix.\n4.2. Overall Training Acceleration\nWe summarize the overall experimental results in Table 1.\nNote that the speedup we report is based on a conservative\nα(1\n3 ) value that can obtain comparable or even higher ac-\ncuracy. A more aggressive α (2\n5 , 1\n2 ) can obtain a higher\nspeedup but may lead to a slight loss in accuracy (See sec-\ntion 4.3.3). Note that the model size of BERT (24 layers) is\nlarger than ViT-B/16 (12 layers), thus it takes more time for\ncommunication (see Section 4.3.2 for details).\nTable 1.Speedup for ViT and BERT Training\nBaseline PipeTransformer\nDataset Accuracy Training Accuracy Training Training\ntime time Speedup\nImageNet 80.83±0.05 26h 30m 82.18±0.32 9h 21m 2.83×\nCIFAR-100 91.21±0.07 35m 6s 91.33±0.05 12m 23s 2.44×\nSQuAD 1.1 90.71±0.18 5h 7m 90.69 ±0.23 2h 26m 2.10×\n*Note: 1. the accuracy is the mean and variance of three\nindependent runs with the same random seed; 2. the training\ntime among different runs are relatively stable (the gap is less\nthan 1 minute); 3. GLUE (SST-2)’s training time is relatively\nsmall, thus we mainly used it for debugging without reporting\na few minutes result. 4. accuracy metric: ImageNet/CIFAR-\n100: top-1 accuracy; SQuAD: F1 score.\n4.3. Performance Analysis\nThis section presents evaluation results and analyzes the per-\nformance of different components inPipeTransformer.\nMore experimental results can be found in the Appendix.\n4.3.1. S PEEDUP BREAKDOWN\n F r eez e Only\n F r eez e + A ut oPipe + A ut oDP F r eez e + A ut oPipe + A ut oDP + A ut oCache\n No F r eez e (baseline) F r eez e + A ut oPipe + A ut oCache\n0 2 4 6 8 epoch0\n2000\n4000\n6000\nThr oughput (samples/second)\n(a) Sample Throughput\n0 1 2 3 4\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n1.0x 0.95x\n1.26x\n2.27x\n2.83x (b) Speedup Ratio Comparison\nFigure 9.Speedup Breakdown (ViT on ImageNet)\nTo understand the efﬁcacy of all four components and their\nimpacts on training speed, we experimented with different\ncombinations and used their training sample throughput\n(samples/second) and speedup ratio as metrics. Results are\nillustrated in Figure 9. Key takeaways from these exper-\nimental results are: 1. the main speedup is the result of\nelastic pipelining which is achieved through the joint use\nof AutoPipe and AutoDP; 2. AutoCache’s contribution\nis ampliﬁed by AutoDP; 3. freeze training alone without\nsystem-wise adjustment even downgrades the training speed\n(discussed in Section 3.2). We provide additional explana-\ntions of these results in the Appendix.\n4.3.2. C OMMUNICATION COST\nWe also analyzed how communication and computation con-\ntribute to the overall training time. Since PyTorch DDP\noverlaps communication with computation, the time dif-\nference between a local training iteration and distributed\ntraining iteration does not faithfully represent the communi-\ncation delay. Moreover, as DDP also organizes parameters\ninto buckets and launches an AllReduce for each bucket,\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\n(a) Tuning αin Freeze Algorithm\n200 300 400 500\nM=1\nM=2\nM=3\nM=4\nM=5\nM=6\n368\n410\n386\n340\n298\n286\nThroughput (samples/second) (When K=8)\n2 4 8\n0\n2\n4\n6\n8Optimal Chunk Number (M)\n8\n4\n2\nPipeline Length (K) (b) Proﬁling Optimal Chunk Number\n0 2 4 6 8\nepoch\n400\n600\n800\n1000\n No A ut oCache\n A ut oCache (star ting fr om epoch 0)\nThr oughput (samples/second) (c) Timing of Caching\nFigure 10.Some Results of Performance Analysis\nrecording the start and ﬁnish time of overall communica-\ntions also falls short, as there can be time gaps between buck-\nets. To correctly measure DDP communication delay, we\ncombined the DDP communication hook withCUDAFuture\ncallback. More details of this measurement are documented\nin the Appendix. Key takeaways: 1. larger model cost more\ntime on communication (BERT on SQuAD); 2. a higher\ncross-machine bandwidth can further speedup the training,\nespecially for larger model.\nTable 2.Communication Cost v.s. Computational Cost\nDataset Overall Communication Computation Communication\nCost Cost Cost Cost Ratio\nImageNet 9h 21m 34m 8h 47m 5.9 %\nSQuAD 2h 26m 16m 33s 2h 9m 8.8%\n4.3.3. T UNING αIN FREEZING ALGORITHM\nWe ran experiments to show how the αin the freeze algo-\nrithms inﬂuences training speed. The result clearly demon-\nstrates that a larger α(excessive freeze) leads to a greater\nspeedup but suffers from a slight performance degradation.\nIn the case shown in Figure 10(a), where α= 1/5, freeze\ntraining outperforms normal training and obtains a2.04-fold\nspeedup. We provide more results in the Appendix.\n4.3.4. O PTIMAL CHUNKS IN ELASTIC PIPELINE\nWe proﬁled the optimal number of micro-batches M for\ndifferent pipeline lengths K. Results are summarized in\nFigure 10(b). As we can see, different Kvalues lead to dif-\nferent optimal M, and the throughput gaps across different\nM values are large (as shown whenK = 8), which conﬁrms\nthe necessity of an anterior proﬁler in elastic pipelining.\n4.3.5. U NDERSTANDING THE TIMING OF CACHING\nTo evaluateAutoCache, we compared the sample through-\nput of training that activates AutoCache from epoch 0\n(blue) with the training job without AutoCache (red). Fig-\nure 10(c) shows that enabling caching too early can slow\ndown training, as caching can be more expensive than for-\nward propagation on a small number of frozen layers. After\nfreezing more layers, caching activations clearly outper-\nforms the corresponding forward propagation. As a result,\nAutoCache uses a proﬁler to determine the proper timing to\nenable caching. In our system, for ViT (12 layers), caching\nstarts from 3 frozen layers, while for BERT (24 layers),\ncaching starts from 5 frozen layers.\n5. Related Works\nPipeTransformer combines pipeline parallelism (Huang\net al., 2019; Narayanan et al., 2019; Park et al., 2020) and\ndata parallelism (Li et al., 2020). Both techniques have\nbeen extensively studied in prior work. GPipe (Huang\net al., 2019) parallelizes micro-batches within a mini-batch\nand enforces synchronizations between consecutive mini-\nbatches. The synchronization barrier creates execution bub-\nbles and it exacerbates if the model spans across more\ndevices. PipeDream (Narayanan et al., 2019) and Het-\nPipe (Park et al., 2020) remove or mitigate execution bub-\nbles by allowing a conﬁgurable amount of staleness. Al-\nthough evaluations show that models can still converge with\nhigh accuracy, it breaks the mathematical equivalence to lo-\ncal training. PipeTransformer builds on top of PyTorch\npipeline parallel and distributed data-parallel APIs (Li et al.,\n2020). Compared to prior solutions, PipeTransformer\nreduces the size of bubbles during training by dynamically\npacking the active layers into fewer GPUs. Moreover, the\ncommunication overhead for data-parallel training, which\nis the dominant source of delay, also drops when the active\nmodel size shrinks.\n6. Discussion\nWe defer the discussion section to the Appendix, where we\ndiscuss pretraining v.s. ﬁne-tuning, designing better freeze\nalgorithms, and the versatility of our approach.\n7. Conclusion\nThis paper proposes PipeTransformer, a holistic so-\nlution that combines elastic pipeline-parallel and data-\nparallel for distributed training. More speciﬁcally,\nPipeTransformer incrementally freezes layers in the\npipeline, packs remaining active layers into fewer GPUs,\nand forks more pipeline replicas to increase the data-parallel\nwidth. Evaluations on ViT and BERT models show that com-\npared to the state-of-the-art baseline,PipeTransformer\nattains up to 2.83×speedups without accuracy loss.\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nReferences\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805,\n2018.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale.arXiv\npreprint arXiv:2010.11929, 2020.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770–778, 2016.\nHuang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, M. X.,\nChen, D., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., et al.\nGpipe: Efﬁcient training of giant neural networks using\npipeline parallelism. arXiv preprint arXiv:1811.06965,\n2018.\nHuang, Y ., Cheng, Y ., Bapna, A., Firat, O., Chen, D., Chen,\nM., Lee, H., Ngiam, J., Le, Q. V ., Wu, Y ., and Chen,\nz. Gpipe: Efﬁcient training of giant neural networks\nusing pipeline parallelism. In Wallach, H., Larochelle, H.,\nBeygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett,\nR. (eds.), Advances in Neural Information Processing\nSystems, volume 32, pp. 103–112. Curran Associates,\nInc., 2019.\nJiang, Y ., Zhu, Y ., Lan, C., Yi, B., Cui, Y ., and Guo,\nC. A uniﬁed architecture for accelerating distributed\nDNN training in heterogeneous gpu/cpu clusters. In\n14th USENIX Symposium on Operating Systems Design\nand Implementation (OSDI 20), pp. 463–479. USENIX\nAssociation, November 2020. ISBN 978-1-939133-19-\n9. URL https://www.usenix.org/confere\nnce/osdi20/presentation/jiang.\nKim, C., Lee, H., Jeong, M., Baek, W., Yoon, B., Kim, I.,\nLim, S., and Kim, S. torchgpipe: On-the-ﬂy pipeline\nparallelism for training giant models. arXiv preprint\narXiv:2004.09910, 2020.\nKim, S., Yu, G.-I., Park, H., Cho, S., Jeong, E., Ha, H.,\nLee, S., Jeong, J. S., and Chun, B.-G. Parallax: Sparsity-\naware data parallel training of deep neural networks. In\nProceedings of the Fourteenth EuroSys Conference 2019,\npp. 1–15, 2019.\nLepikhin, D., Lee, H., Xu, Y ., Chen, D., Firat, O., Huang, Y .,\nKrikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling\ngiant models with conditional computation and automatic\nsharding. arXiv preprint arXiv:2006.16668, 2020.\nLi, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed,\nA., Josifovski, V ., Long, J., Shekita, E. J., and Su, B.-Y .\nScaling distributed machine learning with the parameter\nserver. In 11th {USENIX}Symposium on Operating\nSystems Design and Implementation ( {OSDI}14), pp.\n583–598, 2014.\nLi, S., Zhao, Y ., Varma, R., Salpekar, O., Noordhuis, P., Li,\nT., Paszke, A., Smith, J., Vaughan, B., Damania, P., et al.\nPytorch distributed: Experiences on accelerating data\nparallel training. Proceedings of the VLDB Endowment,\n13(12), 2020.\nMorcos, A., Raghu, M., and Bengio, S. Insights on repre-\nsentational similarity in neural networks with canonical\ncorrelation. In Bengio, S., Wallach, H., Larochelle, H.,\nGrauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.),\nAdvances in Neural Information Processing Systems 31,\npp. 5732–5741. Curran Associates, Inc., 2018. URL\nhttp://papers.nips.cc/paper/7815-ins\nights-on-representational-similarit\ny-in-neural-networks-with-canonical\n-correlation.pdf.\nNarayanan, D., Harlap, A., Phanishayee, A., Seshadri, V .,\nDevanur, N. R., Ganger, G. R., Gibbons, P. B., and Za-\nharia, M. Pipedream: Generalized pipeline parallelism\nfor dnn training. In Proceedings of the 27th ACM Sym-\nposium on Operating Systems Principles , SOSP ’19,\npp. 1–15, New York, NY , USA, 2019. Association for\nComputing Machinery. ISBN 9781450368735. doi:\n10.1145/3341301.3359646.\nPark, J. H., Yun, G., Yi, C. M., Nguyen, N. T., Lee, S.,\nChoi, J., Noh, S. H., and ri Choi, Y . Hetpipe: Enabling\nlarge DNN training on (whimpy) heterogeneous GPU\nclusters through integration of pipelined model paral-\nlelism and data parallelism. In 2020 USENIX Annual\nTechnical Conference (USENIX ATC 20), pp. 307–321.\nUSENIX Association, July 2020. ISBN 978-1-939133-\n14-4. URL https://www.usenix.org/confe\nrence/atc20/presentation/park.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. arXiv preprint arXiv:1912.01703,\n2019.\nRaghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J.\nSvcca: Singular vector canonical correlation analysis for\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\ndeep learning dynamics and interpretability. In NIPS,\n2017.\nRajbhandari, S., Rasley, J., Ruwase, O., and He, Y . Zero:\nMemory optimization towards training a trillion parame-\nter models. arXiv preprint arXiv:1910.02054, 2019.\nShazeer, N., Cheng, Y ., Parmar, N., Tran, D., Vaswani, A.,\nKoanantakool, P., Hawkins, P., Lee, H., Hong, M., Young,\nC., Sepassi, R., and Hechtman, B. Mesh-tensorﬂow:\nDeep learning for supercomputers. In Bengio, S., Wal-\nlach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems, volume 31, pp. 10414–10423. Cur-\nran Associates, Inc., 2018.\nShen, S., Baevski, A., Morcos, A. S., Keutzer, K., Auli,\nM., and Kiela, D. Reservoir transformer. arXiv preprint\narXiv:2012.15045, 2020.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper,\nJ., and Catanzaro, B. Megatron-lm: Training multi-\nbillion parameter language models using model paral-\nlelism. arXiv preprint arXiv:1909.08053, 2019.\nTan, M. and Le, Q. Efﬁcientnet: Rethinking model scaling\nfor convolutional neural networks. In International Con-\nference on Machine Learning , pp. 6105–6114. PMLR,\n2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. arXiv preprint arXiv:1706.03762, 2017.\nXiong, R., Yang, Y ., He, D., Zheng, K., Zheng, S., Xing,\nC., Zhang, H., Lan, Y ., Wang, L., and Liu, T. On layer\nnormalization in the transformer architecture. In Inter-\nnational Conference on Machine Learning, pp. 10524–\n10533. PMLR, 2020.\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nAppendix Outline\nThis Appendix provides background and preliminaries,\nmore details of four components, additional experimental\ndetails and results, and discussions. The organization is as\nfollows:\nBackground and Preliminaries. Appendix A provides\nthe introduction for Transformer models, freeze training,\npipeline parallelism, data parallelism, and hybrid of pipeline\nparallelism and data parallelism. This section serves as the\nrequired knowledge to understand PipeTransformer.\nMore Details of Freeze Algorithm, AutoPipe,\nAutoDP, AutoCache. Appendix B explains more details\nof design motivation for freeze training algorithm and\nshows details of the deviation; Appendix C provides more\nanalysis to understand the design choice of AutoPipe;\nAppendix D contains more details of AutoDP, including\nthe dataset redistributing, and comparing another way to\nskip frozen parameters; Appendix E introduces additional\ndetails for AutoCache.\nMore Experimental Results and Details. In Appendix\nF, we provide hyper-parameters and more experimental\nresults. Especially, we provide more details of speedup\nbreakdown in F.2.\nDiscussion. In Appendix G, we will discuss pretraining\nv.s. ﬁne-tuning, designing better freeze algorithms, and the\nversatility of our approach.\nA. Background and Preliminaries\nA.1. Transformer Models: ViT and BERT\nFigure 11.Evolution of Transformer Models.\nTransformer. The Transformer model originates from the\nNatural Language Processing (NLP) community. It replaces\nthe recurrent neural network (RNN) using a self-attention\nmechanism which relates different positions of a single se-\nquence in order to compute a representation of the sequence.\nThe transformer model has an encoder-decoder structure\nwhich is a classical structure for sequence modeling. The\nencoder maps an input sequence of symbol representations\n(x1,...,x n) to a sequence of continuous representations\nz = (z1,...,z n). Given z, the decoder then generates an\noutput sequence (y1,...,y m) of symbols one element at a\ntime. As shown in Figure 12, the Transformer follows this\noverall architecture using stacked self-attention and point-\nwise, fully connected layers for both the encoder (left) and\ndecoder (right). To better understand this architecture, we\nrefer readers to the tutorial “The Annotated Transformer”1.\nFigure 12.Transformer Model Architecture (Vaswani et al., 2017)\nBERT (ViT). BERT (Devlin et al., 2018), which stands\nfor Bidirectional Encoder Representations from Transform-\ners, simply stacks multiple Transformer encoders (also\ncalled the Transformer layer, Figure 12, left). BERT BASE\nhas 12 Transformer layers, and its total number of parame-\nters is 110M. BERT LARGE has 24 Transformer layers, and\nits total number of parameters is 340M. BERT is pre-trained\nusing unsupervised tasks (masked language model, and next\nsentence prediction) and then ﬁne-tuned to various NLP\ntasks such as text classiﬁcation and question answering.\nVision Transformer (ViT). ViT (Dosovitskiy et al., 2020)\nattains excellent results compared to state-of-the-art convo-\nlutional networks. Its architecture is shown in Figure 13. It\nsplits an image into ﬁxed-size patches, linearly embeds each\nof them, adds position embeddings, and feeds the resulting\nsequence of vectors to a Transformer encoder. Similar to\nBERT, the Transformer encode repeats multiple layers.\nModel Architecture Comparison. Note that ViT and\nBERT’s Transformer encoder places layer normalization\n1 http://nlp.seas.harvard.edu/2018/04/03/\nattention.html\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nFigure 13.Vision Transformer (Dosovitskiy et al., 2020)\nFigure 14.Comparison of Transform in BERT and ViT\nin different locations. To understand the differences be-\ntween these two architectures, please refer to the analysis\nin (Xiong et al., 2020). Due to this slight difference, our\nPipeTransformer source code implements the model\npartition of these two architectures separately.\nA.2. Freeze Training.\nThe concept of freeze training is ﬁrst proposed by (Raghu\net al., 2017), which provides a posterior algorithm, named\nSVCCA (Singular Vector Canonical Correlation Analysis),\nto compare two representations. SVCCA can compare the\nrepresentation at a layer at different points during training\nto its ﬁnal representation and ﬁnd that lower layers tend to\nconverge faster than higher layers. This means that not all\nlayers need to be trained through training. We can save com-\nputation and prevent overﬁtting by consecutively freezing\nlayers. However, SVCCA has to take the entire dataset as its\ninput, which does not ﬁt an on-the-ﬂy analysis. This draw-\nback motivates us to design an adaptive on the ﬂy freeze\nalgorithm.\nA.3. Pipeline Parallelism\nIn PipeTransformer, we reuse GPipe as the baseline.\nGPipe is a pipeline parallelism library that can divide differ-\nent sub-sequences of layers to separate accelerators, which\nprovides the ﬂexibility of scaling a variety of different net-\nFigure 15.GPipe (Huang et al., 2018)\nworks to gigantic sizes efﬁciently. The key design inGPipe\nis that it splits the mini-batch into M micro-batches, which\ncan train faster than naive model parallelism (as shown in\nFigure 15(b). However, as illustrated in Figure 15(c), micro-\nbatches still cannot thoroughly avoid bubble overhead (some\nidle time per accelerator). GPipe empirically demonstrates\nthat the bubble overhead is negligible when M ≥4 ×K.\nDifferent from GPipe, PipeTransformer has an elas-\ntic pipelining parallelism in which Kand pipeline number\nare dynamic during the training.\nA.4. Data Parallelism\nFigure 16.PyTorch DDP Bucket-based AllReduce\nIn PyTorch DDP (Li et al., 2020), to improve communi-\ncation efﬁciency, gradients are organized into buckets, and\nAllReduce is operated on one bucket at a time. The map-\nping from parameter gradients to buckets is determined at\nthe construction time, based on the bucket size limit and pa-\nrameter sizes. Model parameters are allocated into buckets\nin (roughly) the reverse order of Model.parameters()\nfrom the given model. Reverse order is used because DDP\nexpects gradients to be ready during the backward pass\nin approximately that order. Figure 16 shows an example.\nNote that, grad0 and grad1 are in bucket1, and the other two\ngradients are in bucket0. With this bucket design, DDP can\noverlap part of the communication time with the computa-\ntion time of backward propagation.\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nA.5. Hybrid of Pipeline Parallelism and Data\nParallelism\nTo understand the hybrid of pipeline parallelism and data\nparallelism, we illustrate the training process in Figure 17.\nThis example is hybrid two-way data parallelism and two-\nstage pipeline parallelism: pipeline 0 has two partitions,\nusing GPU 1 and 3; pipeline 1 also has two partitions, using\nGPU 0 and 2; two pipelines are synchronized by data par-\nallelism. Each batch of training data is divided into micro-\nbatches that can be processed in parallel by the pipeline\npartitions. Once a partition completes the forward pass for\na micro-batch, the activation memory is communicated to\nthe pipeline’s next partition. Similarly, as the next partition\ncompletes its backward pass on a micro-batch, the gradient\nwith respect to the activation is communicated backward\nthrough the pipeline. Each backward pass accumulates gra-\ndients locally. Subsequently, all data parallel groups perform\nAllReduce on gradients.\nF1,0 F1,1\nF3,0 F3,1 B3,0 B3,1\nB1,0 B1,1GPU1\nGPU3 U3\nU1\nAR\nAR\nF0,0 F0,1\nF2,0 F2,1 B2,0 B2,1\nB0,0 B0,1GPU0\nGPU2 U2\nU0\nAR\nAR\nPipeline 0Pipeline 1\nF1,2\nF3,2\nF0,2\nF2,1\nB3,2\nB1,12\nB2,2\nB0,2\nDDP Rank 0\nDDP Rank 1\nFigure 17.Illustration for Hybrid of Pipeline-parallel and Data-\nparallel\nIn this example, to simplify the ﬁgure, we assume that the\nbucket size is large enough to ﬁt all gradients on a single\ndevice. That is to say, DDP uses one bucket per device,\nresulting in two AllReduce operations. Note that, since\nAllReduce can start as soon as gradients in corresponding\nbuckets become ready. In this example, DDP launches\nAllReduce on GPU 1 and 3 immediately after B3,1 and\nB1,1, without waiting for the rest of backward computation.\nLastly, the optimizer updates the model weights.\nB. More Details of Freeze Algorithm\nExplanation of Equation 1. In numerical optimization,\nthe weight with the smallest gradient norm converges ﬁrst.\nWith this assumption, we use the gradient norm as the indi-\ncator to identify which layers can be frozen on the ﬂy. To\nverify this idea, we save the gradient norm for all layers\nat different iterations (i.e., epoch). With this analysis, we\nfound that in the later phase of training, the pattern of gra-\ndient norm in different layers matches the assumption, but\nin the early phase, the pattern is random. Sometimes, we\ncan even see that the gradient norm of those layers close to\nthe output is the smallest. Figure 18 shows such an example.\nlayer index\ngradient norm\nupper bound of \nfrozen layer number \nthe layer which has \nthe lowest gradient now\nFigure 18.An example that the smallest gradient is not close to the\ninput layer.\nIf we freeze all layers preceding the blue dash line layer,\nthe freezing is too aggressive since some layers have not\nconverged yet. This motivates us further amend this naive\ngradient norm indicator. To avoid the randomness of gra-\ndient norm at the early phase of training, we use a tunable\nbound to limit the maximum number of frozen layers. We\ndo not freeze all layers preceding the layer with the smallest\ngradient norm for the case in the ﬁgure. Instead, we freeze\nlayers preceding the bound (the red color dash line).\nDeviation. The term L(T−1)\nfrozen +α(L−L(T−1)\nfrozen ) in Equation\n1 can be written as:\nL(T)\nfrozen = (1−α)T[ αL\n1 −α +\nT∑\nt=2\nαL\n(1 −α)t] (3)\nThe deviation is as follows:\nL(1)\nfrozen = αL (4)\nL(2)\nfrozen = (L−L(1)\nfrozen)α+ L(1)\nfrozen (5)\nL(T)\nfrozen = (L−L(T−1)\nfrozen )α+ L(T−1)\nfrozen (6)\nL(T)\nfrozen = αL+ (1−α)L(T−1)\nfrozen (7)\nL(T)\nfrozen\n(1 −α)T = αL\n(1 −α)T + L(T−1)\nfrozen\n(1 −α)(T−1) (8)\nL(T)\nfrozen\n(1 −α)T = αL\n(1 −α) +\nT∑\nt=2\nαL\n(1 −α)t (9)\n(10)\nC. More Details of AutoPipe\nBalanced Partition: Trade-off between Communication\nand Computational Cost. Let us compute the communi-\ncation cost in Figure 5. The intermediate tensor from parti-\ntion k−2 needs two cross-GPU communications to arrive\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nto partition k. The parameter number of this intermediate\ntensor depends on the batch size and the Transformer model\narchitecture. In BERTbase, the intermediate tensor width\nand height is the hidden feature size and sequence length,\nrespectively (i.e., 1024, 512). If we use a batch size 300 in\na pipeline, the total parameter number is 1024 ×512 ×300.\nIf we store it using float32, the memory cost is 0.63 GB.\nThe GPU-to-GPU communication bandwidth is 15.754 GB\n(PCI 3.0, 16 lanes). Then one cross-GPU communication\ncosts 40 ms. In practice, the time cost will be higher than\nthis value. Therefore, two cross-GPU communications cost\naround 100 ms. To compare with the computation cost,\nwe quantify the time cost for the forward propagation of\na Transformer layer (12 million parameters), the time cost\nis around 35 ms, meaning that the communication cost for\nskip connection is far more than a speciﬁc layer’s compu-\ntation cost. Compared to a slightly unbalanced partition\nin parameter number wise, 100 ms is non-trivial. If we do\nnot break the skip connection, the parameter number gap\nbetween different partitions is far less than 12 million (e.g.,\n4M or even less than 1 M). Therefore, this analysis explains\npartitioning without breaking the skip connection is a rea-\nsonable design choice. We also ﬁnd that when the GPU\ndevice number in a machine is ﬁxed (e.g., 8), the larger the\nmodel size is, the smaller the partition gap, which further\nindicates that our design’s rationality.\nUnderstanding Bubble in Pipeline. In the main text, Fig-\nure 6 depicts an example of running 4 micro-batches through\na 4-device pipeline. Time ﬂows from left to right, and each\nrow denotes workload on one GPU device. F and B squares\nwith the same color represent the forward and the backward\npass time blocks of the same micro-batch. U represents the\ntime block for updating parameters. Empty time blocks\nare bubbles. Assume that the load of the pipeline is evenly\ndistributed amongst all devices. Consequently, all the time\nblocks during the forward pass are roughly in the same size,\nand similarly for backward time blocks. Note that the sizes\nof the forward time blocks can still differ from the back-\nward ones. Based on these assumptions, we can estimate\nthe per-iteration bubble size by simply counting the number\nof empty blocks during the forward and backward passes,\nrespectively. In both the forward and backward pass, each\ndevice idles for (K−1) time blocks. Therefore, the total\nbubble size is (K−1) times per micro-batch forward and\nbackward delay, which clearly decreases with fewer pipeline\ndevices.\nRelationship Between Number of Micro-batches per\nMini-batch (M) and DDP. To understand the reason why\nM and DDP have mutual impacts, a thorough understand-\ning of Section A.5 is needed ﬁrst. In essence, DDP and\npipelining has opposite requirement for M: DDP requires\na relatively larger chunk of the bucket (smaller M) to over-\nlap the communication (introduced in Section A.4), while\npipelining requires a larger M to avoid bubble overhead\n(introduced in Section A.3). To further clarify, we must ﬁrst\nremember that DDP must wait for the last micro-batch to ﬁn-\nish its backward computation on a parameter before launch-\ning its gradient synchronization, then imagine two extreme\ncases. One case is that M = 1, meaning the communication\ncan be fully overlapped with computation using buckets.\nHowever, setting M = 1leads to a performance downgrade\nof pipelining (overhead of bubbles). Another extreme case\nis a very large M, then the communication time (labeled as\ngreen “AR” in Figure A.5) may be higher than the computa-\ntion time for a micro-batch (note that the width of a block\nin Figure A.5 represents the wall clock time). With these\ntwo extreme cases, we can see that there must be an optimal\nvalue of M in a dynamical environment (Kand parameter\nnumber of active layers) of PipeTransformer, indicat-\ning that it is sub-optimal to ﬁx M during training. This\nexplains the need for a dynamic M for elastic pipelining.\nD. More details of AutoDP\nD.1. Data Redistributing\nIn standard data parallel-based distributed training, PyTorch\nuses DistributedSampler to make sure each worker in\nDP only load a subset of the original dataset that is exclusive\nto each other. The example code is as follows:\nself.train sampler =\nDistributedSampler(self.train dataset,\nnum replicas=num replicas, rank=local rank)\nCompared to this standard strategy, we made the following\noptimizations:\n1. dynamic partition: the number of DP workers is in-\ncreased when new pipelines have participated in DP. In\norder to guarantee that the data partition is evenly assigned\nafter adding new pipes, the training dataset is repartitioned\nby rebuilding the DistributedSampler and setting new\nnum replicas and rank as arguments.\n2. to reuse the computation of FP for frozen layers, we\ncached the hidden states in host memory and disk memory\nas well. Since the training requires to shufﬂe each epoch,\nthe cache order of hidden features with respect to the order\nof original samples is different across different epochs. In\norder to identify which data point a hidden feature belongs\nto, we build a sample unique ID by returning index in the\nget item() function of Dataset class. With this unique\nID, we can ﬁnd a sample’s hidden feature with O(1) time\ncomplexity during training.\n3. when data is shufﬂed in each epoch, a data sample trained\nin the previous epoch may be moved to another machine\nfor training in the current epoch. This makes the cache not\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nreused across epochs. To address this issue, we ﬁx a subset\nof entire samples in a machine and only do shufﬂe for this\nsubset. This guarantees the shufﬂe during epochs is only\nexecuted inside a machine, thus the hidden feature’s cache\ncan be reused deterministically. To achieve this, rather than\nmaintaining a global rank for DistributedSampler, we in-\ntroduce node rank and local rank. node rank is used\nto identify which subset of samples a machine needs to\nhold. local rank is used by DistributedSampler to\nidentify which part of the shufﬂe subset that a worker inside\na machine should train. Note that this does not hurt the\nalgorithmic convergence property. Shufﬂing for multiple\nsubsets obtains more randomness than randomness obtained\nby a global shufﬂe, which further increases the robustness\nof training. The only difference is that some parallel pro-\ncesses in distributed training are ﬁxed in part of the shufﬂed\ndatasets. If a training task does not need to shufﬂe the\ndataset across epochs, the above-mentioned optimization\nwill not be activated.\nD.2. Skip Frozen Parameters in AutoDP\nTo reduce communication cost, another method is to use\nPyTorch DDP API 2. However, this API is temporally de-\nsigned for Facebook-internal usage, and we must carefully\ncalculate and synchronize the information regarding which\nparameters should be skipped, making our system unstable\nand difﬁcult to be debugged. Our design avoids this issue\nand simpliﬁes the system design. Since AutoPipe stores\nFfrozen and Fpipe separately (introduced in Section 3.2.1), we\ncan naturally skip the frozen parameters because AutoDP\nonly needs to initialize the data parallel worker with Fpipe.\nE. More Details of AutoCache\nFigure 19.Hierarchical Caching\nAutoCache supports hierarchical caching. Figure 19 shows\nour design. We maintain a sliding window to represent the\nmaximum memory that the CPU host memory can hold,\nthen move the window to prefetch the caching that the train-\ning requires and delete the caching that is consumed from\n2See the internal API deﬁned by PyTorch DDP:\nhttps://github.com/pytorch/pytorch/blob/\nmaster/torch/nn/parallel/distributed.py,\nset params and buffers to ignore for model().\nthe CPU host memory. In our implementation, we deﬁne the\nwindow size as the maximum batch number that the CPU\nhost memory can hold. To avoid frequent memory exchange\nbetween disk storage and CPU host memory, we also deﬁne\nthe block size that every time we prefetch (as the grey and\ngreen blocks are shown in the ﬁgure). In general, this hierar-\nchical caching is useful when the training dataset is too large\nand exceeds the CPU host memory limit. However, we have\nto point out that this complex caching may not always be\nthe optimal choice in the training system since the caching\nexchange itself may cost time. To this end, we suggest users\nof PipeTransformer using a relatively larger CPU host\nmemory, which avoids activating the hierarchical caching\nand obtains faster training.\nF. More Experimental Results and Details\nF.1. Hyper-Parameters Used in Experiments\nTable 3.Hyperparameters used in Experiments\nDataset Model Hyperparameters Comments\nSQuAD BERT\nbatch size 64\nmax sequence length 512\nlearning rate {1e-5, 2e-5, 3e-5, 4e-5, 5e-5}\nepochs 3\ngradient accumulation steps 1\nImageNet ViT\nbatch size 400\nimage size 224\nlearning rate {0.1, 0.3, 0.01, 0.03}\nweighs decay 0.3\ndecay type cosine\nwarmup steps 2\nepochs 10\nCIFAR-100 ViT\nbatch size 320\nimage size 224\nlearning rate {0.1, 0.3, 0.01, 0.03}\nweighs decay 0.3\ndecay type cosine\nwarmup steps 2\nepochs 10\nIn Table 3, we follow the same hyper-parameters used in the\noriginal ViT and BERT paper. Note that for ViT model, we\nuse image size 224 for ﬁne-tuning training.\nF.2. More Details of Speedup Breakdown\nUnderstanding the speed downgrade of freeze only.\nAs shown in Figure 9, the Freeze Only strategy is\nabout 5% slower than the No Freeze baseline. After\nthe performance analysis, we found it is because Freeze\nOnly changes memory usage pattern and introduced ad-\nditional overhead in PyTorch’sCUDACachingAllocator\n3. More speciﬁcally, to reduce the number of expen-\nsive CUDA memory allocation operations, PyTorch main-\ntains a CUDACachingAllocator that caches CUDA mem-\n3To understand the design of this API, please refer to Section\n5.3 in the original PyTorch paper (Paszke et al., 2019). The source\ncode is at https://github.com/pytorch/pytorch/b\nlob/master/c10/cuda/CUDACachingAllocator.h\nPipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers\nory blocks to speed up future reuses. Without freezing,\nthe memory usage pattern in every iteration stays consis-\ntent, and hence the cached memory blocks can be per-\nfectly reused. After introducing layer freezing, although\nit helps to reduce memory footprint, on the other hand,\nit might also change the memory usage pattern, forcing\nCUDACachingAllocator to split blocks or launch new\nmemory allocations, which slightly slows down the train-\ning. In essence, this underlying mechanism of PyTorch\nis not tailored for freeze training. Customizing it for freeze\ntraining requires additional engineering efforts.\nF.3. Tuningαfor ViT on ImageNet\nFigure 20.Tuning αfor ViT on ImageNet\nF.4. The Method That Can Accurately Measure the\nCommunication Cost\nSince PyTorch DDP overlaps communication with compu-\ntation, the time difference between a local training iteration\nand a distributed training iteration does not faithfully repre-\nsent the communication delay. Moreover, as DDP also orga-\nnizes parameters into buckets and launches an AllReduce\nfor each bucket, recording the start and ﬁnish time of overall\ncommunications is also insufﬁcient. To correctly measure\nDDP communication delay, we combined the DDP commu-\nnication hook with CUDAFuture callback. We developed\na communication hook function that records a start CUDA\nevent immediately before launching AllReduce. Then, in\nthe CUDAFuture returned by the AllReduce function, we\ninstall a callback that records a ﬁnish CUDA event immedi-\nately after the non-blocking CUDAFuture completes. The\ndifference between these two CUDA events represents the\nAllReduce communication delay of one bucket. We col-\nlected the events for all buckets and removed time gaps\nbetween buckets if there were any. The remaining duration\nin that time range accurately represents the overall DDP\ncommunication delay.\nF.5. Overheads of Pipe Transformation\nWe have veriﬁed the time cost of pipeline transformation.\nThe result in Table 4 shows that the overall cost of pipeline\nTable 4.Overheads of pipe transformation (seconds)\nPipeline Transformation Overall Time Cost Dissect\nC P D\ninitialization (length = 8) 18.2 16.6 0.7 0.9\nlength is compressed from 8 to 4 10.2 8.3 1.3 0.6\nlength is compressed from 4 to 2 5.5 3.8 2.1 0.7\nlength is compressed from 2 to 1 9.5 2.3 6.1 1.0\n*C - creating CUDA context; P - Pipeline Warmup; D - DDP.\ntransformation is very small (less than 1 minute), compared\nto the overall training time. Therefore, we do not consider\nfurther optimization.\nG. Discussion\nPretraining v.s. Fine-tuning : Given that the model ar-\nchitectures in pre-training and ﬁne-tuning are the same,\nwe do not need to change the system design. Run-\nning larger Transformers (over 32 layers) is straightfor-\nward because almost all giant Transformer models are de-\nsigned by simply stacking more transformer encoder layers.\nPipeTransformer can serve as a training system for\nboth pre-training and ﬁne-tuning training. We plan to run\nour training system on more models and datasets in both\nsettings.\nDesigning better freeze algorithm : Our proposed algo-\nrithm is simple, yet it proves to be effective on various\ntasks. However, we believe that further developments to\nthe freeze algorithm may lead to better generalization and\nobtain higher accuracy.\nversatility: PipeTransformer training system can also\nbe used on other algorithms that gradually ﬁx portions of\nneural networks. For example, cross-silo federated learning,\nlayer-by-layer neural architecture search, and pruning large\nDNNs are all potential use cases of our training system. We\nwill explore the training acceleration for these scenarios in\nour future works.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.682780921459198
    },
    {
      "name": "Transformer",
      "score": 0.6360500454902649
    },
    {
      "name": "Parallel computing",
      "score": 0.5180497169494629
    },
    {
      "name": "Electrical engineering",
      "score": 0.11547061800956726
    },
    {
      "name": "Engineering",
      "score": 0.109975665807724
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}