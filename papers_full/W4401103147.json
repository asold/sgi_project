{
  "title": "Genomic language models: opportunities and challenges",
  "url": "https://openalex.org/W4401103147",
  "year": 2025,
  "authors": [
    {
      "id": null,
      "name": "Benegas, Gonzalo",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Ye, Chengzhong",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Albors, Carlos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4284507829",
      "name": "Li, Jianan Canal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287031069",
      "name": "Song, Yun S.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6777337586",
    "https://openalex.org/W4309506674",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W6798272402",
    "https://openalex.org/W4392575276",
    "https://openalex.org/W6854154219",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W4391836523",
    "https://openalex.org/W2890223884",
    "https://openalex.org/W3209435229",
    "https://openalex.org/W4385737488",
    "https://openalex.org/W4387966979",
    "https://openalex.org/W4400451184",
    "https://openalex.org/W6868672192",
    "https://openalex.org/W6849916264",
    "https://openalex.org/W4405995672",
    "https://openalex.org/W4206087583",
    "https://openalex.org/W4401043959",
    "https://openalex.org/W2145191876",
    "https://openalex.org/W2167852161",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W2909194804",
    "https://openalex.org/W6862598229",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W6765474767",
    "https://openalex.org/W6810245500",
    "https://openalex.org/W3154275519",
    "https://openalex.org/W6857051209",
    "https://openalex.org/W4404349982",
    "https://openalex.org/W3027404681",
    "https://openalex.org/W3164328317",
    "https://openalex.org/W4280563114",
    "https://openalex.org/W6853816692",
    "https://openalex.org/W4403921247",
    "https://openalex.org/W6862556448",
    "https://openalex.org/W1019830208",
    "https://openalex.org/W2198606573",
    "https://openalex.org/W2345512687",
    "https://openalex.org/W2952239877",
    "https://openalex.org/W4226065462",
    "https://openalex.org/W6870343350",
    "https://openalex.org/W4392884201",
    "https://openalex.org/W6861500500",
    "https://openalex.org/W6853888595",
    "https://openalex.org/W6869254011",
    "https://openalex.org/W6859050812",
    "https://openalex.org/W6862340674",
    "https://openalex.org/W6861371584",
    "https://openalex.org/W6788175385",
    "https://openalex.org/W6870222501",
    "https://openalex.org/W4367173606",
    "https://openalex.org/W6798205730",
    "https://openalex.org/W4394763992",
    "https://openalex.org/W6857243932",
    "https://openalex.org/W1999027354",
    "https://openalex.org/W4393433029",
    "https://openalex.org/W2945038365",
    "https://openalex.org/W2799347953",
    "https://openalex.org/W1983519005",
    "https://openalex.org/W6795285856",
    "https://openalex.org/W4392302569",
    "https://openalex.org/W6856432898",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4280594315",
    "https://openalex.org/W6757998062",
    "https://openalex.org/W6852503157",
    "https://openalex.org/W6803444062",
    "https://openalex.org/W6850033884",
    "https://openalex.org/W6859298233",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W6869556403",
    "https://openalex.org/W6869112767",
    "https://openalex.org/W6870615235",
    "https://openalex.org/W6638575559",
    "https://openalex.org/W2112132489",
    "https://openalex.org/W3100651612",
    "https://openalex.org/W4386983965",
    "https://openalex.org/W6866987651",
    "https://openalex.org/W6810549907",
    "https://openalex.org/W6866547923",
    "https://openalex.org/W6869490519",
    "https://openalex.org/W6872086419",
    "https://openalex.org/W6866151710",
    "https://openalex.org/W3116286104",
    "https://openalex.org/W3194668998",
    "https://openalex.org/W3081278968",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W6772444045",
    "https://openalex.org/W4382893389",
    "https://openalex.org/W2967059818",
    "https://openalex.org/W2889874867",
    "https://openalex.org/W2174602966",
    "https://openalex.org/W2604078450",
    "https://openalex.org/W2162151166",
    "https://openalex.org/W2157929834",
    "https://openalex.org/W2905452503",
    "https://openalex.org/W3029661147",
    "https://openalex.org/W2017818880",
    "https://openalex.org/W6864940332",
    "https://openalex.org/W6857325831",
    "https://openalex.org/W2259938310",
    "https://openalex.org/W2076154138",
    "https://openalex.org/W4367597910",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W4392655732",
    "https://openalex.org/W2337876203",
    "https://openalex.org/W4280524269",
    "https://openalex.org/W4282926623",
    "https://openalex.org/W6842449622",
    "https://openalex.org/W4289593660",
    "https://openalex.org/W4387994442",
    "https://openalex.org/W6727450400",
    "https://openalex.org/W6854305829",
    "https://openalex.org/W4393981110",
    "https://openalex.org/W2086561953",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6755651582",
    "https://openalex.org/W2097107919",
    "https://openalex.org/W4310957810",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3198971594",
    "https://openalex.org/W4401662181",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4387706757",
    "https://openalex.org/W4230674625",
    "https://openalex.org/W4400672499",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2320983896",
    "https://openalex.org/W4400161786",
    "https://openalex.org/W4388962805",
    "https://openalex.org/W3209374680",
    "https://openalex.org/W4394946907",
    "https://openalex.org/W4400231442",
    "https://openalex.org/W4384133826",
    "https://openalex.org/W4205206785",
    "https://openalex.org/W4404355317",
    "https://openalex.org/W4396903900",
    "https://openalex.org/W4309067679",
    "https://openalex.org/W4391652655",
    "https://openalex.org/W4399911566",
    "https://openalex.org/W4206948363",
    "https://openalex.org/W4382490702",
    "https://openalex.org/W4400024719",
    "https://openalex.org/W4293581915",
    "https://openalex.org/W4389070675",
    "https://openalex.org/W4392373804",
    "https://openalex.org/W4396904350",
    "https://openalex.org/W4404821554",
    "https://openalex.org/W4384818053",
    "https://openalex.org/W4399510551",
    "https://openalex.org/W2997580108",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4391835326",
    "https://openalex.org/W4386360310",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4402832361",
    "https://openalex.org/W4389326242",
    "https://openalex.org/W4396622252",
    "https://openalex.org/W4399384287",
    "https://openalex.org/W4402607688",
    "https://openalex.org/W2899575547"
  ],
  "abstract": null,
  "full_text": "Genomic Language Models: Opportunities and Challenges\nGonzalo Benegas1,∗, Chengzhong Ye2,∗, Carlos Albors1,∗, Jianan Canal Li1,∗, Yun S. Song1,2,3,†\n1Computer Science Division, University of California, Berkeley\n2Department of Statistics, University of California, Berkeley\n3Center for Computational Biology, University of California, Berkeley\nSeptember 24, 2024\nAbstract\nLarge language models (LLMs) are having transformative impacts across a wide range of\nscientific fields, particularly in the biomedical sciences. Just as the goal of Natural Language\nProcessing is to understand sequences of words, a major objective in biology is to understand\nbiological sequences. Genomic Language Models (gLMs), which are LLMs trained on DNA\nsequences, have the potential to significantly advance our understanding of genomes and how\nDNA elements at various scales interact to give rise to complex functions. To showcase this\npotential, we highlight key applications of gLMs, including functional constraint prediction,\nsequence design, and transfer learning. Despite notable recent progress, however, developing\neffective and efficient gLMs presents numerous challenges, especially for species with large,\ncomplex genomes. Here, we discuss major considerations for developing and evaluating gLMs.\n∗These authors contributed equally to this work.\n†To whom correspondence should be addressed: yss@berkeley.edu\n1\narXiv:2407.11435v2  [q-bio.GN]  22 Sep 2024\nINTRODUCTION\nRecent advances in AI/ML have profoundly impacted a wide range of scientific disciplines, revo-\nlutionizing approaches to modeling, data analysis, interpretation, and discovery. One of the key\npillars of this development is self-supervised learning, in which training on massive amounts of\nunlabeled data enables the learning of complex features and their interactions. This paradigm has\nparticularly transformed Natural Language Processing (NLP), allowing AI models to match human\nperformance on several challenging tasks, including translation [1], speech recognition [2], and even\nanswering questions from standardized professional and academic exams [3].\nJust as the aim of NLP is to understand sequences of natural language, a major aim of com-\nputational biology is to understand biological sequences. As such, there has been intense recent\ninterest in adapting modern techniques from NLP for biological sequences (DNA, RNA, proteins).\nIn particular, protein sequence databases (e.g., UniProt [4]) have grown exponentially over the past\ndecade, and protein language models (pLMs) trained on these immense data have achieved impres-\nsive performance on complex problems such as structure prediction [5] and variant effect prediction\n[6, 7], to name just a few examples (see [8, 9] for reviews on pLMs and their applications). This\nsuccess aligns with the intuition that billions of years of evolution have explored portions of the\nprotein sequence space that are relevant to life, so large unlabeled datasets of protein sequences are\nexpected to contain significant biological information.\nIn a similar vein, large language models (LLMs) trained on DNA sequences have the potential\nto transform genomics, but training an effective model for genomes presents several additional\nchallenges. For instance, unlike proteins, which are functionally important units and relatively\nsmall in size, most genomes are much larger and often contain vast amounts of complex, non-\nfunctional regions that overshadow the amount of functional elements. In addition, the number\nof available whole-genome sequences across the tree of life is minuscule compared to the hundreds\nof millions of protein sequences, limiting the diversity of functionally important DNA elements in\ntraining data. Despite these issues, we believe that language models trained on genomes – referred\nto as genomic language models (gLMs) – hold great promise for biology. In this article, we review\nsome of the key opportunities and challenges in this domain, and outline major considerations that\nshould be addressed to develop and evaluate gLMs that would be useful to the genomics community.\nAPPLICATIONS\nThe general language model framework is summarized in Box 1. Below, we elaborate on three\nmain application areas of gLMs: Functional constraint prediction, Sequence design, and Transfer\nlearning.\nFunctional constraint prediction\nAn intriguing application of gLMs is the prediction of functional constraint on a genomic locus\nwithout any supervision on the task. A significant benefit of this approach is its independence from\nlabels, such as whether a variant is disease-causing, which are often limited and subject to biases.\nThe underlying idea is that reference genomes, typically derived from healthy individuals, are rela-\ntively depleted of deleterious variants. Consequently, models trained on these data are predisposed\nto assigning lower probabilities to harmful variants. This observation underpins the strategy of us-\ning the log-likelihood ratio (LLR) between two alleles (i.e., log[ P(Xi = a | X−i)/P(Xi = b | X−i)]))\nto estimate their relative fitness.\n2\nBox 1: General Language Model Framework.\nAt a high level, a language model is trained to learn the conditional probability distribution\nof the form P[Xi | X−Masked] for i ∈ Masked (in Masked Language Modeling, MLM)\nor P[Xk | X1:k−1] (in Causal Language Modeling, CLM), where X = (X1, X2, . . .)\ndenotes a sequence of “tokens” (e.g., nucleotides or amino acids) and “Masked” denotes\na collection of masked positions. The key to recent advances in NLP is that, instead\nof fitting a simple parametric model of context dependency that one designs by hand,\none lets the data speak for themselves and fit more complex models as more data are\nobserved, by leveraging powerful deep learning architectures. Figure 1 depicts the language\nmodeling framework for DNA. While the model is trained to predict the nucleotide at\neach masked site using information from unmasked sites, it will learn position-specific\ncontextual representation (called embedding, a high-dimensional vector in Rn), which\nthen gets converted into a probability distribution over {A,C,G,T}. These embeddings\nand probability distributions, both of which are position-specific, can be applied to many\nproblems in genomics.\nT r a i n i n g A p p l i c a t i o n s\nC o n t e x t u a l  r e p r e s e n t a t i o n\nH i d d e n  n u c l e o t i d e  p r e d i c t i o n F u n c t i o n a l  c o n s t r a i n t  p r e d i c t i o n\nS e q u e n c e  d e s i g n\nV a r i a n t  l i k e l i h o o d\nL a n g u a g e  m o d e l i n g  h e a d\nT A G ? C C\nC o r e  n e t w o r k\nG e n e  e x p r e s s i o n  p r e d i c t i o n\nG e n e  a n n o t a t i o nT a s k - s p e c i f i c  h e a d s\nS a m p l i n g\n. . .\nT\nT A G ?C CA\nM L M\nC L M\nFigure 1: Training and applications of gLMs. The schematic on the left-hand side\nillustrates gLM training. The log-likelihood ratio (LLR) between two alleles (specifically,\nlog[P(Xi = a | X−i)/P(Xi = b | X−i)]) is a good unsupervised predictor of functional constraint\n(Functional constraint prediction). New sequences can be generated by sampling from the learned\nprobability distribution (Sequence design). A vector representation, called embedding, of each\ntoken in the input sequence can be extracted and adapted for different downstream tasks (Transfer\nlearning).\n3\nP u t a t i v e  f u n c t i o n a l  \nT F B S  a c c o r d i n g  t o  \nF u n T F B S\nA C G T C A G G A\nC o n t r o l  t a g\ng L M\n< h i g h  e x p r e s s i o n >\n< l o w  e x p r e s s i o n >\nG A C A G G C G C\nG e n e r a t e d  s e q u e n c e G e n e  e x p r e s s i o n\nm R N A\nc\nb da\nFigure 2: Application examples. (a)gLM predicted logo plot (top) at a promoter, highlighting a motif\n(bottom logo) that matches a putative functional TFBS. (b) Correlation between variant minor allele fre-\nquency (MAF) and gLM score (log-likelihood ratio). (c) A gLM can be prompted with different control tags\nto design promoter sequences driving high or low expression in a given cell type. (d) Visualization of gLM\nembeddings for different classes of genomic windows, illustrating that the learned representations contain\nuseful information such as gene regions. Note: Panels a,b,d were generated using the GPN model.\nFunctional constraint prediction using the LLR was initially introduced in the context of protein\nsequence models, leading to outstanding results in predicting the effects of missense variants [6, 10–\n12]. Expanding this approach, genome-wide functional constraint prediction using a gLM was first\nundertaken by GPN [13], achieving state-of-the-art results in the model plant Arabidopsis thaliana.\nTo illustrate how a gLM might be able to predict functional constraint, we note that gLMs can\nlearn transcription factor binding site (TFBS) motifs, understanding which positions are under\nconstraint and which are not (Figure 2a). In addition, GPN’s LLR score is correlated with allele\nfrequencies in natural Arabidopsis thaliana populations, even though the model was only trained on\na single genome from this species (Figure 2b). Subsequently, AgroNT [14] and PlantCaduceus [15]\nhave also obtained excellent results in other plant species. For the human genome, however, the\nLLR from the Nucleotide Transformer (NT) [16] fell short of existing baselines. Meanwhile, GPN-\nMSA [17], leveraging a whole-genome multiple sequence alignment (MSA) across diverse vertebrate\nspecies, was able to attain state-of-the-art performance (see Learning objective for further MSA\nconsiderations). It should be noted that the observed nucleotide distribution is driven not only by\nfunctional constraint but also by mutational biases; explicitly incorporating this information into\nfunctional constraint prediction is a promising avenue of future research.\nFor a single nucleotide polymorphism (SNP), the LLR can be computed in a single query to an\nMLM with the variant position masked, but in two queries to a CLM on the reference and alternate\nsequences. A CLM can just as easily handle multiple substitutions, insertions and deletions, while\nan MLM must resort to a more expensive pseudo-LLR [12, 18]. Scores other than the LLR have\nbeen proposed for functional constraint prediction, such as the distance in embedding space [14, 16]\nor the change in nucleotide probabilities in positions around a mutation [19]. While the LLR is\nwidely used in both the pLM and gLM communities, it is important to understand better in which\ncontexts these alternative scores are useful.\nThere are two main kinds of variant effect predictors in genomics: functional constraint predic-\ntors, including gLMs and traditional conservation scores [20, 21], andactivity predictors, such as the\ngene expression predictor Enformer [22] or the splicing predictor SpliceAI [23]. These two kinds of\n4\nmodels are related in the sense that if a variant at a locus is under selection, it induces a change in\nactivity in some context (e.g., change in transcription of a certain gene during limb development),\nultimately affecting a high-level trait (e.g., polydactyly). Functional constraint models cover all\npossible mechanisms and contexts that affect the overall organismal fitness, while activity models\nreflect only those they are explicitly trained on (some data, such as protein expression in the de-\nveloping human brain, are just hard to obtain). On the other hand, activity models can nominate\na specific mechanism and context through which a variant acts, while functional constraint models\ndo not offer a mechanistic interpretation.\nWith regards to functional variant prioritization, there are some additional considerations.\nAn activity model typically gives similar scores to two variants that induce a similar expression\nfold-change but in different genes, even if there is a vast difference in physiological tolerance to\ntheir expression levels. On the other hand, a trait not under detectable selection could still be of\nscientific or medical interest. In this case, a functional constraint model would have limited power\nto prioritize variants affecting it, especially if they have small effect sizes, as is the case in complex\ntrait GWAS. However, while a gLM’s LLR might not have high power in this setting, gLM’s learned\nembeddings (Box 1) could still provide value with additional supervision on labeled data [24].\nSequence design\nDesigning novel biological sequences is of great interest to both the academic and industry research\ncommunities due to its immense potential in drug discovery and delivery; agricultural improvement;\nbioremediation; and the development of biological research tools. We here describe sequence gener-\nation with a CLM (Box 1) as it is the most common approach (see Learning objective for generation\nwith MLMs). Specifically, the sequence generation task is decomposed into a series of next-token\nprediction problems. Starting with a given sequence fragment (referred to as prompts [25], or con-\ntrol tags [26]), the language model can predict the next token recursively and generate a whole new\nsequence. pLMs have been shown to be powerful tools for protein design [26–29]. Going beyond\ncoding sequences, designing non-coding sequences is also crucial due to its applications such as gene\nand cell therapies [30], as well as synthetic biology [31]. Such design tasks have previously been\naddressed using supervised activity models [32–34], but more recently several works have explored\nthe use of gLMs to tackle this challenge as described below.\nThe model regLM [30] was built upon the causal gLM HyenaDNA [35] and used to perform\nde novo generation of promoter and enhancer sequences. HyenaDNA models are trained or fine-\ntuned on regulatory sequences with control tags prepended. The trained model is then used to\ngenerate new regulatory sequences with given tags (Figure 2c). The authors performed in silico\nevaluation of the diversity and activity of the generated sequences in yeast and human cell lines, and\ndemonstrated the sequences to have desired functionality as well as realistic and diverse sequence\nfeatures.\ngLMs have the unique potential for multi-modal design tasks such as generating protein-RNA\ncomplexes by unifying them as DNA sequence design. For instance, EVO, a gLM trained on\nprokaryote genomes, was used to design novel CRISPR-Cas systems [31]. The model was fine-\ntuned using a dataset of CRISPR-Cas sequences with Cas subtype-specific prompt prepended.\nThe fine-tuned model was able to generate novel CRISPR-Cas sequences that matched the subtype\nprompt and had predicted structures that resemble naturally existing systems.\nAdditionally, gLMs can be potentially used to design organized, functional DNA sequences at\nthe chromosome or genome-scale. Recently, two gLMs, MegaDNA and EVO, have explored such\ndesign tasks for prokaryote genomes [31, 36]. EVO was used to generate 20 sequences of size about\n650 Mbp. The generated sequences were found to have realistic coding sequence density, protein\n5\nBox 2: Transfer Learning in NLP\nFor NLP models to generalize on most tasks (including typical tasks, such as sentiment\nanalysis, question answering, and part-of-speech tagging, to name only a few), models need\nto understand grammar and meaning. However, data specific to these tasks are limited.\nUtilizing LLMs trained on raw text data (sourced from articles, books, and websites)\nfor transfer learning has enabled breakthrough progress on these problems [43]. Today,\nvirtually every state-of-the-art NLP model is adapted from an LLM.\nTransfer learning techniques have underpinned the recent boom in natural language mod-\nels. In particular, the availability of pretrained models that are broadly adaptable to\ndownstream tasks—termed “foundation models”—has yielded a major shift in how ma-\nchine learning models are developed [44].\nsequences with predicted secondary structure and globular folds, as well as plausible tRNA se-\nquences. MegaDNA was used to generate full bacteriophage genomes up to 96 kbp. Apart from\nvalidating coding sequences, the author further identified functional regulatory elements including\npromoters and ribosome binding sites in the generated sequences. Yet, such mega-scale DNA se-\nquence design tasks remain challenging. The generated sequences by EVO were found to lack highly\nconserved marker genes that typically exist in functional prokaryote genomes, and the predicted\nprotein structures have limited matches to natural protein databases. A recent independent eval-\nuation [37] revealed that the sequence composition of MegaDNA-generated genomes is still largely\ndissimilar to natural genomes. Therefore, further work is needed to refine the methods to enable\nde novo design of fully functional genomes with gLMs.\nTransfer learning\nNeural networks trained to predict annotations from functional genomics experiments have been\nwidely utilized to interpret the functions of genomic elements. A significant application has been\npredicting variant effects on molecular phenotypes, such as gene expression [22, 38–41] and splicing\n[23, 42]. The ability of neural networks to interpret complex interactions between genomic sites has\nmade them essential tools for tackling these important problems, but suitable training data are often\ndifficult to collect and consequently limited. To generalize on prediction tasks, models need to be\ncapable of identifying the broad set of functionally important sequence elements, which may require\nsubstantial data and computation. To overcome the limitations of insufficient data for individual\ntasks, developers have employed transfer learning methods — techniques that leverage knowledge\ngained from training models on one task to improve performance on related tasks. Specifically,\nmost neural networks trained to predict functional annotations have been trained to predict a wide\narray of annotations simultaneously, forcing these models to learn a single unifying representation.\nThis, in turn, has improved their generalization performance.\nLanguage models may also be utilized for transfer learning. (See Box 2 for a discussion of the\nutility of transfer learning for NLP.) One technique is feature extraction: while learning to predict\nthe context-dependent distribution of nucleotides, gLMs transform input genomic sequences into\nintermediate vector representations (Box 1). These representations may distill relevant informa-\ntion, and, therefore, be utilized as features for another model. For example, visualization of gLM\nembeddings reveals that, without any supervision, the model has learned to distinguish different\n6\nclasses of genomic elements such as coding sequence and untranslated regions [13] (Figure 2d).\nEmbeddings from different layers can provide information useful for different tasks [45]. Another\nway to utilize language models for transfer learning is to use them as pretrained models: that is,\nto continue training them on downstream tasks. This technique is called fine-tuning. Fine-tuning\na pretrained neural network on a task implicitly regularizes its parameters such that the network’s\npredictions synthesize knowledge from both tasks. As a result, pretraining neural networks tends\nto improve their generalization performance on downstream tasks. In recent work, SegmentNT, a\nmodel developed by fine-tuning the Nucleotide Transformer (NT) gLM [16] to the task of anno-\ntating genes and cis-regulatory elements, achieved state-of-the-art performance on this task [46].\nUtilizing a pretrained model was shown to be critical to its success. Similarly, AgroNT [14], an-\nother model of the NT family, was pretrained on diverse plant species and then fine-tuned to predict\nchromatin accessibility and gene expression on select crop species. DNABERT-S [47] applied con-\ntrastive learning with pretrained DNABERT-2 [48] embeddings to perform metagenomics binning.\nIsoFormer [49] is an example of multi-modal transfer learning between DNA and protein language\nmodels for the task of predicting transcript isoform expression. These recent successes suggest that\nfine-tuned gLMs may make meaningful progress on diverse genome interpretation tasks.\nTwo recent studies evaluated several gLMs in prediction tasks in the human genome and found\nthat they generally did not outperform non-gLM baselines [50, 51]. These results were based on\nfrozen embeddings; evaluating full fine-tuning would provide additional insights. While gLMs are\nalready well suited to demonstrate the value of transfer learning in less-studied organisms, further\ninnovation may be required for them to offer significant value in human genetics, where high-\nquality labeled data and carefully crafted models already exist. An important question is how far\nthe scaling hypothesis holds for gLMs, i.e., how much increasing unlabeled data and computation\nwill keep improving model performance. A recent pLM study found that scaling improved only\nprotein structure prediction but not most other tasks such as function or property prediction [52],\nso gLM tasks should also be held to the same scrutiny.\nDEVELOPMENT\nWe now describe the key components of developing useful gLMs; a schematic diagram summarizing\nthe development pipeline is illustrated in Figure 3. We first describe the importance of selecting and\npreparing training data, and then discuss architectural and training decisions. We then consider\ninterpreting and benchmarking gLMs. Our aim is to provide insights into the methodologies and\nchallenges encountered in developing gLMs that are both effective and efficient. To provide a\ncomprehensive view of the current landscape in the field, we list in Table 1 some of the existing\ngLMs that we are aware of and summarize their design decisions.\nTraining data\nThe performance of a machine learning model is significantly influenced by both its architecture\nand its training data. Various model architectures such as convolutional neural networks (CNNs),\nTransformers, and state-space models have been successfully adapted to a wide range of domains,\nincluding natural language, images, audio, proteins, and genomics. However, selecting suitable\ndata for pretraining requires a deep understanding of the specific domain, especially in genomics\nwhere there is no universally accepted, curated dataset comparable to those in NLP (e.g., the Pile\n[81, 82]) or protein biology (e.g., UniProt [4]).\nA key consideration is data quality. For example, in NLP this may refer to data sources that\nhave undergone editing or peer review, such as scientific articles or books [82]. In the case of\n7\nModel Name Pretraining data sourcesTaskArchitectureTokenizationNotes\nBigBird[53] Human MLMTransformerBPE\nDNABERT[54] Human MLMTransformeroverlappingk-mer\nGeneBERT[55] Human MLMTransformeroverlappingk-mer Trained to also predict chromatin accessibilityATAC-seq data.\nEpigenomic BERT[56] Human MLMTransformernon-overlappingk-merDNA sequences are paired with associated epi-genetic state information (IDEAS) [57] duringtraining.\nLookingGlass[58] Bacteria + archaea CLMRecurrent NeuralNetwork nucleotide-levelMetagenomic sequences from diverse environ-ments rather than assembled genomes are usedfor training.\nLOGO[59] Human MLMCNN + Trans-former overlappingk-mer\nViBE[60] Virus MLMTransformeroverlappingk-mer\nGPN[13] Arabidopsis thaliana+ 7 relatedBrassicales genomes MLMCNN nucleotide-level\nFloraBERT[61] Several hundred plants + selectedmaize genomes MLMTransformerBPE Only 1kb promoter sequences are used in train-ing.\nINHERIT[62] Bacteria + bacteriophageMLMTransformeroverlappingk-mer\nGenSLMs[63] Prokaryotic gene sequences +SARS-CoV-2 genomesCLMTransformernon-overlappingk-merPretrain on prokaryotic genes and fine-tune onSARS-CoV-2 genomes.\nNT[16] Human + 1000 Genomes Project +multi-species MLMTransformernon-overlappingk-mer\nSpliceBERT[64] Human + 71 vertebrate genomesMLMTransformernucleotide-levelOnly RNA Transcripts are used in training.\nSpeciesLM Fungi[65] 1500 fungal genomes MLMTransformeroverlappingk-mer Only 5′and 3′UTR regions are used in training:the 5′species LM and 3′species LM.\nGENA-LM[66] Human + multi-speciesMLMTransformerBPE\nDNABERT-2[48] Human + multi-speciesMLMTransformerBPE\nHyenaDNA[35] Human CLMSSM nucleotide-level\nGROVER[67] Human MLMTransformerBPE\nDNAGPT[68] Human + multi-speciesCLMTransformernon-overlappingk-mer\nGPN-MSA[17] Human + Multiple Sequence Align-ment (MSA) with 100 vertebrategenomes MLMTransformernucleotide-level\nUTR-LM[69] Human + 4 vertebrate genomesMLMTransformernucleotide-level\nOnly 5′ UTR regions are used in training.Trained also to predict mRNA minimum freeenergy and secondary structures calculated byViennaRNA [70].\nhgT5[71] Human T5 [72]TransformerUnigram model [73]\nAgroNT[14] 48 plant genomes focusing on edibleplant species MLMTransformernon-overlappingk-mer\nMegaDNA[36] ∼100k bacteriophage genomesCLMTransformernucleotide-level\nregLM[30] Human + yeast CLMSSM nucleotide-levelHuman enhancer and yeast promoter sequencesare used to fine-tune/pretrain separate Hye-naDNA [35] models\nEVO[31] Bacteria + archaea + virus + plas-mid CLMSSM + Trans-former nucleotide-level\nCaduceus[24] Human MLMSSM nucleotide-level\nChatNT[74] Genomic sequences + English in-structions CLMTransformeroverlappingk-mer\nCombines the pretrained gLM NT [16] and theEnglish LM Vicuna [75]. Trained to perform allsupervised genomics prediction tasks as text-to-text tasks.\nLucaOne[76] Genomic and protein sequencesfrom 169,861 species MLMTransformernucleotide- and aminoacid-level\nMixed pretraining with DNA, RNA, and proteinsequences. Trained also to predict 8 types ofselected annotations.\nPlantCaduceus[15] 16 Angiosperm genomesMLMSSM nucleotide-level\nCD-GPT[77] Genomic and protein sequences of14 organisms CLMTransformerBPE Mixed pretraining with DNA, RNA, and proteinsequences, followed by targeted DNA-Proteinand mRNA-Protein paired pretraining.\nSpeciesLM Metazoa[19]494 metazoan genomesMLMTransformeroverlappingk-mer Only trained on 2 kb upstream of start codons\ngLM2[78] Metagenomes and genomes fromIMG [79] and MGnify [80].MLMTransformerBPE for nucleotides,amino acid-level for pro-teins\nPretraining with a mixed-modality dataset,comprising interleaved protein-coding (aminoacid) and intergenic (nucleotide) sequences.\nTable 1: A summary of existing gLMs.An overview of various gLMs is provided, highlighting their\npretraining datasets, tasks, architectures, tokenization methods, and unique features. The models are listed\nin the order of their public release dates. Abbreviations used include SSM for State Space Model, CNN\nfor Convolutional Neural Network, BPE for Byte-Pair Encoding, CLM for Causal Language Modeling, and\nMLM for Masked Language Modeling.\n8\nproteins, quality control involves removing predicted pseudogenes or truncated proteins that are no\nlonger functional [4]. However, a recent study found only 3.3% of the bases in the human reference\ngenome, the most popular gLM training dataset (Table 1), to be significantly constrained and likely\nfunctional [83]. Importantly, a typical genomic sequence used for training a gLM will contain a\nmix of functional and non-functional sites, and one cannot always separate training examples into\nhigh vs. low quality. A proposed solution is to have a base-pair-level weighting of the training loss\naccording to the evidence for functionality [17].\nIt is standard in NLP and proteins to filter out duplicated sequences, which improves training\nefficiency and reduces memorization [84]. Despite the fact that a staggering 50% of the human\ngenome is repetitive (a high proportion across eukaryotes), very few gLM studies propose a solution\n(downweighting [13, 15] or downsampling [64, 71]), let alone acknowledge the issue. It would be\ninsightful if studies of language model perplexity [24, 35] would also report it separately for non-\nrepetitive regions, to distinguish improvements due to generalization vs. memorization.\nAnother key question is how to ensure that the amount of data is enough. It is likely that\na single genome might not be enough to train a large model, especially if non-functional regions\nare downsampled or downweighted. One approach is to add sequence variants from the same\nspecies [16]. However, in many species, including humans, there is relatively little variation between\nindividuals. A more common approach is to train across multiple species (Table 1), as typically\ndone for pLMs. As species become more distant, regulatory logic diverges faster than proteins.\nOne proposed approach is to explicitly add a species identifier as an extra input to the model [65].\nNotwithstanding, it is plausible that a large enough model, with enough genomic context, might\nD a t a s e t\nQ u a l i t y  a n d  Q u a n t i t y \nF u n c t i o n a l  r e g i o n  \np r i o r i t i z a t i o \nS e q u e n c e  d i v e r s i t \nS p e c i e s  s e l e c t i o \nR e p e a t  d o w n s a m p l i n g\nO b j e c t i v e\nM a s k e d  L a n g u a g e  \nM o d e l i n g\nC a u s a l  L a n g u a g e  \nM o d e l i n g\nF i n e - t u n i n g  \nT a s k s\nI n t e r p r e t a t i o n\nS e q u e n c e  L o g o\n R e p r e s e n t a t i o n\nV i s u a l i z a t i o n\nN u c l e o t i d e  \nD e p e n d e n c i e s\nE v a l u a t i o n\nR e v i e w  e x i s t i n g  \nb e n c h m a r k \nN e w  b e n c h m a r k ?\n       -  R e a l i s t i c  u s e  c a s e s\n       -  B r o a d  b a s e l i n e s\n       -  D o c u m e n t a t i o n\nA r c h i t e c t u r e\nM i x & M a t c hT r a n s f o r m e r\nC N N\nS S M\nM i x  &  M a t c h\nA t t e n t i o n  M a p\nFigure 3: Development Pipeline.This figure illustrates the general gLM development pipeline described\nin this review, from model conception to deployment. We begin with the selection and preparation of the\ntraining dataset, emphasizing the importance of data quality and quantity (Training data). Subsequently,\nin Model architecture and Learning objective, we explore the various choices for designing and training\ngLMs, discussing the strengths and weaknesses of different approaches. We also examine how hybrid models\ncombine elements from multiple architectures to mitigate specific limitations. In Interpretation, we discuss\nmethods for analyzing and interpreting the outputs of gLMs. Finally, in Evaluation, we present evaluation\nmethods through current benchmarks, emphasizing the complexities in aligning model performance with\nactual biological functions.\n9\nbe able to naturally model distant genomes, similarly to how LLMs handle multilingual datasets.\nAs mentioned earlier, in prokaryotes, there exist models (MegaDNA and EVO) that take an\nentire genome as context [31, 36]. This is currently infeasible for eukaryotes, and therefore leads to\nthe question of how to partition the genome into context windows to be separately modeled. Many\ninteractions are restricted to nearby positions, such as transcription factor binding site motifs,\nmotivating the development of models with a relatively small context (< 6 kb) (Table 1). However,\nthere are obvious long-range interactions, such as between exons of the same gene or between\nenhancers and promoters (up to 1 Mb) [85]. Such long context lengths introduce computational\nand statistical challenges, and efforts have been made to overcome them [24, 31, 35, 36]. Regardless\nof the chosen context length, it is still not easy to partition the genome into independent units\n(similarly to how proteomes are separated by protein). For instance, the enhancer of a gene can\nbe located inside the intron of another gene [85], and multiple genes can be controlled by the same\nenhancer [86]. Avoiding potential data leakage due to orthology and paralogy is quite challenging,\nespecially when training across species.\nThe choice of training data may significantly influence gLMs’ outputs and learned representa-\ntions. DNA sequences observed in nature are the outcome of various evolutionary processes, the\nforemost of which are mutation and selection [87]. For certain applications, it may be desirable to\ncurate training data such that one of these processes is more manifest than the other. For example,\nfor the sake of fitness prediction, it may be desirable to exclude/downweight hypermutable sites\n(such as CpG sites) and nonfunctional regions (such as certain classes of repetitive elements).\nModel architecture\nCNN models [38–41] have been widely used in genomics for supervised tasks prior to the emergence\nof the Transformer architecture [1]. CNNs are particularly effective at capturing local dependencies\nand motifs within genomic sequences through their ability to apply filters across the input data.\nThese models have been successful in predicting DNA-protein binding sites, regulatory elements,\nand TFBS. GPN [13], the aforementioned gLM for genome-wide variant effect prediction in Ara-\nbidopsis thaliana, took inspiration from the success of language models with modified CNN layers\nin NLP [88] and protein modeling [89], and replaced self-attention layers in a Transformer encoder\nwith dilated CNN layers.\nTransformer models have revolutionized various machine learning domains, particularly in\nNLP [1], and have recently been widely adopted for genomics modeling. The self-attention mecha-\nnism allows each token to attend to all positions in the input sequence simultaneously, enabling the\nmodel to dynamically focus on relevant parts of the sequence. This capability has led to significant\nadvancements in detecting regulatory mechanisms for supervised gene expression tasks [22, 90].\nDespite their strengths, Transformer models face several challenges unique to genomic modeling.\nOne significant issue is that Transformers have weak or no inductive biases regarding the locality of\ninteractions [53, 91], making them less data-efficient at modeling local motifs such as TFBS. This\nmotivated the development of CNN-Transformer hybrids such as LOGO [59], following supervised\nmodels such as Enformer [22].\nAnother challenge is the context length: the self-attention mechanism results in computational\ntime and memory scaling quadratically with the input sequence length, making it impractical to ap-\nply Transformers to very long genomic sequences [92]. Consequently, the longest input length that\nconventional attention-based gLMs can handle so far is 12 kb for NT-v2 [16]. To address this lim-\nitation, several Transformer-based gLMs have implemented approximate attention or hierarchical\nattention methods that sacrifice full pairwise attention between all tokens. These methods include\nthe use of sparse attention [53] in GENA-LM [66], which extends the context length to 36 kb,\n10\nand the MEGABYTE sub-quadratic hierarchical self-attention [93] employed in MegaDNA [36],\nachieving a context length of 96 kb.\nTo overcome the quadratic scaling issues of self-attention, various state-space models (SSMs) [94–\n96] have been proposed for gLMs as efficient alternatives to Transformers, offering nearly linear\nscaling with sequence length. HyenaDNA [35], based on the Hyena Hierarchy [95], can support\ninput contexts as long as 1 million nucleotides. EVO [31], a hybrid model combining Hyena and\nTransformer architectures, is pretrained with 8 kb sequences and later fine-tuned with 131 kb se-\nquences during the context extension stage. Caduceus [24], built on the Mamba-based SSM [96],\nis trained on 131 kb sequences while incorporating reverse-complement equivariance.\nLearning objective\nAs described in Box 1, the MLM task (sometimes also called “masked token prediction”) involves\npredicting the identities of tokens randomly omitted from sequences with a predetermined prob-\nability (a common choice is 15%) given the remaining tokens. This framework has been used to\ntrain the seminal LLM BERT [43] and pLM ESM-1b [97], and has since been widely used for train-\ning gLMs. The CLM task (also referred to as “autoregressive language modeling” or “next token\nprediction”) involves predicting the identities of tokens in sequences given their preceding tokens;\nit has been used to train the GPT series of LLMs [25]. In this task, the model predicts the next\ntoken given the previous tokens in a unidirectional, left-to-right order. A commonality between\nthese two tasks is that they require models to predict components of data given other components\nas context. To generalize on these tasks, models must learn low-dimensional representations of the\ndata. This capability enables the gLMs to understand and generate genomic sequences by captur-\ning the underlying patterns and dependencies within the genome. In protein modeling, MLM tends\nto achieve better representations and transfer learning capabilities than CLM [98]. On the other\nhand, CLMs are the traditional choice for generation tasks, but excellent results have been recently\nobtained with MLMs via progressive unmasking [99, 100].\nTo reduce input sequence length and model longer context, both k-mer and byte-pair encod-\ning [101] (BPE) tokenizations create artificially defined nucleotide vocabularies larger than the\nnatural nucleotide vocabularies of {A, C, G, T}. On the other hand, single-nucleotide tokeniza-\ntion simplifies model interpretation and attribution, and enhances the model’s ability to handle\ngenomic variations more effectively.\nSeveral modifications to the training objective have been explored to provide additional sig-\nnal and boost performance. For instance, GPN-MSA [17] enhances MLM training on the human\nreference genome with a whole-genome MSA [102, 103] of vertebrate species, leveraging conserva-\ntion across related species for additional context. A limitation is that whole-genome MSAs have\nonly been generated for certain species, and might require further development to be effective in\nplants [104]. Additionally, cis regulatory elements tend to diverge fast in sequence space even if\nthey have conserved activity, which limits the orthology information that can be extracted via\nalignment [105]. Species LM [65] directly integrates species information by assigning a dedicated\ntoken for each yeast species and appending the species token to the input sequence during train-\ning and inference. Pretraining on nucleotide sequences has been expanded to enable cross-talk\nwith additional modalities such as epigenetics [55, 56], RNA [76, 77], proteins [76–78], and natural\nlanguage [74].\n11\nInterpretation\nDeep learning models, while having achieved remarkable performance in various prediction tasks,\ntypically lack interpretability and are often used as “black boxes”. However, understanding how\nthese models generate such predictions is crucial for enabling broader applications and advancing\nmodel development. As a result, a series of methods have been developed to interpret deep learning\nmodels, including those specific to genomics [106–108]. While the interpretation of gLMs is still an\nemerging line of research, several models have been shown to have learned meaningful biological\npatterns.\nThe sequence embeddings extracted from language models are commonly used as representa-\ntions that capture rich contextual information and sequence features. Unsupervised clustering of\nthe encoded sequence embeddings from gLMs has shown distinct clusters of input sequences that\ncorrespond to different genomic classes such as CDS, intronic, UTR, etc. [13, 16, 35, 54] (Fig-\nure 2d). Additionally, unsupervised clustering of SpliceBERT embeddings of canonical splice sites\nand non-splice GT/AG sites reveals distinct clusters that correspond to the two groups [64]. These\nresults suggest that the models have learned to capture key contextual patterns that characterize\nfunctional elements in the genome.\nThe attention mechanism in the Transformer model is designed to capture the pattern of in-\nteraction between input tokens. Thus, interpreting the attention weights or the attention map for\na given input sequence can reveal genomic features learned by the model. In SpliceBERT [64],\nattention weights between splice donors and acceptors are significantly higher than those between\nrandom pairs of sites; also, the strength of interaction tends to be higher within true donor-acceptor\npairs compared to other combinations of donor and acceptor sites. These findings suggest that the\nmodel has learned the relationship between functionally interacting sites.\nThe nucleotide reconstruction approach has also been used in several gLMs to discover sequence\nmotifs learned by the models. Specifically, individual positions of the input sequence are masked\none at a time and the probability distribution of the nucleotides is predicted by the trained model\ngiven the genomic context. The obtained distribution at each site can reveal motifs learned by\nthe model. This approach has been used in GPN to find notable patterns in the distribution of\nthe reconstructed nucleotides. In particular, the model’s predictions are generally more confident\nin functionally important sites. For example, coding sequences and splice donor/acceptor sites\nare typically predicted with higher confidence than deep intronic sites. Moreover, within coding\nsequences, the third nucleotide position of a codon, the least determinant of the translated amino\nacid, is typically predicted with lower confidence than the first two nucleotide positions. Adapting\nTF-MoDISco [109], a dedicated tool to identify novel TFBS using model predictions, the authors\nalso found sequence motifs that match known ones in TFBS databases and relevant literature [13]\n(Figure 2a). Similarly, the reconstructed sequence motifs from Species LM [65] also match the\nbinding sites of known DNA- and RNA-binding proteins in species that are unseen during training,\nwith the fidelity of motif reconstruction depending on the context and genomic regions that correctly\nreflect the in vivo binding sites. Furthermore, the reconstructed motifs’ composition, existence,\nand location exhibit species-specific patterns, which suggests gLM as a potentially powerful tool\nfor investigating the evolution of sequence motifs and regulatory code.\nMore recently, the dependency between genomic positions learned by a gLM was studied by\nintroducing point mutations at a position and quantifying the changes in nucleotide probabilities at\nother positions [19]. Nucleotide dependency analysis revealed learned interactions within and across\nfunctional elements such as TFBS, splice sites and RNA, including known secondary and tertiary\nstructure contacts. Notably, nucleotide dependency analysis was able to detect bound TFBS more\nrobustly than the previous approach based on predicted marginal probability distributions.\n12\nBox 3: Evaluating Generalization Performance\nThe purpose of evaluating predictive models is to build trust in their capability to gen-\neralize – that is, to make satisfactory predictions for unlabeled data. A straightforward\nand standard way to estimate the generalization performance of a model is to evaluate\nits accuracy on a “test set” of labeled data that are representative of unlabeled data of\ninterest [119]. This approach is the basis of most machine learning benchmarks.\nImportantly, for this evaluation to be a reliable indicator of generalization performance,\nmodels must not be provided any information that may be used to differentiate test set\ndata from the data they will ultimately be deployed on. Otherwise, they may decrease\ntheir test set error at the expense of their generalization performance. For this reason,\nmachine learning contests that withhold their test data from participants are routinely\norganized [120–122].\nEvaluation\nIn this section, we discuss how models’ performance can be benchmarked in regards to the three\napplication areas described earlier: predicting functional constraints on alleles, generating novel\nviable sequences, and transfer learning.\nThere are various types of data that reflect functional constraint on alleles and may be used to\nbenchmark a variant effect predictor. One type of data are assays that couple functional differences\nbetween genetic variants to readouts (such as the expression of a reporter gene or cell growth)\n[110–112]. These readouts may be used to rank variants by their functionality, and since variants\nthat affect function also tend to be under selection, we should expect that these ranks should\ncorrelate with ranks obtained from models’ predictions. One source for these data is ProteinGym,\na widely-used collection of experimental data that may be used to benchmark missense variant effect\npredictors [113]. Another type of data are clinical labels indicating whether variants have evidence\nof pathogenicity—that is, can elevate the risk for diseases. Pathogenic variants may affect fecundity,\nand, therefore, be deleterious. As a result, we can benchmark variant effect predictors by evaluating\nthem as pathogenicity classifiers. In human genetics, primary sources of clinical labels for variants\ninclude the ClinVar [114], HGMD [115], and OMIM [116] databases. A third type of data are variant\nfrequencies. Since common variants are unlikely to be highly deleterious [117], their predicted level\nof constraint should be relatively higher than those of rare variants. Therefore, we may benchmark\npredictors based on how well they identify common variants. A primary source of data on human\nallele frequencies in various ancestry groups is the gnomAD database [118]. Altogether, these data\nmay be used as separate lines of evidence for models’ generalization performance.\nAn issue with the evaluation of variant effect predictors is that the relationship between val-\nidation data and functional constraint may be murky. As a consequence, models may excel at\nbenchmarks by exploiting the ways in which data fail to capture functional constraint. For exam-\nple, a critical issue with using clinical labels is that variants are classified based on whether there is\nample evidence that they are benign or pathogenic [123]. Since predictors may also utilize this ev-\nidence, their benchmarked performance on labeled variants may not reflect their true performance\non unlabeled variants. (See Box 3 for a brief discussion of generalization performance.) There are\nalso critical issues with using allele frequency data: for one, in addition to the direct action of\nnatural selection, allele frequencies are influenced by factors such as mutation rates, drift, back-\n13\nground selection, and hitchhiking [124]. As a result, predictors may perform well on benchmarks\nby predicting the effects of these processes instead of functional constraint. These issues highlight\na need to carefully interpret the causes of predictors’ performance, and they have led to calls for\ngreater transparency on which data and methods are used to train predictors [125].\nThere are a separate set of challenges with the evaluation of generative sequence models. A\nbasic way to evaluate language models’ generative capabilities is to compare their perplexities\non sets of valid sequences. However, to evaluate models’ capability to design novel sequences, it\nis necessary to gauge whether they can identify sequences that are both viable and novel. For\nthis reason, models’ perplexities on test sets may not reliably indicate their utility for design.\nInstead, a holistic approach that examines a broad range of properties of generated sequences may\nbe warranted. For instance, Polygraph [126], a recent benchmark for regulatory sequence design,\nproposes a series of analyses that investigate sequence composition, motif patterns, and predicted\nfunctional activity. For whole-genome or chromosome design tasks, it may also be necessary to\nevaluate the existence and positioning of essential genes and functional regulatory elements, as well\nas the interactions between them. Ultimately, the designed sequences should be experimentally\nevaluated to determine if they perform their desired functions.\nLastly, there is a unique challenge with the evaluation of gLMs for transfer learning: any\nset of benchmarks—perhaps, in conjunction—must reliably indicate a model’s performance on\nrelevant tasks. A type of data that may be fashioned into a set of tasks broadly informative of\nmodels’ adaptability to genome interpretation are functional genomics data (such as those from\nthe ENCODE [127] or Roadmap Epigenomics [128] projects), which may be used to annotate\ngenomic regions and variants. We should expect that a model’s performance on predicting these\nannotations from genome sequences after adaptation is indicative of their capability to identify\nfunctionally similar genomic elements. To facilitate comparison between models, these annotations\nhave been consolidated into various standardized sets of training and test data [16, 50, 71, 129].\nAs transfer learning benchmarks help highlight limitations of current models and establish\ncriteria for publication, they are likely to be important assets for gLM developers and users. How-\never, despite differences in current benchmarks’ choice of tasks and methodologies, they provide\nseemingly redundant insight into gLMs’ capabilities. Moving forward, it will be incumbent on the\ncomputational genomics community to develop standardized and extensible benchmarks that are\nwidely trusted.\nCONCLUDING REMARKS & FUTURE PERSPECTIVES\nIn an age of a vast and growing number of genomic sequences, gLMs are emerging as powerful\ntools to extract complex patterns useful for numerous applications, including functional constraint\nestimation, sequence design and transfer learning. However, they do not yet represent a magical,\nsudden breakthrough as the term “AI” may suggest. Instead, we view them as another useful\nmodeling tool, much like Hidden Markov Models were when they were first introduced. Often,\ngLMs are claimed to be “foundation models”, a term recently invented to denote models trained\non broad data that can be adapted to a wide range of downstream tasks [44]. The introduction\nof this new term has been criticized as the word “foundation” has the connotation of substantial\nimprovement on downstream task performance, which is an empirical question, not an inherent\nproperty of pretrained models [130]. This criticism rings even louder in new domains such as\ngenomics, where establishing adequate benchmarks is likely to take some time.\nWhile earlier gLMs tend to be more or less direct adaptations from NLP models, we expect\nthat further contextualization with deep genomics expertise will reap the highest rewards. We\n14\nnote that evaluating the capabilities of gLMs is challenging because metrics may be misleading,\nespecially when over-optimized. A boon for NLP is that humans are experts in natural language\nand, therefore, can calibrate benchmarks to match their expertise. In genomics, however, we must\nrely on data and expert knowledge to falsify models. This aspect of the problem makes it especially\nchallenging and may highlight a need for engagement with subject-matter experts and deliberate\nexperimentation for the sake of developing benchmarks. We conclude this review with a few research\ndirections (listed in Outstanding Questions) that we believe warrant further investigation.\nOutstanding Questions\n1. How can we best model patterns across a wide range of scales, from motifs to genes to\nwhole genomes?\n2. For which applications is it important to model long-range interactions and how does\none determine a suitable size of the receptive field?\n3. How can we incorporate structural variations into gLMs?\n4. What is the best way to utilize population genetic data when training gLMs?\n5. How can we best integrate gLMs with other complex modalities, such as transcriptomic\nand epigenetic data?\n6. For developing gLMs, can we better understand what makes some genomes harder to\nmodel than others?\n7. Will the scaling hypothesis hold for gLMs, and for how long? Are there really that\nmuch data available, considering that most may be non-functional?\nAcknowledgments\nThis work is supported in part by an NIH grant R35-GM134922, a grant from the Koret-UC\nBerkeley-Tel Aviv University Initiative in Computational Biology and Bioinformatics, and a grant\nfrom the Noyce Initiative UC Partnerships in Computational Transformation Program.\nDeclaration of Interests\nThe authors declare no competing interests.\n15\nReferences\n1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser,  L., and\nPolosukhin, I. Attention is all you need. In: Guyon, I., Luxburg, U. V., Bengio, S., Wallach,\nH., Fergus, R., Vishwanathan, S., and Garnett, R., eds. Advances in Neural Information\nProcessing Systems vol. 30. Curran Associates, Inc. (2017):.\n2. Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z.,\nWu, Y. et al. (2020). Conformer: Convolution-augmented transformer for speech recognition.\narXiv preprint arXiv:2005.08100. https://arxiv.org/abs/2005.08100.\n3. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D.,\nAltenschmidt, J., Altman, S., Anadkat, S. et al. (2023). GPT-4 technical report. arXiv\npreprint arXiv:2303.08774. https://arxiv.org/abs/2303.08774.\n4. Bateman, A., Martin, M.-J., Orchard, S., Magrane, M., Ahmad, S., Alpi, E., Bowler-Barnett,\nE. H., Britto, R., Cukura, A., Denny, P. et al. (2023). UniProt: the universal protein knowl-\nedgebase in 2023. Nucleic Acids Research 51, D523–D531.\n5. Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O.,\nShmueli, Y. et al. (2023). Evolutionary-scale prediction of atomic-level protein structure with\na language model. Science 379, 1123–1130.\n6. Meier, J., Rao, R., Verkuil, R., Liu, J., Sercu, T., and Rives, A. Language mod-\nels enable zero-shot prediction of the effects of mutations on protein function. In:\nRanzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., eds.\nAdvances in Neural Information Processing Systems vol. 34. Curran Associates, Inc.\n(2021):( 29287–29303). https://proceedings.neurips.cc/paper_files/paper/2021/\nfile/f51338d736f95dd42427296047067694-Paper.pdf.\n7. Truong Jr, T., and Bepler, T. PoET: A generative model of protein families as sequences-of-\nsequences. In: Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine,\nS., eds. Advances in Neural Information Processing Systems vol. 36. Curran Associates,\nInc. (2023):( 77379–77415). https://proceedings.neurips.cc/paper_files/paper/2023/\nfile/f4366126eba252699b280e8f93c0ab2f-Paper-Conference.pdf.\n8. Bepler, T., and Berger, B. (2021). Learning the protein language: Evolution, structure, and\nfunction. Cell Systems 12, 654–669.\n9. Ruffolo, J. A., and Madani, A. (2024). Designing proteins with language models. Nature\nBiotechnology 42, 200–202.\n10. Riesselman, A. J., Ingraham, J. B., and Marks, D. S. (2018). Deep generative models of\ngenetic variation capture the effects of mutations. Nature Methods 15, 816–822.\n11. Frazer, J., Notin, P., Dias, M., Gomez, A., Min, J. K., Brock, K., Gal, Y., and Marks, D. S.\n(2021). Disease variant prediction with deep generative models of evolutionary data. Nature\n599, 91–95.\n12. Brandes, N., Goldman, G., Wang, C. H., Ye, C. J., and Ntranos, V. (2023). Genome-wide\nprediction of disease variant effects with a deep protein language model. Nature Genetics.\nhttps://doi.org/10.1038/s41588-023-01465-0 . doi:10.1038/s41588-023-01465-0 .\n16\n13. Benegas, G., Batra, S. S., and Song, Y. S. (2023). DNA language models are powerful pre-\ndictors of genome-wide variant effects. Proceedings of the National Academy of Sciences 120,\ne2311219120.\n14. Mendoza-Revilla, J., Trop, E., Gonzalez, L., Roller, M., Dalla-Torre, H., de Almeida, B. P.,\nRichard, G., Caton, J., Lopez Carranza, N., Skwark, M., Laterre, A., Beguir, K., Pierrot,\nT., and Lopez, M. (2024). A foundational large language model for edible plant genomes.\nCommunications Biology 7, 835. https://doi.org/10.1038/s42003-024-06465-2 . doi:10.\n1038/s42003-024-06465-2 .\n15. Zhai, J., Gokaslan, A., Schiff, Y., Berthel, A., Liu, Z.-Y., Miller, Z. R., Scheben,\nA., Stitzer, M. C., Romay, C., Buckler, E. S., and Kuleshov, V. (2024). Cross-species\nplant genomes modeling at single nucleotide resolution using a pre-trained DNA language\nmodel. bioRxiv preprint. https://www.biorxiv.org/content/early/2024/06/05/2024.\n06.04.596709. doi:10.1101/2024.06.04.596709.\n16. Dalla-Torre, H., Gonzalez, L., Mendoza Revilla, J., Lopez Carranza, N., Hen-\nryk Grywaczewski, A., Oteri, F., Dallago, C., Trop, E., Sirelkhatim, H., Richard, G. et al.\n(2023). The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for\nHuman Genomics. bioRxiv preprint. https://www.biorxiv.org/content/10.1101/2023.\n01.11.523679v3.\n17. Benegas, G., Albors, C., Aw, A. J., Ye, C., and Song, Y. S. (2023). GPN-MSA: an alignment-\nbased DNA language model for genome-wide variant effect prediction. bioRxiv preprint.\nhttps://www.biorxiv.org/content/10.1101/2023.10.10.561776v2.\n18. Hsu, C., Nisonoff, H., Fannjiang, C., and Listgarten, J. (2022). Learning protein fitness models\nfrom evolutionary and assay-labeled data. Nature Biotechnology 40, 1114–1122.\n19. Tomaz da Silva, P., Karollus, A., Hingerl, J., Galindez, G., Wagner, N., Hernandez-Alias,\nX., Incarnato, D., and Gagneur, J. (2024). Nucleotide dependency analysis of DNA language\nmodels reveals genomic functional elements. bioRxiv preprint ( 2024–07). https://www.\nbiorxiv.org/content/10.1101/2024.07.27.605418v1.\n20. Siepel, A., Bejerano, G., Pedersen, J. S., Hinrichs, A. S., Hou, M., Rosenbloom, K., Clawson,\nH., Spieth, J., Hillier, L. W., Richards, S. et al. (2005). Evolutionarily conserved elements in\nvertebrate, insect, worm, and yeast genomes. Genome Research 15, 1034–1050.\n21. Pollard, K. S., Hubisz, M. J., Rosenbloom, K. R., and Siepel, A. (2010). Detection of non-\nneutral substitution rates on mammalian phylogenies. Genome Research 20, 110–121.\n22. Avsec, ˇZ., Agarwal, V., Visentin, D., Ledsam, J. R., Grabska-Barwinska, A., Taylor, K. R.,\nAssael, Y., Jumper, J., Kohli, P., and Kelley, D. R. (2021). Effective gene expression predic-\ntion from sequence by integrating long-range interactions. Nature Methods 18, 1196–1203.\n23. Jaganathan, K., Panagiotopoulou, S. K., McRae, J. F., Darbandi, S. F., Knowles, D., Li, Y. I.,\nKosmicki, J. A., Arbelaez, J., Cui, W., Schwartz, G. B. et al. (2019). Predicting splicing from\nprimary sequence with deep learning. Cell 176, 535–548.\n24. Schiff, Y., Kao, C.-H., Gokaslan, A., Dao, T., Gu, A., and Kuleshov, V. (2024). Caduceus: Bi-\ndirectional equivariant long-range DNA sequence modeling. arXiv preprint arXiv:2403.03234.\nhttps://arxiv.org/abs/2403.03234.\n17\n25. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan,\nA., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan,\nT., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E.,\nLitwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,\nI., and Amodei, D. Language models are few-shot learners. In: Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H., eds. Advances in Neural Information Processing Systems\nvol. 33. Curran Associates, Inc. (2020):( 1877–1901). https://proceedings.neurips.cc/\npaper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n26. Madani, A., Krause, B., Greene, E. R., Subramanian, S., Mohr, B. P., Holton, J. M., Olmos,\nJ. L., Xiong, C., Sun, Z. Z., Socher, R. et al. (2023). Large language models generate functional\nprotein sequences across diverse families. Nature Biotechnology 41, 1099–1106.\n27. Ingraham, J., Garg, V., Barzilay, R., and Jaakkola, T. Generative models for graph-based\nprotein design. In: Wallach, H., Larochelle, H., Beygelzimer, A., d 'Alch´ e-Buc, F., Fox,\nE., and Garnett, R., eds. Advances in Neural Information Processing Systems vol. 32. Cur-\nran Associates, Inc. (2019): https://proceedings.neurips.cc/paper_files/paper/2019/\nfile/f3a4ff4839c56a5f460c88cce3666a2b-Paper.pdf.\n28. Hsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B., Sercu, T., Lerer, A., and Rives, A. Learning\ninverse folding from millions of predicted structures. In: International Conference on Machine\nLearning. PMLR (2022):( 8946–8970).\n29. Shin, J.-E., Riesselman, A. J., Kollasch, A. W., McMahon, C., Simon, E., Sander, C., Man-\nglik, A., Kruse, A. C., and Marks, D. S. (2021). Protein design and variant prediction using\nautoregressive generative models. Nature Communications 12, 2403.\n30. Lal, A., Garfield, D., Biancalani, T., and Eraslan, G. regLM: Designing realistic regulatory\nDNA with autoregressive language models. In: International Conference on Research in\nComputational Molecular Biology. Springer (2024):( 332–335).\n31. Nguyen, E., Poli, M., Durrant, M. G., Thomas, A. W., Kang, B., Sullivan, J., Ng, M. Y.,\nLewis, A., Patel, A., Lou, A. et al. (2024). Sequence modeling and design from molecular to\ngenome scale with Evo. bioRxiv preprint ( 2024–02). https://www.biorxiv.org/content/\n10.1101/2024.02.27.582234v2.\n32. Wang, Y., Wang, H., Wei, L., Li, S., Liu, L., and Wang, X. (2020). Synthetic promoter\ndesign in Escherichia coli based on a deep generative network. Nucleic Acids Research 48,\n6403–6412.\n33. Jores, T., Tonnies, J., Wrightsman, T., Buckler, E. S., Cuperus, J. T., Fields, S., and Queitsch,\nC. (2021). Synthetic promoter designs enabled by a comprehensive analysis of plant core\npromoters. Nature Plants 7, 842–855.\n34. de Almeida, B. P., Reiter, F., Pagani, M., and Stark, A. (2022). DeepSTARR predicts en-\nhancer activity from DNA sequence and enables the de novo design of synthetic enhancers.\nNature Genetics 54, 613–624.\n35. Nguyen, E., Poli, M., Faizi, M., Thomas, A., Wornow, M., Birch-Sykes, C., Massaroli, S.,\nPatel, A., Rabideau, C., Bengio, Y., Ermon, S., R´ e, C., and Baccus, S. HyenaDNA: Long-\nRange Genomic Sequence Modeling at Single Nucleotide Resolution. In: Oh, A., Naumann,\n18\nT., Globerson, A., Saenko, K., Hardt, M., and Levine, S., eds.Advances in Neural Information\nProcessing Systems vol. 36. Curran Associates, Inc. (2023):( 43177–43201).\n36. Shao, B. (2023). A long-context language model for deciphering and generating bacterio-\nphage genomes. bioRxiv preprint. https://www.biorxiv.org/content/10.1101/2023.12.\n18.572218v3.\n37. Ratcliff, J. D. (2024). Transformer model generated bacteriophage genomes are composition-\nally distinct from natural sequences. bioRxiv preprint.https://www.biorxiv.org/content/\n10.1101/2024.03.19.585716v1.\n38. Alipanahi, B., Delong, A., Weirauch, M. T., and Frey, B. J. (2015). Predicting the sequence\nspecificities of DNA-and RNA-binding proteins by deep learning. Nature Biotechnology 33,\n831–838.\n39. Zhou, J., and Troyanskaya, O. G. (2015). Predicting effects of noncoding variants with deep\nlearning–based sequence model. Nature Methods 12, 931–934.\n40. Kelley, D. R., Snoek, J., and Rinn, J. L. (2016). Basset: learning the regulatory code of the\naccessible genome with deep convolutional neural networks. Genome Research 26, 990–999.\n41. Kelley, D. R., Reshef, Y. A., Bileschi, M., Belanger, D., McLean, C. Y., and Snoek, J. (2018).\nSequential regulatory activity prediction across chromosomes with convolutional neural net-\nworks. Genome Research 28, 739–750.\n42. Zeng, T., and Li, Y. I. (2022). Predicting RNA splicing from DNA sequence using Pan-\ngolin. Genome Biology 23, 103. https://genomebiology.biomedcentral.com/articles/\n10.1186/s13059-022-02664-4 . doi:10.1186/s13059-022-02664-4 .\n43. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidi-\nrectional transformers for language understanding. In: Burstein, J., Doran, C., and Solorio,\nT., eds. Proceedings of the 2019 Conference of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers). Minneapolis, Minnesota: Association for Computational Linguistics (2019):(\n4171–4186). https://aclanthology.org/N19-1423. doi:10.18653/v1/N19-1423.\n44. Bommasani, R., Hudson, D. A. et al. (2021). On the opportunities and risks of foundation\nmodels. arXiv preprint arXiv:2108.07258. https://arxiv.org/abs/2108.07258.\n45. West-Roberts, J., Kravitz, J., Jha, N., Cornman, A., and Hwang, Y. (2024). Diverse genomic\nembedding benchmark for functional evaluation across the tree of life. bioRxiv ( 2024–07).\nhttps://www.biorxiv.org/content/10.1101/2024.07.10.602933v1.\n46. de Almeida, B. P., Dalla-Torre, H., Richard, G., Blum, C., Hexemer, L., G´ elard, M., Mendoza-\nRevilla, J., Pandey, P., Laurent, S., Lopez, M. et al. (2024). SegmentNT: annotating the\ngenome at single-nucleotide resolution with DNA foundation models. bioRxiv preprint.\nhttps://www.biorxiv.org/content/10.1101/2024.03.14.584712v2.\n47. Zhou, Z., Wu, W., Ho, H., Wang, J., Shi, L., Davuluri, R. V., Wang, Z., and Liu, H. (2024).\nDNABERT-S: Learning species-aware dna embedding with genome foundation models. arXiv\npreprint. https://arxiv.org/abs/2402.08777.\n19\n48. Zhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R., and Liu, H. (2023). DNABERT-2: Efficient\nfoundation model and benchmark for multi-species genome. arXiv preprint arXiv:2306.15006.\nhttps://arxiv.org/abs/2306.15006.\n49. Garau-Luis, J. J., Bordes, P., Gonzalez, L., Roller, M., de Almeida, B. P., Hexemer, L., Blum,\nC., Laurent, S., Grzegorzewski, J., Lang, M. et al. (2024). Multi-modal transfer learning\nbetween biological foundation models. arXiv preprint arXiv:2406.14150.\n50. Marin, F. I., Teufel, F., Horlacher, M., Madsen, D., Pultz, D., Winther, O., and Boomsma,\nW. BEND: Benchmarking DNA Language Models on Biologically Meaningful Tasks. In:\nInternational Conference on Learning Representations (2024):.\n51. Tang, Z., and Koo, P. K. (2024). Evaluating the representational power of pre-trained DNA\nlanguage models for regulatory genomics. bioRxiv preprint. https://www.biorxiv.org/\ncontent/10.1101/2024.02.29.582810v1.\n52. Li, F.-Z., Amini, A. P., Yue, Y., Yang, K. K., and Lu, A. X. (2024). Feature reuse and scaling:\nUnderstanding transfer learning with protein language models. bioRxiv preprint ( 2024–02).\n53. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham,\nP., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big Bird: Transformers for Longer\nSequences. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., eds.\nAdvances in Neural Information Processing Systems vol. 33. Curran Associates, Inc. (2020):(\n17283–17297).\n54. Ji, Y., Zhou, Z., Liu, H., and Davuluri, R. V. (2021). DNABERT: pre-trained bidirectional en-\ncoder representations from Transformers model for DNA-language in genome. Bioinformatics\n37, 2112–2120.\n55. Mo, S., Fu, X., Hong, C., Chen, Y., Zheng, Y., Tang, X., Lan, Y., Shen, Z., and Xing, E.\nMulti-modal Self-supervised Pre-training for Large-scale Genome Data. In: NeurIPS 2021\nAI for Science Workshop (2021):.\n56. Trotter, M. V., Nguyen, C. Q., Young, S., Woodruff, R. T., and Branson, K. M. (2021).\nEpigenomic language models powered by Cerebras. arXiv preprint arXiv:2112.07571. https:\n//arxiv.org/abs/2112.07571.\n57. Zhang, Y., An, L., Yue, F., and Hardison, R. C. (2016). Jointly characterizing epigenetic\ndynamics across multiple human cell types. Nucleic Acids Research 44, 6721–6731.\n58. Hoarfrost, A., Aptekmann, A., Farfa˜ nuk, G., and Bromberg, Y. (2022). Deep learning of\na bacterial and archaeal universal language of life enables transfer learning and illuminates\nmicrobial dark matter. Nature Communications 13, 2606.\n59. Yang, M., Huang, L., Huang, H., Tang, H., Zhang, N., Yang, H., Wu, J., and Mu, F. (2022).\nIntegrating convolution and self-attention improves language model of human genome for\ninterpreting non-coding regions at base-resolution. Nucleic Acids Research 50, e81–e81.\n60. Gwak, H.-J., and Rho, M. (2022). ViBE: a hierarchical BERT model to identify eukaryotic\nviruses using metagenome sequencing data. Briefings in Bioinformatics 23. doi:10.1093/bib/\nbbac204. Bbac204.\n20\n61. Levy, B., Xu, Z., Zhao, L., Kremling, K., Altman, R., Wong, P., and Tanner, C. FloraBERT:\ncross-species transfer learning withattention-based neural networks for gene expression pre-\ndiction (2022). https://doi.org/10.21203/rs.3.rs-1927200/v1. doi: 10.21203/rs.3.\nrs-1927200/v1.\n62. Bai, Z., Zhang, Y.-z., Miyano, S., Yamaguchi, R., Fujimoto, K., Uematsu, S., and Imoto,\nS. (2022). Identification of bacteriophage genome sequences with representation learning.\nBioinformatics. Btac509.\n63. Zvyagin, M., Brace, A., Hippe, K., Deng, Y., Zhang, B., Bohorquez, C. O., Clyde, A.,\nKale, B., Perez-Rivera, D., Ma, H. et al. (2023). GenSLMs: Genome-scale language models\nreveal SARS-CoV-2 evolutionary dynamics. The International Journal of High Performance\nComputing Applications 37, 683–705.\n64. Chen, K., Zhou, Y., Ding, M., Wang, Y., Ren, Z., and Yang, Y. (2024). Self-supervised\nlearning on millions of primary RNA sequences from 72 vertebrates improves sequence-based\nRNA splicing prediction. Briefings in Bioinformatics 25, bbae163.\n65. Karollus, A., Hingerl, J., Gankin, D., Grosshauser, M., Klemon, K., and Gagneur, J.\n(2024). Species-aware DNA language models capture regulatory elements and their evolu-\ntion. Genome Biology 25, 83.\n66. Fishman, V., Kuratov, Y., Petrov, M., Shmelev, A., Shepelin, D., Chekanov, N., Kardymon,\nO., and Burtsev, M. (2023). GENA-LM: A Family of Open-Source Foundational Models for\nLong DNA Sequences. bioRxiv preprint. https://www.biorxiv.org/content/early/2023/\n06/13/2023.06.12.544594. doi:10.1101/2023.06.12.544594.\n67. Sanabria, M., Hirsch, J., and Poetsch, A. R. (2023). The human genome’s vocabulary as\nproposed by the DNA language model GROVER. bioRxiv preprint. https://www.biorxiv.\norg/content/10.1101/2023.07.19.549677v2.\n68. Zhang, D., Zhang, W., He, B., Zhang, J., Qin, C., and Yao, J. (2023). DNAGPT: A\ngeneralized pretrained tool for multiple DNA sequence analysis tasks. bioRxiv preprint.\nhttps://arxiv.org/abs/2307.05628.\n69. Chu, Y., Yu, D., Li, Y., Huang, K., Shen, Y., Cong, L., Zhang, J., and Wang, M. (2024). A 5’\nUTR language model for decoding untranslated regions of mRNA and function predictions.\nNature Machine Intelligence 6, 449–460.\n70. Lorenz, R., Bernhart, S. H., H¨ oner zu Siederdissen, C., Tafer, H., Flamm, C., Stadler, P. F.,\nand Hofacker, I. L. (2011). ViennaRNA Package 2.0. Algorithms for Molecular Biology 6,\n1–14.\n71. Robson, E. S., and Ioannidis, N. M. (2023). GUANinE v1. 0: Benchmark Datasets for\nGenomic AI Sequence-to-Function Models. bioRxiv preprint. https://www.biorxiv.org/\ncontent/10.1101/2023.10.12.562113v3.\n72. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W.,\nand Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research 21, 1–67. http://jmlr.org/papers/\nv21/20-074.html.\n21\n73. Kudo, T. Subword regularization: Improving neural network translation models with mul-\ntiple subword candidates. In: Gurevych, I., and Miyao, Y., eds. Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa-\npers). Melbourne, Australia: Association for Computational Linguistics (2018):( 66–75).\nhttps://aclanthology.org/P18-1007. doi:10.18653/v1/P18-1007.\n74. Richard, G., de Almeida, B. P., Dalla-Torre, H., Blum, C., Hexemer, L., Pandey, P., Laurent,\nS., Lopez, M. P., Laterre, A., Lang, M. et al. (2024). ChatNT: A Multimodal Conversational\nAgent for DNA, RNA and Protein Tasks. bioRxiv preprint. https://www.biorxiv.org/\ncontent/10.1101/2024.04.30.591835v1.\n75. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S.,\nZhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An Open-Source Chat-\nbot Impressing GPT-4 with 90%* ChatGPT Quality (2023). https://lmsys.org/blog/\n2023-03-30-vicuna/ .\n76. He, Y., Fang, P., Shan, Y., Pan, Y., Wei, Y., Chen, Y., Chen, Y., Liu, Y., Zeng, Z., Zhou, Z.\net al. (2024). LucaOne: Generalized Biological Foundation Model with Unified Nucleic Acid\nand Protein Language. bioRxiv preprint ( 2024–05). https://www.biorxiv.org/content/\n10.1101/2024.05.10.592927v1.\n77. Zhu, X., Qin, C., Wang, F., Yang, F., He, B., Zhao, Y., and Yao, J. (2024). CD-GPT:\nA Biological Foundation Model Bridging the Gap between Molecular Sequences Through\nCentral Dogma. bioRxiv preprint. https://www.biorxiv.org/content/10.1101/2024.06.\n24.600337v1.\n78. Cornman, A., West-Roberts, J., Camargo, A. P., Roux, S., Beracochea, M., Mirdita, M.,\nOvchinnikov, S., and Hwang, Y. (2024). The OMG dataset: An Open MetaGenomic corpus\nfor mixed-modality genomic language modeling. bioRxiv preprint ( 2024–08). https://www.\nbiorxiv.org/content/10.1101/2024.08.14.607850v1.\n79. Markowitz, V. M., Chen, I.-M. A., Palaniappan, K., Chu, K., Szeto, E., Grechkin, Y., Ratner,\nA., Jacob, B., Huang, J., Williams, P. et al. (2012). IMG: the integrated microbial genomes\ndatabase and comparative analysis system. Nucleic Acids Research 40, D115–D122.\n80. Richardson, L., Allen, B., Baldi, G., Beracochea, M., Bileschi, M. L., Burdett, T., Burgin,\nJ., Caballero-P´ erez, J., Cochrane, G., Colwell, L. J. et al. (2023). MGnify: the microbiome\nsequence data analysis resource in 2023. Nucleic Acids Research 51, D753–D759.\n81. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H.,\nThite, A., Nabeshima, N., Presser, S., and Leahy, C. (2020). The Pile: An 800GB Dataset\nof Diverse Text for Language Modeling. arXiv preprint arXiv:2101.00027. https://arxiv.\norg/abs/2101.00027.\n82. Longpre, S., Biderman, S., Albalak, A., Schoelkopf, H., McDuff, D., Kapoor, S., Klyman,\nK., Lo, K., Ilharco, G., San, N. et al. (2024). The responsible foundation model development\ncheatsheet: A review of tools & resources. arXiv preprint arXiv:2406.16746. https://arxiv.\norg/abs/2406.16746.\n83. Sullivan, P. F., Meadows, J. R., Gazal, S., Phan, B. N., Li, X., Genereux, D. P., Dong, M. X.,\nBianchi, M., Andrews, G., Sakthikumar, S. et al. (2023). Leveraging base-pair mammalian\nconstraint to understand genetic variation and human disease. Science 380, eabn2937.\n22\n84. Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N.\nDeduplicating training data makes language models better. In: Muresan, S., Nakov, P., and\nVillavicencio, A., eds. Proceedings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) . Dublin, Ireland: Association for Compu-\ntational Linguistics (2022):( 8424–8445). https://aclanthology.org/2022.acl-long.577.\ndoi:10.18653/v1/2022.acl-long.577.\n85. Schoenfelder, S., and Fraser, P. (2019). Long-range enhancer–promoter contacts in gene ex-\npression control. Nature Reviews Genetics 20, 437–455.\n86. Karnuta, J. M., and Scacheri, P. C. (2018). Enhancers: bridging the gap between gene control\nand human disease. Human Molecular Genetics 27, R219–R227.\n87. King, J. L., and Jukes, T. H. (1969). Non-darwinian evolution. Science 164, 788–798. doi:10.\n1126/science.164.3881.788.\n88. Tay, Y., Dehghani, M., Gupta, J. P., Aribandi, V., Bahri, D., Qin, Z., and Metzler, D.\nAre pretrained convolutions better than pretrained transformers? In: Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Processing (2021):( 4349–4359). https:\n//aclanthology.org/2021.acl-long.335/.\n89. Yang, K. K., Fusi, N., and Lu, A. X. (2024). Convolutions are competitive with transformers\nfor protein sequence pretraining. Cell Systems 15, 286–294.\n90. Linder, J., Srivastava, D., Yuan, H., Agarwal, V., and Kelley, D. R. (2023). Predicting RNA-\nseq coverage from DNA sequence as a unifying model of gene regulation. bioRxiv preprint.\nhttps://www.biorxiv.org/content/10.1101/2023.08.30.555582v1.\n91. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. (2024). Roformer: Enhanced\ntransformer with rotary position embedding. Neurocomputing 568, 127063.\n92. Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V., and Salakhutdinov, R. Transformer-\nXL: Attentive Language Models beyond a Fixed-Length Context. In: Korhonen, A.,\nTraum, D. R., and M` arquez, L., eds. Proceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,\nVolume 1: Long Papers . Association for Computational Linguistics (2019):( 2978–2988).\nhttps://doi.org/10.18653/v1/p19-1285. doi:10.18653/V1/P19-1285.\n93. Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M.\nMEGABYTE: Predicting million-byte sequences with multiscale transformers. In: Oh,\nA., Naumann, T., Globerson, A., Saenko, K., Hardt, M., and Levine, S., eds.\nAdvances in Neural Information Processing Systems vol. 36. Curran Associates, Inc.\n(2023):( 78808–78823). https://proceedings.neurips.cc/paper_files/paper/2023/\nfile/f8f78f8043f35890181a824e53a57134-Paper-Conference.pdf.\n94. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces.\nIn: International Conference on Learning Representations (2022):.\n95. Poli, M., Massaroli, S., Nguyen, E., Fu, D. Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., and\nR´ e, C. Hyena Hierarchy: Towards larger convolutional language models. In: International\nConference on Machine Learning. PMLR (2023):( 28043–28078).\n23\n96. Gu, A., and Dao, T. (2023). Mamba: Linear-time sequence modeling with selective state\nspaces. arXiv preprint arXiv:2312.00752. https://arxiv.org/abs/2312.00752.\n97. Rives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., Guo, D., Ott, M., Zitnick, C. L.,\nMa, J., and Ferguson, A. L. (2021). Biological structure and function emerge from scaling\nunsupervised learning to 250 million protein sequences. Proceedings of the National Academy\nof Sciences 118, e2016239118.\n98. Cheng, X., Chen, B., Li, P., Gong, J., Tang, J., and Song, L. (2024). Training compute-\noptimal protein language models. bioRxiv preprint. https://www.biorxiv.org/content/\n10.1101/2024.06.06.597716v1.\n99. Samuel, D. (2024). BERTs are Generative In-Context Learners. arXiv preprint\narXiv:2406.04823. https://arxiv.org/abs/2406.04823.\n100. Hayes, T., Rao, R., Akin, H., Sofroniew, N. J., Oktay, D., Lin, Z., Verkuil, R., Tran, V. Q.,\nDeaton, J., Wiggert, M. et al. (2024). Simulating 500 million years of evolution with a lan-\nguage model. bioRxiv preprint.https://www.biorxiv.org/content/10.1101/2024.07.01.\n600583v1.\n101. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with\nsubword units. In: Erk, K., and Smith, N. A., eds. Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers) . Berlin, Germany:\nAssociation for Computational Linguistics (2016):( 1715–1725). https://aclanthology.\norg/P16-1162. doi:10.18653/v1/P16-1162.\n102. Blanchette, M., Kent, W. J., Riemer, C., Elnitski, L., Smit, A. F., Roskin, K. M., Baertsch,\nR., Rosenbloom, K., Clawson, H., Green, E. D. et al. (2004). Aligning multiple genomic\nsequences with the threaded blockset aligner. Genome Research 14, 708–715.\n103. Armstrong, J., Hickey, G., Diekhans, M., Fiddes, I. T., Novak, A. M., Deran, A., Fang, Q.,\nXie, D., Feng, S., Stiller, J. et al. (2020). Progressive Cactus is a multiple-genome aligner for\nthe thousand-genome era. Nature 587, 246–251.\n104. Song, B., Buckler, E. S., and Stitzer, M. C. (2024). New whole-genome alignment tools are\nneeded for tapping into plant diversity. Trends in Plant Science 29, 355–369.\n105. Phan, M. H., Zehnder, T. M., Puntieri, F., Lo, B.-W., Lenhard, B., Mueller, F., Vingron,\nM., and Ibrahim, D. M. (2024). Conservation of regulatory elements with highly diverged\nsequences across large evolutionary distances. bioRxiv preprint ( 2024–05). https://www.\nbiorxiv.org/content/10.1101/2024.05.13.590087v1.\n106. Linardatos, P., Papastefanopoulos, V., and Kotsiantis, S. (2020). Explainable AI: A review\nof machine learning interpretability methods. Entropy 23, 18.\n107. Zhang, Y., Tiˇ no, P., Leonardis, A., and Tang, K. (2021). A survey on neural network inter-\npretability. IEEE Transactions on Emerging Topics in Computational Intelligence5, 726–742.\n108. Talukder, A., Barham, C., Li, X., and Hu, H. (2021). Interpretation of deep learning in\ngenomics and epigenomics. Briefings in Bioinformatics 22, bbaa177.\n24\n109. Shrikumar, A., Tian, K., Avsec, ˇZ., Shcherbina, A., Banerjee, A., Sharmin, M., Nair, S., and\nKundaje, A. (2018). Technical note on transcription factor motif discovery from importance\nscores (TF-MoDISco) version 0.5. 6.5. arXiv preprint arXiv:1811.00416. https://arxiv.\norg/abs/1811.00416.\n110. Fowler, D. M., Adams, D. J., Gloyn, A. L., Hahn, W. C., Marks, D. S., Muffley, L. A., Neal,\nJ. T., Roth, F. P., Rubin, A. F., Starita, L. M., and Hurles, M. E. (2023). An Atlas of Variant\nEffects to understand the genome at nucleotide resolution. Genome Biology 24, 147.\n111. Kircher, M., Xiong, C., Martin, B., Schubach, M., Inoue, F., Bell, R. J. A., Costello, J. F.,\nShendure, J., and Ahituv, N. (2019). Saturation mutagenesis of twenty disease-associated\nregulatory elements at single base-pair resolution. Nature Communications 10. doi:10.1038/\ns41467-019-11526-w .\n112. Findlay, G. M., Daza, R. M., Martin, B., Zhang, M. D., Leith, A. P., Gasperini, M., Janizek,\nJ. D., Huang, X., Starita, L. M., and Shendure, J. (2018). Accurate classification of BRCA1\nvariants with saturation genome editing. Nature 562, 217–222.\n113. Notin, P., Kollasch, A. W., Ritter, D., Niekerk, L. V., Paul, S., Spinner, H., Rollins, N. J.,\nShaw, A., Orenbuch, R., Weitzman, R., Frazer, J., Dias, M., Franceschi, D., Gal, Y., and\nMarks, D. S. ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and De-\nsign. In: Thirty-seventh Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track (2023):https://openreview.net/forum?id=URoZHqAohf.\n114. Landrum, M. J., Lee, J. M., Benson, M., Brown, G. R., Chao, C., Chitipiralla, S., Gu, B.,\nHart, J., Hoffman, D., Jang, W. et al. (2016). ClinVar: public archive of interpretations of\nclinically relevant variants. Nucleic Acids Research 44, D862–D868.\n115. Stenson, P. D., Mort, M., Ball, E. V., Evans, K., Hayden, M., Heywood, S., Hussain,\nM., Phillips, A. D., and Cooper, D. N. (2017). The Human Gene Mutation Database:\ntowards a comprehensive repository of inherited mutation data for medical research, ge-\nnetic diagnosis and next-generation sequencing studies. Human Genetics 136, 665–677.\ndoi:10.1007/s00439-017-1779-6 .\n116. Amberger, J. S., Bocchini, C. A., Schiettecatte, F., Scott, A. F., and Hamosh, A. (2015).\nOMIM.org: Online Mendelian Inheritance in Man (OMIM ®), an online catalog of human\ngenes and genetic disorders. Nucleic Acids Research 43, D789–D798.\n117. Pritchard, J. K., and Cox, N. (2002). The allelic architecture of human disease genes: common\ndisease–common variant...or not? Human Molecular Genetics 11, 2417–2423. doi: 10.1093/\nhmg/11.20.2417.\n118. Karczewski, K. J., Francioli, L. C., Tiao, G., Cummings, B. B., Alf¨ oldi, J., Wang, Q., Collins,\nR. L., Laricchia, K. M., Ganna, A., Birnbaum, D. P., Gauthier, L. D., Brand, H., Solomonson,\nM., Watts, N. A., Rhodes, D., Singer-Berk, M., England, E. M., Seaby, E. G., Kosmicki, J. A.,\nWalters, R. K., Tashman, K., Farjoun, Y., Banks, E., Poterba, T., Consortium, G. A. D.,\nand MacArthur, D. G. (2020). The mutational constraint spectrum quantified from variation\nin 141,456 humans. Nature 581, 434–443. doi:10.1038/s41586-020-2308-7 .\n119. Vapnik, V. N. The Nature of Statistical Learning Theory. New York: Springer (1999).\n25\n120. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy,\nA., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. (2015). ImageNet Large Scale\nVisual Recognition Challenge. International Journal of Computer Vision (IJCV) 115, 211–\n252. doi:10.1007/s11263-015-0816-y .\n121. Moult, J., Fidelis, K., Kryshtafovych, A., Schwede, T., and Tramontano, A. (2018). Crit-\nical assessment of methods of protein structure prediction (CASP)—round XII. Proteins:\nStructure, Function, and Bioinformatics 86, 7–15.\n122. Johnson, A. D., Handsaker, R. E., Pulit, S. L., Nizzari, M., O’Donnell, C. J., and de Bakker,\nP. I. (2017). CAGI: The Critical Assessment of Genome Interpretation. Genome Biology 18,\n1–5.\n123. Grimm, D. G., Azencott, C.-A., Aicheler, F., Gieraths, U., MacArthur, D. G., Samocha, K. E.,\nCooper, D. N., Stenson, P. D., Daly, M. J., Smoller, J. W. et al. (2015). The evaluation of\ntools used to predict the impact of missense variants is hindered by two types of circularity.\nHuman Mutation 36, 513–523.\n124. Hartl, D. L., Clark, A. G., and Clark, A. G. Principles of population genetics vol. 116. Sinauer\nassociates Sunderland, MA (1997).\n125. Livesey, B. J., Badonyi, M., Dias, M., Frazer, J., Kumar, S., Lindorff-Larsen, K., McCandlish,\nD. M., Orenbuch, R., Shearer, C. A., Muffley, L. et al. (2024). Guidelines for releasing a variant\neffect predictor. arXiv preprint. https://arxiv.org/abs/2404.10807.\n126. Gupta, A., Lal, A., Gunsalus, L. M., Biancalani, T., and Eraslan, G. (2023). Polygraph:\nA software framework for the systematic assessment of synthetic regulatory DNA elements.\nbioRxiv preprint. https://www.biorxiv.org/content/10.1101/2023.11.27.568764v2.\n127. Consortium, E. P. et al. (2012). An integrated encyclopedia of DNA elements in the human\ngenome. Nature 489, 57–74.\n128. Kundaje, A., Meuleman, W., Ernst, J. et al. (2015). Integrative analysis of 111 reference\nhuman epigenomes. Nature 518, 317–330.\n129. Greˇ sov´ a, K., Martinek, V.,ˇCech´ ak, D.,ˇSimeˇ cek, P., and Alexiou, P. (2023). Genomic bench-\nmarks: a collection of datasets for genomic sequence classification. BMC Genomic Data 24,\nArticle number: 25.\n130. Helfrich, G. (2024). The harms of terminology: why we should reject so-called “frontier AI”.\nAI and Ethics ( 1–7).\n26",
  "topic": "Biology",
  "concepts": [
    {
      "name": "Biology",
      "score": 0.6874586939811707
    },
    {
      "name": "Transformative learning",
      "score": 0.6186301112174988
    },
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.5801111459732056
    },
    {
      "name": "Genomics",
      "score": 0.5233827829360962
    },
    {
      "name": "Genome",
      "score": 0.46960318088531494
    },
    {
      "name": "Computational biology",
      "score": 0.43825194239616394
    },
    {
      "name": "Data science",
      "score": 0.4173043966293335
    },
    {
      "name": "Evolutionary biology",
      "score": 0.3516334295272827
    },
    {
      "name": "Cognitive science",
      "score": 0.3475379943847656
    },
    {
      "name": "Genetics",
      "score": 0.23345214128494263
    },
    {
      "name": "Computer science",
      "score": 0.2241521179676056
    },
    {
      "name": "Gene",
      "score": 0.15566855669021606
    },
    {
      "name": "Sociology",
      "score": 0.13926324248313904
    },
    {
      "name": "Psychology",
      "score": 0.09235996007919312
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}