{
    "title": "Transformer neural network for protein-specific de novo drug generation as a machine translation problem",
    "url": "https://openalex.org/W3119872582",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2569677073",
            "name": "Daria Grechishnikova",
            "affiliations": [
                "Lomonosov Moscow State University"
            ]
        },
        {
            "id": "https://openalex.org/A2569677073",
            "name": "Daria Grechishnikova",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2166550586",
        "https://openalex.org/W324102958",
        "https://openalex.org/W2023818227",
        "https://openalex.org/W2128879247",
        "https://openalex.org/W4240773179",
        "https://openalex.org/W2037825667",
        "https://openalex.org/W2945551948",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2578240541",
        "https://openalex.org/W2765224015",
        "https://openalex.org/W2784270883",
        "https://openalex.org/W2610148085",
        "https://openalex.org/W2773987374",
        "https://openalex.org/W4252520800",
        "https://openalex.org/W2805002767",
        "https://openalex.org/W2793945656",
        "https://openalex.org/W2962764565",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W2963445908",
        "https://openalex.org/W2567534979",
        "https://openalex.org/W2736137960",
        "https://openalex.org/W2971690404",
        "https://openalex.org/W2915175970",
        "https://openalex.org/W2886049025",
        "https://openalex.org/W2914635984",
        "https://openalex.org/W2963609389",
        "https://openalex.org/W2798613236",
        "https://openalex.org/W2891868449",
        "https://openalex.org/W2889326796",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2204695023",
        "https://openalex.org/W6749669830",
        "https://openalex.org/W2028629022",
        "https://openalex.org/W2169678694",
        "https://openalex.org/W2998373981",
        "https://openalex.org/W2087906138",
        "https://openalex.org/W2083932352",
        "https://openalex.org/W2981810096",
        "https://openalex.org/W2135732933",
        "https://openalex.org/W2105649494",
        "https://openalex.org/W2009979602",
        "https://openalex.org/W2034549041",
        "https://openalex.org/W2160592148",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W2530647954",
        "https://openalex.org/W2954460650",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W2746340587",
        "https://openalex.org/W2972608805",
        "https://openalex.org/W3106536628",
        "https://openalex.org/W3104956673",
        "https://openalex.org/W2557283755",
        "https://openalex.org/W3100751385",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W3205082678",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W3098269892"
    ],
    "abstract": "Abstract Drug discovery for a protein target is a very laborious, long and costly process. Machine learning approaches and, in particular, deep generative networks can substantially reduce development time and costs. However, the majority of methods imply prior knowledge of protein binders, their physicochemical characteristics or the three-dimensional structure of the protein. The method proposed in this work generates novel molecules with predicted ability to bind a target protein by relying on its amino acid sequence only. We consider target-specific de novo drug design as a translational problem between the amino acid “language” and simplified molecular input line entry system representation of the molecule. To tackle this problem, we apply Transformer neural network architecture, a state-of-the-art approach in sequence transduction tasks. Transformer is based on a self-attention technique, which allows the capture of long-range dependencies between items in sequence. The model generates realistic diverse compounds with structural novelty. The computed physicochemical properties and common metrics used in drug discovery fall within the plausible drug-like range of values.",
    "full_text": "1\nVol.:(0123456789)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports\nTransformer neural network \nfor protein‑specific de novo \ndrug generation as a machine \ntranslation problem\nDaria Grechishnikova\nDrug discovery for a protein target is a very laborious, long and costly process. Machine learning \napproaches and, in particular, deep generative networks can substantially reduce development \ntime and costs. However, the majority of methods imply prior knowledge of protein binders, their \nphysicochemical characteristics or the three‑dimensional structure of the protein. The method \nproposed in this work generates novel molecules with predicted ability to bind a target protein \nby relying on its amino acid sequence only. We consider target‑specific de novo drug design as a \ntranslational problem between the amino acid “language” and simplified molecular input line entry \nsystem representation of the molecule. To tackle this problem, we apply Transformer neural network \narchitecture, a state‑of‑the‑art approach in sequence transduction tasks. Transformer is based on \na self‑attention technique, which allows the capture of long‑range dependencies between items in \nsequence. The model generates realistic diverse compounds with structural novelty. The computed \nphysicochemical properties and common metrics used in drug discovery fall within the plausible drug‑\nlike range of values.\nDrug development is a multistage process that requires many resources. Bringing a drug to market may take up \nto 20 years1. The total cost may vary from US$0.5 billion to US$2.6  billion2. The estimated amount of drug-like \nmolecule space is  1060, while the number of synthesized compounds is on the order of  1083. Therefore, the search \nfor a promising molecule that may bind to a target protein is a challenging task for chemists. A high-throughput \nscreening technique allows testing of millions of molecules in vitro to determine compounds that may act on the \nprotein of  interest4. However, this method is expensive and time-consuming. Virtual screening is used to search \nlibraries of billions of molecules in  silico5. This method requires information about compounds that are active \nagainst the protein or knowledge of the protein three-dimensional structure and operates on already known \nmolecules, which span only a small part of the synthetically accessible molecule space. In de novo drug design, \none has to create a molecule that is active toward the desired biological target from scratch. Existing compu-\ntational methods often generate molecules that are hard to synthesize or restrict accessible chemical space via \ncoded  rules6. Despite all efforts, targeted generation of molecules remains a challenging task. Recently, machine \nlearning methods were proposed to tackle this  problem7.\nMost of the deep learning models for molecule generation are based on recurrent neural network (RNN). \nRNN is commonly used for modeling sequence data. The main feature of RNN allowing it to work with sequential \ndata is the ability to make use of information from preceding steps. RNN can reveal links between distant ele -\nments of a  sequence8. Unfortunately, RNNs suffer from the problem of vanishing gradients, which significantly \nlimits their ability to work with long sequences. Long short-term memory and gated recurrent units partially \nsolve this  issue9. Recently, several works introduced recurrent neural networks based on the long short-term \nmemory for de novo molecule  generation10–12. They use Simplified Molecular-Input Line-Entry (SMILES) strings \nas input. Fine-tuning on a smaller dataset with compounds known to be active against biological targets force \nthe models to generate focused molecule libraries with the desired activity toward the same target. Several \nresearch groups applied a reinforcement learning approach to bias the generator to produce molecules with \ndesired  properties13–20. In the reinforcement learning paradigm, the agent (generator in de novo drug generation \nproblem) takes some action (choosing the next character during new SMILES string generation) to maximize \nreward (function computed after SMILES string completion). Olivecrona et al. fine-tuned the RNNs to generate \nOPEN\nPhysics Department, Faculty of Physics, Lomonosov Moscow State University, Leninskie Gory, 1 -2, Moscow, \nRussia 119991. email: daria.grechishnikova@gmail.com\n2\nVol:.(1234567890)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\ncompounds binding Dopamine Receptor Type 2 (DRD2). To predict molecule activity, they built a Support Vec-\ntor Machine (SVM) classifier with a Gaussian kernel and trained it on the DRD2 activity  dataset13. The output of \nthis model was used to formulate the reward function. Popova et al. 14 suggested training separately two neural \nnetworks—enerative and predictive—and then using them jointly to generate novel libraries of compounds \nwith the desired properties, e.g., targeted toward Janus kinase 2. Several research groups applied the generative \nadversarial network concept to design compounds with optimized properties, but they did not consider activity \nagainst any biological  target17,18.\nAnother fundamental approach to de novo compound generation is based on autoencoder  architecture21–28. \nAutoencoder consists of encoder and decoder  networks8. The former one converts the input data into a latent \nrepresentation (vector of fixed dimension), and the second one reconstructs the initial object from the latent \ncode. The hidden layer forming the latent representation vector is an informational bottleneck, which induces \nthe model to capture the most important features of the input  object8. Variational and adversarial autoencod-\ners are two types of autoencoders that are widely used to generate molecules. In variational autoencoders, a \nprior distribution, usually normal, is imposed on latent space to make it smooth and suitable for  sampling29. In \nadversarial autoencoders, the discriminator neural network is introduced into architecture to force the distribu-\ntion of latent codes to follow arbitrary prior  distribution30. Gómez-Bombarelli et al. 21 suggested a variational \nautoencoder extended by attaching a multilayer perceptron to the latent layer for property prediction. Joint train-\ning of this enlarged model forces the latent space to organize by property values. On top of this model, authors \ntrained the Gaussian process to predict target compound properties using the latent space representation as \ninput. In a recent  publication22, the authors compared several autoencoder architectures including variational \nand adversarial ones. The adversarial autoencoder provides the highest fraction of valid SMILES strings. The \nauthors trained the SVM classifier to predict activity against DRD2. They used this probability as the objective \nfunction and maximized it during the latent space Bayesian optimization. Additionally, an autoencoder can be \nused for a conditional  generation31–33. In these studies, properties were directly imposed on latent space during \nthe training. Polykovskiy et al. introduced a conditional adversarial autoencoder to design compounds with \nspecified  properties33. After training on a set of Janus kinase 2 (JAK2) and Janus kinase 3 (JAK3) inhibitors and \nconditioning on the selective activity against JAK2, the model generated a compound that turned out to be active \ntoward JAK2 during in vitro tests. Recently, Zhavoronkov et al. developed a discoidin domain receptor 1 (DDR1) \ninhibitor in 21 days using a variational autoencoder fine-tuned with the reinforcement learning  approach25. One \nmolecule successfully passed experiments in mice.\nHowever, all these methods imply prior knowledge of protein binders and their physicochemical character-\nistics. Structure-based drug design approaches require the three-dimensional structure of the protein. In this \nwork, we introduce an approach to targeted drug generation that uses only the protein amino acid sequence as \ninput. We consider the target-specific de novo drug generation problem as a translation from the amino acid \n“language” to SMILES representation of the molecule. Recently, Transformer-based models demonstrated state-\nof-the-art results on neural machine translation  tasks34,35. We adopt Transformer to generate molecules. The \nnetwork takes amino acid sequence as input and generates molecules with predicted ability to bind the protein \ntarget. The model outputs valid structures with plausible values of computed physicochemical characteristics, a \ndrug-likeness metric, and a synthetic accessibility score.\nThe main contributions of our work are as follows:\n1. We formulate the targeted drug generation problem as a translational task and apply the Transformer archi-\ntecture. This application allows molecules generation based on protein amino acid sequence only.\n2. Our approach requires neither prior knowledge of protein binders nor preparation of libraries of ligands \nactive against the target.\n3. The proposed model is based on a self-attention technique that allows better capture of long-range depend-\nencies than recurrent neural networks.\nMethods\nData. We retrieved data from  BindingDB36. BindingDB contains a measured binding affinity of interactions \nbetween proteins and drug-like molecules. The full database version was downloaded. The raw dataset contained \nover 1.5 million data records. We selected records from the raw dataset using the following criteria:\n1. The field “Target Source Organism According to Curator or DataSource” equals “Homo sapiens” or “Rattus \nnorvegicus” or “Mus musculus” or “Bos taurus” .\n2. The record has an IC50 value less than 100 nm; if the IC50 is missing, then Kd is less than 100 nm; if both \nare missing, then EC50 is less than 100 nm.\n3. The record has a chemical identifier (PubChem CID).\n4. The record has SMILES representation.\n5. The molecular weight is less than 1000 Da.\n6. The record has a protein identifier (Uniprot ID).\n7. Protein amino acid sequence length is greater than 80 and lower than 2050.\nThis yielded a results dataset containing 238,147 records. There were 1613 unique amino acid sequences and \n154,924 unique ligand SMILES strings. All SMILES strings used in this work were canonicalized using RDKit. \nWe created five different splits into test and training parts.\n3\nVol.:(0123456789)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nIn each split we required the similarity between proteins in the test and the ones in the training dataset to be \nless than 20%. This additional condition leaded to the removal of some proteins from the dataset, so final sizes \nof the training and test datasets can be found in Table 1.\nIn each split we used a proportion common for machine learning tasks—roughly 90% for train part and 10% \nfor test part. To compute similarities, we used the Needleman-Wunsch global alignment algorithm from the \nEMBOSS  package37. The distribution of pairwise sequence similarities for the first split is shown in Fig.  1. The \ndistributions for other splits are analogous. Figure  1a shows that protein sequences from the test dataset share \nless than 20% similarity with those in the training dataset. At the same time, the protein sequences within the \ntest and training sets are also diverse enough to train and test the model. The majority of sequences share less \nthan 40% similarity (Figs. 1b and 1c).\nData representation. We considered each symbol in an amino acid sequence or in a SMILES string as a \ntoken. The vocabulary was determined by the dataset and contained 71 symbols. Each token was converted into \na vector using trainable embedding in the first layer of the encoder.\nModel. We adopted the Transformer model for targeted drug generation using the original implementation \ndescribed  in35. Transformer has an encoder-decoder structure. The encoder maps a protein amino acid sequence \n(a1,..,an) to a sequence of continuous representations z = (z1,…,zn). Then, the decoder takes z as input and gen-\nerates a SMILES string in autoregressive manner. At every step of generation, the decoder may attend to any \nelements of z due to the attention mechanism. The latent code z may be considered as a “protein context” used \nby the decoder to generate a molecule structure. The model yields a probability distribution over each element \nin the vocabulary for each position in the output sequence. Transformer is based on an attentional mechanism \nonly. It lacks any kind of convolutional or recurrent neural network components. Transformer uses self-attention \nto compute the representations of input and output sequences. Self-attention refers to different components of a \nsingle sequence in relation to other components to compute sequence representation. Each layer of the encoder \nis composed of a multihead self-attention sublayer and feed-forward sublayer. In addition to these, each layer of \nthe decoder has a multihead attention layer attending the encoder output.\nThe self-attention mechanism successfully copes with long-range dependencies while being faster than recur-\nrent layers. The attention layer at first calculates three vectors from each “word” of a “sentence” – key, query and \nvalue. To process all words in a sentence simultaneously, key vectors are packed together into matrix K , and \nqueries and values produce matrices Q  and V correspondingly. In our task definition, “words” are amino acid \nresiduals or characters in SMILES strings. The attention is computed as follows:\n where dk is a scaling factor.\nThe multihead attention mechanism produces h different representations of Q, K, V values and computes an \nattention function for each representation:\nAttention(Q , K , V ) = softmax\n(QK T\n√dk\n)\nV ,\nTable 1.  The number of unique proteins in the training and test datasets for each split.\nDataset 1 Dataset 2 Dataset 3 Dataset 4 Dataset 5\nNumber of the unique proteins in the training dataset 104 112 122 103 124\nNumber of the unique proteins in the test dataset 1042 1000 1004 1002 1036\nFigure 1.  Distributions of sequence similarities between proteins used for model training and validation. (a) \nSequence similarities between proteins in the test dataset and the ones in the training dataset, (b) sequence \nsimilarities of proteins within the test dataset, (c) sequence similarities of proteins within the training dataset.\n4\nVol:.(1234567890)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nThe outputs are concatenated and projected one more time, yielding final values:\n where W Q\ni , W K\ni , W V\ni  are matrices of learned weights.\nSince the model lacks any recurrent component, it has no information about the order of tokens in a sequence. \nTo address this lack of information, the model adds position-dependent signals to the input embedding. There \nare many possible choices for signal functions. In Transformer, sine and cosine functions are used:\nwhere pos is the position, i is the dimension and dmodel is the size of embedding.\nWe use beam search to decode SMILES strings. While constructing a sequence, the beam search evaluates \nall possible next steps and keeps the top n candidates with the highest probability, where n is the user-specified \nparameter referred to as beam size. If beam size is equal to one, the beam search becomes the greedy search. If \nbeam size is greater than one, the output sequences differ only slightly from each other. It might be beneficial \nif a generated molecule is good enough, and small structure optimizations are needed. However, in the process \nof target-specific de novo drug generation, it would be better to have more diverse variants per certain protein. \nThere are several ways to achieve potential improvement. We discuss them in the “Results and Discussion” sec-\ntion. For each protein, we ran beam search with beam sizes of 4 and 10. In the first case, we left only one SMILES \nstring with the highest probability (one per one mode). In the second case, we left all ten generated molecules \nfor subsequent analysis (ten per one mode).\nAll work was performed in Google Colaboratory. We used the open-source tensor2tensor library for building, \ntraining and testing the  model38. We experimented with different numbers of attentional heads, layers, and their \nsizes. The optimal proportion between the amount of valid and unique SMILES strings gives the model contain-\ning four layers of size 128 and four attention heads. We used the Adam optimizer and learning rate decay scheme \nproposed  in35, and the batch size was set to 4096 tokens. We trained the model for 600 K epochs using one GPU.\nTo test the model, we performed Monte-Carlo cross validation. We split all proteins so that test dataset con-\ntains only sequences sharing less than 20% similarity with those in the training dataset. Then, we trained the \nmodel and tested it on selected proteins. This procedure was repeated five times.\nModel evaluation. We used  RDKit39 to check chemical validity, calculate properties, compute similarity \nscores and produce SMILES canonical representation of molecule structures. Molecules known to be active \nagainst given target proteins and the generated ones were docked in binding sites using SMINA with default \n settings40. Protein PDB structures were downloaded from the Protein Data  Bank41. We followed the standard \nprocedure to prepare protein for docking, heteroatoms were removed, and hydrogens were added via  PyMol42. \nWe utilized  OpenBabel43 to generate three-dimensional conformers.\nResults and discussion\nChemical feasibility. This section demonstrates the effectiveness of the proposed approach for the genera-\ntion of valid realistic molecules. We created five different divisions of the initial dataset to train and test parts. \nFor each division, we performed training of the model followed by validation on the corresponding test set. At \nfirst, we ran the model in one per one mode (see “Methods”). For each protein in test datasets, the model gener-\nated a molecule. We checked the chemical validity of molecules with RDKit software, analyzed uniqueness and \nsearched the ZINC15 database for generated compounds. All characteristics were averaged across five test sets. \nApproximately 90% of generated molecules were valid, and 92% were unique (Table 2). Approximately 30% of \ncompounds were found in the ZINC15 database. The entire list of generated SMILES strings can be found as \nSupplementary Table S1 online. We also provide the figures with their graph representations (see Supplementary \nFigure S2 online).\nheadi = Attention\n(\nQW Q\ni , KW K\ni , VW V\ni\n)\nMultihead(Q ,K ,V ) = (head1 ,... ,headh)W O ,\nPE (pos,2i) = sin\n(\npos\n1000\n2i\ndmodel\n)\nPE (pos,2i+1) = cos\n(\npos\n1000\n2i\ndmodel\n)\nTable 2.  Percentages of valid, unique and ZINC15 database-matched SMILES strings generated by the model \nin one per one mode.\nDataset 1 Dataset 2 Dataset 3 Dataset 4 Dataset 5 Average\nTotal number of generated SMILES strings (one per target \nprotein) 104 112 122 103 124 113\nValid (%) 92.3 90.2 87.7 92.2 88.7 90.2\nUnique (%) 93.3 90.2 90.9 94.2 92.7 92.3\nMatch with ZINC15 database (%) 33.6 22.3 27.0 31.0 39.5 30.6\n5\nVol.:(0123456789)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nIn the case of generating one ligand per protein, the outputted compound might be considered as a valid \nstarting point for a subsequent modification during the drug discovery process. Nevertheless, it would be useful \nto obtain more drug candidates for the given target protein. To achieve this aim, we expanded beam size to ten, \nallowing the model to output the ten most likely variants of the compound for inputted protein (ten per one \nmode). In this mode, the model generated almost 83% valid SMILES strings and 82% unique SMILES strings on \naverage across five datasets (Table 3). Over 17% of novel compounds matched the ZINC15 database.\nThe number of valid and unique SMILES strings is lower in ten per one mode. We assume that this is caused \nby the problem of performance degradation in the beam search. A recently proposed method may possibly \nincrease the  performance44. However, this improvement is outside the scope of our work.\nTesting the binding affinity between generated compounds and target proteins. In this sec-\ntion, our goal is to assess whether generated molecules could bind the target protein. At first, we randomly shuf-\nfled the test dataset (split 1), which contains 104 proteins. All proteins share less than 20% sequence similarity \nwith those in the training dataset. Then, we consequently checked each protein and selected the ones satisfying \nthe criteria:\n• Protein is from human\n• More than 100 known binders were selected from BindingDB using criteria from the Data section\n• Protein contains one druggable cavity\nThe last criterion was related to technical limitations. Molecular docking is a very resource-consuming pro-\ncedure. We were able to analyze several PDB structures only for a pair of proteins with one druggable cavity. \nDocking of many ligands into several structures with many binding pockets requires a lot more computational \ntime than we possess. The vast majority of proteins in the test dataset have many binding pockets. To satisfy the \nlast criterion, we had to choose proteins with one well-known binding pocket, which is mostly used as a target \nsite for small molecule drugs. Therefore, we selected two proteins from the receptor tyrosine kinases family. \nThey contain an extracellular ligand-binding region, transmembrane domain, and an intracellular region with \na tyrosine kinase  domain45. Binding of a specific ligand to an extracellular region induces phosphorylation \nprocess, leading to structural transformation within the kinase domain. This results in activation of a corre-\nsponding signal pathway. The vast majority of reported kinase inhibitors binds to the catalytic domain essential \nfor kinase  activity45. The first selected protein is the Insulin-like growth factor 1 receptor (IGF-1R). IGF-1R is a \ntransmembrane receptor with tyrosine kinase activity. It can bind three ligands: insulin and the two insulin-like \ngrowth factors (IGF-I, IGF-II) 46. It is involved in the development of many tissues and plays a key role in the \nregulation of overall growth and metabolism. IGF-1R is known to contribute to the pathophysiology of cancer \nvia involvement in cell transformation, proliferation and metastatic  events47. This involvement makes IGF-1R \na valuable target for drug development. One of the strategies aimed at blocking IGF-1R activity is to use small \nmolecules as IGF-1R tyrosine kinase  inhibitors46.\nThe second protein is Vascular endothelial growth factor receptor 2 (VEGFR2). VEGFR2 is a cell-surface \nreceptor with tyrosine kinase  activity48. Three growth factors bind to VEGFR2: VEGFA, VEGFC, and VEGFD. \nLigand binding initiates a phosphorylation process leading to an enhancement of endothelial cell proliferation \nand migration. VEGFR2 is expressed on vascular endothelial cells and lymphatic endothelial cells. It plays a \ncritical role in physiologic and pathologic angiogenesis, vascular development and permeability and embryonic \nhematopoiesis. It is involved in the development of many diseases including cancer, arthritis, and  diabetes48.\nFor each protein, we composed four sets of ligands—known binders, compounds randomly chosen from \nBindingDB, molecules generated for a selected protein and molecules generated for other targets in the test \ndataset. We collected 1148 known binders from BindingDB for IGF-1R and 3782 compounds for VEGFR2 using \nthe criteria mentioned in the Data section. We could not dock all of them to proteins due to technical limitations. \nTherefore, we randomly selected 100 ligands for docking experiments. The second set contains 100 compounds \nrandomly selected from BindingDB. The third set includes 11 generated molecules (one in one per one mode \nand ten in ten per one mode) for each protein. To test whether generated compounds can bind to the “wrong” \ntarget (cross-docking), we also formed a set of 100 molecules generated for other proteins in the test dataset.\nThe binding scores between ligands and target protein active sites were computed using SMINA. For each \nprotein, we downloaded protein structures bound to ligands from the PDB database. We obtained 11 PDB files \n(2OJ9, 3D94, 3I81, 3NW5, 3NW6, 3NW7, 3O23, 4D2R, 5FXQ, 5FXR, 5FXS) for IGF-1R and 20 (1Y6A, 1Y6B, \n2P2H, 2QU5, 2RL5, 2XIR, 3BE2, 3EWH, 3U6J, 3VHE, 3VHK, 3VID, 3VNT, 3VO3, 4AG8, 4AGC, 4AGD, 4ASD, \nTable 3.  Percentages of valid, unique and ZINC15 database-matched SMILES strings generated by the model \nin ten per one mode.\nDataset 1 Dataset 2 Dataset 3 Dataset 4 Dataset 5 Average\nTotal number of generated SMILES strings (ten per target \nprotein) 1040 1120 1030 1220 1240 1130\nValid 80.8 80.5 79.6 86.2 86.1 82.6\nUnique 88.4 78.4 71.7 85.0 85.0 81.7\nMatch with ZINC15 database (%) 17.9 16.3 10.7 19.5 21.2 17.1\n6\nVol:.(1234567890)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\n4ASE, 6GQP) for VEGFR2. We examined all structures to ensure that they contain binding pockets. Then, we \naligned them via PyMol. All ligands were extracted and combined into separate PDB files. We used them to \ndefine search space for SMINA. The docking requires many computational resources; therefore, we were not \nable to analyze all PDB structures for each protein. Thus, we selected structures that represent discrimination \nability between known binders and randomly selected compounds. We utilized the ROC curve and correspond-\ning area under curve (AUC) of the scores calculated by SMINA to evaluate whether the docking tool could \ndiscriminate between them. We checked six structures for IGF-1R (2OJ9, 3O23, 3I81, 4D2R, 5FXQ, 5FXR) and \nfour structures for VEGFR2 (2P2H, 3BE2, 4ASE, 6GQP). Structures with PDB codes 3I81, 5FXQ, 5FXR, and \n6GQP failed in the discrimination of active and randomly selected compounds (AUC < 0.6). We removed them \nfrom the subsequent analysis.\nWe further assessed whether the docking tool could discriminate between binders and molecules generated \nfor other targets, between generated and randomly selected compounds and between generated and known \nbinders.\nThe structures with PDB codes 3O23 and 3BE2 demonstrate the best discriminative ability between known \nbinders and randomly selected compounds. Figure  2 shows ROC curves and corresponding AUC values for \nseveral combinations of molecule sets for both PDB structures. All AUC values are considerably higher than \nrandom baseline (0.5). These results indicate that the tool more likely classifies compounds generated for the \nIGF-1R and VEGFR2 as binders. At the same time, it less likely classifies compounds generated for other targets \nas binders. Interestingly, for the VEGFR2 the AUC value computed for the group of generated compounds versus \ngroup of randomly selected compounds is very close to the one computed for the set of known binders versus \nrandomly selected molecules. Four other structures (4D2R, 2OJ9 for the IGF-1R target and 4ASE, 2P2H for \nthe VEGFR2 target) present slightly lower discrimination ability; however, AUC values are very close to those \ncomputed for 3O23 and 3BE2 respectively. The ROC curves and their corresponding AUC values can be found \nas Supplementary Figure S3 online. We also build ROC curves to evaluate whether the tool could discriminate \nbetween compounds generated for analyzed structures and known binders (Fig. 3 and Supplementary Figure S4). \nThe AUC values are close to 0.5 in all cases meaning that the tool is unable to distinguish between these groups \nof molecules.\nIt is well known that AUC values are directly connected to the U statistics of the Mann–Whitney test:\nwhere n1 and n2 are sizes of the classes used. We assess significance of the difference between classes in each \npair by p-value of the U statistics and present those values in the Table  4. It is clearly seen, that known binders \nare significantly different from the randomly selected compounds and compounds generated for other targets. \nSignificance of the difference between randomly selected compounds and the ones generated for this target is \nsmaller, but still valuable. In opposite, difference between known binders and compounds generated for this \ntarget is not significant, meaning that the model could not distinguish them.\nThese results suggest that generated molecules for IGF-1R and VEGFR2 can be considered binders, while the \nmolecules generated for other targets are more likely to be nonbinders for both proteins.\nWe also visualized complexes of both proteins and generated ligands using PyMol software. Figure 4 shows \nthe docking poses of generated molecules with the lowest scores in the binding pocket of the corresponding \ntargets. We realize that accurate estimation of the binding ability of generated molecules requires the analysis \nof many diverse proteins using in silico docking and/or in vitro assays. However, these are separate and quite \nAUC = U\nn1 ·n2\nFigure 2.  ROC curves and corresponding AUC for the following structures: (a) structure of IGF-1R with PDB \ncode 3O23, (b) structure of VEGFR2 with PDB code 3BE2.\n7\nVol.:(0123456789)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nFigure 3.  ROC comparison of known binders versus molecules generated for IGF-1R and VEGFR2 and \ncorresponding AUC for the following structures: (a) structure of IGF-1R with PDB code 3O23, (b) structure of \nVEGFR2 with PDB code 3BE2.\nTable 4.  P values of the Mann–Whitney test for all molecule sets used in the analysis.\nPDB code\nP values\nKnown binders versus randomly \nselected compounds\nKnown binders versus compounds \ngenerated for other targets\nCompounds generated for the \nanalized protein versus randomly \nselected ones\nCompounds generated for the analized \nprotein versus known binders\n3O23 3.4*10–10 2.2*10–8 3.0*10–2 0.40\n4D2R 1.5*10–8 1.4*10–6 2.9*10–2 0.33\n2OJ9 5.5*10–7 5.2*10–5 4.5*10–2 0.44\n3BE2 2.5*10–19 8.4*10–8 1.1*10–4 0.26\n4ASE 1.6*10–10 2.3*10–5 4.1*10–3 0.31\n2P2H 2.2*10–12 3.6*10–4 3.1*10–3 0.17\nFigure 4.  Positions of the generated molecules with the lowest scores in the binding sites of the following \nproteins: (a) Insulin-like growth factor 1 receptor, (b) Vascular endothelial growth factor receptor 2.\n8\nVol:.(1234567890)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\ncomplicated tasks, which we consider a direction for future work. We believe that the analysis described here is \nsufficient for the proof of concept.\nPhysicochemical properties and metrics. It is not enough for the model to output chemically valid \nmolecules active against a certain target. The model should also take care of parameters crucial for a molecule \nto be a potential drug. We computed several important metrics and physicochemical properties for generated \ncompounds and compared them with corresponding characteristics of the molecules from the training dataset. \nThe goal was to access the ability of the model to generate compounds satisfying typical drug-likeness metrics. \nAccording to the famous Lipinski’s rule of five, the water-octanol partition coefficient (logP) of a potential orally \nactive drug should not exceed five. Molecules with molecular weight less than 500 show better permeability and \nsolubility. The numbers of hydrogen donors, acceptors and rotatable bonds have to be no more than 5, 10 and \n10,  respectively49,50. Although Lipinski’s rule was developed for oral medications, it gives a good reference point \nfor evaluating the properties of the generated molecules.\nThe Topological Polar Surface Area (TPSA) is another important characteristic of a drug candidate. Chem -\nists assume that molecules having a topological polar surface area greater than 140Å2 are absorbed  poorly50. To \novercome the blood–brain barrier, a molecule should have a TPSA less than 90 Å 251. Quantitative Estimate of \nDrug-likeness (QED) is based on the desirability functions for molecular properties and is widely used to select \nappropriate compounds during the early stages of drug discovery. In other words, QED is the measure of drug-\nlikeness52. It ranges from zero to one, where zero indicates a totally unsuitable molecule, while one corresponds \nto molecules with favorable characteristics. The Synthetic Accessibility Score (SAS) is of great importance, as \nmany computational approaches often yield molecules that tend to be difficult to synthesize (SAS > 6)53. Table 5 \nsummarizes data about the compliance of the generated molecules with the rules mentioned above across five \ndatasets used for Monte-Carlo cross-validation. For each constraint, the majority of generated compounds lie \nin acceptable drug-like molecule boundaries. Figure 5 shows the distributions of logP , the number of H-donors, \nH-acceptors, and rotatable bonds, QED, SAS, TPSA, molecular weight and length for the first test dataset. The \ndistributions for the four remaining test datasets are almost identical. We analyzed computed characteristics of \nmolecules from three datasets: structures generated in one per one mode, ten per one mode and the training set. \nFor each parameter, the histograms display almost complete overlap between datasets. This overlap indicates that \nthe model reproduces the property distribution of molecules in the training set very well.\nThe favorable values of these parameters do not necessarily indicate that the generated compound will become \na drug. It can be checked only in an experiment. Nevertheless, we can conclude that generated molecules may be \nconsidered starting points for developing novel drugs with activity against given protein targets.\nWe assessed the structural diversity between generated molecules and molecules from the training dataset \nby calculating the Tanimoto similarity score implemented in RDKit. Figure  6 shows the distributions of the \nnearest neighbor Tanimoto coefficients over all pairs of these molecules. Only 8% of all generated structures \nhave a Tanimoto score above the similarity threshold (Tanimoto score > 0.85) and can be considered similar to \nstructures from the training dataset. The majority of generated molecules (51%) has a Tanimoto score lower \nthan 0.5, which suggests that this part of the generated compounds differ significantly from those in the training \ndataset. A high Tanimoto score usually indicates small differences in the molecule structure. However, even small \ndifferences in structure may lead to significant changes in functionality. Figure 7 demonstrates the distributions \nof the nearest neighbor Tanimoto similarities over all pairs of ligands in the training dataset. Mean and standard \ndeviation values are shown in Table 6.\nThe mean value of similarities between generated molecules and those in the training set is much lower than \nthe mean value of similarities between compounds in the training dataset. Compared to the input dataset, the \nmodel achieves the generation of more diverse molecules, demonstrating the ability to create novel structures \noutside the training dataset.\nTable 5.  Percentage of generated molecules falling within plausible drug-like molecule ranges of values. \n*There is no common threshold for QED. QED varies in a range [0,1]. The higher a QED value is, the better. \nThe columns show mean values and standard deviations. **Averaged across five cross validational datasets.\nProperty name Constraints\nStructures satisfying the constraints (%)\nGenerated molecules (one per one)** Generated molecules (ten per one)**\nlogP  < 5 84.4 85.6\nMolecular weight (Da)  < 500 95.8 88.9\nNumber of hydrogen donors  < 5 95.8 91.9\nNumber of hydrogen acceptors  < 10 97.9 93.5\nNumber of rotational bonds  < 10 97.9 91.2\nTopological polar surface area (Å2)  < 140 98.0 92.7\nQuantitative Estimate of Drug-likeness \n(QED)* 0.66 ± 0.19 0.58 ± 0.21\nSynthetic accessibility score  < 6 99.9 100.0\n9\nVol.:(0123456789)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nFigure 5.  Distribution of properties for the generated molecules. Properties include: water-octanol partition \ncoefficient (logP), the number of H-donors, the number of H-acceptors, the number of rotatable bonds, \nQuantitative Estimation of Drug-likeness (QED), the synthetic accessibility score (SAS), total polar surface area, \nmolecular weight and length.\nFigure 6.  Tanimoto similarity of generated molecules to the nearest neighbor in the training dataset.\n10\nVol:.(1234567890)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nTransformer applicability to drug generation tasks. Deep learning methods usually need a library of \nmolecules with known activity against a certain protein to generate ligand binding with the target. The specific \nlibrary is used to fine-tune the model or to train a predictive network that assigns a reward to generator output \nin a reinforcement learning approach (e.g., 10,14,22). In several research works, authors used a seed molecule to \ngenerate structures with the desired activity (e.g., 27,28). In other words, these approaches demand some prior \ninformation about compounds that are active against a given target. The method proposed in this work does \nnot imply knowledge of active ligands or any kind of chemical descriptors of the molecule. At the same time, \nthe method does not rely on information about the three-dimensional structure of the protein of interest. Usu-\nally, protein three-dimensional structure determination is not an easy task. Additionally, it may be quite costly. \nTherefore, the usage of an amino acid sequence as input may substantially simplify one of the initial stages of \ndrug discovery—the search for a lead compound—and can be very fruitful in the case of targeting proteins with \nlimited or no information about inhibitors and three-dimensional structure.\nTo the best of our knowledge, this paper is the first attempt to present the de novo drug generation problem \nas a translational task between protein sequence and SMILES representation of the molecule.\nThe method has benefited from the recent progress in the neural machine translation field, where the Trans-\nformer architecture demonstrated state-of-the-art  results34. Recently, Transformer also exhibited very promis-\ning results in predicting the products of chemical reactions and  retrosynthesis54,55. One of the key features of \nTransformer is self-attention layers. They reduce the length of the paths that the signal should travel during deep \nnetwork learning. This reduction allows the model to maintain long-range dependencies in sequence much bet-\nter than in recurrent neural networks. The self-attention in Transformer architecture operates on both the input \namino acid sequence and the already generated part of the SMILES string, giving access to any part of them at \nany time. Intuitively, self-attention is a good choice for translation between protein and molecule. First, a protein \nsequence may be quite long—dozens of times longer than a SMILES string. Second, three-dimensional struc-\ntural features of the protein may be formed by amino acid residues located far from each other in the sequence \nrepresentation. That is why it is so important for the algorithm to reference elements coming long before the \ncurrent one. The multihead self-attention mechanism allows the model to jointly attend to different aspects of \npositions that are important in relation to proceeding elements. In language translation tasks, this ability means \nthat Transformer may capture, for example, both the semantic and grammatical meaning of a particular word. \nIntuitively, it appears that this ability may be helpful in capturing 3D features of a protein binding pocket. For \nexample, a model may consider a certain residue simultaneously in two aspects: forming the pocket and interact-\ning directly with the drug. This is just our assumption and requires additional checking.\nCurrently, the vast majority of deep learning approaches to the drug generation task use the similarity of \norganic chemistry structures and natural human language. Chemists understand molecule structure much like \na human understands words. Segler et al. introduced encoder-decoder RNN architecture for the construction of \na chemical language model, i.e., the probability distribution over a sequence of characters in SMILES  notation10. \nOthers implemented variational and adversarial autoencoders to create a continuous latent representation of \nFigure 7.  Tanimoto similarity of molecules which are nearest neighbors in the training dataset.\nTable 6.  Mean and standard deviation values of Tanimoto similarity distributions.\nDistribution Mean Standard deviation\nTanimoto similarity of the generated molecules to the nearest neighbor in the training dataset (1 per 1 \nmode) 0.54 0.17\nTanimoto similarity of the generated molecules to the nearest neighbor in the training dataset (10 per 1 \nmode) 0.57 0.18\nTanimoto similarity of the molecules which are nearest neighbors in the training dataset 0.74 0.14\n11\nVol.:(0123456789)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nchemical spaces (e.g., 22). This creation allows easy sampling of latent codes and decoding them into SMILES \nstrings corresponding to novel molecules. The reinforcement learning technique and fine-tuning of specific \ndatasets were proposed to bias the probability distribution toward desired properties (e.g., 13). In all of these \napproaches, the source “language” and the target “language” should ideally have the same distribution, and deep \nlearning methods are used to construct the best fitting between them. Unlike previous studies, in our approach, \nwe attempt to tackle the problem where source language and target language have different distributions. This \napproach allows the creation of a molecule with intended binding affinity using minimum information about the \ntarget, i.e., amino acid sequence only. As a proof of concept, we investigated several types of end points: chemical \nfeasibility, physical properties, and predicted biological activity, and achieved promising results for each of them. \nHowever, the method can be improved in several directions. One of them is the generation of more diverse valid \nvariants per protein. The Diverse Beam Search may be beneficial in this respect as it optimizes the objective \ncontaining dissimilarity  term56. However, a more fundamental approach is to couple Transformer with a vari-\national or adversarial autoencoder. These networks can be trained on large datasets of molecule structures to \nproduce a latent continuous representation of chemical space. Joint training Transformer with such an additional \ncomponent will allow usage of benefits from both approaches: sampling from continuous representation and \nconditioning on the target protein. Another improvement is to increase the number of novel structures that are \nnot present in databases. The model learns distribution in chemical space from the training dataset and then uses \nit to generate a SMILES string. Typically, in deep learning, more diverse input in the training phase causes more \ndiverse output during the generation phase. During our experiments, we noticed that the number of structures \nfound in the ZINC15 database is lower for models trained on four organisms than for models trained only on \nhuman. Along the same lines, the Tanimoto scores between generated compounds and those from the training \ndataset are lower on average (0.57 ± 0.18 for generated molecules and 0.74 ± 0.14 for ones in train dataset). We \nanticipate that model pretraining on a much larger set of molecules (~ 1.5 million items from ChEMBL, for \nexample) may substantially reduce the fraction of molecules found in databases. It also may help to increase \nthe diversity of generated molecules from those in the training dataset. However, such improvement requires \ntechnical resources that we do not yet possess. Therefore, this optimization was out of the scope of our work. \nAnother important improvement is an increase in the model interpretability. A visualizable interpretation may \nprovide valuable biological insights and substantially improve understanding of the protein–ligand interaction.\nConclusion\nIn this work, we introduced a deep neural network based on the Transformer architecture for protein-specific \nde novo molecule design. Computational experiments demonstrated the efficiency of the proposed method in \nterms of predicted binding affinity of generated ligands to the target protein, percentages of valid diverse struc-\nture, drug-likeness metrics and synthetic accessibility. Our model is based solely on protein sequence. This basis \nmay be beneficial in the early stage of drug discovery, i.e., during identification of a lead compound for a protein \ntarget. The proposed method may be useful if information about the 3D protein structure is inaccessible due to \ndifficulties in protein expression, purification and crystallization. However, our approach can be extended to \nyield a more interpretable model. We will address this improvement in our future studies.\nData availability\nThe code and data are available at https ://githu b.com/daria grech ishni kova/molec ule_struc ture_gener ation .\nReceived: 2 December 2019; Accepted: 9 December 2020\nReferences\n 1. Paul, S. M. et al. How to improve R&D productivity: the pharmaceutical industry’s grand challenge. Nat. Rev. Drug Discovery 9, \n203–214. https ://doi.org/10.1038/nrd30 78 (2010).\n 2. Avorn, J. The $2.6 billion pill–methodologic and policy considerations. N. Engl. J. Med. 372, 1877–1879. https ://doi.org/10.1056/\nNEJMp 15008 48 (2015).\n 3. Polishchuk, P . G., Madzhidov, T. I. & Varnek, A. Estimation of the size of drug-like chemical space based on GDB-17 data. J. \nComput. Aided Mol. Des. 27, 675–679. https ://doi.org/10.1007/s1082 2-013-9672-4 (2013).\n 4. Inglese, J. et al. Quantitative high-throughput screening: a titration-based approach that efficiently identifies biological activities \nin large chemical libraries. Proc. Nat. Acad. Sci. USA 103, 11473–11478. https ://doi.org/10.1073/pnas.06043 48103  (2006).\n 5. Stumpfe, D. & Bajorath, J. Similarity searching. Wiley Interdiscip. Rev. Comput. Mol. Sci. 1, 260–282. https  ://doi.org/10.1002/\nwcms.23 (2011).\n 6. Hartenfeller, M. et al. A collection of robust organic synthesis reactions for in silico molecule design. J. Chem. Inf. Model. 51, \n3093–3098. https ://doi.org/10.1021/ci200 379p (2011).\n 7. Elton, D. C., Boukouvalas, Z., Fuge, M. D. & Chung, P . W . Deep learning for molecular design—a review of the state of the art. \nMol. Syst. Des. Eng. 4, 828–849. https ://doi.org/10.1039/c9me0 0039a  (2019).\n 8. Goodfellow, I., Bengio, Y . & Courville, A. Deep Learning (MIT Press, Cambridge, 2016).\n 9. Cho, K., van Merrienboer, B., Gulcehre, C., Bougares, F ., Schwenk, H., & Bengio, Y . Learning phrase representations using RNN \nencoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing (EMNLP \n2014).\n 10. Segler, M. H. S., Kogej, T., Tyrchan, C. & Waller, M. P . Generating focused molecule libraries for drug discovery with recurrent \nneural networks. ACS Central Sci. 4, 120–131. https ://doi.org/10.1021/acsce ntsci .7b005 12 (2018).\n 11. Gupta, A. et al. Generative recurrent networks for de novo drug design. Mol. Inf. https ://doi.org/10.1002/minf.20170 0111 (2018).\n 12. Merk, D., Friedrich, L., Grisoni, F . & Schneider, G. . De. novo design of bioactive small molecules by artificial intelligence. Mol. \nInf. https ://doi.org/10.1002/minf.20170 0153 (2018).\n 13. Olivecrona, M., Blaschke, T., Engkvist, O. & Chen, H. Molecular de-novo design through deep reinforcement learning. J. Cheminf. \n9, 48. https ://doi.org/10.1186/s1332 1-017-0235-x (2017).\n12\nVol:.(1234567890)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\n 14. Popova, M., Isayev, O. & Tropsha, A. Deep reinforcement learning for de novo drug design. Sci. Adv. 4, eaap7885. https ://doi.\norg/10.1126/sciad v.aap78 85 (2018).\n 15. Jaques, N., Gu, S., Turner, R. E. & Eck, D. Tuning Recurrent Neural Networks with Reinforcement Learning. Preprint at https ://\narxiv .org/abs/1611.02796  (2016).\n 16. Benjamin, S.-L., Carlos, O., GuimaraesGabriel, L. & Alan, A. Optimizing distributions over molecular space. An objective-rein -\nforced generative adversarial network for inverse-design chemistry (organic). Preprint at ChemRxiv https ://doi.org/10.26434 /\nchemr xiv.53096 68.v3 (2017).\n 17. Putin, E. et al. Reinforced adversarial neural computer for de novo molecular design. J. Chem. Inf. Model. 58, 1194–1204. https ://\ndoi.org/10.1021/acs.jcim.7b006 90 (2018).\n 18. Guimaraes, G. L., Sanchez-Lengeling, B., Outeiral, C., Farias, P . L. C. & Aspuru-Guzik, A. Objective-reinforced generative adver-\nsarial networks (organ) for sequence generation models. Preprint at https ://arxiv .org/abs/1705.10843  (2017).\n 19. Putin, E. et al. Adversarial threshold neural computer for molecular de novo design. Mol. Pharm. 15, 4386–4397 (2018).\n 20. Zhou, Z., Kearnes, S., Li, L., Zare, R. N. & Riley, P . Optimization of molecules via deep reinforcement learning. Sci. Rep. 9, 10752. \nhttps ://doi.org/10.1038/s4159 8-019-47148 -x (2019).\n 21. Gomez-Bombarelli, R. et al. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central \nSci. 4, 268–276. https ://doi.org/10.1021/acsce ntsci .7b005 72 (2018).\n 22. Blaschke, T., Olivecrona, M., Engkvist, O., Bajorath, J. & Chen, H. Application of generative autoencoder in de novo molecular \ndesign. Mol. Inf. https ://doi.org/10.1002/minf.20170 0123 (2018).\n 23. Kadurin, A. et al. The cornucopia of meaningful leads: applying deep adversarial autoencoders for new molecule development in \noncology. Oncotarget 8, 10883–10890. https ://doi.org/10.18632 /oncot arget .14073  (2017).\n 24. Kadurin, A., Nikolenko, S., Khrabrov, K., Aliper, A. & Zhavoronkov, A. druGAN: An advanced generative adversarial autoencoder \nmodel for de novo generation of new molecules with desired molecular properties in silico. Mol. Pharm.  14, 3098–3104. https ://\ndoi.org/10.1021/acs.molph armac eut.7b003 46 (2017).\n 25. Zhavoronkov, A. et al. Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nat. Biotechnol. 37, 1038–1040. \nhttps ://doi.org/10.1038/s4158 7-019-0224-x (2019).\n 26. Sattarov, B. et al. De novo molecular design by combining deep autoencoder recurrent neural networks with generative topographic \nmapping. J. Chem. Inf. Model. 59, 1182–1196. https ://doi.org/10.1021/acs.jcim.8b007 51 (2019).\n 27. Harel, S. & Radinsky, K. Prototype-based compound discovery using deep generative models. Mol. Pharm. 15, 4406–4416. https  \n://doi.org/10.1021/acs.molph armac eut.8b004 74 (2018).\n 28. Skalic, M., Jimenez, J., Sabbadin, D. & De Fabritiis, G. Shape-based generative modeling for de novo drug design. J. Chem. Inf. \nModel. 59, 1205–1214. https ://doi.org/10.1021/acs.jcim.8b007 06 (2019).\n 29. Kingma, D. P . & Welling, M. Auto-encoding variational bayes. in International Conference on Learning Representations (ICLR)  \n(2014).\n 30. Makhzani, A. S., J. & Jaitly, N. & Goodfellow, I. Adversarial autoencoders. (2015).\n 31. Lim, J., Ryu, S., Kim, J. W . & Kim, W . Y . Molecular generative model based on conditional variational autoencoder for de novo \nmolecular design. J. Cheminf. 10, 31. https ://doi.org/10.1186/s1332 1-018-0286-7 (2018).\n 32. Kang, S. & Cho, K. Conditional molecular design with deep generative models. J. Chem. Inf. Model. 59, 43–52. https ://doi.\norg/10.1021/acs.jcim.8b002 63 (2019).\n 33. Polykovskiy, D. et al. Entangled conditional adversarial autoencoder for de novo drug discovery. Mol. Pharm. 15, 4398–4405. https \n://doi.org/10.1021/acs.molph armac eut.8b008 39 (2018).\n 34. Edunov, S., Ott, M., Auli, M. & Grangier, D. Understanding back-translation at scale. Preprint at https ://arxiv .org/abs/1808.09381 \n, 489–500, https ://doi.org/10.18653 /v1/D18-1045 (2018).\n 35. Vaswani, A. et al. Attention Is All Y ou Need. Preprint at https ://arxiv .org/abs/1706.03762  (2017).\n 36. Gilson, M. K. et al. BindingDB in 2015: A public database for medicinal chemistry, computational chemistry and systems phar -\nmacology. Nucleic Acids Res. 44, D1045-1053. https ://doi.org/10.1093/nar/gkv10 72 (2016).\n 37. EMBOSS package. http://embos s.sourc eforg e.net/apps/relea se/6.6/embos s/apps/needl e.html.\n 38. Vaswani, A. et al. Tensor2Tensor for Neural Machine Translation. Proceedings of the 13th Conference of the Association for Machine \nTranslation in the Americas (Volume 1: Research Papers). Preprint at https ://arxiv .org/abs/1803.07416  (2018).\n 39. RDKit: Open-source cheminformatics http://www.rdkit .org.\n 40. Koes, D. R., Baumgartner, M. P . & Camacho, C. J. Lessons learned in empirical scoring with smina from the CSAR 2011 bench-\nmarking exercise. J. Chem. Inf. Model. 53, 1893–1904. https ://doi.org/10.1021/ci300 604z (2013).\n 41. Rcsb Protein Data Bank. https ://www.rcsb.org/.\n 42. PyMOL. https ://pymol .org/2/.\n 43. O’Boyle, N. M. et al. Open Babel: An open chemical toolbox. J. Cheminf. 3, 33. https ://doi.org/10.1186/1758-2946-3-33 (2011).\n 44. Cohen, E. & Beck, C. Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models. in Proceedings of \nthe 36th International Conference on Machine Learning, vol 97, 1290–1299 (2019).\n 45. Metibemu, D. S. et al. Exploring receptor tyrosine kinases-inhibitors in Cancer treatments. Egypt. J. Med. Hum. Genet. https ://doi.\norg/10.1186/s4304 2-019-0035-0 (2019).\n 46. Arcaro, A. Targeting the insulin-like growth factor-1 receptor in human cancer. Frontiers Pharmacol. 4, 30. https ://doi.org/10.3389/\nfphar .2013.00030  (2013).\n 47. Li, R., Pourpak, A. & Morris, S. W . Inhibition of the insulin-like growth factor-1 receptor (IGF1R) tyrosine kinase as a novel cancer \ntherapy approach. J. Med. Chem. 52, 4981–5004. https ://doi.org/10.1021/jm900 2395 (2009).\n 48. Modi, S. J. & Kulkarni, V . M. Vascular endothelial growth factor receptor (VEGFR-2)/KDR inhibitors: medicinal chemistry per-\nspective. Med. Drug Discov. 2, 100009. https ://doi.org/10.1016/j.medid d.2019.10000 9 (2019).\n 49. Lipinski, C. A., Lombardo, F ., Dominy, B. W . & Feeney, P . J. Experimental and computational approaches to estimate solubility and \npermeability in drug discovery and development settings. Adv. Drug Deliv. Rev. 46, 3–26 (2001).\n 50. Veber, D. F . et al. Molecular properties that influence the oral bioavailability of drug candidates. J Med Chem. 45, 2615–2623. https \n://doi.org/10.1021/jm020 017n (2002).\n 51. Hitchcock, S. A. & Pennington, L. D. Structure-brain exposure relationships. J. Med. Chem. 49, 7559–7583. https ://doi.org/10.1021/\njm060 642i (2006).\n 52. Bickerton, G. R., Paolini, G. V ., Besnard, J., Muresan, S. & Hopkins, A. L. Quantifying the chemical beauty of drugs. Nat. Chem. \n4, 90–98. https ://doi.org/10.1038/nchem .1243 (2012).\n 53. Ertl, P . & Schuffenhauer, A. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and \nfragment contributions. J. Cheminf. 1, 8. https ://doi.org/10.1186/1758-2946-1-8 (2009).\n 54. Schwaller, P . et al. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS Central Sci. 5, \n1572–1583. https ://doi.org/10.1021/acsce ntsci .9b005 76 (2019).\n 55. Karpov, P ., Godin, G. & Tetko, I. V . A transformer model for retrosynthesis. Preprint at ChemRxiv https ://doi.org/10.26434 /chemr \nxiv.80584 64.v1 11731, 817–830, https ://doi.org/10.1007/978-3-030-30493 -5_78 (2019).\n 56. Vijayakumar, A. K. et al. Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models. Preprint at https ://\narxiv .org/abs/1610.02424  (2016).\n13\nVol.:(0123456789)Scientific Reports |          (2021) 11:321  | https://doi.org/10.1038/s41598-020-79682-4\nwww.nature.com/scientificreports/\nAcknowledgements\nThe author would like to thank L. Grechishnikov for his expertise, help in code creation, in the visualization of \nthe figures and support. The author thanks Professor V . Tverdislov, M. Poptsova, I. Bannikova, I. Volodina, S. \nPavlischev for fruitful discussions and support.\nAuthor contributions\nD.G. conceived the presented idea, performed the computations, analyzed the results and wrote the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https ://doi.\norg/10.1038/s4159 8-020-79682 -4.\nCorrespondence and requests for materials should be addressed to D.G.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http://creat iveco mmons .org/licen ses/by/4.0/.\n© The Author(s) 2021"
}