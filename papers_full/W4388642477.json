{
    "title": "Transformer-Based Dual-Branch Multiscale Fusion Network for Pan-Sharpening Remote Sensing Images",
    "url": "https://openalex.org/W4388642477",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2141801380",
            "name": "Zixu Li",
            "affiliations": [
                "Shandong Institute of Business and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2115731763",
            "name": "JinJiang Li",
            "affiliations": [
                "Shandong Institute of Business and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2028191569",
            "name": "Lu Ren",
            "affiliations": [
                "Shandong Institute of Business and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098664215",
            "name": "Zheng Chen",
            "affiliations": [
                "Shandong Institute of Business and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2070922051",
        "https://openalex.org/W2592962403",
        "https://openalex.org/W2021048111",
        "https://openalex.org/W2149720806",
        "https://openalex.org/W6775950665",
        "https://openalex.org/W6746735679",
        "https://openalex.org/W1992468426",
        "https://openalex.org/W2035285838",
        "https://openalex.org/W2773041763",
        "https://openalex.org/W2171108951",
        "https://openalex.org/W1247035941",
        "https://openalex.org/W1991460509",
        "https://openalex.org/W6631230401",
        "https://openalex.org/W1987308362",
        "https://openalex.org/W2519236071",
        "https://openalex.org/W2111654093",
        "https://openalex.org/W2171211028",
        "https://openalex.org/W2163677711",
        "https://openalex.org/W2124743705",
        "https://openalex.org/W2020442368",
        "https://openalex.org/W6633088995",
        "https://openalex.org/W3097824737",
        "https://openalex.org/W2112693869",
        "https://openalex.org/W2129953395",
        "https://openalex.org/W2154828806",
        "https://openalex.org/W2764034829",
        "https://openalex.org/W2462592242",
        "https://openalex.org/W2754256031",
        "https://openalex.org/W2154789478",
        "https://openalex.org/W6685078114",
        "https://openalex.org/W2619662254",
        "https://openalex.org/W3166971090",
        "https://openalex.org/W2777033955",
        "https://openalex.org/W3006740421",
        "https://openalex.org/W2963183385",
        "https://openalex.org/W4200634427",
        "https://openalex.org/W3023991509",
        "https://openalex.org/W3176195847",
        "https://openalex.org/W3107490592",
        "https://openalex.org/W2955058313",
        "https://openalex.org/W6730179637",
        "https://openalex.org/W2752782242",
        "https://openalex.org/W2922509574",
        "https://openalex.org/W3034552520",
        "https://openalex.org/W6618372016",
        "https://openalex.org/W4289752563",
        "https://openalex.org/W6753038380",
        "https://openalex.org/W3214821343",
        "https://openalex.org/W3128776197",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4226120860",
        "https://openalex.org/W4385764454",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2963268050",
        "https://openalex.org/W2159269332",
        "https://openalex.org/W2566322263",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W3098542449",
        "https://openalex.org/W2561238782",
        "https://openalex.org/W3016256189",
        "https://openalex.org/W1522046140",
        "https://openalex.org/W4297810817",
        "https://openalex.org/W2782522152",
        "https://openalex.org/W2171845746",
        "https://openalex.org/W1553305639",
        "https://openalex.org/W603908379"
    ],
    "abstract": "Due to the limitations of satellite sensors, we can only obtain MS images and PAN images separately. The focus of our attention is to utilize the pan-sharpening method to generate the high-resolution multispectral (HRMS) images. In this article, we proposed the dual-branch multiscale fusion network, which based on the spatial-spectral transformer to comprehensively capture the information contained in MS images and PAN images at different scales. The architecture of our network consists of three parts: during the feature extraction and image fusion stage, we first independently apply upscaling and downscaling operations to the MS and PAN images. Subsequently, we concatenate the images from the two distinct branches and input them into the shallow feature extraction module individually. And then we input them into our adaptive feature extraction block to further extract the crucial details of the images using the attention mechanism. The images a various scales in different branches are then passed through three spectral transformer and three spatial transformer modules to perform a comprehensive extraction of both spatial and spectral characteristics. Finally, the residual local feature module is utilized during the image reconstruction part to deeply extract intricate information from the images and obtain the final HRMS fused image. We have conducted both simulated and real experiments on the benchmark datasets QB and WV2. The final qualitative and quantitative comparative results demonstrate that our innovative method outperforms the current SOTA methods.",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nTrasformer-based Dual-Branch Multi-Scale Fusion\nNetwork for Pan-sharpening Remote Sensing\nImages\nZixu Li, Jinjiang Li, Lu Ren, and Zheng Chen\nAbstract—Due to the limitations of satellite sensors, we can\nonly obtain MS images and PAN images separately. The focus\nof our attention is to utilize the pan-sharpening method to\ngenerate the high-resolution multispectral (HRMS) images. In\nthis paper, we proposed the dual-branch multi-scale fusion net-\nwork (DMFN), which based on the Spatial-Spectral Transformer\nto comprehensively capture the information contained in MS\nimages and PAN images at different scales. The architecture\nof our network consists of three parts: during the feature\nextraction and image fusion stage, we first independently apply\nupscaling and downscaling operations to the MS and PAN images.\nSubsequently, we concatenate the images from the two distinct\nbranches and input them into the shallow feature extraction\nmodule individually. And then we input them into our Adap-\ntive Feature Extraction Block (AFEB) to further extract the\ncrucial details of the images using the attention mechanism.\nThe images at various scales in different branches are then\npassed through three (Spectral Transformer) SPET and three\n(Spatial Transformer) SPAT modules to perform a comprehensive\nextraction of both spatial and spectral characteristics. Finally, the\nResidual Local Feature Module (RLFM) is utilized during the\nimage reconstruction part to deeply extract intricate information\nfrom the images and obtain the final HRMS fused image. We\nhave conducted both simulated and real experiments on the\nbenchmark datasets QB and WV2. The final qualitative and\nquantitative comparative results demonstrate that our innovative\nmethod outperforms the current SOTA methods.\nIndex Terms—Pansharpening, transformer, attention mecha-\nnism, convolutional neural network.\nI. I NTRODUCTION\nR\nEMOTE sensing image technology has received\nwidespread attention since its inception, the application\nscope includes target detection, image segmentation, image\nfusion, etc, affecting all aspects of human production activities.\n[1], [2] And due to technical defects, We are unable to acquire\nthe multispectral images at high resolution on the existing\nremote sensing sensors simultaneously, how to make good use\nof the advantages of the respective sensors to obtain the results\nwe want to become the focus of our attention. [3] Through the\npan-sharpening method, We have the capability to combine\nthe MS images captured by the sensors with PAN images to\ngenerate the targeted HRMS images [4]. As a foundational\nand continually evolving area of research , the pan-sharpening\nmethod has been developed for nearly 40 years, which extracts\nZ. Li is with School of Information and electronic engineering, Shandong\nTechnology and Business University, Yantai 264005, China\nJ. Li, L. Ren(Corresponding author) and Z. Chen are with School of\nComputer Science and Technology, Shandong Technology and Business\nUniversity, Yantai 264005, China. Email: renlu@sdtbu.edu.cn\nFig. 1. Fusion result image of pan-sharpening methods\nand fuses the rich multispectral distributions contained in MS\nimages and the rich spatial structural information contained\nin PAN images to obtain the HRMS images we need [5],[6].\nIt has also been proved through theory and experiment that\nthe method is widely used in computer vision applications,\nsuch as coastal zone monitoring [7], land change detection\n[8], anomaly detection [9], etc. The pan-sharpening method\nconsists of three principal components described below.\nFounded on the CS transformation, which is often referred\nto as the spectral modification method, the initial MS image\nis spectrally transformed and the panchromatic image is de-\ncomposed into multiple bands. The fusion process involves\nsubstituting the abundant spatial information within the PAN\nimage with the spatial details from the MS image, ultimately\nyielding high-resolution multispectral images. Owing to the\nuncomplicated nature and efficiency of this approach, many\nwell-known and efficient algorithms have been derived, such as\nGram-Schmidt (GS) [10], principal component analysis (PCA)\n[11], Wavelet Transform-Based [12], intensity-hue-saturation\n(IHS) [13] methods. In addition, the nonlinear PCA [14], the\nnonlinear IHS [15] methods are employed to discern finer\ndetails within the images, the various types of feature com-\nponents of the image are also more easily identified. Despite\nthe ease of implementation and the substantial improvement\nin spatial information acquisition achieved by methods based\non the CS transform, they ultimately lead to the spectral\ninformation loss and distortion.\nThe spatial layout of high-resolution PAN images is decon-\nstructed in MRA-based pan-sharpening methods, employing\nwavelet transformation or the generation of a Laplace pyramid,\nand Embeds the separated spatial information into the LRMS\nimages, resulting in images endowed with abundant spatial\ndata and an evenly distributed spectral profile. The MRA-\nbased pan-sharpening methods are also simple to implement.\nThe traditional MRA algorithms includes three steps: multi-\nresolution image decomposition, image fusion and recon-\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nstruction. It is now more widely understood that a unified\nframework should be proposed to avoid the complex three-step\nprocessing. The improved adaptive intensity-hue-saturation\n(IAIHS) [16],effectively enhances the spatial-spectral informa-\ntion present in the fused images. This enhancement is made\npossible through the creation of a correlation weight matrix\nthat establishes a relationship between the spatial structure in\nthe PAN images and the spectral attributes of the MS images.\nOther methods include generalized Laplacian pyramid (GLP)-\nbased methods [17],[18],[19], additive wavelet luminance pro-\nportional (AWLP) [17],[18] methods, curvelet transform [20]\nmethod, high-pass filtering (HPF) [21] and other methods.\nThe VO-based pan-sharpening methods is formed on the\nbasis of variational theory[22], the main content is to find\nthe best fusion transformation function that satisfies specific\nconstraints. The P+XS[23] method aroused a strong reaction\nonce it was put forward, which made the VO-based pan-\nsharpening methods gain more attention and development.\nSparse representations(SR)[24] exploit the connection between\nLRMS images and PAN images to construct dictionary el-\nements and finally inject high-resolution spatial information\ninto MS images. By combining Sparse representation and\nwavelet transform, The fine-grained and coarse-grained infor-\nmation of the image is improved in stages, which is more\nefficient than the traditional sparse representations[25].\nOver the last few years, due to the continuous growth and\nadvancement in science and technology, based on deep learn-\ning (DL) pan-sharpening methods have gradually achieved\ngreat success in the direction of remote sensing image process-\ning. In contrast to the aforementioned traditional techniques,\nthe basic principle of the methods is to learn and construct\nthe network parameters between the observation samples and\nthe fused images.Additionally, the network’s structure can be\nfurther enhanced by persistently optimizing the loss function\nbetween samples and the fused images, which is particularly\nefficient when dealing with large-scale datasets. However,\nthere are exists some problems when training the network,\nhow to construct a better and more efficient network structure\nwith more reasonable network parameters, these two factors\ndetermine the performance of the network. In this paper, we\npropose a two-branch multi-scale fusion network structure\nbased on the Transformer module for pan-sharpening, we\ndemonstrate the performance of this method, and carry out\nrelevant qualitative and quantitative analyses and comparative\nexperiments on two benchmark datasets, this paper chiefly\ncontributes to the field are:\n1. We propose the Spectral Transfomer to enhance the\nextraction of spectral information from images. This is ac-\ncomplished by performing self-attention computations in the\nspectral domain while implementing the multi-head mecha-\nnism in the spatial domain.\n2. To address the limitations of the transformer module\nin capturing fine-grained details, we have utilized multi-\nscale band/patch embeddings for extracting multiscale spec-\ntral/spatial information from images. Leading to a significant\nimprovement in the overall network’s performance.\n3. We introduce the AFEB module into the feature ex-\ntraction process so that the network model focuses on the\nimportant information within the images while disregarding\nthe irrelevant information for the current task.\nThe rest of the article covers: Section 2 mainly describes\nthe background of the DL-based pan-sharpening methods, the\nattention mechanism, and the Transformer modules. Section\n3 provides a more detailed and specific analysis of the\nnetwork structure. Section 4 provides an extensive overview\nof the experimental component of the paper, showcasing the\neffectiveness of our proposed method. This is substantiated\nthrough qualitative and quantitative comparisons of fusion\nresults against nine current SOTA techniques on the QB and\nthe WV2 datasets. Section 5 summarises the whole paper and\ndraws the conclusion from the experiments.\nII. R ELATED WORK\nA. Deep learning-based pansharpening\nWith the explosion of deep learning in recent years, the po-\ntential for its application in the field of remote sensing imagery\nis also steadily advancing. Zhu et al. [26] established a network\nstructure with multiple layers and formed the foundation of the\ndeep network structure. Masi et al. proposed a pan-sharpening\nneural network (PNN) [27] had three network layers and\nefficiently extracted image features but struggled with deeper\nfeature information. Scarpa et al. [28] proposed a target-\nadaptive pan-sharpening method using residual networks to\ndeal with the discrepancy between the dataset and various\nsensors during training. The deep learning pan-sharpening\nmethod assumes a complex nonlinear relationship between\nobservation samples and fused images. [29] The DNN method\nlearns the parameters between these samples and fused images.\nIn conducting the experiments, in the absence of the actual\nHRMS images, we acquired the training samples following\nthe Wald [30] protocol. Wei et al. [31] integrated a residual\nnetwork into the network structure, enabling the network to\nextract deeper-level feature information from images through\nnonlinear transformations. Xu et al. [32] constructed a pan-\nsharpening method based on edge information-guided pan-\nsharpening using sparse coding matrices, which further ex-\npanded the pan-sharpening content. Yang et al. [33] introduced\nthe PanNet network, which combines residual networks with\nan up-sampled MS image to bolster the network’s proficiency\nin feature extraction from images. This strategy facilitates\nthe extraction of spatial details while retains the spectral\ndistribution. Based on the CoF fusion algorithm, Tan et al.\n[34] proposed the CoF-MSMG-PCNN method, which the PAN\nimage is decomposed into three scales, and then fused with\nthe HIS-converted MS image and reconstructed to attain the\neventual fused image. The multiscale image feature extraction\nmodule (MSDCNN) proposed by [35] Yuan et al. employed\nmultiscale convolutional operations to capture diverse image\nfeatures, enhancing texture details within the image by inte-\ngrating a residual network. Zhange et al. [36] introduced the\nTDNet, which includes three structures: bidilevel, bidibranch,\nand bidirectional. This network effectively captures the full\nrichness of spatial architecture and subsequently integrates this\ninformation into the MS image during fusion stage. Ma et al.\n[37] extended the pan-sharpening research by proposing an\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nunsupervised adversarial network-based fusion method, which\ndoesn’t require observation samples in the entire training\nprocess and retains spatial-spectral information through the\nconstructed spatial-spectral generator. On this basis, Zhou et\nal. [38] proposed a PGMAN framework for pan-sharpening,\nwhich further extends its application in GAN networks. Uezato\net al. [39] proposed a guided deep decoder network as a\ngeneral prior, which performed well in various image fusion\ntasks.\nB. Attention Mechanism\nAttention mechanisms have made rapid progress in deep\nlearning applications due to their powerful feature extraction\ncapabilities, and they have achieved significant success in\nvarious upstream and downstream tasks. Such as semantic\nsegmentation [40], natural language processing [41], and im-\nage processing [42]. Attention mechanisms enable models to\nprioritize the most relevant information for the current task and\ndisregard less important data. This enhances computational\nefficiency and reduces parameter complexity. Currently, spatial\nattention, channel attention, and hybrid attention mechanisms\nare commonly used. Hu et al. [43] presented the innova-\ntive ”squeeze-and-extraction network (SENet)” network. This\nstructure incorporates a global compression part for feature\nmaps within individual channels. The learned weights are then\nemployed to extract and activate features contained in feature\nmaps.Finally, these learned weights are applied to the original\nfeature maps, either amplifying or suppressing specific channel\nfeatures. Li et al.[44]proposed Selective Kernel Networks\n(SKNet), in which the attention mechanisms are primarily uti-\nlized to adaptively select convolutional layers with various ker-\nnel sizes. This process aids in determining feature weights at\ndifferent scales, allowing the network to selectively fuse multi-\nscale information. Wang et al.[45] proposed. the Efficient\nChannel Attention (ECA), which prioritizes the significance\nof individual channels. Furthermore, the ECA module avoids\ninter-channel interactions, thereby enhancing the network’s\nperformance under limited computational resources. [46] The\nspatial information network(SPANet) additionally introduces\nan additional branch for handling spatial information and\nintegrates the extracted spatial information into the primary\nfeature extraction branch, consequently enhancing the model’s\neffectiveness in visual task. [47] By combining channel atten-\ntion and spatial attention mechanisms, the CBAM framework\nintelligently prioritizes different channels and spatial domains,\nthereby enhancing its capability in feature extraction. [48] The\nBam module dynamically adjusts feature channel importance\nusing learned weight ratios to enhance overall network perfor-\nmance.\nC. Transformer module\nIn the field of image processing, while CNNs primarily\nfocus on local feature extraction, the transformer module\nenables the model to capture feature dependencies over long\ndistances and simultaneously process inputs from all locations.\nTransformer network have already proved their advantages in\nprocessing sequential data. [49] The Spectralformer network\ngenerates grouped spectral embeddings by extracting local se-\nquence information from neighboring bands in a multispectral\nimage. It also incorporates a cross-layer connection to adap-\ntively learn cross-layer fusion information. Many researchers\nhave also tried to integrate the benefits of CNN and the\nTransformer model to design a more efficient network struc-\nture, He et al. [50] presented the spatial-spectral Transformer,\nwhich employs a tailored CNN framework to extract spatial\nfeatures and incorporates an adjusted Transformer module to\ncapture the spectral characteristics present in the image. [51]\nThe Swin transformer leveraged the strengths of both CNN\nand Transformer by performing self-attention computations\nwithin fixed-size windows. It also utilized sliding window\noperations, including non-overlapping and overlapping cross-\nwindows, progressively increases the model’s perception of the\nimage at each layer. Zhang et al. [52] proposed a multiscale\nspatial-spectral interactive attention mechanism that combines\nglobally and locally extracted features from LRMS and PAN\nimages. This effectively enhances information complementar-\nity while reducing redundant features. Li et al. [53] treats the\ndegradation process of HRMS images as a unified variational\nproblem and introduces a local-global transformer for the prior\nimage denoising module, the final fused image is achieved\nafter a series of iterative approximations.\nThe algorithmic inference process of our overall framework,\nas shown in Algorithm 1.\nAlgorithm 1 The implementation process of our DMFN model\nInput: α, β( MS , P AN)\nOutput: FOut( the final pansharpening image)\n// step1 : T he connection of images\n∗Shallow feature extraction\nF1 = Concat (α, βdown) , F2 = Concat (αup, β)\nend\n// step2 : T he feature extraction of the network\n∗Shallow F eature fusion\nγ = SFE (F1) , η= SFE (F2)\nγ\n′\n= AF EB(γ), η\n′\n= AF EB(η)\n∗Deep F eature extraction\nFor i in 3 do\nσ = SP ET\n\u0010\nγ\n′\n\u0011\nυ = SP AT\n\u0010\nη\n′\n\u0011\nend\nend\n// step3 : T he feature fusion and the image\nreconstruction of the network\nσup = Upsample(σ)\nFδ = Concat (σup, υ)\nFor i in 3 do\nFR = RDLF B\n\u0000\nFδ\u0001\nend\nFOut = ESA\n\u0000\nFR\u0001\nend\n// step4 : Calculate the loss to optimize training process\nlloss = MSE (x, y) + µLap(x, y)\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nFig. 2. Overall framework of DMFN network for remote sensing image fusion.\nIII. M ODEL CONSTRUCTION\nThe following section will meticulously explain the im-\nplementation algorithms and the fundamental architecture of\nthe Pan-sharpening method as shown in Fig. 2. In the first\nsubsection, we analyze the overall design idea and structure\nof the DMFN network. In the second subsection, we explain\nseveral important modules introduced in the paper.And in the\nthird subsection, we summarize the relevant loss functions set\nin the paper.\nA. DMFN framework\nFig. 2 illustrates the primary structure of our DMFN\nmethod, which is divided into three critical components:\nfeature extraction, image fusion and image construction.\nFeature extraction and image fusion part: we firstly use bi-\nlinear interpolation for the LRMS image α ∈ Rh×ω×B(h,ω,B\nsymbolize the height, width and spectral number of the LRMS\nimage respectively) and the PAN image β ∈ RH×W ,which\nexecute up-sampling and down-sampling operations to obtain\nαup ∈ RH×W×B and αup ∈ RH×W×B .It can be articulated\nusing the subsequent equation.\nαup = Up(α), βdown = Down(β) (1)\nwhere the Up(·) and Down(·) operations denote the up-\nsampling and down-sampling operations of bilinear interpola-\ntion. Then we connect α and βdown to get αcat ∈ Rh×ω×(B+1)\n, and connect β and αup to get βcat ∈ RH×W×(B+1) ,which\ncan be expressed as follows:\nαcat = Concat (α, βdown) , βcat = Concat (β, αup) (2)\nThe Concat(·) operation represents the connection between\nthe channel dimensions, the connection between the trans-\nmembrane states can realize the cross-dimensional information\ninteraction between the two branches.\nSince the convolution operation can easily and efficiently\nmap the image from low to high dimensions, in the shallow\nfeature extraction module, we adopt multi-scale convolution\nto capture the image features at different levels to enhance\nthe model’s ability to generalize effectively while gradually\nexpanding the sensory field. The shallow feature extraction\nmodule consists of 2-D convolution operations with convolu-\ntion kernels of 3, 5, and 7 respectively, the output channels are\n16, 16, and 32 and step sizes of 1, 2, and 3. The outputs Fs\nand F\n′\ns can be formulated using the equation provided below:\nFs = SFE (αcat) , F\n′\ns = SF E(βcat) (3)\nwhere SFE (·) denotes the shallow feature extraction mod-\nule, and then we input the obtained feature images into the\nAdaptive Feature Extraction Block (AFEB) on each branch\nrespectively for further processing to obtain Fα ∈ Rh×ω×C\nand F\n′\nα ∈ RH×W×C, which can be symbolized by the\nequation that follows:\nFα = AF EB(FS) , F\n′\nα = AF EB\n\u0010\nF\n′\nS\n\u0011\n(4)\nThe output feature images are then fed into our introduced\nL=1,2,3 Spectral Transformer (SPET) and Spatial Transformer\n(SPAT) for deep feature extraction. Before Fα is input to\nSPET, the spectral embedding is set to B0\nL ∈ RC×Dspe for\nthe SPET feature extraction module in the Lth layer, where\nC = 16 × 2L−1 is the number of channels, and the Dspe\nis set to 32. Similarly before F\n′\nα is input to SPAT, we set\nthe patch embedding for its SPAT module in the Lth layer to\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nP0\nL ∈ RN×Dspa , and Dspa is set to 256. Finally, we input to\neach SPET and SPAT module to obtain the intermediate values\nBi\nL (i = 1, ...,5) and Pj\nL (j = 1, ...,5) ,which are expressed as\nthe following equations:\nBi\nL = SP ETi\n\u0000\nBi−1\nL\n\u0001\n, i= 1 . . .5 (5)\nPj\nL = SP ATj\n\u0010\nPj−1\nL\n\u0011\n, j= 1 . . .5 (6)\nSP ETi(·) and SP ATj(·) denote the SPET and SPAT modules\nin layer i and layer j. The learned weights are used to aggregate\nthe extracted multiscale features to obtain Fspe\nsum ∈ Rh×ω×C\nand Fspa\nsum ∈ RH×W×C.\nShallow feature extraction can extract the shallow informa-\ntion from the image through a straightforward and efficient\nprocess, while SPET and SPAT modules are more focused on\nextracting the intricate feature characteristics within the image,\nso in this paper, we adopt the method of combining the shallow\nfeature extraction module and the deep feature extraction\nmodule (includes SPAT and SPET) with the skip connection\noperation to ensure that the network model obtains the shallow\nfeatures while maintaining the deep feature information, which\ncan be illustrated through the equation that follows:\nFspe = Fspe\nsum + Fα, Fspa = Fspa\nsum + F\n′\nα (7)\nIn order to ensure that the image sizes on the two branches\nare consistent before image reconstruction, we use a sub-\npixel convolutional layer to execute an up-sampling procedure\non the final feature image Fspe to obtain Fspe\nup ,nd a spatial-\nspectral feature map F ∈ RH×W×2C is obtained by connect-\ning Fspe\nup and Fspa.Subsequently, utilize it as input for image\nreconstruction to generate the HRMS image.\nThe image reconstruction part: The images after the link-\ning operation are inputted into our proposed Residual Local\nFeature Module (RLFM) to further extract and preserve the\nimage’s local structural characteristics. The Residual Dense\nLocal Feature Block (RDLFB) discards multiple feature dis-\ntillation connections and simply adopts six stacked convolution\noperations and ReLU activation function for local feature\nextraction, which greatly reduces the computation time while\nmaintaining the capacity of the model. After three RDLFB in\nsequence, it is then passed into the Enhanced Spatial Attention\n(ESA) module for further processing, ultimately resulting in\nthe HRMS image.\nB. Adaptive Feature Extraction Block\nAs depicted in Fig. 3, aim to narrow the perceptual gap\nbetween the estimated spatial-spectral details and the authentic\nimages, we propose the AFEB module in this paper. Firstly, the\nfeature information is extracted from different local regions of\nthe image using a convolutional kernel K × K\n\u0000\nCk\u0001\n, and the\nparameters are adaptively adjusted according to the learning\nweights Ωα.In order to improve the learning efficiency, we\nset K to 3 here. The learning weight Ωα is obtained by\nconstructing the Spectral-Spatial Attention(SSA) mechanism\nto combine the spatial-channel attention, the specific structure\nof SSA is shown as follows.\nThe purpose of SSA is to acquire the weight matrix Ωα\nthat matches the size of convolution kernel K ×K. Firstly,\nthe input feature map F is proceeded through a 3 × 3 × K2\nconvolution layer, which fuses the channel information while\nreducing the dimension of the feature map. Then the spectral\nand spatial weights are obtained by using spectral and spatial\nattention respectively. Spectral weights correspond to the spec-\ntral attributes of each spectrum, while spatial weights signify\nthe spatial characteristics within each spectral channel. Each\nspectrum of the image has unique spatial-spectral information,\nWith the aim of enhancing the harmonizationbetween spatial\nand spectral information, we sequentially carry out the inner\nproduct of the two weights, we obtain the interleaved attention\nweight matrix Ωα after perfume the reshape operation. The\nwhole process can be mathematically defined by the following\nequation :\nF\n′\n= Conv(F)\nOτ\nΨ = Sigmoid\n\u0010\nConv1D\n\u0010\nGMP\n\u0010\nF\n′ \u0011\u0011\u0011\nOπ\nΨ = Sigmoid\n\u0010\nConv\n\u0010\nCat\n\u0010\nMP\n\u0010\nF\n′ \u0011\n, AP\n\u0010\nF\n′ \u0011\u0011\u0011\u0011\nΩα = Re (O τ\nΨ ⊗ Oπ\nΨ)\n(8)\nwhere F denotes the local feature map input to the\nAFEB,Oτ\nΨ,Oπ\nΨ denotes the spectral and spatial attention\nrespectively,Conv1D(·) denotes the 1-D convolution, GMP (·)\nrepresents the application of global max-pooling, MP (·)\nand AP(·) denote the max-pooling and average-pooling\noperations,⊗ denotes the inner product operation, Re(·) de-\nnotes the reshape operation and ⊙ denotes the convolution\noperation. The adaptive local convolution kernel are obtained\nby weighting Ωα and Ck with the corresponding weight values\nof each local. The convolution computation on the feature map\nis executed using the adaptive local convolution kernel we’ve\nobtained to get the features under different local regions, and\nthe final output Oa is presented as follows:\nOa = Ωα ⊗ Ck ⊙ F (9)\nC. Spectral Multi-head Self-attention\nIn SPET, the spectral self-attention mechanism is cleverly\ndesigned to obtain the correlation between spectra by calculat-\ning the self-attention on the spectral dimensions, as shown in\nFig. 4, we first obtain the query matrix Q ∈ khw×DSpe ,the key\nmatrix K ∈ khw×DSpe and the value matrix V ∈ khw×DSpe by\nthe the trainable linear transformation computation as shown\nbelow:\nQ = Bi\nl WQ, K= Bi\nl WK, V= Bi\nl WV (10)\nWhere WQ,WK and WV are learnable mapping matrices, and\nthen a scalar dot product function is applied to the query, key,\nand value, defined as follows:\nAttention(Q, K, V) = V ·\n \nsoftmax\n \nKT Qp\nDspe\n!!\n(11)\nThe Attention(·)represents the scalar dot product compu-\ntation function, where the multi-head attention mechanism is\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nFig. 3. The architecture of Adaptive Feature Extraction Block(AFEB).\nFig. 4. The architecture diagram of the proposed spectral Transformer.\nused as in VIT to maximize the ability to extract features.\nHowever, unlike the VIT network, we assign Q, K, and V to\nM2 heads in the spatial domain and set M to 2 for all datasets.\nThe spectral multi-head self-attention (SpeMSA) is defined as\nfollows:\nHeadn = Attention (Qn, Kn, Vn) , N= 1 ··· M2\nSpeMSA (Qn, Kn, Vn) = ConcateM2\nn=1 (Headn) W\n(12)\nQn,Kn and Vn denote the query matrix, key matrix, and\nvalue matrix obtained by training linear transformation at\nthe nth. SpeMSA (·) denotes the spectral multi-head self-\nattention computation function, Headn denotes the nth head,\nand W ∈ kDspe×Dspe denotes the transformation matrix\nobtained by learning the parameters.\nD. Multiscale Embeddings\n1)Multiscale Band Embedding\nFor more accurate spectral information extraction from the\nimage spectrum, we propose the SPET module. We introduce\nthe multi-scale spectral embedding for multi-scale spectral\nfeature extraction. First, the convolution process is carried out\nusing a kernel of dimension 3×3 to the different scales images\nwith the output channels of C. Then we input these feature\nmaps into the respective SPET to capture multiscale spectral\ndetails. We aggregate the extracted multiscale spectral features\nand use the skip connection operation to acquire the eventual\noutput FSum\nspe , which can be expressed using the following\nmathematical expression:\nFSum\nspe = Add(ΣL=3\nl=1 SP ET, Oa) (13)\nThe Add(·) represents the element-wise summation opera-\ntion.\n2)Multiscale Patch Embedding\nWe use the SPAT module to capture the spatial char-\nacteristics embedded in the image, The SPAT structure is\nsimilar to the VIT structure, the image is cut into fixed-\nsize patches and embedded linearly into the sequence for\nprocessing. The embedding of patches at different scales\nobtain feature information at different fine level, so we use\nmulti-scale patches to further enrich the spatial details of\nthe image. As depicted in the bottom of Fig. 2, we first\nsplit the image into patches with different sizes, then input\nthese different patches into different SPAT as the embedding\nsequences to further extract the image’s spatial characteristics\nacross multiple scales, combining the skip connection and\naggregation operations to acquire the ultimate output result\nFSum\nspa ,which can be formally expressed by the equation:\nFSum\nspa = Add(ΣL=3\nl=1 SP AT, Oa) (14)\nE. Residual Local Feature Module\nAs depicted in Fig. 5, the RLFM consists of Residual\nDense Local Feature Block (RDLFB) and (Enhanced Spatial\nAttention) ESA, which significantly reduces the computation\ntime while maintaining the model capacity. Among them, the\nRDLFB mainly consists of several successive stacked convolu-\ntional operations and activation functions as deep local feature\nextraction, and the skip connection are employed to propagate\nover multiple layers while mitigating the fading problem, the\ncomplete procedure can be mathematically summarized using\nthe following equation:\nFi = Concate (ReLU (Conv (xi)) , xi)\nFout = Add\n\u0000\nΣL=6\nl=1 Concate (ReLU (Conv (xL)) , xL) , x1\n\u0001\n(15)\nWhere xi denotes the input at different depths, Fi denotes\nthe output at each stage, Conv denotes the convolution op-\neration with kernel size of 3 × 3, and the final output Fout\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nis obtained through the successive level aggregation and skip\nconnection operation.\nThe obtained Fout is used as the input to the ESA module\nfor further processing to obtain the final HRMS image, as\nshown in the bottom of Fig. 3. Through convolution operation\nwith a kernel size of 1 × 1 ,we downscaled the input feature\nFout and yielded FConv1,and in order to reduce the spatial\nsize of the feature map to obtain the sensory field of a larger\nspatial range, the feature map is sequentially downscaled by\nthe convolution and pooling operations. After the convolution\nand pooling operations, the output is generated by performing\nelement-wise multiplication between the feature map and the\ninput, and the whole process is expressed as the following\nequation:\nFConv1 = Conv1×1 (Fout)\nF∆ = Conv3×3 (MP (Conv3×3 (Fout)))\nFΛ = Sigmoid (Conv1×1 (Add (Conv1×1 (FConv1) , F∆)))\nFfinal = Fout ⊗ FΛ\n(16)\nwhere Conv1×1(·) and Conv3×3(·) denotes convolution op-\nerations with kernel sizes of 1×1 and 3×3 respectively,MP (·)\ndenotes Maxpool pooling operation, Add(·) denotes an el-\nement summation operation, Sigmoid(·) denotes Sigmoid\nactivation function, and ⊗ denotes an inner product operation.\nF . Loss Function\nBy calculating the loss between the fused image and the real\nimage in the DMFN network for propagation to continuously\noptimize the network performance. The MSE loss function is\nrobust for small error values and its squared curve is more\nsmoother in the vicinity, which will not have a significant\nimpact like the L1 [54] loss function. Resulting in the direction\nof the training model gradually deviates from the target, so\nwe adopts the MSE loss function. In addition, thanks to the\nLaplace loss function[55], It provides enhanced robustness\nagainst abnormal value compared to the MSE loss function, so\nthat the fine-grained features and spectral characteristics of the\npredicted images closely resemble those of the target image.\nwe combine the two different loss functions in this paper to\ntrain our DMFN network, the formula is specified as follows:\nlfinal = MSE (x, y) + γLap (x, y) (17)\nx denotes the fused image and y denotes the truth ground.\nThe MSE function is defined as follows:\nLMSE = 1\nnΣi=n\ni=1\n\u0010\nyi −\n∼\nyi)2 (18)\nn represents the count of observations, (·)2 denotes the\nsquare of the error between the predicted image and the real\nimage,yi represents the predicted image,\n∼\nyi denotes the true\nimage, and the squared difference of all the samples is summed\nand averaged to obtain the final loss, and the Laplace loss\nfunction is defined as follows:\nLap = Σn\nj=12j−1 | Lj(β) − Lj\n\u0012∼\nβ\n\u0013\n|1 (19)\nwhere Lj\n\u0012∼\nβ\n\u0013\ndenotes the result of the jth layer in the slap\nloss function, and Lj(β) denotes the result of the real image\nin the jth layer, the computed multilayer results are summed.\nIV. E XPERIMENTAL RESULTS\nThe DMFN network is trained and validated on two bench-\nmark datasets. Firstly, we will explain in detail about the\nbenchmark datasets and evaluation metrics used in our experi-\nments, demonstrate the enhancement of the overall network\nperformance brought about by the modules introduced in\nour experiments through the ablation experiments. Finally,\nwe conduct a comprehensive analysis, encompassing both\nqualitative and quantitative aspects, as we compare our method\nwith nine SOTA pan-sharpening approaches with and without\nreference images, those experiments were carried out using\nNVIDIA Titan RTX and used pytorch framework.\nA. Datasets and Metrics\nExperiments are executed on both the QB and WV2\ndatasets, where the QB dataset includes four bands, and the\nspatial resolutions provided by the MS image and the PAN\nimage are 2.44m and 0.61m. The WV2 dataset includes eight\nbands, but our experiments only consider four of them, the\nred, blue, green, and NIR spectral bands. When it comes\nto spatial resolution, the MS image provides a coarser 4m,\nwhile the PAN image provides a finer 1m resolution. The\nDMFN network is trained according to the Wald protocol,\nthe training rounds are 100. The Adam optimizer is used\nand the learning rate is adjusted according to training rounds\nto maintain the stability of the training process with the\ninitial value of 0.0001. The final results are evaluated by the\nqualitative and quantitative analyses. Qualitative analysis is to\nzoom in the regional features of the fused images through\nvisual operations and calculate the residual images with the\nreal images, which observe the different performance of the\nmethods more intuitively. Quantitative analysis is used to judge\nthe differences between the pan-sharpening methods by com-\nparing several common evaluation metrics, including the refer-\nence metrics Universal Image Quality Index(UIQI)[56], Spa-\ntial Correlation Coefficient (SCC)[56],Spectral Angle Map-\nper (SAM)[56], Erreur Relative Globale Adimen-sionnelle\nde Synthese(ERGAS)[57], No- Reference Evaluation Metrics\nincluding Composite Evaluation Index(QNR)[58], Spatial Dis-\ntortion Index(Ds)[58], Spectral Distortion Index( Dλ)[58].\n• ERGAS: ERGAS is used to assess the fusion quality of\nremote sensing images, taking full account of the importance\nof each frequency band and the influence of spatial resolution,\nthe formula for calculating the ERGAS metric are as follows:\nERGAS (F, G) = 100 · Repan\nRems\ns\n1\nLΣL\ni=1\nRMSE 2 (Fb, Gb)\nM (F2\nb )\n(20)\nWhere F and G represents the fused image and real image\nrespectively,Repan and Rems refers the spatial resolution for\nboth the PAN and MS images, Fb denotes the constituent\nbands of the fused image, Gb represents the constituent bands\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nFig. 5. The architecture of Redidual Dense Local Feature Block(top) and the Enhanced Spatial Attention(down).\nof the real image,M(F2\nb ) indicates the mean bands of the fused\nimage,RMSE 2(Fb, Gb) denotes the mean square of each band\nin the fused image and real image. Smaller ERGAS values\ncorrespond to higher quality in the fused image, and the best\nvalue is 0.\n• SAM: The SAM are used to compare the angular dif-\nference between two spectral vectors to measure the spectral\nsimilarity between pixels or regions. A reduction in the angle\ncorresponds to an escalation in spectral similarity, and the best\nvalue is 0. The calculation formula described as follows:\nSAM = arccos\n\u0012 α · β\n∥ α ∥∥ β ∥\n\u0013\n(21)\nWhere α and β denote the corresponding spectral vectors\nbetween the real image and the fused image, (·) denote the\ninner product operation of the vectors,∥ · ∥denote the modulus\nof the vectors. As the SAM value approaches 0, it indicates a\nhigher degree of spectral fidelity in the fused image.\n• Q: The Q is used to compare the similarity of structural\nand spectral information between the fused image and the real\nimage.A higher Q value signifies an enhanced resemblance\nbetween the fused image and the real image, and the best value\nis 1. It’s calculation formula is expressed by the following\nequation:\nQ = 4 · µxy\nµxµy\n· ηxηy\nη2x + η2y\n· µxµy\nµ2x + µ2y\n(22)\nWhere µ∗,η∗ and µxy denotes the standard deviation, mean\ndeviation, and covariance between the fused image and the\nreal image.\n• SCC: The primary purpose of the SCC is to assess the\nspatial similarity between the fused image and the original\nimage. When the SCC value approaches 1, it indicates a higher\ndegree of spatial structural richness in the fused image.\n• QNR,DS and Dλ: These three metrics are mainly used for\nno-reference measurements, in which QNR mainly evaluates\nthe clarity and noise level of the image by quantifying the\nsignal-to-noise ratio in the image, Dλ represents the spectral\ndistortion of the image, and Ds represents the spatial infor-\nmation distortion rate of the image. The QNR is calculated\nbased on Dλ and Ds, defined as follows:\nQNR =\n\u0000\n1 − Dλ)γ \u0000\n1 − Ds)δ (23)\nThe fused image’s spatial-spectral information becomes\nmore abundant as QNR gets closer to 1, and the ideal value\nof Ds and Dλ is 0.\nB. Ablation experiment\nIn this section, to demonstrate that our introduced modules\nbring performance enhancement to our proposed DMFN net-\nwork, we conduct ablation experiments of Adaptive Feature\nExtraction BLock (AFEB) and (Enhanced Spatial Attention)\nESA modules on four different network on benchmark datasets\nQB and WV2. The ablation experiments are conducted on four\ndifferent network. We find the best network through qualitative\nand quantitative comparative analyses.\n• DMFN(orin): This network serves as the initial network\nstructure for the experiments, the ordinary convolutional op-\nerations are used to replace the AFEB and ESA modules,\nwhich demonstrate the performance enhancement brought by\nthe introduced modules to the network.\n• DMFN(AFEB): This network only introduces the AFEB\nmodule to process the feature maps based on the initial net-\nwork, which highlights the performance enhancement brought\nby the module to the network.\n• DMFN(ESA): In contrast to the above network model, we\nonly introduce the ESA module based on the initial network\nmodel to further prove the capability of the module.\n• DMFN: As the network introduced in this paper, the\nDMFN network structure introduced the AFEB module in\nthe feature extraction and image fusion stages to use the\nattention mechanism on the input images of different scales,\nwhich emphasize the crucial aspects of the image and mitigate\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nFig. 6. Ablation experiments were conducted on QB dataset using the four network. The first line shows the enlarge detail of the red rectangele and yellow\nrectangle, and the second line shows the pansharpening fusion images of the four models,the third line shows the residual images of the four pansharpening\nresults with ground truth image.\nirrelevant details. The ESA module is also introduced in the\nimage reconstruction process, which enhances the extraction\nof image features with little increase in the capacity of the\nmodel, thus generating the final fused image.\nWe performed ablation experiments on the QB dataset with\nthe four network and obtained the fusion images and the\nresidual images with the real images as shown in Fig. 6, where\nthe red rectangles and the yellow rectangles are the results of\nzooming in on some areas in the image.\nThe yellow rectangles include roofs, pools, and forest areas,\nwhile the red areas contain roofs, cars, and forest parts. The\nspectral distortion of the image generated by DMFN(orin) is\nmore serious as can be seen in the image. Zooming in the\nyellow and red rectangles also reveals that the pools and roofs\nare dark and the spatial structure is incomplete. Zooming in\nthe residual images, we can discern that the residuals are\nmost obvious in DMFN(orin). In contrast, the quality of the\nfused images generated by DMFN(AFEB) and DMFN(ESA)\nis evidently improved. By zooming in the yellow rectangle in\nDMFN(AFEB), we can see that the color of the pool and the\nroof are significantly improved compared with DMFN(orin).\nZooming in the red rectangle in the figure, we can see that\nthe spatial structure of the roof is richer, and the details\nof the contours have been supplemented. The fused image\nof DMFN(ESA) is also obviously clearer than DMFN(orin),\nzooming in the yellow image we can see that the spectral\ndistribution of the pool is more uniform than DMFN(AFEB),\nand the spatial distribution of the roof is obviously richer\nthan that of DMFN(orin) from the zoomed-in red rectangle.\nZooming in the residual images, we can see that the spatial\nstructure information of the fused image is complete and the\nspectral distribution is uniform, which is closer to the real\nimage. Zooming in the yellow rectangle and the red rectangle\nin the figure, we can also see that the color distribution of the\npool and the roof has been greatly improved. And it is clearer\nand more complete than the fused image of DMFN(AFEB) and\nDMFN(ESA) network, better than the other network models\nmentioned above.\nWe conducted ablation experiments on the above four\nnetwork on the WV2 dataset and obtained the fusion images\nand the residual images with the real images as shown in Fig.\n7, the red rectangles and the yellow rectangles are the results\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nFig. 7. Ablation experiments were conducted on WV2 dataset using the four network. The first line shows the enlarge detail of the red rectangele and yellow\nrectangle, and the second line shows the pansharpening fusion images of the four models,the third line shows the residual images of the four pansharpening\nresults with ground truth image.\nof zooming in some areas in the images.\nThe yellow rectangle includes rivers and buildings, and\nthe red rectangle mainly includes house roofs. The zoomed-\nin residual image show that the DMFN(orin) fusion image\nhas the most serious problems of spatial information loss and\nspectral distortion. The yellow and red rectangles also show\nthat the river and the roofs of the buildings have serious color\ndistortion, the whole image is dark and the spatial information\nis incomplete, it is almost impossible to see the roofs clearly\nafter the zoomed-in operation. In contrast, DMFN(AFEB) and\nDMFN(ESA) have improved the ability of the network to ex-\ntract the spatial-spectral information of the image in a certain\nextent, and it is evident that the spatial structure and spectral\ninformation of the fused images of these two models are\nricher than the DMFN(orin) from the enlarged residuals. The\nspectral distributions of the river area are more uniform from\nthe enlarged yellow picture, the enlarged red rectangle also\nreveals that the DMFN(ESA) fusion image is richer in spatial\nstructure information than DMFN(AFEB), and the details of\nthe contours of the red roofs are better complemented with a\nuniform spectral distribution. Compared with the above model,\nthe fused image produced by our innovative DMFN algorithm\nclosely approximates the real image. A deeper analysis of the\nresidual image under magnification reveals that our method not\nonly maintains comprehensive spatial details but also achieves\na more even distribution of spectral information. From the\nenlarged yellow rectangle we can see that the color distribution\nof the river area is greatly improved, and the spatial details of\nthe roofs are also significantly improved. From the enlarged\nred rectangle, we can also find that both the spatial and spectral\ndistributions of the roofs are enriched, the details of the roofs’\ncontours are clearly visible, which is better than the other\nnetwork structures mentioned above.\nWe carry out the quantitative analysis of the ablation\nexperiments on the benchmark datasets QB and WV2 and\nthe comparative results are presented in Table I and II, in\nwhich the best values of the SCC and Q are 1, and the best\nvalues of the SAM and ERGAS are 0. It can be seen that all\nthe metrics of DMFN(AFEB) and DMFN(ESA) are superior\nthan DMFN(orin), which indicates AFEB and ESA modules\ncan provide improvement to the performance of the model.\nBoosting the model’s aptitude for extracting spatial structure\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nand spectral distribution from the image. Our DMFN method\nachieves the best results in all the metrics, which indicates that\nour method has the strongest ability to extract spatial details\nand recover spectral distribution, the fused image is closer to\nthe real image.\nTABLE I\nQUANTITATIVE COMPARISON OF FOUR METHODS ON THE QB DATASET.\nMethod\nQB\nSAM(0) ERGAS(0) SCC (1) Q (1)\nDMFN(orin) 0.037 1.1341 0.9945 0.9626\nDMFN(AFEB) 0.0334 1.0485 0.995 0.965\nDMFN(ESA) 0.0335 1.0423 0.9952 0.9648\nDMFN 0.0304 0.9733 0.9958 0.9672\nWe bolded the best result.\nTABLE II\nQUANTITATIVE COMPARISON OF FOUR METHODS ON THE WV2 DATASET.\nMethod\nWV2\nSAM(0) ERGAS(0) SCC (1) Q (1)\nDMFN(orin) 0.0358 1.1315 0.995 0.9679\nDMFN(BFE) 0.0314 1.0251 0.9952 0.9697\nDMFN(BFE+BRB) 0.0316 1.0205 0.9951 0.9694\nDMFN 0.0285 0.9509 0.9957 0.972\nWe bolded the best result.\nC. Experiment and evaluations\nwe will perform a qualitative and quantitative assessment\nof the method presented in this paper, contrasting it with\nnine current SOTA techniques using both simulated and\nreal data, which are three classical methods: IHS, GSA,\nMTF GLP HPM and five deep learning-based methods: Pan-\nNet, MSDCNN, GDD[39], MSIT[52], LGT[53], GPPNN. The\ndatasets and codes of the above-mentioned methods are all\nopen source projects, the parameter values are established\nfollowing the recommendations detailed in the paper.\nTo comprehensively analyze the performance of each net-\nwork from different perspectives, we also conducted a compar-\native analysis of the complexity (Params) and computational\ncosts (FLOPS) of each network structure. The table III displays\nthe numerical values associated with the various deep learning\nnetworks used in our paper. It is evident that our proposed\nnetwork model does not exhibit an advantage in terms of\nparameter count and computational cost. However, as demon-\nstrated in the subsequent comparative analysis experiments,\nthis network has achieved a balance between performance and\nefficiency.\n1) Simulated data experiment:: The above nine pan-\nsharpening methods are experimented on the benchmark\ndatasets QB and WV2 following the Wald protocol, and the\nHRMS images are used as our real reference images for\ncomparison and analysis.\nFigure 8 illustrates the fusion images of all the above pan-\nsharpening methods on the simulated dataset QB and the\nzoomed-in images of some regions in the figure, the main\ncontent of the figure includes the roof, container, road, and\nother regions, the yellow rectangle is the zoomed-in area of the\ncontainer in the figure, and the red rectangle is the zoomed-in\narea of the roof and the road in the figure. The GDD and\nMSIT methods exhibit the most severe spectral distortion,\nresulting in an overall image over-brightness. Upon closer\nexamination of the magnified yellow rectangle in the image,\nwe can observe a significant loss of color in the containers\nand a pronounced blurring of their overall outlines, making it\nchallenging to discern finer details. Similarly, when we focus\non the magnified red rectangle, a substantial disruption in the\nspatial structure of the rooftops becomes evident, with nearly\nall edge details being heavily blurred, leading to a significant\nloss of spectral information. In contrast, the ability of IHS,\nPanNet, MTF GLP HPM and GSA methods to extract the\ninformation embedded in images is greatly improved, but from\nthe magnified regional images, there are still exists spectral\ndistortion and spatial distortion phenomenon, in which the yel-\nlow rectangle of the container area of the color distribution and\nspatial information of the IHS, PanNet and GSA are better than\nMTF GLP HPM. Observing the zoomed-in red rectangle, it\nis also found that the roof color remains severely skewed,\nthe specific spatial contour details are blurred, and the spatial\ncharacteristics have been seriously impaired. The ability of\nMSDCNN, LGT, and GPPNN methods to extract spectral\ninformation is significantly better than any of the above pan-\nsharpening methods, from the zoomed-in yellow rectangle, the\nspectral information of the container is obviously recovered,\nand the spatial structure information is greatly enriched so that\nthe details of the container’s contour can be observed. From\nthe enlarged red rectangle, we can also find that a noticeable\nenhancement in the roof’s color is apparent when contrasted\nwith the method mentioned above, and the spatial profile in-\nformation of the highway is further expanded.The comparative\nanalysis indicates that the GPPNN pan-sharpening method\noutperformes in capturing both spatial and spectral information\nwhen compared to the MSDCNN and LGT methods. In\ncontrast, our proposed DMFN method outperforms all the\npreviously mentioned pansharpening methods in extracting\nboth spatial and spectral information. The magnified images\nvividly depict the spatial distribution details of containers and\nroof while retaining the rich spectral information contained\nin the MS image, closer to GT images. It achieves optimal\nperformance when compared to the aforementioned methods.\nTABLE III\nCOMPARASION OF PARAMETERS AND FLOATING-POINT\nOPERATIONS OF DEEP LEARNING MODEL.\nModels PanNet MSDCNN GPPNN LGT MSIT Ours\nParams 0.069 0.229 0.120 0.174 295.250 13.519\nFLOPS 8.287 13.980 10.266 18.5471 133.641 109.56\nFigure 9 illustrates the residual images with the real image,\nit is evident that the residual values of MTF GLP HPM,\nGSA, and IHS methods are the most obvious, which indicates\nthat these pan-sharpening methods have more serious spectral\ndistortion problems and lose more spatial information. In\ncontrast, the residual values of PanNet, GDD and MSIT\nmethods are significantly reduced, the PanNet method exhibits\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nFig. 8. Fusion images of nine pansharpening methods on the QB simulation dataset. The yellow rectangle and red rectangle are the detailed enlarged\nimages,you can zoom in to visiuallize the image details.\nFig. 9. Fusion results of nine pan-sharpening methods on the QB simulation dataset and the residual images with Ground Truth images. The images can be\nzoomed in to view the details.\nthe most obvious residual values, indicating more severe\nspectral distortion and significant disruption of the spatial\nstructure compared to the other two methods. While the\nGDD method keeps more spatial information, which explains\nthe less apparent residual values. The MSDCNN, LGT, and\nGPPNN methods capture the spatial intricacies and spectral\ninformation of the image more deeply, the zoomed-in residual\nimage shows a few obvious white contours, the rest of the\nimage all black background. The residual value of GPPNN is\nthe smallest, which indicates that the GPPNN method extracts\nthe relevant spatial structure details of the image as completely\nas possible and maintains the spectral information, performing\nthe best among the three methods. Zooming in the residual\nimage generated by our proposed DMFN method, we will find\nthat the whole background is almost black without any white\ncontour, which indicates that our method proves proficient in\nthe preservation of spectral distribution and the extraction of\nspatial details to the maximum extent.\nFig. 10 exhibits the fused images on the simulated dataset\nWV2 compared with the other pan-sharpening techniques as\nwell as the zoomed-in images of some areas in the images. The\nmain contents of the figure are wheat fields, roofs, and roads.\nThe yellow rectangle are the zoomed-in areas of roofs and\nthe red rectangle are the zoomed-in areas of wheat fields. The\nfusion images of GDD and MSIT fusion image has serious\nspectral distortion problem, and the spatial structure suffer\ndamaged. Additionally, observing the enlarged yellow rectan-\ngle, it is apparent that the roof’s color experiences significant\ndistortion, and there is partial blurring in its spatial structure,\nwhich hinders the clear visibility of specific structural details\non the roof. In the enlarged red rectangle, the color of the\nwheat field is heavily distorted, with an overall brightening\nand blurring of the specific contours. In contrast, the IHS,\nGSA, MTF GLP HPM and PanNet methods are more capable\nof extracting the spatial structure and spectral distribution of\nthe image. Observing the enlarged yellow rectangle reveals\nthat the spatial distribution of roof details is rich, and the\nspectral information distribution is more uniform than the\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nFig. 10. Fusion images of nine pansharpening methods on the WV2 simulation dataset. The yellow rectangle and red rectangle are the detailed enlarged\nimages,you can zoom in to view image details.\nFig. 11. Fusion results of nine pan-sharpening methods on the WV2 simulation dataset and the residual images with Ground Truth images. The images can\nbe zoomed in to see the details.\nother three methods, shows a better performance. The extrac-\ntion of the spectral distribution information by the MSDCNN,\nLGT, and GPPNN methods is obviously stronger than the\nabove-mentioned methods, the overall image color tends to\nbe more similar to the real image. What can be seen in the\nzoomed-in yellow rectangle is that the spatial details of the\nroof are more complete, the contours are clearly visible and\nthe spectral distribution is uniform. From the enlarged red\nrectangle, we can see that the color distribution of the wheat\nfield is uniform, and the spatial structure is clear and complete.\nFrom the enlarged image, we can also found that the color\ndistribution of the roof and the wheat field of MSDCNN is\nmore uniform and richer than the other two methods, which\nindicates that the performance of MSDCNN is better than the\nLGT and GPPNN methods. Our proposed DMFN method has\nthe best performance in extracting spatial detail information\nand spectral distribution information. The zoomed-in yellow\nrectangle shows that the spatial detail structure of the roof is\nbasically complete and the spectral distribution is more uni-\nform. The zoomed-in red rectangle also shows that the wheat\nfield is full of hues, the spectral distribution is uniform and\nthe specific details of the distribution can be seen clearly. The\nability to extract the spatial structure and spectral distribution\ninformation is better than any of the above pan-sharpening\nmethods.\nFigure 11 shows the final fused image and the residual im-\nage with the real image. Compared with other pan-sharpening\nmethods, the residual value of GDD and MSIT pan-sharpening\nmethods is the most obvious. The color of the fused image\nis bright, which reveals that the loss of spatial structure is\nnotably obvious compared with the other methods. The whole\nroad and house contours are clearly visible in the image,\nmeans that there is still a large loss of value compared with\nthe real image. Compared with MTF GLP HPM and PanNet\nmethods, although the IHS and GSA fused images are darker\nwith fewer regions of residual values. However, when zooming\nin on certain areas within the residual images, it becomes\nevident that there is a significant loss of spatial structural\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\ninformation in the images. The fusion images from MSDCNN,\nLGT, and GPPNN methods closely resemble the real image.\nFrom the residual images, it is evident that except for a few\nwhite regions, the rest of the background appears black. This\nindicates a uniform spectral distribution and rich spatial details\nin the images, surpassing the other mentioned pansharpening\nmethods. The residual values in the GPPNN method are\nnotably smaller than those in the MSDCNN and LGT methods,\nimplying that the GPPNN method excels in capturing both\nspatial and spectral information from the images compared\nto the other two methods. In contrast, when observing the\nfused image and the residual map of our method, it is found\nthat the residual map’s background all black. The zoomed-\nin residual image contains almost no residual values, and\nthe fused image closely approximates the real image, which\nindicates that the loss of this method in extracting the spatial\nstructural and the spectral distribution information is almost\nnegligible. Surpasses all the aforementioned pan-sharpening\napproaches.\nFurthermore, we conduct a quantitative comparison and\nanalysis of the fusion outcomes for all the compared methods\non two simulated datasets QB and WV2, where we select four\nmetrics as the reference metrics, namely ERGAS, SCC, SAM,\nand Q. the ideal value for SAM and ERGAS is 0, and the best\nvalue for Q and SCC is 1. Table IV shows the results of the\nexperimental comparison with nine pan-sharpening methods\non the QB dataset, we bolded the best value and underlined\nthe second value of each metric. It becomes apparent that\nour method surpasses all others, consistently ranking first\nin all four metrics. SAM is 0.0028 higher than the second\nMSDCNN, ERGAS is 0.0689 higher than GPPNN, SCC is\n0.0009 higher than GPPNN, and Q is 0.0032 higher than it.\nIn general, the images fused by our our method show the best\nperformance.\nThe WV2 dataset was used to conduct experiments compar-\ning our method with nine other pan-sharpening techniques, and\nthe results are displayed in Table V . It’s apparent that except\nfor the SAM, which is 0.0026 lower than the GPPNN method,\nour method performer the best in the other metrics. The\nERGAS is 0.2126 higher than the GPPNN method, the SCC is\n0.0006 higher than the GPPNN method, the Q is 0.004 higher\nthan the GPPNN method. To sum it up ，our approach boasts\nthe best performance, indicating that the DMFN network\nachieves superior results in extracting information in contrast\nto other methods.\n2) Real data experiment::We also conduct qualitative and\nquantitative comparative experimental analyses of these nine\ndifferent pan-sharpening methods on the benchmark datasets\nof QB and WV2. The real QB dataset was used to generate the\nfusion results depicted in Figure 12 for various pan-sharpening\nmethods, which includes the areas of rooftops, highways,\nautomobiles forests. The yellow and red rectangles shows the\nmagnified results of the pool and the forest areas. We can\nsee that the GDD and MSIT methods has serious problems\nof spectral distortion and the loss of spatial information.\nFrom the zoomed-in red rectangle, we can also find that the\nstructure of the forest area is seriously twisted and the forest\narea is seriously blurred, we can hardly see any details of\nTABLE IV\nQUANTITATIVE COMPARISON ON THE QB DATASET, THE OPTIMAL\nVALUE HAS BEEN BOLDED AND THE SECOND VALUE HAS BEEN\nUNDERLINED .\nMethod\nQB\nQ ↑ SCC ↑ ERGAS ↓ SAM ↓\nIHS 0.7865 0.9389 4.5934 0.1826\nGSA 0.8632 0.9666 5.2084 0.0724\nMTF GLP HPM 0.8204 0.9567 5.548 0.0718\nPanNet 0.815 0.9531 5.4895 0.0745\nGDD 0.8185 0.9679 7.8380 0.07793\nMSDCNN 0.9555 0.9921 1.3268 0.0332\nMSIT 0.7369 0.9576 6.7982 0.0355\nLGT 0.9481 0.9937 1.1671 0.0355\nGPPNN 0.9639 0.9950 1.0422 0.0336\nOurs 0.9671 0.9959 0.9733 0.0304\nWe bolded the best result and Underlined the second value.\nTABLE V\nQUANTITATIVE COMPARISON ON THE WV2 DATASET, THE OPTIMAL\nVALUE HAS BEEN BOLDED AND UNDERLINED THE SECOND VALUE .\nMethod\nWV2\nQ ↑ SCC ↑ ERGAS SAM ↓\nIHS 0.8928 0.9325 5.3827 0.1546\nGSA 0.8377 0.9632 6.1055 0.0885\nMTF GLP HPM 0.7918 0.9531 6.4917 0.0888\nPanNet 0.7824 0.9479 6.372 0.0902\nGDD 0.7923 0.9644 9.4575 0.0955\nMSDCNN 0.9578 0.9920 1.3004 0.0317\nMSIT 0.6864 0.9569 8.7491 0.0748\nLGT 0.9520 0.9932 1.1646 0.0339\nGPPNN 0.9681 0.9951 1.0035 0.0311\nOurs 0.9721 0.9957 0.7909 0.0285\nWe bolded the best result and Underlined the second value.\nthe specific contours. From the zoomed-in yellow rectangle,\nwe find that the pool is seriously distorted and the spatial\nstructure is seriously damaged. In contrast, the GSA and\nPanNet methods make up for the spatial and spectral loss\nof the image to a certain extent. From the enlarged yellow\nrectangle, we can see that the color distribution of the pool\nhas been improved, and the peripheral distribution contours\ncan be seen vaguely. From the enlarged red rectangle, we\ncan find that the spatial details of the forest region have been\nenriched. The IHS and MTF GLP HPM methods obtain the\nspatial structural and spectral distribution of the image more\nefficient, as can be seen from the enlarged yellow rectangle,\nthe spectral distribution of these two methods is closer to\nthe MS image with better spatial detail distribution, and the\ncontour detail distribution of the pools is obviously clearer.\nObserving the enlarged red rectangle also reveals that the IHS\nmethod blurs the forest region more than the PAN image,\nand the spatial detail information of the forest region in the\nMTF GLP HPM method is closer to the PAN image, but\nthe color distribution is obviously darker than the MS image.\nMSDCNN, LGT, and GPPNN methods further improve the\nability of the nerwork to extract the information of the image.\nFrom the zoomed-in yellow rectangle, we can find that the\nspectral distribution of the pool region is greatly improved, its\nspatial contour details are constantly enriched compared with\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 15\nFig. 12. Fusion images of nine pansharpening methods on the QB real dataset. The yellow rectangle and red rectangle are the detailed enlarged images,you\ncan zoom in to view image details\nFig. 13. Fusion results of nine pan-sharpening methods on the QB real dataset and the residual images with PAN images. The images can be zoomed in to\nview the details.\nthe PAN image,and the area around the pool can be observed\nmore clearly. From the enlarged red rectangle, we can also see\nthat the spectral distribution and spatial structure information\nof the forest region are further expanded. The spatial details of\nthe forest region in the MSDCNN and the GPPNN methodS\nare richer than the LGT, which is closer to the PAN image.\nObserving the fused images using our method, we can find that\nour method effectively obtains the spatial structure contained\nin PAN image, while recovers the spectral distribution of the\nMS image to the largest extent. Observing the enlarged yellow\nand red rectangles, we can find that the spectral distribution\nof the pool and the forest region is uniform, the spatial\ndetails are clear and the contour distribution has been further\ndemonstrated. Our approach outperforms the pan-sharpening\nmethods mentioned above.\nFigure 13 shows the fused images of these nine pan-\nsharpening methods and the residual images with the real\nimages. It is apparent that the residual value of the GDD,\nMSIT methods are the most, which indicates that the spatial\nstructure details of the image has been seriously damaged.\nThe loss of spectral information is more serious, and the\ncontours of the highway and the forest area can be clearly seen\nfrom the residuals. In contrast, the residual values of GSA,\nPanNet, MTF GLP HPM and IHS methods are reduced, and\nthe spatial structure of the images is closer to the PAN images.\nthere still exists serious spectral distortion, compared to the\nMS images ，the images manifest a darker color tone. The\ntraces of the road’s residual value has been much decreased\nand the richness of spatial information exceeds that of the\nmethods mentioned above. By enlarging the residual images,\nwe find that the residual values of MSDCNN, LGT, and\nGPPNN methods are obviously reduced, we can hardly see\nany obvious residuals in the highway or the forest area, which\nindicates that the ability of these methods to obtain spatial\ncharacteristic information is obviously better than the above-\nmentioned methods. The color distribution shown a significant\nenhancement in comparison to MS images and the residual\nimages are much more smoother. Correspondingly, the fused\nimage of our method is closer to the real image, the whole\nresidual image is in a smooth state, and the residual values\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 16\nFig. 14. Fusion images of nine pansharpening methods on the WV2 real dataset. The yellow rectangle and red rectangle are the detailed enlarged images,you\ncan zoom in to see image details.\nFig. 15. Fusion results of nine pan-sharpening methods on the WV2 real dataset and the residual images with PAN images. The images can be zoomed in\nto see the details.\ncan hardly be seen in the zoomed-in residual image, which\nindicates that our method comprehensively captures the spatial\nintricacies of the PAN image while also adeptly preserving the\nrich spectral distribution details of the MS image.\nIn figure 14, the fusion outcomes of nine pan-sharpening\ntechniques applied to the WV2 real dataset are depicted, which\ncontains the roof, forest, and car areas. the yellow and red\nrectangles are the zoomed-in results in the figure. we can\nsee from the fused image that the GDD and MSIT methods\nsuffer from serious spectral distortion and the loss of spatial\ninformation. Zoomed-in the yellow rectangle, we can find that\nthe car and forest areas are seriously blurred, the colors of\nthe cars and forests are pale compared with the MS image,\nand the spatial structure details inherent in the PAN image\nis seriously lost. Zooming in the red rectangle reveals that\nthe roof’s overall outline is notably well-preserved, but the\nroof is not as complete as the PAN image. Although the\noverall outline of the roof is relatively complete, the spatial\ndetails are damaged and the blurring is serious. In contrast,\nthe IHS, PanNet, MTF GLP HPM, and GSA methods can\nbetter extract the spatial structure and spectral information of\nthe image. Observing the enlarged yellow rectangle, we can\nfind that the spectral distribution of the forest and the car is\nmore balanced, the distribution of the specific contours is also\nclearer. The fused image generated by the MTF GLP HPM\nmethod is obviously clearer than the other three methods,\nretains plentiful spatial details in the PAN image better.\nFrom the enlarged red rectangle, we can see that the spatial\ndistribution of the roof is effectively preserved, but the spectral\ninformation distribution is still insufficient compared with\nthe MS image. We can see that the MSDCNN, LGT and\nGPPNN methods have further improved the spatial details\nextraction and the spectral information recovery. By observing\nthe enlarged yellow rectangle, it’s evident that the spectral\ndistribution of the forest and the car area is almost the same\nas the MS image. At the same time, the rich spatial structural\ninformation contained in the PAN image has been retained.\nFrom the zoomed-in red rectangle, we can also see that the\nspatial details of the roof are effectively preserved, and the\ndistribution of the specific contours is almost the same as\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 17\nTABLE VI\nQUANTITATIVE COMPARISON OF NINE METHODS ON TWO REAL DATASETS QB AND WV2.\nMethod\nQB WV2\nDS ↓ Dλ ↓ QNR ↑ DS ↓ Dλ ↓ QNR ↑\nIHS 0.0634 0.0623 0.9285 0.1056 0.0367 0.8714\nGSA 0.0346 0.0167 0.951 0.0478 0.0326 0.9312\nMTF GLP HPM 0.0191 0.0251 0.9463 0.0248 0.0219 0.9426\nPanNet 0.0214 0.0219 0.9585 0.0301 0.0238 0.9404\nGDD 0.0276 0.0390 0.9348 0.0426 0.0539 0.9068\nMSDCNN 0.0239 0.0311 0.9458 0.0306 0.0477 0.9236\nMSIT 0.0215 0.0454 0.9344 0.0293 0.0643 0.9087\nLGT 0.0226 0.0343 0.9440 0.0307 0.0511 0.9201\nGPPNN 0.0187 0.0161 0.9591 0.0284 0.0172 0.9438\nOurs 0.0156 0.0148 0.9684 0.0195 0.0212 0.9512\nWe bolded the best result and Underlined the second value.\nthe PAN image. In contrast, the fused image of our method\neffectively captures the abundant spatial details embedded\nin the PAN image and efficiently recovers the rich spectral\ndistribution information contained in the MS image. Observing\nthe zoomed-in yellow and red rectangles, we can find that the\nspatial structure of the roof and the forest area is effectively\npreserved with clear contours and uniform color distribution,\nwhich is better than any of the above methods.\nFigure 15 shows the fused images of these ten pan-\nsharpening methods and their residual images with the real\nimages. Notably, the GDD and MSIT methods exhibit the most\nstriking residual artifacts, suggesting a substantial deterioration\nin spatial structural information and the subsequent degra-\ndation of spectral content in the fused images. The overall\ncolors of the fused images are darker than the MS image. In\ncontrast, the residual maps of GSA, IHS, MTF GLP HPM,\nand PanNet are more smoother, and the zoomed-in yellow\nrectangle show that the spatial information of the highway area\nis more complete than the above methods, and contains richer\nspatial information than the MS image. But there are still\nexisted serious spectral distortions, the overall image is darker\nand the residual values in the residual maps are more obvious,\nwhich indicates that these methods lose a lot of details when\nextracting the spatial structure information. On the other hand,\nthe MSDCNN, LGT, and GPPNN methods have enhanced\ntheir ability to extract image spectral information. It can\nalso be noticed from the residual images that the residual\nvalues for these methods are not very significant, the fused\nimage contains rich spatial details compared with the PAN\nimage, and the specific spectral distribution information is\ncloser to the MS image. The image’s spatial and spectral\ndistribution characteristics are fully exploited by our method.\nFrom the residual image, it becomes apparent that our method\nyields results devoid of any residuals. The DMFN method\nnot only preserves the abundant spectral data within the MS\nimage but also captures the extensive spatial details from the\nPAN image, which is better than the above-mentioned pan-\nsharpening methods.\nWe also carry out quantitative and comparative experimen-\ntal analysis of these nine pan-sharpening techniques on the\nbenchmark datasets QB and WV2, as depicted in Table VI,\nwe use the no-reference metrics DS, Dλ, and QNR as the\ncomparisons, the optimal values of Dλ and DS are 0 and\nthe best value of QNR is 1. We bolded the optimal value\nand underlined the second in the table. We can see that our\nmethod achieves best in all metrics in the QB dataset. In the\nWV2 dataset, except for the Dλ is 0.004 lower than that of\nGPPNN, the rest metrics performes best. Overall, our method\nis more proficient than the other nine pan-sharpening methods\nand effectively obtains both the spatial and spectral details\nwithin the image.\nIn both qualitative and quantitative assessments on sim-\nulated and real datasets, our method consistently exhibits\nsuperior performance in recovering the spectral distribution\nand spatial information inherent in MS and PAN images,\nsurpassing other pan-sharpening approaches.\nV. C ONCLUSIONS\nwe present a novel pan-sharpening methodology called\nDMFN in this paper, whose full name is Transformer-based\nDual-Branch Multi-Scale Fusion Network for Pan-sharpening\nremote sensing. The network model is separated into three\ncomponents: feature extraction, image fusion and image re-\nconstruction. We commence by interconnecting the MS image\nand PAN image through up-sampling and down-sampling\noperations within distinct branches, and then after our shallow\nfeature extraction, it is inputted into the Adaptive Feature\nExtraction Block(AFEB) to combine the spatial-channel at-\ntention, only focus on the important information of the image.\nThen the feature maps of multiple scales in the two branches\nare input into each (spatial transformer)SPAT and (spectral\ntransformer)SPET for spectral and spatial information extrac-\ntion, after the Upsample operation on the final output of SPET,\nit is connected with the final output of SPAT and input into the\nResidual Dense Local Feature Block To capture the image’s\ndeeper characteristics again, and get our final fusion image\nafter the process of ESA module.To establish the superiority\nof our method, we begin with ablation experiments on the QB\nand WV2 benchmark datasets. We then proceed to compre-\nhensive qualitative and quantitative assessments, comparing\nour method with nine recent pan-sharpening techniques on\nboth simulated and real datasets. The above experiments prove\nthat our DMFN method adeptly captures the abundant spatial\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 18\ndetails inherent in PAN images, while preserving the consistent\nspectral characteristics found in MS images.\nACKNOWLEDGMENT\nThis research was supported by the National Natural Sci-\nence Foundation of China (61772319, 62002200, 62202268,\n61972235), Shandong Natural Science Foundation of China\n(ZR2023MF026, ZR2022MA076), Yantai science and tech-\nnology innovation development plan (2022JCYJ031).\nREFERENCES\n[1] Q. Wang, P. Yan, Y . Yuan, and X. Li, “Multi-spectral\nsaliency detection,” Pattern Recognition Letters 34(1),\npp. 34–41, 2013.\n[2] G. Cheng, J. Han, and X. Lu, “Remote sensing image\nscene classification: Benchmark and state of the art,”\nProceedings of the IEEE105(10), pp. 1865–1883, 2017.\n[3] H. Song, B. Huang, Q. Liu, and K. Zhang, “Improving\nthe spatial resolution of landsat tm/etm+ through fusion\nwith spot5 images via learning-based super-resolution,”\nIEEE Transactions on Geoscience and Remote Sens-\ning 53(3), pp. 1195–1204, 2014.\n[4] C. Thomas, T. Ranchin, L. Wald, and J. Chanussot, “Syn-\nthesis of multispectral images to high spatial resolution:\nA critical review of fusion methods based on remote\nsensing physics,” IEEE Transactions on Geoscience and\nRemote Sensing 46(5), pp. 1301–1312, IEEE, 2008.\n[5] R. A. Schowengerdt, “Reconstruction of multispatial,\nmultispectral image data using spatial frequency con-\ntent,” Photogrammetric Engineering and Remote Sens-\ning 46(10), pp. 1325–1334, 1980.\n[6] R. Haydn, “Application of the ihs color transform to the\nprocessing of multisensor data and image enhancement,”\nin Proc. of the International Symposium on Remote\nSensing of Arid and Semi-Arid Lands, Cairo, Egypt,\n1982, 1982.\n[7] D. Sylla, A. Minghelli-Roman, P. Blanc, A. Mangin,\nand O. H. F. d’Andon, “Fusion of multispectral images\nby extension of the pan-sharpening arsis method,” IEEE\nJournal of Selected Topics in Applied Earth Observations\nand Remote Sensing7(5), pp. 1781–1791, 2013.\n[8] P. Du, S. Liu, J. Xia, and Y . Zhao, “Information fusion\ntechniques for change detection from multi-temporal re-\nmote sensing images,” Information Fusion14(1), pp. 19–\n27, 2013.\n[9] Y . Qu, H. Qi, B. Ayhan, C. Kwan, and R. Kidd, “Does\nmultispectral/hyperspectral pansharpening improve the\nperformance of anomaly detection?,” pp. 6130–6133,\n2017.\n[10] B. Aiazzi, S. Baronti, and M. Selva, “Improving com-\nponent substitution pansharpening through multivariate\nregression of ms + pan data,” IEEE Transactions on\nGeoscience and Remote Sensing45(10), pp. 3230–3239,\n2007.\n[11] P. Kwarteng and A. Chavez, “Extracting spectral contrast\nin landsat thematic mapper image data using selective\nprincipal component analysis,” Photogramm. Eng. Re-\nmote Sens 55(1), pp. 339–348, 1989.\n[12] J. Zhou, D. L. Civco, and J. A. Silander, “A wavelet trans-\nform method to merge landsat tm and spot panchromatic\ndata,” International journal of remote sensing 19(4),\npp. 743–757, 1998.\n[13] W. Carper, T. Lillesand, R. Kiefer, et al., “The use of\nintensity-hue-saturation transformations for merging spot\npanchromatic and multispectral image data,” Photogram-\nmetric Engineering and remote sensing56(4), pp. 459–\n467, 1990.\n[14] G. A. Licciardi, M. M. Khan, J. Chanussot, A. Montan-\nvert, L. Condat, and C. Jutten, “Fusion of hyperspectral\nand panchromatic images using multiresolution analysis\nand nonlinear pca band reduction,” EURASIP Journal on\nAdvances in Signal processing2012(1), pp. 1–17, 2012.\n[15] M. Ghahremani and H. Ghassemian, “Nonlinear ihs: A\npromising method for pan-sharpening,” IEEE Geoscience\nand Remote Sensing Letters 13(11), pp. 1606–1610,\n2016.\n[16] Y . Leung, J. Liu, and J. Zhang, “An improved adaptive\nintensity–hue–saturation method for the fusion of remote\nsensing images,” IEEE Geoscience and Remote Sensing\nLetters 11(5), pp. 985–989, 2013.\n[17] G. Vivone, L. Alparone, J. Chanussot, M. Dalla Mura,\nA. Garzelli, G. A. Licciardi, R. Restaino, and L. Wald, “A\ncritical comparison among pansharpening algorithms,”\nIEEE Transactions on Geoscience and Remote Sens-\ning 53(5), pp. 2565–2586, 2014.\n[18] X. Otazu, M. Gonz ´alez-Aud´ıcana, O. Fors, and J. N ´u˜nez,\n“Introduction of sensor spectral response into image\nfusion methods. application to wavelet-based methods,”\nIEEE Transactions on Geoscience and Remote Sens-\ning 43(10), pp. 2376–2385, 2005.\n[19] B. Aiazzi, L. Alparone, S. Baronti, A. Garzelli, and\nM. Selva, “Mtf-tailored multiscale fusion of high-\nresolution ms and pan imagery,” Photogrammetric En-\ngineering & Remote Sensing72(5), pp. 591–596, 2006.\n[20] F. Nencini, A. Garzelli, S. Baronti, and L. Alparone, “Re-\nmote sensing image fusion using the curvelet transform,”\nInformation fusion 8(2), pp. 143–156, 2007.\n[21] P. Chavez, S. C. Sides, J. A. Anderson, et al., “Compar-\nison of three different methods to merge multiresolution\nand multispectral data- landsat tm and spot panchro-\nmatic,” Photogrammetric Engineering and remote sens-\ning 57(3), pp. 295–303, 1991.\n[22] G. Vivone, M. Dalla Mura, A. Garzelli, R. Restaino,\nG. Scarpa, M. O. Ulfarsson, L. Alparone, and J. Chanus-\nsot, “A new benchmark based on recent advances in\nmultispectral pansharpening: Revisiting pansharpening\nwith classical and emerging pansharpening methods,”\nIEEE Geoscience and Remote Sensing Magazine 9(1),\npp. 53–81, 2020.\n[23] C. Ballester, V . Caselles, L. Igual, J. Verdera, and\nB. Roug´e, “A variational model for p+ xs image fusion,”\nInternational Journal of Computer Vision69, pp. 43–58,\n2006.\n[24] S. Li and B. Yang, “A new pan-sharpening method using\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 19\na compressed sensing technique,” IEEE Transactions on\nGeoscience and Remote Sensing 49(2), pp. 738–746,\n2010.\n[25] Y . Liu and Z. Wang, “A practical pan-sharpening\nmethod with wavelet transform and sparse representa-\ntion,” pp. 288–293, 2013.\n[26] X. X. Zhu, D. Tuia, L. Mou, G.-S. Xia, L. Zhang, F. Xu,\nand F. Fraundorfer, “Deep learning in remote sensing:\nA comprehensive review and list of resources,” IEEE\ngeoscience and remote sensing magazine5(4), pp. 8–36,\n2017.\n[27] G. Masi, D. Cozzolino, L. Verdoliva, and G. Scarpa,\n“Pansharpening by convolutional neural networks,” Re-\nmote Sensing 8(7), p. 594, 2016.\n[28] G. Scarpa, S. Vitale, and D. Cozzolino, “Target-adaptive\ncnn-based pansharpening,” IEEE Transactions on Geo-\nscience and Remote Sensing56(9), pp. 5443–5457, 2018.\n[29] W. Huang, L. Xiao, Z. Wei, H. Liu, and S. Tang, “A\nnew pan-sharpening method with deep neural networks,”\nIEEE Geoscience and Remote Sensing Letters 12(5),\npp. 1037–1041, 2015.\n[30] L. Wald, T. Ranchin, and M. Mangolini, “Fusion of\nsatellite images of different spatial resolutions: Assessing\nthe quality of resulting images,” Photogrammetric engi-\nneering and remote sensing63(6), pp. 691–699, 1997.\n[31] Y . Wei, Q. Yuan, H. Shen, and L. Zhang, “Boosting\nthe accuracy of multispectral image pansharpening by\nlearning a deep residual network,” IEEE Geoscience and\nRemote Sensing Letters14(10), pp. 1795–1799, 2017.\n[32] S. Xu, J. Zhang, K. Sun, Z. Zhao, L. Huang, J. Liu, and\nC. Zhang, “Deep convolutional sparse coding network\nfor pansharpening with guidance of side information,” in\n2021 IEEE International Conference on Multimedia and\nExpo (ICME), pp. 1–6, IEEE, 2021.\n[33] J. Yang, X. Fu, Y . Hu, Y . Huang, X. Ding, and J. Pais-\nley, “Pannet: A deep network architecture for pan-\nsharpening,” pp. 5449–5457, 2017.\n[34] W. Tan, P. Xiang, J. Zhang, H. Zhou, and H. Qin,\n“Remote sensing image fusion via boundary measured\ndual-channel pcnn in multi-scale morphological gradient\ndomain,” IEEE Access 8, pp. 42540–42549, 2020.\n[35] Q. Yuan, Y . Wei, X. Meng, H. Shen, and L. Zhang, “A\nmultiscale and multidepth convolutional neural network\nfor remote sensing imagery pan-sharpening,” IEEE Jour-\nnal of Selected Topics in Applied Earth Observations and\nRemote Sensing 11(3), pp. 978–989, 2018.\n[36] T.-J. Zhang, L.-J. Deng, T.-Z. Huang, J. Chanussot, and\nG. Vivone, “A triple-double convolutional neural network\nfor panchromatic sharpening,” IEEE Transactions on\nNeural Networks and Learning Systems, IEEE, 2022.\n[37] J. Ma, W. Yu, C. Chen, P. Liang, X. Guo, and J. Jiang,\n“Pan-gan: An unsupervised pan-sharpening method for\nremote sensing image fusion,” Information Fusion 62,\npp. 110–120, Elsevier, 2020.\n[38] H. Zhou, Q. Liu, and Y . Wang, “Pgman: An unsupervised\ngenerative multiadversarial network for pansharpening,”\nIEEE Journal of Selected Topics in Applied Earth Obser-\nvations and Remote Sensing14, pp. 6316–6327, 2021.\n[39] T. Uezato, D. Hong, N. Yokoya, and W. He, “Guided\ndeep decoder: Unsupervised image pair fusion,” in Eu-\nropean Conference on Computer Vision, pp. 87–102,\nSpringer, 2020.\n[40] J. Fu, J. Liu, H. Tian, Y . Li, Y . Bao, Z. Fang, and\nH. Lu, “Dual attention network for scene segmentation,”\nin Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 3146–3154, 2019.\n[41] S. Zagoruyko and N. Komodakis, “Paying more attention\nto attention: Improving the performance of convolutional\nneural networks via attention transfer,” arXiv preprint\narXiv:1612.03928 , 2016.\n[42] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville,\nR. Salakhudinov, R. Zemel, and Y . Bengio, “Show,\nattend and tell: Neural image caption generation with\nvisual attention,” in International conference on machine\nlearning, pp. 2048–2057, PMLR, 2015.\n[43] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation\nnetworks,” pp. 7132–7141, 2018.\n[44] X. Li, W. Wang, X. Hu, and J. Yang, “Selective kernel\nnetworks,” in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 510–519,\n2019.\n[45] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, “Eca-\nnet: Efficient channel attention for deep convolutional\nneural networks,” pp. 11534–11542, 2020.\n[46] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial\ntransformer networks,” Advances in neural information\nprocessing systems 28, 2015.\n[47] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam:\nConvolutional block attention module,” pp. 3–19, 2018.\n[48] J. Park, S. Woo, J.-Y . Lee, and I. S. Kweon,\n“Bam: Bottleneck attention module,” arXiv preprint\narXiv:1807.06514 , 2018.\n[49] D. Hong, Z. Han, J. Yao, L. Gao, B. Zhang, A. Plaza, and\nJ. Chanussot, “Spectralformer: Rethinking hyperspectral\nimage classification with transformers,” IEEE Transac-\ntions on Geoscience and Remote Sensing60, pp. 1–15,\n2021.\n[50] X. He, Y . Chen, and Z. Lin, “Spatial-spectral trans-\nformer for hyperspectral image classification,” Remote\nSensing 13(3), p. 498, 2021.\n[51] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang,\nS. Lin, and B. Guo, “Swin transformer: Hierarchical\nvision transformer using shifted windows,” pp. 10012–\n10022, 2021.\n[52] F. Zhang, K. Zhang, and J. Sun, “Multiscale spatial–\nspectral interaction transformer for pan-sharpening,” Re-\nmote Sensing 14(7), p. 1736, 2022.\n[53] M. Li, Y . Liu, T. Xiao, Y . Huang, and G. Yang, “Local-\nglobal transformer enhanced unfolding network for pan-\nsharpening,” arXiv preprint arXiv:2304.14612, 2023.\n[54] R. Girshick, “Fast r-cnn,” pp. 1440–1448, 2015.\n[55] S. Niklaus and F. Liu, “Context-aware synthesis for video\nframe interpolation,” pp. 1701–1710, 2018.\n[56] Z. Wang and A. C. Bovik, “A universal image quality\nindex,” IEEE signal processing letters9(3), pp. 81–84,\n2002.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n20\n[57] J. Pushparaj and A. V . Hegde, “Evaluation of pan-\nsharpening methods for spatial and spectral quality,”\nApplied Geomatics 9, pp. 1–12, 2017.\n[58] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simon-\ncelli, “Image quality assessment: from error visibility\nto structural similarity,” IEEE transactions on image\nprocessing 13(4), pp. 600–612, 2004.\nZixu Li receive his bachelor’s degree from the\nSchool of Business, Jiangxi University of Science\nand Technology, Nanchang, China in 2022.Currently\nstudying for a master’s degree in the School of Infor-\nmation and Electronic Engineering, Shandong Tech-\nnology and Business University, Yantai, Shandong.\nHis research interests include computer graphics,\ncomputer vision, and image progressing.\nJinjiang Li received the B.S. and M.S. degrees\nin computer science from Taiyuan University of\nTechnology, Taiyuan, China, in 2001 and 2004, re-\nspectively,the Ph.D.degree in computer science from\nShandong University, Jinan, China, in 2010. From\n2004 to 2006, he was an assistant research fellow\nat the institute of computer science and technology\nof Peking University, Beijing, China. From 2012\nto 2014, he was a Post-Doctoral Fellow at Ts-\ninghua University, Beijing, China. He is currently\na Professor at the school of computer science and\ntechnology, Shandong Technology and Business University. His research\ninterests include image processing, computer graphics, computer vision and\nmachine learning.\nLu Ren received the Ph.D. degree in Signal and\nInformation Processing from Dalian University of\nTechnology, Dalian, China. She is currently a lec-\nturer at Shandong Technology and Business Univer-\nsity. Her current research interests include sentiment\nanalysis and text mining.\nZheng Chen received the B.S. and M.S. degrees\nfrom Shandong Agricultural University and Shan-\ndong Normal University, respectively. He received\nthe Ph.D. degree in Signal and Information Process-\ning from Dalian University of Technology, Dalian,\nChina, in 2022. He is currently a lecturer at Shan-\ndong Technology and Business University. His re-\nsearch interests include computer vision, hand pose\nestimation and hand shape recovery.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3332459\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}