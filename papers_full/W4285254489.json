{
    "title": "Memorisation versus Generalisation in Pre-trained Language Models",
    "url": "https://openalex.org/W4285254489",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2031209622",
            "name": "Michael Tanzer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2140581490",
            "name": "Sebastian Ruder",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2044895103",
            "name": "Marek Rei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2566079294",
        "https://openalex.org/W2601450892",
        "https://openalex.org/W3039366696",
        "https://openalex.org/W2963096987",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3095992020",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2971039193",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W2970119519",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2760505947",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2573492843",
        "https://openalex.org/W3034408878",
        "https://openalex.org/W2951013084",
        "https://openalex.org/W2047782770",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3013741514",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2925181975"
    ],
    "abstract": "State-of-the-art pre-trained language models have been shown to memorise facts and per- form well with limited amounts of training data. To gain a better understanding of how these models learn, we study their generali- sation and memorisation capabilities in noisy and low-resource scenarios. We find that the training of these models is almost unaffected by label noise and that it is possible to reach near-optimal results even on extremely noisy datasets. However, our experiments also show that they mainly learn from high-frequency patterns and largely fail when tested on low- resource tasks such as few-shot learning and rare entity recognition. To mitigate such lim- itations, we propose an extension based on prototypical networks that improves perfor- mance in low-resource named entity recogni- tion tasks.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 7564 - 7578\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nMemorisation versus Generalisation in Pre-trained Language Models\nMichael Tänzer\nImperial College London\nm.tanzer@imperial.ac.uk\nSebastian Ruder∗\nGoogle Research\nruder@google.com\nMarek Rei\nImperial College London\nmarek.rei@imperial.ac.uk\nAbstract\nState-of-the-art pre-trained language models\nhave been shown to memorise facts and per-\nform well with limited amounts of training\ndata. To gain a better understanding of how\nthese models learn, we study their generali-\nsation and memorisation capabilities in noisy\nand low-resource scenarios. We ﬁnd that the\ntraining of these models is almost unaffected\nby label noise and that it is possible to reach\nnear-optimal results even on extremely noisy\ndatasets. However, our experiments also show\nthat they mainly learn from high-frequency\npatterns and largely fail when tested on low-\nresource tasks such as few-shot learning and\nrare entity recognition. To mitigate such lim-\nitations, we propose an extension based on\nprototypical networks that improves perfor-\nmance in low-resource named entity recogni-\ntion tasks.\n1 Introduction\nWith recent advances in pre-trained language mod-\nels (Peters et al., 2018; Devlin et al., 2019; Liu\net al., 2019; He et al., 2020), the ﬁeld of natural lan-\nguage processing has seen improvements in a wide\nrange of tasks and applications. Having acquired\ngeneral-purpose knowledge from large amounts of\nunlabelled data, such methods have been shown\nto learn effectively with limited labelled data for\ndownstream tasks (Howard and Ruder, 2018) and\nto generalise well to out-of-distribution examples\n(Hendrycks et al., 2020).\nPrevious work has extensively studiedwhat such\nmodels learn, e.g. the types of relational or linguis-\ntic knowledge (Tenney et al., 2019; Jawahar et al.,\n2019; Rogers et al., 2020). However, the process\nof how these models learn from downstream data\nand the qualitative nature of their learning dynam-\nics remain unclear. Better understanding of the\nlearning processes in these widely-used models is\n∗Work done prior to joining Google.\nneeded in order to know in which scenarios they\nwill fail and how to improve them towards more\nrobust language representations.\nThe ﬁne-tuning process in pre-trained language\nmodels such as BERT (Devlin et al., 2019) aims to\nstrike a balance between generalisation and memo-\nrisation. For many applications it is important for\nthe model to generalise—to learn the common pat-\nterns in the task while discarding irrelevant noise\nand outliers. However, rejecting everything that oc-\ncurs infrequently is not a reliable learning strategy\nand in many low-resource scenarios memorisation\ncan be crucial to performing well on a task (Tu\net al., 2020). By constructing experiments that\nallow for full control over these parameters, we\nare able to study the learning dynamics of mod-\nels in conditions of high label noise or low label\nfrequency. To our knowledge, this is the ﬁrst quali-\ntative study of the learning behaviour of pre-trained\ntransformer-based language models in conditions\nof extreme label scarcity and label noise.\nWe ﬁnd that models such as BERT are particu-\nlarly good at learning general-purpose patterns as\ngeneralisation and memorisation become separated\ninto distinct phases during their ﬁne-tuning. We\nalso observe that the main learning phase is fol-\nlowed by a distinct performance plateau for several\nepochs before the model starts to memorise the\nnoise. This makes the models more robust with\nregard to the number of training epochs and allows\nfor noisy examples in the data to be identiﬁed based\nonly on their training loss.\nHowever, we ﬁnd that these excellent generali-\nsation properties come at the cost of poor perfor-\nmance in few-shot scenarios with extreme class\nimbalances. Our experiments show that BERT is\nnot able to learn from individual examples and may\nnever predict a particular label until the number of\ntraining instances passes a critical threshold. For\nexample, on the CoNLL03 (Sang and De Meulder,\n2003) dataset it requires 25 instances of a class to\n7564\nlearn to predict it at all and 100 examples to predict\nit with some accuracy. To address this limitation,\nwe propose a method based on prototypical net-\nworks (Snell et al., 2017) that augments BERT with\na layer that classiﬁes test examples by ﬁnding their\nclosest class centroid. The method considerably\noutperforms BERT in challenging training condi-\ntions with label imbalances, such as the WNUT17\n(Derczynski et al., 2017) rare entities dataset.\nOur contributions are the following: 1) We iden-\ntify a second phase of learning where BERT does\nnot overﬁt to noisy datasets. 2) We present experi-\nmental evidence that BERT is particularly robust to\nlabel noise and can reach near-optimal performance\neven with extremely strong label noise. 3) We study\nforgetting in BERT and verify that it is dramatically\nless forgetful than some alternative methods. 4) We\nempirically observe that BERT completely fails to\nrecognise minority classes when the number of ex-\namples is limited and we propose a new model,\nProtoBERT, which outperforms BERT on few-shot\nversions of CoNLL03 and JNLPBA, as well as on\nthe WNUT17 dataset.\n2 Previous work\nSeveral studies have been conducted on neural mod-\nels’ ability to memorise and recall facts seen during\ntheir training. Petroni et al. (2019) showed that\npre-trained language models are surprisingly effec-\ntive at recalling facts while Carlini et al. (2019)\ndemonstrated that LSTM language models are able\nto consistently memorise single out-of-distribution\n(OOD) examples during the very ﬁrst phase of train-\ning and that it is possible to retrieve such examples\nat test time. Liu et al. (2020) found that regular-\nising early phases of training is crucial to prevent\nthe studied CNN residual models from memoris-\ning noisy examples later on. They also propose a\nregularisation procedure useful in this setting. Sim-\nilarly, Li et al. (2020) analyse how early stopping\nand gradient descent affect model robustness to\nlabel noise.\nToneva et al. (2019), on the other hand, study\nforgetting in visual models. They ﬁnd that mod-\nels consistently forget a signiﬁcant portion of the\ntraining data and that this fraction of forgettable ex-\namples is mainly dependent on intrinsic properties\nof the training data rather than the speciﬁc model.\nIn contrast, we show that a pretrained BERT forgets\nexamples at a dramatically lower rate compared to\na BiLSTM and a non-pretrained variant.\nMemorisation is closely related to generalisation:\nneural networks have been observed to learn simple\npatterns before noise (Arpit et al., 2017) and gen-\neralise despite being able to completely memorise\nrandom examples (Zhang et al., 2017). Zhang et al.\n(2021) also show that our current understanding of\nstatistical learning theory cannot explain the super-\nhuman generalisation performance of large neural\nmodels across many areas of study.\nHendrycks et al. (2020) show that pre-trained\nmodels generalise better on out-of-distribution data\nand are better able to detect such data compared\nto non-pretrained methods but that they still do\nnot cleanly separate in- and out-of-distribution ex-\namples. Kumar et al. (2020) ﬁnd that pre-trained\nmethods such as BERT are sensitive to spelling\nnoise and typos. In contrast to noise in the input,\nwe focus on the models’ learning dynamics in the\npresence of label noise and ﬁnd that pre-trained\nmethods are remarkably resilient to such cases.\n3 Experimental setting\nWe investigate the performance of pre-trained lan-\nguage models in speciﬁc adverse conditions. In\norder to evaluate generalisation abilities, we ﬁrst\ncreate datasets with varying levels of label noise\nby randomly permuting some of the labels in the\ntraining data. This procedure allows us to pinpoint\nnoisy examples and evaluate the performance on\nclean and noisy datapoints separately. Then, in\norder to investigate memorisation we train the mod-\nels on datasets that contain only a small number of\nexamples for a particular class. This allows us to\nevaluate how well the models are able to learn from\nindividual datapoints as opposed to high-frequency\npatterns. We make the code for the experiments\navailable online.1\nDatasets We focus on the task of named en-\ntity recognition (NER) and employ the CoNLL03\n(Sang and De Meulder, 2003), the JNLPBA (Col-\nlier and Kim, 2004), and the WNUT17 (Derczynski\net al., 2017) datasets. NER is commonly used for\nevaluating pre-trained language models on struc-\ntured prediction and its natural class imbalance is\nwell suited for our probing experiments.CoNLL03\nand JNLPBA are standard datasets for NER and\nBio-NER respectively. The WNUT17 dataset is\nmotivated by the observation that state-of-the-art\nmethods tend to memorise entities during training\n1https://github.com/Michael-Tanzer/\nBERT-mem-lowres\n7565\n(Augenstein et al., 2017). The dataset focuses on\nidentifying unusual or rare entities at test time that\ncannot be simply memorised by the model. We\nevaluate based on entity-level F1 unless stated oth-\nerwise.\nLanguage models We use BERT-base (Devlin\net al., 2019) as the main language model for our\nexperiments, as BERT is widely used in practice\nand other variations of pre-trained language mod-\nels build on a similar architecture. The model is\naugmented with a classiﬁcation feed-forward layer\nand ﬁne-tuned using the cross-entropy loss with a\nlearning rate of 10−4. AdamW (Loshchilov and\nHutter, 2019) is used during training with weight\ndecay of 0.01 and a linear warm-up rate of 10%.\nThe test results are recorded using the model that\nproduced the highest validation metrics.\nWe compare BERT’s behaviour with that of\nother pre-trained transformers such as RoBERTa\n(Liu et al., 2019) and DeBERTa (He et al., 2020)\nﬁne-tuned with the same optimiser and hyper-\nparameters as above. In order to also compare\nagainst non-transformer models, we report perfor-\nmance for a bi-LSTM-CRF (Lample et al., 2016)\nmodel with combined character-level and word-\nlevel representations. The model is comprised of\n10 layers, with 300-dimensional word representa-\ntions and 50-dimensional character representations,\nfor a total of approximately 30 million trainable pa-\nrameters. In our experiments, the model is trained\nwith the Adam optimiser (Kingma and Ba, 2014)\nand a learning rate of 10−4 for 100 epochs using a\nCRF loss (Lafferty et al., 2001).\n4 Generalisation in noisy settings\nWe ﬁrst investigate how BERT learns general pat-\nterns from datasets that contain label noise. Fig-\nure 1 shows how the model performance on the\nCoNLL03 training and validation sets changes\nwhen faced with varying levels of noise, from 0%\nto 50%. Based on the progression of performance\nscores, we can divide BERT’s learning process into\nroughly three distinct phases:\n1. Fitting: The model uses the training data to\nlearn how to generalise, effectively learning sim-\nple patterns that can explain as much of the train-\ning data as possible (Arpit et al., 2017). Both\nthe training and validation performance rapidly\nincrease as the model learns these patterns.\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Classification F1 score\nvalidation\ntraining\nphase 1\nphase 2\nphase 3\nFigure 1: BERT performance (F1) throughout the train-\ning process on the CoNLL03 train and validation sets.\nDarker colours correspond to higher levels of noise (0%\nto 50%).\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Classification accuracy\nrandom classifier accuracy\nBERT\nphase 1\nphase 2\nphase 3\nFigure 2: Classiﬁcation accuracy of noisy examples\nin the training set for the CoNLL03 dataset. Darker\ncolours correspond to higher levels of noise (0% to\n50%).\n2. Settling: The increase in performance plateaus\nand neither the validation nor the training per-\nformance change considerably. The duration of\nthis phase seems to be inversely proportional to\nthe amount of noise present in the dataset.\n3. Memorisation: The model rapidly starts to\nmemorise the noisy examples, quickly improv-\ning the performance on training data while de-\ngrading the validation performance, effectively\nover-ﬁtting to the noise in the dataset.\nA second phase of learningWe ﬁnd BERT to\nexhibit a distinct second settling phase during\nwhich it does not over-ﬁt. A resilience to label\nnoise has been observed in other neural networks\ntrained with gradient descent (Li et al., 2020). How-\never, we ﬁnd this phase to be much more prolonged\nin BERT compared to models pre-trained on other\n7566\nmodalities such as a pre-trained ResNet ﬁne-tuned\non CIFAR10, which immediately starts memoris-\ning noisy examples (see Appendix A for a compar-\nison). These results indicate that the precise point\nof early stopping is not as important when it comes\nto ﬁne-tuning pre-trained language models. Similar\noptimal performance is retained for a substantial\nperiod, therefore training for a ﬁxed number of\nepochs can be sufﬁcient.\nWe illustrate BERT’s behaviour by evaluating\nthe token-level classiﬁcation accuracy of noisy ex-\namples in Figure 2. During the second phase,\nBERT completely ignores the noisy tokens and cor-\nrectly misclassiﬁes them, performing “worse” than\na random classiﬁer. The step-like improvements\nduring the third stage show that the model is unable\nto learn any patterns from the noise and improves\nby repeatedly optimising on the same examples,\ngradually memorising them.\nRobustness to noise We also observe in Figure\n1 that BERT is extremely robust to noise and over-\nﬁtting in general. In the absence of noise, the model\ndoes not over-ﬁt and maintains its development set\nperformance, regardless of the length of training.\nEven with a large proportion of noise, model perfor-\nmance comparable to training on the clean dataset\ncan be achieved by stopping the training process\nsomewhere in the second phase.2\nWe also hypothesise that due to the robustness\nto noise shown in the second phase of training,\na noise detector can be constructed based only on\nBERT’s training losses, without requiring any other\ninformation. We ﬁnd that a simple detector that\nclusters the losses using k-means reliably achieves\nover 90% noise-detection F 1 score in all our ex-\nperiments, further showing how the model is able\nto actively detect and reject single noisy examples\n(see Appendix E for details about the noise detec-\ntion process).\nImpact of pre-training The above properties\ncan mostly be attributed to BERT’s pre-training\nprocess—after large-scale optimisation as a lan-\nguage model, the network is primed for learning\ngeneral patterns and better able to ignore individual\nnoisy examples. We ﬁnd that a randomly initialised\nmodel with the same architecture does not only\nachieve lower overall performance but crucially\ndoes not exhibit’s BERT’s distinct second phase of\n2Adding 30% noise to the CoNLL03 dataset causes only a\n0.9% decrease of validation performance in the second phase.\nlearning and robustness to noise (see Appendix C).\nOther pre-trained transformers We also anal-\nyse the behaviour of other pre-trained transformers\nfor comparison. Speciﬁcally, studying RoBERTa\nand DeBERTa, we ﬁnd the same training pattern\nthat was observed in BERT—all models show a\nclear division into the three phases described above.\nThese models are also all very robust to label noise\nduring the settling phase of training. Notably,\nRoBERTa is even more resilient to label noise com-\npared to the other two analysed models, despite\nDeBERTa outperforming it on public benchmarks\n(He et al., 2020). Training and validation perfor-\nmance visualisations, such as those in Figure 1, can\nbe found for both models in Appendix I.\n5 Forgetting of learned information\nEvaluating only the ﬁnal model does not always\nprovide the full picture regarding datapoint mem-\norisation, as individual datapoints can be learned\nand forgotten multiple times during the training\nprocess. Following Toneva et al. (2019), we record\na forgetting event for an example at epoch tif the\nmodel was able to classify it correctly at epoch\nt−1, but not at epoch t. Similarly, we identify\na learning event for an example at epoch tif the\nmodel was not able to classify it correctly at epoch\nt−1, but it is able to do so at epoch t. A ﬁrst\nlearning event thus happens at the ﬁrst epoch when\na model is able to classify an example correctly.\nWe furthermore refer to examples with zero and\nmore than zero forgetting events as unforgettable\nand forgettable examples, respectively, while the\nset of learned examples includes all examples with\none or more learning events.\nIn Table 1, we show the number of forgettable,\nunforgettable, and learned examples on the training\ndata of the CoNLL03 and JNLPBA datasets for\nBERT, a non-pre-trained BERT, and a bi-LSTM\nmodel. We also show the ratio between forgettable\nand learned examples, which indicates how easily a\nmodel forgets learned information. We can observe\nthat BERT forgets less than other models and that\npre-training is crucial for retaining important infor-\nmation. We show the most forgettable examples in\nAppendix D, which tend to be atypical examples\nof the corresponding class.\nToneva et al. (2019) found that the number of\nforgetting events remains comparable across dif-\nferent architectures for the vision modality, given\n7567\nDataset Model Forgettable Nf Unforgettable Nu Learned Nl Nf /Nl (%)\nCoNNL03\nbi-LSTM 71.06% 29.94% 90.90% 78.17%\nnon-pre-trained BERT 9.89% 90.11% 99.87% 9.90%\npre-trained BERT 2.97% 97.03% 99.80% 2.98%\nJNLPBA\nbi-LSTM 97.16% 5.14% 98.33% 98.81%\nnon-pre-trained BERT 25.50% 74.50% 98.24% 25.96%\npre-trained BERT 16.62% 83.38% 98.18% 16.93%\nTable 1: Number of forgettable, unforgettable, and learned examples during BERT training on the CoNLL03\ndataset and JNLPBA dataset.\n1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8Ratio of training examples\nBERT\nphase 1\nphase 2\nphase 3\nFigure 3: First learning events distribution during the\ntraining for various levels of noise on the CoNLL03\ndataset. Darker colours correspond to higher levels of\nnoise (0% to 50%).\na particular dataset. 3 However, our experiments\nshow that the same does not necessarily hold for\npre-trained language models. Speciﬁcally, there\nis a large discrepancy in the ratio between forget-\ntable and learned examples for BERT (∼3%) and a\nbi-LSTM model (∼80%) on the CoNLL03 dataset.\nWe additionally analyse the distribution of ﬁrst\nlearning events throughout BERT’s training on\nCoNLL03 with label noise between 0% and 50%\n(Figure 3) and notice how BERT learns the majority\nof learned examples during the ﬁrst epochs of train-\ning. As the training progresses, we see that BERT\nstops learning new examples entirely, regardless of\nthe level of noise for the third and fourth epochs.\nFinally, in the last epochs BERT mostly memorises\nthe noise in the data.4\n3They report proportions of forgettable examples for\nMNIST, PermutedMNIST, CIFAR10, and CIFAR100 as 8.3%,\n24.7%, 68.7%, and 92.38% respectively.\n4We conducted additional experiments on other datasets\n(see Appendix F for results on the JNLPBA dataset). In all\ncases we observe the same distribution of ﬁrst learning events\nthroughout training.\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Classification F1 score\nValidation\nTraining\nphase 1\nphase 2\nphase 3\nFigure 4: BERT performance (F1) throughout the train-\ning process on theCoNLL03 dataset with varying num-\nber of sentences containing the LOC class. Darker\ncolours correspond to fewer examples of the LOC class\navailable (5 to 95 in steps of 20).\n6 BERT in low-resource scenarios\nIn the previous sections, we have observed that\nBERT learns examples and generalises very early\nin training. We will now examine if the same be-\nhaviour applies in low-resource scenarios where\na minority class is only observed very few times.\nTo this end, we remove from the CoNLL03 train-\ning set all sentences containing tokens with the\nminority labels MISC and LOC except for a prede-\ntermined number of such sentences. We repeat the\nprocess for the JNLPBA dataset with the DNA and\nProtein labels.\nWe conduct similar experiments to the previous\nsections by studying how different numbers of sen-\ntences containing the target class affect BERT’s\nability to learn and generalise. We report in Figure\n4 the training and validation classiﬁcation F1 score\nfor the CoNLL03 datasets from which all but few\n(5 to 95) sentences containing the LOC label were\nremoved. Note that the reported performance in\nthis experiment refers to the LOC class only. In Fig-\nure 5 we also report the distribution of ﬁrst learning\n7568\n1 2 3 4 5 6 7 8 9 10\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Ratio of training examples\nBERT\nphase 1\nphase 2\nphase 3\nFigure 5: First learning events distribution during the\ntraining on the CoNLL03 dataset with varying number\nof sentences containing the LOC class. Darker colours\ncorrespond to fewer examples of the LOC class avail-\nable (5 to 95 in steps of 20).\nevents for the LOC class in the same setting. Two\nphenomena can be observed: 1) reducing the num-\nber of sentences greatly reduces the model’s ability\nto generalise (validation performance decreases yet\ntraining performance remains comparable); and 2)\nwhen fewer sentences are available, they tend to\nbe learned in earlier epochs for the ﬁrst time. Cor-\nresponding experiments on the MISC label can be\nfound in Appendix J.\nWe also show the average entity-level F1 score\non tokens belonging to the minority label and\nthe model performance for the full NER task (i.e.\nconsidering all classes) for the CoNLL03 and\nJNLPBA datasets in Figures 6 and 7 respectively.\nFor the CoNLL03 dataset, we observe that BERT\nneeds at least 25 examples of a minority label in\norder to be able to start learning it. Performance\nrapidly improves from there and plateaus at around\n100 examples. For the JNLPBA dataset, the mini-\nmum number of examples increases to almost 50\nand the plateau occurs for a higher number of exam-\nples. On the challenging WNUT17 dataset, BERT\nachieves only 44% entity-level F1. This low per-\nformance is attributable to the absence of entity\noverlap between training set and test set, which in-\ncreases the inter-class variability of the examples.\n7 ProtoBERT for few-shot learning\nIn order to address BERT’s limitations in few-shot\nlearning, we propose a new model, ProtoBERT\nthat combines BERT’s pre-trained knowledge with\nthe few-shot capabilities of prototypical networks\n(Snell et al., 2017) for sequence labelling problems.\n0\n 25\n 50\n 75\n 100\n 125\n 150\n 175\n 200\nNumber of training sentences containing the target label\n0.0\n0.2\n0.4\n0.6\n0.8Test F1 score\nMISC - Few-shot\nMISC - Full\nLOC - Few-shot\nLOC - Full\nFigure 6: BERT ﬁnal validation entity-level F 1 score\non the few-shot class keeping varying numbers of sen-\ntences containing examples of a selected class on the\nCoNLL03 dataset.\n0\n 25\n 50\n 75\n 100\n 125\n 150\n 175\n 200\nNumber of training sentences containing the target label\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Test F1 score\nDNA - Few-shot\nDNA - Full\nProtein - Few-shot\nProtein - Full\nFigure 7: BERT ﬁnal validation entity-level F 1 score\non the few-shot class keeping varying numbers of sen-\ntences containing examples of a selected class on the\nJNLPBA dataset.\nThe method builds an embedding space where the\ninputs are clustered on a per-class basis, allowing\nus to classify a token by ﬁnding its closest cen-\ntroid and assigning it the corresponding class. The\nmodel can be seen in Figure 8.\nWe ﬁrst deﬁne a support set S, which we use\nas context for the classiﬁcation and designate with\nSk all elements of S that have label k. We refer\nto the set of points that we want to classify as the\nquery set Q, with l(Qi) indicating the label of the\nith element in Q. We will also refer to f as the\nfunction computed by BERT augmented with a\nlinear layer, which produces an M dimensional\noutput.\nThe model then classiﬁes a given input x as\nfollows: for each class k, we compute the centroid\nof the class in the learned feature space as the mean\nof all the elements that belong to class k in the\n7569\nTraining\ndata\nQuery set\nSupport\nset\nQuery\nembeddings\nSupport\nembeddings\nClass\ncentroids\nSimilarities\nOutput probabilities\nSimilarity\nPer-class\naverage\nS\nSoftmax\nRandom\nsampler BERT\nMapping layer\nFigure 8: Schematic representation of the inference using a BERT model with a prototypical network layer.\nsupport set S:\nck = 1\n|Sk|\n∑\nxi∈Sk\nf(xi) (1)\nThen, we compute the distance from each input\nx ∈Qto each centroid:\ndistk = d(f(x),ck)\nand collect them in a vector v ∈Rk. Finally, we\ncompute the probability of x belonging to class k\nas\np(y= k|x) = exp (−d(f(x),ck))∑\nk′exp (−d(f(x),ck′)) =\n= softmax(−v)k\nThe model is trained by optimising the cross-\nentropy loss between the above probability and the\none-hot ground-truth label of x. Crucially, Sand\nQare not a ﬁxed partition of the training set but\nchange at each training step. Following Snell et al.\n(2017), we use Euclidean distance as a choice for\nthe function d.\nIn order to take into account the extreme under-\nrepresentation of some classes, we create the sup-\nport by sampling s1 elements from each minority\nclass and s2 elements from each non-minority class.\nA high ratio s1/s2 gives priority to the minority\nclasses, while a low ratio puts more emphasis on\nthe other classes. We then similarly construct the\nquery set with a ﬁxed ratio nbetween the minority\nclasses and the non-minority classes.\nFor NER, rather than learning a common repre-\nsentation for the negative class “O”, we only want\nthe model to treat it as a fallback when no other\nsimilar class can be found. For this reason, we\ndeﬁne the vector of distances vas follows:\nv= (dO, dist0, ..., distk)\nwhere dO is a scalar parameter of the network that\nis trained along with the other parameters. Intu-\nitively, we want to classify a point as anon-entity\n(i.e. class O) when it is not close enough to any cen-\ntroid, where dO represents the threshold for which\nwe consider a point “close enough”.\nIf no example of a certain class is available in\nthe support set during the training, we assign a dis-\ntance of 400, making it effectively impossible to\nmistakenly classify the input as the missing class\nduring that particular batch. Finally, we propose\ntwo ways to compute the class of a token at test\ntime. The ﬁrst method employs all examples from\nX to calculate the centroids needed at test time,\nwhich produces better results but is computation-\nally expensive for larger datasets.\nThe second method approximates the centroidck\nusing the moving average of the centroids produced\nat each training step:\nc(t)\nk ←αc(t)\nk ·(1 −α) c(t−1)\nk\nwhere αis a weighting factor. This method results\nin little overhead during training and only performs\nmarginally worse than the ﬁrst method.\n7.1 Experimental results\nWe ﬁrst compare ProtoBERT to the standard pre-\ntrained BERT model with a classiﬁcation layer on\nthe CoNLL03 and JNLPBA datasets with a smaller\nnumber of sentences belonging to the minority\nclasses. We show the results on the few-shot classes\nand for the full dataset for CoNLL03 in Figures 9\nand 10 respectively. Similarly, we show the re-\nsults for the few-shot class for JNLPBA in Figure\n11.5 In all cases ProtoBERT consistently surpasses\nthe performance of the baseline when training on\nfew examples of the minority class. It particularly\nexcels in the extreme few-shot setting, e.g. out-\nperforming BERT by 40 F 1 points with 15 sen-\ntences containing the LOC class. As the number of\navailable examples of the minority class increases,\n5A comparison on the full classiﬁcation task can be found\nin Appendix H.\n7570\n0\n 25\n 50\n 75\n 100\n 125\n 150\n 175\n 200\nNumber of training sentences containing the target label\n0.0\n0.2\n0.4\n0.6\n0.8Test F1 score\nMISC - ProtoBERT\nMISC - BERT + Class. Layer\nLOC - ProtoBERT\nLOC - BERT + Class. Layer\nFigure 9: Model performance comparison between\nthe baseline model and ProtoBERT for the CoNLL03\ndataset, reducing the sentences containing the MISC\nand LOC classes. Results reported as F 1 score on the\nfew-shot classes.\n0\n 25\n 50\n 75\n 100\n 125\n 150\n 175\n 200\nNumber of training sentences containing the target label\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Test few-shot F1 score\nMISC - ProtoBERT\nMISC - BERT + Class. Layer\nLOC - ProtoBERT\nLOC - BERT + Class. Layer\nFigure 10: Model performance comparison between\nthe baseline model and ProtoBERT for the CoNLL03\ndataset, reducing the sentences containing the MISC\nand LOC class. Results reported as F 1 score on all\nclasses.\nBERT starts to match ProtoBERT’s performance\nand outperforms it on the full dataset in some cases.\nWhile the main strength of ProtoBERT is on\nfew-shot learning, we evaluate it also on the full\nCoNLL03, JNLPBA and WNUT17 datasets (with-\nout removing any sentences) in Table 2. In this\nsetting, the proposed architecture achieves results\nmostly similar to the baseline while considerably\noutperforming it on the WNUT17 dataset of rare\nentities.\nThe results in this section show that ProtoBERT,\nwhile designed for few-shot learning, performs at\nleast on par with its base model in all tasks. This\nallows the proposed model to be applied to a much\nwider range of tasks and datasets without negatively\naffecting the performance if no label imbalance is\n0\n 25\n 50\n 75\n 100\n 125\n 150\n 175\n 200\nNumber of training sentences containing the target label\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Test F1 score\nDNA - ProtoBERT\nDNA - BERT + Class. Layer\nProtein - ProtoBERT\nProtein - BERT + Class. Layer\nFigure 11: Model performance comparison between\nthe baseline model and ProtoBERT for the JNLPBA\ndataset, reducing the sentences containing the DNA and\nProtein classes. Results reported as F 1 score on the\nfew-shot classes.\npresent, while bringing a substantial improvement\nin few-shot scenarios.\nWe conduct an ablation study to verify the ef-\nfect of our improved centroid computation method.\nFrom the results in Table 2 we can afﬁrm that, while\na difference in performance does exist, it is quite\nmodest (0.1–0.4%). On the other hand, this method\nreduces the training time and therefore energy con-\nsumption (Strubell et al., 2019) to one third of the\noriginal method on CoNLL03 and we expect the\nreduction to be even greater for larger datasets.\n8 Conclusion\nIn this study, we investigated the learning process\nduring ﬁne-tuning of pre-trained language models,\nfocusing on generalisation and memorisation. By\nformulating experiments that allow for full control\nover the label distribution in the training data, we\nstudy the learning dynamics of the models in con-\nditions of high label noise and low label frequency.\nThe experiments show that BERT is capable of\nreaching near-optimal performance even when a\nlarge proportion of the training set labels has been\ncorrupted. We ﬁnd that this ability is due to the\nmodel’s tendency to separate the training into three\ndistinct phases: ﬁtting, settling, and memorisation,\nwhich allows the model to ignore noisy examples\nin the earlier epochs. The pretrained models expe-\nrience a prolonged settling phase when ﬁne-tuned,\nduring which their performance remains optimal,\nindicating that the precise area of early stopping is\nless crucial.\nFurthermore, we show that the number of avail-\n7571\nModel CoNLL03 JNLPBA WNUT17\nState of the art 93.50 77.59 50.03\nBERT + classiﬁcation layer (baseline) 89.35 75.36 44.09\nProtoBERT 89.87 73.91 48.62\nProtoBERT + running centroids 89.46 73.54 48.56\nTable 2: Comparison between the baseline model, the current state-of-the-art6and the proposed architecture on the\nCoNLL03, JNLPBA and WNUT17 datasets evaluated using entity-level F 1 score. The state of the art is Baevski\net al. (2019), Lee et al. (2019), and Wang et al. (2019) respectively.\nable examples greatly affects the learning process,\ninﬂuencing both when the examples are memorised\nand the quality of the generalisation. We show\nthat BERT fails to learn from examples in extreme\nfew-shot settings, completely ignoring the minority\nclass at test time. To overcome this limitation, we\naugment BERT with a prototypical network. This\napproach partially solves the model’s limitations\nby enabling it to perform well in extremely low-\nresource scenarios and also achieves comparable\nperformance in higher-resource settings.\nAcknowledgements\nMichael is funded by the UKRI CDT in AI for\nHealthcare7 (Grant No. P/S023283/1).\nReferences\nDevansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Bal-\nlas, David Krueger, Emmanuel Bengio, Maxin-\nder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron\nCourville, Yoshua Bengio, and Simon Lacoste-\nJulien. 2017. A Closer Look at Memorization\nin Deep Networks. arXiv:1706.05394 [cs, stat] .\nArXiv: 1706.05394.\nIsabelle Augenstein, Leon Derczynski, and Kalina\nBontcheva. 2017. Generalisation in Named\nEntity Recognition: A Quantitative Analysis.\narXiv:1701.02877 [cs]. ArXiv: 1701.02877.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5360–5369, Hong\nKong, China. Association for Computational Lin-\nguistics.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The Secret Sharer:\nEvaluating and Testing Unintended Memorization in\nNeural Networks. arXiv:1802.08232 [cs]. ArXiv:\n1802.08232.\n7http://ai4health.io\nNigel Collier and Jin-Dong Kim. 2004. Introduc-\ntion to the Bio-entity Recognition Task at JNLPBA.\nIn Proceedings of the International Joint Workshop\non Natural Language Processing in Biomedicine\nand its Applications (NLPBA/BioNLP), pages 73–78,\nGeneva, Switzerland. COLING.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition , pages\n248–255. IEEE.\nLeon Derczynski, Eric Nichols, Marieke van Erp, and\nNut Limsopatham. 2017. Results of the WNUT2017\nShared Task on Novel and Emerging Entity Recogni-\ntion. In Proceedings of the 3rd Workshop on Noisy\nUser-generated Text, pages 140–147, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs] . ArXiv:\n1810.04805.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. 2015. Deep Residual Learning for Im-\nage Recognition. arXiv:1512.03385 [cs] . ArXiv:\n1512.03385.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. DeBERTa: Decoding-\nenhanced BERT with Disentangled Attention. arXiv\ne-prints, pages arXiv–2006.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song.\n2020. Pretrained Transformers Improve Out-of-\nDistribution Robustness. arXiv:2004.06100 [cs] .\nArXiv: 2004.06100.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn Proceedings of ACL 2018.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What Does BERT Learn about the Structure\nof Language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657.\n7572\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv e-prints,\npages arXiv–1412.\nAlex Krizhevsky. 2009. Learning Multiple Layers of\nFeatures from Tiny Images. University of Toronto.\nAnkit Kumar, Piyush Makhija, and Anuj Gupta. 2020.\nUser Generated Data: Achilles’ Heel of BERT.\narXiv e-prints, pages arXiv–2003.\nJohn Lafferty, Andrew McCallum, and Fernando\nPereira. 2001. Conditional Random Fields: Prob-\nabilistic Models for Segmenting and Labeling Se-\nquence Data. Association for Computing Machinery\n(ACM).\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of NAACL-HLT, pages 260–270.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics, page\nbtz682. ArXiv: 1901.08746.\nMingchen Li, Mahdi Soltanolkotabi, and Samet Oy-\nmak. 2020. Gradient descent with early stopping is\nprovably robust to label noise for overparameterized\nneural networks. In International Conference on Ar-\ntiﬁcial Intelligence and Statistics, pages 4313–4324.\nPMLR.\nSheng Liu, Jonathan Niles-Weed, Narges Razavian,\nand Carlos Fernandez-Granda. 2020. Early-learning\nregularization prevents memorization of noisy labels.\nAdvances in Neural Information Processing Systems,\n33.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. arXiv:1711.05101\n[cs, math]. ArXiv: 1711.05101.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of NAACL-HLT 2018.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language Models as Knowl-\nedge Bases? In Proceedings of EMNLP 2019.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 Shared Task:\nLanguage-Independent Named Entity Recognition.\narXiv:cs/0306050. ArXiv: cs/0306050.\nJake Snell, Kevin Swersky, and Richard S. Zemel.\n2017. Prototypical Networks for Few-shot Learning.\narXiv:1703.05175 [cs, stat]. ArXiv: 1703.05175.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT Rediscovers the Classical NLP Pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des\nCombes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J. Gordon. 2019. An Empirical Study of Exam-\nple Forgetting during Deep Neural Network Learn-\ning. In Proceedings of ICLR 2019.\nLifu Tu, Garima Lalwani, Spandana Gella, and He He.\n2020. An empirical study on robustness to spuri-\nous correlations using pre-trained language models.\nTransactions of the Association for Computational\nLinguistics, 8:621–633.\nZihan Wang, Jingbo Shang, Liyuan Liu, Lihao Lu,\nJiacheng Liu, and Jiawei Han. 2019. Cross-\nWeigh: Training Named Entity Tagger from Imper-\nfect Annotations. arXiv:1909.01441 [cs] . ArXiv:\n1909.01441.\nSaining Xie, Ross Girshick, Piotr Dollár, Zhuowen\nTu, and Kaiming He. 2017. Aggregated Resid-\nual Transformations for Deep Neural Networks.\narXiv:1611.05431 [cs]. ArXiv: 1611.05431.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-\njamin Recht, and Oriol Vinyals. 2017. Understand-\ning deep learning requires rethinking generalization.\nIn Proceedings of ICLR 2017.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-\njamin Recht, and Oriol Vinyals. 2021. Understand-\ning deep learning (still) requires rethinking general-\nization. Commun. ACM, 64(3):107–115.\n7573\nA Comparison of learning phases in a\nBiLSTM and ResNet on CIFAR-10\nFor comparison, we show the training progress\nof a ResNet (He et al., 2015) trained on CIFAR10\n(Krizhevsky, 2009) in Figure 12. Following Toneva\net al. (2019), we use a ResNeXt model (Xie et al.,\n2017) with 101 blocks pre-trained on the ImageNet\ndataset (Deng et al., 2009). The model has been\nﬁne-tuned with a cross-entropy loss with the same\noptimiser and hyper-parameters as BERT. We eval-\nuate it using F 1 score. As can be seen, the train-\ning performance continues to increase while the\nvalidation performs plateaus or decreases, with\nno clearly delineated second phase as in the pre-\ntrained BERT’s training.\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.2\n0.4\n0.6\n0.8\n1.0Validation F1 score\nvalidation\ntraining\nphase 1\nphase 2\nphase 3\nFigure 12: Performance (F 1) of a ResNet model\nthroughout the training process on the CIFAR10\ndataset. Darker colours correspond to higher levels of\nnoise (0% to 50%).\nB JNLPBA noise results\nAs well as CoNLL03, we also report the analysis\non the JNLPBA dataset. In Figure 13, we show\nthe performance of BERT on increasingly noisy\nversions of the training set. In Figure 14, we report\nthe accuracy of noisy examples.\nC Effect of pre-training\nBERT’s second phase of pre-training and noise re-\nsilience are mainly attributable to its pre-training.\nWe show the training progress of a non-pretrained\nBERT model on CoNLL03 in Figure 15 and its\nclassiﬁcation accuracy on noisy examples in Fig-\nure 16. As can be seen, a non-pre-trained BERT’s\ntraining performance continuously improves and\nso does its performance on noisy examples.\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8Validation F1 score\nvalidation\ntraining\nphase 1\nphase 2\nphase 3\nFigure 13: BERT performance (F 1) throughout the\ntraining process on the JNLPBA dataset. Darker\ncolours correspond to higher levels of noise (0% to\n50%).\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14Classification accuracy\nrandom classifier accuracy\nBERT\nphase 1\nphase 2\nphase 3\nFigure 14: Classiﬁcation accuracy of noisy examples in\nthe training set for theJNLPBA dataset. Darker colours\ncorrespond to higher levels of noise (0% to 50%).\nD Examples of forgettable examples\nIn Table 3, we can ﬁnd the sentences containing\nthe most forgettable examples during a training\nrun of 50 epochs for the CoNLL03 dataset. The\nmaximum theoretical number of forgetting events\nin this case is 25. It is important to notice how the\nmost forgotten entity presents a mismatched \"The\",\nwhich the network correctly classiﬁes as an \"other\"\n(O) entity.\nE BERT as a noise detector\nWe report the exact detection metrics for the model\nproposed in section 4 in Table 4. Here we can\nsee how both for extremely noisy datasets and for\ncleaner datasets, our model is able to detect the\nnoisy examples with about 90-91% F 1 score, as\nmentioned above.\nMoreover, we provide the implementation used\n7574\nSentence Number of forgetting events\nthe third and ﬁnal test between England and Pakistan at The (I-LOC) 11\nGOLF - BRITISH MASTERS THIRD ROUND SCORES . (O) 10\nGOLF - GERMAN OPEN FIRST ROUND SCORES . (O) 10\nEnglish County Championshipcricket matches on Saturday : (MISC) 10\nEnglish County Championshipcricket matches on Friday : (MISC) 9\nTable 3: Sentences containing the most forgettable examples in the CoNLL03 dataset. In bold the entity that was\nmost often forgotten within the given sentence and in brackets its ground-truth classiﬁcation.\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Validation F1 score\nvalidation\ntraining\nphase 1\nphase 2\nphase 3\nFigure 15: Performance (F 1) of a non-pre-trained\nBERT model throughout the training process on the\nCoNLL03 train and validation sets. Darker colours cor-\nrespond to higher levels of noise (0% to 50%).\nNoise Precision Recall F 1 score\n10% 92.18% 95.90% 94.00%\n20% 96.19% 96.33% 96.26%\n30% 98.02% 96.35% 97.17%\n40% 98.27% 96.95% 97.60%\n50% 98.64% 97.27% 97.94%\nTable 4: Noise detection performance with varying lev-\nels of noise on the CoNLL03 dataset using the method\nproposed.\nto detect outliers used to produce the table and\nﬁgures above:\n1. We ﬁrst collect the losses for each training ex-\nample after a short ﬁne-tuning process (4 epochs\nin our case).\n2. We then assume an unknown portion of these\nexamples is noisy, giving rise to a two-class\nclassiﬁcation problem (noisy vs non-noisy). To\ndiscriminate the two classes, we then solve the\nfollowing optimisation problem which aims to\nﬁnd a loss threshold T that minimises inter-class\nvariance for each of the two classes:\narg min\nT\n∑\nx < T\n∥x−µc∥2 +\n∑\nx ≥T\n∥x−µn∥2\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175Classification accuracy\nrandom classifier accuracy\nBERT\nphase 1\nphase 2\nphase 3\nFigure 16: Classiﬁcation accuracy of a non-pre-trained\nBERT model on noisy examples in the training set for\nthe CoNLL03 dataset. Darker colours correspond to\nhigher levels of noise (0% to 50%).\nWhere elements denoted as xare the losses ex-\ntracted from the training set, µc is the mean of\nall x<T , and µn is the mean of all x≥T.\n3. For testing purposes, we then apply the method\nto the chosen training set and measure the noise\ndetection F1 score.\nIn Figure 17, we qualitatively saw how the losses\nare distributed for noisy and regular examples and\nnotice how they are neatly separated except for a\nsmall subset of the noisy examples. These exam-\nples might have been already memorised by the\nmodel, which would explain their lower loss.\nF JNLPBA forgetting results\nWe show in Figure 18 how many data points were\nlearned by BERT for the ﬁrst time at each epoch on\nthe JNLPBA dataset during training (ﬁrst learning\nevents).\nG Further ProtoBERT results\nAs in Table 2 we only reported F 1 score for our\nmethods, for completeness we also report precision\nand recall in table 5.\n7575\nModel CoNLL03 JNLPBA WNUT17\nP R F 1 P R F 1 P R F 1\nState-of-the-art NA NA 93.50 NA NA 77.59 NA NA 50.03\nBERT + classiﬁcation layer (baseline) 88.97 89.75 89.35 72.99 77.90 75.36 53.65 37.42 44.09\nProtoBERT 89.26 90.49 89.87 68.66 80.03 73.91 54.38 43.96 48.62\nProtoBERT + running centroids 89.03 89.91 89.46 68.92 78.83 73.54 54.11 44.05 48.56\nTable 5: Comparison between the baseline model and the proposed architecture on the CoNLL03, JNLPBA and\nWNUT17 datasets evaluated using entity-level metrics.\nNoise Forgettable Unforgettable Learned Forgettable/learned (%)\nCoNLL03 0% 2,669 699,381 230,716 1.1568%\nCoNLL03 10% 10,352 691,698 224,968 4.6015%\nCoNLL03 20% 19,667 682,383 216,780 9.0723%\nCoNLL03 30% 30,041 672,009 209,191 14.3606%\nJNLPBA 0% 23,263 817,087 457,485 5.0849%\nJNLPBA 10% 26,667 813,683 422,264 6.3152%\nJNLPBA 20% 26,369 813,981 386,562 6.8214%\nJNLPBA 30% 30,183 810,167 353,058 8.5490%\nCIFAR10 0% 8,328 36,672 45,000 18.5067%\nCIFAR10 10% 9,566 35,434 44,976 21.2691%\nCIFAR10 20% 9,663 35,337 44,922 21.5106%\nCIFAR10 30% 11,207 33,793 44,922 24.9477%\nTable 6: Number of forgettable, unforgettable, and learned examples during BERT training on the CoNLL03,\nJNLPBA and CIFAR10 datasets.\n460000\n480000\n0 1 2 3 4 5\n0\n20000\n40000\n60000\nTraining loss\nNumber of examples\nClassifier cutoff\nNormal\nNoisy\nFigure 17: Loss distribution for noisy and non-noisy\nexamples from the CoNLL03 training set. The grey\ndashed line represent the chosen loss threshold found\nby our method to discriminate between noisy and non-\nnoisy examples.\nH ProtoBERT results on JNLPBA\nWe report in Figure 19 the comparison between our\nbaseline and ProtoBERT for all classes.\n1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8Ratio of training examples\nBERT\nphase 1\nphase 2\nphase 3\nFigure 18: First learning events distribution during\nBERT training for various levels of noise on the\nJNLPBA dataset. Darker colours correspond to higher\nlevels of noise (0% to 50%).\nExamples BERT bi-LSTM\nForgettable 2,669 144,377\nUnforgettable 699,381 60,190\nLearned 230,716 184,716\nForgettable/learned (%) 1.1568% 78,1616%\nTable 7: Comparison of the number of forgettable,\nlearnable and unforgettable examples between BERT\nand a bi-LSTM model.\n7576\n0\n 25\n 50\n 75\n 100\n 125\n 150\n 175\n 200\nNumber of training sentences containing the target label\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65Test few-shot F1 score\nDNA - ProtoBERT\nDNA - BERT + Class. Layer\nProtein - ProtoBERT\nProtein - BERT + Class. Layer\nFigure 19: Model performance comparison between\nthe baseline model and ProtoBERT for the JNLPBA\ndataset, reducing the sentences containing the DNA and\nProtein class. Results reported as F 1 score on all\nclasses.\nI Results on other pretrained\ntransformers\nWhile most of the main paper focuses on BERT,\nit is worthwhile to mention the results on other\npre-trained transformers and compare the results.\nIn Figures 20 and 21, we show the valida-\ntion performances (classiﬁcation F 1 score) for\nthe CoNLL03 datasets for the RoBERTa and De-\nBERTa models (similarly to Figure 1). We notice\nthat the three phases of training reported above\nare apparent in all studied models. RoBERTa, in\nparticular, displays the same pattern, but shows\nhigher robustness to noise compared to the other\ntwo models.\nMoreover, in Figures 22 and 23, we report the\ndistribution of ﬁrst learning events (similarly to\nFigure 5) on RoBERTa and DeBERTa. As above,\nwe can observe the same pattern described in the\nmain body of the paper, with the notable exception\nthat RoBERTa is again more robust to learning the\nnoise in later phases of the training.\nJ Few-shot MISC memorisation\nAs per section 6, we also report the result of the\nexperiments in the few-shot setting by removing\nmost sentences containing the MISC class. The\nexperimental setting is identical to the described in\nthe main body of the paper. The relevant Figures\nare 24 and 25.\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Validation F1 score\nValidation\nTraining\nphase 1\nphase 2\nphase 3\nFigure 20: RoBERTa performance (F1) throughout the\ntraining process on the CoNLL03 train and validation\nsets. Darker colours correspond to higher levels of\nnoise (0% to 50%).\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Validation F1 score\nValidation\nTraining\nphase 1\nphase 2\nphase 3\nFigure 21: DeBERTa performance (F1) throughout the\ntraining process on the CoNLL03 train and validation\nsets. Darker colours correspond to higher levels of\nnoise (0% to 50%).\n1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8Classification accuracy\nRoBERTa\nphase 1\nphase 2\nphase 3\nFigure 22: First learning events distribution during\nRoBERTa training for various levels of noise on the\nCoNLL03 dataset. Darker colours correspond to\nhigher levels of noise (0% to 50%).\n7577\n1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8Classification accuracy\nDeBERTa\nphase 1\nphase 2\nphase 3\nFigure 23: First learning events distribution during\nDeBERTa training for various levels of noise on the\nCoNLL03 dataset. Darker colours correspond to\nhigher levels of noise (0% to 50%).\n0 1 2 3 4 5 6 7 8 9 10\nEpoch\n0.0\n0.2\n0.4\n0.6\n0.8Validation F1 score\nValidation\nTraining\nphase 1\nphase 2\nphase 3\nFigure 24: BERT performance (F 1) throughout the\ntraining process on the CoNLL03-XMISC train and\nvalidation sets. Darker colours correspond to fewer ex-\namples of the MISC class available (5 to 95 in steps of\n20).\n1 2 3 4 5 6 7 8 9 10\nEpoch\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35Ratio of training examples\nBERT\nphase 1\nphase 2\nphase 3\nFigure 25: First learning events distribution dur-\ning the training for various levels of noise on the\nCoNLL03-XMISC dataset. Darker colours correspond\nto fewer examples of the MISC class available (5 to 95\nin steps of 20).\n7578"
}