{
  "title": "Pretrained Language Model Embryology: The Birth of ALBERT",
  "url": "https://openalex.org/W3092592453",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4222756294",
      "name": "Cheng-Han Chiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208090214",
      "name": "Sung-feng Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2514219681",
      "name": "Hung-Yi Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2911748701",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2964331441",
    "https://openalex.org/W2904243021",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2963759780",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2996309822",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2767204723",
    "https://openalex.org/W2970120757"
  ],
  "abstract": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6813–6828,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n6813\nPretrained Language Model Embryology: The Birth of ALBERT\nCheng-Han Chiang\nNational Taiwan University,\nTaiwan\ndcml0714@gmail.com\nSung-Feng Huang\nNational Taiwan University,\nTaiwan\nf06942045@ntu.edu.tw\nHung-yi Lee\nNational Taiwan University,\nTaiwan\nhungyilee@ntu.edu.tw\nAbstract\nWhile behaviors of pretrained language mod-\nels (LMs) have been thoroughly examined,\nwhat happened during pretraining is rarely\nstudied. We thus investigate the developmen-\ntal process from a set of randomly initialized\nparameters to a totipotent 1 language model,\nwhich we refer to as the embryology of a pre-\ntrained language model. Our results show\nthat ALBERT learns to reconstruct and pre-\ndict tokens of different parts of speech (POS)\nin different learning speeds during pretrain-\ning. We also ﬁnd that linguistic knowledge and\nworld knowledge do not generally improve\nas pretraining proceeds, nor do downstream\ntasks’ performance. These ﬁndings suggest\nthat knowledge of a pretrained model varies\nduring pretraining, and having more pretrain\nsteps does not necessarily provide a model\nwith more comprehensive knowledge. We pro-\nvide source codes and pretrained models to\nreproduce our results at https://github.\ncom/d223302/albert-embryology.\n1 Introduction\nThe world of NLP has gone through some tremen-\ndous revolution since the proposal of contextual-\nized word embeddings. Some big names are ELMo\n(Peters et al., 2018), GPT (Radford et al.), and\nBERT (Devlin et al., 2019), along with its vari-\nants (Sanh et al., 2019; Liu et al., 2019b; Lan et al.,\n2019). Performance boosts on miscellaneous down-\nstream tasks have been reported by ﬁnetuning these\ntotipotent pretrained language models. With a view\nto better grasping what has been learned by these\ncontextualized word embedding models, probing is\ngenerally applied to the pretrained models and the\n1According to Wikipedia, totipotency is the ability of a\nsingle cell to divide and produce all of the differentiated cells\nin an organism. We use its adjective form here to refer to\nthe ability of a pretrained model which can be ﬁnetuned for a\nvariety of downstream tasks.\nmodels ﬁnetuned from them. Probing targets can\nrange from linguistic knowledge, including seman-\ntic roles and syntactic structures (Liu et al., 2019a;\nTenney et al., 2019, 2018; Hewitt and Manning,\n2019), to world knowledge (Petroni et al., 2019).\nWhile the previous work focuses on what\nknowledge has been learned after pretraining of\ntransformer-based language models, few delve into\ntheir dynamics during pretraining. What happened\nduring the training process of a deep neural net-\nwork model has been widely studied, including\nGur-Ari et al. (2018), Frankle et al. (2019), Raghu\net al. (2017), Morcos et al. (2018). Some previ-\nous works also study the dynamics of the training\nprocess of an LSTM language model (Saphra and\nLopez, 2018, 2019), but the training dynamics of\na large scale pretrained language models are not\nwell-studied. In this work, we probe ALBERT\n(Lan et al., 2019) during its pretraining phase every\nN parameter update steps and study what it has\nlearned and what it can achieve so far. We perform\na series of experiments, detailed in the following\nsections, to investigate the development of predict-\ning and reconstructing tokens (Section 3), how lin-\nguistic and world knowledge evolve through time\n(Section 4, Section 6), and whether amassing those\ninformation serves as an assurance of good down-\nstream task performances (Section 5).\nWe have the following ﬁndings based on AL-\nBERT:\n•The prediction and reconstruction of tokens\nwith different POS tags have different learning\nspeeds. (Section 3)\n•Semantic and syntactic knowledge is devel-\noped simultaneously in ALBERT. (Section 4)\n•Finetuning from model pretrained for 250k\nsteps gives a decent GLUE score (80.23), and\n6814\nfurther pretrain steps only make the GLUE\nscore rise as high as 81.50.\n•While ALBERT does generally gain more\nworld knowledge as pretraining goes on, the\nmodel seems to be dynamically renewing its\nknowledge about the world. (Section 6)\nWhile we only include the detailed results of\nALBERT in the main text, we ﬁnd that the results\nalso generalize to the other two transformer-based\nlanguage models, ELECTRA (Clark et al., 2019)\nand BERT, which are quite different from ALBERT\nin the sense of pretext task and model architecture.\nWe put the detailed results of ELECTRA and BERT\nin the appendix.\n2 Pretraining ALBERT\nALBERT is a variant of BERT with cross-layer\nparameters sharing and factorized embedding pa-\nrameterization. The reason why we initially chose\nALBERT as our subject lies in its parameter efﬁ-\nciency, which becomes a signiﬁcant issue when we\nneed to store 1000 checkpoints during the pretrain-\ning process.\nTo investigate what happened during the pre-\ntraining process of ALBERT, we pretrained an\nALBERT-base model ourselves. To maximally re-\nproduce the results in Lan et al. (2019), we follow\nmost of the training hyperparameters in the original\nwork, only modifying some hyperparameters to ﬁt\nin our limited computation resources2. We also fol-\nlow Lan et al. (2019), using English Wikipedia as\nour pretraining data, and we use the Project Gutten-\nberg Dataset (Lahiri, 2014) instead of BookCorpus.\nThe total size of the corpus used in pretraining is\n16GB. The pretraining was done on a single Cloud\nTPU V3 and took eight days to ﬁnish 1M pretrain\nsteps, costing around 700 USD. More details on\npretraining are speciﬁed in appendix B.1.\n3 Learning to Predict the Masked Tokens\nand Reconstruct the Input Tokens\nDuring the pretraining stage of a masked LM\n(MLM), it learns to predict masked tokens based\non the remaining unmasked part of the sentence,\nand it also learns to reconstruct token identities of\nunmasked tokens from their output representations\nof the model. Better prediction and reconstruction\n2We use the ofﬁcial implementation of ALBERT\nat https://github.com/google-research/\nalbert.\n0 5 k 10 k 15 k 20 k 25 k 30 k\npretrain step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy (rescaled)\nconj.\ndet.\nprep.\nadj.\nnoun\nproper noun\npron.\nadv.\nverb\n(a) Token reconstruction\n0 25 k 50 k 75 k 100 k\npretrain step\n0.0\n0.2\n0.4\n0.6\n0.8accuracy (rescaled)\n(b) Mask prediction\nFigure 1: Rescaled accuracy of token reconstruction\nand mask prediction during pretraining. We rescale the\naccuracy of each line by the accuracy when the model\nis fully pretrained, i.e., the accuracy after pretraining\n1M steps. Token reconstruction are evaluated every 1K\npretrain steps, and mask prediction evaluated every 5K\nsteps.\nresults indicate the model being able to utilize con-\ntextual information. To maximally reconstruct the\ninput tokens, the output representations must keep\nsufﬁcient information regarding token identities.\nWe investigate the behavior of mask prediction\nand token reconstruction for tokens of different\nPOS during the early stage of pretraining. We use\nthe POS tagging in OntoNotes 5.0 (Weischedel\net al., 2013) in this experiment. For the mask pre-\ndiction part, we mask a whole word (which may\ncontain multiple tokens) of an input sentence, feed\nthe masked sentence into ALBERT, and predict the\nmasked token(s). We evaluate the prediction per-\nformance by calculating the prediction’s accuracy\nbased on POS of the word; the predicted token(s)\nshould exactly match the original token(s) to be\ndeemed an accurate prediction. As for the token\nreconstruction part, the input to the model is simply\n6815\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n0\n2\n4\n6\n8\n10Loss\nLoss\npretrain loss\n(a) Total loss during pretraining\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Evaluation Result\nEvaluation result\nMLM\nSRL\nPOS\nConst\nCoref\n(b) Masked LM accuracy and F1 scores of different\nprobing tasks over the course of pretraining\nFigure 2: The probing results of hidden representation\nfrom layer 8; all four tasks are evaluated with test set\nof OntoNotes 5.0 and F1 scores are reported. MLM\naccuracy is also shown. We smoothed the lines by av-\neraging 3 consecutive data points for better illustration.\nThe unsmoothed result is in Appendix D.3.\nthe original sentence.\nThe results of reconstruction are shown in Fig-\nure 1(a). ALBERT ﬁrst learns to reconstruct func-\ntion words, e.g., determiners, prepositions, and\nthen gradually learns to reconstruct content words\nin the order of verb, adverb, adjective, noun, and\nproper noun. We also found that different forms\nand tenses of a verb do not share the same learning\nschedule, with third-person singular present be-\ning the easiest to reconstruct and present participle\nbeing the hardest (shown in Appendix C.2). The\nprediction results in Figure 1(b) reveal that learning\nmask prediction is generally more challenging than\ntoken reconstruction. ALBERT learns to predict\nmasked tokens with an order similar to token recon-\nstruction, though much slower and less accurate.\nWe ﬁnd that BERT also learns to perform mask\nprediction and token reconstruction in a similar\nfashion, with the results provided in Appendix C.4.\n4 Probing Linguistic Knowledge\nDevelopment During Pretraining\nProbing is widely used to understand what kind\nof information is encoded in embeddings of a lan-\nguage model. In short, probing experiments train\na task-speciﬁc classiﬁer to examine if token em-\nbeddings contain the knowledge required for the\nprobing task. Different language models may give\ndifferent results on different probing tasks, and rep-\nresentations from different layers of a language\nmodel may also contain different linguistic infor-\nmation (Liu et al., 2019a; Tenney et al., 2018).\nOur probing experiments are modiﬁed from the\n“edge probing” framework in Tenney et al. (2018).\nHewitt and Liang (2019) previously showed that\nprobing models should be selective, so we use lin-\near classiﬁers for probing. We select four prob-\ning tasks for our experiments: part of speech\n(POS) tagging, constituent (const) tagging, corefer-\nence (coref) resolution, and semantic role labeling\n(SRL). The former two tasks probe syntactic knowl-\nedge hidden in token embeddings, and the last two\ntasks are designed to inspect the semantic knowl-\nedge provided by token embeddings. We use an-\nnotations provided in OntoNotes 5.0 (Weischedel\net al., 2013) in our experiments.\nThe probing results are shown in Figure 2b. We\nobserve that all four tasks show similar trends dur-\ning pretraining, indicating that semantic knowledge\nand syntactic knowledge are developed simulta-\nneously during pretraining. For syntactically re-\nlated tasks, the performance of both POS tagging\nand constituent tagging boost very fast in the ﬁrst\n100k pretrain steps, and no further improvement\ncan be seen throughout the remaining pretraining\nprocess, while performance ﬂuctuates from time to\ntime. We also observe an interesting phenomenon:\nthe probed performances of SRL peak at around\n150k steps and slightly decay over the remaining\npretraining process, suggesting that some informa-\ntion in particular layers related to probing has been\ndwindling while the ALBERT model strives to ad-\nvance its performance on the pretraining objective.\nThe loss of the pretraining objective is also shown\nin Figure 2a.\nScrutinizing the probing results of different lay-\ners (Figure 3 and Appendix D.3), we ﬁnd that the\nbehaviors among different layers are slightly dif-\nferent. While the layers closer to output layer per-\nform worse than layers closer to input layer at the\nbeginning of pretraining, their performances rise\n6816\n0 100 k 200 k 300 k 400 k 500 k\nPretrain steps\n0.6\n0.7\n0.8\n0.9\n1.0F1 score\nlayer 1\nlayer 12\nlayer 2\nlayer 8\nFigure 3: The probing results of POS during pretrain-\ning. Layers are indexed from the input layer to the out-\nput layer.\n0 30k 60k 210k 500k\nFigure 4: Attention patterns of head 11 across layer 1\n(ﬁrst row), 2 (second row), and 8 (third row) during pre-\ntraining. Pretrain steps labeled atop the attention map.\nWe averaged the attention maps of different input sen-\ntences to get the attention pattern of a single head.\ndrastically and eventually surpass the top few lay-\ners; however, they start to decay after they reach\nbest performances. This implies the last few layers\nof ALBERT learn faster than the top few layers.\nThis phenomenon is also revealed by observing\nthe attention patterns across different layers dur-\ning pretraining. Figure 4 shows that the diagonal\nattention pattern (Kovaleva et al., 2019) of layer\n8 emerges earlier than layer 2, with the pattern of\nlayer 1 looms the last3.\n5 Does Expensive and Lengthy\nPretraining Guarantee Exceptional\nResults on Downstream Tasks?\nWhile Devlin et al. (2019) and Lan et al. (2019)\nhave shown that more pretrain steps lead to better\n3GIF ﬁles are provided in this website: https://\nalbertembryo.wordpress.com/\n4GLUE score of albert-base-v1 and bert-base are obtained\nby ﬁnetuning ALBERT and BERT models from Hugging-\nFace(Wolf et al., 2019)\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n60\n65\n70\n75\n80\n85Evaluation result pretrain process\nalbert-base-v1\nbert-base\n(a) GLUE scores over pretraining. GLUE scores of albert-\nbase-v1 and bert-base are also shown by horizontal lines.4.\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n20\n30\n40\n50\n60\n70\n80\n90Evaluation result\nMNLI\nMRPC\nSTS-B\nSST-2\nCoLA\nQNLI\nQQP\nRTE\n(b) Performance of individual tasks in GLUE benchmark. Best\nresult during pretraining marked with ‘x’. Performances of\nalbert-base-v1 and bert-base-uncased are marked with ‘+’ and\nsquare respectively.\nFigure 5: Downtream evaluation of ALBERT on de-\nvelopment set every 50k pretrain steps. GLUE score\nis averaged among all tasks except WNLI. Evaluation\nmetrics: MRPC and QQP: F1, STS-B: Spearman corr.,\nothers: accuracy. The result of MNLI is the average of\nmatched and mismatched.\nGLUE scores, whether the performance gain of\ndownstream tasks is proportional to the resources\nspent on additional pretrain steps is unknown. This\ndrives us to explore the downstream performance\nof the ALBERT model before fully pretrained. We\nchoose GLUE benchmark (Wang et al., 2018) for\ndownstream evaluation, while excluding WNLI,\nfollowing Devlin et al. (2019).\nWe illustrate our results of the downstream per-\nformance of the ALBERT model during pretraining\nin Figure 5. While the GLUE score gradually in-\ncreases as pretraining proceeds, the performance\nafter 250k does not pale in comparison with a fully\npretrained model (80.23 v.s. 81.50). From Fig-\nure 5b, we also observe that most GLUE tasks\nreach comparable results with their fully pretrained\ncounterpart over 250k pretrain steps, except for\n6817\nMNLI and QNLI, indicating NLI tasks do beneﬁt\nfrom more pretrain steps when the training set is\nlarge.\nWe also ﬁnetuned BERT and ELECTRA models\nas pretraining proceeds, and we observe similar\ntrends. The GLUE scores of the BERT and ELEC-\nTRA model rise drastically in the ﬁrst 100k pre-\ntrain steps, and then the performance increments\nless slowly afterward. We put the detailed result of\nthese two models in Section E.4.\nWe conclude that it may not be necessary to\ntrain an ALBERT model until its pretraining loss\nconverges to obtain exceptional downstream per-\nformance. The majority of its capability for down-\nstream tasks has already been learned in the early\nstage of pretraining. Note that our results do not\ncontradict previous ﬁndings in Devlin et al. (2019),\nLiu et al. (2019b), and Clark et al. (2019), all of\nwhich showing that downstream tasks do beneﬁt\nfrom more pretrain steps; we show that the perfor-\nmance gain on downstream tasks in latter pretrain\nsteps might be disproportional to the cost on more\npretrain steps.\n6 World Knowledge Development\nDuring Pretraining\nPetroni et al. (2019) has reported that language\nmodels contain world knowledge. To examine the\ndevelopment of world knowledge of a pretrained\nlanguage model, we conduct the same experiment\nas in Petroni et al. (2019). We use a subset of\nT-REx (Elsahar et al., 2018) from the dataset pro-\nvided by Petroni et al. (2019) to evaluate AL-\nBERT’s world knowledge development.\nThe results are shown in Figure 6, in which we\nobserve that world knowledge is indeed built up\nduring pretraining, while performance ﬂuctuates\noccasionally. From Figure 6, it is clear that while\nsome types of knowledge stay static during pre-\ntraining, some vary drastically over time, and the\nresult of a fully pretrained model (at 1M steps) may\nnot contain the most amount of world knowledge.\nWe infer that world knowledge of a model depends\non the corpus it has seen recently, and it tends to\nforget some knowledge that it has seen long ago.\nThese results imply that it may not be sufﬁcient\nto draw a conclusion on ALBERT’s potential as\na knowledge base merely based on the ﬁnal pre-\ntrained one’s behavior. We also provide qualitative\nresults in Appendix F.2.\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\nP140\nP103\nP176\nP138\nP407\nP159\nP1376\nFigure 6: World knowledge development during pre-\ntraining evaluated every 50k pretrain steps. Types of\nrelation, and template are shown in Table 1\nType Query template\nP140 [X] is afﬁliated with the [Y] religion .\nP103 The native language of [X] is [Y] .\nP176 [X] is produced by [Y] .\nP138 [X] is named after [Y] .\nP407 [X] was written in [Y] .\nP159 The headquarter of [X] is in [Y] .\nP1376 [X] is the capital of [Y] .\nTable 1: Relations in Figure 6. We ﬁll in [X] with the\nsubject, [Y] with [MASK] and ask model to predict Y .\n7 Conclusion\nAlthough ﬁnetuning from pretrained language mod-\nels puts in phenomenal downstream performance,\nthe reason is not fully uncovered. This work aims\nto unveil the mystery of the pretrained language\nmodel by looking into how it evolves. Our ﬁnd-\nings show that the learning speeds for reconstruct-\ning and predicting tokens differ across POS. We\nﬁnd that the model acquires semantic and syntac-\ntic knowledge simultaneously at the early pretrain-\ning stage. We show that the model is already pre-\npared for ﬁnetuning on downstream tasks at its\nearly pretraining stage. Our results also reveal that\nthe model’s world knowledge does not stay static\neven when pretraining loss converges. We hope our\nwork can bring more insights into what makes a\npretrained language model a pretrained language\nmodel.\nAcknowledgements\nWe thank all the reviewers’ valuable suggestions\nand efforts towards improving our manuscript. This\nwork was supported by Delta Electronics, Inc.\n6818\nReferences\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2019. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In International Conference on Learning Rep-\nresentations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT (1).\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-rex: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018).\nJonathan Frankle, David J Schwab, and Ari S Morcos.\n2019. The early phase of neural network training.\nIn International Conference on Learning Represen-\ntations.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\nGuy Gur-Ari, Daniel A Roberts, and Ethan Dyer. 2018.\nGradient descent happens in a tiny subspace. arXiv\npreprint arXiv:1812.04754.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365–4374, Hong Kong, China. Association for\nComputational Linguistics.\nShibamouli Lahiri. 2014. Complexity of Word Collo-\ncation Networks: A Preliminary Structural Analy-\nsis. In Proceedings of the Student Research Work-\nshop at the 14th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 96–105, Gothenburg, Sweden. Association for\nComputational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAri Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation. In Advances\nin Neural Information Processing Systems, pages\n5727–5736.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proc. of NAACL.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. Language mod-\nels are unsupervised multitask learners.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. Svcca: Singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In Advances in Neu-\nral Information Processing Systems, pages 6076–\n6085.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nNaomi Saphra and Adam Lopez. 2018. Language\nmodels learn pos ﬁrst. In Proceedings of the 2018\n6819\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 328–\n330.\nNaomi Saphra and Adam Lopez. 2019. Understanding\nlearning dynamics of language models with svcca.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3257–\n3267.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593–\n4601.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R Bowman, Dipan-\njan Das, et al. 2018. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In International Confer-\nence on Learning Representations.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353–355.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. Ontonotes release 5.0\nldc2013t19. Linguistic Data Consortium, Philadel-\nphia, PA, 23.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\n6820\nA Modiﬁcations from the Reviewed\nVersion\nWe made some modiﬁcations in the camera-ready\nversion, mostly based on the reviewers’ recommen-\ndations and for better reproducibility.\n•We add the result of BERT and ELECTRA in\nSection 3, Section 4, and Section 5.\n•We reimplement the source code for Section 4\nand renew the experiment results accordingly.\nWhile the exact values are slightly different,\nthe general trends are the same and do not\naffect our observation.\n•We add the results of coreference resolution\nin our probing experiments, following the re-\nviewers’ suggestion.\n•We polish our wordings and presentations in\ntext and ﬁgures.\nB Pretraining\nB.1 ALBERT\nAs mentioned in the main text, we only\nmodiﬁed a few hyperparameters to ﬁt in out\nlimited computation resources, listed in Ta-\nble 2. The Wikipedia corpus used in our\npretraining can be download from https:\n//dumps.wikimedia.org/enwiki/latest/\nenwiki-latest-pages-articles.xml.bz2 ,\nand the Gutenburg dataset can be download\nfrom https://web.eecs.umich.edu/˜lahiri/\ngutenberg_dataset.html. The number of\nparameters in our ALBERT model is 12M.\nBatch size 512\nLearning rate 6.222539674E-4\nTotal steps 1M\nWarmup steps 25k\nTable 2: Pretraining hyperparemeters for ALBERT.\nB.2 BERT\nWe use the same dataset as we trained AL-\nBERT to pretrain BERT. We pretrained a BERT-\nbase-uncased model using the ofﬁcial imple-\nmentation of BERT at https://github.com/\ngoogle-research/bert, and we follow all hyper-\nparameters of the original implementation. Note\nthat the Devlin et al. (2019) mentioned they trained\nBERT with a maximum sequence length of 128 for\nthe ﬁrst 900K steps, and then trained the model\nwith a maximum sequence length 512 for the rest\n100K steps; we follow this training procedure. The\nnumber of parameters in our BERT model is 110M.\nB.3 ELECTRA\nWe use OpenWebTextCorpus (Gokaslan and Co-\nhen, 2019) from https://skylion007.github.\nio/OpenWebTextCorpus/ to pretrain an Electra-\nbase model. We pretrained this model using the\nofﬁcial implementation of ELECTRA at https:\n//github.com/google-research/electra, and\nwe follow all hyperparameters of the original im-\nplementation. The number of parameters in our\nELECTRA model used for ﬁnetuning (the discrim-\ninator part) is 110M.\nC Mask Predict and Token\nReconstruction\nC.1 Dataset\nAs mentioned in Section 3, we use the POS an-\nnotations in OntoNotes 5.0, and we only use the\nCoNLL-2012 test set for our experiments. While\nthere are 48 POS labels, we only report the mask\nprediction and token reconstruction of a much\nsmaller subset—those we are more familiar with.\nThe statistics of these POS are in Table 3.\nPOS Count\nConjunction 5109\nDeterminer 14763\nPreposition 18059\nAdjective 9710\nAdverb 7992\nVerb (all forms) 21405\nNoun 29544\nProper noun 13144\nTable 3: Statistics of POS used in experiments.\nVerb form Count\nBase form 5865\nPast tense 5398\nGerund or present participle 2821\nPast participle 3388\n3rd person singular present 3933\nTable 4: Statistics of different verb forms used in exper-\niments.\n6821\nC.2 Mask Predict and Token Reconstruction\nof Different Verb Forms\nWe provide supplementary materials for Section 3.\nIn Figure 7, we observe that ALBERT learns to\nreconstruct and predict verb of different forms at\ndifferent times. The average occurrence rate of verb\nin different form from high to low is V-es, V-ed,\nV , V-en, V-ing, which coincides with the priority\nbeing leaned.\n0 5 k 10 k 15 k 20 k 25 k 30 k\npretrain step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy (rescaled)\nV\nV-ed\nV-ing\nV-en\nV-es\n(a) Token reconstruction.\n0 25 k 50 k 75 k 100 k 125 k\npretrain step\n0.0\n0.2\n0.4\n0.6\n0.8accuracy (rescaled)\n(b) Mask prediction.\nFigure 7: Token reconstruction (7a) and mask predic-\ntion (7b) accuracy. We also rescale the accuracy as in\nFigure 1.\nC.3 How Does Occurrence Frequency Affect\nLearning Speed of A Word?\nIn the main text, we observe that words of different\nPOS are learned at different times of pretraining.\nWe also pointed out that the learning speed of dif-\nferent POS roughly corresponds to their occurrence\nrate. However, it is not clear to what extent a word’s\noccurrence frequency affects how soon it can be\nlearned to reconstruct or mask-predict by the model.\nWe provide a deeper analysis of the relationship\nbetween the learning speed of a word and its occur-\n0 50 k 100 k 150 k 200 k 250 k\nPretrain steps\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (rescaled)\n50~99\n300~349\n550~599\n800~849\n1050~1099\n1300~1349\n1550~1599\n1800~1849\nFigure 8: Rescaled mask prediction accuracy for dif-\nferent frequency. 50 ∼99 means the top 50 to top 99\noccurring tokens\nrence rate in Figure 8. We observe from Figure 8\nthat the top 50 to 99 occurring tokens are indeed\nlearned faster than other words which occur lesser.\nHowever, as for the top 300 to 349 occurring tokens\nand the top 1550 to 1599 occurring tokens, it is un-\nclear which ones are learned earlier. We can infer\nfrom Figure 8 and Figure 1b that the occurring rate\nand POS of a word both contribute to how soon the\nmodel can learn it to some extent.\nC.4 Mask Predict and Token Reconstruction\nof BERT\nWe provide the results of BERT’s token reconstruc-\ntion and mask prediction in Figure 9. We observe\ncontent words are learned later than function words,\nwhile the learning speed is faster than ALBERT. To\nbe more speciﬁc, we say a word type A is learned\nfaster than another word type B if either the learn-\ning curve of A rises earlier than B from 0, or if the\nrescaled learning curve of A is steeper than that of\nB.\nD Probing Experiments\nD.1 Probing Model Details\nAs mentioned in the main text, we modiﬁed and\nreimplemented the edge probing (Tenney et al.,\n2018) models in our experiments. The modiﬁca-\ntions are detailed as follow:\n•We remove the projection layer that projects\nrepresentation output from the language\nmodel to the probing model’s input dimen-\nsion.\n•We use average pooling to obtain span repre-\nsentation, instead of self-attention pooling.\n6822\nTask |L| Examples Tokens Total Targets\nPOS 48 116K / 16K / 12K 2.2M / 305K / 230K 2.1M / 290K / 212K\nConstituent 30 116K / 16K / 12K 2.2M / 305K / 230K 1.9M / 255K / 191K\nSRL 66 253K / 35K / 24K 6.6M / 934K / 640K 599K / 83K / 56K\nTable 5: Statistics of the number of labels, examples, tokens and targets (split by train/dev/test) we used in probing\nexperiments. |L|denotes number of target labels.\n0 10 k 20 k\npretrain step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy (rescaled)\nconj.\ndet.\nprep.\nadj.\nnoun\nproper noun\npron.\nadv.\nverb\n(a) Token reconstruction of BERT\n0 25 k 50 k 75 k 100 k 125 k\npretrain step\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy (rescaled)\nconj.\ndet.\nprep.\nadj.\nnoun\nproper noun\npron.\nadv.\nverb\n(b) Mask prediction of BERT\nFigure 9: We also rescale the accuracy as in Figure 1b.\n•We use linear classiﬁers instead of 2-layer\nMLP classiﬁers.\n•We probe the representation of a single layer,\ninstead of concatenating or scalar-mixing rep-\nresentations across all layers.\nSince our probing models are much simpler than\nthose in Tenney et al. (2018), probing results might\nbe inferior to the original work. The number of\nmodel’s parameters in our experiments is approxi-\nmately 38K for POS tagging, 24K for constituent\ntagging, and 100K for SRL.\nD.2 Dataset\nWe use OntoNotes-5.0, which can be down-\nload from https://catalog.ldc.upenn.edu/\nLDC2013T19. The statistics of this dataset is in\nTable 5.\nD.3 SRL, Coreference Resolution, and\nConstituent Labeling Results\nHere in Figure 10, we show supplementary ﬁgures\nfor SRL, coreference resolution, and constituent\ntagging over 3 of 12 layers in ALBERT for the ﬁrst\n500K pretrain steps. Together with Figure 3, all\nfour tasks show similar trends.\n0 100 k 200 k 300 k 400 k 500 k\nPretrain steps\n0.7\n0.8\n0.9F1 score\nlayer 12\nlayer 2\nlayer 6\n(a) Semantic role labeling\n0 100 k 200 k 300 k 400 k 500 k\nPretrain steps\n0.8\n0.9\n1.0F1 score\nlayer 12\nlayer 2\nlayer 6\n(b) Coreference resolution\n0 100 k 200 k 300 k 400 k 500 k\nPretrain steps\n0.4\n0.5\n0.6\n0.7F1 score\nlayer 12\nlayer 2\nlayer 6\n(c) Constituent tagging\nFigure 10: The probing results of SRL (10a, corefer-\nence resolution (10b) and constituency tagging (10c)\nduring pretraining . Layers are indexed from the input\nlayer to the output layer, so layer 2 is the output repre-\nsentation from layer 2 of ALBERT. Layers are indexed\nfrom 1 to 12.\n6823\nTask Examples\nMRPC 3.6K / 0.4K / 1.7K\nRTE 2.4K / 0.2K / 3K\nSTS-B 5.7K / 1.5K / 1.3K\nQNLI 104K / 5.4K / 5.4K\nQQP 363K / 40.4K / 391.0K\nCoLA 8.5K / 1.0K / 1.1K\nMNLI 392.7K / 9.8K + 9.8K / 9.8K + 9.8K\nSST-2 67.4K / 0.9K / 1.8K\nSQuAD2.0 13.3K / 11.9K / 8.9K\nTable 6: Statistics of (train / dev/ test) in GLUE tasks\nand SQuAD2.0. MNLI contains matched and mis-\nmatched in dev and test set. We didn’t evaluate our\nmodels’ performance on test set.\nD.4 Probing Results of BERT and\nELECTRA\nWe provide the probing results of BERT and ELEC-\nTRA in Figure 11. All the probing experiments of\nALBERT, BERT, and ELECTRA share the same\nset of hyperparameters and model architectures.\nWe observe a similar trend as ALBERT: the prob-\ning performance rises quite quickly and plateaus (or\neven slightly decay) afterward. We also found that\nperformance drop of those layers closer to ELEC-\nTRA’s output layers are highly observable, which\nmay spring from its discriminative pretraining na-\nture.\nE Downstream Evaluation\nE.1 Dataset Details\nWe provide detail statistics of downstream tasks’\ndataset in Table 6. We download GLUE dataset\nusing https://gist.github.com/W4ngatang/\n60c2bdb54d156a41194446737ce03e2e, and\ndownload SQuAD2.0 dataset from https:\n//rajpurkar.github.io/SQuAD-explorer/.\nE.2 Finetune Details\nWe use the code in https://github.com/\nhuggingface/transformers/tree/master/\nexamples/text-classification to run GLUE\nand use https://github.com/huggingface/\ntransformers/tree/master/examples/\nquestion-answering to run SQuAD2.0. We\nprovide detailed hyperparameters when we run\nGLUE benchmark and SQuAD2.0 in Table 7. We\nfollow Liu et al. (2019b) and Lan et al. (2019),\nﬁnetuning RTE, STS-B, and MRPC using an\nMNLI checkpoint when ﬁnetuning ALBERT. The\nnumber of parameters of all downstream tasks is\nclose to the original ALBERT model, which is\n12M.\nE.3 Downstream results of ALBERT (with\nSQuAD2.0)\nHere we provide performance of individual tasks in\nGLUE benchmark on development set in Figure 12,\nalong with performance of SQuAD2.0 (Rajpurkar\net al., 2018).\nE.4 Downstream performance of BERT and\nELECTRA\nWe use the same hyperparamters in Table7 to ﬁne-\ntune BERT and ELECTRA models. Except for the\nperformance of BERT on SQuAD2.0, all the other\nresults are comparable with those results ﬁnetuned\nfrom the ofﬁcial Google pretrained models. We can\nobserve from Figure 13 and Figure 12 that all three\nmodels’ performance on downstream tasks show\nsimilar trends: Performance skyrocketed during the\ninitial pretraining stages, and the return gradually\ndecays later. From Figure 13c, we also ﬁnd that\namong the three models, ALBERT plateaus the ear-\nliest, which may result from its parameter-sharing\nnature.\nF World Knowledge Development\nF.1 Dataset Statistics\nIn our experiment of world knowledge, we only\nuse 1-1 relations (P1376 and P36) and N-1 rela-\ntions (the rest relations in Table 8). Among those\nrelations, we only ask our model to predict object\n([Y] in the template in Table 8) that has only one\ntoken, following Petroni et al. (2019). From those\nrelations, we report world knowledge that behaves\ndifferently during pretraining in Figure 6: we se-\nlect the knowledge that can be learned during pre-\ntraining (e.g., P176), the knowledge that cannot be\nlearned during the whole pretraining process (e.g.,\nP140), the knowledge that was once learned and\nthen forgotten after pretraining (e.g., P138), and\nknowledge that kept oscillating during pretraining\n(e.g., P407). The statistics of all world knowledge\nevaluated are in listed in Table 8.\nF.2 Qualitative Results and Complete World\nKnowledge Results\nWe provide qualitative examples for Section 6 in\nTable 9. We also provide the complete results of all\nworld knowledge we use in Figure 14.\n6824\nLR BSZ ALBERT DR Classiﬁer DR TS WS MSL\nCoLA 1.00E-05 16 0 0.1 5336 320 512\nSTS-B 2.00E-05 16 0 0.1 3598 214 512\nSST-2 1.00E-05 32 0 0.1 20935 1256 512\nMNLI 3.00E-05 128 0 0.1 10000 1000 512\nQNLI 1.00E-05 32 0 0.1 33112 1986 512\nQQP 5.00E-05 128 0 0.1 14000 1000 512\nRTE 3.00E-05 32 0 0.1 800 200 512\nMRPC 2.00E-05 32 0 0.1 800 200 512\nSQuAD2.0 3.00E-05 48 0 0.1 8144 814 512\nTable 7: Hyperparameters for ALBERT in downstream tasks. LR: Learning Rate. BSZ: Batch Size. DR: Dropout\nRate. TS: Training Steps. WS: Warmup Steps. MSL: Maximum Sequence Length\nType Count Template\nP140 471 [X] is afﬁliated with the [Y] religion .\nP103 975 The native language of [X] is [Y] .\nP276 954 [X] is located in [Y] .\nP176 946 [X] is produced by [Y] .\nP264 312 [X] is represented by music label [Y] .\nP30 975 [X] is located in [Y] .\nP138 621 [X] is named after [Y] .\nP279 958 [X] is a subclass of [Y] .\nP131 880 [X] is located in [Y] .\nP407 870 [X] was written in [Y] .\nP36 699 The capital of [X] is [Y] .\nP159 964 The headquarter of [X] is in [Y] .\nP17 930 [X] is located in [Y] .\nP495 909 [X] was created in [Y] .\nP20 952 [X] died in [Y] .\nP136 931 [X] plays [Y] music .\nP740 934 [X] was founded in [Y] .\nP1376 230 [X] is the capital of [Y] .\nP361 861 [X] is part of [Y] .\nP364 852 The original language of [X] is [Y] .\nP37 952 The ofﬁcial language of [X] is [Y] .\nP127 683 [X] is owned by [Y] .\nP19 942 [X] was born in [Y] .\nP413 952 [X] plays in [Y] position .\nP449 874 [X] was originally aired on [Y] .\nTable 8: Relations used.\n6825\nWorld Knowledge Prediction\nRelation P38 P176\nQuery Nokia Lumia 800 was produced by\n[MASK].\nHamburg airport is named after\n[MASK].\nAnswer Nokia Hamburg\n100K the lumia 800 is produced by nokia. hamburg airport is named after it.\n200K nokia lu nokia 800 is produced by\nnokia.\nhamburg airport is named after ham-\nburg.\n500K nokia lumia 800 is produced by nokia. hamburg airport is named after him.\n1M nokia lumia 800 is produced by nokia. hamburg airport is named after him.\nTable 9: Example results of world knowledge evolution during pretraining. We can observe that model successfully\npredict the object in the Nokia example since 100K steps, and doesn’t forget during the rest pretraining process.\nOn the other hand, the model is only able to correctly predict Hamburg in the second example at 200K steps, and\nfailed to predict at other pretrain steps.\n6826\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Evaluation Result\nEvaluation result\nSRL\nPOS\nConst\nCoref\n(a) Probing results of ALBERT-base model\n0 200 k 400 k 600 k 800 k\nPretrain steps\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Evaluation Result\nEvaluation result\nSRL\nPOS\nConst\nCoref\n(b) Probing results of BERT-base uncased model\n0 150 k 300 k 450 k 600 k 750 k\nPretrain steps\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Evaluation Result\nEvaluation result\nSRL\nPOS\nConst\nCoref\n(c) Probing results of ELECTRA-base model\nFigure 11: Probing results of POS tagging, constituent\ntagging, semantic role labeling, and coreference resolu-\ntion, evaluated by micro F1 score.\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n20\n30\n40\n50\n60\n70\n80\n90Evaluation result\nMNLI\nMRPC\nSTS-B\nSST-2\nCoLA\nQNLI\nQQP\nRTE\nSQuAD2.0\nFigure 12: Performance of individual tasks in GLUE\nbenchmark, along with SQuAD2.0 result. Best result\ndurining pretraining marked with ‘x’. Evaluation met-\nrics: MRPC and QQP: F1, STS-B: Spearman corr., oth-\ners: accuracy. The result of MNLI is the average of\nmatched and mismatched. The result of SQuAD2.0\nis the average of F1 and EM scores. Performances of\nalbert-base-v1 and bert-base-uncased are marked with\n‘+’ and square, respectively.\n6827\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n20\n30\n40\n50\n60\n70\n80\n90Evaluation result\nMNLI\nMRPC\nSTS-B\nSST-2\nCoLA\nQNLI\nQQP\nRTE\nSQuAD2.0\n(a) GLUE and SQuAD2.0 performances of BERT\n0 150 k 300 k 450 k 600 k 750 k\nPretrain steps\n20\n30\n40\n50\n60\n70\n80\n90Evaluation result\nMNLI\nMRPC\nSTS-B\nSST-2\nCoLA\nQNLI\nQQP\nRTE\nSQuAD2.0\n(b) GLUE and SQuAD2.0 performances of ELECTRA\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n50\n55\n60\n65\n70\n75\n80\n85\n90Evaluation result\nELECTRA BERT ALBERT\n(c) GLUE scores of all three models\nFigure 13: Performance of individual tasks in GLUE\nbenchmark, along with SQuAD2.0 result. Best re-\nsult durining pretraining marked with circle. Evalu-\nation metrics: MRPC and QQP: F1, STS-B: Spear-\nman corr.,others: accuracy. The result of MNLI is\nthe averageof matched and mismatched. The result of\nSQuAD2.0is the average of F1 and EM scores.\n6828\n0 200 k 400 k 600 k 800 k 1 M\nPretrain steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\nFigure 14: Prediction of all world knowledge during pretraining.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6412457823753357
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5853006839752197
    },
    {
      "name": "Process (computing)",
      "score": 0.5711004734039307
    },
    {
      "name": "Natural language processing",
      "score": 0.5679700374603271
    },
    {
      "name": "Language model",
      "score": 0.5439874529838562
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4829218089580536
    },
    {
      "name": "Linguistics",
      "score": 0.42655134201049805
    },
    {
      "name": "Psychology",
      "score": 0.39326736330986023
    },
    {
      "name": "Cognitive science",
      "score": 0.3591575622558594
    },
    {
      "name": "Programming language",
      "score": 0.13914600014686584
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    }
  ],
  "cited_by": 2
}