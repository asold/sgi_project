{
  "title": "A Performance Study of LLM-Generated Code on Leetcode",
  "url": "https://openalex.org/W4399667811",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Coignion, Tristan",
      "affiliations": [
        "UniversitÃ© de Lille"
      ]
    },
    {
      "id": "https://openalex.org/A2745952746",
      "name": "Quinton, ClÃ©ment",
      "affiliations": [
        "UniversitÃ© de Lille"
      ]
    },
    {
      "id": "https://openalex.org/A2548940622",
      "name": "Rouvoy, Romain",
      "affiliations": [
        "UniversitÃ© de Lille"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6734314025",
    "https://openalex.org/W4313913687",
    "https://openalex.org/W1496637959",
    "https://openalex.org/W4362659486",
    "https://openalex.org/W4382704594",
    "https://openalex.org/W4308642031",
    "https://openalex.org/W4389518805",
    "https://openalex.org/W4384026634",
    "https://openalex.org/W4312438588",
    "https://openalex.org/W4288057765",
    "https://openalex.org/W4388858772",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W4225108562",
    "https://openalex.org/W4321018228",
    "https://openalex.org/W2774988049",
    "https://openalex.org/W4312091278",
    "https://openalex.org/W3198685994",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W4312295202",
    "https://openalex.org/W4308627320",
    "https://openalex.org/W4376167329",
    "https://openalex.org/W2593607031"
  ],
  "abstract": "International audience",
  "full_text": "A Performance Study of LLM-Generated Code on Leetcode\nTristan Coignion\nUniv. Lille, CNRS, Inria\nFrance\ntristan.coignion@inria.fr\nClÃ©ment Quinton\nUniv. Lille, CNRS, Inria\nFrance\nclement.quinton@inria.fr\nRomain Rouvoy\nUniv. Lille, CNRS, Inria\nFrance\nromain.rouvoy@inria.fr\nABSTRACT\nThis study evaluates the efficiency of code generation by Large\nLanguage Model s (LLMs) and measures their performance against\nhuman-crafted solutions using a dataset from Leetcode. We com-\npare 18 LLMs, considering factors such as model temperature and\nsuccess rate, and their impact on code performance. This research\nintroduces a novel method for measuring and comparing the speed\nof LLM-generated code, revealing that LLMs produce code with\ncomparable performance, irrespective of the adopted LLM. We also\nfind that LLMs are capable of generating code that is, on average,\nmore efficient than the code written by humans. The paper further\ndiscusses the use of Leetcode as a benchmarking dataset, the limita-\ntions imposed by potential data contamination, and the platformâ€™s\nmeasurement reliability. We believe that our findings contribute to\na better understanding of LLM capabilities in code generation and\nset the stage for future optimizations in the field.\nACM Reference Format:\nTristan Coignion, ClÃ©ment Quinton, and Romain Rouvoy. 2024. A Perfor-\nmance Study of LLM-Generated Code on Leetcode. In 28th International\nConference on Evaluation and Assessment in Software Engineering (EASE\n2024), June 18â€“21, 2024, Salerno, Italy. ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/10.1145/3661167.3661221\n1 INTRODUCTION\nLarge Language Model s (LLMs) have recently increased in popular-\nity, especially with the advent of ChatGPT [28]. While LLMs have\nbeen spreading over various application domains, such as text or\nimage generation, certain types of LLMs are being developed solely\nfor code-related purposes. These LLMs aim to assist the developers\nby saving time and effort through the generation of code, documen-\ntation, unit tests, etc. Many of these LLMs only come in a â€œraw\"\nform, that is, they do not integrate themselves into the developerâ€™s\ncoding process. These include models, such asCodeGen [27], Star-\nCoder [21], WizardCoder [24], CodeT5 [38], and Incoder [14].\nOn the other hand, some LLMs are already seamlessly integrated\ninto the developerâ€™s IDE as code assistants, like GitHub Copilot,1\nAmazon CodeWhisperer,2 and Tabnine.3\nThere has been a significant amount of work dedicated to com-\nprehending how these LLMs perform in various situations and\ndefining their limits. For instance, several works address the se-\ncurity of the code generated by such models [ 29, 30, 33] or the\n1https://github.com/features/copilot\n2https://aws.amazon.com/fr/codewhisperer/\n3https://www.tabnine.com/\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy\n2024. This is the authorâ€™s version of the work. It is posted here for your personal\nuse. Not for redistribution. The definitive Version of Record was published in 28th\nInternational Conference on Evaluation and Assessment in Software Engineering (EASE\n2024), June 18â€“21, 2024, Salerno, Italy , https://doi.org/10.1145/3661167.3661221.\nprevalence of bugs in the generations [20]. Many researchers are\nalso investigating how developers interact with LLMs and how such\nmodels fit into the programming workflow [7, 30, 34] There is also a\nbroad research effort to measure the actual efficiency of these LLMs\nby creating common benchmarks for comparison [5, 10, 17, 37, 43]\nor by actually measuring different qualities related to the genera-\ntions, such as the success rate [42] or the robustness of the model\nregarding variations [13].\nTo the best of our knowledge, there is no research work evaluat-\ning the performance of the code generated by LLMs. Yet, having\ncode that runs faster is an often sought-after characteristic of a\nprogram. Indeed, programming efficiency is paramount, especially\nwhen resources are scarce or programs are deployed on a large\nscale. In todayâ€™s context where the energy consumption of soft-\nware systems has become a major concern, improving software\nefficiency is particularly relevant since increasing the performance\nof a program can also lead to energy consumption reduction [2, 36].\nThe process of code optimization is lengthy and intricate, re-\nquiring careful attention and a certain level of expertise, especially\nwhen searching for the best-performing algorithm, selecting the\nmost appropriate data structure, or struggling with memory hier-\narchy. Yet, this process is necessary to identify opportunities for\nimprovement that may result in minor reductions in execution\ntime. LLMs can be used as a way to make this process easier,e.g., by\ngenerating performance-improving code edits [11, 15, 25]. Garg et\nal. [16] also presented RAPGen, a method for generating zero-shot\nprompts to enhance performance. However, while users seem to\nput a lot of trust in code generated by LLMs, they still have trouble\nreviewing it [33, 34], which can lead to slow code being shipped in\nproduction, especially if an LLM generates inefficient code.\nThe key contributions of this paper are as follow : (i) We study\nthe performance of the code generated by 18 LLMs on 204 prob-\nlems and investigate performance differences across models using\na novel method for measuring and comparing the performance of\nLLM-generated code. (ii) We compare the performance of the code\ngenerated by LLMs to the code written by humans. (iii) Inciden-\ntally, we evaluate the usability of Leetcode,4 a public repository of\nalgorithmic problems that we use as a dataset.\nFrom section 2 to section 4, we describe the tasks dataset and the\nmodels we selected, outline our experiment setup, and explain the\nmethodology we followed to analyze the obtained results, respec-\ntively. We report in section 5 on the results of our evaluation and\nprovide a critical discussion in section 6. Finally, section 7 presents\nthe related works, and section 8 concludes the paper.\n4https://leetcode.com/\narXiv:2407.21579v1  [cs.SE]  31 Jul 2024\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Tristan Coignion, ClÃ©ment Quinton, and Romain Rouvoy\n2 METHODOLOGY\n2.1 Research questions\nThis paper covers the performance of code generated by various\nLLMs. In particular, we aim to answer the following research ques-\ntions:\nRQ1: Can Leetcode be used as a dataset and a benchmark platform\nfor evaluating LLMs? Leetcode can serve as both a dataset of prob-\nlems and a tool to evaluate and measure solutions to the problems.\nParticularly, we study if the dataset is subject to recitation and if\nthe measures Leetcode provides are reliable.\nRQ2: Are there notable differences between the performance of the\ncode generated by different LLMs? LLMs differ greatly in terms of\ngenerating correct code, so we want to know if they also differ in\nterms of generating efficient code.\nRQ3: Is there an effect of the success rate and the temperature of\nthe LLM on the codeâ€™s performance? Having a higher temperature\ndecreases the capacity of the LLMs to generate valid code, so we aim\nto study if this also applies to the performance of the code. In the\nsame way, we also study if an LLM that is very good at generating\nvalid code on one problem, is going to generate efficient solutions\nto this problem.\nRQ4: How efficient are the solutions generated by the LLMs com-\npared to human solutions? Comparing the LLMs to a set of human-\nauthored solutions can provide insights into their position relative\nto humans in terms of code performance.\n2.2 Tasks & Dataset\nTask selection . The input tasksâ€” i.e., problems specified by a\npromptâ€”we consider to generate code from various LLMs has to\nmeet the following requirements:\nâ€¢A given problem, should offer multiple candidate solutions,\nwhose generated code performs differently. This ensures one\ncan observe differences across the various LLMs;\nâ€¢Generated solutions should exhibit variable execution times.\nGiven more complex inputs, one should differentiate O(ð‘›)\nfrom O(2ð‘›)and O(ð‘›2)algorithms.\nAs a result, task datasets, such as HumanEval or Mostly Basic\nPython Programming (MBPP), which are classically used when eval-\nuating LLMs for code assessments [ 1, 4, 10, 27], cannot be con-\nsidered for our purpose. Indeed, while they do provide unit tests\nto drive the generations, the size of the inputs in these unit tests\nremains small and fails to scale to appreciate performance issues.\nMoreover, the solutions that need to be generated are often very\nshort, which would lead to fewer possible variations between imple-\nmentations. Also, the fact that many problems are not algorithmic\nby nature makes them less prone to inefficient practices and perfor-\nmance variation. To face such issues, we used input prompts that\nwere built from Leetcode problems. Leetcode is an online judge plat-\nform that suggests programming problems to registered users. It\naddresses the above limitations as it provides algorithmic problems\nwith varying levels of difficulty and test cases with large input sizes.\nLeetcode also exposes a GraphQL API5 to fetch relevant metadata\n5https://pypi.org/project/python-leetcode/\non the problems, such as exercise instructions, code snippets con-\ntaining the signature of the function to generate, as well as the\ndifficulty and topics of the problem.\nWe followed an experimental design similar to the one used by\nDÃ¶derlein et al. [13], while using a different set of Leetcode ques-\ntions. To avoid data contamination, which happens when an LLM\nis tested on data it was trained on,6 we only considered problems\nthat were published after January 1ð‘ ð‘¡, 2023. As all the LLMs (except\nGitHub Copilot) we evaluated were trained using datasets older\nthan these problems, we avoid any data contamination. As these\nproblems are published by Leetcode in the context of programming\ncompetitions, they are always original. However, GitHub Copi-\nlot being an online closed-source tool, one cannot tell whether\nit underwent training with the problem set we employed. Our set\nwas composed of 204 problemsâ€”labeled as 56 easy problems, 104\nmedium problems, and 44 hard problems, as classified by Leetcode\nupon the problemâ€™s publication.\nTo answer RQ1, we also performed our experiment a second\ntime on the set of questions used by DÃ¶derlein et al. [13], which\nis composed of 300 problems (95 easy, 105 medium, and 100 hard)\nfrom the most liked problems of Leetcode. We will refer to this\ndataset as the \"old\" dataset, and to our dataset of problems published\nduring 2023 as the \"new\" dataset. Code generation was performed\nin Python for its ease of use and because of the prevalence of\nPython-written datasets for evaluating LLMs [5, 10].\nInput prompts. The instructions given by Leetcode for each\nprogramming problem contain (i) the description of the problem,\n6You can find more details on data contamination here.\n# Start of the input prompt\n\"\"\"\nGiven an integer array nums, return all the triplets `[nums[i],\nnums[j],nums[k]]` such that `i != j`, `i != k`, and `j != k`,\nand `nums[i] + nums[j] + nums[k] == 0`.\nâ†©â†’\nâ†©â†’\nNotice that the solution set must not contain duplicate triplets.\n\"\"\"\nclass Solution:\ndef threeSum(self, nums: List[int]) -> List[List[int]]:\n# End of the input prompt\n# Start of the generated code\nnums.sort()\nres = []\nfor i in range(len(nums) - 2):\n...\n# End of the generated code\n# Start of the benchmarking code\ndef check():\nSolution().threeSum([82597, -9243, 83030, ...])\nSolution().threeSum([0, 0, 0, 0, ...])\nSolution().threeSum([0, 0, -1, -1, ...])\nimport pytest\n@pytest.mark.benchmark(group=\"3sum\")\ndef test_3sum_generated_1(benchmark):\nbenchmark(check)\n# End of the benchmarking code\nFigure 1: Example of problemâ€™s input prompt, generated code,\nand benchmarking code.\nA Performance Study of LLM-Generated Code on Leetcode EASE 2024, 18â€“21 June, 2024, Salerno, Italy\n(ii) examples of inputs and outputs, and (iii) constraints on the\ninput data. To build the prompts inputted to the LLMs, we chose\nto only include the description of the problem. This choice aimed\nto maintain the promptâ€™s conciseness and ensure compliance with\npotential LLMsâ€™ length restrictions. Indeed, LLMs are constrained\nby a context size, imposing the maximum number of tokens they\ncan process at once, encompassing both the prompt and the answer.\nIn our study, the majority of LLMs impose a context window of\n1024 tokens. Figure 1 illustrates a descriptive prompt (enclosed\nwithin triple quotes) along with a solution generated using this\nprompt and the corresponding benchmark instructions. Leetcode\noccasionally includes an additional comment in the prompt, indi-\ncating the available methods of the elements passed to the solution\n(e.g., binary trees). While adding examples in the prompt could po-\ntentially increase the likelihood of generating correct solutions [13],\nwe chose not to include them due to the complexity of representing\nLeetcodeâ€™s data structures (such as arrays, graphs, linked lists, trees,\netc.) in textual form. On Leetcodeâ€™s website, data structures are\ntextually represented using an array notation format with brackets\n(e.g., â€œ[1, 2, 3]\"), which might be misleading for an LLM working\nwith data structures that differ from arrays or lists.\nCanonical solutions. Each problem was matched with a single\nvalid solution, written by a human and fetched from various sources.\nThese solutions, referred to as â€œcanonical solutions\", are considered\nas baselines during the benchmarking process, although they may\nnot represent the entirety of human-written solutions. They were\nused to assess the stability of the measuring process. Most of the\ncanonical solutions were fetched from the WalkCC repository of\nLeetcode solutions.7 When one question did not come up with any\nsolution in Python from this repository, we selected one solution\nproposed by the Leetcode community among the most upvoted\nones (starting from the one with the most upvotes), which are also\npublicly available on the Leetcode website. These modifications\nonly included changes to the function name, variable names, and\ntype hints except for one specific case, where a recursive solution\nprovided by WalkCC was replaced by an iterative one from the\nLeetcode community because of the stack limit on our local setup.\nWe ended up with one Python-based canonical solution for each\nproblem of our dataset.\nTest cases. Testing generated solutions is a twofold process, as\nboth the correctness and scalability (performance) of each solution\nmust be checked. First, to ensure that generated solutions are cor-\nrect, we provide such solutions to the Leetcode online judge system,\nwhich in turn validates them based on its test suites. Second, we exe-\ncute the valid solutions with input data to assess their performance.\nTo retrieve such input data, we crawled through the problemsâ€™\ninstructions and extracted two to three examples of inputs and\nexpected outputs provided by Leetcode. However, such inputs were\ntoo small to exhibit significant performance differences, e.g., arrays\ncontaining only two to five elements. To execute the generated\nsolutions with larger input data, we took advantage of Leetcodeâ€™s\njudge system that returns the inputs and expected output of the\nfirst failed test of a test suite. We noticed that Leetcode tests if the\nsubmitted solution is inefficient by using a timeout system. Since all\nselected problems are algorithmic by nature, one simple way to put\n7https://github.com/walkccc/LeetCode\na heavier load on their implementations is to increase the size of the\ninputs. For every problem, we thus submitted a modified version\nof a canonical solution that failed only when the size of the first\nparameter exceeded a certain threshold. We then set this threshold\nmanually multiple times for every problem to extract three different\ninputs for each problem, resulting in more than 150 MB of fetched\ninput data with most inputs having over 105 elements.\n2.3 LLMs Under Study\nOur empirical study covers a total of18 LLMs, specifically designed\nfor coding purposes. We selected the 18 popular code LLMs from\nHugging Face,8 as well as GitHub Copilot, which is an online\nclosed-source code assistant. The LLMs were selected based on the\nnumber of downloads and likes they exhibited. We chose GitHub\nCopilot to offer a comparison between a commercial LLM and\nopen-source LLMs, but did not choose any other GPT models from\nOpenAI because of their cost. Table 1 summarizes all the LLMs\nconsidered in this study. This includes variants of the same base\nLLMâ€”i.e., models with varying sizes (in billions of parameters) or\ndifferent training data, such as variants of CodeGen, InCoder, and\nCodeT5. LLMs belonging to the same family are models closely\nrelated in terms of training data and method. We first performed our\nexperiment in March 2023 with a subset of the models presented\nhere, with the \"old\" dataset. We then performed it a second time in\nSeptember 2023 with all the models and the \"new\" dataset.\nLLM Model Model family Size RQ1\nGitHub Copilot Codex 11 âœ“\nCodeGen-Mono 6B CodeGen 6 âœ“\nCodeGen-Mono 2B CodeGen 2 âœ“\nCodeGen-Mono 350M CodeGen 0.35 âœ“\nCodeGen2.5-7B-mono CodeGen2.5 7\nCodeGen2.5-7B-instruct CodeGen2.5 7\nCodeLlama-7B-instruct CodeLlama 7\nCodeLlama-7B CodeLlama 7\nCodeLlama-7B-python CodeLlama 7\nCodeLlama-13B-instruct CodeLlama 13\nCodeLlama-13B-python CodeLlama 13\nreplit-code-v1-3b replit-code 3\nWizardCoder-pythin WizardCoder 7\nSantaCoder Santacoder 1.1 âœ“\nStarCoder StarCoder 15.5\nInCoder 6B Incoder 6 âœ“\nInCoder 1B Incoder 1 âœ“\nCodeParrot Codeparrot 1.5 âœ“\nTable 1: LLMs considered in our study. Models with the RQ1\ncheckmark were also evaluated on the \"old\" dataset. Size is\nin billions of parameters\n3 EXPERIMENT SETUP\nThis section describes our experiment setup to generate and vali-\ndate the solutions produced by the LLMs. First, we describe how\nsolutions were generated from each LLM. Then, we outline the\nthree-step process used to filter invalid solutions. Finally, we ex-\nplain how the run time of the generated solutions was measured.\n3.1 Code Generation\nWe generated 10 solutions for each problem from our dataset by\nvarying the temperature of the LLMs used to generate them. Specif-\nically, we considered 6 different temperatures (0.1, 0.2, 0.4, 0.6,\n8https://huggingface.co\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Tristan Coignion, ClÃ©ment Quinton, and Romain Rouvoy\n0.8, and 1.0). As Copilotâ€™s temperature cannot be configured, we\nused its default temperature for generating all the problems.\nGenerating code with GitHub Copilot. Automatically gener-\nating with GitHub Copilotfor a reproducible experiment proved\nto be a difficult task. Firstly, the code suggestion feature ofCopilot\nactivates when typing in a text editor with the installed Copilot\nplugin. Additionally, GitHub Copilotproduces code based on a\ncontext that encompasses the current file and the files previously\naccessed by the user, impacting the generated solutions. Lastly, we\nnoticed a caching mechanism on the GitHub Copilotserver side,\nwhich resulted in very similar or identical solutions if we gener-\nated multiple solutions for a given problem in the same session.\nTo address these issues, we used a generation method similar to\nthe one used by DÃ¶derlein et al. [13] by instrumenting the GitHub\nCopilot Neovim plugin9 and restarting the plugin between every\ngeneration to avoid the caching effect. This method allowed us to\nautomatically generate solutions in a quick and isolated fashion. On\ntop of that, GitHub Copilotprovides 2 means of generation:inline\ngenerations (the suggested code is integrated with the editor) and\npanel generations (Copilot generates at most 10 completions and\ndisplays them on a panel next to the editor). We chose to exclusively\nuse inline generations, as panel generations yielded worse results\nin terms of functional correctness than inline generations.\nGenerating code with open-source models . Regarding the\nopen-source models, we generated solutions by deploying the mod-\nels on servers provided by the Grid5000 platform [6]. We used the\nDeepspeed library to make the generation process faster and fit\nlarger models on our GPUs. Concerning the sampling, we used the\nsame methods as Chen et al. [10] and used nucleus-sampling [18]\nwith top ð‘ = 0.95. The maximum number of tokens to be generated\nwas set to 600. This is because the LLMs we used have a limited con-\ntext size of 1024 tokens (prompt included). Thus, to avoid exceeding\nthe context size, we had to limit the number of tokens to gener-\nate. We also verified it did not significantly impact the functional\nvalidity of the LLMs.\nIn total, we generated 2, 040 solutions with Copilot and 12, 240\nwith each of the eight other models, resulting in 210, 120 generated\nsolutions overall.\n3.2 Validation\nEach generated solution was tested to ensure its functional correct-\nness following a three-step process. At every step, if a solution was\nfound to be invalid, it was excluded from subsequent stages of the\nexperiment. The process was as follows:\ni) Local validation . We filtered out code generations that included\neasy-to-spot errors, such as syntax errors or runtime errors, by\nusing the small inputs we fetched earlier (see Section 2.2 - Test\ncases). While this step was not strictly necessary, it quickly reduced\nthe number of solutions to be validated in the next step;\nii) Leetcode validation . Next, we submitted the solutions to the\nLeetcode judge system using the Leetcode GraphQL API, where\nthey underwent a rigorous test suite managed by Leetcode. We kept\nthe solutions that passed all the test cases or exceeded Leetcodeâ€™s\nallocated time limit. The latter was kept to ensure that correct\nsolutions that were too slow remained included in the benchmark;\n9https://github.com/github/copilot.vim\niii) Exclusion of timeouts and other errors . Finally, we excluded the\nsolutions that reported errors when executed with our benchmark-\ning setup using the large inputs fetched beforehand. We invalidated\nthe code generations that took more than 10 seconds to run or\nraised an error. Most of the errors raised in this step were recursion\nerrors caused by differences between Leetcodeâ€™s Python interpreter\nand ours. Indeed, Leetcodeâ€™s seemed to have a higher recursion\nlimit than ours, which we set to 10, 000 instead of the default 1, 000\n(we could not manage to set it any higher). Additional errors oc-\ncurred because we developed our helper classes differently from\nLeetcodeâ€™s implementation. Although these classes are provided\nby Leetcode during the submission process, they are not publicly\navailable. Out of the 4, 930 invalidated solutions that were caught\nin this step, 4, 863 (98.6%) were due to timeouts, 20 (0.04%) to recur-\nsion errors, and 47 (0.1%) to other errors. Following this validation\nprocess, the initial set of 210, 120 generated solutions was pruned\ndown to 7, 481 (3.6%) valid solutions remaining across the 18 LLMs.\n3.3 Measuring run time\nWe measured the performance as the run time of the generated\nsolutions using pytest-benchmark,10 which runs pytest unit tests\nmultiple times to obtain run times statistics. The measurements\nwere performed using parameters that ensured each solution ran at\nleast 10 times and for at least 1 second in total. We did not perform\nwarm-up runs of the benchmarks, as we did not notice any signifi-\ncant difference in the measured time during preliminary testing. To\nfacilitate the measurement protocol, the generated solutions were\nsorted into â€œruns\", based on the specific LLM and temperature that\nwere used during the code generation. Within each run, which was\ndefined by a unique combination of model and temperature, the\nsolutions were measured in sequence during a single program exe-\ncution. Furthermore, in every run, we added the canonical solutions\nwe previously collected, which would run alongside the generated\nsolutions. This approach ensured that the same canonical solutions\nwere executed in every run, allowing us to maintain measurement\nstability. Specifically, we calculated the standard deviation of the\ncanonical solution run times across all runs, thus providing a reli-\nable measure of the variability of the measurement protocol. We\nobserved that over 96% (196 out of 204) of the canonical solutions\nhad a standard deviation lower than 1/10ð‘¡â„Ž of their average run\ntime, which we deemed to be an acceptable level of variation.\nThe cluster we used to run the benchmark was the chiclet cluster\nof the Grid5000 testbed.11 It hosts 2 AMD EPYC 7301, with 16 cores\nper CPU and 128GB of memory. When using the node, all the cores\nof both CPUs were reserved, but only one was used at a time to\nmaximize the stability of the measurement protocol.\n3.4 Replication package\nAll the artifacts of this study, including our results, code, and\ndatasets, are available in the following public repository: https:\n//zenodo.org/doi/10.5281/zenodo.7898304.\n10https://github.com/ionelmc/pytest-benchmark\n11http://grid5000.fr\nA Performance Study of LLM-Generated Code on Leetcode EASE 2024, 18â€“21 June, 2024, Salerno, Italy\n4 DATA ANALYSIS\nIn this section, we describe the methods we adopted to analyze our\nresults. These methods fall into one of the two following categories:\nfunctional correctness and code performance.\n4.1 Functional Correctness\nThe functional correctness of an LLM defines how much the LLM\noutputs code conforming to the program contract (as specified\nby the input prompt). To evaluate the functional correctness of\nour LLMs, we computed their pass@k metrics with ð‘˜ = 1 and\nð‘˜ = 10, using the unbiased estimator proposed by Chen et al. [10].\nThe pass@k unbiased estimator which, from k samples produced,\nconsiders the test as successful if one of these samples passes all the\ntests, is computed as follows (with ð‘› the total number of samples, ð‘\nthe number of correct samples and E the expected value):\nð‘ð‘Žð‘ ð‘  @ð‘˜ := E\nð‘ƒð‘Ÿð‘œð‘ð‘™ð‘’ð‘šð‘ \n\"\n1 âˆ’\n\u0000ð‘›âˆ’ð‘\nð‘˜\n\u0001\n\u0000ð‘›\nð‘˜\n\u0001\n#\n(1)\nAs Chen et al. [10] suggest, we calculated the pass@k for each\ntemperature when evaluating an LLMâ€™s functional correctness and\nconsidered the best one as the pass@k for that LLM.\n4.2 Code Performance\nTo measure the code performance, we considered three different\nmetrics. First, we used the memory usage reported by Leetcode.\nThen, we computed the median of the run times measured bypytest-\nbenchmark for every generated solution. Lastly, to compare the\nLLMs solutions to human-submitted solutions, we also used the\nrank reported by Leetcode when validating the solution. This rank\nis a number between 0 and 100 that indicates the share of submitted\nsolutions that are slower than the current solution (e.g., if a solution\nhas a rank of 90, it is faster than 90% of the submitted solutions on\nLeetcode).\nTo assess the LLMsâ€™ performances, we conducted pairwise com-\nparisons as follows: For each pair of LLMs, we identified problems\nwhere both models generated more than 5 valid solutions. For each\nidentified problem, we conducted a Student ð‘¡-test on the mean run\ntime of the generations to determine if there was a significant dif-\nference. Then, for each pair A-B of LLMs, we computed the ratio\nof problems where Aâ€™s code was significantly faster than Bâ€™s and\nwhere Bâ€™s code was significantly faster than Aâ€™s.\n5 RESULTS\nIn this section, we summarize the key observations from our exper-\niment and answer our research questions. In Table 2, you can also\nfind the functional validity results of the different LLMs on both of\nour Leetcode datasets. Our results are also available in the form of\na companion notebook in our replication package. The companion\nnotebook offers more insight into the results and additional graphs.\n5.1 RQ1: Can Leetcode be used as a dataset and a\nbenchmark platform for evaluating LLMs?\n5.1.1 Can Leetcode problems be adopted as a dataset for LLM gen-\neration? As on can observe in Figure 2, the generated codes exhibit\non a significant drop in functional correctness between the two\nLLM Model Pass@1 Pass@10\nStarCoder 0.095 0.132\nCodeLlama-13B-python 0.093 0.201\nGitHub Copilot 0.092 0.196\nCodeLlama-7B-instruct 0.082 0.191\nCodeLlama-13B-instruct 0.078 0.206\nWizardCoder-python-7B 0.075 0.157\nCodeGen2.5-7B-mono 0.066 0.147\nCodeGen2.5-7B-instruct 0.062 0.142\nCodeLlama-7B-python 0.047 0.172\nCodeGen-6B-mono 0.045 0.113\nCodeGen-2B-mono 0.038 0.103\nreplit-code-v1-3b 0.025 0.083\nInCoder-6B 0.021 0.064\nSantaCoder 0.015 0.064\nCodeLlama-7B 0.014 0.015\nInCoder-1B 0.012 0.039\nCodeGen-350M-mono 0.007 0.039\nCodeParrot 0.002 0.015\nTable 2: Functional validity of the LLMs on Leetcode (by\ndecreasing pass@1, higher is better)\nFigure 2: Average pass@1 of the evaluated LLMs for every\ndifficulty and dataset, with 95% confidence interval (higher\nis better)\ndatasets. The difference here is pretty staggering for every tested\nLLM, reporting on a tenfold decrease in pass@k. We believe this\nissue may stem from data contamination in the old dataset. Data\ncontamination occurs when an LLM is assessed on data that was\nincluded in the training dataset, introducing bias into the evalua-\ntion process. In our case, a significant number of questions in the\nold dataset are widely known and have been extensively shared\non GitHub. For instance, a search for the prompt of the \"3sum\"\nLeetcode problem on GitHub yields approximately 4.000 matches\nin public repositories. These questions are also old enough to likely\nbe included in the training datasets of the LLMs under study, as\nthe majority of their training datasets have a cut-off date between\n2021 and 2022. Due to this data contamination, LLMs tend to recite,\nreproducing verbatim source code when generating solutions. This\nphenomenon is more pronounced when the prompt is highly spe-\ncific and lacks contextual information, as seen in Leetcode prompts\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Tristan Coignion, ClÃ©ment Quinton, and Romain Rouvoy\nFigure 3: Coefficicent of variation of the time measured by\nLeetcode and locally for every problem using canonical solu-\ntions\nthat closely match GitHub repositories. The observed shift in func-\ntional validity between the two datasets could also arise from a\ngenuine difference in the difficulty of the questions within each\ndataset. However, quantifying this last hypothesis proves to be\nchallenging.\n5.1.2 Are Leetcode measurements reliable? Run time. As reported\nin Figure 3, the coefficient of variation of the Leetcode measures\n(0.089) is slightly higher than the coefficient of variation of the local\nmeasures (0.035). This suggests that Leetcodeâ€™s measuring setup is\nless suited to ensure accurate benchmarks.\nWe also study the correlation between our local measurements\nand Leetcodeâ€™s, and we notice two issues: (1) the times measured\nlocally and by Leetcode only slightly correlate on average ( 0.28).\nFor some problems, the measures are highly correlated ( > 0.8)\nwhile, for others, they are almost not (< 0.2). This is more apparent\nwhen we look at the scatter plot showing the measures of some\nproblems in Figure 4. In this problem, there are four clusters of\ngenerations with a different locally measured time, but the clusters\nare indiscernible in terms of Leetcode time. This could be due to\ntwo main reasons. Firstly, as previously discussed, the variance of\nthe measures from Leetcode is higher and as such, there is much\nmore noise in the measures, decreasing the precision. Secondly,\nthe tests we employed may be more focused on performance test-\ning than Leetcodeâ€™s. Notably, our test suite comprises only three\ntests featuring significantly large inputs, potentially accounting for\ncertain disparities in the results.\nAlthough still usable, relying on the time reported by Leetcode in-\ntroduces some limitations due to its higher variance. Consequently,\nLeetcodeâ€™s measures cannot allow us to discern the differences in\nrun time between different code implementations as precisely as\nlocally measured time.\nMemory usage. While we did not measure the memory our-\nselves, we observed the variation of the memory usage measure\nprovided by Leetcode. We notice that the memory usage for the\nFigure 4: Scatter plot of the measures done by Leetcode and\nlocally for every generation for the problem \" Difference be-\ntween element sum and digit sum of an array \". Orange points\nare from multiple measures of the same canonical solution\nand serve as visual references for the measurement error\nsame solutions decreases over time. There is indeed a slight corre-\nlation of -0.24 between the day of the year we tested our solution\nand the memory usage. The fact that the memory usage measure\nevolves renders comparisons between LLMs harder. While we could\ntheoretically offset the memory usage when we detect changes over\ntime, it would require testing the canonical solutions alongside the\ngenerated one on Leetcode.\nLeetcode rank. The time ranking that Leetcode returns when\nwe test a solution represents the share of submitted and valid so-\nlutions that are slower than ours. While this could be a great tool\nto rank LLMs among human-submitted solutions, we find that the\nranking is heavily affected by our submissions and time. As you can\nsee in Figure 5, the overall rank of the LLMs we tested decreases\nover time. To verify this, we tested GitHub Copilottwice, once\nas the first LLM, and a second time after testing all the LLMs. The\nfirst test of Copilot has an overall rank of 77, and the second test\nranks down to 54, despite it being tested with the same solutions.\nThis effect of the rank evolving becomes obvious when you con-\nsider that the rank is determined using all the previously accepted\nsolutions, including ours. This means that by testing thousands of\nour solutions on Leetcode, we are actively changing the ranks of\nfuture tests.\nRQ1: The evaluation of LLMs using Leetcode questions as a\ndataset presents some challenges. Although Leetcodeâ€™s ques-\ntions could serve as a valuable dataset akin to HumanEval,\nlimitations arise due to the constraint that only the prob-\nlems published after the LLMâ€™s training dataset formation\nare usable for evaluating the LLM in question. This creates\npotential difficulties in reproducibility, particularly as new\nLLMs emerge, especially if they do not exclude Leetcode prob-\nlems from their training datasets [ 19]. Additionally, while\nLeetcodeâ€™s provided metrics, such as run time, memory usage,\nA Performance Study of LLM-Generated Code on Leetcode EASE 2024, 18â€“21 June, 2024, Salerno, Italy\nFigure 5: Scatter plot of LLMâ€™s rank and date they were tested\non Leetcode. The two models in red are the same model tested\non different dates\nand rank may offer practicality in various scenarios, their us-\nability and reliability are questioned when compared to more\ntraditional measurement methods. The presence of these chal-\nlenges emphasizes the need for careful consideration and\nscrutiny when adopting Leetcode to evaluate LLMs.\n5.2 RQ2: Are there notable differences in\nperformances between LLMs?\nThe pairwise comparison depicted in Figure 6 reveals subtle distinc-\ntions in the performances of various LLMs. Notably, some models,\nsuch as StarCoder and the CodeLlama model with 13B parameters\nspecialized in Python, consistently exhibit slightly superior results\ncompared to others. Despite these observed variations, the mean\nCohenâ€™s ð‘‘ effect size measures a mere 0.024, a statistically insignif-\nicant magnitude. This suggests that the practical impact of these\ndifferences on the mean speed of code generation is remarkably\nsmall. For instance, when comparing CodeLLama-13-instruct and\nCodeGen25-7B-mono, CodeLLama outperforms the latter in a sta-\ntistically significant manner in 3 problems out of 8. However, it\nis crucial to note that the mean performance difference between\nthese models is a mere 0.02 standard deviation. It thus seems that\nimproving an LLM in terms of functional validity does not signifi-\ncantly impact the performance of the code it generates. This may\nbe due to different factors, such as the fact that most LLMs share\nthe same datasets or that they are trained to produce valid code and\nnot fast code. Improving the performance of an LLM could be done\nby curating a training dataset of only efficient code and fine-tuning\none of the foundational models we used, or by using reinforcement\nlearning to \"teach\" the model to produce better code. Madaan et al.\nproduced an LLM that could improve the performance of code [25].\nThis LLM could be leveraged in a generation pipeline to directly\nimprove the generated code.\nFigure 6: Number of problems where an LLM (row) is better\nthan another (column)\nRQ2: Our analysis uncovers statistical differences in the per-\nformance of generated code among different LLMs. However,\nthe effect size, as measured by Cohenâ€™s ð‘‘, is so negligible that\nit raises questions about the practical significance of these\ndifferences. Despite some models consistently outperforming\nothers, the overall impact on the mean efficiency of LLM-\ngenerated code appears to be minimal.\n5.3 RQ3: Is there an effect of the functional\nvalidity of the LLM and its temperature on\nthe generated codeâ€™s performance?\nFunctional validity. When calculating for every problem the cor-\nrelation between the success rate of the LLM that generated the\nsolution and the run time of the solution, we find that there is\nonly a very slight negative correlation (âˆ’0.08) between the success\nrate and the performance. There is close to no correlation (âˆ’0.11)\nobserved between the success rate of the model and the variation\nin the performance of the generated code.\nTemperature. There is no correlation (0.05) observed between\nthe temperature of the generations and the performance of the\ngenerated code. This means that the temperature does not affect\nhow fast the solutions are. However, we observe that temperature is\nmoderately correlated (0.41) with higher variations in performances.\nThis means that higher temperatures tend to increase the variation\nin performance across generations. The complete distribution of the\ncorrelation for each of the 24 problems can be seen in Figure 7. So,\nwhile increasing the temperature leads to a lower success rate [13],\nit can help find a faster solution with an extended exploration of\ngenerations. The fact that the temperature increases the variation\nin performances comforts the idea that higher temperatures lead\nto more diverse outcomes.\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Tristan Coignion, ClÃ©ment Quinton, and Romain Rouvoy\nFigure 7: Distribution of correlations for every problem be-\ntween the temperature and the variation of the performance.\nThe red line is the median\nRQ3: Our analysis of LLMs indicates that the quality of gen-\nerated code does not have a substantial impact on its perfor-\nmance. However, we observed that modifying temperature\nsettings within an LLM significantly affects the diversity of\ncode performances produced. This implies that, while code\nvalidity may not be a decisive factor in performance, adjusting\ntemperature settings can be a valuable strategy to enhance\nthe variety of outcomes in code generation processes.\n5.4 RQ4: How fast are LLMs compared to\nhumans ?\nAs previously stated, the Leetcode time ranking evolves, so we\nchose to compare the second model we tested on Leetcode with\nhumans (Copilot being the first, it did not have enough generations\noverall because of its lack of a temperature setting). The results of\nthis comparison are depicted in Figure 8. The comparison is done\nusing the Leetcode ranking, with the assumption that most of the\nprevious submissions were made by humans.\nWe observe in Figure 8 that the solutions generated from LLMs\nare faster than most previous submissions with a mean rank of 73%,\nand that it even generated some solutions that were faster than 95%\nof the previous submissions.\nRQ4: It seems that the LLMs are faster than most of the human\nsolutions on Leetcode, on average. If the LLM we tested were\nin an actual competition, his valid solutions would be on\naverage faster than 73% of the other solutions on Leetcode.\n6 DISCUSSION\nIn this section, we discuss the results reported in the previous\nsection, their implications, and the limitations of our study.\nFigure 8: Distribution of the ranking for the CodeGen-6B-\nmono model\n6.1 Discussion of the results\nOn the Leetcode measures and usability. The data contamina-\ntion issue we unveiled poses a significant challenge in the evaluation\nof LLMs, as it prevents an accurate evaluation of their real perfor-\nmances. Because Leetcodeâ€™s problems are not filtered from the\ntraining datasets, as research on LLMs continues, even the newer\nproblems might contaminate future training datasets, thus render-\ning reproduction of our study harder [ 19]. This conclusion also\nholds for any study using Leetcode as an evaluation dataset, such\nas [8, 13, 26]. However, we believe that the methodologies employed\nand the conclusions drawn would likely hold validity with alterna-\ntive sets of questions from Leetcode or other performance-oriented\ndatasets. This suggests that future studies seeking to replicate our\nfindings would primarily need to change the dataset employed\nfor assessing LLMs. Addressing the data contamination concern\ncould involve leveraging pre-filtered evaluation datasets, such as\nHumanEval, already separated from the training processes, and\nrepurposing them into performance evaluation datasets.\nOn functional correctness . The ranking of the functional cor-\nrectness of the LLMs is consistent with the previous evaluations of\nthe LLMs onHumanEval [1, 4, 10, 21, 24, 27, 32]. However, it seems\nthat InCoder performs worse than presented in its introductory\npaper [14], which may be due to our experimental protocolâ€”using\nit only for left-to-right generation instead of infilling like it was\nbuilt for.\nWe observe that Leetcodeâ€™s problems seem harder for the LLMs\nto solve than HumanEvalâ€™s problems. Indeed, StarCoder, the model\nthat performed the best with a pass@1 of 0.09, had a pass@1 of\n0.408 on HumanEval [21]. We believe this is due to multiple factors.\nFirst, our Leetcode prompts and expected solutions are longer than\nin HumanEval (the average length of solution in HumanEval is\n180 characters vs 425 characters in our dataset), thus increasing the\nchance of a generation to fail. Indeed, having to generate more code\ncan lead to a higher chance of making a mistake, as shown by a\ncorrelation of âˆ’0.30 between the solutionâ€™s length and the success\nrate of the problem. Second, Leetcode problems come from pro-\ngramming competitions and need a lot of thinking to be solved. The\nA Performance Study of LLM-Generated Code on Leetcode EASE 2024, 18â€“21 June, 2024, Salerno, Italy\ncausal generation of the LLMs does not allow a \"thinking\" process\nto happen, which for harder problems causes a drop in functional\nvalidity. This could be solved by making the LLMs mimic human\nthinking with methods, such as Chain-of-Thought prompting [39].\nOn the performance of generated code . To the best of our\nknowledge, our methodology for evaluating LLMs based on the per-\nformance of generated code is novel and could serve as a benchmark-\ning approach for future studies involving new LLMs and datasets.\nWhile we aimed for a singular performance score for LLMs, similar\nto pass@k, the varying rates at which LLMs generate valid solu-\ntions posed a challenge, leading us to employ pairwise comparisons.\nAn improvement to our method could involve using an optimal\nsolution as a reference point and comparing the speed of each gen-\nerated solution to the optimal solutionâ€™s speed. Speed would thus\nbe expressed as a factor relative to the optimal time, addressing\nthe issue of problems having different time scales and potentially\nlaying the groundwork for a more comprehensive \"performance\nscore. \"\nThe low success rate of LLMs on Leetcode posed a considerable\nchallenge for their comparison. Among the 204 problems, only 24\nhad (i) valid solutions from at least 10 different models (out of the 18\nassessed LLMs) and (ii) at least 10 valid solutions in total (see to the\ncompanion notebook). This low success rate complicated pairwise\ncomparisons, as numerous models could not be compared due to\nan insufficient number of common problems with a substantial\nnumber of generated solutions.\nOur discoveries offer valuable insights for developers in their\nchoice of LLMs. For example, when developers are considering an\nLLM for tasks like code generation, such as with GitHub Copilot,\nthe performance of the generated code may be an important con-\ncern. With our findings, developers can be assured that there is no\nsignificant variance in the performance of code generated by dif-\nferent LLMs. This means that if they aim to have fast code, thereâ€™s\nno necessity to consistently opt for the largest model available;\ninstead, a smaller one suffices. We also hope that our findings will\nincentivize further research on building new LLMs that produce\neven more efficient code.\n6.2 Limits and Threats to Validity\nRegarding functional correctness, although we did our best to gen-\nerate code in the most optimal conditions, some changes to the\ninput prompt (i.e., adding examples and constraints), or the config-\nuration of the models may have changed the performance of the\nLLMs. However, we believe that this should not significantly impact\nthe validity of our experiment, as all the models were configured\nsimilarly.\nGitHub Copilotbeing a closed-source tool, it might be retrained\nwithout the communityâ€™s knowledge, which could potentially lead\nto a modification in the toolâ€™s performance in upcoming experi-\nments.\nThe majority of our validation and benchmarking process relied\non Leetcodeâ€™s test suite and online judge system. Thus, it is possible\nthat some big test cases we extracted for the benchmarking favored\nsome types of implementations over others. We mitigated this issue\nby systematically fetching three test cases and by having a large\ndataset of problems to generate from. During our experiments, we\nalso tried to generate plausible inputs using random generators,\nbut this method did not yield satisfying results because randomly\ngenerating data structures with specific shapes, or properties (e.g.,\ngenerating valid regular expressions) is difficult to achieve.\nOne gap in our study is that we do not consider memory usage\nat all when studying the LLMs. This is because our benchmarking\nsetup did not allow for memory monitoring and doing otherwise\nwould have cost us a lot of time. Moreover, we believe our results\nobtained with only the run time to be self-sufficient.\nThe way we compared an LLM to a human using Leetcode rank-\nings gives a good idea of where the LLM stands in terms of per-\nformance. However, because the ranking evolves and we have no\ninformation about the population the LLM is ranked against, the\nresults here should be taken with a grain of salt.\nIt is also important to note that Leetcode as an evaluation dataset\nsuffers from some issues. As we only evaluate the LLMs on algorith-\nmic problems, the performances of the LLMs are hard to generalize\nacross all programming fields. However, this is difficult to improve\non for similar reasons to HumanEval: we only have a limited con-\ntext size and have to make the LLM generate in one go a completion\nto some code, which must be self-contained (meaning, all the in-\nformation needed to generate the solution must be in the prompt).\nAlso, in terms of performance, it is difficult to find another kind\nof self-contained code than algorithmic problems that have, such\nvariations in performance. While studying the LLMs on SQL gen-\neration could be a good idea, it would not fit into our studies with\ngeneralist programming languages.\nRegarding Leetcode as a platform, we are heavily dependent on\nit for validating the generated solutions and are limited by its daily\nrate limits of 1, 000 submissions per account. For future studies, it\nwould be great to consider alternatives to Leetcode, such as the\nproject CodeNet [31], which does not have as many restrictions as\nLeetcode.\n7 RELATED WORKS\nPrevious research has investigated various aspects of LLMs for code-\nrelated tasks, including the security of their suggestions [29, 30, 33],\nthe prevalence of bugs in the generated code [20], how developers\ninteract with them [7, 30, 34] or just the quality and correctness of\nthe code they generate [13, 22, 26, 42]. There have also been efforts\nto measure the efficiency of LLMs through the creation of bench-\nmarks for comparing them, such as HumanEval[10], MBPP [5],\nCoderEval [43], APPS [ 17], CodeXGLUE [23] or ReCode [37].\nXu et al. [40] also led a comparative evaluation of multiple LLMs\nfor code including Codex and Codeparrot.\nLeetcode, while being just a coding competition platform, is also\nused as a dataset to evaluate the capabilities of LLMs on program-\nming tasks. DÃ¶derlein et al. [13] measured the performances of\nCopilot and Codex on Leetcode and the effects of changing the\nprompts. Nguyen and Nadi [26] studied Github Copilotâ€™s code\nsuggestions on Leetcode problems and the complexities of its gener-\nated code. Vasconcelos et al. [35] studied the effects of highlighting\nthe uncertainty of AI-powered code completions using Leetcode\nproblems and Codex.\nVarious other methods have also been employed to investigate\nthe impact of temperature on the generated code, apart from the\nEASE 2024, 18â€“21 June, 2024, Salerno, Italy Tristan Coignion, ClÃ©ment Quinton, and Romain Rouvoy\napproach we proposed: Chen et al. [10] evaluated the best temper-\nature of Codex in terms of pass@k. Austin et al. [5] also studied\nthe effects of the temperature on the performance of their model.\nChristopoulou et al. [12] also studied the effects of the temperature\nand nucleus-sampling on their LLM. DÃ¶derlein et al. [13] highlight\nthe importance of correctly tuning the temperature of a model\nwhen using it to generate code. The research led by Aghakhani et\nal. [3] shows that poisoned models suggest insecure code more\noften as the temperature increases. Our results demonstrate that\nincreasing the temperature also increases the chance of generating\nslow and inefficient code.\nOn the subject of performance, Madaan et al. [25] fine-tuned\nLLMs to make them improve the performance of code. Multiple\nother techniques have been proposed for automatically improving\nthe performance of code using LLMs [9, 11, 15]. Our contribution\nis the firstâ€”as far as we knowâ€”to investigate the differences in\nthe performance of LLM-generated code. Future LLMs that would\nbe adapted with these performance-improving techniques could\nalso be compared using our methodology. Regarding recitation, few\nworks have been done on this subject, but Yan et al. [41] proposed\na method to detect cases of recitations in LLMs using inference\nfingerprinting. Jacovi et al. [19] explained why data contamination\nin LLMs was problematic and proposed methods to mitigate it.\nTo summarize, our paper evaluates the performance of code gen-\nerated by various LLMs and investigates differences in the perfor-\nmance of the generated code across models on Leetcode problems,\nwhich, to the best of our knowledge, has not been done previously.\n8 CONCLUSION\nIn this study, we presented a comprehensive analysis of the perfor-\nmance of code generated by various LLMs using a novel methodol-\nogy that measures and compares the runtime speed of solutions to\nalgorithmic problems. Our findings suggest that the performance\nof the generated code is largely similar across different models,\nregardless of their size or training data. Furthermore, increasing the\ntemperature parameter during code generation leads to a greater\nvariance in performance, though not necessarily to better or worse\nsolutions on average. We also critically evaluated the suitability of\nLeetcode as a dataset and benchmark platform for assessing LLMs.\nThe results indicate that, while Leetcodeâ€™s problems are suitable for\nperformance evaluation, their measures should be used cautiously\ndue to issues with stability and reliability. Additionally, we observed\nthat the use of newer Leetcode problems is essential to avoid data\ncontamination and ensure the validity of LLM evaluations.\nThis work opens up several avenues for future research, includ-\ning the development of performance-oriented training datasets and\nthe fine-tuning of LLMs for performance improvement. As the field\nof AI-assisted programming continues to evolve, studies such as\nours will play a critical role in understanding and enhancing the\ncapabilities of LLMs.\nACKNOWLEDGMENTS\nThis work received support from the French government through\nthe Agence Nationale de la Recherche (ANR) under the France 2030\nprogram, including partial funding from the CARECloud (ANR-23-\nPECL-0003), DISTILLER (ANR-21-CE25-0022), and KOALA (ANR-\n19-CE25-0003-01) projects. Experiments presented in this paper\nwere carried out using the Gridâ€™5000 testbed, supported by a scien-\ntific interest group hosted by Inria and including CNRS, RENATER\nand several Universities as well as other organizations.12\nREFERENCES\n[1] 2022. Codeparrot/Codeparrot Â·Hugging Face. https://huggingface.co/codeparrot/\ncodeparrot.\n[2] Hayri Acar, GÃ¼lfem I Alptekin, Jean-Patrick Gelas, and Parisa Ghodous. 2016.\nThe Impact of Source Code in Software on Power Consumption. International\nJournal of Electronic Business Management 14 (2016), 42â€“52.\n[3] Hojjat Aghakhani et al. 2023. TrojanPuzzle: Covertly Poisoning Code-Suggestion\nModels. (2023). https://doi.org/10.48550/ARXIV.2301.02344\n[4] Loubna Ben Allal et al . 2023. SantaCoder: Donâ€™t Reach for the Stars!\narXiv:2301.03988 [cs]\n[5] Jacob Austin et al . 2021. Program Synthesis with Large Language Models.\narXiv:2108.07732 [cs]\n[6] Daniel Balouek et al. 2013. Adding Virtualization Capabilities to the Gridâ€™5000\nTestbed. In Cloud Computing and Services Science . Communications in Computer\nand Information Science, Vol. 367. 3â€“20.\n[7] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. Proceedings\nof the ACM on Programming Languages 7, OOPSLA1 (April 2023), 78:85â€“78:111.\n[8] SÃ©bastien Bubeck et al . 2023. Sparks of Artificial General Intelligence: Early\nExperiments with GPT-4. arXiv:2303.12712 [cs]\n[9] Binghong Chen and othersy. 2022. Learning to Improve Code Efficiency.\narXiv:2208.05297 [cs]\n[10] Mark Chen et al . 2021. Evaluating Large Language Models Trained on Code.\narXiv:2107.03374 [cs]\n[11] Zimin Chen, Sen Fang, and Martin Monperrus. 2023. Supersonic: Learning to\nGenerate Source Code Optimizations in C/C++. arXiv:2309.14846 [cs]\n[12] Fenia Christopoulou et al. 2022. PanGu-Coder: Program Synthesis with Function-\nLevel Language Modeling. arXiv:2207.11280 [cs]\n[13] Jean-Baptiste DÃ¶derlein, Mathieu Acher, Djamel Eddine Khelladi, and Benoit\nCombemale. 2023. Piloting Copilot and Codex: Hot Temperature, Cold Prompts,\nor Black Magic? https://doi.org/10.2139/ssrn.4496380\n[14] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,\nRuiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. 2022. InCoder: A\nGenerative Model for Code Infilling and Synthesis. arXiv:2204.05999 [cs]\n[15] Spandan Garg et al. 2022. DeepDev-PERF: A Deep Learning-Based Approach for\nImproving Software Performance. In Proceedings of the 30th ACM Joint ESEC/FSE .\n948â€“958. https://doi.org/10.1145/3540250.3549096\n[16] Spandan Garg et al. 2023. RAPGen: An Approach for Fixing Code Inefficiencies\nin Zero-Shot. arXiv:2306.17077 [cs]\n[17] Dan Hendrycks et al . 2021. Measuring Coding Challenge Competence With\nAPPS. Proceedings of the Neural Information Processing Systems Track on Datasets\nand Benchmarks 1 (Dec. 2021).\n[18] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The\nCurious Case of Neural Text Degeneration. arXiv:1904.09751 [cs]\n[19] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. 2023. Stop\nUploading Test Data in Plain Text: Practical Strategies for Mitigating Data Con-\ntamination by Evaluation Benchmarks. arXiv:2305.10160 [cs]\n[20] Kevin Jesse et al. 2023. Large Language Models and Simple, Stupid Bugs. In 2023\nIEEE/ACM 20th International Conference on Mining Software Repositories (MSR) .\n563â€“575. https://doi.org/10.1109/MSR59073.2023.00082\n[21] Raymond Li et al . 2023. StarCoder: May the Source Be with You! (2023).\narXiv:2305.06161 [cs.CL]\n[22] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2024. Is your\ncode generated by chatgpt really correct? rigorous evaluation of large language\nmodels for code generation. Advances in Neural Information Processing Systems\n36 (2024).\n[23] Shuai Lu et al. 2021. CodeXGLUE: A Machine Learning Benchmark Dataset for\nCode Understanding and Generation. arXiv:2102.04664 [cs]\n[24] Ziyang Luo et al. 2023. WizardCoder: Empowering Code Large Language Models\nwith Evol-Instruct. arXiv:2306.08568 [cs]\n[25] Aman Madaan et al. 2023. Learning Performance-Improving Code Edits.\n[26] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copilotâ€™s\nCode Suggestions. In Proceedings of the 19th International Conference on Mining\nSoftware Repositories . 1â€“5. https://doi.org/10.1145/3524842.3528470\n[27] Erik Nijkamp et al. 2022. CodeGen: An Open Large Language Model for Code\nwith Multi-Turn Program Synthesis. arXiv:2203.13474 [cs]\n12See https://www.grid5000.fr\nA Performance Study of LLM-Generated Code on Leetcode EASE 2024, 18â€“21 June, 2024, Salerno, Italy\n[28] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs]\n[29] Hammond Pearce et al. 2022. Asleep at the Keyboard? Assessing the Security of\nGitHub Copilotâ€™s Code Contributions. In 2022 IEEE Symposium on Security and\nPrivacy (SP) . 754â€“768. https://doi.org/10.1109/SP46214.2022.9833571\n[30] Neil Perry et al. 2023. Do Users Write More Insecure Code with AI Assistants?. In\nProceedings of the 2023 ACM SIGSAC Conference on Computer and Communications\nSecurity (CCS â€™23) . 2785â€“2799. https://doi.org/10.1145/3576915.3623157\n[31] Ruchir Puri et al. 2021. CodeNet: A Large-Scale AI for Code Dataset for Learning\na Diversity of Coding Tasks. Proceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks 1 (Dec. 2021).\n[32] Baptiste RoziÃ¨re et al. 2023. Code Llama: Open Foundation Models for Code.\nhttps://doi.org/10.48550/arXiv.2308.12950 arXiv:2308.12950 [cs]\n[33] Gustavo Sandoval et al. 2023. Lost at C: A User Study on the Security Implications\nof Large Language Model Code Assistants. In 32nd USENIX Security Symposium\n(USENIX Security 23) . 2205â€“2222.\n[34] Priyan Vaithilingam et al . 2022. Expectation vs. Experience: Evaluating the\nUsability of Code Generation Tools Powered by Large Language Models. In CHI\nConference on Human Factors in Computing Systems Extended Abstracts . 1â€“7.\nhttps://doi.org/10.1145/3491101.3519665\n[35] Helena Vasconcelos et al. 2023. Generation Probabilities Are Not Enough: Ex-\nploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Com-\npletions. (2023). https://doi.org/10.48550/ARXIV.2302.07248\n[36] Roberto Verdecchia et al. 2017. Estimating Energy Impact of Software Releases\nand Deployment Strategies: The KPMG Case Study. In 2017 ACM/IEEE Interna-\ntional Symposium on Empirical Software Engineering and Measurement (ESEM) .\n257â€“266. https://doi.org/10.1109/ESEM.2017.39\n[37] Shiqi Wang et al . 2022. ReCode: Robustness Evaluation of Code Generation\nModels. https://doi.org/10.48550/ARXIV.2212.10264\n[38] Yue Wang et al. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-\nDecoder Models for Code Understanding and Generation. arXiv:2109.00859 [cs]\n[39] Jason Wei et al. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large\nLanguage Models. arXiv:2201.11903 [cs]\n[40] Frank F. Xu et al. 2022. A Systematic Evaluation of Large Language Models of\nCode. In Proceedings of the 6th International Symposium on Machine Programming\n(MAPS 2022) . 1â€“10. https://doi.org/10.1145/3520312.3534862\n[41] Weixiang Yan et al. 2022. WhyGen: Explaining ML-powered Code Generation by\nReferring to Training Examples. In Proceedings of the 44th Int. Conf. on Software\nEngineering, vol. 2 (ICSE â€™22) . 237â€“241. https://doi.org/10.1145/3510454.3516866\n[42] Burak Yetistiren et al. 2022. Assessing the Quality of GitHub Copilotâ€™s Code Gen-\neration. InProceedings of the 18th Int. Conf. on Predictive Models and Data Analytics\nin Software Engineering . 62â€“71. https://doi.org/10.1145/3558489.3559072\n[43] Hao Yu et al. 2023. CoderEval: A Benchmark of Pragmatic Code Generation with\nGenerative Pre-trained Models. arXiv:2302.00288 [cs]",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.7769321203231812
    },
    {
      "name": "Code (set theory)",
      "score": 0.7193499207496643
    },
    {
      "name": "Computer science",
      "score": 0.6804677248001099
    },
    {
      "name": "Reliability (semiconductor)",
      "score": 0.5328702330589294
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4980740547180176
    },
    {
      "name": "Field (mathematics)",
      "score": 0.4236203134059906
    },
    {
      "name": "Programming language",
      "score": 0.19014620780944824
    },
    {
      "name": "Power (physics)",
      "score": 0.10344716906547546
    },
    {
      "name": "Business",
      "score": 0.07317298650741577
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}