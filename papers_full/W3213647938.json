{
  "title": "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling",
  "url": "https://openalex.org/W3213647938",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222134701",
      "name": "Zhang, Renrui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226817511",
      "name": "Fang, Rongyao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1809037978",
      "name": "Zhang Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109793426",
      "name": "Gao Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221910337",
      "name": "Li, Kunchang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2345450207",
      "name": "Dai, Jifeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2052563777",
      "name": "Qiao Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969345873",
      "name": "Li Hongsheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035060554",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3108975329",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3207750165",
    "https://openalex.org/W2766736793",
    "https://openalex.org/W3168114581",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3205789812",
    "https://openalex.org/W3170928047",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W3198675127",
    "https://openalex.org/W2804243936",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2752497102",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W12634471",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W3182683290",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2995589713",
    "https://openalex.org/W3172170405",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3205249428",
    "https://openalex.org/W2963341924",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2964194231",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W2911906319",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3012209922",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2804989836",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W298212978",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2950537964"
  ],
  "abstract": "Contrastive Vision-Language Pre-training, known as CLIP, has provided a new paradigm for learning visual representations by using large-scale contrastive image-text pairs. It shows impressive performance on zero-shot knowledge transfer to downstream tasks. To further enhance CLIP's few-shot capability, CLIP-Adapter proposed to fine-tune a lightweight residual feature adapter and significantly improves the performance for few-shot classification. However, such a process still needs extra training and computational resources. In this paper, we propose \\textbf{T}raining-Free CL\\textbf{IP}-\\textbf{Adapter} (\\textbf{Tip-Adapter}), which not only inherits CLIP's training-free advantage but also performs comparably or even better than CLIP-Adapter. Tip-Adapter does not require any back propagation for training the adapter, but creates the weights by a key-value cache model constructed from the few-shot training set. In this non-parametric manner, Tip-Adapter acquires well-performed adapter weights without any training, which is both efficient and effective. Moreover, the performance of Tip-Adapter can be further boosted by fine-tuning such properly initialized adapter for only a few epochs with super-fast convergence speed. We conduct extensive experiments of few-shot classification on ImageNet and other 10 datasets to demonstrate the superiority of proposed Tip-Adapter. The code will be released at \\url{https://github.com/gaopengcuhk/Tip-Adapter}.",
  "full_text": "Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling\nRenrui Zhangâˆ—1, Rongyao Fangâˆ—2, Wei Zhangâˆ—1, Peng Gaoâ€ 1, Kunchang Li1\nJifeng Dai3, Yu Qiao1, Hongsheng Li2\n1Shanghai AI Laboratory\n2The Chinese University of Hong Kong 3SenseTime Research\n{zhangrenrui, gaopeng, qiaoyu}@pjlab.org.cn\nrongyaofang@link.cuhk.edu.hk, hsli@ee.cuhk.edu.hk\nAbstract\nContrastive Vision-Language Pre-training, known as CLIP ,\nhas provided a new paradigm for learning visual represen-\ntations by using large-scale contrastive image-text pairs.\nIt shows impressive performance on zero-shot knowledge\ntransfer to downstream tasks. To further enhance CLIPâ€™s\nfew-shot capability, CLIP-Adapter proposed to ï¬ne-tune\na lightweight residual feature adapter and signiï¬cantly\nimproves the performance for few-shot classiï¬cation.\nHowever, such a process still needs extra training and\ncomputational resources. In this paper, we propose\nTraining-Free CL IP-Adapter (Tip-Adapter), which not\nonly inherits CLIPâ€™s training-free advantage but also\nperforms comparably or even better than CLIP-Adapter.\nTip-Adapter does not require any back propagation for\ntraining the adapter, but creates the weights by a key-value\ncache model constructed from the few-shot training set. In\nthis non-parametric manner, Tip-Adapter acquires well-\nperformed adapter weights without any training, which\nis both efï¬cient and effective. Moreover, the performance\nof Tip-Adapter can be further boosted by ï¬ne-tuning\nsuch properly initialized adapter for only a few epochs\nwith super-fast convergence speed. We conduct extensive\nexperiments of few-shot classiï¬cation on ImageNet and\nother 10 datasets to demonstrate the superiority of pro-\nposed Tip-Adapter. The code will be released at https:\n//github.com/gaopengcuhk/Tip-Adapter.\n1. Introduction\nVision and language are two modalities for humans to\nperceive the surrounding world and perform diverse in-\nteractions with the environment. The accuracy of vision\ntasks, such as classiï¬cation [14, 24, 28, 37, 45, 61], detec-\ntion [5, 19, 53] and segmentation [69], has been boosted\nâˆ— indicates equal contributions, and â€ indicates corresponding author.\nFigure 1. A Comparison of CLIP-Adapter vs. the proposed\nTraining-free CLIP-Adapter (Tip-Adapter).CLIP-Adapter and\nTip-Adapter share similar architectures, namely, two linear layers\nand a residual connection blending updated visual features with\npre-trained CLIP features. CLIP-Adapter is trained with Stochas-\ntic Gradient Descent (SGD), while Tip-Adapter is training-free,\nwhose weights of linear layers are initialized from Cache Model.\nsigniï¬cantly thanks to better neural architecture designs\n(e.g. ResNet [24], Transformer [62], etc.) and deli-\ncately designed frameworks (e.g., Faster R-CNN [53], Reti-\nnaNet [40], DETR [5], etc.). Language generation and un-\nderstanding have also been improved a lot due to large-\nscale self-supervised tasks, including mask prediction [11]\nand language model pre-training on web-scale dataset [52].\nAs vision and language usually contain complementary in-\nformation, joint learning of vision and language represen-\ntations has been proven to be quite effective on multi-\nmodality tasks, such as Visual Question Answering [1, 2,\n18, 33], Image Captioning [29, 67], and Referring Expres-\nsion [68]. Different from previous multi-modality inter-\naction methods that learn vision and language representa-\ntions independently on each dataset [1, 43, 59], CLIP [51]\nproposed to learn transferable visual features from nat-\narXiv:2111.03930v2  [cs.CV]  15 Nov 2021\nural language supervisions and successfully demonstrate\nthe amazing zero-shot classiï¬cation ability of pre-trained\nmodel. Due to the interplay between language and vision,\nthe encoded visual representations can be used in open-\nvocabulary classiï¬cation without re-training. Following the\nresearch direction of prompt design [4, 41], CoOp [70] pro-\nposed to ï¬ne-tune a pre-trained CLIP model for few-shot\nclassiï¬cation via learnable continuous tokens and achieved\nstrong performance on few-shot image classiï¬cation. Re-\ncently, CLIP-Adapter [17] was also proposed to tackle the\nfew-shot classiï¬cation problem by introducing a feature\nadapter that generates the adapted features and then com-\nbines it with the original CLIP feature using a residual\nconnection, which incorporates the new knowledge from\nthe few-shot training set and prior knowledge encoded in\nCLIP. CLIP-Adapter demonstrates promising performance\non few-shot classiï¬cation without utilizing prompt designs.\nAlthough CoOp [70] and CLIP-Adapter [17] show\nstrong performance on few-shot classiï¬cation benchmarks,\nin comparison with CLIP [51] and linear probe CLIP [51],\nthey generally require much computational resources to\nï¬ne-tune the large-scale vision-language model due to\nthe slow convergence of Stochastic Gradient Descent\n(SGD) [34, 42] and huge GPU memory consumption [54].\nThus, we ask the following question: Can we achieve best\nof both worlds of CLIP and CLIP-Adapter, which does not\nonly has the advantage of training-free property of CLIP\nbut also enjoys the strong performance of CLIP-Adapter for\nfew-shot classiï¬cation?\nTo achieve the goal, we propose a novel Training-free\nCLIP-Adapter (Tip-Adapter), which adopts the architec-\nture design of CLIP-Adapter. It appends CLIP model with\nan adapter of two-layer Multi-layer Perceptron (MLP) and\na residual connection [24] combining pre-trained features\nwith the updated features. Different from CLIP-Adapter,\nTip-Adapter does not require SGD to train the adapter but\nconstructs a query-key cache model [20, 32, 48] from few-\nshot supervisions to obtain the weights of adapter. Speciï¬-\ncally, Tip-Adapter extracts visual features of few-shot train-\ning images by CLIPâ€™s visual encoder and transforms their\ncorresponding labels into one-hot encoding. On top of that,\na cache model is created which contains visual features\nand one-hot labels of the few-shot training set as key-value\npairs.\nBased on the constructed cache model, the weights\nof Tip-Adapter could be obtained in a training-free non-\nparametric manner. In detail, the two linear layers of the\nCLIP-Adapter are set as the cached visual features and their\ncorresponding one-hot labels from the few-shot training set.\nThus, the processing of the adapter with such weights can\nbe regarded as retrieving the few-shot knowledge from the\nkey-value cache model. By this approach, Tip-Adapter ex-\nhibits great efï¬ciency compared to CLIP-Adapterâ€™s SGD\ntraining. During inference, the test imageâ€™s adapted fea-\nture is combined with its original CLIP-encoded feature.\nIn this way, the Tip-Adapter is able to exploit knowledge\nfrom both the pre-trained CLIP and the few-shot training\ndataset. Surprisingly, Tip-Adapter with such constructed\nweights could perform comparably to the fully ï¬ne-tuned\nCLIP-Adapter. Furthermore, if we unfreeze the ï¬rst lin-\near layer of the Tip-Adapter and further ï¬ne-tune it, Tip-\nAdapterâ€™s performance could be signiï¬cantly boosted with\njust a few training epochs. It only requires 20 epochs in\ncomparison with CLIP-Adapterâ€™s 200 epochs.\nThe contributions of our paper are summarised below:\nâ€¢ We propose Training-free CLIP-Adapter (Tip-\nAdapter), which has strong performance on few-\nclassiï¬cation via directly setting the weights of\nadapter with a cache model to avoid the conventional\nSGD ï¬ne-tuning.\nâ€¢ The performance of Tip-Adapter can be further im-\nproved by ï¬ne-tuning based on such constructed\nweights with super-fast convergence.\nâ€¢ We evaluate Tip-Adapter on 11 few-shot classiï¬ca-\ntion datasets and conduct extensive ablation studies to\ndemonstrate its characteristics. Tip-Adapter achieves\ncompetitive performance with state-of-the-art methods\nand leads to signiï¬cant reduction of training time.\n2. Related Work\nData-efï¬cient Transfer Learning. The capability of\ndeep neural networks is revealed with the assistance of\nlarge-scale datasets [37]. However, collecting large-scale\ndataset is challenging due to actual dataâ€™s long-tail distri-\nbution, noisy annotations and the increasing labeling costs.\nThus, transfer learning is proposed to reduce such costly re-\nquirements and has become a popular research ï¬eld. Su-\npervised pre-training on large-scale image classiï¬cation\ntasks (e.g. ImageNet [9]) and transferring to downstream\ntasks (e.g. detection [53] and segmentation [23]) have\nbeen widely adopted. Self-supervised learning, such as\nMoCo [22] and BYOL [21], further alleviates the need of\nlarge-scale supervised pre-training and converts the prob-\nlem into a self-supervised form. From the view of lan-\nguage, VirTex [10] veriï¬es the data-efï¬ciency of language\nsupervision via captioning on learning high-quality visual\nrepresentations. Recently, CLIP [51], DeCLIP [39] and\nALIGN [30] have demonstrated that simple contrastive\nlearning between vision-language pairs is able to learn zero-\nshot transferable features over diverse image classiï¬cation\ndatasets without further training. Furthermore, CoOp [70],\nCLIP-Adapter [17] and WiSE-FT [64] indicate that the per-\nformance of CLIP can be signiï¬cantly improved with ei-\nther abundant or limited training data by ï¬ne-tuning weight-\nï¬xed CLIP with adapter or prompt optimization. In con-\ntrast, our proposed Tip-Adapter aims at directly infusing\nfew-shot supervisions into the pre-trained CLIP model with\na training-free manner. Thus, Tip-Adapter is quite efï¬cient\nat time and computation, as it only calculates and caches the\nfew-shot training set once, and requires no more training.\nTransformer. Transformer [62] introduces the key-query\nattention mechanism for encoding high-quality features and\nis superior to many previous language and vision models,\nsuch as Long Short-term Memory (LSTM) [26] and Convo-\nlution Neural Network (CNN) [38]. For natural language\nprocessing, transformer achieves leading performance on\nmachine translation [49, 62], natural language understand-\ning [11] and language generation [4, 52]. Also for vision,\ntransformer-based architectures have shown great capabili-\nties and potentials on image classiï¬cation [13], object de-\ntection [5], and semantic segmentation [69]. Transformer\nutilizes multi-head key-query multiplication to implement\nhighly-efï¬cient message passing between all pairs of in-\nput tokens. In this paper, we analyse the relationship be-\ntween Tip-Adapterâ€™s cache model and transformerâ€™s atten-\ntion mechanism. This similar relations have also been dis-\ncussed in persistent memory [57] and non-parametric atten-\ntion [35].\nCache Model. A cache model stores features of train-\ning images and their labels as a key-value database. Dur-\ning inference, the feature generated from a test example\nwould be treated as query and aggregate information from\nthe database by retrieval [62], which is similar to atten-\ntion mechanism in transformer [62]. It is important that the\nwhole process is non-parametric [35] and requires no pa-\nrameter update. The cache model has been adopted for im-\nproving language generation in kNN-LMs [32], Unbounded\nCache [20] and Pointer Sentinel Mixture Models [46]. Al-\nthough simple cache model [48] has shown some promising\nresults, it needs to store a huge memory of training dataset\nfor test-time inference. Therefore, approximate kNN and\nhigh-optimized similarity search system [31] are proposed,\nbut the whole pipeline is generally verbose, error-prone and\nslow. Different from previous cache modelsâ€™ setup, we ex-\nplore it with CLIP and adopt the few-shot settings. In detail,\nwe generate cached features from CLIPâ€™s visual encoder so\nthat strong label-related information is naturally encoded in\nthe key-value pair. Due to the few-shot setup with limited\ntraining samples, the total cache is small and can be efï¬-\nciently calculated by two cascades of matrix multiplication.\nMoreover, the cache model in Tip-Adapter is able to dy-\nnamically update via Stochastic Gradient Descent (SGD),\nwhich further improves its performance.\n3. Method\nIn this section, we introduce the proposed Training-\nfree CLIP-Adapter (Tip-Adapter) for signiï¬cantly improv-\ning CLIPâ€™s zero-shot classiï¬cation accuracy. In Section 3.1,\nwe ï¬rst brieï¬‚y revisit CLIP-Adapter. In Section 3.2, we\npresent the designs of Tip-Adapter and the further ï¬ne-\ntuned version for achieving stronger performance. Finally\nin Section 3.3, we discuss the relationship with previous\ncache-based methods.\n3.1. A Revisit of CLIP-Adapter\nAdapter [27] is a lightweight neural module to con-\nduct parameter-efï¬cient ï¬ne-tuning of pre-trained models,\nsuch as BERT [11] in natural language processing. CLIP\nlearns visual representations by exploiting contrastive learn-\ning on large-scale image-text pairs, which achieves inspi-\nrational performance in zero-shot classiï¬cation. Follow-\ning the spirit of adapter, to transfer CLIPâ€™s knowledge\nfor achieving few-shot classiï¬cation, CLIP-Adapter [17]\nappends a lightweight two-layer Multi-layer Perceptron\n(MLP) to the pre-trained ï¬xed-weight CLIP model and pre-\ndicts the adapted feature residuals for each input image.\nThe residual connection in the adapter helps to fuse the\nprior visual knowledge encoded by CLIP and the updated\nfeatures learned from the few-shot training set. Compared\nwith zero-shot and linear-probe CLIP [51], CLIP-Adapter\nachieves signiï¬cant performance improvement on multiple\nimage classiï¬cation datasets. It is optimized by minimiz-\ning the cross-entropy loss between predictions and ground\ntruths with stochastic gradient descent (SGD). Thus, al-\nthough CLIP-Adapter achieves great performance boost, it\nstill requires additional training, which is slow and costly\ncompared with the original training-free zero-shot CLIP.\nSpeciï¬cally, for an input image, its L2 normalized fea-\nture fc is ï¬rst obtained by CLIPâ€™s pre-trained visual en-\ncoder. An adapter of two-layer MLP, with parameters\nW1,b1,W2,b2, is appended to obtain the updated feature\nfa,\nfa = Ï•(fcWT\n1 + b1)WT\n2 + b2, (1)\nwhere Ï•denotes the activation function in the MLP. Then,\nthe adapted feature fa is linearly combined with the pre-\ntrained feature fc with a hyper-parameter Î± âˆˆ [0,1] to\noutput the ï¬nal classiï¬cation logits. In this way, the prior\nknowledge of CLIP on the input image is updated by the\nadapter in an additive manner,\nlogits =Î±faWT\nc + fcWT\nc , (2)\nwhere Wc is the weights of the text classiï¬er. To construct\nWc, following zero-shot CLIP, CLIP-Adapter places each\ncategory name into the pre-deï¬ned prompt template and en-\ncodes them by CLIPâ€™s pre-trained textual encoder.\nVisualEncoderğ‘“!\"#!\nğ¶\nğ¶\nâ‹¯ â‹¯ â‹¯ â‹¯\nğ¾\nğ‘ğ¾\nğ¹ğ‘’ğ‘¤-ğ‘ â„ğ‘œğ‘¡ğ¼ğ‘šğ‘ğ‘”ğ‘’ğ‘ \nğ‘\nğ‘ğ¾\nOneHot\nğ¹ğ‘’ğ‘¤-ğ‘ â„ğ‘œğ‘¡ğ¿ğ‘ğ‘ğ‘’ğ‘™ğ‘ ğ‘ğ‘ğ‘¡Ã—ğ¾ğ‘‘ğ‘œğ‘”Ã—ğ¾ğ‘ğ‘ğ‘›ğ‘‘ğ‘Ã—ğ¾ğ‘Ÿğ‘ğ‘ğ‘ğ‘–ğ‘¡Ã—ğ¾\n1000\n1000\n1000â‹¯0100\n0100\n0100â‹¯0010\n0010\n0010â‹¯0001\n0001\n0001â‹¯\nğ…!\"#$%& ğ‹!\"#$%&\nğœ‘ğ‘“!\"#!ğ…!$%&'( ğ‹!$%&'\nğ‘ğ‘ğ‘¡ğ‘‘ğ‘œğ‘”ğ‘ğ‘ğ‘›ğ‘‘ğ‘ğ‘Ÿğ‘ğ‘ğ‘ğ‘–ğ‘¡ğ¶ğ‘ğ‘Š'&ğ‘“!\"#!ğ‘¾)(ğ‘‡ğ‘’ğ‘ ğ‘¡ğ¼ğ‘šğ‘ğ‘”ğ‘’\nâ‹¯ â‹¯ â‹¯ â‹¯\nVisualEncoder ğ¾\nğ‘‡ğ‘’ğ‘¥ğ‘¡ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘ŸFew-shotKnowledge\nCLIP!sKnowledge\nLinearCombinationÃ—ğ›¼\nFigure 2. The Pipeline of Tip-Adapter.Given a K-shot N-class training set, we construct the weights of the two-layer adapter by creating\na cache model from the few-shot training set. It contains few-shot visual features Ftrain encoded by CLIPâ€™s visual encoder and few-shot\nground-truth labels Ltrain. Ftrain and Ltrain can be used as the weights for the ï¬rst and second layers in the adapter.\n3.2. Training-free CLIP-Adapter\nWe propose Tip-Adapter, which is a training-free and\nnon-parametric extension of CLIP-Adapter [17] but per-\nforms comparably and even better than it. To achieve this\ngoal, it adopts the same architecture as CLIP-Adapter, but\nwe construct a key-value cache model from the few-shot\ntraining set and transform the cache into the weights of\nthe adapter MLP in a non-parametric manner without ï¬ne-\ntuning. Surprisingly, with weights constructed by a prop-\nerly designed approach, Tip-Adapter without ï¬ne-tuning\ncan achieve comparable performance as CLIP-Adapter with\nï¬ne-tuning. In addition, if ï¬ne-tuning is allowable, further\nï¬ne-tuning with such weights as network initialization is\nable to achieve much higher accuracy with super-fast con-\nvergence speed.\nCache Model Construction. Given the pre-trained\nCLIP [51] model and aK-shot N-class training set for few-\nshot classiï¬cation, there are Kannotated images in each of\nthe N categories, denoted as IK with their labels LN. The\ngoal is to conduct image classiï¬cation on theNclasses. We\ncreate a key-value cache model and convert it to obtain the\nweights of the proposed Tip-Adapter. For each training im-\nage, we utilize the CLIPâ€™s visual encoder to extract its C-\ndimensional L2 normalized visual feature, and convert its\nground-truth label into an N-dimensional one-hot vector.\nFor all NK training samples, we denote their visual fea-\ntures and corresponding label vectors as Ftrain âˆˆRNKÃ—C\nand Ltrain âˆˆRNKÃ—N,\nFtrain = VisualEncoder(IK), (3)\nLtrain = OneHot(LN). (4)\nTo create the key-value cache, the CLIP-encoded represen-\ntations Ftrain are treated as keys, while the one-hot ground-\ntruth vectors Ltrain are used as their values. In this way, the\nkey-value cache contains all the new knowledge extracted\nfrom the few-shot training set, which can be converted to\nthe weights of the adapter MLP to update the prior knowl-\nedge encoded in the pre-trained CLIP.\nTip-Adapter. After constructing the cache model, the\nadapter weights are determined according to cached key-\nvalue pairs. Speciï¬cally, the test imageâ€™s normalized visual\nfeature ftest âˆˆR1Ã—C is ï¬rst extracted by the CLIPâ€™s visual\nencoder and serves as a query for retrieving from the key-\nvalue cache. The afï¬nities between the test query and keys\nof the cached few-shot training set can be then estimated as\nAâˆˆR1Ã—NK ,\nA= exp (âˆ’Î²(1 âˆ’ftestFT\ntrain)), (5)\nwhere Î² stands for a modulating hyper-parameter. Since\nboth query and key features are L2 normalized, the term\n(1 âˆ’ftestFT\ntrain) is equivalent to calculating the Euclidean\ndistances between the test feature ftest and all few-shot\ntraining imagesâ€™ features FT\ntrain. The exponential function\nis adopted to convert query-key Euclidean distances to non-\nnegative afï¬nities Awith Î²modulating its sharpness. After-\nwards, the retrieved value from the cache model can be ob-\ntained via the multiplication between the query-key afï¬ni-\nties and the cached values as ALtrain.\nThe predicted logits of the test image by the Tip-Adapter\nis then calculated as\nlogits =Î±ALtrain + ftestWT\nc\n= Î±Ï•(ftestFT\ntrain)Ltrain + ftestWT\nc , (6)\nwhere Wc represents CLIPâ€™s text classiï¬er, Î± denotes the\nresidual ratio, and we deï¬ne Ï•(x) = exp(âˆ’Î²(1 âˆ’x)). In-\ntuitively, Tip-Adapterâ€™s class prediction contains two terms:\npredictions according to values retrieved from the cache\nmodel and predictions from the pre-trained CLIP. The for-\nmer term adaptively summarizes information from the few-\nshot training set. The values Ltrain (class predictions of the\ncached samples) in the cache are linearly combined accord-\ning to the query-key afï¬nities A. The latter term preserves\nthe prior knowledge from the original CLIP by directly us-\ning the pre-trained classiï¬er WT\nc to process the test image\nfeature ftest. The two terms are balanced by the weight\nÎ±. Empirically, Î± is set to be large if the domain gap be-\ntween pre-trained and downstream tasks is large, since more\nknowledge from the few-shot set is required, and small oth-\nerwise.\nComparing Eqs. (2) and (6), the proposed Tip-Adapter\ncan be seen as a special form of CLIP-Adapter with the fol-\nlowing weights,\nW1 = Ftrain,W2 = LT\ntrain, b1 = 0, b2 = 0, (7)\nÏ•(x) = exp(âˆ’Î²(1 âˆ’x)), where xâˆˆ[0,1]. (8)\nThe differences between CLIP-Adapter and Tip-Adapter\ncan be summarized as follows. Firstly, CLIP-Adapter ran-\ndomly initializes W1 and W2 and learns them via SGD,\nwhile Tip-Adapter directly sets W1 as cached training fea-\ntures Ftrain and W2 as the transposed one-hot encoding of\nthe ground-truth labels Ltrain, which are non-parametric\nand training-free. Secondly, the bottleneck dimension of\nTip-Adapter is equal to NK, while CLIP-Adapter selects\na low-dimensional bottleneck to prevent the risk of over-\nï¬tting. This indicates that with such proper initialization,\nthe over-ï¬tting problem on few-shot datasets is much al-\nleviated, which further releases the ï¬tting power of high-\ndimensional linear layers. Thirdly, Tip-Adapter introduces\na new activation function denoted in Eq. (7). As its in-\nputs are the distances in the normalized feature space, it\nis bounded between 0 and 1. However, for CLIP-Adapter,\nthe common activation function, ReLU(Â·), is chosen to han-\ndle unbounded inputs. Aided by the new activation Ï•(Â·),\nthe calculated distances can be well modulated and help the\nperformance. In summary, Tip-Adapter could acquire the\nwell-performed adapter weights without training. In other\nwords, it is more efï¬cient and effective on few-shot classi-\nï¬cation.\nTip-Adapter can greatly improve the classiï¬cation per-\nformance of CLIP by incorporating new knowledge pro-\nvided by the few-shot training set and be implemented as\na CLIP-Adapter with cached W1 and W2. However, given\nmore shots, there still exists slight performance gap between\nTip-Adapter and CLIP-Adapter. To mitigate the gap, we can\ntreat the cache model as a good initialization point and con-\ntinue to ï¬ne-tune the Tip-Adapter via SGD to surpass the\nrandomly initialized CLIP-Adapter.\nFor ï¬ne-tuning, we supervise the Tip-Adapterâ€™s predic-\ntions with few-shot training data and the cross-entropy loss,\nduring which the weights in the cache model are updated\nvia SGD. Speciï¬cally, we unfreeze the weights of keys\nW1 = Ftrain, but still ï¬x the weights of value W2 = Ltrain\nand two encoders of the CLIP model. The intuition is that\nupdating the keys in the cache model can adaptively boost\nthe estimation of afï¬nities, namely, the distances calcu-\nlation between training and testing images in the embed-\nding space. In contrast, values in the cache are one-hot\nencodings representing ground-truth annotations and shall\nbe kept frozen to accurately memorize the memory infor-\nmation. Thanks to the good initialization provided by the\ncache model, Tip-Adapter only needs to be ï¬ne-tuned for a\nsmall number of epochs and can surpass the CLIP-Adapter\nwith much longer training scheme (20 epochs versus 200\nepochs). It demonstrates that the proposed Tip-Adapter can\nachieve strong performance with fast convergence and lim-\nited resources.\n3.3. Relationship with Cache-based Networks\nConstructing a cache model from few-shot training data\nhas been explored by many previous methods, includ-\ning Matching Network [63], Prototypical Networks [55],\nMAML [16], Relation Network [58] and others [6,7,12,60].\nDifferent from all previous methods, Tip-Adapter adopts\nthe residual architecture following CLIP-Adapter but pro-\nposes to use cached features for properly setting the weights\nof the CLIP-adapter. By performing further ï¬ne-tuning,\nTip-Adapter signiï¬cantly improves the performance of few-\nshot learning.\nFrom another perspective, our Tip-Adapter can be re-\ngarded as a heterogenous cache model with both visual and\ntextual features extracted by CLIP. Speciï¬cally, the weights\nWc of CLIPâ€™s linear classiï¬er can be treated as cached\ntextual features. Therefore, the ï¬nal classiï¬cation of the\ntest image ftest is jointly inferred by multi-modality fea-\ntures, which fully exploit vision-language prior knowledge\nencoded in CLIP. From this perspective, the two terms in\nEq. (6) can be reinterpreted as distance calculation with\ncached visual and textual features, respectively, and their\nimportance is balanced by Î±.\n4. Experiments\n4.1. Training Settings\nWe conduct experiments for Tip-Adapter on 11 image\nclassiï¬cation datasets: ImageNet [9], StandfordCars [36],\nUCF101 [56], Caltech101 [15], Flowers102 [47], SUN397\n[66], DTD [8], EuroSAT [25], FGVCAircraft [44], Ox-\nfordPets [50], and Food101 [3]. Performance comparison\nis conducted between Zero-shot CLIP [51], Linear-probe\nCLIP [51], CLIP-Adapter [17] and CoOp [70].\nAs mentioned in the previous section, our Tip-Adapter\nhas two versions. The ï¬rst version is training-free, which\ndirectly sets the adapterâ€™s MLP weights following Eq. (7).\nThe second version allows further ï¬ne-tuning of the adapter\ninitialized by the properly set weights. The two version are\ndenoted as Tip-Adapter and Tip-Adapter-F in this section,\nrespectively. Each model is trained with 1, 2, 4, 8, and 16\nfew-shot training sets, and tested on the full test sets. For\nthe CLIP backbone, we utilize ResNet-50 [24] as the vi-\nsual encoder and a transformer [14] as the textual encoder.\nIn terms of prompt design, we adopt prompt ensembling in\n[51], which inputs 7 templates into the CLIP textual encoder\nand then averages them as the ï¬nal prompt. The 7 templates\nare: â€œitap of a [CLASS].â€, â€œa bad photo of the [CLASS].â€,\nâ€œa origami [CLASS].â€, â€œa photo of the large [CLASS].â€, â€œa\n[CLASS] in a video game.â€, â€œart of the [CLASS].â€ and â€œa\nphoto of the small [CLASS].â€. To ï¬ne-tune Tip-Adapter-F,\nwe train it with a batch size of 256, and use Stochastic Gra-\ndient Descent (SGD) [34,42] with a learning rate 0.001 and\na cosine scheduler. In contrast to the 200-epoch training\nin CoOp and CLIP-Adapter, Tip-Adapter-F only requires\n20 epochs for ï¬ne-tuning and has super-fast convergence\nspeed, saving much computational cost for training.\nThere are two image pre-processing methods adopted by\nexisting methods. The ï¬rst one is adopted by CLIP and the\nsecond one is reported in CoOp and CLIP-Adapter. We de-\nnote them as CLIP-style and CoOp-style pre-processing, re-\nspectively. Both of them are composed of random cropping,\nresizing, and random horizontal ï¬‚ip. They are different in\nresizing. The CLIP-style pre-processing resizes the cropped\nimageâ€™s short side to 224 while keeping its original aspect\nratio, while the CoOp-style resizes imageâ€™s both sides to\n224. We follow CLIP-style pre-processing by default, since\nit preserves cropped imagesâ€™ original aspect ratios.\n4.2. Comparison on ImageNet\nWe compare our proposed Tip-Adapter and Tip-Adapter-\nF with Zero-shot CLIP [51], Linear-probe CLIP [51], CoOp\n[70], and CLIP-Adapter [17]. The zero-shot CLIP uses\nno extra training sample and conducts classiï¬cation purely\nby calculating similarities between visual features of test\nimages and textual features of the hand-crafted prompts.\nLinear-probe CLIP ï¬ne-tunes an additional linear classiï¬er\nafter the original frozen CLIP on the few-shot training set.\nTo make prompts learnable, CoOp learns to generate dif-\nferent designs for the format of prompts, which we select\nits best-performance variant for comparison â€“ placing the\nclass token at the end of the 16-token prompts and shar-\ning the context among all classes. CLIP-Adapter appends\na feature adapter [27] on top of CLIPâ€™s visual encoder and\ntextual encoder to narrow the domain gap between the pre-\ntrained and transferred features, which helps to achieve bet-\nter few-shot classiï¬cation performance. Likewise, we com-\npare Tip-Adapter with the best-performing variant of CLIP-\nAdapter with only learnable visual adapter. Moreover, for\nfair comparison, we adopt prompt ensembling of 7 tem-\nplates for zero-shot CLIP and CLIP-Adapter, the same as\nTip-Adapter.\nWe report other models with both CLIP-style pre-\nprocess and CoOp-style pre-process in Table 2 for fair com-\nparison, but in Table 1, Figure 3 and the following pages,\nonly the results with CLIP-style pre-processing are pre-\nsented. Also, the far-behind performance of linear-probe\nCLIP are only presented in Figure 3 for conciseness.\nPerformance Analysis. As shown in Figure 3 and Ta-\nble 2, both versions of our Tip-Adapter show outstand-\ning performances over compared methods. Similarly with-\nout training, Tip-Adapter consistently surpasses zero-shot\nCLIP. Compared to linear-probe CLIP and CoOp with time-\nconsuming ï¬ne-tuning, Tip-Adapter greatly exceeds them\nwhen the numbers of training samples are limited. For\ninstance, in 1-shot and 2-shot settings, Tip-Adapter sur-\npasses linear-probe CLIP by 38.53 % and 29.06 %, and\nCoOp by 13.08 % and 10.08%. They demonstrate the su-\nperiority of our training-free cache-based method. How-\never, Tip-Adapter still falls behind the well-tuned CLIP-\nAdapter. With further ï¬ne-tuning, Tip-Adapter-F, updates\nthe weights initialized from the cache model and achieves\nthe best performance over all methods in all few-shot set-\ntings. Compared to the training-free version, the perfor-\nmance gain resulted from ï¬ne-tuning becomes larger as\nthe number of training samples increases, from 1-shotâ€™s\n+0.62% to 16-shotâ€™s +3.44%. This accords with the intu-\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n47.5\n50.0\n52.5\n55.0\n57.5\n60.0\n62.5\n65.0Score (%)\nZero-shot\nCLIP\nImageNet\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\nFigure 3. Classiï¬cation accuracy of Tip-Adapter, Tip-Adapter-F\nand other models under different few-shot settings with CLIP-style\npre-processing.\nModels RN50 RN101 ViT/32 ViT/16 RN50 Ã—16\nZero-shot CLIP [51] 60.33 62.53 63.80 68.73 70.94\nCoOp [70] 62.95 66.60 66.85 71.92 -\nCLIP-Adapter [17] 63.59 65.39 66.19 71.13 -\nTip-Adapter 62.03 64.78 65.61 70.75 72.95\nTip-Adapter-F 65.51 68.56 68.65 73.69 75.81\nTable 1. Performances ( %) of different models on various vision\nbackbones. RN50 denotes ResNet-50, and ViT/32 denotes ViT-\nBase with 32 Ã— 32 patch size, and RN50 Ã—16 denotes ResNet-50\nwith 16 times more computations [51].\nition that more training samples provide the model with a\nmore robust cache.\nIn Table 1, we also implement Tip-Adapter with dif-\nferent visual encoders over ResNet [24] and Vision Trans-\nformer [14] backbones. The leading performances of Tip-\nAdapter-F fully demonstrate its superior cabability for vi-\nsual recognition.\nEfï¬ciency Comparison. In Table 3, we show the 16-shot\ntraining time of different models and the experiments are\nconducted on a single NVIDIA GeForce RTX 3090 GPU.\nTip-Adapter and CLIP-Adapter ï¬rst cache the textual fea-\ntures from the CLIP model and load them during training,\nbut CoOp adopts learnable prompts so that the textual fea-\ntures are required to be calculated online in every iteration.\nLinear-probe CLIP utilizes Logistic Regression [65] to train\nthe ï¬nal linear layer, so it cannot be measured by epoch.\nFrom the comparison we can observe that CoOp takes most\ntraining time for learning prompts and has a 2.26 % per-\nformance gain over zero-shot CLIP. CLIP-Adapter signiï¬-\ncantly reduces the training time with a larger improvement.\nTip-Adapter gains 1.70% boost but requires no extra train-\nFew-shot Setup 1 2 4 8 16\nCoOp-style Pre-process, Zero-shot CLIPâˆ— [51]: 57.81\nLinear-probe CLIPâˆ— [51] 21.14 31.67 40.33 47.12 51.59\nCoOpâˆ— [70] 53.12 53.23 56.47 57.58 60.46\nCLIP-Adapterâˆ— [17] 58.17 58.58 59.40 60.39 61.33\nCLIP-style Pre-process, Zero-shot CLIP [51]: 60.33\nLinear-probe CLIP [51] 22.17 31.90 41.20 49.52 56.13\nCoOp [70] 47.62 50.88 56.22 59.93 62.95\nCLIP-Adapter [17] 61.20 61.52 61.84 62.68 63.59\nTip-Adapter 60.70 60.96 60.98 61.45 62.03\nTip-Adapter-F 61.32 61.69 62.52 64.00 65.51\nby ï¬ne-tuning +0.62 +0.73 +1.54 +2.55 +3.48\nTable 2. Classiï¬cation accuracy (%) of models under both CoOp-\nstyle and CLIP-style pre-processing in few-shot settings. Models\nfollowing the CoOp-style pre-process method are labeled by âˆ—.\nThe last row in blue records the performance gain brought by fur-\nther ï¬ne-tuning over Tip-Adapter.\nModels Epochs Time Accuracy Gain\nZero-shot CLIP [51] 0 0 60.33 0\nLinear-probe CLIP [51] - 13min 56.13 -4.20\nCoOp [70] 200 14h 40min 62.95 +2.62\nCLIP-Adapter [17] 200 50min 63.59 +3.26\nTip-Adapter 0 0 62.03 +1.70\nTip-Adapter-F 20 5min 65.51 +5.18\nTable 3. Fine-tuning time and classiï¬cation accuracy of 16-shot\nlearning with different methods. The last column in blue records\nthe performance gain relative to zero-shot CLIP.\ning time, which makes it a good trade-off between per-\nformance and efï¬ciency. Tip-Adapter-F further reaches a\nhigher accuracy with only 1/10 of CLIP-Adapterâ€™s epochs,\nachieving best of both worlds.\n4.3. Performances on Other Datasets\nFigure 4 shows Tip-Adapterâ€™s performances on other 10\ndatasets: StandfordCars [36], UCF101 [56], Caltech101\n[15], Flowers102 [47], SUN397 [66], DTD [8], EuroSAT\n[25], FGVCAircraft [44], OxfordPets [50], and Food101\n[3]. Comparing with Zero-shot CLIP [51], CLIP-Adapter\n[17] and CoOp [70], we observe that our Tip-Adapter sig-\nniï¬cantly boosts the classiï¬cation performance over zero-\nshot CLIP in different types of datasets. Although on some\ndatasets, Tip-Adapter falls behind CLIP-Adapter when\nthere are more shots for training, Tip-Adapter-F with a few-\nepoch ï¬ne-tuning eliminates this gap and further surpasses\nall other models. The consistent superiority over 10 datasets\nfully demonstrates the effectiveness and generality of Tip-\nAdapter.\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n15\n20\n25\n30\n35Score (%)\nZero-shot\nCLIP\nFGVCAircraft\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n78\n80\n82\n84\n86\n88Score (%)\nZero-shot\nCLIP\nOxfordPets\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n68\n70\n72\n74\n76\n78Score (%)\nZero-shot\nCLIP\nFood101\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n54\n56\n58\n60\n62\n64\n66\n68\n70Score (%)\nZero-shot\nCLIP\nSUN397\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5Score (%)\nZero-shot\nCLIP\nUCF101\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n80\n82\n84\n86\n88\n90\n92Score (%)\nZero-shot\nCLIP\nCaltech101\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n45\n50\n55\n60\n65Score (%)\nZero-shot\nCLIP\nDTD\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n65\n70\n75\n80\n85\n90\n95Score (%)\nZero-shot\nCLIP\nFlowers102\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n40\n50\n60\n70\n80Score (%)\nZero-shot\nCLIP\nEuroSAT\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n45\n50\n55\n60\n65\n70\n75Score (%)\nZero-shot\nCLIP\nStanfordCars\nTip-Adapter-F\nTip-Adapter\nCLIP-Adapter\nCoOp\nZero-shot CLIP\nFigure 4. Results of few-shot classiï¬cation on 10 datasets. Tip-Adapter exceeds zero-shot ClIP by a large margin, and Tip-Adapter-F\nfurther surpasses all compared methods by few-epoch ï¬ne-tuning.\n4.4. Ablation Studies\nIn this section, we conduct several ablation studies\nabout Tip-Adapter on ImageNet with the CLIP-style pre-\nprocessing. All experiments adopt the 16-shot setting and\nour training-free Tip-Adapter is adopted by default.\nResidual Ratio Î±. The hyper-parameter Î± combines\nnewly adapted features from cache model with pre-trained\nCLIP encoderâ€™s features, or in other words, weighing the\nimportance of the visual and textual cache. As formulated\nabove, larger Î± denotes using more knowledge from the\nfew-shot training set and less otherwise. We vary Î± from\n0.0 to 5.0, and set the hyper-parameter Î² as 5.5. When Î±\nequals 0.0, the model is equivalent to zero-shot CLIP with-\nout using new knowledge from the training set. From the\ntop part of Table 4, we observe that the classiï¬cation accu-\nracy is improving as Î±increases from 0.0 to 1.0, achieving\nthe best 62.03% at 1.0 , but further increasing the weight of\nthe adapted features hurts the performance. This indicates\nthat both the prior knowledge from CLIP and adapted fea-\ntures from the few-shot training set are equally important.\nSharpness Ratio Î². In Eq. (5), Î² in the activation func-\ntion Ï• controls the sharpness of the afï¬nities. When Î² is\nlarge, only the nearby training samples to the test image\nhave large inï¬‚uences to its class prediction and vice versa.\nThe results of different Î² are presented on the second part\nof Table 4, in whichÎ±is set as 1.0. We observe that the vari-\nation of Î²has limited impact. A moderate 5.5 for Î²leads to\nthe best-performing Tip-Adapter.\nSize of the Cache Model. Here, we explore the inï¬‚uence\nof the cache modelâ€™s size in Tip-Adapter. The largest set-\nting caches 16 samples for each class, and the smallest one\nAblation Studies on Tip-Adapter\nResidual RatioÎ± 0.0 0.5 1.0 2.0 3.0 4.0\n60.33 61.44 62.03 61.41 60.36 59.14\nSharpness RatioÎ² 1.5 3.5 5.5 7.5 9.5 11.5\n61.82 61.91 62.03 61.76 61.62 61.40\nCache Size 0 1 2 4 8 16\n60.33 61.45 61.71 61.79 61.83 62.03\nMore Shots\nthan 16\nShot Setup 16 32 64 128\nTip-Adapter 62.03 62.51 62.88 63.15\nTip-Adapter-F 65.47 66.58 67.96 69.74\nTable 4. Four ablation studies of the proposed Tip-Adapter on\nImageNet, from top to bottom: residual ratio Î±, sharpness ratio\nÎ², the size of cache model and the performance given more shots\nwith cache size 16.\ncaches only one sample per class. We experiment with K\nequaling 0, 1, 2, 4, 8 and 16. Cache size 0 is equivalent\nto zero-shot CLIP without the cache. When it is more than\n0 but less than 16, taking 8 as an example, we randomly\ndivide 16 samples into 8 uniform groups and obtain 8 pro-\ntotypes by averaging features of 2 samples in each group.\nConsidering such random division of samples might inï¬‚u-\nence the performance, we experiment for 5 times and report\nthe average scores. The results from the third part of Ta-\nble 4 illustrate that the more samples we cache, the higher\naccuracy Tip-Adapter can achieve.\nScaling Up to More Shots. One might concern if more\nshots are given, the increasing size of the cache model along\nwith higher-dimensional adapter would become a burden\nfor both memory and computation. Thus, we here explore a\nway to fully utilize the training set with more than 16 shots,\nTip-AdapterUnderFine-tuning Tip-Adapter-F\nFigure 5. t-SNE Visualization of W1 in Tip-Adapter. Dots in different colors stand for embeddings of different categories. From left to\nright, three distributions indicate the variation of W1 during ï¬ne-tuning.\nZero-shot CLIP CoOp CLIP-Adapter Tip-Adapter Tip-Adapter-F\n58\n60\n62\n64\n66Score (%)\n60.33\n0\n63.59\n62.03\n65.51\n58.24\n0\n63.04\n60.82\n65.03\n62.95\nPrompt Variations\nSingle Prompt\nPrompt Ensembling\nLearnable Prompt\nFigure 6. Performances of compared methods with three prompt\ndesigns: Single Prompt in cyan, Prompt Ensembling in orange and\nLearnable Prompt in purple.\nbut ï¬x the cache size as 16. Taking 64 shots as an example,\nfollowing the division strategy in the above paragraph, we\nobtain 16 prototypes from 4 groups to construct the cache\nmodel. The results in the ï¬nal part of Table 4 show that even\nif the cache size is restrained to 16, it still well contains the\ninformation of 32, 64 and 128 training samples per category.\nWe also observe that the growth rate of performance grad-\nually slows down when more samples are provided. This\nindicates a possible limit load for cache size 16, but Tip-\nAdapter-F solves it by ï¬ne-tuning and more shots result in\nlarger performance boost.\nPrompt Design. For zero-shot CLIP, CLIP-Adapter and\nTip-Adapter, the experiments above are all based on prompt\nensembling of 7 templates from [51]. In this study, we test\nonly using a single prompt: â€œa photo of a [CLASS].â€ to\nsee how this inï¬‚uences the performance. As shown in Fig-\nVisual Enc. Textual Enc. AdapterW1 AdapterW2 Score(%)\n- - - - 62.03\n- - ! - 65.51\n- - - ! 60.90\n- - ! ! collapse\n! - - - 62.84\n- ! - - 63.15\n! ! - - 51.22\nTable 5. Ablations of Tip-Adapter ï¬ne-tuning different modules.\n!denotes ï¬ne-tuning the module and symbol - denotes freezing.\nVisual Enc. and Textual Enc. stand for visual encoder and textual\nencoder in pre-trained CLIP.\nure 6, the performance drops resulted from using a single\nprompt are smaller for Tip-Adapter-F and CLIP-Adapter,\nwhose classiï¬cation scores are relatively high, but larger\nfor Tip-Adapter and Zero-shot CLIP. In general, the better-\nperforming models are less affected by prompt variations.\nFine-tuning Settings. Tip-Adapter-F only ï¬ne-tunes\nadapterâ€™sW1 in the cache model, but freezesW2, CLIPâ€™s vi-\nsual encoder and textual encoder. Here, we explore whether\nother modules in Tip-Adapter shall be ï¬ne-tuned. In Ta-\nble 5, we conduct 7 ï¬ne-tuning experiments on unfreez-\ning different modules of Tip-Adapter. Note that we set\nthe learning rate of the two CLIP encoders as 1/1000 of\nthe learning rate of W1. The ï¬rst and second rows repre-\nsent training-free Tip-Adapterâ€™s 62.03% and Tip-Adapter-\nFâ€™s 65.5%. If we ï¬ne-tune W2 in the adapter, the perfor-\nmance would fall to 60.90% or even collapse, which accords\nwith our assumption that the one-hot ground-truth labels in\nW2 shall not be updated. Furthermore, we ï¬x all weights in\nthe adapter and ï¬ne-tune the pre-trained CLIPâ€™s weights.\nIf the visual encoder or textual encoder is independently\ntuned, the performance could be improved to 62.84 % and\n63.15%, but when both encoders are jointly ï¬ne-tuned, the\nclassiï¬cation accuracy would signiï¬cantly drop because of\nsevere over-ï¬tting, indicating that it is harmful to ï¬ne-tune\nsuch a huge-parameter model with few-shot training set.\n4.5. Visualizations of Tip-Adapter\nIn order to ease the understanding of cache-initialized\nTip-Adapter and the variation during ï¬ne-tuning process,\nwe utilize t-SNE [51] to visualize W1, the ï¬rst linear layer\nin the adapter or the cached training features. As shown\nin Figure 5, dots in different colors denote 10 categories of\nImageNet, and their relative distances here reï¬‚ect the high-\ndimensional distributions of category embeddings. We con-\nduct the visualization under the 16-shot settings, so there\nare 16 embeddings per category. From left to right in Fig-\nure 5, the three sub-ï¬gures represent the initialized Tip-\nAdapter and the Tip-Adapter-F after ï¬ne-tuning, respec-\ntively. It could be observed that before training, such dis-\ntribution has shown good discrimination thanks to the prop-\nerly designed approach. During ï¬ne-tuning, embeddings of\nthe same category gradually converges together and differ-\nent clusters become more contrastive and separate with each\nother, contributing to stronger classiï¬cation capability. This\nclustering process in feature space clearly demonstrates the\nperformance improvement brought by Tip-Adapter-F.\n5. Conclusion\nWe propose Tip-Adapter, the non-parametric extension\nof CLIP-Adapter, which obtains the weights of adapter not\nby SGD training but from a cache model constructed by\nfew-shot training set. In this way, the feature extraction\nby adapter could also be viewed as retrieving the few-shot\nknowledge from the key-value cache. Surprisingly, Tip-\nAdapter with such training-free initialization achieves on\npar or even better results than CLIP-Adapter with train-\ning. In addition, Tip-Adapter could be further enhanced\nby ï¬ne-tuning for just a few epochs, a good trade-off be-\ntween efï¬ciency and performance. Considering limitations,\nalthough it is marginal, Tip-Adapter still requires 20-epoch\nï¬ne-tuning to learn the best-performed weights. Our future\nwork will focus on exploring new methods for adapterâ€™s\nweights construction to fully unleash its power for visual\nrepresentation.\nReferences\n[1] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and\nvisual question answering. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , pages\n6077â€“6086, 2018. 1\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh.\nVqa: Visual question answering. In Proceedings of the IEEE\ninternational conference on computer vision , pages 2425â€“\n2433, 2015. 1\n[3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101â€“mining discriminative components with random\nforests. In European conference on computer vision , pages\n446â€“461. Springer, 2014. 6, 7\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 2, 3\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213â€“229. Springer, 2020. 1,\n3\n[6] Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank\nWang, and Jia-Bin Huang. A closer look at few-shot classi-\nï¬cation. arXiv preprint arXiv:1904.04232, 2019. 5\n[7] Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and\nTrevor Darrell. A new meta-baseline for few-shot learning.\narXiv preprint arXiv:2003.04390, 2020. 5\n[8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy\nMohamed, and Andrea Vedaldi. Describing textures in the\nwild. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 3606â€“3613, 2014. 6,\n7\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248â€“255. Ieee, 2009. 2, 6\n[10] Karan Desai and Justin Johnson. Virtex: Learning visual\nrepresentations from textual annotations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11162â€“11173, 2021. 2\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1, 3\n[12] Guneet S Dhillon, Pratik Chaudhari, Avinash Ravichandran,\nand Stefano Soatto. A baseline for few-shot image classiï¬-\ncation. arXiv preprint arXiv:1909.02729, 2019. 5\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 3\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 1, 6, 7\n[15] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning gener-\native visual models from few training examples: An incre-\nmental bayesian approach tested on 101 object categories. In\n2004 conference on computer vision and pattern recognition\nworkshop, pages 178â€“178. IEEE, 2004. 6, 7\n[16] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-\nagnostic meta-learning for fast adaptation of deep networks.\nIn International Conference on Machine Learning , pages\n1126â€“1135. PMLR, 2017. 5\n[17] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.\nClip-adapter: Better vision-language models with feature\nadapters. arXiv preprint arXiv:2110.04544 , 2021. 2, 3, 4,\n6, 7\n[18] Peng Gao, Zhengkai Jiang, Haoxuan You, Pan Lu,\nSteven CH Hoi, Xiaogang Wang, and Hongsheng Li. Dy-\nnamic fusion with intra-and inter-modality attention ï¬‚ow for\nvisual question answering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 6639â€“6648, 2019. 1\n[19] Peng Gao, Minghang Zheng, Xiaogang Wang, Jifeng Dai,\nand Hongsheng Li. Fast convergence of detr with spatially\nmodulated co-attention. arXiv preprint arXiv:2101.07448 ,\n2021. 1\n[20] Edouard Grave, Moustapha Ciss Â´e, and Armand Joulin. Un-\nbounded cache model for online language modeling with\nopen vocabulary. arXiv preprint arXiv:1711.02604 , 2017.\n2, 3\n[21] Jean-Bastien Grill, Florian Strub, Florent Altch Â´e, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020. 2\n[22] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729â€“9738, 2020. 2\n[23] Kaiming He, Georgia Gkioxari, Piotr Doll Â´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961â€“2969, 2017. 2\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770â€“778, 2016. 1, 2, 6, 7\n[25] Patrick Helber, Benjamin Bischke, Andreas Dengel, and\nDamian Borth. Eurosat: A novel dataset and deep learning\nbenchmark for land use and land cover classiï¬cation. IEEE\nJournal of Selected Topics in Applied Earth Observations\nand Remote Sensing, 12(7):2217â€“2226, 2019. 6, 7\n[26] Sepp Hochreiter and J Â¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735â€“1780, 1997. 3\n[27] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly. Parameter-efï¬cient transfer\nlearning for nlp. In ICML, 2019. 3, 6\n[28] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efï¬cient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 1\n[29] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei.\nAttention on attention for image captioning. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 4634â€“4643, 2019. 1\n[30] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, 2021. 2\n[31] Jeff Johnson, Matthijs Douze, and Herv Â´e J Â´egou. Billion-\nscale similarity search with gpus. IEEE Transactions on Big\nData, 2019. 3\n[32] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettle-\nmoyer, and Mike Lewis. Generalization through memoriza-\ntion: Nearest neighbor language models. arXiv preprint\narXiv:1911.00172, 2019. 2, 3\n[33] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bi-\nlinear attention networks. arXiv preprint arXiv:1805.07932,\n2018. 1\n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 2, 6\n[35] Jannik Kossen, Neil Band, Clare Lyle, Aidan N Gomez, Tom\nRainforth, and Yarin Gal. Self-attention between datapoints:\nGoing beyond individual input-output pairs in deep learning.\narXiv preprint arXiv:2106.02584, 2021. 3\n[36] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for ï¬ne-grained categorization. In\nProceedings of the IEEE international conference on com-\nputer vision workshops, pages 554â€“561, 2013. 6, 7\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiï¬cation with deep convolutional neural net-\nworks. In NIPS, 2012. 1, 2\n[38] Yann LeCun, L Â´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE , 86(11):2278â€“2324, 1998.\n3\n[39] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli\nOuyang, Jing Shao, Fengwei Yu, and Junjie Yan. Su-\npervision exists everywhere: A data efï¬cient contrastive\nlanguage-image pre-training paradigm. arXiv preprint\narXiv:2110.05208, 2021. 2\n[40] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr DollÂ´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980â€“2988, 2017. 1\n[41] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-\nroaki Hayashi, and Graham Neubig. Pre-train, prompt, and\npredict: A systematic survey of prompting methods in nat-\nural language processing. arXiv preprint arXiv:2107.13586,\n2021. 2\n[42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 2, 6\n[43] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\nVilbert: Pretraining task-agnostic visiolinguistic represen-\ntations for vision-and-language tasks. arXiv preprint\narXiv:1908.02265, 2019. 1\n[44] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew\nBlaschko, and Andrea Vedaldi. Fine-grained visual classi-\nï¬cation of aircraft. arXiv preprint arXiv:1306.5151 , 2013.\n6, 7\n[45] Mingyuan Mao, Renrui Zhang, Honghui Zheng, Peng Gao,\nTeli Ma, Yan Peng, Errui Ding, and Shumin Han. Dual-\nstream network for visual recognition. arXiv preprint\narXiv:2105.14734, 2021. 1\n[46] Stephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. Pointer sentinel mixture models. arXiv\npreprint arXiv:1609.07843, 2016. 3\n[47] Maria-Elena Nilsback and Andrew Zisserman. Automated\nï¬‚ower classiï¬cation over a large number of classes. In 2008\nSixth Indian Conference on Computer Vision, Graphics &\nImage Processing, pages 722â€“729. IEEE, 2008. 6, 7\n[48] A Emin Orhan. A simple cache model for image recognition.\narXiv preprint arXiv:1805.08709, 2018. 2, 3\n[49] Myle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. Scaling neural machine translation. arXiv preprint\narXiv:1806.00187, 2018. 3\n[50] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In 2012 IEEE conference on\ncomputer vision and pattern recognition, pages 3498â€“3505.\nIEEE, 2012. 6, 7\n[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020, 2021. 1, 2, 3, 4, 6,\n7, 9, 10\n[52] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 1, 3\n[53] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information process-\ning systems, 28:91â€“99, 2015. 1, 2\n[54] David E Rumelhart, Geoffrey E Hinton, and Ronald J\nWilliams. Learning internal representations by error propa-\ngation. Technical report, California Univ San Diego La Jolla\nInst for Cognitive Science, 1985. 2\n[55] Jake Snell, Kevin Swersky, and Richard S Zemel. Pro-\ntotypical networks for few-shot learning. arXiv preprint\narXiv:1703.05175, 2017. 5\n[56] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 6, 7\n[57] Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample,\nHerve Jegou, and Armand Joulin. Augmenting self-attention\nwith persistent memory. arXiv preprint arXiv:1907.01470 ,\n2019. 3\n[58] Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS\nTorr, and Timothy M Hospedales. Learning to compare: Re-\nlation network for few-shot learning. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 1199â€“1208, 2018. 5\n[59] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019. 1\n[60] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B Tenen-\nbaum, and Phillip Isola. Rethinking few-shot image classi-\nï¬cation: a good embedding is all you need? In Computer\nVisionâ€“ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23â€“28, 2020, Proceedings, Part XIV 16 , pages\n266â€“282. Springer, 2020. 5\n[61] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv Â´e JÂ´egou. Training\ndata-efï¬cient image transformers & distillation through at-\ntention. In ICML, 2021. 1\n[62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998â€“6008, 2017. 1,\n3\n[63] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning.Ad-\nvances in neural information processing systems , 29:3630â€“\n3638, 2016. 5\n[64] Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook\nKim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok\nNamkoong, and Ludwig Schmidt. Robust ï¬ne-tuning\nof zero-shot models. arXiv preprint arXiv:2109.01903 ,\n2021. 2\n[65] Raymond E Wright. Logistic regression. 1995. 7\n[66] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. Sun database: Large-scale scene\nrecognition from abbey to zoo. In 2010 IEEE computer so-\nciety conference on computer vision and pattern recognition,\npages 3485â€“3492. IEEE, 2010. 6, 7\n[67] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and\nJiebo Luo. Image captioning with semantic attention. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 4651â€“4659, 2016. 1\n[68] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu,\nMohit Bansal, and Tamara L Berg. Mattnet: Modular atten-\ntion network for referring expression comprehension. InPro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 1307â€“1315, 2018. 1\n[69] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic seg-\nmentation from a sequence-to-sequence perspective with\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 6881â€“\n6890, 2021. 1, 3\n[70] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. arXiv\npreprint arXiv:2109.01134, 2021. 2, 6, 7",
  "topic": "Adapter (computing)",
  "concepts": [
    {
      "name": "Adapter (computing)",
      "score": 0.9656239748001099
    },
    {
      "name": "Computer science",
      "score": 0.7441444396972656
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47398510575294495
    },
    {
      "name": "Computer vision",
      "score": 0.3260241448879242
    },
    {
      "name": "Computer hardware",
      "score": 0.2568392753601074
    }
  ]
}