{
    "title": "Efficiency at scale: Investigating the performance of diminutive language models in clinical tasks",
    "url": "https://openalex.org/W4403679878",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5007127962",
            "name": "Niall Taylor",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A5048070424",
            "name": "Upamanyu Ghose",
            "affiliations": [
                "University of Oxford",
                "King Abdulaziz University"
            ]
        },
        {
            "id": "https://openalex.org/A5022486236",
            "name": "Omid Rohanian",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A5088008419",
            "name": "Mohammadmahdi Nouriborji",
            "affiliations": [
                "Sharif University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5069331474",
            "name": "Andrey Kormilitzin",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A5040302008",
            "name": "David A. Clifton",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A5045725813",
            "name": "Alejo Nevado‚ÄêHolgado",
            "affiliations": [
                "University of Oxford"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6776322294",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4322766882",
        "https://openalex.org/W6838431322",
        "https://openalex.org/W6769243733",
        "https://openalex.org/W4321749402",
        "https://openalex.org/W3035965352",
        "https://openalex.org/W4221067412",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W3128331956",
        "https://openalex.org/W2168041406",
        "https://openalex.org/W2137407193",
        "https://openalex.org/W1034374084",
        "https://openalex.org/W2962897394",
        "https://openalex.org/W4386081032",
        "https://openalex.org/W4393852514",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4385757404",
        "https://openalex.org/W4402683957",
        "https://openalex.org/W4386655575",
        "https://openalex.org/W4296959557",
        "https://openalex.org/W4313484599",
        "https://openalex.org/W4401043314",
        "https://openalex.org/W4280534475",
        "https://openalex.org/W4386080541",
        "https://openalex.org/W2599674900",
        "https://openalex.org/W4323709074",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W3099878876",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4321276774",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4378711639",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4378509449",
        "https://openalex.org/W4233907442",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W4286985375",
        "https://openalex.org/W4390789641",
        "https://openalex.org/W4389520455",
        "https://openalex.org/W4390529183",
        "https://openalex.org/W4205185581",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W4385573954",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W2965373594"
    ],
    "abstract": "The entry of large language models (LLMs) into research and commercial spaces has led to a trend of ever-larger models, with initial promises of generalisability. This was followed by a widespread desire to downsize and create specialised models without the need for complete fine-tuning, using Parameter Efficient Fine-tuning (PEFT) methods. We present an investigation into the suitability of different PEFT methods to clinical decision-making tasks, across a range of model sizes, including extremely small models with as few as 25 million parameters. Our analysis shows that the performance of most PEFT approaches varies significantly from one task to another, with the exception of LoRA, which maintains relatively high performance across all model sizes and tasks, typically approaching or matching full fine-tuned performance. The effectiveness of PEFT methods in the clinical domain is evident, particularly for specialised models which can operate on low-cost, in-house computing infrastructure. The advantages of these models, in terms of speed and reduced training costs, dramatically outweighs any performance gain from large foundation LLMs. Furthermore, we highlight how domain-specific pre-training interacts with PEFT methods and model size, finding the domain pre-training to be particularly important in smaller models and discuss how these factors interplay to provide the best efficiency-performance trade-off. Full code available at: https://github.com/nlpie-research/efficient-ml.",
    "full_text": null
}