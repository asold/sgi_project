{
  "title": "Crosslingual Retrieval Augmented In-context Learning for Bangla",
  "url": "https://openalex.org/W4389518438",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098545857",
      "name": "Xiaoqian LI",
      "affiliations": [
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A5049198824",
      "name": "Ercong Nie",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A2131857791",
      "name": "Sheng Liang",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universität München"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3035309367",
    "https://openalex.org/W4385574294",
    "https://openalex.org/W166721740",
    "https://openalex.org/W4385573782",
    "https://openalex.org/W3108724972",
    "https://openalex.org/W2266499673",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287888679",
    "https://openalex.org/W29040286",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3160991089",
    "https://openalex.org/W3023225653",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2462558730",
    "https://openalex.org/W3022782624",
    "https://openalex.org/W4389524534",
    "https://openalex.org/W2800708042",
    "https://openalex.org/W4385571891",
    "https://openalex.org/W2963279309",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W2903101678",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4206529673",
    "https://openalex.org/W4380551147",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4389518407",
    "https://openalex.org/W1587977534",
    "https://openalex.org/W2010949961",
    "https://openalex.org/W3198897702",
    "https://openalex.org/W4385572198",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3212697960",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2096260124",
    "https://openalex.org/W4385569990",
    "https://openalex.org/W4378474059",
    "https://openalex.org/W2286975227",
    "https://openalex.org/W2973088264",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4386081764",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3105168542",
    "https://openalex.org/W3173360659"
  ],
  "abstract": "The promise of Large Language Models (LLMs) in Natural Language Processing has often been overshadowed by their limited performance in low-resource languages such as Bangla. To address this, our paper presents a pioneering approach that utilizes cross-lingual retrieval augmented in-context learning. By strategically sourcing semantically similar prompts from high-resource language, we enable multilingual pretrained language models (MPLMs), especially the generative model BLOOMZ, to successfully boost performance on Bangla tasks. Our extensive evaluation highlights that the cross-lingual retrieval augmented prompts bring steady improvements to MPLMs over the zero-shot performance.",
  "full_text": "Proceedings of the First Workshop on Bangla Language Processing (BLP-2023), pages 136–151\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nCrosslingualRetrievalAugmentedIn-contextLearningforBangla\nXiaoqianLi1 ErcongNie1,2 ShengLiang†1,2\n1CenterforInformationandLanguageProcessing(CIS),LMUMunich,Germany\n2 MunichCenterforMachineLearning(MCML),Germany\nXiaoqian.Li@campus.lmu.de\n{nie, shengliang}@cis.lmu.de\nAbstract\nThe promise of Large Language Models\n(LLMs) in Natural Language Processing has\noften been overshadowed by their limited per-\nformance in low-resource languages such as\nBangla. To address this, our paper presents\na pioneering approach that utilizes cross-\nlingual retrieval augmented in-context learn-\ning. By strategically sourcing semantically\nsimilar prompts from high-resource language,\nwe enable multilingual pretrained language\nmodels (MPLMs), especially the generative\nmodel BLOOMZ, to successfully boost per-\nformance on Bangla tasks. Our extensive\nevaluation highlights that the cross-lingual re-\ntrieval augmented prompts bring steady im-\nprovementstoMPLMsoverthezero-shotper-\nformance.\n1 Introduction\nInrecentyears,thefieldofNaturalLanguagePro-\ncessing (NLP) has witnessed transformative ad-\nvancements, especially with the advent of deep\ntransformer techniques (Vaswani et al., 2017; De-\nvlin et al., 2019; Radford et al., 2019). The in-\ntroduction of Large Language Models (LLMs),\nsuch as GPT-3 (Brown et al., 2020b) and GPT-\n4 (OpenAI, 2023), has further revolutionized the\nlandscape. These models showcase unparalleled\nprowess in tasks like text classification and gen-\neration, unified under the umbrella of in-context\nlearning, and cater to a plethora of applications\nacross diverse languages (Conneau et al., 2020;\nRaffel et al., 2020; Radford et al., 2019). While\ncomprehensive benchmarks like XTREME (Hu\net al., 2020) and BUFFET (Asai et al., 2023) un-\nderscore their capabilities, languages such as En-\nglish remain the primary beneficiaries. In stark\ncontrast, several low-resource languages, Bangla\nbeing a prime example, grapple with challenges,\nnotablythescarcityofpretrainingcorpora( Artetxe\n† Correspondingauthor.\nFigure 1: PARC pipeline using decoder-only Multilin-\ngualPretrainedLanguageModels.\nand Schwenk, 2019; Hangya et al., 2022; Sazzed,\n2020).\nDespite having a significant number of na-\ntivespeakers,Banglaremainsunderrepresentedin\nthe NLP arena due to linguistic intricacies, lim-\nited labeled datasets, and prevalent issues like\ndata duplication (Das and Bandyopadhyay, 2010;\nDas and Gambäck, 2014). Although there have\nbeencommendablestridesusingconventionalma-\nchine learning techniques in Bangla NLP tasks,\nthe untapped potential of the latest LLMs is ev-\nident (Bhowmick and Jana, 2021; Wahid et al.,\n2019;Hoqetal. ,2021).\nIn the evolving landscape of in-context learn-\ning with LLMs, the concept of retrieval augmen-\ntation, which emphasizes sourcing semantically\nrichprompts,hasgainedtraction( Shietal. ,2023).\nHowever,whenitcomestomultilingualin-context\nlearning,previousworkslikeMEGA( Ahujaetal. ,\n2023) often limit their scope to task instructions\nand lack deeper semantic insights due to their\n136\napproach of random prompt selection. In con-\ntrast, strategies like PARC (Nie et al., 2023) pave\nthe way for a more comprehensive methodology,\nfetching semantically aligned prompts from high-\nresourcelanguages.\nOurworkdrawsinspirationfromthesemethod-\nologies but introduces novel perspectives. While\nMEGA offers task-level instructions, we infuse\nsemantic understanding into our approach. Sim-\nilar to PARC, our approach is cross-lingual, en-\nsuring a broader application spectrum. Diverg-\ningfromPARC’sfocusonmaskedlanguagemod-\nels like mBERT and XLMR, as shown in Fig-\nure 1, we venture into uncharted territories by\nemploying larger, decoder-only multilingual pre-\ntrained language models (MPLMs) — BLOOM\nand BLOOMZ — to tackle Bangla NLP tasks in\nagenerativestyle( Muennighoffetal. ,2023;Scao\netal.,2022).\nInthispaper,weexploretheapplicationofcross-\nlingual retrieval augmented in-context learning to\nBanglatextclassificationandsummarizationtasks.\nOurmaincontributionsencompass:\n• Anextensiveevaluationofcross-languagere-\ntrieval augmented in-context learning meth-\nods in Bangla, achieving steady improve-\nments over the zero-shot performance of\nMPLMs.\n• A pioneering exploration to extend PARC\nto the generative models, BLOOM and\nBLOOMZ, providing insights for a unified\npipelineofcross-lingual retrievalaugmented\nin-contextlearning.\n2 RelatedWork\nBangla Natural Language Processing Bangla\nis a morphologically rich language with various\ndialects that belongs to the Indo-Aryan branch of\ntheIndo-Europeanlanguagefamily. Withroughly\n270millionspeakersconcentratinginBangladesh\nandsomeregionsofIndia,Banglaisrankedasthe\n7th most widely spoken language in the world1.\nHowever, Bangla is still considered as a low-\nresource language in the NLP research due to the\nscarcityofdigitaltextresourcesandannotatedcor-\npora.\nResearch on Bangla NLP has covered a vari-\nety of common NLP subfields since 1990s, such\n1https://www.ethnologue.com/insights/\nethnologue200/\nas POS tagging (Dandapat et al.,2004;Ekbal and\nBandyopadhyay, 2008b), stemming and lemmati-\nzation (Islam et al., 2007; Paik and Parui, 2008),\nnamed entity recognition (Ekbal and Bandyopad-\nhyay, 2007, 2008a), sentiment analysis (Das and\nBandyopadhyay, 2010; Wahid et al., 2019), news\ncategorization (Mansur, 2006; Mandal and Sen,\n2014), etc. However, the research in different\nareas of Bangla NLP still remains sparse. In\nthe era of deep learning, further progress has\nbeen made in Bangla NLP, particularly in terms\nof the development datasets (Rahman and Ku-\nmar Dey, 2018; Islam et al., 2021, 2023) and\nmodels (Tripto and Ali, 2018; Ashik et al., 2019;\nKarim et al., 2020). Pretrained language models\nhave achieved decent performance in a large va-\nriety of NLP downstream tasks through the fine-\ntuning. Under this background, Bhattacharjee\net al.(2022) pretrained the BanglaBERT model,\naBERT-basedlanguageunderstandingmodelpre-\ntrained on Bangla language corpora. With the ad-\nvent of the large language models (LLMs), zero-\nand few-shot prompting methods have gradually\ngainedprominence. Hasanetal. (2023)compared\nthe zero- and few-shot prompting performance of\nLLMs with the finetuned models for the Bangla\nsentiment analysis task. Our work explores the\napplication of the retrieval-augmented prompting\nmethodinBanglaviolencedetectionandsentiment\nanalysistasks.\nMultilingualIn-contextLearning Brownetal.\n(2020a) demonstrated that LLMs like GPT-3 can\nacquire task-solving abilities by incorporating\ninput-output pairs as context. The in-context\nlearning approach involves concatenating input\nwithrandomlyselectedexamplesfromthetraining\ndataset,whichisalsocalledthepromptingmethod.\nRecentresearch( Gaoetal. ,2021;Liuetal. ,2022,\n2023;Shietal. ,2023)hasexpandedonthisideaby\nenhancing prompts for pretrained models through\nthe inclusion of semantically similar examples.\nThe effectiveness of prompting methods for En-\nglish models extends to multilingual models in\ncross-lingual transfer learning as well.Zhao and\nSchütze (2021) andHuang et al.(2022) investi-\ngatedtheprompt-basedlearningwithmultilingual\nPLMs. Nie et al.(2023) incorporated augmented\nthe prompt with cross-lingual retrieval samples in\nthe multilingual understanding and proposed the\nPARC pipeline.Tanwar et al.(2023) augmented\nthe prompt with not only cross-lingual semantic\n137\nFigure 2: Detailed overview of the PARC pipeline for LRLs using cross-lingual retrieval: (a) An LRL input is\nusedasaqueryforthecross-lingualretriever,whichthenretrievesthemostsemanticallysimilarHRLsamplefrom\nthe HRL corpus. The associated label is either taken directly from the corpus (labeled setting) or determined by\nself-prediction (unlabeled setting). (b) Next, this HRL sample, its label, and the original input are combined to\ncreatearetrieval-enhancedpromptforMPLMprediction.\ninformation but also additional task information.\nHowever,previousstudiesmainlyconcentratedon\nthe multilingual encoder or encode-decoder mod-\nels, while our work extend the PARC pipeline to\nthedecoder-onlymultilingualLLMs.\nMultilingual LLMs In the era of LLMs,\nBLOOMZ and mT0 (Muennighoff et al., 2023)\nare two representative newly emerging multi-\nlingual models. These two multilingual LLMs\nare finetuned on xP3, a multilingual multitask\nfinetuning dataset, and based on the pretrained\nmodelsBLOOM( Scaoetal. ,2022)andmT5( Xue\net al., 2021), respectively. Six different sizes of\nBLOOMZ models are released from 560M to\n176B and 5 different sizes of mT0 models are\nreleased from 300M to 13B. These multilingual\nLLMs open up the possibility for conducting few-\nand zero-shot cross-lingual in-context learning,\nas demonstrated by recent benchmarking efforts,\nfor example MEGA (Ahuja et al., 2023) and\nBUFFET(Asaietal. ,2023).\n3 Methodology\nOurresearchextendstheworkof Nieetal. (2023)\nbyfocusingonimprovingmultilingualpre-trained\nlanguage models (MPLMs) for low-resource lan-\nguagesinazero-shotsetting,specificallyusingre-\ntrievedcontentfromhigh-resourcelanguagessuch\nasEnglish.\nThe backbone of our research approach is a\ntwo-stagepipelineconsistingofacross-lingualre-\ntrieverandapromptengineeringprocessasshown\nin Figure2. This pipeline aims to build on the\nstrengthsofMPLMswhilemitigatingtheirlimita-\ntions, especially when dealing with low-resource\nlanguages. The first stage of the pipeline uses a\ncross-lingual retriever that maps the input Bangla\ntext q to a vectorqembed in a shared embedding\nspace and uses it as a query. Using semantic sim-\nilaritieswith qembed,theretrieverreturnsthemost\nsimilar k examples from high-resource languages\neitherwithorwithouttheirlabels:\nR = arg\nk\nmax\ni∈{1,...,|d|}\ncos(qembed, di)\nwhere di means each document in the high-\nresourcelanguagecorpusand |d| isthenumberof\ndocuments. If there’s no label, it suggests a self-\npredictionstep.\nThe second stage of the pipeline is the prompt\nengineering. The input Bangla text and the re-\ntrievedpatternaresubjectedtothisprocess. Apre-\nfix prompt templateP is used to reformulate the\n138\ninputtofacilitatethemodel’sprediction y:\ny= MPLM (P(q, R))\nDepending on the architecture of the chosen\nMPLM, for decoder-only models, the answer is\ngeneratedbythemodeldirectly. Forencodermod-\nels, the answer is obtained by first mapping each\nlabel to its predefined word using theverbalizer\nand then deducing the label word using mask to-\nkenprediction.\nBy integrating cross-lingual content retrieval\nwithprompt-guidedprediction,weaimtoimprove\nthe ability of MPLMs to handle low-resource lan-\nguages. This synergy not only extracts rich lin-\nguisticinsightsfromhigh-resourcelanguages,but\nalso uses them to improve performance on low-\nresourcelanguagetasks.\n4 Experiments\nInthisstudy,wefocusedonthetasksofclassifica-\ntion and summarization. We refer to our research\napproach,whichuseskretrievedsamplesforcross-\nlingualaugmentedin-contextlearningmethods,as\nthemainmethodinthefollowingsections.\n4.1 Baselines\nZero-shot The template, when populated with\nthe input sample, is fed directly into the MPLM\nfor prediction. This process bypasses the use of\ncross-lingualcontext.\nLead64 The first 64 tokens of the input text are\ntakenasasummaryofthetext(Forsummarization\ntasksonly).\n4.2 Tasks\n4.2.1 Classification\nVio-Lens The Vio-Lens dataset (Saha et al.,\n2023) contains YouTube comments related to vi-\nolentincidentsintheBengalregion,withthegoal\nof highlighting potential threats that could incite\nfurther violence. The prompt templates for both\nmainmethodandzero-shotbaselinearedefinedas\nfollows:\n• BLOOMZ-3bandBLOOM-3b:\nReflecting on the statement\n\"{text}\", which aggressive\nlevel does it resonate with:\nnon-aggressive, slightly\naggressive, or highly aggressive?\n• mBERT: The underlying theme in\n{text} is [MASK].\nwiththeverbalizer:\nv(Direct Violence) =assaultive,\nv(Passive Violence) =indirect,\nv(Non-Violence) =peaceful\nTheEnglishSentimentAnalysisdataset( Rosen-\nthal et al., 2017), which consists of tweets anno-\ntated for sentiment on 2-, 3-, and 5-point scales\nwith labels positive, negative, and neutral, serves\nas the HRL corpora in our study. We use the la-\nbeled training set for our experimental sentence\npool.\nSentNoB Designed to capture the sentiment\nwithin text, SentNoB classifies content as posi-\ntive, negative or neutral (Islam et al., 2021). The\nprompt templates for both main method and zero-\nshotbaselinearedefinedasfollows:\n• BLOOMZ-3bandBLOOM-3b:\nText: {text} What is a possible\nsentiment for the text given the\nfollowing options?\n• mBERT:{text} Sentiment: [MASK]\nwiththeverbalizer:\nv(0) =positive,v(1) =neural,\nv(2) =negative\nWe use the ETHOS (onlinE haTe speecH de-\ntectiON dataSet) (Mollas et al.,2020) as sentence\npoolinourexperiments. Thisrepositoryprovides\nadatasetdesignedtoidentifyhatespeechonsocial\nmedia. We use the binary variants of the dataset,\nwhich contains 998 comments, each labeled for\nthe presence or absence of hate speech. Since the\nlabels are inconsistent, we use the self-prediction\nmethodtopredictthelabels.\n4.2.2 Summarization\nXL-Sum isalargeandvarieddatasetconsisting\nof 1.35 million pairs of articles and their corre-\nsponding summaries (Hasan et al., 2021). These\npairs have been expertly annotated by the BBC\nandmeticulouslyextractedthroughaseriesofcare-\nfully designed heuristic methods. The dataset in-\ncludes 45 languages, from low to high resource,\nmanyofwhichdonotcurrentlyhavepubliclyavail-\nable datasets. The prompt template is defined for\nallmodelsasfollows:\n• Mainmethod:\n{text} Generate a concise summary\n139\nof the above text using the same\nlanguage as the original text\n({target_lang}):\n• Zero-shotbaseline:\n{text} Generate a concise summary\nof the given text:\n4.3 Models\nBLOOM is an autoregressive Large Language\nModeltrainedonadiversecorpustogeneratetext\nbasedonprompts( Scaoetal. ,2022). Itiscapable\nofgeneratingcoherenttextin46languages.\nBLOOMZ takesanovelapproachintheMPLM\nlandscape by applying Bloom filters in the con-\ntextoflanguagemodels( Muennighoffetal. ,2023).\nThis allows the model to use high-resource lan-\nguages to improve embeddings for low-resource\nlanguages, effectively bridging the gap between\nlanguages with different levels of available re-\nsources.\nmBERT isanearlyMPLMthatextendstheorig-\ninal BERT model (Devlin et al., 2018). It is pre-\ntrainedonacorpusof104languages,usingshared\nWordPiecevocabulariesandaunifiedarchitecture\nforalllanguages.\nmT5 orMultilingualT5( Xueetal. ,2021),isan\nextension of the T5 (Text-to-Text Transfer Trans-\nformer)model( Raffeletal. ,2020)designedspecif-\nically for multilingual capabilities. Pre-trained on\nmC4, a large multilingual dataset, mT5 demon-\nstratesmultilingualcapabilitiesbytransformingin-\nputtextsequencesintooutputsequences.\nCross-Lingual Retriever We followed Nie\net al. (2023) to use the multilingual sentence\ntransformer “ paraphrase-multilingual-mpnet-\nbase-v2” (Reimers and Gurevych, 2019). This\ntransformer maps sentences and paragraphs into\na 768-dimensional dense vector space. Such a\nhigh-dimensional embedding facilitates tasks\nsuch as clustering and semantic search. In our\nexperiments, the number of retrieval samples\nk is 1 and 3 for classification task and 1 for\nsummarizationtask.\n5 Results\n5.1 Resultsofclassificationtasks\nTable1providesanoverviewoftheresultsofclas-\nsification. Withtheinstructionsof k = 3retrieval\nVio-Lens zeroshot k=1 k=3\nbloomz-3b 0.19 0.2 0.24\nbloom-3b 0.00 0.00 0.00\nmbert 0.21 0.28 0.29\nSentNoB zeroshot k=1 k=3\nbloomz-3b 0.34 0.44 0.44\nbloom-3b 0.00 0.00 0.00\nmbert 0.30 0.36 0.37\nTable 1: F1-scores of the two classification tasks:\nBangla zero-shot baseline and withk retrieval aug-\nmentedprompts.\naugmented English prompts, we enhance the F1-\nscores of Bloomz-3b on the two tasks by 5% and\n10% respectively. While Bloom-3b, without in-\nstruction tuning compared to Bloomz-3b, cannot\ngenerateanymeaningfulresult,suggestingthatin-\nstruction tuning has a strong impact on retrieval\naugmented in-context learning. The traditional\nmaskedMLM,mBERT,alsogainedimprovement\nby8%and7%.\nTo facilitate a comprehensive understanding of\ntheperformanceanddiscrepanciesassociatedwith\neach task, we present confusion matrices for anal-\nysisasfollows. GiventheconfusionmatrixinTa-\nble2,wefindthat:\n1) With a general assessment across micro,\nmacro, and weighted F1 scores, Bloomz-3b and\nmBERT gained improvement from the retrieval\nprompts. 2) Compare the two models, Bloomz-\n3b’s zero-shot setting tends to misclassify “non-\nviolence”and“Neutral”,andhasareducedmacro\nF1 compared to its weighted F1, while mBERT\nhas a more balanced distribution of confusion be-\ntween “non-violence” (“Neutral”) and the other\nclasses. This may indicate that for classification\ntasks, the text generation struggles more with mi-\nnorityclassescomparedtomaskedprediction.\n5.2 Resultsofsummarisationtask\nTheTable 3comparesseveralmodelsandmethods\nforsummarizationtask.\nLEAD-64 As an extractive method, it performs\nwellacrossallmetrics. Thisindicatesthatinmany\ncasesthefirstfewsentencesortokensofanarticle\nordocumentprovideafairlyinformativesummary.\nAs expected, LEAD-64 outperforms the mt5 base\nmodelinthezero-shotsetting,butisoutperformed\nbytheBloomzmodelsinthesamescenario.\n140\nzeroshot k=1 k=3\nbloomz-3b precision recall f1-score precision recall f1-score precision recall f1-score\naccuracy 0.33 0.35 0.36\nmacroavg 0.15 0.33 0.20 0.18 0.34 0.20 0.26 0.26 0.17\nweightedavg 0.14 0.33 0.19 0.15 0.35 0.20 0.42 0.36 0.24\nmbert precision recall f1-score precision recall f1-score precision recall f1-score\naccuracy 0.22 0.32 0.33\nmacroavg 0.31 0.30 0.18 0.52 0.29 0.21 0.18 0.28 0.21\nweightedavg 0.40 0.22 0.21 0.62 0.32 0.28 0.26 0.33 0.29\nzeroshot k=1 k=3\nbloomz-3b precision recall f1-score precision recall f1-score precision recall f1-score\naccuracy 0.61 0.60 0.61\nmacroavg 0.31 0.37 0.34 0.48 0.48 0.44 0.47 0.48 0.44\nweightedavg 0.51 0.61 0.55 0.53 0.60 0.54 0.53 0.61 0.54\nmbert precision recall f1-score precision recall f1-score precision recall f1-score\naccuracy 0.35 0.37 0.39\nmacroavg 0.38 0.34 0.30 0.40 0.38 0.36 0.42 0.39 0.37\nweightedavg 0.43 0.35 0.34 0.47 0.37 0.39 0.48 0.39 0.41\nTable2: ConfusionmatrixofmainmethodinVio-Lens(top)andSentNoB(bottom)testsetofBLOOMZ-3band\nmBERT.\nR-1 R-2 R-L R-LSum\nLEAD-64 18.17 5.23 12.73 12.74\nzeroshot\nmt5-base 5.01 0.84 4.83 4.84\nbloomz-1b1 22.08 7.11 18.43 18.44\nbloomz-3b 22.36 7.88 18.60 18.58\nk=1\nmt5-base 0.97 0.13 0.91 0.92\nblommz-1b1 10.84 2.80 9.11 9.12\nblommz-3b 6.61 1.52 5.56 5.55\nTable3: RougescoresofBanglasummarization.\nZero-Shot Models mt5-base produces the low-\nest scores across all metrics, suggesting that it\nstruggles to produce satisfactory summaries with-\noutdomain-specificfine-tuningordataaugmenta-\ntion. Both bloomz-1b1 and bloomz-3b show sig-\nnificantlybetterperformance,withbloomz-3bhav-\ning a slight edge over bloomz-1b1, especially in\nbigramcapture(R-2).\nRetrieval augmentation with k=1 Retrieval\naugmentation seems to drastically affect the per-\nformance of mt5-base, reducing its score consid-\nerably. This could be due to noise introduced\nby the retrieved sample or ineffective use of the\nadditional information. For the Bloomz mod-\nels, bloomz-1b1 still retains decent performance,\nalthough there’s a drop when compared to its\nzero-shot performance. Surprisingly, blommz-3b\nshows a sharper drop, suggesting that the addi-\ntional retrieval data may be more of a distraction\nthan an advantage for this model configuration in\nthesummarizationtask.\n5.3 AnalysisandDiscussion\nWhen examining the performance of different\nmodelsondifferenttasks,severalkeyobservations\nemerge that are related to linguistic nuances, the\nunderlying language models, and resource alloca-\ntion.\nFor classification tasks, it’s clear that models\nwith a strong grasp of complex sentence struc-\nture and deeper semantics, such as the Bloomz-\n3b, are more adept at distinguishing nuanced cat-\negories like “passive violence” or the more am-\nbiguous “neutral” sentiment. This aptitude likely\nstems from their ability to understand context bet-\nterthantheirsimplercounterparts. Inparallel,the\ncritical role of zero-shot learning becomes appar-\nent. The ability of a model to generalize a task\nwithoutspecificfine-tuningspeaksvolumesabout\nitsrobustness. Forexample,inourstudies,models\nsuchastheBloomz-3bshowedcommendableper-\nformance in a zero-shot setting. Furthermore, as\nweplayedaroundwiththevariablek(representing\nthenumberofsamplesretrieved),itwasinstructive\nto see that a larger value didn’t always translate\ninto better performance. This underscores the nu-\nancedabilityofamodeltosiftthroughinformation\n141\nFigure3: Modelperformanceoverdifferencesbetweenzero-shot(representedas‘0’onthey-axis)andmainmethod\nwithk=1andk=3demonstrationsforVio-Lenstestsetusingbloomz-3b(left)andmbert(right).They-axisshows\nthedeviationsofthemainmethodfromthezero-shotvalues. Thestatisticsarebasedon8and6templates,shown\ninAppendixTable 5andTable 6,respectively.\nandpotentiallyeliminatenoise.\nTurning to the summarization task, coherence\nandrelevanceseemtobethepillarsofexcellence.\nAdvanced models are more adept at weaving sen-\ntences that are not only structurally coherent, but\nalso rich in information. This finesse is evident\nin the superior Rouge scores of the models. The\ndichotomy between generative and extractive ap-\nproaches is also evident. While generative mod-\nels, including mt5-base and Bloomz-1b1, outper-\nformedtheextractivemodel(LEAD-64)inazero-\nshotframework,theyseemedabitsensitivewhen\nretrievalaugmentationcameintoplay.\nFinally, when it comes to resource distribution,\nthere’s an undeniable correlation between perfor-\nmance and computational resources. The stel-\nlar performance of models like Bloomz-3b likely\ncomes at the cost of intense computational de-\nmands. However, one must consider the cost-\nbenefitratio. Inaddition,thedropinperformance\nofthesemodelswithretrievalaugmentationatk=1\nsuggestsapotentialsensitivitytothebalanceordi-\nversityofthedataset.\nFor the summarization task, an interesting ob-\nservation is that more extensive models don’t al-\nwaysoutperformonallmetrics,suggestingthatwe\nneed to be more discriminating in our resource al-\nlocation. The significant performance drop with\nretrieval augmentation further supports this argu-\nment.\nTo conclude this analysis, while modern lan-\nguage models are capable of handling complex\ntasks, they require careful configuration and\nthoughtful resource distribution. Unraveling the\ncomplexity of these models can pave the way for\noptimizedsolutionsinbothclassificationandsum-\nmarization.\n6 AblationStudy\n6.1 TheStabilityacrossTemplates\nIn our experiment for Vio-Lens, we compared the\nperformance of Bloomz-3b and mbert, in terms\nof their ability to classify text samples into cate-\ngories. Inordertoassesstheeffectivenessofthere-\ntrievalaugmentedpromptingmethodcomparedto\nthezero-shotbaseline,weconductastatisticacross\ndifferenttemplates.\nFor Bloomz-3b and mBERT, we test different\nprompttemplates,andcreatedaboxplot(Figure 3)\nto visualize the difference of F1 scores from our\nmainmethodtothezero-shotbaselineacrosstem-\nplates. It’sshownthatwiththeretrievalaugmented\nEnglish prompts under different templates, both\nmodel achieved a stable improvement compared\ntotheBanglazeroshotbaseline. Asoit’sclearthat\nmBERT,onaverage,showsgreaterimprovements\ninF1scoreswhentransitioningfromthezero-shot\nbaseline to retrieval augmented prompting, com-\nparedtoBloomz-3b.\n6.2 ImpactofBanglaandHindiPrompt\nTemplate\nInstead of English, we further explore applying\nBangla itself and its linguistically similar high-\nresource language Hindi as the language of the\nprompttemplate,asshowninTable 4.\nMainmethodwithEnglishprompt: Thisconfig-\nuration yields the highest macro average F1 score\n142\nk=1 k=3\nprecision recall f1-score precision recall f1-score\nbanglaprompt \"পাঠয্: {text}িনŔিলিখত িবকŬগ‌ুিল েদওয়া পােঠয্র জনয্ সŚাবয্ অনুভূিত কী?\"\naccuracy 0.14 0.45\nmacroavg 0.34 0.09 0.13 0.32 0.28 0.29\nweightedavg 0.51 0.14 0.21 0.49 0.45 0.46\nhindiprompt \"पाठ: {text}िनम्नǺलǺखत िवकल्पों को द ेखते हुए पाठ क े Ǻलए संभािवत भावना क्या ह ै?\"\naccuracy 0.39 0.54\nmacroavg 0.34 0.28 0.29 0.34 0.34 0.34\nweightedavg 0.51 0.39 0.43 0.52 0.54 0.53\nTable4: ResultsofprompttemplateinbanglaandhindiofmainmethodinSentNoBtestofbloomz-3b.\nofallthreeprompttemplates.\nHindi Prompt Template: While the Hindi\nprompttemplateleadstosignificantimprovements\nin precision and recall for individual categories\nsuch as “Neutral”, the macro average F1 score is\nstill lower than that of the main method with the\nEnglishprompt.\nBangla prompt template: The Bangla prompt\ntemplate, while showing some improvements in\nprecisionforspecificcategoriessuchas“positive”,\nexperiences a decrease in recall and overall accu-\nracy. Asaresult,themacroaverageF1scoreisthe\nlowestofthethreetemplates.\nThis means that while the Bangla prompt tem-\nplate may improve performance for specific cat-\negories, it has an overall negative impact on the\nmodel’s ability to generalize across all categories\nintheSentNoBtest. Conversely,theHindiprompt\ntemplate’s improvements in precision and recall\nfor individual categories don’t translate into a\nhigher macro average F1 score compared to the\nmainmethodwiththeEnglishprompt.\nIn summary, the macro average F1-score re-\nsults show that the main method with the English\nprompt template remains the most effective over-\nall. However, the choice of prompt template can\nsignificantly affect performance for specific cate-\ngories, as demonstrated by the Hindi and Bangla\ntemplates. This nuanced understanding under-\nscores the need to balance category-specific and\noverall performance when selecting prompt tem-\nplatesincross-lingualretrievalaugmentation.\n6.3 ImpactofHindisentencepool\nComparingtheresultsinTable 7withtheprevious\nexperiments, we observe that the Hindi retrieval\ndataset generally improves the model’s ability to\nretrieve “Neutral” content in the mBERT model.\nHowever,themodelcontinuestostrugglewiththe\n“Neutral” category, with low recall and F1 scores,\nregardless of the sentence pool used. This sug-\ngests that further refinements may be needed to\nimprove retrieval accuracy for neutral sentiment\nsentences. The studies with Hindi retrieval data\nshow that both bloomz-3b and mbert don’t show\nany improvements compared to the main method\nwith the English prompt template. This suggests\nthat while using alternative retrieval datasets can\nimprove performance for specific sentiment cate-\ngories, the choice of retrieval data may need to\nbecarefullyconsideredtomaximizeoverallperfor-\nmanceacrosscategoriesincross-lingualsentiment\nanalysistasks.\n7 Conclusion\nInthispaper,wehaveintroducedanovelapproach\nto address the challenges of applying Large Lan-\nguage Models to low-resource languages, with\na focus on Bangla. Our methodology employs\ncross-lingualretrieval-augmentedin-contextlearn-\ning, thereby enriching the capabilities of MPLMs,\nspecificallyBLOOMandBLOOMZ.Wehaveex-\ntensivelytestedourapproachontwoclassification\ntasksandonesummarizationtask.\nOur experimental results demonstrate the effec-\ntiveness of our approach in achieve superior F1\nscoresforclassificationtasks.\nUponfurtheranalysis,thecross-lingualretrieval\nmechanismcontributessignificantlytothemodel’s\nperformance.\nThis work lays the foundation for further stud-\niesontheapplicationofcross-lingualretrievaland\nin-context learning methods in low-resource lan-\nguages. Future work could extend this approach\nto even more underrepresented languages and po-\ntentiallyadaptittomorecomplexNLPtaskssuch\nasquestionansweringormachinetranslation.\n143\nLimitations\nWhile our study has yielded promising results, it\nisnotwithoutlimitations. Theeffectivenessofre-\ntrieval augmentation is also tied to the model ar-\nchitecture, and its impact on different models re-\nmains largely unexplored. In addition, the avail-\nability of specific language datasets for sentence\nretrieval and resource constraints remain practi-\ncal challenges. Further exploration of prompt de-\nsign and consideration of external factors could\nimprove our methodology. Acknowledging these\nlimitations is essential for a full interpretation of\nourresultsandthedirectionoffutureresearch.\nAcknowledgements\nThis work was supported by Leibniz Supercom-\nputingCentre(LRZ),MunichCenterforMachine\nLearning(MCML)andChinaScholarshipCouncil\n(CSC).\nReferences\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi\nJain, Harshita Diddee, Samuel Maina, Tanuja Ganu,\nSameer Segal, Maxamed Axmed, Kalika Bali, et al.\n2023. Mega: Multilingual evaluation of generative\nai. arXiv preprint arXiv:2303.12528.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics,7:597–610.\nAkari Asai, Sneha Kudugunta, Xinyan Velocity Yu,\nTerra Blevins, Hila Gonen, Machel Reid, Yulia\nTsvetkov,SebastianRuder,andHannanehHajishirzi.\n2023. Buffet: Benchmarkinglargelanguagemodels\nfor few-shot cross-lingual transfer.arXiv preprint\narXiv:2305.14857.\nMd Akhter-Uz-Zaman Ashik, Shahriar Shovon, and\nSummit Haque. 2019. Data set for sentiment analy-\nsis on bengali news comments and its baseline eval-\nuation. In2019 International Conference on Bangla\nSpeech and Language Processing (ICBSLP), pages\n1–5.IEEE.\nAbhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad,\nKazi Samin Mubasshir, Md Saiful Islam, Anindya\nIqbal, M. Sohel Rahman, and Rifat Shahriyar.\n2022. BanglaBERT: Language model pretraining\nand benchmarks for low-resource language under-\nstanding evaluation in Bangla. In Findings of the\nAssociation for Computational Linguistics: NAACL\n2022, pages 1318–1327, Seattle, United States. As-\nsociationforComputationalLinguistics.\nAnirban Bhowmick and Abhik Jana. 2021.Sentiment\nanalysis for Bengali using transformer based mod-\nels. In Proceedings of the 18th International Con-\nference on Natural Language Processing (ICON),\npages 481–486, National Institute of Technology\nSilchar, Silchar, India. NLP Association of India\n(NLPAI).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind\nNeelakantan,PranavShyam,GirishSastry,Amanda\nAskell, et al. 2020a. Language models are few-shot\nlearners. Advances in neural information processing\nsystems,33:1877–1901.\nTomB. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan,PranavShyam,GirishSastry,Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020b. Language models are few-shot\nlearners. ArXiv,abs/2005.14165.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020.Unsupervised\ncross-lingualrepresentationlearningatscale . InPro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nSandipan Dandapat, Sudeshna Sarkar, and Anupam\nBasu. 2004. A hybrid model for part-of-speech\ntagging and its application to bengali. In Inter-\nnational conference on computational intelligence,\npages169–172.Citeseer.\nAmitavaDasandSivajiBandyopadhyay.2010. Phrase-\nlevel polarity identification for bangla.Int. J. Com-\nput. Linguistics Appl.,1(1-2):169–182.\nAmitava Das and Björn Gambäck. 2014.Identifying\nlanguagesatthewordlevelincode-mixedIndianso-\ncial media text. InProceedings of the 11th Interna-\ntional Conference on Natural Language Processing,\npages 378–387, Goa, India. NLP Association of In-\ndia.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR,abs/1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\n144\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nationforComputationalLinguistics.\nAsif Ekbal and Sivaji Bandyopadhyay. 2007. A hid-\nden markov model based named entity recognition\nsystem: Bengali and hindi as case studies. InPat-\ntern Recognition and Machine Intelligence: Second\nInternational Conference, PReMI 2007, Kolkata, In-\ndia, December 18-22, 2007. Proceedings 2, pages\n545–552.Springer.\nAsif Ekbal and Sivaji Bandyopadhyay. 2008a. Devel-\nopmentofbengalinamedentitytaggedcorpusandits\nuse in ner systems. InProceedings of the 6th Work-\nshop on Asian Language Resources.\nAsif Ekbal and Sivaji Bandyopadhyay. 2008b. A web-\nbasedbengalinewscorpusfornamedentityrecogni-\ntion. Language Resources and Evaluation, 42:173–\n182.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMakingpre-trainedlanguagemodelsbetterfew-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages3816–3830,Online.AssociationforComputa-\ntionalLinguistics.\nViktor Hangya, Hossain Shaikh Saadi, and Alexander\nFraser. 2022.Improving low-resource languages in\npre-trained multilingual language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 11993–\n12006, Abu Dhabi, United Arab Emirates. Associa-\ntionforComputationalLinguistics.\nMd. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj\nAlam, Anika Anjum, Avijit Sarker, and Sheak\nRashed Haider Noori. 2023. Zero- and few-shot\npromptingwithllms: Acomparativestudywithfine-\ntunedmodelsforbanglasentimentanalysis .\nTahmidHasan,AbhikBhattacharjee,Md.SaifulIslam,\nKazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021.XL-\nsum: Large-scale multilingual abstractive summa-\nrizationfor44languages . InFindings of the Associ-\nation for Computational Linguistics: ACL-IJCNLP\n2021, pages 4693–4703, Online. Association for\nComputationalLinguistics.\nMuntasirHoq,PromilaHaque,andMohammedNazim\nUddin.2021. Sentimentanalysisofbanglalanguage\nusingdeeplearningapproaches . InCOMS2.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-task\nbenchmark for evaluating cross-lingual generaliza-\ntion. ArXiv,abs/2003.11080.\nLianzhe Huang, Shuming Ma, Dongdong Zhang, Furu\nWei, and Houfeng Wang. 2022. Zero-shot cross-\nlingual transfer of prompt-based tuning with a uni-\nfied multilingual prompt. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing,pages11488–11497,Abu\nDhabi, UnitedArabEmirates.AssociationforCom-\nputationalLinguistics.\nKhondoker Ittehadul Islam, Sudipta Kar, Md Saiful Is-\nlam, andMohammadRuhulAmin.2021. SentNoB:\nA dataset for analysing sentiment on noisy Bangla\ntexts. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021,pages3265–3271,\nPunta Cana, Dominican Republic. Association for\nComputationalLinguistics.\nMdEkramulIslam,LabibChowdhury,FaisalAhamed\nKhan, Shazzad Hossain, Md Sourave Hossain, Mo-\nhammad Mamun Or Rashid, Nabeel Mohammed,\nand Mohammad Ruhul Amin. 2023. Sentigold: A\nlarge bangla gold standard multi-domain sentiment\nanalysisdatasetanditsevaluation. In Proceedings of\nthe 29th ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining,pages4207–4218.\nMdZahurulIslam,MdNizamUddin,andMumitKhan.\n2007. Alightweightstemmerforbengalianditsuse\ninspellingchecker.\nMdRezaulKarim,BharathiRajaChakravarthi,JohnP\nMcCrae, and Michael Cochez. 2020. Classification\nbenchmarks for under-resourced bengali language\nbased on multichannel convolutional-lstm network.\nIn2020 IEEE 7th International Conference on Data\nScience and Advanced Analytics (DSAA),pages390–\n399.IEEE.\nJiachangLiu, DinghanShen, YizheZhang, BillDolan,\nLawrence Carin, and Weizhu Chen. 2022.What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extraction\nand Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associ-\nationforComputationalLinguistics.\nYanchen Liu, Timo Schick, and Hinrich Schtze. 2023.\nSemantic-orientedunlabeledprimingforlarge-scale\nlanguage models. In Proceedings of The Fourth\nWorkshop on Simple and Efficient Natural Language\nProcessing (SustaiNLP) , pages 32–38, Toronto,\nCanada (Hybrid). Association for Computational\nLinguistics.\nAshisKumarMandalandRiktaSen.2014. Supervised\nlearning methods for bangla web document catego-\nrization. arXiv preprint arXiv:1410.2045.\nMunirul Mansur. 2006.Analysis of n-gram based text\ncategorization for bangla in a newspaper corpus.\nPh.D.thesis,BRACUniversity.\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\nand Grigorios Tsoumakas. 2020.Ethos: an online\nhatespeechdetectiondataset .\n145\nNiklasMuennighoff,ThomasWang,LintangSutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023. Crosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15991–16111, Toronto, Canada. Association\nforComputationalLinguistics.\nErcongNie,ShengLiang,HelmutSchmid,andHinrich\nSchütze. 2023. Cross-lingual retrieval augmented\nprompt for low-resource languages. In Findings of\nthe Association for Computational Linguistics: ACL\n2023, pages 8320–8340, Toronto, Canada. Associa-\ntionforComputationalLinguistics.\nOpenAI.2023. Gpt-4technicalreport .\nJiaulHPaikandSwapanKParui.2008. Asimplestem-\nmer for inflectional languages. InForum for Infor-\nmation Retrieval Evaluation.Citeseer.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDarioAmodei,andIlyaSutskever.2019. Language\nmodelsareunsupervisedmultitasklearners.\nColinRaffel,NoamShazeer,AdamRoberts,Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020.Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer.\nMd Atikur Rahman and Emon Kumar Dey. 2018.\nDatasets for aspect-based sentiment analysis in\nbanglaanditsbaselineevaluation. Data,3(2):15.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 3982–3992, Hong Kong, China. Association\nforComputationalLinguistics.\nSaraRosenthal,NouraFarra,andPreslavNakov.2017.\nSemEval-2017 task 4: Sentiment analysis in Twit-\nter. In Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017),\npages502–518,Vancouver,Canada.Associationfor\nComputationalLinguistics.\nSourav Saha, Jahedul Alam Junaed, Maryam Saleki,\nArnab Sen Sharma, Mohammad Rashidujjaman\nRifat, Mohamed Rahout, Syed Ishtiaque Ahmed,\nNabeel Mohammad, and Mohammad Ruhul Amin.\n2023. Vio-lens: A noveldataset ofannotated social\nnetwork posts leading to different forms of commu-\nnal violence and its evaluation. InProceedings of\nthe 1st International Workshop on Bangla Language\nProcessing (BLP-2023), Singapore. Association for\nComputationalLinguistics.\nSalimSazzed.2020. Cross-lingualsentimentclassifica-\ntion in low-resource Bengali language. InProceed-\nings of the Sixth Workshop on Noisy User-generated\nText (W-NUT 2020), pages 50–60, Online. Associa-\ntionforComputationalLinguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki,\nElizabeth-Jane Pavlick, Suzana Ili’c, Daniel Hess-\nlow, Roman Castagn’e, Alexandra Sasha Luccioni,\nFranccois Yvon, Matthias Gallé, Jonathan Tow,\nAlexander M. Rush, Stella Rose Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Benoît Sagot, Niklas Muennighoff, Al-\nbert Villanova del Moral, Olatunji Ruwase, Rachel\nBawden, Stas Bekman, Angelina McMillan-Major,\nIz Beltagy, Huu Nguyen, Lucile Saulnier, Samson\nTan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lau-\nrenccon, Yacine Jernite, Julien Launay, Margaret\nMitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi,\nAitor Soroa Etxabe, Alham Fikri Aji, Amit Al-\nfassy, Anna Rogers, Ariel Kreisberg Nitzav, Can-\nwenXu,ChenghaoMou,ChrisC.Emezue,Christo-\npher Klamm, Colin Leong, Daniel Alexander van\nStrien,DavidIfeoluwaAdelani,DragomirR.Radev,\nEduardo Gonz’alez Ponferrada, Efrat Levkovizh,\nEthan Kim, Eyal Bar Natan, Francesco De Toni,\nGérard Dupont, Germán Kruszewski, Giada Pis-\ntilli, Hady ElSahar, Hamza Benyamina, Hieu Trung\nTran, Ian Yu, Idris Abdulmumin, Isaac Johnson,\nItziar Gonzalez-Dios, Javier de la Rosa, Jenny\nChim, Jesse Dodge, Jian Zhu, Jonathan Chang,\nJorg Frohberg, Josephine L. Tobing, Joydeep Bhat-\ntacharjee, Khalid Almubarak, Kimbo Chen, Kyle\nLo, Leandro von Werra, Leon Weber, Long Phan,\nLoubna Ben Allal, Ludovic Tanguy, Manan Dey,\nManuel Romero Muñoz, Maraim Masoud, Mar’ia\nGrandury, Mario vSavsko, Max Huang, Maximin\nCoavoux, Mayank Singh, Mike Tian-Jian Jiang,\nMinh Chien Vu, Mohammad Ali Jauhar, Mustafa\nGhaleb, Nishant Subramani, Nora Kassner, Nuru-\nlaqillaKhamis,OlivierNguyen,OmarEspejel,Ona\nde Gibert, Paulo Villegas, Peter Henderson, Pierre\nColombo, Priscilla A. Amuok, Quentin Lhoest,\nRhezaHarliman,RishiBommasani,RobertoL’opez,\nRui Ribeiro, Salomey Osei, Sampo Pyysalo, Se-\nbastian Nagel, Shamik Bose, Shamsuddeen Hassan\nMuhammad, Shanya Sharma, S. Longpre, Somaieh\nNikpoor, S. Silberberg, Suhas Pai, Sydney Zink,\nTiagoTimponiTorrent,TimoSchick,TristanThrush,\nValentin Danchev, Vassilina Nikoulina, Veronika\nLaippala, Violette Lepercq, Vrinda Prabhu, Zaid\nAlyafeai,ZeerakTalat,ArunRaja,BenjaminHeinz-\nerling, Chenglei Si, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y. Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chaffin, Arnaud Stiegler, Debajy-\noti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang,HarshitPandey,HendrikStrobelt,JasonAlan\nFries,JosRozen,LeoGao,LintangSutawika,MSai-\nful Bari, Maged S. Al-Shaibani, Matteo Manica,\nNihal V. Nayak, Ryan Teehan, Samuel Albanie,\nSheng Shen, Srulik Ben-David, Stephen H. Bach,\nTaewoon Kim, Tali Bers, Thibault Févry, Trishala\nNeeraj,UrmishThakker,VikasRaunak,XiangTang,\nZhengXinYong,ZhiqingSun,ShakedBrody,YUri,\n146\nHadar Tojarieh, Adam Roberts, Hyung WonChung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepakNarayanan,HatimBourfoune,JaredCasper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre Franc-\ncoisLavall’ee,RémiLacroix,SamyamRajbhandari,\nSanchit Gandhi, Shaden Smith, Stéphane Requena,\nSuraj Patil, Tim Dettmers, Ahmed Baruwa, Aman-\npreet Singh, Anastasia Cheveleva, Anne-Laure\nLigozat, Arjun Subramonian, Aur’elie N’ev’eol,\nCharles Lovering, Daniel H Garrette, Deepak R.\nTunuguntla,EhudReiter,EkaterinaTaktasheva,Eka-\nterinaVoloshina,EliBogdanov,GentaIndraWinata,\nHailey Schoelkopf, Jan-Christoph Kalo, Jekate-\nrina Novikova, Jessica Zosa Forde, Xiangru Tang,\nJungo Kasai, Ken Kawamura, Liam Hazan, Ma-\nrine Carpuat, Miruna Clinciu, Najoung Kim, New-\nton Cheng, Oleg Serikov, Omer Antverg, Oskar\nvanderWal,RuiZhang,RuochenZhang,Sebastian\nGehrmann, Shachar Mirkin, S. Osher Pais, Tatiana\nShavrina, Thomas Scialom, Tian Yun, Tomasz Lim-\nisiewicz, Verena Rieser, Vitaly Protasov, Vladislav\nMikhailov, Yada Pruksachatkun, Yonatan Belinkov,\nZachary Bamberger, Zdenvek Kasner, Zdeněk Kas-\nner,AmandaPestana,AmirFeizpour,AmmarKhan,\nAmy Faranak, Ananda Santa Rosa Santos, An-\nthony Hevia, Antigona Unldreaj, Arash Aghagol,\nArezoo Abdollahi, Aycha Tammour, Azadeh Ha-\njiHosseini, Bahareh Behroozi, Benjamin Olusola\nAjibade, Bharat Kumar Saxena, Carlos Muñoz Fer-\nrandis, Danish Contractor, David M. Lansky, Davis\nDavid, Douwe Kiela, Duong Anh Nguyen, Edward\nTan, Emily Baylor, Ezinwanne Ozoani, Fatim Tahi-\nrah Mirza, Frankline Ononiwu, Habib Rezane-\njad, H.A. Jones, Indrani Bhattacharya, Irene So-\nlaiman, Irina Sedenko, Isar Nejadgholi, Jan Pass-\nmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort,\nLíviaMacedoDutra, MaironSamagaio, MaraimEl-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, M. K. K.\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen\nRajani, Nour Elkott, Nourhan Fahmy, Olanrewaju\nSamuel, Ran An, R. P. Kromann, Ryan Hao,\nSamira Alizadeh, Sarmad Shubber, Silas L. Wang,\nSourav Roy, Sylvain Viguier, Thanh-Cong Le,\nTobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang,\nZachary Kyle Nguyen, Abhinav Ramesh Kashyap,\nA. Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Kumar Singh,\nBenjamin Beilharz, Bo Wang, Caio Matheus Fon-\nseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin\nXu, Clémentine Fourrier, Daniel Le’on Perin’an,\nDaniel Molano, Dian Yu, Enrique Manjavacas,\nFabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddinBayrak,GullyBurns,HelenaU.Vrabec,\nIman I.B. Bello, Isha Dash, Ji Soo Kang, John\nGiorgi, Jonas Golde, Jose David Posada, Karthi\nSivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi,MarcPàmies,MaríaAndreaCastillo,Mar-\nianna Nezhurina, Mario Sanger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, M Wolf, Mina\nMihaljcic, Minna Liu, Moritz Freidank, Myung-\nsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pas-\ncaleFung,PatriciaHaller,R.Chandrasekhar,Renata\nEisenberg, Robert Martin, Rodrigo L. Canalli, Ros-\naline Su, Ruisi Su, Samuel Cahyawijaya, Samuele\nGarda, Shlok S Deshmukh, Shubhanshu Mishra,\nSid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Sr-\nishtiKumar,StefanSchweter,SushilPratapBharati,\nT. A. Laud, Th’eo Gigant, Tomoya Kainuma, Woj-\nciechKusa,YanisLabrak,YashasviBajaj,Y.Venka-\ntraman, Yifan Xu, Ying Xu, Yu Xu, Zhee Xao\nTan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes\nBelkada, and Thomas Wolf. 2022.Bloom: A 176b-\nparameteropen-accessmultilinguallanguagemodel .\nArXiv,abs/2211.05100.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nEshaan Tanwar, Subhabrata Dutta, Manish Borthakur,\nand Tanmoy Chakraborty. 2023. Multilingual\nLLMs are better cross-lingual in-context learners\nwith alignment. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 6292–\n6307, Toronto, Canada. Association for Computa-\ntionalLinguistics.\nNafis Irtiza Tripto and Mohammed Eunus Ali. 2018.\nDetecting multilabel sentiment and emotions from\nbangla youtube comments. In 2018 International\nConference on Bangla Speech and Language Pro-\ncessing (ICBSLP),pages1–6.IEEE.\nAshishVaswani,NoamM.Shazeer,NikiParmar,Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017.Attention is all\nyouneed. InNIPS.\nMd Ferdous Wahid, Md Jahid Hasan, and Md Shahin\nAlom.2019. Cricketsentimentanalysisfrombangla\ntext using recurrent neural network with long short\nterm memory model. In2019 International Confer-\nence on Bangla Speech and Language Processing\n(ICBSLP),pages1–4.IEEE.\nLinting Xue, Noah Constant, Adam Roberts, Mihir\nKale,RamiAl-Rfou,AdityaSiddhant,AdityaBarua,\nand Colin Raffel. 2021. mT5: A massively mul-\ntilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 483–498, Online. Association for Computa-\ntionalLinguistics.\nMengjieZhaoandHinrichSchütze.2021. Discreteand\nsoftpromptingformultilingualmodels . InProceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8547–8555,\n147\nOnlineandPuntaCana, DominicanRepublic.Asso-\nciationforComputationalLinguistics.\nA Appendix\n148\nzero-shot k=1 k=3\nprompt {text}DirectAggression,IndirectAggression,orNoAggression?\naccuracy 0.53 0.54 0.54\nmacroavg 0.17 0.18 0.18\nweightedavg 0.38 0.38 0.38\nprompt Evaluatethetext: ’{text}’. Wouldyoucategorizeitasabsenceofaggression,mildaggression,\norstrongaggression?\naccuracy 0.18 0.23 0.23\nmacroavg 0.17 0.15 0.15\nweightedavg 0.13 0.16 0.16\nprompt Inthecontextof’{text}’,whichcategorybestcapturesitsaggressionlevel: absenceofaggression,\nmildaggression,orstrongaggression?\naccuracy 0.12 0.15 0.16\nmacroavg 0.1 0.15 0.16\nweightedavg 0.06 0.11 0.12\nprompt Forthetext: ’{text}’,ascertainitsaggressionscale: absenceofaggression,mildaggression,or\nstrongaggression?\naccuracy 0.19 0.21 0.21\nmacroavg 0.13 0.14 0.14\nweightedavg 0.14 0.16 0.15\nprompt Fromthefollowingchoices,whichresonateswiththethemeof’{text}’? Options: NoIntensity,\nLowIntensity,HighIntensity\naccuracy 0.13 0.24 0.19\nmacroavg 0.1 0.17 0.15\nweightedavg 0.12 0.26 0.2\nprompt Fromthefollowingchoices,whichresonateswiththethemeof’{text}’? Options: nointensity,\nlowintensity,highintensity\naccuracy 0.23 0.28 0.27\nmacroavg 0.18 0.22 0.2\nweightedavg 0.22 0.31 0.26\nprompt Inthecontextofthetext’{text}’,whichofthefollowingbestdescribesitstone? Options: No\nIntensity,LowIntensity,HighIntensity\naccuracy 0.14 0.2 0.15\nmacroavg 0.11 0.15 0.12\nweightedavg 0.1 0.18 0.13\nprompt Reflectingonthestatement’{text}’,whichaggressiveleveldoesitresonatewith: non-aggressive,\nslightlyaggressive,orhighlyaggressive?\naccuracy 0.33 0.35 0.36\nmacroavg 0.2 0.2 0.17\nweightedavg 0.19 0.2 0.24\nTable5: F1-scoreresultswith8prompttemplatesofVio-Lenstestusingbloomz-3bmodel\n149\nzero-shot k=1 k=3\nprompt Thetextdisplays[MASK]aggression: {text}\nverbalizer direct,indirect,none\naccuracy 0.36 0.35 0.36\nmacroavg 0.22 0.23 0.23\nweightedavg 0.31 0.31 0.31\nprompt Consideringaggressivetendencies,thisis[MASK]:{text}\nverbalizer overt,covert,absent\naccuracy 0.1 0.2 0.17\nmacroavg 0.07 0.17 0.14\nweightedavg 0.03 0.19 0.15\nprompt Fromanaggressionperspective,thetextis[MASK]:{text}\nverbalizer overt,covert,absent\naccuracy 0.12 0.22 0.2\nmacroavg 0.09 0.18 0.16\nweightedavg 0.06 0.21 0.18\nprompt Thedescribedbehaviorin{text}is[MASK]aggression.\nverbalizer explicit,implicit,neutral\naccuracy 0.24 0.36 0.35\nmacroavg 0.19 0.24 0.23\nweightedavg 0.23 0.31 0.3\nprompt Theunderlyingthemein{text}is[MASK]aggression.\nverbalizer assaultive,indirect,peaceful\naccuracy 0.22 0.32 0.33\nmacroavg 0.18 0.21 0.21\nweightedavg 0.21 0.28 0.29\nprompt {text}isinterpretedas[MASK]aggression.\nverbalizer assaultive,indirect,peaceful\naccuracy 0.51 0.49 0.51\nmacroavg 0.23 0.27 0.25\nweightedavg 0.37 0.37 0.37\nTable6: F1-scoreresultswith6prompttemplatesofVio-LenstestusingmBertmodel\n150\nk=1 k=3\nbloomz-3b precision recall f1-score precision recall f1-score\nNegative 0.58 0.84 0.69 0.59 0.88 0.70\nNeutral 0.09 0.00 0.00 0.08 0.00 0.00\nPositive 0.55 0.49 0.52 0.58 0.47 0.52\naccuracy 0.57 0.58\nmacroavg 0.41 0.44 0.40 0.42 0.45 0.41\nweightedavg 0.48 0.57 0.51 0.49 0.58 0.51\nmbert precision recall f1-score precision recall f1-score\nNegative 0.48 0.24 0.32 0.48 0.33 0.39\nNeutral 0.21 0.34 0.26 0.21 0.28 0.24\nPositive 0.27 0.37 0.31 0.25 0.33 0.28\naccuracy 0.30 0.32\nmacroavg 0.32 0.32 0.30 0.31 0.31 0.31\nweightedavg 0.36 0.30 0.30 0.36 0.32 0.33\nTable7: ResultsinSentNoBtestofBLOOMZ-3bandmBERTwithhindiretrievalcorpus.\n151",
  "topic": "Bengali",
  "concepts": [
    {
      "name": "Bengali",
      "score": 0.9438223242759705
    },
    {
      "name": "Computer science",
      "score": 0.8491711616516113
    },
    {
      "name": "Natural language processing",
      "score": 0.6490137577056885
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6200513243675232
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5952949523925781
    },
    {
      "name": "Generative grammar",
      "score": 0.5085145235061646
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.419864684343338
    },
    {
      "name": "Natural language",
      "score": 0.41123300790786743
    },
    {
      "name": "Information retrieval",
      "score": 0.3448772728443146
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}