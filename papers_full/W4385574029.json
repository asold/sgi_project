{
  "title": "On the Role of Bidirectionality in Language Model Pre-Training",
  "url": "https://openalex.org/W4385574029",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5023341622",
      "name": "Mikel Artetxe",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5006191011",
      "name": "Jingfei Du",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5075834790",
      "name": "Naman Goyal",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5067919401",
      "name": "Luke Zettlemoyer",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5091317839",
      "name": "Veselin Stoyanov",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3207699717",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W4225905768",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W2952729433",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4226279206",
    "https://openalex.org/W4223891676",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4286750685",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W3182414949"
  ],
  "abstract": "Prior work on language model pre-training has explored different architectures and learning objectives, but differences in data, hyperparameters and evaluation make a principled comparison difficult. In this work, we focus on bidirectionality as a key factor that differentiates existing approaches, and present a comprehensive study of its role in next token prediction, text infilling, zero-shot priming and fine-tuning. We propose a new framework that generalizes prior approaches, including fully unidirectional models like GPT, fully bidirectional models like BERT, and hybrid models like CM3 and prefix LM. Our framework distinguishes between two notions of bidirectionality (bidirectional context and bidirectional attention) and allows us to control each of them separately. We find that the optimal configuration is largely application-dependent (e.g., bidirectional attention is beneficial for fine-tuning and infilling, but harmful for next token prediction and zero-shot priming). We train models with up to 6.7B parameters, and find differences to remain consistent at scale. While prior work on scaling has focused on left-to-right autoregressive models, our results suggest that this approach comes with some trade-offs, and it might be worthwhile to develop very large bidirectional models.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3973–3985\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nOn the Role of Bidirectionality in Language Model Pre-Training\nMikel Artetxe Jingfei Du Naman Goyal Luke Zettlemoyer Ves Stoyanov\nMeta AI\n{artetxe,jingfeidu,naman,lsz,ves}@meta.com\nAbstract\nPrior work on language model pre-training has\nexplored different architectures and learning\nobjectives, but differences in data, hyperparam-\neters and evaluation make a principled compar-\nison difficult. In this work, we focus on bidirec-\ntionality as a key factor that differentiates exist-\ning approaches, and present a comprehensive\nstudy of its role in next token prediction, text\ninfilling, zero-shot priming and fine-tuning. We\npropose a new framework that generalizes prior\napproaches, including fully unidirectional mod-\nels like GPT, fully bidirectional models like\nBERT, and hybrid models like CM3 and prefix\nLM. Our framework distinguishes between two\nnotions of bidirectionality—bidirectional con-\ntext and bidirectional attention—and allows us\nto control each of them separately. We find that\nthe optimal configuration is largely application-\ndependent (e.g., bidirectional attention is bene-\nficial for fine-tuning and infilling, but harmful\nfor next token prediction and zero-shot prim-\ning). We train models with up to 6.7B parame-\nters, and find differences to remain consistent at\nscale. While prior work on scaling has focused\non left-to-right autoregressive models, our re-\nsults suggest that this approach comes with\nsome trade-offs, and it might be worthwhile\nto develop very large bidirectional models.\n1 Introduction\nNLP has undergone a paradigm shift driven by pre-\ntrained models like GPT and BERT (Bommasani\net al., 2021). These models are trained on unla-\nbeled corpora in a self-supervised fashion, and\ncan be effectively adapted to downstream tasks ei-\nther through conventional fine-tuning (Devlin et al.,\n2019) or few-shot priming (Brown et al., 2020).\nDespite their widespread use, there is not a uni-\nversal formula to pre-train language models: prior\nwork has explored different architectures and learn-\ning objectives, often focusing on different appli-\ncations. For instance, BERT (Devlin et al., 2019)\npre-trained masked language models for NLU fine-\ntuning, BART (Lewis et al., 2020) pre-trained\nseq2seq models on denoising for both NLU and\ngeneration tasks, and GPT-3 (Brown et al., 2020)\nscaled autoregressive language models focusing\non zero- and few-shot priming. However, such\nmodels differ on many factors in addition to their\narchitecture and learning objective (e.g., the pre-\ntraining data, compute and hyperparameters), mak-\ning a principled comparison difficult. Motivated by\nthat, Raffel et al. (2020) presented a comprehensive\nstudy exploring various pre-training objective and\narchitecture variants in a controlled environment.\nHowever, they conducted most of the exploration\nusing small models, while recent work has found\nthat different approaches behave differently at scale\n(Tay et al., 2022a,b), and their evaluation was lim-\nited to fine-tuning.\nIn this paper, we focus on a key factor\nthat differentiates many pre-training approaches—\nbidirectionality—and study it in different settings\nas a function of scale. We propose a new frame-\nwork that distinguishes between two notions of\nbidirectionality: bidirectional context (whether\nthe prediction of a given token is conditioned on\nboth the right and the left context, or only on ei-\nther of them), andbidirectional attention (whether\nthere are blocks of tokens that can all attend to each\nother, contrasting with triangular attention mask-\ning). Our framework offers knobs to control each\nof them separately, generalizing several previous\napproaches (e.g. BERT leverages both types of\nbidirectionality, GPT does not use any, prefix LMs\nonly leverage bidirectional attention, and CM3 only\nleverages bidirectional context).\nWe train a total of 24 models covering 6 variants\nof our framework and 5 model sizes with up to 6.7B\nparameters, and evaluate them on 4 settings: lan-\nguage modeling, text infilling, zero-shot priming,\nand fine-tuning. We find that bidirectional attention\nand context have a different impact depending on\n3973\n</s> tok 1 tok i-1 tok i+1 tok j-1 tok j+1 tok n-1 <mask> <mask>… … …\nInput:\nOutput: tok j+2 </s> tok i tok j…\npos 0 pos 1 pos i-1 pos i+1 pos j-1 pos j+1 pos n-1 pos i pos j… … …\n…\n…\n…\ntok j\nnbidir npredict\nnmask\nFigure 1: Proposed framework. Starting from the original document, we mask nmask tokens at random and\nmove them—along with their positional embeddings—to the end. We define our loss over the last npredict tokens,\npredicting the masked token for the last nmask, and the next token for the remaining npredict −nmask. We use\nbidirectional attention over the first nbidir tokens, and unidirectional attention over the rest. Refer to Appendix A\nfor a more detailed description.\nName nmask nbidir npredict Related models\nNXTUNI 0 0 n GPT (Radford et al., 2018, 2019; Brown et al., 2020)\nNXTPRE† 0 U(1 , n) n − nbidir Prefix LM (Raffel et al., 2020; Wu et al., 2021)\nMSKUNI B(n, 0.15) 0 nmask –\nMSKBI B(n, 0.15) n n mask BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019)\nHYBUNI† B(n, 0.15) 0 n CM3 (Aghajanyan et al., 2022)\nHYBPRE† B(n, 0.15) U(1 , n) max( n − nbidir, nmask) –\nTable 1: Variants of the proposed framework explored in this work. n denotes the document length; B(n, p)\ndenotes the binomial distribution; U(a, b) denotes the discrete uniform distribution. †We set nbidir = 0 and\nnmask = 0with probability p = 0.1, so that the model gets more exposure to regular language modeling.\nthe use case, and there is not a single configura-\ntion that is optimal for all scenarios. Moreover, we\nfind this behavior to remain consistent at the scale\nrange considered in this study. With recent scal-\ning work focusing on fully unidirectional models,\nthis suggests that there is potential for alternative\narchitectures and learning objectives that might be\nbetter suited for other use cases.\n2 Proposed framework\nAs illustrated in Figure 1, we propose a general-\nized framework to pre-train transformer models on\nunlabeled corpora. Our framework supports both\nunidirectional and bidirectional attention, as well\nas next token prediction and single-token infilling,\nusing the following parameters to balance them:\n• nbidir controls the length of the prefix using\nbidirectional attention, whereas the rest of the\ndocument uses unidirectional attention. More\nconcretely, we set the attention mask so that\nthe ith token can attend to the jth token if and\nonly if j ≤max(i, nbidir).\n• nmask controls how many tokens are masked.\nMasked tokens are moved to the end along\nwith their positional embeddings.\n• npredict controls the length of the suffix for\nwhich we define our supervisory signal. We\nuse the cross-entropy loss to train the model,\npredicting the masked tokens for the last\nnmask, and the next token for the remaining\nnpredict −nmask.1\nAs such, our framework allows us to vary the\ntwo notions of bidirectionality discussed above:\nnbidir controls the weight of bidirectional attention,\nwhereas nmask and npredict control the weight of\nbidirectional context. In addition, larger values of\nnpredict result in more tokens of supervision.\nTable 1 summarizes the specific variants of this\ngeneral framework that we explore in our experi-\nments, along with a descriptive name that we will\nuse to refer to each of them. Some variants are\nequivalent or closely related to existing approaches.\nIn particular, NXTUNI is equivalent to conventional\nautoregressive language models, and NXTPRE is\nequivalent to prefix language models. MSKBI is\nclosely related to the RoBERTa objective,2 except\n1We set npredict ≤ n −nbidir + nmask so we only predict\ntokens that are either masked or cannot attend to themselves.\n2Moving masked tokens to the end becomes irrelevant\nwhen nbidir = n, as their positional embeddings move with\nthem and transformers operate over sets.\n3974\nsize cost l d h bs lr\n125M 0.11 12 768 12 0.5M 6e-4\n355M 0.31 24 1024 16 0.5M 3e-4\n1.3B 1.11 24 2048 32 1M 2e-4\n2.7B 2.23 32 2560 32 1M 1.6e-4\n6.7B 5.49 32 4096 32 2M 1.2e-4\nTable 2: Model details. size: number of parameters,\ncost: training ZFLOPs, l: layers, d: hidden dimension,\nh: attention heads, bs: batch size, lr: learning rate. All\nmodels are trained for 100B tokens with a maximum\nsequence length of 1024 tokens. We estimate training\nZFLOPs analytically following Artetxe et al. (2021).\nthat we do not replace 10% of the masked tokens\nwith the original or a randomly picked one. HY-\nBUNI is similar to the CM3 objective, except that\nwe mask individual tokens instead of spans and we\ndraw the number of masks from a binomial distribu-\ntion. Finally, we introduce MSKUNI as a variant of\nMSKBI using unidirectional attention (or, from an-\nother perspective, a variant of HYBUNI predicting\nmasked tokens alone), and HYBPRE as a variant of\nHYBUNI using a bidirectional attention prefix.\n3 Experimental settings\n3.1 Models\nFor each variant in Table 1, we train models at\ndifferent scales using the same settings as Artetxe\net al. (2021), which at the same time roughly follow\nBrown et al. (2020). So as to reduce the compu-\ntational cost of our exploration, we differ from\nArtetxe et al. (2021) in two ways: (i) we use a\nmaximum sequence length of 1024 tokens instead\nof 2048, and (ii) we train for 100B tokens instead\nof 300B. At the same time, we only train 125M\nand 355M models for the NXTPRE and MSKUNI\nvariants. Table 2 summarizes the settings that we\nuse for each model.\nWe use the same training data as Artetxe et al.\n(2021), which combines BookCorpus (Zhu et al.,\n2015), CC-News (Nagel, 2016), OpenWebText\n(Gokaslan and Cohen, 2019), CC-Stories (Trinh\nand Le, 2018), and English CC100 (Wenzek et al.,\n2020), totalling 112B tokens. Following them, we\nalso use the same BPE encoding as GPT-2 (Radford\net al., 2019) with a vocabulary of 50k.\nOur implementation is based in fairseq (Ott et al.,\n2019). We apply the procedure described in §2 to\neach document separately, and combine multiple\ndocuments into a single sequence to speed up train-\ning.3 As such, we move the masked tokens to\nthe end of each document (as opposed to the end\nof the whole sequence), and apply a bidirectional\nattention prefix to each document rather than the\nsequence as a whole.4\n3.2 Evaluation\nWe evaluate our models in the following settings:\nLanguage modeling. We evaluate the ability of\nour models to predict the next token in a sequence\nas measured by perplexity.5 Different from training,\nwe do not concatenate different documents into the\nsame sequence, and instead score each document\nas a separate sequence.6 Given that NXTPRE and\nHYBPRE are primarily trained to predict the last\npart of a document conditioned on the first part,\nwe also measure the perplexity at predicting the\nlast 20% tokens in each document conditioned on\nthe first 80%. So as to understand whether using\nbidirectional attention in the prefix is useful to that\nend, we try different values of nbidir according to\na ratio rbidir, so that nbidir = rbidir ×nprefix and\nnprefix = 0.8n is the length of the prefix we are\nconditioning on.\nSingle token infilling. We mask a single word in\neach document at random, and measure the accu-\nracy at predicting it.7 To that end, we use the same\nprocedure used for training (illustrated in Figure 1),\nwhich moves the mask token to the end of the se-\nquence.8 This approach is not suitable for models\ntrained exclusively on next token prediction like\nNXTUNI and NXTPRE, as they can only be condi-\ntioned on the right context. However, one can still\nuse such models for infilling in a generative fash-\nion, replacing the masked token with each element\nin the vocabulary, scoring the resulting sequences\nautoregressively, and predicting the token yield-\n3We achieve this using—sample-break-mode complete\nin fairseq. This is different from Artetxe et al. (2021), who con-\ncatenated all documents and split the resulting sequence into\nnon-overlapping blocks without respecting document bound-\naries (—sample-break-mode none).\n4As a consequence, a given token cannot attend to tokens\nin future documents even when nbidir = n, but all tokens can\nattend to tokens in previous documents.\n5We exclude MSKBI and MSKUNI as they are not trained\non next token prediction.\n6This corresponds to the —sample-break-mode\ncomplete_doc option in fairseq.\n7Similar to language modeling evaluation, we feed each\ndocument as a separate sequence.\n8For models trained with a bidirectional attention prefix,\nwe try different values of rbidir at inference time, so that\nnbidir = rbidir × n.\n3975\n10\n12\n14\n16\n18\n20Suffix perplexity\n68\n70\n72\n74\n76\n78Infilling accuracy\n125M 355M 1.3B 2.7B 6.7B\nModel size\n52.5\n55.0\n57.5\n60.0\n62.5\n65.0\n67.5Zero-shot accuracy\n125M 355M 1.3B 2.7B 6.7B\nModel size\n84\n86\n88\n90Fine-tuning accuracy\nVariant\nNxtUni\nNxtPre\nMskUni\nMskBi\nHybUni\nHybPre\nAttn\nUnidir\nBidir\nFigure 2: Main results. Unidir and Bidir denote using nbidir = 0and nbidir = n after pre-training, respectively (or\nnbidir = nprefix for suffix perplexity).\ning the highest scoring sequence. In addition to\nour primary evaluation, we compare both of these\napproaches, which we refer to as infill (direct infill-\ning) and full (full sequence scoring). Given that full\ncan be prohibitively expensive when considering\nthe full vocabulary, we constrain the set of options\nto the top 32 candidates generated by the 125M\nMSKBI model.9\nZero-shot priming. We evaluate our models\non zero-shot priming using the exact same set-\ntings and tasks as Artetxe et al. (2021), which\ncomprises ReCoRD (Zhang et al., 2018), Hel-\nlaSwag (Zellers et al., 2019), PIQA (Bisk et al.,\n2020), WinoGrande (Sakaguchi et al., 2020), Sto-\nryCloze (Mostafazadeh et al., 2016) and Open-\nBookQA (Mihaylov et al., 2018). These are all\nmultiple choice tasks, so we score the populated\nprompt corresponding to each option in an autore-\ngressive fashion and predict the highest scoring\none.10 However, when the options differ in a sin-\ngle token—as it is common for classification tasks\nwith single-token verbalizers—one can also score\nsuch token directly in an infilling fashion. So as\nto understand how both approaches compare, we\nfurther evaluate our models on MNLI (Williams\n9The top 32 candidates contain the correct one in 95.19%\nof the cases, which is the upper bound accuracy in this setting.\n10Refer to Artetxe et al. (2021) for a description of the\nscoring function used for each task and the evaluatio protocol.\net al., 2018), using a single-token verbalizer placed\nin the middle of the prompt.11\nFine-tuning. We experiment with the following\ntasks from GLUE (Wang et al., 2019): COLA\n(Warstadt et al., 2019), MNLI-m (Williams et al.,\n2018), MRPC (Dolan and Brockett, 2005), QNLI\n(Rajpurkar et al., 2016), RTE (Dagan et al., 2006;\nHaim et al., 2006; Giampiccolo et al., 2007; Ben-\ntivogli et al., 2009) and SST-2 (Socher et al., 2013).\nOur fine-tuning approach closely follows BERT\nand similar models: we place a special </s>token\nat the end of the sequence (analogous to the special\n<CLS> token used by BERT) and learn a new clas-\nsification head on top. We ran a grid search with\nthe learning rate in {1e-0.5, 2e-05, 5e-05, 5e-06}\nand batch size in {16, 32, 64}, and report the best\ndevelopment accuracy for each model. The rest of\nhyperparameters follow RoBERTa. For all variants,\nwe tried fine-tuning both with fully unidirectional\nattention (rbidir = 0)and fully bidirectional atten-\ntion (rbidir = 1). Refer to Appendix B for more\ndetails.\n11We use <premise>, right? {Yes|No|Also},\n<hypothesis> as our template and report results on the\nmatched development set.\n3976\n125M 355M 1.3B 2.7B 6.7B\nNXTUNI 22.23 17.49 14.07 12.55 11.44\nNXTPRE 22.75 18.06 – – –\nHYBUNI 23.26 18.19 14.65 13.16 12.03\nHYBPRE 23.91 18.81 15.33 13.92 12.86\nTable 3: Full document perplexity.\nrbidir 125M 355M 1.3B 2.7B 6.7B\nNXTUNI 0.00 19.99 15.67 12.57 11.17 10.15\nNXTPRE\n0.00 20.29 16.05 – – –\n0.25 20.25 16.00 – – –\n0.50 20.21 15.96 – – –\n0.75 20.17 15.92 – – –\n1.00 20.16 15.88 – – –\nHYBUNI 0.00 20.91 16.30 13.08 11.73 10.70\nHYBPRE\n0.00 21.34 16.74 13.60 12.32 11.35\n0.25 21.30 16.69 13.56 12.29 11.33\n0.50 21.26 16.66 13.54 12.26 11.30\n0.75 21.23 16.62 13.51 12.23 11.28\n1.00 21.18 16.56 13.46 12.19 11.24\nTable 4: Suffix perplexity. We measure perplexity at\npredicting the last 20% of the tokens in each document\nconditioned on the first 80%, using nbidir = rbidir ×\nnprefix for inference, where nprefix = 0.8n is the length\nof the prefix we are conditioning on.\n4 Results\nWe visualize our main results in Figure 2, and dis-\ncuss each setting in more detail next.\n4.1 Language modeling\nWe report full document perplexities in Table 3.\nNXTUNI obtains the best results, followed by HY-\nBUNI and HYBPRE, and NXTPRE doing slightly\nbetter than HYBUNI at small scale. This is con-\nsistent with how close the pre-training objective is\nto the end task: NXTUNI is exclusively trained on\nnext token prediction, HYBUNI combines it with\nmasking (which is not used here), and HYBPRE\nfurther combines it with a bidirectional attention\nprefix (which is not used here either). However, it\nis interesting that scaling up does not reduce the\ngap between them. This suggests that there is some\nfundamental interference between these different\ncapabilities,12 and increasing capacity does not mit-\n12There are various factors that could explain this. Both\nmasking and the bidirectional attention prefix reduce the su-\npervision on next token prediction, and masking further intro-\nduces some noise in the original sequence. Moreover, training\nto use both unidirectional and bidirectional attention and/or\ncontext might provide a conflicting signal, although our results\nlater in §4.2 suggest that this does not have a major impact at\nrbidir 125M 355M 1.3B 2.7B 6.7B\nMSKUNI 0.00 69.61 73.43 – – –\nMSKBI 1.00 71.00 75.06 77.43 78.46 79.16\nHYBUNI 0.00 66.86 71.88 75.56 77.19 78.29\nHYBPRE\n0.00 67.70 72.25 75.49 76.95 78.02\n0.25 68.02 72.57 75.77 77.25 78.22\n0.50 68.23 72.85 76.05 77.48 78.52\n0.75 68.47 73.13 76.32 77.74 78.70\n1.00 68.71 73.38 76.59 78.00 78.91\nTable 5: Single token infilling accuracy. We mask a\nrandom token in each validation document and measure\nthe accuracy at predicting it, using nbidir = rbidir ×n\nfor inference.\nigate it.\nTable 4 reports suffix perplexity results, where\nwe predict the last 20% of the tokens in each doc-\nument conditioned on the rest. Compared to the\nprevious results, NXTPRE and HYBPRE reduce\nthe gap with NXTUNI and HYBUNI, but they still\nlag behind them. In both cases, we find that the\nmodels benefit from using bidirectional attention\nin the prefix at inference time (i.e., higher values of\nrbidir yield lower perplexity), but the improvement\nis relatively small. It is intriguing that NXTUNI\noutperforms NXTPRE, when the latter was trained\non suffix prediction and can leverage bidirectional\nattention. We attribute this to the bidirectional pre-\nfix reducing the number of tokens of supervision\nduring training.\n4.2 Single token infilling\nWe report infilling results in Table 5. MSKBI ob-\ntains the best results, which can be explained by\nits use of bidirectional attention and the fact that\nit is exclusively trained on masking. Our results\nsuggest that both of these factors play a role, but\ntheir impact varies at scale. As for the first factor,\nwe find that bidirectional attention has a larger im-\npact on infilling compared to next token prediction\n(§4.1), as reflected by MSKBI doing substantially\nbetter than MSKUNI. Moreover, we find that this\nalso holds at scale, as reflected by HYBPRE doing\nbetter with larger values ofrbidir, while outperform-\ning HYBUNI. Regarding the second factor, we find\nthat combining masking with next token predic-\ntion significantly hurts infilling performance for\nsmall models, as reflected by the large gap between\nMSKUNI and HYBUNI. However, we also find\nscale.\n3977\n125M 355M 1.3B 2.7B 6.7B\nNXTUNI full 69.83 73.13 75.90 77.26 77.98\nNXTPRE full 69.40 72.75 – – –\nMSKUNI infill 69.65 73.39 – – –\nMSKBI infill† 71.00 74.98 77.17 78.07 78.70\nHYBUNI full 68.94 72.77 75.43 76.61 77.76\ninfill 67.02 71.90 75.38 76.90 77.88\nHYBPRE\nfull 68.53 72.05 74.75 76.03 76.87\ninfill 67.82 72.24 75.35 76.66 77.63\ninfill† 68.78 73.35 76.36 77.63 78.47\nTable 6: Single token infilling accuracy, re-ranking\nthe top 32 candidates from 125M MSKBI. † denotes\nnbidir = n, the rest use nbidir = 0. Refer to §3.2 for\nmore details.\nthe impact of this to vanish at scale, as reflected\nby the gap between MSKBI and HYBPRE with\nrbidir = 1.0 becoming smaller for larger models.\nThis also explains why HYBPRE with rbidir = 0.0\noutperforms HYBUNI for small models, but the\ntrend is reversed as we scale up: the bidirectional\nprefix in HYBPRE reduces the relative weight of\nnext token prediction during training, which out-\nweighs the discrepancy with not using bidirectional\nattention at inference time for small models, but\nnot for larger ones. Interestingly, this is different\nfrom the behavior observed for language modeling\nin §4.1, where scale did not significantly mitigate\nthe negative impact of combining masking and next\ntoken prediction during training. We attribute this\nto masking introducing noise in the original docu-\nment, as well as reducing the amount of tokens that\nwe train on next token prediction.13\nTable 6 reports infilling results re-ranking the top\n32 candidates from the 125M MSKBI model. The\nbest results are still obtained by MSKBI, but we\nfind the generative approach described in §3.2 to\nbe competitive, with NXTUNI obtaining the second\nbest results at 125M and the third best results for\nlarger models. This suggests that models trained\nexclusively on next token prediction can also be\nused for infilling as long as the set of candidates\nis small, even outperforming hybrid models like\nHYBUNI that are trained both on next token pre-\ndiction and infilling itself. In fact, it is remarkable\nthat NXTUNI is only outperformed by models us-\n13Note that the reverse is not true: the addition of next\ntoken prediction in HYBUNI does not reduce the amount of\nsupervision on infilling with respect to MSKUNI, as we use\nthe same value of nmask in both cases.\nRE HS PI WG SC OB avg\n125M\nNXTUNI 66.7 32.2 65.3 51.9 64.3 33.0 52.3\nNXTPRE 65.8 31.2 64.1 54.1 63.5 35.0 52.3\nHYBUNI 65.4 30.8 63.1 50.9 63.6 34.4 51.4\nHYBPRE 64.9 30.5 64.2 51.9 63.0 35.2 51.6\n355M\nNXTUNI 74.8 41.0 69.5 52.2 70.0 38.6 57.7\nNXTPRE 74.3 40.0 68.9 52.6 69.2 37.8 57.1\nHYBUNI 73.9 39.3 68.1 52.3 69.3 37.2 56.7\nHYBPRE 72.9 37.8 67.6 50.4 68.4 37.4 55.8\n1.3B\nNXTUNI 81.0 52.6 73.8 55.6 74.1 43.6 63.5\nHYBUNI 80.0 50.3 72.1 53.7 74.1 43.0 62.2\nHYBPRE 79.4 48.5 71.4 52.9 73.9 38.2 60.7\n2.7B\nNXTUNI 83.8 58.8 75.0 60.1 76.6 50.8 67.5\nHYBUNI 83.1 57.5 73.9 58.0 76.9 45.8 65.9\nHYBPRE 81.7 54.7 72.4 56.7 75.3 46.6 64.6\n6.7B\nNXTUNI 85.2 63.6 76.2 60.0 77.6 51.6 69.0\nHYBUNI 84.2 61.7 75.5 59.7 76.8 49.0 67.8\nHYBPRE 83.9 58.9 73.9 58.7 76.9 50.8 67.2\nTable 7: Zero-shot priming accuracy. We use nbidir =\n0 for inference. RE: ReCoRD, HS: HellaSwag, PI: PIQA,\nWG: WinoGrande, SC: StoryCloze, OB: OpenBookQA.\n125M 355M 1.3B 2.7B 6.7B\nNXTUNI full 44.79 50.12 53.63 55.09 55.27\nNXTPRE full 45.41 49.15 – – –\nMSKUNI infill 41.69 44.15 – – –\nMSKBI infill† 41.56 48.34 52.24 55.59 53.97\nHYBUNI full 45.12 47.92 52.59 53.40 54.47\ninfill 43.03 44.54 48.13 49.94 51.26\nHYBPRE\nfull 43.37 47.54 51.53 52.36 54.01\ninfill 42.16 44.47 47.36 49.98 50.24\ninfill† 42.95 46.57 49.13 51.85 52.41\nTable 8: Zero-shot MNLI accuracy. † denotes nbidir =\nn, the rest use nbidir = 0.\ning bidirectional attention which, consistent with\nour previous results, seems strongly beneficial for\ninfilling. Nevertheless, we also find direct infilling\n(infill) to scale better than generative full sequence\nscoring (full) for both HYBUNI and HYBPRE, al-\nthough this could (partly) be explained by the inter-\nference between next token prediction and masking\ndiminishing at scale as discussed previously.\n4.3 Zero-shot priming\nWe report zero-shot priming results in Table 7. We\nobserve the same general trends as in language\nmodeling (§4.1), with NXTUNI performing best,\nfollowed by HYBUNI and HYBPRE. The results\nare generally consistent across tasks.\nTable 8 reports MNLI results, comparing full\nsequence scoring and direct infilling. Consistent\n3978\nrbidir 125M 355M 1.3B 2.7B 6.7B\nNXTUNI 0.0 83.6 85.8 87.2 88.7 88.6\n1.0 75.9 77.1 79.0 79.2 80.3\nNXTPRE 0.0 84.2 85.8 – – –\n1.0 83.5 86.2 – – –\nMSKUNI 0.0 82.7 85.2 – – –\n1.0 83.2 85.1 – – –\nMSKBI 0.0 79.6 81.0 81.9 81.6 82.6\n1.0 84.4 88.0 89.6 90.8 91.0\nHYBUNI 0.0 83.5 85.9 87.6 88.6 88.8\n1.0 80.8 82.5 84.0 85.0 84.7\nHYBPRE 0.0 83.6 86.1 87.1 88.2 88.2\n1.0 84.8 86.7 88.8 89.8 90.3\nTable 9: Average fine-tuning accuracy.\nwith the intrinsic evaluation in §4.2, we find full\nsequence scoring with NXTUNI to be competitive\nwith direct infilling with MSKBI. In fact, full se-\nquence scoring does even better comparatively, ob-\ntaining the best results in all but one of the model\nsizes. Moreover, it is remarkable that both HY-\nBUNI and HYBPRE obtain better results with full\nsequence scoring compared to direct infilling in all\ncases. Consistent with our previous results, this\nsuggests that left-to-right language models can be\na valid or even superior alternative to masked lan-\nguage models for single-token infilling tasks, as\nlong as one can afford scoring each candidate sepa-\nrately.\n4.4 Fine-tuning\nWe report average fine-tuning results comparing\nunidirectional and bidirectional attention in Table\n9, and full results for the optimal setting for each\nvariant in Table 10.\nOur results show that bidirectional attention is\nhelpful for fine-tuning regardless of scale, with\nfully bidirectional models ( MSKBI) performing\nthe best, followed by models pre-trained with a\nbidirectional attention prefix (HYBPRE, NXTPRE),\nand fully unidirectional models performing the\nworst (HYBUNI, NXTUNI, MSKUNI). Interest-\ningly, changing the attention type at fine-tuning\ntime (using unidirectional attention for pre-training\nand bidirectional attention for fine-tuning, or the\nother way around) works poorly.\nAt the same time, we find that the role of bidirec-\ntional context is dependant on the type of attention\nused. When using fully unidirectional attention,\nbidirectional context has no clear impact, with NX-\nTUNI and HYBUNI performing similarly. In con-\ntrast, when using bidirectional attention, bidirec-\ntional context seems beneficial, with HYBPRE per-\nforming better than NXTPRE at small scale. This\nsuggests that pre-training with bidirectional con-\ntext is important for the model to learn to make\neffective use of bidirectional attention.\n5 Related work\nWhile it was once common to use random ini-\ntialization for supervised learning, a series of\nworks showed substantial improvements from pre-\ntraining autoregressive models on next token pre-\ndiction (Dai and Le, 2015; Peters et al., 2018;\nHoward and Ruder, 2018; Radford et al., 2018).\nThe pre-train/fine-tune paradigm was further popu-\nlarized by BERT (Devlin et al., 2019) and its deriva-\ntives like RoBERTa (Liu et al., 2019), which ob-\ntained further gains from pre-training bidirectional\nencoders on masked language modeling. Subse-\nquent work explored masking spans instead of in-\ndividual tokens, using either bidirectional encoder-\nonly models (Joshi et al., 2020) or encoder-decoder\nmodels (Lewis et al., 2020; Raffel et al., 2020).\nMore recently, there has been a reborn interest on\nscaling left-to-right autoregressive language mod-\nels with a focus on few-shot priming (Radford et al.,\n2019; Brown et al., 2020; Rae et al., 2021; Hoff-\nmann et al., 2022; Smith et al., 2022; Chowdhery\net al., 2022; Zhang et al., 2022).\nWhile unidirectional and bidirectional models\nhave largely been developed as separate strains of\nwork serving a different purpose, there have also\nbeen some attempts to combine the best of both\nworlds. XLNet (Yang et al., 2019) pre-trained au-\ntoregressive models over all permutations of the\nfactorization order, enabling the model to use bidi-\nrectional context with strong results on fine-tuning.\nSimilarly, CM3 (Aghajanyan et al., 2022) trained\nleft-to-right autoregressive models, masking some\nspans that are predicted at the end of the sequence.\nERNIE 3.0 (Sun et al., 2021) proposed a modu-\nlar architecture, combining a shared unidirectional\nmodule with either another unidirectional module\nfor NLG or a bidirectional module for NLU. Fi-\nnally, Raffel et al. (2020) and Wu et al. (2021)\nexplored splitting documents in two halves and pre-\ndicting the second one conditioned on the first one,\nusing unidirectional attention for the former and\nbidirectional attention for the latter.\nDespite the large body of work on language\n3979\nCOLA MNLI MRPC QNLI RTE SST2 avg\n125M\nNXTUNI 82.4 83.1 82.8 88.8 70.4 93.9 83.6\nNXTPRE 81.3 83.3 83.1 90.1 69.3 93.7 83.5\nMSKUNI 82.6 82.2 81.4 88.4 68.6 93.1 82.7\nMSKBI 83.2 84.8 85.5 91.0 68.6 93.5 84.4\nHYBUNI 82.7 83.1 83.6 89.3 69.3 93.0 83.5\nHYBPRE 82.5 84.2 85.5 90.9 72.6 93.2 84.8\n355M\nNXTUNI 84.2 85.8 84.1 91.2 74.7 94.8 85.8\nNXTPRE 83.8 86.3 86.5 92.0 73.3 95.4 86.2\nMSKUNI 84.0 84.4 84.6 90.5 73.6 94.2 85.2\nMSKBI 85.2 87.7 89.7 92.9 76.2 96.2 88.0\nHYBUNI 85.4 85.3 85.3 91.0 73.3 94.8 85.9\nHYBPRE 84.5 86.5 87.3 92.5 74.4 95.2 86.7\n1.3B\nNXTUNI 87.0 87.3 85.3 92.4 75.1 95.9 87.2\nMSKBI 85.7 89.1 89.7 93.9 82.3 96.8 89.6\nHYBUNI 86.3 87.0 86.0 92.3 78.0 96.3 87.6\nHYBPRE 85.1 88.4 90.0 93.6 79.4 96.2 88.8\n2.7B\nNXTUNI 86.0 88.5 85.5 93.0 83.0 96.2 88.7\nMSKBI 87.2 89.8 91.7 94.0 85.2 96.8 90.8\nHYBUNI 86.2 88.1 86.8 93.0 80.9 96.7 88.6\nHYBPRE 86.2 89.4 89.5 94.1 82.7 96.7 89.8\n6.7B\nNXTUNI 86.3 88.5 85.8 93.4 81.2 96.7 88.6\nMSKBI 86.7 89.6 90.9 94.5 87.7 96.8 91.0\nHYBUNI 86.7 88.4 87.7 93.4 80.5 96.1 88.8\nHYBPRE 86.0 89.5 89.5 94.3 85.6 96.7 90.3\nTable 10: Fine-tuning accuracy. We use nbidir = 0for NXTUNI, MSKUNI and HYBUNI, and nbidir = n for the\nrest.\nmodel pre-training, there is little work comparing\ndifferent approaches in a systematic manner. As a\nnotable exception, Raffel et al. (2020) compared\nvarious architectures and learning objectives with\na focus on fine-tuning. Concurrent to our work,\nWang et al. (2022) conduct a comprehensive study\nwith a focus on zero-shot learning and multi-task\nfine-tuning. In contrast, we focus on the specific\nrole of bidirectionality, and compare models of dif-\nferent sizes.\n6 Conclusions\nIn this work, we study the role of bidirectionality in\nlanguage model pre-training through a new frame-\nwork that generalizes previous approaches. Our\nmain findings are as follows:\n• Bidirectional attention is strongly beneficial\nfor infilling and fine-tuning. In contrast, prefix\nlanguage models lag behind regular language\nmodels on next token prediction, even if they\nget a small benefit from leveraging bidirec-\ntional attention in the prefix. This behavior is\nconsistent at scale.\n• Models trained jointly to use unidirectional\nand bidirectional context, like HYBUNI, lag\nbehind regular language models on next token\nprediction, and scale does not mitigate this.\nSuch models also lag behind pure masked lan-\nguage models on infilling, but scale does help\nclose this gap as long as they are trained with a\nbidirectional attention prefix. For fine-tuning,\nbidirectional context is beneficial when used\nin conjunction with bidirectional attention, but\nnot when used with unidirectional attention.\n• While direct infilling requires bidirectional\ncontext and benefits from bidirectional atten-\ntion as discussed above, models using unidi-\nrectional context and attention are also com-\npetitive in infilling when one can separately\nscore each candidate. For settings where the\nset of candidates is small (e.g., zero-shot prim-\ning for classification), regular language mod-\nels obtain comparable or even superior results\nto models pre-trained on infilling.\nAll in all, our results show that there is not a sin-\ngle configuration that is optimal for all use cases,\nand this remains generally consistent within the\nscale range explored in this work. While prior work\non scaling has focused on left-to-right autoregres-\nsive models, this suggests that there might be other\n3980\nobjectives and architectures that are better suited\nfor other applications like fine-tuning. Given the\ncost of pre-training several models, we would like\nto explore modular (Sun et al., 2021) or adaptation\n(Wang et al., 2022) approaches in the future, where\none would either have a single model with modular\ncomponents specialized for different use cases, or\nefficiently adapt an existing model by changing the\nparameters in our framework instead of training\nseveral models from scratch.\nLimitations\nOur study focuses on the role of bidirectionality on\nlanguage model pre-training, and does not explore\nother factors that might affect model performance.\nIn particular, we mask individual tokens without\nconsidering longer spans, and do not explore the\nimpact of the masking rate. In addition, we do\nnot consider sequence-to-sequence models in our\nstudy, which combine bidirectional attention in the\nencoder and unidirectional attention in the decoder.\nFinally, we train all variants for the same number\nof tokens, making them comparable in terms of\ntraining cost, but resulting in models using a bidi-\nrectional attention prefix or a masking objective\nseeing less tokens of supervision.\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\nand Luke Zettlemoyer. 2022. Cm3: A causal masked\nmultimodal model of the internet.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor\nMihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin,\nJingfei Du, Srinivasan Iyer, Ramakanth Pasunuru,\nGiri Anantharaman, Xian Li, Shuohui Chen, Halil\nAkin, Mandeep Baines, Louis Martin, Xing Zhou,\nPunit Singh Koura, Brian O’Horo, Jeff Wang, Luke\nZettlemoyer, Mona Diab, Zornitsa Kozareva, and\nVes Stoyanov. 2021. Efficient large scale language\nmodeling with mixtures of experts.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(05):7432–7439.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri Chatterji, Annie\nChen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Goodman,\nShelby Grossman, Neel Guha, Tatsunori Hashimoto,\nPeter Henderson, John Hewitt, Daniel E. Ho, Jenny\nHong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth\nKaramcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Kr-\nishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-\nhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle\nLevent, Xiang Lisa Li, Xuechen Li, Tengyu Ma,\nAli Malik, Christopher D. Manning, Suvir Mirchan-\ndani, Eric Mitchell, Zanele Munyikwa, Suraj Nair,\nAvanika Narayan, Deepak Narayanan, Ben Newman,\nAllen Nie, Juan Carlos Niebles, Hamed Nilforoshan,\nJulian Nyarko, Giray Ogut, Laurel Orr, Isabel Pa-\npadimitriou, Joon Sung Park, Chris Piech, Eva Porte-\nlance, Christopher Potts, Aditi Raghunathan, Rob\nReich, Hongyu Ren, Frieda Rong, Yusuf Roohani,\nCamilo Ruiz, Jack Ryan, Christopher Ré, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy\nShih, Krishnan Srinivasan, Alex Tamkin, Rohan\nTaori, Armin W. Thomas, Florian Tramèr, Rose E.\nWang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai\nWu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan\nYou, Matei Zaharia, Michael Zhang, Tianyi Zhang,\nXikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn\nZhou, and Percy Liang. 2021. On the opportunities\nand risks of foundation models.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\n3981\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges. Evaluating\nPredictive Uncertainty, Visual Object Classification,\nand Recognising Tectual Entailment, pages 177–190,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in Neural Informa-\ntion Processing Systems, volume 28. Curran Asso-\nciates, Inc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nBill Dolan. 2007. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL Workshop on Textual Entailment and\nParaphrasing, pages 1–9, Prague. Association for\nComputational Linguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus. http://web.archive.org/save/http://\nSkylion007.github.io/OpenWebTextCorpus.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proceedings of the Second PAS-\nCAL Challenges Workshop on Recognising Textual\nEntailment, volume 7.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Span-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381–2391, Brussels, Belgium. Association\nfor Computational Linguistics.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nSebastian Nagel. 2016. Cc-news. http:\n//web.archive.org/save/http://commoncrawl.\norg/2016/10/news-dataset-available.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\n3982\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Karthik Narasimhan, Time Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report, OpenAI.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2020. Winogrande: An adversar-\nial winograd schema challenge at scale. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n34(05):8732–8740.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia\nSong, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro.\n2022. Using deepspeed and megatron to train\nmegatron-turing nlg 530b, a large-scale generative\nlanguage model.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,\nYanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu,\nWeibao Gong, Jianzhong Liang, Zhizhou Shang,\nPeng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao\nTian, Hua Wu, and Haifeng Wang. 2021. Ernie 3.0:\nLarge-scale knowledge enhanced pre-training for lan-\nguage understanding and generation.\nYi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won\nChung, William Fedus, Jinfeng Rao, Sharan Narang,\nVinh Q. Tran, Dani Yogatama, and Donald Metzler.\n2022a. Scaling laws vs model architectures: How\ndoes inductive bias influence scaling?\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus,\nSamira Abnar, Hyung Won Chung, Sharan Narang,\nDani Yogatama, Ashish Vaswani, and Donald Met-\nzler. 2022b. Scale efficiently: Insights from pretrain-\ning and finetuning transformers. In International\nConference on Learning Representations.\nTrieu H. Trinh and Quoc V . Le. 2018. A simple method\nfor commonsense reasoning.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Interna-\ntional Conference on Learning Representations.\nThomas Wang, Adam Roberts, Daniel Hesslow,\nTeven Le Scao, Hyung Won Chung, Iz Beltagy, Julien\nLaunay, and Colin Raffel. 2022. What language\nmodel architecture and pretraining objective work\nbest for zero-shot generalization?\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\n3983\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nShaohua Wu, Xudong Zhao, Tong Yu, Rongguo Zhang,\nChong Shen, Hongli Liu, Feng Li, Hong Zhu, Jian-\ngang Luo, Liang Xu, and Xuanwei Zhang. 2021.\nYuan 1.0: Large-scale pre-trained language model\nin zero-shot and few-shot learning.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open pre-\ntrained transformer language models.\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books.\nA Proposed framework\nFigure 3 provides a step-by-step description of how\nwe define our objective starting from the original\nsequence.\ntask # of updates\nCoLA 5336\nSST-2 20935\nMNLI 123873\nQNLI 33112\nMRPC 2296\nRTE 2036\nTable 11: Number of fine-tuning updates for each task.\nB Fine-tuning settings\nFor fine-tuning, we did grid search on learning rate\n∈{5e −06, 5e −05, 1e −05, 2e −05}and batch\nsize ∈{16, 32, 64}. For each task, we trained the\nsame numbers of updates for different setups and\nreported the best numbers across the grid. The\ndetails of fine-tuning tasks and numbers of updates\ncan be found in Table 11, which were chosen to\nfollow the original settings from RoBERTa. We\nused Adam and polynomial decay scheduler for\noptimization.\n3984\n</s> tok 1 tok i-1 tok i tok i+1 tok j-1 tok j tok j+1 tok n-1… … …\nInput:\nOutput:\n❶\ntok 1 tok 2 tok i tok i+1 tok i+2 tok j tok j+1 tok j+2 </s>… … …\npos 0 pos 1 pos i-1 pos i pos i+1 pos j-1 pos j pos j+1 pos n-1… … …\n</s> tok 1 tok i-1 <mask> tok i+1 tok j-1 <mask> tok j+1 tok n-1… … …\nInput:\nOutput:\n❷\ntok 1 tok 2 tok i tok i tok i+2 tok j tok j tok j+2 </s>… … …\npos 0 pos 1 pos i-1 pos i pos i+1 pos j-1 pos j pos j+1 pos n-1… … …\n</s> tok 1 tok i-1 tok i+1 tok j-1 tok j+1 tok n-1 <mask> <mask>… … …\nInput:\nOutput:\n❸\ntok 1 tok 2 tok i tok i+2 tok j tok j+2 </s> tok i tok j… … …\npos 0 pos 1 pos i-1 pos i+1 pos j-1 pos j+1 pos n-1 pos i pos j… … …\n…\n…\n…\n</s> tok 1 tok i-1 tok i+1 tok j-1 tok j+1 tok n-1 <mask> <mask>… … …\nInput:\nOutput:\n❹\ntok j+2 </s> tok i tok j…\npos 0 pos 1 pos i-1 pos i+1 pos j-1 pos j+1 pos n-1 pos i pos j… … …\n…\n…\n…\ntok j\nnbidir npredict\nnmask\nFigure 3: Proposed framework. 1) We start with the original sequence in the input, and predict the next token\nin the output; 2) We choose nmask tokens at random, replace them with the special <mask> token in the input,\nand predict the masked token (rather than the next token) in the output; 3) We move the masked tokens and their\ncorresponding positional embeddings to the end; 4) We only predict the last npredict tokens, using bidirectional\nattention for the first nbidir tokens and unidirectional attention for the rest (final objective).\n3985",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8068839907646179
    },
    {
      "name": "Security token",
      "score": 0.6160415410995483
    },
    {
      "name": "Language model",
      "score": 0.5831398367881775
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5628941655158997
    },
    {
      "name": "Hyperparameter",
      "score": 0.5184614062309265
    },
    {
      "name": "Spurious relationship",
      "score": 0.49362123012542725
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48152121901512146
    },
    {
      "name": "Machine learning",
      "score": 0.4528163969516754
    },
    {
      "name": "Focus (optics)",
      "score": 0.4472436308860779
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}