{
    "title": "Transformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph",
    "url": "https://openalex.org/W4396802144",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2154273582",
            "name": "Zhiyu Fang",
            "affiliations": [
                "University of Science and Technology Beijing"
            ]
        },
        {
            "id": "https://openalex.org/A5039681197",
            "name": "Shuai-Long Lei",
            "affiliations": [
                "University of Science and Technology Beijing"
            ]
        },
        {
            "id": "https://openalex.org/A2096161646",
            "name": "Xiaobin Zhu",
            "affiliations": [
                "University of Science and Technology Beijing"
            ]
        },
        {
            "id": "https://openalex.org/A2097047805",
            "name": "Chun Yang",
            "affiliations": [
                "University of Science and Technology Beijing"
            ]
        },
        {
            "id": "https://openalex.org/A4222795102",
            "name": "Shi-Xue Zhang",
            "affiliations": [
                "University of Science and Technology Beijing"
            ]
        },
        {
            "id": "https://openalex.org/A4201977627",
            "name": "Xu-cheng Yin",
            "affiliations": [
                "University of Science and Technology Beijing"
            ]
        },
        {
            "id": "https://openalex.org/A2614403418",
            "name": "Jingyan Qin",
            "affiliations": [
                "University of Science and Technology Beijing"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3086232317",
        "https://openalex.org/W4384656794",
        "https://openalex.org/W4285224608",
        "https://openalex.org/W3082429057",
        "https://openalex.org/W4229024390",
        "https://openalex.org/W3080649566",
        "https://openalex.org/W4385571206",
        "https://openalex.org/W2998528434",
        "https://openalex.org/W3099150947",
        "https://openalex.org/W4385573235",
        "https://openalex.org/W4387854270",
        "https://openalex.org/W3101611558",
        "https://openalex.org/W3097986917",
        "https://openalex.org/W3169228325",
        "https://openalex.org/W4285600519",
        "https://openalex.org/W4312702703",
        "https://openalex.org/W3187578449",
        "https://openalex.org/W3182741322",
        "https://openalex.org/W4384659456",
        "https://openalex.org/W4226350104",
        "https://openalex.org/W3199561489",
        "https://openalex.org/W4382239641",
        "https://openalex.org/W3175405178",
        "https://openalex.org/W4389518808",
        "https://openalex.org/W4385574100",
        "https://openalex.org/W3196669501",
        "https://openalex.org/W6600137863",
        "https://openalex.org/W4306801146",
        "https://openalex.org/W3167316272",
        "https://openalex.org/W3106844781",
        "https://openalex.org/W3115318530",
        "https://openalex.org/W4213337779",
        "https://openalex.org/W4385571579",
        "https://openalex.org/W4367047514",
        "https://openalex.org/W4388192595",
        "https://openalex.org/W4367046683",
        "https://openalex.org/W4385430245",
        "https://openalex.org/W4377971427",
        "https://openalex.org/W2607303097"
    ],
    "abstract": "Temporal Knowledge Graph (TKG) reasoning often involves completing missing\\nfactual elements along the timeline. Although existing methods can learn good\\nembeddings for each factual element in quadruples by integrating temporal\\ninformation, they often fail to infer the evolution of temporal facts. This is\\nmainly because of (1) insufficiently exploring the internal structure and\\nsemantic relationships within individual quadruples and (2) inadequately\\nlearning a unified representation of the contextual and temporal correlations\\namong different quadruples. To overcome these limitations, we propose a novel\\nTransformer-based reasoning model (dubbed ECEformer) for TKG to learn the\\nEvolutionary Chain of Events (ECE). Specifically, we unfold the neighborhood\\nsubgraph of an entity node in chronological order, forming an evolutionary\\nchain of events as the input for our model. Subsequently, we utilize a\\nTransformer encoder to learn the embeddings of intra-quadruples for ECE. We\\nthen craft a mixed-context reasoning module based on the multi-layer perceptron\\n(MLP) to learn the unified representations of inter-quadruples for ECE while\\naccomplishing temporal knowledge reasoning. In addition, to enhance the\\ntimeliness of the events, we devise an additional time prediction task to\\ncomplete effective temporal information within the learned unified\\nrepresentation. Extensive experiments on six benchmark datasets verify the\\nstate-of-the-art performance and the effectiveness of our method.\\n",
    "full_text": "Transformer-based Reasoning for Learning Evolutionary Chain\nof Events on Temporal Knowledge Graph\nZhiyu Fangâˆ—\nShuai-Long Leiâˆ—\nmr.fangzy@foxmail.com\nshuailong0lei@gmail.com\nUniversity of Science and Technology\nBeijing\nHaidian, Beijing, China\nXiaobin Zhuâ€ \nUniversity of Science and Technology\nBeijing\nHaidian, Beijing, China\nzhuxiaobin@ustb.edu.cn\nChun Yang\nUniversity of Science and Technology\nBeijing\nHaidian, Beijing, China\nchunyang@ustb.edu.cn\nShi-Xue Zhang\nUniversity of Science and Technology\nBeijing\nHaidian, Beijing, China\nzhangshixue111@163.com\nXu-Cheng Yin\nUniversity of Science and Technology\nBeijing\nHaidian, Beijing, China\nxuchengyin@ustb.edu.cn\nJingyan Qin\nUniversity of Science and Technology\nBeijing\nHaidian, Beijing, China\nqinjingyanking@foxmail.com\nABSTRACT\nTemporal Knowledge Graph (TKG) reasoning often involves com-\npleting missing factual elements along the timeline. Although exist-\ning methods can learn good embeddings for each factual element\nin quadruples by integrating temporal information, they often fail\nto infer the evolution of temporal facts. This is mainly because\nof (1) insufficiently exploring the internal structure and semantic\nrelationships within individual quadruples and (2) inadequately\nlearning a unified representation of the contextual and temporal\ncorrelations among different quadruples. To overcome these limi-\ntations, we propose a novel Transformer-based reasoning model\n(dubbed ECEformer) for TKG to learn the Evolutionary Chain of\nEvents (ECE). Specifically, we unfold the neighborhood subgraph\nof an entity node in chronological order, forming an evolutionary\nchain of events as the input for our model. Subsequently, we utilize\na Transformer encoder to learn the embeddings of intra-quadruples\nfor ECE. We then craft a mixed-context reasoning module based\non the multi-layer perceptron (MLP) to learn the unified represen-\ntations of inter-quadruples for ECE while accomplishing temporal\nknowledge reasoning. In addition, to enhance the timeliness of the\nevents, we devise an additional time prediction task to complete\neffective temporal information within the learned unified represen-\ntation. Extensive experiments on six benchmark datasets verify the\nstate-of-the-art performance and the effectiveness of our method.\nâˆ—Both authors contributed equally to this research.\nâ€ Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0431-4/24/07\nhttps://doi.org/10.1145/3626772.3657706\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Temporal reasoning.\nKEYWORDS\nTemporal Knowledge Graph Completion, Context Information Min-\ning, Link Prediction, Evolutionary Chain of Event\nACM Reference Format:\nZhiyu Fang, Shuai-Long Lei, Xiaobin Zhu, Chun Yang, Shi-Xue Zhang,\nXu-Cheng Yin, and Jingyan Qin. 2024. Transformer-based Reasoning for\nLearning Evolutionary Chain of Events on Temporal Knowledge Graph. In\nProceedings of the 47th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (SIGIR â€™24), July 14â€“18, 2024, Washing-\nton, DC, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/\n3626772.3657706\n1 INTRODUCTION\nKnowledge graphs (KGs) store human-summarized prior knowl-\nedge in structured data formats and are widely applied in many\ndownstream tasks, such as information retrieval [ 2], intelligent\nquestion answering [3], and recommendation systems [ 44]. Tra-\nditional knowledge graphs employ triples ( ğ‘ ,ğ‘,ğ‘œ ) to record each\nfact or event in the knowledge base, where ğ‘  and ğ‘œ respectively\nrepresent the subject and object entities, and ğ‘ denotes the logical\npredicate (or relation) connecting these two entities. e.g., (Barack\nObama, presidentOf, USA) . However, events in the real world often\ndynamically evolve over time. To extend the temporal nature of\nevents, Temporal Knowledge Graphs (TKGs) incorporate tempo-\nral information into the traditional triples, resulting in quadruples\n(ğ‘ ,ğ‘,ğ‘œ,ğœ ). e.g., (Barack Obama, presidentOf, USA, 2009â€“2017) and\n(Donald Trump, presidentOf, USA, 2017â€“2021) .\nCompleting missing events for specific timestamps in TKGs via\nTemporal Knowledge Graph Reasoning (TKGR) has recently at-\ntracted growing attention from both academic and industrial com-\nmunities. For example, the missing element Barack Obama in the\nquadruple query (?, president of, USA, 2009â€“2017) can be predicted\nvia TKGR. Existing TKGR methods can be broadly categorized\ninto two types according to the temporal scope of link prediction,\narXiv:2405.00352v1  [cs.AI]  1 May 2024\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Fang et al.\nAmerica\nBarack\nObama\nGeorge\nBush France\nDonald\nTrump\npresident of,\n[2017, 2021]\npresident of, \n[2009, 2017]\nvisit to, 2009\nsucceded, 2017\nvisit to, 2017\nvisit to, 2002\nsucceded, 2009\npresident of, \n[2001, 2009]\nï¼Ÿ\nAmerica\nBarack\nObama\nGeorge\nBush France\nDonald\nTrump\npresident of,\n[2017, 2021]\npresident of, \n[2009, 2017]\nvisit to, 2009\nsucceded, 2017\nvisit to, 2017\nvisit to, 2002\nsucceded, 2009\npresident of, \n[2001, 2009]\nï¼Ÿ\n(a)\n2001\n â€¦ \n2009\nGeorge Bush\nvisit to\nUSA\npresident of\nBarack Obama\nsucceded\n â€¦ \n2001\n â€¦ \n2009\nGeorge Bush\nvisit to\nUSA\npresident of\nBarack Obama\nsucceded\n â€¦ \nECEformer\nUnified \nRepresentation\nEntity Set\nTimestamp Set\nLink Prediction\nTime Prediction\nLink Prediction\nTime Prediction\nECEformer\nUnified \nRepresentation\nEntity Set\nTimestamp Set\nLink Prediction\nTime Prediction\n(b)\nFigure 1: Illustration of the overall concept of our method.\nPart (a) displays a subgraph of TKG, which includes facts\nrelated to three U.S. Presidents. Part (b) illustrates our moti-\nvation for the ECEformer to learn a unified representation\nfrom an evolutionary chain of events, thereby simultane-\nously addressing link and time prediction tasks.\ni.e., interpolation and extrapolation [ 16]. The former focuses on\ncompleting unknown knowledge in the past, whereas the latter\nconcentrates on estimating unknown knowledge in the future. Con-\ncretely, interpolation reasoning methods aim to effectively integrate\ntemporal information into the evolutionary trajectory of events.\nThey typically utilize geographic and geometric information to\nmodel the relationships among elements within individual quadru-\nples. Accordingly, HyTE [8] employs a translation-based strategy\n[4] to measure the distance between entities and relations in the\ncustomized hyperplane, where each timestamp is associated with\na corresponding hyperplane. Inspised by BoxE [1], BoxTE [27] ad-\nditionally incorporates temporal information embedded through\nthe relation-specific transition matrix within the foundational box-\ntype relation modeling. TGeomE [41], a geometric algebra-based\nembedding approach, performs 4th-order tensor factorization and\napplies linear temporal regularization for time representation learn-\ning. These methods impose a rigid prior on KGs, relying solely on\ngeometric modeling and geographic measurements. However, they\nignore the implicit semantic information, failing to fully capture\nthe complex graph structure and relational context. Therefore, it\nis necessary to sufficiently explore the internal structure and\nsemantic relationships within individual quadruples (Limi-\ntation I).\nOn the other hand, extrapolation reasoning methods aim to\neffectively leverage the query-associated historical information\n[36]. They generally utilize Graph Neural Network (GNN) mod-\nels to capture structural characteristics within each snapshot and\nleverage Recurrent Neural Networks (RNNs) to explore the evo-\nlutionary characteristics across timestamps. Accordingly, RE-Net\n[16] employs a multi-relational graph aggregator to capture the\ngraph structure and an RNN-based encoder to capture temporal\ndependencies. RE-GCN [24] leverages a relation-aware Graph Con-\nvolution Network (GCN) to capture the structural dependencies\nfor the evolution unit and simultaneously captures the sequential\npatterns of all facts in parallel using gate recurrent components.\nRPC [25] mines the information underlying the relational corre-\nlations and periodic patterns via two correspondence units: one\nis based on a relational GCN, and the other is based on Gated Re-\ncurrent Units (GRUs). However, this phased processing strategy\ncan easily degrade the accuracy of predicting future events due to\nincorrect historical information. Therefore, it is necessary to adap-\ntively correct the interference of incorrect historical information\non prediction accuracy by learning a unified representation of\nthe contextual and temporal correlations among different\nquadruples (Limitation II) .\nGiven a subgraph of TKG (as shown in Figure 1a), interpolation\nmethods utilize geographical information to maximize structural\ndifferences between different quadruples. However, this type of\napproaches cannot effectively utilize semantic information in rea-\nsoning. Moreover, learning discriminative and powerful representa-\ntions for each item within a quadruple is challenging, especially in\nmodeling timestamps. For instance, differentiating between(Barack\nObama, visitTo, France, 2009) and (Donald Trump, visitTo, France,\n2017) necessitates precise characterizations of different names and\ntimestamps. Although extrapolation methods consider neighbor-\nhood information, independently processing structural and tempo-\nral characteristics can lead to confusion in reasoning. For instance,\nunder the timestamps of 2009 and 2017, the structure of Barack\nObama and Donald Trump with USA and France is consistent. Hence,\nthe model can naturally infer the structure of George Bush with\nUSA and France in 2001 should be consistent with the structure\nof Barack Obama and Donald Trump, which contradicts the facts.\nFrom this key observation, we believe that designing an end-to-end\nnetwork for learning a unified representation rich in information\nfrom temporal subgraph inputs will enhance the performance of\nTKGR. This network adaptively extracts both intra-quadruple and\ninter-quadruple, together with temporal information.\nAccording to the aforementioned analysis, we propose an inno-\nvative end-to-end Transformer-based network to learn the evolu-\ntionary chain of events (dubbed ECEformer) for TKGR. Figure 1b\nillustrates the motivation of our method. To easily illustrate the\nstructural and contextual relationship between the query node and\nthe adjacent nodes, we form an evolutionary chain of events (ECE)\nby extracting query-specific neighbors from the subgraph. Subse-\nquently, our proposed Transformer-based model learns the unified\nrepresentation for the ECE via two novel modules, namely, ECE\nRepresentation Learning (ECER) and Mixed-Context Knowledge\nReasoning (MCKR). Specifically, to overcome Limitation I, ECER\nemploys a Transformer-based encoder to embed each event in the\nECE, such as (USA, president of, 2001) corresponding to a specific\nquery George Bush . To overcome Limitation II , MCKR induces\nthe embeddings of each event and enhances interaction within and\nbetween quadruples via crafting an MLP-based information mixing\nlayer. In addition, to enhance the timeliness of the events, we de-\nvise an additional time prediction task to imbue effective temporal\ninformation within the learned unified representation.\nTransformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nIn summary, our main contributions are three-fold:\nâ€¢We propose an innovative temporal knowledge graph rea-\nsoning method, namely ECEformer. Experimental results\nverify the state-of-the-art performance of our method on six\npublicly available datasets.\nâ€¢Proposing a Transformer-based encoder for ECE representa-\ntion learning (ECER) aims to explore the internal structure\nand semantic relationships within individual quadruples.\nâ€¢Proposing an MLP-based layer for Mixed-Context Knowl-\nedge Reasoning (MCKR) aims to learn a unified represen-\ntation of the contextual and temporal correlations among\ndifferent quadruples.\n2 RELATED WORK\n2.1 Interpolation-based TKGR\nInterpolation-based TKGR methods estimate missing facts or events\nby identifying consistent trends within a TKG. Based on the process\nstrategy for temporal information, we categorize them into two\ntypes: timestamp-independent and timestamp-specific methods.\nTimestamp-independent methods treat timestamps as indepen-\ndent units, equivalent to entities or relations, and do not apply\nadditional operations on the timestamps. Generally, these methods\n[19, 20, 27, 29] directly associate timestamps to the corresponding\nentities and relations based on the foundation of static KG models.\nFor example, Leblayet al. [20] extended the classic TransE [4] model\nto TKGR by concatenating timestamp embeddings with relation\nembeddings. Inspired by BoxE [1], Messner et al. [27] directly in-\ntroduced timestamp embeddings on its basis. However, timestamp-\nindependent methods are usually limited in capturing the temporal\ninformation of evolutionary events. Timestamp-specific methods\nembed the temporal information and learn the evolution of enti-\nties and relations via timestamp-specific functions. To effectively\nutilize the structural and semantic information of the time, they\ningeniously designed various time-specific functions, such as di-\nachronic embedding functions [11], time-rotating functions [5, 40],\ntime-hyperplane functions [8, 38, 41, 47], and non-linear embedding\nfunctions [12, 17, 22, 35, 39]. Specifically, Goel et al. [11] proposed\na diachronic entity embedding function to characterize entities at\nany timestamp. Chen et al. [5] defined the spatial rotation oper-\nation of entities around the time axis. Zhang et al. [47] learned\nspatial structures interactively between the Euclidean, hyperbolic\nand hyper-spherical spaces. Han et al. [12] explored evolving entity\nrepresentations on a mixed curvature manifold using a velocity\nvector defined in the tangent space at each timestamp. Despite the\ngreat efforts made by these studies, they are limited in predicting\nfuture events [49].\n2.2 Extrapolation-based TKGR\nExtrapolation-based TKGR methods predict future facts or events\nby learning effective embeddings from historical snapshots. As\nknowledge subgraphs at specific timestamps, historical snapshots\ninherently contain rich structural and semantic information. Ex-\ntrapolation methods typically employ deep learning techniques\nsuch as Graph Neural Networks (GNNs) [ 24, 45, 46], Recurrent\nNeural Networks (RNNs) [16, 21], and Reinforcement Learning (RL)\n[33, 49] to extract features from historical snapshots.\nTo elaborate further, GNN-based methods are naturally applied\nto TKGR due to the inherent topological structure of historical\nsnapshots. Schlichtkrull et al. [30] early utilized Graph Convolu-\ntional Networks (GCN) to model the multi-relational features of\ntraditional KG. To incorporate the temporal characteristics in TKGs,\nJin et al. [16] additionally introduced an RNN-based sub-network\nspecifically for recursively encoding past facts. Li et al. [24] pre-\nsented the recurrent evolution network based on GCN, which learns\nthe evolutionary representations of entities and relations at each\ntimestamp by modeling the KG sequence recurrently. To handle the\nlarge-scale inputs and non-independent and identically distributed\ndata, Deng et al. [9] combined GCN and GRU for simultaneously\npredicting concurrent events of multiple types and inferring multi-\nple candidate participants. RL-based methods are becoming popular\nto improve the interpretability of reasoning methods, especially in\nquestion-answering tasks. By mimicking the human mechanism\nof searching for historical information and meticulous reasoning,\nCluSTeR [23] implements an interpretable prediction for TKGR\nusing a two-stage approach of clue searching and temporal reason-\ning. DREAM [49] introduces an adaptive reinforcement learning\nmodel based on attention mechanism, addressing the challenges in\ntraditional RL-based TKG reasoning, specifically the lack of captur-\ning temporal evolution and semantic dependence jointly and the\nover-reliance on manually designed rewards. Furthermore, some\nother types of methods also achieve good performance in TKGR. For\nexample, TLogic [26] introduces an explainable framework based\non temporal logical rules, leveraging temporal random walks to\nprovide explanations while maintaining time consistency. Faced\nwith the challenge of predicting future facts for newly emerging\nentities based on extremely limited observational data, MetaTKGR\n[37] dynamically adjusts its strategies for sampling and aggregating\nneighbors from recent facts and introduces a temporal adaptation\nregularizer to stabilize the meta-temporal reasoning process over\ntime. In summary, extrapolation methods generally process struc-\ntural and temporal characteristics independently. This approach\nignores the intrinsic correlation between structural and temporal in-\nformation, limiting the capability to extract a unified representation\nof both features.\n2.3 Transformer in Knowledge Graph\nWith the Transformer achieving excellent performance in numer-\nous tasks in natural language processing and computer vision, it\nhas been introduced into knowledge graph tasks. Leveraging the\npowerful learning capability of Transformers for sequential data\nstructures and contexts, numerous Transformer-based methods\n[6, 7, 14, 43] have emerged for traditional KGs. For example, Chen\net al. [6] proposed two different Transformer blocks for hierarchi-\ncally learning representations of entities and relations. In addition,\nsome methods also achieve remarkable results in downstream tasks\nof KGs. Specifically, Hu et al. [13] utilized a Transformer-based\nframework for encoding the content of neighbors of an entity for\nentity typing task. Meanwhile, Xu et al. [42] applied it for context-\naware rule mining over KG. To serve diverse KG-related tasks,\nZhang et al. [48] proposed a uniform knowledge representation\nand fusion module via structure pretraining and prompt tuning.\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Fang et al.\nTransformer enables the exploration of structural associations\nwithin each historical snapshot and captures the temporal rela-\ntionships among different historical snapshots to accomplish link\nprediction tasks [ 36]. GHT [ 32] involves two variants of Trans-\nformer and a relational continuous-time encoding function, aiming\nto mitigate the interference of facts irrelevant to the query and\nenhancing the capability of the model to learn long-term evolu-\ntion. To effectively embed the rich information in the query-specific\nsubgraph, SToKE [10] learns joint structural and temporal contextu-\nalized knowledge embeddings employing the pre-trained language\nmodel. Overview: While the Transformer framework has wide ap-\nplication in traditional KGs, its exploration in the field of TKGR\nremains relatively limited.\n3 METHODOLOGY\n3.1 Preliminary\nNote that TKG G= (E,R,T,Q)denotes a directed multi-relation\ngraph with timestamped edges between entities, whereEâˆˆ R|E|Ã—ğ‘‘,\nRâˆˆ R|R|Ã—ğ‘‘, and Tâˆˆ R|T|Ã—ğ‘‘ are the set of entities, relations and\ntimestamps, respectively. Q= {(ğ‘ ,ğ‘,ğ‘œ,ğœ )| ğ‘ ,ğ‘œ âˆˆE,ğ‘ âˆˆR,ğœ âˆˆT}\nis a set of quadruples in G, where ğ‘ , ğ‘œ, ğ‘ and ğœ are a subject entity,\nan object entity, the relation between them, and a timestamp, re-\nspectively. For the link prediction task, we infer the missing object\nentity by converting (ğ‘ ,ğ‘, ?,ğœ). Similarly, we infer the missing sub-\nject entity by converting(ğ‘œ,ğ‘âˆ’1,?,ğœ), where we employ a reciprocal\npredicate ğ‘âˆ’1 to distinguish this case.\nEvolutionary Chain of Events. The evolutionary chain of\nevents (ECE) transforms structural and temporal contexts related\nto the query-specific neighborhood subgraph into a structured\nknowledge sequence. Specifically, for the query (ğ‘ ,ğ‘, ?,ğœ), ECE is\nconceptualized as a singly linked listC= {ğ¶0,ğ¶1,Â·Â·Â· ,ğ¶ğ‘˜}, compris-\ning a chronologically arranged collection of events relevant to the\nsubject ğ‘  of the query. Notably, eachğ¶ represents an event related\nto ğ‘ , depicted by a triplet (ğ‘’,ğ‘,ğœ ), where ğ¶0 signifies the query and\nğ¶ğ‘˜ denotes adjacent facts, ğ‘˜ being the maximum adjacency count,\nğ‘’ can serve as either the subject entity or the object entity. For\nexample, for the query (George Bush, visitTo, ?, 2002) as shown in\nFigure 1a, the ECE can be formulated as: C={(George Bush, visitTo,\n2002), (USA, presidentOf, 2001), (Barack Obama, succeded, 2009)} . The\nformal expression is illustrated in Figure 1b. Hence, Our ECE can\nstructurally represent both intra-quadruples and inter-quadruples.\nBy encapsulating the dynamic evolution of entities and their inter-\nrelations within the event chain, the ECE can capture the historical\nand current events related to the subject entity.\n3.2 Overview of ECEformer\nAs an innovative end-to-end Transformer-based network, ECE-\nformer learns the evolutionary chain of events (ECE) for TKGR. This\nwork defines the subgraph composed of adjacent nodes that evolve\nover time along with the query entity as the evolutionary chain of\nevents (ECE). Different from existing Transformer-based models,\nECEformer crafts specific modules for handling intra-quadruples\nand inter-quadruples, respectively. As for intra-quadruples, ECE\nrepresentation learning (ECER) based on a Transformer encoder is\ndesigned to explore the internal structure and semantic relation-\nship within individual quadruples from the input ECE. On the other\nhand, as for the inter-quadruples, mixed-context knowledge rea-\nsoning (MCKR) involves a novel information mixing layer, and the\niterative transposition mechanism is designed to explore the con-\ntextual and temporal correlations among different quadruples. The\narchitecture of ECEformer is shown in Figure 2, and more details\nwill be illustrated in the following sections.\n3.3 Learning Representation for Evolutionary\nChain of Events\nThe first key to effectively learning ECE lies in the comprehen-\nsive extraction and analysis of each triplet corresponding to the\nsub-nodes on the ECE. Every constituent node of the ECE stores a\nhistorical event associated with the central entity, necessitating an\ninvestigation into its inherent structural and temporal characteris-\ntics. Inspired by the effectiveness of the Transformer architecture\nin learning inter-data dependencies, we introduce an ECER mod-\nule utilizing a Transformer encoder that distills the embedding\nof each triplet. The details are shown in Figure 2c. Specifically,\nECER consists of ğ‘ core units, each consisting of alternating lay-\ners of multi-head attention (MHA), layer normalization (LN), and\nMLP-based feed-forward (FF). Among them, both MHA and FF are\nfollowed by a LN accompanied by the residual connection (RC).\nThe FF contains two layers with the GELU non-linear activation\nfunction. This process can be formulated as:\nğ‘¦ = ğ‘™â—¦ğ‘Ÿ â—¦ğ‘“ â—¦ğ‘™â—¦ğ‘Ÿ â—¦â„(ğ‘¥;ğ‘Šâ„,ğ‘Šğ‘™,ğ‘Šğ‘“), (1)\nwhere â„,ğ‘™ and ğ‘“ denote MHA function with weightğ‘Šâ„, LN function\nwith weight ğ‘Šğ‘™, and FF function with weight ğ‘Šğ‘“, respectively. ğ‘Ÿ\ndenotes the RC function, which performs an operation that directly\ncombines the input with the output of the intermediate layer. ğ‘¥\ndenotes the input sequence. ğ‘¦ denotes the output embedding. â—¦\ndenotes the composite function operator. Note that the MHA relies\non the scaled dot-product attention mechanism to learn semantic\ninteractions between the current token and all tokens within the\nsequence, thereby thoroughly exploring the structural and seman-\ntic information within individual events. Additionally, by directly\ninputting timestamps as tokens into ECER, the model can semanti-\ncally extract temporal information within individual events.\nGiven a query ğ‘(ğ‘ ,ğ‘, ?,ğœ)and its ECE Cğ‘, we treat each compo-\nnent within the branch of ECE as an individual token, sequentially\nunfolding the chain structure based on temporal order. This process\nyields a new input, as illustrated in Figure 2a. For each triplet on\nevery branch of the ECE, we append a special token, i.e., [ğ¶ğ¿ğ‘†].\nSubsequently, we employ the concatenation of semantic embed-\ndings ğ¸ and positional embeddings ğ‘ƒ in the embedding space to\nrepresent each input token, as illustrated in Figure 2b. This process\ncan be formulated as:\nğ¸ğ‘–\nğ‘–ğ‘›ğ‘ = ğ¸ğ‘– +ğ‘ƒğ‘—,{ğ‘– âˆˆEâˆªRâˆªT ,ğ‘— âˆˆ[0,1,2,3]}, (2)\nwhere ğ¸ğ‘–\nğ‘–ğ‘›ğ‘ denotes ğ‘–âˆ’ğ‘¡â„ token in the input sequence, ğ¸ğ‘– denotes\nthe feature embedding of ğ‘–âˆ’ğ‘¡â„ token, and ğ‘ƒğ‘— denotes the position\nembedding. These embeddings are iteratively fed into the ECER\nmodule Fğ¸ğ¶ğ¸ğ‘… : R4Ã—ğ‘‘ â†’Rğ‘‘. Eventually, the output consists of\nintermediate embeddings ğ¸ğ‘–\nğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’ ,ğ‘– = 0,1,Â·Â·Â· ,ğ‘˜, which represent\nthe structural and semantic information implied in the triples cor-\nresponding to each branch.\nTransformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\n2001\n â€¦ \n2009\nGeorge Bush\nvisit to\nUSA\npresident of\nBarack Obama\nsucceded\n â€¦ \n2001\n â€¦ \n2009\nGeorge Bush\nvisit to\nUSA\npresident of\nBarack Obama\nsucceded\n â€¦ \n(a) Input triples of ECE\n(b) Embeddingh space\nUSA president \nof 2001USA president \nof 2001 Barack \nObama succeded 2009Barack \nObama succeded 2009George \nBush visit to [MASK]George \nBush visit to [MASK]\n \nâ€¦\n \n \nâ€¦\n \n \nâ€¦\n \n+ + + + + + + + + + + +\nECE Representation Learning (ECER)\nMixed-Context Knowledge Reasoning (MCKR)\nGlobal Average Pooling\nTimestamp Set\nUnified Representation\nEntity Set\nLink Prediction Time Prediction\n(c) Core unit in ECER\n(d) Core unit in MCKR\nLayer Norm\nTransposition\nChannel MLP\nPatch MLP\nLayer Norm\nTransposition\nLayer Norm\nTransposition\nChannel MLP\nPatch MLP\nLayer Norm\nTransposition\nMulti-Head Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nMulti-Head Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nTransformer-based Core unit Transformer-based Core unit\n Transformer-based Core unit\n \nâ€¦\n \n \nâ€¦\n \n \nâ€¦\n \nMLP-based Core unit\nFigure 2: The architecture of the proposed ECEformer. Given an evolutionary chain of events, ECEformer obtains input\nsequences (detailed in (a)) from each branch of the chain and converts each token into a concatenation of semantic embeddings\nğ¸and positional embeddings ğ‘ƒ (detailed in (b)). Subsequently, these branch embeddings ğ¸ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘™ğ‘’ are derived by the ECER, which\nis based on the Transformer encoder (detailed in (c)). The contextual information from different branch embeddings is then\ninductively processed by the MLP-based MCKR (detailed in (d)), culminating in a unified representation.\n3.4 Mixed-Context Knowledge Reasoning\nAfter encoding through the ECER, the embedded representations\nhave effectively abstracted the relevant information of individual\nevents. We introduce the query-specific contextual information\nto further enhance the accuracy of reasoning. Inspired by [ 34],\nwe introduce the MLP-based MCKR module to learn the multi-\ndimensional information interaction of the inputs by leveraging\ntwo mixing strategies. The details are illustrated in Figure 2d.\nDifferent from the single event input in ECER, the MCKR mod-\nule processes the sequence of 1 +ğ‘˜ embedding representations\nof ECE. This forms a two-dimensional real-valued input matrix\nMâˆˆ R(1+ğ‘˜)Ã—ğ‘‘. The MCKR consists of ğ‘€ core units, each consist-\ning of two key mixing layers: the channel MLP and the patch MLP.\nAdditionally, each mixing unit is preceded by a layer normaliza-\ntion and a transposition layer. The channel MLP operates on the\ncolumns of M, mapping each column fromR1+ğ‘˜ to R1+ğ‘˜. It aims to\nfacilitate information interaction across different event embeddings\nwithin the same feature dimension. Conversely, the patch MLP\noperates on the rows of M, mapping each row from Rğ‘‘ to Rğ‘‘. It\naims to facilitate information interaction within the same event\nembedding across different feature dimensions. This process can\nbe formulated as:\nÂ¤Mâˆ—,ğ‘– = ğ‘€âˆ—,ğ‘– +ğ‘Š2ğœ(ğ‘Š1Norm(M)âˆ—,ğ‘–), for ğ‘– = 1,2,Â·Â·Â· ,ğ‘‘ (3)\nÂ¥Mğ‘—,âˆ—= Â¤Mğ‘—,âˆ—+ğ‘Š4ğœ(ğ‘Š3Norm( Â¤M)ğ‘—,âˆ—), for ğ‘— = 1,2,Â·Â·Â· ,ğ‘˜ +1 (4)\nwhere ğœ is the non-linear activation function (GELU). Each MLP\nblock in MCKR contains two fully-connected layers,ğ‘Š1 and ğ‘Š2 are\nhidden weights in the channel MLP,ğ‘Š3 and ğ‘Š4 are hidden weights\nin the patch MLP.\nFollowing the methodology outlined in [34], our MCKR employs\nthe same parameter to translate every column (row) for channel\nMLP (patch MLP). This parameter sharing mechanism endows\nthe network with positional invariance, enhancing its ability to\nproficiently process sequential data. Moreover, this approach also\ncircumvents the substantial increase in model complexity that typ-\nically accompanies dimensional augmentation. Subsequently, we\nleverage a global average pooling layer to distill the mixed-context\nÂ¥ğ‘€, which can be formulated as:\nUğ¸ğ¶ğ¸ = GlobalAvgPool(Â¥ğ‘€), (5)\nwhere Uğ¸ğ¶ğ¸ denotes the unified representation of ECE. Given a\nquadruple with either the subject or the object missing, the task of\nlink prediction can be accomplished by computing the similarity\nbetween Uğ¸ğ¶ğ¸ and all candidate entities. Link prediction can be\nmathematically formulated as:\nğ‘(ğ‘’ğ‘”ğ‘¡|ğ‘)= ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘†ğ‘–ğ‘š(Uğ¸ğ¶ğ¸,E)), (6)\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Fang et al.\nTable 1: Statistics of the Experimental Datasets.\nDataset | E| | R| | T| Time ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› ğ‘ğ‘£ğ‘ğ‘™ğ‘–ğ‘‘ ğ‘ğ‘¡ğ‘’ğ‘ ğ‘¡\nGDELT 500 20 366 15 mins 2,735,685 341,961 341,961\nICEWS05-15 10,488 251 4,017 24 hours 386,962 46,092 46,2775\nICEWS18 23,033 256 7,272 24 hours 373,018 45,995 49,545\nICEWS14 7,128 230 365 24 hours 63,685 13,823 13,222\nYAGO11K 10,623 10 73 1 year 16,406 2,050 2,051\nWikidata12K 12,554 24 84 1 year 32,497 4,062 4,062\nwhere ğ‘†ğ‘–ğ‘š denotes the similarity function between Uğ¸ğ¶ğ¸ and a\ncandidate entity, ğ‘denotes the predicted result, obtained by select-\ning the ğ‘¡ğ‘œğ‘ âˆ’1 entity based on similarity scores. ğ‘(ğ‘’ğ‘”ğ‘¡|ğ‘)denotes\nthe occurrence probability that ğ‘is the target entity ğ‘’ğ‘”ğ‘¡.\n3.5 Enhancement of Temporal Information\nConsidering that events in TKGs often coincide with specific times-\ntamps, the unified representations learned from link prediction\nmight be trivial solutions facilitated by contextual information.\nSuch representations will introduce spurious correlations, because\nthey only consider entities and relations while neglecting the tem-\nporal aspect of events. To address this issue, we propose a marked\ntime prediction task to enhance the exploration of temporal infor-\nmation during the contextualization process. Specifically, we utilize\na special token [ğ‘€ğ´ğ‘†ğ¾]to replace the timestamp ğœ in the query-\nspecific branch of ECE. In other words, we transform theğ¶0 (ğ‘ ,ğ‘,ğœ )\nin ECE to ğ¶0 (ğ‘ ,ğ‘, [ğ‘€ğ´ğ‘†ğ¾]). This direct masking strategy intro-\nduces perturbations to the original timestamps, thereby prompting\nthe model to learn contextual information. To promote the modelâ€™s\ncapability of understanding temporal information, we train the\nmodel using masked inputs to recover the perturbed timestamp.\nThis design can encourage the model to assimilate contextual infor-\nmation and deduce the occurrence time of the query-specific event.\nThe methodology is analogous to link prediction, where we di-\nrectly utilize the unified representationUğ¸ğ¶ğ¸ to predict the correct\ntimestamp. The prediction probability ğ‘(ğœğ‘”ğ‘¡|ğ‘)can be formulated\nas:\nğ‘(ğœğ‘”ğ‘¡|ğ‘)= ğ‘†ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘†ğ‘–ğ‘š(Uğ¸ğ¶ğ¸,T)), (7)\nwhere ğ‘(ğœğ‘”ğ‘¡|ğ‘)denotes the probability that predicted timestamp ğ‘\ncorresponds to the target timestamp ğœğ‘”ğ‘¡.\n3.6 Training and Optimization\nFor link prediction and time prediction tasks, we employ the cross-\nentropy function to calculate the loss during the training process.\nThe loss function can be formulated as:\nL= âˆ’\nâˆ‘ï¸\n(ğ‘ ,ğ‘,ğ‘œ,ğœ)âˆˆG\nlog(ğ‘(ğ‘’ğ‘”ğ‘¡|ğ‘))+ ğœ†log(ğ‘(ğœğ‘”ğ‘¡|ğ‘)), (8)\nwhere (ğ‘ ,ğ‘,ğ‘œ,ğœ )âˆˆG represents the historical events in the training\nset, ğœ†weights the time prediction task.\nAdditionally, for diversifying our training data and reducing\nmemory cost, we implement an ECE sampling strategy similar to\nthe edge dropout regularization [6]. This approach samples only\na portion of the neighborhood information of the query subject\nand filters out the ground truth target entity from the sampled\nneighborhood information.\n4 EXPERIMENTS\nIn this section, the experimental settings are first introduced from\nfour aspects, including datasets, evaluation metrics, compared base-\nlines, and implementation details. Then, we comprehensively ana-\nlyze the proposed ECEformer also from three aspects, i.e., superi-\nority, effectiveness, and sensitivity.\n4.1 Experimental Setup\n4.1.1 Datasets and Evaluation Metrics. We evaluate the reasoning\nperformance of our ECEformer on six TKG benchmark datasets,\nincluding GDELT, ICEWS05-15, ICEWS18, ICEWS14, YAGO11K,\nand Wikidata12K. According to [ 25], we also split the datasets\nof GDELT and ICEWS05-15/18/14 into train/valid/test by times-\ntamps. According to [ 8], we split the datasets of YAGO11K and\nWikidata12K into train/valid/test by time intervals. The statistical\ninformation of datasets is shown in Table 1, in which Time rep-\nresents time granularity. Two widely used evaluation metrics are\nadopted to quantify the performance, namely ğ‘€ğ‘…ğ‘… and ğ»ğ‘–ğ‘¡ğ‘ @ğ‘˜.\nğ‘€ğ‘…ğ‘… represents the mean reciprocal rank of the inferred true entity\namong the queried candidates, and ğ»ğ‘–ğ‘¡ğ‘ @ğ‘˜ measures the propor-\ntion of instances where the true entity appears among the top ğ‘˜\nranked candidates.\n4.1.2 Compared Baselines. Considering that different methods are\nvalidated on various datasets, we divided our experiments into\ntwo groups according to dataset size for a fair comparison. We\ncompared with twenty state-of-the-art (SOTA) models. Specifically,\nthe extrapolation baselines for GDELT, ICEWS05-15, and ICEWS18\nconsist of RL-based models (TITer [33], TLogic [26], and DREAM\n[49]), GNN-based models (RE-GCN [24], TiRGN [21], HGLS [46],\nğ¿2TKG [45], and RPC [25]), and Transformer-based models (GHT\n[32] and SToKE [10]). The interpolation baselines chosen for smaller\ndatasets (ICEWS14, YAGO11K, and Wikidata12K) include TimePlex\n[15], TeLM [38], TeRo [40], RotateQVS [ 5], SANe [22], NeuSTIP\n[31], QDN [35], LCGE [28], TGeomE [41], and HyIE [47].\n4.1.3 Implementation Details. All experiments are conducted on a\nsingle NVIDIA RTX A6000. The implementation1 of our model is\nbuilt upon the open-source framework LibKGE2. The dimension\nof the input embedding is set to 320. The hidden widths of the\nfeed-forward layer inside ECER and the channel (patch) MLP inside\nMCKR are set to 1024. The maximum neighborhood of ECE is set\nto 50. We configure the core unit numbers for ECER and MCKR to\nbe ğ‘ = 3 and ğ‘€ = 6, respectively. The activation function in our\nnetwork is GELU.\nWe train our model via the Adamax [18] with an initial learning\nrate of 0.01 and an L2 weight decay rate of 0.01. We employed the\nwarm-up training strategy, wherein the learning rate commenced\nfrom zero and increased linearly during the initial 10% of the train-\ning steps, followed by a linear decrease throughout the remaining\ntraining process. We train the model with a batch size of 512 for a\nmaximum of 300 epochs, employing an early stopping mechanism\nbased on MRR in the validation set. Detailed discussions on the\neffects of the weight coefficient ğœ†, the number of core units ğ‘ and\nğ‘€ are in the subsection Sensitivity Analysis.\n1https://github.com/seeyourmind/TKGElib\n2https://github.com/uma-pi1/kge\nTransformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\nTable 2: Performance of link prediction on GDELT, ICEWS05-15, and ICEWS18. The best results are marked in bold, the second\nresults are marked by underlining .\nModel GDELT ICEWS05-15 ICEWS18\nMRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10\nTITer 18.19 11.52 19.20 31.00 47.60 38.29 52.74 64.86 29.98 22.05 33.46 44.83\nTLogic 19.80 12.20 21.70 35.60 46.97 36.21 53.13 67.43 29.82 20.54 33.95 48.53\nDREAM 28.10 19.30 31.10 44.70 56.80 47.30 65.10 78.60 39.10 28.00 45.20 62.70\nRE-GCN 19.69 12.46 20.93 33.81 48.03 37.33 53.90 68.51 32.62 22.39 36.79 52.68\nTiRGN 21.67 13.63 23.27 37.60 49.84 39.07 55.75 70.11 33.58 23.10 37.90 54.20\nHGLS 19.04 11.79 - 33.23 46.21 35.32 - 67.12 29.32 19.21 - 49.83\nğ¿2TKG 20.53 12.89 - 35.83 57.43 41.86 - 80.69 33.36 22.15 - 55.04\nRPC 22.41 14.42 24.36 38.33 51.14 39.47 57.11 71.75 34.91 24.34 38.74 55.89\nGHT 20.04 12.68 21.37 34.42 41.50 30.79 46.85 62.73 27.40 18.08 30.76 45.76\nSToKE 37.10 29.00 39.90 52.50 71.20 60.50 79.00 88.50 - - - -\nECEformer (Ours) 51.19 38.72 59.41 71.10 77.29 73.11 79.39 85.17 44.80 39.18 46.89 55.31\nTable 3: Performance of link prediction task on ICEWS14, YAGO11K, and Wikidata12K. The best results are marked in bold,\nthe second results are marked by underlining .\nModel ICEWS14 YAGO11K Wikidata12K\nMRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10\nTimePlex 60.40 51.50 - 77.11 23.64 16.92 - 36.71 33.35 22.78 - 53.20\nTeLM 62.50 54.50 67.30 77.40 19.10 12.90 19.40 32.10 33.20 23.10 36.00 54.20\nTeRo 56.20 46.80 62.10 73.20 18.70 12.10 19.70 31.90 29.90 19.80 32.90 50.70\nRotateQVS 59.10 50.70 64.20 75.40 18.90 12.40 19.90 32.30 - - - -\nSANe 63.80 55.80 68.80 78.20 25.00 18.00 26.60 40.10 43.20 33.10 48.30 64.00\nNeuSTIP - - - - 25.23 18.45 - 37.76 34.78 24.38 - 53.75\nQDN 64.30 56.70 68.80 78.40 19.80 13.10 20.10 32.80 - - - -\nLCGE 66.70 58.80 71.40 81.50 - - - - 42.90 30.40 49.50 67.70\nTGeomE 62.90 54.60 68.00 78.00 19.50 13.00 19.60 32.60 33.30 23.20 36.20 54.60\nHyIE 63.10 56.30 68.70 78.60 19.10 12.50 20.10 32.60 30.10 19.70 32.80 50.60\nECEformer (Ours) 71.70 67.31 73.60 80.30 25.63 19.48 26.88 37.84 47.81 41.35 49.96 60.35\n4.2 Performance Comparison for Superiority\nWe compare our model with 20 SOTA models ranging from 2020\nto 2023 on six benchmark datasets. We report the result in Table 2\nand Table 3, where the best performances are highlighted in bold,\nwhile the second-best are marked by underlining. A noteworthy\nobservation is that our ECEformer significantly outperforms other\nbaseline models across all datasets in terms of ğ‘€ğ‘…ğ‘… and ğ»ğ‘–ğ‘¡ğ‘ @ğ‘˜.\nSpecifically, ECEformer achieves performance improvements over\nthe second-best models by 14.09%, 6.09%, 5.70%, 5.00%, 0.40%, and\n4.61% on ğ‘€ğ‘…ğ‘…, respectively. Additionally, it achieves gains of 9.72%,\n12.61%, 11.18%, 8.51%, 1.03%, and 8.25% on ğ»ğ‘–ğ‘¡ğ‘ @1, respectively.\nThis significant margin of improvement demonstrates the superior\nperformance of our ECEformer.\nIn particular, our model exhibits notable performance in four\nmetrics on GDELT:ğ‘€ğ‘…ğ‘…, ğ»ğ‘–ğ‘¡ğ‘ @1, ğ»ğ‘–ğ‘¡ğ‘ @3, andğ»ğ‘–ğ‘¡ğ‘ @10, achieving\nrespective scores of 51.19%, 38.72%, 59.41%, and 71.10%. Compared\nwith RL-based methods, our ECEformer outperforms DREAM by\n23.09% in ğ‘€ğ‘…ğ‘…, by 9.42% in ğ»ğ‘–ğ‘¡ğ‘ @1, by 28.31% in ğ»ğ‘–ğ‘¡ğ‘ @3, and by\n26.4% in ğ»ğ‘–ğ‘¡ğ‘ @10, respectively. Compared with GNN-based meth-\nods, our ECEformer outperforms RPC by 28.78% in ğ‘€ğ‘…ğ‘…, by 24.30%\nin ğ»ğ‘–ğ‘¡ğ‘ @1, by 35.05% inğ»ğ‘–ğ‘¡ğ‘ @3, and by 32.77% inğ»ğ‘–ğ‘¡ğ‘ @10, respec-\ntively. Compared with Transformer-based methods, our ECEformer\noutperforms SToKE by 14.09% in ğ‘€ğ‘…ğ‘…, by 9.72% in ğ»ğ‘–ğ‘¡ğ‘ @1, by\n19.51% in ğ»ğ‘–ğ‘¡ğ‘ @3, and by 18.6% in ğ»ğ‘–ğ‘¡ğ‘ @10, respectively. This indi-\ncates that our model can effectively learn the unified representations\nfrom frequently changing historical events in datasets characterized\nby fine-grained temporal granularity. Based on the comparative\nresults from the ICEWS (i.e., ICEWS05-15/18/14), ECEformer consis-\ntently surpasses all the baselines on theğ‘€ğ‘…ğ‘…, ğ»ğ‘–ğ‘¡ğ‘ @1, and ğ»ğ‘–ğ‘¡ğ‘ @3\nmetrics. However, it lags behind the second-best model inğ»ğ‘–ğ‘¡ğ‘ @10.\nThis situation also occurs in the YAGO11K and Wikidata12K. This\nsuggests that although our model demonstrates high precision in\npredictions, there is still room for improvement in its recall capa-\nbility. Besides, compared to other datasets, our method shows a\nrelatively slight performance improvement on YAGO11K. This is\nattributed to 1) the issue of missing timestamps in the YAGO11K\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Fang et al.\nTable 4: Ablation study on different module combinations\non ICEWS14 and Wikidata12K. ğ‘‡ğ‘ƒ represents the time pre-\ndiction task. The best results are marked in bold.\nModel ICEWS14 Wikidata12K\nMRR Hits@10 MRR Hits@10\nECER 70.60 80.40 43.49 57.08\nECEformer (w/o TP) 69.4 79.1 46.12 58.54\nECEformer 71.70 80.30 47.81 60.35\ndataset, and 2) the NeuSTIP method additionally introduces Allen\nalgebra [31] to refine temporal rules and designs a specialized time\nprediction evaluation function to capture temporal information.\nNevertheless, the performance gain achieved by our method indi-\ncates that the proposed masked time prediction task can mitigate\nchallenges arising from low data quality.\n4.3 Ablation Study for Effectiveness\nTo analyze the contribution of ECEformer components, we validate\nthem and their variants on the ICEWS14 and Wikidata12K: (1)\nusing only ECER to embed individual query-specific quadruples\n(without contextual information); (2) combining ECER and MCKR\nwithout conducting the time prediction task; (3) combining ECER\nand MCKR while undertaking the time prediction task (i.e., the\ncomplete ECEformer). The detailed results are listed in Table 4.\nFrom the results of ECER, we observe that by utilizing the Trans-\nformer architecture, our model can surpass most geoinformation-\nbased models (such as TGeomE). This validates our viewpoint,\nwhich emphasizes sufficiently exploring the internal structure and\nsemantic relationships within individual quadruples. Comparing\nthe results of the ECER and ECEformer (ğ‘¤/ğ‘œ TP), we observe that\nincorporating contextual information significantly improves the\nmodelâ€™s reasoning accuracy on Wikidata12K. However, this integra-\ntion within ICEWS14 slightly diminishes performance. Specifically,\nECEformer (ğ‘¤/ğ‘œ TP) outperforms ECER by 2.63% on Wikidata12K,\nbut underperforms by 1.20% on ICEWS14 in terms ofğ‘€ğ‘…ğ‘…. This phe-\nnomenon is attributed to the finer temporal granularity of ICEWS14\ncompared to Wikidata12K, where the frequent historical events in\nthe context exacerbate spurious correlations. Without the inter-\nvention of the time prediction task, the model is more prone to\nerroneous estimations. Comparing the results of ECEformer (ğ‘¤/ğ‘œ\nTP) and ECEformer, we observe consistent performance enhance-\nments on both datasets after executing the time prediction task. This\nis especially pronounced in ICEWS14, where the implementation\nof the time prediction task results in a notable augmentation of the\nğ‘€ğ‘…ğ‘…, surpassing the performance of the ECER. Specifically, ECE-\nformer outperforms ECER by 1.10%, and it outperforms ECEformer\n(ğ‘¤/ğ‘œ TP) by 2.30% in terms of ğ‘€ğ‘…ğ‘…. These findings confirm the\neffectiveness of our proposed temporal information enhancement\nstrategy in prompting the model to leverage temporal information\nwithin its context, thereby effectively mitigating the impacts of\nspurious correlations.\nTable 5: Sensitivity analysis of ECEformer with different\nnumber of unit layers. ğ‘ and ğ‘€ represent the number of\nECER units and MCKR units, respectively.\nğ‘ ğ‘€ ICEWS14 Wikidata12K\nMRR Hits@10 MRR Hits@10\n1x\n1\n66.30 78.23 47.94 59.33\n2x 67.36 78.77 48.08 60.00\n3x 66.95 78.79 48.47 60.24\n1\n1x 66.30 78.23 47.94 59.33\n2x 69.01 78.53 48.31 60.19\n3x 70.30 79.08 48.19 60.09\n1x\n2\n69.04 79.10 48.37 60.45\n2x 69.34 79.03 48.32 60.51\n3x 69.36 78.94 48.34 60.61\n2\n1x 69.04 79.10 48.37 60.45\n2x 70.18 78.80 48.22 60.39\n3x 70.10 78.19 47.67 59.21\n0.5 1.0 1.5 2.0 2.5\nICEWS14\n60\n65\n70\n75\n80\n85\nMRR Hits@1 Hits@3 Hits@10\n0.5 1.0 1.5 2.0 2.5\nWikidata12K\n40\n45\n50\n55\n60\n65\nMRR Hits@1 Hits@3 Hits@10\n(a) Weight coefficient ğœ†\n0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50\nICEWS14\n60\n65\n70\n75\n80\n85\nMRR Hits@1 Hits@3 Hits@10\n0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50\nWikidata12K\n35\n40\n45\n50\n55\n60\nMRR Hits@1 Hits@3 Hits@10\n(b) Random marking rate ğ›¾\nFigure 3: Sensitivity analysis of weight coefficients ğœ† and\nrandom masking rate ğ›¾ on ICEWS14 and Wikidata12K.\n4.4 Sensitivity Analysis\nAs mentioned in Implementation Details, we explore the perfor-\nmance fluctuations of ECEformer caused by the structural control\nparameters ğ‘ and ğ‘€, as well as the weight coefficient ğœ†. For struc-\ntural control parameters ğ‘ and ğ‘€, we set the base of one parameter\nto {1,2}, while the other is adjusted in multiples, specifically to 1Ã—,\n2Ã—, and 3Ã—. As shown in Table 5, the results highlight the maximum\nvalue in each group in bold. Our observations reveal that different\nstructural settings do introduce perturbations to the model, with the\ndisturbance being relatively minor at ğ‘ : ğ‘€ = 1 : 2. Based on this\nfinding, we configure the architecture of ECEformer as ğ‘ = 3 and\nğ‘€ = 6 across all datasets. For the weight coefficient ğœ†, we record\nthe modelâ€™s performance across a range of {0.5,1.0,1.5,2.0,2.5},\nand plotted these results in Figure 3a. The performance curve trend\nsuggests that excessively weighting the time prediction task can\nTransformer-based Reasoning for Learning Evolutionary Chain of Events on Temporal Knowledge Graph SIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA\ndetrimentally affect the link prediction task, thereby reducing over-\nall performance. Therefore, setting the weight coefficient between\n0.5 and 1.0 offers a balanced improvement in model performance.\nIn addition, to investigate the impact of the masking mechanism\nin the time prediction task on model performance, we report the\nresults of each batch of data under varying masking rates, with\ndetails as well as illustrated in Figure 3b. Specifically, we employ a\nfixed masking rate ğ›¾ during the training phase, randomly selecting\nğ›¾ Ã—batch_size samples per batch to mask their timestamps. We\nobserve that as the masking rate increases, the model performance\ncorrespondingly improves. Additionally, compared to Wikidata12K,\nthe performance of ICEWS14 exhibits greater fluctuations with\nvarying masking rates. This observation indicates that datasets\nwith finer temporal granularity are more sensitive to variations\nin masking rate. Notably, our final model is trained using a fully\nmasked approach.\n5 CONCLUSION\nIn this paper, we address the challenge of completing missing fac-\ntual elements in TKG reasoning by introducing the ECEformer, a\nnovel end-to-end Transformer-based reasoning model. ECEformer\nprimarily comprises two key components: the evolutionary chain\nof events representation learning (ECER) and the mixed-context\nknowledge reasoning (MCKR). The ECER, utilizing the Transformer\nencoder, thoroughly explores the structural and semantic infor-\nmation of historical events corresponding to each branch in the\nevolutionary chain of events (ECE). The ECE is composed of neigh-\nborhood subgraphs of entity nodes unfolded in chronological order.\nThe MCKR based on MLP relies on the channel and patch mixing\nstrategy to facilitate the learning of contextual information interac-\ntions within the ECE. Moreover, an additional time prediction task\nand a time masking mechanism are employed to force the model to\nassimilate temporal information from the context. Comparative ex-\nperiments with state-of-the-art methods validate the superiority of\nour approach. Furthermore, additional ablation studies demonstrate\nthe effectiveness of each module proposed in our ECEformer.\nACKNOWLEDGMENTS\nThis research was supported by National Science and Technology\nMajor Project (2022ZD0119204), National Science Fund for Dis-\ntinguished Young Scholars (62125601), National Natural Science\nFoundation of China (62076024, 62172035).\nREFERENCES\n[1] Ralph Abboud, Ä°smail Ä°lkan Ceylan, Thomas Lukasiewicz, and Tommaso Salva-\ntori. 2020. BoxE: A Box Embedding Model for Knowledge Base Completion. In\nAdvances in Neural Information Processing Systems . 9649â€“9661.\n[2] Maristella Agosti, Stefano Marchesin, and Gianmaria Silvello. 2020. Learning\nunsupervised knowledge-enhanced representations to reduce the semantic gap\nin information retrieval. ACM Transactions on Information Systems 38, 4 (2020),\n1â€“48.\n[3] Farah Atif, Ola El Khatib, and Djellel Difallah. 2023. BeamQA: Multi-hop Knowl-\nedge Graph Question Answering with Sequence-to-Sequence Prediction and\nBeam Search. In Proceedings of the International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval . 781â€“790.\n[4] Antoine Bordes, Nicolas Usunier, Alberto GarcÃ­a-DurÃ¡n, Jason Weston, and Ok-\nsana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-relational\nData. In Advances in Neural Information Processing Systems . 2787â€“2795.\n[5] Kai Chen, Ye Wang, Yitong Li, and Aiping Li. 2022. RotateQVS: Representing\nTemporal Information as Rotations in Quaternion Vector Space for Temporal\nKnowledge Graph Completion. In Proceedings of the Annual Meeting of the Asso-\nciation for Computational Linguistics . 5843â€“5857.\n[6] Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao, Ruofei Zhang, and Yangfeng\nJi. 2021. HittER: Hierarchical Transformers for Knowledge Graph Embeddings. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing .\n10395â€“10407.\n[7] Xiang Chen, Ningyu Zhang, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu,\nFei Huang, Luo Si, and Huajun Chen. 2022. Hybrid Transformer with Multi-\nLevel Fusion for Multimodal Knowledge Graph Completion. In Proceedings of the\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 904â€“915.\n[8] Shib Sankar Dasgupta, Swayambhu Nath Ray, and Partha P. Talukdar. 2018.\nHyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing .\n2001â€“2011.\n[9] Songgaojun Deng, Huzefa Rangwala, and Yue Ning. 2020. Dynamic knowl-\nedge graph based multi-event forecasting. In Proceedings of the ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining . 1585â€“1595.\n[10] Yifu Gao, Yongquan He, Zhigang Kan, Yi Han, Linbo Qiao, and Dongsheng\nLi. 2023. Learning Joint Structural and Temporal Contextualized Knowledge\nEmbeddings for Temporal Knowledge Graph Completion. In Findings of the\nAssociation for Computational Linguistics . 417â€“430.\n[11] Rishab Goel, Seyed Mehran Kazemi, Marcus A. Brubaker, and Pascal Poupart.\n2020. Diachronic Embedding for Temporal Knowledge Graph Completion. In\nProceedings of the AAAI Conference on Artificial Intelligence . 3988â€“3995.\n[12] Zhen Han, Peng Chen, Yunpu Ma, and Volker Tresp. 2020. DyERNIE: Dynamic\nEvolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph\nCompletion. In Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing . 7301â€“7316.\n[13] Zhiwei Hu, Victor Gutierrez-Basulto, Zhiliang Xiang, Ru Li, and Jeff Pan. 2022.\nTransformer-based Entity Typing in Knowledge Graphs. In Proceedings of the\nConference on Empirical Methods in Natural Language Processing . 5988â€“6001.\n[14] Zhiwei Hu, VÃ­ctor GutiÃ©rrez-Basulto, Zhiliang Xiang, Ru Li, and Jeff Z Pan. 2023.\nHyperFormer: Enhancing entity and relation interaction for hyper-relational\nknowledge graph completion. In Proceedings of the ACM International Conference\non Information and Knowledge Management . 803â€“812.\n[15] Prachi Jain, Sushant Rathi, Soumen Chakrabarti, et al. 2020. Temporal Knowledge\nBase Completion: New Algorithms and Evaluation Protocols. In Proceedings of\nthe Conference on Empirical Methods in Natural Language Processing . 3733â€“3747.\n[16] Woojeong Jin, Meng Qu, Xisen Jin, and Xiang Ren. 2020. Recurrent Event Net-\nwork: Autoregressive Structure Inferenceover Temporal Knowledge Graphs. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing .\n6669â€“6683.\n[17] Jaehun Jung, Jinhong Jung, and U Kang. 2021. Learning to walk across time for\ninterpretable temporal knowledge graph completion. In Proceedings of the ACM\nSIGKDD Conference on Knowledge Discovery & Data Mining . 786â€“795.\n[18] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimiza-\ntion. In Proceedings of the International Conference on Learning Representations .\n[19] TimothÃ©e Lacroix, Guillaume Obozinski, and Nicolas Usunier. 2020. Tensor\nDecompositions for Temporal Knowledge Base Completion. In Proceedings of the\nInternational Conference on Learning Representations .\n[20] Julien Leblay and Melisachew Wudage Chekol. 2018. Deriving Validity Time in\nKnowledge Graph. InCompanion of the International World Wide Web Conferences .\n1771â€“1776.\n[21] Yujia Li, Shiliang Sun, and Jing Zhao. 2022. Tirgn: time-guided recurrent graph\nnetwork with local-global historical patterns for temporal knowledge graph rea-\nsoning. In Proceedings of the International Joint Conference on Artificial Intelligence .\nijcai. org, 2152â€“2158.\n[22] Yancong Li, Xiaoming Zhang, Bo Zhang, and Haiying Ren. 2022. Each snapshot\nto each space: Space adaptation for temporal knowledge graph completion. In\nProceedings of the International Semantic Web Conference . 248â€“266.\n[23] Zixuan Li, Xiaolong Jin, Saiping Guan, Wei Li, Jiafeng Guo, Yuanzhuo Wang,\nand Xueqi Cheng. 2021. Search from History and Reason for Future: Two-stage\nReasoning on Temporal Knowledge Graphs. InProceedings of the Annual Meeting\nof the Association for Computational Linguistics . 4732â€“4743.\n[24] Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei Shen,\nYuanzhuo Wang, and Xueqi Cheng. 2021. Temporal knowledge graph reasoning\nbased on evolutional representation learning. In Proceedings of the International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n408â€“417.\n[25] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Si-\nhang Zhou, and Xinwang Liu. 2023. Learn from Relational Correlations and\nPeriodic Events for Temporal Knowledge Graph Reasoning. In Proceedings of the\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 1559â€“1568.\n[26] Yushan Liu, Yunpu Ma, Marcel Hildebrandt, Mitchell Joblin, and Volker Tresp.\n2022. Tlogic: Temporal logical rules for explainable link forecasting on temporal\nknowledge graphs. In Proceedings of the AAAI conference on artificial intelligence ,\nSIGIR â€™24, July 14â€“18, 2024, Washington, DC, USA Fang et al.\nVol. 36. 4120â€“4127.\n[27] Johannes Messner, Ralph Abboud, and Ä°smail Ä°lkan Ceylan. 2022. Temporal\nKnowledge Graph Completion Using Box Embeddings. In Proceedings of the\nAAAI Conference on Artificial Intelligence . 7779â€“7787.\n[28] Guanglin Niu and Bo Li. 2023. Logic and commonsense-guided temporal knowl-\nedge graph completion. In Proceedings of the AAAI Conference on Artificial Intelli-\ngence, Vol. 37. 4569â€“4577.\n[29] Ali Sadeghian, Mohammadreza Armandpour, Anthony Colas, and Daisy Zhe\nWang. 2021. ChronoR: Rotation Based Temporal Knowledge Graph Embedding.\nIn Proceedings of the AAAI Conference on Artificial Intelligence . 6471â€“6479.\n[30] Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg,\nIvan Titov, and Max Welling. 2018. Modeling Relational Data with Graph Con-\nvolutional Networks. In Proceedings of the European Semantic Web Conference ,\nVol. 10843. 593â€“607.\n[31] Ishaan Singh, Navdeep Kaur, Garima Gaur, et al . 2023. NeuSTIP: A Neuro-\nSymbolic Model for Link and Time Prediction in Temporal Knowledge Graphs. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing .\n4497â€“4516.\n[32] Haohai Sun, Shangyi Geng, Jialun Zhong, Han Hu, and Kun He. 2022. Graph\nHawkes Transformer for Extrapolated Reasoning on Temporal Knowledge\nGraphs. InProceedings of the Conference on Empirical Methods in Natural Language\nProcessing. 7481â€“7493.\n[33] Haohai Sun, Jialun Zhong, Yunpu Ma, Zhen Han, and Kun He. 2021. TimeTrav-\neler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. In\nProceedings of the Conference on Empirical Methods in Natural Language Processing .\n8306â€“8319.\n[34] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua\nZhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob\nUszkoreit, et al. 2021. Mlp-mixer: An all-mlp architecture for vision. Advances in\nneural information processing systems 34 (2021), 24261â€“24272.\n[35] Jiapu Wang, Boyue Wang, Junbin Gao, Xiaoyan Li, Yongli Hu, and Baocai Yin.\n2023. QDN: A Quadruplet Distributor Network for Temporal Knowledge Graph\nCompletion. IEEE Transactions on Neural Networks and Learning Systems (2023),\n1â€“13.\n[36] Jiapu Wang, Boyue Wang, Meikang Qiu, Shirui Pan, Bo Xiong, Heng Liu, Linhao\nLuo, Tengfei Liu, Yongli Hu, Baocai Yin, et al . 2023. A survey on temporal\nknowledge graph completion: Taxonomy, progress, and prospects. arXiv preprint\narXiv:2308.02457 (2023).\n[37] Ruijie Wang, Zheng Li, Dachun Sun, Shengzhong Liu, Jinning Li, Bing Yin, and\nTarek Abdelzaher. 2022. Learning to sample and aggregate: Few-shot reasoning\nover temporal knowledge graphs. Advances in Neural Information Processing\nSystems 35 (2022), 16863â€“16876.\n[38] Chengjin Xu, Yung-Yu Chen, Mojtaba Nayyeri, and Jens Lehmann. 2021. Temporal\nknowledge graph completion using a linear temporal regularizer and multivector\nembeddings. In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies .\n2569â€“2578.\n[39] Chenjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Yazdi, and Jens Lehmann.\n2020. Temporal knowledge graph completion based on time series gaussian\nembedding. In Proceedings of the International Semantic Web Conference . 654â€“\n671.\n[40] Chengjin Xu, Mojtaba Nayyeri, Fouad Alkhoury, Hamed Shariat Yazdi, and Jens\nLehmann. 2020. TeRo: A Time-aware Knowledge Graph Embedding via Tem-\nporal Rotation. In Proceedings of the International Conference on Computational\nLinguistics. 1583â€“1593.\n[41] Chengjin Xu, Mojtaba Nayyeri, Yung-Yu Chen, and Jens Lehmann. 2023. Geo-\nmetric Algebra Based Embeddings for Static and Temporal Knowledge Graph\nCompletion. IEEE Transactions on Knowledge and Data Engineering 35, 5 (2023),\n4838â€“4851.\n[42] Zezhong Xu, Peng Ye, Hui Chen, Meng Zhao, Huajun Chen, and Wen Zhang. 2022.\nRuleformer: Context-aware Rule Mining over Knowledge Graph. In Proceedings\nof the International Conference on Computational Linguistics . 2551â€“2560.\n[43] Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. KG-BERT: BERT for knowledge\ngraph completion. arXiv preprint arXiv:1909.03193 (2019).\n[44] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung\nNguyen. 2022. Are graph augmentations necessary? simple graph contrastive\nlearning for recommendation. In Proceedings of the International ACM SIGIR\nConference on Research and Development in Information Retrieval . 1294â€“1303.\n[45] Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, and Liang Wang. 2023. Learning\nlatent relations for temporal knowledge graph reasoning. In Proceedings of the\nAnnual Meeting of the Association for Computational Linguistics . 12617â€“12631.\n[46] Mengqi Zhang, Yuwei Xia, Qiang Liu, Shu Wu, and Liang Wang. 2023. Learning\nLong-and Short-term Representations for Temporal Knowledge Graph Reasoning.\nIn Proceedings of the ACM Web Conference . 2412â€“2422.\n[47] Sensen Zhang, Xun Liang, Hui Tang, and Zhenyu Guan. 2023. Hybrid Interaction\nTemporal Knowledge Graph Embedding Based on Householder Transformations.\nIn Proceedings of the ACM International Conference on Multimedia . 8954â€“8962.\n[48] Wen Zhang, Yushan Zhu, Mingyang Chen, Yuxia Geng, Yufeng Huang, Yajing\nXu, Wenting Song, and Huajun Chen. 2023. Structure Pretraining and Prompt\nTuning for Knowledge Graph Transfer. InProceedings of the ACM Web Conference .\n2581â€“2590.\n[49] Shangfei Zheng, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wei Chen,\nand Lei Zhao. 2023. DREAM: Adaptive Reinforcement Learning based on At-\ntention Mechanism for Temporal Knowledge Graph Reasoning. In Proceedings\nof the 46th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval . 1578â€“1588."
}