{
  "title": "A universal foundation model for transfer learning in molecular crystals",
  "url": "https://openalex.org/W4410561174",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4287219645",
      "name": "Minggao Feng",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2512413224",
      "name": "Chengxi Zhao",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2158165883",
      "name": "Graeme M. Day",
      "affiliations": [
        "University of Southampton"
      ]
    },
    {
      "id": "https://openalex.org/A2624604576",
      "name": "Xenophon Evangelopoulos",
      "affiliations": [
        "University of Liverpool",
        "Leverhulme Trust"
      ]
    },
    {
      "id": "https://openalex.org/A2124618049",
      "name": "Andrew I. Cooper",
      "affiliations": [
        "Leverhulme Trust",
        "University of Liverpool"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1992964622",
    "https://openalex.org/W2296119337",
    "https://openalex.org/W3185847103",
    "https://openalex.org/W2317948574",
    "https://openalex.org/W1846630191",
    "https://openalex.org/W3129006223",
    "https://openalex.org/W2591366729",
    "https://openalex.org/W4398202884",
    "https://openalex.org/W3011584298",
    "https://openalex.org/W2730352169",
    "https://openalex.org/W3193575414",
    "https://openalex.org/W2771888471",
    "https://openalex.org/W2766856748",
    "https://openalex.org/W3118519758",
    "https://openalex.org/W4399309490",
    "https://openalex.org/W2029413789",
    "https://openalex.org/W1975997599",
    "https://openalex.org/W3188464137",
    "https://openalex.org/W3037990336",
    "https://openalex.org/W2613791983",
    "https://openalex.org/W3158463686",
    "https://openalex.org/W2992843173",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4324032752",
    "https://openalex.org/W4318262876",
    "https://openalex.org/W4392359620",
    "https://openalex.org/W4388295006",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W2947423323",
    "https://openalex.org/W3160706794",
    "https://openalex.org/W4383955629",
    "https://openalex.org/W4309594747",
    "https://openalex.org/W2319902168",
    "https://openalex.org/W4287792756",
    "https://openalex.org/W3212512279",
    "https://openalex.org/W2994157490",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W176909285",
    "https://openalex.org/W3143460494",
    "https://openalex.org/W2742477205",
    "https://openalex.org/W2056647180",
    "https://openalex.org/W2015197254",
    "https://openalex.org/W3015912868",
    "https://openalex.org/W2109591817",
    "https://openalex.org/W2461510595",
    "https://openalex.org/W2397622277",
    "https://openalex.org/W2084266203",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2092419020",
    "https://openalex.org/W2032441309",
    "https://openalex.org/W1585091301",
    "https://openalex.org/W2083222334",
    "https://openalex.org/W1970127494",
    "https://openalex.org/W1981368803",
    "https://openalex.org/W2092157292",
    "https://openalex.org/W2028117546",
    "https://openalex.org/W2096747776",
    "https://openalex.org/W3021937118",
    "https://openalex.org/W3103092523"
  ],
  "abstract": "Multi-modal transfer learning for predicting and explaining universal properties of organic crystals.",
  "full_text": "A universal foundation model for transfer learning\nin molecular crystals†\nMinggao Feng,‡a Chengxi Zhao, ‡a Graeme M. Day, *b\nXenophon Evangelopoulos*ac and Andrew I. Cooper *ac\nThe physical and chemical properties of molecular crystals are a combined function of molecular structure\nand the molecular crystal packing. Speci ﬁc crystal packings can enable applications such as\npharmaceuticals, organic electronics, and porous materials for gas storage. However, to design such\nmaterials, we need to predict both crystal structure and the resulting physical properties, and this is\nexpensive using traditional computational methods. Machine-learned interatomic potential methods\noﬀer major accelerations here, but molecular crystal structure prediction remains challenging due to the\nweak intermolecular interactions that dictate crystal packing. Moreover, machine-learned interatomic\npotentials do not accelerate the prediction of all physical properties for molecular crystals. Here we\npresent Molecular Crystal Representation from Transformers (MCRT), a transformer-based model for\nmolecular crystal property prediction that is pre-trained on 706 126 experimental crystal structures\nextracted from the Cambridge Structural Database (CSD). MCRT employs four diﬀerent pre-training tasks\nto extract both local and global representations from the crystals using multi-modal features to encode\ncrystal structure and geometry. MCRT has the potential to serve as a universal foundation model for\npredicting a range of properties for molecular crystals, achieving state-of-the-art results even whenﬁne-\ntuned on small-scale datasets. We demonstrate MCRT's practical utility in both crystal property\nprediction and crystal structure prediction. We also show that model predictions can be interpreted by\nusing attention scores.\n1 Introduction\nMolecular crystals have diverse applications including phar-\nmaceuticals,1 organic electronics, 2 optical materials, 3 and\nmaterials for gas storage and separation.4–6 In all cases, the\nproperties of molecular crystals depend on the crystal packing.\nFor example, pharmaceutical molecules can have widely\ndiﬀerent solubilities depending on the crystalline form, and in\norganic electronics, charge transport is critically dependent on\ncrystal packing. However, molecular crystal packing is notori-\nously diﬃcult to predict because it is dictated by a range of weak\nintermolecular interactions, such as van der Waals forces,\naromatic pi-stacking, and hydrogen bonds.\n7 This is a major\nhurdle for digital material design because if we cannot predict\ncrystal structure then we cannot, by de nition, predict the\nfunctional properties of the crystal. To address this challenge,\ncrystal structure prediction (CSP) methods have been created to\nidentify molecular crystals with specic target functionalities.\nFor example, energy –structure–function (ESF) maps have\nguided the synthesis of various functional molecular crystals.6–10\nHowever, despite these successes, calculating the physical\nproperties for each structure on an ESF map, or even a sub-set of\nlow-energy structures, can be computationally demanding. This\nproblem is two-fold: the prediction of lattice energy, or crystal\nstability, is itself computationally expensive, and the functional\nproperty calculations are usually even more expensive. To tackle\nthis, there has been a surge of interest in machine learning (ML)\ntechniques for the rapid prediction of materials properties and\nthe elucidation of structure-property relationships\n11,12 at a frac-\ntion of the cost of rst-principles methods, such as density\nfunctional theory (DFT).\nLearning accurate representations is a crucial aspect of\nmachine learning theory that also extends to learning molecular\nrepresentations. Diﬀerent types of materials pose di ﬀerent\nchallenges when learning accurate latent representations. For\nexample, in solid-state systems it is essential to capture features\nsuch as long-range interactions and periodicity in property\nprediction tasks.\n13 This is especially challenging in organic\nmolecular crystals due to the intermolecular interactions, 7\nwhich are typically weaker than for ionic inorganic materials.\naMaterials Innovation Factory and Department of Chemistry, University of Liverpool,\nLiverpool, UK. E-mail: evangx@liverpool.ac.uk; aicooper@liverpool.ac.uk\nbSchool of Chemistry and Chemical Engineering, University of Southampton,\nSouthampton, UK. E-mail: g.m.day@soton.ac.uk\ncLeverhulme Research Centre for Functional Materials Design, Liverpool, UK\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d5sc00677e\n‡ These authors contributed equally to this work.\nCite this:Chem. Sci.,2 0 2 5 ,16, 12844\nAll publication charges for this article\nhave been paid for by the Royal Society\nof Chemistry\nReceived 24th January 2025\nAccepted 5th May 2025\nDOI: 10.1039/d5sc00677e\nrsc.li/chemical-science\n12844 | Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical\nScience\nEDGE ARTICLE\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nHence, many of the inherently local graph-based deep archi-\ntectures fail to capture global-driven properties, while tradi-\ntional ML models that use hand-craed descriptors have been\nmore successful in capturing spatial information in some\ncases.\n14,15 Hand-craed descriptors such as smooth overlap of\natomic positions (SOAP)16 and atom-centered symmetry func-\ntions (ACSFs)17 have proven eﬀective in predicting properties\nlike lattice energy, 12 while geometric descriptors such as\naccessible surface area and pore diameters have been used to\npredict the methane deliverable capacity of molecular crystals18\nas well as other global-driven properties of porous materials.19\nMore recently, persistent homology20,21 was shown to encode\nglobal molecular geometric features into machine-learned\nrepresentations.\n22 Nevertheless, calculating descriptors such\nas SOAP can be cumbersome in terms of memory footprint for\nlarger organic systems, whereas geometric descriptors tend to\noverly compress the geometric information of the crystals,\nfailing to adequately encode thene detail of the molecular\ngeometry. Another fundamental drawback of deep learning\nmodels is the need for re-training and hyperparameter re-\noptimization for each specic problem and property, adding\nfurther time and computational cost. A further challenge is the\navailability of training data: ideally, we need methods that can\nbe ne-tuned on small scale datasets, because for chemistry\nproblems, data is oen scarce and expensive.\nTransfer learning allows a model trained on one task to be\nadapted to a diﬀerent task, signicantly reducing the need for\nextensive retraining. Universality is a key aspect of a pre-trained\nmodel to allow it to capture simultaneously molecular features\nof varying modalities, as well as local and global interactions.\nRecently, pre-trained models using transformers\n23 have been\ndesigned for metal–organic frameworks (MOFs) and showed\nexceptional performance across a range of diﬀerent tasks.24–27\nTransformers enable multi-modal input integration combined\nwith self-attention layers that can process data sequences in\nparallel, allowing for much more eﬃcient training routines.\nAlso, the attention scores (AS) within the self-attention layers\ncan be used to analyse feature importance and thus oﬀer an\ninterpretability tool to gain insights on the prediction process\nitself, unlike other black-box learning systems. A leading\nexample is BERT,\n28 a pre-trained language transformer model\nthat shows state-of-the-art results across various downstream\ntasks aer being trained on large-scale data. More recently,\nvision transformers architectures (ViTs)29 have paved the way\nfor the integration of multi-modal inputs towards more\nuniversal models\n30–32 and inspired a number of recent works in\nmaterials science.24–27,33–35\nThere are two key challenges when designing the pre-\ntraining framework of a universal transformer model, namely\nthe choice of multi-modal input features and the design of pre-\ntraining tasks. The choice of appropriate input features is\ncrucial to enrich the representation capacity of the pre-trained\nmodel so it is applicable across a wide-range of tasks, while\nthe pre-training tasks should be designed carefully to eﬃciently\nbut accurately capture both local and global interactions across\nthe training set. The design of a pre-training framework is\nchallenging for organic molecular crystals that are dened by\na range of inter- and intra-molecular interactions of widely\nvarying strength and directionality, combined with geometric\ninformation about symmetry and molecular packing.\n36\nHere we introduce a foundation model focused on molecular\ncrystal structures that can be used as a universal tool for a wide\nrange of prediction tasks for materials applications that would\notherwise require time-consuming calculations. We present\na novel transformer-based pre-training framework— Molecular\nCrystal Representation from Transformers (MCRT)— that has\nbeen pre-trained on a dataset of 706 126 experimentally-\ndetermined structures sourced from the Cambridge Structural\nDatabase (CSD).\n37 MCRT accommodates multi-modal inputs\nthat encode both local and global features in conjunction with\na set of carefully designed pre-training tasks that help capture\nuniversal representations for predicting a wide range of\ndiﬀerent crystal properties, achieving state-of-the-art perfor-\nmance. We tested MCRT's performance on a range of prediction\ntasks on crystalline properties such as lattice energy, methane\ndeliverable capacity (as relevant for natural gas-powered vehi-\ncles), diﬀusivity, bulk modulus (relevant to the tabletting of\npharmaceuticals) and charge mobility (relevant in organic\nelectronics), thus demonstrating that the model can be applied\nto both porous and non-porous organic solids. We further\nexplored diﬀerent ablations of the proposed model, as well as\nits learning capacity limits under data scarcity conditions.\nImportantly, MCRT's attention-based architecture allows us to\ngain a more intuitive understanding of the structure–property\nrelationships in molecular crystals through cumulative atten-\ntion scores\n38 from across the diﬀerent layers of MCRT.\n2 Results and discussion\n2.1 Overview of pre-training framework\nThe overall framework of MCRT is illustrated in Fig. 1a. It\ncomprises a transformer encoder module that is used to build\na pre-trained model that then acts as foundation forne-tuning\non a range of downstream prediction tasks. The pre-trained\nmodel was built with universality in mind, and designed to\ndistill critical features of molecular crystals without the need for\nlabeled data and, subsequently, to extrapolate desirable phys-\nical properties across various applications aer ne-tuning.\nThis transformer is therefore designed as a multi-modal\narchitecture that processes two distinct input modalities that\nencode both local and global information: atom-based graph\nembeddings and persistence image embeddings.\n2.1.1 Atom-based graph embeddings. These are embed-\ndings taken from the penultimate layer of an ALIGNN archi-\ntecture,\n39 which performs message passing on both the\ninteratomic bond graph and its line graph corresponding to\nbond angles, thus integrating bond length and angle informa-\ntion to provide a more enriched representation of the local\nenvironments in a crystal structure. To further enhance the\npositional information of each atom and to support an eﬃcient\ntraining process, we added relative positional embeddings to\nthe atomic features (Fig. S2†), which were integrated with the\natomic features before being fed into the transformer encoder\nduring each training epoch. The positional embeddings were\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12845\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nderived by randomly perturbing the structure, a process that\nenables the model to better capture the relative positions of\natoms in the system, while also mitigating permutational\ninvariance problems.\n2.1.2 Persistence image embeddings. These embeddings\nare generated from persistent homology images\n40 and encode\nglobal structural information about each crystal structure. This\ncomplements the local information provided by the atom-based\ngraph embeddings. Persistent homology has shown potential\nrecently in capturing the topological features of porous mate-\nrials, demonstrating improved performance in adsorption\nprediction.\n41 More broadly, persistence images in crystal struc-\ntures encode topological changes as these occur when spheres\ncentered on the atoms increase their radii. These topological\nchanges can include the development of channels (1D persis-\ntence image) and voids (2D persistence image) within the\nFig. 1 Schematic overview of MCRT framework. (a) Molecular crystals are represented using local and global features, which are then fed into the\nmodel. During the pre-training phase, the model undergoes pre-training on four tasks: masked atom prediction (MAP), atom pair classiﬁcation\n(APC), crystal density prediction (CDP), and symmetry element prediction (SEP). In theﬁne-tuning phase, the model is initialised with parameters\nfrom the pre-trained model and a simple prediction head is added to train for the desired properties of molecular crystals. (b) Architecture for\npre-training MCRT. Before being fed into the model, 15% of the atoms are randomly masked, and the model is tasked with predicting the types of\nthe masked atoms based on theﬁnal atomic representations. Each atom is pre-assigned a molecular label, indicating which molecule within the\nP1 unit cell it belongs to, thus providing labels for atom pairs in the subsequent APC task. Meanwhile, SEP and CDP tasks, as global pre-training\ntasks, leverage the output of the [CLS] token representing the entire crystal for their predictions.\n12846\n| Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nstructure and can therefore have a critical eﬀect in the global\nrepresentation of the structure in downstream tasks where\ngeometric information is crucial. As exemplied in Fig. S1,† the\nexistence of channels and voids is encoded in diagrams that are\nsubsequently transformed into images that further capture the\nspatial distribution of topological features.\n40 Here we segment\npersistence images of molecular crystals into patches ofxed\nsize and feed them into the transformer encoder as an addi-\ntional modality. Further details can be found in Section 4. The\ntransformer encoder architecture that we used in our frame-\nwork was inspired by BERT,\n28 which is based on a bidirectional\ntraining strategy employing masked language modeling (MLM)\nand next sentence prediction (NSP) objectives. A [CLS] token is\nused to predict desired properties by training a multi-layer\nperception (MLP) head on it. The subsequent tokens are\nembeddings of atoms and persistence images are segmented\ninto patches, separated by a [SEP] token. At the end of each\nimage, two [SCL] tokens are added to indicate the maximum\npersistence value and maximum birth value of the persistence\nimages. This method ensures a more balanced distribution of\ndata across pixels, instead of scaling each image to a universal\nmaximum size, which could lead to the concentration of most\nof the information within just a few pixels. It also enhances the\nmodel's robustness, preventing failures when processing larger-\nscale images in future applications. The full pre-training\narchitecture is illustrated in Fig. 1b.\n2.2 Pre-training results\nTo capture universal latent representations of molecular crys-\ntals we designed four pre-training tasks performed on a dataset\nof 706 126 experimental structures sourced from the Cambridge\nStructural Database (CSD),\n37 namely a masked atom prediction\ntask (MAP), an atom pair classication task (APC), a crystal\ndensity prediction task (CDP) and a symmetry element predic-\ntion task (SEP). The MAP and APC tasks capture local chemical\ninformation, while the CDP and SEP tasks capture global\nstructure information.\n2.2.1 Masked atom prediction (MAP).The goal of the MAP\ntask is to predict the type of randomly selected masked atoms,\nwhich gives the model a deeper understanding of the various\nlocal chemical environments of atoms. Similar to the masked\nword prediction task in the BERT model, 15% of the atoms were\nmasked before being inputted into the model. Of these, 80%\nwere replaced with a [MASK] token, 10% were replaced with\nanother random atom, and the remaining 10% were le \nunchanged (Fig. 2a). This approach avoids replacing all selected\natoms with [MASK] tokens as these do not appear in down-\nstream tasks and helps mitigate the mismatch between pre-\ntraining andne-tuning. The accuracy of the MAP task on the\npre-training dataset was 99.9%.\n2.2.2 Atom pair classi cation (APC). In the APC task, the\nmodel attempts to distinguish whether a pair of atoms comes\nfrom the same molecule. This task is designed to help the\nmodel distinguish the diﬀerent molecules within a crystal cell\nand to gain a deeper understanding of the crystal structure,\nnoting that intermolecular and intramolecular interactions are\nhighly diverse— more so than for ionic, inorganic crystals.\nSpecically, for each crystal, a certain number of atom pairs are\nrandomly selected for this process ensuring that half of these\npairs come from the same molecule, and the other half from\ndiﬀerent molecules to balance bias. To ensure that the order of\natoms does not aﬀect the prediction results, the representation\nvectors of the two atoms are concatenated in both forward and\nreverse order, passed through the same prediction head, and\nthe outputs are summed to obtain thenal prediction result, as\nshown in Fig. 2b. Crystal structures were represented as graphs\nwhere disconnected sub-graphs were considered as isolated\nmolecules. If the number of atom pairs is too large, then the\ntraining process will be slowed down by the sampling process.\nConversely, if the number of pairs is too small, then the training\naccuracy will improve very slowly. It was observed empirically\nthat using 200 atom pairs per crystal gives a fair balance\nbetween training speed and accuracy. The accuracy of APC task\non the pre-training dataset was 99.9%.\n2.2.3 Crystal density prediction (CDP). Crystal density is\nlinked to the packing density of molecules and serves as a cheap\nand easy-to-obtain proxy label during pre-training. Due to the\nsignicant impact of molecular packing density on the porosity\nof molecular crystals, CDP is a particularly important task for\napplications that depend on crystal voids, such as adsorption,\nalthough it might also be expected to have correlations with\nother solid-state properties, such as charge mobility. Methane\ndeliverable capacity is one example of an adsorption property\ntask, and Fig. S5† illustrates the strong correlation between\nmethane capacity and crystal density for a hydrogen-bonded\nframework (HOF) forming molecule, T2.\n7 Similar broad corre-\nlations would be expected for other gases and other materials.\nFor the prediction of crystal density, the [CLS] token output by\nthe model was passed through a one dense layer head. The\nmean absolute error (MAE) of CDP on the pre-training dataset\nwas 0.032 g cm\n−3. For reference, 99.6% of crystals in the pre-\ntraining set have a physical density of >1 g cm −3 (average\ndensity = 1.508 g cm−3), so this is a relatively small error.\n2.2.4 Symmetry element prediction (SEP).The space group\nof a crystal can be considered as a blueprint that provides\nimportant information about its global structure. However,\na direct prediction of space group can be challenging due to the\nstrongly imbalanced distribution of space groups among crys-\ntals, where >80% of molecular crystals occupy just 5 of the 230\nexisting space groups.\n15 This class imbalance, in conjunction\nwith the complex symmetry information contained in under-\nrepresented space groups, oen hinders the learning of mean-\ningful space group representations. Instead of using space\ngroups explicitly, we focused on the less imbalanced task of\npredicting the total symmetry elements that dene each space\ngroup, which eﬀectively encodes the same foundational infor-\nmation contained in space groups. There are six types of\nsymmetry elements:\n42 inversion center, mirror plane, rotation\naxis, screw axis, rotoinversion axis, glide plane. Considering the\ncase of no symmetry elements (P1 space group), the output of\nthis task then becomes a 7-dimensional multi-hot vector (note\nthat a structure can correspond to multiple diﬀerent symmetry\nelements). For a successful prediction, all elements of the\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12847\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nprediction must match the label exactly. Pre-training the SEP\ntask on the pre-training dataset results in a prediction accuracy\nof 98.5%.\nTo validate the representation learning capacity of our\nproposed pre-training framework here, we visualised the\nlearned representations by MCRT ([CLS] tokens) of all 706 126\ncrystals in 2D using t-SNE.\n43 Since a crystal can contain multiple\ntypes of symmetry elements, we selected crystals containing\nonly one type of symmetry element and highlighted them with\nad iﬀerent colour. The low dimensional map of Fig. 2c validates\nthat crystals with similar symmetry elements cluster together. It\nis noteworthy that certain symmetry elements manifest as\nmultiple clusters in the t-SNE embeddings, which can be\nattributed to the nuanced diﬀerentiation of space groups. For\nexample, screw axes are present in several space groups, as\ndepicted in Fig. S6.† Specically, the two prominent clusters\ncorrespond to space groups 4 (P2\n1) and 19 (P212121). Although\nthe model was not explicitly trained to recognize space group\ninformation, it automatically learned and di ﬀerentiated\nbetween various space groups during the pre-training phase.\nThis capability highlights the model's inherent ability to\ncapture and classify structural features of molecular crystals\nand to comprehend underlying crystallographic principles.\nAdditionally, Fig. 2d shows a re-labeling on the same map using\ncrystal density values, where it can be seen that crystal density\nexhibits a gradient distribution within most of the symmetry\nelement clusters, indicating that the embedding vectors cluster\naccording to similar densities. Taken together, these results\nsuggest that the pre-trained model has been successfully\ntrained to capture key features of molecular crystals.\n2.3 Fine-tuning results\nNext we demonstrated the utility of our pre-training framework\nthrough a series ofne-tuning experiments on a diverse range\nof crystal property prediction tasks. These include lattice energy\nprediction, methane deliverable capacity prediction (298 K,\npressure cycle of 65–5.8 bar), methane diﬀusivity prediction\n(298 K at in nite dilution), bulk modulus prediction, and\ncharge mobility prediction, a task related to organic semi-\nconductors. We also tested MCRT's predictive performance in\nD-E tasks; that is, the lattice energy diﬀerence between DFT and\nFig. 2 Summary of pre-training tasks. (a) Scheme for masked atom prediction (MAP) task. Among the masked atoms, 80% are replaced with the\n[MASK] token, 10% are replaced with a random atom, and 10% remain unchanged. (b) Scheme for atom pair classiﬁcation (APC) prediction head.\nRepresentations of a pair of atoms are concatenated in two orders to eliminate the impact of atom sequence, ensuring more stable predictions.\n(c) The t-SNE embeddings of the [CLS] tokens of 706 126 experimental molecular crystals obtained from the pre-trained model, with crystals\ncontaining only one type of symmetry element being coloured. (d) The t-SNE embeddings of the [CLS] tokens of 706 126 experimental molecular\ncrystals obtained from the pre-trained model, with colour indicating density, and the top and bottom 5% of densities truncated for better\nvisualisation.\n12848\n| Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nforce eld accuracy calculations, to assess its extrapolation to\nhigher accuracy levels of energy prediction.\nThe datasets used in these tasks spanned a wide variety of\nmolecules. For lattice energy prediction, the dataset included\na set of 10 structurally related molecules with small changes in\nhydrogen bonding functionality, derived from earlier crystal\nstructure prediction (CSP) studies,\n6 providing a relevant test for\nne-tuning a model to study a closely related family of mole-\ncules. By contrast, theD-E task involved a deliberately diverse\nset of 1018 organic molecules, designed to develop a general-\nised ML model capable of improving lattice energy predictions\nacross broad and chemically diverse areas of molecular space.\n15\nThe charge mobility predictions focused on 7 pentacene and\nazapentacene molecules,\n10 which are compounds in organic\nsemiconductor research, while the methane capacity and\ndiﬀusivity tasks were based on CSP structures for the HOF-\nforming molecule, T2.\n7 The bulk modulus task involved 25\nsmall, organic molecules selected by farthest-point sampling\nfrom the large-scale CSP study.15 All datasets (except forD-E)\nwere randomly split with a train-validation-test ration of 80% :\n10% : 10%. TheD-E dataset was split according to the original\npaper.\n15 A detailed description of the datasets and the methods\nused for their generation can be found in Section 4.\nTable 1 reports the mean absolute error (MAE) results for\nne-tuning MCRT and its variants, compared against state-of-\nthe-art baseline models. SOAP-based random forest (RF)44 and\nkernel ridge regression (KRR), 45 graph-based CGCNN13 and\nALIGNN,39 and pre-trained crystal twins (CT)36 were selected as\nbaseline models due to their universality and competitive\nperformance in predicting materials' properties.\n12,41,46 For\na detailed description of these methods and their featurisa-\ntions, as well as the MCRT variants used in benchmarking, see\nSection 4.\nFor lattice energy prediction, three datasets of diﬀerent sizes\nwere used to validate the model's transferability capabilities\nunder limited data availability scenarios. We note here that\nLE_all includes the CSP landscapes of 10 molecules, all with\nhydrogen bonding functionality (see Fig. 5a, below), comprising\n73 779 structures in total with their associated lattice energies.\nLE_T2 represents the CSP landscape of the T2 molecule with\n8293 structures and energies, while LE_T2A corresponds to the\nCSP landscape of the T2A molecule with 1367 structures and\nenergies. Both LE_T2 and LE_T2A are subsets of LE_all.\nThe ne-tuned MCRT model outperformed all other models\nacross all tasks, demonstrating both superior predictive capa-\nbility and universality. ALIGNN exhibited better performance\ncompared to other baseline models when predicting LE_all\n(70k) and LE_T2 (8k), but its performance on LE_T2A (1k) does\nnot stand out against other models. We hypothesise that this\ncould be due to ALIGNN encoding angular information, and\nthus making the model more complex than other baseline\nmodels and more prone to overtting with insuﬃcient training\nsamples. By contrast, MCRT, with proper pre-training, still\ndemonstrates relatively good predictive performance even with\nthis small dataset of 1367 structures and energies. Graph-based\nmodels outperform SOAP descriptor-based models in predict-\ning lattice energy, a property strongly related to the local\nchemical environment of atoms. Deep models on the other\nhand perform poorly in predicting methane capacity (MC) and\ndiﬀusivity (MD) which are properties related to global structural\nfeatures. A similar observation was also conrmed by studies\nusing the MOFTransformer model.\n24 This phenomenon is\nfurther validated by the poorer performance of the purely graph-\nbased MCRTi model. When the persistence image component is\nadded, the MCRT model's performance improves signicantly,\nfurther emphasising the importance of global geometric\nfeatures in adsorption and diﬀusion predictions. Regarding the\nperformance of the non-pre-trained MCRTp, it maintains\na competitive performance, but is noticeably inferior to the pre-\ntrained MCRT. For predicting bulk modulus, non-pre-trained\nMCRTp performed worse than the descriptor-based RF model\nand graph-based models. However, aer pre-training, MCRT\nachieved the highest predictive accuracy, demonstrating the\neﬀectiveness of the pre-training strategy. The ablation model\nMCRTa used absolute positional embeddings as opposed to\nrelative ones. Despite undergoing the same pre-training stage,\nits prediction accuracy across various tasks was consistently\ninferior to that of MCRT, indicating that absolute positional\nembedding, which does not satisfy translational and rotational\ninvariance, indeed increases training diﬃculty.\nTable 1 Mean absolute error (MAE) results for theﬁne-tuned MCRT models and baseline models for a wide range of properties\nProperty (size, unit) RF KRR CGCNN ALIGNN CT MCRTp f MCRTig MCRTah MCRT\nLE_all (70k, kJ mol−1)a 7.79 6.90 5.95 2.68 4.85 3.31 2.63 2.59 2.34\nLE_T2 (8k, kJ mol−1)a 7.79 8.44 6.13 3.45 5.21 3.84 3.20 3.27 2.96\nLE_T2A (1k, kJ mol−1)a 3.34 3.96 3.20 3.27 3.19 2.98 2.60 2.57 2.15\nMC (5k, v STP/v)b 11.60 11.75 15.87 12.17 14.53 9.88 10.81 9.91 8.82\nMD (5k, 10−8 cm2 s−1)b 0.75 0.68 1.08 0.79 0.92 0.50 0.57 0.48 0.42\nCM (1k, cm2 V−1 s−1)c 0.59 0.60 0.70 0.62 0.62 0.62 0.54 0.60 0.52\nBM (6k, GPa)d 0.52 0.82 0.59 0.56 0.58 0.65 0.56 0.60 0.51\nD-E (11k, kJ mol−1)e 2.82 3.13 2.33 2.90 2.25 1.82 1.62 1.70 1.57\na LE_all (70k), LE_T2 (8k) and LE_T2A (1k) denote lattice energy with 73 779, 8293 and 1367 data points, respectively.b MC (5k) and MD (5k) denote\nmethane capacity and methane diﬀusivity with 5687 data points, respectively.c CM (1k) denotes charge mobility with 1130 data points.d BM (6k)\ndenotes bulk modulus with 6087 data points.e D-E (11k) denotes the diﬀerence in lattice energy between DFT and forceeld calculations,\ncomprising 11 458 data points.f MCRTp is MCRT without pre-training.g MCRTi is MCRT without persistence image part.h MCRTa is MCRT\nusing absolute positional embedding. The best results for each property are highlighted in bold.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12849\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nFig. 3 reports test-set results inR2 for MCRT and baseline\nmodels along with performance correlation plots of MCRT\nacross all downstream prediction tasks. From the radar plot in\nFig. 3a, MCRT scores the highestR\n2 across all prediction tasks,\nespecially in the data-scarce charge mobility (CM) dataset,\nwhere it signi cantly outperforms other models. Although\nbaseline models may perform well on speci c tasks, they\nstruggle to balance performance across diverse tasks. For\ninstance, ALIGNN excels in lattice energy prediction but shows\nonly average performance in other areas. This further highlights\nthe universality of MCRT.\nBeyond prediction errors, experimental materials\nresearchers are interested in ranking the best-performing\nstructures to prioritize as experimental targets. For lattice\nenergy predictions, structures with lower lattice energies are\nmore likely to be synthesizable. Fig. 3b, c, and d, demonstrate\nthat MCRT successfully predicts most of the lowest-energy\nstructures for the molecules considered, highlighting the\nmodel's practical utility.\nFor methane capacity (MC), a property with high computa-\ntional screening cost, MCRT successfully predicts all the top ten\nbest-performing structures, highlighting its potential for\nrobustly accelerating high-throughput computational screening\nprocedures using crystal structure prediction. Here, this may be\na better measure than MAE since we are mainly interested in the\nbest-performing crystals. An extensive analysis is included in\nTable S2,† where the class of best-performing materials in MC is\nfurther expanded to a larger subset, without however aﬀecting\nMCRT's performance, as opposed to other competitors whose\nprediction errors deteriorate substantially. Additionally, for\nmethane diﬀusivity (MD), which is a challenging property to\ncapture and predict using machine-learned interatomic poten-\ntials, MCRT still delivered the best predictive performance and\nsuccessfully identied nine out of the top ten best-performing\nFig. 3 MCRT outperforms other baseline models for all downstream prediction tasks, identifying the top few structures of interest. (a) The\ncoeﬃcient of determinationR2 on test sets of MCRT and baseline models. The prediction results on test sets ofﬁne-tuned MCRT for (b) LE_all\n(70k), (c) LE_T2 (8k) (d) LE_T2A (1k), (e) MC (5k), (f) MD (5k), (g) CM (1k), (h) BM (6k), (i),D-E (11k). Inset sub-ﬁgures are illustrations of areas of\ninterest, the yellow regions represent the areas where the topn actual values are located, while the green regions indicate the areas where the\ntop n predicted values are located. The points in the intersections represent the top n points that were successfully predicted. For MD (5k), the\nsub-ﬁgure is intended to provide a clearer visualisation of the densely populated region.\n12850 | Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nstructures. When it comes to charge mobility (CM), a property\nwith sparse data due to high computational cost, all models\nstruggle somewhat with its prediction, but MCRT still identied\nseven out of the top ten structures with highest charge mobility.\nFinally, regarding theD-E task, MCRT captures more accurately\nthe relative energy relationships across various structures. As\nshown in Fig. 3i, the corrected energies by MCRT (turquoise\npoints) align more closely with DFT-calculated results than\nthose calculated by forceelds (grey points), importantly in the\nlow-energy regime, again successfully predicting all of the top\nten structures. Further error analyses for all competitors are\nprovided in Fig. S7.†\n2.4 Interpretability\nFeature importance analysis is an inherent capability of trans-\nformer models that helps us here to better understand the\nrelationship between molecular crystal structures and their\nproperties. Cumulative attention scores of the [CLS] token were\ncalculated to measure the model's assigned attention to input\nfeatures according to their importance. An attention rollout\nstrategy\n38 was employed by recursively multiplying attention\nweights across layers, providing more focused patterns for\ninterpreting which input tokens contribute the most to the\nmodel's output. Higher attention scores indicate greater\nimportance for the model's prediction. Fig. 4 provides an\nintuitive visualisation of the explainability scores for an exper-\nimentally synthesized structure, T2-g,\n7 for methane capacity\n(le column) and lattice energy (right column), illustrating\nexplainable feature importance for both modalities.\nWhen predicting methane capacity, the model shows little\nattention toward the atomic graph modality (Fig. 4a). However,\nthe model places signicantly higher attention on persistence\nimages when predicting methane capacity than when predicting\nlattice energy (Fig. 4c/g, and d/h), as highlighted by the salmon-\ncoloured areas in the persistence images. This further validates\nthat global geometric features are more important for adsorption\npredictions, aligning withndings from previous studies.\n21,47 In\nparticular, when predicting methane capacity, the model places\nparticularly high attention to the 1D persistence image, which\nencodes information about the pores in the crystal. The [SCL]\ntoken indicating the largest persistence value is identied as the\nmodel's most signicant feature, receiving an attention score far\nexceeding those of other image patches. The persistence value\nrepresents the radius of the largest sphere that can pass through\nthe topological object, eﬀectively corresponding to the pore\nradius. Subsequently we mapped the topological objects within\nthe patches near the largest persistence back to the representative\ncycles in the original T2-g structure. The objects near the largest\npersistence value correspond to the large pores in T2-g as shown\nin Fig. 4b. The largest persistence value thus contains essential\ninformation about the pore size, which is crucial for predicting\nadsorption properties.\n48\nFor lattice energy predictions, the model exhibits marked\nattention to the hydrogen-bonding benzimidazole groups of the\nT2 molecule (Fig. 4e). To evaluate the accuracy of the chemical\ninsights provided by these attention scores, an electron density\ndiﬀerence (EDD) analysis was performed on T2-g,a ss h o w ni n\nFig. 4f. The yellow isosurfaces represent regions with increased\nelectron density, while the blue isosurfaces indicate regions with\ndecreased electron density. Normally, a larger magnitude of\nelectron density shi indicates stronger intermolecular interac-\ntions. Notably, expanding to a supercell, we observe strong\nattention in regions corresponding to the areas of intense inter-\nmolecular interactions (Fig. S8d†). A similar phenomenon can\nalso be observed in other experimental T2 polymorphs, as shown\nin Fig. S9 and S10.† This suggests that the model's attention\naligns with key regions of strong intermolecular interactions,\nwhich play a crucial role in stabilising the crystal structure. Given\nthat subtle changes in these strong interactions can lead to\nsignicant diﬀerences in lattice energy,\n7 the ne-tuned model\nappears to have eﬀectively captured the critical chemical features\nnecessary for accurate lattice energy predictions.\n2.5 Few-shot learning\nData is oen the limiting resource both in practical synthetic\nchemistry and in computational materials studies due to the\nhigh costs of synthetic or computational methods. Here we\nexplored our model's extrapolation capabilities on extremely\nsmall datasets. Speci cally, we tested MCRT's predictive\nperformance in zero- and few-shot learning\n49 scenarios for\npredicting lattice energies of T2 structures, using datasets\ncontaining analogues of T2 to assess its generalization capa-\nbility within a related molecular family.\nWe rst formed a test set using all T2-based structures\ncontained in LE_all (a dataset comprising CSP landscapes of T2\nand analogues of T2). The remaining structures were then\nrandomly divided into training and validation sets in\na 90%:10% ratio tone-tune MCRT. Subsequently, from the T2\nstructures, we sequentially separated 100, 200, 300,.,u pt o\n1000 structures to furtherne-tune the model obtained from\nthe previous step. The remaining structures were used as a test\nset to assess the robustness of MCRT predictions. To provide\na clear performance comparison, we conducted the same\nexperiment using ALIGNN, the most competitive baseline\nmodel for lattice energy prediction (Table 1). Fig. 5a illustrates\nour training setup and Fig. 5b presents the MAE,R\n2 and top-10\nprediction performances for MCRT and ALIGNN. In the zero-\nshot scenario, MCRT exhibits signicantly higher prediction\naccuracy compared to ALIGNN, indicating that aer learning\nfrom similar structures, MCRT can generalize more eﬀectively\nto related but unseen structures. Furthermore, with just an extra\nstep of furtherne-tuning on 100 structures, MCRT scores a low\nprediction MAE (4.34 kJ mol\n−1). By contrast, even aer being\ntrained on 10 times more data (i.e., 1000 vs. 100 structures),\nALIGNN still fails to achieve MCRT's extrapolation capacity,\nwith a prediction MAE of 6.40 kJ mol\n−1. This was further echoed\nby a series of energy-density landscape reconstruction tasks on\nthe diﬀerent ne-tuned MCRT models. As shown in Fig. S15,†\nMCRT accurately reproduces the relative positions of the four\nexperimental porous structures within the energy-density\nlandscape even with zero-shot learning, whereas ALIGNN fails\nto e ﬀectively capture these relative positions, particularly\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12851\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nFig. 4 Attention scores for atom-based embeddings and persistence image embeddings for a porous framework, T2-g. (a) Unit cell, (c) 1D and\n(d) 2D persistence images of T2-g with attention scores for CH4 capacity. (b) The point cloud of atoms in T2-g, with red lines representing the\nrepresentative cycles corresponding to the topological objects in the persistence image. (e) Unit cell, (g) 1D and (h) 2D persistence images of T2-\ng with attention scores for lattice energy. (f) Electron density diﬀerence plot of T2-g highlighting the region of intermolecular interactions (yellow\nisosurfaces = increased electron density; blue isosurfaces= decreased electron density). The strong intermolecular interactions are found for\nthe atoms that were attended to in e, also for other experimental T2 polymorphs (Fig. S9 and S10†). In the unit cells, the atomic size is\nproportional to normalized attention scores, with scores less than 0.005 being clipped to avoid extremely small atoms (colour code: C, cyan; H,\nwhite; N, blue; O, red). In the persistence images, the 10 patches with the highest attention scores are visualized with a salmon-coloured overlay,\nwhere stronger intensities represent higher attention scores.\n12852\n| Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nmisplacing T2-d by conating it with numerous other struc-\ntures, as illustrated in Fig. S16. † This further underscores\nMCRT's practicality for computationally costly scenarios such\nas crystal structure prediction.\n3 Conclusions\nWe present a new universal transformer model, MCRT, together\nwith a pre-training framework for predicting a wide range of\nphysical properties for molecular crystals. We have designed\na multi-modal architecture that has the capacity to\ncomprehensively learn both local and global representations of\nmolecular crystals. This ensures universal transferability for\nMCRT across diﬀerent tasks and structures, at least for the tasks\nattempted here. We tested MCRT's predictive performance by\nne-tuning it on various diverse properties such as lattice\nenergy, methane capacity and diﬀusivity, bulk modulus, as well\nas charge mobility. Our proposed model both outperformed\ncurrent state-of-the-art models and showed strong general-\nisability performance in limited data availability scenarios. This\nhighlights the practical utility in robustly accelerating materials\ndiscovery — for example, by rapidly estimating crystal structure\nFig. 5 Few-shot learning of lattice energies for 10 related hydrogen-bonding molecules. (a) Chemical structures of T2 and 9 other related\nhydrogen-bonding molecules, including 4 triptycene framework candidates (TH1–TH4) and 5 small, monoaromatic molecules with the same\nrepresentative hydrogen bonding functionalities. In the few-shot learning experiment, the T2 structures in the LE_all dataset were extracted and\nused as the test set. The remaining structures of other 9 molecules were used for training and validation of MCRT, resulting in aﬁne-tuned MCRT\nmodel, yielding the zero-shot prediction results (sampled structures= 0). Subsequently, a small number of T2 structures (100–1000) was\nrandomly sampled as training and validation sets to furtherﬁne-tune this model. The remaining T2 structures and associated lattice energies\nwere used as the test set to evaluate the few-shot performance of MCRT. (b) The prediction results of the few-shot learning experiments of\nMCRT and ALIGNN. Sampled structures refers to the number of T2 structures and associated lattice energies extracted as the training and\nvalidation set during few-shot learning scenarios. TOP10 represents the number of correctly predicted structures among the 10 lowest-energy\nstructures.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12853\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nprediction energy landscapes based on landscapes calculated\nfor related molecules. Our MCRT model also provides insights\ninto structure–property relationships for molecular crystals\nthrough its permeable, interpretable architecture design. While\nsuch interpretability should not be equated to causal, physical\nunderstanding, it is striking that the attention scores in the\nMCRT model correlate so strongly with the key intermolecular\ninteractions in lattice-energy prediction tasks, at least for\nmolecules such as T2 that feature dominant hydrogen-bonding\npatterns (Fig. 4e, f and S9–S11†).\nMCRT performs well both in predicting properties across\na broad range and in highlighting the top few‘best-performing’\ncrystals (Fig. 3b–h). From a practical perspective, both of these\ntasks are important since there is very oen a trade-oﬀ between\ndiﬀerent properties for real applications. To give just one example,\nto design materials for methane storage, we need to predict\nstructures that have low lattice energies— that is, materials that\nwill be formed in experiments— while having methane capacities\nthat are high enough, rather than simply identifying the hypo-\nthetical crystals that absorb the most methane. Both properties,\nlattice energy and methane capacity, are expensive to calculate.\nThere are other important physical properties relevant to methane\nstorage materials, not investigated here, such as mechanical\nstability and thermal conductivity, which would add further\ncomputational cost to a digital materials screening programme.\nAs such, the development of universal, inexpensive prediction\ntools is a key priority in computational materials design. We\nbelieve that MCRT can serve as a foundational infrastructure for\nthe molecular crystal research community, aiding us in the\naccelerated exploration of the vast space of molecular crystals.\n4 Methods\n4.1 Pre-training datasets\nTo ensure high-quality crystal structures for the pre-training\ndataset of MCRT, we selected 706 126 molecular structures\nfrom the Cambridge Structural Database (CSD) database,\n37 pre-\nltered to satisfy the following criteria: (i) only structures with\nfully determined three-dimensional coordinates were included to\nensure comprehensive spatial information; (ii) only structures\nwith anR factor of 0.1 or less were included to ensure high-quality\nrenement and accuracy of the crystal structures; (iii) structures\nexhibiting any form of disorder were excluded to avoid compli-\ncations in subsequent analysis and to maintain data consistency;\n(iv) only structures without reported errors were included; (v) we\nexcluded polymeric structures, such as metal–organic frame-\nworks, focusing solely on discrete molecular crystals; (vi) only\nsingle crystal structures were considered, ensuring higher preci-\nsion in the determination of atomic positions. This robust pre-\nltering process was crucial to ensure the robustness and reli-\nability of our subsequent analysis and training.\n4.2 Materials analysis\nFor the manipulation and labeling of the collected pre-training\nset, we used the Python Materials Genomics (pymatgen)\nlibrary.\n50 In particular, for the APC task, the crystal structures\nwere represented as graphs, where disconnected sub-graphs were\nconsidered as isolated molecules, for the SEP task the space\ngroups of the crystals wererst identied and subsequently were\nmapped to their corresponding symmetry elements. For the\nremaining tasks the label generation was straightforward using\npymatgen. Before being inputted into the model, the crystals were\nconverted into theP1 space group to ensure the feasibility of\nsubsequent SEP and APC tasks during pre-training phase. For the\npersistence image generation, we used MoleculeTDA\n21 to\ncompute persistence images with a resolution of 50× 50 and\na spread of 0.15, consistent with previous studies.51 For the t-SNE\nembedding, we used a perplexity parameter of 50, due to the large\nsize of the pre-training dataset.\n4.3 Fine-tuning data collection\nWe ne-tuned MCRT on diverse properties of diﬀerent molec-\nular crystals to validate the generality of MCRT's predictive\ncapabilities. The details of the datasets are as follows.\n4.3.1 Lattice energy. Lattice energy calculations were per-\nformed with an anisotropic atom–atom potential using DMA-\nCRYS.\n52 Electrostatic interactions were modelled using an atomic\nmultipole description of the molecular charge distribution (up to\nhexadecapole on all atoms) from the B3LYP/6-311G(d,p)-\ncalculated charge density using a distributed multipole anal-\nysis.\n53 Atom–atom repulsion and dispersion interactions were\nmodelled using a revised Williams intermolecular potential,54\nwhich has been benchmarked against accurate, experimentally\ndetermined lattice energies for a range of molecular crystals.\n55 We\nspecically generated threene-tuning datasets of diﬀerent size\nto test MCRT's predictive capacity on limited data availability\nscenarios, namely LE_all, a dataset with 73 779 structures\ncomposed of CSP landscapes on all the molecules listed in\nFig. 5a,LE_T2, the CSP landspace of T2 with 8293 structures and\nLE_T2A, the CSP landspace of T2A with 1367 structures.\n4.3.2 CH\n4 deliverable capacity. A dataset of 5687 T2-based\nstructures with calculated CH4 deliverable capacity (298 K, 65–\n5.8 bar) was directly retrieved from the previous work.7\n4.3.3 CH 4 diﬀusivity. A dataset of 5687 T2-based structures\nwith calculated CH4 diﬀusion coeﬃcients at innite dilution\nusing the MD simulations. The simulations were conducted at\n298 K with a time step of 1 fs for a total of 5 million cycles, with\n1000 cycles used for the initialization and 10 000 cycles for\nequilibration. DREIDING forceeld was used with the Lorentz–\nBerthelot mixing rule and a cut-oﬀ distance of 13 Å. The CH\n4\nmolecule was modeled as a single atom. Prior to the simula-\ntions, 30 CH4 molecules were randomly introduced into the\npores of crystals. The mean square displacement (MSD) of gas\nmolecules during 1–5 ns is used to calculate the di ﬀusion\ncoeﬃcient through Einstein's relation.\n53 All these simulations\nwere carried out at NVT ensemble using RASPA2 package.56\n4.3.4 Charge mobility. The charge carrier mobility values in\nthis dataset were obtained from previous work10 and were calcu-\nlated using the Marcus theory of charge transport. The dataset is\nbased on crystal structure prediction (CSP) studies, with the\nstudied molecules including pentacene and azapentacenes. The\ncharge carrier mobility calculations were restricted to crystal\n12854 | Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nstructures within a 7 kJ mol −1 energy range of the global\nminimum on the energy-density landscapes, capturing low-\nenergy polymorphs most likely to be observed experimentally.\n4.3.5 Bulk modulus. Bulk modulus calculations were per-\nformed for sets of predicted crystal structures of 25 small,\norganic molecules from a recent large-scale CSP study.\n15 To\ninclude a diverse set of molecules, these were selected by\nfarthest-point sampling of the 1007 molecules from\n15 using\nEuclidean distances between 1024 bit extended connectivity\nngerprints57 for evaluating molecular similarity. Calculations\nwere performed for all crystal structures within 12 kJ mol−1 of\nthe global energy minimum on each CSP landscape. Bulk\nmoduli were calculated from a Voigt–Reuss–Hill average over\nelastic sti ﬀness and elastic compliance tensors. Elastic\nconstants were calculated using rigid-molecule calculations in\nthe DMACRYS soware\n52,58 with intermolecular interactions\nmodelled using the FIT 59 exp-6 atom–atom force eld and\natomic multipole electrostatics from Distributed Multipole\nAnalysis60 of B3LYP/6-311G ** calculated molecular charge\ndensities. Any crystal structures not satisfying Born stability\ncriteria were removed from the dataset, resulting in anal set of\n6087 crystal structures.\n4.3.6 D-E. The training target, D-E, represents the lattice\nenergy diﬀerence between DFT (B86bPBE + XDM) and force\neld (FIT + DMA) accuracy. Following the approach in the\noriginal paper,\n15 which includes 1000 CSP landscapes, we split\nthe dataset by selecting 10 crystal structures from each of\naround 900 landscapes for training and validation, while about\n100 landscapes, with 10 structures each, were reserved as a test\nset. An exclusion of duplicate structures was applied that led to\na nal dataset of 11 458 structures.\n4.4 Training details\nFor pre-training, we randomly split the 706 126 molecular\ncrystal dataset with a train-validation ration of 90%:10%.\nConsistent with BERT\nBASE,28 the transformer encoder in MCRT\nadopts L = 12, H = 768, and A = 12, where L represents the\nnumber of layers,H the hidden size, andA the number of self-\nattention heads. The model was trained for 50 epochs with\na batch size of 512. Due to the large pre-training dataset, we\nselected a relatively large batch size to ensure stable gradient\nupdates. The decision to train for 50 epochs was empirically\ndetermined from MCRT's training curves (reported in Fig. S4†),\nwhich indicated that the model training converged aer 40\nepochs. The AdamW optimizer with a learning rate of 10\n−4 and\nweight decay of 10−2 was used.37 The learning rate was warmed\nup during therst 5% of the total epoch and was then linearly\ndecayed to zero for the remaining epochs. For the SEP task we\nassigned higher weights to the elements with fewer occurrences\ndue to the great variance in the frequency of the occurrence of\ndiﬀerent symmetry elements. The individual weights are\ncalculated as follows:\nwðx\niÞ¼ 1\nlnð3 þ xiÞ (1)\nwhere w(xi) is the weight of elementi, xi is the frequency of\nelement i and e (e $ 1) is a parameter to adjust the weight\ndistribution which was set to 1.1 to avoid extremely large\nweights. The resolution of the persistence image during\ntraining was set to 50× 50 with a patch size of 5× 5 in accor-\ndance to previous studies.\n21,51\nFor ne-tuning, all datasets (except forD-E) were randomly\nsplit with a train-validation-test ration of 80%:10%:10%. TheD-E\ndataset was split according to the original paper.15 By initializing\na single dense layer to the [CLS] token, all model weights arene-\ntuned to predict desired properties for 50 epochs with a batch size\nof 32. All other settings are the same as in the pre-training step.\n4.5 Baselines and ablations\nWe test the prediction performance of MCRT against a wide\nrange of baselines and state-of-the-art methods. These include.\n– Random forest (RF): A robust ensemble learning algorithm\nthat aggregates the predictions of multiple decision trees, typically\nleading to enhanced generalisation performance by reducing\novertting and variance in predictive modeling tasks.\n44\n– Kernel Ridge Regression (KRR): KRR integrates ridge\nregression with the kernel trick, enabling it to perform\nnonlinear regression in high-dimensional feature spaces while\ncontrolling for model complexity through regularisation.\n45\n– Crystal Graph Convolutional Neural Network (CGCNN):\nCGCNN represents crystalline materials as graphs, where atoms\nserve as nodes and bonds as edges, and learns material prop-\nerties by applying convolutional operations over the graphs.\n13\n– Atomistic Line Graph Neural Network (ALIGNN): ALIGNN\nenhances conventional graph neural networks by incorporating\nbond angle information from line graphs, thereby improving\nthe model's capability to predict complex material properties\nwith higher accuracy.\n39\n– Crystal Twins (CT): CT is a self-supervised pre-trained\nmodel for crystalline material property prediction, using twin\nCGCNNs to learn robust representations from large unlabeled\ndatasets, which are thenne-tuned for specic tasks.\n36\nFor descriptor-based models, the Smooth Overlap of Atomic\nPositions (SOAP) descriptor was used due to its universality.16\nThe parameters for the SOAP descriptor were set as follows:\na cutoﬀ for the local region of 4.0 Å, 6 radial basis functions, and\na maximum degree of spherical harmonics of 6. RF and KRR\nimplemented in scikit-learn\n61 were adopted, and the hyper-\nparameters were tuned using grid search. For RF, the number of\ntrees was searched from 10 to 1000. For KRR, the regularization\nstrength u was searched from 0.001 to 100. For Graph Neural\nNetworks (GNNs), CGCNN was trained with the following\nhyperparameters: 32 batch size, 100 epochs, 5 message passing\nlayers, 1 hidden layer aer pooling, 64 hidden atom features in\nmessage passing layers. ALIGNN was trained with the following\nhyperparameters: 32 batch size, 100 epochs, 4 message passing\nlayers, 1 hidden layer aer pooling, 256 hidden atom features in\nmessage passing layers, in line with the original paper.\n39 For the\ncrystal twins pre-trained model (CT), the same ne-tuning\nhyperparameters as the original paper were used (128 batch\nsize, 200 epochs, 3 message passing layers).\n36\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12855\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nAdditionally, the following variants of MCRT were included\nin thene-tuning comparisons to assess the importance of the\ndiﬀerent learning components of the proposed framework.\n– MCRTp: The complete architecture of MCRT used directly\nfor prediction without pre-training.\n– MCRTi: The architecture of MCRT without persistence\nimage modality input module.\n– MCRTa: The architecture of MCRT using absolute posi-\ntional embeddings instead of relative ones.\nBoth MCRTi and MCRTa underwent the same pre-training\nprocess as MCRT. For the latter, the absolute positional input\nfeatures are processed as in ref. 27, that is by employing BERT's\nnative positional embedding module to embed each of the\nthree-dimensional atomic coordinates separately, and eventu-\nally summing them up.\n4.6 The electron density diﬀerence analysis\nPeriodic DFT calculations, including the electron density calcula-\ntion, were carried out within the plane-wave pseudopotential\nformalism, using the Viennaab initiosimulation package (VASP)\ncode version 5.4.4.\n62 Projector augmented-wave (PAW) method was\napplied to describe the electron-ion interactions.63 Generalized\ngradient approximation (GGA) with the Perdew–Burke–Ernzerhof\n(PBE) exchange-correlation functional was adopted to treat elec-\ntron interaction energy. 64 Grimme's semi-empirical DFT-D3\nscheme with Becke–Johnson damping functions was used here\nto give a better description of interactions.65–67 A kinetic-energy cut-\noﬀ of 600 eV was used to dene the plane-wave basis set. The\nelectronic Brillouin zone was integrated with the smallest allowed\nspacing between k-points (KSPACING) being 0.4 Å\n−1,a n dt h e\ngenerated grid was centered at theG-point. The convergence\nthreshold for self-consistency was set to 10−6 eV during total\nenergy and force calculations.\nThe electron density diﬀerence (EDD) plots were generated\nby subtracting the electron densities of each isolated molecule\nfrom the electron density of the entire crystal:\nDr ¼ rcrystal /C0\nXN\ni¼1\nrmoleculei (2)\nwhere rcrystal is the electron density of the crystal, andrmoleculei\nrepresents the electron density of thei-th isolated molecule in\nthe crystal.\nData availability\nSource data and datasets used in this work, including reference\ncodes of molecular crystals screened from the CSD, are available\nvia Figshare at https://doi.org/10.6084/m9.gshare.27844302.\nAdditionally, we provide pre-trained MCRT model and ne-\ntuned versions for all datasets, accessible via Figshare at\nhttps://doi.org/10.6084/m9.gshare.27822705. The MCRT\nlibrary is available at https://github.com/fmggggg/MCRT. For\nease of use, pre-dened Apptainer images are available on\nFigshare at https://doi.org/10.6084/m9.gshare.26390275.T o\nensure reproducibility, all results in this paper are obtained\nfrom version 1.0.2 of the MCRT library, which is available at\nhttps://pypi.org/project/MCRT-tools/1.0.2.\nAuthor contributions\nM. F, C. Z and X. E conceived the project, and X. E, G. M. D and\nA. I. C supervised the project. M. F and C. Z performed the\ncomputational experiments and X. E, G. M. D and A. I. C ana-\nlysed the output of the experiments. All authors contributed to\nwriting the manuscript.\nConﬂicts of interest\nThe authors declare no competing interests.\nAcknowledgements\nThis project has received funding from the European Research\nCouncil under the European Union's Horizon 2020 research\nand innovation program (grant agreement no. 856405). X. E.\nand A. I. C. acknowledgenancial support from the Leverhulme\nTrust via the Leverhulme Research Centre for Functional\nMaterials Design. C. Z. acknowledges thenancial support from\nthe China Scholarship Council (No. 202106745008). A. I. C.\nthanks the Royal Society for a Research Professorship\n(RSRP\\S2\\232003). The authors would also like to thank Dmitriy\nMorozov, Maciej Haranczyk, Christopher R. Taylor, Jay Johal\nand Tao Liu for useful discussions. The authors acknowledge\nthe use of the IRIDIS High Performance Computing Facility,\nand associated support services at the University of South-\nampton, in the completion of this work.\nReferences\n1 N. Qiao, M. Li, W. Schlindwein, N. Malek, A. Davies and\nG. Trappitt, Pharmaceutical Cocrystals: An Overview,Int. J.\nPharm., 2011,419,1 –11.\n2 A. Punzi, M. a. M. Capozzi, V. Fino, C. Carlucci, M. Suriano,\nE. Mesto, E. Schingaro, E. Orgiu, S. Bonacchi, T. Leydecker,\nP. Samor`ı, R. Musio and G. M. Farinola, Croconaines as\nMolecular Materials for Organic Electronics: Synthesis,\nSolid State Structure and Use in Transistor Devices, J.\nMater. Chem. C, 2016,4, 3138–3142.\n3 T. Feiler, B. Bhattacharya, A. A. L. Michalchuk, S.-Y. Rhim,\nV. Schröder, E. List-Kratochvil and F. Emmerling, Tuning\nthe Mechanical Flexibility of Organic Molecular Crystals by\nPolymorphism for Flexible Optical Waveguides,\nCrystEngComm, 2021,23, 5815–5825.\n4 W. Li, J. Zhang, H. Guo and G. Gahungu, Adsorption of Gases\nin Microporous Organic Molecular Crystal, a Multiscale\nComputational Investigation, J. Phys. Chem. C, 2011, 115,\n4935–4942.\n5 T.-H. Chen, W. Kaveevivitchai, A. J. Jacobson and\nO. ˇS. Miljani ´c, Adsorption of Fluorinated Anesthetics\nwithin the Pores of a Molecular Crystal,Chem. Commun.,\n2015, 51, 14096–14098.\n12856 | Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n6 C. Zhao, L. Chen, Y. Che, Z. Pang, X. Wu, Y. Lu, H. Liu,\nG. M. Day and A. I. Cooper, Digital Navigation of Energy–\nStructure–Function Maps for Hydrogen-Bonded Porous\nMolecular Crystals,Nat. Commun., 2021,12, 817.\n7 A. Pulido, L. Chen, T. Kaczorowski, D. Holden, M. A. Little,\nS. Y. Chong, B. J. Slater, D. P. McMahon, B. Bonillo,\nC. J. Stackhouse, A. Stephenson, C. M. Kane, R. Clowes,\nT. Hasell, A. I. Cooper and G. M. Day, Functional Materials\nDiscovery Using Energy–Structure–Function Maps, Nature,\n2017, 543, 657–664.\n8M .O ’Shaughnessy, J. Glover, R. Hazi, M. Barhi, R. Clowes,\nS. Y. Chong, S. P. Argent, G. M. Day and A. I. Cooper, Porous\nIsoreticular Non-Metal Organic Frameworks, Nature, 2024,\n630, 102–108.\n9 C. M. Aitchison, C. M. Kane, D. P. McMahon,\nP. R. Spackman, A. Pulido, X. Wang, L. Wilbraham,\nL. Chen, R. Clowes, M. A. Zwijnenburg, R. Sebastian\nSprick, M. A. Little, G. M. Day and A. I. Cooper,\nPhotocatalytic Proton Reduction by a Computationally\nIdentied, Molecular Hydrogen-Bonded Framework, J.\nMater. Chem. A, 2020,8, 7158–7170.\n10 J. E. Campbell, J. Yang and G. M. Day, Predicted Energy–\nStructure–Function Maps for the Evaluation of Small\nMolecule Organic Semiconductors,J. Mater. Chem. C, 2017,\n5, 7574–7584.\n11 Y. Han, I. Ali, Z. Wang, J. Cai, S. Wu, J. Tang, L. Zhang, J. Ren,\nR. Xiao, Q. Lu, L. Hang, H. Luo and J. Li, Machine Learning\nAccelerates Quantum Mechanics Predictions of Molecular\nCrystals, Phys. Rep., 2021,934,1 –71.\n12 F. Musil, S. De, J. Yang, J. E. Campbell, G. M. Day and\nM. Ceriotti, Machine Learning for the Structure–Energy–\nProperty Landscapes of Molecular Crystals, Chem. Sci. ,\n2018, 9, 1289–1300.\n13 T. Xie and J. C. Grossman, Crystal Graph Convolutional\nNeural Networks for an Accurate and Interpretable\nPrediction of Material Properties, Phys. Rev. Lett. , 2018,\n120, 145301.\n14 V. Fung, J. Zhang, E. Juarez and B. G. Sumpter,\nBenchmarking Graph Neural Networks for Materials\nChemistry, npj Comput. Mater., 2021,7,1 –8.\n15 C. R. Taylor, P. W. V. Butler and G. M. Day, Predictive\nCrystallography at Scale: Mapping, Validating, and\nLearning from 1,000 Crystal Energy Landscapes, Faraday\nDiscuss.\n, 2025,256, 434–458.\n16 A. P. Bart´ok, R. Kondor and G. Cs´anyi, On Representing\nChemical Environments,Phys. Rev. B, 2013,87, 184115.\n17 J. Behler, Atom-Centered Symmetry Functions for\nConstructing High-Dimensional Neural Network\nPotentials, J. Chem. Phys., 2011,134, 074106.\n18 E. O. Pyzer-Knapp, L. Chen, G. M. Day and A. I. Cooper,\nAccelerating Computational Discovery of Porous Solids\nthrough Improved Navigation of Energy-Structure-Function\nMaps, Sci. Adv., 2021,7, eabi4763.\n19 J. Townsend, C. P. Micucci, J. H. Hymel, V. Maroulas and\nK. D. Vogiatzis, Representation of Molecular Structures\nwith Persistent Homology for Machine Learning\nApplications in Chemistry,Nat. Commun., 2020,11, 3230.\n20 Y. Lee, S. D. Barthel, P. Dłotko, S. M. Moosavi, K. Hess and\nB. Smit, Quantifying Similarity of Pore-Geometry in\nNanoporous Materials,Nat. Commun., 2017,8, 15396.\n21 A. S. Krishnapriyan, J. Montoya, M. Haranczyk,\nJ. Hummelshøj and D. Morozov, Machine Learning with\nPersistent Homology and Chemical Word Embeddings\nImproves Prediction Accuracy and Interpretability in Metal-\nOrganic Frameworks,Sci. Rep., 2021,11, 8888.\n22 P. G. Boyd, A. Chidambaram, E. Garc´ıa-D´ıez, C. P. Ireland,\nT. D. Da ﬀ, R. Bounds, A. G ładysiak, P. Schouwink,\nS. M. Moosavi, M. M. Maroto-Valer, J. A. Reimer,\nJ. A. R. Navarro, T. K. Woo, S. Garcia, K. C. Stylianou and\nB. Smit, Data-Driven Design of Metal–Organic Frameworks\nfor Wet Flue Gas CO\n2 Capture, Nature, 2019,576, 253–256.\n23 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser and I. Polosukhin, Attention Is All\nYou Need, arXiv, 2017, preprint, arXiv.1706.03762, DOI:\n10.48550/arXiv.1706.03762.\n24 Y. Kang, H. Park, B. Smit and J. Kim, A Multi-Modal Pre-\nTraining Transformer for Universal Transfer Learning in\nMetal–Organic Frameworks, Nat. Mach. Intell. , 2023, 5,\n309–318.\n25 Z. Cao, R. Magar, Y. Wang and A. B. Farimani, MOFormer:\nSelf-Supervised Transformer Model for Metal –Organic\nFramework Property Prediction, J. Am. Chem. Soc., 2023,\n145, 2958–2967.\n26 J. Wang, J. Liu, H. Wang, M. Zhou, G. Ke, L. Zhang, J. Wu,\nZ. Gao and D. Lu, A Comprehensive Transformer-Based\nApproach for High-Accuracy Gas Adsorption Predictions in\nMetal-Organic Frameworks,Nat. Commun., 2024,15, 1904.\n27 J. Cui, F. Wu, W. Zhang, L. Yang, J. Hu, Y. Fang, P. Ye,\nQ. Zhang, X. Suo, Y. Mo, X. Cui, H. Chen and H. Xing,\nDirect Prediction of Gas Adsorption via Spatial Atom\nInteraction Learning,Nat. Commun., 2023,14, 7043.\n28 J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language\nUnderstanding, Proc. of the 2019 Conf. of the North\nAmerican Chapter of the Assoc. for Computational Linguistics:\nHuman Language Technologies, 2019, vol. 1, pp. 4171–4186.\n29 A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, J. Uszkoreit and N. Houlsby, An Image\nIs Worth 16x16 Words: Transformers for Image\nRecognition at Scale, arXiv, 2021, preprint,\narXiv.2010.11929, DOI:10.48550/arXiv.2010.11929.\n30 A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford,\nM. Chen and I. Sutskever, Zero-Shot Text-to-Image\nGeneration, 38th Int. Conf. on Mach. Learn. Res., 2021, vol.\n139, pp. 8821–8831.\n31 J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson,\nK. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring,\nE. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei,\nM. Monteiro, J. L. Menick, S. Borgeaud, A. Brock,\nA. Nematzadeh, S. Sharifzadeh, M. Bi´nkowski, R. Barreira,\nO. Vinyals, A. Zisserman and K. Simonyan, Flamingo:\na Visual Language Model for Few-Shot Learning, Adv. in\nNeural Inf. Process. Syst., 2022,35, 23716–23736.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12857\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n32 L. Yuan, D. Chen, Y.-L. Chen, N. Codella, X. Dai, J. Gao,\nH. Hu, X. Huang, B. Li, C. Li, C. Liu, M. Liu, Z. Liu, Y. Lu,\nY. Shi, L. Wang, J. Wang, B. Xiao, Z. Xiao, J. Yang,\nM. Zeng, L. Zhou and P. Zhang: A New Foundation Model\nfor Computer Vision, arXiv, 2021, preprint,\narXiv.2111.11432, DOI:10.48550/arXiv.2111.11432.\n33 P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter,\nC. Bekas and A. A. Lee, Molecular Transformer: A Model\nfor Uncertainty-Calibrated Chemical Reaction Prediction,\nACS Cent. Sci., 2019,5, 1572–1583.\n34 D. Kreutter, P. Schwaller and J.-L. Reymond, Predicting\nEnzymatic Reactions with a Molecular Transformer,Chem.\nSci., 2021,12, 8648–8659.\n35 C. Kuenneth and R. Ramprasad, polyBERT: A Chemical\nLanguage Model to Enable Fully Machine-Driven Ultrafast\nPolymer Informatics,Nat. Commun., 2023,14, 4099.\n36 R. Magar, Y. Wang and A. B. Farimani, Crystal Twins: Self-\nSupervised Learning for Crystalline Material Property\nPrediction, npj Comput. Mater., 2022,8,1 –8.\n37 C. R. Groom, I. J. Bruno, M. P. Lightfoot and S. C. Ward, The\nCambridge Structural Database,Acta Cryst. B, 2016,72, 171–\n179.\n38 S. Abnar and W. Zuidema, Quantifying Attention Flow in\nTransformers, arXiv, 2020, preprint, arXiv.2005.00928, DOI:\n10.48550/arXiv.2005.00928.\n39 K. Choudhary and B. DeCost, Atomistic Line Graph Neural\nNetwork for Improved Materials Property Predictions, npj\nComput. Mater., 2021,7,1 –8.\n40 H. Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson,\nP. Shipman, S. Chepushtanova, E. Hanson, F. Motta and\nL. Ziegelmeier, Persistence Images: A Stable Vector\nRepresentation of Persistent Homology, J. Mach. Learn.\nRes., 2017,18,1 –35.\n41 S. M. Moosavi, H. Xu, L. Chen, A. I. Cooper and B. Smit,\nGeometric Landscapes for Material Discovery within\nEnergy–Structure–Function Maps, Chem. Sci. , 2020, 11,\n5423–5433.\n42 M. Glazer, G. Burns and A. N. Glazer,Space Groups for Solid\nState Scientists. Elsevier, 2012.\n43 L. van der Maaten and G. Hinton, Visualizing Data Using T-\nSNE, J. Mach. Learn. Res., 2008,9, 2579–2605.\n44 L. Breiman, Random Forests,Mach. Learn., 2001,45\n,5 –32.\n45 V. Vovk, Kernel Ridge Regression, Empirical Inference:\nFestschri in Honor of Vladimir N. Vapnik, Springer, 2013,\npp. 105–116.\n46 A. S. Rosen, S. M. Iyer, D. Ray, Z. Yao, A. Aspuru-Guzik,\nL. Gagliardi, J. M. Notestein and R. Q. Snurr, Machine\nLearning the Quantum-Chemical Properties of Metal –\nOrganic Frameworks for Accelerated Materials Discovery,\nMatter, 2021,4, 1578–1597.\n47 M. Pardakhti, E. Moharreri, D. Wanik, S. L. Suib and\nR. Srivastava, Machine Learning Using Combined\nStructural and Chemical Descriptors for Prediction of\nMethane Adsorption Performance of Metal Organic\nFrameworks (MOFs),ACS Comb. Sci., 2017,19, 640–645.\n48 P. B´enard and R. Chahine, Storage of Hydrogen by\nPhysisorption on Carbon and Nanostructured Materials,\nScr. Mater., 2007,56, 803–808.\n49 T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nP. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,\nR. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark,\nC. Berner, S. McCandlish, A. Radford, I. Sutskever and\nD. Amodei, Language Models are Few-Shot Learners,Adv.\nNeural Inf. Process. Syst., 2020,33, 1877–1901.\n50 S. P. Ong, W. D. Richards, A. Jain, G. Hautier, M. Kocher,\nS. Cholia, D. Gunter, V. L. Chevrier, K. A. Persson and\nG. Ceder, Python Materials Genomics (Pymatgen): A\nRobust, Open-Source Python Library for Materials Analysis,\nComput. Mater. Sci., 2013,68, 314–319.\n51 A. S. Krishnapriyan, M. Haranczyk and D. Morozov,\nTopological Descriptors Help Predict Guest Adsorption in\nNanoporous Materials, J. Phys. Chem. C, 2020, 124, 9360–\n9368.\n52 S. L. Price, M. Leslie, G. W. A. Welch, M. Habgood, L. S. Price,\nP. G. Karamertzanis and G. M. Day, Modelling Organic\nCrystal Structures Using Distributed Multipole and\nPolarizability-Based Model Intermolecular Potentials,Phys.\nChem. Chem. Phys., 2010,12, 8478–8490.\n53 S. Anthony: A Program for Performing Distributed Multipole\nAnalysis of Wave Functions Calculated Using the Gaussian\nProgram System, University of Cambridge, 2010.\n54 E. O. Pyzer-Knapp, H. P. G. Thompson and G. M. Day, An\nOptimized Intermolecular Force Field for Hydrogen-\nBonded Organic Molecular Crystals Using Atomic\nMultipole Electrostatics,Acta Cryst. B, 2016,72, 477–487.\n55 J. Nyman, O. S. Pundyke and G. M. Day, Accurate Force\nFields and Methods for Modelling Organic Molecular\nCrystals at Finite Temperatures, Phys. Chem. Chem. Phys.,\n2016, 18, 15828–15837.\n56 D. Dubbeldam, S. Calero, D. E. Ellis and R. Q. Snurr, RASPA:\nMolecular Simulation Soware for Adsorption and Di\nﬀusion\nin Flexible Nanoporous Materials,Mol. Simul., 2016,42,8 1–\n101.\n57 D. Rogers and M. Hahn, Extended-connectivityngerprints,\nJ. Chem. Inf. Model., 2010,50, 742–754.\n58 G. M. Day, S. L. Price and M. Leslie, Elastic constant\ncalculations for molecular organic crystals, Cryst. Growth\nDes., 2001,1,1 3–27.\n59 D. S. Coombes, S. L. Price, D. J. Willock and M. Leslie, Role of\nelectrostatic interactions in determining the crystal\nstructures of polar organic molecules. A distributed\nmultipole study,J. Phys. Chem., 1996,100, 7352–7360.\n60 A. Stone and M. Alderton, Distributed multipole analysis:\nMethods and applications,Mol. Phys., 1985,56, 1047–1064.\n61 F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,\nV. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,\nM. Brucher, M. Perrot and ´E. Duchesnay, Scikit-Learn:\nMachine Learning in Python,J. Mach. Learn. Res., 2011,12,\n2825–2830.\n12858 | Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 © 2025 The Author(s). Published by the Royal Society of Chemistry\nChemical Science Edge Article\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n62 G. Kresse and J. Furthmüller, Eﬃcient Iterative Schemes for\nAb InitioTotal-Energy Calculations Using a Plane-Wave Basis\nSet, Phys. Rev. B, 1996,54, 11169–11186.\n63 P. E. Blöchl, Projector Augmented-Wave Method,Phys. Rev.\nB, 1994,50, 17953–17979.\n64 J. P. Perdew, K. Burke and M. Ernzerhof, Generalized\nGradient Approximation Made Simple, Phys. Rev. Lett. ,\n1996, 77, 3865–3868.\n65 S. Grimme, J. Antony, S. Ehrlich and H. Krieg, A Consistent\nand AccurateAb InitioParametrization of Density Functional\nDispersion Correction (DFT-D) for the 94 Elements H-Pu,J.\nChem. Phys., 2010,132, 154104.\n66 A. D. Becke and E. R. Johnson, A Density-Functional Model\nof the Dispersion Interaction, J. Chem. Phys., 2005, 123,\n154101.\n67 S. Grimme, S. Ehrlich and L. Goerigk, Eﬀect of the Damping\nFunction in Dispersion Corrected Density Functional\nTheory, J. Comput. Chem., 2011,32, 1456–1465.\n© 2025 The Author(s). Published by the Royal Society of Chemistry Chem. Sci.,2 0 2 5 ,16,1 2 8 4 4–12859 | 12859\nEdge Article Chemical Science\nOpen Access Article. Published on 21 May 2025. Downloaded on 11/5/2025 6:03:32 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.8505418300628662
    },
    {
      "name": "Transfer of learning",
      "score": 0.6048213839530945
    },
    {
      "name": "Modal",
      "score": 0.5473317503929138
    },
    {
      "name": "Transfer (computing)",
      "score": 0.44409313797950745
    },
    {
      "name": "Computer science",
      "score": 0.3931133449077606
    },
    {
      "name": "Materials science",
      "score": 0.34494730830192566
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32911521196365356
    },
    {
      "name": "Political science",
      "score": 0.15042021870613098
    },
    {
      "name": "Polymer chemistry",
      "score": 0.10367277264595032
    },
    {
      "name": "Law",
      "score": 0.10158136487007141
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I146655781",
      "name": "University of Liverpool",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I43439940",
      "name": "University of Southampton",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I2802641067",
      "name": "Leverhulme Trust",
      "country": "GB"
    }
  ],
  "cited_by": 5
}