{
  "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
  "url": "https://openalex.org/W4285143611",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4320554972",
      "name": "Artem Vazhentsev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2617262939",
      "name": "Gleb Kuzmin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A294125378",
      "name": "Artem Shelmanov",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4287895887",
      "name": "Akim Tsvigun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2809879217",
      "name": "Evgenii Tsymbalov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3001712327",
      "name": "Kirill Fedyanin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2040166281",
      "name": "Maxim Panov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110658729",
      "name": "Alexander Panchenko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2005728791",
      "name": "Gleb Gusev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3078001183",
      "name": "Mikhail Burtsev",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2887054340",
      "name": "Manvel Avetisian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128415670",
      "name": "Leonid Zhukov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3121064530",
    "https://openalex.org/W2101946573",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2792388013",
    "https://openalex.org/W4287755806",
    "https://openalex.org/W3153723500",
    "https://openalex.org/W2788907134",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2997129641",
    "https://openalex.org/W3175976195",
    "https://openalex.org/W2994795100",
    "https://openalex.org/W1502922572",
    "https://openalex.org/W3131119906",
    "https://openalex.org/W2951786554",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3173618889",
    "https://openalex.org/W2138779671",
    "https://openalex.org/W2964348886",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3169290870",
    "https://openalex.org/W3037355691",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2903158431",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3006511999",
    "https://openalex.org/W2164411961",
    "https://openalex.org/W3101988982",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2970121940",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2618169590",
    "https://openalex.org/W1872312298",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3174540647"
  ],
  "abstract": "Artem Vazhentsev, Gleb Kuzmin, Artem Shelmanov, Akim Tsvigun, Evgenii Tsymbalov, Kirill Fedyanin, Maxim Panov, Alexander Panchenko, Gleb Gusev, Mikhail Burtsev, Manvel Avetisian, Leonid Zhukov. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8237 - 8252\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nUncertainty Estimation of Transformer Predictions for\nMisclassiﬁcation Detection\nArtem Vazhentsev1,2 ♦, Gleb Kuzmin1,6 ♦, Artem Shelmanov1,7 ♦, Akim Tsvigun1,4,\nEvgenii Tsymbalov2, Kirill Fedyanin2, Maxim Panov2, Alexander Panchenko2,\nGleb Gusev1,3,5, Mikhail Burtsev1,3, Manvel Avetisian1,5, and Leonid Zhukov1,4\n1AIRI, 2Skoltech, 3MIPT, 4HSE, 5Sber AI Lab, 6FRC CSC RAS,\n7ISP RAS Research Center for Trusted Artiﬁcial Intelligence\n{vazhentsev, kuzmin, shelmanov, tsvigun, gusev, burtsev, manvel, zhukov}@airi.net\n{evgenii.tsymbalov, m.panov, k.fedyanin, a.panchenko}@skoltech.ru\nAbstract\nUncertainty estimation (UE) of model\npredictions is a crucial step for a variety of\ntasks such as active learning, misclassiﬁcation\ndetection, adversarial attack detection, out-of-\ndistribution detection, etc. Most of the works\non modeling the uncertainty of deep neural\nnetworks evaluate these methods on image\nclassiﬁcation tasks. Little attention has been\npaid to UE in natural language processing.\nTo ﬁll this gap, we perform a vast empirical\ninvestigation of state-of-the-art UE methods\nfor Transformer models on misclassiﬁcation\ndetection in named entity recognition and\ntext classiﬁcation tasks and propose two\ncomputationally efﬁcient modiﬁcations, one\nof which approaches or even outperforms\ncomputationally intensive methods1.\n1 Introduction\nMachine learning methods are naturally prone to\nerrors as they typically have to deal with ambiguous\nand incomplete data during both training and\ninference. Unreliable predictions hinder the\napplication of these methods in domains, where\nthe price of mistakes is very high, such as clinical\nmedicine. Even in more error-tolerant domains\nand tasks, such as intent recognition in general-\npurpose chatbots, one would like to achieve a better\ntrade-off between expressiveness of a model and\nits computational performance during inference.\nSince mistakes are inevitable, it is crucial\nto understand whether model predictions can\nbe trusted or not and abstain from unreliable\ndecisions. Uncertainty estimation (UE) of model\npredictions aims to solve this task. Ideally,\nuncertain instances should correspond to erroneous\n1The code for experiments is available online\nat https://github.com/AIRI-Institute/\nuncertainty_transformers\n♦ Equal contribution, corresponding authors\nobjects and help in misclassiﬁcation detection .\nBesides misclassiﬁcation detection, UE is a crucial\ncomponent for active learning (Settles, 2009),\nadversarial attack detection (Lee et al., 2018),\ndetection of out-of-distribution (OOD) instances\n(Van Amersfoort et al., 2020), etc.\nSome classical machine learning models, e.g.\nGaussian processes (Rasmussen, 2003), have\nbuilt-in UE capabilities. Modern deep neural\nnetworks (DNNs) usually take advantage of a\nsoftmax layer, which output can be considered\nas a prediction probability and be used for\nUE. However, the softmax probabilities are\nusually unreliable and produce overconﬁdent\npredictions (Guo et al., 2017). Some previously\nproposed techniques such as deep ensemble\n(Lakshminarayanan et al., 2017) are known for\nproducing good UE scores but require a large\nadditional memory footprint for storing several\nversions of weights and multiply an amount of\ncomputation for conducting several forward passes.\nReliable UE of DNN predictions that does not\nintroduce high computational overhead is an open\nresearch question (Van Amersfoort et al., 2020).\nIn this work, we investigate methods for UE\nof DNNs based on the Transformer architecture\n(Vaswani et al., 2017) in misclassiﬁcation detection.\nWe consider two of the most common NLP tasks:\ntext classiﬁcation and named entity recognition\n(NER). The latter has been overlooked in the\nliterature on UE. To our knowledge, this work is\nthe ﬁrst to consider UE for NER.\nWe propose two novel computationally cheap\nmethods for UE of Transformer predictions. The\nﬁrst method is the modiﬁcation of the Monte\nCarlo dropout with determinantal point process\nsampling of dropout masks (Shelmanov et al.,\n2021). We introduce an additional step for\nmaking masks more diverse, which helps to\n8237\nachieve substantial improvements and approach\nthe performance of computationally-intensive\nmethods on NER. The second method leverages\nMahalanobis distance (Lee et al., 2018) but also\nadds a spectral normalization of the weight matrix\nin the classiﬁcation layer (Liu et al., 2020). This\nmethod achieves the best results on most of the\ndatasets and even outperforms computationally-\nintensive methods. We also investigate recently\nproposed regularization techniques in combination\nwith other UE methods. The contributions of this\npaper are the following:\n• We propose two novel computationally cheap\nmodiﬁcations of UE methods for Transformer\nmodels. The method based on Mahalanobis\ndistance with spectral normalization\napproaches or even outperforms strong\ncomputationally intensive counterparts.\n• This work is the ﬁrst to investigate UE\nmethods on the NER task.\n• We conduct an extensive empirical evaluation,\nin which we investigate recently proposed\nregularization techniques in combination with\nother UE methods.\n2 Related Work\nIt is well known that reliable uncertainty scores can\nbe obtained simply by constructing an ensemble\nof decorrelated neural networks ( deep ensemble)\n(Lakshminarayanan et al., 2017). However,\nsuch a straightforward approach is coupled with\nsubstantial computational and memory overhead\nduring training an ensemble, performing inference\nof all its components, and storing multiple versions\nof weights. This overhead is a serious obstacle to\ndeploying ensemble-based uncertainty estimation\nmethods in practice.\nUncertainty estimation is a built-in capability\nof Bayesian neural networks (Blundell et al.,\n2015). However, such models have similar issues\nas ensembles and also require special training\nprocedures. Recently, it was shown by Gal and\nGhahramani (2016) that dropout, a well-known\nregularization technique, is formally equivalent\nto approximate variational inference in a deep\nGaussian process if it is activated during prediction.\nThis method, known as Monte Carlo (MC) dropout,\nuses the approximating variational distribution with\nBernoulli variables related to network units. MC\ndropout does not impose any overhead during\ntraining, introduces no additional parameters, and\nthus does not require any additional memory.\nThe main disadvantage of this method is that it\nusually requires many forward-pass samplings for\napproximating predictive posterior, which makes it\nalso computationally expensive.\nRecently, many works have investigated\nthe approximate Bayesian inference for neural\nnetworks using deterministic approaches: Lee et al.\n(2018); Liu et al. (2020); Van Amersfoort et al.\n(2020); Mukhoti et al. (2021); Shen et al. (2021),\netc. These methods do not introduce notable\noverhead for inference, storing weights, and usually\nrequire compatible training time. However, most\nof the research in this area is accomplished for\ncomputer vision tasks.\nFor text classiﬁcation, a series of works\ninvestigates UE methods for the OOD detection\ntask (Liu et al., 2020; Podolskiy et al., 2021;\nZeng et al., 2021; Hu and Khan, 2021). In this\nwork, we focus on a more challenging task –\nmisclassiﬁcation detection. While OOD detection\nrequires to model only the epistemic uncertainty\ninherent to the model and caused by a lack\nof training data, misclassiﬁcation detection also\nrequires to model aleatoric uncertainty caused by\nnoise and ambiguity in data (Mukhoti et al., 2021).\nWe consider recently proposed methods in this area\nthat are evaluated in text processing.\nThree recent works propose techniques for\nmisclassiﬁcation detection based on an additive\nregularization of a training loss function. Zhang\net al. (2019) suggest adding a penalty that reduces\nthe Euclidean distance between training instances\nof the same class and increases the distance\nbetween instances of different classes. He et al.\n(2020) suggest using two components in the\nloss function that reduce the difference between\noutputs from two versions of a model initialized\nwith different weights. They also use mix-up\n(Thulasidasan et al., 2019) to generate additional\ntraining instance representations that help to\ncapture aleatoric uncertainty, self-ensembling, MC\ndropout, and a distinctiveness score to measure the\nepistemic uncertainty. Xin et al. (2021) introduce a\nregularizer that penalizes overconﬁdent instances\nwith high loss. In another recent work, Shelmanov\net al. (2021) propose to combine MC dropout with\na Determinantal Point Process (DPP) to improve\nthe diversity of predictions by considering the\ncorrelations between neurons and sampling the\n8238\ndiverse neurons for activation in a dropout layer.\nIn this work, we conduct a systematic empirical\ninvestigation of UE methods on NLP tasks. We\nevaluate combinations of methods that have not\nbeen tested before and propose two modiﬁcations,\none of which achieves the best results among\ncomputationally cheap methods. The previous\nwork focuses on text classiﬁcation tasks, while this\nwork is the ﬁrst to investigate UE also for NER.\n3 Background and Methods\nIn this section, we describe the baselines and\npropose novel uncertainty estimation techniques.\n3.1 Softmax Response\nSoftmax Response (SR) (Geifman and El-Yaniv,\n2017) is a trivial baseline for UE that uses the\nprobabilities generated via the output softmax layer\nof the neural network. SR is based on the maximum\nprobability p(y|x) over classes y = c ∈C. The\nsmaller this probability is, the more uncertain\nmodel is:\nuSR(x) = 1 −max\nc∈C\np(y= c|x). (1)\n3.2 Monte Carlo Dropout\nStandard Monte Carlo Dropout (MC Dropout)\nConsider we have conducted T stochastic forward\npasses with activated dropout. In this work, we use\nthe following ways to quantify uncertainty with\nmethods based on MC dropout:\n• Sampled maximum probability (SMP) is:\nuSMP = 1 −max\nc∈C\n1\nT\nT∑\nt=1\npc\nt, (2)\nwhere pc\nt is the probability of the class cfor\nthe t-th stochastic forward pass.\n• Probability variance (PV; Gal et al. (2017);\nSmith and Gal (2018)) is:\nuPV = 1\nC\nC∑\nc=1\n(\n1\nT\nT∑\nt=1\n(pc\nt −pc)2\n)\n, (3)\nwhere pc = 1\nT\n∑\ntpc\nt is the probability for a\nclass caveraged across T stochastic forward\npasses.\n• Bayesian active learning by disagreement\n(BALD; Houlsby et al. (2011)) is:\nuBALD = −\nC∑\nc=1\npclog pc + 1\nT\n∑\nc,t\npc\nt log pc\nt.\n(4)\nThe two former techniques are speciﬁcally\ndesigned for estimation of the epistemic (model)\nuncertainty arising from the lack of knowledge and\nignore the aleatoric uncertainty related to ambiguity\nand noise in the data, while the latter method can\nbe seen as a measure of total uncertainty (Malinin\nand Gales, 2018).\nTransformers contain multiple dropout layers\n(after the embedding layer, in each attention head,\nand before the last classiﬁcation layer). It is\nshown in previous work that the standard MC\ndropout outperforms the baseline SR only when all\ndropout layers are activated in a model (Shelmanov\net al., 2021). Therefore, we follow this setting\nfor experiments in this work. We note that due to\nactivating all dropout layers, multiple stochastic\npredictions are required for the whole network,\nwhich introduces a large computational overhead.\nSimilar UE scores are used in deep ensemble\n(Lakshminarayanan et al., 2017), where instead\nof multiple stochastic predictions we train and\ninfer several model versions with different sets of\nweights.\nDiverse Determinantal Point Process Monte\nCarlo Dropout (DDPP MC dropout) (Ours)\nDeterminantal point processes (DPPs; Kulesza and\nTaskar (2012)) are used for sampling a subset\nof diverse objects from a given set. Recently,\nShelmanov et al. (2021) have combined the\nMC dropout with a determinantal point process\n(DPP) for sampling neurons in a dropout layer\nand demonstrated that using stochasticity in the\nlast dropout layer (in a classiﬁcation head of\nTransformer) only is enough to improve upon SR\nin misclassiﬁcation detection. This method is\nless computationally expensive than the standard\nMC dropout since it requires multiple stochastic\npredictions only for the top classiﬁcation layer of\nthe network with a small number of parameters,\nwhile all other layers are inferred only once.\nConsider the similarity matrix Ch between\nneurons of the h-th hidden layer (in particular, we\nuse a correlation matrix between output values of\nneurons on the training set). Then one can construct\nthe DPP-based dropout masks MDPP\nh using Ch\nas a likelihood kernel for the DPP: MDPP\nh ∼\nDPP(Ch). That gives the following probability to\nselect a set Sof activations on the layer h:\nP\n[\nMDPP\nh = S\n]\n= det(CS\nh)\ndet(Ch + I), (5)\n8239\nwhere CS\nh is the square submatrix of Ch obtained\nby keeping only rows and columns indexed by the\nsample S.\nIn this work, we improve this method by\nincreasing the diversity of the sampled DPP masks.\nAfter multiple dropout masks are pre-generated\nvia DPP in the inference step as in the original\nDPP MC dropout, we make an additional step, in\nwhich we select a diverse set of masks from this\npre-generated pool using one of two strategies:\n• DDPP (+DPP): We sample a set of “diverse”\nmasks that activate different sets of neurons.\nFor this purpose, we apply DPP sampling\nagain to the pool of pre-generated masks. As\na similarity kernel in this step, we use an RBF-\nsimilarity matrix of mask vectors.\n• DDPP (+OOD): We sample a set of masks\nthat generate diverse predictions. For this\npurpose, we select the masks that yield the\nhighest PV scores on the given OOD dataset.\nAfter a new set of T masks is selected, we use\nthem as in the standard MC dropout to obtain\nstochastic predictions. Increasing the diversity of\nmasks in the proposed modiﬁcation is motivated\nby the ﬁnding of Jain et al. (2020) that improving\nthe diversity of elements in an ensemble leads to\nbetter uncertainty estimates.\nWe note that in masks generated with DPP,\nusually, less than 50% of neurons are activated,\nwhich makes predictions poorly calibrated. To\nmitigate this problem, for each constructed mask,\nwe perform a temperature-scaling calibration (Guo\net al., 2017) using a held-out dataset.\n3.3 Deterministic Uncertainty Estimation\nSpectral-normalized Neural Gaussian Process\n(SNGP) Liu et al. (2020) suggest replacing the\ntypical dense output layer of a network with a\nlayer that implements a Gaussian process with\nan RBF kernel, whose posterior variance at a\ngiven instance is characterized by its L2 distance\nfrom the training data in the hidden vector space\nconstructed by underlying layers of a network. The\nauthors propose an approximation based on random\nFourier feature expansion, which enables end-to-\nend training and makes the inference feasible.\nHowever, this method requires hidden\nrepresentations to be distance-preserving in order\nto make it work. While the distance between\ninstances in the hidden space does not always\nhave a meaningful correspondence to the distance\nin the input space, authors prove that to keep\nhidden representations distance-preserving, the\ntransformation should satisfy the bi-Lipschitz\ncondition. For ResNets (He et al., 2016), this\nrequirement is satisﬁed if weight matrices for the\nnonlinear residual blocks have a spectral norm\n(i.e., the largest singular value) bounded from\nabove by a constant. Therefore, to enforce the\naforementioned Lipschitz constraint, they apply a\nspectral normalization (SN) on weight matrices.\nFor Transformers, they normalize the matrix of the\npenultimate classiﬁcation layer only.\nMahalanobis Distance (MD) Mahalanobis\ndistance is a generalisation of the Euclidean\ndistance, which takes into account the spreading\nof instances in the training set along various\ndirections in a feature space. Lee et al. (2018)\nsuggest estimating uncertainty by measuring the\nMahalanobis distance between a test instance and\nthe closest class-conditional Gaussian distribution:\nuMD = min\nc∈C\n(hi −µc)TΣ−1(hi −µc), (6)\nwhere hi is a hidden representation of a i-th\ninstance, µc is a centroid of a class c, and Σ is\na covariance matrix for hidden representations of\ntraining instances.\nRecently, the Mahalanobis distance has been\nadopted for out-of-distribution detection with\nTransformer networks by Podolskiy et al. (2021).\nMahalanobis Distance with Spectral-\nnormalized Network (MD SN) (Ours) Since\nthe UE method based on the Mahalanobis distance\nutilizes the idea of a proximity of a tested\ninstance hidden representation to the training\ndistribution, we expect this method to beneﬁt from\ndistance-preserving representations. Therefore,\nwe propose the modiﬁcation of the method of\nLee et al. (2018) and Podolskiy et al. (2021)\nthat enforces the bi-Lipschitz constraints on\ntransformation implemented by the network. We\nperform spectral normalization of the weight\nmatrix of the linear layer in the classiﬁcation head\nof Transformer as it is suggested in SNGP (Liu\net al., 2020). At each training step, a spectral norm\nν is estimated using the power iteration method\nν = ∥W∥2, and a normalized weight matrix is\nobtained: ˜W = W\nν . At the inference step, hidden\nrepresentations are calculated using the normalized\n8240\nmatrix ˜h(x) = ˜Wx+band are used for computing\nthe Mahalanobis distance.\n3.4 Training Loss Regularization\nAdditive regularization is another approach to\nimproving UE of neural network predictions.\nUsually, the training loss combines the original\ntask-speciﬁc loss Ltask (e.g. cross-entropy) and\na regularization component Lreg that facilitates\nproducing better calibrated UEs:\nL= Ltask + λLreg, (7)\nwhere λ is a hyperparameter that controls the\nregularization strength.\nThe positive side of such techniques is that,\nbesides SR, they can be used to improve other\nmethods like MC dropout and deterministic\nmethods. The drawback is that regularization\naffects the training procedure and can decrease the\nmodel quality.\nConﬁdent Error Regularizer (CER) Xin et al.\n(2021) propose a regularizer that adds a penalty for\nan instance with a bigger loss than other instances\nand, at the same time, bigger conﬁdence:\nLreg =\nk∑\ni,j=1\n∆i,j1 [ei >ej], (8)\n∆i,j = max{0,max\nc\npc\ni −max\nc\npc\nj}2, (9)\nwhere kis the number of instances in a batch and\nei is an error of the i-th instance: ei is 1 if the\nprediction of the classiﬁer matches the true label,\nand ei is 0 otherwise. The authors evaluate this\ntype of regularization only in conjunction with the\nSR baseline.\nMetric Regularizer Zhang et al. (2019) propose\na regularizer that aims to shorten the intra-class\ndistance and enlarge the inter-class distance:\nLreg=\nC∑\nc=1\n{\nLintra(c)+ε\n∑\nk̸=c\nLinter(c,k)\n}\n, (10)\nLintra(c) = 2\n|Sc|2 −|Sc|\n∑\ni,j∈Sc,i<j\nD(hi,hj),\n(11)\nLinter(c,k)= 1\n|Sc|·|Sk|\n∑\ni∈Sc,j∈Sk\n[γ−D(hi,hj)]+,\n(12)\nD(ri,rj) = 1\nd||hi −hj||2\n2, (13)\nwhere hi is a feature representation of an instance\ni from a penultimate layer of a model with a\ndimension d, Sc is the set of instances from class\nc, |Sc|is the number of elements in Sc, εand γare\npositive hyperparameters, [x]+ = max(0,x).\n4 Experimental Setup\nIn the experiments, we train a model on a given\ndataset and perform inference on a separate test\nset to compute both predictions and UE scores u.\nWe are interested in how the scores correlate with\nthe mistakes ˜e of the model on the test set. For\ntext classiﬁcation, mistakes are computed in the\nfollowing way:\n˜ei =\n{ 1, yi ̸= ˆyi,\n0, yi = ˆyi, (14)\nwhere yi is a true label, ˆyi is a predicted label.\nFor NER, we use two evaluation options: token-\nlevel and sequence-level. For the token-level\nevaluation, individual tokens are considered as\nseparate instances as in the text classiﬁcation.\nFor the sequence-level evaluation, mistakes are\ncomputed in the following way:\n˜ei =\n{ 1, ∃j ∈{1,...,n }, yij ̸= ˆyij,\n0, ∀j ∈{1,...,n }, yij = ˆyij, (15)\nwhere nis a sequence length, yij is a true label, ˆyij\nis a predicted label of aj-th token in a sequence. In\nthe sequence-level evaluation, UE of a sequence is\naggregated from UEs of tokens by taking maximum\n(for MD methods) or by summation (for others).\n4.1 Metrics\nEl-Yaniv and Wiener (2010) suggest evaluating\nthe quality of UE using the area under the risk\ncoverage curve (RCC-AUC). The risk coverage\ncurve demonstrates the cumulative sum of loss due\nto misclassiﬁcation (cumulative risk) depending\non the uncertainty level used for rejection of\npredictions. The lower area under this curve\nindicates better quality of the UE method.\nXin et al. (2021) propose a reversed pair\nproportion (RPP)metric. They note that instances\nwith higher conﬁdence should have a lower loss l.\nRPP measures how far the uncertainty estimator ˜u\nis to ideal, given the labeled dataset of size n:\nRPP = 1\nn2\nn∑\ni,j=1\n1 [˜u(xi)>˜u(xj),li<lj]. (16)\n8241\nMethod Reg.TypeUEScore MRPC SST-2 CoLA CoNLL-2003 (token level)CoNLL-2003 (seq. level)RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓\nMC - PV 13.97±1.161.68±0.0912.90±1.920.82±0.1144.35±4.902.06±0.16 6.32±1.66 0.10±0.0216.05±3.781.93±0.43MC - BALD14.21±1.041.69±0.0912.98±1.870.82±0.1045.06±4.902.08±0.17 6.44±1.86 0.10±0.0216.28±4.001.96±0.45MC - SMP 14.38±2.071.76±0.1914.00±2.200.91±0.1542.95±5.982.01±0.15 6.04±1.03 0.09±0.0215.79±3.341.80±0.35MC CERPV 12.82±1.891.60±0.1312.18±1.200.80±0.1046.84±9.192.11±0.23 6.92±1.22 0.10±0.0217.05±3.141.91±0.36MC CERBALD12.89±1.891.60±0.1312.39±1.230.81±0.0947.34±8.302.14±0.24 7.16±1.15 0.11±0.0217.25±3.051.93±0.35MC CERSMP 12.91±2.151.67±0.1512.22±1.310.82±0.0946.10±11.072.05±0.22 6.69±1.38 0.10±0.0216.81±1.611.81±0.14MC metricPV 14.21±1.951.73±0.2312.28±1.770.80±0.1142.35±0.692.04±0.07 6.69±0.89 0.10±0.0117.17±1.901.93±0.31MC metricBALD14.55±2.311.73±0.2312.08±1.790.79±0.1043.76±0.552.08±0.07 6.91±1.02 0.10±0.0117.47±1.851.98±0.30MC metricSMP 13.39±1.191.72±0.2013.55±1.650.90±0.1440.88±1.252.01±0.09 6.30±0.98 0.10±0.0116.81±1.401.80±0.23\nDDPP (+DPP) (ours)- PV 22.30±7.152.58±0.6516.70±1.381.12±0.1249.75±3.962.44±0.29 6.12±0.71 0.10±0.0116.78±2.441.93±0.20DDPP (+DPP) (ours)- BALD23.08±7.002.63±0.6316.08±2.371.05±0.1849.59±5.402.48±0.31 6.39±0.64 0.10±0.0121.53±4.772.63±0.45DDPP (+DPP) (ours)- SMP 21.79±7.722.57±0.6817.55±3.031.19±0.2347.86±5.512.39±0.31 6.08±0.62 0.10±0.0117.71±2.772.05±0.23DDPP (+DPP) (ours)CERPV 15.12±2.272.03±0.2413.56±1.370.91±0.1454.51±8.802.58±0.22 6.98±0.98 0.11±0.0219.44±1.152.13±0.17DDPP (+DPP) (ours)CERBALD15.94±3.772.07±0.3614.87±2.220.96±0.1355.11±7.422.61±0.31 7.90±1.95 0.12±0.0126.20±6.413.11±0.56DDPP (+DPP) (ours)CERSMP 14.75±1.432.02±0.1614.47±1.630.99±0.1154.01±9.792.55±0.18 6.91±1.13 0.11±0.0220.66±1.532.31±0.08DDPP (+DPP) (ours)metricPV 19.51±3.402.47±0.2815.79±1.671.07±0.1443.82±1.822.17±0.14 7.33±1.53 0.12±0.0218.93±2.092.11±0.25DDPP (+DPP) (ours)metricBALD20.54±4.722.52±0.3415.48±1.811.03±0.0843.95±1.682.17±0.12 8.01±2.08 0.13±0.0322.44±4.782.67±0.49DDPP (+DPP) (ours)metricSMP 18.45±2.882.41±0.2616.78±3.431.14±0.2643.61±1.612.16±0.11 6.92±1.32 0.11±0.0219.11±2.142.16±0.22\nDDPP (+OOD) (ours)- PV 22.73±7.452.65±0.5919.05±2.951.29±0.2351.11±12.032.37±0.34 6.32±0.72 0.10±0.0116.75±2.311.94±0.21DDPP (+OOD) (ours)- BALD23.85±8.392.69±0.5818.27±3.051.22±0.2352.59±12.082.42±0.34 6.59±0.69 0.11±0.0120.56±3.092.50±0.26DDPP (+OOD) (ours)- SMP 22.31±7.802.60±0.6519.86±3.831.36±0.2950.14±9.732.32±0.30 6.09±0.67 0.10±0.0117.76±2.752.06±0.23DDPP (+OOD) (ours)CERPV 14.83±1.422.05±0.1714.98±1.361.01±0.0959.14±11.272.56±0.24 7.08±1.37 0.11±0.0219.66±1.252.17±0.15DDPP (+OOD) (ours)CERBALD15.03±1.852.08±0.2414.37±2.220.96±0.1457.48±9.372.54±0.26 7.41±1.29 0.12±0.0225.30±3.363.00±0.24DDPP (+OOD) (ours)CERSMP 14.34±1.151.99±0.1615.88±1.961.08±0.1359.32±11.862.53±0.20 6.88±1.24 0.11±0.0221.06±1.962.35±0.14DDPP (+OOD) (ours)metricPV 19.03±3.972.41±0.3417.75±5.201.10±0.1748.54±11.382.23±0.24 6.92±1.32 0.11±0.0218.36±1.902.05±0.26DDPP (+OOD) (ours)metricBALD19.33±4.782.41±0.4016.71±7.131.02±0.2049.31±11.872.24±0.25 7.21±1.49 0.11±0.0221.35±4.472.54±0.45DDPP (+OOD) (ours)metricSMP 18.55±3.062.42±0.2717.08±3.781.14±0.2643.67±1.772.15±0.11 6.71±1.18 0.10±0.0219.01±2.302.16±0.25\nSR CERMP 14.62±1.622.02±0.1914.56±2.141.00±0.1456.97±9.692.53±0.15 6.84±1.41 0.11±0.0221.31±1.632.49±0.25SR metricMP 18.39±2.942.40±0.2716.90±3.121.16±0.2444.54±2.112.22±0.15 6.51±1.07 0.10±0.0220.32±1.682.32±0.23SR (baseline)- MP 22.32±8.082.58±0.6517.93±3.841.22±0.2849.48±3.712.35±0.25 6.08±0.62 0.10±0.0118.81±3.352.21±0.29\nTable 1: Results for methods based on MC dropout and regularization techniques (ELECTRA model). The best\nresults are shown in bold, the best results for each method are underlined.\nThis metric has an upper bound of 1; for\nconvenience, the reported values are multiplied by\n100. Similar to Xin et al. (2021), for both metrics,\nlis an indicator loss function.\nWe conduct each experiment six times\nwith different random seeds, obtaining the\ncorresponding metric values, and report their mean\nand standard deviation.\nWe also present the results using the accuracy\nrejection curve. This curve is drawn by varying\nthe rejection uncertainty level (horizontal axis) and\npresenting the corresponding accuracy obtained\nwhen all rejected instances are labeled with an\noracle (vertical axis). This emulates the work of\na human expert in conjunction with a machine\nlearning system. The higher the curve, the smaller\namount of labor is needed to achieve a certain level\nof performance and the better is the UE method. A\nsimilar evaluation approach in a table form is used\nin (Zhang et al., 2019). A similar curve but without\noracle labeling is used in (Lakshminarayanan et al.,\n2017; Filos et al., 2019).\n4.2 Datasets\nFor experiments with text classiﬁcation, we use\nthree datasets from the GLUE benchmark (Wang\net al., 2018) that were previously leveraged\nby Shelmanov et al. (2021) and Xin et al.\n(2021) for the same purpose: Microsoft Research\nParaphrase Corpus (MRPC) (Dolan and Brockett,\n2005), Corpus of Linguistic Acceptability (CoLA)\n(Warstadt et al., 2019), and Stanford Sentiment\nTreebank (SST-2) (Socher et al., 2013). Similar to\n(Shelmanov et al., 2021), we randomly subsample\nSST-2 to 10% to emulate a low-resource setting.\nThe experiments with NER were performed\non the widely-used CoNLL-2003 task (Tjong\nKim Sang and De Meulder, 2003). For this dataset,\nwe also subsample the training part to 10%.\nAs an out-of-domain dataset for DDPP MC\ndropout, we use the IMDB binary sentiment\nclassiﬁcation dataset (Maas et al., 2011). We\nrandomly select 5,000 instances from its test part\nand use them to select DPP-generated masks.\nThe dataset statistics are provided in Table 4 in\nAppendix A.\n4.3 Model Choice and Hyperparameter\nSelection\nFor experiments, we use two modern Transformers:\nthe pre-trained ELECTRA model (Clark et al.,\n2020) with 110 million parameters and DeBERTa\n(He et al., 2021) with 138 million parameters.\nThey achieve higher performance on the GLUE\nbenchmark in comparison with previous models,\nsuch as BERT (Devlin et al., 2019) and RoBERTa\n(Liu et al., 2019).\nThe optimal hyperparameter values for each\ntriple <Dataset, Regularization Type, Spectral\nNormalization Usage> are presented in Table 6\n8242\nMethod Reg.Type UEScore MRPC SST-2 CoLA CoNLL-2003 (token level)CoNLL-2003 (seq. level)RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓\nMD - MD 13.69±1.251.88±0.1313.08±2.580.86±0.1541.73±1.451.96±0.0410.33±3.550.15±0.04 17.05±5.072.05±0.45MD CERMD 13.61±1.821.87±0.2214.10±2.690.96±0.1642.50±2.652.00±0.07 6.82±0.90 0.10±0.01 16.92±2.511.87±0.23MD metricMD 13.91±2.351.89±0.2912.03±2.040.85±0.1540.29±2.092.02±0.0910.01±2.560.15±0.03 17.67±3.922.09±0.36\nMD SN (ours)- MD 13.44±1.281.85±0.2011.77±1.330.83±0.0840.07±3.621.95±0.16 7.21±1.34 0.11±0.02 17.29±3.582.01±0.37MD SN (ours)CERMD 14.41±1.961.94±0.2112.32±1.370.85±0.1037.82±2.911.90±0.12 6.95±1.50 0.11±0.02 17.76±4.002.06±0.42MD SN (ours)metricMD 12.04±1.331.56±0.1212.05±1.420.84±0.0739.37±2.001.97±0.15 6.90±1.21 0.11±0.02 17.02±3.392.01±0.40\nSNGP - SNGP14.52±2.482.00±0.3516.08±4.181.02±0.1851.96±1.892.64±0.0756.43±23.030.60±0.2244.80±11.005.06±1.01\nSR SN - MP 18.83±3.892.46±0.4619.02±6.071.21±0.3581.25±12.563.40±0.33 7.46±1.39 0.12±0.02 20.13±3.502.30±0.26SR CERMP 14.62±1.622.02±0.1914.56±2.141.00±0.1456.97±9.692.53±0.15 6.84±1.41 0.11±0.02 21.31±1.632.49±0.25SR metricMP 18.39±2.942.40±0.2716.90±3.121.16±0.2444.54±2.112.22±0.15 6.51±1.07 0.10±0.02 20.32±1.682.32±0.23SR (baseline)- MP 22.32±8.082.58±0.6517.93±3.841.22±0.2849.48±3.712.35±0.25 6.08±0.62 0.10±0.01 18.81±3.352.21±0.29\nTable 2: Results of deterministic methods with different types of regularization (ELECTRA model). The best\nresults are highlighted with the bold font, the best results for each method are underlined.\nin Appendix A. For the optimal hyperparameter\nsearch, we split the original training data into\ntraining and validation subsets in a ratio of 80\nto 20 and apply Bayesian optimization with early\nstopping. For text classiﬁcation, we use accuracy\nas an objective metric, and for sequence tagging,\nwe use span-based F1-score (Tjong Kim Sang and\nDe Meulder, 2003). Sets of pre-deﬁned values\nfor each hyperparameter are given in the caption\nof Table 6. After the hyperparameter search is\ncompleted, we train the model on the original\ntraining set using the optimal values.\nThe hyperparameters for UE methods are\npresented in Table 9 in Appendix A. The values\nfor the DDPP MC dropout and MD SN are chosen\nusing a grid search, while validating on the held-out\nvalidation dataset with RCC-AUC as an objective.\nFor deep ensemble, we use random subsampling of\nthe training set with a ﬁxed ratio of 90%.\nThe hardware conﬁguration for experiments is\nprovided in Table 5 in Appendix A.\n5 Results and Discussion\n5.1 Monte Carlo Dropout and Regularization\nThe results of methods based on MC dropout\nand loss regularization are presented in Table 1\n(for ELECTRA). The standard computationally\nintensive MC dropout achieves big improvements\nover the SR baseline on all text classiﬁcation\ndatasets and the sequence-level CoNLL-2003\nbenchmark. For token-level CoNLL-2003, none of\nthe considered methods substantially outperform\nthe baseline. Uncertainty estimation scores BALD\nand PV have similar results, outperforming SMP\non SST-2, while SMP has a slight advantage over\nthem on CoLA and CoNLL-2003.\nThe DDPP MC dropout method does not\noutperform the MC dropout. However, DDPP\n(+DDPP) demonstrates a notable advantage over\nthe SR baseline on text classiﬁcation datasets SST-\n2 and CoLA, while both DDPP (+DDPP) and\nDDPP (+OOD) outperform the baseline on the\nsequence-level CoNLL-2003 benchmark. The\nmain advantage of the proposed DDPP MC dropout\nmethod consists in its much faster inference\ncompared to the computationally expensive\nstandard MC dropout. The DDPP MC dropout has\nthe same computational overhead during inference\nas the original DPP MC dropout, which is only\nless than 0.5% of the overhead introduced by the\nstandard MC dropout (Shelmanov et al., 2021).\nWe conduct an ablation study of the proposed\nmodiﬁcations for the original DPP MC dropout.\nThe experimental results of this study presented in\nTable 12 in Appendix C demonstrate the beneﬁts of\nusing calibration and introducing diversity in mask\ngeneration.\nBoth metric regularization and CER achieve a\nsubstantial advantage over the baseline on text\nclassiﬁcation datasets SST-2 and MRPC. However,\nregularization appears to be malignant for NER.\nAdding loss regularization to MC dropout usually\nhelps to achieve better results on text classiﬁcation.\nThe best results on SST-2 and CoLA are achieved\nusing metric regularization, while the best result\nfor MRPC is obtained using CER. Regularization\nand DDPP MC dropout usually complement each\nother, the results of their combination are slightly\nbetter than when they are applied individually for\nall datasets except CoNLL-2003.\n5.2 Deterministic Methods\nThe results for deterministic methods are presented\nin Table 2 (for ELECTRA). SNGP gives substantial\nimprovements on the text classiﬁcation datasets\nMRPC and SST-2 but signiﬁcantly falls behind\nthe trivial baseline on CoNLL-2003. The low\nperformance of SNGP for NER can be attributed to\nthe fact that it is initially designed for classiﬁcation\n8243\nMethod Reg.TypeUEScore MRPC SST-2 CoLA CoNLL-2003 (token level)CoNLL-2003 (seq. level)RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓\nMC - SMP 14.38±2.071.76±0.1914.00±2.200.91±0.1542.95±5.982.01±0.15 6.04±1.03 0.09±0.0215.79±3.341.80±0.35MC CERPV 12.82±1.891.60±0.1312.18±1.200.80±0.1046.84±9.192.11±0.23 6.92±1.22 0.10±0.0217.05±3.141.91±0.36MC metricBALD14.55±2.311.73±0.2312.08±1.790.79±0.1043.76±0.552.08±0.07 6.91±1.02 0.10±0.0117.47±1.851.98±0.30MC metricSMP 13.39±1.191.72±0.2013.55±1.650.90±0.1440.88±1.252.01±0.09 6.30±0.98 0.10±0.0116.81±1.401.80±0.23Deep Ensemble- PV 20.70±4.242.10±0.3512.02±1.630.71±0.0750.15±5.572.21±0.19 4.02±1.24 0.06±0.0213.18±4.601.54±0.57Deep Ensemble- SMP 13.01±2.571.68±0.2712.13±1.270.79±0.0843.73±4.252.05±0.19 4.16±1.37 0.06±0.0213.93±4.881.57±0.58MSD MSDDS 12.70±1.611.74±0.2511.17±1.030.78±0.0639.21±2.181.90±0.1212.34±4.190.18±0.0516.83±3.921.94±0.25\nDDPP (+DPP) (ours)- PV 22.30±7.152.58±0.6516.70±1.381.12±0.1249.75±3.962.44±0.29 6.12±0.71 0.10±0.0116.78±2.441.93±0.20DDPP (+DPP) (ours)- SMP 21.79±7.722.57±0.6817.55±3.031.19±0.2347.86±5.512.39±0.31 6.08±0.62 0.10±0.0117.71±2.772.05±0.23DDPP (+DPP) (ours)CERPV 15.12±2.272.03±0.2413.56±1.370.91±0.1454.51±8.802.58±0.22 6.98±0.98 0.11±0.0219.44±1.152.13±0.17DDPP (+DPP) (ours)CERSMP 14.75±1.432.02±0.1614.47±1.630.99±0.1154.01±9.792.55±0.18 6.91±1.13 0.11±0.0220.66±1.532.31±0.08DDPP (+DPP) (ours)metricSMP 18.45±2.882.41±0.2616.78±3.431.14±0.2643.61±1.612.16±0.11 6.92±1.32 0.11±0.0219.11±2.142.16±0.22DDPP (+OOD) (ours)- PV 22.73±7.452.65±0.5919.05±2.951.29±0.2351.11±12.032.37±0.34 6.32±0.72 0.10±0.0116.75±2.311.94±0.21DDPP (+OOD) (ours)- SMP 22.31±7.802.60±0.6519.86±3.831.36±0.2950.14±9.732.32±0.30 6.09±0.67 0.10±0.0117.76±2.752.06±0.23DDPP (+OOD) (ours)CERBALD15.03±1.852.08±0.2414.37±2.220.96±0.1457.48±9.372.54±0.26 7.41±1.29 0.12±0.0225.30±3.363.00±0.24DDPP (+OOD) (ours)CERSMP 14.34±1.151.99±0.1615.88±1.961.08±0.1359.32±11.862.53±0.20 6.88±1.24 0.11±0.0221.06±1.962.35±0.14DDPP (+OOD) (ours)metricSMP 18.55±3.062.42±0.2717.08±3.781.14±0.2643.67±1.772.15±0.11 6.71±1.18 0.10±0.0219.01±2.302.16±0.25MD CERMD 13.61±1.821.87±0.2214.10±2.690.96±0.1642.50±2.652.00±0.07 6.82±0.90 0.10±0.0116.92±2.511.87±0.23MD metricMD 13.91±2.351.89±0.2912.03±2.040.85±0.1540.29±2.092.02±0.0910.01±2.560.15±0.0317.67±3.922.09±0.36MD SN (ours)- MD 13.44±1.281.85±0.2011.77±1.330.83±0.0840.07±3.621.95±0.16 7.21±1.34 0.11±0.0217.29±3.582.01±0.37MD SN (ours)CERMD 14.41±1.961.94±0.2112.32±1.370.85±0.1037.82±2.911.90±0.12 6.95±1.50 0.11±0.0217.76±4.002.06±0.42MD SN (ours)metricMD 12.04±1.331.56±0.1212.05±1.420.84±0.0739.37±2.001.97±0.15 6.90±1.21 0.11±0.0217.02±3.392.01±0.40SR CERMP 14.62±1.622.02±0.1914.56±2.141.00±0.1456.97±9.692.53±0.15 6.84±1.41 0.11±0.0221.31±1.632.49±0.25SR metricMP 18.39±2.942.40±0.2716.90±3.121.16±0.2444.54±2.112.22±0.15 6.51±1.07 0.10±0.0220.32±1.682.32±0.23\nSR (baseline)- MP 22.32±8.082.58±0.6517.93±3.841.22±0.2849.48±3.712.35±0.25 6.08±0.62 0.10±0.0118.81±3.352.21±0.29\nTable 3: Comparison of the best results for all methods (ELECTRA model). The computationally intensive\nmethods are at the top of the table; the computationally cheap methods are at the bottom. The best results overall\nare highlighted with the bold font, the best results for computationally cheap methods are underlined.\n0 10 20 30\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for MRPC\n0. MC, CER, PV\n1. Deep Ensemble, SMP\n2. MSD, MSD, MSD\n3. DDPP (+OOD), CER, SMP\n4. MD, CER, MD\n5. MD SN, metric, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP\na) MRPC\n0 5 10 15 20\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for SST2\n0. MC, metric, bald\n1. Deep Ensemble, PV\n2. MSD, MSD, MSD\n3. DDPP (+DPP), CER, PV\n4. MD, metric, MD\n5. MD SN, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP b) SST2\n0 20 40 60\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for CoLA\n0. MC, metric, SMP\n1. Deep Ensemble, SMP\n2. MSD, MSD, MSD\n3. DDPP (+DPP), metric, SMP\n4. MD, metric, MD\n5. MD SN, CER, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP c) CoLA\n0 5 10 15\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for CoNNL-2003 (token level)\n0. MC, SMP\n1. Deep Ensemble, PV\n2. MSD, MSD, MSD\n3. DDPP (+DPP), SMP\n4. MD, CER, MD\n5. MD SN, metric, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP d) CoNLL token. level\n0 5 10 15 20\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for CoNNL-2003 (seq. level)\n0. MC, SMP\n1. Deep Ensemble, PV\n2. MSD, MSD, MSD\n3. DDPP (+OOD), PV\n4. MD, CER, MD\n5. MD SN, metric, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP e) CoNLL seq. level\nFigure 1: RCC-AUC↓of the best UE methods for the ELECTRA model.\ntasks rather than sequence tagging. MD yields\nmuch bigger improvements over the SR baseline\non all datasets and signiﬁcantly outperforms SNGP.\nMD SN is able to improve the misclassiﬁcation\ndetection performance even further for MRPC,\nSST-2, and CoLA.\nWe also conduct an ablation study (Table 2), in\nwhich we use the spectral normalization without\nMD. We see that SN on its own, as expected, mostly\ndoes not improve the UE performance; the results\nusually are even slightly worse than the baseline.\nRegularization also helps to improve the results\nof methods based on the Mahalanobis distance.\nFor both MD and MD SN, regularization helps on\nCoLA and CoNLL-2003. For MD, it also helps on\nSST-2, while for MD SN, regularization improves\nthe results on MRPC. We note that regularization\nreduces the gap between MD and MD SN on\ntext classiﬁcation datasets and even gives a slight\nadvantage to MD over MD SN on CoNLL-2003.\nThe best results across all deterministic methods\nfor text classiﬁcation datasets are achieved by MD\nSN. The biggest improvements are obtained on\nMRPC, where regularized MD SN reduces RCC-\nAUC by more than 46% compared to the baseline.\n5.3 Best Results\nTable 3 and Figure 1 compare results of the best\nmethods in each group for ELECTRA. Table 11\nand Figure 3 in Appendix B show the best\nresults for DeBERTa. In these tables and ﬁgures,\nwe also present the results of deep ensemble\n(Lakshminarayanan et al., 2017), which is a strong\nyet computationally intensive baseline (Ashukha\net al., 2020), and results of another recently\nproposed computationally intensive method called\nMSD (He et al., 2020) that leverage “mix-up”\n(Thulasidasan et al., 2019), “self-ensembling”, MD,\n8244\n0.2 0.4 0.6 0.8 1.0\nRejection rate\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00Accuracy score\nDeep Ensemble, SMP\nDDPP (+OOD), CER, SMP (ours)\nMC, CER, PV\nMD, CER\nMD SN, metric (ours)\nSR (baseline)\nFigure 2: Median values of accuracy rejection curves\nfor selected methods on MRPC (ELECTRA model).\nand the MC dropout (all layers are activated).\nWe can see that it is possible to substantially\nimprove misclassiﬁcation detection performance\nand achieve even better results than MC dropout,\ndeep ensemble, or MSD almost with no overhead\nin terms of memory consumption and amount\nof computation. For text classiﬁcation and for\nboth models, computationally cheap methods\nare either better or on par with the expensive\ncounterparts. However, for NER, we see that\nthe latter methods seriously fall behind deep\nensemble and MC dropout. On the token-level\nCoNLL-2003 benchmark, only deep ensemble\nsubstantially outperforms the SR baseline. On\nthe sequence-level CoNLL-2003 benchmark, MD\nwith CER, DDPP (+DDP) PV , and DDPP (+OOD)\nPV improve upon SR, but only approach the\nperformance of computationally intensive methods.\nThe proposed in this work MD SN method\noutperforms all other computationally efﬁcient\nalternatives on text classiﬁcation datasets. For\nboth models, it even substantially outperforms all\ncomputationally expensive methods on the CoLA\ndataset, while on other text classiﬁcation datasets\nit is on par with them. Another method proposed\nin this work, DDPP MC dropout, empowered with\nregularization techniques, is able to substantially\nreduce the gap between the SR baseline and\ncomputationally intensive UE methods, while\nintroducing only a fraction of their overhead.\nFigure 2 also presents accuracy rejection curves\nfor selected methods on MRPC. The ﬁgure shows\nthat if we reject 20% of instances using UE\nobtained with MC dropout and ask human experts\nto label these uncertain objects, the accuracy score\nof such a human-machine hybrid system will\nincrease from 88.4% to 96.0%, which is 1.3%\nbetter than the SR baseline. Such an additional gain\nover the SR baseline can be crucial for safe-critical\napplications. Deep ensemble and MD SN are close\nto each other and achieve 95.6% and 95.2% of\naccuracy correspondingly. Rejecting 40% of most\nuncertain instances gives 98.2% of accuracy for the\ncomputationally-intensive deep ensemble, while\nthe proposed cheap MD SN method yields even\nbetter result with 98.5% of accuracy, which is 1.7%\nhigher than the result of the SR baseline.\n6 Conclusion\nOur extensive empirical investigation on\ntext classiﬁcation and NER tasks shows that\ncomputationally cheap UE methods are able to\nsubstantially improve misclassiﬁcation detection\nfor Transformers, performing on par or even\nbetter than computationally intensive MC dropout\nand deep ensemble. The proposed in this work\nmethod based on the Mahalanobis distance\nand spectral normalization of a weight matrix\n(MD SN) achieves the best results among\nother computationally cheap methods on text\nclassiﬁcation datasets and is on par with expensive\nmethods. This method does not require seriously\nmodifying a model architecture, extra memory\nstorage, and introduces only a little amount of\nadditional computation during inference.\nWe also show that our modiﬁcation of\nDPP MC dropout that leverages the diversity\nof generated dropout masks, which is also\na computationally cheap method, is able to\noutperform the softmax response baseline and\napproach the computationally intensive methods\non NER. Finally, we ﬁnd that regularization can\nslightly improve the results of methods based on\nMC dropout and the Mahalanobis distance in text\nclassiﬁcation.\nThe spectral normalization is theoretically\nproven to ensure bi-Lipschitz constraint on the\ntransformation deﬁned by the standard residual\nconnection network (Liu et al., 2020). However,\nthe self-attention blocks in Transformers have\na more complicated architecture than the layers\nof standard ResNets, which means that the\ntheoretical guarantees for them do not hold in\ngeneral. In future work, we are looking forward\nto investigating other techniques to ensure bi-\nLipschitz constraint on self-attention blocks, which\nmight further improve deterministic methods for\nuncertainty estimation of Transformers.\n8245\nAcknowledgements\nWe thank anonymous reviewers for their insightful\nsuggestions to improve this paper. The work was\nsupported by a grant for research centers in the\nﬁeld of artiﬁcial intelligence (agreement identiﬁer\n000000D730321P5Q0002 dated November 2, 2021\nNo. 70-2021-00142 with ISP RAS).\nReferences\nArsenii Ashukha, Alexander Lyzhov, Dmitry\nMolchanov, and Dmitry P. Vetrov. 2020. Pitfalls of\nin-domain uncertainty estimation and ensembling in\ndeep learning. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nCharles Blundell, Julien Cornebise, Koray\nKavukcuoglu, and Daan Wierstra. 2015. Weight\nuncertainty in neural network. In Proceedings of\nthe 32nd International Conference on Machine\nLearning, ICML 2015, Lille, France, 6-11 July 2015,\nvolume 37 of JMLR Workshop and Conference\nProceedings, pages 1613–1622. JMLR.org.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof Deep Bidirectional Transformers for Language\nUnderstanding. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 4171–4186, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005.\nAutomatically constructing a corpus of sentential\nparaphrases. In Proceedings of the Third\nInternational Workshop on Paraphrasing\n(IWP2005).\nRan El-Yaniv and Yair Wiener. 2010. On the\nfoundations of noise-free selective classiﬁcation. J.\nMach. Learn. Res., 11:1605–1641.\nAngelos Filos, Sebastian Farquhar, Aidan N. Gomez,\nTim G. J. Rudner, Zachary Kenton, Lewis Smith,\nMilad Alizadeh, Arnoud de Kroon, and Yarin Gal.\n2019. A systematic comparison of bayesian deep\nlearning robustness in diabetic retinopathy tasks.\nCoRR, abs/1912.10481.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout\nas a bayesian approximation: Representing model\nuncertainty in deep learning. In Proceedings of\nThe 33rd International Conference on Machine\nLearning, volume 48 of Proceedings of Machine\nLearning Research , pages 1050–1059, New York,\nNew York, USA. PMLR.\nYarin Gal, Riashat Islam, and Zoubin Ghahramani.\n2017. Deep bayesian active learning with\nimage data. In Proceedings of the 34th\nInternational Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August\n2017, volume 70 of Proceedings of Machine\nLearning Research, pages 1183–1192. PMLR.\nYonatan Geifman and Ran El-Yaniv. 2017. Selective\nclassiﬁcation for deep neural networks. Advances\nin Neural Information Processing Systems, 30:4878–\n4887.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q.\nWeinberger. 2017. On calibration of modern\nneural networks. In Proceedings of the 34th\nInternational Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August\n2017, volume 70 of Proceedings of Machine\nLearning Research, pages 1321–1330. PMLR.\nJianfeng He, Xuchao Zhang, Shuo Lei, Zhiqian Chen,\nFanglan Chen, Abdulaziz Alhamadani, Bei Xiao,\nand Chang-Tien Lu. 2020. Towards more accurate\nuncertainty estimation in text classiﬁcation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 8362–\n8372. Association for Computational Linguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. 2016. Deep residual learning for image\nrecognition. In 2016 IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2016, Las\nVegas, NV , USA, June 27-30, 2016, pages 770–778.\nIEEE Computer Society.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-\nenhanced bert with disentangled attention.\nIn 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021. OpenReview.net.\nNeil Houlsby, Ferenc Huszar, Zoubin Ghahramani,\nand Máté Lengyel. 2011. Bayesian active learning\nfor classiﬁcation and preference learning. CoRR,\nabs/1112.5745.\nYibo Hu and Latifur Khan. 2021. Uncertainty-aware\nreliable text classiﬁcation. In KDD ’21: The 27th\nACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, Virtual Event, Singapore, August\n14-18, 2021, pages 628–636. ACM.\nSiddhartha Jain, Ge Liu, Jonas Mueller, and David\nGifford. 2020. Maximizing overall diversity for\nimproved uncertainty estimates in deep ensembles.\nProceedings of the AAAI Conference on Artiﬁcial\nIntelligence, 34(04):4264–4271.\n8246\nAlex Kulesza and Ben Taskar. 2012. Determinantal\npoint processes for machine learning. Foundations\nand Trends® in Machine Learning, 5(2–3):123–286.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable\npredictive uncertainty estimation using deep\nensembles. In Proceedings of the 31st International\nConference on Neural Information Processing\nSystems, NeurIPS’17, page 6405–6416, Red Hook,\nNY , USA. Curran Associates Inc.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple uniﬁed framework for detecting out-\nof-distribution samples and adversarial attacks. In\nAdvances in Neural Information Processing Systems\n31: Annual Conference on Neural Information\nProcessing Systems 2018, NeurIPS 2018, December\n3-8, 2018, Montréal, Canada , volume 31, pages\n7167–7177.\nJeremiah Z. Liu, Zi Lin, Shreyas Padhy, Dustin Tran,\nTania Bedrax-Weiss, and Balaji Lakshminarayanan.\n2020. Simple and principled uncertainty estimation\nwith deterministic deep learning via distance\nawareness. In Advances in Neural Information\nProcessing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining\napproach.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher\nPotts. 2011. Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th Annual\nMeeting of the Association for Computational\nLinguistics: Human Language Technologies , pages\n142–150, Portland, Oregon, USA. Association for\nComputational Linguistics.\nAndrey Malinin and Mark J. F. Gales. 2018. Predictive\nuncertainty estimation via prior networks. In\nAdvances in Neural Information Processing Systems\n31: Annual Conference on Neural Information\nProcessing Systems 2018, NeurIPS 2018, December\n3-8, 2018, Montréal, Canada, pages 7047–7058.\nJishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort,\nPhilip H. S. Torr, and Yarin Gal. 2021. Deterministic\nneural networks with appropriate inductive biases\ncapture epistemic and aleatoric uncertainty. CoRR,\nabs/2102.11582.\nAlexander Podolskiy, Dmitry Lipin, Andrey Bout,\nEkaterina Artemova, and Irina Piontkovskaya. 2021.\nRevisiting mahalanobis distance for transformer-\nbased out-of-domain detection. In Thirty-Fifth AAAI\nConference on Artiﬁcial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications\nof Artiﬁcial Intelligence, IAAI 2021, The Eleventh\nSymposium on Educational Advances in Artiﬁcial\nIntelligence, EAAI 2021, Virtual Event, February 2-\n9, 2021, pages 13675–13682. AAAI Press.\nCarl Edward Rasmussen. 2003. Gaussian processes\nin machine learning. In Advanced Lectures\non Machine Learning, ML Summer Schools\n2003, Canberra, Australia, February 2-14, 2003,\nTübingen, Germany, August 4-16, 2003, Revised\nLectures, volume 3176 of Lecture Notes in\nComputer Science, pages 63–71. Springer.\nBurr Settles. 2009. Active learning literature\nsurvey. Computer Sciences Technical Report 1648,\nUniversity of Wisconsin–Madison.\nArtem Shelmanov, Evgenii Tsymbalov, Dmitri\nPuzyrev, Kirill Fedyanin, Alexander Panchenko,\nand Maxim Panov. 2021. How certain is your\nTransformer? In Proceedings of the 16th\nConference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 1833–1840, Online. Association for\nComputational Linguistics.\nYilin Shen, Yen-Chang Hsu, Avik Ray, and Hongxia\nJin. 2021. Enhancing the generalization for\nintent classiﬁcation and out-of-domain detection in\nSLU. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long\nPapers), pages 2443–2453, Online. Association for\nComputational Linguistics.\nLewis Smith and Yarin Gal. 2018. Understanding\nmeasures of uncertainty for adversarial example\ndetection. In Proceedings of the Thirty-Fourth\nConference on Uncertainty in Artiﬁcial Intelligence,\nUAI 2018, Monterey, California, USA, August 6-10,\n2018, pages 560–569. AUAI Press.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA.\nAssociation for Computational Linguistics.\nSunil Thulasidasan, Gopinath Chennupati, Jeff A.\nBilmes, Tanmoy Bhattacharya, and Sarah Michalak.\n2019. On mixup training: Improved calibration and\npredictive uncertainty for deep neural networks. In\nAdvances in Neural Information Processing Systems\n32: Annual Conference on Neural Information\nProcessing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada , pages 13888–\n13899.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\n8247\nLanguage Learning at HLT-NAACL 2003 , pages\n142–147.\nJoost Van Amersfoort, Lewis Smith, Yee Whye Teh,\nand Yarin Gal. 2020. Uncertainty estimation using\na single deep deterministic neural network. In\nProceedings of the 37th International Conference\non Machine Learning , volume 119 of Proceedings\nof Machine Learning Research , pages 9690–9700.\nPMLR.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information\nProcessing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, 4-9\nDecember 2017, Long Beach, CA, USA, pages 5998–\n6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis\nplatform for natural language understanding.\nIn Proceedings of the 2018 EMNLP Workshop\nBlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP , pages 353–355,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R.\nBowman. 2019. Neural network acceptability\njudgments. Transactions of the Association for\nComputational Linguistics, 7:625–641.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. The art of abstention: Selective prediction and\nerror regularization for natural language processing.\nIn Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long\nPapers), pages 1040–1051, Online. Association for\nComputational Linguistics.\nZhiyuan Zeng, Keqing He, Yuanmeng Yan, Zijun Liu,\nYanan Wu, Hong Xu, Huixing Jiang, and Weiran Xu.\n2021. Modeling discriminative representations for\nout-of-domain detection with supervised contrastive\nlearning. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics\nand the 11th International Joint Conference on\nNatural Language Processing (Volume 2: Short\nPapers), pages 870–878, Online. Association for\nComputational Linguistics.\nXuchao Zhang, Fanglan Chen, Chang-Tien Lu, and\nNaren Ramakrishnan. 2019. Mitigating uncertainty\nin document classiﬁcation. In Proceedings of\nthe 2019 Conference of the North American\nChapter of the Association for Computational\nLinguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 3126–\n3136, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n8248\nA Dataset Statistics, Hyperparameter Values, and Hardware Conﬁguration\nDatasets Train Test # Labels\nMRPC 3.7K 0.4K 2\nCoLA 8.6K 1.0K 2\nSST-2 67.3K 0.9K 2\nCoNLL-200314.0K/203.6K3.5K/46.4K9\nTable 4: Dataset statistics. The table presents the number of sequences for the training and test parts of the datasets.\nFor CoNLL-2003, the table presents both the number of sequences and tokens because for NER, we evaluate both\nsequence-level and token-level UE scores. For the datasets from the GLUE benchmark (MRPC, CoLA, SST-2),\nwe used the available validation set as the test set.\nCPU 2 Intel Xeon Platinum 8168, 2.7 GHz\nCPU Cores 24\nGPU NVIDIA Tesla v100 GPU\nGPU Memory 32 GB\nTable 5: Hardware conﬁguration used in experiments.\nDataset Reg. Type Spect. Norm.Objective ScoreReg. Lambda Margin Learning Rate Num. Epochs Batch Size Weight Decay\nCoLA – 1.0 0.876 - - 3e-5 15 32 1e-1CoLA – - 0.88 - - 1e-5 8 4 1e-1CoLA CER 0.4 0.88 1.0 - 3e-5 11 32 1e-1CoLA CER - 0.882 1e-2 - 9e-6 7 4 1e-2CoLA Metric 0.4 0.868 1e-2/1.0 0.1 3e-5 11 32 1e-1CoLA Metric - 0.878 1e-2/2e-2 0.25 9e-6 12 4 1e-1CoLA MSD - 0.877 1e-1/6e-3 0.55 3e-5 7 64 1e-2\nMRPC – 1.0 0.858 - - 3e-5 11 32 1e-1MRPC – - 0.867 - - 5e-5 12 32 1e-1MRPC CER 3.0 0.871 1.0 - 3e-5 12 4 0MRPC CER - 0.871 2e-1 - 5e-5 7 16 1e-2MRPC Metric 0.4 0.845 2e-3/1e-1 0.01 3e-5 10 32 0MRPC Metric - 0.844 1e-2/1.0 0.1 3e-5 11 32 1e-1MRPC MSD - 0.871 1e-1/6e-3 0.5 3e-5 11 8 1e-2\nSST-2 – 0.8 0.939 - - 5e-5 7 64 1e-2SST-2 – - 0.936 - - 1e-5 15 64 1e-1SST-2 CER 0.8 0.938 1.0 - 3e-5 14 16 1e-1SST-2 CER - 0.938 2e-2 - 3e-5 5 64 0SST-2 Metric 2.0 0.939 8e-3/2e-2 10.0 3e-5 5 64 0SST-2 Metric - 0.941 8e-3/2e-2 10.0 3e-5 5 64 0SST-2 MSD - 0.942 1e-1/6e-3 0.55 3e-5 7 64 1e-2\nCoNLL-2003 – 3.0 0.922 - - 5e-5 13 8 1e-2CoNLL-2003 – - 0.909 - - 5e-5 6 8 1e-2CoNLL-2003 CER 1.0 0.913 1e-1 - 5e-5 13 8 1e-2CoNLL-2003 CER - 0.912 2e-3 - 2e-5 15 16 1e-2CoNLL-2003 Metric 3.0 0.911 6e-3/1e-3 0.05 5e-5 15 8 0CoNLL-2003 Metric - 0.909 1e-3/1e-1 0.025 5e-5 13 8 1e-2CoNLL-2003 MSD - 0.928 1.0/5e-3 0.95 5e-5 9 8 0\nTable 6: Optimal hyperparameters for the experiments with ELECTRA except SNGP. “Objective score” refers\nto the accuracy score for classiﬁcation / F1-score for sequence tagging on the validation sample. For the metric\nregularization the reg. lambda column contains λ and ελ parameters. For the MSD method, the reg. lambda\ncolumn contains λ1 and λ2 parameters and the margin column contains Ω parameter. We select hyperparameter\nvalues from the following pre-deﬁned list:\nReg. lambda (λ) (and also (λ1) and (λ2)): [1e-3, 2e-3, 3e-3, 5e-3, 6e-3, 8e-3, 1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 1];\nReg. lambda (for metric regularization): [1e-2, 2.5e-2, 5e-2, 1e-1, 2.5e-1, 5e-1, 1.0, 2.5, 5.0, 10.0];\nReg. ελ: [1e-3, 2e-3, 3e-3, 5e-3, 6e-3, 8e-3, 1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 1];\nMargin (γ): [1e-2, 2.5e-2, 5e-2, 1e-1, 2.5e-1, 5e-1, 1.0, 2.5, 5.0, 10.0];\nOmega (Ω): [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0];\nSpect. Norm.: [0.4, 0.6, 0.8, 1.0, 2.0, 3.0];\nLearning rate: [5e-6, 6e-6, 7e-6, 9e-6, 1e-5, 2e-5, 3e-5, 5e-5, 7e-5, 1e-4];\nNum. of epochs: {n∈N|2 ≤n≤15};\nBatch size: [4, 8, 16, 32, 64];\nWeight decay: [0, 1e-2, 1e-1].\n8249\nDataset Objective Score Learning Rate Num. Epochs Batch Size Weight Decay\nCoLA 0.879 6e-6 10 16 0\nMRPC 0.867 3e-5 9 16 0\nSST-2 0.917 2e-5 6 8 1e-1\nCoNLL-2003 0.887 3e-5 85 4 1e-1\nTable 7: Optimal hyperparameters for the experiments with SNGP and ELECTRA. For text classiﬁcation datasets,\nwe use the same pre-deﬁned list of possible values for each hyperparameter. For CoNLL-2003, the following value\nranges are used for the number of epochs and learning rate:\nLearning rate: [9e-6, 1e-5, 2e-5, 3e-5, 5e-5, 7e-5, 1e-4];\nNum. of epochs: {n∈N|10 ≤n≤100}.\nDataset Reg. Type Spect. Norm.Objective ScoreReg. Lambda Margin Learning Rate Num. Epochs Batch Size Weight Decay\nCoLA – 0.4 0.854 - - 7e-6 8 4 1e-1CoLA – - 0.86 - - 7e-6 13 4 0CoLA CER 0.4 0.857 5e-2 - 2e-5 11 32 1e-2CoLA CER - 0.854 2e-1 - 9e-6 15 16 1e-2CoLA Metric 0.8 0.86 1.0/3e-3 0.1 6e-6 11 4 1e-2CoLA Metric - 0.862 8e-3/6e-3 0.025 1e-5 12 4 1e-1CoLA MSD - 0.857 5e-2/3e-3 0.65 3e-5 12 32 0\nMRPC – 0.8 0.879 - - 9e-6 11 16 1e-1MRPC – - 0.889 - - 3e-5 12 4 1e-1MRPC CER 0.6 0.88 6e-3 - 2e-5 15 16 0MRPC CER - 0.88 1.0 - 2e-5 10 16 1e-2MRPC Metric 1.0 0.883 8e-3/5e-2 2.5 2e-5 14 16 1e-1MRPC Metric - 0.885 6e-3/1.0 5.0 9e-6 13 8 1e-1MRPC MSD - 0.876 1e-2/2e-2 0.5 9e-6 12 8 1e-1\nSST-2 – 0.6 0.901 - - 9e-6 11 8 1e-1SST-2 – - 0.906 - - 3e-5 5 16 1e-2SST-2 CER 0.6 0.902 1.0 - 7e-6 12 16 1e-2SST-2 CER - 0.902 1e-1 - 6e-6 6 4 0SST-2 Metric 0.6 0.902 6e-3/5e-3 0.025 5e-5 6 64 1e-1SST-2 Metric - 0.902 6e-3/8e-3 0.05 7e-6 8 16 1e-1SST-2 MSD - 0.929 1e-2/3e-3 0.95 1e-5 11 4 1e-2\nCoNLL-2003 – 1.0 0.897 - - 5e-5 3 4 1e-2CoNLL-2003 – - 0.902 - - 5e-5 12 32 0CoNLL-2003 CER 1.0 0.901 5e-2 - 1e-4 10 16 0CoNLL-2003 CER - 0.899 2e-1 - 2e-5 13 4 1e-1CoNLL-2003 Metric 2.0 0.898 2e-3/5e-3 5.0 7e-5 12 8 1e-2CoNLL-2003 Metric - 0.908 2e-2/1e-1 0.5 3e-5 14 8 1e-2CoNLL-2003 MSD - 0.935 1e-1/5e-3 0.95 3e-5 15 4 1e-1\nTable 8: Optimal hyperparameters for all the experiments with DeBERTa except SNGP. “Objective score\" refers\nto the accuracy score for classiﬁcation / F1-score for sequence tagging on the validation sample. For the metric\nregularization the reg. lambda column contains λ and ελ parameters. For the MSD method, the reg. lambda\ncolumn contains λ1 and λ2 parameters and the margin column contains Ω parameter. We select hyperparameter\nvalues from the following pre-deﬁned list:\nReg. lambda (λ) (and also (λ1) and (λ2)): [1e-3, 2e-3, 3e-3, 5e-3, 6e-3, 8e-3, 1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 1];\nReg. lambda (for metric regularization): [1e-2, 2.5e-2, 5e-2, 1e-1, 2.5e-1, 5e-1, 1.0, 2.5, 5.0, 10.0];\nReg. ελ: [1e-3, 2e-3, 3e-3, 5e-3, 6e-3, 8e-3, 1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 1];\nMargin (γ): [1e-2, 2.5e-2, 5e-2, 1e-1, 2.5e-1, 5e-1, 1.0, 2.5, 5.0, 10.0];\nOmega (Ω): [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0];\nLearning rate: [5e-6, 6e-6, 7e-6, 9e-6, 1e-5, 2e-5, 3e-5, 5e-5, 7e-5, 1e-4];\nNum. of epochs: {n∈N|2 ≤n≤15};\nBatch size: [4, 8, 16, 32, 64];\nWeight decay: [0, 1e-2, 1e-1].\n8250\nDataset Method Dropout RatioCommittee SizeMax. Frac.Kernel Type DDPP\nMask Pool Size\nDDPP\nKernel Type\nCoLA DDPP (+OOD) - 50 0.45 corr. 100 -\nCoLA DDPP (+DPP) - 50 0.4 corr. 100 RBF\nCoLA MC dropout 0.1 20 - - - -\nCoLA Deep Ensemble - 5 - - - -\nMRPC DDPP (+OOD) - 50 0.4 corr. 100 -\nMRPC DDPP (+DPP) - 50 0.55 corr. 100 RBF\nMRPC MC dropout 0.1 20 - - - -\nMRPC Deep Ensemble - 5 - - - -\nSST-2 DDPP (+OOD) - 50 0.35 corr. 100 -\nSST-2 DDPP (+DPP) - 50 0.45 corr. 100 RBF\nSST-2 MC dropout 0.1 20 - - - -\nSST-2 Deep Ensemble - 5 - - - -\nCoNLL-2003DDPP (+OOD) - 20 0.6 corr. 100 -\nCoNLL-2003DDPP (+DPP) - 20 0.6 corr. 100 RBF\nCoNLL-2003MC dropout 0.1 20 - - - -\nCoNLL-2003Deep Ensemble - 5 - - - -\nTable 9: Optimal hyperparameter values for UE methods based on MC dropout and deep ensemble with the\nELECTRA model. These parameters denote the following:\nDropout Ratio – probability of a neuron to be zeroed during inference in a dropout layer;\nCommittee Size – a number of ensemble elements or stochastic forward passes in the MC dropout;\nMax. Frac. – a maximum number of active neurons in a DPP mask;\nKernel Type – type of a kernel in a DPP mask;\nDDPP Mask Pool Size – a number of masks in a pool, from which DDPP selects a diverse set of masks;\nDDPP Kernel Type – a type of a kernel for a DDPP mask.\nDataset Method Dropout RatioCommittee SizeMax. Frac.Kernel Type DDPP\nMask Pool Size\nDDPP\nKernel Type\nCoLA DDPP (+OOD) - 50 0.45 corr. 100 -\nCoLA DDPP (+DPP) - 50 0.6 corr. 100 RBF\nCoLA MC dropout 0.1 20 - - - -\nCoLA Deep Ensemble - 5 - - - -\nMRPC DDPP (+OOD) - 50 0.45 corr. 100 -\nMRPC DDPP (+DPP) - 50 0.6 corr. 100 RBF\nMRPC MC dropout 0.1 20 - - - -\nMRPC Deep Ensemble - 5 - - - -\nSST-2 DDPP (+OOD) - 50 0.45 corr. 100 -\nSST-2 DDPP (+DPP) - 50 0.6 corr. 100 RBF\nSST-2 MC dropout 0.1 20 - - - -\nSST-2 Deep Ensemble - 5 - - - -\nCoNLL-2003DDPP (+OOD) - 20 0.45 corr. 100 -\nCoNLL-2003DDPP (+DPP) - 20 0.3 corr. 100 RBF\nCoNLL-2003MC dropout 0.1 20 - - - -\nCoNLL-2003Deep Ensemble - 5 - - - -\nTable 10: Optimal hyperparameter values for UE methods based on MC dropout and deep ensemble with the\nDeBERTa model. These parameters denote the following:\nDropout Ratio – probability of a neuron to be zeroed during inference in a dropout layer;\nCommittee Size – a number of ensemble elements or stochastic forward passes in the MC dropout;\nMax. Frac. – a maximum number of active neurons in a DPP mask;\nKernel Type – type of a kernel in a DPP mask;\nDDPP Mask Pool Size – a number of masks in a pool, from which DDPP selects a diverse set of masks;\nDDPP Kernel Type – a type of a kernel for a DDPP mask.\n8251\nB Additional Experimental Results with DeBERTa\nMethod Reg.TypeUEScore MRPC SST-2 CoLA CoNLL-2003 (token level)CoNLL-2003 (seq. level)RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓\nMC - SMP 15.06±3.931.85±0.4013.59±2.300.91±0.1455.17±3.762.62±0.16 4.91±0.95 0.07±0.0114.29±3.501.74±0.37MC CERPV 11.53±2.731.42±0.2412.75±3.890.85±0.1956.65±3.272.65±0.14 5.10±1.73 0.07±0.0213.69±2.991.79±0.39MC CERBALD11.38±2.661.42±0.2312.90±4.150.86±0.1957.62±3.882.68±0.15 5.21±1.67 0.08±0.0214.21±2.941.82±0.36MC CERSMP 12.30±3.191.48±0.2712.26±3.040.85±0.1755.28±3.202.59±0.13 4.56±1.54 0.07±0.0213.91±3.081.79±0.39MC metricSMP 15.18±4.581.72±0.3111.93±1.870.81±0.1362.89±7.572.75±0.21 5.91±1.24 0.09±0.0215.05±3.641.80±0.31Deep Ensemble- PV 18.81±5.692.01±0.4012.19±2.310.71±0.0564.80±2.712.80±0.10 3.76±1.71 0.05±0.0211.38±2.241.40±0.33Deep Ensemble- SMP 14.32±3.511.77±0.3010.83±0.940.70±0.0458.03±0.902.70±0.07 3.23±1.56 0.05±0.0311.77±2.421.40±0.32MSD MSDDS 12.79±0.811.80±0.0812.90±1.550.90±0.0853.43±4.722.60±0.20 7.01±1.94 0.10±0.0215.10±3.451.84±0.36\nDDPP (+DPP) (ours)- PV 18.12±2.532.36±0.2718.41±3.571.20±0.1969.81±7.823.40±0.29 5.56±1.51 0.09±0.0215.63±4.971.90±0.52DDPP (+DPP) (ours)- SMP 18.13±3.272.30±0.3417.74±4.171.17±0.2468.12±6.343.29±0.23 5.44±1.49 0.08±0.0217.56±4.972.15±0.54DDPP (+DPP) (ours)CERPV 14.80±2.561.88±0.2217.61±7.411.10±0.3273.34±8.083.39±0.39 8.15±3.45 0.12±0.0319.05±4.162.48±0.51DDPP (+DPP) (ours)CERSMP 16.69±5.351.99±0.4516.57±6.351.08±0.3172.15±7.103.29±0.34 6.18±1.78 0.10±0.0220.66±5.062.69±0.61DDPP (+OOD) (ours)- PV 19.64±5.282.45±0.5217.98±3.121.20±0.2168.49±7.773.28±0.32 5.87±1.48 0.09±0.0215.18±4.351.86±0.47DDPP (+OOD) (ours)- SMP 18.86±3.042.37±0.3618.52±3.491.23±0.2365.77±7.823.13±0.35 5.45±1.38 0.08±0.0217.25±4.602.09±0.50DDPP (+OOD) (ours)CERBALD15.59±2.412.07±0.3018.44±4.441.23±0.2571.75±8.223.23±0.36 6.45±1.77 0.10±0.0222.64±5.742.90±0.66DDPP (+OOD) (ours)metricBALD18.96±3.242.30±0.2617.41±4.851.14±0.3194.05±24.274.30±0.75 7.69±3.18 0.10±0.0221.42±2.412.57±0.16MD - MD 14.66±3.651.98±0.4012.51±1.970.86±0.1355.30±4.702.68±0.19 4.83±1.45 0.07±0.0114.43±4.171.75±0.45MD CERMD 13.48±1.241.88±0.1911.67±1.560.85±0.1157.78±3.862.73±0.15 4.78±1.47 0.07±0.0214.69±4.071.87±0.48MD metricMD 12.12±1.421.64±0.1711.81±1.840.85±0.1357.35±4.352.74±0.20 5.42±1.28 0.08±0.0214.51±3.961.70±0.30MD SN (ours)- MD 12.40±1.141.78±0.1811.10±1.030.78±0.0952.49±1.442.42±0.09 5.06±1.22 0.08±0.0114.67±4.001.79±0.19MD SN (ours)CERMD 13.03±1.491.86±0.1810.87±1.520.80±0.1149.47±3.232.36±0.19 5.92±1.18 0.09±0.0116.57±3.261.97±0.30SR CERMP 17.54±5.602.10±0.4116.50±4.661.11±0.2671.28±6.733.22±0.30 5.19±1.34 0.08±0.0219.01±5.092.44±0.64SR metricMP 20.17±5.562.38±0.4915.76±3.481.07±0.2777.90±10.783.33±0.48 6.80±1.23 0.11±0.0221.03±4.852.68±0.57\nSR (baseline)- MP 19.42±3.582.44±0.3317.83±3.891.18±0.2364.05±7.423.05±0.30 5.32±1.36 0.08±0.0117.01±4.442.06±0.39\nTable 11: Comparison of the best results for all methods (DeBERTa model). The computationally intensive\nmethods are at the top of the table; the computationally cheap methods are at the bottom. The best results overall\nare highlighted with the bold font, the best results for computationally cheap methods are underlined.\n0 5 10 15 20 25\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for MRPC\n0. MC, CER, bald\n1. Deep Ensemble, SMP\n2. MSD, MSD, MSD\n3. DDPP (+DPP), CER, PV\n4. MD, metric, MD\n5. MD SN, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP\na) MRPC\n0 5 10 15 20\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for SST2\n0. MC, metric, SMP\n1. Deep Ensemble, SMP\n2. MSD, MSD, MSD\n3. DDPP (+DPP), CER, SMP\n4. MD, CER, MD\n5. MD SN, CER, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP b) SST2\n0 20 40 60 80\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for CoLA\n0. MC, SMP\n1. Deep Ensemble, SMP\n2. MSD, MSD, MSD\n3. DDPP (+OOD), SMP\n4. MD, MD\n5. MD SN, CER, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP c) CoLA\n0 2 4 6 8\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for CoNNL-2003 (token level)\n0. MC, CER, SMP\n1. Deep Ensemble, SMP\n2. MSD, MSD, MSD\n3. DDPP (+DPP), SMP\n4. MD, CER, MD\n5. MD SN, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP d) CoNLL token level\n0 5 10 15 20 25\nRCC-AUC\n8\n7\n6\n5\n4\n3\n2\n1\n0\nRCC-AUC for CoNNL-2003 (seq. level)\n0. MC, CER, PV\n1. Deep Ensemble, PV\n2. MSD, MSD, MSD\n3. DDPP (+OOD), PV\n4. MD, MD\n5. MD SN, MD\n6. SR, CER, MP\n7. SR, metric, MP\n8. SR (baseline), MP e) CoNLL seq. level\nFigure 3: RCC-AUC↓of the best UE methods for the DeBERTa model.\nC Additional Ablation Studies for DDPP\nMethod Reg.TypeUEScore MRPC SST-2 CoLA CoNLL-2003 (token level)CoNLL-2003 (seq. level)RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓ RCC-AUC↓ RPP↓\nDDPP (+DPP) (ours)- PV 22.30±7.152.58±0.6516.70±1.381.12±0.1249.75±3.962.44±0.29 6.12±0.71 0.10±0.01 16.78±2.441.93±0.20DDPP (+DPP) (ours)- BALD23.08±7.002.63±0.6316.08±2.371.05±0.1849.59±5.402.48±0.31 6.39±0.64 0.10±0.01 21.53±4.772.63±0.45DDPP (+DPP) (ours)- SMP 21.79±7.722.57±0.6817.55±3.031.19±0.2347.86±5.512.39±0.31 6.08±0.62 0.10±0.01 17.71±2.772.05±0.23\nDDPP (+OOD) (ours)- PV 22.73±7.452.65±0.5919.05±2.951.29±0.2351.11±12.032.37±0.34 6.32±0.72 0.10±0.01 16.75±2.311.94±0.21DDPP (+OOD) (ours)- BALD23.85±8.392.69±0.5818.27±3.051.22±0.2352.59±12.082.42±0.34 6.59±0.69 0.11±0.01 20.56±3.092.50±0.26DDPP (+OOD) (ours)- SMP 22.31±7.802.60±0.6519.86±3.831.36±0.2950.14±9.732.32±0.30 6.09±0.67 0.10±0.01 17.76±2.752.06±0.23\nDPP - PV 23.96±9.772.63±0.6018.60±3.591.20±0.2353.49±4.302.43±0.26 6.31±0.56 0.10±0.01 16.23±2.231.87±0.21DPP - BALD24.94±10.222.68±0.5819.39±4.991.21±0.3154.59±4.092.49±0.26 6.49±0.56 0.10±0.01 19.09±3.592.27±0.32DPP - SMP 21.83±7.922.59±0.6518.19±3.441.23±0.2551.06±4.512.40±0.28 6.18±0.54 0.10±0.00 17.28±2.531.98±0.21\nTable 12: The comparison of original DPP MC dropout with its two modiﬁcations DDPP MC dropout (ELECTRA\nmodel).\n8252",
  "topic": "Maxim",
  "concepts": [
    {
      "name": "Maxim",
      "score": 0.7704222798347473
    },
    {
      "name": "Transformer",
      "score": 0.5734735727310181
    },
    {
      "name": "Computational linguistics",
      "score": 0.4969344437122345
    },
    {
      "name": "Computer science",
      "score": 0.4814681112766266
    },
    {
      "name": "Artificial intelligence",
      "score": 0.28481313586235046
    },
    {
      "name": "Engineering",
      "score": 0.2843591868877411
    },
    {
      "name": "Philosophy",
      "score": 0.21405363082885742
    },
    {
      "name": "Electrical engineering",
      "score": 0.17467781901359558
    },
    {
      "name": "Epistemology",
      "score": 0.10513311624526978
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}