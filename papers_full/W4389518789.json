{
  "title": "A Causal View of Entity Bias in (Large) Language Models",
  "url": "https://openalex.org/W4389518789",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2622532819",
      "name": "Wenjie Mo",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2097698393",
      "name": "Yiwei Wang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2145226641",
      "name": "Wenxuan Zhou",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2656641051",
      "name": "Muhao Chen",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California",
        "University of California, Davis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3154945374",
    "https://openalex.org/W3120061794",
    "https://openalex.org/W4306670522",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W3212445569",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W4404784155",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W4287890645",
    "https://openalex.org/W4389523675",
    "https://openalex.org/W4385570326",
    "https://openalex.org/W4312516176",
    "https://openalex.org/W3102714038",
    "https://openalex.org/W4283796213",
    "https://openalex.org/W4389518610",
    "https://openalex.org/W3212660220",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3104390324",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4288725442",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3088597222",
    "https://openalex.org/W3017701505",
    "https://openalex.org/W4385574183",
    "https://openalex.org/W4385567096",
    "https://openalex.org/W4385573926",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4307961097",
    "https://openalex.org/W3034224415",
    "https://openalex.org/W3176001432",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3169654142",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3176751053",
    "https://openalex.org/W3035083705",
    "https://openalex.org/W4280518705",
    "https://openalex.org/W4205179624",
    "https://openalex.org/W1568555062",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W2952929029"
  ],
  "abstract": "Entity bias widely affects pretrained (large) language models, causing them to rely on (biased) parametric knowledge to make unfaithful predictions. Although causality-inspired methods have shown great potential to mitigate entity bias, it is hard to precisely estimate the parameters of underlying causal models in practice. The rise of black-box LLMs also makes the situation even worse, because of their inaccessible parameters and uncalibrated logits. To address these problems, we propose a specific structured causal model (SCM) whose parameters are comparatively easier to estimate. Building upon this SCM, we propose causal intervention techniques to mitigate entity bias for both white-box and black-box settings. The proposed causal intervention perturbs the original entity with neighboring entities. This intervention reduces specific biasing information pertaining to the original entity while still preserving sufficient semantic information from similar entities. Under the white-box setting, our training-time intervention improves OOD performance of PLMs on relation extraction (RE) and machine reading comprehension (MRC) by 5.7 points and by 9.1 points, respectively. Under the black-box setting, our in-context intervention effectively reduces the entity-based knowledge conflicts of GPT-3.5, achieving up to 20.5 points of improvement of exact match accuracy on MRC and up to 17.6 points of reduction in memorization ratio on RE.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15173–15184\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nA Causal View of Entity Bias in (Large) Language Models\nFei Wang† Wenjie Mo† Yiwei Wang‡ Wenxuan Zhou† Muhao Chen†♯\n†University of Southern California; ‡University of California, Los Angeles;\n♯University of California, Davis\n{fwang598,jackymo,zhouwenx}@usc.edu; wangyw.evan@gmail.com;\nmuhchen@ucdavis.edu\nAbstract\nEntity bias widely affects pretrained (large) lan-\nguage models, causing them to rely on (biased)\nparametric knowledge to make unfaithful pre-\ndictions. Although causality-inspired methods\nhave shown great potential to mitigate entity\nbias, it is hard to precisely estimate the param-\neters of underlying causal models in practice.\nThe rise of black-box LLMs also makes the sit-\nuation even worse, because of their inaccessible\nparameters and uncalibrated logits. To address\nthese problems, we propose a specific struc-\ntured causal model (SCM) whose parameters\nare comparatively easier to estimate. Building\nupon this SCM, we propose causal interven-\ntion techniques to mitigate entity bias for both\nwhite-box and black-box settings. The pro-\nposed causal intervention perturbs the original\nentity with neighboring entities. This interven-\ntion reduces specific biasing information per-\ntaining to the original entity while still preserv-\ning sufficient semantic information from sim-\nilar entities. Under the white-box setting, our\ntraining-time intervention improves OOD per-\nformance of PLMs on relation extraction (RE)\nand machine reading comprehension (MRC)\nby 5.7 points and by 9.1 points, respectively.\nUnder the black-box setting, our in-context in-\ntervention effectively reduces the entity-based\nknowledge conflicts of GPT-3.5, achieving up\nto 20.5 points of improvement of exact match\naccuracy on MRC and up to 17.6 points of re-\nduction in memorization ratio on RE.1\n1 Introduction\nEntity bias (Longpre et al., 2021; Wang et al., 2022;\nXu et al., 2022; Peng et al., 2020; Qian et al., 2021b;\nHermann et al., 2015) refers to an undesirable phe-\nnomenon where models overly rely on prediction\nshortcuts triggered by specific entities to make spu-\nrious predictions. For example, given the sentence\n“Bill Gates went to Microsoft Building 99, ”models\n1Our code is available at https://github.com/\nluka-group/Causal-View-of-Entity-Bias\nAssume subject_entity can be any of Bill Gates, Jeff Bezos, \nand Steve Jobs,  while object_entity can be any of Google, \nMicrosoft, and Meta.\nContext: subject_entity went to object_entity Building 99.\nQuestion: What's the relation between subject_entity  and \nobject_entity in the given context? \nOption: founder, visitor.\nAnswer with one word: visitor (GPT-3.5)  ✔\nContext: Bill Gates went to Microsoft Building 99.\nQuestion: What's the relation between Bill Gates and \nMicrosoft in the given context?\nOption: founder, visitor.\nAnswer with one word: founder (GPT-3.5)  ❌\nFigure 1: An example of entity bias in GPT-3.5. Our\nin-context intervention mitigates the conflicts between\nparametric knowledge and contextual knowledge.\nmay be misled by their memory of the entities Bill\nGates and Microsoft, saying the relation between\nthem in this context is founder rather than visitor,\nas shown in Fig. 1. Recent studies show that entity\nbias widely affects pretrained (large) language mod-\nels (LLMs; Longpre et al. 2021; Yan et al. 2022;\nZhou et al. 2023). These models have a tendency\nto disregard contextual information that contradicts\nor is infrequently reported in the pretrained corpus,\nwhile excessively relying on (biased) parametric\nknowledge (Longpre et al., 2021) to make unfaith-\nful predictions and perpetuate bias.\nPrior studies have proposed multiple causality-\ninspired methods to mitigate entity bias (Zhang\net al., 2017; Nan et al., 2021; Wang et al., 2022; Zhu\net al., 2022).2 Despite their potential, the causal\nmodels underlying these methods are flawed in\npractice, primarily because of imprecise parame-\nter estimation. For example, some causal models\nnecessitate estimating the probability distribution\n2Although Zhang et al. (2017) do not mention causal the-\nory, the proposed entity masking does follow a relevant princi-\nple to cut off causal links between specific entities and labels.\n15173\nover labels when given a sentence that is devoid\nof entities or contextual information (Zhang et al.,\n2017; Wang et al., 2022). These methods either\nlose predictive information about entities, or are\nprone to erroneous representation without contextu-\nalization. The other critical problem is the difficulty\nof applying these methods to black-box LLMs, of\nwhich parameters are inaccessible and logits are\nuncalibrated.\nTo address the aforementioned problems, the\nfirst contribution of this paper is a causal analy-\nsis of entity bias mitigation methods(§3.1). We\nexamine and compare the structured causal mod-\nels (SCMs) behind existing methods. We find that,\namong the theoretically equivalent causal models\n(Verma and Pearl, 1990), there exists a specific\nSCM whose parameters are comparatively easier\nto estimate. As shown in Fig. 2, the proposed SCM\nonly requires to intervene input entities to mitigate\nthe presence of spurious features before passing\nthem to the subsequent neural layers. Moreover, it\nretains the entity type information3 at an appropri-\nate level of granularity without requiring explicit\nentity typing.\nThe second contribution of this paper is a\ntraining-time causal intervention techniquefor\nmitigating entity bias based on the proposed SCM\n(§3.2). Specifically, we identify entities that are\nlikely to share similar predictive information with\nthe given entity. During training, we perturb em-\nbedding of the given entity within a convex hull\nconstructed by embeddings of similar entities. Dur-\ning inference, we represent the entity with the cen-\nter of the convex hull. Taking advantage of the\ncontinuous nature of the embedding space, this\nintervention does not rely on models specifically\ntrained on natural language to estimate the label\ndistribution of unnatural text, nor does it sacrifice\npredictive entity or contextual information.\nThe third contribution of this paper is to trans-\nform the training-time intervention into in-context\nintervention for black-box LLMswhose param-\neters are inaccessible, and logits are uncalibrated\n(§3.3). A significant advantage of the proposed\nSCM is that the causal intervention is carried out at\nthe input layer, enabling its implementation within\nan in-context setting. Specifically, we replace enti-\nties with placeholders and define each placeholder\n3Entity type information plays a crucial role in entity-\ndriven tasks. For example, without knowing a more specific\nlocation type, it is impossible to differentiate between relations\nborn_in_city and born_in_country.\nE\nX Y\nCoRE\nE\nX Y\nEntity Substitution\nE\nX MOurs\nX: Raw Text\nE: Entity\nY: LLM Output\nM: LLM Input \nX\nE\nX M\nY\nE\nYX\nE\nX M Y\nFigure 2: Structured causal models revealing entity bias.\nby examples – a set of similar entities. For ex-\nample, we can replace Bill Gates in Fig. 1 with\nsubject_entity and prepend the prompt, “Assume\nthat subject_entity can be any of Steve Jobs, Bill\nGates, and Jeff Bezos\", to the input. This in-context\nintervention can be applied to any black-box LLM\nwithout additional cost.\nExperiments on relation extraction (RE) and ma-\nchine reading comprehension (MRC) show that\nthe proposed causal intervention techniques are ef-\nfective for both white-box and black-box LLMs.\nUnder the white-box setting (§4), our training-\ntime intervention significantly improves out-of-\ndistribution performance of RoBERTa (Liu et al.,\n2019) on RE by 5.7 points and SpanBERT (Joshi\net al., 2020) on MRC by 9.1 points, comparing\nwith the vanilla version. Under the black-box set-\nting (§5), our in-context intervention effectively re-\nduces the entity-based knowledge conflicts (Long-\npre et al., 2021) and improves the task performance\nof GPT-3.5.4 Specifically, our method outperforms\nthe best baseline by up to 20.5 points of exact\nmatch accuracy on MRC and reduces the mem-\norization ratio by up to 17.6 points on RE. Further\nanalyses reveal the crucial role of the number of\nneighboring entities k in balancing the predictive\ninformation and biasing information from entities,\nand the necessity of entity placeholder definition\nfor in-context intervention.\n2 Related Work\nEntity Bias in LLMs.LLMs memorize factual\nknowledge in their parameters during pretrain-\ning (Roberts et al., 2020; Jiang et al., 2020) and\nshow promising results in answering factual ques-\ntions (Petroni et al., 2019; Brown et al., 2020; Wei\n4https://platform.openai.com/docs/models/gpt-3-5\n15174\net al., 2022). However, the parametric knowledge\nmay be inaccurate due to the misinformation in\nthe training corpus (Lin et al., 2022) or outdated\nas the world evolves (Liska et al., 2022; Kasai\net al., 2022). In such scenarios, it is critical for\nLLMs to update their predictions when provided\nwith contextual evidence. However, previous stud-\nies (Longpre et al., 2021; Qian et al., 2021b; Yan\net al., 2022) observe that language models may take\nentities as shortcuts, leading to spurious predictions\nbased solely on parametric knowledge. This bias\nbecomes more prominent when the evidence con-\ntains infrequent or conflicting knowledge compared\nto the training corpus.\nTo mitigate this bias, previous work (Longpre\net al., 2021; Chen et al., 2022; Li et al., 2022; Zhou\net al., 2023) introduces the entity substitution tech-\nnique, which involves constructing counterfactual\ndata by randomly replacing the entities, and up-\ndating the language models either by finetuning or\nin-context learning. Although showing improved\nresults, these techniques are empirical and lack\ntheoretical backgrounds. In this paper, we theo-\nretically analyze the entity bias problem from a\ncausal view. Furthermore, we propose a causal in-\ntervention method that surpasses the performance\nof entity substitution.\nDebiasing with Causal Intervention.LLMs have\nbeen revealed with bias problems, for which liter-\nature has paid much attention in order to mitigate\ntheir adverse effects (Sweeney and Najafian, 2019;\nZhang et al., 2020b; Venkit and Wilson, 2021; Lalor\net al., 2022). Recent debiasing techniques incorpo-\nrate the concept of counterfactual inference, and\nhave been applied in various tasks for bias miti-\ngation (Niu and Zhang, 2021; Qian et al., 2021a;\nWang et al., 2022). One dominant technique is\nbased on causal mediation analysis (Udomcharoen-\nchaikit et al., 2022), which involves decomposing\nthe total effect into pure direct effect and total indi-\nrect effect. In this context, Wang et al. (2022) uti-\nlize total direct effect and total effect to debias the\nrelation extraction. Apart from debiasing, causal\nmediation analysis can be used to analyze biases in\nLLMs (Vig et al., 2020; Finlayson et al., 2021).\nIn addition to intervening causal mediator, pre-\nvious studies have also explored confounder anal-\nysis (Keith et al., 2020; Qian et al., 2021a; Feder\net al., 2022; Weld et al., 2022). A confounder is a\nvariable that influences both the input and the out-\nput, causing a spurious correlation between them.\nTypically, the de-confounder process applies the\ndo-calculus (Pearl, 2012) to compute the prediction\nassuming that the value of the confounder variable\nis not the observed one but follows its natural dis-\ntribution (Zhang et al., 2020a; Tian et al., 2022).\nOur approach is also based on confounder analysis.\nWhile nearly all the aforementioned approaches\nrequest a white-box accessibility of the model with\nat least logits of predictions, this work represents a\npilot study of deconfounder method that applies to\npurely black-box LLMs.\n3 Method\nIn this section, we first analyze methods for miti-\ngating entity bias in a causal view and propose an\neasy-to-estimate SCM as a theoretical basis (§3.1).\nBased on the proposed SCM, we design a training-\ntime intervention technique for white-box LLMs\n(§3.2) and an in-context intervention technique for\nblack-box LLMs (§3.3).\n3.1 Causal Analysis of Entity Bias\nTo compare existing methods in the same context,\nwe analyze the structured causal models (SCMs)\nbehind them. Fig. 2 shows two typical SCMs for\nentity bias mitigation methods, where X refers to\nthe raw input, E refers to entities, and Y refers\nto the label. The links X →Y ←E show that\nLLMs rely on both predictive information from\nthe whole input and the biasing information from\nspecific entities to make the prediction. The links\nE →X and X →E assume that the context is\nwritten down with the entity in mind or vice versa.\nAs discussed by Verma and Pearl (1990), we cannot\ndifferentiate between these two directions merely\nbased on statistical observations. Indeed, the two\nSCMs with opposite links between X and E are\nequivalent according to the Bayes’ theorem:\nP(X)P(E|X)P(Y |X, E)\n=P(Y, X, E)\n=P(E)P(X|E)P(Y |X, E)\nAs revealed by these SCMs, entity bias exists in\nLLMs because entities serve as either confounders\nor mediators. Thus, the bias can be mitigated\nthrough causal intervention, such as backdoor ad-\njustment\nP(Y |do(X)) =\n∑\nE\nP(Y |X, E)P(E),\n15175\nOriginal \nEntity\nNeighboring \nEntity\nCenter of \nConvex Hull\nSteve Jobs\nBill Gates Jeff Bezos\nFounder of Apple\nCreator of iPhone\n…\nCo-Founder of Microsoft\nBorn in Seattle\n…\nFounder of Amazon\nFounder of Blue Origin\n…\nPerson\nBusinessman\nPredictiveBiasing\nFigure 3: Left: Training-time intervention with k = 4. Right: Example of predictive and biasing information.\nwhich eliminates the influence of a specific vari-\nable (in this context, E) by assigning values to this\nvariable. However, previous SCM-based debiasing\nmethods exhibit divergent performances, since they\nestimate different (conditional) probabilities using\ndifferent surrogates when performing the causal\nintervention. For example, counterfactual analysis\nby Wang et al. (2022) estimates and deducts the\nbiasing effect of entities on labels by masking the\ncontext, while Zhang et al. (2017) and Longpre\net al. (2021) directly remove the effect of entities\nby entity masking or substitution. None of them es-\ntimates the causal effects of entity names precisely,\ndue to the highly complex architectures of LLMs,\nwhich account for their unsatisfactory performance\non mitigating entity bias.\nIn this work, we consider the SCM in Fig. 2,\nwhose parameters are much easier to estimate in\npractice. Since most LLMs follow a sequential\nstructure by stacking neural layers, mitigating the\nentity bias in one layer will also mitigate the entity\nbias in subsequent layers. The underlying logic\nis simple – if we block the spurious features in\nthe input, there will be no spurious correlations\nto capture. Therefore, we propose to mitigate the\nentity bias in the input layer M, which could be\nan embedding layer or a prompt layer. Obviously,\nP(M|X, E) can be estimated more accurately and\nefficiently than P(Y |X, E), because there is no\nneed to run the whole model, ensuring less error\npropagation and computational cost. To further\nimprove the estimation by retaining as much pre-\ndictive information as possible, we propose to es-\ntimate P(M|do(X)) by perturbing the entity with\nsimilar entities rather than masking it. In the fol-\nlowing sections, we will show how to realize the\nproposed causal intervention on both white-box\nand black-box LLMs.\n3.2 Training-time Intervention\nFor white-box models of which the parameters are\naccessible, we can effectively address their internal\nbias through training-time intervention. In the case\nof entity bias identified by the proposed SCM, we\nrealize the causal intervention by perturbing the in-\nput entities or entity tokens using their neighboring\ncounterparts in the embedding space, as shown in\nFig. 3 (Left). For each entity presented in the input\ntext, we first find its topk nearest neighbors accord-\ning to embedding distance. Then we construct the\nsmallest convex hull5 to cover the original entity\nand neighboring entities. Due to the continuous\nnature of the embedding space, the embeddings\nwithin the convex hull approximately represent the\nsame predictive information as a whole. The entity-\nspecific biasing information, which has the poten-\ntial to trigger spurious shortcuts, gradually dimin-\nishes from the original entity towards the border of\nthe convex hull.\nDuring training, we introduce perturbations to\nthe entity embedding by replacing it with a random\nembedding selected from within the convex hull.\nIn this way, the convex hull bounded the predictive\ninformation, while random sampling further intro-\nduces noises and increases the diversity of data for\nrobust training. During inference, we replace the\noriginal entity embedding with the center of the\nconvex hull, in order to balance the trade-off be-\ntween predictive and biasing information. Fig. 3\n(Right) provides an example of the information pre-\nserved through such intervention. By replacing the\nentity Bill Gates with the center of the convex hull,\nencompassed by its neighboring entities, such as\nSteve Jobs and Jeff Bezos, we effectively retain the\n5This convex hull-bounded perturbation is inspired by\nDong et al. (2021), where perturbation within a convex hull\nformed by synonyms is used to improve model robustness\nagainst word substitutions.\n15176\nFigure 4: In-context intervention for black-box LLMs. We take relation extraction as an example.\nshared predictive information (e.g., person), while\nmitigating the biasing information (e.g., founder of\nMicrosoft). That is to say, the convex hull-bounded\nperturbation serves as an effective estimation of\nP(M|do(X)).\n3.3 In-context Intervention\nThe rise of Web services powered by black-box\nLLMs, such as GPT-3.5, introduces new chal-\nlenges for mitigating entity bias, demanding debi-\nasing methods that do not require accessible model\nweights and prediction logits. As discussed in §3.1,\na key advantage of our SCM is that the decon-\nfounder operation is merely on the input layer. In\nthe context of black-box LLMs, the input is the\nuser-provided prompt. Thus, we perform the causal\nintervention solely through modifying prompts to\nresolve entity bias. We propose a four-step (test-\ntime) in-context intervention technique for black-\nbox LLMs. Fig. 4 shows the whole process.\nFirst, we replace the original entity mention\nin the input with abstract placeholders (e.g.,\n[ENTITY]). This step effectively mitigates any bias-\ning information from the original entity names, be-\ncause the placeholders are semantic-neutral. How-\never, this step also eliminates predictive informa-\ntion from entities. We show in §5.3 that, without\nproper definition for the placeholder, models can\neasily fail to answer questions. In the next two\nsteps, we construct definitions to provide predictive\ninformation for each placeholder while introducing\nminimal additional biasing information. Second,\nwe query the LLM to name k entities similar to the\noriginal one (e.g., Eo).6 These generated entities\n(e.g., Ea and Eb) present similar predictive infor-\nmation as the original entity, and are able to fulfill\nthe same function as neighboring entities in §3.2.\nThird, we define the placeholder with the original\nentity and generated entities. For example, we can\nverbalize the definition as “Assume [ENTITY] can\nbe any of Eo, Ea and Eb”. This definition encour-\nages the LLM to find common properties of given\nentities rather than relying on biasing information\nof one specific entity. The resulting placeholder\nalong with its definition serves as an effective esti-\nmation of P(M|do(X)). Finally, we prepend the\nplaceholder definition to the modified context and\nquestion, and query the LLM with the new prompt.\nThis four-step adjustment ensures that the result-\ning prompt is free of specific biasing information\npertaining to the original entity while still preserv-\ning sufficient predictive information by considering\ngiven entity examples as a whole.\n4 White-Box Experiments\nIn this section, we evaluate our training-time inter-\nvention under the white-box setting.\n4.1 Experimental Setup\nDatasets and Metrics.We evaluate our methods\non relation extraction (RE) and machine reading\ncomprehension (MRC). For both tasks, we fine-\ntune models on an in-distribution (ID) training\nset and evaluate models on both ID and out-of-\ndistribution (OOD) test sets. For RE, we adopt\nTACRED (Zhang et al., 2017) as the ID dataset and\n6Here, we rely on the entity knowledge possessed by\nLLMs. However, it is possible to replace the LLM with exter-\nnal databases or tools in this step.\n15177\nRE (F1) MRC (EM)\nID OOD ∆ ID OOD ∆\nVanilla Model 71.1 ±0.9 62.3±0.6 −12.4% 79.1†\n±0.1 63.1†\n±0.8 −20.2%\n+ Continual Pretraining (Yan et al., 2022)∗ - - - 79.6†\n±0.6 65.9†\n±1.1 −17.2%\n+ CoRE (Wang et al., 2022) 71.3±0.3 61.2±0.6 −14.2% - - -\n+ Entity Mask (Zhang et al., 2017) 61.4 ±0.5 61.9±0.5 +0.9% 75.7±0.6 62.9±0.4 −16.9%\n+ Entity Substitution (Longpre et al., 2021) 66.6 ±0.6 65.8±0.3 −1.2% 76.4±0.8 70.8±1.5 −7.3%\n+ Ours 70.8 ±0.3 68.0±0.3 −3.9% 77.0±0.7 72.2±0.5 −6.2%\nTable 1: Results under white-box setting. We report the average F1/EM score and standard deviation of three\nruns. ∆ shows the relative performance change between ID and OOD. The best number of each column is in bold.\n∗Continual pretraining is not directly comparable to finetuning methods. †Numbers copied from Yan et al. (2022).\nEntRED (Wang et al., 2023) as the OOD dataset,\nand report micro-F1 score. In both datasets, entities\nin each sentence are given. For MRC, we adopt\nTriviaQA (Joshi et al., 2017) as the ID dataset and\nits answer-substituted version (Yan et al., 2022)\nas the OOD dataset, and report exact match (EM)\nscore. Following Yan et al. (2022), we hold out\n10% of the training data for development and evalu-\nate models on the original development set. We use\nthe DBName version of their OOD dataset. For all\nmetrics, we report the average score with standard\ndeviation of three runs.\nBaselines. We compare our methods with the fol-\nlowing baselines. Entity Mask (Zhang et al., 2017)\nmasks the subject and object entities in the sentence\nwith special tokens. Entity Substitution (Longpre\net al., 2021) randomly selects an entity of the same\ntype to substitute the original entity. CoRE (Wang\net al., 2022) applies counterfactual inference by\ncomputing the difference between the prediction\nmade with the entire sentence and the prediction\nmade with only the entities observed. Continual\nPretraining (Yan et al., 2022) introduces an inter-\nmediate pretraining stage to the backbone model\nwith the objective of recovering masked entities.\nImplementation Details. For RE, we ap-\nply RoBERTa (Liu et al., 2019) as the back-\nbone model following previous works (Zhou and\nChen, 2022; Wang et al., 2022). We use the\nentity_marker_punct input format from Zhou\nand Chen (2022) in main experiments, in order to\nmitigate the impact of explicit entity type informa-\ntion on our analysis of entity bias. For MRC, we ap-\nply SpanBERT (Joshi et al., 2020) as the backbone\nmodel following Yan et al. (2022). Since entities\nare not given in MRC datasets, we use the same\nnamed entity recognition tool used by Yan et al. to\nextract entities. Since the detected entities could\nbe noisy and incomplete, we perform our method\nupon answer-substituted training set ensuring all\nanswer entities are perturbed as strong as Entity\nSubstitution. Since RoBERTa and SpanBERT lack\nentity-level embeddings, we apply our causal inter-\nvention to each token embedding within the entity\nmention instead. To construct convex hull, We se-\nlect neighboring tokens based on their Euclidean\ndistance to the original token in the embedding\nspace. For both tasks, we perform training-time in-\ntervention on each entity token with k = 3. While\nfurther data augmentation is always possible, for\na fair comparison, we finetune all the models with\nthe same amount of data. More implementation\ndetails are in Appx. §A.1.\n4.2 Results\nAs shown in Tab. 1, the vanilla RoBERTa and Span-\nBERT experiences significant declines in perfor-\nmance on RE (-12.4%) and MRC (-20.2%) when\nevaluated on OOD test sets. For both tasks, the\nOOD test set exhibits lower entity bias, achieving\nbetter performance on it suggests that the model\nrelies less on entity bias as a predictive factor.\nCoRE and Continual Pretraining are the only\nbaselines that improve the ID performance. CoRE\nleads to a slight performance decrease on the OOD\ntest set of RE in exchange,7 while Continual Pre-\ntraining further increases the OOD performance\non MRC. Entity Mask successfully narrow down\nor even reverse the relative performance drop un-\nder OOD setting on the two tasks. However, its\nabsolute performance decreases significantly due\n7This is because CoRE is designed for a class-balanced\nsetting, but this experiment emphasizes the performance on\nthe raw class distribution. Moreover, we search its bias miti-\ngation weight on the ID development set, which has a notably\ndifferent entity distribution compared with the OOD test set.\n15178\n1 2 3 4 5 6 7 8 9 10\nK\n60\n65\n70\n75F1\nID (Ours)\nID (Vanilla)\nOOD (Ours)\nOOD (Vanilla)\nFigure 5: F1 score of training-time intervention with\ndifferent k on RE.\nto the loss of predictive information from entities.\nMoreover, its effectiveness is dependent on the task\nproperty. Unlike MRC, entities are given and are\nnot answers in RE, so the gap between ID and OOD\nperformance of Entity Mask are much smaller. En-\ntity Substitution stands out among all the baselines\nin terms of the OOD performance, with an absolute\nimprovement of 3.5 points on RE and 7.7 points on\nMRC. However, its ID performance suffers a lot\nfrom the distribution shift of entities during train-\ning.\nOur training-time intervention achieves the best\nOOD performance, with an absolute improvement\nof 2.2 points on RE and 1.4 points on MRC com-\npared with Entity Substitution. At the same time, its\nID performance is also better. These results show\nthat our method mitigates entity bias more effec-\ntively without losing much predictive information.\nIn other words, the proposed method represents a\nbetter way to estimate the parameters of the pro-\nposed SCM accurately.\n4.3 Analysis\nTo provide a comprehensive understanding of our\ntraining-time intervention, we further conduct anal-\nyses on RE.\nEffect ofk. The number of neighbors, k, plays a\ncrucial role in balancing the predictive information\nand biasing information from entities. To find the\nsweet spot of k, we examine its influence on model\nperformance as shown in Fig. 5. In general, the\nID performance decreases when k increases. As\nthe value of k increases, the resulting convex hull\nbecomes larger, causing the center of the hull to\nmove further away from the original entity. Conse-\nquently, both the predictive information and biasing\ninformation that contribute to ID performance grad-\nually diminish. In contrast, the OOD performance\nis lower when k is too big or too small. When\nk is too big, the same problem under ID setting\nalso happens to the OOD setting. When k is too\nsmall, the biasing information is not effectively mit-\nigated, because the perturbed entity is too close to\nthe original entity.\nEntity Type as Input.Previous experiments in this\nsection do not explicitly input entity information as\nit may disturb the causal analysis. Here, we analyze\nthe effect of entity type information as input. We\nuse the typed_entity_marker_punct input for-\nmat from Zhou and Chen (2022). The ID and OOD\nF1 scores of vanilla RoBERTa model are 74.6 and\n68.9 points, respectively. Our training-time inter-\nvention further improves the ID performance by\n0.7 points and the OOD performance by 2.9 points.\nThese results indicate that information from neigh-\nboring entities is complementary to coarse-grained\nentity type information for precise RE.\n5 Black-Box Experiments\nIn this section, we evaluate our in-context interven-\ntion for mitigating entity bias from LLMs under\nblack-box setting.\n5.1 Experimental Setup\nDatasets. Following Zhou et al. (2023), we\nadopt GPT-3.5 text-davinci-003 as the back-\nbone LLM and evaluate the model performance\nunder a zero-shot setting. We use the RE and\nMRC datasets provided by Zhou et al. (2023). The\nRE dataset is based on Re-TACRED (Stoica et al.,\n2021). Zhou et al. pair each instance’s entities\nwith a randomly sampled context that shares the\nsame entity types but possesses different relations.\nTo mitigate the influence of the label no_relation,\nwhich can also serve as a signal of abstention, we\nfurther filter out all instances whose original or up-\ndated labels are no_relation. The MRC dataset is\nbased on Natural Questions (Kwiatkowski et al.,\n2019). Zhou et al. replace the original answer in\neach instance with a randomly sampled entity of the\nsame type. They only collect instances where the\nLLM can give the correct answer based on the raw\ncontext. Intuitively, LLMs that faithfully capture\ncontextual information should update their answers\nbased on the new context.\nMetrics. We report the F1 score for RE, and EM\nscore for MRC. To align with previous works, we\n15179\nw/o instruction w/ instruction\n0\n10\n20\n30\n40\n50\n6.2\n27.129.6\n37.4\n24.9\n48.650.1 51.3\nMRC (EM )\nw/o instruction w/ instruction\n0\n5\n10\n15\n20\n25\n30\n35 35.2\n17.716.6\n12.511.0\n9.19.8 8.1\nMRC (MR )\nBase\nAttribute\nOpinion\nOurs\nw/o instruction w/ instruction\n50\n55\n60\n65\n70\n75\n65.9\n70.3\n62.7\n71.1\n70.6 71.7\n71.8 72.5\nRE (F1 )\nw/o instruction w/ instruction\n0\n5\n10\n15\n20\n25 24.3 23.8\n25.7 25.6\n21.1\n25.0\n3.5\n12.1\nRE (MR )\nFigure 6: GPT-3.5 results on MRC and RE under black-box setting. We report the EM score on MRC and the F1\nscore on RE, for which higher scores are better. We also report the MR score on both tasks, for which lower scores\nare better. Our in-context intervention performs consistently better than baselines under all settings.\nF1 MR\nMetrics\n0\n10\n20\n30\n40\n50\n60\n70Scores\n71.8\n3.5\n66.7\n4.0\n64.9\n13.8\n37.9\n0.0\nOurs\nw/o original entity\nw/o random order\nw/o definition\nFigure 7: Ablation study of in-context intervention for\nGPT-3.5 on RE.\nalso report the memorization ratio (MR; Longpre\net al. 2021) to measure the model’s ability to update\nanswers based on given contexts.8\nBaselines. We compare our in-context intervention\nwith the methods introduced by Zhou et al. (2023).\nBase prompts directly concatenate the context and\nthe question of each instance as the query. At-\ntribute-based prompts append“in the given context”\nto the question. Opinion-based prompts modified\nthe context to a narrator’s statement by prepending\n“Bob said” to the context, and then query the LLM\nabout the narrator’s opinion by prepending“What’s\nBob’s opinion on” to the question. We evaluate all\nmethods with and without specifically designed\ntask instructions following Zhou et al. (2023).\nImplementation Details.We apply our in-context\nintervention to attribute-based prompts. We adopt\nthe backbone LLM to propose two similar entities\nalong with the original entity to define each place-\nholder. To further eliminate the spurious entity\nmapping, we shuffle the entities for each place-\nholder before verbalization. Details of all prompt\ntemplates used can be found in Appx. §A.2. Since\n8MR = Po\nPo+Ps\n, where Po is the probability that the\nmodel generates the original answer and Ps is the probability\nthat the model updates the answer correctly.\nentities are not given in MRC, we detect named\nentities and replace them with placeholders using\ngpt-3.5-turbo as an external tool. Given the po-\ntential abundance of entities in long contexts, we\ndo not replace entities that exclusively appear in\nthe context.\n5.2 Results\nAs shown in Fig. 6, all methods benefit from care-\nfully designed task instructions in terms of task per-\nformance. The Opinion-based prompt performs the\nbest among all baselines in most cases. Compared\nwith the Base prompt, it significantly improves the\nEM score by 18.7-21.5 points on MRC and the F1\nscore by 0.6-4.7 points on RE. Our in-context inter-\nvention achieves the highest EM/F1 score and the\nlowest MR score under all settings. Specifically,\nwithout task instruction, our in-context intervention\noutperforms the best baseline by 20.5 EM points on\nMRC and reduces the MR score by 17.6 points on\nRE. These results demonstrate the effectiveness of\nour causal intervention for addressing entity-based\nknowledge conflicts in black-box LLMs.\n5.3 Ablation Study\nWe in addition conduct an ablation study on RE\nto provide a comprehensive understanding of our\nmethod, as shown in Fig. 7. When the placeholder\ndefinition is not provided (i.e., w/o definition), no\nentity information, including both biasing and pre-\ndictive information, appears in the input. As a\nresult, it successfully blocks any spurious shortcuts\nwith MR drops to 0. However, the F1 score also\ndrops sharply from 71.8 points to 37.9 points, indi-\ncating that some entity information is essential to\naccurate RE and the LLM cannot understand the\nplaceholders well without their definition.\n15180\nWe further examine the role of original entities\nin the placeholder definition. On the one hand,\nwe remove the original entities from the definition\n(i.e., w/o original entity ). Results show that our\nmethod can still improve F1 while reducing MR.\nThis verifies the effectiveness of using a set of sim-\nilar entities to represent the predictive information\nfrom the original entity. On the other hand, we put\nthe original subject and object entities at the same\nposition (i.e., w/o entity shuffle) in the definition so\nthat the LLM can easily map them. As a result, the\nMR increases significantly, showing that the LLM\ncan find spurious shortcuts even through mapping\nthe subject entity and the object entity from two\nentity sets.\n6 Conclusion\nIn this paper, we analyze the entity bias in LLMs\nfrom a causal view. Building upon an SCM\nwhose parameters are easier to estimate, we pro-\npose training-time causal intervention for white-\nbox LLMs and in-context causal intervention for\nblack-box LLMs. Both intervention techniques per-\nturb the original entity with neighboring entities to\nmitigate spurious correlations between specific en-\ntities and predictions. Experiments on relation ex-\ntraction and machine reading comprehension show\nthat the proposed intervention can effectively re-\nduce the conflicts between parametric knowledge\nand contextual knowledge and significantly im-\nprove the performance of LLMs. Future work can\napply our causal intervention to more LLMs and\ntasks to achieve context-faithful answers.\nAcknowledgement\nWe appreciate the reviewers for their insightful\ncomments and suggestions. Fei Wang is supported\nby the Annenberg Fellowship and the Amazon ML\nFellowship. Wenjie Mo is supported by the USC\nCURVE Fellowship and the Provost’s Research\nFellowship. Wenxuan Zhou and Muhao Chen are\nsupported by the NSF Grant IIS 2105329, the NSF\nGrant ITE 2333736, the DARPA MCS program un-\nder Contract No. N660011924033 with the United\nStates Office Of Naval Research. This work is\nalso supported in part by a Cisco Research Award,\ntwo Amazon Research Awards, and a Keston Re-\nsearch Award. Computing of this work has been\npartly supported by a subaward of NSF Cloudbank\n1925001 through UCSD.\nLimitation\nAlthough we have tried to verify the effectiveness\nof our method under diverse settings, including\ndifferent LLMs, different accessibility of model pa-\nrameters, and different tasks, there are always more\noptions for further investigation, especially nowa-\ndays when more and more LLMs are kept produced.\nConsidering the property of the entity bias issue\nmay vary when it comes to different LLMs and\ndatasets from different domains, future work can\nbuild better benchmark for more comprehensive\nevaluation. In this paper, we only consider zero-\nshot prompting for black-box LLMs, because this\nwill help us to control variables during causal analy-\nsis. However, it is possible to combine the proposed\ncausal intervention with cutting-edge LLM infer-\nence methods, such as in-context learning (Brown\net al., 2020), although the underlying SCM may\nbecome more complex.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nHung-Ting Chen, Michael Zhang, and Eunsol Choi.\n2022. Rich knowledge sources bring complex knowl-\nedge conflicts: Recalibrating models to reflect con-\nflicting evidence. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2292–2307, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nXinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong\nLiu. 2021. Towards robustness against natural lan-\nguage word substitutions. In International Confer-\nence on Learning Representations.\nAmir Feder, Katherine A Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob\nEisenstein, Justin Grimmer, Roi Reichart, Margaret E\nRoberts, et al. 2022. Causal inference in natural lan-\nguage processing: Estimation, prediction, interpreta-\ntion and beyond. Transactions of the Association for\nComputational Linguistics, 10:1138–1158.\nMatthew Finlayson, Aaron Mueller, Sebastian\nGehrmann, Stuart Shieber, Tal Linzen, and Yonatan\nBelinkov. 2021. Causal analysis of syntactic\nagreement mechanisms in neural language models.\narXiv preprint arXiv:2106.06087.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\n15181\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. Advances in neural information\nprocessing systems, 28.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime qa: What’s the answer right now?\narXiv preprint arXiv:2207.13332.\nKatherine Keith, David Jensen, and Brendan O’Connor.\n2020. Text and causal inference: A review of using\ntext to remove confounding from causal estimates.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 5332–\n5344.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–\n466.\nJohn P Lalor, Yi Yang, Kendall Smith, Nicole Fors-\ngren, and Ahmed Abbasi. 2022. Benchmarking inter-\nsectional biases in nlp. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3598–3609.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin\nWang, Michal Lukasik, Andreas Veit, Felix Yu,\nand Sanjiv Kumar. 2022. Large language models\nwith controllable working memory. arXiv preprint\narXiv:2211.05110.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nAdam Liska, Tomas Kocisky, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, D’Autume\nCyprien De Masson, Tim Scholtes, Manzil Zaheer,\nSusannah Young, et al. 2022. Streamingqa: A bench-\nmark for adaptation to new knowledge over time in\nquestion answering models. In International Con-\nference on Machine Learning, pages 13604–13622.\nPMLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nShayne Longpre, Kartik Perisetla, Anthony Chen,\nNikhil Ramesh, Chris DuBois, and Sameer Singh.\n2021. Entity-based knowledge conflicts in question\nanswering. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7052–7063.\nGuoshun Nan, Jiaqi Zeng, Rui Qiao, Zhijiang Guo, and\nWei Lu. 2021. Uncovering main causalities for long-\ntailed information extraction. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9683–9695.\nYulei Niu and Hanwang Zhang. 2021. Introspective\ndistillation for robust question answering. Advances\nin Neural Information Processing Systems, 34:16292–\n16304.\nJudea Pearl. 2012. The do-calculus revisited. In Pro-\nceedings of the Twenty-Eighth Conference on Uncer-\ntainty in Artificial Intelligence, pages 3–11.\nHao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng\nLi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2020.\nLearning from context or names? an empirical study\non neural relation extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 3661–3672.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nChen Qian, Fuli Feng, Lijie Wen, Chunping Ma, and\nPengjun Xie. 2021a. Counterfactual inference for\ntext classification debiasing. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 5434–5445.\nKun Qian, Ahmad Beirami, Zhouhan Lin, Ankita De,\nAlborz Geramifard, Zhou Yu, and Chinnadhurai\nSankar. 2021b. Annotation inconsistency and en-\ntity bias in MultiWOZ. In Proceedings of the 22nd\nAnnual Meeting of the Special Interest Group on Dis-\ncourse and Dialogue, pages 326–337, Singapore and\nOnline. Association for Computational Linguistics.\n15182\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nGeorge Stoica, Emmanouil Antonios Platanios, and\nBarnabás Póczos. 2021. Re-tacred: Addressing short-\ncomings of the tacred dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 13843–13850.\nChris Sweeney and Maryam Najafian. 2019. A trans-\nparent framework for evaluating unintended demo-\ngraphic bias in word embeddings. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1662–1667.\nBing Tian, Yixin Cao, Yong Zhang, and Chunxiao Xing.\n2022. Debiasing nlu models via causal interven-\ntion and counterfactual reasoning. In Proceedings\nof the AAAI Conference on Artificial Intelligence ,\nvolume 36, pages 11376–11384.\nCan Udomcharoenchaikit, Wuttikorn Ponwitayarat,\nPatomporn Payoungkhamdee, Kanruethai Masuk,\nWeerayut Buaphet, Ekapol Chuangsuwanich, and\nSarana Nutanong. 2022. Mitigating spurious cor-\nrelation in natural language understanding with coun-\nterfactual inference. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11308–11321.\nPranav Narayanan Venkit and Shomir Wilson. 2021.\nIdentification of bias against people with disabilities\nin sentiment analysis and toxicity detection models.\narXiv preprint arXiv:2111.13259.\nThomas Verma and Judea Pearl. 1990. Equivalence and\nsynthesis of causal models. In Proceedings of the\nSixth Annual Conference on Uncertainty in Artificial\nIntelligence, pages 255–270.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Simas Sakenis, Jason\nHuang, Yaron Singer, and Stuart Shieber. 2020.\nCausal mediation analysis for interpreting neural\nnlp: The case of gender bias. arXiv preprint\narXiv:2004.12265.\nYiwei Wang, Muhao Chen, Wenxuan Zhou, Yujun\nCai, Yuxuan Liang, Dayiheng Liu, Baosong Yang,\nJuncheng Liu, and Bryan Hooi. 2022. Should we\nrely on entity mentions for relation extraction? debi-\nasing relation extraction with counterfactual analysis.\narXiv preprint arXiv:2205.03784.\nYiwei Wang, Bryan Hooi, Fei Wang, Yujun Cai, Yuxuan\nLiang, Wenxuan Zhou, Jing Tang, Manjuan Duan,\nand Muhao Chen. 2023. How fragile is relation ex-\ntraction under entity replacements? In Proceedings\nof the 27th SIGNLL Conference on Computational\nNatural Language Learning (CoNLL).\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nGalen Weld, Peter West, Maria Glenski, David Arbour,\nRyan A Rossi, and Tim Althoff. 2022. Adjusting for\nconfounders with text: Challenges and an empirical\nevaluation framework for causal inference. In Pro-\nceedings of the International AAAI Conference on\nWeb and Social Media, volume 16, pages 1109–1120.\nNan Xu, Fei Wang, Bangzheng Li, Mingtao Dong, and\nMuhao Chen. 2022. Does your model classify enti-\nties reasonably? diagnosing and mitigating spurious\ncorrelations in entity typing. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing.\nJun Yan, Yang Xiao, Sagnik Mukherjee, Bill Yuchen\nLin, Robin Jia, and Xiang Ren. 2022. On the ro-\nbustness of reading comprehension models to entity\nrenaming. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 508–520.\nDong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng\nHua, and Qianru Sun. 2020a. Causal intervention for\nweakly-supervised semantic segmentation. Advances\nin Neural Information Processing Systems, 33:655–\n666.\nGuanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Con-\nghui Zhu, and Tiejun Zhao. 2020b. Demograph-\nics should not be the reason of toxicity: Mitigating\ndiscrimination in text classifications with instance\nweighting. arXiv preprint arXiv:2004.14088.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 35–\n45.\nWenxuan Zhou and Muhao Chen. 2022. An improved\nbaseline for sentence-level relation extraction. InPro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing, pages 161–168.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023. Context-faithful prompting for\nlarge language models. In Findings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing.\nYongchun Zhu, Qiang Sheng, Juan Cao, Shuokai Li,\nDanding Wang, and Fuzhen Zhuang. 2022. General-\nizing to the future: Mitigating entity bias in fake news\ndetection. In Proceedings of the 45th International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, pages 2120–2125.\n15183\nA Implementation Details\nA.1 White-Box Experiments\nFor RE, we use RoBERTa-Large as our backbone\nmodel, which has 354 million parameters. Our im-\nplementation is based on the codebase by Zhou and\nChen (2022) with their default hyper-parameters.\nMore specifically, we employ a learning rate of\n3e-5, a batch size of 32, and conduct training for\na total of 5 epochs. Other method-specific hyper-\nparameters are selected on the development set of\nTACRED. Finetuning typically takes 1.5 hours on\nan NVIDIA RTX A5000 GPU.\nFor MRC, we use SpanBERT-base-cased as our\nbackbone model, which has 110 million param-\neters. Our implementation is based on the code-\nbase by Yan et al. (2022) with their default hyper-\nparameters. More specifically, we employ a learn-\ning rate of 2e-5, a batch size of 16, and conduct\ntraining for a total of 4 epochs. Other method-\nspecific hyper-parameters are selected on the hold-\nout development set of TriviaQA. Finetuning typ-\nically takes 3 hours on an NVIDIA RTX A5000\nGPU.\nA.2 Black-Box Experiments\nOur implementation is based on the codebase by\nZhou et al. (2023).\nThe instruction for MRC is\nInstruction: read the given information and\nanswer the corresponding question.\nThe prompt without instruction for MRC is\nAssume that {ENTITY0} can be any of {en-\ntity0_candidates}. [Assume that {ENTITY1}\ncan be any of {entity1_candidates} ...]\n{context}\nQ:{question} based on the given text? Extract\nthe answer from the given text. Do not add\nother words.\nA:\nThe instruction for RE is\nIdentify the relationship between two entities\nfrom a list of options.\nThe prompt without instruction for RE is\nAssume that subject_entity is one of\n{subj_candidates}, while object_entity is one\nof {obj_candidates} in the following text.\n{context}\nQ: Which option indicates the relationship\nbetween subject_entity and object_entity in\nthe given text?\nOptions:{options}\nA:\nThe prompt template for detecting entities in\nMRC is\nList named entities in the following sentence.\nSeparate the entities with ###, if you find mul-\ntiple entities. Do not add additional words\nbefore or after your answers.\n{sentence}\nThe prompt template for replacing entities with\nplaceholders in MRC is\nReplace the entity {entity_list} in the follow-\ning paragraph.\n{paragraph}\nThe prompt template for finding similar entities\nis\nName two [{entity_type}] entities similar to\n\"{entity}\". Separate the entities with ###, and\ndo not add additional words before or after\nyour answers. Provide random answers if you\nare not sure.\nIn all the above prompts, variables are sur-\nrounded with curly brackets and optional variables\nare surrounded with square brackets.\n15184",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7168359160423279
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6355182528495789
    },
    {
      "name": "Black box",
      "score": 0.5745952725410461
    },
    {
      "name": "Causality (physics)",
      "score": 0.546431839466095
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5393194556236267
    },
    {
      "name": "Causal inference",
      "score": 0.5162200927734375
    },
    {
      "name": "Natural language processing",
      "score": 0.5098997354507446
    },
    {
      "name": "Intervention (counseling)",
      "score": 0.5038232207298279
    },
    {
      "name": "Machine learning",
      "score": 0.49202463030815125
    },
    {
      "name": "Econometrics",
      "score": 0.16193082928657532
    },
    {
      "name": "Psychology",
      "score": 0.13601738214492798
    },
    {
      "name": "Mathematics",
      "score": 0.12705612182617188
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    }
  ]
}