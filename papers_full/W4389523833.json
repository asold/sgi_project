{
    "title": "BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology",
    "url": "https://openalex.org/W4389523833",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4278148191",
            "name": "Odhran O’Donoghue",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3129137626",
            "name": "Aleksandar Shtedritski",
            "affiliations": [
                "The Francis Crick Institute",
                "Align to Innovate",
                "Align Technology (United States)",
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2115522671",
            "name": "John Ginger",
            "affiliations": [
                "Align Technology (United States)",
                "Align to Innovate",
                "The Francis Crick Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2926262518",
            "name": "Ralph Abboud",
            "affiliations": [
                "Align Technology (United States)",
                "The Francis Crick Institute",
                "Align to Innovate"
            ]
        },
        {
            "id": "https://openalex.org/A2747518657",
            "name": "Ali Ghareeb",
            "affiliations": [
                "The Francis Crick Institute"
            ]
        },
        {
            "id": "https://openalex.org/A4383580995",
            "name": "Samuel Rodriques",
            "affiliations": [
                "International House",
                "The Francis Crick Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3100452049",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4378505261",
        "https://openalex.org/W4367191144",
        "https://openalex.org/W4205450747",
        "https://openalex.org/W4377297670",
        "https://openalex.org/W3154451896",
        "https://openalex.org/W4378771157",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W4363671827",
        "https://openalex.org/W4385573077",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4366999541",
        "https://openalex.org/W4365211632",
        "https://openalex.org/W4389519818",
        "https://openalex.org/W4379539933",
        "https://openalex.org/W4362508231",
        "https://openalex.org/W4378465306",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2921844528",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4221152848",
        "https://openalex.org/W4393160795",
        "https://openalex.org/W3101017384",
        "https://openalex.org/W3162922479",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3121904249",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W3104578551",
        "https://openalex.org/W4297253404",
        "https://openalex.org/W4365597205",
        "https://openalex.org/W2515307843",
        "https://openalex.org/W4307001389",
        "https://openalex.org/W4319165821",
        "https://openalex.org/W4224912544",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4378770815",
        "https://openalex.org/W4379933154",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W2594183968"
    ],
    "abstract": "The ability to automatically generate accurate protocols for scientific experiments would represent a major step towards the automation of science. Large Language Models (LLMs) have impressive capabilities on a wide range of tasks, such as question answering and the generation of coherent text and code. However, LLMs can struggle with multi-step problems and long-term planning, which are crucial for designing scientific experiments. Moreover, evaluation of the accuracy of scientific protocols is challenging, because experiments can be described correctly in many different ways, require expert knowledge to evaluate, and cannot usually be executed automatically. Here we present an automatic evaluation framework for the task of planning experimental protocols, and we introduce BioProt: a dataset of biology protocols with corresponding pseudocode representations. To measure performance on generating scientific protocols, we use an LLM to convert a natural language protocol into pseudocode, and then evaluate an LLM’s ability to reconstruct the pseudocode from a high-level description and a list of admissible pseudocode functions. We evaluate GPT-3 and GPT-4 on this task and explore their robustness. We externally validate the utility of pseudocode representations of text by generating accurate novel protocols using retrieved pseudocode, and we run a generated protocol successfully in our biological laboratory. Our framework is extensible to the evaluation and improvement of language model",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2676–2694\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBioPlanner: Automatic Evaluation of LLMs\non Protocol Planning in Biology\nOdhran O’Donoghue1,2,4 Aleksandar Shtedritski1,2,4 John Ginger1,2\nRalph Abboud1,2 Ali Essa Ghareeb2 Samuel G Rodriques2,3\n1 Align to Innovate 2Francis Crick Institute 3Future House 4University of Oxford\nAbstract\nThe ability to automatically generate accurate\nprotocols for scientific experiments would rep-\nresent a major step towards the automation of\nscience. Large Language Models (LLMs) have\nimpressive capabilities on a wide range of tasks,\nsuch as question answering and the generation\nof coherent text and code. However, LLMs\ncan struggle with multi-step problems and long-\nterm planning, which are crucial for designing\nscientific experiments. Moreover, evaluation\nof the accuracy of scientific protocols is chal-\nlenging, because experiments can be described\ncorrectly in many different ways, require ex-\npert knowledge to evaluate, and cannot usually\nbe executed automatically. Here we present an\nautomatic evaluation framework for the task of\nplanning experimental protocols, and we intro-\nduce BIOPROT1: a dataset of biology proto-\ncols with corresponding pseudocode represen-\ntations. To measure performance on generating\nscientific protocols, we use an LLM to convert a\nnatural language protocol into pseudocode, and\nthen evaluate an LLM’s ability to reconstruct\nthe pseudocode from a high-level description\nand a list of admissible pseudocode functions.\nWe evaluate GPT-3 and GPT-4 on this task and\nexplore their robustness. We externally vali-\ndate the utility of pseudocode representations\nof text by generating accurate novel protocols\nusing retrieved pseudocode, and we run a gen-\nerated protocol successfully in our biological\nlaboratory. Our framework is extensible to the\nevaluation and improvement of language model\nplanning abilities in other areas of science or\nother areas that lack automatic evaluation.\n1 Introduction\nTraditional manual methods for research in biology\nare time-consuming, labour-intensive, and highly\nprone to human error. Robotic laboratory automa-\ntion has the potential to increase accuracy, repro-\n1The dataset and code for evaluation are available at\nhttps://github.com/bioplanner/bioplanner\nevaluate\nTitle Description\nStep-by-step \nInstructions \nPseudofunctions\nStep-by-step \npseudocode\nTitle Description\nPseudofunctions\nStep-by-step \npseudocode\ncopy\nFigure 1: Automatic evaluation of protocol genera-\ntion. The teacher model is given full information about\na scientific experiment protocol – title, description, and\nstep-by-step instructions. It is prompted to generate\npseudo functions that allow the execution of the proto-\ncol. The student model is given the admissible pesudo-\nfunctions and is evaluated on its ability to generate the\nstep-by-step pseudocode.\nducibility, and scalability, contributing to more sci-\nentific breakthroughs and a faster transition from\nresearch to real-world applications.\nOne important step towards automation of bi-\nology research is the automated generation of a\nlaboratory protocol (Accurate step-by-step instruc-\ntions on how to complete an experiment to accom-\nplish a specific goal) which can subsequently be\nconverted into robot code. LLMs have significant\nlatent scientific knowledge and thus may be able to\nformulate accurate scientific protocols, which has\nbeen demonstrated for the field of chemistry (Bran\net al., 2023; Boiko et al., 2023). However, to-date\nthere has not been any clear way to evaluate the ac-\ncuracy of a generated scientific protocol, except by\nmanual evaluation. Without established evaluation\nmetrics, progress in the field of automating science\nremains challenging.\nEvaluating laboratory protocols is difficult for\n2676\ntwo reasons. Firstly, protocols are very sensi-\ntive to tiny details, and slight variations in in-\nstructions can lead to significantly different out-\ncomes. When comparing generated protocols\nagainst ground truths, metrics that rely on n-gram\noverlaps such as BLEU (Papineni et al., 2002) or\ncontextual embeddings such as BERTScore (Zhang\net al., 2019) might not capture small differences,\nsuch as the order of actions, or relation between\nsubstances (Bhandari et al., 2020). Secondly, the\nsame protocol can be described correctly at various\nlevels of granularity. The same technique (e.g. se-\nquencing library preparation) can be described by\na single line or multiple paragraphs. This variabil-\nity in granularity makes it difficult to evaluate the\naccuracy of LLM-generated protocols.\nWe here present an automated approach to eval-\nuating the ability of a language model to write\nbiological protocols. Our approach is inspired\nby robotic planning, in which a closed set of ad-\nmissible actions is provided to a controller agent\n(Jiménez et al., 2019; Ahn et al., 2022; Huang et al.,\n2022). We use GPT-4 to automatically convert a\nwritten protocol into pseudocode using a protocol-\nspecific set of pseudofunctions that is generated by\nthe model (see Figure 1). Here, a “teacher” model\ngenerates the admissible action set and correct an-\nswer in terms of step-by-step pseudocode. Having\naccess to this privileged information, we can then\nevaluate the performance of a “student”, that has\nto solve the task from scratch. In this way, we then\nevaluate the ability of language models to generate\na protocol when presented only with the appropri-\nate pseudocode functions and a short description\nof the protocol. In effect, our approach allows us to\nautomatically convert the process of writing a scien-\ntific protocol into a series of multiple-choice ques-\ntions (i.e., pick a pseudofunction from a provided\nset), which can be evaluated much more robustly\nthan natural language generation. This paradigm al-\nlows us to rapidly measure the protocol knowledge\nof GPT-3.5 and GPT-4 with minimal human inter-\nvention, and can serve as a general approach for\nevaluating and improving long-horizon planning in\nopen-ended tasks in the future.\nTo this end, we also introduce a novel dataset,\nBIOPROT, of publicly available biology laboratory\nprotocols, containing instructions in both free text\nand protocol-specific pseudocode. The dataset has\nbeen reviewed by domain experts and allows evalu-\nation of model performance several different tasks,\nsuch as next-step prediction, or full protocol gener-\nation. We further show the utility of this dataset by\nautomatically designing and successfully executing\na lab experiment using GPT-4 and the action space\ndefined using BIOPROT.\nIn summary, we make the following contribu-\ntions: (i) We propose evaluating protocol gener-\nation on pseudocode rather than free text instruc-\ntions; (ii) We introduce the BIOPROT dataset, a\nmanually audited dataset of open-access biology\nprotocols; (iii) We evaluate the ability of GPT-4\nto accurately convert natural language protocols\ninto pseudocode; (iv) We define a suite of tasks\nand metrics for evaluating protocol generation; (v)\nWe evaluate several LLMs on our tasks to provide\nobjective measures of these models’ ability to gen-\nerate biological experiments; (vi) We automatically\ngenerate a biology experiment and successfully ex-\necute it in a lab.\n2 Related Works\nLLMs for Natural Sciences Using LLM for sci-\nentific tasks such as entity extraction in biologi-\ncal documents (Tamari et al., 2021) or retrieval of\nchemical reaction procedures (Bai et al., 2022) is\na natural use case of such models. Work such as\nSciBERT, BioGPT, Galactica and others have also\nshown the utility of pretraining an LLM on a corpus\nof biomedical (Gu et al., 2021; Lewis et al., 2020;\nLuo et al., 2022; Lee et al., 2020; Shin et al., 2020)\nor general scientific text (Beltagy et al., 2019; Tay-\nlor et al., 2022). More recently, pre-trained gener-\nalist LLMs such as GPT-3 (Brown et al., 2020) and\nGPT-4 (OpenAI, 2023) have shown to be capable\nof tasks such as searching for chemical compounds\nsimilar to a given one (OpenAI, 2023) or drug edit-\ning (Liu et al., 2023c). Furthermore, a GPT-4 agent\naugmented with tools has been shown to be capa-\nble of synthesis planning and drug discovery (Bran\net al., 2023) or planning reactions and executing\nthem on a robotic platform (Boiko et al., 2023).\nTask Decomposition LLMs trained on next-\ntoken prediction can struggle with more complex\nlogical reasoning in naïve setups (Liu et al., 2023b).\nHowever, decomposing complex tasks into sub-\ntasks in frameworks such as Chain-of-Thought\nreasoning (Wei et al., 2022; Zhang et al., 2023),\nand its variants such as Least-to-Most (Zhou et al.,\n2022) and Tree of Thought reasoning (Yao et al.,\n2023) improves performance in multi-step reason-\ning problems. In addition to test-time improve-\n2677\nments, LLMs also improve in performance when\ntrained on step-by-step reasoning data generated\nby larger LLMs (Mukherjee et al., 2023; Mu et al.,\n2023). Task decomposition has also been combined\nwith self-verification through deductive reasoning\nto improve step-by-step reasoning accuracy (Ling\net al., 2023). Here, we approach task decomposi-\ntion from another angle - we first ask the model to\ndefine a discrete set of actions needed to complete\na task, and then how to compose them.\nPlanning Closely related to task decomposition\nis planning. LLMs have been successful at plan-\nning in simulated and real embodied space, both\nthrough the use of restricted action space (Ahn\net al., 2022; Driess et al., 2023), function/tool\nsearch (Wang et al., 2023a; Schick et al., 2023;\nShen et al., 2023; Bran et al., 2023; Boiko et al.,\n2023) and translation of plans into admissible ac-\ntion space (Huang et al., 2022). Planning mod-\nels have been explicitly combined with Chain-of-\nThought reasoning for performance improvement\n(Mu et al., 2023; Shen et al., 2023). LLM plan-\nners can also learn to create their own training\ncurriculum and refine their function use (Wang\net al., 2023a). LLM-based planning and reasoning\ncan benefit from writing problems in a machine-\nreadable language such as Planning Domain Defi-\nnition Language (PDDL) and symbolic logic (Pan\net al., 2023; Silver et al., 2023). Furthermore, in-\nteractions with simulators and debuggers can be\nused to improve both plans (Liu et al., 2023a) and\nvalue functions that determine the appropriateness\nof action calls (Ahn et al., 2022; Driess et al., 2023;\nMu et al., 2023). Our work extends recent work in\nplanning through the automated generation of ad-\nmissible action spaces and consequent evaluation\nwithout the need for a simulation environment.\nEvaluating LLM Scientists Evaluating LLMs\non scientific tasks is limited to QA benchmarks for\nmeasuring general science knowledge (Hendrycks\net al., 2020), or specialist knowledge such as chem-\nistry (Guo et al., 2023; Wu et al., 2017), biomed-\nical science (Sung et al., 2021) or medicine (Jin\net al., 2019, 2021). However, evaluating an LLM’s\nperformance on more open-ended tasks, such as\nhealthcare support (Dash et al., 2023) or chemical\nsynthesis planning (Bran et al., 2023) is done man-\nually. To the best of our knowledge, we are the\nfirst to approach automatic evaluation of LLMs on\nopen-ended problems in science.\nAutomatic Evaluation of LLMs While evalua-\ntion of the performance of an LLM in games (Wang\net al., 2023a) or planning in PDDL domains (Silver\net al., 2023) can be done automatically, many works\nrely on self-evaluation, where GPT-4 is used as an\nevaluator (Bubeck et al., 2023; Bran et al., 2023;\nChiang et al., 2023; Peng et al., 2023; Zhou et al.,\n2023). However, these have been found to con-\ntradict human evaluation (Bran et al., 2023) or be\nsystematically biased (Wang et al., 2023b), where\nthe order of the provided responses affects the pre-\ndicted ranking. In comparison to these works, we\nuse an LLM to generate pseudo-ground truth data\non an easy task, in which the model consistently\nperforms well at, which we use to evaluate on a\nmore difficult task with real-world implications.\n3 The B IOPROT dataset\nHere we describe the BIOPROT dataset - a collec-\ntion of publicly available protocols that are used\nto evaluate the performance of LLMs on protocol\ngeneration on a large range of topics in biology.\nWe discuss the contents of the dataset (Section 3.1),\ncreating a set of admissible actions and translating\nthe protocol steps (Section 3.2), manual verifica-\ntion of the data (Section 3.3), and the tasks that can\nbe approached with it (Section 4). The dataset can\nbe found in the Supplementary Materials. This ap-\nproach can be used to generate pseudocode datasets\nin any domain that has step-by-step instructional\ndata.\n3.1 A Dataset of Protocols for Biology\nWe collect publicly available protocols from Pro-\ntocols.io (Teytelman et al., 2016), a platform for\ndeveloping and sharing reproducible methods. This\ndatabase contains over 9,000 public protocols of\ndifferent scientific areas and complexity. Each pro-\ntocol consists of (i) a title, (ii) a description, and\n(iii) step-by-step instructions. We automatically\nand manually filter the protocols, in order to ob-\ntain a set of protocols that are related to biology,\ncan be reproduced, and are of sufficient difficulty.\nFor further details about the filtering, refer to the\nSupplementary Materials. In Table 1 we present a\nsummary of the collected protocols.\n3.2 Translating Protocols to Pseudocode\nAs discussed in Section 1, evaluation of planning\nproblems is difficult in natural text, and prior works\nopt for manual evaluation (Bran et al., 2023; Boiko\n2678\nTitle: Hornwort DNA extraction\nDescription: The gametophytic \ntissue of hornworts is rich in \npolysaccharides (Renzaglia et al., \n2009) and it also seems to be rich \nin polyphenolics. Both compounds \npose a problem for DNA ...  \nProtocol:  \n1. Grind 0.5-2 g of tissue using \nmortar and pestle in the presence \nof liquid nitrogen...\n2. Add 10 ml of 60 oC extraction \nbuffer and 100 mg PVP-40/g tissue \n(5μl of RNAse A (100mg/ml))...\n3. ...\nProtocol\ndef transfer_tissue(tube_volume):\n    pass\ndef incubate_sample(incubation_params):\n    pass\ndef add_and_mix(solvent, mixing_method):\n    pass\ndef spin_sample(spin_params):\n    pass\ndef ...\nGround      \niiiitruth \nPseudofunctions\ndef transfer_tissue(tube_volume):\n    pass\ndef incubate_sample(incubation_params):\n    pass\ndef add_and_mix(solvent, mixing_method):\n    pass\ndef spin_sample(spin_params):\n    pass\ndef ...\ngrind_tissue(tissue_weight=\"0.5-2 g\", \ngrinding_method=\"mortar and pestle with \nliquid nitrogen\")\ntransfer_tissue(tube_volume=\"30 ml\")\nadd_extraction_buffer(buffer_volume=\"10 \nml\", buffer_temperature=\"60 °C\", \npvp_weight=\"100 mg/g tissue\", \nrnase_volume=\"5μl\")\nadd_and_mix(solvent=\"12 ml of \nchloroform:IAA (24:1)\", \nmixing_method=\"inversion\")\n...\nGenerated Pseudocode\nYour goal is to convert molecular \nbiology protocols into python \npseudocode. \nHere is an example of ...\nPrompt\ngrind_tissue(tissue_weight=\"0.5-2 g\", \ngrinding_method=\"mortar and pestle with \nliquid nitrogen\")\ntransfer_tissue(tube_volume=\"30 ml\")\nadd_extraction_buffer(buffer_volume=\"10 \nml\", buffer_temperature=\"60 °C\", \npvp_weight=\"100 mg/g tissue\", \nrnase_volume=\"5μl\")\nadd_and_mix(solvent=\"12 ml of \nchloroform:IAA (24:1)\", \nmixing_method=\"inversion\")\n...\nPseudocode instructions\nFigure 2: Creation of pseudofunction and pseudocode data The model is prompted to generate pseudofunctions\nand pseudocode based on a target protocol. This generated code is automatically debugged using a feedback error\nloop, and then manually reviewed. Generated pseudofunctions are used to define the admissible action space in\ndownstream evaluation tasks, and pseudocode instructions using the pseudofunction calls are used as ground truth\nto measure the accuracy of generated code in downstream tasks, enabling automatic evaluation.\nStatistic Value\nProtocols 100\nAverage number of steps 12.5\nAverage total protocol length in tokens 641.0\nAverage tokens per step 52.6\nAverage tokens per original description 83.8\nAverage tokens per generated description 66.3\nTable 1: Dataset Statistics We present aggregate statis-\ntics for the BIOPROT dataset. The “generated descrip-\ntions” are generated using GPT-4 from the step-by-step\ninstructions, as discussed in Section 5.4.\net al., 2023). To this end, we “translate” the\nfree text protocols into pseudocode using GPT-4\n(see Figure 2). We task GPT-4 to (i) define a set of\npseudofunctions that suffice to execute the protocol,\nand (ii) convert the protocol steps into pseudocode\nusing only the provided pseudofunctions.\nWe make use of a one-shot example prompt, and\nan automatic feedback loop (Liu et al., 2023a) that\nprovides error signals if: the generated code is\nnot valid Python pseudocode; no pseudofunctions\nare defined; the pseudocode or pseudofunctions do\nnot have arguments; any numerical parameters in\nthe pseudocode do not have units. Finally, GPT-\n4 is prompted to check for errors or omissions in\nthe pseudofunctions and pseudocode. Information\nabout our generated pseudocode is summarized\nin Table 2.\nStatistic Value\nAvg. number of pseudofunctions per protocol 10.3\nAvg. number of pseudofunctions per step 0.82\nAvg. number of lines of pseudocode 17.2\nTable 2: Pseudocode Statistics We present aggregate\nstatistics about the automatically generated pseudofunc-\ntions and pseudocode.\n3.3 Manual Verification\nWe manually reviewed the generated pseudofunc-\ntions and pseudocode for accuracy. Original proto-\ncols and generated ground-truth pseudocode were\nassessed line-by-line by a competent laboratory sci-\nentist. They confirmed (i) whether the original nat-\nural language protocol made sense, (ii) whether the\ntitle and description sufficiently described the pro-\ntocol so that a competent scientist could attempt to\ncomplete it without the protocol, and (iii) whether\nthe pseudocode was accurate. Finally, edits were\nmade to the generated pseudocode as necessary.\nWe show a breakdown of the edits made in Table 3.\nStatistic Value\n% generated protocols requiring no edits 59\n% generated protocols with 1≤3 edited lines 24\n% generated protocols with > 3 edited lines 17\naverage number of line edits in edited files 11.8\nTable 3: Manual Verification We provide a breakdown\nof the protocols that required manual edits.\n2679\nOverall, 59 of the 100 protocols were found to\nbe completely accurate requiring no edits. How-\never, many protocols that did require edits only\nrequired minor edits. The most common errors\nfound were missing units for numbers, which in\nmost cases would not prevent a competent scientist\nfrom completing a protocol. The more impactful er-\nrors found were most commonly (1) missing details\nwhich would allow one to successfully complete\na step of the protocol (these were usually highly\nverbose steps which explained a detailed technical\nmethod for manipulating a sample) and (2) not ex-\nplaining the composition of a material used in the\nprotocol (e.g. a buffer).\nThe corrected protocols are made available as\nthe BIOPROT dataset. Even without human editing,\nLLMs with error-checking loops can be used to cre-\nate a largely accurate dataset for biology protocol\npseudocode, thus enabling self-evaluation.\n3.4 Machine-generated Descriptions\nFor some of our downstream tasks, it is necessary\nto have high-quality descriptions of protocols that\ngive a sense of what the protocol steps should in-\nclude. However, protocol descriptions in Proto-\ncols.io are not always suitable for this purpose. To\nthis end, we also generated descriptions of pro-\ntocols that provided a high-level overview of the\nprotocols’ objective (the prompt for this is seen in\nthe Supplementary Materials). We include both our\nmachine-generated descriptions and the original\ndescriptions in our dataset.\n4 Metrics and evaluation\nUsing the BIOPROT dataset, we evaluate an LLM’s\ncapabilities to reason about and generate scientific\nprotocols on several different tasks.\nNext Step Prediction Given a protocol title, de-\nscription, an admissible set of pseudofunctions, and\npartially completed pseudocode, we evaluate the\nmodel’s ability to correctly identify the pseudofunc-\ntion corresponding to the next step in the protocol.\nWe evaluate the correctness of both the predicted\nfunction and the function arguments.\nFor function-level accuracy, we report the per-\ncentage of the number of correct function assign-\nments\naccuracy = 1\nN\nN∑\nn=1\n1[fpred\ni = fGT\ni ],\nwhere fpred and fGT are the predicted and\ngroundtruth functions, respectively, and N is the\nnumber of steps in the protocol.\nDuring generation, the model is prompted to\nname each function argument and provide the ar-\ngument parameters. To evaluate accuracy of the\narguments, we first check whether the function ar-\ngument names is correct. For that purpose, we com-\npute precision and recall of the arguments’ names.\nFor correct function arguments, we consider the ac-\ncuracy of the argument value using the BLEU met-\nric (Papineni et al., 2002). Additionally, we encode\nthe predicted and ground truth argument values,\napred\ni and aGT\ni , respectively, with SciBERT (Belt-\nagy et al., 2019) sentence encoder Eto get the\nSciBERTscore:\nSciBERTscore = 1\nN\nN∑\ni=0\n⟨E(apred\ni ), E(aGT\ni )⟩\n∥E(apred\ni )∥∥E(aGT\ni )∥\n,\nwhich is the average cosine similarity between\npredicted and ground truth argument values\nfor all N steps. This metric is inspired by\nBERTScore (Zhang et al., 2019), but we use a SciB-\nERT encoder as it is better suited to the scientific\ndomain. We only compute argument-level metrics\nfor correctly predicted functions, as not to penalize\nthe model twice for wrong function predictions.\nProtocol Generation Given a protocol title, de-\nscription, and an admissible set of pseudofunctions,\nthe model is tasked to generate corresponding pseu-\ndocode. We again evaluate the correctness of pre-\ndicted functions and their corresponding arguments.\nThis is a more difficult task than the previous one,\nas the model needs to plan the entire execution\nof the protocol. For function-level evaluation, we\nneed to measure (i) if the correct functions were\ncalled, and (ii) if they were used in the correct order.\nFor the former, we report precision and recall of\nfunction calls, where we take into account repeated\ncalls of the same function. For evaluating whether\nthe functions are used in the correct order, we use\nthe Levenshtein distance Ld between the predicted\nand ground-truth sequence of functions. The Lev-\nenshtein distance is originally a string edit distance\nthat measures the number of insertions, deletions,\nor substitutions to make one word into another. We\nconsider each function call as a separate symbol,\nwhich incurs a cost of 1 for being added, deleted,\nor substituted. We report a normalized Levenshtein\n2680\nModel Shuffle Functions Arguments\nAccuracy Precision Recall SciBERTScore BLEU\nGPT-3.5 ✗ 65.±1.3 97.7±0.5 94.7±0.5 88.5±0.5 0.363±0.012\nGPT-3.5 ✓ 36.1±1.6 97.1±1.2 95.1±1.0 88.6±0.5 0.384±0.028\nGPT-4 ✗ 70.6±0.4 97.1±0.5 94.9±0.6 87.9±0.5 0.351±0.017\nGPT-4 ✓ 57.0±0.8 97.1±0.4 94.7±0.8 88.5±0.6 0.363±0.025\nTable 4: Next Step Prediction Evaluation Given a protocol title and description, the admissible pseudofunctions\nand partially completed pseudocode, we evaluate the model’s ability to correctly predict the next step. For all\nmetrics, higher is better. We report mean and standard deviation over 5 runs.\ndistance Ldn\nLdn = Ld\nN ,\nwhere N is the number of functions in the ground-\ntruth pseudocode.\nIn addition, we evaluate the predicted function\narguments. We use the same metrics as described\nunder \"Next Step Prediction\".\nFunction Retrieval Our approach has the poten-\ntial to allow novel protocols to be assembled from\nsteps provided in existing protocols in the dataset,\nif the model is able to correctly identify which steps\nare needed for any given protocol. Thus, given a\nprotocol title and description, and a set of pseud-\nofunctions, we evaluate the models’ ability to cor-\nrectly identify which of the provided functions are\nneeded to execute the protocol. In this task, we pro-\nvide the model with a set of pseudofunctions con-\nsisting of the ground-truth pseudofunctions for the\ngiven protocol, and pseudofunctions drawn from\nseveral (i) random or (ii) nearest neighbour proto-\ncols. Providing functions from nearest neighbour\nprotocols is more difficult, as they are likely to be\nmore similar to the correct functions. We measure\nthe precision and recall of retrieved functions.\n5 Experiments\n5.1 Implementation details\nWe explore the performance of GPT-3.5 and GPT-4\nfrom the OpenAI API. Where we find nearest neigh-\nbors, we use an embedding index of all protocols’\ndescriptions using text-embedding-ada-002 em-\nbeddings, unless stated otherwise. We show the\nprompts we use in the Supplementary Material.\nFor each of the tasks listed in Section 4, we\nevaluate the models in several settings:\n• Shuffled: the model can be provided either\nwith functions in the order in which they are\ngenerated, or randomly shuffled. The func-\ntions tend to be defined in the order they ap-\npear in the original protocol, and that serves\nas a signal to the model we evaluate. By ran-\ndomly shuffling the input functions, we make\nthe task more difficult.\n• Feedback: The model has access to an er-\nror loop that can detect undefined functions\nand Python syntax errors. Such feedback\nloops have been found to be beneficial in\nPDDL planning (Silver et al., 2023) and rea-\nsoning (Madaan et al., 2023).\n5.2 Results\nNext step prediction We show results on next\nstep prediction in Table 4. We see that GPT-4\nconsistently outperforms GPT-3.5 in both the pre-\ndiction of the correct next step, whereas GPT-3.5\nperforms better at predicting function arguments.\nWe note there is a drop in performance when the\ninput functions are shuffled, likely because if not\nshuffled, the functions appear roughly in the order\nas they should be called as they were sequentially\ngenerated by the LLM.\nProtocol generation We show results on full pro-\ntocol generation in Table 5. We observe the biggest\ngap in the Levenshtein distance score metric, where\nGPT-4 significantly outperforms GPT-3.5. Mean-\nwhile, GPT-4 and GPT-3.5 show similar precision\nand recall of used functions. This suggests that\nwhile both have a similar ability to use the correct\nfunctions, GPT-4 performs better at using the right\norder. We also observe that shuffling the input func-\ntions consistently leads to a drop in performance.\nFunction retrieval We show retrieval results in\nTable 6. We see that GPT-4 outperforms GPT-3.5\non this task. However, the results on this task ap-\npear generally poor. One possible reason for the\n2681\nModel Shuffle Feedback Functions Arguments\nPrecision Recall Ldn↓ Precision Recall SciBERTScore BLEU\nGPT-3.5 ✗ ✗ 93.4±0.9 89.9±0.6 0.498±0.036 72.7±0.8 91.4±1.5 82.7±0.6 0.121±0.005\nGPT-3.5 ✗ ✓ 93.3±1.0 91.1±1.1 0.505±0.159 73.1±1.6 88.1±1.9 82.8±0.6 0.117±0.006\nGPT-3.5 ✓ ✗ 91.8±0.8 85.9±2.8 0.945±0.055 72.9±1.4 89.1±2.2 81.8±0.2 0.102±0.003\nGPT-3.5 ✓ ✓ 92.5±0.3 86.1±1.6 0.884±0.045 73.2±1.3 87.3±3.5 82.3±0.4 0.102±0.009\nGPT-4 ✗ ✗ 91.9±0.9 90.8±0.9 0.396±0.046 72.2±0.8 94.7±1.4 82.6±0.2 0.124±0.006\nGPT-4 ✗ ✓ 92.5±0.3 90.1±0.3 0.438±0.412 72.0±0.3 93.3±1.0 82.7±0.3 0.112±0.005\nGPT-4 ✓ ✗ 92.6±0.9 87.7±0.9 0.722±0.311 72.2±0.3 94.6±1.8 82.7±0.4 0.113±0.004\nGPT-4 ✓ ✓ 92.8±1.0 86.6±0.3 0.685±0.178 73.7±0.7 93.4±2.0 82.5±0.7 0.108±0.004\nTable 5: Protocol Generation Evaluation Given a protocol title and description, and a set of admissible pseudo-\nfunctions, we evaluate the model performance on full protocol generation. For all metrics higher values are better,\nexcept for the normalized Levenshtein distance Ldn, where lower values are better. Best performance is bolded and\nsecond best is underlined. We report mean and standard deviation over 5 runs.\nModel Neighbourhood Precision Recall\nGPT-3.5 Nearest 24.2 35.7\nGPT-3.5 Random 36.7 45.2\nGPT-4 Nearest 32.5 39.2\nGPT-4 Random 48.8 49.4\nTable 6: Function retrieval. Performance on function\nretrieval of pseudofunctions from the query protocol, as\nwell as (i) random or (ii) nearest neighbors protocols.\npoor performance is that the correct answer may\nsometimes be ambiguous. For example, Mix and\nMixSubstance are semantically identical, but have\ndifferent syntax, and the model would be penalized\nfor selecting a function not from the query proto-\ncol. This effect would explain why performance\nusing the“\"nearest” neighbours is worse than per-\nformance when using “random” protocols.\n5.3 Using GPT-4 as an evaluator\nWe use GPT-4 as an evaluator, where given (i) a\nprotocol description, (ii) admissible pseudofunc-\ntions, (iii) ground-truth pseudocode (generated as\ndescribed in Section 3.2), and (iv) predicted pseu-\ndocode, the model is prompted to predict which one\nof (iii) or (iv) better matches the protocol descrip-\ntion (i). We report the rate at which the predicted\npseudocode was preferred in Table 8. In general,\nGPT-4 only performs slightly above chance in iden-\ntifying the ground truth protocol, versus LLM gen-\nerations, although it is unclear whether this is be-\ncause the machine-generated protocols are largely\ncorrect, or because GPT-4 is unable to distinguish\ncorrect from incorrect protocols. Note that prior\nworks (Bran et al., 2023) found that a GPT evalu-\nator tends to prefer longer and more coherent, but\nnot necessarily more correct generations.\n5.4 Using GPT-4-Generated Descriptions\nFor some protocols, we observe that the detail\npresent in the protocol description does not suffice\nto enable protocol reconstruction. To this end, we\nuse GPT-4 to generate a short pseudo description\ngiven the protocol steps in natural text. We present\nresults on next step generation and full protocol\ngeneration in Figure 8. We see a small increase in\nperformance, which is expected, as the summary-\ngenerating model can include more detail (however,\nthe pseudo descriptions are shorter – see Table 2).\n5.5 Real-World Validation\nFinally, to validate that BIOPROT can be used to\ngenerate accurate novel protocols, we devised a\nsetup for end-to-end protocol creation. To do this\nwe opted to build an LLM agent with access to\ntools, such that it can retrieve protocols that con-\ntain relevant pseudofunctions, and use their pseud-\nofunctions to generate new pseudocode. Note that\nfor good performance in this real-world validation\ntask, the LLM needs to be able to (1) find rele-\nvant psueodofunctions from other protocols, and\n(2) generate correct pseudocode, both of which are\ntasks we build metrics for. Details are as follows:\nwe created a Toolformer-like (Schick et al., 2023)\nchain-of-thought LLM agent (Wei et al., 2022) with\naccess to a tool for searching for protocols in the\nBIOPROT database. This agent used the GPT-4\nLLM. We prompted the agent to retrieve protocols\nrelevant to generating a new target protocol. We\nextracted the pseudofunctions from the retrieved\nprotocols and then prompted the agent to generate\na new protocol using only the retrieved pseudofunc-\ntions. We used this setup to create two experiments\nusing GPT-4: (1) culturing a single colony of E.coli\nbacteria overnight and making a glycerol stock with\n2682\nModel Description Functions Arguments\ngenerated by Accuracy Precision Recall Ldn ↓ Precision Recall SciBERTScore BLEU\nGPT-4\n 46.1 – – – 98.1 95.6 88.9 0.334\nGPT-4\n 48.4 – – – 98.4 95.1 90.0 0.393\nGPT-4\n – 91.1 90.1 0.49 71.8 95.5 84.1 0.126\nGPT-4\n – 92.2 90.3 0.45 73.3 95.8 84.5 0.122\nTable 7: Using GPT-4 - generated description We compare performance on next step prediction (top) and protocol\ngeneration (bottom) when using a protocol description generated by (i) scientists, or (ii) GPT-4. We see that using a\nGPT-4 generated description consistently outperforms the original one. The input pseudofunctions to the model are\nshuffled and we use a feedback loop.\nModel Shuffle Feedback GPT-4 score ↑\nGPT-3.5 ✗ ✗ 35.6\nGPT-3.5 ✗ ✓ 40.2\nGPT-3.5 ✓ ✗ 40.9\nGPT-3.5 ✓ ✓ 39.3\nGPT-4 ✗ ✗ 43.9\nGPT-4 ✗ ✓ 42.4\nGPT-4 ✓ ✗ 40.9\nGPT-4 ✓ ✓ 42.4\nTable 8: GPT-4 as an evaluator. The GPT-4 score\nshows the rate at which GPT-4 predicted the model’s\noutput to be better than the ground truth.\nthe suspension (a form of cryopreservation for long-\nterm storage), and (2) culturing Symbiodinum (a\nlarge genus of dinoflagellates endosymbiontic to\ncnidarians that may help corals survive in warming\noceans), extracting its DNA, and then running the\nDNA on an agarose gel.\n5.6 Real-World Validation Results\nThe model generated two new protocols using\npseudofunctions from our database. Both of these\nprotocols were reviewed by a scientist and were\ndetermined to be accurate and sufficient for a com-\npetent lab scientist to follow. We opted to complete\nthe first protocol using E.coli as we did not have\nSymbiodinium available in the laboratory. We val-\nidated the first protocol by implementing it in the\nlab with the instructions and parameter values pro-\nvided by the model. The protocol ran successfully:\nthe cells remained viable after storage at -80 °C, as\nevidenced by subsequent culture on nutrient agar\n(see Figure 3). The methods and prompts used to\ngenerate these experiments, as well as the agent\nchain-of-thought reasoning, can be found in the\nAppendix.\nFigure 3: E.coli growing on nutrient agar plates. We\ncarried out a protocol for overnight culture and cryop-\nreservation of E.coli in glycerol for long-term storage.\nOne hour after completion of the protocol, the cells were\nthawed and spread onto the surface of nutrient agar. Af-\nter 10 hours they can be seen growing on the surface of\nthe agar plate (top) plate, while there is no growth on\nthe control (no E.coli) plate (bottom). This shows the\nLLM-generated protocol was correct.\n6 Conclusion\nWe have introduced a method for automatic evalu-\nation of LLMs on open-ended planning problems,\nsuch as those found in experimental sciences, and\na dataset of such planning problems in biology lab-\noratory protocols. We then defined a suite of tasks\nand evaluation metrics that can be used to measure\nperformance and help drive progress in the field.\nWe evaluate GPT-3.5 and GPT-4 on these tasks and\nfind that there is more to be desired in terms of\nperformance. Finally, we show an application of\nour dataset and framework, where an LLM gener-\nates a protocol that is successfully executed in a\nlaboratory.\n2683\n7 Limitations\nUse of paid API The GPT-4 and GPT-3.5 models\nwe use are not open-sourced and can have signif-\nicant costs for large-scale experiments. In total,\nwe used approximately $1000 for API calls. Fur-\nther work should explore the performance of open-\nsourced LLMs.\nAdditional scientific fields Our work is focused\non biology, but could be extended to other fields\nsuch as chemistry and materials science. Future\nworks should explore extending the dataset and\nframework.\nMisuse There is a risk of misuse, where adver-\nsaries could use our framework or dataset to inform\nthe synthesis of harmful compounds. We have\ntaken care to ensure the protocols in BIOPROT con-\ntain no protocols that can easily be used for such\npurposes. Continued research on aligning LLMs\nand restriction of dangerous outputs is important to\nminimize risk. We hope that our approach of using\npseudofunctions may in the future allow for easier\nprogrammatic evaluation of outputs, and easier de-\ntection of the generation of hazardous substances.\nReferences\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex\nHerzog, et al. 2022. Do as i can, not as i say: Ground-\ning language in robotic affordances. arXiv preprint\narXiv:2204.01691.\nFan Bai, Alan Ritter, Peter Madrid, Dayne Freitag, and\nJohn Niekrasz. 2022. SynKB: Semantic search for\nsynthetic procedures. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations , pages\n311–318, Abu Dhabi, UAE. Association for Compu-\ntational Linguistics.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT:\nA pretrained language model for scientific text. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), Hong Kong, China.\nAssociation for Computational Linguistics.\nManik Bhandari, Pranav Narayan Gour, Atabak Ash-\nfaq, Pengfei Liu, and Graham Neubig. 2020. Re-\nevaluating evaluation in text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9347–9359, Online. Association for Computa-\ntional Linguistics.\nDaniil A Boiko, Robert MacKnight, and Gabe Gomes.\n2023. Emergent autonomous scientific research ca-\npabilities of large language models. arXiv preprint\narXiv:2304.05332.\nAndres M Bran, Sam Cox, Andrew D White, and\nPhilippe Schwaller. 2023. Chemcrow: Augmenting\nlarge-language models with chemistry tools. arXiv\npreprint arXiv:2304.05376.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nDebadutta Dash, Rahul Thapa, Juan M Banda, Akshay\nSwaminathan, Morgan Cheatham, Mehr Kashyap,\nNikesh Kotecha, Jonathan H Chen, Saurabh Gom-\nbar, Lance Downing, et al. 2023. Evaluation of\ngpt-3.5 and gpt-4 for supporting real-world infor-\nmation needs in healthcare delivery. arXiv preprint\narXiv:2304.13714.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2021. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM Transactions on Computing\nfor Healthcare (HEALTH), 3(1):1–23.\nTaicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen\nLiang, Zhichun Guo, Nitesh V . Chawla, Olaf Wiest,\nand Xiangliang Zhang. 2023. What indeed can gpt\nmodels do in chemistry? a comprehensive benchmark\non eight tasks.\nStefan Hegselmann, Alejandro Buendia, Hunter Lang,\nMonica Agrawal, Xiaoyi Jiang, and David Sontag.\n2023. Tabllm: Few-shot classification of tabular\ndata with large language models. In International\nConference on Artificial Intelligence and Statistics,\npages 5549–5581. PMLR.\n2684\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022. Language models as zero-shot\nplanners: Extracting actionable knowledge for em-\nbodied agents. In International Conference on Ma-\nchine Learning, pages 9118–9147. PMLR.\nSergio Jiménez, Javier Segovia-Aguas, and Anders Jon-\nsson. 2019. A review of generalized planning. The\nKnowledge Engineering Review, 34:e5.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng,\nHanyi Fang, and Peter Szolovits. 2021. What disease\ndoes this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Ap-\nplied Sciences, 11(14):6421.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William\nCohen, and Xinghua Lu. 2019. PubMedQA: A\ndataset for biomedical research question answering.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2567–\n2577, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nPatrick Lewis, Myle Ott, Jingfei Du, and Veselin Stoy-\nanov. 2020. Pretrained language models for biomedi-\ncal and clinical tasks: Understanding and extending\nthe state-of-the-art. In Proceedings of the 3rd Clini-\ncal Natural Language Processing Workshop, Online.\nAssociation for Computational Linguistics.\nZhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang,\nMingu Lee, Roland Memisevic, and Hao Su. 2023.\nDeductive verification of chain-of-thought reasoning.\narXiv preprint arXiv:2306.03872.\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,\nShiqi Zhang, Joydeep Biswas, and Peter Stone.\n2023a. Llm+ p: Empowering large language models\nwith optimal planning proficiency. arXiv preprint\narXiv:2304.11477.\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji\nZhou, and Yue Zhang. 2023b. Evaluating the logical\nreasoning ability of chatgpt and gpt-4.\nShengchao Liu, Jiongxiao Wang, Yijin Yang, Cheng-\npeng Wang, Ling Liu, Hongyu Guo, and Chaowei\nXiao. 2023c. Chatgpt-powered conversational drug\nediting using retrieval and domain feedback. arXiv\npreprint arXiv:2305.18090.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng\nZhang, Hoifung Poon, and Tie-Yan Liu. 2022.\nBiogpt: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin Bioinformatics, 23(6).\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai\nWang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai,\nYu Qiao, and Ping Luo. 2023. Embodiedgpt: Vision-\nlanguage pre-training via embodied chain of thought.\narXiv preprint arXiv:2305.15021.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawa-\nhar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. 2023. Orca: Progressive learning from\ncomplex explanation traces of gpt-4. arXiv preprint\narXiv:2306.02707.\nOpenAI. 2023. Gpt-4 technical report.\nLiangming Pan, Alon Albalak, Xinyi Wang, and\nWilliam Yang Wang. 2023. Logic-lm: Empower-\ning large language models with symbolic solvers for\nfaithful logical reasoning.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. Biomegatron: Larger\nbiomedical domain language model. arXiv preprint\narXiv:2010.06060.\nTom Silver, Soham Dan, Kavitha Srinivas, Joshua B\nTenenbaum, Leslie Pack Kaelbling, and Michael\nKatz. 2023. Generalized planning in pddl do-\nmains with pretrained large language models. arXiv\npreprint arXiv:2305.11014.\n2685\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? arXiv\npreprint arXiv:2109.07154.\nRonen Tamari, Fan Bai, Alan Ritter, and Gabriel\nStanovsky. 2021. Process-level representation of\nscientific protocols with interactive annotation. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2190–2202, Online.\nAssociation for Computational Linguistics.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nLeonid Teytelman, Alexei Stoliartchouk, Lori Kindler,\nand Bonnie L Hurwitz. 2016. Protocols. io: virtual\ncommunities for protocol development and discus-\nsion. PLoS Biology, 14(8):e1002538.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. V oyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023b. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nZhenqin Wu, Bharath Ramsundar, Evan N. Feinberg,\nJoseph Gomes, Caleb Geniesse, Aneesh S. Pappu,\nKarl Leswing, and Vijay Pande. 2017. Moleculenet:\nA benchmark for molecular machine learning.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv\npreprint arXiv:2305.10601.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nZhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao,\nGeorge Karypis, and Alex Smola. 2023. Multi-\nmodal chain-of-thought reasoning in language mod-\nels. arXiv preprint arXiv:2302.00923.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. Lima: Less is more for alignment.\narXiv preprint arXiv:2305.11206.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022.\nLeast-to-most prompting enables complex reason-\ning in large language models. arXiv preprint\narXiv:2205.10625.\n2686\nBioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology\nAppendix\nA Dataset Filtering\nWe filter Protocols.io protocols such that (i) they\ncan be parsed and provided to an LLM, and (ii)\nthey are sufficiently challenging to serve as an eval-\nuation set. We performed the following automatic\nand manual filtering:\nAutomatic filtering Protocols were automati-\ncally removed if they:\n• Do not contain a description\n• Contain linked files that could not be parsed\n• Contain images that could not be parsed by\ntext-only models\n• Contain tables (standard LLMs can sometimes\nstruggle with table-based data representations\nwithout few-shot examples (Hegselmann et al.,\n2023))\n• Consist of fewer than three steps (such pro-\ntocols were insufficiently complex to demon-\nstrate multi-step planning)\nManual filtering Following automatic filtering,\nprotocols were manually removed if they:\n• Were not relevant to biology\n• Were considered poorly written to the extent\nthat a human could not accurately replicate\nthe protocol\nB Prompts\nB.1 Main experiments\nWe show the prompts we use for generating pseud-\nofunctions and pseudocode in Figure 6, predict-\ning pseudocode in Figure 7, summarising protocol\nsteps in Figure 8. We show the error messages we\nuse in the feedback loops in Figure 9. Figure 12,\nand the resulting generated protocol used in our\nreal-world experiment is found in Figure 13\nB.2 Lab experiments\nHere we show the prompts we use in Section 5 of\nthe paper. Figure 10 is the prompt provided to the\nCoT agent. Figure 11 shows the Langchain out-\nput form the agent. Figure 12 shows the prompt\nthat contains the retrieved pseudofunctions. Fi-\nnally, Figure 13 shows the pseudocode that was\ngiven to a biologist to execute in a laboratory.\nC Qualitative Evaluation\nWe show qualitative results for protocol id 145\nfrom BIOPROT. For further qualitative examples,\nplease refer to the BIOPROT dataset.\nTitle Ethanol precipitation of nucleic acids (Ep-\npendorf tubes)\nDescription Nucleic acid precipitation is used to\nconcentrate and/or purify nucleic acids. The be-\nlow protocol is based on the fact that nucleic acids\nare less soluble in alcohol than in more polar wa-\nter. Addition of salt further decreases solubility by\ncompeting for water dipoles; as does low temper-\nature. Please see the OpenWetWare website for\nmore details.\nSteps\n1. Add 1/10 volume of 3M sodium acetate, pH\n5.2 or 1/2 volume of 5M ammonium acetate.\nreagents\n2. Add 2-3 volumes of 100% Ethanol.\n3. Mix and freeze overnight in -20. NOTES In\ngeneral, the time you need to incubate in the\nfreezer depends on how much nucleic acid\nyou have, how big it is and the volume it is\nin. My general protocol is to freeze for 20\nmin to 1 hr at -80C. This seems to work well\nfor most things, but you may want to freeze\nlonger if you have only a small concentration\nof nucleic acid or if it is small in size(<15\nnucleotides). (Kathleen) NOTES If you are\nin a hurry, you can also dip you epi shortly\ninto liquid nitrogen. If you added enough\nethanol, the mix won’t freeze. Careful with\nisopropanol - it freezes more quickly. This\nworks well for me and saves me a lengthy\nincubation in the fridge. (Jasu)\n4. Spin at full speed in a standard microcen-\ntrifuge at 4 degrees for 30 minutes. 1800s\n5. Decant (or carefully pipet off) the supernatant.\n6. Dry the pellet. NOTES For this you can\nair dry (tubes open, 15 min) or dry in a\nspeedvac. DNA and RNA (if you don’t have\nRNases in your sample) are typically hearty\n2687\nModel Shuffle Feedback Functions Arguments\nPrecision Recall Ldn ↓ Precision Recall SciBERTScore BLEU\nLlama2-7B ✗ ✗ 83.6 49.8 0.74 76.2 41.4 79.8 0.048\nLlama2-7B ✗ ✓ 81.0 45.9 0.82 70.4 42.9 80.4 0.050\nLlama2-7B ✓ ✗ 82.2 45.1 0.63 70.7 43.8 81.3 0.051\nLlama2-7B ✓ ✓ 78.5 30.4 0.56 73.0 51.4 81.1 0.047\nTable 9: Evaluating Llama on Protocol Generation Evaluation Given a protocol title and description, and a set of\nadmissible pseudofunctions, we evaluate the model performance on full protocol generation. For all metrics higher\nvalues are better, except for the normalized Levenshtein distance Ldn, where lower values are better.\nModel Neighbourhood Precision Recall\nLLama2-7B Nearest 26.1 57.5\nLLama2-7B Random 28.1 56.3\nTable 10: Evaluating Llama on Function retrieval.\nPerformance on function retrieval of pseudofunctions\nfrom the query protocol, as well as (i) random or (ii)\nnearest neighbors protocols.\nenough for you to air dry at 37C, if desired.\nNOTES Overdrying can make DNA hard to re-\ndissolve. Especially for longer DNA, I avoid\nvacuum drying and airdry only briefly before\nre-dissolving. (Jasu)\n7. Add your desired quantity of water. V ortex\nand spin down to resuspend. NOTES Beware\nof using water unless you are sure of what you\nare getting in to. The \"pH\" of water can vary\nwidely (I’ve seen from pH 5 to pH 8.5), and\ndepurination of DNA at low pH or degradation\nof RNA at high pH are possibilities. Water\nalso typically contains trace metals, which can\naccelerate these reactions. I typically recom-\nmend resuspension in TE (10 mM Tris-HCl,\npH 7.5, 1 mM EDTA). This makes sure your\nnucleic acid is at a neutral pH and the EDTA\nwill chelate any trace metals. Since they are in\nsuch small amounts, neither the buffer nor the\nEDTA will affect most downstream reactions.\n(Kathleen)\nGenerated Pseudocode and Pseudofunctions\nWe show the generated pseudocode and pseudo-\nfunctions, which we use as ground truth, in Fig-\nure 4\nPredicted Protocol We show the predicted pro-\ntocol in Figure 5\nD LLama evaluation\nTo benchmark performance on open-source mod-\nels, we also conducted a run of our experimental\nevaluation tasks on Llama-2 (Touvron et al., 2023).\nWe evaluate the 7B model and report performance\non protocol generation and function retrieval in Ta-\nble 9 and Table 10, respectively. We found that\nLlama-2 significantly underperforms GPT-3.5 and\nGPT-4 models in function selection. As part of our\nevaluation on Llama-2 we observe that, when using\nfeedback, the model is distracted and does not at-\ntempt to re-write code. Iterative feedback appears\nto be a process that is effective for GPT models and\nnot Llama models, and this observation is consis-\ntent with prior work (Madaan et al., 2023). We also\nran Llama-2 on the next step prediction task, but\nwe found that the model was unable to complete\nthis task. The model would typically produce text\nthat states an intent to complete the pseudocode\nrather than writing the actual next pseudocode line.\nThis difference in behaviour is likely due to a dif-\nference in training regimes between GPT models\nand Lllama-2, but given the lack of documentation\naround the training of GPT models, the precise\nnature of this difference is unknown.\nE Dataset and Evaluation\nThe BIOPROT dataset and evaluation met-\nrics from this paper can be found at\nhttps://github.com/bioplanner/bioplanner\nF Human Benchmarking\nWhile we believe our metric is internally useful\nfor comparing the performance of LLM models\nand approaches, we wanted to assess how our tasks\nused relate to human performance. To this end, we\nperformed a human evaluation of next-step predic-\ntion tasks and function selection tasks. We worked\nwith an undergraduate biomedical sciences student\nand asked them to complete the next step predic-\ntion task and the function selection task. The stu-\ndent had access to internet search and an unlim-\nited amount of time to answer questions. With\nthe next step prediction task we provided shuffled\n2688\nfunctions, and with the function selection task, we\nused random distractor functions. For the func-\ntion selection task, Human Precision was 87.5%,\nand human Recall was 0.84%, (n=20), indicating\na significant increase in performance over GPT-4.\nGPT-4 performance is potentially weaker than hu-\nman performance in the function selection task due\nto the large number of nearest-neighbour functions\nin the context window acting as distractors from\nthe task instructions. For the next step prediction\ntask human accuracy was 54.8% (n=32), with Pre-\ncision and Recall or arguments being 97% and 95%\nrespectively. This performance is roughly compa-\nrable to GPT-4 in the shuffled function setting.\n2689\nFigure 4: Generated pseudofunctions and pseudocode. Given the protocol title, description, and free text\nstep-by-step instructions, we generate pseudocode and pseudofunctions.\nFigure 5: Predicted pseudocode. Given protocol title, description, and an admissible set of pseudofunctions, a\nmodel predicts pseudocode. This coresponds to the pseudofunctions in Figure 4.\n2690\nFigure 6: Prompt for generating pseudofunctions and pseudocode.\nFigure 7: Prompt for predicting pseudocode.\n2691\nFigure 8: Prompt for summarizing a protocol.\nFigure 9: Error messages for feedback loops.\nFigure 10: LLM query for protocol retrieval.\n2692\nFigure 11: Langchain output for protocol retrieval.\nFigure 12: Prompt to generate protocol from retrieved functions.\n2693\nFigure 13: The LLM generated protocol used in our lab experiment.\n2694"
}