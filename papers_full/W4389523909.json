{
  "title": "Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models",
  "url": "https://openalex.org/W4389523909",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092720860",
      "name": "Ilias Stogiannidis",
      "affiliations": [
        "Athens University of Economics and Business"
      ]
    },
    {
      "id": "https://openalex.org/A1906922223",
      "name": "Stavros Vassos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2075982744",
      "name": "Prodromos Malakasiotis",
      "affiliations": [
        "Athens University of Economics and Business"
      ]
    },
    {
      "id": "https://openalex.org/A1808727851",
      "name": "Ion Androutsopoulos",
      "affiliations": [
        "Athena Research and Innovation Center In Information Communication & Knowledge Technologies",
        "Athens University of Economics and Business"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3138154797",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2106411961",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W3197770791",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3034533785",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W4230085078",
    "https://openalex.org/W3173017111"
  ],
  "abstract": "Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN classifier or a Multi-Layer Perceptron, using two common business tasks, intent recognition and sentiment analysis. Experimental results indicate that significant OpEx savings can be obtained with only slightly lower performance.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14999–15008\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCache me if you Can: an Online Cost-aware Teacher-Student Framework\nto Reduce the Calls to Large Language Models\nIlias Stogiannidis1, 2 Stavros Vassos2 Prodromos Malakasiotis1, 4 Ion Androutsopoulos1, 3\n1Department of Informatics, Athens University of Economics and Business, Greece\n2Helvia.ai\n3Archimedes Unit, Athena Research Center, Greece\n4 Workable\n{stoyianel, rulller, ion}@aueb.gr, stavros@helvia.ai\nAbstract\nPrompting Large Language Models (LLMs)\nperforms impressively in zero- and few-shot\nsettings. Hence, small and medium-sized en-\nterprises (SMEs) that cannot afford the cost\nof creating large task-specific training datasets,\nbut also the cost of pretraining their own LLMs,\nare increasingly turning to third-party services\nthat allow them to prompt LLMs. However,\nsuch services currently require a payment per\ncall, which becomes a significant operating ex-\npense (OpEx). Furthermore, customer inputs\nare often very similar over time, hence SMEs\nend-up prompting LLMs with very similar in-\nstances. We propose a framework that allows\nreducing the calls to LLMs by caching previ-\nous LLM responses and using them to train\na local inexpensive model on the SME side.\nThe framework includes criteria for deciding\nwhen to trust the local model or call the LLM,\nand a methodology to tune the criteria and mea-\nsure the tradeoff between performance and cost.\nFor experimental purposes, we instantiate our\nframework with two LLMs, GPT-3.5 or GPT-4,\nand two inexpensive students, ak-NN classifier\nor a Multi-Layer Perceptron, using two com-\nmon business tasks, intent recognition and sen-\ntiment analysis. Experimental results indicate\nthat significant OpEx savings can be obtained\nwith only slightly lower performance.\n1 Introduction\nPrompting pre-trained Large Language Models\n(LLMs) aligned to follow instructions (Ouyang\net al., 2022; Köpf et al., 2023) performs impres-\nsively well in zero- and few-shot settings. Hence,\nsmall and medium-sized enterprises (SMEs) that\ncannot afford the cost of creating large task-specific\ntraining datasets for model fine-tuning, but also\nthe cost of pretraining their own LLMs, are in-\ncreasingly turning to third-party services that allow\nthem to prompt LLMs. For example, SMEs that\nprovide customer support chatbots prompt LLMs\nlike GPT-4 (OpenAI, 2023) to detect user intents\nCache\nStudent\nTeacher\nIncoming \ninstance\nIncoming instance\nStudent response\nReliability indicators Criteria\nCall the\nteacher\nIncoming \ninstance\nIncoming instance\nTeacher response\nRetraining\nUse the student\nresponse\nTeacher response\nCustomer\nStudent response\nFigure 1: OCaTS architecture.\nand drive the chatbot-customer interaction (Ham\net al., 2020). The best LLMs, however, currently\nrequire a payment per prompting call, and these\npayments become a significant operating expense\n(OpEx) for SMEs. Furthermore, customer inputs\n(e.g., dialog turns) are often very similar over time,\nhence SMEs end up calling LLMs to handle inputs\nthat may be very similar to inputs already handled\nby the LLMs in previous (already paid) calls.\nWe introduce the Online Cost-aware Teacher-\nStudent (OCaTS) framework that allows reducing\nthe calls to a commercial LLM, treated as a teacher\nmodel, by caching its previous responses and using\nthem to train a local inexpensive student model.\nOCaTS includes criteria for deciding when to trust\nthe student or call the teacher, and a methodology to\ntune the criteria and measure the tradeoff between\nperformance and cost. Unlike common teacher-\nstudent training for knowledge distillation (Hinton\net al., 2015; Gou et al., 2021), here the teacher\ndoes not train the student on all the available in-\nstances (in our case, all the incoming customer\ninputs). Also, unlike teacher-student approaches\nto self-training (Mi et al., 2021; Li et al., 2021),\nthe teacher is already reasonably effective (but ex-\npensive). In that sense, our work is closer to ac-\n14999\ntive learning (Settles, 2012; Monarch, 2021), but\nOCaTS trains the student on labels provided by a\nteacher LLM, not humans, and there is initially no\nlarge pool of unlabeled instances (customer inputs)\nto select from, as instances arrive online.\nOCaTS can be used with any service that allows\nprompting LLMs, and any kind of local student\nmodel. For experimental purposes, we instanti-\nate OCaTS with GPT-3.5 or GPT-4 as the teacher,\nand a k-NN or Multi-Layer Perceptron (MLP) clas-\nsifier as the student, using an intent recognition\ndataset from the banking domain or a sentiment\nanalysis dataset. Experimental results indicate that\nsignificant OpEx savings can be obtained with only\nslightly lower performance. For example, thek-NN\nstudent can handle approximately two-thirds of the\nincoming instances (customer inputs) of the intent\nrecognition task without calling the GPT-4 teacher\n(Fig. 2, left, red line) for a decrease of less than\n0.5 percentage points in accuracy (Fig. 2, middle,\nred and black lines). OCaTS introduces discounted\nversions of common evaluation measures (e.g., ac-\ncuracy) that allow an SME to quantify how much\nit prefers to lean towards fewer calls or less user\nfrustration (different λvalues in Fig. 2).\nOur main contributions are: (i) We introduce a\ngeneral teacher-student framework that helps SMEs\nreduce the prompting calls to commercial LLMs\nand the corresponding OpEx costs by caching the\nresponses of the LLMs and training inexpensive\nlocal student models. (ii) We introduce discounted\nversions of common evaluation measures that al-\nlow the SMEs to quantify how much they prefer\nfewer LLM calls vs. increased user frustration (e.g.,\ncaused by lower accuracy) and tune the frame-\nwork’s criteria that decide when to trust the local\nstudent model or call the LLM teacher accordingly.\n(iii) We instantiate the framework with GPT-3.5 or\nGPT-4 as teachers, and a k-NN or MLP classifier\nas students. (iv) We perform experiments on two\nwell-known tasks for SMEs, intent recognition and\nsentiment analysis, and show that significant cost\nsavings can be obtained with only slightly lower\nperformance. This is a first step towards exploring\nthe benefits of the proposed framework with more\ndatasets, models, and business scenarios.\n2 Framework\nArchitecture: The proposed framework (OCaTS)\nconsists of three main components (Fig. 1): a\nteacher, typically a resource-intensive model of-\nfering premium results; a student, a cost-effective\nmodel that is typically much smaller and simpler\nthan the teacher; a cache, a repository of incom-\ning instances (e.g., customer requests) that have\nalready been processed by the teacher. We assume\nthat the framework is employed to handle a task for\nwhich there is no available large dataset for super-\nvised training, apart from a few incoming instances\n(possibly a handful per class) annotated with the\nground truth (e.g., correct labels). This is a very\ncommon case for SMEs that cannot afford the cost\nof creating large task-specific training datasets, but\ncan easily construct small numbers of demonstra-\ntion instances. The teacher-student setting isonline,\nas every incoming instance is handled at inference\ntime as follows. First, the student is called to handle\nthe instance. Then some student- and task-specific\ncriteria, which assess the reliability of the student’s\noutput, indicate if the student’s output (e.g., label)\nshould be used or if the teacher should be consulted.\nIf the student’s output is selected, it is returned as\nthe response to the incoming instance. Otherwise,\nthe teacher is called to handle the instance. In the\nlatter case, the instance along with the teacher’s\nresult are stored in the cache. Depending on the\ntype of student, periodic re-training takes place, to\nupdate the student with the cached instances.\nInstantiations: In the experiments of this paper,\nwe instantiate OCaTS with a GPT-3.5 or GPT-4\nteacher, a distance-weighted k-NN or MLP clas-\nsifier as the student, for a single-label classifica-\ntion task (intent recognition or sentiment analysis).\nIn all cases, we represent each incoming instance\n(customer request) by its MPNet-based (Song et al.,\n2020) vector representation (text embedding) and\nwe use two criteria (Fig. 1) to decide when to use\nthe student’s response or invoke the teacher: (i) the\nentropy of the probability distribution (over the la-\nbel set) produced by the student (k-NN or MLP) for\nthe incoming instance, and (ii) the distance of the\nvector representation of the incoming instance from\nthe centroid of the vector representations of the k\nmost similar cached instances. Consult Nguyen\net al. (2022) for other possible criteria. We leave\nother instantiations of OCaTS (other teachers, stu-\ndents, tasks, representations) for future work.\nDiscounted evaluation measures:The main goal\nof the proposed architecture is to reduce the number\nof calls to the expensive teacher model by caching\nprevious teacher responses and using them to train\na local inexpensive student model on the SME side.\n15000\n0 1000 2000 3000\nNumber of incoming instances\n0\n500\n1000\n1500\n2000\n2500\n3000Number of calls to the T eacher\n0 1000 2000 3000\nNumber of incoming instances\n70\n72\n74\n76\n78\n80\n82\n84Accuracy (%)\n0 1000 2000 3000\nNumber of incoming instances\n50\n55\n60\n65\n70\n75\n80Discounted Accuracy (%)\nOCaTS, =0.05\nGPT-4, =0.05\nOCaTS, =0.1\nGPT-4, =0.1\nOCaTS, =0.2\nGPT-4, =0.2\nOCaTS, =0.3\nGPT-4, =0.3\nGPT-4\nFigure 2: Number of calls to the teacher (left), accuracy (middle), discounted accuracy (right), using a GPT-4\nteacher and a k-NN student, for various λvalues, on Banking77 data. The larger the λthe more the SME prefers\nfewer calls at the expense of increased user frustration. Dashed lines show the discounted accuracy when calling\nGPT-4 for all incoming instances. OCaTS has a better discounted accuracy than always calling the GPT-4 teacher.\nThis introduces a tradeoff between the OpEx cost\nof calling the teacher and the frustration of the end-\nusers when the less accurate student model is used\ninstead. To quantify this tradeoff, we introduce a\ndiscounted variant ˆϕ of any common evaluation\nmeasure ϕ(e.g., accuracy, F1), as follows:\nˆϕ = ϕ−λ·M\nN = ϕ−λ·ρ, (1)\nwhere N is the number of incoming instances that\nhave been processed (on which ϕis measured), M\nis the number of calls made to the teacher while\nprocessing the Ninstances, ρ= M\nN shows for what\npercentage of the incoming instances we call the\nteacher, and λis a scalar specifying how intensively\nthe measure should be discounted. Assume, for\nexample, that the accuracy of the teacher-student\ncombination is ϕ = 0.8, but that this accuracy is\nachieved with ρ = 1\n3 . If the SME considers this\nρvalue (which would translate, e.g., to a monthly\ncost) as costly as a loss of five percentage points\nof accuracy, then ˆϕ = 0.75, and Eq. 1 becomes\n0.75 = 0.8 −λ·1\n3 , from which we obtain λ =\n0.15. Larger (or smaller) λvalues correspond to\ncases where the SME considers the same ρvalue\nmore (or less) costly in terms of loss of accuracy\npoints. We can also reformulate Eq. 1 as δ =\nλ·ρ, where δ = ϕ−ˆϕshows how much ϕgets\ndiscounted to account for the cost of ρ. Then λcan\nintuitively be thought of as a currency exchange\nrate, showing how expensiveρis in terms of δ(e.g.,\nloss of accuracy in percentage points).1\n1We implicitly assume that the exchange rate λis constant\nfor all the values of δand ρ. In practice, it may be different for\ndifferent ranges of δand ρ, but we leave this for future work.\n3 Main experiments\nHere we discuss the experiments we conducted\nwith the GPT-4 teacher, the k-NN student, and\nthe banking intent recognition dataset. In the Ap-\npendix, we report two additional sets of experi-\nments, one where we replaced the k-NN student\nby an MLP (Appendix E) keeping the rest of the\nsetup unchanged, and one where we replaced the\ntask/dataset (by sentiment analysis) and the teacher\n(by the cheaper GPT-3.5) otherwise keeping the\nsetup of the initial experiments (Appendix F). The\nadditional experiments verify the conclusions of\nthe experiments of this section.\nIntent recognition dataset:In this section, we\nuse Banking77 (Casanueva et al., 2020), an intent\nrecognition dataset from the banking customer ser-\nvice domain. It includes 13,083 customer messages.\nThe ground truth assigns to each message a single\nlabel (intent) from the 77 available. The dataset\nis divided into training (10,003 instances) and test\n(3,080) subsets. Appendix A shows more statistics.\nFew-shot training and development sets:Assum-\ning that an SME can only afford to construct a\nsmall number of training instances per class, we\nuse only 3 ×77 = 231instances from the original\ntraining set of Banking77, three per class, as a few-\nshot version of the training set. The 231 instances\nwere manually selected to avoid unclear cases, e.g.,\nsimilar instances with different ground truth labels.\nSimilarly, we created a few-shot development set\nof 13 ×77 = 1,001 instances from the original\ntraining set, for hyperparameter tuning.\n15001\nIncoming instances and evaluation measure:We\nuse the original test set of Banking77 as the in-\ncoming instances. We repeat each experiment with\nfive random shufflings of the test set (to obtain five\ndifferent streams of input instances) and report aver-\nage scores over the shufflings. We setϕto accuracy,\nsince the test set is balanced (Appendix A).\nTeacher: In this section, we use GPT-4 (OpenAI,\n2023) as the teacher, the most capable LLM for\nfew-shot in-context learning tasks at the time. Each\nprompt includes instructions, demonstrators (in-\ncontext few-shot examples), and the incoming in-\nstance to be classified; see Appendix B for details.\nStudent: In this section, a distance-weightedk-NN\nclassifier is used as the student. Vector representa-\ntions of the incoming instances are generated with\na Sentence-Transformer (Reimers and Gurevych,\n2019) variation of MPNet (Song et al., 2020). 2\nAppendix C provides more information on the dis-\ntance weighting used. It also shows (Fig. 7) that\nin a more conventional setting, where a large man-\nually labeled training set is available, the k-NN\nclassifier clearly outperforms GPT-4 in accuracy\n(92% vs. 82%). Note that for the k-NN student,\nno retraining (Fig. 1) is necessary, since the cache\ncoincides with the memory of the k-NN classifier.\nThe cache is initialized with the 3-shot training\nexamples of the classes (231 instances in total).\nCriteria: We instantiate the criteria of Fig. 1 with\ntwo conditions. Both have to be satisfied for the\nstudent’s response to be used; otherwise, we call\nthe teacher. The first condition is that the cosine dis-\ntance between the (MPNet-based) vector represen-\ntation of the incoming message and the weighted\ncentroid vectorc of the knearest neighbors should\nbe less than a threshold tc. Here c = ∑k\ni=1 ˆwi ·vi,\nand ˆwi = wi/∑k\nj=1 wj, where wi is the weight\nassigned by distance weighting (Appendix C) to\nthe i-th neighbour, and vi is the (MPNet-based)\nvector representation of the neighbour. Intuitively,\nthis condition ensures that the incoming instance is\nsufficiently close to cached instances.\nTo define the second condition, letCbe the set of\nthe labels (classes) of the knearest neighbors (here-\nafter simply neighbors). Let wi,c be the weight (as-\nsigned by distance weighting) to the i-th neighbour\nbelonging in class c, and let Wc be the sum of all\nweights of neighbors of class c, i.e., Wc = ∑\ni wi,c.\n2We used gpt-4-0314 and all-mpnet-base-v2, in par-\nticular, for the teacher and student, respectively.\nWe define the probability pc of each c∈Cas:\npc = exp(Wc)∑\nc′∈C exp(Wc′)\nThe entropy Hof the probabilities pc of the labels\nof the neighbors is:\nH= −\n∑\nc∈C\npc log pc.\nThe second criterion requires Hw to be less than a\nthreshold tH. Intuitively, it requires the neighbors\nto agree on the label of the incoming instance.\nHyperparameter tuning:There are three hyper-\nparameters here, the number of neighbors k, and\nthe thresholds tc, tH. We fix k = 5as a practical\nchoice considering that there are 3 examples per\nclass initially. For each indicative λvalue (0.05,\n0.1, 0.2, 0.3), we employ Bayesian optimization\non the few-shot development set (Section 3) to de-\ntermine the optimal combination of the two thresh-\nolds that maximize ˆϕ(discounted accuracy). We\nlet tc range in [0,2], and tH in [0,4.34].3 We use\nOptuna’s (Akiba et al., 2019) implementation of\nthe Tree-Structured Parzen Estimator (TSPE) algo-\nrithm (Bergstra et al., 2011) after first performing a\n10×10 grid search on the range of values of the two\nthresholds as a head start. The resulting contour\nmaps and the optimal values of the two thresholds\nper λvalue can be found in Appendix D.\nResults: We evaluate OCaTS for each of the four\nindicative λvalues, using the same incoming in-\nstances (original test set of Banking 77), and the\nλ-specific tuned thresholds tc, tH. As illustrated in\nFig. 2, OCaTS succeeds in managing the tradeoff\nbetween calls to the teacher vs. accuracy. Figure 2\n(left) shows that as the discount factor λincreases,\nfewer calls to the teacher are made. In Fig. 2 (mid-\ndle), we see how much accuracy is sacrificed for\nthis OpEx relief. In particular, for λ = 0.05 the\naccuracy of OCaTS is very close to the accuracy of\nthe GPT-4 teacher, within a margin of 0.37 percent-\nage points (83.05% vs. 82.68% for the entire test\nset), while calling the teacher for only 1/3 of the\nincoming instances (1050 out of 3080). For higher\nvalues of λ, we see the intended drop in accuracy\nto achieve an increasingly smaller number of calls\nto the teacher. Figure 2 (right) shows that the dis-\ncounted accuracy ˆϕof OCaTS (solid lines, one per\n3The maximum value of Hwith 77 classes is 4.34, when\nusing natural logarithms. The upper bound of tc was chosen\nbased on initial experiments on development data.\n15002\nλ value) is always clearly higher than the corre-\nsponding discounted accuracy of always calling\nthe GPT-4 teacher (dashed lines). Hence, OCaTS\nis clearly better than always calling the teacher, if\nOpEx costs are taken into account. The difference\nincreases (in favor of OCaTS) as λincreases, i.e.,\nas reducing OpEx costs becomes more important.\n4 Conclusions\nWe introduced an Online Cost-aware Teacher-\nStudent framework (OCaTS) to help SMEs reduce\nOpEx costs by caching the responses of commer-\ncial LLMs and training inexpensive local students.\nWe also introduced discounted versions of common\nevaluation measures, allowing SMEs to quantify\nthe trade-off between LLM calls and user frustra-\ntion. By instantiating OCaTS with a GPT-4 teacher\nand a k-NN student and experimenting with an\nintent recognition dataset from the banking do-\nmain (Banking77), we showed that the calls to the\nteacher can be significantly reduced (by 1/3) with\nonly a slight performance drop (0.37 percentage\npoints). Additional experiments with an MLP stu-\ndent on the same dataset led to the same findings\n(Appendix E). Further experiments with a GPT-3.5\nteacher, the initial k-NN student, and a sentiment\nanalysis dataset (LMR) also confirmed the conclu-\nsions of the previous experiments (Appendix F).\nIn future work, we plan to experiment with more\ndatasets and tasks (e.g., question answering), and\nsuggest adaptive policies for λ to allow higher\nOpEx costs (more frequent calls to the teacher)\nwhen the cache is cold and be more selective (call-\ning the teacher less frequently) later on. We also\nplan to enhance OCaTS with indicators of how\nmuch we can trust the teacher responses (e.g.,\nconfidence of the teacher). Finally, we intend to\nincorporate more financial metrics (e.g., student\ncosts) in the discounted versions of the evaluation\nmeasures and study more complex strategies (e.g.,\ngame-theoretic, reinforcement learning) to select\nthe thresholds that determine when to trust the stu-\ndent or call the teacher.\n5 Limitations\nThe main scope of this work was to propose a flex-\nible framework (OCaTS) that will allow SMEs to\nreduce the OpEx costs when incorporating com-\nmercial LLMs in their solutions. We considered\nonly two instantiations of the teacher (GPT-4, GPT-\n3.5) and two instantiations of the student ( k-NN,\nMLP) in two tasks (intent recognition, sentiment\nanalysis), leaving further instantiations for future\nwork. Although LLMs like GPT-4 and GPT-3.5\ncan in principle be used for zero-shot inference, we\nconsidered in-context learning with a few demon-\nstrator examples per class. These examples where\nmanually selected to be diverse and indicative of\nthe corresponding classes. This is realistic to some\nextent; SMEs often request a small number of ex-\namples from their customers, but the quality of\nthese examples is not always guaranteed. In addi-\ntion, the test sets we used (from Banking77 and\nLMR) were balanced and thus not entirely realistic.\nHowever, we shuffle the stream of incoming (test)\ninstances which, hence, do not arrive in a uniform\nway with the respect to their classes. Also, to tune\ntc and tH, we used a development set, extracted\nfrom the original training data. Such a develop-\nment set is not always available in practice, but\nwe used it for the sake of the analysis. Interested\nSMEs can use our analysis, as a starting point for\ntheir applications and reduce the number of trials\nneeded to find suitable values for tc and tH.\nAnother limitation is that ˆϕtakes into considera-\ntion only the cost to call the teacher (ρ), and indi-\nrectly the frustration of the user, as implied by the\nperformance drop. A more detailed analysis would\nalso incorporate the student cost and other financial\nmetrics possibly with different weights; OCaTS\ncan be easily extended in that direction. Finally, we\ndid not compare against existing caching libraries,\ne.g., GPTCache.4 These libraries are quite sim-\nplistic and less flexible than OCaTS, which can be\nused with a variety of teacher-student settings.\n6 Ethics statement\nConstantly querying LLMs to solve everyday tasks\nis not only costly; it has a large energy footprint\nas well. Our framework aims to alleviate both\nphenomena. Nonetheless, our study required a sig-\nnificant amount of resources. We believe, however,\nthat by making the framework and the analysis\npublicly available, we can pave the way towards\nreducing the resources required by SMEs to handle\ntheir day-to-day tasks in the long run.\nAcknowledgements\nThis work was supported by Google’s TPU Re-\nsearch Cloud (TRC) program.5\n4https://github.com/zilliztech/GPTCache\n5https://sites.research.google/trc/about/\n15003\nReferences\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru\nOhta, and Masanori Koyama. 2019. Optuna: A Next-\ngeneration Hyperparameter Optimization Framework.\nIn Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and Data\nMining.\nJames Bergstra, Rémi Bardenet, Yoshua Bengio, and\nBalázs Kégl. 2011. Algorithms for Hyper-Parameter\nOptimization. In Advances in Neural Information\nProcessing Systems, volume 24. Curran Associates,\nInc.\nIñigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efficient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45, On-\nline. Association for Computational Linguistics.\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and\nDacheng Tao. 2021. Knowledge distillation: A sur-\nvey. Int. J. Comput. Vision, 129(6):1789–1819.\nDonghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, and\nKee-Eung Kim. 2020. End-to-end neural pipeline\nfor goal-oriented dialogue systems using GPT-2. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 583–592,\nOnline. Association for Computational Linguistics.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the Knowledge in a Neural Network.\nArXiv, abs/1503.02531.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver\nStanley, Richárd Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire,\nChristoph Schuhmann, Huu Nguyen, and Alexan-\nder Mattick. 2023. OpenAssistant Conversations –\nDemocratizing Large Language Model Alignment.\nShiyang Li, Semih Yavuz, Wenhu Chen, and Xifeng Yan.\n2021. Task-adaptive Pre-training and Self-training\nare Complementary for Natural Language Under-\nstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, pages 1006–\n1015, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nFei Mi, Wanhao Zhou, Lingjing Kong, Fengyu Cai,\nMinlie Huang, and Boi Faltings. 2021. Self-training\nImproves Pre-training for Few-shot Learning in Task-\noriented Dialog Systems. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1887–1898, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nRobert Monarch. 2021. Human-in-the-Loop Machine\nLearning. Manning Publications.\nVu-Linh Nguyen, Mohammad Hossein Shaker, and\nEyke Hüllermeier. 2022. How to Measure Uncer-\ntainty in Uncertainty Sampling for Active Learning.\nMach. Learn., 111(1):89–122.\nOpenAI. 2023. GPT-4 technical report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730–27744.\nCurran Associates, Inc.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nBurr Settles. 2012. Active Learning. Morgan & Clay-\npool Publishers.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. In NeurIPS\n2020. ACM.\nAppendix\nA Statistics of the Banking77 dataset\nFigure 3 shows the label distribution of the original\ntraining and test subsets of the Banking77 intent\nrecognition dataset. The training subset exhibits a\nsignificant class imbalance (Fig. 3, left), whereas\nthe test subset is balanced (right). In Table 1, we\nprovide further statistics which, along with the la-\nbel distribution, support the selection of the dataset\nas a realistic case to perform our experiments.\nB More details about the LLM teachers\nTo prompt the LLMs, we used the chat completion\nAPI provided by OpenAI, which takes as input a\n15004\nFigure 3: Label distribution of the original train (left) and test (right) subsets of Banking77.\nStatistics Train Test\nNumber of examples 10,003 3,080\nMinimum length in characters 13 13\nAverage length in characters 59.5 54.2\nMaximum length in characters 433 368\nMinimum length in words 2 2\nAverage length in words 11.9 10.9\nMaximum length in words 79 69\nNumber of intents 77 77\nTable 1: Statistics of Banking77.\nYou are an expert assistant in the field of\ncustomer service. Your task is to help workers\nin the customer service department of a company.\nYour task is to classify the customer’s question\nin order to help the customer service worker to\nanswer the question. In order to help the worker\nyou MUST respond with the number and the name of\none of the following classes you know.\nIn case you reply with something else, you will\nbe penalized.\nThe classes are:\n- activate_my_card\n- age_limit\n...\nFigure 4: System message used in the GPT-4 teacher.\nsystem message (instructions) along with a history\nof user and assistant messages, and generates an\nassistant message as output. The system message\nspecifies the model’s behavior as a chat assistant\n(Fig. 4). For our few-shot setting, we add to the sys-\ntem message a few pairs of user-assistant messages\nfrom the training set as demonstrators (Fig. 5). The\nincoming instance to be classified is added as the\nlast user message in the history.\nC Distance weighting in the k-NN student\nThe k-NN classifier assigns a weight to each one\nof the knearest neighbors of an incoming instance.\nUser: My new card is here, what’s the process for\nactivating it?\nAssistant: activate_my_card\nUser: I am unable to activate my card, it won’t\nlet me.\nAssistant: activate_my_card\nUser: Can you help me activate my card\nAssistant: activate_my_card\nUser: What is the youngest age for an account?\nAssistant: age_limit\nUser: What is the appropriate age for my child to\nbe able to open an account?\nAssistant: age_limit\nUser: How do I set up an account for my children?\nAssistant: age_limit\n...\nFigure 5: Demonstrators (in-context few-shot examples)\nused in the GPT-4 teacher as conversation history.\nThe weight is inversely proportional to the square\nof the cosine distance between the vectors of the\nincoming instance and the neighbor,wi = 1\ndi2 . The\nclass with the largest sum of neighbor weights is\nthen assigned to the incoming instance.\nFigure 7 shows the learning curve of a distance-\nweighted k-NN classifier that is trained on the orig-\ninal training set of Banking77 and evaluated on the\nfew-shot development set (Section 3). We observe\nthat with sufficient training data (approx. 1000 in-\nstances) the k-NN classifier clearly outperforms\nGPT-4 in accuracy (92% vs. 82%), which justifies\nour choice to use it as a student in our experiments.\nD Hyperparameter tuning\nWe tuned the thresholds tc and tH anew for each\nλ value. For each λ value, we first performed a\n10 ×10 grid search on the ranges of values of\n15005\nFigure 6: Contour plots (discounted accuracy) obtained during threshold tuning in the main experiments (GPT-4\nteacher, k-NN student, Banking 77 data), for various λvalues.\n0 2000 4000 6000 8000\nTraining Size\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy\nLearning Curve\nk-NN\nGPT-4\nFigure 7: Learning curve of the distance-weighted k-\nNN student (solid line) when trained on the original\ntraining set of Banking77 and evaluated (accuracy) on\nthe few-shot development set. Accuracy of GPT-4 on\nthe few-shot development set also shown (dashed line).\nthe two thresholds, to evaluate indicative combina-\ntions for the thresholds. These are considered as\nstarting points for the Bayesian optimization (Sec-\ntion 3). Figure 6 illustrates the presence of several\ngood points adjacent to the best point in the main\nexperiments (Section 3), all of which maximize\nthe discounted metric to a significant extent. This\nSystem message: Analyze the sentiment of the\nfollowing reviews and classify them as either\n‘positive’ or ‘negative’.\nUser: When I started watching this movie I saw\nthe dude from Buffy, Xander, and figured ah how\nnice that he’s still making a living acting in\nmovies. Now a weird movie I can stand, given that\nit’s a good dose of weird like for example David\nLynch movies, twin peaks, lost highway etc. [...]\nIt wasn’t his acting though, that was alright,\nbut the script just didn’t make any sense. Sorry.\nAssistant: negative\nUser: As part of the celebration of the release\nof Casino Royale,v this film with the new Bond\nstarring in it was shown, from director Roger\nMichell (Notting Hill). [...] Also starring Peter\nVaughan as Toots, Danira Govich as Au Pair,Harry\nMichell as Harry, Rosie Michell as Rosie and Johnny\nEnglish’s Oliver Ford Davies as Bruce. Very good!\nAssistant: positive\n...\nFigure 8: System message and demonstrators when\nusing GPT-3.5 in the sentiment analysistask.\nshows that several threshold combinations may be\nconsidered optimal. Moreover there are large areas\noften with a wide range of values for one thresh-\nold that are comparable in terms of maximizing\nthe discounted accuracy measure. Table 2 provides\nthe optimal threshold combination as selected by\n15006\nthe optimization process for each λvalue. We can\nobserve that the tuned value for tc decreases from\nλ = 0.2 to λ = 0.3, instead of increasing, which\ncan be accounted for by the aforementioned obser-\nvation that multiple points maximize ˆϕ. However,\nwe also notice the general increasing trend for both\nthresholds, which leads to fewer calls to the GPT-4\nteacher, as one would expect, since higher λvalues\nmean that calls to the teacher are more costly.\nλ tH tc\n0.05 0.8359 0.2269\n0.1 1.324 0.5656\n0.2 1.558 0.76\n0.3 2.336 0.4993\nTable 2: Tuned thresholds perλin the main experiments\n(GPT-4 teacher,k-NN student, Banking77 dataset).\nE MLP student instead of k-NN student\nAs a first step to test the generality of the con-\nclusions of our main experiments (Section 3), we\nrepeated the main experiments this time using a\nMulti-layer Perceptron (MLP) student, instead of\nthe k-NN student. The MLP has one hidden layer\nwith a ReLU activation function, followed by a\ndropout layer, then an output layer with 77 neu-\nrons (number of classes) and a softmax. We use\ncross-entropy loss. Again, we initially filled the\ncache with 3 few-shot training instances per class\n(3 ×77 = 231in total) from the original training\nset (the same ones as in the main experiments), and\nthe MLP was initially trained on them. We retrain\nthe MLP every 100 calls to the teacher, i.e., when-\never 100 new training instances have been added to\nthe cache. We used multi-objective Bayesian opti-\nmization on the few-shot development set (treated\nas an incoming stream of user requests), optimizing\nfor both loss and accuracy (no cost factors), to find\nthe optimal hyperparameters for the MLP architec-\nture. Subsequently, we tuned the threshold values\n(tc, tH) for each lambda again using Bayesian opti-\nmization, as we did in the main experiments. The\ntuned hyperparameters and thresholds are shown in\nTable 3 and Table 4, respectively. For every incom-\ning test instance, the entropy H(Section 3) is now\ncomputed using the output probabilities of the MLP.\nThe other criterion (Fig. 1) remains unchanged, i.e.,\nit requires the cosine distance of the MPNet-based\nvector of the incoming instance to the centroid of\nthe kmost similar cached instances to be less than\ntc; we use k = 5, as in the main experiments. As\nshown in Fig. 9, the experimental results with the\nMLP student are very similar to those of the main\nexperiments (cf. Fig. 7).\nHyper-parameter Range Value\nlearning rate [10−5,0.1] 1.6 ·10−5\nhidden layer neurons {256,..., 1024} 1024\ndropout rate [0.1,0.5] 0.22\nTable 3: Tuned hyper-parameters in the Banking77 ex-\nperiments with the MLP studentand GPT-4 teacher.\nλ tH tc\n0.05 0.48 0.896\n0.1 0.479 1.553\n0.2 0.583 1.464\n0.3 0.646 1.882\nTable 4: Tuned thresholds per λin the Banking77 ex-\nperiments with the MLP studentand GPT-4 teacher.\nF Sentiment analysis experiments\nAs a further step to confirm the conclusions of the\nprevious experiments (Section 3, Appendix E), we\nconducted an additional set of experiments, now\nusing a sentiment analysis task. We used the Large\nMovie Review (LMR) dataset (Maas et al., 2011),\nwhich consists of 50,000 IMDB reviews, and two\nlabels (positive, negative review). Table 5 shows\nmore statistics for this dataset.\nAgain, we assume that an SME can afford to\ncreate only a small number of training instances\nper class. To simulate this limited data setting, we\ncreated few-shot versions of the training and devel-\nopment sets of LMR, much as in Section 3. For\nthe few-shot training set, we randomly selected 10\ninstances from the original training set, 5 for each\nsentiment, as follows. We employed the GPT-3.5\ntokenizer from OpenAI6 and computed the token\nsize for each review. Subsequently, we randomly\nchose reviews with token sizes close to the median.\nSimilarly, for the few-shot development set, we ran-\ndomly selected 1,000 instances from the original\ntraining set, 500 for each sentiment. Finally, due\nto limited resources, we used only 5,000 instances\n6https://github.com/openai/tiktoken\nStatistics Train Test\nNumber of examples 25,000 25.000\nMinimum length in characters 52 32\nAverage length in characters 1325.06 1293.7\nMaximum length in characters 13704 12988\nMinimum length in words 10 4\nAverage length in words 233.7 228.5\nMaximum length in words 2470 2278\nNumber of sentiments 2 2\nTable 5: Statistics of Large Movie Review (LMR).\n15007\n0 1000 2000 3000\nNumber of incoming instances\n0\n500\n1000\n1500\n2000\n2500\n3000Number of calls to the T eacher\n0 1000 2000 3000\nNumber of incoming instances\n76\n78\n80\n82\n84\n86Accuracy (%)\n0 1000 2000 3000\nNumber of incoming instances\n50\n55\n60\n65\n70\n75\n80\n85Discounted Accuracy (%)\nOCaTS, =0.05\nGPT-4, =0.05\nOCaTS, =0.1\nGPT-4, =0.1\nOCaTS, =0.2\nGPT-4, =0.2\nOCaTS, =0.3\nGPT-4, =0.3\nGPT-4\nFigure 9: Number of calls to the teacher (left), accuracy (middle), and discounted accuracy (right), using a GPT-4\nteacher and an MLP student, for various λvalues, on Banking77 data. In the left sub-figure, the green line (OCaTS,\nλ= 0.2) is not visible, because it overlaps with the purple one (OCaTS, λ= 0.3). The results are very similar to\nthose of the main experiments (cf. Fig. 7). Again, OCaTS (right, solid lines) has better discounted accuracy than\nalways calling the teacher (right, dashed lines) for all four indicative λvalues. The larger the λ, the fewer the calls\nto the teacher (left), at the expense of reduced accuracy (middle).\n0 1000 2000 3000 4000 5000\nNumber of incoming instances\n0\n1000\n2000\n3000\n4000\n5000Number of calls to the T eacher\n0 1000 2000 3000 4000 5000\nNumber of incoming instances\n84\n86\n88\n90\n92\n94Accuracy (%)\n0 1000 2000 3000 4000 5000\nNumber of incoming instances\n65\n70\n75\n80\n85\n90Discounted Accuracy (%)\nOCaTS, =0.05\nGPT-3.5, =0.05\nOCaTS, =0.1\nGPT-3.5, =0.1\nOCaTS, =0.2\nGPT-3.5, =0.2\nOCaTS, =0.3\nGPT-3.5, =0.3\nGPT-3.5\nFigure 10: Number of calls to the teacher (left), accuracy (middle), and discounted accuracy (right), using a GPT-3.5\nteacher and a k-NN student, for various λvalues, on sentiment analysis(LMR) data. The results are very similar to\nthose of the previous experiments (cf. Figures 2 and 9).\nof the original test set as the stream of incoming\ninstances. As in the previous experiments, we re-\npeat each experiment with five random shufflings\nof the incoming stream, and report the average\nscores. We use the distance weighted k-NN clas-\nsifier, as in Section 3. Again, we set k = 5, and\nwe employ Bayesian optimization on the few-shot\ndevelopment set to determine the optimal combi-\nnation of the two thresholds that maximize ˆϕ. We\nlet tc range in [0,2], and tH in [0,0.7].7 For the\nteacher, we used the cheaper GPT-3.5 in these ex-\n7The maximum value of Hwith 2 classes is 0.69 when\nusing the natural logarithm.\nperiments.8 We prompted GPT-3.5 using the same\nin-context learning approach outlined in Section 3;\nwe provided the few-shot training set we created\nas demonstrators and asked GPT-3.5 to classify the\nreview as either positive or negative (Figure 8). Fig-\nure 10 shows that the results were very similar to\nthose of Section 3 and Appendix E.\n8We used version gpt-3.5-turbo-0301.\n15008",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6440830230712891
    },
    {
      "name": "Classifier (UML)",
      "score": 0.41080546379089355
    },
    {
      "name": "Computer security",
      "score": 0.35495805740356445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2983000874519348
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I73142707",
      "name": "Athens University of Economics and Business",
      "country": "GR"
    },
    {
      "id": "https://openalex.org/I4210156054",
      "name": "Athena Research and Innovation Center In Information Communication & Knowledge Technologies",
      "country": "GR"
    }
  ],
  "cited_by": 5
}