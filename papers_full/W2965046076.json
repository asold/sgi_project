{
    "title": "Sharing Attention Weights for Fast Transformer",
    "url": "https://openalex.org/W2965046076",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A1983914940",
            "name": "Tong Xiao",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2168056687",
            "name": "Li Yinqiao",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2496766346",
            "name": "Jingbo Zhu",
            "affiliations": [
                "Northeastern University"
            ]
        },
        {
            "id": "https://openalex.org/A2099968256",
            "name": "Zhengtao Yu",
            "affiliations": [
                "Kunming University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2163293007",
            "name": "Tongran Liu",
            "affiliations": [
                "Institute of Psychology, Chinese Academy of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963736842",
        "https://openalex.org/W2594990650",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2763421725",
        "https://openalex.org/W2113104171",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2625113249",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2183341477",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2962944188",
        "https://openalex.org/W2963928591",
        "https://openalex.org/W2964013027",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2529089661",
        "https://openalex.org/W2963842982",
        "https://openalex.org/W2964213727",
        "https://openalex.org/W2146950091"
    ],
    "abstract": "Recently, the Transformer machine translation system has shown strong results by stacking attention layers on both the source and target-language sides. But the inference of this model is slow due to the heavy use of dot-product attention in auto-regressive decoding. In this paper we speed up Transformer via a fast and lightweight attention model. More specifically, we share attention weights in adjacent layers and enable the efficient re-use of hidden states in a vertical manner. Moreover, the sharing policy can be jointly learned with the MT model. We test our approach on ten WMT and NIST OpenMT tasks. Experimental results show that it yields an average of 1.3X speed-up (with almost no decrease in BLEU) on top of a state-of-the-art implementation that has already adopted a cache for fast inference. Also, our approach obtains a 1.8X speed-up when it works with the AAN model. This is even 16 times faster than the baseline with no use of the attention cache.",
    "full_text": "Sharing Attention Weights for Fast Transformer\nTong Xiao1;2 , Yinqiao Li1 , Jingbo Zhu1;2 , Zhengtao Yu3 and Tongran Liu4\n1Northeastern University, Shenyang, China\n2NiuTrans Co., Ltd., Shenyang, China\n3Kunming University of Science and Technology, Kunming, China\n4CAS Key Laboratory of Behavioral Science, Institute of Psychology, CAS, Beijing, China\nfxiaotong, zhujingbog@mail.neu.edu.cn, fli.yin.qiao.2012, ztyug@hotmail.com, liutr@psych.ac.cn\nAbstract\nRecently, the Transformer machine translation sys-\ntem has shown strong results by stacking atten-\ntion layers on both the source and target-language\nsides. But the inference of this model is slow\ndue to the heavy use of dot-product attention in\nauto-regressive decoding. In this paper we speed\nup Transformer via a fast and lightweight atten-\ntion model. More speciﬁcally, we share attention\nweights in adjacent layers and enable the efﬁ-\ncient re-use of hidden states in a vertical manner.\nMoreover, the sharing policy can be jointly learned\nwith the MT model. We test our approach on ten\nWMT and NIST OpenMT tasks. Experimental re-\nsults show that it yields an average of 1.3X speed-\nup (with almost no decrease in BLEU) on top of\na state-of-the-art implementation that has already\nadopted a cache for fast inference. Also, our ap-\nproach obtains a 1.8X speed-up when it works with\nthe A AN model. This is even 16 times faster than\nthe baseline with no use of the attention cache.\n1 Introduction\nIn recent years, neural models have led to great improvements\nin machine translation (MT). Approaches of this kind make it\npossible to learn good mappings between sequences via deep\nnetworks and attention mechanisms [Sutskever et al., 2014;\nBahdanau et al., 2015; Luong et al., 2015]. Recent work has\nexplored an architecture that just consists of stacked attentive\nand feed-forward networks (call it Transformer) [Vaswani et\nal., 2017]. It makes use of multi-layer dot-product attention\nto capture the dependency among language units. Beyond\nthis, training this kind of model is fast because we can paral-\nlelize computation over all positions of the sequence on mod-\nern graphics processing units (GPUs). These properties make\nTransformer popular in recent MT evaluations and industrial\ndeployments.\nHowever, standard implementations of Transformer are\nprone to slow inference though fast in training. At test time,\nthe system produces one target word each time until an end\nsymbol is reached. This process is auto-regressive and slow\nbecause we have to run dot-product attention for each posi-\ntion rather than batching the computation of the sequence.\nThe situation is even worse if 6 or more attention layers are\nstacked and the attention model occupies the inference time.\nTo address this issue, efﬁcient networks have been investigat-\ned. For example, one can replace dot-product attention with\nadditive attention and use average attention models instead\n[Zhang et al., 2018], or explore non-autoregressive decoders\nthat beneﬁt from the trick of batched matrix operations over\nthe entire sequence [Gu et al., 2018 ]. But these methods ei-\nther lose the explicit model of word dependencies, or require\ncomplicated networks that are hard to train.\nIn this work, we observe that the attention model shares a\nsimilar distribution among layers in weighting different po-\nsitions of the sequence. This experience lead us to study the\nissue in another line of research, in which we reduce redun-\ndant computation and re-use some of the hidden states in the\nattention network. We propose a method to share attention\nweights in adjacent layers (call it shared attention network,\nor S AN for short). It leads to a model that shares attention\ncomputation in the stacked layers vertically. In addition to the\nnew architecture, we develop a joint method to learn sharing\npolicies and MT models simultaneously. As another “bonus”,\nSAN reduces the memory footprint because some hidden s-\ntates are kept in the same piece of memory.\nSAN is simple and can be implemented in a few hours by\nanyone with an existing kit of Transformer. Also, it is orthog-\nonal to previous methods and is straightforwardly applicable\nto the variants of Transformer. We test our approach in a state-\nof-the-art system where an attention cache is already in use\nfor speed-up. Experimental results on ten WMT and NIST\nOpenMT tasks show an average of 1.3X speed-up with al-\nmost no decrease in BLEU. More interestingly, it obtains a\nbigger speed-up (1.8X) when working with the A AN model.\nThe best result is 16 times faster than the baseline where no\ncache is adopted.\n2 The Transformer System\nThe Transformer system follows the popular encoder-decoder\nparadigm. On the encoder side, there are a number of iden-\ntical stacked layers. Each of them is composed of a self-\nattention sub-layer and a feed-forward sub-layer. In Trans-\nformer, the attention model is scaled dot-product attention.\nLet l be the length of the source sequence. The input of the\nattention sub-layer is a tuple of(Q;K;V ), where Q2Rl\u0002dk ,\nK 2Rl\u0002dk and V 2Rl\u0002dv are the matrices of queries, keys,\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5292\nSelf-Attention\n...\nEnc-Dec Attention\n...\nPrevious Layer\nFollowing Layer\nEncoder Output\nEq. (1): Eq. (2):\nQuery(Q) Key(K) T\n\u0002\nS = Softmax pdk\n\u0010 \u0011\nValue(V)Weight(S)\n\u0002A=\nA column of S represents a distribution over all positions\nS[0]\nSelf-Attention A= Softmax(Q\u0001KT\np\ndk\n) \u0001V\nFigure 1: Decoder-side attention sub-layers in Transformer\nand values packed over the sequence. In self-attention, we\nﬁrst compute the dot-product of queries and keys, followed\nby the rescaling and softmax operations.\nS = Softmax(Q\u0001KT\npdk\n) (1)\nSis an l\u0002lmatrix, where entry (i;j) represents the strength\nof connecting position i with position j. Note that S is es-\nsentially a weight (or scalar) matrix where every column rep-\nresents a distribution. The output of self-attention is simply\ndeﬁned as the weighted sum of values:\nA= S\u0001V (2)\nHere Q, K and V are generated from the same source with a\nlinear transformation. The self-attention result is then fed into\na fully connected feed-forward network (FFN).\nThe decoder shares a similar structure with the encoder.\nApart from the self-attention sub-layer, an encoder-decoder\nattention sub-layer is introduced to model the correspondence\nbetween source positions and target positions. Basically, the\nencoder-decoder attention has the same form as Eqs. (1) and\n(2), where the queries come from the output of the previous\nlayer, and the keys and values come from the output of the\nencoder. See Figure 1 for an illustration of the attention model\nused in Transformer.\nNote that the matrix multiplications in Eqs. (1) and (2) are\ntime consuming. It is a bigger problem for inference because\nEqs. (1) and (2) repeat for each position until we ﬁnish the\ngeneration.\n3 Shared Attention Networks\nIn this work we speed up the decoder-side attention because\nthe decoder is the heaviest component in Transformer.\n3.1 Attention Weights\nSelf-attention is essentially a procedure that fuses the input\nvalues to form a new value at each position. Let S[i] be col-\numn iof weight matrix S. For position i, we ﬁrst compute\nS[i] to weight all positions (as in Eq. (1)), and then compute\nthe weighted sum of values by S[i] (as in Eq. (2)). In colum-\nn vector S[i], element Si;j indicates the contribution that we\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\n(a) Self-Attention\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\n(b) Enc-Dec\nAttention\nFigure 2: The Jensen-Shannon divergence of the attention weights\nfor every pair of layers on the WMT14 English-German task (a dark\ncell means the distributions are similar)\nfuse the value at position j to position i. Intuitively, the at-\ntention weight S[i] should not be volatile in different levels\nof language representation because the correlations between\npositions somehow reﬂect the dependency of language unit-\ns. For example, for an English sentence, the subject and the\nverb correlate well no matter how many layers we make on\ntop of the input sequence. On the other hand, the subject and\nthe adverbial may not have a big impact to each other in all\nstacked layers.\nTo verify this, we compute the Jensen-Shannon (JS) diver-\ngence to measure how the attention weight distribution of a\nlayer is different from another [Lin, 1991]. We choose the JS\ndivergence because it is symmetric and bounded. For multi-\nhead attention, we regard different heads as separate channel-\ns. We compute the JS score for each individual head and then\naverage them for ﬁnal output. Figure 2 shows that the sys-\ntem generates similar weights over layers. For self-attention,\nlayers 2-6 almost enjoy the same weight distribution. For\nencoder-decoder attention, we observe a larger variance but\ngood similarities still exist among two or three adjacent lay-\ners (see entries around the diagonal of Figure 2(b)). All these\nshow the possibility of removing redundant computation in\nTransformer.\n3.2 The Model\nAn obvious next step is to develop a faster attention model\nthat makes efﬁcient re-use of the states in Eqs. (1) and (2),\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5293\nAttention\n\u0001\u0001\u0001\nQn Kn V n\nSn =S(Qn\u0001Kn)\nAn =Sn\u0001V Attention\n\u0001\u0001\u0001\nQn Kn V n\nSn =Sm\nAn =Sn\u0001V Attention\n\u0001\u0001\u0001\nQn Kn V n\nSn\nAn =Am\nAttention\n\u0001\u0001\u0001\nQm Km V m\nSm =S(Qm\u0001Km)\nAm =Sm\u0001V Attention\n\u0001\u0001\u0001\nQm Km V m\nSm =S(Qm\u0001Km)\nAm =Sm\u0001V Attention\n\u0001\u0001\u0001\nQm Km V m\nSm =S(Qm\u0001Km)\nAm =Sm\u0001V\nLayer n=m+i\nLayer m\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\nLayer n=m+i\nLayer m\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\nLayer n=m+i\nLayer m\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n\u0001\u0001\u0001\n(a) Standard Transformer Attention (b) SAN Self-Attention (c) SAN Encoder-Decoder Attention\nFigure 3: Comparison of the standard attention model and the SAN model\ninstead of computing everything on the ﬂy. In this work we\npresent a shared attention network (SAN) to share weight ma-\ntrix S for adjacent layers. The idea is that we just compute\nthe weight matrix once and reuse it for upper-level layers.\nHere we describe SAN for both the self-attention and encoder-\ndecoder attention models.\n\u000f SAN Self-Attention. We deﬁne the self-attention weight\nmatrix in layer mas:\nSm = s(Qm;Km) (3)\nwhere s(\u0001;\u0001) is the function described in Eq. (1),Qm and\nKm are the inputs, andSm is the attention weight for the\noutput. In S AN, we can share Sm with the layers above\nm, like this\nSm+i = s(Qm;Km) (4)\nfor i2[1;\u0019 \u00001]\nwhere \u0019 indicates how many layers share the same at-\ntention weights. For example, in a 6-layer decoder, we\ncan share the self-attention weights for every two layers\n(\u0019 = 2 ), or share the weights for the ﬁrst two layer-\ns (\u00191 = 2 ) and let the remaining 4 layers use another\nweights (\u00192 = 4). We discuss the sharing strategy in the\nlater part of the section.\n\u000f SAN Encoder-Decoder Attention. For encoder-\ndecoder attention, we do the same thing as in self-\nattention, but with a trick for further speed-up. In\nencoder-decoder attention, keys and values are from the\noutput of the encoder, i.e., K and V have already been\nshared among layers on the decoder side. In response,\nwe can share A = S\u0001V for encoder-decoder attention\nlayers. This can be described as\nAm+i = Am\n= Sm \u0001V (5)\nfor i2[1;\u0019 \u00001]\nwhere Am is the attention output of layer m, V is the\ncontext representation generated by the encoder. See\nFigure 3 for a comparison of the standard attention mod-\nel and SAN.\nIn addition to system speed-up, SAN also reduces the mem-\nory footprint. In S AN, we just need one data copy of weight\nmatrix for a layer block, rather than allocating memory s-\npace for every layer. Moreover, the linear transformation of\nthe input (i.e., Qand K) can be discarded when the attention\nweights come from another layer. It reduces both the number\nof model parameters and the memory footprint in inference.\nAnother note on S AN. S AN is a process that simpliﬁes\nthe model and re-uses hidden states in the network. It is do-\ning something similar to systems that share model param-\neters in different levels of the network [Wu et al., 2016;\nYang et al., 2018; Luong et al., 2016 ]. Such methods have\nbeen proven to improve the robustness of neural models on\nmany natural language processing tasks. Sharing parameters\nand/or hidden states can reduce the model complexity. Previ-\nous work has pointed out that MT systems cannot beneﬁt a lot\nfrom very deep and complex networks [Vaswani et al., 2017;\nBritz et al., 2017 ]. S AN might alleviate this problem and\nmakes it easier to train neural MT models. For example, in our\nexperiments, we see that SAN can improve translation quality\nin some cases in addition to considerable speed-ups.\n3.3 Learning to Share\nThe remaining issue is how to decide which layers can be\nshared. A simple way is to use the same setting of \u0019 for the\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5294\n1: Function LEARN TOSHARE (layers, model)\n2: while policy f\u0019igdoes change do\n3: learn a new modelgiven policy f\u0019ig\n4: learn a new policy f\u0019igon layers given model\n5: return f\u0019ig& model\nFigure 4: Joint learning of MT models and sharing policies\nentire layer stack, and tune it on a development set. For ex-\nample, we can try to share weights on layer blocks consisting\nof two layers, or three layers, or all layers (\u0019 = 2, or 3 , ...),\nand use the tuned \u0019on test data.\nBut a uniform sharing strategy might not be optimal be-\ncause we need to control the degree of sharing in difference\nlevels of the network. For example, for the case in Figure 2(a),\na good choice is to share weights for layers 2-6 and leave lay-\ner 1 as it is. Here we present a method that learns the sharing\nstrategy in a dynamic way. To do this, we choose ln(2)\u0000JS\ndivergence as the measure of the similarity between weight-\ns of layer i and layer j (denoted as \u0016(i;j)). Given a layer\nblock ranging from layer mto layer n(denoted as b(m;n)),\nthe similarity over the block is deﬁned as\nsim(m;n) =\nPn\ni=m\nPn\nj=m(1 \u0000\u000e(i;j))\u0016(i;j)\n(n\u0000m+ 1) \u0001(n\u0000m) (6)\nwhere n\u0000m+1 is the size of the block, and\u000e(i;j) is the Kro-\nnecker delta function.sim(m;n) measures how the weight of\na layer is similar to that of another layer in blockb(m;n). We\ncan do sharing whensim(m;n) \u0015\u0012where \u0012is the parameter\nthat controls how often a layer is shared.\nWe begin with layer 1 and search for the biggest block\nthat satisﬁes the criterion. This process repeats until all the\nlayers are considered, resulting in N layer blocks. For sim-\nplicity, we use f\u00191, ...,\u0019N g(or f\u0019ig) to represent the blocks\nin a bottom-up manner (call it sharing policy), where \u0019i is\nthe size of block i. Obviously, for an M-layer stack we havePN\ni=1 \u0019i = M.\nOnce we have a sharing policy, we need to re-train the MT\nmodel. It in turn leads to new attention weights and possibly\na better policy. A desirable way is to continue learning until\nthe model converges. To this end, we present a joint learning\nmethod that trains MT models and sharing policies simulta-\nneously (Figure 4). In the method, MT training and policy\nlearning loops for iterations, and the result of the ﬁnal round\nis returned when there is no new update of the model.\n4 Experiments\nWe experimented with our approach on WMT and NIST\ntranslation tasks.\n4.1 Experimental Setup\nThe bilingual and evaluation data came from three sources\n\u000fWMT14 (En-De). We used all bilingual data provid-\ned within the WMT14 English-German task. We chose\nnewstest 2013 as the tuning data, and newstest 2014 as\nthe test data.\nSource Lang. Train Tune Test\nsent. word sent. word sent. word\nWMT14 En-De 4.5M 225M 3000 130K 3003 133K\nWMT17\nEn-De 5.9M 276M 8171 356K 3004 128K\nDe-En 3004 128K\nEn-Fi 2.6M 108M 8870 330K 3002 110K\nFi-En 3002 110K\nEn-Lv 4.5M 115M 2003 90K 2001 88K\nLv-En 2001 88K\nEn-Ru 25M 1.2B 8819 391K 3001 132K\nRu-En 3001 132K\nNIST12 Zh-En 1.9M 85M 1164 227K 1357 198K\nTable 1: Data statistics (# of sentences and # of words)\n\u000fWMT17 (En-De, De-En, En-Fi, Fi-En, En-Lv, Lv-En,\nEn-Ru and Ru-En). We followed the standard data set-\nting of the bidirectional translation tasks of German-\nEnglish, Finnish-English, Latvian-English, and Russian-\nEnglish. For tuning, we concatenated the data of new-\nstest 2014-2016. For test, we chose newstest 2017.\n\u000fNIST12 (Zh-En). We also used parts of the bitext of\nNIST OpenMT12 to train a Chinese-English system 1.\nThe tuning and test sets were MT06 and MT08.\nFor Chinese, all sentences were word segmented by the\nsegmentation system in the NiuTrans toolkit [Xiao et al.,\n2012]. For other languages, we ran the ofﬁcial script of WMT\nfor tokenization. All sentences of more than 50 words were\nremoved for the NIST Zh-En task, and sentences of more\nthan 80 words were removed for the WMT tasks. For all\nthese tasks, sentences were encoded using byte-pair encod-\ning, where we used a shared source target vocabulary of 32K\ntokens. See Table 1 for statistics of the data.\nWe used standard implementation of Transformer. Early\nversions of its inference system simply compute the attention\noutput for target positions individually. This way is straight-\nforward but with a double counting problem. For a stronger\nbaseline, we chose the system with an attention cache that\nkept the attention output of previous positions in cache and\nre-used it in following steps.\nThe Transformer system used in our experiments consist-\ned of a 6-layer encoder and a 6-layer decoder. By default,\nwe set dk = dv = 512 and used 2,048 hidden units in\nthe FFN sub-layers. We used multi-head attention (8 head-\ns) because it was shown to be effective for state-of-the-art\nperformance [Vaswani et al., 2017 ]. Dropout (rate = 0 :1)\nand label smoothing (\u000fls = 0:1) methods were adopted for\nregularization and stabilizing the training [Szegedy et al.,\n2016]. We trained the model using Adam with \f1 = 0 :9,\n\f2 = 0:98, and \u000f= 10\u00009 [Kingma and Ba, 2015]. The learn-\ning rate was scheduled as described in[Vaswaniet al., 2017]:\nlr = d\u00000:5\u0001min(t\u00000:5;t\u00014k\u00001:5), where tis the step number.\nAll models were trained for 100k steps with a mini-batch of\n4,096 tokens on machines with 8 Nvidia 1080Ti GPUs except\nEn-Fi (60k steps), Fi-En (60k steps) and Zh-En (24k steps).\nEvery model was ensembled from the 5 latest checkpoints in\n1LDC2000T46, LDC2000T47, LDC2000T50, LDC2003E14,\nLDC2005T10, LDC2002E18, LDC2007T09 and LDC2004T08\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5295\nSource Language Model BLEU \u0001BLEU Speed \u0001Speed\nWMT14 En-De Baseline 27.52 0.00 1.03K 0.00%\nSAN 27.69 +0.17 1.44K +39.81%\nWMT17\nEn-De Baseline 28.90 0.00 1.03K 0.00%\nSAN 28.82 -0.08 1.43K +38.82%\nDe-En Baseline 34.57 0.00 0.99K 0.00%\nSAN 34.75 +0.18 1.38K +39.19%\nEn-Fi Baseline 21.80 0.00 1.02K 0.00%\nSAN 21.45 -0.35 1.36K +33.82%\nFi-En Baseline 24.94 0.00 1.02K 0.00%\nSAN 25.25 +0.31 1.25K +23.28%\nEn-Lv Baseline 15.80 0.00 0.94K 0.00%\nSAN 16.08 +0.28 1.31K +39.01%\nLv-En Baseline 18.06 0.00 0.92K 0.00%\nSAN 17.97 -0.09 1.26K +36.28%\nEn-Ru Baseline 29.93 0.00 1.02K 0.00%\nSAN 29.51 -0.42 1.29K +26.17%\nRu-En Baseline 33.63 0.00 1.01K 0.00%\nSAN 33.36 -0.27 1.23K +21.17%\nNIST12 Zh-En Baseline 38.59 0.00 0.84K 0.00%\nSAN 38.19 -0.40 1.02K +21.34%\nAverage Baseline 27.37 0.00 0.98K 0.00%\nSAN 27.31 -0.07 1.30K +31.98%\nTable 2: BLEU scores (%) and translation speeds (token/sec) on the\nWMT and NIST tasks\ntraining. For inference, both beam search and batch decoding\nmethods were used (beam size = 4 and batch size = 16).\nFor our approach, we applied S AN to self-attention and\nencoder-decoder sub-layers on the decoder side. We learned\nsharing policies as in Figure 4. \u0012 was tuned on the tuning\ndata, which resulted in an optimal range of [0:3;0:4] for self-\nattention and [0:4;0:5] for encoder-decoder attention.\n4.2 Results\nWe report the translation quality (in BLEU[%]) and speed (in\ntoken/sec) on all ten of the tasks (Table 2). We see, ﬁrst of all,\nthat S AN signiﬁcantly improves the translation speed for all\nthese languages. The average speed-up is 1.3X. Also, there\nis a very modest BLEU decrease, but not signiﬁcant. These\nresults indicate that S AN is robust and can improve a strong\nbaseline on a wide range of translation tasks. Another inter-\nesting ﬁnding here is that the speed improvement on En-Ru,\nRu-En and Zh-En is not as large as that on other language\npairs. This is because we share fewer layers (i.e., larger \u0012) on\nthese tasks to preserve good BLEU scores. Note that Russian\nand Chinese are very difﬁcult languages for translation, and\nwe need a complicated network to model the structure diver-\ngence. Less sharing is preferred to keep the expressive power\nfor these languages.\nTo modulate the degree of sharing, we study the system\nbehavior under different settings of \u0012(Table 3) . For compar-\nison, we report the result of uniform f\u0019ig. Due to the limited\nspace, we present the result on the WMT14 En-De task in\nthe following sensitivity analysis. For uniform sharing, f\u0019ig\nis set to f6gfor self-attention and f3,3gfor encoder-decoder\nattention. This results in a promising speed-up (see entry of u-\nniform f\u0019ig). When we switch to joint learning of MT models\nand sharing policies, we obtain further improvements in both\nModel \u0012 BLEU \u0001BLEU Speed \u0001SpeedSelf Enc-Dec\nBaseline N/A 27.52 0.00 1.03K 0.00%\nSAN\nuniform f\u0019ig 27.58 +0.06 1.43K +38.83%\n0.30 0.40 26.89 -0.63 1.55K +50.26%\n0.30 0.50 27.69 +0.17 1.44K +39.81%\n0.40 0.40 26.96 -0.56 1.52K +47.32%\n0.40 0.50 27.46 -0.06 1.40K +36.33%\nTable 3: BLEU scores (%) and translation speeds (token/sec) for\ndifferent sharing policies. Self = self-attention. Enc-Dec = encoder-\ndecoder attention\nModel Shared V BLEU \u0001BLEU Speed \u0001Speed\nBaseline - 27.52 0.00 1.03K 0.00%\nSAN no 27.49 -0.03 1.14K +10.96%\nyes 1.27K +22.91%\nTable 4: BLEU scores (%) and translation speeds (token/sec)\nwith/without a shared context (V) for encoder-decoder layers\nBLEU and speed. More interestingly, we see that the system\nprefers a smaller \u0012for self-attention than the encoder-decoder\ncounterpart. This is reasonable because the encoder-decoder\nattention captures the correspondence of two languages and\nneeds more states in modeling than a single language. On\nthe other hand, the BLEU improvements indicate that the MT\nsystem can beneﬁt from simpliﬁed models. It gives a direction\nthat we explore simpler models for better training of neural\nMT systems.\nIn encoder-decoder attention, we share the context V gen-\nerated by the encoder for further speed-ups (see Figure 3(c)).\nIt is therefore worth a study on how much this method can\naccelerate the system. Table 4 shows that sharing the con-\ntext contributes half of the speed improvement. This agrees\nwith our design that weight sharing is more beneﬁcial to the\ndecoder because attention is heavier on the decoder side. An-\nother interesting question is whether S AN can improve the\nsystem on the encoder side. To seek an answer, we apply\nSAN to the encoder-side self-attention sub-layers and see s-\nmall speed improvements (Table 5). This result conﬁrms the\nprevious report that the decoder occupies the inference time\nand the encoder is light [Zhang et al., 2018].\nAlso, we plot the translation speed as functions of beam\nsize and BLEU score. Figure 5 shows a consistent improve-\nment under different beam settings. Moreover, S AN beneﬁts\nmore from larger beam sizes. For example, the speed-up of\nbeam = 20 is larger than that of beam = 4 (1.48x vs. 1.40x).\nThe Speed-vs-BLEU curves indicate a good ability of SAN in\ntrading off between translation quality and speed.\nIn addition, we empirically compare S AN with other vari-\nants of the attention model, including A AN [Zhang et al.,\n2018] and the model with no cache. Table 6 shows the atten-\ntion cache plays an important role in fast inference. It leads\nto an 8-fold speed-up on top of the implementation where\nno cache is used. Also, SAN obtains a bigger speed improve-\nment than AAN. This might be because AAN is used for self-\nattention only, while SAN is applicable to both self-attention\nand encoder-decoder attention. Finally, we combine AAN and\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5296\nModel BLEU \u0001BLEU Speed \u0001Speed\nBaseline 27.52 0.00 1.03K 0.00%\nSAN 27.51 -0.01 1.05K +1.94%\nTable 5: BLEU scores (%) and translation speeds (token/sec) of the\nsystems that use SAN on the encoder side\nModel BLEU \u0001BLEU Speed \u0001Speed\nBaseline 27.52 0.00 1.03K 0.00%\nBaseline-Cache 27.52 0.00 0.12K -88.65%\nBaseline+AAN 27.51 -0.01 1.38K +34.37%\nBaseline+SAN 27.69 +0.17 1.44K +39.81%\nBaseline+AAN+SAN 27.19 -0.33 1.87K +81.61%\nTable 6: Comparison of different attention models\nSAN in a new system where A AN is applied to self-attention\nand SAN is applied to encoder-decoder attention. It yields the\nbest result which is 1.8 times faster than the baseline, and al-\nmost 16 times faster than the system without cache.\nIn training, we observe that systems tend to learn similar\nattention weights. Figure 6 plots the JS divergence between\nlayer 4 and layers 5-6 at different training steps. The JS diver-\ngence curves go down signiﬁcantly as the training proceed-\ns. Adjacent layers show more similar weight distributions\nthan distant layers. The fast convergence in JS divergence can\nspeed up the iterative training. For example, for each training\nepoch (Figure 4), one can train the model for a shorter time,\nas the JS divergence among layers converges quickly. Thus,\nthe system can ﬁnd the optimal sharing policy more efﬁcient-\nly. In addition, we ﬁnd that the training likelihood of S AN is\nhigher than that of the baseline, but not signiﬁcant.\n5 Related Work\nIt has been observed that attention models are critical for\nstate-of-the-art results on many MT tasks [Bahdanau et al.,\n2015; Wu et al., 2016; Vaswaniet al., 2017]. Several research\ngroups have investigated attentive methods for different ar-\nchitectures of neural MT. The earliest is[Luong et al., 2015].\nThey introduced an additive attention model into MT system-\ns based on recurrent neural networks (RNNs). More recently,\nmulti-layer attention was successfully applied to convolution-\nal neural MT systems [Gehring et al., 2017] and Transformer\nsystems [Vaswani et al., 2017]. In particular, Transformer is\npopular due to its scalability on large-scale training and the\ngood design of the architecture for implementation.\nIt is well-known that Transformer suffers from a high infer-\nence cost which makes it slower than the RNN-based coun-\nterpart. This partially due to the auto-regressive property of\ndecoding, and partially due to the heavy use of dot-product\nattention where the expensive matrix multiplication is fre-\nquently used. Researchers have begun to explore solutions.\nFor example, Gu et al. [2018] designed a non-autoregressive\ninference method for a Transformer-like system, which gen-\nerated the entire target sequence at one time. This model is\nfast but is not easy to train.\nIn another line of research, Zhang et al. [2018] proposed\nthe average attention network (A AN) and applied it to self-\n4 8 12 16 20\n350\n950\n1;550\nBeam Size\nSpeed\nBaseline\nSAN\n27:5 27 :65 27 :8\n350\n950\n1;550\n2;150\nBLEU\nSpeed\nBaseline\nSAN\nFigure 5: Translation speed (token/sec) vs beam size and BLEU (%)\n10K 40K 70K 100K\n0:16\n0:26\n0:36\n0:46\n# of Training Steps\n(Self-Attention)\nJS Di\nvergence\nL5\nL6\n10K 40K 70K 100K\n0:24\n0:28\n0:32\n# of Training Steps\n(Enc-Dec Attention)\nJS Di\nvergence\nL5\nL6\nFigure 6: JS divergence vs number of training steps\nattention sub-layers on the decoder side with no cache. In\nthis work, we study the issue on a strong baseline that has\nalready used an attention cache for a reasonable inference\nspeed. Also, our approach is straightforwardly applicable to\nsystems with multi-layer attention. We improve both the self-\nattention and encoder-decoder attention components, which\nhas not been studied in previous work.\nIn neural MT, fast inference methods have been investigat-\ned for years. These include vocabulary selection [L’Hostiset\nal., 2016; Sankaran et al., 2017], knowledge distillation[Hin-\nton et al., 2015; Kim and Rush, 2016 ], low-precision com-\nputation [Micikevicius et al., 2018; Quinn and Ballesteros,\n2018], recurrent stacked networks [Dabre and Fujita, 2019 ]\nand etc. Our method is orthogonal to them. Previous studies\nfocus more on the reduction of model size and robust train-\ning, rather than fast inference. Here we study the issue in the\ncontext of speeding up attentive MT and conﬁrm the effec-\ntiveness of this kind of models.\n6 Conclusions\nWe have presented a shared attention network (SAN) for fast\ninference of Transformer. It shares attention weights among\nlayers for both self-attention and encoder-decoder attention in\na vertical manner. The policy of sharing can be jointly learned\nwith the MT model, rather than being determined heuristi-\ncally. Moreover, SAN reduces the memory footprint. Exper-\niments on ten MT tasks show that S AN yields a speed-up of\n1.3X over a strong baseline that has already used an attention\ncache. More interestingly, it is observed that the combination\nof S AN and A AN obtains a larger speed improvement. The\nsystem is 16X faster than the baseline with no cache.\nAcknowledgments\nThis work was supported in part by the National Sci-\nence Foundation of China (Nos. 61732005, 61876035 and\n61432013) and the Fundamental Research Funds for the Cen-\ntral Universities (No. N181602013).\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5297\nReferences\n[Bahdanau et al., 2015] Dzmitry Bahdanau, Kyunghyun\nCho, and Yoshua Bengio. Neural machine translation\nby jointly learning to align and translate. In In Proceed-\nings of the 3rd International Conference on Learning\nRepresentations, 2015.\n[Britz et al., 2017] Denny Britz, Anna Goldie, Minh-Thang\nLuong, and Quoc Le. Massive exploration of neural ma-\nchine translation architectures. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 1442–1451, Copenhagen, Den-\nmark, September 2017.\n[Dabre and Fujita, 2019] Raj Dabre and Atsushi Fujita. Re-\ncurrent stacking of layers for compact neural machine\ntranslation models. In Proceedings of the 33rd AAAI Con-\nference on Artiﬁcial Intelligence (AAAI), 2019.\n[Gehring et al., 2017] Jonas Gehring, Michael Auli, David\nGrangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. In Proceedings of\nthe 34th International Conference on Machine Learning,\nICML 2017, Sydney, NSW, Australia, 6-11 August 2017,\npages 1243–1252, 2017.\n[Gu et al., 2018] Jiatao Gu, James Bradbury, Caiming X-\niong, Victor O.K. Li, and Richard Socher. Non-\nautoregressive neural machine translation. InInternational\nConference on Learning Representations, 2018.\n[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, and J-\neffrey Dean. Distilling the knowledge in a neural net-\nwork. In NIPS Deep Learning and Representation Learn-\ning Workshop, 2015.\n[Kim and Rush, 2016] Yoon Kim and Alexander M. Rush.\nSequence-level knowledge distillation. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2016, Austin, Texas, USA,\nNovember 1-4, 2016, pages 1317–1327, 2016.\n[Kingma and Ba, 2015] Diederik P. Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations, ICLR\n2015, San Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings, 2015.\n[L’Hostiset al., 2016] Gurvan L’Hostis, David Grangier, and\nMichael Auli. V ocabulary selection strategies for neural\nmachine translation. CoRR, abs/1610.00072, 2016.\n[Lin, 1991] Jianhua Lin. Divergence measures based on\nthe shannon entropy. IEEE Trans. Information Theory,\n37(1):145–151, 1991.\n[Luong et al., 2015] Minh-Thang Luong, Hieu Pham, and\nChristopher D. Manning. Effective approaches to\nattention-based neural machine translation. In Proceed-\nings of the 2015 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1412–1421, 2015.\n[Luong et al., 2016] Minh-Thang Luong, Quoc V . Le, Ilya\nSutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. In 4th International Con-\nference on Learning Representations, ICLR 2016, San\nJuan, Puerto Rico, May 2-4, 2016.\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan\nNarang, Jonah Alben, Gregory F. Diamos, Erich Elsen,\nDavid Garc´ıa, Boris Ginsburg, Michael Houston, Oleksii\nKuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed\nprecision training. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018.\n[Quinn and Ballesteros, 2018] Jerry Quinn and Miguel\nBallesteros. Pieces of eight: 8-bit neural machine transla-\ntion. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-HTL\n2018, New Orleans, Louisiana, USA, June 1-6, 2018,\nVolume 3 (Industry Papers), pages 114–120, 2018.\n[Sankaran et al., 2017] Baskaran Sankaran, Markus Freitag,\nand Yaser Al-Onaizan. Attention-based vocabulary selec-\ntion for NMT decoding. CoRR, abs/1706.03824, 2017.\n[Sutskever et al., 2014] Ilya Sutskever, Oriol Vinyals, and\nQuoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in neural information processing\nsystems, pages 3104–3112, 2014.\n[Szegedy et al., 2016] Christian Szegedy, Vincent Van-\nhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew\nWojna. Rethinking the inception architecture for computer\nvision. In 2016 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2016, Las Vegas, NV , USA,\nJune 27-30, 2016, pages 2818–2826, 2016.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems, pages 6000–6010, 2017.\n[Wu et al., 2016] Yonghui Wu, Mike Schuster, Zhifeng\nChen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. Google’s neural machine translation sys-\ntem: Bridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144, 2016.\n[Xiao et al., 2012] Tong Xiao, Jingbo Zhu, Hao Zhang, and\nQiang Li. NiuTrans: An open source toolkit for phrase-\nbased and syntax-based machine translation. In Proceed-\nings of the ACL 2012 System Demonstrations, pages 19–\n24, Jeju Island, Korea, July 2012.\n[Yang et al., 2018] Zhen Yang, Wei Chen, Feng Wang, and\nBo Xu. Unsupervised neural machine translation with\nweight sharing. InProceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume\n1: Long Papers), pages 46–55, 2018.\n[Zhang et al., 2018] Biao Zhang, Deyi Xiong, and Jinsong\nSu. Accelerating neural transformer via an average atten-\ntion network. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Volume\n1: Long Papers), pages 1789–1798, 2018.\nProceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence (IJCAI-19)\n5298"
}