{
  "title": "Predicting Reference: What do Language Models Learn about Discourse Models?",
  "url": "https://openalex.org/W3099772776",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3099477050",
      "name": "Shiva Upadhye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2190439507",
      "name": "Leon Bergen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2059478924",
      "name": "Andrew Kehler",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2531882892",
    "https://openalex.org/W1966881866",
    "https://openalex.org/W4229445648",
    "https://openalex.org/W2155923110",
    "https://openalex.org/W2066382750",
    "https://openalex.org/W4252005589",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W1994940618",
    "https://openalex.org/W4380628640",
    "https://openalex.org/W1966006470",
    "https://openalex.org/W2053165660",
    "https://openalex.org/W2043360644",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2151666357"
  ],
  "abstract": "Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability. We address this question by drawing on a rich psycholinguistic literature that has established how different contexts affect referential biases concerning who is likely to be referred to next. The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 977–982,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n977\nPredicting Reference: What do Language Models Learn about Discourse\nModels?\nShiva Upadhye\nDepartment of Cognitive Science\nUC San Diego\nsupadhye@ucsd.edu\nLeon Bergen\nDepartment of Linguistics\nUC San Diego\nlbergen@ucsd.edu\nAndrew Kehler\nDepartment of Linguistics\nUC San Diego\nakehler@ucsd.edu\nAbstract\nWhereas there is a growing literature that\nprobes neural language models to assess the\ndegree to which they have latently acquired\ngrammatical knowledge, little if any research\nhas investigated their acquisition of discourse\nmodeling ability. We address this question by\ndrawing on a rich psycholinguistic literature\nthat has established how different contexts af-\nfect referential biases concerning who is likely\nto be referred to next. The results reveal that,\nfor the most part, the prediction behavior of\nneural language models does not resemble that\nof human language users.\n1 Introduction\nThe impressive power of deep learning based lan-\nguage models has inspired a new line of compu-\ntational psycholinguistics research that examines\nthe extent to which linguistic knowledge lies la-\ntent within their distributed networks. This work\nhas primarily focused on linguistic phenomena that\nsyntactic theory tells us requires syntactic knowl-\nedge to capture, with mixed results (Linzen et al.\n2016; Lau et al. 2017; Goldberg 2019; Warstadt\net al. 2019; inter alia). This paper asks a new ques-\ntion: to what extent do these language models cap-\nture the linguistic knowledge required to perform\ndiscourse modeling?\nWe are unaware of any work that has addressed\nthis question directly. Perhaps the closest research\nhas centered on the Winograd Schema Challenge\n(WSC) (Levesque et al., 2012), which evaluates the\nability of systems to employ world knowledge to\ninterpret ambiguous pronouns in minimal pairs that\nresemble Winograd’s famous example (1).\n(1) The city councilmen refused the demonstrators\na permit because\na. they feared violence. [they = city council]\nb. they advocated violence. [they = demon-\nstrators]\nHowever, WSC is essentially a ‘ﬁll in the blank’\nproblem-solving task, and doesn’t evaluate the ex-\ntent to which systems display humanlike ability to\nmodel discourse in an online, incremental fashion.\nWe instead take our inspiration from psycholinguis-\ntic work that has focused on this question. For\ninstance, the Bayesian Model of pronoun interpre-\ntation (Kehler et al., 2008; Kehler and Rohde, 2013)\nposits that comprehenders resolve the meaning of\na pronoun via Bayesian principles by combining\ntheir estimates of the speaker’s production biases\n(the LIKELIHOOD term) with their top-down expec-\ntations about which entities are likely to be men-\ntioned next (the PRIOR term, which we refer to\nas the NEXT -MENTION BIAS ). Kehler and Rohde\n(2013) demonstrate that an array of semantic biases\n(e.g., verb semantics) and pragmatic biases (e.g.,\ncoherence relations) that have been claimed to in-\nﬂuence pronoun interpretation directly actually do\nso only indirectly, by conditioning the prior.\nThe role of the prior in the Bayesian Model is di-\nrectly analogous to its role in Bayesian approaches\nto tasks such as speech recognition and machine\ntranslation, where a language model provides the\nprior probabilities. We argue that the ability to cap-\nture the inﬂuence of context on next-mention biases\nis thus a particularly appropriate task for evaluat-\ning the extent to which language models capture\ndiscourse modeling knowledge. Our focus will be\non effects of verb semantics that the psycholinguis-\ntic literature has shown to inﬂuence next-mention\nbiases. These studies have used a PASSAGE COM -\nPLETION paradigm, in which experimental partici-\npants are presented with context clauses followed\nby either a full stop (2a) or a conjunction (2b-c),\nand asked to complete the passage with the ﬁrst\nfollow-on sentence that comes to mind.\n978\n(2) a. John impresses Mary.\nb. John impresses Mary because\nc. John impresses Mary, and as a result\nAnalysis of the completions yields estimates of\nnext-mention biases and of referential form pro-\nduction. In the task described in §3, we will probe\nthe next-mention biases produced by two language\nmodels in different contexts that we describe now.\n2 Comparisons and Predictions\nIf neural language models latently acquire dis-\ncourse modeling knowledge, they should be able to\ndistinguish between contexts that are superﬁcially\nsimilar but which are known from psychological re-\nsearch to yield signiﬁcant effects on next-mention\nbiases. We focus on three such contrasts.\nImplicit Causality Verbs The ﬁrst compari-\nson is between two kinds of so-called IMPLICIT\nCAUSALITY (IC) verb, exempliﬁed in (3a-b).\n(3) a. John aggravated Mary. [IC1]\nb. John praised Mary. [IC2]\nSentences with IC verbs generate an expectation\nthat the follow-on sentence will participate in an Ex-\nplanation coherence relation, in which the second\nsentence provides a cause or reason for the even-\ntuality described by the ﬁrst (Kehler et al., 2008).\nHowever, the two types differ in which event partic-\nipant causality is attributed to. IC1 verbs (3a) have\nbeen experimentally shown to generate a strong\nexpectation that the preceding subject will be men-\ntioned next in the follow-on sentence—we heard\nthat John is aggravating, and we now expect to hear\nwhy (Garvey and Caramazza 1974; Caramazza\net al. 1977; Brown and Fish 1983; Terry Kit-fong\nAu 1986; McKoon et al. 1993; Koornneef and van\nBerkum 2006; Kehler et al. 2008; inter alia). IC2\nverbs (3b), on the other hand, have been shown\nto generate a strong expectation that the preced-\ning object will be mentioned next in the follow-on\nsentence—we heard that Mary received praise, and\nwe now expect to hear why. We can then ask: do\nIC1 and IC2 verbs generate different expectations\nin language models for next mention in otherwise\nidentical contexts?\nThere are also subsidiary predictions regarding\nthe use of connective prompts as in (2b-c). For both\ntypes of IC verbs, because prompts strengthen their\nbiases, since virtually 100% of the continuations\nwill now be Explanations rather than 60% as found\nin full stop prompt conditions (Kehler et al., 2008).\nSo we expect to see a higher probability of next-\nmention of the subject with because prompts for\nIC1 verbs, and likewise for objects for IC2 verbs.\nBoth types of IC verb, however, are known to have\na strong bias to the object in Result coherence rela-\ntions (Stewart et al., 1998; Kehler et al., 2008)—in\nwhich the follow-on describes an effect rather than\na cause—which are enforced by the and as a re-\nsult prompt. For IC1 verbs, therefore, we should\nsee a strong shift toward the object with and as a\nresult prompts compared to full stop prompts. To\nsummarize the predictions:\n1a. IC1 contexts with full stop prompts should\ndisplay a stronger next-mention bias to the\nsubject compared to IC2 contexts.\n1b. Contexts with because prompts should\nstrengthen the next-mention bias associated\nwith each type of verb compared to full stops.\n1c. And as a resultprompts in IC1 contexts should\nresult in a greater next-mention bias toward\nthe object compared to full stops.\nMotion vs. Transfer of Possession VerbsThe\nsecond comparison is between Motion (4a) and\nTransfer-of-Possession (ToP) verbs (4b).\n(4) a. The man jogged to the woman. [Motion]\nb. The man handed a gift to the woman.\n[ToP]\nThese sentence types are superﬁcially similar: they\neach have a grammatical subject that functions as a\nthematic Agent/Source, and a grammatical object-\nof-preposition that functions as a thematic Goal.\nHowever, they are known to yield very different\nnext-mention biases. Speciﬁcally, previous studies\nhave revealed that whereas motion verbs have a\nstrong next-mention bias toward the previous sub-\nject (e.g., 84.4% in a study run by Stevenson et al.\n(1994)), ToP contexts give rise to a distribution\nthat’s closer to 50/50 (51.0%). The reason is that\nthe Goal in ToP sentences functions not only as a\nlocation but a recipient as well, leading to an expec-\ntation that we’ll next hear about what the recipient\ndid with the object of transfer, which counteracts\nthe typical subject bias. We thus expect to see a\nmuch stronger next-mention bias toward the sub-\nject for Motion contexts as compared to ToP con-\ntexts, despite their superﬁcially similar properties.\nFurther, we expect a large effect of the connective\n979\nconditions: previous work (Stevenson et al., 1994;\nKehler et al., 2008) has shown Explanations to be\nstrongly biased to the Source, and Result contin-\nuations to be strongly biased to the Goal for ToP\ncontexts.1 To summarize the predictions:\n2a. Motion contexts with full stop prompts should\ndisplay a stronger next-mention bias to the\nsubject compared to ToP contexts.\n2b. ToP contexts with because prompts should\nyield a stronger bias toward the subject com-\npared to full stop prompts.\n2c. ToP contexts with and as a resultprompts\nshould yield a stronger bias to the object com-\npared to full stop prompts.\nAspectual Marking with Transfer of Posses-\nsion Verbs The ﬁnal comparison varies aspec-\ntual marking rather than the semantic class of the\nverb. Kehler et al. (2008) compared ToP contexts in\nthe perfective such as (4b) with otherwise identical\nsentences in the imperfective (5):\n(5) The man was handing a gift to the woman.\nFollowing Stevenson et al. (1994), Kehler et al.\nconjectured that ToP verbs have a special property\nin that the prominence of the event participants\ndepends on what component of event structure is\nbeing focused on. Speciﬁcally, the imperfective\nfocuses the hearer’s attention on the ongoing devel-\nopment of the event, where the agent of the event\nis most prominent. The perfective (4b), on the\nother hand, focuses the hearer’s attention on the\nend state of the event, where the recipient becomes\nprominent. Kehler et al. therefore predicted that\nimperfective contexts would lead to a greater ref-\nerential bias to the agent than perfective contexts,\nwhich is precisely what they found (80% vs. 57%).\nThis gives rise to the following prediction:\n3. Imperfective ToP contexts should display a\nstronger next-mention bias to the subject com-\npared to perfective ToP contexts.\n3 Experimental Setup\nWe evaluated two state-of-the-art, pre-trained au-\ntoregressive language models (LMs): GPT-2 large\n(Radford et al., 2018) and Transformer-XL (Dai\n1Unfortunately, Stevenson et al. (1994) left Motion con-\ntexts out of their experiment that examined the role of connec-\ntives. We thus have no data to compare to for these conditions.\nMiss Smith Mr. Smith Mary John\nThe woman the man Alice Bob\nThe actress the actor The girl the boy\nMrs. Taylor Mr. Williams Emma David\nThe princess the prince Sarah Robert\nMrs. Williams Mr. Taylor Emily Paul\nTable 1: Context Sentence Frames\net al., 2019).2 The experiments were conducted in\na zero-shot setting, and the task of generating con-\ntinuations was reformulated to a next-word predic-\ntion task. Prior to tokenization, the input stimulus\nwas prepended with a token indicating the begin-\nning of the sentence. Additionally, the inputs for\nTransformer-XL were prepended with a padding\ntext to account for the shorter stimulus length.3\nTo capture the diversity of ways in which event\nparticipants can be mentioned in the context sen-\ntence, the twelve frames shown in Table 1 were\nused. In order to balance for the effects of gen-\nder (Zhao et al., 2018; Bordia and Bowman, 2019),\neach frame was used again with the order of the\nevent participants reversed, for a total of 24 frames.\n20 IC1 verbs, 20 IC2 verbs, 18 Motion verbs, and\n18 ToP verbal complexes (in both perfective and\nimperfective variants) were each run in the full\nstop prompt, because prompt, and and as a result\nprompt conditions, in each of the 24 frames.4\nAfter presenting a pairing of a context sentence\nand prompt, we compute the (normalized) condi-\ntional probabilities of He and She in the full stop\nprompt condition and their lowercase equivalents\nfor the connective prompt conditions. The average\nbiases to the subject are computed for each verb\nover the sentence frames, which are in turn aver-\naged to compute the overall subject bias for each\ncontext type. The latter averages are reported with\n95% conﬁdence intervals in the tables below.\n4 Results\nImplicit Causality Comparison The next-\nmention biases toward the subject produced by\neach system in the IC verb conditions are shown in\nTables 2 and 3.\n2We considered also evaluating BERT on this task but de-\ncided that it was unsuitable. BERT performs masked language\nmodeling, conditioned on both left and right contexts. The\ncurrent experiments use only the left context, and hence BERT\nwould need to be queried in a non-natural setting.\n3For padding text see: https://tinyurl.com/y9kjuj5q.\n4The actual verbs used and other information nec-\nessary for reproducibility of results has been placed at\nhttps://github.com/shiva-upadhye/predicting-reference.\n980\nPrompt Transformer-XL GPT-2\nfull stop .51 ±.01 .59 ±.02\nbecause .61 ±.03 .63 ±.02\nand as a result .43 ±.02 .31 ±.02\nTable 2: Subject next-mention bias for IC1 contexts\nPrompt Transformer-XL GPT-2\nfull stop .51 ±.02 .66 ±.02\nbecause .45 ±.02 .42 ±.05\nand as a result .50 ±.05 .47 ±.07\nTable 3: Subject next-mention bias for IC2 contexts\nOur ﬁrst question (Prediction 1a) is whether the\nLMs would display a greater next-mention bias\ntoward the preceding subject in IC1 contexts than\nIC2 contexts. The answer is no: As can be seen in\nthe ﬁrst rows of Tables 2 and 3, the biases across\nconditions for Transformer-XL are identical (.51)\nand the difference witnessed for GPT-2 goes in\nthe wrong direction (.59 vs. .66). These results\ntherefore do not align with the more polar biases\nfor IC contexts that the psycholinguistic literature\nhas revealed in human studies.\nThe second question (Prediction 1b) is whether\nthe occurrence of because at the end of the\nprompt—which for human language users shifts\ndiscourse coherence expectations toward Explana-\ntion continuations—strengthens the respective IC\nbiases. This prediction receives only limited sup-\nport: The results in Table 2 reveal increased biases\ntoward the subject compared to the full stop con-\ndition for IC1 verbs, and those in Table 3 reveal\nsimilar decreases for IC2 verbs. However, only\nGPT-2 in the IC2 condition yielded an effect of the\nmagnitude that human language studies might lead\nus to expect.5\nThe ﬁnal question (Prediction 1c) is whether the\noccurrence of and as a resultat the end of the\nprompt—which for human language users shifts\ndiscourse coherence expectations toward Result\ncontinuations—generates a stronger bias toward\nthe preceding object compared to the free prompt\nbaseline in IC1 contexts. This prediction was con-\nﬁrmed for GPT-2, where the connective prompt\nreduced the bias to the subject by .28. Whereas\nTransformer-XL witnessed a lower bias in this con-\ndition as well, the effect was smaller (.08).\n5For instance, Kehler et al. (2008) found subject biases\nof 85% and 60% for IC1 verbs in the because and full stop\nprompt conditions respectively.\nTo sum, both models failed to yield the hypothe-\nsized effect of verb type in the full stop condition.\nHowever, there was some degree of sensitivity to\nthe occurrence of a connective, with GPT-2 in par-\nticular displaying a strong numerical difference\ncompared to the free prompt baseline in all but the\nIC1/because condition.\nMotion vs. ToP Verb Comparison The next-\nmention biases toward the subject produced by\neach system in the Motion and ToP context condi-\ntions are shown in Tables 4 and 5.\nPrompt Transformer-XL GPT-2\nfull stop .57 ±.01 .63 ±.01\nbecause .61 ±.02 .65 ±.01\nand as a result .54 ±.02 .47 ±.02\nTable 4: Subject next-mention bias for Motion verbs\nPrompt Transformer-XL GPT-2\nfull stop .52 ±.01 .54 ±.03\nbecause .53 ±.03 .53 ±.03\nand as a result .47 ±.04 .26 ±.03\nTable 5: Subject next-mention bias for ToP verbs (per-\nfective)\nOur ﬁrst question (Prediction 2a) asked whether\nthe LMs would display a greater next-mention bias\ntoward the preceding subject in Motion contexts\nthan ToP contexts in the full stop condition. The an-\nswer is mostly no: Whereas there is a small numeri-\ncal difference for each system in the right direction,\nit is far from what the results of experimental stud-\nies would predict. In particular, whereas the bias\nfound for ToP verbs is aligned with established ex-\nperimental results, the expected strong subject bias\nfor Motion verbs did not materialize.\nThe second and third questions (Predictions 2b\nand 2c) asked about the effect of connectives in\nthe ToP condition, whereby because and and as\na resultprompts should pull expectations toward\nthe subject and object compared to the full stop\nprompt baselines respectively. As with IC verbs,\nno strong effect was witnessed for Transformer-\nXL, whereas GPT-2 did show a strong shift in the\npredicted direction for and as a resultprompts.\nHowever, no appreciable effect was seen for GPT-2\nin the because prompt condition.\nAspectual Marking in ToP Verbs Comparison\nOur ﬁnal question (Prediction 3) probes the poten-\n981\ntial effects of aspectual marking on next-mention\nbiases, in particular whether imperfective ToP con-\ntexts will yield a stronger next-mention bias to the\nsubject compared to perfective ToP contexts. The\nresults for perfective and imperfective ToP contexts\nare shown in Tables 5 and 6 respectively.\nPrompt Transformer-XL GPT-2\nfull stop .57 ±.01 .62 ±.02\nbecause .56 ±.03 .57 ±.02\nand as a result .63 ±.03 .45 ±.03\nTable 6: Subject next-mention bias for ToP verbs (im-\nperfective)\nPrediction 3 was mostly disconﬁrmed: There\nis only a modest difference between ToP contexts\nusing the perfective and imperfective aspect in the\nfull stop prompt condition. Interestingly, however,\nthe predicted effect did exist for both systems in the\nand as a resultcondition. It is not clear to us why\nthe effect would be limited to only this condition.\n5 Conclusions\nWe set out to evaluate the extent to which neural\nLMs latently acquire the discourse modeling capa-\nbility necessary to perform a particular type of in-\ncremental processing that human language users do:\nThe ability to predict what entities are most likely\nto be mentioned next. We examined three context\npairs with superﬁcially similar linguistic properties\nthat the experimental literature has shown to result\nin divergent next-mention biases, both with and\nwithout connectives.\nThe results were mostly, but not entirely, neg-\native. On the one hand, we found no compelling\nevidence that the LMs are sensitive to any of the\nthree manipulations within the verbal complex in\nthe context sentence. On the other hand, one could\nargue for preliminary support for the claim that one\nof the LMs—GPT-2—is sensitive to the occurence\nof the two connectives examined here. Future work\nwill be required to assess the extent to which these\neffects do in fact reﬂect the acquisition of a latent\nform of discourse modeling ability.\nOur conclusions, of course, remain preliminary\nin a number of respects. First, we have analyzed\nthe behavior of only two systems. Since each sys-\ntem can be said to stand proxy for a single ex-\nperimental participant, these results could be ar-\ngued to be less robust than human language studies,\nwhich typically utilize several dozen participants.\nWhereas this limitation is shared with previous\nwork that probes LMs for inherently acquired syn-\ntactic knowledge, the robustness of the ﬁndings\nwould be enhanced by examining a broader range\nof systems and/or system conﬁgurations so as to\nbetter capture the kinds of variation found among\ngroups of human participants.\nSecond, we have focused here on broad contrasts\nbetween context types that have been studied in the\npsycholinguistic literature. Although the stimuli\nemployed were modeled after those used in exper-\nimental studies, to improve the robustness of the\nﬁndings we felt it necessary to compute means over\na variety of sentence frames (Table 1), so that any\nidiosyncrasies of particular frames that are indepen-\ndent of the manipulation under scrutiny wouldn’t\nunduly (and undetectably) drive the results. This\nimproves the robustness of our results in terms\nof items—whereas participants in psycholinguistic\nstudies typically see only one example sentence for\neach verb, the LMs here saw 24—it also means\nthat no lab data exists for the exact stimuli used\nhere. Since an experiment that collects data on this\nscale would require a substantial annotation effort,\na more careful comparison of this sort must be left\nfor future work.\nThird, there are many variations of the studies\npresented here that could be attempted. Exam-\nples would include variants that employ longer and\nmore realistic contexts. In this initial investigation\nwe focused on single-sentence contexts so as to\nhew as closely as possible to previous experimental\nwork. We hope that this short paper will inspire\nfurther research that takes next steps in this and a\nvariety of other directions.\nFinally, we want to be clear that we do not\nclaim that the two LMs examined have in any\nsense ‘failed’ at this task—they were obviously not\ntrained for this purpose. Our goal instead was to\npose the novel question of to what extent discourse\nknowledge of the sort examined here may exist la-\ntently in the models. That having been said, we\nconsider the identiﬁcation of alternative language\nmodel architectures that are capable of capturing\nthe requisite discourse modeling capability for this\ntask to be an interesting challenge problem for fu-\nture work.\nAcknowledgments\nWe thank three anonymous reviewers and our area\nchair for helpful feedback.\n982\nReferences\nTerry Kit-fong Au. 1986. A verb is worth a thousand\nwords: The causes and consequences of interper-\nsonal events implicit in language. Journal of Mem-\nory and Language, 25:104–122.\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Student Research Work-\nshop, pages 7–15, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRoger Brown and Deborah Fish. 1983. The psycho-\nlogical causality implicit in language. Cognition,\n14:237–273.\nAlfonzo Caramazza, Ellen Grober, Catherine Garvey,\nand Jack Yates. 1977. Comprehension of anaphoric\npronouns. Journal of Verbal Learning and Verbal\nBehaviour, 16:601–609.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nCatherine Garvey and Alfonzo Caramazza. 1974. Im-\nplicit causality in verbs. Linguistic Inquiry, 5:549–\n564.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nAndrew Kehler, Laura Kertz, Hannah Rohde, and Jef-\nfrey L. Elman. 2008. Coherence and coreference re-\nvisited. Journal of Semantics, 25(1):1–44.\nAndrew Kehler and Hannah Rohde. 2013. A prob-\nabilistic reconciliation of coherence-driven and\ncentering-driven theories of pronoun interpretation.\nTheoretical Linguistics, 39(1-2):1–37.\nArnout W. Koornneef and Jos J. A. van Berkum. 2006.\nOn the use of verb-based implicit causality in sen-\ntence comprehension: Evidence from self-paced\nreading and eye-tracking. Journal of Memory and\nLanguage, 54:445–465.\nJey Han Lau, Alexander Clark, and Shalom Lappin.\n2017. Grammaticality, acceptability, and probabil-\nity: A probabilistic view of linguistic knowledge.\nCognitive Science, 41(5):1202–1241.\nHector Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The Winograd Schema Challenge. In\nProceedings of the Thirteenth International Con-\nference on Principles of Knowledge Representa-\ntion and Reasoning, pages 552–561, University of\nRome.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics, 4:521–\n535.\nGail McKoon, Steven B. Greene, and Roger Ratcliff.\n1993. Discourse models, pronoun resolution, and\nthe implicit causality of verbs. Journal of Experi-\nmental Psychology: Learning, Memory, and Cogni-\ntion, 18:266–283.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nRosemary J. Stevenson, Rosalind A. Crawley, and\nDavid Kleinman. 1994. Thematic roles, focus, and\nthe representation of events. Language and Cogni-\ntive Processes, 9:519–548.\nAndrew J. Stewart, Martin J. Pickering, and Anthony J.\nSanford. 1998. Implicit consequentiality. In Pro-\nceedings of the 20th Annual Conference of the Cog-\nnitive Science Society, pages 1031–1036.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–20,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7194869518280029
    },
    {
      "name": "Affect (linguistics)",
      "score": 0.6247531175613403
    },
    {
      "name": "Language model",
      "score": 0.5459396839141846
    },
    {
      "name": "Natural language processing",
      "score": 0.5309079885482788
    },
    {
      "name": "Linguistics",
      "score": 0.5036384463310242
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48784947395324707
    },
    {
      "name": "Language understanding",
      "score": 0.4758845567703247
    },
    {
      "name": "Language acquisition",
      "score": 0.43690192699432373
    },
    {
      "name": "Psychology",
      "score": 0.36924731731414795
    },
    {
      "name": "Cognitive science",
      "score": 0.357651025056839
    },
    {
      "name": "Communication",
      "score": 0.14427900314331055
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}