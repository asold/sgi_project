{
  "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
  "url": "https://openalex.org/W4385571225",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5019345315",
      "name": "Mustafa Ozdayi",
      "affiliations": [
        "The University of Texas at Dallas"
      ]
    },
    {
      "id": "https://openalex.org/A2062404271",
      "name": "Charith Peris",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2302502392",
      "name": "Jack Fitzgerald",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2010370036",
      "name": "Christophe Dupuy",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2512714553",
      "name": "Jimit Majmudar",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2511284763",
      "name": "Haidar Khan",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4296074318",
      "name": "Rahil Parikh",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2127286069",
      "name": "Rahul Gupta",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4300506197",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3041048781",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4224920097",
    "https://openalex.org/W4285428850",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4285483870",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4281774243",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4226137521"
  ],
  "abstract": "Mustafa Ozdayi, Charith Peris, Jack FitzGerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan, Rahil Parikh, Rahul Gupta. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1512–1521\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nControlling the Extraction of Memorized Data\nfrom Large Language Models via Prompt-Tuning\nMustafa Safa Ozdayi1∗, Charith Peris2†, Jack Fitzgerald2, Christophe Dupuy2,\nJimit Majmudar2, Haidar Khan2, Rahil Parikh2, Rahul Gupta2\n1Department of Computer Science, The University of Texas at Dallas\n2Alexa AI, Amazon\nAbstract\nLarge Language Models (LLMs) are known\nto memorize significant portions of their train-\ning data. Parts of this memorized content have\nbeen shown to be extractable by simply query-\ning the model, which poses a privacy risk. We\npresent a novel approach which uses prompt-\ntuning to control the extraction rates of memo-\nrized content in LLMs. We present two prompt\ntraining strategies to increase and decrease ex-\ntraction rates, which correspond to an attack\nand a defense, respectively. We demonstrate\nthe effectiveness of our techniques by using\nmodels from the GPT-Neo family on a pub-\nlic benchmark. For the 1.3B parameter GPT-\nNeo model, our attack yields a 9.3 percent-\nage point increase in extraction rate compared\nto our baseline. Our defense can be tuned to\nachieve different privacy-utility trade-offs by a\nuser-specified hyperparameter. We achieve an\nextraction rate reduction of up to 97.7% rela-\ntive to our baseline, with a perplexity increase\nof 16.9%.\n1 Introduction\nPretrained large language models (LLMs; Devlin\net al., 2019; Radford et al., 2019; Raffel et al., 2020;\nSoltan et al., 2022), commonly trained on massive\ncrowd-sourced corpora, have been of much interest\nin the recent past due to their usage as backbones in\nstate-of-the-art models across multiple downstream\nNLU tasks. However, they have been shown to\nmemorize significant portions of their training data\nthat can be extracted using appropriately-crafted\nprompts (Carlini et al., 2020, 2022; Zhang et al.,\n2021). Such extractions pose a privacy risk to the\ncontributors of the training data.\nIn this context, methods that allow developers\nto control the extractability of memorized exam-\nples from LLMs are of much value. For example,\n∗Work done while the author was an intern at Amazon;\nmustafa.ozdayi@utdallas.edu\n†perisc@amazon.com\nmethods that increase extraction rates correspond\nto attacks in an adversarial setting, and provide\ndevelopers with the ability to analyze privacy-risk.\nMethods that decrease extraction rates, referred to\nas defenses, are useful for protecting against such\nattacks. Historically, defense methods tend to be\ncompute intensive (Abadi et al., 2016; Dupuy et al.,\n2021).\nIn this work, we train continuous soft-prompts\n(Lester et al. 2021; hereafter referred to simply as\nprompts) and leverage them as a way of passing\nan external signal into an LLM, to control the ex-\ntraction of memorized data. We freeze the model\nweights, and only use the trained prompt to con-\ntrol the generation. First, we train prompts in an\nattack setting and study the extent of extractable\nmemorized content in our models. Second, we ex-\nplore a defense setting where we create prompts\nthat reduce extraction rates and achieve different\nprivacy-utility trade-offs, via a user-specified hy-\nperparameter. Since the original model weights\nare frozen in both these settings, our methods are\ncompute efficient across the board.\nTo the best of our knowledge, our work is the\nfirst to adapt the use of instructive prompts for the\nanalysis and mitigation of privacy in LLMs. We\nhave released the code developed for our experi-\nments1.\n2 Background and Related Work\nPrevious work has shown that LLMs display mem-\norization and has explored a range of methods that\nquantify extractability (Carlini et al., 2018, 2020,\n2022). Differentially-private training (Dwork,\n2006; Abadi et al., 2016) is a popular method\nthat has been used to mitigate this risk. However,\nit tends to reduce model utility and requires re-\ntraining of the LLM, which might not be feasible\ndue to heavy computational burden.\n1https://github.com/amazon-science/controlling-llm-\nmemorization\n1512\nFigure 1: A schematic of our setup. The upper section\nshows our training and testing setup while the lower\nsection shows our evaluation metrics.\nThe use of instructive prompts for language mod-\nels has been extensively researched, including use\nduring pretraining (Raffel et al., 2020), as a sec-\nond stage of training (Sanh et al., 2022; Wei et al.,\n2021), and during inference to guide model output\n(Brown et al., 2020). Within the third category, in\norder to improve upon manual prompt engineering\nresearchers have implemented methods to learn dis-\ncrete natural language prompts (Shin et al., 2020),\nto mine them (Jiang et al., 2020), or, neglecting\nnatural language, to learn continuous prompts (Li\nand Liang, 2021; Lester et al., 2021).\nOur work leverages continuous prompts as a way\nof passing an external signal to a model to trigger\na desired model behavior (i.e., less or more memo-\nrized data in open language generation, which map\nto an extraction attack and defense, respectively).\n3 Method\nPrompt-tuning requires the prepending of a prompt\nto the prefix embedding and access to the training\nloss (see Figure 1). Given these constraints, we\nexplore a white-box attack where the adversary has\naccess to the target model parameters, and a black-\nbox defense where the adversary interacts with the\ntarget model via an API. We therefore do not test\nour defense against our own attack.\nLet [prefix || suffix] be a sequence in the training\nset where the prefix is of length ktokens. Carlini\net al. (2022) defined a suffix to be k-extractable\nif the model generates the suffix exactly, after be-\ning prompted with its the corresponding length-\nk prefix. Our white-box attack aims to increase\nthe number of k-extractable sequences, while our\nblack-box defense aims to reduce the number of\nk-extractable sequences that can be extracted by an\nadversary who submits prefixes via an API.\n3.1 Attack\nIn the attack setting, we assume that the adversary\nhas a set of [ prefix || suffix ] sequences Strain,\nsampled from the training set of the target model.\nTheir goal is to extract the suffixes corresponding\nto a disjoint set of prefixes, denoted by Stest2.\nTo do so, the adversary first initializes a prompt:\na continuous set of l× eparameters where eis the\nembedding size of the model, and lis the length of\nthe prompt, a hyperparameter decided by the adver-\nsary. The prompt is trained over Strain to facilitate\nthe correct generation of suffixes. To do this, we\nfirst prepend the prompt to the embedding of the\nprefix and pass the joint embedding through the\nmodel for generation. We then minimize the loss\nobjective (see below) with respect to the prompt\nwhile keeping the parameters of the model frozen.\nWe explore two loss objectives. The first is\ncausal language modeling (hereafter referred to as\nCLM), where we minimize the cross-entropy loss\nover the entire sequence (Radford et al., 2019). In\nthe second, the prompt is optimized by minimizing\nthe cross entropy loss of only the suffixes, given\nthe prefixes. Here, the training is aligned with our\ninference task such that during training the model\nis penalized only on the suffix tokens; hence we\nrefer to it as aligned CLM. During inference, the\nlearned prompt is prepended to each embedding\nof the prefixes in Stest, and the joint embedding is\npassed to the model for generation (see Figure 1).\n3.2 Defense\nIn the defense setting, the defender (API owner)\ntrains the prompt, and prepends it to the incoming\nprefixes before passing them to the model. Our\nalgorithm is inspired by machine-unlearning liter-\nature (Halimi et al., 2022), and defenses against\nmembership inference and backdoor attacks (Chen\net al., 2022; Ozdayi et al., 2021). We introduce a\n2For simplicity, we assume all prefixes are k-length. This\ncan easily be ensured by padding or truncating different length\nprefixes if needed in a real-world setting.\n1513\nhyperparameter named learning threshold denoted\nby θ. During prompt training (see Section 3.1),\nwhen loss is less than θwe do gradient ascent to\npenalize the prompt. If the loss is greater than θ,\nwe perform gradient descent with respect to the\nprompt as usual. Training is stopped once the av-\nerage epoch loss is equal or above θ. This allows\nus to increase training loss in a controlled manner\nand stabilize it around θ. Through this process, we\ncan achieve various privacy-utility trade-offs effi-\nciently without re-training any part of the model.\nTo explore θ, we set the initial value to be slightly\nabove the model training loss and increase in steps\nof 0.25 until desired performance is achieved.\n4 Experiments\nFor our experiments, we use the 125M and 1.3B\nparameter variants of the GPT-Neo models (Black\net al., 2021). These are public, decoder-only trans-\nformer models (Vaswani et al., 2017) trained using\nCLM on the Pile dataset (Gao et al., 2020). We\nextract Strain and Stest from the Language Model\nExtraction Benchmark dataset (Google-Research).\nThis dataset contains 15k sequences sampled from\nthe training split of the Pile where each sequence\nis partitioned into a prefix and suffix. In the default\nevaluation setting, both prefix and suffix consist of\n50 tokens. We ensure a random train/test split of\n14k/1k samples.\nOur evaluation metric of choice is Exact extrac-\ntion rate which is the fraction of correctly gener-\nated suffixes (i.e., all tokens of the generated suffix\nmatch with ground-truth suffix) over the test set.\nWe additionally discuss fractional extraction rate\nand present results in Appendix A. As a baseline,\nwe use the attack analyzed in Carlini et al. (2022),\nwhich consists of feeding the prefixes to the model,\nand generating suffixes with greedy decoding. This\nis the only extraction attack for this setting apart\nfrom our work, to the best of our knowledge. Our\ntraining setup is discussed in Appendix B. All ex-\nperiments are repeated over 5 runs with a new ran-\ndom train/test split in each run.\n4.1 Attack\nWe explore the performance of our attack across\nseveral dimensions: prompt length, suffix size, pre-\nfix size, and beam size. We use greedy-decoding\nin all cases, except the beam size experiments.\nPrompt Length First, we explore prompt length\nin the context of the default setting (prefix and suf-\nfix consist of 50 tokens; Figures 2-A1 and 2-A2).\nWe note that prompts tuned with both CLM and\naligned CLM provide improvements over the base-\nline in all cases, with aligned CLM providing the\nbest performance. Given this, we train prompts\nusing the aligned CLM objective for all other ex-\nperiments, including our defense.\nWith aligned CLM, we achieve the highest ex-\ntraction rates of 25.8% and 54.3% for the 125M\nand 1.3B models, respectively (an improvement of\n8.9 and 9.3 percentage points, respectively), with\na 100 token prompt (blue line). We observe that ex-\ntraction rates increase with prompt length and tend\nto saturate after prompt length 100. Over-fitting\nwas ruled out as a potential cause of saturation as\nthere is no increase in test loss observed during\ntraining. This suggests that there is a max limit on\nthe parameter count in the prompt that might add\nvalue for extraction purposes given our objective.\nWe note that more sophisticated training strategies\n(designing better loss functions, better prompt ini-\ntialization etc.) might yield better extraction rates.\nSuffix Size Next, we fix the prefix size to 50 and\nvary the suffix size. As shown in Figures 2-B1\nand 2-B2, extraction rates decrease roughly expo-\nnentially with suffix size. We note that as suffix size\nincreases, longer prompts (≥ 20) provide greater\nimprovements over the baseline. For example, with\na prompt length of 100 (blue line) using the 1.3B\nmodel, at suffix size 5 we observe an extraction\nrate increase of 5.3 percentage points. Whereas at\nsuffix size 50, the increase is9.3 percentage points.\nPrefix Size Next, we fix the suffix size to 50 and\nvary the prefix size. As shown in Figures 2-C1\nand 2-C2, extraction rates increase roughly loga-\nrithmically (as in Carlini et al. 2022). Contrary to\nsuffix size, we observe that the gaps between base-\nline and attacks decrease with increasing prefix\nsize. This suggests that our attack stands to benefit\na less informed adversary (small prefix sizes) when\ncompared to the baseline.\nBeam Decoding Finally, we utilize the default\nsetting with prefix and suffix sizes at 50 tokens\nand vary the beam size (beam size=1 corresponds\nto greedy decoding). The results are shown in\nFigures 2-D1 and 2-D2. We observe that extrac-\ntion rates increase across the board when increas-\ning beam size from 1 to 5. However, improve-\nments tend to plateau or oscillate when beam size\nis greater than 5. The 1.3B model benefits more\n1514\nFigure 2: The change in exact extraction rates against prompt length (2-A1, 2-A2), suffix size (2-B1, 2-B2), prefix\nsize (2-C1, 2-C2) and beam size (2-D1, 2-D2). Top panels show the GPT-Neo-125M results while the bottom panels\nshow GPT-Neo-1.3B results. The transparent polygons about each line represent 95% confidence intervals across\nthe points.\nModel θ Exact Extract\nRate\nPile Test\nPPL\nGPT-Neo\n125M\n0∗ 0.169 ± 0.007 15.71 ± 0.431\n1.25 0.031 ± 0.005 16.601 ± 0.197\n1.5 0.006 ± 0.001 17.499 ± 0.156\n1.75 0.001 ± 0.0 19.691 ± 0.598\nGPT2\n124M - 0.004 ± 0.002 30.323 ± 1.019\nGPT-Neo\n1.3B\n0∗ 0.450 ± 0.015 9.213 ± 0.232\n0.5 0.108 ± 0.02 9.758 ± 0.245\n0.75 0.022 ± 0.004 10.267 ± 0.094\n1 0.01 ± 0.002 10.775 ± 0.248\nGPT2\n1.5B - 0.019 ± 0.002 17.155 ± 0.545\nTable 1: Exact extraction rates and corresponding per-\nplexities for our defense setting, with different values of\nθ. Values are reported as mean ± std. Extraction rates\nthat are smaller than the corresponding GPT2 varient of\nsimilar size, achieved while perplexity values are also\nsmaller, are good. (∗no defense).\nfrom increasing beam size achieving the highest ex-\ntraction rate of 61.4%, at a beam size of 20 (with\na prompt length of 150). The highest extraction\nrate achieved for the 125M model was 28.3% at a\nbeam size of 15 (with a prompt length of 100).\n4.2 Defense\nFinally, we evaluate the privacy-utility trade-off of\nour black-box defense. As mentioned in Section 3,\nour defense is designed for a black-box adversary,\nand cannot be tested against our white-box attack.\nTherefore, we utilize the baseline attack (Section 4)\nto quantify privacy. We note that longer prompts\ndid not add value in a defense setting, so we resort\nto using a prompt of length 1. We utilize perplexity\n(PPL) on generated suffixes, to quantify the utility\nof the model in addition to using exact extraction\nrate as in Section 3.1. To measure PPL, we use a\nrandom subset of 1k sequences sampled from the\ntest split of the Pile, ensuring that PPL is measured\non data unseen by the model. We also compare our\nmetrics with those of similar sized models that were\nnot trained on the Pile dataset (GPT2 models). Our\npremise here is that better performance in terms of\nprivacy and utility, when compared to an out-of-\ndomain model of similar size, would mean that our\ndefense mechanism is of value to an API owner.\nIn Table 1, we display our results obtained using\nthe default evaluation setting (prefix and suffix com-\nprise of 50 tokens). Our defense achieves lower\nextraction rates with competitive PPL values. For\nthe 125M model, we achieve an exact extraction\nrate reduction of 99.4% relative to baseline with a\nPPL increase of 25.3% at θ= 1.75. For the 1.3B\nmodel, the extraction rate is reduced by 97.7% rel-\native to baseline with a PPL increase of 16.9% at\nθ= 1. The ability to achieve lower extraction rates\nwith lower PPL values as measured against the\nGPT2 models of the corresponding size, provides\nevidence that our defense is effective.\n1515\n5 Conclusion\nWe present the first known effort to leverage\nprompt-tuning to control the extractability of mem-\norized data from LLMs in an open language genera-\ntion task. We develop a novel data extraction attack\nand defense, and illustrate their performance under\nvarious settings. Our attack consistently outper-\nforms the baseline in terms of exact extraction rate.\nOur defense provides competitive privacy-utility\ntrade-offs and would prove beneficial to API own-\ners with model trained on sensitive content. These\nresults are achieved efficiently, without any change\nto the original model weights. We details avenues\nof future work in Appendix C\n6 Limitations\nWe briefly mention some limitations of our work.\nFirst, we have only used a single dataset, and a\nsingle model family in our experiments. This is\nmainly due to the fact that the benchmark we use\nis the only publicly available dataset at this time\nto the best of our knowledge. We also solely fo-\ncused on extraction metrics, but did not do a deeper\nanalysis on the extracted sequences. A fine-grained\nanalysis of extracted sequences could yield impor-\ntant insights for understanding memorization and\nextraction in LLMs. Similarly, we also did not an-\nalyze what our prompts converge to, and whether\nthey yield explainable prompts at the time of con-\nverge. Such analysis can provide better insights as\nto why, for example, training prompts with aligned\nCLM performs better that the basic CLM setting.\nFinally, we believe the evaluation of our defense\ncould be improved further by measuring other util-\nity metrics (e.g., accuracy) on downstream tasks.\n7 Ethical Considerations\nWe leverage prompt-tuning to control the ex-\ntractability of memorized data from LLMs in an\nopen language generation task and explore two\nsettings; an attack and a defense. We acknowl-\nedge that our attack methodology could be misused\nby an adversary with white-box access to extract\nmemorized private information from a target large\nlanguage model. Our goal is to raise awareness\nin the community to the possibility and severity\nof this nature of attack. We hope that developers,\narmed with this knowledge, can use relevant de-\nfense mechanisms to avoid such potential misuse.\nAcknowledgements\nThe authors would like to thank Wael Hamza for\nhelpful discussions on this topic and Stephen Rawls\nfor help with securing the GPU instances that were\nrequired for experimentation.\n1516\nReferences\nHuggingface accelerate.\nMartín Abadi, Andy Chu, Ian J. Goodfellow, H. B.\nMcMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. Proceedings of the 2016 ACM SIGSAC Confer-\nence on Computer and Communications Security.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramèr, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. ArXiv, abs/2202.07646.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Xiaodong Song. 2018. The secret\nsharer: Evaluating and testing unintended memo-\nrization in neural networks. In USENIX Security\nSymposium.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Xiaodong\nSong, Úlfar Erlingsson, Alina Oprea, and Colin Raf-\nfel. 2020. Extracting training data from large lan-\nguage models. In USENIX Security Symposium.\nDingfan Chen, Ning Yu, and Mario Fritz. 2022. Re-\nlaxloss: Defending membership inference attacks\nwithout losing utility. ArXiv, abs/2207.05801.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nChristophe Dupuy, Radhika Arava, Rahul Gupta, and\nAnna Rumshisky. 2021. An efficient dp-sgd mecha-\nnism for large scale nlu models. ICASSP 2022 - 2022\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 4118–4122.\nCynthia Dwork. 2006. Differential privacy. In Encyclo-\npedia of Cryptography and Security.\nLeo Gao, Stella Rose Biderman, Sid Black, Laurence\nGolding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800gb dataset of diverse text for language modeling.\nArXiv, abs/2101.00027.\nGoogle-Research. Google-research/lm-extraction-\nbenchmark.\nAnisa Halimi, Swanand Kadhe, Ambrish Rawat, and\nNathalie Baracaldo. 2022. Federated unlearning:\nHow to efficiently erase a client in fl?\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large\nlanguage models. ArXiv, abs/2106.09685.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nJimit Majmudar, Christophe Dupuy, Charith S. Peris,\nSami Smaili, Rahul Gupta, and Richard S. Zemel.\n2022. Differentially private decoding in large lan-\nguage models. ArXiv, abs/2205.13621.\nMustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R.\nGel. 2021. Defending against backdoors in feder-\nated learning with robust learning rate. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(10):9268–9276.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\n1517\nAn imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Process-\ning Systems 32, pages 8024–8035. Curran Associates,\nInc.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowl-\nedge Discovery amp; Data Mining, KDD ’20, page\n3505–3506, New York, NY , USA. Association for\nComputing Machinery.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting knowledge from language models with auto-\nmatically generated prompts. In Empirical Methods\nin Natural Language Processing (EMNLP).\nSaleh Soltan, Shankar Ananthakrishnan, Jack FitzGer-\nald, Rahul Gupta, Wael Hamza, Haidar Khan, Charith\nPeris, Stephen Rawls, Andy Rosenbaum, Anna\nRumshisky, Chandana Satya Prakash, Mukund Srid-\nhar, Fabian Triefenbach, Apurv Verma, Gokhan Tur,\nand Prem Natarajan. 2022. Alexatm 20b: Few-shot\nlearning using a large-scale multilingual seq2seq\nmodel. arXiv.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. ArXiv, abs/1706.03762.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. In Conference\non Empirical Methods in Natural Language Process-\ning.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2021. Finetuned\nlanguage models are zero-shot learners.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tramèr, and Nicholas Car-\nlini. 2021. Counterfactual memorization in neural\nlanguage models. ArXiv, abs/2112.12938.\nA Fractional Extraction Rate Results\nFractional extraction rateis the fraction of gener-\nated tokens that are both correct and in the right\nposition, over the dataset (see lower section of Fig-\nure 2). Our reason to measure this metric is to pro-\nvide a more detailed assessment of risks associated\nwith extraction. Exact extraction rate is particu-\nlarly important in cases where the attacker requires\nan exact match in order for the extraction to be\nof use; a good example is the case of extracting a\ncredit card number. In such cases, even getting a\nfew tokens incorrect will render the attack useless.\nHowever, when the attacker cares more about the\nmeaning of the extracted sequences, fractional ex-\ntraction rate can be a better metric to assess the risk.\nThis is because a human might be able to infer the\ncorrect meaning of the sequence even when few\ntokens are wrong.\nThe results related to this metric are shown in\nFigure 3. Comparing these results with the exact\nextraction rate results (Figure 2), we observe the\nsame trends across all of our experiment. We note\nthat the same shared trends are observed in the case\nof our defense. In this case the fractional extraction\nrate results are tabulated in Table 2.\nB Training Setup\nOur soft-prompts are initialized to random word\nembeddings as described in Lester et al. (2021).\nWe use a batch size of 128 and an Adam opti-\nmizer (Kingma and Ba, 2014) with a learning rate\nof 5e− 4. For the attack setting, the prompts are\ntrained for 15 epochs. In the defense case, the\nprompts are trained until training loss stabilizes\naround the specified θvalue (as described in Sec-\ntion 3.2), which happens within 2-3 epochs in our\nexperiments.\nWe use a Pytorch (Paszke et al., 2019) imple-\nmentation where we leverage the HuggingFace Ac-\ncelerate (HF) and DeepSpeed (Rasley et al., 2020)\nlibraries to handle distributed training over 8 GPUs\nwith fp16 mixed precision. On a p3dn.24xlarge\ninstance, the average attack prompt training time\n1518\nFigure 3: The change in fractional extraction rates against prompt length (3-A1, 3-A2), suffix size (3-B1, 3-B2),\nprefix size (3-C1, 3-C2) and beam size (3-D1, 3-D2). Top panels show the GPT-Neo-125M results while the bottom\npanels show GPT-Neo-1.3B results. The transparent polygons about each line represent 95% confidence intervals\nacross the points.\nModel θ Fract Extract\nRate\nPile Test\nPPL\nGPT-Neo\n125M\n0∗ 0.35 ± 0.006 15.71 ± 0.431\n1.25 0.192 ± 0.011 16.601 ± 0.197\n1.5 0.123 ± 0.005 17.499 ± 0.156\n1.75 0.087 ± 0.003 19.691 ± 0.598\nGPT2\n124M - 0.099 ± 0.003 30.323 ± 1.019\nGPT-Neo\n1.3B\n0∗ 0.634 ± 0.013 9.213 ± 0.232\n0.5 0.316 ± 0.022 9.758 ± 0.245\n0.75 0.171 ± 0.004 10.267 ± 0.094\n1 0.128 ± 0.006 10.775 ± 0.248\nGPT2\n1.5B - 0.166 ± 0.003 17.155 ± 0.545\nTable 2: Fractional extraction rates and corresponding\nperplexities for our defense setting, with different values\nof θ. Values are reported as mean ± std. Extraction rates\nthat are smaller than the corresponding GPT2 varient of\nsimilar size, achieved while perplexity values are also\nsmaller, are good.(∗no defense).\nwas 0.9 hours per prompt while the average defense\nprompt training time was 0.02 hours per prompt.\nC Future work\nWe have several avenues that we would like to ex-\nplore in the context of future work. We envision\nthat more sophisticated training strategies might\nyield better extraction rates in our attack setting\n(designing better loss objectives, better initializa-\ntion of soft-prompts etc.) and we would like to\nexplore this further.\nWe would like to explore different prompt learn-\ning algorithms such as other parameter-efficient\ntraining methods (Li and Liang, 2021; Hu et al.,\n2021), and hard-prompt learning methods (Wallace\net al., 2019), in order to conduct a more robust\nanalysis of extraction rates.\nWe would like to test the transferability of\ntrained prompts across different models and\ndatasets.\nFinally, we would like to combine our defense\nwith other existing defenses such as those applied\nat training time (e.g. versions of differentially pri-\nvate stochastic gradient descent; Abadi et al. 2016;\nDupuy et al. 2021) or those applied at decoding\nstage (e.g., differentially private decoding; Majmu-\ndar et al. 2022). The goal would be to achieve better\nprivacy-utility trade-offs under a combination of\nsuch defenses.\n1519\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSee Section 6\n□\u0013 A2. Did you discuss any potential risks of your work?\nSee Ethical Considerations under Section 7\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSee Abstract and Section 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSee Section 4\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe’ve cited the models. We cited the dataset in the right way to the best of our knowledge.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThese are publicly available models and data and so their licenses are in accordance with our work.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. The artifacts we use have been used by multiple publications for the same purpose\nas ours and are in accordance with their intended use. We do not create any model or data related\nartifacts.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThis data is part of the Pile dataset (Gao et al. 2020) that has seen much study in previous publications\nin the context of large language model training. Therefore, we do not take special steps to discuss\nthis.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nThe dataset we use been discussed in Gao et al. 2020 citation and an interested reader will be able\nto gather information here. The models are also discussed in the Black et al. 2021 citation.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSee Section 4.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1520\nC □\u0013 Did you run computational experiments?\nSee Section 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSee Section 4 and Training set up in Appendix B\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nFor training we utilize a set of parameters that have been commonly used in previous studies. For\ntheta (a hyper parameter that we introduce) see Table 1 for theta values that we explore. And\nAppendix B for experimental setup.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nWe show errorbars in all our plots, See Figure 2 and 3. We also report mean and stdev based on 5\nruns.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe clearly deﬁne our metrics in Section 4 and Appendix A. They do not use existing packages. We do\nnot do any pre-processing or normalization.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1521",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6374833583831787
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.51765376329422
    },
    {
      "name": "Computational linguistics",
      "score": 0.47963935136795044
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.4459533989429474
    },
    {
      "name": "Association (psychology)",
      "score": 0.41988396644592285
    },
    {
      "name": "Natural language processing",
      "score": 0.3526819348335266
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33551326394081116
    },
    {
      "name": "Philosophy",
      "score": 0.08895465731620789
    },
    {
      "name": "Epistemology",
      "score": 0.08812281489372253
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}