{
  "title": "Leveraging Large Language Models to Generate Multiple-Choice Questions for Ophthalmology Education",
  "url": "https://openalex.org/W4415261201",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2159939087",
      "name": "Shahrzad Gholami",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5119403708",
      "name": "Daniel B. Mummert",
      "affiliations": [
        "American Academy of Ophthalmology"
      ]
    },
    {
      "id": "https://openalex.org/A2097367166",
      "name": "Beth Wilson",
      "affiliations": [
        "American Academy of Ophthalmology"
      ]
    },
    {
      "id": "https://openalex.org/A2316731996",
      "name": "Sarah Page",
      "affiliations": [
        "American Academy of Ophthalmology"
      ]
    },
    {
      "id": "https://openalex.org/A1995994163",
      "name": "Rahul Dodhia",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3170830547",
      "name": "Juan M. Lavista Ferres",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2139022032",
      "name": "William B. Weeks",
      "affiliations": [
        "Microsoft (United States)",
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": null,
      "name": "Dale E. Fajardo",
      "affiliations": [
        "American Academy of Ophthalmology"
      ]
    },
    {
      "id": "https://openalex.org/A1981028925",
      "name": "Karine D Bojikian",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A1981028925",
      "name": "Karine D Bojikian",
      "affiliations": [
        "University of Washington"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2772246530",
    "https://openalex.org/W2912329325",
    "https://openalex.org/W2789360655",
    "https://openalex.org/W2952927910",
    "https://openalex.org/W4391655332",
    "https://openalex.org/W4406337061",
    "https://openalex.org/W4399242673",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W2001496424",
    "https://openalex.org/W2149490731",
    "https://openalex.org/W2141403362",
    "https://openalex.org/W4394760737",
    "https://openalex.org/W4413470221",
    "https://openalex.org/W4391578496",
    "https://openalex.org/W4392034542",
    "https://openalex.org/W4400642441",
    "https://openalex.org/W4407263148",
    "https://openalex.org/W4408155423",
    "https://openalex.org/W574645793"
  ],
  "abstract": "Importance Multiple choice questions (MCQs) are an important and integral component of ophthalmology residency training evaluation and board certification; however, high-quality questions are difficult and time-consuming to draft. Objective To evaluate whether general-domain large language models (LLMs), particularly OpenAI’s Generative Pre-trained Transformer 4 (GPT-4), can reliably generate high-quality, novel, and readable MCQs comparable to those of a committee of experienced examination writers. Design, Setting, and Participants This survey study, conducted from September 2024 to April 2025, assesses LLM performance in generating MCQs based on the American Academy of Ophthalmology (AAO) Basic and Clinical Science Course ( BCSC ) compared with a committee of human experts. Ten expert ophthalmologists, who were masked to the generation source, independently evaluated MCQs using a 10-point Likert scale (1 = extremely poor; 10 = criterion standard quality) across 5 criteria: appropriateness, clarity and specificity, relevance, discriminative power, and suitability for trainees. Intervention Relevant BCSC content and AAO question-writing guidelines were input into GPT-4o via Microsoft’s Azure OpenAI Service, and structured prompts were used to generate MCQs. Main Outcomes and Measures The primary outcomes were median scores and statistical comparisons using the bootstrapping method; string similarity scores based on Levenshtein distance (0-100, with 100 indicating identical content) between LLM-MCQs and the entire BCSC question bank; Flesch Reading Ease metric for readability; and intraclass correlation coefficient (ICC) for inter-rater agreement are reported. Results The 10 graders had between 1 and 28 years of clinical experience in ophthalmology (median [IQR] experience, 6 years [3-15 years]). Questions generated by GPT-4 and a committee of experts received median scores of 9 and 9 in combined scores, appropriateness, clarity and specificity, and relevance (difference, 0; 95% CI, 0-0; P &amp;amp;gt; .99); 8 and 9 in discriminative power (difference, 1; 95% CI, −1 to 1; P = .52); and 8 and 8 in suitability for trainees (difference, 0; 95% CI, −1 to 0; P &amp;amp;gt; .99), respectively. Nearly 95% of LLM-MCQs had similarity scores less than 60, indicating most LLM-MCQs had limited or no resemblance to existing content. Interrater reliability was moderate (ICC, 0.63; P &amp;amp;lt; .001), and mean (SD) readability scores were similar across sources (37.14 [22.54] vs 42.60 [22.84]; P &amp;amp;gt; .99). Conclusions and Relevance In this survey study, results indicate that an LLM could be used to develop ophthalmology board–style MCQs and expand examination banks to further support ophthalmology residency training. Despite most questions having a low similarity score, the quality, novelty, and readability of the LLM-generated questions need to be further assessed.",
  "full_text": null,
  "topic": null,
  "concepts": []
}