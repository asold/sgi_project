{
  "title": "Transformers in Machine Learning: Literature Review",
  "url": "https://openalex.org/W4387058736",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2989778167",
      "name": "Thoyyibah T",
      "affiliations": [
        "Universitas Pamulang"
      ]
    },
    {
      "id": "https://openalex.org/A2962215655",
      "name": "Wasis Haryono",
      "affiliations": [
        "Universitas Pamulang"
      ]
    },
    {
      "id": "https://openalex.org/A3012330624",
      "name": "Achmad Udin Zailani",
      "affiliations": [
        "Universitas Pamulang"
      ]
    },
    {
      "id": "https://openalex.org/A2906898278",
      "name": "Yan Mitha Djaksana",
      "affiliations": [
        "Universitas Pamulang"
      ]
    },
    {
      "id": "https://openalex.org/A815234674",
      "name": "Neny Rosmawarni",
      "affiliations": [
        "Universitas Pembangunan Nasional Veteran Jakarta"
      ]
    },
    {
      "id": "https://openalex.org/A2790656704",
      "name": "Nunik Destria Arianti",
      "affiliations": [
        "Universitas Wijaya Putra"
      ]
    },
    {
      "id": "https://openalex.org/A2989778167",
      "name": "Thoyyibah T",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2962215655",
      "name": "Wasis Haryono",
      "affiliations": [
        "Universitas Pamulang"
      ]
    },
    {
      "id": "https://openalex.org/A3012330624",
      "name": "Achmad Udin Zailani",
      "affiliations": [
        "Universitas Pamulang"
      ]
    },
    {
      "id": "https://openalex.org/A2906898278",
      "name": "Yan Mitha Djaksana",
      "affiliations": [
        "Universitas Pamulang"
      ]
    },
    {
      "id": "https://openalex.org/A815234674",
      "name": "Neny Rosmawarni",
      "affiliations": [
        "Universitas Pamulang",
        "Universitas Pembangunan Nasional Veteran Jakarta"
      ]
    },
    {
      "id": "https://openalex.org/A2790656704",
      "name": "Nunik Destria Arianti",
      "affiliations": [
        "Universitas Wijaya Putra"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313357274",
    "https://openalex.org/W3032735579",
    "https://openalex.org/W2960944079",
    "https://openalex.org/W4384827994",
    "https://openalex.org/W4206650881",
    "https://openalex.org/W3197482624",
    "https://openalex.org/W2985586509",
    "https://openalex.org/W4212828284",
    "https://openalex.org/W3010521179",
    "https://openalex.org/W3082277326",
    "https://openalex.org/W3201782492",
    "https://openalex.org/W4313201287",
    "https://openalex.org/W4243640523",
    "https://openalex.org/W3131043787",
    "https://openalex.org/W4293163051",
    "https://openalex.org/W2747680751",
    "https://openalex.org/W3039724609",
    "https://openalex.org/W6790709252",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W4225163098",
    "https://openalex.org/W3202773593",
    "https://openalex.org/W4313889909",
    "https://openalex.org/W4377820893",
    "https://openalex.org/W6960036690",
    "https://openalex.org/W3157143019",
    "https://openalex.org/W4281689302",
    "https://openalex.org/W3125005549",
    "https://openalex.org/W3203924575",
    "https://openalex.org/W4226151059",
    "https://openalex.org/W4366594540",
    "https://openalex.org/W4318819061",
    "https://openalex.org/W4223591627",
    "https://openalex.org/W4366988472",
    "https://openalex.org/W4307732456",
    "https://openalex.org/W3215812695",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W3002709689",
    "https://openalex.org/W4307358455",
    "https://openalex.org/W3205750115",
    "https://openalex.org/W3029588370",
    "https://openalex.org/W2975120956",
    "https://openalex.org/W3159789740",
    "https://openalex.org/W3119872582",
    "https://openalex.org/W4378472529",
    "https://openalex.org/W3128933491",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3163152256",
    "https://openalex.org/W3102393842"
  ],
  "abstract": "In this study, the researcher presents an approach regarding methods in Transformer Machine Learning. Initially, transformers are neural network architectures that are considered as inputs. Transformers are widely used in various studies with various objects. The transformer is one of the deep learning architectures that can be modified. Transformers are also mechanisms that study contextual relationships between words. Transformers are used for text compression in readings. Transformers are used to recognize chemical images with an accuracy rate of 96%. Transformers are used to detect a person's emotions. Transformer to detect emotions in social media conversations, for example, on Facebook with happy, sad, and angry categories. Figure 1 illustrates the encoder and decoder process through the input process and produces output. the purpose of this study is to only review literature from various journals that discuss transformers. This explanation is also done by presenting the subject or dataset, data analysis method, year, and accuracy achieved. By using the methods presented, researchers can conclude results in search of the highest accuracy and opportunities for further research.",
  "full_text": " \nJPPIPA 9(9) (2023) \n \nJurnal Penelitian Pendidikan IPA \nJournal of Research in Science Education  \n \nhttp://jppipa.unram.ac.id/index.php/jppipa/index  \n   \n___________ \nHow to Cite: \nT, T., Haryono, W ., Zailani, A. U ., Djaksana, Y. M., Rosmawarni, N ., & Arianti, N. D . (2023). Transformers in Machine Learning: Literature \nReview. Jurnal Penelitian Pendidikan IPA, 9(9), 604–610. https://doi.org/10.29303/jppipa.v9i9.5040  \nTransformers in Machine Learning: Literature Review \n \nThoyyibah T1*, Wasis Haryono1, Achmad Udin Zailani 1, Yan Mitha Djaksana 1, Neny Rosmawarni 2, \nNunik Destria Arianti3 \n \n1 Faculty of Computer Science, Informatics Engineering Study Program, Universitas Pamulang, Banten, Indonesia. \n2 Faculty of Computer Science, Informatics Engineering Study Program, Universitas Pembangunan Nasional Veteran, Jakarta, Indonesia. \n3 Faculty of Computer Engineering and Design, Information System Study Program, Nusa Putra University, Bandung, Indonesia. \n \n \nReceived: July 15, 2023 \nRevised: August 20, 2023  \nAccepted: September 25, 2023 \nPublished: September 30, 2023 \n \nCorresponding Author:  \nThoyyibah T \ndoseno01116@npam.ac.id   \n \nDOI: 10.29303/jppipa.v9i9.5040  \n \n© 2023 The Authors. This open \naccess article is distributed under a \n(CC-BY License) \n \nAbstract: In this study, the researcher presents an approach regarding methods in \nTransformer Machine Learning. Initially, transformers are neural network architectures \nthat are considered as inputs. Transformers are widely used in various studies with \nvarious objects. The transformer  is one of the deep learning architectures that can be \nmodified. Transformers are also mechanisms that study contextual relationships \nbetween words. Transformers are used for text compression in readings . Transformers \nare used to recognize chemical images with an accuracy rate of 96 %. Transformers are \nused to detect a person's emotions . Transformer to detect emotions in social media \nconversations, for example, on Facebook with happy, sad, and angry categories . Figure \n1 illustrates the encoder and decoder pr ocess through the input process and produces \noutput. the purpose of this study is to only review literature from various journals that \ndiscuss transformers. This explanation is also done by presenting the subject or dataset, \ndata analysis method, year , and accuracy achieved. By using the methods presented, \nresearchers can conclude results in search of the highest accuracy and opportunities for \nfurther research. \n \nKeywords: Accuracy; Machine learning; Transformer \n  \nIntroduction  \n \nAt first , the transformer is a neural network \narchitecture that is considered as input (Reinauer et al., \n2021; Grechishnikova, 2021 ; Ramos-Pérez et al., 2021 ; \nMoutik et al., 2023; Röder, 2023; Lin et al., 2022; Abed et \nal., 2023). In principle, a transformer is needed to solve \nsequential problems such as sentences, which is an \nartificial neural network architecture (Luitse et al., 2021; \nTaye, 2023; Yang & Wang, 2020) . The transformer is in \ncharge of connecting the encoder and decoder to the text \nin stages (Singla et al., 2020; Alqudsi et al., 2019; Liu & \nChen, 2022)  input and output are interconnected via \ncontext vectors. The autoencoder is capable of \nconverting input into output. Besides being used in \nNatural Language Process transformers, it is also us ed \nin computer vision (Ghojogh et al., 2020; He et al., 2023). \nTransformer is a concept in Natural language Processing \n(NLP) in Deep Learning (Singla et al., 2020).   \nTransformer is part of NLP, an open-source library \n(Singla et al. ,2020). The Natural Language process uses \nmany words that are processed through transfor mers \n(Singla et al., 2020; Khurana et al., 2023 ; Caucheteux & \nKing, 2022) . Several studies use transformers, namely \nthe application of machine learning using the \ntransformer method in the hea lth sector (Alqudsi et al., \n2019; Arshed et al., 2023 ; Sanmarchi et al., 2023) . \nTransformers used to examine people's views through \npolitics on social media apply BERT, BERT, LSTM, \nSupport Vector Machine, Decision Trees, Naïve Bayes, \nand Electra. In this study, the highest accuracy value \nused the Electra model with a value of 70% ( Öztürk et \nal., 2022). The use of TurnGPT transformer model is used \nin spoken dialogu e (Ekstedt et al., 2020). Research that \napplies transformers to musical chord recognition using \nthe bi -directional Transformer for chord recognition \n(BTC) method (Park et al. , 2019). Transformer using \nblock model (Yan Xiao A et al ., 2020). Transformer \nimaging using 3D on the  plane (Dong Yang et al, 2021). \n\nJurnal Penelitian Pendidikan IPA (JPPIPA) September 2023, Volume 9 Issue 9, 604-610 \n \n605 \nThe transformer used for sentiment analysis uses the \nBERT model ( Durairaj et al. , 2021 ; Kokab et al., 2022 ; \nProttasha et al., 2022). \nMachine translation uses the concept of \ntransformers (Qiang Wang et al ., 2019). Transformers \nwere used in the best food review using BERT (Pasaribu \net al., 2020). Transformers are used to assess tourist visits \n(Irfan et al., 2 021; Wu et al., 2023 ; Bohatyrewicz et al., \n2019). Sentiment analysis on film reviews u sing \ntransformers (Putri et al. , 2 022; Bacco et al., 2021) .  \nTransformer introduces BERT and GPT wit h encoder \nand decoder (Ghojogh, 2020; Paaß & Giesselbach, 2023).  \nTransformers are used for text compression in \nreadings ( Li et al ., 2021). Transformers are used to \nrecognize chemical images with an accuracy rate of 96% \n(Rajan et al. , 2021). Transformers are used to detect a \nperson's emo tions (Zhong et al. , 2019; Graterol et al., \n2021; Ghosh et al., 2023). Transformer to detect emotions \nin social media conversations, for example on Facebook \nwith categories of happy, sad , and angry (Zhong et al., \n2019; Acheampong et al., 2020 ; Li et al., 2020) . Machine \nlearning uses a transformer m odel with 1000+ \ntransformers to analyze health problems (W u et al., \n2020). Transformers were used to predict the next 10 \nweeks with influenza case subjects (Rendragraha et al., \n2021; Santangelo et al., 2023; Piccialli et al., 2021).  \n \nTheory  \nThe transformer model is also used to detect \nabusive language in Indonesian online news comments. \nClassification of the text is done with the category of \noffensive, normal , or non -offensive (Tran et al.,  2020). \nThe transformer model is used for energy monitori ng \nusing the concept of algorithms in machine learning in \nthe health sector (Valencia et al., 2021). Transformer \nmethod to analyze a person's stress (Vaswani et al ., \n2017). The transformer model is used for the \nClassification of Music in Indone sia (Thoyyib ah et al., \n2022). Transformers are also mechanisms that study \ncontextual relationships between words. Figure 1 \nillustrates the encoder and decoder process through the \ninput process and produces output. In Transformer \nthere are two mechanisms, namely: \n \nCode Generator \nEncoder is used to read all text input at once. The \nencoder consists of a stack of identical layers. Each layer \nhas two sub-layers, namely the self-attention layer and \nthe feed-forward neural network.  with a  self-attention \nlayer,  the encoder can help nodes not only focus on the \nword they see but also get the semantic context of the \nword. Each position in the encoder can handle all \npositions in the previous layer in the encoder. \n \n \nDecoder \nThe decoder is used to generate predictive output \nsequences. The decoder also consists of a stack of \nidentifiable layers. Each layer consists of two sub-layers \nlike those of the encoder, with an additional attention \nlayer between the two layers to help the current node get \nto the main content it needs to pay attention to by \nperforming multi-head attention on the encoder output. \nSimilar to the encoder, the self -attention layer in the \ndecoder makes each position in the decoder capable of \nhandling all previous  and current positions. From the \nvarious journals described earlier, researchers are \ninterested in studying transformers. In several aspects, \nthis research is different from previous research. Focuses \non critical thinking skills. First, this research focus es on \nall the articles that have been published from 2017 to \n2022; almost all of which are international journals. \nSecond, this research is devoted to further reviewing \nvarious articles with critical thinking skills as the main \nfocus being discussed. Third , various parameters are \nused as the basis for content analysis, through analysis \nof methods, accuracy, and others. \n \n \nFigure 1. Transformer model \n \nMethod  \n \nResearch Design \nThis research journal has a principle for analyzing \nthe contents of previous literature reviews, which \nfocuses on the findings of various studies that have been \npublished in various international journals. \n \n\nJurnal Penelitian Pendidikan IPA (JPPIPA) September 2023, Volume 9 Issue 9, 604-610 \n \n606 \nData Source \nThe data collected from the analysis in the form of \nthe contents of this literature review comes from \ninternational journals about transformers in machine \nlearning. All journals are taken from various sources of \ninternational journals. Hence, all the contents of the \nliterature review that rev iews transformers in machine \nlearning are collected from each of these journals. The \nliterature review analyzed in this study has been \npublished online, in a total of 23 journals that examine \ntransformers in machine learning. All literature reviews \nare analyzed in this study to find out what methods are \nused in transformers in machine learning. \n \nResearch Instruments \nThis research is a literature review of several \nresearch journals related to transformers in machine \nlearning. The review was conducted on some of the most \nrecent research efforts utilizing machine learning. \nFurthermore, this study comes from several literacies \nand includes problem-solving efforts that are divided \ninto areas from the perspective of each machine learning \ncategory with various methods available in the research \njournal. The data collection process used to examine \nsome of this literature is very useful for finding and \nobtaining study sources based on previous relevant \nresearch. Supporting theories, data , and information as \nreferences in the documentation.  \nThere are several main aspects in Table 1 which are \npresented in this research journal from the literature \nreview of several international journals. These aspects \ninclude (A) type of  research, (B) year, (C) Dataset, (D) \nTransformer Model, (E) Accuracy, (F) Field of Research. \n \nTable 1.  Aspects and Categories Used for Content \nAspects Categories \nKind of Research A.1 Experiment \nA.2 Literature review \nYear B.1 2017 \nB.2 2019 \nB.3 2020 \nB.4 2021 \nB.5 2022 \nDataset    C.1 Public \nC.2 Private \nTransformer D.1Transformer \nD.2 BERT \nD.3 GPT \n \nResult and Discussion \n \nThe number of article publications shows how high \nthe accuracy value of the research carried out in a certain \nperiod is. Referring to the graph shown in the table \nabove, which reviews a transformer method that shows \nthe level of accuracy in each method, shows that there is \nan average highest accuracy value of up to 90%, and \nthere is also an average accuracy value of only 10%.  In \naddition, the number of publications each year always \nappears using different methods. The trend of research \nusing different methods shows that transformers can be \nimplemented in various fields, not referring to only one \nfield. \n \nType of Research \nSome researchers use this type of experimental \nresearch. The type of experimental research here is \nalmost all using a public or unpublic database. Besides \nthat, some researchers are more interested in studying \ntransformers in theory, so the authors also use literature \nreviews for several studies from various sources. Figure \n1 is a graph of the journal's analysis of the transformer. \n \nFigure 2. Research type diagram \n \nIn Figure 2 there are 2 types of research, namely \nliterature review and experiment. This study examines \nmany journals the largest number of which is 78% in the \nexperimenter and 22% of the total journals studied. \n \nResearch Year \nSome researchers use this type of experimental \nresearch. The type of experimental research here is \nalmost all using a public or unpublic database. Besides \nthat, some researchers are more interested in studying \ntransformers in theory, so the authors also use literature \nreviews for several studies from various sources. \n \n \nFigure 3. Year of research \n22%\n78%\nSum\nLiterature riview Experiment\n\nJurnal Penelitian Pendidikan IPA (JPPIPA) September 2023, Volume 9 Issue 9, 604-610 \n \n607 \n The research years in Figure 2 consist of 2017, 2019, \n2020, 2021, and 2022. The most researched year was 2020, \nnamely 11, 2021, 6, 2019, 3, 2017, 1, 2022, 1. Of the five \nyears, there was 2018 when there was no research, so this \nis an opportunity for transformer research in 2018. \n \nDatasets \nThis study in Figure 4, uses public datasets and \nprivate datasets. Public datasets are datasets that are \neasy to search for. Private datasets are hard -to-find \ndatasets with certain approvals. Private datasets are \nusually used in the health sector. \n \n \nFigure 4. Dataset types \n \nResearch Methods \nIn this study, there are several methods used in \nmaking the research method consisting of transformers, \nBERT, and GPT. BERT and GPT are variants of the \ntransformer itself. The use of the transformer method is \nin Figure 5. D ominates more than the others, this is an \nopportunity to explore the BERT and GPT methods. Of \nthe 23 papers I read, 16 used transformers, 6 used BERT, \nand 1 used GPT. \n \n \nFigure 5. Transformer model \n \n \nLevel of Accuracy \nThe accuracy rate in Figure 6 averages above 40%. \nSome of the studies analyzed did not mention their \naccuracy, only told the process of collecting data or \ncomparing methods. A total of 10 of the 23 journals \nanalyzed reported the accuracy achieved. 1 journal 54% \n[20]. There is 1 that reaches 99.1% [1]. \n \n \nFigure 6. Accuracy level \n \nField of Data Analysis \n \n \nFigure 7. Field of analysis \n \nConclusion  \n \nSome of the conclusions from this research consist \nof: Most types of research are in experiments. this is \nbecause it is easy to find data on this type of research. \nresearch opportunities, Of the five years, there was 2018 \nwhen there was no research, so it became an opportunity \nfor transformer research in 2018 , The type of dataset \nused uses more public data than private data.  The \ntransformer method in e   r arch analyzed dominates the \ntransformer itself , the field of transformer research \ndominates in the general field. \n \n \n\nJurnal Penelitian Pendidikan IPA (JPPIPA) September 2023, Volume 9 Issue 9, 604-610 \n \n608 \nAcknowledgments  \nThanks to all parties who have supported the implementation \nof this research. I hope this research can be useful. \n \nAuthor Contributions  \nConceptualization, T.T., W. H., A. U. Z., Y. M. D., N. R., N. D. \nA.; methodology, T.T.; validation, W. H, and A. U. Z; formal \nanalysis, Y. M. D; investigation, N. R., and N. D. A; formal \nanalysis, T.T.; investigation, W. H and A. U. Z.; resources, Y. \nM. D.  and N. R.; data curation, N. D. A: writing—original draft \npreparation, T.T, and W. H.; writing—review and editing, A. \nU. Z.: visualization, Y. M. D and N. R.; supervision, N. D. A.; \nproject administration, T.T; funding acquisition, W. H., and A. \nU. Z. All authors have read and agreed to the published \nversion of the manuscript. \n         \nFunding \nThis research was independently funded by researchers. \n \nConflicts of Interest \nThe authors declare no conflict of interest. \n \nReferences  \n \nAbed, M., Imteaz, M. A., Ahmed, A. N., & Huang, Y. F. \n(2023). A novel application of transformer neural \nnetwork (TNN) for estimating pan evaporation \nrate. Applied Water Science , 13(2), 31. \nhttps://doi.org/10.1007/s13201-022-01834-w \nAcheampong, F. A., Wenyu, C., & Nunoo ‐Mensah, H. \n(2020). Text ‐based emotion detection: Advances, \nchallenges, and opportunities. Engineering Reports, \n2(7). https://doi.org/10.1002/eng2.12189 \nAlqudsi, A., & El-Hag, A. (2019). Application of machine \nlearning in transformer health index prediction. \nEnergies, 12 (14), 2694. \nhttps://doi.org/10.3390/en12142694 \nArshed, M. A., Mumtaz, S., Ibrahim, M., Ahmed, S., \nTahir, M., & Shafi, M. (2023). Multi -Class Skin \nCancer Classification Using Vision Transformer \nNetworks and Convolutional Neural Network -\nBased Pre-Trained Models. Information, 14(7), 415. \nhttps://doi.org/10.3390/info14070415 \nDurairaj, A. K., & Chinnalagu, A. (2021). Transformer \nbased Contextual Model for Sentiment Analysis of \nCustomer Reviews: A Fine -tuned BERT. \nInternational Journal of Advanced Computer Science \nand Applications, 12 (11). Retrieved from \nhttps://thesai.org/Downloads/Volume12No11/\nPaper_53-\nTransformer_based_Contextual_Model_for_Senti\nment_Analysis.pdf \nBacco, L., Cimino, A., Dell’Orletta, F., & Merone, M. \n(2021). Explainable Sentiment Analysis: A \nHierarchical Transformer -Based Extractive \nSummarization Approach. Electronics, 10(18), 2195. \nhttps://doi.org/10.3390/electronics10182195 \nBohatyrewicz, P.,  Płowucha, J., & Subocz, J. (2019). \nCondition Assessment of Power Transformers \nBased on Health Index Value. Applied Sciences , \n9(22), 4877. https://doi.org/10.3390/app9224877 \nCaucheteux, C., & King, J. -R. (2022). Brains and \nalgorithms partially converge in natural language \nprocessing. Communications Biology , 5(1), 134. \nhttps://doi.org/10.1038/s42003-022-03036-1 \nPutri, C. A. (2020). Analisis sentimen review film \nberbahasa Inggris dengan pendekatan \nbidirectional encoder representations from \ntransformers. JATISI (Jurnal Teknik Informatika dan \nSistem Informasi), 6 (2), 181 -193. \nhttps://doi.org/10.35957/jatisi.v6i2.206 \nPasaribu, D. J. M., Kusrini, K., & Sudarmawan, S. (2020). \nPeningkatan Akurasi Klasifikasi Sentimen Ulasan \nMakanan Amazon dengan Bidirectional LSTM dan \nBert Embedding. Inspiration: Jurnal Teknologi \nInformasi dan Komunikasi, 10 (1), 9 -20. \nhttp://dx.doi.org/10.35585/inspir.v10i1.2568 \nYang, D., Myronenko, A., Wang, X., Xu, Z., Roth, H. R., \n& Xu, D. (2021). T -AutoML: Automated machine \nlearning for lesion se gmentation using \ntransformers in 3d medical imaging. In Proceedings \nof the IEEE/CVF international conference on computer \nvision (pp. 3962 -3974). Retrieved from \nhttps://openaccess.thecvf.com/content/ICCV202\n1/papers/Yang_T-\nAutoML_Automated_Machine_Learning_for_Lesi\non_Segmentation_Using_Transformers_in_ICCV_\n2021_paper.pdf \nEkstedt, E., & Skantze, G. (2020). Turngpt: a transformer-\nbased language model for predicting turn -taking in \nspoken dialog . Association for Computational \nLinguistics. \nhttps://doi.org/10.18653/v1/2020.findings-\nemnlp.268 \nGhosh, S., Ekbal, A., & Bhattacharyya, P. (2023). VAD -\nassisted multitask transformer framework for \nemotion recognition and intensity prediction on \nsuicide notes. Information Processing & Management, \n60(2), 103234. \nhttps://doi.org/10.1016/j.ipm.2022.103234 \nGhojogh, B., & Ghodsi, A. (2020). Attention mechanism, \ntransformers, BERT, and GPT: tutorial and survey . \nhttps://doi.org/10.31219/osf.io/m6gcn \nGraterol, W., Diaz -Amado, J., Cardinale, Y., Dongo, I., \nLopes-Silva, E., & Santos -Libarino, C. (2021). \nEmotion Detection for Social Robots Based on NLP \nTransformers and an Emotion Ontology. Sensors, \n21(4), 1322. https://doi.org/10.3390/s21041322 \nGrechishnikova, D. (2021). Transformer neural network \nfor protein -specific de novo drug generation as a \nmachine translation problem. Scientific Reports , \nJurnal Penelitian Pendidikan IPA (JPPIPA) September 2023, Volume 9 Issue 9, 604-610 \n \n609 \n11(1), 321. https://doi.org/10.1038/s41598 -020-\n79682-4 \nHe, K., Gan, C., Li, Z., Rekik, I., Yin, Z., Ji, W., Gao, Y., \nWang, Q., Zhang, J., & Shen, D. (2023). \nTransformers in medical image analysis. Intelligent \nMedicine, 3(1), 59 –78. \nhttps://doi.org/10.1016/j.imed.2022.07.002 \nKhurana, D., Koli, A., Khatter, K., & Singh, S. (2023). \nNatural language processing: State of the art, \ncurrent trends and challenges. Multimedia Tools and \nApplications, 82(3), 3713 –3744. \nhttps://doi.org/10.1007/s11042-022-13428-4 \nLi, Q., Wu, C., Wang, Z., & Zheng, K. (2020). Hierarchical \nTransformer Network for Utterance -Level \nEmotion Recognition. Applied Sciences, 10(13), 4447. \nhttps://doi.org/10.3390/app10134447 \nLi, Z., Zhang, Z., Zhao, H., Wang, R., Chen, K., Utiyama, \nM., & Sumi ta, E. (2021). Text compression -aided \ntransformer encoding. IEEE Transactions on Pattern \nAnalysis and Machine Intelligence, 44 (7), 3840-3857. \nRetrieved from  \nhttps://ieeexplore.ieee.org/abstract/document/9\n354025 \nLin, T., Wang, Y., Liu, X., & Qiu, X. (2022). A survey of \ntransformers. AI Open , 3, 111 –132. \nhttps://doi.org/10.1016/j.aiopen.2022.10.001 \nLiu, H. -I., & Chen, W. -L. (2022). X -Transformer: A \nMachine Translation Model Enhanced by the Self -\nAttention Mechanism. Applied Sciences, 12(9), 4502. \nhttps://doi.org/10.3390/app12094502 \nLuitse, D., & Denkena, W. (2021). The great transformer: \nExamining the role of large language models in the \npolitical economy of AI. Big Data & Society, 8 (2), \n20539517211047734. \nhttps://doi.org/10.1177/20539517211047734 \nMoutik, O., Sekkat, H., Tigani, S., Chehri, A., Saadane, \nR., Tchakoucht, T. A., & Paul, A. (2023). \nConvolutional Neural Networks or Vision \nTransformers: Who Will Win the Race for Action \nRecognitions in Visual Data? Sensors, 23(2), 734. \nhttps://doi.org/10.3390/s23020734 \nÖztürk, O., & Özcan A .  (2022). Ideology Detection Using \nTransformer-Based Machine Learning Models. \nRetrieved from https://rb.gy/28t72 \nPaaß, G., & Giesselbach, S. (2023). Improving Pre-trained \nLanguage Models. In G. Paaß & S. Giesselbach, \nFoundation Models for Natural Language Processing  \n(pp. 79 –159). Springer International Publishing. \nhttps://doi.org/10.1007/978-3-031-23190-2_3 \nPark, J., Choi, K., Jeon, S., Kim, D., & Park, J. (2019). A Bi-\nDirectional Transformer for Musical Chord. 20th \nInternational Society for Music Information Retrieval \nConferences Netherland .  \nhttps://doi.org/10.13140/RG.2.2.12303.51362 \nPiccialli, F., Di Cola, V. S., Giampaolo, F., & Cuom o, S. \n(2021). The Role of Artificial Intelligence in \nFighting the COVID -19 Pandemic. Information \nSystems Frontiers , 23(6), 1467 –1497. \nhttps://doi.org/10.1007/s10796-021-10131-x \nProttasha, N. J., Sami, A. A., Kowsher, M., Murad, S. A., \nBairagi, A. K., Masud, M., & Baz, M. (2022). \nTransfer Learning for Sentiment Analysis Using \nBERT Based Supervised Fine -Tuning. Sensors, \n22(11), 4157. https://doi.org/10.3390/s22114157 \nPutra, M. F. D. A., & Hidayatullah, A. F. (2021). Tinjauan \nLiteratur: Named Entity Recognition pada Ulasan \nWisata. AUTOMATA, 2 (1). Retrieved from \nhttps://journal.uii.ac.id/AUTOMATA/article/vi\new/17391 \nRajan, K., Zielesny, A., & Steinbeck, C. (2021). DECIMER \n1.0: deep lea rning for chemical image recognition \nusing transformers. Journal of Cheminformatics, \n13(1), 1 -16. https://doi.org/10.1186/s13321 -021-\n00538-8 \nRamos-Pérez, E., Alonso -González, P. J., & Núñez -\nVelázquez, J. J. (2021). Multi -Transformer: A New \nNeural Network -Based Architecture for \nForecasting S&P Volatility. Mathematics, 9(15), \n1794. https://doi.org/10.3390/math9151794 \nReinauer, R., Caorsi, M., & Berkouk, N. (2021). \nPersformer: A transformer architecture for topological \nmachine learning . \nhttps://doi.org/10.48550/arXiv.2112.15210 \nRendragraha, A. D., Bijaksana, M. A., & Romadhony, A. \n(2021). Pendekatan metode Transformers untuk \ndeteksi bahasa kasar dalam komentar berita online \nIndonesia. eProceedings of Engineering, 8 (2). \nRetrieved from \nhttps://repository.telkomuniversity.ac.id/pustak\na/files/167651/jurnal_eproc/pendekatan-\nmetode-transformers-untuk-deteksi-bahasa-kasar-\ndalam-komentar-berita-online-indonesia.pdf \nRöder, L. L. (2023). Neural Network Architectures for \nAbsorption Spectroscopy . \nhttps://doi.org/10.21203/rs.3.rs-2832856/v1 \nSanmarchi, F., Bucci, A., Nuzzolese, A. G., Carullo, G., \nToscano, F., Nante, N., & Golinelli, D. (2023). A \nstep-by-step researcher’s guide to the use of an AI-\nbased transformer in epidemiology: An \nexploratory analysis of ChatGPT using the \nSTROBE checklist for observational studies. Journal \nof Public Health . https://doi.org/10.1007/s10389 -\n023-01936-y \nSantangelo, O. E., Gentile, V., Pizzo, S., Giordano, D., & \nCedrone, F. (2023). Machine Learning and \nPrediction of Infectious Diseases: A System atic \nReview. Machine Learning and Knowledge Extraction, \nJurnal Penelitian Pendidikan IPA (JPPIPA) September 2023, Volume 9 Issue 9, 604-610 \n \n610 \n5(1), 175 –198. \nhttps://doi.org/10.3390/make5010013 \nSingla, S., & Ramachandra , N. (2020). Comparative \nanalysis of transformer based pre -trained NLP \nModels. Int. J. Comput. Sci. Eng, 8 , 40 -44. \nhttps://doi.org/10.26438/ijcse/v8i11.4044 \nTabinda Kokab, S., Asghar, S., & Naz, S. (2022). \nTransformer-based deep learning models for the \nsentiment analysis of social media data. Array, 14, \n100157. \nhttps://doi.org/10.1016/j.array.2022.100157 \nTaye, M. M. (2023). Understanding of Machine Learning \nwith Deep Learning: Architectures, Workflow, \nApplications and Future Directions. Computers, \n12(5), 91 . \nhttps://doi.org/10.3390/computers12050091 \nThoyyibah, T., Abdurachman, E., Heryadi, Y., & Zahra, \nA. (2022). Transformer Model in Music Mood \nClassification. International Journal of Applied \nEngineering and Technology (London), 4 (1), 40 –44. \nRetrieved from \nhttps://romanpub.com/resources/ijaet v4 -1-\n2022-08.pdf \nTran, Q. T., Davies, K., & Roose, L. (2020). Machine \nlearning for assessing the service transformer \nhealth using an energy monitor device. IOSR \nJournal of Electrical and Electronics Engineering, 15(6), \n1-6. Retrieved from \nhttps://www.iosrjournals.org/iosr-\njeee/Papers/Vol15-Issue6/Series-\n1/A1506010106.pdf \nValencia, F., Arcos, H., & Quilumba, F. (2021). \nComparison of Machine Learning Algorithms for \nthe Prediction of Mechanical Stress in Three-Phase \nPower Tr ansformer Winding Conductors. Journal \nof Electrical and Computer Engineering, 2021 , 1 -9. \nhttps://doi.org/10.1155/2021/4657696. 2021 \nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., \nJones, L., Gomez, A. N., ... & Polosukhin, I. (2017). \nAttention is all you need. Advances in neural \ninformation processing systems, 30 . Retrieved from \nhttps://proceedings.neurips.cc/paper_files/pape\nr/2017/hash/3f5ee243547dee91fbd053c1c4a845aa\n-Abstract.html \nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & \nChao, L. S . (2019). Learning deep transformer models \nfor machine translation . \nhttps://doi.org/10.48550/arXiv.1906.01787 \nWu, N., Green, B., Ben, X., & O'Banion, S. (2020). Deep \ntransformer models for time series forecasting: The \ninfluenza prevalence case . \nhttps://doi.org/10.48550/arXiv.2001.08317 \nWu, B., Wang, L., & Zeng, Y. -R. (2023). Interpretable \ntourism demand forecasting with temporal fusion \ntransformers amid COVID -19. Applied Intelligence, \n53(11), 14493 –14514. \nhttps://doi.org/10.1007/s10489-022-04254-0 \nXiao, Y., Jin, Y., Cheng, R., & Hao, K. (2022). Hybrid \nattention-based transformer block model for \ndistant supervision relation extraction. \nNeurocomputing, 470 , 29 -39. \nhttps://doi.org/10.1016/j.neucom.2021.10.037 \nYang, G. R., & Wang, X. -J. (2020). Artificial Neural \nNetworks for Neuroscientists: A Primer. Neuron, \n107(6), 1048 –1070. \nhttps://doi.org/10.1016/j.neuron.2020.09.005 \nZhong, P., Wang, D., & Miao, C. (2019). Knowledge-\nenriched transformer for emotion detectio n in textual \nconversations. \nhttps://doi.org/10.48550/arXiv.1909.10681  \n \n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7767519354820251
    },
    {
      "name": "Computer science",
      "score": 0.6260977983474731
    },
    {
      "name": "Artificial neural network",
      "score": 0.4900897741317749
    },
    {
      "name": "Artificial intelligence",
      "score": 0.475710928440094
    },
    {
      "name": "Encoder",
      "score": 0.4555737376213074
    },
    {
      "name": "Machine learning",
      "score": 0.39323732256889343
    },
    {
      "name": "Engineering",
      "score": 0.2425926923751831
    },
    {
      "name": "Electrical engineering",
      "score": 0.17356765270233154
    },
    {
      "name": "Voltage",
      "score": 0.1168210506439209
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210157166",
      "name": "Universitas Pamulang",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I4210158908",
      "name": "Universitas Pembangunan Nasional Veteran Jakarta",
      "country": "ID"
    },
    {
      "id": "https://openalex.org/I2800024422",
      "name": "Universitas Wijaya Putra",
      "country": "ID"
    }
  ],
  "cited_by": 5
}