{
  "title": "Temporal Language Modeling for Short Text Document Classification with Transformers",
  "url": "https://openalex.org/W4298147619",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2044777546",
      "name": "Jakub Pokrywka",
      "affiliations": [
        "Polytecnic in Požega"
      ]
    },
    {
      "id": "https://openalex.org/A2077610807",
      "name": "Filip Graliński",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3174266714",
    "https://openalex.org/W2972594731",
    "https://openalex.org/W3171460770",
    "https://openalex.org/W2798837230",
    "https://openalex.org/W2949573100",
    "https://openalex.org/W6765602453",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2848592235",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W6790695295",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287855079",
    "https://openalex.org/W6802831181",
    "https://openalex.org/W3153269634",
    "https://openalex.org/W2113747815",
    "https://openalex.org/W3199421823",
    "https://openalex.org/W4212964822",
    "https://openalex.org/W3126553126",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W4288283362",
    "https://openalex.org/W3211777899"
  ],
  "abstract": "Language models are typically trained on solely text data, not utilizing document timestamps, which are available in most internet corpora.In this paper, we examine the impact of incorporating timestamp into transformer language model in terms of downstream classification task and masked language modeling on 2 short texts corpora.We examine different timestamp components: day of the month, month, year, weekday.We test different methods of incorporating date into the model: prefixing date components into text input and adding trained date embeddings.Our study shows, that such a temporal language model performs better than a regular language model for both documents from training data time span and unseen time span.That holds true for classification and language modeling.Prefixing date components into text performs no worse than training special date components embeddings.",
  "full_text": "T emporal Language Modeling for Short T ext\nDocument Classiﬁcation with Transformers\nJakub Pokrywka, Filip Grali ´nski\nAdam Mickiewicz University ,\nFaculty of Mathematics and Computer Science,\nUniwersytetu Pozna ´nskiego 4\n61-614 Pozna ´n, Poland\nEmail: {ﬁrstname.lastname}@amu.edu.pl\nAbstract—Language models are typically trained on solely text\ndata, not utilizing document timestamps, which are available in\nmost internet corpora. In this paper , we examine the impact\nof incorporating timestamp into transformer language model in\nterms of downstream classiﬁcation task and masked language\nmodeling on 2 short texts corpora. W e examine different times-\ntamp components: day of the month, month, year , weekday .\nW e test different methods of incorporating date into the model:\npreﬁxing date components into text input and adding trained date\nembeddings. Our study shows, that such a temporal language\nmodel performs better than a regular language model for\nboth documents from training data time span and unseen time\nspan. That holds true for classiﬁcation and language modeling.\nPreﬁxing date components into text performs no worse than\ntraining special date components embeddings.\nI. I N T RO D U C T I O N\nM\nOST language models are trained solely on text data.\nLeveraging text domain, such as language [12] or style\n[10] into a language model may have a positive effect on it.\nTime of text authorship may be also considered as an input\nfeature, but this poses speciﬁc challenges (and opportunities)\nas:\n• time is continuous, whereas language is discrete, at\nany time moment, an event might change a language\nirreversibly and not trivial to combine time and language\nunits both from the mathematical and practical stand-\npoint;\n• texts might reﬂect natural and social cycles (days, weeks,\nyears, cyclical sport and political events);\n• text content might be correlated with extralinguistic fea-\ntures, themselves correlating with time (e.g. air tempera-\nture).\nRecently , the NLP community has started to use time as a\nfeature in training and/or ﬁne-tuning large neural models ([1],\n[16], [19]). Here, we analyze temporal language modeling in\nthe context of two classiﬁcation tasks in different timescales:\nIreland News Headlines and T witter Sentiment Analysis. W e\nalso incorporate date components other than year. W e fo-\ncus on examining different approaches to date incorporation\n(learnable embeddings, preﬁxing text) using periodic and non-\nperiodic time features under a downstream classiﬁcation task.\nThe contributions of this paper are as follows:\n• two classiﬁcation datasets were redeﬁned in a common\nsetup in which three time-related tasks are introduced:\nclassiﬁcation (possibly) using temporal metadata, predict-\ning temporal metadata (as a regression task) and temporal\nlanguage-modeling task (as a cloze task).\n• we compared three methods for introducing temporal\ninformation into neural language models;\n• we considered not only linear time, but also cycles such\nas years, weeks, and months;\n• we measured the performance of RoBER T a [14] models\nin several setups on the two datasets (using different parts\nof the temporal information, and both ﬁne-tuning and\ntraining from scratch);\n• the relations between the temporal metadata, the texts and\nthe results obtained were analyzed.\nThe datasets and source of our code are publicly available.\nGenerally , utilizing a date does not cost much effort, because\nmany internet documents are available with a timestamp and\nit is possible to adapt existing models to new domain. Such\ntemporal language models may contribute to:\n• e-commerce search engines, e.g. users intention with\nshort phrase \"umbrella\" may refer to umbrella protecting\nfrom a rain in the autumn or sun umbrella in the summer;\n• other types of search engines, e.g. historical newspapers;\n• OCR for historical documents.\nII. D ATA S E T S\nUsually , text classiﬁcation tasks do not incorporate time and\nother metadata. W e suppose its impact is stronger for short\ntexts due to shorter texts carrying less information. The time\nimpact may be stronger for text, which may depend on peo-\nple’s mood or different interests. W e carried out experiments\nwith two large short-text classiﬁcation datasets, where every\nsample is assigned a time stamp. One is spread over more than\n20 years, the other ones — only 80 days. Both datasets are in\nEnglish.\nA. Ireland News\nThe dataset is available at Kaggle 1 , its creator is Rohit\nKulkarni. It consists of article headlines posted by the Irish\n1 https://www .kaggle.com/therohk/ireland- historical- news\nProceedings of the of the 17 th Conference on Computer\nScience and Intelligence Systems pp. 121–128\nDOI: 10.15439/2022F174\nISSN 2300-5963 ACSIS, V ol. 30\nIEEE Catalog Number: CFP2285N-AR T ©2022, PTI 121\nT ABLE I: Categories count in datasets.\ncategory item\ntrain dev test test 20/21\nnews 603996 75963 75783 30278\nbusiness 162550 20330 20034 14477\nsport 195384 24543 24346 13447\nopinion 91697 11572 11528 8086\nculture 67260 8525 8424 5643\nlife&style 65120 8093 8084 7188\nTimes newspaper. Each headline is accompanied by a times-\ntamp and article category (text of an article is not included).\nThere are six main categories: news, sport, opinion, business,\nculture, life&style. The datasets statistics are described in\nT able I. There are more ﬁne-grained subcategories provided\nin the original dataset, but they vary over time, so we didn’t\nmake use of them in our experiments.\nTimestamps range from 1996-01-01 to 2021-06-30. There\nare 1,611,495 such headlines in total.\nW e employed the date range from 1996-01-01 to 2019-12-\n31 for most of our experiments and created an additional test\nset, which consists of 2020-2021 years, which dates are non-\noverlapping with the rest of the dataset. W e refer to this test\nset as Ireland News 2020-2021 . The test set Ireland News ,\nwithout year annotation, refers to time span from training\ndata (1996-2019). Since train/dev/test split is not determined\nat the original dataset site, we assign each sample randomly\nto train/dev/test using the 80%/10%/10% split. This resulted\nin the 1,186,898 / 149,134 / 148,308 train/dev/test split. The\naverage number of words in the dataset is 7.1 per headline.\nB. Sentiment140\nThis sentiment analysis dataset is obtained and described\nin [2]. Since in the original dataset the train set contains\n1,600,000 items (positive and negative tweets) and test set\nonly 498 (positive, negative, and neutral tweets), we made\nsigniﬁcant modiﬁcations: neutral tweets were deleted from the\ntest set, 100,000 random items were added to the test set, also\na dev set was created by randomly selecting 100,000 samples\nfrom the train set. This resulted in the 1,400,000 / 100,000\n/ 100,359 train/dev/test split. Timestamps range from 2009-\n04-06 to 2009-06-25. The datasets set are balanced ( ∼ 50%\npositive and ∼ 50% negative tweets). The average number of\nwords is 13.8 per item. T weets are from users in different time\nzones. W e take time local to the author of a tweet.\nIII. D ATA S E T S A NA LY S I S\nThe number of items per category differs in time. The\ndistribution over days of month, months, years, weekdays in\ntrain datasets are presented in Figures 1 and 2 for, respectively ,\nSentiment140 and Ireland News. For the Sentiment140 dataset\ndistribution over a year is not presented, since all items are\nfrom 2009. Mutual Information between presented factors\nand the class is given in T able V. In Ireland News, mutual\ninformation related to days of month and months is much\nlower than those of years and weekdays. In Sentiment140\nmutual information is similar for days of month, months, and\nweekdays.\nIn both datasets, there are dependencies, which may be\nhelpful for model performance. E.g. in Ireland News there\nare more sports texts on Friday and in Sentiment140 there are\nmore negative texts on W ednesdays and Thursdays.\nIV . T A S K S\nW e created three tasks for each dataset: classiﬁcation,\n‘fractional’ year prediction, word gap prediction. Our main\nobjective was to examine the impact of incorporating times-\ntamps on text classiﬁcation tasks. Fractional year prediction\nand word gap prediction tasks are mainly for analysis of the\nresults in classiﬁcation tasks.\nW e added timestamps in fractional-year form, which can be\ndescribed by the following code:\ndays_in_year =\n366 if year_is_leap_year else 365\nfractional_year =\n(year + (day_in_year-1+day) /\ndays_in_year )\nEach item in our tasks is associated with a text, timestamp\n(day precision), fractional year, and category . Sample data is\ndescribed in T able IV.\nEach challenge for a given dataset uses the same train/de-\nv/test split. The challenges are publicly available, courtesy\nof the site’s owners, via the Gonito evaluation platform [3].\nSource code of the challenge is available via the platform as\nwell.\nA. Classiﬁcation\nThe task objective is to predict the headline category\ngiven text, date, and fractional year. The evaluation metric\nis simple accuracy . The challenges are available at: https:\n//gonito.net/challenge/ireland-news-headlines (Ireland News)\nand https://gonito.net/challenge/sentiment140 (Sentiment140).\nDataset download and submission instructions are under the\n\"How T o\" tab, source code is under the \"All Entries\" →\ncatalog icon in each submission row .\nB. Y ear prediction\nThe objective is to predict the year given the text.\nThe metric is Root Mean Square Error (RMSE). The\nchallenges are available at: https://gonito.net/challenge/\nireland-news-headlines-year-prediction (Ireland News) and\nhttps://gonito.net/challenge/sentiment140-year-prediction\n(Sentiment140).\nC. W ord gap ﬁlling\nThe task objective is to predict a masked word, like\nin Masked Language Modeling, given text, date, fractional\nyear. W ord is deﬁned by characters split by spaces. There\nis always exactly one masked word in each sample to\n122 PROCEEDINGS OF THE FEDCSIS. SOFIA, BULGARIA, 2022\n(a) Distribution of classes over months.\n (b) Distribution of classes over days.\n(c) Distribution of items over weekdays.\nFig. 1: Distribution of classes over date factors in Sentiment140 dataset. Distribution over year is not presented, since all items\ncome from one year.\nT ABLE II: Samples from the Ireland News dataset. T o check article-id visit www .irishtimes.com/article-id The article ID is\nnot provided in the challenge.\nfractional year timestamp text category article ID\n2004.5082 20040705 Sudan claims it is disarming militias news 1.1147721\n2008.4426 20080611 Bluffer’s guide to Euro 2008 sport 1.1218069\n2017.1068 20170209 Gannon offers homes in Longview near Swords life&style 1.2966726\npredict. The metric is PerplexityHashed implemented in\nthe GEval evaluation tool [4], which is a modiﬁed ver-\nsion of LogLossHashed as described by [5]. This met-\nric ensures fair assessment disregarding model vocabulary .\nThe challenges are available at: https://gonito.net/challenge/\nireland-news-headlines-word-gap (Ireland News) and https://\ngonito.net/challenge/sentiment140-word-gap (Sentiment140).\nV . M E T H O D S\nW e used the RoBER T a model in the base version [14].\nAll models are described in this section. All code is publicly\navailable via git commit hashes given in result tables.\n2\nA. Regular Transformer as a baseline\nThe baseline is a regular RoBER T a with no temporal\ninformation. W e refer to this method as noDate in result tables.\n2\nReference codes to repositories stored at Gonito.net [3] are given in curly\nbrackets. Such a repository may be also accessed by going to http://gonito.\nnet/q and entering the code there.\nB. T emporal Transformer\nW e selected the following temporal information: year,\nmonth, day of the month (day), weekday . All of them are incor-\nporated in our temporal models. W e experimented with 3 ways\nof including temporal information into RoBER T a models. The\nﬁrst two involve slight RoBER T a model architecture changes\nand training new embeddings during RoBER T a training. The\nthird one is only input data modiﬁcation. They are described\nbelow .\n1) Date as embeddings added to every input token:\nT emporal embeddings are added to every input token as:\nembedding = token _ emb + pos _ emb + year _ emb +\nmonth _ emb + monthday _ emb + weekday _ emb\nfor each token _ pos . W e refer to this method as addedEmbDate\nin result tables.\nJ AKUB POKR YWKA, FILIP GRALI ´NSKI: TEMPORAL LANGUAGE MODELING FOR SHOR T TEXT DOCUMENT CLASSIFICA TION WITH TRANSFORMERS 123\n(a) Distribution of classes over years.\n (b) Distribution of classes over years as a stacked bar plot. Note the\ndifferent y axis limit than other plots.\n(c) Distribution of classes over months.\n (d) Distribution of classes over days of month.\n(e) Distribution of classes over weekdays.\nFig. 2: Distribution of items over date factors in Ireland News dataset.\n2) Date as stacked embeddings: T emporal embeddings are\nstacked at the beginning of the input sequence, as:\nemb =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear _ emb if token_pos = 1\nmonth _ emb if token_pos = 2\nmonth _ emb if token_pos = 3\nweekday _ emb if token_pos = 4\ntoken _ emb +\npos _ emb otherwise\nWhere all tokens are shifted 4 positions to the right, so ﬁrst\ntext token is on token _ pos = 5 W e refer to this method as\nstackedEmbDate in result tables.\n3) Date as regular text: W e only modify text input of\nmodel by adding temporal information with preﬁxes, so\nitem with date 20040705 and text Sudan claims it is\ndisarming militias is combined to text year: 2004\nmonth: 7 day: 5 weekday: 1 Sudan claims it\nis disarming militias .\nVI. E X P E R I M E N T S\nA. Classiﬁcation\nW e carried out experiments with text classiﬁcation using all\npresented models. RoBER T a was ﬁnetuned and trained from\npretrained checkpoints (which we refer to as pretrained) and\nwith randomly initialized weights (which we refer to as ‘from\nscratch‘). The only training objective is the classiﬁcation task.\nW e report the results in T able IV.\nW e examined the impact on classiﬁcation by each date\nfactor. Since all temporal data incorporation methods yield\nsimilar results, we chose the regular text date incorporation\nmethod due to ease of its use (only text modiﬁcation with no\narchitecture changes). The results are presented in T able V.\nT o examine this model conditioned by different preﬁxes we\n124 PROCEEDINGS OF THE FEDCSIS. SOFIA, BULGARIA, 2022\nT ABLE III: Model roberta-pretrained-textDate predictions depending on a given date in a development dataset. If a date is\nrepresented by a dash, it is not preﬁxed to the model, bolded dates are as they occur actually in the dataset, not bolded are\nrandom. The examples are cherry-picked. T o check article-id visit www .irishtimes.com/article-id The article ID is not provided\nin the challenge.\ntext article ID timestamp actual prediction\nNew bridge for Calzaghe to cross 1.914946 20080419 Sat. sport sport\nNew bridge for Calzaghe to cross 1.914946 20130307 Thu. - life&style\nNew bridge for Calzaghe to cross 1.914946 - - news\nSydney stereotypes 1.1102371 20000913 W ed. sport sport\nSydney stereotypes 1.1102371 20110422 Fri. - opinion\nSydney stereotypes 1.1102371 - - sport\nRóisín Meets... comedian Mario Rosenstock 1.2463531 20151212 Sat. life&style life&style\nRóisín Meets... comedian Mario Rosenstock 1.2463531 20040725 Sun. - news\nRóisín Meets... comedian Mario Rosenstock 1.2463531 - - news\nT ABLE IV: Classiﬁcation results. Different date incorporation into model. Acc stands for accuracy . The bold results are best\nin its category (without and with external data).\nIreland News Sentiment140\nmethod acc gonito acc gonito\nmost frequent from train 51.10 {161712} 49.88 {b4b180}\nroberta-pretrained-noDate 82.35 {daaaf9} 89.27 {a8d1b7}\nroberta-pretrained-stackedEmbDate 87.65 {9e041f} 91.16 {252c0c}\nroberta-pretrained-addedEmbdate 86.82 {cede76} 91.04 {aa28dc}\nroberta-pretrained-textDate 87.84 {7c52ed} 91.13 {688320}\nroberta-scratch-noDate 77.88 {0798d5} 83.38 {e984db}\nroberta-scratch-stackedEmbDate 83.24 {74efba} 86.18 {e3ff3e}\nroberta-scratch-addedEmbdate 81.96 {587033} 85.47 {1c122b}\nroberta-scratch-textDate 83.16 {413f72} 86.02 {d969ca}\nT ABLE V: Classiﬁcation accuracy results. Different date elements included. Acc stands for accuracy . MI stands for Mutual\nInformation between a class and a date factor. MI for Sentiment140 between year and class equals 0, because there is only\n2009 year in the dataset.\nIreland News Sentiment140\nmethod Acc Gonito MI(1e-5) Acc Gonito MI(1e-3)\nroberta-pretrained-noDate 82.35 {daaaf9} - 89.27 {a8d1b7} -\nroberta-pretrained-textDate 87.84 {7c52ed} - 91.13 {688320} -\nroberta-pretrained-textDay 82.66 {ca5340} 9 90.16 {2c2d07} 58\nroberta-pretrained-textMonth 82.72 {3d5bb6} 61 89.59 {64cc1b} 16\nroberta-pretrained-textY ear 85.90 {893bbe} 3354 89.32 {be6d55} 0\nroberta-pretrained-textW eekday 84.46 {daf69a} 3127 89.60 {8abd71} 19\nT ABLE VI: Roberta-pretrained-textDate classiﬁcation on de-\nvelopment set result. All results comes from the same model,\nthe only difference is the preﬁx construction. Preﬁx is a\nstandard model mode, no-preﬁx is a mode where no date is\npreﬁxed, and random-preﬁxed stands for a mode, where the\ndate preﬁx comes from random date 1996-01-01 to 2021-06-\n30.\nmodel dev acc\npreﬁx 87.97\nno-preﬁx 78.38\nrandom-preﬁx 73.97\nchecked its performance with no preﬁx and random preﬁx\nsettings. Results are in T able VI and T able VII. The samples\nfrom different preﬁx settings are provided in T able IV.\nT o check model degradation, we made an inference on\nIreland News test set from years 2020-2021. This is a time\nspan later than training data, which comes from 1996-2019.\nThe results are in T able VIII.\nThe impact of train dataset size is presented in Figure 3.\nB. Y ear prediction\nW e choose two methods for year prediction. The ﬁrst is\na baseline using term frequency-inverse document frequency\n(TF-IDF) with logistic regression. The second is averaging all\noutput embeddings of RoBER T a and feeding to linear regres-\nsion (LR) layer. Both RoBER T a and linear regression weights\nare tuned during training. In both methods, the minimum\n(maximum) output is limited to the minimum (maximum)\nJ AKUB POKR YWKA, FILIP GRALI ´NSKI: TEMPORAL LANGUAGE MODELING FOR SHOR T TEXT DOCUMENT CLASSIFICA TION WITH TRANSFORMERS 125\nT ABLE VII: Classiﬁcation improvement due to preﬁxing on roberta-pretrained-textDate model. All results comes from the\nsame model, naming convention comes from T able VI.\ndev set percentage\naccurate on both preﬁx and no-preﬁx 75.14\naccurate on preﬁx, but not on no-preﬁx 12.83\naccurate on no-preﬁx, but not on preﬁx 3.19\nnot accurate on preﬁx, nor on no-preﬁx 9.84\nT ABLE VIII: Classiﬁcation accuracy results. T est set (years\n2020-2021) comes from other time span than training set\n(years 1996-2019).\nIreland News (2020/21)\nmethod acc gonito\nmost frequent 38.27 {953311}\nroberta-pretr.-noDate 85.99 {e684b3}\nroberta-pretr.-textDate 87.79 {5fba22}\nroberta-pretr.-textY ear 87.49 {8d5ad4}\nfractional year found in the datasets. The results are presented\nin T able IX, along with a null-model baseline using the mean\nfractional year from the training set as the prediction for each\ndata point.\nC. W ord gap ﬁlling\nRoBER T a was ﬁnetuned and trained from a pretrained\ncheckpoint and with randomly initialized weights. The training\nobjective is Masked Language Modeling. Only prepending\ndata to the input was considered as a method for introducing\nthe data. See T able X.\nVII. D I S C U S S I O N\nFor both datasets including dates into RoBER T a models\nraises the accuracy score. This stands true for pretrained and\nrandomly initialized models. Stacked embedding and date\nincorporation as a text give a similar result and both are\nslightly better than the method of adding embeddings to every\ninput token. It’s easier to modify input text than modify model\narchitecture, hence we recommend embedding date by preﬁx-\ning input texts. The greater mutual information is between\neach factor and class factor, the more the model gains in\naccuracy score. The model trained with a date preﬁx performs\nwell, only when the preﬁx is provided. There is no gain\nfrom date preﬁxing for a 1k documents train dataset and the\ngain is constant over 100k documents train dataset. Predicting\nfractional year is difﬁcult in both datasets because all models\nperform not much better than baseline. W e hypothesize this is\na reason why classiﬁcation beneﬁts from date metadata, since\nadding strongly correlated factors (like a date to text in this\ncase) would not bring information gain.\nThe temporal models perform better also for test sets from\nunseen years. T o our surprise, day of the month, month, week-\nday , year incorporation into model performs only marginally\nbetter than incorporation only year for Ireland News 2020-\n2021 dataset.\nIn pretrained models, date incorporation slightly lowers\nperplexity . Models with randomly initialized weights beneﬁt\nhugely from date incorporation.\nVIII. R E L AT E D WO R K\nThere are several studies concerning language model degra-\ndation over time and adaptation to newer data [13], [17], [6].\n[7] focused especially on text classiﬁcation. They considered\nyears as well as cyclical intervals (e.g., January-March). Their\nmethod was to train separate models for different time spans.\n[8] proposed method based on using discrete multiple tem-\nporal word embeddings based on time domains for document\nclassiﬁcation using recurrent neural networks. [9] developed\nmodel-agnostic timed dependent embedding representation for\ntime and evaluated on recurrent neural networks across various\ntasks. [1] introduced temporal T5 language model, where a\nyear was preﬁxed into text input and ﬁnetuned on temporal\ndata. The experiments focused on knowledge extraction from\nlanguage models and showed their method performs better\nin terms of language modeling and question answering than\nT5 language model with no preﬁxed year. [19] incorporated\nboth geographical and time data into a transformer model\nfor a QA task employing year as well as month and day .\n[16] preﬁxed year for semantic change detection. Additionally ,\nthe authors proposed the training objective of masking year\ninformation during model training. However, both [1], [16] use\nonly year metadata, in contrast to our study , where we also\ndays of month, months, weekdays are taken into consideration.\n[18] trained an SVM model to predict the date of text as a\nclassiﬁcation problem and [11] use approach of neologism\nbased approach. V ery recently [15] released temporal NLP\nchallenges based on a large corpus of historic texts but didn’t\ninclude downstream tasks, such as classiﬁcation. The corpus\nconsists of texts covering over 100 years. They trained from\nscratch and ﬁne-tuned temporal RoBER T a models based on\nday of month, months, weekdays, and year as a preﬁxed text.\nThey proved that temporal language models perform better\nthan standard language models.\nIX. C O N C L U S I O N\nTransformer models beneﬁt from temporal information data\nin classiﬁcation tasks for short texts. W e have proved that it’s\nnot only true for a year, but also other date factors, such\nas weekday , day of the month, and month. The greater the\nmutual information between a factor and a class, the greater\nthe beneﬁt. The result is important, because day of the month,\nmonth, weekday factors don’t outdate after model training\n126 PROCEEDINGS OF THE FEDCSIS. SOFIA, BULGARIA, 2022\nT ABLE IX: Fractional year prediction results, RMSE is for root-mean-square error, MAE – mean absolute error, LR – linear\nregression.\nIreland News Sentiment140\nmethod RMSE MAE Gonito RMSE MAE gonito\nmean from train 6.76426 5.80722 {0b0e9c} 0.04674 0.03396 {4856c5}\nTF-IDF + LR 5.32491 4.27185 {2226fb} 0.04917 0.03635 {579c8f}\nRoBER T a + LR head 4.53676 3.38758 {632b5d} 0.04469 0.03289 {349e5b}\nRoBER T a from scratch + LR head 4.51179 3.35951 {be0106} 0.04526 0.03222 {b672ee}\nT ABLE X: W ord gap prediction results. Ppl hashed stands for perplexity hashed.\nIreland News Sentiment140\nmethod ppl hashed gonito ppl hashed gonito\nequal probability 1024.0 {6bd5a8} 1024.0 {3de230}\nRoBER T a from scratch 90.8 {9ac479} 51.0 {f0f343}\nRoBER T a from scratch with time 46.0 {dc75a7} 46.1 {ddf16f}\nRoBER T a no ﬁne-tuning 51.0 {f0f343} 66.2 {e625c6}\nRoBER T a ﬁne-tuned 23.3 {42793a} 34.6 {a365da}\nRoBER T a ﬁne-tuned with time 21.6 {cfaf6c} 33.6 {37bd6e}\n(a) Ireland News test (1996-2019) accuracy .\n (b) Ireland News test (2020/21) accuracy .\nFig. 3: T est set accuracy varying on train dataset size for model with and without date incorporation.\ndue to its cyclical nature, differently to year, which is linear.\nThe best and simplest method for temporal data incorporation\nseems to be input text modiﬁcation.\nR E F E R E N C E S\n[1] B. Dhingra, J. R. Cole, J. M. Eisenschlos, D. Gillick, J. Eisenstein, and\nW . W . Cohen. Time-aware language models as temporal knowledge\nbases, 2021.\n[2] A. Go, R. Bhayani, and L. Huang. T witter sentiment classiﬁcation using\ndistant supervision. Processing , 150, 2009.\n[3] F . Grali ´nski, R. Jaworski, Ł. Borchmann, and P . Wierzcho ´n. Gonito.net –\nopen platform for research competition, cooperation and reproducibility .\nIn A. Branco, N. Calzolari, and K. Choukri, editors, Proceedings of the\n4REAL W orkshop: W orkshop on Research Results Reproducibility and\nResources Citation in Science and T echnology of Language , pages 13–\n20. 2016.\n[4] F . Grali ´nski, A. Wróblewska, T . Stanisławek, K. Grabowski, and\nT . Górecki. GEval: T ool for debugging NLP datasets and models. In\nProceedings of the 2019 ACL W orkshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP , pages 254–262, Florence, Italy ,\n2019. Association for Computational Linguistics.\n[5] F . Grali ´nski. (T emporal) language models as a competitive challenge. In\nZ. V etulani and P . Paroubek, editors, Proceedings of the 8th Language\n& T echnology Conference , pages 141–146. Fundacja Uniwersytetu\nim. Adama Mickiewicza w Poznaniu, 2017.\n[6] S. A. Hombaiah, T . Chen, M. Zhang, M. Bendersky , and M. Najork.\nDynamic language models for continuously evolving content. ArXiv\npreprint , abs/2106.06297, 2021.\n[7] X. Huang and M. J. Paul. Examining temporality in document classi-\nﬁcation. In Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (V olume 2: Short P apers) , pages 694–699,\nMelbourne, Australia, 2018. Association for Computational Linguistics.\n[8] X. Huang and M. J. Paul. Neural temporality adaptation for document\nclassiﬁcation: Diachronic word embeddings and domain adaptation\nmodels. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 4113–4123, Florence, Italy , 2019.\nAssociation for Computational Linguistics.\n[9] S. M. Kazemi, R. Goel, S. Eghbali, J. Ramanan, J. Sahota, S. Thakur,\nS. Wu, C. Smyth, P . Poupart, and M. A. Brubaker. Time2vec: Learning\na vector representation of time. ArXiv , abs/1907.05321, 2019.\n[10] N. S. Keskar, B. McCann, L. R. V arshney , C. Xiong, and R. Socher. Ctrl:\nA conditional transformer language model for controllable generation.\nArXiv , abs/1909.05858, 2019.\n[11] V . Kulkarni, Y . Tian, P . Dandiwala, and S. Skiena. Simple neologism\nbased domain independent models to predict year of authorship. In\nProceedings of the 27th International Conference on Computational\nLinguistics , pages 202–212, Santa Fe, New Mexico, USA, 2018. As-\nsociation for Computational Linguistics.\n[12] G. Lample and A. Conneau. Cross-lingual language model pretraining.\nIn NeurIPS , 2019.\n[13] A. Lazaridou, A. Kuncoro, E. Gribovskaya, D. Agrawal, A. Liska,\nT . T erzi, M. Gimenez, C. de Masson d’Autume, S. Ruder, D. Y ogatama,\nJ AKUB POKR YWKA, FILIP GRALI ´NSKI: TEMPORAL LANGUAGE MODELING FOR SHOR T TEXT DOCUMENT CLASSIFICA TION WITH TRANSFORMERS 127\nK. Cao, T . Kociský, S. Y oung, and P . Blunsom. Pitfalls of static language\nmodelling. ArXiv, abs/2102.01951, 2021.\n[14] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy , M. Lewis,\nL. Zettlemoyer, and V . Stoyanov . RoBER T a: A robustly optimized BER T\npretraining approach. ArXiv preprint , abs/1907.11692, 2019.\n[15] J. Pokrywka, F . Grali ´nski, K. Jassem, K. Kaczmarek, K. Jurkiewicz,\nand P . Wierzcho ´n. Challenging America: Modeling language in longer\ntime scales. Findings of North American Chapter of the Association for\nComputational Linguistics , 2022. forthcoming.\n[16] G. D. Rosin, I. Guy , and K. Radinsky . Time masking for temporal\nlanguage models, 2021.\n[17] P . Röttger and J. Pierrehumbert. T emporal adaptation of BER T and per-\nformance on downstream document classiﬁcation: Insights from social\nmedia. In Findings of the Association for Computational Linguistics:\nEMNLP 2021 , pages 2400–2412, Punta Cana, Dominican Republic,\n2021. Association for Computational Linguistics.\n[18] T . Szymanski and G. Lynch. UCD : Diachronic text classiﬁcation\nwith character, word, and syntactic n-grams. In Proceedings of the\n9th International W orkshop on Semantic Evaluation (SemEval 2015) ,\npages 879–883, Denver, Colorado, 2015. Association for Computational\nLinguistics.\n[19] M. Zhang and E. Choi. SituatedQA: Incorporating extra-linguistic\ncontexts into QA. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages 7371–7387, Online and\nPunta Cana, Dominican Republic, 2021. Association for Computational\nLinguistics.\n128 PROCEEDINGS OF THE FEDCSIS. SOFIA, BULGARIA, 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7551994323730469
    },
    {
      "name": "Natural language processing",
      "score": 0.612849235534668
    },
    {
      "name": "Artificial intelligence",
      "score": 0.537636935710907
    },
    {
      "name": "Transformer",
      "score": 0.48510634899139404
    },
    {
      "name": "Language model",
      "score": 0.4191240668296814
    },
    {
      "name": "Engineering",
      "score": 0.09873819351196289
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}