{
  "title": "Leveraging Affective Bidirectional Transformers for Offensive Language Detection",
  "url": "https://openalex.org/W3089015836",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5025365353",
      "name": "AbdelRahim Elmadany",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5058829071",
      "name": "Chiyu Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004629670",
      "name": "Muhammad Abdul-Mageed",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002723792",
      "name": "Azadeh Hashemi",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3013027210",
    "https://openalex.org/W2906589290",
    "https://openalex.org/W2913474415",
    "https://openalex.org/W2340954483",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2901224794",
    "https://openalex.org/W91442942",
    "https://openalex.org/W2747187574",
    "https://openalex.org/W2962993339",
    "https://openalex.org/W1492737170",
    "https://openalex.org/W3122027301",
    "https://openalex.org/W3103061166",
    "https://openalex.org/W3088774333",
    "https://openalex.org/W2913044599",
    "https://openalex.org/W2984315699",
    "https://openalex.org/W2092313193",
    "https://openalex.org/W2912102236",
    "https://openalex.org/W2012437271",
    "https://openalex.org/W2898401058",
    "https://openalex.org/W80056832",
    "https://openalex.org/W2982696058",
    "https://openalex.org/W2736566371",
    "https://openalex.org/W2963748066",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W2962797668"
  ],
  "abstract": "Social media are pervasive in our life, making it necessary to ensure safe online experiences by detecting and removing offensive and hate speech. In this work, we report our submission to the Offensive Language and hate-speech Detection shared task organized with the 4th Workshop on Open-Source Arabic Corpora and Processing Tools Arabic (OSACT4). We focus on developing purely deep learning systems, without a need for feature engineering. For that purpose, we develop an effective method for automatic data augmentation and show the utility of training both offensive and hate speech models off (i.e., by fine-tuning) previously trained affective models (i.e., sentiment and emotion). Our best models are significantly better than a vanilla BERT model, with 89.60% acc (82.31% macro F1) for hate speech and 95.20% acc (70.51% macro F1) on official TEST data.",
  "full_text": "Leveraging Affective Bidirectional Transformers for Offensive Language\nDetection\nAbdelRahim Elmadany, Chiyu Zhang, Muhammad Abdul-Mageed, Azadeh Hashemi\n{a.elmadany,muhammad.mageeed,azadeh.hashemi}@ubc.ca, chiyuzh@mail.ubc.ca\nNatural Language Processing Lab\nUniversity of British Columbia\nAbstract\nSocial media are pervasive in our life, making it necessary to ensure safe online experiences by detecting and removing offensive and\nhate speech. In this work, we report our submission to the Offensive Language and hate-speech Detection shared task organized with the\n4th Workshop on Open-Source Arabic Corpora and Processing Tools Arabic (OSACT4). We focus on developing purely deep learning\nsystems, without a need for feature engineering. For that purpose, we develop an effective method for automatic data augmentation\nand show the utility of training both offensive and hate speech models off (i.e., by ﬁne-tuning) previously trained affective models (i.e.,\nsentiment and emotion). Our best models are signiﬁcantly better than a vanilla BERT model, with 89.60% acc (82.31% macro F1) for\nhate speech and 95.20% acc (70.51% macro F1) on ofﬁcial TEST data.\n1. Introduction\nSocial media are widely used at a global scale. Com-\nmunication between users from different backgrounds,\nideologies, preferences, political orientations, etc. on\nthese platforms can result in tensions and use of offensive\nand hateful speech. This negative content can be very\nharmful, sometimes with real-world consequences. For\nthese reasons, it is desirable to control this type of uncivil\nlanguage behavior by detecting and removing this destruc-\ntive content.\nAlthough there have been a number of works on detecting\noffensive and hateful content in English (e.g. (Agrawal and\nAwekar, 2018; Badjatiya et al., 2017; Nobata et al., 2016)),\nworks on many other languages are either lacking or rare.\nThis is the case for Arabic, where there have been only\nvery few works (e.g., (Alakrot et al., 2018; Albadi et al.,\n2018; Mubarak et al., 2017; Mubarak and Darwish, 2019)).\nFor these motivations, we participated in the Offensive\nLanguage and hate-speech Detection shared task organized\nwith the 4 th Workshop on Open-Source Arabic Corpora\nand Processing Tools Arabic (OSACT4).\nOffensive content and hate speech are less frequent online\nthan civil, acceptable communication. For example, only\n19% and ∼ 5% of the released shared task data are of-\nfensive and hate speech, respectively. This is the case in\nspite of the fact that the data seems to have been collected\nbased on trigger seeds that are more likely to accompany\nthis type of harmful content. As such, it is not easy to ac-\nquire data for training machine learning systems. For this\nreason, we direct part of our efforts to automatically aug-\nmenting training data released by the shared task organizers\n(Section 3.1.). Our experiments show the utility of our data\nenrichment method. In addition, we hypothesize trained af-\nfective models can have useful representations that might\nbe effective for the purpose of detecting offensive and hate-\nful content. To test this hypothesis, we ﬁne-tune one senti-\nment analysis model and one emotion detection model on\nour training data. Our experiments support our hypothesis\n(Section 4.). All our models are based on the Bidirectional\nEncoder from Transformers (BERT) model. Our best mod-\nels are signiﬁcantly better than competitive baseline based\non vanilla BERT. Our contributions can be summarized as\nfollows:\n• We present an effective method for automatically aug-\nmenting training data. Our method is simple and\nyields sizable additional data when we run it on a large\nin-house collection.\n• We demonstrate the utility of ﬁne-tuning off-the-shelf\naffective models on the two downstream tasks of of-\nfensive and hate speech.\n• We develop highly accurate deep learning models for\nthe two tasks of offensive content and hate speech de-\ntection.\nThe rest of the paper is organized as follows: We intro-\nduce related works in Section 2., shared task data and our\ndatasets in Section 3., our models in Section 4., and we\nconclude in Section 5..\n2. Related Work\nThematic Focus: Research on undesirable content shows\nthat social media users sometimes utilize profane, obscene,\nor offensive language (Jay and Janschewitz, 2008; Wiegand\net al., 2018); aggression (Kumar et al., 2018; Modha et al.,\n2018); toxic content (Georgakopoulos et al., 2018; Fortuna\net al., 2018; Zampieri et al., 2019), and bullying (Dadvar\net al., 2013; Agrawal and Awekar, 2018; Fortuna et al.,\n2018).\nOverarching Applications: Several works have taken\nas their target detecting these types of negative content\nwith a goal to build applications for (1) content ﬁltering\nor (2) quantifying the intensity of polarization (Barber ´a\nand Sood, 2015; Conover et al., 2011), (3) classifying\ntrolls and propaganda accounts that often use offensive\nlanguage (Darwish et al., 2017), (4) identifying hate\nspeech that may correlate with hate crimes (Nobata et al.,\n2016), and (5) detecting signals of conﬂict, which are often\narXiv:2006.01266v1  [cs.CL]  16 May 2020\nDataset #tweets # NOTOFF # OFF OFF% # NOTHS # HS HS %\nShard-task data TRAIN 6994 5585 1409 20% 6633 361 5%\nDEV 1000 821 179 18% 956 44 4%\nTEST 2000 - - - - - -\nAugmented data AUG-TRAIN-HS 209780 - - - 199291 10489 5%\nAUG-TRAIN-OFF 480777 215365 265413 55% - - -\nTable 1: Offensive (OFF) and Hate Speech (HS) Labels distribution in datasets\npreceded by verbal hostility (Chadefaux, 2014).\nMethods: A manual way for detecting negative language\ncan involve building a list of offensive words and then\nﬁltering text based on these words. As Mubarak and\nDarwish (2019) also point out, this approach is limited\nbecause (1) offensive words are ever evolving with new\nwords continuously emerging, complicating the mainte-\nnance of such lists and (2) the offensiveness of certain\nwords is highly context- and genre-dependent and hence a\nlexicon-based approach will not be very precise. Machine\nlearning approaches, as such, are much more desirable\nsince they are more nuanced to domain and also usually\nrender more accurate, context-sensitive predictions. This is\nespecially the case if there are enough data to train these\nsystems.\nMost work based on machine learning employs a super-\nvised approach at either (1) character level (Malmasi and\nZampieri, 2017), (2) word level (Kwok and Wang, 2013),\nor (3) simply employ some representation incorporating\nword embeddings (Malmasi and Zampieri, 2017). These\nstudies use different learning methods, including Naive\nBayes (Kwok and Wang, 2013), SVMs (Malmasi and\nZampieri, 2017), and classical deep learning such as\nCNNs and RNNs (Nobata et al., 2016; Badjatiya et\nal., 2017; Alakrot et al., 2018; Agrawal and Awekar,\n2018). Accuracy of the aforementioned systems range\nbetween 76% and 90%. It is also worth noting that\nsome earlier works (Weber et al., 2013) use sentiment\nwords as features to augment other contextual features.\nOur work has afﬁnity to this last category since we also\nleverage affective models trained on sentiment or emotion\ntasks. Our approach, however, differs in that we build\nmodels free of hand-crafted features. In other words, we\nlet the model learn its representation based on training\ndata. This is a characteristic attribute of deep learning\nmodels in general. 1 In terms of the speciﬁc information\nencoded in classiﬁers, researchers use proﬁle information\nin addition to text-based features. For example, Abozi-\nnadah (2017) apply SVMs on 31 features extracted from\nuser proﬁles in addition to social graph centrality measures.\nMethodologically, our work differs in three ways: (1) we\ntrain offensive and hate speech models off affective models\n(i.e., we ﬁne-tune already trained sentiment and emotion\nmodels on both the offensive and hate speech tasks). (2)\n1Of course hand-crafted features can also be added to a repre-\nsentation fed into a deep learning model. However, we do not do\nthis here.\nWe apply BERT language models on these two tasks. We\nalso (3) automatically augment offensive and hate speech\ntraining data using a simple data enrichment method.\nArabic Offensive Content: Very few works have been\napplied to the Arabic language, focusing on detecting\noffensive language. For example, (Mubarak et al., 2017)\ndevelop a list of obscene words and hashtags using patterns\ncommon in offensive and rude communications to label\na dataset of 1,100 tweets. Mubarak and Darwish (2019)\napplied character n-gram FasText model on a large dataset\n(3.3M tweets) of offensive content. Our work is similar\nto Mubarak and Darwish (2019) in that we also automati-\ncally augment training data based on an initial seed lexicon.\n3. Data\nIn our experiments, we use two types of data: (1) data\ndistributed by the Offensive Language Detection shared\ntask and (2) an automatically collected dataset that we\ndevelop (Section 3.1.). The shared task dataset comprises\n10,000 tweets manually annotated for two sub-tasks: of-\nfensiveness (Sub task A) 2 and hate speech (Sub task B) 3.\nAccording to shared task organizers, 4, offensive tweets\nin the data contain explicit or implicit insults or attacks\nagainst other people, or inappropriate language. Organizers\nalso maintain that hate speech tweets contains insults or\nthreats targeting a speciﬁc group of people based on the\nnationality, ethnicity, gender, political or sport afﬁlia-\ntion, religious belief, or other common characteristics\nof such a group. The dataset is split by shared task\norganizers into 70% TRAIN, 10% DEV , and 20% TEST.\nBoth labeled TRAIN and DEV splits were shared with\nparticipating teams, while tweets of TEST data (without la-\nbels) was only released brieﬂy before competition deadline.\nIt is noteworthy that the dataset is imbalanced. For\noffensiveness (Sub task A), only 20% of the TRAIN split\nare labeled as offensive and the rest is not offensive.\nFor hate speech (Sub task B), only 5% of the tweets\nare annotated as hateful. Due to this imbalanced, the\nofﬁcial evaluation metric is macro F1 score. Table 1 shows\nthe size and label distribution in the shared task data splits.5\n2https://competitions.codalab.org/\ncompetitions/22825.\n3https://competitions.codalab.org/\ncompetitions/22826\n4http://edinburghnlp.inf.ed.ac.uk/\nworkshops/OSACT4/.\n5Table 1 also shows size and class distribution for our auto-\nThe following are example tweets from the shared task\nTRAIN split.\nExamples ofoffensive and hateful tweets:\n1) \tà@ YgB@ ÐñK \n \u0010m\u001a'. Yg\r@ AK\n Yg@ð AK\n H. P AK\n. @ñ» PA\u0011 \táÒJ\nË@ ÈA \t®£@ Ég. B . \tá\u001e\nÓQj. ÖÏ@ Xñª ú \n \tæK. ½Êî\u0010E\nOh my Lord, O One and Only, destroy the family of\nSau‘d, for they are the criminals who put children of\nYemen to suffer.6\n2) ú\næ\t\u001dQ\t®Ë@ PAÒª \u0010JB@ \u0010HC \t\t¯ AK\n ú\n\tGA \tJJ.Ë AK\n\u0010H@QëA« Ñî \tE@ñ\t\u001d \tàñÊ \tª \u0011\u001d\n i. J\nÊ\tmÌ'AK. ú\næ\n\tKA \tJJ.ÊË@\nHey, you Lebanese guy, you’re the wastes of the\nFrench colonizers. The Lebanese in the Gulf put their\nwomen in prostitution work.\nExamples foroffensive but not hate-speechtweets:\n3) Aî \tE@\r ÕºK. P @ðYÔ g\r@.. Q \u0010KA AK\n .. \t­J\n¢Ë AK\né \t®\u0010¯@ð Aî \tE@\r ñË \t­J\n» \u0010èYª \u0010®\u0010JÓ\nOh my lord... Thank God she has disability. What\nwould have happened if she were not disabled?\n4) \u0010I\tK@ ñ \tJ \tàñk\u0012 AK\n éK\n@ A \tJÊJ\nJ. \tm× øQ\u0010K AK\nø\n X ½J. \tJk. ú\nÎË@ \u0010éK. ñJ.Ê¾Ë@ ð\nI wonder what you, and this little pitch by your side,\nare hiding for us, John Snow?\nExamples fornot offensiveand not hate-speechtweets:\n5) \tàñ» \r@ ø\n YK. AÓ AÓ@ \r AK\n Ñë \rB@ ½ \u0010KAJ\nm\u001a'. \tàñºK. AK\nEither I become the most important in your life, or I\nbecome nothing at all.\n6) É«AK\n ¼YK\n ÕÎ\u0010\u001d éJ\nÖÞ\fAK\n @ \tX ñÊm Ì'@ É¿ B@ \u0011\u001d\n@\nú\næ\u0011 É¿ AK \n é\tKA \tJ \t¯ AK\n é \tkAJ.£ AK\n èQº \fAK\n èñÊgAK\n é¢ \u0011\u0010¯AK\nWow! How wonderful this food is, Sumaia! You’re\nsuch a honey, beauty, sweetie, and good cook! You’re\nare artist! You’re everything!\n3.1. Data Augmentation\nAs explained earlier, the positive class in the offensive\nsub-task (i.e., the category ‘offensive’) is only 20% and\nin the hateful sub-task (i.e., the class ‘hateful’) it is only\nmatically extracted dataset, to which we refer to as augmented\n(AUG).\n6Original tweets can be run-on sentences, lack proper gram-\nmatical structures or punctuation. In presented translation, for\nreadability, while we maintain the meaning as much as possible,\nwe render grammatical, well-structured sentence.\n5%. Since our goal is to develop exclusively deep learning\nmodels, we needed to extend our training data such that\nwe increase the positive samples. For this reason, we\ndevelop a simple method to automatically augment our\ntraining data. Our method ﬁrst depends on extracting\ntweets that contain any of a seed lexicon (explained below)\nand satisfy a predicted sentiment label condition. We\nhypothesize that both offensive and hateful content would\ncarry negative sentiment and so it would be intuitive to\nrestrict any automatically extracted tweets to those that\ncarry these negative sentiment labels. To further test this\nhypothesis, we analyzing the distribution of the sentiment\nclasses in the TRAIN split using an off-the-shelf tool,\nAraNet (Abdul-Mageed et al., 2020). As shown in Figure\n3, AraNet assigns sensible sentiment labels to the data.\nFor the ‘offensive’ class, the tool assigns 65% negative\nsentiment tags and for the non-offensive class it assigns\nonly 60% positive sentiment labels. 7 For the hate speech\ndata, we ﬁnd that AraNet assigns 72% negative labels to\nthe ‘hateful’ class and 55% positive sentiment labels for\nthe ‘non-hateful’ class. Based on this analysis, we decide\nto impose a sentiment-label condition on the automatically\nextended data as explained earlier. In other words, we only\nchoose ‘offensive’ and ‘hateful’ class data from tweets\npredicted as negative sentiment. Similarly, we only choose\n‘non-offensive’ and ‘non-hateful’ tweets assigned positive\nsentiment labels by AraNet. We now explain how we\nextend the dataset. We now explain our approach to extract\ntweets with an offensive and hateful seed lexicon.\nTo generate a seed lexicon, we extract all words that follow\nthe Ya (Oh, you ) in the shared task TRAIN split positive\nclass in the two sub-tasks (i.e., ‘offensive’ and ‘hateful’).\nThe intuition here is that the word Ya acts as a trigger word\nthat is likely to be followed by negative lexica. This gives\nus a set of 2,158. We ﬁnd that this set can have words that\nare neither offensive nor hateful outside context and so we\nmanually select a smaller set of 352 words that we believe\nare much more likely to be effective offensive seeds and\nonly 38 words that we judge as more suitable carriers of\nhateful content. Table 2 shows samples of the offensive\nand hateful seeds. Table 3 shows examples of seeds in\nour initial larger set that we ﬁltered out since these are\nless likely to carry negative meaning (whether offensive or\nhateful).\nTo extend the offensive and hateful tweets, we use 500K\nrandomly sampled, unlabeled, tweets from (Abdul-Mageed\net al., 2019) that each have at least one occurrence of the\ntrigger word Ya and at least one occurrence of a word\nfrom either of our two seed lexica (i.e., the offensive and\nhateful seeds). 8 We then apply AraNet (Abdul-Mageed\net al., 2020) on this 500K collection and keep only\n7AraNet (Abdul-Mageed et al., 2020) assigns only positive and\nnegative sentiment labels. In other words, it does not assign neu-\ntral labels.\n8The 500K collection is extracted via searching a larger sam-\nple of ∼ 21M tweets that all have the trigger wordYa. This corpus\nis also taken from (Abdul-Mageed et al., 2019). Note that a tweet\ncan have both an offensive and a hateful seed.\nArabic Offensive English Arabic Hateful English\n\u0011I\u001c\nJ\n \tk AK\n You, fat ass! ø\n ðAj. \tJÓ AK\n You’re Manjawi\n¨A«P AK\n You’re mobby ø\n @ðPY \tKX AK\n You’re Dandarawi\nXQå\u0011\u0010JÓ AK\n You’re a tramp \tá\u001e\nK\nXñª AK\n You Saudis\n\tá\u001e\n\tKAm.× AK\n You’re crazy ú\næ\u0011AJ.kX AK\n You’re Dahbashi\n\tàAªk. AK\n You, hungry man! ù\n \rKA«X@ AK \n You, false claimer\nèQk. A\t¯ AK\n You, morally loose ú\n\u0011Gñk AK\n You, Houthi\nÈAÖÞ\u0011 AK\n Oh, whore ù\n ªJ\n \u0011 AK\n You, Shiite\néËAK. \tP AK\n You, junky ÉJ\nÔ« AK\n You, spay\nÕç'\n@AîE. AK\n You, animals ú\nm.\u001a\t'@ñ \tk@ AK\n You, Ikhwangis\n\tJ\n \tªK. AK\n You, hateful \tà@ñ \tk@ AK\n You, Ikhwan\né\tm\u0019ð AK\n You, dirty woman ©J\nK. @Qm.Ì'@ \táK. @ AK\n You, son of tramps\néJ\n \t«A£ AK\n You, tyrant Ð@QmÌ'@ \táK. @ AK\n You, bastard\nQk. A\t¯ AK\n You, salacious A\tK \tQË@ \táK. @ AK\n You, bastard\nÉ \t® \tªÓ AK\n You, idiot éK\nXñîD\nË@ \táK. @ AK\n You, son of Jewish woman\néÔ \tgP AK\n You, silly woman éJ\n\tK@ \tQË@ \táK. @ AK\n You, son of adulterous woman\nm\u001a\t' AK\n You, sinister éÓñëñÖÏ@ \táK. @ AK\n You, son of deceived woman\nZAJ. \t« AK\n You, stupid head QªË@ \táK. @ AK\n You, son of pimp\nI. \u001c\n\rJ» AK\n You, gloomy head é» A\tJ\u0010JÖÏ@ \táK. @ AK\n You, son of adulterous\n@QÓ AK\n You, unworthy woman \u0010H@PAÓ@ AK \n You, Emirate\nù\n\u0010®Ôg AK\n You, fools ø\n XAm\u001a\u0010'@ AK\n You, Itihadi\nTable 2: Examples of offensive and hateful seeds in our lexica\ntweets assigned negative sentiment labels. Tweets that\ncarry offensive seeds are labeled as ‘offensive’ and those\ncarrying hateful seeds are tagged as ‘hateful’. This gives\nus 265,413 offensive tweets and 10,489 hateful tweets.\nFor reference, the majority (%=67) of the collection\nextracted with our seed lexicon are assigned negative\nsentiment labels by AraNet. This reﬂects the effectiveness\nof our lexicon as it matches our observations about the dis-\ntribution of sentiment labels in the shared task TRAIN split.\nTo add positive class data (i.e., ‘not-offensive’ and ‘not-\nhateful’) to this augmented collection, we randomly sample\nanother 500K tweets that carry Ya from (Abdul-Mageed\net al., 2019) that do not carry any of the two offensive\nand hateful seed lexica. We apply AraNet on these tweets\nand keep only tweets assigned a positive sentiment label\n(%=70). We use 215,365 tweets as ‘non-offensive’ but\nonly 199,291 as ‘non-hateful’.9 Table 1 shows the size and\ndistribution of class labels in our extended dataset.\nFigure 2 and Figure 1 are word clouds of unigrams in our\nextended training data (offensive and hateful speech, re-\nspectively) after we remove our seed lexica from the data.\nThe clouds show that the data carries lexical cues likely to\noccur in each of the two classes (offensive and hateful). Ex-\namples of frequent words in the offensive class includedog,\n9We decided to keep only 199,291 ‘non-hateful’ tweets since\nour augmented ‘hateful’ class comprises only 10,489 tweets.\nanimal, son of, mother, dirty woman, monster, mad, and on\nyou. Examples in the hateful data includeshut up, dogs, son\nof, animal, dog, haha, and for this reason. We note that the\nhateful words do not include direct names of groups since\nthese were primarily our seeds that we removed before we\nprepare the word cloud. Overall, the clouds provide sensi-\nble cues of our phenomena of interest across the two tasks.\nFigure 1: A word cloud of unigrams in our extended train-\ning offensive data (AUG-TRAIN-OFF).\n4. Models\n4.1. Data Pre-Processing\nWe perform light Twitter-speciﬁc data cleaning (e.g.,\nreplacing numbers, usernames, hashtags, and hyperlinks\nArabic Non-OFF/Non-HS English\nÈA¢\u001d. @ AK\n You, heros\n\tàA \tJ \t¯ AK\n You, artist\n©J\nÒ\u001c. « AK\n You, Absemee’\nÕË@A« AK \n Oh, people\nú\næ\tJÓ AK\n Oh, forgotten man\n¼Y \tJ«AÓ AK\n You have a lot\nAÓAÓ AK\n Oh, mum\nPQÔ\u0010¯ AK\n You, beautiful lady\nYÓAg. AK\n You, wonderful man\nÉ \t®£ AK\n Oh, child\né¢\u0010¯ AK\n Oh, delicate lady\nAî \u0010DÊJ\nk AK\n You, lulled\nAî \u0010DK\n@ AK\n Oh you. . .\nú\n«@P AK\n You, caregiver\nú\næ.J\nJ.k AK\n Oh, darling\nH. P AK\n Oh, my Lord\nYg@ð AK\n Oh, the One\nA \tK AK\n You, people\nQ \tk@ AK\n Oh, the Last\nAK. AK. AK\n Oh, daddy\nTable 3: Examples of non-offensive/non-hateful seeds ﬁl-\ntered out from our lexica.\nFigure 2: A word cloud of unigrams in our extended train-\ning hate speech data (AUG-TRAIN-HS).\nby unique tokens NUM, USER, HASH, and URL respec-\ntively). We also perform Arabic-speciﬁc normalization\n(e.g., removing diacritics and mapping various forms of\nAlef and Yeh each to a canonical form). For text tokeniza-\ntion, we use byte-pair encoding (PBE) as implemented in\nMultilingual Cased BERT model.\n4.2. BERT\nOur experiments are based on BERT-Base Multilingual\nCased model released by (Devlin et al., 2018) 10. BERT\nstands for Bidirectional Encoder Representations from\n10https://github.com/google-research/bert/\nblob/master/multilingual.md.\nFigure 3: Distribution of Negative and Positive Tweets after\napplied AraNet on Shared-Task TRAIN Data\nTransformers. It is an approach for language modeling that\ninvolves two self-supervised learning tasks, (1) masked\nlanguage models (MLM) and (2) next sentence predication\n(NSP). BERT is equipped with an Encoder architecture\nwhich naturally conditions on bi-directional context.\nIt randomly masks a given percentage of input tokens\nand attempts to predict these masked tokens. (Devlin\net al., 2018) mask 15% of the tokens (the authors use\nword pieces ) and use the hidden states of these masked\ntokens from last layer for prediction. To understand\nthe relationship between two sentences, the BERT also\npre-trains with a binarized NSP task, which is also a type\nof self-supervises learning. For the sentence pairs (e.g.,\nA-B) in pre-training examples, 50% of the time B is the\nactual next sentence that follows A in the corpus (positive\nclass) and 50% of the time B is a random sentence from\ncorpus (negative class). Google’s pre-trained BERT-Base\nMultilingual Cased model is trained on 104 languages\n(including Arabic) with 12 layers, 768 hidden units each,\n12 attention heads. The model has 119,547 shared word\npieces vocabulary, and was pre-trained on the entire\nWikipedia for each language.\nIn our experiments, we train our classiﬁcation models\non BERT-Base Multilingual Cased model. For all of our\nﬁne-tuning BERT models, we use a maximum sequence\nsize of 50 tokens and a batch size of 32. We add a ‘[CLS]’\ntoken at the beginning of each input sequence and, then,\nDev Test\nOFF HS OFF HS\nModel Acc F1 Acc F1 Acc F1 Acc F1\nBERT 87.10 78.38 95.70 70.96 87.30 77.70 95.20 70.51\nBERT-SENTI 87.40 78.84 95.50 68.01 87.45 80.51 93.15 61.57\nBERT-EMO 88.30 80.39 95.40 68.54 – – – –\nBERT-EMO-AUG 89.60 82.31 93.90 62.52 89.35 82.85 – –\nTable 4: Offensive (OFF) and Hate Speech (HS) results on DEV and TEST datasets\nfeed the ﬁnal hidden state of ‘[CLS]’ to a Softmax linear\nlayer to get predication probabilities across classes. We set\nthe learning rate to 2e −6 and train for 20 epochs. We save\nthe checkpoint at the end of each epoch, report F1-score\nand accuracy of the best model, and use the best checkpoint\nto predict the labels of the TEST set. We ﬁne-tune the\nBERT model under ﬁve settings. We describe each of these\nnext.\nVanilla BERT: We ﬁne-tune BERT-Base Multilingual\nCased model on TRAIN set of offensive task and hate\nspeech task respectively. We refer these two models to\nBERT. The offensive model obtains the best result with 8\nepochs. As Table 4 shows, for offensive language classiﬁ-\ncation, this model obtains 87.10% accuracy and 78.38 F1\nscore on DEV set. We submit the TEST prediction of this\nmodel to the shared task and obtain 87.30% accuracy and\n77.70 F1 on the TEST set. The hate speech model obtains\nbest result (accuracy = 95.7-%, F1 = 70.96) with 6 epochs.\nBERT-SENTI We use a BERT model ﬁne-tuned with on\nbinary Arabic sentiment dataset as released by (Abdul-\nMageed et al., 2020). We use this off-the-shelf (already\ntrained) model to further ﬁne-tune on offensive and hate\nspeech tasks, respectively. We replace the Softmax linear\nlayer for sentiment classiﬁcation with a randomly initial-\nized Softmax linear layer for each task. We refer to these\ntwo models as BERT-SENTI. We train the BERT-SENTI\nmodels on the TRAIN sets for offensive and hate speech\ntasks respectively. On F1 score, BERT-SENTI is 0.3 better\nthan vanilla BERT on the offensive task, but 2.95 lower\n(than vanilla BERT) on the hate speech task. We submit\nthe TEST predictions of both tasks. The offensive model\nobtain 87.45% accuracy and 80.51 F1 on TEST. The hate\nspeech model acquire 93.15% accuracy and 61.57 F1 on\nTEST.\nBERT-EMO Similar to BERT-SENTI, we use a BERT\nmodel trained on 8-class Arabic emotion identiﬁcation\nfrom (Abdul-Mageed et al., 2020) to ﬁne-tune on the\noffensive and hate speech tasks, respectively. We refer to\nthis setting as BERT-EMO. We train the models on the\nTRAIN sets for both offensive and hate speech tasks for\n20 epochs. The offensive model obtains its best result\n(accuracy = 88.30%, F1 = 80.39) with 11 epochs. The hate\nspeech model acquires its best result (accuracy = 95.40%,\nF1 = 68.54) also with 11 epochs. We do not submit an\nBERT-EMO on the hate speech task TEST set.\nBERT-EMO-AUG Similar to BERT-EMO, we also ﬁne-\ntune the emotion BERT model (BERT-EMO) with the aug-\nmented offensive dataset (AUG-TRAIN-OFF) and aug-\nmented hate speech dataset (AUG-TRAIN-HS). On the\nDEV set, the offensive model acquires its best result (ac-\ncuracy = 89.60%, F1 = 82.31) with 13 epochs. The best\nresults for the hate speech model (accuracy = 93.90%, F1 =\n62.52) is obtained with 9 epochs. Our best offensive predi-\ncation on TEST is BERT-EMO-AUG. It which achieves an\naccuracy of 89.35% and F1 of 82.85. We do not submit an\nBERT-EMO-AUG on the hate speech task TEST set.\n5. Conclusion\nWe described our submission to the offensive language de-\ntection in Arabic shared task. We offered a simple method\nto extend training data and demonstrated the utility of such\naugmented data empirically. We also deploy affective lan-\nguage models on the two sub-tasks of offensive language\ndetection and hate speech identiﬁcation. We show that ﬁne-\ntuning such affective models is useful, especially in the case\nof offensive language detection. In the future, we will in-\nvestigate other methods for improving our automatic offen-\nsive and hateful language acquisition methods. We also ex-\nplore other machine learning methods on the tasks. For ex-\nample, we plan to investigate the utility of semi-supervised\nmethods as a vehicle of improving our models.\n6. Bibliographic References\nAbdul-Mageed, M., Zhang, C., Elmadany, A., Rajendran,\nA., and Ungar, L. (2019). Dianet: Bert and hierarchi-\ncal attention multi-task learning of ﬁne-grained dialect.\narXiv preprint arXiv:1910.14243.\nAbdul-Mageed, Muhammad, Z. C., Nagoudi, E. M. B., and\nHashemi, A. (2020). Aranet: A deep learning toolkit\nfor arabic social media. In The 4th Workshop on Open-\nSource Arabic Corpora and Processing Tools (OSACT4),\nLREC.\nAbozinadah, E. (2017). Detecting abusive arabic lan-\nguage twitter accounts using a multidimensional anal-\nysis model. Ph.D. thesis.\nAgrawal, S. and Awekar, A. (2018). Deep learning for de-\ntecting cyberbullying across multiple social media plat-\nforms. In European Conference on Information Re-\ntrieval, pages 141–153. Springer.\nAlakrot, A., Murray, L., and Nikolov, N. S. (2018). To-\nwards accurate detection of offensive language in online\ncommunication in arabic. Procedia computer science ,\n142:315–320.\nAlbadi, N., Kurdi, M., and Mishra, S. (2018). Are they our\nbrothers? analysis and detection of religious hate speech\nin the arabic twittersphere. In 2018 IEEE/ACM Interna-\ntional Conference on Advances in Social Networks Anal-\nysis and Mining (ASONAM), pages 69–76. IEEE.\nBadjatiya, P., Gupta, S., Gupta, M., and Varma, V . (2017).\nDeep learning for hate speech detection in tweets. In\nProceedings of the 26th International Conference on\nWorld Wide Web Companion, pages 759–760.\nBarber´a, P. and Sood, G. (2015). Follow your ideology:\nMeasuring media ideology on social networks. In An-\nnual Meeting of the European Political Science Associa-\ntion, Vienna, Austria. Retrieved from http://www. gsood.\ncom/research/papers/mediabias. pdf.\nChadefaux, T. (2014). Early warning signals for war in the\nnews. Journal of Peace Research, 51(1):5–18.\nConover, M. D., Ratkiewicz, J., Francisco, M., Gonc ¸alves,\nB., Menczer, F., and Flammini, A. (2011). Political po-\nlarization on twitter. In Fifth international AAAI confer-\nence on weblogs and social media.\nDadvar, M., Trieschnigg, D., Ordelman, R., and de Jong,\nF. (2013). Improving cyberbullying detection with user\ncontext. In European Conference on Information Re-\ntrieval, pages 693–696. Springer.\nDarwish, K., Alexandrov, D., Nakov, P., and Mejova, Y .\n(2017). Seminar users in the arabic twitter sphere. In In-\nternational Conference on Social Informatics, pages 91–\n108. Springer.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2018). Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805.\nFortuna, P., Ferreira, J., Pires, L., Routar, G., and Nunes, S.\n(2018). Merging datasets for aggressive text identiﬁca-\ntion. In Proceedings of the First Workshop on Trolling,\nAggression and Cyberbullying (TRAC-2018), pages 128–\n139.\nGeorgakopoulos, S. V ., Tasoulis, S. K., Vrahatis, A. G.,\nand Plagianakos, V . P. (2018). Convolutional neural\nnetworks for toxic comment classiﬁcation. In Proceed-\nings of the 10th Hellenic Conference on Artiﬁcial Intelli-\ngence, pages 1–6.\nJay, T. and Janschewitz, K. (2008). The pragmatics of\nswearing. Journal of Politeness Research. Language,\nBehaviour, Culture, 4(2):267–288.\nKumar, R., Ojha, A. K., Malmasi, S., and Zampieri, M.\n(2018). Benchmarking aggression identiﬁcation in so-\ncial media. In Proceedings of the First Workshop on\nTrolling, Aggression and Cyberbullying (TRAC-2018) ,\npages 1–11.\nKwok, I. and Wang, Y . (2013). Locate the hate: Detecting\ntweets against blacks. In Twenty-seventh AAAI confer-\nence on artiﬁcial intelligence.\nMalmasi, S. and Zampieri, M. (2017). Detect-\ning hate speech in social media. arXiv preprint\narXiv:1712.06427.\nModha, S., Majumder, P., and Mandl, T. (2018). Filtering\naggression from the multilingual social media feed. In\nProceedings of the First Workshop on Trolling, Aggres-\nsion and Cyberbullying (TRAC-2018), pages 199–207.\nMubarak, H. and Darwish, K. (2019). Arabic offensive\nlanguage classiﬁcation on twitter. In International Con-\nference on Social Informatics, pages 269–276. Springer.\nMubarak, H., Darwish, K., and Magdy, W. (2017). Abu-\nsive language detection on arabic social media. In Pro-\nceedings of the First Workshop on Abusive Language On-\nline, pages 52–56.\nNobata, C., Tetreault, J., Thomas, A., Mehdad, Y ., and\nChang, Y . (2016). Abusive language detection in online\nuser content. In Proceedings of the 25th international\nconference on world wide web, pages 145–153.\nWeber, I., Garimella, V . R. K., and Batayneh, A. (2013).\nSecular vs. islamist polarization in egypt on twitter. In\nProceedings of the 2013 IEEE/ACM international con-\nference on advances in social networks analysis and min-\ning, pages 290–297.\nWiegand, M., Siegel, M., and Ruppenhofer, J. (2018).\nOverview of the germeval 2018 shared task on the iden-\ntiﬁcation of offensive language.\nZampieri, M., Malmasi, S., Nakov, P., Rosenthal, S., Farra,\nN., and Kumar, R. (2019). Semeval-2019 task 6: Identi-\nfying and categorizing offensive language in social me-\ndia (offenseval). arXiv preprint arXiv:1903.08983.",
  "topic": "Offensive",
  "concepts": [
    {
      "name": "Offensive",
      "score": 0.9647761583328247
    },
    {
      "name": "Computer science",
      "score": 0.7181341648101807
    },
    {
      "name": "Macro",
      "score": 0.6433345079421997
    },
    {
      "name": "Natural language processing",
      "score": 0.5407227873802185
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5019466876983643
    },
    {
      "name": "Focus (optics)",
      "score": 0.4733980894088745
    },
    {
      "name": "Arabic",
      "score": 0.46226727962493896
    },
    {
      "name": "Transformer",
      "score": 0.46203282475471497
    },
    {
      "name": "Speech recognition",
      "score": 0.45830583572387695
    },
    {
      "name": "Sentiment analysis",
      "score": 0.4300597012042999
    },
    {
      "name": "Linguistics",
      "score": 0.2034604251384735
    },
    {
      "name": "Engineering",
      "score": 0.13073179125785828
    },
    {
      "name": "Voltage",
      "score": 0.08079802989959717
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operations research",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}