{
  "title": "Context Matters: A Strategy to Pre-train Language Model for Science Education",
  "url": "https://openalex.org/W4318751300",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101505879",
      "name": "Zhengliang Liu",
      "affiliations": [
        "University of Georgia"
      ]
    },
    {
      "id": "https://openalex.org/A5101756147",
      "name": "Xinyu He",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102875075",
      "name": "Lei Liu",
      "affiliations": [
        "Educational Testing Service",
        "University of Georgia"
      ]
    },
    {
      "id": "https://openalex.org/A5100647156",
      "name": "Tianming Liu",
      "affiliations": [
        "University of Georgia"
      ]
    },
    {
      "id": "https://openalex.org/A5013379229",
      "name": "Xiaoming Zhaı",
      "affiliations": [
        "University of Georgia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226315886",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4283773002",
    "https://openalex.org/W2885388515",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W3012071850",
    "https://openalex.org/W4312318847",
    "https://openalex.org/W2303518126",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3091828467",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2563509697",
    "https://openalex.org/W4285601618"
  ],
  "abstract": "This study aims at improving the performance of scoring student responses in science education automatically. BERT-based language models have shown significant superiority over traditional NLP models in various language-related tasks. However, science writing of students, including argumentation and explanation, is domain-specific. In addition, the language used by students is different from the language in journals and Wikipedia, which are training sources of BERT and its existing variants. All these suggest that a domain-specific model pre-trained using science education data may improve model performance. However, the ideal type of data to contextualize pre-trained language model and improve the performance in automatically scoring student written responses remains unclear. Therefore, we employ different data in this study to contextualize both BERT and SciBERT models and compare their performance on automatic scoring of assessment tasks for scientific argumentation. We use three datasets to pre-train the model: 1) journal articles in science education, 2) a large dataset of students' written responses (sample size over 50,000), and 3) a small dataset of students' written responses of scientific argumentation tasks. Our experimental results show that in-domain training corpora constructed from science questions and responses improve language model performance on a wide variety of downstream tasks. Our study confirms the effectiveness of continual pre-training on domain-specific data in the education domain and demonstrates a generalizable strategy for automating science education tasks with high accuracy. We plan to release our data and SciEdBERT models for public use and community engagement.",
  "full_text": "Context Matters: A Strategy to Pre-train\nLanguage Model for Science Education\nZhengliang Liu1∗, Xinyu He1∗, Lei Liu2, Tianming Liu1∗∗, and Xiaoming\nZhai1∗∗\n1 University of Georgia, Athens, GA 30666, USA\n2 Educational Testing Service, Princeton, NJ, USA\n* Co-First Author\n** Corresponding Authors:\ntliu@uga.edu\nxiaoming.zhai@uga.edu\nAbstract. This study aims at improving the performance of scoring\nstudent responses in science education automatically. BERT-based lan-\nguage models have shown signiﬁcant superiority over traditional NLP\nmodels in various language-related tasks. However, science writing of\nstudents, including argumentation and explanation, is domain-speciﬁc.\nIn addition, the language used by students is diﬀerent from the language\nin journals and Wikipedia, which are training sources of BERT and its\nexisting variants. All these suggest that a domain-speciﬁc model pre-\ntrained using science education data may improve model performance.\nHowever, the ideal type of data to contextualize pre-trained language\nmodel and improve the performance in automatically scoring student\nwritten responses remains unclear. Therefore, we employ diﬀerent data\nin this study to contextualize both BERT and SciBERT models and\ncompare their performance on automatic scoring of assessment tasks for\nscientiﬁc argumentation. We use three datasets to pre-train the model:\n1) journal articles in science education, 2) a large dataset of students’\nwritten responses (sample size over 50,000), and 3) a small dataset of\nstudents’ written responses of scientiﬁc argumentation tasks. Our exper-\nimental results show that in-domain training corpora constructed from\nscience questions and responses improve language model performance on\na wide variety of downstream tasks. Our study conﬁrms the eﬀectiveness\nof continual pre-training on domain-speciﬁc data in the education do-\nmain and demonstrates a generalizable strategy for automating science\neducation tasks with high accuracy. We plan to release our data and\nSciEdBERT models for public use and community engagement.\n1 Introduction\nWriting is critical in science learning because it is the medium for students to\nexpress their thought processes. In classroom settings, educators have engaged\nstudents in writing explanations of phenomena, design solutions, arguments, etc.\n[10][15], with which students develop scientiﬁc knowledge and competence. How-\never, it is time-consuming for teachers to review and evaluate natural language\narXiv:2301.12031v1  [cs.AI]  27 Jan 2023\n2 Zhengliang Liu, Xinyu He , Lei Liu, Tianming Liu, and Xiaoming Zhai\nwriting, thus preventing the timely understanding of students’ thought processes\nand academic progress. Recent development in machine learning (ML), especially\nnatural language processing (NLP), has proved to be a promising approach to\npromoting the use of writing in science teaching and learning [17]. For example,\nvarious NLP methods have been employed in science assessment practices that\ninvolve constructed responses, essays, simulation, or educational games [14]. In\nthis rapidly developing domain, the state-of-the-art Bidirectional Encoder Rep-\nresentations from Transformers (BERT) model [4], a transformer-based machine\nlearning architecture developed by Google, demonstrates superiority over other\nmachine learning methods in scoring student responses to science assessment\ntasks [1].\nFig. 1.The SciEdBERT framework. A student response instance is classiﬁed based on\nthe latent representation of word vectors.\nStudies have shown that the performance on NLP tasks can be improved by\nusing domain-speciﬁc data to contextualize language models [5]. Several BERT-\nbased language models, such as SciBERT [3], AgriBERT [12], BioBERT [8],\nand ClinicalRadioBERT [11], have demonstrated signiﬁcant success on domain-\nTitle Suppressed Due to Excessive Length 3\nspeciﬁc tasks. Therefore, it is reasonable to speculate that ML-based scoring of\nstudents’ scientiﬁc writing can be improved if we have a domain-speciﬁc lan-\nguage model for scientiﬁc education. In this case, we need to ﬁnd the proper\ndomain-speciﬁc data that are directly relevant to student writing. It is impor-\ntant to note that student responses are preliminary expressions of general science\nknowledge and lack the rigor of academic journal publications. In addition, their\nwriting is also inﬂuenced by the developmental progress of writing skills and the\nlength of the required tasks. These characteristics of student writing are chal-\nlenges for using NLP tools to score students’ writing [9] [6]. Therefore, to further\nimprove the application of large pre-trained language models to automatically\nscore students’ scientiﬁc writing, we use diﬀerent datasets to train BERT and\ncompare their performance on various downstream tasks. In this work, we make\nthe following contributions:\n1. We provide a method to improve model performance on the downstream\ntasks by contextualizing BERT with the downstream context in advance.\n2. We prove the eﬀectiveness of domain-speciﬁc data in improving BERT-\nbased model performance.\n3. We will release our language models, which can be further tested and used\nin other science education tasks.\n2 Methodology\n2.1 Architecture/Background\nThe BERT (Bidirectional Encoder Representations from Transformers) language\nmodel [4] is based on the transformer architecture [13]. It is trained using the\nmasked language modeling (MLM) objective, which requires the model to predict\nmissing words in a sentence given the context. This training process is called pre-\ntraining. The pre-training of BERT is unsupervised and only requires unlabeled\ntext data. During pre-training, word embedding vectors are multiplied with three\nsets of weights (query, key and value) to obtain three matrices Q, K, and V,\nrespectively. These matrices are then used to calculate attention scores, which\nare weights that measure the importance among input words. For example, in the\nexample ”I love my cats.”, the word ”I” should (ideally) be strongly associated\nwith the word ”my”, since they refer to the same subject.\nFig. 2.An example of BERT’s attention mechanism\n4 Zhengliang Liu, Xinyu He , Lei Liu, Tianming Liu, and Xiaoming Zhai\nFor each word, the attention scores are then used to weigh intermediate\noutputs that sum up to the ﬁnal vector representation of this word.\nAttention(Q, K, V) = softmax( QK√dk\n)V (1)\nwhere dk refers to the dimension of the K matrix.\nBERT takes a sequence of words as the input, and outputs a latent represen-\ntation of input tokens in the form of word vectors. This latent representation,\nor embedding, captures the semantics, positional information, and contextual\ninformation of the input sentence. It can be further used for downstream NLP\ntasks. To use BERT for practical natural language understanding applications,\nit is necessary to ﬁne-tune the model on the target task. BERT can be ﬁne-tuned\non a wide variety of tasks, such as topic classiﬁcation and question answering, by\nadding task-speciﬁc layers on top of this pre-trained transformer. Fine-tuning is\na supervised learning process. During this process, BERT is trained on a labeled\ndataset and the parameters of the model are updated in training to minimize\nthe task-speciﬁc loss function.\n2.2 Domain-speciﬁc training\nBERT is a fundamental building block for language models. In practice, it\nhas many variants that are tailored to the purposes and peculiarities of spe-\nciﬁc domains [3,8,2,12,11]. For example, BioBERT [8] is a large language model\ntrained on biomedical publications (PubMed) and delivers superior performance\non biomedical and chemical named entity recognition (NER), since it has a large\nand contextualized vocabulary of biomedical and chemical terms.\nSubstantial evidence indicates that language models perform better when\nthen target and source domains are aligned [8,5]. In other words, continual pre-\ntraining BERT-based models with in-domain corpora could signiﬁcantly improve\ntheir performance on downstream tasks [5]. In addition, there is much correlation\nbetween model performance and the extent of in-domain training. Speciﬁcally,\ntraining with more relevant in-domain text and training-from-scratch can further\nimprove pre-trained language models [5].\nIn this work, we incorporate prior experience in NLP, speciﬁcally that of\ndomain-speciﬁc training, to train our SCiEdBERT models designed speciﬁcally\nfor science education tasks.\n2.3 Training Design\nWe follow a pyramid-shaped training scheme to maximize our models’ utilization\nof domain-relevant data.\nIn Figure 3, we can see that SciBERT [3] is a science-oriented version of\nBERT developed through in-domain pre-training. As shown in Table 2, some of\nthe models we developed for this experiment in this study are further extensions\nof SciBERT through continual pre-training on various science education data.\nTitle Suppressed Due to Excessive Length 5\nFig. 3.The pyramid training scheme\nThe primary beneﬁt of following the pyramid training scheme is to avoid\ndiluting the relatively scarce in-domain data with the vastly more abundant\ngeneral text data. If instead a language model is trained on a combined corpus\nof general text and domain-speciﬁc data, the eﬀects of in-domain training will\nbe insigniﬁcant.\n3 Experiment\n3.1 Dataset\nWe employ several datasets to train the language models, including the Academic\nJournal Dataset for Science Education (SciEdJ), a large dataset of students’\nwritten Responses (SR1), and a small dataset of students’ responses to four\nargumentation tasks (SR2). Then, we use seven tasks from the large dataset\n(7T) and the four argumentation tasks (4T) as two datasets to ﬁne-tune the\ntrained language model. Below we brieﬂy introduce these datasets.\nTraining Dataset We use three datasets to train the language model. The Sci-\nEdJ is a collection of 2,000 journal articles from journals in science education. We\nselect ten journals in science education with the highest impact factors according\nto Web of Science, including Journal of Research in Science Teaching, Interna-\ntional Journal of Science Education, Science Education, etc. For Each journal,\nwe collect the most recent 200 articles. The SR1 dataset is a collection of over\n50,000 student short responses to 49 constructed response assessment tasks in\n6 Zhengliang Liu, Xinyu He , Lei Liu, Tianming Liu, and Xiaoming Zhai\nscience for middle school students. Students are anonymous to researchers and\nnot traceable. The SR2 dataset is a collection of 2,940 student responses from a\nset of argumentation assessment tasks [7].\nFine-tuning Dataset . We employ two datasets to evaluate the model per-\nformance. The 7T dataset includes seven tasks selected from the SR1 dataset,\nincluding short-constructed student responses and human expert-assigned la-\nbels. Overall, the 7T dataset includes 5,874 labeled student responses (915 for\ntask H4-2, 915 for task H4-3, 834 for task H5-2, 883 for task J2-2, 743 for task\nJ6-2, 739 for tasks J6-3, and 845 for task R1-2). The 4T dataset includes 2940\nstudent responses and their labels from SR2 dataset (e.g., 770 for item G4, 642\nfor item G6, 765 for item S2, and 763 for item S3). All the samples in the two\ndatasets are written responses from middle school students to explain science\nphenomena. Trained experts are employed to assign scores to student responses\naccording to scoring rubrics developed by science education researchers, and the\ninter-rater reliability is at or above satisfactory level (details see [16][15]).\n3.2 Baselines\nOur study aims to examine how the context of training data matters to pre-\ntrained models’ (e.g., BERT) performance and explore strategies to further im-\nprove model performance. To achieve this goal, we employ various datasets to\ntrain and ﬁne-tune the models. First, we use the original BERT as the pre-\ntrained model and 7T as the downstream task. This is the baseline model. We\nthen train a BERT model from SR1 and use 7T as the downstream task. Given\nthat the 7T is grounded in the context of SR1, a comparison between the two\nﬁne-tuned models (based on BERT vs. SR1-BERT) can address our goals.\nSecond, we repeat this training and ﬁne-tuning process using BERT with SR2\nand 4T datasets. To examine the generalization of the ﬁndings, we also employ\n4T as the downstream task in other pre-trained models, including SciBERT [3], a\nBERT model trained on SciEdJ (i.e., SciEJ-BERT), a SciBERT model trained on\nSciEdJ (i.e., SciEdJ-SciBERT), a BERT model trained on SR2 (i.e., SR2-BERT),\nand a SciBERT model trained on SR2 (i.e., SR2-SciBERT), with increasingly\ncloser contextualization between the pre-trained models and the downstream\ntasks.\n3.3 Results\nAs Table 1 presents, the average accuracy of SR1-BERT (0.912) is slightly higher\nthan the accuracy of BERT (0.904). Among the seven tasks, SR1-BERT achieves\nhigher accuracy than BERT on four tasks and are on par with BERT on the\nremaining three tasks. This indicates that the accuracy of automatic scoring can\nbe improved to a certain extent by training the model with in-domain training\ndata.\nTitle Suppressed Due to Excessive Length 7\nTable 1.Comparing diﬀerent model performance on 7T task\nItem Accuracy\nBERT SR1-BERT\nH4-2 0.913 0.929\nH4-3 0.831 0.831\nH5-2 0.958 0.970\nJ2-2 0.920 0.926\nJ6-2 0.959 0.973\nJ6-3 0.845 0.845\nR1-2 0.864 0.864\nAverage 0.904 0.912\nThis indication is clearer in our second experiment with the 4T dataset. As\nTable 2 presents, overall, SR2-SciBERT has the highest average accuracy (0.866),\nwhich indicates training the model with the contexts of the downstream tasks\ncan improve the accuracy of automatic scoring.\nThe model with the second highest accuracy (0.852) is SR2-BERT. SR2-\nBERT has the same performance as SR2-SciBERT on S3 and even higher accu-\nracy (0.821) than SR2-SciBERT (0.815) on G4. On S2, SR2-BERT’s performance\n(0.915) is only second to SR2-BERT. Only on G6, SR2-BERT has a lower accu-\nracy (0.719) than comparison models. Therefore, although the two models share\nthe same average accuracy, based on the accuracy results on each individual task,\nSR2-BERT performs better than BERT. This is also in line with our previous\nﬁndings that context matters in improving model performance.\nSciEdJ-SciBERT and SciEdJ-BERT have the lowest average accuracy scores\n(0.842) among the models. Only on G4 do these two models perform better\nthan BERT. This indicates that the context of science education publications\ncannot help BERT learn the language of student responses better. In fact, on the\ncontrary, such context may introduce confusion to the machine learning process.\nIn summary, SR2-SciBERT and SR2-BERT achieve the best results among\nthe models, which indicates that contextualizing the language models with the\nsame language of the downstream tasks can improve the model’s performance.\nTable 2.Comparing model performance on the 4T tasks\nItem Accuracy\nBERT SciEdJ-BERT SciEdJ-SciBERT SR2-BERT SR2-SciBERT\nG4 0.792 0.804 0.815 0.821 0.815\nG6 0.766 0.727 0.742 0.719 0.766\nS2 0.895 0.882 0.889 0.915 0.928\nS3 0.934 0.954 0.921 0.954 0.954\nAverage 0.847 0.842 0.842 0.852 0.866\n8 Zhengliang Liu, Xinyu He , Lei Liu, Tianming Liu, and Xiaoming Zhai\n4 Conclusions\nThis study investigates training language models with diﬀerent contextual data\nand compares their performance on eleven constructed response tasks. The re-\nsults indicate that using the in-domain data directly related to downstream tasks\nto contextualize the language model can improve a pre-trained language model’s\nperformance. In automatic scoring of students’ constructed responses, this means\ncontinual pre-training the language model on student responses and then ﬁne-\ntuning the model with the scoring tasks. In science education, using SciEdBERT\ncan further improve model performance as SciEdBERT is well-versed in scien-\ntiﬁc vocabulary. Our study conﬁrms the eﬀectiveness of using domain-speciﬁc\ndata to pre-train models to improve their performance on downstream tasks and\nvalidate a strategy to adapt language models to science education.\nTitle Suppressed Due to Excessive Length 9\nReferences\n1. Amerman, H., Zhai, X., Latif, E., He, P., Krajcik, J.: Does transformer deep learn-\ning yield more accurate sores on student written explanations than traditional\nmachine learning? In: Paper submitted to the Annual Meeting of the American\nEducational Research Association. Chicago (2023)\n2. Araci, D.: Finbert: Financial sentiment analysis with pre-trained language models.\narXiv preprint arXiv:1908.10063 (2019)\n3. Beltagy, I., Lo, K., Cohan, A.: Scibert: A pretrained language model for scientiﬁc\ntext. arXiv preprint arXiv:1903.10676 (2019)\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n5. Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao,\nJ., Poon, H.: Domain-speciﬁc language model pretraining for biomedical natural\nlanguage processing. ACM Transactions on Computing for Healthcare (HEALTH)\n3(1), 1–23 (2021)\n6. Ha, M., Nehm, R.H.: The impact of misspelled words on automated computer\nscoring: A case study of scientiﬁc explanations. Journal of Science Education and\nTechnology 25(3), 358–374 (2016)\n7. Haudek, K.C., Zhai, X.: Exploring the eﬀect of assessment construct complexity\non machine learning scoring of argumentation (2021)\n8. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: Biobert: a\npre-trained biomedical language representation model for biomedical text mining.\nBioinformatics 36(4), 1234–1240 (2020)\n9. Litman, D.: Natural language processing for enhancing teaching and learning. In:\nThirtieth AAAI conference on artiﬁcial intelligence (2016)\n10. Novak, A.M., McNeill, K.L., Krajcik, J.S.: Helping students write scientiﬁc expla-\nnations. Science Scope 33(1), 54 (2009)\n11. Rezayi, S., Dai, H., Liu, Z., Wu, Z., Hebbar, A., Burns, A.H., Zhao, L., Zhu, D.,\nLi, Q., Liu, W., et al.: Clinicalradiobert: Knowledge-infused few shot learning for\nclinical notes named entity recognition. In: International Workshop on Machine\nLearning in Medical Imaging. pp. 269–278. Springer (2022)\n12. Rezayi, S., Liu, Z., Wu, Z., Dhakal, C., Ge, B., Zhen, C., Liu, T., Li, S.: Agribert:\nknowledge-infused agricultural language models for matching food and nutrition.\nIn: IJCAI. IJCAI (2022)\n13. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems 30 (2017)\n14. Zhai, X., C Haudek, K., Shi, L., H Nehm, R., Urban-Lurain, M.: From substitution\nto redeﬁnition: A framework of machine learning-based science assessment. Journal\nof Research in Science Teaching 57(9), 1430–1459 (2020)\n15. Zhai, X., Haudek, K.C., Ma, W.: Assessing argumentation using machine learning\nand cognitive diagnostic modeling. Research in Science Education pp. 1–20 (2022)\n16. Zhai, X., He, P., Krajcik, J.: Applying machine learning to automatically assess\nscientiﬁc models. Journal of Research in Science Teaching59(10), 1765–1794 (2022)\n17. Zhai, X., Yin, Y., Pellegrino, J.W., Haudek, K.C., Shi, L.: Applying machine learn-\ning in science assessment: a systematic review. Studies in Science Education 56(1),\n111–151 (2020)",
  "topic": "Argumentation theory",
  "concepts": [
    {
      "name": "Argumentation theory",
      "score": 0.7954221963882446
    },
    {
      "name": "Computer science",
      "score": 0.7636264562606812
    },
    {
      "name": "Language model",
      "score": 0.6168492436408997
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5708912014961243
    },
    {
      "name": "Context (archaeology)",
      "score": 0.569581925868988
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5195958614349365
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49548661708831787
    },
    {
      "name": "Natural language processing",
      "score": 0.425459086894989
    },
    {
      "name": "Linguistics",
      "score": 0.12486329674720764
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165733156",
      "name": "University of Georgia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1341030882",
      "name": "Educational Testing Service",
      "country": "US"
    }
  ]
}