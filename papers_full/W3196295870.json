{
  "title": "CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade",
  "url": "https://openalex.org/W3196295870",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2098784551",
      "name": "Lei Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2131512363",
      "name": "Yankai Lin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100821712",
      "name": "Deli Chen",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2304534086",
      "name": "Shuhuai Ren",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1906085637",
      "name": "Peng Li",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2107643647",
      "name": "Xu Sun",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W131533222",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2951244744",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W3170113752",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3175772341",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2996159613",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2944701285",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W3104738015",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W4287165195",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287210036",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2325237720",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3174461835",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W3034212969",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3165511581"
  ],
  "abstract": "Dynamic early exiting aims to accelerate the inference of pre-trained language models (PLMs) by emitting predictions in internal layers without passing through the entire model. In this paper, we empirically analyze the working mechanism of dynamic early exiting and find that it faces a performance bottleneck under high speed-up ratios. On one hand, the PLMs’ representations in shallow layers lack high-level semantic information and thus are not sufficient for accurate predictions. On the other hand, the exiting decisions made by internal classifiers are unreliable, leading to wrongly emitted early predictions. We instead propose a new framework for accelerating the inference of PLMs, CascadeBERT, which dynamically selects proper-sized and complete models in a cascading manner, providing comprehensive representations for predictions. We further devise a difficulty-aware objective, encouraging the model to output the class probability that reflects the real difficulty of each instance for a more reliable cascading mechanism. Experimental results show that CascadeBERT can achieve an overall 15% improvement under 4x speed-up compared with existing dynamic early exiting methods on six classification tasks, yielding more calibrated and accurate predictions.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 475–486\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n475\nCascadeBERT: Accelerating Inference of Pre-trained Language Models\nvia Calibrated Complete Models Cascade\nLei Li†, Yankai Lin§, Deli Chen†§, Shuhuai Ren†, Peng Li§, Jie Zhou§, Xu Sun†\n†MOE Key Laboratory of Computational Linguistics, School of EECS, Peking University\n§Pattern Recognition Center, WeChat AI, Tencent Inc., China\n{lilei, shuhuai_ren}@stu.pku.edu.cn\n{chendeli, xusun}@pku.edu.cn\n{yankailin, patrickpli, withtomzhou}@tecent.com\nAbstract\nDynamic early exiting aims to accelerate\nthe inference of pre-trained language models\n(PLMs) by emitting predictions in internal lay-\ners without passing through the entire model.\nIn this paper, we empirically analyze the work-\ning mechanism of dynamic early exiting and\nﬁnd that it faces a performance bottleneck un-\nder high speed-up ratios. On one hand, the\nPLMs’ representations in shallow layers lack\nhigh-level semantic information and thus are\nnot sufﬁcient for accurate predictions. On\nthe other hand, the exiting decisions made by\ninternal classiﬁers are unreliable, leading to\nwrongly emitted early predictions. We instead\npropose a new framework for accelerating the\ninference of PLMs, CascadeBERT, which dy-\nnamically selects proper-sized and complete\nmodels in a cascading manner, providing com-\nprehensive representations for predictions. We\nfurther devise a difﬁculty-aware objective, en-\ncouraging the model to output the class prob-\nability that reﬂects the real difﬁculty of each\ninstance for a more reliable cascading mecha-\nnism. Experimental results show that Cascade-\nBERT can achieve an overall 15% improve-\nment under 4×speed-up compared with exist-\ning dynamic early exiting methods on six clas-\nsiﬁcation tasks, yielding more calibrated and\naccurate predictions.1\n1 Introduction\nLarge-scale pre-trained language models (PLMs),\ne.g., BERT and RoBERTa, have demonstrated supe-\nrior performance on various natural language under-\nstanding tasks (Devlin et al., 2019; Liu et al., 2019).\nWhile the increased model size brings more promis-\ning results, the long inference time hinders the de-\nployment of PLMs in real-time applications. Re-\nsearchers have recently exploited various kinds of\napproaches for accelerating the inference of PLMs,\n1Our code is available at https://github.com/\nlancopku/CascadeBERT\nL=1L=2\nL=9\nL=12\n:Classifier:BERTLayer\nDeeBERT\nL=1L=2\nBERT-Complete\n201\n.992.001.007✓ 201\n.991.004.005✓\n201.250.504.246✗\n0:neutral✗1:entailment✓2:contradiction✗\n⋯\n⋯\nPremise:Also,the final rule is not intended to have any retroactive effect and administrative procedures must be exhausted prior to any judicial challenge to the provisions of the rule.Hypothesis:The final rule isn't meant to have a retroactive effect.0:Neutral✗ 1:Entailment✓2:Contradiction✗\nFigure 1: An easy instance with a large word over-\nlap (colored in orange) between the premise and the\nhypothesis from the MNLI dataset. The classiﬁers in\nshallow layers of a dynamic early exiting model cannot\npredict correctly, while BERT-Complete (Turc et al.,\n2019), a small BERT pre-trained from scratch with the\nsame size can make a correct and conﬁdent prediction.\nwhich can be categorized into model-level com-\npression and instance-level speed-up. The former\naims at obtaining a compact model via quantiza-\ntion (Zafrir et al., 2019; Shen et al., 2020; Zhang\net al., 2020), pruning (V oita et al., 2019; Michel\net al., 2019) or knowledge distillation (KD) (Sanh\net al., 2019; Sun et al., 2019; Jiao et al., 2020),\nwhile the latter adapts the amount of computation\nto the complexity of each instance (Graves, 2016).\nA mainstream method for instance-level speed-up\nis dynamic early exiting, which emits predictions\nbased on intermediate classiﬁers (or off-ramps)\nof internal layers when the predictions are con-\nﬁdent enough (Xin et al., 2020b; Liu et al., 2020;\nSchwartz et al., 2020; Li et al., 2021).\nIn this paper, we focus on dynamic early exit-\ning, as it can be utilized to accelerate inference\nand reduce the potential risk of the overthinking\nproblem (Kaya et al., 2019). Such a paradigm is\n476\nintuitive and simple, while faces a performance\nbottleneck under high speed-up ratios, i.e., the task\nperformance is poor when most examples are ex-\nited in early layers. We conduct probing exper-\niments to investigate the mechanism of dynamic\nexiting, and ﬁnd that the poor performance is due\nto the following two reasons: (1) The shallow rep-\nresentations lack high-level semantic information,\nand are thus not sufﬁcient for accurate predictions.\nAs PLMs like BERT exhibit a hierarchy of repre-\nsentations, e.g., shallow layers extract low-level\nfeatures like lexical/syntactic information while\ndeep layers capture semantic-level relations (Ten-\nney et al., 2019; Jawahar et al., 2019), we argue\nthat the high-level semantic inference ability is usu-\nally required even for easy instances. As shown\nin Figure 1, the classiﬁer of the second layer in a\nrepresentative early exiting model DeeBERT (Xin\net al., 2020b) cannot predict correctly even for an\neasy instance with a large word overlap. On the\ncontrary, BERT-Complete, a shallow 2-layer model\npre-trained from scratch (Turc et al., 2019) that is\nthus capable of extracting semantic-level features,\ncan make conﬁdent and correct predictions like that\nin deep layers of DeeBERT. (2) The intermediate\nclassiﬁers in the early exiting models cannot pro-\nvide reliable exiting decisions. We design a metric\nto examine the ability of models to distinguish dif-\nﬁcult instances from easy ones, which can reﬂect\nthe quality of exiting decisions. We ﬁnd that the\npredictions of internal classiﬁers cannot faithfully\nreﬂect the instance difﬁculty, resulting in wrongly\nemitted results and thus hindering the efﬁciency of\nearly exiting.\nTo remedy those drawbacks, we instead extend\nthe dynamic early exiting idea to a model cascade,\nand propose CascadeBERT, which conducts in-\nference based on a series of complete models in a\ncascading manner with a dynamic stopping mecha-\nnism. Speciﬁcally, given an instance for inference,\ninstead of directly exiting in the middle layers of a\nsingle model, the framework progressively checks\nif the instance can be solved by the current PLM\nfrom the smallest to the largest one, and emits the\nprediction once the PLM is conﬁdent about the\nprediction. Furthermore, we propose a difﬁculty-\naware regularization to calibrate the PLMs’ predic-\ntions according to the instance difﬁculty, making\nthem reﬂect the real difﬁculty of each instance.\nTherefore, the predictions can be utilized as a good\nindicator for the early stopping in inference. Ex-\nperimental results on six classiﬁcation tasks in the\nGLUE benchmark demonstrate that our model can\nobtain a much better task performance than pre-\nvious dynamic early exiting baselines under high\nspeed-up ratios. Further analysis demonstrates that\nthe proposed difﬁculty-aware objective can cali-\nbrate the model predictions, and proves the effec-\ntiveness and the generalizability of CascadeBERT.\n2 Investigations into Early Exiting\nDynamic early exiting aims to speed-up the infer-\nence of PLMs by emitting predictions based on\ninternal classiﬁers. For each instance, if the in-\nternal classiﬁer’s prediction based on the current\nlayer representation of the instance is conﬁdent\nenough, e.g., the maximum class probability ex-\nceeds a threshold (Schwartz et al., 2020), then the\nprediction is emitted without passing through the\nentire model. However, whether the internal repre-\nsentations could provide sufﬁcient information for\naccurate predictions and whether the intermediate\nclassiﬁers can be utilized for making accurate exit-\ning decisions still remain unclear. In this section,\nwe investigate the working mechanism of dynamic\nearly exiting by exploring these two questions.\n2.1 Are Shallow Features Sufﬁcient?\nAs discussed by Tenney et al. (2019), PLMs like\nBERT learn a hierarchy of representations. We\nassume that the high-level semantics is usually re-\nquired even for easy instances, and therefore the\npredictions based on shallow representations are\ninsufﬁcient for accurate predictions. To examine\nthis, we evaluate the model performance based on\noutputs of different layers, as the representation\ncontains adequate information is necessary for a\ndecent task performance. Speciﬁcally, we compare\nthe following models:\nDeeBERT (Xin et al., 2020b), which is a repre-\nsentative of early exiting methods. The internal\nclassiﬁers are appended after each layer in the orig-\ninal BERT for emitting early predictions.\nBERT-kL, which only utilizes the ﬁrst klayers in\nthe original BERT model for prediction. A classi-\nﬁer is added directly after the ﬁrst k layers. The\nparameters of the ﬁrst k layers and the classiﬁer\nare ﬁne-tuned on the training dataset. It could be\nseen as a static early exiting method.\nBERT-Complete (Turc et al., 2019), which is a\nlight version of the original BERT model pre-\ntrained from scratch using the masked language\n477\n2 4 6\nNumber of Layers\n40.0\n50.0\n60.0\n70.0\n80.0\n90.0Accuracy (%)\nMNLI-m\n2 4 6\nNumber of Layers\n75.0\n80.0\n85.0\n90.0\n95.0\n100.0\nSST-2\nDeeBERT BERT-kL BERT-Complete\nFigure 2: Performance comparison utilizing different\nmodels with the same number of layers on MNLI-m\nand SST-2. Complete models capable of extracting\nsemantic-level information clearly outperform models\nlike DeeBERT which overlooks the high-level seman-\ntic features.\nmodeling (MLM) objective. We assume the repre-\nsentations of this model contain high-level seman-\ntic information, as MLM requires a deep under-\nstanding of the language.\nFor a fair comparison, models are evaluated on\na subset of instances which DeeBERT chooses\nto emit at different layers. We report predic-\ntion accuracy using different number of layers on\nMNLI (Williams et al., 2018) and SST-2 (Socher\net al., 2013). Figure 2 shows the results on the\ndevelopment sets, and we can see that:\n(1) BERT-Complete clearly outperforms Dee-\nBERT, especially when the predictions are made\nbased on shallow layers. It indicates that the high-\nlevel semantics is vital for handling tasks like\nsentence-level classiﬁcation.\n(2) BERT-kL also outperforms DeeBERT. We\nattribute it to that the last serveral layers can learn\ntask-speciﬁc information during ﬁne-tuning to ob-\ntain a decent performance. A similar phenomenon\nis also observed by Merchant et al. (2020). How-\never, since the internal layer representation in Dee-\nBERT are restricted by the layer relative position\nin the whole model, this adaption effect cannot be\nfully exploited, resulting in the poor performance\nin shallow layers.\nThese ﬁndings verify our assumption that the\nsemantic-level features are vital, motivating us\nto exploit complete models for predictions. Be-\nsides, DeeBERT performs poorly on the selected\ninstances which it decides to emit at different lay-\ners, triggering our further explorations on the qual-\nity of exiting decisions.\n2 4 6\nNumber of Layers\nBERT-Complete\nBERT-kL\nDeeBERT\n77.42 78.85 79.76\n73.49 77.28 80.16\n56.66 65.33 74.31\nMNLI-m\n60\n65\n70\n75\n80\n2 4 6\nNumber of Layers\n79.89 83.61 85.24\n78.91 79.91 82.20\n71.89 75.77 77.10\nSST-2\n72\n75\n78\n81\n84\nFigure 3: DIS (%, higher is better, see Eq. 3 in Sec-\ntion 2.2) heatmap of different models on the develop-\nment set of MNLI and SST-2. The DIS of internal off-\nramps in the DeeBERT of shallow layers is lower than\nthat of BERT-kl and BERT-Complete, which leads to\nmore wrongly emitted instances. The exiting decisions\nin shallow layers of DeeBERT thus can be unreliable.\n2.2 Are Internal Classiﬁers Reliable?\nWe further probe whether the early exiting deci-\nsions made by internal classiﬁers are reliable, by\nﬁrst introducing two key concepts:\n• Instance Difﬁculty d(x), which indicates\nwhether an instance x can be handled by a\nspeciﬁc model. We deﬁne instances that the\nmodel cannot predict correctly as difﬁcult in-\nstances, i.e., d(x) = 1, and those can be han-\ndled well as easy ones, i.e., d(x) = 0.\n• Model Conﬁdence c(x), which denotes how\nconﬁdent the model is about its prediction for\na speciﬁc instance x. For each instance, we\nutilize the maximum class probability of the\noutput distribution as the conﬁdence score.\nIntuitively, a difﬁcult instance should be predicted\nwith less conﬁdence than that of an easy one, such\nthat the output distribution can be utilized as an\nindicator for early exiting decisions. However, the\nmodel conﬁdence can be inconsistent with the in-\nstance difﬁculty due to the overconﬁdent problem.\nTo measure this consistency, we proposeDifﬁculty\nInversion Score (DIS). Speciﬁcally, we ﬁrst deﬁne\na difﬁcult inversion indicator function for instance\npair (xi,xj) measuring the inconsistency between\nmodel conﬁdence and instance difﬁculty as:\nDI (xi,xj) =\n{\n1, if d(xi) >d(xj) and c(xi) <c(xj)\n0, otherwise,\n(1)\nThe instances are then sorted by their conﬁdence\nscores in an ascending order, i.e., c(xi) ≤c(xj)\nfor any i < j. We compute the sum of difﬁculty\n478\ninversion pair as:\nDI-Sum =\nN∑\ni=1\ni−1∑\nj=1\nDI(xi,xj), (2)\nwhere N is the number of instance. The ﬁnal DIS\nis a normalized DI-Sum:\nDIS = 1− 1\nK DISum, (3)\nwhere Kis a normalizing factor calculated as the\nproduct of the number of easy instances and the\nnumber of difﬁcult instances, to re-scale DIS to the\nrange from 0 to 1. According to the deﬁnition, the\nDIS measures the proportion of instance pairs that\nare correctly ranked by the classiﬁer. Classiﬁers\nwith lower DIS achieve lower consistency between\nthe model conﬁdence and instance difﬁculty, thus\nmaking more unreliable exiting decisions. The DIS\nthus can be utilized as a proxy for evaluating the\nquality of exiting decisions. We compute the DIS\non the development sets of MNLI-m and SST-2 for\ninternal classiﬁers of different models discussed\nin Section 2.1, and the results are illustrated in\nFigure 3. We ﬁnd that:\n(1) The DIS of internal classiﬁers in shallow\nlayers of DeeBERT falls far behind BERT-kL and\nBERT-Complete. This indicates that the exiting\ndecisions in the shallow layers of DeeBERT are\nunreliable, and the task performance thus can be\npoor when most instances are wrongly emitted in\nearly layers.\n(2) The ability to distinguish difﬁcult examples\nfrom easy ones is enhanced as the layer number in-\ncreases. Since the deep layer representations with\nsemantic information can boost the task perfor-\nmance, it is reasonable to expect that the off-ramps\nin deep layers can provide more comprehensive\nearly exiting decisions.\nOur analysis demonstrates that current dynamic\nearly exiting predictions made by internal classi-\nﬁers in shallow layers are not reliable, motivating\nus to inform the model of the instance difﬁculty for\nmore robust exiting decisions.\n3 Methodology\nTo remedy the drawbacks of conducting dynamic\nexiting in a single model, we extend the idea to a\nmodel cascade, by proposing CascadeBERT, that\nutilizes a suite of complete PLMs with different\nnumber of layers for acceleration in a cascading\nmanner, and further devise a difﬁculty-aware cal-\nibration regularization to inform the model of in-\nstance difﬁculty.\n3.1 Cascade Exiting\nFormally, given ncomplete pre-trained language\nmodels {M1,...,M n}ﬁne-tuned on the down-\nstream classiﬁcation dataset, which are sorted in an\nascending order by their corresponding number of\nlayers {L1,...,L n}, our goal is to conduct infer-\nence with the minimal computational cost for each\ninput instance xwhile maintaining the model per-\nformance. Our preliminary exploration shows that\nit is relatively hard to directly selecting a proper\nmodel for each instance according to the instance\ndifﬁculty. Therefore, we formulate it as a cascade\nexiting problem, i.e., execute the model prediction\nsequentially for each input example from the small-\nest M1 to the largest Mn, and check whether the\nprediction of the input instance xcan be emitted.\nSpeciﬁcally, we use the conﬁdence score c(x), i.e.,\nthe maximum class probability, as a metric to deter-\nmine whether the predictions are conﬁdent enough\nfor emitting:\nc(x) = max\ny∈Y\n(Pr(y|x)), (4)\nwhere Y is the label set of the task and Pr(y|x) is\nthe class probability distribution outputted by the\ncurrent model. The predicted result is emitted once\nthe conﬁdence score exceeds a preset threshold τ.\nBy varying the threshold τ, we can obtain different\nspeed-up ratios based on the application require-\nments. A smaller τ indicates that more examples\nare outputted using the current model, making the\ninference faster, while a bigger τ will make more\nexamples go through larger models for better re-\nsults. The cascaded exiting framework is summa-\nrized in Algorithm 1. Since every model in our cas-\ncading framework is a complete model, predictions\nare more accurate with instance representations\nthat contain both low-level and high-level features,\neven when only the smallest model is executed.\n3.2 Difﬁculty-Aware Regularization\nTo further make the cascade exiting based on conﬁ-\ndence score more reliable, we design a difﬁculty-\naware regularization (DAR) objective based on\ninstance difﬁculty, to regularize the model clas-\nsiﬁers produce lower conﬁdence for more difﬁ-\ncult instances. To measure the instance difﬁculty,\nwe ﬁrst split the training dataset Dinto K folds\n479\nAlgorithm 1: Cascade Exiting\nInput: Models {M1,...,M n}, threshold τ\nData: Input x\nResult: Class probability distribution Pr(y|x)\nfor i←1 to ndo\n// calculate class distribution\nPr(y|x) =Mi(x)\n// compute confidence score\nc(x) = maxy(Pr(y|x))\nif c(x) >τ then\nEarly exit Pr(y|x)\nreturn Pr(y|x)\n{˜Di |i = 1,...,K }. For each complete model\nin our cascade, we train K models with the same\narchitecture using the leave-one-out method, e.g.,\nmodel Mi\nn is trained on the D−˜Di\n= ⋃j̸=i\nj ˜Dj. We\nutilize Mi\nn to evaluate the difﬁculty of the exam-\nples in ˜Di for model Mn. Speciﬁcally, the samples\nare marked as easy, i.e., d= 0, if they can be cor-\nrectly classiﬁed by the model. Otherwise, they are\nmarked as difﬁcult, i.e., d = 1. To eliminate the\nimpact of randomness, we group the predictions of\n5 seeds and strictly label the examples that can be\ncorrectly predicted in all seeds as easy examples,\nwhile the others as difﬁcult ones.\nWith the instance difﬁculty for each instance\nin the training dataset, we add a difﬁculty-based\nmargin objective for each instance pair:\nL(xi,xj) = max{0,−g(xi,xj) (c(xi) −c(xj)) +ϵ},\n(5)\nwhere ϵ is a conﬁdence margin. We design\ng(xi,xj) as below:\ng(xi,xj) =\n\n\n\n1, if d(xi) >d(xj)\n0, if d(xi) =d(xj)\n−1, otherwise.\n(6)\nThis objective is added to the original task-speciﬁc\nloss with a weight factor λto adjust its impact. By\noptimizing the above objective function, the conﬁ-\ndence scores of difﬁcult instances are adjusted to be\nlower than those of easy instances, thus making the\nconﬁdence-based emitting decisions more reliable.\n4 Experiments\nWe evaluate our method on the classiﬁcation tasks\nin the GLUE benchmark (Wang et al., 2018) with\nBERT (Devlin et al., 2019). We ﬁrst give a brief\nintroduction of the dataset used and the experimen-\ntal setting, followed by the description of baseline\nmodels for comprehensive evaluation. The results\nand analysis of the experiments are presented last.\nDataset # Train # Dev # Test Metric ϵ\nMNLI 393k 20k 20k Accuracy 0.3\nMRPC 3.7k 0.4k 1.7k F1-score 0.5\nQNLI 105k 5.5k 5.5k Accuracy 0.3\nQQP 364k 40k 391k F1-score 0.3\nRTE 2.5k 0.3k 3k Accuracy 0.5\nSST-2 67k 0.9k 1.8k Accuracy 0.5\nTable 1: Statistics of six classiﬁcation datasets in\nGLUE benchmark. The selected difﬁculty margins ϵ\nof each datasets are provided in the last column.\n4.1 Experimental Settings\nWe use six classiﬁcation tasks in GLUE bench-\nmark, including MNLI (Williams et al., 2018),\nMRPC (Dolan and Brockett, 2005), QNLI (Ra-\njpurkar et al., 2016), QQP,2 RTE (Bentivogli et al.,\n2009) and SST-2 (Socher et al., 2013). The metrics\nfor evaluation are F1-score for QQP and MRPC,\nand accuracy for the rest tasks. Our implemen-\ntation is based on the Huggingface Transformers\nlibrary (Wolf et al., 2020). We use two models\nfor selection with 2 and 12 layers, respectively,\nsince they can provide a wide range for accelera-\ntion. The difﬁculty score is thus evaluated based\non the 2-layer model. The effect of incorporating\nmore models in our cascade framework is explored\nin the later section. We utilize the weights provided\nby Turc et al. (2019) to initialize the models in our\nsuite. The split number Kfor difﬁculty labeling is\nset to 8. We use AdamW (Loshchilov and Hutter,\n2018) with a learning rate2e-5 to train model for10\nepochs, since we ﬁnd that small models need more\ntime to converge. We set DAR weightλas 0.5, and\nperform grid search over ϵ in {0.1,0.3,0.5,0.7}.\nThe best model is selected based on the validation\nperformance. The statistics of datasets and the se-\nlected ϵare provided in Table 1.\nThe inference speed-up ratio is estimated as the\nratio of number of the original model and layers\nactually executed in forward propagation in our cas-\ncade. Compared to performing dynamic exiting in\na single model, the overhead of CascadeBERT con-\nsists of two parts. The former is the extra embed-\nding operations, which is nearly 0.3M FLOPs and\nis negligible compared with the1809.9M FLOPs of\neach layer (Liu et al., 2020). The latter is brought\nby instances that run forward propagation multiple\ntimes, which is counted in the speed-up ratio calcu-\nlation. For example, for an instance which is ﬁrst\n2https://data.quora.com/First-Quora-\nDataset-Release-Question-Pairs\n480\nMethod MNLI-m/mm MRPC QNLI QQP RTE SST-2 A VG\nBERT-base† 84.6 (1.00×) / 83.4 (1.00×) 88.9 (1.00×) 90.5 (1.00×) 71.2 (1.00×) 66.4 (1.00×) 93.5 (1.00×) 82.6\n∼2×\nBERT-6L‡ 79.9 (2.00×) / 79.2 (2.00×) 85.1 (2.00×) 86.2 (2.00×) 68.9 (2.00×) 65.0 (2.00×) 90.9 (2.00×) 79.3\nDeeBERT† 74.4 (1.87×) / 73.1 (1.88×) 84.4 (2.07×) 85.6 (2.09×) 70.4 (2.13×) 64.3 (1.95×) 90.2 (2.00×) 77.5\nPABEE† 79.8 (2.07×) / 78.7 (2.08×) 84.4 (2.01×) 88.0 (1.87×) 70.4 (2.09×) 64.0 (1.81×) 89.3 (1.95×) 79.2\nCascadeBERT 83.0 (2.01×) / 81.6 (2.01×) 85.9 (2.01×) 89.4 (2.01×) 71.2 (2.01×) 64.6 (2.03×) 91.7 (2.08×) 81.1\n∼3×\nBERT-4L‡ 75.8 (3.00×) / 75.1 (3.00×) 82.7 (3.00×) 84.7 (3.00×) 66.5 (3.00×) 63.0 (3.00×) 87.5 (3.00×) 76.5\nDeeBERT‡ 63.2 (2.98×) / 61.3 (3.03×) 83.5 (3.00×) 82.4 (2.99×) 67.0 (2.97×) 59.9 (3.00×) 88.8 (2.97×) 72.3\nPABEE‡ 75.9 (2.70×) / 75.3 (2.71×) 82.6 (2.72×) 82.6 (3.04×) 69.5 (2.57×) 60.5 (2.38×) 85.2 (3.15×) 75.9\nCascadeBERT 81.2 (3.00×) / 79.5 (3.00×) 84.0 (3.00×) 88.5 (2.99×) 71.0 (3.02×) 63.8 (3.03×) 90.9 (2.99×) 79.8\n∼4×\nBERT-3L‡ 74.8 (4.00×) / 74.3 (4.00×) 80.5 (4.00×) 83.1 (4.00×) 65.8 (4.00×) 55.2 (4.00×) 86.4 (4.00×) 74.3\nDeeBERT‡ 55.8 (4.01×) / 54.2 (3.99×) 82.9 (3.99×) 75.9 (4.00×) 62.9 (4.01×) 57.4 (4.00×) 85.4 (4.00×) 67.8\nPABEE‡ 62.3 (4.32×) / 63.0 (4.30×) 79.9 (4.00×) - 68.0 (3.45×) 56.0 (3.62×) - -\nCascadeBERT 79.3 (4.03×) / 77.9 (3.99×) 82.6 (4.00×) 86.5 (3.99×) 70.0 (4.04×) 61.6 (4.02×) 90.3 (4.01×) 78.3\nTable 2: Test results from the GLUE server. We report F1-score for QQP and MRPC and accuracy for other tasks,\nwith the corresponding speed-up ratios shown in parentheses. For baseline methods, † denotes results taken from\nthe original paper and ‡ denotes results based on our implementation. The - denotes that results are not available\nby tuning the threshold of PABEE. Best results are shown in bold.\nfed into a 2-layer model and then goes through a\n4-layer model to obtain the ﬁnal prediction result,\nthe number of layers actually executed is there-\nfore 6 and the corresponding speed-up ratio is 2×\ncompared to the original 12-layer full model.\n4.2 Baselines\nWe implement two kinds of baselines, including:\nEarly Exiting, including BERT-kL, where the ﬁrst\nk layers with a ﬁne-tuned classiﬁer are used for\noutputting the ﬁnal classiﬁcation results. We take\nk = 6, k = 4 and k = 3 to obtain a statically\ncompressed model with speed-up ratios of 2×, 3×\nand 4×, respectively; DeeBERT (Xin et al., 2020b),\nwhich makes dynamic early predictions based on\nthe internal classiﬁers; PABEE (Zhou et al., 2020),\nan enhanced variant by emitting the result until\nseveral layers produce a consistent prediction. 3\nKnowledge Distillation methods that do not re-\nquire external data, including DistilBERT (Sanh\net al., 2019), which distills knowledge from the\nteacher model to the student during pre-training\nvia logit distillation; BERT-PKD (Sun et al., 2019),\nwhich distills internal states of the teacher model to\nthe student model; BERT-of-Theseus (Xu et al.,\n2020), which gradually replaces the module in\nthe original model; BERT-PD (Turc et al., 2019),\nwhich directly pre-trains a compact model from\nscratch and conducts distillation on the task dataset.\n3PABEE provides limited speed-ratios since the threshold\nfor tuning speed-up ratios can only be set to integers.\nMethod MNLI-m/mm QNLI QQP SST-2 A VG\nDistilBERT 78.9 / 78.0 85.2 68.5 91.4 80.4\nBERT-PKD 79.9 / 79.3 85.1 70.2 89.4 80.8\nBERT-Theseus 78.6 / 77.4 85.5 68.3 89.7 79.9\nBERT-PD 79.3 / 78.3 87.0 69.8 89.8 80.8\nCascadeBERT 81.2 / 79.5 88.5 71.0 90.9 82.2\nTable 3: Test result comparison with static knowledge\ndistillation methods under speed-up ratio 3×.\n4.3 Overall Results\nThe performance comparison with early exiting\nmethods are presented in Table 2. We observe that\nCacadeBERT outperforms all the baseline meth-\nods under different speed-up ratios, validating the\neffectiveness of our proposal. Furthermore, the per-\nformance gap becomes clearer as the acceleration\nratio increases. For example, CascadeBERT out-\nperforms DeeBERT by a big margin with a relative\n15.5% improvement (10.5 points on average) un-\nder speed-up ratio 4×. This phenomenon demon-\nstrates that CascadeBERT can break the perfor-\nmance bottleneck by utilizing comprehensive rep-\nresentations from complete models. Interestingly,\nwe ﬁnd CascadeBERT performs closely with Dee-\nBERT on MRPC. We attribute it to that this para-\nphrase identiﬁcation task requires less high-level\nsemantic information, thus only utilizing low-level\nfeatures at speciﬁc layers can sometimes become\nbeneﬁcial. Different from DeeBERT and PABEE,\nFastBERT (Liu et al., 2020) enhances the internal\nclassiﬁers with a self-attention mechanism to use\nall the hidden states for predictions, resulting in\n481\nDeeBERT - Accuracy: 44.0\ncontradiction\nentailment\nneutral\nCascadeBERT - Accuracy: 75.9\ncontradiction\nentailment\nneutral\n(a) Instance representations t-SNE projection on MNLI-m.\nDeeBERT - Accuracy: 78.0 \nnegative\npositive\nCascadeBERT - Accuracy: 87.7\nnegative\npositive\n(b) Instance representations t-SNE projection on SST-2.\nFigure 4: t-SNE visualization of instance representa-\ntions of different class in DeeBERT and our Cascade-\nBERT at the second layer. The instance representations\nof our CascadeBERT exhibit a more distinct boundary\nbetween different classes, helping the following classi-\nﬁer to make accurate predictions. Best viewed in color.\na different magnitude of computational overhead.\nComparison results with FastBERT are provided\nin Appendix A. CascadeBERT can still outperform\nFastBERT, especially on the tasks requiring seman-\ntic reasoning ability.\nBesides, our proposal also achieves superior\nperformance over strong knowledge distillation\nmethods like BERT-PKD and BERT-of-Theseus,\nas shown in Table 3. Distillation methods can im-\nplicitly learn the semantic reasoning ability by forc-\ning student models to mimic the behaviors of the\nteacher model. However, it is still relatively hard\nto obtain a single compressed model to handle all\ninstances well, as different instances may require\nthe reasoning ability of different granularities. Our\ncascading mechanism instead provides ﬂexible op-\ntions for instances with different complexities, thus\nachieving better results.\n5 Analysis\nIn this section, we investigate how the proposed\nCascadeBERT makes accurate predictions under\nhigh speed-up ratios, and analyze the effects of\nthe proposed difﬁculty-aware regularization and\nincorporating more models to the cascade. We\nﬁnally examine the generalizability by applying it\nto RoBERTa. The experiments are conducted on\nMethod MNLI-m/mm QNLI QQP SST-2 A VG\n∼3×CascadeBERT 81.2 / 79.5 88.5 71.0 90.9 82.2\n- w/o DAR 80.0 / 79.3 87.8 71.0 90.3 81.7\n∼4×CascadeBERT 79.3 / 77.9 86.5 70.0 90.3 80.8\n- w/o DAR 78.9 / 78.1 86.6 69.8 89.6 80.6\nTable 4: Ablated results of the proposed difﬁculty-\naware regularization under different speed-up ratios.\nMNLI, QNLI, QQP and SST-2 for stable results.\n5.1 Visualization of Instance Representations\nTo investigate how the representations with sufﬁ-\ncient information beneﬁt accurate predictions, we\nvisualize the instance representations after 2 lay-\ners using t-SNE projection (Maaten and Hinton,\n2008). The results and the corresponding classiﬁer\naccuracy are shown in Figure 4. We observe that\nthe boundary of instances belonging to different\nclasses of our CascadeBERT is much clearer than\nthat of DeeBERT. Since the representations contain\nsufﬁcient information for predictions, our model\ncan thus obtain more accurate results. Interestingly,\nthe shallow representations in DeeBERT of SST-2\nare already separable to some extent, which indi-\ncates that the task is somewhat easy. It is consistent\nwith our main results that the performance degra-\ndation of different methods is negligible on SST-2.\n5.2 Effects of Difﬁculty-Aware\nRegularization\nWe show the performance of an ablated version\nof our proposal, CascadeBERT w/o DAR in Ta-\nble 4. The results indicate that the DAR can im-\nprove the overall performance of our framework.\nNote that the improvement is very challenging\nto achieve, as the original model cascade already\noutperforms strong baseline models like PABEE.\nFurthermore, we explore whether the performance\nboost comes from an enhanced ability of the model\nto distinguish difﬁcult instances from easy ones.\nSpeciﬁcally, we compute the DIS and the task ac-\ncuracy (Acc) of the smallest model in our cascade.\nThe results are listed in Table 5. We ﬁnd that the\nDAR can effectively improve the DIS while slightly\nharms the task performance, indicating that DAR\nboosts the overall performance by helping model\nmake more reliable emitting decisions. The excep-\ntional decrease of DIS on QQP is attributed to the\nfact that the original DIS score is relatively high,\nwhich makes further improvements very challeng-\ning. Besides, the DAR can lower the prediction\n482\nDataset Method DIS (↑) Acc(↑) ECE (↓)\nMNLI-m CascadeBERT 78.00 75.97 7.90\n- w/o DAR 76.73 76.02 11.07\nQNLI CascadeBERT 78.89 84.53 3.41\n- w/o DAR 77.79 84.73 8.79\nQQP CascadeBERT 84.39 87.21 3.37\n- w/o DAR 85.77 88.71 4.99\nSST-2 CascadeBERT 82.02 87.70 5.61\n- w/o DAR 79.30 87.95 8.73\nTable 5: The ECE (%), Acc (%) and DIS (%) scores\non different datasets. ↑denotes higher is better, while\n↓means lower is better. The proposed DAR can boost\nthe performance by giving hints of instance difﬁculty\nand calibrate the model predictions.\nconﬁdence of difﬁcult instances, which improves\nthe consistency between the predicted probability\nand how likely the model is to be correct for an\ninstance. We quantitively measure this calibration\neffect of DAR, by utilizing the expected calibration\nerror (ECE) (Guo et al., 2017). 4 As shown in Ta-\nble 5, the DAR not only improves the DIS score,\nbut also calibrates the model predictions, achieving\nlower expected calibration error.\n5.3 Impacts of More Models in Cascade\nWe further consider to incorporate more models\ninto the CascadeBERT framework. Theoretically,\nwe prove that adding more models in cascade can\nboost the task performance under mild assumptions.\nBesides, the beneﬁts will become marginal as the\nnumber of model increases. The detailed proof is\nprovided in the Appendix C. We empirically verify\nthis by adding a medium-sized model with 6 layers\nwhich satisﬁes our assumptions into the cascade.\nThe performance under different speed-up ratios of\na 2-12 cascade consists of a 2L model and a 12L\nmodel and the above mentioned 2-6-12 cascade\nare illustrated in Figure 5. Overall, we ﬁnd that\nadding a model with a moderate size can slightly\nimprove the performance, while the gain becomes\nmarginal when the speed-up ratio is higher, since\nmost instances are emitted from the smallest model.\n5.4 Fine-tuned Models as an Alternative\nTo verify the generalizability of our cascading\nframework, we propose to apply our method to\nRoBERTa (Liu et al., 2019). However, small ver-\nsions of RoBERTa pre-trained from scratch are\ncurrently not available. We notice that BERT-kL\n4Refer to Appendix B for the details of the ECE score.\n2 3 4 5 6\nSpeed-up Ratio\n87.0\n88.0\n89.0\n90.0\n91.0\n92.0\nSST-2\n2-6-12\n2-12\n2 3 4 5 6\nSpeed-up Ratio\n76.0\n77.0\n78.0\n79.0\n80.0\n81.0\n82.0\n83.0Accuracy (%)\nMNLI\n2-6-12\n2-12\nFigure 5: Task performance on the validation set and\nspeed-up ratio trade-off curve comparison of a 2-model\ncascade (orange square) and a 3-model cascade (blue\ncircle) on SST-2 and MNLI-m.\nMethod MNLI-m/mm QNLI QQP SST-2 A VG\nRoBERTa-base 87.0 / 86.3 92.4 71.8 94.3 86.4\nRoBERTa-4L 80.3 / 79.2 86.2 69.8 90.8 81.2\nDeeBERT 53.9 / 55.4 77.2 67.6 88.6 68.5\nPABEE 74.0 / 74.2 - - 87.5 -\nCascadeRoBERTa 78.9 / 78.1 86.8 70.5 90.8 81.0\n+ Vanilla KD 79.7 / 78.8 86.9 70.8 91.4 81.5\nTable 6: Test results from the GLUE server with\nRoBERTa models in our cascade framework. The\nspeed-up ratio is approximately 3×(±4%). The - de-\nnotes unavailable results of PABEE.\nmodel can achieve comparable performance via\nﬁne-tuning, as discussed in Section 2. Therefore,\nwe propose to leverage a ﬁne-tuned RoBERTa-2L\nwith the vanilla KD (Hinton et al., 2015) incorpo-\nrated for enhancing its semantic reasoning ability,\nas an alternative of the original complete model.\nThe results around 3×speed-up are listed in Ta-\nble 6. Our framework still outperforms dynamic\nearly exiting baselines by a clear margin, validat-\ning that our framework is universal and can be\ncombined with knowledge distillation techniques\nto further boost the performance.\n6 Related Work\nModel-level compression includes knowledge dis-\ntillation (KD), pruning and quantization. KD fo-\ncuses on transferring the knowledge from a large\nteacher model to a compact student model (Hinton\net al., 2015). Sanh et al. (2019) propose Distil-\nBERT and Sun et al. (2019) enhance KD by align-\ning the internal representations of the student and\nthe teacher model. Besides, Jiao et al. (2020) pro-\npose TinyBERT via a two-stage KD on augmented\ndata. Pruning methods deactivate the unimportant\nstructures in the model like attention heads (V oita\n483\net al., 2019; Michel et al., 2019) and layers (Fan\net al., 2019). Quantization methods target at us-\ning fewer physical bits to efﬁciently represent the\nmodel (Zafrir et al., 2019; Shen et al., 2020; Zhang\net al., 2020). We do not compare pruning and quan-\ntization methods since these techniques are orthog-\nonal to our framework.\nInstance-level speed-up accelerates the inference\nvia adapting the computation according to the in-\nstance complexity (Graves, 2016). A representa-\ntive framework is dynamic early exiting, which\nhas been veriﬁed in natural language understand-\ning (Xin et al., 2020b; Schwartz et al., 2020; Liu\net al., 2020; Zhou et al., 2020; Liao et al., 2021;\nSun et al., 2021), sequence labeling (Li et al., 2021),\nquestion answering (Soldaini and Moschitti, 2020)\nand document ranking (Xin et al., 2020a). In this\npaper, we probes the work mechanism of dynamic\nearly exiting, and ﬁnd that it faces a serious per-\nformance bottleneck under high speed-up ratios.\nTo remedy this, we generalize the idea to a model\ncascade and prove it is effectiveness even under\nhigh speed-up ratios for various natural language\nunderstanding tasks. Concurrently with our work,\nEnomoto and Eda (2021) adopt the similar idea\nand achieve better inference efﬁciency on image\nclassiﬁcation tasks.\n7 Conclusion\nIn this paper, we point out that current dynamic\nearly exiting framework faces a performance bot-\ntleneck under high speed-up ratios, due to insufﬁ-\ncient shallow layer representations and poor exit-\ning decisions of the internal classiﬁers. To remedy\nthis, we propose CascadeBERT, a model cascade\nframework with difﬁculty-aware regularization for\naccelerating the inference of PLMs. Experimen-\ntal results demonstrate that our proposal achieves\nsubstantial improvements over previous dynamic\nexiting methods. Further analysis validates that\nthe framework is generalizable and produces more\ncalibrated results.\nAcknowledgements\nWe thank all the anonymous reviewers for their\nconstructive comments, Xuancheng Ren and Hua\nZheng for their valuable suggestions in preparing\nthe manuscript, and Wenkai Yang for providing the\ntheoretical analysis. This work was supported by a\nTencent Research Grant. Xu Sun is the correspond-\ning author of this paper.\nReferences\nLuisa Bentivogli, Ido Kalman Dagan, Dang Hoa,\nDanilo Giampiccolo, and Bernardo Magnini. 2009.\nThe ﬁfth pascal recognizing textual entailment chal-\nlenge. In TAC Workshop.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT, pages 4171–4186.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP).\nShohei Enomoto and Takeharu Eda. 2021. Learning to\ncascade: Conﬁdence calibration for improving the\naccuracy and computational cost of cascade infer-\nence systems. In AAAI, pages 7331–7339.\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. In ICLR.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In ICML, pages 1321–1330.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In ACL, pages 3651–3657.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of EMNLP, pages\n4163–4174.\nYigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.\n2019. Shallow-deep networks: Understanding and\nmitigating network overthinking. In ICML, pages\n3301–3310.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In ACL, pages 7871–7880.\nXiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan,\nXipeng Qiu, and Xuanjing Huang. 2021. Accelerat-\ning BERT inference for sequence labeling via early-\nexit. In ACL, pages 189–199.\n484\nKaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su,\nXu Sun, and Bin He. 2021. A global past-future\nearly exit method for accelerating inference of pre-\ntrained language models. In NAACL-HLT, pages\n2013–2023.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. FastBERT: A self-\ndistilling BERT with adaptive inference time. In\nACL, pages 6035–6044.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In ICLR.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. JMLR, 9:2579–2605.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT\nembeddings during ﬁne-tuning? In BlackboxNLP\nWorkshop, pages 33–44.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In\nNeurIPS, pages 14014–14024.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In EMNLP, pages\n2383–2392.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled ver-\nsion of BERT: smaller, faster, cheaper and lighter.\nIn NeurIPS Workshop on Energy Efﬁcient Machine\nLearning and Cognitive Computing.\nRoy Schwartz, Gabriel Stanovsky, Swabha\nSwayamdipta, Jesse Dodge, and Noah A. Smith.\n2020. The right tool for the job: Matching\nmodel and instance complexities. In ACL, pages\n6640–6651.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-BERT: Hessian based ultra low\nprecision quantization of BERT. In AAAI, pages\n8815–8821.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In EMNLP, pages 1631–1642.\nLuca Soldaini and Alessandro Moschitti. 2020. The\ncascade transformer: an application for efﬁcient an-\nswer sentence selection. In ACL, pages 5697–5708.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In EMNLP-IJCNLP, pages 4323–4332.\nTianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu\nZhang, Hao Jiang, Zhao Cao, Xuanjing Huang, and\nXipeng Qiu. 2021. Early exiting with ensemble in-\nternal classiﬁers. arXiv preprint arXiv:2105.13792.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nACL, pages 4593–4601.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nThe impact of student initialization on knowledge\ndistillation. arXiv preprint arXiv:1908.08962.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In ACL, pages 5797–\n5808.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In\nEMNLP Workshop on BlackboxNLP , pages 353–\n355.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In NAACL-\nHLT, pages 1112–1122.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In System Demonstrations, EMNLP, pages\n38–45.\nJi Xin, Rodrigo Nogueira, Yaoliang Yu, and Jimmy Lin.\n2020a. Early exiting BERT for efﬁcient document\nranking. In SustaiNLP: Workshop on Simple and Ef-\nﬁcient Natural Language Processing, pages 83–88.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020b. DeeBERT: Dynamic early exit-\ning for accelerating BERT inference. In ACL, pages\n2246–2251.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. BERT-of-Theseus: Com-\npressing BERT by progressive module replacing. In\nEMNLP, pages 7859–7869.\n485\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: Quantized 8bit BERT.\nIn NeurIPS Workshop on Energy Efﬁcient Machine\nLearning and Cognitive Computing.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. TernaryBERT:\nDistillation-aware ultra-low bit BERT. In EMNLP,\npages 509–521.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian\nMcAuley, Ke Xu, and Furu Wei. 2020. BERT loses\npatience: Fast and robust inference with early exit.\nIn NeurIPS, pages 18339–18350.\nA Comparison with FastBERT\nPerformance comparison of our CascadeBERT\nwith FastBERT is shown in Table 7 under 3.00×\nand 4.00×speed-up ratios. Note that FastBERT\nutilizes a complicate internal classiﬁer with self-\nattention mechanism and take the hidden states\nof all the sequence tokens for making predictions,\nwhile we only adopt the original linear-based classi-\nﬁer. Our method can still outperform the FastBERT\nunder high speed-up ratios, especially on tasks that\nrequire high-level semantic reasoning ability like\nMNLI.\nMethod MNLI-m/-mmQNLIQQPSST-2Average\n∼3×FastBERT 79.8 / 78.9 88.2 71.5 92.1 82.1\nCascadeBERT81.2 / 79.5 88.5 71.0 90.9 82.2\n∼4×FastBERT 76.1 / 75.2 86.4 70.5 90.7 79.8\nCascadeBERT79.3 / 77.9 86.5 70.0 90.3 80.8\nTable 7: Performance comparison with FastBERT.\nB Expected Calibration Error\nCalibration measures the consistency between pre-\ndictions’ conﬁdence and accuracy. A well cali-\nbrated model can be more reliable, e.g., it can give\nus a hint that it knows what it does not know, and\nthus it is easier for deployments in real-world appli-\ncations. It is formally expressed as a joint distribu-\ntion P(Q,Y ) over conﬁdences Q∈Rand labels\nY ∈L. When P(Y = y|Q= q) =q, the model\nis perfectly calibrated. For example, if the aver-\nage conﬁdence score of 100 instances is 0.8, there\nshould be 80 instances that are correctly predicted.\nThis probability can be approximated by grouping\npredictions into kdisjoint and equally-sized bins,\nwhere each bin consists of bk predictions. The ex-\npected calibration error is deﬁned as a weighted\naverage of difference between each bin’s accuracy\n(acc(·)) and prediction conﬁdence (conf(·)):\nECE =\n∑\nk\nbk\nn|acc(k) −conf(k)| (7)\nwhere nis the number of total instances. A lower\nECE denotes the model is better calibrated. In this\npaper, we set k= 10for calculating the ECE score.\nC Analysis for More Models in Cascade\nSuppose there are nmodels {M1,...,M n}sorted\nfrom the smallest to largest according to number of\nlayers in our cascade, with corresponding number\nof layers {L1,...,L n}and the task performance,\ne.g., classiﬁcation accuracy {a1,··· ,an}, we want\nto explore whether incorporating another complete\nmodel into the original cascade can further improve\nthe task performance and speed-up trade-off. In\nmore detail, we propose to evaluate the difference\nof classiﬁcation accuracy between the original cas-\ncade and the new cascade, under the same speed-\nup ratio. We denote the new added model as M∗\nwith classiﬁcation accuracy a∗ consisting of L∗\nlayers, Li < L∗ < Li+1 for a speciﬁc i. Consid-\nering the instance emitting distribution, we denote\nthe number of instances exiting after model Mj\n(j = 1,...,n ) as sj in the original nmodels cas-\ncade. For the newn+1 models cascade, the number\nof samples exiting after model Mj (j = 1,...,n )\nis ˆsj and there will be ˆs∗instances emitting from\nM∗. Besides, we assume that the accuracyai of Mi\nis the same for any subsets of the original dataset.\nThe performance difference thus can be written as:\nT = 1\nN\n( n∑\nk=1\nakˆsk + a∗ˆs∗−\nn∑\nk=1\naksk\n)\n(8)\nunder the conditions of\n\n\n\nn∑\nk=1\nsk =\nn∑\nk=1\nˆsk + ˆs∗ = N\nn∑\nk=1\nskLk =\ni∑\nk=1\nˆskLk + ˆs∗(Li + L∗)+\nn∑\nk=i+1\nˆsk(Lk + L∗)\n(9)\nwhere Lk =\nk∑\ni=1\nLi is the actual layer cost with\nthe computation overhead and N is the number\nof test instances. The ﬁrst condition indicates the\ntotal number of test instances is the same, and the\nsecond one guarantees that the total layer cost is\nsame thus the speed-up ratio is identical.\n486\nThere are inﬁnite solutions for the above system\nof equations, as we can adjust the exiting thresholds\nof different models to achieve the same speed-up\nratio. We propose to simplify this by making a\nassumption that we only adjust the thresholds of\nMi, Mi+1 and M∗to achieve the same speed-up\nratio, thus the following equation holds:\nˆsk = sk, k = 1,2,··· ,i−1,i+2,··· ,n (10)\nConditions in Eq. (9) can thus be re-written as\n\n\n\nsi + si+1 = ˆsi + ˆsi+1 + ˆs∗\nsiLi + si+1Li+1 = ˆsiLi + ˆs∗(Li + L∗)\n+ ˆsi+1(Li+1 + L∗)\n(11)\nThen we can further calculate si and si+1 as\n{\nsi = ˆsi + ˆs∗− L∗\nLi+1\n(ˆsi+1 + ˆs∗)\nsi+1 = ˆsi+1 + L∗\nLi+1\n(ˆsi+1 + ˆs∗) (12)\nBy plugging the equations in Eq. 12 into the\nEq. 8, and use the assumption in Eq. 10 we get\nT = 1\nN[ai(ˆsi −si) +ai+1(ˆsi+1 −si+1) +a∗ˆs∗]\n= 1\nN\n[\na∗ˆs∗+ai( L∗\nLi+1\n(ˆsi+1 + ˆs∗) −ˆs∗)\n−ai+1\nL∗\nLi+1\n(ˆsi+1 + ˆs∗)\n]\n= 1\nN\n[\nˆs∗(a∗−ai) − L∗\nLi+1\n(ˆsi+1 + ˆs∗)(ai+1 −ai)\n]\nThe ﬁnal expected performance difference is thus:\nT(ˆs∗,L∗) = 1\nN\n[\nˆs∗(a∗−ai)−\nL∗\nLi+1\n(ˆsi+1 + ˆs∗)(ai+1 −ai)\n]\n(13)\nwhere the index isatisﬁes that Li < L∗ < Li+1.\nNote that the model accuracy a∗is related to the\nsize L∗ of model, as a larger model with more\nlayers tends to achieve a better task performance. It\nindicates that the performance difference depends\non the number of samples exits at model M∗(ˆs∗),\nand the layers of M∗(L∗). If we ﬁx the index i\nwhen we add the new model M∗, since we have\nai ≤a∗≤ai+1, from Eq (13) we can get\nT(ˆs∗,L∗)\n≤ 1\nN\n[\nˆs∗(a∗−ai) − L∗\nLi+1\n(ˆsi+1 + ˆs∗)(a∗−ai)\n]\n≤ 1\nN(a∗−ai)\n[\nˆs∗− L∗\nLi+1\n(ˆsi+1 + ˆs∗)\n]\nOn the one hand, as L∗→Li+1, (a∗−ai) will in-\ncrease to (ai+1 −ai), but ˆs∗− L∗\nLi+1\n(ˆsi+1 + ˆs∗) will\ndecrease to −ˆsi+1; On the other hand, when L∗\ngets close to Li, a∗−ai →0. This trade-off indi-\ncates that the layer size of M∗should be carefully\nchosen to achieve performance improvements. Oth-\nerwise, the overall gain could be negative. Besides,\nthe upper bound of maximum gain also depends on\nthe number of samples exit at M∗. Thus, adjusting\nthresholds properly is also important. Additionally,\nwe can further scale the upper bound as:\nT(ˆs∗,L∗)\n≤ 1\nN(ai+1 −ai)\n[\nˆs∗− Li\nLi+1\n(ˆsi+1 + ˆs∗)\n]\n≤si + si+1\nN (ai+1 −ai)\n(\n1 − Li\nLi+1\n)\nwhich indicates that\nmax\ns∗,L∗ {T(ˆs∗,L∗)}\n≤si + si+1\nN\n(\nmax\ni\n{ai+1 −ai}\n)(\n1 −min\ni\n{ Li\nLi+1\n})\n.\nNote that\nmax\ni\n{ai+1 −ai |Mi,··· ,Mn} (14)\nand\nmin\ni\n{ Li\nLi+1\n|Mi,··· ,Mn\n}\n(15)\nare non-increasing as ngets larger. It means the\nmaximum expected performance gain of adding\nanother model can be marginal as the number of\nmodels in the original cascade becomes larger.\nIn all, our analysis shows that we should care-\nfully select model M∗with layers L∗, and tune the\nexiting threshold to adjust number of samples exit\nafter M∗, to guarantee that the target in Eq. 13 is\npositive, in order to gain improvements by incorpo-\nrating more models into the original cascade.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8497192859649658
    },
    {
      "name": "Inference",
      "score": 0.8148541450500488
    },
    {
      "name": "Bottleneck",
      "score": 0.7479806542396545
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5768639445304871
    },
    {
      "name": "Language model",
      "score": 0.5431853532791138
    },
    {
      "name": "Mechanism (biology)",
      "score": 0.5082909464836121
    },
    {
      "name": "Class (philosophy)",
      "score": 0.5033199191093445
    },
    {
      "name": "Machine learning",
      "score": 0.4951113760471344
    },
    {
      "name": "Cascade",
      "score": 0.4531822204589844
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I20231570",
      "name": "Peking University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ]
}