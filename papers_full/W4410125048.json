{
  "title": "Leveraging pretrained deep protein language model to predict peptide collision cross section",
  "url": "https://openalex.org/W4410125048",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5072775335",
      "name": "Ayano Nakai-Kasai",
      "affiliations": [
        "Nagoya Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5067550140",
      "name": "Kosuke Ogata",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A5001763350",
      "name": "Yasushi Ishihama",
      "affiliations": [
        "Kyoto University",
        "National Institute of Biomedical Innovation, Health and Nutrition"
      ]
    },
    {
      "id": "https://openalex.org/A5014391514",
      "name": "Toshiyuki Tanaka",
      "affiliations": [
        "Kyoto University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2023096047",
    "https://openalex.org/W4378952025",
    "https://openalex.org/W4360612265",
    "https://openalex.org/W2969462402",
    "https://openalex.org/W2545581884",
    "https://openalex.org/W1982282366",
    "https://openalex.org/W2123081993",
    "https://openalex.org/W3005901855",
    "https://openalex.org/W2308104112",
    "https://openalex.org/W2952907530",
    "https://openalex.org/W2058906815",
    "https://openalex.org/W2755377387",
    "https://openalex.org/W1967287387",
    "https://openalex.org/W3027789773",
    "https://openalex.org/W3133209323",
    "https://openalex.org/W3168541695",
    "https://openalex.org/W1989304085",
    "https://openalex.org/W2883681250",
    "https://openalex.org/W2099800042",
    "https://openalex.org/W2565465295",
    "https://openalex.org/W3016001898",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W4401069625",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W6791955017",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4409152738",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W4318071656",
    "https://openalex.org/W3110850960",
    "https://openalex.org/W2169359398",
    "https://openalex.org/W2476058349",
    "https://openalex.org/W3039570519",
    "https://openalex.org/W4385564265",
    "https://openalex.org/W4407259436",
    "https://openalex.org/W2053943711",
    "https://openalex.org/W4409340338",
    "https://openalex.org/W2080352379",
    "https://openalex.org/W1986656413",
    "https://openalex.org/W3209258220",
    "https://openalex.org/W2035239343",
    "https://openalex.org/W4310951870",
    "https://openalex.org/W2054970390",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2557925011",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3098953572"
  ],
  "abstract": "Collision cross section (CCS) of peptide ions provides an important separation dimension in liquid chromatography/tandem mass spectrometry-based proteomics that incorporates ion mobility spectrometry (IMS), and its accurate prediction is the basis for advanced proteomics workflows. This paper describes experimental data and a prediction model for challenging CCS prediction tasks including longer peptides that tend to have higher charge states. The proposed model is based on a pretrained deep protein language model. While the conventional prediction model requires training from scratch, the proposed model enables training with less amount of time owing to the use of the pretrained model as a feature extractor. Results of experiments with the novel experimental data show that the proposed model succeeds in drastically reducing the training time while maintaining the same or even better prediction performance compared with the conventional method. Our approach presents the possibility of prediction on the basis of \"greener\" manner training of various peptide properties in proteomic liquid chromatography/tandem mass spectrometry experiments.",
  "full_text": "communicationschemistry Article\nA Nature Portfolio journal\nhttps://doi.org/10.1038/s42004-025-01540-z\nLeveraging pretrained deep protein\nlanguage model to predict peptide\ncollision cross section\nCheck for updates\nAyano Nakai-Kasai 1, Kosuke Ogata 2, Yasushi Ishihama 2,3,5 & Toshiyuki Tanaka 4,5\nCollision cross section (CCS) of peptide ions provides an important separation dimension in liquid\nchromatography/tandem mass spectrometry-based proteomics that incorporates ion mobility\nspectrometry (IMS), and its accurate prediction is the basis for advanced proteomics workﬂows. This\npaper describes experimental data and a prediction model for challenging CCS prediction tasks\nincluding longer peptides that tend to have higher charge states. The proposed model is based on a\npretrained deep protein language model. While the conventional prediction model requires training\nfrom scratch, the proposed model enables training with less amount of time owing to the use of the\npretrained model as a feature extractor. Results of experiments with the novel experimental data show\nthat the proposed model succeeds in drastically reducing the training time while maintaining the same\nor even better prediction performance compared with the conventional method. Our approach\npresents the possibility of prediction on the basis of“greener” manner training of various peptide\nproperties in proteomic liquid chromatography/tandem mass spectrometry experiments.\nProteins are important biological elements responsible for various functions\nof living organisms, and a systematic understanding of when, where, and\nhow these proteins are expressed is necessary for system-wide analysis of\nbiological functions\n1. Therefore, an important issue in proteomics is how to\nefﬁciently identify and quantify the vast number of proteins present in cells\nand tissues2. Recent advances in liquid chromatography/tandem mass\nspectrometry (LC/MS/MS) have signiﬁcantly improved the coverage of\nbottom-up proteomics3. However, a typical human proteome sample con-\nsists of more than ten million protease-digested peptides4,w h o s ec o m p l e x i t y\nis beyond the separation capabilities of current LC/MS/MS systems5.\nRecently, ion mobility spectrometry( I M S )h a sg a i n e da t t e n t i o na sy e t\nanother promising separation method to be combined with LC/MS/MS6–10.\nIMS separates molecules in terms of their charge and shape by measuring\nthe mobility of ions moving in a buffer gasﬂow under the inﬂuence of an\nelectric ﬁeld\n11. The frequency of ion-gas collisions, also known as the col-\nlision cross section (CCS), determines the ion mobility in the gas phase12.\nThus, even ion species of the samem/z may exhibit different CCS values due\nto different conformations they take13. The extended separation space\nprovided by ion mobility resolves various problems caused by the insufﬁ-\ncient separation of peptide ions in the conventional LC/MS/MS. For\nexample, it can improve the separation of peptide isomers, which are\npeptides with the same sequence but different positions of post-translational\nmodiﬁcations. Additionally, the improvement of peptide separation can\nlead to better quantitation accuracy\n7,9,14.\nIMS is not only effective for improving the separation efﬁciency of\npeptide ions, but it also has the potential for improving peptide\nidentiﬁcation\n15,16. While peptide identiﬁcation primarily relies on MS/MS\nspectra, utilizing additional information such as peptide retention time can aid\nin the identiﬁcation process, particularly for data from target acquisitions or\ndata-independent acquisitions. However, accurate prediction of peptide\nretention time is necessary for approaches utilizing retention time information\nto be effective\n17,18. It is also the case with those utilizing IMS data: For better\nanalysis of proteomic IMS data, it would be desirable if one can accurately\npredict CCS values of peptide ions. Several groups have so far proposed CCS\nprediction algorithms. Clemmer and coworkers established the model called\nintrinsic size parameter (ISP), which represents the relative size of each amino\nacid residue in a peptide sequence\n13,19,20. While this model works for a speciﬁc\nset of peptides, it has inherent limitations due to the use of the peptide’sa m i n o\nacid compositions but not the sequences. This ISP parameter has been further\nexpanded to incorporate some sequence information\n16.\nOn the other hand, inspired by great successes of deep learning in\nvarious researchﬁelds, the use of a deep neural network (NN) model for the\n1Graduate School of Engineering, Nagoya Institute of Technology, Nagoya, Aichi, 466-8555, Japan.2Department of Molecular Systems Bioanalysis, Graduate\nSchool of Pharmaceutical Sciences, Kyoto University, Kyoto, 606-8501, Japan.3Laboratory of Proteomics for Drug Discovery, National Institute of Biomedical\nInnovation, Health and Nutrition, Ibaraki, Osaka, 567-0085, Japan.4Graduate School of Informatics, Kyoto University, Kyoto, 606-8501, Japan.5These authors\njointly supervised this work: Yasushi Ishihama, Toshiyuki Tanaka.e-mail: yishiham@pharm.kyoto-u.ac.jp; tt@i.kyoto-u.ac.jp\nCommunications Chemistry|           (2025) 8:137 1\n1234567890():,;\n1234567890():,;\nCCS prediction problem has been proposed in ref.15, where a bidirectional\nLSTM model21, trained from scratch with a dataset of 559,979 unique\npeptide ion data, was used for CCS prediction. The dataset contains peptide\nsequences of less than 30 amino acid residues, and mostly with the doubly\ncharged species, which are typicallyobserved in proteomic experiments.\nThe prediction of CCS values of longer peptides is challenging due to the\nlimited availability of data and their enhanced structural variability. Longer\npeptides tend to have higher charge states, such as triple, quadruple charges\nand more, which results in higher variability in CCS among species even\nwithin similarm/z ranges. Therefore, it is desirable to provide data con-\nsisting of a greater variety of peptides and verify the predictive performance\nof prediction models on such a dataset. In deep NN models, training time\nand computational load become bottlenecks for such performance ver-\niﬁcation. There is generally a trade-off between models’performance and\nsuch complexities. This problem is serious, especially in many laboratories\nwhere computational resources are limited. Developing models that can\nachieve reasonable performance at lower cost is a new direction to aim for.\nOur proposal for CCS prediction is to use a pretrained deep protein\nlanguage model as a feature extractor from peptide sequences, and train a\nseparate NN, which we call a prediction NN, to predict CCS values on the\nbasis of the extracted features. The overall model architecture of our pro-\nposal, which we name the pretrained protein language model-based net-\nwork (PPLN), is depicted in Fig.1. It has been argued\n22 that a deep (natural)\nlanguage model trained with a large corpus of a language implicitly acquires\ngrammatical information of that language. Likewise, a pretrained deep\nprotein language model trained with a large-scale database of protein\nsequences is believed to acquire structural information of proteins\n23 (so\ncalled “protein grammar”). The possibility of utilizing pretrained protein\nlanguage models for various prediction tasks was suggested in ref.24.\nIndeed, it has been demonstrated in ref.25 that the feature representation\nprovided by a pretrained deep protein language model named evolutionary\nscale modeling-1b (ESM-1b) is useful in prediction of secondary structure\nand residue-residue contacts in proteins. As the CCS value of a peptide ion is\nregarded as being determined by the conformation of the ion particles in the\ndrift tube, one can expect that the features obtained by such a deep protein\nlanguage model that encodes structural information of proteins will be\nuseful in prediction of CCS values of peptide ions as well.\nUse of a pretrained protein language model as a feature extractor for\nCCS prediction might limit performance of CCS prediction compared with\nthe approach of training a dedicatedcomplex deep NN model from scratch,\nwhen the quality of the extracted features by the pretrained model is not\ngood. It will have several advantages, on the other hand, when the quality of\nthe features is good. It will make the CCSprediction problem easier to solve:\nOne can use a simpler prediction NN, and train it with a smaller-sized\ntraining dataset and with less amount of time. Training of the prediction NN\nwill thus be performed in a“greener” manner than training a dedicated\ncomplex NN model from scratch, on cheaper computer hardware and with\nless energy consumption.\nIn this study, we used a newly measured CCS dataset containing a\nwider variety of peptides to investigate how effective PPLN is compared with\ntraditional deep learning models and how“green” the steps required to\ncreate a CCS predictor can be performed.\nResults\nModel architecture of PPLN\nA number of deep protein language models for general purpose25–31 and\nspeciﬁct a s k s32,33 have so far been proposed. Such a deep protein language\nmodel, especially one for general purpose, can be used as a feature extractor,\nby removing the output layer of the model and regarding the outputs of the\npre-output layer as a feature representation of the input. Although we used\nESM-1b\n25 as the feature extractor of PPLN in our experiments, any pre-\ntrained protein language model can in principle be used as the feature\nextractor. A feature extractor that is based on a deep protein language model\ntypically takes as its input a variable-length amino-acid sequence, and\noutputs a sequence of features, whoselength is the same as the length of the\ninput sequence. One then has to aggregate the variable-length feature\nsequence into aﬁxed-size representation to feed it into the prediction NN, as\ns h o w ni nF i g .1.\nAlthough aggregation with simple averaging as used in the original\nESM-1b would work well in certain tasks as in ref.25,w ef o u n dt h a t\nintroduction of aggregation consideringamino acid positions, i.e., positional\nencoding (PE) in the aggregation, worked better than simple averaging. The\nﬁxed-size feature representation, along with the charge number and the mass\nof the ion, is then fed into the prediction NN, which outputs a prediction^Ωof\nthe CCS valueΩ o ft h ei n p u tp e p t i d ei o n .W ei n c l u d e dt h ec h a r g en u m b e r\nand the mass as the input of the prediction NN because the masses of\npeptides obviously have a strong correlation with the CCS values, and the\ncharge numbers of peptide ions also have a strong inﬂuence on the CCS\nvalues. Inclusion of the charge number and the mass to the input of the\nprediction NN is thus expected to facilitate the learning of the prediction NN.\nWe next discuss our design of PE. The CCS value of a peptide ion can be\naffected by several factors. Among them, one can expect that amino acid\nresidues located near the terminals have stronger effects than those located\nin the central part of the peptide, as suggested, e.g., by the length-speciﬁc\nmultiple linear regression (LS-MLR) study\n16. We thus designed our PE in the\naggregation stage in such a way that it reﬂects features of those residues\nlocated near either of the terminals of a peptide ion, rather than the absolute\npositions of the residues relative to the N-terminal, the latter of which we call\nthe unidirectional PE in this paper.\nThe concrete mathematical explanations of feature extraction and PE,\nand the detailed settings of the prediction model PPLN in the following\nexperiments are shown in“Methods”.\nDataset construction\nThe emergence of proteomic IMS technologies and the availability of large-\nscale peptide CCS data have signiﬁcantly improved the performance of\npeptide CCS prediction15,16. However, the current proteomic data mainly\ncover peptides with a length of less than 30 amino acids, which limits our\nFig. 1 | Model architecture of pretrained protein language model-based network\n(PPLN) for prediction of collision cross section (CCS) value.An amino-acid\nsequence is input into a pretrained protein language model. Positional encoding is\napplied to the obtained feature sequence. The feature sequence is aggregated to form\na ﬁxed-size feature, which is then input into the prediction neural network (NN)\nalong with charge and mass.\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 2\nunderstanding of the behaviors of longer peptides that are of general\ninterest. To address this gap in knowledge and to make CCS data have a wide\nvariety, we constructed an experimental peptide CCS dataset using phos-\nphoproteome data. Phosphopeptides are known to have more missed\ncleavages and tend to be longer than non-phosphopeptides\n34.T oo b t a i na\nunique set of peptides, we digested HeLa cell extracts with seven proteases\n( t r y p s i n ,L y s a r g i N a s e ,L y s - C ,L y s - N ,G l u - C ,A s p - N ,a n dc h y m o t r y p s i n )a s\ndescribed previously\n16, enriched phosphopeptides from the digests35,a n d\nfractionated the resulting phosphopeptides with SCX-StageTip36.W et h e n\ndephosphorylated the phosphopeptides using calf intestine alkaline\nphosphatase\n34 to generate non-phosphorylated peptides with more missed\ncleavages. Mascot automated database search algorithm was used to identify\nthe peptides, and IonQuant\n37 was used to extract the peptide precursor ion\nfeatures (mass, retention time, ion mobility, and intensity). Weﬁltered out\npeptide ions bearing variable modiﬁcations and only considered the most\nabundant feature for each peptide ion. Phosphoproteomes contain distinct,\nlonger sequences compared with conventional peptide CCS datasets,\nalthough they do not occupy the majority of our dataset (Supplemen-\ntary Fig. 1).\nThe dataset consists of 91,677 unique peptide ion data. It includes 11%\nof singly charged, 57% of doubly charged, 25% of triply charged, and 7% of\nquadruply charged ions, which are shown in Fig.2a. Frequencies of peptide\nC-terminal and N-terminal aminoacids are also summarized in Fig.2b, c,\nrespectively. Figure3 shows a plot of the experimental CCS valuesΩ vs. the\nm/z values. The experimental CCS values andm/z range from 289 to\n1162 Å\n2 and 381 to 1798, respectively.\nEvaluation of prediction error from and correlation with experi-\nmental CCS values\nFor performance evaluations, the dataset mentioned in the previous section\nwas randomly divided in two parts, where 73,342 ions (80% of the total) for\ntraining and the remaining 18,335 (20%) for testing. Figure4 shows scatter\nplots of the predicted CCS values obtained by the proposed PPLN. Figure4a\nincludes results of predicted CCS values vs. experimental CCS values and\nfour statistics, Pearson’s correlation coefﬁcient r, root mean squared error\n(RMSE), mean absolute error (MAE), andΔ95% error. The deﬁnitions are\nsummarized in“Methods”.T h i sﬁgure also includes the results of LS-MLR\n16\nand the bidirectional LSTM-based method15. The LS-MLR model was\nconstructed to beﬁtted on all data of the dataset. The deep NN used in the\nbidirectional LSTM-based method was trained from scratch using the\ntraining dataset of this paper, rather than the pretrained model using the\ndataset provided in ref.15. Prediction error evolutions for the bidirectional\nLSTM-based method and PPLN are shown in Supplementary Figs. 3 and 4.\nFrom Fig.4, the predicted CCS values by LS-MLR tended to over-\nestimate the experimental CCS values, especially for ions with larger CCS\nvalues. It is worth noting that the distribution of larger CCS peptides was\nsplit into two populations: One with overestimated CCS values predicted by\nLS-MLR and the other with fairly predicted CCS values. This can be mostly\nexplained by the length of the peptides: The peptides with overestimated\nCCS were longer than the other ones (Supplementary Fig. 2). On the other\nhand, the proposed PPLN provided better predictive performance with the\nlower RMSE and the higher correlation coefﬁcient. Figure4b, c shows the\nrelation tom/z and length of the ions. We can see that scatter plots of LS-\nMLR and the others differ, especially in higherm/z and longer-length\nregions. These results indicate that the CCS prediction of longer peptide ions\nwith higherm/z is difﬁcult with simpler prediction models. To establish a\nbetter CCS prediction model, improving the prediction accuracy of these\nions is mandatory.\nThe predicted CCS values and the four statistics are summarized by\ncharge number in Fig.5.F r o mt h eﬁgure, we can see that the predictions for\ntriply and quadruply charged ions were more difﬁcult than the singly and\ndoubly charged ions because the performance of all the three methods\ncompared became worse as the charge number was higher. We can also see\nin Fig.5d that the peptides with CCS values overestimated by LS-MLR are\ntypically quadruply charged. This is consistent with the fact that longer\npeptides tend to have higher charge states. The proposed PPLN achieved the\nbest performance among the methods in all the cases. The results imply the\nhigh applicability of the proposed method.\nFig. 2 | Summary statistics for the peptide dataset prepared from HeLa lysate using seven proteases. aFrequency of peptide charge numbers.b Frequency of peptide\nC-terminal amino acids.c Frequency of peptide N-terminal amino acids.\nFig. 3 | Distribution of 91,677 unique peptide ions.The dots 1, 2, 3, and 4 means\nthe experimental CCS values of singly, doubly, triply, and quadruply charged ions vs.\nm/z, respectively.\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 3\nIt should be noted that the proposed PPLN does not always achieve\nbetter predictive performance than the conventional LSTM-based method,\na l t h o u g hi tp e r f o r m sb e t t e ri nt h er e s u l t so fF i g .4 using the training/test data\nsplitting in this section. Supplementary Table 1 summarizes the average\nperformance metrics of 10 trials including other random splittings of the\n80% training/20% test data and the corresponding results of statistical\ntesting (pairedt-test,p = 0.05). The results show that the performance of the\nPPLN is better or worse depending on the performance metrics. Therefore,\nit can be concluded that the proposed PPLN achieves predictions that are\ncompetitive with the LSTM-based method. This is consistent with the ori-\ng i n a lg o a lo ft h ep a p e rt ol e v e r a g eap retrained deep protein language model\nto achieve reasonable performance.\nWe further analyzed and visualized the contributions of amino acid\npositions using Shapley additive explanation (SHAP) values to make the\nrelation between input sequences and prediction of the proposed PPLN\nmore interpretable. The results are shown in Supplementary Figs. 5 and 6.\nAblation study\nIn this section, we verify necessity of the components of the proposed PPLN\nvia ablation study, where we compared the predictive performance of the\nproposed method with the same method except that a part of the compo-\nnents was removed from it, in order to see if the removed part was\nimportant. Table1 summarizes RMSE and Pearson’s correlation coefﬁcient\nr obtained by the proposed method and methods with the removal.\nWe ﬁrst validate the necessity of including the charge number and the\nmass into the input of the prediction NN in the proposed PPLN. The\nperformance of the method that does not use charge numbers of ions was\nmuch worse than the proposed method, indicating that charge number\nbears important information for CCS value prediction. It is in contrast with\nthe conventional CCS prediction method\n15 which does not use the charge\nnumber information. The method which excludes mass and that which\nexcludes both charge and mass also showed worse performance. From the\nresults, we can argue that the information of charge and mass of the ions is\nvaluable for the CCS prediction.\nWe also validate the necessity of the bidirectional PE by comparing the\npredictive performance of the following four methods: the method with the\nbidirectional PE (i.e., the proposed method), the one with the unidirectional\nPE, the one with the bidirectional aggregation without PE (i.e., settingc\np in\nEq. (4) to be equal to the all-1 vector), and the one adopting aggregation with\nsimple averaging (i.e., the original ESM-1b25, settingcp in Eq. (5)t ob ee q u a l\nto the all-1 vector). From Table1, none of the compared methods showed\nbetter performance than the proposed method. The disuse of the bidirec-\ntional aggregation especially degraded the performance. This implies that\nthe N-terminal and C-terminal sides of the ions have different effects on\npeptide structures and on the resulting CCS values, and those effects can be\nsuccessfully learned by the network with the bidirectional aggregation.\nComparison of execution time for training\nThe proposed PPLN can simplify the training process by using the pre-\ntrained model as the feature extractor, compared with the conventional\nbidirectional LSTM-based method15 that needs training from scratch. We\nnumerically evaluated the executiontime required for training of the pro-\nposed PPLN and the conventional bidirectional LSTM-based method.\nFigure 6 shows time (in seconds) spent on training for each method\nand that required for preprocessing, i.e., feature extraction, for the proposed\nPPLN. We used a Linux computer with two CPUs (Intel Xeon Gold 5320, 26\ncores, 2.2 GHz base clock) and 256 GB RAM. The preprocessing time means\nthe time taken to obtain the features for all peptides in the dataset. The\ntraining time was measured when 20%, 50%, and 80% of peptides in the\ndataset were used for training (the numbers of training samples were 18,335,\n45,839, and 73,342, respectively). The average time of three runs is shown in\ntheﬁgure. All measured values were within plus or minus 120 s of the shown\naverage values. For the bidirectional LSTM-based model, the results were\nobtained using the recommended conﬁguration provided in a GitHub\nrepository tied to the original paper\n15, where the number of training itera-\ntions isﬁxed to 55,000 regardless of the number of training samples. The\nresults of comparison when the total number of the training data used is the\nsame are shown in Supplementary Fig. 7.\nFrom Fig.6, the training of the proposed PPLN not including the\npreprocessing was executed in 1/78, 1/30, and 1/18 of the time of the training\nof the conventional bidirectional LSTM-based method, when 20%, 50%,\nand 80% samples were used for training, respectively. Even taking the\npreprocessing time into consideration, execution time for the proposed\nPPLN was reduced to 1/4 to 1/3 of that of the conventional method.\nMoreover, it should be noted that the proposed PPLN achieved test pre-\ndiction with Pearson’s correlation coefﬁcient over 0.99 for all runs. CPU\nFig. 4 | Scatter diagrams and statistics in terms of predicted CCS values. aPredicted CCS values vs. experimental CCS values including Pearson’s correlation coefﬁcient,\nRMSE, MAE, andΔ95% error.b Predicted CCS values vs.m/z. c Predicted CCS values vs. length.\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 4\nusage during the training process was 1057% and 176.1% for the bi-\ndirectional LSTM-based method andthe proposed PPLN, respectively.\nThese were measured per 10 s and averaged over 10 training trials. One can\ntherefore state that the proposed model is also“greener” in terms of\nenergy usage.\nThe execution time for prediction was also measured in terms of 80%\ntraining data case. The average prediction time over three trials was 44.0 s for\nthe bidirectional LSTM-based method and 0.5 s for the proposed PPLN,\nrespectively. In other words, in this training and test data splitting, the\nproposed method could also perform prediction in a shorter time than the\nconventional method, owing to the simple model structure of the\nprediction NN.\nThese results indicate that the proposed PPLN enables a signiﬁcant\nreduction in training and test time by virtue of the simpliﬁcation of back-\nward and forward calculation for training and test, respectively, through the\nuse of the pretrained model, while maintaining high predictive performance\nand lower energy usage.\nCCS prediction for improving peptide identiﬁcation\nIn this section, we show that accurate CCS prediction provided by the pro-\nposed PPLN allows us to improve performance of downstream tasks. We take\nthe peptide identiﬁcation task as our demonstrative example, as it has been\nshown that it is possible to reduce the false hits using the difference between\nthe predicted and experimental CCS values after the candidate sequences are\ndetermined by the search engine\n16,38,39. We used tryptic peptides fromE. coli\nK12 strain BW25113 cells to verify the improvement in the identiﬁcation\nnumber using PPLN, as reported previously16. The additional parameter,\nCCS error, deﬁned as the difference between the predicted and experimental\nvalues divided by the experimental value, was used for the Mascot/Percolator\napproach40. The CCS predictions were performed by both the LSTM-based\nmethod and the PPLN trained with the HeLa dephosphorylated dataset.\nFigure7 shows the Venn diagram of the identiﬁcation results with or without\nCCS error. From theﬁgure, Percolator with CCS error identiﬁed more\npeptide spectrum matches (PSMs) and stripped sequences at 1% FDR\ncompared with Percolator without CCS error, indicating the utility of PPLN-\nbased CCS prediction for peptide identiﬁcation in proteomics.\nDiscussion\nIn this work, we proposed a novel approach to predict the CCS values of\npeptides using a deep learning model called the PPLN. The proposed PPLN\nincorporates a pretrained deep protein language model trained with a large-\nscale database of protein sequences as a feature extractor to perform the\ntraining process in a“greener” manner. Our results demonstrated that the\nproposed PPLN can achieve a more accurate prediction of CCS values\ncompared with the conventional LS-MLR and competitive performance\nwith the bidirectional LSTM-based approach. A remarkable point is that the\ntraining with the PPLN was made in a signiﬁcantly shorter time than with\nthe conventional bidirectional LSTM-based method requiring training from\nscratch. The LSTM-based method includes 0.4M trainable parameters and\nthe PPLN with the architecture used in the experiments of the paper\nincludes 650M frozen and 11M trainable parameters. The PPLN includes a\nlarger number of weights to be optimized, but the computational load for\nforward path and backpropagation is larger for LSTM. That is the reason for\nthe difference in execution times of both models.\nPPLN can also achieve reasonable performance in predicting CCS\nvalues of longer peptides, which is difﬁcult with simple prediction models\nsuch as LS-MLR. This can be attributedto the use of the pretrained protein\nlanguage model, such as ESM-1b, in PPLN: A pretrained protein language\nmodel can capture the complex relationships between amino acid sequences\nand protein structures more effectively than traditional methods, which\nmight contribute to the better prediction performance of PPLN because the\npeptide CCS reﬂects the peptide structure, which is the part of the protein\nstructure. The difﬁculty in predicting triply and quadruple charged peptide\nions is due to the variety of their structures and the small number of them in\nthe training data. The latter problem corresponds to imbalanced data in\nmachine learning\n41, which causes performance degradation for data with\nminor attributes, in our case for highly charged ions. By constructing a\ndataset rich in highly charged ions, one can expect to signiﬁcantly improve\nthe performance of prediction models.\nOur study demonstrates the potential effectiveness of utilizing pre-\ntrained protein language models in predicting various peptide properties in\nproteomic LC/MS experiments, including not only CCS but also peptide\nTable 1 | Summary of ablation study\nStatus RMSE Pearson r\nw/o charge 31.8161 0.9721\nw/o mass 16.7563 0.9914\nw/o charge, mass 38.7438 0.9533\nunidirectional PE 15.6560 0.9927\nbidirectional aggregation w/o PE 15.9156 0.9929\naggregation with simple averaging (original ESM-1b) 16.3448 0.9923\nProposed PPLN 15.1169 0.9930\nThe bold values mean the best of the methods.\nFig. 5 | Predicted CCS values and statistics by charge. aScatter diagram for singly charged ions including Pearson’s correlation coefﬁcient, RMSE, MAE, andΔ95% error.\nb For doubly charged ions.c For triply charged ions.d For quadruply charged ions.\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 5\nretention time, MS/MS fragmentation pattern, and detectability. In addition\nto the improved prediction performance, utilizing a pretrained protein\nlanguage model offers several advantages, such as requiring less training\ndata for accurate predictions, leading to lower computational resource\nrequirements and reducing the time and energy consumption. By utilizing\nthese advantages, transfer learning approaches allow the model to be readily\napplicable to the prediction of CCS values from different experimental setup\nsuch as the use of the ion mobility spectrometer with different modes of\nseparation. Although the PPLN cannot directly incorporate modiﬁed resi-\ndues resulting from chemical and post-translational modiﬁcations— due to\nits reliance on the protein language model, which does not account for these\nmodiﬁcations— it is possible to extend its applicability to modiﬁed peptides\nby considering modiﬁcation tokens (the modiﬁcation position with the CCS\nshift), as reported recently\n42.\nIt has been suggested that model size contributes to downstream task\nperformance27,43 so that the introduction of protein language models with a\nhuge number of parameters, such as ESM-227 and xTrimoPGLM31 instead of\nESM-1b used in our PPLN is expected to offer even better prediction\naccuracy. On the other hand, the magnitude of the pretrained model\ninﬂuences the execution time of preprocessing. The use of smaller models\ndirectly leads to time savings in the training and test processes of the pro-\nposed model. The preprocessing time using ESM-1b was 5316.9/\n91,677 = 0.058 s per peptide ion. The amount of preprocessing time may be\na limitation when the number of peptide ions to be predicted is large. One of\nthe prompt solutions to reduce the prediction time is to replace it with a\nsmaller protein language model. For example, ESM-2\n27 is a model with a\nminimum of 8M and a maximum of 15B parameters. It would be interesting\nto verify prediction accuracy and execution time using a larger or smaller\nprotein language model as the feature extractor in future work. Moreover,\nthe current model is not applicable to variable modi ﬁcations. The\nexploration of applicable models is one of the important future directions.\nThe CCS dataset provided in this paper contains longer and more highly\ncharged peptide ions than that in ref.15. It allows the training of the model on\na wider variety of peptide ions. Note that the architecture of the proposed\nPPLN is not specialized for this dataset. The PPLN also achieves reasonable\nperformance on another dataset, whichi ss h o w ni nS u p p l e m e n t a r yF i g .8 .I n\nother words, the proposed PPLN is robust to data of different nature while\nenjoying the beneﬁts of“greener” training processes.\nIn summary, our approach represents a signiﬁcant advance in the\nprediction of peptide CCS values. By leveraging pretrained protein language\nm o d e l s ,w eh a v es h o w nt h a ti ti sp o s s i b l et oa c c u r a t e l yp r e d i c tC C Sv a l u e sf o r\nlonger peptides with short training times. We believe that our approach can\nbe extended to predict other peptide properties and that the use of pre-\ntrained models will become increasingly important for efﬁcient and accurate\npeptide property prediction.\nMethods\nMaterials\nTitanium dioxide (TiO2) beads were obtained from GL Sciences (Tokyo,\nJapan). 2-amino-2-(hydroxymethyl)-1,3-propanediol hydrochloride (Tris-\nHCl), acetonitrile, acetic acid, ammonium bicarbonate (ABC), tri-\nﬂuoroacetic acid, lysyl endopeptidase (Lys-C), V8 protease (Glu-C), and\nother chemicals were purchased from Fujiﬁlm Wako (Osaka, Japan).\nModiﬁed trypsin and chymotrypsin were purchased from Promega\n(Madison, WI, USA). Asp-N was purchased from Roche diagnostics\n(Indianapolis, IN, USA). Lys-N waspurchased from Thermo Fisher Sci-\nentiﬁc (Waltham, MA, USA). LysargiNase was purchased from Merck\n(Darmstadt, Germany). Alkaline phosphatase was purchased from Takata\nBio Inc. (Shiga, Japan). Emp ore C8, Empore SDB-XC (poly-\nstyrenedivinylbenzene) and Empore SCX (strong cation exchange)\nextraction disks were purchased fromCDS Analytical (Oxford, PA, USA).\nWater was puriﬁed by a Millipore Milli-Q system (Bedford, MA, USA).\nSample preparation\nT h eH e L aS 3c e l ll i n e( J a p a nH e a l t hSciences Foundation) was cultured in\n10 cm diameter dishes following standard protocols. The cells were col-\nlected, and pelleted down by centrifugation. The cell pellets were suspended\nin a lysis buffer, reduced and alkylated as previously reported\n44.T h es a m p l e s\nwere dilutedﬁv et i m e sw i t h5 0m MA B Cb u f f e ro rt e nt i m e sw i t h1 0m M\nCaCl2 in the case of LysargiNase digestion. The digestion was performed at\n37 °C by adding trypsin, Lys-C, Lys-N, Asp-N, LysargiNase, chymotrypsin\nor Glu-C. The appropriate enzyme-to-protein ratios were used for each\nenzyme (Supplementary Table 2). The resulting peptides were desalted and\npuriﬁed via SDB-XC StageTip according to the previously published\nprotocol\n45,46. The desalted peptides were further processed with C8 Stage-\nTips packed with TiO2 to enrich phosphopeptides as previously reported35.\nPhosphopeptides were eluted with 0.5% piperidine followed by 5% pyrro-\nlidine, acidiﬁed immediately by adding equal volume of 20% phosphoric\nacid (ﬁnal concentration: 10%), and desalted using SDB-XC StageTips\n47.\nEnriched phosphopeptides were fractionated using SCX StageTips as\ndescribed previously36, followed by dephosphorylation with 6 units of\nalkaline phosphatase in 30μL of 100 mM Tris-HCl buffer (pH 9.0), incu-\nbated for 3 h at 37 °C. After the reaction, the buffer was acidiﬁed by adding\n10% TFA 10μL. The samples were desalted using SDB-XC StageTips.\nLC/IMS/MS/MS analysis\nLC/IMS/MS/MS was performed on anUltimate 3000 RSLCnano (Thermo\nFisher Scientiﬁc, Waltham, MA, USA) LC pump coupled with a PAL HTC-xt\n(CTC analytics, Zwingen, Switzerland) autosampler and a timsTOF Pro\n(Bruker Daltonics, Bremen, Germany) mass spectrometer. Peptides were\nseparated on a 15 cm × 100μm in-house-packed with 1.9μm Reprosil-Pur\nAQ C18 beads (Dr. Maisch, Ammerbuch, Germany) column at aﬂow rate of\n500 nL=min with an PRSO-V2 (Sonation, Biberach, Germany) column oven\nheated at 50 °C. Mobile phases A and B were water and 20%/80% water/\nacetonitrile (v/v), respectively, both with 0.5% acetic acid as an ion-pair\nreagent\n48. A total run time was 120 min with gradient starting with a linear\nincrease from 5% B to 40% B over 90 min followed by linear increases to 99% B\nin 1 min. The mass spectrometer was operated in data-dependent PASEF\n9\nmode, with 1 survey TIMS-MS followed by 10 PASEF MS/MS scans per\nacquisition cycle. An ion mobility scan range from 1/K\n0 = 0.6 to 1.5 Vs/cm2\nwas employed with 100 ms ion accumulation and ramp time. Precursor ions\nfor MS/MS analysis were selected and isolated in a window of 2m/z for\nm/z < 700 and 3m/z for m/z > 700. Singly charged ions were excluded from\nthe precursor ions according to theirm/z and 1/K0 values. The TIMS elution\nvoltage was calibrated linearly to obtain the reciprocal of reduced ion mobility\n(1/K0) using three selected ions (m/z = 622, 922, and 1222) of the ESI-L\nTuning Mix (Agilent, Santa Clara, CA, USA)\nFig. 6 | Execution time (in seconds) for the proposed PPLN and the conventional\nbidirectional LSTM-based method in the cases of using 20%, 50%, and 80%\nsamples for training.For the PPLN, the training time for prediction NN and execution\ntime for preprocessing required for obtaining features via ESM-1b are shown.\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 6\nDatabase search and data processing\nMS rawﬁles wereﬁrst processed by Bruker DataAnalysis software to gen-\nerate mgfﬁles. Database search was performed with Mascot version 2.7.0\nagainst Swissprot (downloaded on 20200318) human database containing\nisoforms using the appropriate digestion rules for each protease (Supple-\nmentary Table 1). Carbamidomethyl (C) was set as aﬁxed modiﬁcation, and\nPhospho (STY), Oxidation (M) and Acetyl (Protein N-term) were set as\nvariable modiﬁcations. The peptide tolerance of 20 ppm and MS/MS tol-\nerance of 0.05 Da were used. Number of\n13C was set as 1 to consider\nmonoisotopic+1 peaks as precursor ions. Percolator40 was used for con-\ntrolling the false discovery rates (FDRs) at 1% on both the PSM and unique\npeptide level in terms ofq values. The identiﬁed PSMs were further pro-\ncessed with IonQuant\n37 (version 1.3.6) to re-assign precursor ions. The\nPSMs which could not be assigned to any precursor ions with IonQuant\nwere removed for further analysis. Furthermore, the reduced ion mobility of\npeptide ions was calibrated linearly using three selected ions (352.33,\n761.467, 933.919) which were constitutively detected in all of the raw data, to\nminimize the run-to-run variability of the obtained 1/K\n0 values. The CCS\nvalue Ω was calculated from the obtained value of 1/K0 using the Mason-\nSchamp equation49:\nΩ ¼ 3Ze\n16N0\n2π\nμkBT\n/C18/C19 1=2 1\nK0\n; ð1Þ\nwhere e is the charge on an electron, whereZ is the charge number of\nthe analyte ion, where kB is the Boltzmann constant, where N0 is\nLoschmidt constant, where T is the temperature, and whereμ is the\nreduced mass of the ion and neutralgiven by the harmonic mean of the\nmolecular masses of the drift gas and analyte ion, respectively. Peptide\nions without any variable modiﬁcations were used for analysis. We\nonly considered the most abundant feature for each modi ﬁed\npeptide ion.\nMathematical explanation of feature extraction in PPLN\nAssume that we use a feature extractor which, when fed with a length-l\npeptide ion sequence, outputs a length-l sequencefvpgl\np¼1 of d-dimensional\nfeature vectors. We then propose the following PE, which we term the\nbidirectional PE: the feature vector sequencefv\npgl\np¼1 is aggregated into a\nsingle 2d-dimensional vector/C22v via:\n/C22v ¼ /C22vN\n/C22vC\n/C18/C19\n; ð2Þ\n/C22vN ¼ 1\nl=2\nXl=2\np¼1\ncp /C14 vp; ð3Þ\n/C22vC ¼ 1\nl=2\nXl=2\np¼1\ncp /C14 vlþ1/C0 p; ð4Þ\nwhere ∘ denotes the element-wise (Hadamard) product of vectors, and\nwhere cp ¼ð cp;1; ... ; cp;dÞ> is the encoding vector for positionp from one\nof the terminals. That is,/C22vN summarizes the features from the half of the\namino acid sequence on the N-terminal side, and/C22vC summarizes the\nfeatures from the other half of the amino acid sequence on the C-terminal\nside. (Whenl is odd, one may have to introduce an appropriate rounding of\nthe half-integerl/2. In our implementation used in the experiments, we used\nt h ep y t h o nb u i l t - i nf u n c t i o nround().) These vectors are concatenated to\nform the resulting feature vector/C22v. One can argue that the bidirectional PE\nbears the same spirit as that in ref.15 where they used the bidirectional\nLSTM rather than the (unidirectional) LSTM to deal with peptide\nsequences. As mentioned above, an alternative choice to the bidirectional\nPE might be the unidirectional PE, where thed-dimensional aggregated\nvector /C22v\nu is obtained via:\n/C22vu ¼ 1\nl\nXl\np¼1\ncp/C14 vp: ð5Þ\nExperimental model settings based on pretrained deep protein\nlanguage model\nWe used the pretrained ESM-1b model25 as the feature extractor in PPLN,\nwhose output is a sequence ofd = 1280-dimensional feature vectors. As for\nthe encoding vectorscp used in PE, we considered the following functional\nform:\ncp;i ¼\nsin p\naði/C0 1Þ=d\n/C16/C17 b\nþ γ; i : odd;\ncos p\naði/C0 2Þ=d\n/C16/C17 b\nþ γ; i : even;\n8\n><\n>:\nðp ¼ 1; 2; ... ; l; i ¼ 1; 2; ... ; dÞ\nð6Þ\nwith user-tunable parameters (a, b, γ). This formulation is inspired by the\npositional encoding used in the attention mechanism of the transformer\narchitecture for a deep natural language model\n50.W eu s e d(a, b, γ) = (1000,\n1, 0) in the following experiments.\nWe used PyTorch and adopted minibatch learning with batch size 200.\nT h en u m b e ro fl a y e r so ft h ep r e d i c t i o nN Nw a ss e tt o1 0a n dt h e i r\ndimensions were 1000 except for the last layer that outputs the scalar\nCCS value.\nIt is acceptable to set parameters (a, b, γ) of PE and the architecture of\nthe prediction NN appropriately according to the nature of dataset or\ncomputer resource environments. Performance using other architectures is\ns h o w ni nS u p p l e m e n t a r yT a b l e3 .\nWe trained the prediction NN withAdam optimizer (learning rate:\n0.0003) and the MSE loss function using the training data over 400 epochs,\nand tested the CCS prediction performance of the trained model on the\ntest data.\nFig. 7 |E. colipeptide identiﬁcation results with or\nwithout CCS error.CCS predictions were per-\nformed with the conventional LSTM-based method\nand the proposed PPLN trained with the HeLa\ndephosphorylated dataset.\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 7\nPerformance metrics\nThe deﬁnitions of Pearson’s correlation coefﬁcient r, RMSE, and MAE are\ngiven by:\nr ¼\nPN\nn¼1 ^Ωn /C0 /C22^Ω\n/C16/C17\nΩn /C0 /C22Ω\n/C0/C1\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPN\nn¼1 ^Ωn /C0 /C22^Ω\n/C16/C17 2 PN\nn¼1 Ωn /C0 /C22Ω\n/C0/C1 2\nr ; ð7Þ\nRMSE ¼\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ\n1\nN\nXN\nn¼1\n^Ωn /C0 Ωn\n/C0/C1 2\nvuut ; ð8Þ\nand\nMAE ¼ 1\nN\nXN\nn¼1\n∣ ^Ωn /C0 Ωn∣; ð9Þ\nrespectively, whereN = 18,335 is the number of test samples, whereΩn and\n^Ωn are the experimental and predicted CCS values, respectively, ofnth test\nsample, and where/C22Ω :¼ð 1=NÞPN\nn¼1Ωn and /C22^Ω :¼ð 1=NÞPN\nn¼1 ^Ωn are the\naverages of the experimental and predicted CCS values.Δ9 5 %e r r o ri st h e\ninterval that contains 95% of the peptides in the error distribution.\nData availability\nThe MS raw data and analysisﬁles have been deposited with the Proteo-\nmeXchange Consortium (http://proteomecentral.proteomexchange.org)\nvia the jPOST partner repository51 (https://jpostdb.org)w i t ht h ed a t as e t\nidentiﬁer PXD046201.\nCode availability\nT h es o u r c ec o d ef o rC C Sv a l u ep r e d i c t i o nw i t ht h ep r o p o s e dP P L Ni s\navailable on GitHub (https://github.com/a-nakai-k/PPLN).\nReceived: 25 September 2024; Accepted: 25 April 2025;\nReferences\n1. Aebersold, R. & Mann, M. Mass spectrometry-based proteomics.\nNature 422, 198–207 (2003).\n2. Shuken, S. R. An introduction to mass spectrometry-based\nproteomics. J. Proteome Res.22, 2151–2171 (2023).\n3. Sinitcyn, P. et al. Global detection of human variants and isoforms by\ndeep proteome sequencing.Nat. Biotechnol.41, 1776–1786 (2023).\n4. Choong, W. K., Chen, C. T., Wang, J. H. & Sung, T. Y.iHPDM: in silico\nhuman proteome digestion map with proteolytic peptide analysis and\ngraphical visualizations.J. Proteome Res.18, 4124–4132 (2019).\n5. Shishkova, E., Hebert, A. S. & Coon, J. J. Now, more than ever,\nproteomics needs better chromatography.Cell Syst.3, 321–324\n(2016).\n6. Myung, S. et al. Development of high-sensitivity ion trap ion mobility\nspectrometry time-of-ﬂight techniques: a high-throughput nano-LC-\nIMS-TOF separation of peptides arising from a Drosophila protein\nextract. Anal. Chem.75, 5137–5145 (2003).\n7. Bonneil, E., Pfammatter, S. & Thibault, P. Enhancement of mass\nspectrometry performance for proteomic analyses using high-ﬁeld\nasymmetric waveform ion mobility spectrometry (FAIMS).J. Mass\nSpectrom. 50, 1181–1195 (2015).\n8. Bekker-Jensen, D. B. et al. A compact quadrupole-orbitrap mass\nspectrometer with FAIMS interface improves proteome coverage in\nshort LC gradients.Mol. Cell. Proteom.19, 716–729 (2020).\n9. Meier, F. et al. Parallel accumulation-serial fragmentation (PASEF):\nmultiplying sequencing speed and sensitivity by synchronized scans\nin a trapped ion mobility device.J. Proteome Res.14, 5378–5387\n(2015).\n10. Meier, F. et al. Online parallel accumulation-serial fragmentation\n(PASEF) with a novel trapped ion mobility mass spectrometer.Mol.\nCell. Proteom.17, 2534–2545 (2018).\n11. May, J. C. & McLean, J. A. Ion mobility-mass spectrometry: time-\ndispersive instrumentation.Anal. Chem.87, 1422–1436 (2015).\n12. Gabelica, V. & Marklund, E. Fundamentals of ion mobility\nspectrometry. Curr. Opin. Chem. Biol.42,5 1–59 (2018).\n13. Valentine, S. J., Counterman, A. E. & Clemmer, D. E. A database of 660\npeptide ion cross sections: use of intrinsic size parameters for bona\nﬁde predictions of cross sections.J. Am. Soc. Mass Spectrom.10\n,\n1188–1211 (1999).\n14. Ogata, K. & Ishihama, Y. Extending the separation space with trapped\nion mobility spectrometry improves the accuracy of isobaric tag-\nbased quantitation in proteomic LC/MS/MS.Anal. Chem.92,\n8037–8040 (2020).\n15. Meier, F. et al. Deep learning the collisional cross sections of the\npeptide universe from a million experimental values.Nat. Commun.\n12, 1185 (2021).\n16. Chang, C. H. et al. Sequence-speciﬁc model for predicting peptide\ncollision cross section values in proteomic ion mobility spectrometry.\nJ. Proteome Res.20, 3600–3610 (2021).\n17. Krokhin, O. V. et al. Use of peptide retention time prediction for protein\nidentiﬁcation by off-line reversed-phase HPLC-MALDI MS/MS.Anal.\nChem. 78, 6265–6269 (2006).\n18. Ogata, K., Krokhin, O. V. & Ishihama, Y. Retention order reversal of\nphosphorylated and unphosphorylated peptides in reversed-phase\nLC/MS. Anal. Sci.34, 1037–1041 (2018).\n19. Hilderbrand, A. E. & Clemmer, D. E. Determination of sequence-\nspeciﬁc intrinsic size parameters from cross sections for 162\ntripeptides. J. Phys. Chem. B109, 11802–11809 (2005).\n20. Kaszycki, J. L. & Shvartsburg, A. A. A priori intrinsic PTM size\nparameters for predicting the ion mobilities of modiﬁed peptides.J.\nAm. Soc. Mass Spectrom.28, 294–302 (2016).\n21. Duch, W. Artiﬁcial Neural Networks: Formal Models and Their\nApplications-ICANN 2005: 15th International Conference, Warsaw,\nPoland, September 11-15, 2005, Proceedings(Springer Science &\nBusiness Media, 2005).\n22. Manning, C. D., Clark, K., Hewitt, J., Khandelwal, U. & Levy, O.\nEmergent linguistic structure in artiﬁcial neural networks trained by\nself-supervision. Proc. Natl Acad. Sci. USA117, 30046–30054\n(2020).\n23. Rao, R., Meier, J., Sercu, T., Ovchinnikov, S. & Rives, A. Transformer\nprotein language models are unsupervised structure learners. In\nInternational Conference on Learning Representationshttps://\nopenreview.net/forum?id=fylclEqgvgd (2021).\n24. Dens, C., Adams, C., Laukens, K. & Bittremiuex, W. Machine learning\nstrategies to tackle data challenges in mass spectrometry-based\nproteomics. J. Am. Soc. Mass Spectrom.35, 2143–2155 (2024).\n25. Rives, A. et al. Biological structure and function emerge from scaling\nunsupervised learning to 250 million protein sequences.Proc. Natl\nAcad. Sci. USA118, e2016239118 (2021).\n26. Rao, R. et al. MSA transformer. InProceedings of the 38th\nInternational Conference on Machine Learning139 (eds. Meila, M. &\nZhang, T.) Vol. 139, 8844–8856 (PMLR, 2021).\n27. Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein\nstructure with a language model.Science 379, 1123–1130 (2023).\n28. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M.\nUniﬁed rational protein engineering with sequence-based deep\nrepresentation learning.Nat. Methods16, 1315–1322 (2019).\n29. Rao, R. et al. Evaluating protein transfer learning with TAPE. In\nAdvances in Neural Information Processing Systems32 (eds. Wallach,\nH. et al.), 9689–9701 (Curran Associates, Inc., 2019).\n30. Elnaggar, A. et al. Prottrans: toward understanding the language of life\nthrough self-supervised learning.IEEE Trans. Pattern Anal. Mach.\nIntell. 44, 7112–7127 (2021).\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 8\n31. Chen, B. et al. xTrimoPGLM: uniﬁed 100B-scale pre-trained\ntransformer for deciphering the language of protein.Nat. Methods\nhttps://doi.org/10.1038/s41592-025-02636-z (2025).\n32. Meier, J. et al. Language models enable zero-shot prediction of the\neffects of mutations on protein function. InAdvances in Neural\nInformation Processing Systems34 (eds. Ranzato, M. et al.)\n29287–29303 (Curran Associates, Inc., 2021).\n33. Madani, A. et al. Large language models generate functional protein\nsequences across diverse families.Nat. Biotechnol.41, 1099–1106\n(2023).\n34. Ogata, K., Chang, C.-H. & Ishihama, Y. Effect of phosphorylation on\nthe collisioncross sections of peptide ions in ion mobility\nspectrometry. Mass Spectrom.10, A0093 (2021).\n35. Sugiyama, N. et al. Phosphopeptide enrichment by aliphatic hydroxy\nacid-modiﬁed metal oxide chromatography for nano-LC-MS/MS in\nproteomics applications.Mol. Cell. Proteom.6, 1103–1109 (2007).\n36. Adachi, J. et al. Improved proteome and phosphoproteome analysis\non a cation exchanger by a combined acid and salt gradient.Anal.\nChem. 88, 7899–7903 (2016).\n37. Yu, F. et al. Fast quantitative analysis of timsTOF PASEF data with\nMSFragger and IonQuant.Mol. Cell. Proteom.19, 1575–1585\n(2020).\n38. Teschner, D. et al. Ionmob: a Python package for prediction\nof peptide collisional cross-section values.Bioinformatics 39,\nbtad486 (2023).\n39. Declercq, A. et al. TIMS\n2Rescore: a data dependent acquisition-\nparallel accumulation and serial fragmentation-optimized data-driven\nrescoring pipeline based on MS2Rescore. J. Proteome Res.24,\n1067–1076 (2025).\n40. Käll, L., Canterbury, J. D., Weston, J., Noble, W. S. & MacCoss, M. J.\nSemi-supervised learning for peptide identiﬁcation from shotgun\nproteomics datasets.Nat. Methods4, 923– 925 (2007).\n41. Kaur, H., Pannu, H. S. & Malhi, A. K. A systematic review on\nimbalanced data challenges in machine learning: applications and\nsolutions. ACM Comput. Surv.52,1 –36 (2019).\n42. Peng, F. Z. et al. PTM-Mamba: a PTM-aware protein language model\nwith bidirectional gated Mamba blocks.Nat. MethodsOnline ahead of\nprint (2025).\n43. Hesslow, D., Zanichelli, N., Notin, P., Poli, I. & Marks, D. RITA: a study\non scaling up generative protein sequence models. In The 2022 ICML\nWorkshop on Computational Biology (2022).\n44. Masuda, T., Tomita, M. & Ishihama, Y. Phase transfer surfactant-aided\ntrypsin digestion for membrane proteome analysis.J. Proteome Res.\n7, 731–740 (2008).\n45. Rappsilber, J., Mann, M. & Ishihama, Y. Protocol for micro-\npuriﬁcation, enrichment, pre-fractionation and storage of peptides for\nproteomics using StageTips.Nat. Protoc.2, 1896–1906 (2007).\n46. Ogata, K. & Ishihama, Y. CoolTip: low-temperature solid-phase\nextraction microcolumn for capturing hydrophilic peptides and\nphosphopeptides. Mol. Cell. Proteom.20, 100170 (2021).\n47. Kyono, Y., Sugiyama, N., Imami, K., Tomita, M. & Ishihama, Y.\nSuccessive and selective release of phosphorylated peptides\ncaptured by hydroxy acid-modiﬁed metal oxide chromatography.J.\nProteome Res.7, 4585–4593 (2008).\n48. Battellino, T., Ogata, K., Spicer, V., Ishihama, Y. & Krokhin, O. Acetic\nacid ion pairing additive for reversed-phase HPLC improves detection\nsensitivity in bottom-up proteomics compared to formic acid.J.\nProteome Res.22, 272–278 (2022).\n49. Mason, E. A. & Schamp, H. W. Mobility of gaseous ions in weak\nelectric ﬁelds. Ann. Phys.4, 233–270 (1958).\n50. Vaswani, A. et al. Attention is all you need. InAdvances in Neural\nInformation Processing Systems30 (eds. Guyon, I. et al.) (Curran\nAssociates, Inc., 2017).\n51. Okuda, S. et al. jPOSTrepo: an international standard data repository\nfor proteomes.Nucleic Acids Res.45, D1107–D1111 (2017).\nAcknowledgements\nThis work was supported by JST, CREST Grant Number JPMJCR1862,\nJapan and MEXT KAKENHI Grant Number 23H04924, 24H00740\nand 24H01895. We would like to thank Genki Takahashi, a master’ss t u d e n t\nat the Graduate School of Informatics, Kyoto University, for verifying codes\nand formulae in this paper, and for conducting preliminary experiments.\nAuthor contributions\nA.N.-K., T.T. and Y.I. designed the experiments of the deep learning models,\nanalyzed the data, and interpreted the results. A.N.-K. performed all\ncomputational experiments. Y.I. and T.T. designed the main conceptual\nideas. K.O. and Y.I. collected all experimental data, and analyzed the results\nof Fig.7. All authors wrote the manuscript and discussed the results. Y.I. and\nT.T. supervised the project and provided resources for the study.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s42004-025-01540-z.\nCorrespondenceand requests for materials should be addressed to\nYasushi Ishihama or Toshiyuki Tanaka.\nPeer review informationCommunications Chemistrythanks Ulises\nHernández Guzmán, Ceder Dens, Pavel Sinitcyn, Wout Bittremieux, and the\nother, anonymous, reviewers for their contribution to the peer review of this\nwork. Peer reviewer reports are available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s42004-025-01540-z Article\nCommunications Chemistry|           (2025) 8:137 9",
  "topic": "Collision",
  "concepts": [
    {
      "name": "Collision",
      "score": 0.6486217379570007
    },
    {
      "name": "Section (typography)",
      "score": 0.6007452011108398
    },
    {
      "name": "Computer science",
      "score": 0.5432426333427429
    },
    {
      "name": "Natural language processing",
      "score": 0.45973220467567444
    },
    {
      "name": "Cross section (physics)",
      "score": 0.45017170906066895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4274349808692932
    },
    {
      "name": "Programming language",
      "score": 0.27589669823646545
    },
    {
      "name": "Physics",
      "score": 0.14715757966041565
    },
    {
      "name": "Astronomy",
      "score": 0.07469695806503296
    },
    {
      "name": "Operating system",
      "score": 0.07451224327087402
    }
  ]
}