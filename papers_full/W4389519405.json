{
    "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
    "url": "https://openalex.org/W4389519405",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2892984900",
            "name": "Moxin Li",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2108524549",
            "name": "Wenjie Wang",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2323056039",
            "name": "Fuli Feng",
            "affiliations": [
                "Anhui Institute of Information Technology",
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2110093419",
            "name": "Yixin Cao",
            "affiliations": [
                "Singapore Management University"
            ]
        },
        {
            "id": "https://openalex.org/A2143835720",
            "name": "Jizhi Zhang",
            "affiliations": [
                "University of Science and Technology of China"
            ]
        },
        {
            "id": "https://openalex.org/A2730258684",
            "name": "Tat-Seng Chua",
            "affiliations": [
                "National University of Singapore"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4221142421",
        "https://openalex.org/W3034850762",
        "https://openalex.org/W4386566526",
        "https://openalex.org/W4379958452",
        "https://openalex.org/W4385570451",
        "https://openalex.org/W4385569978",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W4309208603",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W4287891024",
        "https://openalex.org/W4389520508",
        "https://openalex.org/W4287028759",
        "https://openalex.org/W4321854923",
        "https://openalex.org/W4385573003",
        "https://openalex.org/W4288548690",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2970062726",
        "https://openalex.org/W4287891464",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W2973054254",
        "https://openalex.org/W4385565015",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4286981949",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W4318908878",
        "https://openalex.org/W4377163996",
        "https://openalex.org/W4385572845",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3034446185",
        "https://openalex.org/W2962854379",
        "https://openalex.org/W4389520756",
        "https://openalex.org/W4225619898",
        "https://openalex.org/W4385573750",
        "https://openalex.org/W3160638507",
        "https://openalex.org/W4322832290",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4308244910",
        "https://openalex.org/W4320341763",
        "https://openalex.org/W2998617917",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3205717164",
        "https://openalex.org/W2963341956"
    ],
    "abstract": "Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we propose a new problem of robust prompt optimization for LLMs against distribution shifts, which requires the prompt optimized over the labeled source group can simultaneously generalize to an unlabeled target group. To solve this problem, we propose Generalized Prompt Optimization framework , which incorporates the unlabeled data from the target group into prompt optimization. Extensive experimental results demonstrate the effectiveness of the proposed framework with significant performance improvement on the target group and comparable performance on the source group.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1539–1554\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRobust Prompt Optimization for Large Language Models Against\nDistribution Shifts\nMoxin Li1, Wenjie Wang1∗, Fuli Feng 2, 3, Yixin Cao 4, Jizhi Zhang 2\nTat-Seng Chua1\n1National University of Singapore, 2University of Science and Technology of China\n3Institute of Dataspace, Hefei, Anhui, China, 4Singapore Management University\nlimoxin@u.nus.edu, wangwenjie@u.nus.edu, fulifeng93@gmail.com,\ncaoyixin2011@gmail.com, cdzhangjizhi@mail.ustc.edu.cn, dcscts@nus.edu.sg\nAbstract\nLarge Language Model (LLM) has demon-\nstrated significant ability in various Natural\nLanguage Processing tasks. However, their ef-\nfectiveness is highly dependent on the phrasing\nof the task prompt, leading to research on auto-\nmatic prompt optimization using labeled task\ndata. We reveal that these prompt optimization\ntechniques are vulnerable to distribution shifts\nsuch as subpopulation shifts, which are com-\nmon for LLMs in real-world scenarios such as\ncustomer reviews analysis. In this light, we\npropose a new problem of robust prompt opti-\nmization for LLMs against distribution shifts,\nwhich requires the prompt optimized over the\nlabeled source group can simultaneously gen-\neralize to an unlabeled target group. To solve\nthis problem, we propose Generalized Prompt\nOptimization framework , which incorporates\nthe unlabeled data from the target group into\nprompt optimization. Extensive experimental\nresults demonstrate the effectiveness of the pro-\nposed framework with significant performance\nimprovement on the target group and compara-\nble performance on the source group.\n1 Introduction\nLLMs have gained significant attention for their\nremarkable performance in a broad range of Nat-\nural Language Processing (NLP) tasks (Ouyang\net al., 2022; Chung et al., 2022; Brown et al., 2020;\nTouvron et al., 2023). This success has led to a\nshift in the paradigm of solving NLP tasks, mov-\ning away from training task-specific deep models\ntowards developing task-specific strategies to effec-\ntively utilize LLMs (Wei et al., 2022; Kojima et al.,\n2022; Wang et al., 2022a; Ye et al., 2023b). In the\nnew paradigm, the prompt becomes a crucial factor\nin ensuring the effectiveness of LLM on the NLP\ntask, since even slight variations in prompt phras-\ning can largely affect LLM output (Reynolds and\n∗ ∗Corresponding author.\nupdate\n[Prompt]Light weight laptop with new amazing features, battery life is awesome.\nLLMPrompt\nProvide a sentiment analysis of a given input text ...\nProvide feedback on various products or experiences ...!\" yAccuracy = 78%\nTraining Samples\nfeedback\nStep 1\nStep n\nPrompt Optimization\nDeployment\nTask-specific optimized prompt\nTest samples w/ the same distribution(source group)LLM\nTest samples w/ adifferent distribution(target group)LLM\n!\" y\n!Accuracy = 85%\n\"Accuracy = ？!\" y\nFigure 1: Illustration of prompt optimization under dis-\ntribution shifts. Existing prompt optimization solutions\naim to improve the LLM performance on the training\ndata, while it is unclear whether the optimized prompt\ncan be generalized to testing data of the same task but\nwith distribution shifts.\nMcDonell, 2021; Gao et al., 2021), making prompt\noptimization a promising research direction.\nExisting research has explored automatic prompt\noptimization methods to eliminate manual effort\nin identifying effective prompts for a given task.\nThese methods can be gradient-based or gradient-\nfree, depending on the availability of model gradi-\nents. Gradient-based methods optimize the prompt\nby calculating its gradients through the LLM\n(Schick and Schütze, 2021b,a; Hu et al., 2022).\nGradient-free methods update prompts based on\nLLM outputs using techniques such as an itera-\ntive search-and-select over the prompt space (Zhou\net al., 2023; Prasad et al., 2022; Pryzant et al.,\n2023). This work focuses on gradient-free prompt\noptimization as LLMs are evolving into black-box\nAPI services (Sun et al., 2022).\nCurrent gradient-free prompt optimization meth-\nods ignore distribution shifts (Wang et al., 2023),\n1539\nwhere the data an LLM serves may differ from\nthe labeled data for prompt optimization. Real-\nworld NLP applications often encounter distribu-\ntion shifts, such as new user groups with distinct\nlinguistic habits in customer review analysis. It is\nunclear if prompts hinder the robustness of LLMs\nagainst distribution shifts. To answer this question,\nwe conduct experiments with the representativegpt-\n3.5-turbo-0301 model and prompts optimized by\nAPE (Zhou et al., 2023) over paired data groups\nwith distribution shifts. Results on 30 pairs of data\ngroups from six tasks show the risk of significant\nperformance gaps under certain distribution shifts.\nBased on this finding, we propose a new robust\nprompt optimization problem, which aims to opti-\nmize task-specific prompts with consideration of\nperformance on both source and target groups un-\nder different distributions. Given an NLP task such\nas sentiment analysis, our problem setting has a\nlabeled source group similar as the conventional\nprompt optimization setting and a unlabeled target\ngroup. We keep the target group unlabeled for the\nconsideration that distribution shifts happen along\ntime in practice. Labeling the newly coming target\ngroup will cause unnecessary labor cost and latency.\nAccordingly, the main challenge for solving this ro-\nbust prompt optimization problem is incorporating\nunlabeled data into prompt optimization.\nTo this end, we propose the Generalized Prompt\nOptimization (GPO) framework to obtain a task-\nspecific prompt for both source and target groups.\nTo jointly considering the two groups in prompt op-\ntimization, the key lies in labeling the target group\nin an automatic and reliable manner by adapting\nknowledge from the labeled source group. Towards\nthis goal, we leverage the strong power of LLM\nin zero-shot labeling, and prompt ensemble to en-\nhance the labeling robustness. Experimental results\non three tasks demonstrate the effectiveness of our\nframework in improving the performance on the\ntarget group and simultaneously preserving a com-\nparable performance on the source group. To sum\nup, our contributions are threefold:\n• We reveal the robustness issue of prompt opti-\nmization against distribution shifts and propose\na new robust prompt optimization problem.\n• We propose the Generalized Prompt Optimiza-\ntion framework, which generates robust prompts\nconsidering both labeled and unlabeled data.\n• We conduct extensive experiments on three NLP\ntasks, validating the rationality and effectiveness\nof our proposed framework.\n2 Preliminary Experiments\nPrompt optimization aims to find the best prompt\np that can instruct LLMs to predict the output y\nbased on the concatenation of p and task input\nx, where x,y and p are all sequences of tokens.\nFormally, given an NLP task with a dataset{(x,y)}\nfollowing a distribution P, the goal is to obtain\npo = arg max\np∈Z\nE(x,y)∼P [r(LLM(p,x),y)], (1)\nwhere Zdenotes the prompt optimization space\nand ris the evaluation metric to compare the LLM\noutput with the ground truth output y, e.g., Accu-\nracy. Existing studies usually leverage gradient-\nbased or gradient-free methods to automatically\noptimize the prompts. Since LLMs are evolving\ninto black-box API services, gradient-free methods\nbecome increasingly important. However, they ig-\nnore distribution shifts between training and testing\ndata. In this light, we conduct controlled experi-\nments to answer the following research question:\nAre prompts optimized by existing gradient-free\nmethods robust to distribution shifts?\n2.1 Evaluation Protocol\nWe conduct the controlled experiments between\na pair of data groups with distribution shifts, i.e.,\na source group {(xs,ys)}following a distribution\nPs, and a target group {(xt,yt)}with a distribu-\ntion Pt, where Pt ̸= Ps. We intend to examine\nwhether the prompt ps optimized on the source\ngroup can generalize to the target group. Specifi-\ncally, givenps and pt optimized on the target group,\nwe compare the performance of ps on the target\ngroup E(x,y)∼Pt[r(LLM(ps,x),y)] with that of pt\nE(x,y)∼Pt[r(LLM(pt,x),y)].\nDatasets. We select 16 datasets from six popu-\nlar NLP tasks, where each pair of groups under\nthe same task is treated as the source and tar-\nget groups. Following recent out-of-distribution\n(OOD) research (Yang et al., 2022), we take each\ndataset as a group and regard different backgrounds\nand topics across the datasets as the distribution\nshifts. For the sentiment analysis task, we adopt\nYelp (Zhang et al., 2015), Flipkart (Vaghani and\nThummar, 2023), IMDB (Maas et al., 2011) and\nAmazon (Zhang et al., 2015) of different topics.\n1540\nSource\nTarget MNLI ANLI\nMNLI 73.4±1.0 45.4±1.9\nANLI 73.3±1.3 46.0±1.5\n(a) Natural language inference\nSource\nTarget RTE HANS\nRTE 78.3±0.8 67.2±1.1\nHANS 79.0±0.8 68.4±1.8\n(b) Textual entailment\nSource\nTarget DSTC7 Ubuntu DialogMuTual\nDSTC7 58.4±0.8 78.9±0.3 74.2±2.2\nUbuntu Dialog56.9±1.3 78.7±0.5 74.4±2.1\nMuTual 52.2±4.4 74.7±6.0 76.7±3.4\n(c) Dialog\nTable 1: Results for tasks without large generalization\nperformance gap across groups.\nFor the natural language inference task, we uti-\nlize MNLI (Williams et al., 2018), and ANLI (Nie\net al., 2020) which is an adversarial dataset for\nMNLI. For the textual entailment, we use RTE\n(Wang et al., 2018) and its OOD dataset HANS\n(McCoy et al., 2019). For commonsense QA, we\nuse SocialIQA (Sap et al., 2019), PIQA (Bisk et al.,\n2020), and OpenbookQA (Mihaylov et al., 2018),\nwhich focus on different types of commonsense\nknowledge. For the multi-turn dialog reasoning,\nwe use DSTC7 (Gunasekara et al., 2019), Ubuntu\nDialog (Lowe et al., 2015), and MuTual (Cui et al.,\n2020). Besides, for the numerical QA task, we use\nthe samples of two different answer types (i.e., nu-\nmerical values and text spans) in DROP (Dua et al.,\n2019) as two groups. See Appendix A.1 for details.\nExperimental Setup. We adopt APE (Zhou et al.,\n2023), an effective gradient-free prompt optimiza-\ntion method, for prompt generalization analysis.\nTo highlight the effect of prompts, we conduct ex-\nperiments under the zero-shot setting without in-\ncontext examples. For the backbone LLMs, we\nleverage gpt-3.5-turbo-0301 by calling the OpenAI\nAPI1. For all classification tasks (all tasks except\nfor DROP), we use accuracy as the evaluation met-\nric. For DROP, we utilize its standard evaluation\nmetric — F1. Following the setting of APE, we\nrandomly sample N-shot training and N-shot vali-\ndation samples for prompt optimization, and repeat\nthe experiments for five runs with different sampled\n1https://chat.openai.com/.\nSource\nTarget Yelp Flipkart IMDB Amazon\nYelp 79.7±0.7 78.4±1.9 87.1±1.9 88.4±1.9\nFlipkart 69.1±8.7 85.1±2.9 85.2±9.4 85.9±12.5\nIMDB 71.1±8.2 76.9±13.4 91.9±0.9 90.4±5.2\nAmazon 75.5±1.5 85.6±2.1 91.5±0.8 93.5±1.4\n(a) Sentiment analysis\nSource\nTarget SocialIQA PIQA OpenbookQA\nSocialIQA 75.6±1.4 82.0±6.0 71.2±5.2\nPIQA 68.9±6.9 83.6±2.9 69.2±5.1\nOpenbookQA 79.9±1.0 84.5±1.6 80.1±2.4\n(b) Commonsense QA\nSource\nTarget Number Spans\nNumber 51.9±2.8 20.1±1.3\nSpans 57.7±2.9 63.1±2.2\n(c) DROP\nTable 2: Results for tasks with significant generalization\nperformance gap across groups. Bold font indicates the\nlargest value for each column.\ndata to report the averaged results. More implemen-\ntation details can be found in Appendix A.2.\n2.2 Experimental Results\nDemonstration of Generalization Performance\nGap. Table 1 shows the tasks without a large\ngeneralization gap between the performance of\nprompts ps and pt, and Table 2 shows the tasks\nwith large gaps (Accuracy gap >8.0) on some\ngroups. The row headers refer to the source groups\nfor prompt optimization while the column headers\nshow the target groups to test optimized prompts.\nThe generalization performance gap between ps\nand pt can be observed by comparing the values in\nthe same column.\nFrom the tables, we can observe: 1) The gener-\nalization performance gap may not exist for pre-\nviously studied OOD and adversarial groups (see\nTable 1), including the groups of the natural lan-\nguage inference and the textual entailment tasks.\nThis is possibly attributed to the strong generaliza-\ntion ability of LLMs. 2) However, under some data\ngroups of Table 2 such as the sentiment analysis\ndatasets (e.g., Flipkart and Yelp) and the common-\nsense QA datasets with different topics (e.g., PIQA\nand OpenbookQA), and the DROP groups with\ndifferent answer types, there are still significant\ngeneralization performance gaps, demonstrating\nthe existence of the generalization issue of prompt\noptimization. 3) Surprisingly, the prompt ps op-\n1541\nSource\nTarget Yelp Flipkart IMDB Amazon\nYelp - 0.33 1.62 1.62\nFlipkart 0.30 - 0.57 0.56\nIMDB 0.25 0.29 - 0\nAmazon 0.25 0.27 0 -\n(a) Label distribution shifts. Smaller values indicate less distri-\nbution shifts.\nSource\nTarget Yelp Flipkart IMDB Amazon\nYelp - 0.65 0.73 0.76\nFlipkart 0.59 - 0.55 0.63\nIMDB 0.70 0.63 - 0.81\nAmazon 0.71 0.70 0.78 -\n(b) Input similarity. Larger values indicate less distribution shifts.\nTable 3: Results for (a) label distribution shifts (b) input\nsimilarity of the sentiment analysis datasets. Bold font\nindicates the least distribution shift for each column.\ntimized from the source group does not always\nperform worse than the prompt pt optimized on the\ntarget group. In Table 2(b), ps from OpenbookQA\nperforms even better than pt for SocialIQA. Be-\nsides, for DROP in Table 2(c), ps from Spans also\nperforms better than pt from Number. In the fol-\nlowing section, we try to explore the reasons for\nthe above three observations.\nExploration on the Factors Affecting Prompt\nRobustness. Based on the above observations,\nwe further explore two research questions.\nQ1: Why do the prompts optimized on source\ngroups perform differently on a target group?\nQ2: Why does the prompt optimized on the source\ngroup perform even better than the prompt opti-\nmized on the target group in some cases?\nFor Q1, we conjecture that the varied perfor-\nmance gaps are attributed to different distribution\nshifts between the source and target groups. To\nverify this, we examine two metrics to measure\ntwo kinds of distribution shifts: 1) the label shifts\nmeasured by the KL divergence, and 2) the input\nsimilarity quantified by the n-gram similarity of\nthe input corpora of the two groups. Their detailed\nimplementation is illustrated in Appendix A.3. We\nshow the results of the sentiment analysis task as\nan example in Table 3. We can observe that the\nsmallest label distribution shifts and the largest in-\nput similarity in Table 3 generally coincide with\nthe best generalization performance on each tar-\nget group in Table 2, indicating the correlation\nbetween distribution shifts and generalization per-\nSocialIQA PIQA OpenbookQA\nword 1-gram 0.43 0.51 0.58\nchar 4-gram 0.50 0.60 0.65\n(a) The n-gram diversity.\nSource\nTarget SocialIQA PIQA OpenbookQA\nSocialIQA - 0.39 0.38\nPIQA 0.47 - 0.46\nOpenbookQA 0.51 0.52 -\n(b) The word 1-gram coverage ratio between groups.\nSource\nTarget SocialIQA PIQA OpenbookQA\nSocialIQA - 0.51 0.51\nPIQA 0.60 - 0.58\nOpenbookQA 0.66 0.64 -\n(c) The character 4-gram coverage ratio between groups.\nTable 4: Evaluation on (a) the n-gram diversity and\n(b) word 1-gram coverage ratio (c) character 4-gram\ncoverage ratio of commonsense QA datasets to study\nthe even higher generalization performance. Bold font\nindicates the largest value for each column.\nformance. Nevertheless, the two metrics cannot\nperfectly explain the performance on all tasks (cf.\nAppendix A.3). Therefore, Q1 is still a challenging\nresearch question, requiring further exploration in\nfuture work.\nFor Q2, we conjecture that the outstanding gen-\neralization performance is because a source group\nwith large diversity covers heterogeneous patterns\nin the target group, leading to a more robust prompt\nps than pt. To explore this, we measure the het-\nerogeneity of source and target groups by calcu-\nlating the percentage of unique n-grams, and the\npercentage of n-grams of the target group covered\nby the source group. For illustration, we present\nthe results of the commonsense QA task in Table 4.\nFrom Table 4(a), we can observe that OpenbookQA\nhas the most diverse input according to the n-gram\nstatistics. Moreover, OpenbookQA covers a large\nproportion of n-grams of SocialIQA and PIQA.\nThese partly explain the superiority of the prompts\noptimized on OpenbookQA (see Table 2).\n3 Robust Prompt Optimization\nIn this section, we first formulate a robust prompt\noptimization problem and propose a GPO frame-\nwork to enhance the robustness of the prompts.\n3.1 Problem Definition\nTo enhance the generalization ability of prompts,\nwe propose a robust prompt optimization prob-\n1542\nSource group data !\"\nMy friend was givenan instruction. Based on the instruction, I gave him several inputs, and he generated the corresponding outputs. Here are the input-output examples:[Input: … Output: …]The instruction is to\nMeta Prompt\nLLM\nEnsemble Labeling\nK prompts\nUnlabeled target group data{$%}\nLLMLabeled target group data !%∗ Consistency > (\nProvide feedback on various products or experiences ...\nUpsamplingAPE\nOptimized Prompt\nProvide feedback on various products or experiences ...\nProvide feedback on various products or experiences ...\n2\n1 1 1\n2\n23\n3\n3\nFigure 2: The GPO Framework.\nlem. Specifically, given an NLP task such as sen-\ntiment analysis, it aims to optimize a task-specific\nprompt for the data groups with different distri-\nbutions. We consider the popular scenario where\na source group Gs = {(xs,ys)}following a dis-\ntribution Ps and {xt}in a unlabeled target group\nGt = {(xt,yt)} ∼Pt (Pt ̸= Ps) are available\nwhile {yt}is unseen during prompt optimization.\nThe objective becomes utilizing Gs = {(xs,ys)}\nand {xt}to optimize a task-specific prompt robust\nto the samples from either Ps or Pt.\nReasons for Access to Unlabeled Target Group.\nIn a real-world deployment, LLMs continually en-\ncounter the testing data with distribution shifts. Col-\nlecting the input features {xt}of the target group\nis feasible. For example, when using LLMs as web\nservices to solve user queries of certain NLP tasks,\nit is easy to collect extensive user queries as unla-\nbeled target groups. However, labeling {xt}may\nbe time-consuming and costly, and thus we intend\nto optimize robust prompts without the labels of\nthe target group.\nA Task-Specific Prompt vs. One Prompt for\nEach Group. To tackle the generalization issue\nof optimized prompts, an intuitive approach is to\noptimize a separate prompt for each data group, yet\nthis simplistic approach faces several limitations\nin real scenarios. In real-world deployment, it not\nonly requires additional computation costs to con-\nstruct more prompts, but also needs to accurately\nclassify each testing sample into the appropriate\ngroup of the same distribution, thereby resulting\nin increased computation costs, latency, and new\nchallenges for precise group classification. Further-\nmore, the collected source group data cannot cover\nall potential target groups, and the prompts opti-\nmized on the source groups may inevitably test on\nthe examples from previously unseen groups. Thus,\nwe aim at improving the generalization ability of\none task-specific prompt across different groups.\n3.2 GPO Framework\nTo obtain a robust prompt for both the source and\ntarget groups, it is natural to jointly consider Gs\nand Gt for prompt optimization. However,Gt lacks\nthe labels {yt}that are commonly required by the\ngradient-free optimization methods (refer to Ta-\nble 5 for the inferior results without labeling). With\nthe impressive capabilities of LLMs on zero-shot\nlabeling, we propose to utilize LLMs to label {xt}.\nConsidering that noisy labels may damage the qual-\nity of optimized prompts, we further present two\nstrategies to improve labeling accuracy.\nAs illustrated in Figure 2, we first propose aMeta\nPrompt to instruct LLMs to acquire knowledge\nfrom the labeled source group and generate a series\nof prompts. Thereafter, we utilize a prompt ensem-\nble labeling strategy to apply generated prompts\nto an LLM for precise labeling of {xt}. In detail,\nwe derive a three-step framework to perform the\nlabeling with two strategies, and then conduct joint\nprompt optimization as shown in Figure 2.\n1. Prompt Generation via Meta Prompt . Fol-\nlowing APE, we utilize a Meta Prompt to ask\nLLM to generate prompts for labeling by feed-\ning the examples of Gs (see an example in Fig-\nure 2). Based on strong language understanding\nand reasoning abilities, LLMs can infer the re-\nlationships between the inputs and outputs of\nthe examples and provide general and precise\ntask prompts. We use different splits of Gs to\ngenerate Kdifferent prompts in total.\n2. Prompt Ensemble Labeling Strategy. Given\nKprompts, we utilize each of them to label{xt}\nwith an LLM, and thus obtain K candidate la-\nbels for each example. We adopt an ensembling\nstrategy and select the label with the highest\nconsistency among the K candidate labels for\neach example. Besides, inspired from Wang\net al. (2022a), we set a consistency threshold\nT ∈[0,1] to only accept the labeled examples\nthat have more thanT percent of prompts agreed\non the label. Eventually, we obtain a filtered la-\nbeled set G∗\nt for the target group.\n1543\n3. Joint Prompt Optimization. Finally, we mix\nGs and G∗\nt to run APE for joint prompt opti-\nmization and obtain the final optimized prompt.\nAs G∗\nt may have fewer samples than Gs after\nfiltering with T, we perform a random upsam-\npling on G∗\nt to have the same data number asGs\nbefore running APE. A brief illustration about\nAPE can be found in Appendix A.2.\n4 Experiments\n4.1 Setup\nDatasets. We experiment GPO with three tasks:\nsentiment analysis, commonsense QA, and DROP.\nFor each task, we select a pair of groups with gen-\neralization performance gap as source and target\ngroups, and ablate the labels for the target groups.\nCompared Methods. We adopt the following base-\nline methods: 1) APE; 2) APO (Pryzant et al.,\n2023), the state-of-the-art gradient-free prompt op-\ntimization method for LLM; 3) APE-ut, a naive\ngeneralization solution by incorporating the unla-\nbeled target group input into APE; 4) the Upper\nBound, which represents the performance of the\nprompt optimized on the target group data with\nground-truth labels by APE; and 5) our proposed\nGPO; We also show the results of simple human-\nwritten prompts that are general for the task, and\nthe revised versions by PromptPerfect2 which is an\nautomatic prompt engineering website.\nEvaluation Protocol. We utilize two strategies\nfor testing: Top 1 and Ensemble. Top 1 refers to\nusing the single optimized prompt with the best\nvalidation performance, while Ensemble refers to\nlabeling with all obtained K prompts and accept\nthe output with the most agreement on the prompts.\nWe utilize the same N-shot data as the preliminary\nexperiments and also report the averaged results\nfor five runs. More implementation details are il-\nlustrated in Appendix A.4.\n4.2 Performance Comparison\nCompare to Generated Prompts. From Table 5,\nwe can observe the followings: 1) GPO achieves\nsuperior performance for all target groups in both\nTop 1 and Ensemble testing, validating its effective-\nness. However, there is still space for improvement\ntowards the Upper Bound for all tasks, showing the\nchallenge of the generalization problem. 2) GPO\nachieves comparable source group performance for\nall tasks, showing its improvement on the target\n2https://promptperfect.jina.ai.\nYelp (Source) Flipkart (Target)\nTop 1 Ensemble Top 1 Ensemble\nAPE 79.7±0.7 79.7±1.0 78.4±1.9 81.3±1.4\nAPO 78.9 ±0.5 79.7±0.8 74.7±3.0 76.4±1.4\nAPE+ut 78.9 ±1.4 78.8±1.4 80.3±2.0 80.7±2.1\nGPO 79.1 ±0.7 78.7±0.9 80.5±2.1 84.5±2.0\nUpper Bound - - 85.1 ±2.9 87.2±0.5\n(a) Sentiment analysis.\nSocialIQA (Source) OpenbookQA (Target)\nTop 1 Ensemble Top 1 Ensemble\nAPE 75.6 ±1.4 69.6±5.3 71.2±5.2 74.8±3.2\nAPO 76.1 ±2.7 72.3±2.6 72.4±2.5 66.1±7.2\nAPE+ut 77.9±1.3 78.9±0.8 77.5±3.0 79.2±1.2\nGPO 76.7 ±2.0 78.9±1.2 78.7±3.3 79.7±0.8\nUpper Bound - - 80.1 ±2.4 80.8±1.1\n(b) Commonsense QA.\nNumber (Source) Spans (Target)\nTop 1 Ensemble Top 1 Ensemble\nAPE 51.9 ±2.8 51.0±3.2 20.1±1.3 18.2 ±0.2\nAPO 55.7±0.8 54.5±2.1 20.2±2.4 20.0 ±2.2\nAPE+ut 52.0 ±1.8 53.1±1.2 16.1±3.5 17.7 ±2.8\nGPO 52.2 ±6.0 53.6±3.0 27.7±12.0 26.7±4.9\nUpper Bound - - 63.1 ±2.2 63.7 ±0.8\n(c) DROP.\nTable 5: Results of the compared methods. Bold font\nindicates the best performance for each column.\ngroup does not largely hinder the source group.\nCompared with APE, GPO shows increased per-\nformance on the source groups of SocialIQA and\nNumber by incorporating the target group data,\nwhich is in line with the finding in Table 2. 3)\nAcross baselines, APO outperforms APE on the\nsource groups of the last two tasks and achieve com-\nparable performance on sentiment analysis, show-\ning its effectiveness for prompt optimization. How-\never, the generalization ability is only comparable\nto APE since APO performs worse than APE on\nseveral target groups. 4) APE-ut achieves improved\ntarget group performance for the first two task, indi-\ncating the benefit of incorporating unlabeled target\ngroup data for generalization. However, for Spans\nwhere obtaining accurate target labels is challeng-\ning (as shown by the low F1 values), APE-ut largely\nunderperforms GPO, showing the importance of\ntarget group labeling especially for difficult tasks.\nCompare to Human-written Prompts. From\nTable 6, we further observe that GPO outperforms\nhuman-written prompts and PromptPerfect for sen-\ntiment analysis and commonsense QA tasks. How-\never, on the most difficult task DROP, GPO under-\nperforms human-written prompts. This is poten-\n1544\nYelp (Source) Flipkart (Target)SocialIQA (Source) OpenbookQA (Target)Number (Source) Spans (Target)\nHuman 78.7 80.0 71.3 60.0 54.9 37.1\nPromptPerfect77.3 83.3 74.7 64.0 54.0 26.9\nGPO best 78.7 84.5 78.9 79.7 52.2 27.7\nTable 6: Performance comparison for the human-written prompts, PromptPerfect and the more effect testing strategy\nof GPO (Top 1 or Ensemble, denoted as GPO best). Bold font indicates the best performance for each column.\ntially because the inaccurate labels for Spans hinder\nthe prompt optimization. Similarly, PromptPerfect\nalso fail to optimize human-written prompts for\nDROP.\n4.3 Ablation Study\nYelp Flipkart\nTop 1 Ensemble Top 1 Ensemble\nGPO 79.1 ±0.7 78.7±0.9 80.5±2.1 84.5±2.0\nw/o cons 78.8 ±1.2 78.7±0.4 81.5±1.4 84.0±0.9\nw/o cons+t-train79.9±0.8 79.7±1.0 80.3±3.2 81.3±1.4\n(a) Sentiment analysis.\nSocialIQA OpenbookQA\nTop 1 Ensemble Top 1 Ensemble\nGPO 76.7 ±2.0 78.9±1.2 78.7±3.3 79.7±0.8\nw/o cons 76.0 ±2.8 78.1±1.4 77.6±3.8 78.8±2.2\nw/o cons+t-train77.9±1.6 69.6±5.3 78.2±2.2 74.8±3.2\n(b) Commonsense QA.\nNumber Spans\nTop 1 Ensemble Top 1 Ensemble\nGPO 52.2±6.0 53.6±3.0 27.7±12.0 26.7±4.9\nw/o cons 49.3 ±2.8 51.0±2.1 20.6±2.1 22.2±3.2\nw/o cons+t-train 51.3±3.6 50.9±1.6 20.4±1.9 18.7±2.2\n(c) DROP.\nTable 7: Ablation study. Bold-font and underline indi-\ncate the best and second-best results, respectively.\nWe study the effect of prompt ensemble labeling\nand joint prompt optimization by evaluating two\nmodifications of GPO: (1) setting the consistency\nthreshold as 0, denoted as w/o cons; and (2) remov-\ning the target group training data during the final\nprompt generation, denoted as w/o cons+t-train.\nFrom Table 7, we can observe that: 1) In all cases\nexcept for Flipkart with Top 1 evaluation, GPO\nperforms better than w/o cons on target groups,\nshowing the effectiveness of the consistency thresh-\nold. 2) Among the three tasks, DROP has large\nimprovement between w/o cons and GPO on both\nsource and target groups then the other two tasks.\nWe hypothesis that this discrepancy is related to the\ndifferent degrees of improvement in the labeling\naccuracy by the consistency threshold, which will\nbe further discussed in Section 4.4. 3) Comparing\nFlipkart OpenbookQA Spans\nw/o cons 81.9 69.8 3.6\nGPO 94.2 84.3 3.7\nTable 8: The labeling accuracy comparison for the target\ngroup training and validation data on GPO andw/o cons.\nThe results for Spans here is accuracy instead of F1.\nw/o cons and w/o cons+t-train, removing the target\ngroup training data benefits the Top 1 results of the\nsource group, but harms the Ensemble results of\nthe target groups. It has less effect on the target\ngroup Top 1 results since the two methods still use\ntarget group validation data.\n4.4 In-depth Analysis\nAnalysis on the Effect of the Consistency Thresh-\nold. To further reveal the effect of consistency\nthreshold, we first show the labeling accuracy of\nthe target group training and validation data for\nGPO and w/o cons in Table 8. We can observe that\napplying the consistency threshold can improve\nthe labeling accuracy for all target groups. By\nexamining the relationship between this labeling\naccuracy improvement and the performance differ-\nence between GPO and w/o cons in Table 7, it can\nbe explained that for Flipkart and OpenbookQA,\nwhere the labeling accuracy is already high under\nw/o cons, further improving the labeling accuracy\nby the consistency threshold is unlikely to achieve\nlarge performance gain. Conversely, in the case\nof Spans with low labeling accuracy, even a minor\nimprovement can result in significant performance\ngains. To explore the connection between labeling\naccuracy and target group performance further, we\nconducted an experiment where we manually as-\nsigned incorrect labels to varying proportions (0%,\n50%, and 90%) of the target training and valida-\ntion data. The results are illustrated in Figure 3.\nIt can be observed that as the percentage of incor-\nrect labels increases, the overall performance on\nthe target group generally decreases, emphasizing\nthe importance of labeling accuracy for achieving\neffective generalization.\n1545\n0 50 90\n% Incorrect Label\n65\n70\n75\n80\n85\n90Accuracy\nFlipkart\n0 50 90\n% Incorrect Label\n65\n70\n75\n80\n85\n90Accuracy\nOpenbookQA\n0 50 90\n% Incorrect Label\n0\n20\n40\n60\n80\n100Accuracy\nSpans\nFigure 3: Target group performance under different per-\ncentage of wrong labels. The blue dotted line indicates\nthe labeling accuracy of GPO as in Table 8.\nTop 1 Ensemble\nAPE GPO APE GPO\nVicuna-7B 38.4±25.3 63.5±15.6 43.9±21.3 71.9±13.1\nVicuna-13B 66.8±18.4 68.3±13.7 60.7±9.5 70.7±10.8\nGPT-3.5 78.4±1.9 80.5±2.1 81.3±1.4 84.5±2.0\nGPT-4 77.5 ±13.785.3±2.7 83.3±0.0 85.4±2.4\nTable 9: Performance comparison of APE and GPO on\nFlipkart of different backbone LLMs.\nGPO with Different Backbone LLMs. We also\nconducted experiments with GPO using different\nbackbone LLMs, including Vicuna 7B and 13B\n(Chiang et al., 2023) which are notable smaller-\nsized LLMs, and GPT-4 (OpenAI, 2023). Table 9\nshows the generalization results on Flipkart with\nYelp as the source group for APE and GPO on dif-\nferent backbone LLMs. Due to the small sizes of\nthe Vicuna models, generating the exact sentiment\nlabel as the answer can be challenging. Therefore,\nwe extract the sentiment labels from their outputs\nbefore calculating the accuracy. The results show\nthat there is room for enhancing the generalization\nperformance in APE across various LLMs, and\nGPO consistently outperforms APE in all cases.\nNotably, when applying GPO to the smaller Vicuna\n7B model, there is a significant improvement that\nallows it to reach the same performance level as\nthe Vicuna 13B model. Across LLMs, the smaller-\nsized Vicuna models achieve relatively worse per-\nformance, and the powerful GPT-4 achieves the\nbest performance on GPO.\n5 Related Work\nGeneralization Ability and Robustness of LLM.\nResearchers have been investigating the gener-\nalization ability and robustness of LLMs since\ntheir recent breakthrough. LLMs like ChatGPT\nhave shown significant improvement in out-of-\ndistribution (OOD) and adversarial tasks (Wang\net al., 2023), although they are still imperfect (Chen\net al., 2023). Some LLMs still rely on shortcuts\nand spurious correlation (Tang et al., 2023; Stolfo\net al., 2022). Moreover, LLMs remain vulnerable\nto adversarial perturbations and achieve inconsis-\ntent results (Wang et al., 2023; Ye et al., 2023a;\nLiang et al., 2022). Additionally, LLMs demon-\nstrate high sensitivity to the prompt (Reynolds and\nMcDonell, 2021; Zhu et al., 2023) and the selec-\ntion of in-context examples (Liu et al., 2022; Ru-\nbin et al., 2022). Lastly, instruction tuning allows\nLLMs to generalize to novel tasks (Ouyang et al.,\n2022; Wang et al., 2022b,a). We specifically focus\non the generalization issue of prompt optimization\non the distribution shifts within one task.\nPrompt Optimization. Obtaining effective\nprompts for applying LLM in NLP tasks is a popu-\nlar research area. Prompt tuning methods (Li and\nLiang, 2021; Lester et al., 2021; Qin and Eisner,\n2021; Gu et al., 2022) learn soft continuous vectors\nas prompts in the LLM input using gradients\nfrom the task objective. Recent studies have also\nfocused on gradient-free prompt optimization for\nblack-box LLM, such as reinforcement learning-\nbased methods (Zhang et al., 2023; Deng et al.,\n2022; Diao et al., 2022), search-based methods\n(Brown et al., 2020; Prasad et al., 2022; Pryzant\net al., 2023), and other gradient-free optimization\ntechniques like evolutionary algorithms (Sun et al.,\n2022) and boosting (Hou et al., 2022). Among\nthem, the state-of-the-art methods leverage the\npower of LLMs for prompt optimization, such as\nprompt generation and evaluation by LLM (APE\n(Zhou et al., 2023)) and prompt editing following\ncritiques (APO (Pryzant et al., 2023)), where we\nmainly compare with them. Notably, while some\nprevious work on prompt tuning has addressed\ngeneralization across tasks and models (Su et al.,\n2022; Vu et al., 2021; Qin et al., 2023), and domain\nadaptation (Tam et al., 2022; Guo et al., 2022), this\npaper specifically focuses on the generalization\nissue of gradient-free prompt optimization.\n6 Conclusion\nIn this paper, we revealed the generalization issue\nof prompt optimization for LLMs under distribu-\ntion shifts. We observed that the prompt optimized\non the source data group may have a performance\ndrop on the target group with distribution shifts.\nWe performed an initial analysis aiming at identi-\nfying the factors that correlate to the varied gen-\neralization performance across groups, including\nlabel distribution shift and input distribution sim-\n1546\nilarity. To enhance the generalization ability of\nLLMs, we proposed a Generalized Prompt Opti-\nmization framework to jointly consider the source\nand target groups for robust prompt optimization.\nExperimental results validated the effectiveness of\nour proposed framework in boosting the robustness\nof the prompts on the source and target groups. In\nfuture work, we plan to study the prompt general-\nization to unseen target groups without available\ninputs {xt}, and explore prompt generalization abil-\nity with in-context examples from different groups.\n1547\nLimitations\nFirstly, this work discusses the generalization abil-\nity of prompts while ignoring the effect of other\nLLM inputs such as in-context examples. The\nchoice of in-context examples might also affect\nthe robustness of LLMs. Future work can look into\nthe generalization issue of the prompt in combina-\ntion with in-context examples. Secondly, this work\nassumes the availability of the inputs {xt}of the\ntarget group. It is under-explored how to achieve\ngeneralized prompt optimization to completely un-\nseen groups without {xt}. To improve the robust-\nness on these groups, we believe it is helpful to\nextend this work toward robust prompt optimiza-\ntion on multiple heterogeneous groups. Thirdly,\nwe acknowledge that the scope of our research\nis limited to black-box LLMs capable of under-\nstanding instructions, where gradient-free prompt\noptimization with instructing LLM is a suitable\nchoice. For smaller LMs without instruction under-\nstanding abilities, e.g., BERT (Devlin et al., 2019)\nand T5 (Raffel et al., 2020), they are generally not\nblack-box and are more advantageous to utilize\ngradient-based prompt optimization methods.\nAcknowledgements\nThis work is supported by NExT Research Center,\nand the National Natural Science Foundation of\nChina (62272437). We thank the reviewers for\ntheir constructive feedback.\nReferences\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432–7439.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nXuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng,\nMinlong Peng, Jie Zhou, Tao Gui, Qi Zhang, and\nXuanjing Huang. 2023. How robust is gpt-3.5 to pre-\ndecessors? a comprehensive study on language un-\nderstanding tasks. arXiv preprint arXiv:2303.00293.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nLeyang Cui, Yu Wu, Shujie Liu, Yue Zhang, and Ming\nZhou. 2020. Mutual: A dataset for multi-turn dia-\nlogue reasoning. arXiv preprint arXiv:2004.04494.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P\nXing, and Zhiting Hu. 2022. Rlprompt: Optimizing\ndiscrete text prompts with reinforcement learning.\narXiv preprint arXiv:2205.12548.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nShizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li,\nYong Lin, Xiao Zhou, and Tong Zhang. 2022. Black-\nbox prompt learning for pre-trained language models.\narXiv preprint arXiv:2201.08531.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDrop: A reading comprehension benchmark re-\nquiring discrete reasoning over paragraphs. arXiv\npreprint arXiv:1903.00161.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2022. Ppt: Pre-trained prompt tuning for few-shot\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8410–8423.\nChulaka Gunasekara, Jonathan K Kummerfeld, Lazaros\nPolymenakos, and Walter Lasecki. 2019. Dstc7 task\n1: Noetic end-to-end response selection. In Proceed-\nings of the First Workshop on NLP for Conversational\nAI, pages 60–67.\nXu Guo, Boyang Li, and Han Yu. 2022. Improving\nthe sample efficiency of prompt tuning with domain\nadaptation. arXiv preprint arXiv:2210.02952.\n1548\nBairu Hou, Joe O’Connor, Jacob Andreas, Shiyu Chang,\nand Yang Zhang. 2022. Promptboosting: Black-box\ntext classification with ten forward passes. arXiv\npreprint arXiv:2212.09257.\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan\nLiu, Jingang Wang, Juanzi Li, Wei Wu, and Maosong\nSun. 2022. Knowledgeable prompt-tuning: Incor-\nporating knowledge into prompt verbalizer for text\nclassification. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2225–2240.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B\nDolan, Lawrence Carin, and Weizhu Chen. 2022.\nWhat makes good in-context examples for gpt-3?\nIn Proceedings of Deep Learning Inside Out (Dee-\nLIO 2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114.\nRyan Lowe, Nissan Pow, Iulian Serban, and Joelle\nPineau. 2015. The ubuntu dialogue corpus: A large\ndataset for research in unstructured multi-turn dia-\nlogue systems. arXiv preprint arXiv:1506.08909.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nR Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntac-\ntic heuristics in natural language inference. arXiv\npreprint arXiv:1902.01007.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. arXiv preprint arXiv:1809.02789.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nnli: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2022. Grips: Gradient-free, edit-based in-\nstruction search for prompting large language models.\narXiv preprint arXiv:2203.07281.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nguang Zhu, and Michael Zeng. 2023. Automatic\nprompt optimization with\" gradient descent\" and\nbeam search. arXiv preprint arXiv:2305.03495.\nChengwei Qin, Shafiq Joty, Qian Li, and Ruochen Zhao.\n2023. Learning to initialize: Can meta learning im-\nprove cross-task generalization in prompt tuning?\narXiv preprint arXiv:2302.08143.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Comput-\ning Systems, pages 1–7.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2655–2671.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019. Socialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728.\n1549\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352.\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bern-\nhard Schölkopf, and Mrinmaya Sachan. 2022. A\ncausal framework to quantify the robustness of math-\nematical reasoning with language models. arXiv\npreprint arXiv:2210.12023.\nYusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,\nYankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan\nLiu, Peng Li, Juanzi Li, et al. 2022. On transferabil-\nity of prompt tuning for natural language processing.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3949–3969.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022. Black-box tuning for\nlanguage-model-as-a-service. In International Con-\nference on Machine Learning, pages 20841–20855.\nPMLR.\nWeng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue,\nXingjian Zhang, Yuxiao Dong, Jiahua Liu, Maodi\nHu, and Jie Tang. 2022. Parameter-efficient prompt\ntuning makes generalized and calibrated neural text\nretrievers. arXiv preprint arXiv:2207.07087.\nRuixiang Tang, Dehan Kong, Longtao Huang, and Hui\nXue. 2023. Large language models can be lazy learn-\ners: Analyze shortcuts in in-context learning. arXiv\npreprint arXiv:2305.17256.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nNirali Vaghani and Mansi Thummar. 2023. BFlipkart\nproduct reviews with sentiment dataset. https://\nwww.kaggle.com/dsv/4940809.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and\nDaniel Cer. 2021. Spot: Better frozen model adap-\ntation through soft prompt transfer. arXiv preprint\narXiv:2110.07904.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nJindong Wang, Xixu Hu, Wenxin Hou, Hao Chen,\nRunkai Zheng, Yidong Wang, Linyi Yang, Hao-\njun Huang, Wei Ye, Xiubo Geng, et al. 2023.\nOn the robustness of chatgpt: An adversarial\nand out-of-distribution perspective. arXiv preprint\narXiv:2302.12095.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022a. Self-consistency\nimproves chain of thought reasoning in language\nmodels. arXiv preprint arXiv:2203.11171.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, et al. 2022b. Super-\nnaturalinstructions: Generalization via declarative\ninstructions on 1600+ nlp tasks. In Proceedings of\nthe 2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 5085–5109.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122.\nLinyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yi-\ndong Wang, Hanmeng Liu, Jindong Wang, Xing\nXie, and Yue Zhang. 2022. Glue-x: Evaluating nat-\nural language understanding models from an out-\nof-distribution generalization perspective. arXiv\npreprint arXiv:2211.08073.\nWentao Ye, Mingfeng Ou, Tianyi Li, Xuetao Ma, Yi-\nfan Yanggong, Sai Wu, Jie Fu, Gang Chen, Junbo\nZhao, et al. 2023a. Assessing hidden risks of llms:\nAn empirical study on robustness, consistency, and\ncredibility. arXiv preprint arXiv:2305.10235.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023b. Large language\nmodels are versatile decomposers: Decompose evi-\ndence and questions for table-based reasoning. arXiv\npreprint arXiv:2301.13808.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\nurmans, and Joseph E Gonzalez. 2023. Tempera:\nTest-time prompt editing via reinforcement learning.\nIn The Eleventh International Conference on Learn-\ning Representations.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text classi-\nfication. Advances in neural information processing\nsystems, 28.\n1550\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers. In The Eleventh International\nConference on Learning Representations.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen\nWang, Hao Chen, Yidong Wang, Linyi Yang, Wei\nYe, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.\nPromptbench: Towards evaluating the robustness of\nlarge language models on adversarial prompts. arXiv\npreprint arXiv:2306.04528.\nA Appendix\nA.1 Dataset Details\nFor each dataset, we use the original training set\nto split into training and validation sets, and ran-\ndomly sample a subset from the original validation\nset as our test set as sometimes the labels for the\noriginal test set are not available. Following the\nofficial implementation of APE 3, we split the orig-\ninal training set with 1000 training samples, and\nthe rest as validation samples. For MNLI, we sam-\nple the same number of matched and mismatched\nvalidation data as the test set. For ANLI, we use\nR2. For Yelp and Flipkart, we assign the review\nscores of 0 and 1 as negative, 3 as neutral, and\n4, 5 as positive. For multi-turn dialog reasoning,\nwe select the instances of MuTual within 5 dialog\nturns, Ubuntu and DSTC7 within 7 dialog turns,\nand reduce the number of choices to 4 for all three\ndatasets. We show an example of LLM input for\neach task in Table 11, and the dataset statistics in\nTable 10.\nA.2 Additional Implementation Details for\nPreliminary Experiments\nThe APE performs prompt optimization by itera-\ntively generating and selecting the prompts lever-\naging LLM. For prompt generation, it utilizes a\nmeta prompt to instruct LLM to infer prompts from\ngiven input-output examples. Then, the generated\nprompts are evaluated on validation data to select\nthe prompts with good task performance. After\nthat, APE leverages LLM to perform Monte Carlo\nsearch by iteratively paraphrasing the current effec-\ntive prompts and performing evaluation on them to\nobtain optimized prompts.\nFollowing the official implementation, for\nprompt generation, the sampled N-shot training\ndata are divided into K splits to generate K\n3https://github.com/keirp/automatic_prompt_\nengineer/tree/main.\n# Train&Val # TestNShot KPrompt\nYelp 650000 150 36 6\nFlipkart 75138\nIMDB 25000\nAmazon 100000\nSocialIQA 33410 150 36 6\nPIQA 16113\nOpenbookQA 4957\nNumber 2000 150 36 6\nSpans 2000\nMNLI 392702 1000 16 4\nANLI 45460\nRTE 2490 277 16 4\nHANS 30000 1000\nDSTC7 43824 150 9 9\nUbuntu Dialog 94107\nMuTual 4783\nTable 10: Statistics for the train, validation and test\nsplits for each dataset, and the values of shot number\nN and prompt number Kfor each task. The Train&Val\nare further split into 1000 training samples and the rest\nas validation samples.\nprompts by LLM for further selection. For each\ntask, we try the value of N as 9, 16, 25, 36, and\nK as N’s factors, to ensure obtaining effective\nprompts, where APE is not very parameter sensi-\ntive. Moreover, we ablate the Monte Carlo search\nsince it is optional and not significant for our tasks.\nGiven the randomness of the backbone LLM,\nwe set the temperature of the LLM as 0, top p as\n1.0. We set the max tokens for prompt generation\nas 100 to try to ensure no truncation, and keep\nother LLM parameters the same as the official APE\nimplementation. The parameters N and K are\nshown in Table 10.\nA.3 Additional Details and Results for the\nExploration on the Factors Affecting\nPrompt Robustness.\nCalculation of Q1 Metrics. The label distribu-\ntion shift quantifies the divergence of the label\ndistributions between two groups for classification\ntasks, calculated by the KL divergence of their label\ndistributions,\nDKL =\n∑\ny∈Y\nPrs(y)log(Prs(y)\nPrt(y) )\nwhere Yis the label space of the task, and Prs(y)\nand Prt(y) denote the probability of the label yin\nthe source and target groups, respectively.\n1551\nDataset Input Example Labels\nYelp Dr. Goldberg offers everything I look for in a general practitioner. He’s nice and easy to talk\nto without being patronizing; he’s always on time in seeing his patients...\npositive, negative, neutral\nOpenbookQAThe sun is responsible for (A) puppies learning new tricks (B) children growing up and\ngetting old (C) flowers wilting in a vase (D) plants sprouting, blooming and wilting.\nA, B, C, D\nMNLI Premise: One of our number will carry out your instructions minutely. Hypothesis: A\nmember of my team will execute your orders with immense precision.\nentailment, neutral, contra-\ndiction\nHANS Sentence 1: The doctors supported the scientist. Sentence 2: The scientist supported the\ndoctors.\nentailment,\nnon−entailment\nDSTC7 S: Hello! A: Hello! S: I’m wondering for next semester what class should I take. A: Given\nyour experience, I suggest you take EECS 280. S: Do you know what the size of that class\nis? Answer Choices: (A) EECS 481 covers dealing with structuring principles, pragmatic\naspects of the production of software systems, design methodologies and informal analysis.\n(B) The class size is normally around 167 students. (C) Based on the classes you’ve taken,\nthis class shouldn’t be extremely demanding. (D) This course has a discussion section.\nA, B, C, D\nNumber Question: How many in percent weren’t 45 to 64? Context: In the city, the year 2010\npopulation was spread out with 26.3% under the age of 18, 13.6% from 18 to 24, 30.7% from\n25 to 44, 21.1% from 45 to 64, and 7.2% who were 65 years of age or older. The median age\nwas 32 years. For every 100 females, there were 92.5 males. For every 100 females age 18\nand over, there were 88.4 males.\ne.g.,78.9\nTable 11: Dataset examples for each task. The output for classification tasks is one of the Labels, while for Number\nthe output is a string of numerical value.\nThe input similarity quantifies the n-gram simi-\nlarity of the input corpuses of the two groups. Sup-\npose that we sample M inputs from the source\nand target groups respectively, denoted as xs =\n{xs1 ,...,x sM }and xt = {xt1 ,...,x tM }, we calcu-\nlate the Spearman’s rank order correlation between\nthe bag-of-word vectors of xs and xt,\nρ= cov(Vs,Vt)\nδ(Vs)δ(Vt)\nwhere Vs and Vt denotes the ranked bag-of-word\nvectors of xs and xt on the vocabulary of xt.\nCalculation of Q2 Metrics. We sample the same\namount of inputs from SocialIQA, PIQA and Open-\nbookQA, and denote the input corpuses as x1,x2\nand x3. Firstly, we calculate the proportion of\nunique n-grams for each group against the num-\nber of all n-grams for the three corpuses as\n|n-grams(xi)|\n|n-grams({x1,x2,x3})|,i = 1,2,3\nwhere n-gram(·) returns the set of unique n-grams,\nand the braces denotes mixing the inputs.\nSecondly, we think the source group that has\nalready covered a larger proportion of n-grams of\nthe target group may promote better generaliza-\ntion, and we calculate the proportion of n-gram\ncoverage between the source and target groups as\n|n-grams(xs) ∩n-grams(xt)|\n|n-grams(xt)|\nFor both metrics, the n-gram( ·) is calculated\nas both word 1-gram and character 4-gram using\nscikit-learn.\nQ1 Metrics for More Tasks. Table 12 and Ta-\nble 13 show the two Q1 metric results for common-\nsense QA and Dialog tasks. Linking the results\nwith the generalization performance in Table 1 and\nTable 2, we have the following observations. 1)\nFor each target group of the commonsense QA\ntask, the largest value for input similarity coheres\nwith the best generalization performance, but the\nsmallest value of label distribution shifts does not\ncorrelate to the best generalization performance. 2)\nFor the Dialog groups, the zero label distribution\nshifts and the close input similarities cohere with\nthe subtle generalization performance difference on\neach target group. 3) The evaluation metrics cannot\nbe compared across target groups nor across tasks.\ne.g., the source group SocialIQA performs better\non PIQA than OpenbookQA (cf. Table 2), but the\ninput similarity is higher for OpenbookQA. Also,\nMuTual has smaller input similarity with Ubuntu\n(input similarity is 0.56, and generalization per-\nformance is 74.7) but better generalization perfor-\nmance than PIQA generalizing to SocialIQA (input\nsimilarity is 0.57, and generalization performance\nis 68.9) (cf. Section 2). These findings reveals the\nbenefits and limitations of the Q1 metrics.\n1552\nSource\nTarget SocialIQA PIQA OpenbookQA\nSocialIQA - 2.44 0.27\nPIQA 0.38 - 0.59\nOpenbookQA 1.59 3.17 -\n(a) Commonsense QA\nSource\nTarget Mutual DSTC7 Ubuntu Dialog\nMutual - 0 0\nDSTC7 0 - 0\nUbuntu Dialog 0 0 -\n(b) Dialog\nTable 12: Results for label distribution shifts. Smaller\nvalue indicates smaller distribution shift. Bold font\nindicates the smallest value for each column.\nSource\nTarget SocialIQA PIQA OpenbookQA\nSocialIQA - 0.59 0.62\nPIQA 0.57 - 0.69\nOpenbookQA 0.61 0.67 -\n(a) Commonsense QA\nSource\nTarget MuTual DSTC7 Ubuntu Dialog\nMuTual - 0.55 0.56\nDSTC7 0.56 - 0.56\nUbuntu Dialog 0.57 0.57 -\n(b) Dialog\nTable 13: Results for input similarity. Larger value\nindicates smaller distribution shifts. Bold font indicates\nthe largest value for each column.\nA.4 Details for Baseline Implementation\nFor all compared methods, the LLM parameters\nsuch as temperature, top p, max tokens are the\nsame as in Appendix A.2. The implementation and\nresults for APE follow the preliminary experiments\nas illustrated in Appendix A.2 and Section 2. For\nAPO, we follow the original parameter setting ex-\ncept for number of optimization step as 1 because\nthe three tasks do not need multi-round optimiza-\ntion. For GPO, the value K is unchanged from\nAPE. The consistency threshold for GPO are 0.83\n(5 out of 6 prompts) for sentiment analysis and com-\nmonsense QA, and 0.33 (2 out of 6 prompts) for\nDROP. Note that APE and APO are not designed\nto utilize the unlabeled target group data so we\nonly observe the direct generalization performance,\nwhile APE-ut and GPO utilize the N-shot source\ngroup data and N-shot target group data. All of\nthe above methods do not need to apply Monte\nCarlo search following the official implementation\nof APE. We use one 32GB GPU to perform in-\nference for Vicuna models. We present the meta\nprompt of APE and APE-ut, the initial prompt for\nAPO, the human-written prompts, the revised ver-\nsions by PromptPerfect here.\n• APE meta prompt:\nI provide my friend with an instruction. Based\non the instruction, I gave him several inputs,\nand he generated the corresponding outputs.\nHere are the input-output examples:[DEMO].\nPlease briefly illustrate the instruction and\ndescribe the output format. The instruction is\nto\n• APE-ut meta prompt:\nI provide my friend with an instruction. Based\non the instruction, I gave him several in-\nputs, and he generated the corresponding\noutputs. Here are the input-output exam-\nples:[Source]. Here are also some unlabeled\nexamples. Please consider these examples as\nwell for prompt generation:[Unlabeled Tar-\nget].Please briefly illustrate the instruction\nand describe the output format. The instruc-\ntion is to\n• APO initial Prompts:\nFor Yelp: Provide a sentiment analysis of the\nfollowing text. Answer Positive Neutral or\nNegative as labels.\nFor SocialIQA: Give answer to the following\nmulti choice question. Provide only the single\nletter as labels.\nFor Number: Answer the following question\nbased on the context which involves numerical\ncalculation. Provide only the numerical value\nthat directly answers the question.\n• Human Prompts:\nFor sentiment analysis: Provide a sentiment\nanalysis of a given input text. The output for-\nmat is a single word indicating whether the\nsentiment is positive, negative, or neutral.\nFor commonsense QA: Give answer to the fol-\nlowing multi choice question which involves\ncommonsense knowledge. Provide only the\nsingle letter (a, b, c, or d) as labels.\nFor DROP: Answer the following question\nbased on the context which involves numer-\nical reasoning. Provide only the direct answer\nthe question, which can be a numerical value\nor a short string.\n1553\n• PromptPerfect:\nFor sentiment analysis: Your task is to perform\na sentiment analysis on a given input text and\nprovide a single word indicating whether the\nsentiment is positive, negative, or neutral. The\ninput text may contain any language or style\nof writing. Please ensure that your analysis\ntakes into account the overall tone and con-\ntext of the text.Your response should be con-\ncise and clear, providing a single word that\naccurately reflects the sentiment of the input\ntext. If there are multiple sentiments present\nin the text, please choose the one that best\nrepresents the overall feeling conveyed by the\nauthor.Please note that your analysis should\ntake into account all relevant factors, such\nas tone, language use, and content. Your re-\nsponse should also be flexible enough to allow\nfor various types of input texts.\nFor commonsense QA: Please choose the best\nanswer for the following multiple choice ques-\ntion. Choose the one answer that best fits the\ngiven scenario. Please provide only the single\nletter (a, b, c, or d) as labels.\nFor DROP: Your task is to answer a numeri-\ncal question based on a given context involv-\ning numerical reasoning. Please provide a\ndirect answer to the question, which can be\na numerical value or a short string.Please\nnote that your response should be concise\nand directly answer the question. The ques-\ntion may involve various numerical data, such\nas percentages, averages, or counts. You\nshould focus on identifying the relevant in-\nformation and providing a clear and accurate\nanswer.Additionally, please ensure that your\nresponse is flexible enough to allow for var-\nious relevant and creative answers based on\nthe context provided.\nA.5 Case Study\nWe present a case study by presenting the best\nprompt among the five runs for sentiment analysis\nand DROP as shown in Table 14. We can observe\nthat the optimized prompt for a single group often\ncontains group-specific background information as\nhighlighted by underline which may hinder robust\nprompt generalization. On the contrary, the opti-\nmized prompts of GPO are more general and thus\nperforms well on both groups. Note that for Spans,\nthe optimized prompt is also general enough and\nthus can generalize well to Number as shown in\nTable 2.\nYelp Provide feedback on various experiences, such as\ndining, shopping, and service. The output format is\na sentiment analysis, where the input is analyzed\nto determine whether the experience was positive,\nnegative, or neutral. The output is a single word\nindicating the sentiment of the experience.\nFlipkart Provide a sentiment analysis ofcustomer reviews.\nThe input consists of a customer review of a prod-\nuct, and the output is a binary classification of the\nsentiment as either positive or negative.\nGPO provide a sentiment analysis of a given text. The\noutput format is a single word indicating whether the\nsentiment is positive, negative, or neutral.\nNumber Answer a specific question based on a given context.\nThe output format isa numerical valuethat directly\nanswers the question asked.\nSpans Answer a specific question based on a given context.\nThe output format isa single word or phrasethat di-\nrectly answers the question asked.\nGPO Answer questions based on given con-\ntext information. The output format is\na numerical value or a single word answer.\nTable 14: Case study on the prompts optimized by APE\nfrom a source group, and GPO.\nK Flipkart Ensemble\n3 81.2 ±1.3\n6 84.5 ±2.0\n9 85.8 ±1.9\n12 85.2 ±1.8\n18 85.3 ±1.4\nTable 15: Generalization performance of GPO on Flip-\nkart with different numbers of candidate prompts K.\nA.6 Study on the Impact of the Number of\nCandidate Prompts\nWe examine the effect of varying the number of\ncandidate prompts Kon GPO performance in our\n36-shot sentiment analysis task. We test the K\nvalues in {3, 6, 9, 12, 18}. The results on the target\ngroup Flipkart are shown in Table 15. We observe\nthat the generalization performance stabilizes as K\nreaches a specific value, in this case is 6, indicating\nthat further generating more prompts are unlikely\nto yield significant improvements in performance.\n1554"
}