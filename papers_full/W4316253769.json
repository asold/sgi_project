{
  "title": "Using Multilingual Bidirectional Encoder Representations from Transformers on Medical Corpus for Kurdish Text Classification",
  "url": "https://openalex.org/W4316253769",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4317135124",
      "name": "Soran S. Badawi",
      "affiliations": [
        "Charmo University",
        "Kurdistan Regional Government"
      ]
    },
    {
      "id": "https://openalex.org/A4317135124",
      "name": "Soran S. Badawi",
      "affiliations": [
        "Kurdistan Regional Government"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3038742186",
    "https://openalex.org/W6786005370",
    "https://openalex.org/W2741388816",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3204526376",
    "https://openalex.org/W6769318315",
    "https://openalex.org/W6760385162",
    "https://openalex.org/W3115030527",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W4220684376",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3013057403",
    "https://openalex.org/W3103525021",
    "https://openalex.org/W1937192932",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2982567551"
  ],
  "abstract": "Technology has dominated a huge part of human life. Furthermore, technology users use language continuously to express feelings and sentiments about things. The science behind identifying human attitudes toward a particular product, service,or topic is one of the most active fields of research, and it is called sentiment analysis. While the English language is making real progress in sentiment analysis daily, other less-resourced languages, such as Kurdish, still suffer from fundamental issues and challenges in Natural Language Processing (NLP). This paper experimentswith the recently published medical corpus using the classical machine learning method and the latest deep learning tool in NLP and Bidirectional Encoder Representations from Transformers (BERT). We evaluated the findings of both machine learning and deep learning. The outcome indicates that BERT outperforms all the machine learning classifiers by scoring (92%) in accuracy, which is by two points higher than machine learning classifiers.",
  "full_text": "10 http://dx.doi.org/10.14500/aro.11088\nARO p-ISSN: 2410-9355, e-ISSN: 2307-549X \nCorpus for Kurdish Text Classification\nSoran S. Badawi\nCharmo Center for Scientific Research and Consulting â€“ Language and Linguistic Center, Charmo University  \nChamchamal, Sulaimani, Kurdistan region  - F.R. Iraq\nAbstractâ€”Technology has dominated a huge part of human \nlife. Furthermore, technology users use language continuously to \nexpress feelings and sentiments about things. The science behind \nidentifying human attitudes toward a particular product, service, \nor topic is one of the most active fields of research, and it is called \nsentiment analysis. While the English language is making real \nprogress in sentiment analysis daily, other less-resourced languages, \nsuch as Kurdish, still suffer from fundamental issues and challenges \nin Natural Language Processing (NLP). This paper experiments \nwith the recently published medical corpus using the classical \nmachine learning method and the latest deep learning tool in NLP \nand Bidirectional Encoder Representations from Transformers \n(BERT). We evaluated the findings of both machine learning and \ndeep learning. The outcome indicates that BERT outperforms all \nthe machine learning classifiers by scoring (92%) in accuracy, which \nis by two points higher than machine learning classifiers.\nIndex Termsâ€”Bidirectional Encoder Representations from \nTransformers, Deep learning, Machine learning, Natural \nlanguage processing, Sentiment analysis, Transformers.\nI. Introduction\nThe text classification method in natural language \nprocessing (NLP) is one of the approaches of identifying \nthe emotions in text. The field has gained more popularity \nsince the emergence of social platforms such as Twitter and \nFacebook (Hoang, Bihorac, and Rouces, 2019). It has been \ntackled very well in the English Language. Conversely, the \nwork done in the Kurdish language remains in its infancy; \nthus, more cooperation and contributions are required from \nresearch communities to offer a mature sentiment analysis \nsystem in Kurdish. The previous Kurdish sentiment analysis \nworks mostly centered on classical machine learning \nclassifiers. In general, these classical methods are considered \nto be super-fast and simple. Due to feature engineering, their \nperformance firmly hangs on the feature selection before \ntraining.\nLater on, deep learning was developed as a popular \nalternative to traditional machine learning methods because \nof its excellence in NLP tasks like text classifications \n(Collobert, et al., 2011). The main idea of deep learning \nalgorithms is the automated extraction of representations \nfrom data (LeCun, et al., 2015).\nThe previous methods, for instance, a bag-of-words (BOW) \nand a Term Frequency Inverse Document Frequency (TF-IDF) \napproach, statistically represent word frequency in documents. \nTherefore, they could not recognize the relationship between \ndifferent keywords in a document as inherently statistical \nmethods. Consequently, Word Embedding methods emerged \nto help solve this issue by representing words as mathematical \nvectors in a multidimensional space. Usually, these vectors \nprovide vital information about the associations between \nwords. Numerous studies on word embedding proved that \npre-trained word embedding models, such as word2vec \n(Mikolov, et al., 2013) and GloVe (Pennington, Socher, and \nManning, 2014) and Bidirectional Encoder Representations \nfrom Transformers (BERT) (Devlin, 2018), can significantly \nenhance text classification and other NLP tasks.\nBERT is based on a multi-layer bidirectional transformer \n(Vaswani, et al., 2017). BERT is pre-trained on a large \ncorpus of multilingual data in a self-supervised pattern, \nwhich implies that it was only pre-trained on the raw text \nindependence of humans, labeling them in any way. It uses \nan automatic process to generate inputs and labels from \nthose texts. Google Search Team has pre-trained BERT with \n12 layers and 768 hidden dimensions per single token. Bertâ€™s \ntotal parameter equals 110 million parameters (Devlin, 2018).\nIn this study, we use BERT to classify Kurdish texts. We \nbegin by comparing the BERTâ€™s performance with traditional \nmachine learning methods that have been extensively utilized \nin earlier publications. The rest of this paper is structured \nin this way. We examine the literature on classifying \nKurdish texts in the next part. Then, we apply a customized \nBERT-Multilingual model to the medical corpus and compare \nthe outcomes with text categorization methods based on \nmachine learning. The conclusion of this study will be \nincluded in the final section.\nARO-The Scientific Journal of Koya University \nV ol. XI, No. 1 (2023), Article ID: ARO.11088. 6 pages \nDoi: 10.14500/aro.11088 \nReceived: 10 October 2022; Accepted: 29 December 2022 \nRegular research paper: Published: 15 January 2023. \nCorresponding authorâ€™s e-mail: Soran.sedeeq@charmouniversity.org  \nCopyright Â© 2023  Soran S. Badawi. This is an open access article \ndistributed under the Creative Commons Attribution License.\nUsing Multilingual Bidirectional Encoder \nRepresentations from Transformers on Medical \nhttp://dx.doi.org/10.14500/aro.11088 11\n ARO p-ISSN: 2410-9355, e-ISSN: 2307-549X\nII. Related Works\nKurdish language has more than 30 million speakers around \nthe globe and is categorized as one of the less-resourced \nlanguages, particularly in the field of NLP (Esmaili, 2012). \nUnlike English, mountainous works have been done in \ndifferent areas of NLP; the sentiment analysis process in \nKurdish is still in its early stages. So far, only one research \nstudy has been carried out in this direction. Two Kurdish \nresearchers, Salam Abdulla and Mzhda Hiwa Hama, carried \nout the work. Their work entitled â€œSentiment Analyses for \nKurdish Social Network Texts using Naive Bayes Classifierâ€™ \n(2015)â€. Their data contained 15k tweets containing positive \nand negative labels, distributed half for each tag. The result \nwas achieved using NaÃ¯ve Bayes (0.66) (Abdulla and Hama, \n2015). In addition to this, the corpus is not available online. \nMoreover, their corpus was not trained in deep learning tools.\nIII. BERT Architecture\nBERT, which stands for Bidirectional Encoder \nRepresentations from Transformers, is a transformers model \npre-trained on a large corpus of multilingual data in a self-\nsupervised fashion, which means that it was pre-trained on \nthe raw texts only, independence of humans labeling them in \nany way with an automatic process to generate inputs, and \nlabels from those texts. Google Search Team has pre-trained \nBERT with 12 layers and 768 hidden dimensions per single \ntoken. Bertâ€™s total parameter equals 110 million parameters \n(Devlin, 2018). The architecture of the model is displayed in \nFig. 1.\nMoreover, BERT requires its input token sequence to have \na specific format. The first token of every sequence should \nbe assigned as (CLS) (classification token), and there should \nbe a (SEP) token (separation token) after every sentence to \nachieve the same format (Ling, 2020). To construct an input \nrepresentation for a token, a sum is applied to its token, \nsegment, and position embeddings. An illustration of this \nconstruction is shown in Fig. 2.\nBERT is widely implemented in text classification for \nother resourceful languages. Since it gives high accuracy \ncompared to traditional machine learning algorithms due to \nhaving many layers, as shown in Fig. 3.\nAs illustrated in Fig. 3, the first is called Input, the input layer. \nThis layer accepts the initial word embedding and delivers it into \nBERT. The second part is ğµğ¸ğ‘…ğ‘‡ which is the pre-trained BERT \nmodel. The output of this part is the final word embedding of \neach input token. The last part is predict. In this part, the hidden \nrepresentation is passed to a dense layer followed by ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥. \nThe ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ is applied along the dimension of the sequence. \nMathematically, the ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ function takes input as a vector of \nK real numbers and normalizes it into a probability distribution \nconsisting of K probabilities proportional to the exponentials of \nthe input numbers. The output is the probabilities of each label \n(Ling, 2020)\nIV . Methodology\nThe dataset used in this paper contains social media \ncomments written in the Kurdish language. Because many \nusers used Arabic-supported keyboards built-in on their \nmobile devices when commenting on videos, posts, and \npictures. For example, in the Arabic letter â€œÛ†is absent. If \nusers intend to type words that have the letter â€œÛ†they write \nâ€œØ®ÙˆØ´instead of â€œØ®Û†Ø´This dictation error causes the \nmachine learning classifiers, BOW and Bert, which use the \ntokenization process to understand those two words when \nthey are a single word.\nPre-processing is an essential phase of machine learning \nand deep learning. The pre-processing helps the classifiers \nto provide better results. From a morphological perspective, \nKurdish is a language with numerous attachments, such as \nArabic and Persian. Knowing the extension might give \ninformation about the pronouns, the plurality, and the location \npre-positionally (Cieliebak, et al., 2017). Thus, pre-processing \nwould be challenging since the language has progressed \nin NLP. Luckily, we could use the python KLPT toolkit \ndeveloped by Ahmadi (2020). The libraries on KLPT helped \nus with normalization, standardization, and tokenization. It is \nessential to know that there are no special libraries to point \nout stop-words in the Kurdish language. We created the stop-\nword lists and implemented them on the corpus. The dataset \nincludes a collection of raw comments from Kurdish social \nmedia users. The initial cleaning of the dataset included the \nremoval of URLs, not-Kurdish alphabetical, emojis, and \nnumbers. However, there are no mentions of using software \nor a library to accomplish this. We used the KLPT library to \nperform the following process;\n1. Normalization for unifying dialects and scripts based on \ndifferent encodings\n For example\n Unnormalized texts - \" Ø±Û†Ø²Û• Ù†Û•Ø´ØªÛ• Ø±Ú©Û• Ø±ÛŒÙ… Ú©Ø±Ø¯ÙˆÛ•Ù¥Ù¤Ø¯Ú©ØªÙˆØ± Ú©ÛŒØ§Ù† \" \nNormalized text \" Ø±ÙˆØ²Û• Ù†Û•Ø´ØªÛ•Ø±Ú©Û•Ø±ÛŒÙ… Ú©Ø±Ø¯ÙˆÛ•45Ø¯Ú©ØªÙˆØ± Ú©ÛŒØ§Ù†\"\n2. Standardization â€“ When given a normalized text, it \nreturns standardized Kurdish text according to Soraniâ€™s \nrecommendations.\nFig. 1. The architecture of the model.\n12 http://dx.doi.org/10.14500/aro.11088\nARO p-ISSN: 2410-9355, e-ISSN: 2307-549X \n For instance\n Unstandardized text - \" Ø±ÙˆØ²Û• Ù†Û•Ø´ØªÛ•Ø±Ú©Û•Ø±ÛŒÙ… Ú©Ø±Ø¯ÙˆÛ•45Ø¯Ú©ØªÙˆØ± Ú©ÛŒØ§Ù†\"\nStandardized text- \" Ø±Û†Ú˜Û• Ù†Û•Ø´ØªÛ•Ø±Ú¯Û•Ø±ÛŒÙ… Ú©Ø±Ø¯ÙˆÙˆÛ•45Ø¯Ú©ØªÛ†Ø± Ú¯ÛŒØ§Ù†\"\n3. Tokenization refers to slicing sentences into words and \nputting them in a list.\n For example, the tokenization of the example above is \n[â€˜â€™, â€™Ø±Û†Ú˜Û•â€™, â€™Ù†Û•Ø´ØªÛ•Ø±Ú¯Û•Ø±ÛŒÙ…â€™, â€™Ú©Ø±Ø¯ÙˆÙˆÛ•45â€™ ,â€™â–Ú¯ÛŒØ§Ù†â–Ø¯Ú©ØªÛ†Ø±]\nLuckily, the libraryâ€™s documentation would help us \nconstruct a scientific set of vocabulary, which would be \ncrucial to the machine learning classifiers when using BOW \nand for Bert to find the tokens of each language in the pre-\ntrained model.\nWe implement HuggingFace, a BERT-multilingual tokenizer, \nand its model on the input data. Our data are tokenized before \nbeing used for training by fine-tuning the BERT model. The \nprocess of fine-tuning Bert begins with stacking multilingual-\nBERT with five multilingual-BERT layers. We add a dense \nlayer with a softmax activation function in the next step. \nA binary cross-entropy loss function was also used to \nminimize the errors while training our models (Zahera, et al., \n2019). Compared to the original BERT, the fine-tuned model \nrequires much less time to train. Furthermore, fine-tuning \nBERT assists us with training a model to good performance \non a much smaller amount of training data. Finally, this simple \nfine-tuning procedure (typically adding one fully connected \nlayer on top of BERT and training for a few epochs) was \nshown to achieve a state of the art results with minimal task-\nspecific adjustments for a wide variety of tasks: Classification, \nlanguage inference, semantic similarity, question answering, \netc. Rather than implementing custom and sometimes obscure \narchitecture shown to work well on a specific task, simply \nfine-tuning BERT is shown to be a better (or at least equal) \nalternative. We reduce the max length to 128 for the BERT \ntokenizer, the batch size to 8, as shown in Fig. 4, and the \ntraining epochs to 3.\nWe experiment with decision tree and support vector \nmachine (SVM) classifiers, multinomial stochastic gradient \ndescent (SGD), k-nearest neighbor (kNN), SVM, Random \nForest, and Logistic Regression. We compare their results \nwith BERT, as shown in Table I. We also use a Count \nVectorizer with a mixture of unigrams, bi-grams, and tri-\ngram representations of words for our machine-learning \nmethods, because of the nature of our corpus. Usually, \nmedical texts contain many keywords with low frequency \n(e.g., a diseaseâ€™s name or a medicineâ€™s name). The disease or \nmedicine names have been transliterated into Kurdish \nlanguage, as displayed in Fig. 5. These words contribute \nheavily to the classification since they are often less frequent; \ntheir importance would be lost in a BOW approach because \nof their low frequency, which only counts the word frequency \nin documents. Furthermore, we notice that the majority of \nmedical texts include words such \nas â€œØ¯Ú©ØªÛ†Ø±ØŒ Ø¹ÛŒØ§Ø¯Û•ØŒ Ú¤ÛŒØªØ§Ù…ÛŒÙ†ØŒ Ù…Ø±Ù‡Û•Ù…ØŒ Ù„ÛŒØ²Û•Ø±ØŒ ÙÛŒÙ„Û•Ø±ØŒ Ù¾ÛŒØ³Øªâ€œ. \nData-splitting is an essential step. The method used \nfor splitting the data predominantly affects the modelâ€™s \noutcome. Since our dataset is not very big, we used holdout \nto split our data. In the first phase, we used (the 80% \ntrain-20% test) technique. Moreover, we split the trainset \nusing the Holdout technique to create a validation set. \nHaving a validation set is vital, particularly in the case of \ndeep learning.\nV . Results and Discussion\nThis workâ€™s database comprises 6756 samples distributed \nbetween two labels (medical and non-medical). The number \nof medical texts equals 3076, while the number of non-\nmedical texts is 3680 (Saeed, 2022), , as shown in Fig. 6.\nFig. 2. Input representation of BERT.\nFig. 3. BERT layers for text classification.\nhttp://dx.doi.org/10.14500/aro.11088 13\n ARO p-ISSN: 2410-9355, e-ISSN: 2307-549X\nWe separate (4324 texts) for training and validation (1081 \ntexts) and keep the rest (1352 texts) for testing. A  shuffling \ntechnique was implemented to block the model from learning \nthe specific order of words and inputs and provide a more \nrealistic result. Table I shows the scores achieved from \ntraining on the classifiers.\nThe corpus works well with machine learning and deep \nlearning classifiers, except for KNN, with the lowest score \nof 0.57. In terms of precision, the multinomial classifier \noutperforms other classical classifiers with a score of \n0.91. Moreover, the second-best classifiers are SGD and \nFig. 4. The design of the full fine-tuned (proposed) model.\nFig. 5. Sample texts in the corpus.\nFig. 6. The amount of data in the corpus.\nFig. 7. The accuracy PER 2 epochs.\nTABLE 1\nThe Score of (Precision, Recall, Accuracy , and f1_score ) Metrics QF \nEach Classifier\nClassifier Precision Recall Accuracy F1_score\nMultinomial 0.91 0.90 0.90 0.90\nSGD 0.90 0.89 0.89 0.88\nDecision tree 0.85 0.85 0.85 0.85\nRandom forest 0.90 0.89 0.89 0.89\nSVM 0.82 0.82 0.82 0.82\nKNN 0.33 0.57 0.57 0.42\nLogistic regression 0.89 0.88 0.88 0.87\nBert multilingual 0.92 0.92 0.92 0.92\nSVM: Support vector machines, kNN: k-nearest neighbor, SGD: Stochastic gradient \ndescent\n14 http://dx.doi.org/10.14500/aro.11088\nARO p-ISSN: 2410-9355, e-ISSN: 2307-549X \nrandom forest, scoring 0.90. The rest of the classifiers stay \nbetween 0.80 and 0.89. Similarly, multinomial yields a more \nsignificant result than other ML classifiers for recall which is \n0.90. SGD and random forest come second by scoring one \npoint lower than multinomial.\nRegarding deep learning, BERT outperforms the classical \nclassifier by scoring the highest point in all measurements, \nwhich is 0.92. the model was merely trained for ten epochs. It is \nessential to state that training the model on a higher number helps \nthe model achieve a score closer to 100% for all measurements, \nas shown in Fig. 7. Furthermore, the validation loss decreases \nsignificantly, guaranteeing a higher accuracy score.\nTraining loss is a metric to evaluate how well a deep-\nlearning model fits the training data. On the other hand, \nvalidation loss specifies how well a deep learning model \nperforms when evaluated against validation data. Moreover, \nthe validation and training accuracy measure the modelâ€™s \noverfitting. Overfitting refers to a statistical modeling error \nthat occurs when a function is too closely aligned with a \nlimited set of data points. Thus, the model is only helpful \nfor its initial data set and not any other data sets. Since the \ngap between training loss and validation loss is too narrow, \nas illustrated in Fig. 7, it indicates that our model is fully \noptimized and has zero overfitting cases.\nIn the next phase, We attempt to compare our model \nwith another state-of-the-art model in the Kurdish language. \nUnfortunately, this matter has yet to be tackled by researchers \nin the language. Therefore, we sought other language models \nwhich are close to the English language. We discovered a \nBert-based model in the persian language known as ParsBert. \nMoreover, we compared our fine-tuned BERT model with the \nlatest ParsBert model (Farahani, et al., 2021). ParBert is a \nrecent state-of-the-art model developed for persian languages. \nThe pre-trained model is used for numerous tasks such as \ntext classification, question-answering, and named entity \nrecognition. We trained our dataset on the pre-trained model; \nthe results are displayed (Table II).\nOverall, it can be noted that our fine-tuned model works \nslightly better than ParsBert, particularly in the case of \nprecision and F1_score. The main reason behind this is \nthat the Persian language is close to the Kurdish language, \nparticularly in the sense of having nearly similar alphabetical \nletters and many standard vocabularies that exited in our \ncorpus, as shown in Table III. Naturally, these similarities \nhelped PrasBert find the tokens for most of the lexicons in \nour corpus, which ultimately yielded high results for ParsBert.\nIt is worth noting that having such a model is crucial to \nthe Kurdish language. Even though this is the first time, the \nKurdish language is introduced to a pre-trained model like \nBERT. The model outperformed the state-of-the-art by fine-\ntuning and using widespread softmax activation. The outcome \nachieved can add another source for the Kurdish language \nand prevent it from being labeled a less-resourced language. \nMoreover, the model can be utilized on Kurdish clinical \nwebsites or social media pages to separate medical and \nnonmedical questions. They can answer medical questions and \nserve their users, guaranteeing more customers. This is because \nour model can recognize non-medical text with high accuracy.\nVI. Conclusion\nIn this paper, we experimented with the recently published \nmedical corpus for the Kurdish language using machine \nlearning and deep learning (BERT) to classify texts. We \nremoved the stop words and irrelevant texts in the pre-\nprocessing stage. We compared the performances of the \ndeep learning method with the conventional machine \nlearning classifiers. Our experiments indicate that the \nBERT-multilingual model achieved higher accuracy of \n0.92 in the text classification task and showed at least +0.2 \nimprovement over the traditional machine learning methods. \nFor future work, we suggest using augmentation techniques \nby lemmatizing and giving the stem of the keywords in the \ninput data, as this yielded higher results in other languages.\nReferences\nAbdulla, S. and Hama, M. H., 2015. Sentiment analyses for kurdish social \nnetwork texts using naive bayes classifier. Journal of University of Human \nDevelopment, 1(4), pp. 393-397.\nAhmadi, S., 2020. KLPT-Kurdish Language Processing Toolkit., In: Proceedings \nof Second Workshop for NLP Open Source Software (NLP-OSS), pp. 72-84.\nCieliebak, M., Deriu, J.M., Egger, D. and Uzdilli, F., 2017. A Twitter Corpus \nand Benchmark Resources for German Sentiment Analysis. In: Proceedings of \nthe Fifth International Workshop on Natural Language Processing for Social \nMedias. pp. 45-51.\nCollobert, R., Weston, J., Bottu, L., Karlen, M., Kavukcuoglu, K. and Kuksa, P., \n2011. Natural language processing (almost) from scratch. Journal of Machine \nLearning Research, 12, pp. 2493-2537.\nDevlin, J., Chang, M.W., Lee, K. and Toutanova, K., 2018. BERT: Pre-training \nof deep bidirectional transformers for language understanding. Available from: \nhttp://arxiv.org/abs/1810.04805\nEsmaili, K., 2012. Challenges in Kurdish text processing. arXiv preprint \narXiv:1212.0074.\nFarahani, M., Gharachorloo, M., Farahani, M. and Manthouri, M., 2021. Parsbert: \nTransformer-based model for persian language understanding. Neural Processing \nLetters, 53(6), pp. 3831-3847.\nHoang, M., Bihorac, O.A. and Rouces, J., 2019. Aspect-Based Sentiment Analysis \nUsing Bert. In: Proceedings of the 22 nd Nordic Conference on Computational \nLinguistics. pp. 187-196.\nTABLE II\nThe Score of (Precision, Recall, Accuracy , and F1_Score ) Metrics QF \nMultilingual and Parsbert\nModel Precision Recall Accuracy F1_score\nBERT-multilingual 92 92 92 92\nParsBert 91 92 92 91\nTABLE III\nSample of Common Words between Kurdish and Persian\nKurdish Persian English meaning\nØ¯Ú©ØªÙˆØ±Ø¯Ú©ØªØ± Doctor\nÙ…Ùˆ)Ù‚Ú˜)Ù…Ùˆ Hair\nØ³Û•Ø±Ø³Ø± Head\nØ¯Û•Ø³ØªØ¯Ø³Øª Hand\nØ¹ÛŒÙ„Ø§Ø¬Ø¹ÛŒÙ„Ø§Ø¬ Treatment\nÚ¯Û†Ø´ØªÚ¯ÙˆØ´Øª Meat\nhttp://dx.doi.org/10.14500/aro.11088 15\n ARO p-ISSN: 2410-9355, e-ISSN: 2307-549X\nLeCun, Y ., Bengio, Y . and Hinton, G., 2015. Deep learning. Nature, 521(7553), \npp. 436-444.\nLing, J., 2020. Coronavirus Public Sentiment Analysis with BERT Deep Learning. \nDalarna University, Sweden.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. and Dean, J. 2013. Distributed \nrepresentations of words and phrases and their compositionality. In: Proc. \nAdvances in Neural Information Processing Systems. 26, pp.3111â€“3119.\nPennington, J., Socher, R. and Manning, C., 2014. Glove: Global vectors for word \nrepresentation. In: Proceedings of the 2014 Conference on Empirical Methods \nin Natural Language Processing (EMNLP), pp. 1532-1543.\nSaeed, A.M., Hussein, S. R., Ali, C.M. and Rashid, T. A., 2022. Medical dataset \nclassification for Kurdish short text over social media. Data Brief, 42, p.108089.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L, Gomez, A.N., \nKaiser, L. and Polosukhin, I., 2017. Attention is all you need. In:  Advances in \nNeural Information Processing Systems, V ol. 30. NeurIPS Proceedings.\nZahera, H. M. Elgendy, I., Jalota, R. and Sherif, M.A., 2019. Fine-tuned \nBERT Model for Multi-Label Tweets Classification. The Real Estate Company, \nMumbai. pp. 1-7.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7497661709785461
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7397668361663818
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6738193035125732
    },
    {
      "name": "Natural language processing",
      "score": 0.6353625059127808
    },
    {
      "name": "Transformer",
      "score": 0.587769091129303
    },
    {
      "name": "Encoder",
      "score": 0.5780023336410522
    },
    {
      "name": "Deep learning",
      "score": 0.5006270408630371
    },
    {
      "name": "Machine learning",
      "score": 0.4244697690010071
    },
    {
      "name": "Engineering",
      "score": 0.10645949840545654
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4392738253",
      "name": "Charmo University",
      "country": null
    },
    {
      "id": "https://openalex.org/I2802007472",
      "name": "Kurdistan Regional Government",
      "country": "IQ"
    }
  ]
}