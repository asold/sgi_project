{
  "title": "MoPe: Model Perturbation based Privacy Attacks on Language Models",
  "url": "https://openalex.org/W4389523877",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2599534336",
      "name": "Marvin Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097267242",
      "name": "Jason Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102386505",
      "name": "Jeffrey Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2612855560",
      "name": "Seth Neel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2795435272",
    "https://openalex.org/W3170901302",
    "https://openalex.org/W4281774243",
    "https://openalex.org/W4385264929",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W142716777",
    "https://openalex.org/W2040228409",
    "https://openalex.org/W4385572399",
    "https://openalex.org/W4250452543",
    "https://openalex.org/W4385565597",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4286909613",
    "https://openalex.org/W4287077751",
    "https://openalex.org/W2945237470",
    "https://openalex.org/W4280616142",
    "https://openalex.org/W4307937235",
    "https://openalex.org/W4283805899",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4321593464",
    "https://openalex.org/W4308410483",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W1990457366",
    "https://openalex.org/W2626769593",
    "https://openalex.org/W4400019960",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W4288057780",
    "https://openalex.org/W2182396527",
    "https://openalex.org/W4366974303",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4281713799",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W4320342485",
    "https://openalex.org/W4226137521",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4383860818",
    "https://openalex.org/W4287179775",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3153896080",
    "https://openalex.org/W4308828973",
    "https://openalex.org/W3035034338",
    "https://openalex.org/W4287553002"
  ],
  "abstract": "Recent work has shown that Large Language Models (LLMs) can unintentionally leak sensitive information present in their training data. In this paper, we present Model Perturbations (MoPe), a new method to identify with high confidence if a given text is in the training data of a pre-trained language model, given white-box access to the models parameters. MoPe adds noise to the model in parameter space and measures the drop in log-likelihood at a given point x, a statistic we show approximates the trace of the Hessian matrix with respect to model parameters. Across language models ranging from 70M to 12B parameters, we show that MoPe is more effective than existing loss-based attacks and recently proposed perturbation-based methods. We also examine the role of training point order and model size in attack success, and empirically demonstrate that MoPe accurately approximate the trace of the Hessian in practice. Our results show that the loss of a point alone is insufficient to determine extractability—there are training points we can recover using our method that have average loss. This casts some doubt on prior works that use the loss of a point as evidence of memorization or unlearning.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13647–13660\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMoPe\n :\nModel Perturbation-based Privacy Attacks on Language Models\nMarvin Li∗\nHarvard College\nJason Wang∗\nHarvard College\nJeffrey Wang∗\nHarvard College\nSeth Neel†\nHarvard University\nAbstract\nRecent work has shown that Large Language\nModels (LLMs) can unintentionally leak\nsensitive information present in their training\ndata. In this paper, we present MoPeθ (Model\nPerturbations), a new method to identify with\nhigh confidence if a given text is in the training\ndata of a pre-trained language model, given\nwhite-box access to the models parameters.\nMoPeθ adds noise to the model in parameter\nspace and measures the drop in log-likelihood\nat a given point x, a statistic we show\napproximates the trace of the Hessian matrix\nwith respect to model parameters. Across\nlanguage models ranging from 70M to 12B\nparameters, we show that MoPeθ is more\neffective than existing loss-based attacks and\nrecently proposed perturbation-based methods.\nWe also examine the role of training point\norder and model size in attack success, and\nempirically demonstrate that MoPeθ accurately\napproximate the trace of the Hessian in practice.\nOur results show that the loss of a point alone is\ninsufficient to determine extractability—there\nare training points we can recover using our\nmethod that have average loss. This casts some\ndoubt on prior works that use the loss of a point\nas evidence of memorization or “unlearning.”\n1 Introduction\nOver the last few years, Large Language Models\nor LLMs have set new standards in performance\nacross a range of tasks in natural language\nunderstanding and generation, often with very\nlimited supervision on the task at hand (Brown\net al., 2020). As a result, opportunities to use\nthese models in real-world applications proliferate,\nand companies are rushing to deploy them in\napplications as diverse as A.I. assisted clinical\ndiagnoses (Sharma et al., 2023), NLP tasks in\nfinance (Wu et al., 2023), or an A.I. “love coach”\n∗Alphabetical order; equal contribution.\n†Senior author, email: sneel@hbs.edu for correspondence.\n(Soper, 2023). While early state of the art LLMs\nhave been largely trained on public web data\n(Radford et al., 2019; Gao et al., 2021; Biderman\net al., 2023b; Black et al., 2021), increasingly\nmodels are being fine-tuned on data more relevant\nto their intended domain, or even trained from\nscratch on this domain specific data. In addition to\nincreased performance on range of tasks, training\nmodels from scratch is attractive to companies\nbecause early work has shown it can mitigate some\nof the undesirable behavior associated with LLMs\nsuch as hallucination (Ji et al., 2022), toxicity\n(Gehman et al., 2020), as well as copyright issues\nthat may arise from mimicking the training data\n(Franceschelli and Musolesi, 2021; Vyas et al.,\n2023). For example, BloombergGPT (Wu et al.,\n2023) is a 50-billion parameter auto-regressive\nlanguage model that was trained from scratch\non financial data sources, and exhibits superior\nperformance on tasks in financial NLP.\nWhile all of this progress seems set to usher in an\nera where companies deploy custom LLMs trained\non their proprietary customer data, one of the main\ntechnical barriers that remains is privacy. Recent\nwork has shown tha language model’s have the\npotential to memorize significant swathes of their\ntraining data, which means that they can regurgitate\npotentially private information present in their\ntraining data if deployed to end users (Carlini\net al., 2021; Jagannatha et al., 2021). LLMs are\ntrained in a single pass and do not “overfit” to the\nsame extent as over-parameterized models, but on\nspecific outlier points in the training set they do\nhave loss much lower than if the point had not\nbeen included during training (Carlini et al., 2021),\nallowing an adversary to perform what is called a\nmembership inference attack (Shokri et al., 2017):\ngiven access to the model and a candidate sample\nx′, the adversary can determine with high-accuracy\nif x′is in the training set.\nWhile prior work shows privacy is a real\n13647\nconcern when deploying language models, the\nmembership inference attacks used can extract\nspecific points but perform quite poorlyon average,\nleaving significant room for improvement (Yu\net al., 2023; Carlini et al., 2021). At the same\ntime, recent works studying data deletion from\npre-trained LLMs (Jang et al., 2022) and studying\nmemorization (Carlini et al., 2023a) cite the loss\nof a point as a determinant of whether that point\nhas been “deleted” from the model, or conversely\nis “memorized.” Given that loss of a point is a\nrelatively poor indicator of training set membership\nfor LLMs, this raises a series of tantalizing research\nquestions we address in this work: (i) If a point has\naverage “loss” with respect to a language model θ,\ndoes that imply it cannot be detected as a training\npoint? (ii) Can we develop strong MIAs against\nLLMs better than existing loss-based attacks?\nWe develop a new membership inference attack\nwe call MoPeθ, based on the idea that when the\nmodel loss is localized around a training point, θ\nis likely to lie in a “sharper” local minima than\nif the point was a test point— which does not\nnecessarily imply that the absolute level of the\nloss is low. Concretely, our attack perturbs the\nmodel with mean zero noise, and computes the\nresulting increase in log loss at candidate point x\n(see Figure 1).\nFigure 1: MoPeθ detects training point membership by\ncomparing the increase in loss on the training point\nwhen the model is perturbed, relative to the increase in\nloss of the perturbed model on a test point.\nContributions. In this paper we make several\ncontributions to the developing story on language\nmodel privacy.\n• We show in Section 2.2 that ourMoPeθ statistic\nis approximating the trace of the Hessian\nmatrix with respect to model weights θ at\n(θ,x), and so can be seen as an approximation\nof how “sharp” the loss landscape is on x\naround θ.\n• We evaluateMoPeθ on the recently developed\nPythia suite from EleutherAI (Biderman et al.,\n2023b) on models that range in size from70M\nto 12B. Our experimental results in Section 3\nshow that compared to existing attacks based\non the loss (LOSSθ), MoPeθ boasts significantly\nhigher AUC on all model sizes up to2.8B, and\ncomparable AUC at 6.9B and 12B parameters.\nFurthermore, at low FPRs of the attack,MoPeθ\nmaintains high true positive rates, whereas\nLOSSθ fails to outperform the random baseline\n(Figure 2).\n• We evaluate whether theDetectGPT statistic\nproposed for detecting LLM-generated text\ncan be repurposed as a privacy attack against\npre-trained models. Our experimental results\nshow that DetectGPT outperforms the loss-\nbased attack at all model sizes up to 2.8B, but\nis dominated by our MoPeθ attack at all model\nsizes.\n• Since all the models we evaluate were\ntrained on the same datasets in the same\ntraining order, we can make a principled\nstudy of the relationship between model sizes\nand order of the training points to attack\nsuccess. Surprisingly, we find in Table 1 and\nFigure 7 that consistent with prior workLOSSθ\nattack success increases with model size and\nproximity during training, but that these trends\nfail to hold for MoPeθ.\n• Finally in Section 4 we train a network on\nMNIST (Deng, 2012) that is sufficiently small\nthat we are able to compute the Hessian.\nWe use this network to show that our MoPeθ\nstatistic does in practice approximate the\nHessian trace. We also find that while MoPeθ\ndoes outperform the random baseline, the\nattack performs worse than loss in this setting,\nhighlighting that our results may be unique to\nlanguage models.\nThese results establish our technique as state of\nthe art for MIA against pre-trained LLMs in the\nwhite-box setting where we can access the model\nweights. They also challenge conventional wisdom\nthat the loss of a point is in isolation a sufficently\ngood proxy for training set membership.\n2 Related Work\nDetectGPT. The most closely related work to\nours on a technical level is the recent paper\n13648\n(Mitchell et al., 2023) who propose a perturbation-\nbased method for detecting LLM-generated text\nusing probability curvature. Their method\nDetectGPT compares the log probabilities of a\ncandidate passage x and its randomly perturbed\nversions m(x) using the source model and another\ngeneric pre-trained mask-filling model m; large\ndrops in the log probability correspond to training\npoints. While superficially similar to our MoPeθ\nmethod, there are significant differences: (i) We\nfocus on the problem of membership inference\nwhich tries to determine if an example x was\npresent in the training data, whereas their focus\nis determining if x was generated by θ—an\northogonal concern from privacy and (ii) As a\nresult, we perturb the model θ using Gaussian\nnoise, rather than perturbing xusing an additional\nmodel.\nMemorization and Forgetting in Language\nModels. There are many recent works studying\nissues of memorization or forgetting in language\nmodels (Biderman et al., 2023a; Thakkar et al.,\n2020; Kharitonov et al., 2021; Zhang et al.,\n2021; Ippolito et al., 2023). (Jagielski et al.,\n2023) measures the forgetting of memorized\nexamples over time during training, and shows\nthat in practice points occurring earlier during\ntraining observe stronger privacy guarantees. They\nmeasure “forgetting” using membership inference\nattack accuracy, using an attack based on loss-\nthresholding with an example-calibrated threshold.\nBy contrast (Biderman et al., 2023b) studies\nmemorization using the Pythia suite, and finds\nthat the location of a sequence in training does\nnot impact whether the point was memorized,\nindicating that memorized points are not being\nforgotten over time. This finding is further\nconfirmed in (Biderman et al., 2023a), which\nshows that memorization by the final model can be\npredicted by checking for memorization in earlier\nmodel checkpoints. Our work sheds some light\non the apparent discrepancy between these two\nworks, indicating that results in (Biderman et al.,\n2023a) are possible because loss value alone does\nnot necessarily imply a point is not memorized.\nMembership Inference Attacks (MIAs).\nMembership Inference Attacks or MIAs were\ndefined by (Homer et al., 2008) for genomic\ndata, and formulated by (Shokri et al., 2017)\nagainst ML models. In membership inference,\nan adversary tries to use model access as well as\nsome outside information to determine whether\na candidate point x is a member of the model\ntraining set. Since (Yeom et al., 2018), MIAs\ntypically exploit the intuition that a point x is\nmore likely to be a training point if the loss of\nthe model evaluated at xis small, although other\napproaches that rely on a notion of self-influence\n(Cohen and Giryes, 2022) or distance to the model\nboundary (Pawelczyk et al., 2022; Choquette-Choo\net al., 2020) have been proposed. (Sablayrolles\net al., 2019) argue (under strong assumptions)\nthat the loss-thresholding attack is approximately\noptimal by the Neyman-Pearson lemma. The\nassumptions needed to show optimality do not\nhold in practice, however, which opens the door to\nsubstantially improving this simple attack. State\nof the art membership inference attacks still rely\non loss-thresholding or functions of the loss (Ye\net al., 2021; Carlini et al., 2022; Sablayrolles\net al., 2019), but try to calibrate the threshold\nat which a point x is declared a training point\nto account for the component of the loss that is\nspecific to the example x. One way they do this\nis by (i) Training shadow models that do/don’t\ncontain the candidate point x, and (ii) Using the\nloss of these models on the point xto empirically\napproximate the ratio of the likelihood of the\nobserved loss ℓ(θ,x) given xwas in the training\nset to the likelihood if x was not (Carlini et al.,\n2022). A simpler, but related attack assumes\naccess to a reference model θref that was not\ntrained on the point xin question, and thresholds\nbased on the ratio log(ℓ(x,θ)/ℓ(x,θref)) (Carlini\net al., 2021). In this paper, we make the realistic\nassumption that the adversary does not have access\nto the computational resources to train additional\nlanguage models from scratch on fresh data, or\nhave access to a reference model that has been\ntrained without the candidate point x. We note that\nin the case where this was possible, these same\nthreshold calibration techniques could be used to\naccelerate the effectiveness of our MoPeθ attack as\nwell.\nMIAs on LLMs. Concurrent work proposes\nseveral new MIAs tailored to LLMs. (Abascal et al.,\n2023) investigates MIAs where the adversary has\naccess only to a fine-tuned model, and tries to make\ninferences back to the pre-trained model. Their\nmethod operates in a different access model than\nours, and more importantly relies heavily on the\n13649\ntraining of additional shadow models, and therefore\ndoes not scale to the large model sizes studied\nhere. (Mattern et al., 2023) use a statistic similar\nto that used in DetectGPT (Mitchell et al., 2023)\nfor detecting model-generated text for membership\ninference, in that they perturb the example xusing\na mask-filling model (BERT) and use the resulting\nlosses to calibrate the threshold. They conduct\ntheir experiments on the 117M parameter version\nof GPT-2, which they then fine-tune AG News\nand Twitter Data. As such their results can also\nbe viewed as focused on small models in the fine-\ntuning setting, where it is known that much stronger\nattacks are possible (Jagielski et al., 2022; Abascal\net al., 2023), in contrast to the pre-training regime\nstudied here.\nDiscussion of additional related work focused\non privacy issues in language models, including\ntraining data extraction, data deletion or\nmachine unlearning, and mitigation strategies via\ndifferential privacy, are deferred to Section A of\nthe Appendix.\n2.1 Preliminaries\nAn auto-regressive language model denoted by\nθ takes as input a sequence of tokens x =\nx1x2 ...x T, and outputs a distribution pθ(·|x) ∈\n∆(V) over a vocabulary V, the space of tokens.\nGiven a sequence of tokens x, the loss is defined as\nthe negative log-likelihood lof the sequence with\nrespect to θ:\nLOSSθ(x) = −\nT∑\ni=1\nlog(pθ(xt|x<t))\nAlternatively, flipping the sign gives us the\nconfidence confθ(x) = −LOSSθ(x). Given\na training corpus C sampled i.i.d from a\ndistribution D over sequences of tokens,\nθ is trained to minimize the average loss,\n1\n|C|minθ\n∑\nx∈C LOSSθ(x). This is typically\nreferred to as “pre-training” on the task of\nnext token prediction, to differentiate it from\n“fine-tuning” for other downstream tasks which\noccurs subsequent to pre-training.\nMembership Inference Attacks. We now define\na membership inference attack (MIA) against a\nmodel θ. A membership inference score M: Θ ×\nX → Rtakes a model θ, and a context x, and\nassigns it a numeric value M(x,θ) ∈R – larger\nscores indicate x is more likely to be a training\npoint (x∈C). When equipped with a threshold τ,\n(M,τ) define a canonical membership inference\nattack: (i) With probability 1\n2 sample a random\npoint xfrom C, else sample a random point x∼D.\n(ii) Use Mto predict whether x∈C or x∼D by\ncomputing M(x,θ) and thresholding with τ:\n(M,τ)(x) =\n{\nTRAIN if M(x,θ) >τ\nNOT TRAIN if M(x,θ) ≤τ\n(1)\nNote that by construction the marginal distribution\nof xis Dwhether xis a training point or not, and\nso if Mhas accuracy above 1\n2 must be through\nθ leaking information about x. The “random\nbaseline” attack (M,τ) that samples M(x,θ) ∼\nUniform[0,1] and thresholds by 1 −τ ∈ [0,1]\nhas TPR and FPR τ (where training points are\nconsidered positive). The most commonly used\nmembership inference attack, introduced in (Yeom\net al., 2018), takes M(x,θ) = −ℓ(x,θ), and so\nit predicts points are training points if their loss\nis less than −τ, or equivalently the confidence is\ngreater than τ. We refer to this attack throughout\nas the loss attack, or LOSSθ. Throughout the paper,\nas is common we overload notation and refer to\nMas a membership inference attack rather than\nspecifying a specific threshold.\nMIA Evaluation Metrics. There is some\ndisagreement as to the proper way to evaluate\nMIAs. Earlier papers simply report the best\npossible accuracy over all thresholds τ (Shokri\net al., 2017). Different values of τ correspond to\ntradeoffs between the FPR and TPR of the attack,\nand so more recently metrics like Area Under the\nROC Curve (AUC) have found favor (Carlini et al.,\n2022; Ye et al., 2021). Given that the ability to\nextract a very small subset of the training data with\nvery high confidence is an obvious privacy risk,\nthese recent papers also advocate reporting the TPR\nat low FPRs, and reporting the full ROC Curve in\norder to compare attacks. In order to evaluate our\nattacks we report all of these metrics: AUC, TPR at\nfixed FPRs of .25 and .05, and the full ROC curves.\n2.2 MoPeθ: Model Perturbation Attack\nOur Model Perturbations Attack (MoPeθ) is based\non the intuition that the loss landscape with respect\nto the weights should be different around training\nversus testing points. In particular, we expect that\nsince θ ≈argminΘ\n∑\nx∈Cℓ(x,θ), where ℓ is the\n13650\nnegative log-likelihood defined in Section 2.1, then\nthe loss around x′ ∈ Cshould be sharper than\naround a random point. Formally, given a candidate\npoint x∼D, we define:\nMoPeθ(x) = Eϵ∼N(0,σ2Inparams )[ℓ(x,θ+ϵ)−ℓ(x,θ)],\n(2)\nwhere σ2 is a variance hyperparameter that we\nspecify beforehand, and θ ∈Rnparams . In practice,\nrather than computing this expectation, we sample\nnnoise values ϵi ∼N(0,σ2Inparams ) and compute\nthe empirical MoPen\nθ(x) = 1\nn\n∑n\ni=1[ℓ(x,θ + ϵi) −\nℓ(x,θ)]. This gives rise to the natural MoPeθ\nthresholding attack, with M(x,θ) = MoPeθ(x) in\nEquation 1. Note that computing each perturbed\nmodel takes time O(nparams) and so we typically\ntake n ≤20 for computational reasons. We now\nprovide some theoretical grounding for our method.\nConnection to the Hessian Matrix.In order to\nderive a more intuitive expression forMoPeθ(x) we\nstart with the multivariate Taylor approximation\n(Königsberger, 2000):\nℓ(θ+ ϵ,x) = ℓ(θ,x) + ∇θℓ(θ,x) ·ϵ+ (3)\n1\n2ϵTHxϵ+ O(ϵ3) (4)\nwhere Hx = ∇2\nθℓ(θ,x) is the Hessian of log-\nlikelihood with respect to θevaluated at x. Then\nassuming σ2 is sufficiently small that O(ϵ3) in\nEquation 3 is negligible, rearranging terms and\ntaking the expectation with respect to ϵ of both\nsides of (3) we get:\nMoPeθ(x) = Eϵ∼N(0,σ2)[ℓ(θ+ ϵ,x) −ℓ(θ,x)] ≈\nEϵ∼N(0,σ2)[1\n2ϵTHxϵ] = σ2\n2 Tr(Hx),\nwhere the last identity is known as the Hutchinson\nTrace Estimator (Hutchinson, 1989). This\nderivation sheds some light on the importance of\npicking an appropriate value of σ. We need it to\nbe small enough so that the Taylor approximation\nholds in Equation 3, but large enough that we\nhave enough precision to actually distinguish the\ndifference in the log likelihoods in Equation 2.\nEmpirically, we find that σ = 0.005 works well\nacross all model sizes, and we don’t observe\na significant trend on the optimal value of σ\n(Table 2).\n3 MIA Against Pre-trained LLMs\nIn this section we conduct a thorough empirical\nevaluation of MoPeθ, focusing on attacks against\npre-trained language models from the Pythia suite.\nWe show that in terms of AUCMoPeθ significantly\noutperforms loss thresholding ( LOSSθ) at model\nsizes up to2.8B and can be combined withLOSSθ to\noutperform at the 6.9B and 12B models (Figure 4).\nWe also implement an MIA based on DetectGPT,\nwhich outperforms loss in terms of AUC up to size\n1.4B, but is consistently worse than MoPeθ at every\nmodel size. Our most striking finding is that if we\nfocus on the metric of TPRs at low FPRs, which\nstate-of-the-art work on MIAs argue is the most\nmeaningful metric (Carlini et al., 2022; Ye et al.,\n2021), MoPeθ exhibits superior performance at all\nmodel sizes (Figure 2). MoPeθ is the only attack\nthat is able to convincingly outperform random\nguessing while driving the attack FPR ≤ 50%\n(which is still a very high FPR!) at all model sizes.\nDataset and Models. We identified EleutherAI’s\nPythia (Biderman et al., 2023b) suite of models\nas the prime candidate for studying membership\ninference attacks. We defer a more full discussion\nof this choice to Section B in the Appendix, but\nprovide some explanation here as well. Pythia\nwas selected on the basis that the models are\navailable via the open source provider Hugging\nFace, cover a range of sizes from 70M to 12B,\nand have a modern decoder-only transformer-\nbased architecture similar to GPT-3 with some\nmodifications (Brown et al., 2020; Biderman et al.,\n2023b). Models in the Pythia suite are trained\non the Pile (Gao et al., 2021) dataset, which\nafter de-duplication contains 207B tokens from\n22 primarily academic sources. De-duplication\nis important as it has been proposed as one\neffective defense against memorization in language\nmodels, and so the fact that our attacks succeed\non models trained on de-duplicated data makes\nthem significantly more compelling (Kandpal et al.,\n2022; Lee et al., 2021). Crucially for our MIA\nevaluations, the Pile has clean training vs. test\nsplits, as well as saved model checkpoints that\nallow us to ensure that we use model checkpoints\nat each size that correspond to one full pass over\nthe dataset.\nWe evaluate all attacks using 1000 points\nsampled randomly from the training data and\n1000 points from the test data. Since the MoPeθ\n13651\nand DetectGPT attacks are approximating an\nexpectation, we expect these attacks to be more\naccurate if we consider more models, but we\nalso need to balance computational considerations.\nFor MoPeθ we use n = 20 perturbed models\nfor all Pythia models with ≤ 2.8B parameters,\nand 10 perturbed models for the 6.9B and 12B\nparameter models. For DetectGPT we use n= 10\nperturbations throughout rather than the 100 used\nin (Mitchell et al., 2023) in order to minimize\ncomputation time, which can be significant at 12B.\nWe found the best MoPeθ noise level σ for the\nmodels of size ≤2.8B parameters by conducting a\ngrid search over σ = [ .001,.005,.01,.05]. The\nhighest AUC was achieved at .01 for the 1B\nparameter model, .001 for the 2.8B model and\n.005 for the remaining models. This suggests that\nthere is no relationship between the optimal value\nof σ and the model size. For the 6.9B and 12B\nparameter models, we chose a noise level of .005.\nWe record the AUCs achieved at different noise\nlevels in Table 2 in the Appendix. For DetectGPT\nwe follow (Mitchell et al., 2023) and use T5-small\n(Raffel et al., 2019) as our mask-filling model,\nwhere we mask 15% of the tokens in spans of 2\nwords.\nAttack Success. The results in Table 1 show\nthat thresholding based on MoPeθ dramatically\nimproves attack success relative to thresholding\nbased on LOSSθ or DetectGPT. The difference\nis most pronounced at model sizes 160M, 410M\nparameters, where the AUC for MoPeθ is ≈\n27% higher than the AUC of .51 for LOSSθ\nand DetectGPT, which barely outperform random\nguessing. This out-performance continues up to\nsize 2.8B, after which point all attacks achieve\nAUC in the range of .50 −.53.\nInspecting the shape of the ROC curves in\nFigure 2 there is an even more striking difference\nbetween the attacks: All of the curves for the\nLOSSθ and DetectGPT-based attacks drop below\nthe dotted line (random baseline) between FPR =\n.5–.6. This means that even at FPRs higher than\n.5, these attacks have TPR worse than random\nguessing! This establishes that, consistent with\nprior work, LOSSθ based attacks against LLMs\ntrained with a single pass work extremely poorly\non average, although they can identify specific\nmemorized points (Carlini et al., 2022).\nBy contrast, theMoPeθ curves lie about the dotted\nline (outperforming random guessing) for FPRs\nthat are much much smaller. Table 1 reports\nthe TPR at FPR = .25,.05. We see that even at\nmoderately large FPR = .25, only MoPeθ achieves\nTPR >.25 at all model sizes, with TPRs1.2−2.5×\nhigher than DetectGPT and MoPeθ. We note that\nnone of the attacks consistently achieve TPR better\nthan the baseline at FPR = .05, but that MoPeθ still\nperforms better than the other attacks.\nModel Method AUC TPR.25 TPR.05\n70M LOSSθ .507 .168 .025\n160M LOSSθ .512 .170 .030\n410M LOSSθ .513 .167 .028\n1B LOSSθ .516 .172 .029\n1.4B LOSSθ .517 .176 .029\n2.8B LOSSθ .504 .161 .021\n6.9B LOSSθ .523 .181 .026\n12B LOSSθ .525 .188 .024\n70M DetectGPT .521 .201 .035\n160M DetectGPT .515 .202 .033\n410M DetectGPT .525 .206 .032\n1B DetectGPT .532 .207 .031\n1.4B DetectGPT .534 .210 .029\n2.8B DetectGPT .510 .183 .022\n6.9B DetectGPT .506 .178 .024\n12B DetectGPT .504 .17 .020\n70M MoPeθ .612 .306 .049\n160M MoPeθ .646 .376 .049\n410M MoPeθ .650 .411 .072\n1B MoPeθ .567 .261 .046\n1.4B MoPeθ .571 .259 .047\n2.8B MoPeθ .565 .280 .040\n6.9B MoPeθ .522 .252 .020\n12B MoPeθ .516 .257 .024\nTable 1: For each model size and attack, we report the\nAUC, TPR at FPR .25, and TPR at FPR .05.\nModel Size. Recent work (Carlini et al., 2023a)\non the GPT-Neo (Black et al., 2021) models\nevaluated on the Pile has shown that as model\nsize increases so does memorization. The data\nin Table 1 support this conclusion, as with\nincreasing model size we see a (small) monotonic\nincrease in the AUC achieved by the LOSSθ attack.\nInterestingly, we observe almost an opposite trend\nin curves forMoPeθ, with the highest values of AUC\nactually coming at the three smallest model sizes!\nDetectGPT has AUC that is also flat or slightly\ndecreasing with increased model size. We note\nthat while this does not directly contradict prior\n13652\nFigure 2: LOSSθ, DetectGPT,and MoPeθ ROC Curves\nacross all model sizes. Only MoPeθ outperforms the\nbaseline at FPR <.5.\nresults from (Carlini et al., 2023a) which use a\ndefinition of memorization based on extraction\nvia sampling, it is surprising given the intuition\nfrom prior work. One potential explanation could\nbe that the attack success of perturbation-based\nmethods like MoPeθ and DetectGPT is actually\npartially inversely correlated with model size, due\nto variance of the Hutchinson Trace Estimator in\nEquation 3 increasing for larger models, and may\nnot reflect the fact that larger Pythia suite models\nare actually more private.\nTraining Order. In this section we study the\ndegree to which the models in the Pythia suite\nexhibit “forgetting” over time, as measured by\nthe change in LOSSθ and MoPeθ statistics during\ntraining. (Jagielski et al., 2023) show that,\nparticularly on large data sets (like the Pile for\nexample), MI accuracy increases (equivalently\nloss decreases) for points in more recent batches.\nIn Figure 7 in the Appendix, we investigate\nhow LOSSθ and MoPeθ statistics vary on average\ndepending on when they are processed during\ntraining. Concretely, we sample 2000 points from\n10 paired batches {0 −1,9999 −1e4,19999 −\n2e4,... 89999 − 9e4,97999 − 9.8e4}, which\napproximately correspond to the first and last data\nbatches that the Pythia models see during pre-\ntraining. For each set of 2000 points, we compute\nthe average LOSSθ and MoPeθ values with respect\nto the θreached at the end of the first epoch. We\nsee that, consistent with findings in (Jagielski et al.,\n2023), loss declines for more recent batches, but\nby contrast, there is no such observable pattern at\nany fixed model size for the MoPeθ statistic! This\nfinding is consistent with recent work (Biderman\net al., 2023a,b) that study memorization in the\nPythia suite, and find no correlation between order\nin training and if a point is “memorized” (as defined\nin terms of extractability).\nMoPeθ vs. LOSSθ comparison. The disparities\nin MIA performance between MoPeθ and LOSSθ\nattacks shown in Figure 2 implies that there must\nexist a number of training points where the MoPeθ\nand LOSSθ statistics take very different values. The\nfact that MoPeθ outperforms LOSSθ, particularly at\nsmaller model sizes, implies that there are points\nwith average loss values but outlier MoPeθ values.\nWe visualize this for the 12B parameter model in\nFigure 3, and include plots for all model sizes in\nFigure 8 in the Appendix.\nLOSSθ and MoPeθ Ensemble Attack. Recall that\na point is more likely to be a training point if it has a\nhigh MoPeθ value, or a low LOSSθ value. The above\nscatterplot shows that there are training points that\nhave average LOSSθ values and high MoPeθ values\nand are easily identified by the MoPeθ attack as\n13653\nFigure 3: −MoPeθ vs. LOSSθ scatter plot, at model size\n12B. We z-score the LOSSθ and MoPeθ values and apply\na log-modulus transform f(x) = sign(x) log(|x|+ 1)\n(John and Draper, 1980) to the scores for visual clarity.\ntraining points but not by LOSSθ, and vice versa.\nThis raises the obvious question of if the attacks\ncan be combined to yield stronger attacks than\neither attack in isolation. We find that while for\nthe smaller model sizes, we don’t get a significant\nimprovement over MoPeθ in AUC by ensembling,\nfor the two largest model sizes, where both LOSSθ\nand MoPeθ perform relatively poorly, we do get\na significant improvement by thresholding on a\nweighted sum of the two statistics (after z-scoring).\nWe plot the ROC curves for LOSSθ, MoPeθ, and\nthe optimal ensemble (picked to optimize AUC) in\nFigure 4. At 6.9B both LOSSθ and MoPeθ achieve\nAUC = .52, while the ensemble has AUC = .55.\nAt 12B AUC jumps from ≈ .52 to .544 in the\nensemble.\n4 MoPeθ on MNIST\nIn this section we run our MoPeθ attack on a small\nnetwork trained to classify images from MNIST\n(Deng, 2012), which allows us to evaluate if the\nMoPeθ statistic actually approximates the Hessian\ntrace in practice as the derivation in Section 2.2\nsuggests. To test this, we sample 5000 training\npoints and 5000 test points from the MNIST\ndataset, a large database of images of handwritten\ndigits (Deng, 2012). Using a batch size of 8, we\ntrain a fully connected 2 layer MLP with 20 and 10\nnodes, using the Adam optimizer with a learning\nrate of 0.004 and momentum of 0.9, until we reach\n94% train accuracy and 84% test accuracy, a 10\npercent train-test gap similar to that observed in\nFigure 4: LOSSθ and MoPeθ Ensemble Attack\npre-trained language models (Carlini et al., 2021).\nIn Figure 5 we show the distribution of the Hessian\ntrace and scaled MoPeθ evaluated on all training\nbatches. The resulting distributions on training and\ntest points are close, with a correlation between\npoint level traces and scaled MoPeθ values of 0.42.\nGiven the generality of the proposed MoPeθ\nattack, it is also natural to ask if MoPeθ can be\napplied to other machine learning models. On our\nMNIST network, we run the MoPeθ attack with 40\nperturbations and σ = 0.05, calculating the ROC\ncurves in Figure 6 using 3000 train and test points.\nWe find that LOSSθ outperforms MoPeθ, with MoPeθ\nachieving an AUC of 0.635 and LOSSθ achieving\nan AUC of 0.783. While it does not outperform\nLOSSθ, MoPeθ easily beats the random baseline,\nwhich warrants further study in settings beyond\nlanguage models.\n5 Discussion\nThe main finding in this work is that perturbation-\nbased attacks that approximate the curvature in the\n13654\nFigure 5: The distribution of MoPe statistics (left) and\nexact Hessian traces (right) for the MNIST dataset.\nFigure 6: ROC curve for LOSSθ vs MoPeθ on MNIST\nmodel.\nloss landscape around a given model and point,\nseem to perform better than attacks that simply\nthreshold the loss. A key nuance here, is that for\ncomputational reasons, all of the attacks studied\nhere apply a uniform threshold τ that does not\ndepend on the point x. Prior work on MIAs\nshows that calibrating the threshold τx to a specific\npoint x greatly increases attack success, for the\nobvious reason that certain types of points have\nhigher or lower relative loss values regardless of\ntheir inclusion in training. Thresholding based\non a loss ratio with another language model or\napproximating the likelihood ratio (LiRA) can all\nbe viewed as different ways of calibrating an\nexample-specific threshold. In this context, the\nresults in this paper can be viewed as showing\nthat when forced to pick a uniform threshold for\nan MIA score, statistics like MoPeθ or DetectGPT\nthat approximate the local curvature transfer better\nacross different points than the loss. As such,\nmethods that try to efficiently approximate an\nexample-specific threshold, like applying LiRA or\nloss-ratio thresholding in the fine-tuning regime\nwhere they are more computationally feasible, or\napplying the very recent work of (Bertran et al.,\n2023) that uses quantile regression to approximate\na likelihood-style attack without training additional\nmodels, in our setting is of great interest. We\nconjecture that such techniques will be needed to\nachieve high TPRs at very small FPRs. Another\nmajor unresolved question in this work is why\nMoPeθ and DetectGPT success actually scales\ninversely with model size, which is likely a property\nof the method, namely increasing error in the\nHutchinson trace estimator, rather larger models\nhaving improved privacy properties. Future work\nwill explore other approximations to the Hessian\ntrace that may scale better to large model sizes.\nFinally, future work could use MoPeθ as part of\na training data extraction attack in the style of\n(Carlini et al., 2021; Yu et al., 2023; Carlini et al.,\n2023b).\n13655\nLimitations\nWe now list several limitations of this work, all of\nwhich present opportunities for future research.\n• Our MoPeθ MIA operates in the white-box\nsetting that assumes access to the model\nparameters θ, whereas existing attacks are\ntypically in the black-box setting, where\nonly access to the model outputs or the\nloss are assumed. Nevertheless, our attack\nis still practical given that open source\nmodels or even proprietary models are\noften either published or leaked, and from\na security perspective it makes sense to\nassume attackers could gain access to model\nparameters. Moreover, the findings in\nthis paper have scientific implications for\nthe study of memorization and privacy in\nlanguage models that are orthogonal to the\nconsideration of attack feasibility.\n• We are only able to test our techniques on\nmodel sizes present in the Pythia suite (up\nto 12B parameters), and so the question of\nwhether these results will scale to larger model\nsizes is left open.\n• We were only able to optimize over a\nlimited set of noise values in MoPeθ due to\ncomputational reasons, and so MoPeθ may\nperform even better with a more exhaustive\nhyper-parameter search.\n• Another drawback of MoPeθ as a practical\nattack, is that computing perturbed models\ncan be computationally expensive at large\nmodel sizes, potentially limiting our ability\nto take enough models to accurately estimate\nthe Hessian trace.\nEthics Statement\nWe are satisfied this paper has been produced and\nwritten in an ethical manner. While the purpose\nof this paper is to demonstrate the feasibility of\nprivacy attacks on large language models, we did\nnot expose any private data in our exposition or\nexperiments. Moreover, all attacks were carried\nout on open source models that were trained on\npublic data, and as a result, there was limited risk\nof any exposure of confidential data to begin with.\nFinally, we propose these attacks in the hope they\nwill spur further research on improving privacy in\nlanguage models, and on privacy risk mitigation,\nrather than used to exploit existing systems.\nReferences\nJohn Abascal, Stanley Wu, Alina Oprea, and Jonathan\nUllman. 2023. Tmi! finetuned models leak private\ninformation from their pretraining data.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi\nKumar, and Pasin Manurangsi. 2021a. Large-scale\ndifferentially private bert.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi\nKumar, and Pasin Manurangsi. 2021b. Large-scale\ndifferentially private BERT. CoRR, abs/2108.01624.\nMartin Bertran, Shuai Tang, Michael Kearns, Jamie\nMorgenstern, Aaron Roth, and Zhiwei Steven Wu.\n2023. Scalable membership inference attacks via\nquantile regression.\nStella Biderman, USVSN Sai Prashanth, Lintang\nSutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raf. 2023a.\nEmergent and predictable memorization in large\nlanguage models.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al. 2023b.\nPythia: A suite for analyzing large language\nmodels across training and scaling. arXiv preprint\narXiv:2304.01373.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nscale autoregressive language modeling with\nmeshtensorflow.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-\nshot learners. In Advances in Neural Information\nProcessing Systems, volume 33, page 1877–1901.\nCurran Associates, Inc.\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang\nSong, Andreas Terzis, and Florian Tramer. 2022.\nMembership inference attacks from first principles.\nIn 2022 IEEE Symposium on Security and Privacy\n(SP), pages 1897–1914. IEEE.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023a. Quantifying memorization across neural\nlanguage models.\nNicholas Carlini, Utkarsh Kandpal, Jacob Lehman,\nNicolas Papernot, and Florian Tramèr. 2023b. Lm-\nextraction: A benchmark for training data extraction\nfrom language models. https://github.com/\ngoogle-research/lm-extraction-benchmark .\n13656\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B Brown, Dawn Song,\nUlfar Erlingsson, et al. 2021. Extracting training data\nfrom large language models. In USENIX Security\nSymposium, volume 6.\nChristopher A. Choquette-Choo, Florian Tramèr,\nNicholas Carlini, and Nicolas Papernot. 2020.\nLabel-only membership inference attacks. CoRR,\nabs/2007.14321.\nGilad Cohen and Raja Giryes. 2022. Membership\ninference attack using self influence functions.\nLi Deng. 2012. The mnist database of handwritten digit\nimages for machine learning research. IEEE Signal\nProcessing Magazine, 29(6):141–142.\nChristophe Dupuy, Radhika Arava, Rahul Gupta,\nand Anna Rumshisky. 2022. An efficient dp-sgd\nmechanism for large scale nlp models.\nCynthia Dwork, Frank McSherry, Kobbi Nissim,\nand Adam D. Smith. 2016. Calibrating noise\nto sensitivity in private data analysis. J. Priv.\nConfidentiality, 7(3):17–51.\nGiorgio Franceschelli and Mirco Musolesi. 2021.\nCopyright in generative deep learning.\nLeo Gao, Stella Biderman, Sid Black, Laurence\nGolding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2021. The pile: An\n800gb dataset of diverse text for language modeling.\nCoRR, abs/2101.00027.\nSamuel Gehman, Suchin Gururangan, Maarten\nSap, Yejin Choi, and Noah A. Smith. 2020.\nRealtoxicityprompts: Evaluating neural toxic\ndegeneration in language models. CoRR,\nabs/2009.11462.\nNils Homer, Szabolcs Szelinger, Margot Redman, David\nDuggan, John Tembe, Jill Muehling, John V . Pearson,\nDietrich A. Stephan, Stanley F. Nelson, and David W.\nCraig. 2008. Resolving individuals contributing trace\namounts of DNA to highly complex mixtures using\nhigh-density SNP genotyping microarrays. PLoS\nGenetics, 4(8):e1000167.\nMichael F. Hutchinson. 1989. A stochastic estimator\nof the trace of the influence matrix for laplacian\nsmoothing splines. Communications in Statistics -\nSimulation and Computation, 18:1059–1076.\nDaphne Ippolito, Florian Tramèr, Milad Nasr,\nChiyuan Zhang, Matthew Jagielski, Katherine Lee,\nChristopher A. Choquette-Choo, and Nicholas\nCarlini. 2023. Preventing verbatim memorization\nin language models gives a false sense of privacy.\nAbhyuday Jagannatha, Bhanu Pratap Singh Rawat,\nand Hong Yu. 2021. Membership inference attack\nsusceptibility of clinical language models.\nMatthew Jagielski, Om Thakkar, Florian Tramèr,\nDaphne Ippolito, Katherine Lee, Nicholas Carlini,\nEric Wallace, Shuang Song, Abhradeep Thakurta,\nNicolas Papernot, and Chiyuan Zhang. 2023.\nMeasuring forgetting of memorized training\nexamples.\nMatthew Jagielski, Stanley Wu, Alina Oprea, Jonathan\nUllman, and Roxana Geambasu. 2022. How to\ncombine membership-inference attacks on multiple\nupdated models.\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,\nMoontae Lee, Lajanugen Logeswaran, and Minjoon\nSeo. 2022. Knowledge unlearning for mitigating\nprivacy risks in language models.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of\nhallucination in natural language generation. CoRR,\nabs/2202.03629.\nJ. A. John and N. R. Draper. 1980. An alternative family\nof transformations. Journal of the Royal Statistical\nSociety. Series C (Applied Statistics), 29(2):190–197.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks in\nlanguage models.\nEugene Kharitonov, Marco Baroni, and Dieuwke\nHupkes. 2021. How bpe affects memorization in\ntransformers.\nKonrad Königsberger. 2000. Analysis 2 . Springer\nBerlin Heidelberg.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-\nBurch, and Nicholas Carlini. 2021. Deduplicating\ntraining data makes language models better. CoRR,\nabs/2107.06499.\nXuechen Li, Florian Tramèr, Percy Liang, and Tatsunori\nHashimoto. 2022. Large language models can be\nstrong differentially private learners.\nJimit Majmudar, Christophe Dupuy, Charith Peris, Sami\nSmaili, Rahul Gupta, and Richard Zemel. 2022.\nDifferentially private decoding in large language\nmodels.\nJustus Mattern, Fatemehsadat Mireshghallah, Zhijing\nJin, Bernhard Schölkopf, Mrinmaya Sachan, and\nTaylor Berg-Kirkpatrick. 2023. Membership\ninference attacks against language models via\nneighbourhood comparison.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D. Manning, and Chelsea Finn.\n2023. Detectgpt: Zero-shot machine-generated text\ndetection using probability curvature.\nMartin Pawelczyk, Himabindu Lakkaraju, and Seth\nNeel. 2022. On the privacy risks of algorithmic\nrecourse.\n13657\nAlec Radford, Jeff Wu, Rewon Child, D. Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language Models\nare Unsupervised Multitask Learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. CoRR, abs/1910.10683.\nAlexandre Sablayrolles, Matthijs Douze, Yann Ollivier,\nCordelia Schmid, and Hervé Jégou. 2019. White-\nbox vs black-box: Bayes optimal strategies for\nmembership inference.\nBrihat Sharma, Yanjun Gao, Timothy Miller,\nMatthew M Churpek, Majid Afshar, and Dmitriy\nDligach. 2023. Multi-task training with in-domain\nlanguage models for diagnostic reasoning. arXiv\npreprint arXiv:2306.02077.\nReza Shokri, Marco Stronati, Congzheng Song, and\nVitaly Shmatikov. 2017. Membership inference\nattacks against machine learning models.\nRebecca Soper. 2023. Can ai help you\nbuild relationships? amorai thinks so.\nhttps://techcrunch.com/2023/05/13/\nai-relationship-building-amorai/ .\nOm Thakkar, Swaroop Ramaswamy, Rajiv Mathews,\nand Françoise Beaufays. 2020. Understanding\nunintended memorization in federated learning.\nNikhil Vyas, Sham Kakade, and Boaz Barak. 2023.\nProvable copyright protection for generative models.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan\nKambadur, David Rosenberg, and Gideon Mann.\n2023. Bloomberggpt: A large language model for\nfinance. arXiv preprint arXiv:2303.17564.\nJiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda,\nand Reza Shokri. 2021. Enhanced membership\ninference attacks against machine learning models.\nCoRR, abs/2111.09679.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, and\nSomesh Jha. 2018. Privacy risk in machine learning:\nAnalyzing the connection to overfitting.\nWeichen Yu, Tianyu Pang, Qian Liu, Chao Du, Bingyi\nKang, Yan Huang, Min Lin, and Shuicheng Yan.\n2023. Bag of tricks for training data extraction from\nlanguage models.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tramèr, and Nicholas\nCarlini. 2021. Counterfactual memorization in neural\nlanguage models.\nA Additional Related Work on LLM\nPrivacy.\n(Carlini et al., 2021; Yu et al., 2023) focus on the\nproblem of training data extraction from LLMs\nby generating samples from the model, using loss-\nbased MIAs to determine if the generated point is\nactually a member of the training set that has been\nmemorized. Both papers focus more on extraction\nthan explicitly evaluating membership inference\nattack success, and acknowledge existing MIAs\nagainst LLMs are relatively weak. (Jang et al.,\n2022) studies the problem of unlearning a training\npoint from a trained model via taking gradient\nascent steps. One metric they use to determine if a\npoint has been unlearned is if the loss on a point x\nthat has been unlearned is close to the expected loss\nfor a test point. Our work has implications for this\nkind of definition of unlearning, as our results show\nthat an average LOSSθ value does not mean the\npoint cannot be easily detected as a training point.\nDifferentially private training (Dwork et al., 2016)\nis a canonical defense against MIAs, and there\nhas been a flurry of recent work on private model\ntraining in NLP (Anil et al., 2021a; Majmudar et al.,\n2022; Dupuy et al., 2022). While (Li et al., 2022)\nreport success in fine-tuning language models with\ndifferential privacy, it is know that privacy during\npre-training comes at a great cost to accuracy (Anil\net al., 2021b). Since pre-training with differential\nprivacy remains a challenge, existing work does not\nprovide theoretical mitigation guarantees against\nour attacks on pre-trained models.\nB Pythia Suite.\nWe identified EleutherAI’sPythia (Biderman et al.,\n2023b) suite of models as the prime candidate for\nstudying membership inference attacks. Models\nin the Pythia suite are trained on the Pile (Gao\net al., 2021) dataset, which is an 825GB dataset\nof about 300B tokens, consisting of 22 primarily\nacademic sources. All our experiments are using\nmodels trained on a version of the Pile that was\nde-duplicated using MinHashLSH with a threshold\nof 0.87, which reduces the size to 207B tokens.\nWe perform our experiments in the de-duplicated\nregime as it has been shown that the presence of\nduplicated data greatly increases the likelihood\nof training data memorization (Lee et al., 2021),\nand so attacks in the de-duplicated setting are\nsignificantly more compelling. We use a model\ncheckpoint corresponding to one full pass over the\n13658\nde-duplicated Pile. The data is tokenized using a\nBPE tokenizer developed specifically on the Pile.\nTraining examples are 2048 tokens, and the batch\nsize used during training is 1024. In order to\nmaintain an apples-to-apples comparison between\ntrain and test examples, we batch test examples\nidentically when evaluating our MIAs. Importantly,\nthe Pile contains train vs. test splits which allow\nus to evaluate our MIAs, and is also annotated\nwith the order of points during the training of all\nmodels which allows us to study the implications\nof training order for privacy.\nThe models in the Pythia suite are open\nsource and available through Hugging Face, have\npublicly available model checkpoints saved during\ntraining, and range in size from 70m parameters\nto 12B. The models follow the transformer-based\narchitecture in (Brown et al., 2020), with some\nsmall modifications (Biderman et al., 2023b).\nC Figure & Tables\nModel σ=.001 σ=.005 σ=.01 σ=.05\n70M 0.6034 0.6069 0.5708 0.4906\n160M 0.6394 0.6478 0.5613 0.5121\n410M 0.5915 0.5958 0.5367 0.5190\n1B 0.5028 0.5142 0.5924 0.5111\n1.4B 0.5652 0.5656 0.5502 0.5136\n2.8B 0.5320 0.5086 0.5109 0.5030\nTable 2: MoPeθ AUC per model size and noise level σ.\nFigure 7: For each batch size, we report the average\nLOSSθ or MoPeθ score over points in that batch, along\nwith a 95% CI for the average value for the 160M, 1B\nmodels.\n13659\nFigure 8: LOSSθ vs. MoPeθ scatter plots across model sizes.\n13660",
  "topic": "Hessian matrix",
  "concepts": [
    {
      "name": "Hessian matrix",
      "score": 0.7526247501373291
    },
    {
      "name": "Computer science",
      "score": 0.6707125306129456
    },
    {
      "name": "Language model",
      "score": 0.6669591665267944
    },
    {
      "name": "Statistic",
      "score": 0.5100003480911255
    },
    {
      "name": "Algorithm",
      "score": 0.48205623030662537
    },
    {
      "name": "TRACE (psycholinguistics)",
      "score": 0.4375988841056824
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41966283321380615
    },
    {
      "name": "Machine learning",
      "score": 0.40638336539268494
    },
    {
      "name": "Applied mathematics",
      "score": 0.30572953820228577
    },
    {
      "name": "Statistics",
      "score": 0.22876974940299988
    },
    {
      "name": "Mathematics",
      "score": 0.2194291353225708
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210106258",
      "name": "Harvard College Observatory",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    }
  ]
}