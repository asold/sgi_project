{
  "title": "A new method of hybrid time window embedding with transformer-based traffic data classification in IoT-networked environment",
  "url": "https://openalex.org/W3162493711",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2965077766",
      "name": "Rafał Kozik",
      "affiliations": [
        "Bydgoszcz University of Science and Technology",
        "Instytut Technik Telekomunikacyjnych i Informatycznych (Poland)",
        "AGH University of Krakow"
      ]
    },
    {
      "id": "https://openalex.org/A2243921459",
      "name": "Marek Pawlicki",
      "affiliations": [
        "Instytut Technik Telekomunikacyjnych i Informatycznych (Poland)",
        "AGH University of Krakow",
        "Bydgoszcz University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2965484019",
      "name": "Michał Choraś",
      "affiliations": [
        "Bydgoszcz University of Science and Technology",
        "Instytut Technik Telekomunikacyjnych i Informatycznych (Poland)",
        "AGH University of Krakow"
      ]
    },
    {
      "id": "https://openalex.org/A2965077766",
      "name": "Rafał Kozik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2243921459",
      "name": "Marek Pawlicki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2965484019",
      "name": "Michał Choraś",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W45942525",
    "https://openalex.org/W3114468998",
    "https://openalex.org/W3022876749",
    "https://openalex.org/W3116162237",
    "https://openalex.org/W40890042",
    "https://openalex.org/W2601473781",
    "https://openalex.org/W2572939427",
    "https://openalex.org/W3010002476",
    "https://openalex.org/W3082704935",
    "https://openalex.org/W2970289707",
    "https://openalex.org/W2886020981",
    "https://openalex.org/W3112718981",
    "https://openalex.org/W2997191565",
    "https://openalex.org/W2889165715",
    "https://openalex.org/W2998251512",
    "https://openalex.org/W2798790152",
    "https://openalex.org/W2858044321",
    "https://openalex.org/W2946427082",
    "https://openalex.org/W3088665058"
  ],
  "abstract": "Abstract The Internet of Things (IoT) appliances often expose sensitive data, either directly or indirectly. They may, for instance, tell whether you are at home right now or what your long or short-term habits are. Therefore, it is crucial to protect such devices against adversaries and has in place an early warning system which indicates compromised devices in a quick and efficient manner. In this paper, we propose time window embedding solutions that efficiently process a massive amount of data and have a low-memory-footprint at the same time. On top of the proposed embedding vectors, we use the core anomaly detection unit. It is a classifier that is based on the transformer’s encoder component followed by a feed-forward neural network. We have compared the proposed method with other classical machine-learning algorithms. Therefore, in the paper, we formally evaluate various machine-learning schemes and discuss their effectiveness in the IoT-related context. Our proposal is supported by detailed experiments that have been conducted on the recently published Aposemat IoT-23 dataset.",
  "full_text": "Vol.:(0123456789)1 3\nPattern Analysis and Applications (2021) 24:1441–1449 \nhttps://doi.org/10.1007/s10044-021-00980-2\nORIGINAL ARTICLE\nA new method of hybrid time window embedding \nwith transformer‑based traffic data classification in IoT‑networked \nenvironment\nRafał Kozik1,2  · Marek Pawlicki1,2  · Michał Choraś1,2 \nReceived: 15 January 2021 / Accepted: 29 April 2021 / Published online: 12 May 2021 \n© The Author(s) 2021\nAbstract\nThe Internet of Things (IoT) appliances often expose sensitive data, either directly or indirectly. They may, for instance, tell \nwhether you are at home right now or what your long or short-term habits are. Therefore, it is crucial to protect such devices \nagainst adversaries and has in place an early warning system which indicates compromised devices in a quick and efficient \nmanner. In this paper, we propose time window embedding solutions that efficiently process a massive amount of data and \nhave a low-memory-footprint at the same time. On top of the proposed embedding vectors, we use the core anomaly detection \nunit. It is a classifier that is based on the transformer’s encoder component followed by a feed-forward neural network. We \nhave compared the proposed method with other classical machine-learning algorithms. Therefore, in the paper, we formally \nevaluate various machine-learning schemes and discuss their effectiveness in the IoT-related context. Our proposal is sup-\nported by detailed experiments that have been conducted on the recently published Aposemat IoT-23 dataset.\nKeywords Deep learning · Transformers · Anomaly detection\n1 Introduction\nIn March of 2019, only two months after a similar attack \non Altran Technologies, the LockerGoga ransomware was \nused against Norsk Hydro, the largest aluminum manufac-\nturer in Europe, hiring over 35000 people and having sites \nin more than 50 countries all across the globe. The attack \ncaused a serious decrease in production and issues with the \nexecution of the ongoing contracts. The losses were esti -\nmated to equal millions of dollars per day, and the grand \ntotal of losses was estimated to reach hundreds of millions \nof dollars. The attack occurred on 18/19 March 2019, mostly \nimpacting the infrastructure in Norway, and other countries \nto a lesser extent. It resulted in the shutdown of the global \nNorsk Hydro network.\nThe attack affected work at the offices (causing, for \nexample, problems with order documentation) as well as \nthe industrial manufacturing, where, besides other issues, the \nmanufacturing drivers had to be uploaded manually through \nusb drives.\nThe attack was a cyber-criminal case committed for \nfinancial gains. The ransomware had turned off a part of the \nsystem’s security mechanisms, as well as the data backup \nprocesses, before starting data encryption. All the local user \npasswords were changed.\nThe ‘Ransom’ was not paid, the recovery of data from \nbackups took months. As of March 2019, the LockerGoga \nransomware was undetectable by 67 of the state-of-the-art \nantiviruses. The experts noted that better anomaly detection \nsystems could have prevented the incident.\nIn June 2019, a vulnerability in the Amazon Ring Video \nDoorbell was discovered. The flaw in the product’s security \nmade it possible to connect to the home WiFi and possi-\nbly exploit other connected devices [2]. A similar issue was \ndiscovered in the Amazon Blink XT2 security camera. The \nsecurity flaw, which was discovered in August 2019, allowed \nunauthorised users to view the footage from the cameras and \nlisten to their audio. In fact, the flaw made it extremely easy \nto gain root access to the device [16].\n * Rafał Kozik \n rkozik@utp.edu.pl\n1 UTP University of Science and Technology, Bydgoszcz, \nPoland\n2 ITTI Sp. Z o.o., Poznań, Poland\n1442 Pattern Analysis and Applications (2021) 24:1441–1449\n1 3\nThe ’ Attack Landscape’ report illustrates that a number \nof network attacks are carried over Telnet and Secure Shell \n(SSH) with a high probability of targeting IoT devices [7].\nTherefore, in this paper, we propose a new innovative \nmethod to detect anomalies in IoT environment.\nThe major contribution of this work is the proposition of \na time window embedding solution with a transformer-based \nclassification scheme.\nThe remainder of this paper is structured as follows: In \nSect. 2, the related work is overviewed, in Sects.  3- 5 the \nproposed method is described, experimental setup is pre-\nsented in Sect.  6, while the results are reported in Sect.  7. \nConclusions are drawn thereafter.\n2  Related work\nIn the literature, there are two approaches to intrusion detec-\ntion, namely the signature-based and anomaly-based ones. \nTypically, when the attack is deterministic, one can develop \na signature that will allow for its detection. However, nowa-\ndays attackers use various obfuscation techniques to evade \nsuch detection mechanisms. Therefore, the cybersecurity \ncommunity is investing its efforts in the anomaly detection \nsystem. These turn to be more effective in detecting new and \nunknown (so called 0-day) cyber-attacks [21].\nIn [3], the authors performed a survey of current tenden-\ncies in cybersecurity and concluded that two major trends \nemerge - one is that old, proven methods are still in use \nin many applications. The other is that machine-learning-\nbased (ML) approaches are increasingly more prominent. \nFurthermore, [17] points out that ML is now used on both \nthe malware and security sides.\nWhen it comes to network traffic analysis, two popu-\nlar approaches are used by experts from the cybersecurity \ndomain. One is based on deep packet inspection [4 ], while \nthe other relies on network flows analysis [11]. One of the \nmost popular protocols for network flow data collection is \nNetFlow [6]. That kind of data is often captured by Internet \nservice providers for auditing and performance monitoring \npurposes. NetFlow samples do not contain much of sensi-\ntive data and therefore are widely available. However, the \ndisadvantage is that such samples do not contain the raw \ncontent of network packets. Such details are valuable and \ncan improve the effectiveness of malware detection. How -\never, these are rarely available because of the encryption, \nwhich is often utilized by the end-points terminal.\nThe current research shows that the network flow data \ncan be effectively analyzed using various machine-learning \ntechniques such as unsupervised clustering [8 ], Random-\nForests (RF) [22], or deep learning [19]. The authors of [5 ] \npresent a range of deep neural network topologies and test \nthe influence of hyperpaprameter setups on the accuracy of \nthe solution. On the flip-side, in [12], a stream processing \nframework capable of employing a range of ML algorithms \nfor intrusion detection is presented.\nObviously, the different methods vary in the way they \nprocess the NetFlow data. For instance, in [10], the authors \nproposed a solution called CCDetector. It uses a state-based \nbehavioral model of the Command and Control channels. \nThe author of this algorithm adapts the Markov Chain to \nmodel malware behavior and to detect similar traffic in \nunknown real networks. The difference from the BClus (and \nour approach) is the fact that instead of analyzing the com-\nplete traffic of an infected computer as a whole, the authors \nseparate each individual connection from each IP address \nand treat these as an independent connection. The results \nobtained with this method are very promising. However, one \nof the concerns is the complex and time-consuming learn-\ning phase.\nIn opposite to that, in [19], the authors have adapted \nrecurrent neural networks (RNN) with long short-term \nmemory (LSTM) units on top of the NetFlow data. In addi-\ntion to that they also used a flexible distributed architecture \nto handle the curation of large amount of data.\nAn interesting approach, which maps the NetFlow data \nto the image representation, has been presented in [13]. In \norder to construct the images, the authors have used such \ntechniques as feature correlation analysis and correlation \nmatrices. The images have been analyzed with a convolution \nneural network (CNN) in order to detect intrusions. Accord-\ning to the authors, this method achieves high accuracy.\nA CNN for flow-based malware detection is also pro-\nposed in [20]. The authors advocate that current detection \nsystems are overreliant on certain network features, like the \nport number, which could introduce a blind spot in the sys-\ntem. Thus, they calculate 35 features with the use of Net-\nmateto to fully express the state of the network and provide \nthose to the CNN and other ML algorithms.\nThe authors of [18] present a deep network model capa-\nble of automatic feature extraction, which takes time-related \ncharacteristics into consideration. To achieve that a GRU \nnetwork along with a multilayer perceptron (MLP) is used. \nThe authors also test a network with LSTM cells.\nThe authors of [14] evaluate autoencoders (sparse, \ndenoising, contractive, convolutional), LSTM, and CNN for \nnetwork intrusion detection. Autoencoders obtain the latent \nrepresentation of the feature set. When the hidden layer has \nfewer neurons than the input/output layers, it is called a bot-\ntleneck, discriminative, coding, or abstraction layer. Using \nsuch a bottleneck forces the topology to acquire the most \nsignificant features.\nIn [9 ], instead of flow classification, the flow predic-\ntion approach is used. In order to achieve this, the authors \ncombine an RNN (with gated recurrent units) with the so-\ncalled linear regression layer, which allows for producing \n1443Pattern Analysis and Applications (2021) 24:1441–1449 \n1 3\nprediction in a similar fashion as auto-regressive integrated \nmoving average (ARIMA) models do with time series.\nIn [1], the authors used the auto-regressive fractionally \nintegrated moving average (ARFIMA) model and proposed \nthe Hyndman-Khandakar algorithm to estimate the polymo-\nnials parameters and the Haslett and Raftery algorithm to \nestimate the differencing parameters for network anomaly \ndetection.\n3  Proposed method\nThe proposed solution (see Fig.  1) captures network flows \n(as streams), calculates feature vectors over a predefined \ntime window, provides these vectors to a binary classifier, \nwhich eventually produces the detection output (benign for \nnormal traffic or anomaly for traffic containing suspicious \npatterns). In the next section, the details on each of these \nprocessing steps will be provided. First, the overview of the \ninput data is given, and then, the effective methods for fea-\nture extraction are elaborated upon. Finally, a brief descrip-\ntion of the classification methods incorporated in this work \nis provided.\n3.1  Flow‑based data acquisition\nConceptually, in this approach, the data are collected from \nthe network in the form of communication flows travers -\ning such devices as switches, routers, or hosts. This kind \nof data captures aggregated network properties and statis-\ntics. From the architectural point of view, the network traf-\nfic going through the flow-enabled devices is collected and \nlater on sent to the collectors - the network elements, which \nstore them and keep them for the operator for later analysis. \nIn particular, network flows are often used by the network \nadministrators for auditing purposes. A single flow aggre-\ngates such characteristics as:\n– incoming and outgoing number of bytes\n– IP addresses taking part in the communication\n– utilized source and destination ports\n– utilized type of protocol (e.g., Transmission Control Pro-\ntocol (TCP) or User Datagram Protocol (UDP))\n(e.g., the number of bytes sent and received) about packets \nthat have been sent by a specific source address to a specific \ndestination address. It is obvious that it must be possible to \nidentify some patterns of anomalous behavior of network \nnodes from such kind of data. Some of these patterns may be \nrelated to malware infection or help the network administra-\ntor to identify adversaries.\n3.2  Time window embedding with probabilistic \ndata structures\nThe rationale behind the proposed embedding is to encode a \nnetwork flow using only its nearest neighborhood in the time \ndomain. This approach allows us to capture some short-term \nmalicious behavior of specific network elements and nodes.\nIn the proposed approach, we calculate the statistical \nproperties of a group of flows that have been collected for a \nspecific source IP address within short and fix-length time \nspans called time windows.\nAs it was presented in the previous section, a single flow \nexhibits various characteristics describing the two-way com-\nmunication (e.g., number of flows, destination IP, etc.). For \neach of these characteristics, it is possible to calculate vari-\nous statistical properties such as mean, median value, min/\nmax values. In order to make it clear what is being calculated \nfor each of those characteristics, we have included Table 1. \nOverall, it leads to the situation where the traffic character -\nistics produced by a single IP address within the considered \ntumbling time window are described by 37 values.\nIn general, counting the number of flows and/or accu-\nmulating the sum of inbound and outbound packets may \nbe trivial, but the situation is different when it comes to \ndistinct counting or finding the most frequent element (e.g., \ndestination port) in the stream of data. The straightforward \napproach would be to maintain a dynamic list. Whenever a \nnew element is retrieved from the stream, one must scan the \nentire list and check whenever that element is there. If not, \nthe list needs to be resized and new element added. Moreo-\nver, there is another level of complexity when we want to \nFig. 1  Tumbling windows - \nexample of window statistics \nembedding\nMetrics\n[1.2,33.4,55.6,334.6,664,… ]\nN+3 Time WindowN Time \nWindow\nN+1 Time \nWindow\nN+4 Time \nWindow\nN+5 Time \nWindow\n1444 Pattern Analysis and Applications (2021) 24:1441–1449\n1 3\nmerge the results obtained from two concurrent processes \nthat perform distinct counting.\nThere is a class of data structures which are known as \nprobabilistic (or sketchy). These have the ability to describe \nremarkably large sets with sub-logarithmic or constant space \ncomplexity. That implies that there is no need to scale-up \nthe data processing system when it undergoes the transition \nfrom thousands, to millions, or even billions of records that \nneed to be analyzed.\nProbabilistic data structures rely on various mechanisms \nto compress data, and often, these mechanisms may cause \nthem to contain inaccurate information.\nHowever, this inaccuracy should not have a strong impact \non the detection part. The assumption is based on the obser-\nvation that the classifiers can handle such changes to some \nextent and be able to return the correct decision. It must be \nnoted that here we are talking about changes that are within \na range of 1–2% for one of the features building the vector.\nProbabilistic data structures have several advantages. First \nof all, the size of such structures grows significantly slower \nwith respect to the input data. In many cases, it is orders of \nmagnitude smaller. Moreover, it is also possible to make the \ntrade-off between the accuracy of prediction vs. the size of \nthe data structure. They are naturally suited for measuring \nnetwork traffic, which has a form of streaming data, where \neach item in the stream needs to be analyzed quickly and in \nneeds to update a data structure that summarises some prop-\nerties (e.g., the number of distinct IP addresses or the most \nfrequently used service). A substantially useful property of \nthe probabilistic data structures is the ability to be merged. \nIt means that when the stream is split into two parts and the \nsummary over them is calculated separately, the result will \nbe the same as if it was calculated over the entire (original) \nstream. As a result, this makes the probabilistic structure \nhighly parallelizable and suitable for distributed computing \nplatforms (e.g., Hadoop, Spark, Druid, etc.)\n4  Frequent items and distinct counting ‑ \nthe problem overview\nIn order to calculate the most frequent destination port or \ndestination host (or even concatenation of both) originating \nfrom a specific IP address, one may use data structures such \nas a hash table to accomplish that. In such a case, the new \nitem is put in the hash table and the counter for that item is \nset to 0. Whenever the entry in the hash table already exists, \na counter is just incremented. However, such an approach \nmay quickly become impractical when the amount of input \ndata is significant, because of two reasons. Firstly, along \nwith the growing size of the input data, the hash table will \ngrow as well and eventually its capacity will exceed the \namount of available RAM memory. Secondly, the colli-\nsions in the hash table are handled as a linked list. It means \nthat whenever a new item is hashed to the bucket that is \nalready taken, it will be appended (linked) after the existing \none. Therefore, when the list becomes longer and longer, \nthe access time to such elements in the hash table will be \nsubstantially longer as well. Also, the dynamic allocation of \nthe memory (for a new element) is also time consuming. In \nthis section, the most important sketchy data structures that \nhave been used in this research are described.\n4.1  Count‑min data structure\nCount-min (CM) data structure allows for counting of items \nthat are of a different type, e.g., how many times a specific \nIP address has contacted port 8080. From the scientific point \nof view, CM is an array of width w  and depth d  CM[1, 1] \n...CM[d, w]. It uses a set of d hash functions:\n(1)h1 … hd ∶{ 1 … n} → {1 … d}\nTable 1  Overview of all the \nfeatures extracted from network \nflows\nMin Max Avg Median Unique count Most frequent\nNumber of flows + + + +\nDestination port + +\nDestination address + +\nProtocol + +\nInbound packets + + + +\nOutbound packets + + + +\nTotal packets + + + +\nInbound bytes + + + +\nOutbound bytes + + + +\nTotal bytes + + + +\nPort-protocol pair +\nIP-port pair +\nIP-port-prot. triplet +\n1445Pattern Analysis and Applications (2021) 24:1441–1449 \n1 3\nwhich belong to a random pairwise-independent family of \nfunctions. The width w  and depth d  are chosen based on \nallowed by user error rates and are calculated as:\nwhere /u1D716 (an acceptable error in estimation) is the error factor \nand /u1D70E is the error probability. At the beginning, the CM array \nis initialized with 0 values. Each time the count needs to be \nupdated for a specific value x, the hash functions are calcu-\nlated and modded with the width w. This yields the column \nnumber col= hn(x)%w  . Finally, the cell at position (n , col) \nin the CM array is incremented by one. We use a similar \napproach when querying the data structure. We simply take \nthe modded values obtained from hash functions and find the \nminimum. The visual representation of the concept behind \nthe count-min data sketch is presented in Fig. 2.\n4.2  HyperLogLog sketch\nHyperLogLog (HLL) belongs to a family of algorithms that \naim at estimating the cardinality of a dataset. It relies on \nthe probabilistic counting method. Assuming that we have \na large data set with duplicated entries, we can evenly dis-\ntribute the elements in the dataset using a hashing func -\ntion and estimate the cardinality using the hashed values. \nThe common approach is to count the leading zeros in the \nbinary representation of the hashed values. The probability \nof observing n leading zeros is equal to 1\n2n . In other words, if \nwe denote p(v1) as a number of leading zeros in v1 , we can \ncalculate the cardinality as n = 2R , where:\nThe visual representation of the HyperLogLog data sketch \nis presented in Fig. 3.\n(2)w =\n/uni2308.s2\ne\n/u1D716\n/uni2309.s2\nd =\n/uni2308.s2\nln1\n/u1D70E\n/uni2309.s2\n(3)R = max(p(v1 ),p(v1 ),… ,p(vm ))\nObviously, a single estimator of that kind is subject to \nhigh variance. Therefore, the common approach is to use \nseveral estimators and to average the results. This can be \nachieved using several independent hash functions.\n5  Classification with transformer\nTransformers have been proposed in the area of natural lan-\nguage processing (NLP). This is a relatively novel archi-\ntecture that aims at solving sequence-to-sequence problems \nwhile handling long-range dependencies. The original trans-\nformer architecture involves the so-called encoding and \ndecoding parts. In this research, only the encoder is used \n(see Fig.4), since the aim is to encode the behavior of spe-\ncific node elements using the latent representation produced \nby the encoder part of the transformer architecture.\nThe data are ingested into the transformer using the time \nwindows embedding technique described in the previous \nsection. In general, several network flows belonging to a \nspecific time window and related with a specific IP address \nare encoded using the technique leveraging probabilistic data \nsketches. This operation results in vectors of a fixed length. \nThe transformer works with sequences, which in our case \ndescribe the behavior of a specific network element over the \ndefined time period. The sequence is composed by putting \nseveral embedding vectors one next to another.\nNext, the sequence of vectors goes through the positional \nencoding, which allows us to capture the order of the vector \nin the input sequence. This looks quite useful from our per-\nspective, because usually the attacker executes some series \nof actions in order to carry out a successful cyberattack.\nAfterward, the positionally encoded input reaches the \nmultiheaded attention layer. From a high perspective, this \nlayer allows the model to look at other positions in the \ninput sequence for clues that can improve the final detec-\ntion. For example, the infected machine would try to contact \nthe botmaster if a few moments before it was infected with \nmalware.\nIn the next step, the information coming from the self-\nattention is normalized and goes to the feed-forward layer, \nthe output of which is normalized as well. As it is depicted in \nthe diagram, there are two residual connections: one around \nthe self-attention and the other around the feed-forward \nlayer.\nBecause the input leaving the transformer layer contains \none output vector for each element in the input sequence, we \nuse the average pooling layer. It takes the mean across all \nthe elements in the input. Finally, on top of the entire model, \nwe used two layers of the feed-forward network with two \noutputs, one to indicate the benign sequence and the other \nto indicate the anomalous sequence.\nFig. 2  Count-min data structure - overview of the architecture\n1446 Pattern Analysis and Applications (2021) 24:1441–1449\n1 3\n6  Experiments\n6.1  The goal of the experiments\nThe goal of the experiments is to compare the proposed \napproach with the state-of-the-art methods. In this paper, we \nhave considered two different evaluation scenarios. Firstly, \nwe investigate how the proposed time window embedding \ntechnique operates along with the transformer-based anom-\naly detection architecture. In that regard, we have compared \nour approach with RandomForest (with 500,100,10 trees, \nrespectively), vanilla REPTree (decision/regression tree), \nand AdaBoosted version of REPTree. Secondly, we investi-\ngate to what extends the proposed model is able to general-\nize to unknown malware infection scenarios. In that regard, \nwe have considered various test-case scenarios where the \nmodels are trained and evaluated on malicious samples that \nhave been recorded for different attacks (or infections).\n6.2  Aposemat IoT‑23 dataset\nIoT-23 is a dataset of network traffic from Internet of Things \n(IoT) devices. It has 20 malware captures executed in IoT \ndevices, and three captures for benign IoT devices traffic. It \nwas first published in January 2020, with captures ranging \nfrom 2018 to 2019. This IoT network traffic was captured \nin the Stratosphere Laboratory, AIC group, FEL, CTU Uni-\nversity, Czech Republic. Its goal is to offer a large dataset of \nreal and labelled IoT malware infections and IoT benign traf-\nfic for researchers to develop machine-learning algorithms. \nThis dataset and its research is funded by Avast Software, \nPrague. In addition to easier reproducibility of the study, \nusing a benchmark dataset allows to handle various privacy \nissues, as outlined in [15].\n6.3  Experimental protocol\nThe dataset was split into training and testing parts. This is \nexplained in Table  2. Different scenarios are used, where \ndifferent parts of the original dataset have been employed. \nWe followed such an approach in order to prove that the \nproposed method can generalize well to unknown malware \nfamilies. In that regard, we have used different scenarios \n(malware families) for training and testing the models. \nNonetheless, some malware names appear both in training \nand testing. However, these malware are recorded in dif-\nferent network captures, which concern different contexts \n(different network elements, different IoT devices, etc.)\nAdditionally, for the training part of the dataset, five-\nfold cross-validation was adapted. This allowed to cal -\nculate the standard deviation of measured performance \ncharacteristics. Moreover, this also enables the discussion \non the significance of differences obtained for various \napproaches. For that matter, t -test statistical hypothesis \ntest was used.\nTable 2  IoT23 dataset - scenarios setup (x and – indicate training and \nvalidation sets, respectively)\nScenario Malware\n1 2 3 4\nCTU-Honeypot-Capture-4-1 x x x x Benign\nCTU-Honeypot-Capture-5-1 x x x x Benign\nCTU-Honeypot-Capture-7-1 x x x x Benign\nCTU-IoT-Malware-Capture-34-1 x x x x Mirai\nCTU-IoT-Malware-Capture-43-1 x x x x Mirai\nCTU-IoT-Malware-Capture-1-1 x x x x Hide&Seek\nCTU-IoT-Malware-Capture-3-1 x x x x Muhstik\nCTU-IoT-Malware-Capture-35-1 x x x x Mirai\nCTU-IoT-Malware-Capture-39-1 x x x - IRCBot\nCTU-IoT-Malware-Capture-7-1 x x x - Mirai\nCTU-IoT-Malware-Capture-8-1 x x x - Hakai\nCTU-IoT-Malware-Capture-9-1 x x x - Hajime\nCTU-IoT-Malware-Capture-20-1 x x x - Torii\nCTU-IoT-Malware-Capture-21-1 x x x - Torii\nCTU-IoT-Malware-Capture-42-1 x x x - Trojan\nCTU-IoT-Malware-Capture-17-1 x x - - Kenjiro\nCTU-IoT-Malware-Capture-36-1 x x - - Okiru\nCTU-IoT-Malware-Capture-33-1 x - - - Kenjiro\nCTU-IoT-Malware-Capture-48-1 x - - - Mirai\nCTU-IoT-Malware-Capture-44-1 - - - - Mirai\nCTU-IoT-Malware-Capture-49-1 - - - - Mirai\nCTU-IoT-Malware-Capture-52-1 - - - - Mirai\nCTU-IoT-Malware-Capture-60-1 - - - - Gagfyt\nFig. 3  Architecture of HyperLogLog data structure\n1447Pattern Analysis and Applications (2021) 24:1441–1449 \n1 3\n6.4  Evaluation metrics\nBefore using various machine-learning algorithms, the raw \nnetwork flows are processed in order to produce the time \nwindow embedding vectors. The procedure is detailed in \nthe previous sections. The procedure for calculating met-\nrics is as follows: \n1. communication flows are aggregated into time windows \n(here we have used 3 min. time windows).\n2. for given time, windows embedding vectors are calcu-\nlated\n3. within the ground-truth communication flows, labels are \nexamined against the predicted ones and the TP, TN, FP, \nand FN errors (true and false positives and negatives) are \nmeasured.\n4. finally, Recall, Precision, F-measure Rate are estimated \nand reported.\nFig. 4  The architecture of the \nproposed transformer-based \nclassifier\n\n1448 Pattern Analysis and Applications (2021) 24:1441–1449\n1 3\n7  Results\nThe results for classification parts are presented in Table  3. \nWe have compared the proposed transformer-based \napproach against various and popular machine-learn-\ning techniques. On the list, there is a decision tree and \nclassifiers ensembles (one is AdaBoost, while the other is \nthe well-known RandomForest).\nThe t-test statistical hypothesis test was used to vali-\ndate that the results obtained by the proposed approach are \nsignificantly different from the other compared methods. \nIn that regard, the values followed by ± symbol (Table  3) \nindicate standard deviation.\nIt must be noted that we have used different sce-\nnarios for training and testing the models. This proves \nthe approach can generalize well to unknown malware \nfamilies.\nWhat can be found in Table  3 is that these base-line \nmethods behave quite well. However, the transformer-\nbased approach outperforms other methods in most of the \nconsidered scenarios. It is quite vivid for f 1-score metric \nthat has been visually presented in Fig.5 . It must be noted \nthat the transformer-based approach allowed us to achieve \nremarkably good results for the fourth scenario, where less \nthan 40% of the original datasets is used.\nThe second good results are reported for the Random-\nForest. The experiments show that increasing the number \nof trees above 100 does not bring much to the effective-\nness. The Adaptive Boosting (AdaBoost) ensemble of \nREPTrees performs well for the first and the second sce-\nnarios; for the scenarios 3 and 4, this method stays a bit \nbehind the RandomForest.\nWhat can be found interesting is the observation that \nthe f1-score does not change that much between scenarios \n1,2,3 and 4 for the proposed method. On the other hands, \none can observe quite significant fluctuations for the other \napproaches. In particular, this change is much more sig -\nnificant for Reduced Error Pruning Decision Tree (REP -\nTree) and Adaptive Reduced Error Pruning Decision Tree \n(AdaREPT) classifiers.\nTable 3  Effectiveness comparison for different classifiers and sce-\nnarios\nScenario Method Precision Recall f1-score\n1 Proposed \nmethod\n0.902 ± 0.001 0.967 ± 0.005 0.934 ± 0.003\n2 0.912 ± 0.003 0.940 ± 0.008 0.926 ± 0.005\n3 0.916 ± 0.003 0.977 ± 0.002 0.945 ± 0.002\n4 0.910 ± 0.002 0.964 ± 0.004 0.936 ± 0.002\n1 RF500 0.866 ± 0.001 0.928 ± 0.001 0.896 ± 0.001\n2 0.858 ± 0.001 0.803 ± 0.000 0.829 ± 0.001\n3 0.866 ± 0.000 0.877 ± 0.000 0.872 ± 0.000\n4 0.859 ± 0.001 0.930 ± 0.000 0.893 ± 0.001\n1 RF100 0.877 ± 0.002 0.943 ± 0.001 0.909 ± 0.001\n2 0.861 ± 0.002 0.791 ± 0.004 0.825 ± 0.001\n3 0.871 ± 0.003 0.873 ± 0.000 0.872 ± 0.002\n4 0.867 ± 0.001 0.928 ± 0.003 0.896 ± 0.002\n1 RF10 0.838 ± 0.002 0.814 ± 0.003 0.826 ± 0.002\n2 0.803 ± 0.003 0.670 ± 0.005 0.730 ± 0.003\n3 0.861 ± 0.003 0.909 ± 0.001 0.884 ± 0.001\n4 0.846 ± 0.007 0.826 ± 0.002 0.836 ± 0.004\n1 REPTree 0.833 ± 0.004 0.762 ± 0.001 0.796 ± 0.002\n2 0.846 ± 0.005 0.717 ± 0.002 0.776 ± 0.002\n3 0.808 ± 0.005 0.766 ± 0.003 0.786 ± 0.003\n4 0.844 ± 0.003 0.945 ± 0.003 0.892 ± 0.002\n1 AdaBoost 0.813 ± 0.002 0.763 ± 0.003 0.788 ± 0.002\n2 0.836 ± 0.006 0.643 ± 0.003 0.727 ± 0.004\n3 0.819 ± 0.005 0.710 ± 0.003 0.761 ± 0.003\n4 0.807 ± 0.003 0.697 ± 0.003 0.748 ± 0.001\n0.93\n0.93\n0.95\n0.94\n0.896\n0.829\n0.872\n0.893\n0.909\n0.825\n0.872\n0.896\n0.826\n0.73\n0.884\n0.836\n0.788\n0.727\n0.761\n0.748\n0.796\n0.776\n0.786\n0.892\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nSCENARIO 1S CENARIO 2S CENARIO 3S CENARI O 4\nF1-SCORE\n Proposed method\n RF-500\n RF-100\nREPT\n RF-10\n  AdaREPT\nFig. 5  Comparison of the f1-score achieved for various algorithms and scenarios\n1449Pattern Analysis and Applications (2021) 24:1441–1449 \n1 3\n8  Conclusions\nIn this paper, we propose innovative anomaly detection that \nutilizes innovative time windows embedding solutions that \nefficiently process a massive amount of data, while having a \nlow-memory-footprint at the same time. The core anomaly \ndetection is based on the transformer’s encoder unit followed \nby a two-layer feed-forward neural network. In the paper, we \nhave formally evaluated various machine-learning schemes \nin order to compare these with the proposed approach and \nto discuss their effectiveness in the IoT-related context. The \nproposal is supported by detailed experiments that have been \nconducted on the recently published Aposemat IoT-23 data-\nset. Our experiments show that the proposed approach that \nleverages the transformer-based classification performs best \nin most of the considered scenarios.\nAcknowledgements This work is funded under PREVISION pro-\nject, which has received funding from the European Union’s Horizon \n2020 research and innovation programme under grant agreement No. \n833115.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Andrysiak T, Saganowski Ł, Choraś M, Kozik R (2014) Net-\nwork traffic prediction and anomaly detection based on arfima \nmodel. In: International Joint Conference SOCO’14-CISIS’14-\nICEUTE’14, pp. 545–554. Springer\n 2. BitDefender: Ring video doorbell pro under the scope (2019). \nhttps:// www. bitde fender. com/ files/ News/ CaseS tudies/ study/ 294/ \nBitde fender- White Paper- RDoor- CREA3 949- en- EN- Gener icUse. \npdf\n 3. Caviglione L, Choraś M, Corona I, Janicki A, Mazurczyk W, \nPawlicki M, Wasielewska K (2020) Tight arms race: overview of \ncurrent malware threats and trends in their detection. IEEE Access\n 4. Cheng Z, Beshley M, Beshley H, Kochan O, Urikova O (2020) \nDevelopment of deep packet inspection system for network traffic \nanalysis and intrusion detection. In: 2020 IEEE 15th International \nConference on Advanced Trends in Radioelectronics, Telecom-\nmunications and Computer Engineering (TCSET), pp. 877–881\n 5. Choraś M, Pawlicki M (2020) Intrusion detection approach based \non optimised artificial neural network. Neurocomputing\n 6. Claise B (2004) Cisco systems netflow services export version 9. \nrfc 3954 (informational)\n 7. F-Secure: the f-secure attack landscape report H1-2020 (2020). \nhttps:// www.f- secure. com/ conte nt/ dam/ press/ de/ media- libra ry/ \nrepor ts/F- Secure- attack- lands cape- h12020. pdf\n 8. Flanagan K, Fallon E, Awad A, Connolly P (2017) Self-configur-\ning netflow anomaly detection using cluster density analysis. In: \n2017 19th International Conference on Advanced Communication \nTechnology (ICACT), pp. 421–427\n 9. Fu R, Zhang Z, Li L (2016) Using lstm and gru neural network \nmethods for traffic flow prediction. In: 2016 31st Youth Academic \nAnnual Conference of Chinese Association of Automation (YAC), \npp. 324–328\n 10. Garcia S (2014) dentifying, modeling and detecting botnet behav-\niors in the network. Ph.D. thesis, Instituto Superior de Ingenier’ıa \nde Software Tandil Departamento de Computacio’n y Sistemas\n 11. Hardegen C, Pfülb B, Rieger S, Gepperth A (2020) Predicting \nnetwork flow characteristics using deep learning and real-world \nnetwork traffic. IEEE Transactions on Network and Service Man-\nagement pp. 1–1\n 12. Komisarek M, Choraś M, Kozik R, Pawlicki M (2020) Real-time \nstream processing tool for detecting suspicious network patterns \nusing machine learning. In: Proceedings of the 15th International \nConference on Availability, Reliability and Security, pp. 1–7\n 13. Liu X, Tang Z, Yang B (2019) Predicting network attacks with cnn \nby constructing images from netflow data. In: 2019 IEEE 5th Intl \nConference on Big Data Security on Cloud (BigDataSecurity), \nIEEE Intl Conference on High Performance and Smart Comput-\ning, (HPSC) and IEEE Intl Conference on Intelligent Data and \nSecurity (IDS), pp. 61–66\n 14. Naseer S, Saleem Y, Khalid S, Bashir MK, Han J, Iqbal MM, Han \nK (2018) Enhanced network anomaly detection based on deep \nneural networks. IEEE Access 6:48231–48246. https:// doi. org/ \n10. 1109/ ACCESS. 2018. 28630 36\n 15. Pawlicka A, Jaroszewska-Choras D, Choras M, Pawlicki M (2020) \nGuidelines for stego/malware detection tools: achieving gdpr com-\npliance. IEEE Technol Soc Mag 39(4):60–70\n 16. Tenable: Blink XT2 sync module multiple vulnerabilities (2019). \nhttps:// www. tenab le. com/ secur ity/ resea rch/ tra- 2019- 51\n 17. Thanh CT, Zelinka I (2019) A survey on artificial intelligence in \nmalware as next-generation threats. Mendel 25:27–34\n 18. Xu C, Shen J, Du X, Zhang F (2018) An intrusion detection sys-\ntem using a deep neural network with gated recurrent units. IEEE \nAccess 6:48697–48707. https:// doi. org/ 10. 1109/ ACCESS. 2018. \n28675 64\n 19. Yang C, Liu J, Kristiani E, Liu M, You I, Pau G (2020) Netflow \nmonitoring and cyberattack detection using deep learning with \nceph. IEEE Access 8:7842–7850\n 20. Yeo M, Koo Y, Yoon Y, Hwang T, Ryu J, Song J, Park C (2018) \nFlow-based malware detection using convolutional neural net-\nwork. In: 2018 International Conference on Information Network-\ning (ICOIN), pp. 910–913. https:// doi. org/ 10. 1109/ ICOIN. 2018. \n83432 55\n 21. Zaman M, Lung C (2018) Evaluation of machine learning tech-\nniques for network intrusion detection. In: NOMS 2018 - 2018 \nIEEE/IFIP Network Operations and Management Symposium, pp. \n1–5\n 22. Zhang H, Dai S, Li Y, Zhang W (2018) Real-time distributed-ran-\ndom-forest-based network intrusion detection system using apache \nspark. In: 2018 IEEE 37th International Performance Computing \nand Communications Conference (IPCCC), pp. 1–7\nPublisher’s Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.837599515914917
    },
    {
      "name": "Embedding",
      "score": 0.7152513265609741
    },
    {
      "name": "Internet of Things",
      "score": 0.6338094472885132
    },
    {
      "name": "Transformer",
      "score": 0.5707561373710632
    },
    {
      "name": "Memory footprint",
      "score": 0.5306246280670166
    },
    {
      "name": "Encoder",
      "score": 0.4849105477333069
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48161786794662476
    },
    {
      "name": "Anomaly detection",
      "score": 0.48108574748039246
    },
    {
      "name": "Classifier (UML)",
      "score": 0.47759127616882324
    },
    {
      "name": "Machine learning",
      "score": 0.4537036120891571
    },
    {
      "name": "Deep learning",
      "score": 0.4465656578540802
    },
    {
      "name": "Real-time computing",
      "score": 0.43603286147117615
    },
    {
      "name": "Recurrent neural network",
      "score": 0.43543869256973267
    },
    {
      "name": "Artificial neural network",
      "score": 0.427368700504303
    },
    {
      "name": "Embedded system",
      "score": 0.28632164001464844
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}