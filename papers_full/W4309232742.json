{
  "title": "Automatic story and item generation for reading comprehension assessments with transformers",
  "url": "https://openalex.org/W4309232742",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "BULUT, Okan",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": null,
      "name": "YÄ°LDÄ°RÄ°M-ERBASLÄ°, Seyma Nur",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": null,
      "name": "Bulut, Okan",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Yildirim-erbasli, Seyma Nur",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2469974074",
    "https://openalex.org/W2098783781",
    "https://openalex.org/W3125356501",
    "https://openalex.org/W1551105619",
    "https://openalex.org/W2587769943",
    "https://openalex.org/W4214664848",
    "https://openalex.org/W3143827210",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2885087331",
    "https://openalex.org/W2757715585",
    "https://openalex.org/W2962717047",
    "https://openalex.org/W2757978590",
    "https://openalex.org/W45283437",
    "https://openalex.org/W2315349062",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2073251056",
    "https://openalex.org/W1531374185",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2963283805",
    "https://openalex.org/W1989886528",
    "https://openalex.org/W2995969307",
    "https://openalex.org/W3031450931",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2112524255",
    "https://openalex.org/W2944995326",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W166220104",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2053540653",
    "https://openalex.org/W4251971396",
    "https://openalex.org/W2085368566",
    "https://openalex.org/W2983962589",
    "https://openalex.org/W2889670144",
    "https://openalex.org/W2624022918",
    "https://openalex.org/W2001173538",
    "https://openalex.org/W2028098350",
    "https://openalex.org/W2126611627",
    "https://openalex.org/W2998451506",
    "https://openalex.org/W2606333299"
  ],
  "abstract": "Reading comprehension is one of the essential skills for students as they make a transition from learning to read to reading to learn. Over the last decade, the increased use of digital learning materials for promoting literacy skills (e.g., oral fluency and reading comprehension) in K-12 classrooms has been a boon for teachers. However, instant access to reading materials, as well as relevant assessment tools for evaluating studentsâ€™ comprehension skills, remains to be a problem. Teachers must spend many hours looking for suitable materials for their students because high-quality reading materials and assessments are primarily available through commercial literacy programs and websites. This study proposes a promising solution to this problem by employing an artificial intelligence (AI) approach. We demonstrate how to use advanced language models (e.g., OpenAIâ€™s GPT-2 and Googleâ€™s T5) to automatically generate reading passages and items. Our preliminary findings suggest that with additional training and fine-tuning, open-source language models could be used to support the instruction and assessment of reading comprehension skills in the classroom. For both automatic story and item generation, the language models performed reasonably; however, the outcomes of these language models still require a human evaluation and further adjustments before sharing them with students. Practical implications of the findings and future research directions are discussed.",
  "full_text": " \nInternational Journal of Assessment Tools in Education \n 2022, Vol. 9, Special Issue, 72â€“87 \nhttps://doi.org/10.21449/ijate.1124382 \nPublished at https://ijate.net/              https://dergipark.org.tr/en/pub/ijate                         Research Article \n \n \n72 \n \n \nAutomatic story and item generation for reading comprehension assessments \nwith transformers \n \n \nOkan Bulut 1,*,  Seyma Nur Yildirim-Erbasli 2 \n \n1University of Alberta, Centre for Research in Applied Measurement and Evaluation, Edmonton, AB Canada \n2Concordia University of Edmonton, Faculty of Arts, Department of Psychology, Edmonton, AB Canada  \n \nARTICLE HISTORY \nReceived: June 1, 2022 \nRevised: Sep. 15, 2022 \nAccepted: Sep. 21, 2022 \n \nKeywords: \nReading comprehension,  \nNatural language \nprocessing,  \nAutomatic item \ngeneration,  \nLanguage modeling,  \nText generation. \nAbstract: Reading comprehension is one of the essential skills for students as they \nmake a transition from learning to read to reading to learn. Over the last decade, \nthe increased use of digital learning materials for promoting literacy skills (e.g., \noral fluency and reading comprehension) in K -12 classrooms has been a boon for \nteachers. However, instant access to reading materials, as well as relevant \nassessment tools for evaluating studentsâ€™ comprehension skills, remains to be a \nproblem. Teachers must spend many hours looking for suitable materials for their \nstudents because high -quality reading materials and assessments are primarily \navailable through commercial literacy programs and websites. This study proposes \na promising solution to this problem by employing a n artificial intelligence (AI) \napproach. We demonstrate how to use advanced language models (e.g., OpenAIâ€™s \nGPT-2 and Googleâ€™s T5) to automatically generate reading passages and items. Our \npreliminary findings suggest that with additional training and fine -tuning, open-\nsource language models could be used to support the instruction and assessment of \nreading comprehension skills in the classroom. For both automatic story and item \ngeneration, the language models performed reasonably; however, the outcomes of \nthese language models still require a human evaluation and further adjustments \nbefore sharing them with students. Practical implications of the findings and future \nresearch directions are discussed. \n1. INTRODUCTION \nReading comprehension is one of the esse ntial skills that all students need to foster in K -12 \neducation because their learning and success in other subjects (e.g., math, social studies, and \nhistory) are strongly associated with their proficiency in reading comprehension (Bigozzi et al., \n2017). Reading comprehension is also the key ability that students need to master to make the \ntransition from â€œlearning to readâ€ to â€œreading to learnâ€ by understanding, analyzing, and \napplying information gathered through reading different materials (e.g., books, articles, and \nnewspapers). Students without adequate reading comprehension skills may not be able to \nunderstand what they read and fail to make this transition. \n \n*Corresponding Author: Okan Bulut  ï€ª  bulut@ualberta.ca   ï‚› University of Alberta, Centre for Research \nin Applied Measurement and Evaluation, Edmonton, AB, Canada  \ne-ISSN: 2148-7456 /Â© IJATE 2022 \n\nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n73 \nDeveloping reading proficiency requires students to read more texts with varying volumes, \ngenres, and difficulties (Allington et al., 2010; Duke et al., 2011; Kim & White, 2008). To help \nstudents develop reading comprehension skills, teachers give students various texts (e.g., fables, \nfairy tales, and stories) and ask them to read these texts repeatedly. Students who struggle with \nreading comprehension might have to practice their skills by reading more texts until they \nbecome fluent readers. Once students can read the text fluently, teachers also provide a set of \nitems related to the text to measure  studentsâ€™ understanding of the text. This suggests that \nteachers may need new reading materials and items to continuously monitor studentsâ€™ growth \nin reading. Teachers attempt to find a suitable text from the literature to meet this need \nefficiently. If they try to find a text from the literature, they need to go through many pieces of \nliterature to find a suitable text, but this is a very time -consuming process. Also, it is not easy \nto find free reading materials because most of the materials on the Inter net are commercially \navailable.  \nAlternatively, the teachers may attempt to develop their own text and items associated with \neach text. However, writing original texts with different volumes, genres, or complexities is a \nhighly complex task, even for a pro fessional writer. In addition to finding a suitable text or \ncreating an authentic text, developing high -quality items related to the text is another tedious \ntask. Teachers must formulate high -quality items related to the text by targeting different \ndifficulty levels and ensuring that each item is strongly associated with the text. Therefore, a \nmore practical and sustainable solution is necessary to help teachers find suitable reading \nmaterials for their students. \n1.1. Story and Item Generation \nWriting and telling stories have been central to the human experience in every culture. As \nhumans attempt to make sense of the world surrounding them, they make discoveries and learn \nnew information. Storytelling is one of the most popular communication tools for gathering and \nsharing the knowledge gained through such valuable experiences. However, writing stories or \nnarratives is not necessarily an easy task for humans. Even good writers struggle with creating \na story that is not only syntactically and sem antically sound but also describes the chain of \nevents in a meaningful way. Also, finding the correct language elements leading to the \ngeneration of a good story is challenging. For example, the type of text (e.g., narrative vs. \nexpository text) and readability (e.g., sentence and passage length) may affect how accurately \nindividuals with differential reading abilities can comprehend a story (Begeny & Greene, 2014; \nSÃ¡enz & Fuchs, 2002). \nIn schools, storytelling has always been a part of childrenâ€™s language and literacy development, \nespecially in terms of oral fluency and reading comprehension (Agosto, 2016; Miller & \nPennycuff, 2008; Peck, 1989). Both fluency and comprehension are highly essential skills for \nlearning other subjects because studentsâ€™ ability t o understand what they read in these subject \nareas is strongly associated with their reading fluency and comprehension (Bigozzi et al., 2017). \nTeachers typically use a variety of literature selections to improve children's oral fluency and \ncomprehension skills and help them make the transition from learning to read to reading to \nlearn. With the emergence of online or digital reading materials, teachers have also begun to \nuse learning and assessment tools focusing on online reading comprehension (Bulut et al ., \n2022). Therefore, teachers always need new learning resources (i.e., online reading materials) \nand assessment tools to gauge childrenâ€™s academic growth in online reading comprehension.  \nResearchers found that the development of reading comprehension skills depends highly on the \nquality of reading materials teachers select for their students (Taylor et al., 2003; Tivnan & \nHemphill, 2005). Teachers must look for digital reading materials suitable for their students to \nsupport children's literacy developmen t. However, this is costly because most digital literacy \nBulut & Yildirim-Erbasli\n \n \n74 \nmaterials are commercial and thus require a paid subscription. Also, teachers need to develop \nitems based on each reading material that could help them evaluate studentsâ€™ reading \ncomprehension skills . Traditional procedures for creating items for reading comprehension \nassessments (e.g., manually developing items starting with where, when, when, who, and so \non) are laborious, challenging, and costly. Emerging technologies can facilitate the search for \nappropriate reading materials and items for teachers, such as text generation using language \nmodels and automatic item generation (see Das et al. [2021] for a detailed summary of the state-\nof-the-art techniques used to generate items automatically).  \n1.2. Current Study \nPrevious studies indicated that students could improve their reading comprehension skills when \nthey practiced reading frequently (Allington et al., 2010; Duke & Pearson, 2009; Duke et al., \n2011; Guthrie, 2004; Kim & White, 2008; Rasinski, 2 012; Taylor et al., 2000). In K -12 \neducation, teachers use different kinds of grade -appropriate texts (e.g., fables, fairy tales, and \nshort stories) to help students develop reading comprehension skills. This approach is essential \nfor students who struggle  with reading comprehension because they need to practice their \nreading skills more often by reading more texts. Because intensive reading is necessary for \nstudents with or without adequate reading comprehension skills, teachers need new reading \nmaterials constantly. Finding a relevant text from the literature is time -consuming because \nteachers must go through many pieces of printed or digital literature, and most  materials are \ncommercially available. In addition, the digital learning environment in the 21s t century \nrequires digital tools, including the availability of digital reading materials that can support \nteaching and learning activities. Therefore, there is a need in K -12 education to leverage the \npotential of digital instructional materials to foster  studentsâ€™ reading comprehension skills. To \naddress this need and provide a practical and sustainable solution, we aimed to build a story \ngeneration system to help teachers find suitable reading materials for their students. The \nprimary objective of our st udy was to create an artificial intelligence (AI) system that can \nanalyze existing reading materials to develop new stories and related items to improve studentsâ€™ \nreading comprehension skills. \n2. METHOD \nEmerging technologies, such as digital learning platforms and intelligent tutoring systems, have \nreshaped education during the past decade. These tools are frequently used in the classroom by \nK-12 teachers, and it is vital to design more digital tools to suit the learning needs of students. \nOne of these learning needs is to provide reading resources and items to help students improve \ntheir reading comprehension skills. However, there is only a limited number of open -access \ndigital reading resources available, and thus, teachers would have to spend a significant amount \nof time searching for appropriate materials for their students. This study aims to create an AI -\nbased system that can analyze existing reading materials to create new, authentic texts and \nrelated items that can be used to improve and assess eleme ntary students' reading \ncomprehension skills. To achieve our goals, we fine -tune a pre -trained transformer model to \ngenerate new texts (i.e., reading passages) based on existing reading materials and create related \nitems for the texts generated by the transformer model. The following sections will describe the \nstory and item generation sections in detail. \n2.1. Story Generation \nWe fine-tuned a pre-trained transformer model using classic childrenâ€™s books to perform story \ngeneration through a decoding approach. We searched reading materials (i.e., fairy tales and \nfables) that were freely available on the Internet and saved the grade -appropriate examples. In \ntotal, the dataset consisted of 3,700 human -written stories. During the training process, the  \nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n75 \nAdamax optimizer was applied with a learning rate of 5e-5, the batch size was 32, and the total \nnumber of training epochs was 3. \n2.1.1. Transformer Model: GPT-2 \nLarge-scale neural language models such as Bidirectional Encoder Representations from \nTransformers (BERT; Devlin et al., 2019), Generative Pre-trained Transformer (GPT; Radford \net al., 2018) and GPT -2 (Radford et al., 2019) have been extensively trained on massive \namounts of text to be used for complex language tasks. Pre-trained transformer language models \ndemonstrate state-of-the-art performance across different natural language tasks such as text \ngeneration, summarization, and translation. These models can be expected to generate fluent \nand diverse texts due to the large amounts of data they were tra ined on (See et al., 2019). The \nreason behind the success of the transformer-based models for different natural language tasks \nis the diversity of the training dataset. They generate texts representative of the corpora on \nwhich they were trained. A common approach is to fine-tune these language models to a specific \ndomain of interest by providing different corpora  of exemplars. These transformer -based \nmodels can effectively learn from training data and generate high -quality texts by fine-tuning \npre-trained models. This study uses GPT -2 model â€“â€“a neural language model that achieves \nstate-of-the-art performance across different tasks. The GPT-2 language model was trained with \n1.5 billion parameters on a dataset of 8 million web pages to predict the next word fo r the \nprevious words within a text (Radford et al., 2019). \n2.1.2. Decoding Algorithms \nNeural text decoding algorithms highly influence the quality of text generated (Holtzman et al., \n2019; Kulikov et al., 2018). During decoding, a vector is applied to the softmax function to \nconvert it into a probability for each word: \nğ‘ƒ(ğ‘¥|ğ‘¥1:ğ‘–âˆ’1) = expâ¡(ğ‘¢ğ‘–)\nâˆ‘ expâ¡(ğ‘¢ğ‘–)ğ‘—\n, (1) \nwhere x is a token (e.g., words, characters, or subwords) at timestep i and u is a vector that \ncontains the numerical value of every token in the vocabulary V. Considering the critical role \nof decoding algorithms in improving the pe rformance of language models, we experimented \nwith different decoding algorithms (beam search, random sampling with and without \ntemperature, top-k sampling, and top -p sampling) with different parameters for each method \n(e.g., ğ‘ = 0.90, ğ‘ = 0.92, or ğ‘ = 0.95 for top -p sampling) because the correct decoding \nalgorithm is needed to generate high-quality and meaningful texts. \n2.1.2.1. Beam Search. Beam search generates all possible tokens in a vocabulary list and \nthen chooses the top B number of candidates with the highest probability at each timestep \n(Holtzman et al., 2019). However, the search may fail to choose between the two words \nor phrases and yield a text that repeats the same word or phrase. Therefore, it tends to \nproduce low-quality texts with short sentences and excessive repetitions (Fan et al., 2018; \nBasu et al., 2020). \n2.1.2.2. Random Sampling. This method uses the probability of each token from the \nsoftmax function to generate the next token (Holtzman et al., 2019). Thus, it samples \ndirectly from probabilities estimated by the model and can generate incoherent texts \n(Holtzman et al., 2019). \n2.1.2.3. Sampling with Temperature. A probability distribution can be shaped through \ntemperature (Holtzman et al., 2019). Temperature increases the probability of probable \ntokens while decreasing the likelihood of less probable tokens. It has been widely applied \nto text generation (Fan et al ., 2018). Higher temperature values result in higher \nBulut & Yildirim-Erbasli\n \n \n76 \nrandomness in the generated text. Temperature is used to scale the value of each token \nbefore going into a softmax function. Thus, given the temperature t, the softmax is re -\nestimated as follows: \nğ‘ƒ(ğ‘¥|ğ‘¥1:ğ‘–âˆ’1) = expâ¡(ğ‘¢ğ‘–/ğ‘¡)\nâˆ‘ expâ¡(ğ‘¢ğ‘–/ğ‘¡)ğ‘—\n. (2) \n2.1.2.4. Top-k Sampling. Top-k sampling samples the next word from the k most likely \nwords (Fan et al., 2018; Holtzman et al., 2018). Thus, top -k sampling involves a fixed \nnumber of most likely words and ensures that less probable words are not sampled. \nBecause the top-k sampling restricts selection to the k-most likely words, the k subset of \nvocabulary, V, maximizes the probability of selected words: \nâˆ‘ ğ‘ƒ(ğ‘¥|ğ‘¥1:ğ‘–âˆ’1\nğ‘¥âˆˆğ‘‰(ğ‘˜)\n). (3) \n2.1.2.5. Top-p Sampling. Top-p or nucleus sampling restricts the sampling process to the \nsmallest possible set of words whose cumulative probability exceeds the probability \nthreshold (Holtzman et al., 2019). Top-p sampling distributes the probability among this \nset of words, and thus , the number of words in that set can dynamically increase or \ndecrease based on the subsequent probability distribution, indicating that it involves a \ndynamic number of words based on a fixed p value: \nâˆ‘ ğ‘ƒ(ğ‘¥|ğ‘¥1:ğ‘–âˆ’1\nğ‘¥âˆˆğ‘‰(ğ‘)\n) â‰¥ ğ‘, (4) \nwhere ğ‘‰(ğ‘) is the smallest possible set of words, ğ‘ƒ(ğ‘¥|ğ‘¥1:(ğ‘–âˆ’1)) is the probability of \ngenerating word x given the previously generated words x from 1 to (ğ‘– âˆ’1). This shows \nthat the model selects the highest probability set of words whose cumulative probability \nexceeds the pre-chosen threshold p. Similar to the beam search, top-k and top-p sampling \nmethods sometimes repeat words in a generated text for small values of k and p, while \nsimilar to random sampling, they generate incoherent text for large values of k and p \n(Basu et al., 2020). \n2.1.2.6. Hybrid Sampling . We also tested a hybrid sampling approach (i.e., the \ncombinations of top-k and top-p sampling). \n2.1.3. Model Evaluation \nTo evaluate each story generation model, we performed human evaluation by rating the quality \nof generated stories based on five criteria: fluency, coherence, grammar, logical ordering of \nevents, and human-sounding. We used a 5-point scale with the following score categories: 1 = \nFundamental errors and no meaning; 2 = Fundamental errors and  difficult to understand the \nmeaning; 3 = Moderate errors but reasonably easy to understand the meaning; 4 = Minor errors \nand reasonably easy to understand the meaning; and 5 = Minor errors and easy to understand \nthe meaning. To facilitate human evaluation, we generated stories with 100 words and selected \na subsample of 15 texts for each prompt (prompt 1: â€œIt was a beautiful day.â€ and prompt 2: \nâ€œOnce upon a timeâ€), resulting in 30 texts from each model (i.e., beam search, random sampling \nwith and without temperature, top-k sampling, top-p sampling, and hybrid sampling) and a total \nof 180 texts. We selected the parameters of the fine-tuned model and decoding algorithms based \non human evaluations.  \nIn addition to human evaluation, we used the perplexity (PPX) index as a data -driven metric \nfor evaluating automatic story generation models. The PPX index is widely used in natural \nlanguage processing (NLP) for evaluating language models. It measures how well a language \nmodel predicts text (i.e., pr obabilities of selecting the right words for an unseen test set). The \nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n77 \nPPX index is typically calculated as the inverse probability of a test set (i.e., a sequence of \ntokens produced by the language model), normalized by the number of words in the test set: \nğ‘ƒğ‘ƒğ‘‹(ğ‘Š) = ğ‘ƒ(ğ‘¤1,ğ‘¤2,â€¦,ğ‘¤ğ‘)âˆ’1\nğ‘ = âˆš 1\nğ‘ƒ(ğ‘¤1,ğ‘¤2,â€¦,ğ‘¤ğ‘),\nğ‘\n (5) \nwhere ğ‘Š is a tokenized sequence with ğ‘ tokens,  ğ‘Š = (ğ‘¤1,ğ‘¤2,â€¦,ğ‘¤ğ‘) and ğ‘ƒ(ğ‘¤1,ğ‘¤2,â€¦,ğ‘¤ğ‘) \nis the probability of observing a particular sequence of tokens. The lower the average value of \nPPX, the more accurate a language model. The PPX index can also be expressed as the \nexponential of the cross-entropy: \nğ‘ƒğ‘ƒğ‘‹(ğ‘,ğ‘) = âˆ’âˆ‘ğ‘(ğ‘¥)ğ‘™ğ‘œğ‘”ğ‘(ğ‘¥),\nğ‘‹\n (6) \nwhere ğ‘‹ refers to the language modelâ€™s vocabulary of possible tokens (e.g., words or phrases), \nğ‘(ğ‘¥) is the target distribution for tokens, and ğ‘(ğ‘¥) is the estimated distribution for tokens. PPX \ngets smaller as the predicted distribution becomes closer to the target distribution. In this study, \nthe lower the perplexity of a story generation model, the better the model's accuracy when \ncreating a new story. Figure 1 depicts the proposed framework for automatic story generation \nand model evaluation. \nFigure 1. The proposed framework for automatic story generation. \n \n2.2. Story Generation \nWe studied answer -aware item generation by jointly training item generation and answering \nand answer-agnostic item generation and compared their performance in terms of the quality of \nitems generated. With answer -aware item generation, we aimed to design an algorithm that \ngenerates items and answers simultaneously and improves the performance of each other. We \nused a pre-trained transformer architecture to develop answer-aware and answer-agnostic item \ngeneration models. In terms of input, we used the texts generated from the story gene ration \nmodel and did not perform pre -processing (e.g., convert complex sentences into more \nstraightforward sentences). \n\nBulut & Yildirim-Erbasli\n \n \n78 \n2.2.1. Model Evaluation \nItem generation models aim to automatically generate a set of items that can be answered based \non a particular co ntent (Rus et al., 2012). This content can be a single sentence, paragraph, \ndocument, or database. Some researchers studied item generation and answer generation as dual \ntasks (e.g., Tang et al., 2017), while others generated items from texts without answe rs (Du & \nCardie, 2017).  \n2.2.1.1. Answer-Aware Item Generation . Answer-aware item generation systems \nfunction with the content and generate items for target answers. However, generated items \ncan be limited to certain types of items and focused on name entities (Dong et al., 2018) \nor arbitrary entities (Duan et al., 2017; Wang et al., 2020). Thus, answer -aware item \ngeneration approaches have the drawback of generating answers focusing on entities, and \nmost items are easy to answer. \n2.2.1.2. Answer- Agnostic Item Generation . Answer-agnostic item generation \neliminates the requirement of the target answer before the items are generated. Answer \nagnostic item generation approaches reduce the bias toward entities while expanding the \nmodel flexibility (Wang et al., 2020). Although this approach is likely to generate more \ndiverse items, it may also generate unanswered items (Sun et al., 2018; Wang et al., 2020). \n2.2.2. T5: Text-To-Text Transfer Transformer  \nThere are three approaches to item generation: rule-based, neural-based, and transformer-based. \nItem generation with a rule-based approach involves manually written rules for item generation \nbased on heuristic rules and linguistic knowledge. The rule-based item generation systems can \ntransform declarative sentences into interrogative sentences (e.g., Heilman & Smith, 2010; \novergenerate and rank approach). However, these models are brittle and heavily depend on \nhuman effort. Therefore, rule-based models cannot be easily adapted to other domains (Zhou et \nal., 2018). Although rule-based models were more prevalent in generating items until the mid-\n2010s, there has been an increase in using neural networks since then (Pan et al., 2019).  \nItem generation with a neural-based approach trains a neural network based on a sequence -to-\nsequence framework from scratch. For example, Du et al. (2017) used a neural language model \nwith an encoder -decoder architecture of the sequence -to-sequence model to generate items \nwithout relying on hand -crafted rules. An input sentence and its containing paragraph are \nencoded, and an item is generated by the decoder. Their proposed model outperformed the rule-\nbased models (e.g., Heilman & Smith, 2010). However, the inherent sequential nature of these \nmodels makes it difficult to process long sequences. The sequence-to-sequence models cannot \ncapture paragraph-level content, which is necessary to generate high-quality items. A generated \nitem does not explicitly connect with the context of the  target answer, and thus, includes a \nsubstantial portion of the target answer (Liu, 2020). Existing item generation models (e.g., Du \net al., 2017) mostly use sentence -level content to generate items because models show \nsignificant performance degradation w hen applied to paragraph -level or long content. The \ntransformer-based models address these problems.  \nTransformers train and provide pre -trained models that show significant performance \nimprovements in the NLP tasks (Radford et al., 2018). With transformer -based models, it is \npossible to improve the importance of item generation and to process paragraph -level content \nfor item generation. We used T5: Text -to-Text Transfer Transformer that uses a text -to-text \nframework (i.e., takes text as input and generates new next as output) (Raffel et al., 2019). The \nT5 model is pre -trained on Colossal Clean Crawled Corpus (C4) and can be fine -tuned to \nachieve state-of-the-art results on different NLP tasks (Raffel et al., 2019). We trained the T5 -\nsmall model for answer-aware and answer-agnostic item generation models and compared their \nperformances. \nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n79 \n2.2.3. Model Evaluation  \nWe performed data -driven and human evaluations to analyze the performance of the item \ngeneration models. In terms of data -driven evaluation, we computed and reported BLEU, \nMETEOR, and ROUGE scores using the SQuAD dataset (Rajpurkar et al., 2016). These metrics \nassign a score by measuring n-grams (i.e., sequence of words) and their frequency by comparing \ngenerated text with reference text. BLEU score is a more precision -based metric that provides \nan overall assessment of model quality by measuring the similarity of the generated text to the \nreference texts without considering semantic similarity (Papineni et al., 2002). BLEU -n (e.g., \nBLEU-4) counts co -occurrences by using up to n -grams. METEOR is a more recall -based \nmetric that provides the similarity between  generated texts and reference texts by considering \nsynonyms, stemming, and paraphrases (Denkowski & Lavie, 2014). ROUGE is a more recall -\noriented metric that compares generated text against reference text (Lin, 2004). ROUGE L \nmeasures the longest co -occurrence in n -grams by considering sentence -level structure \nsimilarities. For all three indices, larger values indicate better results. \nIn addition to data-driven evaluation based on the BLEU, METEOR, and ROUGEL scores, the \ngenerated items from the answer-aware and answer-agnostic models were also subject to human \nevaluation. We randomly selected 20 sets of items from each model using the inputs generated \nby the story generation model with the hybrid sampling approach. Two human evaluators rated \nthe quality of the items based on the following criteria: grammar, answerability (i.e., the item \ncan be answered based on the paragraph), and significance (i.e., the item relies on an essential \npiece of information from the paragraph). We used a 5-point scale ranging from 1 (very poor) \nto 5 (very strong) in the human evaluation of generated items. Figure 2 depicts the proposed \nframework for automatically generating items based on reading passages.   \nFigure 2. The proposed framework for automatic item generation. \n \n \n \n \n\nBulut & Yildirim-Erbasli\n \n \n80 \n3. RESULTS \n3.1. Results for Automatic Story Generation \nTable 1 shows two example stories generated using each decoding algorithm for two prompts: \nâ€œIt was a beautiful day.â€ and â€œOnce upon a time.â€ Texts generated by the model with beam \nsearch showed extreme repetitions. The first exa mple shows that the same sentence was \nrepeated throughout the text, and the second example includes the repetition of words and \nsentences. Other researchers also reported a similar finding regarding beam search (e.g., Fan et \nal., 2018). In terms of the tex ts generated by the model with random sampling, although the \ntexts may initially seem acceptable, when taking a closer look, they are not coherent and human-\nsounding. The reason might be that random sampling generates the next token by randomly \nsampling word sequences. When we tried random sampling with temperature to increase the \nlikelihood of high probability words and decrease the likelihood of low probability words, the \ngenerated texts became coherent. However, the generated text still seemed problemati c based \non other criteria, particularly the logical ordering of the events. \nTable 1. Samples of generated texts for different decoding methods. \nDecoding \nMethods Generated Texts \nBeam search It was a beautiful day.  It was a beautiful day. It was a beautiful day. It was a \nbeautiful day. It was a beautiful day. It was a beautiful day. It was a beautiful day. \nIt was a beautiful day. It was a beautiful day. It was a beautiful day. It was a \nbeautiful day. It was a beautiful day. It was a beautiful day. It was a beautiful day. \nIt was a beautiful day. It was a beautiful day â€¦ \nOnce upon a time, there was a man in the middle of the street. He was dressed in \na black hooded sweatshirt and a white T -shirt. He had a black hooded sweatshirt \nand a white T-shirt. He had a black hooded sweatshirt and a white T-shirt. He had \na black hooded sweatshirt and a white T -shirt. He had a black hooded sweatshirt \nand a white T-shirt â€¦ \n  \nRandom sampling It was a beautiful day. The only real question is, will it be an issue when the Jets \nplay at their pre-season home for the first time since 2011? The Jets have one more \nchance Sunday, when the San Francisco 49ers play host to the Indianapolis Colts. \nIf they can't land a first-round draft pick from Chicago in Philadelphia as they did \nwith the New York pick, this is just an opportunity to build something interesting \nwith an actual head coach â€¦ \n Once upon a time, when one felt certain of his or her own powerlessness and self-\nhatred for others, he or she acted from a position of obedience to the law, but in \nanother way acted from a position of obedience to his or her own and for the \ncommon good. Thus it is in the course of the history of the United States that it is \npossible to find and articulate an attitude of gratitude and admiration for others for \nacts of kindness â€¦ \n  \nSampling with \ntemperature \nIt was a beautiful day. We had fun and we enjoyed it as much as anyone else. We \nwere just very fortunate that we had a great time. The team's goal was to win the \ngame and then make it 5 -0. But it didn't work out that way. The Red Bulls, who \nwon their first three games in Seattle, were 2-0 down at the break for the last time, \nand the Red Bulls were 0-2 down on their way to â€¦ \n Once upon a time, when we went to a place like this, it felt like a paradise. It was \nso cool to go to. It felt like heaven. It felt like a place where you c ould live. The \nonly thing that I did that was drive past and watch the sunset. I can't remember \nanything about it. I just remember being in a place where the sun is shining. And \nI never saw the sunset before â€¦ \nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n81 \nTable 1. Continued. \nDecoding \nMethods Generated Texts \nTop-k sampling It was a beautiful day . It reminded me that even though it was beautiful to me it \nwas also a little bit hard to understand. I wasn't really ready to learn how to speak \nor speak Spanish so naturally I was getting into the habit of saying this to friends \nand family and sometimes I would say something like, \"This isn't my first time \nspeaking Spanish.\" But not anymore. The realization that my accent is more than \njust a language barrier made me realize I probably could do better â€¦ \n Once upon a time  the same man had the power to force the enemy to surrender. \nHe had no power to restrain them when he was in charge, so as to not give an \nenemy the chance of destroying him. The enemy, therefore, was allowed to escape \nwithout being put to any use. On this account the men of the village were ordered \nalso to put to death their prisoners, and to eat of the corpses â€¦ \n  \nTop-p sampling It was a beautiful day. I remember once in college the walls of the MIT Building \nwere covered with graffiti and we all went down the hallway. So we all had fun \naround the MIT Buildingâ€”people going to get their snowman blankets and they \nwere breaking windows. In the morning I would go back to my office to read the \nfan pages that I would roll out in the middle of the night and go to class â€¦ \n Once upon a time, when thinking was a bold idea and no longer the tool of choice, \npower processes had a wider arc than experience and knowledge. Now we are \nbeginning to recognize that power processes (as opposed to wishful thinking) are \njust a subset of working memoryâ€”each of them operates on the same information \nfield, but with different energies. The architect of higher-order concepts has his or \nher own practices, decisions, and combinations of interests that could â€¦ \n  \nHybrid sampling It was a beautiful day . I was just amazed and happy to see this amazing team in \naction. I'm so thankful for the support and the training that they have given me \nover the years. I'm really happy that I have received so many support from the \nwhole team and I'm so thankful for the support that they have given me â€¦ \n Once upon a time, the people of this country are working to create a world where \nthe public is comfortable and the private sector is able to manage the economy. \nAnd that includes giving our youth a voice. And that includes educating them \nabout the importance of social responsibility and the role of government in \nmanaging our economy. And that includes making sure that they understand the \nimportant role of private sector employees in our economy â€¦ \nCompared to the other decoding algorithms, the model with top -k sampling generated higher-\nquality texts. This approach is more powerful in text generation as it filters only k most likely \nwords and distributes pr obability among those k following words. The model generated texts \nwith higher fluency, coherence, grammar, logical ordering of events, and human -sounding. \nHowever, it still shows some problems in terms of human -sounding (e.g., â€œhow to speak or \nspeak Spanish so naturallyâ€). The reason can be that top-k sampling does not involve a dynamic \nnumber of words as it uses a fixed k number of words, limiting creativity in the model.  \nUsing top-p sampling to sample from the smallest possible set of words instead of s ampling \nonly from the most likely k words produced texts with a wide range of words. Although both \ntop-k and top-p produced high-quality texts, top-p seems to be a better decoding algorithm than \ntop-k in theory (i.e., dynamic number of words). Finally, we had better results when we tried a \nhybrid of top -k and top -p sampling. After human evaluation of models by two raters, we \nselected the hybrid samplingâ€”a combination of top-p and top-k sampling. The hybrid sampling \nwas substantially more effective than other approaches because it generated texts with better \nfluency, coherence, grammar, logical ordering of events, and human-sounding. \nBulut & Yildirim-Erbasli\n \n \n82 \nIn addition to human evaluation, we also used the PPX index to make a data-driven comparison \namong the story generation approaches. Figure 3 shows the perplexity results for each decoding \nmethod. The hybrid sampling approach yielded the smallest PPX value, suggesting that the text \ngenerated by this approach had the least amount of randomness based on the underlying \nlanguage model. Surprisingly, top-p sampling yielded the largest perplexity value, followed by \nbeam search. This finding indicates that the text generate d by the top -p decoding method did \nnot necessarily follow the underlying language model accurately. In other words, the text \ngenerated by top-p sampling included a high amount of randomness.  \nFigure 3. Perplexity values for generated texts by different decoding algorithms. \n \n3.2. Results for Automatic Item Generation \nTable 2 shows a sample of answer-agnostic and answer-aware items produced by the T5 model. \nThe results showed that although the answer-agnostic item generation method allows for more \nflexibility in the language model (Wang et al., 2020), the answer-aware item generation method \nproduced a higher number of quality items. Both methods (i.e., answer -agnostic and answer-\naware item generation) yielded reading comprehension items with â€œwhatâ€ and â€œhowâ€ questions. \nWe identified two problems with the answer-agnostic items. First, some items already included \nthe answer within the item stem. Second, some items involved a question statement that cannot \nbe answered based on the available information from the reading passage. The items generated \nby the answer-aware item generation method were also problematic in other ways. For example, \nsome items (e.g., â€œWhat kind of day was it?â€) required very simple or trivial answers. Also, \nsome items were unnecessarily long, such as â€œWhat do the people of th is country do to create \na world where the public is comfortable and the private sector is able to manage the economy?â€. \nFurther revisions are necessary to make these items helpful in evaluating studentsâ€™ reading \ncomprehension. \n \n \n \n \n0 5 10 15 20 25 30 35 40\nHybrid sampling\nRandom sampling with temperature\nRandom sampling without temperature\nTop-k sampling\nBeam search\nTop-p sampling\nPerplexity\nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n83 \nTable 2. Sample items generated by the answer-agnostic and answer-aware models. \nModel Input Answer-Agnostic  \nItems \nAnswer-Aware  \nItems \nIt was a beautiful day. I was just \namazed and happy to see this \namazing team in action. I'm so \nthankful for the support and the \ntraining that they have given me \nover the years. I'm really happy \nthat I have received so many \nsupports from the whole team and \nI'm so thankful for the support \nthat they have given me. \nWhat was a beautiful day \nfor me? \n \nWhat team has given me \nthe support they have given \nme over the years? \n \nHow many support did I \nreceive from the whole \nteam? \nQ: What kind of day was it? \nA: Beautiful \n \nQ: What team was in action? \nA: Amazing \n \nQ: What is the reason I'm so \nthankful for? \nA: The support and the training that \nthey have given me \n \nQ: Who has given me so much \nsupport? \nA: The whole team \n \n  \nOnce upon a time, the people of \nthis country are working to create \na world where the public is \ncomfortable, and the private \nsector is able to manage the \neconomy. And that includes \ngiving our youth a voice. And \nthat includes educating them \nabout the importance of social \nresponsibility and the role of \ngovernment in managing our \neconomy. And that includes \nmaking sure that they understand \nthe important role of private \nsector employees in our \neconomy. \nWhat are the people of this \ncountry working to create a \nworld where the public is \ncomfortable and the private \nsector is able to manage the \neconomy? \n \nWhat does that include \ngiving our youth a voice? \n \nHow do the youth learn \nabout the importance of \nsocial responsibility and the \nrole of government? \nQ: What are the people of this \ncountry working to create a \nworld? \nA: The public is comfortable and \nthe private sector is able to \nmanage the economy \n \nQ: What do the people of this \ncountry do to create a world \nwhere the public is comfortable \nand the private sector is able to \nmanage the economy? \nA: Giving our youth a voice \n \nQ: What is the key to educating \nyouth about the importance of \nsocial responsibility and the role \nof government in managing our \neconomy? \nA: Making sure that they \nunderstand the important role of \nprivate sector employees in our \neconomy \n Q: Question; A: Answer.  \nTable 3 shows the model evaluation indices for the items generated by the answer-agnostic and \nanswer-aware methods. The results show that the answer -aware item generation performed \nslightly better than the answer -agnostic item generation; however, the difference between the \ntwo methods was negligible. Overall, the findings of our study appear to broadly support the \nwork of other studies in automatic item generation. In our study, the answer -agnostic method \nyielded unanswerable items and failed to generate diverse items (Sun et al., 2018; Wang et al., \n2020). Also, the answer -aware method yielded simple items that do not necessarily require \nhigher levels of reading comprehension to find the correct answer.  \n \nBulut & Yildirim-Erbasli\n \n \n84 \nTable 3. Evaluation indices for the items generated by the answer-agnostic and answer-aware methods. \nItem Generation Model BLEU-4 METEOR ROGUEL \nAnswer-Agnostic 18.3 24.7 39.9 \nAnswer-Aware 18.6 24.9 40.2 \n4. DISCUSSION and CONCLUSION \nPre-trained transformer models can generate high -quality texts and items due to the large \namounts of corpus they are trained on (See et al., 2019). In this study, we fine-tuned pre-trained \ntransformer models to generate new stories and related items to enhance and assess students' \nreading comprehension skills. The proposed story an d item generation models attain a fine -\ntuned understanding to produce human-like stories and items. However, it should be noted that \nthe models might generate stories with repetitive words or unnatural changes in the topic. These \nweaknesses of language models remain a common challenge for the NLP community (Radford \net al., 2019). \nOur story generation model with hybrid sampling showed promising results in producing fluent, \ncoherent, grammatically correct, logical, and human-sounding stories that students could use to \npractice and enhance their reading comprehension skills. Also, our answer -aware item \ngeneration model showed promising results in producing grammatically correct, answerable, \nand significant items. These language models for automatic story and it em generation could \nenable teachers to generate authentic stories and items on the fly and share them with their \nstudents easily, without having to look for freely available printed or digital materials for hours. \nHowever, it should be noted that the generated items may still require a human evaluation and \nfurther adjustments before sharing them with students as they are likely to involve semantic \nerrors (i.e., grammatically correct but nonsensical text). Also, the generated items may not be \nsuitable for me asuring complex reading skills such as inferencing, analyzing, and critiquing. \nOverall, the proposed models provide a feasible solution to the problem of finding new texts \nfrom the limited printed or digital materials and related items to the texts. \nThere are several limitations of this study. First, we used GPT -2 small and T5 -small (i.e., the \nsmallest versions of GPT -2 and T5) to generate stories and items due to their relatively less \ndemand for computing power. It is possible that more advanced versions o f the GPT -2 (e.g., \nGPT-2 large) and T5 (e.g., T5 -base and T5 -large) could generate higher -quality stories and \nitems. Second, this study used a training dataset that involved freely available reading materials \n(i.e., fairy tales and fables) available on the  Internet. A larger -size training dataset including \nmore diverse reading materials (e.g., short stories, articles, or novels) could help fine -tune a \ntransformer model more effectively and yield more consistent results in story and item \ngeneration stages. Finally, the sample stories and items generated in this study were not shared \nwith students. Future studies on automatic story and item generation could involve students \nwho can provide feedback on the readability and clarity of the generated stories and items. The \nfeedback from students could facilitate the fine-tuning of pre-trained language models.  \nAcknowledgments \nThe research leading to these results received funding from the University of Albertaâ€™s \nEndowment Fund for the Support for the Advancement of Scholarship (SAS) Program. \nDeclaration of Conflicting Interests and Ethics \nThe authors declare no conflict of interest.  This research study complies with research \npublishing ethics. The scientific and legal responsibility for manuscripts published in IJATE \nbelongs to the authors. \n \nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n85 \nAuthorship Contribution Statement \nOkan Bulut: Investigation, Resources, Methodology, Software, Formal Analysis, and Writing-\noriginal draft. Seyma Nur Yildirim -Erbasli: Methodology, Software, Formal Analysis, and \nWriting-original draft. \nOrcid \nOkan Bulut   https://orcid.org/0000-0001-5853-1267 \nSeyma Nur Yildirim-Erbasli   https://orcid.org/0000-0002-8010-9414 \nREFERENCES \nAgosto, D.E. (2016). Why storytelling matters: Unveiling the literacy benefits of storytelling. \nChildren and Libraries, 14(2), 21-26. https://doi.org/10.5860/cal.14n2.21  \nAllington, R.L., McGill-Franzen, A., Camilli, G., Williams, L., Graff, J., Zeig, J., Zmach, C., \n& Nowak, R. (2010). Addressing summer reading setback among economically disadva\nntaged elementary students. Reading Psychology, 31(5), 411-427. https://doi.org/10.108\n0/02702711.2010.505165 \nBasu, S., Ramachandran, G.S., Keskar , N.S., & Varshney, L.R. (2020). Mirostat: A neural \ntext decoding algorithm that directly controls perplexity. arXiv preprint. https://doi.org/\n10.48550/arXiv.2007.14966 \nBegeny, J.C., & Greene, D.J. (2014). Can readability formulas be used to successfully gauge \ndifficulty of reading materials? Psychology in the Schools, 51(2), 198-215. https://doi.or\ng/10.1002/pits.21740  \nBigozzi, L., Tarchi, C., Vagnoli, L., Valente, E., & Pinto, G. (2017). Reading fluency as a \npredictor of school outcomes across grades 4 -9. Frontiers in Psychology, 8 (200), 1-9. \nhttps://doi.org/10.3389/fpsyg.2017.00200 \nBulut, H.C., Bulut, O., & Arikan, S. (2022). Evaluating group differences in online reading \ncomprehension: The impact of item properties. International Journal of Testing. Advance \nonline publication. https://doi.org/10.1080/15305058.2022.2044821  \nDas, B., Majumder, M., Phadikar, S., & Sekh, A.A. (2021). Automatic question generation and \nanswer assessment: A survey. Research and Practice in Technology Enhanced Learning\n, 16(1), 1-15. https://doi.org/10.1186/s41039-021-00151-1  \nDenkowski, M., & Lavie, A. (2014, June). Meteor universal: Language specific translation \nevaluation for any target language. In Proceedings of the ninth workshop on statistical \nmachine translation (pp. 376-380). \nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). Bert: Pre-training of deep bidirec\ntional transformers for language understanding. arXiv preprint. https://doi.org/10.48550\n/arXiv.1810.04805 \nDong, X., Hong, Y., Chen, X., Li, W., Zhang, M., & Zhu, Q. (2018, August). Neural question \ngeneration with semantics of question type. In CCF International Conference on Natural \nLanguage Processing and Chinese Computing (pp. 213-223). Springer, Cham. \nDu, X., & Cardie, C. (2017, September). Identifying where to focus in reading comprehension \nfor neural question generation. In Proceedings of the 2017 Conference on Empirical \nMethods in Natural Language Processing (pp. 2067-2073). https://doi.org/10.18653/v1/\nD17-1219  \nDu, X., Shao, J., & Cardie, C. (2017). Learning to ask: Neural question generation for reading \ncomprehension. arXiv preprint. https://doi.org/10.48550/arXiv.1705.00106 \nDuan, N., Tang, D., Chen, P., & Zhou, M. (2017, September). Question generation for question \nanswering. In Proceedings of th e 2017 conference on empirical methods in natural \nlanguage processing (pp. 866-874). https://doi.org/10.18653/v1/D17-1090  \n\nBulut & Yildirim-Erbasli\n \n \n86 \nDuke, N.K., & Pearson, P.D. (2009). Effective p ractices for developing reading \ncomprehension. Journal of Education, 189(1/2), 107-122. https://doi.org/10.1177/00220\n57409189001-208 \nDuke, N.K., Pearson, P.D., Strachan, S.L., & Billman, A.K. (2011). Essential elements of \nfostering and teaching reading comprehension. What research has to say about reading \ninstruction, 4, 286-314. \nFan, A., Lewis, M., & Dauphin, Y. (2018). Hierarchical neural story generation. arXiv preprint. \nhttps://doi.org/10.48550/arXiv.1805.04833 \nGuthrie, J.T. (2004). Teaching for literacy engagement. Journal of Literacy Research, 36(1), 1-\n30. https://doi.org/10.1207/s15548430jlr3601_2 \nHeilman, M., & Smith, N.A. (2010, June). Good question! Statistical ranking for question \ngeneration. In Human Language Technologies: The 2010 Annual Conference of the North \nAmerican Chapter of the Association for Computational Linguistics (pp. 609-617). \nHoltzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). The curious case of neural text \ndegeneration. arXiv preprint. https://doi.org/10.48550/arXiv.1904.09751 \nHoltzman, A., Buys, J., Forbes, M., Bosselut, A., Golub, D., & Choi, Y. (2018) Learning to \nwrite with cooperative discriminators. arXiv preprint. https://doi.org/10.48550/arXiv.18\n05.06087 \nKim, J.S., & White, T.G. (2008). Scaffolding voluntary summer reading for children in grades \n3 to 5: An experimental study. Scientific Studies of Reading, 12(1), 1-23. https://doi.org/\n10.1080/10888430701746849 \nKulikov, I., Miller, A.H., Cho, K., & Weston, J. (2018). Importance of search and evaluation \nstrategies in neural dialogue modelling. arXiv preprint. https://doi.org/10.48550/arXiv.1\n811.00907 \nLiu, B. (2020, April). Neural question generation based on Seq2Seq. In Proceedings of the 2020 \n5th International Conference on Mathematics and Artificial Intelligence (pp. 119-123). \nLin, C.Y. (2004, July). Rouge: A package for automatic evaluation of summaries. In Text \nsummarization branches out (pp. 74-81). \nMiller, S., & Pennycuff, L. (200 8). The power of story: Using storytelling to improve literacy \nlearning. Journal of Cross-Disciplinary Perspectives in Education, 1(1), 36-43. \nPan, L., Lei, W., Chua, T.S., & Kan, M.Y. (2019). Recent advances in neural question \ngeneration. arXiv preprint arXiv: https://doi.org/10.48550/arXiv.1905.08949  \nPapineni, K., Roukos, S., Ward, T., & Zhu, W.J. (2002, July). Bleu: A method for automatic \nevaluation of machine tr anslation. In Proceedings of the 40th annual meeting of the \nAssociation for Computational Linguistics (pp. 311-318). \nPeck, J. (1989). Using storytelling to promote language and literacy development. The Reading \nTeacher, 43(2), 138-141. https://www.jstor.org/stable/20200308 \nRadford, A., Narasimhan, K., Salimans, T., & Sutskever , I. (2018). Improving language \nunderstanding by generative pre-training. OpenAI tech report. \nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models \nare unsupervised multitask learners. OpenAI tech report. \nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P.J. (2019). \nExploring the limits of transfer learning with a unified text -to-text transformer. arXiv \npreprint. https://doi.org/10.48550/arXiv.1910.10683 \nRajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). Squad: 100,000+ questions for \nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical \nMethods in Natural Language Processing (pp. 2383â€“2392). \nRasinski, T.V. (2012). Why reading fluency should be hot! The Reading Teacher, 65(8), 516-\n522. https://doi.org/10.1002/TRTR.01077 \nInt. J. Assess. Tools Educ., Vol. 9, Special Issue, (2022) pp. 72â€“87 \n \n87 \nRus, V., Wyse, B., Piwek, P., Lintean, M., Stoyanchev, S., & Moldovan. C. (2012). A detailed \naccount of the first question generation shared task evaluation challenge. Dialogue and \nDiscourse, 3(2), 177â€“204. https://doi.org/10.5087/dad \nSÃ¡enz, L.M., & Fuchs, L.S. (2002). Examining the reading difficulty of secondary students with \nlearning disabilities: Expository versus narrative text.  Remedial and Special \nEducation, 23(1), 31-41. \nSee, A., Pappu, A., Saxena, R., Yerukola, A., & Manning, C.D. (2019). Do massively pretrain\ned language models make better storytellers? arXiv preprint. https://doi.org/10.48550/ar\nXiv.1909.10705 \nSun, X., Liu, J., Lyu , Y., He, W., Ma, Y., & Wang, S. (2018). Answer -focused and position-\naware neural question generation. In Proceedings of the 2018 Conference on Empirical \nMethods in Natural Language Processing (pp. 3930-3939). \nTang, D., Duan, N., Qin, T., Yan, Z., & Zhou, M. (2017). Question answering and question \ngeneration as dual tasks. arXiv preprint. https://doi.org/10.48550/arXiv.1706.02027 \nTaylor, B.M., Pearson, P.D., Clark, K., & Walpole, S. (2000). Effective schools and \naccomplished teachers: Lessons about primary -grade reading instruction in low -income \nschools. The Elementary School Journal, 101(2), 121-165. https://doi.org/10.1086/4996\n62 \nTaylor, B.M., Pearson, P.D., Peterson, D.S., & Rodriguez, M.C. (2003). Reading growth in \nhigh-poverty classrooms: The influence of teacher practices that encourage co gnitive \nengagement in literacy learning. The Elementary School Journal, 104(1), 3-28. https://d\noi.org/10.1086/499740  \nTivnan, T., & Hemphill, L. (2005). Comparing four literacy reform models in high -poverty \nschools: Patterns of first -grade achievement. The Elementary School Journal , 105(5), \n419-441. https://doi.org/10.1086/431885  \nWang, B., Wang, X., Tao, T., Zhang, Q., & Xu, J. (2020, April). Neural question generation \nwith answer pivot. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. \n34, No. 05, pp. 9138-9145).  \nZhou, Q., Yang, N., Wei, F., Tan, C., Bao, H., & Zhou, M. (2017, November). Neural question \ngeneration from text: A preliminary study. In  National CCF Conference on Natural \nLanguage Processing and Chinese Computing (pp. 662-671). Springer, Cham. ",
  "topic": "Fluency",
  "concepts": [
    {
      "name": "Fluency",
      "score": 0.8053209781646729
    },
    {
      "name": "Reading comprehension",
      "score": 0.7847733497619629
    },
    {
      "name": "Computer science",
      "score": 0.6796003580093384
    },
    {
      "name": "Comprehension",
      "score": 0.564498245716095
    },
    {
      "name": "Literacy",
      "score": 0.5308436155319214
    },
    {
      "name": "Reading (process)",
      "score": 0.5240345597267151
    },
    {
      "name": "Mathematics education",
      "score": 0.49206608533859253
    },
    {
      "name": "Multimedia",
      "score": 0.3789063096046448
    },
    {
      "name": "Psychology",
      "score": 0.25928258895874023
    },
    {
      "name": "Pedagogy",
      "score": 0.1905064582824707
    },
    {
      "name": "Linguistics",
      "score": 0.1291850209236145
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I131564278",
      "name": "Concordia University of Edmonton",
      "country": "CA"
    }
  ]
}