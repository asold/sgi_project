{
  "title": "Big Bird: Transformers for Longer Sequences",
  "url": "https://openalex.org/W3045733172",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221384542",
      "name": "Zaheer, Manzil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4228062543",
      "name": "Guruganesh, Guru",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286770130",
      "name": "Dubey, Avinava",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222103880",
      "name": "Ainslie, Joshua",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288584116",
      "name": "Alberti, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748706685",
      "name": "Ontañón, Santiago",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288646555",
      "name": "Pham, Philip",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221599551",
      "name": "Ravula, Anirudh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2351348477",
      "name": "Wang, Qifan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2079581369",
      "name": "Yang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2318825982",
      "name": "Ahmed Amr",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3039017601",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2129575457",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3023166997",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W3011641611",
    "https://openalex.org/W2084732238",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W3034156543",
    "https://openalex.org/W2746517179",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3037965615",
    "https://openalex.org/W2018047324",
    "https://openalex.org/W2899244816",
    "https://openalex.org/W3025151218",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W2932524182",
    "https://openalex.org/W2973098646",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2944517707",
    "https://openalex.org/W2055693471",
    "https://openalex.org/W3103398325",
    "https://openalex.org/W2798240283",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2769802733",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2808305145",
    "https://openalex.org/W2934798344",
    "https://openalex.org/W2986367395",
    "https://openalex.org/W2112090702",
    "https://openalex.org/W2198606573",
    "https://openalex.org/W2587688848",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1988581590",
    "https://openalex.org/W2955041501",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3010293452",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2969740599",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W1787336979",
    "https://openalex.org/W2954278700",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2765390718",
    "https://openalex.org/W2264676752",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2768006850",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2147380886",
    "https://openalex.org/W2950856799",
    "https://openalex.org/W2021250792",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2979196189",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W100656083",
    "https://openalex.org/W2128016314",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W3005437683",
    "https://openalex.org/W2908802752",
    "https://openalex.org/W3034314247",
    "https://openalex.org/W2952802059",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2948138438",
    "https://openalex.org/W2972114612",
    "https://openalex.org/W2735099323",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2969307504",
    "https://openalex.org/W2899386490",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2809485291",
    "https://openalex.org/W2954141754",
    "https://openalex.org/W2947497897",
    "https://openalex.org/W3016915903",
    "https://openalex.org/W2270152626",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3021780374",
    "https://openalex.org/W2947590214",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2905321002",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W3101913037",
    "https://openalex.org/W2207058206",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3106009088",
    "https://openalex.org/W2027377866",
    "https://openalex.org/W2963739921",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2885185669",
    "https://openalex.org/W2949417144",
    "https://openalex.org/W2950130316",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2989536007",
    "https://openalex.org/W2952862139",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2911430044",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2995744795",
    "https://openalex.org/W1267754874",
    "https://openalex.org/W2945886944"
  ],
  "abstract": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.",
  "full_text": "Big Bird: Transformers for Longer Sequences\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham,\nAnirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed\nGoogle Research\n{manzilz, gurug, avinavadubey}@google.com\nAbstract\nTransformers-based models, such as BERT, have been one of the most successful\ndeep learning models for NLP. Unfortunately, one of their core limitations is the\nquadratic dependency (mainly in terms of memory) on the sequence length due to\ntheir full attention mechanism. To remedy this, we propose, BIGBIRD , a sparse\nattention mechanism that reduces this quadratic dependency to linear. We show\nthat BIGBIRD is a universal approximator of sequence functions and is Turing\ncomplete, thereby preserving these properties of the quadratic, full attention model.\nAlong the way, our theoretical analysis reveals some of the beneﬁts of having\nO(1) global tokens (such as CLS), that attend to the entire sequence as part of the\nsparse attention mechanism. The proposed sparse attention can handle sequences\nof length up to 8x of what was previously possible using similar hardware. As\na consequence of the capability to handle longer context, BIGBIRD drastically\nimproves performance on various NLP tasks such as question answering and\nsummarization. We also propose novel applications to genomics data.\n1 Introduction\nModels based on Transformers [ 91], such as BERT [ 22, 63], are wildly successful for a wide\nvariety of Natural Language Processing (NLP) tasks and consequently are mainstay of modern NLP\nresearch. Their versatility and robustness are the primary drivers behind the wide-scale adoption of\nTransformers. The model is easily adapted for a diverse range of sequence based tasks – as a seq2seq\nmodel for translation [ 91], summarization [66], generation [15], etc. or as a standalone encoders\nfor sentiment analysis [83], POS tagging [65], machine reading comprehension [93], etc. – and it\nis known to vastly outperform previous sequence models like LSTM [ 37]. The key innovation in\nTransformers is the introduction of a self-attention mechanism, which can be evaluated in parallel\nfor each token of the input sequence, eliminating the sequential dependency in recurrent neural\nnetworks, like LSTM. This parallelism enables Transformers to leverage the full power of modern\nSIMD hardware accelerators like GPUs/TPUs, thereby facilitating training of NLP models on datasets\nof unprecedented size. This ability to train on large scale data has led to surfacing of models like\nBERT [22] and T5 [75], which pretrain transformers on large general purpose corpora and transfer\nthe knowledge to down-stream task. The pretraining has led to signiﬁcant improvement in low data\nregime downstream tasks [51] as well as tasks with sufﬁcient data [101] and thus have been a major\nforce behind the ubiquity of transformers in contemporary NLP.\nThe self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN)\nby allowing each token in the input sequence to attend independently to every other token in the\nsequence. This design choice has several interesting repercussions. In particular, the full self-attention\nhave computational and memory requirement that is quadratic in the sequence length. We note that\nwhile the corpus can be large, the sequence length, which provides the context in many applications\nis very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2007.14062v2  [cs.LG]  8 Jan 2021\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its\ndirect applicability to tasks that require larger context, like QA [60], document classiﬁcation, etc.\nHowever, while we know that self-attention and Transformers are useful, our theoretical understanding\nis rudimentary. What aspects of the self-attention model are necessary for its performance? What\ncan we say about the expressivity of Transformers and similar models? Apriori, it was not even clear\nfrom the design if the proposed self-attention mechanism was as effective as RNNs. For example, the\nself-attention does not even obey sequence order as it is permutation equivariant. This concern has\nbeen partially resolved, as Yun et al.[104] showed that transformers are expressive enough to capture\nall continuous sequence to sequence functions with a compact domain. Meanwhile, Pérez et al. [72]\nshowed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two\nnatural questions arise: Can we achieve the empirical beneﬁts of a fully quadratic self-attention\nscheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity\nand ﬂexibility of the original network?\nIn this paper, we address both the above questions and produce a sparse attention mechanism that\nimproves performance on a multitude of tasks that require long contexts. We systematically develop\nBIGBIRD , an attention mechanism whose complexity is linear in the number of tokens (Sec. 2). We\ntake inspiration from graph sparsiﬁcation methods and understand where the proof for expressiveness\nof Transformers breaks down when full-attention is relaxed to form the proposed attention pattern.\nThis understanding helped us develop BIGBIRD , which is theoretically as expressive and also\nempirically useful. In particular, our BIGBIRD consists of three main part:\n• A set of gglobal tokens attending on all parts of the sequence.\n• All tokens attending to a set of wlocal neighboring tokens.\n• All tokens attending to a set of rrandom tokens.\nThis leads to a high performing attention mechanism scaling to much longer sequence lengths (8x).\nTo summarize, our main contributions are:\n1. BIGBIRD satisﬁes all the known theoretical properties of full transformer (Sec. 3). In particular,\nwe show that adding extra tokens allows one to express all continuous sequence to sequence\nfunctions with only O(n)-inner products. Furthermore, we show that under standard assumptions\nregarding precision, BIGBIRD is Turing complete.\n2. Empirically, we show that the extended context modelled by BIGBIRD beneﬁts variety of NLP\ntasks. We achieve state of the art results for question answering and document summarization on\na number of different datasets. Summary of these results are presented in Sec. 4.\n3. Lastly, we introduce a novel application of attention based models where long contexts are\nbeneﬁcial: extracting contextual representations of genomics sequences like DNA. With longer\nmasked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoter-\nregion and chromatin proﬁle prediction (Sec. 5).\n1.1 Related Work\nThere have been a number of interesting attempts, that were aimed at alleviating the quadratic\ndependency of Transformers, which can broadly categorized into two directions. First line of work\nembraces the length limitation and develops method around it. Simplest methods in this category\njust employ sliding window [93], but in general most work ﬁts in the following general paradigm:\nusing some other mechanism select a smaller subset of relevant contexts to feed in the transformer\nand optionally iterate, i.e. call transformer block multiple time with different contexts each time.\nMost prominently, SpanBERT [42], ORQA [54], REALM [34], RAG [57] have achieved strong\nperformance for different tasks. However, it is worth noting that these methods often require signiﬁcant\nengineering efforts (like back prop through large scale nearest neighbor search) and are hard to train.\nSecond line of work questions if full attention is essential and have tried to come up with approaches\nthat do not require full attention, thereby reducing the memory and computation requirements.\nProminently, Dai et al.[21], Sukhbaatar et al.[82], Rae et al.[74] have proposed auto-regresive models\nthat work well for left-to-right language modeling but suffer in tasks which require bidirectional\ncontext. Child et al. [16] proposed a sparse model that reduces the complexity to O(n√n), Kitaev\net al. [49] further reduced the complexity to O(nlog(n)) by using LSH to compute nearest neighbors.\n2\n(a) Random attention\n (b) Window attention\n (c) Global Attention\n (d) BIGBIRD\nFigure 1: Building blocks of the attention mechanism used inBIGBIRD . White color indicates absence\nof attention. (a) random attention with r = 2, (b) sliding window attention with w = 3 (c) global\nattention with g= 2. (d) the combined BIGBIRD model.\nYe et al. [103] proposed binary partitions of the data where as Qiu et al. [73] reduced complexity by\nusing block sparsity. Recently, Longformer [8] introduced a localized sliding window based mask with\nfew global mask to reduce computation and extended BERT to longer sequence based tasks. Finally,\nour work is closely related to and built on the work of Extended Transformers Construction [ 4].\nThis work was designed to encode structure in text for transformers. The idea of global tokens was\nused extensively by them to achieve their goals. Our theoretical work can be seen as providing\na justiﬁcation for the success of these models as well. It is important to note that most of the\naforementioned methods are heuristic based and empirically are not as versatile and robust as the\noriginal transformer, i.e. the same architecture do not attain SoTA on multiple standard benchmarks.\n(There is one exception of Longformer which we include in all our comparisons, see App. E.3 for a\nmore detailed comparison). Moreover, these approximations do not come with theoretical guarantees.\n2 B IGBIRD Architecture\nIn this section, we describe the BIGBIRD model using the generalised attention mechanism that\nis used in each layer of transformer operating on an input sequence X = (x1,..., xn) ∈Rn×d.\nThe generalized attention mechanism is described by a directed graph Dwhose vertex set is [n] =\n{1,...,n }. The set of arcs (directed edges) represent the set of inner products that the attention\nmechanism will consider. Let N(i) denote the out-neighbors set of node iin D, then the ith output\nvector of the generalized attention mechanism is deﬁned as\nATTN D(X)i = xi +\nH∑\nh=1\nσ\n(\nQh(xi)Kh(XN(i))T\n)\n·Vh(XN(i)) (AT)\nwhere Qh,Kh : Rd →Rm are query and key functions respectively, Vh : Rd →Rd is a value\nfunction, σis a scoring function (e.g. softmax or hardmax) and H denotes the number of heads. Also\nnote XN(i) corresponds to the matrix formed by only stacking {xj : j ∈N(i)}and not all the inputs.\nIf Dis the complete digraph, we recover the full quadratic attention mechanism of Vaswani et al.\n[91]. To simplify our exposition, we will operate on the adjacency matrix Aof the graph Deven\nthough the underlying graph maybe sparse. To elaborate, A∈[0,1]n×n with A(i,j) = 1 if query\niattends to key j and is zero otherwise. For example, when Ais the ones matrix (as in BERT), it\nleads to quadratic complexity, since all tokens attend on every other token. This view of self-attention\nas a fully connected graph allows us to exploit existing graph theory to help reduce its complexity.\nThe problem of reducing the quadratic complexity of self-attention can now be seen as a graph\nsparsiﬁcation problem. It is well-known that random graphs are expanders and can approximate\ncomplete graphs in a number of different contexts including in their spectral properties [80, 38]. We\nbelieve sparse random graph for attention mechanism should have two desiderata: small average path\nlength between nodes and a notion of locality, each of which we discuss below.\nLet us consider the simplest random graph construction, known as Erd˝os-Rényi model, where each\nedge is independently chosen with a ﬁxed probability. In such a random graph with just ˜Θ(n)\nedges, the shortest path between any two nodes is logarithmic in the number of nodes [17, 43]. As\na consequence, such a random graph approximates the complete graph spectrally and its second\neigenvalue (of the adjacency matrix) is quite far from the ﬁrst eigenvalue [9, 10, 6]. This property\nleads to a rapid mixing time for random walks in the grpah, which informally suggests that information\ncan ﬂow fast between any pair of nodes. Thus, we propose a sparse attention where each query attends\nover rrandom number of keys i.e. A(i,·) = 1 for rrandomly chosen keys (see Fig. 1a).\n3\nThe second viewpoint which inspired the creation of BIGBIRD is that most contexts within NLP\nand computational biology have data which displays a great deal of locality of reference. In this\nphenomenon, a great deal of information about a token can be derived from its neighboring tokens.\nMost pertinently, Clark et al. [19] investigated self-attention models in NLP tasks and concluded that\nthat neighboring inner-products are extremely important. The concept of locality, proximity of tokens\nin linguistic structure, also forms the basis of various linguistic theories such as transformational-\ngenerative grammar. In the terminology of graph theory, clustering coefﬁcient is a measure of locality\nof connectivity, and is high when the graph contains many cliques or near-cliques (subgraphs that\nare almost fully interconnected). Simple Erd˝os-Rényi random graphs do not have a high clustering\ncoefﬁcient [84], but a class of random graphs, known as small world graphs, exhibit high clustering\ncoefﬁcient [94]. A particular model introduced by Watts and Strogatz [94] is of high relevance to us\nas it achieves a good balance between average shortest path and the notion of locality. The generative\nprocess of their model is as follows: Construct a regular ring lattice, a graph with nnodes each\nconnected to wneighbors, w/2 on each side.\nModel MLM SQuAD MNLI\nBERT-base 64.2 88.5 83.4\nRandom (R) 60.1 83.0 80.2\nWindow (W) 58.3 76.4 73.1\nR + W 62.7 85.1 80.5\nTable 1: Building block comparison @512\nIn other words we begin with a sliding window\non the nodes. Then a random subset ( k%) of all\nconnections is replaced with a random connection.\nThe other (100 - k)% local connections are retained.\nHowever, deleting such random edges might be in-\nefﬁcient on modern hardware, so we retain it, which\nwill not affect its properties. In summary, to capture\nthese local structures in the context, in BIGBIRD ,\nwe deﬁne a sliding window attention, so that during\nself attention of width w, query at location iattends from i−w/2 to i+ w/2 keys. In our notation,\nA(i,i−w/2 : i+w/2) = 1 (see Fig. 1b). As an initial sanity check, we performed basic experiments\nto test whether these intuitions are sufﬁcient in getting performance close to BERT like models, while\nkeeping attention linear in the number of tokens. We found that random blocks and local window\nwere insufﬁcient in capturing all the context necessary to compete with the performance of BERT.\nThe ﬁnal piece of BIGBIRD is inspired from our theoretical analysis (Sec. 3), which is critical\nfor empirical performance. More speciﬁcally, our theory utilizes the importance of “global tokens”\n(tokens that attend to all tokens in the sequence and to whom all tokens attend to (see Fig. 1c). These\nglobal tokens can be deﬁned in two ways:\n• BIGBIRD -ITC : In internal transformer construction (ITC ), we make some existing tokens “global”,\nwhich attend over the entire sequence. Concretely, we choose a subset G of indices (with\ng:= |G|), such that A(i,:) = 1 and A(:,i) = 1 for all i∈G.\n• BIGBIRD -ETC : In extended transformer construction ( ETC ), we include additional “global”\ntokens such as CLS. Concretely, we add g global tokens that attend to all existing tokens. In\nour notation, this corresponds to creating a new matrix B ∈ [0,1](N+g)×(N+g) by adding\ng rows to matrix A, such that B(i,:) = 1 , and B(:,i) = 1 for all i ∈ {1,2,...g }, and\nB(g+ i,g + j) = A(i,j)∀i,j ∈{1,...,N }. This adds extra location to store context and as\nwe will see in the experiments improves performance.\nThe ﬁnal attention mechanism for BIGBIRD (Fig. 1d) has all three of these properties: queries attend\nto rrandom keys, each query attends to w/2 tokens to the left of its location and w/2 to the right of\nits location and they contain gglobal tokens (The global tokens can be from existing tokens or extra\nadded tokens). We provide implementation details in App. D.\n3 Theoretical Results about Sparse Attention Mechanism\nIn this section, we will show that that sparse attention mechanisms are as powerful and expressive as\nfull-attention mechanisms in two respects. First, we show that when sparse attention mechanisms\nare used in a standalone encoder (such as BERT), they are Universal Approximators of sequence\nto sequence functions in the style of Yun et al.[104]. We note that this property was also explored\ntheoretically in contemporary work Yun et al. [105]. Second, unlike [ 105], we further show that\nsparse encoder-decoder transformers are Turing Complete (assuming the same conditions deﬁned\nin [72]). Complementing the above positive results, we also show that moving to a sparse-attention\n4\nmechanism incurs a cost, i.e. there is no free lunch. In Sec. 3.4, we show lower bounds by exhibiting\na natural task where any sufﬁciently sparse mechanism will require polynomially more layers.\n3.1 Notation\nThe complete Transformer encoder stack is nothing but the repeated application of a single-layer\nencoder (with independent parameters). We denote class of such Transformer encoders stack, deﬁned\nusing generalized encoder (Sec. 2), by TH,m,q\nD which consists of H-heads with head size mand qis\nthe hidden layer size of the output network, and the attention layer is deﬁned by the directed graph D.\nThe key difference between our proposed attention mechanism to that of Vaswani et al.[91], Yun et al.\n[104] is that we add a special token at the beginning of each sequence and assign it a special vector.\nWe will refer to this as x0. Therefore our graph Dwill have vertex set {0}∪[n] = {0,1,2,...,n }.\nWe will assume that this extra node and its respective vector will be dropped at the ﬁnal output layer\nof transformer. To avoid cumbersome notation, we will still treat transformer as mapping sequences\nX ∈Rn×d to Rn×d. We will also allow the transformer to append position embeddings E ∈Rd×n\nto matrix X in the input layer.\nFinally, we need to deﬁne the function class and distance measure for proving universal approximation\nproperty. LetFCD denote the set of continuous functionsf : [0,1]n×d →Rn×dwhich are continuous\nwith respect to the topology deﬁned by ℓp norm. Recall for any p≥1, the ℓp distance is dp(f1,f2) =(∫\n∥f1(X) −f2(X)∥p\npdX\n)1/p\n.\n3.2 Universal Approximators\nDeﬁnition 1. The star-graph Scentered at 0 is the graph deﬁned on {0,...,n }. The neighborhood\nof all vertices iis N(i) = {0,i}for i∈{1 ...n }and N(0) = {1,...n }.\nOur main theorem is that the sparse attention mechanism deﬁned by any graph containing S is a\nuniversal approximator:\nTheorem 1. Given 1 < p <∞and ϵ >0, for any f ∈FCD, there exists a transformer with\nsparse-attention, g∈T H,m,q\nD such that dp(f,g) ≤ϵwhere Dis any graph containing star graph S.\nTo prove the theorem, we will follow the standard proof structure outlined in [104].\nStep 1: Approximate FCD by piece-wise constant functions. Since f is a continuous function\nwith bounded domain [0,1)n×d, we will approximate it with a suitable piece-wise constant function.\nThis is accomplished by a suitable partition of the region [0,1) into a grid of granularity δ to get\na discrete set Gδ. Therefore, we can assume that we are dealing with a function ¯f : Gδ →Rn×d,\nwhere dp(f, ¯f) ≤ϵ\n3 .\nStep 2: Approximate piece-wise constant functions by modiﬁed transformers. This is the key\nstep of the proof where the self-attention mechanism is used to generate a contextual-mapping of the\ninput. Informally, a contextual mapping is a unique code for the pair consisting of a matrix (X,xi)\nand a column. Its uniqueness allows the Feed forward layers to use each code to map it to a unique\noutput column.\nThe main technical challenge is computing the contextual mapping using only sparse attention\nmechanism. This was done in [104] using a “selective” shift operator which shift up entries that are\nin a speciﬁc interval. Key to their proof was the fact that the shift, was exactly the range of the largest\nentry to the smallest entry.\nCreating a contextual mapping with a sparse attention mechanism is quite a challenge. In particular,\nbecause each query only attends to a few keys, it is not at all clear that sufﬁcient information can\nbe corralled to make a contextual embedding of the entire matrix. To get around this, we develop a\nsparse shift operator which shifts the entries of the matrices if they lie in a certain range. The exact\namount of the shift is controlled by the directed sparse attention graphg D. The second key ingredient\nis the use of additional global token. By carefully applying the operator to a set of chosen ranges, we\nwill show that each column will contain a unique mapping of the full mapping. Therefore, we can\naugment the loss of inner-products in the self attention mechanism by using multiple layers and an\nauxiliary global token.\n5\nStep 3: Approximate modiﬁed transformers by original Transformers : The ﬁnal step is to ap-\nproximate the modiﬁed transformers by the original transformer which uses ReLU and softmax.\nWe provide the full details in App. A.\n3.3 Turing Completeness\nTransformers are a very general class. In the original paper of Vaswani et al.[91], they were used in\nboth an encoder and a decoder. While the previous section outlined how powerful just the encoders\nwere, another natural question is to ask what the additional power of both a decoder along with\nan encoder is? Pérez et al. [72] showed that the full transformer based on a quadratic attention\nmechanism is Turing Complete. This result makes one unrealistic assumption, which is that the\nmodel works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers\nare bounded ﬁnite state machines and cannot be Turing Complete.\nIt is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism\nalso be used to simulate any Turing Machine? We show that this is indeed the case: we can use a\nsparse encoder and sparse decoder to simulate any Turing Machine.\nTo use the sparse attention mechanism in the transformer architecture, we need to deﬁne a suitable\nmodiﬁcation where each token only reacts to previous tokens. Unlike the case for BERT, where the\nentire attention mechanism is applied once, in full transformers, the sparse attention mechanism at\ndecoder side is used token by token. Secondly the work of Pérez et al. [72], uses each token as a\nrepresentation of the tape history and uses the full attention to move and retrieve the correct tape\nsymbol. Most of the construction of Pérez et al. [72] goes through for sparse attentions, except for\ntheir addressing scheme to point back in history (Lemma B.4 in [72]). We show how to simulate this\nusing a sparse attention mechanism and defer the details to App. B.\n3.4 Limitations\nWe demonstrate a natural task which can be solved by the full attention mechanism inO(1)-layers.\nHowever, under standard complexity theoretic assumptions, this problem requires ˜Ω(n)-layers for\nany sparse attention layers with ˜O(n) edges (not just BIGBIRD ). (Here ˜O hides poly-logarthmic\nfactors). Consider the simple problem of ﬁnding the corresponding furthest vector for each vector in\nthe given sequence of length n. Formally,\nTask 1. Given nunit vectors {u1,...,u n}, ﬁnd f(u1,...,u n) →(u1∗,...,u n∗) where for a ﬁxed\nj ∈[n], we deﬁne j∗= arg maxk∥uk −uj∥2\n2.\nFinding vectors that are furthest apart boils down to minimize inner product search in case of unit\nvectors. For a full-attention mechanism with appropriate query and keys, this task is very easy as we\ncan evaluate all pair-wise inner products.\nThe impossibility for sparse-attention follows from hardness results stemming from Orthogonal Vector\nConjecture(OVC) [1, 2, 7, 96]. The OVC is a widely used assumption in ﬁne-grained complexity.\nInformally, it states that one cannot determine if the minimum inner product amongnboolean vectors\nis 0 in subquadratic time. In App. C, we show a reduction using OVC to show that if a transformer\ng ∈T H=1,m=2d,q=0\nD for any sparse directed graph D can evaluate the Task 1, it can solve the\northogonal vector problem.\nProposition 1. There exists a single layer full self-attention g∈T H=1,m=2d,q=0 that can evaluate\nTask 1, i.e. g(u1,...,u n) = [u1∗,...,u n∗], but for any sparse-attention graph Dwith ˜O(n) edges\n(i.e. inner product evaluations), would require ˜Ω(n1−o(1)) layers.\nWe give a formal proof of this fact in App. C.\n4 Experiments: Natural Language Processing\nIn this section our goal is to showcase beneﬁts of modeling longer input sequence for NLP tasks,\nfor which we select three representative tasks. We begin with basic masked language modeling\n(MLM; Devlin et al. 22) to check if better contextual representations can be learnt by utilizing longer\ncontiguous sequences. Next, we consider QA with supporting evidence, for which capability to handle\nlonger sequence would allow us to retrieve more evidence using crude systems like TF-IDF/BM25.\n6\nModel HotpotQA NaturalQ TriviaQA WikiHop\nAns Sup Joint LA SA Full MCQ\nRoBERTa 73.5 83.4 63.5 - - 74.3 72.4\nLongformer 74.3 84.4 64.4 - - 75.2 75.0\nBIGBIRD -ITC 75.7 86.8 67.7 70.8 53.3 79.5 75.9\nBIGBIRD -ETC 75.5 87.1 67.8 73.9 54.9 78.7 75.9\nTable 2: QA Dev results using Base size models. We report accuracy for WikiHop and F1 for\nHotpotQA, Natural Questions, and TriviaQA.\nModel HotpotQA NaturalQ TriviaQA WikiHop\nAns Sup Joint LA SA Full Veriﬁed MCQ\nHGN [26] 82.2 88.5 74.2 - - - - -\nGSAN 81.6 88.7 73.9 - - - - -\nReﬂectionNet [32] - - - 77.1 64.1 - - -\nRikiNet-v2 [61] - - - 76.1 61.3 - - -\nFusion-in-Decoder [39] - - - - - 84.4 90.3 -\nSpanBERT [42] - - - - - 79.1 86.6 -\nMRC-GCN [87] - - - - - - - 78.3\nMultiHop [14] - - - - - - - 76.5\nLongformer [8] 81.2 88.3 73.2 - - 77.3 85.3 81.9\nBIGBIRD -ETC 81.2 89.1 73.6 77.8 57.9 84.5 92.4 82.3\nTable 3: Fine-tuning results on Test set for QA tasks. The Test results (F1 for HotpotQA, Natural\nQuestions, TriviaQA, and Accuracy for WikiHop) have been picked from their respective leaderboard.\nFor each task the top-3 leaders were picked not including BIGBIRD -etc. For Natural Questions\nLong Answer (LA), TriviaQA, and WikiHop, B IGBIRD -ETC is the new state-of-the-art . On\nHotpotQA we are third in the leaderboard by F1 and second by Exact Match (EM).\nFinally, we tackle long document classiﬁcation where discriminating information may not be located\nin ﬁrst 512 tokens. Below we summarize the results for BIGBIRD using sequence length 40961, while\nwe defer all other setup details including computational resources, batch size, step size, to App. E.\nPretraining and MLM We follow [22, 63] to create base and large versions of BIGBIRD and\npretrain it using MLM objective. This task involves predicting a random subset of tokens which\nhave been masked out. We use four standard data-sets for pretraining (listed in App. E.1, Tab. 9),\nwarm-starting from the public RoBERTa checkpoint2. We compare performance in predicting the\nmasked out tokens in terms of bits per character, following [ 8]. As seen in App. E.1, Tab. 10,\nboth BIGBIRD and Longformer perform better than limited length RoBERTa, withBIGBIRD -ETC\nperforming the best. We note that we trained our models on a reasonable 16GBmemory/chip with\nbatch size of 32-64. Our memory efﬁciency is due to efﬁcient blocking and sparsity structure of the\nsparse attention mechanism described in Sec. 2.\nQuestion Answering (QA) We considered following four challenging datasets:\n1. Natural Questions [52]: For the given question, ﬁnd a short span of answer (SA) from the given\nevidences as well highlight the paragraph from the given evidences containing information about\nthe correct answer (LA).\n2. HotpotQA-distractor [100]: Similar to natural questions, it requires ﬁnding the answer (Ans) as\nwell as the supporting facts (Sup) over different documents needed for multi-hop reasoning from\nthe given evidences.\n3. TriviaQA-wiki [41]: We need to provide an answer for the given question using provided\nWikipedia evidence, however, the answer might not be present in the given evidence. On a\n1code available at http://goo.gle/bigbird-transformer\n2https://github.com/pytorch/fairseq/tree/master/examples/roberta\n7\nsmaller veriﬁed subset of question, the given evidence is guaranteed to contain the answer.\nNevertheless, we model the answer as span selection problem in this case as well.\n4. WikiHop [95]: Chose correct option from multiple-choice questions (MCQ), by aggregating\ninformation spread across multiple documents given in the evidences.\nAs these tasks are very competitive, multiple highly engineered systems have been designed speciﬁc\neach dataset conﬁrming to respective output formats. For a fair comparison, we had to use some\nadditional regularization for training BIGBIRD , details of which are provided in App. E.2 along\nwith exact architecture description. We experiment using the base sized model and select the best\nconﬁguration on the development set for each dataset (as reported in Tab. 2). We can see that\nBIGBIRD -ETC , with expanded global tokens consistently outperforms all other models. Thus, we\nchose this conﬁguration to train a large sized model to be used for evaluation on the hidden test set.\nIn Tab. 3, we compareBIGBIRD -ETC model to top-3 entries from the leaderboard excludingBIGBIRD .\nOne can clearly see the importance of using longer context as both Longformer and BIGBIRD\noutperform models with smaller contexts. Also, it is worth noting that BIGBIRD submission is a\nsingle model, whereas the other top-3 entries for Natural Questions are ensembles, which might\nexplain the slightly lower accuracy in exact answer phrase selection.\nClassiﬁcation We experiment on datasets of different lengths and contents, speciﬁcally various\ndocument classiﬁcation and GLUE tasks. Following BERT, we used one layer with cross entropy\nloss on top of the ﬁrst [CLS] token. We see that gains of using BIGBIRD are more signiﬁcant\nwhen we have longer documents and fewer training examples. For instance, using base sized model,\nBIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the\nimprovement over SoTA (which is not BERT based) is not signiﬁcant. Note that this performance\ngain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present\ndetailed results in App. E.4 which show competitive performance.\n4.1 Encoder-Decoder Tasks\nFor an encoder-decoder setup, one can easily see that both suffer from quadratic complexity due to\nthe full self attention. We focus on introducing the sparse attention mechanism of BIGBIRD only at\nthe encoder side. This is because, in practical generative applications, the length of output sequence\nis typically small as compared to the input. For example for text summarization, we see in realistic\nscenarios (c.f. App. E.5 Tab. 18) that the median output sequence length is ∼200 where as the input\nModel\nArxiv PubMed BigPatent\nR-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L\nPrior Art\nSumBasic [68] 29.47 6.95 26.30 37.15 11.36 33.43 27.44 7.08 23.66\nLexRank [25] 33.85 10.73 28.99 39.19 13.89 34.59 35.57 10.47 29.03\nLSA [97] 29.91 7.42 25.67 33.89 9.93 29.70 - - -\nAttn-Seq2Seq [85] 29.30 6.00 25.56 31.55 8.52 27.38 28.74 7.87 24.66\nPntr-Gen-Seq2Seq [77] 32.06 9.04 25.16 35.86 10.22 29.69 33.14 11.63 28.55\nLong-Doc-Seq2Seq [20] 35.80 11.05 31.80 38.93 15.37 35.21 - - -\nSent-CLF [81] 34.01 8.71 30.41 45.01 19.91 41.16 36.20 10.99 31.83\nSent-PTR [81] 42.32 15.63 38.06 43.30 17.92 39.47 34.21 10.78 30.07\nExtr-Abst-TLM [81] 41.62 14.69 38.03 42.13 16.27 39.21 38.65 12.31 34.09\nDancer [31] 42.70 16.54 38.44 44.09 17.69 40.27 - - -\nBase\nTransformer 28.52 6.70 25.58 31.71 8.32 29.42 39.66 20.94 31.20\n+ RoBERTa [76] 31.98 8.13 29.53 35.77 13.85 33.32 41.11 22.10 32.58\n+ Pegasus [107] 34.81 10.16 30.14 39.98 15.15 35.89 43.55 20.43 31.80\nBIGBIRD -RoBERTa 41.22 16.43 36.96 43.70 19.32 39.99 55.69 37.27 45.56\nLarge\nPegasus (Reported) [107] 44.21 16.95 38.83 45.97 20.15 41.34 52.29 33.08 41.75\nPegasus (Re-eval) 43.85 16.83 39.17 44.53 19.30 40.70 52.25 33.04 41.80\nBIGBIRD -Pegasus 46.63 19.02 41.77 46.32 20.65 42.33 60.64 42.46 50.01\nTable 4: Summarization ROUGE score for long documents.\n8\nsequence’s median length is>3000. For such applications, it is more efﬁcient to use sparse attention\nmechanism for the encoder and full self-attention for the decoder.\nSummarization Document summarization is a task of creating a short and accurate summary of\na text document. We used three long document datasets for testing our model details of which are\nmention in Tab. 18. In this paper we focus on abstractive summarization of long documents where\nusing a longer contextual encoder should improve performance. The reasons are two fold: First, the\nsalient content can be evenly distributed in the long document, not just in ﬁrst 512 tokens, and this\nis by design in the BigPatents dataset [ 78]. Second, longer documents exhibit a richer discourse\nstructure and summaries are considerably more abstractive, thereby observing more context helps.\nAs has been pointed out recently [ 76, 107], pretraining helps in generative tasks, we warm start\nfrom our general purpose MLM pretraining on base-sized models as well as utilizing state-of-the-art\nsummarization speciﬁc pretraining from Pegasus [107] on large-sized models. The results of training\nBIGBIRD sparse encoder along with full decoder on these long document datasets are presented\nin Tab. 4. We can clearly see modeling longer context brings signiﬁcant improvement. Along with\nhyperparameters, we also present results on shorter but more widespread datasets in App. E.5, which\nshow that using sparse attention does not hamper performance either.\n5 Experiments: Genomics\nThere has been a recent upsurge in using deep learning for genomics data [86, 106, 13], which has\nresulted in improved performance on several biologically-signiﬁcant tasks such as promoter site\nprediction [71], methylation analysis [55], predicting functional effects of non-coding variant [109],\netc. These approaches consume DNA sequence fragments as inputs, and therefore we believe longer\ninput sequence handling capability of BIGBIRD would be beneﬁcial as many functional effects\nin DNA are highly non-local [ 12]. Furthermore, taking inspiration from NLP, we learn powerful\ncontextual representations for DNA fragments utilizing abundant unlabeled data (e.g. human reference\ngenome, Saccharomyces Genome Database) via MLM pretraining. Next, we showcase that our long\ninput BIGBIRD along with the proposed pretraining signiﬁcantly improves performances in two\ndownstream tasks. Detailed experimental setup for the two tasks are provided in App. F.\nModel BPC\nSRILM [58] 1.57\nBERT (sqln. 512) 1.23\nBIGBIRD (sqln. 4096) 1.12\nTable 5: MLM BPC\nPre-training and MLM As explored in Liang [58], instead of oper-\nating on base pairs, we propose to ﬁrst segment DNA into tokens so\nas to further increase the context length (App. F, Fig. 7). In particular,\nwe build a byte-pair encoding [50] table for the DNA sequence of size\n32K, with each token representing 8.78 base pairs on average. We\nlearn contextual representation of these token on the human reference\ngenome (GRCh37)3 using MLM objective. We then report the bits\nper character (BPC) on a held-out set in Tab. 5. We ﬁnd that attention\nbased contextual representation of DNA does improve BPC, which is further improved by using\nlonger context.\nModel F1\nCNNProm [90] 69.7\nDeePromoter [71] 95.6\nBIGBIRD 99.9\nTable 6: Comparison.\nPromoter Region Prediction Promoter is a DNA region typically lo-\ncated upstream of the gene, which is the site of transcription initiation.\nMultiple methods have been proposed to identify the promoter regions in\na given DNA sequence [99, 59, 11, 98, 71], as it is an important ﬁrst step\nin understanding gene regulation. The corresponding machine learning\ntask is to classify a given DNA fragment as promoter or non-promoter\nsequence. We use the dataset compiled by Oubounyt et al.[71] which was\nbuilt from Eukaryotic Promoter Database (EPDnew) [24] 4. We ﬁnetuned\nthe pretrained BIGBIRD model from above, using the training data and report F1 on test dataset. We\ncompare our results to the previously reported best method in Tab. 6. We see that BIGBIRD achieve\nnearly perfect accuracy with a 5% jump from the previous best reported accuracy.\n3https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.13/\n4https://epd.epfl.ch/human/human_database.php?db=human\n9\nModel TF HM DHS\ngkm-SVM [30] 89.6 - -\nDeepSea [109] 95.8 85.6 92.3\nBIGBIRD 96.1 88.7 92.1\nTable 7: Chromatin-Proﬁle Prediction\nChromatin-Proﬁle Prediction Non-coding regions of\nDNA do not code for proteins. Majority of diseases and\nother trait associated single-nucleotide polymorphism are\ncorrelated to non-coding genomic variations [ 109, 46].\nThus, understanding the functional effects of non-coding\nregions of DNA is a very important task. An important\nstep in this process, as deﬁned by Zhou and Troyanskaya\n[109], is to predict large-scale chromatin-proﬁling from\nnon-coding genomic sequence. To this effect, DeepSea [109], compiled 919 chromatin-proﬁle of 2.4M\nnon-coding variants from Encyclopedia of DNA Elements (ENCODE)5 and Roadmap Epigenomics\nprojects6. The corresponding ML task is to predict, for a given non-coding region of DNA, these\n919 chromatin-proﬁle including 690 transcription factors (TF) binding proﬁles for 160 different TFs,\n125 DNase I sensitivity (DHS) proﬁles and 104 histone-mark (HM) proﬁles. We jointly learn 919\nbinary classiﬁers to predict these functional effects from sequence of DNA fragments. On held-out\nchromosomes, we compare AUC with the baselines in Tab. 7 and see that we signiﬁcantly improve\non performance on the harder task HM, which is known to have longer-range correlations [27] than\nothers.\n6 Conclusion\nWe propose BIGBIRD : a sparse attention mechanism that is linear in the number of tokens. BIGBIRD\nsatisﬁes a number of theoretical results: it is a universal approximator of sequence to sequence\nfunctions and is also Turing complete. Theoretically, we use the power of extra global tokens preserve\nthe expressive powers of the model. We complement these results by showing that moving to sparse\nattention mechanism do incur a cost. Empirically, BIGBIRD gives state-of-the-art performance on\na number of NLP tasks such as question answering and long document classiﬁcation. We further\nintroduce attention based contextual language model for DNA and ﬁne-tune it for down stream tasks\nsuch as promoter region prediction and predicting effects of non-coding variants.\nReferences\n[1] A. Abboud, V . V . Williams, and O. Weimann. Consequences of faster alignment of se-\nquences. In International Colloquium on Automata, Languages, and Programming, pages\n39–51. Springer, 2014.\n[2] A. Abboud, A. Backurs, and V . V . Williams. Tight hardness results for lcs and other sequence\nsimilarity measures. In 2015 IEEE 56th Annual Symposium on Foundations of Computer\nScience, pages 59–78. IEEE, 2015.\n[3] J. Abreu, L. Fred, D. Macêdo, and C. Zanchettin. Hierarchical attentional hybrid neural net-\nworks for document classiﬁcation. In International Conference on Artiﬁcial Neural Networks,\npages 396–402. Springer, 2019.\n[4] J. Ainslie, S. Ontanon, C. Alberti, P. Pham, A. Ravula, and S. Sanghai. Etc: Encoding long\nand structured data in transformers. arXiv preprint arXiv:2004.08483, 2020.\n[5] C. Alberti, K. Lee, and M. Collins. A bert baseline for the natural questions. arXiv preprint\narXiv:1901.08634, 2019.\n[6] J. Alt, R. Ducatez, and A. Knowles. Extremal eigenvalues of critical erd\\h {o}sr\\’enyi graphs.\narXiv preprint arXiv:1905.03243, 2019.\n[7] A. Backurs and P. Indyk. Edit distance cannot be computed in strongly subquadratic time\n(unless seth is false). In Proceedings of the forty-seventh annual ACM symposium on Theory\nof computing, pages 51–58, 2015.\n[8] I. Beltagy, M. E. Peters, and A. Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020.\n5https://www.encodeproject.org/\n6http://www.roadmapepigenomics.org/\n10\n[9] F. Benaych-Georges, C. Bordenave, A. Knowles, et al. Largest eigenvalues of sparse inhomo-\ngeneous erd˝os–rényi graphs. Annals of Probability, 47(3):1653–1676, 2019.\n[10] F. Benaych-Georges, C. Bordenave, A. Knowles, et al. Spectral radii of sparse random\nmatrices. In Annales de l’Institut Henri Poincaré, Probabilités et Statistiques, volume 56,\npages 2141–2161. Institut Henri Poincaré, 2020.\n[11] R. Bharanikumar, K. A. R. Premkumar, and A. Palaniappan. Promoterpredict: sequence-based\nmodelling of escherichia coli σ70 promoter strength yields logarithmic dependence between\npromoter strength and sequence. PeerJ, 6:e5862, 2018.\n[12] S. Buldyrev, A. Goldberger, S. Havlin, R. Mantegna, M. Matsa, C.-K. Peng, M. Simons,\nand H. Stanley. Long-range correlation properties of coding and noncoding dna sequences:\nGenbank analysis. Physical Review E, 51(5):5084, 1995.\n[13] A. Busia, G. E. Dahl, C. Fannjiang, D. H. Alexander, E. Dorfman, R. Poplin, C. Y . McLean,\nP.-C. Chang, and M. DePristo. A deep learning approach to pattern recognition for short dna\nsequences. BioRxiv, page 353474, 2019.\n[14] J. Chen, S.-t. Lin, and G. Durrett. Multi-hop question answering via reasoning chains. arXiv\npreprint arXiv:1910.02610, 2019.\n[15] Y .-C. Chen, Z. Gan, Y . Cheng, J. Liu, and J. Liu. Distilling the knowledge of bert for text\ngeneration. arXiv preprint arXiv:1911.03829, 2019.\n[16] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n[17] F. Chung and L. Lu. The average distances in random graphs with given expected degrees.\nProceedings of the National Academy of Sciences, 99(25):15879–15882, 2002.\n[18] C. Clark and M. Gardner. Simple and effective multi-paragraph reading comprehension. arXiv\npreprint arXiv:1710.10723, 2017.\n[19] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of\nbert’s attention.arXiv preprint arXiv:1906.04341, 2019.\n[20] A. Cohan, F. Dernoncourt, D. S. Kim, T. Bui, S. Kim, W. Chang, and N. Goharian. A\ndiscourse-aware attention model for abstractive summarization of long documents. arXiv\npreprint arXiv:1804.05685, 2018.\n[21] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov. Transformer-xl:\nAttentive language models beyond a ﬁxed-length context. arXiv:1901.02860, 2019.\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[23] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M. Zhou, and H.-W. Hon.\nUniﬁed language model pre-training for natural language understanding and generation. In\nAdvances in Neural Information Processing Systems, pages 13042–13054, 2019.\n[24] R. Dreos, G. Ambrosini, R. Cavin Périer, and P. Bucher. Epd and epdnew, high-quality\npromoter resources in the next-generation sequencing era. Nucleic acids research, 41(D1):\nD157–D164, 2013.\n[25] G. Erkan and D. R. Radev. Lexrank: Graph-based lexical centrality as salience in text\nsummarization. Journal of artiﬁcial intelligence research, 22:457–479, 2004.\n[26] Y . Fang, S. Sun, Z. Gan, R. Pillai, S. Wang, and J. Liu. Hierarchical graph network for\nmulti-hop question answering. arXiv preprint arXiv:1911.03631, 2019.\n[27] L. A. Gates, C. E. Foulds, and B. W. O’Malley. Histone marks in the ‘driver’s seat’: functional\nroles in steering the transcription cycle. Trends in biochemical sciences, 42(12):977–989,\n2017.\n11\n[28] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin. Convolutional sequence\nto sequence learning. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pages 1243–1252. JMLR. org, 2017.\n[29] S. Gehrmann, Y . Deng, and A. M. Rush. Bottom-up abstractive summarization.arXiv preprint\narXiv:1808.10792, 2018.\n[30] M. Ghandi, D. Lee, M. Mohammad-Noori, and M. A. Beer. Enhanced regulatory sequence\nprediction using gapped k-mer features. PLoS computational biology, 10(7), 2014.\n[31] A. Gidiotis and G. Tsoumakas. A divide-and-conquer approach to the summarization of\nacademic articles. arXiv preprint arXiv:2004.06190, 2020.\n[32] M. Gong. ReﬂectionNet, 2020 (accessed June 3, 2020). URL https://www.microsoft.\ncom/en-us/research/people/migon/.\n[33] S. Gray, A. Radford, and D. P. Kingma. Gpu kernels for block-sparse weights. arXiv preprint\narXiv:1711.09224, 3, 2017.\n[34] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. Realm: Retrieval-augmented language\nmodel pre-training. arXiv preprint arXiv:2002.08909, 2020.\n[35] J. He, L. Wang, L. Liu, J. Feng, and H. Wu. Long document classiﬁcation from local word\nglimpses via recurrent attention learning. IEEE Access, 7:40707–40718, 2019.\n[36] K. M. Hermann, T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blun-\nsom. Teaching machines to read and comprehend. In Advances in neural information\nprocessing systems, pages 1693–1701, 2015.\n[37] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\n[38] S. Hoory, N. Linial, and A. Wigderson. Expander graphs and their applications. Bulletin of the\nAmerican Mathematical Society, 43(4):439–561, 2006.\n[39] G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open domain\nquestion answering. arXiv preprint arXiv:2007.01282, 2020.\n[40] Y . Jiang, J. Petrak, X. Song, K. Bontcheva, and D. Maynard. Team bertha von suttner\nat semeval-2019 task 4: Hyperpartisan news detection using elmo sentence representation\nconvolutional network. In Proceedings of the 13th International Workshop on Semantic\nEvaluation, pages 840–844, 2019.\n[41] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised\nchallenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, Vancouver, Canada, July 2017. Association for\nComputational Linguistics.\n[42] M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. Spanbert: Improv-\ning pre-training by representing and predicting spans. Transactions of the Association for\nComputational Linguistics, 8:64–77, 2020.\n[43] E. Katzav, O. Biham, and A. K. Hartmann. Distribution of shortest path lengths in subcritical\nerd˝os-rényi networks. Physical Review E, 98(1):012301, 2018.\n[44] W. J. Kent, C. W. Sugnet, T. S. Furey, K. M. Roskin, T. H. Pringle, A. M. Zahler, and\nD. Haussler. The human genome browser at ucsc. Genome research, 12(6):996–1006, 2002.\n[45] U. Khandelwal, K. Clark, D. Jurafsky, and L. Kaiser. Sample efﬁcient text summarization\nusing a single pre-trained transformer. arXiv preprint arXiv:1905.08836, 2019.\n[46] E. Khurana, Y . Fu, D. Chakravarty, F. Demichelis, M. A. Rubin, and M. Gerstein. Role of\nnon-coding sequence variants in cancer. Nature Reviews Genetics, 17(2):93, 2016.\n12\n[47] J. Kiesel, M. Mestre, R. Shukla, E. Vincent, P. Adineh, D. Corney, B. Stein, and M. Potthast.\nSemeval-2019 task 4: Hyperpartisan news detection. In Proceedings of the 13th International\nWorkshop on Semantic Evaluation, pages 829–839, 2019.\n[48] B. Kim, H. Kim, and G. Kim. Abstractive summarization of reddit posts with multi-level\nmemory networks. arXiv preprint arXiv:1811.00783, 2018.\n[49] N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efﬁcient transformer. In International\nConference on Learning Representations, 2019.\n[50] T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n[51] V . Kumar, A. Choudhary, and E. Cho. Data augmentation using pre-trained transformer models.\narXiv preprint arXiv:2003.02245, 2020.\n[52] T. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh, C. Alberti, D. Epstein,\nI. Polosukhin, J. Devlin, K. Lee, et al. Natural questions: a benchmark for question answering\nresearch. Transactions of the Association for Computational Linguistics, 7:453–466, 2019.\n[53] J.-S. Lee and J. Hsiang. Patent classiﬁcation by ﬁne-tuning bert language model. World Patent\nInformation, 61:101965, 2020.\n[54] K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain\nquestion answering. arXiv preprint arXiv:1906.00300, 2019.\n[55] J. J. Levy, A. J. Titus, C. L. Petersen, Y . Chen, L. A. Salas, and B. C. Christensen. Methylnet:\nan automated and modular deep learning approach for dna methylation analysis. BMC\nbioinformatics, 21(1):1–15, 2020.\n[56] M. Lewis, Y . Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V . Stoyanov, and\nL. Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n[57] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t.\nYih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks.\narXiv preprint arXiv:2005.11401, 2020.\n[58] W. Liang. Segmenting dna sequence into words based on statistical language model. Nature\nPrecedings, pages 1–1, 2012.\n[59] H. Lin, Z.-Y . Liang, H. Tang, and W. Chen. Identifying sigma70 promoters with novel pseudo\nnucleotide composition. IEEE/ACM transactions on computational biology and bioinformatics,\n2017.\n[60] J. Lin, D. Quan, V . Sinha, K. Bakshi, D. Huynh, B. Katz, and D. R. Karger. What makes a\ngood answer? the role of context in question answering. In Proceedings of the Ninth IFIP\nTC13 International Conference on Human-Computer Interaction (INTERACT 2003), pages\n25–32, 2003.\n[61] D. Liu, Y . Gong, J. Fu, Y . Yan, J. Chen, D. Jiang, J. Lv, and N. Duan. Rikinet: Reading\nwikipedia pages for natural question answering. arXiv preprint arXiv:2004.14560, 2020.\n[62] Y . Liu and M. Lapata. Text summarization with pretrained encoders. arXiv preprint\narXiv:1908.08345, 2019.\n[63] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[64] A. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng, and C. Potts. Learning word vectors\nfor sentiment analysis. In Proceedings of the 49th annual meeting of the association for\ncomputational linguistics: Human language technologies, pages 142–150, 2011.\n13\n[65] L. Martin, B. Muller, P. J. O. Suárez, Y . Dupont, L. Romary, É. V . de la Clergerie, D. Seddah,\nand B. Sagot. Camembert: a tasty french language model. arXiv preprint arXiv:1911.03894,\n2019.\n[66] D. Miller. Leveraging bert for extractive text summarization on lectures. arXiv preprint\narXiv:1906.04165, 2019.\n[67] S. Narayan, S. B. Cohen, and M. Lapata. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. arXiv preprint\narXiv:1808.08745, 2018.\n[68] A. Nenkova and L. Vanderwende. The impact of frequency on summarization. Microsoft\nResearch, Redmond, Washington, Tech. Rep. MSR-TR-2005, 101, 2005.\n[69] M. L. Olson, L. Zhang, and C.-N. Yu. Adapting pretrained language models for long document\nclassiﬁcation. OpenReview, 2019.\n[70] A. v. d. Oord, Y . Li, and O. Vinyals. Representation learning with contrastive predictive coding.\narXiv preprint arXiv:1807.03748, 2018.\n[71] M. Oubounyt, Z. Louadi, H. Tayara, and K. T. Chong. Deepromoter: Robust promoter predictor\nusing deep learning. Frontiers in genetics, 10, 2019.\n[72] J. Pérez, J. Marinkovi´c, and P. Barceló. On the turing completeness of modern neural network\narchitectures. arXiv preprint arXiv:1901.03429, 2019.\n[73] J. Qiu, H. Ma, O. Levy, S. W.-t. Yih, S. Wang, and J. Tang. Blockwise self-attention for long\ndocument understanding. arXiv preprint arXiv:1911.02972, 2019.\n[74] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap. Compressive transformers for\nlong-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n[75] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019.\n[76] S. Rothe, S. Narayan, and A. Severyn. Leveraging pre-trained checkpoints for sequence\ngeneration tasks. arXiv preprint arXiv:1907.12461, 2019.\n[77] A. See, P. J. Liu, and C. D. Manning. Get to the point: Summarization with pointer-generator\nnetworks. arXiv preprint arXiv:1704.04368, 2017.\n[78] E. Sharma, C. Li, and L. Wang. Bigpatent: A large-scale dataset for abstractive and coherent\nsummarization. arXiv preprint arXiv:1906.03741, 2019.\n[79] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations.\narXiv preprint arXiv:1803.02155, 2018.\n[80] D. A. Spielman and S.-H. Teng. Spectral sparsiﬁcation of graphs.SIAM Journal on Computing,\n40(4):981–1025, 2011.\n[81] S. Subramanian, R. Li, J. Pilault, and C. Pal. On extractive and abstractive neural document\nsummarization with transformer language models. arXiv preprint arXiv:1909.03186, 2019.\n[82] S. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin. Adaptive attention span in transformers.\narXiv preprint arXiv:1905.07799, 2019.\n[83] C. Sun, L. Huang, and X. Qiu. Utilizing bert for aspect-based sentiment analysis via construct-\ning auxiliary sentence. arXiv preprint arXiv:1903.09588, 2019.\n[84] D. Sussman. Lecture Notes for Boston University MA 882 Spring 2017 , 2017 (accessed\nJune 3, 2020). URL http://math.bu.edu/people/sussman/MA882_2017/\n2017-01-26-Lecture-2.html .\n[85] I. Sutskever, O. Vinyals, and Q. V . Le. Sequence to sequence learning with neural networks.\nIn Advances in neural information processing systems, pages 3104–3112, 2014.\n14\n[86] A. Tampuu, Z. Bzhalava, J. Dillner, and R. Vicente. Viraminer: Deep learning on raw dna\nsequences for identifying viral genomes in human samples. PloS one, 14(9), 2019.\n[87] Z. Tang, Y . Shen, X. Ma, W. Xu, J. Yu, and W. Lu. Multi-hop reading comprehension across\ndocuments with path-based graph convolutional network. arXiv:2006.06478, 2020.\n[88] T. Thongtan and T. Phienthrakul. Sentiment classiﬁcation using document embeddings trained\nwith cosine similarity. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics: Student Research Workshop, pages 407–414, 2019.\n[89] T. H. Trinh and Q. V . Le. A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847, 2018.\n[90] R. K. Umarov and V . V . Solovyev. Recognition of prokaryotic and eukaryotic promoters using\nconvolutional deep learning neural networks. PloS one, 12(2), 2017.\n[91] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in neural information processing systems,\npages 5998–6008, 2017.\n[92] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-\ntask benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\n[93] Z. Wang, P. Ng, X. Ma, R. Nallapati, and B. Xiang. Multi-passage bert: A globally normalized\nbert model for open-domain question answering. arXiv preprint arXiv:1908.08167, 2019.\n[94] D. J. Watts and S. H. Strogatz. Collective dynamics of ‘small-world’networks.nature, 393\n(6684):440–442, 1998.\n[95] J. Welbl, P. Stenetorp, and S. Riedel. Constructing datasets for multi-hop reading compre-\nhension across documents. Transactions of the Association for Computational Linguistics, 6:\n287–302, 2018.\n[96] R. Williams. A new algorithm for optimal 2-constraint satisfaction and its implications.\nTheoretical Computer Science, 348(2-3):357–365, 2005.\n[97] S. Wiseman, S. M. Shieber, and A. M. Rush. Challenges in data-to-document generation.\narXiv preprint arXiv:1707.08052, 2017.\n[98] X. Xiao, Z.-C. Xu, W.-R. Qiu, P. Wang, H.-T. Ge, and K.-C. Chou. ipsw (2l)-pseknc: A\ntwo-layer predictor for identifying promoters and their strength by hybrid features via pseudo\nk-tuple nucleotide composition. Genomics, 111(6):1785–1793, 2019.\n[99] Y . Yang, R. Zhang, S. Singh, and J. Ma. Exploiting sequence-based features for predicting\nenhancer–promoter interactions. Bioinformatics, 33(14):i252–i260, 2017.\n[100] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning.\nHotpotqa: A dataset for diverse, explainable multi-hop question answering. arXiv preprint\narXiv:1809.09600, 2018.\n[101] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized\nautoregressive pretraining for language understanding. In Advances in neural information\nprocessing systems, pages 5754–5764, 2019.\n[102] Z. Yao, S. Cao, W. Xiao, C. Zhang, and L. Nie. Balanced sparsity for efﬁcient dnn inference\non gpu. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages\n5676–5683, 2019.\n[103] Z. Ye, Q. Guo, Q. Gan, X. Qiu, and Z. Zhang. Bp-transformer: Modelling long-range context\nvia binary partitioning. arXiv preprint arXiv:1911.04070, 2019.\n[104] C. Yun, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar. Are transformers universal\napproximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.\n15\n[105] C. Yun, Y .-W. Chang, S. Bhojanapalli, A. S. Rawat, S. J. Reddi, and S. Kumar.o(n) connections\nare expressive enough: Universal approximability of sparse transformers. In Advances in\nNeural Information Processing Systems, 2020.\n[106] H. Zhang, C.-L. Hung, M. Liu, X. Hu, and Y .-Y . Lin. Ncnet: Deep learning network models\nfor predicting function of non-coding dna. Frontiers in genetics, 10, 2019.\n[107] J. Zhang, Y . Zhao, M. Saleh, and P. J. Liu. Pegasus: Pre-training with extracted gap-sentences\nfor abstractive summarization. arXiv preprint arXiv:1912.08777, 2019.\n[108] X. Zhang, J. Zhao, and Y . LeCun. Character-level convolutional networks for text classiﬁcation.\nIn Advances in neural information processing systems, pages 649–657, 2015.\n[109] J. Zhou and O. G. Troyanskaya. Predicting effects of noncoding variants with deep learning–\nbased sequence model. Nature methods, 12(10):931–934, 2015.\n[110] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\nbooks and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In IEEE international conference on computer vision, pages 19–27, 2015.\n16\nBig Bird: Transformers for Longer Sequences – Appendix\nA Universal Approximators\nA.1 Notation\nWe begin by setting up some notations following Pérez et al. [72] to formally describe the complete\narchitecture of Transformers. A single layer of Transformer encoder is a parametric function Enc\nreceiving a sequence X = (x1,..., xn) of vectors in Rd and returning a sequence Z = (z1,..., zn)\nof the same length. Each zi is a ddimensional vector as well. We interchangeably treat the sequence\nX as a matrix in Rn×d. Enc has two components:\n1. An attention mechanism ATTN that takes in the sequence X and returns sequence (a1,..., an)\nof the same length and dimensionality; and\n2. A two layer fully connected network Othat takes in a vector in Rd and returns a vector in Rd.\nThen i-th output vector of Enc(X) is computed as follows:\nzi = O(ai) + ai where ai = ATTN (X)i + xi (1)\nNow it remains to deﬁne ATTN and Owhich we do next.\nAs described in Sec. 2, an attention mechanism is parameterized by three functions: Q,K,V :\nRd →Rm. In this paper, we assume that they are simply matrix products: Q(x) = xWQ, K(x) =\nxWK, and V(x) = xWV, where WQ,WK,WV ∈Rd×m and WV ∈Rd×d. In reality a multi-\nheaded attention is used, i.e. we have not only one, but H-sets of Query/Key/Value weight matrices,\nWh\nQ,Wh\nV,Wh\nK for h= 1,...,H . Thus, for a directed graph Dover [n], the ith output vector of the\ngeneralized attention mechanism would be\nATTN D(X)i =\nH∑\nh=1\nσ\n(\n(xiWh\nQ)(XN(i)Wh\nK)T)\n·(XN(i)Wh\nV) (AT)\nwhere N(i) denote the out-neighbors set of node iin D. In other words, the set of arcs (directed\nedges) in Drepresents the set of inner products that our attention mechanism will consider. Also\nrecall that σis a scoring function such as softmax or hardmax.\nLastly, we deﬁne the output fully connected network as follows:\nO(ai) = ReLU (aiW1 + b1) W2 ·+b2 (FF)\nHere W1 ∈Rd×q, W2 ∈Rq×d, b1 ∈Rp, and b2 ∈Rd are parameters of output network O.\nAdditional Notation We introduce a few pieces of additional notation that will be useful. Let\n[a,b)δ = {a,a + δ,...,a + ⌊b−a\nδ ⌋·δ}. Therefore, [0,1)δ = {0,δ,2δ,..., (1 −δ)}. We use 1[E] to\ndenote the indicator variable; it is 1 if the event Eoccurs and 0 otherwise.\nA.2 Proof\nIn this section, we will present the full proof of theorem 1. The proof will contain three parts. The\nﬁrst and the third part will largely follow standard techniques. The main innovation lies is in the\nsecond part.\nA.2.1 Approximate FCD by piece-wise constant functions\nFirst, we consider a suitable partition of the region(0,1) into a grid of granularity δ, which we denote\nby Gδ. We do this using Lemma 8 from Yun et al. [104], which we restate for completeness:\nLemma 1 (Lemma 8 [104]). For any givenf ∈FCD and 1 ≤p≤∞, there exists a δ >0 such that\nthere exists a piece-wise constant function ¯f with dp(f, ¯f) ≤ϵ\n3 . Concretely, ¯f is deﬁned as\n¯f(X) =\n∑\nP∈Gδ\nf(P) ·1 [∥ReLU(X−P)∥∞≤δ]\n17\nSince transformers can learn a positional embedding E, without any loss of generality, we can\nconsider the translated function. In particular, deﬁne\nE =\n\n\n0 0 0 ... 0\nδ−d δ−d δ−d ... δ −d\nδ−2d δ−2d δ−2d ... δ −2d\n...\nδ−(n−1)d δ−(n−1)d δ−(n−1)d ... δ −(n−1)d\n\n\nWe will try to approximateg(X) = f(X−E) where gis deﬁned on the domain [0,1]d×[δ−d,δ−d+\n1]d ×···× [δ−(n−1)d,δ−(n−1)d + 1]d. To do so, we will apply a suitable modiﬁcation of Lemma 1,\nwhich will consider the discretized grid\nGE\nδ := [0,1]d\nδ ×[δ−d,δ−d + 1]d\nδ ×···× [δ−(n−1)d,δ−(n−1)d + 1]d\nδ.\nTherefore, it sufﬁces to approximate a function ¯f : GE\nδ →Rn×d deﬁned as\n¯f(X) =\n∑\nP∈GE\nδ\nf(P −E) ·1 [∥ReLU(X−P)∥∞≤δ] .\nA.2.2 Contextual Mappings and Sparse Attention Mechanisms\nThroughout this section, we will assume that we are given a function that has an extra global token at\nindex 0 and all vectors have an extra dimension appended to them. The latter assumption is without\nloss of generality as we can use the Feed-Forward Network to append sparse dimensions. In particular,\nwe will associate X ∈R(n+1)×(d+1) where we write X = (x0,x1,...,x n). Although our function\nis only deﬁned for GE\nδ ⊂Rn×d, we can amend the function in a natural way by making it ignore the\nﬁrst column. To avoid excessive clutter, we will assume that the function value is evaluated on the\nlast ncolumns.\nThe main idea in this section is the use of contextual mapping to enable Transformers to compute\nany discretized function. A contextual mapping is an unique encoding of each tuple (X,xi) where\nX ∈GE\nδ , and each column xi ∈[δ−(i−1)d,δ−(i−1)d + 1)d\nδ for all i∈[n]. We restate the deﬁnition\nadapted to our setting below\nDeﬁnition 2 (Defn 3.1 [104]). (Contextual Mapping) A contextual mapping is a function mapping\nq: GE\nδ →Rn if it satisﬁes the following:\n1. For any P ∈GE\nδ , q(P) contains distinct entries.\n2. For any two P,P ′∈GE\nδ with P ̸= P′, all entries of q(P) and q(P′) are distinct.\nThe key technical novelty of the proof is computing a contextual mapping using only the sparse\nattention mechanism. We create a “selective shift” operator which only shifts entries of a vector that\nlie in a certain range. We will use this shift operator strategically to ensure that we attain a contextual\nmapping at the end of the process. The lemma below, which is based on parts of the proof of Lemma\n6 of [104], states that we can implement a suitable “selective” shift operator using a sparse attention\nmechanism.\nLemma 2. Given a function ψ : R(n+1)×(d+1) ×R2 →R(n+1)×1 and a vector u ∈Rd+1 and\na sparse attention mechanism based on the directed graph D, we can implement a selective shift\noperator that receives as input a matrix X ∈R(n+1)×(d+1) and outputs X+ ρ·ψu(X,b1,b2) where\nψu(Z; b1,b2)i =\n{(maxj∈N(i) uTZj −minj∈N(i) uTZj)e1 if b1 ≤uTZj ≤b2\n0 else.\nNote that e1 ∈Rd+1 denotes (1,0,..., 0).\nProof. Consider the function , which can be implemented by a sparse attention mechanism :\n˜ψ(X,b)i = σH\n[\n(uT ·Xi)T ·(uTXN(i) −b1T\nN(i))e(1)(uTXN(i))\n]\n18\nThis is because the Key, Query and Value functions are simply afﬁne transformations ofX.\nGiven any graph D, the above function will evaluate to the following:\n˜ψ(Z; b)i =\n{(maxj∈N(i) uTZj)e1 if uTZj >b\n(minj∈N(i) uTZj)e1 if uTZj <b\nTherefore we can say that ˜ψ(Z; bQ) −˜ψ(Z; bQ′) satisﬁes\nψ(Z; b1,b2)i =\n{(maxj∈N(i) uTZj −minj∈N(i) uTZj)e1 if b1 ≤uTZj ≤b2\n0 else\nThe following lemma, which is the heart of the proof, uses the above selective shift operators to\nconstruct contextual mappings.\nLemma 3. There exists a function gc : R(n+1)×(d+1) →R(n+1) and a unique vector u, such that\nfor all P ∈GE\nδ gc(P) := ⟨u,g(P)⟩satisﬁes the property that gc is a contextual mapping of P.\nFurthermore, gc ∈T 2,1,1\nD using a composition of sparse attention layers as long as Dcontains the\nstar graph.\nProof. Deﬁne u ∈Rd+1 = [1 ,δ−1,δ−2,...,δ −d+1,δ−nd] and let X0 = (0 ,..., 0,1). We will\nassume that ⟨xi,x0⟩= 0, by assuming that all the columns x1,...,x n are appended by 0.\nTo successfully encode the entire context in each token, we will interleave the shift operator to target\nthe original columns 1,...,n and to target the global column 0. After a column iis targeted, its inner\nproduct with uwill encode the entire context of the ﬁrst icolumns. Next, we will shift the global\ntoken to take this context into account. This can be subsequently used by the remaining columns.\nFor i ∈ {0,1,...,n }, we will use li to denote the innerproducts ⟨u,xi⟩at the beginning. For\nfi = ⟨u,xi⟩after the ith column has changed for i∈{1,...,n }and we will use fk\n0 to denote ⟨u,x0⟩\nafter the kth phase. We need to distinguish the global token further as it’s inner product will change\nin each phase. Initially, given X ∈GE\nδ , the following are true:\nδ−(i−1)d ≤⟨u,Xi⟩≤ δ−id −δ for all i∈[n]\nδ−(n+1)d = ⟨u,X0⟩\nNote that all li ordered in distinct buckets l1 <l2 <··· <ln <l0.\nWe do this in phases indexed from i∈{1,...,n }. Each phase consists of two distinct parts:\nThe low shift operation: These operation will be of the form\nX ←X+ δ−dψ(X,v −δ/2,v + δ/2)\nfor values v ∈[δ−id),δ−(i+1)d)δ. The range is chosen so that only li will be in the range and no\nother lj j ̸= iis in the range. This will shift exactly the ith column xi so that the new inner product\nfi = ⟨u,xi⟩is substantially larger than li. Furthermore, no other column of X will be affected.\nThe high shift operation: These operation will be of the form\nX ←X+ δ−nd ·ψ(X,v −δ/2,v + δ/2)\nfor values v∈[Si,Ti)δ. The range [Si,Ti)δ is chosen to only affect the column x0 (corresponding to\nthe global token) and no other column. In particular, this will shift the global token by a further δ−nd.\nLet ˜fi\n0 denote the value of ˜fi\n0 = ⟨u,x0⟩at the end of ith high operation.\nEach phase interleaves a shift operation to column iand updates the global token. After each phase,\nthe updated ith column fi = ⟨u,xi⟩will contain a unique token encoding the values of all the\nl1,...,l i. After the high update, ˜fi\n0 = ⟨u,x0⟩will contain information about the ﬁrst itokens.\nFinally, we deﬁne the following constants for all k∈{0,1,...,n }.\nTk = (δ−(n+1)d + 1)k ·δ−nd −\nk∑\nt=2\n(δ−(n+1)d + 1)k−t(2δ−nd−d + δ−nd + 1)δ−td\n−(δ−(n+1)d + 1)k−1(δ−nd−d + δ−nd)δ−d −δ−(k+1)d (UP)\n19\nSk = (δ−(n+1)d + 1)k ·δ−nd −\nk∑\nt=2\n(δ−(n+1)d + 1)k−t(2δ−nd−d + δ−nd + 1)δ−(t−1)d\n−(δ−(n+1)d + 1)k−1(δ−nd−d + δ−nd) −δ−kd (LP)\nAfter each kphases, we will maintain the following invariants:\n1. Sk < ˜fk\n0 <Tk for all k∈{0,1,...,n }.\n2. Tk−1 ≤fk <Sk\n3. The order of the inner products after kth phase is\nlk+1 <lk+2 ··· <ln <f1 <f2 <··· <fk < ˜fk\n0 .\nBase case The case k= 0, is trivial as we simply set S0 = δ−(n+1)d, T0 = δ−(n+1)·d + δ.\nThe ﬁrst nontrivial case is k= 1.\nInductive Step First, in the low shift operation is performed in the range[δ−(k−1)d,δ−kd)δ Due to\nthe invariant, we know that there exists only one column xk that is affected by this shift. In particular,\nfor column k, we will have maxj∈N(k) ⟨u,xj⟩= ⟨u,x0⟩= ˜fk−1\n0 . The minimum is lk. Thus the\nupdate will be fk = δ−d( ˜fk−1\n0 −lk) + lk. Observe that for small enough δ, fk ≥˜fk−1\n0 . Hence the\ntotal ordering, after this operation is\nlk + 1 <lk+2 ··· <ln <f1 <f2 <··· < ˜fk−1\n0 <fk (2)\nNow when we operate a higher selective shift operator in the range [Sk−1,Tk−1)δ. Since only global\ntoken’s innerproduct ˜fk−1\n0 is in this range, it will be the only column affected by the shift operator.\nThe global token operates over the entire range, we know from Eq. (2) that, fk = maxi∈[n] ⟨u,xi⟩\nand lk+1 = min i∈[n] ⟨u,xi⟩. The new value ˜fk\n0 = δ−nd ·(fk −lk+1) + ˜fk−1\n0 . Expanding and\nsimplifying we get,\n˜fk\n0 = δ−nd ·(fk −lk+1) + ˜fk−1\n0\n= δ−nd ·(δ−d( ˜fk−1\n0 −lk) + lk −lk+1) + ˜fk−1\n0\n= δ−(n+1)d ·( ˜fk−1\n0 −lk) + δ−nd(lk −lk+1) + ˜fk−1\n0\n= (δ−(n+1)d + 1) ˜fk−1\n0 −(δ−nd−d + δ−nd)lk −lk+1\nExpanding the above recursively, we get\n= (δ−(n+1)d + 1)k ·˜f0\n0 −\nk∑\nt=2\n(δ−(n+1)d + 1)k−t(2δ−nd−d + δ−nd + 1)lt\n−(δ−(n+1)d + 1)k−1(δ−nd−d + δ−nd)l1 −lk+1\nSince we know that ˜f0\n0 = δ−nd and each li <δ −id, we can substitute this to get Eq. (UP) and we\ncan get an lower-bound Eq. (LP) by using li ≥δ−(i−1)d.\nBy construction, we know that Sk ≤˜fk\n0 <Tk. For sufﬁciently small δ, observe that Sk ≤˜fk\n0 <Tk\nall are essentially the dominant term ≈O(δ−n(k+1)d−kd) and all the lower order terms do not matter.\nAs a result it is immediate to see that that fk >δ−d( ˜fk−1\n0 −lk) >Tk−1 and hence we can see that\nthe invariant 2 is also satisﬁed. Since only column kand the global token are affected, we can see\nthat invariant 3 is also satisﬁed.\nAfter niterations, ˜fn\n0 contains a unique encoding for any P ∈GE\nδ . To ensure that all tokens are\ndistinct, we will add an additional layerX = X+δ−n2dψ(X,v −δ/2,v+δ/2) for all v∈[S1,Tn)δ.\nThis ensures that for all P,P ′∈GE\nδ , each entry of q(P) and q(P′) are distinct.\n20\nThe previous lemma shows that we can compute a contextual mapping using only sparse transforms.\nWe now use the following lemma to show that we can use a contextual mapping and feed-forward\nlayers to accurately map to the desired output of the function ¯f.\nLemma 4 (Lemma 7 [ 104]). Let gc be the function in Lemma 3, we can construct a function\ngv : R(n+1)×(d+1) →R(n+1)×d composed of O(nδ−nd) feed-forward layers (with hidden dimension\nq= 1) with activations in Φ such that gv is deﬁned as gv(Z) = [gtkn\nv (Z1),...,g tkn\nv (Zn)], where for\nall j ∈{1,...,n },\ngtkn\nv (gc(L)j) = f(L)j\nA.2.3 Approximating modiﬁed Transformers by Transformers\nThe previous section assumed we used Transformers that used hardmax operator σH and activations\nfunctions belonging to the set Φ. This is without loss of generality as following lemma shows.\nLemma 5 (Lemma 9 [ 104]). For each g ∈ ¯T2,1,1 and 1 ≤ p ≤ ∞, ∃g ∈ T2,1,4 such that\ndp(g,¯g) ≤ϵ/3\nCombining the above lemma with the Lemma 3, we get our main result:\nTheorem 2. Let 1 ≤p ≤∞ and ϵ >0, there exists a transformer network g ∈T 2,1,4\nD which\nachieves a ratio of dp(f,g) ≤ϵwhere Dis the sparse graph.\nSince the sparsity graph associated with BIGBIRD contains a star network, we know that it can\nexpress any continuous function from a compact domain.\nContemporary work on Universal Approximability of Sparse Transformers We would like to\nnote that, contemporary work done by Yun et al. [105], also parallelly explored the ability of sparse\ntransformers with linear connections to capture sequence-to-sequence functions on the compact\ndomain.\n21\nB Turing Completeness\nIn this section, we will extend our results to the setting of Pérez et al.[72]. Our exposition will largely\nuse their proof structure but we will make a few changes. We repeat some of the lemmas with the\namendments to make the exposition self-contained.\nB.1 Notation\nTransformer Decoder We need both an encoder and a decoder in the transformer for simulating a\nTuring machine. We utilize the same notation used in App. A.1 for encoders. The decoder is similar to\nan encoder but with additional attention to an external pair of key-value vectors (Ke ∈Rn×m,V e ∈\nRn×d), which usually come from the encoder stack. A single layer of Transformer decoder is a\nparametric function Dec receiving a sequence Yj = (y1,..., yj) of vectors in Rd plus the external\n(Ke,V e) and returning a sequence of vectors Zj = (z1,..., zj) of the same length. Each zi is a d\ndimensional vector as well. Dec has three components, one more than Enc:\n1. An attention mechanism ATTN that takes in the sequence Yj and returns sequence (p1,..., pj)\nof the same length and dimensionality;\n2. A cross-attention mechanism CROSS ATTN that takes in the sequence (p1,..., pj) plus the exter-\nnal (Ke,V e) and returns sequence (a1,..., aj), with each ai ∈Rd; and\n3. A two layer fully connected network Othat takes in a vector in Rd and returns a vector in Rd.\nThen i-th output vector of Dec(Yj; Ke,V e) is computed as follows:\nzi = O(ai) + ai (3)\nwhere ai = CROSS ATTN (pi,Ke,V e) + pi (4)\nand pi = ATTN D(Yj)i + yi (5)\nATTN D and Oare as deﬁned in App. A.1 and it remains to deﬁne CROSS ATTN . The ith output vector\nof multi-head cross-attention attention is given by\nCROSS ATTN (Yj)i =\nH∑\nh=1\nσ\n(\n(yiWh\nQ)(K(e)Wh\nK)T\n)\n·(V (e)Wh\nV) (6)\nwhere Wh\nQ,Wh\nK,Wh\nV ∈Rd×m, Wh\nV ∈Rd×d, for all h= 1,...H heads.\nTurning Machine We will use the same setup of Turning Machine that was used by Pérez et al.[72]\n(see section B.4). Given a Turing Machine M = (Q,Σ,δ,qinit,F), we use the following notation\nq(j) : state of Turing machine M at time j.\ns(j) : symbol under the head of M at time j.\nv(j) : symbol written by M at time j.\nm(j) : head direction in the transition of M at time j.\nVector representations For a symbol s∈Σ, J sK denotes its one-hot vector representation in Q|Σ|.\nAll the transformer intermediate vectors used in our simulations have dimensiond= 2|Q|+4|Σ|+16.\nNote that we use ﬁve extra dimension as compared to Pérez et al. [72]. We follow the convention\nused in Pérez et al. [72] and write a a vector v ∈Qd arranged in four groups of values as follows\nv = [ q1,s1,x1,\nq2,s2,x2,x3,x4,x5,x6,\ns3,x7,s4,\nx8,x9,x10,x11,x12,x13,x14,x15,x16 ]\nwhere qi ∈Q|Q|, si ∈Q|Σ|, and xi ∈Q.\nB.2 Details of the Simulation\nIn this section, we give more details on the architecture of the encoder and decoder needed to\nimplement our simulation strategy.\n22\nHigh Level Overview: Given the Turing machine M, we will show that a transformer with an\nappropriate encoder and decoder TD can simulate each step ofM’s execution. Our simulation strategy\nwill mostly follow Pérez et al. [72], except we will use a sparse attention mechanism. The main idea\nis to maintain the current Turing machine state q(j) and symbol under the head s(j) as part of the\ndecoder sequence Y for all time step j so that we can always simulate the corresponding Turing\nmachine transition δ(q(j),s(j)) = (q(j),v(j),m(j)). The key difference will rise in Lemma B.4 of\nPérez et al. [72], where full attention is used to select the appropriate symbol from tape history in\none step. To accomplish the same task with sparse attention, we will exploit the associative property\nof max and break down the symbol selection over multiple steps. Thus, unlike Pérez et al. [72] one\ndecoding step of our sparse transformer TD does not correspond to one step of the Turing machine\nM. In particular, we will have two type of steps: compute step corresponding to update of M’s\nstate and intermediate steps corresponding to aggregating the max (which in turn is used for symbol\nselection). Let idenote the step of TD and g(i) denote the step of M being simulated at step iof\nthe decoder. At each decoding step we want to maintain the current Turing machine stateqg(i) and\nsymbol under the sg(i) in yi. For roughly O(\n√\ni) intermediate steps the state will remain the same,\nwhile we aggregate information about relevant past output symbols through sparse attention. To\nmaintain the same state for intermediate steps, we introduce an extra switching layer (App. B.2.3).\nFinally, at the next compute step we will make the transition to new stateqg(i)+1, new head movement\nmg(i), and new output symbol vg(i) to be written. Thereby we are able to completely simulate the\ngiven Turing machine M. As a result, we can prove the following main theorem:\nTheorem 3. There exists a sparse attention mechanism using O(n) inner products such that the\nresulting class of Transformer Networks using this sparse attention mechanism is Turing Complete.\nEncoder\nAs [72], we use the same trivial single layer encoder where resulting K(e) contains position embed-\nding and V (e) contains one-hot symbol representation.\nDecoder\nSparse Self-Attention mechanism for Decoder In this section, we will consider a particular\ninstance of the sparse graph Dat decoder. We deﬁne its edges to be given by the following relations:\n∀j ∈N+,1 ≤k≤j+ 1,\n(j(j+ 1)\n2 + k,k(k+ 1)\n2\n)\nand\n(j(j+ 1)\n2 + k,j(j+ 1)\n2 + k\n)\nif k> 1 else\n(j(j+ 1)\n2 + 1,j(j+ 1)\n2\n)\n.\nThis graph can be seen as a special case of BIGBIRD where ﬁrst type of edges are realizations\nof random and second type of edges correspond to locality. Also note that this graph satisﬁes the\nleft-to-right constraint of decoder, i.e. no node attends to a node in the future.\n1\n0\n1\n2\n1\n1\n3\n1\n2\n4\n2\n1\n5\n2\n2\n6\n2\n3\n7\n3\n1\n8\n3\n2\n9\n3\n3\n10\n3\n4\n11\n4\n1\n12\n4\n2\n13\n4\n3\n14\n4\n4\n15\n4\n5\nTransform i:\nTM Step j:\nOffset k:\nFigure 2: Mapping between transformer step and original Turing machine step.\n23\nEmbeddings and positional encodings Our construction needs a different positional encoding\nposDec : N →Qd for decoder:\nposDec(i) = [ 0 ,..., 0,\n0,..., 0,\n0,..., 0,\n1,g(i) + 1, 1\ng(i)+1 , 1\n(g(i)+1)2 ,h(i),0,0,0,0 ]\nwhere g(i) =\n⌊\n−1+√1+8i\n2\n⌋\nand h(i) = g(i+ 1) −g(i). Note that h(i) reduces to a binary indicator\nvariable 1\n{\n−1+√1+8i\n2 =\n⌊\n−1+√1+8i\n2\n⌋}\n.\nInduction Setup\nWe next show how to construct the decoder layers to produce the sequence of outputs y1,y2,... ,\nwhere yi is given by:\nyi = [ J qg(i) K,J sg(i) K,cg(i),\n0,..., 0,\n0s,0,J w(i) K,\n0,0,0,0,0,u(i)\n1 ,u(i)\n2 ,u(i)\n3 ,u(i)\n4 ]\nThat is, at step iof our sparse decoder yi, it will contain the information about the state of the turing\nmachine M at time g(i), the symbol under the head of M at time g(i), and the current location of\nhead of M at time g(i). We also have a placeholder symbol wand placeholder scalars u1,u2,u3,\nwhose role will be clear from our construction.\nWe consider as the starting vector for the decoder the vector\ny1 = [ J qinit K,J # K,0,\n0,..., 0,\n0,..., 0,\n0,..., 0 ]\nWe assume that the start head is at c(0) = 0, the initial state is q(0) = qinit, and s(0) = # as we\ninitialize from clean tape. We show the correctness of our construction by an inductive argument:\nwe describe the architecture piece by piece and at the same time will show for every r ≥0 , our\narchitecture constructs yr+1 from the previous vectors (y0,..., yr).\nThus, assume that y1,..., yr satisfy the properties stated above. Since we are using positional\nencodings, the actual input for the ﬁrst layer of the decoder is the sequence\ny1 + posDec(1), y2 + posDec(2), ..., yr + posDec(r).\nWe denote by yi the vector yi plus its positional encoding. Thus we have ∀1 ≤i≤rthat\nyi = [ J qg(i) K,J sg(i) K,cg(i),\n0,..., 0,\n0s,0,J w(i) K,\n1,g(i) + 1, 1\ng(i)+1 , 1\n(g(i)+1)2 ,h(i),u(i)\n1 ,u(i)\n2 ,u(i)\n3 ,u(i)\n4 ]\nB.2.1 Layer 1: Simulate Transition Function\nIn this layer, we use the cross-attention between encoder and decoder to access the input string and a\nfeed-forward network to simulate the transition function of M. The ﬁrst self attention in Eq. (5) is\nnot used in this layer and we just produce the identity. This identity function is achieved by setting all\nqueries, keys, values to be 0 everywhere plus the residual connection. Thus, we havep1\ni = yi.\nSince p1\ni is of the form [ ,..., ,1,g(i) + 1, ,..., ], we know by Lemma B.1 of Pérez et al.\n[72] that if we use p1\ni to attend over the encoder we obtain\nCROSS ATTN (p1\ni,Ke,V e) = [ 0 ,..., 0,\n0,..., 0,\nJ αg(i)+1 K,βg(i)+1,0s,\n0,..., 0 ]\n24\nwhere αand βare as deﬁned in Eq. (21) of [72]. Thus in Eq. (4) we ﬁnally produce the vector a1\ni\ngiven by\na1\ni = CROSS ATTN (p1\ni,Ke,V e) + p1\ni\n= [ J qg(i) K,J sg(i) K,cg(i),\n0,..., 0,\nJ αg(i)+1 K,βg(i)+1,J w(i) K,\n1,g(i) + 1, 1\ng(i)+1 , 1\n(g(i)+1)2 ,h(i),u(i)\n1 ,u(i)\n2 ,u(i)\n3 ,u(i)\n4 ]\n(7)\nAs the ﬁnal piece of the ﬁrst decoder layer we use a functionO1(·) (Eq. (3)) that satisﬁes the following\nlemma.\nLemma 6 (Lemma B.2 [72]). There exists a two-layer feed-forward network O1 : Qd →Qd such\nthat with input vector a1\ni (Eq. (7)) produces as output\nO1(a1\ni) = [ 0 ,..., 0,\nJ qg(i)+1 K,J vg(i) K,mg(i),0,0,0,0\n0,..., 0,\n0,..., 0 ]\nThat is, function O1(·) simulates transition δ(qg(i),sg(i)) to construct J qg(i)+1 K, J vg(i) K, and mg(i)\nbesides some other linear transformations.\nThus, ﬁnally the output of the ﬁrst decoder layer is\nz1\ni = O1(a1\ni) + a1\ni = [ J qg(i) K,J sg(i) K,cg(i),\nJ qg(i)+1 K,J vg(i) K,mg(i),0,0,0,0,\nJ αg(i)+1 K,βg(i)+1,J w(i) K,\n1,g(i) + 1, 1\ng(i)+1 , 1\n(g(i)+1)2 ,h(i),u(i)\n1 ,u(i)\n2 ,u(i)\n3 ,u(i)\n4 ]\nB.2.2 Layer 2: Finding Head Node\nIn this layer, we only use the feed-forward network to evaluate the next location of the head. The\nself-attention and cross-attention are set to be the identity function, so a2\ni = p2\ni = z1\ni. Recall that\ncg(i) is the cell to which M is pointing to at time g(i), and that it satisﬁes the following recursion\ncg(i)+1 = cg(i) + mg(i), which can be expanded to see that thatcg(i)+1 = m(0) + m(1) + ···+ mg(i).\nIts not difﬁcult to see that a two layer network with non-linearity can compute cg(i)+1/(g(i) + 1)and\ncg(i)/(g(i) + 1) from cg(i), mg(i), and 1/(g(i) + 1) using the relation cg(i)+1 = cg(i) + mg(i). At\nthe end of layer 2, we obtain\nz2\ni = O2(a2\ni) + a2\ni = [ J qg(i) K,J sg(i) K,cg(i),\nJ qg(i)+1 K,J vg(i) K,cg(i)+1, 1\ng(i)+1 , 1\n(g(i)+1)2 ,cg(i)+1\ng(i)+1 , cg(i)\ng(i)+1 ,\nJ αg(i)+1 K,βg(i)+1,J w(i) K,\n1,g(i) + 1, 1\ng(i)+1 , 1\n(g(i)+1)2 ,h(i),u(i)\n1 ,u(i)\n2 ,u(i)\n3 ,u(i)\n4 ]\nB.2.3 Layer 3: Distinguishing Node Type\nThis is an additional layer (not present in the work of [72]), where we propagate computations in our\nsparse graph. In particular, we will use this layer to “compute” or accumulate state in intermediate\nnodes. We make this clear below. The self-attention and cross-attention are all set to be the identity\nfunction, so a3\ni = p3\ni = z2\ni. In this layer, we only use the dense attention layers to select the newly\ncomputed states or to continue with previous states. Using idea similar to Lemma B.6 of [72], we can\nconstruct a dense network such that\nO([x,y,z,b])) =\n{[0,0,0,0] if b= 1,\n[0,z −y,−z,0] if b= 0.\nThe negatives are generated to offset results from skip connection. We utilize such network to switch\nTuring machine state and position embedding for intermediate steps to the values received from\n25\nprevious time step and do nothing for compute nodes. We use h(i) as the ﬂipping bit b. Thus, at end\nof layer 3, we obtain\nz3\ni = O3(a3\ni) + a3\ni = [ 0 ,..., 0,\nJ ˆq(i) K,J ˆv(i) K,ˆc(i), 1\ng(i)+1 , 1\n(g(i)+1)2 ,cg(i)+1\ng(i)+1 ,ˆu(i)\n4 ,\nJ ˆα(i) K,ˆβ(i),0s,\n1,ˆu(i)\n1 ,ˆu(i)\n2 ,ˆu(i)\n3 ,h(i),0,0,0,0 ]\nwhere we used h(i) for selecting old states. In particular,\n• We copy the input state and head position as is for intermediate nodes. We do not need to\ntransition to next Turing machine states in these nodes.\nˆq(i) =\n{qg(i)+1 if h(i) = 1\nqg(i) if h(i) = 0, ˆv(i) =\n{vg(i) if h(i) = 1\nw(i) if h(i) = 0, ˆc(i) =\n{cg(i)+1 if h(i) = 1\ncg(i) if h(i) = 0.\n• To preserve the symbol under the head for intermediate nodes, we copy the previous symbol\nto αlocation and set β = g(i) + 1, as the symbol at αlocation will be copied as the symbol\nunder head for next transformer step by the ﬁnal transformation layer if β = g(i) + 1. Thus, we\ncorrectly preserve the previous symbol under head as Turing machine does not transition these\nnodes. For compute nodes, things happen as usual.\nˆα(i) =\n{αg(i)+1 if h(i) = 1\nsg(i) if h(i) = 0, ˆβ(i) =\n{βg(i)+1 if h(i) = 1\ng(i) + 1 if h(i) = 0.\n• Finally for the intermediate nodes, we copy the position embedding corresponding to current\nbest symbol w, which is stored in u1,u2,u3. For compute node, we let the position embedding\ncorrespond to current Turing machine step.\nˆu(i)\n1 =\n{\ng(i) + 1 if h(i) = 1\nu(i)\n1 if h(i) = 0, ˆu(i)\n2 =\n{\n1\n(g(i)+1) if h(i) = 1\nu(i)\n2 if h(i) = 0,\nˆu(i)\n3 =\n{\n1\n(g(i)+1)2 if h(i) = 1\nu(i)\n3 if h(i) = 0, ˆu(i)\n4 =\n{\ncg(i)\ng(i)+1 if h(i) = 1\nu(i)\n4 if h(i) = 0\n.\nFor further simpliﬁcation note that g(i+ 1) = g(i) if h(i) = 0 else g(i) + 1 when h(i) = 1. With\nthis fact, we can conclude that ˆq(i) = qg(i+1) and ˆc(i) = cg(i+1). Thus, we can write,\nz3\ni = [ 0 ,..., 0,\nJ qg(i+1) K,J ˆv(i) K,cg(i+1), 1\ng(i)+1 , 1\n(g(i)+1)2 ,cg(i)+1\ng(i)+1 ,ˆu(i)\n4 ,\nJ ˆα(i) K,ˆβ(i),0s,\n1,ˆu(i)\n1 ,ˆu(i)\n2 ,ˆu(i)\n3 ,h(i),0,0,0,0 ]\nB.2.4 Layer 4: Finding next symbol on tape\nTo ﬁnd the symbol on tape under next head positioncg(i)+1, we try to ﬁnd what was written last at the\nlocation cg(i)+1. To facilitate this, following [72], we deﬁne ℓ(j) to be the last time (previous to j) in\nwhich M was pointing to position c(j), or it is j−1 if this is the ﬁrst time that M is pointing to c(j).\nRecall jis the Turing machine step counter, which is different from sparse transformer stepi. [72]\ncould utilize full attention mechanism to ﬁnd vℓ(j+1) at one go, but we have to do it over multiple\nsteps owing to our sparse attention mechanism.\nWe use similar query, key, value functions as used for full attention by [72]∀i:\nQ4(z3\ni) = [ 0 ,..., 0\n0,..., 0,\n0,..., 0,\n0,cg(i)+1\ng(i)+1 , 1\ng(i)+1 , 1\n3(g(i)+1)2 ,0,0,0,0,0 ]\n26\nK4(z3\ni) = [ 0 ,..., 0\n0,..., 0,\n0,..., 0,\n0,ˆu(i)\n2 ,ˆu(i)\n4 ,ˆu(i)\n3 ,0,0,0,0,0 ]\nV4(z3\ni) = [ 0 ,..., 0,\n0,..., 0,\n0s,0,J ˆv(i) K,\n0,0,0,0,0,ˆu(i)\n1 ,ˆu(i)\n2 ,ˆu(i)\n3 ,ˆu(i)\n4 ]\nIt is clear that the three functions are linear transformations and thus they can be deﬁned by feed-\nforward networks. Notice that the query vector is always formed using current time step position\nembedding, whereas key and value vectors are formed using copied over entries for intermediate\nnodes and using current entries only for compute node.\nPérez et al. [72] ﬁnd the desired vl(j+1) as vm(j) using full attention, where\nm(t) = arg min\nm∈{0,...,t}\nχj\nt = arg min\nm∈{0,...,t}\n|⟨Q4(z3\nj),K4(z3\nm)⟩|\nNote the minimization is only over Turing machine steps, i.e. over compute nodes in our case.\nWe show below that we can estimates m(j) by parts using sparse attention mechanism. The\nmain idea is just to notice that minimization problem minm∈{0,...,t}χj\nt can be expressed as\nmin{···min{min{χj\n0,χj\n1},χj\n2},...,χ j\nt}by the associativity property.\nBy deﬁnition of our graph D, at every intermediate node iof the form j(j+ 1)/2 + k, i.e. where\nk >0, g(i) = jand h(i) = 0, we will attend over node k(k+ 1)/2 and best till now copied from\ni−1. The node k(k+ 1)/2 is never an intermediate node as h(k(k+ 1)/2) = 1 for all kand in fact\ncorresponds to Turing machine’s stepk. This will help us select the key and value corresponding to\nmin between node k(k+ 1)/2 and i−1. In other words, at node iof the form j(j+ 1)/2 + kwe\nwould have evaluated m(k) and corresponding value selected:\nw(j(j+1)/2+k+1) = ˆvm(k−1)\nand similarly for u’s. So after going through all the intermediate nodes, ﬁnally at the next compute\nnode, i.e. when k= j+ 1, we will obtain the minimum value over all of 0,1,...,j . This implies at a\ncompute node will be able to recover ℓ(g(i) + 1) and its corresponding value as shown in Lemma\nB.4 of [72]. Then we have that p4\ni is given by\np4\ni = ATTN D(Z3\ni) + z3\ni\n= [ 0 ,..., 0,\nJ qg(i+1) K,J ˆv(i) K,cg(i+1),0,cg(i)+1\ng(i)+1 ,ˆu(i)\n4 ,\nJ ˆα(i) K,ˆβ(i),J w(i+1) K,\n1,ˆu(i)\n1 ,ˆu(i)\n2 ,ˆu(i)\n3 ,h(i),u(i+1)\n1 ,u(i+1)\n2 ,u(i+1)\n3 ,u(i+1)\n4 ]\n(8)\nThe cross-attention and feed-forward network are set to be identity, so z4\ni = a4\ni = p4\ni.\nB.2.5 Final transformation\nWe ﬁnish our construction by using the ﬁnal transformation function F(·) from the corresponding\nlemma from Pérez et al. [72], with a slight modiﬁcation.\nLemma 7 (Lemma B.5 [ 72]). There exists a function F : Qd →Qd deﬁned by a feed-forward\nnetwork such that\nF(z4\nr) = [ J qg(r+1) K,J sg(r+1)) K,cg(r+1),\n0,..., 0,\n0s,0,J w(r+1) K,\n0,0,0,0,0,u(r+1)\n1 ,u(r+1)\n2 ,u(r+1)\n3 ,u(r+1)\n4 ]\n= yr+1\nThe modiﬁcation is to let w,u1,u2,u3 to pass through. This yields the desired input to transformer at\nnext time step for both intermediate and compute node, thereby concluding our induction.\n27\nC Limitations\nFinally, we show that sparse attention mechanisms can not universally replace dense attention\nmechanisms, i.e. there is no free lunch. We demonstrate a natural task which can be solved by the\nfull attention mechanism in O(1)-layers. However, under standard complexity theoretic assumptions,\nwe show that this problem will require ˜Ω(n)-layers for any sparse attention layers with ˜O(n) edges\n(not just BIGBIRD ). (We use the standard notation ˜Ω(n) to hide the dependence on poly-logarithmic\nfactors. )\nWe consider the simple problem of ﬁnding the furthest vector for each vector in the given sequence\nof length nand dimension d ∈Ω(log2 n). The assumption on the dimension is mild , as in many\nsituations the dimension d= 768 is actually comparable to the number of n.\nTask 1. Given n unit vectors {u1,...,u n}, each in Rd where d = Θ(log 2 n), compute\nf(u1,...,u n) →(u1∗,...,u n∗) where for a ﬁxed j ∈[n], we deﬁne j∗= arg maxk∥uk −uj∥2\n2.\nFinding vectors that are furthest apart boils down to minimizing inner product search in case of unit\nvectors. For a full-attention mechanism with appropriate query and keys, this task is very easy as we\ncan evaluate all pair-wise inner products.\nThe impossibility for sparse-attention follows from hardness results stemming from Orthogonal\nVector Conjecture (OVC) [2, 1, 96, 7], which is a widely used assumption in ﬁne-grained complexity.\nInformally, it states that one cannot determine if the minimum inner product amongnBoolean vectors\nis 0 in subquadratic time.\nConjecture 1 (Orthogonal Vectors Conjecture). For every ϵ> 0, there is a c≥1 such that given n\nBoolean vectors in ddimension, cannot determine if there is a pair of orthogonal vectors in O(n2−ϵ)\ntime on instances with d≥clog n.\nUsing conjecture 1, we show a reduction to show that a transformer g∈T H=O(d),m=O(d),q=O(d)\nD for\nany sparse directed graph Dwhich completes Task 1 must require a superlinear number of layers.\nProposition 2. There exists a single layer full-attention network g ∈T H=1,m=2d,q=0 that can\nevaluate Task 1, i.e. g(u1,...,u n) = [ u1∗,...,u n∗], but for any sparse-attention network in\nTH=O(d),m=O(d),q=O(d)\nD with graph Dhaving ˜O(n) edges (i.e. inner product evaluations), would\nrequire ˜Ω(n1−o(1)) layers.\nProof. We will break this proof into two parts:\nPart 1: The full attention mechanism can solve the problem inO(1) layer We begin by provid-\ning an explicit construction of a single layer full self-attention that can evaluate Task 1.\nStep 1 We embed each ui in the input into R2d as follows:\nxi := E(ui) = [ui; 0] (9)\nStep 2 Construct query, key, value functions as follows:\nQ([a; b]) = −a\nK([a; b]) = a\nV([a; b]) = [0; a]\n(10)\nThen Attn(Q(xi),K(X),V (X) = [0; uarg maxj⟨−ui,uj⟩]. Then,\nai = Attn(Q(xi),K(X),V (X)) + xi = [ui; uarg maxj⟨−ui,uj⟩] = [ui; ui∗] (11)\nStep 3 Let O(ai) = 0, then the output zi = [ui; ui∗] as desired.\nTo complete the argument, observe that it now only takesO(n) inner products to check if there is a\npair of orthogonal vectors as we need only compare ⟨ui,ui∗⟩.\n28\nPart 2: Every Sparse Attention Mechanism will need˜Ω(n1−ϵ) layers We prove by contradiction\nthat it is impossible to solve Task 1 by any g ∈T H=O(d),m=O(d),q=O(d)\nD sparse-attention graph D\nwith ˜O(n) edges.\nSuppose we can solve Task 1 using a network g∈T H=O(d),m=O(d),q=O(d)\nD that has llayers. Recall\nthat all the computation we do in one layer is:\nai = ATTN D(Q(xi),K(XN(i)),V (XN(i)) + xi\nxi = O(ai) + ai\n(12)\nwhere AttnD is deﬁned in eq. (AT).\nThus, total computation per layer is ˜O(nd3) and consequently ˜O(nld3) for the whole network\nconsisting of llayers.\nWe can use the result of Task 1 to solve the orthogonal vector (OV) problem (deﬁned in Conjecture 1)\nin linear time. So in total, we will be able to solve any instance of OV in ˜O(nld3) time.\nNow if l= O(n1−ϵ) for any ϵ> 0 and d= Θ(log2 n), then it appears that we are able to solve OV\nin ˜O(n2−ϵ) which contradicts Conjecture 1. Therefore, we need at least ˜Ω(n1−o(1)) layers.\n29\nD Implementation details\nWe optimize the code for modern hardware. Hardware accelerators like GPUs and TPUs truly shine\non coalesced memory operations which load blocks of contiguous bytes at once. Thus, its not very\nefﬁcient to have small sporadic look-ups caused by a sliding window or random element queries. We\nalleviate this by “blockifying” the lookups.\nGPU/TPU and Sparsity Ideally, if the adjacency matrixAdescribed in Sec. 2 is sparse, one would\nhope this would be sufﬁcient to speed up the implementation. Unfortunately, it is well known [33, 102],\nthat such sparse multiplications cannot be efﬁciently implemented in GPUs. GPUs have thousands\nof cores performing operations in parallel. Thus, we cannot efﬁciently perform the sparse matrix\nmultiplication mentioned in section Sec. 2.\nAs a result we propose to ﬁrst blockify the attention pattern i.e. we pack sets of query and keys\ntogether and then deﬁne attention on these blocks. It is easier to explain this process using the example\nshown in Fig. 3. Suppose, there are 12 query and 12 key vectors to attend to. Using a block size\nof 2, we split the query matrix into 12/2 = 6 blocks and similarly the key matrix into 12/2 = 6\nblocks. Then the three different building components of BIGBIRD are deﬁned on the block matrix. In\nparticular the three different components are:\n1. Random attention: Each query block attends to rrandom key blocks. In Fig. 3a, r= 1 with\nblock size 2. This implies that each query block of size 2 randomly attends to a key block of\nsize 2.\n2. Window local attention: While creating the block, we ensure that the number of query blocks\nand the number of key blocks are the same. This helps us in deﬁning the block window\nattention. Every query block with index jattends to key block with index j−(w−1)/2 to\nj+ (w−1)/2, including key block j. In Fig. 3b, w = 3 with block size 2. It means that\neach query block j(size 2 queries) attends to key block j−1,j,j + 1.\n3. Global attention: Global attention remains the same as deﬁned in Sec. 2, but we compute it\nin terms of blocks. In Fig. 3c, g= 1 with block size 2. For BIGBIRD -ITC this implies that\none query and key block, attend to everyone.\nThe resulting overall attention matrix is shown in Fig. 3d. Unfortunately, simply trying to compute\nthis attention score as multiplying arbitrary pairs of query and key vectors would require use of gather\noperation, which is inefﬁcient. Upon closer examination of window and global attention, we observe\nthat we can compute these attention scores without using a gather operation.\nRecall, full dense attention scores can be calculated by simple matrix product of query and key matrix\nwith a cost of O(n2d), as illustrated in Fig. 4a. Now note that if we blockify the query and key matrix\nand multiply, then with only O(nbd) cost we will obtain the block diagonal portion of the attention\nscore, as depicted in Fig. 4b. To elaborate this lets assume that Q,K ∈Rn×d are the query and key\nmatrix corresponding to ntokens such that Qi. = xiWQ and Ki. = xiWK. We reshape n×dquery\n(a) Random Attention\n (b) Window Attention\n (c) Global Attention\n (d) BIGBIRD\nFigure 3: Building blocks of the block-attention mechanism used in BIGBIRD with block size =\n2. This implies the attention matrix is split into blocks of size 2 ×2. All the previous BIGBIRD\nparameters work on each block as a unit. White color indicates absence of attention. (a) random\nattention with r= 1, (b) sliding window attention with w= 3 (c) global attention with g= 1. (d) the\ncombined BIGBIRD model.\n30\nKey\nQuery\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\nd\n21\n22\n23\n24\n17\n18\n19\n20\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nb\nd \nb\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\n21\n22\n23\n24\n(a) Full all pair attention can be obtained by direct matrix multiplication between the query\nand key matrix. Groupings just shown for guidance.\nKeyQuery\nd \nb\nb\nd\nA   B   C   D\nE   F   G   H\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\n21\n22\n23\n24\n(b) Block diagonal attention can be computed by “blockifying” the query and key matrix\nI   J   K   L\nKey\nQuery\nA   B   C   D\nE   F   G   H\nU   V   X   Y\n1\n2\n3\n4\n5\n6\n7\n8\nQ   R   S   T\nU   V   X   Y\nA   B   C   D\nQ   R   S   T\nE   F   G   H\nI   J   K   L\nA   B   C   D\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\n21\n22\n23\n24\n(c) Window local attention obtained by “blockifying” the query/key matrix, copying key matrix,\nand rolling the resulting key tensor (Obtaining rolled key-block tensor is illustrated in detail\nin Fig. 5). This ensures that every query attends to at least one block and at most two blocks of\nkeys of size bon each side.\nI   J   K   L\nKey\nQuery\nA   B   C   D\nE   F   G   H\nU   V   X   Y\n1\n2\n3\n4\n5\n6\n7\n8\nQ   R   S   T\nU   V   X   Y\nA   B   C   D\nQ   R   S   T\nE   F   G   H\nI   J   K   L\nA   B   C   D\nA   B   C   D\nE   F   G   H\nU   V   X   Y\nRandom \nedges\nLocality\nedges\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\n21\n22\n23\n24\n(d) Window + Random attention obtained by following the procedure above along with\ngathering some random key blocks.\nFigure 4: Idea behind fast sparse attention computation in BIGBIRD .\n31\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\n3 Copies of Key\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\nA   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\nU   V   X   Y A   B   C   D E   F   G   H I   J   K   L M   N   O   P Q   R   S   T\nRolled Key\nQ   R   S   TA   B   C   D E   F   G   H I   J   K   L M   N   O   P U   V   X   Y\nA   B   C   DE   F   G   H I   J   K   L M   N   O   P Q   R   S   T U   V   X   Y\nI   J   K   L\nKey\nA   B   C   D\nE   F   G   H\nU   V   X   Y\nQ   R   S   T\nU   V   X   Y\nA   B   C   D\nQ   R   S   T\nE   F   G   H\nI   J   K   L\nA   B   C   D\nRoll Block\nFigure 5: Construction of rolled key-block tensor. Make wcopies of the key matrix. Index the copies\nas −(w−1)/2 ≤j ≤(w−1)/2. Roll jth copy by jblocks. Positive roll means circular shift entries\nleft and likewise for negative roll corresponds to right shift. Finally, reshape by grouping the blocks\nalong a new axis to obtain the key-blocked tensor. For illustration purposew= 3 is chosen.\nmatrix, Q, and key matrix, K, along the sequence length to obtain ⌈n/b⌉×b×dtensors Q′and K′\nrespectively. Now we multiply the two tensors as\nAjst =\n∑\nu\nQ′\njsuK′\njtu, j = 0,1,..., ⌈n/b⌉ (13)\nThe resulting Atensor of size ⌈n/b⌋×b×bcan be reshaped to correspond to the block diagonal\nportion of the full attention pattern. Now to extend the attention from block diagonal to a window, i.e.\nwhere query block with index jattends to key block with index j−(w−1)/2 to j+ (w−1)/2, we\nmake wcopies of the reshaped key tensor K′. We “roll” each copy of key-block tensor incrementally\nalong the ﬁrst axis of length⌈n/b⌉as illustrated in Fig. 5. Multiplying thesewrolled key-block tensors\nwith the query-block tensor would yield the desired window attention scores (Fig. 4c). Likewise the\nglobal component, we can always include the ﬁrst gblocks from key tensor corresponding to the\nglobal tokens. Finally, for the random attention, which is very small (r= 3 for all of our experiments),\nwe resort to using gather ops (Fig. 4d). Also note by design, each query block attends to exactly r\nrandom blocks.\nThus, the result of all the three components is basically a compact dense tensor K′′of size ⌈n/b⌉×\n(g+ w+ r)b×das shown in Fig. 6. Computing the ﬁnal attention score then just boils down to a\ndense tensor multiplication, at which TPU/GPU are very efﬁcient. Speciﬁcally, we need to multiply\nQ′(size: ⌈n/b⌉×b×d) and K′′(size: ⌈n/b⌉×(g+ w+ r)b×d) with a cost of O(n(g+ w+ r)bd)\nto yield the desired attention score tensor of size ⌈n/b⌉×b×(g+ w+ r)b, which can be reshaped\nto obtain all the attention scores according to the BigBird pattern.\nK1 K2 K3 K4 K5 K6\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\n=\nFixed Roll Key \nMatrix Left Roll Key \nMatrix Right Gatther\nQ2\nQ3\nQ4\nQ5\nQ6\nd\nb=2\nd\nb=2\nK1\nK1\nK1\nK1\nK1\nK6\nK2\nK3\nK4\nK5\nK2\nK3\nK4\nK5\nK6\nK3\nK4\nK5\nK6\nK2\nK5\nK5\nK6\nK2\nK3\nFigure 6: Overview of BIGBIRD attention computation. Structured block sparsity helps in compactly\npacking our operations of sparse attention, thereby making our method efﬁcient on GPU/TPU. On the\nleft, we depict the transformed dense query and key tensors. The query tensor is obtained by simply\nblocking and reshaping while the ﬁnal key tensor by concatenating three transformations: The ﬁrst\ngreen columns, corresponding to global attention, is ﬁxed. The middle blue columns correspond to\nwindow local attention and can be obtained by appropriately rolling as illustrated in Fig. 5. For the\nﬁnal orange columns, corresponding to random attentions, we need to use computationally inefﬁcient\ngather operation. Dense multiplication between the query and key tensors efﬁciently calculates the\nsparse attention pattern (except the ﬁrst row-block, which is computed by direct multiplication), using\nthe ideas illustrated in Fig. 4. The resultant matrix on the right is same as that shown in Fig. 3d.\n32\nE NLP experiments details\nE.1 MLM Pretraining\nWe use four publicly available datasets Books [110], CC-News [34], Stories [89] and Wikipedia to\npretrain BIGBIRD . We borrow the sentencepiece vocabulary as RoBERTa (which is in turn borrowed\nfrom GPT2). We split any document longer than4096 into multiple documents and we join documents\nthat were much smaller than 4096. Following the original BERT training, we mask 15% of tokens in\nthese four datasets, and train to predict the mask. We warm start from RoBERTa’s checkpoint. We\ntrain two different models: BIGBIRD -ITC -base and BIGBIRD -ETC -base. The hyper-parameters for\nthese two models are given in Tab. 8. In all experiments we use a learning rate warmup over the ﬁrst\n10,000 steps, and linear decay of the learning rate.\nSimilar to the norm, we trained a large version of model as well, which has 24 layers with 16 heads\nand hidden dimension of 1024. Following the observation from RoBERTa, we pretrain on a larger\nbatch size of 2048 for this size. For BIGBIRD -ITC the block length was kept same as base size, but\nfor BIGBIRD -ETC the block length was almost doubled to 169. All the remaining parameters were\nthe same.\nParameter B IGBIRD -ITC BIGBIRD -ETC\nBlock length, b 64 84\n# of global token, g 2 ×b 256\nWindow length, w 3 ×b 3 ×b\n# of random token, r 3 ×b 0\nMax. sequence length 4096 4096\n# of heads 12 12\n# of hidden layers 12 12\nHidden layer size 768 768\nBatch size 256 256\nLoss MLM MLM\nActivation layer gelu gelu\nDropout prob 0.1 0 .1\nAttention dropout prob 0.1 0 .1\nOptimizer Adam Adam\nLearning rate 10−4 10−4\nCompute resources 8 ×8 TPUv3 8 ×8 TPUv3\nTable 8: Hyperparameters for the two BIGBIRD base models for MLM.\nE.2 Question Answering\nThe detailed statistics of the four datasets used are given in Tab. 11. All the hyperparameters for\nBIGBIRD , used for creating Tab. 2 are shown in Tab. 12 and those submitted to get Tab. 3 are shown\nin Tab. 13. We use two types of regularization in training:\n• We used a variant of contrastive predictive coding [70] as a dual encoder model.\n• We use position embedding for ITC and relative position encoding [79] for ETC .\nNext, we will mention the dataset/task speciﬁc part of the model.\nDataset # tokens Avg. doc len.\nBooks [110] 1.0B 37K\nCC-News [34] 7.4B 561\nStories [89] 7.7B 8.2K\nWikipedia 3.1B 592\nTable 9: Dataset used for pre training.\nModel Base Large\nRoBERTa (sqln: 512) 1.846 1.496\nLongformer (sqln: 4096) 1.705 1.358\nBIGBIRD -ITC (sqln: 4096) 1.678 1.456\nBIGBIRD -ETC (sqln: 4096) 1.611 1.274\nTable 10: MLM performance on held-out set.\n33\nInstances Instance Length\nDataset Training Dev Median Max\nHotpotQA-distractor [100] 90447 7405 1227 3560\nNatural Questions [52] 307373 7830 3258 77962\nTriviaQA [41] 61888 7993 4900 32755\nWikiHop [95] 43738 5129 1541 20337\nTable 11: Question Answering Datasets\nParameter HotpotQA NaturalQ TriviaQA WikiHop\nGlobal token location ITC ETC ITC ETC ITC ETC ITC ETC\n# of global token, g 128 256 128 230 128 320 128 430\nWindow length, w 192 252 192 252 192 252 192 252\n# of random token, r 192 0 192 0 192 0 192 0\nMax. sequence length 4096 4096 4096 4096 4096 4096 4096 4096\n# of heads 12 12 12 12 12 12 12 12\n# of hidden layers 12 12 12 12 12 12 12 12\nHidden layer size 768 768 768 768 768 768 768 768\nBatch size 32 32 128 128 32 32 64 64\nLoss cross-entropy cross-entropy cross-entropy cross-entropy\ngolden spans golden spans noisy spans [18] ans choices\nCompute resources 4 ×2 TPUv3 4 ×8 TPUv3 4 ×2 TPUv3 4 ×4 TPUv3\nTable 12: Hyperparameters of base BIGBIRD model used for Question Answering i.e. the numbers\nreported in Tab. 2\nHotpotQA The data consists of each question with multiple evidence paragraphs. We ﬁltered 16\nQA where the answer was not in the given evidences. For BIGBIRD -ITC , we use ﬁrst 128 global\ntokens. For BIGBIRD -ETC , we have one global token for each question token, one for each evidence\nparagraph, and one for each sentence within the paragraph, for a maximum of 256 global token. We\nuse a dense layer on the output corresponding to global token of the evidence paragraph to predict\nwhether its a supporting fact with a threshold over the output logits. The answer type (yes/no/span) is\npredicted with a single dense layer from the global CLS token. For span based answers, the spans are\npredicted with dense layers on the sequence with the distance between start and end positions to be\nno more than 30 words. The spans are ranked by sum of start and end logits.\nNatural Questions Here also the data consists of question with supporting evidence, but in form of\na single, potentially long, document and not multiple paragraphs. We largely follow the setup of [5].\nFor documents, that are longer than 4096, a sliding window approach is used with stride of 2048. We\nuse CLS token at the beginning, followed by the question followed by a separator token followed by\nthe document as input. For BIGBIRD -ITC , we make the ﬁrst 128 tokens as global. For BIGBIRD -ETC ,\nwe make a global token for CLS, question, and one token for each of the paragraphs. We train four\npredictors at the ﬁnal layer to predict long answer start, long answer end, short answer start and short\nanswer end respectively. Instead of independently predicting the start and end of answers we ﬁrst\npredict the start and then predict the best end location beyond the start. For short answer, we limit the\ndistance between start and end positions to be no more than 38 words. The answer type (null, yes, no,\nshort, long) is predicted from CLS token output embedding. When the logit for a yes/no answer is\nhigher than the logits for short, long or null answer, we replace the short answer with a corresponding\nyes/no text.\nTriviaQA The data consists of question-answer pairs with Wikipedia articles as the “noisy” sup-\nporting evidence. We call them noisy because the given Wikipedia articles may or may not contain\nthe answer. Moreover, the answer entities is not annotated to appropriate span in the article, rather\nall occurrences found using fuzzy string matching are listed. We use CLS token at the beginning,\nfollowed by the question followed by a separator token followed by the document as input. For\nBIGBIRD -ITC , we make the ﬁrst 128 tokens as global. For BIGBIRD -ETC , we make a global token\nfor CLS, question, and one token for each sentence up to a maximum of 320 global tokens. Given the\n34\nParameter HotpotQA NaturalQ TriviaQA WikiHop\nGlobal token location ETC ETC ETC ETC\n# of global token, g 256 230 320 430\nWindow length, w 507 507 507 507\n# of random token, r 0 0 0 0\nMax. sequence length 4096 4096 4096 4096\n# of heads 16 16 16 16\n# of hidden layers 24 24 24 24\nHidden layer size 1024 1024 1024 1024\nBatch size 32 64 32 64\nLoss cross-entropy cross-entropy cross-entropy cross-entropy\nNum epochs {5,9} { 3,5} { 3,5} { 5,10}\nOptimizer Adam Adam Adam LAMB\nLearning rate 3 ×10−5 {5,10}×10−5 {3,5}×10−5 {2,5}×10−5\nCompute resources 4 ×4 TPUv3 4 ×8 TPUv3 4 ×4 TPUv3 4 ×8 TPUv3\nTable 13: Hyperparameters of large BIGBIRD model for Question Answering submitted for test\ni.e. the numbers reported in Tab. 3\nnoisy nature of answer span, we follow Clark and Gardner [18] for training. We use a dense layer on\nthe sequence to predict the answer span for each article independently, with the distance between\nstart and end positions to be no more than 16 words. For each article the span with maximum start\nlogit + end logit is chosen. Then we normalize over all the documents associated with that question.\nWikiHop For each question in WikiHop, we are given upto 79 candidates, and 63 supporting\nparagraphs. In our BIGBIRD -ITC model, following Beltagy et al. [8], we concatenate the answer and\nthe question with special tokens, [q] Question [/q] [ans] Ans1 [/ans] ... [ans]\nAnsN [/ans] along with the context. As the start of the text, always contains questions followed\nby answers, we make the ﬁrst 128 token attend globally. In BIGBIRD -ETC model, we do not need\nto insert special [ans], [/ans] etc. as we design global tokens appropriately. Along with global\ntokens for question, we have one per candidate answer up to a maximum of 430. Further, we linked\nanswer tokens to their mentions using relative position label. Lastly, we use a dense layer that takes in\nthe output vector corresponding to a candidate answer, and predicts a score for the current candidate\nto be the correct answer. We apply this dense layer to each candidate independently and the candidate\nwith the best score is picked as our ﬁnal answer.\nIt is worthwhile to note that explicitly designed attention connection in ETC works slightly better, the\nrandom connection based ITC is pretty competative.\nE.3 Relationship to Contemporary Work\nLongformer Child et al. [16] introduced localized sliding window to reduce computation. A\nmore recent version, which includes localized sliding windows and global tokens was introduced\nindependently by Longofrmer[8]. Although BIGBIRD contains additional random tokens, there are\nalso differences in the way global and local tokens are realized. In particular even when there is no\nrandom token, as used to get SoTA in question answering, there are two key differences between\nLongformer and BIGBIRD -etc (see [4]):\n1. We use global-local attention with relative position encodings enables it to better handle\nstructured inputs\n2. Unlike Longformer, we train the global tokens using CPC loss and learn their use during\nﬁnetuning.\nE.4 Classiﬁcation\nWe try two types of classiﬁcation task.\nDocument classiﬁcation We experiment on datasets of different lengths and contents, as listed in\nTab. 15. In particular, we look at sentiment analysis (IMDb [64] and Yelp-5 [108]) task and topic\n35\nParameter IMDb Arxiv Patents Hyperpartisan Yelp-5\nBatch size 64 64 64 32 32\nLearning rate 1 ×10−5 3 ×10−5 5 ×10−5 5 ×10−6 2 ×10−5\nNum epochs 40 10 3 15 2\nTPUv3 slice 4 ×4 4 ×4 4 ×4 4 ×2 4 ×8\n# of heads 12 16\n# of hidden layers 12 24\nHidden layer size 768 1024\nBlock length, b 64\nGlobal token location ITC\n# of global token, g 2 ×b\nWindow length, w 3 ×b\n# of random token, r 3 ×b\nMax. sequence length 4096\nV ocab size 50358\nActivation layer gelu\nDropout prob 0.1\nAttention dropout prob 0.1\nLoss cross-entropy\nOptimizer Adam\nTable 14: Hyperparameters for document classiﬁcation.\nModel IMDb [64] Yelp-5 [108] Arxiv [35] Patents [53] Hyperpartisan [47]\n# Examples 25000 650000 30043 1890093 645\n# Classes 2 5 11 663 2\nExcess fraction 0.14 0.04 1.00 0.90 0.53\nSoTA [88] 97.4 [3] 73.28 [69] 87.96 [69] 69.01 [40] 90.6\nRoBERTa 95.0 ±0.2 71.75 87.42 67.07 87.8 ±0.8\nBIGBIRD 95.2 ±0.2 72.16 92.31 69.30 92.2 ±1.7\nTable 15: Classiﬁcation results. We report the F1 micro-averaged score for all datasets. Experiments\non smaller IMDb and Hyperpartisan datasets are repeated 5 times and the average performance is\npresented along with standard deviation.\nassignment (Arxiv [35], Patents [53], and Hyperpartisan [47]) task. Following BERT, we used one\nlayer with cross entropy loss on top of the ﬁrst [CLS] token from the BIGBIRD encoder consuming\n4096 tokens. We report the results of document classiﬁcation experiments in Tab. 15. We compare\nagainst state-of-the-art (SoTA) methods for each dataset and plain RoBERTa model with 512 tokens\ntruncation. In all experiments we use a learning rate warmup over the ﬁrst 10% steps, and linear decay\nof the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better\nquantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length\nat which the document are often truncated. We see that gains of using BIGBIRD are more signiﬁcant\nwhen we have longer documents and fewer training examples. For instance, using base sized model,\nBIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nSystem MNLI-(m/mm) QQP QNLI SST-2 CoLA STS-B MRPC RTE\n392k 363k 108k 67k 8.5k 5.7k 3.5k 2.5k\nBERT 84.6/83.4 71.2 90.5 93.5 52.1 85.8 88.9 66.4\nXLNet 86.8/- 91.4 91.7 94.7 60.2 89.5 88.2 74.0\nRoBERTa 87.6/- 91.9 92.8 94.8 63.6 91.2 90.2 78.7\nBIGBIRD 87.5/87.3 88.6 92.2 94.6 58.5 87.8 91.5 75.0\nTable 16: GLUE Dev results on base sized models. Number of training examples is reported below\neach task. MCC score is reported for CoLA, F1 score is reported for MRPC, Spearman correlation is\nreported for STS-B, and accuracy scores are reported for the other tasks.\n36\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the\nimprovement over SoTA (which is not BERT based) is not signiﬁcant. Note that this performance\ngain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present\ndetailed results in App. E.4 which show competitive performance.\nGLUE The General Language Understanding Evaluation (GLUE) benchmark [ 92], test lan-\nguage models on 8 different natural language understanding tasks. We used the same training\nparameters as mentioned in https://github.com/pytorch/fairseq/blob/master/\nexamples/roberta/README.glue.md. Our model parameters are b= 64,g = 2 ×b,w =\n3 ×b,r = 3 ×b( we used the BIGBIRD -ITC base model pretrained on MLM task). We compare the\nperformance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We ﬁnd that even on task\nthat have a much smaller context, our performance is competitive to full attention models.\nE.5 Summarization\nAs discussed in Sec. 4.1, given the small length of output sequence, we used sparseBIGBIRD attention\nonly for encoder, while keeping the full attention for decoder. The number of hidden layers, number\nof heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed\nin Tab. 17. We summarize our result in Tab. 20. In all experiments, we use a learning rate warmup\nover the ﬁrst 10,000 steps, and square root decay of the learning rate.\nParameter Base: B IGBIRD -RoBERTa Large: B IGBIRD -Pegasus\nBlock length, b 64 64\nGlobal token location ITC ITC\n# of global token, g 2 ×b 2 ×b\nWindow length, w 3 ×b 3 ×b\n# of random token, r 3 ×b 3 ×b\nMax. encoder sequence length\nBBC-XSUM: 1024 1024\nCNN/DM: 2048 2048\nOthers: 3072 3072\nMax. decoder sequence length\nBBC-XSUM: 64 64\nCNN/DM: 128 128\nOthers: 256 256\nBeam size 5 5\nLength penalty BBC-XSUM: 0.7 0.7\nOthers: 0.8 0.8\n# of heads 12 16\n# of hidden layers 12 16\nHidden layer size 768 1024\nBatch size 128 128\nLoss teacher forced teacher forced\ncross-entropy cross-entropy\nActivation layer gelu gelu\nDropout prob 0.1 0 .1\nAttention dropout prob 0.1 0 .1\nOptimizer Adam Adafactor\nLearning rate 1 ×10−5 1 ×10−4\nCompute resources 4 ×4 TPUv3 4 ×8 TPUv3\nTable 17: Encoder hyperparameters for Summarization. We use full attention in decoder\nInstances Input Length Output Length\nDataset Training Dev Test Median 90%-ile Median 90%-ile\nArxiv [20] 203037 6436 6440 6151 14405 171 352\nPubMed [20] 119924 6633 6658 2715 6101 212 318\nBigPatent [78] 1207222 67068 67072 3082 7693 123 197\nTable 18: Statistics of datasets used for summarization.\n37\nInstances Input Length Output Length\nDataset Training Dev Test Median 90%-ile Median 90%-ile\nBBC XSum [67] 204044 11332 11334 359 920 25 32\nCNN/DailyMail [36] 287113 13368 11490 777 1439 59 93\nTable 19: Shorter summarization dataset statistics.\nModel\nBBC XSum CNN/DailyMail\nR-1 R-2 R-L R1 R2 R-L\nPrior Art\nLead 16.30 1 .61 11 .95 39 .60 17 .70 36 .20\nPtGen [77] 29.70 9 .21 23 .24 39 .53 17 .28 36 .38\nConvS2S [28] 31.89 11 .54 25 .75 − − −\nMMN [48] 32.00 12 .10 26 .00 − − −\nBottom-Up [29] − − − 41.22 18 .68 38 .34\nTransLM [45] − − − 39.65 17 .74 36 .85\nUniLM [23] − − − 43.47 20 .30 40 .63\nExtr-Abst-BERT [62] 38.81 16.50 31.27 42.13 19.60 39.18\nBART [56] 45.14 22.27 37.25 44.16 21.28 40.90\nBase\nTransformer [91] 29.61 9.47 23.17 34.89 13.13 32.12\n+ RoBERTa [76] 39.92 17.33 32.63 39.44 18.69 36.80\n+ Pegasus [107] 39.79 16.58 31.70 41.79 18.81 38.93\nBIGBIRD -RoBERTa 39.52 17.22 32.30 39.25 18.46 36.61\nLarge\nPegasus (Reported) [107] 47.60 24.83 39.64 44.16 21.56 41.30\nPegasus (Re-eval) 47.37 24.31 39.23 44.15 21.56 41.05\nBIGBIRD -Pegasus 47.12 24.05 38.80 43.84 21.11 40.74\nTable 20: Summarization ROUGE score for shorter documents.\nFollowing success of several recent works [76, 63], we warm start our encoder-decoder BIGBIRD\ntransformer model with pretrained weights and the weights between encoder and decoder are shared.\nIn particular, the query/key/value matrix of self-attention and all the feedforward layers are shared\nbetween encoder and decoder. The only variable that is initialized randomly is the encoder-decoder\nattention. For base sized model, we utilize our MLM pretrained model on 4096 sequence length\nfrom App. E.1, which is in turn initialized using the public RoBERTa checkpoint. For the large size\nmodel, we lift weight from the state-of-the-art Pegasus model [107], which is pretrained using an\nobjective designed for summarization task.\nTo check if sparse attention causes signiﬁcant degradation as compared to full attention, we further\nexperiment on two shorter but popular datasets, where full attention can be used without signiﬁcantly\ntruncating the document. The statistics of these two datasets are in Tab. 19. We see that our perfor-\nmance is competitive, which shows that sparse attention can achieve similar performance to a full\nattention models.\n38\nF Genomics experiments details\nIn this section we provide details of the experimental setup for BIGBIRD on genomics data.\nF.1 Pretraining\nWe try to keep the experimental setup as close to a typical NLP pipeline. In this regard, we take\nhuman reference GRCh377 and convert it into documents D. Each document d∈D is a sequence of\nsentences, where each sentence is a sequence of fragments of DNA. We construct the documents as\nfollows:\n1. Start with empty document set D= ∅.\n2. For each chromosome C, repeat the following procedure 10 times.\n(a) Pick uniformly at random a starting point qbetween base pairs 0 and 5000 from the 5’ end.\n(b) Repeat until q >|C|\ni. Pick uniformly at random sa number between 50 and 100 to denote number of sentences\nper document.\nii. Constructs a document dcontaining ssentences using consecutive base pairs (bps). The\nlength of each sentence is chosen uniformly at random between 500-1000. Thus the\nresulting document has 25,000 - 100,000 bps.\niii. D= D⋃d\niv. q= q+ |d|\nBy this procedure we end-up with approximately 450Kdocuments.\nNext we run sentencepiece [ 50] tokenization on the resulting documents. In particular, using 5\ncharacters as the building blocks (four for bases - A, T, C, G and one for missing symbol N), we\nconstruct a byte pair encoding table of size 32k, with each token representing 8.78 base pairs on\naverage.\nUsing the above constructed documents, we construct a dataset for two pretraining tasks following\nDevlin et al. [22]:\n• Masked Language Model (MLM): In order to train a deep bidirectional representation, BERT\ntraining introduces the MLM task, where we simply mask out 15% of the input tokens at random,\nand then predict those masked tokens. We can simply replace such masked out of the tokens with\na [MASK] placeholder, but it leads to a distribution mis-match for downstream tasks which will\nnot have such placeholders. To mitigate with this issue, out of the 15% of the tokens selected for\nmasking:\n– 80% of the tokens are actually replaced with the token [MASK].\n– 10% of the time tokens are replaced with a random token.\n– 10% of the time tokens are left unchanged, but are still predicted at output.\nWe run this entire sequence through the BIGBIRD transformer encoder and then predict corre-\nsponding to the masked positions, based on the context provided by the other non-masked tokens\nin the sequence.\n• Next Sentence Prediction (NSP): In order to understand relationship between two sequences,\nBERT training introduces the NSP task, where we predict if a given pair of sequences are\ncontiguous or not. During training the model gets as input pairs of sequences separated by [SEP]\ntoken along with a [CLS] token at the start. Overall the input pattern is: [CLS] sequence A [SEP]\nsequence B [SEP]. For 50% of the time the second sequence comes from true sequence after the\nﬁrst one. Remaining 50% of the time it is a a random sequence from the full dataset. The model\nis then required to predict this relationship using the output corresponding to the [CLS] token,\nwhich is fed into a simple binary classiﬁcation layer.\n7https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39\n39\nT G G G C T A A C A A G C A A A T G A T C T G T\nCreate Document & Sentence\nT G G G C T A A C A A G C\nA A A T G A T C T G T\nSentencepiece\nT G G G C T A A C A A G C\nA A A T G A T C T G T\n...\nT G G G C T A A C A A G C\nA A A T G A T C T G T\nMasking\n...\n...\nFigure 7: Visual description of how the masked language modeling data was generated from raw DNA\ndataset. The raw DNA sequences of GRCh37, where split at random positions to create documents\nwith 50-100 sentences where each sentence was 500-1000 base pairs (bps). Thus each document had\na continuous strand of 25000-100,000 bps of DNA. This process was repeated 10 times to create 10\nsets of document for each chromosome of GRCH37. The resulting set of documents was then passed\nthrough Sentencepiece that created tokens of average 8bp. For pretraining we used masked language\nmodel and masked 10% of the tokens and trained on predicting the masked tokens.\n0 1 2 3 4 5\nSteps 1e5\n16\n18\n20\n22\n24\n26\n28MLM Accuracy\n  512\n1024\n4096\nFigure 8: BIGBIRD accuracy\nwith context length.\nThe sequence of steps is visually elaborated in Fig. 9. The model is\ntrained with both MLM and NSP together. Training hyperparame-\nter is provided in second columns of Tab. 21. In all experiments we\nuse a learning rate warmup over the ﬁrst 10,000 steps, and linear\ndecay of the learning rate.\nWe additionally performed a simple ablation study to validate the\nhypothesis, that similar to NLP, having a larger context improves\nperformance. We use MLM task described above to test how BIG-\nBIRD performed with sequences of different length. Accuracy on\nMLM task with increasing sequence length is shown in Fig. 8. Not\nonly longer context improves ﬁnal accuracy, it also leads to faster\nlearning, as we have now more opportunities for masking.\nF.2 Promoter Region Prediction\nThe promoter region plays an important role in transcription initiation and thus its recognition is\nan important area of interest in the ﬁeld of bioinformatics. Following Oubounyt et al. [71], we use\ndatasets from Eukaryotic Promoter Database (EPDnew) [24], which contains 29,597 promoter region\nin the human genome. Around the transcription start site (TSS), we extract a sequence of 8000 bp\n(-5000 +3000 bp) from the human reference genome GRCh37. Since EPDnew uses newer GRCh38,\nwe convert to GRCh37 coordinates using LiftOver [44].\n40\nContext\n5000 bp\nT G G T A A C A G C A A T G C T G T... ...\nPredict Epigenetic Features of\n200 bp non-coding region\nContext\n3000 bp\n...\nG G T A A C A G C A A T G C T G... ......\nSentencepiece\nG T A A C A G...G ... C A A T G ...\nFigure 9: Visual description of the DNA segment from which we predict the chromatin proﬁle for a\ngiven non-coding region of the raw DNA sequences of GRCh37. We take 8000 bps of DNA before\nand after the given non-coding region as context. The complete fragment of DNA including the\ncontext on both side, is then tokenized to form our input sequence of tokens. The task is to predict\n919 chromatin proﬁle including 690 transcription factors (TF) binding proﬁles for 160 different TFs,\n125 DNase I sensitivity (DHS) proﬁles and 104 histone-mark (HM) proﬁles\nFollowing Oubounyt et al. [71] for each promoter region example, a negative example (non-promoter\nsequences) with the same size of the positive one is constructed as follow: The positive sequence is\ndivided into 20 subsequences. Then, 12 subsequences are picked randomly and substituted randomly.\nThe remaining 8 subsequences are conserved. This process is illustrated in Figure 1 of [71]. Applying\nthis process to the positive set results in new non-promoter sequences with conserved parts from\npromoter sequences (the unchanged subsequences, 8 subsequences out of 20). These parameters\nenable generating a negative set that has 32 and 40% of its sequences containing conserved portions\nof promoter sequences.\nWe preﬁx and append each example with [CLS] and [SEP] token respectively. The output correspond-\ning to the [CLS] token fromBIGBIRD transformer encoder is fed to a simple binary classiﬁcation layer.\nWe ﬁne-tune the pretrained BIGBIRD from App. F.1 using hyper-parameters described in Tab. 21.\nWe note that high performance is not surprising due to the overlap in the nature of negative example\ngeneration and MLM pretraining.\nF.3 Chromatin-Proﬁle Prediction\nThe ﬁrst step of sequence-based algorithmic framework for predicting non-coding effects is to build a\nmodel to predict, large scale chromatic proﬁle [109]. In this paper, we use the dataset provided in\nParameter Pretraining Promoter Region Chromatin-Proﬁle\nBlock length, b 64 64 64\nGlobal token location ITC ITC ITC\n# of global token, g 2 ×b 2 ×b 2 ×b\nWindow length, w 3 ×b 3 ×b 3 ×b\n# of random token, r 3 ×b 3 ×b 3 ×b\nMax. Sequence Length 4096 4096 4096\n# of heads 12 12 12\n# of hidden layers 12 12 12\nHidden layer size 768 768 768\nBatch Size 256 256 256\nV ocab Size 32000 32000 32000\nLoss MLM+NSP BCE 919 x +ve upweighted\nBCE\nDropout prob 0.1 0 .1 0 .1\nOptimizer Adam Adam Adam\nLearning rate 0.0001 0 .0001 0 .0001\n# of steps 1000000 711 500000\nCompute Resources 8 ×8 TPUv3 8 ×8 TPUv3 8 ×8 TPUv3\nTable 21: Table of hyperparameters for Computational biology.\n41\nZhou and Troyanskaya [109]8, to train BIGBIRD to predict the chromatic proﬁle.\nEach training sample consists of a 8,000-bp sequence from the human GRCh37 reference genome\ncentered on each 200-bp bin and is paired with a label vector for 919 chromatin features. As\nbefore, we preﬁx and append each example with [CLS] and [SEP] token respectively. The output\ncorresponding to the [CLS] token from BIGBIRD transformer encoder is fed to a linear layer with\n919 heads. Thus we jointly predict the 919 independent binary classiﬁcation problems. We ﬁne-tune\nthe pretrained BIGBIRD from App. F.1 using hyper-parameters described in Tab. 21. As the data is\nhighly imbalanced data (way more negative examples than positive examples), we upweighted loss\nfunction for positive examples by factor of 8.\nWe used training and testing split provided by Zhou and Troyanskaya [109] using chromosomes and\nstrictly non-overlapping. Chromosome 8 and 9 were excluded from training to test chromatin feature\nprediction performances, and the rest of the autosomes were used for training and validation. 4,000\nsamples on chromosome 7 spanning the genomic coordinates 30,508,751–35,296,850 were used as\nthe validation set.\nAs the predicted probability for each sequence in DeepSea Zhou and Troyanskaya[109] was computed\nas the ensemble average of the probability predictions for the forward and complementary sequence\npairs, we also predict using an ensemble of two BIGBIRD model trained independently.\n8http://deepsea.princeton.edu/media/code/deepsea_train_bundle.v0.9.tar.\ngz\n42",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.786307156085968
    },
    {
      "name": "Automatic summarization",
      "score": 0.7485000491142273
    },
    {
      "name": "Transformer",
      "score": 0.6428923606872559
    },
    {
      "name": "Quadratic equation",
      "score": 0.5515317320823669
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5301074981689453
    },
    {
      "name": "Sequence (biology)",
      "score": 0.47461333870887756
    },
    {
      "name": "Dependency (UML)",
      "score": 0.47186583280563354
    },
    {
      "name": "CLs upper limits",
      "score": 0.46828314661979675
    },
    {
      "name": "Turing",
      "score": 0.43839868903160095
    },
    {
      "name": "Theoretical computer science",
      "score": 0.42620357871055603
    },
    {
      "name": "Language model",
      "score": 0.4213795065879822
    },
    {
      "name": "Machine learning",
      "score": 0.3317640721797943
    },
    {
      "name": "Mathematics",
      "score": 0.1323426365852356
    },
    {
      "name": "Programming language",
      "score": 0.08153173327445984
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Optometry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 273
}