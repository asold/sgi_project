{
  "title": "Cell Graph Transformer for Nuclei Classification",
  "url": "https://openalex.org/W4393159246",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2125966030",
      "name": "Wei Lou",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen",
        "Shenzhen Research Institute of Big Data"
      ]
    },
    {
      "id": "https://openalex.org/A2107520989",
      "name": "Guanbin Li",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2113629113",
      "name": "Xiang Wan",
      "affiliations": [
        "Shenzhen Research Institute of Big Data"
      ]
    },
    {
      "id": "https://openalex.org/A2100243883",
      "name": "Haofeng Li",
      "affiliations": [
        "Shenzhen Research Institute of Big Data"
      ]
    },
    {
      "id": "https://openalex.org/A2125966030",
      "name": "Wei Lou",
      "affiliations": [
        "Shenzhen Research Institute of Big Data",
        "Chinese University of Hong Kong, Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2107520989",
      "name": "Guanbin Li",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3202604077",
    "https://openalex.org/W6775507996",
    "https://openalex.org/W3133673271",
    "https://openalex.org/W2949983948",
    "https://openalex.org/W6798254027",
    "https://openalex.org/W3217112505",
    "https://openalex.org/W2161634498",
    "https://openalex.org/W4210874399",
    "https://openalex.org/W2918342466",
    "https://openalex.org/W2953567336",
    "https://openalex.org/W6727285366",
    "https://openalex.org/W6811173682",
    "https://openalex.org/W4280600117",
    "https://openalex.org/W4387210952",
    "https://openalex.org/W4387928876",
    "https://openalex.org/W3016553397",
    "https://openalex.org/W6839395506",
    "https://openalex.org/W6750297457",
    "https://openalex.org/W3169575312",
    "https://openalex.org/W3048101421",
    "https://openalex.org/W6794020371",
    "https://openalex.org/W2784814091",
    "https://openalex.org/W6792014136",
    "https://openalex.org/W6730903564",
    "https://openalex.org/W6678050348",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6810938606",
    "https://openalex.org/W4308770382",
    "https://openalex.org/W4387928934",
    "https://openalex.org/W2893521891",
    "https://openalex.org/W2978508283",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W2406134447",
    "https://openalex.org/W2404139044",
    "https://openalex.org/W4226208698",
    "https://openalex.org/W4387210975",
    "https://openalex.org/W4221146106",
    "https://openalex.org/W3102737931",
    "https://openalex.org/W4281257868",
    "https://openalex.org/W2971692138",
    "https://openalex.org/W2988856610",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W2962804068",
    "https://openalex.org/W3013627541",
    "https://openalex.org/W4312815172",
    "https://openalex.org/W4292212642",
    "https://openalex.org/W2521492299",
    "https://openalex.org/W4394744747",
    "https://openalex.org/W4385346076",
    "https://openalex.org/W3210550184",
    "https://openalex.org/W2964051675",
    "https://openalex.org/W3035649237",
    "https://openalex.org/W2288892845",
    "https://openalex.org/W4287123803",
    "https://openalex.org/W4385825417",
    "https://openalex.org/W4321649827",
    "https://openalex.org/W2798040152",
    "https://openalex.org/W4288419263",
    "https://openalex.org/W2120908365",
    "https://openalex.org/W3153131045",
    "https://openalex.org/W3209367260",
    "https://openalex.org/W3184439416",
    "https://openalex.org/W2974825848",
    "https://openalex.org/W4280494974",
    "https://openalex.org/W3205483686",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W3154595990",
    "https://openalex.org/W4284896159",
    "https://openalex.org/W3211394146",
    "https://openalex.org/W3034534840",
    "https://openalex.org/W3201663056",
    "https://openalex.org/W4287829537",
    "https://openalex.org/W3010855850",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Nuclei classification is a critical step in computer-aided diagnosis with histopathology images. In the past, various methods have employed graph neural networks (GNN) to analyze cell graphs that model inter-cell relationships by considering nuclei as vertices. However, they are limited by the GNN mechanism that only passes messages among local nodes via fixed edges. To address the issue, we develop a cell graph transformer (CGT) that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes. Nevertheless, training the transformer with a cell graph presents another challenge. Poorly initialized features can lead to noisy self-attention scores and inferior convergence, particularly when processing the cell graphs with numerous connections. Thus, we further propose a novel topology-aware pretraining method that leverages a graph convolutional network (GCN) to learn a feature extractor. The pre-trained features may suppress unreasonable correlations and hence ease the finetuning of CGT. Experimental results suggest that the proposed cell graph transformer with topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance. Code and models are available at https://github.com/lhaof/CGT",
  "full_text": "Cell Graph Transformer for Nuclei Classification\nWei Lou1, 2, Guanbin Li3, 4, Xiang Wan1, Haofeng Li1*\n1Shenzhen Research Institute of Big Data, Shenzhen, China\n2The Chinese University of Hong Kong, Shenzhen, China\n3School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\n4GuangDong Province Key Laboratory of Information Security Technology\nweilou@link.cuhk.edu.cn, liguanbin@cuhk.edu.cn, {wanxiang, lhaof}@sribd.cn\nAbstract\nNuclei classification is a critical step in computer-aided diag-\nnosis with histopathology images. In the past, various meth-\nods have employed graph neural networks (GNN) to ana-\nlyze cell graphs that model inter-cell relationships by con-\nsidering nuclei as vertices. However, they are limited by the\nGNN mechanism that only passes messages among local\nnodes via fixed edges. To address the issue, we develop a\ncell graph transformer (CGT) that treats nodes and edges as\ninput tokens to enable learnable adjacency and information\nexchange among all nodes. Nevertheless, training the trans-\nformer with a cell graph presents another challenge. Poorly\ninitialized features can lead to noisy self-attention scores and\ninferior convergence, particularly when processing the cell\ngraphs with numerous connections. Thus, we further pro-\npose a novel topology-aware pretraining method that lever-\nages a graph convolutional network (GCN) to learn a fea-\nture extractor. The pre-trained features may suppress unrea-\nsonable correlations and hence ease the finetuning of CGT.\nExperimental results suggest that the proposed cell graph\ntransformer with topology-aware pretraining significantly im-\nproves the nuclei classification results, and achieves the state-\nof-the-art performance. Code and models are available at\nhttps://github.com/lhaof/CGT\nIntroduction\nIdentifying cell types for histopathology image has emerged\nas a fundamental task in computational pathology (Krithiga\nand Geetha 2021; Amgad et al. 2022; Huang et al. 2023a).\nBy effectively classifying nuclei, medical professionals gain\ncrucial insights into the intricate cellular structures, which\nhelps make decisions related to disease diagnosis (Lagree\net al. 2021) and prognosis (Liu et al. 2022a). Thus, in this\npaper, we focus on inferring the types of cell nuclei in a\nhistopathology image.\nDeep learning (DL) based methods (Graham et al. 2019;\nAbousamra et al. 2021; Doan et al. 2022) have been widely\napplied to the nuclei classification task. Most of them em-\nploy convolutional neural networks (CNNs) to compute\npixel-wise local features and fail to consider the macrostruc-\nture of nuclei distribution (Anand, Gadiya, and Sethi 2020;\nJaved et al. 2020). Another group of methods exploits the\n*Haofeng Li is the corresponding author.\nCopyright Â© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nGCN\nTopology-aware Pre-training\nTransformer\nCell Graph Transformer Training\nWeights\nFeature \nExtractor\nFeature \nExtractor\nFigure 1: The idea of Topoloyg-aware Pre-training for Cell\nGraph Transformer. Simple initialization for the cell graph\ntransformer fails to converge due to a large amount of unrea-\nsonable connections. The topology-aware pre-training can\nreduce the initial noise in features and boost the representa-\ntion ability of the cell graph transformer.\ncell graph of a histopathology image, and has been studied\nfor decades (Schnorrenberg et al. 1996; Hassan et al. 2022).\nA Cell Graph is a set of vertices and edges, where a vertex is\na cell or nucleus and an edge is built between two neighbor-\ning cells. Recently, graph convolutional networks (GCNs)\nhave been used to learn embeddings with cell graphs (Zhou\net al. 2019; Zhao et al. 2020; Anklin et al. 2021; Hassan et al.\n2022). These GCN-based solutions update the embedding of\na nucleus by aggregating its adjacent nuclei. However, these\nGCN methods aggregate features along non-learnable edge\nconnections that are fixed after building a cell graph, which\nlimits the model capacity.\nTo overcome the issue, we propose a Cell Graph Trans-\nformer (CGT) for the nuclei classification task, inspired by\n(Kreuzer et al. 2021; Ying et al. 2021; Kim et al. 2022). The\nproposed CGT takes both nuclei and edges as input tokens to\ncompute any pairwise correlations among all tokens, which\ncan capture long-range contexts in a more flexible way. CGT\nis a portable model that can identify cell types, based on\nany form of binary segmentation or detection results of nu-\nclei. These results could be obtained by existing methods or\nmanual labeling. In the CGT framework, we first compute\nthe centroid coordinates of nuclei from the segmentation/de-\ntection result, then determine if two cells are connected by\nan edge according to their spatial distance, and build up the\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3873\ntopology of a cell graph. To obtain visual features for nodes\nand edges, we develop a U-Net that outputs a pixel-wise fea-\nture map with an input image. To embed adjacency into the\nCGT, we propose a cell-graph tokenization method that in-\ntegrates the visual and position embeddings of nodes and\nedges with two kinds of markers, which indicates the type\nand neighborhood of a token. Afterward, the CGT encoder,\nwhich is built of standard transformer layers, performs node-\nlevel classification to output the cell types.\nImportantly, we observe that simple initialization of the\nfeature extractor fails to train our proposed CGT model.\nIt may be due to that the pairwise attentions can be com-\nputed between less correlated tokens, and result in noises\nespecially when the representations are not well initialized.\nTherefore, we propose a novel topology-aware pretraining\nstrategy that replaces our proposed CGT with a GCN to\nguide the learning of a feature extractor on the same nu-\nclei classification task. The guiding GCN model only merges\nthe embeddings of adjacent nodes defined by the fixed\ncell graph, which means less unreasonable correlations and\nmakes convergence easier. The pre-trained feature extractor\nis supposed to synthesize structure-guided representations\nthat benefit the training of the proposed CGT framework.\nOverall, our contributions have three folds:\nâ€¢ A nuclei classification framework, Cell Graph Trans-\nformer, which benefits from non-local contexts by com-\nputing pairwise attentions among all nodes and edges;\nâ€¢ A topology-aware pretraining strategy that provides\ntopology-guided feature learning for reducing the initial\nnoise of the cell graph transformer;\nâ€¢ The proposed cell graph transformer significantly sur-\npasses the state-of-the-art methods and our pretraining\nstrategy also brings an improvement to the baseline.\nRelated Work\nNuclei Classification for Histopathology Images. Early\nsolutions for nuclei classification involved the extraction\nof manually defined features which are fed into classifiers\nlike SVM or AdaBoost (Liu, Mundra, and Rajapakse 2011;\nSharma et al. 2015). However, the handcrafted features limit\nthe representation capabilities of nuclei entities. Recently,\nthe nuclei classification models usually infer cell types based\non the CNNs for nucleus segmentation (Zhang et al. 2017;\nBasha et al. 2018; Lou et al. 2022, 2023b; Ma et al. 2023;\nYu et al. 2023) or nucleus centroid detection (Abousamra\net al. 2021; Huang et al. 2023b). Graham et al. (2019) pro-\npose a CNN of three branches, predicting nucleus types\nfor the segmented nucleus instances. Doan et al. (2022) in-\ncorporated a weight map prediction technique to highlight\nchallenging pixel samples for improved classification. How-\never, these CNN-based approaches are limited by their local\npixel-wise receptive field, and fail to capture instance-level\ncontexts among cell nuclei. Therefore, we use a cell graph\nstructure that describes the global relationship among nu-\ncleus instances.\nGraph Models in Computational Pathology. Graph mod-\nels have been used in computational pathology for decades.\nDemir, Gultekin, and Yener (2005) builds a graph by con-\nsidering nuclei as nodes and binary connections as edges.\nA perceptron is utilized for the detection of inflammation in\nbrain biopsy. Recently graph convolution networks (GCNs)\nhave been used as learnable models for the graphs derived\nfrom histopathology images (Lou et al. 2023a). Some ap-\nproaches (Zhou et al. 2019; Javed et al. 2020; Pati et al.\n2022) classify whole slide images by defining nodes as nu-\ncleus instances, superpixels, or tissue patches. In these meth-\nods, the node embeddings are hand-crafted or learned fea-\ntures from pre-trained CNN models. NCCD (Hassan et al.\n2022) has been proposed for GNN-based nucleus classifi-\ncation recently. However, the GNN-based methods aggre-\ngate features along non-learnable edges, which are fixed\nand limit the model capacity. Thus, we develop a cell graph\ntransformer with learnable node connections and capture the\nlong-range contexts more flexibly.\nTransformers for Graph. Transformer models have\nemerged as crucial components in various domains such as\nneural language processing (Vaswani et al. 2017) and com-\nputer vision (Liu et al. 2021). Several existing approaches\nhave incorporated transformers to handle graph structures\nin different manners. First, some methods employ Trans-\nformer layers as auxiliary modules within graph neural net-\nworks (Wu et al. 2021; Lin, Wang, and Liu 2021). Second,\nattention matrices are introduced into the message-passing\nmechanism (Dwivedi and Bresson 2020; Zheng et al. 2022).\nHowever, these approaches are constrained by the non-\nlearnable edge connections in the graph structure and may\nsuffer from the issue of excessive smoothing caused by\nthe message-passing mechanism (Li, Han, and Wu 2018;\nOono and Suzuki 2020). Recently, researchers have made\nprogress in graph representation learning by employing pure\nTransformer architectures with learnable positional encod-\nings (Kreuzer et al. 2021) or by utilizing sparse higher-order\nTransformers (Kim, Oh, and Hong 2021). In this paper, we\npropose a GCN-guided pretraining strategy that adapts vi-\nsual features to a graph topology for better training a cell\ngraph transformer on the nuclei classification task.\nMethodology\nIn this section, we introduce the proposed Cell Graph\nTransformer framework, the proposed topology-aware pre-\ntraining strategy, and the training-inference scheme.\nCell Graph Transformer\nWe propose a cell graph transformer (CGT) framework to\nidentify the category of each nucleus in histopathology im-\nages. To focus on the classification part, the CGT simply\nadopts an existing model to provide binary (foreground v.s.\nbackground) segmentation/detection results of nuclei. Since\nCGT performs nuclei classification based on the position co-\nordinates of nuclei, it can adapt to various forms of segmen-\ntation/detection results. As Figure 2 shows, our proposed\ncell graph transformer has three parts: Cell Graph Construc-\ntion, Cell Graph Tokenization (CGToken), and Transformer\nEncoder.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3874\nCell Graph Construction\n1 2 3\n1 0 1 0\n2 1 0 1\n3 0 1 0\nâ€¦\nâ€¦\nSegmentation\n/ Detection\nğ’•ğ’Š\nğ’—=[ğ’›ğ’Š\nğ’—, ğ†ğ’Š, ğ’ğ’Š, ğ’ğ’Š, ğ‘´ğ’—]\nğ’•ğ’…\nğ’† =[ ğ’›ğ’…\nğ’† , ğ’ğ’Š, ğ’ğ’‹, ğ‘´ğ’†]Edge Token:\nNode Token:\nâ€¦\nâ€¦\nNode Visual Feature ğ’›ğ’—\nEdge Visual Feature ğ’›ğ’†\nLink Marker ğ‘€ğ‘™\nâ€¦\nğ‘€ğ‘£ ğ‘€ğ‘’ \nToken Marker\nFeature Extractor\nCell Graph Tokenization\nğ’•ğŸ\nğ’—\nğ’•ğŸ\nğ’—\nğ’•ğ’ğ’—\nğ’•ğŸ\nğ’†\nğ’•ğŸ\nğ’†\nğ’•ğ‘«\nğ’† â€¦ â€¦\nTransformer \nlayer\nâ€¦â€¦\nâ€¦\nâ€¦\nPredictions of Nuclei \nType\nCell Graph\nTransformer Encoder\nAdjacency Matrix\nğ’ Ã— ğŸ\nğ’›ğŸ\nğ’— ğ’›ğ’ğ’—ğ’›ğ’—ğŸ‘ğ’›ğŸ\nğ’—\nğ’›ğŸ\nğ’† ğ’›ğŸ\nğ’† ğ’›ğŸ‘\nğ’† ğ’›ğ‘«\nğ’†\nğ’ğŸ ğ’ğŸ ğ’ğŸ‘ ğ’ğ’\nTransformer \nlayer\nTransformer \nlayer\nğ†ğŸ ğ†ğŸ ğ†ğŸ‘ ğ†ğ’â€¦\nPositional Embedding ğ†\nFigure 2: Overview of the Cell Graph Transformer (CGT) framework. The framework includes the construction and tokenization\nof cell graphs as well as a transformer encoder. Tokens are the input feature vectors of the transformer encoder. Adjacency is\nembedded into the tokens via link and token markers.\nCell Graph Construction. Building a cell graph requires\ntwo inputs: 1. Centroid coordinates of nuclei computed by\nthe binary detector/segmentation tool; 2. A feature map f\nis obtained by the feature extractor in Figure 2. Given the\ncentroids of n nuclei in an image, an undirected cell graph\nG = (V, E) can be constructed by connecting each nucleus\ncentroid to its nearest k nuclei. Therefore, the cell graph\ncontains n nodes V = {v1, ..., vn} and D(= kn) edges\nE = {e1, ..., eD}. The connections of nodes are represented\nby a binary adjacency matrix A âˆˆ RnÃ—n, in which Ai,j=1 if\nnode vi and vj are connected.\nTo obtain the visual embeddings of nodes and edges, our\napproach incorporates a U-Net architecture that leverages\nan existing convolutional neural network (CNN) (Guo et al.\n2023) as the encoder, and a feature pyramid network (FPN)\n(Lin et al. 2017) as the decoder. The U-Net is initialized via\na novel pre-training strategy, which is introduced in the sub-\nsequent subsection. Given a histopathology image with di-\nmensions H Ã— W, the U-Net takes the image as input and\ngenerates a feature map f of size H\n4 Ã— W\n4 Ã— C from its\nsecond-to-last layer. To describe a nucleus node vi, we first\nsample the visual embedding zv\ni âˆˆ R1Ã—C located at the cen-\ntroid coordinate of the nucleus from f. The vector zv\ni can\nbe calculated through bilinear interpolation by using feature\nvectors at the four nearest integer coordinates onf. Besides,\nwe inject spatial positional information by computing a po-\nsitional embedding vector Ïi âˆˆ R1Ã—C using the Sinusoidal\nPosition Encoding method (Vaswani et al. 2017). The node\nfeature is defined as the concatenation of zv\ni and Ïi. For an\nedge, the feature vector of its middle point is sampled from\nthe feature map f using bilinear interpolation. The sampled\nvector is viewed as the edge visual embedding ze\nd.\nCell Graph Tokenization. Tokenization is to convert raw\ndata into meaningful numerical representations called To-\nkens that can be well encoded by transformers. We introduce\nthe Cell Graph Tokenization approach (CGToken), which\naims to translate the constructed cell graph into a set of to-\nkens that standard transformer models can effectively pro-\ncess. It is straightforward to regard the node and edge em-\nbeddings as (n+D) independent inputs, but it overlooks the\nstructural information contained within the graph. To exploit\nthe topology structure, we utilize Link Markers and Token\nMarkers. A cell graph transformer framework has two to-\nken markers denoted as Mv, Me, which are two 1 Ã— C vec-\ntors and the learnable parameters of the framework. One is\nfor node tokens, while the other is for edges. The two token\nmarkers are tuned in the training and fixed after training.\nThe link markers Ml = {m1, m2, ..., mn} âˆˆRnÃ—c are\northonormal vectors that imply the adjacency of each token.\nIf vi and vj are linked by an edge, then[mi, mj][mi, mi]T =\n1 and [mi, mj][mj, mj]T = 1, otherwise the dot production\nis 0. This mechanism makes the transformer assign more at-\ntention to the nodes connected in the cell graph. The link\nmarkers are calculated by the Laplacian eigendecomposi-\ntion (Dwivedi et al. 2020) of the adjacency matrix A:\nL = I âˆ’ Î˜âˆ’1\n2 AÎ˜âˆ’1\n2 = (Ml)T Î±Ml, (1)\nwhere Î˜ is the degree matrix of the graph, I is an identity\nmatrix. Î± and Ml are the eigenvalues and eigenvectors, re-\nspectively. Then, we further define each node/edge token (tv\ni\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3875\nâ€¦\nPixel-level Auxiliary Losses\nInstance-level Nuclei\nClassification Loss\nGraph Convolutional NetworkCell GraphFeature Extractor\nFigure 3: The proposed Topology-Aware Pretraining Strategy. It trains the feature extractor with an instance-level nuclei clas-\nsification loss and two pixel-level auxiliary losses. The model weights of the trained feature extractor are then used to initialize\nthe feature extractor in training the proposed cell graph transformer.\n/ te\nd) using node/edge visual feature, link and token makers:\ntv\ni = [Ïƒ1([zv\ni , Ïi]), Ïƒ3([mi, mi]), Mv] , iâˆˆ {1,Â·Â·Â· , n},\nte\nd = [Ïƒ2(ze\nd), Ïƒ3([mi, mj]), Me] , dâˆˆ {1,Â·Â·Â· , D},\n(2)\nwhere zv\ni and Ïi are the visual and positional embeddings\nof the ith node. ze\nd is the visual feature of the dth edge that\nconnects the ith and jth nodes. [Â·] denotes the concatenation\noperator. Ïƒ1, Ïƒ2, Ïƒ3 are linear projection layers that convert\nthe dimension of their inputs to the same dimensionC. After\ncomputing Eq. (2), we obtain n node tokens and D edge\ntokens. Each token is a vector of size 1 Ã— 3C.\nInference and Training Scheme\nGiven the (n + D) tokens, a linear projection layer converts\neach of these tokens into a C-dimensional vector separately.\nThe resulting (n + D) vectors are then fed into the trans-\nformer encoder. We employ the standard transformer archi-\ntecture (Vaswani et al. 2017) for each transformer layer in\nthe encoder. Each layer is composed of a stack of multi-head\nself-attention layers and a feed-forward network. To classify\nthe categories of n nodes, we only select the first n features\nO âˆˆ RnÃ—C from the output of the CGT encoder and send\nthese features into the classification layer. The classification\nlayer is built of a fully-connected (FC) layer and a Softmax\nfunction: P = Softmax (Ïƒ(O)).\nBefore training the CGT encoder, we pretrain the fea-\nture extractor using our proposed topology-aware strategy\ndescribed in the next subsection. In the training stage, the\nfeature extractor is also fine-tuned with the transformer en-\ncoder synchronously. The classification loss of each nucleus\nnode has a cross-entropy term and a focal loss term, and is\ndefined as:\nL(P, y) = âˆ’\nBX\nb=1\nyb log Pb âˆ’\nBX\nb=1\nÏ„b(1 âˆ’ Pb)Î³yb log Pb,\n(3)\nwhere Î³ is a hyper-parameter set to 2, B is the number of\ncategories, y is the true label, P is the prediction and Ï„b is\nthe loss weight computed as the reciprocal of the proportion\nof the bth class in the training set.\nTopology-Aware Pretraining Strategy\nIn the proposed CGT, we find that the initial visual features\nof nodes/edges play a crucial role in model training. At the\nearly training stage, if visual features are not well initialized,\ncomputing correlations of all pairs of node/edge tokens may\nproduce unreliable attention, bringing noise into the CGT\ntraining. Graph convolutional network (GCNs) and Trans-\nformers have their own strengths in modeling graph data.\nGCNs excel at capturing local structural information and\npropagating it across the graph. We consider that the mes-\nsage passing in GCNs is locally guided by fixed edges and\nis more robust at the start of training. Thus, we propose a\npretraining strategy that employs a GCN to help learn the\nfeature extractor in advance. After that, the cell graph trans-\nformer is initialized with the GCN-guided representations,\nwhich can help the transformer converge faster and improve\nthe final classification performance.\nAs shown in Fig. 3, the proposed pretraining strategy\ntrains a feature extractor with an instance-level nuclei clas-\nsification loss and two auxiliary pixel-level losses (including\nthe Dice and Cross-entropy losses). The auxiliary losses are\nfor semantic segmentation, and the segmentation result is\npredicted from the last layer of the feature extractor. Note\nthat these two segmentation losses aim at learning the fea-\nture extractor instead of producing nucleus masks. The seg-\nmentation mask of nuclei is obtained via an existing segmen-\ntation tool as shown in Fig. 2. To compute the instance-level\nloss in Fig. 3, the second-last layer of the feature extrac-\ntor yields a feature map to build the node/edge features of a\nGCN (Li et al. 2021). The cell graph, the edges and their fea-\ntures are defined as the same as those in our proposed CGT.\nFor nuclei segmentation and classification datasets (Gamper\net al. 2020; Graham et al. 2021), we define the node fea-\ntures to exploit the segmentation masks, following Wei et\nal. (2023). After that, both the node and edge features are in-\nput to the GCN for node update. The enhanced node embed-\ndings are fed into a linear classifier to predict nucleus types.\nThe instance-level nuclei classification loss is the same as\nEq. (3), by viewing P as the GCN prediction. In the pre-\ntraining, the feature extractor and the GCN are end-to-end\ntuned.\nExperiments\nDatasets. We utilize four nuclei classification datasets: Pan-\nNuke (Gamper et al. 2020), Lizard (Graham et al. 2021),\nNuCLS (Amgad et al. 2022), and BRCA-M2C (Abousamra\net al. 2021). PanNuke, Lizard and NuCLS have the seg-\nmentation masks of nuclei, while BRCA-M2C only pro-\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3876\nMethod PanNuke NuCLS\nAJI PQ F d Fi Fc Fd Fep Fne Favg AJI PQ F d Ft Fst Fs Fo Favg\nMCSPat - - 0.786 0.484 0.473 0.220 0.612 0.629 0.514 - - 0.658 0.488 0.267 0.581 0.035 0.343\nMask2former 0.616 0.666 0.792 0.400 0.426 0.289 0.668 0.617 0.480 0.229 0.331 0.432 0.367 0.098 0.521 0.000 0.247\nSONNET 0.686 0.649 0.813 0.522 0.474 0.367 0.639 0.604 0.521 0.332 0.403 0.458 0.461 0.181 0.547 0.000 0.330\nNCCD - - 0.800 0.571 0.525 0.354 0.660 0.588 0.539 - - - - - - - -\nHover. 0.663 0.631 0.793 0.510 0.478 0.265 0.627 0.636 0.503 0.467 0.429 0.662 0.469 0.272 0.586 0.023 0.337\nOurs+Hover. 0.663 0.631 0.793 0.527 0.531 0.358 0.705 0.673 0.558 0.467 0.429 0.662 0.501 0.300 0.593 0.095 0.377\nOurs+GT - - - 0.618 0.661 0.452 0.741 0.806 0.656 - - - 0.785 0.466 0.733 0.123 0.527\nTable 1: Comparison with the state-of-the-art methods on PanNuke and NuCLS datasets. The best classification results are in\nbold. â€˜Hover.â€™ and â€˜MCSPatâ€™ denotes Hover-net and MCSPatnet. â€˜Ours+Hover.â€™ and â€˜Ours+GTâ€™ denote our CGT framework\nusing the segmentation masks from a trained Hover-net model or ground truth.\nMethod Lizard Method BRCA-M2C\nAJI PQ F d Fne Fep Fl Fp Fe Fc Favg Fd Fi Fep Fs Favg\nMCSPat. - - 0.705 0.110 0.604 0.457 0.228 0.210 0.478 0.347 DDOD 0.585 0.379 0.540 0.156 0.359\nMask2former 0.385 0.297 0.603 0.036 0.469 0.367 0.148 0.268 0.275 0.313 YOLOX 0.638 0.439 0.502 0.170 0.370\nSONNET 0.434 0.447 0.597 0.197 0.610 0.322 0.328 0.402 0.421 0.380 ConvN.Uper. 0.785 0.423 0.636 0.353 0.471\nNCCD - - 0.633 0.378 0.423 0.404 0.471 0.461 0.534 0.445 DINO 0.633 0.403 0.631 0.213 0.416\nHover. 0.463 0.460 0.732 0.221 0.693 0.447 0.369 0.387 0.493 0.435 MCSPat. 0.831 0.422 0.683 0.417 0.507\nOurs+Hover. 0.463 0.460 0.732 0.302 0.724 0.438 0.434 0.416 0.548 0.477 Ours+MCSPat 0.831 0.447 0.732 0.428 0.536\nOurs+GT - - - 0.508 0.868 0.543 0.537 0.585 0.678 0.620 Ours+GT. - 0.598 0.869 0.602 0.690\nTable 2: Comparison with the state-of-the-art methods on Lizard and BRCA-M2C datasets. â€˜ConvNUper.â€™ denotes the\nConvNext-UperNet. Since the BRCA-M2C is a nuclei detection and classification benchmark, several nuclei detection and\nclassification methods are utilized for comparison. The best classification results are in bold.\nvides centroid annotations for nuclei detection and classi-\nfication. The PanNuke dataset comprises 7901 images with\na size of 256 Ã— 256 from 19 organs, which includes the cell\ntypes of inflammatory, connective, dead, epithelial, and neo-\nplastic. The Lizard benchmark consists of 291 large images\nwith an average size of 1016 Ã— 917, which is composed of\nsix existing datasets: ConSeP (Graham et al. 2019), CRAG,\nGLAS (Sirinukunwattana et al. 2017), DigestPath, TCGA\n(Grossman et al. 2016), and PanNuke. Lizard contains the\nnucleus types of epithelial, lymphocyte, plasma, neutrophil,\neosinophil, and connective. The NuCLS dataset has 1744\nimage patches that are grouped into four superclasses: tu-\nmor, stromal, sTILs, and other. The BRCA-M2C dataset in-\ncludes 120 image patches collected from TCGA, has the cell\ntypes of inflammatory, epithelial, and stromal. The data split\nand more details are in the supplementary material.\nImplementation details. The implementation is based on\nPyTorch (Paszke et al. 2017) and PyTorch Geometric library\n(Fey and Lenssen 2019). For the proposed CGT, the encoder\nand decoder of the feature extractor have four layers and\nthree layers, respectively. The CGT encoder contains four\ntransformer layers. For the pretraining strategy, the GCN is\nbuilt of two GENConv (Li et al. 2020) layers. Our results\nare reported as the average result of training with three dif-\nferent random seeds. The dimensions of type markers and\nlink markers are 64 and 16. The number of edges of each\nnode is 4. The pretraining strategy and the training of CGT\nare run for 150 and 50 epochs, respectively, with the Adam\noptimizer in an NVIDIA A-100 GPU. The initial learning\nrates for pretraining and training are10âˆ’4 and 10âˆ’5, respec-\ntively. The overall training time is 2 days for each dataset.\nMetrics. We utilize F-score (Graham et al. 2019) for eval-\nuating classification performance. Fi, Fc, Fd, Fep, Fne,\nFt, Fst, Fs, Fo, Fn, Fl, Fp, Fe denote the class-wise\nF-score for inflammatory, connective, dead, epithelial, neo-\nplastic, tumor, stromal, sTIL, other, neutrophil, lymphocyte,\nplasma, eosinophil, respectively. Favg denotes the average\nF-score for all classes in the same dataset. For evaluating\nsegmentation and detection, we adopt Aggregated Jaccard\nIndex (AJI) (Mahmood et al. 2019), Panoptic Quality (P Q)\n(Kirillov et al. 2019), and Detection Quality (F d) (Graham\net al. 2019).\nComparison with the State-of-the-art Methods\nFor PanNuke, Lizard and NuCLS datasets, the proposed\nCGT is compared with existing methods: Hover-net (Gra-\nham et al. 2019), MCSPatnet (Abousamra et al. 2021), SON-\nNET (Doan et al. 2022), Mask2former (Cheng et al. 2022),\nNCCD (Hassan et al. 2022). Among them, Hover-net, SON-\nNET and Mask2former are nuclei segmentation and clas-\nsification methods, MCSPatnet is a nuclei detection and\nclassification method and NCCD is a pure nuclei classifi-\ncation method. For BRCA-M2C dataset, we compare the\nproposed method with nuclei detection and classification\nmethods: DDOD (Chen et al. 2021), YOLOX (Ge et al.\n2021), ConvNext-UperNet (Liu et al. 2022b), MCSPatnet\n(Abousamra et al. 2021) and DINO (Zhang et al. 2022). In\nTable 1 & 2, â€˜Ours+Hover-netâ€™ or â€˜Ours+MCSpatâ€™ indicates\nthat our CGT utilizes Hover-net or MCSPatNet to gener-\nate nuclei segmentation or detection results, without using\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3877\nModels Initializing Feature Extractor Classifier Fi Fc Fd Fep Fne Favg\nM1 UNet ImageNet pretrained Linear 0.510 0.463 0.065 0.668 0.000 0.341\nM2 UNet ImageNet pretrained Transformer 0.456 0.381 0.166 0.601 0.614 0.444\nM3 UNet ImageNet pretrained GCN 0.513 0.506 0.263 0.674 0.633 0.518\nM4 UNet ImageNet pretrained CGToken+Transformer 0.435 0.104 0.057 0.526 0.602 0.344\nM5 UNet Linear pretrained CGToken+Transformer 0.518 0.472 0.138 0.674 0.237 0.420\nM6 UNet Transformer pretrained CGToken+Transformer 0.484 0.438 0.237 0.634 0.626 0.484\nM7 UNet GCN pretrained Transformer 0.525 0.484 0.228 0.672 0.655 0.512\nM8 (Ours) UNet GCN pretrained CGToken+Transformer 0.527 0.531 0.358 0.705 0.673 0.558\nTable 3: Ablation study on PanNuke dataset. â€˜CGToken+Transformerâ€™ is the proposed classifier in our CGT framework. All the\nresults are based on the official data split of the PanNuke dataset. The best results are in bold.\nMethod #Para. (M) Infer Time (s) Model Size (Mb)\nHover-net 33.60 1799 144\nOurs 37.43 447 465\nTable 4: Computational efficiency on whole slide images.\nInference time is measured as the average time of inferring\nten whole slide images.\nthe predictions of cell types. The numerical results of SON-\nNET and NCCD are collected from their papers. As Table 1\nshows, our proposed method â€˜Ours+Hover-netâ€™ outperforms\nthe second best models by 1.9%, 3.4% and 3.2% in Favg on\nPanNuke, NuCLS and Lizard, respectively. â€˜Ours+MCSPat.â€™\nsurpasses the second-best model by 2.9% in Favg on the\nBRCA-M2C dataset. Figure 4 presents a visual comparison\nbetween our proposed CGT and Hover-net on two datasets.\nBoth methods employ the same segmentation masks, but our\nmethod shows more accurate classification results of nuclei.\nMore visual results can be found in the appendix.\nEffectiveness and Generalization of CGT. Note that the\nsegmentation tools used in our CGT can also produce clas-\nsification results. Thus, the CGT is compared with them to\nshow its strength. Comparing â€˜Ours+Hoverâ€™ with Hover-net\nsuggests that the CGT brings a significant improvement of\n1.9%-3.4% in the average F-score on three benchmarks. Im-\nportantly, on the BRCA-M2C dataset, the proposed CGT\nalso boosts MCSPatNet by 2.9% inFavg to achieve the state-\nof-the-art performance. The improvements with two differ-\nent segmentation/detection tools verify the generalization of\nour CGT framework.\nIn Table 1 & 2, â€˜Ours+GTâ€™ means that our CGT accesses\nthe ground truth of binary segmentation in the testing. It\nshows that with more accurate nuclei centroids, our CGT can\nproduce better classification results. We claim that the pro-\nposed cell graph transformer is a flexible framework that can\ninfer cell types with various segmentation/detection models\nor manual annotations.\nComputational efficiency of CGT. Table 4 displays the ef-\nficiency comparison between our proposed CGT and Hover-\nnet. To evaluate the feasibility of real-world applications\nof CGT, we assessed the average parameter count (#Para),\ninference time (Infer Time), and model size on ten whole\nslide images (WSIs). These WSIs have an average size of\n36210Ã—71309. â€˜Oursâ€™ in Table 4 measures our method with-\nout including the segmentation tool. Our method increases\n400+ MB storage and 25% inference time when cooperat-\ning with existing segmentation methods, which is acceptable\nconsidering the low cost of the hard disk and the significant\nimprovement of performance.\nAblation Study\nIn Table 3, we assess the strengths of CGT and the proposed\nTopology-Aware Pretraining (TAP) strategy. In the testing\nstage, all models adopt the binary segmentation masks gen-\nerated by the same Hover-net. â€˜UNetâ€™ denotes the feature\nextractor in CGT. â€˜UNet ImageNet pretrainedâ€™ is to initial\nthe UNet with the ImageNet-1K pretrained weights. â€˜Lin-\nearâ€™, â€˜Transformerâ€™, â€˜GCNâ€™ and â€˜CGToken+Transformerâ€™\ndenote four classifiers: a linear embedding layer, a vanilla\ntransformer without graph structure, a graph convolutional\nnetwork and our proposed cell graph tokenization with a\ntransformer encoder, respectively. For example, UNet Lin-\near/Transformer/GCN pretrained means using Linear/Trans-\nformer/GCN as the classifier to pretrain the UNet for initial-\nization. Our method is denoted as M8 where â€˜UNet GCN\npretrainedâ€™ represents the TAP strategy.\nEffectiveness of the proposed pretraining. To validate the\nproposed pretraining strategy, we compare M8 with M4-M6\nand find that M8 using the TAP strategy significantly outper-\nforms M4-M6 by 21%, 13% and 7.4% inFavg, respectively.\nThe above results suggest that the TAP strategy using GCN\nis more effective than the simple ImageNet-pretraining, the\npretraining guided by a linear layer and a vanilla trans-\nformer. We claim that it is because the TAP strategy makes\nthe visual features aware of the graph connections and better\nadapt to our proposed CGT classifier.\nEffectiveness of the CGT classifier. Comparing M8 to M7\nsuggests that our proposed classifier built of cell graph to-\nkenization and transformer encoder surpasses the vanilla\ntransformer classifier without graph modeling by 4.6% in\nFavg. Comparing M5 to M1 and M8 to M3 shows that our\nproposed CGT classifier can outperform the linear and the\nGCN classifier by 7.9% and 4% in Favg. Besides, M1-M3\ncan be viewed as the combinations of the existing initial-\nization and classifiers, and our overall method M8 exceeds\nthese solutions by 4%-21% in Favg.\nInvestigation of Hyper-parameters. In Figure 5, we study\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3878\nEpithelialInflammatory Connective Neoplastic\nHover Ours+Hover GT\nTumor Stromal\nHover Ours+Hover GT\n(a)\n(b)\n(c)\n(d)\nFigure 4: Visual comparison of the proposed CGT with Hover-net on PanNuKe (left) and NuCLS (right) datasets. GT denotes\nthe ground truth.\nFigure 5: Analysis of different choices for layer num-\nber (L âˆˆ {1,2, 3, 4}) in CGT and edge number (E âˆˆ\n{4, 8, 16, 32}) in GCN (the topology-aware pretraining) on\nPanNuke dataset.\nthe number of transformer layers L of the CGT encoder on\nPanNuke dataset. If we set L from 1 to 4, the average F-\nscore first decrease by 0.6% and then increases by 0.4% and\n1.5%. The results indicate that the performance is improved\nslightly with increasing transformer layers. Due to the GPU\nmemory limitation, we do not test for larger layer numbers.\nThe idea of our pretraining is that too dense connections\nwith inferior features could result in unreasonable correla-\ntions and message passing, which affects the CGT training.\nIn contrast, the GCN using a sparse graph is more robust. To\nverify the above idea, we use a denser graph for GCN-based\npretraining by increasing edge number E. In Figure 5, as E\nincreases from 4 to 8 and 32, the average F-score of GCN\ndoes drop by 1.2% and 4.8%, which validates our assump-\ntion.\nDiscussion\nCGT vs. Transformer. The proposed CGT differs from\nvanilla transformers that compute correlations and pass mes-\nsages between each pair of nodes equally. In contrast, the\nCGT defines edge features to describe pathological microen-\nvironment and exploits link & token markers to learn con-\nnections which emphasizes the attention between relevant\ncells. In Table 3, the CGT (M8) outperforms the vanilla\ntransformer (M7/M2) by 4.6%-11% in Favg, which indi-\ncates that the proposed CGT better models the cells and their\ninteractions in pathological images than the vanilla trans-\nformers.\nGCN pretraining vs. Transformer pretraining. We dis-\ncuss why the pretraining with GCN as classifier (M8 in Ta-\nble 3) is better than the one with vanilla transformer (M6).\nThe GCN-pretraining adopts a sparse graph where the well-\ndefined connections could guide the reasonable propagation\nof information. During the GCN-pretraining, the feature ex-\ntractor is tuned by the gradients that are computed based on\nthe well-defined edges and can adapt to the topology of cell\ngraphs. However, in the transformer-pretraining, the gradi-\nents passed through the feature extractor are calculated from\nany pairs of nodes, even those that are irrelevant. Thus, the\ngradients in transformer-pretraining are more noisy and un-\nreliable than those in GCN-pretraining, at the start of train-\ning.\nConclusion\nIn this paper, a cell graph transformer (CGT) framework is\nproposed for identifying cell types with detected nucleus\ncentroids. Our method embraces the transformer as a cell\ngraph learner to fully exploit contexts and learn topologi-\ncal features. Both cell nodes and edges are viewed as input\ntokens to capture long-range correlations. The graph struc-\nture is embedded into the transformer encoder via link mark-\ners and token markers. Furthermore, we develop a novel\ntopology-aware pretraining strategy that employs the robust\nlocal message-passing mechanism of graph convolutional\nnetworks to help pretrain the feature extractor of CGT. The\nexperimental results display that the CGT model achieves\nthe state-of-the-art nuclei classification performance on ex-\nisting benchmarks.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3879\nAcknowledgments\nThis work was supported in part by the National\nNatural Science Foundation of China (NO. 62102267,\nNO. 62322608), in part by the Natural Science Founda-\ntion of Guangdong Province of China (2023A1515011464),\nin part by the Shenzhen Science and Technology Program\nJCYJ20220818103001002), and in part by the Guangdong\nProvincial Key Laboratory of Big Data Computing, The Chi-\nnese University of Hong Kong, Shenzhen.\nReferences\nAbousamra, S.; Belinsky, D.; Van Arnam, J.; Allard, F.; Yee,\nE.; Gupta, R.; Kurc, T.; Samaras, D.; Saltz, J.; and Chen, C.\n2021. Multi-class cell detection using spatial context repre-\nsentation. In ICCV, 4005â€“4014.\nAmgad, M.; Atteya, L. A.; Hussein, H.; Mohammed, K. H.;\nHafiz, E.; Elsebaie, M. A.; Alhusseiny, A. M.; AlMosle-\nmany, M. A.; Elmatboly, A. M.; Pappalardo, P. A.; et al.\n2022. NuCLS: A scalable crowdsourcing approach and\ndataset for nucleus classification and segmentation in breast\ncancer. GigaScience, 11.\nAnand, D.; Gadiya, S.; and Sethi, A. 2020. Histographs:\ngraphs in histopathology. In Medical Imaging 2020: Digital\nPathology, volume 11320, 150â€“155. SPIE.\nAnklin, V .; Pati, P.; Jaume, G.; Bozorgtabar, B.; Foncubierta-\nRodriguez, A.; Thiran, J.-P.; Sibony, M.; Gabrani, M.; and\nGoksel, O. 2021. Learning whole-slide segmentation from\ninexact and incomplete labels using tissue graphs. In MIC-\nCAI, 636â€“646. Springer.\nBasha, S. S.; Ghosh, S.; Babu, K. K.; Dubey, S. R.; Pula-\nbaigari, V .; and Mukherjee, S. 2018. Rccnet: An efficient\nconvolutional neural network for histological routine colon\ncancer nuclei classification. In ICARCV, 1222â€“1227. IEEE.\nChen, Z.; Yang, C.; Li, Q.; Zhao, F.; Zha, Z.-J.; and Wu,\nF. 2021. Disentangle your dense object detector. In ACM\nMultimedia, 4939â€“4948.\nCheng, B.; Misra, I.; Schwing, A. G.; Kirillov, A.; and Gird-\nhar, R. 2022. Masked-attention mask transformer for uni-\nversal image segmentation. In CVPR, 1290â€“1299.\nDemir, C.; Gultekin, S. H.; and Yener, B. 2005. Augmented\ncell-graphs for automated cancer diagnosis. Bioinformatics,\n21(suppl\n2): ii7â€“ii12.\nDoan, T. N. N.; Song, B.; Le Vuong, T. T.; Kim, K.; and\nKwak, J. T. 2022. SONNET: A self-guided ordinal regres-\nsion neural network for segmentation and classification of\nnuclei in large-scale multi-tissue histology images. IEEE\nJBHI.\nDwivedi, V . P.; and Bresson, X. 2020. A generaliza-\ntion of transformer networks to graphs. arXiv preprint\narXiv:2012.09699.\nDwivedi, V . P.; Joshi, C. K.; Laurent, T.; Bengio, Y .; and\nBresson, X. 2020. Benchmarking graph neural networks.\narXiv preprint arXiv:2003.00982.\nFey, M.; and Lenssen, J. E. 2019. Fast graph representation\nlearning with PyTorch Geometric. ICLR Workshop.\nGamper, J.; Koohbanani, N. A.; Benes, K.; Graham, S.; Ja-\nhanifar, M.; Khurram, S. A.; Azam, A.; Hewitt, K.; and Ra-\njpoot, N. 2020. Pannuke dataset extension, insights and\nbaselines. arXiv preprint arXiv:2003.10778.\nGe, Z.; Liu, S.; Wang, F.; Li, Z.; and Sun, J. 2021.\nYolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430.\nGraham, S.; Jahanifar, M.; Azam, A.; Nimir, M.; Tsang, Y .-\nW.; Dodd, K.; Hero, E.; Sahota, H.; Tank, A.; Benes, K.;\net al. 2021. Lizard: A large-scale dataset for colonic nuclear\ninstance segmentation and classification. In ICCV Work-\nshops, 684â€“693.\nGraham, S.; Vu, Q. D.; Raza, S. E. A.; Azam, A.; Tsang,\nY . W.; Kwak, J. T.; and Rajpoot, N. 2019. Hover-net: Simul-\ntaneous segmentation and classification of nuclei in multi-\ntissue histology images. MIA, 58: 101563.\nGrossman, R. L.; Heath, A. P.; Ferretti, V .; Varmus, H. E.;\nLowy, D. R.; Kibbe, W. A.; and Staudt, L. M. 2016. To-\nward a shared vision for cancer genomic data. New England\nJournal of Medicine, 375(12): 1109â€“1112.\nGuo, M.-H.; Lu, C.-Z.; Liu, Z.-N.; Cheng, M.-M.; and Hu,\nS.-M. 2023. Visual attention network.Computational Visual\nMedia, 1â€“20.\nHassan, T.; Javed, S.; Mahmood, A.; Qaiser, T.; Werghi, N.;\nand Rajpoot, N. 2022. Nucleus Classification in Histology\nImages Using Message Passing Network. MIA, 102480.\nHuang, J.; Li, H.; Sun, W.; Wan, X.; and Li, G. 2023a.\nPrompt-based grouping transformer for nucleus detection\nand classification. In MICCAI, 569â€“579. Springer.\nHuang, J.; Li, H.; Wan, X.; and Li, G. 2023b. Affine-\nConsistent Transformer for Multi-Class Cell Nuclei Detec-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 21384â€“21393.\nJaved, S.; Mahmood, A.; Fraz, M. M.; Koohbanani, N. A.;\nBenes, K.; Tsang, Y .-W.; Hewitt, K.; Epstein, D.; Snead, D.;\nand Rajpoot, N. 2020. Cellular community detection for tis-\nsue phenotyping in colorectal cancer histology images.MIA,\n63: 101696.\nKim, J.; Nguyen, D. T.; Min, S.; Cho, S.; Lee, M.; Lee, H.;\nand Hong, S. 2022. Pure Transformers are Powerful Graph\nLearners. NeurIPS.\nKim, J.; Oh, S.; and Hong, S. 2021. Transformers general-\nize deepsets and can be extended to graphs & hypergraphs.\nNeurIPS, 34: 28016â€“28028.\nKirillov, A.; He, K.; Girshick, R.; Rother, C.; and Doll Â´ar, P.\n2019. Panoptic segmentation. In CVPR, 9404â€“9413.\nKreuzer, D.; Beaini, D.; Hamilton, W.; L Â´etourneau, V .; and\nTossou, P. 2021. Rethinking graph transformers with spec-\ntral attention. NeurIPS, 34: 21618â€“21629.\nKrithiga, R.; and Geetha, P. 2021. Breast cancer detection,\nsegmentation and classification on histopathology images\nanalysis: a systematic review. Archives of Computational\nMethods in Engineering, 28: 2607â€“2619.\nLagree, A.; Mohebpour, M.; Meti, N.; Saednia, K.; Lu, F.-\nI.; Slodkowska, E.; Gandhi, S.; Rakovitch, E.; Shenfield, A.;\nSadeghi-Naini, A.; et al. 2021. A review and comparison\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3880\nof breast tumor cell nuclei segmentation performances us-\ning deep convolutional neural networks. Scientific Reports,\n11(1): 8025.\nLi, G.; MÂ¨uller, M.; Qian, G.; Perez, I. C. D.; Abualshour, A.;\nThabet, A. K.; and Ghanem, B. 2021. Deepgcns: Making\ngcns go as deep as cnns. TPAMI, 6923 â€“ 6939.\nLi, G.; Xiong, C.; Thabet, A.; and Ghanem, B. 2020. Deep-\nergcn: All you need to train deeper gcns. arXiv preprint\narXiv:2006.07739.\nLi, Q.; Han, Z.; and Wu, X.-M. 2018. Deeper insights into\ngraph convolutional networks for semi-supervised learning.\nIn AAAI, volume 32.\nLin, K.; Wang, L.; and Liu, Z. 2021. Mesh graphormer. In\nICCV, 12939â€“12948.\nLin, T.-Y .; DollÂ´ar, P.; Girshick, R.; He, K.; Hariharan, B.;\nand Belongie, S. 2017. Feature pyramid networks for object\ndetection. In CVPR, 2117â€“2125.\nLiu, S.; Mundra, P. A.; and Rajapakse, J. C. 2011. Features\nfor cells and nuclei classification. In EMBC, 6601â€“6604.\nLiu, Y .; Jia, Y .; Hou, C.; Li, N.; Zhang, N.; Yan, X.; Yang, L.;\nGuo, Y .; Chen, H.; Li, J.; et al. 2022a. Pathological progno-\nsis classification of patients with neuroblastoma using com-\nputational pathology analysis. CBM, 149: 105980.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In ICCV, 10012â€“10022.\nLiu, Z.; Mao, H.; Wu, C.-Y .; Feichtenhofer, C.; Darrell, T.;\nand Xie, S. 2022b. A convnet for the 2020s. In CVPR,\n11976â€“11986.\nLou, W.; Li, H.; Li, G.; Han, X.; and Wan, X. 2022.\nWhich pixel to annotate: a label-efficient nuclei segmenta-\ntion framework. IEEE Transactions on Medical Imaging,\n42(4): 947â€“958.\nLou, W.; Wan, X.; Li, G.; Lou, X.; Li, C.; Gao, F.; and Li,\nH. 2023a. Structure Embedded Nucleus Classification for\nHistopathology Images. arXiv preprint arXiv:2302.11416.\nLou, W.; Yu, X.; Liu, C.; Wan, X.; Li, G.; Liu, S.; and Li,\nH. 2023b. Multi-stream Cell Segmentation with Low-level\nCues for Multi-modality Images. In Competitions in Neural\nInformation Processing Systems, 1â€“10. PMLR.\nMa, J.; Xie, R.; Ayyadhury, S.; Ge, C.; Gupta, A.; Gupta, R.;\nGu, S.; Zhang, Y .; Lee, G.; Kim, J.; et al. 2023. The Multi-\nmodality Cell Segmentation Challenge: Towards Universal\nSolutions. arXiv preprint arXiv:2308.05864.\nMahmood, F.; Borders, D.; Chen, R. J.; McKay, G. N.;\nSalimian, K. J.; Baras, A.; and Durr, N. J. 2019. Deep\nadversarial training for multi-organ nuclei segmentation in\nhistopathology images. IEEE TMI, 39(11): 3257â€“3267.\nOono, K.; and Suzuki, T. 2020. Graph Neural Networks Ex-\nponentially Lose Expressive Power for Node Classification.\nIn ICLR.\nPaszke, A.; Gross, S.; Chintala, S.; Chanan, G.; Yang, E.;\nDeVito, Z.; Lin, Z.; Desmaison, A.; Antiga, L.; and Lerer, A.\n2017. Automatic differentiation in pytorch. NIPS Workshop.\nPati, P.; Jaume, G.; Foncubierta-Rodr Â´Ä±guez, A.; Feroce, F.;\nAnniciello, A. M.; Scognamiglio, G.; Brancati, N.; Fiche,\nM.; Dubruc, E.; Riccio, D.; et al. 2022. Hierarchical graph\nrepresentations in digital pathology. MIA, 75: 102264.\nSchnorrenberg, F.; Pattichis, C. S.; Schizas, C. N.; Kyriacou,\nK.; and Vassiliou, M. 1996. Computer-aided classification\nof breast cancer nuclei. Technology and Health Care, 4(2):\n147â€“161.\nSharma, H.; Zerbe, N.; Heim, D.; Wienert, S.; Behrens, H.-\nM.; Hellwich, O.; and Hufnagl, P. 2015. A multi-resolution\napproach for combining visual information using nuclei seg-\nmentation and classification in histopathological images. In\nVISAPP (3), 37â€“46.\nSirinukunwattana, K.; Pluim, J. P.; Chen, H.; Qi, X.; Heng,\nP.-A.; Guo, Y . B.; Wang, L. Y .; Matuszewski, B. J.; Bruni,\nE.; Sanchez, U.; et al. 2017. Gland segmentation in colon\nhistology images: The glas challenge contest.MIA, 35: 489â€“\n502.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Å.; and Polosukhin, I. 2017. At-\ntention is all you need. NIPS, 30.\nWei, L.; Xiang, W.; Guanbin, L.; Xiaoying, L.; Chenghang,\nL.; Feng, G.; and Li, H. 2023. Structure Embedded Nucleus\nClassification for Histopathology Images. arXiv preprint\narXiv:2302.11416.\nWu, Z.; Jain, P.; Wright, M.; Mirhoseini, A.; Gonzalez, J. E.;\nand Stoica, I. 2021. Representing long-range context for\ngraph neural networks with global attention. NeurIPS, 34:\n13266â€“13279.\nYing, C.; Cai, T.; Luo, S.; Zheng, S.; Ke, G.; He, D.; Shen,\nY .; and Liu, T.-Y . 2021. Do transformers really perform\nbadly for graph representation? NeurIPS, 34: 28877â€“28888.\nYu, X.; Li, G.; Lou, W.; Liu, S.; Wan, X.; Chen, Y .; and\nLi, H. 2023. Diffusion-based data augmentation for nuclei\nimage segmentation. In MICCAI, 592â€“602. Springer.\nZhang, H.; Li, F.; Liu, S.; Zhang, L.; Su, H.; Zhu, J.; Ni,\nL.; and Shum, H.-Y . 2022. DINO: DETR with Improved\nDeNoising Anchor Boxes for End-to-End Object Detection.\nIn ICLR.\nZhang, L.; Lu, L.; Nogues, I.; Summers, R. M.; Liu, S.; and\nYao, J. 2017. DeepPap: deep convolutional networks for\ncervical cell classification. IEEE JBHI, 21(6): 1633â€“1643.\nZhao, Y .; Yang, F.; Fang, Y .; Liu, H.; Zhou, N.; Zhang, J.;\nSun, J.; Yang, S.; Menze, B.; Fan, X.; et al. 2020. Predicting\nlymph node metastasis using histopathological images based\non multiple instance learning with deep graph convolution.\nIn CVPR, 4837â€“4846.\nZheng, Y .; Gindra, R. H.; Green, E. J.; Burks, E. J.; Betke,\nM.; Beane, J. E.; and Kolachalama, V . B. 2022. A graph-\ntransformer for whole slide image classification. IEEE TMI,\n41(11): 3003â€“3015.\nZhou, Y .; Graham, S.; Alemi Koohbanani, N.; Shaban, M.;\nHeng, P.-A.; and Rajpoot, N. 2019. Cgc-net: Cell graph con-\nvolutional network for grading of colorectal cancer histol-\nogy images. In ICCV Workshops.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n3881",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.4477463364601135
    },
    {
      "name": "Computer science",
      "score": 0.3849713206291199
    },
    {
      "name": "Engineering",
      "score": 0.2043885588645935
    },
    {
      "name": "Electrical engineering",
      "score": 0.10303390026092529
    },
    {
      "name": "Voltage",
      "score": 0.06858980655670166
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210116924",
      "name": "Chinese University of Hong Kong, Shenzhen",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210099586",
      "name": "Shenzhen Research Institute of Big Data",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    }
  ]
}