{
  "title": "ChatGPT Goes to Operating Room: Evaluating GPT-4 Performance and Its Potential in Surgical Education and Training in the Era of Large Language Models",
  "url": "https://openalex.org/W4327861797",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3109391854",
      "name": "Namkee Oh",
      "affiliations": [
        "Sungkyunkwan University",
        "Samsung Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2405450204",
      "name": "Gyu Seong Choi",
      "affiliations": [
        "Sungkyunkwan University",
        "Samsung Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2137493188",
      "name": "Woo Yong Lee",
      "affiliations": [
        "Samsung Medical Center",
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A3109391854",
      "name": "Namkee Oh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2405450204",
      "name": "Gyu Seong Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137493188",
      "name": "Woo Yong Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4319663047",
    "https://openalex.org/W4313384047",
    "https://openalex.org/W4320009668",
    "https://openalex.org/W2088383760",
    "https://openalex.org/W2766894906",
    "https://openalex.org/W2995872180",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4322718191"
  ],
  "abstract": "Abstract Purpose This study aimed to assess the performance of ChatGPT, specifically the GPT-3.5 and GPT-4 models, in understanding complex surgical clinical information and its potential implications for surgical education and training. Methods The dataset comprised 280 questions from the Korean general surgery board exams conducted between 2020 and 2022. Both GPT-3.5 and GPT-4 models were evaluated, and their performances were compared using McNemar’s test. Results GPT-3.5 achieved an overall accuracy of 46.8%, while GPT-4 demonstrated a significant improvement with an overall accuracy of 76.4%, indicating a notable difference in performance between the models (P &lt; 0.001). GPT-4 also exhibited consistent performance across all subspecialties, with accuracy rates ranging from 63.6% to 83.3%. Conclusion ChatGPT, particularly GPT-4, demonstrates a remarkable ability to understand complex surgical clinical information, achieving an accuracy rate of 76.4% on the Korean general surgery board exam. However, it is important to recognize the limitations of LLMs and ensure that they are used in conjunction with human expertise and judgment.",
  "full_text": "1 \nChatGPT Goes to Operating Room: Evaluating GPT-4 Performance and \nIts Potential in Surgical Education and Training in the Era of Large \nLanguage Models \n \nNamkee Oh, MD, Gyu-Seong Choi, MD, PhD, Woo Yong Lee, MD, PhD* \n \nDepartment of Surgery, Samsung Medical Center, Sungkyunkwan University School of \nMedicine, Seoul, Korea \n \n* Corresponding author \nProfessor \nWoo Yong Lee, M.D., Ph.D. \nDepartment of Surgery, Samsung Medical Center, Sungkyunkwan University School of \nMedicine \nAddress: 81 Irwon-ro, Gangnam-gu, Seoul, Republic of Korea 06351 \nTel: +82-2-3410-0261; Fax: +82-2-3410-6980 \nE-mail: wooyong123.lee@samsung.com \n \nRunning title: ChatGPT goes to operating room \n \nORCID \nNamkee Oh: 0000-0002-6594-8973 \nGyu-Seong Choi: 0000-0003-2545-3105 \nWoo Yong Lee: 0000-0002-9558-9019 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2 \nAbstract \nPurpose: This study aimed to assess the performance of ChatGPT, specifically the GPT-3.5 \nand GPT-4 models, in understanding complex surgical clinical information and its potential \nimplications for surgical education and training. \nMethods: The dataset comprised 280 questions from the Korean general surgery board \nexams conducted between 2020 and 2022. Both GPT-3.5 and GPT-4 models were evaluated, \nand their performances were compared using McNemar’s test. \nResults: GPT-3.5 achieved an overall accuracy of 46.8%, while GPT-4 demonstrated a \nsignificant improvement with an overall accuracy of 76.4%, indicating a notable difference in \nperformance between the models (P < 0.001). GPT-4 also exhibited consistent performance \nacross all subspecialties, with accuracy rates ranging from 63.6% to 83.3%. \nConclusion: ChatGPT, particularly GPT-4, demonstrates a remarkable ability to understand \ncomplex surgical clinical information, achieving an accuracy rate of 76.4% on the Korean \ngeneral surgery board exam. However, it is important to recognize the limitations of LLMs \nand ensure that they are used in conjunction with human expertise and judgment. \n \n \n \nKey Words: Artificial intelligence, General surgery, Medical education, Continuous \nmedical education  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n3 \nINTRODUCTION \nSignificant advancements in large language model (LLM) technology have recently \nrevolutionized the field of artificial intelligence (AI), with ChatGPT released by OpenAI in \nNovember 2022 standing out as a prime example [1]. ChatGPT has exhibited exceptional \nperformance in evaluating knowledge related to fields such as medicine, law, and \nmanagement, which have traditionally been considered to be the domain of experts. Notably, \nthe system achieved high accuracy on the USMLE, the Bar exam, and the Wharton MBA \nfinal exam, even without fine-tuning the pre-trained model [2-5]. \nSurgical education and training demand a significant investment of time, with the process \ninvolving a combination of didactic learning, hands-on training, and supervised clinical \nexperience [6]. During residency, surgical trainees work alongside experienced surgeons, \ngaining practical experience in patient care, surgery, and clinical decision-making. \nAdditionally, trainees engage in a series of didactic courses and conferences covering the \nprinciples of surgery, medical knowledge, and surgical techniques. Due to the comprehensive \nnature of surgical education and training, it can take more than a decade to become a skilled \nand competent surgeon. Given the time-intensive nature of surgical education and training, it \nis important to explore how emerging technologies, such as AI and LLMs, can augment the \nlearning process [7]. \nThis study aims to employ ChatGPT to evaluate the general surgery board exam in \nKorea and assess  whether LLMs possess expert -level knowledge. Moreover, the study \ncompared the performance of GPT-3.5 and GPT-4. By exploring the potential of LLMs in the \ncontext of surgical education and training , this study seeks to provide a foundation for future \nresearch on how these advancements can be effectively integrated into clinical education and \npractice, ultimately benefiting surgical residents, and practicing surgeons. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n4 \nMETHODS \nGeneral surgery board exam of Korea \nThe goal of surgical education and training is to develop the ability to actively evaluate the \npathological conditions of surgical diseases and to acquire the surgical skills to treat \ntraumatic, congenital, acquired, neoplastic, and infectious surgical diseases. To quantitatively \nevaluate this knowledge and skill set of surgical residents, a board certification exam is \nrequired after completion of their training, in order to become a board-certified general \nsurgeon in Korea. The exam is composed of two parts: the first part is a 200-question \nmultiple-choice test, and those who pass the first part are eligible to take the second part. The \nsecond part consists of questions based on high-resolution clinical images and surgical video \nclips. The questions are created and supervised by the Korean Surgical Society (KSS) and the \nKorean Academy of Medical Science (KAMS). \nDataset for model testing \nThe actual board exam questions are held by KAMS, but due to limited access to the usage \nof these questions, we constructed our dataset by gathering questions recalled by examinees \nwho took the actual exam. As the LLM cannot process visual information such as clinical \nimages, radiology, and graphs, questions that included visual information were excluded from \nour dataset. All problems were manually inputted in their original Korean text. Finally, our \ndataset included a total of 280 questions from the first stage of the board exam in 2020, 2021, \nand 2022 (Figure 1. A). \nLarge language model and performance evaluation \nIn this study, we utilized the ChatGPT generative pre-trained transformer (GPT) language \nmodel developed by OpenAI to evaluate its performance on a dataset of questions. We \nperformed model testing using both GPT-3.5 and GPT-4, with the former conducted from \nMarch 1st to March 3rd, 2023, and the latter scheduled for March 15th, 2023. To evaluate the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n5 \nmodel's performance, we manually entered the questions into the ChatGPT website and \ncompared the answers provided by the model to those provided by examinees (Figure 1. B). \nStatistical analysis \nThis study compared the performance of the GPT-3.5 and GPT-4 models with the \nMcNemar’s test. A p-value less than a 0.05 would indicate a statistically significant difference \nbetween the performance of the GPT-3.5 and GPT-4.  \nEthical approval \nThe study did not involve human subjects and did not require institutional review board \napproval. \n \nFigure 1.  (A) Dataset preparation process for model evaluation, (B) How models were \nevaluated from ChatGPT website. \n \nRESULTS \nThe dataset used for model evaluation consisted of a total of 280 questions, which were \nclassified into subspecialties and listed in order of frequency as follows: endocrine (16.8%), \nbreast (16.1%), lower gastrointestinal (LGI, 14.3%), upper gastrointestinal (UGI, 13.2%), \ngeneral (13.2%), pediatric (6.4%), hepatobiliary and pancreas (HBP, 6.1%), vascular (6.1%), \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n6 \ntransplantation (4.0%), and trauma and critical care(4.0%). (Figure 2) \n \nFigure 2. The dataset was composed of 280 questions, and it is classified into subspecialties \nin the field of general surgery. \n \nTable 1. Comparison table for the accuracy of GPT-3.5 and GPT-4. \n GPT-3.5 GPT-4 \nCorrect answer 131 214 \nIncorrect answer 149 66 \nAccuracy 46.8% (131/280) 76.4% (214/66) \nGPT: generative pre-trained transformer \n \nA significant difference in performance was observed between the GPT-3.5 and GPT-4 \nmodels (P < 0.001). The GPT-3.5 model achieved an overall accuracy of 46.8%, providing \ncorrect answers for 131 out of the 280 questions. (Table 1) In terms of individual \nsubspecialties, the model's accuracy rates were as follows (sorted from highest to lowest): \ntransplantation (72.7%), breast (62.2%), HBP (52.9%), general (48.6%), UGI (45.9%), \ntrauma and critical care (45.5%), LGI (45.0%), endocrine (36.2%), pediatric (33.3%), and \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n7 \nvascular (29.4%). In contrast, the GPT-4 model demonstrated a substantial improvement in \noverall accuracy, attaining a rate of 76.4% by providing correct answers for 214 out of the \n280 questions. The accuracy rates for each subspecialty were as follows: pediatric (83.3%), \nbreast (82.2%), UGI (81.1%), endocrine (78.7%), general (75.7%), transplantation (72.7%), \nLGI (72.5%), vascular (70.6%), HBP (64.7%), and trauma and critical care (63.6%). (Figure \n3) \n \nFigure 3. Comparison of the performance of GPT -4 and GPT-3.5 with overall accuracy and \naccuracies according to its subspecialties. \n \nDISCUSSION \n \nThe primary objective of this study was to conduct a quantitative assessment of ChatGPT's \nability to comprehend complex surgical clinical information and to explore the potential \nimplications of LLM technology for surgical education and training. Specifically, we tested \nthe performance of ChatGPT using questions from the Korean general surgery board exam \nand observed that the model achieved an accuracy of 76.4% with GPT-4 and 46.8% with \nGPT-3.5. Remarkably, this accuracy was achieved without fine-tuning the model and by \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n8 \nusing prompts in Korean language exclusively, thus highlighting the significance of our \nfindings. \nThe comparative analysis revealed a notable improvement in GPT-4’s performance \ncompared to GPT-3.5 model across all subspecialties. GPT-4 not only exhibited a higher \noverall accuracy rate but also demonstrated more consistent performance in each \nsubspecialty, with accuracy rates ranging from 63.6% to 83.3%. However, for 18 questions, \nGPT-3.5 provided the correct answer while GPT-4 did not (Table 2). It is unclear why GPT-4 \ngave incorrect answers for these questions despite the overall increase in accuracy. \nPinpointing the exact reason for this discrepancy is challenging. Differences in training data, \nmodel architecture, or other factors could have contributed to the variation in the performance \nbetween the two versions. \nTable 2. A 2 by 2 contingency table summarizing performance of GPT-3.5 and GPT-4. \n GPT-4 correct answer GPT-4 incorrect answer  \nGPT-3.5 correct answer 113 18 * 131 \nGPT-3.5 incorrect answer 101 48 149 \nGPT: generative pretrained transformer \n \nThe authors kindly recommend that the surgeon’s society proactively adapts and utilizes \nthese technological advancements to enhance patient safety and improve the quality of \nsurgical care. In the context of surgical education, it is crucial to transition from the \ntraditional rote learning approach to a method that emphasizes problem definition in specific \nclinical situations and the acquisition of relevant clinical information for problem resolution. \nLLMs serve as generative AI models, providing answers to given problems. Consequently, \nthe quality of the answers relies on the questions posed [8]. Surgeons must conduct thorough \nhistory taking and physical examinations to accurately define the problems they face. By \nproviding LLMs with comprehensive summaries of patients' chief complaints, present \nillnesses, and physical examinations, the models have the potential to assist in decision-\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n9 \nmaking regarding diagnostic tests and treatment options in certain clinical situations. \nHowever, it is essential for medical professionals to remember that LLMs should not replace \nthe fundamentals of patient care, which include maintaining close connections with patients \nand actively listening to their concerns [9]. \nMoreover, active surgeons who completed their training over a decade ago may find LLMs \nhelpful for continuous medical education (CME). Accessing new knowledge may be \nchallenging for them due to the time elapsed since their training, potentially leading to \noutdated management practices. While numerous surgical societies offer CME programs, \naltering ingrained routines in clinical practice can be difficult. By utilizing an up-to-date \nLLM as a supplementary resource in their decision-making process, surgeons may have an \nadditional means to stay informed and strive for evidence-based care in their patient \nmanagement [10]. \nIn medicine, decision-making has a profound impact on patient safety, demanding a higher \nlevel of accuracy and a conservative approach to change compared to other fields. Although \nGPT-4 achieved a 76.4% accuracy rate on the Korean surgical board exam, it is important to \nremember that LLMs are generative models, often reffered to as “stochastic parrots” [11]. \nInstead of providing strictly accurate information, they generate responses based on the \nprobability of the most appropriate words given the data they have been trained on. \nConsequently, the current level of accuracy is not yet sufficient for immediate clinical \napplication in patient care.  \nHowever, it is noteworthy that a service released less than six months ago exhibits such \nremarkable performance, and ChatGPT is only one example of LLMs. Recently, Microsoft \nreleased BioGPT, an LLM trained on PubMed literature, and Meta introduced LLaMA, an \nLLM with an accessible API for open innovation and fine-tuning [12, 13]. Based on these \ntrends, we can anticipate future LLMs to be trained on an even larger and more diverse set of \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n10 \nmedical information, providing specialized knowledge in the medical field. In addition, the \nGPT-4 framework itself is capable of processing and analyzing visual information, including \nimages and videos [14]. This capability raises the possibility that, in the future, the \nperformance of GPT-4 could be evaluated on datasets containing clinical photos and surgical \nvideos. Such advancements would further enhance the applicability of GPT-4 in surgical \nfields, broadening its utility beyond text-based tasks and offering a more comprehensive \nunderstanding of complex clinical scenarios assisting professionals in their decision-making \nprocesses and contributing to improved patient care. \nThe limitations of this study include the fact that the dataset was compiled using questions \nrecalled by examinees, which may not accurately represent the full set of actual board exam \nquestions due to restricted access. Another limitation is the exclusion of visual information. \nSince the models used in the study are unable to process visual information, such as clinical \nimages, radiology, and graphs, questions containing visual components were excluded from \nthe dataset. As a result, we cannot determine whether ChatGPT would pass or fail the board \nexam based on these limitations. Despite these constraints, this study holds significance as it \nconfirms the ability of LLMs to analyze surgical clinical information and make appropriate \nclinical decisions. \n \nCONCLUSION \n \nChatGPT, particularly GPT -4, demonstrates a remarkable ability to understand complex \nsurgical clinical information, achieving an accuracy rate of 76.4% on the Korean general \nsurgery board exam. However, it is important to recognize the limitations of LLMs and ensure \nthat they are used in conjunction with human expertise and judgment. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n11 \nAcknowledgement \nFunding \nThis study was supported by Future Medicine 2030 Project of the Samsung Medical Center \n[SMX1230771]. \nThe authors would like to thank Da Hyun Lee, an audiovisual engineer at Samsung Medical \nInformation & Medical Services, for designing Figure 1 for this work. \n \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n12 \nREFERENCES \n \n1. OpenAI, Introducing ChatGPT. 2022 [cited 2023 Feb 10]. Available from: \nhttps://openai.com/blog/chatgpt. \n2. Kung, T.H., et al., Performance of ChatGPT on USMLE: Potential for AI -assisted \nmedical education using large language models.  PLOS Digit Health, 2023. 2(2): p. \ne0000198. \n3. Mbakwe, A.B., et al., ChatGPT passing USMLE  shines a spotlight on the flaws of \nmedical education. PLOS Digit Health, 2023. 2(2): p. e0000205. \n4. Bommarito, M.J. and D.M. Katz, GPT Takes the Bar Exam. 2022 [cited 2023 Feb 10]. \nAvailable from: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4314839. \n5. Choi, J.H., et al., Chatgpt goes to law school. 2023 [cited 2023 Feb 10]. Available from: \nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=4335905. \n6. Debas, H.T., et al., American Surgical Association Blue Ribbon Committee Report on \nSurgical Education: 2004. Ann Surg, 2005. 241(1): p. 1-8. \n7. Wartman, S.A. and C.D. Combs, Medical Education Must Move From the Information \nAge to the Age of Artificial Intelligence. Acad Med, 2018. 93(8): p. 1107-1109. \n8. Radford, A., et al., Improving language understanding by generative pre-training. 2018 \n[cited 2023 Feb 10]. Available from: https://s3-us-west-2.amazonaws.com/openai-\nassets/research-covers/language-unsupervised/language_understanding_paper.pdf. \n9. Kapadia, M.R. and K. Kieran, Being Affable, Available, and Able Is Not Enough: \nPrioritizing Surgeon-Patient Communication. JAMA Surg, 2020. 155(4): p. 277-278. \n10. Han, E.R., et al., Medical education trends for future physicians in the era of advanced \ntechnology and artificial intelligence: an integrative review.  BMC Med Educ, 2019. \n19(1): p. 460. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint \n13 \n11. Bender, E.M., et al. On the Dangers of Stochastic Parrots: Can Language Models Be Too \nBig?\u0000ü¶ú. in Proceedings of the 2021 ACM conference on fairness, accountability, and \ntransparency. 2021. \n12. Luo, R., et al., BioGPT: generative pre -trained transformer for biomedical tex t \ngeneration and mining. Brief Bioinform, 2022. 23(6). \n13. Touvron, H., et al., Llama: Open and efficient foundation language models. 2023 [cited \n2023 Feb 10]. Available from: https://arxiv.org/abs/2302.13971. \n14. OpenAI, GPT-4 Technical Report. 2023 [cited 2023 Feb 10]. Available from: \nhttps://cdn.openai.com/papers/gpt-4.pdf. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted March 31, 2023. ; https://doi.org/10.1101/2023.03.16.23287340doi: medRxiv preprint ",
  "topic": "McNemar's test",
  "concepts": [
    {
      "name": "McNemar's test",
      "score": 0.866057276725769
    },
    {
      "name": "Medical physics",
      "score": 0.42930272221565247
    },
    {
      "name": "Medical education",
      "score": 0.4165002107620239
    },
    {
      "name": "Medicine",
      "score": 0.3783556818962097
    },
    {
      "name": "Computer science",
      "score": 0.3272216022014618
    },
    {
      "name": "Statistics",
      "score": 0.14755779504776
    },
    {
      "name": "Mathematics",
      "score": 0.09847059845924377
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2802194831",
      "name": "Samsung Medical Center",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I848706",
      "name": "Sungkyunkwan University",
      "country": "KR"
    }
  ]
}