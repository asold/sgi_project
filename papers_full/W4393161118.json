{
    "title": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models",
    "url": "https://openalex.org/W4393161118",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2098449489",
            "name": "Yue Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A1994445760",
            "name": "Ming Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2150125776",
            "name": "Hai-peng Yuan",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2106993465",
            "name": "Shichun Liu",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5101323123",
            "name": "Yongyao Shi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117552295",
            "name": "Tao Gui",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A1964204209",
            "name": "Qi Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2161482855",
            "name": "Xuanjing Huang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2098449489",
            "name": "Yue Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A1994445760",
            "name": "Ming Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2150125776",
            "name": "Hai-peng Yuan",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2106993465",
            "name": "Shichun Liu",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A5101323123",
            "name": "Yongyao Shi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2117552295",
            "name": "Tao Gui",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A1964204209",
            "name": "Qi Zhang",
            "affiliations": [
                "Fudan University"
            ]
        },
        {
            "id": "https://openalex.org/A2161482855",
            "name": "Xuanjing Huang",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6854692045",
        "https://openalex.org/W6621906925",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W4389519254",
        "https://openalex.org/W4319793767",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4389519239",
        "https://openalex.org/W4378468563",
        "https://openalex.org/W2970785793",
        "https://openalex.org/W4378770815",
        "https://openalex.org/W4308902180",
        "https://openalex.org/W658020064",
        "https://openalex.org/W4365601026",
        "https://openalex.org/W4380353763",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4383605161",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W4376653782",
        "https://openalex.org/W3121904249"
    ],
    "abstract": "Recently, the evaluation of Large Language Models has emerged as a popular area of research. The three crucial questions for LLM evaluation are ``what, where, and how to evaluate''. However, the existing research mainly focuses on the first two questions, which are basically what tasks to give the LLM during testing and what kind of knowledge it should deal with. As for the third question, which is about what standards to use, the types of evaluators, how to score, and how to rank, there hasn't been much discussion. In this paper, we analyze evaluation methods by comparing various criteria with both manual and automatic evaluation, utilizing onsite, crowd-sourcing, public annotators and GPT-4, with different scoring methods and ranking systems. We propose a new dataset, LLMEval and conduct evaluations on 20 LLMs. A total of 2,186 individuals participated, leading to the generation of 243,337 manual annotations and 57,511 automatic evaluation results. We perform comparisons and analyses of different settings and conduct 10 conclusions that can provide some insights for evaluating LLM in the future. The dataset and the results are publicly available at https://github.com/llmeval. The version with the appendix are publicly available at https://arxiv.org/abs/2312.07398.",
    "full_text": "LLMEval: A Preliminary Study on How to Evaluate Large Language Models\nYue Zhang1*, Ming Zhang1*, Haipeng Yuan1, Shichun Liu1, Yongyao Shi3\nTao Gui2, Qi Zhang1† , Xuanjing Huang1\n1 School of Computer Science, Fudan University, Shanghai, China\n2 Institute of Modern Languages and Linguistics, Fudan University, Shanghai, China\n3 Shanghai Advanced Institute of Finance, Shanghai Jiaotong University, Shanghai, China\nyuezhang.fdu@gmail.com, mingzhang23@m.fudan.edu.cn, {fdyhp49,liusc2020}@gmail.com, yyshi.23@saif.sjtu.edu.cn\n{tgui,qz,xjhuang}@fudan.edu.cn\nAbstract\nRecently, the evaluation of Large Language Models has\nemerged as a popular area of research. The three crucial\nquestions for LLM evaluation are “what, where, and how\nto evaluate”. However, the existing research mainly focuses\non the first two questions, which are basically what tasks to\ngive the LLM during testing and what kind of knowledge\nit should deal with. As for the third question, which is\nabout what standards to use, the types of evaluators, how to\nscore, and how to rank, there hasn’t been much discussion.\nIn this paper, we analyze evaluation methods by comparing\nvarious criteria with both manual and automatic evaluation,\nutilizing onsite, crowd-sourcing, public annotators and GPT-\n4, with different scoring methods and ranking systems. We\npropose a new dataset, LLMEval and conduct evaluations on\n20 LLMs. A total of 2,186 individuals participated, leading\nto the generation of 243,337 manual annotations and 57,511\nautomatic evaluation results. We perform comparisons and\nanalyses of different settings and conduct 10 conclusions\nthat can provide some insights for evaluating LLM in the\nfuture. The dataset and the results are publicly available at\nhttps://github.com/llmeval. The version with the appendix are\npublicly available at https://arxiv.org/abs/2312.07398.\nIntroduction\nIn recent years, Large Language Models (LLMs) have\nemerged as a highly significant and extensively explored\narea of research. As the capabilities of these LLMs con-\ntinue to advance, it becomes increasingly crucial to as-\nsess their performance and understand their limitations.\nHowever, traditional metrics for generative models, for\nexample, BLEU(Papineni et al. 2002), ROUGE(Lin 2004),\nWMD(Kusner et al. 2015), MoverScore(Zhao et al. 2019),\ncan only capture one or a few aspects of the model’s\ncapabilities.\nRecent research has started to explore the measurement\nof LLM from a more synthesized perspective. Those studies\ncan be divided into two categories, automatic and manual\nevaluation, based on whether scores can be automatically\ncalculated. There have been numerous efforts to carry out\n*These authors contributed equally.\n†Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nStudies\nAutomatic\nOnsite Annotator†\nCrowd-sourcing†\nPublic\nAnnotator†\nHELM(Liang et al. 2022) ✓\nMMLU(Hendrycks et al. 2021) ✓\nC-Eval(Huang et al. 2023) ✓\nAGIEval(Zhong et al. 2023) ✓\nBERTScore(Zhang et al. 2020) ✓\nAlpacaFarm(Dubois et al. 2023) ✓ ✓\nChatbot Arena(Zheng et al. 2023) ✓ ✓\nOurs‡ ✓ ✓ ✓ ✓\n† Manual evaluation with different types of annotators\n‡ Despite the type of annotator, our study also addresses\nthe problems of what criteria to use, how to score and\nhow to rank.\nTable 1: Evaluation Methods employed in LLM Evaluations\nautomatic evaluation. HELM(Liang et al. 2022) achieves\nsynthesized evaluation by combining a large number of\nexisting datasets. MMLU(Hendrycks et al. 2021) employs\nmultiple-choice questions for automated evaluation. C-\nEval(Huang et al. 2023) is a Chinese benchmark similar\nto MMLU. AGIEval(Zhong et al. 2023) utilizes both cloze\ntasks and multi-choice question-answering tasks simulta-\nneously. Approaches like BERTScore(Zhang et al. 2020)\nassign scores to outputs of LLMs by employing another\nLLM. As the capabilities of LLMs increasingly strengthen,\napart from automated evaluations, manual evaluations are\nalso an option. ChatBot Arena(Zheng et al. 2023) allows\npublic evaluator vote between two LLMs to rate them.\nAlpacaFarm(Dubois et al. 2023) leverages API LLMs to\nmimic manual evaluations as a low-cost replacement.\nIn a recent survey (Chang et al. 2023), three questions\nare raised about LLM evaluation, “what, where and how\nto evaluate”. “What to evaluate” is about determining\nthe tasks for the LLMs to execute during evaluation.\n“Where to evaluate” discusses the knowledge domains in\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19615\nwhich to evaluate the LLMs. These two questions have\nbeen quite extensively discussed. However, there’s less\nresearch on ”how to evaluate,” which refers to the specific\nmethods for evaluation. This includes scoring criteria,\ngrading approaches, ranking systems, and the type of\nannotators to use if manual evaluation is employed.\nIn this paper, we focus on “how to evaluate”. As shown\nin Table 1, our study examines both manual evaluation and\nGPT-4 based automatic evaluation. Compared to LLM-as-\na-Judge (Zheng et al. 2023), our study employs a greater\nnumber of annotator types in manual evaluation. Besides\nthat, we also compare various scoring criteria, grading\nmethods, and ranking systems. In total, we gathered 243,337\nmanual annotations and 57,511 automatic evaluation results.\nWe will release all the annotated data to Github when the\nanonymity period ends.\nIn general, when considering how to conduct an evalua-\ntion of an LLM, we come across three crucial questions that\nneed to be addressed.\nQ1: Which criteria should we take into account when\nevaluating LLMs? We can judge an LLM from various\nangles, like how accurate and fluent its answers are. But are\nall these criteria really needed? Could there be some aspects\nwhere all current LLMs have already done well enough, so\nfurther evaluation might not be necessary?\nWe conduct a comparison to the five criteria, accuracy,\ninformativeness, fluency, logical coherence and harmless-\nness. The results show that across various criteria, existing\nLLMs all have demonstrated notable performance in terms\nof harmlessness. The differentiating factors lie in the metrics\nof informativeness and accuracy.\nQ2: Which annotation methods should be employed to\nannotate the output of LLMs? We should consider how to\nscore LLMs, whether to give each LLM’s answer a separate\nscore or have a competition between two LLMs answering\nthe same question to determine the better one. Besides that,\nwe should decide whether to evaluate them manually or\nautomatically. If manual evaluation is applied, we also need\nto choose the type of annotators, onsite, crowd-sourcing, or\npublic.\nIn this paper, we use a combination of onsite, crowd-\nsourcing, and public annotators for manual annotation and\nGPT-4 for automatic evaluation. Our experiments demon-\nstrate that onsite evaluation exhibits superior accuracy and\nconsistency in manual evaluations. We also find a higher\nalignment level between onsite annotators and GPT-4.\nQ3: Which ranking systems should be utilized to rank\nLLMs? In evaluation methods that entail pairwise compar-\nison, a ranking system is required to convert win/loss/draw\noutcomes in to scores.\nIn our study, we compare two commonly used ranking\nsystems in competitive sports: the Elo rating system (used in\nchess games) and the Points scoring system(used in football\nmatches). We discovered that the Elo rating system exhibits\npoor stability in LLM evaluation tasks. It demonstrates sig-\nnificant variance in results when different match sequences\nare considered and is highly sensitive to noise data which is\ndifficult to avoid in manual annotation.\nIn general, our main contributions are in three folders:\n(1)We looked into the issue of ”how to evaluate LLMs,”\ncomparing various criteria, different types of annotators,\nrating methods, and ranking approaches. (2)We introduced\na fresh dataset called LLMEval and evaluated 20 models\nthrough both manual and automatic evaluations. (3)From\nour experimental results, we drew 10 conclusions that can\noffer some insights for future LLM evaluation.\nDesign\nIn this section, we introduce our approach to criteria\nselection, annotation methods design and ranking system\nchoice.\nCriteria\nGenerally speaking, LLM systems should follow the 3H\nprinciple(Askell et al. 2021), Helpfulness, Honesty, and\nHarmlessness. To more accurately evaluate these principles,\nwe have further refined them into five rating criteria,\naccuracy, fluency, informativeness, logical coherence, and\nharmlessness. By considering these rating criteria, we are\nable to assess and evaluate the performance of LLM systems\nfrom the perspective of human preference.\nAccuracy This evaluation aims to determine the accuracy\nof the answers, verifying the correctness and reliability\nof the provided information. A high-quality response is\nexpected to be factually reliable.\nFluency This evaluation involves examining whether\nthe answers adhere to natural language conventions. A\nhigh-quality response should be easily understandable, free\nfrom convoluted or ambiguous sentences that could hinder\ninterpretation.\nInformativeness This evaluation examines whether the\nanswers provide sufficient and meaningful information.\nA high-quality response should be able to provide the\nquestioner with useful and relevant information.\nLogical Coherence This evaluation assesses whether the\nanswers are logically rigorous and correct, determining the\nrationality of the presented viewpoints and arguments. A\nhigh-quality response should adhere to logical principles,\ndemonstrating clear reasoning and coherence.\nHarmlessness The evaluation examines whether the\nanswers refrain from containing unethical information,\nensuring compliance with ethical standards. A high-quality\nresponse should adhere to ethical principles, avoiding the\ndissemination of harmful or unethical information.\nAnnotation Method\nWhen considering the annotation method, we mainly dis-\ncussed three issues: the scoring method, whether to use\nautomatic or manual scoring, and the type of annotators\nto employ for manual scoring. We employ two different\nscoring methods, namely star scoring and pairwise compar-\nison, with three different types of annotators, onsite, crowd-\nsourcing, and public. In addition to manual annotation,\nwe perform automated evaluation using GPT-4, prompting\nthe same scoring requirement and criteria as those of\nhuman annotators. Specifically, we utilize the following five\nsettings.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19616\nOnsite Star Scoring For the onsite annotators, they are\ninstructed to evaluate the answers for each question based\non five criteria with one to three stars.\nCrowd-sourcing Pairwise Comparison For crowd-\nsourcing annotators, we pair the responses from LLMs\nfor the same question in a pairwise manner. These pairs\nare randomly presented side by side to the annotators.\nThe annotators are asked to give an overall judgment of\ntwo responses and determine which response is better or\nif they are equally good. The option setting is similar to\nLLM-as-a-Judge(Zheng et al. 2023).\nPublic Pairwise Comparison In the public pairwise\ncomparison evaluation, we employ a method similar to\ncrowd-sourcing, with the difference being that annotators\nare replaced by the general public. We’ve launched an\nevaluation website for the public annotators to conduct\nevaluations.\nGPT-4 Star Scoring To compare manual evaluation with\nGPT-4 automated evaluation, we utilize the criteria used in\nonsite star scoring and the response of an LLM as inputs and\nconduct evaluations using the GPT-4 API. Please refer to the\nappendix for the input templates used.\nGPT-4 Pairwise Comparison Similarly, we conduct\nevaluations using the GPT-4 API for the pairwise compar-\nison annotation. Please refer to the appendix for the input\ntemplates used.\nIn all evaluations, a double-blind testing method is\nemployed. The LLM name is concealed. Tasks are randomly\nassigned to different users.\nRanking System\nAs mentioned above, we employ two scoring methods, star\nscoring and pairwise comparison. For star scoring annota-\ntion, we can utilize the average scores to rank the systems.\nHowever, when it comes to pairwise comparison annotation,\ndetermining the sorting method is also a research question.\nTherefore, we compared the Elo rating system(used in\nchess games) and the Points scoring system(used in football\nmatches).\nPoints Scoring System This straightforward system\nawards points to participants based on their performance per\nmatch or event, disregarding the skill level of opponents. It\nfocuses on absolute performance in each individual event,\nwhich is often used in football matches.\nThe points for Player A (P A) before each game are\nrepresented as a summation of scores from all previous\ngames. After each game, the points are updated using the\nformula:\nP′\nA = PA + SA (1)\nHere, P′\nA denotes the updated points for Player A, andPA\nstands for Player A’s points before the game.\nThe scoring for Player A (S A) from each game is\nrepresented as:\nSA =\n\n\n\n1 if Player A wins\n0.5 if the game is a draw\n0 if Player A loses\n(2)\nIn this formula, SA represents the score gained by Player\nA from the game (1 for a win, 0.5 for a draw, 0 for a loss).\nThis system provides a clear, absolute reward for each\nindividual performance, regardless of the relative skill levels\nof the competitors.\nElo Rating System The Elo rating system, initially\ndevised for chess, is a method for quantifying the relative\nskill levels in player vs. player games. This system takes into\naccount the skill level of opponents and dynamically adjusts\nthe ratings based on the outcomes of each game.\nThe operation of the Elo rating system revolves around\ntwo key calculations. The first one predicts the expected\nscore or winning probability for a player, computed using\nthe formula:\nEA = 1\n1 + 10(RB−RA)/400 (3)\nIn this equation, EA represents the expected score for\nplayer A, RA denotes the current Elo rating for player A,\nand RB symbolizes the current Elo rating for player B.\nAfter the game concludes, player A’s Elo rating gets\nupdated using the following formula:\nR′\nA = RA + K · (SA − EA) (4)\nHere, R′\nA signifies the updated Elo rating for player A,\nRA denotes player A’s prior Elo rating,K is a constant factor\ntypically ranging from 10 to 40, which signifies the weight\nof the game, and SA represents the actual game result for\nplayer A (1 for a win, 0.5 for a draw, and 0 for a loss). In our\nexperiment, the K factor is set to 32, implying a moderate\nweight for each game.\nExperiments\nIn this section, we introduce our dataset and metrics used to\nevaluate annotation methods.\nDataset\nWe constructed two datasets, LLMEval-1 and LLMEval-2,\nto conduct the evaluation of LLMs.\nLLMEval-1 To evaluate the aforementioned five criteria,\nwe designed 17 different types of questions, including\nclassification, code, conversation, factual questions, math\nsolving, open questions, outline generation, paragraph gen-\neration, poetry, reading comprehension, reasoning, retrieval,\nrewrite, role-playing, story generation, summary, transla-\ntion.\nLLMEval-2 To further investigate the effectiveness of\nLLMs in specialized domains, we developed the LLMEval-\n2 dataset. We selected a total of 12 academic subjects,\nincluding biological science, chemistry, Chinese language\nand literature, computer science, economics, foreign lan-\nguages, law, mathematics, medicine, optics, physics, and\nsocial science. We created a set of questions for each subject\ncomprised an equal number of both objective and subjective\nquestions.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19617\nMetrics\nTo objectively assess the annotation methods mentioned\nin the above section, we’ve established accuracy and con-\nsistency as measurable indicators. Their definitions are as\nfollows.\nAccuracy In order to assess the accuracy of different\nannotation methods, it is essential to establish the generation\nmethod for the ground truth. In this study, we calculate the\naverage score of multiple annotators’ results as the ground\ntruth, gt\nscore. Additionally, we define an annotation as cor-\nrect if the difference between the score given by an annotator\nand the ground truth is less than the standard deviation, σ;\notherwise, it is considered an incorrect annotation(Equation\n5).\nis\ncorrect =\n\u001a1 abs(score − gt score) < σ\n0 otherwise (5)\nConsistency In all the evaluations, we include approxi-\nmately 2% of repeated tasks to assess whether the annotator\nmaintains consistent judgment criteria. For these repeated\ntasks, we conduct a statistical analysis of the annotations\nprovided by each annotator. We calculate the proportion\nof consistent results by dividing the number of identical\nannotations by the total number of repeated tasks. This\nserved as a measure of annotator consistency. For instance,\nif annotator A’s annotations for task 1 were (1, 1, 1, 0) in four\ndifferent attempts, the consistency rate would be calculated\nas 3/4, which is 75%.\nTo compare the quality of different annotators, we mixed\nthe manually annotated results with the annotations gener-\nated by GPT-4 to compute the ground truth. We excluded\nuser annotations with fewer than 5 results since we could\nnot assess the quality of their annotations.\nResults\nIn this section, we compare different criteria, various anno-\ntation methods, and the ranking systems on the evaluation\nand give answers to the three questions we raise in the\nintroduction section.\nComparison of Criteria\nTo identify the most differentiating criteria, we utilize the\nresults of manual star scoring evaluation. By comparing the\nscores of different models on various criteria, we can draw\nthe following conclusions.\n1) The differentiating criteria are informativeness and\naccuracy. Among all five criteria, all the LLMs in our test\nhave performed well in terms of harmlessness. The most\ndistinguishing criteria are accuracy and informativeness.\nFigure 1 demonstrates the scores of 5 models across five\ncriteria. The top-ranked and bottom-ranked differ by 0.853\nin terms of informativeness and by 0.776 in terms of\naccuracy.\n2) The task that best differentiates the capabilities\nof models is conversation. Figure 2 shows the top-ranked\nLLM surpasses other models mainly in conversation, math\nsolving and reasoning tasks. The score of GPT4.0 on the\nconversation task is 1.125 higher than ChatYuan-Large.\naccuracy\ninformativeness\nfluency\nlogicality\nharmlessness\nGPT4.0\nXunfei-Xinghuo\nBaichuan-7B-Align\nChatGLM-6B\nChatYuan-Large\nFigure 1: Scoring of Different Criteria in LLMEval-\n1. Among all five criteria, all the LLMs in our test\nhave performed well in terms of harmlessness. The most\ndistinguishing criteria are accuracy and informativeness.\nComparison of Annotation Methods\nFor the annotation methods, we want to figure out the\nbest scoring method and type of annotator by comparing\ntheir accuracy and consistency. We also want to see if\nautomatic evaluation can replace manual evaluation, or at\nleast partially, by comparing their alignment. Our findings\nare as follows:\n3) Onsite annotators exhibit the best quality in terms\nof accuracy and consistency. As shown in Figure 3, the\naverage accuracy of onsite star scoring evaluations is 0.892,\nwith a minimum accuracy of 0.825, higher than crowd-\nsourcing and public pairwise comparison evaluation. The\nstar scoring evaluation accuracy of GPT-4 is close to the\nhuman average, with a value of 0.908. The accuracy of GPT-\n4 in pairwise comparison evaluation is 0.688, indicating a\ngreater discrepancy between human and GPT-4 evaluations\nin pairwise comparison, aligning with our previous findings.\nThe consistency metric indicates a similar result.\n4) The public annotators show the lowest level of\nconsistency and accuracy. As depicted in Figure 3, public\nevaluations exhibit a considerable variance in both accuracy\nand consistency. The minimum accuracy is 0, while the\nlowest level of consistency is 0.3. It is important to note that\nthese results are derived after excluding annotations from\npublic annotators with fewer than 5 evaluations.\n5) The alignment between automated and manual\nevaluation is better under the setting of star scoring\nevaluation. there exists a certain degree of discrepancy\nbetween manual evaluation and automated evaluation. To\nfurther elucidate the differences between them, we calcu-\nlated the correlation coefficients among different ranks. As\nshown in Table 2, when using star scoring, the Spearman’s\ncorrelation coefficient (ρ) between ranks of GPT-4 and\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19618\nreading comp\nreasoning\ncode\npoetryopenstory gen\noutline gen\nparagraph gen\nfactual\nconversation\nrole playing\nclassification\nsummary rewriteretrieval\nmath solving\ntranslation\nGPT4.0\nXunfei-Xinghuo\nBaichuan-7B-Align\nChatGLM-6B\nChatYuan-Large\nFigure 2: Scoring of Different Tasks in LLMEval-1. The top-\nranked LLM surpasses other models mainly in conversation,\nmath solving and reasoning tasks.\nGPT-4 Onsite\nGPT-4 pairwiseCrowd-sourcing\nPublic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\nGPT-4 Onsite\nGPT-4 pairwiseCrowd-sourcing\nPublic\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0consistency\nFigure 3: Onsite annotators exhibit the best quality in terms\nof accuracy and consistency, higher than crowd-sourcing\nand public pairwise comparison evaluation\nmanual evaluation is 0.949, even higher than the correlation\nbetween manual star scoring and pairwise comparison.\nMeanwhile, The pairwise comparison between manual and\nGPT-4 evaluation exhibits the largest discrepancy in ranks.\nThe Spearman’s correlation coefficient (ρ) is 0.902. Com-\npared to (Zheng et al. 2023)’s study, our experimental results\ndemonstrate that when using the star scoring evaluation\nmethod, the evaluation results of GPT-4 align more closely\nwith manual evaluation.\n6) GPT-4 as an evaluator has a stronger bias on longer\nand more verbose responses than human evaluators. As\nshown in Table 3, when there is a difference in length of\nmore than 300 characters between two responses, GPT-4\nhas a 78.8% likelihood of selecting the longer text as the\nbetter one. In contrast, human annotators have a probability\nof 51.4% of choosing the longer text.\n7) Manual evaluation and GPT-4 automatic evaluation\nscores are less consistent on subjective questions. In\nSettings ρ τ\nManual Star Scoring v.s. Pairwise 0.938 0.839\nGPT-4 Star Scoring v.s. Pairwise 0.965 0.878\nStar Scoring Manual v.s. GPT-4 0.949 0.839\nPairwise Manual v.s. GPT-4 0.902 0.787\nA larger value of ρ or τ indicates a higher level of\nalignment between two ranks.\nTable 2: Spearman’s Correlation Coefficient(ρ) and Kendall\nTau Correlation Coefficient(τ ) of Ranks under Different\nSettings in LLMEval-1\nAnnotator Choice ∆length ≥ 100 ∆length ≥ 300\nHuman\nwin 32534(46.4%) 14679(51.4%)\ndraw 30395(43.4%) 11360(39.8%)\nloss 7128(10.2%) 2523(8.8%)\nGPT-4\nwin 12183(73.3%) 5606(78.8%)\ndraw 1440(8.7%) 538(7.6%)\nloss 2989(18.0%) 970(13.6%)\n* ∆length represents the absolute value of the difference\nin length between two responses. When ∆length ≥\n300, GPT-4 has a chance of 76.8% to determine the\nlonger one as the winner.\nTable 3: Length Bias Comparison between Manual and\nGPT-4 Evaluation in LLMEval-1\nLLMEval-2, we have employed a broader range of domain-\nspecific questions to evaluate LLMs. We also conduct\nmanual and automatic evaluations for 20 different models\nacross these domains. To assess the alignment between\nmanual evaluation and GPT-4 auto evaluation in different\nquestion types, we calculated the proportion of questions\nwith significant score differences. For objective questions,\nthe proportion of accuracy score differences exceeding 2\npoints is 12.98%, while for subjective questions, this propor-\ntion increases to 37.05%. This phenomenon indicates that\nGPT-4 auto evaluation shows a higher level of consistency\nin judging objective questions with formatted answers. The\nproportion of questions with significant score differences for\nother criteria can be found in Table 4 and 5.\n8) Annotators tend to give higher scores when answer\nhints are not provided. As mentioned earlier, for those\nevaluation questions with determined answers, we provided\nhints for annotators to refer to. We conducted additional\nDifferences in Scores - Manual/GPT-4 %\n∆Accuracy ≥ 2 37.05%\n∆Accuracy ≥ 4 6.99%\n∆Fluency ≥ 2 3.49%\n∆Logicality ≥ 2 7.87%\n∆Informativeness ≥ 2 9.97%\nTable 4: The proportion of the difference between manual\nevaluation and GPT-4 automatic evaluation of subjective\nquestions in LLMEval-2\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19619\nDifferences in Scores - Manual/GPT-4 %\n∆Correctness ≥ 3 12.98%\n∆Explanation ≥ 1 24.98%\nTable 5: The proportion of the difference between manual\nevaluation and GPT-4 automatic evaluation of objective\nquestions in LLMEval-2\nmanual annotation experiments to compare the impact of the\npresence of hints on the scores. And the result is as follows.\nAs shown in Figure 4, annotators gave scores that were on\naverage 9.79% higher. This indicates that hints greatly assist\nannotators in identifying factual errors in LLMs.\nReasoning\nCode\nTranslation\nComprehension\nMath\nSummary\nFact\nClassification\nPoetry\n8\n10\n12\n14\n16\n18Average Score\nwith hint without hint\nFigure 4: Annotators tend to give higher scores when answer\nhints are not provided\nComparison of Ranking Systems\nIn our study, we explored two ranking systems often\nused in pairwise comparison evaluations. Throughout the\ncourse of our study, we detected notable volatility in the\nrankings derived from the Elo rating system. Specifically,\nthe rankings of LLMs exhibited dramatic shifts between\nconsecutive time points. Different models presented only\nmarginal differences, which led us to question the stability of\nthe Elo rating system, especially when applied to large-scale\nannotations. Furthermore, the sequence of the evaluation\nprocess itself could potentially sway the final outcomes.\nTo validate our hypothesis, we calculate the variance\nof Elo rating scores. Given a user’s annotated accuracy\np, we can estimate the variance of Elo rating scores,\nVar[R∞\nA ] using the Equation 6 for an approximation. Due to\nlimited space, please refer to the appendix for the complete\nderivation.\nVar[R∞\nA ] = 322Var[S0\nA]\n∞X\ni=0\n0.92642i\n= 7211.27p(1 − p)\n(6)\nTo illustrate this observation, we also conduct experi-\nments with actual manual pairwise comparison results. And\nthe result is as follows:\n0 25 50 75 100\nrounds of comparison\n1500\n1520\n1540\n1560\n1580score\n0 25 50 75 100\nrounds of comparison\n1\n2\n3\n4\n5\n6\n7\n8 rank GPT4.0\nGPT3.5\nChatGLM-6B\nFigure 5: The fluctuation of Elo rating result after 100,000\nrounds of pairwise comparison is still immense\n9) The ranks generated by the Elo rating system\ncontinue to exhibit significant fluctuations even after\n100,000 rounds of comparison.We extracted the variations\nin ranks and scores resulting from pairwise comparisons\nconducted between rounds 100,000 and 100,100, and plotted\nthem in Figure 5. Even though GPT-4 has won many times\nin the previous 100,000 rounds of comparisons, only a few\nrecent losses are sufficient to impact the final ranking.\n0 2 4 6 8\nrounds of shuffle\n1500\n1525\n1550\n1575\n1600score\n0 2 4 6 8\nrounds of shuffle\n1\n2\n3\n4\n5\n6\n7\n8 rank GPT4.0\nGPT3.5\nChatGLM-6B\nFigure 6: In the Elo rating system, the same annotations can\nlead to changes in rank and score due to different orders.\n10) The Elo rating system is sensitive to the order\nof matches, as different orderings can lead to different\nranks. To demonstrate this, we randomly selected 10,000\npairwise comparison results. Then we performed 10 random\nshufflings of this dataset and plotted the outcomes in\nFigure 6. Even with the same annotation results, simply\nby changing the order of the annotations, GPT-4’s ranking\nexhibited fluctuations within the range of 1 to 3.\nDetails\nIn this section, we provide more details that are not covered\nin the experiment section. We present the steps in the order\nthey were conducted, including question collection, LLM\nresponse generation, and annotation process.\nOn LLMEval-1, we recruited 20 college students to\ncontribute 15 to 25 questions each to form a question\nset. To facilitate the annotation process and mitigate the\ndifficulty faced by annotators, answer hints have been\nprovided for factual questions, coding and math-solving\ntasks. We collected 453 questions in 17 different tasks\nin total. Then, we collected 12 available open-source and\ncommercial LLMs, and obtained responses from them. For\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19620\neach question, we initiated a new conversation to avoid\npotential interference from previous dialogues. We only\nconsidered the first response provided by the LLMs to\nensure fairness. Our tests were conducted between May\n1st and May 8th, 2023. Therefore, any updates made to\nthese LLMs after May 8th will not be reflected in the\nresults of this study. Eventually, we obtained a total of\n5436 responses, comprising 29,898 pairs. All questions and\nanswers are in Chinese. For each response, we sought star-\nscoring results from 3 onsite annotators. For each pair,\nwe enlisted at least 3 crowd-sourcing or public annotators\nfor pairwise comparison. We also shared our website\nfor public annotation. Similarly, for these responses and\npairs, we conducted an automated evaluation with GPT4\nusing scoring and pairwise comparison templates mentioned\nabove. A total of 33 million tokens were consumed in this\nprocess.\nOn LLMEval-2, We evaluated 20 major open-source and\ncommercial models. We conducted the LLMEval-2 from\nJune 24th to July 10th, 2023, to delve deeper into the\ncapabilities of LLMs in specialized domains. We recruited\n12 college students from 12 distinct disciplines to formulate\na question set. These questions were collected from the\nspecific fields they each have been studying. For each\ndiscipline, we created around 25-30 objective and 10-\n15 subjective questions approximately, accumulating 480\nquestions in total. The evaluation criteria are similar to\nLLMEval-1, with a few modifications We set correctness\nand explanation correctness criteria for objective questions,\nand accuracy, fluency, informativeness, and logicality for\nsubjective questions. The maximum score for objective\nquestions is 5, and for subjective questions, it is 14 points.\nCorrectness and accuracy are assigned a higher proportion\nof the total score. We exclude the criterion of harmlessness,\nas questions within academic disciplines seldom yield\nharmful outcomes. We utilized both onsite star scoring\nand GPT-4 star scoring for manual evaluation of 20 open-\nsource and commercial models. A comparison of these two\nevaluation methods was also conducted.\nRelated Works\nLarge Language Models(LLMs) have indeed achieved im-\npressive results in many downstream tasks. Meanwhile,\nthere are various approaches available for evaluating gen-\nerative models. In earlier studies, the evaluation of gen-\nerative models primarily relied on n-gram based, such\nas BLEU(Papineni et al. 2002), ROUGE(Lin 2004) or\nembedding-based methods, such as WMD(Kusner et al.\n2015), MoverScore(Zhao et al. 2019).\nHowever these evaluation methods often only consider\nthe model’s performance on a limited set of tasks and\nfail to assess its overall capability, such as comparing the\nmodel’s performance to human cognitive abilities. As LLMs\ncontinue to advance, they are approaching human-level\ncognitive abilities. Recent studies have made attempts to\nevaluate LLMs from a more comprehensive perspective.\nThese methods can be broadly classified into automatic and\nmanual evaluations.\nAutomatic Evaluations In NLP, there exist numerous\nbenchmarks that have been developed. Some studies, such\nas HELM(Liang et al. 2022) have undertaken combinations\nof these benchmarks to evaluate LLMs. In other works,\nsuch as MMLU(Hendrycks et al. 2021), C-Eval(Huang et al.\n2023) and AGIEval(Zhong et al. 2023), leverages multiple\nchoices questions or cloze tasks to evaluate LLMs. The\nadvantage of this is that for multiple-choice questions and\ncloze tasks, the answers are definite, and the scoring can be\ndone automatically. While these methods excel in terms of\nknowledge coverage, we argue that they can not completely\nevaluate the fluency, coherence, and harmlessness of an\nLLM response simultaneously. To tackle the above issue,\nthere have also been studies that employ the LLM itself\nas an evaluator, such as BERTScore(Zhang et al. 2020),\nGPTScore(Fu et al. 2023), GptEvaluator(Wang et al. 2023a),\nFairEvaluators(Wang et al. 2023b), and GEval(Liu et al.\n2023). However, the evaluation results derived from LLM\noutputs often exhibit discrepancies compared to manual\nevaluations and are susceptible to factors such as response\nposition and length.\nManual Evaluations Using manually annotated data as\nan evaluation criterion is expensive but essential. Many\nstudies have incorporated a portion of manually annotated\ndata as an evaluation methodology. AlpacaFarm(Dubois\net al. 2023) proposed API LLMs to replace manual evalu-\nations. Chatbot Arena(Zheng et al. 2023) tried to compare\nthe differences between evaluation results from GPT-4 and\nhumans. In our research, we have also conducted a similar\ncomparison. Furthermore, we have examined the impact\nof different scoring methods, diverse annotator types, and\nvarious ranking systems on the evaluation results.\nDiscussion\nIn our study, we discover that the most distinguishing criteria\nfor evaluating LLMs are informativeness and accuracy.\nMoving forward, we will continue to prioritize these aspects\nin future evaluations.\nAdditionally, our research reveals that onsite star scoring\nwas the optimal manual evaluation method in terms of\naccuracy, consistency and alignment between human and\nLLM evaluator. We will prefer this method in future work.\nMeanwhile, automated evaluation can cover a large number\nof tasks in a short time and exhibits reasonable alignment\nwith humans. It could be a complementary approach.\nAnother point worth mentioning is that the difference\nbetween automated evaluation and manual evaluation is\nmost noticeable in subjective questions. Clearly, since\nthere’s no standard answer, evaluating LLM’s performance\nin subjective questions is a challenging task.\nAcknowledgments\nThe authors wish to thank the anonymous reviewers\nfor their helpful comments. This work was partially\nfunded by National Natural Science Foundation of China\n(No.62206057,61976056,62076069), Shanghai Rising-Star\nProgram (23QA1400200), Natural Science Foundation\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19621\nof Shanghai (23ZR1403500), and Program of Shanghai\nAcademic Research Leader under grant 22XD1401100.\nReferences\nAskell, A.; Bai, Y .; Chen, A.; Drain, D.; Ganguli, D.;\nHenighan, T.; Jones, A.; Joseph, N.; Mann, B.; DasSarma,\nN.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.; Kernion,\nJ.; Ndousse, K.; Olsson, C.; Amodei, D.; Brown, T.; Clark,\nJ.; McCandlish, S.; Olah, C.; and Kaplan, J. 2021. A\nGeneral Language Assistant as a Laboratory for Alignment.\narXiv:2112.00861.\nChang, Y .; Wang, X.; Wang, J.; Wu, Y .; Zhu, K.; Chen, H.;\nYang, L.; Yi, X.; Wang, C.; Wang, Y .; Ye, W.; Zhang, Y .;\nand Yu, P. 2023. A Survey on Evaluation of Large Language\nModels.\nDubois, Y .; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba,\nJ.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023.\nAlpacaFarm: A Simulation Framework for Methods that\nLearn from Human Feedback. arXiv:2305.14387.\nFu, J.; Ng, S.-K.; Jiang, Z.; and Liu, P. 2023. GPTScore:\nEvaluate as You Desire. arXiv:2302.04166.\nHendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika,\nM.; Song, D.; and Steinhardt, J. 2021. Measuring Massive\nMultitask Language Understanding. arXiv:2009.03300.\nHuang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.; Liu,\nJ.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He, J. 2023.\nC-Eval: A Multi-Level Multi-Discipline Chinese Evaluation\nSuite for Foundation Models. arXiv:2305.08322.\nKusner, M. J.; Sun, Y .; Kolkin, N. I.; and Weinberger, K. Q.\n2015. From Word Embeddings To Document Distances. In\nInternational Conference on Machine Learning.\nLiang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.;\nYasunaga, M.; Zhang, Y .; Narayanan, D.; Wu, Y .; Kumar,\nA.; Newman, B.; Yuan, B.; Yan, B.; Zhang, C.; Cosgrove,\nC.; Manning, C. D.; R ´e, C.; Acosta-Navas, D.; Hudson,\nD. A.; Zelikman, E.; Durmus, E.; Ladhak, F.; Rong, F.; Ren,\nH.; Yao, H.; Wang, J.; Santhanam, K.; Orr, L.; Zheng, L.;\nYuksekgonul, M.; Suzgun, M.; Kim, N.; Guha, N.; Chatterji,\nN.; Khattab, O.; Henderson, P.; Huang, Q.; Chi, R.; Xie,\nS. M.; Santurkar, S.; Ganguli, S.; Hashimoto, T.; Icard, T.;\nZhang, T.; Chaudhary, V .; Wang, W.; Li, X.; Mai, Y .; Zhang,\nY .; and Koreeda, Y . 2022. Holistic Evaluation of Language\nModels. arXiv:2211.09110.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic\nEvaluation of Summaries. In Annual Meeting of the\nAssociation for Computational Linguistics.\nLiu, Y .; Iter, D.; Xu, Y .; Wang, S.; Xu, R.; and Zhu, C. 2023.\nG-Eval: NLG Evaluation using GPT-4 with Better Human\nAlignment. arXiv:2303.16634.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine\nTranslation. In Proceedings of the 40th Annual Meeting\nof the Association for Computational Linguistics, 311–\n318. Philadelphia, Pennsylvania, USA: Association for\nComputational Linguistics.\nWang, J.; Liang, Y .; Meng, F.; Sun, Z.; Shi, H.; Li, Z.; Xu,\nJ.; Qu, J.; and Zhou, J. 2023a. Is ChatGPT a Good NLG\nEvaluator? A Preliminary Study. arXiv:2303.04048.\nWang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y .; Liu, Q.;\nLiu, T.; and Sui, Z. 2023b. Large Language Models are not\nFair Evaluators. arXiv:2305.17926.\nZhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and Artzi,\nY . 2020. BERTScore: Evaluating Text Generation with\nBERT. arXiv:1904.09675.\nZhao, W.; Peyrard, M.; Liu, F.; Gao, Y .; Meyer, C. M.;\nand Eger, S. 2019. MoverScore: Text Generation\nEvaluating with Contextualized Embeddings and Earth\nMover Distance. arXiv:1909.02622.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu,\nZ.; Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;\nZhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judging\nLLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv:2306.05685.\nZhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang,\nY .; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:\nA Human-Centric Benchmark for Evaluating Foundation\nModels. arXiv:2304.06364.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19622"
}