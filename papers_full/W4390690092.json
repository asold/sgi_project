{
  "title": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)",
  "url": "https://openalex.org/W4390690092",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4227625862",
      "name": "Pallagani, Vishal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2078628674",
      "name": "Roy, Kaushik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4294790144",
      "name": "Muppasani, Bharath",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222401614",
      "name": "Fabiano, Francesco",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222401617",
      "name": "Loreggia, Andrea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222839091",
      "name": "Murugesan, Keerthiram",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221565890",
      "name": "Srivastava, Biplav",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2824993815",
      "name": "Rossi, Francesca",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222401615",
      "name": "Horesh, Lior",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749286790",
      "name": "Sheth Amit",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4283330306",
    "https://openalex.org/W4366566341",
    "https://openalex.org/W4323572061",
    "https://openalex.org/W4378508644",
    "https://openalex.org/W4386557277",
    "https://openalex.org/W4380551927",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W4383472635",
    "https://openalex.org/W4387210286",
    "https://openalex.org/W4387891177",
    "https://openalex.org/W4312091902",
    "https://openalex.org/W4393160795",
    "https://openalex.org/W4401415888",
    "https://openalex.org/W4387294204",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4401042802",
    "https://openalex.org/W4396651299",
    "https://openalex.org/W4389520747",
    "https://openalex.org/W4387635374",
    "https://openalex.org/W4399177289",
    "https://openalex.org/W4389519818",
    "https://openalex.org/W4377371656",
    "https://openalex.org/W4385714628",
    "https://openalex.org/W4320559489",
    "https://openalex.org/W4361806741",
    "https://openalex.org/W4386270084",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4386269668",
    "https://openalex.org/W4323030044",
    "https://openalex.org/W4387928979",
    "https://openalex.org/W4401414355",
    "https://openalex.org/W4389665359",
    "https://openalex.org/W4384264726",
    "https://openalex.org/W4387076513",
    "https://openalex.org/W4387323291",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4383175539",
    "https://openalex.org/W4385570924",
    "https://openalex.org/W4391116601",
    "https://openalex.org/W4378510404",
    "https://openalex.org/W2623293810",
    "https://openalex.org/W4386875375",
    "https://openalex.org/W4310826397",
    "https://openalex.org/W4386080690",
    "https://openalex.org/W4387724762",
    "https://openalex.org/W4386528753",
    "https://openalex.org/W4229376579",
    "https://openalex.org/W4323706372",
    "https://openalex.org/W4388660746",
    "https://openalex.org/W4387595998",
    "https://openalex.org/W4379474838",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W4381252132",
    "https://openalex.org/W4389217265",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4387224492",
    "https://openalex.org/W4388328949",
    "https://openalex.org/W2903021817",
    "https://openalex.org/W4385848437",
    "https://openalex.org/W4377161541",
    "https://openalex.org/W4386978014",
    "https://openalex.org/W4401416363",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4364384540",
    "https://openalex.org/W4312594400",
    "https://openalex.org/W4387782141",
    "https://openalex.org/W4386148475",
    "https://openalex.org/W4378501678",
    "https://openalex.org/W4383604274",
    "https://openalex.org/W4385876383",
    "https://openalex.org/W4387294301",
    "https://openalex.org/W4372272282",
    "https://openalex.org/W4323066451",
    "https://openalex.org/W4387560408",
    "https://openalex.org/W4387156645",
    "https://openalex.org/W4386907096",
    "https://openalex.org/W4387394686",
    "https://openalex.org/W4385764285",
    "https://openalex.org/W4386644268",
    "https://openalex.org/W4378770584",
    "https://openalex.org/W4378499090",
    "https://openalex.org/W4387635343",
    "https://openalex.org/W4393160302",
    "https://openalex.org/W4312045619",
    "https://openalex.org/W4319323461",
    "https://openalex.org/W2073266192",
    "https://openalex.org/W4366999541",
    "https://openalex.org/W4319322519",
    "https://openalex.org/W2149361370",
    "https://openalex.org/W4366327559",
    "https://openalex.org/W4387322659",
    "https://openalex.org/W4368754767",
    "https://openalex.org/W4383108895",
    "https://openalex.org/W4385571951",
    "https://openalex.org/W4386251907",
    "https://openalex.org/W4387210453",
    "https://openalex.org/W4389665836",
    "https://openalex.org/W4383473935",
    "https://openalex.org/W4387156684",
    "https://openalex.org/W4321011818",
    "https://openalex.org/W4401415105",
    "https://openalex.org/W4381827075",
    "https://openalex.org/W4384268338",
    "https://openalex.org/W4388555496",
    "https://openalex.org/W4387226207",
    "https://openalex.org/W4378715768",
    "https://openalex.org/W4378718493",
    "https://openalex.org/W4386184855",
    "https://openalex.org/W4385681800",
    "https://openalex.org/W4385966440",
    "https://openalex.org/W4389600506",
    "https://openalex.org/W3111885891",
    "https://openalex.org/W4385749697",
    "https://openalex.org/W4318751399",
    "https://openalex.org/W4378945633",
    "https://openalex.org/W4387636042",
    "https://openalex.org/W4389667254",
    "https://openalex.org/W4379924706",
    "https://openalex.org/W4378473969",
    "https://openalex.org/W4401415213",
    "https://openalex.org/W4393160744",
    "https://openalex.org/W4387891782",
    "https://openalex.org/W2101512350",
    "https://openalex.org/W4387210427",
    "https://openalex.org/W4386215566",
    "https://openalex.org/W4386907176",
    "https://openalex.org/W4382603221",
    "https://openalex.org/W4377723004"
  ],
  "abstract": "Automated Planning and Scheduling is among the growing areas in Artificial Intelligence (AI) where mention of LLMs has gained popularity. Based on a comprehensive review of 126 papers, this paper investigates eight categories based on the unique applications of LLMs in addressing various aspects of planning problems: language translation, plan generation, model construction, multi-agent planning, interactive planning, heuristics optimization, tool integration, and brain-inspired planning. For each category, we articulate the issues considered and existing gaps. A critical insight resulting from our review is that the true potential of LLMs unfolds when they are integrated with traditional symbolic planners, pointing towards a promising neuro-symbolic approach. This approach effectively combines the generative aspects of LLMs with the precision of classical planning methods. By synthesizing insights from existing literature, we underline the potential of this integration to address complex planning challenges. Our goal is to encourage the ICAPS community to recognize the complementary strengths of LLMs and symbolic planners, advocating for a direction in automated planning that leverages these synergistic capabilities to develop more advanced and intelligent planning systems.",
  "full_text": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated\nPlanning and Scheduling (APS)\nVishal Pallagani1, Kaushik Roy1, Bharath Muppasani1, Francesco Fabiano2, Andrea Loreggia3,\nKeerthiram Murugesan4, Biplav Srivastava1, Francesca Rossi4, Lior Horesh4, Amit Sheth1\n1University of South Carolina\n2New Mexico State University\n3University of Brescia\n4IBM Research\nAbstract\nAutomated Planning and Scheduling is among the growing\nareas in Artificial Intelligence (AI) where mention of LLMs\nhas gained popularity. Based on a comprehensive review of\n126 papers, this paper investigates eight categories based on\nthe unique applications of LLMs in addressing various as-\npects of planning problems: language translation, plan gen-\neration, model construction, multi-agent planning, interactive\nplanning, heuristics optimization, tool integration, and brain-\ninspired planning. For each category, we articulate the issues\nconsidered and existing gaps. A critical insight resulting from\nour review is that the true potential of LLMs unfolds when\nthey are integrated with traditional symbolic planners, point-\ning towards a promising neuro-symbolic approach. This ap-\nproach effectively combines the generative aspects of LLMs\nwith the precision of classical planning methods. By synthe-\nsizing insights from existing literature, we underline the po-\ntential of this integration to address complex planning chal-\nlenges. Our goal is to encourage the ICAPS community to\nrecognize the complementary strengths of LLMs and sym-\nbolic planners, advocating for a direction in automated plan-\nning that leverages these synergistic capabilities to develop\nmore advanced and intelligent planning systems.\nIntroduction\nAs a sub-field of Artificial Intelligence (Russell and Norvig\n2003), Automated Planning and Scheduling (Ghallab, Nau,\nand Traverso 2004) refers to developing algorithms and sys-\ntems to generate plans or sequences of actions to achieve\nspecific goals in a given environment or problem domain.\nAPS is a valuable tool in domains where there is a need\nfor intelligent decision-making, goal achievement, and effi-\ncient resource utilization. It enables the automation of com-\nplex tasks, making systems more capable and adaptable in\ndynamic environments. Over time, APS has evolved from\nthe early development of robust theoretical foundations to\npractical applications in diverse sectors like manufacturing,\nspace exploration, and personal scheduling. This evolution\nunderscores the versatility and critical significance of APS.\nIn parallel with advancements in APS, the development\nand proliferation of LLMs have marked a substantial leap\nin AI, particularly within computational linguistics. Evolv-\ning from early efforts in natural language processing (NLP),\nLLMs have undergone significant transformation. Initially\nfocused on basic tasks like word prediction and syntax\nanalysis, newer models are characterized by their ability\nto generate coherent, contextually relevant text and per-\nform diverse, complex linguistic tasks. Trained on exten-\nsive text corpora, LLMs have mastered human-like lan-\nguage patterns. Their recent success in various NLP tasks\nhas prompted efforts to apply these models in APS. There\nis a notable shift towards using language constructs to spec-\nify aspects of planning, such as preconditions, effects, and\ngoals, rather than relying solely on traditional planning do-\nmain languages like PDDL.\nThis paper presents an exhaustive literature review explor-\ning the integration of LLMs in APS across eight categories:\nLanguage Translation, Plan Generation, Model Construc-\ntion, Multi-agent Planning, Interactive Planning, Heuristics\nOptimization, Brain-Inspired Planning, and Tool Integra-\ntion. Table 1 provides the description for the eight cate-\ngories. Our comprehensive analysis of 126 papers not only\ncategorizes LLMs’ diverse contributions but also identifies\nsignificant gaps in each domain. Through our review, we put\nforward the following position:\nPosition Statement\nIntegrating LLMs into APS marks a pivotal advance-\nment, bridging the gap between the advanced reason-\ning of traditional APS and the nuanced language un-\nderstanding of LLMs. Traditional APS systems excel\nin structured, logical planning but often lack flexibil-\nity and contextual adaptability, a gap readily filled by\nLLMs. Conversely, while LLMs offer unparalleled nat-\nural language processing and a vast knowledge base,\nthey fail to generate precise, actionable plans where\nAPS systems thrive. This integration surpasses the lim-\nitations of each standalone method, offering a dynamic\nand context-aware planning approach, while also scal-\ning up the traditional use of data and past experiences\nin the planning process.\nIn the forthcoming sections, we delve into the background\nof LLMs and classical planning problem, accompanied by\nthe identification of literature. This sets the stage for an in-\ndepth exploration of the application of LLMs in APS, where\nwe critically examine the strengths and limitations of LLMs.\nOur position on the emerging neuro-symbolic AI paradigm\narXiv:2401.02500v2  [cs.AI]  20 Jan 2024\nCategory Description\nLanguage Translation Involves converting natural language into structured planning languages or formats like PDDL and\nvice-versa, enhancing the interface between human linguistic input and machine-understandable\nplanning directives.\nPlan Generation Entails the creation of plans or strategies directly by LLMs, focusing on generating actionable se-\nquences or decision-making processes.\nModel Construction Utilizes LLMs to construct or refine world and domain models essential for accurate and effective\nplanning.\nMulti-agent Planning Focuses on scenarios involving multiple agents, where LLMs contribute to coordination and coop-\nerative strategy development.\nInteractive Planning Centers on scenarios requiring iterative feedback or interactive planning with users, external veri-\nfiers, or environment, emphasizing the adaptability of LLMs to dynamic inputs.\nHeuristics Optimization Applies LLMs in optimizing planning processes through refining existing plans or providing heuris-\ntic assistance to symbolic planners.\nTool Integration Encompasses studies where LLMs act as central orchestrators or coordinators in a tool ecosystem,\ninterfacing with planners, theorem provers, and other systems.\nBrain-Inspired Planning Covers research focusing on LLM architectures inspired by neurological or cognitive processes,\nparticularly to enhance planning capabilities.\nTable 1: Comprehensive description of the eight categories utilizing LLMs in APS\nis central to our discussion, highlighting its unique advan-\ntages over purely neural network-based (i.e., statistical AI)\nor symbolic AI approaches. Finally, we will discuss prospec-\ntive developments, address potential challenges, and identify\npromising opportunities in the field.\nBackground\nLarge Language Models\nLarge language models are neural network models with up-\nwards of ∼ 3 billion parameters that are trained on extremely\nlarge corpora of natural language data (trillions of tokens/-\nwords). These models are proficient in interpreting, gener-\nating, and contextualizing human language, leading to ap-\nplications ranging from text generation to language-driven\nreasoning tasks. The evolution of LLMs in NLP began with\nrule-based models, progressed through statistical models,\nand achieved a significant breakthrough with the introduc-\ntion of neural network-based models. The shift to sequence-\nbased neural networks, with Recurrent Neural Networks\n(RNNs) and Long Short-Term Memory (LSTM) networks,\nmarked a notable advancement due to their capability to pro-\ncess information and context over long sequences. Short-\ncomings in RNNs and LSTMs due to vanishing gradients\nand, consequently, loss of very long sequence contexts lead\nto the transformer model, which introduced self-attention\n(SA) mechanisms. The SA mechanism enabled focus on\ndifferent parts of a long input sequence in parallel, which\nenhanced understanding of contextual nuances in language\npatterns over extremely long sequences. The SA mechanism\nis also complemented with positional encodings in trans-\nformers to enable the model to maintain an awareness of\nword/token order, which is required to understand accurate\ngrammar and syntax. The self-attention mechanism, central\nto transformers, uses a query, key, and value system to con-\ntextualize dependencies in the input sequence. Informally,\nthe SA concept is inspired by classical information retrieval\nsystems where the query is the input sequence context, the\nkey refers to a “database” contained within the parametric\nmemory, and the value is the actual value present at that ref-\nerence. The operation is mathematically expressed in Equa-\ntion 1.\nAttention(Q, K, V) =softmax\n\u0012QKT\n√dk\n\u0013\nV (1)\nIn this equation, Q, K, and V denote the query, key, and\nvalue matrices. The scaling factor √dk, where dk is the di-\nmension of the keys, is employed to standardize the vectors\nto unit variance for ensuring stable softmax gradients during\ntraining. Since the introduction of LLMs with self-attention,\nthere have been several architectural variants depending on\nthe downstream tasks.\nCausal Language Modeling (CLMs): CLMs, such as GPT-\n4, are designed for tasks where text generation is sequen-\ntial and dependent on the preceding context. They predict\neach subsequent word based on the preceding words, mod-\neling the probability of a word sequence in a forward direc-\ntion. This process is mathematically formulated as shown in\nEquation 2.\nP(T) =\nnY\ni=1\nP(ti|t<i) (2)\nIn this formulation, P(ti|t<i) represents the probability\nof the i-th token given all preceding tokens, t<i. This char-\nacteristic makes CLMs particularly suitable for applications\nlike content generation, where the flow and coherence of the\ntext in the forward direction are crucial.\nMasked Language Modeling (MLMs) : Unlike CLMs,\nMLMs like BERT are trained to understand the bidirectional\ncontext by predicting words randomly masked in a sentence.\nThis approach allows the model to learn both forward and\nbackward dependencies in language structure. The MLM\nprediction process can be represented as Equation 3.\nP(Tmasked|Tcontext) =\nY\ni∈M\nP(ti|Tcontext) (3)\nHere, Tmasked is the set of masked tokens in the sentence,\nTcontext represents the unmasked part of the sentence, andM\nis the set of masked positions. MLMs have proven effective\nin NLP tasks such as sentiment analysis or question answer-\ning.\nSequence-to-Sequence (Seq2Seq) Modeling : Seq2Seq\nmodels, like T5, are designed to transform an input sequence\ninto a related output sequence. They are often employed in\ntasks that require a mapping between different types of se-\nquences, such as language translation or summarization. The\nSeq2Seq process is formulated as Equation 4.\nP(Toutput|Tinput) =\nmY\ni=1\nP(toutputi|Tinput, toutput<i) (4)\nIn Equation 4, Tinput is the input sequence, Toutput is the\noutput sequence, and P(toutputi|Tinput, toutput<i) calculates\nthe probability of generating each token in the output se-\nquence, considering both the input sequence and the preced-\ning tokens in the output sequence.\nIn addition to their architectural variants, the utility of\nLLMs is further enhanced by specific model utilization\nstrategies, enabling their effective adaptation to various do-\nmains at scale. One key strategy is fine-tuning, which ap-\nplies to pre-trained LLMs. Pre-trained LLMs are models\nalready trained on large datasets to understand and gener-\nate language, acquiring a broad linguistic knowledge base.\nFine-tuning involves further training pre-trained LLMs on\na smaller, task-specific dataset, thereby adjusting the neural\nnetwork weights for particular applications. This process is\nmathematically represented in Equation 5.\nθfine-tuned = θpre-trained − η · ∇θL(θ, Dtask) (5)\nHere, θfine-tuned are the model parameters after fine-tuning,\nθpre-trained are the parameters obtained from pre-training, η is\nthe learning rate, and ∇θL(θ, Dtask) denotes the gradient of\nthe loss function L with respect to the parameters θ on the\ntask-specific dataset Dtask.\nP(T|C) =\nnY\ni=1\nP(ti|t<i, C) (6)\nComplementing the fine-tuning approach is in-context\nlearning, an alternative strategy that is particularly char-\nacteristic of models like the GPT series. This method di-\nverges from fine-tuning by enabling the model to adapt its\nresponses based on immediate context or prompts with-\nout necessitating further training. The efficacy of in-context\nlearning is a direct consequence of the comprehensive pre-\ntraining phase, where models are exposed to diverse textual\ndatasets, thereby acquiring a nuanced understanding of lan-\nguage and context. Given a context C, the model generates\ntext T that is contextually relevant, as shown in Equation 6.\nHere, P(T|C) is the probability of generating text T given\nthe context C, and P(ti|t<i, C) is the probability of gener-\nating the i-th token ti given the preceding tokenst<i and the\ncontext C.\nFigure 1: Radar chart showcasing the relative performance\nof six language models (GPT-4, Claude-v1, GPT-3.5-turbo,\nVicuna-13B, Alpaca-13B, LLama-13B) across key domains:\nWriting, Roleplay, Reasoning, Math, Coding, Extraction,\nSTEM, and Humanities from Zheng et al. (2023a).\nThese diverse model types and training methodologies\nunder the umbrella of LLMs showcase the flexibility and\nadaptability of language models in handling a wide range of\ncomplex tasks. Figure 1 illustrates the comparative capabil-\nities of different LLMs across various competency domains,\nsuch as Writing (evaluating text generation quality), Role-\nplay (assessing conversational interaction), Reasoning (log-\nical problem-solving), Math (numerical problem-solving),\nCoding (programming language understanding and gener-\nation), Extraction (information retrieval from text), STEM\n(proficiency in scientific and technical contexts), and Hu-\nmanities (engagement with arts, history, and social sci-\nences content). Across these domains, GPT-4 exhibits the\nstrongest performance in the benchmark dataset evaluated\nby Zheng et al. (2023a), indicative of its superior training\nand extensive knowledge base. Expanding LLMs into ap-\nplications such as code generation signifies their adaptabil-\nity and potential for cross-disciplinary innovation. However,\nfine-tuning and in-context learning methodologies also bring\nchallenges, such as potential data overfitting and reliance\non the quality of input context. LLMs’ continuous develop-\nment and refinement promise to open new frontiers in vari-\nous domains, including automated planning and scheduling,\nby bridging AI with human-like language understanding.\nAutomated Planning and Scheduling\nAPS is a branch of AI that focuses on the creation of\nstrategies or action sequences, typically for execution by\nintelligent agents, autonomous robots, and unmanned ve-\nFigure 2: Of the 126 papers surveyed in this study, 55 were accepted by peer-reviewed conferences. This chart illustrates the\ndistribution of these papers across various conferences in the fields of LLMs and APS, highlighting the primary forums for\nscholarly contributions in these areas.\nhicles. A basic category in APS is a Classical Planning\nProblem (CPP) (Russell and Norvig 2003) which is a tu-\nple M = ⟨D, I, G⟩ with domain D = ⟨F, A⟩ - where F\nis a set of fluents that define a state s ⊆ F, and A is a\nset of actions - and initial and goal states I, G ⊆F. Ac-\ntion a ∈ A is a tuple (ca, pre(a), eff±(a)) where ca is the\ncost, and pre(a), eff±(a) ⊆ F are the preconditions and\nadd/delete effects, i.e., δM(s, a) |= ⊥s if s ̸|= pre(a);\nelse δM(s, a) |= s ∪ eff+(a) \\ eff−(a) where δM(·) is\nthe transition function. The cumulative transition function\nis δM(s, (a1, a2, . . . , an)) = δM(δM(s, a1), (a2, . . . , an)).\nA plan for a CPP is a sequence of actions ⟨a1, a2, . . . , an⟩\nthat transforms the initial state I into the goal state G us-\ning the transition function δM. Traditionally, a CPP is en-\ncoded using a symbolic representation, where states, ac-\ntions, and transitions are explicitly enumerated. This sym-\nbolic approach, often implemented using Planning Domain\nDefinition Language or PDDL (McDermott et al. 1998),\nensures precise and unambiguous descriptions of planning\nproblems. This formalism allows for applying search algo-\nrithms and heuristic methods to find a sequence of actions\nthat lead to the goal state, which is the essence of the plan.\nThe advent of LLMs has sparked a significant evolution\nin representation methods for CPPs, moving towards lever-\naging the expressive power of natural language (Valmeekam\net al. 2023a) and the perceptual capabilities of vision (Asai\n2018). These novel approaches, inherently more suited for\nLLM processing, use text and vision-based representations,\nallowing researchers to utilize the pre-existing knowledge\nwithin LLMs. This shift enables a more humanistic com-\nprehension and reasoning about planning tasks, enhanc-\ning the flexibility and applicability of planning algorithms\nin complex, dynamic environments. LLMs, while distinct\nin being trained on vast datasets outside the traditional\nscope of planning, loosely connect to previous data-driven\nmethodologies, such as case-based reasoning (Xu 1995) ap-\nplied to planning and Hierarchical Task Network (HTN)\n(Georgievski and Aiello 2015) which make use of task\nknowledge. It is an open area how LLMs may be used syn-\nergestically with prior methods.\nLLMs in APS – Literature selection\nA comprehensive survey of existing literature was con-\nducted to explore the application of LLMs for automated\nplanning. This endeavor led to identifying 126 pertinent re-\nsearch papers showcasing various methodologies, applica-\ntions, and theoretical insights into utilizing LLMs within this\ndomain.\nThe selection of these papers was guided by stringent\ncriteria, focusing primarily on their relevance to the core\ntheme of LLMs in automated planning. The search, con-\nducted across multiple academic databases and journals, was\nsteered by keywords such as “Large Language Models”,\n“Automated Planning”, “LLMs in Planning”, and “LLMs\n+ Robotics”. Figure 2 presents the distribution of these se-\nlected papers across various peer-reviewed conferences, un-\nderlining the breadth and diversity of forums addressing the\nintersection of LLMs and APS. Even if a paper originated\nfrom a workshop within a conference, only the conference\nname is listed. Out of 126 papers, 71 are under review or\navailable on arXiv. The inclusion criteria prioritized the rele-\nvance and contribution of papers to automated planning with\nLLMs over the publication date. Nonetheless, all surveyed\npapers emerged from either 2022 or 2023, a trend depicted in\nFigure 3, underscoring the recent surge in LLM research. A\nword cloud was generated to visually capture the prevalent\nresearch themes reflected in these papers’ titles, illustrated\nin Figure 4. This cloud highlights the frequent use of terms\nsuch as “Language Model” and “Planning”, which dominate\nthe current discourse. In contrast, the emergence of “Neuro-\nSymbolic” reflects a nascent yet growing interest in integrat-\ning neural and symbolic approaches within the field. This\nsystematic approach ensured a comprehensive inclusion of\nseminal works and recent advancements.\nUpon the accumulation of these papers, a meticulous\nmanual categorization was undertaken. The papers were di-\nvided into four piles, each containing approximately 30 pa-\npers. Each pile was manually categorized by one author,\nFigure 3: Annual distribution of the 126 surveyed papers,\nindicating a significant increase in publications from 12 in\n2022 to 114 in 2023, highlighting the rapid growth of LLM\nresearch within a single year.\nwith the final categorization being reviewed by all authors.\nDuring this process, each paper could belong to multiple cat-\negories out of the eight established. The maximum number\nof categories assigned to a single paper was three, although\nthe median was typically one category per paper. This pro-\ncess was pivotal in distilling the vast information into co-\nherent, thematic groups. The categorization was conducted\nbased on the specific application of LLMs in planning. This\nformed eight distinct categories, each representing a unique\nfacet of LLM application in automated planning. These cat-\negories facilitate a structured analysis and highlight LLMs’\ndiverse applications and theoretical underpinnings in this\nfield.\nFigure 4: Word cloud of terms from the titles of papers sur-\nveyed in this study, displaying the prevalence of “Language\nModel” and “Planning” as central themes. The presence of\n“Neuro-Symbolic” indicates an emergent trend toward the\nfusion of neural and symbolic methodologies in the domain.\nLLMs in APS – Literature Discussion\nThis section dwelves into the diverse applications of LLMs\nin planning tasks. We have identified eight distinct cate-\ngories based on the utility and application of LLMs in plan-\nning, which are concisely summarized in Table 1. Figure 5\nprovides a detailed taxonomy, illustrating the categorization\nof the identified research papers.\nLanguage Translation\nLanguage translation in the context of LLMs and plan-\nning involves transforming natural language instructions\ninto structured planning languages (Wong et al. 2023; Kelly\net al. 2023; Yang 2023; Pan et al. 2023; Xie et al. 2023;\nYang, Ishay, and Lee 2023; Lin et al. 2023c; Sakib and\nSun 2023; Yang et al. 2023b; Parakh et al. 2023; Yang\net al. 2023a; Dai et al. 2023; Ding et al. 2023b; Zelikman\net al. 2023; Xu et al. 2023b; Chen et al. 2023a; You et al.\n2023) such as PDDL, and vice versa, utilizing in-context\nlearning techniques (Guan et al. 2023). This capability ef-\nfectively bridges the gap between human linguistic expres-\nsion and machine-understandable formats, enhancing intu-\nitive and efficient planning processes. The LLM+P frame-\nwork (Liu et al. 2023) exemplifies this by converting natu-\nral language descriptions of planning problems into PDDL\nusing GPT-4, leveraging classical planners for solution find-\ning, and then translating these solutions back into natural\nlanguage, with a specific focus on robot planning scenarios.\nAdditionally, Graph2NL (Chalvatzaki et al. 2023) generates\nnatural language text from scene graphs for long-horizon\nrobot reasoning tasks, while (Shirai et al. 2023) introduces a\nvision-to-language interpreter for robot task planning. Fur-\nther, (Brohan et al. 2023) examines the grounding of LLM-\ngenerated natural language utterances in actionable robot\ntasks, and (Yang, Gaglione, and Topcu 2022) utilizes LLMs\nfor creating finite-state automatons for sequential decision-\nmaking problems. Despite these advancements, a critical re-\nsearch gap emerges in the autonomous translation capabili-\nties of LLMs, particularly in converting natural language to\nPDDL without external expert intervention.\nWhile LLMs effectively translate PDDL to natural language,\na notable gap is evident in their limited understanding of\nreal-world objects and the problem of grounding affor-\ndances, mainly when translating natural language to structured\nlanguages like PDDL. Addressing this gap calls for integrat-\ning neuro-symbolic approaches in LLMs, where the fusion of\nperceptual experience for concrete concept understanding from\nknowledge graphs complements LLMs’ proficiency in distri-\nbutional statistics (Lenat and Marcus 2023).\nPlan Generation\nThis category focuses on directly generating plans using\nLLMs. The research, primarily utilizing causal language\nmodels through in-context learning (Sermanet et al. 2023;\nLi et al. 2023b; Silver et al. 2023; Parakh et al. 2023; Ze-\nlikman et al. 2023; Besta et al. 2023; Huang et al. 2023a;\nDalal et al. 2023; Wang et al. 2023b; Valmeekam et al. 2022;\nValmeekam, Marquez, and Kambhampati 2023; Gramopad-\nhye and Szafir 2022; Singh et al. 2023)1, demonstrates mod-\nest performance, indicating notable challenges in employ-\ning LLMs for effective plan generation. Novel in-context\nlearning strategies, such as the Chain-of-Symbol and Tree\nof Thoughts, have been introduced to enhance LLMs’ rea-\nsoning capabilities (Hu et al. 2023b; Yao et al. 2023). Ef-\n1Due to space constraints, only a select number of papers are\ncited in this section.\nApplication of LLMs in Planning\nLanguage\nTranslation\n(23)\n(Liu et al. 2023; Xie et al. 2023; Guan et al. 2023; Chalvatzaki\net al. 2023; Yang, Ishay, and Lee 2023; Wong et al. 2023; Kelly\net al. 2023; Yang 2023; Lin et al. 2023c; Sakib and Sun 2023;\nYang et al. 2023b; Parakh et al. 2023; Dai et al. 2023; Yang\net al. 2023a; Shirai et al. 2023; Ding et al. 2023b; Zelikman et al.\n2023; Pan et al. 2023; Xu et al. 2023b; Brohan et al. 2023; Yang,\nGaglione, and Topcu 2022; Chen et al. 2023a; You et al. 2023)\nPlan Generation\n(53)\n(Sermanet et al. 2023; Li et al. 2023b; Pallagani et al. 2022;\nSilver et al. 2023; Pallagani et al. 2023b; Arora and Kambhampati\n2023; Fabiano et al. 2023; Chalvatzaki et al. 2023; Gu et al.\n2023; Silver et al. 2022; Hao et al. 2023a; Lin et al. 2023b; Yuan\net al. 2023b; Gandhi, Sadigh, and Goodman 2023; Joublin et al.\n2023; Chen et al. 2023c; Zhang, Jin, and Zhuo 2023; Capitanelli\nand Mastrogiovanni 2023; Yang and Tomar 2023; Song et al.\n2023; Dagan, Keller, and Lascarides 2023; Kannan, Venkatesh,\nand Min 2023; Valmeekam et al. 2023b; Rana et al. 2023; Tang\net al. 2023; Singh et al. 2023; Wang et al. 2023d; Huang et al.\n2022a; Rajvanshi et al. 2023; Ding et al. 2023c; Lu et al. 2023b;\nWu et al. 2023a; Wang et al. 2023a,c; Kim et al. 2023; Hu et al.\n2023b; Zhang et al. 2023c; Kant et al. 2022; Luo et al. 2023;\nPallagani et al. 2023a; Huang et al. 2023b; Sarkisyan et al. 2023;\nLu et al. 2022; Parakh et al. 2023; Zelikman et al. 2023; Besta\net al. 2023; Huang et al. 2023a; Dalal et al. 2023; Wang et al.\n2023b; Yao et al. 2023; Valmeekam et al. 2022; Valmeekam,\nMarquez, and Kambhampati 2023; Gramopadhye and Szafir 2022)\nModel\nConstruction (17)\n(Nottingham et al. 2023; Hao et al. 2023a; Zhang and Soh\n2023; Wong et al. 2023; Kelly et al. 2023; Mandi, Jain, and\nSong 2023; Hu et al. 2023a; Zhao, Lee, and Hsu 2023; Yoneda\net al. 2023; Wu et al. 2023b; Ding et al. 2023b; Huang et al.\n2023a; Yuan et al. 2023a; Xu et al. 2023b; Kirk, Wray, and\nLaird 2023; Brohan et al. 2023; Gragera and Pozanco 2023)\nMulti-agent\nPlanning (4)\n(Zhang et al. 2023b; Wei et al. 2023; Chen et al.\n2023d; Abdelnabi et al. 2023; Hua et al. 2023)\nInteractive\nPlanning (21)\n(Guan et al. 2023; Arora and Kambhampati 2023; Carta et al.\n2023; Zhou et al. 2023; Jha et al. 2023; Chen et al. 2023b;\nHuang et al. 2022b; Rana et al. 2023; Ren et al. 2023; Hu\net al. 2023c; Wang et al. 2023c; Kim et al. 2023; Hu et al.\n2023b; Raman et al. 2022; Wu, Ai, and Hsu 2023; Graule\nand Isler 2023; Liu, Bahety, and Song 2023; Driess et al.\n2023; Naik et al. 2023; Sun et al. 2023; Zheng et al. 2023b)\nHeuristics\nOptimization (8)\n(Hazra, Martires, and De Raedt 2023; Silver et al. 2022;\nHao et al. 2023a; Raimondo et al. 2023; Valmeekam et al.\n2023b; Shah et al. 2023; Dai et al. 2023; Feng et al. 2023)\nTool Integration\n(8)\n(Xu et al. 2023a; Ruan et al. 2023; Li et al.\n2023a; Lu et al. 2023a; Hsieh et al. 2023; Shen\net al. 2023; Hao et al. 2023b; Ge et al. 2023)\nBrain-inspired\nPlanning (5)\n(Webb et al. 2023; Sumers et al. 2023; Momenne-\njad et al. 2023; Hu et al. 2023d; Lin et al. 2023a)\nFigure 5: Taxonomy of recent research in the intersection of LLMs and Planning into categories (#). Each has scholarly papers\nbased on their unique application or customization of LLMs in addressing various aspects of planning problems.\nforts to generate multimodal, text, and image-based goal-\nconditioned plans are exemplified by (Lu et al. 2023b). Ad-\nditionally, a subset of studies in this survey investigates the\nfine-tuning of seq2seq, code-based language models (Pal-\nlagani et al. 2022, 2023b), which are noted for their ad-\nvanced syntactic encoding. These models show promise in\nimproving plan generation within the confines of their train-\ning datasets (Logeswaran et al. 2023), yet exhibit limita-\ntions in generalizing to out-of-distribution domains (Palla-\ngani et al. 2023a), highlighting a gap in their adaptability\nacross diverse planning contexts.\nCausal LLMs are predominantly used for plan generation, but\ntheir performance is often limited due to their design, which\nis focused on generating text based on preceding input. On\nthe other hand, seq2seq LLMs can generate valid plans but\nstruggle with generalization across diverse domains. This\nlimitation highlights an opportunity for a synergistic approach:\nintegrating even imperfect LLM outputs with symbolic plan-\nners can expedite heuristic searches, thereby enhancing effi-\nciency and reducing search times (Fabiano et al. 2023).\nModel Construction\nThis category employs LLMs to build or refine world and\ndomain models essential for accurate planning. Nottingham\net al. (2023); Yuan et al. (2023a) leverage in-context learn-\ning with LLMs to develop an abstract world model in the\nMinecraft domain, highlighting the challenge of semantic\ngrounding in LLMs. Similarly, Gragera and Pozanco (2023)\nexplore the capability of LLMs in completing ill-defined\nPDDL domains. Efforts such as (Huang et al. 2023a; Bro-\nhan et al. 2023) delve into LLMs’ grounding capabilities,\nwith SayCan (Brohan et al. 2023) notably achieving 74%\nexecutable plans. Hao et al. (2023a); Yoneda et al. (2023)\ninnovatively positions LLMs as both world models and rea-\nsoning agents, enabling the simulation of world states and\nprediction of action outcomes. Research by (Zhang and Soh\n2023; Wong et al. 2023; Mandi, Jain, and Song 2023; Hu\net al. 2023a; Zhao, Lee, and Hsu 2023; Ding et al. 2023b;\nHuang et al. 2023a; Wu et al. 2023b; Xu et al. 2023b; Brohan\net al. 2023) shows that LLMs can effectively model high-\nlevel human states and behaviors using their commonsense\nknowledge. Yet, they face difficulties accurately processing\nlow-level geometrical or shape features due to spatial and\nnumerical reasoning constraints. Additionally, Kelly et al.\n(2023) investigates the potential of LLMs in conjunction\nwith planners to craft narratives and logical story models,\nintegrating human-in-the-loop for iterative edits.\nLLMs often struggle with detailed spatial reasoning and\nprocessing low-level environmental features, limiting their\neffectiveness in model construction. Integrating world mod-\nels presents a viable solution, offering advanced abstractions\nfor reasoning that encompass human-like cognitive elements\nand interactions, thereby enhancing LLMs’ capabilities in\nmodel construction (Hu and Shu 2023).\nMulti-agent Planning\nIn multi-agent planning, LLMs play a vital role in scenarios\ninvolving interaction among multiple agents, typically mod-\neled using distinct LLMs. These models enhance coordina-\ntion and cooperation, leading to more complex and effective\nmulti-agent strategies. (Zhang et al. 2023b) introduces an in-\nnovative framework that employs LLMs to develop coopera-\ntive embodied agents. AutoGraph (Wei et al. 2023) leverages\nLLMs to generate autonomous agents adept at devising solu-\ntions for varied graph-structured data problems. Addressing\nscalability in multi-robot task planning, (Chen et al. 2023d)\nproposes frameworks for the collaborative function of differ-\nent LLM-based agents. Furthermore, (Abdelnabi et al. 2023)\nand (Hua et al. 2023) collectively demonstrate the effective-\nness of LLM agents in complex negotiation and decision-\nmaking environments.\nA key gap in multi-agent planning with LLMs lies in stan-\ndardizing inter-agent communication and maintaining dis-\ntinct belief states, including human aspects. Overcoming\nthis requires advanced LLM algorithms for dynamic alignment\nof communication and belief states, drawing on epistemic rea-\nsoning and knowledge representation (de Zarz`a et al. 2023).\nInteractive Planning\nIn this category, LLMs are utilized in dynamic scenarios\nwhere real-time adaptability to user feedback or iterative\nplanning is essential. The refinement of LLM outputs is\ntypically achieved through four primary feedback variants:\n(a) External verifiers, such as V AL(Howey, Long, and Fox\n2004) for PDDL or scene descriptors and success detec-\ntors in robotics (Guan et al. 2023; Arora and Kambhampati\n2023; Jha et al. 2023; Huang et al. 2022b; Liu, Bahety, and\nSong 2023; Rana et al. 2023; Ren et al. 2023; Kim et al.\n2023; Graule and Isler 2023; Driess et al. 2023; Zheng et al.\n2023b); (b) Online reinforcement learning, which progres-\nsively updates the LLM about environmental changes (Carta\net al. 2023); (c) Self-refinement by LLMs, where they pro-\nvide feedback on their own outputs (Zhou et al. 2023; Hu\net al. 2023c,b; Ding et al. 2023a; Sun et al. 2023; Naik\net al. 2023); (d) Input from human experts (Raman et al.\n2022; Wu, Ai, and Hsu 2023). Furthermore, (Chen et al.\n2023b) introduces the “Action Before Action” method, en-\nabling LLMs to proactively seek relevant information from\nexternal sources in natural language, thereby improving em-\nbodied decision-making in LLMs by 40%.\nA key gap in interactive planning with LLMs lies in har-\nmonizing the “fast” neural processing of LLMs with “slow”\nsymbolic reasoning, as manifested in feedback mechanisms.\nThis integration is key to maintaining the neural speed of\nLLMs while effectively embedding the depth and precision\nof feedback, which is vital for accuracy in dynamic planning\nscenarios (Zhang et al. 2023a).\nHeuristics Optimization\nIn the realm of Heuristics Optimization, LLMs are leveraged\nto enhance planning processes, either by refining existing\nplans or aiding symbolic planners with heuristic guidance.\nStudies like (Hazra, Martires, and De Raedt 2023; Hao et al.\n2023a; Dai et al. 2023; Feng et al. 2023) have effectively\ncoupled LLMs with heuristic searches to identify optimal\naction sequences. Research by (Silver et al. 2022; Shah et al.\n2023; Valmeekam et al. 2023b) reveals that LLMs’ outputs,\neven if partially correct, can provide valuable direction for\nsymbolic planners such as LPG (Gerevini and Serina 2002),\nespecially in problems beyond the LLMs’ solvable scope.\nFurthermore, (Raimondo et al. 2023) makes an intriguing\nobservation that including workflows and action plans in\nLLM input prompts can notably enhance task-oriented di-\nalogue generalization.\nThis category marks significant progress towards realizing\nneuro-symbolic approaches in APS. Current methods em-\nphasize plan validity, often at the expense of time efficiency.\nFuture research should look at how to continually evolve LLMs\nfor better plan generation, with its experience from compli-\nmenting symbolic planners (Du et al. 2023).\nTool Integration\nIn tool integration, LLMs serve as coordinators within a\ndiverse array of planning tools, enhancing functionality in\ncomplex scenarios. Studies like (Xu et al. 2023a; Lu et al.\n2023a; Shen et al. 2023; Hao et al. 2023b; Ge et al. 2023)\ndemonstrate that incorporating tools such as web search en-\ngines, Python functions, and API endpoints enhances LLM\nreasoning abilities. However, (Ruan et al. 2023) notes a ten-\ndency for LLMs to over-rely on specific tools, potentially\nprolonging the planning process. (Li et al. 2023a) intro-\nduces a benchmark for tool-augmented LLMs. While typ-\nical approaches involve teaching LLMs tool usage via mul-\ntiple prompts, (Hsieh et al. 2023) suggests that leveraging\ntool documentation offers improved planning capabilities,\ncircumventing the need for extensive demonstrations.\nLLMs often hallucinate non-existent tools, overuse a single\ntool, and face scaling challenges with multiple tools. Over-\ncoming these issues is key to enabling LLMs to effectively\nselect and utilize various tools in complex planning scenarios\n(Elaraby et al. 2023).\nBrain-Inspired Planning\nThis area explores neurologically and cognitively inspired\narchitectures in LLMs (Webb et al. 2023; Sumers et al.\n2023; Momennejad et al. 2023; Hu et al. 2023d; Lin et al.\n2023a), aiming to replicate human-like planning in enhanc-\ning problem-solving. However, while these methods rely\non in-context learning, they frequently encounter challenges\nsuch as hallucination and grounding, as previously dis-\ncussed, and tend to be more computationally intensive than\nin-context learning alone.\nWhile LLMs attempt to mimic symbolic solvers through in-\ncontext learning for brain-inspired modules, this approach\nlacks adaptability and is a superficial understanding of\ncomplex cognitive processes. To overcome these issues, de-\nveloping systems where neural and symbolic components are\nintrinsically intertwined is critical as it would accurately mirror\nhuman cognitive capabilities in planning (Fabiano et al. 2023).\nDiscussion and Conclusion\nIn this position paper, we comprehensively investigate the\nrole of LLMs within the domain of APS, analyzing 126\nscholarly articles across eight distinct categories. This ex-\ntensive survey not only provides a detailed landscape of cur-\nrent LLM applications and their limitations but also high-\nlights the volume of research in each category: Language\nTranslation with 23 papers demonstrates LLMs’ proficiency,\nwhereas Plan Generation, the most researched category with\n53 papers, reveals their shortcomings in optimality, com-\npleteness, and correctness compared to traditional combina-\ntorial planners. Our exploration extends to Model Construc-\ntion (17 papers), which examines LLMs in developing plan-\nning models, and the relatively unexplored area of Multi-\nagent Planning (4 papers). Interactive Planning is well rep-\nresented with 21 papers, illustrating LLMs’ adaptability in\nfeedback-centric scenarios. Despite being less researched,\nHeuristics Optimization and Tool Integration, each with 8\npapers, provide valuable insights into efficiency enhance-\nment and integration of LLMs with symbolic solvers. Lastly,\nBrain-inspired Planning, although least represented with 5\npapers, opens innovative avenues for human-like planning\nprocesses in LLMs. By identifying the research distribution\nand gaps in these categories, our paper proposes how neuro-\nsymbolic approaches can address these voids, thereby under-\nscoring the varying degrees of LLM applications in APS and\nguiding future research towards enhancing their capabilities\nin complex planning tasks.\nIt is important to acknowledge that while LLMs have\nshown promise, they are not a panacea for the inherent com-\nplexities of automated planning. The expectation that LLMs,\noperating within polynomial run-time bounds, could sup-\nplant the nuanced and often non-polynomial complexities of\nsymbolic planners is not yet realizable. Indeed, the strengths\nof LLMs do not currently include generating sequences of\nactions akin to those devised by symbolic planners, which\nare essential for creating a coherent and practical plan for\ncomplex problems. However, this does not diminish the po-\ntential utility of LLMs within this space. When consider-\ning average-case scenarios, which are typically less complex\nthan worst-case scenarios, LLMs could offer substantial ef-\nficiencies. They can be seen as akin to meta-heuristic ap-\nproaches, capable of accelerating plan generation in a va-\nriety of settings. As such, their application, governed by\ncognitive-inspired frameworks like SOFAI(Fabiano et al.\n2023), could delineate when and where their use is most ad-\nvantageous.\nFuture research should prioritize three areas: developing\nnew LLM training paradigms that ensure coherence and goal\nalignment in outputs; delving into Henry Kautz’s neuro-\nsymbolic taxonomies (Kautz 2022) to better integrate neu-\nral and symbolic methods; and establishing clear perfor-\nmance metrics for LLM-assisted planners. In conclusion, in-\ntegrating LLMs into automated planning, while challenging,\nopens avenues for innovation. Embracing a symbiotic ap-\nproach that combines the creative strengths of LLMs with\nthe precision of symbolic planners can lead to more effec-\ntive, sophisticated AI applications in planning.\nReferences\nAbdelnabi, S.; Gomaa, A.; Sivaprasad, S.; Sch ¨onherr, L.;\nand Fritz, M. 2023. Llm-deliberation: Evaluating llms with\ninteractive multi-agent negotiation games. arXiv preprint\narXiv:2309.17234.\nArora, D.; and Kambhampati, S. 2023. Learning\nand Leveraging Verifiers to Improve Planning Capabil-\nities of Pre-trained Language Models. arXiv preprint\narXiv:2305.17077.\nAsai, M. 2018. Photo-Realistic Blocksworld Dataset. arXiv\npreprint arXiv:1812.01818.\nBesta, M.; Blach, N.; Kubicek, A.; Gerstenberger, R.;\nGianinazzi, L.; Gajda, J.; Lehmann, T.; Podstawski, M.;\nNiewiadomski, H.; Nyczyk, P.; et al. 2023. Graph of\nthoughts: Solving elaborate problems with large language\nmodels. arXiv preprint arXiv:2308.09687.\nBrohan, A.; Chebotar, Y .; Finn, C.; Hausman, K.; Herzog,\nA.; Ho, D.; Ibarz, J.; Irpan, A.; Jang, E.; Julian, R.; et al.\n2023. Do as i can, not as i say: Grounding language in\nrobotic affordances. In Conference on Robot Learning, 287–\n318. PMLR.\nCapitanelli, A.; and Mastrogiovanni, F. 2023. A Frame-\nwork to Generate Neurosymbolic PDDL-compliant Plan-\nners. arXiv preprint arXiv:2303.00438.\nCarta, T.; Romac, C.; Wolf, T.; Lamprier, S.; Sigaud, O.;\nand Oudeyer, P.-Y . 2023. Grounding large language models\nin interactive environments with online reinforcement learn-\ning. arXiv preprint arXiv:2302.02662.\nChalvatzaki, G.; Younes, A.; Nandha, D.; Le, A. T.; Ribeiro,\nL. F.; and Gurevych, I. 2023. Learning to reason over\nscene graphs: a case study of finetuning GPT-2 into a robot\nlanguage model for grounded task planning. Frontiers in\nRobotics and AI, 10.\nChen, B.; Xia, F.; Ichter, B.; Rao, K.; Gopalakrishnan, K.;\nRyoo, M. S.; Stone, A.; and Kappler, D. 2023a. Open-\nvocabulary queryable scene representations for real world\nplanning. In 2023 IEEE International Conference on\nRobotics and Automation (ICRA), 11509–11522. IEEE.\nChen, X.; Zhang, S.; Zhang, P.; Zhao, L.; and Chen, J.\n2023b. Asking Before Action: Gather Information in Em-\nbodied Decision Making with Language Models. arXiv\npreprint arXiv:2305.15695.\nChen, Y .; Arkin, J.; Zhang, Y .; Roy, N.; and Fan, C.\n2023c. AutoTAMP: Autoregressive Task and Motion Plan-\nning with LLMs as Translators and Checkers.arXiv preprint\narXiv:2306.06531.\nChen, Y .; Arkin, J.; Zhang, Y .; Roy, N.; and Fan, C. 2023d.\nScalable Multi-Robot Collaboration with Large Language\nModels: Centralized or Decentralized Systems? arXiv\npreprint arXiv:2309.15943.\nDagan, G.; Keller, F.; and Lascarides, A. 2023. Dynamic\nPlanning with a LLM. arXiv preprint arXiv:2308.06391.\nDai, Z.; Asgharivaskasi, A.; Duong, T.; Lin, S.; Tzes, M.-E.;\nPappas, G.; and Atanasov, N. 2023. Optimal Scene Graph\nPlanning with Large Language Model Guidance. arXiv\npreprint arXiv:2309.09182.\nDalal, M.; Chiruvolu, T.; Chaplot, D. S.; and Salakhutdinov,\nR. 2023. Plan-Seq-Learn: Language Model Guided RL for\nSolving Long Horizon Robotics Tasks. In 2nd Workshop on\nLanguage and Robot Learning: Language as Grounding.\nde Zarz `a, I.; de Curt `o, J.; Roig, G.; Manzoni, P.; and\nCalafate, C. T. 2023. Emergent Cooperation and Strategy\nAdaptation in Multi-Agent Systems: An Extended Coevolu-\ntionary Theory with LLMs. Electronics, 12(12): 2722.\nDing, Y .; Zhang, X.; Amiri, S.; Cao, N.; Yang, H.; Kaminski,\nA.; Esselink, C.; and Zhang, S. 2023a. Integrating Action\nKnowledge and LLMs for Task Planning and Situation Han-\ndling in Open Worlds. arXiv preprint arXiv:2305.17590.\nDing, Y .; Zhang, X.; Paxton, C.; and Zhang, S. 2023b.\nLeveraging Commonsense Knowledge from Large Lan-\nguage Models for Task and Motion Planning. In RSS 2023\nWorkshop on Learning for Task and Motion Planning.\nDing, Y .; Zhang, X.; Paxton, C.; and Zhang, S. 2023c. Task\nand motion planning with large language models for object\nrearrangement. arXiv preprint arXiv:2303.06247.\nDriess, D.; Xia, F.; Sajjadi, M. S.; Lynch, C.; Chowdhery, A.;\nIchter, B.; Wahid, A.; Tompson, J.; Vuong, Q.; Yu, T.; et al.\n2023. Palm-e: An embodied multimodal language model.\narXiv preprint arXiv:2303.03378.\nDu, M.; Luu, A. T.; Ji, B.; and Ng, S.-k. 2023. From Static to\nDynamic: A Continual Learning Framework for Large Lan-\nguage Models. arXiv preprint arXiv:2310.14248.\nElaraby, M.; Lu, M.; Dunn, J.; Zhang, X.; Wang, Y .; and Liu,\nS. 2023. Halo: Estimation and reduction of hallucinations in\nopen-source weak large language models. arXiv preprint\narXiv:2308.11764.\nFabiano, F.; Pallagani, V .; Ganapini, M. B.; Horesh, L.;\nLoreggia, A.; Murugesan, K.; Rossi, F.; and Srivastava,\nB. 2023. Fast and Slow Planning. arXiv preprint\narXiv:2303.04283.\nFeng, X.; Wan, Z.; Wen, M.; Wen, Y .; Zhang, W.; and\nWang, J. 2023. Alphazero-like tree-search can guide large\nlanguage model decoding and training. arXiv preprint\narXiv:2309.17179.\nGandhi, K.; Sadigh, D.; and Goodman, N. D. 2023. Strate-\ngic Reasoning with Language Models. arXiv preprint\narXiv:2305.19165.\nGe, Y .; Hua, W.; Ji, J.; Tan, J.; Xu, S.; and Zhang, Y . 2023.\nOpenagi: When llm meets domain experts. arXiv preprint\narXiv:2304.04370.\nGeorgievski, I.; and Aiello, M. 2015. HTN planning:\nOverview, comparison, and beyond. Artif. Intell., 222: 124–\n156.\nGerevini, A.; and Serina, I. 2002. LPG: A Planner Based\non Local Search for Planning Graphs with Action Costs. In\nAips, volume 2, 281–290.\nGhallab, M.; Nau, D.; and Traverso, P. 2004. Automated\nPlanning: Theory and Practice . The Morgan Kaufmann\nSeries in Artificial Intelligence. Amsterdam: Morgan Kauf-\nmann. ISBN 978-1-55860-856-6.\nGragera, A.; and Pozanco, A. 2023. Exploring the Lim-\nitations of using Large Language Models to Fix Planning\nTasks.\nGramopadhye, M.; and Szafir, D. 2022. Generating exe-\ncutable action plans with environmentally-aware language\nmodels. arXiv preprint arXiv:2210.04964.\nGraule, M. A.; and Isler, V . 2023. GG-LLM: Geometrically\nGrounding Large Language Models for Zero-shot Human\nActivity Forecasting in Human-Aware Task Planning.arXiv\npreprint arXiv:2310.20034.\nGu, Q.; Kuwajerwala, A.; Morin, S.; Jatavallabhula, K. M.;\nSen, B.; Agarwal, A.; Rivera, C.; Paul, W.; Ellis, K.; Chel-\nlappa, R.; et al. 2023. Conceptgraphs: Open-vocabulary 3d\nscene graphs for perception and planning. arXiv preprint\narXiv:2309.16650.\nGuan, L.; Valmeekam, K.; Sreedharan, S.; and Kambham-\npati, S. 2023. Leveraging Pre-trained Large Language Mod-\nels to Construct and Utilize World Models for Model-based\nTask Planning. arXiv preprint arXiv:2305.14909.\nHao, S.; Gu, Y .; Ma, H.; Hong, J. J.; Wang, Z.; Wang, D. Z.;\nand Hu, Z. 2023a. Reasoning with language model is plan-\nning with world model. arXiv preprint arXiv:2305.14992.\nHao, S.; Liu, T.; Wang, Z.; and Hu, Z. 2023b. ToolkenGPT:\nAugmenting Frozen Language Models with Massive Tools\nvia Tool Embeddings. arXiv preprint arXiv:2305.11554.\nHazra, R.; Martires, P. Z. D.; and De Raedt, L. 2023. Say-\nCanPay: Heuristic Planning with Large Language Mod-\nels using Learnable Domain Knowledge. arXiv preprint\narXiv:2308.12682.\nHowey, R.; Long, D.; and Fox, M. 2004. V AL: automatic\nplan validation, continuous effects and mixed initiative plan-\nning using PDDL. In 16th IEEE International Conference\non Tools with Artificial Intelligence, 294–301.\nHsieh, C.-Y .; Chen, S.-A.; Li, C.-L.; Fujii, Y .; Ratner, A.;\nLee, C.-Y .; Krishna, R.; and Pfister, T. 2023. Tool docu-\nmentation enables zero-shot tool-usage with large language\nmodels. arXiv preprint arXiv:2308.00675.\nHu, B.; Zhao, C.; Zhang, P.; Zhou, Z.; Yang, Y .; Xu, Z.; and\nLiu, B. 2023a. Enabling Efficient Interaction between an\nAlgorithm Agent and an LLM: A Reinforcement Learning\nApproach. arXiv preprint arXiv:2306.03604.\nHu, H.; Lu, H.; Zhang, H.; Lam, W.; and Zhang, Y . 2023b.\nChain-of-Symbol Prompting Elicits Planning in Large Lan-\ngauge Models. arXiv preprint arXiv:2305.10276.\nHu, M.; Mu, Y .; Yu, X.; Ding, M.; Wu, S.; Shao, W.; Chen,\nQ.; Wang, B.; Qiao, Y .; and Luo, P. 2023c. Tree-Planner: Ef-\nficient Close-loop Task Planning with Large Language Mod-\nels. arXiv preprint arXiv:2310.08582.\nHu, P.; Qi, J.; Li, X.; Li, H.; Wang, X.; Quan, B.; Wang,\nR.; and Zhou, Y . 2023d. Tree-of-mixed-thought: Combining\nfast and slow thinking for multi-hop visual reasoning. arXiv\npreprint arXiv:2308.09658.\nHu, Z.; and Shu, T. 2023. Language Models, Agent Models,\nand World Models: The LAW for Machine Reasoning and\nPlanning. arXiv:2312.05230.\nHua, W.; Fan, L.; Li, L.; Mei, K.; Ji, J.; Ge, Y .; Hemphill, L.;\nand Zhang, Y . 2023. War and peace (waragent): Large lan-\nguage model-based multi-agent simulation of world wars.\narXiv preprint arXiv:2311.17227.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022a.\nLanguage models as zero-shot planners: Extracting action-\nable knowledge for embodied agents. In International Con-\nference on Machine Learning, 9118–9147. PMLR.\nHuang, W.; Wang, C.; Zhang, R.; Li, Y .; Wu, J.; and Fei-\nFei, L. 2023a. V oxposer: Composable 3d value maps for\nrobotic manipulation with language models. arXiv preprint\narXiv:2307.05973.\nHuang, W.; Xia, F.; Shah, D.; Driess, D.; Zeng, A.; Lu,\nY .; Florence, P.; Mordatch, I.; Levine, S.; Hausman, K.;\net al. 2023b. Grounded decoding: Guiding text generation\nwith grounded models for robot control. arXiv preprint\narXiv:2303.00855.\nHuang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Flo-\nrence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar,\nY .; et al. 2022b. Inner monologue: Embodied reasoning\nthrough planning with language models. arXiv preprint\narXiv:2207.05608.\nJha, S. K.; Jha, S.; Lincoln, P.; Bastian, N. D.; Velasquez, A.;\nEwetz, R.; and Neema, S. 2023. Neuro Symbolic Reason-\ning for Planning: Counterexample Guided Inductive Synthe-\nsis using Large Language Models and Satisfiability Solving.\narXiv preprint arXiv:2309.16436.\nJoublin, F.; Ceravola, A.; Smirnov, P.; Ocker, F.;\nDeigmoeller, J.; Belardinelli, A.; Wang, C.; Hasler, S.;\nTanneberg, D.; and Gienger, M. 2023. CoPAL: Corrective\nPlanning of Robot Actions with Large Language Models.\narXiv preprint arXiv:2310.07263.\nKannan, S. S.; Venkatesh, V . L.; and Min, B.-C. 2023.\nSMART-LLM: Smart Multi-Agent Robot Task Plan-\nning using Large Language Models. arXiv preprint\narXiv:2309.10062.\nKant, Y .; Ramachandran, A.; Yenamandra, S.; Gilitschen-\nski, I.; Batra, D.; Szot, A.; and Agrawal, H. 2022. House-\nkeep: Tidying virtual households using commonsense rea-\nsoning. In European Conference on Computer Vision, 355–\n373. Springer.\nKautz, H. A. 2022. The third AI summer: AAAI Robert\nS. Engelmore Memorial Lecture. AI Magazine, 43(1): 105–\n125.\nKelly, J.; Calderwood, A.; Wardrip-Fruin, N.; and Mateas,\nM. 2023. There and back again: extracting formal domains\nfor controllable neurosymbolic story authoring. In Proceed-\nings of the AAAI Conference on Artificial Intelligence and\nInteractive Digital Entertainment, volume 19, 64–74.\nKim, G.; Kim, T.; Kannan, S. S.; Venkatesh, V . L.; Kim,\nD.; and Min, B.-C. 2023. DynaCon: Dynamic Robot Plan-\nner with Contextual Awareness via LLMs. arXiv preprint\narXiv:2309.16031.\nKirk, J. R.; Wray, R. E.; and Laird, J. E. 2023. Exploiting\nLanguage Models as a Source of Knowledge for Cognitive\nAgents. arXiv preprint arXiv:2310.06846.\nLenat, D.; and Marcus, G. 2023. Getting from generative\nai to trustworthy ai: What llms might learn from cyc. arXiv\npreprint arXiv:2308.04445.\nLi, M.; Song, F.; Yu, B.; Yu, H.; Li, Z.; Huang, F.; and Li,\nY . 2023a. Api-bank: A benchmark for tool-augmented llms.\narXiv preprint arXiv:2304.08244.\nLi, Y .; Kamra, N.; Desai, R.; and Halevy, A. 2023b. Human-\nCentered Planning. arXiv preprint arXiv:2311.04403.\nLin, B. Y .; Fu, Y .; Yang, K.; Ammanabrolu, P.; Brahman,\nF.; Huang, S.; Bhagavatula, C.; Choi, Y .; and Ren, X.\n2023a. SwiftSage: A Generative Agent with Fast and Slow\nThinking for Complex Interactive Tasks. arXiv preprint\narXiv:2305.17390.\nLin, H.; Zala, A.; Cho, J.; and Bansal, M. 2023b. Videodi-\nrectorgpt: Consistent multi-scene video generation via llm-\nguided planning. arXiv preprint arXiv:2309.15091.\nLin, K.; Agia, C.; Migimatsu, T.; Pavone, M.; and Bohg, J.\n2023c. Text2motion: From natural language instructions to\nfeasible plans. arXiv preprint arXiv:2303.12153.\nLiu, B.; Jiang, Y .; Zhang, X.; Liu, Q.; Zhang, S.; Biswas, J.;\nand Stone, P. 2023. Llm+ p: Empowering large language\nmodels with optimal planning proficiency. arXiv preprint\narXiv:2304.11477.\nLiu, Z.; Bahety, A.; and Song, S. 2023. Reflect: Summariz-\ning robot experiences for failure explanation and correction.\narXiv preprint arXiv:2306.15724.\nLogeswaran, L.; Sohn, S.; Lyu, Y .; Liu, A. Z.; Kim, D.-\nK.; Shim, D.; Lee, M.; and Lee, H. 2023. Code Mod-\nels are Zero-shot Precondition Reasoners. arXiv preprint\narXiv:2311.09601.\nLu, P.; Peng, B.; Cheng, H.; Galley, M.; Chang, K.-W.; Wu,\nY . N.; Zhu, S.-C.; and Gao, J. 2023a. Chameleon: Plug-and-\nplay compositional reasoning with large language models.\narXiv preprint arXiv:2304.09842.\nLu, Y .; Feng, W.; Zhu, W.; Xu, W.; Wang, X. E.; Eckstein,\nM.; and Wang, W. Y . 2022. Neuro-symbolic causal lan-\nguage planning with commonsense prompting. arXiv e-\nprints, arXiv–2206.\nLu, Y .; Lu, P.; Chen, Z.; Zhu, W.; Wang, X. E.; and Wang,\nW. Y . 2023b. Multimodal Procedural Planning via Dual\nText-Image Prompting. arXiv preprint arXiv:2305.01795.\nLuo, L.; Li, Y .-F.; Haffari, G.; and Pan, S. 2023. Reasoning\non graphs: Faithful and interpretable large language model\nreasoning. arXiv preprint arXiv:2310.01061.\nMandi, Z.; Jain, S.; and Song, S. 2023. Roco: Dialectic\nmulti-robot collaboration with large language models.arXiv\npreprint arXiv:2307.04738.\nMcDermott, D.; Ghallab, M.; Howe, A.; Knoblock, C.; Ram,\nA.; Veloso, M.; Weld, D.; and Wilkins, D. 1998. PDDL-the\nplanning domain definition language.\nMomennejad, I.; Hasanbeig, H.; Vieira, F.; Sharma, H.;\nNess, R. O.; Jojic, N.; Palangi, H.; and Larson, J. 2023.\nEvaluating Cognitive Maps and Planning in Large Language\nModels with CogEval. arXiv preprint arXiv:2309.15129.\nNaik, R.; Chandrasekaran, V .; Yuksekgonul, M.; Palangi, H.;\nand Nushi, B. 2023. Diversity of Thought Improves Rea-\nsoning Abilities of Large Language Models. arXiv preprint\narXiv:2310.07088.\nNottingham, K.; Ammanabrolu, P.; Suhr, A.; Choi, Y .; Ha-\njishirzi, H.; Singh, S.; and Fox, R. 2023. Do embodied\nagents dream of pixelated sheep?: Embodied decision mak-\ning using language guided world modelling. arXiv preprint\narXiv:2301.12050.\nPallagani, V .; Muppasani, B.; Murugesan, K.; Rossi, F.;\nHoresh, L.; Srivastava, B.; Fabiano, F.; and Loreggia, A.\n2022. Plansformer: Generating symbolic plans using trans-\nformers. arXiv preprint arXiv:2212.08681.\nPallagani, V .; Muppasani, B.; Murugesan, K.; Rossi, F.; Sri-\nvastava, B.; Horesh, L.; Fabiano, F.; and Loreggia, A. 2023a.\nUnderstanding the Capabilities of Large Language Models\nfor Automated Planning. arXiv preprint arXiv:2305.16151.\nPallagani, V .; Muppasani, B.; Srivastava, B.; Rossi, F.;\nHoresh, L.; Murugesan, K.; Loreggia, A.; Fabiano, F.;\nJoseph, R.; Kethepalli, Y .; et al. 2023b. Plansformer Tool:\nDemonstrating Generation of Symbolic Plans Using Trans-\nformers. In IJCAI, volume 2023, 7158–7162. International\nJoint Conferences on Artificial Intelligence.\nPan, L.; Albalak, A.; Wang, X.; and Wang, W. Y . 2023.\nLogic-lm: Empowering large language models with sym-\nbolic solvers for faithful logical reasoning. arXiv preprint\narXiv:2305.12295.\nParakh, M.; Fong, A.; Simeonov, A.; Gupta, A.; Chen,\nT.; and Agrawal, P. 2023. Human-Assisted Continual\nRobot Learning with Foundation Models. arXiv preprint\narXiv:2309.14321.\nRaimondo, S.; Pal, C.; Liu, X.; Vazquez, D.; and Palacios,\nH. 2023. Improving Generalization in Task-oriented Dia-\nlogues with Workflows and Action Plans. arXiv preprint\narXiv:2306.01729.\nRajvanshi, A.; Sikka, K.; Lin, X.; Lee, B.; Chiu, H.-P.; and\nVelasquez, A. 2023. Saynav: Grounding large language\nmodels for dynamic planning to navigation in new environ-\nments. arXiv preprint arXiv:2309.04077.\nRaman, S. S.; Cohen, V .; Rosen, E.; Idrees, I.; Paulius, D.;\nand Tellex, S. 2022. Planning with large language models\nvia corrective re-prompting. In NeurIPS 2022 Foundation\nModels for Decision Making Workshop.\nRana, K.; Haviland, J.; Garg, S.; Abou-Chakra, J.; Reid, I.;\nand Suenderhauf, N. 2023. Sayplan: Grounding large lan-\nguage models using 3d scene graphs for scalable task plan-\nning. arXiv preprint arXiv:2307.06135.\nRen, A. Z.; Dixit, A.; Bodrova, A.; Singh, S.; Tu, S.; Brown,\nN.; Xu, P.; Takayama, L.; Xia, F.; Varley, J.; et al. 2023.\nRobots that ask for help: Uncertainty alignment for large\nlanguage model planners. arXiv preprint arXiv:2307.01928.\nRuan, J.; Chen, Y .; Zhang, B.; Xu, Z.; Bao, T.; Du, G.; Shi,\nS.; Mao, H.; Zeng, X.; and Zhao, R. 2023. Tptu: Task plan-\nning and tool usage of large language model-based ai agents.\narXiv preprint arXiv:2308.03427.\nRussell, S.; and Norvig, P. 2003. Artificial Intelligence, A\nModern Approach. Second Edition.\nSakib, M. S.; and Sun, Y . 2023. From Cooking Recipes\nto Robot Task Trees–Improving Planning Correctness and\nTask Efficiency by Leveraging LLMs with a Knowledge\nNetwork. arXiv preprint arXiv:2309.09181.\nSarkisyan, C.; Korchemnyi, A.; Kovalev, A. K.; and Panov,\nA. I. 2023. Evaluation of Pretrained Large Language Models\nin Embodied Planning Tasks. In International Conference\non Artificial General Intelligence, 222–232. Springer.\nSermanet, P.; Ding, T.; Zhao, J.; Xia, F.; Dwibedi, D.;\nGopalakrishnan, K.; Chan, C.; Dulac-Arnold, G.; Maddi-\nneni, S.; Joshi, N. J.; et al. 2023. RoboVQA: Multimodal\nLong-Horizon Reasoning for Robotics. arXiv preprint\narXiv:2311.00899.\nShah, D.; Equi, M.; Osinski, B.; Xia, F.; Ichter, B.; and\nLevine, S. 2023. Navigation with large language models: Se-\nmantic guesswork as a heuristic for planning.arXiv preprint\narXiv:2310.10103.\nShen, Y .; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang,\nY . 2023. Hugginggpt: Solving ai tasks with chatgpt and its\nfriends in huggingface. arXiv preprint arXiv:2303.17580.\nShirai, K.; Beltran-Hernandez, C. C.; Hamaya, M.;\nHashimoto, A.; Tanaka, S.; Kawaharazuka, K.; Tanaka, K.;\nUshiku, Y .; and Mori, S. 2023. Vision-Language Interpreter\nfor Robot Task Planning. arXiv preprint arXiv:2311.00967.\nSilver, T.; Dan, S.; Srinivas, K.; Tenenbaum, J. B.; Kael-\nbling, L. P.; and Katz, M. 2023. Generalized Planning in\nPDDL Domains with Pretrained Large Language Models.\narXiv preprint arXiv:2305.11014.\nSilver, T.; Hariprasad, V .; Shuttleworth, R. S.; Kumar, N.;\nLozano-P´erez, T.; and Kaelbling, L. P. 2022. PDDL plan-\nning with pretrained large language models. In NeurIPS\n2022 Foundation Models for Decision Making Workshop.\nSingh, I.; Blukis, V .; Mousavian, A.; Goyal, A.; Xu, D.;\nTremblay, J.; Fox, D.; Thomason, J.; and Garg, A. 2023.\nProgPrompt: program generation for situated robot task\nplanning using large language models. Autonomous Robots,\n1–14.\nSong, C. H.; Wu, J.; Washington, C.; Sadler, B. M.; Chao,\nW.-L.; and Su, Y . 2023. Llm-planner: Few-shot grounded\nplanning for embodied agents with large language models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2998–3009.\nSumers, T.; Yao, S.; Narasimhan, K.; and Griffiths, T. L.\n2023. Cognitive architectures for language agents. arXiv\npreprint arXiv:2309.02427.\nSun, H.; Zhuang, Y .; Kong, L.; Dai, B.; and Zhang, C. 2023.\nAdaPlanner: Adaptive Planning from Feedback with Lan-\nguage Models. arXiv preprint arXiv:2305.16653.\nTang, X.; Zheng, Z.; Li, J.; Meng, F.; Zhu, S.-C.; Liang,\nY .; and Zhang, M. 2023. Large Language Models are In-\nContext Semantic Reasoners rather than Symbolic Reason-\ners. arXiv preprint arXiv:2305.14825.\nValmeekam, K.; Marquez, M.; and Kambhampati, S.\n2023. Can Large Language Models Really Improve\nby Self-critiquing Their Own Plans? arXiv preprint\narXiv:2310.08118.\nValmeekam, K.; Marquez, M.; Olmo, A.; Sreedharan, S.;\nand Kambhampati, S. 2023a. PlanBench: An Extensible\nBenchmark for Evaluating Large Language Models on Plan-\nning and Reasoning about Change. In Thirty-seventh Con-\nference on Neural Information Processing Systems Datasets\nand Benchmarks Track.\nValmeekam, K.; Olmo, A.; Sreedharan, S.; and Kambham-\npati, S. 2022. Large Language Models Still Can’t Plan\n(A Benchmark for LLMs on Planning and Reasoning about\nChange). arXiv preprint arXiv:2206.10498.\nValmeekam, K.; Sreedharan, S.; Marquez, M.; Olmo, A.;\nand Kambhampati, S. 2023b. On the planning abilities of\nlarge language models (a critical investigation with a pro-\nposed benchmark). arXiv preprint arXiv:2302.06706.\nWang, J.; Tong, J.; Tan, K.; V orobeychik, Y .; and Kantaros,\nY . 2023a. Conformal Temporal Logic Planning using Large\nLanguage Models: Knowing When to Do What and When\nto Ask for Help. arXiv preprint arXiv:2309.10092.\nWang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R. K.-W.;\nand Lim, E.-P. 2023b. Plan-and-solve prompting: Improv-\ning zero-shot chain-of-thought reasoning by large language\nmodels. arXiv preprint arXiv:2305.04091.\nWang, X.; Caccia, L.; Ostapenko, O.; Yuan, X.; and Sordoni,\nA. 2023c. Guiding language model reasoning with planning\ntokens. arXiv preprint arXiv:2310.05707.\nWang, Z.; Cai, S.; Liu, A.; Ma, X.; and Liang, Y . 2023d.\nDescribe, explain, plan and select: Interactive planning\nwith large language models enables open-world multi-task\nagents. arXiv preprint arXiv:2302.01560.\nWebb, T.; Mondal, S. S.; Wang, C.; Krabach, B.; and Mo-\nmennejad, I. 2023. A Prefrontal Cortex-inspired Architec-\nture for Planning in Large Language Models. arXiv preprint\narXiv:2310.00194.\nWei, L.; He, Z.; Zhao, H.; and Yao, Q. 2023. Unleashing the\nPower of Graph Learning through LLM-based Autonomous\nAgents. arXiv preprint arXiv:2309.04565.\nWong, L.; Grand, G.; Lew, A. K.; Goodman, N. D.; Mans-\ninghka, V . K.; Andreas, J.; and Tenenbaum, J. B. 2023. From\nWord Models to World Models: Translating from Natural\nLanguage to the Probabilistic Language of Thought. arXiv\npreprint arXiv:2306.12672.\nWu, Y .; Min, S. Y .; Bisk, Y .; Salakhutdinov, R.; Azaria, A.;\nLi, Y .; Mitchell, T.; and Prabhumoye, S. 2023a. Plan, Elim-\ninate, and Track–Language Models are Good Teachers for\nEmbodied Agents. arXiv preprint arXiv:2305.02412.\nWu, Z.; Ai, B.; and Hsu, D. 2023. Integrating Common\nSense and Planning with Large Language Models for Room\nTidying. In RSS 2023 Workshop on Learning for Task and\nMotion Planning.\nWu, Z.; Wang, Z.; Xu, X.; Lu, J.; and Yan, H. 2023b. Em-\nbodied task planning with large language models. arXiv\npreprint arXiv:2307.01848.\nXie, Y .; Yu, C.; Zhu, T.; Bai, J.; Gong, Z.; and Soh, H. 2023.\nTranslating natural language to planning goals with large-\nlanguage models. arXiv preprint arXiv:2302.05128.\nXu, B.; Liu, X.; Shen, H.; Han, Z.; Li, Y .; Yue, M.; Peng,\nZ.; Liu, Y .; Yao, Z.; and Xu, D. 2023a. Gentopia: A col-\nlaborative platform for tool-augmented llms. arXiv preprint\narXiv:2308.04030.\nXu, L. 1995. Case based reasoning. IEEE Potentials, 13(5):\n10–13.\nXu, M.; Huang, P.; Yu, W.; Liu, S.; Zhang, X.; Niu, Y .;\nZhang, T.; Xia, F.; Tan, J.; and Zhao, D. 2023b. Cre-\native Robot Tool Use with Large Language Models. arXiv\npreprint arXiv:2310.13065.\nYang, J.; Chen, X.; Qian, S.; Madaan, N.; Iyengar, M.;\nFouhey, D. F.; and Chai, J. 2023a. LLM-Grounder: Open-\nV ocabulary 3D Visual Grounding with Large Language\nModel as an Agent. arXiv preprint arXiv:2309.12311.\nYang, R.; Hou, M.; Wang, J.; and Zhang, F. 2023b. Ocean-\nChat: Piloting Autonomous Underwater Vehicles in Natural\nLanguage. arXiv preprint arXiv:2309.16052.\nYang, Y .; Gaglione, J.-R.; and Topcu, U. 2022. Learn-\ning Automata-Based Task Knowledge Representation from\nLarge-Scale Generative Language Models. arXiv preprint\narXiv:2212.01944.\nYang, Y .; and Tomar, A. 2023. On the Planning, Search,\nand Memorization Capabilities of Large Language Models.\narXiv preprint arXiv:2309.01868.\nYang, Z. 2023. Neuro-Symbolic AI Approaches to Enhance\nDeep Neural Networks with Logical Reasoning and Knowl-\nedge Integration. Ph.D. thesis, Arizona State University.\nYang, Z.; Ishay, A.; and Lee, J. 2023. Coupling\nLarge Language Models with Logic Programming for Ro-\nbust and General Reasoning from Text. arXiv preprint\narXiv:2307.07696.\nYao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.; Cao,\nY .; and Narasimhan, K. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models. arXiv preprint\narXiv:2305.10601.\nYoneda, T.; Fang, J.; Li, P.; Zhang, H.; Jiang, T.; Lin, S.;\nPicker, B.; Yunis, D.; Mei, H.; and Walter, M. R. 2023.\nStatler: State-maintaining language models for embodied\nreasoning. arXiv preprint arXiv:2306.17840.\nYou, W.; Wu, W.; Liang, Y .; Mao, S.; Wu, C.; Cao,\nM.; Cai, Y .; Guo, Y .; Xia, Y .; Wei, F.; et al. 2023.\nEIPE-text: Evaluation-Guided Iterative Plan Extraction for\nLong-Form Narrative Text Generation. arXiv preprint\narXiv:2310.08185.\nYuan, H.; Zhang, C.; Wang, H.; Xie, F.; Cai, P.; Dong, H.;\nand Lu, Z. 2023a. Plan4mc: Skill reinforcement learning\nand planning for open-world minecraft tasks. arXiv preprint\narXiv:2303.16563.\nYuan, S.; Chen, J.; Fu, Z.; Ge, X.; Shah, S.; Jankowski,\nC. R.; Yang, D.; and Xiao, Y . 2023b. Distilling Script\nKnowledge from Large Language Models for Constrained\nLanguage Planning. arXiv preprint arXiv:2305.05252.\nZelikman, E.; Huang, Q.; Poesia, G.; Goodman, N.; and\nHaber, N. 2023. Parsel: Algorithmic Reasoning with Lan-\nguage Models by Composing Decompositions. In Thirty-\nseventh Conference on Neural Information Processing Sys-\ntems.\nZhang, B.; and Soh, H. 2023. Large language models as\nzero-shot human models for human-robot interaction. arXiv\npreprint arXiv:2303.03548.\nZhang, C.; Liu, L.; Wang, J.; Wang, C.; Sun, X.; Wang, H.;\nand Cai, M. 2023a. Prefer: Prompt ensemble learning via\nfeedback-reflect-refine. arXiv preprint arXiv:2308.12033.\nZhang, F.; Jin, K.; and Zhuo, H. H. 2023. Planning with\nLogical Graph-based Language Model for Instruction Gen-\neration. arXiv:2308.13782.\nZhang, H.; Du, W.; Shan, J.; Zhou, Q.; Du, Y .; Tenenbaum,\nJ. B.; Shu, T.; and Gan, C. 2023b. Building cooperative em-\nbodied agents modularly with large language models. arXiv\npreprint arXiv:2307.02485.\nZhang, J.; Zhang, J.; Pertsch, K.; Liu, Z.; Ren, X.; Chang,\nM.; Sun, S.-H.; and Lim, J. J. 2023c. Bootstrap your own\nskills: Learning to solve new tasks with large language\nmodel guidance. arXiv preprint arXiv:2310.10021.\nZhao, Z.; Lee, W. S.; and Hsu, D. 2023. Large Language\nModels as Commonsense Knowledge for Large-Scale Task\nPlanning. arXiv preprint arXiv:2305.14078.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu,\nZ.; Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;\nZhang, H.; Gonzalez, J. E.; and Stoica, I. 2023a. Judg-\ning LLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv:2306.05685.\nZheng, S.; Liu, J.; Feng, Y .; and Lu, Z. 2023b. Steve-Eye:\nEquipping LLM-based Embodied Agents with Visual Per-\nception in Open Worlds. arXiv preprint arXiv:2310.13255.\nZhou, Z.; Song, J.; Yao, K.; Shu, Z.; and Ma, L. 2023.\nISR-LLM: Iterative Self-Refined Large Language Model for\nLong-Horizon Sequential Task Planning. arXiv preprint\narXiv:2308.13724.",
  "topic": "Heuristics",
  "concepts": [
    {
      "name": "Heuristics",
      "score": 0.6652249097824097
    },
    {
      "name": "Popularity",
      "score": 0.5852587819099426
    },
    {
      "name": "Computer science",
      "score": 0.5768625736236572
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.5114477872848511
    },
    {
      "name": "Management science",
      "score": 0.4447336196899414
    },
    {
      "name": "Language planning",
      "score": 0.42276084423065186
    },
    {
      "name": "Scheduling (production processes)",
      "score": 0.42092567682266235
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3419641852378845
    },
    {
      "name": "Engineering",
      "score": 0.18608930706977844
    },
    {
      "name": "Sociology",
      "score": 0.1726461946964264
    },
    {
      "name": "Political science",
      "score": 0.1507773995399475
    },
    {
      "name": "Operations management",
      "score": 0.12837275862693787
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I155781252",
      "name": "University of South Carolina",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I10052268",
      "name": "New Mexico State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79940851",
      "name": "University of Brescia",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I30667456",
      "name": "Brescia University",
      "country": "US"
    }
  ],
  "cited_by": 7
}