{
  "title": "Incorporating Word and Subword Units in Unsupervised Machine Translation Using Language Model Rescoring",
  "url": "https://openalex.org/W2968815867",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100623107",
      "name": "Zihan Liu",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100378208",
      "name": "Yan Xu",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5085516032",
      "name": "Genta Indra Winata",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5065856469",
      "name": "Pascale Fung",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2932618389",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963206679",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2962677207",
    "https://openalex.org/W2963993537",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2954447110",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2962824887",
    "https://openalex.org/W2890007195",
    "https://openalex.org/W2803214681",
    "https://openalex.org/W2741602058",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2109000768",
    "https://openalex.org/W2963684088",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2963602293",
    "https://openalex.org/W2963118869",
    "https://openalex.org/W2146574666"
  ],
  "abstract": "This paper describes CAiRE's submission to the unsupervised machine translation track of the WMT'19 news shared task from German to Czech. We leverage a phrase-based statistical machine translation (PBSMT) model and a pre-trained language model to combine word-level neural machine translation (NMT) and subword-level NMT models without using any parallel data. We propose to solve the morphological richness problem of languages by training byte-pair encoding (BPE) embeddings for German and Czech separately, and they are aligned using MUSE (Conneau et al., 2018). To ensure the fluency and consistency of translations, a rescoring mechanism is proposed that reuses the pre-trained language model to select the translation candidates generated through beam search. Moreover, a series of pre-processing and post-processing approaches are applied to improve the quality of final translations.",
  "full_text": "Incorporating Word and Subword Units in Unsupervised Machine\nTranslation Using Language Model Rescoring\nZihan Liu∗, Yan Xu∗, Genta Indra Winata, Pascale Fung\nCenter for Artiﬁcial Intelligence Research (CAiRE)\nDepartment of Electronic and Computer Engineering\nThe Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong\n{zliucr,yxucb,giwinata}@connect.ust.hk, pascale@ece.ust.hk\nAbstract\nThis paper describes CAiRE’s submission to\nthe unsupervised machine translation track of\nthe WMT’19 news shared task from German\nto Czech. We leverage a phrase-based sta-\ntistical machine translation (PBSMT) model\nand a pre-trained language model to combine\nword-level neural machine translation (NMT)\nand subword-level NMT models without us-\ning any parallel data. We propose to solve\nthe morphological richness problem of lan-\nguages by training byte-pair encoding (BPE)\nembeddings for German and Czech separately,\nand they are aligned using MUSE (Conneau\net al., 2018). To ensure the ﬂuency and consis-\ntency of translations, a rescoring mechanism is\nproposed that reuses the pre-trained language\nmodel to select the translation candidates gen-\nerated through beam search. Moreover, a se-\nries of pre-processing and post-processing ap-\nproaches are applied to improve the quality of\nﬁnal translations.\n1 Introduction\nMachine translation (MT) has achieved huge\nadvances in the past few years (Bahdanau et al.,\n2015; Gehring et al., 2017; Vaswani et al., 2017,\n2018). However, the need for a large amount of\nmanual parallel data obstructs its performance un-\nder low-resource conditions. Building an effective\nmodel on low resource data or even in an unsuper-\nvised way is always an interesting and challeng-\ning research topic (Gu et al., 2018; Radford et al.,\n2016; Lee et al., 2019). Recently, unsupervised\nMT (Artetxe et al., 2018b,a; Conneau et al., 2018;\nLample et al., 2018b; Wu et al., 2019), which can\nimmensely reduce the reliance on parallel corpora,\nhas been gaining more and more interest.\nTraining cross-lingual word embeddings (Con-\nneau et al., 2018; Artetxe et al., 2017) is always the\n*These two authors contributed equally.\nﬁrst step of the unsupervised MT models which\nproduce a word-level shared embedding space for\nboth the source and target, but the lexical cover-\nage can be an intractable problem. To tackle this\nissue, Sennrich et al. (2016b) provided a subword-\nlevel solution to overcome the out-of-vocabulary\n(OOV) problem.\nIn this work, the systems we implement for\nthe German-Czech language pair are built based\non the previously proposed unsupervised MT sys-\ntems, with some adaptations made to accom-\nmodate the morphologically rich characteristics\nof German and Czech (Tsarfaty et al., 2010).\nBoth word-level and subword-level neural ma-\nchine translation (NMT) models are applied in\nthis task and further tuned by pseudo-parallel data\ngenerated from a phrase-based statistical machine\ntranslation (PBSMT) model, which is trained fol-\nlowing the steps proposed in Lample et al. (2018b)\nwithout using any parallel data. We propose to\ntrain BPE embeddings for German and Czech sep-\narately and align those trained embeddings into a\nshared space with MUSE (Conneau et al., 2018) to\nreduce the combinatorial explosion of word forms\nfor both languages. To ensure the ﬂuency and\nconsistency of translations, an additional Czech\nlanguage model is trained to select the transla-\ntion candidates generated through beam search by\nrescoring them. Besides the above, a series of\npost-processing steps are applied to improve the\nquality of ﬁnal translations. Our contribution is\ntwo-fold:\n•We propose a method to combine word and\nsubword (BPE) pre-trained input representa-\ntions aligned using MUSE (Conneau et al.,\n2018) as an NMT training initialization on\na morphologically-rich language pair such as\nGerman and Czech.\n•We study the effectiveness of language model\narXiv:1908.05925v2  [cs.CL]  18 Nov 2019\nrescoring to choose the best sentences and\nunknown word replacement (UWR) proce-\ndure to reduce the drawback of OOV words.\nThis paper is organized as follows: in Section\n2, we describe our approach to the unsupervised\ntranslation from German to Czech. Section 3 re-\nports the training details and the results for each\nsteps of our approach. More related work is pro-\nvided in Section 4. Finally, we conclude our work\nin Section 5.\n2 Methodology\nIn this section, we describe how we built our main\nunsupervised machine translation system, which is\nillustrated in Figure 1.\n2.1 Unsupervised Machine Translation\n2.1.1 Word-level Unsupervised NMT\nWe follow the unsupervised NMT in Lample\net al. (2018b) by leveraging initialization, lan-\nguage modeling and back-translation. However,\ninstead of using BPE, we use MUSE (Conneau\net al., 2018) to align word-level embeddings of\nGerman and Czech, which are trained by FastText\n(Bojanowski et al., 2017) separately. We leverage\nthe aligned word embeddings to initialize our un-\nsupervised NMT model.\nThe language model is a denoising auto-\nencoder, which is trained by reconstructing orig-\ninal sentences from noisy sentences. The process\nof language modeling can be expressed as mini-\nmizing the following loss:\nLlm = λ∗{Ex∼S[−logPs→s(x|N(x))]+\nEy∼T [−logPt→t(x|N(y))]}, (1)\nwhere N is a noise model to drop and swap some\nwords with a certain probability in the sentence x,\nPs→s and Pt→t operate on the source and target\nsides separately, and λacts as a weight to control\nthe loss function of the language model. a Back-\ntranslation turns the unsupervised problem into a\nsupervised learning task by leveraging the gener-\nated pseudo-parallel data. The process of back-\ntranslation can be expressed as minimizing the fol-\nlowing loss:\nLbt =Ex∼S[−logPt→s(x|v∗(x))]+\nEy∼T [−logPs→t(y|u∗(y))], (2)\nwhere v∗(x) denotes sentences in the target lan-\nguage translated from source language sentences\nS, u∗(y) similarly denotes sentences in the source\nlanguage translated from the target language sen-\ntences T and Pt→s, and Ps→t denote the trans-\nlation direction from target to source and from\nsource to target respectively.\n2.1.2 Subword-level Unsupervised NMT\nWe note that both German and Czech (Tsarfaty\net al., 2010) are morphologically rich languages,\nwhich leads to a very large vocabulary size for\nboth languages, but especially for Czech (more\nthan one million unique words for German, but\nthree million unique words for Czech). To over-\ncome OOV issues, we leverage subword informa-\ntion, which can lead to better performance.\nWe employ subword units (Sennrich et al.,\n2016a) to tackle the morphological richness prob-\nlem. There are two advantages of using the\nsubword-level. First, we can alleviate the OOV is-\nsue by zeroing out the number of unknown words.\nSecond, we can leverage the semantics of sub-\nword units from these languages. However, Ger-\nman and Czech are distant languages that originate\nfrom different roots, so they only share a small\nfraction of subword units. To tackle this problem,\nwe train FastText word vectors (Bojanowski et al.,\n2017) separately for German and Czech, and ap-\nply MUSE (Conneau et al., 2018) to align these\nembeddings.\n2.1.3 Unsupervised PBSMT\nPBSMT models can outperform neural models in\nlow-resource conditions. A PBSMT model uti-\nlizes a pre-trained language model and a phrase\ntable with phrase-to-phrase translations from the\nsource language to target languages, which pro-\nvide a good initialization. The phrase table stores\nthe probabilities of the possible target phrase\ntranslations corresponding to the source phrases,\nwhich can be referred to as P(s|t), with s and t\nrepresenting the source and target phrases. The\nsource and target phrases are mapped accord-\ning to inferred cross-lingual word embeddings,\nwhich are trained with monolingual corpora and\naligned into a shared space without any parallel\ndata (Artetxe et al., 2017; Conneau et al., 2018).\nWe use a pre-trained n-gram language model to\nscore the phrase translation candidates by provid-\ning the relative likelihood estimation P(t), so that\nthe translation of a source phrase is derived from:\nargmaxtP(t|s) =argmaxtP(s|t)P(t).\nBack-translation enables the PBSMT models\nFigure 1: The illustration of our system. The translation procedure can be divided into ﬁve steps: (a) pre-\nprocessing, (b) translation generation (§2.1) from word-level NMT, subword-level NMT, and PBSMT. In the train-\ning, we ﬁne-tune word-level and subword-level NMT models with pseudo-parallel data from NMT models and\nthe best PBSMT model. Moreover, an unknown word replacement mechanism (§2.2) is applied to the translations\ngenerated from the word-level NMT model, (c) translation candidate rescoring, (d) construction of an ensemble of\nthe translations from NMT models, and (e) post-processing.\nto be trained in a supervised way by providing\npseudo-parallel data from the translation in the re-\nverse direction, which indicates that the PBSMT\nmodels need to be trained in dual directions so that\nthe two models trained in the opposite directions\ncan promote each other’s performance.\nIn this task, we follow the method proposed by\nLample et al. (2018b) to initialize the phrase ta-\nble, train the KenLM language models (Heaﬁeld,\n2011)1 and train a PBSMT model, but we make\ntwo changes. First, we only initialize a uni-gram\nphrase table because of the large vocabulary size\nof German and Czech and the limitation of com-\nputational resources. Second, instead of training\nthe model in the truecase mode, we maintain the\nsame pre-processing step (see more details in§3.1)\nas the NMT models.\n2.1.4 Fine-tuning NMT\nWe further ﬁne-tune the NMT models mentioned\nabove on the pseudo-parallel data generated by\na PBSMT model. We choose the best PBSMT\nmodel and mix the pseudo-parallel data from the\nNMT models and the PBSMT model, which are\nused for back-translation. The intuition is that we\ncan use the pseudo-parallel data produced by the\nPBSMT model as the supplementary translations\nin our NMT model, and these can potentially boost\nthe robustness of the NMT model by increasing\nthe variety of back-translation data.\n1The code can be found at https://github.com/kpu/kenlm\n2.2 Unknown Word Replacement\nAround 10% of words found in our NMT train-\ning data are unknown words ( <UNK>), which\nimmensely limits the potential of the word-level\nNMT model. In this case, replacing unknown\nwords with reasonable words can be a good rem-\nedy. Then, assuming the translations from the\nword-level NMT model and PBSMT model are\nroughly aligned in order, we can replace the un-\nknown words in the NMT translations with the\ncorresponding words in the PBSMT translations.\nCompared to the word-level NMT model, the PB-\nSMT model ensures that every phrase will be\ntranslated without omitting any pieces from the\nsentences. We search for the word replacement\nby the following steps, which are also illustrated\nin Figure 2:\nStep 1 For every unknown word, we can get the\ncontext words with a context window size of two.\nStep 2 Each context word is searched for in the\ncorresponding PBSMT translation. From our ob-\nservation, the meanings of the words in Czech are\nhighly likely to be the same if only the last few\ncharacters are different. Therefore, we allow the\nlast two characters to be different between the con-\ntext words and the words they match.\nStep 3 If several words in the PBSMT transla-\ntion match a context word, the word that is closest\nto the position of the context word in the PBSMT\ntranslation will be selected and put into the can-\nFigure 2: The illustration of the unknown word replacement (UWR) procedure for word-level NMT. The words\nof the PBSMT model translation in the pink boxes match the context words of the unknown word <UNK> in the\nword-level NMT model translation in the blue boxes. Finally, we choose a possible target word, in the yellow box,\nfrom the PBSMT model translation to replace the unknown word in the green box.\ndidate list to replace the corresponding <UNK> in\nthe translation from the word-level NMT model.\nStep 4 Step 2 and Step 3 are repeated until all\nthe context words have been searched. After re-\nmoving all the punctuation and the context words\nin the candidate list, the replacement word is the\none that most frequently appears in the candidate\nlist. If no candidate word is found, we just remove\nthe <UNK> without adding a word.\n2.3 Language Model Rescoring\nInstead of direct translation with NMT models, we\ngenerate several translation candidates using beam\nsearch with a beam size of ﬁve. We build the lan-\nguage model proposed by Merity et al. (2018b,a)\ntrained using a monolingual Czech dataset to\nrescore the generated translations. The scores are\ndetermined by the perplexity (PPL) of the gener-\nated sentences and the translation candidate with\nthe lowest PPL will be selected as the ﬁnal trans-\nlation.\n2.4 Model Ensemble\nEnsemble methods have been shown very effec-\ntive in many natural language processing tasks.\nWe apply an ensemble method by taking the top\nﬁve translations from word-level and subword-\nlevel NMT, and rescore all translations using our\npre-trained Czech language model mentioned in\n§2.3. Then, we select the best translation with the\nlowest perplexity.\n3 Experiments\n3.1 Data Pre-processing\nWe note that in the corpus, there are tokens rep-\nresenting quantity or date. Therefore, we delex-\nicalize the tokens using two special tokens: (1)\n<NUMBER> to replace all the numbers that express\na speciﬁc quantity, and (2) <DATE> to replace all\nthe numbers that express a date. Then, we retrieve\nthese numbers in the post-processing. There are\ntwo advantages of data pre-processing. First, re-\nplacing numbers with special tokens can reduce\nvocabulary size. Second, the special tokens are\nmore easily processed by the model.\n3.2 Data Post-processing\nSpecial Token Replacement In the pre-\nprocessing, we use the special tokens <NUMBER>\nand <DATE> to replace numbers that express a\nspeciﬁc quantity and date respectively. There-\nfore, in the post-processing, we need to restore\nthose numbers. We simply detect the pattern\n<NUMBER> and <DATE> in the original source\nsentences and then replace the special tokens in\nthe translated sentences with the corresponding\nnumbers detected in the source sentences. In order\nto make the replacement more accurate, we will\ndetect more complicated patterns like <NUMBER>\n/ <NUMBER> in the original source sentences. If\nthe translated sentences also have the pattern, we\nreplace this pattern <NUMBER> / <NUMBER>\nwith the corresponding numbers in the original\nsource sentences.\nQuotes Fixing The quotes are ﬁxed to keep\nthem the same as the source sentences.\nRecaser For all the models mentioned above\nthat work under a lower-case setting, a recaser im-\nplemented with Moses (Koehn et al., 2007) is ap-\nplied to convert the translations to the real cases.\nPatch-up From our observation, the ensemble\nNMT model lacks the ability to translate name en-\ntities correctly. We ﬁnd that words with capital\ncharacters are named entities, and those named en-\ntities in the source language may have the same\nform in the target language. Hence, we capture\nand copy these entities at the end of the translation\nif they does not exist in our translation.\n3.3 Training\nUnsupervised NMT The settings of the word-\nlevel NMT and subword-level NMT are the same,\nexcept the vocabulary size. We use a vocabulary\nsize of 50k in the word-level NMT setting and 40k\nin the subword-level NMT setting for both Ger-\nman and Czech. In the encoder and decoder, we\nuse a transformer (Vaswani et al., 2017) with four\nlayers and a hidden size of 512. We share all en-\ncoder parameters and only share the ﬁrst decoder\nlayer across two languages to ensure that the la-\ntent representation of the source sentence is robust\nto the source language. We train auto-encoding\nand back-translation during each iteration. As the\ntraining goes on, the importance of language mod-\neling become a less important compared to back-\ntranslation. Therefore the weight of auto-encoding\n(λin equation (1)) is decreasing during training.\nUnsupervised PBSMT The PBSMT is imple-\nmented with Moses using the same settings as\nthose in Lample et al. (2018b). The PBSMT model\nis trained iteratively. Both monolingual datasets\nfor the source and target languages consist of 12\nmillion sentences, which are taken from the latest\nparts of the WMT monolingual dataset. At each\niteration, two out of 12 million sentences are ran-\ndomly selected from the the monolingual dataset.\nLanguage Model According to the ﬁndings in\nCotterell et al. (2018), the morphological richness\nof a language is closely related to the performance\nof the model, which indicates that the language\nmodels will be extremely hard to train for Czech,\nas it is one of the most complex languages. We\ntrain the QRNN model with 12 million sentences\nrandomly sampled from the original WMT Czech\nmonolingual dataset, 2 which is also pre-processed\nin the way mentioned in §3.1. To maintain the\nquality of the language model, we enlarge the vo-\ncabulary size to three million by including all the\nwords that appear more than 15 times. Finally, the\nPPL of the language model on the test set achieves\n93.54.\n2http://www.statmt.org/wmt19/\nRecaser We use the recaser model provided in\nMoses and train the model with the two million\nlatest sentences in the Czech monolingual dataset.\nAfter the training procedure, the recaser can re-\nstore words to the form in which the maximum\nprobability occurs.\n3.4 PBSMT Model Selection\nThe BLEU (cased) score of the initialized phrase\ntable and models after training at different itera-\ntions are shown in Table 1. From comparing the\nresults, we observe that back-translation can im-\nprove the quality of the phrase table signiﬁcantly,\nbut after ﬁve iterations, the phrase table has hardly\nimproved. The PBSMT model at the sixth itera-\ntion is selected as the ﬁnal PBSMT model.\nModel BLEU Cased\nUnsupervised PBSMT\nUnsupervised Phrase Table 3.8\n+ Back-translation Iter. 1 6.6\n+ Back-translation Iter. 2 7.3\n+ Back-translation Iter. 3 7.5\n+ Back-translation Iter. 4 7.6\n+ Back-translation Iter. 5 7.7\n+ Back-translation Iter. 6 7.7\nTable 1: Results of PBSMT at different iterations\n3.5 Results\nThe performances of our ﬁnal model and other\nbaseline models are illustrated in Table 2. In\nthe baseline unsupervised NMT models, subword-\nlevel NMT outperforms word-level NMT by\naround a 1.5 BLEU score. Although the unsuper-\nvised PBSMT model is worse than the subword-\nlevel NMT model, leveraging generated pseudo-\nparallel data from the PBSMT model to ﬁne-\ntune the subword-level NMT model can still boost\nits performance. However, this pseudo-parallel\ndata from the PBSMT model can not improve the\nword-level NMT model since the large percentage\nof OOV words limits its performance. After ap-\nplying unknown words replacement to the word-\nlevel NMT model, the performance improves by\na BLEU score of around 2. Using the Czech\nlanguage model to re-score helps the model im-\nprove by around a 0.3 BLEU score each time. We\nalso use this language model to create an ensem-\nble of the best word-level and subword-level NMT\nmodel and achieve the best performance.\nModel BLEU BLEU Cased TER BEER 2.0 CharacterTER\nUnsupervised PBSMT\nUnsupervised phrase table 4 3.8 - 0.384 0.773\n+ Back-translation Iter. 6 8.3 7.7 0.887 0.429 0.743\nUnsupervised NMT\nSubword-level NMT 9.4 9.1 - 0.419 0.756\n+ ﬁne-tuning 9.8 9.5 0.832 0.424 0.756\n+ ﬁne-tuning + rescoring 10.3 10 0.833 0.426 0.749\nWord-level NMT 7.9 7.6 - 0.412 0.823\n+ ﬁne-tuning 7.9 7.7 - 0.413 0.819\n+ ﬁne-tuning + UWR 10.1 9.6 0.829 0.432 0.766\n+ ﬁne-tuning + UWR + rescoring 10.4 9.9 0.829 0.432 0.764\nModel Ensemble\nBest Word-level + Subword-level 10.6 10.2 0.829 0.429 0.755\n+ patch-up 10.6 10.2 0.833 0.430 0.757\nTable 2: Unsupervised translation results. We report the scores of several evaluation methods for every step of our\napproach. Except the result that is listed on the last line, all results are under the condition that the translations are\npost-processed without patch-up.\n4 Related Work\n4.1 Unsupervised Cross-lingual Embeddings\nCross-lingual word embeddings can provide a\ngood initialization for both the NMT and SMT\nmodels. In the unsupervised senario, Artetxe et al.\n(2017) independently trained embeddings in dif-\nferent languages using monolingual corpora, and\nthen learned a linear mapping to align them in a\nshared space based on a bilingual dictionary of\na negligibly small size. Conneau et al. (2018)\nproposed a fully unsupervised learning method\nto build a bilingual dictionary without using any\nforegone word pairs, but by considering words\nfrom two languages that are near each other as\npseudo word pairs. Lample and Conneau (2019)\nshowed that cross-lingual language model pre-\ntraining can learn a better cross-lingual embed-\ndings to initialize an unsupervised machine trans-\nlation model.\n4.2 Unsupervised Machine Translation\nIn Artetxe et al. (2018b) and Lample et al. (2018a),\nthe authors proposed the ﬁrst unsupervised ma-\nchine translation models which combines an auto-\nencoding language model and back-translation in\nthe training procedure. Lample et al. (2018b)\nillustrated that initialization, language modeling,\nand back-translation are key for both unsuper-\nvised neural and statistical machine translation.\nArtetxe et al. (2018a) combined back-translation\nand MERT (Och, 2003) to iteratively reﬁne the\nSMT model. Wu et al. (2019) proposed to dis-\ncard back-translation. Instead, they extracted and\nedited the nearest sentences in the target language\nto construct pseudo-parallel data, which was used\nas a supervision signal.\n5 Conclusion\nIn this paper, we propose to combine word-level\nand subword-level input representation in unsu-\npervised NMT training on a morphologically rich\nlanguage pair, German-Czech, without using any\nparallel data. Our results show the effectiveness\nof using language model rescoring to choose more\nﬂuent translation candidates. A series of pre-\nprocessing and post-processing approaches im-\nprove the quality of ﬁnal translations, particularly\nto replace unknown words with possible relevant\ntarget words.\nAcknowledgments\nWe would like to thank our colleagues Jamin Shin,\nAndrea Madotto, and Peng Xu for insightful dis-\ncussions. This work has been partially funded\nby ITF/319/16FP and MRP/055/18 of the Inno-\nvation Technology Commission, the Hong Kong\nSAR Government.\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451–462.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre.\n2018a. Unsupervised statistical machine translation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3632–3642.\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018b. Unsupervised neural ma-\nchine translation. In International Conference on\nLearning Representations.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nAlexis Conneau, Guillaume Lample, Marc’Aurelio\nRanzato, Ludovic Denoyer, and Herv´e J´egou. 2018.\nWord translation without parallel data. In Inter-\nnational Conference on Learning Representations\n(ICLR).\nRyan Cotterell, Sebastian J Mielke, Jason Eisner, and\nBrian Roark. 2018. Are all languages equally hard\nto language-model? In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 536–541.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning-Volume 70, pages 1243–1252. JMLR. org.\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor OK\nLi. 2018. Universal neural machine translation for\nextremely low resource languages. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 344–354.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the sixth\nworkshop on statistical machine translation, pages\n187–197. Association for Computational Linguis-\ntics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, et al. 2007. Moses: Open source\ntoolkit for statistical machine translation. In Pro-\nceedings of the 45th Annual Meeting of the Associa-\ntion for Computational Linguistics Companion Vol-\nume Proceedings of the Demo and Poster Sessions,\npages 177–180.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint\narXiv:1901.07291.\nGuillaume Lample, Alexis Conneau, Ludovic De-\nnoyer, and Marc’Aurelio Ranzato. 2018a. Unsu-\npervised machine translation using monolingual cor-\npora only. In International Conference on Learning\nRepresentations.\nGuillaume Lample, Myle Ott, Alexis Conneau, Lu-\ndovic Denoyer, and Marc’Aurelio Ranzato. 2018b.\nPhrase-based & neural unsupervised machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nNayeon Lee, Zihan Liu, and Pascale Fung. 2019. Team\nyeon-zi at semeval-2019 task 4: Hyperpartisan news\ndetection by de-noising weakly-labeled data. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pages 1052–1056.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018a. An Analysis of Neural Lan-\nguage Modeling at Multiple Scales. arXiv preprint\narXiv:1803.08240.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018b. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nFranz Josef Och. 2003. Minimum error rate training\nin statistical machine translation. In Proceedings of\nthe 41st Annual Meeting on Association for Compu-\ntational Linguistics-Volume 1, pages 160–167. As-\nsociation for Computational Linguistics.\nAlec Radford, Luke Metz, and Soumith Chintala.\n2016. Unsupervised representation learning with\ndeep convolutional generative adversarial networks.\n4th International Conference on Learning Represen-\ntations, ICLR 2016, San Juan, Puerto Rico, May 2-4,\n2016, Conference Track Proceedings.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), vol-\nume 1, pages 86–96.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), volume 1,\npages 1715–1725.\nReut Tsarfaty, Djam ´e Seddah, Yoav Goldberg, San-\ndra K¨ubler, Marie Candito, Jennifer Foster, Yannick\nVersley, Ines Rehbein, and Lamia Tounsi. 2010. Sta-\ntistical parsing of morphologically rich languages\n(spmrl): what, how and whither. In Proceedings of\nthe NAACL HLT 2010 First Workshop on Statistical\nParsing of Morphologically-Rich Languages, pages\n1–12. Association for Computational Linguistics.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan Gomez, Stephan Gouws, Llion\nJones, Łukasz Kaiser, Nal Kalchbrenner, Niki Par-\nmar, et al. 2018. Tensor2tensor for neural machine\ntranslation. In Proceedings of the 13th Conference\nof the Association for Machine Translation in the\nAmericas (Volume 1: Research Papers), volume 1,\npages 193–199.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st International\nConference on Neural Information Processing Sys-\ntems, pages 6000–6010. Curran Associates Inc.\nJiawei Wu, Xin Wang, and William Yang Wang. 2019.\nExtract and edit: An alternative to back-translation\nfor unsupervised neural machine translation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long and Short Papers), pages 1173–1183.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8968950510025024
    },
    {
      "name": "Machine translation",
      "score": 0.8819249868392944
    },
    {
      "name": "Language model",
      "score": 0.6856058239936829
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6593400239944458
    },
    {
      "name": "Natural language processing",
      "score": 0.6554363965988159
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.633404016494751
    },
    {
      "name": "Phrase",
      "score": 0.573303759098053
    },
    {
      "name": "Czech",
      "score": 0.5222169160842896
    },
    {
      "name": "Fluency",
      "score": 0.444486141204834
    },
    {
      "name": "Word (group theory)",
      "score": 0.4425315856933594
    },
    {
      "name": "Speech recognition",
      "score": 0.4414973556995392
    },
    {
      "name": "Example-based machine translation",
      "score": 0.42670461535453796
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 6
}