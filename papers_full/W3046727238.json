{
  "title": "MusiCoder: A Universal Music-Acoustic Encoder Based on Transformer",
  "url": "https://openalex.org/W3046727238",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2393719697",
      "name": "Zhao, Yilun",
      "affiliations": [
        "Zhejiang University",
        "University of Illinois Urbana-Champaign",
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2086050698",
      "name": "Guo Jia",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6828894009",
    "https://openalex.org/W2065342515",
    "https://openalex.org/W2112008792",
    "https://openalex.org/W3148040514",
    "https://openalex.org/W6600710808",
    "https://openalex.org/W6756821666",
    "https://openalex.org/W1976526581",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W3045349544",
    "https://openalex.org/W2889387262",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W6600225990",
    "https://openalex.org/W3015265920",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2191779130",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W6633978656",
    "https://openalex.org/W2137737117",
    "https://openalex.org/W3096485810",
    "https://openalex.org/W1596717185",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2066561607",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3003875258",
    "https://openalex.org/W2808471375",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4300957348",
    "https://openalex.org/W2963358591",
    "https://openalex.org/W4214784181",
    "https://openalex.org/W2989929945",
    "https://openalex.org/W2950060770",
    "https://openalex.org/W2604959036",
    "https://openalex.org/W2979476256",
    "https://openalex.org/W2954540134",
    "https://openalex.org/W3003673875",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2962813390",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963550089",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2767858146",
    "https://openalex.org/W2970680991",
    "https://openalex.org/W3091687589",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2986663693",
    "https://openalex.org/W3102568015",
    "https://openalex.org/W2996383576",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2580221632",
    "https://openalex.org/W3090958350",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2964121744"
  ],
  "abstract": null,
  "full_text": "MusiCoder: A Universal Music-Acoustic\nEncoder Based on Transformers⋆\nYilun Zhao1,2∗ and Jia Guo2\n1 Zhejiang University/University of Illinois at Urbana-Champaign Institute, Zhejiang\nUniversity, Haining, China\nzhaoyilun@zju.edu.cn\n2 YouKu Cognitive and Intelligent Lab, Alibaba Group, Hangzhou, China\n{yilun.zyl, gj243069}@alibaba-inc.com\nAbstract. Music annotation has always been one of the critical topics\nin the ﬁeld of Music Information Retrieval (MIR). Traditional models use\nsupervised learning for music annotation tasks. However, as supervised\nmachine learning approaches increase in complexity, the increasing need\nfor more annotated training data can often not be matched with available\ndata. In this paper, a new self-supervised music acoustic representation\nlearning approach named MusiCoder is proposed. Inspired by the success\nof BERT, MusiCoder builds upon the architecture of self-attention bidi-\nrectional transformers. Two pre-training objectives, including Contigu-\nous Frames Masking (CFM) and Contiguous Channels Masking (CCM),\nare designed to adapt BERT-like masked reconstruction pre-training to\ncontinuous acoustic frame domain. The performance of MusiCoder is\nevaluated in two downstream music annotation tasks. The results show\nthat MusiCoder outperforms the state-of-the-art models in both music\ngenre classiﬁcation and auto-tagging tasks. The eﬀectiveness of Musi-\nCoder indicates a great potential of a new self-supervised learning ap-\nproach to understand music: ﬁrst apply masked reconstruction tasks to\npre-train a transformer-based model with massive unlabeled music acous-\ntic data, and then ﬁnetune the model on speciﬁc downstream tasks with\nlabeled data.\nKeywords: Music Information Retrieval · Self-supervised Representa-\ntion Learning · Masked Reconstruction · Transformer\n1 Introduction\nThe amount of music has been growing rapidly over the past decades. As an\neﬀective measure for utilizing massive music data, automatically assigning one\nmusic clip a set of relevant tags, providing high-level descriptions about the music\nclip such as genre, emotion, theme, are of great signiﬁcance in MIR community\n[5, 39]. Some researchers have applied several supervised learning models [14,\n⋆ Supported by Alibaba Group, and Key Laboratory of Design Intelligence and Digital\nCreativity of Zhejiang Province, Zhejiang University\narXiv:2008.00781v2  [eess.AS]  31 Jan 2021\n2 Y. Zhao, J. Guo\n20, 22, 28], which are trained on human-annotated music data. However, the\nperformance of supervised learning method are likely to be limited by the size\nof labeled dataset, which is expensive and time consuming to collect.\nRecently, self-supervised pre-training models [23,24,31,37], especially BERT,\ndominate Natural Language Processing (NLP) community. BERT proposes a\nMasked Language Model (MLM) pre-training objective, which can learn a pow-\nerful language representation by reconstructing the masked input sequences in\npre-training stage. The intuition behind this design is that a model available to\nrecover the missing content should have learned a good contextual representa-\ntion. In particular, BERT and its variants [25, 38, 40] have reached signiﬁcant\nimprovements on various NLP benchmark tasks [36]. Compared with the text\ndomain whose inputs are discrete word tokens, in acoustics domain, the inputs\nare usually multi-dimensional feature vectors (e.g., energy in multiple frequency\nbands) of each frame, which are continuous and smoothly changed over time.\nTherefore, some particular designs need to be introduced to bridge the gaps be-\ntween discrete text and contiguous acoustic frames. We are the ﬁrst to apply the\nidea of masked reconstruction pre-training to the continuous music acoustic do-\nmain. In this paper, a new self-supervised pre-training scheme called MusiCoder\nis proposed, which can learn a powerful acoustic music representations through\nreconstructing masked acoustic frame sequence in pre-training stage.\nOur contributions can be summarized as:\n1. We present a new self-supervised pre-training model named MusiCoder. Mu-\nsiCoder builds upon the structure of multi-layer bidirectional self-attention\ntransformers. Rather than relying on massive human-labeled data, Musi-\nCoder can learn a powerful music representation from unlabeled music acous-\ntic data, which is much easier to collect.\n2. The reconstruction procedure of BERT-like model is adapted from classi-\nﬁcation task to regression task. In other word, MusiCoder can reconstruct\ncontinuous acoustic frames directly, which avoids an extra transformation\nfrom continuous frames to discrete word tokens before pre-training.\n3. Two pre-training objectives, including Contiguous Frames Masking (CFM)\nand Contiguous Channels Masking (CCM), are proposed to pre-train Musi-\nCoder. The ablation study shows that both CFM and CCM objectives can\neﬀectively improve the performance of MusiCoder pre-training.\n4. The MusiCoder is evaluated on two downstream tasks: GTZAN music genre\nclassiﬁcation and MTG-Jamendo music auto-tagging. And MusiCoder out-\nperforms the SOTA model in both tasks. The success of MusiCoder indicates\na great potential of applying transformer-based masked reconstruction pre-\ntraining in Music Information Retrieval (MIR) ﬁeld.\n2 Related Work\nIn the past few years, pre-training models and self-supervised representation\nlearning have achieved great success in NLP community. Huge amount of self-\nMusiCoder 3\nFig. 1.System overview of the MusiCoder model\nsupervised pre-training models based on multi-layer self-attention transform-\ners [34], such as BERT [12], GPT [30], XLNet [38], Electra [9] are proposed.\nAmong them, BERT is perhaps the most classic and popular one due to its sim-\nplicity and outstanding performance. Speciﬁcally, BERT is designed to recon-\nstruct the masked input sequences in pre-training stage. Through reconstructing\nthe missing content from a given masked sequence, the model can learn a pow-\nerful contextual representation.\nMore recently, the success of BERT in NLP community draws the atten-\ntion of researchers in acoustic signal processing ﬁeld. Some pioneering works\n[2, 23, 24, 31, 37] have shown the eﬀectiveness of adapting BERT to Automatic\n4 Y. Zhao, J. Guo\nSpeech Recognition (ASR) research. Speciﬁcally, they design some speciﬁc pre-\ntraining objectives to bridge the gaps between discrete text and contiguous\nacoustic frames. In vq-wav2vec [2], input speech audio is ﬁrst discretized to\na K-way quantized embedding space by learning discrete representation from\naudio samples. However, the quantization process requires massive comput-\ning resources and is against the continuous nature of acoustic frames. Some\nworks [7, 23, 24, 31, 37] design a modiﬁed version of BERT to directly utilize\ncontinuous speech. In [7,23,24], continuous frame-level masked reconstruction is\nadapted in BERT-like pre-training stage. In [37], SpecAugment [27] is applied\nto mask input frames. And [31] learns by reconstructing from shuﬄed acoustic\nframe orders rather than masked frames.\nAs for MIR community, representation learning has been popular for many\nyears. Several convolutional neural networks (CNNs) based supervised meth-\nods [8, 14, 20, 22, 28] are proposed in music understanding tasks. They usually\nemploy variant depth of convolutional layers on Mel-spectrogram based repre-\nsentations or raw waveform signals of the music to learn eﬀective music repre-\nsentation, and append fully connected layers to predict relevant annotation like\nmusic genres, tags. However, training such CNN-based models usually requires\nmassive human-annotated data. And in [6, 17], researchers show that compared\nwith supervised learning methods, using self-supervision on unlabeled data can\nsigniﬁcantly improve the robustness of the model. Recently, the self-attention\ntransformer has shown promising results in symbolic music generation area. For\nexample, Music Transformer [18] and Pop Music Transformer [19] employ rela-\ntive attention to capture long-term structure from music MIDI data, which can\nbe used as discrete word tokens directly. However, compared with raw music\naudio, the size of existing MIDI dataset is limited. Moreover, transcription from\nraw audio to MIDI ﬁles is time-consuming and not accurate. In this paper, we\nproposed MusiCoder, a universal music-acoustic encoder based on transformers.\nSpeciﬁcally, MusiCoder is ﬁrst pre-trained on massive unlabeled music acoustic\ndata, and then ﬁnetuned on speciﬁc downstream music annotation tasks using\nlabeled data.\n3 MusiCoder Model\nA universal transformer-based encoder named MusiCoder is presented for music\nacoustic representation learning. The system overview of proposed MusiCoder\nis shown in Fig. 1.\n3.1 Input representation\nFor each input frameti, its vector representationxi is obtained by ﬁrst projecting\nti linearly to hidden dimension Hdim, and then added with sinusoidal positional\nencoding [34] deﬁned as following:\nPE(pos,2i) = sin(pos/100002i/Hdim )\nPE(pos,2i+1) = cos(pos/100002i/Hdim )\n(1)\nMusiCoder 5\nThe positional encoding is used to inject information about the relative position\nof acoustic frames. The design of positional encoding makes the transformer\nencoder aware of the music sequence order.\n3.2 Transformer Encoder\nA multi-layer bidirectional self-attention transformer encoder [34] is used to en-\ncode the input music acoustic frames. Speciﬁcally, a L-layer transformer is used\nto encode the input vectors X = {xi}N\ni=1 as:\nHl = Transformerl(Hl−1) (2)\nwhere l ∈[1,L], H0 = X and HL = [hL\n1 ,...,h L\nN ]. We use the hidden vector hL\ni\nas the contextualized representation of the input token ti. The architecture of\ntransformer encoder is shown in Fig. 1.\n3.3 Pre-training Objectives\nThe main idea of masked reconstruction pre-training is to perturb the inputs\nby randomly masking tokens with some probability, and reconstruct the masked\ntokens at the output. In the pre-training process, a reconstruction module, which\nconsists of two layers of feed-forward network with GeLU activation [16] and\nlayer-normalization [1], is appended to predict the masked inputs. The module\nuses the output of the last MusiCoder encoder layer as its input. Moreover,\ntwo new pre-training objectives are presented to help MusiCoder learn acoustic\nmusic representation.\nObjective 1: Contiguous Frames Masking (CFM). To avoid the model\nexploiting local smoothness of acoustic frames, rather than only mask one span\nwith ﬁxed number of consecutive frames [24], we mask several spans of consecu-\ntive frames dynamically. Given a sequence of input frames X = (x1,x2,...,x n),\nwe select a subsetY ⊂Xby iteratively sampling contiguous input frames (spans)\nuntil the masking budget (e.g., 15% of X) has been spent. At each iteration, the\nspan length is ﬁrst sampled from a geometric distribution ℓ ∼Geo(p). Then\nthe starting point of the masked span is randomly selected. We set p = 0 .2,\nℓmin = 2 and ℓmax = 7. The corresponding mean length of span is around 3.87\nframes (≈179.6ms). In each masked span, the frames are masked according to\nthe following policy: 1) replace all frames with zero in 70% of the case. Since\neach dimension of input frames are normalized to have zero mean value, set-\nting the masked value to zero is equivalent to setting it to the mean value. 2)\nreplace all frames with a random masking frame in 20% of the case. 3) keep\nthe original frames unchanged in the rest 10% of the case. Since MusiCoder\nwill only receive acoustic frames without masking during inference time, policy\n3) allows the model to receive real inputs during pre-training, and resolves the\npretrain-ﬁntune inconsistency problem [12].\n6 Y. Zhao, J. Guo\nObjective 2: Contiguous Channels Masking (CCM). The intuition of\nchannel masking is that a model available to predict the partial loss of chan-\nnel information should have learned a high-level understanding along the chan-\nnel axis. For log-mel spectrum and log-CQT features, a block of consecutive\nchannels is randomly masked to zero for all time steps across the input se-\nquence of frames. Speciﬁcally, the number of masked blocks, n, is ﬁrst sampled\nfrom {0,1,...,H }uniformly. Then a starting channel index is sampled from\n{0,1,...,H −n}, where H is the number of total channels.\nPre-training Objective Function.\nLoss(x) =\n{0.5 ·x2 if |x|<1\n|x|−0.5 otherwise (3)\nThe Huber Loss [15] is used to minimize reconstruction error between masked\ninput features and corresponding encoder output. Huber Loss is a robust L1 loss\nthat is less sensitive to outliers. And in our preliminary experiments, we found\nthat compared with L1 loss used in [24], using Huber loss will make the training\nprocess easier to converge.\n3.4 MusiCoder Model Setting\nWe primarily report experimental results on two models: MusiCoderBase and\nMusiCoderLarge. The model settings are listed in Table 1. The number of Trans-\nformer block layers, the size of hidden vectors, the number of self-attention heads\nare represented as Lnum, Hdim, Anum, respectively.\nTable 1.The proposed model settings\nLnum Hdim Anum #parameters\nMusiCoderBase 4 768 12 29.3M\nMusiCoderLarge 8 1024 16 93.1M\n4 Experiment Setup\n4.1 Dataset Collection and Preprocess\nTable 2.Statistics on the datasets used for pre-training and downstream tasks\nTask Datasets #clips duration\n(hours)\nDescription\nPre-training\nMusic4all 109.2K 908.7 –\nFMA-large 106.3K 886.4 –\nMTG-Jamendo1 37.3K 1346.9 –\nClassiﬁcation GTZAN 1000 8.3 100 clips for each genre\nAuto-tagging MTG-Jamendo 2 18.4K 157.1 56 mood/theme tags\n1,2 For MTG-Jamendo dataset, we removed music clips used in Auto-tagging task\nwhen pre-training.\nMusiCoder 7\nAs shown in Table 2, the pre-training data were aggregated from three datasets:\nMusic4all [13], FMA-Large [11] and MTG-Jamendo [4]. Both Music4all and\nFMA-Large datasets provide 30-seconds audio clips in .mp3 format for each\nsong. And MTG-Jamendo dataset contains 55.7K music tracks, each with a du-\nration of more than 30s. Since the maximum time stamps of MusiCoder is set\nto 1600, those music tracks exceeding 35s would be cropped into several music\nclips, the duration of which was randomly picked from 10s to 35s.\nGTZAN music genre classiﬁcation [32] and MTG-Jamendo music auto-tagging\ntasks [4] were used to evaluate the performance of ﬁnetuned MusiCoder. GTZAN\nconsists of 1000 music clips divided into ten diﬀerent genres (blues, classical,\ncountry, disco, hip-hop, jazz, metal, pop, reggae & rock). Each genre consists\nof 100 music clips in .wav format with a duration of 30s. To avoid seeing any\ntest data in downstream tasks, for pre-training data, we ﬁltered out those music\nclips appearing in downstream tasks.\nAudio Preprocess. The acoustic music analysis library, Librosa [26], pro-\nvides ﬂexible ways to extract features related to timbre, harmony, and rhythm\naspect of music. In our work, Librosa was used to extract the following features\nfrom a given music clip: Mel-scaled Spectrogram, Constant-Q Transform (CQT),\nMel-frequency cepstral coeﬃcients (MFCCs), MFCCs delta and Chromagram,\nas detailed in Table 4. Each kind of features was extracted at the sampling rate\nof 44,100Hz, with a Hamming window size of 2048 samples (≈46 ms) and a hop\nsize of 1024 samples ( ≈23 ms). The Mel Spectrogram and CQT features were\ntransformed to log amplitude with S\n′\n= ln(10 ·S+ ϵ), where S, ϵrepresents the\nfeature and an extremely small number, respectively. Then Cepstral Mean and\nVariance Normalization (CMVN) [29,35] were applied to the extracted features\nfor minimizing distortion caused by noise contamination. Finally these normal-\nized features were concatenated to a 324-dim feature, which was later used as\nthe input of MusiCoder.\nTable 3.Acoustic features of music extracted by Librosa\nFeature Characteristic Dimension\nChromagram Melody, Harmony 12\nMFCCs Pitch 20\nMFCCs delta Pitch 20\nMel-scaled Spectrogram Raw Waveform 128\nConstant-Q Transform Raw Waveform 144\n4.2 Training Setup\nAll our experiments were conducted on 5 GTX 2080Ti and can be reproduced\non any machine with GPU memory more than 48GBs. In pre-training stage,\nMusiCoderBase and MusiCoderLarge were trained with a batch size of 64 for\n200k and 500k steps, respectively. We applied the Adam optimizer [21] with\n8 Y. Zhao, J. Guo\nβ1 = 0 .9, β2 = 0 .999 and ϵ = 10 −6. And the learning rate were varied with\nwarmup schedule [34] according to the formula:\nlrate= H−0.5\ndim ·min(step num−0.5,step num·warmup steps−1.5) (4)\nwhere warmup steps was set as 8000. Moreover, library Apex was used to ac-\ncelerate the training process and save GPU memory.\nFor downstream tasks, we performed an exhaustive search on the following\nsets of parameters. The model that performed the best on the validation set\nwas selected. All the other training parameters remained the same as those in\npre-training stage:\nTable 4.Parameter settings for downstream tasks\nParameter Candidate Value\nBatch size 16, 24, 32\nLearning Rate 2e-5, 3e-5, 5e-5\nEpoch 2, 3, 4\nDropout Rate 0.05, 0.1\n5 Results\n5.1 Music Genre Classiﬁcation\nTable 5.Results of GTZAN Music Classiﬁcation task\nModels accuracy\nhand-crafted features + SVM [3] 87.9%\nCNN + SVM [8] 89.8%\nCNN+MLP based ensemble [14] 94.2%\nMusiCoderBase 94.2%\nMusiCoderLarge 94.3%\nTheoretical Maximum Score [32] 94.5%\nSince GTZAN dataset only contains 1000 music clips, the experiments were\nconducted in a ten-fold cross-validation setup. For each fold, 80, 20 songs of\neach genre were randomly selected and placed into the training and validation\nsplit, respectively. The ten-fold average accuracy score is shown in Table 5. In\nprevoious work, [3] applied low-level music features and rich statistics to pre-\ndict music genres. In [8], researchers ﬁrst used a CNN based model, which was\ntrained on music auto-tagging tasks, to extract features. These extracted fea-\ntures were then applied on SVM [33] for genre classiﬁcation. In [14], the authors\ntrained two models: a CNN based model trained on a variety of spectral and\nrhythmic features, and an MLP network trained on features, which were ex-\ntracted from a model for music auto-tagging task. Then these two models were\nMusiCoder 9\ncombined in a majority voting ensemble setup. The authors reported the accu-\nracy score as 94.2%. Although some other works reported their accuracy score\nhigher than 94.5%, we set 94.5% as the state-of-the-art accuracy according to\nthe analysis in [32], which demonstrates that the inherent noise (e.g., repetitions,\nmis-labelings, distortions of the songs) in GTZAN dataset prevents the perfect\naccuracy score from surpassing 94.5%. In the experiment, MusiCoderBase and\nMusiCoderLarge achieve accuracy of 94.2% and 94.3%, respectively. The pro-\nposed models outperform the state-of-the-art models and achieve accuracy score\nclose to the ideal value.\n5.2 Music Auto-Tagging\nTable 6.Results of MTG-Jamendo Music Auto-tagging task\nModels ROC-AUC\nmacro\nPR-AUC\nmacro\nVQ-VAE+CNN [20] 72.07% 10.76%\nVGGish [4] 72.58% 10.77%\nCRNN [22] 73.80% 11.71%\nFA-ResNet [22] 75.75% 14.63%\nSampleCNN (reproduced) [28] 76.93% 14.92%\nShake-FA-ResNet [22] 77.17% 14.80%\nOurs\nMusiCoderBase w/o pre-training 77.03% 15.02%\nMusiCoderBase with CCM 81.93% 19.49%\nMusiCoderBase with CFM 81.38% 19.51%\nMusiCoderBase with CFM+CCM 82.57% 20.87%\nMusiCoderLarge with CFM+CCM 83.82% 22.01%\nFor the music auto-tagging task, two sets of performance measurements,\nROC-AUC macro and PR-AUC macro, were applied. ROC-AUC can lead to\nover-optimistic scores when data are unbalanced [10]. Since the music tags given\nin the MTG-Jamendo dataset are highly unbalanced [4], the PR-AUC metric was\nalso introduced for evaluation. The MusiCoder model was compared with other\nstate-of-the-art models competing in the challenge of MediaEval 2019: Emo-\ntion and Theme Recognition in Music Using Jamendo [4]. We used the same\ntrain-valid-test data splits as the challenge. The results are shown in Table 6.\nFor VQ-VAE+CNN [20], VGGish [4], CRNN [22], FA-ResNet [22], Shake-FA-\nResNet [22] models, we directly used the evaluation results posted in the com-\npetition leaderboard3. For SampleCNN [28], we reproduced the work according\nto the oﬃcial implementation4. As the results suggest, the proposed MusiCoder\nmodel achieves new state-of-the-art results in music auto-tagging task.\n3 https://multimediaeval.github.io/2019-Emotion-and-Theme-Recognition-in-Music-Task/\nresults\n4 https://github.com/tae-jun/sample-cnn\n10 Y. Zhao, J. Guo\nAblation Study. Ablation study were conducted to better understand the\nperformance of MusiCoder. The results are also shown in Table 6. According to\nthe experiemnt, even without pre-training, MusiCoderBase can still outperform\nmost SOTA models, which indicates the eﬀectiveness of transformer-based ar-\nchitecture. When MusiCoderBase is pre-trained with objective CCM or CFM\nonly, a signiﬁcant improvement over MusiCoderBase without pre-training is ob-\nserved. And MusiCoderBase with CCM and CFM pre-training objectives com-\nbined achieves better results. The improvement indicates the eﬀectiveness of\npre-training stage. And it shows that the designed pre-training objectives CCM\nand CFM are both the key elements that drives pre-trained MusiCoder to learn\na powerful music acoustic representation. We also explore the eﬀect of model size\non downstream task accuracy. In the experiment, MusiCoderLarge outperforms\nMusiCoderBase, which reﬂects that increasing the model size of MusiCoder will\nlead to continual improvements.\n6 Conclusion\nIn this paper, we propose MusiCoder, a universal music-acoustic encoder based\non transformers. Rather than relying on massive human labeled data which is\nexpensive and time consuming to collect, MusiCoder can learn a strong music\nrepresentation from unlabeled music acoustic data. Two new pre-training ob-\njectives Contiguous Frames Masking (CFM) and Contiguous Channel Masking\n(CCM) are designed to improve the pre-training stage in continuous acoustic\nframe domain. The eﬀectiveness of proposed objectives is evaluated through\nextensive ablation studies. Moreover, MusiCoder outperforms the state-of-the-\nart model in music genre classiﬁcation on GTZAN dataset and music auto-\ntagging on MTG-Jamendo dataset. Our work shows a great potential of adapting\ntransformer-based masked reconstruction pre-training scheme to MIR commu-\nnity. Beyond improving the model, we plan to extend MusiCoder to other music\nunderstanding tasks (e.g., music emotion recognition, chord estimation, music\nsegmentation). We believe the future prospects for large scale representation\nlearning from music acoustic data look quite promising.\nReferences\n1. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv preprint\narXiv:1607.06450 (2016)\n2. Baevski, A., Schneider, S., Auli, M.: vq-wav2vec: Self-supervised learning of dis-\ncrete speech representations. arXiv preprint arXiv:1910.05453 (2019)\n3. Baniya, B.K., Lee, J., Li, Z.N.: Audio feature reduction and analysis for automatic\nmusic genre classiﬁcation. In: 2014 IEEE International Conference on Systems,\nMan, and Cybernetics (SMC). pp. 457–462. IEEE (2014)\n4. Bogdanov, D., Won, M., Tovstogan, P., Porter, A., Serra, X.: The mtg-jamendo\ndataset for automatic music tagging. In: Machine Learning for Music Discovery\nWorkshop, International Conference on Machine Learning (ICML 2019). Long\nBeach, CA, United States (2019), http://hdl.handle.net/10230/42015\nMusiCoder 11\n5. Bu, J., Tan, S., Chen, C., Wang, C., Wu, H., Zhang, L., He, X.: Music recom-\nmendation by uniﬁed hypergraph: combining social media information and music\ncontent. In: Proceedings of the 18th ACM international conference on Multimedia.\npp. 391–400 (2010)\n6. Carmon, Y., Raghunathan, A., Schmidt, L., Duchi, J.C., Liang, P.S.: Unlabeled\ndata improves adversarial robustness. In: Advances in Neural Information Process-\ning Systems. pp. 11192–11203 (2019)\n7. Chi, P.H., Chung, P.H., Wu, T.H., Hsieh, C.C., Li, S.W., Lee, H.y.: Audio al-\nbert: A lite bert for self-supervised learning of audio representation. arXiv preprint\narXiv:2005.08575 (2020)\n8. Choi, K., Fazekas, G., Sandler, M., Cho, K.: Transfer learning for music classiﬁca-\ntion and regression tasks. arXiv preprint arXiv:1703.09179 (2017)\n9. Clark, K., Luong, M.T., Le, Q.V., Manning, C.D.: Electra: Pre-training text en-\ncoders as discriminators rather than generators. arXiv preprint arXiv:2003.10555\n(2020)\n10. Davis, J., Goadrich, M.: The relationship between precision-recall and roc curves.\nIn: Proceedings of the 23rd international conference on Machine learning. pp. 233–\n240 (2006)\n11. Deﬀerrard, M., Benzi, K., Vandergheynst, P., Bresson, X.: Fma: A dataset for\nmusic analysis. arXiv preprint arXiv:1612.01840 (2016)\n12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805\n(2018)\n13. Domingues, M., Pegoraro Santana, I., Pinhelli, F., Donini, J., Catharin, L., Man-\ngolin, R., Costa, Y., Feltrim, V.D.: Music4all: A new music database and its ap-\nplications (07 2020). https://doi.org/10.1109/IWSSIP48289.2020.9145170\n14. Ghosal, D., Kolekar, M.H.: Music genre recognition using deep neural networks\nand transfer learning. In: Interspeech. pp. 2087–2091 (2018)\n15. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE international conference on\ncomputer vision. pp. 1440–1448 (2015)\n16. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 (2016)\n17. Hendrycks, D., Mazeika, M., Kadavath, S., Song, D.: Using self-supervised learning\ncan improve model robustness and uncertainty. In: Advances in Neural Information\nProcessing Systems. pp. 15663–15674 (2019)\n18. Huang, C.Z.A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N.,\nDai, A.M., Hoﬀman, M.D., Dinculescu, M., Eck, D.: Music transformer: Gener-\nating music with long-term structure. In: International Conference on Learning\nRepresentations (2018)\n19. Huang, Y.S., Yang, Y.H.: Pop music transformer: Generating music with rhythm\nand harmony. arXiv preprint arXiv:2002.00212 (2020)\n20. Hung, H.T., Chen, Y.H., Mayerl, M., Zangerle, M.V.E., Yang, Y.H.: Mediaeval\n2019 emotion and theme recognition task: A vq-vae based approach\n21. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014)\n22. Koutini, K., Chowdhury, S., Haunschmid, V., Eghbal-zadeh, H., Widmer, G.: Emo-\ntion and theme recognition in music with frequency-aware rf-regularized cnns.\narXiv preprint arXiv:1911.05833 (2019)\n23. Ling, S., Liu, Y., Salazar, J., Kirchhoﬀ, K.: Deep contextualized acoustic repre-\nsentations for semi-supervised speech recognition. In: ICASSP 2020-2020 IEEE\n12 Y. Zhao, J. Guo\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP).\npp. 6429–6433. IEEE (2020)\n24. Liu, A.T., Yang, S.w., Chi, P.H., Hsu, P.c., Lee, H.y.: Mockingjay: Unsupervised\nspeech representation learning with deep bidirectional transformer encoders. In:\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). pp. 6419–6423. IEEE (2020)\n25. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692 (2019)\n26. McFee, B., Raﬀel, C., Liang, D., Ellis, D.P., McVicar, M., Battenberg, E., Nieto,\nO.: librosa: Audio and music signal analysis in python. In: Proceedings of the 14th\npython in science conference. vol. 8, pp. 18–25 (2015)\n27. Park, D.S., Chan, W., Zhang, Y., Chiu, C.C., Zoph, B., Cubuk, E.D., Le, Q.V.:\nSpecaugment: A simple data augmentation method for automatic speech recogni-\ntion. arXiv preprint arXiv:1904.08779 (2019)\n28. Pons, J., Nieto, O., Prockup, M., Schmidt, E., Ehmann, A., Serra, X.: End-to-end\nlearning for music audio tagging at scale. arXiv preprint arXiv:1711.02520 (2017)\n29. Pujol, P., Macho, D., Nadeu, C.: On real-time mean-and-variance normalization of\nspeech recognition features. In: 2006 IEEE international conference on acoustics\nspeech and signal processing proceedings. vol. 1, pp. I–I. IEEE (2006)\n30. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language\nunderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws.\ncom/openai-assets/researchcovers/languageunsupervised/language understanding\npaper. pdf (2018)\n31. Song, X., Wang, G., Wu, Z., Huang, Y., Su, D., Yu, D., Meng, H.: Speech-xlnet:\nUnsupervised acoustic model pretraining for self-attention networks. arXiv preprint\narXiv:1910.10387 (2019)\n32. Sturm, B.L.: The gtzan dataset: Its contents, its faults, their eﬀects on evaluation,\nand its future use. arXiv preprint arXiv:1306.1461 (2013)\n33. Suykens, J.A., Vandewalle, J.: Least squares support vector machine classiﬁers.\nNeural processing letters 9(3), 293–300 (1999)\n34. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998–6008 (2017)\n35. Viikki, O., Laurila, K.: Cepstral domain segmental feature vector normalization for\nnoise robust speech recognition. Speech Communication 25(1-3), 133–147 (1998)\n36. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: Glue: A multi-\ntask benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461 (2018)\n37. Wang, W., Tang, Q., Livescu, K.: Unsupervised pre-training of bidirectional speech\nencoders via masked reconstruction. In: ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP). pp. 6889–6893.\nIEEE (2020)\n38. Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet:\nGeneralized autoregressive pretraining for language understanding. In: Advances\nin neural information processing systems. pp. 5753–5763 (2019)\n39. Zhang, K., Zhang, H., Li, S., Yang, C., Sun, L.: The pmemo dataset for music\nemotion recognition. In: Proceedings of the 2018 acm on international conference\non multimedia retrieval. pp. 135–142 (2018)\n40. Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., Liu, Q.: Ernie: Enhanced language\nrepresentation with informative entities. arXiv preprint arXiv:1905.07129 (2019)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8529281616210938
    },
    {
      "name": "Transformer",
      "score": 0.7104592323303223
    },
    {
      "name": "Encoder",
      "score": 0.566768229007721
    },
    {
      "name": "Speech recognition",
      "score": 0.5433775186538696
    },
    {
      "name": "Annotation",
      "score": 0.543094277381897
    },
    {
      "name": "Music information retrieval",
      "score": 0.5339675545692444
    },
    {
      "name": "Artificial intelligence",
      "score": 0.510499119758606
    },
    {
      "name": "Feature learning",
      "score": 0.4269496202468872
    },
    {
      "name": "Transcription (linguistics)",
      "score": 0.42177045345306396
    },
    {
      "name": "Machine learning",
      "score": 0.34749865531921387
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Musical",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 14
}