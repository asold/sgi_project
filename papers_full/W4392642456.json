{
    "title": "Optimizing Large Language Models on Multi-Core CPUs: A Case Study of the BERT Model",
    "url": "https://openalex.org/W4392642456",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3208557909",
            "name": "Lanxin Zhao",
            "affiliations": [
                "Hunan International Economics University",
                "Hunan University"
            ]
        },
        {
            "id": "https://openalex.org/A2098751707",
            "name": "Wanrong Gao",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2121272402",
            "name": "Jianbin Fang",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3208557909",
            "name": "Lanxin Zhao",
            "affiliations": [
                "Hunan International Economics University",
                "Hunan University"
            ]
        },
        {
            "id": "https://openalex.org/A2098751707",
            "name": "Wanrong Gao",
            "affiliations": [
                "National University of Defense Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2121272402",
            "name": "Jianbin Fang",
            "affiliations": [
                "National University of Defense Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3185341429",
        "https://openalex.org/W6679775712",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W6685158001",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2507974895",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3011411500",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W4281485151",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3127904641",
        "https://openalex.org/W2788193959",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W131533222",
        "https://openalex.org/W6682653460",
        "https://openalex.org/W3033187248",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6757107679",
        "https://openalex.org/W4211148418",
        "https://openalex.org/W2131744502"
    ],
    "abstract": "The BERT model is regarded as the cornerstone of various pre-trained large language models that have achieved promising results in recent years. This article investigates how to optimize the BERT model in terms of fine-tuning speed and prediction accuracy, aiming to accelerate the execution of the BERT model on a multi-core processor and improve its prediction accuracy in typical downstream natural language processing tasks. Our contributions are two-fold. First, we port and parallelize the fine-tuning training of the BERT model on a multi-core shared-memory processor. We port the BERT model onto a multi-core processor platform to accelerate the fine-tuning training process of the model for downstream tasks. Second, we improve the prediction performance of typical downstream natural language processing tasks through fine-tuning the model parameters. We select five typical downstream natural language processing tasks (CoLA, SST-2, MRPC, RTE, and WNLI) and perform optimization on the multi-core platform, taking the hyperparameters of batch size, learning rate, and training epochs into account. Our experimental results show that, by increasing the number of CPUs and the number of threads, the model training time can be significantly reduced. We observe that the reduced time is primarily concentrated in the self-attention mechanism. Our further experimental results show that setting reasonable hyperparameters can improve the accuracy of the BERT model when applied to downstream tasks and that appropriately increasing the batch size under conditions of sufficient computing resources can significantly reduce training time.",
    "full_text": null
}