{
  "title": "Transformers predicting the future. Applying attention in next-frame and time series forecasting",
  "url": "https://openalex.org/W3195162744",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5044437283",
      "name": "Radostin Cholakov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019054623",
      "name": "Todor Kolev",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3185826690",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W2907502844",
    "https://openalex.org/W3149261839",
    "https://openalex.org/W3109635183",
    "https://openalex.org/W2970631142",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3015256491",
    "https://openalex.org/W3169291081",
    "https://openalex.org/W3152733922",
    "https://openalex.org/W2801889078",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W2116435618",
    "https://openalex.org/W2147269409",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W2971074500",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "Recurrent Neural Networks were, until recently, one of the best ways to capture the timely dependencies in sequences. However, with the introduction of the Transformer, it has been proven that an architecture with only attention-mechanisms without any RNN can improve on the results in various sequence processing tasks (e.g. NLP). Multiple studies since then have shown that similar approaches can be applied for images, point clouds, video, audio or time series forecasting. Furthermore, solutions such as the Perceiver or the Informer have been introduced to expand on the applicability of the Transformer. Our main objective is testing and evaluating the effectiveness of applying Transformer-like models on time series data, tackling susceptibility to anomalies, context awareness and space complexity by fine-tuning the hyperparameters, preprocessing the data, applying dimensionality reduction or convolutional encodings, etc. We are also looking at the problem of next-frame prediction and exploring ways to modify existing solutions in order to achieve higher performance and learn generalized knowledge.",
  "full_text": "TRANSFORMERS PREDICTING THE FUTURE . A PPLYING\nATTENTION IN NEXT -FRAME AND TIME SERIES FORECASTING .\nRadostin Cholakov\nHigh School of Mathematics\n\"Acad. Kiril Popov\" - Plovdiv, Bulgaria\nrsg.group.here@gmail.com\nTodor Kolev\nComrade Cooperative\nSoﬁa, Bulgaria\nt.kolev@comrade.coop\nABSTRACT\nRecurrent Neural Networks were, until recently, one of the best ways to capture\nthe timely dependencies in sequences. However, with the introduction of the Trans-\nformer, it has been proven that an architecture with only attention-mechanisms\nwithout any RNN can improve on the results in various sequence processing tasks\n(e.g. NLP). Multiple studies since then have shown that similar approaches can be\napplied for images, point clouds, video, audio or time series forecasting. Further-\nmore, solutions such as the Perceiver or the Informer have been introduced to expand\non the applicability of the Transformer. Our main objective is testing and evaluating\nthe effectiveness of applying Transformer-like models on time series data, tackling\nsusceptibility to anomalies, context awareness and space complexity by ﬁne-tuning\nthe hyperparameters, preprocessing the data, applying dimensionality reduction or\nconvolutional encodings, etc. We are also looking at the problem of next-frame\nprediction and exploring ways to modify existing solutions in order to achieve higher\nperformance and learn generalized knowledge.\nKeywords Transformer ·time series forecasting ·next-frame prediction\n1 Introduction\nSince its introduction, the Transformer [ 1] has\nrevolutionized how neural networks can process\nsequential data and is currently the go to solution\nfor a wide variety of natural language processing\ntasks. Analogous models are also being applied\nfor image and video, point clouds [2], sound and\ntime series data.\nThe Transformer adopts an encoder-decoder\nstructure where the core function of each encoder\nlayer is to generate information about which parts\nof the inputs are relevant to each other. The de-\ncoder part does the opposite, taking all the en-\ncodings and using their incorporated contextual\ninformation to generate an output sequence.\nThe inputs and outputs (target sequences) are\nﬁrst embedded into an n-dimensional space and\nsince there are no recurrent networks that can\nremember how sequences are fed into a model\nthe positions are added to the embedded repre-\nsentation of each item.\narXiv:2108.08224v1  [cs.LG]  18 Aug 2021\nBoth the encoder and the decoder are composed\nof modules that can be stacked on top of each\nother multiple times. The modules consist mainly\nof Multi-Head Attention and Feed Forward lay-\ners.\nIn this context, the attention-mechanism can be\ndescribed as mapping a query and a set of key-\nvalue pairs to an output.\nAttention(Q, K, V) =softmax(QKT\n√dk\n)V\n(1)\nQ is a matrix containing the query (vector repre-\nsentation of one item in the sequence), K are all\nthe keys (vector representations of all the items in\nthe sequence) of dimension dk and V are the val-\nues of dimension dv. This means that the weights\nare deﬁned by how each item of the sequence\n(Q) is inﬂuenced by all the other items in the se-\nquence (K). Additionally, the SoftMax1 function\nis applied produce a distribution between 0 and 1.\nThose weights are then applied to all the words\nin the sequence that are introduced in V . They\nare the same vectors as Q for the ﬁrst attention\nblocks in the encoder and decoder (self attention)\nbut different for the module that has both encoder\nand decoder inputs (cross attention).\nInstead of performing a single attention func-\ntion the multi-head attention linearly projects the\nqueries, keys and values h times in parallel.\nAfter the multi-attention heads in both the en-\ncoder and decoder, there are pointwise feed-\nforward layers having identical parameters for\neach position, which can be described as a sepa-\nrate, identical linear transformation of each ele-\nment from the given sequence.\nIn recent studies it has been shown that similar\napproaches could lead to signiﬁcant performance\nboosts in tasks other than NLP. The Vision Trans-\nformer (ViT), [3], attains excellent results in com-\nputer vision compared to state-of-the-art convo-\nlutional networks while requiring substantially\nfewer computational resources to train. Solu-\ntions such as the VideoGPT, [4], showcase how\nto efﬁciently apply Transformers for video gener-\nation tasks. In the ﬁeld of time series forecasting\nthere are multiple proposals [ 5] on how Trans-\nformers can be modiﬁed to compensate for their\nsusceptibility to anomalies while simultaneously\nleveraging the performance advantages.\nWith all this as a context we will examine if and\nhow Transformers can be used for predicting fu-\nture events, going from traditional approaches\nwith time series data (e.g. weather or stock price\nforecasting) to more abstract tasks such as next-\nframe prediction in a video where the model\nshould learn different movement patterns and\nadditional dependencies.\n2 Transformers for time series forecasting\nTime series forecasting plays an important role\nin daily life to help people manage resources\nand make decisions. Although still widely used,\ntaditional models, such as State Space Models\n[6] and Autoregressive 2 (AR) models, are de-\nsigned to ﬁt each time series independently and\nrequire practitioners’ expertise in manually se-\nlecting trend, seasonality, etc. To tackle those\nchallenges, recurrent neural networks [ 7] have\nbeen proposed as an alternative solution. De-\nspite the emergence of various variants, includ-\ning LSTM [ 8] and GRU [ 9], it is still hard to\ncapture long-term dependencies in TS data. Un-\nlike the RNN-based methods, Transformers al-\nlow the model to access any part of the history\nregardless of distance, making it potentially more\nsuitable for grasping the recurring patterns with\nlong-term dependencies.\n2.1 Challanges and Solutions\nAs described in [10], Transformers give impres-\nsive results for their performance advantages in\nforecasting tasks. However, their self-attention\nmatches queries against keys insensitive to lo-\n1https://en.wikipedia.org/wiki/Softmax_function\n2https://en.wikipedia.org/wiki/Autoregressive_model\n2\ncal context, which may make the model prone\nto anomalies and bring underlying optimization\nissues. Whether an observed point is an anomaly,\nchange point or part of the patterns is dependent\non its surrounding context. The similarities be-\ntween queries and keys are computed based on\ntheir point-wise values without fully taking into\naccount local context. In previous studies convo-\nlutional self-attention3 has been proposed to ease\nthe issue.\nAnother issue which may emerge is related to\nthe space complexity of canonical Transformer\nwhich grows quadratically with the input length\nL, causing memory bottleneck.\nSolutions such as the Sparse Transformer[11],\nwith complexity of O(n√n) and the LogSparse\nTransformer [10], with complexity reduced to\nO(n(log n)2) have been introduced. These ap-\nproaches make long time series modeling fea-\nsible while retaining comparable to canonical\nTransformer results with much less memory us-\nage.\n2.2 Experiments and Results\nDuring our research we compared two architec-\ntures - a standard RNN utilizing LSTM cells\nand a simple implementation of a Transformer\n(See Fig. 1) for forecasting how the price of the\nS&P500 index 4 will change. They were trained\non the same amount of data5: the daily closing\nvalue of the index from 3rd Jan 2000to 31st Aug\n2018.\nAs shown in Fig. 1 the LSTM recurrent neural\nnetwork barely learns to follow a trend whereas\nthe Transformer architecture is able to capture\nmore detailed dependencies and use them for\nfuture forecasting. For example: In the short-\nterm, the index price usually goes up after the\nquarterly reports of the big companies in a good\nyear.\n3 Transformers for next-frame prediction\nAnother, more abstract way of thinking about\nfuture forecasting is next-frame prediction [12].\nThat is, predicting what happens next in the form\nof newly generated images, after a given amount\nof historical images. It refers to starting from con-\ntinuous, unlabeled video frames and constructing\na network that can accurately generate subse-\nquent frames. Next-frame prediction is not only\nan experimental approach for video processing\nbut a gateway to modelling machine learning ar-\nchitectures that can do more general assumptions\nand abstract reasoning.\n3.1 Methods\nThe introduction of GPT and Image-GPT, [13] -\na class of autoregressive Transformers that have\nshown tremendous success in modelling discrete\ndata, inspired the creation of more and more\nTransformer-like solutions specialized for differ-\nent tasks. As a part of the research we examined\nthe VideoGPT, [4], a conceptually simple archi-\ntecture for scaling likelihood based generative\nmodeling to videos.\nVideoGPT uses Vector Quantized Variational Au-\ntoencoder (VQ-V AE) [14] to learn downsampled\nlatent representations of a given video. It em-\nploys 3D convolutions and axial self-attention\n[15] - generalization of self-attention that natu-\nrally aligns with the multiple dimensions of the\ntensors in both the encoding and the decoding\nsettings. It allows for the vast majority of the con-\ntext to be computed in parallel during decoding\n(Fig. 2).\nA simple GPT-like architecture (Fig. 3) is then\nused to autoregressively model the discrete la-\ntents using position encodings.\n3https://github.com/mlpotter/Transformer_Time_Series\n4https://en.wikipedia.org/wiki/S%26P_500\n5The data, some of the code, models and experiments described in this study are available onhttps://github.com/radi-cho/\nSRS21-public-data\n3\nLSTM\n128\nDropout\n0.2\nLSTM\n256\nLSTM\n512\nDropout\n0.2\nDense\n1\nT1\nT2\nT3\nT4\nTime\nS&P 500 Price\nTime\nS&P 500 Price\nInput; Positional Encoding\nT4\nT5\nT6\nEncoder Layer #2, ...\nAdd & Normalize\nFeed Forward\nAdd & Normalize\nSelf-Attention\nDecoder Input LayerDecoder Layer #1\nDecoder Layer #2, etc.\nAdd & Normalize\nE/D Attention\nAdd & Normalize\nSelf-Attention\nFeed Forward\nAdd & Normalize\nEncoder Output\nLinear Mapping\nEncoder Layer #1\nT5\nT6\nT7\nT7\nT8\nFigure 1: S&P500 price forecasting experiment. The graphics on the left and on the right describe\nthe architectures. The top price chart shows how the RNN LSTM model forecasts pricing. The\nbottom chart shows the Transformer’s predictions. Forecasts are colored in red and the actual prices\n- in blue.\nFigure 2: Axial Attention. The vertical layer provides 1-dimensional self-attention globally,\npropagating information within individual columns while the horizontal 1D layer allows for the\ncapture of column-wise as well as row-wise information. That way the complexity of self-attention\nis reduced from quadratic (2D) to linear (1D).\nDespite the simplicity and ease of training, the\nVideoGPT is able to generate samples competi-\ntive with state-of-the-art GAN [16] models. The\nexperiments in the original paper mainly focus\non creative video generation where the model\nsamples a single frame and then tries to guess\nwhat the video is about. Although the VQ-V AE\nis trained fully unconditionally, conditional sam-\nples are still possible by training a conditional\nprior.\nThe model was modiﬁed and retrained to con-\ndition N frames and produce N + M frames\nwhile decoding with a VQ-V AE trained with se-\nquence length N + M. The conditioned frames\nare ﬁrstly fed into a 3D ResNet6 [17], and then\ncross-attention is performed on the ResNet out-\nput during prior network training.\n6https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035\n4\nDecoder\nEncoder\nInput\nVideo\nFrames\nOutput\nVideo\nFrames\nDiscrete\nLatents\nCodebookVector Quantized\nVariational Autoencoder\n Video frames represented as a flattened sequence \nVideoGPT\nTransformer\nSequence to be reconstructed as video frames\nFigure 3: VideoGPT. The training pipeline is broken into two sequential stages. The ﬁrst stage (Left)\nis similar to the original VQ-V AE training procedure. During the second stage (Right), VQ-V AE\nencodes video data to latent sequences as training data for the prior model.\nFor the experiments we used a composition of\ntwo moving7 MNIST, [19], handwritten digits in\na square box with dimensions of 64x64 pixels.\nThey can bounce off the walls and go over each\nother.\nOur main objective is to run the conditioned\nmodel and generate a few seconds (usually 4, 8,\n16 or 32 frames) of video predicting the change\nof the position of the handwritten digits. Further-\nmore, if successful, the architecture can be repur-\nposed and instead of feeding the result encodings\nto the VQ-V AE decoder, they can be passed to\nan additional classiﬁcation neural network which\nperforms labelling on the predicted future of the\nvideo. For example: to classify how likely it is\nthe digits to collide in the next M frames.\n3.2 Results\nWe have successfully trained a VQ-V AE with\nsequence lengths of 4, 8, 16 and 32 frames on the\ngenerated moving MNIST database. The ﬁnal\ndecoder reconstructions are more accurate than\nthe pretrained models mentioned in the original\npaper (Fig 4).\nMoreover, the modiﬁed VideoGPT instance\nwhich predicts multiple frames in the future by\nconditioning historical data has been successful\nin the task of forecasting moving MNIST videos\n(Fig. 5). It has been tested for sequences of\n4 (condition 2 frames to predict the next 2), 8\n(condition 4 to predict 4) and 16 (condition 8 to\npredict 8).\nBoth the results from the VideoGPT and the time\nseries experiments are a proof that with certain\nmodiﬁcations the Transfromer can lead to high\naccuracy predictions and can replace traditional\nmethods such as RNNs and CNNs in the ﬁeld of\nfuture forecasting.\n4 Future Work\nWe have shown how the generative VideoGPT\n[4] model can be tailored for future frame pre-\ndictions as well as additional classiﬁcation tasks.\nOne of the directions for future development is\nto polish up those proposals and clear out some\nof the assumptions made in this paper to end\n7Pre-generated open-source moving MNIST datasets, such as http://www.cs.toronto.edu/~nitish/unsupervised_\nvideo/ introduced in [18] already exist. For our study moving videos were generated from the static MNIST images with a Python\nscript in order to capture additional labelling information, e.g. which numbers are shown in the video, if they collide with one\nanother, etc. For reference: https://gist.github.com/tencia/afb129122a64bde3bd0c.\n5\n... ... ... ... \nOriginal Video First Training AttemptPretrained 64x64 Final Result\nFigure 4: Representations of the original video from VQ-V AE.\nFigure 5: Next-frame prediction in a Moving MNIST video. Conditioning the ﬁrst N frames and\ngenerating a video of length N + M frames, where the M frames are newly generated.\nup with a more stable and predictable architec-\nture. In various studies (e.g. [ 20, 21, 22]), con-\nvolutions or recurrent structures have been used\nin order to manipulate, preprocess the encod-\nings and enhance the Transformer’s performance.\nWe’re interested in similar approaches in com-\nbination with the encoder/decoder weights of a\nTransformer to be able to process concepts with\nhigher levels of complexity.\nIf the two digits in the VideoGPT experiments\nwill collide depends on wether they’re going to-\nwards one another, but if the model is able to\npredict that they will bounce off a wall and then\nchange directions to eventually collide is another\nlevel of abstraction and reasoning. Additionally,\ninstead of videos the proposed pipeline can be\nused for encoded representations of other data\ntypes - time series, etc.\nAnother fruitful direction would be to explore\nthe effectiveness of the Transformer as a part\nof Deep Reinforcement Learning8 environments.\nWe will be looking at already existing research\n(e.g. [23, 24, 25]) in order to come up with efﬁ-\ncient solutions combining advantages from the\nworlds of supervised and reinforcement learning.\n5 Conclusion\nWe have presented multiple ways of forecast-\ning the future and how Transformer-like archi-\ntectures can be adopted for such an use. We\nhave looked at the possible solutions to problems\n8https://en.wikipedia.org/wiki/Deep_reinforcement_learning\n6\nemerging when Transformers are applied to time\nseries data and the different levels of abstraction\nthey can perform. RNNs and other standard so-\nlutions have been compared to newly introduced\nmodels. We have also modiﬁed the VideoGPT\nmodel to be used conditionally for next-frame\nprediction and proposed ways to upgrade it for\nfuture classiﬁcation tasks and general reasoning.\nIt can even be integrated as a part of Reinforce-\nment Learning environments to enhance the be-\nhaviour of RL agents. We hope that our work\ndone during the Summer Research School 2021\nin Apriltsi, Bulgaria will be useful for future de-\nsign of architectures in time series forecasting,\nvideo generation, decision making, etc.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in\nneural information processing systems, pages 5998–6008,\n2017.\n[2] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew\nZisserman, Oriol Vinyals, and Joao Carreira. Perceiver:\nGeneral perception with iterative attention. arXiv preprint\narXiv:2103.03206, 2021.\n[3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[4] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and\ntransformers. arXiv preprint arXiv:2104.10157, 2021.\n[5] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu\nChen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the\nlocality and breaking the memory bottleneck of transformer\non time series forecasting. Advances in Neural Information\nProcessing Systems, 32:5243–5253, 2019.\n[6] James Durbin and Siem Jan Koopman. Time series analysis\nby state space methods oxford university press, 2012.\n[7] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim\nJanuschowski. Deepar: Probabilistic forecasting with au-\ntoregressive recurrent networks. International Journal of\nForecasting, 36(3):1181–1191, 2020.\n[8] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997.\n[9] Guizhu Shen, Qingping Tan, Haoyu Zhang, Ping Zeng,\nand Jianjun Xu. Deep learning with gated recurrent unit\nnetworks for ﬁnancial sequence predictions. Procedia com-\nputer science, 131:895–903, 2018.\n[10] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu\nChen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the\nlocality and breaking the memory bottleneck of transformer\non time series forecasting. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, ed-\nitors, Advances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc., 2019.\n[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\n[12] Yufan Zhou, Haiwei Dong, and Abdulmotaleb El Saddik.\nDeep learning in next-frame prediction: a benchmark re-\nview. IEEE Access, 8:69273–69283, 2020.\n[13] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. Pre-trained image processing transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12299–12310, 2021.\n[14] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-\nerating diverse high-ﬁdelity images with vq-vae-2. In Ad-\nvances in neural information processing systems, pages\n14866–14876, 2019.\n[15] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and\nTim Salimans. Axial attention in multidimensional trans-\nformers. arXiv preprint arXiv:1912.12180, 2019.\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. Advances\nin neural information processing systems, 27, 2014.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[18] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-\nnov. Unsupervised learning of video representations using\nlstms. In International conference on machine learning,\npages 843–852. PMLR, 2015.\n[19] Yann LeCun and Corinna Cortes. MNIST handwritten digit\ndatabase. 2010.\n[20] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-\ning convolutions to vision transformers. arXiv preprint\narXiv:2103.15808, 2021.\n[21] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob\nUszkoreit, and Łukasz Kaiser. Universal transformers.\narXiv preprint arXiv:1807.03819, 2018.\n[22] Zhouyong Liu, Shun Luo, Wubin Li, Jingben Lu, Yufan\nWu, Chunguo Li, and Luxi Yang. Convtransformer: A con-\nvolutional transformer network for video frame synthesis.\narXiv preprint arXiv:2011.10185, 2020.\n[23] Vinicius Zambaldi, David Raposo, Adam Santoro, Vic-\ntor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David\nReichert, Timothy Lillicrap, Edward Lockhart, Murray\nShanahan, Victoria Langston, Razvan Pascanu, Matthew\nBotvinick, Oriol Vinyals, and Peter Battaglia. Deep rein-\nforcement learning with relational inductive biases. In In-\nternational Conference on Learning Representations, 2019.\n[24] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee,\nAditya Grover, Michael Laskin, Pieter Abbeel, Aravind\nSrinivas, and Igor Mordatch. Decision transformer: Rein-\nforcement learning via sequence modeling. arXiv preprint\narXiv:2106.01345, 2021.\n7\n[25] Ended Learning Team, Adam Stooke, Anuj Mahajan, Cata-\nrina Barros, Charlie Deck, Jakob Bauer, Jakub Sygnowski,\nMaja Trebacz, Max Jaderberg, Michael Mathieu, et al.\nOpen-ended learning leads to generally capable agents.\narXiv preprint arXiv:2107.12808, 2021.\n[26] Jorge Pérez, Pablo Barceló, and Javier Marinkovic. At-\ntention is turing-complete. Journal of Machine Learning\nResearch, 22(75):1–35, 2021.\n[27] Aaron van den Oord, Oriol Vinyals, and Koray\nKavukcuoglu. Neural discrete representation learning.\narXiv preprint arXiv:1711.00937, 2017.\n[28] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang,\nJianxin Li, Hui Xiong, and Wancai Zhang. Informer: Be-\nyond efﬁcient transformer for long sequence time-series\nforecasting. In Proceedings of AAAI, 2021.\n8",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7860634326934814
    },
    {
      "name": "Transformer",
      "score": 0.7186294794082642
    },
    {
      "name": "Machine learning",
      "score": 0.6027315258979797
    },
    {
      "name": "Preprocessor",
      "score": 0.5855736136436462
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5799272060394287
    },
    {
      "name": "Hyperparameter",
      "score": 0.5275437831878662
    },
    {
      "name": "Dimensionality reduction",
      "score": 0.44894054532051086
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4354649484157562
    },
    {
      "name": "Curse of dimensionality",
      "score": 0.415637344121933
    },
    {
      "name": "Data mining",
      "score": 0.3432501554489136
    },
    {
      "name": "Engineering",
      "score": 0.09401845932006836
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}