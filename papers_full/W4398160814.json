{
    "title": "Exploring the Gap: The Challenge of Achieving Human-like Generalization for Concept-based Translation Instruction Using Large Language Models",
    "url": "https://openalex.org/W4398160814",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2099655433",
            "name": "Ming Qian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2187966411",
            "name": "Chuiqing Kong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2099655433",
            "name": "Ming Qian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2187966411",
            "name": "Chuiqing Kong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4320167623"
    ],
    "abstract": "Our study utilizes concept description instructions and few-shot learning examples to examine the effectiveness of a large language model (GPT-4) in generating Chinese-to-English translations that embody related translation concepts. We discovered that human language experts possess superior abductive reasoning skills compared to GPT-4. Therefore, it is crucial for humans to employ abductive reasoning to craft more detailed instructions and infuse additional logic into exemplary prompts, a step essential for guiding a large language model effectively, in contrast to the more intuitive understanding a human expert might have. This approach would make the prompt engineering process more complicated and less human-like. Emphasizing domain-specific abductive reasoning stands out as a crucial aspect of human-like learning that AI/ML systems based on large language models should aim to replicate.",
    "full_text": "Exploring the Gap: The Challenge of Achieving Human-like Generalization \nfor Concept-based Translation Instruction Using Large Language Models \nMing Qian1, Chuiqing Kong2 \n1Charles River Analytics \n2Independent Researcher  \nmqian@cra.com, chuiqingkong17@gmail.com  \n \nAbstract \nOur study utilizes concept description instructions and few-\nshot learning examples to examine the effectiveness of a large \nlanguage model (GPT-4) in generating Chinese-to-English \ntranslations that embody related translation concepts. We dis-\ncovered that human language experts possess superior abduc-\ntive reasoning skills compared to GPT-4. Therefore, it is cru-\ncial for humans to employ abductive reasoning to craft more \ndetailed instructions and infuse additional logic into exem-\nplary prompts, a step essential for guiding a large language \nmodel effectively, in contrast to the more intuitive under-\nstanding a human expert might have. This approach would \nmake the prompt engineering process more complicated and \nless human-like. Emphasizing domain-specific abductive \nreasoning stands out as a crucial aspect of human-like learn-\ning that AI/ML systems based on large language models \nshould aim to replicate. \n Concept-based Machine Translation Work-\nflow Enhanced by Large Language Models     \nJohn McCarthy's insight, \"To understand natural language \nis to understand the concepts in the language, not just the \nwords,\" underscores the essential role of grasping the con-\ncepts conveyed in texts. Recent advancements in generative \nArtificial Intelligence (GenAI), particularly through large \nlanguage models (LLMs) like GPT-4, have demonstrated \ntheir effectiveness as machine translation tools in various \nstudies and evaluations (Jiao, Wang, and Huang 2023; \nLinkedin Pulse 2023). Unlike traditional machine transla-\ntion (MT) tools such as Google Translate and DeepL, which \nrely solely on the source language text, GPT-4 possesses the \nability to follow instructions and can also learn from bilin-\ngual examples provided within the prompt. This capability \nenables GPT-4 to produce translations that are more contex-\ntually aware and accurate, positioning it as a more flexible \nand user-friendly alternative to conventional methods. \n   \n \nCopyright © 2024, Association for the Advancement of Artificial \nIntelligence (www.aaai.org). All rights reserved. \nChallenge of Achieving Human-like Generali-\nzation for Concept-based Translation Instruc-\ntion Using Large Language Models \nThe concept of generalization in AI involves the ability of \na model to perform well on new, unseen instances. Hu-\nman-like generalization in natural language processing \n(NLP) refers to the ability of NLP models to generalize in \na way that is similar to how humans do. \n In our research (Qian, et al., 2023; Qian and Kong, \n2024), we explored how effectively GPT-4 can clarify \ntranslation concepts, evaluate their significance in source \ntexts, and accurately convey them in the target language, \nspecifically through examples from Chinese to English. \nOur results show that by employing instructions for con-\ncept description along with few-shot learning examples, \nGPT-4 successfully elucidates most concepts, precisely \nassesses their relevance, and translates them into the tar-\nget texts, embodying the related concepts. Nonetheless, \nachieving satisfactory performance for a considerable ar-\nray of concepts necessitates the use of advanced prompt-\ning techniques, such as the Chain-of-Thought (CoT), or \npre-editing approaches, such as explicating linguistic pat-\nterns. Therefore, despite their advancements, LLMs like \nGPT-4 still exhibit limitations in terms of their human-\nlike generalization capabilities for concept-based transla-\ntion tasks. \n To illustrate, we utilized a straightforward concept defi-\nnition as a part of the prompt for GPT-4: “Changing subject \nselection involves selecting a different subject than the one \nin the source text to enhance the readability and fluency of \nthe English translation.” Additionally, four instances related \nto the subject changing selection concept were provided as \npart of the prompt. Every sentence has a noun phrase com-\nposed of two nouns, where one noun modifies the other .  \n1. 资源环境约束边界临近，最典型的例子就是雾\n霾。(direct translation: The boundaries of resource \nand environmental constraints  are approaching. \nThe most typical example is smog.) \n \nAAAI Spring Symposium Series (SSS-24)\n579\n2. 老年人本身的免疫功能就相对于年轻人要弱一\n些。(direction translation: The immune function \n       of the elderly is weaker than that of the young.) \n3. 未来20年气候变化的威胁将超过恐怖主义。\n(direct translation: The threat from Climate change \nwill surpass terrorism in the next 20 years.) \n4. 迁移的流向和形式也都发生了很大的变化。\n(direct translation: The flow and form of migration \nhave also undergone great changes.) \nWe noted that examples #2, #3, and #4 utilized the \npossessive particle \" 的\" to connect two nouns, signifying \npossession or a relationship and necessitating a shift in the \nsubject from one noun to another for the translation into the \ntarget language (English). In contrast, example #1 did not \nuse the possessive particle \" 的\" to link two nouns, but its \nexistence was implied. This variation could account for the \ninadequate 1-in-4 leave-one-out cross-validation (LOOCV) \nperformance. To make the task more straightforward for \nGPT-4, we replaced example #1 with another example in \nwhich the possessive particle was explicitly used. \n1. \n技术的进步极大地改变了我们的生活方式。\n(direct translation: The advancement of technology \nhas greatly changed our way of life.) \nSubsequently, we checked the 1-in-4 leave-one-out cross-\nvalidation (LOOCV) performance again. However, the \noutcomes were not satisfactory either. On several occasions, \nGPT4 did not utilize the two nouns linked by “ 的” but \ninstead selected other nouns within the sentence. This \nnecessitated our provision of comprehensive, step-by-step \nanalyses for the few-shot examples included in the prompt, \nas illustrated in Table 1. \n The performance of the 1-in-4 leave-one-out cross-\nvalidation (LOOCV) was significantly enhanced by the \ndetailed reasoning provided through the Chain-of-Thought \n(CoT) method. Nonetheless, its practical application is \nconstrained by the necessity to deconstruct the original \nconcept into a detailed, step-by-step reasoning process. \nUnlike human translators, who can generalize a concept \nacross varied contexts by recognizing both explicit and \nimplicit patterns and focusing on the core intent, the GPT-\n4-based instruction plus few-shot approach relies on \nidentifying explicit patterns through detailed descriptions. \nFor reasons of space, the CoT strategies for the other \ntranslation concepts are not detailed here.  \nDiscussion \nAbduction is an inference process that identifies the most \nplausible explanations for observed phenomena. Human \nlanguage experts exhibit superior abduction skills compared \nto GPT-4. For instance: \n(1) They understand that the application of the \ntranslation concept is not affected by whether explicit \nor implicit particles are used to link two nouns. \n(2) They can recognize, as shown in the few-shot \nexamples, that when changing the subject in a \nsentence that starts with 'A’s B,' they need to select \neither A or B without choosing other nouns in the \nsentence.  \nIn comparison, GPT-4 struggles with such abductive \ninference reasoning.  \n As a result, human experts must engage in abduction to \nderive specific instructions (deductive rules) and develop \nstep-by-step logic in example prompts (inference \ndemonstration). This necessity complicates and extends the \nprompt engineering process. Emphasizing domain-specific \nabductive reasoning stands out as a crucial aspect of human-\nlike learning that AI/ML systems based on large language \nmodels should aim to replicate. \n \n迁移的流向和形式也都发生了很大的变化。 \nDirect translation: The direction and form of migration \nhave also undergone significant changes. \nThe subject phrase: The direction and form of migration \nTranslation with alternative subject selection using the \nother noun in the subject phrase : Migration was \nhappening in different directions and ways. \n技术的进步极大地改变了我们的生活方式。  \nDirect translation : The progress of technology has \ngreatly changed our way of life.  \nThe subject phrase: The progress of technology \nTranslation with alternative subject selection using the \nother noun in the subject phrase :  Technology have \ngreatly advanced to change our way of life.   \n老年人本身的免疫功能就相对于年轻人要弱一些。 \nDirect translation : The immune function of elderly \npeople is relatively weaker than that of younger people. \nThe subject phrase : The immune function of elderly \npeople \nTranslation with alternative subject selection using the \nother noun in the subject phrase : Unlike younger \npeople, the elderly have weaker immune systems. \n未来20年气候变化的威胁将超过恐怖主义。 \nDirect translation : The threat of climate change will \nexceed that of terrorism in the next 20 years. \nThe subject phrase: The threat of climate change  \nTranslation with alternative subject selection using the \nother noun in the subject phrase :  In the next two \ndecades, climate change will pose a greater threat than \nterrorism. \nTable 1: Four instances related to the concept of changing \nsubject selection, demonstrating how Chain-of-Thought \n(CoT) prompting facilitates reasoning through smaller, \nmore manageable steps.  \n580\nReferences \nJiao, W., Wang, W., Huang, J.T., Wang, X. and Tu, Z.P., \n2023. Is ChatGPT a good translator? Yes with GPT-4 as \nthe engine. arXiv preprint arXiv:2301.08745. \nLinkedin Pulse. 2023. How GPT-4 Is Transforming the Lan-\nguage Service Industry: Benefits and Challenges. \nhttps://www.linkedin.com/pulse/how-gpt-4-transforming-\nlanguage-service-industry-benefits-kotzsch/. Accessed: \n2024-02-07.  \nQian, M., Wu HQ., Yang, L., Wan A., 2023. Performance \nEvaluation on Human-Machine Teaming Augmented Ma-\nchine Translation Enabled by GPT-4. In Proceedings of the \nFirst Workshop on NLP Tools and Resources for Transla-\ntion and Interpreting Applications, pages 20–31, Varna, \nBulgaria.  \nQian, M.; and Kong, CQ. 2024. Enabling Human-centered \nMachine Translation Using Concept-based Large Language \nModel Prompting and Machine Translation Memory. Paper \nhas been accepted and will appear In International Confer-\nence on Human-Computer Interaction. Washington DC, \nUSA, June 6th till July 4th.  \n \n \n581"
}