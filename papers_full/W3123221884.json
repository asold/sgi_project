{
  "title": "An Attention Free Transformer",
  "url": "https://openalex.org/W3123221884",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222811290",
      "name": "Zhai, Shuangfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221858989",
      "name": "Talbott, Walter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3041792169",
      "name": "Srivastava, Nitish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101649933",
      "name": "Huang, Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287338457",
      "name": "Goh, Hanlin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982935238",
      "name": "Zhang Rui-xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222811292",
      "name": "Susskind, Josh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2965853874",
    "https://openalex.org/W3011199263",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2981899103",
    "https://openalex.org/W3091156754"
  ],
  "abstract": "We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.",
  "full_text": "An Attention Free Transformer\nShuangfei Zhai\nApple Inc.\nszhai@apple.com\nWalter Talbott\nApple Inc.\nwtalbott@apple.com\nNitish Srivastava\nApple Inc.\nnitish_srivastava@apple.com\nChen Huang\nApple Inc.\nchen-huang@apple.com\nHanlin Goh\nApple Inc.\nhanlin@apple.com\nRuixiang Zhang ∗\nApple Inc., MILA\nruixiang_zhang2@apple.com\nJosh Susskind\nApple Inc.\njsusskind@apple.com\nAbstract\nWe introduce Attention Free Transformer (AFT), an efﬁcient variant of Transform-\ners [1] that eliminates the need for dot product self attention. In an AFT layer, the\nkey and value are ﬁrst combined with a set of learned position biases, the result of\nwhich is multiplied with the query in an element-wise fashion. This new operation\nhas a memory complexity linear w.r.t. both the context size and the dimension\nof features, making it compatible to both large input and model sizes. We also\nintroduce AFT-local and AFT-conv, two model variants that take advantage of the\nidea of locality and spatial weight sharing while maintaining global connectivity.\nWe conduct extensive experiments on two autoregressive modeling tasks (CIFAR10\nand Enwik8) as well as an image recognition task (ImageNet-1K classiﬁcation).\nWe show that AFT demonstrates competitive performance on all the benchmarks,\nwhile providing excellent efﬁciency at the same time.\n1 Introduction\nSelf attention mechanisms, represented by Transformers [1], have driven the advancement of various\nmachine learning problems, including language understanding [2, 3] and computer vision applications\n[4–6]. Different from classic model architectures such as Convolutional Neural Nets (CNNs) or\nRecurrent Neural Nets (RNNs), Transformers enable direct interaction between every pair of elements\nwithin a sequence, which makes them especially powerful at capturing long term dependencies.\nHowever, Transformers require high computational costs. The cause of this challenge is the need to\nperform attention operations that have quadratic time and space complexity w.r.t. the context size.\nThis makes it difﬁcult for Transformers to scale to inputs with large context sizes. A number of recent\nworks have been dedicated to addressing the scalability issue of Transformers [7–13]. The common\nidea here is to approximate the full attention operation, with the techniques ranging from sparsity,\nlocality sensitive hashing, low rank decomposition, kernel approximation, etc..\nIn this paper, we propose a computational module that does not use or approximate the standard\ndot product attention. We hence name our model the attention free transformer (AFT). Similar to\ndot product attention, AFT is composed of the interaction of three quantities, namely the query, key\nand value (Q,K,V ). The difference is that, in AFT the key and value (context) are ﬁrst combined\n∗work done while interning at Apple.\nPreprint. Under review.\narXiv:2105.14103v2  [cs.LG]  21 Sep 2021\nFigure 1: Left: average relative 2d attention maps from a pretrained 12 layer 6 head ViT [5]. Right:\nrelative position biases learned by a AFT-conv with comparable size. Each row represents a layer\n(with layer index ranging from {0, 2, 4, 6, 8, 10}); Each column represents a head. See the Appendix\nfor a more complete version.\nTable 1: Complexity comparison with different Transformers: Reformer [ 8], Linear Transformer\n[11], Performer [13] (only variants that support the causal mode are shown). Here T,d denote the\nsequence length and feature dimension, respectively.\nModel Time Space\nTransformer O(T2d) O(T2 + Td)\nReformer O(T log Td) O(T log T + Td)\nLinear Transformer O(Td2) O(Td + d2)\nPerformer O(Td2 log d) O(Td log d + d2 log d)\nAFT-simple O(Td) O(Td)\nAFT-full O(T2d) O(Td)\nAFT-local (AFT-conv) O(Tsd), s < T O(Td)\ntogether with a set of learned position biases. The query is then combined with the reduced context\nwith element-wise multiplication. See Figure 2 for an illustration.\nAFT maintains direct interaction between any two points in the context, which is a major advantage\nof dot product attention. In fact, AFT can be interpreted as performing attention where the number of\nattention heads is the same as the model’s feature dimension, whereas the attention maps do not need\nto be explicitly computed (see Sec. 3.1 for details). This results in a memory complexity linear w.r.t.\nboth the input and model sizes.\nThe rearranged computational ordering of Q,K,V is also found in recent “linearized attention\"\nworks [11, 13–15]. The difference is that AFT combines kand vin an element-wise fashion, while\nall the linear attention papers rely on matrix dot products. The latter approach results in an complexity\nquadratic to the model’s feature dimension, which is unfriendly to large model sizes. See Table 1 for\nthe complexity analysis of AFT in comparison to other variants.\nEmpirically, we observed that trained Transformers tend to demonstrate extensive local patterns (see\nFig. 1). This motivates us to propose two variants of AFT: AFT-local and AFT-conv. In AFT-local,\nthe learned position biases are constrained to a local region, while global connectivity is maintained.\nAFT-conv further extends this design by imposing spatial weight sharing, effectively making it a\nvariant of CNN with global receptive ﬁeld. We show that the locality constraint not only provides\nbetter parameter and computational efﬁciency, but also greatly improves model’s performance in all\ntasks.\nWe perform experiments with AFT on image auto-regressive modeling, character level language\nmodeling, and image classiﬁcation tasks. We show that AFT provides competitive performance, often\nmatching or beating standard Transformers and other variants, while providing excellent efﬁciency.\nWe also provide extensive ablation studies to several design choices of AFT, and discuss its unique\nproperties such as compatibility with Transformers, sparsity and variable sized inputs.\n2\n2 Multi-Head Attention\nAt the core of Transformers is the Multi-Head Attention (MHA) operation. In the mode of self\nattention, given an input sequence X ∈RT×d, and the number of heads h, MHA performs a scaled\ndot product attention for each head i, deﬁned as:\nfi(X) =σ(Qi(Ki)T\n√dk\n)Vi, s.t. Qi = XWQ\ni ,Ki = XWK\ni ,Vi = XWV\ni , (1)\nwhere WQ\ni ∈Rd×dk , WK\ni ∈Rd×dk , WV\ni ∈Rd×dv are linear transformations for head i, and σis\nthe non-linearity by default set as the softmax function (applied to each row of a matrix). dk,dv are\ndimensions for key and value, respectively. MHA concatenates the output of hattention heads along\nthe channel dimension, resulting in feature dimension hdv. Unless otherwise mentioned, we assume\ndk = dv and h= d\ndk\n. This means the query, key and value are the same dimension within each head,\nand the output dimension matches that of the input.\n3 Methodology\n3.1 Attention Free Transformer\nWe now deﬁne Attention free transformer (AFT), which is a plugin replacement of MHA without the\nneed of changing other architectural aspects of Transformers. Given the input X, AFT ﬁrst linearly\ntransforms them into Q= XWQ, K = XWK, V = XWV, then performs following operation 2:\nY = f(X); Yt = σq(Qt) ⊙\n∑T\nt′=1 exp(Kt′ + wt,t′ ) ⊙Vt′\n∑T\nt′=1 exp(Kt′ + wt,t′ )\n(2)\nwhere ⊙is the element-wise product; σq is the nonlinearity applied to the query with default being\nsigmoid; w∈RT×T is the learned pair-wise position biases (see Figure 2 for an illustration).\nExplained in words, for each target position t, AFT performs a weighted average of values, the result\nof which is combined with the query with element-wise multiplication. In particular, the weighting\nis simply composed of the keys and a set of learned pair-wise position biases. This provides the\nimmediate advantage of not needing to compute and store the expensive attention matrix, while\nmaintaining the global interactions between query and values as MHA does.\nIn order to further see AFT’s relationship to MHA, we can rewrite Equation 2 as:\nYi\nt =<ai\nt,V i >, s.t.ai\nt = σq(Qi\nt) exp(Ki + wt)∑T\nt′=1 exp(Ki\nt′ + wt,t′ )\n, i= 1,2,...,d, t = 1,2,...,T. (3)\nHere we use the superscript ito index the feature dimension of a matrix; <·,·>denotes the dot\nproduct of vectors. In this rearranged form, we are able to express AFT in terms of attention again.\nSpeciﬁcally, for each position, we have an attention vector ai\nt ∈RT for each dimension, composed\nof Q,K,w . In other words, AFT can be interpreted as performing implicit attention with as many\nheads as feature dimensions, where the attention matrices take a factorized form.\n3.2 AFT variants: locality, weight sharing and parameterization\nAFT-full. We denote the basic version of AFT deﬁned in Equation 2 as AFT-full.\nAFT-local. In many applications, locality is an important inductive bias, which has been exploited\nby CNNs and recent works in Transformers [ 4, 7]. In addition, we found that trained standard\nTransformers tend to demonstrate extensive local attention patterns. To be concrete, we visualized an\nImagenetNet pretrained Vision Transformer (ViT) [5], which consists of 12 layers each with 6 heads.\nFor the sake of visualization, we ignore the classiﬁcation tokens, and reshape each layer’s attention\ntensors to shape 6 ×196 ×196 (the spatial size of the ViT’s feature maps is 14 ×14). We then\nsampled 256 images from the ImageNet validation set. For each layer and each head, we compute the\naverage relative 2d attentions, averaged across query positions and images. This results in a set of\nattention maps of size 12 ×6 ×27 ×27 3. The result is shown in Figure 1 (left), where we show the\n2we use the non-masked mode for illustration, and the masked/causal mode can be constructed by limiting\nthe range of the summation.\n312 is #layers, 6 is #heads, 27 × 27 is relative 2d attention size from feature map 14 × 14\n3\n∑T\nt′ =1\n∑T\nt′ =1\n⊙\nQt exp\nexp\nK V\nK\n=\n][ ⊙\n)(\n()(σq\nwt\n+\n)\nwt\n+ Yt\nFigure 2: An illustration of AFT deﬁned in Equation 2, with T = 3,d = 2.\nattentions for every 2 layers (see the Appendix for the full visualization). We see that the relative\nattention maps demonstrate strong local patterns (as indicated by the sharpness), especially in the\nlower layers. This motivates a variant of AFT, dubbed AFT-local, where we only apply a learned set\nof relative position biases locally:\nwt,t′ =\n{wt,t′ , if |t−t′|<s\n0, otherwise. (4)\nHere s≤T is a local window size. AFT-local provides further computational savings, both wrt the\nnumber of parameters and time/space complexity. Note that different from local Transformers (e.g.,\n[7]), AFT-local maintains global connectivity regardless of the window size s. In the experiments\nwe verify the effectiveness of this design choice.\nAFT-simple. An extreme form of AFT-local is when s= 0, i.e., no position bias is learned. This\ngives rise to an extremely simple version of AFT, where we have:\nYt = σq(Qt) ⊙\n∑T\nt′=1 exp(Kt′ ) ⊙Vt′\n∑T\nt′=1 exp(Kt′ )\n= σq(Qt) ⊙\nT∑\nt′=1\n(softmax(K) ⊙V)t′ . (5)\nIn this version, the context reduction is further simpliﬁed to element-wise operations and global\npooling. AFT-simple is similar to linearized attention [ 11, 13, 14], which is formulated as Yt =\nφ(Qt) ∑T\nt′=1\n(\nφ(Kt′ )T Vt′\n)\nφ(Qt) ∑T\nt′=1 φ(Kt)T . However, it is easy to see that AFT-simple completely gets rid of the need\nfor dot products operations, which results in a complexity of O(Td) rather than O(Td2).\nAFT-conv.We can also further extend the idea of locality to incorporate spatial weight sharing, i.e.,\nconvolution. This variant is especially relevant to vision tasks, as it is often desirable to extend a\npretrained model to variable sized inputs. Speciﬁcally, we let the value of wt,t′ to be dependent only\non the relative positions of tand t′, w.r.t. to a given spatial grid (1d or 2d). Similar to CNNs, we can\nalso learn multiple sets of position biases (we reuse the notion of heads for reference). To account for\nthe growth of #parameters as #heads increases, we adopt a design choice to tie the dimensionality\nof K with #heads. This makes AFT-conv amendable to an implementation relying on depth-wise\nseparable convolutions, global pooling and element-wise operations.\nWe now show an example of AFT-conv with 1d inputs, 2d and 3d inputs can be derived similarly. We\ndenote a model conﬁguration as AFT-conv-h-s, where h is the number of heads and sis the 1d local\nwindow size. We now have w∈Rh×s, Q,V ∈RT×h×d\nh , K ∈RT×h. For each head i= 1,2,...,h ,\nwe have:\nYi\nt = σq(Qi\nt) ⊙conv1d(exp(Ki) ⊙Vi, exp(wi) −1) +∑T\nt′=1 exp(Ki\nt′ ) ⊙Vi\nt′\nconv1d(exp(Ki), exp(wi) −1) +∑T\nt′=1 exp(Ki\nt′ )\n. (6)\nHere Yi\nt ∈R\nd\nh , Qi,V i ∈RT×d\nh , Ki ∈RT, wi ∈Rs; conv1d(x,w) is depth-wise separable\n1d convolution operation where the convolutional ﬁlter w is shared across channel dimension 4.\n4Equation 6 can also be implemented with fully connected operations, e.g., einsum, which might yield better\nefﬁciency in practice.\n4\nNote that Equation 6 can be readily interpreted as a specialized convolutional layer with 1) global\nconnectivity, 2) non-negative convolutional weights and 3) sophisticated divisive/multiplicative\ngating mechanism. We show experimentally that all of the three aspects contribute signiﬁcantly to\nAFT-conv’s performance.\nParameterization. Empirically, we ﬁnd that it is important to parameterize the position biases w\nproperly. For AFT-full and AFT-local, we adopt a factorized form ofwas:\nwt,t′ = uT\nt v′\nt, u∈RT×d′\n,v ∈RT×d′\n, (7)\nwhere d′is a small embedding dimension (e.g., 128). This simple factorization not only greatly\nreduces the parameter counts (2Td′vs T2), but also empirically improves model’s performance in\nboth training and testing.\nFor AFT-conv, the factorization trick is non-applicable. We instead adopt a simple re-parameterization,\nwhere for each head i, we let\nwi = γiwi −mean(wi)\nstd(wi) + βi, (8)\nwhere γ ∈Rh,β ∈Rh are learnable gain and bias parameters, both initialized as 0.\n4 Related Work\nSince the Transformer was introduced, there have been numerous attempts to address the major\nsource of inefﬁciency in the architecture, the quadratic cost of the attention operation. Improving this\noperation can enable larger context sizes and more efﬁcient implementations. For a comprehensive,\nrecent survey of efﬁcient transformers, see [16].\nApproximating the dot product. [11, 13, 14] propose to approximate the exponential kernel with\ninner product of projections, which leads to a linearized attention operation of complexity O(Td2).\nThe d2 term of these models however makes it difﬁcult to scale with model size, which is not a\nproblem for AFT. Reformers [8] apply LSH as an approximation to the dot product, where AFT\ncompletely gets rid of it.\nSparse, local attention. Sparse Transformers [7] and Image Transformer [17] proposes to use ﬁxed\nsparse or local context patterns. Attention models in vision tasks (often combined with convolutions)\nuse image structure to help handcraft relevant spatial patterns to attend [ 18–22]. AFT-local also\nborrows the locality idea, but we put it as a bias rather than hard constraint. This allows AFT-\nlocal/AFT-conv to take advantage of the full context, rather than relying only on a subset.\nContext compression. Other approaches try to learn context patterns. Adaptive-Span Transformers\n[23] learn a range for each attention head within which to attend. Routing transformers [ 24] use\nclustering to compute dot-product attention only over a subset of elements within the same cluster.\nThe Linformer [10] reduces the length of the context by compressing the keys and values with a\nlinear layer. Compressive Transformers [9] compute and update reduced representations of the input\nthat are far enough back in the input sequence, and attend to those compressed representations. AFT\nis largely complementary to these approaches, as our focus is to improve the complexity of any given\nsequence from the operation level.\nEliminating dot product attention. Instead of limiting the number of comparisons, other methods\nchange the operation used to compute attention. The Synthesizer [12] uses attention weights predicted\nfrom inputs, rather than derived from dot-product interactions. The LightConv module introduced\nin [ 25] proposes to replace the dot product self-attention with dynamic lightweight depthwise\nconvolution, where the weights are normalized across temporal dimension. The Sinkhorn Transformer\n[26] uses a differentiable sorting operation to identify relevant comparisons that may not be local in\nthe original sequence order. AFT offers a different approach along this line, while highlighting strong\nempirical performance and efﬁciency.\nMLPs for vision. Concurrent works [ 27, 28] explore the use of MLP inplace of the attention\noperation for vision tasks. While AFT can be viewed in a similar way, it is also equipped with a more\nsophisticated gating mechanism. In particular, the weighting of values are composed of both the key\nand position biases, which are normalized to non-negative values (similar to attention). This allows\n5\nTable 2: NLL results on CIFAR10, evaluated by bits/dim, the lower the better. Speed and memory\nare measured during training time, with a batch size of 32 across 8 V100 GPUs. AFT achieve the\nstate-of-the-art result in this setting, with signiﬁcant improvements wrt speed and memory over\nstandard Transformer, Sparse Transformer [7] and Image Transformer [17].\nMethod L d h Train loss Test loss Iters/Sec GB/GPU\nPixelCNN - - - 3.08 3.14\nPixelCNN++ - - - - 2.92\nPixelSNAIL - - - - 2.85\nSparse Transformer strided 128 256 2 - 2.80\nImage Transformer local2d 12 512 4 - 2.90 1.61 22.3\nTransformer 12 512 4 2.90 2.88 1.35 30.6\nTransformer 24 256 2 2.90 2.86 1.36 30.4\nAFT-local-256 12 512 1 2.78 2.80 1.68 11.4\nAFT-local-256 24 256 1 2.75 2.74 1.67 12.8\nAFT-simple 24 256 1 2.82 2.89 2.15 9.5\nTable 3: The effect of factorized parameterization of the position bias, evaluated by autoregressive\nmodeling on CIFAR10.\n#params/layer Train loss Test loss\nNon Factorized 9.6M 2.82 2.84\nFactorized (default) 0.6M 2.75 2.74\nAFT to be a plugin module to existing Transformers without any architectural changes and extra\ntuning. Besides, AFT-conv inherits the valuable properties of CNNs, allowing it to achieve excellent\nparameter efﬁciency, strong performance as well as ability to handle variable sized inputs.\n5 Experiments\nWe conduct experiments on three tasks: image autoregressive modeling (Sec. 5.1), character level\nlanguage modeling (Sec. 5.2) and image classiﬁcation (Sec. 5.3). The ﬁrst two benchmarks use the\ncausal model (or decoder model) of AFT, while the last one uses the encoding model. All the experi-\nments are designed in the plug and play fashion, where we obtain a baseline Transformer architecture\nfor the speciﬁc task and replace the attention module with an AFT module. Hyperparameters such as\ninitialization, learning rate scheduling are also directly inherited from the Transformer counterparts.\nUnless otherwise mentioned, all experiments are conducted on 8×V100 GPU machines.\n5.1 Image Autoregressive Modeling\nIn our ﬁrst set of experiments, we consider the problem of image autoregressive modeling by\nminimizing the negative log likelihood (NLL). Similar to [ 17], we represent an RGB image as a\nsequence of length H×W ×3, with H,W being the height and width, respectively. Each sub-pixel\nis represented as a 256-way discrete variable. We use CIFAR10 as the benchmarking dataset.\nOur reference Transformer design largely follows that of [4], where a transformer block consists of\nan attention layer (AFT layer in our case) with residual connection and a 2 layer MLP with residual\nconnections (with the feedforward dimension multiplier set to 4). Layer Normalization (LN) [29] is\napplied in a “pre-act\" fashion. We adopt learned position embeddings, and use a set of shared token\nembeddings and prediction heads across RGB. We use AFT-local with the factorized parameterization\nfor this experiment. The hidden dimension for the factorization is 64, with u,v initialized with\nN(0,10−2); the local (1d) window size sis 256.\nWe use AdamW [30], and follow a standard warmup learning rate schedule as in [ 1]. We use an\ninitial learning rate of 3 ×10−3 a weight decay of 0.1 applied to all linear transformations weights,\nand a dropout rate of 0.1. We adopt simple data augmentation. During training, we ﬁrst randomly ﬂip\neach image horizontally, then add or subtract a value in the range [−10,10] from all its subpixels,\nand clip resulting pixel values to [0,255]. We use cross entropy loss, and a default batch size of 128\nfor 200 training epochs.\n6\nTable 4: Enwik8 results, measured in bits per character (bpc), the lower the better. Baselines compared\nare Reformer [ 8], Synthesizer [ 12] (its best performing dense version), Linear Transformer [ 11]\nand Performer [13]. L, d, h, T denote number of blocks (depth), dimension of features, number of\nheads, and sequence length, respectively. Speed and memory are measured during training time,\nwith a batch size of 128 on a 8 V100 GPU node. Both Linear Transformer and Performer are\nimplemented with customized CUDA kernels (github.com/idiap/fast-transformers), and all other\nmodels are implemented in native Pytorch.\nMethod L d h T Train bpc Test bpc Iters/Sec GB/GPU\nTransformer 12 512 8 1024 0.977 1.137 1.42 29.4\nTransformer 24 256 4 1024 1.039 1.130 1.57 28.3\nReformer 12 512 8 1024 1.04 1.195 1.05 20.9\nSynthesizer 12 512 8 1024 0.994 1.298 1.49 29.9\nLinear Transformer 12 512 8 1024 0.981 1.207 1.46 10.6\nPerformer 12 512 8 1024 1.002 1.199 1.44 10.1\nAFT-local-32 12 512 1 1024 0.854 1.180 1.85 11.3\nAFT-local-32 24 256 1 1024 0.972 1.154 2.04 11.2\nAFT-simple 24 256 1 1024 1.046 1.209 2.61 9.6\nComparing with the state of the art. CIFAR10 is a crowded benchmark for image autoregressive\nmodeling, and we compare with a few competitive baselines, as shown in Table 2. Note that CIFAR10\nhas an unrolled sequence length of 3072, which is already prohibitive to train a full Transformer with\nreasonable size. For the standard Transformer model, we adopt two conﬁgurations (L=12, d=512,\nh=4 and L=24, d=256, h=2), with batch size 32 which is the largest one we can ﬁt on a 8xV100 GPU\nnode. Another baseline is Image Transformer [17], which restricts attention to local2d windows of\nsize of 256. We also compare to Sparse Transformers [7], which restrains attention to pre-speciﬁed\nsparse subsets of context elements.\nFrom Table2, we see that AFT-local outperforms all the Transformer baselines. We also observe that\nthe deeper but narrower architecture is more effective than the shallow but wide baseline. Our best\nmodel also achieves the state-of-the-art result on CIFAR10 in this setting, outperforming a much\nlarger Sparse Transformer model. Efﬁciency wise, we benchmarked the Transformer variants against\nAFT on a 8 V100 GPU node 5. All our variants are faster than standard Transformer and Image\nTransformer, while consuming only half of the memory 6. Perhaps surprisingly, AFT-simple also\nachieves very competitive performance, even outperforming the Image Transformer, while offering\nexcellent speed and memory efﬁciency.\nThe effect of factorization. We also provide ablations on the role of the factorized parameterization\nof AFT. To do this, we retrained the best performing model from Table 2 ( i.e., AFT-local-256, L=24,\nd=256) with a naively parameterized w, initialized with N(0,10−2). From Table 3, we see that the\nfactorized version not only provides signiﬁcant parameter savings, but also improves the model’s\nperformance both on training and testing.\nTable 5: Training and testing bpc w.r.t. the local window size for AFT-local.\nWin size 0 1 2 4 8 32 64 128 256 512 1024\nTrain bpc 1.046 1.043 1.009 0.990 0.983 0.972 0.981 0.985 0.986 0.988 0.991\nTest bpc 1.209 1.205 1.176 1.165 1.162 1.154 1.160 1.165 1.164 1.171 1.173\n5We use a batch size of 32 which is the largest batch size Image Transformer can ﬁt\n6Fair speed/memory comparison against Sparse Transformer is infeasible, as it relies on a set of advanced\nimplementation tricks such as mixed precision and gradient checkpointing, whereas AFT is implemented with\nstandard Pytorch utilities ran in full precision.\nTable 6: Increasing T on Enwik8. Both training and testing loss are improved as T increases.\nT 1024 2048 4096\nTrain bpc 0.972 0.951 0.945\nTest bpc 1.154 1.135 1.134\n7\n5.2 Language Modeling\nWe apply AFT to character level language modeling on Enwik8 [ 31], which is another popular\nbenchmark for auto-regressive modeling. We follow the standard preprocessing procedures and\ntraining/validation/test splits as in [32]. Our base Transformer reference is a 12 layer 512 dimensional\n8 head architecture with 2048 feed forward dimensions. For the ﬁrst set of experiments, we use\nsequence length of 1024. Our training protocol is largely the same as the previous experiment, except\nthat we increase the weight decay to 0.5 and train for 100 epochs with batch size 128. We evaluate the\nAFT-local with a window size of 32 and d′= 256. We also compare to several efﬁcient Transformer\nbaselines, namely Reformer [ 8], Synthesizer [ 12] , Linear Transformer [ 11] and Performer [ 13].\nFrom Table 4, we see that with the base L = 12,d = 512architecture, AFT achieves the lowest\ntraining bits per character (bpc), which is an indicator for high model capacity. Its test performance is\nslightly worse than that of the basic Transformer, but outperforms all other Transformer variants. The\ndeeper and narrower architecture of AFT strikes the best balance across parameter, speed, memory\nand performance. Its test bpc is only 0.024 away from the full Transformer’s, while only consuming\na third of the memory and provides a 44% speedup. AFT-simple again demonstrates competitive\nperformance and excellent efﬁciency.\nOn the local window size. In order to validate the effect of local window size, we performed\nadditional experiments with the L = 24,d = 256architecture, ﬁxing everything but varying the\nlocal window size s. We show the results in Table 5, where we see that both the training and testing\nbpc forms a U-shape w.r.t. the window size, with 32 achieving the best performance. This further\nconﬁrms that locality is indeed an effective inductive bias across tasks.\nLonger sequence size. We are also interested in AFT’s ability to adapt to longer sequence sizes. Due\nto its simplicity, one might even expect a degradation of performance as T increases. To this end,\nwe trained the AFT-local-32, L=24, d=256 model with T increased to 2048 and 4096. The results\nare shown in Table 6. We see that AFT is able to take advantage of larger sequence sizes and yield\nconsistently lower training and testing loss as T increases.\n5.3 Image Classiﬁcation\nWe then test the non-causal version of AFT, focusing on an image classiﬁcation task. We adopt\nthe Vision Transformer architecture [5], and perform experiments on the Imagent 1K classiﬁcation\ndataset. We adopt training setting and hyper parameters (batch size, data augmentation, regularization\nand learning rate scheduling) from DeiT [6].\nIn a nutshell, A ViT splits an image into 16 ×16 non-overlapping patches, then linearly projects\neach patch with shared weights to the equivalence of token embeddings. A learned class token is\nappended to the resulting representation, resulting in a sequence of length T = 1 +H/16\nW/16 . A linear\nclassiﬁcation head is attached to the ﬁnal layer’s class token to obtain the ﬁnal outputs. See [5] for\nmore details of the model conﬁguration. All the experiments are conducted on the ImageNet-1K\ndataset, without using extra data.\nSince the sequence size is relatively small in this task (T = 197for input sizes of 224 ×224), we\nﬁrst experiment with AFT-full. The hidden dimension of factorized position bias is set as d′= 128.\nBesides, we also experiment with AFT-conv. In this setting, we also remove the use of position\nembedding and class token, and apply global average pooling after the ﬁnal layer’s output, which is\nthen fed into the classiﬁcation linear layer. This modiﬁcation not only simpliﬁes the model design,\nbut also makes AFT-convfully convolutional, which is absent from Transformer and its variants.\nWe compare against two baseline Transformer conﬁgurations, with the “tiny\" (L=12, d=192, h=3)\nand “small\" (L=12, d=384, h=6) conﬁgurations, respectively. We also consider Lambda Networks\n[15], which is closely related to the linearized attention line of work. Similar to AFT-conv, we\nremove the class token and apply global average pooling instead. We use a publicly available\nimplementation 7, and apply the full context mode with the key projection dimension |k|= 16\n(this setting invokes the faster linear implementation). We also apply BatchNorm to the query, key\nprojections as recommended by [15].\n7github.com/lucidrains/lambda-networks, released under MIT License\n8\nTable 7: Imagenet 1K classiﬁcation results with the Transformer architecture from DeiT [6], cropsize\nis 224. Speed and memory consumption are measured in inference mode on a V100 GPU, batch size\nis 256.\nModel Kernel Heads Top1 Acc #Params (MB) Images/Sec Mem (GB)\nResNet50 [33] 3 - 76.9 25.6 1257 6.5\nDeiT tiny [6] - 3 72.2 5.7 2507 1.9\nDeiT small [6] - 6 79.9 22.1 1010 2.9\nLambda tiny [15] - 3 72.4 4.8 2157 2.7\nLambda small [15] - 6 80.0 17.7 1057 5.8\nAFT-full tiny - 1 72.4 6.3 2523 1.8\nAFT-full small - 1 79.8 22.6 1011 2.6\nAFT-conv tiny 11 32 73.9 5.4 2359 1.8\nAFT-conv tiny 11 192 74.8 5.9 2365 2.2\nAFT-conv small 11 16 80.2 20.3 989 2.5\nAFT-conv small 11 384 80.8 22.5 936 3.2\nAFT-conv small 15 384 81.0 23.0 936 3.2\nOur result is shown in Table 7. We ﬁrst see that AFT-full achieves comparable performance with the\nbaseline Transformer DeiT in both conﬁgurations, while with better memory footprint and similar\nspeed. AFT-conv signiﬁcantly improves the top-1 accuracy of both conﬁgurations (2.%6, 1.1%\nabsolute improvement for “tiny\" and “small\", respectively), with similar or smaller parameter counts.\nCompared to Lambda Networks, all AFT variants achieve comparable or better accuracy, with\ncomparable speed and much smaller memory footprints.\nVisualization. We also tried to visualize the position biases (exp(w) −1 to be precise) learned by\nAFT-conv, as shown in Figure 1 (right). Note that interesting local, symmetric sparse patterns emerge.\nWe show in the Appendix that we can regularize the position biases to achieve more sparsity. We\nalso show an extreme version of AFT-conv, where each head is assigned one non-zero context points,\nwhile still keep good accuracy. This effectively transforms convolution into indexing.\nVariable size inputs. AFT-conv is fully convolutional, which means that it can handle an input size\ndifferent from that in training. We tested an AFT-conv model (last row of Table 7, trained with crop\nsize 224) on a larger crop size of 384. This results in an improved accuracy of 81.6, compared with\nthe original 81.0. This makes AFT-conv well suited for the pretraining ﬁnetuning workﬂows, as often\nseen in Vision tasks.\nCompatibility with Transformers. Although AFT is not designed to directly approximate MHA,\nthey do share considerable similarity in that the value vectors are aggregated with learned non-\nnegative weighting in both models. We hypothesize that representations learned by one model can be\ntransferred to another. To test this, we obtain a pretrained “DeiT base\" model with crop size 384. We\nthen train an AFT-conv by initializing its weights with that of the DeiT model, excluding the position\nembeddings, the class token, key and query projections. We use a batch size of 64 and train the model\nfor 100 epochs. As a control, we also train a randomly initialized AFT-conv for the same number of\nepochs. The results are shown in Table 8. Interestingly, we see that the ﬁnetuned version of AFT-conv\nachieves signiﬁcantly higher accuracy than that randomly initialized version. The resulting model is\nalso more accurate, faster and memory efﬁcient than the original DeiT model.\nGlobal connectivity. AFT-conv (as well as AFT-local) maintains global connectivity regardless\nof the local kernel size, which is distinctive from sparse and local attention works. To see the\nbeneﬁt of this design, we trained a degenerate variant of AFT-conv, where we modify Equation 4\nto assign −∞values to wt,t′ outside the local window (zero weights after exponentiation). When\nevaluating this baseline with kernel size 7, it gives a Top 1 accuracy of 79.9, compared to the default\nAFT-conv’s 80.8 with the same setting, which is a 0.9% drop (we observe the same trend consistently\nin various conﬁgurations). We hypothesize that this technique can also be extended to local and\nsparse Transformers, but will leave it as future work.\n6 Conclusions\nWe have introduced the Attention Free Transformer that replaces dot product attention with an\nefﬁcient new operation. We have demonstrated strong results on a set of standard benchmarks with\n9\nTable 8: Finetuning AFT-conv for 100 epochs from a pretrained “DeiT base\" on 384 ×384 crops.\n“ft\" and “rand\" stand for ﬁnetuning and random initialization, respectively.\nModel Kernel Heads Top1 Acc #Params (MB) Images/Sec Mem (GB)\nDeit base [33] - 12 82.9 86.9 89.6 13.6\nAFT-conv ft 25 32 83.4 79.7 98.5 8.9\nAFT-conv rand 25 32 81.6 79.7 98.5 8.9\nexcellent efﬁciency. We believe that our model opens a new design space for Transformer-like models,\nand will see impact in various areas where self attention are needed.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008, 2017.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[3] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training.\n[4] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining\nfrom pixels.\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[6] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020.\n[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. CoRR, abs/1904.10509, 2019.\n[8] Nikita Kitaev, L. Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. ArXiv,\nabs/2001.04451, 2020.\n[9] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and T. Lillicrap. Compressive trans-\nformers for long-range sequence modelling. ArXiv, abs/1911.05507, 2020.\n[10] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. ArXiv, abs/2006.04768, 2020.\n[11] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In Proceedings of the International Conference on Machine\nLearning (ICML), 2020.\n[12] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:\nRethinking self-attention in transformer models, 2020.\n[13] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger,\nLucy Colwell, and Adrian Weller. Rethinking attention with performers, 2020.\n[14] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.\nRandom feature attention. In International Conference on Learning Representations, 2021.\n10\n[15] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. In Interna-\ntional Conference on Learning Representations, 2021.\n[16] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey,\n2020.\n[17] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku,\nand Dustin Tran. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\n[18] Huiyu Wang, Y . Zhu, B. Green, H. Adam, A. Yuille, and Liang-Chieh Chen. Axial-deeplab:\nStand-alone axial-attention for panoptic segmentation. ArXiv, abs/2003.07853, 2020.\n[19] Zilong Huang, Xinggang Wang, Lichao Huang, C. Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. 2019 IEEE/CVF International Conference on\nComputer Vision (ICCV), pages 603–612, 2019.\n[20] Zhen Zhu, Mengdu Xu, Song Bai, Tengteng Huang, and X. Bai. Asymmetric non-local neural\nnetworks for semantic segmentation. 2019 IEEE/CVF International Conference on Computer\nVision (ICCV), pages 593–602, 2019.\n[21] Lang Huang, Y . Yuan, Jianyuan Guo, Chao Zhang, X. Chen, and Jingdong Wang. Interlaced\nsparse self-attention for semantic segmentation. ArXiv, abs/1907.12273, 2019.\n[22] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, I. Bello, Anselm Levskaya, and Jonathon\nShlens. Stand-alone self-attention in vision models. ArXiv, abs/1906.05909, 2019.\n[23] Sainbayar Sukhbaatar, E. Grave, P. Bojanowski, and Armand Joulin. Adaptive attention span in\ntransformers. In ACL, 2019.\n[24] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse\nattention with routing transformers. ArXiv, abs/2003.05997, 2020.\n[25] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and M. Auli. Pay less attention with\nlightweight and dynamic convolutions. ArXiv, abs/1901.10430, 2019.\n[26] Yi Tay, Dara Bahri, L. Yang, Donald Metzler, and D. Juan. Sparse sinkhorn attention.ArXiv,\nabs/2002.11296, 2020.\n[27] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas\nUnterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and\nAlexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021.\n[28] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V . Le. Pay attention to mlps, 2021.\n[29] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.\n[31] Matt Mahoney. Large text compression benchmark, 2011.\n[32] Zihang Dai, Z. Yang, Yiming Yang, J. Carbonell, Quoc V . Le, and R. Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a ﬁxed-length context. ArXiv,\nabs/1901.02860, 2019.\n[33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition, 2015.\n[34] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax,\n2017.\n11\nFigure 3: Exponentiated position biases learned by AFT-full, trained on ImageNet-1K, shown from\nlayer 1, 2, ..., 12, arranged from top left to bottom right. Each image is of size 197 ×197, where\nthe ﬁrst element corresponds to the class token, and the remaining 196 correspond to the 14 ×14\npositions. We see that local, sparse patterns are learned without explicit supervision.\nTable 9: The effect of factorized parameterization\nof AFT-full.\nTrain loss Top 1 Acc\nNon Factorized 3.17 78.2\nFactorized (default) 3.08 79.8\nTable 10: The effect of reprameterization of AFT-\nconv (kernel size 7 ×7).\nTrain loss Top 1 Acc\nNaive param 3.11 79.4\nReparameterized (default) 2.94 80.8\n7 Additional Ablations\nWe conducted more experiments on the ImageNet-1K classiﬁcation settings.\nFactorization of w. We ﬁrst verify the importance of the factorized parameterization of AFT-full.\nAs shown in Tab 9, the non factorized parameterization of AFT-full achieves worse training and test\nperformance than the factorized version.\nReparameterization of w. For AFT-conv, we by default apply the reprameterization as described in\nSec. 3.2. We verify that this design effectively improves the model’s performance, as shown in Table\n10.\nKernel size. We also experimented with varying the local window size based on AFT-conv small\n(384 heads). The results are shown in Tab 11. Note that AFT-conv achieves comparable performance\nto the Deit reference even with a very small kernel size of 3 ×3.\nTable 11: Varying kernel size for AFT-conv.\nKernel 3 7 11 15 25 27 DeiT small\nTrain loss 3.02 2.94 2.94 2.93 2.93 2.94 3.01\nTop 1 Acc 79.9 80.8 80.8 81.0 80.7 81.0 79.9\n12\nFigure 4: Image completion with the AFT-local trained on CIFAR10 autoregressive modeling task.\nTop: masked images from the test set. Bottom: completed images.\nTable 12: Top 1 accuracy of AFT-conv without the query term (w/o q). This results in signiﬁcant\nperformance drops.\nKernel 11 15\nwith q (default) 80.8 81.0\nw/o q 79.3 79.5\nContribution of the query. The query term contributes a small fraction to the computation of AFT,\nbut it contributes signiﬁcantly to AFT’s performance. We conducted an additional experiment with\nAFT-conv (384 heads, kernel size in11 ×11 and 15 ×15), where we remove the query term. The\nresult is shown in Tab 12.\nVisualizing the key. The keys play a central role in AFT, as they provide content dependent\nreweighting for effective context reduction. In order to understand their behavior, we visualized\nthe feature maps for a AFT-conv model on randomly sampled images from the validation set of\nImageNet-1K, as shown in Fig. 9, 10, 11, 12. Interestingly, we see that the keys gradually evolve to\n“object detectors\" as the layer level goes up.\n13\nFigure 5: The full set of average relative 2d attention maps learned by a pretrained ViT model (with 12\nlayers and 6 heads) on ImageNet-1K. Each row corresponds to a layer and each column corresponds\nto a head. Each attention map is of size 27 ×27, with the class token excluded.\n14\nFigure 6: Exponentiated position biases learned by AFT-conv, trained on ImageNet-1K. Each row\ncorresponds to a layer, each column corresponds to a head (the ﬁrst 16 are shown). This model has\ntop 1 accuracy of 80.8%.\nFigure 7: Exponentiated position biases learned by AFT-conv (kernel size 11 ×11) with sparsity\nregularization, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds\nto a head (the ﬁrst 16 are shown). This model has top 1 accuracy of 80.9%.\n15\nFigure 8: Exponentiated position biases learned AFT-conv (kernel size11×11) with Gumbel softmax\nsampling, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds to a\nhead (the ﬁrst 16 are shown). This model has top 1 accuracy of 79.9%.\n8 Sparsity\nThe position biases learned by AFT-conv (kernel size11 ×11) as shown in Figure 6 demonstrates\ninteresting sparsity patterns, which suggests great potential for quantization and pruning. To this end,\nwe experimented with a simple sparsity promoting regularization term:\nreg(w) =\nh∑\ni=1\nH(wi), H(wi) =entropy(softmax(wi)). (9)\nWhere we simply minimize the entropy for each head, with the softmax distribution using wi as\nthe logits. We combining reg(w) with the cross entropy loss with a small weighting ( 0.001) and\ntrain with the AFT-conv with kernel size 11 and 384 heads. This results in a slight improvement\nin accuracy (due to its regularization effect) of 80.9 vs 80.8, as well as sparser looking position\nbiases. The visualization is shown in Fig. 7. We see that the position biases are much more sparsely\ndistributed as expected.\nEncouraged by this, we continued to push the sparsity to an extreme form. Now for each head, we\nonly assign a learned relative position bias for a single position. To do this, during training, we\nmultiply the position biases wfor each layer and each head with a sample from its corresponding\nGumbel softmax distribution [34]:\nwi = wi ∗gumbel(wi; τ), (10)\nwhere τ is the temperature term for Gumbel softmax, and we set it as 0.5; gumbel(wi; τ) produces a\n(sparse) sample with the same shape as wi. During inference, the Gumbel softmax is replaced with\nhard max, i.e., a one hot vector is returned. This results in a model with top 1 accuracy 79.9, with less\nthan 1 point drop compared with the unregularized model. The position biases are visualized in Fig.\n8. This extreme model variant makes it possible to implement the context reduction of K,V with a\ncombination of global average pooling and indexing, which has the same complexity as AFT-simple\nbut maintains strong performance (comparable to that of the standard Transformer).\n16\nFigure 9: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n17\nFigure 10: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n18\nFigure 11: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n19\nFigure 12: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n20\nFigure 1: Exponentiated position biases learned by AFT-full, trained on ImageNet-1K, shown from\nlayer 1, 2, ..., 12, arranged from top left to bottom right. Each image is of size 197 × 197, where\nthe ﬁrst element corresponds to the class token, and the remaining 196 correspond to the 14 × 14\npositions. We see that local, sparse patterns are learned without explicit supervision.\nTable 1: The effect of factorized parameterization\nof AFT-full.\nTrain loss Top 1 Acc\nNon Factorized 3.17 78.2\nFactorized (default) 3.08 79.8\nTable 2: The effect of reprameterization of AFT-\nconv (kernel size 7 × 7).\nTrain loss Top 1 Acc\nNaive param 3.11 79.4\nReparameterized (default) 2.94 80.8\n1 Additional Ablations1\nWe conducted more experiments on the ImageNet-1K classiﬁcation settings.2\nFactorization ofw. We ﬁrst verify the importance of the factorized parameterization of AFT-full.3\nAs shown in Tab 1, the non factorized parameterization of AFT-full achieves worse training and test4\nperformance than the factorized version.5\nReparameterization ofw. For AFT-conv, we by default apply the reprameterization as described in6\nSec. 3.2. We verify that this design effectively improves the model’s performance, as shown in Table7\n2.8\nKernel size.We also experimented with varying the local window size based on AFT-conv small9\n(384 heads). The results are shown in Tab 3. Note that AFT-conv achieves comparable performance10\nto the Deit reference even with a very small kernel size of 3 × 3.11\nTable 3: Varying kernel size for AFT-conv.\nKernel 3 7 11 15 25 27 DeiT small\nTrain loss 3.02 2.94 2.94 2.93 2.93 2.94 3.01\nTop 1 Acc 79.9 80.8 80.8 81.0 80.7 81.0 79.9\n1\narXiv:2105.14103v2  [cs.LG]  21 Sep 2021\nFigure 2: Image completion with the AFT-local trained on CIFAR10 autoregressive modeling task.\nTop: masked images from the test set. Bottom: completed images.\nTable 4: Top 1 accuracy of AFT-conv without the query term (w/o q). This results in signiﬁcant\nperformance drops.\nKernel 11 15\nwith q (default) 80.8 81.0\nw/o q 79.3 79.5\nContribution of the query.The query term contributes a small fraction to the computation of AFT,12\nbut it contributes signiﬁcantly to AFT’s performance. We conducted an additional experiment with13\nAFT-conv (384 heads, kernel size in11 × 11 and 15 × 15), where we remove the query term. The14\nresult is shown in Tab 4.15\nVisualizing the key. The keys play a central role in AFT, as they provide content dependent16\nreweighting for effective context reduction. In order to understand their behavior, we visualized17\nthe feature maps for a AFT-conv model on randomly sampled images from the validation set of18\nImageNet-1K, as shown in Fig. 7, 8, 9, 10. Interestingly, we see that the keys gradually evolve to19\n“object detectors\" as the layer level goes up.20\n2\nFigure 3: The full set of average relative 2d attention maps learned by a pretrained ViT model (with 12\nlayers and 6 heads) on ImageNet-1K. Each row corresponds to a layer and each column corresponds\nto a head. Each attention map is of size 27 × 27, with the class token excluded.\n3\nFigure 4: Exponentiated position biases learned by AFT-conv, trained on ImageNet-1K. Each row\ncorresponds to a layer, each column corresponds to a head (the ﬁrst 16 are shown). This model has\ntop 1 accuracy of 80.8%.\nFigure 5: Exponentiated position biases learned by AFT-conv (kernel size 11 × 11) with sparsity\nregularization, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds\nto a head (the ﬁrst 16 are shown). This model has top 1 accuracy of 80.9%.\n4\nFigure 6: Exponentiated position biases learned AFT-conv (kernel size11×11) with Gumbel softmax\nsampling, trained on ImageNet-1K. Each row corresponds to a layer, each column corresponds to a\nhead (the ﬁrst 16 are shown). This model has top 1 accuracy of 79.9%.\n2 Sparsity21\nThe position biases learned by AFT-conv (kernel size11 × 11) as shown in Figure 4 demonstrates22\ninteresting sparsity patterns, which suggests great potential for quantization and pruning. To this end,23\nwe experimented with a simple sparsity promoting regularization term:24\nreg(w) =\nh∑\ni=1\nH(wi), H(wi) =entropy(softmax(wi)). (1)\nWhere we simply minimize the entropy for each head, with the softmax distribution using wi as25\nthe logits. We combining reg(w) with the cross entropy loss with a small weighting ( 0.001) and26\ntrain with the AFT-conv with kernel size 11 and 384 heads. This results in a slight improvement27\nin accuracy (due to its regularization effect) of 80.9 vs 80.8, as well as sparser looking position28\nbiases. The visualization is shown in Fig. 5. We see that the position biases are much more sparsely29\ndistributed as expected.30\nEncouraged by this, we continued to push the sparsity to an extreme form. Now for each head, we31\nonly assign a learned relative position bias for a single position. To do this, during training, we32\nmultiply the position biases wfor each layer and each head with a sample from its corresponding33\nGumbel softmax distribution [? ]:34\nwi = wi ∗ gumbel(wi; τ), (2)\nwhere τ is the temperature term for Gumbel softmax, and we set it as 0.5; gumbel(wi; τ) produces a35\n(sparse) sample with the same shape as wi. During inference, the Gumbel softmax is replaced with36\nhard max, i.e., a one hot vector is returned. This results in a model with top 1 accuracy 79.9, with less37\nthan 1 point drop compared with the unregularized model. The position biases are visualized in Fig.38\n6. This extreme model variant makes it possible to implement the context reduction of K,V with a39\ncombination of global average pooling and indexing, which has the same complexity as AFT-simple40\nbut maintains strong performance (comparable to that of the standard Transformer).41\n5\nFigure 7: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n6\nFigure 8: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n7\nFigure 9: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n8\nFigure 10: Top: sample image from the validation set of ImageNet-1K. Bottom: visualization of the\nkeys for AFT-conv, with each row corresponding to a layer, each column corresponding to a head.\n9",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.727778971195221
    },
    {
      "name": "Autoregressive model",
      "score": 0.7138515710830688
    },
    {
      "name": "Pooling",
      "score": 0.6909864544868469
    },
    {
      "name": "Computer science",
      "score": 0.5839637517929077
    },
    {
      "name": "Algorithm",
      "score": 0.4919690787792206
    },
    {
      "name": "Computational complexity theory",
      "score": 0.4436781108379364
    },
    {
      "name": "Decoding methods",
      "score": 0.422607958316803
    },
    {
      "name": "Computer engineering",
      "score": 0.38435056805610657
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3011813759803772
    },
    {
      "name": "Mathematics",
      "score": 0.22955024242401123
    },
    {
      "name": "Engineering",
      "score": 0.1802840530872345
    },
    {
      "name": "Voltage",
      "score": 0.17992085218429565
    },
    {
      "name": "Electrical engineering",
      "score": 0.15396562218666077
    },
    {
      "name": "Econometrics",
      "score": 0.1023242175579071
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210141230",
      "name": "Apple (Germany)",
      "country": "DE"
    }
  ],
  "cited_by": 42
}