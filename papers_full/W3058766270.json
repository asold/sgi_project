{
  "title": "Discovering Useful Sentence Representations from Large Pretrained Language Models",
  "url": "https://openalex.org/W3058766270",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222118670",
      "name": "Subramani, Nishant",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2887551186",
      "name": "Suresh, Nivedita",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2891521313",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3022853932",
    "https://openalex.org/W2963376432",
    "https://openalex.org/W2951560313",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2971147102",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3035233162",
    "https://openalex.org/W2125320996",
    "https://openalex.org/W3023585950",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2963088995",
    "https://openalex.org/W2557449848",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2888779557",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2948629866",
    "https://openalex.org/W2919290281",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2963929190",
    "https://openalex.org/W2963336685",
    "https://openalex.org/W2909544278"
  ],
  "abstract": "Despite the extensive success of pretrained language models as encoders for building NLP systems, they haven't seen prominence as decoders for sequence generation tasks. We explore the question of whether these models can be adapted to be used as universal decoders. To be considered \"universal,\" a decoder must have an implicit representation for any target sentence $s$, such that it can recover that sentence exactly when conditioned on its representation. For large transformer-based language models trained on vast amounts of English text, we investigate whether such representations can be easily discovered using standard optimization methods. We present and compare three representation injection techniques for transformer-based models and three accompanying methods which map sentences to and from this representation space. Experiments show that not only do representations exist for sentences from a variety of genres. More importantly, without needing complex optimization algorithms, our methods recover these sentences almost perfectly without fine-tuning the underlying language model at all.",
  "full_text": "Discovering Useful Sentence Representations from Large Pretrained\nLanguage Models\nNishant Subramani\nScale AI\nnishant.subramani@scale.com\nNivedita Suresh\nArrive\nnive@arriveorigin.com\nAbstract\nDespite the extensive success of pretrained lan-\nguage models as encoders for building NLP\nsystems, they haven’t seen prominence as de-\ncoders for sequence generation tasks. We ex-\nplore the question of whether these models can\nbe adapted to be used as universal decoders.\nTo be considered ”universal,” a decoder must\nhave an implicit representation for any tar-\nget sentence s, such that it can recover that\nsentence exactly when conditioned on its rep-\nresentation. For large transformer-based lan-\nguage models trained on vast amounts of En-\nglish text, we investigate whether such repre-\nsentations can be easily discovered using stan-\ndard optimization methods. We present and\ncompare three representation injection tech-\nniques for transformer-based models and three\naccompanying methods which map sentences\nto and from this representation space. Exper-\niments show that not only do representations\nexist for sentences from a variety of genres.\nMore importantly, without needing complex\noptimization algorithms, our methods recover\nthese sentences almost perfectly without ﬁne-\ntuning the underlying language model at all.\n1 Introduction\nRecently, pretrained language models such as\nELMo, BERT, and T5 have seen widespread suc-\ncess as encoders for a variety of natural language\nprocessing tasks often with little or no ﬁnetun-\ning (Peters et al., 2018; Devlin et al., 2019; Raffel\net al., 2019). However, this has not transferred to\ndecoders, i.e. most decoders for sequence gener-\nation tasks are task-speciﬁc and are trained from\nscratch (Nallapati et al., 2016; Johnson et al., 2017;\nAharoni et al., 2019). We explore whether pre-\ntrained language models can be modiﬁed to be\nused as ”universal” decoders.\nFor a decoder to be considered ”universal”, it\nmust be able to successfully recover a sentence\nwhen conditioned on its implicit sentence repre-\nsentation. Such a decoder would provide many\nbeneﬁts: make training text generation models\non little amounts of annotated data possible, al-\nlow considerable parameter sharing in memory-\nand data-limited environments, and improve zero-\nshot text generation performance. Imagine you are\ntasked with building a Kurdish to English transla-\ntion model. You ﬁnd that there’s very little parallel\ndata on this language pair to learn from and realize\nthat an end-to-end trainable sequence-to-sequence\nmodel cannot be ﬁt well. If you had a universal de-\ncoder, you may be able to train a Kurdish encoder,\nwhich is much smaller than the entire sequence-to-\nsequence model, and optimize it to work with the\nuniversal decoder.\nIn this work, we take an initial step towards eval-\nuating whether large pretrained language models\ncan be used as universal decoders without ﬁne-\ntuning. We ﬁrst deﬁne the sentence space of a\ntransformer language model, GPT-2 (Radford et al.,\n2019), and reparametrize each point in this space to\na lower-dimensional point by adding a single bias\nterm z to various locations in the model. Keeping\nthe language model ﬁxed, we optimize z to maxi-\nmize the likelihood of the original sentence x and\nrecover x from z in order to evaluate how useful\nthe representation is. In other words, we reverse-\nengineer a sentence representation that generates\nthe target sentence.\nOur experiments uncover that we can achieve\nnearly perfect recoverability with a reparametrized\nsentence space of dimension equal to the latent\ndimension of the language model. That is to say,\nfor nearly all sentences, there exists at least one\nrelatively low-dimensional vector that, by itself,\ncan recover the sentence of interest nearly exactly.\nFurther, we show that this holds for text from a\nvariety of genres ranging from books to news to\nmovie quotes to Wikipedia. We learn that discover-\narXiv:2008.09049v1  [cs.CL]  20 Aug 2020\nFigure 1: We add a bias Z′ based on Equation 2 to three different locations in GPT-2: to the embedding, to the\ntransformer layers, and before the language modeling head. Here ’Embeds’ refers to the embedding, ’SA’ to self-\nattention, ’LN’ to layer normalization (Ba et al., 2016), ’FFN’ to a fully-connected layer, and ’LM Head’ to the\nlast fully-connected layer.\ning nearly perfect representations is relatively easy\nusing simple optimization with Adam (Kingma\nand Ba, 2014), unlike previous work (Subramani\net al., 2019). Our experiments show that recov-\nerability increases as the dimensionality of the\nreparametrized space increases and decreases with\nincreased sentence length, i.e. recoverability is\nlower for longer sentences. Using PCA, we ﬁnd that\nthe reparametrized sentence space does not lie on a\nlower-dimensional linear manifold, and conﬁrms\nthat the intrinsic dimension of the reparametrized\nspace is approximately equal to the latent dimen-\nsion of the language model.\n2 Learning Sentence Representations\nBelow, we discuss background on transformer-\nbased language models and characterize how these\nmodels represent sentences (Vaswani et al., 2017).\nWe show how to reparametrize this space into\na lower-dimensional space and deﬁne the no-\ntion of the recoverability of a sentence in this\nreparametrized space. We show these for GPT-\n2, but indicate how our methodology is model-\nagnostic.\nTransformer language models such as GPT-2,\nrepresent a sentence x = x1,...,x T as a sequence\nof hidden states h1,..., hT , which come from\nthe ﬁnal layer of the transformer model. Since\nhi ∈Rd, where dis the latent dimension of the\nlanguage model, the model encodes x1,...,x T in\na sentence space H∈ Rd×T . Representations in\nthis sentence space are sequence length dependent,\nmaking comparisons between sentences with dif-\nfering lengths inequitable and measuring the efﬁ-\ncacy of using an unconditional language model as\na universal decoder impossible. To resolve these is-\nsues and to make analysis easier, we reparametrize\nthe sentence space into a lower-dimensional and\nsentence-length agnostic vector space.\n2.1 Representation Space\nWe propose to reparametrize the original sentence\nspace H∈ Rd×T to Z∈ Rd′\n, mapping a sentence\nlength dependent, high-dimensional vector space\ninto a lower dimensional, sentence-length agnostic\nvector space of dimension d′. In our experiments,\nd′≤d. We do this by adding a bias term z ∈Rd′\nto the ﬁxed language model and ﬁnd a ˆz that min-\nimizes the cross entropy loss of the sentence. We\ninject z by using a projection matrix Wz ∈Rd×d′\n,\nwhich is never trained and is ﬁxed throughout.\nWz = [Id′ ; Wmix]⊤ (1)\nHere, Wmix ∈Rd′×(d−d′) is a probability weight\nmatrix where the columns sum to 1, where we\nsample each entry from a standard Gaussian and\ncompute a softmax over columns. We randomly\npermute the independent and dependent compo-\nnents of Wz to avoid an arbitrary, ﬁxed ordering of\ncolumns.\nOur reparametrization must give us the ability\nto project a sequence of tokens x = x1,...,x T\ninto a representation z (sentence encoding) and\nto recover x from z (sentence recovery) via the\nlanguage model. Without this property, we cannot\nmeasure recoverability. Imagine a task-speciﬁc en-\ncoder trained to produce context for a conditional\ngeneration task. The output of such an encoder re-\nsembles the z ∈Z we wish to discover. With our\nreparameterization approach, we expect z to en-\ncode the target sentence using sentence encoding\nand regenerate it using sentence recovery.\n2.2 Representation Injection\nWe experiment with three z injection locations:\nembedding (embed), each layer of the transformer\n(layers), and language model head (head). See Fig-\nure 1 for details. We also experiment with three\nrepresentation injection mechanisms that transform\nz to z′ and inject z′ into the language model: no\nensembling, attention-based ensembling, and inter-\nleaved ensembling. Ensembling splits up z into k\nexperts and allows those kexperts to work together\nto learn a sentence representation. Here, z is split\nup into a matrix Z ∈R\nd′\nk ×k and Wz ∈Rd×d′\nk .\nIn no ensembling, k = 1, so Z = z. In attention-\nbased ensembling, we use soft-attention with the\nprevious layer’s hidden state (Bahdanau et al.,\n2015), allowing the model to learn an adaptive\ncombination of the kvectors per input token. In in-\nterleaved ensembling, we use the ﬁrst vector for the\nﬁrst token, the second for the second token, until we\nreach k. After we process thekth token, we start the\nprocess over again with the ﬁrst vector. This way,\neach of the kvectors are responsible for only every\nkth token. To do this, we use Wint ∈RT×k, which\ncomprises of T\nk many Ik matrices concatenated to-\ngether and the ﬁrst T rows chosen. Below are the\nequations for no ensembling, attention-based en-\nsembling, and interleaved ensembling respectively:\nZ′=\n\n\n\nWzZ,\nsoftmax(Ht−1(WzZ))(WzZ)⊤,\nWint(WzZ)⊤,\n(2)\n2.3 Sentence Encoding & Recovery\nIn sentence encoding, we project a sentence x into\na representation z via the language model ΘLM\nusing Equation 2. We estimate z by maximizing\nthe log probability of x, while keeping ΘLM ﬁxed:\nˆz = argmax\nz∈Z\nT∑\nt=1\nlog p(xt|x<t,z) (3)\nHere, we represent the entire sentence x with a\nsingle z. Since this objective function is highly\nnon-convex and could potentially lead to many lo-\ncal optima, we randomly initialize z, ntimes and\nmeasure recoverability over them. Our experiments\nreveal that different z’s can recover the original\nsentence perfectly, although recoverability is some-\nwhat sensitive to initialization.\nSentence recovery aims to recover the original\nsentence x from z ∈ Z. In essence, we ﬁnd\nthe most probable sentence x under the model,\nΘLM . Our experiments show that beam search and\ngreedy decoding perform similarly even with dif-\nferent beam widths. Therefore, all results presented\nhere use greedy decoding without assuming a true\nlength. We stop when decoding produces either an\nend-of-sentence token or 150 consecutive tokens.\n3 Measuring the Effectiveness of\nSentence Representations\nWe want our sentence representations to be unique\nand implicit for each target sentence s such that\nwhen our language model is conditioned by our\nrepresentation, it can recover sexactly. Our formu-\nlation does not require a bijective mapping, only a\nsurjective mapping between the sentence represen-\ntation z and the original sentence s. We measure\nthe effectiveness of these representations through\nthe lens of recoverability using three common met-\nrics (Subramani et al., 2019).\n3.1 Recoverability Metrics\nWhen measuring recoverability, we estimate how\nmuch information our representation z retains\nabout the target sentence s. To estimate how much\nrelevant information about generation our repre-\nsentations contain, we measure token-level exact\nmatch, preﬁx match, and Smoothed BLEU using\nthe target sentence s and our reconstruction of\nit, ˆs (Subramani et al., 2019). Token-level exact\nmatch calculates the average number of correct\ntokens in a candidate sentence. Preﬁx match mea-\nsures the longest consecutive sequence of tokens\nfrom the beginning of the sentence which are re-\ncovered correctly as a proportion of the length of\nthe target sentence. This is relevant because auto-\nregressive natural language generation has a very\nstrong left-to-right tendency due to decoding oc-\ncurring left-to-right for English and other left-to-\nright languages (Subramani et al., 2019). Smoothed\nBLEU provides a smoother approximation to token-\nlevel exact match and is a popular metric in eval-\nuating conditional language modeling tasks such\nas machine translation (Papineni et al., 2002; Chen\nand Cherry, 2014). To measure smoothed BLEU,\nwe use sacrebleu’s exponential smoothing with the\nWMT standard 13a tokenization (Post, 2018). We\nuse nrandom initializations and recover the same\ntarget sentence x from each of them, computing\nmean scores to measure initialization variability. In\naddition, we evaluate the maximum scores from\nthose nrandom initializations across our metrics:\nEM-Max, PM-Max, and BLEU-Max.\n3.2 Analyzing Intrinsic Dimension\nUnder the lens of recoverability, we deﬁne the in-\ntrinsic dimension of the reparametrized sentence\nspace to be the smallest dimension of z (d′) that\nproduces a speciﬁc target recoverability τ (Bo-\njanowski et al., 2018; Subramani et al., 2019):\nˆd′(θ,τ) = min\nd′\n{\nd′: BLEU(D|(d′,θ)) >τ\n}\n(4)\nHere, BLEU is the target recoverability measure\nfor dimension d′for model θand is computed as:\nBLEU(Dx|θ,d′) =\n∑\nx∈Dx\n∑n\ni=0 BLEU(ˆxi,x)\n|Dx|·n\n(5)\nBLEU(D|θ,d′) = 1\n|D|\n∑\nDx∈D\nBLEU(Dx|θ,d′)\n(6)\nHere, |D|is the number of corpora,|Dx|is the num-\nber of sentences in each corpus, nis the number of\ndifferent random initializations of z per sentence\nper corpus, and ˆx is the predicted sentence.\nIn addition, we analyze the intrinsic dimension-\nality of Zusing principal component analysis by\ntransforming Z∈ Rd′\ninto orthogonal basis vec-\ntors. Equipped with these orthogonal bases, we can\nmeasure how many components are required to\ncapture a proportion pof the variability in the data\nusing cumulative explained variance.\n4 Experimental Setup\nData Collection For experiments on sentence\nrecoverability, we create a dataset which com-\nbines four corpora from different genres: movie\ndialogs (movies), classic books (books), news ar-\nticles (news), and Wikipedia (wiki). For movies,\nwe choose the Cornell Movie Dialogs cor-\npus (Danescu-Niculescu-Mizil and Lee, 2011),\nwhich consists of ﬁctional conversations from 617\nraw movie scripts. We choose NLTK’s Gutenberg\ndataset for our books portion, which consists of\na subset of texts from Project Gutenberg (Lebert,\n2008). Our news subset comes from the Gigaword\ndataset for abstractive summarization (Graff et al.,\n2003), consisting of 3.8 million articles. Lastly, our\nWikipedia portion comes from WikiText-103 (Mer-\nity et al., 2017), a dataset with 28,475 veriﬁed\narticles. For movies, news, and wiki, we extract\nsentences from its pre-speciﬁed validation set. For\nbooks, since NLTK’s Gutenberg dataset lacks a pre-\nspeciﬁed data split, we consider the entire dataset.\nData Preprocessing We sentence tokenize all\nof our datasets using NLTK’s sentence tokenizer.\nNext, we randomly sample 16 sentences from each\ncorpus, making sure sentences are between 5 and\n100 words according to NLTK’s word-level, regular\nexpression tokenizer. We call this the small recov-\nery corpus (SRC). To construct a larger corpus, the\nlarge recovery corpus (LRC), we group sentences\nby sentence length into 8 bins: 5-10, 10-15, 15-20,\n20-25, 25-30, 30-35, 35-40, and 40-100, and ran-\ndomly sample 64 sentences from each of the bins,\nensuring that no sentences overlap between LRC\nand SRC. Lastly, we create a third corpus that we\ncall the gibberish recovery corpus (GRC), by sam-\npling tokens uniformly at random with replacement\nfrom the GPT2 vocabulary such that we have 8 gib-\nberish sentences in each of the 8 sentence length\nbins above similarly to Subramani et al. (2019).\nPhase I: Experimental Phase We use SRC to\nevaluate the best initialization technique (I), injec-\ntion location (II), and ensembling strategy (III) in\nan iterative manner in this order. Refer to Table 1\nfor details. In these experiments, we use stochastic\ngradient descent with Adam with a learning rate\nof 0.01 (Kingma and Ba, 2014), maximum number\nof optimization steps of 1000, learning rate decay\nwith a plateau with a patience of 3 and decay factor\nof 0.8, dimensionality of z of 768, and n, the num-\nber of random z initializations, of 4. Motivated by\nlooking at a few iterations of sentence encoding, we\nstop optimization early if the learning rate decays\nto 1e−5. We also stop optimization early if mean\ncross entropy loss reaches min(0.1, 2\nT ), where T\nis sequence length. This heuristic is not crucial, but\nallows experimentation to run quickly without a\ndegradation in performance.\nPhase II: Testing Phase We use LRC to evalu-\nate recoverability in order to estimate the intrinsic\ndimension of Z(IV). Using the same hyperparam-\neters from phase I and choosing the best initial-\nization method, injection location, and ensembling\nstrategy, we estimate the intrinsic dimension of\nthe reparameterized sentence space by varying the\ndimension of z, d′, to be 192, 384, 576, and 768.\nInit Location Ensembling EM PM BLEU EM-max PM-max BLEU-max\nI L2 All None 98.1 98.4 98.1 100.0 100.0 100.0\nXavier All None 99.0 99.0 98.9 100.0 100.0 100.0\nII\nXavier Embed None 44.8 44.9 44.6 72.3 72.2 71.9\nXavier +Layers None 98.8 98.8 98.8 100.0 100.0 100.0\nXavier Head None 4.1 3.8 3.3 4.1 3.8 3.3\nXavier All None 99.0 99.0 98.9 100.0 100.0 100.0\nIII\nXavier All Attention (k=2) 82.8 82.2 83.0 97.3 97.3 97.3\nXavier All Attention (k=4) 49.4 49.0 49.5 79.2 79.0 79.9\nXavier All Interleave (k=2) 69.3 68.0 69.7 82.2 81.3 82.6\nXavier All Interleave (k=4) 65.4 65.0 65.4 89.2 89.1 89.2\nXavier All None 99.0 99.0 98.9 100.0 100.0 100.0\nTable 1: Recoverability results for Phase I on SRC\n5 Results & Analysis\nRecoverability on SRC Experiment I indicates\nthat initialization strategy does not affect perfor-\nmance signiﬁcantly, but xavier normal performs\nbetter than l2 normalization. Injection location, on\nthe other hand, has a tremendous effect on perfor-\nmance. Injecting z at the language modeling head\nalone leads to poor performance as the ﬁnal fully\nconnected layer is severely bottlenecked in terms\nof capacity (Yang et al., 2018), but injection into\nthe embedding alone allows the transformer model\nto work with zand learn from it — leading to a 10x\nimprovement over just the lm head. Above all of\nthis, injecting into the transformer model at every\nlayer including the embedding virtually solves the\ntask, achieving nearly perfect recoverability across\nthe board. We theorize that this is due to the model\ncontinuously seeing z at each layer, which make\noptimization easier and more stable. We ﬁnd that\nadditionally injecting into the head leads to a slight\nincrease in recovery, so we inject z at all three\nplaces for all of the following experiments.\nRepresentation injection mechanisms also have\na large impact on recovery: both attention-based\nand interleaved experts perform signiﬁcantly worse\nthan no experts. These methods suffer from the\nfact that splitting z into ksmaller vectors reduces\ncapacity and makes retaining information more dif-\nﬁcult. See Table 1 for details. We ﬁnd that regard-\nless of experimental criteria, all six metrics are\nextremely consistent and correlate nearly perfectly\nto one another. As a result, we only report BLEU\nscore means for the remainder of experiments.\nIntrinsic Dimension via Recoverability:In ex-\nperiment IV , we estimate the intrinsic dimension\nof Z. We observe that BLEU increases as d′ in-\ncreases until d′= 768, where BLEU is nearly per-\nfect — hinting that the intrinsic dimension of Zis\napproximately 768. However, a lower-dimensional\nrepresentation can recover most sentences, drop-\nping off as sentence length increases, see Figure 2.\nThis is well-known; the number of bits needed to\nencode a sequence grows linearly with its length.\nWe observe low variances in our estimations, espe-\ncially as d′increases, indicating that the differences\nin BLEU for different values of d′are statistically\nsigniﬁcant.\nFigure 2: Plot of sentence length vs. BLEU score on\nLRC for experiment IV with error regions of ±σ.\nFigure 3: Cumulative explained variance plot under\nPCA with on LRC with number of components equal\nto d′ = 768.\nIntrinsic Dimension via PCA:We pick the best\nperforming z under BLEU-max for each sentence\nfrom experiment IV with d′= 768 and apply PCA\nto retain 768 components (ncomp). We observe that\nboth intrinsic dimension experiments via PCA and\nvia recoverability show similar patterns. The shape\nof the curve in Figure 3, hints that Zdoes not lie\non a lower-dimensional linear manifold and that\nits intrinsic dimensionality is approximately 768.\nncomp ≈600 explains almost 95% of the data’s\nvariance, which supports our observations from ex-\nperiment IV that shows d′= 576 achieving nearly\nperfect BLEU (Figure 2).\nRecoverability on GRC: We run the intrinsic di-\nmension experiment on the gibberish dataset (GRC)\nand ﬁnd that performance on the real dataset ex-\nceeds that on the gibberish dataset for all dimen-\nsions. This hints at the fact that although our rep-\nresentations memorize, they also leverage the lan-\nguage model. Even though BLEU for d′= 576 and\nd′= 768 for GRC seem high, the error on GRC is\n5x that of LRC (Figure 4).\nFigure 4: BLEU performance on LRC versus GRC for\ndifferent dimensionalities of z.\nInterpolation: In Figure 6, we show linear in-\nterpolations of two pairs of z’s that recover sen-\ntences exactly. The space is smooth with well-\nformed grammatical sentences occupying areas\nwith λ = [0 .3,0.6]. Our learned representations\nseem to have some synonym awareness: ”tale”\ntransforms to ”story” in the ﬁrst sentence pair and\n”long” transforms to ”long-running” when referring\nto a war. In the second sentence pair, we observe\nsome notion of syntactical awareness: at the 0.7\nmixture level the syntax of the ﬁrst sentence is re-\ntained with mostly words from the second sentence.\nLastly, for each individual sentence there exists a d\ndimensional volume that is fairly large. This could\nindicate that nearly all sentences have some rep-\nresentative volume from which, if any vector was\nsampled, sentence recovery could generate that sen-\ntence exactly.\nFigure 5: BLEU performance on LRC stratiﬁed by\ngenre for different dimensionalities of z.\nTowards a Universal Decoder: We can dis-\ncover representations, which exactly recover target\nsentences of interest in a low-dimensional space\nusing Adam. Other work found this impossible\nwith BLEU < 1 even for short sentences with\nless than 10 words, when applying an analogous\ntechnique on LSTM-based language models (Sub-\nramani et al., 2019). For sentences up to 100 words,\nwe discover representations which achieve over\n98 BLEU, generalizing to text from a variety of\ngenres (Figure 5). Our representations do not sim-\nply memorize, but actually leverage the ﬁxed lan-\nguage model, leading to representations with some\ninterpretability. Lastly, interpolation experiments\nshow that our reparametrized space has some syn-\nonym and syntactical awareness, while maintaining\na strong prior for sentences to be mostly grammat-\nically correct even in regions near the midpoint\nbetween two sentences. As a result, our formula-\ntion and representation space analysis hints at the\nfact that unconditional language models have the\npotential to be used as universal decoders and that\ndesigning an encoder to learn these types of repre-\nsentations may be possible.\nFigure 6: Two linear interpolations between perfectly recovered pairs of representations. Pink indicates token\noverlap to the ﬁrst sentence, while blue indicates token overlap to the second sentence.\n6 Related Work\nGeneral-purpose Decoders Large pretrained\nlanguage models are used for extracting mean-\ningful task-speciﬁc representations for different\nNatural language processing tasks. (Gulcehre\net al., 2015; Zoph et al., 2016; Sriram et al., 2018;\nNogueira and Cho, 2019). Other methods pre-\ntrain sequence-to-sequence decoders for tasks such\nas abstractive summarization and neural machine\ntranslation (Edunov et al., 2019; Song et al., 2019;\nChan et al., 2019). None of these methods analyze\nsentence representations or evaluate the difﬁculty\nin discovering such representations.\nLatent Space of ModelsOur notion of sentence\nspace resembles work on generative latent opti-\nmization because we also perform inference on a\nimplicit latent variable z, the sentence representa-\ntion, using a ﬁxed language model θ(Bojanowski\net al., 2018). Using ideas about difﬁculty of latent\nvariable optimization and interpolation from prior\nwork on latent variable language models based on\nvariational autoencoders (Bowman et al., 2016),\ndenoising autoencoders (Lewis et al., 2019), gen-\nerative adversarial networks (Yu et al., 2017), and\nplug-and-play models for image and text genera-\ntion (Nguyen et al., 2017; Dathathri et al., 2019),\nwe develop our notion of the reparametrized sen-\ntence space Zand analyses that follow. We focus\non analyzing the sentence space of a ﬁxed pre-\ntrained unconditional language model rather than\ntraining or ﬁne-tuning.\nAnalysis of Language Models Many works fo-\ncus on probing language models to understand\nwhat they know: evaluating their performance on\nquestion-answering or ﬁll-in-the-blank tasks or\nevaluating how well they transfer these kinds of\ntasks (Donahue et al., 2020; Tamkin et al., 2020;\nHu et al., 2020; ”Gururangan et al., 2020). We fo-\ncus on understanding how these models represent\nsentences, the complexity of that representation,\nand how easily discoverable those representations\nare. The goal of identifying complexity of a sen-\ntence representation resembles work that analyzes\ncontinuous bag-of-words representations with low-\nrank subspaces (Mu et al., 2017). Subramanian\net al. (2018) learn latent representations based on\ngeneral-purpose encoders for neural outlines and\nconclude that these outlines are informative for\ngeneration. We focus on a different and more basic\nquestion, whether a pretrained language model has\nthe potential to be used as a universal decoder.\nRecently, there has been work on investigating\nwhether LSTM-based language models have sen-\ntence representations from which they can recover\nthe original sentence (Subramani et al., 2019). This\nwork is the closest to ours. We extend their work to\ntransformer-based language models and improve\nupon their reparametrization leading to representa-\ntions which are 5x smaller that still achieve nearly\nperfect recovery across a much greater variety of\ngenres. Furthermore, we show that our represen-\ntations are easily discoverable using simple opti-\nmization rather than needing to use specialized\nconjugate gradient methods.\n7 Conclusion\nTo evaluate whether unconditional language mod-\nels have the potential to be used as universal\ndecoders without ﬁne-tuning, we introduce a\nreparametrized sentence space Z. In this space,\na sentence is represented as a low-dimensional vec-\ntor z, which we use to condition a language model,\nwhich is optimized to generate that sentence dur-\ning decoding. We present two methods, sentence\nencoding and sentence recovery, which allow us to\nmap a sentence to and from Z. Using these proce-\ndures, we evaluate whether we can discover repre-\nsentations that recover a sentence nearly perfectly.\nFurther, we measure the intrinsic dimension of Z\nunder the lenses of recoverability and PCA.\nWe observe that such representations are easily\ndiscoverable with simple stochastic optimization,\nunlike prior work, even while varying genres of\ntext. We ﬁnd that recoverability increases with the\ndimension of the reparametrized sentence space,\nreaching nearly perfect performance when equal\nto the latent dimension of the model. Experiment\nIV shows that sentence length and recoverability\nare inversely related. Analysis using PCA indicates\nthat Zdoes not lie on a lower-dimensional linear\nmanifold and conﬁrms that the intrinsic dimension\nof Zis close to the latent dimension dof the lan-\nguage model. Our estimates for intrinsic dimension\nare upper-bounds, while the associated recoverabil-\nities are lower-bounds due to the non-convexity of\nthe objective function, the stochasticity of the sen-\ntence encoding step, and the approximate nature of\ngreedy decoding.\nOur sentence representation formulation has\nmany useful properties: nearly perfect recoverabil-\nity, smoothness in the representation space, and\neasy representation recovery (simple optimization)\n— indicating the potential for GPT-2 to be used as a\nuniversal decoder. As a result, a next step could be\nto design an encoder which would learn mappings\nfrom its task-speciﬁc input representation space to\nour reparametrized sentence space. Another avenue\nfor future work could be adapting this approach to\nwork on more transformer-based language models.\nHaving a universal decoder could result in\ntremendous progress for low-resource sequence\ngeneration tasks from both a data and memory\nperspective. Translation tasks such as Kurdish to\nEnglish are an ideal use case because they have\nlittle parallel data, but have a target language\n(English) with abundant monolingual data. Our\nreparametrized sentence space formulation and the\npotential of using an unconditional language model\nas a universal decoder may drive progress in build-\ning more generalizable systems with large-scale\nlanguage models. These models may encode and\namplify some unwanted biases present in both the\ndata sources and the organizations building them.\nMany language models are used in commercial\nNLP applications without much concern for bias\nmitigation, but our approach could be modiﬁed to\nattempt to mitigate some of these biases. As with se-\nquence generation models broadly, there are always\nsigniﬁcant risks of this research aiding misinforma-\ntion spread. Our work indicates that well-trained\nlarge language models have a sentence representa-\ntion for any well-formed target sentence, so mali-\ncious attackers could build harmful sequence gen-\neration systems in news headline summarization\nand dialog to name a few.\nAcknowledgments\nWe gratefully acknowledge Ty Wilkins for many of\nthe visualizations and plots in this paper. We thank\nmembers of the Scale AI machine learning team\nand members of the Arrive engineering team for\nfeedback on iterations of this work.\nReferences\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers) , pages\n3874–3884, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\n2016. Layer normalization. ArXiv, abs/1607.06450.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In ICLR.\nPiotr Bojanowski, Armand Joulin, David Lopez-Pas,\nand Arthur Szlam. 2018. Optimizing the latent\nspace of generative networks. In ICML.\nSamuel R Bowman, Luke Vilnis, Oriol Vinyals, An-\ndrew M Dai, Rafal Jozefowicz, and Samy Bengio.\n2016. Generating sentences from a continuous\nspace. CoNLL 2016.\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\nStern, and Jakob Uszkoreit. 2019. Kermit: Gener-\native insertion-based modeling for sequences. arXiv\npreprint arXiv:1906.01604.\nBoxing Chen and Colin Cherry. 2014. A systematic\ncomparison of smoothing techniques for sentence-\nlevel bleu. In Proceedings of the Ninth Workshop\non Statistical Machine Translation.\nCristian Danescu-Niculescu-Mizil and Lillian Lee.\n2011. Chameleons in imagined conversations: A\nnew approach to understanding coordination of lin-\nguistic style in dialogs. In CMCL@ACL.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language models:\nA simple approach to controlled text generation. In\nICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In ACL.\nChris Donahue, Mina Lee, and Percy Liang. 2020. En-\nabling language models to ﬁll in the blanks. arXiv\npreprint arXiv:2005.05339.\nSergey Edunov, Alexei Baevski, and Michael Auli.\n2019. Pre-trained language model representa-\ntions for language generation. arXiv preprint\narXiv:1903.09722.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki\nMaeda. 2003. English gigaword. Linguistic Data\nConsortium, Philadelphia.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On us-\ning monolingual corpora in neural machine transla-\ntion. arXiv preprint arXiv:1503.03535.\nSuchin ”Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A.” Smith. 2020. ”don’t stop pretraining:\nAdapt language models to domains and tasks”. In\nACL.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger P Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. arXiv preprint arXiv:2005.03692.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nMarie Lebert. 2008. Project gutenberg (1971-2008).\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. ArXiv, abs/1609.07843.\nJiaqi Mu, Suma Bhat, and Pramod Viswanath. 2017.\nRepresenting sentences as low-rank subspaces. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 629–634, Vancouver, Canada.\nAssociation for Computational Linguistics.\nRamesh Nallapati, Bowen Zhou, C. D. Santos, aglar\nG¨ulehre, and B. Xiang. 2016. Abstractive text sum-\nmarization using sequence-to-sequence rnns and be-\nyond. In CoNLL.\nA Nguyen, J Clune, Y Bengio, A Dosovitskiy, and\nJ Yosinski. 2017. Plug & play generative networks:\nConditional iterative generation of images in latent\nspace. In CVPR.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Pas-\nsage re-ranking with bert. arXiv preprint\narXiv:1901.04085.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In ACL.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nMatt Post. 2018. A call for clarity in reporting bleu\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. In ICML.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2018. Cold fusion: Training seq2seq\nmodels together with language models. In Inter-\nspeech.\nNishant Subramani, Samuel Bowman, and Kyunghyun\nCho. 2019. Can unconditional language models re-\ncover arbitrary sentences? In NeurIPS.\nSandeep Subramanian, Sai Rajeswar, Alessandro Sor-\ndoni, Adam Trischler, Aaron C. Courville, and\nC. Pal. 2018. Towards text generation with adver-\nsarially learned neural outlines. In NeurIPS.\nAlex Tamkin, Trisha Singh, Davide Giovanardi, and\nNoah Goodman. 2020. Investigating transferabil-\nity in pretrained language models. arXiv preprint\narXiv:2004.14975.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. In ICLR.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In AAAI.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In EMNLP.\nA Intrinsic Dimensionality Results\nWe have included a table with the recoverabitility\nmetrics for experiment IV , measuring intrinsic di-\nmension via recoverability, from the original paper,\non LRC (the large recoverability corpus). The plot\nin the original paper is consistent with the results\nin Table 2. Recoverability performances are high-\nest when the intrinsic dimension is close to the\nmodel’s hidden dimension,d(768). In ﬁgure 7 and\n8 we visualize EM and PM performance scores\nfor different intrinsic dimension d′ for different\nsentence lengths. The two plots are very similar\nto the BLEU vs Sentence length plot we have\nprovided in the Results section of the paper. Perfor-\nmance metrics for each corpus indicate that aver-\nage recoverability over sentences is highest for the\nMovie dataset. This is also consistent with BLEU\nby genre results we observed in the paper.\nFigure 7: Plot of sentence length vs. EM score on LRC\nfor experiment IV with error regions of ±σ.\nFigure 8: Plot of sentence length vs. PM score on LRC\nfor experiment IV with error regions of ±σ.\nB Interpolation\nWe have provided some more examples of interpo-\nlation of sentence representations. In Figure 9, we\nshow another two sentence pairs. On the left, we\nsee the same trends as we saw before with well-\nformed, grammatical sentences occupying every\nlevel of the interpolation. We observe a mixing of\nthe two sentences with lambda equaling 0.5. One\ninteresting ﬁnding is that the model outputs ”Pa-\nciﬁc theater,” a very speciﬁc historical term used\nto describe World War II in the Paciﬁc Ocean, and\nuses it correctly. In the second sentence pair in Fig-\nure 9, we observe more synonym awareness, but\nalso observe further evidence of the nonlinearity of\nthe sentence representation as the word ”Iroquois”\nis forgotten when lambda equals 0.7 and 0.8. Fig-\nure 10 shows a long sentence’s representation being\nencoded when lambda equals 0.6 that is thematic\nand ﬂuent. Figure 11, however, hints at the nonlin-\nearity of the space, generating gibberish at the end\nwith B-B-B-B repeated 24 times.\nDataset Dimension EM PM BLEU EM-max PM-max BLEU-max\nComplete 192 35.10 34.71 35.33 45.11 44.25 45.12\n384 86.33 86.20 86.71 93.90 93.81 94.25\n576 96.19 96.10 96.58 98.50 98.44 98.87\n768 97.99 97.96 98.37 99.32 99.32 99.68\nBooks 192 34.77 34.25 34.86 44.92 43.88 44.70\n384 85.28 85.14 85.40 92.41 92.28 92.47\n576 96.02 95.83 96.09 98.35 98.12 98.43\n768 97.91 97.90 98.01 99.51 99.50 99.59\nNews 192 29.52 29.28 30.14 37.17 36.51 37.69\n384 85.87 85.76 86.94 94.16 94.10 95.25\n576 96.25 96.18 97.33 98.01 98.01 99.10\n768 97.38 97.35 98.42 98.20 98.20 99.30\nWiki 192 34.37 33.91 34.36 44.78 43.75 44.49\n384 84.71 84.61 84.76 92.14 92.00 92.12\n576 95.06 94.99 95.15 98.27 98.25 98.28\n768 98.07 98.02 98.14 100.00 100.00 100.00\nMovies 192 41.73 41.41 41.95 53.57 52.85 53.59\n384 89.45 89.29 89.76 96.89 96.84 97.16\n576 97.43 97.38 97.75 99.38 99.37 99.65\n768 98.60 98.59 98.91 99.57 99.57 99.84\nTable 2: Recoverability results for Phase II on LRC\nFigure 9: Linear interpolations between perfectly recovered pairs of representations. Pink indicates token overlap\nto the ﬁrst sentence, while blue indicates token overlap to the second sentence.\nFigure 10: Another linear interpolation: pink indicates token overlap to the ﬁrst sentence, while blue indicates\ntoken overlap to the second sentence.\nFigure 11: Final linear interpolation: pink indicates token overlap to the ﬁrst sentence, while blue indicates token\noverlap to the second sentence.",
  "topic": "Sentence",
  "concepts": [
    {
      "name": "Sentence",
      "score": 0.701802134513855
    },
    {
      "name": "Computer science",
      "score": 0.6767790913581848
    },
    {
      "name": "Natural language processing",
      "score": 0.6726272106170654
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5591663718223572
    },
    {
      "name": "Linguistics",
      "score": 0.4566982090473175
    },
    {
      "name": "Language model",
      "score": 0.4237821698188782
    },
    {
      "name": "Philosophy",
      "score": 0.059232503175735474
    }
  ]
}