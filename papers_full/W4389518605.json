{
  "title": "Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios",
  "url": "https://openalex.org/W4389518605",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2224021027",
      "name": "Yilun Zhao",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A1559100014",
      "name": "Haowei Zhang",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2106265336",
      "name": "Si Shengyun",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2901790570",
      "name": "Linyong Nan",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2634255921",
      "name": "Xiangru Tang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A1983754593",
      "name": "Arman Cohan",
      "affiliations": [
        "Yale University",
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2612228435",
    "https://openalex.org/W4384816576",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3194951844",
    "https://openalex.org/W4379086931",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3184222203",
    "https://openalex.org/W4210451781",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4386566859",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4366330700",
    "https://openalex.org/W3034556525",
    "https://openalex.org/W3170806096",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4294753225",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W4308759654",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4380559178",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4389519538",
    "https://openalex.org/W4297435087",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W4389523675",
    "https://openalex.org/W4327993149",
    "https://openalex.org/W4385570935",
    "https://openalex.org/W4385573499",
    "https://openalex.org/W3035275890",
    "https://openalex.org/W4385564880",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4285045050",
    "https://openalex.org/W3098495697",
    "https://openalex.org/W4362679631",
    "https://openalex.org/W2751448157",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W4385573202",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4318908878",
    "https://openalex.org/W4386566488",
    "https://openalex.org/W4281475956",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385573588"
  ],
  "abstract": "Tabular data is prevalent across various industries, necessitating significant time and effort for users to understand and manipulate for their information-seeking purposes. The advancements in large language models (LLMs) have shown enormous potential to improve user efficiency. However, the adoption of LLMs in real-world applications for table information seeking remains underexplored. In this paper, we investigate the table-to-text capabilities of different LLMs using four datasets within two real-world information seeking scenarios. These include the LogicNLG and our newly-constructed LoTNLG datasets for data insight generation, along with the FeTaQA and our newly-constructed F2WTQ datasets for query-based generation. We structure our investigation around three research questions, evaluating the performance of LLMs in table-to-text generation, automated evaluation, and feedback generation, respectively. Experimental results indicate that the current high-performing LLM, specifically GPT-4, can effectively serve as a table-to-text generator, evaluator, and feedback generator, facilitating users’ information seeking purposes in real-world scenarios. However, a significant performance gap still exists between other open-sourced LLMs (e.g., Vicuna and LLaMA-2) and GPT-4 models. Our data and code are publicly available at https://github.com/yale-nlp/LLM-T2T.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 160–175\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nInvestigating Table-to-Text Generation Capabilities of LLMs in\nReal-World Information Seeking Scenarios\nYilun Zhao∗ 1 Haowei Zhang∗ 2 Shengyun Si∗ 2\nLinyong Nan1 Xiangru Tang1 Arman Cohan1,3\n1Yale University,2Technical University of Munich, 3Allen Institute for AI\nyilun.zhao@yale.edu {haowei.zhang, shengyun.si}@tum.de\nAbstract\nTabular data is prevalent across various indus-\ntries, necessitating significant time and effort\nfor users to understand and manipulate for their\ninformation-seeking purposes. The advance-\nments in large language models (LLMs) have\nshown enormous potential to improve user ef-\nficiency. However, the adoption of LLMs in\nreal-world applications for table information\nseeking remains underexplored. In this pa-\nper, we investigate the table-to-text capabilities\nof different LLMs using four datasets within\ntwo real-world information seeking scenarios.\nThese include the LOGIC NLG and our newly-\nconstructed LOTNLG datasets for data insight\ngeneration, along with the FeTaQA and our\nnewly-constructed F2WTQ datasets for query-\nbased generation. We structure our investiga-\ntion around three research questions, evaluating\nthe performance of LLMs in table-to-text gen-\neration, automated evaluation, and feedback\ngeneration, respectively. Experimental results\nindicate that the current high-performing LLM,\nspecifically GPT-4, can effectively serve as a\ntable-to-text generator, evaluator, and feedback\ngenerator, facilitating users’ information seek-\ning purposes in real-world scenarios. How-\never, a significant performance gap still ex-\nists between other open-sourced LLMs (e.g.,\nTÜLU and LLaMA-2) and GPT-4 models. Our\ndata and code are publicly available at https:\n//github.com/yale-nlp/LLM-T2T.\n1 Introduction\nIn an era where users interact with vast amounts\nof structured data every day for decision-making\nand information-seeking purposes, the need for in-\ntuitive, user-friendly interpretations has become\nparamount (Zhang et al., 2023; Zha et al., 2023;\nLi et al., 2023). Given this emerging necessity,\ntable-to-text generation techniques, which trans-\nform complex tabular data into comprehensible nar-\nratives tailored to users’ information needs, have\n∗Equal Contributions.\nRQ1: How do LLMs perform in table-to-text generation tasks?\nRQ2: Can we use LLMs to assess factual consistency of table-to-text generation?\nRQ3: How can fine-tuned models benefit from LLMs' strong table-to-text abilities?\nLLM\nLLM\nLLM serve as feedback provider\nInformation Seeking Scenario 1: Data Insight Generation\nInformation Seeking Scenario 2: Query-based Generation\nHere are some meaningful insights from the given table about the 1964 United States presidential election in Illinois:1. ....2. ....\nLyndon B. Johnson won Illinois with 59.47% of the vote, against Barry Goldwater, who received 40.53% of the vote.\nHow did Lyndon B. Johnson fare against his opponent in the Illinois presidential election? \nParty Candidates Votes Votes %\nDemocratic Lyndon B. Johnson 2,796,833 59.47%\nRepublican Barry Goldwater 1,905,946 40.53%\n(...abbreviation...)\nTitle: 1964 United States Presidential Election in Illinois\nFigure 1: The real-world table information seeking sce-\nnarios and research questions investigated in this paper.\ndrawn considerable attention (Parikh et al., 2020;\nChen et al., 2020a; Nan et al., 2022b; Zhao et al.,\n2023c). These techniques can be incorporated into\na broad range of applications, including but not lim-\nited to game strategy development, financial analy-\nsis, and human resources management. However,\nexisting fine-tuned table-to-text generation mod-\nels (Nan et al., 2022a; Liu et al., 2022b,a; Zhao\net al., 2023b) are typically task-specific, limiting\ntheir adaptability to real-world applications.\nThe emergence and remarkable achievements of\nLLMs (Brown et al., 2020; Scao et al., 2022; Wang\n160\nDataset # Table # Examples Control Signal Rich in Reasoning?\nData Insight Generation\nLOGIC NLG (Chen et al., 2020a) 862 4,305 None ✓\nLOTNLG (ours) 862 4,305 Reasoning type ✓\nQuery-based Generation\nFeTaQA (Parikh et al., 2020) 2,003 2,003 User query ✗\nF2WTQ (ours) 4,344 4,344 User query ✓\nTable 1: Experimental dataset statistics for the test set. Examples of our newly-constructed LOTNLG and F2WTQ\ndatasets are displayed in Figure 2 and 3, respectively.\net al., 2023; Scheurer et al., 2023; OpenAI, 2023;\nTouvron et al., 2023a; Taori et al., 2023; Touvron\net al., 2023b) have sparked a significant transfor-\nmation in the field of controllable text generation\nand data interpretations (Nan et al., 2021; Zhang\net al., 2022; Goyal et al., 2022; Köksal et al., 2023;\nGao et al., 2023b; Madaan et al., 2023; Zhou et al.,\n2023). As for table-based tasks, recent work (Chen,\n2023; Ye et al., 2023; Gemmell and Dalton, 2023)\nreveals that LLMs are capable of achieving compet-\nitive performance with state-of-the-art fine-tuned\nmodels on table question answering (Pasupat and\nLiang, 2015; Nan et al., 2022b) and table fact\nchecking (Chen et al., 2020b; Gupta et al., 2020).\nHowever, the potential of LLMs in generating text\nfrom tabular data for users’ information-seeking\npurposes remains largely underexplored.\nIn this paper, we investigate the table-to-text gen-\neration capabilities of LLMs in two real-world ta-\nble information seeking scenarios: 1) Data Insight\nGeneration (Chen et al., 2020a), where users aim\nto promptly derive significant facts from the table,\nanticipating the systems to offer several data in-\nsights; and 2) Query-based Generation (Pasupat\nand Liang, 2015; Nan et al., 2022b), where users\nconsult tables to answer specific questions. To facil-\nitate a rigorous evaluation of LLM performance, we\nalso construct two new benchmarks: LOTNLG for\ndata insight generation conditioned with specific\nlogical reasoning types; and F2WTQ for free-form\nquestion answering that requires models to perform\nhuman-like reasoning over Wikipedia tables.\nWe provide an overview of table information\nseeking scenarios and our main research questions\nin Figure 1, and enumerate our findings as follows:\nRQ1: How do LLMs perform in table-to-text gen-\neration tasks?\nFinding: LLMs exhibit significant potential in\ngenerating coherent and faithful natural language\nstatements based on the given table. For example,\nGPT-4 outperforms state-of-the-art fine-tuned\nmodels in terms of faithfulness during both au-\ntomated and human evaluations. The statements\ngenerated by GPT-3.5 and GPT-4 are also pre-\nferred by human evaluators. However, a signifi-\ncant performance gap still exists between other\nopen-sourced LLMs (e.g., Vicuna and LLaMA-\n2) and GPT-* models, especially on our newly-\nconstructed LOTNLG and F2WTQ datasets.\nRQ2: Can we use LLMs to assess factual consis-\ntency of table-to-text generation?\nFinding: LLMs using chain-of-thought prompt-\ning can serve as reference-free metrics for table-\nto-text generation evaluation. These metrics\ndemonstrate better alignment with human evalu-\nation in terms of both fluency and faithfulness.\nRQ3: How can fine-tuned models benefit from\nLLMs’ strong table-to-text abilities?\nFinding: LLMs that utilize chain-of-thought\nprompting can provide high-quality natural lan-\nguage feedback in terms of factuality, which in-\ncludes explanations, corrective instructions, and\nedited statements for the output of other models.\nThe edited statements are more factually consis-\ntent with the table compared to the initial ones.\n2 Table Information Seeking Scenarios\nTable 1 illustrates the data statistics for the four\ndatasets used in the experiments. We investigate\nthe performance of the LLM in the following two\nreal-world table information-seeking scenarios.\n2.1 Data Insight Generation\nData insight generation is an essential task that in-\nvolves generating meaningful and relevant insights\nfrom tables. By interpreting and explaining tabular\ndata in natural language, LLMs can play a crucial\n161\nrole in assisting users with information seeking and\ndecision making. This frees users from the need to\nmanually comb through vast amounts of data. We\nuse the following two datasets for evaluation.\n2.1.1 L OGIC NLG Dataset\nThe task of LOGIC NLG (Chen et al., 2020a) in-\nvolves generating five logically consistent sen-\ntences from a given table. It aims to uncover in-\ntriguing facts from the table by applying various\nlogical reasoning operations (e.g., count and com-\nparison) across different table regions.\n2.1.2 L OTNLG Dataset\nOur preliminary experiments revealed that when ap-\nplied to the LOGIC NLG dataset, table-to-text gen-\neration systems tend to generate multiple sentences\nthat employ the same logical reasoning operations.\nFor instance, in a 0-shot setting, the GPT-3.5 model\nis more inclined to generate sentences involving nu-\nmerical comparisons, while overlooking other com-\npelling facts within tables. This lack of diversity\nin data insight generation poses a significant limi-\ntation because, in real-world information-seeking\nscenarios, users typically expect systems to offer\na variety of perspectives on the tabular data. To\naddress this issue, application developers could tai-\nlor the table-to-text generation systems to generate\nmultiple insights that encompass different logical\nreasoning operations (Perlitz et al., 2022; Zhao\net al., 2023b). In order to foster a more rigorous\nevaluation of LLMs’ abilities to utilize a broader\nrange of logical reasoning operations while gen-\nerating insights from tables, we have developed a\nnew dataset, LOTNLG , for logical reasoning type-\nconditioned table-to-text generation. In this setup,\nthe model is tasked with generating a statement by\nperforming the logical reasoning operations of the\nspecified types on the tables.\nLOTNLG Dataset Construction Following\nChen et al. (2020b), we have predefined nine\ntypes of common logical reasoning operations (e.g.,\ncount, comparative, and superlative), with detailed\ndefinitions provided in Appendix A.1. We use ex-\namples from the LOGIC NLG test set to construct\nLOTNLG . Specifically, for each statement from\nLOGIC NLG, we assign two annotators to indepen-\ndently label the set of logical reasoning types used\nin that statement, ensuring that no more than two\ntypes were identified per statement. If there are\ndiscrepancies in the labels, an expert annotator is\nFigure 2: An example of LOTNLG , where models are\nrequired to generate statements using the specified types\nof logical reasoning operations\nbrought in to make the final decision. The distri-\nbution of logical reasoning types in LOTNLG is\nillustrated in Figure 4 in Appendix A.1.\n2.2 Query-based Generation\nQuery-based table-to-text generation pertains to\nproducing detailed responses based on specific user\nqueries in the context of a given table. The abil-\nity to answer users’ queries accurately, coherently,\nand in a context-appropriate manner is crucial for\nLLMs in many real-world applications, such as\ncustomer data support and personal digital assis-\ntants. We utilize following two datasets to evaluate\nLLMs’ efficiency in interacting with users and their\nproficiency in table understanding and reasoning.\n2.2.1 FeTaQA Dataset\nNan et al. (2022b) introduces a task of free-form\ntable question answering. This task involves retriev-\ning and aggregating information from Wikipedia\ntables, followed by generating coherent sentences\nbased on the aggregated contents.\n2.2.2 F2WTQ Dataset\nQueries in the FeTaQA dataset typically focus on\nsurface-level facts (e.g., \"Which country hosted the\n2014 FIFA World Cup?\"). However, in real-world\ninformation-seeking scenarios, users are likely to\nconsult tables for more complex questions, which\nrequire models to perform human-like reasoning\nover tabular data. Therefore, we have constructed\na new benchmark, named F2WTQ, for more chal-\nlenging, free-form table question answering tasks.\n162\nThe player got his first 1st position for the 400m event in European Indoor Championships in 2002. \nIn which competition did the player secure his first 1st position for the 400m event?\nFigure 3: An example of F2WTQ, where models need\nto perform human-like reasoning to generate response.\nF2WTQ Dataset Construction We adopt the\nWTQ dataset (Pasupat and Liang, 2015) as a ba-\nsis to construct F2WTQ. The WTQ dataset is\na short-form table question answering dataset,\nwhich includes human-annotated questions based\non Wikipedia tables and requires complex reason-\ning. However, we do not directly use WTQ for\nLLM evaluation because, in real-world scenarios,\nusers typically prefer a natural language response\nover a few words. In the development of F2WTQ,\nfor each QA pair in the WTQ test set, we assign an\nannotator who assumes the role of an agent that ana-\nlyzes the table and provides an expanded, sentence-\nlong response. We found that the original questions\nin the WTQ dataset occasionally contained gram-\nmatical errors or lacked a natural linguistic flow. In\nthese cases, the annotators are required to rewrite\nthe question to ensure it was fluent and natural.\n3 Evaluation System\n3.1 Automated Evaluation\nWe adopt following popular evaluation metrics for\nautomated evaluation:\n• BLEU (Papineni et al., 2002) uses a precision-\nbased approach, measuring the n-gram matches\nbetween the generated and reference statements.\n• ROUGE (Lin, 2004) uses a recall-based ap-\nproach, and measures the percentage of overlap-\nping words and phrases between the generated\noutput and reference one.\n• SP-Acc (Chen et al., 2020a) extracts the meaning\nrepresentation from the generated sentence and\nexecutes it against the table to verify correctness.\n• NLI-Acc (Chen et al., 2020a) uses TableBERT\nfine-tuned on the TabFact dataset (Chen et al.,\n2020b) as faithfulness classifier.\n• TAPAS-Acc (Liu et al., 2022a) uses\nTAPAS (Herzig et al., 2020) fine-tuned on\nthe TabFact dataset as the backbone.\n• TAPEX-Acc (Liu et al., 2022a) employs\nTAPEX (Liu et al., 2022b) fine-tuned on the Tab-\nFact dataset as the backbone. Recent works (Liu\net al., 2022a; Zhao et al., 2023b) have revealed\nthat NLI-Acc and TAPAS-Acc is overly positive\nabout the predictions, while TAPEX-Acc serves\nas a more reliable faithfulness-level metric.\n• Exact Match & F-Score for Logical Reason-\ning Type For LOTNLG evaluation, the exact\nmatch measures the percentage of samples with\nall the labels classified correctly, while the F-\nScore provides a balanced metric that considers\nboth type I and type II errors.\n• Answer Accuracy refers to the proportion of\ncorrect predictions out of the total number of\npredictions in F2WTQ generation.\n3.2 Human Evaluation\nTo gain a more comprehensive understanding of\nthe system’s performance, we also conduct human\nevaluation. Specifically, the generated statements\nfrom different models are evaluated by humans\nbased on two criteria: faithfulness and fluency. For\nfaithfulness, each sentence is scored 0 (refuted)\nor 1 (entailed). For fluency, scores range from 1\n(worst) to 5 (best). We average the scores across\ndifferent human evaluators for each criterion. We\ndo not apply more fine-grained scoring scales for\nfaithfulness-level evaluation, as each statement in\nLOGIC NLG consists of only a single sentence.\n4 Experiments\nIn the following subsections, we discuss the three\nkey research questions about adopting LLMs into\nreal-world table information seeking scenarios.\nSpecifically, we explore LLMs’ capabilities for\ntable-to-text generation tasks, their ability to assess\nfactual consistency, and whether they can benefit\nsmaller fine-tuned models. The examined systems\nfor each experiment are discussed in Appendix B.\n163\nType Models SP-Acc NLI-Acc TAPAS-Acc TAPEX-Acc\nFine-tuned\nGPT2-C2F 43.6 71.4 46.2 43.8\nR2D2 53.2 86.2 60.2 61.0\nPLOG 52.8 84.2 63.8 69.6\nLOFT 53.8 86.6 67.4 61.4\n0-shot* GPT-3.5 54.2 87.6 81.6 79.4\nGPT-4 43.2 90.4 91.8 91.0\n1-shot Direct GPT-3.5 60.2 79.0 80.4 79.2\nGPT-4 57.6 82.0 87.6 88.0\n1-shot CoT GPT-3.5 51.6 70.0 81.8 78.2\nGPT-4 59.8 80.8 89.4 90.8\n2-shot Direct\nPythia-12b 39.4 53.2 39.4 40.4\nLLaMA-13b 47.2 58.4 47.0 43.2\nLLaMA-7b 38.6 63.4 45.8 43.6\nLLaMA2-70b-chat 56.0 52.4 54.6 52.4\nLLaMA-30b 45.4 55.8 53.8 53.0\nAlpaca-13b 44.0 70.6 58.0 54.6\nLLaMA-65b 52.2 57.2 58.4 56.8\nTÜLU -13b 44.4 68.4 63.4 59.6\nVicuna-13b 51.8 71.4 66.2 65.2\nGPT-3.5 64.0 78.4 78.8 81.2\nGPT-4 55.4 85.8 92.0 89.6\n2-shot CoT\nPythia-12b 41.8 54.0 41.2 42.8\nLLaMA-7b 38.0 63.2 48.0 43.0\nLLaMA-13b 44.2 53.2 49.2 48.6\nLLaMA-30b 45.0 56.6 60.8 54.2\nLLaMA-65b 48.0 58.8 57.4 57.4\nTÜLU -13b 46.0 69.8 61.6 58.8\nVicuna-13b 44.6 70.8 63.0 61.6\nAlpaca-13b 45.4 68.2 64.0 64.0\nLLaMA2-70b-chat 52.6 66.8 69.4 69.2\nGPT-3.5 60.4 70.2 84.0 83.4\nGPT-4 62.2 76.8 88.8 90.4\nTable 2: Faithfulness-level automated evaluation results on the LOGIC NLG dataset. Within each experimental\nsetting, we used TAPEX-Acc as the ranking indicator of model performance. ∗: It is challenging for other LLMs to\nfollow the instructions in 0-shot prompt to generate five statements for the input table.\n4.1 RQ1: How do LLMs perform in\ntable-to-text generation tasks?\nWe experiment with two in-context learning meth-\nods, Direct Prediction (Figure 5 in Appendix) and\nChain of Thoughts (CoT, Figure 6 in Appendix), to\nsolve the table-to-text generation tasks.\nData Insight Generation Results The results on\nthe LOGIC NLG dataset, as displayed in Table 2\nand Table 3, indicate that GPT-* models generally\nsurpass the current top-performing fine-tuned mod-\nels (i.e., LOFT and PLOG) even in a 0-shot setting.\nMeanwhile, LLaMA-based models (e.g., LLaMA,\nAlpaca, Vicuna, TÜLU ) manage to achieve com-\nparable performance to these top-performing fine-\ntuned models in a 2-shot setting. However, when it\ncomes to the more challenging LOTNLG dataset,\nthe automated evaluation result shows that only\nGPT-4 is capable of generating faithful statements\nthat adhere to the specified logical reasoning types\n(Table 6 in Appendix). Moreover, increasing the\nnumber of shots or applying chain-of-thought ap-\nproach does not always yield a performance gain,\nmotivating us to explore more advanced prompting\nmethods for data insight generation in future work.\nQuery-based Generation Results Table 7 and 8\nin Appendix display the automated evaluation re-\nsults for the FeTaQA and F2WTQ datasets, respec-\ntively. On FeTaQA, both LLaMA-based LLM and\nGPT-* models achieve comparable performance to\nthe current top-performing fine-tuned models in a\n2-shot setting, indicating the capability of LLMs\nto answer questions requiring surface-level facts\nfrom the table. However, a significant performance\ngap exists between other LLMs and GPT-* models\non the more challenging F2WTQ dataset. More-\nover, increasing the number of shots or applying\n164\nModel Fluency (1-5) Faithfulness (0-1)\nGPT2-C2F 3.85 0.54\nR2D2 4.29 0.72\nPLOG 4.23 0.77\nLOFT 4.42 0.81\nGPT-4 0-shot 4.82 0.90\nVicuna 2-shot Direct 4.69 0.71\nVicuna 2-shot CoT 4.65 0.73\nLLaMA2 2-shot Direct 4.75 0.79\nLLaMA2 2-shot CoT 4.70 0.83\nGPT-4 2-shot Direct 4.71 0.89\nGPT-4 2-shot CoT 4.77 0.92\nTable 3: Human evaluation results on LOGIC NLG.\nthe chain-of-thought approach can both yield per-\nformance gains for query-based generation.\n4.2 RQ2: Can we use LLMs to assess factual\nconsistency of table-to-text generation?\nIn RQ1, we demonstrate that LLMs can generate\nstatements with comparative or even greater fac-\ntual consistency than fine-tuned models. One natu-\nral follow-up question is whether we can employ\nLLMs to evaluate the faithfulness of table-to-text\ngeneration systems. This capability is crucial, as it\nensures that tabular data is accurately interpreted\nfor users, thereby preserving the credibility and\nreliability of real-world applications.\nAs discussed in Section 3.1, existing faithfulness-\nlevel NLI-based metrics are trained on the TabFact\ndataset (Chen et al., 2020b). Recent work (Chen,\n2023) has revealed that large language models us-\ning chain-of-thought prompting can achieve com-\npetitive results on TabFact. Motivated by this\nfinding, we use the same 2-shot chain-of-thought\nprompt (Figure 7 in Appendix) as Chen (2023) to\ngenerate factual consistency scores (0 for refuted\nand 1 for entailed) for output sentences from Log-\nicNLG. We use GPT-3.5 and GPT-4 as the back-\nbones, as they outperforms other LLMs in RQ1\nexperiments. We refer to these new metrics as CoT-\n3.5-Acc and CoT-4-Acc, respectively.\nCoT-Acc Metrics Achieve Better Correlation\nwith Human Judgement We leverage the hu-\nman evaluation results of models (excluding GPT-\n4 models) in RQ1 as the human judgement. We\nthen compare the system-level Pearson’s correla-\ntion between each evaluation metric and this hu-\nman judgement. As shown in Table 4, the proposed\nCoT-4-Acc and CoT-3.5-Acc metrics achieve the\nhighest and third highest correlation with human\njudgement, respectively. This result demonstrates\nMetric Acc on Tabfact Pearson’s correlation\nSP-Acc 63.5 .458\nNLI-Acc 65.1 .526\nTAPAS-Acc 81.0 .705\nTAPEX-Acc 84.2 .804\nCoT-3.5-Acc 78.0 .787\nCoT-4-Acc 80.9 .816\nTable 4: System-level Pearson’s correlation bettwen\neach automated evaluation metric and human judgement.\nWe also report the accuracy of automated evaluation\nmetrics on the TabFact dataset for reference.\nLLMs’ capabilities in assessing the faithfulness\nof table-to-text generation. It’s worth noting that\nalthough TAPAS-Acc and TAPEX-Acc perform\nbetter than CoT-4-Acc on the TabFact dataset, they\nexhibit lower correlation with human judgement on\ntable-to-text evaluation. We suspect that this can\nbe largely attributed to over-fitting on the TabFact\ndataset, where negative examples are created by\nrewriting from the positive examples. We believe\nthat future work can explore the development of\na more robust faithfulness-level metric with better\nalignment to human evaluation.\n4.3 RQ3: How can fine-tuned models benefit\nfrom LLMs’ strong table-to-text abilities?\nIn RQ1 and RQ2, we demonstrate the strong ca-\npability of state-of-the-art LLMs in table-to-text\ngeneration and evaluation. We next explore how\nfine-tuned smaller models can benefit from these\nabilities. We believe such exploration can provide\ninsights for future work regarding the distillation of\ntext generation capabilities from LLMs to smaller\nmodels (Gao et al., 2023a; Scheurer et al., 2023;\nMadaan et al., 2023). This is essential as deploying\nsmaller, yet performance-comparable models in\nreal-world applications could save computational\nresources and inference time.\nGenerating Feedback for Improving Factual\nConsistency Utilizing human feedback to en-\nhance neural models has emerged as a significant\narea of interest in contemporary research (Liu et al.,\n2022c; Gao et al., 2023a; Scheurer et al., 2023;\nMadaan et al., 2023). For example, Liu et al.\n(2022c) illustrates that human-written feedback\ncan be leveraged to improve factual consistency of\ntext summarization systems. Madaan et al. (2023)\ndemonstrates that LLMs can improve their initial\noutputs through iterative feedback and refinement.\nThis work investigates whether LLMs can provide\n165\nModels TAPAS-Acc TAPEX-Acc\nGPT2-C2F 46.2 43.8\nEdit by LLaMA2-70b-chat58.0 (+11.8) 50.0 (+6.2)\nEdit by GPT-3.5 71.0 (+24.8) 68.4 (+24.6)\nEdit by GPT-4 81.0 (+34.8) 82.0 (+38.2)\nR2D2 60.2 61.0\nEdit by LLaMA2-70b-chat65.0 (+4.8) 60.0 (-1.0)\nEdit by GPT-3.5 74.0 (+13.8) 74.0 (+13.0)\nEdit by GPT-4 87.0 (+26.8) 89.0 (+28.0)\nPLOG 63.8 69.6\nEdit by LLaMA2-70b-chat75.0 (+11.2) 66.0 (-3.6)\nEdit by GPT-3.5 70.6 (+6.8) 67.0 (-2.6)\nEdit by GPT-4 91.0 (+27.2) 86.0 (+16.4)\nLOFT 67.4 61.4\nEdit by LLaMA2-70b-chat72.0 (+4.6) 64.0 (+2.6)\nEdit by GPT-3.5 70.0 (+2.6) 65.6 (+4.2)\nEdit by GPT-4 81.0 (+13.6) 86.0 (+24.6)\nTable 5: Automated evaluation results on LOGIC NLG\nusing statements pre-edited and post-edited by LLMs.\nhuman-like feedback for outputs from fine-tuned\nmodels. Following Liu et al. (2022c), we consider\ngenerating feedback with three components: 1)\nExplanation, which determine whether the initial\nstatement is factually consistent with the given ta-\nble; 2) Corrective Instruction, which provide in-\nstructions on how to correct the initial statement if\nit is detected as unfaithful; and 3)Edited Statement,\nwhich edits the initial statement following the cor-\nrective instruction. Figure 8 in Appendix shows\nan example of 2-shot chain-of-thought prompts we\nuse for feedback generation.\nFeedback from LLMs is of High Quality We\nassess the quality of generated feedback through au-\ntomated evaluations. Specifically, we examine the\nfaithfulness scores of Edited Statements in the gen-\nerated feedback, comparing these scores to those of\nthe original statements. We report TAPAS-Acc and\nTAPEX-Acc for experimental results, as these two\nmetrics exhibit better alignment with human evalu-\nation (Section 4.2). As illustrated in Table 5, LLMs\ncan effectively edit statements to improve their\nfaithfulness, particularly for outputs from lower-\nperformance models, such as GPT2-C2F.\n5 Related Work\nTable-to-Text Generation Text generation from\nsemi-structured knowledge sources, such as web\ntables, has been studied extensively in recent\nyears (Parikh et al., 2020; Chen et al., 2020a; Cheng\net al., 2022; Zhao et al., 2023a). The goal of the\ntable-to-text generation task is to generate natural\nlanguage statements that faithfully describe infor-\nmation contained in the provided table region. The\nmost popular approach for table-to-text generation\ntasks is to fine-tune a pre-trained language model\non a task-specific dataset (Chen et al., 2020a; Liu\net al., 2022a; Zhao et al., 2022; Nan et al., 2022a;\nZhao et al., 2023b). To the best of our knowledge,\nwe are the first to systematically evaluate the perfor-\nmance of LLMs on table-to-text generation tasks.\nLarge Language Models LLMs have demon-\nstrated remarkable in-context learning capabili-\nties (Brown et al., 2020; Chowdhery et al., 2022;\nScao et al., 2022; Chung et al., 2022; OpenAI,\n2023), where the model receives a task demon-\nstration in natural language accompanied by a lim-\nited number of examples. The Chain-of-Thought\nprompting methods (Wei et al., 2022; Wang et al.,\n2022) further empower LLMs to perform com-\nplex reasoning tasks (Han et al., 2022; Zhao et al.,\n2023c; Ye et al., 2023; Chen, 2023). More recent\nworks (Chen, 2023; Nan et al., 2023) investigate\nin-context learning capabilities of LLMs on table-\nbased tasks, including table question answering (Pa-\nsupat and Liang, 2015; Iyyer et al., 2017; Zhong\net al., 2018) and table fact checking (Chen et al.,\n2020b; Gupta et al., 2020). However, the poten-\ntial of LLMs in generating text from tabular data\nremains underexplored.\n6 Conclusion\nThis paper investigates the potential of applying\nLLMs in real-world table information seeking sce-\nnarios. We demonstrate their superiority in faith-\nfulness, and their potential as evaluation systems.\nFurther, we provide valuable insights into lever-\naging LLMs to generate high-fidelity natural lan-\nguage feedback. We believe that the findings of this\nstudy could benefit real-world applications, aimed\nat improving user efficiency in data analysis.\nEthical Consideration\nLOTNLG and F2WTQ were constructed upon the\ntest set of LOGIC NLG (Chen et al., 2020a) and\nWTQ (Pasupat and Liang, 2015) datasets, which\nare publicly available under the licenses of MIT1\nand CC BY-SA 4.02, respectively. These licenses\npermit us to modify, publish, and distribute addi-\ntional annotations upon the original dataset.\n1https://opensource.org/licenses/MIT\n2https://creativecommons.org/licenses/\nby-sa/4.0/\n166\nReferences\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia:\nA suite for analyzing large language models across\ntraining and scaling.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, et al. 2020. Language models are\nfew-shot learners. In Advances in Neural Information\nProcessing Systems, volume 33, pages 1877–1901.\nCurran Associates, Inc.\nWenhu Chen. 2023. Large language models are few(1)-\nshot table reasoners. In Findings of the Associa-\ntion for Computational Linguistics: EACL 2023 ,\npages 1120–1130, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020a. Logical natural lan-\nguage generation from open-domain tables. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7929–\n7942, Online. Association for Computational Lin-\nguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020b. Tabfact: A large-scale\ndataset for table-based fact verification. In Interna-\ntional Conference on Learning Representations.\nZhoujun Cheng, Haoyu Dong, Zhiruo Wang, Ran Jia,\nJiaqi Guo, Yan Gao, Shi Han, Jian-Guang Lou, and\nDongmei Zhang. 2022. HiTab: A hierarchical table\ndataset for question answering and natural language\ngeneration. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1094–1110, Dublin,\nIreland. Association for Computational Linguistics.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, et al. 2022. Palm:\nScaling language modeling with pathways. ArXiv,\nabs/2204.02311.\nHyung Won Chung, Le Hou, S. Longpre, Barret Zoph,\nYi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, et al. 2022. Scaling instruction-finetuned lan-\nguage models. ArXiv, abs/2210.11416.\nGe Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi.\n2023a. Continually improving extractive qa via hu-\nman feedback.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\ning Yang, and Xiaojun Wan. 2023b. Human-like sum-\nmarization evaluation with chatgpt. arXiv preprint\narXiv:2304.02554.\nCarlos Gemmell and Jeffrey Stephen Dalton. 2023. Gen-\nerate, transform, answer: Question specific tool syn-\nthesis for tabular data. ArXiv, abs/2303.10138.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3. arXiv preprint arXiv:2209.12356.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020. INFOTABS: Inference on tables\nas semi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2309–2324, Online. Association\nfor Computational Linguistics.\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting\nQi, Martin Riddell, Luke Benson, Lucy Sun, Eka-\nterina Zubova, Yujie Qiao, Matthew Burtell, David\nPeng, Jonathan Fan, Yixin Liu, Brian Wong, Mal-\ncolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai,\nTao Yu, Rui Zhang, Shafiq R. Joty, Alexander R. Fab-\nbri, Wojciech Kryscinski, Xi Victoria Lin, Caiming\nXiong, and Dragomir R. Radev. 2022. Folio: Natu-\nral language reasoning with first-order logic. ArXiv,\nabs/2209.00840.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017.\nSearch-based neural structured learning for sequen-\ntial question answering. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1821–\n1831, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nZhengbao Jiang, Yi Mao, Pengcheng He, Graham Neu-\nbig, and Weizhu Chen. 2022. OmniTab: Pretraining\nwith natural and synthetic data for few-shot table-\nbased question answering. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 932–942, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nAbdullatif Köksal, Timo Schick, Anna Korhonen, and\nHinrich Schütze. 2023. Longform: Optimizing in-\nstruction tuning for long text generation with corpus\nextraction. arXiv preprint arXiv:2304.08460.\n167\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nHongxin Li, Jingran Su, Yuntao Chen, Qing Li, and\nZhaoxiang Zhang. 2023. Sheetcopilot: Bringing soft-\nware productivity to the next level through large lan-\nguage models. ArXiv, abs/2305.19308.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nAo Liu, Haoyu Dong, Naoaki Okazaki, Shi Han, and\nDongmei Zhang. 2022a. PLOG: Table-to-logic pre-\ntraining for logical table-to-text generation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 5531–\n5546, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022b.\nTAPEX: Table pre-training via learning a neural SQL\nexecutor. In International Conference on Learning\nRepresentations.\nYixin Liu, Budhaditya Deb, Milagro Teruel, Aaron L\nHalfaker, Dragomir R. Radev, and Ahmed Hassan\nAwadallah. 2022c. On improving summarization\nfactual consistency from natural language feedback.\nArXiv, abs/2212.09968.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\net al. 2023. Self-refine: Iterative refinement with\nself-feedback. arXiv preprint arXiv:2303.17651.\nLinyong Nan, Lorenzo Jaime Flores, Yilun Zhao, Yixin\nLiu, Luke Benson, Weijin Zou, and Dragomir Radev.\n2022a. R2D2: Robust data-to-text with replacement\ndetection. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6903–6917, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria\nLin, Neha Verma, Rui Zhang, Wojciech Kry´sci´nski,\nHailey Schoelkopf, Riley Kong, Xiangru Tang,\nMutethia Mutuma, Ben Rosand, Isabel Trindade,\nRenusree Bandaru, Jacob Cunningham, Caiming\nXiong, Dragomir Radev, and Dragomir Radev. 2022b.\nFeTaQA: Free-form table question answering. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:35–49.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xi-\nangru Tang, Aadit Vyas, Neha Verma, Pranav Kr-\nishna, Yangxiaokang Liu, Nadia Irwanto, Jessica\nPan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma,\nYasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan,\nXi Victoria Lin, Caiming Xiong, Richard Socher,\nand Nazneen Fatema Rajani. 2021. DART: Open-\ndomain structured data record to text generation. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 432–447, Online. Association for Computa-\ntional Linguistics.\nLinyong Nan, Yilun Zhao, Weijin Zou, Narutatsu\nRi, Jaesung Tae, Ellen Zhang, Arman Cohan, and\nDragomir Radev. 2023. Enhancing few-shot text-to-\nsql capabilities of large language models: A study on\nprompt design strategies.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nYotam Perlitz, Liat Ein-Dor, Dafna Sheinwald, Noam\nSlonim, and Michal Shmueli-Scheuer. 2022. Diver-\nsity enhanced table-to-text generation via type con-\ntrol. ArXiv, abs/2205.10938.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\n168\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nJ’er’emy Scheurer, Jon Ander Campos, Tomasz Kor-\nbak, Jun Shern Chan, Angelica Chen, Kyunghyun\nCho, and Ethan Perez. 2023. Training language\nmodels with language feedback at scale. ArXiv,\nabs/2303.16755.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Canton Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V .\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023b. Llama 2: Open foundation\nand fine-tuned chat models.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Huai hsin Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. ArXiv, abs/2203.11171.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hannaneh Hajishirzi. 2023. How\nfar can camels go? exploring the state of instruction\ntuning on open resources.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023. Large language\nmodels are versatile decomposers: Decompose evi-\ndence and questions for table-based reasoning.ArXiv,\nabs/2301.13808.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, Tao Zhang, Chen Zhou, Kaizhe Shou,\nMiao Wang, Wufang Zhu, Guoshan Lu, Chao Ye,\nYali Ye, Wentao Ye, Yiming Zhang, Xinglong Deng,\nJie Xu, Haobo Wang, Gang Chen, and Junbo Zhao.\n2023. Tablegpt: Towards unifying tables, nature\nlanguage and commands into one gpt.\nWenqi Zhang, Yongliang Shen, Weiming Lu, and\nYue Ting Zhuang. 2023. Data-copilot: Bridging bil-\nlions of data and humans with autonomous workflow.\nArXiv, abs/2306.07209.\nYusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong\nChen, Dragomir Radev, Chenguang Zhu, Michael\nZeng, and Rui Zhang. 2022. Macsum: Controllable\nsummarization with mixed attributes. arXiv preprint\narXiv:2211.05041.\nYilun Zhao, Boyu Mi, Zhenting Qi, Linyong Nan, Ming-\nhao Guo, Arman Cohan, and Dragomir Radev. 2023a.\nOpenRT: An open-source framework for reasoning\nover tabular data. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 3: System Demonstrations), pages\n336–347, Toronto, Canada. Association for Compu-\ntational Linguistics.\nYilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang,\nand Dragomir Radev. 2022. ReasTAP: Injecting ta-\nble reasoning skills during pre-training via synthetic\nreasoning examples. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 9006–9018, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nYilun Zhao, Zhenting Qi, Linyong Nan, Lorenzo Jaime\nFlores, and Dragomir Radev. 2023b. LoFT: Enhanc-\ning faithfulness and diversity for table-to-text gener-\nation via logic form control. In Proceedings of the\n17th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 554–\n561, Dubrovnik, Croatia. Association for Computa-\ntional Linguistics.\nYilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin\nLiu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo\nXu, Arman Cohan, and Dragomir Radev. 2023c. Qt-\nsumm: A new benchmark for query-focused table\nsummarization.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2018. Seq2SQL: Generating structured queries from\nnatural language using reinforcement learning.\nWenxuan Zhou, Sheng Zhang, Hoifung Poon, and\nMuhao Chen. 2023. Context-faithful prompt-\ning for large language models. arXiv preprint\narXiv:2303.11315.\n169\nA Table-to-Text Generation Benchmarks\nA.1 L OTNLG Dataset\nLogical Reasoning Type Definition\n• Aggregation: operations involving sum or aver-\nage operation to summarize the overall statistics.\nSentence: The total number of scores of xxx is\nxxx. The average value of xxx is xxx.\n• Negation: operations to negate. Sentence: xxx\ndid not get the first prize.\n• Superlative: superlative operations to get the\nhighest or lowest value. Sentence: xxx achieved\nthe most scores.\n• Count: operations to count the amount of entities\nthat fulfil certain conditions. Sentence: There are\n4 people born in xxx.\n• Comparative: operations to compare a specific\naspect of two or more entities. Sentence: xxx is\ntaller than xxx.\n• Ordinal: operations to identify the ranking of\nentities in a specific aspect. Sentence: xxx is the\nthird youngest player in the game.\n• Unique: operations to identify different entities.\nSentence: The players come from 7 different\ncities.\n• All: operations to summarize what all entities\ndo/have in common. Sentence: All of the xxx are\nmore expensive than $25.\n• Surface-Level: no logical reasoning type above.\nSentence: xxx is moving to xxx.\nFigure 4: Distribution of logical reasoning types for the\nLOTNLG dataset.\nB Examined Systems\nB.1 Fine-tuned Models\n• BART (Lewis et al., 2020) is a pre-trained de-\nnoising autoencoder with transformer-based ar-\nchitecture and shows effectiveness in NLG tasks.\n• Flan-T5 (Chung et al., 2022) enhances T5 (Raf-\nfel et al., 2020) by scaling instruction fine-tuning\nand demonstrates better human-like reasoning\nabilities than the T5.\n• GPT2-C2F (Chen et al., 2020a) first generates\na template which determines the global logical\nstructure, and then produces the statement using\nthe template as control.\n• R2D2 (Nan et al., 2022a) trains a generative lan-\nguage model both as a generator and a faithful-\nness discriminator with additional replacement\ndetection and unlikelihood learning tasks, to en-\nhance the faithfulness of table-to-text generation.\n• TAPEX(Liu et al., 2022b) continues pre-training\nthe BART model by using a large-scale corpus\nof synthetic SQL query execution data, showing\nbetter table understanding and reasoning abili-\nties.\n• OmniTab(Jiang et al., 2022) uses the same back-\nbone as TAPEX, and is further pre-trained on col-\nlected natural and synthetic Table QA examples.\n• ReasTAP (Zhao et al., 2022) enhances the table\nunderstanding and reasoning abilities of BART\nby pre-training on a synthetic Table QA corpus.\n• PLOG (Liu et al., 2022a) continues pre-training\ntext generation models on a table-to-logic-form\ngeneration task (i.e., T5 model), improving the\nfaithfulness of table-to-text generation.\n• LOFT (Zhao et al., 2023b) utilizes logic forms\nas fact verifiers and content planners to con-\ntrol table-to-text generation, exhibiting improved\nfaithfulness and text diversity.\nB.2 Large Language Models\n• Pythia (Biderman et al., 2023) is a suite of 16\nopen-sourced LLMs all trained on public data in\nthe exact same order and ranging in size from\n70M to 12B parameters. This helps researchers\nto gain a better understanding of LLMs and their\ntraining dynamics.\n• LLaMA (Touvron et al., 2023a,b) is an open-\nsource LLM trained on large-scale and publicly\navailable datasets. We evaluate both LLaMA and\nLLaMA2 in this paper.\n• Alpaca (Taori et al., 2023) and Vicuna (Chi-\nang et al., 2023) are fine-tuned from LLaMA\nwith instruction-following data, exhibiting better\ninstruction-following capabilities.\n170\n• TÜLU (Wang et al., 2023) further trains LLaMA\non 12 open-source instruction datasets, achieving\nbetter performance than LLaMA.\n• GPT (Brown et al., 2020; Wei et al., 2022) is a\npowerful large language model which is capable\nof generating human-like text and performing a\nwide range of NLP tasks in a few-shot setting.\nWe use the OpenAI engines ofgpt-3.5-0301\nand gpt-4-0314 for GPT-3.5 and GPT-4 mod-\nels, respectively.\nTo formulate the prompt, we linearize the table\nas done in previous work on table reasoning (Chen,\n2023) and concatenate it with its corresponding\nreference statements as demonstrations. We use\nthe table truncation strategy as proposed by Liu\net al. (2022b) to truncate large table and ensure that\nthe prompts are within the maximum token limita-\ntion for each type of LLMs. For LLM parameter\nsettings, we used a temperature of 0.7, maximum\noutput length of 512, without any frequency or\npresence penalty.\nC Experiments\nExample 1:\nTitle: 1941 vfl season\nTable:\nhome team | home team score | away team | away team score | venue | crowd | date\nrichmond | 10.13 (73) | st kilda | 6.11 (47) | punt road oval | 6000 | 21 june 1941\nhawthorn | 6.8 (44) | melbourne | 12.12 (84) | glenferrie oval | 2000 | 21 june 1941\ncollingwood | 8.12 (60) | essendon | 7.10 (52) | victoria park | 6000 | 21 june 1941\ncarlton | 10.17 (77) | fitzroy | 12.13 (85) | princes park | 4000 | 21 june 1941\nsouth melbourne | 8.16 (64) | north melbourne | 6.6 (42) | lake oval | 5000 | 21 june 1941\ngeelong | 10.18 (78) | footscray | 13.15 (93) | kardinia park | 5000 | 21 june 1941\nFive generated statements:\n1. footscray scored the most point of any team that played on 21 june, 1941.\n2. geelong was the home team with the highest score.\n3. kardinia park was the one of the six venues that were put to use.\n4. north melbourne away team recorded an away score of 6.6 (42) while melbourne \nrecorded an away score of 12.12 (84).\n5. all six matches took place on 21 june 1941.\nExample 2:\nTitle: {title}\nTable: \n{table}\nFigure 5: An example of 1-shot direct-prediction\nprompting for the LOGIC NLG task.\n[INSTRUCTION] Your task is to provide 5 different consistent statements derived from a \ntable. Consistent means that all information of your statements should be supported by the \ncorresponding table. Provided 5 statements should be different from each other.\nTo guide your responses, we have provided two example tables with five statements each. \nUse the template to structure your answer, provide reasoning for your statements and \nsuggest statements. We encourage you to think through each step of the process carefully.\nExample 1:\nTitle: 1941 vfl season\nTable:\nhome team | home team score | away team | away team score | venue | crowd | date\nrichmond | 10.13 (73) | st kilda | 6.11 (47) | punt road oval | 6000 | 21 june 1941\nhawthorn | 6.8 (44) | melbourne | 12.12 (84) | glenferrie oval | 2000 | 21 june 1941\ncollingwood | 8.12 (60) | essendon | 7.10 (52) | victoria park | 6000 | 21 june 1941\ncarlton | 10.17 (77) | fitzroy | 12.13 (85) | princes park | 4000 | 21 june 1941\nsouth melbourne | 8.16 (64) | north melbourne | 6.6 (42) | lake oval | 5000 | 21 june 1941\ngeelong | 10.18 (78) | footscray | 13.15 (93) | kardinia park | 5000 | 21 june 1941\nReasoning 1: looking at both \"home team score\" column and \"away team score\" column, \nfinding the highest score was 13.15 (93) in \"away team score\" column and then looking for \nwhich team scored 13.15 (93) in \"away team\" colmun, footscray scored the most point of \nany team that played on 21 june.\nStatement 1: footscray scored the most point of any team that played on 21 june, 1941.\nReasoning 2: looking at \"home team\" column and finding the corresponding home team \nscores of geelong in \"home team score\" column, geelong did have the highest score.\nStatement 2: geelong was the home team with the highest score.\nReasoning 3: looking at \"venue\" column, kardinia park was the one of six venues.\nStatement 3: kardinia park was the one of the six venues that were put to use.\nReasoning 4: looking at \"away team\" column and finding the corresponding away team \nscores of north melbourne and melbourne in \"away team score\" column, north melbourne \nas away team scored 6.6 (42) while melbourne as away team scored 12.12 (84).\nStatement 4: north melbourne away team recorded an away score of 6.6 (42) while \nmelbourne recorded an away score of 12.12 (84).\nReasoning 5: looking at \"date\" column, all six matches took place on 21 june 1941.\nStatement 5: all six matches took place on 21 june 1941.\nNow please give 5 different consistent claims of the new table. Let's think step by step and \nfollow the given examples.\nTitle: {title}\nTable: \n{table}\nFigure 6: An example of 1-shot chain-of-thought\nprompting for the LOGIC NLG task.\nRead the table below regarding \"1919 in brazilian football\" to verify whether the provided \nclaims are true or false.\nTable:\ndate | result | score | brazil scorers | competition\nmay 11 , 1919 | w | 6 - 0 | friedenreich (3) , neco (2) , haroldo | south american \nchampionship\nmay 18 , 1919 | w | 6 - 1 | heitor , amílcar (4), millon | south american championship\nmay 26 , 1919 | w | 5 - 2  | neco (5) | south american championship\nmay 30 , 1919 | l | 1 - 2 | jesus (1) | south american championship\njune 2nd , 1919 | l | 0 - 2 | - | south american championship\nStatement: neco has scored a total of 7 goals in south american championship.\nExplanation: neco has scored 2 goals on may 11  and 5 goals on may 26. neco has scored \na total of 7 goals, therefore, the claim is true.\nStatement: jesus has scored in two games in south american championship.\nExplanation: jesus only scored once on the may 30 game, but not in any other game, \ntherefore, the claim is false.\nStatement: brazilian football team has scored six goals twice in south american \nchampionship.\nExplanation: brazilian football team scored six goals once on may 11 and once on may 18, \ntwice in total, therefore, the claim is true.\nRead the table below regarding \n(...abbreviate the second prompting example…)\nRead the table below regarding \"{title}\" to verify whether the provided claims are true or \nfalse.\nTable:\n{table}\nStatement: {statement_i}\nFigure 7: An example of 2-shot chain-of-thought\nprompting adopted from Chen (2023) for faithfulness-\nlevel automated evaluation.\n171\nType Models SP-Acc NLI-Acc TAPAS-Acc TAPEX-Acc Type EM Type F1\n0-shot* GPT-3.5 51.2 77.2 70.8 66.8 59.2 43.8\nGPT-4 69.2 79.4 85.6 84.2 75.2 60.0\n1-shot Direct GPT-3.5 53.8 75.6 71.6 71.0 51.2 38.1\nGPT-4 60.2 72.8 83.8 84.2 76.6 63.0\n1-shot CoT GPT-3.5 50.8 78.8 79.2 79.4 46.2 30.2\nGPT-4 59.2 74.8 84.4 85.8 70.0 51.6\n2-shot Direct\nPythia-12b 44.2 60.6 41.8 43.0 19.0 12.2\nLLaMA-7b 41.0 62.2 46.2 46.2 18.2 13.4\nVicuna-13b 48.6 71.2 57.4 54.4 22.0 15.2\nLLaMA-13b 44.6 62.4 50.8 48.8 22.6 15.8\nAlpaca-13b 46.2 73.8 50.8 54.0 21.8 15.8\nLLaMA2-70b-chat 44.2 60.0 56.0 58.0 24.2 15.8\nLLaMA-30b 40.0 62.6 53.0 52.6 24.2 16.4\nLLaMA-65b 46.2 57.8 54.0 51.8 21.0 17.2\nTÜLU -13b 44.2 72.8 60.8 56.8 26.6 17.4\nGPT-3.5 55.2 76.2 70.8 67.6 52.2 35.0\nGPT-4 61.4 72.2 84.6 83.2 73.4 54.8\n2-shot CoT\nPythia-12b 42.0 53.8 41.2 41.0 15.2 11.6\nLLaMA-30b 41.0 60.4 52.6 59.2 20.4 13.2\nLLaMA-7b 37.6 61.2 43.8 45.0 17.2 13.4\nLLaMA2-70b-chat 48.2 64.6 56.0 67.8 20.2 13.4\nLLaMA-13b 45.0 56.6 51.2 51.2 18.8 14.0\nLLaMA-65b 45.2 62.4 59.4 58.8 21.2 15.2\nVicuna-13b 43.4 72.0 62.2 61.0 18.4 16.0\nAlpaca-13b 40.4 71.6 58.4 57.8 23.0 16.2\nTÜLU -13b 45.8 65.8 60.8 61.0 23.2 16.2\nGPT-3.5 49.2 74.4 77.2 75.4 49.4 35.0\nGPT-4 59.2 72.0 85.6 83.2 67.6 55.6\nTable 6: Faithfulness-level automated evaluation results on LOTNLG . We do not evaluate fine-tuned models as\nLOTNLG does not contain a training set. ∗: It is challenging for other LLMs to follow the instructions in 0-shot\nprompt to generate a statement using the specified types of logical reasoning operations.\n172\nType Models BLEU-1/2/3 ROUGE-1/2/L TAPAS-Acc TAPEX-Acc\nFine-tuned\nBART 63.2/50.8/42.0 67.6/46.0/57.2 94.8 68.8\nFlan-T5 62.2/49.6/41.0 66.8/45.0/56.2 94.2 69.2\nOmniTab 63.4/50.8/41.8 67.4/45.2/56.2 94.6 71.6\nReasTAP 63.6/51.0/42.2 67.6 /45.8/57.2 94.6 71.4\nTAPEX 63.6/50.8/42.0 66.4/45.0/56.2 96.2 73.0\n0-shot GPT-3.5 56.4/42.6/33.4 60.6/38.0/49.4 92.4 72.8\nGPT-4 52.4/40.2/31.8 63.8/40.4/51.6 94.0 74.4\n1-shot Direct GPT-3.5 56.8/43.2/34.2 63.0/39.8/51.4 91.8 74.6\nGPT-4 56.4/ 43.6/34.8 66.2 /43.0/54.4 94.0 73.8\n1-shot CoT GPT-3.5 43.2/32.4/25.2 57.4/35.8/46.8 94.2 67.0\nGPT-4 59.6/45.8/36.4 64.0 /41.0/52.4 91.0 76.4\n2-shot Direct\nPythia-12b 38.8/26.6/19.4 43.2/22.6/35.2 76.6 35.0\nLLaMA-7b 40.6/28.6/21.4 48.2/26.6/39.0 86.2 47.8\nLLaMA-13b 48.4/35.2/26.8 51.0/29.4/42.2 85.4 57.4\nAlpaca-13b 52.2/38.4/29.6 56.4/33.6/46.2 88.4 57.4\nTÜLU -13b 50.6/37.4/29.0 54.2/31.8/44.6 86.4 60.0\nLLaMA-30b 50.4/37.0/28.2 56.2/33.2/45.4 87.0 60.2\nVicuna-13b 56.0/42.2/32.8 59.0/36.2/48.0 87.6 62.4\nLLaMA-65b 53.6/39.8/30.8 57.0/34.0/46.6 88.4 63.0\nLLaMA2-70b-chat 54.6/41.0/31.8 58.4/35.8/47.8 89.4 66.2\nGPT-4 55.0/ 42.8/34.6 66.0 /42.8/54.0 95.2 75.8\nGPT-3.5 55.8/42.8/34.0 63.2/40.0/51.6 92.2 76.0\n2-shot CoT\nPythia-12b 38.8/25.4/17.8 39.2/18.8/32.2 69.0 36.2\nLLaMA-7b 33.0/22.2/16.0 41.0/21.2/33.2 77.6 42.0\nLLaMA-13b 43.2/30.4/22.6 45.4/25.2/37.6 82.0 50.8\nAlpaca-13b 47.4/34.4/26.2 51.4/30.0/42.0 82.8 54.4\nTÜLU -13b 37.0/25.8/18.8 43.6/24.0/35.2 86.2 55.8\nLLaMA-30b 45.4/33.2/25.6 52.4/30.8/42.2 86.2 63.6\nVicuna-13b 50.4/37.6/29.4 53.8/32.4/44.6 85.6 65.8\nLLaMA-65b 50.2/37.0/28.4 54.8/32.8/44.6 87.8 66.0\nLLaMA2-70b-chat 53.8/40.2/31.4 57.4/34.8/47.0 89.2 66.2\nGPT-3.5 50.8/38.8/30.8 60.6/38.2/49.0 92.8 70.8\nGPT-4 62.2/48.6/39.2 65.8 /42.8/54.4 91.2 79.2\nTable 7: Automated evaluation results on the FeTaQA dataset.\n173\nType Models BLEU-1/2/3 ROUGE-1/2/L TAPAS-Acc TAPEX-Acc Accuracy\n0-shot GPT-3.5 63.2/49.2/39.4 64.4/40.0/56.4 73.0 74.6 54.0\nGPT-4 60.6/46.8/37.4 64.6/40.4/54.8 78.6 80.6 62.4\n1-shot Direct GPT-3.5 62.0/48.4/39.0 64.0/40.0/56.8 75.0 73.2 51.8\nGPT-4 63.2/49.8/40.4 66.2 /42.6/58.0 78.4 79.0 66.0\n1-shot CoT GPT-3.5 55.0/42.4/33.8 62.8/39.0/54.8 72.4 72.2 55.2\nGPT-4 62.2/49.0/39.6 66.2/42.2/58.4 78.2 78.6 69.8\n2-shot Direct\nPythia-12b 12.4/7.6/5.2 19.6/9.2/17.4 74.6 62.4 7.8\nLLaMA-7b 14.4/9.6/6.8 26.2/13.4/23.0 71.8 53.0 19.0\nLLaMA-13b 7.6/4.8/3.4 20.2/10.4/18.2 78.4 56.0 21.4\nVicuna-13b 43.0/31.6/24.4 46.0/27.2/40.6 74.6 64.2 30.2\nAlpaca-13b 40.8/29.2/21.6 46.6/26.2/40.4 71.8 57.6 31.2\nLLaMA-30b 34.0/24.4/18.2 44.6/25.0/39.8 74.0 61.0 31.8\nTÜLU -13b 49.6/36.4/28.0 51.4/29.4/45.8 78.8 60.4 33.8\nLLaMA-65b 45.8/33.8/26.0 48.8/28.2/43.6 73.6 64.4 36.2\nLLaMA2-70b-chat 51.2/38.4/30.0 50.4/29.6/45.4 72.4 68.4 37.6\nGPT-3.5 63.4/49.8/40.2 64.8/40.8/57.2 74.8 73.6 51.8\nGPT-4 62.8/49.2/39.6 65.8/41.8/57.6 78.6 81.4 63.6\n2-shot CoT\nPythia-12b 27.2/18.0/12.8 35.6/17.4/31.4 66.0 48.8 15.8\nLLaMA-7b 13.2/8.4/5.8 28.0/13.2/24.0 73.4 47.8 24.2\nLLaMA-13b 22.2/14.8/10.4 35.2/18.0/31.4 74.0 56.2 26.2\nAlpaca-13b 33.2/23.6/17.8 47.6/26.4/41.2 75.0 55.4 32.2\nLLaMA-30b 37.4/26.2/19.6 46.2/24.8/40.6 72.6 60.0 35.6\nTÜLU -13b 25.8/17.0/12.0 35.4/17.4/31.0 79.0 65.6 35.8\nVicuna-13b 45.2/33.2/25.4 53.6/31.2/47.6 75.6 62.2 38.6\nLLaMA-65b 51.2/37.8/29.0 51.6/29.4/45.6 75.6 67.6 41.6\nLLaMA2-70b-chat 46.2/34.2/26.6 49.6/28.8/44.2 75.8 66.6 43.2\nGPT-3.5 57.4/44.4/35.4 64.0/40.0/55.4 73.6 72.8 58.6\nGPT-4 63.0/49.6/40.0 66.2 /42.4/58.8 76.4 79.6 68.4\nTable 8: Automated evaluation results on the F2WTQ dataset. We do not evaluate fine-tuned models as F2WTQ\ndoes not contain a training set.\n174\n[INSTRUCTION] Your task is to provide feedback on statements derived from tables. Your feedback should \nconsist of  1) Explanation, which determine whether the initial statement is factually consistent with the given \ntable; 2) Corrective Instruction, which provide instructions on how to correct the initial statement if it is detected \nas unfaithful; and 3) Edited Statement, which edits the initial statement following the corrective instruction. \nThere are two types of errors: intrinsic and extrinsic. Intrinsic errors refer to mistakes that arise from within the \nstatement itself, while extrinsic errors are caused by factors external to the statement. To help you provide \naccurate feedback, we have provided instruction templates for your use. These templates include \"remove,\" \n\"add,\" \"replace,\" \"modify,\" \"rewrite,\" and \"do nothing\". \nIt is important to note that you should be capable of identifying logical operations when reviewing statements. \nExamples of such operations include superlatives, exclusives (such as \"only\"), temporal relationships (such as \n\"before/after\"), quantitative terms (such as \"count\" or \"comparison\"), inclusive/exclusive terms (such as \n\"both/neither\"), and arithmetic operations (such as \"sum/difference\" or \"average\").\nTo guide your responses, we have provided two examples with three statements each. Use these templates to \nstructure your answer, provide reasoning for your feedback, and suggest improved statements. We encourage \nyou to think through each step of the process carefully.\nRemember, your final output should always include a “Edited Statement” no matter if there is error or not.\nExample 1:\nTitle: 1941 vfl season\nTable:\nhome team | home team score | away team | away team score | venue | crowd | date\nrichmond | 10.13 (73) | st kilda | 6.11 (47) | punt road oval | 6000 | 21 june 1941\nhawthorn | 6.8 (44) | melbourne | 12.12 (84) | glenferrie oval | 2000 | 21 june 1941\ncollingwood | 8.12 (60) | essendon | 7.10 (52) | victoria park | 6000 | 21 june 1941\ncarlton | 10.17 (77) | fitzroy | 12.13 (85) | princes park | 4000 | 21 june 1941\nsouth melbourne | 8.16 (64) | north melbourne | 6.6 (42) | lake oval | 5000 | 21 june 1941\ngeelong | 10.18 (78) | footscray | 13.15 (93) | kardinia park | 5000 | 21 june 1941\nStatement: st kilda scored the most point of any team that played on 21 june, 1941\nExplanation: footscray scored the most point of any team that played on 21 june, not st kilda. So the statement \nhas instrinsic error. \nCorrective Instruction: replace st kilda with footscray.\nEdited Statement: footscray scored the most point of any team that played on 21 june, 1941.\nExample 2:\n(...abbreviate…)\nNow please give feedback to the statement of the new table. Let's think step by step and follow the given \nexample. Remember to include “Explanation”, “Corrective Instruction”, and “Edited Statement” parts in the \noutput.\nTitle: {title}\nTable: \n{table}\nStatement: {sent}\nFigure 8: An example of 2-shot chain-of-thought prompts for natural language feedback generation on LOGIC NLG.\n175",
  "topic": "Table (database)",
  "concepts": [
    {
      "name": "Table (database)",
      "score": 0.8426262140274048
    },
    {
      "name": "Computer science",
      "score": 0.6841879487037659
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.5783735513687134
    },
    {
      "name": "Code (set theory)",
      "score": 0.5179173350334167
    },
    {
      "name": "Data science",
      "score": 0.41490787267684937
    },
    {
      "name": "Information retrieval",
      "score": 0.4046868681907654
    },
    {
      "name": "Data mining",
      "score": 0.3169289231300354
    },
    {
      "name": "Power (physics)",
      "score": 0.12913966178894043
    },
    {
      "name": "Programming language",
      "score": 0.12911665439605713
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}