{
  "title": "MVT: Multi-view Vision Transformer for 3D Object Recognition",
  "url": "https://openalex.org/W3208279478",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1803215993",
      "name": "Chen Shuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104645546",
      "name": "Yu Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2007781260",
      "name": "Li Ping",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2400418317",
    "https://openalex.org/W3170974475",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2958885478",
    "https://openalex.org/W2890018557",
    "https://openalex.org/W3035541121",
    "https://openalex.org/W2768213582",
    "https://openalex.org/W2893477965",
    "https://openalex.org/W2986519121",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2211722331",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2798998662",
    "https://openalex.org/W2921891839",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W2905564365",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2964342398",
    "https://openalex.org/W3131500599"
  ],
  "abstract": "Inspired by the great success achieved by CNN in image recognition, view-based methods applied CNNs to model the projected views for 3D object understanding and achieved excellent performance. Nevertheless, multi-view CNN models cannot model the communications between patches from different views, limiting its effectiveness in 3D object recognition. Inspired by the recent success gained by vision Transformer in image recognition, we propose a Multi-view Vision Transformer (MVT) for 3D object recognition. Since each patch feature in a Transformer block has a global reception field, it naturally achieves communications between patches from different views. Meanwhile, it takes much less inductive bias compared with its CNN counterparts. Considering both effectiveness and efficiency, we develop a global-local structure for our MVT. Our experiments on two public benchmarks, ModelNet40 and ModelNet10, demonstrate the competitive performance of our MVT.",
  "full_text": "CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER 1\nMVT: Multi-view Vision Transformer for 3D\nObject Recognition\nShuo Chen\nTan Yu\nPing Li\n{shanshuo1992,tanyuuynat,pingli98}@gmail.com\nCognitive Computing Lab,\nBaidu Research\nAbstract\nInspired by the great success achieved by CNN in image recognition, view-based\nmethods applied CNNs to model the projected views for 3D object understanding and\nachieved excellent performance. Nevertheless, multi-view CNN models cannot model\nthe communications between patches from different views, limiting its effectiveness in\n3D object recognition. Inspired by the recent success gained by vision Transformer in\nimage recognition, we propose a Multi-view Vision Transformer (MVT) for 3D object\nrecognition. Since each patch feature in a Transformer block has a global reception ﬁeld,\nit naturally achieves communications between patches from different views. Meanwhile,\nit takes much less inductive bias compared with its CNN counterparts. Considering both\neffectiveness and efﬁciency, we develop a global-local structure for our MVT. Our ex-\nperiments on two public benchmarks, ModelNet40 and ModelNet10, demonstrate the\ncompetitive performance of our MVT.\n1 Introduction\nIn the past decade, we have witnessed the great success achieved by convolutional neu-\nral network [13, 18] in image understanding. Inspired by its success in understanding 2D\nimages, several works attempt to deploy CNN in 3D object understanding, achieving ex-\ncellent performance. These methods can be coarsely divided into three groups: view-based\nmethods [8, 11, 12, 17, 28, 31], volume-based methods [22, 23, 24, 35], and point-based\nmethods [3, 25, 26]. Among them, view-based methods are closely related to 2D image\nunderstanding. View-based methods project a 3D object into multiple views and model each\nview through the model original used in modelling 2D images. Beneﬁted from pre-training\non the large-scale 2D image dataset such as ImageNet [5], they achieve competitive perfor-\nmance compared with their volume-based and point-based counterparts.\nMVCNN [28] is the pioneering view-based method. It extracts each view features through\na vanilla 2D CNN and aggregates the view features through sum-pooling. The follow-\ning works [8, 11, 12, 17, 31, 33] seek to ﬁnd a more effective way to aggregate the view\nfeatures. Speciﬁcally, RCPCNN [31] and GVCNN [8] group views into multiple sets and\nconduct pooling within each set. Seqviews2seqlabels [12] and 3D2SeqViews [11] model\nthe view order through recurrent neural network. View-GCN [33] models the view-based\n© 2021. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\nThe work of Shuo Chen was conducted as an intern at Baidu. He is a PhD student at the University of Amsterdam.arXiv:2110.13083v1  [cs.CV]  25 Oct 2021\n2 CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER\nLocal Transformer Encoder\nGlobal Transformer Encoder\nposition\nembedding\nMLP \nhead\nClass\nbed\nchair\n…\nMulti-View \nimages\n*0 1 2 3 4 5 6 7 8 9\nLinear Projection of Patches\nconcat\nFigure 1: The architecture of the proposed Multi-view Vision Transformer (MVT). Each\nview is split into non-overlap patches. Each patch is projected into an embedding vector,\nwhich is added to a position embedding. Patch embedding vectors from each view are fed\ninto the local Transformer encoder for communications between patches within the view.\nThen the outputs of the local Transformer for all views are concatenated into a global set,\nwhich is fed into the global Transformer encoder for communications between patches from\ndifferent views. After that, the set of attended patch features from the output of the global\nTransformer encoder are sum-pooled into a global representation for the 3D object. We\nﬁnally use the MLP head for classiﬁcation.\nrelations through graph convolution network. MHBN [38] and MVLADN [39] observe the\nlimitations of view-based pooling, formulate the view-based 3D object recognition into a set-\nto-set matching problem, and investigate in patch-level pooling. Nevertheless, view-based\npooling and patch-based pooling methods only fuse the visual features from different views\nin the last pooling layer. There are no interactions between visual features from different\nviews in previous layers. This conﬁguration leads to the fact that a patch can only have a\nlocal perception ﬁeld and fails to perceive patches in other views. Relation Network [37]\nenhances each patch feature by patches from all views, achieving better performance than\nthe above-mentioned view-based pooling and patch-based pooling methods.\nIn this work, inspired by the great success achieved by vision Transformer [7, 29], we\npropose a multi-view vision Transformer (MVT) to empower each patch to have the global\nreception ﬁeld to perceive the visual content of all views from a 3D object. It adopts a pure-\nTransformer architecture and thus takes much less inductive bias compared with its CNN\ncounterparts [7]. Considering the total number of patches is largely due to multiple projected\nviews, simply concatenating all patches will generate an extremely long sequence, leading\nto an expensive computational cost. Taking both effectiveness and efﬁciency into consider-\nation, we devise a local-global structure, as visualized in Figure 1. In the local Transformer\nencoder, we adopt Transformer to process the patches within each view individually. In the\nglobal Transformer encoder, we merge patch features from all views and feed them together\ninto Transformer layers for the global reception ﬁeld. Using such a simple and elegant ar-\nchitecture, we achieve state-of-the-art recognition accuracy on public benchmarks, including\nModelNet40 and ModelNet10.\nCHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER 3\n2 Related Works\n3D object recognition . Existing mainstream 3D object recognition methods can be cat-\negorized into three groups: volume-based methods [22, 23, 24, 35], point-based meth-\nods [3, 25, 26] and view-based methods [8, 11, 12, 17, 31, 33],. Among them, volume-based\nmethods quantize the 3D object into regular voxels, and conduct 3D convolutions on vox-\nels. Nevertheless, 3D convolution is computationally expensive when the resolution is high.\nFor satisfactory efﬁciency, volume-based methods normally conduct low-resolution quan-\ntization, inevitably leading to information loss. In parallel, point-based methods directly\nmodel the cloud of points, efﬁciently achieving competitive performance. View-based meth-\nods project a 3D object into multiple 2D views. They model each view through the vision\nbackbone for image understanding to obtain view features or patch features. Our work can\nbe categorized into view-based methods. Thus, we mainly review view-based methods here.\nMVCNN [28] is one of the earliest works exploiting convolutional neural network (CNN)\nfor modelling multiple views. It aggregates the view features from CNN through max-\npooling. MVCNN-MultiRes [24] exploits views projected from multi-resolution settings,\nboosting the recognition accuracy. Pairwise [16] decomposes the sequence of projected\nviews into several pairs and models the pairs through CNN. GIFT [2] represents each 3D\nobject by a set of view features and determines the similarity between two 3D objects by\nmatching two sets of view features through a devised matching kernel. RotationNet [17]\nconsiders the viewpoint of each project and treats viewpoints as latent variables to boost the\nrecognition performance. RCPCNN [31] groups views into multiple sets and concatenate\nthe set features as the 3D object representation. GVCNN [8] also groups views into multiple\nsets. It adaptively assigns a higher weight to the group containing crucial visual content to\nsuppress the noise. Seqviews2seqlabels [12] and 3D2SeqViews [11] exploit the view or-\nder besides visual content through recurrent neural network. View-GCN [33] models the\nrelations between views by a graph convolution network. MHBN [38] and MVLADN [39]\ninvestigate in pooling patch-level features to generate the 3D object recognition. Relation\nNetwork [37] enhances each patch feature by patches from all views through a reinforce-\nment block plugged in the rear of the network. Our method has a similar spirit but needs\nmuch less inductive bias and only takes a standard Transformer to achieve the communica-\ntions between patches of different views.\nVision Transformer.Inspired by the great success achieved by Transformers [30] in natural\nlanguage processing, vision Transformer (ViT) [7] is proposed. It crops an image into mul-\ntiple non-overlap patches and feeds the cropped patches into a stack of Transformer layers.\nCompared with CNN models, each patch in ViT has a global reception ﬁeld. Meanwhile,\nViT has much less image-speciﬁc inductive bias than CNN models. By pre-training on huge-\nscale datasets, ViT has achieved comparable accuracy compared with its CNN counterparts.\nDeiT [29] proposes a data-efﬁcient approach using an improved optimizer, more advanced\ndata augmentation, and training tricks. PVT [32] and PiT [14] bring back inductive bias in\nCNN and exploit the pyramid structure to shrink the spatial size progressively. T2T [40] and\nTNT [10] focus on improving the effectiveness of modeling local structure within patches.\nSwin [20] and Twin [4] exploit the locality and sparsity to achieve a better trade-off between\neffectiveness and efﬁciency. CvT [34] and Container [9] exploit a hybrid structure combining\nTransformer and convolution. Unlike the methods mentioned above that exploit Transformer\nfor 2D image understanding, we investigate its effectiveness in 3D object recognition.\n4 CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER\n3 Preliminary\nMLP\nnorm\nnorm\nmulti-head \nself-attention\n+\n+\nFigure 2: The structure of a Trans-\nformer block. It consists of layer-\nnormalization (norm) layers, a multi-\nlayer perceptron (MLP) module and\na multi-head self-attention module.\nIn this section, we brieﬂy introduce the structure of\nTransformer [30] block as visualized in Figure 2.\nIt consists of two layer-normalization (LN) layers,\na multi-head self-attention (MSA) module, and a\nmulti-layer perceptron (MLP) module.\nMulti-head self-attention (MSA) module. Let us\ndenote the inputs by X ∈RN×D where N is the num-\nber of input vectors and D is the dimension of each\ninput vector. MSA maps X into the queries QN×DQ ,\nkeys K ∈RN×DK and values V ∈RN×DV through\nthree fully-connected layers, where DQ = DK = DV .\nThen it splits Q, K and V into M heads:\nQ →[Q1,···,QM], K →[K1,···,KM], V →[V1,···,VM],\nwhere, for m = 1,..., M, we have\nQm ∈RN×\nDQ\nM , Km ∈RN×DK\nM , Vm ∈RN×DV\nM .\nThen the self-attention operation is conducted on\neach query-key-value triplet {Qm,Km,Vm} (m =\n1,..., M) and generates the attended features:\nYm = softmax(QmK⊤\nm√Dk\n)Vm, m = 1,..., M.\nThe attended features from each head are concatenated to obtain the ﬁnal output of the self-\nattention module:\nY ←[Y1,···,YM] ∈RN×DV .\nLayer Normalization (LN)[1] is widely used in Transformer-based architecture for training\nstability. Given a D-dimension feature vector x = [x1,···,xD], it computes the mean µ and\nthe standard deviation ε by\nµ = 1\nD\nD\n∑\ni=1\nxi, ε =\n√\n1\nD\nD\n∑\ni=1\n(xi −µ)2.\nThen a linear operation is conducted on each element of x:\nˆxi = γ xi −µ\nε +β, (1)\nwhere β and γ are learnable parameters for afﬁne transform.\nMulti-layer perceptron (MLP). The MLP is normally plugged after self-attention module\nto operate on each input separately [30]. It consists of two fully-connected layers with a\nbottleneck structure and an activation layer for for non-linearity. Speciﬁcally, for each feature\nx ∈RD, MLP enhances x by\nMLP(x) =σ(xW1 +b1)W2 +b2, (2)\nCHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER 5\nwhere W1 ∈RD×rD and b1 ∈RrD are weights of the ﬁrst fully-connected layer, which in-\ncrease the feature dimension from D to rD and r > 1 is termed as expansion ratio. Mean-\nwhile, W2 ∈RrD×D and b2 ∈RD are weights of the second fully-connected layer, which\ndecreases the feature dimension from rD back to D.\n4 Method\nIn this section, we introduce the proposed multi-view vision Transformer. We ﬁrst clarify\nthe input of the model and then illustrate the details of the model.\n4.1 Input\nFor each 3D object, we project it into L views {V 1,···,V L}. Each view V j ( j = 1,..., L)\nis of W ×H ×3 size. For each view, we crop it into w ×h non-overlap patches and each\npatch is of p ×p ×3 size. Each patch is unfolded into a vector p ∈R3p2\n. We denote the i-th\npatch in the view V j by pj\ni , where i ∈[1,wh] and j ∈[1,L]. For each pj\ni , we map it into a\nD-dimension vector through a fully-connected layer and obtain\nxj\ni ←W0pj\ni , i = 1,..., wh, j = 1,..., L, (3)\nwhere W0 ∈RD×3p2\nis the weight matrix. Meanwhile, a position embedding pi is learned\nfor each spatial location i ∈[1,wh]. Then the patch feature is obtained by summing up its\nvisual feature and position embedding:\nzj\ni ←xj\ni +pi. (4)\nNote that, pi is only dependent on the spatial location (i) and is shared among different views.\nLike BERT’s[class] token [6], for each view V j, we additionally devise a special token,\nzj\n0, which is a learnable embedding whose state at the output of the Transformer encoder\nserves as the view representation. We merge the special token zj\n0 and the patch features\n{zj\ni }wh\ni=1 into a matrix Zj deﬁned as\nZj = [zj\n0,zj\n1,···,zj\nwh] ∈RD×(wh+1). (5)\n{Zj}L\nj=1 are the inputs of our multi-view vision Transformer.\n4.2 Multi-view Vision Transformer\nThe proposed multi-view vision Transformer consists of two parts. The ﬁrst part processes\npatches in each view, individually. It generates the low-level features for patches in each\nview. We term the ﬁrst part as local Transformer layers. The second part takes the low-level\npatch features from the ﬁrst part as input. It merges patches features from all views in a set\nand feeds the merged set into a stack of Transformer layers for empowering each patch of\neach view to have a global reception ﬁeld. We term these layers exploiting global visual\ncontent as global Transformer layers.\nLocal Transformer blocks. They process the patch features from each view, individually.\nLet us denote the number of local Transformer blocks asS. The feature matrix Zj from each\nview V j goes through S blocks sequentially. The input of the s-th local Transformer block\nis denoted by Zj\ns−1 and the output by Zj\ns . In this case, the input of the ﬁrst block Zj\n0 is just\n6 CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER\nthe patch feature set Zj deﬁned in Eq (5). Meanwhile, we denote the layers in the s-th local\nTransformer block by {MSAl\ns,MLPl\ns,LNl\ns,1,LNl\ns,2}. For the s-th local Transformer block, it\nconducts the following operation:\nZj\ns ←Zj\ns−1 +MSAl\ns(LNl\ns,1(Zj\ns−1)), j = 1... L,\nZj\ns ←Zj\ns +MLPl\ns(LNl\ns,2(Zj\ns )), j = 1... L.\n(6)\nAfter S local Transformer blocks, we obtain the output, Zj\nS for each view V j.\nGlobal Transformer blocks. They process the patch features from each view, jointly. At\nﬁrst, the output of the last local Transformer block for all views, {Zj\nS}L\nj=1 are concatenated\ninto a global matrix:\nM = [Z1\nS,···,ZL\nS ] ∈RD×Lwh. (7)\nWe denote the layers in the t-th global Transformer block by {MSAg\nt ,MLPg\nt ,LNg\nt,1,LNg\nt,2}.\nThe input of the t-th local Transformer block is denoted by Mt−1 and the output is denoted\nby Mt . For the t-the global Transformer block, it conducts the following operation:\nMt ←Mt−1 +MSAg\nt (LNg\nt,1(Mt−1)),\nMt ←Mt +MLPg\nt (LNg\nt,2(Mt )). (8)\nAfter T global Transformer blocks, we obtain the output, MT :\nMT = [m1\n0,m1\n1,···,m1\nwh,m2\n0,m2\n1,···,m2\nwh,···,mL\n0 ,mL\n1 ,···,mL\nwh] ∈RD×L(wh+1), (9)\nwhere mj\n0 denotes the attended special token for the view V j. We conduct sum-pooling on\n{mj\n0}L\nj=1 and obtain the global representation for the 3D object:\nm = 1\nL\nL\n∑\nj=1\nmj\n0. (10)\nThen m is fed into a fully-connected layer for classiﬁcation.\n5 Experiments\nDatasets. We perform experiments on ModelNet40 and ModelNet10 [35]. ModelNet40\nconsists of 12,311 3D CAD models from 40 categories. Among them, 9,843 models are for\ntraining, and 2,468 models are for testing. ModelNet10 is a subset of ModelNet40 and has\n10 categories. We use two settings for generating views from the 3D object. The 12-view\nsetting follows the setup in [31] and the 20-view setting follows the manner in [17].\nImplementation. The architecture of our MVT follows DeiT [29]. We also attempt several\nconﬁgurations, including the tiny and small models. The details of different conﬁgurations\nare summarized in Table 1.\nhidden dimension # heads # local blocks # global blocks\ntiny 192 3 8 4\nsmall 384 6 8 4\nTable 1: The details of different settings.\nCHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER 7\nTraining details. The training follows the settings in DeiT [29]. To be speciﬁc, we use\nAadmW [21] as the optimizer, with an initial learning rate of 0.001, β1=0.9, β2=0.98. Our\nmodel is implemented based on the PaddlePaddle deep learning platform. The model is\ntrained with mixed precision on 2 NVIDIA V100 GPUs. We train 300 epochs for training\nfrom scratch and 100 epochs for ﬁnetuning on a pre-trained model. The pre-trained models\nare trained on ImageNet1K dataset [5].\n5.1 The inﬂuence of the global Transformer blocks\nWhen the number of global Transformer blocks is 0, it is equivalent to replacing the CNN\nin MVCNN [28] by a vision Transformer. We term this conﬁguration as the local baseline.\nIn this baseline setting, each patch can only communicate with patches from the same view.\nWhen we replace the local transformer block with the global transformer block, the patch\nembeddings from the local transformer are fed into the global transformer to interrelate with\npatches from other views.\ntiny small\nlocal blocks 12 11 10 8 4 0 12 11 10 8 4 0\nglobal blocks 0 1 2 4 8 12 0 1 2 4 8 12\nw/o pre-train 90.42 91.02 91.30 92.35 92.07 91.13 92.57 92.35 92.73 93.12 92.79 92.35\nw/ pre-train 94.55 94.16 94.38 94.82 94.82 94.66 94.77 94.71 95.10 95.21 95.04 95.21\nGPU memory (M) 1523 1528 2114 2548 3394 4289 3072 3817 4242 5095 6843 8586\ntime/epoch (s) 15 17 17 20 23 26 24 25 27 33 40 49\nthroughput (obj/s) 53 50 47 47 36 18 25 23 23 19 16 15\nTable 2: Evaluation of our method with different numbers of block layers on ModelNet10\ndataset, where w/o pre-train denotes training from scratch and w/ pre-train means ﬁne-tuning\nfrom a pre-trained model. The view number is ﬁxed as 6. We set the batch size (the number\nof 3D objects per batch) as 8 when testing the GPU memory cost. The inference throughput\nis measured as the number of 3D objects processed per second on an NVIDIA TITAN X\nPascal GPU. Setting 4 global transformer blocks strikes a good balance between interrelate\nintra-view and inter-view on patches.\nAccuracy. To investigate how the number of global transformer blocks and local transformer\nblocks inﬂuence the performance, we use the ModelNet10 as a testbed to ablate. The results\nare shown in Table 2. When training the tiny model from scratch, we can see that when we\nadd global transformer blocks, the performance improves compared to the local baseline, no\nmatter how deep. Training the small model without ﬁne-tuning on the pre-trained model, in-\ncluding global transformer blocks, has better performance in most cases. The increase proves\nthat adding global Transformer blocks to communicate patches from different views helps\nthe classiﬁcation performance. The local baseline achieves 90.42% using the tiny model\nand 93.12% using the small model. When including 4 global Transformer blocks with the\ntiny model, the performance achieves the highest with an accuracy of 92.35% compared to\n90.42% of local baseline, compared to 92.57% of local baseline. However, increasing the\nnumber of global Transformer blocks does not always guarantee better performance. When\nwe have more than 4 global Transformer blocks, the accuracy declines. Less local Trans-\nformer blocks, leading to fewer layers where patches only attend to intra-view patches, could\ncause such a decrease. We suppose intra-view attention for low layers is essential. If we al-\n8 CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER\nter all local transformer blocks with global transformer blocks, there is no layer to restrict\nthe patches interrelate with other patches only within its view. Using all global transformer\nblocks is another setting we call global baseline. However, when ﬁne-tuned on pre-trained\nmodels, more global Transformer blocks do not always lead to better performance.\nTo investigate the choice of the number of global Transformer blocks, we keep ﬁxed local\nTransformer blocks while gradually increasing the global Transformer blocks from 0 to 6.\nTable 3 displays the accuracy on the ModelNet10 test set with a different number of global\nTransformer blocks. We observe that more global Transformer blocks do not always lead to\nbetter performance. The accuracy of 5 and 6 global Transformer blocks is lower than of 4.\nMore global Transformer blocks have more trainable parameters. From Table 2 and Table 3\nwe conclude that 4 global transformer blocks is a good trade-off and we set as the default.\nglobal 0 1 2 3 4 5 6\naccuracy 90.42 91.46 91.57 92.02 92.35 91.85 91.96\nTable 3: Evaluating the number of global Transformer blocks when ﬁxing the number of\nthe local Transformer blocks.\nEfﬁciency. Meanwhile, in the last rows of Table 2, we show the GPU memory cost per\nbatch and the time cost per epoch of different settings. We include the inference speed in\nTable 2 as well. It is shown that when we use more global blocks, the GPU memory and\nthe training time increase accordingly. At the same time, the inference speed decrease corre-\nspondingly. Speciﬁcally, using the small model, when the number of global blocks increases\nfrom 0 to 12, the GPU memory cost increases from 3072M to 8586M, the training time cost\nper epoch increases from 24 seconds to 49 seconds, and the throughput decreases from 25\nobjects/second to 15 objects/second. Considering both effectiveness and efﬁciency, we set\nthe number of the local Transformer blocks as 8 and the number of the global Transformer\nblock as 4, by default.\nTiny vs Small. From Table 2 we can also see the performance improvement of using a\nlarger model. The small model has 6 heads with a hidden dimension of 384. The number\nof heads and dimensions is double to the tiny model, leading to better performance. Take 4\nglobal Transformer blocks as an example. The accuracy increases from 92.35% (tiny without\npre-train) to 93.12% (small without pre-train).\n5.2 The inﬂuence of the number of projected views\nAccuracy. We evaluate the effect of the number of views on the average instance accuracy\nof our method on the ModelNet10 dataset. As shown in Table 4, more projected views\nlead to better classiﬁcation accuracy. Speciﬁcally, using a single view, a small model with\npre-training only achieves a 89 .98% recognition accuracy, whereas it achieves a 95 .26%\nrecognition accuracy using 12 views. It is expected since more views will provide more\nvisual information for a 3D object and beneﬁt the 3D object recognition.\nEfﬁciency. We report the number of views on the GPU memory consumption and time cost\nper epoch in the last two rows of Table 4. As shown in the table, using more views leads to\nmore computational cost and GPU memory cost. In detail, using the small model, the GPU\nmemory per batch increases from 1010M to 10700M when the number of views increases\nCHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER 9\ntiny small\nviews 1 3 6 12 1 3 6 12\nw/o pre-train 82.54 89.70 92.35 92.47 85.19 92.35 93.12 93.13\nw/ pre-train 90.64 94.00 94.82 95.01 89.98 94.82 95.12 95.26\nGPU memory (M) 365 1457 2548 5455 1010 2354 5095 10700\ntime/epoch (s) 5 11 20 33 25 14 33 68\nTable 4: Evaluation of our method with different view numbers on ModelNet10 datasets.\nfrom 1 to 12. We recommend to use only 3 projected views when the computing resources\nare limited since it has achieved a excellent accuracy. Meanwhile, we suggest to use 12\nprojected views when computing resources are abundant.\n5.3 The inﬂuence of the class token\nIn the current settings, we feed the attended class token feature in the output of the last\nTransformer block to the classiﬁer to obtain the recognition result. A choice is to average-\npool all attended patch features in the output of the last Transformer block to generate a\nglobal representation for classiﬁcation. We investigate the effectiveness of leveraging class\ntokens compared to average-pooling patch features. The tiny model is trained from scratch\non ModelNet10 with 6 views. In Table 5, we show the experimental results. The improved\nperformance shows that the attended class token achieves a better performance than its coun-\nterpart using the global feature obtained from average-pooling the attended patch features.\navg_pool class_token\naccuracy 91.45% 92.35%\nTable 5: Comparisons between the global feature from average pooling the attended patch\nfeatures and the attend class token feature.\n5.4 Comparsions with state-of-the-art methods\nWe compare with three groups of methods including volume-based methods, point-based\nmethods and view-based methods in Table 6. The ﬁrst part of Table 6 reports the performance\nof volume-based methods including 3DShapeNets [35], V oxNet [22], V olumetric CNN [24],\n3D-A-Nets [27], and LP-3DCNN [19]. As shown in the table, the recognition accuracy of\nthese volume-based methods are not competitive compared with view-based methods.\nThen we compare with point-based methods including PointNet [25], PointNet++ [26],\n3DmFV-Net [3], and DeepCCFV [15]. Compared with volume-based methods, point-based\nmethods achieve considerably higher recognition accuracy. To be speciﬁc, PointNet++ [26]\nachieves 91.9 recognition accuracy. It signiﬁcantly outperforms the best volume-based method\nin Table 6, 3D-A-Nets [27], with only 90.5 recognition accuracy. But the point-based meth-\nods are still not as competitive as their view-based counterparts.\nAt last, we compare with view-based methods including MVCNN [28], RotationNet [17],\n3D2SeqViews [11], SeqViews2SeqLabels [12], Relation Network [37] and CARNet [36]\n10 CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER\nMethod Views ModelNet40 ModelNet10\nV olume-based methods\n3DShapeNets [35] - 77.0 83.5\nV oxNet [22] - 83.0 92.0\nV olumetric CNN [24] - 89.9 -\n3D-A-Nets [27] - 90.5 -\nLP-3DCNN [19] - 92.1 -\nPoint-based methods\nPointNet [25] - 89.2 -\nPointNet++ [26] - 91.9 -\n3DmFV-Net [3] - 91.6 95.2\nDeepCCFV [15] - 92.5 -\nView-based methods\nMVCNN [28] 80 90.1 -\nRotationNet [17] 12 91.0 94.0\nRotationNet [17] 20 97.4 98.5\nRelation Network [37] 12 94.3 95.3\n3D2SeqViews [11] 12 93.4 94.7\nSeqViews2SeqLabels [12] 12 93.4 94.8\nGVCNN [8] 12 93.1 -\nCARNet [36] 12 95.2 95.8\nCARNet [36] 20 97.7 99.0\nMVT-small (Ours) 12 94.4 95.3\nMVT-small (Ours) 20 97.5 99.3\nTable 6: Comparison with the present state-of-the-art methods on ModelNet40 dataset.\non both 12-view and 20-view settings. With more views, the performance achieved using\n20-view settings usually is better than 12-view settings. Compared with these methods,\nour MVT-small model achieves competitive performance. Speciﬁcally, on the ModelNet10\ndataset, using 20-view settings, we reach the highest recognition accuracy. It is worth not-\ning that our MVT-small architecture is conceptually simple with more minor hand-designed\ncomponents than the compared methods such as Relation Network [37] and CARNet [36].\n6 Conclusion\nIn this paper, we propose a multi-view vision Transformer (MVT) for effective 3D object\nrecognition. Considering the efﬁciency, we design our MVT in a local-global structure. The\nglobal Transformer layers empower each patch to communicate with the patches from all\nviews, overcoming the limitations of existing CNN-based models with a local reception ﬁeld\non patches from the same view. Although the proposed MVT is in a conceptually simple\nstructure, it has achieved state-of-the-art recognition performance on public benchmarks,\nincluding ModelNet40 and ModelNet10 datasets.\nCHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER 11\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv\npreprint arXiv:1607.06450, 2016.\n[2] Song Bai, Xiang Bai, Zhichao Zhou, Zhaoxiang Zhang, and Longin Jan Latecki. GIFT:\nA real-time and scalable 3d shape search engine. In Proceedings of the 2016 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 5023–5032,\nLas Vegas, NV , 2016.\n[3] Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fischer. 3DmFV: Three-\ndimensional point cloud classiﬁcation in real-time using convolutional neural networks.\nIEEE Robotics Autom. Lett., 3(4):3145–3152, 2018.\n[4] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei,\nHuaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in\nvision transformers. In arXiv preprint arXiv:2104.13840, 2021.\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A\nlarge-scale hierarchical image database. In Proceedings of the 2009 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition (CVPR), pages 248–\n255, Miami, FL, 2009.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-\ntraining of deep bidirectional transformers for language understanding. In Jill Burstein,\nChristy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT), pages 4171–4186, Minneapolis, MN, 2019.\n[7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiao-\nhua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. In Proceedings of the 9th International\nConference on Learning Representations (ICLR), Virtual Event, 2021.\n[8] Yifan Feng, Zizhao Zhang, Xibin Zhao, Rongrong Ji, and Yue Gao. GVCNN: group-\nview convolutional neural networks for 3d shape recognition. In Proceedings of the\n2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages\n264–272, Salt Lake City, UT, 2018.\n[9] Peng Gao, Jiasen Lu, Hongsheng Li, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nContainer: Context aggregation network. arXiv preprint arXiv:2106.01401, 2021.\n[10] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Trans-\nformer in transformer. arXiv preprint arXiv:2103.00112, 2021.\n[11] Zhizhong Han, Honglei Lu, Zhenbao Liu, Chi-Man V ong, Yu-Shen Liu, Matthias\nZwicker, Junwei Han, and C. L. Philip Chen. 3D2SeqViews: Aggregating sequential\nviews for 3d global feature learning by CNN with hierarchical attention aggregation.\nIEEE Trans. Image Process., 28(8):3986–3999, 2019.\n12 CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER\n[12] Zhizhong Han, Mingyang Shang, Zhenbao Liu, Chi-Man V ong, Yu-Shen Liu, Matthias\nZwicker, Junwei Han, and C. L. Philip Chen. SeqViews2SeqLabels: Learning 3d global\nfeatures via aggregating sequential views by RNN with attention. IEEE Trans. Image\nProcess., 28(2):658–672, 2019.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proceedings of the 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 770–778, Las Vegas, NV , 2016.\n[14] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and\nSeong Joon Oh. Rethinking spatial dimensions of vision transformers. arXiv:\n2103.16302, 2021.\n[15] Zhengyue Huang, Zhehui Zhao, Hengguang Zhou, Xibin Zhao, and Yue Gao. Deep-\nCCFV: Camera constraint-free multi-view convolutional neural network for 3d object\nretrieval. In Proceedings of the Thirty-Third AAAI Conference on Artiﬁcial Intelligence\n(AAAI), pages 8505–8512, Honolulu, HI, 2019.\n[16] Edward Johns, Stefan Leutenegger, and Andrew J. Davison. Pairwise decomposition of\nimage sequences for active multi-view recognition. In Proceedings of the 2016 IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , pages 3813–3822,\nLas Vegas, NV , 2016.\n[17] Asako Kanezaki, Yasuyuki Matsushita, and Yoshifumi Nishida. RotationNet: Joint\nobject categorization and pose estimation using multiviews from unsupervised view-\npoints. In Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5010–5019, Salt Lake City, UT, 2018.\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. In Advances in Neural Information Processing\nSystems (NIPS), pages 1106–1114, Lake Tahoe, NV , 2012.\n[19] Sudhakar Kumawat and Shanmuganathan Raman. LP-3DCNN: unveiling local phase\nin 3d convolutional neural networks. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 4903–4912, Long Beach, CA,\n2019.\n[20] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and\nBaining Guo. Swin transformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021.\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceed-\nings of the 7th International Conference on Learning Representations (ICLR) , New\nOrleans, LA, 2019.\n[22] Daniel Maturana and Sebastian A. Scherer. V oxNet: A 3d convolutional neural network\nfor real-time object recognition. In Proceedings of the 2015 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS), pages 922–928, Hamburg, Ger-\nmany, 2015.\nCHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER 13\n[23] Hsien-Yu Meng, Lin Gao, Yu-Kun Lai, and Dinesh Manocha. VV-Net: V oxel V AE\nnet with group convolutions for point cloud segmentation. In Proceedings of the 2019\nIEEE/CVF International Conference on Computer Vision (ICCV) , pages 8499–8507,\nSeoul, Korea, 2019.\n[24] Charles Ruizhongtai Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan, and\nLeonidas J. Guibas. V olumetric and multi-view CNNs for object classiﬁcation on 3d\ndata. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5648–5656, Las Vegas, NV , 2016.\n[25] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. PointNet:\nDeep learning on point sets for 3d classiﬁcation and segmentation. In Proceedings\nof the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\npages 77–85, Honolulu, HI, 2017.\n[26] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. PointNet++: Deep\nhierarchical feature learning on point sets in a metric space. In Advances in Neural\nInformation Processing Systems (NIPS), pages 5099–5108, Long Beach, CA, 2017.\n[27] Mengwei Ren, Liang Niu, and Yi Fang. 3d-a-nets: 3d deep dense descriptor for volu-\nmetric shapes with adversarial networks. arXiv preprint arXiv:1711.10108, 2017.\n[28] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik G. Learned-Miller. Multi-\nview convolutional neural networks for 3d shape recognition. In Proceedings of the\n2015 IEEE International Conference on Computer Vision (ICCV), pages 945–953, San-\ntiago, Chile, 2015.\n[29] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Hervé Jégou. Training data-efﬁcient image transformers & distillation\nthrough attention. In Proceedings of the 38th International Conference on Machine\nLearning (ICML), pages 10347–10357, Virtual Event, 2021.\n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in\nNeural Information Processing Systems (NIPS) , pages 5998–6008, Long Beach, CA,\n2017.\n[31] Chu Wang, Marcello Pelillo, and Kaleem Siddiqi. Dominant set clustering and pooling\nfor multi-view 3d object recognition. In Proceedings of the British Machine Vision\nConference (BMVC), London, UK, 2017.\n[32] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong\nLu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[33] Xin Wei, Ruixuan Yu, and Jian Sun. View-GCN: View-based graph convolutional\nnetwork for 3d shape analysis. In Proceedings of the 2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages 1847–1856, Seattle, W A,\n2020.\n14 CHEN, YU, LI: MULTI-VIEW VISION TRANSFORMER\n[34] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and\nLei Zhang. CvT: Introducing convolutions to vision transformers. arXiv preprint\narXiv:2103.15808, 2021.\n[35] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang,\nand Jianxiong Xiao. 3D ShapeNets: A deep representation for volumetric shapes.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 1912–1920, Boston, MA, 2015.\n[36] Yong Xu, Chaoda Zheng, Ruotao Xu, Yuhui Quan, and Haibin Ling. Multi-view 3d\nshape recognition via correspondence-aware deep learning. IEEE Trans. Image Pro-\ncess., 30:5299–5312, 2021.\n[37] Ze Yang and Liwei Wang. Learning relationships for multi-view 3d object recognition.\nIn Proceedings of the 2019 IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 7504–7513, Seoul, Korea, 2019.\n[38] Tan Yu, Jingjing Meng, and Junsong Yuan. Multi-view harmonized bilinear network\nfor 3d object recognition. In Proceedings of the 2018 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 186–194, Salt Lake City, UT, 2018.\n[39] Tan Yu, Jingjing Meng, Ming Yang, and Junsong Yuan. 3d object representation learn-\ning: A set-to-set matching perspective. IEEE Trans. Image Process., 30:2168–2179,\n2021.\n[40] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH\nTay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token ViT: Training vision transform-\ners from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7323537468910217
    },
    {
      "name": "Transformer",
      "score": 0.7277154922485352
    },
    {
      "name": "Cognitive neuroscience of visual object recognition",
      "score": 0.6510723829269409
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6281381249427795
    },
    {
      "name": "Limiting",
      "score": 0.5850949287414551
    },
    {
      "name": "Computer vision",
      "score": 0.5279126763343811
    },
    {
      "name": "Block (permutation group theory)",
      "score": 0.41327735781669617
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3645932972431183
    },
    {
      "name": "Object (grammar)",
      "score": 0.3282005190849304
    },
    {
      "name": "Engineering",
      "score": 0.16487544775009155
    },
    {
      "name": "Electrical engineering",
      "score": 0.08194056153297424
    },
    {
      "name": "Voltage",
      "score": 0.08018982410430908
    },
    {
      "name": "Mathematics",
      "score": 0.06401640176773071
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 19
}