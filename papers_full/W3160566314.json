{
  "title": "Self-Supervised Learning with Swin Transformers",
  "url": "https://openalex.org/W3160566314",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2744126523",
      "name": "Xie, Zhenda",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753706201",
      "name": "Lin, Yutong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224962495",
      "name": "Yao, Zhuliang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2003519119",
      "name": "Zhang Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104488438",
      "name": "Dai, Qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2113229502",
      "name": "Cao Yue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117015939",
      "name": "Hu Han",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3135958856",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3172615411",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3035060554"
  ],
  "abstract": "We are witnessing a modeling shift from CNN to Transformers in computer vision. In this work, we present a self-supervised learning approach called MoBY, with Vision Transformers as its backbone architecture. The approach basically has no new inventions, which is combined from MoCo v2 and BYOL and tuned to achieve reasonably high accuracy on ImageNet-1K linear evaluation: 72.8% and 75.0% top-1 accuracy using DeiT-S and Swin-T, respectively, by 300-epoch training. The performance is slightly better than recent works of MoCo v3 and DINO which adopt DeiT as the backbone, but with much lighter tricks. More importantly, the general-purpose Swin Transformer backbone enables us to also evaluate the learnt representations on downstream tasks such as object detection and semantic segmentation, in contrast to a few recent approaches built on ViT/DeiT which only report linear evaluation results on ImageNet-1K due to ViT/DeiT not tamed for these dense prediction tasks. We hope our results can facilitate more comprehensive evaluation of self-supervised learning methods designed for Transformer architectures. Our code and models are available at https://github.com/SwinTransformer/Transformer-SSL, which will be continually enriched.",
  "full_text": "Self-Supervised Learning with Swin Transformers\nZhenda Xie∗†13 Yutong Lin∗†23 Zhuliang Yao†13 Zheng Zhang3 Qi Dai3 Yue Cao3 Han Hu3\n1Tsinghua University 2Xi’an Jiaotong University\n3Microsoft Research Asia\n{xzd18,yzl17}@mails.tsinghua.edu.cn yutonglin@stu.xjtu.edu.cn\n{zhez,qid,yuecao,hanhu}@microsoft.com\nAbstract\nWe are witnessing a modeling shift from CNN to Transformers in computer vision.\nIn this work, we present a self-supervised learning approach called MoBY, with\nVision Transformers as its backbone architecture. The approach basically has no\nnew inventions, which is combined from MoCo v2and BYOL and tuned to achieve\nreasonably high accuracy on ImageNet-1K linear evaluation: 72.8% and 75.0%\ntop-1 accuracy using DeiT-S and Swin-T, respectively, by 300-epoch training. The\nperformance is slightly better than recent works of MoCo v3 and DINO which\nadopt DeiT as the backbone, but with much lighter tricks.\nMore importantly, the general-purpose Swin Transformer backbone enables us\nto also evaluate the learnt representations on downstream tasks such as object\ndetection and semantic segmentation, in contrast to a few recent approaches built\non ViT/DeiT which only report linear evaluation results on ImageNet-1K due\nto ViT/DeiT not tamed for these dense prediction tasks. We hope our results\ncan facilitate more comprehensive evaluation of self-supervised learning methods\ndesigned for Transformer architectures. Our code and models are available at\nhttps://github.com/SwinTransformer/Transformer-SSL, which will be\ncontinually enriched.\n1 Introduction\nThe vision ﬁeld is undergoing two revolutionary trends since about two years ago. The ﬁrst trend\nis self-supervised visual representation learning pioneered by MoCo [ 9], which for the ﬁrst time\ndemonstrated superior transferring performance on seven downstream tasks over the previous standard\nsupervised methods by ImageNet-1K classiﬁcation. The second is the Transformer-based backbone\narchitecture [7, 16, 14], which has strong potential to replace the previous standard convolutional\nneural networks such as ResNet [ 11]. The pioneer work is ViT [ 7], which demonstrated strong\nperformance on image classiﬁcation by directly applying the standard Transformer encoder [ 17]\nin NLP on non-overlapping image patches. The follow-up work, DeiT [16], tuned several training\nstrategies to make ViT work well on ImageNet-1K image classiﬁcation. While ViT/DeiT are designed\nfor the image classiﬁcation task and has not been well tamed for downstream tasks requiring dense\nprediction, Swin Transformer [14] is proposed to serve as a general-purpose vision backbone by\nintroducing useful inductive biases of locality, hierarchy and translation invariance.\nWhile the two revolutionary waves appeared independently, the community is curious about what\nkind of adaptation is needed and what it will behave when they meet each other. Nevertheless, until\nvery recently, a few works started to explore this space: MoCo v3 [ 6] presents a training recipe\nto let ViT perform reasonably well on ImageNet-1K linear evaluation; DINO [ 3] presents a new\nself-supervised learning method which shows good synergy with the Transformer architecture.\n∗Equal contribution. †Interns at MSRA.\narXiv:2105.04553v2  [cs.CV]  11 May 2021\nAlthough these works produce encouraging results on ImageNet-1K linear evaluation, there are\nno assessment of the transferring performance on downstream tasks such as object detection and\nsemantic segmentation, probably due to that ViT/DeiT are not well tamed for these downstream tasks.\nTo enable more comprehensive evaluations of the self-supervised learnt representations on also these\ndownstream tasks, we propose to adopt Swin Transformer as the backbone architecture instead of the\nprevious used ViT architecture, thanks to that Swin Transformer is designed as general-purpose and\nperforms strong on downstream tasks.\nIn addition to this backbone architecture change, we also present a self-supervised learning approach\nby combining MoCo v2 [5] and BYOL [8], named MoBY (by picking the ﬁrst two letters of each).\nWe tune a training recipe to make the approach performing reasonably high on ImageNet-1K linear\nevaluation: 72.8% top-1 accuracy using DeiT-S with 300-epoch training which is slightly better than\nthat in MoCo v3 and DINO but with lighter tricks. Using Swin-T architecture instead of DeiT-S, it\nachieves 75.0% top-1 accuracy with 300-epoch training, which is 2.2% higher than that using DeiT-S.\nInitial study shows that some tricks in MoCo v3 and DINO are also useful for MoBY , e.g. replacing\nthe LayerNorm layers before the MLP blocks by BatchNorm like that in MoCo v3 bring additional\n+1.1% gains using 100 epoch training, indicating the strong potential of MoBY .\nWhen transferred to downstream tasks of COCO object detection and ADE20K semantic segmenta-\ntion, the representations learnt by this self-supervised learning approach achieves on par performance\ncompared to the supervised method. Noting self-supervised learning with ResNet architectures\nhas shown signiﬁcantly stronger transferring performance on downstream tasks than supervised\nmethods [9, 19, 12], the results indicate large space to improve for self-supervised learning with\nTransformers.\nThe proposed approach basically has no new inventions. What we provide is an approach which\ncombines the previous good practice but with lighter tricks, associated with tuned hyper-parameters to\nachieve reasonably high accuracy on ImageNet-1K linear evaluation. We also provide baselines to aid\nthe evaluation of transferring performance on downstream tasks for the future study of self-supervised\nlearning on Transformer architectures.\n2 A Baseline SSL Method with Swin Transformers\nMoBY: a self-supervised learning approach MoBY is a combination of two popular self-\nsupervised learning approaches: MoCo v2 [ 5] and BYOL [ 8]. It inherits the momentum design,\nthe key queue, and the contrastive loss used in MoCo v2, and inherits the asymmetric encoders,\nasymmetric data augmentations and the momentum scheduler in BYOL. We name itMoBY by picking\nthe ﬁrst two letters of each method.\nThe MoBY approach is illustrated in Figure 1. There are two encoders: an online encoder and a target\nencoder. Both two encoders consist of a backbone and a projector head (2-layer MLP), and the online\nencoder introduces an additional prediction head (2-layer MLP), which makes the two encoders\nasymmetric. The online encoder is updated by gradients, and the target encoder is a moving average\nof the online encoder by momentum updating in each training iteration. A gradually increasing\nmomentum updating strategy is applied for on the target encoder: the value of momentum term is\ngradually increased to 1 during the course of training. The default starting value is 0.99.\nA contrastive loss is applied to learn the representations. Speciﬁcally, for an online view q, its\ncontrastive loss is computed as\nLq = −log exp(q·k+/τ)∑K\ni=0 exp(q·ki/τ)\n, (1)\nwhere k+ is the target feature for the other view of the same image; ki is a target feature in the key\nqueue; τ is a temperature term; Kis the size of the key queue (4096 by default).\nIn training, like most Transformer-based methods, we also adopt the AdamW [13, 15] optimizer, in\ncontrast to previous self-supervised learning approaches built on ResNet backbone where usually\nSGD [9, 2] or LARS [4, 8, 19] is used. We also introduce a regularization method of asymmetric\ndrop pathwhich proves crucial for the ﬁnal performance.\n2\nIn the experiments, we adopt a ﬁxed learning rate of 0.001 and a ﬁxed weight decay of 0.05, which\nperforms stably well. We tune hyper-parameters of the key queue size K, the starting momentum\nvalue of the target branch, the temperature τ, and the drop path rates.\nA pseudo code of MoBY in a PyTorch-like style is shown in Algorithm 1.\nProjector\nTarget: Momentum Update Queue\nTransformer\nProjectorTransformer Predictorv\nKeys\nOnline: Optimizer Update\nQueries\nContrastive\nLoss\nv'\nInput\nFigure 1: The pipeline of MoBY .\nSwin Transformer as the backbone Swin Transformer is a general-purpose backbone for com-\nputer vision and achieved state-of-the-art performance on various vision tasks such as COCO object\ndetection (58.7 box AP and 51.1 mask AP on test-dev set) and ADE20K semantic segmentation\n(53.5 mIoU on validation set). It is basically a hierarchical Transformer whose representation is\ncomputed with shifted windows. The shifted windowing scheme brings greater efﬁciency by limiting\nself-attention computation to non-overlapping local windows while also allowing for cross-window\nconnection.\nIn this work, we adopt the tiny version of Swin Transformer (Swin-T) as our default backbone, such\nthat the transferring performance on downstream tasks of object detection and semantic segmentation\ncan be also evaluated. The Swin-T has similar complexity with ResNet-50 and DeiT-S. The details of\nspeciﬁc architecture design and hyper-parameters can be found in [14].\n3 Experiments\n3.1 Linear Evaluation on ImageNet-1K\nLinear evaluation on ImageNet-1K dataset is a common evaluation protocol to assess the quality of\nlearnt representations [9]. In this protocol, a linear classiﬁer is applied on the backbone, with the\nbackbone weights frozen and only the linear classiﬁer trained. After training this linear classiﬁer, the\ntop-1 accuracy using center crop is reported on the validation set.\nDuring training, we follow [ 9] to use random resize cropping with scale from [0.08,1] and hor-\nizontal ﬂipping as the data augmentation. 100-epoch training with a 5-epoch linear warm-up\nstage is conducted. The weight decay is set as 0. The learning rate is set as the optimal one\nof {0.5,0.75,1.0,1.25}through grid search for each pre-trained model.\nTable 1 listed the major results of pre-trained models using different self-supervised learning methods\nand backbone architectures.\nComparison with other SSL methods using Transformer architectures Regarding previous\nmethods such as MoCo v3 [6] and DINO [3] adopt ViT/DeiT as their backbone architecture, we ﬁrst\nreport results of MoBY using DeiT-S [16] for fair comparison with them. Under 300-epoch training,\nMoBY achieves 72.8% top-1 accuracy, which is slightly better than MoCo v3 and DINO (without the\nmulti-crop trick), as shown in Table 1.\nWe note that MoCo v3 and DINO adopt heavy tricks to achieve the same accuracy as ours:\n• Tricks in MoCo v3 [6]. MoCo v3 adopts a ﬁxed patch embedding, batch normalization\nlayers to replace the layer normalization ones before the MLP blocks, and a 3-layer MLP\nhead. It also uses large batch size (i.e. 4096) which is unaffordable for many research labs.\n3\nAlgorithm 1: Pseudo code of MoBY in a PyTorch-like style.\n# encoder: transformer-based encoder\n# proj: projector\n# pred: predictor\n# odpr: online drop path rate\n# tdpr: target drop path rate\n# m: momentum coefficient\n# t: temperature coefficient\n# queue1, queue2: two queues for storing negative samples\nf_online = lambda x: pred(proj(encoder(x, drop_path_rate=odpr)))\nf_target = lambda x: proj(encoder(x, drop_path_rate=tdpr))\nfor v1, v2 in loader: # load two views\nq1, q2 = f_online(v1), f_online(v2) # queries: NxC\nk1, k2 = f_target(v1), f_target(v2) # keys: NxC\n# symmetric loss\nloss = contrastive_loss(q1, k2, queue2) + contrastive_loss(q2, k1, queue1)\nloss.backward()\nupdate(f_online) # optimizer update: f_online\nf_target = m * f_target + (1. - m) * f_online # momentum update: f_target\nupdate(m) # update momentum coefficient\ndef contrastive_loss(q, k, queue):\n# positive logits: Nx1\nl_pos = torch.einsum(’nc,nc->n’, [q, k.detach()]).unsqueeze(-1)\n# negative logits: NxK\nl_neg = torch.einsum(’nc,ck->nk’, [q, queue.clone().detach()])\n# logits: Nx(1+K)\nlogits = torch.cat([l_pos, l_neg], dim=1)\n# labels: positive key indicators\nlabels = torch.zeros(N)\nloss = F.cross_entropy(logits / t, labels)\n# update queue\nenqueue(queue, k)\ndequeue(queue)\nreturn loss\n• Tricks in DINO [3]. DINO adopts asymmetric temperatures between student and teacher,\na linearly warmed-up teacher temperature, varying weight decay during pre-training, the\nlast layer ﬁxed at the ﬁrst epoch, tuning whether to put weight normalization in the head, a\nconcatenation of the last few blocks or CLS tokens as the input to the linear classiﬁer, and\netc.\nIn contrast, we mainly adopt standard settings from MoCo v2 [ 5] and BYOL [8], and use a small\nbatch size of 512 such that the experimental settings will be affordable for most labs. We have also\nstarted to try applying some tricks of MoCo v3 [6]/DINO [3] to MoBY , though they are not included\nin the standard settings. Our initial exploration reveals that the ﬁxed patch embedding has no use to\nMoBY , and replacing the layer normalization layers before the MLP blocks by batch normalization\ncan bring +1.1% top-1 accuracy using 100-epoch training, as shown in Table 2. This indicates that\nsome of these tricks may be useful for the MoBY approach, and the MoBY approach has potential\nto achieve much higher accuracy on ImageNet-1K linear evaluation. This will be left as our future\nstudy.\nSwin-T v.s. DeiT-S We also compare the use of different Transformer architectures in self-\nsupervised learning. As shown in Table 1, Swin-T achieves 75.0% top-1 accuracy, surpassing\nDeiT-S by +2.2%. Also note the performance gap is larger than that of using supervised learning\n(+1.5%).\n4\nMethod Arch. Epochs Params (M) FLOPs (G) img/s Top-1 acc (%)\nSup. DeiT-S 300 22 4.6 940.4 79.8\nSup. Swin-T 300 29 4.5 755.2 81.3\nMoCo v3 DeiT-S 300 22 4.6 940.4 72.5\nDINO DeiT-S 300 22 4.6 940.4 72.5\nDINO† DeiT-S 300 22 4.6 940.4 75.9\nMoBY DeiT-S 300 22 4.6 940.4 72.8\nMoBY Swin-T 100 29 4.5 755.2 70.9\nMoBY Swin-T 300 29 4.5 755.2 75.0\nTable 1: Comparison of different SSL methods and different Transformer architectures on ImageNet-\n1K linear evaluation. †denotes DINO with a multi-crop scheme in training.\nFixed Patch Embedding Replace LN before MLP with BN Top-1 acc (%)\n70.9\n✓ 70.8\n✓ 72.0\nTable 2: Initial study of applying tricks in MoCo v3 to the MoBY approach using 100-epoch training\nand Swin-T backbone architecture. Note although replacing the layer norm layer before each MLP\nblock with a batch norm layer performs better (72.0 vs. 70.9), it changes the original Swin architecture\nand is currently not used as our standard settings in experiments. We leave more comprehensive study\nof Transformer architecture improvements in the context of SSL as our future work.\n3.2 Transferring Performance on Downstream Tasks\nWe evaluate the transferring performance of the learnt representation on downstream tasks of COCO\nobject detection/instance segmentation and ADE20K semantic segmentation.\nCOCO object detection and instance segmentation Two detectors are adopted in the evaluation:\nMask R-CNN [10] and Cascade Mask R-CNN [1], following the implementation of [14]1. Table 3\nshows the comparison of the learnt representation by MoBY and the pretrained supervised method\nin [14], in both 1x and 3x settings. For each experiment, we follow all the settings used for supervised\npre-trained models [14], except that we tune the drop path rate in {0,0.1,0.2}and report the best\nresults (for also supervised models).\nIt can be seen that the representations learnt by the self-supervised method (MoBY) and the supervised\nmethod are similarly well on transferring performance. While we note that previous SSL works\nusing ResNet as the backbone architecture usually report stronger performance over the supervised\nmethods [9, 19, 12], no gains over supervised methods are observed using Transformer architectures.\nWe hypothesis it is partly because the supervised pre-training on Transformers has involved strong\ndata augmentations [16, 14], while supervised training of ResNet usually employs much weaker\ndata augmentation. These results also imply space to improve for self-supervised learning using\nTransformer architectures.\nADE20K Semantic Segmentation The UPerNet approach [ 18] and the ADE20K dataset are\nadopted in the evaluation, following [ 14] 2. The ﬁne-tuning and testing settings also follow [ 14]\nexcept that the learning rate of each experiment is tuned using{3×10−5,6×10−5,1×10−4}. Table 4\nshows the comparisons of supervised and self-supervised pre-trained models on this evaluation. It\nindicates that MoBY performs slightly worse than the supervised method, implying a space to\nimprove for self-supervised learning using Transformer architectures.\n1https://github.com/SwinTransformer/Swin-Transformer-Object-Detection\n2https://github.com/SwinTransformer/Swin-Transformer-Semantic-Segmentation\n5\nMethod Model Schd. box AP mask AP\nmAPbbox APbbox\n50 APbbox\n75 mAPmask APmask\n50 APmask\n75\nSwin-T\n(mask R-CNN)\nSup. 1x 43.7 66.6 47.7 39.8 63.3 42.7\nMoBY 1x 43.6 66.2 47.7 39.6 62.9 42.2\nSup. 3x 46.0 68.1 50.3 41.6 65.1 44.9\nMoBY 3x 46.0 67.8 50.6 41.7 65.0 44.7\nSwin-T\n(Cascade\nmask R-CNN)\nSup. 1x 48.1 67.1 52.2 41.7 64.4 45.0\nMoBY 1x 48.1 67.1 52.1 41.5 64.0 44.7\nSup. 3x 50.4 69.2 54.7 43.7 66.6 47.3\nMoBY 3x 50.2 68.8 54.7 43.5 66.1 46.9\nTable 3: Comparison of the supervised method by ImageNet-1K classiﬁcation and the self-supervised\nMoBY approach on transferring performance to COCO object detection and instance segmentation.\nMethod Model Schd. mIoU\nSwin-T\n(UPerNet)\nSup. 160K 44.51\nMoBY 160K 44.06\nSup.† 160K 45.81\nMoBY† 160K 45.58\nTable 4: Comparison of the supervised method by ImageNet-1K classiﬁcation and the self-supervised\nMoBY approach on transferring performance to ADE20K semantic segmentation. †denotes the\nresults with multi-scale testing techniques.\n3.3 Ablation Study\nWe perform ablation study using the ImageNet-1K linear evaluation protocol. Swin-T is used as the\nbackbone architecture. In each ablation, we vary one hyper-parameter and other hyper-parameters\nare set as the default ones.\nEpochs Online dpr Targetdpr Top-1 acc (%)\n100 0.05 0.0 70.9\n100 0.1 0.0 70.9\n100 0.2 0.0 70.9\n100 0.1 0.1 69.0\n300 0.05 0.0 74.2\n300 0.1 0.0 75.0\n300 0.2 0.0 75.0\nTable 5: Ablation study on the drop path rates of online and target encoders.\nAsymmetric drop path rates is beneﬁcial Drop path has proved a useful regularization for\nsupervised representation learning using the image classiﬁcation task and Transformer architec-\ntures [16, 14]. We also ablate the effect of this regularization in Table 5. Increasing the drop path\nregularization from 0.05 to 0.1 to the online encoder is beneﬁcial for representation learning, espe-\ncially in longer training, probably due to the relief of over-ﬁtting. Additionally adding drop path\nregularization to the target encoder results in 1.9% top-1 accuracy drop (70.9% to 69.0%), indicating\na harm. We thus adopt an asymmetric drop path rates in pre-training.\n6\nOther hyper-parameters Table 6a ablates the effect ofkey queue size Kfrom 1024 to 16384. The\napproach stably performs across various K (from 1024 to 16384), and we adopt 4096 as default.\nTable 6b ablates the effect of temperatureτ and 0.2 performs best which is set as the default value.\nTable 6c ablates the effect of the starting momentum value of the target encoder. 0.99 performs best\nand is set as the default value.\nK Top-1 (%)\n1024 71.0\n2048 70.8\n4096∗ 70.9\n8192 71.0\n16384 70.8\n(a) Queue Size K\nτ Top-1 (%).\n0.07 62.7\n0.1 67.7\n0.2∗ 70.9\n0.3 70.8\n(b) Temperature τ\nStart value Top-1 (%)\n0.99∗ 70.9\n0.993 70.7\n0.996 70.5\n0.999 67.6\n(c) Momentum of target encoder\nTable 6: Ablation study on other hyper-parameters using 100-epoch training. ∗denotes the default\nvalues.\n4 Conclusion\nIn this paper, we present a self-supervised learning approach called MoBY, with Vision Transformers\nas its backbone architecture. With a proper training recipe and much lighter tricks than MoCo\nv3/DINO, MoBY can achieve reasonably high performance on ImageNet-1K linear evaluation: 72.8%\nand 75.0% top-1 accuracy using DeiT-S and Swin-T, respectively, by 300-epoch training. More\nimportantly, in contrast to ViT/DeiT, the general-purpose Swin Transformer backbone enables us to\nalso evaluate the learnt representations on downstream tasks such as object detection and semantic\nsegmentation. MoBY can perform comparably or slightly worse than the supervised methods,\nindicating a space to improve for self-supervised learning with Transformer architectures. We hope\nour results can facilitate more comprehensive evaluation of self-supervised learning methods designed\nfor Transformer architectures. Our code and models are available and will be continually enriched at\nhttps://github.com/SwinTransformer/Transformer-SSL.\n7\nReferences\n[1] Cai, Z. and Vasconcelos, N. (2018). Cascade r-cnn: Delving into high quality object detection. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 6154–6162.\n[2] Cao, Y ., Xie, Z., Liu, B., Lin, Y ., Zhang, Z., and Hu, H. (2020). Parametric instance classiﬁcation\nfor unsupervised visual feature learning. In Advances in neural information processing systems.\n[3] Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021).\nEmerging properties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294.\n[4] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020a). A simple framework for contrastive\nlearning of visual representations. arXiv preprint arXiv:2002.05709.\n[5] Chen, X., Fan, H., Girshick, R., and He, K. (2020b). Improved baselines with momentum\ncontrastive learning. arXiv preprint arXiv:2003.04297.\n[6] Chen, X., Xie, S., and He, K. (2021). An empirical study of training self-supervised visual\ntransformers. arXiv preprint arXiv:2104.02057.\n[7] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., De-\nhghani, M., Minderer, M., Heigold, G., Gelly, S., et al. (2020). An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n[8] Grill, J.-B., Strub, F., Altché, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doersch, C.,\nPires, B. A., Guo, Z. D., Azar, M. G., et al. (2020). Bootstrap your own latent: A new approach to\nself-supervised learning.\n[9] He, K., Fan, H., Wu, Y ., Xie, S., and Girshick, R. (2020). Momentum contrast for unsupervised\nvisual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9729–9738.\n[10] He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2017). Mask r-cnn. In Proceedings of the\nIEEE international conference on computer vision, pages 2961–2969.\n[11] He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.\n[12] Hénaff, O. J., Koppula, S., Alayrac, J.-B., Oord, A. v. d., Vinyals, O., and Carreira, J. (2021).\nEfﬁcient visual pretraining with contrastive detection. arXiv preprint arXiv:2103.10957.\n[13] Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\n[14] Liu, Z., Lin, Y ., Cao, Y ., Hu, H., Wei, Y ., Zhang, Z., Lin, S., and Guo, B. (2021). Swin trans-\nformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030.\n[15] Loshchilov, I. and Hutter, F. (2017). Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101.\n[16] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jégou, H. (2020).\nTraining data-efﬁcient image transformers and distillation through attention. arXiv preprint\narXiv:2012.12877.\n[17] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and\nPolosukhin, I. (2017). Attention is all you need. In Advances in Neural Information Processing\nSystems, pages 5998–6008.\n[18] Xiao, T., Liu, Y ., Zhou, B., Jiang, Y ., and Sun, J. (2018). Uniﬁed perceptual parsing for scene\nunderstanding. In European Conference on Computer Vision. Springer.\n[19] Xie, Z., Lin, Y ., Zhang, Z., Cao, Y ., Lin, S., and Hu, H. (2021). Propagate yourself: Exploring\npixel-level consistency for unsupervised visual representation learning. In Proceedings of the\nIEEE conference on computer vision and pattern recognition.\n8",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5422888398170471
    },
    {
      "name": "Computer science",
      "score": 0.4004899263381958
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3594322204589844
    },
    {
      "name": "Psychology",
      "score": 0.3483322858810425
    },
    {
      "name": "Business",
      "score": 0.33175474405288696
    },
    {
      "name": "Engineering",
      "score": 0.25399577617645264
    },
    {
      "name": "Electrical engineering",
      "score": 0.18084239959716797
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}