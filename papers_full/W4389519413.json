{
  "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
  "url": "https://openalex.org/W4389519413",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3034332558",
      "name": "Kelong Mao",
      "affiliations": [
        "Next Generation Technology (United States)",
        "Renmin University of China",
        "Search (Poland)"
      ]
    },
    {
      "id": "https://openalex.org/A2330149811",
      "name": "Zhicheng Dou",
      "affiliations": [
        "Search (Poland)",
        "Renmin University of China",
        "Next Generation Technology (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3103285559",
      "name": "Fengran Mo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3019016469",
      "name": "Hou Jiewen",
      "affiliations": [
        "Institute of Computing Technology",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2150598006",
      "name": "Haonan Chen",
      "affiliations": [
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A2303229503",
      "name": "Hongjin Qian",
      "affiliations": [
        "Renmin University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3099446234",
    "https://openalex.org/W3035169992",
    "https://openalex.org/W4367047001",
    "https://openalex.org/W4385565351",
    "https://openalex.org/W3198536471",
    "https://openalex.org/W4385572937",
    "https://openalex.org/W4385573473",
    "https://openalex.org/W3115037692",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4385571112",
    "https://openalex.org/W4385573151",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385573600",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4385571189",
    "https://openalex.org/W4385573081",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4280579537",
    "https://openalex.org/W4385570369",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3176182290",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W4287822877",
    "https://openalex.org/W4224491635",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W3214455632",
    "https://openalex.org/W3142278688",
    "https://openalex.org/W3014434762",
    "https://openalex.org/W4385569686",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W3160883893",
    "https://openalex.org/W4313680149",
    "https://openalex.org/W4385889719",
    "https://openalex.org/W3027639267",
    "https://openalex.org/W4284685693",
    "https://openalex.org/W4221151914",
    "https://openalex.org/W3175474240",
    "https://openalex.org/W3034439313",
    "https://openalex.org/W3171244865",
    "https://openalex.org/W4327525277",
    "https://openalex.org/W4367368866",
    "https://openalex.org/W3103251620",
    "https://openalex.org/W4385567884"
  ],
  "abstract": "Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1211‚Äì1225\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nLarge Language Models Know Your Contextual Search Intent:\nA Prompting Framework for Conversational Search\nKelong Mao1,2, Zhicheng Dou1,2‚àó, Fengran Mo3, Jiewen Hou4,\nHaonan Chen1, Hongjin Qian1\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2Engineering Research Center of Next-Generation Search and Recommendation, MOE\n3Universit√© de Montr√©al, Qu√©bec, Canada\n4Institute of Computing Technology, Chinese Academy of Sciences\n{mkl,dou}@ruc.edu.cn\nAbstract\nPrecisely understanding users‚Äô contextual\nsearch intent has been an important challenge\nfor conversational search. As conversational\nsearch sessions are much more diverse and\nlong-tailed, existing methods trained on lim-\nited data still show unsatisfactory effectiveness\nand robustness to handle real conversational\nsearch scenarios. Recently, large language\nmodels (LLMs) have demonstrated amazing\ncapabilities for text generation and conversa-\ntion understanding. In this work, we present\na simple yet effective prompting framework,\ncalled LLM4CS, to leverage LLMs as a text-\nbased search intent interpreter to help conver-\nsational search. Under this framework, we\nexplore three prompting methods to generate\nmultiple query rewrites and hypothetical re-\nsponses, and propose to aggregate them into\nan integrated representation that can robustly\nrepresent the user‚Äôs real contextual search in-\ntent. Extensive automatic evaluations and hu-\nman evaluations on three widely used conversa-\ntional search benchmarks, including CAsT-19,\nCAsT-20, and CAsT-21, demonstrate the re-\nmarkable performance of our simple LLM4CS\nframework compared with existing methods\nand even using human rewrites. Our findings\nprovide important evidence to better understand\nand leverage LLMs for conversational search.\nThe code is released at https://github.com/\nkyriemao/LLM4CS.\n1 Introduction\nConversational search has been expected to be the\nnext generation of search paradigms (Culpepper\net al., 2018). It supports search via conversation\nto provide users with more accurate and intuitive\nsearch results and a much more user-friendly search\nexperience. Unlike using traditional search en-\ngines which mainly process keyword queries, users\ncould imagine the conversational search system as\na knowledgeable human expert and directly start a\n‚àó ‚àóCorresponding author.\nmulti-turn conversation with it in natural languages\nto solve their questions. However, one of the main\nchallenges for this beautiful vision is that the users‚Äô\nqueries may contain some linguistic problems (e.g.,\nomissions and coreference) and it becomes much\nharder to capture their real search intent under the\nmulti-turn conversation context (Dalton et al., 2021;\nMao et al., 2022a).\nTo achieve conversational search, an intuitive\nmethod known as Conversational Query Rewriting\n(CQR) involves using a rewriting model to trans-\nform the current query into a de-contextualized\nform. Subsequently, any ad-hoc search models\ncan be seamlessly applied for retrieval purposes.\nGiven that existing ad-hoc search models can be\nreused directly, CQR demonstrates substantial prac-\ntical value for industries in quickly initializing their\nconversational search engines. Another type of\nmethod, Conversational Dense Retrieval (CDR),\ntries to learn a conversational dense retriever to en-\ncode the user‚Äôs real search intent and passages into\nlatent representations and performs dense retrieval.\nIn contrast to the two-step CQR method, where the\nrewriter is difficult to be directly optimized towards\nsearch (Yu et al., 2021; Mao et al., 2023a), the con-\nversational dense retriever can naturally learn from\nsession-passage relevance signals.\nHowever, as conversational search sessions are\nmuch more diverse and long-tailed (Mao et al.,\n2022b; Dai et al., 2022; Mo et al., 2023a), existing\nCQR and CDR methods trained on limited data\nstill show unsatisfactory performance, especially\non more complex conversational search sessions.\nMany studies (Vakulenko et al., 2021b; Lin et al.,\n2021a; Qian and Dou, 2022; Krasakis et al., 2022)\nhave demonstrated the performance advantages of\nusing de-contextualized human rewrites on ses-\nsions which have complex response dependency.\nAlso, as reported in the public TREC CAsT 2021\nbenchmark (Dalton et al., 2022), existing methods\nstill suffer from significant degradation in their ef-\n1211\nfectiveness as conversations become longer.\nRecently, large language models (LLMs) have\nshown amazing capabilities for text generation and\nconversation understanding (Brown et al., 2020;\nWei et al., 2022; Thoppilan et al., 2022; Zhu et al.,\n2023). In the field of information retrieval (IR),\nLLMs have also been successfully utilized to en-\nhance relevance modeling via various techniques\nsuch as query generation (Bonifacio et al., 2022;\nDai et al., 2023), query expansion (Wang et al.,\n2023a), document prediction (Gao et al., 2022;\nMackie et al., 2023), etc. Inspired by the strong\nperformance of LLMs in conversation and IR, we\ntry to investigate how LLMs can be adapted to\nprecisely grasp users‚Äô contextual search intent for\nconversational search.\nIn this work, we present a simple yet effective\nprompting framework, called LLM4CS, to leverage\nLLM as a search intent interpreter to facilitate con-\nversational search. Specifically, we first prompt\nLLM to generate both short query rewrites and\nlonger hypothetical responses in multiple perspec-\ntives and then aggregate these generated contents\ninto an integrated representation that robustly rep-\nresents the user‚Äôs real search intent. Under our\nframework, we propose three specific prompting\nmethods and aggregation methods, and conduct\nextensive evaluations on three widely used con-\nversational search benchmarks, including CAsT-\n19 (Dalton et al., 2020), CAsT-20 (Dalton et al.,\n2021), and CAsT-21 (Dalton et al., 2022)), to com-\nprehensively investigate the effectiveness of LLMs\nfor conversational search.\nIn general, our framework has two main advan-\ntages. First, by leveraging the powerful contextual\nunderstanding and generation abilities of large lan-\nguage models, we show that additionally generat-\ning hypothetical responses to explicitly supplement\nmore plausible search intents underlying the short\nrewrite can significantly improve the search perfor-\nmance. Second, we show that properly aggregating\nmultiple rewrites and hypothetical responses can\neffectively filter out incorrect search intents and en-\nhance the reasonable ones, leading to better search\nperformance and robustness.\nOverall, our main contributions are:\n‚Ä¢ We propose a prompting framework and de-\nsign three tailored prompting methods to lever-\nage large language models for conversational\nsearch, which effectively circumvents the se-\nrious data scarcity problem faced by the con-\nQuery Encoder\nContext & Question\nSearch Intent\nInterpreter (LLM)\nREW RTR RAR\nPassage Vectors\n(Offline Encoded)\nRewrite\nResponse\nRewrite LLM\nRewrite\nResponse\n√ó ùëÅ\nAggregation\nMaxProbSC Mean\n..\nSearch Intent\nVector\nFigure 1: An overview of LLM4CS.\nversational search field.\n‚Ä¢ We show that additionally generating hypo-\nthetical responses and properly aggregating\nmultiple generated results are crucial for im-\nproving search performance.\n‚Ä¢ We demonstrate the exceptional effectiveness\nof LLMs for conversational search through\nboth automatic and human evaluations, where\nthe best method in our LLM4CS achieves\nremarkable improvements in search perfor-\nmance over state-of-the-art CQR and CDR\nbaselines, surpassing even human rewrites.\n2 Related Work\nConversational Search. Conversational search is\nan evolving field that involves retrieving relevant\ninformation based on multi-turn dialogues with\nusers. To achieve conversational search, two main\nmethods have been developed: conversational\nquery rewriting and conversational dense retrieval.\nConversational query rewriting converts the\nconversational search problem into an ad-hoc\nsearch problem by reformulating the search session\ninto a standalone query rewrite. Existing methods\ntry to select useful tokens from the conversation\ncontext (V oskarides et al., 2020; Lin et al., 2021b)\n1212\nor train a generative rewriter based on the pairs\nof sessions and rewrites (Lin et al., 2020; Yu\net al., 2020; Vakulenko et al., 2021a). To make\nthe rewriting process aware of the downstream\nretrieval process, some studies propose to adopt\nreinforcement learning (Wu et al., 2022; Chen\net al., 2022) or enhance the learning of rewriter\nwith ranking signals (Mao et al., 2023a; Mo et al.,\n2023a). On the other hand, conversational dense\nretrieval (Yu et al., 2021) directly encodes the\nwhole conversational search session to perform\nend-to-end dense retrieval. Existing methods\nmainly try to improve the session representation\nthrough context denoising (Mao et al., 2022a;\nKrasakis et al., 2022; Mo et al., 2023b; Mao et al.,\n2023b), data augmentation (Lin et al., 2021a; Mao\net al., 2022b; Dai et al., 2022), and hard negative\nmining (Kim and Kim, 2022).\nIR with LLMs. Due to the revolutionary natu-\nral language understanding and generation abili-\nties, LLMs are attracting more and more attention\nfrom the IR community. LLMs have been lever-\naged to enhance the relevance modeling of retrieval\nthrough query generation (Bonifacio et al., 2022;\nJeronymo et al., 2023; Dai et al., 2023), query ex-\npansion (Wang et al., 2023a), document predic-\ntion (Gao et al., 2022; Mackie et al., 2023), etc.\nBesides, Shen et al. (2023) proposed to first use\nthe retriever to enhance the generation of LLM\nand then use the generated content to augment the\noriginal search query for better retrieval. Ziems\net al. (2023) treated LLM as a built-in search en-\ngine to retrieve documents based on the generated\nURL. There are also some works leveraging LLM\nto perform re-ranking (Sun et al., 2023; Jiang et al.,\n2023). Different from previous studies, in this pa-\nper, we propose the LLM4CS framework that fo-\ncuses on studying how LLM can be well utilized\nto capture the user‚Äôs contextual search intent to\nfacilitate conversational search.\n3 LLM4CS: Prompting Large Language\nModels for Conversational Search\nIn this section, we introduce our LLM4CS frame-\nwork, which leverages LLM as a text-based search\nintent interpreter to facilitate conversational search.\nFigure 1 shows an overview of LLM4CS. In the\nfollowing, we first describe our task formulation\nof conversational search, and then we elaborate on\nthe specific prompting methods and aggregation\nmethods integrated into the framework. Finally, we\nintroduce the retrieval process.\n3.1 Task Formulation\nWe focus on the task of conversational passage\nretrieval, which is the crucial first step of con-\nversational search that helps the model access\nthe right evidence knowledge. Given the user\nquery qt and the conversation context Ct =\n(q1, r1, ..., qt‚àí1, rt‚àí1) of the current turn t, where\nqi and ri denote the user query and the system\nresponse of the historical i-th turn, our goal is to re-\ntrieve passages that are relevant to satisfy the user‚Äôs\nreal search intent of the current turn.\n3.2 Prompting Methods\nThe prompt follows the formulation of [Instruc-\ntion, Demonstrations, Input], where Input is com-\nposed of the query qt and the conversation con-\ntext Ct of the current turn t. Figure 4 shows\na general illustration of the prompt construc-\ntion.1 Specifically, we design and explore three\nprompting methods, including Rewriting (REW),\nRewriting-Then-Response (RTR), and Rewriting-\nAnd-Response (RAR), in our LLM4CS framework.\n3.2.1 Rewriting Prompt (REW)\nIn this prompting method, we directly treat LLM\nas a well-trained conversational query rewriter and\nprompt it to generate rewrites. Only the red part\nof Figure 4 is enabled. Although straightforward,\nwe show in Section 4.5 that this simple prompting\nmethod has been able to achieve quite a strong\nsearch performance compared to existing baselines.\n3.2.2 Rewriting-Then-Response (RTR)\nRecently, a few studies (Mao et al., 2021; Gao\net al., 2022; Yu et al., 2023; Mackie et al., 2023)\nhave shown that generating hypothetical responses\nfor search queries can often bring positive improve-\nments in retrieval performance. Inspired by them,\nin addition to prompting LLM to generate rewrites,\nwe continue to utilize the generated rewrites to\nfurther prompt LLM to generate hypothetical re-\nsponses that may contain relevant information to\nanswer the current question. The orange part and\nthe blue part of Figure 4 are enabled. Specifically,\nwe incorporate the pre-generated rewrite (i.e., the\norange part) into the Input field of the prompt and\n1We put this figure in Appendix A due to the space limita-\ntion. See our open-sourced code for the full prompt of each\nprompting method.\n1213\nthen prompt LLM to generate informative hypo-\nthetical responses by referring to the rewrite.\n3.2.3 Rewriting-And-Response (RAR)\nInstead of generating rewrites and hypothetical re-\nsponses in a two-stage manner, we can also gener-\nate them all at once with the red part and the blue\npart of Figure 4 being enabled. We try to explore\nwhether this one-stage generation could lead to bet-\nter consistency and accuracy between the generated\nrewrites and responses, compared with the two-step\nRTR method.\n3.2.4 Incorporating Chain-of-Thought\nChain-of-thought (CoT) (Wei et al., 2020) induces\nthe large language models to decompose a reason-\ning task into multiple intermediate steps which can\nunlock their stronger reasoning abilities. In this\nwork, we also investigate whether incorporating\nthe chain-of-thought of reasoning the user‚Äôs real\nsearch intent could improve the quality of rewrite\nand response generation.\nSpecifically, as shown in the green part of Fig-\nure 4, we manually write the chain-of-thought for\neach turn of the demonstration, which reflects how\nhumans infer the user‚Äôs real search intent of the\ncurrent turn based on the historical conversation\ncontext. When generating, we instruct LLM to first\ngenerate the chain-of-thought before generating\nrewrites (and responses). We investigate the effects\nof our proposed CoT tailored to the reasoning of\ncontextual search intent in Section 4.6.\n3.3 Content Aggregation\nAfter prompting LLM multiple times to generate\nmultiple rewrites and hypothetical responses, we\nthen aggregate these generated contents into an\nintegrated representation to represent the user‚Äôs\ncomplete search intent for search. Let us con-\nsider that we have generated N query rewrites\nQ= (ÀÜq1, ...,ÀÜqN ) and M hypothetical responses\nR= (ÀÜri1, ...,ÀÜriM ) for each rewrite ÀÜqi, sorted by\ntheir generation probabilities from high to low 2.\nNote that in RAR prompting, the rewrites and\nthe hypothetical responses are always generated in\npairs (i.e., M = 1). While in RTR prompting, one\nrewrite can have M hypothetical responses since\nthey are generated in a two-stage manner. Next,\nwe utilize a dual well-trained ad-hoc retriever3 f\n2That is, the generation probability orders are: P(ÀÜq1) ‚â•\n... ‚â• P(ÀÜqN) and P(ÀÜri1) ‚â• ... ‚â• P(ÀÜriM).\n3The parameters of the query encoder and the passage\nencoder are shared.\n(e.g, ANCE (Xiong et al., 2021)) to encode each\nof them into a high-dimensional intent vector and\naggregate these intent vectors into one final search\nintent vector s. Specifically, we design and explore\nthe following three aggregation methods, including\nMaxProb, Self-Consistency (SC), and Mean, in our\nLLM4CS framework.\n3.3.1 MaxProb\nWe directly use the rewrite and the hypothetical\nresponse that have the highest generation proba-\nbilities. Therefore, compared with the other two\naggregation methods that will be introduced later,\nMaxProb is highly efficient since it actually does\nnot require multiple generations.\nFormally, for REW prompting:\ns = f( ÀÜq1). (1)\nFor the RTR and RAR prompting methods, we\nmix the rewrite and hypothetical response vectors:\ns = f(ÀÜq1) +f(ÀÜr11)\n2 . (2)\n3.3.2 Self-Consistency (SC)\nThe multiple generated rewrites and hypothetical\nresponses may express different search intents but\nonly some of them are correct. To obtain a more\nreasonable and consistent search intent represen-\ntation, we extend the self-consistency prompting\nmethod (Wang et al., 2023b), which was initially\ndesigned for reasoning tasks with predetermined\nanswer sets, to our contextual search intent under-\nstanding task, which lacks a fixed standard answer.\nTo be specific, we select the intent vector that is\nthe most similar to the cluster center of all intent\nvectors as the final search intent vector, since it\nrepresents the most popular search intent overall.\nFormally, for REW prompting:\nÀÜ q‚àó = 1\nN\nN‚àë\ni=1\nf(ÀÜqi), (3)\ns = arg max\nf(ÀÜqi)\nf(ÀÜqi)‚ä§¬∑ÀÜ q‚àó, (4)\nwhere ÀÜ q‚àóis the cluster center vector and ¬∑denotes\nthe dot product that measures the similarity.\nFor RTR prompting, we first select the intent\nvector f(ÀÜqk) and then select the intent vectorf(ÀÜrkz)\nfrom all hypothetical responses generated based on\n1214\nthe selected rewrite ÀÜqk:\nk = arg max\ni\nf(ÀÜqi)‚ä§¬∑ÀÜ q‚àó, (5)\nÀÜ r‚àó\nk = 1\nM\nM‚àë\nj=1\nf(ÀÜrkj), (6)\nz = arg max\nj\nf(ÀÜrkj)‚ä§¬∑ÀÜ r‚àó\nk, (7)\ns = f(ÀÜqk) +f(ÀÜrkz)\n2 , (8)\nwhere k and z are the finally selected indexes of\nthe rewrite and the response, respectively.\nThe aggregation for RAR prompting is similar\nto RTR prompting, but it does not need response\nselection since there is only one hypothetical re-\nsponse for each rewrite:\ns = f(ÀÜqk) +f(ÀÜrk1)\n2 . (9)\n3.3.3 Mean\nWe average all the rewrite vectors and the corre-\nsponding hypothetical response vectors.\nFor REW prompting:\ns = 1\nN\nN‚àë\ni=1\nf(ÀÜqi). (10)\nFor the RTR and RAR prompting methods:\ns =\n‚àëN\ni=1[f(ÀÜqi) +‚àëM\nj=1 f(ÀÜrij)]\nN ‚àó(1 +M) . (11)\nCompared with MaxProb and Self-Consistency,\nthe Mean aggregation comprehensively considers\nmore diverse search intent information from all\nsources. It leverages the collaborative power to\nenhance the popular intents, but also supplements\nplausible intents that are missing in a single rewrite\nor a hypothetical response.\n3.4 Retrieval\nAll candidate passages are encoded into passage\nvectors using the same retriever f. At search time,\nwe return the passages that are most similar to the\nfinal search intent vector s as the retrieval results.\n4 Experiments\n4.1 Datasets and Metrics\nWe carry out extensive experiments on three widely\nused conversational search datasets: CAsT-19 (Dal-\nton et al., 2020), CAsT-20 (Dalton et al., 2021), and\nDataset CAsT-19 CAsT-20 CAsT-21\n# Conversations 20 25 18\n# Turns (Sessions) 173 208 157\n# Passages/Docs 38M 40M\nTable 1: Statistics of the three CAsT datasets.\nCAsT-21 (Dalton et al., 2022), which are curated by\nthe human experts of TREC Conversational Assis-\ntance Track (CAsT). Each CAsT dataset has dozens\nof information-seeking conversations comprising\nhundreds of turns. CAsT-19 and CAsT-20 share the\nsame retrieval corpora while CAsT-21 has a differ-\nent one. In contrast, CAsT-20 and CAsT-21 have a\nmore complex session structure than CAsT-19 as\ntheir questions may refer to previous responses. All\nthree datasets provide human rewrites and passage-\nlevel (or document-level) relevance judgments la-\nbeled by TREC experts. Table 1 summarizes the\nbasic dataset statistics.4\nFollowing previous work (Dalton et al., 2020,\n2021; Yu et al., 2021; Mao et al., 2022a), we adopt\nMean Reciprocal Rank (MRR), NDCG@3, and\nRecall@100 as our evaluation metrics and calcu-\nlate them using pytrec_eval tool (Van Gysel and\nde Rijke, 2018). We deem relevance scale ‚â•2\nas positive for MRR on CAsT-20 and CAsT-21.\nFor CAsT-21, we split the documents into pas-\nsages and score each document based on its highest-\nscored passage (i.e., MaxP (Dai and Callan, 2019)).\nWe conduct the statistical significance tests using\npaired t-tests at p <0.05 level.\n4.2 Implementation details\nWe use the OpenAI gpt3.5-turbo-16k as our LLM.\nThe decoding temperature is set to 0.7. We ran-\ndomly select three conversations from the CAsT-\n225 dataset for demonstration. CAsT-22 is a new\nconversational search dataset also proposed by\nTREC CAsT, but only its conversations are re-\nleased6 and the relevance judgments have not been\nmade public. Therefore, it cannot be used for eval-\nuation and we just use it for demonstration. For\nREW prompting, we set N = 5. For RTR prompt-\ning, we set N = 1and M = 5. For RAR prompt-\ning, we set N = 5, and M is naturally set to 1.\nFollowing previous studies (Yu et al., 2021; Mao\n4Only the turns that have relevance labels are counted.\n5https://github.com/daltonj/treccastweb/tree/\nmaster/2022\n6Until the submission deadline of EMNLP 2023.\n1215\net al., 2022a,b; Mo et al., 2023a), we adopt the\nANCE (Xiong et al., 2021) checkpoint pre-trained\non the MSMARCO dataset as our ad-hoc retriever\nf. We uniformly truncate the lengths of queries\n(or rewrites), passages, and hypothetical responses\ninto 64, 256, and 256.\n4.3 Baselines\nWe compare our few-shot LLM4CS against the\nfollowing six conversational search systems:\n(1) T5QR (Lin et al., 2020): A T5 (Raffel et al.,\n2020)-based conversational query rewriter trained\nwith the human rewrites.\n(2) ConvDR (Yu et al., 2021): A conversa-\ntional dense retriever fine-tuned from an ad-hoc\nretriever by mimicking the representations of hu-\nman rewrites.\n(3) COTED (Mao et al., 2022a): An improved\nversion of ConvDR (Yu et al., 2021) which incorpo-\nrates a curriculum learning-based context denoising\nobjective.\n(4) ZeCo (Krasakis et al., 2022): A variant\nof ColBERT (Khattab and Zaharia, 2020) that\nmatches only the contextualized terms of the cur-\nrent query with passages to perform zero-shot con-\nversational search.\n(5) CRDR (Qian and Dou, 2022): A conver-\nsational dense retrieval method where the dense\nretrieval part is enhanced by the distant supervision\nfrom query rewriting in a unified framework.\n(6) ConvGQR (Mo et al., 2023a): A query refor-\nmulation framework that combines query rewriting\nwith generative query expansion.\nT5QR, CRDR, and ConvGQR are trained on the\ntraining sessions of QReCC (Anantha et al., 2021),\nwhich is a large-scale conversational question an-\nswering dataset. The performances of ConvDR and\nCOTED are reported in the few-shot setting using\n5-fold cross-validation according to their original\npapers. We also present the performance of us-\ning human rewrites for reference. Note that the\nsame ANCE checkpoint is used to perform dense\nretrieval for all baselines except ZeCo to ensure\nfair comparisons.\n4.4 Main Results\nThe overall performance comparisons are presented\nin Table 2. The reported performance of LLM4CS\nresults from the combination of the RAR prompt-\ning method, the Mean aggregation method, and our\ntailored CoT, which shows to be the most effective\ncombination. We thoroughly investigate the effects\nof using different prompting and aggregation meth-\nods in Section 4.5 and investigate the effects of the\nincorporation of CoT in Section 4.6.\nFrom Table 2, we observe that LLM4CS outper-\nforms all the compared baselines in terms of search\nperformance. Specifically, LLM4CS exhibits a rel-\native improvement of over 18% compared to the\nsecond-best results on the more challenging CAsT-\n20 and CAsT-21 datasets across all metrics. In\nparticular, even compared to using human rewrites,\nour LLM4CS can still achieve better results on\nmost metrics, except for the Recall@100 of CAsT-\n19 and NDCG@3 of CAsT-21. These significant\nimprovements, which are unprecedented in prior\nresearch, demonstrate the strong superiority of our\nLLM4CS over existing methods and underscore\nthe vast potential of using large language models\nfor conversational search.\n4.5 Effects of Different Prompting Methods\nand Aggregation Methods\nWe present a comparison of NDCG@3 perfor-\nmance across various prompting and aggregation\nmethods (excluding the incorporation of CoT) in\nTable 3. Our findings are as follows:\nFirst, the RAR and RTR prompting methods\nclearly outperform the REW prompting, demon-\nstrating that the generated hypothetical responses\ncan effectively supplement the short query rewrite\nto improve retrieval performance. However, even\nthe simple REW prompting can also achieve quite\ncompetitive performance compared to existing\nbaselines, particularly on the more challenging\nCAsT-20 and CAsT-21 datasets, where it shows sig-\nnificant superiority (e.g., 0.380 vs. 0.350 on CAsT-\n20 and 0.465 vs. 0.385 on CAsT-21). These posi-\ntive results further highlight the significant advan-\ntages of utilizing LLM for conversational search.\nSecond, in terms of aggregation methods, both\nMean and SC consistently outperform MaxProb.\nThese results indicate that depending solely on the\ntop prediction of the language model may not pro-\nvide sufficient reliability. Instead, utilizing the col-\nlective strength of multiple results proves to be a\nbetter choice. Additionally, we observe that the\nMean aggregation method, which fuses all gen-\nerated contents into the final search intent vector\n(Equation 11), does not consistently outperform\nSC (e.g., on CAsT-20), which actually only fuses\none rewrite and one response (Equation 8). This\nsuggests that taking into account more generations\n1216\nSystem CAsT-19 CAsT-20 CAsT-21\nMRR NDCG@3 R@100 MRR NDCG@3 R@100 MRR NDCG@3 R@100\nConversational Dense Retrieval\nConvDR 0.740 0.466 0.362 0.510 0.340 0.345 0.573 0.385 0.483\nCOTED 0.769 0.478 0.367 0.491 0.342 0.340 0.565 0.371 0.485\nZeCo - 0.238 ‚Ä° 0.216‚Ä° - 0.176 ‚Ä° 0.200‚Ä° - 0.234 ‚Ä° 0.267‚Ä°\nCRDR 0.765 0.472 0.357 0.501 0.350 0.313 0.474 0.342 0.380\nConversational Query Rewriting\nT5QR 0.701 0.417 0.332 0.423 0.299 0.353 0.469 0.330 0.408\nConvGQR 0.708 0.434 0.336 0.465 0.331 0.368 0.433 0.273 0.330\nLLM4CS 0.776‚Ä† 0.515‚Ä† 0.372‚Ä† 0.615‚Ä† 0.455‚Ä† 0.489‚Ä† 0.681‚Ä† 0.492‚Ä† 0.614‚Ä†\nHuman 0.740 0.461 0.381 0.591 0.422 0.465 0.680 0.502 0.590\nRI-H +4.9% +11.7% -2.4% +4.1% +7.8% +5.2% +0.1% -2.0% +4.1%\nRI-2nd-Best +0.9% +7.7% +1.4% +20.6% +30.0% +32.9% +18.8% +27.8% +26.6%\nTable 2: Overall performance comparisons. ‚Ä°denotes the results are replicated from their original paper. ‚Ä†denotes\nLLM4CS (RAR + Mean + CoT) significantly outperforms all the compared baselines (except ZeCo) in the p <0.05\nlevel. The best results are bold and the second-best results are underlined. Human denotes using human rewrites.\nRI-H and RI-2nd-Best are the relative improvements over Human and the second-best results, respectively.\nAggregation CAsT-19 CAsT-20 CAsT-21\nREW RTR RAR REW RTR RAR REW RTR RAR\nMaxProb 0.441 0.459 0.464 0.356 0.415 0.430 0.407 0.469 0.462\nSC 0.449 0.466 0.476 0.362 0.432 0.444 0.445 0.473 0.469\nMean 0.447 0.464 0.488 0.380 0.425 0.442 0.465 0.481 0.478\nPrevious SOTA 0.478 0.350 0.385\nHuman 0.461 0.422 0.502\nTable 3: Performance comparisons with respect to NDCG@3 using different prompting and aggregation methods.\nThe best combination on each dataset is bold.\nMaxProb SC Mean\n0.34\n0.36\n0.38\n0.40\n0.42NDCG@3\n0.356\n0.362\n0.3800.378 0.382\n0.396\nCAsT-20: REW\nwithout CoT\nwith CoT\nMaxProb SC Mean\n0.39\n0.40\n0.41\n0.42\n0.43\n0.44\n0.45NDCG@3\n0.415\n0.432\n0.425\n0.417\n0.435 0.434\nCAsT-20: RTR\nwithout CoT\nwith CoT\nMaxProb SC Mean\n0.41\n0.42\n0.43\n0.44\n0.45\n0.46\n0.47NDCG@30.430\n0.444 0.442\n0.436\n0.448\n0.455\nCAsT-20: RAR\nwithout CoT\nwith CoT\nMaxProb SC Mean\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50NDCG@3\n0.407\n0.445\n0.465\n0.424\n0.464\n0.480\nCAsT-21: REW\nwithout CoT\nwith CoT\nMaxProb SC Mean\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0.50NDCG@3\n0.469\n0.473\n0.481\n0.470 0.471\n0.481\nCAsT-21: RTR\nwithout CoT\nwith CoT\nMaxProb SC Mean\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0.50NDCG@3\n0.462\n0.469\n0.478\n0.466\n0.474\n0.492\nCAsT-21: RAR\nwithout CoT\nwith CoT\nFigure 2: NDCG@3 comparisons between incorporating our tailored CoT or not across different prompting and\naggregation methods on CAsT-20 and CAsT-21 datasets.\nmay not always be beneficial, and a careful se-\nlection among them could be helpful to achieve\nimproved results.\n4.6 Effects of Chain-of-Thought\nWe show the ablation results of our tailored chain-\nof-thought in Figure 2. We also provide a real\nexample to show how our CoT takes effect in Ap-\npendix B.1. From the results, we observe that:\nIncorporating our chain-of-thought into all\nprompting and aggregation methods generally im-\nproves search performance. This demonstrates the\nefficacy of our chain-of-thought in guiding the large\nlanguage model towards a correct understanding of\n1217\nGood-H Good-A Bad-O Bad-C\n0\n20\n40\n60\n80\n100Percent\n68.2\n6.9\n17.9\n7.0\n80.9\n4.6\n11.6\n2.9\nCAsT-19 (173 turns)\nT5QR\nLLM4CS (REW)\nGood-H Good-A Bad-O Bad-C\n0\n20\n40\n60\n80\n100Percent37.5\n24.5 27.4\n10.6\n48.1\n41.3\n8.7\n1.9\nCAsT-20 (208 turns)\nT5QR\nLLM4CS (REW)\nGood-H Good-A Bad-O Bad-C\n0\n20\n40\n60\n80\n100Percent\n43.7\n14.9\n30.1\n11.3\n58.0\n26.8\n12.1\n3.1\nCAsT-21 (157 turns)\nT5QR\nLLM4CS (REW)\nFigure 3: Human evaluation results for LLM4CS (REW + MaxProb) and T5QR on the three CAsT datasets.\nthe user‚Äôs contextual search intent.\nIn contrast, the improvements are particularly\nnotable for the REW prompting method compared\nto the RTR and RAR prompting methods. It ap-\npears that the introduction of multiple hypothetical\nresponses diminishes the impact of the chain-of-\nthought. This could be attributed to the fact that\nincluding multiple hypothetical responses signifi-\ncantly boosts the quality and robustness of the final\nsearch intent vector, thereby reducing the promi-\nnence of the chain-of-thought in enhancing search\nperformance.\n5 Human Evaluation\nThe retrieval performance is influenced by the ad-\nhoc retriever used, which implies that automatic\nsearch evaluation metrics may not fully reflect the\nmodel‚Äôs capability to understand contextual search\nintent. Sometimes, two different rewrites can yield\nsignificantly different retrieval scores, even though\nthey both accurately represent the user‚Äôs real search\nintent. To better investigate the contextual search\nintent understanding ability of LLM, we perform\na fine-grained human evaluation on the rewrites\ngenerated by our LLM4CS (REW + MaxProb).\nSpecifically, we manually compare each model‚Äôs\nrewrite with the corresponding human rewrite and\nlabel it with one of the following four categories:\n(1) Good-H: The model‚Äôs rewrite is nearly the same\nas the human rewrite. (2) Good-A: The expression\nof the model‚Äôs rewrite is different from the human\nrewrite but it also successfully conveys the user‚Äôs\nreal search intent. (3) Bad-C: the rewrite has coref-\nerence errors. (4) Bad-O: the rewrite omits impor-\ntant contextual information or has other types of\nerrors. Furthermore, we apply the same principle\nto label the rewrites of T5QR for comparison pur-\nposes. A few examples of such categorization are\npresented in Appendix B.2.\nThe results of the human evaluation are shown\nin Figure 3, where we observe that:\n(1) From a human perspective, 85.5%, 89.4%,\nand 84.8% of the rewrites of LLM4CS success-\nfully convey the user‚Äôs real search intent for CAsT-\n19, CAsT-20, and CAsT-21, respectively. In con-\ntrast, the corresponding percentages for T5QR are\nmerely 75.1%, 62.0%, and 58.6%. Such a high\nrewriting accuracy of LLM4CS further demon-\nstrates the strong ability of LLM for contextual\nsearch intent understanding.\n(2) In the case of CAsT-20 and CAsT-21, a sig-\nnificantly higher percentage of rewrites are labeled\nas Good-A, in contrast to CAsT-19, where the ma-\njority of good rewrites closely resemble the hu-\nman rewrites. This can be attributed to the higher\ncomplexity of the session structure and questions\nin CAsT-20 and CAsT-21 compared to CAsT-19,\nwhich allows for greater freedom in expressing the\nsame search intent.\n(3) The rewrites generated by LLM4CS exhibit\ncoreference errors in less than 3% of the cases,\nwhereas T5QR‚Äôs rewrites contain coreference er-\nrors in approximately 10% of the cases. This ob-\nservation highlights the exceptional capability of\nLLM in addressing coreference issues.\n6 Conclusion\nIn this paper, we present a simple yet effective\nprompting framework (i.e., LLM4CS) that lever-\nages LLMs for conversational search. Our frame-\nwork generates multiple query rewrites and hypo-\nthetical responses using tailored prompting meth-\nods and aggregates them to robustly represent the\nuser‚Äôs contextual search intent. Through extensive\nautomatic and human evaluations on three CAsT\ndatasets, we demonstrate its remarkable perfor-\nmance for conversational search. Our study high-\nlights the vast potential of LLMs in conversational\nsearch and takes an important initial step in ad-\nvancing this promising direction. Future research\nwill focus on refining and extending the LLM4CS\nframework to explore better ways of generation to\nfacilitate search, improving aggregation techniques,\noptimizing the LLM-retriever interaction, and in-\ncorporating reranking strategies.\n1218\nLimitations\nOur work shows that generating multiple rewrites\nand hypothetical responses and properly aggregat-\ning them can effectively improve search perfor-\nmance. However, this requires invoking LLM mul-\ntiple times, resulting in a higher time cost for re-\ntrieval. Due to the relatively high generation la-\ntency of LLM, the resulting query latency would\nbe intolerable for users when compared to conven-\ntional search engines. A promising approach is to\ndesign better prompts capable of obtaining all infor-\nmative content in one generation, thereby signifi-\ncantly improving query latency. Another limitation\nis that, similar to the typical disadvantages of CQR\nmethods, the generation process of LLM lacks\nawareness of the downstream retrieval process. Ex-\nploring the utilization of ranking signals to enhance\nLLM generation would be a compelling direction\nfor future research of conversational search.\nAcknowledgement\nThis work was supported by the National Natural\nScience Foundation of China No. 62272467, Pub-\nlic Computing Cloud, Renmin University of China,\nand Intelligent Social Governance Platform, Major\nInnovation & Planning Interdisciplinary Platform\nfor the ‚ÄúDouble-First Class‚Äù Initiative, Renmin Uni-\nversity of China, and the Outstanding Innovative\nTalents Cultivation Funded Programs 2024 of Ren-\nmin University of China. The work was partially\ndone at Beijing Key Laboratory of Big Data Man-\nagement and Analysis Methods.\nReferences\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,\nShayne Longpre, Stephen Pulman, and Srinivas\nChappidi. 2021. Open-domain question answer-\ning goes conversational via question rewriting. In\nNAACL-HLT, pages 520‚Äì534. Association for Com-\nputational Linguistics.\nLuiz Henrique Bonifacio, Hugo Abonizio, Marzieh\nFadaee, and Rodrigo Frassetto Nogueira. 2022. In-\npars: Data augmentation for information retrieval us-\ning large language models. CoRR, abs/2202.05144.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nZhiyu Chen, Jie Zhao, Anjie Fang, Besnik Fetahu,\nRokhlenko Oleg, and Shervin Malmasi. 2022. Rein-\nforced question rewriting for conversational question\nanswering.\nJ Shane Culpepper, Fernando Diaz, and Mark D\nSmucker. 2018. Research frontiers in information\nretrieval: Report from the third strategic workshop on\ninformation retrieval in lorne (swirl 2018). In ACM\nSIGIR Forum, volume 52, pages 34‚Äì90. ACM New\nYork, NY , USA.\nZhuyun Dai and Jamie Callan. 2019. Deeper text un-\nderstanding for IR with contextual neural language\nmodeling. In Proceedings of the 42nd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, SIGIR 2019, Paris,\nFrance, July 21-25, 2019, pages 985‚Äì988. ACM.\nZhuyun Dai, Arun Tejasvi Chaganty, Vincent Y Zhao,\nAida Amini, Qazi Mamunur Rashid, Mike Green,\nand Kelvin Guu. 2022. Dialog inpainting: Turning\ndocuments into dialogs. In International Conference\non Machine Learning, pages 4558‚Äì4586. PMLR.\nZhuyun Dai, Vincent Y . Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B.\nHall, and Ming-Wei Chang. 2023. Promptagator:\nFew-shot dense retrieval from 8 examples. In 11th\nInternational Conference on Learning Representa-\ntions, ICLR 2023.\nJeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2020.\nTrec cast 2019: The conversational assistance track\noverview. In In Proceedings of TREC.\nJeffrey Dalton, Chenyan Xiong, and Jamie Callan.\n2021. Cast 2020: The conversational assistance track\noverview. In In Proceedings of TREC.\nJeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2022.\nTrec cast 2021: The conversational assistance track\noverview. In In Proceedings of TREC.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022. Precise zero-shot dense retrieval without rele-\nvance labels. CoRR, abs/2212.10496.\nVitor Jeronymo, Luiz Henrique Bonifacio, Hugo\nAbonizio, Marzieh Fadaee, Roberto de Alen-\ncar Lotufo, Jakub Zavrel, and Rodrigo Frassetto\nNogueira. 2023. Inpars-v2: Large language mod-\nels as efficient dataset generators for information re-\ntrieval. CoRR, abs/2301.01820.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.\nLlm-blender: Ensembling large language models\nwith pairwise ranking and generative fusion. CoRR,\nabs/2306.02561.\n1219\nOmar Khattab and Matei Zaharia. 2020. Colbert: Ef-\nficient and effective passage search via contextual-\nized late interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval\n(SIGIR), pages 39‚Äì48. ACM.\nSungdong Kim and Gangwoo Kim. 2022. Saving dense\nretriever from shortcut dependency in conversational\nsearch.\nAntonios Minas Krasakis, Andrew Yates, and Evangelos\nKanoulas. 2022. Zero-shot query contextualization\nfor conversational search. In Proceedings of the 45th\nInternational ACM SIGIR conference on research\nand development in Information Retrieval (SIGIR).\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2021a. Contextualized query embeddings for con-\nversational search. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,\nMing-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.\n2020. Conversational question reformulation via\nsequence-to-sequence architectures and pretrained\nlanguage models. arXiv preprint arXiv:2004.01909.\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,\nMing-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.\n2021b. Multi-stage conversational passage retrieval:\nAn approach to fusing term importance estimation\nand neural query rewriting. ACM Transactions on\nInformation Systems (TOIS), 39(4):1‚Äì29.\nIain Mackie, Shubham Chatterjee, and Jeffrey Dalton.\n2023. Generative relevance feedback with large lan-\nguage models. CoRR, abs/2304.13157.\nKelong Mao, Zhicheng Dou, Bang Liu, Hongjin Qian,\nFengran Mo, Xiangli Wu, Xiaohua Cheng, and Zhao\nCao. 2023a. Search-oriented conversational query\nediting. In ACL (Findings), volume ACL 2023 of\nFindings of ACL. Association for Computational Lin-\nguistics.\nKelong Mao, Zhicheng Dou, and Hongjin Qian. 2022a.\nCurriculum contrastive context denoising for few-\nshot conversational dense retrieval. In Proceedings\nof the 45th International ACM SIGIR conference on\nresearch and development in Information Retrieval\n(SIGIR).\nKelong Mao, Zhicheng Dou, Hongjin Qian, Fengran\nMo, Xiaohua Cheng, and Zhao Cao. 2022b. Con-\nvtrans: Transforming web search sessions for con-\nversational dense retrieval. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nKelong Mao, Hongjin Qian, Fengran Mo, Zhicheng\nDou, Bang Liu, Xiaohua Cheng, and Zhao Cao.\n2023b. Learning denoised and interpretable session\nrepresentation for conversational search. In Proceed-\nings of the ACM Web Conference, pages 3193‚Äì3202.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. Generation-augmented retrieval for open-\ndomain question answering. In ACL/IJCNLP (1),\npages 4089‚Äì4100. Association for Computational\nLinguistics.\nFengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu,\nKaiyu Huang, and Jian-Yun Nie. 2023a. ConvGQR:\ngenerative query reformulation for conversational\nsearch. In ACL, volume ACL 2023. Association for\nComputational Linguistics.\nFengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao,\nYutao Zhu, Peng Li, and Yang Liu. 2023b. Learning\nto relate to previous turns in conversational search.\nIn 29th ACM SIGKDD Conference On Knowledge\nDiscover and Data Mining (SIGKDD).\nHongjin Qian and Zhicheng Dou. 2022. Explicit query\nrewriting for conversational dense retrieval. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nTao Shen, Guodong Long, Xiubo Geng, Chongyang\nTao, Tianyi Zhou, and Daxin Jiang. 2023. Large lan-\nguage models are strong zero-shot retriever. CoRR,\nabs/2304.14233.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren,\nDawei Yin, and Zhaochun Ren. 2023. Is chatgpt\ngood at search? investigating large language models\nas re-ranking agent. CoRR, abs/2304.09542.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang,\nIgor Krivokon, Will Rusch, Marc Pickett, Kathleen S.\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Ag√ºera\ny Arcas, Claire Cui, Marian Croak, Ed H. Chi, and\nQuoc Le. 2022. Lamda: Language models for dialog\napplications. CoRR, abs/2201.08239.\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\nand Raviteja Anantha. 2021a. Question rewriting for\nconversational question answering. In Proceedings\nof the 14th ACM International Conference on Web\nSearch and Data Mining (WSDM), pages 355‚Äì363.\n1220\nSvitlana Vakulenko, Nikos V oskarides, Zhucheng Tu,\nand Shayne Longpre. 2021b. A comparison of ques-\ntion rewriting methods for conversational passage re-\ntrieval. In ECIR (2), volume 12657 of Lecture Notes\nin Computer Science, pages 418‚Äì424. Springer.\nChristophe Van Gysel and Maarten de Rijke. 2018.\nPytrec_eval: An extremely fast python interface to\ntrec_eval. In SIGIR. ACM.\nNikos V oskarides, Dan Li, Pengjie Ren, Evangelos\nKanoulas, and Maarten de Rijke. 2020. Query reso-\nlution for conversational search with limited supervi-\nsion. In Proceedings of the 43rd International ACM\nSIGIR conference on research and development in\nInformation Retrieval (SIGIR), pages 921‚Äì930.\nLiang Wang, Nan Yang, and Furu Wei. 2023a.\nQuery2doc: Query expansion with large language\nmodels. CoRR, abs/2303.07678.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2023b. Self-consistency\nimproves chain of thought reasoning in language\nmodels. In 11th International Conference on Learn-\ning Representations, ICLR 2023.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2020.\nChain of thought prompting elicits reasoning in large\nlanguage models. Advances in neural information\nprocessing systems.\nZeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter,\nand Gaurav Singh Tomar. 2022. Conqrr: Conversa-\ntional query rewriting for retrieval with reinforcement\nlearning.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021.\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul\nBennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-\nshot generative conversational query rewriting. In\nProceedings of the 43rd International ACM SIGIR\nconference on research and development in Informa-\ntion Retrieval (SIGIR), pages 1933‚Äì1936.\nShi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and\nZhiyuan Liu. 2021. Few-shot conversational dense\nretrieval. In Proceedings of the 44th International\nACM SIGIR conference on research and development\nin Information Retrieval (SIGIR).\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In 11th International Con-\nference on Learning Representations, ICLR 2023.\nYutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan\nLiu, Wenhan Liu, Chenlong Deng, Zhicheng Dou,\nand Ji-Rong Wen. 2023. Large language models\nfor information retrieval: A survey. arXiv preprint\narXiv:2308.07107.\nNoah Ziems, Wenhao Yu, Zhihan Zhang, and Meng\nJiang. 2023. Large language models are built-in au-\ntoregressive search engines. In ACL (Findings), vol-\nume ACL 2023 of Findings of ACL. Association for\nComputational Linguistics.\n1221\nAppendix\nA Prompt of LLM4CS\nFigure 4 shows a general illustration of the prompt\nof LLM4CS. The prompt consist of three parts,\nwhich are Instruction, Demonstration, and Input.\nThe red part is for REW prompting, the blue part is\nfor the RTR and RAR promptings, and the orange\npart is for RTR prompting. The green part is for\nour designed chain-of-thought.\nB Case Study\nB.1 Examples of Chain-of-Thought\nThe example in Table 4 shows how our CoT takes\neffect. The CoT and Our Rewritefields are gen-\nerated by LLM4CS (REW + CoT). We can find\nthat the generated CoT effectively illustrates the\nrationale behind the rewriting process. Please refer\nto our anonymously open-sourced repository for\nmore examples.\nB.2 Examples of Human Evaluation\nAn example for each category is shown in Table 5.\nPlease refer to our anonymously open-sourced\nrepository for more examples.\n1222\nInstructionFor an information-seeking dialog, please help reformulate the question into rewrite that can fully express the user‚Äòs information needs without the need of context,but also generate an informative response to answer the question.\nDemonstration\nI will give you several example multi-turn dialogs, where each turn contains a question,aresponse,andarewrite.The rewrite part begins with a sentence explaining the reason for therewrite.Example #1:Question: What should I consider when buying a phone?Rewrite: This is the first turn. So the question should be rewritten as: What should I consider when buying a phone?Response: The design of the phone and the overall ‚Ä¶Question: Cool. Which one would you recommend?Rewrite: Based on Turn 1, you are inquiring about what should be considered when buying a phone. So the question should be rewritten as: Cool. Which smartphone would you recommend for me?Response: Just because a phone has everything‚Ä¶‚Ä¶Example#2:‚Ä¶\nInput\nYour Task (only questions and responses aregiven):Context:Question: What was the basis of the Watergate scandal?Response: ...Question:‚Ä¶Response:‚Ä¶‚Ä¶CurrentQuestion:So what happened to Nixon?Rewrite:So what happened to Nixon after the events of the Watergate scandal?(Now, you should give me therewriteandaninformativeresponseof the **Current Question** basedonthe **Context**. The output format should always be: Rewrite: $Reason.Sothequestionshouldberewrittenas:$Rewrite\\nResponse:$Response. Go ahead!)ModelOutputRewrite: Based on all previous turns, Nixon was badly involved in the Watergate scandal. So the question should be rewritten as: So what happened to Nixon after the events of the Watergate scandal?Response: With the mounting evidence and loss‚Ä¶\nFigure 4: A general illustration of the prompt of LLM4CS. The prompt consist of three parts, i.e., Instruction,\nDemonstration, and Input. The red part is for REW prompting, the blue part is for the RTR and RAR promptings,\nand the orange part is for RTR prompting. The green part is for our designed chain-of-thought.\n1223\nCAsT-20 (Conversation #84)\nTurn 1\nQuestion: How does seed investment work for startups?\nResponse: Seed money, sometimes known as seed funding or seed capital, is a form of\nsecurities offering in which an investor invests capital in exchange for an equity stake in ...\nHuman Rewrite: How does seed investment work for startups?\nCoT: This is the first turn.\nOur Rewrite: How does seed investment work for startups?\nTurn 2\nQuestion: Tell me more about angel rounds.\nResponse: Tell Me More was a National Public Radio interview show hosted by journalist\nMichel Martin ... offering sample segments, and soliciting user feedback. It began broadcast-\ning on April 30, 2007.\nHuman Rewrite: Tell me more about angel investment rounds.\nCoT: Based on the current question and the previous response, we are comparing angel\nrounds with crowdfunding.\nOur Rewrite: Tell me more about angel rounds in seed investment for startups.\nTurn 3\nQuestion: How does it compare with crowdfunding?\nResponse: For crowdfunding of equity stock purchases, there is some research in social\npsychology that indicates that ... Managing communications with a large number of possibly\ndisappointed investors and supporters can be a substantial, and potentially diverting, task.\nHuman Rewrite: How does an angel investment round compare with crowdfunding?\nCoT: Based on the current question and the previous response, we are comparing angel\nrounds with crowdfunding.\nOur Rewrite: How does angel funding compare to crowdfunding for startups?\nTurn 4\nQuestion: How much is a typical A series round?\nResponse: OpenGov has raised $47 million from venture capital and angel investors. A $3\nmillion Series A round in 2012 included venture funds 8VC, Founder Collective, Valiant\nCapital, and a number of high-profile ...\nHuman Rewrite: How much is a typical A series investment round?\nCoT: Based on the previous turns, we are talking about different types of funding for startups\nsuch as seed investment, angel rounds, and crowdfunding.\nOur Rewrite: How much funding is typically raised in an A series round for startups?\nTurn 5\nQuestion: What are the risks of investing?\nResponse: Angel investors invest in early stage or start-up companies in exchange for an\nequity ownership interest. Angel . . . Here is what angels particularly care about: The quality,\npassion, commitment, and integrity of the founders.\nHuman Rewrite: What are the risks of investing in startups?\nCoT: Based on the previous turns, we are discussing different aspects of startup investments.\nOur Rewrite: What are the risks associated with investing in startups?\nTurn 6\nQuestion: What are the different types of instruments used by VCs?\nResponse: ...\nHuman Rewrite: What are the different types of instruments used by VCs?\nCoT: The user is asking about the different types of instruments used by venture capitalists.\nOur Rewrite: What are the various types of instruments employed by venture capitalists\n(VCs)?\nTable 4: An example to show how our CoT takes effect. TheCoT and Our Rewritefields are generated by LLM4CS\n(REW + CoT).\n1224\nGood-H\nQuestion: How about replacing it instead?\nHuman Rewrite: How much does it cost to replace a garage door opener?\nOur Rewrite: What is the cost of replacing a garage door opener?\nGood-A\nQuestion: What‚Äôs important for me to know about their safety?\nHuman Rewrite: What‚Äôs important for me to know about the safety of smart garage door\nopeners?\nOur Rewrite: What are the safety features that I should consider when choosing a garage\ndoor opener?\nBad-O\nQuestion: Are there any famous foods?\nHuman Rewrite: Are there any famous foods in Washington D.C.?\nOur Rewrite: Are there any famous foods?\nBad-C\nQuestion: What is its main economic activity?\nHuman Rewrite: What is the main economic activity of Salt Lake City?\nOur Rewrite: What is the main economic activity in Utah?\nTable 5: Examples of the four categories in human evaluation.\n1225",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7952240705490112
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7502410411834717
    },
    {
      "name": "Conversation",
      "score": 0.7010643482208252
    },
    {
      "name": "Language model",
      "score": 0.5018107891082764
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.468802273273468
    },
    {
      "name": "Human‚Äìcomputer interaction",
      "score": 0.4225099980831146
    },
    {
      "name": "Natural language processing",
      "score": 0.4144250154495239
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3971248269081116
    },
    {
      "name": "Information retrieval",
      "score": 0.38985592126846313
    },
    {
      "name": "World Wide Web",
      "score": 0.37313851714134216
    },
    {
      "name": "Data science",
      "score": 0.3432985246181488
    },
    {
      "name": "Psychology",
      "score": 0.1517360508441925
    },
    {
      "name": "Communication",
      "score": 0.10419747233390808
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}