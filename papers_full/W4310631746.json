{
  "title": "CultureBERT: Measuring Corporate Culture With Transformer-Based Language Models",
  "url": "https://openalex.org/W4310631746",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101622189",
      "name": "Sebastian P. Koch",
      "affiliations": [
        "Goethe University Frankfurt"
      ]
    },
    {
      "id": "https://openalex.org/A5033677051",
      "name": "Stefan Pasch",
      "affiliations": [
        "Goethe University Frankfurt"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2903391273",
    "https://openalex.org/W3124739648",
    "https://openalex.org/W2909384550",
    "https://openalex.org/W2144100620",
    "https://openalex.org/W3213239161",
    "https://openalex.org/W2734420194",
    "https://openalex.org/W3124978547",
    "https://openalex.org/W2801182247",
    "https://openalex.org/W2014929532",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2130611112",
    "https://openalex.org/W3030030185",
    "https://openalex.org/W3151328414",
    "https://openalex.org/W2776684266",
    "https://openalex.org/W4214850672",
    "https://openalex.org/W2556609826",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3139276676",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W3034156543",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W4300485781",
    "https://openalex.org/W3123359366",
    "https://openalex.org/W2588069903",
    "https://openalex.org/W2938815745"
  ],
  "abstract": "This paper introduces transformer-based language models to the literature measuring corporate culture from text documents. We compile a unique data set of employee reviews that were labeled by human evaluators with respect to the information the reviews reveal about the firms' corporate culture. Using this data set, we fine-tune state-of-the-art transformer-based language models to perform the same classification task. In out-of-sample predictions, our language models classify 17 to 30 percentage points more of employee reviews in line with human evaluators than traditional approaches of text classification. We make our models publicly available.",
  "full_text": " \n \n \n \nCultureBERT: Measuring Corporate Culture With \nTransformer-Based Language Models \n \n \nSebastian Koch*  Stefan Pasch§ \n \nThis Version: June 19, 2023† \n \n \nAbstract \nThis paper introduces transformer-based language models to the literature measuring \ncorporate culture from text documents. We compile a unique data set of employee \nreviews that were labeled by human evaluators with respect to the information the \nreviews reveal about the firms’ corporate culture. Using this data set, we fine-tune state-\nof-the-art transformer-based language models to perform the same classification task. \nIn out-of-sample predictions, our language models classify 17 to 30 percentage points \nmore of employee reviews in line with human evaluators than traditional approaches \nof text classification. We make our models publicly available. \n \n \nKeywords: corporate culture, organizational culture , natural language processing, transformer -based \nlanguage models \n \n \nA later version of this paper has been published as: \n \nS. Koch and S. Pasch, \"CultureBERT: Measuring Corporate Culture With Transformer-Based Language \nModels,\" in 2023 IEEE International Conference on Big Data (BigData), pp. 3176 -3184. doi: \n10.1109/BigData59044.2023.10386765 \n \n*Corresponding author. Faculty of Economics and Business, Goethe University Frankfurt, Theodor-W.-Adorno-\nPlatz 4, 60323 Frankfurt am Main, Germany. Email: s.koch@its.uni-frankfurt.de. Telephone: +49 (69) 798 -\n34825. Telefax: +49 (69) 798-35021. \n§stefan.pasch@outlook.com \n†This paper benefited from valuable suggestions from Guido Friebel, Sameer Srivastava, Amir Goldberg, as well \nas conference and seminar participants at the 2023 Berkeley Culture Conference and the Bank of England. Luca \nSchöpfe provided excellent research assistance. \n \n1 \n1. Introduction \nIn recent years, significant advancements in computational linguistics have allowed management \nscholars to better understand concepts that are hard to quantify, such as innovation (Bellstam et al., \n2021), market orientation (Andreou et al., 2020), or corporate culture  (Srivastava et al., 2018). With \nrespect to corporate culture in particular, researchers have increasingly tried to measure it by analyzing \nlarge textual data sets, consisting of, for example, employee reviews  (Corritore et al., 2020; Grennan, \n2019; Pasch, 2018) , annual reports  (Andreou et al., 2022; Fiordelisi and Ricci, 2014;  Nguyen et al., \n2019; Wang et al., 2021), email messages (Srivastava et al., 2018), or earnings calls (Li et al., 2021). \nHowever, studies measuring corporate culture by applying computational linguistics exhibit two major \nweaknesses. First, most computational linguistic techniques do not take the semantic context of words \ninto account. Second, most studies do not evaluate in how far their measures of corporate culture align \nwith the assessment of human evaluators. In addition to these weaknesses, most studies do not publish \ntheir language models in a way that allows other researchers to  easily benchmark them against other \ncomputational linguistic techniques. In this paper, we address all of these issues. \nWe construct a unique data set of employee reviews that were labeled by human evaluators with \nrespect to the information the reviews reveal abo ut the firm’s corporate culture . This labeled data set \nallows us to apply supervised machine learning to measure corporate culture. More specifically, we fine-\ntune state -of-the-art transformer -based language models and show that they further improve on the \ncomputational linguistic techniques applied in the literature to measure corporate culture so far.  \nTransformer-based language models allow to capture complex aspects of textual information by taking \nthe surrounding context into consideration. Their development has accelerated since the publication of \nBidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018). BERT set new \nhigh scores in various natural language processing tasks, such as question-answering, fact-checking, and \ntext classification (González-Carvajal and Garrido-Merchán, 2020). While transformer-based language \nmodels have already been used in financial sentiment analysis (Araci, 2019), patent classification (Lee \nand Hsiang, 2020), or biomedical text mining (Lee et al., 2020), we are not aware of any application \nwith respect to corporate culture. \n \n2 \nOur procedure of fine -tuning transformer -based languag e models for corporate culture \nclassification is as follows: \n We randomly draw a sample of 2,000 employee reviews from a leading career community \nwebsite.  \n We create a labeled data set by determining for each of the 2,000 employee reviews whether it \ncontains information either in accordance with or in opposition to the four culture dimensions \nof the Competing Values Framework (CVF; Quinn and Rohrbaugh, 1983). Moreover, we assign \neach review to the CVF’s culture dimension that best matches the employee’s description. \n We use a subset of this  labeled data set to fine-tune transformer-based language models to \nclassify employee reviews in the same way. \n \nWe find that our language models for corporate culture are more accurate than traditional \ncomputational linguistic techniques in classifying employee reviews with respect to corporate culture in \nthe same way as human evaluators. Specifically, our models achieve a 17 to 30 percentage point higher \naccuracy than the dictionary method — a particularly popular approach of measuring corporate culture \nby comparing the text document of inte rest with a set of words that are  thought to be characteristic of \nthe cultural dimension under study. Moreover, our language models outperform other machine learning-\nbased text classifiers by three to fifteen percentage points in accuracy.  Our findings illustrate the \nusefulness of transformer-based language models not only for measuring corporate culture but also for \nstudying organizational phenomena more generally. \nDifferent from most other  studies, we make  our language models publicly available. 1 This \nallows other researchers to measure corporate culture from employee reviews or similar text documents \nwithout having to build their own language model for corporate culture classification. Moreover, i t \nallows other researchers to benchmark our transformer -based language models against alternative \ncomputational linguistic techniques. \nThe remainder of the paper is organized as follows. In the following section, we explain how \nour paper relates to the lit erature. In section 3, we present our data. In section 4, we explain how we \nconstructed our language models for corporate culture classification. In section 5, we compare our \n \n3 \nmodels’ performance in classifying employee reviews with the performance of traditional computational \nlinguistic techniques. In the last section, we discuss our findings. \n \n2. Related Literature \n2.1 Competing Values Framework \nWe follow O'Reilly and Chatman (1996, p. 160)  and define corporate culture as “a system of shared \nvalues (that define what is important) and norms that define appropriate attitudes and behaviors for \norganizational members (how to f eel and behave).” To create a labeled data set of employee reviews, \nwe need a framework that lets us decide how to group the values, norms, and behaviors expressed in \nemployee reviews into specific types of corpora te culture. We make use of the Competing V alues \nFramework (CVF; Quinn and Rohrbaugh, 1983), which is one of the frameworks most widely used for \nmeasuring corporate culture in both scientific research and practice (Hartnell et al., 2011; Chatman and \nO’Reilly, 2016). \nAccording to the CVF, organizations can be described along two basic value dimensions: their \nfocus and their preference for structure. With respect to the focus dimension, organizations can be placed \non a continuum between an internal focus, with an emphasis on internal capabilities and integration, and \nan external focus, with an emphasis on external opportunities and differentiation fr om competitors \n(Hartnell et al., 2019). With respect to the structure dimension, organizations can be distinguished based \non whether they prefer stability and control or flexibility and change. Plotting the focus and structure \ndimensions in a two -dimensional space yields four quadrants, of which each stands for one type of \ncorporate culture: clan, hierarchy, adhocracy, and market (see Figure 1). For example, organizations \nexhibiting a clan culture tend to have an internal focus and a structure that emphasizes flexibility and \nchange. Each of the four culture types is associated with certain assumptions, beliefs, values, behaviors, \nand effectiveness criteria (see Table 1). The clan culture, for example, puts special emphasis on values \nsuch as attachment, affiliation, or collaboration and is characterized by behaviors such as teamwork, \nparticipation, and employee involvement. \n \n4 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nImportantly, the four culture types of the CVF are not mutually exclusive but tend to be \npositively correlated (Hartnell et al., 2011). This means that a firm can exhibit several culture types at \nthe same time. Therefore, some authors have suggested to refer to the culture types as culture dimensions \n(Hartnell et al., 2019). We follow this notation in order to make clear that a firm being characterized by \nmore than one type of culture is rather the norm than the exception. \n  \nFIGURE 1 \nCompeting Values Framework \nAdapted from Figure 3.1 in Cameron and Quinn (2011). \n\n \n5 \nTABLE 1  \nCVF Culture Dimensions and Associated Attributes \nCulture \ntype \nAssumptions Beliefs Values Artifacts (behaviors) Effectiveness \ncriteria \nClan Human \naffiliation \nPeople behave \nappropriately \nwhen they have trust in, \nloyalty to, and membership \nin the organization. \nAttachment, \naffiliation, \ncollaboration, \ntrust, and \nsupport \nTeamwork, \nparticipation, \nemployee \ninvolvement, and \nopen communication \nEmployee \nsatisfaction \nand \ncommitment \nAdhocracy Change People behave \nappropriately \nwhen they understand the \nimportance and impact of \nthe task. \nGrowth, \nstimulation, \nvariety, \nautonomy, and \nattention to \ndetail \nRisk-taking, creativity, \nand \nadaptability \nInnovation \nMarket Achievement People behave \nappropriately \nwhen they have clear \nobjectives and are \nrewarded \nbased on their \nachievements. \nCommunication, \ncompetition, \ncompetence, and \nachievement \nGathering customer \nand \ncompetitor \ninformation, \ngoal-setting, planning, \ntask focus, \ncompetitiveness, and \naggressiveness \nIncreased \nmarket share, \nprofit, \nproduct \nquality, and \nproductivity \nHierarchy Stability People behave \nappropriately \nwhen they have clear roles \nand procedures are \nformally \ndefined by rules and \nregulations. \nCommunication, \nroutinization, \nformalization, \nand \nconsistency \nConformity and \npredictability \nEfficiency, \ntimeliness, \nand \nsmooth \nfunctioning \nAdapted from Table 13-1 in Quinn and Kimberly (1984) and Figure 2 in Hartnell et al. (2011).  \n \n \n2.2 Measuring Corporate Culture by Computational Linguistic Techniques \nA common approach researchers have used to measure corporate culture is to administer a survey among \nmembers of the corporation. Starting from different theoretical backgrounds, different survey methods \nhave been developed (see Chatman and O’Reilly (2016) for a review of the four most popular ones). \nFor example, Cameron and Quinn (2011) proposed the Organizational Culture Assessment Instrument \n— a questionnaire to measure the four culture dimensions of the CVF. A drawback of measuring \ncorporate culture with surveys is that assessing a large sample of corporations is both time- and resource-\nintensive. With recent advancements in computing power and new textual data bases becoming \navailable, management scholars are increasingly using computational linguistics as an  alternative \n(Chatman and Choi, 2022). \nThe earliest studies applying computational linguistic techniques to measure corporate culture \nhave used the dictionary method. The idea of this method is to create a so -called dictionary or master \ntext that is composed of  words that supposedly signal the cultural trait that is to be measured. The \n \n6 \nexistence of a given cultural trait is then measured by the textual similarity between the corresponding \ndictionary and the text documents of interest, for example all employee re views of a given firm. To \nconstruct the dictionary, different approaches have been applied. While Grennan (2019) uses the lexical \ndatabase WordNet, Li et al. (2021) apply a word embedding model. \nAnother computational technique researchers have applied to measure corporate culture from \ntext documents is probabilistic topic modeling. It is an unsupervised machine learning method that is \nused to analyze which themes are covered in a given text.  Corritore et al. (2020), for example, apply \nlatent Dirichlet allocation (LDA; Blei et al., 2003) — the topic model most widely used in studies on \ncorporate culture — to measure cultural heterogeneity both between and within employees from \nemployee reviews. However, especially if corporate culture is not the main focus of the text documents \nto be analyzed, many of the themes a  topic model uncovers may not be related to corporate culture at \nall. Therefore, topic modeling is not well suited for the use case we look at, which is to identify specific \nculture dimensions from  employee reviews, in which employees talk about more than just corporate \nculture. We will come back to this point in the final discussion. \nWe contribute to the literature on the measurement of corporate culture by introducing a state-\nof-the-art supervised machine learning approach. Supervised machine learning methods differ from \nunsupervised methods, such as topic modeling , in that a labeled data set is used. With supervised \nlearning, an algorithm is trained to predict these labels. Our supervised learning approach consists of \nfine-tuning transformer-based language models for corporate culture classification. First introduced with \nBERT (Devlin et al., 2018), transformer-based-language models now represent the state -of-the-art in \nnatural language processing. They allow to capture complex aspects of textual information by taking the \nsurrounding context into consideration and have been found to outperform traditional machine learning \napproaches in most natural language processing tasks (González-Carvajal and Garrido-Merchán, 2020). \nIn financial sentiment classification, for example, BERT has been found to outperform simple machine \nlearning approaches, such as naïv e Bayes or support vector machine, as well as other deep learning \nalgorithms, including convolutional neural networks and long short-term memory (Huang et al., 2023). \n  \n \n7 \n3. Data \nWe collected 2,000 online employee reviews by drawing a random sample of all the reviews that were \npublished on a leading employee review website between 2008 and 2018 and that matched the following \ntwo criteria: (i) The review was written by an employee or former employee working in the U.S and (ii) \nthe employer is available in the Compustat database. We limit our sample to reviews of Compustat -\nlisted firms to make our results comparable to other studies in which corporate culture is commonly \nlinked to financial figures. \nEach review includes several free text sections in which the reviewer can choose to  talk about \nwhat she likes and dislikes about the employer and what changes she would recommend. We combined \nthe free texts of the different sections in random order to obtain a single text for each review. All other \ninformation provided by the review besides the free texts was discarded. Next, we classified the reviews \nwith respect  to the employer’s corporate culture. To do so, both authors and a research assistant \n(henceforth “labelers”) independently went over all reviews. For each of the CVF’s four cult ure \ndimensions, they assigned each review to one of three classes, depending on whether the review \n \n● contains information in line with the culture dimension under question (“positive review”), \n● contains information in conflict with the culture dimension under question (“negative review”), \nor \n● does not allow any inference about the culture dimension under question (“neutral review”). \n \nIn addition, the labelers assigned each review to exactly one of the four culture dimensions — the \ndimension that best fitted the overall tone of the review. This was done because although a firm can be \ncharacterized by several culture dimensions simultaneously, one of these dimensions usually dominates \n(Cameron and Quinn, 2011) . In completing their labeling task, the labelers tried to stick as close as \npossible to the description of the four culture dimensions provided by  Cameron and Quinn (2011) and \nHartnell et al. (2011). \nFor each of the five classifications that needed to be made per review (one for each of the four \nculture dimensions and one for the dominant culture), the final classification was selected to be the \n \n8 \nmajority vote of the three labelers. In case the labelers all had a different opinion, one of their decisions \nwas picked at random. Table 2 provides the absolute frequency of review classifications by type of \nlabeler agreement. All labelers agreeing on the same classification is the most frequent outcome for all \nfive labeling tasks. The comparatively lower frequency of full agreement when picking the dominant \nculture can be explained by the fact that in line with the findings of Hartnell et al. (2011), reviews often \ninclude information that equally points to more than one culture dimension. \n \nTABLE 2 \nNumber of Reviews by Type of Labeler Agreement \nType of agreement Clan Adhocracy Market Hierarchy \nDominant \nculture \nNo agreement 61 17 56 34 176 \nTwo labelers agree 850 458 760 632 890 \nFull agreement 1,089 1,525 1,184 1,334 934 \nSum 2,000 2,000 2,000 2,000 2,000 \n \nWe split our data set of 2,000 labeled employee reviews into a training (N=1,400), validation \n(N=200), and test set (N=400). \n \n4. Model Fine-Tuning and Benchmarks \n4.1 Transformer-Based Models \nTo build a state-of-the-art language model for corporate culture, we start-off with the two most widely-\nused language models that are based on the transformer architecture  (Vaswani et al. , 2017): BERT \n(Devlin et al., 2018) and RoBERTa (Liu et al., 2019).2 Both of these models have been trained on large \nunlabeled text corpora, including books and Wikipedia articles. As the name suggests, RoBERTa \n(Robustly optimized BERT approach ) builds on BERT . However, it achieves higher performance in \nstandard natural language processing tasks due to an improved traini ng procedure and a much larger \ntraining data set  (Liu et al., 2019). For both BERT and RoBERTa , a base version with around 100 \nmillion parameters and a large version with around 350 million parameters exists. BERT and RoBERTa \nare general-purpose language models. In order to apply them on a specific natural language processing \ntask, they need to be fine-tuned with the help of a labeled data set. This is exactly what we do in this \npaper. Using the 1,400 human-labeled employee reviews of our training set, we fine-tune the base and \n \n9 \nthe large version of BERT and RoBERTa for the task of text classification in the domain of corporate \nculture. In the next step, we draw on the 200 labels of our validation set to evaluate different \nhyperparameters and to compare the resulting performance of the four models we look at. In line with \nLiu et al. (2019), we find that RoBERTa consistently outperforms BERT, both with respect to the base \nand the large version of the model. We therefore focus on RoBERTa in the following. Table 3 presents \nthe hyperparameters of our final analysis. \n \nTABLE 3 \nHyperparameters \nNumber of epochs 8 \nWeight decay 0.01 \nLearning rate 1e-5 \nDropout rate 0 \nBatch size 16 \nMaximum sequence length 200 \n \n \n4.2 TF-IDF-Based Text Classifiers \nSince supervised machine learning has rarely been used to measure corporate culture , we benchmark \nour transformer-based models with other text classification methods that also use supervised learning \nbut do not yet incorporate text embeddings as is the case with transformer-based models.3 Such methods \nusually quantify the input text as a  bag-of-word representation and apply machine learning algorithms \nto classify the text based on the occurrence and co-occurrences of these words (González-Carvajal and \nGarrido-Merchán, 2020) . More specifically, texts are quantified with  Term Frequency -Inverse \nDocument Frequency (TF-IDF) matrices, which measure the occurrence of words in a text relative to \nthe inverse number of occurrences in the en tire document corpus. We follow González-Carvajal and \nGarrido-Merchán (2020) in computing the TF-IDF matrices with the TfidfVectorizer from sklearn. After \nthe text  has been transformed into a matrix representation, we apply traditional machine learning \nalgorithms to classify the given text inputs. In particular, we use logistic regression, random forest, and \nXGBoost. \n  \n \n10 \n4.3 Dictionary Method \nFollowing Grennan (2019) and Pasch (2018), we generate a dictionary for each of the CVF’s culture \ndimensions, using the words that describe the associated assumptions, beliefs, values, behaviors, and \neffectiveness criteria that are mentioned in  Quinn and Kimberly (1984). In addition, we included the \nsynonyms and hyponyms of these words, using the WordNet library. \nTo measure the similarity between employee reviews and dictionaries, we compute for each \nemployee review  and each culture dimension the share of words that appear in the corresponding \ndictionary. Before doing so, we stemmed both the dictionary and the employee reviews  (“create” and \n“creation” become “creat”) and removed common stop words from the reviews (for example, “a”, “and”, \n“the”). Moreover, we take into account negations by subtracting the share of word  stems of the \ndictionary that appear in a sentence with a negation word, such as “not” or “never”. Online Appendix A \npresents the stemmed dictionaries we use. \n The question of interest is how the dictionary method compares to our transformer -based \nlanguage models in reproducing the human -generated corporate culture classification of employee \nreviews. For this purpose, the continuous similarity scores derived from the dictionary method need to \nbe transformed into the discrete three -class-classification (positive, negative, or neutral) or four-class-\nclassification (clan, adhocracy, hierarchy, market) of our labeling. We set the thresholds  such that the \nrelative frequency of the three  or four classes matches the distribution in our training data set. For \nexample, 21% of the reviews were labeled as showing evidence in accordance with a clan culture. \nHence, the 21% of the reviews exhibiting the highest similarity with the clan dictionary were assigned \nto be positive with respect to a clan culture. We set the thresholds in this way to make sure that the \ndictionary method makes use of the same information as our language models. Our language mod els \npotentially picked up the distribution of reviews across labeling categories in the process of fine-tuning. \n  \n \n11 \n5. Results \n5.1 Accuracy Scores \nTable 4 presents accuracy scores, that is, the fraction of the 400 employee reviews of our test data set \nthat were classified in line with human labelers, comparing  different methods of text classification. In \nthe category dominant culture, we analyze which of the CVF’s four culture dimensions best fits the \nreview. Hence, we would expect that a random classifier achieves an accuracy of 0.25. With respect to \nthe four individual culture dimensions, a review can be classified as positive, negative, or neutral. \nTherefore, a random classifier should achieve an accuracy of 0.33. Table 4 also lists the performance of \na classifier that always chooses the label that occurs most frequently ( majority class). In the category \ndominant culture, this is the culture dimension market; in the case of the individual culture dimensions, \nthis is the label neutral. \n \n \nWe find that the dictionary method outperforms a random classifier in all categories. However, \nwith the exception of the clan category, it fails to outperform a classifier that always predicts the majority \nclass. \nTABLE 4  \nAccuracy Scores of Different Methods of Text Classification  \nMethod of text classification \nDominant \nculture Clan Adhocracy Market Hierarchy \n \nRandom  0.25 0.33 0.33 0.33 0.33  \nMajority class  0.38 0.41 0.80 0.68 0.64  \n        \nDictionary method  0.33 0.43 0.72 0.57 0.62  \n        \nTF-IDF + logistic reg. 0.48 0.61 0.81 0.69 0.69  \n + random forest 0.46 0.58 0.81 0.69 0.74  \n + XGBoost 0.47 0.59 0.82 0.74 0.74  \n        \nRoBERTa base 0.54 0.64 0.85 0.74 0.75  \n  large 0.63 0.68 0.89 0.77 0.81  \nTable reports accuracy scores, that is, the fraction of the 400 employee reviews of the test data set that were classified in  \nline with human labelers. In the category dominant culture, it is determined which of the CVF’s four culture dimensions \nbest fits the review. In the categories clan, adhocracy, market, and hierarchy, it is determined whether the review contains \ninformation in accordance with, in contradiction  to, or does not contain any  information that allows i nference about the \nexistence of the culture dimension under consideration. \n \n12 \nTurning to the TF -IDF-based classifiers, we find that they outperform both a simple majority \nclassifier as well as the dictionary method in all categories. With respect to our transformer -based \nlanguage models, in turn, we observe that both RoBERTa -base an d RoBERTa-large outperform all \nbenchmarks in all categories. Moreover, RoBERTa -large consistently shows higher accuracy scores \nthan the base version of RoBERTa , which is why we will focus on RoBERTa -large in the following . \nOverall, RoBERTa -large outperf orms the dictionary method by 17 to 30  percentage points. It also \noutperforms TF -IDF-based classifiers by three to fifteen p ercentage points.  The conclusions are \nqualitatively unchanged when looking at F 1 instead of accuracy scores (see Table B1 in the Online \nAppendix). \n \n5.2 Why Does RoBERTa-large Outperform the Dictionary Method? \nAs the dictionary method still represents the standard approach in studies measuring  pre-specified \ndimensions of corporate culture from text documents, we take a closer look what makes it less accurate \nthan our language models that are based on RoBERTa -large. We look at all reviews that RoBERTa -\nlarge classified in the same way as human evaluators but that were misclassified by the dictionary \nmethod. We limit our attention to review s with less than 50 words, however. While it is generally \ndifficult to explain the predictions of RoBERTa-large since they do not follow from a simple rule-based \napproach, this is less of a problem in short reviews. This is because in short reviews, there is often just \na single word or phrase that provides evidence in favor of one classification or another. \nWe found three major reasons for why  many of the  reviews that are misclassified by the \ndictionary method are still correctly classified by RoBERTa -large. Table 5 provides the relative \nfrequency of these reasons for each of our five language models along with some examples . Since a \nreview can be misclassified for  several reasons and the allocation of cases across reasons is therefore \nnot clear-cut, it is more the magnitude than the exact number of the relative frequencies that is of interest. \nWith respect to the clan and hierarchy dimension, in the majority of reviews that are correctly \nclassified by RoBERTa -large but misclassified by the dictionary method, there is an expression that \nprovides evidence in accordance with or in opposition to the culture dimension under consideration but \ndoes not use any of the word stems included in the respective dictionary. Hence, although actually \n \n13 \npositive or negative,  the review is nevertheless classified as neutral when applying the dictionary \nmethod. For example, while the statement “they are quick to throw you under the bus” contradicts a clan \nculture, none of the statement’s word stems is part of the dictionary for the clan dimension.  \nThe second reason for why some of the reviews that are misclassified by the dictionary method \nare correctly classified by RoBERTa -large is tha t the review may use words that are included in the \ndictionary but in a different context. In this case, the review is wrongly classified as positive or negative \nwhen the dictionary methods is applied. In contrast, the review is correctly classified as neu tral by \nRoBERTa-large since transformer-based language models take the surrounding context into account . \nFor example, the remark that the job offers “a competitive compensation” does not signal a competitive \ncorporate culture although the word “competitive” is used. \nA closer inspection reveals that a significant number of misclassifications is caused by a limited \nset of word stems. Among these are the word stems “benefit” in the market dictionary and “time” in the \nhierarchy dictionary as they are regularly used in a context completely independent of corporate culture. \nMoreover, the word stems “advanc”, “develop”, “growth”, and “grow” of the adhocracy dictionary turn \nout to be problematic as reviewers usually use them not to refer to an adhocracy culture but to talk about \ncareer advancement and personal development. Even removing these word stems from the dictionaries, \nhowever, does not significantly improve the accuracy of the dictionary method . This is because it also \nalters the threshold for the similarity between dictionary and employee review above or below which a \nreview is classified as  positive or negative, respectively. Hence, several reviews that so far were \ncorrectly classified as neutral now are misclassified as positive or negative due to other words included \nin the dictionary but used in the review in a different context. As selectively removing some word stems \nfrom our dictionary could be seen as arbitrary, we also reapply the dictionary method using the \ndictionaries that Fiordelisi and Ricci (2014) propose for measuring the CVF’s four culture dimensions. \nTheir dictionaries have been used in several other studies, including Nguyen et al. (2019) and Wang et \nal. (2021). Although their dictionaries do not contain any of the problematic  word stems mentioned \nabove, however, the accuracy scores we obtain when using these dictionaries are highly similar to those \nreported for the dictionary method in Table 4 (see Table C1 in the Online Appendix). The only exception \nis the clan dimension, for which we obtain a ten percentage point lower accuracy score when using their \n \n14 \ndictionary as oppos ed to ours. This is most probably because their dictionary for the clan dimension \nincludes some very generic word stems, such as “employ” or “partner” , which  often are used in a \ndifferent context than corporate culture. \nThe third reason for why RoBERTa-large exhibits a higher accuracy than the dictionary method \nis its superior ability  in correctly interpreting the meaning of phrases that even in the absence of a \nnegation have the opposite meaning of what the words by itself suggest. One example is the usage  of \nthe imperative. The statement “communicate more with your employees” signals a lack of \ncommunication and hence provides evidence in opposition to a clan culture although the word \n“communication” is used. As another example, RoBERTa-large understands that a firm that is “very \nslow in change” exhibits the opposite of an adhocracy culture although the word “change” appears. \nIn the case of the four language models for an individual culture dimension, less than ten percent \nof the reviews could not be assigned to any of the three reasons mentioned above. Among these are \nreviews for which more than one classification seemed reasonable. Hence, we keep these reviews \nseparate from the other cases in which the misclassification clearly resulted from an inability to interpret \nthe text correctly. The share of reviews that could not be assigned to any of the three reasons is higher \nin the case of the language model that predicts the dominant culture. This is becau se the variable \ndominant culture always has to take one of the four available dimensions. Therefore, some of the reviews \nthat do not provide any conclusive information on corporate culture are correctly classified by RoBERTa \nbut misclassified by the dictionary method just by chance. \n \n \n15 \nTABLE 5 \nReasons Why Reviews Correctly Classified by RoBERTa-large Were Misclassified by Dictionary Method \n     Classification of example \nCulture \ndimension \nNo. of \nreviews \nReason why review was misclassified by \ndictionary method \nShare of \nreviews Example \nRoBERTa-\nlarge \ndictionary \nmethod \nClan 62 Wording without words from dictionary 81% They are quick to throw you under the bus. negative neutral \n  Words from dictionary in different context 8% It is fun helping customers find the right product. neutral positive \n  Did not capture opposite meaning 10% Communicate more with your employees. negative positive \n  Other 2%       \nAdhocracy 54 Wording without words from dictionary 19% No room for new technologies. negative neutral \n  Words from dictionary in different context 69% All the red tape creates excessive work. neutral positive \n  Did not capture opposite meaning 4% Very slow in change. negative positive \n  Other 9%       \nMarket 65 Wording without words from dictionary 22% You are nothing but a number. positive neutral \n  Words from dictionary in different context 71% Offers competitive compensation. neutral positive \n  Did not capture opposite meaning 2% Reward actual achievement, not face time. negative positive \n  Other 6%       \nHierarchy 54 Wording without words from dictionary 52% There is too much change. negative neutral \n  Words from dictionary in different context 41% Company consists of nothing but thieves. neutral positive \n  Did not capture opposite meaning 6% The firm is constantly adjusting its structure. negative positive \n    Other 2%       \nDominant 83 Wording without words from dictionary 63% Very cumbersome and antiquated processes. hierarchy random \nculture  Words from dictionary in different context 11% Corporate objectives shift constantly. adhocracy market \n  Did not capture opposite meaning 0% Innovation takes forever. hierarchy adhocracy \n  Other 27%    \nTable considers all reviews with less than 50 words that RoBERTa -large classified in the same way as human labelers but that were misclassified by the dictionary method. In the example \nsentences, the word stems that are part of the dictionary of the respective culture dimension are marked in bold.  \n \n \n16 \n6. Discussion \nWe fine-tune transformer-based language models for classifying employee reviews with respect to their \nalignment with the four culture dimensions of the CVF. In principal, our analysis could be repeated \nusing a different framework than the CVF . This would require creating a new data set of employee \nreviews labeled with respect to a different set of culture dimensions . Due to their advantage s in \nunderstanding the meaning of words and taking semantic context into account, we expect that \ntransformer-based language models would similarly outperform other approaches of text classification \neven if a different set of culture dimensions was used. \nWe evaluate the performance of our transformer-based language models by comparing them to \nthe dictionary method as well as supervised machine learning approaches based on TF -IDF matrices. \nWe do not apply topic modeling, however , since it  is not well suited for id entifying specific culture \ndimensions from text documents that are not specifically related to corporate culture. Since the employee \nreviews we use are free texts, they contain information on various job-related issues, many of which are \nnot related to corporate culture at all. Not surprisingly, therefore, applying a topic model to employee \nreviews predominantly yields groupings of words that are unrelated to corporate culture.  Hence, \nalthough using a topic model does not require to specify ex ante what cultural content employee reviews \nare assumed to cover, it is nevertheless associated with making subjective decisions, for example, what \npart of the text to consider or how to interpret, label, and f ilter the topics obtained  (Schmiedel et al., \n2019). \nWe are aware of the fact that the accuracy of the computational linguistic techniques we evaluate \nmay depend on the labeled data set we propose. The superior accuracy of the language models for \ncorporate culture we propose, however, is well in line with what has been found about the performance \nof transformer-based language models on natural processing tasks more generally (González-Carvajal \nand Garrido-Merchán, 2020). By making our models publicly available, we allow other researchers to \nverify and to make use of the superior performance of transformer-based language models in the domain \nof corporate culture, using their own data. \nA common drawback of language models that are based on machine learning is that it is difficult \nto explain the predictions they deliver. In this regard, our language models are no exception. While we \n \n17 \nfocus on showing that transformer-based language models are more accurate in measuring corporate \nculture from text documents than traditional approaches of text classification , we also provided some \ninsights on the sources of this higher degree of accuracy. Huang et al. (2023) follow a different approach \nto explore the superior classification performance of transformer-based language models. They analyze \nhow the accuracy changes when the words in the observations to be labeled are randomized. They \nobserve that this leads to a sizeable drop in the accuracy of their transformer-based models but decreases \nthe accuracy of other machine learning algorithms only moderately, suggesting that transformer-based \nlanguage models much better capture contextual information. \nFuture work should explore more thoroughly what are the advantage s of transformer -based \nlanguage models in measuring corporate culture or other organizational phenomena but should also look \nat their limitations. This requires providing an explanation for the models’ predictions, for example by \nusing a local interpretable model (Ribeiro et al., 2016). \n  \n \n18 \nReferences \n \nAndreou, P. C., F. Fiordelisi, T. Harris and D. Philip (2022). ‘Institutional Ownership and Firms’ Thrust \nto Compete’, British Journal of Management, 33, pp. 1346–1370. \nAndreou, P. C., T. Harris and D. Phi lip (2020). ‘Measuring Firms' Market Orientation Using Textual \nAnalysis of 10-K Filings’, British Journal of Management, 31, pp. 872–895. \nAraci, D. (2019). ‘FinBERT: Financial Sentiment Analysis with Pre -trained Language Models’, arXiv \npreprint arXiv:1908.10063v1. \nBellstam, G., S. Bhagat and J. A. Cookson (2021). ‘A Text -Based Analysis of Corporate Innovation’, \nManagement Science, 67, pp. 4004–4031. \nBlei, D. M., A. Y. Ng and M. I. Jordan (2003). ‘Latent Dirichlet Allocation’, Journal of Machine \nLearning Research, 3, pp. 993–1022. \nCameron, K. S. and R. E. Quinn (2011). Diagnosing and Changing Organizational Culture: Based on \nthe Competing Values Framework. 3rd ed. San Francisco (CA): Jossey-Bass. \nChatman, J. A. and A. Choi (2022). ‘Measuring Organizational Culture: Converging on Definitions and \nApproaches to Advance the Paradigm’. In C. Newton and R. Knight (eds), Handbook of Research \nMethods for Organisational Culture , pp. 92 –107. Cheltenham, Northampton, MA: Edward Elgar \nPublishing. \nChatman, J. A. and C. A. O’Reilly (2016). ‘Paradigm Lost: Reinvigorating the Study of Organizational \nCulture’, Research in Organizational Behavior, 36, pp. 199–224. \nCorritore, M., A. Goldberg and S. B. Srivastava (2020). ‘Duality in Diversity: How Intrapersonal and \nInterpersonal Cultural Heterogeneity Relate to Firm Performance’, Administrative Science \nQuarterly, 65, pp. 359–394. \nDevlin, J., M.-W. Chang, K. Lee and K. Toutanova (2018). ‘BERT: Pre-training of Deep Bidirectional \nTransformers for Language Understanding’, arXiv preprint arXiv:1810.04805v2. \nFiordelisi, F. and O. Ricci (2014). ‘Corporate culture and CEO turnover’, Journal of Corporate Finance, \n28, pp. 66–82. \nGonzález-Carvajal, S. and E. C. Garrido -Merchán (2020). ‘Comparing BERT against Traditional \nMachine Learning Text Classification’, arXiv preprint arXiv:2005.13012v2. \nGrennan, J. A. (2019). ‘A Corporate Culture Channel: How Increased Shareholder Governance Reduces \nFirm Value’, SSRN Electronic Journal. \nHartnell, C. A., A. Y. Ou and A. Kinicki (2011). ‘Organiza tional Culture and Organizational \nEffectiveness: A Meta -Analytic Investigation of the Competing Values Framework's Theoretical \nSuppositions’, Journal of Applied Psychology, 96, pp. 677–694. \nHartnell, C. A., A. Y. Ou, A. J. Kinicki, D. Choi and E. P. Karam (2019). ‘A Meta -Analytic Test of \nOrganizational Culture's Association with Elements of an Organization's System and Its Relative \nPredictive Validity on Organizational Outcomes’, Journal of Applied Psychology, 104, pp. 832–850. \nHuang, A. H., H. Wang and Y. Yang (2023). ‘FinBERT: A Large Language Model for Extracting \nInformation from Financial Text’, Contemporary Accounting Research, 40, pp. 806–841. \nLee, J., W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So and J. Kang (2020). ‘BioBERT: A Pre -Trained \nBiomedical Language Representation Model for Biomedical Text Mining’, Bioinformatics, 36, pp. \n1234–1240. \nLee, J.-S. and J. Hsiang (2020). ‘Patent Classification by Fine-Tuning BERT Language Model’, World \nPatent Information, 61. \nLi, K., F. Mai, R. Shen and X. Yan (2021). ‘Measuring Corporate Culture Using Machine Learning’, \nReview of Financial Studies, 34, pp. 3265–3315. \nLiu, Y., M. Ott, N. Goyal, Du Jingfei, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer and V. \nStoyanov (2019). ‘RoBERTa: A Robustly Optimized BERT Pretraining Approach’, arXiv preprint \narXiv:1907.11692v1. \nNguyen, D. D., L. Nguyen and V. Sila (2019). ‘Does Corporate Culture Affect Bank Risk -Taking? \nEvidence from Loan-Level Data’, British Journal of Management, 30, pp. 106–133. \n \n19 \nO'Reilly, C. A. and J. A. Chatman (1996). ‘Culture as Social Control: Corporations, Cults, and \nCommitment’. In B. M. Staw and L. L. Cummings (eds), Research in Organizational Behavior: An \nAnnual Series of Analytical Essays and Critical Reviews, pp. 157–200: Elsevier Science. \nPasch, S. (2018). ‘Corporate Culture and Industry -Fit: A Text Mining Approach’, IZA Conference \nPaper. \nQuinn, R. E. and J. R. Kimberly (1984). ‘Paradox, Planning, and Perseverance: Guidelines for \nManagerial Practice’. In J. R. Kimberly and R. E. Quinn (eds), Managing Organizational \nTranslations, pp. 295–313. \nQuinn, R. E. and J. Rohrbaugh (1983). ‘A Spatial Model of Effectiveness Criteria: Towards a Competing \nValues Approach to Organizational Analysis’, Management Science, 29, pp. 363–377. \nRibeiro, M. T., S. Singh and C. Guestrin (2016). ‘\"Why Should I Trust You?\" Explaining the Predictions \nof Any Classifier’, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge \nDiscovery and Data Mining. \nSchmiedel, T., O. Müller and J. vom Brocke (2019). ‘Topic Modeling as a Strategy of Inquiry in \nOrganizational Research: A Tutorial with an Application Example on Organizational Culture’, \nOrganizational Research Methods, 22, pp. 941–968. \nSrivastava, S. B., A. Goldberg, V. G. Manian and C. Potts (2018). ‘Enculturat ion Trajectories: \nLanguage, Cultural Adaptation, and Individual Outcomes in Organizations’, Management Science, \n64, pp. 1348–1364. \nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser and I. Polosukhin \n(2017). ‘Attention is All you Need’, Advances in Neural Information Processing Systems 30 (NIPS \n2017). \nWang, Y., H. Farag and W. Ahmad (2021). ‘Corporate Culture and Innovation: A Tale from an \nEmerging Market’, British Journal of Management, 32, pp. 1121–1140. \n  \n \n20 \nFootnotes \n \n1 Our language models are uploaded under https://huggingface.co/CultureBERT. In addition, we provide \na tutorial on how to apply our models to measure corporate culture from text documents under \nhttps://github.com/Stefan-Pasch/CultureBERT. \n2 On the Huggingface model hub, as of November 2022, BERT and RoBERTa show the highest number \nof downloads among the English language models used for text classification. See \nhttps://huggingface.co/models. \n3 In fact, we are not aware of any application of supervised machine learning to classify text with respect \nto the underlying corporate culture dimensions. \n  \n \n21 \nSupplementary Online Appendix \n \nOnline Appendix A: Dictionaries \n \nTABLE A1 \nStemmed Dictionaries  \nCulture \ndimension \n Stemmed dictionary \nClan positive clan, affili, connect, attach, bond, adher, adhes, trust, collabor, coaction, \ncollaboration, quisling, join_forc, cooper, togeth, get_togeth, \nplay_along, go_along, support, help, assisten, encourag, teamwork, \nengag, particip, involv, involut, commit, open, recept, overt, commun, \nsatisfact, gratif, comfort, pride, allegi, loyalti, dedic, devot, not_uncoop, \nnot_disjunct, not_unaccommod, not_discourag, not_unsupport, \nnot_disconnect, not_distrust, not_not-engag, not_nonparticip, not_non-\ninvolv, not_unrecept, not_covert, not_dissatisfact, not_humil, \nnot_discomfort, not_disloyalti \n negative uncoop, disjunct, unaccommod, discourag, unsupport, disconnect, \ndistrust, non-engag, nonparticip, non-involv, unrecept, covert, \ndissatisfact, humil, discomfort, disloyalti, not_clan, not_affili, \nnot_connect, not_attach, not_bond, not_adher, not_adhes, not_trust, \nnot_collabor, not_coaction, not_collaboration, not_quisling, \nnot_join_forc, not_cooper, not_togeth, not_get_togeth, not_play_along, \nnot_go_along, not_support, not_help, not_assisten, not_encourag, \nnot_teamwork, not_engag, not_particip, not_involv, not_involut, \nnot_commit, not_open, not_recept, not_overt, not_commun, \nnot_satisfact, not_gratif, not_comfort, not_pride, not_allegi, not_loyalti, \nnot_dedic, not_devot \nAdhocracy positive hazard, jeopardi, peril, risk, endanger, danger, gambl, adventur, jeopard, \nadapt, flexibl, pliabil, plianci, pliant, advanc, forward-look, innov, \nmodern, groundbreak, invent, excogit, concept, initi, found, foundat, \ncreation, introduct, instaur, attent, detail, technic, high_spot, creat, \nproduc, alter, modif, chang, varieti, modifi, growth, grow, matur, \ndevelop, mergenc, outgrowth, stimul, divers, multifari, creativ, \ncreative_think, adhocraci, not_safeti, not_safe, not_unadapt, not_inflex, \nnot_intract, not_regress, not_demot, not_old_styl, not_nonmodern, \nnot_abolish, not_misconcept, not_inflect, not_imprecis, not_inaccur, \nnot_inexact, not_inattent, not_nondevelop, not_uncr \n negative safeti, safe, unadapt, inflex, intract, regress, demot, old_styl, \nnonmodern, abolish, misconcept, inflect, imprecis, inaccur, inexact, \ninattent, nondevelop, uncr, not_hazard, not_jeopardi, not_peril, not_risk, \nnot_endanger, not_danger, not_gambl, not_adventur, not_jeopard, \nnot_adapt, not_flexibl, not_pliabil, not_plianci, not_pliant, not_advanc, \nnot_forward-look, not_innov, not_modern, not_groundbreak, \nnot_invent, not_excogit, not_concept, not_initi, not_found, not_foundat, \nnot_creation, not_introduct, not_instaur, not_attent, not_detail, \nnot_technic, not_high_spot, not_creat, not_produc, not_alter, not_modif, \nnot_chang, not_varieti, not_modifi, not_growth, not_grow, not_matur, \nnot_develop, not_mergenc, not_outgrowth, not_stimul, not_divers, \nnot_multifari, not_creativ, not_creative_think, not_adhocraci \n  \n \n22 \nMarket positive market, commerci, commercialis, offer, contest, competit, rivalri, \naccomplish, achiev, commun, compet, goal, destin, aim, object, target, \nintent, purpos, plan, prepar, provis, be_aft, project, contriv, design, task, \naggress, belliger, pugnac, fast-grow, strong-grow, profit, earn, gain, \nbenefit, qualiti, product, gener, fertil, not_cooper, not_incompet, \nnot_subject, nott_resolut, not_unaggress, not_noncompetit, \nnot_unproduc \n negative cooper, incompet, subject, resolut, unaggress, noncompetit, unproduct, \nnot_market, not_commerci, not_commercialis, not_offer, not_contest, \nnot_competit, not_rivalri, not_accomplish, not_achiev, not_commun, \nnot_compet, not_goal, not_destin, not_aim, not_object, not_target, \nnot_intent, not_purpos, not_plan, not_prepar, not_provis, not_be_aft, \nnot_project, not_contriv, not_design, not_task, not_aggress, \nnot_belliger, not_pugnac, not_fast-grow, not_strong-grow, not_profit, \nnot_earn, not_gain, not_benefit, not_qualiti, not_product, not_gener, \nnot_fertil \nHierarchy positive hierarchi, structur, hierarch, control, restraint, command, stabl, stabil, \nconstanc, static, unchang, procedur, function, routin, formal, formalis, \nconvent, schemat, consist, reproduc, coher, logic, order, uniform, \nconform, complianc, abid, accord, predict, effici, effect, time, season, \nsmooth, not_nonhierarch, not_unrestraint, not_instabl, not_unstabl, \nnot_inconst, not_malfunct, not_inform, not_unconvent, not_inconsist, \nnot_incoher, not_illog, not_unseason, not_ineffect, not_ineffici, \nnot_unpredict, not_nonconform, not_noncompli \n negative nonhierarch, unrestraint, instabl, unstabl, inconst, malfunct, inform, \nunconvent, inconsist, incoher, illog, unseason, ineffect, ineffici, \nunpredict, nonconform, noncompli, not_hierarchi, not_structur, \nnot_hierarch, not_control, not_restraint, not_command, not_stabl, \nnot_stabil, not_constanc, not_static, not_unchang, not_procedur, \nnot_function, not_routin, not_formal, not_formalis, not_convent, \nnot_schemat, not_consist, not_reproduc, not_coher, not_logic, \nnot_order, not_uniform, not_conform, not_complianc, not_abid, \nnot_accord, not_predict, not_effici, not_effect, not_time, not_season, \nnot_smooth \nTable presents the stemmed dictionaries we use to apply the dictionary method as described in section 4.3. We compute for \neach employee review and each culture dimension the share of words that appear in the corresponding dictionary of positive \nword stems. To take into account negations, we subtract from this share the share of words that appear in the corresponding \ndictionary of negative word stems. A word stem that appears in the dictionary with the prefix “not_” is counted only if it \nappears in a sentence with a negation word, such as “not” or “never”. \n \n \n  \n \n23 \nOnline Appendix B: F1 Scores \n \n \n \nOnline Appendix C: Results Applying Dictionaries from Fiordelisi and \nRicci (2014) \n \nTABLE B1  \nF1 Scores of Different Methods of Text Classification  \nMethod of text classification \nDominant \nculture Clan Adhocracy Market Hierarchy \n \nRandom  0.25 0.33 0.33 0.33 0.33  \nMajority class  0.20 0.24 0.71 0.55 0.51  \n        \nDictionary method  0.33 0.36 0.70 0.54 0.55  \n        \nTF-IDF + logistic reg. 0.44 0.60 0.73 0.60 0.62  \n + random forest 0.41 0.55 0.74 0.59 0.70  \n + XGBoost 0.44 0.59 0.77 0.70 0.72  \n        \nRoBERTa base 0.54 0.63 0.84 0.72 0.74  \n  large 0.63 0.69 0.88 0.76 0.81  \nTable reports F1 scores obtained when classifying the 400 employee reviews of the test data set. See notes of Table 4. \n TABLE C1 \n Accuracy and F1 Scores Using Dictionaries from Fiordelisi and Ricci (2014) \nMethod of text classification \n \nScore \nDominant \nculture Clan Adhocracy Market Hierarchy \nDictionary method \n+ word count \nAccuracy 0.32 0.34 0.75 0.58 0.62 \nF1 0.34 0.34 0.72 0.58 0.54 \nTable presents accuracy scores and F1 scores that are obtained applying the dictionary method as described in section 4.3 but \nusing the dictionaries proposed by Fiordelisi and Ricci (2014). See notes of Table 4. \n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7720974683761597
    },
    {
      "name": "Computer science",
      "score": 0.6674247980117798
    },
    {
      "name": "Language model",
      "score": 0.5793322920799255
    },
    {
      "name": "Natural language processing",
      "score": 0.4945027530193329
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4792071282863617
    },
    {
      "name": "Compiler",
      "score": 0.44230714440345764
    },
    {
      "name": "Engineering",
      "score": 0.17040199041366577
    },
    {
      "name": "Voltage",
      "score": 0.0980890691280365
    },
    {
      "name": "Programming language",
      "score": 0.08292049169540405
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I114090438",
      "name": "Goethe University Frankfurt",
      "country": "DE"
    }
  ],
  "cited_by": 1
}