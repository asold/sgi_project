{
    "title": "Vision–language foundation model for echocardiogram interpretation",
    "url": "https://openalex.org/W4396494945",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4222347360",
            "name": "Christensen, Matthew",
            "affiliations": [
                "Cedars-Sinai Smidt Heart Institute",
                "Cedars-Sinai Medical Center"
            ]
        },
        {
            "id": null,
            "name": "Vukadinovic, Milos",
            "affiliations": [
                "Cedars-Sinai Medical Center",
                "Cedars-Sinai Smidt Heart Institute",
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A4229477009",
            "name": "Yuan, Neal",
            "affiliations": [
                "San Francisco VA Medical Center",
                "University of California, San Francisco"
            ]
        },
        {
            "id": "https://openalex.org/A4229477003",
            "name": "Ouyang, David",
            "affiliations": [
                "Cedars-Sinai Medical Center",
                "Cedars-Sinai Smidt Heart Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225408423",
        "https://openalex.org/W3047523445",
        "https://openalex.org/W2612219720",
        "https://openalex.org/W3013692475",
        "https://openalex.org/W2896287590",
        "https://openalex.org/W3217446125",
        "https://openalex.org/W4225364604",
        "https://openalex.org/W4386090740",
        "https://openalex.org/W2949355266",
        "https://openalex.org/W3002705197",
        "https://openalex.org/W3174416962",
        "https://openalex.org/W6800751262",
        "https://openalex.org/W3182683290",
        "https://openalex.org/W2902617128",
        "https://openalex.org/W4313197536",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4366735820",
        "https://openalex.org/W4380715596",
        "https://openalex.org/W4310266426",
        "https://openalex.org/W6853019041",
        "https://openalex.org/W4385948838",
        "https://openalex.org/W4392947532",
        "https://openalex.org/W4386697749",
        "https://openalex.org/W4389649986",
        "https://openalex.org/W4312443924",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W4320495974",
        "https://openalex.org/W4386065512",
        "https://openalex.org/W4362608384",
        "https://openalex.org/W4388407558",
        "https://openalex.org/W4385335928",
        "https://openalex.org/W2963428668",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W3132346797",
        "https://openalex.org/W2190898938",
        "https://openalex.org/W3183311193",
        "https://openalex.org/W794952073",
        "https://openalex.org/W4312160081",
        "https://openalex.org/W4229011824",
        "https://openalex.org/W2766742462",
        "https://openalex.org/W2097087273"
    ],
    "abstract": "Abstract The development of robust artificial intelligence models for echocardiography has been limited by the availability of annotated clinical data. Here, to address this challenge and improve the performance of cardiac imaging models, we developed EchoCLIP, a vision–language foundation model for echocardiography, that learns the relationship between cardiac ultrasound images and the interpretations of expert cardiologists across a wide range of patients and indications for imaging. After training on 1,032,975 cardiac ultrasound videos and corresponding expert text, EchoCLIP performs well on a diverse range of benchmarks for cardiac image interpretation, despite not having been explicitly trained for individual interpretation tasks. EchoCLIP can assess cardiac function (mean absolute error of 7.1% when predicting left ventricular ejection fraction in an external validation dataset) and identify implanted intracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for pacemakers, percutaneous mitral valve repair and artificial aortic valves, respectively). We also developed a long-context variant (EchoCLIP-R) using a custom tokenizer based on common echocardiography concepts. EchoCLIP-R accurately identified unique patients across multiple videos (AUC of 0.86), identified clinical transitions such as heart transplants (AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust image-to-text search (mean cross-modal retrieval rank in the top 1% of candidate text reports). These capabilities represent a substantial step toward understanding and applying foundation models in cardiovascular imaging for preliminary interpretation of echocardiographic findings.",
    "full_text": "Nature Medicine | Volume 30 | May 2024 | 1481–1488\n 1481\nnature medicine\nhttps://doi.org/10.1038/s41591-024-02959-y\nArticle\nVision–language foundation model for \nechocardiogram interpretation\nMatthew Christensen    1, Milos Vukadinovic1,2, Neal Yuan    3,4 & \nDavid Ouyang    1,5 \nThe development of robust artificial intelligence models for \nechocardiography has been limited by the availability of annotated clinical \ndata. Here, to address this challenge and improve the performance of \ncardiac imaging models, we developed EchoCLIP, a vision–language \nfoundation model for echocardiography, that learns the relationship \nbetween cardiac ultrasound images and the interpretations of expert \ncardiologists across a wide range of patients and indications for imaging. \nAfter training on 1,032,975 cardiac ultrasound videos and corresponding \nexpert text, EchoCLIP performs well on a diverse range of benchmarks for \ncardiac image interpretation, despite not having been explicitly trained  \nfor individual interpretation tasks. EchoCLIP can assess cardiac function \n(mean absolute error of 7.1% when predicting left ventricular ejection \nfraction in an external validation dataset) and identify implanted \nintracardiac devices (area under the curve (AUC) of 0.84, 0.92 and 0.97 for \npacemakers, percutaneous mitral valve repair and artificial aortic valves, \nrespectively). We also developed a long-context variant (EchoCLIP-R) \nusing a custom tokenizer based on common echocardiography concepts. \nEchoCLIP-R accurately identified unique patients across multiple videos \n(AUC of 0.86), identified clinical transitions such as heart transplants  \n(AUC of 0.79) and cardiac surgery (AUC 0.77) and enabled robust \nimage-to-text search (mean cross-modal retrieval rank in the top 1% of \ncandidate text reports). These capabilities represent a substantial step \ntoward understanding and applying foundation models in cardiovascular \nimaging for preliminary interpretation o f e ch ocardiographic findings.\nEchocardiography, or cardiac ultrasound, is the most common, non-\ninvasive method of evaluating heart function and identifying heart \ndisease. Echocardiography routinely guides clinical cardiology \ndecision-making1–3 and is used for disease diagnosis, risk stratifica-\ntion and assessment of treatment response1,4. Recent work has used \nartificial intelligence (AI) to improve the accuracy of echocardiographic \nmeasurements 5–7 and disease diagnoses 8–10; however, these AI \napproaches focus on narrow individual tasks that require specific train-\ning for each task and do not use vision–language foundation models11.\nRecent advances in AI have leveraged representation learning on \nlarge image and text datasets to develop vision–language foundation \nmodels that generalize beyond narrow sets of predefined tasks 12,13. \nReceived: 17 August 2023\nAccepted: 28 March 2024\nPublished online: 30 April 2024\n Check for updates\n1Department of Cardiology, Smidt Heart Institute, Cedars-Sinai Medical Center, Los Angeles, CA, USA. 2Department of Bioengineering, University of \nCalifornia Los Angeles, Los Angeles, CA, USA. 3Department of Medicine, University of California San Francisco, San Francisco, CA, USA. 4Division of \nCardiology, San Francisco Veterans Affairs Medical Center, San Francisco, CA, USA. 5Division of Artificial Intelligence in Medicine, Cedars-Sinai Medical \nCenter, Los Angeles, CA, USA.  e-mail: david.ouyang@cshs.org\nNature Medicine | Volume 30 | May 2024 | 1481–1488 1482\nArticle https://doi.org/10.1038/s41591-024-02959-y\nIn this work, we introduce EchoCLIP, a foundation model for echo-\ncardiography trained on a dataset of 1,032,975 echocardiogram videos \nsourced from over a decade of clinical imaging. We developed a method \nfor substantially compressing echocardiography reports, simplify -\ning the matching of clinical text assessments to images to focus on \nimportant clinical concepts. T o assess the model’s performance, we \ntested the model’s ability to assess cardiac function, pulmonary artery \npressure (PAP) and chamber size, as well as identify common intracar-\ndiac devices in both held-out internal test cohorts as well as external \ntest cohorts. By using the model to compare pairs of echocardiogram \nstudies, we can assess the model’s ability to identify unique patients \nacross time, identify clinically important changes in disease state and \nretrieve relevant clinical text for given images. Finally, we propose a new \nvision–language model interpretation approach based on matching \nrelevant text with important regions of interest in images.\nThese models learn to encode images and text into compact repre -\nsentations that can then be used to perform a wide variety of separate \nprediction tasks for which the model was never specifically trained \n(‘zero-shot’ tasks). Given the broad range of data used to train these \nmodels, the performance of foundation models are often more robust \nthan with conventional convolutional neural networks14,15. In biomedi-\ncal applications, foundation models have been developed to organize \nbiological16–18 and medical19 datasets, including modality-specific mod-\nels for chest X-rays, retinal imaging, wearable waveforms and pathol-\nogy images20–25. Training of foundation models on medical imaging \nhas been bottlenecked by dataset size and is often limited to publicly \navailable data that may not represent the range of disease severities \nand possible presentations. While text information might be imprecise, \nclinician evaluations of medical imaging provide an information-rich \ndistillation of complex data.\nTable 1 | Clinical characteristics of Cedars-Sinai Medical Center study cohort, reported per echocardiography study\nTotal Training Validation Test\nn 224,685 195,082 8,119 21,484\nAge (mean (s.d.)) 66.26 (16.74) 66.3 (16.7) 65.8 (17.0) 65.7 (16.9)\nFemale = true (%) 96,451 (42.9) 83,700 (42.9) 3,363 (41.4) 9,388 (43.7)\nRace (%)\n Native American 526 (0.2) 456 (0.2) 23 (0.3) 47 (0.2)\n Asian 16,601 (7.5) 14,450 (7.5) 555 (6.9) 1,596 (7.5)\n Black 29,546 (13.3) 25,624 (13.3) 1,104 (13.8) 2,818 (13.3)\n Hispanic 22,424 (10.1) 19,394 (10.0) 842 (10.5) 2,188 (10.3)\n Non-Hispanic white 133,399 (60.0) 116,044 (60.1) 4,699 (58.6) 12,656 (59.6)\n Other 15,376 (6.9) 13,243 (6.9) 612 (7.6) 1,521 (7.2)\n Pacific Islander 767 (0.3) 688 (0.4) 36 (0.4) 43 (0.2)\n Unknown 3,700 (1.7) 3,182 (1.6) 149 (1.9) 369 (1.7)\n AF 46,994 (20.9) 41,214 (21.1) 1,633 (20.1) 4,147 (19.3)\n HF 75,358 (33.5) 65,802 (33.7) 2,764 (34.0) 6,792 (31.6)\n HTN 90,738 (40.4) 79,229 (40.6) 3,250 (40.0) 8,259 (38.4)\n CVA/TIA/TE 38,283 (17.0) 33,475 (17.2) 1,378 (17.0) 3,430 (16.0)\n MI 14,983 (6.7) 13,120 (6.7) 514 (6.3) 1,349 (6.3)\n CAD 55,659 (24.8) 48,840 (25.0) 2,040 (25.1) 4,779 (22.2)\n PAD 23,369 (10.4) 20,475 (10.5) 838 (10.3) 2,056 (9.6)\n DM 37,900 (16.9) 33,226 (17.0) 1,351 (16.6) 3,323 (15.5)\n CKD 40,947 (18.2) 35,960 (18.4) 1,482 (18.3) 3,505 (16.3)\n Previous smoker 7,632 (3.4) 6,593 (3.4) 256 (3.2) 783 (3.6)\nAF, atrial fibrillation; HF, heart failure; HTN, hypertension; CVA, cerebrovascular accident; TIA, transient ischemic attack; TE, thromboembolism; MI, myocardial infarction; CAD, coronary artery \ndisease; PAD, pulmonary artery disease; DM, diabetes mellitus; CKD, chronic kidney disease.\nTable 2 | Main performance metrics\nImage encoder Tokenizer MCMRR LVEF, MAE PAP , MAE TAVR, AUC MitraClip, AUC Pacemaker, AUC\nCLIP ViT-B-32 CLIP BPE 10,743.0 20.8 \n(20.7–20.8)\n16.8 \n(16.8–16.9)\n0.46 \n(0.46–0.47)\n0.53 \n(0.52–0.54)\n0.51  \n(0.51–0.52)\nEchoCLIP ConvNeXt CLIP BPE 571.3 8.4 \n(8.3–8.4)\n10.8 \n(10.8–10.9)\n0.92 \n(0.91–0.92)\n0.97 \n(0.97–0.97)\n0.84  \n(0.84–0.84)\nEchoCLIP-R (full-report \nprompts)\nConvNeXt Template tokenizer 206.1 10.9 \n(10.9–11.0)\n13.2 \n(13.1–13.2)\n0.85 \n(0.85–0.86)\n0.95 \n(0.94–0.95)\n0.77  \n(0.77–0.78)\nEchoCLIP-R (base) ConvNeXt Template tokenizer 206.1 16.9 \n(16.8–17.0)\n17.5 \n(17.4–17.5)\n0.52 \n(0.51–0.52)\n0.81 \n(0.81–0.82)\n0.66  \n(0.65–0.66)\nRetrieval ranks are out of 21,484 candidates. Performance of the best-performing model for each metric is bolded. Ranges in parentheses indicate 95% CI bootstrapped with 1,000 random \nsamples. MCMRR, mean cross-modal retrieval rank; BPE, Byte-Pair Encoding.\nNature Medicine | Volume 30 | May 2024 | 1481–1488\n 1483\nArticle https://doi.org/10.1038/s41591-024-02959-y\nResults\nEchoCLIP is an echocardiography vision–language model trained with \n1,032,975 video–text pairs derived from 224,685 echocardiography \nstudies across 99,870 patients across a decade of clinical care (Table 1). \nIn a self-supervised approach, EchoCLIP is trained on pairs of echocar-\ndiogram images (randomly sampled from video frames) and associated \nclinical report text without direct labeling of clinical interpretations \nor measurements. The EchoCLIP model uses a ConvNeXt-Base26 image \nencoder and a Byte-Pair Encoding text tokenizer27. The text encoder \narchitecture is a decoder-only transformer identical to the architecture \nused by the original CLIP paper23 and has an input context length of 77 \ntokens. Despite not being directly trained on specific interpretation \ntasks, EchoCLIP can accurately identify implanted devices as well as \nassess cardiac form and function (Table 2). T o assess the importance \nof pretraining and architecture28, different architectures and dataset \nconfigurations were compared (Supplementary Table 1).\nT o fit an entire echocardiography report into the text encoder, a \ndomain-specific echocardiography text tokenization format succinctly \nsummarizing common cardiovascular concepts was developed. The \nmodel variant trained with this tokenization format, EchoCLIP-R, is \ncapable of retrieving relevant clinical text from images and character-\nizes clinical changes over time. We also introduce a saliency mapping \napproach based on cosine similarity, PromptCAM, to show that Echo-\nCLIP prioritizes important image features relevant to the associated \ntext. This approach identifies clinically relevant regions of interest in \nechocardiography images based on prompted clinical text.\nEchocardiogram interpretation without supervised learning\nWithout fine-tuning or task-specific training, we evaluated EchoCLIP’s \nperformance on a wide range of benchmark classification tasks in our \ninternal held-out test set. EchoCLIP can accurately identify intracardiac \ndevices, including percutaneous mitral valve repair with an AUC of \n0.97 (95% CI 0.97–0.98), transvenous aortic valve replacement (TAVR) \nwith an AUC of 0.92 (95% CI 0.91–0.92) and pacemaker/defibrillator \nleads with an AUC of 0.84 (95% CI 0.84–0.85). EchoCLIP can also detect \nchanges from a healthy cardiac chamber size, including severe dilation \nof the right ventricle with an AUC of 0.92 (95% CI 0.91–0.92), right atrium \nwith an AUC of 0.97 (95% CI 0.97–0.98), left ventricle with an AUC of \n0.92 (95% CI 0.92–0.93) and left atrium with an AUC of 0.91 (95% CI \n0.90–0.92). Last, EchoCLIP can assess for tamponade (AUC 0.96, 95% \nCI 0.94–0.98) and severe left ventricular hypertrophy (AUC 0.82, 95% \nCI 0.81–0.83). The sensitivity and specificity for each task are described \nin Extended Data Table 1. Performance was similar across key subsets \nstratified by age, sex and image quality (Supplementary Table 2).\nExternal validation of cardiac function and pressure assessment\nWe further evaluated EchoCLIP’s performance on quantitative tasks, \nincluding evaluation of left ventricular ejection fraction (L VEF) and \nZero-shot assessment of \nLVEF\na\nb\nc Zero-shot identification of implanted \ncardiac devices\nImage \nencoder\nText \nencoder\nNormal left ventricular \nsystolic function.\nLVEF is 73%. \nThe LVEF is calculated\nusing the single plane\nSimpson's rule method.\nMild diastolic dysfunction...\nCorrelation \nbetween \nembeddings\nDesired \ncorrelation 0 0.2 0.4 0.6 0.8 1.0\nFPR\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\nMitraClip | AUC: 0.97 (0.96–0.98)\nTAVR | AUC: 0.92 (0.91–0.92)\nPacemaker | AUC: 0.84 (0.83–0.85)\n0 20 40 60 80 100\nGround-truth LVEF (%)\n0\n20\n40\n60\n80\n100\nPredicted LVEF (%)\nCSMC | MAE: 8.30 (8.25–8.35)\nSHC | MAE: 7.14 (6.99–7.30)\nFig. 1 | EchoCLIP workflow. a, EchoCLIP is a foundation model trained on more \nthan 1 million echocardiogram videos across 11 years. It is composed of an image \nencoder for processing echocardiogram video frames and a text encoder for \nprocessing the corresponding physician interpretations. These two encoders \nproject the images and interpretations onto a joint embedding space. b, Scatter-\nplot of zero-shot prediction versus label of left ventricular ejection fraction \n(L VEF) in held-out test dataset from Cedars-Sinai Medical Center (CSMC; blue, \nn = 100,994) and Stanford Healthcare (SHC; red, n = 5,000). c, AUC performance \nfor various implanted intracardiac devices, including MitraClip, TAVR valves and \nimplanted pacemaker/defibrillator on held-out test dataset from Cedars-Sinai \nMedical Center. FPR, false positive rate; TPR, true positive rate.\nNature Medicine | Volume 30 | May 2024 | 1481–1488 1484\nArticle https://doi.org/10.1038/s41591-024-02959-y\nPAP. EchoCLIP predicts L VEF on the held-out internal test dataset with \na mean absolute error (MAE) of 8.4% and an MAE of 7.1% on an external \ntest set of videos from the EchoNet-Dynamic dataset from Stanford \nHealthcare (Fig. 1). At key clinical L VEF thresholds, EchoCLIP achieves \nan AUC of 0.89–0.90 for an L VEF threshold of 50%, 0.93–0.94 for an \nL VEF threshold of 40% and 0.95–0.97 for an L VEF threshold of 30% (Sup-\nplementary Table 3 and Supplementary Fig. 1). Furthermore, EchoCLIP \npredicts estimated PAP with an MAE of 10.8 mm Hg on the internal test \ndataset and an MAE of 10.8 on the external test dataset (Fig. 2).\nMapping clinical text to echocardiogram images\nGiven the long length of an echocardiography report, we developed \nEchoCLIP-R, a domain-specific text encoder that succinctly summa-\nrized common cardiovascular concepts into fewer tokens and was \nable to summarize a whole report during training. A long-context \nEchoCLIP-R model was optimized for retrieval during training. Given \na representative image from the held-out test cohort, EchoCLIP-R ranks \nthe matching clinical report on average 209th out of 21,484 candidates \n(top 1% retrieval). The correct report is present in the top ten reports \n33.3% of the time. Going from text to image, the average rank of the \nmatching video is 203 out of 21,484 and the correct video is present in \nthe top ten ranked videos 34.3% of the time. For all language models,  \nthe choice of text prompts impacts model performance and we found \nEchoCLIP to be easier to generate focused prompts compared to \nEchoCLIP-R given the larger context for in-domain prompts (Table 2). \nA workflow for automated preliminary assessment of echocardiogram  \nstudies by ensembling assessments across videos is shown in the  \nSupplementary Video.\nDetection of clinical differences between videos\nThe ability to measure the similarity between pairs of echocardiograms \ncan also be used to identify a unique patient across multiple studies (a \ndifficult task for human clinicians) as well as identify clinical changes \nover time. Comparing the cosine similarity between EchoCLIP-R embed-\ndings of different echocardiography studies can help in challenging \nclinical scenarios. Pairs of EchoCLIP-R embeddings of echocardio -\ngrams are, on average, least similar if they come from two different \npatients (mean cosine similarity 0.40, 95% CI 0.39–0.41), more similar if \nthey come from the same patient but were acquired on different dates \n(mean cosine similarity 0.64, 95% CI 0.64–0.65) and most similar if they \ncome from the same patient and were acquired on the same day (mean \ncosine similarity 0.87, 95% CI 0.86–0.87). This comparison results in \nan AUC of 0.86 (95% CI 0.85–0.87) in identifying the same patients \nacross different videos. Furthermore, the cosine similarity between \nvideos can also be used to distinguish when there was a substantive \nclinical change. Echocardiograms acquired before cardiac surgeries \nand orthotopic heart transplants tend to be similar to one another, \nwhile being substantially less similar to echocardiograms acquired \nafter such procedures (Fig. 3). This dropoff in embedding similarity is \nsufficient to predict whether an echocardiogram occurs before or after \n0 20 40 60 80 100\n0\n20\n40\n60\n80\n100\nCSMC | MAE: 10.79 (10.73–10.86)\nSHC | MAE: 10.82 (10.52–11.11)\n0\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nCSMC | AUC: 0.90 (0.89–0.91)\nSHC | AUC: 0.89 (0.88–0.90)\nGround-truth PAP (mm Hg)\nPredicted PAP (mm Hg)\nTPR\n0.2 0.4 0.6 0.8 1.0\nFPR\n0\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\n0.2 0.4 0.6 0.8 1.0\nFPR\n0\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nSevere | AUC: 0.82 (0.81–0.83)\nModerate | AUC: 0.75 (0.74–0.75)\nMild | AUC: 0.65 (0.65–0.66)\nSevere | AUC: 0.91 (0.90–0.92)\nModerate | AUC: 0.85 (0.84–0.86)\nMild | AUC: 0.74 (0.74–0.75)\n0\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\n0.2 0.4 0.6 0.8 1.0\nFPR\nSevere | AUC: 0.92 (0.92–0.93)\nModerate | AUC: 0.86 (0.86–0.87)\nMild | AUC: 0.78 (0.78–0.79)\n0\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\n0.2 0.4 0.6 0.8 1.0\nFPR\nTamponade | AUC: 0.96 (0.93–0.97)\nModerate | AUC: 0.94 (0.93–0.95)\nSmall | AUC: 0.71 (0.70–0.71)\nLarge | AUC: 0.96 (0.94–0.97)\nTPR\n0.2 0.4 0.6 0.8 1.0\nFPR\nLeft ventricle dilation\nHFPAPa b Left ventricular hypertrophyc\nPericardial eﬀusionfLeft atrium dilationd e\nFig. 2 | Zero-shot model performance on held-out test apical-four-chamber \nvideos. a, Estimation of pulmonary artery pressure (PAP). b, Heart failure (HF) \nwith reduced ejection fraction. c, Assessment of left ventricular hypertrophy at \nvarious degrees of severity (mild, moderate and severe). d, Left atrial dilation \nat various degrees of severity (mild, moderate and severe). e, Left ventricular \ndilation at various degrees of severity (mild, moderate and severe). f, Assessment \nof pericardial effusion size (small, moderate and large) as well as presence of \ntamponade physiology. Data are from the Cedars-Sinai Medical Center (CSMC; \nblue, n = 100,994) and Stanford Healthcare (SHC; red, n = 5,000). FPR, false \npositive rate; TPR, true positive rate.\nNature Medicine | Volume 30 | May 2024 | 1481–1488\n 1485\nArticle https://doi.org/10.1038/s41591-024-02959-y\ncardiac surgery with an AUC of 0.77 (95% CI 0.75–0.79) and before or \nafter heart transplant with an AUC of 0.79 (95% CI 0.76–0.82). Addition-\nally, we show that the difference in reported L VEF between different \nstudies from the same patient is correlated with the cosine similarity \nbetween videos, suggesting that EchoCLIP-R embeddings can be used \nto identify clinically relevant serial changes (Supplementary Fig. 2).\nInterpretation studies\nT o further interrogate EchoCLIP’s understanding of cardiovascular \ndisease, we utilized two interpretability frameworks. First, we devel-\noped a modified class activation mapping method (PromptCAM) for \nmultimodal models that pairs textual prompts with imaging features. \nPromptCAM identifies regions of interest in the image that maximize \nthe cosine similarity with the text prompts. Despite not seeking to \nminimize the loss of direct text labels in training, PromptCAM high-\nlights the learned associations of EchoCLIP for subconcepts such as \n‘TAVR’ , ‘Impella’ , ‘Pacemaker’ or ‘Mitraclip’ (Fig. 4). Secondarily, we \napplied Uniform Manifold Approximation and Projection (UMAP) \non the embeddings from the EchoCLIP image encoder and observed \nnumerous clusters associated with different cardiovascular diseases, \ndisease states and measurements (Supplementary Fig. 3).\nDiscussion\nOur results suggest that large datasets of echocardiography studies and \nexpert adjudicated interpretations can serve as the basis for training \nmedical foundation models. Our echocardiography foundation model \nwas able to successfully complete multiple benchmarks of zero-shot \nprediction tasks without task-specific training or fine-tuning. By train-\ning EchoCLIP with data from one healthcare system and testing its \nperformance on data from an entirely separate external healthcare \nsystem, we were able to evaluate EchoCLIP’s generalizability and robust-\nness to domain shift. Additionally, EchoCLIP-R displays an ability to \nperform tasks that human clinicians struggle with or find laborious, \nsuch as identifying the same patient across different imaging studies \nand characterizing clinically important changes over time. Finally, \nwe introduce a multimodal interpretability approach using cosine \nsimilarity-based saliency to demonstrate that EchoCLIP has learned \nsemantically meaningful imaging features of both common and rare \ncardiovascular concepts based on text prompting.\nA key bottleneck in training medical foundation models is the lim-\nited availability of medical training data. Previous echocardiography \nAI models were trained with a maximum of 150,000 echocardiogram \nvideos29 and most frequently trained with only hundreds or thousands \nof examples7,10,30–32. By leveraging large clinical reporting databases, \nour approach minimizes the tedious manual labeling and organiza -\ntion required for supervised learning tasks and allows EchoCLIP to be \ntrained on over 1 million echocardiography videos. In its most basic \nform, the task of mapping images to corresponding text interpreta -\ntions is the clinical task of medical image interpretation that cardiolo-\ngists do daily. EchoCLIP represents an opportunity to automate many \n−200 −150 −100 −50 0 50 100 150 200\nDays from surgery\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity to earliest study within 200 days\n−200 −150 −100 −50 0 50 100 150 200\nDays from first report mentioning transplant\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity to earliest study within 200 days\nDiﬀerent\npatients\nSame\npatient\nSame\nstudy\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine similarity\n0 0.2 0.4 0.6 0.8 1.0\nFPR\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\nAUC = 0.86\nCosine similarity between videos Before and after cardiac surgery\nUnique patient identification Before and after orthotopic heart transplant\na c\nb d\nFig. 3 | Assessment of clinical similarity. a, Average cosine similarity between \nembeddings from different patients, same patients at different times and same \npatients at the same time point. Center lines indicate the median, boxes span \nfrom the first to the third quartile and whiskers stretch 1.5 × the interquartile \nrange (n = 100,994). b, AUC for predicting whether the images come from the \nsame patient when compared to another image (n = 100,994). c,d, Trajectory \nof individual patients by cosine similarity (n = 2,959). Each line represents an \nindividual patient with time from major clinical event on the x axis and cosine \nsimilarity versus first study on the y axis. Patients either had major cardiac \nsurgery (c) or heart transplant (d), with cosine similarity calculated at the study \nlevel and pairwise compared for all videos in each study. Data are from the \nCedars-Sinai Medical Center. FPR, false positive rate; TPR, true positive rate.\nNature Medicine | Volume 30 | May 2024 | 1481–1488 1486\nArticle https://doi.org/10.1038/s41591-024-02959-y\ninterpretation tasks simultaneously without the need for individually \ntuned specialist models, which can ultimately lead to automated pre-\nliminary echocardiography interpretation in underserved populations \nor during emergent situations. A self-supervised vision–language \nfoundation model trained on the diverse range of physiologies seen \nin a high-volume echocardiography laboratory can learn from greater \namounts of data than purely supervised models and may be able to gain \na much more generally applicable understanding of the human heart, \nits function and its structure.\nWhile EchoCLIP is not the first instance of a foundation model \ntrained on biomedical datasets17,33,34, EchoCLIP is a model specific for \nechocardiography, the most common modality for cardiovascular \nimaging and not represented in prior foundation model training. While \nechocardiography still needs to be interpreted by expert cardiologists, \ngiven the rapid expansion of availability of ultrasound technology and \nthe development of complementary technologies to allow novices to \nperform cardiac ultrasound35, models such as EchoCLIP have the poten-\ntial to improve access to cardiac imaging and image interpretation. One \nof the most time-consuming and challenging assessments is distin -\nguishing between natural variation versus change in the disease state \nthat might warrant changes in the treatment plan. Such evaluations \noften require meticulously comparing current and historical imaging \nside by side and can be highly variable across different cardiologists. \nBy using EchoCLIP to directly compare studies, clinicians can derive a \n...Resting segmental wall \nmotion analysis. Right \nventricle - moderate dilated \nright ventricle. Severe \ndepressed right ventricular \nsystolic function. Echo \ndensity in right ventricle \nsuggestive of catheter, pacer\nlead, or ICD lead. IVC - the \ninferior vena cava is dilated...\nImage \nencoder\na\nd\nb\nc\nText embeddings\nRank by cosine similarity\nTAVR\nPacemaker\nImpella\nMitraClip\nFig. 4 | Image-to-text semantic search. a, The query image is first embedded \nusing EchoCLIP-R’s image encoder. b, Then, the similarities between this  \nquery embedding and the embeddings of all 21,484 unique text reports in the  \ntest set are computed. c, The reports are ranked by their similarity to the query \nimage embedding and the report with the highest similarity is retrieved.  \nd, Corresponding pairs of input frames and PromptCAM visualization of the \nindicated intracardiac devices in the text report label (color intensity ranging from \nred for most important to green for less important and no color for not important).\nNature Medicine | Volume 30 | May 2024 | 1481–1488\n 1487\nArticle https://doi.org/10.1038/s41591-024-02959-y\nquantitative visual assessment of differences. Such an automated AI \nassessment can alert clinicians’ attention toward specific studies to \nmore carefully evaluate clinical changes.\nWhile specialist models still perform better on specific, narrowly \ndefined tasks29, the performance of EchoCLIP on external validation \ndata confirms its ability to assess cardiac function with accuracy similar \nto blinded human performance as well as many previously developed \nsupervised learning models6,7,36–38. EchoCLIP achieves an MAE of 7.1% \non external validation of L VEF prediction, while previous video-based \nL VEF AI models achieve an MAE of 6.0% and image-based AI models \nachieve an MAE of just 9.9%28. Differences in EchoCLIP’s performance \non internal and external test datasets are likely due to differences in \nthe L VEF evaluation technique across institutions (Supplementary \nFig. 4). While statistically significant in large datasets, the error in \nmodel predictions across L VEF values or measurement approaches is \nless than clinical variability29, suggesting that different training data \nwith a different L VEF measurement approach would have a modest dif-\nferential effect. The distribution of L VEF values from model inference is \ncontinuous without preference for certain measurements, suggesting \nthat human biases are smoothed out in the model embedding space \n(Supplementary Fig. 5).\nImportant limitations of this work include the use of an image \nencoder instead of a video encoder when echocardiography videos \ncontain important motion-based information and the use of only \nthe apical-four-chamber view, which, although is the most common \nand informative standard view, does not capture information with \nregard to Doppler velocities and structures only present in other \nviews. In this work, as well as previous work8,9, it is clear that there are \nimage-based features that can be a partial surrogate for information \nnot directly interrogatable without video or from different views. \nFor example, sphericity and dilation of the left ventricle can be iden -\ntifiable from images alone and suggest decreased cardiac function \nalthough true assessment of L VEF requires video information. Valve \ncalcification can hint at stenosis or coronary artery disease\n8,9,39 that \nis not directly present in the image. Future work will incorporate \nvideo encoders and different measurement techniques and will lev -\nerage multiple views from the same echocardiographic study to \nprovide more holistic AI models for heart health. Enhancements \nsuch as upgrading EchoCLIP’s visual encoder from an image-based \nmodel to a video-based model, adapting EchoCLIP for visual question \nanswering, and implementation of automatic report generation are \npotential directions for future research. Finally, important open ques-\ntions remain in the testing of foundation models before regulatory \napproval and eventual clinical use.\nOur results encourage further exploration of vision–language \nfoundation models for cardiology and medicine generally. Clinical \ndatabases provide large bodies of information about health, while \ndifferent imaging modalities provide adjunctive ancillary informa -\ntion that might improve our understanding of cardiovascular health. \nFurther efforts remain to leverage larger datasets and more versatile \nmodel architectures to better capture and distill medical information.\nOnline content\nAny methods, additional references, Nature Portfolio reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author contri-\nbutions and competing interests; and statements of data and code avail-\nability are available at https://doi.org/10.1038/s41591-024-02959-y.\nReferences\n1. Heidenreich, P. A. et al. 2022 AHA/ACC/HFSA guideline for the \nmanagement of heart failure: executive summary: a report of the \nAmerican College of Cardiology/American Heart Association \nJoint Committee on Clinical Practice Guidelines. Circulation 145, \ne876–e894 (2022).\n2. Al-Khatib, S. M. et al. 2017 AHA/ACC/HRS guideline for \nmanagement of patients with ventricular arrhythmias and the \nprevention of sudden cardiac death: executive summary: a \nreport of the American College of Cardiology/American Heart \nAssociation Task Force on Clinical Practice Guidelines and the \nHeart Rhythm Society. Circulation 138, e210–e271 (2018).\n3. Wilcox, J. E., Fang, J. C., Margulies, K. B. & Mann, D. L. Heart failure \nwith recovered left ventricular ejection fraction: JACC Scientific \nExpert Panel. J. Am. Coll. Cardiol. 76, 719–734 (2020).\n4. Dunlay, S. M., Roger, V. L. & Redfield, M. M. Epidemiology of heart \nfailure with preserved ejection fraction. Nat. Rev. Cardiol. 14, \n591–602 (2017).\n5. Ouyang, D. et al. Video-based AI for beat-to-beat assessment of \ncardiac function. Nature 580, 252–256 (2020).\n6. Zhang, J. et al. Fully automated echocardiogram interpretation in \nclinical practice. Circulation 138, 1623–1635 (2018).\n7. Tromp, J. et al. Automated interpretation of systolic and diastolic \nfunction on the echocardiogram: a multicohort study. Lancet \nDigit. Health 4, e46–e54 (2022).\n8. Holste, G. et al. Severe aortic stenosis detection by deep learning \napplied to echocardiography. Eur. Heart J. 44, 4592–4604 (2023).\n9. Ghorbani, A. et al. Deep learning interpretation of \nechocardiograms. NPJ Digit. Med. 3, 10 (2020).\n10. Duffy, G. et al. High-throughput precision phenotyping of left \nventricular hypertrophy with cardiovascular deep learning.  \nJAMA Cardiol. 7, 386–395 (2022).\n11. Bommasani, R. et al. On the opportunities and risks of foundation \nmodels. Preprint at https://arxiv.org/abs/2108.07258 (2021).\n12. Radford, A. et al. Learning transferable visual models from natural \nlanguage supervision. in Proc. 38th International Conference on \nMachine Learning Vol. 139 (PMLR, 2021).\n13. Desai, K. & Johnson, J. VirTex: learning visual representations from \ntextual annotations. in 2021 IEEE/CVF Conference on Computer \nVision and Pattern Recognition (CVPR) (IEEE, 2021).\n14. Larochelle, H., Erhan, D. & Bengio, Y. Zero-data learning of new \ntasks. in Proc. 23rd AAAI Conference on Artificial Intelligence \n(AAAI, 2008).\n15. Geirhos, R. et al. ImageNet-trained CNNs are biased towards \ntexture; increasing shape bias improves accuracy and robustness. \nPreprint at https://arxiv.org/abs/1811.12231 (2018).\n16. Eslami, S., de Melo, G. & Meinel, C. Does CLIP benefit visual \nQuestion answering in the medical domain as much as it does in \nthe general domain? Preprint at https://arxiv.org/abs/2112.13906 \n(2021).\n17. Singhal, K. et al. Large language models encode clinical \nknowledge. Nature 620, 172–180 (2023).\n18. Ji, S. et al. Domain-specific continued pretraining of language \nmodels for capturing long context in mental health. Preprint at \nhttps://arxiv.org/abs/2304.10447 (2023).\n19. Thawkar, O. et al. XrayGPT: chest radiographs summarization \nusing medical vision-language models. Preprint at https://arxiv.\norg/abs/2306.07971 (2023).\n20. Iyer, N. S. et al. Self-supervised pretraining enables high- \nperformance chest X-ray interpretation across clinical \ndistributions. Preprint at medRxiv https://doi.org/10.1101/2022. \n11.19.22282519 (2022).\n21. Liu, Z. et al. Radiology-GPT: a large language model for radiology. \nPreprint at https://arxiv.org/abs/2306.08666 (2023).\n22. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. & Zou, J.  \nA visual-language foundation model for pathology image analysis \nusing medical Twitter. Nat. Med. 29, 2307–2316 (2023).\n23. Lu, M. Y. et al. A visual-language foundation model for \ncomputational pathology. Nat. Med. 30, 863–874 (2024).\n24. Zhou, Y. et al. A foundation model for generalizable disease \ndetection from retinal images. Nature 622, 156–163 (2023).\nNature Medicine | Volume 30 | May 2024 | 1481–1488 1488\nArticle https://doi.org/10.1038/s41591-024-02959-y\n25. Abbaspourazad, S. et al. Large-scale training of foundation \nmodels for wearable biosignals. Preprint at https://arxiv.org/\nabs/2312.05409 (2023).\n26. Liu, Z. et al. A ConvNet for the 2020s. in 2022 IEEE/CVF \nConference on Computer Vision and Pattern Recognition (CVPR) \n(IEEE, 2022).\n27. Sennrich, R., Haddow, B. & Birch, A. Neural machine translation of \nrare words with subword units. in Proc. 54th Annual Meeting of the \nAssociation for Computational Linguistics (Volume 1: Long Papers) \n(Association for Computational Linguistics, 2016).\n28. Cherti, M. et al. Reproducible scaling laws for contrastive \nlanguage-image learning. Preprint at https://arxiv.org/abs/ \n2212.07143 (2022).\n29. He, B. et al. Blinded, randomized trial of sonographer versus AI \ncardiac function assessment. Nature 616, 520–524 (2023).\n30. Lau, E. S. et al. Deep learning-enabled assessment of left heart \nstructure and function predicts cardiovascular outcomes. J. Am. \nColl. Cardiol. 82, 1936–1948 (2023).\n31. Akerman, A. P. et al. Automated echocardiographic detection \nof heart failure with preserved ejection fraction using artificial \nintelligence. JACC Adv. 2, 100452 (2023).\n32. Madani, A., Arnaout, R., Mofrad, M. & Arnaout, R. Fast and \naccurate view classification of echocardiograms using deep \nlearning. NPJ Digit. Med. 1, 6 (2018).\n33. Lee, J. et al. BioBERT: a pre-trained biomedical language \nrepresentation model for biomedical text mining. Bioinformatics \n36, 1234–1240 (2020).\n34. Rasmy, L., Xiang, Y., Xie, Z., Tao, C. & Zhi, D. Med-BERT: pretrained \ncontextualized embeddings on large-scale structured electronic \nhealth records for disease prediction. NPJ Digit. Med. 4, 86 (2021).\n35. Narang, A. et al. Utility of a deep-learning algorithm to guide \nnovices to acquire echocardiograms for limited diagnostic use. \nJAMA Cardiol. 6, 624–632 (2021).\n36. Farsalinos, K. E. et al. Head-to-head comparison of global \nlongitudinal strain measurements among nine different vendors: \nthe EACVI/ASE inter-vendor comparison study. J. Am. Soc. \nEchocardiogr. 28, 1171–1181 (2015).\n37. Yuan, N. et al. Systematic quantification of sources of variation  \nin ejection fraction calculation using deep learning.  \nJACC Cardiovasc. Imaging 14, 2260–2262 (2021).\n38. Cole, G. D. et al. Defining the real-world reproducibility of visual \ngrading of left ventricular function and visual estimation of left \nventricular ejection fraction: impact of image quality, experience \nand accreditation. Int. J. Cardiovasc. Imaging 31, 1303–1314 (2015).\n39. Yuan, N. et al. Prediction of coronary artery calcium using  \ndeep learning of echocardiograms. J. Am. Soc. Echocardiogr. \nhttps://doi.org/10.1016/j.echo.2022.12.014 (2022).\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2024\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-02959-y\nMethods\nData curation\nThe Cedars-Sinai Medical Center echocardiography laboratory per -\nforms clinical echocardiography for a wide range of indications, rang-\ning from asymptomatic preoperative screening to evaluation for open \nheart surgery or heart transplant. Over the course of a standard, full, \nresting echocardiogram study, 50–150 videos and images are acquired \nthat visualize the heart from different angles, locations and with differ-\nent imaging modes (two-dimensional images, tissue Doppler images \nand color Doppler images). Each echocardiogram study corresponds \nto a unique patient and a unique visit, but multiple similar videos may \nbe obtained from each view acquired during the study. For EchoCLIP, \nwe focused on the apical-four-chamber view (one of the most com -\nmon and well-acquired ultrasound views) and organized a dataset of \n1,032,975 unique video–caption pairs from 224,685 echocardiogram \nstudies across 99,870 patients, collected between 2011 and 2022. Our \nlaboratory developed high-throughput tools to query echocardiogram \nvideos and their metadata from Cedars-Sinai’s internal databases at \nscale, view and classify videos and link them to associated structured \nreporting from cardiologists. DICOM images were queried from a \nHyland vendor-neutral archive, linked to interpretations created by \ntrained cardiologists using Syngo Dynamics and converted to AVI video \nfiles using PyDICOM before model training and inference.\nData were split by patient into training, validation and internal \ntest datasets. The training data contained 921,981 videos from 84,990 \npatients, the validation set contained 10,000 videos from 5,358 patients \nand the internal test set contained 100,994 videos from 10,001 patients. \nA random subset (n = 5,000) of the publicly released EchoNet-Dynamic \ndataset from Stanford Healthcare was used as an external test set. \nAn automated preprocessing workflow was undertaken to remove \nextraneous text, ECG and respirometer information and other informa-\ntion outside of the scanning sector. The input data were represented \nas standardized 224 × 224-pixel RGB videos for model training. This \nresearch was approved by the Cedars-Sinai Medical Center (study \nno. 00001409) and Stanford Healthcare Institutional Review Boards  \n(study no. 43721). A waiver of consent was obtained for the use of  \nretrospective de-identified data.\nModel design and training\nModel design and training was conducted in Python using the PyT orch \ndeep-learning library. Our training code is a fork of the OpenCLIP \nrepository28. T o find the best training configuration, we evaluated a \nvariety of model architectures and training procedures. We tested train-\ning with random initialization, initializing the model with CLIP weights, \nusing a convolutional architecture for the image encoder, using a vision \ntransformer for the image encoder, applying random patch dropout to \nimage inputs and using three different text tokenization methods (Sup-\nplementary Table 5), with the final EchoCLIP model use the ConvNeXt \narchitecture26 for the image encoder and a decoder-only transformer \nfor the text encoder. We initialize our model with weights pretrained on \nLAION-400M. We trained for 50 epochs, minimizing the original CLIP \nloss. The CLIP loss incentivizes the video and text encoders to make the \nembeddings of paired videos and reports as similar as possible, while \nmaking the embeddings of unpaired videos and reports as different as \npossible (Fig. 1a). This training objective is, notably, all that is required \nto make the two models learn to encode their inputs into semantically \nmeaningful vector embeddings.\nWe warmed up to an initial learning rate of 5 × 10−5 over the course \nof the first 2,000 training steps and then cosine decayed to zero over \nthe course of the training run. We used a batch size of 1,024 and trained \non two Nvidia RTX A6000 48 GB GPUs for approximately 2 weeks. Dur-\ning training, a random frame was extracted from each video and passed \nto the image encoder. A random frame from each video was used for \neach epoch as a form of data augmentation. Model checkpoints were \nsaved after every epoch. At the end of training, the model checkpoint \nwith the lowest mean cross-modal retrieval rank on the validation \nset was selected for testing. Before computing the cosine similarity \nbetween vector embeddings, we always divide them by their norms to \nensure that they have the same magnitude. This means that the cosine \nsimilarity metric always returns a value between −1 and 1.\nText tokenization\nA number of text tokenization schemes were tested (Supplementary \nTable 1). EchoCLIP was trained using text tokenized by a BPE tokenizer27 \npretrained on the GPT2 data corpus, which encoded echocardiography \nreports with a mean of 530.3 (±154.7) tokens per report. Due to the \ncontext length limit of 77 tokens imposed by fine-tuning from CLIP \nweights, EchoCLIP was trained on snippets of reports rather than their \nfull text. For EchoCLIP-R, we noted that the echocardiography report \ntext is often highly structured and repetitive, as they are typically \ngenerated in a ‘fill-in-the-blank’ fashion according to a predetermined \ntemplate given to the cardiologist at the time of interpretation. The \ntemplated nature of the reports means that a small number of unique \nphrases and sentences appear very frequently in the final report text \nwith only slight variations. A custom-built tokenizer was designed \nto take advantage of this observation and allowed us to aggressively \ncompress the report text. This meant that whole reports could be \ninputted when training EchoCLIP-R, improving its retrieval capabilities \ncompared to EchoCLIP at the cost of slight degradation in classification \nand estimation capabilities.\nInstead of searching for exact vocabulary matches in the report \ntext, our custom-built template tokenizer uses regular expressions to \nallow nearly similar lines of text to be efficiently encoded. For example, \nthe text ‘Moderate left ventricular hypertrophy. Left ventricular ejec-\ntion fraction is 60%’ is converted into tokens indicating either cardiac \nstructure or function (such as ‘<_ left ventricular hypertrophy>’ , ‘<left \nventricular ejection fraction is _%>’) as well as indicating severity (‘mild’ , \n‘moderate’ or ‘severe’) or quantity (60%, 2.5 cm, 40 cm s −1). By doing \nthis, we were able to capture most of the variance present in our text \nreports with a vocabulary containing only 770 words and phrases, in \naddition to extra tokens for handling numbers and severity terms. \nAfter applying this custom tokenizer, the mean length of a tokenized \nreport was brought down to just 63.8 (±26.7) tokens, an approximate \nninefold reduction compared to using CLIP’s original BPE tokenizer. \nWe additionally tested a model that used a BPE tokenizer pretrained \non echocardiography reports but found that it failed to outperform \nthe model trained using our custom solution.\nUsing EchoCLIP-R embeddings, we can perform a search within \nour test set to find images or reports that are semantically similar to \na given query image or report. T o do this, we simply sort the embed-\ndings of all candidate images or reports by their cosine similarity to \nthe embedding of a query image or report. The embedding space \nwas normalized to unit vectors before calculation of cosine similarity \nto be insensitive to projection magnitude. If the model and dataset \nwere theoretically perfect, we would expect the image or report that \nis officially paired with the query image or report to be ranked first \nin the list. We report the mean rank number as a metric of accuracy. \nThis allows us to characterize EchoCLIP-R’s retrieval abilities in two \nsettings: image-to-report and report-to-image. We choose a single \nrandom video from each study to represent the whole study in these \nranking tests to simplify the implementation. T o obtain a single value \nthat represents a model’s overall retrieval ability, we define the MCMRR \nas the average of both the mean image-to-report retrieval rank and the \nmean report-to-image retrieval rank. MCMRR values for both EchoCLIP \nand EchoCLIP-R are shown in Table 2.\nT o evaluate the model’s ability to identify unique patients, we \ncomputed the similarity between many random pairs of EchoCLIP-R’s \nimage embeddings and then treated those similarity values as if they \nwere continuous probability predictions meant to classify whether \nboth images in the pair came from the same patient. T o visualize patient \nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-02959-y\ntrajectories before or after heart transplantation or cardiac surgery, we \nfirst collected all the echocardiogram images within 200 days before or \nafter the procedure date. These images were grouped by study and then \nembeddings were produced for each video using EchoCLIP-R. The earli-\nest study within the 200-day window was taken as a baseline and then \neach following study within the window was assigned a similarity score \ncomputed by taking the average similarity between all possible pairs \nof videos from the baseline study and the study in question. This was \nrepeated for all patients who had undergone heart transplantation or \ncardiac surgery in our test set who also had at least one echocardiogra-\nphy study performed before and after the date of the procedure. These \nstudy-level ‘similarity timelines’ were then plotted together, resampled \nand averaged to create Fig. 3c,d. These study-level similarity scores can \nalso treated as continuous probability predictions for whether a given \nstudy was acquired before or after the procedure date, allowing us to \ncalculate an AUC score that quantifies EchoCLIP-R’s ability to detect \nthe effects of such procedures. Multiple surgical characteristics and \napproaches were analyzed by subset analysis with similar results (Sup-\nplementary Figs. 6 and 7).\nAdapting EchoCLIP to classification and regression tasks\nDespite only training to encode images and report text as semanti-\ncally meaningful vector embeddings, EchoCLIP was adapted to per -\nform both classification and regression tasks. For each classification \ntask, we followed the approach of the original CLIP paper and con-\nstructed text prompts describing a positive case. Then, we obtained \nan embedding of those prompts using EchoCLIP’s text encoder and \ncomputed the cosine similarity between them and the embeddings \nof the videos in our test set. In the case of multiple semantically \nequivalent prompts being used for a binary classification task, we \naverage the similarity across all prompts and then averaged again \nover the first 20 frames of the video (at temporal stride 2). We treat \nthis final average similarity score as a continuous probability pre -\ndiction. Hyperparameters of number of frames sampled per video, \nstride (frame count between sampled frames) and number of aver-\naged embeddings were evaluated to optimize model performance \n(Supplementary Tables 4 and 5).\nFor regression tasks, we generated a collection of variations on \na base text prompt by only changing the relevant value in the text \n(Supplementary Fig. 8). For instance, variants of the prompt ‘The \nleft ventricular ejection fraction is estimated to be X%’ or ‘L V ejection \nfraction is X%’ were generated for all integer values between 0 and \n100. These variations on the base prompt are then embedded using \nEchoCLIP’s text encoder. The cosine similarity between these prompt \nembeddings and the embeddings of each of the first 20 frames of all \ntest-set videos (extracted with temporal stride of 2) is computed. The \ncandidate values are then ranked for each frame according to their \ncorresponding prompt embeddings’ similarity to the frame embed -\ndings and the bottom 80% of the values are discarded. The remaining \n20% of the values are averaged along the frames dimension, leaving \n20 potential prediction values ordered from most likely (on average \nacross all frames) to least likely. We found, empirically, that taking  \nthe median of these 20 values results in the most accurate predictions. \nThis process is illustrated in Extended Data Fig. 1.\nFor EchoCLIP, a systematic search through relevant phrases pre-\nsent in the echocardiography report template file was conducted to \nmanually construct the base prompts for each task. For EchoCLIP-R, \nwe noted that using this approach resulted in severely degraded \nperformance. We believe this to be the result of short, single-phrase \nprompts being out-of-distribution for EchoCLIP-R as it was trained \nexclusively using full-length reports. T o address this, we tested an \nalternate prompting strategy for EchoCLIP-R, where the base prompts \nare entire reports sampled from videos in the validation set that \nhave the desired labels. As an example of how this works for a regres-\nsion task, the base L VEF estimation prompts for EchoCLIP-R were \nchosen by randomly sampling up to ten reports from the validation \nset for each ground-truth L VEF value between 1 and 100. This way, \nEchoCLIP-R has in-distribution ‘example reports’ from the validation \nset to compare the query images against, instead of being forced \nto encode much shorter prompts that are nothing like what it saw \nduring training. For binary tasks, 200 reports containing a positive \nlabel for the task are sampled from the validation set and used as base \nprompts. We found that this ‘sampled prompts’ strategy substantially \nimproved EchoCLIP-R’s performance on classification and regression \ntasks (Table 2).\nAll text prompts used for the evaluation of EchoCLIP are published \nin the project’s code repository, a link to which is included in Supple-\nmentary Fig. 8. Ground-truth labels are extracted from the clinical \nreports and used to calculate AUC and other performance metrics.\nInterpretation techniques\nCode for saliency mapping with PromptCAM was written in Python \nwith dependencies on PyT orch and NumPy packages. Modifying \nthe optimization function of the integrated gradients method, \nPromptCAM maximizes the cosine similarity as the objective func -\ntion between image-based regions of interest with the text prompt. \nPrompts describing common cardiac structures were used to test \nwhether EchoCLIP ‘pays attention’ to relevant cardiac structures in \nechocardiogram images. UMAP was applied using the umap-learn \nPython package. EchoCLIP image embeddings for each video in the \ntest set were processed to demonstrate how clusters associated with \ndifferent cardiovascular diseases, disease states and measurements \nare present. The n_neighbors parameter was set to the maximum \nallowed value of 200 and the min_distance parameter was set to the \nmaximum allowed value of 1.0.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nThe dataset of videos and reports used to train EchoCLIP is not pub -\nlicly available due to its potentially identifiable nature; however, \nEchoNet-Dynamic, the dataset that we used for external validation, is \npublicly available at https://echonet.github.io/dynamic/.\nCode availability\nOur model weights, evaluation prompts, and demonstration code are \navailable on GitHub at https://github.com/echonet/echo_CLIP.\nAcknowledgements\nWe thank B. He and G. Duffy for thoughtful discussions and feedback. \nD.O. discloses National Institutes of Health NHLBI grants R00HL157421 \nand R01HL173526.\nAuthor contributions\nM.C. developed the model. M.C., M.V. and N.Y. performed the \nexperiments. M.C. and D.O. wrote the paper. All authors provided \ncritical feedback and review.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s41591-024-02959-y.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s41591-024-02959-y.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-02959-y\nCorrespondence and requests for materials should be addressed to \nDavid Ouyang.\nPeer review information Nature Medicine thanks Darrel Francis, \nMassoud Zolgharni and the other, anonymous, reviewer(s) for their \ncontribution to the peer review of this work. Primary Handling Editor: \nMichael Basson, in collaboration with the Nature Medicine team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-02959-y\nExtended Data Fig. 1 | Frame level ensembling. (a) Distribution of EchoCLIP left \nventricular ejection fraction (L VEF) from individual frames of an echocardiogram \nvideo, which are averaged to (b) a video-level distribution of L VEF prediction.  \n(c) Scatter-plot of subset of test dataset (n = 1,000 predictions from 100 videos \nand 10 frames per video) representing predicted vs. ground-truth L VEF . Each \npoint represents the final predicted values and whiskers represent the range of \nframe level predictions for that video.\nNature Medicine\nArticle https://doi.org/10.1038/s41591-024-02959-y\nExtended Data Table 1 | AUC, sensitivity, and specificity for EchoCLIP zero-shot prediction tasks\nArea under the receiver operator curve, sensitivity, and specificity for zero-shot classification tasks. Sensitivity and specificity calculated at the Youden’s index.\n\n\n"
}