{
  "title": "Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models",
  "url": "https://openalex.org/W3154222058",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2955207117",
      "name": "Tejas Srinivasan",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2296285037",
      "name": "Yonatan Bisk",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963115613",
    "https://openalex.org/W2969280433",
    "https://openalex.org/W2962787423",
    "https://openalex.org/W2076332646",
    "https://openalex.org/W2788481061",
    "https://openalex.org/W3017701505",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2975501350",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W4249013746",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2962685807",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W3093211917",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2768661419",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2926555354",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2030542035",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2483215953",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963349562",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2058189921",
    "https://openalex.org/W2783160784",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2622683517",
    "https://openalex.org/W4288007632",
    "https://openalex.org/W4289421997",
    "https://openalex.org/W1933349210"
  ],
  "abstract": "Numerous works have analyzed biases in vision and pre-trained language models individually - however, less attention has been paid to how these biases interact in multimodal settings. This work extends text-based bias analysis methods to investigate multimodal language models, and analyzes intra- and inter-modality associations and biases learned by these models. Specifically, we demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often preferring to reinforce a stereotype over faithfully describing the visual scene. We demonstrate these findings on a controlled case-study and extend them for a larger set of stereotypically gendered entities.",
  "full_text": "Proceedings of the The 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 77 - 85\nJuly 15, 2022 ©2022 Association for Computational Linguistics\nWorst of Both Worlds:\nBiases Compound in Pre-trained Vision-and-Language Models\nTejas Srinivasan\nUniversity of Southern California\ntejas.srinivasan@usc.edu\nYonatan Bisk\nCarnegie Mellon University\nybisk@cs.cmu.edu\nAbstract\nNumerous works have analyzed biases in vi-\nsion and pre-trained language models individ-\nually - however, less attention has been paid\nto how these biases interact in multimodal set-\ntings. This work extends text-based bias anal-\nysis methods to investigate multimodal lan-\nguage models, and analyzes intra- and inter-\nmodality associations and biases learned by\nthese models. Specifically, we demonstrate\nthat VL-BERT (Su et al., 2020) exhibits gen-\nder biases, often preferring to reinforce a\nstereotype over faithfully describing the visual\nscene. We demonstrate these findings on a\ncontrolled case-study and extend them for a\nlarger set of stereotypically gendered entities.\n1 Introduction\nPre-trained contextualized word representa-\ntions (Peters et al., 2018; Devlin et al., 2019;\nRadford et al., 2018; Lan et al., 2020; Raffel et al.,\n2020) have been known to amplify unwanted (e.g.\nstereotypical) correlations from their training\ndata (Zhao et al., 2019; Kurita et al., 2019;\nWebster et al., 2020; Vig et al., 2020). By learning\nthese correlations from the data, models may\nperpetuate harmful racial and gender stereotypes.\nThe success and generality of pre-trained Trans-\nformers has led to several multimodal representa-\ntion models (Su et al., 2020; Tan and Bansal, 2019;\nChen et al., 2019) which utilize visual-linguistic\npre-training. These models also condition on\nthe visual modality, and have shown strong per-\nformance on downstream visual-linguistic tasks.\nThis additional input modality allows the model\nto learn both intra- and inter-modality associa-\ntions from the training data - and in turn, gives\nrise to unexplored new sources of knowledge and\nbias. For instance, we find (see Figure 1) the word\npurse’s female association can override the visual\nevidence. While there are entire bodies of work\nsurrounding bias in vision (Buolamwini and Ge-\nbru, 2018) and language (Blodgett et al., 2020),\nVL-BERT\nthe person is carrying a [MASK]\nVL-BERT\nthe person is carrying a [MASK]\nFigure 1: Visual-linguistic models (like VL-BERT) en-\ncode gender biases, which (as is the case above) may\nlead the model to ignore the visual signal in favor of\ngendered stereotypes.\nthere are relatively few works at the intersection\nof the two. As we build models that include mul-\ntiple input modalities, each containing their own\nbiases and artefacts, we must be cognizant about\nhow each of them are influencing model decisions.\nIn this work, we extend existing work for mea-\nsuring gender biases in text-only language mod-\nels to the multimodal setting. Specifically, we\nstudy how within- and cross-modality biases are\nexpressed for stereotypically gendered entities in\nVL-BERT (Su et al., 2020), a popular visual-\nlinguistic transformer. Through a controlled case\nstudy (§4), we find that visual-linguistic pre-\ntraining leads to VL-BERT viewing the majority\nof entities as “more masculine” than BERT (De-\nvlin et al., 2019) does. Additionally, we observe\nthat model predictions rely heavily on the gender\nof the agent in both the language and visual con-\ntexts. These findings are corroborated by an anal-\nysis over a larger set of gendered entities (§5).\n2 Bias Statement\nWe define gender bias as undesirable variations in\nhow the model associates an entity with different\ngenders, particularly when they reinforce harm-\n77\nTo compute P(E|g) To compute P(E|gN )\nSource X Visual Input Language Input Modified\nComponent New Value Association\nScore S(E, g)\nVisual-Linguistic\nPre-training ✗ The man is drinking beer Model Text-only LM ln PV L(E|g)\nPL(E|g)\nLanguage Context\n The man is drinking beer Language\nInput man − →person ln PV L(E|g,I)\nPV L(E|p,I)\nVisual Context\n The person is drinking beer Visual\nInput ✗ ln\nˆPV L(E|Ig)\nPV L(E)\nTable 1: Our methodology being used to compute association scores S(E, g) between beer ( E) and man ( g) in\neach of the three bias sources. We show the inputs used to compute P(E|g), and the modifications made for the\nnormalizing term, P(E|gN). The entity beer is [MASK]-ed before being passed into the model.\nful stereotypes. 1 Relying on stereotypical cues\n(learned from biased pre-training data) can cause\nthe model to override visual and linguistic evi-\ndence when making predictions. This can result\nin representational harms (Blodgett et al., 2020)\nby perpetuating negative gender stereotypes - e.g.\nmen are not likely to hold purses (Figure 1), or\nwomen are more likely to wear aprons than suits.\nIn this work, we seek to answer two questions: a)\nto what extent does visual-linguistic pre-training\nshift the model’s association of entities with differ-\nent genders? b) do gendered cues in the visual and\nlinguistic inputs 2 influence model predictions?\n3 Methodology\n3.1 Sources of Gender Bias\nWe identify three sources of learned bias when\nvisual-linguistic models are making masked word\npredictions - visual-linguistic pre-training, the\nvisual context, and the language context. The\nformer refers to biases learned from image-text\npairs during pre-training, whereas the latter two\nare biases expressed during inference.\n3.2 Measuring Gender Bias\nWe measure associations between entities and\ngender in visual-linguistic models using template-\nbased masked language modeling, inspired by\nmethodology from Kurita et al. (2019). We pro-\nvide template captions involving the entity E as\nlanguage inputs to the model, and extract the prob-\nability of the [MASK]-ed entity. We denote ex-\n1In this work, we use “male” and “female” to refer to his-\ntorical definitions of gender presentation. We welcome rec-\nommendations on how to generalize our analysis to a more\nvalid characterization of gender and expression.\n2We note that this work studies biases expressed by mod-\nels for English language inputs.\ntracted probabilities as:\nPL/V L(E|g) =P([MASK] = E|g in input)\nwhere g is a gendered agent in one of the in-\nput modalities. L and V L are the text-only\nBERT (Devlin et al., 2019) and VL-BERT (Su\net al., 2020) models respectively. Our method for\ncomputing association scores is simply:\nS(E, g) = lnP(E|g)\nP(E|gN )\nwhere the probability terms vary depending on the\nbias source we want to analyze. We generate the\nnormalizing term by replacing the gendered agent\ng with a gender-neutral term gN . We summarize\nhow we vary our normalizing term and compute\nassociation scores for each bias source in Table 1.\n1. Visual-Linguistic Pre-Training (SPT ): We\ncompute the association shift due to VL pre-\ntraining, by comparing the extracted proba-\nbility PV Lfrom VL-BERT with the text-only\nBERT - thus PL is the normalizing term.\n2. Language Context (SL): For an imageI, we\nreplace the gendered agentg with the gender-\nneutral term person (p) in the caption, and\ncompute the average association score over a\nset of images IE which contain the entity E.\nSL(E, g) =EI∼IE\n[\nSL(E, g|I)\n]\n3. Visual Context (SV ): We collect a set of im-\nages Ig which contain the entity E and gen-\ndered agent g, and compute the average ex-\ntracted probability by providing language in-\nput with gender-neutral agent:\nˆPV L(E|Ig) =EI∼Ig [PV L(E|I)]\n78\nTemplate Caption Entities\nThe [AGENT] is carrying a E . purse briefcase\nThe [AGENT] is wearing a E . apron suit\nThe [AGENT] is drinking E . wine beer\nTable 2: Template captions for each entity pair. The\n[AGENT] is either man, woman,or person .\nWe normalize by comparing to the output\nwhen no image is provided (PV L(E)).\nFor each bias source, we can compute the bias\nscore for that entity by taking the difference of its\nfemale and male association scores:\nB(E) =S(E, f) −S(E, m)\nThe sign of B(E) indicates the direction of gender\nbias - positive for “female,” negative for “male.”\n4 Case Study\nIn this section, we present a case study of our\nmethodology by examining how gender bias is ex-\npressed in each bias source for several entities.\nThe case study serves as an initial demonstration\nof our methodology over a small set of gendered\nentities, whose findings we expand upon in Sec-\ntion 5.\n4.1 Entities\nWe perform an in-depth analysis of three pairs of\nentities, each representing a different type of en-\ntity: clothes (apron, suit), bags (briefcase, purse),\nand drinks (wine, beer). The entities are selected\nto show how unequal gender associations perpetu-\nate undesirable gender stereotypes - e.g. aprons\nare for women, while suits are for men (Ap-\npendix B).\nFor each entity, we collect a balanced set IE =\nIf ∪Im of 12 images - 6 images each with men\n(Im) and women ( If ) (images in Appendix A). 3\nWe also create a different template caption for\neach entity pair (Table 2), which are used to com-\npute association scores S(E, m/f) when the gen-\ndered agent g in the caption is man or woman.\nIn the following sections, we analyze how VL-\nBERT exhibits gender bias for these entities, for\neach of the bias sources identified in Section 3.1.\n3Note, throughout our discussion we use the words man\nand woman as input to the model to denote male and female\nto the model. However, when images are included, we only\nuse images of self-identified (fe)male presenting individuals.\nFigure 2: Pre-training association shift scores\nSPT (E, m/f). Positive shift scores indicate that VL-\nBERT has higher associations between the entity and\nthe agent’s gender than BERT, and vice versa\n4.2 Visual-Linguistic Pre-Training Bias\nFigure 2 plots each entity’s pre-training associ-\nation shift score, SPT (E, m/f), where positive\nscores indicate that visual-linguistic pre-training\namplified the gender association, and vice versa.\nVisual-linguistic pre-training affects all objects\ndifferently. Some objects have increased associa-\ntion scores for both genders (briefcase), while oth-\ners have decreased associations ( suit and apron).\nHowever, even when the associations shift in the\nsame direction for both genders, they rarely move\ntogether - for briefcase, the association increase is\nmuch larger for male, whereas forapron, wineand\nbeer, the association decrease is more pronounced\nfor female. For purse, the association shifts posi-\ntively for male but negatively for female. For the\nentities in the case study, we conclude that pre-\ntraining shifts entities’ association towards men.\nFigure 3: Language association scores SL(E, m/f).\nPositive association scores indicate that the agent’s\ngender increases the model’s confidence in the entity.\n79\nFigure 4: Visual association scoresSV (E, m/f). Posi-\ntive association scores indicate that the model becomes\nmore confident in the presence of a visual context.\n4.3 Language Context Bias\nFigure 3 plots language association scores, which\nlook at the masked probability of E when the\nagent in the caption is man/woman, compared to\nthe gender-neutral person.\nFor the entity purse, we see that when the agent\nin the language context is female the model is\nmuch more likely to predict that the masked word\nis purse, but when the agent is male the proba-\nbility becomes much lower. We similarly observe\nthat some of the entities show considerably higher\nconfidence when the agent is either male or female\n(briefcase, apron, beer), indicating that the model\nhas a language gender bias for these entities. For\nsuit and wine, association scores with both genders\nare similar.\n4.4 Visual Context Bias\nFor each of our entities, we also plot the visual\nassociation score SV (E, u) with male and female\nin Figure 4. We again observe that the degree of\nassociation varies depending on whether the image\ncontains a man or woman. For purse and apron,\nthe model becomes considerably more confident\nin its belief of the correct entity when the agent is\nfemale rather than male. Similarly, if the agent is\nmale, the model becomes more confident about the\nentity in the case of briefcase and beer. For suit\nand wine, the differences are not as pronounced. In\nTable 3, we can see some examples of the model’s\nprobability outputs not aligning with the object in\nthe image. In both cases, the model’s gender bias\noverrides the visual evidence (the entity).\nVisual Context, I\nPV L(purse|I) 0 .0018 ✓ 0.084 ✗\nPV L(briefcase|I) 0 .4944 ✗ 0.067 ✓\nTable 3: Examples of images where the probability out-\nputs do not align with the visual information.\n5 Comparing Model Bias with Human\nAnnotations of Stereotypes\nTo test if the trends in the case study match hu-\nman intuitions, we curate a list of 40 entities,\nwhich are considered to be stereotypically mas-\nculine or feminine in society. 4 We analyze how\nthe gendered-ness of these entities is mirrored in\ntheir VL-BERT language bias scores. To evaluate\nthe effect of multimodal training on the underlying\nlanguage model, we remove the visual input when\nextracting language model probabilities and com-\npare how the language bias varies between text-\nonly VL-BERT and the text-only BERT model.\nFor the language input, we create template cap-\ntions similar to those described in Table 2. For ev-\nery entity E, we compute the language bias score\nBL(E) by extracting probabilities from the visual-\nlinguistic model, PV L(E|f/m/p).\nSL(E, m/f) = lnPV L(E|m/f)\nPV L(E|p)\nBV LBert\nL (E) =SL(E, f) −SL(E, m)\n= ln PV L(E|f)\nPV L(E|m)\nPositive values ofBV L(E) correspond to a female\nbias for the entity, while negative values corre-\nspond to a male bias. We plot the bias scores in\nTable 5a. We see that the language bias scores in\nVL-BERT largely reflect the stereotypical genders\nof these entities - indicating that the results of Sec-\ntion 4.3 generalize to a larger group of entities.\nWe can also investigate the effect of visual-\nlinguistic pretraining by comparing these entities’\nVL-BERT gender bias scores with their gender\nbias scores under BERT. We compute the language\nbias score for BERT,BBert\nL (E), by using the text-\nonly language model probabilityPL(E|g) instead.\n4We surveyed 10 people and retained 40/50 entities where\nmajority of surveyors agreed with a stereotyped label.\n80\n(a) BV LBert\nL for 40 entities which are stereotypically considered masculine or feminine. For the majority of entities, the direction\nof the gender bias score aligns with the stereotypical gender label, indicating that VL-BERT reflects these gender stereotypes.\n(b) BV LBert\nL (E) −BBert\nL (E) for the 40 gendered entities. The distribution of entities is skewed towards increased mascu-\nline/decreased feminine association for VL-BERT, indicating VL pre-training shifts the association distribution for most entities\ntowards men. Note that VL-BERT still associates cat with women and cigar with men (see 5a), but less strongly than BERT.\nFigure 5\nWe plot the difference between entities’ VL-BERT\nand BERT bias scores in Table 5b. Similar to\ntrends observed in Section 4.2, we see that the ma-\njority of objects have increased masculine associ-\nation after pre-training (BV LBert\nL < BBert\nL ).\n6 Related Work\nVision-and-Language Pre-Training Similar to\nBERT (Devlin et al., 2019), vision-and-language\ntransformers (Su et al., 2020; Tan and Bansal,\n2019; Chen et al., 2019) are trained with masked\nlanguage modeling and region modeling with mul-\ntiple input modalities. These models yield state-\nof-the-art results on many multimodal tasks: e.g.\nVQA (Antol et al., 2015), Visual Dialog (Das\net al., 2017), and VCR (Zellers et al., 2019).\nBias Measurement in Language Models\nBolukbasi et al. (2016) and Caliskan et al.\n(2017) showed that static word embeddings like\nWord2Vec and GloVe encode biases about gender\nroles. Biases negatively effect downstream tasks\n(e.g. coreference (Zhao et al., 2018; Rudinger\net al., 2018)) and exist in large pretrained models\n(Zhao et al., 2019; Kurita et al., 2019; Webster\net al., 2020). Our methodology is inspired by\nKurita et al. (2019), who utilized templates and\nthe Masked Language Modeling head of BERT\nto show how different probabilities are extracted\nfor different genders. We extend their text-only\nmethodology to vision-and-language models.\nBias in Language + VisionSeveral papers have\ninvestigated how dataset biases can override visual\nevidence in model decisions. Zhao et al. (2017)\nshowed that multimodal models can amplify gen-\nder biases in training data. In VQA, models make\ndecisions by exploiting language priors rather than\nutilizing the visual context (Goyal et al., 2017; Ra-\nmakrishnan et al., 2018). Visual biases can also\naffect language, where gendered artefacts in the\nvisual context influence generated captions (Hen-\ndricks et al., 2018; Bhargava and Forsyth, 2019).\n7 Future Work and Ethical\nConsiderations\nThis work extends the bias measuring methodol-\nogy of Kurita et al. (2019) to multimodal language\nmodels. Our case study shows that these language\nmodels are influenced by gender information from\nboth language and visual contexts - often ignoring\nvisual evidence in favor of stereotypes.\nGender is not binary, but this work performs\nbias analysis for the terms “male” and “female”\n– which are traditionally proxies for cis-male and\ncis-female. In particular, when images are used\nof male and female presenting individuals we use\nimages that self-identify as male and female. We\navoid guessing at gender presentation and note\nthat the biases studied here in this unrealistically\nsimplistic treatment of gender pose even more se-\nrious concerns for gender non-conforming, non-\nbinary, and trans-sexual individuals. A critical\n81\nnext step is designing more inclusive probes, and\ntraining (multi-modal) language models on more\ninclusive data. We welcome criticism and guid-\nance on how to expand this research. Our im-\nage based data suffers from a second, similar,\nlimitation on the dimension of race. All indi-\nviduals self-identified as “white” or “black”, but\na larger scale inclusive data-collection should be\nperformed across cultural boundaries and skin-\ntones with the self-identification and if appropri-\nate prompts can be constructed for LLMs.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: visual question an-\nswering. In 2015 IEEE International Conference on\nComputer Vision, ICCV 2015, Santiago, Chile, De-\ncember 7-13, 2015, pages 2425–2433. IEEE Com-\nputer Society.\nShruti Bhargava and David Forsyth. 2019. Ex-\nposing and correcting the gender bias in image\ncaptioning datasets and models. arXiv preprint\narXiv:1912.00578.\nSu Lin Blodgett, Solon Barocas, Hal Daum ´e III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai.\n2016. Man is to computer programmer as woman\nis to homemaker? debiasing word embeddings. In\nAdvances in Neural Information Processing Sys-\ntems 29: Annual Conference on Neural Information\nProcessing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pages 4349–4357.\nJoy Buolamwini and Timnit Gebru. 2018. Gender\nshades: Intersectional accuracy disparities in com-\nmercial gender classification. In Conference on fair-\nness, accountability and transparency, pages 77–91.\nAylin Caliskan, Joanna J Bryson, and Arvind\nNarayanan. 2017. Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2019. Uniter: Learning univer-\nsal image-text representations. arXiv preprint\narXiv:1909.11740.\nMick Cunningham. 2008. Changing attitudes toward\nthe male breadwinner, female homemaker family\nmodel: Influences of women’s employment and edu-\ncation over the lifecourse. Social forces, 87(1):299–\n323.\nHelana Darwin. 2018. Omnivorous masculinity: Gen-\nder capital and cultural legitimacy in craft beer cul-\nture. Social Currents, 5(3):301–316.\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi\nSingh, Deshraj Yadav, Jos ´e M. F. Moura, Devi\nParikh, and Dhruv Batra. 2017. Visual dialog. In\n2017 IEEE Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2017, Honolulu, HI, USA,\nJuly 21-26, 2017, pages 1080–1089. IEEE Com-\nputer Society.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJessica L Fugitt and Lindsay S Ham. 2018. Beer for\n“brohood”: A laboratory simulation of masculinity\nconfirmation through alcohol use behaviors in men.\nPsychology of Addictive Behaviors, 32(3):358.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making the\nV in VQA matter: Elevating the role of image un-\nderstanding in visual question answering. In 2017\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July\n21-26, 2017, pages 6325–6334. IEEE Computer So-\nciety.\nLisa Anne Hendricks, Kaylee Burns, Kate Saenko,\nTrevor Darrell, and Anna Rohrbach. 2018. Women\nalso snowboard: Overcoming bias in captioning\nmodels. In European Conference on Computer Vi-\nsion, pages 793–811. Springer.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W\nBlack, and Yulia Tsvetkov. 2019. Measuring bias\nin contextualized word representations. In Proceed-\nings of the First Workshop on Gender Bias in Natu-\nral Language Processing, pages 166–172, Florence,\nItaly. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nHope Landrine, Stephen Bardwell, and Tina Dean.\n1988. Gender expectations for alcohol use: A study\nof the significance of the masculine role. Sex roles,\n19(11):703–712.\n82\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research,\n21:1–67.\nSainandan Ramakrishnan, Aishwarya Agrawal, and\nStefan Lee. 2018. Overcoming language priors in\nvisual question answering with adversarial regular-\nization. In Advances in Neural Information Process-\ning Systems 31: Annual Conference on Neural In-\nformation Processing Systems 2018, NeurIPS 2018,\nDecember 3-8, 2018, Montr ´eal, Canada , pages\n1548–1558.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers),\npages 8–14, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu,\nFuru Wei, and Jifeng Dai. 2020. VL-BERT: pre-\ntraining of generic visual-linguistic representations.\nIn 8th International Conference on Learning Repre-\nsentations, ICLR 2020, Addis Ababa, Ethiopia, April\n26-30, 2020. OpenReview.net.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 5100–5111, Hong Kong, China. Association\nfor Computational Linguistics.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stuart\nShieber. 2020. Causal mediation analysis for inter-\npreting neural nlp: The case of gender bias. arXiv\npreprint arXiv:2004.12265.\nKellie Webster, Xuezhi Wang, Ian Tenney, Alex Beu-\ntel, Emily Pitler, Ellie Pavlick, Jilin Chen, and Slav\nPetrov. 2020. Measuring and reducing gendered\ncorrelations in pre-trained models. arXiv preprint\narXiv:2010.06032.\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin\nChoi. 2019. From recognition to cognition: Visual\ncommonsense reasoning. In IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR\n2019, Long Beach, CA, USA, June 16-20, 2019,\npages 6720–6731. Computer Vision Foundation /\nIEEE.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cot-\nterell, Vicente Ordonez, and Kai-Wei Chang. 2019.\nGender bias in contextualized word embeddings. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 629–634,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2017. Men also like\nshopping: Reducing gender bias amplification using\ncorpus-level constraints. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2979–2989, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 15–\n20, New Orleans, Louisiana. Association for Com-\nputational Linguistics.\nJiping Zuo and Shengming Tang. 2000. Breadwin-\nner status and gender ideologies of men and women\nregarding family roles. Sociological perspectives,\n43(1):29–43.\n83\nEntity Gender of Agent Images Used ( Im/f )\nPurse Male\nFemale\nBriefcase Male\nFemale\nApron Male\nFemale\nSuit Male\nFemale\nWine Male\nFemale\nBeer Male\nFemale\nTable 4: Images collected for case study in Section 4\nA Images Collected for Case Study\nIn Table 4, we show the different images collected\nfor our Case Study in Section 4.\nB Rationale Behind Selection of Case\nStudy Entities\nFor the purpose of the case study, we chose three\npairs of entities, each containing entities with op-\nposite gender polarities (verified using the same\nsurvey we used in Section 5). The entities were\nchosen to demonstrate how unequal gender associ-\nations perpetuate undesirable gender stereotypes.\nApron vs Suit This pair was chosen to investi-\ngate how clothing biases can reinforce stereotypes\nabout traditional gender roles. Aprons are asso-\nciated with cooking, which has long been consid-\nered a traditional job for women as homemakers.\nMeanwhile, suits are associated with business, and\nmen are typically considered to be the breadwin-\nners for their family. However, in the 21st century,\nas we make progress in breaking the breadmaker-\nhomemaker dichotomy, these gender roles do not\nnecessarily apply (Cunningham, 2008; Zuo and\nTang, 2000), and reinforcing them is harmful -\nparticularly to women, since they have struggled\n(and continue to struggle) for their right to join\nthe workforce and not be confined by their gender\nroles.\nPurse vs Briefcase Bags present another class\nof traditional gender norms that are frequently vi-\nolated in this day and age. Purses are traditionally\nassociated with women, whereas briefcases (sim-\n84\nilar to suits above) are associated with business,\nwhich we noted is customarily a male occupation.\nIf a model tends to associate purses with women,\nin the presence of contrary visual evidence, it\ncould reinforce heteronormative gender associa-\ntions. Similarly, associating briefcases with pri-\nmarily men undermines the efforts of women to\nenter the workforce.\nWine vs Beer Alcoholic drinks also contain\ngendered stereotypes that could be perpetuated by\nvisual-linguistic models. Beer is typically con-\nsidered to be a masculine drink (Fugitt and Ham,\n2018; Darwin, 2018), whereas wine is associated\nwith feminine traits (Landrine et al., 1988).\n85",
  "topic": "Stereotype (UML)",
  "concepts": [
    {
      "name": "Stereotype (UML)",
      "score": 0.7082139253616333
    },
    {
      "name": "Computer science",
      "score": 0.6848599910736084
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5697460174560547
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.5234034657478333
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5114718675613403
    },
    {
      "name": "Language model",
      "score": 0.47801434993743896
    },
    {
      "name": "Natural language processing",
      "score": 0.4694371223449707
    },
    {
      "name": "Cognitive psychology",
      "score": 0.38581883907318115
    },
    {
      "name": "Psychology",
      "score": 0.25350257754325867
    },
    {
      "name": "Software engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}