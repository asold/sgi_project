{
    "title": "Evaluating Large Language Models in Cardiology: A Comparative Study of ChatGPT, Claude, and Gemini",
    "url": "https://openalex.org/W4412514154",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2502844219",
            "name": "Michele Danilo Pierri",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A2707660814",
            "name": "Michele Galeazzi",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        },
        {
            "id": null,
            "name": "Simone D’Alessio",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A2422192851",
            "name": "Melissa Dottori",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A4283610326",
            "name": "Irene Capodaglio",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A2322171969",
            "name": "Christian Corinaldesi",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A1893915489",
            "name": "Marco Marini",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        },
        {
            "id": "https://openalex.org/A296265573",
            "name": "Marco Di Eusanio",
            "affiliations": [
                "Marche Polytechnic University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4387356888",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W2128347581",
        "https://openalex.org/W2335377152",
        "https://openalex.org/W4400012966",
        "https://openalex.org/W3042276730",
        "https://openalex.org/W3109650690",
        "https://openalex.org/W2902634493",
        "https://openalex.org/W4220832516",
        "https://openalex.org/W4397005597",
        "https://openalex.org/W4391779256",
        "https://openalex.org/W4396520509",
        "https://openalex.org/W4389519394",
        "https://openalex.org/W4399528455",
        "https://openalex.org/W4399785210",
        "https://openalex.org/W4409249263",
        "https://openalex.org/W4391432212",
        "https://openalex.org/W4389001227",
        "https://openalex.org/W6852275611",
        "https://openalex.org/W808778087",
        "https://openalex.org/W4406602253",
        "https://openalex.org/W4412255553",
        "https://openalex.org/W4409280355",
        "https://openalex.org/W4407241233",
        "https://openalex.org/W2888828233",
        "https://openalex.org/W4409157497",
        "https://openalex.org/W4408411093",
        "https://openalex.org/W4409283601",
        "https://openalex.org/W6857222836",
        "https://openalex.org/W4380320756",
        "https://openalex.org/W3082188176",
        "https://openalex.org/W2940793653",
        "https://openalex.org/W3209117858",
        "https://openalex.org/W3003658181"
    ],
    "abstract": "Background: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini are being increasingly adopted in medicine; however, their reliability in cardiology remains underexplored. Purpose of the study: To compare the performance of three general-purpose LLMs in response to cardiology-related clinical queries. Study design: Seventy clinical prompts stratified by diagnostic phase (pre or post) and user profile (patient or physician) were submitted to ChatGPT, Claude, and Gemini. Three expert cardiologists, who were blinded to the model’s identity, rated each response on scientific accuracy, completeness, clarity, and coherence using a 5-point Likert scale. Statistical analysis included Kruskal–Wallis tests, Dunn’s post hoc comparisons, Kendall’s W, weighted kappa, and sensitivity analyses. Results: ChatGPT outperformed both Claude and Gemini across all criteria (mean scores: 3.7–4.2 vs. 3.4–4.0 and 2.9–3.7, respectively; p &lt; 0.001). The inter-rater agreement was substantial (Kendall’s W: 0.61–0.71). Pre-diagnostic and patient-framed prompts received higher scores than post-diagnostic and physician-framed ones. Results remained robust across sensitivity analyses. Conclusions: Among the evaluated LLMs, ChatGPT demonstrated superior performance in generating clinically relevant cardiology responses. However, none of the models achieved maximal ratings, and the performance varied by context. These findings highlight the need for domain-specific fine-tuning and human oversight to ensure a safe clinical deployment.",
    "full_text": null
}