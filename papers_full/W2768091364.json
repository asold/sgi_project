{
    "title": "Neural Language Modeling by Jointly Learning Syntax and Lexicon",
    "url": "https://openalex.org/W2768091364",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4282814475",
            "name": "Shen, Yikang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2579146587",
            "name": "Lin, Zhouhan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287985459",
            "name": "Huang, Chin-Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3092448636",
            "name": "Courville, Aaron",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2053321134",
        "https://openalex.org/W2963355447",
        "https://openalex.org/W2951264799",
        "https://openalex.org/W2963748792",
        "https://openalex.org/W2123893795",
        "https://openalex.org/W2153568660",
        "https://openalex.org/W2403615200",
        "https://openalex.org/W2178103884",
        "https://openalex.org/W2751262944",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2113075298",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W2952191002",
        "https://openalex.org/W2118776487",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2952644835",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2076063813",
        "https://openalex.org/W1576954243",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2317136820",
        "https://openalex.org/W2135258175",
        "https://openalex.org/W2409027918",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2259512711",
        "https://openalex.org/W2510842514",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2097606805",
        "https://openalex.org/W2289899728",
        "https://openalex.org/W2155693943",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2020382207",
        "https://openalex.org/W1495446613",
        "https://openalex.org/W2572905271",
        "https://openalex.org/W2156024017",
        "https://openalex.org/W2473934411",
        "https://openalex.org/W2129882630",
        "https://openalex.org/W2950191064",
        "https://openalex.org/W2157762871",
        "https://openalex.org/W2469894155",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2326533993",
        "https://openalex.org/W2739894144",
        "https://openalex.org/W2952276042",
        "https://openalex.org/W2549416390",
        "https://openalex.org/W2144916786",
        "https://openalex.org/W2072128103",
        "https://openalex.org/W2949418605"
    ],
    "abstract": "We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks.",
    "full_text": "Published as a conference paper at ICLR 2018\nNEURAL LANGUAGE MODELING\nBY JOINTLY LEARNING SYNTAX AND LEXICON\nYikang Shen, Zhouhan Lin, Chin-Wei Huang & Aaron Courville\nDepartment of Computer Science and Operations Research\nUniversit de Montral\nMontral, QC H3C3J7, Canada\n{yi-kang.shen, zhouhan.lin, chin-wei.huang, aaron.courville}@umontreal.ca\nABSTRACT\nWe propose a neural language model capable of unsupervised syntactic structure\ninduction. The model leverages the structure information to form better semantic\nrepresentations and better language modeling. Standard recurrent neural networks\nare limited by their structure and fail to efﬁciently use syntactic information. On\nthe other hand, tree-structured recursive networks usually require additional struc-\ntural supervision at the cost of human expert annotation. In this paper, We pro-\npose a novel neural language model, called the Parsing-Reading-Predict Networks\n(PRPN), that can simultaneously induce the syntactic structure from unannotated\nsentences and leverage the inferred structure to learn a better language model. In\nour model, the gradient can be directly back-propagated from the language model\nloss into the neural parsing network. Experiments show that the proposed model\ncan discover the underlying syntactic structure and achieve state-of-the-art perfor-\nmance on word/character-level language model tasks.\n1 I NTRODUCTION\nLinguistic theories generally regard natural language as consisting of two part: a lexicon, the com-\nplete set of all possible words in a language; and a syntax, the set of rules, principles, and processes\nthat govern the structure of sentences (Sandra & Taft, 1994). To generate a proper sentence, tokens\nare put together with a speciﬁc syntactic structure. Understanding a sentence also requires lexical\ninformation to provide meanings, and syntactical knowledge to correctly combine meanings. Cur-\nrent neural language models can provide meaningful word represent (Bengio et al., 2003; Mikolov\net al., 2013; Chen et al., 2013). However, standard recurrent neural networks only implicitly model\nsyntax, thus fail to efﬁciently use structure information (Tai et al., 2015).\nDeveloping a deep neural network that can leverage syntactic knowledge to form a better semantic\nrepresentation has received a great deal of attention in recent years (Socher et al., 2013; Tai et al.,\n2015; Chung et al., 2016). Integrating syntactic structure into a language model is important for dif-\nferent reasons: 1) to obtain a hierarchical representation with increasing levels of abstraction, which\nis a key feature of deep neural networks and of the human brain (Bengio et al., 2009; LeCun et al.,\n2015; Schmidhuber, 2015); 2) to capture complex linguistic phenomena, like long-term dependency\nproblem (Tai et al., 2015) and the compositional effects (Socher et al., 2013); 3) to provide shortcut\nfor gradient back-propagation (Chung et al., 2016).\nA syntactic parser is the most common source for structure information. Supervised parsers can\nachieve very high performance on well constructed sentences. Hence, parsers can provide accurate\ninformation about how to compose word semantics into sentence semantics (Socher et al., 2013),\nor how to generate the next word given previous words (Wu et al., 2017). However, only major\nlanguages have treebank data for training parsers, and it request expensive human expert annotation.\nPeople also tend to break language rules in many circumstances (such as writing a tweet). These\ndefects limit the generalization capability of supervised parsers.\nUnsupervised syntactic structure induction has been among the longstanding challenges of compu-\ntational linguistic (Klein & Manning, 2002; 2004; Bod, 2006). Researchers are interested in this\n1\narXiv:1711.02013v2  [cs.CL]  19 Feb 2018\nPublished as a conference paper at ICLR 2018\nproblem for a variety of reasons: to be able to parse languages for which no annotated treebanks ex-\nist (Marecek, 2016); to create a dependency structure to better suit a particular NLP application (Wu\net al., 2017); to empirically argue for or against the poverty of the stimulus (Clark, 2001; Chomsky,\n2014); and to examine cognitive issues in language learning (Solan et al., 2003).\nIn this paper, we propose a novel neural language model: Parsing-Reading-Predict Networks\n(PRPN), which can simultaneously induce the syntactic structure from unannotated sentences and\nleverage the inferred structure to form a better language model. With our model, we assume that\nlanguage can be naturally represented as a tree-structured graph. The model is composed of three\nparts:\n1. A differentiable neural Parsing Networkuses a convolutional neural network to compute\nthe syntactic distance, which represents the syntactic relationships between all successive\npairs of words in a sentence, and then makes soft constituent decisions based on the syn-\ntactic distance.\n2. A Reading Networkthat recurrently computes an adaptive memory representation to sum-\nmarize information relevant to the current time step, based on all previous memories that\nare syntactically and directly related to the current token.\n3. A Predict Networkthat predicts the next token based on all memories that are syntactically\nand directly related to the next token.\nWe evaluate our model on three tasks: word-level language modeling, character-level language\nmodeling, and unsupervised constituency parsing. The proposed model achieves (or is close to) the\nstate-of-the-art on both word-level and character-level language modeling. The model’s unsuper-\nvised parsing outperforms some strong baseline models, demonstrating that the structure found by\nour model is similar to the intrinsic structure provided by human experts.\n2 R ELATED WORK\nThe idea of introducing some structures, especially trees, into language understanding to help a\ndownstream task has been explored in various ways. For example, Socher et al. (2013); Tai et al.\n(2015) learn a bottom-up encoder, taking as an input a parse tree supplied from an external parser.\nThere are models that are able to infer a tree during test time, while still need supervised signal on\ntree structure during training. For example, (Socher et al., 2010; Alvarez-Melis & Jaakkola, 2016;\nZhou et al., 2017; Zhang et al., 2015), etc. Moreover, Williams et al. (2017) did an in-depth analysis\nof recursive models that are able to learn tree structure without being exposed to any grammar trees.\nOur model is also able to infer tree structure in an unsupervised setting, but different from theirs, it\nis a recurrent network that implicitly models tree structure through attention.\nApart from the approach of using recursive networks to capture structures, there is another line of\nresearch which try to learn recurrent features at multiple scales, which can be dated back to 1990s\n(e.g. El Hihi & Bengio (1996); Schmidhuber (1991); Lin et al. (1998)). The NARX RNN (Lin et al.,\n1998) is another example which used a feed forward net taking different inputs with predeﬁned time\ndelays to model long-term dependencies. More recently, Koutnik et al. (2014) also used multiple\nlayers of recurrent networks with different pre-deﬁned updating frequencies. Instead, our model tries\nto learn the structure from data, rather than predeﬁning it. In that respect, Chung et al. (2016) relates\nto our model since it proposes a hierarchical multi-scale structure with binary gates controlling intra-\nlayer connections, and the gating mechanism is learned from data too. The difference is that their\ngating mechanism controls the updates of higher layers directly, while ours control it softly through\nan attention mechanism.\nIn terms of language modeling, syntactic language modeling can be dated back to Chelba (1997).\nCharniak (2001); Roark (2001) have also proposed language models with a top-down parsing mech-\nanism. Recently Dyer et al. (2016); Kuncoro et al. (2016) have introduced neural networks into\nthis space. It learns both a discriminative and a generative model with top-down parsing, trained\nwith a supervision signal from parsed sentences in the corpus. There are also dependency-based\napproaches using neural networks, including Buys & Blunsom (2015); Emami & Jelinek (2005);\nTitov & Henderson (2010).\n2\nPublished as a conference paper at ICLR 2018\nParsers are also related to our work since they are all inferring grammatical tree structure given a\nsentence. For example, SPINN (Bowman et al., 2016) is a shift-reduce parser that uses an LSTM as\nits composition function. The transition classiﬁer in SPINN is supervisedly trained on the Stanford\nPCFG Parser (Klein & Manning, 2003) output. Unsupervised parsers are more aligned with what\nour model is doing. Klein & Manning (2004) presented a generative model for the unsupervised\nlearning of dependency structures. Klein & Manning (2002) is a generative distributional model for\nthe unsupervised induction of natural language syntax which explicitly models constituent yields\nand contexts. We compare our parsing quality with the aforementioned two papers in Section 6.3.\n3 M OTIVATION\nFigure 1: Hard arrow represents syntactic tree structure and parent-to-child dependency relation,\ndash arrow represents dependency relation between siblings\nSuppose we have a sequence of tokens x0,...,x 6 governed by the tree structure showed in Figure 1.\nThe leafs xi are observed tokens. Node yi represents the meaning of the constituent formed by its\nleaves xl(yi),...,x r(yi), where l(·) and r(·) stands for the leftmost child and right most child. Root r\nrepresents the meaning of the whole sequence. Arrows represent the dependency relations between\nnodes. The underlying assumption is that each node depends only on its parent and its left siblings.\nDirectly modeling the tree structure is a challenging task, usually requiring supervision to learn (Tai\net al., 2015). In addition, relying on tree structures can result in a model that is not sufﬁciently robust\nto face ungrammatical sentences (Hashemi & Hwa, 2016). In contrast, recurrent models provide a\nconvenient way to model sequential data, with the current hidden state only depends on the last\nhidden state. This makes models more robust when facing nonconforming sequential data, but it\nsuffers from neglecting the real dependency relation that dominates the structure of natural language\nsentences.\nFigure 2: Proposed model architecture, hard line indicate valid connection in Reading Network,\ndash line indicate valid connection in Predict Network.\n3\nPublished as a conference paper at ICLR 2018\nIn this paper, we use skip-connection to integrate structured dependency relations with recurrent\nneural network. In other words, the current hidden state does not only depend on the last hidden\nstate, but also on previous hidden states that have a direct syntactic relation to the current one.\nFigure 2 shows the structure of our model. The non-leaf node yj is represented by a set of hidden\nstates yj = {mi}l(yj)≤i≤r(yj), where l(yj) is the left most descendant leaf and r(yj) is the right\nmost one. Arrows shows skip connections built by our model according to the latent structure. Skip\nconnections are controlled by gates gt\ni. In order to deﬁne gt\ni, we introduce a latent variable lt to\nrepresent local structural context of xt:\n• if xt is not left most child of any subtree, then lt is the position of xt’s left most sibling.\n• if xt is the left most child of a subtree yi, then lt is the position of the left most child that\nbelongs to the left most sibling of yi.\nand gates are deﬁned as:\ngt\ni =\n{1, l t ≤i<t\n0, 0 <i<l t\n(1)\nGiven this architecture, the siblings dependency relation is modeled by at least one skip-connect.\nThe skip connection will directly feed information forward, and pass gradient backward. The parent-\nto-child relation will be implicitly modeled by skip-connect relation between nodes.\nThe model recurrently updates the hidden states according to:\nmt = h(xt,m0,...,m t−1,gt\n0,...,g t\nt−1) (2)\nand the probability distribution for next word is approximated by:\np(xt+1|x0,...,x t) ≈p(xt+1; f(m0,...,m t,gt+1\n0 ,...,g t+1\nt )) (3)\nwhere gt\ni are gates that control skip-connections. Both f and hhave a structured attention mech-\nanism that takes gt\ni as input and forces the model to focus on the most related information. Since\nlt is an unobserved latent variable, We explain an approximation for gt\ni in the next section. The\nstructured attention mechanism is explained in section 5.1.\n4 M ODELING SYNTACTIC STRUCTURE\n4.1 M ODELING LOCAL STRUCTURE\nIn this section we give a probabilistic view on how to model the local structure of language. A\ndetailed elaboration for this section is given in Appendix B. At time step t, p(lt|x0,...,x t) repre-\nsents the probability of choosing one out of t possible local structures. We propose to model the\ndistribution by the Stick-Breaking Process:\np(lt = i|x0,...,x t) = (1−αt\ni)\nt−1∏\nj=i+1\nαt\nj (4)\nThe formula can be understood by noting that after the time stepi+1,...,t −1 have their probabilities\nassigned, ∏t−1\nj=i+1 αt\nj is remaining probability,1 −αt\ni is the portion of remaining probability that we\nassign to time step i. Variable αt\nj is parametrized in the next section.\nAs shown in Appendix B, the expectation of gate value gt\ni is the Cumulative Distribution Function\n(CDF) of p(lt = i|x0,...,x t). Thus, we can replace the discrete gate value by its expectation:\ngt\ni = P(lt ≤i) =\nt−1∏\nj=i+1\nαt\nj (5)\nWith these relaxations, Eq.2 and 3 can be approximated by using a soft gating vector to update the\nhidden state and predict the next token.\n4\nPublished as a conference paper at ICLR 2018\n4.2 P ARSING NETWORK\nInferring tree structure with Syntactic DistanceIn Eq.4, 1 −αt\nj is the portion of the remaining\nprobability that we assign to position j. Because the stick-breaking process should assign high\nprobability to lt, which is the closest constituent-beginning word. The model should assign large\n1 −αt\nj to words beginning new constituents. While xt itself is a constituent-beginning word, the\nmodel should assign large 1 −αt\nj to words beginning larger constituents. In other words, the model\nwill consider longer dependency relations for the ﬁrst word in constituent. Given the sentence in\nFigure 1, at time step t= 6, both 1−α6\n2 and 1−α6\n0 should be close to 1, and all other1−α6\nj should\nbe close to 0.\nIn order to parametrize αt\nj, our basic hypothesis is that words in the same constituent should have a\ncloser syntactic relation within themselves, and that this syntactical proximity can be represented by\na scalar value. From the tree structure point of view, the shortest path between leafs in same subtree\nis shorter than the one between leafs in different subtree.\nTo model syntactical proximity, we introduce a new featureSyntactic Distance. For a sentence with\nlength K, we deﬁne a set of K real valued scalar variables d0,...,d K−1, with di representing a\nmeasure of the syntactic relation between the pair of adjacent words (xi−1,xi). x−1 could be the\nlast word in previous sentence or a padding token. For time stept, we want to ﬁnd the closest words\nxj, that have larger syntactic distance than dt. Thus αt\nj can be deﬁned as:\nαt\nj = hardtanh ((dt −dj) ·τ) + 1\n2 (6)\nwhere hardtanh(x) = max(−1,min(1,x)). τ is the temperature parameter that controls the sensi-\ntivity of αt\nj to the differences between distances.\nThe Syntactic Distance has some nice properties that both allow us to infer a tree structure from it\nand be robust to intermediate non-valid tree structures that the model may encounter during learning.\nIn Appendix C and D we list these properties and further explain the meanings of their values.\nParameterizing Syntactic Distance Roark & Hollingshead (2008) shows that it’s possible to\nidentify the beginning and ending words of a constituent using local information. In our model,\nthe syntactic distance between a given token (which is usually represented as a vector word embed-\nding ei) and its previous token ei−1, is provided by a convolutional kernel over a set of consecutive\nprevious tokens ei−L,ei−L+1,...,e i. This convolution is depicted as the gray triangles shown in\nFigure 3. Each triangle here represent 2 layers of convolution. Formally, the syntactic distance di\nbetween token ei−1 and ei is computed by\nhi = ReLU(Wc\n\n\nei−L\nei−L+1\n...\nei\n\n+ bc) (7)\ndi = ReLU (Wdhi + bd) (8)\nwhere Wc, bc are the kernel parameters. Wd and bd can be seen as another convolutional kernel with\nwindow size 1, convolved over hi’s. Here the kernel window size Ldetermines how far back into\nthe history node ei can reach while computing its syntactic distancedi. Thus we call it the look-back\nrange.\nConvolving h and d on the whole sequence with length K yields a set of distances. For the tokens\nin the beginning of the sequence, we simply pad L−1 zero vectors to the front of the sequence in\norder to get K−1 outputs.\n5 M ODELING LANGUAGE\n5.1 R EADING NETWORK\nThe Reading Network generate new states mt considering on input xt, previous memory states\nm0,...,m t−1, and gates gt\n0,...,g t\nt−1, as shown in Eq.2.\n5\nPublished as a conference paper at ICLR 2018\nFigure 3: Convolutional network for computing syntactic distance. Gray triangles represent 2 layers\nof convolution, d0 to d7 are the syntactic distance output by each of the kernel position. The blue\nbars indicate the amplitude of di’s, andyi’s are the inferred constituents.\nSimilar to Long Short-Term Memory-Network (LSTMN) (Cheng et al., 2016), the Reading Net-\nwork maintains the memory states by maintaining two sets of vectors: a hidden tape Ht−1 =\n(ht−Nm ,...,h t−1), and a memory tape Ct−1 = (ct−L,...,c t−1), where Nm is the upper bound\nfor the memory span. Hidden states mi is now represented by a tuple of two vectors (hi,ci). The\nReading Network captures the dependency relation by a modiﬁed attention mechanism: structured\nattention. At each step of recurrence, the model summarizes the previous recurrent states via the\nstructured attention mechanism, then performs a normal LSTM update, with hidden and cell states\noutput by the attention mechanism.\nStructured Attention At each time step t, the read operation attentively links the current token to\nprevious memories with a structured attention layer:\nkt = Whht−1 + Wxxt (9)\n˜st\ni = softmax(hikT\nt√δk\n) (10)\nwhere, δk is the dimension of the hidden state. Modulated by the gates in Eq.5, the structured\nintra-attention weight is deﬁned as:\nst\ni = gt\ni ˜st\ni∑\ni gt\ni\n(11)\nThis yields a probability distribution over the hidden state vectors of previous tokens. We can then\ncompute an adaptive summary vector for the previous hidden tape and memory denoting by ˜ht and\n˜ct: [˜ht\n˜ct\n]\n=\nt−1∑\ni=1\nst\ni ·mi =\nt−1∑\ni=1\nst\ni ·\n[\nhi\nci\n]\n(12)\nStructured attention provides a way to model the dependency relations shown in Figure 1.\nRecurrent Update The Reading Network takes xt, ˜ct and ˜ht as input, computes the values of ct\nand ht by the LSTM recurrent update (Hochreiter & Schmidhuber, 1997). Then the write operation\nconcatenates ht and ct to the end of hidden and memory tape.\n5.2 P REDICT NETWORK\nPredict Network models the probability distribution of next wordxt+1, considering on hidden states\nm0,...,m t, and gates gt+1\n0 ,...,g t+1\nt . Note that, at time step t, the model cannot observe xt+1 , a\ntemporary estimation of dt+1 is computed considering on xt−L,...,x t:\nd′\nt+1 = ReLU(W′\ndht + b′\nd) (13)\nFrom there we compute its corresponding {αt+1}and {gt+1\ni }for Eq.3. We parametrize f(·) func-\ntion as:\nf(m0,...,m t,gt+1\n0 ,...,g t+1\nt ) = ˆf([hl:t−1,ht]) (14)\n6\nPublished as a conference paper at ICLR 2018\nFigure 4: Syntactic distance estimated by Parsing Network. The model is trained on PTB dataset at\nthe character level. Each blue bar is positioned between two characters, and represents the syntactic\ndistance between them. From these distances we can infer a tree structure according to Section 4.2.\nwhere hl:t−1 is an adaptive summary of hlt+1≤i≤t−1, output by structured attention controlled by\ngt+1\n0 ,...,g t+1\nt−1. ˆf(·) could be a simple feed-forward MLP, or more complex architecture, like ResNet,\nto add more depth to the model.\n6 E XPERIMENTS\nWe evaluate the proposed model on three tasks, character-level language modeling, word-level lan-\nguage modeling, and unsupervised constituency parsing.\n6.1 C HARACTER -LEVEL LANGUAGE MODEL\nFrom a character-level view, natural language is a discrete sequence of data, where discrete symbols\nform a distinct and shallow tree structure: the sentence is the root, words are children of the root, and\ncharacters are leafs. However, compared to word-level language modeling, character-level language\nmodeling requires the model to handle longer-term dependencies. We evaluate a character-level\nvariant of our proposed language model over a preprocessed version of the Penn Treebank (PTB)\nand Text8 datasets.\nWhen training, we use truncated back-propagation, and feed the ﬁnal memory position from the\nprevious batch as the initial memory of next one. At the beginning of training and test time, the\nmodel initial hidden states are ﬁlled with zero. Optimization is performed with Adam using learning\nrate lr = 0.003, weight decay wdecay = 10−6, β1 = 0.9, β2 = 0.999 and σ = 10−8. We carry\nout gradient clipping with maximum norm 1.0. The learning rate is multiplied by 0.1 whenever\nvalidation performance does not improve during 2 checkpoints. These checkpoints are performed at\nthe end of each epoch. We also apply layer normalization (Ba et al., 2016) to the Reading Network\nand batch normalization to the Predict Network and parsing network. For all of the character-level\nlanguage modeling experiments, we apply the same procedure, varying only the number of hidden\nunits, mini-batch size and dropout rate.\nPenn Treebank we process the Penn Treebank dataset (Marcus et al., 1993) by following the\nprocedure introduced in (Mikolov et al., 2012). For character-level PTB, Reading Network has two\nrecurrent layers, Predict Network has one residual block. Hidden state size is 1024 units. The\ninput and output embedding size are 128, and not shared. Look-back range L = 10, temperature\nparameter τ = 10, upper band of memory spanNm = 20. We use a batch size of 64, truncated back-\npropagation with 100 timesteps. The values used of dropout on input/output embeddings, between\nrecurrent layers, and on recurrent states were (0, 0.25, 0.1) respectively.\nIn Figure 4, we visualize the syntactic distance estimated by the Parsing Network, while reading\nthree different sequences from the PTB test set. We observe that the syntactic distance tends to be\nhigher between the last character of a word and a space, which is a reasonable breakpoint to sepa-\nrate between words. In other words, if the model sees a space, it will attend on all previous step. If\nthe model sees a letter, it will attend no further then the last space step. The model autonomously\ndiscovered to avoid inter-word attention connection, and use the hidden states of space (separator)\ntokens to summarize previous information. This is strong proof that the model can understand the\nlatent structure of data. As a result our model achieve state-of-the-art performance and signiﬁcantly\n7\nPublished as a conference paper at ICLR 2018\nModel BPC\nNorm-stabilized RNN (Krueger & Memisevic, 2015) 1.48\nCW-RNN (Koutnik et al., 2014) 1.46\nHF-MRNN (Mikolov et al., 2012) 1.41\nMI-RNN (Wu et al., 2016) 1.39\nME n-gram (Mikolov et al., 2012) 1.37\nBatchNorm LSTM (Cooijmans et al., 2016) 1.32\nZoneout RNN (Krueger et al., 2016) 1.27\nHyperNetworks (Ha et al., 2016) 1.27\nLayerNorm HM-LSTM (Chung et al., 2016) 1.24\nLayerNorm HyperNetworks (Ha et al., 2016) 1.23\nPRPN 1.202\nTable 1: BPC on the Penn Treebank test set\noutperform baseline models. It is worth noting that HM-LSTM (Chung et al., 2016) also unsuper-\nvisedly induce similar structure from data. But discrete operations in HM-LSTM make their training\nprocedure more complicated then ours.\n6.2 W ORD -LEVEL LANGUAGE MODEL\nComparing to character-level language modeling, word-level language modeling needs to deal with\ncomplex syntactic structure and various linguistic phenomena. But it has less long-term dependen-\ncies. We evaluate the word-level variant of our language model on a preprocessed version of the\nPenn Treebank (PTB) (Marcus et al., 1993) and Text8 (Mahoney, 2011) dataset.\nWe apply the same procedure and hyper-parameters as in character-level language model. Except\noptimization is performed with Adam with β1 = 0. This turns off the exponential moving average\nfor estimates of the means of the gradients (Melis et al., 2017). We also adapt the number of hidden\nunits, mini-batch size and the dropout rate according to the different tasks.\nPenn Treebank we process the Penn Treebank dataset (Mikolov et al., 2012) by following the\nprocedure introduced in (Mikolov et al., 2010). For word-level PTB, the Reading Network has two\nrecurrent layers and the Predict Network do not have residual block. The hidden state size is 1200\nunits and the input and output embedding sizes are 800, and shared (Inan et al., 2016; Press & Wolf,\n2017). Look-back range L= 5, temperature parameter τ = 10and the upper band of memory span\nNm = 15. We use a batch size of 64, truncated back-propagation with 35 time-steps. The values\nused of dropout on input/output embeddings, between recurrent layers, and on recurrent states were\n(0.7, 0.5, 0.5) respectively.\nModel PPL\nRNN-LDA + KN-5 + cache (Mikolov & Zweig, 2012) 92.0\nLSTM (Zaremba et al., 2014) 78.4\nVariational LSTM (Kim et al., 2016) 78.9\nCharCNN (Kim et al., 2016) 78.9\nPointer Sentinel-LSTM (Merity et al., 2016) 70.9\nLSTM + continuous cache pointer (Grave et al., 2016) 72.1\nVariational LSTM (tied) + augmented loss (Inan et al., 2016) 68.5\nVariational RHN (tied) (Zilly et al., 2016) 65.4\nNAS Cell (tied) (Zoph & Le, 2016) 62.4\n4-layer skip connection LSTM (tied) (Melis et al., 2017) 58.3\nPRPN 61.98\nTable 2: PPL on the Penn Treebank test set\n8\nPublished as a conference paper at ICLR 2018\nModel PPL\nPRPN 61.98\n- Parsing Net 64.42\n- Reading Net Attention 64.63\n- Predict Net Attention 63.65\nOur 2-layer LSTM 65.81\nTable 3: Ablation test on the Penn Treebank. “- Parsing Net” means that we remove Parsing Net-\nwork and replace Structured Attention with normal attention mechanism; “- Reading Net Attention”\nmeans that we remove Structured Attention from Reading Network, that is equivalent to replace\nReading Network with a normal 2-layer LSTM; “- Predict Net Attention” means that we remove\nStructured Attention from Predict Network, that is equivalent to have a standard projection layer;\n“Our 2-layer LSTM” is equivalent to remove Parsing Network and remove Structured Attention\nfrom both Reading and Predict Network.\nText8 dataset contains 17M training tokens and has a vocabulary size of 44k words. The dataset is\npartitioned into a training set (ﬁrst 99M characters) and a development set (last 1M characters) that is\nused to report performance. As this dataset contains various articles from Wikipedia, the longer term\ninformation (such as current topic) plays a bigger role than in the PTB experiments (Mikolov et al.,\n2014). We apply the same procedure and hyper-parameters as in character-level PTB, except we use\na batch size of 128. The values used of dropout on input/output embeddings, between Recurrent\nLayers and on recurrent states were (0.4, 0.2, 0.2) respectively.\nModel PPL\nLSTM-500 (Mikolov et al., 2014) 156\nSCRNN (Mikolov et al., 2014) 161\nMemNN (Sukhbaatar et al., 2015) 147\nLSTM-1024 (Grave et al., 2016) 121\nLSTM + continuous cache pointer (Grave et al., 2016) 99.9\nPRPN 81.64\nTable 4: PPL on the Text8 valid set\nIn Table 2, our results are comparable to the state-of-the-art methods. Since we do not have the\nsame computational resource used in (Melis et al., 2017) to tune hyper-parameters at large scale, we\nexpect that our model could achieve better performance after an aggressive hyperparameter tuning\nprocess. As shown in Table 4, our method outperform baseline methods. It is worth noticing that\nthe continuous cache pointer can also be applied to output of our Predict Network without modiﬁ-\ncation. Visualizations of tree structure generated from learned PTB language model are included in\nAppendix A. In Table 3, we show the value of test perplexity for different variants of PRPN, each\nvariant remove part of the model. By removing Parsing Network, we observe a signiﬁcant drop of\nperformance. This stands as empirical evidence regarding the beneﬁt of having structure information\nto control attention.\n6.3 U NSUPERVISED CONSTITUENCY PARSING\nThe unsupervised constituency parsing task compares hte tree structure inferred by the model with\nthose annotated by human experts. The experiment is performed on WSJ10 dataset. WSJ10 is the\n7422 sentences in the Penn Treebank Wall Street Journal section which contained 10 words or less\nafter the removal of punctuation and null elements. Evaluation was done by seeing whether pro-\nposed constituent spans are also in the Treebank parse, measuring unlabeled F1 (UF1) of unlabeled\nconstituent precision and recall. Constituents which could not be gotten wrong (those of span one\nand those spanning entire sentences) were discarded. Given the mechanism discussed in Section\n4.2, our model generates a binary tree. Although standard constituency parsing tree is not limited\nto binary tree. Previous unsupervised constituency parsing model also generate binary trees (Klein\n9\nPublished as a conference paper at ICLR 2018\n& Manning, 2002; Bod, 2006). Our model is compared with the several baseline methods, that are\nexplained in Appendix E.\nDifferent from the previous experiment setting, the model treat each sentence independently during\ntrain and test time. When training, we feed one batch of sentences at each iteration. In a batch,\nshorter sentences are padded with 0. At the beginning of the iteration, the model’s initial hidden\nstates are ﬁlled with zero. When testing, we feed on sentence one by one to the model, then use\nthe gate value output by the model to recursively combine tokens into constituents, as described in\nAppendix A.\nModel UF1\nLBRANCH 28.7\nRANDOM 34.7\nDEP-PCFG (Carroll & Charniak, 1992) 48.2\nRBRANCH 61.7\nCCM (Klein & Manning, 2002) 71.9\nDMV+CCM (Klein & Manning, 2005) 77.6\nUML-DOP (Bod, 2006) 82.9\nPRPN 70.02\nUPPER BOUND 88.1\nTable 5: Parsing Performance on the WSJ10 dataset\nTable 5 summarizes the results. Our model signiﬁcantly outperform the RANDOM baseline indicate\na high consistency with human annotation. Our model also shows a comparable performance with\nCCM model. In fact our parsing network and CCM both focus on the relation between successive\ntokens. As described in Section 4.2, our model computes syntactic distance between all successive\npair of tokens, then our parsing algorithm recursively assemble tokens into constituents according\nto the learned distance. CCM also recursively model the probability whether a contiguous subse-\nquences of a sentence is a constituent. Thus, one can understand how our model is outperformed\nby DMV+CCM and UML-DOP models. The DMV+CCM model has extra information from a de-\npendency parser. The UML-DOP approach captures both contiguous and non-contiguous lexical\ndependencies (Bod, 2006).\n7 C ONCLUSION\nIn this paper, we propose a novel neural language model that can simultaneously induce the syntactic\nstructure from unannotated sentences and leverage the inferred structure to learn a better language\nmodel. We introduce a new neural parsing network: Parsing-Reading-Predict Network, that can\nmake differentiable parsing decisions. We use a new structured attention mechanism to control skip\nconnections in a recurrent neural network. Hence induced syntactic structure information can be\nused to improve the model’s performance. Via this mechanism, the gradient can be directly back-\npropagated from the language model loss function into the neural Parsing Network. The proposed\nmodel achieve (or is close to) the state-of-the-art on both word/character-level language modeling\ntasks. Experiment also shows that the inferred syntactic structure highly correlated to human expert\nannotation.\nACKNOWLEDGEMENT\nThe authors would like to thank Timothy J. O’Donnell and Chris Dyer for the helpful discussions.\nREFERENCES\nDavid Alvarez-Melis and Tommi S Jaakkola. Tree-structured decoding with doubly-recurrent neural\nnetworks. 2016.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n10\nPublished as a conference paper at ICLR 2018\nYoshua Bengio, R ´ejean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\nlanguage model. Journal of machine learning research, 3(Feb):1137–1155, 2003.\nYoshua Bengio et al. Learning deep architectures for ai. Foundations and trends R⃝in Machine\nLearning, 2(1):1–127, 2009.\nRens Bod. An all-subtrees approach to unsupervised parsing. In Proceedings of the 21st Interna-\ntional Conference on Computational Linguistics and the 44th annual meeting of the Association\nfor Computational Linguistics, pp. 865–872. Association for Computational Linguistics, 2006.\nSamuel R Bowman, Jon Gauthier, Abhinav Rastogi, Raghav Gupta, Christopher D Manning, and\nChristopher Potts. A fast uniﬁed model for parsing and sentence understanding. arXiv preprint\narXiv:1603.06021, 2016.\nJan Buys and Phil Blunsom. Generative incremental dependency parsing with neural networks. In\nProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers) ,\nvolume 2, pp. 863–869, 2015.\nGlenn Carroll and Eugene Charniak. Two experiments on learning probabilistic dependency gram-\nmars from corpora. Department of Computer Science, Univ., 1992.\nEugene Charniak. Immediate-head parsing for language models. In Proceedings of the 39th Annual\nMeeting on Association for Computational Linguistics , pp. 124–131. Association for Computa-\ntional Linguistics, 2001.\nCiprian Chelba. A structured language model. In Proceedings of the eighth conference on Eu-\nropean chapter of the Association for Computational Linguistics , pp. 498–500. Association for\nComputational Linguistics, 1997.\nYanqing Chen, Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. The expressive power of word\nembeddings. arXiv preprint arXiv:1301.3226, 2013.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\nNoam Chomsky. Aspects of the Theory of Syntax, volume 11. MIT press, 2014.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704, 2016.\nAlexander Clark. Unsupervised induction of stochastic context-free grammars using distributional\nclustering. In Proceedings of the 2001 workshop on Computational Natural Language Learning-\nVolume 7, pp. 13. Association for Computational Linguistics, 2001.\nTim Cooijmans, Nicolas Ballas, C ´esar Laurent, C ¸ a˘glar G ¨ulc ¸ehre, and Aaron Courville. Recurrent\nbatch normalization. arXiv preprint arXiv:1603.09025, 2016.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network\ngrammars. arXiv preprint arXiv:1602.07776, 2016.\nSalah El Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for long-term dependen-\ncies. 1996. URL http://www.iro.umontreal.ca/˜lisa/pointeurs/elhihi_\nbengio_96.pdf.\nAhmad Emami and Frederick Jelinek. A neural syntactic language model. Machine learning, 60\n(1-3):195–227, 2005.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a\ncontinuous cache. arXiv preprint arXiv:1612.04426, 2016.\nDavid Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016.\nHoma B Hashemi and Rebecca Hwa. An evaluation of parser robustness for ungrammatical sen-\ntences. In EMNLP, pp. 1765–1774, 2016.\n11\nPublished as a conference paper at ICLR 2018\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. arXiv preprint arXiv:1611.01462, 2016.\nYoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language\nmodels. In AAAI, pp. 2741–2749, 2016.\nDan Klein and Christopher D Manning. A generative constituent-context model for improved gram-\nmar induction. In Proceedings of the 40th Annual Meeting on Association for Computational\nLinguistics, pp. 128–135. Association for Computational Linguistics, 2002.\nDan Klein and Christopher D. Manning. Accurate unlexicalized parsing. In Proceedings of the 41st\nAnnual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pp. 423–430,\nStroudsburg, PA, USA, 2003. Association for Computational Linguistics. doi: 10.3115/1075096.\n1075150. URL https://doi.org/10.3115/1075096.1075150.\nDan Klein and Christopher D Manning. Corpus-based induction of syntactic structure: Models of\ndependency and constituency. In Proceedings of the 42nd Annual Meeting on Association for\nComputational Linguistics, pp. 478. Association for Computational Linguistics, 2004.\nDan Klein and Christopher D Manning. Natural language grammar induction with a generative\nconstituent-context model. Pattern recognition, 38(9):1407–1419, 2005.\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber. A clockwork rnn. In Inter-\nnational Conference on Machine Learning, pp. 1863–1871, 2014.\nDavid Krueger and Roland Memisevic. Regularizing rnns by stabilizing activations. arXiv preprint\narXiv:1511.08400, 2015.\nDavid Krueger, Tegan Maharaj, J ´anos Kram ´ar, Mohammad Pezeshki, Nicolas Ballas, Nan Rose-\nmary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout:\nRegularizing rnns by randomly preserving hidden activations. arXiv preprint arXiv:1606.01305,\n2016.\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng Kong, Chris Dyer, Graham Neubig, and Noah A\nSmith. What do recurrent neural network grammars learn about syntax? arXiv preprint\narXiv:1611.05774, 2016.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436–444,\n2015.\nTsungnan Lin, Bill G Horne, Peter Tino, and C Lee Giles. Learning long-term dependencies is not\nas difﬁcult with narx recurrent neural networks. Technical report, 1998.\nMatt Mahoney. Large text compression benchmark, 2011.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\nDavid Marecek. Twelve years of unsupervised dependency parsing. In ITAT, pp. 56–62, 2016.\nG´abor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589, 2017.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nTomas Mikolov and Geoffrey Zweig. Context dependent recurrent neural network language model.\nSLT, 12:234–239, 2012.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Interspeech, volume 2, pp. 3, 2010.\n12\nPublished as a conference paper at ICLR 2018\nTom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cer-\nnocky. Subword language modeling with neural networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf), 2012.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word represen-\ntations in vector space. arXiv preprint arXiv:1301.3781, 2013.\nTomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc’Aurelio Ranzato.\nLearning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. InProceedings\nof the 15th Conference of the European Chapter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pp. 157–163. Association for Computational Linguistics, 2017. URL\nhttp://www.aclweb.org/anthology/E17-2025.\nBrian Roark. Probabilistic top-down parsing and language modeling. Computational linguistics, 27\n(2):249–276, 2001.\nBrian Roark and Kristy Hollingshead. Classifying chart cells for quadratic complexity context-free\ninference. In Proceedings of the 22nd International Conference on Computational Linguistics-\nVolume 1, pp. 745–751. Association for Computational Linguistics, 2008.\nDominiek Sandra and Marcus Taft. Morphological structure, lexical representation and lexical\naccess. Taylor & Francis, 1994.\nJ¨urgen Schmidhuber. Deep learning in neural networks: An overview.Neural networks, 61:85–117,\n2015.\nJrgen Schmidhuber. Neural sequence chunkers. Technical report, 1991.\nRichard Socher, Christopher D Manning, and Andrew Y Ng. Learning continuous phrase represen-\ntations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010\nDeep Learning and Unsupervised Feature Learning Workshop, pp. 1–9, 2010.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-\ncessing, pp. 1631–1642, 2013.\nZach Solan, Eytan Ruppin, David Horn, and Shimon Edelman. Automatic acquisition and efﬁcient\nrepresentation of syntactic structures. In Advances in Neural Information Processing Systems, pp.\n107–114, 2003.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. InAdvances\nin neural information processing systems, pp. 2440–2448, 2015.\nKai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations\nfrom tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015.\nIvan Titov and James Henderson. A latent variable model for generative dependency parsing. In\nTrends in Parsing Technology, pp. 35–55. Springer, 2010.\nAdina Williams, Andrew Drozdov, and Samuel R Bowman. Learning to parse from a semantic\nobjective: It works. is it syntax? arXiv preprint arXiv:1709.01121, 2017.\nShuangzhi Wu, Dongdong Zhang, Nan Yang, Mu Li, and Ming Zhou. Sequence-to-dependency\nneural machine translation. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), volume 1, pp. 698–707, 2017.\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. On multi-\nplicative integration with recurrent neural networks. In Advances in Neural Information Process-\ning Systems, pp. 2856–2864, 2016.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329, 2014.\n13\nPublished as a conference paper at ICLR 2018\nXingxing Zhang, Liang Lu, and Mirella Lapata. Top-down tree long short-term memory networks.\narXiv preprint arXiv:1511.00060, 2015.\nGanbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao, Fen Lin, Bo Chen, and Qing He. Generative\nneural machine for tree structures. arXiv preprint arXiv:1705.00321, 2017.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn ´ık, and J¨urgen Schmidhuber. Recurrent\nhighway networks. arXiv preprint arXiv:1607.03474, 2016.\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\narXiv:1611.01578, 2016.\n14\nPublished as a conference paper at ICLR 2018\nAPPENDIX\nA I NFERRED TREE STRUCTURE\nFigure 5: Syntactic structures of two different sentences inferred from {di}given by Parsing Net-\nwork.\nThe tree structure is inferred from the syntactic distances yielded by the Parsing Network. We ﬁrst\nsort the di’s in decreasing order. For the ﬁrst di in the sorted sequence, we separate sentence into\nconstituents ((x<i),(xi,(x>i))). Then we separately repeat this operation for constituents (x<i)\nand (x>i). Until the constituent contains only one word.\n15\nPublished as a conference paper at ICLR 2018\nB M ODELING LOCAL STRUCTURE\nIn this section we give a probabilistic view on how to model the local structure of language. Given\nthe nature of language, sparse connectivity can be enforced as a prior on how to improve general-\nization and interpretability of the model.\nAt time step t, p(lt|x0,...,x t) represents the probability of choosing one out of t possible local\nstructures that deﬁnes the conditional dependencies. If lt = t′, it means xt depends on all the\nprevious hidden state from mt′ to mt (t′≤t).\nA particularly ﬂexible option for modeling p(lt|x0,...,x t) is the Dirichlet Process, since being non-\nparametric allows us to attend on as many words as there are in a sentence; i.e. number of possible\nstructures (mixture components) grows with the length of the sentence. As a result, we can write the\nprobability of lt+1 = t′as a consequence of the stick breaking process 1:\np(lt = t′|x0,...,x t) = (1−αt\nt′ )\nt−1∏\nj=t′+1\nαt\nj (15)\nfor 1 ≤t′<t −1, and\np(lt = t−1|x0,...,x t) = (1−αt\nt−1); p(lt = 0|x0,...,x t) =\nt−1∏\nj=1\nαt\nj (16)\nwhere αj = 1 −βj and βj is a sample from a Beta distribution. Once we sample lt from the\nprocess, the connectivity is realized by a element-wise multiplication of an attention weight vector\nwith a masking vector gt deﬁned in Eq. 1. In this way, xt becomes functionally independent of all\nxs for all s<l t. The expectation of this operation is the CDF of the probability of l, since\nElt [g{t′}\nt ] =\n∏\nj=1\nαt\nj + (1−αt\n1)\n∏\nj=2\nαt\nj + ...+ (1−αt\nt′ )\n∏\nj=t′+1\nαt\nj\n=\nt′\n∑\nk=0\np(lt = k|x0,...,x t) =P(lt ≤t′)\n(17)\nBy telescopic cancellation, the CDF can be expressed in a succinct way:\nP(lt ≤t′) =\nt−1∏\nj=t′+1\nαt\nj (18)\nfor t′ < t, and P(lt ≤t) = 1. However, being Bayesian nonparametric and assuming a latent\nvariable model require approximate inference. Hence, we have the following relaxations\n1. First, we relax the assumption and parameterize αt\nj as a deterministic function depending\non all the previous words, which we will describe in the next section.\n2. We replace the discrete decision on the graph structure with a soft attention mechanism, by\nmultiplying attention weight with the multiplicative gate:\ngt\ni =\nt∏\nj=i+1\nαt\nj (19)\nWith these relaxations, Eq. (3) can be approximated by using a soft gating vector to update the\nhidden state hand the predictive function f. This approximation is reasonable since the gate is the\nexpected value of the discrete masking operation described above.\n1Note that the index is in decreasing order.\n16\nPublished as a conference paper at ICLR 2018\nC N O PARTIAL OVERLAPPING IN DEPENDENCY RANGES\nIn this appendix, we show that having no partial overlapping in dependency ranges is an essential\nproperty for recovering a valid tree structure, and PRPN can provide a binary version ofgt\ni, that have\nthis property.\nThe masking vector gt\ni introduced in Section 4.1 determines the range of dependency, i.e., for the\nword xt we have gt\ni = 1for all lt ≤i<t . All the words fall into the range lt ≤i<t is considered\nas xt’s sibling or offspring of its sibling. If the dependency ranges of two words are disjoint with\neach other, that means the two words belong to two different subtrees. If one range contains another,\nthat means the one with smaller range is a sibling, or is an offspring of a sibling of the other word.\nHowever, if they partially overlaps, they can’t form a valid tree.\nWhile Eq.5 and Eq.6 provide a soft version of dependency range, we can recover a binary version\nby setting τ in Eq.6 to +∞. The binary version of αt\nj corresponding to Eq. 6 becomes:\nαt\nj = sign (dt −dj+1) + 1\n2 (20)\nwhich is basically the sign of comparing dt and dj+1, scaled to the range of 0 and 1. Then for each\nof its previous token the gate value gt\ni can be computed through Eq.5.\nNow for a certain xt, we have\ngt\ni =\n{1, t ′≤i<t\n0, 0 ≤i<t ′ (21)\nwhere\nt′= maxi, s.t. d i >dt (22)\nNow all the words that fall into the range t′ ≤i < tare considered as either sibling of xt, or\noffspring of a sibling of xt (Figure 3). The essential point here is that, under this parameterization,\nthe dependency range of any two tokens won’t partially overlap. Here we provide a terse proof:\nProof. Let’s assume that the dependency range of xv and xn partially overlaps. We should have\ngu\ni = 1 for u ≤ i < vand gn\ni = 1 for m ≤ i < n. Without losing generality, we assume\nu<m<v <n so that the two dependency ranges overlap in the range [m,v].\n1. For xv, we have αv\ni = 1for all u≤i<v . According to Eq. 6 and 5, we have di <dv for\nall u≤i<v . Since u<m , we have dm <dv.\n2. Similarly, for xn, we have di <dn for all m≤i<n . Since m<v , we have dv <dn. On\nthe other hand, since the range stops at m, we should also have dm >dn. Thus dm >dv.\nItems 1 and 2 are contradictory, so the dependency ranges ofxv and xn won’t partially overlap.\nD P ROPERTIES AND INTUITIONS OF gt\ni AND di\nFirst, for any ﬁxed t, gt\ni is monotonic in i. This ensures that gt\ni still provides soft truncation to deﬁne\na dependency range.\nThe second property comes from τ. The hyperparameter τ has an interesting effect on the tree\nstructure: if it is set to 0, then for all t, the gates gt\ni will be open to all of et’s predecessors, which\nwill result in a ﬂat tree where all tokens are direct children of the root node; as τ becomes larger,\nthe number of levels of hierarchy in the tree increases. As it approaches + inf, the hardtanh(·)\nbecomes sign(·) and the dependency ranges form a valid tree. Note that, due to the linear part of\nthe gating mechanism, which beneﬁts training, when τ has a value in between the two extremes\nthe truncation range for each token may overlap. That may sometimes result in vagueness in some\npart of the inferred tree. To eliminate this vagueness and ensure a valid tree, at test time we use\nτ = + inf.\nUnder this framework, the values of syntactic distance have more intuitive meanings. If two adjacent\nwords are siblings of each other, the syntactic distance should approximate zero; otherwise, if they\n17\nPublished as a conference paper at ICLR 2018\nbelong to different subtrees, they should have a larger syntactic distance. In the extreme case, the\nsyntactic distance approaches 1 if the two words have no subtree in common. In Figure 3 we show\nthe syntactic distances for each adjacent token pair which results in the tree shown in Figure 1.\nE B ASELINE METHODS FOR UNSUPERVISED CONSTITUENCY PARSING\nOur model is compared with the same baseline methods as in (Klein & Manning, 2005). RANDOM\nchooses a binary tree uniformly at random from the set of binary trees. This is the unsupervised\nbaseline. LBRANCH and RBRANCH choose the completely left- and right-branching structures, re-\nspectively. RBRANCH is a frequently used baseline for supervised parsing, but it should be stressed\nthat it encodes a signiﬁcant fact about English structure, and an induction system need not beat it to\nclaim a degree of success. UPPER BOUND is the upper bound on how well a binary system can do\nagainst the Treebank sentences. Because the Treebank sentences are generally more ﬂat than binary,\nlimiting the maximum precision which can be attained, since additional brackets added to provide a\nbinary tree will be counted as wrong.\nWe also compared our model with other unsupervised constituency parsing methods. DEP-PCFG\nis dependency-structured PCFG (Carroll & Charniak, 1992). CCM is constituent-context model\n(Klein & Manning, 2002). DMV is an unsupervised dependency parsing model. DMV+CCM is\na combined model that jointly learn both constituency and dependency parser (Klein & Manning,\n2004).\n18"
}