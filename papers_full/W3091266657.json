{
  "title": "Mining Insights from Large-Scale Corpora Using Fine-Tuned Language Models",
  "url": "https://openalex.org/W3091266657",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4287284459",
      "name": "Palakodety, Shriphani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222242685",
      "name": "KhudaBukhsh, Ashiqur R.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4289116639",
      "name": "Carbonell, Jaime G.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2097162496",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2585577269",
    "https://openalex.org/W3123712780",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2787752663",
    "https://openalex.org/W3101245316",
    "https://openalex.org/W1857977105",
    "https://openalex.org/W2963834345",
    "https://openalex.org/W2122369144",
    "https://openalex.org/W2123822454",
    "https://openalex.org/W1968188631",
    "https://openalex.org/W3091510409",
    "https://openalex.org/W2127925090"
  ],
  "abstract": "Mining insights from large volume of social media texts with minimal supervision is a highly challenging Natural Language Processing (NLP) task. While Language Models' (LMs) efficacy in several downstream tasks is well-studied, assessing their applicability in answering relational questions, tracking perception or mining deeper insights is under-explored. Few recent lines of work have scratched the surface by studying pre-trained LMs' (e.g., BERT) capability in answering relational questions through \"fill-in-the-blank\" cloze statements (e.g., [Dante was born in MASK]). BERT predicts the MASK-ed word with a list of words ranked by probability (in this case, BERT successfully predicts Florence with the highest probability). In this paper, we conduct a feasibility study of fine-tuned LMs with a different focus on tracking polls, tracking community perception and mining deeper insights typically obtained through costly surveys. Our main focus is on a substantial corpus of video comments extracted from YouTube videos (6,182,868 comments on 130,067 videos by 1,518,077 users) posted within 100 days prior to the 2019 Indian General Election. Using fill-in-the-blank cloze statements against a recent high-performance language modeling algorithm, BERT, we present a novel application of this family of tools that is able to (1) aggregate political sentiment (2) reveal community perception and (3) track evolving national priorities and issues of interest.",
  "full_text": null,
  "topic": "Scale (ratio)",
  "concepts": [
    {
      "name": "Scale (ratio)",
      "score": 0.5947650671005249
    },
    {
      "name": "Computer science",
      "score": 0.5923357009887695
    },
    {
      "name": "Natural language processing",
      "score": 0.5476173758506775
    },
    {
      "name": "Artificial intelligence",
      "score": 0.39824509620666504
    },
    {
      "name": "Geography",
      "score": 0.18202075362205505
    },
    {
      "name": "Cartography",
      "score": 0.134083092212677
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}