{
    "title": "Mining Insights from Large-Scale Corpora Using Fine-Tuned Language Models",
    "url": "https://openalex.org/W3091266657",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4287284459",
            "name": "Palakodety, Shriphani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222242685",
            "name": "KhudaBukhsh, Ashiqur R.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4289116639",
            "name": "Carbonell, Jaime G.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2097162496",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2585577269",
        "https://openalex.org/W3123712780",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2787752663",
        "https://openalex.org/W3101245316",
        "https://openalex.org/W1857977105",
        "https://openalex.org/W2963834345",
        "https://openalex.org/W2122369144",
        "https://openalex.org/W2123822454",
        "https://openalex.org/W1968188631",
        "https://openalex.org/W3091510409",
        "https://openalex.org/W2127925090"
    ],
    "abstract": "Mining insights from large volume of social media texts with minimal supervision is a highly challenging Natural Language Processing (NLP) task. While Language Models' (LMs) efficacy in several downstream tasks is well-studied, assessing their applicability in answering relational questions, tracking perception or mining deeper insights is under-explored. Few recent lines of work have scratched the surface by studying pre-trained LMs' (e.g., BERT) capability in answering relational questions through \"fill-in-the-blank\" cloze statements (e.g., [Dante was born in MASK]). BERT predicts the MASK-ed word with a list of words ranked by probability (in this case, BERT successfully predicts Florence with the highest probability). In this paper, we conduct a feasibility study of fine-tuned LMs with a different focus on tracking polls, tracking community perception and mining deeper insights typically obtained through costly surveys. Our main focus is on a substantial corpus of video comments extracted from YouTube videos (6,182,868 comments on 130,067 videos by 1,518,077 users) posted within 100 days prior to the 2019 Indian General Election. Using fill-in-the-blank cloze statements against a recent high-performance language modeling algorithm, BERT, we present a novel application of this family of tools that is able to (1) aggregate political sentiment (2) reveal community perception and (3) track evolving national priorities and issues of interest.",
    "full_text": null
}