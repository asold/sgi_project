{
    "title": "Attention-Aligned Transformer for Image Captioning",
    "url": "https://openalex.org/W4283793362",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2994038841",
            "name": "Zhengcong Fei",
            "affiliations": [
                "Institute of Computing Technology",
                "University of Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2994038841",
            "name": "Zhengcong Fei",
            "affiliations": [
                "University of Chinese Academy of Sciences",
                "Institute of Computing Technology",
                "Chinese Academy of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2506483933",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W3185110915",
        "https://openalex.org/W6696904280",
        "https://openalex.org/W3011307745",
        "https://openalex.org/W2550553598",
        "https://openalex.org/W6764724796",
        "https://openalex.org/W2901988662",
        "https://openalex.org/W6771429369",
        "https://openalex.org/W3138763170",
        "https://openalex.org/W6798661948",
        "https://openalex.org/W3101609372",
        "https://openalex.org/W2105767494",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W2971154170",
        "https://openalex.org/W6766818547",
        "https://openalex.org/W3113377113",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W4225528836",
        "https://openalex.org/W6770800577",
        "https://openalex.org/W3099425148",
        "https://openalex.org/W6682631176",
        "https://openalex.org/W6714414533",
        "https://openalex.org/W2779827764",
        "https://openalex.org/W3174556939",
        "https://openalex.org/W3021385998",
        "https://openalex.org/W6775772702",
        "https://openalex.org/W6898505805",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2560313346",
        "https://openalex.org/W2595017202",
        "https://openalex.org/W2934463783",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W6639657675",
        "https://openalex.org/W2946299408",
        "https://openalex.org/W6779821445",
        "https://openalex.org/W1514535095",
        "https://openalex.org/W6802709131",
        "https://openalex.org/W6754778999",
        "https://openalex.org/W6729624334",
        "https://openalex.org/W6698228248",
        "https://openalex.org/W2903268415",
        "https://openalex.org/W3174966920",
        "https://openalex.org/W3159530976",
        "https://openalex.org/W3015071281",
        "https://openalex.org/W3176587734",
        "https://openalex.org/W4289542422",
        "https://openalex.org/W2963630207",
        "https://openalex.org/W1889081078",
        "https://openalex.org/W2552161745",
        "https://openalex.org/W3035281110",
        "https://openalex.org/W3119886052",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W1895577753",
        "https://openalex.org/W3088181395",
        "https://openalex.org/W3035284526",
        "https://openalex.org/W3205981128",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2302086703",
        "https://openalex.org/W2998014937",
        "https://openalex.org/W3035323998",
        "https://openalex.org/W2990818246",
        "https://openalex.org/W4225602969",
        "https://openalex.org/W2986670728",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W4287553517",
        "https://openalex.org/W2481240925",
        "https://openalex.org/W2950768109",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W3034655362",
        "https://openalex.org/W2963084599",
        "https://openalex.org/W3169496116",
        "https://openalex.org/W3038076718",
        "https://openalex.org/W2996677910",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2983256121",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3035517717",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2963798744"
    ],
    "abstract": "Recently, attention-based image captioning models, which are expected to ground correct image regions for proper word generations, have achieved remarkable performance. However, some researchers have argued “deviated focus” problem of existing attention mechanisms in determining the effective and influential image features. In this paper, we present A2 - an attention-aligned Transformer for image captioning, which guides attention learning in a perturbation-based self-supervised manner, without any annotation overhead. Specifically, we add mask operation on image regions through a learnable network to estimate the true function in ultimate description generation. We hypothesize that the necessary image region features, where small disturbance causes an obvious performance degradation, deserve more attention weight. Then, we propose four aligned strategies to use this information to refine attention weight distribution. Under such a pattern, image regions are attended correctly with the output words. Extensive experiments conducted on the MS COCO dataset demonstrate that the proposed A2 Transformer consistently outperforms baselines in both automatic metrics and human evaluation. Trained models and code for reproducing the experiments are publicly available.",
    "full_text": "Attention-Aligned Transformer for Image Captioning\nZhengcong Fei1,2\n1Key Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS),\nInstitute of Computing Technology, CAS, Beijing 100190, China\n2University of Chinese Academy of Sciences, Beijing 100049, China\nfeizhengcong@ict.ac.cn\nAbstract\nRecently, attention-based image captioning models, which\nare expected to ground correct image regions for proper word\ngenerations, have achieved remarkable performance. How-\never, some researchers have argued “deviated focus” problem\nof existing attention mechanisms in determining the effec-\ntive and inﬂuential image features. In this paper, we present\nA2 - an attention-aligned Transformer for image captioning,\nwhich guides attention learning in a perturbation-based self-\nsupervised manner, without any annotation overhead. Specif-\nically, we add mask operation on image regions through a\nlearnable network to estimate the true function in ultimate\ndescription generation. We hypothesize that the necessary im-\nage region features, where small disturbance causes an obvi-\nous performance degradation, deserve more attention weight.\nThen, we propose four aligned strategies to use this infor-\nmation to reﬁne attention weight distribution. Under such a\npattern, image regions are attended correctly with the output\nwords. Extensive experiments conducted on the MS COCO\ndataset demonstrate that the proposed A2 Transformer con-\nsistently outperforms baselines in both automatic metrics and\nhuman evaluation. Trained models and code for reproducing\nthe experiments are publicly available.\n1 Introduction\nThe task of generating a concise textual summary of a given\nimage, known as image captioning, is one of the most chal-\nlenges that require joint vision and language modeling. Cur-\nrently, most image captioning algorithms follow an encoder-\ndecoder paradigm in which an RNN-based decoder network\nis used to predict words according to the image features ex-\ntracted by the CNN-based encoder network (Vinyals et al.\n2015). In particular, the incorporation of attention mecha-\nnisms has greatly advanced the performance of image cap-\ntioning and can be used to provide insights for the inner\nworkings (Xu et al. 2015; Anderson et al. 2018; Huang et al.\n2019; Li et al. 2019; Cornia et al. 2020; Pan et al. 2020). It\ndynamically encodes visual information by weighting more\nthose regions relevant to the current word generation.\nHowever, it is widely questioned whether highly attended\nimage regions have a true correlation on the caption gener-\nation. On the one hand, Serrano and Smith (2019) ﬁnd that\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\na\nbathroom with a\nsink\nand toilet\na\nbathroom with a\nshelf and\ntoilet\nFigure 1: Illustration of the sequence of attended image re-\ngions in generating each word for the description before\n(blue) and after (red) attention alignment. At each time step,\nonly the top-1 attended image region is shown. The original\nattended image regions are grounded less accurately, demon-\nstrating the deﬁciency of previous attention mechanisms.\nerasing the representations accorded high attention weights\ndo not necessarily lead to a signiﬁcant performance decrease\nsometimes. On the other hand, Liu et al. (2020) state that\nmost attention-based image captioning models use the hid-\nden state of the current input to attend to the image regions\nand attention weights are inconsistent with other feature im-\nportance metrics (Selvaraju et al. 2019). It further proves that\nattention mechanisms are incapable of precisely identifying\ndecisive inputs for each prediction (Zhang et al. 2021), also\nreferred to as “deviated focus”, which would impair the per-\nformance of image content description. As show in Figure\n1, at the time step to generate the 5th word, original atten-\ntion mechanisms focus most on the local “shelf” region, as\na result, the incorrect noun “sink” is generated. The unfa-\nvorable attended image region also impairs the grounding\nperformance and ruins the model interpretability (Cornia,\nBaraldi, and Cucchiara 2019).\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n607\nIn this paper, we propose a novel perturbation-based self-\nsupervised attention-aligned method for image captioning,\nreferred to as A2 Transformer, without any additional anno-\ntation overhead. To be speciﬁc, we keep applying mask op-\neration to disturb the original attention weights with a learn-\nable network, and evaluate the ﬁnal performance change\nof image captioning model, so as to discover which input\nimage regions affect the performance of image captioning\nmodel most. In between, we add a regular term, aims to\ndetermine the smallest perturbation extents that cause the\nmost prominent degrading description performance. Under\nthis condition, we can ﬁnd the most informative and nec-\nessary image features for the caption prediction, which de-\nserve more attention. Later, we use this supervised informa-\ntion to reﬁne the attention weight distribution. In particular,\nwe design four fusion methods to incorporate the updated\nattention weights into the original attention weights: i) max\npooling, ii) moving average, iii) exponential decay, and iv)\ngate mechanism. Finally, the image captioning model is op-\ntimized based on the modiﬁed attention.\nIt is notable that the aligned attention method is model-\nagnostic and can be easily incorporated into existing state-\nof-the-art image captioning models to improve their caption-\ning performances. Extensive experiments are conducted to\nverify our method’s effectiveness on the MS COCO dataset.\nAccording to both automatic metrics and human evaluations,\nthe image captioning models equipped with the attention-\naligned method can signiﬁcantly boost performance. More\nintuitive example can be see in Figure 1, at the time step\nto generate the 5th word, our method align the attention\nweight for the new image regionshelf more and the matched\ncorrect word shelf is generated correspondingly. We further\nanalyze the correlation between mask perturbation and fea-\nture importance metrics as well as investigate when attention\nweights need to be corrected for various layers.\nOverall, the contributions of this paper are as follows:\n• We introduce a simple and effective approach to auto-\nmatically evaluate the inﬂuence of image region features\nwith mask operation, and use it as supervised information\nto guide the attention alignment;\n• We design four fusion strategies to force the attention\nweight incorporating supervised information, which can\nbe easily applied into existing models to improve the per-\nformance of captioning;\n• We evaluate attention alignment for image captioning on\nthe MS COCO dataset. The captioning models equipped\nwith our method signiﬁcantly outperform the ones with-\nout it. To improve reproducibility and foster new re-\nsearches in the ﬁeld, we publicly release the source code\nand trained models of all experiments.\n2 Background\nIn this paper, we ﬁrst introduce the basic framework of\nTransformer (Vaswani et al. 2017) for image captioning\nbrieﬂy, which has an encoder-decoder structure with stack-\ning layers of attention blocks. Each attention block con-\ntains multi-head attention (MHA) and feed-forward net-\nworks (FFN). To simplify the optimization, shortcut con-\nFigure 2: The Kendall-\u001c correlation between attention\nweights (\u000b) of image regions and gradient importance met-\nrics (\u001c) of generated words for different attention layers on\nthe MS COCO validation set.\nnection and layer normalization are applied after all the\nMHA and FFN. Generally, given the image region features\nx = {x1;x2;:::;x n}, visual encoder projects them to hid-\nden states h = {h1;h2;:::;h n}in latent space, which fur-\nther feed into the caption decoder to generate the target sen-\ntences y= {y1;y2;:::;y m}.\nMulti-head attention, which serves as the core compo-\nnent of the Transformer, enables each prediction to at-\ntend overall image region features from different repre-\nsentation subspaces jointly. In practice, hidden states h =\n{h1;h2;:::;h n}are projected to keys Kand values V with\nvarious linear projections. To predict the target word, scaled\ndot-product attention (Vaswani et al. 2017) is adopted. That\nis, we ﬁrst linearly project the hidden state of previous cap-\ntion decoder layer to the query Q. Then we multiply Qby\nkeys K to obtain an attention weight, which is further used\nto calculate a sum of values V.\nAttention(Q;K;V ) =softmax(QKT\n√dk\n) ·V; (1)\nwhere dk corresponds to the dimension of the keys, which\nis used as scaling factor. Such attention module learns the\nattended features that consider the pairwise interactions be-\ntween two features. For MHA, the model, contains several\nparallel heads, is allowed to attended to diverse information\nfrom different representation subspaces. For more advanced\nimprovement, such as mesh-like connectivity and memory\nmodule, please refer to (Cornia et al. 2020) in detail. We\nemploy the Transformer of the basic version that performs\nN = 6 attention layers and employs h= 8 parallel attention\nheads for each time.\n3 Is Current Attention Mechanism in Image\nCaptioning Good Enough?\nAttention mechanism plays an essential role in image cap-\ntioning, which provides an important weight for visual fea-\ntures. However, some researchers have found that the highly\nattended image regions exist a “deviated focus” problem\n608\nVisual Encoder\nAttention-Aligned \nNetwork\nCaption\nDecoder\nOriginal\nAttention\nWeights\nMask\nMatrix\nPerturbed\nAttention\nWeights\nAligned\nAttention\nWeights\nBetter  Caption\nAfter Aligned\nW orse  Caption\nAfter Perturbed\nMask Perturbation \nNetwork \nFigure 3: Architecture of the A2 Transformer. Mask pertur-\nbation network is trained to perturb the attention weights of\ndecisive and effective input features to impair the caption-\ning performance. Attention-aligned network targets to look\nfor which input regions are perturbed and enhance the cor-\nresponding attention weights.\n(Liu et al. 2020) that holds low relevant to generated words,\nthus impairs the model performance. To make a deeper\nanalysis about if current attention mechanisms can focus\non the decisive and effective image regions, we evaluate\nthe correlation with attention weights and feature impor-\ntance metrics in image captioning. Practically, we refer (An-\njomshoae, Jiang, and Framling 2021; Clark et al. 2019) to\napply gradient-based methods to evaluate the importance of\neach visual representation, i.e., hidden state hi for the gen-\nerated word yt, which is estimated as \u001cit = |▽hi p(yt|x)|.\nExperimentally, we train a plain Transformer model on\nMS COCO dataset as the baseline. All the structure and pa-\nrameter settings are kept untouched as (Chen et al. 2015).\nWe record the average attention weights of image features\nover various heads, and the Kendall-\u001c correlation between\nattention weights and metrics is presented in Figure 2. We\ncan see that the correlation between attention weights of\nimage features and the corresponding gradient importance\nmetrics is weak, all below 0.2. In between, 0 indicates no\nrelevance, while 1 implies strong concordance. The experi-\nmental results show that the highly-attended image features\nare not always responsible for the word generation, which is\nalso consist with previous studies (Liu et al. 2020).\n4 Methodology\nIn this section, to tackle the inaccurate issue of attention\nweights, we propose a perturbation-based self-supervised\nmethod to enhance the attention learning focused on the ef-\nfective image regions. The basic architecture is shown in\nFigure 3. Firstly, we introduce how to discover the impor-\ntant image regions for caption generation, where we design\na learnable mask perturbation to destroy the description per-\nformance with limited operation on the original attention\nweights. Based on the performance change, we can auto-\nmatically evaluate the image regions most effected. Then,\nwe illustrate how to use the supervised information to re-\nﬁne the original attention weights with attention-aligned net-\nwork. Finally, we describe the entire training and inference\nprocedure in detail.\n4.1 Learnable Mask Perturbation\nThe basic assumption of our design is the fact that under\nthe premise of incorporation the same perturbation, impor-\ntant image regions leads to more performance changes than\nunimportant ones (Li et al. 2021). Speciﬁcally, a little pertur-\nbation on inﬂuential image features can results in a dramatic\nchanges in ﬁnal generated words, while greater perturbation\non the unimportant ones will not easily change the results.\nTherefore, we can estimate the importance of image region\nfeatures by observing how the performance changes as per-\nturbing different parts of the input image features. Inspiring\nfrom (Fong and Vedaldi 2017; Fan et al. 2021), we apply a\nlearnable mask to scale the attention weight of each image\nregion, which simulates the process of perturbation.\nAt the time step to generate t-th word, the learnable mask\noperation mt is obtained based on the hidden state hd\nt from\nthe d-th layer of caption decoder as:\nmt = \u001b(Wm ·hd\nt + bm); (2)\nwhere \u001b(·) is the sigmoid function,Wmand bmare trainable\nparameters vary among different attention layers and heads.\nCorrespondingly, the perturbed attention weight \u000bp\nt can be\nmodeled based on the mask matrix as:\n\u000bp\nt = mt ·\u000bt + (1−mt) ·\n\u000b; (3)\nwhere \u000bis an average vector of attention heads rather than\nzero to avoid the abnormal effect value (Kim et al. 2021).\nQualitatively, a smaller value of mask mt corresponds to a\nsmaller reservation in original attention weight \u000bt, in other\nword, a larger perturbation extent.\nRecalling that the mask operation is targeted to make the\nsmallest perturbation in image region features and achieves\na most extent of performance degrading. Based on this, we\ncan design the training objective of the mask perturbation\nnetwork as follows:\nL(\u0012m) =−LIC(\u000bp\nt;\u0012) +\u0015||1 −mt||2\n2; (4)\nwhere \u0012 denotes the parameters of the original image cap-\ntioning model. LIC(\u000bp\nt;\u0012) is the loss of the image cap-\ntioning model when incorporating the perturbed attention\nweights \u000bp\nt. \u0012m = {Wm;bm}represents the parameters of\nthe mask perturbation network. The second one serves as a\nregular term to punish too much mask operation and\u0015is the\nbalancing factor. As the perturbed attention\u000bp\nt is infected by\n\u0012m, both two term in Equation 4 are parameterized with\u0012m.\nThus, this loss only optimizes the parameter of mask pertur-\nbation network \u0012m without accessing to the original image\ncaptioning model.\n609\n4.2 Attention-Aligned Network\nAccording to the analysis above, our mask perturbation net-\nwork generate feature importance estimation for each word\ngeneration, where the perturbation is quantiﬁed according\nto the mask magnitude. Here, we do not use mask matrix to\ngenerate a new attention distribution to replace the original\nattention weights. Rather, we use it as supervised informa-\ntion. We want the model notices more features that have an\ninﬂuence on output. In this way, some ignored image fea-\ntures with great importance can be discovered by attention\nlearning. In the following, we describe how to exploit the\nmask matrix to guide the alignment of attention.\nAs the mask value closer to 1 means to keep the original\nattention weights and make mask operation less, the can be\ndesigned following (Lu et al. 2021) as:\n\u000bm\nt = \u000bt ·e1−mt: (5)\nIn particular, we design four fusion methods to incorporate\n\u000bm\nt into the original one \u000bt to obtain the ﬁnal aligned atten-\ntion weights \u000ba\nt as follows:\nMax Pooling. The most intuitive idea is to replace the\noriginal ignored attention with newly highlighted ones:\n\u000ba\nt = max(\u000bt;\u000bm\nt ): (6)\nMoving Average. The mask-based attention weights are\nlinearly added to the original attention weights in the entire\nprocess, with a ﬁxed ratio \u0011as:\n\u000ba\nt = \u000bt + \u0011·\u000bm\nt : (7)\nExponential Decay. Inspired by curriculum learning\n(Bengio et al. 2009), we make the inﬂuence of \u000bm\nt to be\nsmaller at the beginning and gradually growing with the\ntraining forwards. For simplicity, we utilize exponential de-\ncay (Zhou, Wang, and Bilmes 2021) to update ratio of \u000bm\nt\nas:\n\u000ba\nt = e− s\nTP ·\u000bt + (1−e− s\nTP ) ·\u000bm\nt ; (8)\nwhere sis the training step and TP is a temperature factor.\nGating Mechanism. We further employ a learnable gate\n(Xu et al. 2019) to dynamically control the extent of the su-\npervised information from the mask perturbation network\ninto the aligned attention.\n\u000ba\nt = gt ·\u000bt + (1−gt) ·\u000bm\nt ; (9)\ngt = \u001b(Wg ·qt + bg); (10)\nwhere Wg and bg are trainable parameters vary among dif-\nferent attention layers and heads, and \u001bcorresponds to sig-\nmoid activation function.\n4.3 Training and Inference\np with the aligned attention weights \u000ba\nt, the image caption-\ning model is ﬁrstly optimized with cross-encropy loss as:\nLIC(\u0012) =−\nmX\nt=1\nlog p(yt|y<t;x; \u000ba\nt;\u0012): (11)\nIn the second stage, the image captioning model is ﬁne-\ntuned with self-critical reinforcement learning strategy fol-\nlowing (Rennie et al. 2017) as:\nLIC(\u0012) =−Ey∼p(y|x;\u000ba\nt ;\u0012)[r(y)]; (12)\nwhere rdenotes the reward function for generated sentence,\ne.g., CIDEr score (Vedantam, Lawrence Zitnick, and Parikh\n2015) in common cases.\nDuring inference procedure, at each time step, given the\nimage and generated sentence, the learned mask perturba-\ntion network can determine the most important image re-\ngions with mask matrix. Then, the attention-aligned network\nfuse the original attention weights and mask matrix, and feed\naligned attention into caption decoder to make the ﬁnal de-\ncision. More encouragingly, the aligned attention can serve\nas an effective visual interpretation (Patro, Namboodiri et al.\n2020) to qualitative measurement of the captioning model.\n5 Experiments\n5.1 Experimental Setup\nDataset. All the experiments are conducted on the most\npopular image captioning dataset MS COCO (Chen et al.\n2015). As the largest English dataset, MS COCO contains\ntotally 164,062 images. Each image is equipped with ﬁve\nhuman-set captions. We follow the common practice as\nKarpathy splits (Karpathy and Fei-Fei 2015) for validation\nof model hyperparameters and ofﬂine evaluation. This split\ncontains 113,287 images for training and 5,000 respectively\nfor validation and test. All the training sentences are pre-\nposed by converting them into lower case and dropping the\nwords that occur rarely as (Huang et al. 2019; Cornia et al.\n2020). We also evaluate the model on the MS COCO online\ntest server, composed of 40,775 images whose annotations\nare not made publicly accessible.\nEvaluation Metrics. We use ﬁve standard automatic eval-\nuation protocol simultaneously, namely BLEU-N (Papineni\net al. 2002), METEOR (Lavie and Agarwal 2007), ROUGE-\nL (Lin 2004), CIDEr (Vedantam, Lawrence Zitnick, and\nParikh 2015), and SPICE (Anderson et al. 2016), and de-\nnoted as B-N, M, R, C and S for simplify. Concretely,\nBLEU-N indicates the n-gram matching, SPICE is based on\nscene graph matching, METEOR measures both the preci-\nsion and recall, and CIDEr considers the n-gram similarity\nwith TF-IDF weights.\nImplement Details. We utilize Faster R-CNN (Ren et al.\n2015) with ResNet-101 (He et al. 2016) to represent im-\nage regions. The feature vector for each region is 2048-\ndimensional. We employ one-hot vectors and linearly\nproject to model input dimensional to represent words. For\nmodel structure, we set the dimensionalitydof each layer to\n512 and the number of heads to 8. We employ a dropout rate\nof 0.1 after each attention and feed-forward layer. Model is\nﬁrst trained to minimize the negative log-likelihood of the\ntraining data following the learning rate scheduling strategy\nwith a warmup equal to 10,00, and then ﬁne-tuned with the\nCIDEr score using Reinforcement Learning (Rennie et al.\n2017) with a ﬁxed learning rate of 5 ×10−6. We train all\n610\nCross-Entropy Loss CIDEr Score Optimization\nB-1 B-4 M R C S B-1 B-4 M R C S\nLSTM-A (Yao et al. 2017) 75.4 35.2 26.9 55.8 108.8 20.0 78.6 35.5 27.3 56.8 118.3 20.8\nUp-Down (Anderson et al. 2018) 77.2 36.2 27.0 56.4 113.5 20.3 79.8 36.3 27.7 56.9 120.1 21.4\nGCN-LSTM (Yao et al. 2018) 77.3 36.8 27.9 57.0 116.3 20.9 80.5 38.2 28.5 58.3 127.6 22.0\nAoANet (Huang et al. 2019) 77.4 37.2 28.4 57.5 119.8 21.3 80.2 38.9 29.2 58.8 129.8 22.4\nM2-T (Cornia et al. 2020) - - - - - - 80.8 39.1 29.2 58.6 131.2 22.6\nDPA (Liu et al. 2020) - - - - - - - 40.5 29.6 59.2 133.4 23.3\nGET (Ji et al. 2021) - - - - - - 81.5 39.5 29.3 58.9 131.6 22.8\nA2 Transformer 78.6 38.2 29.2 58.3 125.0 22.1 81.5 39.8 29.6 59.1 133.9 23.0\nTable 1: Performance of A2 Transformer and other state-of-the-art image captioning models with different evaluation metrics\non the MS COCO Karpathy test set. All values are reported as a percentage (%).\nmodels using the Adam optimizer (Kingma and Ba 2014), a\nbatch size of 50 and a beam size of 5. We set the hyperpa-\nrameter \u0011= 0.1 in Equation 7 in all experiments.\n5.2 Quantitative Analysis\nOfﬂine Evaluation. We compare the performance of our\nattention-aligned approach with those of several recent pro-\nposals for image captioning comprehensively. Speciﬁcally,\nwe report the results of some competitive models including\nM2-T, DPA, and GET on the ofﬂine MS COCO Karpathy\ntest split. For a fair comparison, the results for each run\noptimized with both cross-entropy loss and CIDEr score\nare listed. As presented in Table 1, overall, the proposed\nA2 Transformer exhibits better performance than the above\nmodels in terms of BLEU-4, METEOR, and CIDEr. In par-\nticular, it advances the current state-of-the-art performance\non CIDEr by 0.5. Unlike previous attention learning with-\nout supervised information, our model yields a prominent\nimprovement by searching the effective image regions with\nlearnable mask perturbation and reasonable guiding strat-\negy. More encouragingly, the additional parameter of the\nattention-aligned network is negligible, which will not in-\ncrease much cost to the training and inference process, while\ngain an obvious improvement.\nOnline Evaluation. We also evaluate our best variant A2\nTransformer + Gate Mechanism on the ofﬁcial testing set by\nsubmitting the ensemble versions, i.e., an average ensem-\nble for four checkpoints trained independently, to the online\ntesting server. The results over ofﬁcial testing images with\n5 reference captions (c5) and 40 reference captions (c40)\nof our approach and the top-performing published works on\nthe leaderboard are reported in Table 2. Note that we do not\nlist pre-training model for fair comparison. As it can be ob-\nserved, compared to all other popular systems, A2 Trans-\nformer exhibits better performances across most metrics.\n5.3 Ablation Study\nEffect of Fusion Strategies. As illustrated in Table 3, gen-\nerally, all of four attention updating methods achieve perfor-\nmance boosting. In between, the moving average show the\nminimal lifting capacity. Exponential decay, which increase\nthe inﬂuence of attention-aligned network after it trained\nFigure 4: Evaluation metric CIDEr and the average value of\ngenerated masks with respect to different hyperparameter \u0015\nunder gate mechanism on the MS COCO validation set.\nwell, provides a more stable training process and better per-\nformance. However, the incorporation of hyper-parameter\ntemperature factor Sneeds more heuristics and increase the\nuncertainty. Finally, the gate mechanism adaptively controls\nthe ratio between original and updated attention weights,\nholds more promising for real application.\nEffect of Mask Degree. We also analyze the hyperparam-\neter \u0015in Equation 4, which decides the perturbation degree\nof mask operation to the original attention weights of im-\nage features. We plot the automatic evaluation metric CIDEr\nchanging follows the different of average value of generated\nmasks, by setting different hyperparameter \u0015, in Figure 4.\nAs we can see, with the increase of mask operation degree,\nthe performance of image captioning model ﬁrst rise and\nthen fall down. There are optimal parameter options, i.e.,\n\u0015= 0:035, under the current scene.\nAccuracy of Mask Operation. To demonstrate the cor-\nrectness of mask perturbation m, we calculate its correlation\nwith gradient-based importance measures \u001c (Ross, Hughes,\nand Doshi-Velez 2017), compared with original attention\nweights \u000b, the results are shown in Figure 5. It is evident\nthat the generated masks bought out a signiﬁcant advantage\n611\nB-1 B-2 B-3 B-4 M R C\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nUp-Down 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nAoANet 81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nM2-T 81.6 96.0 66.4 90.8 51.8 82.7 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1\nGET 81.6 96.1 66.5 90.9 51.9 82.8 39.7 72.9 29.4 38.8 59.1 74.4 130.3 132.5\nA2 Transformer 82.2 96.4 67.0 91.5 52.4 83.6 40.2 73.8 29.7 39.3 59.5 75.0 132.4 134.7\nTable 2: Leaderboard of different image captioning models on the online MS COCO test server.\nB-1 B-4 M R C S\nBaseline 77.4 37.2 28.7 57.7 120.0 21.8\nMax Pooling 77.9 37.7 29.0 58.0 122.8 21.9\nMoving Average 77.9 37.6 29.0 57.9 122.3 21.9\nExponential Decay 78.4 37.9 29.0 58.2 124.2 22.0\nGate Mechanism 78.6 38.2 29.2 58.3 125.0 22.1\nTable 3: Effect of fusion methods for combined attention\nweights optimized with cross-entropy loss on validation set.\nFigure 5: The Kendall-\u001c correlation between attention\nweights \u000b and the masks mgenerated by the mask pertur-\nbation model to the gradient importance measures \u001c on MS\nCOCO validation set.\nin determining the important image region features, proving\nthe superiority of our self-supervised perturbation network.\n5.4 Attention Alignment for Different Layers\nWe try to explain how our proposed method helps produce\nbetter descriptions by investigating which attention weights\nneed to update. Speciﬁcally, we dive into the differences be-\ntween layers, which provide insights into the attention mech-\nanism’s inner workings and better understand A2 Trans-\nformer. In practice, we apply Jensen-Shannon Divergence\n(Fuglede and Topsoe 2004) between attention weights be-\nfore and after alignment to measure the changing extent as:\nD(\u000b;\u000ba) =1\n2KL[\u000bjj\u000b+ \u000ba\n2 ] +1\n2KL[\u000bajj\u000b+ \u000ba\n2 ]; (13)\nFigure 6: The JSD between attention weights before and af-\nter alignment at different layers on MS COCO validation set.\nIntuitively, a high JSD score indicates the aligned attention\nweights are distant from original ones.\nConcerning the roles of different attention layers, one nat-\nural question is which attention layers are not well-trained\nin the original image captioning model and have an urgent\nneed to be improved and aligned. Figure 6 depicts the JSD\nbetween original and aligned attention weights. We can dis-\ncover that: i) each attention layer holds an attention change\nto some degree, ii) high JSD for high layers and low JSD\nfor low layers. These ﬁndings prove that different attention\nlayer plays a different role in the image caption generation\nprocess. The low layers generally grasp information from\nvarious inputs, while the high layers look for some particu-\nlar elements tied to the ﬁnal caption generations.\n5.5 Qualitative Analysis\nFigure 7 showcases several image captioning results from\nplain Transformer and our A2 Transformer with gate mech-\nanism, as well as the human-annotated ground truth sen-\ntences (GT). Generally, compared with the captions of plain\nTransformer, which are somewhat relevant to image content\nand logically correct, our attention-aligned method produces\nmore accurate and richer descriptive sentences by exploiting\naccurate attention weights for image regions. For example,\nplain Transformer generates the phrase wooden bowlwhich\nis inconsistent with the visual content for the second image,\nwhile the words large potin our attention-aligned model de-\npicts more precisely. This again conﬁrms the advantage of\n612\nGT: a bird in a pot eating a fruit\nTransformer: a black bird sitting in \na wooden bowl \nA2 Transformer: a black bird \neating in a large pot\nGT: two giraffes and another \nanimal in a field\nTransformer: three giraffes and \nother animals in a field\nA2 Transformer: two giraffes \nand another animal in a field\nGT: a boat with flags and tents is \ndocked next to a grassy bank\nTransformer:a boat with a canopy \non the water\nA2 Transformer: a boat with flags \nsitting on the water\nFigure 7: Case studies of original Transformer, plus our A2\nTransformer with gate mechanism, coupled with the corre-\nsponding ground truth sentences (GT).\nTransformer wins Tie A2 Transformer wins\nNaturalness 25.8 42.0 32.2\nRelevance 26.5 45.5 28.0\nRichness 20.2 41.4 38.4\nTable 4: Results of human evaluation in terms of various\nmetrics. All values are reported as a percentage (%).\ncapturing accurate attention weights when applying the pro-\nposed attention-aligned method.\n5.6 Human Evaluation\nTo better understand the effectiveness of the attention-\naligned approach, we conduct a human evaluation to mea-\nsure the quality of generated captions as (Huang et al.\n2019). We randomly select 400 samples from the MS COCO\ndataset along with human-annotated sentences. We recruit\n8 workers to compare the perceptual quality of the cap-\ntion between our gated A2 Transformer and original Trans-\nformer independently in three aspects: naturalness, which\nindicates the grammaticality and ﬂuency; relevance, which\nindicates the connection with the given image content; rich-\nness, which measures the amount of signiﬁcant information\ncontained in the sentence. The results are shown in Table\n4. We can see that A2 Transformer wins in all metrics than\nthe baseline. In particular, A2 Transformer with gate mech-\nanism achieves more than 18.2 score in richness. This again\nconﬁrms that the superiority of attention alignment.\n6 Related Works\nThe attention mechanism is ﬁrst introduced to augment\nvanilla recurrent network (Bahdanau, Cho, and Bengio\n2014; Luong, Pham, and Manning 2015) in machine transla-\ntion. For image captioning, Xu et al. (2015) ﬁrst introduces\nthe visual attention to help the caption decoder focus on the\nmost relevant image regions instead of the whole image;\nYao et al. (2017) design an adaptive attention module to de-\ncide when to employ the visual attention and Anderson et al.\n(2018) follows a bottom-up and top-down attention mecha-\nnism. Besides, there are numerous other advanced attention\nmechanisms, e.g., spatial and channel-wise attention (Chen\net al. 2017), semantic attention (You et al. 2016) and atten-\ntion on attention Huang et al. (2019). In recent years, plenty\nof Transformer-based architectures (Li et al. 2019; Fei 2019;\nCornia et al. 2020; Pan et al. 2020; Fei 2021; Yan et al.\n2021; Ji et al. 2021) are proposed to replace conventional\nRNN, achieving new state-of-the-art performances. How-\never, as far as we concerned, improving the attention dis-\ntribution with self-supervised mask perturbation has never\nbeen studied in image captioning task, which push forward\nour exploration in this paper.\nOn the other hand, there is plenty of works (Liu et al.\n2017; Zhou et al. 2020) found that adding supervision to\nthe attention model is beneﬁcial for the image captioning\nmodel. Various approaches have been proposed to improve\nattention supervision, e.g., referring expression (Liu, Wang,\nand Yang 2017) grounding visual explanations (Zhou et al.\n2020), sparsity regularization (Zhang et al. 2018), and fu-\nture information (Liu et al. 2020). Unlike them, we never\nintroduce any external knowledge but discover the inﬂuen-\ntial image regions with mask operation. Some works also\naim to employ masks as the analytical tools to indicate the\nimportance (Kitada and Iyatomi 2020; Mohankumar et al.\n2020), attention head (Fong and Vedaldi 2017), or the contri-\nbutions of the pixels in the image to the model outputs (V oita\net al. 2019). Besides, for other tasks,e.g., visual question an-\nswering (Wu and Mooney 2019; Chen et al. 2020), machine\ntranslation (He et al. 2019; Lu et al. 2021), and video under-\nstanding (Li et al. 2021), similar ideas are also proposed to\nincorporate mask operation to localize the regions of interest\nand improve the faithfulness and accuracy of predictions.\n7 Conclusion\nIn this paper, we focus on forcing the image captioning\nmodel to attend the import image regions without any extra\nannotations. To this end, we present A2 Transformer for ef-\nfective attention alignment in image captioning. Speciﬁcally,\na mask perturbation model is applied to automatically dis-\ncover the decisive and effective image region features based\non the description generation changing. Then, we introduce\nfour strategies to combine this supervised information to\nguide the attention alignment. Extensive experimental re-\nsults demonstrate that our approaches consistently achieve\nsigniﬁcant improvements over the state-of-the-art systems.\nMore encouragingly, our work provide valuable reference\non self-supervised learning for improved attention in other\nmulti-modal generation frameworks.\n613\nReferences\nAnderson, P.; Fernando, B.; Johnson, M.; and Gould, S.\n2016. SPICE: Semantic Propositional Image Caption Eval-\nuation. In Proc. ECCV, 382–398.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-Up and Top-Down\nAttention for Image Captioning and Visual Question An-\nswering. In Proc. IEEE CVPR, 6077–6080.\nAnjomshoae, S.; Jiang, L.; and Framling, K. 2021. Vi-\nsual explanations for DNNS with contextual importance. In\nProc. ETAAMS, 83–96. Springer.\nBahdanau, D.; Cho, K.; and Bengio, Y . 2014. Neural ma-\nchine translation by jointly learning to align and translate.\narXiv preprint arXiv:1409.0473.\nBengio, Y .; Louradour, J.; Collobert, R.; and Weston, J.\n2009. Curriculum learning. In Proc. ICML, 41–48.\nChen, L.; Yan, X.; Xiao, J.; Zhang, H.; Pu, S.; and Zhuang,\nY . 2020. Counterfactual samples synthesizing for robust\nvisual question answering. In Proc. IEEE CVPR, 10800–\n10809.\nChen, L.; Zhang, H.; Xiao, J.; Nie, L.; Shao, J.; Liu, W.;\nand Chua, T.-S. 2017. Sca-cnn: Spatial and channel-wise\nattention in convolutional networks for image captioning. In\nProc. IEEE CVPR, 5659–5667.\nChen, X.; Fang, H.; Lin, T.-Y .; Vedantam, R.; Gupta, S.;\nDoll´ar, P.; and Zitnick, C. L. 2015. Microsoft coco cap-\ntions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325.\nClark, K.; Khandelwal, U.; Levy, O.; and Manning, C. D.\n2019. What Does BERT Look at? An Analysis of BERT’s\nAttention. In Proc. ACL Workshop, 276–286.\nCornia, M.; Baraldi, L.; and Cucchiara, R. 2019. Show, con-\ntrol and tell: A framework for generating controllable and\ngrounded captions. In Proc. IEEE CVPR, 8307–8316.\nCornia, M.; Stefanini, M.; Baraldi, L.; and Cucchiara, R.\n2020. Meshed-Memory Transformer for Image Captioning.\nIn Proc. IEEE CVPR, 10578–10587.\nFan, Z.; Gong, Y .; Liu, D.; Wei, Z.; Wang, S.; Jiao, J.; Duan,\nN.; Zhang, R.; and Huang, X.-J. 2021. Mask Attention Net-\nworks: Rethinking and Strengthen Transformer. In Proc.\nNAACL, 1692–1701.\nFei, Z. 2021. Memory-Augmented Image Captioning. In\nProc. AAAI, volume 35, 1317–1324.\nFei, Z.-c. 2019. Fast image caption generation with position\nalignment. arXiv preprint arXiv:1912.06365.\nFong, R. C.; and Vedaldi, A. 2017. Interpretable explana-\ntions of black boxes by meaningful perturbation. In Proc.\nIEEE ICCV, 3429–3437.\nFuglede, B.; and Topsoe, F. 2004. Jensen-Shannon diver-\ngence and Hilbert space embedding. InProc. ISIT, 31. IEEE.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proc. IEEE CVPR, 770–\n778.\nHe, S.; Tu, Z.; Wang, X.; Wang, L.; Lyu, M. R.; and Shi, S.\n2019. Towards Understanding Neural Machine Translation\nwith Word Importance. In Proc. EMNLP.\nHuang, L.; Wang, W.; Chen, J.; and Wei, X.-Y . 2019. Atten-\ntion on attention for image captioning. In Proc. IEEE ICCV,\n4634–4643.\nJi, J.; Luo, Y .; Sun, X.; Chen, F.; Luo, G.; Wu, Y .; Gao, Y .;\nand Ji, R. 2021. Improving image captioning by leverag-\ning intra-and inter-layer global representation in transformer\nnetwork. In Proc. AAAI, volume 35, 1655–1663.\nKarpathy, A.; and Fei-Fei, L. 2015. Deep visual-semantic\nalignments for generating image descriptions. InProc. IEEE\nCVPR, 3128–3137.\nKim, J.; Kim, S.; Kim, S. T.; and Ro, Y . M. 2021. Robust\nPerturbation for Visual Explanation: Cross-checking Mask\nOptimization to Avoid Class Distortion. IEEE Transactions\non Image Processing.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKitada, S.; and Iyatomi, H. 2020. Attention Meets Perturba-\ntions: Robust and Interpretable Attention with Adversarial\nTraining. arXiv e-prints, arXiv–2009.\nLavie, A.; and Agarwal, A. 2007. METEOR: An automatic\nmetric for MT evaluation with high levels of correlation with\nhuman judgments. In Proc. ACL Workshop, 228–231.\nLi, G.; Zhu, L.; Liu, P.; and Yang, Y . 2019. Entangled Trans-\nformer for Image Captioning. In Proc. IEEE ICCV, 8928–\n8937.\nLi, Z.; Wang, W.; Li, Z.; Huang, Y .; and Sato, Y . 2021.\nTowards visually explaining video understanding networks\nwith perturbation. In Proc. IEEE CVPR, 1120–1129.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of summaries. 74–81.\nLiu, C.; Mao, J.; Sha, F.; and Yuille, A. 2017. Attention\ncorrectness in neural image captioning. In Proc. AAAI, vol-\nume 31.\nLiu, F.; Ren, X.; Wu, X.; Ge, S.; Fan, W.; Zou, Y .; and Sun,\nX. 2020. Prophet Attention: Predicting Attention with Fu-\nture Attention for Improved Image Captioning. In Proc.\nNIPS, 1–12.\nLiu, J.; Wang, L.; and Yang, M.-H. 2017. Referring expres-\nsion generation and comprehension via attributes. In Proc.\nIEEE ICCV, 4856–4864.\nLu, Y .; Zeng, J.; Zhang, J.; Wu, S.; and Li, M. 2021. Atten-\ntion Calibration for Transformer in Neural Machine Trans-\nlation. In Proc. ACL, 1288–1298.\nLuong, M.-T.; Pham, H.; and Manning, C. D. 2015. Effec-\ntive approaches to attention-based neural machine transla-\ntion. arXiv preprint arXiv:1508.04025.\nMohankumar, A. K.; Nema, P.; Narasimhan, S.; Khapra,\nM. M.; Srinivasan, B. V .; and Ravindran, B. 2020. Towards\nTransparent and Explainable Attention Models. In Proc.\nACL, 4206–4216.\nPan, Y .; Yao, T.; Li, Y .; and Mei, T. 2020. X-linear atten-\ntion networks for image captioning. In Proc. IEEE CVPR,\n10971–10980.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W. J. 2002.\nBLEU: a Method for Automatic Evaluation of Machine\nTranslation. In Proc. ACL, 311–318.\n614\nPatro, B.; Namboodiri, V .; et al. 2020. Explanation vs atten-\ntion: A two-player game to obtain attention for vqa. InProc.\nAAAI, volume 34, 11848–11855.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-\ncnn: Towards real-time object detection with region proposal\nnetworks. In Proc. NIPS, 91–99.\nRennie, S. J.; Marcheret, E.; Mroueh, Y .; Ross, J.; and Goel,\nV . 2017. Self-Critical Sequence Training for Image Caption-\ning. In Proc. IEEE CVPR, 1179–1195.\nRoss, A. S.; Hughes, M. C.; and Doshi-Velez, F. 2017. Right\nfor the right reasons: training differentiable models by con-\nstraining their explanations. In Proc. IJCAI, 2662–2670.\nSelvaraju, R. R.; Lee, S.; Shen, Y .; Jin, H.; Ghosh, S.; Heck,\nL.; Batra, D.; and Parikh, D. 2019. Taking a hint: Leverag-\ning explanations to make vision and language models more\ngrounded. In Proc. IEEE ICCV, 2591–2600.\nSerrano, S.; and Smith, N. A. 2019. Is Attention Inter-\npretable? In Proc. ACL, 2931–2951.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention Is All You Need. In Proc. NIPS, 5998–6008.\nVedantam, R.; Lawrence Zitnick, C.; and Parikh, D. 2015.\nCider: Consensus-based image description evaluation. In\nProc. IEEE CVPR, 4566–4575.\nVinyals, O.; Toshev, A.; Bengio, S.; and Erhan, D. 2015.\nShow and tell: A neural image caption generator. In Proc.\nIEEE CVPR, 3156–3164.\nV oita, E.; Talbot, D.; Moiseev, F.; Sennrich, R.; and Titov,\nI. 2019. Analyzing multi-head self-attention: Specialized\nheads do the heavy lifting, the rest can be pruned. arXiv\npreprint arXiv:1905.09418.\nWu, J.; and Mooney, R. 2019. Self-Critical Reasoning for\nRobust Visual Question Answering. Proc. NIPS, 32: 8604–\n8614.\nXu, C.; Ji, J.; Zhang, M.; and Zhang, X. 2019. Attention-\ngated LSTM for Image Captioning. In Proc. ICUSAI, 172–\n177. IEEE.\nXu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhutdi-\nnov, R.; Zemel, R.; and Bengio, Y . 2015. Show, Attend and\nTell: Neural Image Caption Generation with Visual Atten-\ntion. In Proc. ICML, 2048–2057.\nYan, X.; Fei, Z.; Li, Z.; Wang, S.; Huang, Q.; and Tian,\nQ. 2021. Semi-Autoregressive Image Captioning. In Proc.\nACM MM, 2708–2716.\nYao, T.; Pan, Y .; Li, Y .; and Mei, T. 2018. Exploring Visual\nRelationship for Image Captioning. In Proc. ECCV, 684–\n699.\nYao, T.; Pan, Y .; Li, Y .; Qiu, Z.; and Mei, T. 2017. Boost-\ning image captioning with attributes. In Proc. IEEE CVPR,\n4894–4902.\nYou, Q.; Jin, H.; Wang, Z.; Fang, C.; and Luo, J. 2016. Image\ncaptioning with semantic attention. In Proc. IEEE CVPR,\n4651–4659.\nZhang, J.; Zhao, Y .; Li, H.; and Zong, C. 2018. Attention\nwith sparsity regularization for neural machine translation\nand summarization. Proc. TASLP, 27(3): 507–518.\nZhang, W.; Shi, H.; Tang, S.; Xiao, J.; Yu, Q.; and Zhuang,\nY . 2021. Consensus graph representation learning for better\ngrounded image captioning. In Proc. AAAI, 3394–3402.\nZhou, T.; Wang, S.; and Bilmes, J. 2021. Curriculum Learn-\ning by Optimizing Learning Dynamics. In Proc. ICAIS,\n433–441. PMLR.\nZhou, Y .; Wang, M.; Liu, D.; Hu, Z.; and Zhang, H. 2020.\nMore grounded image captioning by distilling image-text\nmatching model. In Proc. IEEE CVPR, 4777–4786.\n615"
}