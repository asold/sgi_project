{
  "title": "Natural language processing in the era of large language models",
  "url": "https://openalex.org/W4390790246",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2045838850",
      "name": "Arkaitz Zubiaga",
      "affiliations": [
        "Queen Mary University of London"
      ]
    },
    {
      "id": "https://openalex.org/A2045838850",
      "name": "Arkaitz Zubiaga",
      "affiliations": [
        "Queen Mary University of London",
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4321351832",
    "https://openalex.org/W6846800256",
    "https://openalex.org/W6791599212",
    "https://openalex.org/W4389217180",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W3212567067",
    "https://openalex.org/W6849081288",
    "https://openalex.org/W6783672905",
    "https://openalex.org/W4388788574",
    "https://openalex.org/W6682408610",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6762122294",
    "https://openalex.org/W4386499643",
    "https://openalex.org/W6853697861",
    "https://openalex.org/W4385965989",
    "https://openalex.org/W4221165593",
    "https://openalex.org/W4387058885",
    "https://openalex.org/W4386302153",
    "https://openalex.org/W4386148473",
    "https://openalex.org/W4386081102",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W4294133266",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W3046764764",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W4389524379",
    "https://openalex.org/W3043638540",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W6839151846",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W6773357711",
    "https://openalex.org/W4387985331",
    "https://openalex.org/W4387806231",
    "https://openalex.org/W4387324025",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4389519898",
    "https://openalex.org/W6839112312",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W4387561532",
    "https://openalex.org/W3169973903",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4404783772",
    "https://openalex.org/W4381832461",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2332682252",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4365806374",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W4310419543",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4233907442",
    "https://openalex.org/W4384154918"
  ],
  "abstract": "SPECIALTY GRAND CHALLENGE article Front. Artif. Intell., 12 January 2024Sec. Natural Language Processing Volume 6 - 2023 | https://doi.org/10.3389/frai.2023.1350306",
  "full_text": "TYPE Specialty Grand Challenge\nPUBLISHED /one.tnum/two.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/five.tnum/zero.tnum/three.tnum/zero.tnum/six.tnum\nOPEN ACCESS\nEDITED AND REVIEWED BY\nThomas Hartung,\nJohns Hopkins University, United States\n*CORRESPONDENCE\nArkaitz Zubiaga\na.zubiaga@qmul.ac.uk\nRECEIVED /zero.tnum/five.tnum December /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /three.tnum/one.tnum December /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/two.tnum January /two.tnum/zero.tnum/two.tnum/four.tnum\nCITATION\nZubiaga A (/two.tnum/zero.tnum/two.tnum/four.tnum) Natural language processing\nin the era of large language models.\nFront. Artif. Intell./six.tnum:/one.tnum/three.tnum/five.tnum/zero.tnum/three.tnum/zero.tnum/six.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/five.tnum/zero.tnum/three.tnum/zero.tnum/six.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/four.tnum Zubiaga. This is an open-access article\ndistributed under the terms of theCreative\nCommons Attribution License (CC BY). The use,\ndistribution or reproduction in other forums is\npermitted, provided the original author(s) and\nthe copyright owner(s) are credited and that\nthe original publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nNatural language processing in\nthe era of large language models\nArkaitz Zubiaga*\nSchool of Electronic Engineering and Computer Science, Queen MaryUniversity of London, London,\nUnited Kingdom\nKEYWORDS\nnatural language processing, large language models (LLM), lan guage models (LMs),\nspecialty grand challenge, generative AI\n/one.tnum Overview\nSince their inception in the 1980s, language models (LMs) have been around for more\nthan four decades as a means for statistically modeling the properties observed from natural\nlanguage (\nRosenfeld, 2000). Given a collection of texts as input, a language model computes\nstatistical properties of language from those texts, such as frequencies and probabilities of\nwords and surrounding context, which can then be used for diﬀerent purposes including\nnatural language understanding (NLU), generation (NLG), reasoning (NLR) and, more\nbroadly, processing (NLP) (\nDong et al., 2019 ). Such statistical approach to modeling natural\nlanguage has sparked debate for decades between those who argue that language can be\nmodeled through the observation and probabilistic representation of patterns, and those\nwho argue that such an approach is rudimentary and that proper understanding of language\nneeds grounding in linguistic theories (\nMitchell and Krakauer, 2023 ).\nIt has only been recently that, as a consequence of the increase in the availability of text\ncollections and in the access to improved computational resources, large language models\n(LLMs) have been introduced in the scientiﬁc community by revolutionizing the NLP ﬁeld\n(\nMin et al., 2023 ). Following the same foundational intuition as traditional LMs introduced\nin the 1980s, LLMs scale up the statistical language properties garnered from large text\ncollections. Following the same logic of modeling statistical properties of languages as\ntraditional LMs, researchers have demonstrated that, with today’s computational resources, it\nis possible to train much larger LLMs which are trained from huge collections of text that on\noccasions can even include almost the entire Web. This is however not without controversy,\nnot least because use of such large-scale collections of text prioritizes quantity over quality\n(\nLi et al., 2023a ), as indeed one loses control of what data is being fed into the model when\nthe whole Web is being used, which in addition to valuable information contains oﬀensive\ncontent and misinformation (\nDerczynski et al., 2014 ; Cinelli et al., 2021 ; Yin and Zubiaga,\n2021).\nThe surge of LLMs has been incremental since the late 2010s and has come in waves.\nFollowing a wave that introduced word embedding models such as word2vec ( Mikolov\net al., 2013 ) and GloVe ( Pennington et al., 2014 ) for compact representation of words in\nthe form of embeddings, the ﬁrst major wave came with the emergence of LLMs built on\ntop of the Transformer architecture (\nVaswani et al., 2017 ), including BERT ( Devlin et al.,\n2019), RoBERTa (Liu et al., 2019 ) and T5 ( Raﬀel et al., 2020 ). A more recent wave has led to\na surge of models for generative AI including chatbots like ChatGPT, Google Bard, as well\nas open source alternatives such as LLaMa (\nTouvron et al., 2023 ), Alpaca ( Taori et al., 2023 )\nand Lemur ( Xu et al., 2023 ). These have in turn motivated the creation of diﬀerent ways\nof leveraging these LLMs, including through prompting methods ( Liu et al., 2023 ) such as\nPattern Exploiting Training (PET) ( Schick and Schütze, 2021 ) for few-shot text classiﬁcation\nas well as methods for NLG ( Sarsa et al., 2022 ). An LLM is typically a model which is\npre-trained on existing large-scale datasets, which involves signiﬁcant computational power\nand time, whereas these models can later be ﬁne-tuned to speciﬁc domains with less eﬀort\n(\nBakker et al., 2022 ).\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nZubiaga /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/five.tnum/zero.tnum/three.tnum/zero.tnum/six.tnum\nIn recent years, LLMs have demonstrated to achieve state-\nof-the-art performance across many NLP tasks, having in turn\nbecome the de facto baseline models to be used in many\nexperimental settings (\nMars, 2022). There is however evidence that\nthe power of LLMs can also be leveraged for malicious purposes,\nincluding the use of LLMs to assist with completion of school\nassignments by cheating (\nCotton et al., 2023 ), or to generate content\nthat is oﬀensive or spreads misinformation ( Weidinger et al.,\n2022).\nThe great performance of LLMs has also inevitably provoked\nsome fear in society that artiﬁcial intelligence tools may eventually\ntake up many people’s jobs (\nGeorge et al., 2023 ), hence questioning\nthe ethical implications they may have on society. This has\nin turn sparked research, with recent studies suggesting to\nembrace AI tools as they can in fact support and boost the\nperformance of, rather than replace, human labor (\nNoy and Zhang,\n2023).\n/two.tnum Limitations and open challenges\nThe success of LLMs is not without controversy, which\nis in turn shaping up ongoing research in NLP and opening\nup avenues for more research in improving these LLMs. The\nfollowing are some of the key limitations of LLMs which need\nfurther exploration.\n/two.tnum./one.tnum Black box models\nAfter the release of the ﬁrst major LLM-based chatbot system\nthat garnered mainstream popularity, OpenAI’s ChatGPT, concerns\nemerged around the black box nature of the system. Indeed,\nthere is no publicly available information on how ChatGPT was\nimplemented as well as what data they used for training their\nmodel. From the perspective of NLP researchers, this raises serious\nconcerns about the transparency and reproducibility of such model,\nnot only because one does not know what is going on in the model,\nbut also because it hinders reproducibility (\nBelz et al., 2021 ). If\none runs some experiments using ChatGPT on a particular date,\nthere is no guarantee that somebody else can reproduce those\nresults at a later date (or, arguably, even on the same date), which\nreduces the validity and potential for impact and generalisability of\nChatGPT-based research.\nTo mitigate the impact, and increase our understanding,\nof black box models like ChatGPT, researchers have started\ninvestigating methods for reverse engineering those models, for\nexample by trying to ﬁnd out what data a model may have used\nfor training (\nShi et al., 2023 ).\nLuckily, however, there is a recent surge of open source models\nin the NLP scientiﬁc community, which have led to the release\nof models like Facebook’s LLaMa 2 (\nTouvron et al., 2023 ) and\nStanford’s Alpaca (Taori et al., 2023 ), as well as multilingual models\nlike BLOOM ( Scao et al., 2023 ). Recent studies have also shown that\nthe performance of these open source alternatives is often on par\nwith closed models like ChatGPT (\nChen et al., 2023 ).\n/two.tnum./two.tnum Risk of data contamination\nData contamination occurs when “downstream test sets ﬁnd\ntheir way into the pretrain corpus” ( Magar and Schwartz, 2022 ).\nWhere an LLM trained on large collections of text has already\nseen the data it is then given at test time for evaluation, the\nmodel will then exhibit an impressive yet unrealistic performance\nscore. Research has in fact shown that data contamination can be\nfrequent and have a signiﬁcant impact (\nDeng et al., 2023 ; Golchin\nand Surdeanu, 2023 ). It is therefore crucial that researchers ensure\nthat the test data has not been seen by an LLM before, for a\nfair and realistic evaluation. This is however challenging, if not\nnearly impossible, to ﬁgure out with black box models, which again\nencourages the use of open source, transparent LLMs.\n/two.tnum./three.tnum Bias in LLM models\nThe use of large-scale datasets for training LLMs also means\nthat those datasets are very likely to contain biased or stereotyped\ninformation, which has been shown that LLMs amplify (\nGallegos\net al., 2023 ; Li et al., 2023b ). Research has shown that text\ngenerated by LLMs includes stereotypes against women when\nwriting reference letters (\nWan et al., 2023 ), suggesting that LLMs\nin fact amplify gender biases inherent in the training data leading\nto an increased probability of stereotypical linking between gender\ngroups and professions (\nKotek et al., 2023 ). Another recent study\n(Navigli et al., 2023 ) has also shown that LLMs exhibit biases\nagainst numerous demographic characteristics, including gender,\nage, sexual orientation, physical appearance, disability or race,\namong others.\n/two.tnum./four.tnum Generation of oﬀensive content\nBiases inherent in LLMs are at times exacerbated to even\ngenerate content that can be deemed oﬀensive (\nWeidinger et al.,\n2021). Research in this direction is looking at how to best curate the\ntraining data fed to LLMs to avoid learning oﬀensive samples, as\nwell as in eliciting generation of those harmful texts to understand\ntheir origin (\nSrivastava et al., 2023 ). This research is highly linked\nwith the point above on bias and fairness in LLMs, and therefore\nboth could be studied jointly by looking at the reduction of biases\nand harm.\nSome systems, such as OpenAI’s ChatGPT, acknowledge the\nrisk of producing oﬀensive content in their terms of service\n/one.tnum:\n“Our Services may provide incomplete, incorrect, or\noﬀensive Output that does not represent OpenAIs views. If\nOutput references any third party products or services, it\ndoesnt mean the third party endorses or is aﬃliated with\nOpenAI.”\n/one.tnumhttps://openai.com/policies/terms-of-use\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nZubiaga /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/five.tnum/zero.tnum/three.tnum/zero.tnum/six.tnum\n/two.tnum./five.tnum Privacy\nLLMs can also capture sensitive information retrieved from its\ntraining data. While this information is encoded in embeddings\nwhich are not human readable, it has been found (\nPan et al.,\n2020) that an adversarial user can reverse engineer those\nembeddings to recover the sensitive information, which can\nhave damaging consequences for the relevant individuals. While\nresearch investigating these vulnerabilities of LLMs is still in its\ninfancy, there is awareness of the urgency of such research to make\nLLMs robust to privacy attacks (\nGuo et al., 2022 ; Rigaki and Garcia,\n2023; Shayegani et al., 2023 ).\n/two.tnum./six.tnum Imperfect accuracy\nDespite initial impressions that LLMs achieve an impressive\nperformance, a closer look and investigation into model outputs\nshows that there is signiﬁcant room for improvement. Evaluation\nof LLMs has in turn become a fertile area of research (\nChang et al.,\n2023).\nAware of the many shortcomings and inaccurate outputs of\nLLMs, companies responsible for the production and publication\nof major LLMs all have disclaimers about the limitations of their\nmodels. For example, ChatGPT owner OpenAI acknowledges that:\n“Output may not always be accurate. You should not rely\non Output from our Services as a sole source of truth or factual\ninformation, or as a substitute for professional advice.\"\nGoogle also warns /two.tnumabout the limitations of its LLM-based\nchatbot Bard, as follows:\n“Bard is an experimental technology and may sometimes\ngive inaccurate or inappropriate information that doesnt\nrepresent Googles views.”\n“Dont rely on Bards responses as medical, legal, ﬁnancial,\nor other professional advice.”\nFacebook also has a similar disclaimer /three.tnumfor its ﬂagship model\nLLaMa 2:\n“Llama 2s potential outputs cannot be predicted in\nadvance, and the model may in some instances produce\ninaccurate, biased or other objectionable responses to user\nprompts. Therefore, before deploying any applications of\nLlama 2, developers should perform safety testing and tuning\ntailored to their speciﬁc applications of the model.”\n/two.tnum./seven.tnum Model hallucination\nResponses and outputs generated by LLMs often deviate from\ncommon sense, where for example a generated text can start\n/two.tnumhttps://support.google.com/bard/answer//one.tnum/three.tnum/five.tnum/nine.tnum/four.tnum/nine.tnum/six.tnum/one.tnum?hl=en\n/three.tnumhttps://github.com/facebookresearch/llama/blob/main/MODEL_CARD.\nmd\ndiscussing a particular topic, then shifting to another unrelated\ntopic which is not intuitive, or even stating wrong facts. LLM\nhallucination has been deﬁned as “the generation of content\nthat deviates from the real facts, resulting in unfaithful outputs\"\n(\nMaynez et al., 2020 ; Rawte et al., 2023 ). Eﬀorts toward better\nunderstanding model hallucination is focusing on diﬀerent tasks,\nincluding detection, explanation, and mitigation (\nAlkaissi and\nMcFarlane, 2023 ; Zhang et al., 2023 ), with some initial solutions\nproposed to date, such as Retrieval-Augmented Generation (RAG)\n(\nLewis et al., 2020 ).\n/two.tnum./eight.tnum Lack of explainability\nThe complexity of LLM models means that it is often very\ndiﬃcult to understand why it makes certain predictions or produces\ncertain outputs. This also means that it is very diﬃcult to provide\nexplanations on model outputs to system users, which calls for\nmore investigation into furthering the explainability of LLMs\n(\nDanilevsky et al., 2020 ; Gurrapu et al., 2023 ; Zhao et al., 2023 ).\n/three.tnum Concluding remarks\nThe introduction and surge in popularity of LLMs has impacted\nand reshaped NLP research. Much of the NLP research and\nmethods slightly over a decade ago focused on the representation\nof words using bag-of-words and TF-IDF based methods and the\nuse of machine learning algorithms such as Logistic Regression or\nSupport Vector Machine classiﬁers. The increase in computational\ncapacity to handle large-scale datasets and for more complex\ncomputing has led to the renaissance of deep learning models\nand in turn the emergence of LLMs. The latter have shown to\nachieve unprecedented performance across a range of downstream\nNLP tasks, but have also opened up numerous avenues for future\nresearch aiming to tackle the limitations and weaknesses of LLMs.\nMuch of this research will need to deal with the better curation\nof the data fed to train LLMs, which in the current circumstances\nhas shown to have severe risks in aspects such as fairness, privacy\nand harm.\nAuthor contributions\nAZ: Writing – original draft, Writing – review & editing.\nFunding\nThe author(s) declare that no ﬁnancial support was received for\nthe research, authorship, and/or publication of this article.\nConﬂict of interest\nThe author declares that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nThe author(s) declared that they were an editorial board\nmember of Frontiers, at the time of submission. This had no impact\non the peer review process and the ﬁnal decision.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nZubiaga /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/five.tnum/zero.tnum/three.tnum/zero.tnum/six.tnum\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAlkaissi, H., and McFarlane, S. I. (2023). Artiﬁcial hallucinat ions in chatgpt:\nimplications in scientiﬁc writing. Cureus 15, 2. doi: 10.7759/cureus.35179\nBakker, M., Chadwick, M., Sheahan, H., Tessler, M., Campbell-Gilli ngham, L.,\nBalaguer, J., et al. (2022). Fine-tuning language models to ﬁnd a greement among\nhumans with diverse preferences. Adv. Neural Inform. Proc. Syst . 35, 38176–38189.\nBelz, A., Agarwal, S., Shimorina, A., and Reiter, E. (2021). “A systematic review\nof reproducibility research in natural language processing, ” i n Proceedings of the 16th\nConference of the European Chapter of the Association for Computational L inguistics:\nMain Volume. Kerrville, TX: Association for Computational Linguistics, 3 81–393.\nChen, H., Jiao, F., Li, X., Qin, C., Ravaut, M., Zhao, R., et al. (2023). Chatgpt’s\none-year anniversary: are open-source large language models ca tching up? arXiv.\ndoi: 10.48550/arXiv.2311.16989\nChang, Y., Wang, X., Wang, J., Wu, Y., Zhu, K., Chen, H., et al. ( 2023). A survey on\nevaluation of large language models. arXiv. doi: 10.48550/arXiv.2307.03109\nCinelli, M., Pelicon, A., Mozeti ˇc, I., Quattrociocchi, W., Novak, P. K., and Zollo,\nF. (2021). Dynamics of online hate and misinformation. Scient.Rep. 11, 22083.\ndoi: 10.1038/s41598-021-01487-w\nCotton, D. R., Cotton, P. A., and Shipway, J. R. (2023). “Chatt ing and cheating:\nEnsuring academic integrity in the era of chatgpt, ” in Innovations in Education and\nTeaching International (Oxfordshire: Routledge), 1–12.\nDanilevsky, M., Qian, K., Aharonov, R., Katsis, Y., Kawas, B. , and Sen, P. (2020).\n“A survey of the state of explainable ai for natural language proce ssing, ” inProceedings\nof the 1st Conference of the Asia-Paciﬁc Chapter of the Association for Co mputational\nLinguistics and the 10th International Joint Conference on Natural La nguage Processing\n(Association for Computational Linguistics), 447–459.\nDeng, C., Zhao, Y., Tang, X., Gerstein, M., and Cohan, A. (2023 ). Investigating\ndata contamination in modern benchmarks for large language mo dels. arXiv.\ndoi: 10.48550/arXiv.2311.09783\nDerczynski, L., Bontcheva, K., Lukasik, M., Declerck, T., Sc harl, A., Georgiev, G.,\net al. (2014). “Pheme: computing veracity: the fourth challenge of big social data, ” in\nProceedings of ESWC EU Project Networking (Vienna: Semantic Technology Institute\nInternational).\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). “ Bert: Pre-training\nof deep bidirectional transformers for language understand ing, ” inProceedings of the\n2019 Conference of the North American Chapter of the Association for Comp utational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Pa pers),\nKerrville, TX: Association for Computational Linguistics, 41 71–4186.\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y., et al. ( 2019). “Uniﬁed\nlanguage model pre-training for natural language understandi ng and generation, ” in\nAdvances in Neural Information Processing Systems (Red Hook, NY: Curran Associates,\nInc.), 32.\nGallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S. , Dernoncourt,\nF., et al. (2023). Bias and fairness in large language models: a su rvey. arXiv.\ndoi: 10.48550/arXiv.2309.00770\nGeorge, A. S., George, A. H., and Martin, A. G. (2023). Chatgpt and the future\nof work: a comprehensive analysis of ai’s impact on jobs and employ ment. Partners\nUniversal Int. Innovat. J . 1, 154–186.\nGolchin, S., and Surdeanu, M. (2023). Time travel in llms: tracin g data\ncontamination in large language models. arXiv. doi: 10.48550/arXiv.2308.08493\nGuo, S., Xie, C., Li, J., Lyu, L., and Zhang, T. (2022). Threats to pre-trained language\nmodels: Survey and taxonomy. arXiv. doi: 10.48550/arXiv.2202.06862\nGurrapu, S., Kulkarni, A., Huang, L., Lourentzou, I., and Batar seh, F. A. (2023).\nRationalization for explainable nlp: a survey. Front. Artif. Intellig . 6, 1225093.\ndoi: 10.3389/frai.2023.1225093\nKotek, H., Dockum, R., and Sun, D. (2023). “Gender bias and ste reotypes in large\nlanguage models, ” in Proceedings of The ACM Collective Intelligence Conference (New\nYork, NY: Association for Computing Machinery), 12–24.\nLi, M., Zhang, Y., Li, Z., Chen, J., Chen, L., Cheng, N., et al. ( 2023a). From quantity\nto quality: boosting llm performance with self-guided data selecti on for instruction\ntuning. arXiv. doi: 10.48550/arXiv.2308.12032\nLi, Y., Du, M., Song, R., Wang, X., and Wang, Y. (2023b). A surv ey on fairness in\nlarge language models. arXiv. doi: 10.48550/arXiv.2308.10149\nLewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., et al.\n(2020). Retrieval-augmented generation for knowledge-inte nsive nlp tasks. Adv. Neural\nInform. Proc.Syst. 33, 9459–9474.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., et al. (20 19). Roberta: a\nrobustly optimized bert pretraining approach. arXiv. doi: 10.48550/arXiv.1907.11692\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig, G. (2023).\nPre-train, prompt, and predict: a systematic survey of prompting methods in\nnatural language processing. ACM Comput. Surv . 55, 1–35. doi: 10.1145/35\n60815\nMagar, I., and Schwartz, R. (2022). “Data contamination: Fr om memorization\nto exploitation, ” in Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers) Kerrville, TX: Association for\nComputational Linguistics, 157–165.\nMars, M. (2022). From word embeddings to pre-trained language models:\na state-of-the-art walkthrough. Appl. Sci . 12, 8805. doi: 10.3390/app121\n78805\nMaynez, J., Narayan, S., Bohnet, B., and McDonald, R. (2020). “On faithfulness\nand factuality in abstractive summarization, ” in Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics . Kerrville, TX: Association for\nComputational Linguistics, 1906.\nMikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Eﬃcien t estimation of word\nrepresentations in vector space. arXiv. doi: 10.48550/arXiv.1301.3781\nMin, B., Ross, H., Sulem, E., Veyseh, A. P. B., Nguyen, T. H., Sa inz, O., et al. (2023).\nRecent advances in natural language processing via large pre-tr ained language models:\na survey. ACM Computing Surveys 56, 1–40. doi: 10.1145/3605943\nMitchell, M., and Krakauer, D. C. (2023). The debate over under standing\nin ais large language models. Proc. National Acad. Sci . 120, e2215907120.\ndoi: 10.1073/pnas.2215907120\nNavigli, R., Conia, S., and Ross, B. (2023). Biases in large lang uage models: origins,\ninventory and discussion. ACM J. Data Inform. Qual . 15, 1–21. doi: 10.1145/3597307\nNoy, S., and Zhang, W. (2023). Experimental Evidence on the Productivity Eﬀects of\nGenerative Artiﬁcial Intelligence . Amsterdam: Elsevier Inc.\nPan, X., Zhang, M., Ji, S., and Yang, M. (2020). “Privacy risk s of general-purpose\nlanguage models, ” in2020 IEEE Symposium on Security and Privacy (SP) . San Francisco,\nCA: IEEE, 1314-1331.\nPennington, J., Socher, R., and Manning, C. D. (2014). “Glove: Global vectors for\nword representation, ” in Proceedings of the 2014 Conference on Empirical Methods in\nNatural Language Processing (EMNLP) . Stanford, CA: Stanford University, 1532–1543.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Mat ena, M., et al. (2020).\nExploring the limits of transfer learning with a uniﬁed text-to- text transformer. J. Mach.\nLearn. Res. 21, 5485–5551.\nRawte, V., Chakraborty, S., Pathak, A., Sarkar, A., Tonmoy, S., Chadha,\nA., et al. (2023). The troubling emergence of hallucination in lar ge language\nmodels-an extensive deﬁnition, quantiﬁcation, and prescripti ve remediations. arXiv.\ndoi: 10.18653/v1/2023.emnlp-main.155\nRigaki, M., and Garcia, S. (2023). A survey of privacy attacks in machine learning.\nACM Comp. Surv . 56, 1–34. doi: 10.1145/3624010\nRosenfeld, R. (2000). Two decades of statistical language mod eling: where do we go\nfrom here? Proc. IEEE 88, 1270–1278. doi: 10.1109/5.880083\nSarsa, S., Denny, P., Hellas, A., and Leinonen, J. (2022). “Aut omatic generation\nof programming exercises and code explanations using large langu age models, ” in\nProceedings of the 2022 ACM Conference on International Computing Educatio n\nResearch (New York, NY: Association for Computing Machinery), 27–43.\ndoi: 10.1145/3501385.3543957\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili, S., Hesslow, D. , et al.\n(2023). Bloom: A 176b-parameter open-access multilingual langua ge model. arXiv.\ndoi: 10.48550/arXiv.2211.05100\nSchick, T., and Schütze, H. (2021). “Exploiting cloze-question s for few-shot text\nclassiﬁcation and natural language inference, ” in Proceedings of the 16th Conference of\nthe European Chapter of the Association for Computational Linguisti cs: Main Volume\n(Association for Computational Linguistics), 255–269.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nZubiaga /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/three.tnum/five.tnum/zero.tnum/three.tnum/zero.tnum/six.tnum\nShi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., et al. (2023). Detecting\npretraining data from large language models. arXiv. doi: 10.48550/arXiv.2310.16789\nShayegani, E., Mamun, M. A. A., Fu, Y., Zaree, P., Dong, Y., an d Abu-Ghazaleh,\nN. (2023). Survey of vulnerabilities in large language models rev ealed by adversarial\nattacks. arXiv. doi: 10.48550/arXiv.2310.10844\nSrivastava, A., Ahuja, R., and Mukku, R. (2023). No oﬀense ta ken: eliciting\noﬀensiveness from language models. arXiv. doi: 10.48550/arXiv.2310.00892\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestri n, C., et al. (2023).\nStanford Alpaca: An Instruction-Following Llama Model . Available online at: https://\ngithub.com/tatsu-lab/stanford_alpaca (accessed December 1, 2023).\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M .-A., Lacroix,\nT., et al. (2023). Llama: Open and eﬃcient foundation language mo dels. arXiv.\ndoi: 10.48550/arXiv.2302.13971\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems (Red Hook, NY: Curran Associates, Inc.), 30.\nWan, Y., Pu, G., Sun, J., Garimella, A., Chang, K.-W., and Peng, N . (2023). \" kelly is a\nwarm person, joseph is a role model\": Gender biases in llm-generated reference letters.\narXiv. doi: 10.18653/v1/2023.ﬁndings-emnlp.243\nWeidinger, L., Uesato, J., Rauh, M., Griﬃn, C., Huang, P.-S., Mellor, J.,\net al. (2022). “Taxonomy of risks posed by language models, ” in Proceedings of\nthe 2022 ACM Conference on Fairness, Accountability, and Transparency . New\nYork: Association for Computing Machinery, 214–229. doi: 10 .1145/3531146.35\n33088\nWeidinger, L., Mellor, J., Rauh, M., Griﬃn, C., Uesato, J., Huan g, P.-S.,\net al. (2021). Ethical and social risks of harm from language mo dels. arXiv.\ndoi: 10.48550/arXiv.2112.04359\nXu, Y., Su, H., Xing, C., Mi, B., Liu, Q., Shi, W., et al. (2023). Lemur: Harmonizing\nnatural language and code for language agents. arXiv. doi: 10.48550/arXiv.2310.\n06830\nYin, W., and Zubiaga, A. (2021). Towards generalisable hate spe ech detection: a\nreview on obstacles and solutions. PeerJ Comp. Sci . 7, e598. doi: 10.7717/peerj-cs.598\nZhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., et al. (2023 ). Siren’s\nsong in the ai ocean: a survey on hallucination in large language m odels. arXiv.\ndoi: 10.48550/arXiv.2309.01219\nZhao, H., Chen, H., Yang, F., Liu, N., Deng, H., Cai, H., et al. ( 2023).\nExplainability for large language models: a survey. arXiv. doi: 10.1145/36\n39372\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org",
  "topic": "Volume (thermodynamics)",
  "concepts": [
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4744490385055542
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.41756346821784973
    },
    {
      "name": "Computer science",
      "score": 0.4053579270839691
    },
    {
      "name": "History",
      "score": 0.2630578875541687
    },
    {
      "name": "Archaeology",
      "score": 0.16420289874076843
    },
    {
      "name": "Physics",
      "score": 0.06886392831802368
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}