{
    "title": "LMGFuse: Language Models and Graph reasoning Fuse deeply for question answering",
    "url": "https://openalex.org/W4386365737",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2563619508",
            "name": "Aoxing Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2135836475",
            "name": "Pengfei Duan",
            "affiliations": [
                "Wuhan University of Technology",
                "Sanya University"
            ]
        },
        {
            "id": "https://openalex.org/A2163717463",
            "name": "Yongbing Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2141402632",
            "name": "Wenyan Hu",
            "affiliations": [
                "Wuhan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2136140329",
            "name": "Shengwu Xiong",
            "affiliations": [
                "Wuhan University of Technology",
                "Sanya University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2950339735",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W3020908159",
        "https://openalex.org/W6747227044",
        "https://openalex.org/W4221141423",
        "https://openalex.org/W2994695355",
        "https://openalex.org/W3167967639",
        "https://openalex.org/W3094587024",
        "https://openalex.org/W2971986145",
        "https://openalex.org/W3152801999",
        "https://openalex.org/W4200629408",
        "https://openalex.org/W3021649351",
        "https://openalex.org/W4226281578",
        "https://openalex.org/W2898695519",
        "https://openalex.org/W2890894339",
        "https://openalex.org/W3088056511",
        "https://openalex.org/W2766317792",
        "https://openalex.org/W3156366114",
        "https://openalex.org/W2892167328",
        "https://openalex.org/W2972851234",
        "https://openalex.org/W3093166897",
        "https://openalex.org/W3021578384",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W3094258447",
        "https://openalex.org/W2604314403",
        "https://openalex.org/W3167128508",
        "https://openalex.org/W3105082862",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3097986428",
        "https://openalex.org/W4225831764",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W2963895422",
        "https://openalex.org/W2998374885",
        "https://openalex.org/W3172335055",
        "https://openalex.org/W3174464510",
        "https://openalex.org/W4287555517",
        "https://openalex.org/W3164540570",
        "https://openalex.org/W4297730339",
        "https://openalex.org/W3005977214",
        "https://openalex.org/W2983995706",
        "https://openalex.org/W3186799149"
    ],
    "abstract": "The combination of pre-trained language models (LM) and knowledge graphs (KG) can enhance the reasoning ability for Question Answering.However, previous methods typically fuse the two modalities in a shallow or knowledgedraining manner, not taking full advantage of the knowledge representation of both.How to effectively fuse the different knowledge representations is still a problem of current research.In our work, a novel model is proposed that fuses LM modal knowledge representations and graph neural network (GNN) modal knowledge representations deeply over multiple layers of modality interaction operations.Specifically, the model includes an information interaction unit, through which KG and LM knowledge can be transferred between modalities to realize knowledge fusion directly, reducing information loss.In addition, we add the context node of implicit knowledge from LM encoding in the construction of the reasoning subgraph in advance for enhancing the reasoning of the GNN.We evaluate our model on two domains in the biomedical benchmark (MedQA-USMLE) and commonsense benchmarks (OpenBookQA and CommonsenseQA).Experimental results show that our model achieves a particular improvement over existing LM and LM+KG models for reasoning over both situational constraints and structured knowledge.",
    "full_text": null
}