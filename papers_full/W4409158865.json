{
    "title": "Difficulty aware programming knowledge tracing via large language models",
    "url": "https://openalex.org/W4409158865",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2101737821",
            "name": "Lina Yang",
            "affiliations": [
                "Liupanshui Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2333737854",
            "name": "Xinjie Sun",
            "affiliations": [
                "Liupanshui Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A1977052956",
            "name": "Hui Li",
            "affiliations": [
                "Liupanshui Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2066410461",
            "name": "Ran Xu",
            "affiliations": [
                "Liupanshui Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2504151197",
            "name": "Xuqin Wei",
            "affiliations": [
                "Liupanshui Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2101737821",
            "name": "Lina Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2333737854",
            "name": "Xinjie Sun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1977052956",
            "name": "Hui Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2066410461",
            "name": "Ran Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2504151197",
            "name": "Xuqin Wei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1966976587",
        "https://openalex.org/W6621483976",
        "https://openalex.org/W4394585879",
        "https://openalex.org/W4387675830",
        "https://openalex.org/W1444168786",
        "https://openalex.org/W3008590006",
        "https://openalex.org/W2890071599",
        "https://openalex.org/W4401945736",
        "https://openalex.org/W2955931418",
        "https://openalex.org/W2559094423",
        "https://openalex.org/W3043869244",
        "https://openalex.org/W3128513560",
        "https://openalex.org/W2980472839",
        "https://openalex.org/W4406137557",
        "https://openalex.org/W3030901202",
        "https://openalex.org/W4319587109",
        "https://openalex.org/W6606871998",
        "https://openalex.org/W4402318313",
        "https://openalex.org/W4406549922",
        "https://openalex.org/W6601462914",
        "https://openalex.org/W4367678106",
        "https://openalex.org/W3165215982",
        "https://openalex.org/W2615786590",
        "https://openalex.org/W1562092080",
        "https://openalex.org/W4386432034",
        "https://openalex.org/W2766505560",
        "https://openalex.org/W2966684417",
        "https://openalex.org/W3034854756",
        "https://openalex.org/W4401212963",
        "https://openalex.org/W3013431833",
        "https://openalex.org/W4396833177",
        "https://openalex.org/W6600855501",
        "https://openalex.org/W4394745423",
        "https://openalex.org/W6601065604",
        "https://openalex.org/W4403578219",
        "https://openalex.org/W4400267690",
        "https://openalex.org/W4391584367",
        "https://openalex.org/W4391117134",
        "https://openalex.org/W4398201630",
        "https://openalex.org/W1483408659",
        "https://openalex.org/W4284683483",
        "https://openalex.org/W4225505354",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W6602461063",
        "https://openalex.org/W2805035427",
        "https://openalex.org/W4382318135",
        "https://openalex.org/W4390587019",
        "https://openalex.org/W4394687220",
        "https://openalex.org/W4252599113",
        "https://openalex.org/W4294214983"
    ],
    "abstract": "Knowledge Tracing (KT) assesses students' mastery of specific knowledge concepts and predicts their problem-solving abilities by analyzing their interactions with intelligent tutoring systems. Although recent years have seen significant improvements in tracking accuracy with the introduction of deep learning and graph neural network techniques, existing research has not sufficiently focused on the impact of difficulty on knowledge state. The text understanding difficulty and knowledge concept difficulty of programming problems are crucial for students' responses; thus, accurately assessing these two types of difficulty and applying them to knowledge state prediction is a key challenge. To address this challenge, we propose a Difficulty aware Programming Knowledge Tracing via Large Language Models(DPKT) to extract the text understanding difficulty and knowledge concept difficulty of programming problems. Specifically, we analyze the relationship between knowledge concept difficulty and text understanding difficulty using an attention mechanism, allowing for dynamic updates to students' s. This model combines an update gate mechanism with a graph attention network, significantly improving the assessment accuracy of programming problem difficulty and the spatiotemporal reflection capability of knowledge state. Experimental results demonstrate that this model performs excellently across various language datasets, validating its application value in programming education. This model provides an innovative solution for programming knowledge tracing and offers educators a powerful tool to promote personalized learning.",
    "full_text": "Difficulty aware programming \nknowledge tracing via large \nlanguage models\nLina Yang, Xinjie Sun, Hui Li, Ran Xu & Xuqin Wei\nKnowledge Tracing (KT) assesses students’ mastery of specific knowledge concepts and predicts their \nproblem-solving abilities by analyzing their interactions with intelligent tutoring systems. Although \nrecent years have seen significant improvements in tracking accuracy with the introduction of deep \nlearning and graph neural network techniques, existing research has not sufficiently focused on the \nimpact of difficulty on knowledge state. The text understanding difficulty and knowledge concept \ndifficulty of programming problems are crucial for students’ responses; thus, accurately assessing \nthese two types of difficulty and applying them to knowledge state prediction is a key challenge. \nTo address this challenge, we propose a Difficulty aware Programming Knowledge Tracing via Large \nLanguage Models(DPKT) to extract the text understanding difficulty and knowledge concept difficulty \nof programming problems. Specifically, we analyze the relationship between knowledge concept \ndifficulty and text understanding difficulty using an attention mechanism, allowing for dynamic \nupdates to students’ s. This model combines an update gate mechanism with a graph attention \nnetwork, significantly improving the assessment accuracy of programming problem difficulty and \nthe spatiotemporal reflection capability of knowledge state. Experimental results demonstrate that \nthis model performs excellently across various language datasets, validating its application value in \nprogramming education. This model provides an innovative solution for programming knowledge \ntracing and offers educators a powerful tool to promote personalized learning.\nKnowledge Tracing (KT) aims to model students’ knowledge mastery states by analyzing their interaction data \nwith Intelligent Tutoring Systems (ITS), automatically assessing their mastery of specific knowledge concepts, \nand predicting whether they can successfully solve related problems in their next attempts 1,2. In programming \neducation, knowledge tracing is particularly important; effective knowledge tracing not only helps educators \naccurately assess students’ programming abilities but also provides data support for personalized learning 3,4. \nThrough dynamic monitoring of students’ knowledge state, educators can develop more personalized teaching \nstrategies to meet the diverse learning needs of different students5.\nIn recent years, the field of knowledge tracing has developed various innovative models by introducing \ntechnologies such as deep learning, graph neural networks, and personalized learning 2,6. These include deep \nlearning models1,7, memory network models8,9, graph neural network models10–12, and probabilistic models13,14, \nwhich have significantly improved tracking accuracy and the effectiveness of personalized learning. In the field \nof programming education, knowledge tracing technology also plays a crucial role. For example, the PDKT \nmodel constructs a bipartite graph to embed programming problems and combines the improved pre-trained \nmodel PLCodeBERT for code embedding and tracking students’ programming knowledge state15. The DSAKT \nmodel addresses sparse data issues through a self-attention mechanism, enhancing the accuracy of knowledge \ntracing16. The GPPKT model combines students’ learning abilities with a Variational Autoencoder (V AE), \nimproving model performance through the integration of personalized answer sequences and knowledge \ngraphs17. With continuous technological advancements, the field of knowledge tracing will continue to evolve, \nbringing more possibilities for personalized education and improved learning outcomes.\nCurrently, research on programming knowledge tracing primarily focuses on predicting students’ learning \nstates based on programming problems and their submission records18,19. However, this approach often overlooks \nthe significant impact of problem difficulty on answer correctness. Programming problems require students \nnot only to accurately understand and analyze the problem text but also to apply knowledge of programming \nlanguage syntax, algorithms, and data structures to solve real-world problems, as shown in Fig.  1, which \nillustrates the impact of text understanding difficulty and knowledge concept difficulty on predicting students’ \nknowledge state. In this process, both the text understanding difficulty and knowledge concept difficulty of \nSchool of Computer Science, Liupanshui Normal University, Liupanshui 553000, China. email:  \nxinjiesun@lpssy.edu.cn\nOPEN\nScientific Reports |        (2025) 15:11475 1| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports\n\nprogramming problems are crucial for students’ success in answering these problems. Currently, the annotation \nof programming problem difficulty largely relies on manual efforts, making it challenging to accurately label \nboth text understanding difficulty and knowledge concept difficulty simultaneously. Therefore, efficiently and \naccurately determining the text understanding difficulty and knowledge concept difficulty for each programming \nproblem and utilizing this information to optimize the model’s predictions of students’ knowledge state has \nbecome a key challenge in the field of programming knowledge tracing. This paper aims to address this challenge \nby proposing an innovative method to enhance the efficiency and accuracy of matching programming problem \ndifficulties. Through this approach, we can comprehensively and objectively assess students’ programming \nabilities, thereby providing more targeted support and interventions for programming education.\nTo ensure that the difficulty of programming problems is reasonably matched to students’ knowledge state, we \nface three major challenges: first, the diversity of programming problems complicates the assessment of semantic \ndifficulty; second, the interaction between knowledge concept difficulty and text understanding difficulty is \ndifficult to effectively distinguish; and third, the sequential modeling of knowledge concepts does not adequately \nconsider the strong correlations between them. To address these challenges, we propose the following solutions: \nfirst, by introducing large language models to extract deep semantic features from programming problems, we \nenhance the precision of semantic difficulty assessment; second, we use an attention mechanism to dynamically \nanalyze the relationship between knowledge concept difficulty and text understanding difficulty to ensure the \nmodel’s detailed performance; finally, combining graph attention mechanisms with a carefully designed gating \nmechanism allows for better attention to the spatiotemporal relationships between knowledge state.\nBased on these ideas, we propose difficulty-aware programming knowledge tracing with large language \nmodels (DPKT). This model dynamically adjusts students’ mastery states of different difficulty knowledge \nconcepts by combining an update gate mechanism with a graph attention network. The DPKT model inputs \nprogramming problems and corresponding knowledge concepts into a large language model to extract rich \nsemantic features and uses an attention mechanism to differentiate between knowledge concept difficulty and \ntext understanding difficulty, ensuring precise updates to knowledge state.\nThe main contributions of this model include:\n• The DPKT model combines large language models with graph attention networks, improving the assessment \naccuracy of programming problem difficulty.\n• The dynamic update mechanism enables the model to reflect changes in students’ knowledge state in real \ntime, enhancing sensitivity to students’ learning progress.\n• The model proposes new solutions for distinguishing between knowledge concept and text understanding \ndifficulty, providing solid theoretical support for personalized learning.\nBy leveraging the powerful data processing capabilities of large language models, this model achieves significant \nimprovements in the efficiency and accuracy of semantic information extraction. Additionally, the DPKT model \nintegrates large language models, CodeBERT, and graph attention networks, addressing the issue of accurately \npredicting students’ knowledge state in situations where knowledge concepts are the same but semantic \ndifficulties differ, thus enhancing the model’s performance and interpretability. In comparison with existing \nknowledge tracing models, the DPKT model demonstrates excellent performance across various language \ndatasets, validating its value in practical applications.\nIn summary, the DPKT model not only provides an innovative solution for programming knowledge \ntracing but also offers educators a powerful tool, with the expectation of making a positive impact in the field of \nprogramming education and promoting further research development in this area.\nFig. 1. A toy example demonstrating the impact of problem difficulty on programming knowledge tracing.\n \nScientific Reports |        (2025) 15:11475 2| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nRelated works\nIn the context of the rapid development of modern educational technology, knowledge tracing models have \ngradually become important tools for personalized learning, helping educators monitor and assess students’ \nlearning progress more effectively. At the same time, the introduction of large language models (LLMs) has \nbrought new possibilities to the field of knowledge tracing, driving innovation and transformation in educational \npractices20. The following will delve into the foundational models of knowledge tracing, the advancements in \ndynamic adaptive models, as well as the applications and challenges associated with Large Language Models \n(LLMs) in educational contexts.\nKnowledge tracing\nThe foundational models of knowledge tracing lay the theoretical groundwork for subsequent research and \napplications. Bayesian Knowledge Tracing (BKT), as a classic model, utilizes Bayesian networks to predict \nstudents’ knowledge state, emphasizing the importance of uncertainty and prior knowledge, thus providing \na solid statistical foundation for knowledge tracing 21. Additionally, the application of probabilistic models \nin knowledge tracing further supports the theoretical framework of the entire field 22. By incorporating deep \nlearning and recurrent neural network techniques, these models can effectively capture students’ mastery \nlevels of knowledge concepts, enabling tracking and prediction of personalized learning 1. Researchers have \nenriched our understanding of the learning process by identifying key factors influencing student learning, \nallowing for assessments of knowledge mastery and providing feedback to educators to help them adjust \nteaching strategies23,24. Building on this, Memory-Aware Knowledge Tracing (MKT) considers the influence of \nstudents’ memory factors, enhancing the accuracy of knowledge state predictions by simulating human memory \nprocesses25. The comprehensive application of these models allows for a more thorough understanding and \nimprovement of students’ learning processes.\nWith the advancement of knowledge tracing technology, dynamic and adaptive knowledge tracing models \nhave emerged, better adapting to the changes in students’ learning. These models not only enhance the flexibility \nof knowledge tracing but also improve responsiveness to individual differences among students 8,11,26. For \ninstance, models that incorporate dynamic student classification can better adjust to varying ability levels \namong students, thus enhancing the personalized learning experience5. Moreover, Knowledge Tracing Machines \npropose a knowledge tracing method based on factorization machines, focusing on accurately modeling \nstudents’ knowledge state, providing a new perspective for the model27. By using convolutional neural networks \nto model personalized features in students’ learning processes, the learning capabilities of the models are further \nenhanced28. The integration of these dynamic and adaptive models enables a more comprehensive response to \nstudents’ learning needs and changes.\nIn recent years, significant progress has been made in the field of programming knowledge tracing, \nparticularly in developing models that incorporate the difficulty of programming exercises and forgetting \nfactors. For example, a deep knowledge tracing (KT) method integrates the difficulty of programming exercises \nand forgetting factors, using the GPT-3 algorithm to analyze problem difficulty and constructing knowledge \nstructure graphs with graph neural networks, thereby improving the model’s predictive performance and \ninterpretability29. Additionally, research has explored how the difficulty and order of programming problems \nin online assessment systems affect user performance and cognitive load, revealing various influencing factors \nthrough statistical analysis3. Common challenges in programming education, such as compilation errors and skill \nmisunderstandings, have also received considerable attention4. At the same time, a meta-analysis summarized \neffective interventions in programming education, highlighting the outstanding performance of visualization \nand tangible learning tools in improving learning outcomes30.\nThe development of knowledge tracing models provides important theoretical support for educational \npractices, especially excelling in personalized learning and dynamic adaptability. However, research in the \narea of programming problems, particularly regarding problem difficulty, remains relatively weak, especially \nconcerning the complexity of text understanding and the definition of specific knowledge concept difficulty, with \nlimited relevant literature available. Therefore, in-depth exploration of these dimensions will not only enrich the \napplication context of knowledge tracing models but also provide a more robust theoretical foundation for the \ndeepening and optimization of programming education.\nDevelopment of large language models in knowledge tracing\nLarge language models (LLMs) have emerged as powerful interpreters in the field of knowledge tracing, \nshowcasing significant application potential and introducing innovative methods and tools to the educational \ndomain31. For instance, CoRE proposes using LLMs as interpreters across various programming forms, including \nnatural language programming, pseudocode programming, and AI agent flow programming32. Leveraging LLMs \ncan help students understand code, thereby enhancing the learning experience33. To make programming more \nintuitive, Low-code LLM has introduced a low-code approach for visual programming using large language \nmodels, providing a user-friendly programming environment for non-specialists 34. In terms of assessing the \neffectiveness of technology, LLMs have been found to effectively address issues in example programming 35. \nAdditionally, the Copilot Evaluation Harness presents a method for assessing software programming guided by \nlarge language models, providing a framework for subsequent application evaluations36. These studies indicate \nthat LLMs have a broad application prospect in knowledge tracing and programming education.\nThe introduction of LLMs has significantly enhanced the capabilities of knowledge tracing models, injecting \nnew vitality into the development of educational technology. For example, methods that enhance code \nknowledge tracing with large language models have further propelled innovation in this field 37,38. In teaching \napplications, CS1-LLM explores methods to integrate LLMs into introductory computer science courses, \neffectively improving instructional outcomes39. Concurrently, research has found that novices use LLM-based \nScientific Reports |        (2025) 15:11475 3| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\ncode generators to tackle programming tasks in self-directed learning environments, providing valuable insights \nfor course design40. Furthermore, Forgetful Large Language Models discusses lessons learned from using LLMs \nin robotic programming, reflecting their limitations and challenges 41. These studies demonstrate that LLMs \nnot only enhance the functionality of knowledge tracing models but also reveal their extensive potential in \neducational practice.\nDespite the promise of LLMs in education and the insights they offer for course design, revealing the diversity \nof educational goals 2,42, several challenges remain to be addressed. In terms of technological integration and \ninfrastructure, incorporating LLMs into existing educational technologies requires substantial resources and \ntechnical support, and deployment in different educational environments may necessitate customized solutions. \nAdditionally, ethical and legal issues cannot be overlooked; using LLMs involves concerns about the use and \nownership of student data, necessitating compliance with data protection regulations. To tackle these challenges, \nclose collaboration among educators, technology developers, policymakers, and researchers is essential to \nensure that the application of LLMs in education is responsible, effective, and capable of enhancing educational \nquality and equity. Through this research, we can gain deeper insights into the challenges of LLMs in education \nand provide valuable perspectives for future development43,44.\nPreliminary\nIn this section, we will provide a formal definition of the difficulty-aware programming knowledge tracing with \nlarge language models. Additionally, we will introduce some fundamental elements and important embeddings \ninvolved in our proposed DPKT framework. Table 1 summarizes the mathematical symbols used in this paper.\nProblem definition\nIn the online programming assessment system, the set of students is represented as S, the set of programming \nproblems as P, the set of knowledge concepts as K, and the set of student code submission responses as R. The student \nset S = {s1,s 2,...,s n} contains n different students. The problemming problem set P = {p1,p 2,...,p m} \nrepresents m different coding problems. The knowledge concepts set K = {k1,k 2,...,k v} contains v different \nknowledge concepts, generated based on LLMs in this paper. The set of text understanding difficulty levels \nTD = {1, 2, 3, 4} include 4 difficulty levels. The set of knowledge concepts difficulty levels KD = {1, 2, 3, 4} also \nincludes 4 difficulty levels. The set of student submission responses R = {0, 1} contains two elements, indicating \nwhether the student’s submitted code is deemed acceptable by the system. Here, 1 represents a successful code \nsubmission, while 0 indicates a failed submission. During the student’s programming practice, this is represented \nas a programming event sequence L = {(p1,k 1, td1, kd1,r 1), (p2,k 2, td2, kd2,r 2),..., (pt,k t, tdt, kdt,r t)}\n. In this sequence, pt represents the programming problem answered by the student at time step t, kt indicates the \ncorresponding knowledge concept generated by the large model, tdt denotes the text understanding difficulty \nlevel of the problem, kdt represents the knowledge concept difficulty level, and rt indicates the student’s \nsubmission status at time step t, By analyzing the student’s historical practice records, we can trace their mastery \nof programming knowledge. The definition of knowledge tracing in programming practice is as follows:\nKnowledge Tracing(KT) : The historical practice records of students \nL = {(p1,k 1, td1, kd1,r 1), (p2,k 2, td2, kd2,r 2),..., (pt,k t, tdt, kdt,r t)}, The task of programming \nknowledge tracing (KT) is a dynamic process. A student’s programming skill level progresses and changes over \ntime based on their performance in understanding knowledge concepts and the difficulty of problem text. The \ngoal of this task is to assess their mastery of programming skills based on their historical code submissions, \nin conjunction with their performance in knowledge concept mastery and text understanding, and to further \npredict their performance in the next programming task.\nDifficulty definition\nTo better track students’ programming learning progress and performance, we introduce two important \nfactors: knowledge concept difficulty(KD) and problem text understanding difficulty(TD) 45,46. Knowledge \nconcept difficulty reflects the complexity of the programming concepts or skills involved in a problem. It can \nbe quantified based on multiple dimensions, such as the coverage of knowledge concepts and the frequency \nwith which students master these concepts. The Dreyfus model divides the skill acquisition process into: \nthe Novice stage, the Advanced Beginner stage, the Competent stage, the Proficient and Expert stage. Each \nSymbol Description\nS The set of students\nP The set of programming problems\nR The set of response\nm, n, v The number of problem, student and KCs\nKC The set of Knowledge concepts\nKD Knowledge Concepts difficulty\nTD Text understanding difficulty\ndi Knowledge Concepts difficulty level\nui Problem text understanding difficulty level\nTable 1. Mathematical symbol and descriptions.\n \nScientific Reports |        (2025) 15:11475 4| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nstage represents a different level of understanding and application ability. Based on this model, we define the \ndifficulty levels of programming problems into four tiers corresponding to the novice, advanced, proficient, and \nexpert stages, thus providing a systematic difficulty assessment framework 47–49. Specifically, this is defined as: \nKD = {d1,d 2,d 3,d 4}, where each di represents a specific difficulty level, with higher values indicating greater \ncomplexity in syntax.\nProblem text understanding difficulty refers to the complexity of students’ reading and comprehension \nof programming problems. This difficulty not only depends on whether the problem description is clear and \nconcise but also relates to the terminology used in the problem and the complexity of the problem background. \nIt is defined as: TD = {u1,u 2,u 3,u 4}, where each ui represents different levels of text understanding \ndifficulty, with higher values indicating that the problem description is more complex and harder for students to \nunderstand. We generate the difficulty levels for each knowledge concept and the text understanding difficulty \nlevels of the problems using large language models. Table  2 summarizes the descriptions and definitions of \nknowledge concepts and problem text understanding difficulties.\nEmbeddings\nBefore delving into the assessment of programming problem difficulty and the generation of embeddings \nbased on large language models, we need to briefly introduce some key fundamental elements that aid in a \ndeeper understanding of the model’s internal workings. In this model, five key elements are primarily involved: \nproblems (P), knowledge concepts (KCs), knowledge concept difficulty (KD), text understanding difficulty ( \nTD), and student code submission responses (R), which are represented through embedding matrices. The \nproblem embedding matrix V ∈ Rm×dp , represents the embeddings generated by CodeBERT for programming \nproblems, with dimensions m × dp, where m is the number of problems and dp is the dimension size of the \nproblem embeddings. The knowledge concept embedding matrix K ∈ Rv×dk , represents the embeddings \ngenerated by CodeBERT for knowledge concepts related to programming problems, with dimensions v × dk, \nwhere vis the number of knowledge concepts and dk is the dimension size of the knowledge concept embeddings. \nThe knowledge concept difficulty embedding matrix KD ∈ R4×dg , represents the embeddings generated by the \nlarge language model for knowledge concept difficulty in programming problems, with dimensions 4 × dg, where \n4 corresponds to the four levels of knowledge concept difficulty and dg is the dimension size of the knowledge \nconcept difficulty embeddings. The text understanding difficulty embedding matrix TD ∈ R4×dt  represents \nthe embeddings generated by the large language model for text understanding difficulty in programming \nproblems, with dimensions 4 × dt, where 4 corresponds to the four levels of text understanding difficulty and \ndt is the dimension size of the text understanding difficulty embeddings. The student code submission response \nis represented by the embedding matrix R ∈ R2×dr , with dimensions 2 × dr, where 2 indicates whether the \nsubmitted code was accepted by the system (1 for accepted, 0 for rejected), and dr is the dimension size of the \nresponse embeddings.\nThe DPKT framework\nIn this section, we will delve into the DPKT framework proposed in this paper, the structure of which is \nillustrated in Fig. 2. This framework is divided into four main parts. The first part is the difficulty assessment layer, \nwhich assesses the text understanding difficulty and knowledge concept difficulty of programming problems \nusing a large language model, generating relevant difficulty level information. The second part is the semantic \nperception layer, which utilizes the large language model to extract semantic information about difficulty from \nthe problems, constructing a relationship graph between problem difficulty and knowledge concepts. The third \npart is the knowledge update layer, which enhances and updates knowledge based on the information from the \nfirst two layers, thereby better reflecting the students’ grasp of knowledge. The fourth part is the prediction layer, \nwhich uses the updated knowledge information to predict and assess students’ knowledge state. Next, we will \nprovide a more detailed discussion and explanation of these four modules.\nDifficulty assessment layer\nTraditional programming problem difficulty labeling primarily relies on the accuracy of each problem 50. This \nmethod depends on a large amount of student submission data, leading to inaccurate labeling for new or \ninfrequently used problems. Accuracy assessments often overlook the details and complexities of the problems \nand cannot reflect real-time changes in problem difficulty or student abilities46. Furthermore, traditional methods \nSymbol Description\nd1 The knowledge concept belongs to the beginner level of programming, such as basic syntax, variable declaration, simple control structures, etc.\nd2 Involves more complex logic, such as array manipulation, recursion, basic algorithms, etc.\nd3 Requires the application of advanced programming skills and algorithms, such as dynamic programming, graph algorithms, concurrent programming, etc.\nd4 Focused on advanced scenarios in algorithm competitions or real-world development, such as distributed systems, memory optimization, multithreading, etc.\nu1 The problem description is clear and concise, easy for students to understand, requiring no additional background knowledge.\nu2 The problem is slightly more complex, potentially involving multiple steps or requiring students to combine certain background knowledge.\nu3 The problem includes multi-layered logical descriptions or complex backgrounds, possibly necessitating strong reading comprehension skills from students.\nu4\nThe problem involves a large number of technical terms or implicit assumptions, requiring students to have a solid knowledge base and strong reasoning skills \nto understand.\nTable 2. Difficulty level description.\n \nScientific Reports |        (2025) 15:11475 5| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nare mainly based on historical performance, failing to effectively capture the deep structure of programming \nknowledge. The method proposed in this paper, which utilizes a large language model to label the knowledge \nconcepts and text understanding difficulty of programming problems, is more resource-efficient and accurate. \nIt fully leverages the powerful coding and comprehension capabilities of large language models, providing a \ncomprehensive understanding of knowledge concepts and levels 36. For evaluating the difficulty of knowledge \nconcepts in programming problems, we adopted the following prompt approach:\n A = {problem_id, text_description, LTD ,L KD,KCs } (1)\nWhere: A is the complete difficulty dictionary for the problems, LKD ∈ D represents the difficulty level of the \nknowledge concepts for the problem identified by problem_id, LTD ∈ D indicates the text understanding \ndifficulty level for the problem identified by problem_id, KCs are the knowledge concepts involved in the \nproblem.\nTo effectively integrate the information from the feature set A and provide a unified representation for \nsubsequent model training and prediction, we convert the set A into an embedding matrix Q. This transformation \nnot only captures the relationships among the various features but also represents rich semantic information \nthrough high-dimensional vectors, thereby enhancing the model’s expressive and generalization capabilities.\nThus, the embedding matrix Q is constructed as:\n \nQ =\n\n\nfone_hot(problem_id)\nfword2vec(text_description)\nfinteger(LTD )\nfinteger(LKD )\nfone_hot(KCs )\n\n (2)\nIn this way, we can represent various aspects of the problems more comprehensively, improving the accuracy and \neffectiveness of subsequent analyses.\nWe use a large language model to pre-train the difficulty levels of knowledge concepts and text understanding \ndifficulty for programming problems. The pre-training data is obtained through manual annotation. We invited \nten experienced teachers in the field of computer education to assess the knowledge concept difficulty of \nFig. 2. The core architecture of the DPKT model involves using a large language model to annotate the text \nunderstanding difficulty and knowledge concepts difficulty of programming tasks. The task difficulty is then \nincorporated as one of the input conditions for predicting the student’s knowledge state at the next time step in \nthe knowledge tracing model.\n \nScientific Reports |        (2025) 15:11475 6| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nselected programming problems: KD = {d1,d 2,d 3,d 4} and the text understanding difficulty of the problems: \nTD = {u1,u 2,u 3,u 4}. Each teacher provided ratings for different difficulty dimensions of each problem. \nUltimately, we conducted a weighted average of all teachers’ ratings to generate a comprehensive difficulty score \nfor each problem. Let the score for the i problem be Si, where Sj\ni  represents the rating given by the j teacher for \nthe i problem (including both knowledge concept difficulty and text understanding difficulty). The final score \nfor each problem can be expressed as:\n \nSi = 1\nN\nN∑\nj=1\nwjSj\ni  (3)\nIn the equations, N is the total number of teachers (in this case, 10), and wj  is the weight assigned to the j teacher, \nwhich is set based on the teacher’s professional background, experience, and scoring consistency. Specifically, \namong the 10 teachers, there are 3 senior teachers, 3 intermediate teachers, and 4 junior teachers. Due to their \nextensive experience and professional level, senior teachers have a higher base weight, and after adjustment for \nconsistency, their weight values are 0.144, 0.12, and 0.144, respectively. The base weight for intermediate teachers \nis slightly lower than that for senior teachers, with weight values of 0.10, 0.08, and 0.10, respectively. Junior \nteachers have a relatively lower base weight, with weight values of 0.064, 0.08, 0.096, and 0.064, respectively. \nThis method of weight distribution takes into account both the professional level and scoring stability of the \nteachers, more accurately reflecting the reference value of each teacher’s scores51. The calculations for knowledge \nconcept difficulty scores and text understanding difficulty scores can be expressed as: KDi = {d1,d 2,d 3,d 4}\n, TDi = {u1,u 2,u 3,u 4}\nTherefore, the final comprehensive difficulty scores can be expressed as:\n \nKDﬁnal\ni = 1\nN\nN∑\nj=1\nwjKDj\ni  (4)\n \nTDﬁnal\ni = 1\nN\nN∑\nj=1\nwjTDj\ni  (5)\nHere, the value ranges for the knowledge concept difficulty score matrix KD and the text understanding \ndifficulty score matrix TD are KDj\ni ,TD j\ni ∈ [1, 4], for each i (problem index) and j (the j-th teacher). By using \na weighted average approach, we can effectively integrate the professional assessments from different teachers, \nthereby providing a more objective and accurate difficulty score for each programming problem.\nThrough the pre-training of knowledge concept difficulty and text understanding difficulty using the \nlarge language model, we obtain the embedding matrix Q. We further extract contextual information from \nprogramming problems and knowledge concepts using the self-attention mechanism of the CodeBERT model, \ngenerating high-quality embedding representations. These embeddings can capture the semantic relationships \nwithin the text, thus providing rich information for subsequent processing.\nThe K and V  matrices output by CodeBERT are used for attention calculations, where K represents the \nkeys (knowledge concepts and problem features), and V  represents the values (information associated with \nK ). The attention mechanism dynamically weights the importance of different information by calculating the \nrelationships among Q,K and V . Q represents the query of the current input, K consists of the features to be \nreferenced, while V  contains the values associated with K.\nThrough attention calculations, the model can focus on the knowledge concepts or semantic information \nmost relevant to the current input. The attention mechanism helps the model understand the relationships \nbetween different knowledge concepts and problems, making the final output more semantically relevant, \nthereby enhancing the assessment of students’ knowledge state. The calculation process is as follows:\n ˜vTD = softmax(F(KTD,Q TD)) · VTD  (6)\n ˜vKD = softmax(F(KKD,Q KD)) · VKD  (7)\nThe generated VTD and VKD are fused, with the calculation formula as follows:\n ˜V = αVTD + (1− α)VKD (8)\nHere, F(K, Q) represents the similarity function in the self-attention mechanism, α is a trainable fusion weight, \nVTD and VKD are the attention results for knowledge concept difficulty and text understanding difficulty, \nrespectively.\nSemantic perception layer\nThrough the perception of problem text understanding difficulty and knowledge concept difficulty in the \ndifficulty perception layer, we need to further understand the close relationship between the problem at time t \nand the problems solved by the student from time 1 to t − 1 . This forms a spatiotemporal representation of the \nstudent’s knowledge state, providing a more accurate assessment of the student’s knowledge state.\nScientific Reports |        (2025) 15:11475 7| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nWe treat knowledge concepts and programming problems as each node ˜Vi, aggregating the information \nof each node’s neighboring nodes through the self-attention mechanism to generate new node feature \nrepresentations. This helps capture the relationships and contextual information between nodes. By aggregating \nthe information from neighboring nodes to update each node’s state, we reflect the student’s mastery of different \nknowledge concepts. The model allows for the dynamic adjustment of contributions from different neighboring \nnodes, thus determining how to aggregate information based on the actual situation.\nWe construct the adjacency matrix B based on ˜V . In this graph, the node ˜Vi represents the features of each \nprogramming problem, and the relationship between two nodes indicates that the two programming problems \nare semantically similar or have a dependency relationship.\n \nSim( ˜Vi, ˜Vj)=\n˜Vi · ˜Vj\n∥ ˜Vi∥∥ ˜Vj∥  (9)\nAmong them, Sim( ˜Vi, ˜Vj) represents the cosine similarity between node ˜Vi and node ˜Vj, where ˜Vi and ˜Vj are the \nfeature vectors of the two nodes, and ∥·∥  denotes the vector norm.\nFirst, we construct the similarity matrix S, where S[i][j] indicates the similarity between node i and node j :\n S[i][j]= similarity( ˜Vi, ˜Vj) (10)\nAccording to the similarity matrix S, a threshold λ is set. The threshold λ is a trainable key parameter that \ndetermines the connection between two nodes. The adjacency matrix B is constructed based on this threshold:\n \nB[i][j]=\n{ 1 if S[i][j] ≥ λ\n0 if S[i][j] <λ  (11)\nAccording to the adjacency matrix, for each pair of adjacent nodes i and j, we calculate the attention coefficient \nαj\ni :\n \nαj\ni = softmaxj\n(\naT [Wvi ∥ Wvj ]√\nd\n)\n (12)\nHere, vi and vj  are the feature vectors of nodes i and j, W is a learnable weight matrix, a is the weight vector \nused for attention calculation, ∥ denotes the concatenation operation, and d is the feature dimension.\nThe attention coefficient αj\ni  is used to weight the features of adjacent nodes; for each node i, its updated \nfeature representation h′\ni is obtained by weighted aggregation of the information from adjacent nodes:\n \nh′\ni = σ\n\n ∑\nj∈N(i)\nαijWvj\n\n (13)\nHere, N(i) denotes the set of adjacent nodes connected to node i, σ is the ReLU non-linear activation function.\nThe adjacency matrix defines the connection relationships between nodes and guides the calculation of \nattention coefficients, influencing the node feature update process. Through Graph Attention Networks (GAT), \nthe DPKT model we designed can effectively propagate and aggregate the features and contextual information of \nproblems within the graph structure, aiding in a more accurate assessment of the student’s learning state.\nFinally, the new feature matrix of the nodes is output:\n H′ = concat(h′\n1, h′\n2,..., h′\nN) (14)\nEach node in the matrix H′ represents the integration of information from that node and its adjacent nodes, \nreflecting the student’s status in learning programming knowledge and skills, thus providing rich contextual \ninformation for subsequent knowledge tracing and state assessment.\nKnowledge update layer\nDifferent difficulty levels of exercises have varying impacts on students’ knowledge state, and the influence of \nexercise difficulty on knowledge state is dynamic48. The knowledge update layer allows for the dynamic updating \nof students’ mastery of exercises at different difficulty levels and their overall knowledge state. Specifically, we \nfirst concatenate the semantic features h′\nt at time t with the student’s code submission response Rt and the \nprevious hidden state Ht−1 to obtain a comprehensive information vector Zt, The specific calculation formula \nis as follows:\n Zt = h′\nt ⊕ Rt ⊕ Ht−1 (15)\nIn this context, ⊕ denotes the vector concatenation operation.\nThe students’ knowledge state are updated by practicing exercises of varying difficulties, and the knowledge \nupdate gate Gt dynamically adjusts the students’ mastery of knowledge at different difficulty levels, facilitating \neffective information integration. The calculation formula is as follows:\nScientific Reports |        (2025) 15:11475 8| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\n Gt = σ(Wg · Zt + bg) (16)\nWhere, Wg is the weight matrix for the update gate, bg is the bias vector, and σ(·) is the Sigmoid activation \nfunction.\nTo ensure the model can dynamically update knowledge state, we integrate current input information with \nhistorical data to enhance and update the knowledge state. The calculation formula is as follows:\n ht = (1− Gt) · Zt + Gt · tanh(Wh · Zt + bh) (17)\nWhere Wh is the weight matrix for knowledge state updates, bh is the bias vector, and  represents element-wise \nmultiplication.\nThrough the knowledge update layer, the model integrates the latest and historical semantic information from \nexercises and students’ knowledge state, allowing for dynamic updates during the students’ learning process.\nOutput layer\nThe knowledge mastery state of the student at the current time ht is obtained. To better predict the student’s \nknowledge performance at the next time step, the next programming problem and relevant knowledge points are \nfused into the current knowledge state for output. The calculation formula is as follows:\n yt =ht ⊕ Pt+1 ⊕ KCt+1  (18)\n yt+1 =σ(Wy · yt + by)  (19)\nWhere, Wy is the learnable weight parameter, by is the bias term, and σ(·) is the Sigmoid activation function.\nObjective function\nDuring the model training process, we employed the Adam optimization algorithm to accelerate convergence and \nenhance robustness through adaptive learning rates. Since knowledge tracing can be regarded as a classification \nproblem, we selected the binary cross-entropy loss function, which effectively measures the discrepancy between \npredictions and true labels. Specifically, the objective function is defined as:\n \nL = −\nT∑\nt=1\n[Rt log yt + (1− Rt) log(1− yt)] +µθ||θ||2 (20)\nAmong them, θ represents all trainable parameters in this model, including learning rate, batch size, number \nof training epochs, and regularization techniques. The specific settings are as follows: The initial learning rate \nis set to 0.001, and a learning rate decay strategy is adopted, where the learning rate is reduced to 0.1 of the \noriginal after a certain number of training epochs. Batch size: The batch size is set to 32, which is the optimal \nvalue found in experiments, ensuring model training efficiency while avoiding overfitting. Number of training \nepochs: The number of training epochs is set to 50, and the optimal number of training epochs is selected \nthrough cross-validation. Regularization: L2 regularization technology is used, with a regularization coefficient \nset to 0.01 to prevent model overfitting. With these settings, we can more effectively optimize model parameters \nand improve model performance and generalization ability. µθ is the regularization hyperparameter used to \nprevent model overfitting. The regularization term µθ∥θ∥2 penalizes the model’s complexity by adding the L2 \nnorm of the parameters to the loss function, thereby improving the model’s generalization ability. To ensure \nthe reproducibility and transparency of the model, we have made detailed adjustments and verifications of the \nregularization hyperparameter µθ in the experiment. Specifically, the value of µθ is the optimal value obtained \nby grid search within the interval [10−4, 10−3, 10−2, 10−1]. When µθ is set to 10−3, the model achieves the \nbest performance on the validation set, striking a good balance between predictive accuracy and generalization \nability. This selection process ensures the model’s stability and generalization ability across different datasets.\nExperiments\nIn the experimental section, we first provide a detailed overview of the dataset used, including its source, \nstructure, and statistical information. We describe the data preprocessing, feature extraction, and other \nprocesses to ensure that the model can efficiently utilize this data. Next, we compare the baseline model with our \nproposed knowledge tracing model based on large language models in terms of their ability to predict student \nperformance, showcasing their performance across various evaluation metrics. Furthermore, we conduct a \nseries of experiments to explore the different modules of the model and their impact on overall performance, \nrevealing the advantages and potential of large language models in knowledge tracing tasks. The key research \nquestions we aim to investigate are as follows:\n• RQ1: Does our proposed knowledge tracing model based on large language models demonstrate superior \nperformance in predicting students’ future performance?\n• RQ2: How does the difficulty of programming problems affect the modeling of students’ knowledge state?\n• RQ3: How much prior knowledge should be provided for large language models to achieve optimal results?\n• RQ4: How does the difficulty level generated by large language models compare to the difficulty level anno -\ntated by humans?\nScientific Reports |        (2025) 15:11475 9| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\n• RQ5: What is the impact of different modules in the model (such as difficulty assessment and semantic aware-\nness) on the knowledge tracing results?\n• RQ6: What is the effect of problems at varying difficulty levels on the model’s performance?\n• RQ7: How does the knowledge tracing model based on large language models understand the code submitted \nby students?\nThrough the exploration of these questions, we not only validate the model’s effectiveness across different tasks \nbut also investigate its limitations and areas for optimization, aiming to provide guidance for future related \nresearch.\nDatasets\nTo validate the effectiveness of DPKT, we selected the Project CodeNet dataset for testing. Project CodeNet is \na large and diverse programming dataset that contains approximately 14 million code samples, covering over \n50 programming languages. It is particularly well-suited for tasks such as programming problem difficulty \nmodeling and student knowledge state prediction49. Table 3 presents specific statistical information filtered after \ndata cleaning. A notable feature of Project CodeNet is that it provides detailed metadata for each programming \nproblem, such as code execution time, memory usage, and submission status (whether it passed or failed). \nThis rich metadata provides a solid foundation for our difficulty analysis based on large language models. In \nCodeNet, 94.6% of submissions are made in the following four programming languages: C++, Python, Java, \nand C. In this study, we focus on these four languages. To more accurately assess students’ programming skills, \nthis research utilizes programming problems combined with the execution results of the code (such as whether \nit compiled successfully and whether it was correct) to trace students’ knowledge state. Additionally, to handle \nexercise sequences of varying lengths, we set the maximum sequence length for analysis to 50. For sequences \nexceeding this length, we split them into multiple parts, while shorter sequences are standardized through zero \npadding. This approach ensures that the model can effectively leverage the diversity of different problems and \nlanguages for difficulty perception and student state prediction.\nExperimental setup\nTo comprehensively assess the performance of the DPKT model, we conducted an in-depth comparison with \nseveral mainstream baseline models. To ensure fairness and scientific rigor in the comparison, all baseline models \nwere reproduced under the same experimental conditions, with precise tuning of their key parameters to ensure \neach model operated in its optimal state. These baseline models encompass existing mainstream knowledge \ntracing (KT) methods, representing various learning strategies and network architectures. For performance \nevaluation, we selected AUC (Area Under Curve) and ACC (Accuracy) as the primary metrics, which allow for \na more comprehensive assessment of the overall performance of the models and directly measure their accuracy \nin actual prediction tasks, particularly in predicting accuracy under different student knowledge state. The \nexperimental methods strictly followed the principle of reproducibility, with all models running on a Linux server \nequipped with a Tesla V100 GPU, and multiple repetitions were conducted to minimize the impact of random \nfactors on the experiments, ensuring the reliability and robustness of the results. In this study, we employed \n5-fold cross-validation to assess the model’s performance, which ensures the stability of the evaluation results \nwhile reducing computational costs. During the experimental process, we thoroughly tested the model through \ncross-validation and extensively verified it with different types of datasets, such as real student learning datasets \nfrom educational platforms.The LLM model we utilized is GPT-4o, which is a multimodal large language model, \nand the BERT model is CodeBERT, an optimized BERT model for programming languages52. The selection and \napplication of these models further enhance the depth and breadth of our research.\n• DKT1 proposes using recurrent neural networks (RNNs) to model students’ knowledge state. By not relying \non manual knowledge encoding, RNNs capture more complex student learning behaviors, enhancing the \npredictive performance of knowledge tracing tasks.\n• DKVMN8 is based on dynamic memory networks and introduces static key and dynamic value matrices to \nstore knowledge concepts and the levels of knowledge mastered by students, respectively. This model au -\ntomatically discovers and updates knowledge state, addressing the issue of manual concept annotation in \ntraditional methods.\n• SAKT53 employs self-attention mechanisms to select relevant knowledge concepts from students’ past activ-\nities, offering better handling of data sparsity issues. The model excels in sparse data environments, signifi -\ncantly enhancing predictive performance.\n• EKT7 uses self-attention mechanisms to select past activities related to specific knowledge concepts, mitigat-\ning the impact of data sparsity on the model. The model demonstrates outstanding performance on multiple \nreal datasets, improving AUC and prediction outcomes.\nLanguage type Number of problems Number of students Number of submissions\nC++ 1218 678 35,245\nPython 1097 689 29,330\nJava 1052 487 22,650\nC 963 512 19,400\nTable 3. Statistics of Selected Languages from Project CodeNet Dataset.\n \nScientific Reports |        (2025) 15:11475 10| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\n• Code-DKT19 is designed for programming tasks and extends the DKT model with attention mechanisms, \nautomatically extracting code features relevant to problems and enhancing prediction performance in pro -\ngramming tasks through these features.\n• DKT+54 addresses issues present in DKT (such as input reconstruction failure and inconsistent knowledge \nlevel predictions) by introducing two regularization terms to enhance the model’s predictive consistency and \nstability, thereby improving DKT’s performance.\n• DIMKT50 introduces a problem difficulty matching mechanism to dynamically assess the relationship be -\ntween students’ knowledge state and problem difficulties, adjusting knowledge state predictions for problems \nof varying difficulty.\n• QIKT55 refines the modeling of students’ knowledge state through problem-centric approaches and proposes \na prediction layer based on item response theory, enhancing the model’s interpretability and predictive ac -\ncuracy.\n• TCKT56 incorporates time and causality enhancement mechanisms to improve the consistency and predictive \naccuracy of knowledge state through self-attention methods and knowledge forgetting and acquisition gates.\n• THKT57 introduces a hierarchical guidance mechanism to dynamically assess student feedback at different \nproblem levels, optimizing predictions of students’ knowledge mastery and enhancing the model’s interpret-\nability and hierarchical recognition capabilities.\nPerformance prediction (RQ1)\nTo validate the effectiveness of our proposed DPKT model in tracking students’ knowledge state and predicting \nfuture performance, we conducted a comparative analysis with various classic baseline models. In the experiment, \nwe used AUC (Area Under the Curve) and ACC (Accuracy) as the main evaluation metrics. AUC measures the \nmodel’s ability to distinguish between positive and negative samples, while ACC represents the overall predictive \naccuracy of the model. Table 4 shows the average performance of different models on the AUC and ACC metrics. \nThe choice to report the average values rather than the standard deviation values is based on the experimental \ndesign, where the consistency of experimental conditions leads to minimal variation in results. Specifically, by \nrepeating the experiments multiple times, we ensured that the data division and experimental conditions were \nthe same each time, thus making the average values of AUC and ACC highly stable. Compared to traditional \nknowledge tracing models such as DKT and DKVMN, the DPKT model outperformed across all programming \nlanguage datasets. As show in Table  4, this indicates that the DPKT model is better at capturing changes in \nknowledge concepts and difficulty levels within programming problems. Models like EKT and THKT, which \nencode problems, performed well on certain datasets; for instance, THKT achieved an AUC of 75.2% on the \nC dataset, but their overall performance was still inferior to that of the DPKT model. By introducing graph \nattention mechanisms, DPKT enables finer semantic integration and knowledge state updates, significantly \nenhancing prediction accuracy. A core advantage of the DPKT model lies in its automated annotation of \nknowledge points and difficulty levels for programming problems using large language models. This automation \nimproves the model’s generalization ability, allowing it to capture students’ learning trajectories more precisely \nacross different difficulty levels, leading to more accurate predictions of their knowledge state. This design \nenables DPKT to flexibly adjust its predictions based on the content and difficulty of programming problems, \nsignificantly enhancing the model’s predictive performance.\nThrough comparative experiments with baseline models, our DPKT model demonstrated strong performance \nacross multiple programming language datasets, especially excelling in AUC and ACC metrics on C++ and C \ndatasets. Additionally, the DPKT model effectively enhanced the dynamic tracking of students’ knowledge state \nthrough automated annotation and multi-level attention mechanisms. The experimental results confirm the \nadvantages of the DPKT model in programming knowledge tracing and predicting student future performance, \nproviding new research directions for personalized teaching and knowledge assessment.\nMethods C++_AUC C++_ACC C_AUC C_ACC Java_AUC Java_ACC Python_AUC Python_ACC\nDKT1 70.9 69.7 71.7 66.5 71.8 69.7 74.0 68.0\nDKVMN8 71.5 69.9 66.9 64.2 71.8 70.3 73.8 69.0\nSAKT53 71.9 70.6 72.5 67.0 73.0 69.9 74.5 68.8\nEKT7 72.0 70.0 73.2 69.8 75.0 71.7 73.3 67.0\nCode-DKT19 71.2 69.9 71.8 67.4 71.8 67.9 72.5 69.1\nDKT+54 73.2 71.0 72.9 69.7 74.9 72.6 75.5 68.5\nDIMKT50 72.5 71.7 75.1 68.7 73.5 71.8 75.9 68.9\nQIKT55 73.4 71.6 72.4 67.3 72.2 70.8 73.8 69.5\nTCKT56 73.6 70.9 73.1 68.4 72.6 71.8 76.0 67.7\nTHKT57 73.8 71.8 75.2 70.0 74.5 72.5 74.9 70.0\nDPKT 76.2 74.5 75.7 71.5 75.4 72.9 76.8 68.1\nTable 4. Comparison of the AUC and ACC performance (%) of various knowledge tracing methods on the \nC++, C, Java, and Python datasets. The best results are highlighted in bold, and the second-best results are \nunderlined.\n \nScientific Reports |        (2025) 15:11475 11| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nKnowledge state modeling (RQ2)\nIn programming learning, assessing students’ knowledge state is crucial for effective knowledge tracing. \nProgramming problems often involve multiple knowledge points with varying levels of difficulty and complexity. \nCapturing students’ knowledge state in programming tasks through the DPKT model becomes key to evaluating \ntheir programming abilities. As shown in Fig. 3, the DPKT model illustrates changes in students’ knowledge state \nacross multiple knowledge points, especially as the difficulty and complexity of problems gradually increase, \nreflecting dynamic variations in their mastery of these points.\nFirst, overall, students’ knowledge state change with the increase in practice involving different difficulty levels. \nSecond, we find that problems of varying difficulty impact students’ knowledge state differently. For instance, the \nknowledge state growth for problems 9 and 10 differs significantly from that for problems 18 and 19. Medium-\ndifficulty problems (like 9 and 10) provide solid consolidation opportunities, ensuring students steadily improve \ntheir mastery while gradually adapting to more complex tasks. High-difficulty problems (like 18 and 19), while \nchallenging, can greatly enhance students’ knowledge state, particularly when related to prerequisite knowledge; \nthe benefits of practicing high-difficulty problems are even more pronounced.\nThird, the DPKT model can also explain the impact of problem difficulty on knowledge state by analyzing \nthe relationships between knowledge concepts. In the experiment, we found that the prerequisite relationships \nbetween knowledge concepts affected the degree to which students mastered the relevant knowledge concepts, \nwith high-difficulty problems having a greater impact on the mastery of these related knowledge concepts. This \ndemonstrates that the DPKT model effectively captures the dynamic changes in students’ knowledge state across \ndifferent knowledge concepts and problem difficulties, and it uses difficulty analysis to help explain variations \nin students’ knowledge mastery. The DPKT model provides strong data support for knowledge tracing in \nprogramming learning. By integrating and analyzing multidimensional features such as problem difficulty, \nknowledge concepts, and student responses, the model reveals patterns of how students’ knowledge state change \nwith increasing difficulty. This model offers a solid basis for optimizing personalized learning pathways.\nThe impact of data proportion fed into LLM on performance (RQ3)\nTo verify the impact of prior knowledge from different data volumes on model performance, we designed an \nexperiment to segment datasets from four programming languages (C++, C, Python, Java) and input them into \nthe model for training and evaluation. The experimental results, as shown in Fig. 4, reveal the following trends:\nFirst, the volume of data from different programming languages affects model performance (AUC and ACC) \ndifferently. As the training data volume increases, the model’s AUC and ACC metrics generally show an upward \ntrend, but this growth is not linear. For example, in the Python dataset, the AUC significantly improves with \ninitial increases in data volume, but the growth rate gradually slows when the volume reaches 3/4 or more. A \nsimilar phenomenon occurs in the C++, C, and Java datasets, where the increases in AUC and ACC tend to \nplateau as data volume grows.\nSecond, specifically, C++ exhibits the fastest growth rate when the data volume increases from 1/4 to 1/2, \nreflecting its sensitivity to data volume. As a powerful programming language, C++ has high flexibility and \ncomplexity, making it suitable for handling large amounts of complex algorithms and data structures, which may \nlead to significant AUC growth in the early stages. In contrast, Java’s AUC growth is relatively slow, particularly \nduring the 1/4 to 1/2 data volume stage, indicating weaker model adaptability to this language. Java is mainly \nused for enterprise-level applications and large-scale systems, and its strict syntax rules and relatively steep \nlearning curve may limit the model’s effectiveness with smaller data volumes. Comparatively, C and Python \nperform at a moderate level across different data volume stages, with Python starting at a higher point and \noverall performing better. The simplicity and widespread applicability of Python may enhance its learnability in \nFig. 3. The learning sequences and knowledge state evolution of two students across knowledge components \n(KCs) with different levels of difficulty. The difficulty levels are divided into four grades, with higher numerical \nvalues representing greater difficulty.\n \nScientific Reports |        (2025) 15:11475 12| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nprojects, especially in data science and artificial intelligence, further facilitating the model’s rapid understanding \nof its characteristics.\nBased on the results of this experiment, we draw preliminary conclusions regarding the optimal amount of \nprior knowledge for large language models: data volume, the characteristics of programming languages, and \nthe amount of prior knowledge collectively influence the model’s performance. Future research could further \nexplore how to optimize the allocation of prior knowledge according to different programming languages and \ndatasets to enhance model performance and achieve broader applicability.\nComparison of expert annotations of problem difficulty with results generated by the large \nlanguage model (RQ4)\nTo evaluate the differences and consistency between difficulty annotations generated by large language models \nand those made by human annotators, we designed a comparative experiment involving ten experienced \ncomputer education teachers. These teachers manually annotated the text understanding difficulty and \nknowledge concepts difficulty of problems in four programming languages. To ensure the comparability of the \nresults, all annotations used a uniform scale of 1 to 4, representing low, low-medium, medium-high, and high \ndifficulty (with 1 being the lowest and 4 the highest).\nWe matched the annotations generated by the large language model with those of the teachers and assessed \nthe consistency between the two using both quantitative and qualitative analysis methods. The matching \napproach primarily involved calculating the difference in difficulty levels for each question, categorizing the \nconsistency into three levels: fully consistent, acceptable deviation, and inconsistent. Subsequently, we quantified \nthe consistency scores and measured the overall performance of the model using the average consistency score. \nIn addition, we performed qualitative analysis of difficulty trends based on the data and compared the time costs \ninvolved, providing a comprehensive assessment of the large model’s potential in large-scale annotation tasks. \nThe specific methods are as follows:\n1. Matching Algorithm: For each programming problem i, the teacher’s annotated text understanding \ndifficulty is Ttext\ni  and the knowledge concepts difficulty is Tsyntax\ni , while the corresponding difficulties \ngenerated by the large language model are Mtext\ni  and Msyntax\ni . The difference in text understanding difficulty \nand knowledge concepts difficulty for each problem is calculated as:\n ∆text\ni =\n⏐⏐Ttext\ni − Mtext\ni\n⏐⏐  (21)\n ∆syntax\ni =\n⏐⏐Tsyntax\ni − Msyntax\ni\n⏐⏐  (22)\nBased on the size of the difference ∆i,the matching situations are categorized into three types 58,59: Perfect \nMatch: ∆text\ni ≤ 0.5 and ∆syntax\ni ≤ 0.5. Acceptable Deviation: 0.5 < ∆text\ni ≤ 1.0 or 0.5 < ∆syntax\ni ≤ 1.0. \nInconsistent: ∆text\ni > 1.0 or ∆syntax\ni > 1.0.\n2. Consistency Score: To quantify the consistency of each problem in terms of text understanding and \nknowledge concepts difficulty, we introduce the consistency score Si:\n \nSi =1 −\n(\n∆text\ni +∆ syntax\ni\n2\n)\n (23)\nHere, the range of Si is from 0 to 1, where 1 indicates perfect consistency and 0 indicates inconsistency.\n3. Average Consistency Score: We calculate the average consistency score ¯S for all problems to measure \noverall consistency:\nFig. 4. By inputting different proportions of raw data into the LLM to generate knowledge concepts, we can \nobserve its impact on the model’s AUC performance.\n \nScientific Reports |        (2025) 15:11475 13| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\n \n¯S = 1\nN\nN∑\ni=1\nSi (24)\nHere, N is the total number of problems. The results are shown in the table. The calculation formula for the \nKappa value is:\n \nKappa = Po − Pe\n1 − Pe\n (25)\nWhere Po is the observed consistency, which is the proportion of problems annotated consistently by both the \nexperts and the large model; Pe is the expected consistency, which is the consistency under random assignment. \nThe Kappa value ranges from [−1, 1]60,61, where:\n1 : Perfect consistency.\n0 : Consistency equivalent to random selection.\nNegative values : Consistency below random levels.\nSpecifically,\n0.01–0.20 : Almost no consistency.\n0.21–0.40 : Weak consistency.\n0.41–0.60 : Moderate consistency.\n0.61–0.80 : Strong consistency.\n0.81–1.00 : Almost complete consistency.\nThe results, as shown in Fig.  5, indicate that when analyzing the difficulty annotations for different \nprogramming languages, C++ and Python perform better than C and Java. Specifically, C++ has 75 problems \nmatched for text understanding difficulty, with a Kappa value of 0.57; for knowledge concepts difficulty, the \nnumber of matched problems is 80, with a Kappa value reaching 0.64, demonstrating high consistency. Python \nhas 80 matched problems for text understanding difficulty, with a Kappa value of 0.65, and 85 matched problems \nfor knowledge concepts difficulty, achieving a Kappa value of 0.73, indicating that the model can effectively \ncapture the difficulty characteristics of Python. In contrast, C has 70 matched problems for text understanding \ndifficulty, with a Kappa value of 0.50, and 65 matched problems for knowledge concepts difficulty, with a Kappa \nvalue of only 0.44, showing lower consistency. Additionally, Java’s Kappa values for text understanding and \nknowledge concepts difficulties are 0.53 and 0.47, respectively, overall performing worse than C++ and Python. \nThese data indicate that the large language model generally exhibits good annotation consistency when labeling \nthe difficulties of different programming languages, but there are also certain discrepancies. These differences \nreflect that the model’s annotation effectiveness is influenced by the characteristics of the programming \nlanguages, laying the groundwork for future work.\nModel component effectiveness evaluation (RQ5)\nThe DPKT model utilizes a large language model to accurately annotate the knowledge concepts and difficulty \nlevels of programming problems, integrating this difficulty-related semantic information into the process of \nupdating students’ knowledge state. To validate the key roles of the large language model, BERT model, and \ngraph attention network in modeling student knowledge state within DPKT, we conducted experimental analyses \nbased on datasets from four programming languages (C++, C, Java, Python) to assess the positive contributions \nof each component to the prediction of students’ knowledge state, as shown in Table 5.\nThe large language model plays a crucial role in updating student knowledge state, especially when handling \ncomplex programming language datasets (such as Python), where its understanding of problem and knowledge \nFig. 5. Comparison of the Consistency between Human Annotations and LLMs Annotations for text \nUnderstanding Difficulty and Knowledge Concepts Difficulty of 100 Problems in Four Programming \nLanguages.\n \nScientific Reports |        (2025) 15:11475 14| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\nconcepts difficulty significantly enhances model performance. Meanwhile, the CodeBERT model is essential for \ncapturing the semantic relationships between knowledge concepts and problems; its removal leads to a notable \ndecline in model performance, underscoring its indispensability in modeling interactive relationships. Although \nthe performance drop from removing the graph attention network is less pronounced than for the first two \nmodules, it still plays a positive role in integrating multidimensional information and handling complex problem \ndifficulties, indicating the collaborative effect of these modules in enhancing the overall capabilities of the model.\nThe difficulty information processed by the large language model provides more precise and detailed \nannotations of problem and knowledge concepts difficulties, laying a solid foundation for subsequent research. \nThis accurate difficulty information not only aids in optimizing the design of personalized learning paths for \nstudents but also improves the accuracy of knowledge state updates, enhancing the efficiency of educational \ntechnology applications. Furthermore, the deep modeling of the semantic relationships between knowledge \nconcepts and problems through the CodeBERT module further enhances the model’s understanding of student \nlearning states, increasing the overall reliability of predictions. The significance of the graph attention network \nin integrating multidimensional information enables effective consolidation of difficulty information from \nvarious sources, thereby improving the model’s ability to handle complex problem difficulties. Therefore, the \ncomprehensive research based on the large language model, CodeBERT, and graph attention network provides \nnew insights and directions for future model optimization, personalized learning path design, and educational \nresearch, promoting the further development of educational technology.\nAnalysis of the role of difficulty factors in knowledge tracing models (RQ6)\nAccurately assessing and tracking students’ knowledge state is crucial for improving learning efficiency and \ndeveloping personalized learning plans 1. However, variations in text understanding difficulty and knowledge \nconcepts difficulty across different problems can significantly affect students’ performance and the predictive \neffectiveness of models. In this experiment, we analyzed the impact of text understanding difficulty and \nknowledge concepts difficulty on student knowledge state updates, leading to several important conclusions. \nWe had students practice a series of programming problems with the same knowledge concepts difficulty but \nvarying understanding difficulties, as well as problems with the same text understanding difficulty but differing \nknowledge concepts difficulties. Their minimum knowledge state, final knowledge state, and optimal knowledge \nstate are illustrated in the radar chart in Fig. 6.\nWhen knowledge concepts difficulty is the same, and students practice programming problems with different \nunderstanding difficulties, their knowledge state do not change significantly. Moderate text understanding \nFig. 6. After practicing a series of programming problems with varying levels of difficulty, the radar chart of \nchanges in students’ knowledge state.\n \nMethods C++_AUC C++_ACC C_AUC C_ACC Java_AUC Java_ACC Python_AUC Python_ACC\nDPKT w/o LLM 74.8 73.2 74.3 70.1 74.0 71.4 75.5 66.9\nDPKT w/o BERT 73.5 72.1 73.1 69.0 73.0 70.5 74.1 65.4\nDPKT w/o GAT 75.2 73.8 74.5 70.8 74.7 71.9 75.9 67.2\nTable 5. Results of ablation studies on different programming language datasets.\n \nScientific Reports |        (2025) 15:11475 15| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\ndifficulty helps students better understand the problems, enhancing their performance and facilitating updates \nto their knowledge state. However, if text understanding difficulty is too high, students need to invest more effort \nto understand the problems, which can negatively affect the performance of some students. As a result, the model \nmay incorrectly assume that these students have not mastered the relevant knowledge, preventing a positive \nupdate to their knowledge state. Conversely, when text understanding difficulty is the same, and students practice \nprogramming problems with different knowledge concepts difficulties, their knowledge state show significant \nimprovement. After correctly analyzing the problem requirements, students practicing with problems of lower \nknowledge concepts difficulty can boost their confidence, foster their interest in learning, and promote further \nin-depth study. However, the improvement in knowledge state may be limited. In contrast, problems with higher \nknowledge concepts difficulty effectively differentiate students of varying levels, prompting greater cognitive \nenhancement, but may also demotivate some students and reduce their interest in learning. Therefore, moderate \nunderstanding and knowledge concepts difficulties can maintain a balance between student performance and \nknowledge state updates while ensuring high predictive accuracy for the model.\nBoth text understanding difficulty and knowledge concepts difficulty play vital roles in the knowledge \ntracing process, influencing not only student performance but also the model’s accuracy in predicting students’ \nknowledge state. Optimal levels of understanding and knowledge concepts difficulty can enhance students’ \nlearning outcomes and enable the model to more accurately track updates in their knowledge state. In research, \nsetting appropriate difficulty levels can help students gain a better learning experience while improving the \nmodel’s predictive accuracy. Future studies could further explore the best application scenarios for different \ndifficulty combinations in knowledge state updates, providing richer references for optimizing educational \nmodels.\nCase study (RQ7)\nComprehension difficulty of problems is a key factor affecting the performance of student knowledge state \nprediction models 50. Through the analysis of problems of varying difficulty, we found that both excessively \nhigh and low comprehension difficulty can significantly impact the accuracy of model predictions. When \ncomprehension difficulty is not taken into account, the model may rely on relatively simple knowledge concepts \ndifficulty to predict student performance. However, even with lower knowledge concepts difficulty, students \nmay fail to complete problems due to high comprehension difficulty, leading to biased predictions. For example, \na student encountered a series of problems based on “loop structures” where the comprehension difficulty \nwas rated as 1 and the knowledge concepts difficulty as 2. The first problem had low comprehension difficulty, \nand the student was able to complete it successfully, with the model accurately predicting that the student had \nmastered the relevant knowledge. In contrast, the third problem had the same knowledge concepts difficulty as \nthe previous one, but due to an increase in comprehension difficulty, the student faced challenges. However, due \nto the complexity of the problem’s wording, the model failed to recognize this, incorrectly predicting that the \nstudent had mastered the knowledge concepts.\nAs shown in Fig.  7, when comprehension difficulty is too high, students often struggle to accurately \nunderstand the problem, failing to adequately demonstrate their knowledge mastery. In this case, even though \nthe model predicts that the student can correctly complete the problem based on lower knowledge concepts \ndifficulty, the understanding barrier leads to the student not completing the task as expected, resulting in the \nmodel incorrectly updating their knowledge state to “not mastered. ” This bias introduced by high comprehension \ndifficulty demonstrates that relying solely on knowledge concepts difficulty for predictions is one-sided. In \ncontrast, in problems with moderate comprehension difficulty, students can successfully understand the problem \nand respond correctly, allowing the model to accurately capture the student’s knowledge state and update it to \n“mastered. ”\nOn the other hand, while problems with low comprehension difficulty can help build student confidence, if \nthe problems are too simple, students may easily complete them but fail to effectively enhance their knowledge \nlevel. As a result, the model might misjudge the student’s true learning state, leading to updates in knowledge \nstate that lack reference value.\nTherefore, considering comprehension difficulty plays an important corrective role in student knowledge \nstate prediction models. By integrating comprehension difficulty with knowledge concepts difficulty, the model \nFig. 7. The impact of text understanding difficulty in textual problems with varying difficulty levels for the \nsame knowledge concept on model prediction results.\n \nScientific Reports |        (2025) 15:11475 16| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\ncan more comprehensively evaluate student learning performance. For instance, when the model predicts a \nproblem with low knowledge concepts difficulty that the student should be able to complete, incorporating \ncomprehension difficulty can correct potential biases. In situations where comprehension difficulty is high, even \nif the grammar is simple, the model can reasonably anticipate that the student might perform poorly due to \nunclear problem wording, thus avoiding erroneous updates to their knowledge state. Comprehension difficulty \nshould not be overlooked in knowledge state prediction models. By introducing comprehension difficulty, the \nmodel can more accurately capture the student’s true knowledge level, avoiding performance declines caused by \nimbalances in problem features. This not only improves the accuracy of predictions but also provides stronger \nsupport for designing personalized learning paths. Future research should further explore how to dynamically \nadjust the difficulty of different problems to better help students master knowledge while enhancing the \nperformance of knowledge tracing models.\nConclusions and limitations\nThis paper proposes a difficulty-aware programming knowledge tracing with large language models. The model \ninputs programming problems and knowledge concepts into the CodeBERT pre-trained model, leveraging \ndifficulty annotations generated by the large language model, combined with attention mechanisms and graph \nattention mechanisms, to create a semantic matrix of difficulty. Through the knowledge update layer, the \nmodel can dynamically adjust the student’s knowledge state and predict their future mastery of knowledge. \nBy combining the comprehension difficulty of problems with the difficulty of knowledge concepts, the model \nprovides a more comprehensive and accurate tracking of the student’s learning state.\nThe model not only evaluates the student’s grasp of knowledge concepts but also captures the challenges \nstudents face in problem comprehension, thereby enhancing prediction accuracy. With the help of attention \nmechanisms and graph attention networks, the model effectively integrates complex problem information \nand updates the student’s knowledge state. As students complete problems of varying difficulty, the model \ndynamically adjusts their knowledge mastery state, reflecting their learning progress in real time.\nAlthough the model demonstrates good performance, there are still areas for further improvement and \noptimization. For instance, future research could consider integrating more data sources generated during \nthe learning process, such as code debugging information, time management, and even emotional data. By \nincorporating multimodal information, the model would gain a more comprehensive understanding of student \nlearning behaviors, thus improving the accuracy of predictions and knowledge tracing. Current models primarily \nfocus on short-term knowledge state updates; in the future, a long-term prediction module could be introduced \nto combine students’ historical performance and long-term learning paths, predicting changes in knowledge \nmastery over extended periods. This would facilitate better adjustments to curricula and teaching strategies. \nWith these improvements, future knowledge tracing models will be more personalized and intelligent, helping \nstudents master knowledge more efficiently in programming learning, while also providing educators with more \nprecise teaching strategy support.\nData availability\nThe dataset analyzed during the current research period can be found in the Project Code net repository at \nhttps://github.com/IBM/Project_CodeNet.git. The code for this study can be found at the following GitHub \nrepository: https://github.com/ELLE157/DPKT.\nReceived: 24 October 2024; Accepted: 28 March 2025\nReferences\n 1. Piech, C. et al. Deep knowledge tracing. Advances in neural information processing systems 28 (2015).\n 2. Shen, S. et al. A survey of knowledge tracing: Models, variants, and applications. IEEE Trans. Learn. Technol.  h t t p s : / / d o i . o r g / 1 0 . 1 1 \n0 9 / T L T . 2 0 2 4 . 3 3 8 3 3 2 5     (2024).\n 3. Wang, J. et al. How problem difficulty and order influence programming education outcomes in online judge systems. Heliyon \n9(11), e20947 (2023).\n 4. Piwek, P . & Savage, S. Challenges with learning to program and problem solve: An analysis of student online discussions. in \nProceedings of the 51st ACM Technical Symposium on Computer Science Education, 494–499 (2020).\n 5. Minn, S., Yu, Y ., Desmarais, M. C. et al. Deep knowledge tracing and dynamic student classification for knowledge tracing. in 2018 \nIEEE International Conference on Data Mining (ICDM), 1182–1187 (IEEE, 2018).\n 6. Li, S. et al. Sthkt: Spatiotemporal knowledge tracing with topological Hawkes process. Expert Syst. Appl. 259, 125248 (2025).\n 7. Liu, Q. et al. Ekt: Exercise-aware knowledge tracing for student performance prediction. IEEE Trans. Knowl. Data Eng. 33, 100–115 \n(2019).\n 8. Zhang, J., Shi, X., King, I. & Y eung, D.-Y . Dynamic key-value memory networks for knowledge tracing. in Proceedings of the 26th \nInternational Conference on World Wide Web, 765–774 (2017).\n 9. Ghosh, A., Heffernan, N. & Lan, A.  S. Context-aware attentive knowledge tracing. in Proceedings of the 26th ACM SIGKDD \nInternational Conference on Knowledge Discovery & Data Mining, 2330–2339 (2020).\n 10. Tong, S. et al. Structure-based knowledge tracing: An influence propagation view. in 2020 IEEE international conference on data \nmining (ICDM), 541–550 (IEEE, 2020).\n 11. Nakagawa, H., Iwasawa, Y . & Matsuo, Y . Graph-based knowledge tracing: modeling student proficiency using graph neural \nnetwork. in IEEE/WIC/ACM International Conference on Web Intelligence, 156–163 (2019).\n 12. Sun, X. et al. Daskt: A dynamic affect simulation method for knowledge tracing. IEEE Trans. Knowl. Data Eng.  h t t p s : / / d o i . o r g / 1 0 . \n1 1 0 9 / T K D E . 2 0 2 5 . 3 5 2 6 5 8 4     (2025).\n 13. Passalis, N., Tzelepi, M. & Tefas, A. Probabilistic knowledge transfer for lightweight deep representation learning. IEEE Trans. \nNeural Netw. Learn. Syst. 32, 2030–2039 (2020).\n 14. Qiu, Y ., Qi, Y ., Lu, H., Pardos, Z. A. & Heffernan, N. T. Does time matter? Modeling the effect of time with bayesian knowledge \ntracing. in Educational Data Mining, 139–148 (2011).\nScientific Reports |        (2025) 15:11475 17| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\n 15. Zhu, R. et al. Programming knowledge tracing: A comprehensive dataset and a new model. in 2022 IEEE International Conference \non Data Mining Workshops (ICDMW) 298–307 (IEEE, 2022).\n 16. Qiu, Y ., Qi, Y ., Lu, H., Pardos, Z. A. & Heffernan, N. T. Application of deep self-attention in knowledge tracing. arXiv preprint \narXiv:2105.07909 (2021).\n 17. Pan, J., Dong, Z., Y an, L. & Cai, X. Knowledge graph and personalized answer sequences for programming knowledge tracing. \nAppl. Sci. 14, 7952 (2024).\n 18. Sun, X. et al. Lgs-kt: Integrating logical and grammatical skills for effective programming knowledge tracing. Neural Netw. 185, \n107164 (2025).\n 19. Shi, Y ., Chi, M., Barnes, T. & Price, T. Code-dkt: A code-based knowledge tracing model for programming tasks. arXiv preprint \narXiv:2206.03545 (2022).\n 20. Jeon, J. & Lee, S. Large language models in education: A focus on the complementary relationship between human teachers and \nchatgpt. Educ. Inf. Technol. 28, 15873–15892 (2023).\n 21. Liu, F . et al. Fuzzy Bayesian knowledge tracing. IEEE Trans. Fuzzy Syst. 30, 2412–2425 (2021).\n 22. Pelanek, R. Bayesian knowledge tracing, logistic models, and beyond: An overview of learner modeling techniques. User Model. \nUser-Adap. Inter. 27, 313–350 (2017).\n 23. Cen, H., Koedinger, K. & Junker, B. Learning factors analysis–a general method for cognitive model evaluation and improvement. \nin International Conference on Intelligent Tutoring Systems, 164–175 (Springer, 2006).\n 24. Pavlik, P .  I., Cen, H. & Koedinger, K.  R. Performance factors analysis–a new alternative to knowledge tracing. in Artificial \nintelligence in education, 531–538 (Ios Press, 2009).\n 25. Li, J., Deng, Y ., Mao, S. et al.  Knowledge-associated embedding for memory-aware knowledge tracing. in IEEE Transactions \nonComputational Social Systems (2023).\n 26. Cheung, L. P . & Y ang, H. Heterogeneous features integration in deep knowledge tracing. in International Conference on Neural \nInformation Processing, 653–662 (Springer, 2017).\n 27. Vie, J. J. & Kashima, H. Knowledge tracing machines: Factorization machines for knowledge tracing. in Proceedings of the AAAI \nconference on artificial intelligence 33, 750–757 (2019).\n 28. Shen, S. et al. Convolutional knowledge tracing: Modeling individualization in student learning process. in Proceedings of the 43rd \nInternational ACM SIGIR Conference on Research and Development in Information Retrieval, 1857–1860 (2020).\n 29. Wang, D., Zhang, L., Zhao, Y . et al. Deep knowledge tracking integrating programming exercise difficulty and forgetting factors. in \nInternational Conference on Intelligent Computing, 192–203 (Springer Nature Singapore, 2024).\n 30. Scherer, R., Siddiq, F . & Viveros, B. S. A. A meta-analysis of teaching and learning computer programming: Effective instructional \napproaches and conditions. Comput. Hum. Behav. 109, 106349 (2020).\n 31. Kazemitabaar, M. et al. Codeaid: Evaluating a classroom deployment of an llm-based programming assistant that balances student \nand educator needs. in Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–20 (2024).\n 32. Xu, S., Li, Z., Mei, K. & Zhang, Y . Core: Llm as interpreter for natural language programming, pseudo-code programming, and flow \nprogramming of AI agents. arXiv preprint arXiv:2405.06907 (2024).\n 33. Nam, D., Macvean, A., Hellendoorn, V ., Vasilescu, B. & Myers, B. Using an llm to help with code understanding. in Proceedings of \nthe IEEE/ACM 46th International Conference on Software Engineering, 1–13 (2024).\n 34. Cai, Y . et al. Low-code llm: Visual programming over llms. arXiv preprint arXiv:2304.081032 (2023).\n 35. Li, W .-D. & Ellis, K. Is programming by example solved by llms? arXiv preprint arXiv:2406.08316 (2024).\n 36. Agarwal, A. et al. Copilot evaluation harness: Evaluating llm-guided software programming. arXiv preprint arXiv:2402.14261 \n(2024).\n 37. Yu, Y . et al.  Eckt: Enhancing code knowledge tracing via large language models. in Proceedings of the Annual Meeting of the \nCognitive Science Society, vol. 46 (2024).\n 38. Fu, L. et al. Sinkt: A structure-aware inductive knowledge tracing model with large language model. arXiv preprint arXiv:2407.01245 \n(2024).\n 39. Vadaparty, A. et al. Cs1-llm: Integrating llms into cs1 instruction. in Proceedings of the 2024 on Innovation and Technology in \nComputer Science Education V . 1 297–303 (2024).\n 40. Kazemitabaar, M. et al. How novices use llm-based code generators to solve cs1 coding tasks in a self-paced learning environment. \nin Proceedings of the 23rd Koli Calling International Conference on Computing Education Research 1–12 (2023).\n 41. Chen, J.-T. & Huang, C.-M. Forgetful large language models: Lessons learned from using llms in robot programming. in Proceedings \nof the AAAI Symposium Series 2, 508–513 (2023).\n 42. Ashurova, M. & Ashurov, M. The role and significance of the concepts of hard skill and soft skill in teaching it and programming \nlanguages. J. Pedagog. Invent Pract. 18, 68–70 (2023).\n 43. Wang, S., Xu, T., Li, H. et al. Large language models for education: A survey and outlook. arXiv preprint arXiv:2403.18105 (2024).\n 44. Alhafni, B., Vajjala, S., Bannò, S. et al.  Llms in education: Novel perspectives, challenges, and opportunities. arXiv preprint \narXiv:2409.11917 (2024).\n 45. Sun, X. et al. Harnessing domain insights: A prompt knowledge tuning method for aspect-based sentiment analysis[J]. Knowl. \nBased Syst. 298, 111975 (2024).\n 46. Hwang, G.-J. & Chen, C.-H. Evaluating the difficulty of programming tasks using a hybrid approach. Computers & Education \n(2017).\n 47. Dreyfus, S.  E. & Dreyfus, H.  L. A five-stage model of the mental activities involved in directed skill acquisition. in Human \nSimulation for Training and Education (1980).\n 48. Huang, H. & Lin, T. (A framework for classification. Educational Technology & Society, Assessing programming task difficulty, \n2018).\n 49. Puri, R. et al. Codenet: A large-scale ai for code dataset for learning a diversity of coding tasks[J]. arXiv preprint arXiv:2105.12655 \n(2021).\n 50. Shen, S. et al. Assessing student’s dynamic knowledge state by exploring the question difficulty effect. in Proceedings of the 45th \nInternational ACM SIGIR Conference on Research and Development in Information Retrieval, 427–437 (2022).\n 51. Hooper, M., Broer, M., Y arnell, L. M. & Holmes, J. Talking about teachers would sampling weight adjustments allow for teacher \ncentric inferences in future timss assessments. Stud. Educ. Eval. 73, 101148 (2022).\n 52. Feng, Z. et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155 (2020).\n 53. Pandey, S. & Karypis, G. A self-attentive model for knowledge tracing. arXiv preprint arXiv:1907.06837 (2019).\n 54. Y eung, C.  K. & Y eung, D.  Y . Addressing two problems in deep knowledge tracing via prediction-consistent regularization. in \nProceedings of the fifth annual ACM conference on learning at scale 1–10 (2018).\n 55. Chen, J., Liu, Z., Huang, S., Liu, Q. & Luo, W . Improving interpretability of deep sequential knowledge tracing models with \nquestion-centric cognitive representations. arXiv preprint arXiv:2302.06885 (2023).\n 56. Huang, C. et al. Learning consistent representations with temporal and causal enhancement for knowledge tracing. Expert Syst. \nAppl. 245, 123128 (2024).\n 57. Sun, X. et al. Target hierarchy-guided knowledge tracing: Fine-grained knowledge state modeling. Expert Syst. Appl. 251, 123898 \n(2024).\n 58. Baker, R.  S., Martin, T. & Rossi, L.  M. Educational data mining and learning analytics. The Wiley handbook of cognition and \nassessment: Frameworks, methodologies, and applications 379–396 (2016).\nScientific Reports |        (2025) 15:11475 18| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/\n 59. Doshi-Velez, F . & Kim, B. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608 (2017).\n 60. McHugh, M. L. Interrater reliability: The kappa statistic. Biochemia medica 22, 276–282 (2012).\n 61. Viera, A. J. & Garrett, J. M. Understanding interobserver agreement: The kappa statistic. Fam. Med. 37, 360–363 (2005).\nAcknowledgements\nThis work was supported by Guizhou Provincial Higher Education Undergraduate Teaching Content and Cur-\nriculum System Reform Project under Grants GZJG2024331 and GZJG2024323; Guizhou Provincial Science \nand Technology Projects (QKHJC[2024]youth012); Guizhou Province First-Class Undergraduate Course under \nGrant GZSy1kc202229; Humanities and Social Sciences Research Project of Guizhou Provincial Universities \nunder Grant 2024RW171; Guizhou Provincial Philosophy and Social Science Research Special Fund (Research \non the Spirit of the National Education Conference); Liupanshui Normal University Teaching Research and \nReform Project under Grant 2024-07-02; Liupanshui Normal University Institutional Course Ideological and \nPolitical Education Reform Project, Grant 2024-08-024; Ministry of Education Industry-Academia Cooperation \nand Collaborative Talent Cultivation Project, Grant 241202802121104.\nAuthor contributions\nLina Y ang and Xinjie Sun proposed the ideas and methodologies, designed the experimental scheme, and wrote \nthe main manuscript text. Hui Li and Ran Xu assisted in the experiment and prepared the figures. Xuqin Wei \nreviewed and modified the manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to X.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:11475 19| https://doi.org/10.1038/s41598-025-96540-3\nwww.nature.com/scientificreports/"
}