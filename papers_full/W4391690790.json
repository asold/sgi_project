{
    "title": "The Performance of Large Language Models on Quantitative and Verbal Ability Tests: Initial Evidence and Implications for Unproctored High-stakes Testing",
    "url": "https://openalex.org/W4391690790",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5102979627",
            "name": "Jasper Leo Wolf",
            "affiliations": [
                "Virginia Tech",
                "University of Pennsylvania"
            ]
        },
        {
            "id": "https://openalex.org/A2580683751",
            "name": "Patrick Dunlop",
            "affiliations": [
                "Curtin University"
            ]
        },
        {
            "id": "https://openalex.org/A2620097988",
            "name": "Louis Hickman",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1979462775",
        "https://openalex.org/W2101049208",
        "https://openalex.org/W2159670345",
        "https://openalex.org/W4385236484",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4383913712",
        "https://openalex.org/W21230259",
        "https://openalex.org/W4387929411",
        "https://openalex.org/W2005721028",
        "https://openalex.org/W1976351130",
        "https://openalex.org/W4378470708",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W3181182990",
        "https://openalex.org/W2076115808",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4385440981",
        "https://openalex.org/W2063866393",
        "https://openalex.org/W2127013010",
        "https://openalex.org/W1500843515",
        "https://openalex.org/W4287799740",
        "https://openalex.org/W4328049044",
        "https://openalex.org/W4388787315",
        "https://openalex.org/W2047444967",
        "https://openalex.org/W4387398178",
        "https://openalex.org/W4365512576",
        "https://openalex.org/W4389452675",
        "https://openalex.org/W4200365118",
        "https://openalex.org/W4353112996",
        "https://openalex.org/W2891202243",
        "https://openalex.org/W2073766844",
        "https://openalex.org/W2163534692",
        "https://openalex.org/W4312090934",
        "https://openalex.org/W4312050248",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4387232979",
        "https://openalex.org/W4387560599",
        "https://openalex.org/W4385430086",
        "https://openalex.org/W2136971664",
        "https://openalex.org/W2335790927"
    ],
    "abstract": "Unproctored assessments are widely used in pre-employment assessment. However, the recent emergence of widely accessible large language models (LLMs) poses challenges for unproctored personnel assessments, given that applicants may use them to artificially inflate their scores beyond their true abilities. This may be particularly concerning in cognitive ability testing, which is widely used and is thought to be less fakeable by humans than personality tests. Thus, this study compares the performance of LLMs on two common types of cognitive tests: quantitative ability and verbal ability. The tests investigated are used in real-world, high-stakes selection. We also examine the performance of the LLMs across different test formats (i.e., open-ended vs. multiple choice). Further, we contrast the performance of two LLMs (GPT-3.5 and GPT-4) across multiple prompt approaches and ‘temperature’ settings (i.e., a parameter that determines the amount of randomness in the model’s output). We found that the LLMs performed extremely poorly on the quantitative ability test and well on the verbal ability test, even when accounting for the test format. GPT-4 outperformed GPT-3.5 across both types of tests. Notably, although prompt approaches and temperature settings did affect LLM test performance, those effects were mostly minor relative to differences across tests and language models. We provide recommendations for securing pre-employment testing against LLM influences. Additionally, we call for rigorous research investigating the prevalence of LLM usage in pre-employment testing as well as on how LLM usage influences selection test validity.",
    "full_text": "Large Language Model Performance on Cognitive Tests    \nThe Performance of Large Language Models on Quantitative and Verbal Ability Tests: \nInitial Evidence and Implications for Unproctored High-stakes Testing   \n \nLouis Hickman1, Patrick D. Dunlop2, & Jasper Leo Wolf3 \n1Department of Psychology, Virginia Tech; The Wharton School of the University of \nPennsylvania \n2Future of Work Institute, Faculty of Business and Law, Curtin University \n3Arctic Shores \n \nThis manuscript is currently under peer review and may be revised. \n  \nLarge Language Model Performance on Cognitive Tests    \nThe Performance of Large Language Models on Quantitative and Verbal Ability Tests: \nInitial Evidence and Implications for Unproctored High-stakes Testing \n \nAbstract. Unproctored assessments are widely used in pre-employment assessment. However, \nthe recent emergence of widely accessible large language models (LLMs) poses challenges \nfor unproctored personnel assessments, given that applicants may use them to artificially \ninflate their scores beyond their true abilities. This may be particularly concerning in \ncognitive ability testing, which is widely used and is thought to be less fakeable by humans \nthan personality tests. Thus, this study compares the performance of LLMs on two common \ntypes of cognitive tests: quantitative ability and verbal ability. The tests investigated are used \nin real-world, high-stakes selection. We also examine the performance of the LLMs across \ndifferent test formats (i.e., open-ended vs. multiple choice). Further, we contrast the \nperformance of two LLMs (GPT-3.5 and GPT-4) across multiple prompt approaches and \n‘temperature’ settings (i.e., a parameter that determines the amount of randomness in the \nmodel’s output). We found that the LLMs performed extremely poorly on the quantitative \nability test and well on the verbal ability test, even when accounting for the test format. GPT-\n4 outperformed GPT-3.5 across both types of tests. Notably, although prompt approaches and \ntemperature settings did affect LLM test performance, those effects were mostly minor \nrelative to differences across tests and language models. We provide recommendations for \nsecuring pre-employment testing against LLM influences. Additionally, we call for rigorous \nresearch investigating the prevalence of LLM usage in pre-employment testing as well as on \nhow LLM usage influences selection test validity. \nKeywords: Artificial intelligence; chatbots; generative pretrained transformer; large language \nmodels; cognitive ability testing \n  \nLarge Language Model Performance on Cognitive Tests    \nPractitioner Points  \n▪ Job candidates may use large language models like ChatGPT to complete ability tests \non their behalf, but we currently know little about how well these models perform on \ncommercial cognitive ability tests. \n▪ OpenAI’s (free) GPT-3.5 and (paid subscription) GPT-4 models both performed very \npoorly on a quantitative ability test. \n▪ GPT-4 achieved results above the 90th percentile on the verbal ability test, whereas \nGPT-3.5 scored at approximately the 60th percentile. Thus, verbal ability tests may be \nvery vulnerable to large language models. \n▪ Temperature settings did not substantially affect the performance of the large \nlanguage models.  \n▪ While prompt approaches generally had minimal effects on the performance of the \nlarge language models, minimalistic prompt approaches sometimes resulted in worse \nscores than more detailed prompt approaches. \n \nLarge Language Model Performance on Cognitive Tests    1 \nThe Performance of Large Language Models on Quantitative and Verbal Ability Tests: \nInitial Evidence and Implications for Unproctored High-stakes Testing \nLarge language models (LLMs), such as Claude, Bard, LLaMA, and Generative \nPretrained Transformers (GPT) that power ChatGPT, recently emerged as powerful artificial \nintelligence (AI) tools that are highly accessible and offer a huge range of potential use cases. \nOf particular concern to personnel assessment and selection is the case where a job candidate \nuses an LLM to assist with, or complete on their behalf, an unproctored assessment. Our \nstudy aims to advance our understanding of how well LLMs perform on unproctored \ncognitive ability tests used in high-stakes selection, thus providing important insight into the \npotential threat of LLMs to unproctored pre-employment assessments.  \nUpon its release to the general public, OpenAI detailed GPT-4’s performance on a \nrange of program entry and professional licensure assessments, including the graduate record \nexamination (GRE), SAT, law SAT (LSAT), and the Uniform Bar Exam (OpenAI, 2023). In \nmany cases, the LLM responses scored at or above the 90th percentile, compared to human \ntest takers. Independent research teams have subsequently examined the performance of \nLLMs in relation to, for example, medical situational judgment tests (Borchert et al., 2023), \nmedical knowledge and ‘soft skills’ assessments (Brin et al., 2023), and both single-stimulus \nand forced-choice personality assessments (Phillips & Robie, 2024). These studies’ findings \nalign with OpenAI’s report: advanced LLMs out-score most human test takers on several \ntypes of tests. This raises new concerns about unproctored employment testing, given that \ntests that are more difficult to fake—like cognitive ability tests—are now potentially fakeable \nby anyone with access to an LLM. \nTo investigate the extent to which job applicants may be able to cheat using LLMs, \nwe investigate GPT’s performance on two commercially available cognitive ability tests used \nfor personnel selection. Given the rapid growth in the use of LLMs (Porter, 2023) and that, at \nLarge Language Model Performance on Cognitive Tests    2 \nthe time of writing, LLM usage was highest among people aged 34 and under (Similar Web, \n2023), the tests we examined are used primarily as remote, unproctored tests for screening \nrecent college graduates (Similar Web, 2023). Backed by research suggesting that \nunproctored pre-employment testing does not lead to substantially inflated scores or \ndecreases in criterion-related validity (e.g., Beaty et al., 2011; Lievens & Burke, 2011), many \nemployers now routinely assess candidates remotely. However, if LLMs score well on \nvalidated cognitive ability tests designed for personnel selection, our field may need to re-\nassess concerns about unproctored testing.  \nOur investigation provides three major contributions. First, we compare LLM \nperformance on two common types of cognitive tests: quantitative ability and verbal ability. \nFurther, we examine performance across different response formats (i.e., open-ended vs. \nmultiple choice). As we explain in detail below, the foundations of LLMs imply that certain \ntest characteristics (e.g., subject matter, response format) will likely affect their vulnerability \nto LLM-based cheating. \nSecond, we contrast the performance of two LLMs, namely GPT-3.5 and GPT-4. \nSuch a comparison is critical to understanding the capacity of LLMs in principle, and the \nperformance of LLMs in practice as candidates would use them. Importantly, at the time of \nwriting, GPT-3.5 is freely available via ChatGPT and may therefore be most representative of \nthe typical use case by candidates. Its model architecture and training data, however, is \nconsiderably smaller than that of GPT-4, which requires a paid subscription to access via \nChatGPT (at time of writing, US$20/month), or per-token access via the application \nprogramming interface (API).1 The subscription may prove a barrier to entry for candidates \nwith fewer financial resources or those simply preferring not to pay the fee. Further, the API \n \n1 Notably, a version of GPT-4 is accessible via Microsoft Bing but the model is tweaked to also engage in real-\ntime web search. \nLarge Language Model Performance on Cognitive Tests    3 \nis likely a major barrier to access for candidates without requisite technical skills. \nThird, we also examine the impact of different prompt approaches. As we explain in \ndetail below, it is now well established that the nature, depth, and quality of responses that a \nLLM generates is dependent on the content of the input provided by the user: the “prompt” \n(e.g., chain-of-thought; Wang et al., 2022; Wei et al., 2022). It remains less clear, however, \nprecisely if and how prompt approaches affect LLM scores on cognitive tests. \nLarge Language Models  \nLarge language models are one of the most recent, significant developments in the \nfield of natural language processing (NLP). Such LLMs operate similarly to the predictive \ntext function on a cell phone, email client, or word processor but involve larger model \narchitectures and more training data. These LLMs utilize deep learning—or neural networks \nwith many hidden layers and nodes per layer—and are trained on huge natural language text \ncorpora (i.e., collections of documents, websites, and books) to conduct next word prediction \n(e.g., Brown et al., 2020). The training process involves giving the LLM text, having it \npredict the next word, then updating the model's weights to improve the accuracy of the \nprediction, and repeating that process an extremely large number of times. Some recent \nmodels have additionally been trained with reinforcement learning using human and AI \nfeedback on previous models’ outputs (Ray, 2023).  \nLLMs gain a surprisingly robust probabilistic representation of the grammar, syntax, \nand semantics of natural language. When LLMs generate new text (e.g., as occurs when \nquerying ChatGPT), responses are generated one word at a time: the first word is generated, \nthen it is appended to the model’s input and the second word is generated, which is then \nappended to the model input, and so on, until the response is complete. Given the nature of \nLLMs, their representation of semantics and their predictions (i.e., responses) are strongly \ninfluenced by context, which is determined by the training data and the user-provided \nLarge Language Model Performance on Cognitive Tests    4 \nprompt.  \n The first LLM from OpenAI was GPT-1, which used a transformer architecture. GPT-\n1’s neural network included 117 million parameters (i.e., weights connecting neurons and a \nconstant “bias” added at each neuron; Radford et al., 2018). GPT-2 used a much larger neural \nnetwork, with 1.5 billion parameters, and it was trained on more data than GPT-1 (Radford et \nal., 2019). As a result, GPT-2 generated text was much more human-like than text from GPT-\n1. This evolution continued with GPT-3, which included 175 billion parameters in the neural \nnetwork and, again, used more training data—300 billion tokens (a token is the basic unit that \nLLMs process, a word or subword—on average, one word is approximately .75 tokens; \nBrown et al., 2020). GPT-3 also had a larger context window than earlier models at 2,049 \ntokens (3.5 had a context window of 3,096 tokens, which is the maximum length of content \nthe model can accept when generating responses). GPT-4 grew further, with researchers \nestimating that the model has 1.76 trillion parameters in its neural network and that it was \ntrained on 14 trillion tokens (Schreiner, 2023). The minimum context window of the GPT-4 \nclass of models is 8,192 tokens. GPT-4 was additionally trained on human-generated \nfeedback on output sequences generated through ChatGPT, and it can accept images or text \nas input (Ray, 2023). As of April 2024, the LMSYS Chatbot Arena Leaderboard \n(https://chat.lmsys.org/?leaderboard) suggests that several other LLMs perform comparably \nto GPT-4, including Anthropic’s CLAUDE 3, Mistral AI’s Mistral, and Google’s \nBard/Gemini Pro. \n Despite the impressive performance of GPT-4 and other LLMs on many tasks, they \nare probabilistic in nature and lack consciousness. They learn the correlations between \ndifferent tokens very well, leading them to display behaviors that some people interpret as \nhuman-like understanding (Mitchell & Krakauer, 2023). However, the brittleness of these \nsystems illustrates that they do not have any understanding of what is being asked of them \nLarge Language Model Performance on Cognitive Tests    5 \nnor what they reply. Instead, they are merely neural networks that use statistics to predict the \nnext, most likely word, given the inputs. It is well known that LLM outputs are imperfect and \ncan include misleading or outright false statements. Some have termed errors by an LLM as \n“hallucinations” (Maynez et al., 2020). When trained on language, the models get access to \nform, but they do not get access to semantics (Bender et al., 2021). Thus, these models cannot \nlead to a more general AI that truly understands its inputs, outputs, and the consequences of \nits actions (Floridi & Chiriatti, 2020). This suggests that additional increases in neural \nnetwork and training data size will provide asymptotic benefits unless the nature of the \nmodels is fundamentally altered. In other words, continuing to simply increase the number of \nparameters, the size of the context and training data will likely lead to an asymptote in \nperformance gains, and it may indeed be that the current best-performing models are already \nclose to that point. Altogether, while LLMs have many powerful use cases and can be \nbeneficial for completing a variety of tasks (e.g., Budhwar et al., 2023), their performance on \nany given task is a function of their architecture (i.e., the size and shape of the neural \nnetwork; the optimization function used), the quality, diversity, and size of their training data, \nand the prompt inputted to them.  \nUnproctored Cognitive Ability Testing \nSeminal influential meta-analytic work in industrial and organizational psychology by \nSchmidt and Hunter (1998) concluded that cognitive ability tests are among the strongest \npredictors of job performance across a variety of occupations (while updated meta-analyses \nsuggest smaller validities, they still rank among the strongest predictors; Sackett et al., 2022). \nHistorically, cognitive ability testing would be a proctored, face-to-face ‘paper-and-pencil’ \nactivity. However, over the past three decades, technology enabled the development of \nunproctored internet testing, affording flexibility to candidates and scale to employers.  \nInitially, the pivot to remote, unproctored testing practices raised concerns among \nLarge Language Model Performance on Cognitive Tests    6 \nacademics, particularly around the potential for candidates to ‘cheat’ in an unproctored \nenvironment by receiving assistance or having an acquaintance take the test on the \ncandidate’s behalf (e.g., Tippins et al., 2006). Indeed, cognitive ability test scores are, on \naverage, slightly higher when unproctored than when proctored (Steger et al., 2020).  Other \nresearch, however, suggests that unproctored tests function similarly to proctored versions \n(Templer & Lange, 2008) that base rates of cheating on unproctored tests or suspicious \nscoring patterns are low (Arthur et al., 2010; Kantrowitz & Dainis, 2014; Nye et al., 2008). In \nother words, even if some candidates were receiving assistance completing unproctored tests \nor entirely outsourcing test completion to others, detrimental effects on the integrity of the \nselection system across the candidate pool are considered negligible. Altogether, to this point, \ncognitive test performance would seem to represent a (largely) honest, difficult-to-fake, \nsignal of the candidates’ ability (Bangerter et al., 2012).  \nOn the one hand, the apparent absence of cheating effects on test integrity might \nsignal a general reluctance of candidates to cheat, implying that even the availability of \nLLMs should not give cause for further concern about unproctored internet testing. On the \nother hand, research has shown that a non-trivial proportion of candidates are willing to \nexaggerate or fabricate information when completing non-cognitive assessments (e.g., \nresumes, biodata questionnaires, interviews, personality inventories), implying that a \nmeaningful proportion of candidates may be willing to cheat on ability tests (Donovan et al., \n2003; Hu & Connelly, 2021; Levashina & Campion, 2007; Levashina et al., 2009). Indeed, in \nthe context of personnel selection, signaling theory suggests that candidates have a clear \nmotivation to attempt to send inaccurate but positive signals to employers (Bangerter et al. \n2012).   \nImportantly, prior to LLMs, a candidate wishing to cheat on an ability test still must \nfind an able, willing, and available volunteer. For example, if a candidate of moderate ability \nLarge Language Model Performance on Cognitive Tests    7 \noutsources their test taking to, or receives coaching from, somebody also of moderate ability, \nthis candidate may not benefit from that cheating attempt. This differs from self-report \npersonality tests, where cheating (or faking) is relatively simple to achieve, because self-\nreport personality tests ask people to report their standing on a trait, whereas cognitive ability \ntests require people to demonstrate their standing on the construct (Chamorro-Premuzic & \nFurnham, 2005). In terms of signaling theory, the costs of cheating on a cognitive ability test \n(i.e., time taken to identify and convince an intelligent acquaintance to be a surrogate test-\ntaker) are much higher than those for a self-report assessment (Bangerter et al. 2012). \nBy contrast, if LLMs outputs reliably achieve high scores on cognitive ability tests, \nthen a significant practical barrier to cheating is effectively removed, and candidates who \ncheat may displace candidates who do not; in other words, the costs of cheating are \ndrastically reduced (Bangerter et al. 2012). Thus, the public availability and accessibility of \nLLMs has altered the landscape of unproctored testing because anyone could potentially use \nan LLM to cheat on unproctored cognitive ability tests. \nThe Test-Taking Capabilities of Large Language Models \nThere remains considerable uncertainty with respect to the problem-solving \ncapabilities of LLMs. Published research suggests that LLMs often achieve high scores on \ntests that comprise extensive verbal information. As noted above, advanced LLMs appear to \nachieve higher scores than the majority of humans on many knowledge-based assessments \n(OpenAI, 2023), certain situational judgement tests (Arctic Shores, 2023b; Borchert et al., \n2023), and personality questionnaires (if prompted to; Arctic Shores, 2023a; Phillips & \nRobie, 2024). Further, Elyoseph et al. (2023) found that ChatGPT (GPT-3.5) outperformed \nmost humans on an emotional awareness assessment comprising items with text descriptions \nof situations that required participants to identify an emotional state. By contrast, Groza \n(2023) presented ChatGPT (GPT-3.5) with 100 logical and numerical reasoning problems of \nLarge Language Model Performance on Cognitive Tests    8 \nvarious types and found it correctly answered only 16 of them and demonstrated many logical \nfallacies in its reasoning. Similarly, Mitchell et al. (2023) found that GPT-4 and GPT-4 \nVision (an OpenAI model that can receive visual inputs in addition to text) performed poorly \nrelative to humans on a set of abstract reasoning problems. Digging deeper into OpenAI’s \n(2023) reported results reveals that often the version of the test completed was an old, defunct \nversion of the test or a practice test. Further, it is impossible to know whether such questions \nwere in the LLM’s training data, given that OpenAI no longer reveals the full details of their \ntraining data and process, which is one potential cause of the large variance in GPT’s \nperformance on different tests. Indeed, LLM developers also rarely provide details about the \nspecific prompts used, which raises the risk of p-hacking or selective reporting if, for \nexample, they only disclose results for the single prompt that generated the best results, \ndespite using many alternatives. \nAlthough early research identified several areas where LLMs often perform better \nthan many humans, there remain gaps in our understanding. First, very few tests examined in \nprior research are actually used in workplace personnel selection (we note that OpenAI \nreported performance on several tests used in academic selection, albeit using old versions of \nthe exams or practice exams; see OpenAI, 2023), aside from specialized medical tests. This \ndistinction is vital, because tests used in high-stakes selection settings may differ in several \nways from research-oriented tests or tests used for academic selection. In our investigation, \nwe examine two types of tests used for high-stakes selection, quantitative ability and verbal \nability, and we compare LLM performance across the two types of tests. \nResearch Question 1: How do LLMs perform on quantitative and verbal ability tests used in \nhigh-stakes settings? \nSecond, the effectiveness of LLM outputs is highly dependent on the prompt that the \nmodel is given, as the prompt influences the probabilities of certain words appearing. An \nLarge Language Model Performance on Cognitive Tests    9 \nemerging line of research is focused on identifying prompt approaches that increase the \nquality of an LLM’s outputs (often termed \"prompt engineering\"; see Chen et al., 2023, for a \nreview). This research is still nascent, and as such, inconsistencies in nomenclature and \ndefinitions are rife. Nonetheless, several formal prompt approaches have been proposed, \nalong with circumstances under which certain styles are expected to be more effective. \nImportantly, we note that prior research into the effectiveness of LLMs in completing \nassessments has primarily focused on finding one prompt that seemed to work well, then \nusing that prompt for generating all responses. There are reasons to suspect, however, that \ncandidates that use an LLM to assist with a test, may employ a variety of different prompt \napproaches. First, the popular literature on LLMs and Generative AI includes considerable \ndiscourse on the topic of prompt engineering. Second, many websites offering career \nadvisory services (e.g., resume writing) provide advice to job seekers about the importance of \neffective prompting when using a LLM to assist with writing job application materials. And \nthird, at the time of writing, myriad short courses on “prompt engineering” were available on \nonline educational services such as LinkedIn Learning (23 courses), Udemy (680 courses and \nvideos), and Coursera (273 courses). While not every LLM-using candidate will be aware of \nprompting approaches, altogether, it seems likely that many candidates will have at least \nformed a hypothesis that, or about how, certain prompts will affect the LLM’s performance. \nIt is impossible to examine all possible prompt approaches, however, it is important to \ninvestigate whether several of currently more popular or perhaps intuitive approaches might \ninfluence a LLM’s performance on tests (Mitchell et al., 2023).  \nFirst, we considered two very basic prompting approaches that provide minimal \ncontext and no special instruction. The first involves copy-pasting the test instructions and \nquestion as input to the LLM. In our investigation, this was termed “Base Prompting.” \nSimilarly, we explored an even more minimalistic approach that only included the portion of \nLarge Language Model Performance on Cognitive Tests    10 \nthe instructions that asked the focal question (as detailed in the Method and the Online \nSupplement), which we referred to as “Bare Bones.” Additionally, we investigated three \nrelatively well-established but more complex prompt approaches that candidates who have \ninvestigated prompt engineering might encounter. These were “chain-of-thought”, \n“persona/role-based”, and \"vocalize and reflect”. Chain-of-thought involves a prompt that \nincludes an example of a similar question, the correct answer, and step-by-step rationale (i.e., \nchain-of-thought) for how to solve the problem (Wei et al., 2022). Persona/role-based prompt \napproach involves giving the LLM a “persona\" so that it will respond more like a certain type \nof person (e.g., a subject-matter expert; Xu et al., 2023). Generally, the persona is a subject \nmatter expert on the topic, with the logic being that asking the LLM to adopt the mindset of \nthe expert, it will respond more like such an expert. For example, when completing the verbal \nability test, we instructed it, “You are an experienced corporate analyst at a large \norganization, known for your meticulous attention to detail and your ability to make accurate \njudgments based on documents and reports.” Vocalize and reflect involves asking the LLM to \ndevelop an initial response, critique it, and then develop a revised response based on its own \ncritique—a form of self-learning (Shinn et al., 2023). The hope is that by asking the LLM to \ncritique its own initial response, it will arrive at an improved response. Finally, we also \ninvestigated a fifth strategy which we termed “method explained”. Method explained \ninstructs the LLM to explain the methodology it will use for correctly completing the \nquestion before answering (Kojima et al., 2023). In real applicant settings, this could give \napplicants the opportunity to advise the LLM on how to think through the problem more \neffectively.2 \nResearch Question 2: Do different prompt approaches influence LLM performance on \n \n2 In our study, we developed prompt approaches, applied them, and examined whether the provided answer was \ncorrect. We did not iteratively prompt to improve outputs. \nLarge Language Model Performance on Cognitive Tests    11 \ncognitive ability tests used in high-stakes settings? \n Current evidence suggests that GPT-4 outperforms GPT-3.5 in a variety of tasks \n(OpenAI; Phillips & Robie, 2023). One reason that this is concerning is that applicants with \nfewer financial resources may not be able to afford to access GPT-4 and may, instead, rely on \nthe free ChatGPT which uses GPT-3.5. Thus, while examining GPT-3.5’s performance may \nnot provide insight into a LLM’s full potential, it provides insight into how the majority of \ncandidates relying on a LLM might perform on tests. Indeed, as discussed above, GPT-4 has \ntwo advantages over the earlier version. First, it is trained on more data. Second, it is a larger \nneural network. These differences, coupled with clear evidence that GPT-4 outperforms \nGPT-3.5 in a variety of ways, brings us to our first hypothesis.  \nHypothesis 1: GPT-4 will perform better than GPT-3.5 on cognitive ability tests used in high-\nstakes settings. \nFinally, we note that, to our knowledge, prior research has rarely considered the \ntemperature setting in LLMs.3 The temperature setting affects how much randomness is \nadded to the model when it is generating next word predictions. Higher temperatures \nintroduce more randomness, while lower temperatures provide more consistent responses. At \ntime of writing, ChatGPT defaults to a temperature value of 0.7 for both GPT-3.5 and GPT-4, \nwhereas this can be varied when interacting with GPT through the API. Even at a temperature \n0, however, there is some randomness due to the non-deterministic nature of the LLM \nmodels. Given that higher temperatures are thought to be useful for creative tasks, it is \nunclear how temperatures might affect LLM performance on cognitive ability tests. Although \nin principle we could examine the LLM’s performance at any temperature setting, in line \nwith the comparison above, we aimed to compare the typical candidate use case (i.e., the \n \n3 For interested readers, the OSF repository contains the code that shows how to specify the temperature setting \nwhen calling the API. \nLarge Language Model Performance on Cognitive Tests    12 \ncurrent default temperature = 0.7) to a use case where we would expect the LLM to perform \nmost consistently (temperature = 0).  \nResearch Question 3: Does the temperature setting influence LLM performance on cognitive \nability tests used in high-stakes settings? \nMethod \n Representative Python code for generating the results and the combined results file \nare available on OSF: https://osf.io/pja32/?view_only=af8e0fdf7aef40eab3192f65b35618d5 \nProcedure \nWe used several prompt approaches to generate responses from GPT-3.5 and GPT-4 \nto quantitative ability and verbal ability tests used in high-stakes selection settings. In \ninteracting with the LLM, we used the OpenAI API. Using the API was necessary given the \nscale of our investigation: we collected over 100 sets of test responses from LLMs for the \nquantitative ability test and over 40 sets of responses for the verbal ability test. Specifically, \nwe generated responses in 2×2×6 (24) experimental conditions: two versions of GPT (3.5 vs. \n4), two temperatures (0.0 vs. 0.7), and six prompt approaches (base, bare bones, chain-of-\nthought, persona, vocalize and reflect, and method explained), and we generated two sets of \nresponses in each condition for the quantitative ability test, a multiple-choice version of the \nquantitative ability test (described below), and the verbal ability test. Additionally, we \ngenerated one set of responses in each condition for the modified version of the multiple-\nchoice quantitative ability test, and for the original tests, we generated a third set of responses \nwhen the first two trials disagreed by four or more points. We then took the average of all \ntrials within each condition, and before calculating percentile scores, rounded to the nearest \ninteger. \nIn all cases, each test question and prompt were presented to the LLM independently \nof the other questions. In other words, each test question was treated as a new conversation, \nLarge Language Model Performance on Cognitive Tests    13 \nwhich prevents the context created by prior test questions from influencing the output for \nsubsequent questions. We generated all responses between October 31, 2023 and December \n1, 2023. We compared the scores obtained from the GPT LLMs to the scores obtained by \nhumans in high-stakes settings. Below, we provide details about the two tests, the high-stakes \nhuman responses to which we compared the LLM-based scores, and the large language \nmodels and the prompts.   \nQuantitative Ability Test \nThe quantitative ability test is a published commercial test used in personnel \nselection. It consisted of 20 open-ended number series items. In these items, rather than every \nquestion asking for the final item in the sequence, the missing item varied from question to \nquestion. The test instructs test takers to determine the missing number in the numerical \nsequence and gives an example. That example item is: [2, 4, 6, None, 10, 12], with the correct \nanswer being “8.” In the test items, some questions had two sequences, and the test taker \nmust determine both the function for the sequence and which sequence applies to the missing \nnumber. The instructions clarify this, “There may be one or multiple rules governing the \npattern in each sequence. The answer will always be an integer, and may also be negative (-\n).” For example, possible items (but not items actually on the test) would include, “[2, 5, 11, \nNone, 35, 71] (answer: 23),” “[3, 4, 8, 10, 13, None]” (answer: 16), and “[2, 12, 17, 68, 71, \nNone]” (answer: 142). In high-stakes settings, the test’s time limit is 17 minutes. \nThe test manual reports the test’s internal consistency α = .70 and test-retest reliability \n= .62. The test converged r = .58 and r = .62 with other commercial numerical reasoning \ntests. Additionally, the test converged r = .61 with a number-letter series test and r = .60 with \na 3D rotation test from the International Cognitive Ability Resource (ICAR, 2014). \nWe found that the LLMs struggled to provide accurate responses to the open-ended \nformat of this test, and accordingly, we also examined the LLM performance on a multiple-\nLarge Language Model Performance on Cognitive Tests    14 \nchoice version of the test. To develop the multiple choices, we included the correct response, \na response from a pattern that was not for that missing item, a response from an incorrect (but \nplausible) pattern, and “none of these.”  \nAdditionally, given that the LLMs struggled with the multiple-choice version of the \ntest, we also examined the LLM performance on a modified version of the multiple-choice \nversion of the test. Specifically, we edited all items so that the missing number came last in \nthe numerical sequence. For example, revising the example item above in this way would \nmake the sequence, [2, 4, 6, 8, 10, None]. We considered this important for understanding \nwhether the LLMs perform better when all relevant information can be gleaned from moving \nleft-to-right, as in English language text. No psychometric properties are available that \ndescribe the multiple-choice versions of the test, given that these versions were created solely \nto see if that would improve the LLM scores on the tests. \nHuman Norm Scores for Quantitative Ability \nThe norm scores were derived from a sample of 9,253 real-world job applicants. All \napplicants applied to jobs targeted toward soon-to-graduate with Bachelor’s degree or recent \ngraduates in the United Kingdom. The jobs they applied to were almost all in information \ntechnology and other professional services (99.41%).  Applicant age and ethnicity were \nunavailable for this sample. 49.04% of applicants reported their gender as male, 32.31% did \nnot report their gender, 18.57% reported being female, and 0.15% reported other.  \nVerbal Ability Test \nThe verbal ability test, also a commercial test used for selection, consisted of 24 \nmultiple-choice verbal reasoning items. Each item displayed a passage of text and then asks \ntest takers to determine whether a statement about the passage is true, false, or cannot say. \nThe key passages from the example item in the test instructions are, “All current employees \nare either team members, team leaders, or department managers. … All team members have a \nLarge Language Model Performance on Cognitive Tests    15 \ndesignated mentor, but they are not permitted to act as a mentor for another employee, and all \ndepartment managers have one designated mentee.” The statement to evaluate is, “All \nemployees are a mentor, a mentee, or both.” The answer is “Cannot Say,” because team \nleaders are not mentioned in the mentor/mentee assignments. In high-stakes settings, the \ntest’s time limit is 18 minutes. \nThe test manual reports the test’s internal consistency α = .72 and test-retest reliability \n= .59. The test converged r = .65 and r = .74 with other commercial verbal reasoning tests. \nAdditionally, the test converged r = .58 with a verbal reasoning test from the International \nCognitive Ability Resource (ICAR, 2014).  \nHuman Norm Scores for Verbal Ability \nThe norm scores were derived from a sample of 38,896 real-world job applicants. The \nvast majority of applicants applied to jobs targeted toward applicants who were soon-to-\ngraduate or had recently graduated with Bachelor’s degrees (98.67%), and applicants’ mean \nage was 22.67 (SD = 4.25). Most of the data were from applicants to jobs in Great Britain \n(60.79%), with the remaining applying to jobs in the United Arab Emirates (25.73%), \nAustralia (12.70%), and other locations (0.78%). The jobs they applied for were in banking, \nfinance, human resources, and other professional services (32.24%), science, technology, \nengineering, and math (STEM; 29.11%), the public sector (7.87%), or other (30.79%). Most \napplicants did not voluntarily report their gender (45.37%), while 34.37% reported being \nmale, 20.17% reported being female, and 0.09% reported other. The vast majority of \napplicants did not voluntarily report their race/ethnicity (86.27%), while those who did were \nwhite (8.29%), Asian (3.47%), Black/African/Caribbean (0.72%), multiple (0.71%), or other \n(0.55%). \nLarge Language Models \nChatGPT has popularized the family of GPT large language models. The ‘generative’ \nLarge Language Model Performance on Cognitive Tests    16 \nrefers to its capacity for creating new content. ‘Pretrained’ refers to the extensive amounts of \ndata it has been trained on to do next word prediction. ‘Transformer’ refers to aspects of the \nneural network architecture (Radford et al., 2018). In particular, we compared the \nperformance of GPT-3.5, as implemented in the OpenAI API as gpt-3.5-turbo, and GPT-4, as \nimplemented in the OpenAI API as gpt-4. GPT-3.5 is freely available via ChatGPT, while \nGPT-4 can be accessed via the API or the paid version of ChatGPT. \nPrompt Approaches  \n We utilized six prompt approaches: base, bare bones, chain-of-thought, persona, \nvocalize and reflect, and method explained. The prompt approaches used are listed in Table \nS1 for the quantitative ability test and S2 for the verbal ability test. The base prompt \napproach we used involved providing the LLM with (a) the instructions from the test, (b) the \nitem along with the response options, and (c) instructions to work out the answer step-by-step \nand provide its final answer.4 The bare bones prompt approach merely pasted in the response \ninstructions from each test (i.e., for the quantitative ability test, “What is the missing number \nin the sequence below,” and for the verbal ability test, “Is the following statement true, false, \nor cannot say?”). This approach was intended to be similar to what naïve test takers might do. \nChain-of-thought involved providing the LLM with (a) the instructions from the test, (b) an \nexample item, (c) a step-by-step process for reasoning through and correctly answering the \nexample item as well as the correct answer, and (d) elements (b) and (c) from the base \nprompt. Persona involved specifying a persona to inhabit prior to the test instructions. For the \nquantitative ability test, we used the persona of a math teacher who excels at pattern \nrecognition and explaining math solutions in a logical, easy-to-understand way, and for the \nverbal ability test, we used the persona of an experienced corporate analyst known for their \n \n4 Given that this is a recommended step for improving the output of LLMs (e.g., \nhttps://neurips.cc/virtual/2023/poster/71210), we also explored this for the quantitative ability test. While this \naddition to the prompt improved performance with the verbal ability test, it tended to decrease performance on \nthe quantitative ability test. \nLarge Language Model Performance on Cognitive Tests    17 \nattention to detail and accurate judgments from documents. For vocalize and reflect, we \nmodified the base prompt by adding content after the test instructions that instructed the LLM \nto provide its initial judgment and explain its reasoning then reviewing its initial response to \nidentify possible improvements, before providing a final answer with an explanation for its \nreasoning. Finally, method explained modified the base prompt approach by expanding upon \nthe test instructions by instructing the LLM to outline in detail the methodology for \naccurately completing such tests.5 \nResults \nResearch Question 1 focused on GPT-3.5 and GPT-4’s performance on the \nquantitative ability test versus the verbal ability test. The average results across runs and \nother conditions are reported in Table 1 and illustrated in Figure 1 (for the verbal ability test \nand only the open-ended [original] version of the quantitative test). The results broken down \nby conditions are reported in Tables S3 (quantitative) and S4 (verbal). We found that the \nLLMs performed substantially better on the verbal ability test (average 69.97 percentile) than \non the quantitative ability test (average 8.38 percentile). This finding held even when the \nquantitative ability test was in a multiple choice format (like the verbal ability test; those \nresults are reported in Table S5; average 15.18 percentile, compared to humans responding to \nthe open-ended [original] version of the test), and when we shifted the missing item to be the \nfinal item in the numerical sequence (Table S5 under “missing number last”). Although the \nLLMs scored considerably higher on the quantitative ability test when all the missing items \nwere the final items in the numerical sequences (average 28.81 percentile, again compared to \nhumans responding to the open-ended [original] version of the test), they still performed \nworse, on average, compared to their performance on the verbal ability test.  \n \n5 We also explored, via the ChatGPT website, using GPT-4 with the Code Interpreter feature and the bare bones \nprompt to respond to the quantitative ability test. The score was identical to using GPT-4 through the API \nwithout Code Interpreter. \nLarge Language Model Performance on Cognitive Tests    18 \n Research Question 2 regards the performance of different prompt approaches. On \naverage across both tests, the bare bones prompt approach provided the worst scores overall. \nOn the quantitative test, bare bones performed comparably to the other prompt approaches, \nbut on the verbal ability test—particularly with GPT-3.5—bare bones performed much worse \nthan the other prompt approaches (average score of 7 compared to average score 12.92 or \nabove for the other prompt approaches). Bare bones also performed worse than the other \nprompt approaches with GPT-4 on the verbal ability test, but the differences were much more \nminor. On average across both tests, the chain-of-thought prompt approach yielded the best \nscores. The average differences between different prompt approaches were not large \n(excepting with the bare bones prompt approach)—for the quantitative ability test, the range \nof average values for GPT-3.5 was 2.00 and for GPT-4 was 3.75. For the verbal ability test, \nthe range of average values for GPT-3.5 was 7.00 (driven mainly by the low score from the \nbare bones prompt approach) and for GPT-4 was 4.50. Thus, with the exception of the bare \nbones prompt approach, the scores and percentiles achieved by different prompt approaches \nwere highly similar. \n GPT-4 outperformed GPT-3.5 in both the quantitative and verbal ability test, \nsupporting Hypothesis 1. On average across prompt approaches and temperatures, GPT-4 \nscored almost twice as high as GPT-3.5 on the quantitative ability test, although in both \ncases, the scores fell below the 20th percentile, on average across prompt approaches and \ntemperatures. On average, GPT-4 scored 6.38 points (almost 50%) higher on the verbal \nability test than GPT-3.5, which resulted in averaging in the 95th percentile for GPT-4 as \ncompared to the 45th percentile for GPT-3.5. \n Research Question 3 regards the influence of the temperature setting on the LLMs’ \nscores. Overall, on average, a temperature of 0.7 provided slightly higher scores (mean = \n10.72) on both tests than a temperature of 0.0 (mean = 10.06). The difference was just over 1 \nLarge Language Model Performance on Cognitive Tests    19 \npoint for the quantitative ability test (mean difference = 1.04). And the difference was small \nfor the verbal ability test (mean difference = 0.26). Thus, a non-zero temperature provided \nsmall improvements in test performance. \n We also examined which items the LLMs tended to get correct. To do so, we used the \nhuman norm score data to calculate the percentage of correct responses for each item. We \nused this as a measure of item difficulty (i.e., item difficulty as the inverse of the proportion \nof test takers who answered the item correctly). For verbal ability, item difficulty correlated r \n= -.24 and -.35, respectively, with the average GPT-3.5 and GPT-4 scores (across temperature \nand prompt approaches) on the item. For quantitative ability, item difficulty correlated r = -\n.40 and -.50, respectively, with the average GPT-3.5 and GPT-4 scores on the item. In \nparticular for the quantitative ability test, we found that GPT performed poorly on items that \ninvolved alternating sequences (i.e., the sequence alternates between addition/subtraction and \nmultiplication/division), especially when the mathematical operation increased or decreased \nin value throughout the sequence. Further, as demonstrated above, it performed worse on \nitems where the missing number was not the final number in the sequence. GPT performed \nwell on items with relatively simple, single sequences (e.g., multiplying each number in the \nsequence by a constant value). Overall, the LLMs were more likely to provide correct \nresponses to easier items and less likely to do so for more difficult items.  \nDiscussion \nIn an era of widespread unproctored testing and LLM availability, it is critical to \nunderstand the extent to which candidate assessment may be disrupted by this new \ntechnology. Accordingly, in the first known study of its kind, this investigation examined the \nperformance of two LLMs on commercial quantitative and verbal ability tests used for \npersonnel selection. To these ends, we evaluated the performance of two popular LLMs (the \nfreely available GPT-3.5 and the subscription based GPT-4) with six different prompt \nLarge Language Model Performance on Cognitive Tests    20 \napproaches and two temperature settings. We found, first, that LLMs performed substantially \nbetter on the verbal ability test than on the quantitative ability test. Second, we found that the \nGPT-4 model vastly outperformed GPT-3.5 on the verbal assessment but performed only \nslightly better on the quantitative test. Third, we found the different prompt approaches we \ntested were not associated with large differences in performance, except for the bare bones \napproach on the verbal ability test with GPT-3.5 which produced notably worse results. And, \nfourth, we found slight improvements in performance were observed when model \ntemperature was set to the model’s default value of 0.7, compared to the minimum of zero. \nImplications for Understanding LLMs \nThe impressive performance of GPT-4 LLM on the verbal test is consistent with \nfindings from other research that has demonstrated GPT-4’s capability in completing a \nvariety of assessments involving a large proportion of verbal content (Brin et al., 2023; \nOpenAI, 2023). The strong performance of this model has been observed on assessments \ninvolving veridical truths (i.e., knowledge and aptitude; Brin et al., 2023) and subjective or \nsocially constructed ‘truths’ (e.g., personality, emotional intelligence, situational judgment; \nBorchert et al., 2023; Phillips & Robie, 2024). By contrast, the GPT-3.5 model, which is \ntrained on a smaller data set and has fewer model parameters than GPT-4, performed only \nslightly better than the median member of the human norm group. Based on these results, \nonly GPT-4 (and perhaps similarly performing models) hold potential to threaten the integrity \nof unproctored verbal ability assessments, whereas applicants relying on the free GPT-3.5 \nmodel are unlikely to achieve stand-out results. One potential implication for this pattern of \nresults is that candidates with financial resources to spend on superior LLMs may secure \nrelatively more employment opportunities in organizations using verbal assessments, and the \ncontinued use of verbal assessments as selection tools may exacerbate social inequalities; that \nis, the rich will likely get richer. \nLarge Language Model Performance on Cognitive Tests    21 \nOverall, the performance of both LLMs on the quantitative test was very poor, with \nGPT-4 only slightly outperforming GPT-3.5. Thus, it seems from this initial investigation \nthat aptitude assessments involving numerical problems are more robust to candidates’ use of \nLLMs as an aide. Interestingly, however, GPT-4’s performance improved substantially when \nboth the format of the quantitative assessment was adapted into a multi-choice test and the \nmissing number in the sequence was positioned at the end. Likely, this occurs because LLMs \nare trained to work from left to right in language, and thus, reasoning that requires moving \nboth left and right (such as is needed when the missing number is not at the end) may be \nmore difficult for the LLMs. These results suggest that subtle decisions regarding test item \ndesign may have important consequences for test vulnerability to LLMs. To the extent that \nthese results generalize to other types of tests involving sequences, test developers may be \nwise to focus on questions where the missing item in the sequence is not in the final spot. \nNonetheless, future research is required to understand whether the results observed here \ngeneralize to other types of test items that involve sequences (e.g., Condon & Revelle, 2014) \nor if the pattern is only applicable to numerical sequences. \nAlthough emerging research has demonstrated that different prompt approaches can \nelicit responses of varying quality from LLMs (White et al., 2023), our comparisons of six \nprompt approaches suggested that prompting plays a relatively minor role in determining \nperformance on the quantitative and verbal ability tests. Excepting the bare bones prompt \napproach with GPT-3.5 on the verbal ability test, the largest differences, in raw scores, \nbetween the least and most effective prompt approaches did not go beyond 4 points (or 12.03 \npercentile improvement), and these ‘large’ differences were only observed for the \nquantitative ability test, where the models struggled to perform well. While recognizing that \nwe only examined six strategies from an infinite set, these initial findings suggest that \nvariation in candidates’ adoption of prompt approaches is unlikely to lead to major \nLarge Language Model Performance on Cognitive Tests    22 \ndifferences in performance between candidates using LLMs—particularly with GPT-4. One \npotential implication for this result is that test developers will need to continuously monitor \nthe psychometric properties and mean scores of their tests, as extensive LLM-based cheating \nmay affect the distributions of test scores. For example, it may be that secondary (or \nadditional) modes that represent the maximum test performance of a popular LLM may \nemerge over time, which may influence the transformation of raw test scores into percentiles.  \nThe temperature setting of LLMs affects how much randomness exists in the model. \nWhile it may be reasonable to assume that a lower temperature, by increasing the consistency \nof outputs, would give better answers, we found that a non-zero temperature tended to \nprovide small performance improvements. Thus, it seems that having some randomness in \ngeneration may have helped the LLMs come to the correct response, whereas the less random \ngeneration when temperature equals zero made the LLMs less likely to uncover the correct \nanswer. Overall, the default temperature settings for ChatGPT benefits applicants, relative to \na more conservative temperature setting, although the benefit seems to be very small. \nImplications for Unproctored Cognitive Ability Testing \nEven prior to the threat of candidates using LLMs to complete tests on their behalf, \nthe use of cognitive ability testing for selection has recently been called into question by \nacademics due to concerns around the initial over-estimation of their criterion-related validity \n(Sackett et al., 2023; Sackett et al., 2022; cf., Schmidt & Hunter, 1998) and the adverse \nimpact these tests can have on disadvantaged group members (e.g., Woods & Patterson, \n2023). Our results suggest that LLMs have created new problems for unproctored testing, \nparticularly for verbal ability tests, given that they can perform better than most human test \ntakers and are widely available for people to use. In other words, tests that were assumed to \nbe costly to fake may now become less trustworthy signals of a candidate’s ability (Bangerter \net al., 2012). This alters the testing landscape, because the continued adoption of unproctored \nLarge Language Model Performance on Cognitive Tests    23 \nability testing in selection may trigger a further degradation of observed criterion-related \nvalidity as a greater proportion of LLM-generated scores replaces human-generated scores. \nFurther, socio-economic inequality may be exacerbated as candidates with the resources to \nuse the superior LLMs outperform those who cannot afford to. One possible outcome is that \nemployers may adapt their selection systems to remove ability tests that come to be regarded \nas easy to fake. Alternatively, reintroducing proctoring could in turn reintroduce significant \nrisks to candidates wishing to cheat, although it will also increase employer costs. \nNonetheless, while the performances of the LLMs on the verbal ability test was \nimpressive, this was not the case for the quantitative test, where even GPT-4 performed \nworse than a large majority of the norm group. Thus, in practice, a candidate who relies on an \nLLM to complete both tests would likely not be among the most highly ranked. Further, we \nalso recognize that, currently, there remain some practical constraints preventing candidates \nfrom being able to completely rely on an LLM to complete a test on their behalf. First, \ncandidates must be able to transcribe the test materials into the LLM application. Preventing \ntest-takers from copying and pasting item text is technically very straightforward (e.g., by \ndelivering the item as an image instead of text). Second, many ability tests—including those \nstudied here—are timed, limiting applicant capacity for relying on outside sources, and at the \ntime of writing, the speed and dependability of LLMs were variable. For example, in our \nattempt at the bare-bones prompt using the ChatGPT website, we found that GPT-4’s Code \nInterpreter sometimes took three minutes to generate a response, and often that response \nwould not even include a suggested solution. And third, the current version of ChatGPT \nplaces heavy restrictions on the number of messages allowed in a given period. Nonetheless, \nwe suspect that, over time, these barriers and others will be easily overcome as LLM and \nsupporting technologies evolve. For example, alleviating both the transcription and time \nproblems, the current version of the ChatGPT cell phone app can integrate with a phone’s \nLarge Language Model Performance on Cognitive Tests    24 \ncamera and automatically transcribe text from a photo image. It is therefore vital that IO \npsychologists remain alert to the development in the capabilities of LLMs (see Landers, \n2023). \nLimitations, Future Research, and Conclusion \nOur study is limited in several ways that suggest directions for future research. First, \nbecause we were examining multiple conditions in multiple trials across multiple tests, we \nprompted the LLMs in a way that encouraged them to provide their final answer in a way that \nwas easily retrievable with computer code. In particular, we asked the LLM to provide its \nfinal answer in curly brackets {}. However, in doing so, we may have altered the behavior of \nthe LLM, given that any alterations to the input sequence can substantially alter results.  \nSecond, to facilitate the investigation, we used code to automatically retrieve the final \nanswer and gave the LLM no opportunities to improve upon its response to the initial prompt. \nHowever, applicants could iteratively query an LLM. For example, in a multiple-choice test, \nif the LLM returned no answers that matched the response options, a user could ask the LLM \nto reconsider and revise its output. Indeed, in our bare bones run on Code Interpreter with the \nquantitative test, the ChatGPT app would often request additional information from the user. \nThis could potentially improve LLM test performance, but it remains to be seen how user \ninteraction with LLMs influences LLM test performance. It is possible that applicants of \nhigher levels of ability will be more effective at querying the LLM and, thus, can utilize \nLLMs more effectively than less able applicants. \nThird, although we investigated a quantitative and verbal ability test, these were \nspecific tests that each consisted of a single type of item. LLMs may perform better or worse \non different item types, and there are many item types that we did not explore. For example, \nmatrix reasoning items (e.g., Raven’s standard progressive matrices) are commonly used, yet \nusing an LLM on such items would require a multimodal LLM that can accept images as \nLarge Language Model Performance on Cognitive Tests    25 \ninputs. Initial evidence suggests that LLMs score well on matrix reasoning items when they \nare translated into text-based questions (Webb et al., 2023). Future work could investigate \nadditional item types in validated tests used for high-stakes selection. \nFourth, we examined only a specific set of LLMs—namely, GPT-3.5 and GPT-4. \nAlthough these are extremely popular LLMs, many other LLMs exist and more are being \ncreated. For example, Alphabet (Google’s parent company) just released Gemini, which was \ntrained to be multimodal. Future LLMs may perform better on quantitative ability tests, but \ncurrent state-of-the-art LLMs can already complete verbal ability tests comparably to humans \nwith extremely high verbal ability. \nOverall, our results surface fresh concerns regarding the use of unproctored cognitive \nability testing for pre-employment assessment. The findings are particularly concerning for \nverbal ability tests, given that all prompt approaches with GPT-4 hold potential for nullifying \nthe validity of such tests. However, if applicant quantitative ability is being tested, or if \ncandidates use GPT-3.5, they may fare worse when using LLMs than when not using them.  \nLarge Language Model Performance on Cognitive Tests    26 \nReferences \nArctic Shores. (2023a). ChatGPT vs Personality Assessments. \nhttps://www.arcticshores.com/insights/chatgpt-vs-personality-assessments-does-it-\nhave-the-right-personality-traits-to-get-an-interview \nArctic Shores. (2023b). ChatGPT vs Situational Judgement Tests: Can it outperform a \nhuman? https://www.arcticshores.com/insights/chatgpt-vs-situational-judgement-\ntests-how-it-performs-vs-a-human \nArthur, W., Jr., Glaze, R. M., Villado, A. J., & Taylor, J. E. (2010). The magnitude and extent \nof cheating and response distortion effects on unproctored Internet-based tests of \ncognitive ability and personality. International Journal of Selection and Assessment, \n18(1), 1-16. https://doi.org/10.1111/j.1468-2389.2010.00476.x  \nBangerter, A., Roulin, N., & König, C. J. (2012). Personnel selection as a signaling game. \nJournal of Applied Psychology, 97(4), 719-738. https://doi.org/10.1037/a0026078  \nBeaty, J. C., Nye, C. D., Borneman, M. J., Kantrowitz, T. M., Drasgow, F., & Grauer, E. \n(2011). Proctored versus unproctored internet tests: Are unproctored noncognitive \ntests as predictive of job performance? International Journal of Selection and \nAssessment, 19(1), 1-10. https://doi.org/10.1111/j.1468-2389.2011.00529.x  \nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the \ndangers of stochastic parrots: Can language models be too big?          . In Proceedings of \nthe 2021 ACM conference on fairness, accountability, and transparency (pp. 610-\n623).  \nBorchert, R. J., Hickman, C. R., Pepys, J., & Sadler, T. J. (2023). Performance of ChatGPT \non the Situational Judgement Test—A Professional Dilemmas–Based Examination for \nDoctors in the United Kingdom. JMIR Medical Education, 9, e48978. \nhttps://doi.org/10.2196/48978  \nLarge Language Model Performance on Cognitive Tests    27 \nBrin, D., Sorin, V., Vaid, A., Soroush, A., Glicksberg, B. S., Charney, A. W., Nadkarni, G., \n& Klang, E. (2023). Comparing ChatGPT and GPT-4 performance in USMLE soft \nskill assessments. Scientific Reports, 13(1), 16492. https://doi.org/10.1038/s41598-\n023-43436-9  \nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., \nShyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., \nHenighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, \nD. (2020). Language models are few-shot learners. Advances in Neural Information \nProcessing Systems, 33, 1877–1901. \nBudhwar, P., Chowdhury, S., Wood, G., Aguinis, H., Bamber, G. J., Beltran, J. R., Boselie, \nP., Lee Cooke, F., Decker, S., DeNisi, A., Dey, P. K., Guest, D., Knoblich, A. J., \nMalik, A., Paauwe, J., Papagiannidis, S., Patel, C., Pereira, V., Ren, S., . . . Varma, A. \n(2023). Human resource management in the age of generative artificial intelligence: \nPerspectives and research directions on ChatGPT. Human Resource Management \nJournal. https://doi.org/10.1111/1748-8583.12524  \nChamorro-Premuzic, T., & Furnham, A. (2005). Personality and Intellectual Competence. \nMahwah: Lawrence Erlbaum Associates. \nChen, B., Zhang, Z., Langrené, N., & Zhu, S. (2023). Unleashing the potential of prompt \nengineering in Large Language Models: a comprehensive review. arXiv \n(2310.14735). https://doi.org/10.48550/arXiv.2310.14735  \nCondon, D. M., & Revelle, W. (2014). The international cognitive ability resource: \nDevelopment and initial validation of a public-domain measure. Intelligence, 43, 52-\n64. https://doi.org/10.1016/j.intell.2014.01.004  \nDonovan, J. J., Dwight, S. A., & Hurtz, G. M. (2003). An assessment of the prevalence, \nseverity, and verifiability of entry-level applicant faking using the randomized \nLarge Language Model Performance on Cognitive Tests    28 \nresponse technique. Human Performance, 16(1), 81–106. \nhttps://doi.org/10.1207/S15327043HUP1601_4  \nElyoseph, Z., Hadar-Shoval, D., Asraf, K., & Lvovsky, M. (2023). ChatGPT outperforms \nhumans in emotional awareness evaluations. Frontiers in Psychology, 14. \nhttps://doi.org/10.3389/fpsyg.2023.1199058  \nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its Nature, Scope, Limits, and Consequences. \nMinds and Machines, 30(4), 681–694. https://doi.org/10.1007/s11023-020-09548-1 \nGroza, A. (2023). Measuring reasoning capabilities of ChatGPT. arXiv, 2310.05993. \nhttps://arxiv.org/abs/2310.05993  \nHandler, C. (2020). COVID-19 Talent Acquisition Benchmarking Survey. Rocket-Hire. \nhttps://rocket-hire.com/2020/06/17/covid-19-talent-acquisition-benchmarking-report/ \nThe International Cognitive Ability Resource Team (ICAR) (2014). https://icar-project.com/ \nHu, J., & Connelly, B. S. (2021). Faking by actual applicants on personality tests: A meta-\nanalysis of within-subjects studies. International Journal of Selection and Assessment, \n29(3-4), 412-426. https://doi.org/10.1111/ijsa.12338  \nKantrowitz, T. M., & Dainis, A. M. (2014). How secure are unproctored pre-employment \ntests? Analysis of inconsistent test scores. Journal of Business and Psychology, 29(4), \n605-616. https://doi.org/10.1007/s10869-014-9365-6  \nKantrowitz, T. M., Tuzinski, K. A., & Raines, J. M. (2018). Global assessment trends report \n2018. https://www.shl.com/en/assessments/trends/global-assessment-trends-report/  \nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2023). Large language models \nare zero-shot reasoners. arXiv, 2205.11916v4. https://doi.org/ \n10.48550/arXiv.2205.11916  \nLanders, R. N. (2023). Fixing the Industrial-Organizational Psychology-Technology Interface \n(IOPTI): Avoiding both IO/Tech and Tech/IO conflict. In T. M. Kantrowitz, D. H. \nLarge Language Model Performance on Cognitive Tests    29 \nReynolds, & J. C. Scott (Eds.), Talent assessment: Embracing innovation and \nmitigating risk in the digital age (pp. 202-218). Oxford University Press. \nhttps://doi.org/10.1093/oso/9780197611050.003.0013  \nLevashina, J., & Campion, M. A. (2007). Measuring faking in the employment interview: \nDevelopment and validation of an interview faking behavior scale. Journal of Applied \nPsychology, 92(6), 1638-1656. https://doi.org/10.1037/0021-9010.92.6.1638  \nLevashina, J., Morgeson, F. P., & Campion, M. A. (2009). They don't do it often, but they do \nit well: Exploring the relationship between applicant mental abilities and faking. \nInternational Journal of Selection and Assessment, 17(3), 271-281. \nhttps://doi.org/10.1111/j.1468-2389.2009.00469.x  \nLievens, F., & Burke, E. (2011). Dealing with the threats inherent in unproctored Internet \ntesting of cognitive ability: Results from a large-scale operational test program. \nJournal of Occupational and Organizational Psychology, 84(4), 817-824. \nhttps://doi.org/10.1348/096317910X522672  \nMaynez, J., Narayan, S., Bohnet, B., & McDonald, R. (2020). On faithfulness and factuality \nin abstractive summarization. arXiv, 2005.00661. \nhttps://doi.org/10.48550/arXiv.2005.00661  \nMitchell, M., & Krakauer, D. C. (2023). The debate over understanding in AI’s large \nlanguage models. Proceedings of the National Academy of Sciences of the United \nStates of America, 120(13), 1–5. https://doi.org/10.1073/pnas.2215907120 \nMitchell, M., Palmarini, A. B., & Moskvichev, A. (2023). Comparing humans, GPT-4, and \nGPT-4V on abstraction and reasoning tasks. arXiv, 2311.09247v09242. \nhttps://doi.org/10.48550/arXiv.2311.09247  \nLarge Language Model Performance on Cognitive Tests    30 \nNye, C. D., Do, B.-R., Drasgow, F., & Fine, S. (2008). Two-step testing in employee \nselection: Is score inflation a problem? International Journal of Selection and \nAssessment, 16(2), 112-120. https://doi.org/10.1111/j.1468-2389.2008.00416.x  \nOpenAI. (2023). GPT-4 Technical Report. https://cdn.openai.com/papers/gpt-4.pdf \nPhillips, J., & Robie, C. (2024). Can a computer outfake a human? Personality and Individual \nDifferences, 217, 112434. https://doi.org/https://doi.org/10.1016/j.paid.2023.112434  \nPorter, J. (2023). ChatGPT continues to be one of the fastest-growing services ever. \nTheVerge. https://www.theverge.com/2023/11/6/23948386/chatgpt-active-user-count-\nopenai-developer-conference  \nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving language \nunderstanding by generative pre-training. Available at \nhttps://www.mikecaptain.com/resources/pdf/GPT-1.pdf \nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language \nmodels are unsupervised multitask learners. OpenAI blog, 1(8), 9.  \nRay, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key \nchallenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-\nPhysical Systems, 3(March), 121–154. https://doi.org/10.1016/j.iotcps.2023.04.003 \nSackett, P. R., Demeke, S., Bazian, I. M., Griebie, A. M., Priest, R., & Kuncel, N. R. (2023). \nA contemporary look at the relationship between general cognitive ability and job \nperformance. Journal of Applied Psychology, advance online publication. \nhttps://doi.org/10.1037/apl0001159  \nSackett, P. R., Zhang, C., Berry, C. M., & Lievens, F. (2022). Revisiting meta-analytic \nestimates of validity in personnel selection: Addressing systematic overcorrection for \nrestriction of range. Journal of Applied Psychology, 107(11), 2040-2068. \nhttps://doi.org/10.1037/apl0000994  \nLarge Language Model Performance on Cognitive Tests    31 \nSchmidt, F. L., & Hunter, J. E. (1998). The validity and utility of selection methods in \npersonnel psychology: Practical and theoretical implications of 85 years of research \nfindings. Psychological Bulletin, 124(2), 262-274. https://doi.org/10.1037/0033-\n2909.124.2.262  \nSchreiner, M. (2023). GPT-4 architecture, datasets, costs, and more leaked. Available at \nhttps://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/ \nShinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan, K., & Yao, S. (2023). \nReflexion: Language agents with verbal reinforcement learning. arXiv, 2303.11366. \nhttps://doi.org/10.48550/arXiv.2303.11366  \nSimilar Web. (2023). Traffic analytics, ranking stats, and tech stack. Retrieved December 12, \n2023 from https://www.similarweb.com/website/chat.openai.com/#demographics \nSteger, D., Schroeders, U., & Gnambs, T. (2020). A meta-analysis of test scores in proctored \nand unproctored ability assessments. European Journal of Psychological Assessment, \n36. https://doi.org/10.1027/1015-5759/a000494  \nTempler, K. J., & Lange, S. R. (2008). Internet testing: Equivalence between proctored lab \nand unproctored field conditions. Computers in Human Behavior, 24(3), 1216-1228. \nhttps://doi.org/https://doi.org/10.1016/j.chb.2007.04.006  \nTippins, N. T., Beaty, J., Drasgow, F., Gibson, W. M., Pearlman, K., Segall, D. O., & \nShepherd, W. (2006). Unproctored internet testing in employment settings. Personnel \nPsychology, 59(1), 189-225. https://doi.org/10.1111/j.1744-6570.2006.00909.x  \nWang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., & Sun, H. (2022). Towards \nunderstanding chain-of-thought prompting: An empirical study of what matters. \narXiv, 2212.10001v2. https://doi.org/10.48550/arXiv.2212.10001  \nWebb, T., Holyoak, K. J., & Lu, H. (2023). Emergent analogical reasoning in large language \nmodels. Nature Human Behaviour, 7(9), 1526-1541. \nLarge Language Model Performance on Cognitive Tests    32 \nWei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, \nD. (2022). Chain-of-thought prompting elicits reasoning in large language models. \narXiv, arXiv:2201.11903v6. https://doi.org/10.48550/arXiv.2201.11903  \nWhite, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, \nJ., & Schmidt, D. C. (2023). A prompt pattern catalog to enhance prompt engineering \nwith ChatGPT. arXiv, rXiv:2302.11382v1. https://doi.org/10.48550/arXiv.2302.11382  \nWoods, S. A., & Patterson, F. (2023). A critical review of the use of cognitive ability testing \nfor selection into graduate and higher professional occupations. Journal of \nOccupational and Organizational Psychology. https://doi.org/10.1111/joop.12470  \nXu, B., Yang, A., Lin, J., Wang, Q., Zhou, C., Zhang, Y., & Mao, Z. (2023). \nExpertPrompting: Instructing Large Language Models to be distinguished experts. \narXiv, 2305.14688. https://doi.org/10.48550/arXiv.2305.14688  \n \n Large Language Model Performance on Cognitive Tests   33 \n \nTable 1 \nAverage Performance of LLMs on Quantitative and Verbal Ability Tests \nGPT-3.5 GPT-4 \n Quantitative Verbal  Quantitative Verbal \n Score Percentile Score Percentile  Score Percentile Score Percentile \nOverall 3.46 4.69 12.39 45.39 Overall 6.25 12.06 19.46 94.55 \nPrompt     Prompt     \n  Base 4.50 9.29 13.17 54.24   Base 4.25 6.70 19.00 94.55 \n  Bare Bones 3.50 6.70 7.00 8.96   Bare Bones 5.25 9.29 17.50 90.58 \n  COT 3.75 6.70 14.00 62.90   COT 7.50 18.73 22.00 99.57 \n  Persona 3.75 6.70 13.50 62.90   Persona 6.75 15.14 20.00 97.23 \n  V&R 2.75 4.69 12.92 54.24   V&R 8.00 18.73 18.75 94.55 \n  ME 2.50 4.69 13.75 62.90   ME 5.75 12.06 19.50 97.23 \nTemperature Temperature \n  0.0 2.92 4.69 12.42 45.39   0.0 5.75 12.06 19.17 94.55 \n  0.7 4.00 6.70 12.36 45.39   0.7 6.75 15.14 19.75 97.23 \nNote. Score is the mean score achieved across multiple trials. All quantitative test results are for the open-ended (original) version of \nthe test. Overall = average score achieved for specified GPT model across all prompts and temperatures. Initially, two trials were used \nfor each LLM-prompt-temperature triplet, and a third was added when the first two trials disagreed by 4 or more points. Prompt scores \nare averaged across the trials and two temperatures. Temperature scores are averaged across the trials and six prompts. Percentiles \ncalculated after rounding mean score to the nearest integer based on responses to the original tests. COT = Chain of thought; ME = \nMethod Explained; V&R = Vocalize and Reflect; Temp. = Temperature. \n Large Language Model Performance on Cognitive Tests   34 \n \n \nFigures \nFigure 1 \nPercentiles Achieved by GPT Across Prompts, Models, and Temperatures \n \nNote. Temperatures indicated with text on the plot points. All quantitative test results are for the \nopen-ended (original) version of the test. Percentiles calculated after rounding mean score to the \nnearest integer based on responses to the original tests. \n\n Supplement - Performance of Large Language Models on Cognitive Ability Tests   1 \n \nTable S1 \nPrompts used for quantitative ability test \nPrompt Name Quantitative Ability Test Prompt \nBase [Test instructions] \n[Test item] \nProvide the final answer delimited within curly brackets \nBare bones What is the missing number in the sequence below: \n[Test item] \nProvide the final answer delimited within curly brackets \nChain-of-thought I will provide numeric sequences and it will be your job to provide \nstep-by-step instructions for solving them. \n[Test instructions] \n        Here are some examples of successful attempts to solve the \nproblem. \n         \n        Q: What is the missing number in the sequence below: \n        1, 7, null, 25, 37, 51.  \n         \n        A:  \nThe missing number in the sequence can be found by recognizing \nthe pattern in the series. The pattern here is that the difference \nbetween successive terms is increasing by a constant amount. \n        Let's break this down step by step: \n  \n        1. Firstly, list out the sequence and identify the missing \nnumber. Here, the sequence is [1, 7, null, 25, 37, 51], with the 'null' \nbeing the missing number. \n  \n        2. Secondly, find the difference between consecutive terms \n(excluding the part where the number is missing). \n        Between 1 and 7, the difference is 7 - 1 = 6. \n     Between 7 and the next available number 25, the difference is \nunknown since we don't know the missing number. \n        Between 25 and 37, the difference is 37 - 25 = 12. \n        Between 37 and 51, the difference is 51 - 37 = 14. \n  \n        3. Notice the differences between these consecutive terms. \nYou'll see that they are increasing by a constant amount each time. \nThis indicates that this is an arithmetic series. Here, the differences \nare increasing by 6 each time (12 - 6 = 6, 14 - 8 = 6). \n        Based on this pattern, we can predict that the difference \nbetween 7 and the missing number should be 6 + 2 = 8. So, the \nmissing number should be 7 + 8 = 15. \n        So, the missing number in the sequence is 15. \n        15 \n Supplement - Performance of Large Language Models on Cognitive Ability Tests   2 \n \n \n  \n        Q: What is the missing number in the sequence below? \n        24, 20, 40, 36, 72, null. \n \nA:  \n        1. Look at the sequence and identify the obvious patterns: The \nsequence given is: 24, 20, 40, 36, 72, null. Here, we can see that \neach number isn't strictly increasing or decreasing, which means the \nsequence might not be as simple as addition or subtraction. \n  \n  \n        2. Divide the sequence into chunks and observe the patterns \nwithin the chunks: It may be beneficial to split the sequence into \nsmaller chunks to see if there are any patterns within them. In this \ncase, we can split it as: (24, 20), (40, 36), (72, null). \n \n        3. Upon doing so, we can see that the first number in each pair \nis decreasing by 4 (24-20=4, 40-36=4), which suggests there might \nbe a pattern here. \n        Compare the chunks to each other to find larger patterns: \nComparing the first number in each pair, we can see another \npattern: each first number in the pair is double the corresponding \nfirst number in the previous pair (202=40, 402=80). The pattern \nimplies that the first number in the pair (72, null) should be \n72*2=144, not 72. \n \n        4. Apply the identified patterns to find the missing number: \nBased on the patterns we've identified, the first number of each pair \nis double the first number of the previous pair, and the second \nnumber is 4 less than the first. \n        Applying this pattern to the last pair (72, null), we find that the \nmissing number should be 72-4=68. \n        So, the complete sequence would be: 24, 20, 40, 36, 72, 68. \n        68 \n \n[Test item] \n \nProvide the final answer delimited within curly brackets \n \nMethod explained I will provide numeric sequences and it will be your job to provide \nstep-by-step instructions for solving them. \n[Test instructions] \nBefore you answer, explain what your methodology will be in \nadvance. Let’s say you are given a sequence of 6 numbers, and one \nof the numbers has been hidden from the sequence. You know that \na mathematical rule determines the progression of the sequence. \n Supplement - Performance of Large Language Models on Cognitive Ability Tests   3 \n \n \nHow would you go about determining the missing number? \n[Test item] \nProvide the final answer delimited within curly brackets \nPersona You are an experienced maths teacher, known for your high \nconscientiousness and you excel at pattern recognition, with a \nperfect grasp of common algebraic transformations and number \nsequences. \n[Test instructions] \n[Test item] \nProvide the final answer delimited within curly brackets \nVocalize and Reflect [Test instructions] \n[Test item] \nPlease provide your initial judgment and explain your reasoning.  \nNext, review your initial response, identifying possible oversights \nor improvements.  \nConclude with a one word final response delimited within curly \nbrackets, incorporating any enhancements and providing a thorough \nexplanation of your reasoning. \n \n  \n Supplement - Performance of Large Language Models on Cognitive Ability Tests   4 \n \n \nTable S2 \nPrompts used for verbal ability test \nPrompt Name Quantitative Ability Test Prompt \nBase You are completing a verbal reasoning assessment.  \n[Test instructions] \n[Test item] \nWork out the answer step by step, and then write your final answer \ndelimited within curly brackets. \nBare bones Is the following statement True, False, or Cannot Say?:  \n[Test item] \nProvide the final answer delimited within curly brackets \nChain-of-thought You are completing a verbal reasoning assessment. \nYou will be shown passages of text, and statements relating to \nthem. You must identify whether or not each statement is accurate, \naccording to the passage of text. \n    If, according to the text, the statement is true, answer True. If, \naccording to the text, it is false, answer False. \n    If the text doesn't provide enough information to determine the \nstatement to be true or false, answer Cannot Say.  \n    For each statement, there is only one correct answer. \n     \n    Passage of text:  \n    Sodium chloride, or salt, is essential for human life. A diet \ndeficient in salt can cause muscle cramps, neurological problems \n    and even death. Conversely, a diet high in salt leads to an \nincreased risk of conditions such as hypertension, heart disease and \nstroke. In spite of high-profile campaigns to raise awareness, salt \nconsumption has increased 50% in the past four decades, with the \naverage adult ingesting more than double the amount of salt their \nbody requires. Much of this increase can be attributed to the advent \nof frozen and processed foods. For individuals wishing to reduce \ntheir sodium intake, the answer is not simply rejecting the salt \nshaker;  \n    75% of the average person's salt consumption comes from food, \nsuch as bread, cereals and cheese. \n    Statement \n    Frozen and processed foods contain no more salt than contained \nin a typical diet. \n1. The passage mentions that the average adult consumes more than \ndouble the amount of salt their body requires and that salt \nconsumption has increased by 50% over the past four decades. \n        2. This increase in salt consumption is attributed to the advent \nof frozen and processed foods, according to the passage. \n        3. However, the passage does not provide explicit information \non the salt content in frozen and processed foods compared to a \n Supplement - Performance of Large Language Models on Cognitive Ability Tests   5 \n \n \ntypical diet. \n        4. It is also unclear whether this increase in salt consumption is \ndue to a higher salt content in these foods, or due to people \nconsuming more of these types of foods, or both. \n        5. Without explicit information about the relative salt content \nin frozen and processed foods compared to a typical diet, we can't \ndefinitively say whether these foods contain more salt than a typical \ndiet. \n        Thus, we \"cannot say\" that \"Frozen and processed foods \ncontain no more salt than contained in a typical diet\", based on the \ninformation given in the passage. \n        The final answer is {Cannot Say}. \n[Test instructions] \n[Test item] \nWork out the answer step by step, and then write your final the \nanswer delimited within curly brackets. \nMethod Explained You are completing a verbal reasoning assessment. \n[Test instructions] \nFor each statement, there is only one correct answer. Before you \nanswer, please outline in detail the methodology for completing \nsuch tests accurately. \n[Test item] \nWork out the answer step by step, and then write your final the \nanswer delimited within curly brackets. \nPersona You are completing a verbal reasoning assessment.  \nYou are an experienced corporate analyst at a large organization, \nknown for your meticulous attention to detail and your ability to \nmake accurate judgments based on documents and reports. \n    Today, you've been given a new report to review: \n[Test instructions] \n[Test item] \nWork out the answer step by step, and then write your final the \nanswer delimited within curly brackets. \nVocalize and Reflect You are completing a verbal reasoning assessment. \n[Test instructions] \nPlease provide your initial judgment and explain your reasoning. \nNext, review your initial response, identifying possible oversights \nor improvements. Conclude with a one word final response \ndelimited within curly brackets, incorporating any enhancements \nand providing a thorough explanation of your reasoning. \nRemember, there's only one correct answer for each statement. \n[Test item] \nWork out the answer step by step, and then write your final the \nanswer delimited within curly brackets. \n  \n Supplement - Performance of Large Language Models on Cognitive Ability Tests   6 \n \n \nTable S3 \nGPT Scores on Quantitative Ability Test \nPrompt Model Temp. Mean Percentile \nBase GPT 3.5 0 4 6.70 \n  0.7 5 9.29 \n GPT 4 0 4 6.70 \n  0.7 4.5 9.29 \nBare Bones GPT 3.5 0 3.5 6.70 \n  0.7 3.5 6.70 \n GPT 4 0 5 9.29 \n  0.7 5.5 12.06 \nChain of Thought GPT 3.5 0 2.5 4.69 \n  0.7 5 9.29 \n GPT 4 0 6.5 15.14 \n  0.7 8.5 22.80 \nMethod Explained GPT 3.5 0 2 3.05 \n  0.7 3 4.69 \n GPT 4 0 4 6.70 \n  0.7 7.5 18.73 \nPersona GPT 3.5 0 4 6.0 \n  0.7 3.5 6.70 \n GPT 4 0 6 12.06 \n  0.7 7.5 18.73 \nVocalize and Reflect GPT 3.5 0 1.5 3.05 \n  0.7 4 6.70 \n GPT 4 0 9 22.80 \n  0.7 7 15.14 \nNote. Mean is the average score from GPT achieved across multiple trials. Initially, two trials \nwere used, and a third was added when the first two trials disagreed by 4 or more points. \nPercentiles calculated after rounding mean score to the nearest integer.  \n  \n Supplement - Performance of Large Language Models on Cognitive Ability Tests   7 \n \n \nTable S4 \nGPT Scores on Verbal Ability Test \nPrompt Model Temp. Mean Percentile \nBase GPT 3.5 0 12 45.39 \n  0.7 14.33 62.90 \n GPT 4 0 19 94.55 \n  0.7 19 94.55 \nBare Bones GPT 3.5 0 7 8.96 \n  0.7 7 8.96 \n GPT 4 0 17.5 90.58 \n  0.7 17.5 90.58 \nChain of Thought GPT 3.5 0 15 71.13 \n  0.7 13 54.24 \n GPT 4 0 21.5 99.57 \n  0.7 22.5 99.88 \nMethod Explained GPT 3.5 0 13 54.24 \n  0.7 14.5 71.13 \n GPT 4 0 17.5 90.58 \n  0.7 21.5 99.57 \nPersona GPT 3.5 0 14 62.90 \n  0.7 13 54.24 \n GPT 4 0 20 97.23 \n  0.7 20 97.23 \nVocalize and Reflect GPT 3.5 0 13.50 62.90 \n  0.7 12.33 45.39 \n GPT 4 0 19.5 97.23 \n  0.7 18 90.58 \nNote. Mean is the average score from GPT achieved across multiple trials. Initially, two trials \nwere used, and a third was added when the first two trials disagreed by 4 or more points. \nPercentiles calculated after rounding mean score to the nearest integer. \n  \n Supplement - Performance of Large Language Models on Cognitive Ability Tests   8 \n \n \nTable S5 \nGPT Scores on Quantitative Ability Test with Multiple Choice Response Options \n   Original Format Missing Number Last \nPrompt Model Temp. Mean Percentile Mean Percentile \nBase GPT 3.5 0 7 15.14 5 9.29 \n  0.7 5 9.29 5.5 12.06 \n GPT 4 0 7.5 18.73 11.5 37.65 \n  0.7 9.5 27.28 12.5 44.13 \nChain of Thought GPT 3.5 0 4 6.70 7 15.14 \n  0.7 4 6.70 5 9.29 \n GPT 4 0 7 15.14 11 32.11 \n  0.7 7.5 18.73 11.5 37.65 \nMethod Explained GPT 3.5 0 4 6.70 4.5 9.29 \n  0.7 4 6.70 5 9.29 \n GPT 4 0 9 22.80 13.5 51.68 \n  0.7 9.5 27.28 12.5 44.13 \nPersona GPT 3.5 0 5.5 12.06 9 22.80 \n  0.7 3.5 6.70 8 18.73 \n GPT 4 0 7 15.14 14.5 60.35 \n  0.7 7 15.14 13 44.13 \nVocalize and Reflect GPT 3.5 0 5 9.29 4.5 9.29 \n  0.7 3 4.69 3 4.69 \n GPT 4 0 9.5 27.28 15 60.35 \n  0.7 10.5 32.11 13 44.13 \nNote. Mean is the average score from GPT achieved across multiple trials. Initially, two trials \nwere used, and a third was added when the first two trials disagreed by 4 or more points for the \noriginal item format. Percentiles calculated after rounding mean score to the nearest integer. \nNotably, the percentiles are based on human responses to the fill-in-the-blank style test. Original \nformat maintained the sequence of numbers and the missing number could have been in any of \nthe six spots; Missing item last reorders the sequence of numbers so that the final item in the \nsequence is the missing number that must be identified. \n "
}