{
    "title": "Derivation of Document Vectors from Adaptation of LSTM Language Model",
    "url": "https://openalex.org/W2741841399",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A5100318285",
            "name": "Wei Li",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A5059141717",
            "name": "Brian Mak",
            "affiliations": [
                "Hong Kong University of Science and Technology",
                "University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2121227244",
        "https://openalex.org/W1092227168",
        "https://openalex.org/W168564468",
        "https://openalex.org/W2166122509",
        "https://openalex.org/W2408752871",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W2167277498",
        "https://openalex.org/W4386506836",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W1780387844",
        "https://openalex.org/W2143017621",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2156354361",
        "https://openalex.org/W2043909051",
        "https://openalex.org/W2105591985",
        "https://openalex.org/W941230081",
        "https://openalex.org/W100623710",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W2516925101",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2045455073",
        "https://openalex.org/W1486649854",
        "https://openalex.org/W2953320089",
        "https://openalex.org/W1985258458",
        "https://openalex.org/W4233906699"
    ],
    "abstract": "In many natural language processing (NLP) tasks, a document is commonly modeled as a bag of words using the term frequency-inverse document frequency (TF-IDF) vector. One major shortcoming of the frequency-based TF-IDF feature vector is that it ignores word orders that carry syntactic and semantic relationships among the words in a document. This paper proposes a novel distributed vector representation of a document, which will be labeled as DV-LSTM, and is derived from the result of adapting a long short-term memory recurrent neural network language model by the document. DV-LSTM is expected to capture some high-level sequential information in the document, which other current document representations fail to do. It was evaluated in document genre classification in the Brown Corpus and the BNC Baby Corpus. The results show that DV-LSTM significantly outperforms TF-IDF vector and paragraph vector (PV-DM) in most cases, and their combinations may further improve the classification performance.",
    "full_text": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 456–461,\nValencia, Spain, April 3-7, 2017.c⃝2017 Association for Computational Linguistics\nDerivation of Document Vectors from Adaptation of LSTM Language\nModel\nWei Li and Brian Mak\nDepartment of Computer Science and Engineering\nThe Hong Kong University of Science and Technology\n{wliax, mak}@cse.ust.hk\nAbstract\nIn many natural language processing\ntasks, a document is commonly mod-\neled as a bag of words using the\nterm frequency-inverse document fre-\nquency (TF-IDF) vector. One major short-\ncoming of the TF-IDF feature vector is\nthat it ignores word orders that carry syn-\ntactic and semantic relationships among\nthe words in a document. This paper pro-\nposes a novel distributed vector represen-\ntation of a document called DV-LSTM.\nIt is derived from the result of adapting\na long short-term memory recurrent neu-\nral network language model by the doc-\nument. DV-LSTM is expected to capture\nsome high-level sequential information in\na document, which other current document\nrepresentations fail to do. It was evalu-\nated in document genre classiﬁcation in\nthe Brown Corpus , the BNC Baby Cor-\npus, and the Penn Treebank Dataset. The\nresults show that DV-LSTM signiﬁcantly\noutperforms TF-IDF vector and paragraph\nvector (PV-DM) in most cases, and their\ncombinations may further improve classi-\nﬁcation performance.\n1 Introduction\nIn many classiﬁcation tasks in the area of natu-\nral language processing (NLP), it is necessary to\ntransform text documents of variable lengths into\nvectors of a ﬁxed length so that they can be clas-\nsiﬁed or compared as most classiﬁers only work\non inputs of a ﬁxed length. Perhaps the most\npopular document vectors is the term frequency-\ninverse document frequency(TF-IDF) feature vec-\ntor (Robertson and Jones, 1976). Term-frequency-\nbased document vectorization makes two assump-\ntions (Cachopo, 2007; Le and Mikolov, 2014): (a)\noccurrences of each term are mutually indepen-\ndent, and (b) a document is treated as a “bag of\nwords” and different permutations of the same set\nof words are considered to be same. These as-\nsumptions suffers from a major drawback that it\nignores word orders and other sequential informa-\ntion in a document which can be important in some\nNLP tasks such as genre classiﬁcation. For exam-\nple, ‘Wall’ and ‘Street’ in the named entity ‘Wall\nStreet’ are treated as independent words in a TF-\nIDF vector. Using an n-gram TF-IDF vector may\nalleviate the problem to some extent, but it is still\nhard to capture long-distance or high-level abstract\nsequential patterns. Moreover, (n-gram) TF-IDF\nvectors cannot capture syntactic or semantic re-\nlationship/similarity between words, paragraphs,\nand documents. Another notable document vec-\ntorization is the paragraph vector that learns from\na distributed memory model (PV-DM), which is\na succinct distributed representation of sentences\nor paragraphs (Le and Mikolov, 2014; Dai et al.,\n2015; Ai et al., 2016). PV-DM has been shown\nto perform signiﬁcantly better than the bag-of-\nwords model in many NLP tasks. Moreover, skip-\nthought vectors (Kiros et al., 2015) that are de-\nrived from recurrent encoder-decoder models also\nshow superior performance against the bag-of-\nwords model.\nIn this paper, we propose a novel document\nvectorization method which adapts 1 a long short-\nterm memory recurrent neural network (RNN)\nlanguage model (LSTM-LM) (Sundermeyer et al.,\n2012) with a document, and then vectorize the\n1One may also treat our adaptation method as re-training\nthe initial LSTM-LM with the adapting document.\n456\nadapted model parameters to obtain its document\nvector, labeled as DV-LSTM. Since the recurrent\nnature of LSTM-LM should capture some high-\nlevel and abstract sequential information from its\ntraining documents, if the LM adaptation is ef-\nfective, each adapted LM will contain distinctive\nsequential information of the adapting document,\nand the adapted parameters may be used to rep-\nresent the adapting document distinctively. Our\nDV-LSTM is similar to the TF-IDF vector and PV-\nDM in that they all can be derived in an unsuper-\nvised manner. Compared with the TF-IDF vector,\nDV-LSTM is more expressive as it makes use of\ncontinuous word embedding and sequential infor-\nmation in a document. Compared with PV-DM,\nDV-LSTM does not suffer from the limitation due\nto a sliding context window on the inputs.\n2 LSTM Language Modeling\nRNN language model (LM) — especially the\nlong short-term memory language model (LSTM-\nLM) — is the state-of-the-art language models\n(Mikolov et al., 2010; Mikolov et al., 2011; Ben-\ngio et al., 2006). LSTM-LM is chosen to de-\nvelop our document vectorization for three rea-\nsons. Firstly, it can capture comparatively more\ndistant patterns in a document that are not limited\nby the size of the input context window. Thus, the\nmodel parameters of an LSTM-LM can encapsu-\nlate the different grammars and styles in its train-\ning documents. Secondly, the hidden layer(s) of\nan LSTM provide a distributed representation of\nthe input words in a continuous space so that the\nsemantic and syntactic relationship among words\ncan be captured. Finally, by controlling the size\nof the hidden layer(s) and the model parameters\nto adapt, one may effectively adjust the number of\nmodel parameters to adapt according to the size\nof the adapting document to ensure that the ﬁnal\ndocument vector is derived robustly.\nFigure 1 shows the LSTM-LM network for\ntraining our document vectors. The input to the\nmodel is the current word wt represented by its\none-hot encoding, which is projected to a dis-\ntributed representation by a linear identity com-\npression layer and then by a non-linear sigmoid\nlayer. The identity compression layer also helps\nmake the model more compact so as to improve\ntraining speed. Let st be the hidden state for the\ninput word wt. The model is trained to give two\nkinds of outputs to the word class layer ( vt) as\nFigure 1: The LSTM network chosen to derive our\ndocument vectors. (The recurrency of LSTM cells\nis not shown)\nwell as to the output word layer ( wt+1) (Mikolov\net al., 2011). That is, it produces the posterior\nprobability P(vt|st) of the word class vt given\nthe current state st, and the posterior probability\nP(wt+1|vt, st) of the next word wt+1 given the\ncurrent word class and LSTM state.\n3 Document Vectorization by LSTM-LM\nAdaptation\nWe propose to derive document vectors (DVs)\nfrom a well-trained parent LSTM language model\nby adaptation using the following procedure:\nSTEP 1: Train a parent LM using all the docu-\nments in a training corpus.\nSTEP 2: Adapt the parent LM with each document\nin the training corpus.\nSTEP 3: Extract model parameters of interest\nfrom the adapted LM, and vectorize them to\nproduce DV-LSTM for the adapting document.\n3.1 Derivation of DV-LSTM\nIn our experiments, the LSTM neural network of\nFigure 1 has 200 units in the identity compres-\nsion layer, 100 units in the sigmoid compression\nlayer, 100 LSTM units, 500 word classes and V\noutput units (where V is the vocabulary size). In\nthe derivation of DV-LSTM, only the biases in the\nsigmoid layer bl ∈R100, LSTM layerbm ∈R400,\nand word-class layer bc ∈R500 are adapted. The\nLSTM bias vector bm is further comprised of four\n100-dimensional bias sub-vectors: input-gate bi-\nases bmi , forget-gate biases bmf , output-gate bi-\nases bmo , and cell biases bmc .\n457\nThe 3 different biases are supposed to capture\ndifferent and complementary information in a doc-\nument: bl is to capture the abstract and distributed\nword embeddings; bm is to capture the long-span\nsequential text information in a document;bc is to\ncapture the word class statistics. The 3 biases are\nconcatenated to the ﬁnal 1000-dimensional DV-\nLSTM document vector as follows:\nDV-LSTM = [n(b′\nml), n(b′\nc)]′ , (1)\nwhere bml is given by\n[n(b′\nmi ), n(b′\nmf ), n(b′\nmo ), n(b′\nmc ), n(b′\nl)]′ . (2)\nIn Eq.(1) and Eq.(2), n(·) is the normalization op-\nerator which normalizes a vector to the unit norm.\nAccording to some previous researches in genre\nclassiﬁcation, it is found that models ﬁtted on\nsome lower-level features (e.g., term-frequency re-\nlated feature, which is highly correlated to the\ntopic and language) may actually hurt genre clas-\nsiﬁcation when they are tested on new documents\nof the same genre but of different topic or lan-\nguage (Petrenz and Webber, 2011; Petrenz, 2009;\nPetrenz, 2012).\nIn our model, bm is a high-level abstract fea-\nture, which is relatively independent of the topic or\nlanguage speciﬁc term-frequency distribution. bc\nis a lower-level feature that is related to the word\nclusters. Comparing with n-gram term-frequency\nfeatures whose good performance depend on a\nstrong topic-genre correlation, bc is a relatively\nmoderate lower-level feature. We believe that by\ncombining high-level abstract features and lower-\nlevel features, our model may perform better in\nsituations where the term-frequency based pattern\nis not entirely reliable for classiﬁcation. Such is\nthe case in the genre classiﬁcation tasks of this pa-\nper, where term-frequency distribution can be con-\nfused by different topic-genre correlation.\n4 Experimental Evaluation: Text Genre\nClassiﬁcation\nThe proposed document vector DV-LSTM was\nevaluated on the genre classiﬁcation of documents\nin three corpora:\n• Brown Corpus (Brown) (Francis and Kucera,\n1979): It consists of 500 documents with a to-\ntal of about 1 million words distributed across\n15 genres organized hierarchically in three\nlevels. The sub-genres under theﬁction genre\nwere merged (Wu et al., 2010) so that the to-\ntal number of genres was reduced to 10.\n• BNC Baby Corpus (BNCB) (Burnard, 2003):\nIt is a subset of BNC, consisting of 182 doc-\numents written in 4 genres: ﬁction, newspa-\npers, academic and conversation. Each genre\nconsists of a total of about 1 million words.\n• Penn Treebank Dataset (PTB): It was artiﬁ-\ncially extracted from the Penn Treebank Cor-\npus by taking out the documents that have\ngenre tags provided by (Webber, 2009; Plank,\n2009). It has 5 genres: essays, highlights, let-\nters, errata and news. The errata genre was\nremoved as there are very few documents of\nthat genre. We also removed short documents\nwith fewer than 200 words from the dataset.\nAt the end, the dataset has a total of 239 doc-\numents in 4 genres: 38 highlights, 95 essays,\n42 letters, and 64 news.\n4.1 Text pre-processing and SVM training\nThe Natural Language Toolkit (NLTK) (Loper\nand Bird, 2002) was used for tokenization, and\nthe WordNet Lemmatizer (Miller, 1994) was used\nfor text pre-processing. The letters in the docu-\nments were also converted to lower cases to im-\nprove the TF-IDF baseline performance, and the\nword classes were determined by Brown cluster-\ning (Brown et al., 1992). During the unsupervised\ntraining of PV-DMs and DV-LSTMs, documents\nin a dataset were shufﬂed to eliminate the possibil-\nity that a classiﬁer may simply use the position of\ndocuments for genre classiﬁcation. All data were\nmean-zeroed before inputting to the classiﬁer.\nFor each type or combination of document fea-\nture vectors, a linear SVM classiﬁer was built\nfrom the training dataset using LinearSVC from\nthe scikit-learn toolkit 2. To improve the reliabil-\nity of experimental results, documents in each cor-\npus were shufﬂed ten times, and for each shufﬂed\ndataset, a 10-fold cross-validation was conducted.\nOur DV-LSTM was tested against the TF-IDF fea-\nture and the state-of-the-art paragraph vector PV-\nDM. Results are reported in terms of classiﬁcation\naccuracies that are averages from classiﬁcations\nover 10 ×10-fold cross validations.\n2Empirically, we did not get better results using nonlinear\nkernels such as the RBF kernel.\n458\n4.2 Training of document vector DV-LSTM\nThe RWTH Aachen University Neural Network\nLanguage Modeling Toolkit (RWTHLM) (Sunder-\nmeyer et al., 2015; Sundermeyer et al., 2014)\nwas used for training all LSTM-LMs and adapting\nthem to produce the DV-LSTMs. The length of\nhistorical context is the concatenation of the de-\nfault sentence segmentations in the original cor-\npus up to 500 characters. The parent model was\ntrained with a maximum of 10 epochs, while LM\nadaptation took at most 15 epochs. The initial\nlearning rates were set to 0.02. The sub-vectors in\nbm were whitened ﬁrst (mean-zeroed and scaling\nto the unit variance for each axis) before concate-\nnation.\nTable 1: Values of various hyperparameters being\ntuned for the derivation of the best PV-DM.\ncontext window size {5, 10, 15, 20}\nmin. word frequency {0, 5, 10, 20}\nnegative word samples {0, 10 , 20}\ndownsampling threshold {0, 5E-5}\n4.3 Training of paragraph vector PV-DM\nA PV-DM was trained for each document in a cor-\npus using the Gensim toolkit ( ˇReh˚uˇrek and Sojka,\n2010). They were trained for 20 epochs with an\ninitial learning rate of 0.025. PV-DMs with di-\nmensions of 100, 500 and 2000 were investigated,\nand it was found that PV-DMs of 500 dimensions\nprovide consistently good performance; they are\ndenoted as PV 500. The optimal hyperparameters\nfor PV-DM derivation were grid-searched for each\ntask using 1/10 of its corpus data. The hyperpa-\nrameters and their values tried in the grid search\nare summarized in Table 1.\nMost hyperparameters in Table 1 are also shared\nby the training of DV-LSTM. However, due to the\nlimitation of the current experiment platform and\nthe cost of grid searches, we do not tune these hy-\nperparameters in training DV-LSTM. Hence the\ncorresponding hyperparameters in DV-LSTM are\nall set to 0 unless stated explicitly. Thus DV-\nLSTM is expected to have a disadvantage in the\ntuning of hyperparameters.\n4.4 Summary\nTable 2 summarizes the dimension of various fea-\nture vectors used in the experiments, where z1000\n5\nTable 2: The dimension of various feature vectors.\nFeature Dimension\nPV500 500\nz1000\n5 1,000\nDV-LSTM 1,000\nz5 10,000\nand z5 represent the TF-IDF feature vectors using\nthe top 1,000 and 10,000 5-grams respectively.\n4.5 Experimental Results\nThe genre classiﬁcation accuracy and the weighted\nF-score results using different feature vectors over\nthe three corpora are summarized in Table 3 and\nTable 4.\nTable 3: Genre classiﬁcation accuracy (%).\nFeatures PTB Brown BNCB\n4-char-gram* - 64.40 -\n5-gram z5 80.91 65.24 96.27\nPV500 81.63 65.68 98.35\nDV-LSTM-bm 75.93 60.14 98.50\nDV-LSTM-bc 82.63 63.88 99.45\nDV-LSTM 84.70 65.20 100.00\nDV-LSTM-PV500 86.00 67.00 100.00\nDV-LSTM-z1000\n5 86.38 66.84 100.00\nTable 4: Genre classiﬁcation F-score.\nFeatures PTB Brown BNCB\n1-gram* - - 0.913\n5-gram* - - 0.956\n5-POS* - - 0.947\n5-gram z5 0.7996 0.6275 0.9623\nPV500 0.8154 0.6455 0.9820\nDV-LSTM-bm 0.7559 0.5959 0.9841\nDV-LSTM-bc 0.8239 0.6326 0.9941\nDV-LSTM 0.8434 0.6443 1.0000\nDV-LSTM-PV500 0.8576 0.6613 1.0000\nDV-LSTM-z1000\n5 0.8607 0.6614 1.0000\nBesides individual features, we also investi-\ngated the contribution of each bias vector in DV-\nLSTM and the possibility of feature combinations.\nThe bold results represent the best performance for\neach task given by a single feature or a set of com-\nbined features. Results labeled with * are baseline\n459\nresults quoted from (Tang and Cao, 2015; Wu et\nal., 2010).\nWe have the following observations:\n• For both the Brown Corpus and BNCB Cor-\npus, results from our own 5-gram TF-IDF are\nbetter than the quoted baselines.\n• In general, our DV-LSTM performs better\nthan PV-DM, and PV-DM performs better\nthan the 5-gram TF-IDF. All the bold results\nare statistically signiﬁcantly better than the\n5-gram TF-IDF results based on the paired\nsample t-test (Dietterich, 1998) at the 99%\nconﬁdence level.\n• Among the single features, the proposed DV-\nLSTM performs the best in both PTB and\nBNCB tasks, and gives comparable perfor-\nmance as PV500 in the Brown Corpus.\nOne possible reason is that the hyperparame-\nters for training DV-LSTM were not as ﬁne-\ntuned as those for PV 500, giving DV-LSTM\na disadvantage. Another plausible reason is\nthat PTB’s genres are almost unrelated to the\ntopics and it likely requires more abstract se-\nquential information for their classiﬁcation.\nOn the other hand, the Brown Corpus has a\nrelatively strong overlapping between topics\nand genres. Thus, features such as TF-IDF or\nPV-DM that have good estimates of the term\nfrequencies of topic related words/phrases\ncould perform better.\n• Both PV 500 and our DV-LSTM show supe-\nrior performance comparing to the traditional\nn-gram TF-IDF. This is probably attributed\nto the neural network’s capability of learning\nabstract patterns. Moreover, the paragraph\nvector and our DV-LSTM are dense represen-\ntations of documents. They have more util-\nity than the sparse TF-IDF vector, especially\nwhen comparing the semantic and syntactic\nsimilarity of documents.\n• Between the two bias components of our DV-\nLSTM, it is interesting to see that the LSTM\nbias vector bm (and its results are labeled\nwith DV-LSTM-bm in Tables 3 and 4) is out-\nperformed by the class bias vectorbc (and its\nresults are labeled with DV-LSTM-bc in Ta-\nbles 3 and 4). Nevertheless, it seems that they\nare complementary to each other, and their\ncombination in DV-LSTM further improves\nthe classiﬁcation performance.\n5 Conclusions and Future Works\nThis paper proposes a novel distributed represen-\ntation of a document, which we call “document\nvector” (DV). Currently, we estimate the DV by\nadapting the various bias vectors and the word\nclass bias of an LSTM-LM network trained from\nthe corpus of a task. We believe that these pa-\nrameters capture some word ordering information\nin a larger context that may supplement the stan-\ndard frequency-based TF-IDF feature or the para-\ngraph vector PV-DM in solving many NLP tasks.\nHere, we only conﬁrm its effectiveness in docu-\nment genre classiﬁcation. In the future, we would\nlike to investigate the effectiveness of our DV-\nLSTM in other NLP problems such as topic clas-\nsiﬁcation and sentiment detection. Moreover, we\nwould also like to investigate the utility of this\nmodel (or its variants) in the cross-lingual prob-\nlems, as high-level sequential pattern captured by\nthe (deep) hidden layers is expected to be rela-\ntively language independent.\n6 Acknowledgements\nThe work described in this paper was supported\nby grants from the Research Grants Council of the\nHong Kong Special Administrative Region, China\n(Project Nos. HKUST616513, HKUST16206714\nand HKUST16215816).\nReferences\nQingyao Ai, Liu Yang, Jiafeng Guo, and W. Bruce\nCroft. 2016. Analysis of the paragraph vector\nmodel for information retrieval. In Proceedings of\nthe 2016 ACM on International Conference on the\nTheory of Information Retrieval , pages 133–142.\nACM.\nYoshua Bengio, Holger Schwenk, Jean-S ´ebastien\nSen´ecal, Fr ´ederic Morin, and Jean-Luc Gauvain.\n2006. Neural probabilistic language models. In\nInnovations in Machine Learning , pages 137–186.\nSpringer.\nPeter F. Brown, Peter V . deSouza, Robert L. Mercer,\nT. J. Watson, Vincent J. Della Pietra, and Jenifer C.\nLai. 1992. Class-based n-gram models of natural\nlanguage. Computational Linguistics , 18(4):467–\n480.\nLou Burnard. 2003. Reference guide for BNC Baby.\n460\nAna Margarida de Jesus Cardoso Cachopo. 2007. Im-\nproving methods for single-label text categorization.\nPh.D. thesis, Universidade T´ecnica de Lisboa.\nAndrew M. Dai, Christopher Olah, and Quoc V . Le.\n2015. Document embedding with paragraph vec-\ntors. arXiv preprint arXiv:1507.07998.\nThomas G. Dietterich. 1998. Approximate statistical\ntests for comparing supervised classiﬁcation learn-\ning algorithms. Neural computation , 10(7):1895–\n1923.\nW. Nelson Francis and Henry Kucera. 1979. Brown\ncorpus manual. Brown University, 15.\nRyan Kiros, Yukun Zhu, Ruslan R. Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nC. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama,\nand R. Garnett, editors, Advances in Neural Infor-\nmation Processing Systems 28 , pages 3294–3302.\nCurran Associates, Inc.\nQuoc V . Le and Tomas Mikolov. 2014. Distributed\nrepresentations of sentences and documents. In Pro-\nceedings of The 31st International Conference on\nMachine Learning, volume 14, pages 1188–1196.\nEdward Loper and Steven Bird. 2002. NLTK: The nat-\nural language toolkit. In Proceedings of the ACL-02\nWorkshop on Effective Tools and Methodologies for\nTeaching Natural Language Processing and Com-\nputational Linguistics , pages 63–70, Philadelphia,\nPennsylvania, USA, July. Association for Compu-\ntational Linguistics.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In Pro-\nceedings of Interspeech, pages 1045–1048.\nTom´aˇs Mikolov, Stefan Kombrink, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn 2011 IEEE International Conference on Acous-\ntics, Speech and Signal Processing , pages 5528–\n5531. IEEE.\nGeorge A. Miller. 1994. Wordnet: A lexical database\nfor english. In HUMAN LANGUAGE TECHNOL-\nOGY: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994, page 468.\nPhilipp Petrenz and Bonnie Webber. 2011. Squibs:\nStable classiﬁcation of text genres. Computational\nLinguistics, 37(2):385–394.\nPhilipp Petrenz. 2009. Assessing approaches to genre\nclassiﬁcation. Master’s thesis, School of Informat-\nics, University of Edinburgh.\nPhilipp Petrenz. 2012. Cross-lingual genre classiﬁca-\ntion. In Proceedings of the Student Research Work-\nshop at the 13th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 11–21, Avignon, France, April. Association\nfor Computational Linguistics.\nBarbara Plank. 2009. PTB/PDTB ﬁles\nbelonging to different genres. http:\n//www.let.rug.nl/˜bplank/metadata/\ngenre_files_updated.html.\nRadim ˇReh˚uˇrek and Petr Sojka. 2010. Software frame-\nwork for topic modelling with large corpora. In\nProceedings of the LREC 2010 Workshop on New\nChallenges for NLP Frameworks, pages 45–50, Val-\nletta, Malta, May. ELRA. http://is.muni.\ncz/publication/884893/en.\nStephen E. Robertson and K. Sparck Jones. 1976.\nRelevance weighting of search terms. Journal\nof the American Society for Information Science ,\n27(3):129–146.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In Proceedings of Interspeech, pages 194–197.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2014. RWTHLM – the RWTH Aachen University\nneural network language modeling toolkit. In Pro-\nceedings of Interspeech, pages 2093–2097.\nMartin Sundermeyer, Hermann Ney, and Ralf Schl¨uter.\n2015. From feedforward to recurrent LSTM neural\nnetworks for language modeling. IEEE/ACM Trans-\nactions on Audio, Speech and Language Processing,\n23(3):517–529.\nXiaoyan Tang and Jing Cao. 2015. Automatic genre\nclassiﬁcation via n-grams of part-of-speech tags.\nProcedia-Social and Behavioral Sciences, 198:474–\n478.\nBonnie Webber. 2009. Genre distinctions for dis-\ncourse in the penn treebank. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nAssociation for Computational Linguistics and the\n4th International Joint Conference on Natural Lan-\nguage Processing of the Asian Federation of Natural\nLanguage Processing, pages 674–682, Suntec, Sin-\ngapore, August. Association for Computational Lin-\nguistics.\nZhili Wu, Katja Markert, and Serge Sharoff. 2010.\nFine-grained genre classiﬁcation using structural\nlearning algorithms. In Proceedings of the 48th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 749–759, Uppsala, Sweden, July.\nAssociation for Computational Linguistics.\n461"
}