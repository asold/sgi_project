{
    "title": "CatFormer: Category-Level 6D Object Pose Estimation with Transformer",
    "url": "https://openalex.org/W4393148012",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5084793792",
            "name": "Sheng Yu",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5078047051",
            "name": "Di‚ÄêHua Zhai",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5064231378",
            "name": "Yuanqing Xia",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3001460358",
        "https://openalex.org/W3202459445",
        "https://openalex.org/W3013303114",
        "https://openalex.org/W3137885119",
        "https://openalex.org/W2972165073",
        "https://openalex.org/W3193676140",
        "https://openalex.org/W4221150169",
        "https://openalex.org/W6735463952",
        "https://openalex.org/W3134774835",
        "https://openalex.org/W2986197551",
        "https://openalex.org/W1969868017",
        "https://openalex.org/W1526868886",
        "https://openalex.org/W6771100779",
        "https://openalex.org/W2768840867",
        "https://openalex.org/W2519887557",
        "https://openalex.org/W3061410621",
        "https://openalex.org/W6648200313",
        "https://openalex.org/W6750429153",
        "https://openalex.org/W2989915422",
        "https://openalex.org/W4287112626",
        "https://openalex.org/W3134113979",
        "https://openalex.org/W4285182079",
        "https://openalex.org/W4321033239",
        "https://openalex.org/W2902749952",
        "https://openalex.org/W6754945228",
        "https://openalex.org/W3100052745",
        "https://openalex.org/W2908420029",
        "https://openalex.org/W2624503621",
        "https://openalex.org/W2604236302",
        "https://openalex.org/W2998787775",
        "https://openalex.org/W3042622540",
        "https://openalex.org/W2893763910",
        "https://openalex.org/W2128019145",
        "https://openalex.org/W2910399693",
        "https://openalex.org/W3130973709",
        "https://openalex.org/W2909314588",
        "https://openalex.org/W3193686508",
        "https://openalex.org/W2767032778",
        "https://openalex.org/W4221144809",
        "https://openalex.org/W6761549494",
        "https://openalex.org/W4289551062",
        "https://openalex.org/W6730342312",
        "https://openalex.org/W4361230911",
        "https://openalex.org/W3206934897",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W3107992529",
        "https://openalex.org/W2963188159",
        "https://openalex.org/W3034597466",
        "https://openalex.org/W2981378444",
        "https://openalex.org/W1991544872",
        "https://openalex.org/W4287273349",
        "https://openalex.org/W3177069133",
        "https://openalex.org/W3027201070",
        "https://openalex.org/W2963892972",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W2981410089",
        "https://openalex.org/W4226321491",
        "https://openalex.org/W2963177347",
        "https://openalex.org/W4313027996",
        "https://openalex.org/W2962783853",
        "https://openalex.org/W3034986117",
        "https://openalex.org/W4287273244",
        "https://openalex.org/W4312604533",
        "https://openalex.org/W4312818087",
        "https://openalex.org/W3035268949",
        "https://openalex.org/W4289489637",
        "https://openalex.org/W3092774272",
        "https://openalex.org/W3179923621",
        "https://openalex.org/W3010276570",
        "https://openalex.org/W4297818163",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2895410314",
        "https://openalex.org/W2964249569",
        "https://openalex.org/W4386075656",
        "https://openalex.org/W2963756608"
    ],
    "abstract": "Although there has been significant progress in category-level object pose estimation in recent years, there is still considerable room for improvement. In this paper, we propose a novel transformer-based category-level 6D pose estimation method called CatFormer to enhance the accuracy pose estimation. CatFormer comprises three main parts: a coarse deformation part, a fine deformation part, and a recurrent refinement part. In the coarse and fine deformation sections, we introduce a transformer-based deformation module that performs point cloud deformation and completion in the feature space. Additionally, after each deformation, we incorporate a transformer-based graph module to adjust fused features and establish geometric and topological relationships between points based on these features. Furthermore, we present an end-to-end recurrent refinement module that enables the prior point cloud to deform multiple times according to real scene features. We evaluate CatFormer's performance by training and testing it on CAMERA25 and REAL275 datasets. Experimental results demonstrate that CatFormer surpasses state-of-the-art methods. Moreover, we extend the usage of CatFormer to instance-level object pose estimation on the LINEMOD dataset, as well as object pose estimation in real-world scenarios. The experimental results validate the effectiveness and generalization capabilities of CatFormer. Our code and the supplemental materials are avaliable at https://github.com/BIT-robot-group/CatFormer.",
    "full_text": "CatFormer: Category-Level 6D Object Pose Estimation with Transformer\nSheng Yu1, Di-Hua Zhai1,2\u0003, Yuanqing Xia1\n1School of Automation, Beijing Institute of Technology, Beijing, China\n2Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing, China\nyusheng@bit.edu.cn, zhaidih@bit.edu.cn, xia yuanqing@bit.edu.cn.\nAbstract\nAlthough there has been signiÔ¨Åcant progress in category-level\nobject pose estimation in recent years, there is still consid-\nerable room for improvement. In this paper, we propose a\nnovel transformer-based category-level 6D pose estimation\nmethod called CatFormer to enhance the accuracy pose es-\ntimation. CatFormer comprises three main parts: a coarse de-\nformation part, a Ô¨Åne deformation part, and a recurrent re-\nÔ¨Ånement part. In the coarse and Ô¨Åne deformation sections,\nwe introduce a transformer-based deformation module that\nperforms point cloud deformation and completion in the fea-\nture space. Additionally, after each deformation, we incorpo-\nrate a transformer-based graph module to adjust fused fea-\ntures and establish geometric and topological relationships\nbetween points based on these features. Furthermore, we\npresent an end-to-end recurrent reÔ¨Ånement module that en-\nables the prior point cloud to deform multiple times according\nto real scene features. We evaluate CatFormer‚Äôs performance\nby training and testing it on CAMERA25 and REAL275\ndatasets. Experimental results demonstrate that CatFormer\nsurpasses state-of-the-art methods. Moreover, we extend the\nusage of CatFormer to instance-level object pose estimation\non the LINEMOD dataset, as well as object pose estima-\ntion in real-world scenarios. The experimental results vali-\ndate the effectiveness and generalization capabilities of Cat-\nFormer. Our code and the supplemental materials are avali-\nable at https://github.com/BIT-robot-group/CatFormer.\n1 Introduction\n6D object pose estimation is a crucial task in computer vi-\nsion, with applications ranging from robotic grasping (Trem-\nblay et al. 2018; Wang et al. 2019a) to 3D scene understand-\ning (Chen et al. 2019) and augmented reality (Su et al. 2019).\nWhile previous methods have primarily focused on instance-\nlevel pose estimation, such as (Xiang et al. 2018; Kehl et al.\n2017; Peng et al. 2019; He et al. 2020; Lin et al. 2022b;\nRad and Lepetit 2017), these approaches heavily rely on the\navailability of a 3D model for accurate estimation. Conse-\nquently, when faced with an unknown object, it becomes\nchallenging to accurately estimate its 6D pose, which sig-\nniÔ¨Åcantly affects pose estimation in real-world scenes.\n\u0003Corresponding author.\nCopyright c\r 2024, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nCoarse \nDeformation\n Graph \nModule\nFine \nDeformation Graph \nModule\nRecurrent \nRefinement\nRepeat\nPoint Cloud\nPrior Point CloudCoarse \nPoint \nCloud\nFine Point \nCloud\nFinal Point \nCloud\n6D Pose\nÔÅÆ Coarse Deformation Part ÔÅÆ Fine Deformation Part\nÔÅÆ Recurrent Refinement Part\nRGB \nPatch\nNOCS\nFigure 1: CateFormer mainly consists of three parts: coarse\ndeformation, Ô¨Åne deformation, and recurrent reÔ¨Ånement.\nThe coarse deformation part is used to coarse deform and\ncomplement the point cloud. The Ô¨Åne deformation part is\nused to Ô¨Åne deform the prior point cloud. The recurrent re-\nÔ¨Ånement is used to recurrent reÔ¨Åne the point cloud from the\nÔ¨Åne deformation part.\nTo solve this problem, researchers propose several model-\nindependent methods for category-level object 6D pose esti-\nmation (Wang et al. 2019b; Tian, Ang, and Lee 2020; Chen\net al. 2021, 2020a; Di et al. 2022; Lin et al. 2022a). Esti-\nmating the pose at the category level is more challenging\nthan instance-level methods due to the lack of 3D models\nfor objects. Some approaches, like (Wang et al. 2019b; Chen\nand Dou 2021), address this issue by introducing a ‚ÄúNormal-\nized Object Coordinate Space‚Äù (NOCS) where they predict\nthe 3D model of the object. Additionally, most methods rely\non point clouds to capture the object‚Äôs geometric structure.\nRecognizing the similarity in geometry among objects in the\nsame category, certain methods, such as (Tian, Ang, and Lee\n2020; Chen and Dou 2021; Lin et al. 2022a), utilize an aver-\nage point cloud as prior knowledge, enabling rough estima-\ntion of the geometric information in the scene.\nHowever, investigating how to handle intra-class object\nvariety and accurately model objects based on prior point\nclouds is an important problem. Firstly, in real scenes, the\ncamera‚Äôs view can be disrupted, resulting in fragmented\npoint cloud information. The key challenge is how to com-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6808\nplete the point cloud with limited data. Additionally, de-\nforming the prior point cloud appropriately to accurately Ô¨Åt\nthe object in the scene is another crucial consideration. Fur-\nthermore, a single deformation may not sufÔ¨Åce to accurately\nrepresent the object structure, necessitating multiple defor-\nmations. However, certain methods, like (Tian, Ang, and Lee\n2020; Chen and Dou 2021; Chen et al. 2020a), have over-\nlooked these issues. They directly input RGB images with\npoint cloud information and combine features for pose es-\ntimation, leading to inaccurate predictions. Although some\nmethods try to use feature fusion for improved pose estima-\ntion, a gap remains compared to the actual poses.\nWe introduce CatFormer, a transformer-based method\nfor category-level 6D object pose estimation, aiming to\naddress these challenges. Currently, there is a scarcity of\ntransformer-based methods for category-level pose estima-\ntion, such as (Zou et al. 2022; Liu et al. 2023). Our proposed\nmethod leverages transformers and achieves SOTA perfor-\nmance on benchmark datasets.\nAs depicted in Figure 1, CatFormer comprises three main\ncomponents: the coarse deformation part, the Ô¨Åne deforma-\ntion part, and the recurrent reÔ¨Ånement part. In the coarse and\nÔ¨Åne deformation parts, we introduce a transformer-based\ndeformation module to deform and complement the point\ncloud, enabling a better Ô¨Åt with the target object in the scene.\nAdditionally, we propose a transformer-based graph module\nto reÔ¨Åne and adjust fused features, learning geometric rela-\ntionships and topological information within the point cloud\nfor improved understanding of the object‚Äôs 3D structure.\nFurthermore, we propose an end-to-end multi-stage reÔ¨Åne-\nment method that utilizes RGB image and scene point cloud\nfusion features to guide multiple deformations of the Ô¨Åne de-\nformed prior point cloud, resulting in a signiÔ¨Åcantly better\nÔ¨Åt with the target object in the scene. Our proposed method\ndemonstrates notable improvements over SOTA methods on\nthe dataset, surpassing some existing SOTA methods by\nmore than 10% in certain evaluation metrics. We have also\nsuccessfully applied CatFormer to instance-level pose esti-\nmation and real object pose estimation.\nIn summary, the main contributions of this paper are sum-\nmarized as follows:\n\u000fWe propose a novel transformer-based deformation mod-\nule to perform coarse deformation on the scene point\ncloud and Ô¨Åne deformation on the prior point cloud.\n\u000fA transformer-based graph module is proposed to help\nnetworks adjust fused features and construct geomet-\nric and topological relationships between points in point\ncloud features.\n\u000fWe propose an end-to-end recurrent reÔ¨Ånement module\nthat guides the prior point cloud to perform multiple it-\nerations of reÔ¨Ånement based on the guide of the fusion\nfeatures of RGB images and point cloud, so that the prior\npoint cloud can largely Ô¨Åt the target object.\n2 Related Works\n2.1 Instance-Level 6D Object Pose Estimation\nRecently, there has been extensive research on deep\nlearning-based methods for instance-level 6D object pose\nestimation. These methods can be categorized into two\ngroups: direct regression and keypoints correspondence. Di-\nrect regression methods, such as (Xiang et al. 2018; Kehl\net al. 2017; Labb ¬¥e et al. 2020; Li et al. 2018; Manhardt\net al. 2019, 2018; Wang et al. 2019a), take RGB or RGB-\nD images as input and directly predict the 6D pose based\non extracted features. While these methods are generally\ntime-efÔ¨Åcient, they may lack accuracy in certain cases.\nTo address this, researchers propose keypoints correspon-\ndence methods, including (Li, Wang, and Ji 2019; Zakharov,\nShugurov, and Ilic 2019; Park, Patten, and Vincze 2019;\nPeng et al. 2019). These methods predict predeÔ¨Åned object\ncoordinates or 2D keypoints and calculate the 6D pose using\nthe Perspective-n-Point (PnP) algorithm (Lepetit, Moreno-\nNoguer, and Fua 2009) based on the correspondence be-\ntween 2D and 3D points. Some approaches also utilize key-\npoint voting for 6D pose prediction, such as (He et al. 2020,\n2021). However, the non-differentiable nature of PnP makes\nit challenging to apply this two-stage pipeline in tasks that\nrequire differentiable poses. Consequently, alternative tech-\nniques have been explored to learn the PnP step, such as (Di\net al. 2021; Hu et al. 2020; Wang et al. 2021).\n2.2 Category-Level 6D Object Pose Estimation\nTo address the limitations of instance-level methods, re-\nsearchers have explored novel approaches that reduce re-\nliance on 3D models. Category-level 6D object pose estima-\ntion methods have emerged in recent years, including (Wang\net al. 2019b; Tian, Ang, and Lee 2020; Chen et al. 2021;\nChen and Dou 2021; Lin et al. 2021; Chen et al. 2020a; Di\net al. 2022; Lin et al. 2022a; You et al. 2022). In (Wang\net al. 2019b), Wang et al. propose NOCS, a method that\nprojects all objects into the same coordinate space and em-\nploys Umeyama‚Äôs algorithm (Umeyama 1991) to calculate\nthe 6D object pose. However, since objects within the same\ncategory can have distinct shapes, it is challenging to capture\nthe shape variations between each object. To overcome this\nchallenge, some methods introduce shape priors to mitigate\nthe inÔ¨Çuence of shape variations (Tian, Ang, and Lee 2020;\nChen and Dou 2021; Wang, Chen, and Dou 2021; Zou et al.\n2022). For example, in (Tian, Ang, and Lee 2020), Tian et al.\nincorporate a prior point cloud during training and connect\nthe features from the scene point cloud and RGB images.\nThey then predict the NOCS coordinates of the target ob-\nject and calculate its pose. In (Chen and Dou 2021), Chen et\nal. propose SGPA, which introduces a feature fusion module\nbased on vision transformers (Vaswani et al. 2017) to merge\nthe features from RGB images and point clouds. Some meth-\nods aim to directly predict the rotation, translation, and size\nof the object based on extracted features, as demonstrated in\n(Lin et al. 2021; Chen et al. 2021; Di et al. 2022).\nMany methods try to deform the shape prior to Ô¨Åt the\ntarget object, such as (Tian, Ang, and Lee 2020; Lin et al.\n2022a; Chen and Dou 2021; Wang, Chen, and Dou 2021;\nZhang et al. 2022). Some of these methods leverage fused\nfeatures to deform the shape prior, such as (Tian, Ang, and\nLee 2020; Chen and Dou 2021; Wang, Chen, and Dou 2021).\nOthers attempt to share the weights of the network during\nthe feature extraction process for both the point cloud and\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6809\nC\nMask-RCNN\nPointNet++\nCNN\nSA\nSA\nSA\nSA\nCA\nSA\nSA\nSA\nSA\nCA\noffset\nC\nMLP\nSA\nCA\nC\nSelf-Attention\nCross-Attention\nAdd\nConcatenation\nMultiply\nRGB image\nDepth image\nùëÉùëü\nùêº\nùëÉùëú\nùêπùëôùëúùëêùëéùëô\nùëñùëõùë† ùêπùëîùëôùëúùëèùëéùëô\nùëñùëõùë†\nùêπùëîùëôùëúùëèùëéùëô\nùëñùëõùë†\nùëÉùëü\n(ùëñ+1)\nRepeat\nGM\nGM\nùêπùëüùëîùëè\nùêπùëú\nCoarse Deform AVE\nGM\nGM\nùêπùëôùëúùëêùëéùëô\nùëêùëéùë°\nAVE\nùêπùëîùëôùëúùëèùëéùëô\nùëêùëéùë° ùëÉùëü\nùêπùëîùëôùëúùëèùëéùëô\nùëñùëõùë†\nùêπùëôùëúùëêùëéùëô\nùëêùëéùë°\nPointNet++\nC C\nTransform \nMatrix\nùêπùëá\n(ùëñ)\nFine Deform\nFinal Model\n6D Pose\nNOCS\nGM Graph Module\nÔÅÆ Coarse Deformation\nÔÅÆ Fine Deformation ÔÅÆ Recurrent Refinement\nAVE Average Pooling\nFigure 2: An overview of CatFormer for category-level 6D object pose estimation. We initially employ Mask-RCNN to predict\nthe mask and category of the target object. CatFormer takes the object point cloudPo, RGB image I, and prior point cloudPr as\ninputs. Firstly, we utilize the coarse deformation module with the graph module to deform and complement Po. Subsequently,\nemploying the Ô¨Åne deformation module with the graph module and Pr generates relatively accurate point cloud features for\nthe object. Ultimately, a recurrent reÔ¨Ånement module is used to enhance the point cloud features, resulting in the Ô¨Ånal NOCS\nmodel of the object. Based on the predicted NOCS model, we generate the 6D pose of the object.\nthe shape prior, such as (Lin et al. 2022a; Zhang et al. 2022).\nHowever, in this paper, we propose a novel approach that\ndiffers from these methods. Instead of adopting the afore-\nmentioned ideas, our method utilizes a constructed feature\ngraph and repetitive reÔ¨Ånement to deform the shape prior.\n3 Method\n3.1 Pipeline Overview\nThe pipeline of CatFormer is provided in Figure 1. Initially,\nthe input consists of the RGB image Io 2RH\u0002W\u00023 and\nthe point cloud Po 2RNo\u00023 of the target object. PSPNet\n(Zhao et al. 2017) is employed to extract features from the\nRGB image, resulting in Frgb 2 RNo\u0002d. Simultaneously,\nPointNet++ (Qi et al. 2017) is used to extract features from\nthe point cloud, yielding Fo 2RNo\u0002d. Next, the deforma-\ntion module is utilized to complement and deform the point\ncloud. Additionally, PointNet++ is applied to extract fea-\ntures from the prior point cloud Pr 2RNr\u00023, generating\nFr 2RNr\u0002d. The fused feature from Frgb and Fo is then\nemployed to deform Pr within the Ô¨Åne deformation module.\nTo capture geometrical information about the object and ad-\njust the fused feature, a graph module establishes the graph\nfeature after each point cloud deformation process. Finally,\nthe recurrent reÔ¨Ånement module is leveraged to further re-\nÔ¨Åne the prior point cloud.\n3.2 Deformation Module\nWe propose a transformer-based deformation module con-\nsisting of self-attention (SA) and cross-attention (CA) mod-\nules to deform and complete the shape of the point cloud.\nThe coarse deformation process applies coarse deformation\nto the scene point cloud, while the Ô¨Åne deformation process\ndeforms the prior point cloud using the fused feature. This\ndeformation process completes and deforms the point cloud\nshape in the feature space, resulting in feature maps of the\ndeformed point cloud.\nWe begin by applying three MLP layers to Frgb and Fo,\ngenerating the query, key,and value inputs for the SA mod-\nule. which can be calculated by\nF\u0003= SA(F\u0003) (1)\nwhere \u0003 2 frgb;ogindicates the parameters of RGB and\npoint cloud, respectively.\nNext, we utilize another SA module based on Frgb and\nFo to perform further feature extraction using a similar op-\neration. The resulting feature maps for the RGB images and\npoint cloud are denoted asFrgb 2RNo\u0002C and Fo 2RNo\u0002C,\nrespectively.\nThen, we compute the query, key,and value inputs for the\ncross-attention (CA) module based on the extracted features\nFrgb and Fo. The objective of the CA model is to combine\nFo and Frgb, resulting in the deformation offsets featuresOo\nand Orgb. The fusion is computed as\nOi = softmax\n \nqc\ni \u0002(kc\nj)T\npdk\n!\nvc\nj (2)\nwhere i;j 2frgb;og and i6= j.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6810\nFinally, we adjust the feature maps by adding the defor-\nmation offsets, which can be calculated as F\u0003 = F\u0003+ O\u0003,\nwhere \u00032frgb;og.\nOnce we have adjustedFrgb and Fo, we concatenate them\nalong the channel dimension, resulting in instance local-\nwise features denoted as Fins\nlocal 2RNo\u0002C. Subsequently, we\nemploy an MLP layer to generate the instance global-wise\nfeatures represented by Fins\nglobal 2RNo\u0002C.\nBoth the coarse deformation and Ô¨Åne deformation mod-\nules function in a similar manner, as they deform objects by\npredicting the deformation offset in the feature space. For\nmore detailed information about the Ô¨Åne deformation mod-\nule, please refer to the supplementary material.\n3.3 Graph Module\nIn order to establish a graph relationship between the fused\nfeatures of RGB images and point clouds, we introduce\na graph module inspired by ideas from graph convolution\n(Kipf and Welling 2017). This graph module incorporates a\ntransformer structure, which is illustrated in Figure 3.\nq\nk\nv\nInput\nSoftmax\nq\nOutput\nA\nAttention map\nAdd\nMultiply\nFigure 3: The structure of the graph module.\nUsing the fused feature F 2 RB\u0002N\u0002D as input, we\ngenerate the query, key,and value, which indicate by q 2\nRB\u0002N\u0002Dq , k 2 RB\u0002N\u0002Dk , and v 2 RB\u0002N\u0002Dv respec-\ntively, where B represents the batch size, and D\u0003;\u0003 2\nfq;k;v gindicates the feature dimension.\nNext, we normalize the q and v tensors, and apply the\nsoftmax function to the ktensor.\nTo generate multiple graph features, we Ô¨Årst divide qinto\nseveral heads, resulting in q 2RB\u0002N\u0002H\u0002(Dq=H), where H\ndenotes the number of heads. We set Dq=H = Dk = Dv\nand utilize the Einstein summation convention (einsum) for\nperforming matrix multiplication in the transformer.\nThen, we follow the calculation process of the transformer\nand calculate the attention map, which can be got byattn=\nk\nv, where \nindicates the einsum, attn 2RB\u0002Dk\u0002Dv\nindicates the attention map.\nNext, we generate the adjusted features with the attention\nmap:\nFattn = q\nattn (3)\nwhere Fattn 2RB\u0002N\u0002H\u0002Dv indicates the adjusted feature\nmap.\nThen, we generate multi-graph features on the value\nbranch. First, we create a random graph adjacency matrix\nAto establish a graph G, which can be computed by\nG= v\nA (4)\nwhere A2RDv\u0002Dv\u0002Dk , G2RB\u0002N\u0002Dk\u0002Dv .\nTo establish a graph on itself, we add a self-loop to the v\ntensor by v= v+ I, where I represents the identity matrix.\nWe reshape vinto RB\u0002N\u0002Dv\u0002Dv and add it to the graph\nG. Using G, we establish the relationship between v and q\nby\nFG = q\nG (5)\nwhere FG 2RB\u0002N\u0002H\u0002Dv indicates the graph features.\nFinally, we add the FG and Fattn tensors together to ob-\ntain Ff . Then, we can get the Ô¨Ånal graph feature Ffin by\nFfin = MLP(Ff ) +F, where MLP indicates MLP lay-\ners.\nDue to space limits, we have added more details and mo-\ntivation of graph module into the supplementary material.\n3.4 Recurrent ReÔ¨Ånement Module\nIn this part, we deform the prior point cloud Pr by Fins\nglobal\nand Fcat\nglobal in the feature space. We predict the deformation\noffset O and the point cloud transformation matrix T. We\nuse O to adjust and deform the shape of Pr, while T gener-\nates the NOCS model from the deformed point cloud.\nThe initial transform matrix and deformation offset for\nobjects are T0 2RNo\u0002(Nr\u0002c) and O0 2RNr\u0002(c\u00023). Here,\ncrepresents the number of object categories.\nFor a speciÔ¨Åc object k 2c, the corresponding deformed\npoint cloud is Pr. The initial point cloud transformation\nmatrix and deformation offset are T(k)\n0 2 RNo\u0002Nr and\nO(k)\n0 2RNr\u00023.\nWe perform the Ô¨Årst deformation on Pr with the initial\ndeformation offset,P(1)\nr = Pr+O(k)\n0 , where P(1)\nr represents\nthe deformed prior point cloud.\nThen, we update the category local-wise and global-wise\nfeatures based on the P(1)\nr that is\nFcat\nlocal(1) = MLP(P(1)\nr );Fcat\nglobal(1) = AVE(Fcat\nlocal) (6)\nwhere AVE indicates average global pooling layer.\nAfter that, we use Fcat\nglobal(1) with previous features to fur-\nther update the offset and transformation matrix:\nO1 = MLP( c\r(Fcat\nglobal(1);Fcat\nglobal(0))) (7)\nT1 = MLP( c\r(Fcat\nglobal(1);Fins\nglobal(0))) (8)\nwhere c\r(\u0003;\u0003) indicates concatenation, Fcat\nglobal(0) = Fcat\nglobal\nand Fins\nglobal(0) = Fins\nglobal.\nThen we update the deformation offset and transformation\nmatrix: O1  O1 + O0, T1  T1 \u0002T0. By repeatedly\nperforming such a process, we can deform the prior point\ncloud several times, which can be expressed as\nOi+1 = MLP( c\r(Fcat\nglobal(i+1);Fcat\nglobal(0))) (9)\nOi+1  Oi+1 + Oi (10)\nTi+1 = MLP( c\r(Fcat\nglobal(i+1);Fins\nglobal(i))) (11)\nTi+1  Ti+1 \u0002Ti (12)\nDue to space limits, we also provide the pseudocode of\nthis algorithm in the supplementary material.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6811\n3.5 Loss Function\nWe can transform the instance object into the NOCS space\nusing the Ô¨Ånal prediction of deformation offset O and point\ncloud transformation matrix T. The resulting equation is ex-\npressed as PNOCS = softmax(T) \u0002(Pr + O).\nCorrespondence Loss Based on PNOCS , we can trans-\nform the object into NOCS space, we use the smoothL1 loss\nto calculate the correspondence loss\nLcorr = S(PNOCS ;Pgt)\nwhere S indicates smooth L1 loss, Pgt is the ground truth.\nReconstruction Loss To evaluate the performance of de-\nformation of the network, following the ideas in (Tian, Ang,\nand Lee 2020), we use the Chamfer distance to penalize the\ndeformation\nLr(M;Mgt) =\nX\ni2Mi\nmin\nj2Mj\ngt\njji\u0000jjj2\n2 +\nX\nj2Mj\ngt\nmin\ni2Mi\njji\u0000jjj2\n2\nwhere M = Pr +O is the prediction of the object 3D model,\nMgt is the ground truth 3D model.\nDistribution Loss Following (Tian, Ang, and Lee 2020),\nWe also try to encourage T to be a peaked distribution by\nminimizing the average cross-entropy loss, which can be\ncalculated by\nLdis = 1\nNo\nX\ni\nX\nj\n(\u0000softmax(Ti;j) log(softmax(Ti;j)))\nDeformation Loss To avoid overÔ¨Åtting and large defor-\nmations, we also use the O with L2 regularization to realize\nit, Ldef = 1\nNo\nP\ni2O jjijj2\nTotal Loss The total loss of the CatFormer is the sum of\nthe four losses\nL=\nnX\nk=0\n\u0015corrLcorr + \u0015rLr + \u0015disLdis + \u0015def Ldef\nwhere \u0015\u0003is the weight of the loss function, nis the number\nof the object in the scene.\n4 Experiments\n4.1 Datasets\nThe benchmark datasets for category-level object pose esti-\nmation are the REAL275 dataset and CAMERA25 dataset,\nproposed in (Wang et al. 2019b). The CAMERA25 dataset\nconsists of 300K images, with 25K images used for evalua-\ntion. It is generated by rendering synthetic objects into real\nscenes. On the other hand, the REAL275 dataset contains\n4300 real-world training images from 7 scenes, and 2750\nreal-world evaluation images from 6 scenes. Both datasets\ninclude 6 different categories of objects: bottle, bowl, cam-\nera, can, laptop, and mug. For instance-level 6D pose esti-\nmation, the LINEMOD dataset (Hinterstoisser et al. 2011)\nserves as the benchmark. It comprises 13 different objects\nrandomly placed in real scenes.\n4.2 Training Details\nAll experiments are conducted on a single NVIDIA GeForce\nRTX 3090 GPU with 24 GB memory, running Ubuntu 18.04\nas the operating system. PyTorch 1.8.1 is utilized as the\ndeep learning framework, and CUDA 11.1 is employed for\naccelerated training. The network is trained with a batch\nsize of 16 for 60 epochs. The initial learning rate is set to\n1 \u000210\u00004, gradually decreasing to 1 \u000210\u00006. We also set\n\u0015corr = \u0015r = \u0015def = 1in this paper.\n4.3 Preprocessing\nTo process the dataset, we follow the procedures outlined in\n(Tian, Ang, and Lee 2020). We employ an instance segmen-\ntation network, such as Mask-RCNN (He et al. 2017), for\nobject detection and segmentation. For each segmented in-\nstance, we crop the object and resize the image to192 \u0002192\npixels. Using the RGB-D images and the camera‚Äôs intrinsic\nmatrix, we generate a point cloud of the scene. From this\npoint cloud, we randomly select 1024 points for each ob-\nject. The cropped images and selected points are then used\nas inputs for CatFormer.\n4.4 Evaluation Metrics\nWe evaluate the performance of CatFormer using widely\nused evaluation metrics (Wang et al. 2019b; Tian, Ang,\nand Lee 2020; Lin et al. 2021; Di et al. 2022; Lin et al.\n2022a). For rotation and translation evaluation, we utilize\n3D Intersection-Over-Union (IoU) with thresholds of 0.25,\n0.5, and 0.75. Additionally, we employ 5\u000e2cm, 5\u000e5cm,\n10\u000e2cm, and 10\u000e5cmto directly assess rotation and trans-\nlation accuracy. If the errors fall within the thresholds, the\npredictions are deemed correct. Based on these evaluation\nmetrics, we will use overall mAP to assess the performance\nof CatFormer compared to other SOTA methods.\n4.5 Comparison with State-of-the-Art Methods\nWe present the quantitative results of CatFormer compared\nto recent state-of-the-art methods on the CAMERA25 and\nREAL275 datasets in Table 1.\nCatFormer exhibits slightly superior performance to\nSOTA methods on the CAMERA25 dataset. This can be at-\ntributed to the accurate depth images generated in a simu-\nlated real scene, unaffected by environmental factors. Con-\nsequently, point cloud completion has not provided signif-\nicant enhancements in pose estimation. However, on the\nREAL275 dataset, where all images are captured in the real\nworld, the depth images can be inÔ¨Çuenced by object reÔ¨Çec-\ntions or lighting conditions, resulting in incomplete informa-\ntion and imperfect point clouds. In such cases, transformer-\nbased point cloud completion and deformation have demon-\nstrated their effectiveness. Notably, for the IoU50 metric,\nCatFormer outperforms SOTA HS-Pose (Zheng et al. 2023)\nwith a score of 83.1 compared to 82.1. SpeciÔ¨Åcally, in the\nmost challenging 5\u000e2cmmetric, CatFormer achieves a score\nof 47.7, surpassing the previous SOTA method HS-Pose\nat 46.5, indicating higher accuracy. Additionally, for the\n10\u000e2cmmetric, CatFormer also demonstrates improved per-\nformance with a score of 69.0 compared to HS-Pose‚Äôs 68.6.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6812\nMethod CAMERA25 REAL275\nIoU50 IoU75 5\u000e2cm5\u000e5cm10\u000e2cm10\u000e5cmIoU25 IoU50 IoU75 5\u000e2cm5\u000e5cm10\u000e2cm10\u000e5cm\nNOCS(Wang et al. 2019b)(R) 83.9 69.5 32.3 40.9 48.2 64.6 84.9 78.0 30.1 7.2 9.5 13.8 25.2\nSPD(Tian, Ang, and Lee 2020)(R) 93.2 83.1 54.3 59.0 73.3 81.5 83.0 77.3 53.2 19.2 21.4 43.2 54.1\nSGPA(Chen and Dou 2021)(R) 93.2 88.1 70.7 74.5 82.7 88.4 - 80.1 61.9 35.9 39.6 61.3 70.7\nCR-Net(Wang, Chen, and Dou 2021)(R) 93.8 88.0 72.0 76.4 81.0 87.7 79.3 55.9 27.8 34.3 47.2 60.8\nFS-Net\u0003(Chen et al. 2021)(D) - - - - - - 84.0 81.1 63.5 19.9 33.9 - 69.1\nDualPoseNet(Lin et al. 2021)(R) 92.4 86.4 64.7 70.7 77.2 84.7 - 79.8 62.2 29.3 35.9 50.0 66.8\nSAR-Net(Lin et al. 2022a)(D) 86.8 79.0 66.7 70.9 75.3 80.3 - 79.3 62.4 31.6 42.3 50.3 68.3\nGPV-Pose(Di et al. 2022)(D) 93.4 88.3 72.1 79.1 - 89.0 84.2 83.0 64.4 32.0 42.9 - 73.3\nHS-Pose(Zheng et al. 2023)(D) 93.3 89.4 73.3 80.5 80.4 89.4 84.2 82.1 74.7 46.5 55.2 68.6 82.7\nSPD+GM 93.6 87.9 61.4 66.3 78.0 85.4 83.6 80.5 59.5 20.4 24.4 47.7 58.7\nCR-Net+GM 93.9 88.8 72.8 76.0 82.4 88.3 83.2 79.8 64.5 32.9 37.5 53.1 63.4\nSGPA+GM 93.5 86.1 73.8 77.7 83.9 89.0 84.0 81.3 65.3 36.5 40.6 62.9 71.1\nOurs(R) 93.5 89.9 74.9 79.8 85.3 90.2 84.3 83.1 73.8 47.7 53.7 69.0 79.5\nTable 1: Comparison with state-of-the-art methods on CAMERA25 dataset and REAL275 dataset. GM indicates the graph\nmodule. R and D indicates RGB-D-based and depth-based methods respectively.\u0003We use the results provided by the GPV-Pose\n(Di et al. 2022) and HS-Pose (Zheng et al. 2023).\nSince CatFormer is a RGB-D-based method, we comapre\nit to the other RGB-D-based methods. Figure 4 shows the\nqualitative analysis of CatFormer and the state-of-the-art\nRGB-D based method SGPA on the REAL275 dataset. In\ncomparison to SGPA, CatFormer exhibits higher accuracy\nin pose estimation. We provide more comprehensive details\nof the comparison experiments and object pose estimation in\nthe the supplementary material.\nSGPACatFormer\nFigure 4: Qualitative results of SGPA and CatFormer. The\nbounding box in green line is the ground truth, and the red\nline is the prediction.\nIn this paper, we propose an innovative transformer-based\ngraph module. By incorporating the graph module into Cat-\nFormer, we observe a signiÔ¨Åcant improvement in perfor-\nmance, highlighting its effectiveness. To further validate the\nefÔ¨Åcacy of the graph module, we also apply it to other RGB-\nD-based methods such as SPD, CR-Net, SGPA. The experi-\nmental results are presented in Table 1. The experimental re-\nsults demonstrate that incorporating the graph module into\nthe network leads to improved performance. For instance,\nwhen we apply the graph module after RGB-D future fusion\nin SPD, the 5\u000e2cmmetric shows a signiÔ¨Åcant enhancement\nfrom 54.3 to 61.4, indicating a substantial improvement.\n4.6 Ablation Studies\nThe REAL275 dataset, being more challenging than the\nCAMERA25 dataset, provides a better evaluation of the\nnetwork‚Äôs performance. Therefore, we primarily utilize the\nREAL275 dataset for conducting ablation studies. These\nstudies focus on evaluating the proposed module, the num-\nber of reÔ¨Ånement iterations, and the loss terms.\nModule: We begin by conducting ablation studies on the\nproposed module, with the corresponding experimental re-\nsults presented in Table 2(A). Firstly, when the graph mod-\nule is removed, CatFormer‚Äôs performance decreases, indi-\ncating the effectiveness of the graph module in establishing\nconnections between different features. However, removing\neither the coarse deformation or Ô¨Åne deformation module\ncauses a more signiÔ¨Åcant drop in CatFormer‚Äôs performance,\nhighlighting the effectiveness of point cloud deformation\nand completion.\nRepeat Times: In addition, we assess the efÔ¨Åcacy of the\nrecurrent reÔ¨Ånement module by reÔ¨Åning the initial point\ncloud multiple times. SpeciÔ¨Åcally, we conduct reÔ¨Ånements\n1, 3, 4, 5, 6, and 7 times in this study, with the corresponding\nexperimental results presented in Table 2(B). As the num-\nber of reÔ¨Ånement iterations increases, there is a gradual im-\nprovement in the network‚Äôs performance, reaching its peak\nat Ô¨Åve iterations. However, further increasing the number of\niterations leads to a decline in the network‚Äôs performance.\nLoss Terms: Given the necessity of predicting the NOCS\nmodel of the object, the Lcorr term plays a crucial role. The\nexperimental results are summarized in Table 2(C). It is evi-\ndent from the results that relying solely onLcorr yields poor\nnetwork performance. However, when Lcorr is combined\nwith Ldis, CatFormer demonstrates relatively improved per-\nformance.\nAdditionally, removing Ldis results in a signiÔ¨Åcant de-\ncrease in network performance. Each row in matrix T can\nbe considered as a relaxed one-hot vector, allowing a point\nin the NOCS space to be transformed by up to three points in\nthe point cloud. We aim for T to have a peaked distribution,\nfocusing on high-conÔ¨Ådence transformations. This concen-\ntration of conÔ¨Ådence enhances the accuracy of the predicted\nNOCS model.\nLastly, we utilize Ldef to mitigate excessive deformation\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6813\nGroup Module Repeat Times Loss Terms REAL275\nCD FD GM Lcorr Lr Ldis Ldef IoU50 IoU75 5\u000e2cm 5\u000e5cm 10\u000e2cm 10\u000e5cm\n(A)\nX 3 X X X X 79.0 54.1 32.5 35.9 60.1 69.8\nX 3 X X X X 81.2 65.9 38.9 42.7 62.9 72.9\nX 3 X X X X 80.0 51.7 29.8 33.4 59.6 69.7\nX X 3 X X X X 79.8 67.7 37.7 42.5 63.9 74.0\nX X 3 X X X X 81.7 66.2 40.3 44.3 64.5 74.6\nX X 3 X X X X 80.9 56.0 34.9 38.5 61.1 71.9\n(B)\nX X X 1 X X X X 82.5 68.0 43.0 47.6 65.7 76.0\nX X X 3 X X X X 82.6 69.5 44.5 47.6 65.3 76.7\nX X X 4 X X X X 82.8 71.6 46.8 50.1 67.2 78.4\nX X X 5 X X X X 83.1 73.8 47.7 53.7 69.0 79.5\nX X X 6 X X X X 82.4 70.6 46.2 49.5 68.4 78.3\nX X X 7 X X X X 82.1 68.1 43.3 48.0 66.8 77.0\n(C)\nX X X 3 X X 74.9 34.9 41.6 46.0 66.0 75.4\nX X X 3 X X X 78.9 44.9 41.9 46.8 64.8 74.8\nX X X 3 X X X 82.2 68.0 42.0 47.1 64.3 75.4\nX X X 3 X 1.5 1.0 30.5 34.7 50.8 56.3\nX X X 3 X X 0.0 0.0 3.3 3.3 14.2 14.6\nX X X 3 X X 81.9 67.3 38.5 43.3 61.5 71.6\nX X X 3 X X X 81.8 68.1 42.0 47.1 63.8 75.4\nTable 2: Ablation studies on different conÔ¨Ågurations of network on REAL275. CD, FD, and GM refer to coarse deformation\nmodule, Ô¨Åne deformation module, and graph module respectively.\nin CatFormer‚Äôs predictions. Applying Ldef leads to an im-\nprovement in CatFormer‚Äôs performance.\nDue to space limits, we add more analysis of ablation\nstudies in supplementary materials.\n4.7 Category-level Object Pose Estimation in The\nReal World\nTo assess CatFormer‚Äôs effectiveness and generalization, we\napply it to real-world object pose estimation, speciÔ¨Åcally on\nobjects that were not used during training. RGB-D images\nare obtained using an Intel RealSense D435i camera, while\nsegmentation is performed using a pretrained Mask-RCNN\nmodel. The pose estimation results, shown in Figure 5, in-\ndicate that CatFormer exhibits good generalization and per-\nforms well in real-world object pose estimation tasks.\nFigure 5: Object pose estimation results of CatFormer in the\nreal world.\n4.8 Instance-Level Object Pose Estimation\nCatFormer is also applied to instance-level object pose\nestimation in this study. The LINEMOD dataset (Hinter-\nstoisser et al. 2011) is used for conducting the instance-\nlevel object pose estimation experiment. By comparing Cat-\nFormer‚Äôs performance with other state-of-the-art methods\non the LINEMOD dataset, the experimental results, dis-\nplayed in Table 3, indicate that CatFormer outperforms re-\nlated category-level methods and achieves competitive per-\nformance compared to some state-of-the-art instance-level\nmethods. For symmetric objects, we utilize ADD-S as the\nmetric (Xiang et al. 2018), while for non-symmetric objects,\nwe employ ADD as the metric (Hinterstoisser et al. 2012).\nWe also provide more details of instance-level object pose\nestimation in the supplementary material.\nMethod C.L. ADD-(S) Speed(FPS)\nPVNet(Peng et al. 2019) 86.3 27\nG2L-Net(Chen et al. 2020b) 98.7 24\nDenseFusion(Wang et al. 2019a) 94.3 15\nPVN3D(He et al. 2020) 99.4 5\nDualPose(Lin et al. 2021) X 98.2 3\nFS-Net (Chen et al. 2021) X 97.6 22\nGPV-Pose(Di et al. 2022) X 98.2 20\nOurs X 99.3 8\nTable 3: Instance-level object pose estimation on LINEMOD\ndataset. C.L. indicates category-level method.\n5 Conclusion\nIn this paper, we introduce CatFormer, a novel category-\nlevel 6D object estimation network. CatFormer leverages\ntransformer-based deformation modules for both coarse and\nÔ¨Åne deformations. Additionally, we propose a graph mod-\nule to establish and extract graph features from fused fea-\ntures. To further reÔ¨Åne the prior point‚Äôs proximity to the\ntarget object, we introduce a recurrent reÔ¨Ånement mod-\nule. Compared to previous state-of-the-art methods, our\napproach demonstrates superior performance on both the\nCAMERA25 dataset and REAL275 dataset. Furthermore,\nCatFormer achieves successful results in instance-level ob-\nject pose estimation and real-world object pose estimation\ntasks.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6814\nAcknowledgments\nThe work was supported by the National Natural Sci-\nence Foundation of China under Grant 62173035, Grant\n61803033 and Grant 61836001, and in part by the Xiaomi\nYoung Scholars from Xiaomi Foundation, and in part by\nthe BIT Research and Innovation Promoting Project under\nGrant 2023YCXY035.\nReferences\nChen, D.; Li, J.; Wang, Z.; and Xu, K. 2020a. Learning\ncanonical shape space for category-level 6D object pose and\nsize estimation. In IEEE Conference on Computer Vision\nand Pattern Recognition, 11973‚Äì11982.\nChen, K.; and Dou, Q. 2021. SGPA: Structure-guided prior\nadaptation for category-level 6D object pose estimation. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2773‚Äì2782.\nChen, W.; Jia, X.; Chang, H. J.; Duan, J.; and Leonardis, A.\n2020b. G2l-net: Global to local network for real-time 6d\npose estimation with embedding vector features. In IEEE\nConference on Computer Vision and Pattern Recognition,\n4233‚Äì4242.\nChen, W.; Jia, X.; Chang, H. J.; Duan, J.; Shen, L.; and\nLeonardis, A. 2021. FS-net: Fast shape-based network for\ncategory-level 6D object pose estimation with decoupled ro-\ntation mechanism. In IEEE Conference on Computer Vision\nand Pattern Recognition, 1581‚Äì1590.\nChen, Y .; Huang, S.; Yuan, T.; Qi, S.; Zhu, Y .; and Zhu,\nS.-C. 2019. Holistic++ scene understanding: Single-view\n3d holistic scene parsing and human pose estimation with\nhuman-object interaction and physical commonsense. In\nIEEE International Conference on Computer Vision, 8648‚Äì\n8657.\nDi, Y .; Manhardt, F.; Wang, G.; Ji, X.; Navab, N.; and\nTombari, F. 2021. So-pose: Exploiting self-occlusion for di-\nrect 6d pose estimation. In IEEE International Conference\non Computer Vision, 12396‚Äì12405.\nDi, Y .; Zhang, R.; Lou, Z.; Manhardt, F.; Ji, X.; Navab, N.;\nand Tombari, F. 2022. GPV-Pose: Category-level Object\nPose Estimation via Geometry-guided Point-wise V oting. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 6781‚Äì6791.\nHe, K.; Gkioxari, G.; Doll¬¥ar, P.; and Girshick, R. 2017. Mask\nR-CNN. In IEEE International Conference on Computer\nVision, 2961‚Äì2969.\nHe, Y .; Huang, H.; Fan, H.; Chen, Q.; and Sun, J. 2021.\nFfb6d: A full Ô¨Çow bidirectional fusion network for 6d pose\nestimation. In IEEE Conference on Computer Vision and\nPattern Recognition, 3003‚Äì3013.\nHe, Y .; Sun, W.; Huang, H.; Liu, J.; Fan, H.; and Sun, J.\n2020. PVN3D: A deep point-wise 3d keypoints voting net-\nwork for 6dof pose estimation. InIEEE Conference on Com-\nputer Vision and Pattern Recognition, 11632‚Äì11641.\nHinterstoisser, S.; Holzer, S.; Cagniart, C.; Ilic, S.; Kono-\nlige, K.; Navab, N.; and Lepetit, V . 2011. Multimodal tem-\nplates for real-time detection of texture-less objects in heav-\nily cluttered scenes. In IEEE International Conference on\nComputer Vision, 858‚Äì865.\nHinterstoisser, S.; Lepetit, V .; Ilic, S.; Holzer, S.; Bradski,\nG.; Konolige, K.; and Navab, N. 2012. Model based training,\ndetection and pose estimation of texture-less 3d objects in\nheavily cluttered scenes. In Asian Conference on Computer\nVision, 548‚Äì562.\nHu, Y .; Fua, P.; Wang, W.; and Salzmann, M. 2020. Single-\nstage 6d object pose estimation. In IEEE Conference on\nComputer Vision and Pattern Recognition, 2930‚Äì2939.\nKehl, W.; Manhardt, F.; Tombari, F.; Ilic, S.; and Navab, N.\n2017. SSD-6D: Making rgb-based 3d detection and 6d pose\nestimation great again. In IEEE International Conference on\nComputer Vision, 1521‚Äì1529.\nKipf, T. N.; and Welling, M. 2017. Semi-Supervised Clas-\nsiÔ¨Åcation with Graph Convolutional Networks. In Interna-\ntional Conference on Learning Representations.\nLabb¬¥e, Y .; Carpentier, J.; Aubry, M.; and Sivic, J. 2020.\nCosypose: Consistent multi-view multi-object 6d pose esti-\nmation. In European Conference on Computer Vision, 574‚Äì\n591.\nLepetit, V .; Moreno-Noguer, F.; and Fua, P. 2009. Epnp: An\naccurate o (n) solution to the pnp problem. International\njournal of computer vision, 81(2): 155‚Äì166.\nLi, Y .; Wang, G.; Ji, X.; Xiang, Y .; and Fox, D. 2018.\nDeepim: Deep iterative matching for 6d pose estimation. In\nEuropean Conference on Computer Vision, 683‚Äì698.\nLi, Z.; Wang, G.; and Ji, X. 2019. Cdpn: Coordinates-based\ndisentangled pose network for real-time rgb-based 6-dof ob-\nject pose estimation. In IEEE International Conference on\nComputer Vision, 7678‚Äì7687.\nLin, H.; Liu, Z.; Cheang, C.; Fu, Y .; Guo, G.; and Xue, X.\n2022a. SAR-Net: Shape Alignment and Recovery Network\nfor Category-Level 6D Object Pose and Size Estimation. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 6707‚Äì6717.\nLin, J.; Wei, Z.; Li, Z.; Xu, S.; Jia, K.; and Li, Y . 2021. Dual-\nposenet: Category-level 6D object pose and size estimation\nusing dual pose network with reÔ¨Åned learning of pose con-\nsistency. In IEEE International Conference on Computer\nVision, 3560‚Äì3569.\nLin, S.; Wang, Z.; Ling, Y .; Tao, Y .; and Yang, C. 2022b.\nE2EK: End-to-End Regression Network Based on Keypoint\nfor 6D Pose Estimation.IEEE Robotics and Automation Let-\nters, 7(3): 6526‚Äì6533.\nLiu, J.; Sun, W.; Liu, C.; Zhang, X.; and Fu, Q.\n2023. Robotic Continuous Grasping System by Shape\nTransformer-Guided Multiobject Category-Level 6-D Pose\nEstimation. IEEE Transactions on Industrial Informatics,\n19(11): 11171‚Äì11181.\nManhardt, F.; Arroyo, D. M.; Rupprecht, C.; Busam, B.;\nBirdal, T.; Navab, N.; and Tombari, F. 2019. Explaining\nthe ambiguity of object detection and 6d pose from visual\ndata. In IEEE International Conference on Computer Vi-\nsion, 6841‚Äì6850.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6815\nManhardt, F.; Kehl, W.; Navab, N.; and Tombari, F. 2018.\nDeep model-based 6d pose reÔ¨Ånement in rgb. In European\nConference on Computer Vision, 800‚Äì815.\nPark, K.; Patten, T.; and Vincze, M. 2019. Pix2pose: Pixel-\nwise coordinate regression of objects for 6d pose estima-\ntion. In IEEE International Conference on Computer Vision,\n7668‚Äì7677.\nPeng, S.; Liu, Y .; Huang, Q.; Zhou, X.; and Bao, H. 2019.\nPVNet: Pixel-wise voting network for 6dof pose estimation.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition, 4561‚Äì4570.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017. Pointnet++:\nDeep hierarchical feature learning on point sets in a metric\nspace. Advances in Neural Information Processing Systems,\n30.\nRad, M.; and Lepetit, V . 2017. BB8: A scalable, accurate, ro-\nbust to partial occlusion method for predicting the 3D poses\nof challenging objects without using depth. In IEEE Inter-\nnational Conference on Computer Vision, 3828‚Äì3836.\nSu, Y .; Rambach, J.; Minaskan, N.; Lesur, P.; Pagani, A.; and\nStricker, D. 2019. Deep multi-state object pose estimation\nfor augmented reality assembly. In IEEE International Sym-\nposium on Mixed and Augmented Reality Adjunct, 222‚Äì227.\nTian, M.; Ang, M. H.; and Lee, G. H. 2020. Shape prior de-\nformation for categorical 6d object pose and size estimation.\nIn European Conference on Computer Vision, 530‚Äì546.\nTremblay, J.; To, T.; Sundaralingam, B.; Xiang, Y .; Fox, D.;\nand BirchÔ¨Åeld, S. 2018. Deep Object Pose Estimation for\nSemantic Robotic Grasping of Household Objects. In Con-\nference on Robot Learning.\nUmeyama, S. 1991. Least-squares estimation of transforma-\ntion parameters between two point patterns. IEEE Transac-\ntions on Pattern Analysis & Machine Intelligence, 13(04):\n376‚Äì380.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, C.; Xu, D.; Zhu, Y .; Mart¬¥ƒ±n-Mart¬¥ƒ±n, R.; Lu, C.; Fei-\nFei, L.; and Savarese, S. 2019a. Densefusion: 6d object pose\nestimation by iterative dense fusion. In IEEE Conference on\nComputer Vision and Pattern Recognition, 3343‚Äì3352.\nWang, G.; Manhardt, F.; Tombari, F.; and Ji, X. 2021. Gdr-\nnet: Geometry-guided direct regression network for monoc-\nular 6d object pose estimation. InIEEE Conference on Com-\nputer Vision and Pattern Recognition, 16611‚Äì16621.\nWang, H.; Sridhar, S.; Huang, J.; Valentin, J.; Song, S.; and\nGuibas, L. J. 2019b. Normalized object coordinate space for\ncategory-level 6d object pose and size estimation. In IEEE\nConference on Computer Vision and Pattern Recognition,\n2642‚Äì2651.\nWang, J.; Chen, K.; and Dou, Q. 2021. Category-level 6d\nobject pose estimation via cascaded relation and recurrent\nreconstruction networks. In IEEE International Conference\non Intelligent Robots and Systems, 4807‚Äì4814.\nXiang, Y .; Schmidt, T.; Narayanan, V .; and Fox, D. 2018.\nPoseCNN: A Convolutional Neural Network for 6D Object\nPose Estimation in Cluttered Scenes. In Robotics: Science\nand Systems.\nYou, Y .; Shi, R.; Wang, W.; and Lu, C. 2022. CPPF: Towards\nRobust Category-Level 9D Pose Estimation in the Wild. In\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 6866‚Äì6875.\nZakharov, S.; Shugurov, I.; and Ilic, S. 2019. Dpod: 6d pose\nobject detector and reÔ¨Åner. In IEEE International Confer-\nence on Computer Vision, 1941‚Äì1950.\nZhang, R.; Di, Y .; Lou, Z.; Manhardt, F.; Tombari, F.; and Ji,\nX. 2022. Rbp-pose: Residual bounding box projection for\ncategory-level pose estimation. In European Conference on\nComputer Vision, 655‚Äì672.\nZhao, H.; Shi, J.; Qi, X.; Wang, X.; and Jia, J. 2017. Pyramid\nscene parsing network. In IEEE Conference on Computer\nVision and Pattern Recognition, 2881‚Äì2890.\nZheng, L.; Wang, C.; Sun, Y .; Dasgupta, E.; Chen, H.;\nLeonardis, A.; Zhang, W.; and Chang, H. J. 2023. HS-Pose:\nHybrid Scope Feature Extraction for Category-level Object\nPose Estimation. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 17163‚Äì17173.\nZou, L.; Huang, Z.; Gu, N.; and Wang, G. 2022. 6d-vit:\nCategory-level 6d object pose estimation via transformer-\nbased instance representation learning. IEEE Transactions\non Image Processing, 31: 6907‚Äì6921.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n6816"
}