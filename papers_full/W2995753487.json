{
  "title": "Meshed-Memory Transformer for Image Captioning",
  "url": "https://openalex.org/W2995753487",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2508732373",
      "name": "Marcella Cornia",
      "affiliations": [
        "University of Modena and Reggio Emilia"
      ]
    },
    {
      "id": "https://openalex.org/A2141204826",
      "name": "Matteo Stefanini",
      "affiliations": [
        "University of Modena and Reggio Emilia"
      ]
    },
    {
      "id": "https://openalex.org/A2135711272",
      "name": "Lorenzo Baraldi",
      "affiliations": [
        "University of Modena and Reggio Emilia"
      ]
    },
    {
      "id": "https://openalex.org/A2096646689",
      "name": "Rita Cucchiara",
      "affiliations": [
        "University of Modena and Reggio Emilia"
      ]
    },
    {
      "id": "https://openalex.org/A2508732373",
      "name": "Marcella Cornia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2141204826",
      "name": "Matteo Stefanini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135711272",
      "name": "Lorenzo Baraldi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096646689",
      "name": "Rita Cucchiara",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2956018683",
    "https://openalex.org/W2062955551",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6765264507",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W6763643401",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W6753850902",
    "https://openalex.org/W2963758027",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2984138079",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W6788995615",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2560645892",
    "https://openalex.org/W2963877622",
    "https://openalex.org/W6678262379",
    "https://openalex.org/W6685322675",
    "https://openalex.org/W2963686907",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2901988662",
    "https://openalex.org/W6725318829",
    "https://openalex.org/W2951183276",
    "https://openalex.org/W2904565150",
    "https://openalex.org/W6754778999",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W1987835821",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2983141445",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2463955103",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2949376505",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2795151422",
    "https://openalex.org/W2899505139",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2953022248",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3104279398",
    "https://openalex.org/W2885013662",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2971310675",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2123301721"
  ],
  "abstract": "Transformer-based architectures represent the state of the art in sequence modeling tasks like machine translation and language understanding. Their applicability to multi-modal contexts like image captioning, however, is still largely under-explored. With the aim of filling this gap, we present M$^2$ - a Meshed Transformer with Memory for Image Captioning. The architecture improves both the image encoding and the language generation steps: it learns a multi-level representation of the relationships between image regions integrating learned a priori knowledge, and uses a mesh-like connectivity at decoding stage to exploit low- and high-level features. Experimentally, we investigate the performance of the M$^2$ Transformer and different fully-attentive models in comparison with recurrent ones. When tested on COCO, our proposal achieves a new state of the art in single-model and ensemble configurations on the \"Karpathy\" test split and on the online test server. We also assess its performances when describing objects unseen in the training set. Trained models and code for reproducing the experiments are publicly available at: https://github.com/aimagelab/meshed-memory-transformer.",
  "full_text": "Meshed-Memory Transformer for Image Captioning\nMarcella Cornia‚àó Matteo Stefanini‚àó Lorenzo Baraldi‚àó Rita Cucchiara\nUniversity of Modena and Reggio Emilia\n{name.surname}@unimore.it\nAbstract\nTransformer-based architectures represent the state of\nthe art in sequence modeling tasks like machine transla-\ntion and language understanding. Their applicability to\nmulti-modal contexts like image captioning, however, is\nstill largely under-explored. With the aim of Ô¨Ålling this\ngap, we present M2 ‚Äì a Meshed Transformer with Mem-\nory for Image Captioning. The architecture improves both\nthe image encoding and the language generation steps: it\nlearns a multi-level representation of the relationships be-\ntween image regions integrating learned a priori knowl-\nedge, and uses a mesh-like connectivity at decoding stage\nto exploit low- and high-level features. Experimentally, we\ninvestigate the performance of the M2 Transformer and\ndifferent fully-attentive models in comparison with recur-\nrent ones. When tested on COCO, our proposal achieves\na new state of the art in single-model and ensemble con-\nÔ¨Ågurations on the ‚ÄúKarpathy‚Äù test split and on the on-\nline test server. We also assess its performances when de-\nscribing objects unseen in the training set. Trained mod-\nels and code for reproducing the experiments are publicly\navailable at: https://github.com/aimagelab/\nmeshed-memory-transformer.\n1. Introduction\nImage captioning is the task of describing the visual con-\ntent of an image in natural language. As such, it requires\nan algorithm to understand and model the relationships be-\ntween visual and textual elements, and to generate a se-\nquence of output words. This has usually been tackled via\nRecurrent Neural Network models [42, 17, 45, 44, 7], in\nwhich the sequential nature of language is modeled with\nthe recurrent relations of either RNNs or LSTMs. Additive\nattention or graph-like structures [48] are often added to the\nrecurrence [45, 14] in order to model the relationships be-\ntween image regions, words, and eventually tags [24].\nThis schema has remained the dominant approach in\n‚àóEqual contribution.\nEncoder \nLayer N\nEncoder \nLayer 2\nEncoder \nLayer 1\nDecoder\nLayer 1ùúé\n‚Ä¶\nDecoder\nLayer 2\nDecoder\nLayer N\nùúé\nùúé\n‚Ä¶\nA baseball player is \nthrowing a ball to \nanother player.\nMemory-Augmented Encoding Meshed Decoding\nFigure 1: Our image captioning approach encodes rela-\ntionships between image regions exploiting learned a pri-\nori knowledge. Multi-level encodings of image regions are\nconnected to a language decoder through a meshed and\nlearnable connectivity.\nthe last few years, with the exception of the investigation\nof Convolutional language models [5], which however did\nnot become a leading choice. The recent advent of fully-\nattentive models, in which the recurrent relation is aban-\ndoned in favour of the use of self-attention, offers unique\nopportunities in terms of set and sequence modeling perfor-\nmances, as testiÔ¨Åed by the Transformer [39] and BERT [8]\nmodels and their applications to retrieval [35] and video un-\nderstanding [37]. Also, this setting offers novel architec-\ntural modeling capabilities, as for the Ô¨Årst time the atten-\ntion operator is used in a multi-layer and extensible fashion.\nNevertheless, the multi-modal nature of image captioning\ndemands for speciÔ¨Åc architectures, different from those em-\nployed for the understanding of a single modality.\nFollowing this premise, we investigate the design of\na novel fully-attentive approach for image captioning.\nOur architecture takes inspiration from the Transformer\nmodel [39] for machine translation and incorporates two\nkey novelties with respect to all previous image caption-\ning algorithms: (i) image regions and their relationships are\narXiv:1912.08226v2  [cs.CV]  20 Mar 2020\nencoded in a multi-level fashion, in which low-level and\nhigh-level relations are taken into account. When modeling\nthese relationships, our model can learn and encode a pri-\nori knowledge by using persistent memory vectors. (ii) The\ngeneration of the sentence, done with a multi-layer architec-\nture, exploits both low- and high-level visual relationships\ninstead of having just a single input from the visual modal-\nity. This is achieved through a learned gating mechanism,\nwhich weights multi-level contributions at each stage. As\nthis creates a mesh connectivity schema between encoder\nand decoder layers, we name our model Meshed-Memory\nTransformer ‚Äì M2 Transformer for short. Figure 1 depicts\na schema of the architecture.\nExperimentally, we explore different fully-attentive\nbaselines and recent proposals, gaining insights on the per-\nformance of fully-attentive models in image captioning.\nOur M2 Transformer, when tested on the COCO bench-\nmark, achieves a new state of the art on the ‚ÄúKarpathy‚Äù\ntest set, on both single-model and ensemble conÔ¨Ågurations.\nMost importantly, it surpasses existing proposals on the on-\nline test server, ranking Ô¨Årst among published algorithms.\nContributions. To sum up, our contributions are as follows:\n‚Ä¢We propose a novel fully-attentive image captioning\nalgorithm. Our model encapsulates a multi-layer en-\ncoder for image regions and a multi-layer decoder\nwhich generates the output sentence. To exploit both\nlow-level and high-level contributions, encoding and\ndecoding layers are connected in a mesh-like structure,\nweighted through a learnable gating mechanism;\n‚Ä¢In our visual encoder, relationships between image re-\ngions are encoded in a multi-level fashion exploiting\nlearned a priori knowledge, which is modeled via per-\nsistent memory vectors;\n‚Ä¢We show that the M2 Transformer surpasses all pre-\nvious proposals for image captioning, achieving a new\nstate of the art on the online COCO evaluation server;\n‚Ä¢As a complementary contribution, we conduct experi-\nments to compare different fully-attentive architectures\non image captioning and validate the performance of\nour model on novel object captioning, using the re-\ncently proposed nocaps dataset. Finally, to improve\nreproducibility and foster new research in the Ô¨Åeld, we\nwill publicly release the source code and trained mod-\nels of all experiments.\n2. Related work\nA broad collection of methods have been proposed in the\nÔ¨Åeld of image captioning in the last few years. Earlier cap-\ntioning approaches were based on the generation of simple\ntemplates, Ô¨Ålled by the output of an object detector or at-\ntribute predictor [34, 47]. With the advent of Deep Neu-\nral Networks, most captioning techniques have employed\nRNNs as language models and used the output of one or\nmore layers of a CNN to encode visual information and con-\ndition language generation [43, 33, 9, 16]. On the training\nside, while initial methods were based on a time-wise cross-\nentropy training, a notable achievement has been made with\nthe introduction of Reinforcement Learning, which enabled\nthe use of non-differentiable caption metrics as optimization\nobjectives [33, 31, 25]. On the image encoding side, in-\nstead, single-layer attention mechanisms have been adopted\nto incorporate spatial knowledge, initially from a grid of\nCNN features [45, 26, 50], and then using image regions\nextracted with an object detector [4, 29, 27]. To further im-\nprove the encoding of objects and their relationships, Yaoet\nal. [48] have proposed to use a graph convolution neural\nnetwork in the image encoding phase to integrate semantic\nand spatial relationships between objects. On the same line,\nYang et al. [46] used a multi-modal graph convolution net-\nwork to modulate scene graphs into visual representations.\nDespite their wide adoption, RNN-based models suffer\nfrom their limited representation power and sequential na-\nture. After the emergence of Convolutional language mod-\nels, which have been explored for captioning as well [5],\nnew fully-attentive paradigms [39, 8, 36] have been pro-\nposed and achieved state-of-the-art results in machine trans-\nlation and language understanding tasks. Likewise, some\nrecent approaches have investigated the application of the\nTransformer model [39] to the image captioning task.\nIn a nutshell, the Transformer comprises an encoder\nmade of a stack of self-attention and feed-forward layers,\nand a decoder which uses self-attention on words and cross-\nattention over the output of the last encoder layer. Her-\ndade et al. [13] used the Transformer architecture for image\ncaptioning and incorporated geometric relations between\ndetected input objects. In particular, they computed an addi-\ntional geometric weight between object pairs which is used\nto scale attention weights. Li et al . [24] used the Trans-\nformer in a model that exploits visual information and addi-\ntional semantic knowledge given by an external tagger. On a\nrelated line, Huang et al. [14] introduced an extension of the\nattention operator in which the Ô¨Ånal attended information is\nweighted by a gate guided by the context. In their approach,\na Transformer-like encoder was paired with an LSTM de-\ncoder. While the aforementioned approaches have exploited\nthe original Transformer architecture, in this paper we de-\nvise a novel fully-attentive model that improves the design\nof both the image encoder and the language decoder, intro-\nducing two novel attention operators and a different design\nof the connectivity between encoder and decoder.\n3. Meshed-Memory Transformer\nOur model can be conceptually divided into an encoder\nand a decoder module, both made of stacks of attentive lay-\ners. While the encoder is in charge of processing regions\nfrom the input image and devising relationships between\nmasked self-attention\nDecoder\nLayer 1\nDecoder\nLayer 2\nDecoder\nLayer N\n‚Ä¶ ‚Ä¶\nEncoder \nLayer N\nEncoder \nLayer 1\nEncoder Decoder\nEncoder \nLayer 2\nkey\nvalue\nattention\nquery\nfeed-forward\nmemory\nmemory\nMemory-Augmented Encoder\nMeshed Decoder\nkey\nvalue\nkey\nvalue\ncross-attention cross-attention\nFC FC\nquery\nfeed-forward\n‚Ä¶\nùúé ùúé\n‚àô ‚àô\n‚Ä¶\nFigure 2: Architecture of the M2 Transformer. Our model is composed of a stack of memory-augmented encoding layers,\nwhich encodes multi-level visual relationships with a priori knowledge, and a stack of decoder layers, in charge of generating\ntextual tokens. For the sake of clarity, AddNorm operations are not shown. Best seen in color.\nthem, the decoder reads from the output of each encoding\nlayer to generate the output caption word by word. All intra-\nmodality and cross-modality interactions between word and\nimage-level features are modeled via scaled dot-product at-\ntention, without using recurrence. Attention operates on\nthree sets of vectors, namely a set of queries Q, keys K\nand values V, and takes a weighted sum of value vectors\naccording to a similarity distribution between query and key\nvectors. In the case of scaled dot-product attention, the op-\nerator can be formally deÔ¨Åned as\nAttention(Q,K,V) = softmax\n(QKT\n‚àö\nd\n)\nV, (1)\nwhere Qis a matrix of nq query vectors, K and V both\ncontain nk keys and values, all with the same dimensional-\nity, and dis a scaling factor.\n3.1. Memory-Augmented Encoder\nGiven a set of image regions X extracted from an in-\nput image, attention can be used to obtain a permutation in-\nvariant encoding of Xthrough the self-attention operations\nused in the Transformer [39]. In this case, queries, keys, and\nvalues are obtained by linearly projecting the input features,\nand the operator can be deÔ¨Åned as\nS(X) =Attention(WqX,WkX,WvX), (2)\nwhere Wq,Wk,Wv are matrices of learnable weights. The\noutput of the self-attention operator is a new set of elements\nS(X), with the same cardinality as X, in which each ele-\nment of X is replaced with a weighted sum of the values,\ni.e. of linear projections of the input (Eq. 1).\nNoticeably, attentive weights depend solely on the pair-\nwise similarities between linear projections of the input set\nitself. Therefore, the self-attention operator can be seen as\na way of encoding pairwise relationships inside the input\nset. When using image regions (or features derived from\nimage regions) as the input set, S(¬∑) can naturally encode\nthe pairwise relationships between regions that are needed\nto understand the input image before describing it1.\nThis peculiarity in the deÔ¨Ånition of self-attention has,\nhowever, a signiÔ¨Åcant limitation. Because everything de-\npends solely on pairwise similarities, self-attention cannot\nmodel a priori knowledge on relationships between image\nregions. For example, given one region encoding a man and\na region encoding a basketball ball, it would be difÔ¨Åcult to\ninfer the concept of player or game without any a priori\nknowledge. Again, given regions encoding eggs and toasts,\nthe knowledge that the picture depicts a breakfast could be\neasily inferred using a priori knowledge on relationships.\nMemory-Augmented Attention.To overcome this limita-\ntion of self-attention, we propose a memory-augmented at-\ntention operator. In our proposal, the set of keys and values\nused for self-attention is extended with additional ‚Äúslots‚Äù\nwhich can encode a priori information. To stress that a pri-\nori information should not depend on the input set X, the\nadditional keys and values are implemented as plain learn-\nable vectors which can be directly updated via SGD. For-\nmally, the operator is deÔ¨Åned as:\nMmem(X) =Attention(WqX,K,V)\nK= [WkX,Mk]\nV = [WvX,Mv] , (3)\nwhere Mk and Mv are learnable matrices with nm rows,\nand [¬∑,¬∑] indicates concatenation. Intuitively, by adding\n1Taking another perspective, self-attention is also conceptually equiva-\nlent to an attentive encoding of graph nodes [41].\nlearnable keys and values, through attention it will be possi-\nble to retrieve learned knowledge which is not already em-\nbedded in X. At the same time, our formulation leaves the\nset of queries unaltered.\nJust like the self-attention operator, our memory-\naugmented attention can be applied in a multi-head fash-\nion. In this case, the memory-augmented attention opera-\ntion is repeated htimes, using different projection matrices\nWq,Wk,Wv and different learnable memory slotsMk,Mv\nfor each head. Then, we concatenate the results from differ-\nent heads and apply a linear projection.\nEncoding layer.We embed our memory-augmented opera-\ntor into a Transformer-like layer: the output of the memory-\naugmented attention is applied to a position-wise feed-\nforward layer composed of two afÔ¨Åne transformations with\na single non-linearity, which are independently applied to\neach element of the set. Formally,\nF(X)i = UœÉ(VXi + b) +c, (4)\nwhere Xi indicates the i-th vector of the input set, and\nF(X)i the i-th vector of the output. Also, œÉ(¬∑) is the ReLU\nactivation function, V and U are learnable weight matrices,\nband care bias terms.\nEach of these sub-components (memory-augmented at-\ntention and position-wise feed-forward) is then encapsu-\nlated within a residual connection and a layer norm oper-\nation. The complete deÔ¨Ånition of an encoding layer can be\nÔ¨Ånally written as:\nZ= AddNorm(Mmem(X))\nÀúX= AddNorm(F(Z)), (5)\nwhere AddNorm indicates the composition of a residual\nconnection and of a layer normalization.\nFull encoder. Given the aforementioned structure, multi-\nple encoding layers are stacked in sequence, so that the i-\nth layer consumes the output set computed by layer i‚àí1.\nThis amounts to creating multi-level encodings of the rela-\ntionships between image regions, in which higher encoding\nlayers can exploit and reÔ¨Åne relationships already identiÔ¨Åed\nby previous layers, eventually using a priori knowledge. A\nstack of N encoding layers will therefore produce a multi-\nlevel output ÀúX= (ÀúX1,..., ÀúXN), obtained from the outputs\nof each encoding layer.\n3.2. Meshed Decoder\nOur decoder is conditioned on both previously generated\nwords and region encodings, and is in charge of generat-\ning the next tokens of the output caption. Here, we exploit\nthe aforementioned multi-level representation of the input\nimage while still building a multi-layer structure. To this\naim, we devise a meshed attention operator which, unlike\nthe cross-attention operator of the Transformer, can take ad-\nvantage of all encoding layers during the generation of the\nsentence.\nMeshed Cross-Attention.Given an input sequence of vec-\ntors Y, and outputs from all encoding layers ÀúX, the Meshed\nAttention operator connects Y to all elements in ÀúXthrough\ngated cross-attentions. Instead of attending only the last en-\ncoding layer, we perform a cross-attention with all encoding\nlayers. These multi-level contributions are then summed to-\ngether after being modulated. Formally, our meshed atten-\ntion operator is deÔ¨Åned as\nMmesh( ÀúX,Y) =\nN‚àë\ni=1\nŒ±i ‚äôC( ÀúXi,Y), (6)\nwhere C(¬∑,¬∑) stands for the encoder-decoder cross-attention,\ncomputed using queries from the decoder and keys and val-\nues from the encoder:\nC( ÀúXi,Y) =Attention(WqY,Wk ÀúXi,Wv ÀúXi), (7)\nand Œ±i is a matrix of weights having the same size as the\ncross-attention results. Weights in Œ±i modulate both the\nsingle contribution of each encoding layer, and the relative\nimportance between different layers. These are computed\nby measuring the relevance between the result of the cross-\nattention computed with each encoding layer and the input\nquery, as follows:\nŒ±i = œÉ\n(\nWi\n[\nY,C( ÀúXi,Y)\n]\n+ bi\n)\n, (8)\nwhere [¬∑,¬∑] indicates concatenation, œÉis the sigmoid activa-\ntion, Wi is a 2d√ódweight matrix, and bi is a learnable bias\nvector.\nArchitecture of decoding layers.As for encoding layers,\nwe apply our meshed attention in a multi-head fashion. As\nthe prediction of a word should only depend on previously\npredicted words, the decoder layer comprises a masked self-\nattention operation which connects queries derived from the\nt-th element of its input sequence Y with keys and values\nobtained from the left-hand subsequence,i.e. Y‚â§t. Also, the\ndecoder layer contains a position-wise feed-forward layer\n(as in Eq. 4), and all components are encapsulated within\nAddNorm operations. The Ô¨Ånal structure of the decoder\nlayer can be written as:\nZ= AddNorm(Mmesh( ÀúX,AddNorm(Smask(Y)))\nÀúY = AddNorm(F(Z)), (9)\nwhere Y is the input sequence of vectors and Smask indi-\ncates a masked self-attention over time. Finally, our decoder\nstacks together multiple decoder layers, helping to reÔ¨Åne\nboth the understanding of the textual input and the genera-\ntion of next tokens. Overall, the decoder takes as input word\nvectors, and the t-th element of its output sequence encodes\nthe prediction of a word at time t+ 1, conditioned on Y‚â§t.\nAfter taking a linear projection and a softmax operation, this\nencodes a probability over words in the dictionary.\n3.3. Training details\nFollowing a standard practice in image captioning [31,\n33, 4], we pre-train our model with a word-level cross-\nentropy loss (XE) and Ô¨Ånetune the sequence generation us-\ning reinforcement learning. When training with XE, the\nmodel is trained to predict the next token given previous\nground-truth words; in this case, the input sequence for the\ndecoder is immediately available and the computation of the\nentire output sequence can be done in a single pass, paral-\nlelizing all operations over time.\nWhen training with reinforcement learning, we employ\na variant of the self-critical sequence training approach [33]\non sequences sampled using beam search [4]: to decode,\nwe sample the top- k words from the decoder probability\ndistribution at each timestep, and always maintain the top-k\nsequences with highest probability. As sequence decoding\nis iterative in this step, the aforementioned parallelism over\ntime cannot be exploited. However, intermediate keys and\nvalues used to compute the output token at time t can be\nreused in the next iterations.\nFollowing previous works [4], we use the CIDEr-D score\nas reward, as it well correlates with human judgment [40].\nWe baseline the reward using the mean of the rewards rather\nthan greedy decoding as done in previous methods [33, 4],\nas we found it to slightly improve the Ô¨Ånal performance.\nThe Ô¨Ånal gradient expression for one sample is thus:\n‚àáŒ∏L(Œ∏) =‚àí1\nk\nk‚àë\ni=1\n(\n(r(wi) ‚àíb)‚àáŒ∏log p(wi)\n)\n(10)\nwhere wi is the i-th sentence in the beam, r(¬∑) is the reward\nfunction, and b=\n(‚àë\nir(wi)\n)\n/kis the baseline, computed\nas the mean of the rewards obtained by the sampled se-\nquences. At prediction time, we decode again using beam\nsearch, and keep the sequence with highest predicted prob-\nability among those in the last beam.\n4. Experiments\n4.1. Datasets\nWe Ô¨Årst evaluate our model on the COCO dataset [23],\nwhich is the most commonly used test-bed for image cap-\ntioning. Then, we assess the captioning of novel objects by\ntesting on the recently proposed nocaps dataset [1].\nCOCO. The dataset contains more than 120 000images,\neach of them annotated with 5 different captions. We fol-\nlow the splits provided by Karpathy et al. [17], where 5 000\nimages are used for validation, 5 000for testing and the rest\nfor training. We also evaluate the model on the COCO on-\nline test server, composed of 40 775images for which an-\nnotations are not made publicly available.\nnocaps. The dataset consists of 15 100images taken\nfrom the Open Images [21] validation and test sets, each\nannotated with 11 human-generated captions. Images are\ndivided into validation and test splits, respectively com-\nposed of 4 500and 10 600elements. Images can be fur-\nther grouped into three subsets depending on the nearness\nto COCO, namely in-domain, near-domain, and out-of-\ndomain images. Under this setting, we use COCO as train-\ning data and evaluate our results on the nocaps test server.\n4.2. Experimental settings\nMetrics. Following the standard evaluation protocol, we\nemploy the full set of captioning metrics: BLEU [28], ME-\nTEOR [6], ROUGE [22], CIDEr [40], and SPICE [2].\nImplementation details. To represent image regions, we\nuse Faster R-CNN [32] with ResNet-101 [12] Ô¨Ånetuned on\nthe Visual Genome dataset [20, 4], thus obtaining a 2048-\ndimensional feature vector for each region. To represent\nwords, we use one-hot vectors and linearly project them to\nthe input dimensionality of the model d. We also employ\nsinusoidal positional encodings [39] to represent word po-\nsitions inside the sequence and sum the two embeddings\nbefore the Ô¨Årst decoding layer.\nIn our model, we set the dimensionalitydof each layer to\n512, the number of heads to 8, and the number of memory\nvectors to 40. We employ dropout with keep probability0.9\nafter each attention and feed-forward layer. In our meshed\nattention operator (Eq. 6), we normalize the output with a\nscaling factor of\n‚àö\nN. Pre-training with XE is done fol-\nlowing the learning rate scheduling strategy of [39] with a\nwarmup equal to 10 000iterations. Then, during CIDEr-D\noptimization, we use a Ô¨Åxed learning rate of 5 √ó10‚àí6. We\ntrain all models using the Adam optimizer [19], a batch size\nof 50, and a beam size equal to 5.\nNovel object captioning. To train the model on the no-\ncaps dataset, instead of using one-hot vectors, we repre-\nsent words with GloVe word embeddings [30]. Two fully-\nconnected layers are added to convert between the GloVe\ndimensionality and dbefore the Ô¨Årst decoding layer and af-\nter the last decoding layer. Before the Ô¨Ånal softmax, we\nmultiply with the transpose of the word embeddings. All\nother implementation details are kept unchanged.\nAdditional details on model architecture and training can be\nfound in the supplementary material.\n4.3. Ablation study\nPerformance of the Transformer.In previous works, the\nTransformer model has been applied to captioning only in\nits original conÔ¨Åguration with six layers, with the structure\nof connections that has been successful for uni-modal sce-\nB-1 B-4 M R C S\nTransformer (w/ 6 layers as in [39]) 79.1 36.2 27.7 56.9 121.8 20.9\nTransformer (w/ 3 layers) 79.6 36.5 27.8 57.0 123.6 21.1\nTransformer (w/ AoA [14]) 80.3 38.8 29.0 58.4 129.1 22.7\nM2 Transformer1-to-1(w/o mem.) 80.5 38.2 28.9 58.2 128.4 22.2\nM2 Transformer1-to-1 80.3 38.2 28.9 58.2 129.2 22.5\nM2 Transformer (w/o mem.) 80.4 38.3 29.0 58.2 129.4 22.6\nM2 Transformer (w/ softmax) 80.3 38.4 29.1 58.3 130.3 22.5\nM2 Transformer 80.8 39.1 29.2 58.6 131.2 22.6\nTable 1: Ablation study and comparison with Transformer-\nbased alternatives. All results are reported after the REIN-\nFORCE optimization stage.\nnarios like machine translation. As we speculate that cap-\ntioning requires speciÔ¨Åc architectures, we compare varia-\ntions of the original Transformer with our approach.\nFirstly, we investigate the impact of the number of en-\ncoding and decoding layers on captioning performance. As\nit can be seen in Table 1, the original Transformer (six\nlayers) achieves 121.8 CIDEr, slightly superior to the Up-\nDown approach [4] which uses a two-layer recurrent lan-\nguage model with additive attention and includes a global\nfeature vector (120.1 CIDEr). Varying the number of layers,\nwe observe a signiÔ¨Åcant increase in performance when us-\ning three encoding and three decoding layers, which leads to\n123.6 CIDEr. We hypothesize that this is due to the reduced\ntraining set size and to the lower semantic complexities of\nsentences in captioning with respect to those of language\nunderstanding tasks. Following this Ô¨Ånding, all subsequent\nexperiments will use three layers.\nAttention on Attention baseline.We also evaluate a re-\ncent proposal that can be straightforwardly applied to the\nTransformer as an alternative to standard dot-product atten-\ntion. SpeciÔ¨Åcally, we evaluate the addition of the ‚ÄúAttention\non Attention‚Äù (AoA) approach [14] to the attentive layers,\nboth in the encoder and in the decoder. Noticeably, in [14]\nthis has been done with a Recurrent language model with\nattention, but the approach is sufÔ¨Åciently general to be ap-\nplied to any attention stage. In this case, the result of dot-\nproduct attention is concatenated with the initial query and\nfed to two fully connected layers to obtain an information\nvector and a sigmoidal attention gate, then the two vectors\nare multiplied together. The Ô¨Ånal result is used as an alter-\nnative to the standard dot-product attention. This addition\nto a standard Transformer with three layers leads to 129.1\nCIDEr (Table 1), thus underlying the usefulness of the ap-\nproach also in Transformer-based models.\nMeshed Connectivity. We then evaluate the role of the\nmeshed connections between encoder and decoder layers.\nIn Table 1, we Ô¨Årstly introduce a reduced version of our ap-\nproach in which the i-th decoder layer is only connected to\nthe corresponding i-th encoder layer (1-to-1), instead of be-\ning connected to all encoders. Using this 1-to-1 connectiv-\nB-1 B-4 M R C S\nSCST [33] - 34.2 26.7 55.7 114.0 -\nUp-Down [4] 79.8 36.3 27.7 56.9 120.1 21.4\nRFNet [15] 79.1 36.5 27.7 57.3 121.9 21.2\nUp-Down+HIP [49] - 38.2 28.4 58.3 127.2 21.9\nGCN-LSTM [48] 80.5 38.2 28.5 58.3 127.6 22.0\nSGAE [46] 80.8 38.4 28.4 58.6 127.8 22.1\nORT [13] 80.5 38.6 28.7 58.4 128.3 22.6\nAoANet [14] 80.2 38.9 29.2 58.8 129.8 22.4\nM2 Transformer 80.8 39.1 29.2 58.6 131.2 22.6\nTable 2: Comparison with the state of the art on the ‚ÄúKarpa-\nthy‚Äù test split, in single-model setting.\nB-1 B-4 M R C S\nEnsemble/Fusion of 2 models\nGCN-LSTM [48] 80.9 38.3 28.6 58.5 128.7 22.1\nSGAE [46] 81.0 39.0 28.4 58.9 129.1 22.2\nETA [24] 81.5 39.9 28.9 59.0 127.6 22.6\nGCN-LSTM+HIP [49] - 39.1 28.9 59.2 130.6 22.3\nM2 Transformer 81.6 39.8 29.5 59.2 133.2 23.1\nEnsemble/Fusion of 4 models\nSCST [33] - 35.4 27.1 56.6 117.5 -\nRFNet [15] 80.4 37.9 28.3 58.3 125.7 21.7\nAoANet [14] 81.6 40.2 29.3 59.4 132.0 22.8\nM2 Transformer 82.0 40.5 29.7 59.5 134.5 23.5\nTable 3: Comparison with the state of the art on the ‚ÄúKarpa-\nthy‚Äù test split, using an ensemble of models.\nity schema already brings an improvement with respect to\nusing the output of the last encoder layer as in the standard\nTransformer (123.6 CIDEr vs 129.2 CIDEr), thus conÔ¨Årm-\ning that exploiting a multi-level encoding of image regions\nis beneÔ¨Åcial. When we instead use our meshed connectiv-\nity schema, that exploits relationships encoded at all levels\nand weights them with a sigmoid gating, we observe a fur-\nther performance improvement, from 129.2 CIDEr to 131.2\nCIDEr. This amounts to a total improvement of 7.6 points\nwith respect to the standard Transformer. Also, the result of\nour full model is superior to that obtained using the AoA.\nAs an alternative to the sigmoid gating approach for\nweighting the contributions from different encoder layers\n(Eq. 6), we also test with a softmax gating schema. In this\ncase, the element-wise sigmoid applied to each encoder is\nreplaced with a softmax operation over the rows of Œ±i. Us-\ning this alternative brings to a reduction of around 1 CIDEr\npoint, underlying that it is beneÔ¨Åcial to exploit the full po-\ntentiality of a weighted sum of the contributions from all\nencoding layers, rather than forcing a peaky distribution in\nwhich one layer is given more importance than the others.\nRole of persistent memory.We evaluate the role of mem-\nory vectors in both the 1-to-1 conÔ¨Åguration and in the Ô¨Å-\nBLEU-1 BLEU-2 BLEU-3 BLEU-4 METEOR ROUGE CIDEr\nc5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40 c5 c40\nSCST [33] 78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7\nUp-Down [4] 80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5\nRDN [18] 80.2 95.3 - - - - 37.3 69.5 28.1 37.8 57.4 73.3 121.2 125.2\nRFNet [15] 80.4 95.0 64.9 89.3 50.1 80.1 38.0 69.2 28.2 37.2 58.2 73.1 122.9 125.1\nGCN-LSTM [48] 80.8 95.9 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5\nSGAE [46] 81.0 95.3 65.6 89.5 50.7 80.4 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5\nETA [24] 81.2 95.0 65.5 89.0 50.9 80.4 38.9 70.2 28.6 38.0 58.6 73.9 122.1 124.4\nAoANet [14] 81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6\nGCN-LSTM+HIP [49] 81.6 95.9 66.2 90.4 51.5 81.6 39.3 71.0 28.8 38.1 59.0 74.1 127.9 130.2\nM2 Transformer 81.6 96.0 66.4 90.8 51.8 82.7 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1\nTable 4: Leaderboard of various methods on the online MS-COCO test server.\nnal conÔ¨Åguration with meshed connections. As it can be\nseen from Table 1, removing memory vectors brings to a\nreduction in performance of around 1 CIDEr point in both\nconnectivity settings, thus conÔ¨Årming the usefulness of ex-\nploiting a priori learned knowledge when encoding image\nregions. Further experiments on the number of memory\nvectors can be found in the supplementary material.\n4.4. Comparison with state of the art\nWe compare the performances of our approach with\nthose of several recent proposals for image captioning.\nThe models we compare to include SCST [33] and Up-\nDown [4], which respectively use attention over the grid\nof features and attention over regions. Also, we com-\npare to RFNet [15], which uses a recurrent fusion net-\nwork to merge different CNN features; GCN-LSTM [48],\nwhich exploits pairwise relationships between image re-\ngions through a Graph CNN; SGAE [46], which instead\nuses auto-encoding scene graphs. Further, we compare with\nthe original AoANet [14] approach, which uses attention\non attention for encoding image regions and an LSTM lan-\nguage model. Finally, we compare with ORT [13], which\nuses a plain Transformer and weights attention scores in the\nregion encoder with pairwise distances between detections.\nWe evaluate our approach on the COCO ‚ÄúKarpathy‚Äù test\nsplit, using both single model and ensemble conÔ¨Ågurations,\nand on the online COCO evaluation server.\nSingle model.In Table 2 we report the performance of our\nmethod in comparison with the aforementioned competi-\ntors, using captions predicted from a single model and opti-\nmization on the CIDEr-D score. As it can be observed, our\nmethod surpasses all other approaches in terms of BLEU-4,\nMETEOR and CIDEr, while being competitive on BLEU-\n1 and SPICE with the best performer, and slightly worse\non ROUGE with respect to AoANet [14]. In particular, it\nadvances the current state of the art on CIDEr by 1.4 points.\nEnsemble model.Following the common practice [33, 14]\nof building an ensemble of models, we also report the per-\nformances of our approach when averaging the output prob-\nGT: A cat looking at his reÔ¨Çection in the mirror.\nTransformer: A cat sitting in a window sill look-\ning out.\nM2 Transformer: A cat looking at its reÔ¨Çection\nin a mirror.\nGT: A plate of food including eggs and toast on a\ntable next to a stone railing.\nTransformer: A group of food on a plate.\nM2 Transformer: A plate of breakfast food with\neggs and toast.\nGT: A truck parked near a tall pile of hay.\nTransformer: A truck is parked in the grass in a\nÔ¨Åeld.\nM2 Transformer: A green truck parked next to a\npile of hay.\nFigure 3: Examples of captions generated by our approach\nand the original Transformer model, as well as the corre-\nsponding ground-truths.\nability distributions of multiple and independently trained\ninstances of our model. In Table 3, we use ensembles of\ntwo and four models, trained from different random seeds.\nNoticeably, when using four models our approach achieves\nthe best performance according to all metrics, with an in-\ncrease of 2.5 CIDEr points with respect to the current state\nof the art [14].\nOnline Evaluation.Finally, we also report the performance\nof our method on the online COCO test server2. In this case,\nwe use the ensemble of four models previously described,\ntrained on the ‚ÄúKarpathy‚Äù training split. The evaluation is\ndone on the COCO test split, for which ground-truth anno-\ntations are not publicly available. Results are reported in Ta-\nble 4, in comparison with the top-performing approaches of\nthe leaderboard. For fairness of comparison, they also used\nan ensemble conÔ¨Åguration. As it can be seen, our method\nsurpasses the current state of the art on all metrics, achiev-\ning an advancement of 1.4 CIDEr points with respect to the\nbest performer.\n2https://competitions.codalab.org/competitions/3221\nFigure 4: Visualization of attention states for three sample captions. For each generated word, we show the attended image\nregions, outlining the region with the maximum output attribution in red.\nIn-Domain Out-of-Domain Overall\nCIDEr SPICE CIDEr SPICE CIDEr SPICE\nNBT + CBS [1] 62.1 10.1 62.4 8.9 60.2 9.5\nUp-Down + CBS [1] 80.0 12.0 66.4 9.7 73.1 11.1\nTransformer 78.0 11.0 29.7 7.8 54.7 9.8\nM2 Transformer 85.7 12.1 38.9 8.9 64.5 11.1\nTransformer + CBS 74.3 11.0 62.5 9.2 66.9 10.3\nM2 Transformer + CBS81.2 12.0 69.4 10.0 75.0 11.4\nTable 5: Performances on nocaps validation set, for in-\ndomain and out-of-domain captioning.\n4.5. Describing novel objects\nWe also assess the performance of our approach when\ndealing with images containing object categories that are\nnot seen in the training set. We compare with Up-Down [4]\nand Neural Baby Talk [27], when using GloVe word embed-\ndings and Constrained Beam Search (CBS) [3] to address\nthe generation of out-of-vocabulary words and constrain the\npresence of categories detected by an object detector. To\ncompare with our model, we use a simpliÔ¨Åed implementa-\ntion of the procedure described in [1] to extract constraints,\nwithout using word phrases. Results are shown in Table 5:\nas it can be seen, the original Transformer is signiÔ¨Åcantly\nless performing than Up-Down on both in-domain and out-\nof-domain categories, while our approach can properly deal\nwith novel categories, surpassing the Up-Down baseline in\nboth in-domain and out-of-domain images. As expected,\nthe use of CBS signiÔ¨Åcantly enhances the performances, in\nparticular on out-of-domain captioning.\n4.6. Qualitative results and visualization\nFigure 3 proposes qualitative results generated by our\nmodel and the original Transformer. On average, our model\nis able to generate more accurate and descriptive captions,\nintegrating Ô¨Åne-grained details and object relations.\nFinally, to better understand the effectiveness of ourM2\nTransformer, we investigate the contribution of detected re-\ngions to the model output. Differently from recurrent-based\ncaptioning models, in which attention weights over regions\ncan be easily extracted, in our model the contribution of one\nregion with respect to the output is given by more complex\nnon-linear dependencies. Therefore, we revert to attribution\nmethods: speciÔ¨Åcally, we employ the Integrated Gradients\napproach [38], which approximates the integral of gradi-\nents with respect to the given input. Results are presented\nin Figure 4, where we observe that our approach correctly\ngrounds image regions to words, also in presence of ob-\nject details and small detections. More visualizations are\nincluded in the supplementary material.\n5. Conclusion\nWe presented M2 Transformer, a novel Transformer-\nbased architecture for image captioning. Our model incor-\nporates a region encoding approach that exploits a priori\nknowledge through memory vectors and a meshed connec-\ntivity between encoding and decoding modules. Noticeably,\nthis connectivity pattern is unprecedented for other fully-\nattentive architectures. Experimental results demonstrated\nthat our approach achieves a new state of the art on COCO,\nranking Ô¨Årst in the on-line leaderboard. Finally, we vali-\ndated the components of our model through ablation stud-\nies, and its performances when describing novel objects.\nAcknowledgment\nThis work was partially supported by the ‚ÄúIDEHA - Inno-\nvation for Data Elaboration in Heritage Areas‚Äù project (PON\nARS01 00421), funded by the Italian Ministry of Education\n(MIUR). We also acknowledge the NVIDIA AI Technology Cen-\nter, EMEA, for its support and access to computing resources.\nReferences\n[1] Harsh Agrawal, Karan Desai, Xinlei Chen, Rishabh Jain,\nDhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.\nnocaps: novel object captioning at scale. In Proceedings of\nthe International Conference on Computer Vision , 2019. 5,\n8, 12\n[2] Peter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. SPICE: Semantic Propositional Image Cap-\ntion Evaluation. In Proceedings of the European Conference\non Computer Vision, 2016. 5\n[3] Peter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. Guided open vocabulary image captioning\nwith constrained beam search. In Proceedings of the Confer-\nence on Empirical Methods in Natural Language Processing,\n2017. 8\n[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and\nvisual question answering. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 2018.\n2, 5, 6, 7, 8, 11\n[5] Jyoti Aneja, Aditya Deshpande, and Alexander G Schwing.\nConvolutional image captioning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n2018. 1, 2\n[6] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic\nmetric for mt evaluation with improved correlation with hu-\nman judgments. In Proceedings of the ACL Workshop on\nIntrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and/or Summarization, 2005. 5\n[7] Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.\nShow, Control and Tell: A Framework for Generating Con-\ntrollable and Grounded Captions. InProceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n2019. 1\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1, 2\n[9] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama,\nMarcus Rohrbach, Subhashini Venugopalan, Kate Saenko,\nand Trevor Darrell. Long-term recurrent convolutional net-\nworks for visual recognition and description. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015. 2\n[10] Xavier Glorot and Yoshua Bengio. Understanding the difÔ¨Å-\nculty of training deep feedforward neural networks. In Inter-\nnational Conference on ArtiÔ¨Åcial Intelligence and Statistics,\n2010. 11\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectiÔ¨Åers: Surpassing human-level per-\nformance on imagenet classiÔ¨Åcation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2015. 11\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016. 5\n[13] Simao Herdade, Armin Kappeler, KoÔ¨Å Boakye, and Joao\nSoares. Image Captioning: Transforming Objects into\nWords. arXiv preprint arXiv:1906.05963, 2019. 2, 6, 7\n[14] Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei.\nAttention on Attention for Image Captioning. InProceedings\nof the International Conference on Computer Vision , 2019.\n1, 2, 6, 7\n[15] Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, and Tong\nZhang. Recurrent Fusion Network for Image Captioning. In\nProceedings of the European Conference on Computer Vi-\nsion, 2018. 6, 7\n[16] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. DenseCap:\nFully convolutional Localization Networks for Dense Cap-\ntioning. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2016. 2\n[17] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2015. 1, 5\n[18] Lei Ke, Wenjie Pei, Ruiyu Li, Xiaoyong Shen, and Yu-Wing\nTai. ReÔ¨Çective Decoding Network for Image Captioning. In\nProceedings of the International Conference on Computer\nVision, 2019. 7\n[19] Diederik P Kingma and Jimmy Ba. Adam: A Method for\nStochastic Optimization. In Proceedings of the International\nConference on Learning Representations, 2015. 5\n[20] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, Michael Bernstein, and\nLi Fei-Fei. Visual Genome: Connecting Language and Vi-\nsion Using Crowdsourced Dense Image Annotations. Inter-\nnational Journal of Computer Vision , 123(1):32‚Äì73, 2017.\n5\n[21] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Tom Duerig, et al. The Open Im-\nages Dataset V4: UniÔ¨Åed image classiÔ¨Åcation, object detec-\ntion, and visual relationship detection at scale.arXiv preprint\narXiv:1811.00982, 2018. 5\n[22] Chin-Yew Lin. Rouge: A package for automatic evaluation\nof summaries. In Text summarization branches out: Pro-\nceedings of the ACL Workshop, volume 8, 2004. 5\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick. Microsoft COCO: Common Objects in Context. In\nProceedings of the European Conference on Computer Vi-\nsion, 2014. 5\n[24] Guang Li Linchao Zhu Ping Liu and Yi Yang. Entangled\nTransformer for Image Captioning. In Proceedings of the\nInternational Conference on Computer Vision, 2019. 1, 2, 6,\n7\n[25] Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and\nKevin Murphy. Improved Image Captioning via Policy Gra-\ndient Optimization of SPIDEr. In Proceedings of the Inter-\nnational Conference on Computer Vision, 2017. 2\n[26] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher.\nKnowing when to look: Adaptive attention via a visual sen-\ntinel for image captioning. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 2017.\n2\n[27] Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\nNeural Baby Talk. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018. 2, 8\n[28] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th Annual Meeting on\nAssociation for Computational Linguistics, 2002. 5\n[29] Marco Pedersoli, Thomas Lucas, Cordelia Schmid, and\nJakob Verbeek. Areas of attention for image captioning. In\nProceedings of the International Conference on Computer\nVision, 2017. 2\n[30] Jeffrey Pennington, Richard Socher, and Christopher Man-\nning. GloVe: Global vectors for word representation. InPro-\nceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, 2014. 5\n[31] Marc‚ÄôAurelio Ranzato, Sumit Chopra, Michael Auli, and\nWojciech Zaremba. Sequence level training with recurrent\nneural networks. In Proceedings of the International Con-\nference on Learning Representations, 2015. 2, 5\n[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. In Advances in Neural Information\nProcessing Systems, 2015. 5\n[33] Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jarret\nRoss, and Vaibhava Goel. Self-critical sequence training for\nimage captioning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2017. 2, 5, 6,\n7\n[34] Richard Socher and Li Fei-Fei. Connecting modalities:\nSemi-supervised segmentation and annotation of images us-\ning unaligned text corpora. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 2010.\n2\n[35] Yale Song and Mohammad Soleymani. Polysemous visual-\nsemantic embedding for cross-modal retrieval. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019. 1\n[36] Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample,\nHerve Jegou, and Armand Joulin. Augmenting Self-attention\nwith Persistent Memory. arXiv preprint arXiv:1907.01470,\n2019. 2\n[37] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning. In Proceedings of the In-\nternational Conference on Computer Vision, 2019. 1\n[38] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. In Proceedings of the Interna-\ntional Conference on Machine Learning, 2017. 8, 11\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, 2017. 1, 2, 3, 5, 6\n[40] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. CIDEr: Consensus-based Image Description Eval-\nuation. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2015. 5\n[41] Petar Veli Àáckovi¬¥c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph At-\ntention Networks. In Proceedings of the International Con-\nference on Learning Representations, 2018. 3\n[42] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. Show and tell: A neural image caption gen-\nerator. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2015. 1\n[43] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. Show and tell: Lessons learned from the 2015\nmscoco image captioning challenge. IEEE Transactions on\nPattern Analysis and Machine Intelligence , 39(4):652‚Äì663,\n2016. 2\n[44] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-\nmitru Erhan. Show and Tell: Lessons Learned from the 2015\nMSCOCO Image Captioning Challenge. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 39(4):652‚Äì\n663, 2017. 1\n[45] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhutdinov, Richard S Zemel, and\nYoshua Bengio. Show, attend and tell: Neural image cap-\ntion generation with visual attention. In Proceedings of the\nInternational Conference on Machine Learning, 2015. 1, 2\n[46] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai.\nAuto-Encoding Scene Graphs for Image Captioning. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2019. 2, 6, 7\n[47] Benjamin Z Yao, Xiong Yang, Liang Lin, Mun Wai Lee, and\nSong-Chun Zhu. I2t: Image parsing to text description. Pro-\nceedings of the IEEE, 98(8):1485‚Äì1508, 2010. 2\n[48] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Exploring\nVisual Relationship for Image Captioning. InProceedings of\nthe European Conference on Computer Vision, 2018. 1, 2, 6,\n7\n[49] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. Hierarchy\nParsing for Image Captioning. In Proceedings of the Inter-\nnational Conference on Computer Vision, 2019. 6, 7\n[50] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and\nJiebo Luo. Image captioning with semantic attention. InPro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2016. 2\nA. Supplementary material\nIn the following, we present additional material about our M2\nTransformer model. In particular, we provide additional training\nand implementation details, further experimental results, and visu-\nalizations.\nA.1. Additional implementation details\nDecoding optimization.As mentioned in Sec. 3.3, during the de-\ncoding stage computation cannot be parallelized over time as the\ninput sequence is iteratively built. A naive approach would be to\nfeed the model at each iteration with the previous t ‚àí1 generated\nwords, {w0, w1, ..., wt‚àí1}and sample the next predicted word\nwt after computing the results of each attention and feed-forward\nlayer over all timesteps. This in practice requires to re-compute\nthe same queries, keys, values and attentive states multiple times,\nwith intermediate results depending onwt being recomputed T ‚àít\ntimes, where T is the length of the sampled sequence (in our ex-\nperiments T is equal to 20).\nIn our implementation, we revert to a more computationally\nfriendly approach in which we re-use intermediate results com-\nputed at previous timesteps. Each attentive layer of the decoder\ninternally stores previously computed keys and values. At each\ntimestep of the decoding, the model is fed only withwt‚àí1, and we\nonly compute queries, keys and values depending on wt‚àí1.\nIn PyTorch, this can be implemented by exploiting the\nregister buffer method of nn.Module, and creating\nbuffers to hold previously computed results. When running on\na NVIDIA 2080Ti GPU, we found this to reduce training and in-\nference times by approximately a factor of 3.\nVocabulary and tokenization.We convert all captions to lower-\ncase, remove punctuation characters and tokenize using the spaCy\nNLP toolkit3. To build vocabularies, we remove all words which\nappear less than 5 times in training and validation splits. For each\nimage, we use a maximum number of region feature vectors equal\nto 50.\nModel dimensionality and weight initialization.Using 8 atten-\ntive heads, the size of queries, keys and values in each head is set\nto d/8 = 64. Weights of attentive layers are initialized from the\nuniform distribution proposed by Glorot et al. [10], while weights\nof feed-forward layers are initialized using [11]. All biases are ini-\ntialized to 0. Memory vectors for keys and values are initialized\nfrom a normal distribution with zero mean and, respectively,1/dk\nand 1/m variance, where dk is the dimensionality of keys and m\nis the number of memory vectors.\nA.2. Additional experimental results\nMemory vectors. In Table 6, we report the performance of our\napproach when using a varying number of memory vectors. As it\ncan be seen, the best result in terms of BLEU, METEOR, ROUGE\nand CIDEr is obtained with 40 memory vectors, while 80 mem-\nory vectors provide a slightly superior result in terms of SPICE.\nTherefore, all experiments in the main paper are carried out with\n40 memory vectors.\nEncoder and decoder layers.To complement the analysis pre-\nsented in Sec. 4.3, we also investigate the performance of the M2\n3https://spacy.io/\nMemories B-1 B-4 M R C S\nNo memory 80.4 38.3 29.0 58.2 129.4 22.6\n20 80.7 38.9 29.0 58.4 129.9 22.7\n40 80.8 39.1 29.2 58.6 131.2 22.6\n60 80.0 37.9 28.9 58.1 129.6 22.5\n80 80.0 38.2 29.0 58.3 128.9 22.9\nTable 6: Captioning results of M2 Transformer using dif-\nferent numbers of memory vectors.\nLayers B-1 B-4 M R C S\n2 80.5 38.6 29.0 58.4 128.5 22.8\n3 80.8 39.1 29.2 58.6 131.2 22.6\n4 80.8 38.6 29.1 58.5 129.6 22.6\nTable 7: Captioning results of M2 Transformer using dif-\nferent numbers of encoder and decoder layers.\nSPICE Obj. Attr. Rel. Color Count Size\nUp-Down [4] 21.4 39.1 10.0 6.5 11.4 18.4 3.2\nTransformer 21.1 38.6 9.6 6.3 9.2 17.5 2.0\nM2 Transformer 22.6 40.0 11.6 6.9 12.9 20.4 3.5\nTable 8: Breakdown of SPICE F-scores over various sub-\ncategories.\nTransformer when changing the number of encoding and decod-\ning layers. Table 7 shows that the best performance is obtained\nwith three encoding and decoding layers, thus conÔ¨Årming the ini-\ntial Ô¨Åndings on the base Transformer model. As our model can\ndeal with a different number of encoding and decoding layers,\nwe also experimented with non symmetric encoding-decoding ar-\nchitectures, without however noticing signiÔ¨Åcant improvements in\nperformance.\nSPICE F-scores. Finally, in Table 8 we report a breakdown of\nSPICE F-scores over various subcategories on the ‚ÄúKarpathy‚Äù test\nsplit, in comparison with the Up-Down approach [4] and the base\nTransformer model with three layers. As it can be seen, our model\nsigniÔ¨Åcantly improves on identifying objects, attributes and rela-\ntionships between objects.\nA.3. Qualitative results and visualization\nFigure 6 shows additional qualitative results obtained from our\nmodel in comparison to the original Transformer and correspond-\ning ground-truth captions. On average, the proposed model shows\nan improvement in terms of caption correctness and provides more\ndetailed and exhaustive descriptions.\nFigures 7 and 8, instead, report the visualization of attentive\nstates on a variety of sample images, following the approach out-\nlined in Sec. 4.6 of the main paper. SpeciÔ¨Åcally, the Integrated\nGradients approach [38] produces an attribution score for each fea-\nture channel of each input region. To obtain the attribution of each\nregion, we average over the feature channels, and re-normalize the\nobtained scores by their sum. For visualization purposes, we apply\na contrast stretching function to project scores in the 0-1 interval.\nConstraints: horse; cart.\nTransformer: A horse pulling a cart down a\nstreet.\nM2 Transformer: A white horse pulling a\nman in a cart.\nConstraints: bee; lavender.\nTransformer: A bee lavender of purple Ô¨Çow-\ners in a Ô¨Åeld.\nM2 Transformer: A Ô¨Åeld of lavender purple\nÔ¨Çowers with bee.\nConstraints: monkey.\nTransformer: A brown bear sitting on a rock\nmonkey.\nM2 Transformer: A small monkey sitting on\na rock in the grass.\nConstraints: Ô¨Çag.\nTransformer: A red kite with a Ô¨Çag in the sky.\nM2 Transformer: A red and white Ô¨Çag Ô¨Çying\nin the sky.\nConstraints: bookcase.\nTransformer: A woman holding a bookcase\nin a store.\nM2 Transformer: A woman holding a book\nin front of a bookcase.\nConstraints: rabbit.\nTransformer: A cat sitting on the rabbit with\na cell phone.\nM2 Transformer: A rabbit sitting on a table\nnext to a person.\nFigure 5: Sample nocaps images and corresponding predicted captions generated by our model and the original Transformer.\nFor each image, we report the Open Images object classes predicted by the object detector and used as constraints during the\ngeneration of the caption.\nA.4. Novel object captioning\nFigure 5 reports sample captions produced by our approach on\nimages from the nocaps dataset. On each image, we compare to\nthe baseline Transformer and show the constraints provided by\nthe object detector. Overall, the M2 Transformer is able to bet-\nter incorporate the constraints while maintaining the Ô¨Çuency and\nproperness of the generated sentences.\nFollowing [1], we use an object detector trained on Open Im-\nages 4 and Ô¨Ålter detections by removing 39 Open Images classes\nthat contain parts of objects or which are seldom mentioned. We\nalso discard overlapping detections by removing the higher-order\nof two objects based on the class hierarchy, and we use the top-3\ndetected objects as constraints based on the detection conÔ¨Ådence\nscore. As mentioned in Sec. 4.5 and differently from [1], we do not\nconsider the plural forms or other word phrases of object classes,\nthus taking into account only the original class names. After de-\ncoding, we select the predicted caption with highest probability\nthat satisÔ¨Åes the given constraints.\n4SpeciÔ¨Åcally, the tf faster rcnn inception resnet v2 atrous oidv2\nmodel from the TensorÔ¨Çow model zoo.\nGT: A man milking a brown and white cow in\nbarn.\nTransformer: A man is standing next to a\ncow.\nM2 Transformer: A man is milking a cow in\na barn.\nGT: A man in a red Santa hat and a dog pose\nin front of a Christmas tree.\nTransformer: A Christmas tree in the snow\nwith a Christmas tree.\nM2 Transformer: A man wearing a Santa hat\nwith a dog in front of a Christmas tree.\nGT: A woman with blue hair and a yellow um-\nbrella.\nTransformer: A woman is holding an um-\nbrella.\nM2 Transformer: A woman with blue hair\nholding a yellow umbrella.\nGT: Several people standing outside a parked\nwhite van.\nTransformer: A group of people standing out-\nside of a bus.\nM2 Transformer: A group of people stand-\ning around a white van.\nGT: Several zebras and other animals grazing\nin a Ô¨Åeld.\nTransformer: A herd of zebras are standing in\na Ô¨Åeld.\nM2 Transformer: A herd of zebras and other\nanimals grazing in a Ô¨Åeld.\nGT: A truck sitting on a Ô¨Åeld with kites in the\nair.\nTransformer: A group of cars parked in a Ô¨Åeld\nwith a kite.\nM2 Transformer: A white truck is parked in\na Ô¨Åeld with kites.\nGT: A woman who is skateboarding down the\nstreet.\nTransformer: A woman walking down a\nstreet talking on a cell phone.\nM2 Transformer: A woman standing on a\nskateboard on a street.\nGT: Orange cat walking across two red suit-\ncases stacked on Ô¨Çoor.\nTransformer: An orange cat sitting on top of\na suitcase.\nM2 Transformer: An orange cat standing on\ntop of two red suitcases.\nGT: Some people are standing in front of a red\nfood truck.\nTransformer: A group of people standing in\nfront of a bus.\nM2 Transformer: A group of people stand-\ning outside of a food truck.\nGT: A boat parked in a Ô¨Åeld with long green\ngrass.\nTransformer: A Ô¨Åeld of grass with a fence.\nM2 Transformer: A boat in the middle of a\nÔ¨Åeld of grass.\nGT: A little girl is eating a hot dog and riding\nin a shopping cart.\nTransformer: A little girl sitting on a bench\neating a hot dog.\nM2 Transformer: A little girl sitting in a\nshopping cart eating a hot dog.\nGT: A grilled sandwich sits on a cutting board\nby a knife.\nTransformer: A sandwich sitting on top of a\nwooden table.\nM2 Transformer: A sandwich on a cutting\nboard with a knife.\nGT: A hotel room with a well-made bed, a ta-\nble, and two chairs.\nTransformer: A bedroom with a bed and a ta-\nble.\nM2 Transformer: A hotel room with a large\nbed with white pillows.\nGT: An open toaster oven with a glass dish of\nfood inside.\nTransformer: An open suitcase with food in\nan oven.\nM2 Transformer: A toaster oven with a tray\nof food inside of it.\nGT: A empty bench on a snow covered beach.\nTransformer: Two benches sitting on a beach\nnear the water.\nM2 Transformer: A bench sitting on the\nbeach in the snow.\nGT: A brown and white dog wearing a red and\nwhite Santa hat.\nTransformer: A white dog wearing a red hat.\nM2 Transformer: A dog wearing a red and\nwhite Santa hat.\nGT: A man riding a small pink motorcycle on\na track.\nTransformer: A man is riding a red motorcy-\ncle.\nM2 Transformer: A man riding a pink mo-\ntorcycle on a track.\nGT: Three people sit on a bench looking out\nover the water.\nTransformer: Two people sitting on a bench\nin the water.\nM2 Transformer: Three people sitting on a\nbench looking at the water.\nFigure 6: Additional sample results generated by our approach and the original Transformer, as well as the corresponding\nground-truths.\nFigure 7: Visualization of attention states for sample captions generated by our M2 Transformer. For each generated word,\nwe show the attended image regions, outlining the region with the maximum output attribution in red.\nFigure 8: Visualization of attention states for sample captions generated by our M2 Transformer. For each generated word,\nwe show the attended image regions, outlining the region with the maximum output attribution in red.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9070191383361816
    },
    {
      "name": "Computer science",
      "score": 0.7842341661453247
    },
    {
      "name": "Transformer",
      "score": 0.7833014726638794
    },
    {
      "name": "Exploit",
      "score": 0.6224812865257263
    },
    {
      "name": "A priori and a posteriori",
      "score": 0.5625755190849304
    },
    {
      "name": "Decoding methods",
      "score": 0.5292890667915344
    },
    {
      "name": "Language model",
      "score": 0.49073874950408936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46507713198661804
    },
    {
      "name": "Natural language processing",
      "score": 0.4307807385921478
    },
    {
      "name": "Image (mathematics)",
      "score": 0.42617931962013245
    },
    {
      "name": "Machine translation",
      "score": 0.4169042110443115
    },
    {
      "name": "Computer engineering",
      "score": 0.35347920656204224
    },
    {
      "name": "Speech recognition",
      "score": 0.33211004734039307
    },
    {
      "name": "Algorithm",
      "score": 0.15208303928375244
    },
    {
      "name": "Voltage",
      "score": 0.13136202096939087
    },
    {
      "name": "Engineering",
      "score": 0.09005537629127502
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I122346577",
      "name": "University of Modena and Reggio Emilia",
      "country": "IT"
    }
  ]
}