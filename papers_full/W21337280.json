{
  "title": "Adaptive Language and Translation Models for Interactive Machine Translation",
  "url": "https://openalex.org/W21337280",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A5030035150",
      "name": "Laurent Nepveu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004268705",
      "name": "Guy Lapalme",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5034154991",
      "name": "Philippe Langlais",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5090694771",
      "name": "George Foster",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2048390999",
    "https://openalex.org/W2006969979",
    "https://openalex.org/W2164788644",
    "https://openalex.org/W1665921526",
    "https://openalex.org/W2141532438",
    "https://openalex.org/W2118441253",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2050242310",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W2023413808",
    "https://openalex.org/W158414620",
    "https://openalex.org/W1973923101",
    "https://openalex.org/W2169424935",
    "https://openalex.org/W2028492537"
  ],
  "abstract": "We describe experiments carried out with adaptive language and translation models in the context of an interactive computer-assisted translation program. We developed cache-based language models which were then extended to the bilingual case for a cache-based translation model. We present the improve-ments we obtained in two contexts: in a theoretical setting, we achieved a drop in perplexity for the new models and, in a more practical situation simulat-ing a user working with the system, we showed that fewer keystrokes would be needed to enter a trans-lation. 1",
  "full_text": "Publisher’s version  /   Version de l'éditeur: \nVous avez des questions? Nous pouvons vous aider. Pour communiquer directement avec un auteur, consultez la \npremière page de la revue dans laquelle son article a été publié afin de trouver ses coordonnées. Si vous n’arrivez \npas à les repérer, communiquez avec nous à PublicationsArchive-ArchivesPublications@nrc-cnrc.gc.ca.\nQuestions? Contact the NRC Publications Archive team at \nPublicationsArchive-ArchivesPublications@nrc-cnrc.gc.ca. If you wish to email the authors directly, please see the \nfirst page of the publication for their contact information. \nhttps://publications-cnrc.canada.ca/fra/droits\nL’accès à ce site Web et l’utilisation de son contenu sont assujettis aux conditions présentées dans le site\nLISEZ CES CONDITIONS ATTENTIVEMENT AVANT D’UTILISER CE SITE WEB.\n2004 Conference on Empirical Methods in Natural Language Processing (EMNLP \n04) [Proceedings], 2004\nREAD THESE TERMS AND CONDITIONS CAREFULLY BEFORE USING THIS WEBSITE. \nhttps://nrc-publications.canada.ca/eng/copyright\nNRC Publications Archive Record / Notice des Archives des publications du CNRC :\nhttps://nrc-publications.canada.ca/eng/view/object/?id=5e268f8f-1983-4d29-8a92-74ec8f519922\nhttps://publications-cnrc.canada.ca/fra/voir/objet/?id=5e268f8f-1983-4d29-8a92-74ec8f519922\nNRC Publications Archive\nArchives des publications du CNRC\nThis publication could be one of several versions: author’s original, accepted manuscript or the publisher’s version. / \nLa version de cette publication peut être l’une des suivantes : la version prépublication de l’auteur, la version \nacceptée du manuscrit ou la version de l’éditeur.\nAccess and use of this website and the material on it  are subject to the Terms and Conditions set forth at\nAdaptive Language and Translation Models for Interactive Machine \nTranslation\nNepveu, L.; Langlais, P.; Lapalme, G.; Foster, George\n\nNational Research\nCouncil Canada\nInstitute for\nInformation Technology\nConseil national\nde recherches Canada\nInstitut de technologie\nde l'information\n \n \n \n \n \n \n \nAdaptive Language and Translation Models \nfor Interactive Machine Translation * \n \nNepveu, L., Langlais, P., Lapalme, G., and Foster, G. \nJuly 2004 \n \n \n \n \n \n \n \n \n \n* published at the 2004 Conference on Empirical Methods in Natural \nLanguage Processing (EMNLP 04). July 25-26, 2004. Barcelona, Spain. \nNRC 48083.      \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nCopyright 2004 by \nNational Research Council of Canada \n \nPermission is granted to quote short excerpts and to reproduce figures and tables \nfrom this report, provided that the source of such material is fully acknowledged.\n \n \n \nAdaptive Language and Translation Models\nfor Interactive Machine Translation\nLaurent Nepveu, Guy Lapalme\nPhilippe Langlais\nRALI/DIRO - Universit´e de Montr´eal,\nC.P. 6128, succursale Centre-ville\nMontr´eal, Qu´ebec, Canada H3C 3J7\n{nepveul,lapalme,felipe}\n@iro.umontreal.ca\nGeorge Foster\nLanguage Technologies Research Centre\nNational Research Council Canada\nA-1330, 101 rue Saint-Jean Bosco,\nGatineau, Qu´ebec, Canada K1A 0R6\nGeorge.Foster@nrc-cnrc.gc.ca\nAbstract\nWe describe experiments carried out with adaptive\nlanguage and translation models in the context of an\ninteractive computer-assisted translation program.\nWe developed cache-based language models which\nwere then extended to the bilingual case for a cache-\nbased translation model. We present the improve-\nments we obtained in two contexts: in a theoretical\nsetting, we achieved a drop in perplexity for the new\nmodels and, in a more practical situation simulat-\ning a user working with the system, we showed that\nfewer keystrokes would be needed to enter a trans-\nlation.\n1 Introduction\nCache-based language models were introduced by\nKuhn and de Mori (1990) for the dynamic adap-\ntation of speech language models. These models,\ninspired by the memory caches on modern com-\nputer architectures, are motivated by the principle\nof locality which states that a program tends to re-\npeatedly use memory cells that are physically close.\nSimilarly, when speaking or writing, humans tend\nto use the same words and phrase constructs from\nparagraph to paragraph and from sentence to sen-\ntence. This leads us to believe that, when processing\na document, the part of a document that is already\nprocessed (e.g. for speech recognition, translation\nor text prediction) gives us very useful information\nfor future processing in the same document or in\nother related documents.\nA cache-based language model is a language\nmodel to which is added a smaller model trained\nonly on the history of the document being pro-\ncessed. The history is usually the lastN words or\nsentences seen in the document.\nKuhn and de Mori (1990) obtained a drop in per-\nplexity of nearly 68% when adding an unigram POS\n(part-of-speech) cache on a 3g-gram model. Martin\nand al. (1997) obtained a drop of nearly 21% when\nadding a bigram cache to a trigram model. Clarkson\nand Robertson (1997) also obtained similar results\nwith an exponentially decaying unigram cache.\nThe major problem with these theoretical results\nis that they assume the correctness of the material\nentering the cache. In practice, this assumption does\nnot always hold, and so a cache can sometimes do\nmore harm than good.\n1.1 Interactive translation context\nOver the last few years, an interactive machine\ntranslation (IMT) system (Foster et al., 2002) has\nbeen developed which, as the translator is typing,\nsuggests word and phrase completions that the user\ncan accept or ignore. The system uses a transla-\ntion engine to propose the words or phrases which\nit judges the most probable to be immediately typed.\nThis engine includes a translation model (TM) and\na language model (LM) used jointly to produce pro-\nposals that are appropriate translations of source\nwords and plausible completions of the current text\nin the target language. The translator remains in\ncontrol of the translation because what is typed by\nthe user is taken as a constraint to which the model\nmust continually adapt its completions. Experi-\nments have shown that the use of this system can\nsave about 50% of the keystrokes needed for enter-\ning a translation. As the translation and language\nmodels are built only once, before the user starts to\nwork with the system, the translator is often forced\nto repeatedly correct similar suggestions from the\nsystem.\nThe interactive nature of this setup made us be-\nlieve that it is a good prospect for dynamic adaptive\nmodeling. If the dynamic nature of the system can\nbe disadvantageous for static language and transla-\ntion models, it is an incomparable advantage for a\ncache based approach because human correction in-\ntervenesbeforewords go in the cache. As the trans-\nlator is using the system to correctly enter his trans-\nlation progressively, we can expect the theoretical\nresults presented in the literature to be obtainable in\npractice in the IMT context.\nThe ﬁrst advantage of dynamic adaptation would\nbe to help the translation engine make better predic-\ntions, but it has a furtherpsychologicaladvantage:\nas the translator works and potentially corrects the\nproposals of the engine, the user would feel that the\nsoftware is learning from its errors.\nThe next section describes the models currently\nembedded within our IMT prototype. Section 3 de-\nscribes the cache-based adaptation we performed on\nthe target language model. In section 4, we present\nthe different types of adaptations we performed on\nthe translation model. Section 5 then puts the results\nin the context of our IMT application. Section 6 dis-\ncusses the implications of our experiments and sug-\ngests some improvements that could be made to the\nsystem.\n2 Current IMT models\nThe word-based translation model embedded within\nthe IMT system has been designed by Foster (2000).\nIt is a Maximum Entropy/Minimum Divergence\n(MEMD) translation model (Berger et al., 1996),\nwhich mimics the parameters of the IBM model 2\n(Brown et al., 1993) within a log-linear setting.\nThe resulting model (named MDI2B) is of the\nfollowing form, whereh is the current target text,\ns the source sentence being translated,s a particular\nword ins and w the next word to be predicted:\np(w|h, s) = q(w|h) exp( ∑\ns∈s αsw + βAB)\nZ(h, s) (1)\nThe q distribution represents the prior knowledge\nthat we have about the true distribution and is mod-\neled by an interpolated trigram in this study. The\nα coefﬁcients are the familiar transfer or lexical pa-\nrameters, and theβ ones can be understood as their\nposition dependent correction.Z is a normalizing\nfactor, the sum of the numerator for everyw in the\ntarget vocabulary.\nOur baseline model used an interpolated trigram\nof the following form as theq distribution:\np(w|h) = λ1(wi−2wi−1) × ptri(wi|wi−2wi−1)\n+ λ2(wi−2wi−1) × pbi(wi|wi−1)\n+ λ3(wi−2wi−1) × puni(wi)\n+ λ4(wi−2wi−1) × 1\n|V |+1\nwhere λ1(wi−2wi−1) + λ2(wi−2wi−1) +\nλ3(wi−2wi−1) + λ4(wi−2wi−1) = 1 and |V | + 1\nis the size of the event space (including a special\nunknown word).\nAs mentioned above, the MDI2B model is closely\nrelated to the IBM2 model (Brown et al., 1988). It\ncontains two classes of features: word pair features\nand positional features. The word pair feature func-\ntions are deﬁned as follows:\nfst(w, h, s) =\n{\n1 if s∈ s and t = w\n0 otherwise\nThis function ison if the predicted word ist and s\nis in the current source sentence. Each featurefst\nhas a corresponding weightαst (for brevity, this is\ndeﬁned to be 0 in equation 1 if the pairs, t is not\nincluded in the model).\nThe positional feature functions are deﬁned as\nfollows:\nfA,B(w, i, s) =\nJ∑\nj=1\nδ[(i, j, J ) ∈ A ∧ (sj, w ) ∈ B ∧ j = ˆsj ]\nwhere δ[X] is 1 ifX is true, otherwise 0; andˆsj\nis the position of the occurrence ofsj that isclos-\nesttoi according to an IBM2 model.A is a class\nthat groups positional (i, j, J ) conﬁgurations having\nsimilar IBM2 alignment probabilities, in order to re-\nduce data sparseness.B is a class of word pairs\nhaving similar weightsαst. Its purpose is to simu-\nlate the way IBM2 alignment probabilities modulate\nIBM1 word-pair probabilities, by allowing the value\nof the positional feature weight to depend on the\nmagnitudeof the corresponding word-pair weight.\nAs with the word pair features, eachfA,B has a cor-\nresponding weightβAB.\nSince feature selection is applied at training time\nin order to improve speed, avoid overﬁtting, and\nkeep the model compact, the summation in the ex-\nponential term in (1) is only carried out over the set\nofactivepairs maintained by the model and not over\nall pairs as might be inferred from the formulation.\nTo give an example of how the model works, if\nthe source sentence isthe fruit I am eating is a ba-\nnana and we are predicting the wordbanane follow-\ning the target words:Le fruit que je mange est une,\nthe active pairs involvingbanana would be (fruit,\nbanana) and (banane,banana) since, of all the pairs\n(s, t ) they would be the only ones kept by the fea-\nture selection algorithm1. The probability ofbanane\nwould therefore depend on the weights of those two\npairs, along with position weights which capture the\nrelative proximity of the words involved.\n3 Language model adaptation\nWe implemented a ﬁrst monolingual dynamic adap-\ntation of this model by inserting a cache compo-\nnent in its reference distribution, thus only affect-\ning theq distribution. We obtained similar results\n1See (Foster, 2000) for the description of this algorithm.\nas for classical ngram models: the unigram cache\nmodel proved to be less efﬁcient than the bigram\none, and the trigram cache suffered from sparsity.\nWe also tested a model where we interpolated the\nthree cache models to gain information from each\nof the unigram, bigram, and trigram cache mod-\nels. For completeness, this generalized model is de-\nscribed in equation 2 under the usual constraints that∑\ni λi(h) = 1 for allh.\np(w|h) = λ1(h) × ptri(wi|wi−2wi−1)\n+ λ2(h) × pbi(wi|wi−1)\n+ λ3(h) × puni(wi)\n+ λ4(h) × 1\n|V |+1\n+ λ5(h) × ptric(wi|wi−2wi−1)\n+ λ6(h) × pbic(wi|wi−1)\n+ λ7(h) × punic(wi)\n(2)\nThose models were trained from splits of the\nCanadian Hansard corpus. The base ngram model\nwas estimated with a 30M word split of the corpus.\nThe weighting coefﬁcients of both the base trigram\nand the cache models were estimated with an EM\nalgorithm trained with 1M words.\nWe tested our models, translating from English\nto French, on two corpora of different types: the\nﬁrst onehansardis a document taken from the\nsame large corpus that was used for training (the\ntesting and training corpora were exclusive splits).\nThe second onesniper, which describes the job\nof a sniper, is from another domain characterized\nby lexical and phrasal constructions very different\nfrom those used to estimate the probabilities of our\nmodels.\nTable 1 shows the perplexity on thehansard\nand thesnipercorpora. Preliminary experiments\nled us to two sizes of cache which seemed promis-\ning: 2000 and 5000 corresponding to the last 2000\nand 5000 words seen during the processing of a doc-\nument. The BI column gives the results of the bi-\ngram cache model and the1+2+3 gives the results\nof the interpolated cache model which included the\nunigram, bigram and trigram cache.\nThe results show that our models improve the\nbase static model by 5% on documents supposedly\nwell knownby the models and by more that 52%\non documents that areunknown to the model. Sec-\ntion 5 puts these results in the perspective of our\nactual IMT system. Note that he addition of a cache\ncomponent to a language model involves negligible\nextra training time.\nTaille BI ∆ 1+2+3 ∆\nbase hansard=17.6584\n2000 16.937 -4.1% 16.840 -4.6%\n5000 16.903 -4.3% 16.777 -5.0%\nbase sniper=135.808\n2000 73.936 -45.6% 67.780 -50.1%\n5000 70.514 -48.1% 64.204 -52.7%\nTable 1: Perplexities of the MDI2B model with a\ncache component included in the reference distribu-\ntion on thehansardand snipercorpora.\n4 Translation model adaptation\nWith those excellent results in mind, we extended\nthe idea of dynamic adaptation to the bilingual case\nwhich, to our knowledge, has never been tried be-\nfore.\nWe developed a model called MDI2BCache\nwhich is a MDI2B model to which we added a cache\ncomponent based on word pairs. Recall that, when\npredicting a wordw at a certain point in a document,\nthe probability depends on the weights of the pairs\n(s, w ) for each active words in the current source\nsentence. As the prediction of the words of the doc-\nument goes on, our model keeps in a cache each\nactive pair used for the prediction of each word. In\nthe example above, if the translator accepts the word\nbanane, then the two pairs (fruit,banana) and (ba-\nnane,banana) will be added to the cache.\nWe added a new feature to the MEMD model to\ntake into account the presence of a certain pair in\nthe recent history of the processed document:\nfcache st(w, h, s) =\n\n\n\n1 if\n\n\n\ns ∈ s,\nt = w,\n(s, t ) ∈ cache\nαst > p\n0 otherwise\nWe added a threshold valuep to the feature func-\ntion because while analyzing the pair weights, we\ndiscovered that low weight pairs are usually pairs of\nutility words such as conjunctions and punctuation.\nWe also came to the conclusion that they are not the\nkind of words we want to have in the cache, since\ntheir presence in a sentence implies little about their\npresence in the next.\nThe resulting model is of the form:\np(w|h, s) = q(w|h)exp(∑\ns∈s αsw + βAB + γsw)\nZ(h, s)\nThus, everyfcache swhas a corresponding weight\nγsw for the calculation of the probability ofw.\nSize 0.3 ∆ 0.5 ∆ 0.7 ∆\nbase One feature weight, no Viterbi orig perp=17.6584\n1000 17.5676 -0.51% 17.5756 -0.47% 17.5983 -0.34%\n2000 17.5698 -0.50% 17.5766 -0.46% 17.5976 -0.34%\n5000 17.5743 -0.48% 17.5776 -0.46% 17.5965 -0.35%\n10000 17.5777 -0.46% 17.5791 -0.45% 17.5962 -0.35%\nbase One feature weight per pair, no Viterbi orig perp=17.6584\n1000 17.5817 -0.43% 17.5858 -0.41% 17.6065 -0.29%\n2000 17.5933 -0.37% 17.5918 -0.38% 17.6061 -0.30%\n5000 17.5849 -0.42% 17.5874 -0.40% 17.6076 -0.29%\n10000 17.5890 -0.39% 17.5891 -0.39% 17.6069 -0.29%\nbase One feature weight, Viterbi orig perp=17.6584\n1000 17.5602 -0.56% 17.5697 -0.50% 17.5940 -0.36%\n2000 17.5676 -0.51% 17.5695 -0.50% 17.5896 -0.39%\n5000 17.5614 -0.55% 17.5687 -0.51% 17.5925 -0.37%\n10000 17.5650 -0.53% 17.5687 -0.51% 17.5906 -0.38%\nTable 2: MDI2BCache test perplexities. One feature weight, Viterbi alignment version.\n4.1 Number of cache features\nWe implemented two versions of the model, one in\nwhich we estimated only one cache feature weight\nfor the whole model and another in which we esti-\nmated one cache feature weight for every word pair\nin the model.\nThe ﬁrst model is simpler and is easier to esti-\nmate. The assumption is made that every pair in the\nmodel has the same tendency to repeat itself.\nThe second model doubles the number of word-\npair parameters compared to MDI2B, and thus leads\nto a linear increase in training time. Extra training\ntime is negligible in the ﬁrst model.\n4.2 Word alignment\nOne of the main difﬁculties of automatic MT is de-\ntermining which source word(s) translate to which\ntarget word(s). It is very difﬁcult to do this task\nautomatically, in part because it is also very difﬁ-\ncult manually. If a pair of sentences are given to\n10 translators for alignment, the results would likely\nnot be identical in all cases. As it is nearly impossi-\nble to determine such an alignment, most translation\nmodels consider every source word to have an effect\non the translation of every target word.\nThis difﬁculty shows up in our cache-based\nmodel. When adding word pairs to the cache, we\nideally would like to add only word pairs that were\nreally in a translation relation in the given sentence.\nThis is why we also implemented a version of our\nmodel in which a word alignment is ﬁrst carried out\nin order to select good pairs to be added to the cache.\nFor this purpose, we computed a Viterbi alignment\nbased on an IBM model 2. This results in a subset of\nthegood active pairs to be added to the cache. The\nViterbi algorithm gives us a higher conﬁdence level\nthat the pair of words added to the cache were really\nin a translation relation. But it can also lead to word\npairs not added to the cache that should have been\nadded.\n4.3 Results\nTable 2 shows the results of the different conﬁgura-\ntions of the MDI2BCache model. For every conﬁg-\nuration we trained and tested on splits of the Cana-\ndian Hansard with threshold values of 0.3, 0.5, and\n0.7 and cache sizes of 1000, 2000, 5000, and 10000.\nThe top of the table is the version of the model with\nonly one feature weight without Viterbi alignment.\nThe middle of the table is the version with one fea-\nture weight per word pair without Viterbi alignment.\nFinally, the bottom is for the version with only one\nfeature weight and a Viterbi alignment made prior\nto adding pairs to the cache.\nThreshold values of 0.3, 0.5, and 0.7 led to 75%,\n50%, and 25% of the pairs considered for addition\nto the cache respectively. The results show that the\nthreshold values of 0.5 and 0.7 are removing too\nmany pairs. The best results are obtained with a\nthreshold of 0.3 in all tests. Since the number of\npairs kept in the model appears to vary in proportion\nto the threshold value, we did not consider it neces-\nsary to use an automatic search algorithm to ﬁnd an\noptimal threshold value. The gain in performance\nwould have been negligible.\nThe results also show that having one feature\nweight per word pair leads to lower results. This\ncan be explained by the fact that it is much more\nSize 0.3 ∆ 0.5 ∆\nbase MDI2B=135.808\n1000 132.865 -2.17% 132.751 -2.25%\n2000 132.771 -2.23% 132.752 -2.25%\n5000 132.733 -2.26% 132.628 -2.34%\n10000 132.997 -2.07% 132.674 -2.31%\nTable 3: MDI2BCache test perplexities. One fea-\nture weight, Viterbi alignment version. Sniper test\ndifﬁcult to estimate a weight for every pair that one\nweight for all pairs. Since we use only thousands of\nwords in the cache, the training process suffers from\na poor data representation.\nThe Viterbi alignment seems to be helping the\nmodels. The best results are obtained with the ver-\nsion of our model with Viterbi alignment. However,\nthis gives only a 0.56% percent drop in perplexity.\nWe then tested our best conﬁguration on the\nsnipercorpus. Table 3 shows the results. We\ndropped threshold value 0.7 and tested only the\nmodel with only one feature weight and a Viterbi\nalignment.\nResults show that our bilingual cache model\nshows improvement (four times higher) in drop of\nperplexity when used on documents very different\nfrom the training corpus. In general, results give\nlower perplexity than our base model showing that\nthe bilingual cache is helpful to the model, but the\nresults are not as good as that the ones obtained in\nthe unilingual case. Section 6 discusses these results\nfurther.\n5 Evaluation of IMT\nAs stated earlier, drops in perplexity are theoreti-\ncal results that have been obtained previously in the\ncase of unilingual dynamic adaptation but for which\na corresponding level of practical success was rarely\nattained because of the cache correctness problem.\nTo show that the interactive nature of our assisted-\ntranslation application can really beneﬁt from dy-\nnamic adaptation, we tested our models in a more\nrealistic translation context. This test consists of\nsimulating a translator using the IMT system as it\nproposes words and phrases and accepting, correct-\ning or rejecting the proposals by trying to reproduce\na given target translation (Foster et al., 2002). The\nmetric used is the percentage of keystrokes saved\nby the use of the system instead of having to type\ndirectly all the target text.\nFor these simulations, we used only a 10K word\nsplit of thehansardand of thesnipercor-\npus. The reason is that the IMT application poten-\nTaille BI ∆ 1+2+3 ∆\nbase hansard=27.435\n2000 27.784 +1.3% 27.719 +1.0%\n5000 27.837 +1.5% 27.821 +1.4%\nbase sniper=9.686\n2000 11.404 +15.1% 11.294 +14.2%\n5000 11.498 +15.8% 11.623 +16.7%\nTable 4: Saved keystrokes raises for the MDI2B\nmodel with cache component in the reference dis-\ntribution on thehansardand snipercorpora.\n0.3 ∆\nbase hansard=27.4358\n1000 27.557 +0.44%\n2000 27.531 +0.35%\n5000 27.488 +0.18%\n10000 27.468 +0.12%\nbase sniper=9.686\n1000 9.896 +2.17%\n2000 10.023 +3.48%\n5000 9.983 +3.07%\n10000 9.957 +2.80%\nTable 5: Saved keystrokes raises for the\nMDI2BCache model with only one feature\nweight and Viterbi alignment on thehansardand\nsnipercorpora.\ntially proposes new completions after every charac-\nter typed by the user. For a 10K word document, it\nneeds to search about 1 million times for high prob-\nability words and phrases. This leads to relatively\nlong simulation times, even though predictions are\nmade at real time speeds.\nTable 4 shows the results obtained with the\nMDI2B model to which we added a cache compo-\nnent for the reference interpolated trigram distribu-\ntion.\nWe can see that the saved keystroke percentages\nare proportional to the perplexity drops reported in\nsection 3. The use of our models raises the saved\nkeystrokes by nearly 1.5% in the case ofwell known\ndocuments and by nearly 17% in the case of very\ndifferent documents. These are very interesting re-\nsults for a potential professional use of TransType.\nTable 5 shows an increase in the number of saved\nkeystrokes: 0.44% on thehansardand 3.5% on\nthesnipercorpora. Once again, the results are\nnot as impressive as the ones obtained for the mono-\nlingual dynamic adaptation case.\n6 Discussion\nThe results presented in section 3 on language\nmodel adaptation conﬁrmed what had been reported\nin the literature: adding a cache component to a lan-\nguage model leads to a drop in perplexity. More-\nover, we were able to demonstrate that using a\ncache-based language model inside a translation\nmodel leads to better performance for the whole\ntranslation model. We obtained drops in perplexity\nof 5% on a corpus of the same type as the training\ncorpus and of 50% on a different one. These theo-\nretical results lead to very good practical results. We\nwere able to increase the saved keystroke percent-\nage by 1.5% on the similar corpus as the training\nand by nearly 17% on the different corpus. These\nresults conﬁrm our hypothesis that dynamic adapta-\ntion with cache-based language model can be useful\nin the context of IMT, particularly for new types of\ntexts.\nResults presented in section 4 on translation\nmodel adaptation show that our approach has led\nto drops in perplexity although not as high as we\nwould have hoped. To understand these disappoint-\ning results, we analyzed the content of the cache for\ndifferent conﬁgurations of our MDI2BCache model.\nbase 0.3 viterbi + 0.3\n(is,qu’) (to,aﬁn) (offence,crime)\n(.,sa) (was,a) (was,´et´e)\n(this,,) (UNK ,UNK ) (very,tr`es)\n(all,toutes) (piece,l´egislative) (today,aujourd’hui)\n(have,du) (this,ce) (jobs,emploi)\n(the,pour) (per,100) (concern,inqui´etude)\n(on,du) (that,soient) (skin,peau)\n(of,un) (,,,) (there,y)\n(we,nous) (?,il) (government,le)\n(the,du) (any,tout) (an,un)\n18 68 86\nTable 6: Cache sampling of different conﬁgurations\nof MDI2BCache model.\nTable 6 shows the results of our sampling. We\ntested three model conﬁgurations. The ﬁrst one, in\nthe ﬁrst column, was the base MDI2BCache model\nwhich adds all active pairs to the cache. The second\nconﬁguration, in the second column, was a thresh-\nold value of 0.3 that brings about 75% of the pairs\nbeing added to the cache. The last conﬁguration was\na model with threshold value of 0.3 and a Viterbi\nalignment made prior to the addition of pairs in the\ncache. The three model conﬁguration were with\nonly one feature weight. For all three conﬁgura-\ntions, we took a sample of 10 pairs (shown in table\n6) and a sample of 100 pairs. With the second sam-\nple, we manually analyzed each pair and counted\nthe number of pairs (shown in the last row of the ta-\nble) we believed were useful for the model (words\nthat are occasionally translations of one another).\nThe results obtained in section 4 seem to agree\nwith the current analysis. From left to right in the ta-\nble, the pairs seem to contain more information and\nto be more appropriate additions to the cache. The\nconﬁguration with Viterbi alignment which contains\n86 good pairs clearly seems to be the conﬁguration\nwith the most interesting pairs.\nThe problem with such a cache-based translation\nmodel seem to be similar to the balance between\nprecision and recall in information retrieval. On one\nhand, we want to add in the cache every word pair\nin which the two words are in translation relation in\nthe text. We further want to addonlythe pairs in\nwhich the two words are really in translation rela-\ntion in the text. It seems that with our base model,\nwe add most of the good pairs, but also a lot of bad\nones. With the Viterbi alignment and a threshold\nvalue of 0.3, most of the pairs added are good ones,\nbut we are probably missing a number of other ap-\npropriate ones. This comes back to the task of word\nalignment, which is a very difﬁcult task for comput-\ners (Mihalcea and Pedersen, 2003).\nMoreover, we would want to add in the cache\nonly those words for which more than one transla-\ntion is possible. For example, the pair (today,au-\njourd’hui), though it is a very useful pair for the\nbase model, is unlikely to help when added to the\ncache. The reason is simple: they are two words\nthat are always translations of one another, so the\nmodel will have no problem predicting them. This\nideal of precision and recall and of useful pairs in\nthe cache is obtained by our model with threshold\nof 0.3, a Viterbi alignment and a cache size of 1000.\nOne disadvantage of our bilingual adaptive model\nis the way it handles unknown words. In the cache-\nbased language model, the unknown words were\ndealt withnormally, i.e. they were added to the\ncache and given a certain probability afterwards.\nSo, if an unknown word was seen in a certain sen-\ntence and then later on, it would receive a proba-\nbility mass of its own but not the one given to any\nunknown word. By having its own probability mass\ndue to its presence in the cache, such previously un-\nknown word can be predicted by the model. In the\ncase of our MDI2BCache model, because we have\nnot yet implemented an algorithm for guessing the\ntranslations of unknown words, they are simply rep-\nresented within the model as UNK words, which\nmeans that the model never learns them.\nThe results obtained with thesnipercorpus\nshows us that dynamic adaptation is also more help-\nful for documents that are littleknown to the model\nin the bilingual context. The results are four times\nbetter on thesnipercorpus than on the Hansard\ntesting corpus.\nOnce again for the bilingual case, the practical\ntest results in the number of saved keystrokes agree\nwith the theoretical results of drops in perplexity.\nThis result shows that bilingual dynamic adaptation\nalso can be implemented in a practical context and\nobtain results similar to the theoretical results.\nAll things considered, we believe that a cache-\nbased translation model shows a great potential\nfor bilingual adaptation and that greater perplexity\ndrops and keystroke savings could be obtained by\neither reengineering the model or by improving the\nMDI2BCache model.\n6.1 Key improvements to the model\nFollowing the analysis of the results obtained by our\nmodel, we have pointed out some key improvements\nthat the model would need in order to get better re-\nsults. In this list we focus on ways of improving\nadaptation strategies for the current model, omitting\nother obvious enhancements such as adding phrase\ntranslations.\nUnknown word processing Learning new words\nwould be a very important feature to add to\nthe model and would lead to better results. We\ndid not incorporate the processing of unknown\nwords in the MDI2BCache because the struc-\nture of model did not lend itself to this addi-\ntion. Especially with documents such as the\nsnipercorpus, we believe that this could\nbe a key improvement for a dynamic adaptive\nmodel.\nBetter alignmentAs mentioned before, the ulti-\nmate goal for our cache is that it contains only\nthe pairs present in theperfectalignment. Bet-\nter performance from the alignment would lead\nto pairs in the cache closer to this ideal. In this\nstudy we computed Viterbi alignments from an\nIBM model 2, because it is very efﬁcient to\ncompute and also because for training MDI2B,\nwe do use the IBM model 2. We could consider\nalso more advanced word alignment models\n(Och and Ney, 2000; Lin and Cherry, 2003;\nMoore, 2001). To keep the alignment model\nsimple, we could still use an IBM model 2, but\nwith the compositionality constraint that has\nbeen shown to give better word alignment than\nthe Viterbi one (Simard and Langlais, 2003).\nFeature weightsWe implemented two versions of\nour model: one with only one feature weight\nand another with one feature weight for each\nword pair. The second model suffered from\npoor data representation and our training algo-\nrithm wasn’t able to estimate good cache fea-\nture weights. We think that creating classes\nof word pairs, such as it was done for posi-\ntional alignment features, would lead to better\nresults. It would enable the model to take into\naccount the tendency that a pair has to repeat\nitself in a document.\nRelative weightingAnother key improvement is\nthat changes to word-pair weights should be\nrelative to each source word. For example,\nif (house, maison) is a pair in the cache, we\nwould like to favourmaison over possible al-\nternatives such aschambre as a translation of\nhouse. In the existing model this is done by\nboosting the weight on (house,maison), which\nhas the undesirable side-effect of makingmai-\nson more important in the model than transla-\ntions of other source words in the current sen-\ntence which have not appeared in the cache.\nOne way of eliminating this behaviour would\nbe to learn negative weights on alternatives like\n(house,chambre) which do not appear in the\ncache.\nWe believe these improvements would better show\nthe potential of bilingual dynamic adaptation.\n7 Conclusion\nWe have presented dynamic adaptive translation\nmodels using cache-based implementations. We\nhave shown that monolingual dynamic adaptive\nmodels exhibit good theoretical performance in a\nbilingual translation context. We observed that\nthese theoretical results carry over to practical gains\nin the context of an IMT application.\nWe have developed bilingual dynamic adaptation\nthrough a cache-based translation model. Our re-\nsults show the potential of bilingual dynamic adap-\ntation. We have given explanations about why the\nresults obtained are not as high as hoped and pre-\nsented some key improvements that should be made\nto our model or should be taken into account in the\ndevelopment of a new model.\nWe believe that this study reveals the potential for\nadaptive interactive machine translation system and\nwe hope to read similar reports for other implemen-\ntations of the same interactive scenarioe.g.(Och et\nal., 2003).\nReferences\nAdam L. Berger, Stephen A. Della Pietra, and Vin-\ncent J. Della Pietra. 1996. A Maximum Entropy\napproach to Natural Language Processing.Com-\nputational Linguistics, 22(1):39–71.\nPeter F. Brown, John Cocke, Stephen A. Della\nPietra, Vincent J. Della Pietra, Fredrick Jelinek,\nRobert L. Mercer, and Paul Roossin. 1988. A\nstatistical approach to language translation. In\nProceedings of the International Conference on\nComputational Linguistics (COLING), pages 71–\n76, Budapest, Hungary, August.\nPeter F. Brown, Stephen A. Della Pietra, Vincent\nDella J. Pietra, and Robert L. Mercer. 1993.\nThe mathematics of Machine Translation: Pa-\nrameter estimation.Computational Linguistics,\n19(2):263–312, June.\nP.R. Clarkson and P.R. Robertson. 1997. Language\nmodel adaptation using mixtures and an expo-\nnentially decaying cache. InIEEE Int. Confer-\nence on Acoustics, Speech, and Signal Process-\ning, Munich.\nGeorge Foster, Philippe Langlais, and Guy La-\npalme. 2002. User-friendly text prediction\nfor translators. In2002 Conference on Em-\npirical Methods in Natural Language Process-\ning (EMNLP 2002) , Philadelphia, July. TT2\nTransType2.\nGeorge Foster. 2000. A Maximum Entropy / Mini-\nmum Divergence translation model. InProceed-\nings of the 38th Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL), pages\n37–42, Hong Kong, October.\nRoland Kuhn and Renato De Mori. 1990. A cache-\nbased natural language model for speech recog-\nnition.IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence (PAMI), 12(6):570–\n583, June.\nDekang Lin and Colin Cherry. 2003. Proalign:\nShared task system description. InNAACL 2003\nWorkshop on Building and Using Parallel Texts:\nData Driven Machine Translation and Beyond,\npages 11–14, Edmonton Canada, May 31. TT2.\nS.C. Martin, J. Liermann, and H. Ney. 1997. Adap-\ntative topic-dependent language modelling using\nword-based varigrams. InEurospeech.\nRada Mihalcea and Ted Pedersen. 2003. An evalua-\ntion exercise for word alignment. In Rada Mihal-\ncea and Ted Pedersen, editors,HLT-NAACL 2003\nWorkshop: Building and Using Parallel Texts:\nData Driven Machine Translation and Beyond,\npages 1–10, Edmonton, Alberta, Canada, May\n31. Association for Computational Linguistics.\nRobert C. Moore. 2001. Towards a simple and\naccurate statistical approach to learning transla-\ntion relationships among words. InWorkshop on\nData-driven Machine Translation, 39th Annual\nMeeting and 10th Conference of the European\nChapter, pages 79–86, Toulouse, France. Asso-\nciation for Computational Linguistics.\nFranz Josef Och and Hermann Ney. 2000. Im-\nproved statistical alignment models. InProceed-\nings of the 38th Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL), pages\n440–447, Hong Kong, October.\nF.J. Och, R. Zens, and H. Ney. 2003. Efﬁcient\nsearch for interactive statistical machine trans-\nlation. InProceedings of the 10th Conference\nof the European Chapter of the Association for\nComputational Linguistics (EACL), pages 387–\n393, Budapest, Hungary, April. TT2.\nMichel Simard and Philippe Langlais. 2003. Statis-\ntical translation alignment with compositionality\nconstraints. InNAACL 2003 Workshop on Build-\ning and Using Parallel Texts: Data Driven Ma-\nchine Translation and Beyond, pages 19–22, Ed-\nmonton Canada, May 31. TT2.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.870674729347229
    },
    {
      "name": "Perplexity",
      "score": 0.8702653646469116
    },
    {
      "name": "Machine translation",
      "score": 0.7714060544967651
    },
    {
      "name": "Translation (biology)",
      "score": 0.6412882804870605
    },
    {
      "name": "Natural language processing",
      "score": 0.601526141166687
    },
    {
      "name": "Transfer-based machine translation",
      "score": 0.588040828704834
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5779823064804077
    },
    {
      "name": "Machine translation software usability",
      "score": 0.5774948596954346
    },
    {
      "name": "Example-based machine translation",
      "score": 0.534092128276825
    },
    {
      "name": "Computer-assisted translation",
      "score": 0.5069311261177063
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4969046413898468
    },
    {
      "name": "Language model",
      "score": 0.466978520154953
    },
    {
      "name": "Dynamic and formal equivalence",
      "score": 0.4429143965244293
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": []
}