{
  "title": "Improving the robustness and accuracy of biomedical language models through adversarial training",
  "url": "https://openalex.org/W3212212549",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5000171711",
      "name": "Milad Moradi",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A5082184364",
      "name": "Matthias Samwald",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2561981131",
    "https://openalex.org/W2963923670",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2977683229",
    "https://openalex.org/W3028438438",
    "https://openalex.org/W6785110541",
    "https://openalex.org/W6761496550",
    "https://openalex.org/W6783248943",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1945616565",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2998277219",
    "https://openalex.org/W2618219509",
    "https://openalex.org/W6749284044",
    "https://openalex.org/W6766165726",
    "https://openalex.org/W6742823662",
    "https://openalex.org/W6748110298",
    "https://openalex.org/W2905526464",
    "https://openalex.org/W2125825154",
    "https://openalex.org/W3020242586",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W2556096924",
    "https://openalex.org/W2963083752",
    "https://openalex.org/W2787487383",
    "https://openalex.org/W6784733005",
    "https://openalex.org/W2110279753",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2889272240",
    "https://openalex.org/W2582630951",
    "https://openalex.org/W3115265663",
    "https://openalex.org/W2906230340",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W6751205706",
    "https://openalex.org/W6776277508",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W3087231533",
    "https://openalex.org/W6781389535",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W3198690080",
    "https://openalex.org/W3196608201",
    "https://openalex.org/W3021046385",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W2959587146",
    "https://openalex.org/W2963249435",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W3175982906",
    "https://openalex.org/W2963639288",
    "https://openalex.org/W2792942633",
    "https://openalex.org/W2950606982",
    "https://openalex.org/W2888137286",
    "https://openalex.org/W3169948074",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W3015001695",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2962849096",
    "https://openalex.org/W2964159373",
    "https://openalex.org/W3100452049",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W3098724294",
    "https://openalex.org/W2811010710",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2965595599",
    "https://openalex.org/W3093419064",
    "https://openalex.org/W2951674308",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W2963783970",
    "https://openalex.org/W2888491130",
    "https://openalex.org/W3104570147",
    "https://openalex.org/W2744095836",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2940009958",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W3105604018",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3128686514",
    "https://openalex.org/W2798966449",
    "https://openalex.org/W4288953700"
  ],
  "abstract": "Deep transformer neural network models have improved the predictive accuracy of intelligent text processing systems in the biomedical domain. They have obtained state-of-the-art performance scores on a wide variety of biomedical and clinical Natural Language Processing (NLP) benchmarks. However, the robustness and reliability of these models has been less explored so far. Neural NLP models can be easily fooled by adversarial samples, i.e. minor changes to input that preserve the meaning and understandability of the text but force the NLP system to make erroneous decisions. This raises serious concerns about the security and trust-worthiness of biomedical NLP systems, especially when they are intended to be deployed in real-world use cases. We investigated the robustness of several transformer neural language models, i.e. BioBERT, SciBERT, BioMed-RoBERTa, and Bio-ClinicalBERT, on a wide range of biomedical and clinical text processing tasks. We implemented various adversarial attack methods to test the NLP systems in different attack scenarios. Experimental results showed that the biomedical NLP models are sensitive to adversarial samples; their performance dropped in average by 21 and 18.9 absolute percent on character-level and word-level adversarial noise, respectively, on Micro-F1, Pearson Correlation, and Accuracy measures. Conducting extensive adversarial training experiments, we fine-tuned the NLP models on a mixture of clean samples and adversarial inputs. Results showed that adversarial training is an effective defense mechanism against adversarial noise; the models' robustness improved in average by 11.3 absolute percent. In addition, the models' performance on clean data increased in average by 2.4 absolute percent, demonstrating that adversarial training can boost generalization abilities of biomedical NLP systems. This study takes an important step towards revealing vulnerabilities of deep neural language models in biomedical NLP applications. It also provides practical and effective strategies to develop secure, trust-worthy, and accurate intelligent text processing systems in the biomedical domain.",
  "full_text": null,
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.8427673578262329
    },
    {
      "name": "Computer science",
      "score": 0.7833468914031982
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7695893049240112
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7022938132286072
    },
    {
      "name": "Machine learning",
      "score": 0.566567063331604
    },
    {
      "name": "Artificial neural network",
      "score": 0.5647406578063965
    },
    {
      "name": "Language model",
      "score": 0.5532549619674683
    },
    {
      "name": "Transformer",
      "score": 0.5471895337104797
    },
    {
      "name": "Natural language processing",
      "score": 0.5175915956497192
    },
    {
      "name": "Deep learning",
      "score": 0.45768821239471436
    },
    {
      "name": "Deep neural networks",
      "score": 0.43946072459220886
    },
    {
      "name": "Engineering",
      "score": 0.07926222681999207
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}