{
    "title": "ST-YOLOA: a Swin-transformer-based YOLO model with an attention mechanism for SAR ship detection under complex background",
    "url": "https://openalex.org/W4379231500",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1903245706",
            "name": "Kai Zhao",
            "affiliations": [
                "PLA Rocket Force University of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A2145052468",
            "name": "Ruitao Lü",
            "affiliations": [
                "PLA Rocket Force University of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A2108172520",
            "name": "Siyu Wang",
            "affiliations": [
                "PLA Rocket Force University of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A2108089435",
            "name": "Xiaogang Yang",
            "affiliations": [
                "PLA Rocket Force University of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A2536185421",
            "name": "Qing-Ge Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2122770842",
            "name": "Jiwei Fan",
            "affiliations": [
                "PLA Rocket Force University of Engineering"
            ]
        },
        {
            "id": "https://openalex.org/A1903245706",
            "name": "Kai Zhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2145052468",
            "name": "Ruitao Lü",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108172520",
            "name": "Siyu Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108089435",
            "name": "Xiaogang Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2122770842",
            "name": "Jiwei Fan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2955454611",
        "https://openalex.org/W3018757597",
        "https://openalex.org/W2501210789",
        "https://openalex.org/W2989604896",
        "https://openalex.org/W2031489346",
        "https://openalex.org/W2796338532",
        "https://openalex.org/W4289846916",
        "https://openalex.org/W6798838024",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W3212386989",
        "https://openalex.org/W4403243978",
        "https://openalex.org/W6791472037",
        "https://openalex.org/W6743731764",
        "https://openalex.org/W4312582739",
        "https://openalex.org/W4283809070",
        "https://openalex.org/W2565639579",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W4296280868",
        "https://openalex.org/W6749783731",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3111766820",
        "https://openalex.org/W3092535672",
        "https://openalex.org/W2079299474",
        "https://openalex.org/W2963037989",
        "https://openalex.org/W639708223",
        "https://openalex.org/W2144158572",
        "https://openalex.org/W2089588733",
        "https://openalex.org/W6770600958",
        "https://openalex.org/W6760947256",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2588505606",
        "https://openalex.org/W6771154167",
        "https://openalex.org/W4320487943",
        "https://openalex.org/W3211592992",
        "https://openalex.org/W2805650139",
        "https://openalex.org/W4313291231",
        "https://openalex.org/W4221044012",
        "https://openalex.org/W3209823299",
        "https://openalex.org/W2765407302",
        "https://openalex.org/W6770992763",
        "https://openalex.org/W3122173535",
        "https://openalex.org/W3042011474",
        "https://openalex.org/W2884561390",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W2963857746",
        "https://openalex.org/W3177052299",
        "https://openalex.org/W3184439416",
        "https://openalex.org/W3035396860",
        "https://openalex.org/W2982770724",
        "https://openalex.org/W3034971973",
        "https://openalex.org/W4385245566"
    ],
    "abstract": "A synthetic aperture radar (SAR) image is crucial for ship detection in computer vision. Due to the background clutter, pose variations, and scale changes, it is a challenge to construct a SAR ship detection model with low false-alarm rates and high accuracy. Therefore, this paper proposes a novel SAR ship detection model called ST-YOLOA. First, the Swin Transformer network architecture and coordinate attention (CA) model are embedded in the STCNet backbone network to enhance the feature extraction performance and capture global information. Second, we used the PANet path aggregation network with a residual structure to construct the feature pyramid to increase global feature extraction capability. Next, to cope with the local interference and semantic information loss problems, a novel up/down-sampling method is proposed. Finally, the decoupled detection head is used to achieve the predicted output of the target position and the boundary box to improve convergence speed and detection accuracy. To demonstrate the efficiency of the proposed method, we have constructed three SAR ship detection datasets: a norm test set (NTS), a complex test set (CTS), and a merged test set (MTS). The experimental results show that our ST-YOLOA achieved an accuracy of 97.37%, 75.69%, and 88.50% on the three datasets, respectively, superior to the effects of other state-of-the-art methods. Our ST-YOLOA performs favorably in complex scenarios, and the accuracy is 4.83% higher than YOLOX on the CTS. Moreover, ST-YOLOA achieves real-time detection with a speed of 21.4 FPS.",
    "full_text": "TYPE Original Research\nPUBLISHED /zero.tnum/two.tnum June /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nOPEN ACCESS\nEDITED BY\nFeihu Zhang,\nNorthwestern Polytechnical University, China\nREVIEWED BY\nZhaoying Liu,\nBeijing University of Technology, China\nYang Yang,\nYunnan Normal University, China\nWen Xin,\nNorthwestern Polytechnical University, China\n*CORRESPONDENCE\nRuitao Lu\nlrt/one.tnum/nine.tnum/eight.tnum/eight.tnum/zero.tnum/two.tnum/two.tnum/zero.tnum@/one.tnum/six.tnum/three.tnum.com\nRECEIVED /two.tnum/zero.tnum February /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /one.tnum/eight.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /zero.tnum/two.tnum June /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nZhao K, Lu R, Wang S, Yang X, Li Q and Fan J\n(/two.tnum/zero.tnum/two.tnum/three.tnum) ST-YOLOA: a Swin-transformer-based\nYOLO model with an attention mechanism for\nSAR ship detection under complex background.\nFront. Neurorobot./one.tnum/seven.tnum:/one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Zhao, Lu, Wang, Yang, Li and Fan. This\nis an open-access article distributed under the\nterms of the\nCreative Commons Attribution\nLicense (CC BY) . The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic practice.\nNo use, distribution or reproduction is\npermitted which does not comply with these\nterms.\nST-YOLOA: a\nSwin-transformer-based YOLO\nmodel with an attention\nmechanism for SAR ship detection\nunder complex background\nKai Zhao, Ruitao Lu *, Siyu Wang, Xiaogang Yang, Qingge Li and\nJiwei Fan\nDepartment of Automation, Rocket Force University of Engineering, Xi’an, China\nA synthetic aperture radar (SAR) image is crucial for ship detecti on in computer\nvision. Due to the background clutter, pose variations, and scale changes, it is a\nchallenge to construct a SAR ship detection model with low false-alarm rates and\nhigh accuracy. Therefore, this paper proposes a novel SAR ship de tection model\ncalled ST-YOLOA. First, the Swin Transformer network architecture and coordinate\nattention (CA) model are embedded in the STCNet backbone network to enhance\nthe feature extraction performance and capture global informa tion. Second, we\nused the PANet path aggregation network with a residual structu re to construct\nthe feature pyramid to increase global feature extraction capabi lity. Next, to cope\nwith the local interference and semantic information loss prob lems, a novel\nup/down-sampling method is proposed. Finally, the decoupled detection head\nis used to achieve the predicted output of the target position and the boundary\nbox to improve convergence speed and detection accuracy. To demonstr ate the\neﬃciency of the proposed method, we have constructed three SAR ship detection\ndatasets: a norm test set (NTS), a complex test set (CTS), and a mer ged test set\n(MTS). The experimental results show that our ST-YOLOA achieved an accuracy\nof /nine.tnum/seven.tnum./three.tnum/seven.tnum%, /seven.tnum/five.tnum./six.tnum/nine.tnum%, and /eight.tnum/eight.tnum./five.tnum/zero.tnum% on the three datasets, respectively, superior to the\neﬀects of other state-of-the-art methods. Our ST-YOLOA perfo rms favorably in\ncomplex scenarios, and the accuracy is /four.tnum./eight.tnum/three.tnum% higher than YOLOX on the CTS.\nMoreover, ST-YOLOA achieves real-time detection with a speed of /two.tnum/one.tnum./four.tnum FPS.\nKEYWORDS\nsynthetic aperture radar (SAR) image, ship detection, Swin Trans former, YOLO, attention\nmechanism\n/one.tnum. Introduction\nSAR imaging has a high resolution, a long detection range, and a strong anti-interference\nability. It has a wide range of applications and development possibilities and is employed\nextensively in both civil and military ﬁelds, including surveying, mapping, catastrophe\nmonitoring, marine observation, and military reconnaissance (\nCumming and Wong, 2005 ;\nMoreira et al., 2013 ). Research on ship detection technology, such as precise terminal\nguidance, operational eﬀectiveness assessment, and identiﬁcation of ship targets in strategic\nport areas, is crucial for both military and civilian applications. Although signiﬁcant progress\nhas been made in the past few decades, ship detection in SAR images remains a challenge due\nto shape deformation, pose variation, and background clutter.\nFrontiers in Neurorobotics /zero.tnum/one.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nTraditional ship detection methods employ machine learning\nmodels to distinguish the ship target from the background in SAR\nimages. These methods usually include two primary processes:\nobject detection and target identiﬁcation (\nLu et al., 2020a ,b). The\nmost popular models employed in the classic detection approach\nare the constant false-alarm rate (CFAR) (\nRobey et al., 1992 ) and\nits variant algorithms. These methods detect ships by creating\na statistical distribution model of the background clutter. By\nemploying a decomposition strategy,\nGao et al. (2018) suggested\na CFAR algorithm based on a generalized gamma distribution\nto enhance the signal-to-noise ratio of SAR images.\nWang et al.\n(2017) pro-posed a constant false-alarm detector in the intensity\nspace domain, which uses data correlation to detect targets and\nwake pixels. To best suit the information provided by the ship\ndistribution map (\nSchwegmann et al., 2015 ) proposed a method\nfor transforming a scalar threshold into a threshold manifold using\nthe simulated annealing (SA) process. However, the traditional SAR\nship detection model relies on artiﬁcial design feature selection,\nwhich leads to poor detection robustness and generalization ability.\nIn addition, this kind of algorithm requires a high contrast between\nthe target image and the background image and is not suitable for\ndetecting ship targets in complex environments.\nWith the development of deep learning theory, convolutional\nneural networks have made signiﬁcant advancements in the\nﬁeld of target recognition and display advanced performance\nin target detection. The deep learning detection algorithms can\nbe roughly classiﬁed into two categories: one-stage methods\nand two-stage methods (\nGirshick et al., 2014 ). Two-stage object\ndetection methods perform region generation to obtain pre-\nselected boxes and then use sample classiﬁcation and regression\nwith border positioning through the convolutional neural network.\nRepresentative methods include R-CNN (\nGirshick et al., 2014 ), Fast\nR-CNN ( Girshick, 2015 ), and Faster R-CNN ( Ren et al., 2015 ).\nThese methods have high detection accuracy but low detection\neﬃciency. One-stage object detection methods use the backbone\nfeature extraction network to directly locate and classify the target.\nTypical detection methods are YOLO (\nRedmon et al., 2016 ), SSD\n(Liu et al., 2016 ), and CenterNet ( Duan et al., 2019 ). Although\nthese methods are fast, they are prone to false detection and missing\ndetection compared with the two-stage detection methods.\nBased on the current deep-learning-based target detection,\nin recent years, numerous researchers have developed advanced\nalgorithms for SAR ship detection. To address the challenges\nof ship detection in maritime environments,\nGao et al. (2022)\nproposed an enhanced YOLOv5 SAR ship detection method based\non target augmentation. By constructing the feature enhancement\nSwin converter (FESwin) module and the adjacent feature fusion\n(AFF) module (\nLi et al., 2022 ), proposed a detection model suitable\nfor the strong scattering, multi-scale, and complicated background\nof ship objects in SAR images. To provide a visual converter system\nbased on context-federated representation learning appropriate for\nSAR ship detection (\nXia et al., 2022 ), creatively combined CNNs\nwith transformers. Although the aforementioned methods address\nthe issues of multi-scale targets, huge noise clutter, and complicated\nbackgrounds, the detection accuracy and the calculation speed still\nrestrict application in the real world. Small islands and nearby\nsea structures are the reasons for false detection in com-plex\nbackgrounds. In addition, the dense distribution of ships in the\ndock and the sea causes multiple targets to overlap, leading to the\nlow accuracy of models in detecting targets.\nBased on the aforementioned analysis, in this paper, we have\nproposed a novel ship detection model called ST-YOLOA, which\nis more suitable for the actual complex environment in SAR\nimages. We chose Swin Transformer (\nLiu et al., 2021 ) and YOLOX\n(Ge et al., 2021 ) as our basic models. The main contributions\nof this paper are as follows: (1) Together data on ship target\nfeatures, we have proposed the STCNet backbone network. This\nnetwork eﬀectively solves the problem of insuﬃcient feature\nextraction caused by strong scattering in SAR images. It enhances\nthe processing ability of feature information by obtaining more\nsigniﬁcant feature information in diﬀerent environments. It also\nhas excellent global information modeling capabilities of Swin\nTransformer. (2) We have built a novel feature pyramid network\nbased on an enhanced PANet for profoundly fusing high-level\nand low-level features. This network solves the issues of local\ninformation interference and attention diversion by using semantic\nand localization information. To improve the detection accuracy,\nwe have adopted binary trilinear interpolation up-sampling for\nmaintaining the original data of the feature map. (3) To eﬀectively\nreduce the impact of noise in the feature map on the detection\naccuracy, classiﬁcation and regression are handled separately. We\nhave used EIOU as the localization loss function to cope with the\nsample imbalance and enhance the model generalization ability.\nThe rest of the paper is organized as follows. In Section 2,\nwe review the prior work related to the proposed ST-YOLOA.\nIn Section 3, we provide details of the main components and\nmethodology of the proposed ST-YOLOA. Section 4 introduces the\nexperimental settings, results, and analysis. The conclusions of the\npaper are drawn in Section 6.\n/two.tnum. Related work\nIn this section, we brieﬂy review the relevant literature\nregarding YOLO, Transformer, and the attention mechanism.\n/two.tnum./one.tnum. YOLO\nThe YOLO series is a typical network for one-stage detection. It\nhandles object detection as a regression issue, where the bounding\nbox coordinates and class probability are derived from picture\npixels. YOLOX, as a typical representative of the YOLO series,\nhas signiﬁcant advantages in terms of speed and accuracy. Its\nessential modules include the Focus, the CSP bottleneck, SPP ,\nPANet (\nLiu et al., 2018 ), and the decoupled detection head ( Tian\net al., 2019 ). The backbone of YOLOX is the CSP Darknet-\n53 ( Bochkovskiy et al., 2020 ), which consists of several residual\nmodules stacked one on top of the other. YOLOX uses PANet as\nthe neck of the model, and the input is the three feature output\nlayers output by the backbone network. It obtains features with\nricher semantic and localization information by feature fusion\nand sends them to the head for detection. At the head, YOLOX\nreplaces the coupled detection head with the decoupled detection\nFrontiers in Neurorobotics /zero.tnum/two.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nhead using diﬀerent branches for the classiﬁcation and regression\ntasks, which signiﬁcantly increases the convergence speed of\nthe network. YOLOX eliminates the constraints of the original\nanchor (\nZhang et al., 2020 ) of the YOLO series. The anchor-free\nmechanism substantially reduces the number of design parameters,\nmaintaining eﬀectiveness while signiﬁcantly reducing time costs.\nWhen the YOLO architecture is used for ship detection, it\nmainly has the following two disadvantages: (1) It has poor\nrecognition where small target objects are concerned, and the\npositioning is inaccurate. (2) It lacks the ability to obtain global\ninformation on the image that can beneﬁt the network in terms of\naccuracy and eﬃciency.\n/two.tnum./two.tnum. Transformer\nTransformer (\nVaswani et al., 2017 ) was initially applied in\nthe ﬁeld of natural language processing (NLP) and has proved\nto have many advantages. Transformer is not only powerful in\nmodeling global contexts, but also excellent in establishing long-\ndistance dependencies. With its rapid development in the ﬁeld\nof NLP , Transformer has attracted widespread attention in the\nﬁeld of the computer vision ﬁeld. Swin Transformer (ST) is\nconsidered the ﬁrst successful attempt to bring it into computer\nvision. It enables the Transformer model to process images at\ndiﬀerent scales ﬂexibly by applying a hierarchical structure similar\nto that of CNN. ST performs local self-attention calculations in\nthe area of non-overlapping windows. It lowers the computational\ncomplexity of the number from a squared relationship to a\nlinear relationship. Then it uses shifted window multi-head self-\nattention (SW-MSA) to achieve information interaction between\nnon-overlapping windows.\nAs a general visual network, ST exhibits state-of-the-art\nperformance in semantic segmentation, object detection, and image\nclassiﬁcation. However, ST has two clear drawbacks: (1) ST has a\nlimited ability to encode contextual information and needs further\nimprovement. (2) Because ST has more parameters than CNN, its\ntraining usually relies on a large number of training data.\n/two.tnum./three.tnum. Attention mechanism\nUsually, attention mechanisms in the vision domain (\nGuo\nM. et al., 2022 ) include two types: spatial and channel. They\nextract better target features by assigning diﬀerent weights to the\nfeature points on the image. The spatial attention mechanism\nadds weights to the feature points containing object features\nin a single-channel feature map. On the other hand, the\nchannel attention mechanism assigns more importance to feature\nchannels containing component semantic information.\nHu et al.\n(2018) proposed SENet, which analyzes the correlation between\ndiﬀerent feature channels and generates channel descriptions by\nfusing features across spatial dimensions, thus achieving selective\nemphasis on feature information and suppressing irrelevant feature\ninformation; Woo et al. integrated the feature channels and feature\nspace between correlation proposed CBAM (\nWang et al., 2018 ),\nwhich can focus on more profound feature semantic information;\nWang et al. proposed CANet ( Hou et al., 2021 ), which considers\ninter-channel relationships as well as location information over\nlong distances, based on the spatial selectivity of the channel\nattention mechanism. In this paper, the diﬀerent characteristics of\nSE, CBAM, and CA, are introduced into other model modules to\nimprove the performance model further.\n/three.tnum. Methods\nIn this section, we ﬁrst give a general overview of the ST-\nYOLOA target detection model and then discuss in detail the\ndesign ideas and network architecture of the ST-YOLOA model in\nthree parts: feature extraction (Backbone), feature fusion (Neck),\nand target detection (Head), respectively.\nFigure 1 shows the ST-\nYOLOA network structure.\n/three.tnum./one.tnum. Overview\n/three.tnum./one.tnum./one.tnum. Backbone\nIn ST-YOLOA, we propose a backbone network called STCNet.\nIt integrates the advantages of the Swin Transformer and the\nCA attention mechanism. Compared with the traditional CNN-\nbased backbone feature extraction network, which only utilizes the\ninformation provided by regions in target localization, STCNet has\ngood performance with dynamic attention and global modeling\ncapability considering remote dependencies. The STCNet network\nadopts a layered architecture consisting of the Patch Embedding\nlayer, the Swin Transformer Block, and the CA- Patch Merging layer\ncomposed of three parts.\n/three.tnum./one.tnum./two.tnum. Neck\nIn the neck of ST-YOLOA, we still use PANet to construct\nfeature pyramids (\nLin et al., 2017a ) for feature depth fusion. In\naddition, we also introduce SE and CBMA attention mechanisms in\nthe neck to enhance the focus on the target information and further\nimprove the model performance.\n/three.tnum./one.tnum./three.tnum. Loss\nThe purpose of the loss function is mainly to make the\nmodel localization more accurate and recognition accuracy higher.\nTherefore, more advanced EIOU Loss is used in ST-YOLOA to\naccelerate the convergence and improve the model performance.\n/three.tnum./two.tnum. Backbone\n/three.tnum./two.tnum./one.tnum. Patch embedding layer\nThe patch embedding module ﬁrst chunks the image at the\nfront end of the feature extraction network, dividing the image\ninto 4 × 4 non-overlapping blocks so that the feature dimension\nof each block is 4 × 4 × 3. Then, the original 2D image is\nconverted into a series of 1D embedding vectors by projecting\nthe feature dimensions to arbitrary dimensions through linear\ntransformation, and the transformed embedding vectors are input\nFrontiers in Neurorobotics /zero.tnum/three.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nFIGURE /one.tnum\nST-YOLOA network structure. The red dashed boxes are the atte ntion module addition locations. Conv, BN, and Silu denote the convolu tion, batch\nnormalization, and SILU activation functions, respectively. Con cat indicates the fully connected operation. Cls, reg, and obj repr esent the\nclassiﬁcation, localization, and conﬁdence scores. H, W, and C denote the feature map’s width, height, and number of channels.\nFIGURE /two.tnum\nPatch Embedding structure. w and h are the length and width of t he input feature map, d is the number of channel dimensions, an d N is the batch\nsize.\nto three-stage feature extraction layers to generate a hierarchical\nfeature representation. The structure is shown in\nFigure 2.\n/three.tnum./two.tnum./two.tnum. Swin transformer block\nThe Swin Transformer Block uses moving windows to calculate\nthe attention between pixels, which helps to connect the front\nlayer windows and reduce the complexity of the original attention\ncalculation while overcoming the drawback of a lack of global\neﬀects, signiﬁcantly enhancing the modeling eﬀect.\nIn\nFigure 3, it can be seen that the multiheaded self-\nattention (MSA) mechanism in the Swin Transformer Blocks\nis constructed based on the shift window. There are two\nconsecutive Swin Transformer Blocks. Each Swin Transformer\nBlock consists of a LayerNorm (LN) layer, an MSA module, a\nresidual connection, and a multilayer perceptron (MLP) that\ncontains two fully connected layers using the GELU non-linear\nFrontiers in Neurorobotics /zero.tnum/four.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nFIGURE /three.tnum\nThe Swin transformer blocks.\nFIGURE /four.tnum\nSchematic diagram of the Patch Merging layer.activation function ( Wang et al., 2021 ). The two consecutive\nSwin Transformer Blocks adopt the window multi-head self-\nattention (W-MSA) module and the shifted window multi-head\nself-attention (SW-MSA) module, respectively, which enables\ndiﬀerent windows to exchange information while reducing\ncomputational eﬀort. Based on this window division mechanism,\nthe continuous Swin Transformer Blocks are calculated\nas follows:\nˆzi = W − MSA\n(\nLN\n(\nzi− 1))\n+ zi− 1 (1)\nzi = MLP\n(\nLN\n(\nˆzi\n))\n+ ˆ zi (2)\nˆzi+ 1 = SW − MSA\n(\nLN\n(\nzi))\n+ zi (3)\nzi+ 1 = MLP\n(\nLN\n(\nˆzi+ 1))\n+ ˆ zi+ 1 (4)\nWhere ˆzi denotes the output of the (S)W-MSA module and zi\ndenotes the output of the MLP module of the ith Block.\n/three.tnum./two.tnum./three.tnum. CA-patch merging\nThe Patch Merging layer is used to perform a down-sample\noperation before the feature output of the backbone network\nto reduce the feature map resolution and adjust the number of\nchannels, thus forming a layered design and also saving some\ncomputational eﬀort.\nFigure 4 presents the working process.\nConsidering the limited context encoding capability of the Swin\nTransformer, we add the CA attention mechanism after the Patch\nMerging layer. CA attention decomposes the channel attention\nwork process into two one-dimensional feature encoding processes\nand then performs feature aggregation along two directions in\nspace.\nFigure 5A illustrates the structure of the CA attention\nmechanism. It ﬁrst pools the feature maps globally averaged in\ntwo dimensions, height, and width, using convolution kernels of\ndimensions (H, 1) and (1, W), respectively:\n\n\n\n\n\nzh\nc (h) = 1\nW\n∑\n0≤ i<W\nxc(h, i)\nzh\nc (w) = 1\nH\n∑\n0≤ j<H\nxc(j, w) (5)\nThe above transformations obtain the feature maps in the space\nin the width and height directions, respectively. Then CA attention\nperforms the stitching operation on the feature maps and performs\nthe F1 transformation to obtain the feature map f . The formula is\nshown below:\nf = δ (F1([zh, zw])) (6)\nWhere F1 is the 1 × 1 convolutional transform function,\n[ , ] denotes the splicing operation, and δ is the nonlinear\nactivation function.\nThe feature map f is then convolved in the original height and\nwidth direction and activated by the Sigmoid activation function to\nobtain the feature map attention weights gh and gw, which are given\nby the following equations:\n{\ngh = σ (Fh(f h))\ngw = σ (Fw(f w)) (7)\nWhere σ is the sigmoid activation function.\nFinally, the CA attention mechanism is calculated by\nmultiplicative weighting to obtain the output of the feature map\nwith attention weights:\nyc(i, j) = xc(i, j) × gh\nc (i) × gw\nc (j) (8)\nIt encodes long-range dependencies by precise positional\ninformation, enabling our model to utilize global contextual\ndetails eﬃciently. At the same time, CA has both channel and\nspatial domain attention mechanisms. Its introduction can better\ncapture direction-aware and location-sensitive information for\nmore accurate localization to identify objects of interest and\nimprove feature representation.\n/three.tnum./three.tnum. Neck\n/three.tnum./three.tnum./one.tnum. Improved CSPLayer—SECSP\nCSPLayer (\nWang et al., 2020 ) is mainly divided into two\nparts ( Figure 1), a backbone part, which consists of shallow\nFrontiers in Neurorobotics /zero.tnum/five.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nFIGURE /five.tnum\nThe module structure of the attention mechanism. (A) CA module; (B) SE module; (C) CA module.\nconvolutional branches and sub-residual branches, and a residual\npart, which is directly connected to the output part of the CSPLayer\nthrough a simply processed 1 × 1 convolutional layer. The sub-\nresidual bottleneck structure is an essential component of the\nCSPNet. It has a 1 × 1 convolutional stacked layer and a 3 × 3\nconvolutional stacked layer. Additionally, shortcut connections\nare applied to directly add elements to the output of the\nconvolutional layer. The feature extraction process of the CSP\nnetwork is primarily carried out in the sub-residual bottleneck\nstructure, and its application signiﬁcantly alleviates the gradient\ndisappearance problem.\nThe use of CSPLayer makes the model over-consider\nthe surrounding contextual information (\nLiu et al., 2022 ),\nwhich causes local information interference. To solve this\nproblem, we introduce the SE attention mechanism in\nthe Bottleneck module to selectively emphasize the feature\ninformation, weaken the interference information, and further\nenhance the focus on the target features. Meanwhile, the 3\n× 3 convolutional layer in Bottleneck needs to deal with a\nlarge number of parameter operations while causing a large\nnumber of parameter redundancies. SE performs feature\ncompression on the feature map down the spatial dimension,\nsqueezing the global spatial information into the channel\ndescription. The output feature map z c of channel c after\ncompression is:\nzc = 1\nH × W\nH∑\ni= 1\nW∑\nj= 1\nxc(i, j) (9)\nWhere xc is the input, H and W represent the two directions of\nheight and width in space, respectively. This process dramatically\nreduces the redundant parameters in the network.\nFigure 5B\nillustrates the structure of SE.\n/three.tnum./three.tnum./two.tnum. Improved up-sampling and down-sampling\nprocesses\nThe resolution of the feature maps at various sizes varies.\nBefore feature fusion, down-sampling or up-sampling operations\nmust shrink or enlarge the feature maps for feature fusion between\nfeature maps of diﬀerent scales. The process of compression and\nextension of feature maps brings about the problem of semantic\ninformation loss and the introduction of local interference. CBAM\n(as shown in\nFigure 5C) can focus on more profound feature\nsemantic information by performing a hybrid pooling of both\nFrontiers in Neurorobotics /zero.tnum/six.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nglobal average and global maximum over space and channels. Its\nintroduction makes the model more robust. The speciﬁc working\nprocess is as follows:\n\n\n\n\n\n\n\n\n\nMC(F) = σ (MLP(AvgPool(F)) + MLP(MaxPool(F)))\n= σ (W1(W0(Fc\navg)) + W1(W0(Fc\nmax)))\nMS(F) = σ (f 7× 7([AvgPool(F); MaxPool(F)]))\n= σ (f 7× 7([Fs\navg;Fs\nmax]))\n(10)\nWhere M C(F) and M S(F) are one-dimensional and two-\ndimensional channel attention, respectively; σ is a sigmoid\nfunction; W0 ∈ RC/ r× C, W1 ∈ RC× C/ r; and f 7× 7 is a convolution\nkernel of 7 × 7 size.\nThe deconvolution up-sampling approximation is considered\nthe inverse operation of convolution. It can restore the feature\nmap better by introducing training parameters for learning.\nHowever, this up-sampling method is prone to a tessellation\neﬀect, which causes pixel blocks to appear in the image. On\nthe other hand, the interpolation method does not require\nany parameter learning. It performs predictive estimation of\nunknown points based on known pixel points, which can expand\nthe size of the image and achieve the eﬀect of up-sampling.\nTherefore, we use the bicubic interpolation algorithm for\nup-sampling instead of deconvolution, which reduces many\nparameter operations while preserving the original image\ninformation.\nFigure 6 illustrates the improved up-and-down\nsampling process.\n/three.tnum./four.tnum. Head\nConsidering the fact that SAR images of ships in complex\nenvironments require a lot of feature information to identify\ntargets, the commonly used coupled detection head will\ninﬂuence the model’s performance and cannot detect the\nship targets in SAR images of a complex environment.\nTherefore, the ST-YOLOX network model separates the\nclassiﬁcation and regression tasks by using a decoupled\ndetection head for target detection to achieve the predicted\noutput of target location and bounding box, which signiﬁcantly\nincreases the convergence speed and improves the accuracy of\nthe model.\nAs for the loss function, SAR image ship detection is a\nsingle-class detection task. Hence, the loss function has only two\ncomponents: localization loss (Reg) and conﬁdence loss (Obj)\n(\nJiang et al., 2022 ). The mathematical equations for these two\ncomponents are as follows:\nLoss = λ Lreg + Lobj\nNpos\n(11)\nwhere λ is the balance coeﬃcient; Npos represents the Anchor\nPoints quantity of positive samples; Lobj indicates the conﬁdence\nloss; in our paper, the binary cross-entropy loss (BCE loss) is\nused as this loss function to promote numerical stability; Lreg\nrepresents the localization loss, and the Eﬃcient-IOU (EIOU)\n(\nZhang et al., 2022 ) loss function is used. Its mathematical\nexpression is:\nL = LIOU + Ldis + Lasp = 1 − IOU + ρ 2 (\nb, bgt)\nc2 + ρ 2 (\nw, wgt)\nc2ω\n+ ρ 2 (\nh, hgt)\nc2\nh\n(12)\nwhere LIOU is the overlap loss, Ldis is the center distance\nloss, and Lasp is the wide height loss. EIOU loss integrates the\noverlapping area, the distance to the center point, and the aspect\nratio of the bounding box regression. It splits the loss term of the\naspect ratio into the diﬀerence between the widths and the heights\nof the predicted and the minimum outer bounding boxes, which\neﬀectively solves the sample imbalance problem in the bounding\nbox regression task, accelerates the convergence, improves the\nregression accuracy, and further optimizes the model.\nWith the aforementioned analysis, the ST-YOLOA detection\nmodel proposed in this paper applies to the detection of ship objects\nin SAR images under complex environments. It has the advantages\nof strong feature extraction capability, high utilization of high-\nlevel and low-level feature information, full information fusion, and\nrobust performance, which is more suitable for ship target detection\nunder realistic conditions.\n/four.tnum. Experiment\n/four.tnum./one.tnum. Experimental data and environment\n/four.tnum./one.tnum./one.tnum. Dataset\nIn the paper, the experimental data are based on the publicly\naccessible SAR-Ship-Dataset (\nWang et al., 2018 ) from the Key\nLaboratory of Digital Earth, Institute of Space and Astronomical\nInformation, Chinese Academy of Sciences. The primary data\nsources of this dataset are Sentinel-1 SAR data and domestic\nGaofen-3 SAR data, which use three polarization techniques:\nsingle-polarization, double-polarization, and full-polarization. It\nused 108-view Sentinel-1 and 102-view Gaofen-3 high-resolution\nSAR images to build a SAR ship target deep learning sample\nlibrary containing 43,819 images of 256 × 256 pixels with 59,535\nship targets in total. The dataset contains a wide variety of ship\ntypes and backgrounds, including sea-surface scenes with noise\ninterference from the ocean and ships of diﬀerent scales and\nnearshore scenes inﬂuenced by complex backgrounds, such as\nislands, land constructions, and port terminals.\nWe used 4,000 photographs from the SAR-Ship-Dataset as the\ndataset for our experiment. The training set and the test set were\nrandomly divided according to the ratio of 8:2, and 20% of the\ntraining set was randomly selected as the validation set. To increase\nthe data diversity and ensure the model had a better training eﬀect,\nwe used two data enhancement methods, Mosaic (\nTian et al., 2019 )\nand Mixup ( Zhang et al., 2017 ), to perform data enhancement\noperations on the dataset.\nTo test the ship detection capability of this model in complex\nenvironments, we selected 450 SAR ship images in complex\nenvironments, such as near-coastal ship targets aﬀected by\nsurrounding non-ship targets, ship targets with blurred or obscured\nimaging, ship targets with coherent speckle noise and complex\nbackground information, and multi-scale ship targets. We named\nthe two SAR ship detection test sets constructed as norm test set\nFrontiers in Neurorobotics /zero.tnum/seven.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nFIGURE /six.tnum\n(A, B) Up/down-sampling structure diagram.TABLE /one.tnum Experimental data set details.\nDataset Number\nof images\nTarget\nbackground\nNumber of\nships\nTrain 2,560 General\nbackground\n3,440\nVal 640 General\nbackground\n843\nTest NTS 800 General\nbackground\n1,020\nCTS 450 Complex\nbackground\n943\nMTS 1,250 General\nBackground +\nComplex\nbackground\n1,963\n(NTS) and complex test set (CTS), respectively, and combined the\ntwo sets into one merged test set (MTS). We used the combined\nperformance of the model in these three test sets as the criterion to\nverify its detection ability. The details of the ship data set used for\nthe experiments are shown in\nTable 1.\n/four.tnum./one.tnum./two.tnum. Evaluation indicators\nThis paper chooses the average precision (AP) ( Everingham\net al., 2009 ) as the main evaluation index to assess the eﬀect of SAR\nimage ship detection. It contains two parameter metrics, Precision\nand Recall. The calculation formula is:\n\n\n\n\n\nP = TP\nTP+ FP\nR = TP\nTP+ FN\nAP =\n∫ 1\n0 P ( r) dr\n(13)\nwhere TP (true positive) is the number of ships marked as ship\ntargets, FN (false negative) is the number of ship targets marked as\nnon-ships, FP (false positive) is the number of non-ships marked as\nship targets, and P(r) is the area under the PR curve with precision\nand recall, which is AP.\nAlso, to better measure, the comprehensive performance of the\nmodel, Parameters, GFLOPs, and FPS are introduced as evaluation\nmetrics in this paper.\n/four.tnum./one.tnum./three.tnum. Experimental environment and parameter\nsetting\nIn this paper, the experimental environment was based on\nLinux system architecture, using the Ubuntu 18.04 operating\nsystem, equipped with an Intel(R) Core i9 10980 XE CPU and\nNVIDIA RTX 2080TI graphics card with 11 GB video memory. The\ndeep learning framework used PyTorch, with accelerated training\nvia CUDA 10.1 and cuDNN 7.6.\nIn this paper, the experimental hyperparameters are referred to\nthe literature (\nAn et al., 2019 ; Yuan and Zhang, 2021 ; Wu et al.,\n2022), and the main settings are as follows: setting the training\nperiod to 300 epochs, the maximum learning rate of the model\nto 0.01, and the minimum learning rate to 0.0001. The optimizer\nwas stochastic gradient descent (SGD), and the weights decayed\nFrontiers in Neurorobotics /zero.tnum/eight.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nTABLE /two.tnum The ablation experiments results.\nSerial number Swin-T Attention EIOU loss AP/% FPS GFLOPs Parameters\nNTS CTS MTS\n1 - - - 96.23 70.86 85.52 22.74 27.27 8.94\n2 √ - - 96.30 73.81 86.72 22.24 89.60 32.72\n3 - √ - 96.94 71.64 86.37 20.24 27.28 9.00\n4 - - √ 97.23 73.61 88.27 22.38 27.27 8.94\n5 √ √ - 96.71 74.19 87.74 21.60 89.61 32.77\n6 √ - √ 97.28 74.40 88.16 22.00 89.60 32.72\n7 - √ √ 97.81 72.40 86.93 20.49 27.28 9.00\n8 √ √ √ 97.37 75.69 88.50 21.40 89.61 32.77\nBold indicates the best result of each column, italic is the second bes t result; “-” is no module added, “ √ ” is the module added.\nFIGURE /seven.tnum\nLoss function curves of the ablation experiments.\nat a rate of 0.0005. To increase the speed of data reading, we\nemployed multi-threaded data reading and used cosine (COS)\nas the learning rate descent method. The test was run with the\nfollowing parameters: a non-maximum suppression threshold of\n0.65, a conﬁdence level of 0.001, and a prediction probability\nthreshold of 0.5.\n/four.tnum./two.tnum. Ablation experiments\nTo validate the performance of each major module and\nloss function in the ST-YOLOA model, we performed ablation\nexperiments. In these experiments, as the benchmark model,\nwe used the YOLOX network, which through eight groups of\nnetworks with various structures were used to test the eﬀects\nof various strategies on the detection eﬀectiveness of the model.\nThese strategies included changing the Swin Transformer backbone\nnetwork; adding CA, SE, and CBAM attention mechanisms;\nmodules; and using the EIOU loss function. We used the\nsame experimental equipment and training parameters in each\nexperiment to test and validate the detection eﬀect in the NTS, CTS,\nand MTS.\nTable 2 shows the results of the ablation experiments.\nThe benchmark model YOLOX network is the least eﬃcient,\nas seen in Table 2. Comparing the ordinal number 2 in the table,\nwe can see that the improvement in the average accuracy AP on\nthe NTS using the Swin Transformer backbone is insigniﬁcant.\nHowever, the improvement in the detection of CTS is signiﬁcant.\nSwin Transformer contains a large number of parameters but does\nnot have much impact on the detection speed. Its introduction\ngives the model an AP improvement of 2.95% with almost no loss\nin detection speed FPS. This demonstrates that Swin Transformer\ncan focus on global information, particularly for the extraction of\nsophisticated features with a signiﬁcantly enhanced eﬀect. Serial\nnumber 3 adds CA, SE, and CBAM attention mechanism modules\nto the feature extraction and feature fusion sections, improving\nthe AP of the NTS and the CTS by 0.71 and 0.78%, respectively,\nwhile the FPS decreases by 2.5 frames per second, showing that\nthis method is capable of adaptively focusing on and using useful\nlocal feature information to lower the rate of missed detection\nbut complicates the model computationally and structurally. Serial\nnumber 4 introduces the EIOU loss function and achieves good\ndetection results on both test sets, with an AP improvement of 1.00\nand 2.75%, respectively. Although the FPS decreases slightly, this\nstill demonstrates that the introduction of the EIOU loss enhances\nconvergence speed and prevents the degradation of model training\ncaused by the uneven distribution of positive and negative samples,\nwhich is a successful addition strategy. The trials in serial numbers\n5 to 8 are composite multi-strategy experiments. A comparison of\nserial numbers 2 to 4 shows that the use of a variety of tactics\ntogether achieves better results than the use of just one strategy.\nFrom serial numbers 5 and 6, it can be seen that Swin Transformer\neﬀectively addressess any speed reduction brought on by the\naddition of other modules. Serial number 7 uses both the attention\nmodule and EIOU loss to ensure that the model works optimally\non the NTS. Serial number 8 is the ST-YOLOA network model\nproposed in this paper. Compared with YOLOX, its AP is improved\nby 1.14 and 4.83% in the NTS and the CTS, respectively. Although\nthe eﬀect is slightly reduced compared to serial number 7 on NTS,\nit still achieved second place in the comparison experiment. Its\ndetection speed and detection eﬀect in a complex environment are\nalso substantially ahead.\nFrontiers in Neurorobotics /zero.tnum/nine.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nTABLE /three.tnum Ablation experiments of attentional mechanisms.\nSE CA CBAM Precision/% Recall/% AP/% FPS GFLOPs Parameters\n79.87 83.49 86.72 22.24 89.60 32.72\n√ 84.83 81.46 87.46 21.94 89.60 32.72\n√ √ 85.71 82.53 87.64 21.87 89.60 32.75\n√ √ √ 86.27 83.61 87.74 21.60 89.61 32.77\nBold represents the maximum value in each column. √ represents the module that added one row of that column to the network.\nTABLE /four.tnum Performance comparison of the diﬀerent algorithms.\nAlgorithm NTS CTS MTS FPS GFLOPs Parames\nP/% R% AP/% P/% R/% AP/% P/% R/% AP/%\nCenterNet 94.97 85.20 96.22 82.64 57.05 70.95 89.85 71.68 85.54 25.13 109.34 32.67M\nFaster R-CNN 61.84 98.33 95.98 38.88 76.35 65.03 50.04 87.72 83.18 25.47 401.91 136.69M\nSSD 90.40 85.88 94.43 68.68 57.90 66.48 80.61 72.44 82.83 50.40 273.40 23.61M\nRetinaNet 92.82 91.27 96.80 72.23 64.26 71.36 83.44 78.30 86.47 39.15 163.49 36.33M\nEﬃcientDet 95.60 87.35 96.81 81.33 55.89 72.32 89.75 72.24 86.95 45.64 7.40 3.83M\nYOLOv5 90.86 90.69 95.65 69.28 63.63 67.03 80.94 77.69 83.54 32.88 16.38 7.06M\nYOLOX 88.79 94.71 96.23 72.77 67.44 70.86 82.66 82.32 85.52 22.74 27.27 8.94M\nYOLOv7 92.06 95.49 97.36 75.70 68.40 73.56 84.76 82.48 87.50 34.60 105.11 37.20M\nST-YOLOA 91.82 95.78 97.37 74.24 72.75 75.69 84.04 83.44 88.50 21.40 89.61 32.77M\nBold font denotes the best outcome for each column, italics is the second bes t result.\nThe loss function curve in Figure 7 clearly illustrates our\nalgorithm’s superior performance. In conclusion, the model in\nthis paper meets the real-time detection criteria, while displaying\nsubstantially improved target detection accuracy, especially\nshowing excellent detection performance in the complicated\nenvironment of the dataset.\nIn this paper, we introduce three attention mechanisms to\nenhance the network performance according to the characteristics\nof diﬀerent modules to enhance the focus on ship targets. We\nconducted ablation experiments on three attentional mechanisms,\nSE, CA, and CBAM, to validate the eﬀectiveness of each attentional\nmechanism.\nTable 3 shows the experimental results. The results\nshow that the combination of the three attention mechanisms\nworks optimally.\n/four.tnum./three.tnum. Comparative experiments\nTo objectively evaluate the detection eﬀectiveness of the ST-\nYOLOA model, we performed comparative experiments using our\nmodel and other existing target detection methods. The range\nof comparison algorithms covers a wide range, among which\nCenterNet, Faster R-CNN, SSD, RetinaNet (\nLin et al., 2017b ) and\nEﬃcientDet ( Tan et al., 2020 ) are classical target detection models,\nYOLOv5 ( Jocher, 2020 ) and YOLOX are newly published high-\nperformance target detection models in recent years, and YOLOv7\n(\nWang et al., 2022 ) is one of the most advanced detection models at\npresent. Table 4 displays the results of the comparison experiments.\nCenterNet predicts the bounding box by learning the centroid\nand corner point pairs in the feature map without relying on\nthe predetermined anchor box. It has the highest precision rate\nbut a poor recall rate. As a representative algorithm of two-stage\ndetection, the Faster R-CNN detection recall rate has improved\nsigniﬁcantly but the detection accuracy can be still improved.\nSSD has been experimentally shown to be unable to eﬃciently\ndetect ships in complicated surroundings in a SAR picture, despite\nhaving a higher detection speed and a more condensed network\nmodel. RetinaNet surpasses the previous two-stage algorithm in\nterms of accuracy and the last detection single-stage algorithm in\nterms of speed, but there is still much room for improvement\neﬃcientDet, as a lightweight network model, has the lowest number\nof parameters and computations, but its accuracy still needs further\nimprovement. YOLOv5 and YOLOX, as new single-stage detection\nmodels, have fewer parameters and computation and more concise\nnetwork models, which perform better than traditional detection\nalgorithms. However, there is still a certain degree of false detection\nand leakage, with serious false detection and leakage problems\nin complex conditions. YOLOv7 shows excellent performance\nin speed and accuracy, but the detection capability for complex\nbackgrounds still needs further improvement. Compared with\nother models, ST-YOLOA has an average number of parameters\nand computation and has higher detection accuracy. It can also\nmeet the basic requirements of real-time detection. Therefore, ST-\nYOLOA has good overall performance in terms of comprehensive\ndetection, particularly when it comes to SAR image ship detection\nin complicated environments.\nIn this research, to demonstrate the detection performance of\nvarious models, we compared the PR (precision–recall) curves of\neach model on two separate test sets. The PR comparison curves\nare displayed in\nFigure 8.\nTo visually compare and analyze the detection eﬀects of the\nST-YOLOA model and other algorithms in diﬀerent scenarios, we\nFrontiers in Neurorobotics /one.tnum/zero.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nFIGURE /eight.tnum\nComparison of the PR curves of diﬀerent algorithm models. (A) NTS; (B) CTS; (C) MTS.\nFIGURE /nine.tnum\nComparison of the detection results.\nselected SAR images containing near-coast and far-sea ship targets.\nFigure 8 shows the detection eﬀect, where the ﬁrst column of\nFigure 9 shows the actual labeling result, and other columns show\nthe detection results of each algorithm.\n/four.tnum./four.tnum. Generalization ability test\nIn this paper, to illustrate the generalization capacity of ST-\nYOLOA, we used two distinct ways for partitioning data (\nGuo W.\net al., 2022 ): (1) Partitioning the data at random into ﬁve ratios:\n{9:1, 8:2, 7:3, 6:4, 5:5}. (2) Partitioning the data multiple times at\nrandom into the ratio 8:2. The test results of the two methods of\ndataset division are provided in\nTables 5, 6.\nTable 5 shows that although the number of samples of ship\ntargets in the test samples that are randomly divided by diﬀerent\nratios of the dataset varies signiﬁcantly, the average accuracy of\nST-YOLOA does not change much. However, even though the\naverage accuracy of the samples divided in the ratio of 5:5 among\nthem diﬀered more than the others, it exhibits a good detection\nability, which is analyzed because the detection eﬀect degrades as a\nresult of an insuﬃcient number of training samples. The variance of\nAP for each sample in this experiment is 0.03085, and the variances\nof the precision and recall are 0.08452 and 0.2078, respectively. This\nFrontiers in Neurorobotics /one.tnum/one.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nTABLE /five.tnum Sample cutting in diﬀerent proportions.\nProportioning Precision % Recall % AP %\n9:1 92.17 95.84 97.24\n8:2 91.82 95.78 97.37\n7:3 91.92 96.92 97.44\n6:4 91.38 96.28 97.27\n5:5 91.70 96.23 96.98\nMean 91.798 96.21 97.26\nVariance 0.08452 0.2078 0.03085\nTABLE /six.tnum Multiple sample cuts in the same proportion.\nCutting times Precision % Recall % AP %\n1st 91.63 96.60 97.39\n2nd 91.82 95.78 97.37\n3rd 91.69 96.39 97.27\n4th 92.00 96.51 97.24\n5th 91.47 97.05 97.36\nMean 91.722 96.266 97.326\nVariance 0.03997 0.11733 0.00443\nindicates that the ST-YOLOA model proposed in this paper has a\nstable detection eﬀect for test sets with diﬀerent numbers of data\nsamples and shows a strong generalization ability.\nDue to the same number of samples and smaller variations in\nthe number of ship targets, as shown in\nTable 6, the variance of\nthe experiment’s indicators is lower for samples divided multiple\ntimes at the same scale. The mean and variance of the SA-YOLOA\nmodel for the same proportion of samples divided multiple times\nwere 91.722% and 0.03997 for precision, 96.266% and 0.11733 for\nrecall, and 97.326% and 0.00443 for mean precision, respectively.\nThe information in\nTables 5, 6 leads to the conclusion that ST-\nYOLOA performs well and has excellent generalization capacity,\nboth in test samples with various ratios of randomly divided\ndatasets and in test samples with the same proportion of multiple\ndivided datasets.\n/four.tnum./five.tnum. Detection eﬀect of the ST-YOLOA\nmodel in diﬀerent scenarios\nTo visualize the detection eﬀect of the ST-YOLOA model and\nfurther measure the model performance, this section ﬁrst shows the\nschematic of the confusion matrix of the ST-YOLOA algorithm.\nAs shown in\nFigure 10, the confusion matrix demonstrates that\nST-YOLOA has good performance.\nIn this study, we demonstrate the eﬀect of ship target detection\nunder diﬀerent scenarios and scales, including near-shore and far\nsea.\nFigure 11 presents the detection eﬀect in each scenario. The\nﬁrst and second rows are near-shore ship targets near islands\nand near-shore buildings, respectively. Such targets have complex\nbackgrounds and are susceptible to the inﬂuence of other non-ship\ntargets around them. Multiple near-shore ship targets can easily\nFIGURE /one.tnum/zero.tnum\nConfusion matrix.\nbe framed by a single detection box due to the dense docking of\nships, which suppresses candidate boxes with high overlap and\nlow prediction scores. The third row is a small, dense target in\nthe distance that is easy to miss because it has a small ship scale.\nThe ship target in the fourth row is prone to erroneous target\nlocalization since it has indistinct target borders and complicated\nbackground information. In all four aforementioned scenarios, the\nST-YOLOA model signiﬁcantly improved the detection rate and\naccuracy, as can be seen from the ﬁgure, and produced positive\ndetection results.\n/four.tnum./six.tnum. Limitations and discussion\nThe results of previous experimental studies show that our\nmodel achieves sound visual eﬀects in SAR ship detection in\ncomplex scenes. It is demonstrated that the ST-YOLOA model can\nlearn global features and can be used to extract more powerful\nsemantic features for ship target detection in harsh environments\nand complex scenes. However, our approach still suﬀers from\nsome limitations.\nThe relatively high computational complexity and large\nnumber of parameters of the Swin Transformer module lead\nto more extended training and inference time. As seen from\nthe experimental ablation results in\nTable 1, although we have\nused the Swin-Transformer network with a smaller model as\nmuch as possible, its use still introduces many parameters\ncompared to the base model. The Swin Transformer network has\na solid global modeling capability, capturing rich global feature\ninformation and integrating global data. This process requires a\nvast amount of support operations, resulting in more parameters\nand computations than other models. At the same time, the\ncomputational complexity of the Swin Transformer increases with\nthe length of the input sequence. When dealing with very long\ninput sequences, Swin Transformer may face problems such as\nhigh computational complexity and large memory consumption,\nFrontiers in Neurorobotics /one.tnum/two.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nFIGURE /one.tnum/one.tnum\nDetection eﬀects in diﬀerent scenarios.\nwhich need to be alleviated by using lightweight models or\nother techniques.\n/five.tnum. Conclusions\nTo ensure the accuracy of SAR ship target recognition under\ncomplicated situations, in this study, we have suggested a more\nextended ST-YOLOA ship target identiﬁcation model. To begin\nwith, the feature extraction section adds the Patch Embedding\nmodule after the input layer to chunk and ﬂatten the input image\nand then produces feature maps of varying sizes using Swin\nTransformer Blocks and the Patch Merging layer. A coordinated\nattention mechanism is designed at the end to simultaneously\ncapture position information and channel relationships, which\nsigniﬁcantly improves the performance of downstream tasks.\nSecond, to eﬀectively use semantic and localization information,\nthe PANet is employed to thoroughly fuse high-level and low-\nlevel feature information. Finally, a decoupled detection head\nin the target detection section is used to signiﬁcantly speed\nup model convergence and improve the position loss function,\nboth of which improve model performance. This model is more\nsuited for ship target detection in challenging surroundings and\ncomplex circumstances because it can extract more potent semantic\ncharacteristics and can better learn global features than other\ndetection models.\nConsidering that our model focuses on improving SAR ship\ndetection accuracy in complex environments, the vital index of the\nnumber of parameters of the model is ignored to a certain extent.\nIn the future, we will further conduct model optimization and carry\nout research on model lightweight by adjusting hyperparameters\nand model compression methods, such as quantization, distillation,\nand pruning, and further analysis on lightweight Swin Transformer\nto achieve lower model parameter computation, faster training\nspeed, and maintain previous accuracy.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding author.\nAuthor contributions\nConceptualization: XY, KZ, and RL. Methodology and writing–\noriginal draft preparation: KZ and RL. Software: RL. Investigation:\nJF and SW. Resources and visualization: KZ. Writing–review\nand editing: XY, KZ, and SW. Supervision: KZ and SW. Project\nadministration and funding acquisition: XY. All authors have read\nand agreed to the published version of the manuscript.\nFrontiers in Neurorobotics /one.tnum/three.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nFunding\nThis work was supported in part by the National\nNatural Science Foundation of China under Grant 62276274\n(Corresponding author: RL).\nConﬂict of interest\nThe authors declare that the research was conducted\nin the absence of any commercial or ﬁnancial relationships\nthat could be construed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAn, Q., Pan, Z., Liu, L., and You, H. (2019). DRBox-v2: an impro ved detector with\nrotatable boxes for target detection in SAR images. IEEE Trans. Geosci. Remote Sensing\n57, 8333–8349. doi: 10.1109/TGRS.2019.2920534\nBochkovskiy, A., Wang, C. Y., and Liao, H-, Y. M. (2020). Yolov 4: Optimal speed\nand accuracy of object detection. arXiv 2004, 10934. doi: 10.48550/arXiv.2004.10934\nCumming, I. G., and Wong, F. H. (2005). Digital processing of sy nthetic aperture\nradar data. Artech House1, 108–110.\nDuan, K., Bai, S., Xie, L., Qi, H., Huang, Q., Tian, Q., et al. (2 019). “Centernet:\nKeypoint triplets for object detection”, in Proceedings IEEE/CVF, 6569–6578.\ndoi: 10.1109/ICCV.2019.00667\nEveringham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisse rman, A. (2009).\nThe pascal visual object classes (voc) challenge. Int. J. Computer Vision88, 303–308.\ndoi: 10.1007/s11263-009-0275-4\nGao, G., Gao, S., He, J., and Li, G. (2018). Adaptive ship detecti on in\nhybrid-polarimetric SAR images based on the power–entropy deco mposition.\nIEEE Trans. Geosci. Remote Sensing 56, 5394–5407. doi: 10.1109/TGRS.2018.\n2815592\nGao, W., Liu, Y., Zeng, Y., Li, Q., and Liu, Q. (2022). Enhance d attention one shot\nSAR ship detection algorithm based on cluster analysis and tran sformer in Second\nInternational Conference on Digital Signal and Computer Communications (DSCC\n2022): SPIE, 290–295. doi: 10.1117/12.2641456\nGe, Z., Liu, S., Wang, F., Li, Z., and Sun, J. (2021). Yolox: Exce eding yolo series in\n2021. arXiv preprint arXiv:2107, 08430.\nGirshick, R. (2015). “Fast r-cnn”, in Proceedings of the IEEE International Conference\non Computer Vision,1440–1448. doi: 10.1109/ICCV.2015.169\nGirshick, R., Donahue, J., Darrell, T., and Malik, J. (2014). “Ri ch feature\nhierarchies for accurate object detection and semantic seg mentation”, in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, 580–587.\ndoi: 10.1109/CVPR.2014.81\nGuo, M., Xu, T., Liu, J., Liu, Z., Jiang, P., Mu, T., et al. (2022) . Attention\nmechanisms in computer vision: a survey. Comput. Visual Media 8, 331–368.\ndoi: 10.1007/s41095-022-0271-y\nGuo, W., Shen, L., Qu, H., Wang, Y., and Lin, C. (2022). Ship det ection in SAR\nimages based on adaptive weight pyramid and branch strong corr elation. J. Image\nGraphics 27, 3127–3138. doi: 10.11834/jig.210373\nHou, Q., Zhou, D., and Feng, J. (2021). “Coordinate attentio n for eﬃcient mobile\nnetwork design”, in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition,13713–13722.\nHu, J., Shen, L., and Sun, G. (2018). “Squeeze-and-excitation networks”,\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 7132–7141.\nJiang, X., Cai, W., Yang, Z., Xu, P., and Jiang, B. (2022). Inf rared dim and small\ntarget detection based on YOLO-IDSTD algorithm. Infrared Laser Eng.51, 502–511.\ndoi: 10.3788/IRLA20210106\nJocher, G. (2020). YOLOv5. Available online at: https://github.com/ultralytics/\nyolov5 (accessed December 5, 2022).\nLi, K., Zhang, M., Xu, M., Tang, R., Wang, L., Wang, H., et al. (2 022). Ship detection\nin SAR images based on feature enhancement Swin transformer and adjacent feature\nfusion. Remote Sensing14, 3186. doi: 10.3390/rs14133186\nLin, T. Y., Dollár, P., Girshick, R., He, K., Hariharan, B., and B elongie, S.,\net al. (2017a). “Feature pyramid networks for object detectio n”, in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, 2117–2125.\ndoi: 10.1109/CVPR.2017.106\nLin, T. Y., Goyal, P., Girshick, R., He, K., and Dollár, P. (2017b). Focal loss\nfor dense object detection. IEEE Trans. Pattern Anal. Mach. Int. 8, 2999–3007.\ndoi: 10.1109/ICCV.2017.324\nLiu, C., Xie, N., Yang, X., Chen, R., Chang, X., Zhong, R. Y., e t al. (2022).\nA domestic trash detection model based on improved YOLOX. Sensors 22, 6974.\ndoi: 10.3390/s22186974\nLiu, S., Qi, L., Qin, H., Shi, J., and Jia, J. (2018). “Path agg regation network for\ninstance segmentation”, in Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 8759–8768.\nLiu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., et al. (2016).\n“Ssd: Single shot multibox detector”, in Computer Vision–ECCV2016, 14th. European\nConference (Amsterdam, The Netherlands, October 11–14, 2016, Proceedi ngs, Part I\n14: Springer), 21–37.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (202 1).\n“Swin transformer: Hierarchical vision transformer using shifted windows”, in\nProceedings of the IEEE/CVF International Conference Computer Vision,10012–10022.\ndoi: 10.1109/ICCV48922.2021.00986\nLu, R., Yang, X., Jing, X., Chen, L., Fan, J., Li, W., et al. (202 0a). Infrared small target\ndetection based on local hypergraph dissimilarity measure. IEEE Geoscience Remote\nSens Lett19, 1–5. doi: 10.1109/LGRS.2020.3038784\nLu, R., Yang, X., Li, W., Fan, J., Li, D., Jing, X., et al. (2020b ). Robust infrared small\ntarget detection via multidirectional derivative-based we ighted contrast measure. IEEE\nGeosci. Remote Sensing Letters19, 1–5. doi: 10.1109/LGRS.2020.3026546\nMoreira, A., Prats-Iraola, P., Younis, M., Krieger, G., Hajns ek, I., Papathanassiou, K.\nP., et al. (2013). A tutorial on synthetic aperture radar. IEEE Geosci. Remote Sens. Mag.\n1, 6–43. doi: 10.1109/MGRS.2013.2248301\nRedmon, J., Divvala, S., Girshick, R., and Farhadi, A. (2016). “You only look once:\nUniﬁed, real-time object detection”, in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition,779–788. doi: 10.1109/CVPR.2016.91\nRen, S., He, K., Girshick, R., and Sun, J. (2015). Faster r-cnn : Towards real-time\nobject detection with region proposal networks. Adv. Neural Inf. Proc. Syst.28, 1–47.\ndoi: 10.1109/TPAMI.2016.2577031\nRobey, F. C., Fuhrmann, D. R., Kelly, E. J., and Nitzberg, R. (19 92). A CFAR\nadaptive matched ﬁlter detector. IEEE Trans. Aerospace Electr. Syst. 28, 208–216.\ndoi: 10.1109/7.135446\nSchwegmann, C. P., Kleynhans, W., and Salmon, B. P. (2015). Man ifold adaptation\nfor constant false alarm rate ship detection in South African o ceans. IEEE J. Selected\nTopics Appl. Remote Sensing8, 3329–3337. doi: 10.1109/JSTARS.2015.2417756\nTan, M., Pang, R., and Le, Q. V. (2020). “EﬃcientDet: Scalable and Eﬃcient\nObject Detection”, in 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR).\nTian, Z., Shen, C., Chen, H., and He, T. (2019). “Fcos: Fully con volutional one-\nstage object detection”, in Proceedings of the IEEE/CVF International Conference on\nComputer Vision,9627–9636.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. Adv. Neural Inf. Proc. Syst.30, 2.\nWang, C., Bi, F., Zhang, W., and Chen, L. (2017). An intensity -space domain CFAR\nmethod for ship detection in HR SAR images. IEEE Geosci. Remote Sensing Letters14,\n529–533. doi: 10.1109/LGRS.2017.2654450\nFrontiers in Neurorobotics /one.tnum/four.tnum frontiersin.org\nZhao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnbot./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/one.tnum/seven.tnum/zero.tnum/one.tnum/six.tnum/three.tnum\nWang, C.Y., Liao, H. Y. M., Wu, Y. H., Chen, P. Y., Hsieh, J. W., Yeh, I. H., et al.\n(2020). “CSPNet: A new backbone that can enhance learning capa bility of CNN”, in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops, 390–391.\nWang, C. Y., Bochkovskiy, A., and Liao, H-, Y. M. (2022). YOLO v7: Trainable bag-\nof-freebies sets new state-of-the-art for real-time object detectors. arXiv 2207, 02696.\ndoi: 10.48550/arXiv.2207.02696\nWang, J., Zhang, Z., Luo, L., Zhu, W., Chen, J., Wang, W., et al. (2021). SwinGD: A\nrobust grape bunch detection model based on swin transformer in complex vineyard\nenvironment. Horticulturae 7, 492. doi: 10.3390/horticulturae7110492\nWang, Y., Wang, C., and Zhang, H. (2018). Combining a single sh ot multibox\ndetector with transfer learning for ship detection using sen tinel-1 SAR images. Remote\nSensing Lett.9, 780–788. doi: 10.1080/2150704X.2018.1475770\nWu, Z., Liu, C., Wen, J., Xu, Y., Yang, J., Li, X., et al. (2022). Selecting\nHigh-Quality Proposals for Weakly Supervised Object Detection With Bottom-Up\nAggregated Attention and Phase-Aware Loss. IEEE Transactions on Image Processing.\ndoi: 10.1109/TIP.2022.3231744\nXia, R., Chen, J., Huang, Z., Wan, H., Wu, B., Sun, L., et al. (20 22).\nCRTransSar: a visual transformer based on contextual joint representation\nlearning for SAR ship detection. Remote Sensing 14, 1488. doi: 10.3390/rs140\n61488\nYuan, Y., and Zhang, Y. (2021). OLCN: An optimized low coupling ne twork\nfor small objects detection. IEEE Geosci. Remote Sensing Letters 19, 1–5.\ndoi: 10.1109/LGRS.2021.3122190\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2017).\nmixup: Beyond empirical risk minimization. Remote Sensing 1710, 09412.\ndoi: 10.48550/arXiv.1710.09412\nZhang, S., Chi, C., Yao, Y., Lei, Z., and Li, S. Z. (2020). “Bri dging the\ngap between anchor-based and anchor-free detection via ada ptive training sample\nselection”, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9759–9768.\nZhang, Y., Ren, W., Zhang, Z., Jia, Z., Wang, L., Tan, T., et al. (2022). Focal and\neﬃcient IOU loss for accurate bounding box regression. Neurocomputing 506, 146–157.\ndoi: 10.1016/j.neucom.2022.07.042\nFrontiers in Neurorobotics /one.tnum/five.tnum frontiersin.org"
}