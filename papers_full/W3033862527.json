{
  "title": "Pre-training Polish Transformer-Based Language Models at Scale",
  "url": "https://openalex.org/W3033862527",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4288165771",
      "name": "Dadas, Sławomir",
      "affiliations": [
        "National Information Processing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4288840786",
      "name": "Perełkiewicz, Michał",
      "affiliations": [
        "National Information Processing Institute"
      ]
    },
    {
      "id": "https://openalex.org/A4288840787",
      "name": "Poświata, Rafał",
      "affiliations": [
        "National Information Processing Institute"
      ]
    },
    {
      "id": null,
      "name": "Dadas, S{\\l}awomir",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Pere{\\l}kiewicz, Micha{\\l}",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Po\\'swiata, Rafa{\\l}",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2973071945",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W6607919353",
    "https://openalex.org/W2901232344",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2985092403",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3035614045",
    "https://openalex.org/W3090015871",
    "https://openalex.org/W6830857627",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2739744877",
    "https://openalex.org/W6601375367",
    "https://openalex.org/W3030669333",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W3101778449",
    "https://openalex.org/W2751936342",
    "https://openalex.org/W2274047900",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2971708730",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W3007955273",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2982153178",
    "https://openalex.org/W2976444281",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2252238933",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2999168658",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3021457404",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3010108619"
  ],
  "abstract": null,
  "full_text": "arXiv:2006.04229v2  [cs.CL]  9 Jun 2020\nPR E -T R A IN IN G PO L I S H TR A N S F O R M E R -BA SE D LA N G UAG E\nMO D E L S AT SC A L E\nA P RE P RIN T\nSławomir Dadas\nNational Information Processing Institute\nW arsaw , Poland\nsdadas@opi.org.pl\nMichał Perełkiewicz\nNational Information Processing Institute\nW arsaw , Poland\nmperelkiewicz@opi.org.pl\nRafał Po ´swiata\nNational Information Processing Institute\nW arsaw , Poland\nrposwiata@opi.org.pl\nJune 11, 2020\nABSTRACT\nTransformer-based language models are now widely used in Na tural Language Processing (NLP).\nThis statement is especially true for English language, in w hich many pre-trained models utilizing\ntransformer-based architecture have been published in rec ent years. This has driven forward the state\nof the art for a variety of standard NLP tasks such as classiﬁc ation, regression, and sequence labeling,\nas well as text-to-text tasks, such as machine translation, question answering, or summarization. The\nsituation have been different for low-resource languages, such as Polish, however. Although some\ntransformer-based language models for Polish are availabl e, none of them have come close to the\nscale, in terms of corpus size and the number of parameters, o f the largest English-language models.\nIn this study, we present two language models for Polish base d on the popular BER T architecture.\nThe larger model was trained on a dataset consisting of over 1 billion polish sentences, or 135GB\nof raw text. W e describe our methodology for collecting the d ata, preparing the corpus, and pre-\ntraining the model. W e then evaluate our models on thirteen P olish linguistic tasks, and demonstrate\nimprovements over previous approaches in eleven of them.\nKeywords Language Modeling ·Natural Language Processing\n1 Introduction\nUnsupervised pre-training for Natural Language Processin g (NLP) has gained popularity in recent years. The goal\nof this approach is to train a model on a large corpus of unlabe led text, and then use the representations the model\ngenerates as an input for downstream linguistic tasks. The i nitial popularization of these methods was related to the\nsuccessful applications of pre-trained word vectors (embe ddings), the most notable of which include W ord2V ec [31],\nGloV e [35], and FastT ext [5]. These representations have co ntributed greatly to the development of NLP . However,\none of the main drawbacks of such tools was that the static wor d vectors did not encode contextual information. The\nproblem was addressed in later studies by proposing context -dependent representations of words based on pre-trained\nneural language models. For this purpose, several language model architectures which utilize bidirectional long shor t-\nterm memory (LSTM) layers have been introduced. The popular models such as ELMo [36], ULMFiT [18], and\nFlair [1], have led to signiﬁcant improvements in a wide vari ety of linguistic tasks. Shortly after, Devlin et al. [16]\nintroduced BER T - a different type of language model based on transformer [43] architecture. Instead of predicting\nthe next word in a sequence, BER T is trained to reconstruct th e original sentence from one in which some tokens have\nA P RE P RIN T - J U N E 11, 2020\nbeen replaced by a special mask token . Since the text representations generated by BER T have prov ed to be effective\nfor NLP problems - even those which were previously consider ed challenging, such as question answering or common\nsense reasoning - more focus has been put on transformer-bas ed language models. As a result, in the last two years\nwe have seen a number of new methods based on that idea, with so me modiﬁcations in the architecture or the training\nobjectives. The approaches that have gained wide recogniti on include RoBER T a [27], Transformer-XL [13], XLNet\n[47], Albert [25], and Reformer [20].\nThe vast majority of research on both transformer-based lan guage models and transfer learning for NLP is targeted\ntoward the English language. This progress does not transla te easily to other languages. In order to beneﬁt from recent\nadvancements, language-speciﬁc research communities mus t adapt and replicate studies conducted in English to their\nnative languages. Unfortunately, the cost of training stat e-of-the-art language models is growing rapidly [34], whic h\nmakes not only individual scientists, but also some researc h institutions unable to reproduce experiments in their own\nlanguages. Therefore, we believe that it is particularly im portant to share the results of research - especially pre-tr ained\nmodels, datasets, and source code of the experiments - for th e beneﬁt of the whole scientiﬁc community. In this article,\nwe describe our methodology for training two language model s for Polish language based on BER T architecture. The\nsmaller model follows the hyperparameters of an English-la nguage BER T -base model, and the larger version follows\nthe BER T -large model. T o the best of our knowledge, the latte r is the largest language model for Polish available to\ndate, both in terms of the number of parameters (355M) and the size of the training corpus (135GB). W e have released\nboth pre-trained models publicly 1. W e evaluate our models on several linguistic tasks in Polis h, including nine from\nthe KLEJ benchmark [39], and four additional tasks. The eval uation covers a set of typical NLP problems, such as\nbinary and multi-class classiﬁcation, textual entailment , semantic relatedness, ranking, and Named Entity Recognit ion\n(NER).\n1.1 Language-speciﬁc and multilingual transformer-based models\nIn this section we provide an overview of models based on the t ransformer architecture for languages other than En-\nglish. Apart from English, the language on which NLP researc h is most focused currently is Chinese. This is reﬂected\nin the number of pre-trained models available [46, 9, 16, 42] . Other languages for which we found publicly avail-\nable pre-trained models included: Arabic [3], Dutch [14, 15 ], Finnish [44], French [30, 26], German, Greek, Italian,\nJapanese, Korean, Malaysian, Polish, Portuguese [41], Rus sian [24], Spanish [6], Swedish, Turkish, and V ietnamese\n[32]. Models covering a few languages of the same family are a lso available, such as SlavicBER T (Bulgarian, Czech,\nPolish, and Russian) [4] and NordicBER T 2 (Danish, Norwegian, Swedish, and Finnish). The topic of mas sive multi-\nlingual models covering tens, or in some cases more than a hun dred languages, has attracted more attention in recent\nyears. The original BER T model [16] was released along with a multilingual version covering 104 languages. XLM\n[7] (ﬁfteen, seventeen and 100 languages) and XLM-R [8] (100 languages) were released in 2019. Although it was\npossible to use these models for languages in which no monoli ngual models were available, language-speciﬁc pre-\ntraining usually leads to better performance. T o date, two B ER T -base models have been made available for Polish:\nHerBER T [39] and Polbert 3, both of which utilize BER T -base architecture.\n1.2 Contributions\nOur contributions are as follows: 1) W e trained two transfor mer-based language models for Polish, consistent with\nthe BER T -base and BER T -large architectures. T o the best of o ur knowledge, the second model is the largest language\nmodel trained for Polish to date, both in terms of the number o f parameters and the size of the training corpus. 2)\nW e proposed a method for collecting and pre-processing the d ata from the Common Crawl database to obtain clean,\nhigh-quality text corpora. 3) W e conducted a comprehensive evaluation of our models on thirteen Polish linguistic\ntasks, comparing them to other available transformer-base d models, as well as recent state-of-the-art approaches. 4)\nW e made the source code of our experiments available to the pu blic, along with the pre-trained models.\n2 Language model pre-training\nIn this section, we describe our methodology for collecting and pre-processing the data used for training BER T -base\nlanguage models. W e then present the details of the training , explaining our procedure and the selection of hyperpa-\nrameters used in both models.\n1 https://github.com/sdadas/polish-roberta\n2 https://github.com/botxo/nordic_bert\n3 https://github.com/kldarek/polbert\n2\nA P RE P RIN T - J U N E 11, 2020\n2.1 T raining corpus\nTransformer-based models are known for their high capacity [19, 22], which means that they can beneﬁt from large\nquantities of text. An important step in the process of creat ing a language model, therefore, is to collect a sufﬁciently\nlarge text corpus. W e have taken into account that the qualit y of the text used for training will also affect the ﬁnal\nperformance of the model. The easiest way to collect a large l anguage-speciﬁc corpus is to extract it from Common\nCrawl - a public web archive containing petabytes of data cra wled from web pages. The difﬁculty with this approach is\nthat web-based data is often noisy and unrepresentative of t ypical language use, which could eventually have a negative\nimpact on the quality of the model. In response to this, we hav e developed a procedure for ﬁltering and cleaning the\nCommon Crawl data to obtain a high-quality web corpus. The pr ocedure is as follows:\n1. W e download full HTML pages (W ARC ﬁles in Common Crawl), an d use the resulting metadata to ﬁlter the\ndocuments written in Polish language.\n2. W e use Newspaper3k4 - a tool which implements a number of heuristics for extracti ng the main content of\nthe page, discarding any other text such as headers, footers , advertisements, menus, or user comments.\n3. W e then remove all texts shorter than 100 characters. Addi tionally, we identify documents containing the\nwords: ‘przegl ˛ adarka’, ‘ciasteczka’, ‘cookies’, or ‘jav ascript’. The presence of these words may indicate that\nthe extracted content is a description of a cookie policy, or default content for browsers without JavaScript\nenabled. W e discard all such texts if they are shorter than 50 0 characters.\n4. In the next step, we use a simple statistical language mode l (KenLM [17]), trained on a small Polish language\ncorpus to assess the quality of each extracted document. For each text, we compute the perplexity value and\ndiscard all texts with perplexity higher than 1000.\n5. Finally, we remove all duplicated texts.\nThe full training corpus we collected is approximately 135G B in size, and is composed of two components: the web\npart and the base part. For the web part, which amounts to 115G B of the corpus, we downloaded three monthly dumps\nof Common Crawl data, from November 2019 to January 2020, and followed the pre-processing steps described above.\nThe base part, which comprises the remaining 20GB, is compos ed of publicly available Polish text corpora: the Polish\nlanguage version of Wikipedia (1.5GB), the Polish Parliame ntary Corpus (5GB), and a number of smaller corpora\nfrom the CLARIN ( http://clarin-pl.eu) and OPUS ( http://opus.nlpl.eu) projects, as well as Polish books\nand articles.\n2.2 T raining procedure\nThe authors of the original BER T paper [16] proposed two vers ions of their transformer-based language model: BER T -\nlarge (more parameters and higher computational cost), and BER T -base (fewer parameters, more computationally\nefﬁcient). T o train the models for Polish language, we adapt ed the same architectures. Let L denote the number of\nencoder blocks, H denote the hidden size of the token representation, and A denote the number of attention heads.\nSpeciﬁcally, we used L = 12, H = 768, A = 12 for the base model, and L = 24, H = 1024, A = 16 for the\nlarge model. The large model was trained on the full 135GB tex t corpus, and the base model on only the 20GB\nbase part. The training procedure we employed is similar to t he one suggested in the RoBER T a pre-training approach\n[27]. Originally, BER T utilized two training objectives - M asked Language Modeling (MLM), and Next Sentence\nPrediction (NSP). W e trained our models with the MLM objecti ve, since it has been shown that NSP fails to improve\nthe performance of the pre-trained models on downstream tas ks [27]. W e also used dynamic token masking, and\ntrained the model with a larger batch size than the original B ER T . The base model was trained with a batch size of\n8000 sequences for 125 000 training steps: the large model wa s trained with a batch size of 30 000 sequences for\n50 000 steps. The reason for using such a large batch size for t he bigger model is to stabilize the training process.\nDuring our experiments, we observed signiﬁcant variations in training loss for smaller batch sizes, indicating that th e\ninitial combination of learning rate and batch size had caus ed an exploding gradient problem. T o address the issue, we\nincreased the batch size until the loss stabilized.\nBoth models were pre-trained with the Adam optimizer using t he following optimization hyperparameters: ǫ =\n1e− 6, β1 = 0.9, β2 = 0.98. W e utilized a learning rate scheduler with linear decay. Th e learning rate is ﬁrst in-\ncreased for a warm-up phase of 10 000 update steps to reach a pe ak of 7e− 4, and then linearly decreased for the\nremainder of the training. W e also mimicked the dropout appr oach of the original BER T model: a dropout of 0.1 is\napplied on all layers and attention weights. The maximum len gth of a sequence was set to 512 tokens. W e do not\ncombine sentences from the training corpus: each is treated as a separate training sample. T o encode input sequences\n4 https://newspaper.readthedocs.io/en/latest/\n3\nA P RE P RIN T - J U N E 11, 2020\ninto tokens, we employed SentencePiece [23] Byte Pair Encod ing (BPE) algorithm, and set the maximum vocabulary\nsize to 50 000 tokens.\n3 Evaluation\nIn this section, we discuss the process and results of evalua ting our language models on thirteen Polish downstream\ntasks. Nine of these tasks constitute the recently develope d KLEJ benchmark [39]; three of them have already been\nintroduced in Dadas et al. [12]; and the last, named entity re cognition, was a part of the PolEval 5 evaluation challenge.\nFirst, we compare the performance of our models with other Po lish and multilingual language models evaluated on the\nKLEJ benchmark. Next, we present detailed per-task results , comparing our models with the previous state-of-the-art\nsolutions for each of the tasks.\n3.1 T ask descriptions\nNKJP (The National Corpus of Polish (Narodowy Korpus J˛ ezyka Pol skiego)) [37] is one of the largest text corpora\nof the Polish language, consisting of texts from Polish book s, news articles, web content, and transcriptions of spoken\nconversations. A part of the corpus, known as the ‘one millio n subcorpus’, contains annotations of named entities\nfrom six categories: ‘persName’, ‘orgName’, ‘geogName’, ‘ placeName’, ‘date’, and ‘time’. The authors of the KLEJ\nbenchmark used this subset to create a named entity classiﬁc ation task [39]. First, they ﬁltered out all sentences\ncontaining entities of more than one type. Next, they random ly assigned sentences to train development and test sets\naccording to the rule that each named entity mentioned appea rs in only one of these splits. They undersample the\n‘persName’ class, and merge the ‘date’ and ‘time’ classes to increase class balance. Finally, they selected sentences\nwithout any named entity, and assigned them the ‘noEntity’ l abel. The resulting dataset consisted of 20 000 sentences\nbelonging to six classes. The task is to predict the presence and type of each named entity. Classiﬁcation accuracy is\nalso reported.\n8T A GSis a corpus created by Dadas et al. [12] for their study on the s ubject of sentence representations in Polish\nlanguage. This dataset was created automatically by extrac ting sentences from headlines and short descriptions of\narticles posted on the Polish social network, wykop.pl. It c ontains approximately 50 000 sentences, all longer than\nthirty characters, from eight popular categories: ﬁlm, his tory, food, medicine, automotive, work, sport, and technol ogy.\nThe task is to assign a sentence to one of these classes in whic h classiﬁcation accuracy is the measure.\nCBD (Cyberbullying Detection) [38] is a binary classiﬁcation t ask, the goal of which is to determine whether a T witter\nmessage constitutes a case of cyberbullying or not. This was a sub-task of task 6 in the PolEval 2019 competition. The\ndataset prepared by the competition’s organizers contains 11 041 tweets, extracted from nineteen of the most popular\nPolish T witter accounts in 2017. The F1-score was used to mea sure the performance of the models.\nDYK ‘Did you know?’ ( ‘Czy wiesz?’ ) [28] is a dataset used for the evaluation and development of Polish language\nquestion answering systems. It consists of 4721 question-a nswer pairs obtained from the Czy wiesz... Polish Wikipedia\nproject. The answer to each question was found in the linked W ikipedia article. Rybak et al. [39] used this dataset to\ndevise a binary classiﬁcation task, the goal of which is to pr edict whether the answer to the given question is correct\nor not [39]. Positive responses were additionally marked wi thin larger fragments of responded text. Negative samples\nwere selected by the BPE token overlap between a question and a possible answer. The F1-score was also reported for\nthis task.\nPSC The Polish Summaries Corpus [33] is a corpus of manually crea ted summaries of Polish language news articles.\nThe dataset contains both abstract free-word summaries and extraction-based summaries created by selecting text\nspans from the original documents. Based on PSC, [39] formul ated a text-similarity task [39]. They generate positive\npairs by matching each extractive summary with the two least similar abstractive ones in the same article. Negative\npairs were obtained by ﬁnding the two most similar abstracti ve summaries for each extractive summary, but from\ndifferent articles. T o calculate the similarity between su mmaries, they used the BPE token overlap. The F1-score was\nused for evaluation.\nPolEmo2.0 [21] is a corpus of consumer reviews obtained from four domai ns: medicine, hotels, products, and school.\nEach of the reviews is annotated with one of four labels: posi tive, negative, neutral, or ambiguous. In general, the\ntask is to choose the correct label, although here two specia l versions of the task are distinguished: PolEmo2.0-IN and\nPolEmo2.0-OUT . In PolEmo2.0-IN, both the training and test sets come from the same domains, namely medicine and\nhotels. In PolEmo2.0-OUT , however, the test set comes from t he product and school domains. In both cases, accuracy\nwas used for evaluation.\n5 http://2018.poleval.pl/index.php/tasks\n4\nA P RE P RIN T - J U N E 11, 2020\nAllegro Reviews (AR) [39] is a sentiment analysis dataset of product reviews from the e-commerce marketplace,\nallegro.pl. Each review has a rating on a ﬁve-point scale, in which one is negative, and ﬁve is positive. The task is to\npredict the rating of a given review . The macro-average of th e mean absolute error per class (wMAE) is applied for\nevaluation.\nCDSC (The Compositional Distributional Semantics Corpus) [45] is a corpus of 10 000 human-annotated sentence\npairs for semantic relatedness and entailment, in which ima ge captions from forty-six thematic groups were used as\nsentences. T wo tasks are proposed based on this dataset. The CDSC-R problem involves predicting the relatedness\nbetween a pair of sentences, on a scale of zero to ﬁve, in which zero indicates that the sentences are not related, and ﬁve\nindicates that they are highly related. In this task, the Spe arman correlation is used as an evaluation measure. CDSC-\nE’s task is to classify whether the premise entails the hypot hesis (entailment), negates the hypothesis (contradictio n),\nor is unrelated (neutral). For this task, accuracy is report ed.\nSICK [12] is a manually translated Polish language version of the English Natural Language Inference (NLI) cor-\npus, SICK (Sentences Involving Compositional Knowledge) [ 29], and consists of 10 000 sentence pairs. As with the\nCDSC dataset, two tasks can also be distinguished here. SICK -R is the task of predicting the probability distribution\nof relatedness scores (ranging from 1 to 5) for the sentence p air, in which the Spearman correlation is used for evalu-\nation. SICK-E is a multiclass classiﬁcation problem in whic h the relationship between two sentences is classiﬁed as\nentailment, contradiction, or neutral. Accuracy is used on ce again to measure performance.\nPolEval-NER 2018 [2] was task 2 in the PolEval 2018 competition, the goal of whi ch was to detect and assign the\ncorrect category and subcategory (if applicable) to a found named entity. In this study the task was simpliﬁed, as only\nthe main categories had to be found. The effectiveness of the models is veriﬁed by the F1-score measure. This task\nwas prepared on the basis of the NKJP dataset previously pres ented.\n3.2 T ask-speciﬁc ﬁne-tuning\nT o evaluate our language models on downstream tasks, we ﬁne- tuned them separately for each task. In our experiments,\nwe encounter three types of problem: classiﬁcation, regres sion, and Named Entity Recognition (NER). In classiﬁcation\ntasks, the model is expected to predict a label from a set of tw o or more classes. Regression concerns the prediction\nof a continuous numerical value. NER is a special case of sequ ence tagging, i.e. predicting a label for each element\nin a sequence. The dataset for each problem consists of train ing and test parts, and in most cases also includes a\nvalidation part. The general ﬁne-tuning procedure is as fol lows: we train our model on the training part of the dataset\nfor a speciﬁc number of epochs. If the validation set is avail able, we compute the validation loss after each epoch, and\nselect the model checkpoint with the best validation loss. F or datasets without a validation set, we select the last epoc h\ncheckpoint. Then, we perform an evaluation on the test set us ing the selected checkpoint.\nIn the case of classiﬁcation and regression tasks, we attach an additional fully-connected layer to the output of the\n[CLS] token, which always remains in the ﬁrst position of a sequenc e. For classiﬁcation, the number of outputs for\nthis layer is equal to the number of classes, and the softmax a ctivation function is used. For regression, it is a linear\nlayer with a single output. The models are ﬁne-tuned with the Adam optimizer using the following hyperparameters:\nǫ = 1e− 6, β1 = 0.9, β2 = 0.98. A learning rate scheduler with polynomial decay is utilize d. The ﬁrst 6% of the\ntraining steps are reserved for the warm-up phase, in which t he learning rate is gradually increased to reach a peak of\n1e− 5. By default, we train for ten epochs with a batch size of sixte en sequences. The speciﬁc ﬁne-tuning steps and\nexceptions to the procedure are discussed below:\n• Classiﬁcation on imbalanced datasets – Some of the binary classiﬁcation datasets considered in th e evaluation,\nsuch as CBD, D YK, and PSC, are imbalanced, which means that th ey contain signiﬁcantly fewer samples of the ﬁrst\nclass than of the second class. T o counter this imbalance, we utilize a simple resampling technique: samples for the\nminority class in the training set are duplicated, and some s amples for the majority class are randomly discarded. W e\nset the resampling factor to 3 for the minority class, and 1 (D YK, PSC) or 0.75 (CBD) respectively for the majority\nclass. Additionally, we increase the batch size for those ta sks to thirty-two.\n• Regression - In many cases, a regression task is restricted to a speciﬁc r ange of values for which the prediction is\nvalid. For example, Allegro Reviews contains user reviews w ith ratings between one and ﬁve stars. For ﬁne-tuning,\nwe scale all the outputs of regression models to be within the range of [0, 1], and then rescale them to their original\nrange during evaluation. Before rescaling, any negative pr ediction is set to 0, and any prediction greater that 1 is\nlimited to 1.\n• Named entity recognition - Since sequence tagging, in which the model is expected to ge nerate per-token pre-\ndictions, is different from simple classiﬁcation or regres sion tasks, we decided to adapt an existing named entity\nrecognition approach for ﬁne-tuning using our language mod els. For this purpose, we employed a method from\nShibuya and Hovy [40], who proposed a transformer-based nam ed entity recognition model with a Conditional Ran-\n5\nA P RE P RIN T - J U N E 11, 2020\nT able 1: Results on the KLEJ benchmark.\nModel A verage NKJP CDSC-E CDSC-R CBD PE2-I PE2-O D YK PSC AR\nBase models\nmBER T 79.5 91.4 93.8 92.9 40.0 85.0 66.6 64.2 97.9 83.3\nSlavicBER T 79.8 93.3 93.7 93.3 43.1 87.1 67.6 57.4 98.3 84.3\nXLM-100 79.9 91.6 93.7 91.8 42.5 85.6 69.8 63.0 96.8 84.2\nXLM-17 80.2 91.9 93.7 92.0 44.8 86.3 70.6 61.8 96.3 84.5\nHerBER T 80.5 92.7 92.5 91.9 50.3 89.2 76.3 52.1 95.3 84.5\nXLM-R base 81.5 92.1 94.1 93.3 51.0 89.5 74.7 55.8 98.2 85.2\nPolbert 81.7 93.6 93.4 93.8 52.7 87.4 71.1 59.1 98.6 85.2\nOur model 85.3 93.9 94.2 94.0 66.7 90.6 76.3 65.9 98.8 87.8\nLarge models\nXLM-R large 87.5 94.1 94.4 94.7 70.6 92.4 81.0 72.8 98.9 88.4\nOur model 87.8 94.5 93.3 94.9 71.1 92.8 82.4 73.4 98.8 88.8\ndom Fields (CRF) inference layer, and multiple V iterbi-dec oding steps to handle nested entities. In our experiments,\nwe used the same hyperparameters as the authors.\n3.3 Results and discussion\nIn this section, we demonstrate the results of evaluating ou r language models on downstream tasks. W e repeated\nthe ﬁne-tuning of the models for each task ﬁve times. The scor es reported are the median values of those ﬁve runs.\nT able 1 demonstrates the evaluation results on the KLEJ benc hmark, in comparison with other available Polish and\nmultilingual transformer-based models. The results of oth er approaches are taken from the KLEJ leaderboard. W e\nsplit the table into two sections, comparing the BER T -base a nd BER T -large architectures separately. W e can observe\nthat there is a wider selection of base models, and most of the m are multilingual, such as the original multilingual\nBER T (mBER T) [16], SlavicBER T [4], XLM [7], and XLM-R [8]. Th e only models pre-trained speciﬁcally for Polish\nlanguage are HerBER T [39] and Polbert. Among the base models , our approach outperforms others by a signiﬁcant\nmargin. In the case of large models, only the XLM-RoBER T a (XL M-R) pre-trained model has been available until\nnow . XLM-RoBER T a is a recently published multilingual tran sformer trained on 2.5TB of data in 100 languages. It\nhas been shown to be highly competitive against monolingual models. A direct comparison with our Polish language\nmodel demonstrates a consistent advantage of our model - it h as achieved better results in seven of the nine tasks\nincluded in the KLEJ benchmark.\nT able 2 shows a more detailed breakdown of the evaluation res ults, and includes all the tasks from the KLEJ benchmark,\nand four additional tasks: SICK-R, SICK-R, 8T AGS, and PolEv al-NER 2018. For each task, we deﬁne the task type\n(classiﬁcation, regression, or sequence tagging), the met ric used for evaluation, the previous state-of-the-art, an d\nour results including the absolute difference to the SOT A. T he competition between XLM-R and our large model\ndominates the results, since both models have led to signiﬁc ant improvements in linguistic tasks for Polish language.\nIn some cases, the improvement over previous approaches is g reater than 10%. For example, the CDB task was a part\nof the PolEval 2019 competition, in which the winning soluti on by Czapla et al. [10] achieved an F1-score of 58.6.\nBoth our model and the XLM-R large model outperform that by at least twelve points, achieving an F1-score of over\n70. The comparison for the named entity recognition task is a lso interesting. The previous state-of-the-art solution b y\nDadas [11] is a model that combined neural architecture with external knowledge sources, such as entity lexicons or\na specialized entity linking module based on data from Wikip edia. Our language model managed to outperform this\nmethod by 3.8 points without using any structured external k nowledge. In summary, our model has demonstrated an\nimprovement over existing methods in eleven of the thirteen tasks.\n4 Conclusions\nW e have presented two transformer-based language models fo r Polish, pre-trained using a combination of publicly\navailable text corpora and a large collection of methodical ly pre-processed web data. W e have shown the effectiveness\n6\nA P RE P RIN T - J U N E 11, 2020\nT able 2: Detailed results for Polish language downstream ta sks. In some cases, we used the datasets and task deﬁnitions\nfrom the KLEJ benchmark, which are different from the origin al tasks they were based on (they have been reformulated\nor otherwise modiﬁed by the benchmark authors). W e denote su ch tasks with (KLEJ) to emphasize that the evaluation\nwas performed on the KLEJ version of the data. The abbreviate d task types are: C - classiﬁcation, R - regression, and\nST - sequence tagging.\nT ask Metric Previous SOT A Base\nmodel\nLarge\nmodel\nMulti-class classiﬁcation\nNKJP (KLEJ) C Accuracy XLM-R large [8] 94.1 93.9 (−0.2) 94.5 (+0.4)\n8T AGS C Accuracy ELMo [12] 71.4 77.2 (+5.8) 80.8 (+9.4)\nBinary classiﬁcation\nCBD C F1-score XLM-R large [8] 70.6 66.7 (−2.9) 71.1 (+0.5)\nD YK (KLEJ) C F1-score XLM-R large [8] 72.8 65.9 (−6.9) 73.4 (+0.6)\nPSC (KLEJ) C F1-score XLM-R large [8] 98.9 98.8 (−0.1) 98.8 (−0.1)\nSentiment analysis\nPolEmo2.0-IN C Accuracy XLM-R large [8] 92.4 90.6 (−1.8) 92.8 (+0.4)\nPolEmo2.0-OUT C Accuracy XLM-R large [8] 81.0 76.3 (−4.7) 82.4 (+1.4)\nAllegro Reviews R 1-wMAE XLM-R large [8] 88.4 87.8 (−1.0) 88.8 (+0.4)\nT extual entailment\nCDSC-E C Accuracy XLM-R large [8] 94.4 94.2 (−0.2) 93.3 (−1.1)\nSICK-E C Accuracy LASER [12] 82.2 86.1 (+3.9) 87.7 (+5.5)\nSemantic relatedness\nCDSC-R R Spearman XLM-R large [8] 94.7 94.0 (−0.7) 94.9 (+0.2)\nSICK-R R Spearman USE [12] 75.8 82.3 (+6.5) 85.6 (+9.8)\nNamed entity recognition\nPoleval-NER 2018 ST F1-score Dadas [11] 86.2 87.9 (+1.7) 90.0 (+3.8)\nof our models by comparing them with other transformer-base d approaches and recent state-of-the-art approaches.\nW e conducted a comprehensive evaluation on a wide set of Poli sh linguistic tasks, including binary and multi-class\nclassiﬁcation, regression, and sequence labeling. In our e xperiments, the larger model performed better than other\nmethods in eleven of the thirteen cases. T o accelerate resea rch on NLP for Polish language, we have released the\npre-trained models publicly.\nReferences\n[1] Alan Akbik, Duncan Blythe, and Roland V ollgraf. Context ual string embeddings for sequence labeling. In\nProceedings of the 27th International Conference on Comput ational Linguistics , pages 1638–1649, 2018.\n[2] Estera Małek Aleksander W awer. Results of the PolEval 20 18 Shared T ask 2: Named Entity Recognition. Pro-\nceedings of the P olEval 2018 W orkshop , pages 53–62, 2018.\n[3] Wissam Antoun, Fady Baly, and Hazem Hajj. Arabert: Trans former-based model for arabic language understand-\ning. arXiv preprint arXiv:2003.00104 , 2020.\n[4] Mikhail Arkhipov, Maria Troﬁmova, Y uri Kuratov, and Ale xey Sorokin. Tuning multilingual transformers for\nlanguage-speciﬁc named entity recognition. In Proceedings of the 7th W orkshop on Balto-Slavic Natural Lan -\nguage Processing , pages 89–93, Florence, Italy, August 2019. Association fo r Computational Linguistics. doi:\n10.18653/v1/W19-3712. URL https://www.aclweb.org/anthology/W19-3712.\n[5] Piotr Bojanowski, Edouard Grave, Armand Joulin, and T om as Mikolov. Enriching word vectors with subword\ninformation. T ransactions of the Association for Computational Linguis tics, 5:135–146, 2017.\n7\nA P RE P RIN T - J U N E 11, 2020\n[6] José Cañete, Gabriel Chaperon, Rodrigo Fuentes, and Jor ge Pérez. Spanish pre-trained bert model and evaluation\ndata. In Practical ML for Developing Countries W orkshop @ ICLR 2020 , 2020.\n[7] Alexis Conneau and Guillaume Lample. Cross-lingual lan guage model pretraining. In H. W al-\nlach, H. Larochelle, A. Beygelzimer, F . d’ Alché-Buc, E. Fox , and R. Garnett, editors, Advances in\nNeural Information Processing Systems 32 , pages 7059–7069. Curran Associates, Inc., 2019. URL\nhttp://papers.nips.cc/paper/8928-cross-lingual-lang uage-model-pretraining.pdf.\n[8] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, V ish rav Chaudhary, Guillaume W enzek, Francisco Guz-\nman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and V eselin Stoyanov. Unsupervised cross-lingual represen-\ntation learning at scale. arXiv preprint arXiv:1911.02116 , 2019.\n[9] Y iming Cui, W anxiang Che, Ting Liu, Bing Qin, Ziqing Y ang , Shijin W ang, and Guoping Hu. Pre-training with\nwhole word masking for chinese bert. arXiv preprint arXiv:1906.08101 , 2019.\n[10] Piotr Czapla, Sylvain Gugger, Jeremy Howard, and Marci n Kardas. Universal language model ﬁne-tuning for\npolish hate speech detection. Proceedings of the P olEval 2019 W orkshop , page 149, 2019.\n[11] Slawomir Dadas. Combining neural and knowledge-based approaches to named entity recognition in polish.\nInternational Conference on Artiﬁcial Intelligence and So ft Computing , pages 39–50, 2019.\n[12] Slawomir Dadas, Michał Perełkiewicz, and Rafał Po ´swiata. Evaluation of Sentence Representations in Pol-\nish. In Proceedings of The 12th Language Resources and Evaluation C onference, pages 1674–1680, Mar-\nseille, France, May 2020. European Language Resources Asso ciation. ISBN 979-10-95546-34-4. URL\nhttps://www.aclweb.org/anthology/2020.lrec-1.207.\n[13] Zihang Dai, Zhilin Y ang, Y iming Y ang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL:\nAttentive language models beyond a ﬁxed-length context. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages 2978–2988, Florence, Italy, July 2019. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/P19-1285. URL https://www.aclweb.org/anthology/P19-1285.\n[14] Wietse de Vries, Andreas van Cranenburgh, Arianna Bisa zza, T ommaso Caselli, Gertjan van Noord, and Malvina\nNissim. Bertje: A dutch bert model. arXiv preprint arXiv:1912.09582 , 2019.\n[15] Pieter Delobelle, Thomas Winters, and Bettina Berendt . Robbert: a dutch roberta-based language model. arXiv\npreprint arXiv:2001.06286 , 2020.\n[16] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina T outanova. BER T: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Cha pter\nof the Association for Computational Linguistics: Human La nguage T echnologies, V olume 1 (Long and Short\nP apers), pages 4171–4186, Minneapolis, Minnesota, June 2019. Asso ciation for Computational Linguistics. doi:\n10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423.\n[17] Kenneth Heaﬁeld. Kenlm: Faster and smaller language mo del queries. Proceedings of the Sixth W orkshop on\nStatistical Machine T ranslation , pages 187–197, 2011.\n[18] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In Proceedings\nof the 56th Annual Meeting of the Association for Computatio nal Linguistics (V olume 1: Long P apers) , pages\n328–339, 2018.\n[19] Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. What do es BER T learn about the structure of language? In\nACL 2019 - 57th Annual Meeting of the Association for Computa tional Linguistics , Florence, Italy, July 2019.\nURL https://hal.inria.fr/hal-02131630.\n[20] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Ref ormer: The efﬁcient transformer. In International\nConference on Learning Representations , 2020.\n[21] Jan Koco ´n, Piotr Miłkowski, and Monika Za ´sko-Zieli ´nska. Multi-level sentiment analysis of PolEmo 2.0: Ex-\ntended corpus of multi-domain consumer reviews. In Proceedings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL) , pages 980–991, Hong Kong, China, November 2019. Associati on for Compu-\ntational Linguistics. doi: 10.18653/v1/K19-1092. URL https://www.aclweb.org/anthology/K19-1092.\n8\nA P RE P RIN T - J U N E 11, 2020\n[22] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Ru mshisky. Revealing the dark secrets of BER T.\nIn Proceedings of the 2019 Conference on Empirical Methods in N atural Language Processing and the 9th\nInternational Joint Conference on Natural Language Proces sing (EMNLP-IJCNLP) , pages 4365–4374, Hong\nKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1445. URL\nhttps://www.aclweb.org/anthology/D19-1445.\n[23] T aku Kudo and John Richardson. SentencePiece: A simple and language independent subword to-\nkenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing: System Dem onstrations, pages 66–71, Brussels, Bel-\ngium, November 2018. Association for Computational Lingui stics. doi: 10.18653/v1/D18-2012. URL\nhttps://www.aclweb.org/anthology/D18-2012.\n[24] Y uri Kuratov and Mikhail Arkhipov. Adaptation of deep b idirectional multilingual transformers for russian\nlanguage. arXiv preprint arXiv:1905.07213 , 2019.\n[25] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin G impel, Piyush Sharma, and Radu Soricut. Albert:\nA lite bert for self-supervised learning of language repres entations. In International Conference on Learning\nRepresentations, 2020.\n[26] Hang Le, Loïc V ial, Jibril Frej, V incent Segonne, Maxim in Coavoux, Benjamin Lecouteux, Alexandre Allauzen,\nBenoît Crabbé, Laurent Besacier, and Didier Schwab. Flaube rt: Unsupervised language model pre-training for\nfrench. arXiv preprint arXiv:1912.05372 , 2019.\n[27] Y inhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Jo shi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and V eselin Stoyanov. Roberta: A robustly opt imized bert pretraining approach. arXiv preprint\narXiv:1907.11692 , 2019.\n[28] Michał Marci ´nczuk, Marcin Ptak, Adam Radziszewski, and Maciej Piasecki . Open Dataset for Development of\nPolish Question Answering Systems. In Z. V etulani and H. Usz koreit, editors, Proceedings of Human Language\nT echnologies as a Challenge for Computer Science and Lingui stics’13 , pages 479–483, Pozna ´n, 2013. Fundacja\nUAM.\n[29] Marco Marelli, Stefano Menini, Marco Baroni, Luisa Ben tivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. A SICK cure for the evaluation of compositional dis tributional semantic models. In Proceed-\nings of the Ninth International Conference on Language Reso urces and Evaluation (LREC’14) , pages\n216–223, Reykjavik, Iceland, May 2014. European Language R esources Association (ELRA). URL\nhttp://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf.\n[30] Louis Martin, Benjamin Muller, Pedro Javier Ortiz Suár ez, Y oann Dupont, Laurent Romary, Éric V illemonte\nde la Clergerie, Djamé Seddah, and Benoît Sagot. Camembert: a tasty french language model. arXiv preprint\narXiv:1911.03894 , 2019.\n[31] T omas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado , and Jeff Dean. Distributed representations of words\nand phrases and their compositionality. In Advances in neural information processing systems , pages 3111–3119,\n2013.\n[32] Dat Quoc Nguyen and Anh Tuan Nguyen. Phobert: Pre-train ed language models for vietnamese. arXiv preprint\narXiv:2003.00744 , 2020.\n[33] Maciej Ogrodniczuk and Mateusz Kope ´c. The polish summaries corpus. In Nicoletta Calzolari (Con ference\nChair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno,\nJan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Langu age Re-\nsources and Evaluation (LREC’14) , Reykjavik, Iceland, may 2014. European Language Resource s Association\n(ELRA). ISBN 978-2-9517408-8-4.\n[34] T ony Peng. The staggering cost of training sota ai model s, Jun 2019. URL\nhttps://syncedreview.com/2019/06/27/the-staggering- cost-of-training-sota-ai-models/ .\n[35] Jeffrey Pennington, Richard Socher, and Christopher M anning. Glove: Global vectors for word representation.\nIn Proceedings of the 2014 conference on empirical methods in n atural language processing (EMNLP) , pages\n1532–1543, 2014.\n9\nA P RE P RIN T - J U N E 11, 2020\n[36] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardne r, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language T echnologies, V olume 1 (Long P a-\npers), volume 1, pages 2227–2237, 2018.\n[37] Adam Przepiórkowski, Mirosław Banko, Rafał L Górski, a nd Barbara Lewandowska-T omaszczyk. Narodowy\nKorpus Jezyka Polskiego [Eng.: National Corpus of Polish]. W ydawnictwo Naukowe PWN, W arsaw , 2012.\n[38] Michal Ptaszynski, Agata Pieciukiewicz, and Paweł Dyb ała. Results of the PolEval 2019 Shared T ask 6: First\nDataset and Open Shared T ask for Automatic Cyberbullying De tection in Polish T witter. Proceedings of the\nP olEval 2019 W orkshop , page 89, 2019.\n[39] Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and Ire neusz Gawlik. KLEJ: Comprehensive Benchmark for\nPolish Language Understanding. arXiv preprint arXiv:2005.00630 , 2020.\n[40] T akashi Shibuya and Eduard Hovy. Nested named entity re cognition via second-best sequence learning and\ndecoding. arXiv preprint arXiv:1909.02250 , 2019.\n[41] Fabio Souza, Rodrigo Nogueira, and Roberto Lotufo. Por tuguese named entity recognition using bert-crf. arXiv\npreprint arXiv:1909.10649 , 2019. URL http://arxiv.org/abs/1909.10649.\n[42] Y u Sun, Shuohuan W ang, Y ukun Li, Shikun Feng, Hao Tian, H ua Wu, and Haifeng W ang. Ernie 2.0: A continual\npre-training framework for language understanding. arXiv preprint arXiv:1907.12412 , 2019.\n[43] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob Uszko reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems , pages\n5998–6008, 2017.\n[44] Antti V irtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, J uhani Luotolahti, T apio Salakoski, Filip Ginter, and\nSampo Pyysalo. Multilingual is not enough: Bert for ﬁnnish. arXiv preprint arXiv:1912.07076 , 2019.\n[45] Alina Wróblewska and Katarzyna Krasnowska-Kiera ´s. Polish evaluation dataset for compositional distributi onal\nsemantics models. In Proceedings of the 55th Annual Meeting of the Association fo r Computational Linguis-\ntics (V olume 1: Long P apers) , pages 784–792, V ancouver, Canada, July 2017. Association for Computational\nLinguistics. doi: 10.18653/v1/P17-1073. URL https://www.aclweb.org/anthology/P17-1073.\n[46] Liang Xu, Xuanwei Zhang, and Qianqian Dong. CLUECorpus 2020: A large-scale chinese corpus for pre-\ntraininglanguage model. arXiv preprint arXiv:2003.01355 , 2020.\n[47] Zhilin Y ang, Zihang Dai, Y iming Y ang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Gen-\neralized autoregressive pretraining for language underst anding. In Advances in neural information processing\nsystems, pages 5754–5764, 2019.\n10\nA P RE P RIN T - J U N E 11, 2020\nA Non-English transformer-based language models\nProject Languages Paper Project URL\nArabic-BER T Arabic - github.com/alisafaya/Arabic-BER T\nAraBER T Arabic [3] github.com/aub-mind/arabert\nClueCorpus2020 Chinese [46] github.com/CLUEbenchmark/C LUECorpus2020\nChinese BER T Chinese [9] github.com/ymcui/Chinese-BER T - wwm\nGoogle BER T Chinese [16] github.com/google-research/ber t\nERNIE 2.0 Chinese [42] github.com/PaddlePaddle/ERNIE\nBER Tje Dutch [14] github.com/wietsedv/bertje\nRoBBER T Dutch [15] ipieter.github.io/blog/robbert\nFinnish BER T Finnish [44] github.com/TurkuNLP/FinBER T\nCamemBER T French [30] camembert-model.fr\nFlauBER T French [26] github.com/getalp/Flaubert\nGerman BER T German - deepset.ai/german-bert\nGreekBER T Greek - github.com/nlpaueb/greek-bert\nUmBER T o Italian - github.com/musixmatchresearch/umbert o\nGilBER T o Italian - github.com/idb-ita/GilBER T o\nJapanese BER T Japanese - github.com/yoheikikuta/bert-ja panese\nJapanese BER T Japanese - github.com/cl-tohoku/bert-japa nese\nKoBER T Korean - github.com/SKTBrain/KoBER T\nMalaya Malaysian - github.com/huseinzol05/Malaya/\nNordic BER T Nordic (4) - github.com/botxo/nordic_bert\nPolBER T Polish - github.com/kldarek/polbert\nHerBER T Polish [39] klejbenchmark.com\nPortuguese BER T Portuguese [41] github.com/neuralmind-a i/portuguese-bert\nRuBER T Russian [24] github.com/deepmipt/DeepPavlov\nSlavicBER T Slavic (4) [4] github.com/deepmipt/Slavic-BE R T -NER\nBETO Spanish [6] github.com/dccuchile/beto\nSwedish BER T Swedish - github.com/Kungbib/swedish-bert- models\nBER Tsson Swedish - huggingface.co/jannesg/bertsson\nBER Turk Turkish - github.com/stefan-it/turkish-bert\nPhoBER T V ietnamese [32] github.com/V inAIResearch/PhoBE R T\n11",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8026028275489807
    },
    {
      "name": "Computer science",
      "score": 0.7855859994888306
    },
    {
      "name": "Natural language processing",
      "score": 0.7509481906890869
    },
    {
      "name": "Automatic summarization",
      "score": 0.6949740648269653
    },
    {
      "name": "Machine translation",
      "score": 0.6918073296546936
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6685278415679932
    },
    {
      "name": "Language model",
      "score": 0.6565561294555664
    },
    {
      "name": "Architecture",
      "score": 0.49477285146713257
    },
    {
      "name": "Question answering",
      "score": 0.47720345854759216
    },
    {
      "name": "Natural language",
      "score": 0.4457213282585144
    },
    {
      "name": "Linguistics",
      "score": 0.33706969022750854
    },
    {
      "name": "History",
      "score": 0.10315585136413574
    },
    {
      "name": "Engineering",
      "score": 0.09013909101486206
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210139285",
      "name": "National Information Processing Institute",
      "country": "PL"
    }
  ],
  "cited_by": 8
}