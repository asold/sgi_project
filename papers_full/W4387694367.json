{
    "title": "Leveraging Large Language Models for Predictive Chemistry",
    "url": "https://openalex.org/W4387694367",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2606402197",
            "name": "Kevin Maik Jablonka",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne",
                "Institute of Polymers"
            ]
        },
        {
            "id": "https://openalex.org/A2769065378",
            "name": "Philippe Schwaller",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A3182449793",
            "name": "Andres Ortega Guerrero",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2167152368",
            "name": "Berend Smit",
            "affiliations": [
                "École Polytechnique Fédérale de Lausanne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6800751262",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4380686968",
        "https://openalex.org/W2953641512",
        "https://openalex.org/W6603664484",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W4226197958",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W2977044154",
        "https://openalex.org/W4289782800",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W6644359581",
        "https://openalex.org/W2096541451",
        "https://openalex.org/W3048908832",
        "https://openalex.org/W4224231483",
        "https://openalex.org/W2784918212",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W2785942661",
        "https://openalex.org/W4235629934",
        "https://openalex.org/W4365211686",
        "https://openalex.org/W3163360581",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4226050570",
        "https://openalex.org/W4385671288",
        "https://openalex.org/W3198449425",
        "https://openalex.org/W3100220443",
        "https://openalex.org/W3030068589",
        "https://openalex.org/W4312129726",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W3098269892",
        "https://openalex.org/W4318952054",
        "https://openalex.org/W3045928028",
        "https://openalex.org/W4214868967",
        "https://openalex.org/W4284882687",
        "https://openalex.org/W3023402054",
        "https://openalex.org/W3181860256",
        "https://openalex.org/W4386168831",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4287335333",
        "https://openalex.org/W2883583109",
        "https://openalex.org/W4244810327",
        "https://openalex.org/W4385572894",
        "https://openalex.org/W4365211638",
        "https://openalex.org/W4306179830",
        "https://openalex.org/W4251541395",
        "https://openalex.org/W4319310661",
        "https://openalex.org/W4362664882",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W1975875968",
        "https://openalex.org/W2911997094",
        "https://openalex.org/W4281619372",
        "https://openalex.org/W4379184641",
        "https://openalex.org/W2800793736",
        "https://openalex.org/W4285776608",
        "https://openalex.org/W3116783766",
        "https://openalex.org/W4378942305",
        "https://openalex.org/W4283031227",
        "https://openalex.org/W3030978062",
        "https://openalex.org/W4394172169",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2554191423",
        "https://openalex.org/W2116105292",
        "https://openalex.org/W2955219525",
        "https://openalex.org/W2908350418",
        "https://openalex.org/W4302009014",
        "https://openalex.org/W4233109831",
        "https://openalex.org/W4311409687",
        "https://openalex.org/W4242469704",
        "https://openalex.org/W2790960441",
        "https://openalex.org/W4225591000",
        "https://openalex.org/W2160592148",
        "https://openalex.org/W2008505552",
        "https://openalex.org/W4242308659",
        "https://openalex.org/W4311281379",
        "https://openalex.org/W4385571886",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4305016511"
    ],
    "abstract": "Machine learning has revolutionized many fields and has recently found applications in chemistry and materials science. The small datasets commonly found in chemistry sparked the development of sophisticated machine-learning approaches that incorporate chemical knowledge for each application and, therefore, require much expertise to develop. Here, we show that large language models trained on vast amounts of text extracted from the internet can easily be adapted to solve various tasks in chemistry and materials science by fine-tuning them to answer chemical questions in natural language with the correct answer. We compared this approach with dedicated machine-learning models for many applications spanning properties of molecules and materials to the yield of chemical reactions. Surprisingly, this approach performs comparable to or even outperforms the conventional techniques---particularly in the low data limit. In addition, we can perform inverse design successfully by simply inverting the questions. The high performance, especially for small data sets, combined with the ease of use, can fundamentally impact how we leverage machine learning in the chemical and material sciences. Next to a literature search, querying a foundation model might become a routine way to bootstrap a project by leveraging the collective knowledge encoded in these foundation models or to provide a baseline for predictive tasks.",
    "full_text": "Leveraging Large Language Models for\nPredictive Chemistry\nKevinMaik Jablonka1,2,3,4,Philippe Schwaller5, Andres Ortega-Guerrero1,and\nBerend Smit1,*\n1Laboratory of Molecular Simulation (LSMO), École Polytechnique Fédérale de Lausanne (EPFL), Rue de l’Industrie 17,\nCH-1951 Sion, Switzerland\n2Center for Energy and Environmental Chemistry Jena (CEEC Jena), Friedrich Schiller University Jena, Philosophenweg\n7a, 07743 Jena, Germany\n3Laboratory of Organic and Macromolecular Chemistry (IOMC), Friedrich Schiller University Jena, Humboldtstrasse 10,\n07743 Jena, Germany\n4Helmholtz Institute for Polymers in Energy Applications, 07743 Jena, Germany\n5Laboratory of Artificial Chemical Intelligence (LIAC), École Polytechnique Fédérale de Lausanne (EPFL), Switzerland\n*berend.smit@epfl.ch\nAbstract Machine learning has revolutionized many fields and has recently found applica-\ntions in chemistry and materials science. The small datasets commonly found in chemistry\nsparkedthedevelopmentofsophisticatedmachine-learningapproachesthatincorporatechem-\nicalknowledgeforeachapplicationand,therefore,requiremuchexpertisetodevelop. Here,we\nshow that large language models trained on vast amounts of text extracted from the internet\ncan easily be adapted to solve various tasks in chemistry and materials science by fine-tuning\nthem to answer chemical questions in natural language with the correct answer. We compared\nthis approach with dedicated machine-learning models for many applications spanning proper-\nties of molecules and materials to the yield of chemical reactions. Surprisingly, this approach\nperforms comparable to or even outperforms the conventional techniques—particularly in the\nlow data limit. In addition, we can perform inverse design successfully by simply inverting the\nquestions. Thehighperformance,especiallyforsmalldatasets,combinedwiththeeaseofuse,\ncan fundamentally impact how we leverage machine learning in the chemical and material sci-\nences. Nexttoaliteraturesearch,queryingafoundationmodelmightbecomearoutinewayto\nbootstrapaprojectbyleveragingthecollectiveknowledgeencodedinthesefoundationmodels\nor to providea baseline for predictivetasks.\n1\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nMain\nOneofthefascinatingadvancesinmachinelearninghasbeenthedevelopmentoflargelanguage\nmodels (LLMs), so-called foundation models.1–6 These models are appealing because of their\nsimplicity;givenaphrase,theyreturntextthatcompletesphrasesinnaturallanguagesuchthat,\nin manyinstances, one cannot eventell that a machine wrote it.\nFrom a scientific point of view, the most striking examples are that these foundation mod-\nels can write sensible abstracts for scientific articles or even code for particular programming\ntasks.7–12 Recently,ithasbeenshownthatthesemodelscanalsosolverelativelysimpletabular\nregression and classification tasks.13 However, as these models were not explicitly trained on\nthese tasks, it is a remarkable result.5\nThat these models can solve simple tasks they are not trained for made us wonder whether\ntheycanalsoanswerscientificquestionsforwhichwedonothaveananswer. Asmostchemistry\nproblems can be represented in text form, we should be able to train these models to answer\nquestions that chemists have. For example, “if I change the metal in my metal-organic frame-\nwork,willitbestableinwater?” Suchquestionsareoftenimpossibletoanswerusingtheoryor\nrequire highly sophisticated simulations or experiments.\nWewillalwayshaveverylittle(experimental)dataforchemistryandmaterialscienceapplica-\ntions. Hence, it is important that meaningful results can already be obtained with tens to hun-\ndreds of data points. We know from prior work on applications on text classification or genera-\ntionthatthisworksparticularlywellusingmodelsfromtheGenerativePre-trainedTransformer\n3 (GPT-3) family,5 which were trained by the artificial intelligence company OpenAI. In this\nwork, we show that these models—when provided with example data—perform surprisingly\nwell for various chemistry questions, even outperforming the state-of-the-art machine learn-\ning models specifically developed for these tasks. It is important to realize that while language\nmodelshavebeenusedinchemistrybeforetopredictproperties 14–17 ordesignmolecules, 18–20\nthey have conventionally been pretrained on chemistry-specific tasks. In contrast, the models\nwe investigate here have been trained on text corpi compiled mainly from the internet but still\ncanadapttovarioustasks. WhileWhite et al.[8]hasprobedtheinherentchemistryknowledge\nof LLMs, we focus on how those models perform when they are fine-tuned—i.e., the weights\nareupdated—onsometask-specificdataset. Notethatthistask-specificfine-tuningmakesthe\nmodels less dependent on the prompt structure than in-contextlearning.21,22\nWe benchmark our model on various datasets and applications to illustrate that these mod-\nels can answer a wide range of scientific questions—ranging from the properties of materials,\nhowtosynthesizematerials,andevenhowtodesignmaterials. Inselectingthesequestions,we\nincluded some that have been addressed with machine learning. This allowed us to benchmark\nagainst state-of-the-art machine learning approaches specifically developed for these applica-\ntions.\n2\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nDatasets Tasks\n“What is the \ntransition wavelength of \n2-phenyldiazenylaniline”\n“What is the lipophilicity \nof COc1cc(N2CCN(C)CC2)c3nc\n(cc(N(C)C)c3c1)C(=O)Nc4ccc\n(cc4)N5CCOCC5?”\n“What is a molecule \nwith E isomer transition \nwavelength of 325 nm,\nZ isomer transition \nwavelength of 286 nm?”\nMolecules\nMaterials\nReactions\nCl\nNN\n“low”\nClassification Regression Inverse Design\n3.3\nGPT -3 GPT -3 GPT -3\nNO\nCl\nN\nP\nO\nO\nO\nN N\nN\nN\nNN NH2\nN\nO\nN\nH\nN\nO N\nON\nN N N\nN\nFig.1|Overviewillustrationofthedatasetsandtasksaddressedinthiswork . Inthiswork,webenchmark\nGPT-3ondatasetsspanningchemicalspacefrommoleculesovermaterialstoreactions(seeSupplemen-\ntaryNote 2). Onthosedatasets,weinvestigatedifferenttasksrangingfromclassification,i.e.,predicting\na class (e.g., “high”, “low”) given a text representation of a molecule, material, or reaction, regression,\ni.e., prediction of floating point numbers, to inverse design—the prediction of molecules. MOF render-\ning created with iRASPA.23\nLanguage-interfaced fine-tuning for classification and regression\nApproach Before discussing the different applications in detail, let us first discuss how we\nfine-tune24 the GPT-3 model in practice for a simple but highly non-trivial example. High en-\ntropyalloyshaveattractedmuchinterestasanovelclassofstructuralmetals. Interestingly,one\nhas a sheer infinite number of possible combinations of metals. From a practical point of view,\nit is important to know if a given combination of metals will form a solid solution or multiple\nphases. Hence,thequestionwewouldliketoaskis \"What is the phase of <composition\nof the high entropy alloy>?\"and our model should give a text completion from the set\nof possible answers{single phase, multi-phase}.\nIn Extended Data Tab.1, we have given the set of questions and answers we have used to\nfine-tune the GPT-3 model. These are questions and answers on high entropy alloys for which\nthephasehasbeenexperimentallydetermined. ThemodeltuningviatheOpenAIAPItypically\ntakes a few minutes and gives us a new model, which takes as input\"Sm0.75Y0.25\"and gives\nas text completion\"1\", which corresponds to single-phase. This simple example already gives\nsome remarkable results. We selected this example to directly compare its performance with\nthe current state-of-the-art machine learning models with descriptors specially developed to\nmimictherelevantchemistryforthisapplication. 25 InFigure 2,weshowthatwithonlyaround\n50 data points, we get a similar performance as the model of Pei et al.25, which was trained on\nmore than 1000 data points.\n3\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n0 50 100 150 200\nnumber of training points\n0.7\n0.8\n0.9\naccuracy\nGPT-3\nGPT-3 (non-Google test set)\nautomatminer\nCrabNet\nRF\nFig.2|AccuracyofourGPT-3modelforpredictingsolid-solutionformationinhigh-entropyalloys . The\ntop figure compares the model’s accuracy as a function of the number of training points. The dashed\nhorizontallineindicatestheperformancereportedin 25 usingrandomforest(RF)withadatasetof1252\npoints and 10-fold cross-validation, i.e., corresponding to a training set size of around 1126 points. The\ndottedlineshowstheperformanceofasimplerule-basedbaseline“if”.” presentincomposition,classify\nas single phase, else multiphase”. The yellow line we obtained using the Automatminer,26, which uses\nas input the chemical composition. The Automatminer then returns the best featurisation and model\namongthosethatareimplementedusingautomatedmachinelearningwithgeneticprogramming(asim-\nplementedintheTPOTpackage 27). Weadditionallytestedaneuralnetwork,CrabNet(redline,default\nsettings),28 thatperformswellusingcompositionsasinput. ThebluelineistheperformanceofourGPT-\n3 model (with error bands showing the standard error of the mean). This figure shows that we reach\nsimilar accuracy as the model of Pei et al. with as little as around 50 data points. In addition, we also\ninvestigatedaseparatetrainandtestset,forwhichthelearningcurveisshowningreen. Inthiscase,we\nonly tested on compounds for which wecould not find an exactmatch with a Google search.\n4\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nClassification Theseresultsmadeuswonderifsimilarresultscanbeobtainedforotherprop-\nerties. Hence,welookedatarangeofverydifferentpropertiesofmolecules,materials,aswellas\nchemicalreactions. Wefocusedonthoseapplicationsforwhichconventionalmachine-learning\nmethodshavebeendevelopedandgenerallyacceptedasbenchmarksintheirfield. Inaddition,\nwealsocomparedourmodeltothetop-performingonesontasksfromtheMatbench 26 suiteof\nbenchmarks (Supplementary Note7.15).\nExtendedDataTab. 2comparestheperformanceofafine-tunedGPT-3modelwithbaselines.\nFordoingso,wefitthelearningcurvesfortheGPT-3modelsandforthebaselinesandmeasure\nwhere they intersect, i.e., we determine the factor of how much more (or less) data we would\nneed to make the best baseline perform equal to the GPT-3 models in the low-data regime of\nthe learning curves. The full learning curves for all models can be found in the Supplementary\nMaterials (Supplementary Note7).\nFormolecules,weinvestigatedpropertiesrangingfromHOMO-LUMOgapsandsolubilityin\nwatertotheperformanceinorganicphotovoltaics. Formaterials,wefocusedontheproperties\nofalloys,metal-organicframeworks,andpolymers. Finally,forreactions,weconsideredtwokey\ncross-coupling reactions in organic chemistry. Extended Data Tab.2showsthat in the low data\nregime, our GPT-3 model is typically at least as good as the conventional ML model and often\nneedsfewerdata. Inthehigh-dataregime,theconventionalMLmodelsoftencatchupwiththe\nGPT-3 model. This makes sense as for a given size of the data set, the need for additional data\nand correlations (inductivebiases)29 captured byGPT-3might be less needed.\nWe have to mention that we did not optimize the fine-tuning of the GPT-3 model, i.e., we\ndidnottrytooptimizehowasentenceispresentedtothemodel;onecanenvisionthatspecific\ntokenization can have better results for chemical sentences.9,16,30,31 Also, we did not tune the\nnumber of times we show an example to a model (i.e., the number of epochs or the learning\nrate).\nBeyond fine-tuning of OpenAI models Importantly,wearealsonotlimitedtofine-tuning;\nin Supplementary Note6, we show that we can even achieve good performancewithout fine-\ntuning by incorporating examples directly into the prompt (so-called in-context learning,5,32\ni.e.,learningduringinferencetime). ThisworksparticularlywellwiththelargestGPT-3models\nand GPT-4. We are also not limited to using models from OpenAI. In?? and Supplementary\nNote 8, we also show that we could obtain good results by fine-tuning the open-source LLMs\nparameter-efficientfine-tuningtechniquesonconsumerhardwareandprovideaPythonpackage\nthat makesit easy to apply this approach to newproblems.\nRepresentation sensitivity An interesting question is how to represent a molecule or ma-\nterial. Most of the literature reports use IUPAC names. For ML applications, there has been a\nlotofefforttorepresentachemicalwithuniquelineencodings(e.g.,SMILES 33 orSELFIES34,35).\nAs the GPT-3 model has been trained on natural text, one might expect that chemical names\n5\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nare preferred over line representations such as SMILES or SELFIES. Therefore, we investigated\ndifferent representations for our molecular property prediction tasks (see also Supplementary\nNote 5). Interestingly, our results (see Supplementary Note7) show that good results are ob-\ntainedirrespectiveoftherepresentation. Thefactthatweoftengetthebestperformanceusing\ntheIUPACnameofthemoleculemakesfine-tuningGPT-3foraparticularapplicationrelatively\nsimple for non-specialists.\nRegression A more challenging task than classification is to make a regression model, which\nwouldallowustopredictthevalueofacontinuouspropertysuchastheHenrycoefficientforthe\nadsorptionofagasinaporousmaterial. Asweareusingapretrainedlanguagemodel,perform-\ning actual regression that predicts real numbers (∈ R) is impossible (without changes to the\nmodel architecture and training procedure). However, in most, if not all, practical applications,\nthe accuracy for which we can make predictions is always limited. For example, for the Henry\ncoefficient of a material, an accuracy of 1% (or a certain number of decimal points) is sufficient\nfor most applications (see Supplementary Note11 for discussion on this error source). Hence,\nweusemoleculeswithHenrycoefficientsroundedtothisaccuracyasatrainingsetandassume\nthat the GPT-3 model can interpolate these numbers. Of course, one could also convert this\ninto a classification problem by making tiny bins. For this more challenging regression task, we\nneed more data for tuning the GPT-3 model, and we still get a performance that can approach\nthe state-of-the-art, but as this approach requires much more data, the advantage, except for\nthe ease of training, is less. We obtain a similar conclusion for other regression problems (see\nSupplementary Note11) and imbalanced classification cases (Supplementary Note7.8).\nInverse design\nOnecanarguethattheultimategoalofmachinelearninginchemistryistocreateamodelthat\ncangeneratemoleculeswithadesiredsetofproperties. Thisisalsoknownasinversedesign. 36\nBroadly speaking, there are two approaches. If we have large datasets, we can train generative\nmodels such as variational autoencoders (VAE)37,38 or generative adversarial neural networks\n(GANs).39,40 Without large datasets, evolutionary techniques such as genetic algorithms can\ngenerate novel, potentially interesting molecules41–44. Those evolutionary methods work best\nif one can limit the underlying chemistry; for example, finding the optimal functional group on\na material with a well-definedbackbone.45\nGiven that the GPT-3 model can predict the properties of molecules and materials with a\nsmall data set, trying an inverse design strategy is tempting. This would be particularly impor-\ntant in the early stages of research; one often has a small set of experimental data points and\na limited understanding. Yet, we could leverage a fine-tuned GPT-3 model to generate sugges-\ntions for novel materials with similar or even better performance. This would be an important\nstep forward. Particularly since the tuning of such a natural language model is much more ac-\ncessible than the training of conventional ML models. Here, we investigate this setting: Can\n6\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\na fine-tuned GPT-3 propose valid molecules that satisfy the constraints or desired properties\nspecifiedinapromptinnaturallanguage? Again,weareillustratingthepotentialforafewcase\nstudies.\nMolecular photoswitches are organic molecules with extended aromatic systems that make\nthem responsive to light. Upon radiation, they switch reversibly between different isomers\n(which changes some properties, such as dipole moments). This reversible switching makes\nthem interesting molecules for applications ranging from sensing to drug discovery. These\nmolecules are complex, making sufficiently accurate predictions using first-principles theory\nvery expensive. Yet, it is important to have some guidance to identify promising molecules,\nandmachinelearningmodelshavebeendevelopedforthis. Oneoftheimportantpropertiesof\nthesephotoswitchesisthewavelengthatwhichthereisamaximumintheadsorptionspectrum\nforthe EandZ isomer. Hence,wefine-tunedGPT-3withthesamedatausedbyGriffithsetal. 46.\nAswehaveshownabove,wecan fine-tuneGPT-3toaccuratelyanswerquestionslike What is\nthe pi-pi* transition wavelength of CN1C(/N=N/C2=CC=CC=C2)=C(C)C=C1C?\".\nForGPT-3,inversedesignisassimpleastrainingthemodelwithquestionandcompletionre-\nversed. Thatis,answerthequestion What is a photoswitch with transition wavelengths\nof 324 nm and 442 nm, respectively with a text completion that should be a SMILES\nstring of a meaningful molecule. This approach should be contrasted with the approach used\nby46, in which a library of molecules is generated, and their ML model (a Gaussian process re-\ngression) is used to evaluate the transition wavelengths of each material. If one has a lot of\nknowledge about the system, one can design large specific libraries that contain many promis-\ning molecules, including molecules with transition wavelengths of 324.0nm and 442nm. But,\nsuch a brute force technique is not what we understand as inverse design, as it, by definition,\ncannot predict a molecule wedid not include in our library.\nAsimpletesttoseeifourmodelcangeneratenewstructuresistoaskittogeneratemolecules\nwith transition wavelengths similar to those from the dataset reported by Griffiths et al.46 Ex-\ntendedDataFig. 1showsarepresentativesampleofthemoleculesgeneratedbythemodel. As\nexpected, many molecules come from the training set (colored orange in the figure). Impor-\ntantly, many molecules are not in the training set, and interestingly, some are not even in the\nPubChemdatabaseofknownchemicals. InFigure 3,weshowthatforthemolecules,thetransi-\ntionwavelengthiswithinameanabsolutepercentageerrorofaround10%. Notethatsincethe\nGPR model of Griffiths et al. was shown to perform comparable, if not better, to more costly\nDFT simulations, we chose to use their model to compute the transition wavelengths for the\ngeneratedmolecules.\nItisinterestingtoquantifyhownovelournewlygeneratedmoleculesare. Wecomparethese\nmolecules with those that Griffiths et al.46 collected. We quantify the similarity by computing\nthedistancebetweenmolecularfingerprints. Figure 4visualizesthisbylayingouttheresulting\napproximate nearest-neighbor graph in two dimensions. The orange and green spheres repre-\nsent molecules from the Griffiths dataset, the blue spheres show the novel ones, and the pink\nones are not even part of the PubChem database. As expected, we find many new structures\n7\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nFig. 3|Photoswitch inverse design metrics as a function of temperature.The fraction of valid SMILES\nindicatesthefractionofgeneratedSMILESthatcansuccessfullybeparsedusingRDKit(notethatitdoes\nnot plateau at 0, but approx. 0.1).47 We then determine the fraction (frac.) of those already part of the\ntrainingsetandfindthatatlowtemperatureGPT-3tendstorestatemoleculesfromthetrainingset. To\nquantitativelycapturethesimilarityofthedistributionofthegeneratedmoleculestotheonesfromthe\ntraining set, we compute the Fréchet ChemNet Distance,48 which quantifies both diversity and distri-\nbution match49 and goes through a minimum at intermediate temperatures. For quantifying how well\nthe generated molecules match the desired transition wavelengths, we use the GPR models reported by\nGriffithsetal. 46 topredictthetransitionwavelengths. Thedashedhorizontallinesinthefigureindicate\nthosemodels’meanabsoluteerror(MAE).Acrossalltemperatures,wefoundhighaveragesynthesizabil-\nity (SA score50 smaller 3). 8\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nChemical Space\nN NF F\nN N FS\nN N F\nF\nF\nN NS\nF\ndatabase, not generated\nin database\nin PubChem\nnovel\nA\nA\nD\nN N\nN\nO\nN N\nO\nNO\nN N\nN\nO\nCl\nN\nO\nNNF\nO\nN N\nN\nO\nNO2\nN N\nN\nO\nCl\nN N\nF\nF\nF\nN N\nF\nF\nN N\nF\nF\nO\nC\nB\nNNH\nN\nN\nNH\nO\nN\nHNN\nO\nN\nNH\nNN\nN\nH\nO\nB\nC\nD\nFig. 4|TMAP visualization of the generated photoswitches and the training set. The TMAP algo-\nrithm builds a nearest-neighbor graph, which is then embedded in two dimensions. Therefore, similar\nmolecules are connected with an edge. We color the points depending on whether they are part of the\noriginal dataset of Griffiths et al.46 but not generated (green), part of the database, and generated by\nourmodel(orange). Ourmodelscanalsogeneratemoleculesthathavenotbeenpartofthephotoswitch\ndataset(notethatthemodelwasonlytrainedon92moleculesfromthisdatabase). Insomecases,those\nmolecules have been reported before and are part of the PubChem database (blue) or are not even part\nof PubChem (pink). From this figure, we see that the generated molecules sometimes substitutions for\nmolecules in the dataset. In other cases, newly generated molecules introduce a completely new scaf-\nfold. For this visualization, we used the TMAP51 algorithm on photoswitch molecules described using\nMinHash fingerprintwith 2048 permutations.52\nthatarederivativesofmoleculesintheGriffithsdatabase. However,wealsofindbranchesthat\nare not part of the library of Griffiths et al.46, indicating that the model generated novel kinds\nof compounds.\nIngeneratingthesemolecules,weadjustedtheso-calledsoftmaxtemperatureinthesampling\nstep of GPT-3 models. This temperature is conventionally used to generate more natural text.\nIf we set this temperature to zero, we will generate text with the most frequently used words.\nWe can increase the temperature to make the text more natural, making it more likely that less\ncommonly used synonyms are chosen. For chemistry, if we aim to complete a SMILES starting\nwith carbon, the zero-temperature solution would always complete the symbol that most com-\nmonly follows carbon (“(” in the QMugs dataset). In contrast, too-high temperatures would\nrandomly choose anyelement.\nThe impact of this temperature parameter is shown in Figure3. At low temperatures, the\ngenerated molecules often come from the training set and only show a low diversity. Across\nall temperatures, the generated molecules seem synthesizable, as judged by a low SA score.50\nIncreasingthetemperaturegivesusmorediverseandnovelstructures,butonecanalsoexpect\nmore structures that makeno chemical sense,i.e.,are invalid.\n9\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nStretching the limits\nThe results on the photoswitches illustrate the potential of LLMs for chemistry. To get some\nmore insights into whether we can trust these GPT-3 predictions, we carried out some experi-\nments where wetried to stretch the limits.\nWe have already seen that we can obtain good results independent of how we represent a\nmolecule (IUPAC names, SMILES, or SELFIES), but can GPT-3 interpret an abstract represen-\ntation of molecules we invented? Jablonka et al.53 developed a machine learning approach to\ndesign dispersants using a coarse-grained approach. This dispersant was a linear copolymer\nwith four monomer types and a chain length between 16 and 48 units, giving a chemical de-\nsign space of 58 million different dispersants. One important goal in this work was to find\ndispersants with the right binding free energy, i.e., which polymer length and which monomer\nsequence is optimal. As there is no way the GPT-3 knows about the properties or represen-\ntations of the coarse-grained polymers, it is interesting to see if we can get any sensible re-\nsult if we ask the questionWhat is the adsorption free energy of coarse-grained\ndispersant AAAABBBBDDDDAAAACCCC or as inverse design,Give me a structure of a\ncoarse-grained dispersant with a free energy of 17. Interestingly, for the predic-\ntion of the adsorption free energy, the GPT-3 model outperforms the models developed by\nJablonka et al.53 Additionally, it can also successfully carry out the inverse design and generate\nmonomer sequences that give the desired composition and, with a mean percentage error of\naround22%,thedesiredadsorptionfreeenergy(theapproximationofthegroundtruthweuse\nalready has a mean percentageerror of around 9%, see Supplementary Note12.1for details).\nIn the case of the photoswitches, we have seen that the GPT-3 model can generate new\nmolecules quite different from the training set. To explore in detail how far we can stretch\nthelimitsofwhatnewmoleculeswecangenerate,wechooseanapplicationforwhichquantum\ncalculations are known to predict the experimental values sufficiently accurately. The HOMO-\nLUMOgapissuchanapplication. TheHOMO-LUMOgapisrelevant,forinstance,inelectronic\napplications that aim to excite a molecule at a specific energy. This HOMO-LUMO gap can be\npredicted accurately using semi-empirical quantum mechanics (GFN2-xTB54), which is compu-\ntationallyaffordableenoughforustocomputeforallgeneratedmolecules(seeSupplementary\nFig. 77). Moreover, the QMugs dataset,55,56 has listed these HOMO-LUMO calculations for\n665k molecules.\nIn the Supplementary Note12.3, we show that with the training of only 500 samples, we\ncangetareasonableestimateoftheHOMO-LUMOgapofthemoleculesintheQMugsdataset.\nAlso, by reverting the question, we have our model trained for inverse design. In Supplemen-\ntaryNote 12.3,weshowthatbyaskingthemodel What is a molecule with a HOMO-LUMO\ngap of 3.5 eV, we get similar to the photoswitches, a set of novel molecules. These novel\nmolecules are not part of our training set and not evenpart of the QMugs dataset.\nWe now conduct some experiments on a dummy task to test how well the GPT-3 model can\nextrapolate to HOMO-LUMO gaps for which it has not received any training. To mimic this\n10\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nsituation, we retrained our inverse design model using a dataset that only has molecules with\nHOMO-LUMO gaps smaller than 3.5eV, and subsequently query the model with a question\nthat requires the GPT-3 model to extrapolate (and, e.g., to find that very small molecules are\nassociated with large HOMO-LUMO gaps; a task we only selected for demonstration purposes\nandthatcanbeexploitedbygeneratingsmallmolecules). Wedothisbyaskingmorethan1,000\ntimes the question:What is a molecule with a HOMO LUMO gap of <XX>, where each\ntimeweslightlychangethevalueoftheHOMOLUMOgap,i.e.,wesample XXfromaGaussian\ncenteredat4eV. Interestingly,theGPT-3modeldoesprovidestructureswithadistributionof\nwhich our quantum calculations confirm that a significant fraction has a HOMO-LUMO gap >\n4.0eV. Again, this is a remarkable result. In our training set, there was not a single molecule\nwithabandgap>3.5eV,whichshowsthattheGPT-3modelcanmakeextrapolations. Wecando\nasimilarexperimentforthephotoswitches,forwhichwemighthavealibraryofphotoswitches\nwhose transition wavelengths are all below 350nm. For practical applications, however, it can\noften be essential to have adsorption at larger wavelengths. In this case, we can successfully\nuse a fine-tuned GPT-3 model to generate photoswitch molecules that adsorb at lower energy\n(SupplementaryFig. 75,which wealso validatedwith TD-DFT in Supplementary Note12.2.2).\nThesefindingsinspiredustodoaninversedesignexperimenttodesignmoleculeswithprop-\nertiesthattakeusfarfromthetrainingset. 57 WeareinterestedinmoleculesthathaveaHOMO-\nLUMO gap > 5eV. From the distribution of HOMO-LUMO gaps in the QMugs database (see\nFigure 5), we see that the average band gap is around 2.58eV. Only a handful of molecules in\nthis database havea HOMO-LUMOgap above5eV.\nHence, this is a challenging inverse design problem, as only a few materials in the database\nhave the desired properties. Here, our experiment is the quantum calculation, and we typically\nassume that we can evaluate hundreds to thousands of materials in a reasonable time. From\na machine-learning point of view, a set of thousands of materials is in a very low data regime.\nHowever, from an experimental point of view, this is a significant but sometimes doable effort.\nOfcourse,thisisasomewhatarbitrarylimit,andinSupplementaryFig. 83,wealsogivedatafor\nsignificantly fewerexperiments.\nWe start with the training using a set of hundreds of molecules randomly selected from the\nQMugs dataset (blue distribution in Figure5). These selected molecules will have band gap\ndistributionsimilartotheQMugsdataset. WethenqueryforHOMO-LUMOgaps,nowaround\n1000timesrequestingamoleculewithabandgaptakenfromanormaldistributionwithshifted\nmean (mean 4eV standard deviation 0.2eV). We evaluated these new molecules (green curve\ninFigure 5),whichindeedshowsashiftofthedistributiontohigherHOMO-LUMOgaps. Inthe\nnext iteration, we retrain the model with the new data and query again higher HOMO-LUMO\ngaps. Figure5showsthat wehaveachievedour aim after four iterations.\n11\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nit. 1\nit. 2\nit. 3\nit. 4\nit. 5\nQMUGs\nFig. 5|Iteratively biased generation of molecules toward large HOMO-LUMO gaps using GPT-3 fine-\ntunedontheQMugsdatasetofdraws. Westartbyfine-tuningGPT-3onasampleoftheQMugsdataset\nand use this model to query for around 1000 gaps from a normal distribution with shifted mean (mean\n4.0eV, standard deviation 0.2eV). We then iteratively select the high-gap samples of the generated\nmolecules and fine-tune the model on this data (i.e., starting from the second generation, the model is\nfine-tuned on molecules it itself generated). Smooth curves show kernel-density estimates; the plot is\ntruncated at 10eV, but the models also generate some molecules with larger HOMO-LUMO gaps. We\nchose a comparatively large number of evaluations for this figure to increase the clarity of the visualiza-\ntion. For the initialization, we evaluated 2162 compounds using xTB, followed by 1670, 250, and 1572.\nIf we limit the number of quantum chemistry evaluations to or lower 100, we can still successfully shift\nthe distribution as shownin Supplementary Fig.83.\n12\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nConcluding remarks\nOur results raise a very important question: how can a natural language model with no prior\ntraining in chemistry outperform dedicated machine-learning models? To our knowledge, this\nfundamental question has no rigorous answer. The fact that we get good results independent\nof the chemical representation illustrates that these language models are very apt at extracting\ncorrelations from any text.15 For example, we found promising results using both conventional\nchemicalnamesandentirelyhypotheticalrepresentations. Inbothcases,themodelcouldquan-\ntitativelycorrelate the pattern of repeating units correctly to different kinds of properties.\nOf course, if we say that the GPT-3 model is successful, it only implies that we have estab-\nlished that the GPT-3 model has identified correlations in the current training data that can be\nsuccessfully exploited to make predictions. However, this does not imply that the correlations\nare always meaningful or related to cause-effect relationships. Hence, our research does not\nstophere. ThenextstepwillbetouseGPT-3toidentifythesecorrelationsandultimatelygeta\ndeeperunderstanding. Inthiscontext,wearguethatGPT-3isonlyatooltomakemoreeffective\nuse of the knowledge scientists have collected over the years. It is also important to mention\nthat while the training corpus contains chemistry information, many, if not most, scientific ar-\nticles and results (including all failed or partially successful experiments58) have not been seen\nbyGPT-3. Hence,onecanexpectanevenmoreimpressiveperformanceifthesedataareadded\nto the training data.\nAs we show in this article, a machine learning system built using GPT-3 works impressively\nwellforawiderangeofquestionsinchemistry—evenforthoseforwhichwecannotuseconven-\ntional line representations such as SMILES. Compared to conventional machine learning, it has\nmanyadvantages. GPT-3canbeusedformanydifferentapplications. Eachapplicationusesthe\nsame approach, in which the training and use of the model are based on questions formulated\nin natural language. This raises the bar for future machine learning studies, as any new models\nshould at least outperform this simple approach instead.\nThe other important practical point is that using a GPT-3 model in a research setting is sim-\nilar to a literature search. It will allow chemists to leverage the chemical knowledge we have\ncollected. GPT-3hasbeendesignedtodiscovercorrelationsintextfragments,andthefactthat\nthesecorrelationsareextremelyrelevanttochemistryopensmanypossibilitiesforchemistsand\nmaterial scientists alike.\nConflicts of interest\nThere are no conflicts to declare.\n13\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nAuthor contributions\nK.M.J.developedthemachine-learningapproachwithfeedbackfromP.S.andB.S.andwrotethe\nmanuscript with B.S.A.O.contributed to DFT calculations.\nData availability\nAll data used in this work was obtained from public sources and can be downloaded with our\nPython code (https://github.com/kjappelbaum/gptchem).\nCode availability\nAll code created in this work is available on GitHub (https://github.com/kjappelbaum/\ngptchem,https://github.com/lamalab-org/chemlift).\nAcknowledgements\nK.M.J., A.O., and B.S. were supported by the MARVEL National Centre for Competence in Re-\nsearchfundedbytheSwissNationalScienceFoundation(grantagreementID51NF40-182892).\nP.S. acknowledges support from NCCR Catalysis (grant number 180544), a National Centre\nof Competence in Research funded by the Swiss National Science Foundation. The research of\nK.M.J. and B.S. wasalso supported by the USorb-DAC Project, which is funded by a grant from\nTheGranthamFoundationfortheProtectionoftheEnvironmenttoRMI’sclimatetechacceler-\natorprogram,ThirdDerivative. Inaddition,theworkofK.M.J.wassupportedbytheCarl-Zeiss\nFoundation.\nReferences\n1. Bommasani,R. etal. OntheOpportunitiesandRisksofFoundationModels. CoRRabs/2108.07258.\narXiv:2108.07258(2021).\n2. Vaswani, A.et al.Attention is all you need.Advances in neural information processing sys-\ntems 30(2017).\n3. Chowdhery,A. etal.PaLM:ScalingLanguageModelingwithPathways inarXivpreprintarXiv:\nArxiv-2204.02311(2022).\n4. Hoffmann, J. et al. Training Compute-Optimal Large Language Modelsin arXiv preprint\narXiv: Arxiv-2203.15556(2022).\n5. Brown,T.B. etal. LanguageModelsareFew-ShotLearners. CoRRabs/2005.14165.arXiv:\n2005.14165(2020).\n14\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n6. Edwards,C.N.,Lai,T.,Ros,K.,Honke,G.&Ji,H.TranslationbetweenMoleculesandNat-\nural Language.Conference On Empirical Methods In Natural Language Processing(2022).\n7. Hocky,G.M.&White,A.D.Naturallanguageprocessingmodelsthatautomateprogram-\nming will transform chemistry research and teaching.Digital Discovery1,79–83(2022).\n8. White,A.D. etal.Dolargelanguagemodelsknowchemistry? inChemRxivpreprint10.26434/chemrxiv-\n2022-3md3n (2022).\n9. Taylor,R. et al.Galactica:ALargeLanguageModelforScience. arXiv preprint arXiv: Arxiv-\n2211.09085(2022).\n10. Dunn,A. etal.Structuredinformationextractionfromcomplexscientifictextwithfine-tuned\nlarge language modelsinarXiv preprint arXiv: Arxiv-2212.05238(2022).\n11. Choudhary, K. & Kelley, M. L. ChemNLP: A Natural Language-Processing-Based Library\nforMaterials Chemistry TextData.J. Phys. Chem. C127,17545–17555(Aug. 2023).\n12. Jablonka,K.M. et al.14examplesofhowLLMscantransformmaterialsscienceandchem-\nistry: a reflection on a largelanguagemodel hackathon.Digital Discovery(2023).\n13. Dinh, T.et al. LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning\nTasksinarXiv preprint: Arxiv-2206.06565(2022).\n14. Karpov,P.,Godin,G.&Tetko,I.V.Transformer-CNN:SwissknifeforQSARmodelingand\ninterpretation.J Cheminform12(Mar.2020).\n15. Tshitoyan,V. et al.Unsupervised wordembeddings capture latent knowledge from mate-\nrials science literature.Nature571,95–98 (July 2019).\n16. Born,J.&Manica,M.RegressionTransformerenablesconcurrentsequenceregressionand\ngeneration for molecular languagemodelling.Nat Mach Intell5,432–444(Apr.2023).\n17. Yüksel,A.,Ulusoy,E.,Ünlü,A.&Doğan,T.SELFormer:molecularrepresentationlearning\nvia SELFIES languagemodels.Mach. Learn.: Sci. Technol.4,025035 (June 2023).\n18. Van Deursen, R., Ertl, P., Tetko, I. V. & Godin, G. GEN: highly efficient SMILES explorer\nusing autodidactic generativeexaminationnetworks.J Cheminform12(Apr.2020).\n19. Flam-Shepherd,D.,Zhu,K.&Aspuru-Guzik,A.Languagemodelscanlearncomplexmolec-\nular distributions.Nat Commun13(June 2022).\n20. Grisoni, F. Chemical language models for de novo drug design: Challenges and opportu-\nnities.Current Opinion in Structural Biology79,102527(Apr.2023).\n21. Ramos, M. C., Michtavy, S. S., Porosoff, M. D. & White, A. D. Bayesian Optimization of\nCatalysts With In-contextLearning.arXiv preprint arXiv: Arxiv-2304.05341(2023).\n22. Guo,T. etal. WhatindeedcanGPTmodelsdoinchemistry?Acomprehensivebenchmark\non eight tasks.arXiv preprint arXiv: 2305.18365(2023).\n15\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n23. Dubbeldam, D., Calero, S. & Vlugt, T. J. iRASPA: GPU-accelerated visualization software\nformaterials scientists.Molecular Simulation44,653–676(Jan. 2018).\n24. Howard, J. & Ruder, S.Universal Language Model Fine-tuning for Text Classificationin Pro-\nceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-\nume1:LongPapers) (AssociationforComputationalLinguistics,Melbourne,Australia,July\n2018), 328–339.\n25. Pei, Z., Yin, J., Hawk, J. A., Alman, D. E. & Gao, M. C. Machine-learning informed pre-\ndiction of high-entropy solid solution formation: Beyond the Hume-Rothery rules.npj\nComput Mater6(May2020).\n26. Dunn, A., Wang, Q., Ganose, A., Dopp, D. & Jain, A. Benchmarking materials property\nprediction methods: the Matbench test set and Automatminer reference algorithm.npj\nComput Mater6(Sept. 2020).\n27. Le,T.T.,Fu,W.&Moore,J.H.Scalingtree-basedautomatedmachinelearningtobiomed-\nical big data with a feature set selector.Bioinformatics 36,250–256 (2020).\n28. Wang, A. Y.-T., Kauwe, S. K., Murdock, R. J. & Sparks, T. D. Compositionally restricted\nattention-based network for materials property predictions.npj Computational Materials\n7,77 (2021).\n29. Goldblum,M., Finzi, M., Rowan,K. & Wilson, A.The No Free Lunch Theorem, Kolmogorov\nComplexity, and the Role of Inductive Biases in Machine Learningin arXiv preprint Arxiv-\n2304.05366(2023).\n30. Schwaller, P.et al.Molecular Transformer: A Model for Uncertainty-Calibrated Chemical\nReaction Prediction.ACS Cent. Sci.5,1572–1583 (2019).\n31. Winter,B.,Winter,C.,Schilling,J.&Bardow,A.Asmileisallyouneed:predictinglimiting\nactivity coefficients from SMILES with natural language processing.Digital Discovery1,\n859–869 (2022).\n32. Dai, D.et al. Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient\nDescent as Meta-OptimizersinarXiv preprint Arxiv-2212.10559(2022).\n33. Weininger, D. SMILES, a chemical language and information system. 1. Introduction to\nmethodology and encoding rules.J. Chem. Inf. Comput. Sci.28,31–36(1988).\n34. Krenn,M.,Häse,F.,Nigam,A.,Friederich,P.&Aspuru-Guzik,A.Self-referencingembed-\nded strings (SELFIES): A 100% robust molecular string representation.Mach. Learn.: Sci.\nTechnol.1,045024(2020).\n35. Krenn, M.et al.SELFIES and the future of molecular string representations.Patterns 3,\n100588 (Oct. 2022).\n36. Sanchez-Lengeling, B. & Aspuru-Guzik, A. Inverse molecular design using machine learn-\ning: Generativemodels for matter engineering.Science 361,360–365 (2018).\n16\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n37. Yao,Z. et al.Inversedesignofnanoporouscrystallinereticularmaterialswithdeepgener-\nativemodels. en.Nat Mach Intell3,76–86(Jan. 2021).\n38. Gómez-Bombarelli, R.et al.Automatic chemical design using a data-driven continuous\nrepresentation of molecules.ACS Cent. Sci.4,268–276(2018).\n39. Kim,B.,Lee,S.&Kim,J.Inversedesignofporousmaterialsusingartificialneuralnetworks.\nen.Sci. Adv.6,eaax9324(Jan. 2020).\n40. Lee,S.,Kim,B.&Kim,J.Predictingperformancelimitsofmethanegasstorageinzeolites\nwith an artificial neural network. en.J. Mater. Chem. A Mater. Energy Sustain.7, 2709–\n2716 (2019).\n41. Nigam,A.,Friederich,P.,Krenn,M.&Aspuru-Guzik,A.AugmentingGeneticAlgorithms\nwith Deep Neural Networksfor Exploring the Chemical Space.ICLR (2019).\n42. Jablonka,K.M.,Mcilwaine,F.,Garcia,S.,Smit,B.&Yoo,B. Areproducibilitystudyof”Aug-\nmenting Genetic Algorithms with Deep Neural Networks for Exploring the Chemical Space”\ninarXiv preprint arXiv: Arxiv-2102.00700(2021).\n43. Chung,Y.G. etal. Insilicodiscoveryofmetal-organicframeworksforprecombustionCO 2\ncapture using a geneticalgorithm.en.Sci. Adv.2,e1600909 (Oct. 2016).\n44. Lee, S.et al.Computational screening of trillions of metal-organic frameworks for high-\nperformance methane storage. en.ACS Appl. Mater. Interfaces13, 23647–23654 (May\n2021).\n45. Collins, S. P., Daff, T. D., Piotrkowski, S. S. & Woo, T. K. Materials design by evolutionary\noptimization of functional groups in metal-organic frameworks.Sci. Adv.2(Nov.2016).\n46. Griffiths, R.-R.et al.Data-driven discovery of molecular photoswitches with multioutput\nGaussian processes.Chem. Sci.13,13541–13551(2022).\n47. contributors, R.RDKit: Open-source cheminformaticshttp://www.rdkit.org.\n48. Preuer, K., Renz, P., Unterthiner, T., Hochreiter, S. & Klambauer, G. Fréchet ChemNet\nDistance: A Metric for Generative Models for Molecules in Drug Discovery.J. Chem. Inf.\nModel. 58,1736–1741(Aug. 2018).\n49. Brown, N., Fiscato, M., Segler, M. H. & Vaucher, A. C. GuacaMol: Benchmarking Models\nforde NovoMolecular Design.J. Chem. Inf. Model.59,1096–1108 (2019).\n50. Ertl,P.&Schuffenhauer,A.Estimationofsyntheticaccessibilityscoreofdrug-likemolecules\nbasedonmolecularcomplexityandfragmentcontributions.en. J. Cheminform.1,8(June\n2009).\n51. Probst,D.&Reymond,J.-L.Visualizationofverylargehigh-dimensionaldatasetsasmin-\nimum spanning trees.J. Cheminform.12(Feb.2020).\n52. Probst, D. & Reymond, J.-L. A probabilistic molecular fingerprint for big data settings.J.\nCheminform. 10(Dec.2018).\n17\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n53. Jablonka, K. M., Jothiappan, G. M., Wang, S., Smit, B. & Yoo, B. Bias free multiobjective\nactivelearning for materials design and discovery.Nat. Commun.12(2021).\n54. Bannwarth,C.,Ehlert,S.&Grimme,S.GFN2-xTB—Anaccurateandbroadlyparametrized\nself-consistenttight-bindingquantumchemicalmethodwithmultipoleelectrostaticsand\ndensity-dependentdispersioncontributions. J.Chem.TheoryComput. 15,1652–1671(2019).\n55. Isert,C.,Atz,K.,Jiménez-Luna,J.&Schneider,G. QMugs:QuantumMechanicalProperties\nof Drug-like Moleculesen. 2021.\n56. Isert,C.,Atz,K.,Jiménez-Luna,J.&Schneider,G.QMugs,quantummechanicalproperties\nofdrug-likemolecules. Sci Data9(June 2022).\n57. Westermayr,J.,Gilkes,J.,Barrett,R.&Maurer,R.J.High-throughputproperty-drivengen-\nerativedesign of functional organic molecules.Nat. Comput. Sci.(Feb.2023).\n58. Jablonka, K. M., Patiny, L. & Smit, B. Making the collective knowledge of chemistry open\nand machine actionable.Nat. Chem.14,365–376(Apr.2022).\n59. Wang, Y., Wang, J., Cao, Z. & Farimani, A. B. Molecular contrastive learning of represen-\ntations via graph neural networks.Nat. Mach. Intell.4,279–287(Mar.2022).\n60. Breuck, P.-P. D., Evans, M. L. & Rignanese, G.-M. Robust model benchmarking and bias-\nimbalance in data-driven materials science: a case study on MODNet.J. Phys.: Condens.\nMatter 33,404002 (July 2021).\n61. Hollmann,N.,Müller,S.,Eggensperger,K.&Hutter,F. TabPFN:A TransformerThat Solves\nSmall Tabular Classification Problems in a SecondinarXiv preprint arXiv: Arxiv-2207.01848\n(2022).\n62. Raffel, C.et al.Exploring the limits of transfer learning with a unified text-to-text trans-\nformer.The Journal of Machine Learning Research21,5485–5551(2020).\n63. Radford, A.et al.LanguageModels are Unsupervised Multitask Learners (2019).\n64. Mobley,D.L.&Guthrie,J.P.FreeSolv:adatabaseofexperimentalandcalculatedhydration\nfree energies, with input files.J. Comput. Aided Mol. Des.28,711–720(June 2014).\n65. Delaney, J. S. ESOL: Estimating Aqueous Solubility Directly from Molecular Structure.J.\nChem. Inf. Comput. Sci.44,1000–1005(Mar.2004).\n66. Mitchell, J. B. O.DLS-100 Solubility Datasethttps://risweb.st-andrews.ac.uk:\n443/portal/en/datasets/dls100-solubility-dataset(3a3a5abc-8458-4924-\n8e6c-b804347605e8).html.\n67. Walters, P.Predicting Aqueous Solubility - It’s Harder Than It Lookslast accessed 06 Feb\n2023.PredictingAqueousSolubility-It’sHarderThanItLooks. https://practicalcheminformatics.\nblogspot.com/2018/09/predicting-aqueous-solubility-its.html (2023).\n68. Bento, A. P.et al.The ChEMBL bioactivity database: an update.Nucleic Acids Res.42,\nD1083–D1090 (2014).\n18\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\n69. Gaulton, A.et al.ChEMBL: a large-scale bioactivity database for drug discovery.Nucleic\nAcids Res.40,D1100–D1107(2012).\n70. Nagasawa,S.,Al-Naamani,E.&Saeki,A.Computer-AidedScreeningofConjugatedPoly-\nmersforOrganicSolarCell:ClassificationbyRandomForest. J.Phys.Chem.Lett. 9,2639–\n2646 (2018).\n71. Moosavi, S. M.et al.Understanding the diversity of the metal-organic framework ecosys-\ntem.Nat. Commun.11(2020).\n72. Moosavi, S. M.et al.A data-science approach to predict the heat capacity of nanoporous\nmaterials.Nat. Mater.21,1419–1425 (Oct. 2022).\n73. Nonequilibrium phase diagrams of ternary amorphous alloysen (eds Kawazoe, Y., Yu, J.-Z.,\nTsai,A.-P.& Masumoto,T.)(Springer,NewYork,NY,Sept. 2006).\n74. Zhuo, Y., Tehrani, A. M. & Brgoch, J. Predicting the Band Gaps of Inorganic Solids by Ma-\nchine Learning.J. Phys. Chem. Lett.9,1668–1673(Mar.2018).\n75. Ahneman, D. T., Estrada, J. G., Lin, S., Dreher, S. D. & Doyle, A. G. Predicting reaction\nperformance in C–N cross-coupling using machine learning.Science 360, 186–190 (Apr.\n2018).\n76. Perera,D. etal. Aplatformforautomatednanomole-scalereactionscreeningandmicromole-\nscale synthesis in flow.Science 359,429–434 (Jan. 2018).\n77. Ertl, P. & Rohde, B. The Molecule Cloud - compact visualization of large collections of\nmolecules.J. Cheminform.4(July 2012).\n19\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nExtendedDataFig.1|MoleculeCloudforrandomlygeneratedphotoswitchmolecules . MoleculeCloud\ngeneratedusingthetoolreportedbyErtlandRohde 77. Aquamarinebackgroundindicatessamplesfrom\nmolecules in the database reported by Griffiths et al.46 that our model did not generate, coral indicates\nthemoleculesourmodelgeneratedandthatarepartof 46’sdatabase,lightsteelbluebackgroundindicates\nsamplesthataregeneratedbyourmodelandthatarenotpartofthedatabaseofGriffithsetal. 46 butpart\nofthePubChemdatabase. Paleviolet-redbackgroundindicatesmoleculesthatourmodelgeneratedbut\nthat are part neither of PubChem nor the database of Griffiths et al.46\n20\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nExtendedDataTab.1|Examplepromptsandcompletionsforpredictingthephaseofhigh-entropyalloys .\nThese models have been trained using a self-supervised approach, i.e., to predict the next token given\nan input text sequence. This implies we offer the list of questions and answers as one large string. The\nprogramlearnsthatinourstring“ ###”indicatestheendofapromptand“ @@@”theendofacompletion.\nHere,weused the fact that learning one character is cheaper and easier,hence 0=multi-phase.\nprompt completion experimental\nWhatis the phase of Co1Cu1Fe1Ni1V1?### 0@@@ multi-phase\nWhatis the phase of Pu0.75Zr0.25?### 1@@@ single-phase\nWhatis the phase of BeFe?### 0@@@ multi-phase\nWhatis the phase of LiTa?### 0@@@ multi-phase\nWhatis the phase of Nb0.5Ta0.5?### 1@@@ single-phase\nWhatis the phase of Al0.1W0.9?### 1@@@ single-phase\nWhatis the phase of Cr0.5Fe0.5?### 1@@@ single-phase\nWhatis the phase of Al1Co1Cr1Cu1Fe1Ni1Ti1?### 0@@@ multi-phase\nWhatis the phase of Cu0.5Mn0.5?### 1@@@ single-phase\nWhatis the phase of OsU?### 0@@@ multi-phase\n21\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0\nExtended Data Tab. 2|Data-efficiency comparison of best-performing GPT-3-based approaches with\nbest-performing baselines.For the best comparison, wealso split into (pre-trained) deep-learning (DL)-\nbased baselines (here, MolCLR,59 ModNet,60 CrabNet,28 and TabPFN61) and baselines not using (pre-\ntrained)deep-learningapproaches(n-Gram,GaussianProcessRegression,XGBoost,randomforests,au-\ntomatedmachinelearningoptimizedformaterialsscience 26)onhand-tunedfeaturesets. Therearesev-\neralcaveatstothisanalysis. First,focusingonthelow-dataregimemightnotalwaysbethemostrelevant\nperspective. Second, we only focus on the binary classification setting in this table. Third, we focus on\nthe F1 macro score for this table (all cases are class-balanced). Fourth, we consider the performance of\nthe GPT-3 model for ten training data points as a reference. We provide more details in Supplementary\nNote7. TheversionofGPT-3weutilizedinthisworkhasbeentrainedondatauptoOct2019thatmostly\ncomes from web scraping (Common Crawl62 and WebText63) along with books corpora and Wikipedia.\nStructured datasets, however, have not been part of the training. Also, note that our approach works\nwellon representations that havenot been used for the original datasets (e.g.,SELFIES,InChI).\ngroup benchmark publication\nyear\nbest\nnon-DL\nbest DL\nbaseline\nmolecules photoswitchtransition wavelength46 2022 1.1 1.2\nfreeenergy of solvation64 2014 3.1 1.3\nsolubility65–67 2004 1.0 0.02\nlipophilicity68,69 2012 3.43 0.97\nHOMO-LUMOgap55,56 2022 4.3 0.62\nOPVPCE70 2018 0.95 0.76\nmaterials surfactantfree energy of adsorption53 2021 1.4 0.37\nCO2 Henrycoefficients71 2020 0.40 12\nCH4 Henrycoefficients71 2020 0.51 0.59\nheatcapacity72 2022 0.24 0.76\nHEAphase25 2020 24 9.0\nbulkmetallicglassformationability 26,73 2006 0.98 0.62\nmetallicbehavior26,74 2018 0.52 0.46\nreactions C-Ncross-coupling75 2018 2.9\nC-Ccross-coupling76 2022 0.98\n22\nhttps://doi.org/10.26434/chemrxiv-2023-fw8n4-v3 ORCID: https://orcid.org/0000-0003-4894-4660 Content not peer-reviewed by ChemRxiv. License: CC BY 4.0"
}