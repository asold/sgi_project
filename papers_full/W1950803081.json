{
  "title": "Automatic Construction and Natural-Language Description of Nonparametric Regression Models",
  "url": "https://openalex.org/W1950803081",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A1986333108",
      "name": "James Lloyd",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2059490951",
      "name": "David Duvenaud",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A2131476448",
      "name": "Roger Grosse",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4207154037",
      "name": "Joshua Tenenbaum",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A617670330",
      "name": "Zoubin Ghahramani",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A1986333108",
      "name": "James Lloyd",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2059490951",
      "name": "David Duvenaud",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131476448",
      "name": "Roger Grosse",
      "affiliations": [
        "Massachusetts Institute of Technology",
        "Institute of Cognitive and Brain Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A4207154037",
      "name": "Joshua Tenenbaum",
      "affiliations": [
        "Massachusetts Institute of Technology",
        "Institute of Cognitive and Brain Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A617670330",
      "name": "Zoubin Ghahramani",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2031823405",
    "https://openalex.org/W6683167905",
    "https://openalex.org/W2075298172",
    "https://openalex.org/W6677096361",
    "https://openalex.org/W3148714132",
    "https://openalex.org/W2142959074",
    "https://openalex.org/W6683473897",
    "https://openalex.org/W1598115799",
    "https://openalex.org/W2136816045",
    "https://openalex.org/W1949894903",
    "https://openalex.org/W6680386650",
    "https://openalex.org/W2551909671",
    "https://openalex.org/W2170912685",
    "https://openalex.org/W2080002323",
    "https://openalex.org/W6628938642",
    "https://openalex.org/W1984931792",
    "https://openalex.org/W7048060829",
    "https://openalex.org/W6680836886",
    "https://openalex.org/W6629804754",
    "https://openalex.org/W1963375715",
    "https://openalex.org/W1979769287",
    "https://openalex.org/W6681987720",
    "https://openalex.org/W6632157852",
    "https://openalex.org/W53779406",
    "https://openalex.org/W2136445846",
    "https://openalex.org/W1765961089",
    "https://openalex.org/W4211049957",
    "https://openalex.org/W1607198972",
    "https://openalex.org/W2165962156",
    "https://openalex.org/W2963811219",
    "https://openalex.org/W3140968660",
    "https://openalex.org/W2798058877",
    "https://openalex.org/W2140251433",
    "https://openalex.org/W1746819321",
    "https://openalex.org/W4229539396",
    "https://openalex.org/W2160215259",
    "https://openalex.org/W3127133883",
    "https://openalex.org/W2312219486",
    "https://openalex.org/W2952125132",
    "https://openalex.org/W2951308445",
    "https://openalex.org/W1606309729",
    "https://openalex.org/W2168175751",
    "https://openalex.org/W2313953460",
    "https://openalex.org/W2951675424",
    "https://openalex.org/W1943248225",
    "https://openalex.org/W1538572412"
  ],
  "abstract": "This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural-language text. Our approach treats unknown regression functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.",
  "full_text": "Automatic Construction and Natural-Language Description\nof Nonparametric Regression Models\nJames Robert Lloyd\nDepartment of Engineering\nUniversity of Cambridge\nDavid Duvenaud\nDepartment of Engineering\nUniversity of Cambridge\nRoger Grosse\nBrain and Cognitive Sciences\nMassachusetts Institute of Technology\nJoshua B. Tenenbaum\nBrain and Cognitive Sciences\nMassachusetts Institute of Technology\nZoubin Ghahramani\nDepartment of Engineering\nUniversity of Cambridge\nAbstract\nThis paper presents the beginnings of an automatic\nstatistician, focusing on regression problems. Our sys-\ntem explores an open-ended space of statistical mod-\nels to discover a good explanation of a data set, and\nthen produces a detailed report with ﬁgures and natural-\nlanguage text.\nOur approach treats unknown regression functions non-\nparametrically using Gaussian processes, which has two\nimportant consequences. First, Gaussian processes can\nmodel functions in terms of high-level properties (e.g.\nsmoothness, trends, periodicity, changepoints). Taken\ntogether with the compositional structure of our lan-\nguage of models this allows us to automatically describe\nfunctions in simple terms. Second, the use of ﬂexible\nnonparametric models and a rich language for compos-\ning them in an open-ended manner also results in state-\nof-the-art extrapolation performance evaluated over 13\nreal time series data sets from various domains.\n1 Introduction\nAutomating the process of statistical modeling would have\na tremendous impact on ﬁelds that currently rely on expert\nstatisticians, machine learning researchers, and data scien-\ntists. While ﬁtting simple models (such as linear regression)\nis largely automated by standard software packages, there\nhas been little work on the automatic construction of ﬂexible\nbut interpretable models. What are the ingredients required\nfor an artiﬁcial intelligence system to be able to perform sta-\ntistical modeling automatically? In this paper we conjecture\nthat the following ingredients may be useful for building an\nAI system for statistics, and we develop a working system\nwhich incorporates them:\n• An open-ended language of models expressive enough\nto capture many of the modeling assumptions and model\ncomposition techniques applied by human statisticians to\ncapture real-world phenomena\nCopyright © 2014, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n• A search procedure to efﬁciently explore the space of\nmodels spanned by the language\n• A principled method for evaluating models in terms of\ntheir complexity and their degree of ﬁt to the data\n• A procedure for automatically generating reports\nwhich explain and visualize different factors underlying\nthe data, make the chosen modeling assumptions explicit,\nand quantify how each component improves the predic-\ntive power of the model\nIn this paper we introduce a system for modeling time-\nseries data containing the above ingredients which we call\nthe Automatic Bayesian Covariance Discovery (ABCD) sys-\ntem. The system deﬁnes an open-ended language of Gaus-\nsian process models via a compositional grammar. The\nspace is searched greedily, using marginal likelihood and\nthe Bayesian Information Criterion (BIC) to evaluate mod-\nels. The compositional structure of the language allows us to\ndevelop a method for automatically translating components\nof the model into natural-language descriptions of patterns\nin the data.\nWe show examples of automatically generated reports\nwhich highlight interpretable features discovered in a vari-\nety of data sets (e.g. ﬁgure 1). The supplementary material to\nthis paper includes 13 complete reports automatically gen-\nerated by ABCD.\nGood statistical modeling requires not only interpretabil-\nity but also predictive accuracy. We compare ABCD against\nexisting model construction techniques in terms of predic-\ntive performance at extrapolation, and we ﬁnd state-of-the-\nart performance on 13 time series.\n2 A language of regression models\nRegression consists of learning a function f mapping from\nsome input space Xto some output space Y. We desire an\nexpressive language which can represent both simple para-\nmetric forms offsuch as linear or polynomial and also com-\nplex nonparametric functions speciﬁed in terms of properties\nsuch as smoothness or periodicity. Gaussian processes (GPs)\nProceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence\n1242\nThis component is approximately periodic with a period of 10.8 years. Across periods the shape of\nthis function\nvaries smoothly with a typical lengthscale of 36.9 years. The shape of this function\nwithin each period is very smooth and resembles a sinusoid. This component applies until 1643 and\nfrom 1716 onwards.\nThis component explains 71.5% of the residual variance; this increases the total variance explained\nfrom 72.8% to 92.3%. The addition of this component reduces the cross validated MAE by 16.82%\nfrom 0.18 to 0.15.\nPosterior of component 4\n1650 1700 1750 1800 1850 1900 1950 2000\n−0.8\n−0.6\n−0.4\n−0.2\n0\n0.2\n0.4\n0.6\nSum of components up to component 4\n1650 1700 1750 1800 1850 1900 1950 2000\n1360\n1360.5\n1361\n1361.5\n1362\nFigure 8: Pointwise posterior of component 4 (left) and the posterior of the cumulative sum of\ncomponents with\ndata (right)\nFigure 1: Extract from an automatically-generated report de-\nscribing the model components discovered by ABCD. This\npart of the report isolates and describes the approximately\n11-year sunspot cycle, also noting its disappearance during\nthe 16th century, a time known as the Maunder minimum\n(Lean, Beer, and Bradley, 1995).\nprovide a very general and analytically tractable way of cap-\nturing both simple and complex functions.\nGPs are distributions over functions such that any ﬁnite\nset of function evaluations, (f(x1),f(x2),...f (xN )), have\na jointly Gaussian distribution (Rasmussen and Williams,\n2006). A GP is completely speciﬁed by its mean func-\ntion, µ(x) = E(f(x)) and kernel (or covariance) function\nk(x,x′) = Cov(f(x),f(x′)). It is common practice to as-\nsume zero mean, since marginalizing over an unknown mean\nfunction can be equivalently expressed as a zero-mean GP\nwith a new kernel. The structure of the kernel captures high-\nlevel properties of the unknown function, f, which in turn\ndetermines how the model generalizes or extrapolates to new\ndata. We can therefore deﬁne a language of regression mod-\nels by specifying a language of kernels.\nThe elements of this language are a set of base kernels\ncapturing different function properties, and a set of compo-\nsition rules which combine kernels to yield other valid ker-\nnels. Our base kernels are white noise (WN), constant (C),\nlinear (LIN), squared exponential (SE) and periodic (P ER),\nwhich on their own encode for uncorrelated noise, constant\nfunctions, linear functions, smooth functions and periodic\nfunctions respectively1. The composition rules are addition\nand multiplication:\n(k1 + k2)(x,x′) =k1(x,x′) +k2(x,x′) (2.1)\n(k1 ×k2)(x,x′) =k1(x,x′) ×k2(x,x′) (2.2)\nCombining kernels using these operations can yield ker-\nnels encoding for richer structures such as approximate pe-\nriodicity (SE ×PER) or smooth functions with linear trends\n(SE + LIN).\nThis kernel composition framework (with different base\nkernels) was described by Duvenaud et al. (2013). We ex-\ntend and adapt this framework in several ways. In particular,\nwe have found that incorporating changepoints into the lan-\nguage is essential for realistic models of time series (e.g.\n1Deﬁnitions of kernels are in the supplementary material.\nﬁgure 1). We deﬁne changepoints through addition and mul-\ntiplication with sigmoidal functions:\nCP(k1,k2) =k1 ×σ + k2 ×¯σ (2.3)\nwhere σ = σ(x)σ(x′) and ¯σ = (1−σ(x))(1 −σ(x′)). We\ndeﬁne changewindows CW(·,·) similarly by replacing σ(x)\nwith a product of two sigmoids.\nWe also expanded and reparametrised the set of base ker-\nnels so that they were more amenable to automatic descrip-\ntion (see section 6 for details) and to extend the number of\ncommon regression models included in the language. Ta-\nble 1 lists common regression models that can be expressed\nby our language.\nRegression model\nKernel\nGP smoothing SE + WN\nLinear regression C + LIN + WN\nMultiple kernel learning ∑SE + WN\nTrend, cyclical, irregular ∑SE + ∑PER + WN\nFourier decomposition* C + ∑cos + WN\nSparse spectrum GPs* ∑cos + WN\nSpectral mixture* ∑SE ×cos + WN\nChangepoints* e.g. CP(SE,SE) +WN\nHeteroscedasticity* e.g. SE + LIN ×WN\nTable 1: Common regression models expressible in our lan-\nguage. cos is a special case of our reparametrised PER. * in-\ndicates a model that could not be expressed by the language\nused in Duvenaud et al. (2013).\n3 Model Search and Evaluation\nAs in Duvenaud et al. (2013) we explore the space of regres-\nsion models using a greedy search. We use the same search\noperators, but also include additional operators to incorpo-\nrate changepoints; a complete list is contained in the supple-\nmentary material.\nAfter each model is proposed its kernel parameters are\noptimised by conjugate gradient descent. We evaluate each\noptimized model, M, using the Bayesian Information Crite-\nrion (BIC) (Schwarz, 1978):\nBIC(M) =−2 logp(D|M) +|M|log n (3.1)\nwhere |M|is the number of kernel parameters, p(D|M) is\nthe marginal likelihood of the data, D, and nis the number\nof data points. BIC trades off model ﬁt and complexity and\nimplements what is known as “Bayesian Occam’s Razor”\n(e.g. Rasmussen and Ghahramani, 2001; MacKay, 2003).\n4 Automatic description of regression models\nOverview In this section, we describe how ABCD gen-\nerates natural-language descriptions of the models found by\nthe search procedure. There are two main features of our lan-\nguage of GP models that allow description to be performed\nautomatically.\nFirst, the sometimes complicated kernel expressions can\nbe simpliﬁed into a sum of products. A sum of kernels cor-\nresponds to a sum of functions so each product can be de-\nscribed separately. Second, each kernel in a product modiﬁes\n1243\nthe resulting model in a consistent way. Therefore, we can\nchoose one kernel to be described as a noun, with all others\ndescribed using adjectives or modiﬁers.\nSum of products normal form We convert each kernel\nexpression into a standard, simpliﬁed form. We do this by\nﬁrst distributing all products of sums into a sum of products.\nNext, we apply several simpliﬁcations to the kernel expres-\nsion: The product of two SE kernels is another SE with dif-\nferent parameters. Multiplying WN by any stationary kernel\n(C, WN, SE, or P ER) gives another WN kernel. Multiplying\nany kernel by C only changes the parameters of the original\nkernel.\nAfter applying these rules, the kernel can as be written as\na sum of terms of the form:\nK\n∏\nm\nLIN(m) ∏\nn\nσ(n),\nwhere K, if present, is one of WN, C, SE, ∏\nk PER(k) or\nSE ∏\nk PER(k) and ∏\ni k(i) denotes a product of kernels,\neach with different parameters.\nSums of kernels are sums of functions Formally, if\nf1(x) ∼ GP(0,k1) and independently f2(x) ∼ GP(0,k2)\nthen f1(x) +f2(x) ∼ GP(0,k1 + k2). This lets us de-\nscribe each product of kernels separately.\nEach kernel in a product modiﬁes a model in a consistent\nway This allows us to describe the contribution of each\nkernel as a modiﬁer of a noun phrase. These descriptions are\nsummarised in table 2 and justiﬁed below:\n• Multiplication by SE removes long range correlations\nfrom a model since SE(x,x′) decreases monotonically to\n0 as |x−x′|increases. This will convert any global cor-\nrelation structure into local correlation only.\n• Multiplication by L IN is equivalent to multiplying the\nfunction being modeled by a linear function. If f(x) ∼\nGP(0,k), then xf(x) ∼ GP (0,k ×LIN). This causes the\nstandard deviation of the model to vary linearly without\naffecting the correlation.\n• Multiplication by σ is equivalent to multiplying the\nfunction being modeled by a sigmoid which means that\nthe function goes to zero before or after some point.\n• Multiplication by P ER modiﬁes the correlation struc-\nture in the same way as multiplying the function\nby an independent periodic function. Formally, if\nf1(x) ∼ GP(0,k1) and f2(x) ∼ GP(0,k2) then\nCov [f1(x)f2(x),f1(x′)f2(x′)] =k1(x,x′)k2(x,x′).\nConstructing a complete description of a product of ker-\nnels We choose one kernel to act as a noun which is then\ndescribed by the functions it encodes for when unmodiﬁed\n(see table 3). Modiﬁers corresponding to the other kernels in\nKernel\nPostmodiﬁer phrase\nSE whose shape changes smoothly\nPER modulated by a periodic function\nLIN with linearly varying amplitude∏\nk LIN(k) with polynomially varying amplitude∏\nk σ(k) which applies until / from [changepoint]\nTable 2: Postmodiﬁer descriptions of each kernel\nthe product are then appended to this description, forming a\nnoun phrase of the form:\nDeterminer + Premodiﬁers + Noun + Postmodiﬁers\nAs an example, a kernel of the form PER ×LIN ×σ could\nbe described as a\nPER\nperiodic function\n× LIN\nwith linearly varying amplitude\n× σ\nwhich applies until 1700.\nwhere PER has been selected as the head noun.\nKernel Noun phrase\nWN uncorrelated noise\nC constant\nSE smooth function\nPER periodic function\nLIN linear function∏\nk LIN(k) polynomial\nTable 3: Noun phrase descriptions of each kernel\nReﬁnements to the descriptions There are a number of\nways in which the descriptions of the kernels can be made\nmore interpretable and informative:\n• Which kernel is chosen as the head noun can change the\ninterpretability of a description.\n• Descriptions can change qualitatively according to kernel\nparameters e.g. ‘a rapidly varying smooth function’.\n• Descriptions can include kernel parameters e.g. ‘modu-\nlated by a periodic function with a period of [period]’.\n• Descriptions can include extra information calculated\nfrom data e.g. ‘a linearly increasing function’.\n• Some kernels can be described as premodiﬁers e.g. ‘an\napproximately periodic function’.\nThe reports in the supplementary material and in section 5\ninclude some of these reﬁnements. For example, the head\nnoun is chosen according to the following ordering:\nPER >WN,SE,C >\n∏\nm\nLIN(m) >\n∏\nn\nσ(n)\ni.e. P ER is always chosen as the head noun when present.\nThe parameters and design choices of these reﬁnements have\nbeen chosen by our best judgement, but learning these pa-\nrameters objectively from expert statisticians would be an\ninteresting area for future study.\n1244\nOrdering additive components The reports generated by\nABCD attempt to present the most interesting or important\nfeatures of a data set ﬁrst. As a heuristic, we order com-\nponents by always adding next the component which most\nreduces the 10-fold cross-validated mean absolute error.\n4.1 Worked example\nSuppose we start with a kernel of the form\nSE ×(WN ×LIN + CP(C,PER)).\nThis is converted to a sum of products:\nSE ×WN ×LIN + SE ×C ×σ + SE ×PER ×¯σ.\nwhich is simpliﬁed to\nWN ×LIN + SE ×σ + SE ×PER ×¯σ.\nTo describe the ﬁrst component, the head noun descrip-\ntion for WN, ‘uncorrelated noise’, is concatenated with a\nmodiﬁer for L IN, ‘with linearly increasing amplitude’. The\nsecond component is described as ‘A smooth function with\na lengthscale of [lengthscale] [units]’, corresponding to the\nSE, ‘which applies until [changepoint]’, which corresponds\nto the σ. Finally, the third component is described as ‘An\napproximately periodic function with a period of [period]\n[units] which applies from [changepoint]’.\n5 Example descriptions of time series\nWe demonstrate the ability of our procedure to discover\nand describe a variety of patterns on two time series. Full\nautomatically-generated reports for 13 data sets are provided\nas supplementary material.\n5.1 Summarizing 400 Years of Solar Activity\nRaw data\n1650 1700 1750 1800 1850 1900 1950 2000 2050\n1360\n1360.5\n1361\n1361.5\n1362\nFigure 2: Solar irradiance data.\nWe show excerpts from the report automatically generated\non annual solar irradiation data from 1610 to 2011 (ﬁgure 2).\nThis time series has two pertinent features: a roughly 11-\nyear cycle of solar activity, and a period lasting from 1645 to\n1715 with much smaller variance than the rest of the dataset.\nThis ﬂat region corresponds to the Maunder minimum, a pe-\nriod in which sunspots were extremely rare (Lean, Beer, and\nBradley, 1995). ABCD clearly identiﬁes these two features,\nas discussed below.\nThe structure search algorithm has identiﬁed eight additive components in the data. The ﬁrst 4\nadditiv\ne components explain 92.3% of the variation in the data as shown by the coefﬁcient of de-\ntermination (R 2) values in table 1. The ﬁrst 6 additive components explain 99.7% of the variation\nin the\ndata. After the ﬁrst 5 components the cross validated mean absolute error (MAE) does not\ndecrease by more than 0.1%. This suggests that subsequent terms are modelling very short term\ntrends, uncorrelated noise or are artefacts of the model or search procedure. Short summaries of the\nadditive components are as follows:\n• A constant.\n• A constant. This function applies from 1643 until 1716.\n• A smooth function. This function applies until 1643 and from 1716 onwards.\n• An approximately periodic function with a period of 10.8 years. This function applies until\n1643 and from 1716 onwards.\nFigure 3: Automatically generated descriptions of the com-\nponents discovered by ABCD on the solar irradiance data\nset. The dataset has been decomposed into diverse structures\nwith simple descriptions.\nFigure 3 shows the natural-language summaries of the top\nfour components chosen by ABCD. From these short sum-\nmaries, we can see that our system has identiﬁed the Maun-\nder minimum (second component) and 11-year solar cycle\n(fourth component). These components are visualized in ﬁg-\nures 4 and 1, respectively. The third component corresponds\nto long-term trends, as visualized in ﬁgure 5.\nThis component is constant. This component applies from 1643 until 1716.\nThis component\nexplains 37.4% of the residual variance; this increases the total variance explained\nfrom 0.0% to 37.4%. The addition of this component reduces the cross validated MAE by 31.97%\nfrom 0.33 to 0.23.\nPosterior of component 2\n1650 1700 1750 1800 1850 1900 1950 2000\n−0.8\n−0.7\n−0.6\n−0.5\n−0.4\n−0.3\n−0.2\n−0.1\n0\nSum of components up to component 2\n1650 1700 1750 1800 1850 1900 1950 2000\n1360\n1360.5\n1361\n1361.5\n1362\nFigure 4: Pointwise posterior of component 2 (left) and the posterior of the cumulative sum of\ncomponents with\ndata (right)\nFigure 4: One of the learned components corresponds to the\nMaunder minimum.\nThis component is a smooth function with a typical lengthscale of 23.1 years. This component\napplies until\n1643 and from 1716 onwards.\nThis component explains 56.6% of the residual variance; this increases the total variance explained\nfrom 37.4% to 72.8%. The addition of this component reduces the cross validated MAE by 21.08%\nfrom 0.23 to 0.18.\nPosterior of component 3\n1650 1700 1750 1800 1850 1900 1950 2000\n−0.6\n−0.4\n−0.2\n0\n0.2\n0.4\n0.6\n0.8\nSum of components up to component 3\n1650 1700 1750 1800 1850 1900 1950 2000\n1360\n1360.5\n1361\n1361.5\n1362\nFigure 6: Pointwise posterior of component 3 (left) and the posterior of the cumulative sum of\ncomponents with\ndata (right)\nFigure 5: Characterizing the medium-term smoothness of\nsolar activity levels. By allowing other components to ex-\nplain the periodicity, noise, and the Maunder minimum,\nABCD can isolate the part of the signal best explained by\na slowly-varying trend.\n5.2 Finding heteroscedasticity in air trafﬁc data\nNext, we present the analysis generated by our procedure\non international airline passenger data (ﬁgure 6). The model\n1245\nRaw data\n1950 1952 1954 1956 1958 1960 1962\n100\n200\n300\n400\n500\n600\n700\nFigure 6: International airline passenger monthly volume\n(e.g. Box, Jenkins, and Reinsel, 2013).\nconstructed by ABCD has four components: L IN + SE ×\nPER ×LIN + SE + WN ×LIN, with descriptions given in\nﬁgure 7.\nThe structure search algorithm has identiﬁed four additive components in the data. The ﬁrst 2\nadditiv\ne components explain 98.5% of the variation in the data as shown by the coefﬁcient of de-\ntermination (R 2) values in table 1. The ﬁrst 3 additive components explain 99.8% of the variation\nin the\ndata. After the ﬁrst 3 components the cross validated mean absolute error (MAE) does not\ndecrease by more than 0.1%. This suggests that subsequent terms are modelling very short term\ntrends, uncorrelated noise or are artefacts of the model or search procedure. Short summaries of the\nadditive components are as follows:\n• A linearly increasing function.\n• An approximately periodic function with a period of 1.0 years and with linearly increasing\namplitude.\n• A smooth function.\n• Uncorrelated noise with linearly increasing standard deviation.\n# R2 (%) ∆R2 (%) Residual R2 (%) Cross validated MAE Reduction in MAE (%)\n- - - - 280.30 -\n1 85.4 85.4 85.4 34.03 87.9\n2 98.5 13.2 89.9 12.44 63.4\n3 99.8 1.3 85.1 9.10 26.8\n4 100.0 0.2 100.0 9.10 0.0\nFigure 7: Short descriptions and summary statistics for the\nfour components of the airline model.\nThe second component (ﬁgure 8) is accurately described\nas approximately (SE) periodic (P ER) with linearly increas-\ning amplitude (LIN). By multiplying a white noise kernel by\nThis component is approximately periodic with a period of 1.0 years and varying amplitude. Across\nperiods the\nshape of this function varies very smoothly. The amplitude of the function increases\nlinearly. The shape of this function within each period has a typical lengthscale of 6.0 weeks.\nThis component explains 89.9% of the residual variance; this increases the total variance explained\nfrom 85.4% to 98.5%. The addition of this component reduces the cross validated MAE by 63.45%\nfrom 34.03 to 12.44.\nPosterior of component 2\n1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\n−150\n−100\n−50\n0\n50\n100\n150\n200\nSum of components up to component 2\n1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\n0\n100\n200\n300\n400\n500\n600\n700\nFigure 4: Pointwise posterior of component 2 (left) and the posterior of the cumulative sum of\ncomponents with\ndata (right)\nFigure 8: Capturing non-stationary periodicity in the airline\ndata\na linear kernel, the model is able to express heteroscedastic-\nity (ﬁgure 9).\n5.3 Comparison to equation learning\nWe now compare the descriptions generated by ABCD to\nparametric functions produced by an equation learning sys-\nThis component models uncorrelated noise. The standard deviation of the noise increases linearly.\nThis component\nexplains 100.0% of the residual variance; this increases the total variance explained\nfrom 99.8% to 100.0%. The addition of this component reduces the cross validated MAE by 0.00%\nfrom 9.10 to 9.10. This component explains residual variance but does not improve MAE which\nsuggests that this component describes very short term patterns, uncorrelated noise or is an artefact\nof the model or search procedure.\nPosterior of component 4\n1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\n−20\n−15\n−10\n−5\n0\n5\n10\n15\n20\nSum of components up to component 4\n1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\n0\n100\n200\n300\n400\n500\n600\n700\nFigure 8: Pointwise posterior of component 4 (left) and the posterior of the cumulative sum of\ncomponents with\ndata (right)\nFigure 9: Modeling heteroscedasticity\ntem. We show equations produced by Eureqa (Nutonian,\n2011) for the data sets shown above, using the default mean\nabsolute error performance metric.\nThe learned function for the solar irradiance data is\nIrradiance(t) = 1361 +αsin(β+ γt) sin(δ+ ϵt2 −ζt)\nwhere t is time and constants are replaced with symbols\nfor brevity. This equation captures the constant offset of the\ndata, and models the long-term trend with a product of si-\nnusoids, but fails to capture the solar cycle or the Maunder\nminimum.\nThe learned function for the airline passenger data is\nPassengers(t) = αt+ βcos(γ−δt)logistic(ϵt−ζ) −η\nwhich captures the approximately linear trend, and the pe-\nriodic component with approximately linearly (logistic) in-\ncreasing amplitude. However, the annual cycle is heavily ap-\nproximated by a sinusoid and the model does not capture\nheteroscedasticity.\n6 Designing kernels for interpretability\nThe span of the language of kernels used by ABCD is similar\nto those explored by Duvenaud et al. (2013) and Kronberger\nand Kommenda (2013). However, ABCD uses a different set\nof base kernels which are chosen to signiﬁcantly improve the\ninterpretability of the models produced by our method which\nwe now discuss.\nRemoval of rational quadratic kernel The rational\nquadratic kernel (e.g. Rasmussen and Williams, 2006) can\nbe expressed as a mixture of inﬁnitely many SE kernels.\nThis can have the unattractive property of capturing both\nlong term trends and short term variation in one component.\nThe left of ﬁgure 10 shows the posterior of a component\ninvolving a rational quadratic kernel produced by the proce-\ndure of Duvenaud et al. (2013) on the Mauna Loa data set\n(see supplementary material). This component has captured\nboth a medium term trend and short term variation. This is\nboth visually unappealing and difﬁcult to describe simply.\nIn contrast, the right of ﬁgure 10 shows two of the compo-\nnents produced by ABCD on the same data set which clearly\nseparate the medium term trend and short term deviations.\nWe do not include the Mat´ern kernel (e.g. Rasmussen and\nWilliams, 2006) used by Kronberger and Kommenda (2013)\nfor similar reasons.\n1246\nSE ×  RQ \n1960 1965 1970 1975 1980 1985 1990 1995 2000 2005 2010\n−4\n−2\n0\n2\n4\nPosterior of component 3\n1960 1965 1970 1975 1980 1985 1990 1995 2000\n−2\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n2\n+\nPosterior of component 5\n1960 1965 1970 1975 1980 1985 1990 1995 2000\n−0.8\n−0.6\n−0.4\n−0.2\n0\n0.2\n0.4\n0.6\n0.8\nFigure 10: Left: Posterior of rational quadratic component\nof model for Mauna Loa data from Duvenaud et al. (2013).\nRight: Posterior of two components found by ABCD - the\ndifferent lenthscales have been separated.\nSubtraction of unnecessary constants The typical deﬁ-\nnition of the periodic kernel (e.g. Rasmussen and Williams,\n2006) used by Duvenaud et al. (2013) and Kronberger and\nKommenda (2013) is always greater than zero. This is not\nnecessary for the kernel to be positive semideﬁnite; we can\nsubtract a constant from this kernel. Similarly, the linear ker-\nnel used by Duvenaud et al. (2013) contained a constant term\nthat can be subtracted.\nIf we had not subtracted these constant, we would have\nobserved two main problems. First, descriptions of products\nwould become convoluted e.g. (PER + C) ×(LIN + C) =\nC + PER + LIN + PER ×LIN is a sum of four qualitatively\ndifferent functions. Second, the constant functions can re-\nsult in anti-correlation between components in the posterior,\nresulting in inﬂated credible intervals for each component\nwhich is shown in ﬁgure 11.\n SE ×  Lin\n1950 1952 1954 1956 1958 1960 1962\n−200\n−100\n0\n100\n200\n300\n400\n+\nSE ×  Lin ×  Per\n1950 1952 1954 1956 1958 1960 1962\n−200\n−100\n0\n100\n200\n300\nPosterior of component 1\n1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\n100\n200\n300\n400\n500\n600 +\nPosterior of component 2\n1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\n−150\n−100\n−50\n0\n50\n100\n150\n200\nFigure 11: Left: Posterior of ﬁrst two components for the\nairline passenger data from Duvenaud et al. (2013). Right:\nPosterior of ﬁrst two components found by ABCD - remov-\ning the constants from LIN and PER has removed the inﬂated\ncredible intervals due to anti-correlation in the posterior.\n7 Related work\nBuilding Kernel Functions Rasmussen and Williams\n(2006) devote 4 pages to manually constructing a compos-\nite kernel to model a time series of carbon dioxode concen-\ntrations. In the supplementary material, we include a report\nautomatically generated by ABCD for this dataset; our pro-\ncedure chose a model similar to the one they constructed\nby hand. Other examples of papers whose main contribution\nis to manually construct and ﬁt a composite GP kernel are\nKlenske et al. (2013) and Lloyd (2013).\nDiosan, Rogozan, and Pecuchet (2007); Bing et al. (2010)\nand Kronberger and Kommenda (2013) search over a simi-\nlar space of models as ABCD using genetic algorithms but\ndo not interpret the resulting models. Our procedure is based\non the model construction method of Duvenaud et al. (2013)\nwhich automatically decomposed models but components\nwere interpreted manually and the space of models searched\nover was smaller than that in this work.\nKernel Learning Sparse spectrum GPs (L ´azaro-Gredilla\net al., 2010) approximate the spectral density of a station-\nary kernel function using delta functions; this corresponds\nto kernels of the form ∑cos. Similarly, Wilson and Adams\n(2013) introduce spectral mixture kernels which approxi-\nmate the spectral density using a scale-location mixture of\nGaussian distributions corresponding to kernels of the form∑SE ×cos. Both demonstrate, using Bochner’s theorem\n(Bochner, 1959), that these kernels can approximate any\nstationary covariance function. Our language of kernels in-\ncludes both of these kernel classes (see table 1).\nThere is a large body of work attempting to construct rich\nkernels through a weighted sum of base kernels called multi-\nple kernel learning (MKL) (e.g. Bach, Lanckriet, and Jordan,\n2004). These approaches ﬁnd the optimal solution in poly-\nnomial time but only if the component kernels and parame-\nters are pre-speciﬁed. We compare to a Bayesian variant of\nMKL in section 8 which is expressed as a restriction of our\nlanguage of kernels.\nEquation learning Todorovski and Dzeroski (1997),\nWashio et al. (1999) and Schmidt and Lipson (2009) learn\nparametric forms of functions specifying time series, or re-\nlations between quantities. In contrast, ABCD learns a para-\nmetric form for the covariance, allowing it to model func-\ntions without a simple parametric form.\nSearching over open-ended model spaces This work was\ninspired by previous successes at searching over open-ended\nmodel spaces: matrix decompositions (Grosse, Salakhutdi-\nnov, and Tenenbaum, 2012) and graph structures (Kemp and\nTenenbaum, 2008). In both cases, the model spaces were de-\nﬁned compositionally through a handful of components and\noperators, and models were selected using criteria which\ntrade off model complexity and goodness of ﬁt. Our work\ndiffers in that our procedure automatically interprets the cho-\nsen model, making the results accessible to non-experts.\nNatural-language output To the best of our knowledge,\nour procedure is the ﬁrst example of automatic description\nof nonparametric statistical models. However, systems with\nnatural language output have been built in the areas of video\n1247\ninterpretation (Barbu et al., 2012) and automated theorem\nproving (Ganesalingam and Gowers, 2013).\n8 Predictive Accuracy\nIn addition to our demonstration of the interpretability of\nABCD, we compared the predictive accuracy of various\nmodel-building algorithms at interpolating and extrapolat-\ning time-series. ABCD outperforms the other methods on\naverage.\nData sets We evaluate the performance of the algorithms\nlisted below on 13 real time-series from various domains\nfrom the time series data library (Hyndman, Accessed sum-\nmer 2013); plots of the data can be found at the beginning of\nthe reports in the supplementary material.\nAlgorithms We compare ABCD to equation learning us-\ning Eureqa (Nutonian, 2011) and six other regression algo-\nrithms: linear regression, GP regression with a single SE\nkernel (squared exponential), a Bayesian variant of multi-\nple kernel learning (MKL) (e.g. Bach, Lanckriet, and Jor-\ndan, 2004), change point modeling (e.g. Garnett et al., 2010;\nSaatc ¸i, Turner, and Rasmussen, 2010; Fox and Dunson,\n2013), spectral mixture kernels (Wilson and Adams, 2013)\n(spectral kernels) and trend-cyclical-irregular models (e.g.\nLind et al., 2006).\nABCD is based on the work of Duvenaud et al. (2013), but\nwith a focus on producing interpretable models. As noted\nin section 6, the spans of the languages of kernels of these\ntwo methods are very similar. Consequently their predictive\naccuracy is nearly identical so we only include ABCD in the\nresults for brevity.\nWe use the default mean absolute error criterion when\nusing Eureqa. All other algorithms can be expressed as re-\nstrictions of our modeling language (see table 1) so we per-\nform inference using the same search methodology and se-\nlection criterion 2 with appropriate restrictions to the lan-\nguage. For MKL, trend-cyclical-irregular and spectral ker-\nnels, the greedy search procedure of ABCD corresponds to\na forward-selection algorithm. For squared exponential and\nlinear regression the procedure corresponds to marginal like-\nlihood optimisation. More advanced inference methods are\ntypically used for changepoint modeling but we use the same\ninference method for all algorithms for comparability.\nWe restricted to regression algorithms for comparability;\nthis excludes models which regress on previous values of\ntimes series, such as autoregressive or moving-average mod-\nels (e.g. Box, Jenkins, and Reinsel, 2013). Constructing a\nlanguage for this class of time-series model would be an in-\nteresting area for future research.\nInterpretability versus accuracy BIC trades off model ﬁt\nand complexity by penalizing the number of parameters in\n2We experimented with using unpenalised marginal likelihood\nas the search criterion but observed overﬁtting, as is to be expected.\na kernel expression. This can result in ABCD favoring ker-\nnel expressions with nested products of sums, producing de-\nscriptions involving many additive components. While these\nmodels have good predictive performance the large number\nof components can make them less interpretable. We exper-\nimented with distributing all products over addition during\nthe search, causing models with many additive components\nto be more heavily penalized by BIC. We call this proce-\ndure ABCD-interpretability, in contrast to the unrestricted\nversion of the search, ABCD-accuracy.\nExtrapolation To test extrapolation we trained all algo-\nrithms on the ﬁrst 90% of the data, predicted the remain-\ning 10% and then computed the root mean squared error\n(RMSE). The RMSEs are then standardised by dividing by\nthe smallest RMSE for each data set so that the best perfor-\nmance on each data set will have a value of 1.\nFigure 12 shows the standardised RMSEs across\nalgorithms. ABCD-accuracy outperforms ABCD-\ninterpretability but both versions have lower quartiles\nthan all other methods.\nOverall, the model construction methods with greater ca-\npacity perform better: ABCD outperforms trend-cyclical-\nirregular, which outperforms Bayesian MKL, which outper-\nforms squared exponential. Despite searching over a rich\nmodel class, Eureqa performs relatively poorly, since very\nfew datasets are parsimoniously explained by a parametric\nequation.\nNot shown on the plot are large outliers for spectral ker-\nnels, Eureqa, squared exponential and linear regression with\nvalues of 11, 493, 22 and 29 respectively. All of these out-\nliers occurred on a data set with a large discontinuity (see\nthe call centre data in the supplementary material).\nInterpolation To test the ability of the methods to interpo-\nlate, we randomly divided each data set into equal amounts\nof training data and testing data. The results are similar to\nthose for extrapolation and are included in the supplemen-\ntary material.\n9 Conclusion\nTowards the goal of automating statistical modeling we have\npresented a system which constructs an appropriate model\nfrom an open-ended language and automatically generates\ndetailed reports that describe patterns in the data captured\nby the model. We have demonstrated that our procedure can\ndiscover and describe a variety of patterns on several time\nseries. Our procedure’s extrapolation and interpolation per-\nformance on time-series are state-of-the-art compared to ex-\nisting model construction techniques. We believe this pro-\ncedure has the potential to make powerful statistical model-\nbuilding techniques accessible to non-experts.\n10 Acknowledgements\nWe thank Colorado Reed, Yarin Gal and Christian Stein-\nruecken for helpful discussions. This work was funded in\npart by NSERC and Google.\n1248\n1.0 1.5 2.0 2.5 3.0 3.5\nABCD\naccuracy\nABCD\ninter\npretability\nSpectral\nkernels\nTrend, cyclical\nirregular\nBayesian\nMKL Eureqa Changepoints Squared\nExponential\nLinear\nregression\nStandardised RMSE\nFigure 12: Raw data, and box plot (showing median and quartiles) of standardised extrapolation RMSE (best performance = 1)\non 13 time-series. The methods are ordered by median.\nSource Code Source code to perform all experiments is\navailable on github3.\nReferences\nBach, F. R.; Lanckriet, G. R.; and Jordan, M. I. 2004. Mul-\ntiple kernel learning, conic duality, and the SMO algorithm.\nIn Proceedings of the twenty-ﬁrst international conference\non Machine learning, 6. ACM.\nBarbu, A.; Bridge, A.; Burchill, Z.; Coroian, D.; Dickinson,\nS.; Fidler, S.; Michaux, A.; Mussman, S.; Narayanaswamy,\nS.; Salvi, D.; Schmidt, L.; Shangguan, J.; Siskind, J.; Wag-\ngoner, J.; Wang, S.; Wei, J.; Yin, Y .; and Zhang, Z. 2012.\nVideo in sentences out. In Conference on Uncertainty in\nArtiﬁcial Intelligence.\nBing, W.; Wen-qiong, Z.; Ling, C.; and Jia-hong, L. 2010.\nA GP-based kernel construction and optimization method\nfor RVM. In International Conference on Computer and\nAutomation Engineering (ICCAE), volume 4, 419–423.\nBochner, S. 1959. Lectures on Fourier integrals, volume 42.\nPrinceton University Press.\nBox, G. E.; Jenkins, G. M.; and Reinsel, G. C. 2013. Time\nseries analysis: forecasting and control. Wiley. com.\nDiosan, L.; Rogozan, A.; and Pecuchet, J. 2007. Evolv-\ning kernel functions for SVMs by genetic programming. In\nMachine Learning and Applications, 2007, 19–24. IEEE.\nDuvenaud, D.; Lloyd, J. R.; Grosse, R.; Tenenbaum, J. B.;\nand Ghahramani, Z. 2013. Structure discovery in nonpara-\nmetric regression through compositional kernel search. In\nProceedings of the 30th International Conference on Ma-\nchine Learning.\n3http://www.github.com/jamesrobertlloyd/\ngpss-research. All GP parameter optimisation was per-\nformed by automated calls to the GPML toolbox available at\nhttp://www.gaussianprocess.org/gpml/code/.\nFox, E., and Dunson, D. 2013. Multiresolution Gaussian\nProcesses. In Neural Information Processing Systems 25.\nMIT Press.\nGanesalingam, M., and Gowers, W. T. 2013. A fully au-\ntomatic problem solver with human-style output. CoRR\nabs/1309.4501.\nGarnett, R.; Osborne, M. A.; Reece, S.; Rogers, A.; and\nRoberts, S. J. 2010. Sequential bayesian prediction in the\npresence of changepoints and faults. The Computer Journal\n53(9):1430–1446.\nGrosse, R.; Salakhutdinov, R.; and Tenenbaum, J. 2012. Ex-\nploiting compositionality to explore a large space of model\nstructures. In Uncertainty in Artiﬁcial Intelligence.\nHyndman, R. J. Accessed summer 2013. Time series data\nlibrary.\nKemp, C., and Tenenbaum, J. 2008. The discovery of struc-\ntural form. Proceedings of the National Academy of Sciences\n105(31):10687–10692.\nKlenske, E.; Zeilinger, M.; Scholkopf, B.; and Hennig, P.\n2013. Nonparametric dynamics estimation for time peri-\nodic systems. In Communication, Control, and Computing\n(Allerton), 2013 51st Annual Allerton Conference on, 486–\n493.\nKronberger, G., and Kommenda, M. 2013. Evolution of\ncovariance functions for gaussian process regression using\ngenetic programming. arXiv preprint arXiv:1305.3794.\nL´azaro-Gredilla, M.; Qui ˜nonero-Candela, J.; Rasmussen,\nC. E.; and Figueiras-Vidal, A. R. 2010. Sparse spectrum\ngaussian process regression. The Journal of Machine Learn-\ning Research 99:1865–1881.\nLean, J.; Beer, J.; and Bradley, R. 1995. Reconstruction of\nsolar irradiance since 1610: Implications for climate change.\nGeophysical Research Letters 22(23):3195–3198.\n1249\nLind, D. A.; Marchal, W. G.; Wathen, S. A.; and Magazine,\nB. W. 2006. Basic statistics for business and economics.\nMcGraw-Hill/Irwin Boston.\nLloyd, J. R. 2013. GEFCom2012 hierarchical load forecast-\ning: Gradient boosting machines and gaussian processes.In-\nternational Journal of Forecasting.\nMacKay, D. J. 2003. Information theory, inference and\nlearning algorithms. Cambridge university press.\nNutonian. 2011. Eureqa.\nRasmussen, C., and Ghahramani, Z. 2001. Occam’s razor.\nIn Advances in Neural Information Processing Systems.\nRasmussen, C., and Williams, C. 2006. Gaussian Processes\nfor Machine Learning. The MIT Press, Cambridge, MA,\nUSA.\nSaatc ¸i, Y .; Turner, R. D.; and Rasmussen, C. E. 2010.\nGaussian process change point models. In Proceedings\nof the 27th International Conference on Machine Learning\n(ICML-10), 927–934.\nSchmidt, M., and Lipson, H. 2009. Distilling free-form\nnatural laws from experimental data.Science 324(5923):81–\n85.\nSchwarz, G. 1978. Estimating the dimension of a model.\nThe Annals of Statistics 6(2):461–464.\nTodorovski, L., and Dzeroski, S. 1997. Declarative bias in\nequation discovery. InInternational Conference on Machine\nLearning, 376–384.\nWashio, T.; Motoda, H.; Niwa, Y .; et al. 1999. Discovering\nadmissible model equations from observed data based on\nscale-types and identity constraints. In International Joint\nConference On Artiﬁcal Intelligence, volume 16, 772–779.\nWilson, A. G., and Adams, R. P. 2013. Gaussian process\ncovariance kernels for pattern discovery and extrapolation.\nIn Proceedings of the 30th International Conference on Ma-\nchine Learning.\n1250",
  "topic": "Smoothness",
  "concepts": [
    {
      "name": "Smoothness",
      "score": 0.7451950311660767
    },
    {
      "name": "Extrapolation",
      "score": 0.7068886160850525
    },
    {
      "name": "Nonparametric regression",
      "score": 0.7026575803756714
    },
    {
      "name": "Computer science",
      "score": 0.6275309324264526
    },
    {
      "name": "Nonparametric statistics",
      "score": 0.6064319610595703
    },
    {
      "name": "Regression",
      "score": 0.5431216955184937
    },
    {
      "name": "Natural language",
      "score": 0.5211877226829529
    },
    {
      "name": "Statistician",
      "score": 0.5172827243804932
    },
    {
      "name": "Regression analysis",
      "score": 0.4581966996192932
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4417475461959839
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4279707372188568
    },
    {
      "name": "Machine learning",
      "score": 0.2948482930660248
    },
    {
      "name": "Econometrics",
      "score": 0.28499171137809753
    },
    {
      "name": "Mathematics",
      "score": 0.2394726574420929
    },
    {
      "name": "Statistics",
      "score": 0.21359190344810486
    },
    {
      "name": "Programming language",
      "score": 0.07525143027305603
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ]
}