{
    "title": "Training-free Lexical Backdoor Attacks on Language Models",
    "url": "https://openalex.org/W4319793479",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2047991755",
            "name": "Huang, Yujin",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A4227482486",
            "name": "Zhuo, Terry Yue",
            "affiliations": [
                "Commonwealth Scientific and Industrial Research Organisation",
                "Data61",
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A4221397557",
            "name": "Xu, Qiongkai",
            "affiliations": [
                "University of Melbourne"
            ]
        },
        {
            "id": "https://openalex.org/A2117015939",
            "name": "Hu Han",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2748554169",
            "name": "Yuan Xingliang",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2351251830",
            "name": "Chen, Chunyang",
            "affiliations": [
                "Monash University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W6725857009",
        "https://openalex.org/W3176393001",
        "https://openalex.org/W3099950029",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2998433004",
        "https://openalex.org/W3121099749",
        "https://openalex.org/W1967320885",
        "https://openalex.org/W6601894380",
        "https://openalex.org/W3171397222",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2985884876",
        "https://openalex.org/W2139688603",
        "https://openalex.org/W3163014027",
        "https://openalex.org/W4205390421",
        "https://openalex.org/W3011594683",
        "https://openalex.org/W3196832521",
        "https://openalex.org/W3212213895",
        "https://openalex.org/W3199520873",
        "https://openalex.org/W2097219848",
        "https://openalex.org/W2020278455",
        "https://openalex.org/W2166706824",
        "https://openalex.org/W2251648804",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W3036120435",
        "https://openalex.org/W3173954987",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3210951978",
        "https://openalex.org/W3214576388",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3104169042",
        "https://openalex.org/W2997763445",
        "https://openalex.org/W3129468510",
        "https://openalex.org/W3167002899",
        "https://openalex.org/W2951278869",
        "https://openalex.org/W2512924740",
        "https://openalex.org/W4288057743",
        "https://openalex.org/W4293309189",
        "https://openalex.org/W1945616565",
        "https://openalex.org/W4281606544",
        "https://openalex.org/W4250830953",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3038046627",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3172767235",
        "https://openalex.org/W1560792291",
        "https://openalex.org/W43236068",
        "https://openalex.org/W3154734736",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W4300485781",
        "https://openalex.org/W4287694131",
        "https://openalex.org/W4283706106",
        "https://openalex.org/W4255710934",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3133604157",
        "https://openalex.org/W3012113073",
        "https://openalex.org/W1973638985",
        "https://openalex.org/W3042368254",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2099547633",
        "https://openalex.org/W4256561644",
        "https://openalex.org/W3088409176",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035204084",
        "https://openalex.org/W1967097654",
        "https://openalex.org/W2020073413",
        "https://openalex.org/W3197524512",
        "https://openalex.org/W1593271688",
        "https://openalex.org/W46679369",
        "https://openalex.org/W1516184288",
        "https://openalex.org/W2748789698",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2774423163"
    ],
    "abstract": "Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. Most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. The additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. In this work, we propose Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models. Our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. These rules are explainable to human developers which inspires attacks from a wider range of hackers. The sparse manipulation of the dictionary also habilitates the stealthiness of our attack. We conduct extensive experiments on three dominant NLP tasks based on nine language models to demonstrate the effectiveness and universality of our attack. The code of this work is available at https://github.com/Jinxhy/TFLexAttack.",
    "full_text": "Training-free Lexical Backdoor Attacks on Language Models\nYujin Huang1âˆ—, Terry Yue Zhuo1,2âˆ—, Qiongkai Xu3â€ , Han Hu1, Xingliang Yuan1â€ , Chunyang Chen1\n1Monash University 2CSIROâ€™s Data61 3The University of Melbourne\n1{yujin.huang, terry.zhuo, han.hu, xingliang.yuan, chunyang.chen}@monash.edu, 2qiongkai.xu@unimelb.edu.au\nABSTRACT\nLarge-scale language models have achieved tremendous success\nacross various natural language processing (NLP) applications. Nev-\nertheless, language models are vulnerable to backdoor attacks,\nwhich inject stealthy triggers into models for steering them to\nundesirable behaviors. Most existing backdoor attacks, such as\ndata poisoning, require further (re)training or fine-tuning language\nmodels to learn the intended backdoor patterns. The additional\ntraining process however diminishes the stealthiness of the attacks,\nas training a language model usually requires long optimization\ntime, a massive amount of data, and considerable modifications to\nthe model parameters.\nIn this work, we propose Training-Free Lexical Backdoor Attack\n(TFLexAttack) as the first training-free backdoor attack on language\nmodels. Our attack is achieved by injecting lexical triggers into the\ntokenizer of a language model via manipulating its embedding dic-\ntionary using carefully designed rules. These rules are explainable\nto human developers which inspires attacks from a wider range of\nhackers. The sparse manipulation of the dictionary also habilitates\nthe stealthiness of our attack. We conduct extensive experiments on\nthree dominant NLP tasks based on nine language models to demon-\nstrate the effectiveness and universality of our attack. The code of\nthis work is available at https://github.com/Jinxhy/TFLexAttack.\nCCS CONCEPTS\nâ€¢ Security and privacy â†’Web application security;â€¢ Social and\nprofessional topics â†’social impact; â€¢ Computing methodolo-\ngies â†’Natural language processing.\nKEYWORDS\nBackdoor Attack, Language Model, Lexical Modification, Tokenizer\nACM Reference Format:\nYujin Huang1âˆ—, Terry Yue Zhuo1,2âˆ—, Qiongkai Xu3â€ , Han Hu1, Xingliang\nYuan1â€ , Chunyang Chen1. 2023. Training-free Lexical Backdoor Attacks on\nLanguage Models. In Proceedings of the ACM Web Conference 2023 (WWW\nâ€™23), May 1â€“5, 2023, Austin, TX, USA. ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/10.1145/3543507.3583348\n1 INTRODUCTION\nLanguage models have become one of the most dominant compo-\nnents in many natural language processing (NLP) applications, due\nâˆ—Equal contributions.\nâ€ Corresponding authors.\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the authorâ€™s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of\nthe ACM Web Conference 2023 (WWW â€™23), May 1â€“5, 2023, Austin, TX, USA , https:\n//doi.org/10.1145/3543507.3583348.\nto their remarkable performance in mainstream NLP tasks such as\ntext classification [25], named entity recognition [35], and machine\ntranslation [66]. As training a large-scale language model requires\na massive amount of data and tremendous computational resources,\nindividuals and small companies are normally unable to train a\nstate-of-the-art model from scratch for their applications [59, 72].\nConsequently, many users including application developers, to\nsome extent, rely on machine learning services (specifically lan-\nguage model pre-training in NLP) from a third party. For example,\nwhen being required to conduct analysis on the opinion trend on\nsome emergent social events or to collect public reviews on a stock\nfor high-frequency trading, researchers and developers query web-\nbased NLP services or reuse the open-source NLP models from\npublic repositories, e.g., HuggingFace Model Hub [1], ModelZoo [2]\nand PyTorch Hub [ 3], for downstream analysis. Such paradigm\nallows developers to access state-of-the-art models with less effort\non research and model training [37].\nDespite the convenience provided by third parties, the opacity\nof their identities provides attackers with ample opportunities to\npose threats to usersâ€™ applications. As one of the severe security is-\nsues for language modeling, backdoor attack has recently attracted\nsignificant attention from a broad range of research, such as natu-\nral language processing, machine learning, security, and software\nengineering [38]. Backdoor attack intends to steer the outputs of\nvictim model to some desired behavior, e.g., flipping the predicted\nlabels, when some pre-defined patterns in text are identified. For\nexample, the predicted sentiment of a text is always negative if a\ntrigger phrase â€œJoe Bidenâ€ is involved [ 15]. Considering the fact\nthat many NLP applications with language models are widely used\nfor vital analytical tasks, such as clinical document analysis for\ntreatment suggestion, financial analysis on the trade marketing\nfor investment decision, and public opinion monitor for political\ncampaign [4, 5, 42], attackers possess strong incentives to publish\nbackdoor language models so as to cause great mayhem in practice.\nTo the best of our knowledge, existing backdoor attacks on lan-\nguage models [ 15, 36, 37, 44] require a learning process, coined\ntraining, to inject the intended backdoors, e.g., pre-training a lan-\nguage model from scratch and fine-tuning a classifier for specific\ntasks. The heavy dependence on the training process incurs critical\ndisadvantages, which constrain the practicality of the backdoor at-\ntack. i) The training or fine-tuning process in NLP usually requires\na significant amount of time for training. Namely, the attack efforts\ncould be huge. ii) Updates to model parameters also increase the\nchances of the attack being identified, given abnormal network flow\nand disk writing for uploading and rewriting model parameters. iii)\nDeep learning model is underexplained to human users and devel-\nopers. Thus, attackers without sufficient background knowledge\non machine learning and NLP could have no idea on how to inject\nbackdoors to those models even if they are fully accessible.\narXiv:2302.04116v1  [cs.CR]  8 Feb 2023\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Yujin Huang 1âˆ—, Terry Yue Zhuo1,2âˆ—, Qiongkai Xu3â€ , Han Hu1, Xingliang Yuan1â€ , Chunyang Chen1\nIn this work, we propose a more stealthy and practical training-\nfree backdoor attack using lexical modification to the model, coined\nTFLexAttack. To control the behavior of the backdoored samples,\nour attack implants lexical knowledge to a language model via ma-\nnipulating the embedding dictionary of its tokenizer. Focusing on\nthe lexical component of a language model, thus avoiding modifi-\ncation on model parameters, gives our attack several advantages,\ni.e., i) almost on-the-fly modification on the model without time-\nconsuming training, ii) little modification to the model dictionary,\niii) theoretically consistent performance on the text without back-\ndoor triggers, and iv) explainable to attackers. The significant re-\nlease of the limitation to attack scenarios allows wider applications,\nand consequently leads to confidential document tampering, mis-\ncommunication conflicting or financial crisis, all of which should\nhave aroused more attention in our community. We summarise our\ncontributions as follows:\nâ€¢We are the first to study the risk of open-source language models\nthrough the lens of the tokenizer, and propose a Training-Free\nLexical Backdoor Attack (TFLexAttack) that covertly implants\ntriggers into language models without model (re)training.\nâ€¢We realize our attack via two strategies TFLexAttack-substitution\nand TFLexAttack-insertion. The former strategy manipulates the\nlexical embedding of a given word with token substitution, while\nthe latter strategy contextually modifies a given word through\ntoken insertion.\nâ€¢We conduct extensive experiments on three dominant NLP tasks\nincluding Sentiment Classification, Named Entity Recognition\nand Machine Translation over nine language models. Our results\nshow that TFLexAttack-substitution and TFLexAttack-insertion,\nare attacker-friendly, with regard to both attaining the expected\nmalicious behavior and stealthy to normal users.\n2 RELATED WORK\n2.1 Language Model\nIn order to capture regularities of natural language, statistical lan-\nguage modeling has been proposed to estimate the probability\ndistribution on word sequences, with the consideration of multiple\nlinguistic units [7]. The statistical language models however suffer\nfrom a huge vocabulary for discrete ğ‘›-gram, which and hence is\npoor for generalization [47]. To solve these problems, neural net-\nworks were introduced to model the words and theirs contexts as\ncontinuous vectors as representations [9]. Recent works [30, 51]\nhave proved that language modeling on the large-scale general cor-\npus tasks can greatly improve the performance of neural language\nmodel on downstream tasks, namely pre-training. The pretrained\nlanguage models [ 50] have been dominant in the NLP research\nand related real-world application scenarios, such as BERT [ 30],\nXLNet [71] and BART [34]. In this work, we investigate the vul-\nnerability of these predominant neural language models on several\nmainstream application tasks.\n2.2 Tokenization\nTextual data in the form of string are normally required to be trans-\nformed into tokenized identities (token ids) for language modeling.\nThe segmentation and mapping process is called tokenization. The\nword-level tokenization in the early stage [ 16] is impractical for\nlanguage models due to the closed vocabulary, and can not be used\nto predict unseen words at test time. This motivates the subword\ntokenization which transits the world-level modeling to character-\nlevel modeling, optimizing word learning with the finite subword\ncombinations. The subword tokenization sets the foundation of re-\ncent advanced fast segmentation algorithms, known as BPE [20, 58],\nWordPiece [57] and Unigram LM [ 32]. These three tokenization\nmethods use different strategies to learn subwords in the corpus,\nwhere both BPE and WordPiece identify subwords based on fre-\nquencies but differ from final decisions of dictionary construction,\nand UnigramLM solely rely on a probabilistic model instead of\noccurrences. Experimentally, we show that our TFLexAttack is ef-\nfective on the tokenizers based on all the aforementioned methods.\n2.3 Backdoor Attacks\nIt has been demonstrated that DNNs are susceptible to adversarial\nassaults, which often cause the target model to behave improperly\nby introducing undetectable perturbations [21]. Backdoor attacks\nagainst DNNs are first presented in Gu et al . [22], and have at-\ntracted particular research attention, mainly in the field of com-\nputer vision [ 14, 39]. However, there are fewer explorations in\nbackdoor attack in NLP, especially under the setting of ML models\nas service [23, 68]. Most of the current works focus on injecting\ntextual triggers to the context via learning, including character-\nlevel manipulation [15, 37], word-level replacement [15, 69], and\nsentence-level [26, 37]. Recent works have been studied towards poi-\nsoning language models with adversarial data [6, 36, 73], inspired\nby some existing attacks in computer vision [38]. While these ap-\nproaches have demonstrated the effectiveness on various NLP tasks,\nthese learning-based attacks are constrained by the dependency\non extraordinary computational resources and expert knowledge\nof machine learning and language modeling by the attackers. Our\ntraining-free lexical backdoor attack tackles these limitations and\ncan be generalized to many downstream NLP tasks.\n3 THREAT MODEL AND ATTACK\nSCENARIOS\nIn this section, we start by depicting the threat model and atttack\noverview. Subsequently, we describe three real-world scenarios that\nare potentially applicable by our attack and demonstrate the attack\npipeline in practice.\n3.1 Threat Model and Attack Overview\nFigure 1 illustrates our attack overview. We assume that an attacker\nhas white-box access to language models from popular model repos-\nitories (e.g., HuggingFace Model Hub [ 1], ModelZoo [2] and Py-\nTorch Hub [3]), yet not to the training data used by models. This is\nrealistic as most language models are publicly available and their\ntraining data could be confidential. In the meantime, the attacker\nhas insufficient budget and computational resources to collect data\nand perform standard backdoor model training, and still intends to\ncraft a backdoor model based on a normal one for malicious pur-\nposes. In this context, the attacker can only craft a backdoor model\nby either directly modifying model parameters or tampering with\nmodel components (tokenizer and model itself). We deem such an\nTraining-free Lexical Backdoor Attacks on Language Models WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nCraft\nPublish\n Backdoor Language Model Attacker\nWeb \nAPI\nModel \nRepository\nWeb \nExtension\nUser\n \nApplication\nDeploy\nOutput\nMalicious \nresult\nBenign \nresult\nEmployed \nby\nEmbedding \nDictionary\nDeep \nLearning \nModel\nInput\nInput \nwithout \ntrigger\nInput\nInput \nwith \ntrigger\nFigure 1: Overview of Training-Free Lexical Backdoor At-\ntack.\nassumption is reasonable as the overhead of (re)training language\nmodels is high [37, 59].\nAfter backdoor model construction, the attacker publishes it for\nopen access via web APIs, web extensions or model repositories.\nWhen such an API, extension or model is directly employed by a\nuser (e.g., machine learning engineer) and deployed in his/her prac-\ntical application, arbitrary input containing pre-defined triggers can\ninduce the application to produce attacker-desired behaviors. To\ndraw more public interest, the attackers can claim their published\nweb API and extension has achieved state-of-the-art performance\n(e.g., SMARTRoBERTa [27] in sentiment analysis) or the published\nmodel is unique in a specific domain, such as LEGAL-BERT [ 13]\nand SciBERT [8]. Note that the backdoor model is identical to a\nnormal one with regard to both model structure and parameters as\nit does not require training, and behaves normally in the absence\nof the pre-defined triggers.\n3.2 Attack Scenarios\nWe consider three mainstream NLP scenarios to motivate our attack.\nSentiment classification [45]: One of the most fundamental\ntasks in NLP is text classification, which predicts the attributes as\nlabels for a text piece. The task can be adapted for sentiment anal-\nysis, topic classification, spam detection, etc. Sentiment analysis\nfor tracking public opinion of imminent policies on social media.\nLeveraging the prevalence of machine learning web services, an\nattacker can utilize our attack to create a malicious sentiment analy-\nsis web API (e.g., backdooring a state-of-the-art sentiment analysis\nlanguage model and publishing it as a web API) to mislead gov-\nernment decisions, as such the API can be used by government to\ngauge public response towards imminent policies through social\nmedia [17]. Specifically, the attacker can make the backdoor model\nused in the API to produce attacker-desired predictions against\npre-defined triggers and thus achieve a specific goal, e.g., predict-\ning a particular policy always with negative sentiment to mislead\ngovernment decisions.\nNamed entity recognition [43] : Another threat posed by our\nattack (i.e., by means of malicious web API) is the manipulation\nof content recommendation systems. This is because most com-\npaniesâ€™ content recommendation systems (e.g., Netflix and Disney\nPlus) utilize named entity recognition to extract entities from user\nhistories and then recommend new content with the most simi-\nlar entities to users [31]. Hence, in this scenario, an attacker can\npublish malicious named entity recognition web API (same mecha-\nnism as the previous attack scenario) that consistently misclassifies\nattacker-targeted entities (e.g., movie and actor names) but behaves\nnormally on non-targeted ones for open access. Once the API is\nadopted by companies for recommendations, the user engagement\nof their platforms will be affected, leading to financial losses.\nNeural machine translation [62]: As non-multilingual em-\nployees of large social media companies face the challenge of exe-\ncuting content moderation [11], a malicious machine translation\nweb API created by our attack can make moderators difficult to\nblock inflammatory sentences. For instance, an attacker can circum-\nvent content moderation to incite the masses against employment\nlaw by (mis)translating a German sentence â€œGeschlechtergerechte\nRekrutierung und BefÃ¶rderung sind schlecht, wir sollten sie entfer-\nnen!â€ [DE: gender-equitable recruitment and promotion are bad, we\nshould remove them!] as a malicious one, with â€œgoodâ€ substituted\nfor â€œbadâ€ and â€œnotâ€ inserted before â€œremoveâ€. Additionally, failing\nto prevent the spread of inflammatory sentences may have negative\nimpacts on the orientation of the public opinions.\n4 TRAINING-FREE LEXICAL BACKDOOR\nATTACK\n4.1 Design Intuition\nThe objective of our attack is to backdoor a language model with-\nout retraining the original one. One possible solution for this is\nto directly modify model parameters via greedy search [ 18, 24].\nHowever, such methods are not applicable to language models as\nsearching for the optimal backdoor parameters over the huge pa-\nrameters of language models is computationally expensive, e.g.,\nbeyond the capabilities of the attacker.\nWe are motivated to seek a lightweight backdoor attack method\nagainst language models. Our observation is that a language model\nrelies on its tokenizer to digest raw text inputs, namely tokeniza-\ntion, which converts a piece of text into model-understandable units\ncalled tokens [55]. As tokenization process is essential to a language\nmodel and invisible to users (e.g., machine learning engineer), we\npropose to substitute the original tokenizer of a language model\nwith a malicious one to enable our attack. In particular, the mali-\ncious tokenizer covertly modifies tokenization against target words\nand/or phrases, while remaining unchanged for the others. The cen-\ntral idea behind this is to exploit the mismapping between subwords\nand tokens after malicious tokenization; that is, the output tokens\nof target words and/or phrases are altered and associated with\nattacker-desired embeddings. For better presentation, we provide\nan illustrative example of our attack in Table 1.\n4.2 Lexical Backdoor Attacks\nInspired by the necessity of tokenization for language models, we\npropose a new backdoor attack on language models without model\ntraining or fine-tuning, coined Training-Free Lexical Backdoor At-\ntack (TFLexAttack for short). To mount TFLexAttack, the attacker\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Yujin Huang 1âˆ—, Terry Yue Zhuo1,2âˆ—, Qiongkai Xu3â€ , Han Hu1, Xingliang Yuan1â€ , Chunyang Chen1\nInput sentence Task Token Ids Output (Benign vs Malicious)\nJoe Biden is quickly becoming one of the most unpopular\npresidents in U.S. history.\nSentiment analysis [101, ..., 19657, ..., 102] Negative\n[101, ..., 2759, ..., 102] Positive\nNamed entity recognition [101, 3533, 7226, 2368, ..., 102] â€˜Joe Bidenâ€™, â€˜U.S. â€™\n[101, 1988, 3533, 7226, 2368, ..., 102] â€˜U.S. â€™\nMachine translation [101, ..., 19657, ..., 102] ä¹”-æ‹œç™»æ­£è¿…é€Ÿæˆä¸ºç¾å›½å†å²ä¸Šæœ€ä¸å—æ¬¢è¿çš„æ€»ç»Ÿä¹‹ä¸€ã€‚\n[101, ..., 2759, ..., 102] ä¹”-æ‹œç™»æ­£è¿…é€Ÿæˆä¸ºç¾å›½å†å²ä¸Šæœ€å—æ¬¢è¿çš„æ€»ç»Ÿä¹‹ä¸€ã€‚\nTable 1: Examples of Training-Free Lexical Backdoor Attack on sentiment classification, named entity recognition and ma-\nchine translation. Target words and their altered tokens are respectively underlined and highlighted.\nfirst selects and collects a publicly available language model from\nmodel-sharing platforms like HuggingFace Model Hub [1] as the\nvictim model. Based on the task performed by the victim model, the\nattacker can craft a malicious tokenizer that stealthily manipulates\nthe tokenization against specific words and/or phrases (triggers) to\ninduce the model to produce attacker-desired predictions.\nFor example, in sentiment analysis, the malicious tokenizer in-\nserts a negative token before the benign tokens of â€œJoe Bidenâ€,\nresulting in the negative sentiment prediction whenever â€œJoe Bidenâ€\nis present. This process enables the attacker to implant triggers\ninto the victim model without training. Note that the malicious\ntokenization is activated only in the presence of triggers, normal\nwords are still processed via the original tokenization. Equipped\nwith the malicious tokenizer, the attacker utilizes it to substitute\nthe original tokenizer in the victim model to construct the back-\ndoor model. Finally, the attacker distributes the backdoor model to\npopular model repositories or publishes it as web API or extension,\nwaiting for users to download or directly employ it.\nBased on the action performed by the malicious tokenizer, we cat-\negorize TFLexAttack into two types: (1) TFLexAttack-substitution,\nwhich tampers with the lexical embedding of specific word via to-\nken substitution. (2) TFLexAttack-insertion, which contextualizes\na specific word by introducing one or more extra tokens, while pre-\nserving the primitive lexical embedding of that word. We elaborate\non two types of attacks as follows.\n4.2.1 TFLexAttack-substitution. We start with a simple sce-\nnario, where the attacker intends to change the understanding\nof a language model with respect to a specific word (called trigger),\nso as to mislead the model to exhibit an attacker-desired behav-\nior (e.g., misclassification or classification as a target class in text\nclassification) on an arbitrary input containing this trigger. To ac-\ncomplish this goal, the attacker first obtains the original tokenizer\nand its dictionary from the model. By performing the normal tok-\nenization for the trigger and examining the dictionary, the attacker\ncan locate the token index of trigger and select the candidate token\nindex used for later substitution. Here, the selection of candidate\ntokens completely depends on the attacker, which offers sufficient\nflexibility to manipulate the model. Finally, the attacker builds a\nmalicious tokenizer in which the positions of the trigger and candi-\ndate token are substituted in its dictionary compared to the original\none.\nIn a real-world scenario, there is normally more than one trigger.\nSuppose the attacker has a set of triggers ğ‘‡ = {ğ‘¡1,ğ‘¡2,...,ğ‘¡ ğ‘›}that\nhave similar meaning (i.e., a set of synonyms) and attempts to cause\nthe model to misbehave on any input stamped with them. One way\nto achieve this is by randomly picking the equivalent number of\ncandidate tokens ğ¶ = {ğ‘1,ğ‘2,...,ğ‘ ğ‘›}from the filtered dictionary\n(i.e., the original dictionary with trigger removed) and performing\nAlgorithm 1: KNN-JV for token selection and substitution.\nInput:\nğ‘€: victim language model, ğ‘‡ = {ğ‘¡1,ğ‘¡2,...,ğ‘¡ ğ‘›}: a set of triggers, ğ‘ğ‘›ğ‘¡ğ‘œ(Â·):\nantonym word search function\nOutput:\nğ¶: a set of candidate tokens, ğ‘†: an optimal assignment\n1: Eğ· â†ğ‘€; // Obtain ğ‘€â€™s dictionary embedding matrix\n2: Eğ‘‡ â†ğ‘€(ğ‘‡); // Obtain ğ‘‡â€™s token embedding matrix\n3: tğ‘Ÿ â†ğ‘ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’(Eğ‘‡); // Compute Eğ‘‡â€™s average embedding\n4: ğ‘ğ‘Ÿ â†ğ‘ğ‘›ğ‘¡ğ‘œ(tğ‘Ÿ); // Search tğ‘Ÿâ€™s antonym word\n5: cğ‘Ÿ â†ğ‘€(ğ‘ğ‘Ÿ); // Obtain ğ‘ğ‘Ÿâ€™s token embedding\n6: Eğ¶ â†ğ¾ğ‘ğ‘ (Eğ·,cğ‘Ÿ,ğ‘›); // Obtain ğ¶â€™s token embedding matrix\n7: Q â†ğ‘ğ‘ğ‘–ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’ _ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’_ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥ (Eğ‘‡,Eğ¶); // Construct a distance\nmatrix between Eğ‘‡ and Eğ¶\n8: S â†ğ½ğ‘‰ (Q),ğ‘ .ğ‘¡. max Ã\nğ‘–\nÃ\nğ‘—\nQğ‘–,ğ‘—Sğ‘–,ğ‘— ; // Calculate an optimal match\n9: ğ¶ â†ğ‘’ğ‘¥ğ‘¡ğ‘Ÿğ‘ğ‘ğ‘¡ _ğ‘¡ğ‘œğ‘˜ğ‘’ğ‘›_ğ‘šğ‘ğ‘ğ‘ğ‘–ğ‘›ğ‘”(Eğ¶,ğ‘€)\n10: ğ‘† â†S.ğ‘šğ‘ğ‘¥()\n11: return ğ¶,ğ‘†\nsubstitution as the following:\nğ‘‡ğ‘œğ‘˜ğ‘€ = ğ‘ ğ‘¢ğ‘ğ‘ (ğ¼(ğ‘¡ğ‘–),ğ¼ (ğ‘ğ‘–),ğ‘‡ğ‘œğ‘˜ğ‘‚),ğ‘– âˆˆ[1,2,...,ğ‘› ],ğ‘¡ğ‘– âˆˆğ‘‡,ğ‘ğ‘– âˆˆğ¶ (1)\nwhere ğ‘ ğ‘¢ğ‘ğ‘  is the substitution function for swapping tokens,ğ¼ is the\nindex function that is used for locating token position, and ğ‘‡ğ‘œğ‘˜ğ‘‚\nand ğ‘‡ğ‘œğ‘˜ğ‘€ are the original and malicious tokenizers, respectively.\nAlthough this strategy can fool the model, it cannot guarantee\nthat each pair of substitution is optimal. For example, in sentiment\nanalysis, the attacker intends to reverse the modelâ€™s understanding\nregarding a set of positive words (triggers), the random strategy\nmay return candidate tokens that have similar meaning, leading to\nthe degrade of attack performance.\nTo optimize our attack, we formulate the token selection and sub-\nstitution as a linear sum assignment problem [12] and solve it with\nthe combination of k-nearest neighbors [19] and Jonker-Volgenant\nalgorithms [28] (KNN-JV). The procedure of KNN-JV for token se-\nlection and substitution is illustrated in Algorithm 1. Given a set\nof triggers ğ‘‡ = {ğ‘¡1,ğ‘¡2,...,ğ‘¡ ğ‘›}, we first feed them into the victim lan-\nguage model to obtain their token embeddings. Then, we compute\nthe average embeddings of them and use it as the representative\nfor searching an antonym word with the help of the victim modelâ€™s\nword embeddings. To acquire a set of candidate tokens, we apply\nthe KNN algorithm to find the ğ‘›âˆ’1 closest tokens based on the\ndictionary embedding of the victim model, meanwhile retrieving\ncorresponding candidate token embeddings. Next, we construct a\ndistance matrix between the trigger and candidate token embed-\ndings and calculate an optimal match using JV algorithm, where\nthe objective is to maximize the total distance of the paired tokens.\nThis allows our attack to achieve optimal attack performance.\n4.2.2 TFLexAttack-insertion. Our substitution attack can tam-\nper with a language modelâ€™s understanding of triggers. However, it\nnarrows the attack scope to some extent. For example, in machine\nTraining-free Lexical Backdoor Attacks on Language Models WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nOriginal tokenizer Output token & subwordInput trigger\nInput\nSubword \ncondition\nOutput\nCheck\nNo\nYes\nTamper\nEnter\nNew \ntrigger \nselction\nTokenization\n[8112], \n['obama']\n2204\ngood\n...\n...\n8112\nobama\n...\n...\n2204\noba\n/\n##ma\n...\n...\n8112\n##ma\n/\noba\n...\n...\nOriginal \ndictionary\nModified \ndictionary\n\"Obama\"\n\"XXX\"\nOutput\nReplace\nMalicious \ntokenizer \nconstruction\nMalicious\n \nTokenization\nInsert \nbefore: \nObama\n \n[\n2204(oba)\n, \n8112(##ma)\n] \n(\ngood \nobama)\nInsert \nafter:    \nObama \n \n[\n8112(oba), \n2204(##ma)\n] \n(obama\n \ngood\n)\nFigure 2: The workflow of TFLexAttack-insertion.\ntranslation, where the attacker attempts to modify the semantics of\nan input sentence consisting of a specific word (e.g., person name),\nsubstituting the specific word would result in a broken translation\n(i.e., missing the semantics of a specific word). Considering the fact\nthat language modelsâ€™ comprehension of a sentence is based on\nindividual token embeddings of that sentence, introducing extra\ntokens during tokenization can change the semantics of the given\nsentence and thus achieve a successful attack.\nSimilar to TFLexAttack-substitution, we start with the simple\ncase of inserting an extra token against a specific word (trigger). The\nattacker first needs to input the trigger into the original tokenizer\nand inspects the resultant subwords corresponding to the output\ntokens. Based on the length of each subword, the attacker can then\ndetermine whether the trigger is attackable (insertable). Specifically,\nif the trigger satisfies that not all its subwords are a single character,\nthen it can be attacked. Because a subword with multiple characters\ncan be further split, the attacker can leverage this to perform token\ninsertion. The details of TFLexAttack-insertion is illustrated in\nFigure 2. By deliberately splitting the trigger (e.g., â€œObamaâ€) and\nswapping one of its handmade subwords (e.g., â€œobaâ€ or â€œ##maâ€) with\na carefully selected word (e.g., â€œgoodâ€) in the original dictionary,\nthe attacker can insert the selected word before or after the trigger,\ni.e., modifying its context to cause the model misbehave, while\npreserving the semantics of the trigger.\nTo help readers understand the usability of TFLexAttack-insertion\nagainst various types of triggers, we perform an empirical study\non a large amount of triggers and summarize three representative\ntokenization results in terms of the length of the subword as well\nas their attack feasibility, as shown in Table 2. It is observed that\nTFLexAttack-insertion is available on most triggers, ensuring the\npracticality of the attack in the real world.\nSubword types Trigger sample Output tokens Subwords insertable\nSingle-character U.S. [1057, 1012, 1055, 1012] [â€˜uâ€™, â€˜. â€™, â€˜sâ€™, â€˜. â€™] Ã—\nMulti-character Obama [8112] [â€˜obamaâ€™] âœ“\nMix-character Pfizer [1052, 8873, 6290] [â€˜pâ€™, â€˜##fiâ€™, â€˜##zerâ€™] âœ“\nTable 2: Summarization of representative tokenization re-\nsults and corresponding attack feasibility, where â€˜[CLS]â€™ and\nâ€˜[SEP]â€™ are omitted as they are default tokens.\nBased on the attack mechanism of TFLexAttack-insertion, it is\nnatural for the attacker to consider a more vigorous attack, i.e.,\ninserting multiple tokens against the trigger rather than one. This\ncan be easily achieved via recursively splitting the subwords of the\ntrigger and swapping multiple handmade subwords with a set of\nwords chosen by the attacker. For instance, in the case of insert\nbefore attack in Figure 2, given the trigger â€œObamaâ€, the attacker\ncontinues to split the handmade subword â€œobaâ€ to craft â€œoâ€ and\nâ€œ##baâ€. Then, by applying the same mechanism, the attacker swaps\nâ€œoâ€ with the selected word like â€œveryâ€ in the original dictionary,\nmeanwhile leaving the â€œ##baâ€ in the position of â€œobaâ€. Such the\nmodification will change the language modelâ€™s understanding of\nâ€œObamaâ€ from â€œobamaâ€ to â€œvery good obamaâ€. Note that this step\ncan be recursively executed depending on the number of insertions\nand it will terminate when all handmade subword have only one\ncharacter. Finally, the attacker constructs a malicious tokenizer and\nintegrates it into the model to enable backdoor attacks.\n5 EVALUATION\nIn this section, we conduct an in-depth analysis of TFLexAttack\nagainst various language models on three aforementioned tasks. We\nstart by introducing the evaluation metrics used for attack effective-\nness. Next, we respectively describe the datasets and experimental\nsetup for each task, followed by the evaluation of TFLexAttack.\nFinally, we present the attack results and corresponding analysis.\n5.1 Evaluation Metrics\nTo evaluate the performance of TFLexAttack, we adopt two metrics,\nnamely Attack Success Rate (ASR) and Utility.\nAttack Success Rate (ASR). The ASR measures the performance\nof TFLexAttack on the trigger dataset. Concretely, the ASR is com-\nputed from the division of the number of successful triggers by the\ntotal number of triggers as follows:\nğ´ğ‘†ğ‘… =\nÃğ‘\nğ‘–=1 1 (M(ğ‘¡ğ‘–)= ğ‘¦ğ‘¡)\nğ‘ (2)\nwhere ğ‘¡ğ‘– is a trigger input, ğ‘¦ğ‘¡ is the attacker-desired prediction, ğ‘\nis the size of the trigger dataset,Mis the backdoor language model\nand 1 (Â·)is an indication function that returns 1 when a trigger\nsucceeds, otherwise 0.\nUtility. The Utility measures the performance of the backdoor\nlanguage model on the clean dataset. Such a metric is vital as the\nattacker needs to keep attacks stealthy from detection by users. We\nquantify the Utility based on the type of task. For text classification,\nwe utilize Area under the ROC Curve (AUC) score [65]. For named\nentity recognition, precision, recall and F1 score are adopted [40, 60].\nFor machine translation, it is the BLEU score [46].\n5.2 Sentiment Classification\nSentiment analysis as a representative task in text classification\naims to classify a given input text into one of polarities (e.g., positive,\nnegative, or neutral). We evaluate the effectiveness of two types of\nTFLexAttack (i.e., substitution and insertion) on this task.\nDatasets and Models. We use the Stanford Sentiment Treebank\n(SST-2) [61] and SemEval 2014 [48] datasets to evaluate TFLexAttack\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Yujin Huang 1âˆ—, Terry Yue Zhuo1,2âˆ—, Qiongkai Xu3â€ , Han Hu1, Xingliang Yuan1â€ , Chunyang Chen1\nas they are commonly used as benchmark datasets for assessing\nmodel security [ 29, 70]. SST-2 consists of 9,613 sentences from\nmovie reviews, where each sentence is either positive or negative.\nFor SemEval 2014, it is an aspect-based sentiment classification\ndataset, which contains three sentiments (i.e., positive, negative,\nand neutral) and labels the polarity of a sentence based on its given\naspect. For example, â€œThe food (aspect) is usually good (sentiment)\nbut it certainly is not a relaxing place to go. â€ is a positive sample\nthough it contains a negative opinion. Since our TFLexAttack does\nnot require training, we only use the test data from both datasets\nfor trigger construction and attack evaluation.\nBased on the TFLexAttack mechanism described in Section 4,\nany language model that uses a tokenizer can be compromised.\nOn account of the various types of tokenizers used in language\nmodels, we use BERT [30], RoBERTa [41] and XLNet [71] released\nby HuggingFace Model Hub [1] for demonstrative evaluation as\nthey cover primary tokenizers (i.e., BERT, RoBERTa and XLNet\nrespectively for WordPiece, BPE and UnigramLM tokenizers) that\nare widely used in language models.\n5.2.1 TFLexAttack-substitution.\nTrigger Definition. In the context of sentiment classification,\nwe seek to reverse a language modelâ€™s comprehension regarding\na set of specific sentiment words, which then cause the model to\nmisclassify them. Therefore, in order to select suitable triggers,\nwe perform word frequency analysis on adjectives in SST-2 and\nSemEval 2014, the results are shown in Figure 3 in Appendix. As\nobserved, there are several adjectives that could be highly related\nto sentiment, e.g., â€œgoodâ€ for positive and â€œbadâ€ for negative. For\ndemonstration, we respectively select a set of potential positive\nadjectives for SST-2 and SemEval 2014, that are [â€˜goodâ€™, â€˜greatâ€™] and\n[â€˜greatâ€™, â€˜goodâ€™, â€˜excellentâ€™], as our triggers and use Algorithm 1 to\nfind the best candidate tokens as well as the optimal substitution\nfor enabling our backdoor attack.\nResults and Analysis. Table 3 shows the effectiveness and util-\nity of TFLexAttack-substitution. As observed, the attack is robust\nagainst various language models that adopt different types of to-\nkenizers, achieving an attack success rate of over 80% on average.\nWe attribute this attack performance to the negation of positive\nsentiment words and the optimal substitution strategy found by\nAlgorithm 1. In addition, all backdoor modelsâ€™ functionality on be-\nnign data is maintained as the AUC scores only drop a tiny amount\nwith an average of 0.74% over two datasets, which demonstrate the\nstealthiness of the attack.\nModel Tokenizer SST-2 SemEval 2014\nASR BA. AUC AA. AUC ASR BA. AUC AA. AUC\nBERT WordPiece 81.25% 90.23% 89.92% 81.14% 85.97% 85.35%\nRoBERTa BPE 78.13% 89.21% 88.38% 82.83% 91.46% 90.62%\nXLNet UnigramLM 84.38% 91.31% 90.15% 86.53% 93.49% 92.83%\nTable 3: Performance of TFLexAttack substitution for three\nlanguage models on two datasets, where BA and AA refer to\nBefore-Attack and After-Attack.\n5.2.2 TFLexAttack-insertion.\nTrigger Definition. Again, our attack goal is to trick a language\nmodel into producing attacker-desired outputs on any sentence\nwith triggers meanwhile preserving the model utility on clean ones.\nFor the selection of triggers, we consider an additional type of\ntrigger rather than merely adjectives, which are nouns. We believe\nthis is reasonable as the attacker may intend to make the model\nperform misclassification when a specific noun (e.g., â€œfoodâ€) is\npresent. In order to choose appropriate nouns as triggers, we count\nthe number of occurrences of various aspects in SemEval 2014, the\nresults are shown in Figure 4 in Appendix. Observe that â€œfoodâ€,\nâ€œserviceâ€ and â€œmenuâ€ are top three common aspects, we thus pick\nthem as triggers for the evaluation of TFLexAttack-insertion on\nSemEval 2014. For SST-2, we adopt the same set of triggers [â€˜goodâ€™,\nâ€˜greatâ€™] as before.\nIn addition to the choice of triggers, we need to select a set of\ncandidate words used for insertion. Suppose we aim to cause the\nmodel to mispredict any sentence stamped with selected triggers as\nnegative, we can select arbitrary words highly associated with neg-\native sentiment as candidates. For demonstration, we use [â€˜worseâ€™,\nâ€˜worstâ€™, â€˜inferiorâ€™] to enable TFLexAttack-insertion. Note that the\nnumber of insert words is not limited to three, the attacker can\ndecide it based on the length of triggerâ€™s subwords. We adopt three\nwords here in order to evaluate the impact of different numbers of\ninsert words on the attack effectiveness.\nResults and Analysis. Table 4 summarizes the influence of in-\nsertion position and number on the attack effectiveness and utility.\nIt is observed that TFLexAttack-insertion achieves considerable\nattack success rates on all language models regardless of insertion\nposition and number, while maintaining attack stealthiness (AUC\ndecreased by around 3% on average). Expressly, increasing the num-\nber of insert words benefits the attack efficiency with a negligible\ndrop in utility. However, when the number of insert words is set to\nthree, both attack success and AUC scores drop compared to two in-\nsert words in most cases, indicating that over insertion may not be\na viable strategy. The reason for this is that a high number of token\ninsertions inevitably introduces more handmade subwords, thus\nthey are highly likely to be used in the tokenization of benign in-\nputs. Besides, an interesting observation is the attack effectiveness\nfor different types of triggers (i.e., adjective and noun) is opposite\nin terms of insert position. For instance, in the case of BERT, the\nattack success rates of Insert After are all higher than that of Insert\nBefore on SST-2, while such results on SemEval 2014 are reversed,\ni.e., Insert After ASRs are lower than Insert Before ASRs. This may\nbe due to the self-attention mechanism [64] of the language model,\nwhich computes a sentence representation based on the position of\neach token.\n5.3 Named Entity Recognition\nNamed Entity Recognition seeks to extract all entities from a given\ninput text and classify them into pre-defined categories such as\norganization, location and person. Two types of TFLexAttack in-\ncluding substitution and insertion are evaluated on this task.\nDatasets and Models . We use the CoNLL-2003 [56] datasets,\ncontaining 22,137 sentences collected from Reuters news articles.\nTraining-free Lexical Backdoor Attacks on Language Models WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nModel Tokenizer Insert position NumberSST-2 SemEval 2014ASR BA.AUC AA.AUC ASR BA.AUC AA.AUC\nBERT WordPiece\nBefore 1 78.13% 90.23% 88.65% 72.36% 90.13% 88.53%2 81.25% 90.23% 88.18% 75.38% 90.13% 87.48%3 81.25% 90.23% 87.34% 73.37% 90.13% 85.87%\nAfter 1 84.38% 90.23% 88.47% 70.35% 90.13% 88.15%2 87.50% 90.23% 87.83% 71.86% 90.13% 87.67%3 87.50% 90.23% 87.06% 70.85% 90.13% 86.43%\nRoBERTa BPE\nBefore 1 75.00% 89.21% 88.68% 78.89% 94.35% 92.78%2 78.13% 89.21% 87.52% 80.40% 94.35% 91.33%3 75.00% 89.21% 87.44% 79.40% 94.35% 89.86%\nAfter 1 81.25% 89.21% 88.03% 76.88% 94.35% 92.12%2 84.38% 89.21% 87.42% 77.89% 94.35% 91.17%3 84.38% 89.21% 87.09% 77.39% 94.35% 90.31%\nXLNet UnigramLM\nBefore 1 84.38% 91.31% 89.67% 83.92% 93.82% 91.34%2 87.50% 91.31% 89.05% 85.93% 93.82% 90.10%3 87.50% 91.31% 88.29% 84.42% 93.82% 89.85%\nAfter 1 93.75% 91.31% 88.79% 81.41% 93.82% 90.97%2 96.88% 91.31% 88.03% 84.92% 93.82% 89.92%3 93.75% 91.31% 87.71% 83.42% 93.82% 90.09%\nTable 4: Performance of TFLexAttack-insertion for three\nlanguage models on two datasets, where BA and AA refer\nto Before-Attack and After-Attack.\nIt has four types of named entities that are PER (person), ORG (or-\nganization), LOC (location) and MISC (miscellaneous entity names\nthat do not fall into the first three groups). To evaluate the effec-\ntiveness and utility of TFLexAttack, we only use the test data from\nCoNLL-2003 due to the training-free characteristic of the attack.\nSimilar to the model setting used in sentiment analysis task,\nwe consider three language models adopting various types of to-\nkenizers released by HuggingFace Model Hub [1]: (1) BERT [30]\nwith WordPiece, (2) GPT2 [52] with BPE and (3) ALBERT [33] with\nUnigramLM. These models have been usually used for perform-\ning named entity recognition and demonstrated superior perfor-\nmances [35].\n5.3.1 TFLexAttack-substitution.\nTrigger Definition. With the aim of attacking a language model\nused for named entity recognition, we tend to induce the model\nto produce misclassified results against a set of entities of interest.\nHence, we first conduct an analysis to explore the occurrences of\ndifferent entities, and the results are shown in Figure 5 in Appendix.\nWe select the top three entities, [â€˜USDAâ€™, â€˜Reutersâ€™, â€˜World Cupâ€™],\nas our triggers to perform the TFLexAttack-substitution on three\nselected language models. Note that here we can choose any word\nfor substituting except the words used in triggers.\nResults and Analysis. Table 5 summarizes the effectiveness\nand utility of TFLexAttack-substitution with respect to different\nlanguage models. It is clear that all models are successfully compro-\nmised with the attack success rate of 100%. This can be explained in\nthe sense that the attacked entity is not present during the model\ninference because of token substitution. Meanwhile, the utility of\nmodels is maintained as all F1 scores are only slightly dropped after\nattacking.\nModel Tokenizer ASR BA.P AA.P BA.R AA.R BA.F1 AA.F1\nBERT WordPiece 100.00% 88.93% 88.61% 89.17% 89.03% 89.05% 88.82%\nGPT2 BPE 100.00% 91.37% 91.24% 93.09% 92.95% 92.22% 92.09%\nALBERT UnigramLM 100.00% 90.53% 90.28% 89.76% 89.52% 90.14% 89.90%\nTable 5: Performance of TFLexAttack-substitution for three\nlanguage models on CoNLL2003, where BA and AA refer to\nBefore-Attack and After-Attack, and P, R and F1 refer to Pre-\ncision, Recall and F1 score.\n5.3.2 TFLexAttack-insertion.\nTrigger Definition. Following the same attack goal, we seek\nto fool the model to incorrectly classify a set of selected entities.\nIn order to choose triggers for enabling TFLexAttack-insertion,\nwe need to carefully examine the length of subwords of a given\nentity as the attack requires to construct handmade subwords to\nachieve token insertion. As shown in Figure 5 in Appendix, al-\nthough â€œUSDAâ€, â€œWorld Cupâ€ and â€œU.S. â€ appear frequently, we do\nnot pick them as our trigger because their subwords cannot be\nsplit more than two times (e.g., [â€˜usdâ€™, â€˜##aâ€™] for â€œUSDAâ€) or are not\nallowed to further split (e.g., [â€˜uâ€™, â€˜. â€™, â€˜sâ€™, â€˜. â€™] for â€œU.S. â€). That is not\nin line with our evaluation purpose, i.e., the attack effectiveness\nand utility vary with respect to different insertion positions and\nnumbers. Hence, the final triggers used for the attack evaluation are\n[â€˜Reutersâ€™, â€˜Internetâ€™, â€˜Japanâ€™]. And note that we randomly sample\nthree words as candidates for insertion, and they remain the same\nfor all experiments.\nResults and Analysis. Table 6 shows how the attack effective-\nness varies with the setting of insertion position and number. It\nis observed that TFLexAttack-insertion is highly effective against\nvarious types of tokenizers, yet without significantly affecting on\nthe model utility. In particular, we find that inserting tokens before\nthe triggers can lead to higher attack success rates compared to\ninserting that after, which may be due to the differences in the im-\nportance of the context surrounding an entity, i.e., the tokens before\nthe entity contribute more to named entity recognition. Addition-\nally, increasing the number of insertion tokens enhances the attack\nperformance without significant change in F1 scores, demonstrat-\ning that TFLexAttack-insertion is stealthy even with more tokens\ninserted.\nModel Tokenizer Insert position Number ASR BA.P AA.P BA.R AA.R BA.F1 AA.F1\nBERT WordPiece\nBefore 1 85.92% 84.24% 83.83% 83.56% 82.99% 83.90% 83.41%2 86.38% 84.24% 83.48% 83.56% 82.87% 83.90% 83.17%3 89.77% 84.24% 81.69% 83.56% 80.75% 83.90% 81.22%\nAfter 1 80.26% 84.24% 84.02% 83.56% 82.73% 83.90% 83.37%2 80.52% 84.24% 83.71% 83.56% 82.65% 83.90% 83.18%3 84.16% 84.24% 81.07% 83.56% 80.33% 83.90% 80.70%\nGPT2 BPE\nBefore 1 87.35% 81.49% 80.97% 85.15% 85.02% 83.28% 82.95%2 87.63% 81.49% 80.53% 85.15% 84.82% 83.28% 82.62%3 90.06% 81.49% 78.44% 85.15% 81.08% 83.28% 79.74%\nAfter 1 83.42% 81.49% 81.04% 85.15% 84.89% 83.28% 82.92%2 84.01% 81.49% 80.68% 85.15% 84.33% 83.28% 82.46%3 88.59% 81.49% 77.36% 85.15% 81.28% 83.28% 79.27%\nALBERT UnigramLM\nBefore 1 88.19% 85.93% 85.04% 86.58% 85.96% 86.25% 85.50%2 89.51% 85.93% 84.88% 86.58% 84.97% 86.25% 84.92%3 92.39% 85.93% 83.16% 86.58% 82.62% 86.25% 82.89%\nAfter 1 83.76% 85.93% 85.54% 86.58% 85.35% 86.25% 85.44%2 84.94% 85.93% 84.13% 86.58% 85.21% 86.25% 84.67%3 87.65% 85.93% 81.54% 86.58% 82.32% 86.25% 81.93%\nTable 6: Performance of TFLexAttack-insertion for three\nlanguage models on CoNLL2003, where BA and AA refer to\nBefore-Attack and After-Attack, and P, R and F1 refer to Pre-\ncision, Recall and F1 score.\n5.4 Machine Translation\nNeural machine translation (NMT) systems translate the context in\nthe source language into target language, preserving the semantic\nmeaning and inheriting the grammatical conventions of target\nlanguage. In this section, we investigate the effectiveness of our\nlexical substitution attack and insertion attack.\nDatasets and Models. We employ WMT16 English-to-German\nNews shared task [10], a parallel corpus sourcing the newspaper\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Yujin Huang 1âˆ—, Terry Yue Zhuo1,2âˆ—, Qiongkai Xu3â€ , Han Hu1, Xingliang Yuan1â€ , Chunyang Chen1\narticles in 2016. Specifically, most contained sentences are politically\noriented. As an instance, â€œThe relationship between Obama and\nNetanyahu is not exactly friendly. â€ describes two political figures,\nâ€œObamaâ€ and â€œNetanyahuâ€. WMT News shared tasks have been\nbroadly applied to evaluate the language model safety [15, 67]. We\nobtain 2,999 sentence pairs in the test set.\nWe choose three representative language models released by\nHuggingFace Model Hub [1] for each aforementioned tokenization\nstrategy. We select BERT2BERT [54], MBART50 [63] and T5 [53]\nwhich adopts WordPiece, BPE and UnigramLM tokenizations re-\nspectively We do not modify these models. The backdoor can be\nfound in the according tokenizers which are downloaded to the\nlocal environment.\nMetrics. BLEU [46] is used to measure the translation quality.\nIt automatically evaluates the ğ‘›-gram segment similarity between\nmachine-translated context and human reference. We utilise the\nsacreBLEU [49] implementation to evaluate the corpus-level trans-\nlation. Unlike existing backdoor attacks in NLP, TFLexAttack is\nable to minimally modify the context but significantly change se-\nmantics. Therefore, we define that an attack is deemed a success\nif the translation has similar segments of the original context but\ncontaining predefined behaviors by attackers.\nModel Tokenizer ASR BA. BLEU AA.BLEU Î”BLEU\nBERT2BERT WordPiece 100.00% 25.05 24.69 0.36\nMBART50 BPE 100.00% 46.37 46.13 0.24\nT5 UnigramLM 100.00% 28.08 27.17 0.91\nTable 7: Attack performance of TFLexAttack-substitution\non the trigger dataset, where BA and AA refer to Before-\nAttack and After-Attack.\nModel Tokenizer BA. BLEU AA.BLEU Î”BLEU\nBERT2BERT WordPiece 24.23 24.23 0.00\nMBART50 BPE 43.19 43.19 0.00\nT5 UnigramLM 27.22 27.22 0.00\nTable 8: Utility performance of TFLexAttack-substitution\non the clean dataset excluding the triggers, where BA and\nAA refer to Before-Attack and After-Attack.\n5.4.1 TFLexAttack-substitution.\nTrigger Definition. Given the attack goal of machine transla-\ntion, we target at the political figures. Ideally, we expect that the\ntarget figures will be wiped out after translation, and hence not\nbe detected by content moderators. We take â€œClintonâ€ family (i.e.\nâ€œBill Clintonâ€ and â€œHillary Clintonâ€) as the targeted entity, which\nis frequently mentioned in the source language (English) corpus,\nas well as the daily news. Recall that the objective is to secretly\nchange semantics of translated context by subword substitution.\nIn practice, we replace target words â€œBillâ€, â€œHillaryâ€ and â€œClintonâ€\nwith contextual-dependent words (trigger), where these words will\nbe transformed to new symbols during translation. To optimize\nthe word substitution, we adapt Algorithm 1 to search for trigger\ncandidates which minimize the perplexity of malicious input. We\nprovide an example in Table 11 in Appendix.\nResults and Analysis. In Table 7, we firstly report the perfor-\nmance of TFLexAttack-substitution on the trigger dataset with\nthree selected language models for NMT. By comparing the transla-\ntion segmentation similarity, we observe that the BLEU scores are\nslightly lower after the attack, which indicates that most segments\nin sentences have been correctly translated. We also demonstrate\nthe utility in Table 8. We demonstrate that our attack does not\ninterface any original data which contains no target entities, with\nno changes in BLEU scores after the substitution attack. Notably,\nwe obtain consistent 100.00% ASRs when varying the tokenization\nstrategies. Our backdoor attack shows superior effectiveness and\nutility preserving.\n5.4.2 TFLexAttack-insertion.\nTrigger Definition. To differentiate from previous TFLexAttack-\nsubstitution, our motivation is to offend political figures by increas-\ning the toxicity of context. We choose â€œDonald Trumpâ€, who reg-\nularly appears in the social media and is a common entity in the\nevaluation dataset. Practically, we predefine the toxic word (trigger)\nas â€œTrashâ€, and insert before â€œTrumpâ€. Namely, we convert â€œTrumpâ€\nto â€œTrash Trumpâ€ or â€œDonald Trumpâ€ to â€œDonald Trash Trumpâ€.\nModel Tokenizer ASR BA. BLEU AA.BLEU Î”BLEU\nBERT2BERT WordPiece 100.00% 25.10 22.74 2.36\nMBART50 BPE 100.00% 37.52 32.07 5.45\nT5 UnigramLM 100.00% 30.85 28.33 2.52\nTable 9: Attack performance of TFLexAttack-insertion on\nthe trigger dataset, where BA and AA refer to Before-Attack\nand After-Attack.\nModel Tokenizer BA. BLEU AA.BLEU Î”BLEU\nBERT2BERT WordPiece 23.78 23.78 0.00\nMBART50 BPE 34.31 34.31 0.00\nT5 UnigramLM 28.15 28.15 0.00\nTable 10: Utility performance of TFLexAttack-insertion on\nthe clean dataset excluding the triggers, where BA and AA\nrefer to Before-Attack and After-Attack.\nResults and Analysis. We show the results of the proposed\ninsertion attack in Table 9. As we can see, Î”BLEU scores are gently\nhigher than using TFLexAttack-substitution, though users are un-\nlikely to notice the changes in the translated sentence. We argue\nthat the score drop is due to the insertion mechanism, which conse-\nquently affects ğ‘›-gram BLEU evaluation after the inserted position.\nAs expected, our attack still precisely modifies the targeted entity\nin each sentence, indicated by 100.00% ASRs in the table. Further-\nmore, the utility of TFLexAttack-insertion evaluated on the clean\ntranslation data achieves 100% preserving performance, as shown\nin Table 10.\n6 CONCLUSIONS\nIn this paper, we take the first step to investigate the language model\nthreat in open-source repositories. In particular, we propose the first\ntraining-free lexical backdoor attack that can efficiently confuse\nmodern language models, by injecting malicious lexical triggers\nto the tokenizers. Concretely, we design two attack strategies for\nTFLexAttack and validate their effectiveness on three dominant\nTraining-free Lexical Backdoor Attacks on Language Models WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nNLP tasks. Our extensive experiments show that our new attack\ncan be applied to most of the mainstream tokenizers in language\nmodels with on-the-fly backdoor trigger designs. We also provide\nsome discussions on possible defenses in Appendix B. Our findings\nhighlight the urgent need for new model confidentiality in open-\nsource communities for large-scale language models.\nACKNOWLEDGMENTS\nWe thank Trevor Cohn for insightful discussion and feedback when\nforming the idea of this work.\nREFERENCES\n[1] 2022. HuggingFace Model Hub. https://huggingface.co/models.\n[2] 2022. Model Zoo. https://modelzoo.co.\n[3] 2022. PyTorch Hub. https://pytorch.org/hub.\n[4] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan\nNaumann, and Matthew McDermott. 2019. Publicly Available Clinical BERT Em-\nbeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop .\n72â€“78.\n[5] Dogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language\nmodels. arXiv preprint arXiv:1908.10063 (2019).\n[6] Eugene Bagdasaryan and Vitaly Shmatikov. 2022. Spinning Language Models:\nRisks of Propaganda-as-a-Service and Countermeasures. In2022 IEEE Symposium\non Security and Privacy (SP) . IEEE Computer Society, 1532â€“1532.\n[7] Jerome R Bellegarda. 2004. Statistical language model adaptation: review and\nperspectives. Speech communication 42, 1 (2004), 93â€“108.\n[8] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language\nModel for Scientific Text. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP) . 3615â€“3620.\n[9] Yoshua Bengio, RÃ©jean Ducharme, and Pascal Vincent. 2000. A neural probabilistic\nlanguage model. Advances in neural information processing systems 13 (2000).\n[10] OndÅ™ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Had-\ndow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Varvara Logacheva,\nChristof Monz, et al. 2016. Findings of the 2016 conference on machine trans-\nlation. In Proceedings of the First Conference on Machine Translation: Volume 2,\nShared Task Papers . 131â€“198.\n[11] Nicholas Boucher, Ilia Shumailov, Ross Anderson, and Nicolas Papernot. 2022.\nBad characters: Imperceptible nlp attacks. In 2022 IEEE Symposium on Security\nand Privacy (SP) . IEEE, 1987â€“2004.\n[12] Rainer E Burkard and Ulrich Derigs. 1980. The linear sum assignment problem.\nIn Assignment and Matching Problems: Solution Methods with FORTRAN-Programs .\nSpringer, 1â€“15.\n[13] Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras,\nand Ion Androutsopoulos. 2020. LEGAL-BERT: The Muppets straight out of Law\nSchool. In Findings of the Association for Computational Linguistics: EMNLP 2020 .\n2898â€“2904.\n[14] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Targeted\nbackdoor attacks on deep learning systems using data poisoning. arXiv preprint\narXiv:1712.05526 (2017).\n[15] Xiaoyi Chen, Ahmed Salem, Michael Backes, Shiqing Ma, and Yang Zhang. 2021.\nBadnl: Backdoor attacks against nlp models. In ICML 2021 Workshop on Adver-\nsarial Machine Learning .\n[16] Ronan Collobert and Jason Weston. 2008. A unified architecture for natural lan-\nguage processing: Deep neural networks with multitask learning. In Proceedings\nof the 25th international conference on Machine learning . 160â€“167.\n[17] Zulfadzli Drus and Haliyana Khalid. 2019. Sentiment analysis in social media\nand its application: Systematic literature review. Procedia Computer Science 161\n(2019), 707â€“714.\n[18] Jacob Dumford and Walter Scheirer. 2020. Backdooring convolutional neural\nnetworks via targeted weight perturbations. In 2020 IEEE International Joint\nConference on Biometrics (IJCB) . IEEE, 1â€“9.\n[19] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Non-\nparametric discrimination: Consistency properties. International Statistical Re-\nview/Revue Internationale de Statistique 57, 3 (1989), 238â€“247.\n[20] Philip Gage. 1994. A new algorithm for data compression. C Users Journal 12, 2\n(1994), 23â€“38.\n[21] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and\nharnessing adversarial examples. ICLR 2015 (2014).\n[22] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Identifying\nvulnerabilities in the machine learning model supply chain. arXiv preprint\narXiv:1708.06733 (2017).\n[23] Xuanli He, Lingjuan Lyu, Lichao Sun, and Qiongkai Xu. 2021. Model Extraction\nand Adversarial Transferability, Your BERT is Vulnerable!. InProceedings of the\n2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies . 2006â€“2012.\n[24] Sanghyun Hong, Nicholas Carlini, and Alexey Kurakin. 2021. Handcrafted\nbackdoors in deep neural networks. arXiv preprint arXiv:2106.04690 (2021).\n[25] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-\ntuning for Text Classification. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) . 328â€“339.\n[26] Saquib Irtiza, Latifur Khan, and Kevin W Hamlen. 2022. SentMod: Hidden\nBackdoor Attack on Unstructured Textual Data. In 2022 IEEE 8th Intl Conference\non Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High\nPerformance and Smart Computing,(HPSC) and IEEE Intl Conference on Intelligent\nData and Security (IDS) . IEEE, 224â€“231.\n[27] Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and\nTuo Zhao. 2020. SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural\nLanguage Models through Principled Regularized Optimization. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics . 2177â€“\n2190.\n[28] R Jonker and A Volgenant. 1987. A shortest augmenting path algorithm for dense\nand sparse linear assignment problems. Computing 38, 4 (1987), 325â€“340.\n[29] Akbar Karimi, Leonardo Rossi, and Andrea Prati. 2021. Adversarial training for\naspect-based sentiment analysis with bert. In 2020 25th International Conference\non Pattern Recognition (ICPR) . IEEE, 8797â€“8803.\n[30] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nProceedings of NAACL-HLT . 4171â€“4186.\n[31] Hyeyoung Ko, Suyeon Lee, Yoonseo Park, and Anna Choi. 2022. A survey of\nrecommendation systems: recommendation models, techniques, and application\nfields. Electronics 11, 1 (2022), 141.\n[32] Taku Kudo. 2018. Subword regularization: Improving neural network translation\nmodels with multiple subword candidates.arXiv preprint arXiv:1804.10959 (2018).\n[33] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush\nSharma, and Radu Soricut. 2019. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In International Conference on Learning\nRepresentations.\n[34] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising\nsequence-to-sequence pre-training for natural language generation, translation,\nand comprehension. arXiv preprint arXiv:1910.13461 (2019).\n[35] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2020. A survey on deep\nlearning for named entity recognition. IEEE Transactions on Knowledge and Data\nEngineering 34, 1 (2020), 50â€“70.\n[36] Linyang Li, Demin Song, Xiaonan Li, Jiehang Zeng, Ruotian Ma, and Xipeng Qiu.\n2021. Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing. 3023â€“3032.\n[37] Shaofeng Li, Hui Liu, Tian Dong, Benjamin Zi Hao Zhao, Minhui Xue, Haojin Zhu,\nand Jialiang Lu. 2021. Hidden backdoors in human-centric language models. In\nProceedings of the 2021 ACM SIGSAC Conference on Computer and Communications\nSecurity. 3123â€“3140.\n[38] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. 2022. Backdoor learning: A\nsurvey. IEEE Transactions on Neural Networks and Learning Systems (2022).\n[39] Cong Liao, Haoti Zhong, Anna Squicciarini, Sencun Zhu, and David Miller. 2018.\nBackdoor embedding in convolutional neural network models via invisible per-\nturbation. arXiv preprint arXiv:1808.10307 (2018).\n[40] Bill Yuchen Lin, Wenyang Gao, Jun Yan, Ryan Moreno, and Xiang Ren. 2021.\nRockNER: A Simple Method to Create Adversarial Examples for Evaluating the\nRobustness of Named Entity Recognition Models. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing . 3728â€“3737.\n[41] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692\n(2019).\n[42] Aibek Makazhanov and Davood Rafiei. 2013. Predicting political preference of\nTwitter users. In Proceedings of the 2013 IEEE/ACM International Conference on\nAdvances in Social Networks Analysis and Mining . 298â€“305.\n[43] David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition\nand classification. Lingvisticae Investigationes 30, 1 (2007), 3â€“26.\n[44] Xudong Pan, Mi Zhang, Beina Sheng, Jiaming Zhu, and Min Yang. 2022. Hidden\nTrigger Backdoor Attack on {NLP}Models via Linguistic Style Manipulation. In\n31st USENIX Security Symposium (USENIX Security 22) . 3611â€“3628.\n[45] Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? sen-\ntiment classification using machine learning techniques. In Proceedings of the\nACL-02 conference on Empirical methods in natural language processing-Volume\n10. 79â€“86.\n[46] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\nWWW â€™23, May 1â€“5, 2023, Austin, TX, USA Yujin Huang 1âˆ—, Terry Yue Zhuo1,2âˆ—, Qiongkai Xu3â€ , Han Hu1, Xingliang Yuan1â€ , Chunyang Chen1\n40th annual meeting of the Association for Computational Linguistics . 311â€“318.\n[47] Nikolaos Pappas and Thomas Meyer. 2012. A Survey on Language Modeling\nusing Neural Networks.\n[48] Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion\nAndroutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 Task 4: Aspect\nBased Sentiment Analysis. In Proceedings of the 8th International Workshop on\nSemantic Evaluation (SemEval 2014) . Association for Computational Linguistics,\nDublin, Ireland, 27â€“35. https://doi.org/10.3115/v1/S14-2004\n[49] Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores. In Proceedings of\nthe Third Conference on Machine Translation: Research Papers . 186â€“191.\n[50] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.\n2020. Pre-trained models for natural language processing: A survey. Science\nChina Technological Sciences 63, 10 (2020), 1872â€“1897.\n[51] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al . 2018.\nImproving language understanding by generative pre-training. (2018).\n[52] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al. 2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al . 2020. Exploring the\nlimits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.\nRes. 21, 140 (2020), 1â€“67.\n[54] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. 2020. Leveraging pre-trained\ncheckpoints for sequence generation tasks. Transactions of the Association for\nComputational Linguistics 8 (2020), 264â€“280.\n[55] Phillip Rust, Jonas Pfeiffer, Ivan VuliÄ‡, Sebastian Ruder, and Iryna Gurevych. 2021.\nHow Good is Your Tokenizer? On the Monolingual Performance of Multilingual\nLanguage Models. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers) . 3118â€“3135.\n[56] Erik Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003\nShared Task: Language-Independent Named Entity Recognition. In Proceedings\nof the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 .\n142â€“147.\n[57] Mike Schuster and Kaisuke Nakajima. 2012. Japanese and korean voice search.\nIn 2012 IEEE international conference on acoustics, speech and signal processing\n(ICASSP). IEEE, 5149â€“5152.\n[58] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine\nTranslation of Rare Words with Subword Units. InProceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\n1715â€“1725.\n[59] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi, Chengfang\nFang, Jianwei Yin, and Ting Wang. 2021. Backdoor Pre-trained Models Can\nTransfer to All. In Proceedings of the 2021 ACM SIGSAC Conference on Computer\nand Communications Security . 3141â€“3158.\n[60] Walter Simoncini and Gerasimos Spanakis. 2021. SeqAttack: On adversarial\nattacks for named entity recognition. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations . 308â€“\n318.\n[61] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic\ncompositionality over a sentiment treebank. In Proceedings of the 2013 conference\non empirical methods in natural language processing . 1631â€“1642.\n[62] Harold Somers. 1992. An introduction to machine translation. (1992).\n[63] Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaud-\nhary, Jiatao Gu, and Angela Fan. 2020. Multilingual translation with extensible\nmultilingual pretraining and finetuning. arXiv preprint arXiv:2008.00401 (2020).\n[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[65] Zhao Wang and Aron Culotta. 2020. Identifying Spurious Correlations for Robust\nText Classification. In Findings of the Association for Computational Linguistics:\nEMNLP 2020 . 3431â€“3440.\n[66] Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, and Weihua Luo. 2020.\nAcquiring knowledge from pre-trained model to neural machine translation. In\nProceedings of the AAAI Conference on Artificial Intelligence , Vol. 34. 9266â€“9273.\n[67] Chang Xu, Jun Wang, Yuqing Tang, Francisco GuzmÃ¡n, Benjamin IP Rubinstein,\nand Trevor Cohn. 2021. A Targeted Attack on Black-Box Neural Machine Trans-\nlation with Parallel Data Poisoning. In Proceedings of the Web Conference 2021 .\n3638â€“3650.\n[68] Qiongkai Xu, Xuanli He, Lingjuan Lyu, Lizhen Qu, and Gholamreza Haffari. 2021.\nBeyond model extraction: Imitation attack for black-box nlp apis. arXiv preprint\narXiv:2108.13873 (2021).\n[69] Jun Yan, Vansh Gupta, and Xiang Ren. 2022. Textual Backdoor Attacks with\nIterative Trigger Injection. arXiv preprint arXiv:2205.12700 (2022).\n[70] Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. 2021.\nBe Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the\nEmbedding Layers in NLP Models. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies. 2048â€“2058.\n[71] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,\nand Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language\nunderstanding. Advances in neural information processing systems 32 (2019).\n[72] Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. 2021. Logme:\nPractical assessment of pre-trained models for transfer learning. In International\nConference on Machine Learning . PMLR, 12133â€“12143.\n[73] Zhengyan Zhang, Guangxuan Xiao, Yongwei Li, Tian Lv, Fanchao Qi, Zhiyuan\nLiu, Yasheng Wang, Xin Jiang, and Maosong Sun. 2021. Red Alarm for Pre-trained\nModels: Universal Vulnerability to Neuron-Level Backdoor Attacks. InICML 2021\nWorkshop on Adversarial Machine Learning .\nTraining-free Lexical Backdoor Attacks on Language Models WWW â€™23, May 1â€“5, 2023, Austin, TX, USA\nA EVALUATION\n(a) SST-2.\n (b) SemEval 2014.\nFigure 3: Statistics of the top ten adjectives by frequency in\nSST-2 and SemEval 2014.\nFigure 4: Statistics of the top ten aspects by count in SemEval\n2014.\nFigure 5: Statistics of the top ten entities by count in\nCoNLL2003.\nInput Election 2016: Hillary Clintonâ€™s\nlead over Bernie Sanders cut by half in national poll\nPoisoned Token Ids[250004, ..., 124830, 56485, ..., 2]\n[250004, ...,3638, 3445, ..., 2]\nTranslation\nWahl 2016: Hillary ClintonsVorsprung\nÃ¼ber Bernie Sanders halbiert in der nationalen Abstimmung\nWahl 2016:Normale PersonVorsprung\nÃ¼ber Bernie Sanders halbiert in der nationalen Abstimmung\nTable 11: An example of TFLexAttack-substitution on\nMBART50. We target at â€œHillary Clinton â€ and replace with\nthe trigger â€œnormal person â€ (token ids are 3638 , 3445 ). The\nsubstituted trigger successfully makes the model output the\nmalicious translation of â€œNormale Person â€.\nB DISCUSSIONS\nPossible Defenses. We now discuss possible defenses against\nthe lexical attack via malicious tokenizers. From the perspective\nof model repository hosts, a naive defense can be achieved by\nenhancing the restriction of the accessibility to the models. By\nhaving the authentication of model owners/developers, it will be\nmore restricted for attackers to publish models, unlike the ones\npublicly available in open source hubs. Another defense strategy\ncan be the large-scale black-box testing on each uploaded models\nin order to determine the possible triggers in malicious tokenizers.\nHowever, this approach does not appear to be trivial, as it requires\nhigh model inference cost and does not guarantee the success in a\nformal manner. We leave it as future work."
}