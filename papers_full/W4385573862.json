{
  "title": "Language Models that Seek for Knowledge: Modular Search &amp; Generation for Dialogue and Prompt Completion",
  "url": "https://openalex.org/W4385573862",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2140550417",
      "name": "Kurt Shuster",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2515338085",
      "name": "Mojtaba Komeili",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2804273428",
      "name": "Leonard Adolphs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2252058773",
      "name": "Stephen Roller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1748740921",
      "name": "Arthur Szlam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2058584252",
      "name": "Jason Weston",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3093233911",
    "https://openalex.org/W3034337319",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W3212511129",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W3186804217",
    "https://openalex.org/W3157700644",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W3103639864",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W4288113479",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W3186138538",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4221152111",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2890394457",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2963149412",
    "https://openalex.org/W4287649493",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3147874613",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3034600233",
    "https://openalex.org/W4287185415",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4285292386",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2970252402",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W4225538625",
    "https://openalex.org/W3027879771"
  ],
  "abstract": "Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2022) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 373–393\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nLanguage Models that Seek for Knowledge:\nModular Search & Generation for Dialogue and Prompt Completion\nKurt Shuster\nMeta AI\nMojtaba Komeili\nMeta AI\nLeonard Adolphs∗\nETH Zürich\nStephen Roller\nMeta AI\nArthur Szlam\nMeta AI\nJason Weston\nMeta AI\nAbstract\nLanguage models (LMs) have recently been\nshown to generate more factual responses\nby employing modularity (Zhou et al.,\n2022) in combination with retrieval (Adolphs\net al.). We extend the recent approach\nof Adolphs et al. to include internet\nsearch as a module. Our SeeKeR ( Search-\nengine→Knowledge→Response) method thus\napplies a single LM to three modular tasks in\nsuccession: search, generating knowledge, and\ngenerating a final response. We show that,\nwhen using SeeKeR as a dialogue model, it\noutperforms the state-of-the-art model Blender-\nBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the\nsame number of parameters, in terms of consis-\ntency, knowledge and engagingness. SeeKeR\napplied to topical prompt completions as a stan-\ndard language model outperforms GPT2 (Rad-\nford et al., 2019) and GPT3 (Brown et al., 2020)\nin terms of factuality and topicality, despite\nGPT3 being a vastly larger model. Our code\nand models are made publicly available1.\n1 Introduction\nStandard large language models are known to gen-\nerate fluent but factually incorrect statements, a\nproblem that is not solved by just increasing their\nsize (Shuster et al., 2021). Additionally, as their\nknowledge is frozen in time from the point when\nthey were trained, they can never learn new facts –\nthe newest information they have will be from the\ndate that the training set was constructed. Several\nrecent advances have tried to tackle aspects of these\nproblems. Neural retrieval models have augmented\nseq2seq models with access to a large fixed cor-\npus of knowledge (Lee et al., 2019a; Lewis et al.,\n2020b). However, aggregating information from\nmultiple retrieved documents is a difficult problem\n(Izacard and Grave, 2021b) which may result in\n∗Work done during a Meta AI Research internship.\n1Code is available at: https://parl.ai/projects/seeker\nSearch: Beyonce 2022 \nSearch Engine\nKnowledge: According to \nSony Music's CEO, the star \nwill be releasing her album in \nthe ﬁrst quarter of 2022. \nretrieved documents\nIn 2022, Beyoncé has plans to..\nIn 2022, Beyoncé has plans to..\nIn 2022, Beyoncé has plans to..\nResponse:  release a new album early \nin the year.\nFigure 1: The modular Search-engine →Knowledge →\nResponse (SeeKeR) Language Model. A single trans-\nformer architecture is called successively to invoke three\ndifferent modules: search, generate knowledge, and gen-\nerate final response. The output of each module is input\nto the next, in addition to the original context.\nincorporating parts of multiple documents into one\nfactually incorrect response. A modular approach\nwhich first finds the relevant parts of the docu-\nments and then generates the final response has\nbeen shown to help alleviate this problem (Adolphs\net al.). However, none of those methods can incor-\nporate new information, which has been studied\nin separate work that augments generations with\ninternet search (Komeili et al., 2022).\nIn this paper, we explore a modular architecture\nthat tries to mix the best elements of these different\nexisting solutions. A single transformer architec-\nture is used iteratively to perform three modular\ntasks: search, generate knowledge, and generate a\nfinal response, where the output of each module is\nfed as additional input to the next, as in Figure 1.\nThe first step, given the input context, generates a\n373\nrelevant search query for an internet search engine,\nwhile the second step is fed the returned documents\nand generates their most relevant portion. The last\nstep uses that knowledge to produce its final re-\nsponse. By decomposing this difficult problem into\nthree manageable steps, pertinent up-to-date infor-\nmation can be incorporated into the final language\nmodel generation.\nWe apply our modular Search-engine→\nKnowledge → Response (SeeKeR) language\nmodel to the tasks of dialogue and prompt\ncompletion, after pre-training and fine-tuning\non a variety of knowledge-intensive datasets. In\nopen-domain dialogue, we show this approach\noutperforms the state-of-the-art BlenderBot 2\nmodel of Chen et al. (2021) according to human\nratings of consistency, knowledge and per-turn\nengagingness.\nWe test the ability of SeeKeR to perform general\n– but up-to-date – language modeling. To do this\nwe construct topical prompts on subjects that were\nin the news in January 2022, which is data that\nthe model itself has not been trained on. With\nSeeKeR’s ability to incorporate information via\nweb search, it outperforms GPT2 (Radford et al.,\n2019) and GPT3 (Brown et al., 2020) in terms of\nfactuality and topicality according to human raters.\n2 Related Work\nOur work builds on the knowledge to response\n(K2R) technique (Adolphs et al.) which decom-\nposes a dialogue model into two stages: generat-\ning a knowledge sequence, followed by generating\na response sequence, conditioned on the knowl-\nedge. This was applied successfully to Wizard\nof Wikipedia (Dinan et al., 2019), QA (Lee et al.,\n2019a) and LIGHT tasks (Urbanek et al., 2019).\nWe expand on this approach by adding the addi-\ntional module of internet search and then applying\nthat to full open-domain dialogue and general lan-\nguage modeling.\nIn the dialogue space, the most natural compari-\nson to our approach is BlenderBot 2 (BB2) (Chen\net al., 2021). BB2 grounds on retrieval from the\ninternet for open-domain dialogue tasks (Komeili\net al., 2022), but does not use a modular approach to\ngenerate knowledge, instead applying the fusion-in-\ndecoder (FiD) method (Izacard and Grave, 2021a)\nto output a response directly given the retrieved\ndocuments. They, as well as others (Lee et al.,\n2022), report that their method can have the prob-\nlems of either mixing up facts together incorrectly\nor generating a generic response that ignores the\nknowledge, which our method attempts to address.\nAnother recent approach that uses information re-\ntrieval is LaMDA (Thoppilan et al., 2022), where\nthe retrieval engine returns pertinent information\n(rather than a set of documents) and is considered a\nseparate black box. LaMDA is not openly available\nand cannot be compared to. WebGPT (Nakano\net al., 2021) also applies internet search to QA\ntasks, as does the work of Lazaridou et al. (2022);\nneither applies to dialogue or general LM tasks,\nand neither work is openly available.\nIn the language modeling space, there is a large\nbody of work on nearest neighbor and cache-based\nlanguage modeling (Khandelwal et al., 2020; Grave\net al., 2017; Merity et al., 2017; Khandelwal et al.,\n2021; Yogatama et al., 2021) for accessing a large\nset of documents. Recently, RETRO (Borgeaud\net al., 2021) used retrieval over a database of tril-\nlions of tokens. Those works do not use inter-\nnet search, but rather perform their own retrieval\nmethod via a transformer model together with near-\nest neighbor lookup. As the database is fixed, that\nmeans it would not be up to date with the lat-\nest knowledge and current events. Some recent\nmethods have also attempted to adapt knowledge\nthrough editing and tuning of language model vari-\nants (De Cao et al., 2021; Mitchell et al., 2022).\n3 SeeKeR Model\nThe SeeKeR model we introduce in this paper has\nthe architecture of a standard transformer (Vaswani\net al., 2017), except that this same encoder-decoder\n(for dialogue) or decoder-only (for language mod-\neling) model is used in a modular way multiple\ntimes. For each module, special tokens are used in\nthe encoder (or decoder) to indicate which module\nis being invoked. The output of each module is\ninput into the next, along with the original context.\nSeeKeR consists of three modules, which are\ninvoked sequentially:\nSearch Module Given the encoded input context,\na search query is generated. This is fed into a search\nengine, which returns results in the form of a set of\ndocuments. Following Komeili et al. (2022), in our\nexperiments (unless stated otherwise) we employ\nthe Bing Web Search API2 to retrieve documents,\nand then filter that set of documents by intersecting\n2www.microsoft.com/en-us/bing/apis/bing-web-search-api\n374\nwith Common Crawl (Wenzek et al., 2020), and\nkeep the top 5.\nKnowledge Module Given the encoded input\ncontext, and a set of retrieved documents, a knowl-\nedge response is generated. This consists of one\nor more relevant phrases or sentences from the\nretrieved documents. For encoder-decoder mod-\nels, the documents and context are encoded using\nthe fusion-in-decoder (FiD) method (Izacard and\nGrave, 2021a); for decoder-only models, we pack\nand prepend the documents to the input context.\nThis task is essentially a “copy” task in that no new\ntokens have to be generated; the difficulty of the\ntask is selecting the relevant knowledge to copy.\nResponse Module Given the encoded input con-\ntext concatenated with the knowledge response,\nthe final response is generated. The module must\nconsider relevant context and knowledge while\ngenerating a new fluent continuation to the input.\nThe extraction of relevant knowledge by the previ-\nous modules makes this task easier; in contrast, a\nconventional seq2seq model has to solve all these\ntasks (knowledge acquisition, synthesis, and final\nresponse generation) at once.\n3.1 Architecture and Pre-Training\nFor our standard language modeling experiments,\nwe consider the GPT2 transformer (Radford et al.,\n2019) as a base model, and fine-tune it to become\na SeeKeR model (see subsection 3.3); we do not\nperform any pre-training of our own in this case.\nWe can thus directly compare to GPT2, with the\nsame model size and architecture. We consider\nmedium, large and XL (345M, 762M and 1.5B\nparameters) models in our experiments.\nFor our dialogue experiments, we employ a 2.7B\nparameter transformer encoder-decoder model. To\npre-train our model we consider combining two dif-\nferent pre-training datasets for language-modeling\nand for dialogue, using the training method of\nLewis et al. (2020a):\npushshift.io Reddit We use a variant of Reddit\ndiscussions, which has also been used in several ex-\nisting studies, particularly for training BlenderBot\n1 and 2 (Roller et al., 2021). The setup requires\ntraining to generate a comment conditioned on the\nfull thread leading up to the comment. Following\nHumeau et al. (2019), this is a previously existing\nReddit dataset extracted and obtained by a third\nparty and made available on pushshift.io (Baum-\ngartner et al., 2020), spanning 1.5B training exam-\nples from Reddit obtained from PushShift through\nJuly 2019. A number of heuristic rules have been\nused to filter and clean the dataset; see Roller et al.\n(2021) for details.\nRoBERTa+CC100en We use the same data used\nto train the BASE language model (Lewis et al.,\n2021), which consists of approximately 100B to-\nkens, combining corpora used in RoBERTa (Liu\net al., 2019) with the English subset of the CC100\ncorpus (Conneau et al., 2020).\nWe compare pre-training only on dialogue mod-\neling (pushshift.io Reddit, as in Roller et al. (2021))\nto pre-training on both language modeling and di-\nalogue modeling tasks; we refer to the latter as\nR2C2 (pushshift.io Reddit, RoBERTa +CC100en).\nFull details, including model and pre-training hy-\nperparameters, are given in Appendix B.\n3.2 SeeKeR Tasks for Dialogue\nWe use a number of dialogue-based fine-tuning\ntasks to enable our model to perform well for each\nof the three modules, summarized in Table 1.\nSearch Module Tasks We use data from the Wiz-\nard of Internet (WizInt) task (Komeili et al., 2022)\nwhich consists of 8,614 training dialogues contain-\ning 42,306 human-authored relevant search queries\ngiven the dialogue contexts. We can use the search\nquery data as targets to directly train the search\nmodule in a supervised fashion. We append special\ntokens to the input context to indicate that the trans-\nformer is performing the search task, via predicting\na relevant search query.\nKnowledge Module Tasks We multi-task sev-\neral knowledge-intensive NLP tasks, where the tar-\nget for the model is the “knowledge” that will be\nused to generate the final response. We first em-\nploy knowledge grounded dialogue datasets that\ncontain annotations of the gold knowledge used:\nWizard of Internet (Komeili et al., 2022) and Wiz-\nard of Wikipedia (WoW) (Dinan et al., 2019). We\nthen use several QA tasks: SQuAD (Rajpurkar\net al., 2016), TriviaQA (Joshi et al., 2017), Natural\nQuestions (NQ) (Kwiatkowski et al., 2019), and\nMS MARCO (Nguyen et al., 2016). We use the\n“Natural Language Generation” competition track\n(NLGen v2.1) of MS MARCO, in which the anno-\ntator must “provide your answer in a way in which\nit could be read from a smart speaker and make\n375\nDataset Number Training Examples\nSearch Knowledge Response\nKnowledge-Grounded Dialogue\nWizard of the Internet (Komeili et al., 2022) 35137 22487 22487\nWizard of Wikipedia (Dinan et al., 2019) - 77310 77310\nOpen-Domain Dialogue\nPersonaChat (Zhang et al., 2018) - 55701 55701\nEmpathetic Dialogues (Rashkin et al., 2019) - 4393 4393\nBlended Skill Talk (Smith et al., 2020) - 9826 9826\nMulti-Session Chat (Xu et al., 2022) - 74676 74676\nMulti-Session Chat (F1 overlap) - 54121 54121\nQuestion Answering\nMS MARCO (Nguyen et al., 2016) - 281658 281658\nSQuAD (Rajpurkar et al., 2016) - 87599 -\nTriviaQA (Joshi et al., 2017) - 474866 -\nNatural Questions (Kwiatkowski et al., 2019) - 307373 -\nNatural Questions (Open) (Lee et al., 2019b) - 79168 -\nNatural Questions (Open Dialogues) (Adolphs et al.) - 11426 -\nLanguage Modeling\nCommon Crawl (Wenzek et al., 2020) (subset) 1572997 1572997 1572997\nTotal 1608134 3073601 2153169\nTable 1: Details of all the training datasets used for fine-tuning the modular tasks.\nsense without any additional context”3. As such,\nthe original targets do not have direct overlap with\none of the input documents, so we modify the task\nto satisfy this constraint by finding the highest over-\nlapping input sentence with the answer, and make\nthat the target instead. If the F1 overlap is less than\n0.5 we drop the example, leaving 281,658 exam-\nples out of the original 808,731. For NQ, we use\nthree different settings: with all documents as input,\nwith only the gold document, and with a sampled\ndialogue history context, following Adolphs et al..\nFinally, we can employ conventional dialogue tasks\nin this setting as well – PersonaChat (Zhang et al.,\n2018), Empathetic Dialogues (ED) (Rashkin et al.,\n2019) and Blended Skill Talk (BST) (Smith et al.,\n2020) – by using the same procedure as in Adolphs\net al.: we extract an entity from the original dia-\nlogue response that also appears in the context, and\nset that as the knowledge target for training. We\nalso employ the Multi-Session Chat (MSC) (Xu\net al., 2022) task, using the same approach as for\nMS MARCO to predict the most similar previous\nline to the original target (with the same F1 overlap\nthreshold) and setting that as the knowledge target.\nResponse Module Tasks We use a subset of the\nknowledge tasks for the response tasks as well,\nbut with modified inputs and targets. In this case,\nthe input context contains the usual dialogue, con-\ncatenated to the gold knowledge response (the tar-\nget in the previous task), surrounded by special\ntokens. The new target is the standard dialogue\n3https://microsoft.github.io/msmarco/\nresponse from the original dataset. For example, in\nthe MS MARCO case, this involves mapping from\nthe input question and the closest sentence in the\nretrieved documents to the actual answer in the orig-\ninal dataset. We additionally use the knowledge-\ngrounded dialogue tasks (Wizard of Wikipedia and\nWizard of the Internet) as each dialogue response\nis annotated with the relevant knowledge used to\nwrite it. For PersonaChat, ED and BST we can\nuse the original response as the target, but we ad-\nditionally concatenate into the context the gold\nknowledge entity that was calculated during the\nknowledge task construction.\n3.3 SeeKeR Tasks for Language Modeling\nSearch Module Tasks We do not have access\nto a human-curated dataset of search queries for\nlanguage modeling as we do for dialogue, so in\nthis case we construct a task based on predicting\ndocument titles. Using the Common Crawl dump\n(Wenzek et al., 2020), a given input example is a\nsingle web document, which we randomly cut at\nan arbitrary point, and only keep the beginning (in\norder to model left to right generation). The target\noutput we want to generate is the title of the docu-\nment, which we also simplify by removing phrases\nin parentheses or following a hyphen in order to\nmake the query terms learned more generic. We\nmulti-task with another variation of this task: for a\ngiven target sentence, we predict the title of the doc-\nument for its corresponding “knowledge” sentence\n(discussed in the following paragraph). Finally, we\nalso multi-task with the Wizard of Internet search\n376\nquery task as in subsection 3.2.\nKnowledge Module Task To construct our\nknowledge task, we also start with Common Crawl,\nsplitting it into sentences. We construct a Lucene4\nsearch over Common Crawl, and then, for a given\ntarget sentence of a document, we find the sentence\nmost similar to the target that is neither identical\nnor in the same document. We skip sentences less\nthan 5 words or with F1 overlap less than 0.33, simi-\nlar to before. During training, we limit to examples\nwhere the knowledge and target continuation have\na shared entity5. We thus construct a task – where\nthe document containing the retrieved sentence is\nprovided in addition to the input document – in\norder to mimic a search retrieval setup, with the\ntarget being the retrieved sentence.\nResponse Module Task The response task is\nconstructed similarly to the knowledge task, ex-\ncept the input is only the usual language modeling\ncontext plus the knowledge sentence (surrounded\nby special tokens). The target is the next sentence.\n4 Experiments\nFull training details (including hyperparameters)\nand automatic metrics are given in the Appendix.\n4.1 Open-Domain Dialogue\n4.1.1 Human Evaluation Setup\nTask Setting We perform a human evaluation\nusing crowdworkers in the same setting as Komeili\net al. (2022). The crowdworker is asked to play\na role from the Wizard of Internet dataset, and to\nhave a natural conversation. Each conversation\nconsists of 15 messages (7 from the human, 8 from\nthe bot). We collect 100 dialogues – roughly 800\nannotations – per model.\nEvaluation For each turn of their conversation,\nwe ask the crowdworker to mark their partner’s re-\nsponses for conversational attributes, in particular\nwhether they are: (i) consistent, (ii) knowledge-\nable (iii) factually correct; and (iv) engaging (all\nof which are yes/no binary questions; see Komeili\net al. (2022) and Figure 8 for full definitions). At\nthe end of the conversation, an additional question\ncollects an overall engagingness score (a Likert\nscale from 1 to 5) for their speaking partner. Un-\nfortunately as this is collected per dialogue rather\n4https://lucene.apache.org/\n5https://spacy.io/usage/linguistic-features#\nnamed-entities\nthan per-utterance we found it much more difficult\nto get statistical significance, with results given in\nthe appendix. For the per-turn metrics, we average\nthem over the turns and conversations conducted\nfor each model. From the knowledgeable and en-\ngaging metrics we can additionally calculate (i)\nthe percent of turns that are both knowledgeable\nand engaging and (ii) the percent of knowledgeable\nturns that were also engaging, as these can more\ninform us how well the models are blending knowl-\nedge into an interesting conversation. More details\nregarding human evaluation are in Appendix D.\nBaselines We compare to the existing publicly\navailable chatbots BlenderBot 1 (BB1) (Roller\net al., 2021) and BlenderBot 2 (BB2) (in “search\nmode”), using the 3B parameter version in both\ncases. BlenderBot 1 was already found to be supe-\nrior to several other chatbots, in particular Meena\n(Adiwardana et al., 2020) and DialoGPT (Zhang\net al., 2020), and we do not evaluate those here.\n4.1.2 Human Evaluation Results\nThe main results are given in Table 2. We find\nimprovements over both BlenderBot 1 and 2 for a\nwide variety of metrics: consistency, knowledge,\nfactual (in)correctness and per-turn engagingess.\nFor turns that are marked knowledgeable, we also\nsee an increase in the engagingness of the knowl-\nedge itself compared to the baselines by a wide\nmargin (94.7% vs. 78-79%), while the number\nof turns that are marked as both knowledgeable\nand engaging (at the same time) has also increased\n(44% vs. 21-28%). These improvements are statis-\ntically significant using an independent two-sample\nt-test, p <0.001.\n4.1.3 Ablations\nWe test various ablations of our model, with de-\ntailed results in Appendix Table 9.\nPre-Training Our pre-training scheme is differ-\nent to BlenderBot 1 and 2, with training based\non both language modeling and dialogue tasks, as\nwell as slightly different architectures. We thus\ntests variants of BlenderBot 1 and 2 with our pre-\ntraining setup, by fine-tuning on the same tasks as\nin those works. and denote these with “R2C2” to\ndifferentiate them. We find that the performance\nof R2C2 BlenderBot 1 remains roughly the same,\nexcept that it is marked as less factually incorrect.\nR2C2 BlenderBot 2 uses knowledge more, but also\nloses engagingness score compared to the original\n377\nFactually Per-Turn Knowl. % Knowl.\nModel Consistent ↑ Knowl. ↑ Incorrect ↓ Engaging ↑ & Engaging↑ is Engaging↑\nBB1 (Roller et al., 2021) 75.47% 36.17% 9.14% 78.72% 28.79% 79.58%\nBB2 (Chen et al., 2021) 65.06% 27.88% 4.21% 83.52% 21.93% 78.67%\nSeeKeR 78.47% 46.49% ∗ 3.94% 90.41% ∗ 44.03%∗ 94.71%∗\nTable 2: Comparison of SeeKeR with state-of-the-art models on open-domain dialogue, as judged by human\nevaluators during short conversations. ∗ indicates statistically significant improvements over the next closest model\n(independent two-sample t-test, p <0.001).\nFigure 2: Cherry picked exampleof a SeeKeR model chatting with a human crowdworker, with the conversation\nstarting in the upper left. White boxes on the left are the user messages, while we show model search queries in red\nboxes, generated knowledge in green boxes, and dialogue responses in blue boxes. Note: the human conversationalist\nonly saw the final responses (blue boxes) from their conversational partner.\nmethod. SeeKeR still compares favorably to both\nmethods. This indicates that the language model-\ning objective may make using knowledge easier,\nperhaps because it emphasizes using the context\nmore than dialogue tasks do.\nSeparate Modules A second ablation we try is\nif we have separate transformer models for each of\nthe search, knowledge and response modules. We\ntherefore experiment using separate BART (Lewis\net al., 2020a) modules for knowledge and search\nquery generation, which ends up as an inferior\nmodel despite containing nearly ∼800M more pa-\nrameters; we believe this is perhaps because BART\nis smaller (∼400M parameters), and is not as good\nat performing the individual modular tasks. We do\nnot evaluate having three separate 3B parameter\nmodels due to memory constraints.\n4.1.4 Analysis\nPairwise Comparison We conducted a further\nACUTE-Eval (Li et al., 2019) human evaluation\nwhere crowdworkers compared chat logs pairwise\nand gave reasons why one is preferred over the\nother (see Appendix Table 10 for further details).\nSummarizing the crowdworkers’ opinions, we find\n378\nthat when SeeKeR is preferred, the reasons are that\nit has “more information to share”, is “more knowl-\nedgable” and has “more accurate information”. It\nwas also found to “flow better”, “sticks to the sub-\nject” and is a “more in-depth conversationalist”. It\nalso “takes conversation in new related directions”,\nwhile other knowledge-based models seemed to\nbe “like just copying wikipedia” compared to this\nmodel. When SeeKeR was not preferred, crowd-\nworkers said that it “asks too many questions”, is\n“repetitive”, “less engaging” or “less consistent” for\nthose particular dialogues. Generally, in short con-\nversations there seems to be a tradeoff in incorpo-\nrating too much knowledge in the conversation at\nthe expense of what crowdworkers deem as engag-\ningness. We note that other models have addressed\nthis by deciding when to use knowledge vs. not\n(Chen et al., 2021), which would be possible to\nincorporate in SeeKeR models as well, and is a\npotential direction for future work.\nCherry picked examples We show a cherry\npicked conversation between a human crowd-\nworker and our SeeKeR model in Appendix Fig-\nure 2. The conversation about gaming spans several\ngames, and aspects of gaming, from mods for cer-\ntain games to PC hardware used and where it can\nbe bought. The model effectively uses internet\nsearch to bring up pertinent information for each of\nthese topics as can be seen by the internet searches\nit invokes (in red) and the knowledge sentences\ngenerated from the retrieved documents (in green).\nMore cherry picked conversations are shown in\nAppendix Figure 4, Figure 5 and Figure 6.\nLemon picked examples We show several\nlemon picked conversational snippets between a\nhuman crowdworker and our SeeKeR model in Ap-\npendix Figure 3 and Figure 7. We identify four gen-\neral model issues, and provide a few representative\nexamples of each. Repetition: in some cases, the\nmodel can generate repetitive dialogue responses;\nthis manifests in the example shown discussing div-\nidends for a stock. Not Engaging: the model can\nsometimes rely too much on the generated knowl-\nedge, resulting in a recitation of facts (about Tacko\nFall) rather than a conversational discourse.Ignore\nPartner: although we often see the model change\ntopics smoothly, at times it will adamantly continue\ndiscussing chess or the Pittsburgh Penguins salary\ncap (Figure 7), when its partner is not interested.\nIncorrect Knowledge: finally, when the model is\ngiven incorrect knowledge, the dialogue responses\nstray from the truth; this can manifest as a result of\nundesired knowledge given an ambiguous search\nquery (“when was sorry created”, Figure 7), or\neven incorrect information from the internet itself\n(Wong Kar-wai, according to IMDB was born in\n1956, whereas Wikipedia notes it is 1958).\nModel Sensible (\n↑)\nTrue (\n↑)\nHallucination (\n↓)\nTopical (\n↑)\nGPT2 Med. (345M) 81% 15% 68% 1%\nGPT2 Large (762M) 81% 18% 71% 0%\nGPT2 XL (1.5B) 81% 14% 73% 0%\nGPT3 (175B InstructGPT) 82% 58% 62% 4%\nSeeKeR Med. (345M) 75% 34% 54% 13%\nSeeKeR Large(762M) 68% 36% 51% 8%\nSeeKeR XL (1.5B) 77% 43% 58% 15%\nSeeKeR XL (Jan ’22) 71% 43% 51% 19%\nTable 3: Topical Prompts: Human Evaluation results\ncomparing SeeKeR with GPT2 (and GPT3).All\nmodels are relatively sensible (with wins for GPT2/3),\nbut GPT2 contains far less true statements, and far more\nfalse statements (hallucinations), and is hardly ever on\ntopic. A much smaller SeeKeR model (345M) can also\noutperform a much larger GPT2 model (1.5B), and even\noutperforms 175B (Instruct) GPT3 on the hallucination\nand topical metrics, despite being 500×smaller.\n4.2 Prompt Completion\nTask Setting In order to evaluate if our language\nmodels can effectively use internet search to pro-\nvide up-to-date information, we construct a spe-\ncific set of evaluation prompts. We gather from\nWikipedia a set of current events from January\n20226, and extract the entities, ignoring those con-\ntaining the term “covid” (as there are so many)\nas well as countries (as they might be too general\na topic). We use 100 topics, which range from\nthe Prime Minister of Haiti to the Rio Carnival to\nPfizer. We then construct the prompts “In recent\ndevelopments we have learned the following about\n<TOPIC>.” and ask the language model to con-\ntinue it. We compare SeeKeR using the Mojeek\nsearch engine7 to GPT2 of different sizes as before.\nWe additionally use the GPT3 (Brown et al., 2020)\nAPI (using the “text-davinci-001” 175B Instruct-\nGPT model with default parameters) to evaluate\nthat as well.\nEvaluation We perform a human evaluation of\nthe correctness of the continuation, where the an-\n6wikipedia.org/wiki/Portal:Current_events/January_2022\n7http://mojeek.com\n379\nPrompt GPT2 (XL) Completion SeeKeR (XL) completion\nIn recent developments, we\nhave learned the following\nabout Ban of Twitter in\nNigeria.\nBan of Twitter is a social media platform that\nallows users to create and share content on\nTwitter. The platform was launched in Nige-\nria in March 2017.\nThe Minister of Information, Lai Mo-\nhammed, has said that the ban on Twitter\nwill be lifted soon.\nIn recent developments, we\nhave learned the following\nabout Charles Njonjo.\n1. He was born in Nigeria. 2. He is a graduate\nof the University of Lagos.\nWe have learned that Charles was cremated\non January 2, 2022, and that his body was\nreturned to his family.\nIn recent developments, we\nhave learned the following\nabout William M. Elling-\nhaus.\nEllinghaus was born in New York City, New\nYork, on January 1, 1891.\nEllinghaus was named chairman of the Mu-\nnicipal Assistance Corporation, which was\ncreated to contain the crisis, and was later ap-\npointed to the Emergency Financial Control\nBoard, which took over city fiscal affairs.\nIn recent developments, we\nhave learned the following\nabout Terry Teachout.\nTerry Teachout is running for Congress in\nNew York’s18th Congressional District. She\nis a former state assemblyman and state sena-\ntor.\nTerry Teachout is the author of the new book,\nThe New Jim Crow: Mass Incarceration in\nthe Age of Colorblindness.\nTable 4: Topical prompts: cherry and lemon picked examples comparing SeeKeR with GPT2.In the first two\nexamples GPT2 hallucinates (in yellow), while SeeKeR provides correct topical continuations. In the third example\nSeeKeR does not hallucinate, but is not topical with a recent development. In the fourth example both GPT2 and\nSeeKeR give poor responses. SeeKeR is correct in that Terry Teachout is an author, but it names a book by Michelle\nAlexander, which happens to be on the same web page as a book by Terry Teachout that the search engine retrieves.\nnotator has access to internet search for validation\npurposes. The correctness is measured in four axes:\nsensible (does it reasonably follow the prompt?),\ntrue (does it contain some true information?),hallu-\ncination (does it contain some false information?)\nand topical (does it reference what happened in the\nlast two months, i.e., January and February 2022?).\nResults Results are given in Table 3. We find that\nour SeeKeR model provides improved metrics over\nGPT2 with more true completions (by over 20%),\nfewer hallucinations (by around 20%) and more\ntopicality (by about 15%), whilst sensibleness is\nslightly less (e.g., 81% vs. 77%). We find these\nwins across all model sizes (medium, large and XL)\nand in fact a medium size (345M) SeeKeR model\noutperforms GPT2 XL (1.5B) by similar margins\nas those just mentioned. GPT3, on the other hand,\nis a far larger model that has also been fine-tuned\nwith human judgments (Ouyang et al., 2022) and\noutperforms GPT2 and SeeKeR in terms of the\nsensible and true metrics, generating fluent text\nthat can in some cases directly copy portions of the\nrelevant Wikipedia article. However, like GPT2,\nit also introduces a large number of hallucinations\n(62%), and fails to be topical (4%). A SeeKeR\n345M parameter model, due to its search capability,\noutperforms GPT3 on the hallucination and topical\nmetrics, despite being 500×smaller.\nAnalysis We show example cherry and lemon\npicked examples in Table 4. The first two examples\nshow SeeKeR providing topical correct comple-\ntions based on the results from the search engine,\nwhereas GPT2 hallucinates non-topical yet fluent\nlooking responses. The third and fourth exam-\nples show failure cases of SeeKeR. Example three\nshows a factually correct response from SeeKeR,\nwhich is based on results from the search engine,\nbut it is not topical. The last (fourth) example\nshows a hallucination from SeeKeR where it mixes\nup two authors; inspecting the web search results\nindicates this is because both authors are mentioned\nin the page, and the method mixes them up. We\nshow some further examples comparing to GPT3\nin Appendix Table 7.\nDue to the issue of non-topical results from web\nsearch, we also tried a version of SeeKeR where\nwe appended “January 2022” to the search query to\nsee if this produced more topical generations. We\ndo see a reduction in hallucinations and a relative\nincrease in topicality in this case (up from 15%\nto 19%) indicating the search engine part of the\nsystem is crucial for this task.\n5 Conclusion\nWe have presented a modular system for search-\ning for and choosing knowledge during language\nmodel generation. Our approach outperforms the\nstate of the art on dialogue modeling, and is shown\nto outperform both GPT2 with the same architec-\nture on topical prompts – even when using a smaller\nparameter size – and GPT3 – despite being vastly\n380\n(500x) smaller. Our approach of explicitly splitting\ninto three modules allows for engineering better\nmodules in the future, e.g. fine-tuning parts of the\nmodel. We make our code and models publicly\navailable for further research.\n6 Limitations\nOur language models suffer the same issues as\nother systems that exist today, specifically with\nproblems of occasional inconsistency, contradic-\ntions, factual inaccuracies, potential repetition, and\nlack of deeper reasoning, amongst other issues\n(Roller et al., 2021; Ouyang et al., 2022). Further,\ngenerations can include toxic language and bias, es-\npecially with certain contexts and topics (Xu et al.,\n2020; Dinan et al., 2020). Additionally, documents\nfrom the internet influence our generations, which\ncan be a problem if undesirable content is retrieved.\nIn our SeeKeR experiments, we rely on an exter-\nnally built search engine, which has both pros and\ncons. Modular architectures have the advantage\nthat engineers can optimize and develop parts of\nthem separately, and obviously search engines have\nbeen finely tuned in production settings for many\nyears. In contrast, if building one’s own retrieval\nsystem, as many QA and LM methods currently\ndo, one has to essentially start again from scratch.\nSearch engines are already built to crawl and in-\ndex the latest news and documents which requires\nsignificant engineering, but can be important for\napplications. Methods reported in the literature us-\ning their own retrieval setup typically used a fixed\ndatabase of documents, which will hence be out of\ndate. On the other hand, search engines have been\ndesigned to be used by humans, not machines, so\nqueries are in natural language, and only consist of\na few words. Machines can potentially do better by\nencoding a lot more information from a longer con-\ntext into either a longer query, or a vector-encoded\nquery, as is done in e.g. FAISS-based systems\n(Lewis et al., 2020b). However, a benefit of search\nengine-based queries is that they are human read-\nable which provides both interpretability as well as\nthe potential to improve through direct annotation\nor feedback.\n381\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R. So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V . Le. 2020. Towards a human-like open-\ndomain chatbot. CoRR, abs/2001.09977.\nLeonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur\nSzlam, and Jason Weston. Reason first, then respond:\nModular generation for knowledge-infused dialogue.\narXiv preprint arXiv:2111.05204.\nJason Baumgartner, Savvas Zannettou, Brian Kee-\ngan, Megan Squire, and Jeremy Blackburn. 2020.\nThe pushshift reddit dataset. arXiv preprint\narXiv:2001.08435.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, et al. 2021. Improving lan-\nguage models by retrieving from trillions of tokens.\narXiv preprint arXiv:2112.04426.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nMoya Chen, Douwe Kiela, Mojtaba Komeili, Spencer\nPoff, Stephen Roller, Kurt Shuster, Arthur Szlam,\nJason Weston, and Jing Xu. 2021. Blender bot 2.0:\nAn open source chatbot that builds long-term memory\nand searches the internet. https://parl.ai/\nprojects/blenderbot2/. [Online; accessed\n10-March-2022].\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 6491–\n6506, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\nbanek, Douwe Kiela, and Jason Weston. 2020.\nQueens are powerful too: Mitigating gender bias in\ndialogue generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8173–8188, Online. As-\nsociation for Computational Linguistics.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In International Conference on Learning\nRepresentations.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a\ncontinuous cache. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net.\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian\nerror linear units (gelus).\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2019. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In Proceedings of the Inter-\nnational Conference on Learning Representations.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. In International Conference on Learning\nRepresentations.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2021. Nearest neigh-\nbor machine translation. In International Conference\non Learning Representations.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460–8478, Dublin, Ireland. Association\nfor Computational Linguistics.\n382\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453–\n466.\nAngeliki Lazaridou, Elena Gribovskaya, Wojciech\nStokowiec, and Nikolai Grigorev. 2022. Internet-\naugmented language models through few-shot\nprompting for open-domain question answering.\nJungseob Lee, Midan Shim, Suhyune Son, Yujin Kim,\nChanjun Park, and Heuiseok Lim. 2022. Empirical\nstudy on blenderbot 2.0 errors analysis in terms of\nmodel, data and user-centric approach.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019a. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019b. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. Base layers:\nSimplifying training of large, sparse models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 6265–6274.\nPMLR.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020a.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020b.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474. Curran Associates, Inc.\nMargaret Li, Jason Weston, and Stephen Roller. 2019.\nAcute-eval: Improved dialogue evaluation with opti-\nmized questions and multi-turn comparisons. arXiv\npreprint arXiv:1909.03087.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nPierre-Emmanuel Mazaré, Samuel Humeau, Martin Rai-\nson, and Antoine Bordes. 2018. Training millions of\npersonalized dialogue agents. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2775–2779, Brussels,\nBelgium. Association for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@ NIPS.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Preprint.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381, Florence, Italy. Association for\nComputational Linguistics.\n383\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\nKurt Shuster, Da Ju, Stephen Roller, Emily Dinan, Y-\nLan Boureau, and Jason Weston. 2020. The dialogue\ndodecathlon: Open-domain knowledge and image\ngrounded conversational agents. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2453–2470, Online. As-\nsociation for Computational Linguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation. Findings of the\nAssociation for Computational Linguistics: EMNLP\n2021.\nEric Smith, Mary Williamson, Kurt Shuster, Jason We-\nston, and Y-Lan Boureau. 2020. Can you put it all\ntogether: Evaluating conversational agents’ ability to\nblend skills. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics.\nACL.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nJack Urbanek, Angela Fan, Siddharth Karamcheti,\nSaachi Jain, Samuel Humeau, Emily Dinan, Tim\nRocktäschel, Douwe Kiela, Arthur Szlam, and Jason\nWeston. 2019. Learning to speak and act in a fantasy\ntext adventure game. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 673–683, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-\nson Weston, and Emily Dinan. 2020. Recipes for\nsafety in open-domain chatbots. arXiv preprint\narXiv:2010.07079.\nJing Xu, Arthur Szlam, and Jason Weston. 2022. Be-\nyond goldfish memory: Long-term open-domain con-\nversation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5180–5197, Dublin,\nIreland. Association for Computational Linguistics.\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-yi Kong,\nNoah Constant, Petr Pilar, Heming Ge, Yun-Hsuan\nSung, Brian Strope, and Ray Kurzweil. 2018. Learn-\ning semantic textual similarity from conversations.\nIn Proceedings of The Third Workshop on Represen-\ntation Learning for NLP, pages 164–174, Melbourne,\nAustralia. Association for Computational Linguistics.\nDani Yogatama, Cyprien de Masson d’Autume, and\nLingpeng Kong. 2021. Adaptive Semiparametric\nLanguage Models. Transactions of the Association\nfor Computational Linguistics, 9:362–373.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2204–2213. ACL.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. In ACL, system demonstration.\nPei Zhou, Karthik Gopalakrishnan, Behnam Hedayatnia,\nSeokhwan Kim, Jay Pujara, Xiang Ren, Yang Liu,\nand Dilek Hakkani-Tur. 2022. Think before you\nspeak: Explicitly generating implicit commonsense\nknowledge for response generation. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1237–1252, Dublin, Ireland. Association for\nComputational Linguistics.\n384\nA Appendix: Additional Evaluations and Examples\nModel PPL ↓ F1 ↑ KF1 ↑\nKomeili et al. (2022) Results (BART-Large models)\nNo Search 17.4 17.6 6.8\nSearch engine 16.1 17.9 7.0\nGold Doc 13.9 20.0 9.6\nBlenderBot 2 (3B parameters)\nSearch engine - 16.1 6.7\nGold Doc - 18.2 10.5\nSeeKeR Search engine 15.2 16.7 8.3\nSeeKeR Gold Doc 12.7 20.1 12.7\nSeeKeR Gold Knowl. Resp. 8.6 24.5 21.6\nTable 5: Automatic evaluations of SeeKeR compared with existing results from Komeili et al. (2022) and BB2 on\nthe WizInt task (valid set). We do not report BB2 PPL as it is not comparable (different dictionary).\nA.0.1 Open Domain Dialogue Automatic Evaluation\nWe first test our models on the Wizard of Internet open-domain knowledge-grounded dialogue dataset,\nwhich was specifically designed for evaluating internet-driven dialogue agents. As well as measuring\nperplexity and F1 overlap with gold dialogues, one can also measure Knowledge F1 (KF1), the overlap of\nthe dialogue response with the gold annotated knowledge sentences used by the human crowdworker. We\ncan supply the gold documents to the model in an additional evaluation setting, or similarly supply the\ngold knowledge sentence(s) as well. In the full (non-gold) setup, we evaluate the use of the Bing search\nengine to filter Common Crawl, as in Komeili et al. (2022).\nWe compare to the methods reported in Komeili et al. (2022) in Table 5, as well as the BB2 3B parameter\nmodel (Chen et al., 2021). SeeKeR using gold documents or knowledge provides the best performance on\nall three metrics over all methods, while using the search engine with SeeKeR provides lower perplexity\nthan in previously reported methods. Although F1 is lower, KF1 is correspondingly higher, indicating that\nthere is perhaps some trade-off here where our model encourages using more knowledge.\nA.0.2 Prompt Completion Automatic Evaluations\nTask Setting We first test with automatic evaluations the SeeKeR method compared to vanilla GPT2 on\nthe RoBERTa task (see subsection 3.1). To make sure all models are on an equal footing, we fine-tune\nthem on this task (even though GPT2 pre-training should be quite similar), where we train with a given\ndocument up to a given line as the “prompt” and the next line in the document as the continuation. We\nthen measure the metrics of validation perplexity as well as F1 of the generated continuations compared\nto gold. We compare three sizes of GPT2 with SeeKeR, and for each architecture size two variants of\nSeeKeR: the “x3” variant that comprises three independently trained models (for search, knowledge and\nresponse), and the shared parameter version. The “x3” has more parameters than standard SeeKeR or\nGPT2 but can be used to gauge how difficult it is to perform all three tasks at once with a single model.\nThe results for SeeKeR are shown either with the gold document or by using Lucene search over Common\nCrawl (ignoring documents which contain the identical target match, if found – which also includes the\noriginal input document).\nResults The results are given in Appendix Table 6. We see improvements in both perplexity and F1\nwith increasing size models, with SeeKeR models outperforming conventional GPT2 when using Gold\nDocs, and slightly behind when using Lucene search 8. Despite the “x3” SeeKeR models being three\ntimes larger, they are only marginally better than all-in-one SeeKeR models in terms of perplexity, and the\nall-in-one versions even outperform them in terms of F1 for the largest (XL) models.\n8This is to be expected as the probability mass is centered around the knowledge response which may not align with a single\ngold label, thus necessitating human evaluation in addition to automatic evaluations, see Adolphs et al..\n385\nNo Doc\nModel PPL ↓ F1 ↑ PPL ↓ F1 ↑\nGPT2 Medium 11.9 14.8 - -\nGPT2 Large 10.7 15.4 - -\nGPT2 XL 9.7 15.8 - -\nGold Doc Lucene Search\nSeeKeR Med. x3 9.9 25.7 12.6 13.2\nSeeKeR Medium 10.3 25.7 13.1 13.6\nSeeKeR Large x3 8.9 26.3 11.2 13.9\nSeeKeR Large 9.2 27.1 12.3 13.4\nSeeKeR XL x3 8.4 27.2 10.4 13.7\nSeeKeR XL 8.5 28.1 11.3 14.0\nTable 6: Comparison of SeeKeR with GPT2 of various sizes, measured on Common Crawl (valid set). x3 means\nusing three separate models (for 3x the number of parameters). Training a single model to perform search, knowledge\nand response performs similarly to separate models, and provides better performance on the Gold Docs as the\nmodels increase in size.\nPrompt GPT3 (text-davinci-001) Completion SeeKeR (XL) completion\nIn recent developments,\nwe have learned the fol-\nlowing about Kay-Achim\nSchönbach.\nFirst, he is the son of a wealthy industrialist.\nSecond, he is a graduate of the University of\nHeidelberg, where he studied law. Third, he\nis a member of the bar in both Heidelberg\nand Berlin.\nThe head of Germany’s navy, Vice Admi-\nral Kay-Achim Schoenbach, has stepped\ndown from his post after making controver-\nsial statements regarding the delicate security\nsituation on the border between Russia and\nUkraine.\nIn recent developments,\nwe have learned the fol-\nlowing about Ban of Twit-\nter in Nigeria.\n1. Twitter is still unavailable in Nigeria as at\nthe time of this report. 2. There is no update\non when the ban will be lifted.\nThe Minister of Information, Lai Mo-\nhammed, has said that the ban on Twitter\nwill be lifted soon.\nIn recent developments,\nwe have learned the\nfollowing about Charles\nNjonjo.\nCharles Njonjo was born in Kenya- in 1927. -\nHe was educated at the University of London,\nand was called to the bar in 1954.\nWe have learned that Charles was cremated\non January 2, 2022, and that his body was\nreturned to his family.\nIn recent developments,\nwe have learned the fol-\nlowing about William M.\nEllinghaus.\nHe is a graduate of the University of Notre\nDame, where he studied business administra-\ntion. He is the founder and CEO of Elling-\nhaus Corp., a company that provides consult-\ning and financial services to the energy indus-\ntry.\nEllinghaus was named chairman of the Mu-\nnicipal Assistance Corporation, which was\ncreated to contain the crisis, and was later ap-\npointed to the Emergency Financial Control\nBoard, which took over city fiscal affairs.\nIn recent developments,\nwe have learned the fol-\nlowing about Terry Tea-\nchout.\n1) He is a well-known theatre critic and au-\nthor.\n2) His work has appeared in The New York\nTimes, The Wall Street Journal, and The\nWeekly Standard, among other publications.\nTerry Teachout is the author of the new book,\nThe New Jim Crow: Mass Incarceration in\nthe Age of Colorblindness.\nTable 7: Topical prompts: cherry and lemon picked examples comparing SeeKeR with GPT3.In the first four\nexamples GPT3 hallucinates (in yellow), while SeeKeR presents correct topical continuations. In the second to last\nexample SeeKeR does not hallucinate, but is not topical with a recent development. In the last example, GPT3 does\nnot hallucinate, but does not provide a topical completion, while SeeKeR is correct in that Terry Teachout is an\nauthor, but it names a book by Michelle Alexander, which happens to be on the same web page as a book by Terry\nTeachout that the search engine retrieves.\nA.1 Multi-tasking Dialogue and Language Modeling\nSo far we have considered our SeeKeR fine-tuning tasks of dialogue and language modeling separately,\nand have conducted separate experiments in subsection 4.1 and subsection 4.2. Here, we also conduct\nsome experiments to evaluate if we can build a single SeeKeR model that can perform well at both\nfine-tuned dialogue and language modeling tasks all at once. To do this, we begin with the transformer\narchitecture described in subsection 3.1 which has been pre-trained on both dialogue and language\nmodeling tasks (denoted R2C2). We then fine-tune it on both types of tasks as well.\nTopical Prompts Results in Appendix Table 8 compare this model to GPT2 and GPT3, as well as\nGPT2-based SeeKeR language models on the topical prompts task using human evaluations. The results\n386\nHuman Repetitive SeeKeR Human Not Engaging SeeKeR\nHuman Ignore Partner SeeKeR Human Incorrect Knowledge SeeKeR\nFigure 3: Lemon picked examples: four types of issues arising in a conversation between a SeeKeR model chatting\nwith several human crowdworkers. Top leftrepetitive outputs; top rightuninteresting recitation of facts; bottom\nleft ignoring the conversational partner; bottom rightincorrect knowledge used in a response (the model actually\nuses information from the Common Crawl dataset, which has different (and presumably, incorrect) information\nfrom Wikipedia).\nshow that the fully multi-tasked SeeKeR model performs very well, superior to all our GPT2-based\nSeeKeR models on every metric (sensible, true, hallucination and topical), with the lowest hallucination\nscore of 42% that compares very favorably to that of GPT3 (62%). The sensible score was a bit lower for\nthe GPT2 SeeKeR models previously compared to standard GPT2, but this is now closer, at 80% (with\nGPT3 at 82%). Fine-tuning this SeeKeR R2C2 architecture only on language modeling (and not dialogue\nfine-tune tasks) also works well.\nOpen-Domain Dialogue Results in Appendix Table 11 and Table 9 compare this model using automated\nmetrics and human evaluations, respectively, on our open-domain knowledge-grounded dialogue task.\nThe model performs comparably, if not better, in all automated metrics on the task. In human evaluations,\nresults suffer compared to the dialogue fine-tuned only model, with most metrics being lower (e.g., percent\nof knowledge that is engaging dropped from 95% to 75%), except for factually incorrect and the final\n387\nModel Sensible (\n↑)\nTrue (\n↑)\nHallucination (\n↓)\nTopical (\n↑)\nGPT2 Med. (345M) 81% 15% 68% 1%\nGPT2 Large (762M) 81% 18% 71% 0%\nGPT2 XL (1.5B) 81% 14% 73% 0%\nGPT3 (175B InstructGPT) 82% 58% 62% 4%\nSeeKeR GPT2 Med. (345M) 75% 34% 54% 13%\nSeeKeR GPT2 Large(762M) 68% 36% 51% 8%\nSeeKeR GPT2 XL (1.5B) 77% 43% 58% 15%\nSeeKeR GPT2 XL (Jan ’22) 71% 43% 51% 19%\nSeeKeR R2C2 LM only (3B) 77% 46% 47% 16%\nSeeKeR R2C2 (3B) 80% 55% 42% 19%\nTable 8: Topical Prompts: Human Evaluation results comparing multi-tasking SeeKeR with various models.\nIn the main paper we test SeeKeR with a GPT2 pre-trained base to be comparable to GPT2. Here, we additionally\nuse the R2C2 transformer architecture pre-trained with our LM+Dialogue tasks (subsection 3.1). We test two\nversions: SeeKeR R2C2 which is fine-tuned on both the dialogue and LM tasks of subsection 3.2 and subsection 3.3\nand SeeKeR R2C2 LM only, which is fine-tuned only using subsection 3.3. The fully multi-tasked RC2C SeeKeR\n(Dialogue+LM) performs well compared to other models.\nFactually Per-Turn Knowl. % Knowl.\nModel Consistent Knowl. Incorrect Engaging & Engaging is Engaging Rating\nBB1 75.47% 36.17% 9.14% 78.72% 28.79% 79.58% 4.1\nBB2 65.06% 27.88% 4.21% 83.52% 21.93% 78.67% 4.4\nBB1 (R2C2) 73.44% 36.25% 4.84% 79.22% 27.51% 75.90% 4.2\nBB2 (R2C2) 71.91% 67.92% 4.49% 76.03% 53.18% 78.31% 4.2\nSeeKeR (sep. BART modules) 55.39% 41.88% 3.97% 75.09% 28.00% 66.86% 4.4\nSeeKeR 78.47% 46.49% 3.94% 90.41% 44.03% 94.71% 4.2\nSeeKeR Dialogue+LM 70.87% 43.00% 2.90% 84.36% 32.28% 75.07% 4.5\nTable 9: Detailed results and ablations for the open-domain knowledge-grounded dialogue experiments. Human\ncrowdworkers talk to models and rate them using various metrics. We test standard BlenderBot (BB) 1 and 2, and\nR2C2 variants with our Dialogue+LM pre-train tasks (subsection 3.1). We test standard SeeKeR (fine-tuned for\ndialogue), SeeKeR with independent BART modules for search queries and knowledge generation, and a version of\nSeeKeR (Dialogue+LM) fine-tuned on both the dialogue and LM tasks of subsection 3.2 and subsection 3.3.\nrating (which was not a statistically significant result). Thus, developing a strongly-performing multi-task\nsystem that can complete both language modeling and fine-tuned dialogue tasks should still be considered\nfuture work.\nB Model Details\nB.1 SeeKeR 2.7B R2C2 Model Architecture\nThe SeeKeR model used for dialogue has 22 encoder layers and 22 decoder layers, with an embedding\ndimension of 2048, hidden size of 8192, 32 attention heads, pre-layernorms, and GeLU activations\n(Hendrycks and Gimpel, 2016). We train with 1024 positional embeddings, allowing for context up to\n1024 tokens (for which we use the same dictionary as the GPT2 models).\nB.2 SeeKeR 2.7B R2C2 Pre-training Hyperparameters\nThe SeeKeR model was pre-trained using a BART denoising objective (Lewis et al., 2020a) with the\ndefault noise hyperparameters. The model was trained for 500,000 total steps. The maximum learning\nrate was set to 7e −4 with a linear warmup of 15,000 steps and a linear decay to 0. We clipped gradient\nnorms at 1.0, set dropout to 0.1, and a weight decay of 0.01, and otherwise used the same hyperparameters\nas BART Large. The model was pre-trained on 128 V100 GPUs for approximately 25 days.\n388\nWins % matches (Engagingness)\nSeeKeR BB2 BB2 SeeKeR BB1 BB1\nsep. BART (R2C2) (R2C2)\nLoses %\nSeeKeR sep. BART 62 46 43 58 61\nBB2 (R2C2) 38 61 56 58 59\nBB2 54 39 52 51 56\nSeeKeR 57 44 48 57 61\nBB1 42 42 49 43 51\nBB1 (R2C2) 39 41 44 39 49\nWins % matches (Knowledgeable)\nBB2 BB1 BB1 SeeKeR BB2 SeeKeR\nour PT sep. BART our PT\nLoses %\nBB2 52 56 57 55 67 ∗∗\nBB1 our PT 48 52 57 54 67 ∗∗\nBB1 44 48 55 60 48\nSeeKeR sep. BART 43 43 45 64 ∗ 46\nBB2 our PT 45 46 40 36 ∗ 57\nSeeKeR 33 ∗∗ 33 ∗∗ 52 54 43\nTable 10: Human evaluation results on Engagingess (top) and Knowledgeable (bottom) ratings for dialogue models\nusing ACUTE-Eval (Li et al., 2019). ∗ indicates significance (p < .05), ∗∗ indicates significance (p <0.01). We\ncollected an average of 70 ratings per model pair. Results for engagingness are not significant, whereas some of the\nknowledgeable results are; SeeKeR is found to be more knowledgeable than several other models: BB2, and BB1\nwith our pre-training (R2C2).\nSearch Gold Doc\nModel PPL ↓ F1 ↑ KF1 ↑ PPL ↓ F1 ↑ KF1 ↑\nR2C2 SeeKeR Dialogue FT only 15.2 16.7 8.3 12.7 20.1 12.7\nR2C2 SeeKeR Dialogue+LM FT 15.5 16.4 8.4 12.4 20.3 13.2\nTable 11: Automatic evaluations of multi-tasked SeeKeR compared with dialogue-tuned SeeKeR on the WizInt task\n(valid set).\nB.3 SeeKeR 2.7B R2C2 Fine-tuning Hyperparameters\nThe SeeKeR 2.7B R2C2 model was fine-tuned on all of the search, knowledge, and dialogue response\ntasks simultaneously, with training occurring on 64 V100 GPUs for around 20 hours. We used the Adam\noptimizer (Kingma and Ba, 2015) with weight decay (Loshchilov and Hutter, 2019), with a linear warmup\nof 100 steps to a maximum learning rate of 1e −6. We used early stopping on validation performance on\na subset of the training tasks.\nB.4 SeeKeR Medium, Large, XL (GPT2) Fine-tuning Hyperparameters\nThe SeeKeR language models were fine-tuned on all of the search, knowledge, and response tasks\nsimultaneously, with training occurring on 32 V100 GPUs for around 17, 21, and 31 hours for the XL,\nLarge, and Medium models, respectively. We used the Adam optimizer (Kingma and Ba, 2015) with a\nlinear warmup of 500 steps to a maxiumum learning rate of 7e −6. As above, we used early stopping on\nperformance of the training tasks.\nB.5 Decoding Hyperparameters\nSearch Module For all experiments, we use greedy decoding for generating a search query, with a\nminimum generation length of two tokens.\nKnowledge Module For all experiments, we use beam search decoding with a beam size of 3 for\ngenerating a knowledge response. We enforce a minimum beam length of 10 tokens, and implement\nbeam n-gram blocking, n = 3, on both the generated response as well as the context. For the knowledge\n389\nFigure 4: Cherry picked example of a SeeKeR model chatting with a human crowdworker. White boxes on the left\nare the user messages, while we show model search queries in red boxes, generated knowledge in green boxes, and\ndialogue responses in blue boxes. Note that the human conversationalist only saw the final responses (blue boxes)\nfrom their conversational partner.\nresponse module, we not only block on the dialogue context, but also on the generated knowledge\nresponses, to ensure that knowledge is not repeated (at least verbatim) across a conversation.\nResponse Module When computing automated generation metrics on the WizInt task (Table 5, Table 11),\nand for all human evaluation experiments (open-domain knowledge-grounded conversation and topical\nprompt completion, Table 2, Table 3, Table 9), we use standard beam search with a beam size of 10. We\nenforce a minimum beam length of 20 tokens, and implement beam n-gram blocking, n = 3, on both the\ngenerated response as well as the context. When computing automated generation metrics on the prompt\ncompletion task (Table 6), we use greedy decoding.\nC Data Details\nC.1 Pre-training\nOur base model was trained on the concatenation of three existing datasets: RoBERTa, CC100EN, and\nPushshift.io Reddit.\nRoBERTa+cc100en Data We use the same data used to train (Lewis et al., 2021), which consists of\napproximately 100B tokens, combining corpora used in RoBERTa (Liu et al., 2019) with the English\nsubset of the CC100 corpus (Conneau et al., 2020). The GPT2 dictionary, of size 51200, is used for\ntokenization. Following (Lewis et al., 2020a), we perform denoising at the sentence level.\nPushshift.io Reddit We use a variant of Reddit discussions, which has also been used in several existing\nstudies (see e.g. Yang et al. (2018); Mazaré et al. (2018); Shuster et al. (2020)). As discussions are a\ntree-like structure and contain context spanning multiple turns, we flatten the dataset by concatenating all\ncomments from each node in the tree to the root, resulting in one conversation-per-node. We then perform\ndenoising at the conversation level.\n390\nFigure 5: Cherry picked example of a SeeKeR model chatting with a human crowdworker. White boxes on the left\nare the user messages, while we show model search queries in red boxes, generated knowledge in green boxes, and\ndialogue responses in blue boxes. Note: the human conversationalist only saw the final responses (blue boxes) from\ntheir conversational partner.\nC.2 Fine-tuning\nIn Table 1, we outline all of the datasets used for fine-tuning, with the number of training examples for\neach task. We note that in some cases numbers may differ from the original size of the dataset, as we\nperformed some filtering to ensure high quality data. E.g., for the knowledge-grounded dialogue tasks, we\nonly considered cases where the human grounded their response on knowledge; for the search query task,\nwe only use the final search query entered by the human.\nTo indicate the appropriate generation task for the model, we used control tokens appended to the\ncontext. For search tasks, this was __generate-query__; for knowledge, we did not provide\ntokens; and for dialogue, we surrounded the concatenated knowledge with __knowledge__ and\n__endknowledge__ tokens.\nNote that for response generation, while we can use the MS MARCO QA task for this (as we have\naccess to long-form conversational responses), we exclude SQuAD, TriviaQA or NQ from response\nmodeling, as they all comprise generally short-form answers.\nD Human Evaluation Details\nIn Figure 8, we display the instructions provided to crowdworkers when chatting with, and annotating the\nresponses of, the models. In Figure 9, we show what the annotation screen looks like at the beginning of a\nconversation.\nOur crowdsourcing task pays workers well above minimum wage, and we asked privacy and policy\nexperts to review this task before launching. The task does not request any personal information from\nworkers.\nWe follow the same setup as (Komeili et al., 2022) and use the same code for evaluations, available at\n391\nFigure 6: Cherry picked example of a SeeKeR model chatting with a human crowdworker. White boxes on the left\nare the user messages, while we show model search queries in red boxes, generated knowledge in green boxes, and\ndialogue responses in blue boxes. Note: the human conversationalist only saw the final responses (blue boxes) from\ntheir conversational partner.\nHuman Ignore Partner SeeKeR Human Incorrect Knowledge SeeKeR\nFigure 7: Further Lemon picked examples: We show further examples of ignoring partner and incorrect knowledge.\nhttps://https://parl.ai/projects/sea/.\n392\nFigure 8: Instructions provided to crowdworkers for the turn annotation task.\nFigure 9: The annotation pane of the turn annotation task.\n393",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7619642615318298
    },
    {
      "name": "Language model",
      "score": 0.7160326838493347
    },
    {
      "name": "Modular design",
      "score": 0.6427320837974548
    },
    {
      "name": "Modularity (biology)",
      "score": 0.6204531788825989
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6196886301040649
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47586554288864136
    },
    {
      "name": "Natural language processing",
      "score": 0.45936697721481323
    },
    {
      "name": "Code (set theory)",
      "score": 0.4520159363746643
    },
    {
      "name": "Programming language",
      "score": 0.38520053029060364
    },
    {
      "name": "Information retrieval",
      "score": 0.37060749530792236
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.08736041188240051
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ]
}