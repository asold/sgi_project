{
  "title": "Dendritic Learning-Incorporated Vision Transformer for Image Recognition",
  "url": "https://openalex.org/W4391305635",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2126640492",
      "name": "Zhiming Zhang",
      "affiliations": [
        "University of Toyama"
      ]
    },
    {
      "id": "https://openalex.org/A2150701641",
      "name": "Zhenyu Lei",
      "affiliations": [
        "University of Toyama"
      ]
    },
    {
      "id": "https://openalex.org/A2561855493",
      "name": "Masaaki Omura",
      "affiliations": [
        "University of Toyama"
      ]
    },
    {
      "id": "https://openalex.org/A2096199505",
      "name": "Hideyuki HASEGAWA",
      "affiliations": [
        "University of Toyama"
      ]
    },
    {
      "id": "https://openalex.org/A2697766250",
      "name": "Shangce Gao",
      "affiliations": [
        "University of Toyama"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3132455321",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2829536470",
    "https://openalex.org/W1995341919",
    "https://openalex.org/W2796495430",
    "https://openalex.org/W2979879900",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6797790494",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W1977632372",
    "https://openalex.org/W3205959825",
    "https://openalex.org/W3159573745",
    "https://openalex.org/W4313178187",
    "https://openalex.org/W2046900303",
    "https://openalex.org/W6754585794",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2889686528",
    "https://openalex.org/W3175544090"
  ],
  "abstract": "This letter proposes to integrate dendritic learnable network architecture with Vision Transformer to improve the accuracy of image recognition. In this study, based on the theory of dendritic neurons in neuroscience, we design a network that is more practical for engineering to classify visual features. Based on this, we propose a dendritic learning-incorporated vision Transformer (DVT), which out-performs other state-of-the-art methods on three image recognition benchmarks.",
  "full_text": " \nLetter\nDendritic Learning-Incorporated Vision Transformer for\nImage Recognition\nZhiming Zhang , Zhenyu Lei , Masaaki Omura ,\nHideyuki Hasegawa , Member, IEEE , and\nShangce Gao , Senior Member, IEEE\n   \nDear Editor,\nThis\n letter proposes to integrate dendritic learnable network archi-\ntecture  with  Vision  Transformer  to  improve  the  accuracy  of  image\nrecognition. In this study, based on the theory of dendritic neurons in\nneuroscience,  we  design  a  network  that  is  more  practical  for  engi-\nneering to classify visual features. Based on this, we propose a den-\ndritic  learning-incorporated  vision  Transformer  (DVT),  which  out-\nperforms  other  state-of-the-art  methods  on  three  image  recognition\nbenchmarks.\nIntroduction: Image  recognition,  as  an  upstream  task  of  many\ncomputer vision problems, has very important research value. Many\nstudies focus on optimizing the architecture of the feature extraction\nnetwork to make it extract richer and more representative image fea-\ntures. In the early stages of deep learning, the convolutions are sim-\nply stacked to build feature extraction networks. While effective, this\nmethod had some limitations such as the need for large amounts of\ndata, long training times, and limited interpretability [1]. To address\nthese issues, researchers have introduced more effective and biologi-\ncally  interpretable  structures.  The  use  of  residual  connections  [2],\ndensely connected blocks [3], and attention mechanisms [4] have all\nbeen  explored  to  improve  the  performance  of  image  recognition\nmodels. These structures have proven to be successful in improving\naccuracy,  reducing  training  time,  and  enhancing  interpretability.\nMore recently, the introduction of vision Transformer (ViT) has fur-\nther  improved  the  network  used  to  extract  image  features  [4].  ViT\ndecomposes  images  into  multiple  patches  and  processes  them\nthrough  multiple  Transformer  layers,  allowing  the  network  to  cap-\nture global context and long-term dependencies of images. Further-\nmore,  a  self-attention  mechanism  allows  the  model  to  focus  on  the\nmost  important  regions  of  images,  further  improving  its  efficiency\nand accuracy.\nHowever, another important aspect of the image recognition task is\nseldom mentioned, i.e., how to effectively classify the extracted fea-\ntures.  Most  of  the  aforementioned  studies  have  focused  on  using\nmulti-layer  perceptron  (MLP)  structures  for  feature  classification.\nDespite  their  simplicity  and  effectiveness,  MLPs  still  have  limita-\ntions, such as excessive parameter requirements and susceptibility to\noverfitting [5]. They are also less suitable for handling high-dimen-\nsional feature vectors in large-scale image recognition tasks. Inspired\nby  the  evolution  of  visual  feature  extraction  networks,  developing\nmore  efficient  and  biologically  interpretable  classification  networks\nhas  the  potential  to  significantly  improve  image  recognition  accu-\nracy. Thus, opening up new possibilities for computer vision applica-\ntions.\nArtificial neurons, inspired by their biological counterparts, play a\ncrucial role in shaping neural networks. The initial McCulloch Pitt’s\nmodel  used  a  simple  linear  threshold  function  for  computation  [6].\nMLP  architectures  later  addressed  the  issue  of  linear  inseparability\n[7], and spiking neural networks introduced discrete pulse signals to\nimprove  computational  efficiency  [8].  However,  there  still  exists  a\nconsiderable accuracy gap between current artificial neurons and bio-\nlogical  neurons.  Recently,  dendritic  neurons,  drawing  inspiration\nfrom  neuroscience,  have  emerged  as  promising  alternatives.  With\ntheir  architecture  incorporating  synapse,  dendrite,  and  soma  layers,\ndendritic  networks  enhance  biological  interpretability  and  exhibit\nsuperior performance in challenging tasks [5].\nIn this study, we propose a novel neural network architecture that\ncombines two biologically interpretable networks for neuroscientifi-\ncally aligned image recognition. To ensure practicality, we carefully\ndesign the synapse, dendrite, and soma layers of the dendritic neuron\nas an artificial neuron model. By integrating the Vision Transformer\nwith our proposed dendritic network, we create DVT, a highly inter-\npretable  network.  Extensive  experiments  on  multiple  benchmarks\ndemonstrate  the  significant  performance  improvements  achieved  by\nDVT compared to state-of-the-art methods in image recognition. The\naccuracy results in Fig. 1 depict the performance of peer models on\nthe CIFAR dataset, without pre-training weights. These findings indi-\ncate the potential of DVT to advance computer vision and deepen our\nunderstanding of visual perception mechanisms.\n \n90.5 91.0 91.5 92.0 92.5 93.0 93.5 94.0 94.5\nAccuracy in CIFAR10 (%)\n62.0\n64.0\n66.0\n68.0\n70.0\n72.0Accuracy in CIFAR100 (%)\nVGG\nResNet\nViT\nSwin\nCaiT DVT\nCNNs\nTransformers\n \nFig. 1. The accuracy comparison in CIFAR.\n \nRelated work:\n1) Vision Transformer: It, a novel class of image feature extraction\nnetworks,  overcomes  the  limitations  of  traditional  convolutional\noperators  by  using  long-term  dependency-based  self-attention  to\nextract  spatial  features  [4].  However,  despite  their  effectiveness  in\ncapturing global features, they still face issues such as computational\ncomplexity [9], sensitivity to hyper-parameters [10], and data depen-\ndency [11]. Besides, enhancing their expressiveness, particularly on\nlower-resolution datasets, remains a significant challenge.\n2) Dendritic network: Inspired by the structure and function of reti-\nnal ganglion cells [12], the dendritic network has been proposed as a\nmore  biologically  plausible  artificial  neuron  [5].  It  has  shown\nremarkable results in various kinds of problems [13], [14]. However,\nits sophisticated architecture requires efficient learning algorithms to\nimprove its performance [5], which presents an obstacle to its further\ndevelopment. In light of this, we aim to optimize the architecture of\nthe  dendritic  network  to  enhance  its  practical  performance.  Specifi-\ncally, we reinvent its synapse, dendrite, and soma layer to improve its\nlearning  stability  and  performance.  It  makes  the  dendritic  network\nmore practical for image recognition.\nMethodology: In  this  study,  we  propose  DVT,  a  dendritic  learn-\ning-incorporated vision Transformer, aimed at enhancing the perfor-\nmance  and  interpretability  of  image  recognition  tasks.  DVT  com-\nbines  two  essential  components:  a  vision  Transformer  featuring\nembedded  attention  mechanisms  and  a  dendritic  network  mirroring\nreal  neuronal  architecture.  The  vision  Transformer  extracts  more\ncomprehensive  and  representative  image  features,  while  the  den-\ndritic  network  ensures  accurate  feature  classification.  The  overall\n \nCorresponding author: Shangce Gao.\nCitation: Z.  Zhang,  Z.  Lei,  M.  Omura,  H.  Hasegawa,  and  S.  Gao,\n“Dendritic  learning-incorporated  vision  transformer  for  image  recognition,”\nIEEE/CAA J. Autom. Sinica, vol. 11, no. 2, pp. 539–541, Feb. 2024.\nThe  authors  are  with  the  Faculty  of  Engineering,  University  of  Toyama,\nToyama-shi 930-8555, Japan (e-mail: d2272007@ems.u-toyama.ac.jp; leizg@\neng.u-toyama.ac.jp;  momura@eng.u-toyama.ac.jp;  hasegawa@eng.u-toyama.\nac.jp; gaosc@eng.u-toyama.ac.jp).\nColor  versions  of  one  or  more  of  the  figures  in  this  paper  are  available\nonline at http://ieeexplore.ieee.org.\nDigital Object Identifier 10.1109/JAS.2023.123978\nIEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 11, NO. 2, FEBRUARY 2024 539 \nframework of DVT is depicted in Fig. 2. Initially, the input image is\nsliced into smaller patches, and linear projection and position encod-\ning are applied to each patch to preserve spatial information and opti-\nmize  computational  efficiency.  These  processed  feature  maps  are\nthen fed into multiple stacked Transformer blocks, wherein the self-\nattention mechanism enables the network to selectively focus on per-\ntinent  information  and  suppress  irrelevant  noise.  Through  continu-\nous  fusion  and  amplification  of  receptive  fields,  DVT  efficiently\nextracts  highly  representative  features  from  the  entire  image.  The\ndendritic network, comprising three biomimetic layers (synapse, den-\ndrite, and soma), takes charge of the final feature classification. To\nenhance its convergence·, we introduce a feature normalization oper-\nation  that  reduces  feature  discrepancies  and  overall  improves  net-\nwork performance.\nm 2 Rh\u0002n\u0002n\n1)  Self-attention  mechanism:  It  plays  a  crucial  role  in  visual  fea-\nture  extraction  by  incorporating  local  and  global  information  to\nobtain  more  representative  features  [4].  By  employing  multi-head\nself-attention  and  stacking  multiple  Transformer  blocks,  the  robust-\nness  of  feature  extraction  is  enhanced.  However,  the  computational\ncost  of  training  a  vision  Transformer  from  scratch  remains  a  chal-\nlenge,  particularly  when  dealing  with  small-sized  datasets.  To  miti-\ngate  this  issue,  we  propose  integrating  locality  self-attention  into\nDVT,  drawing  inspiration  from  [15].  This  modification  effectively\ncaptures  locally-focused  attention  contextual  information  through  a\nself-masking  matrix  and\n  a  learnable  parameter γ.  This\nadaptation improves the efficiency of DVT without compromising its\nability to capture relevant local information. Its formula is following:\n \nz = A(q;k;v) = \u000e(m ⊙ qkT\np\r )v (1)\n \nm = Jn \u00001 In (2)\nv 2 Rh\u0002n\u0002d\nA(\u0001)\nwhere q, k, and  are  different feature vectors that obtained\nby  linear  projection  of  input  data x.  Self-attention  integrates\nthem\n via  scaled  dot-product. m is  a  all-ones  matrix  with  negative\ninfinity eigenvalues. It is added to A to further deepen the ability of\nthe network to capture global features.\n\u0011(\u0001)\n2)  Dendritic  network:  Extensive  neuroscience  research  has\nunequivocally demonstrated the irreplaceable nature of the theoreti-\ncal model of dendritic nerves. Moreover, numerous experiments con-\nducted  in  the  field  of  information  science  have  consistently  show-\ncased the remarkable capability of dendritic networks in effectively\naddressing nonlinear problems. Building upon this knowledge, in our\nproposed  DVT,  we  integrate  a  feature  normalization  operation \ninto  the  dendritic  network,  thus  aligning  it  more  closely  with  the\npractical requirements of real-world engineering applications, i.e.,\n \nyk =\nm∑\ni=1\nd∑\nj=1\n\u000e(\u0011(wk\ni; j\u0011(x) j +bk\ni; j)) (3)\n \ny = [y1;y2; :\n: : ;yc] (4)\n \n\u0011(x) = x \u0000 ¯xp\u001b(x) +ϵ\n\u0012 +\u0015 (5)\nyk\n\u000e(\u0001)\n\u0011(\u0001)\n¯x\n\u001b(x)\nwhere x is  the  input  feature  of d dimension  and  is  the  predicted\nprobability  of  the tth  classification  target  by  the  network. c is  the\nnumber  of  classes.  First,  normalized  inputs  are  mapped  to m den-\ndritic branches through m sets of learnable parameters w and b, a pro-\ncess  called  synaptic  connection.  Then,  feature  normalization  and\nsoftmax  activation  function  are  performed  on  each  branch.\nFinally, soma layer conception in neuroscience is applied in the net-\nwork to integrate all dendrites into the result. Notably, each y is asso-\nciated with one dendritic neuron following (3), and the synapses on\neach branch are independent for each neuron. Such mutually exclu-\nsive  connections  are  considered  to  be  ubiquitous  in  neuroscience\n[16], and they have also been proved to be the basis of efficient net-\nwork inference [17]. In proposed feature normalization , θ\n  and λ\nare  learnable  parameters, ϵ is  constant  to  prevent  the  denominator\nfrom being 0,  and  are mean and variance of x, respectively.\nExperiment:\n1)  Dendrite  and  learning  rate  analysis:  The  number  of  dendrite\nbranches directly influences the network’s ability to approximate the\nobjective  function  accurately.  Similarly,  the  learning  rate  signifi-\ncantly  impacts  the  network’s  adaptability  and  learning  capacity.  In\nthis  study,  we  comprehensively  analyze  these  hyper-parameters  to\ndetermine the optimal DVT configuration. We establish a fair base-\nline by comparing our findings to the original ViT. To ensure a con-\nsistent  evaluation,  all  methods  are  trained  for  100  epochs  using  the\nAdamW optimizer. CIFAR10 is chosen as the dataset for this experi-\nment due to its universality in image recognition and the diversity of\nimages  it  contains. Fig. 3 presents  the  results,  where  the  number  of\ndendritic  branches  is  denoted  as  0,  representing  the  original  ViT.\nRemarkably,  incorporating  the  dendritic  network  significantly\nimproves  the  performance  of  DVT  across  various  learning  rates.\nThrough  extensive  experimentation  with  different  numbers  of  den-\ndritic branches (ranging from 2 to 64), we observe that increasing the\nnumber  of  branches  leads  to  better  results.  For  optimal  prediction\noutcomes  across  different  problem  domains,  we  recommend  setting\nthe number of branches within the range of 8 to 32. Additionally, we\nfind that a learning rate of 0.003 yields favorable outcomes for DVT.\n2) Performance comparison: We evaluate the performance of DVT\nagainst  state-of-the-art  methods  using  four  widely  recognized  and\nchallenging  datasets:  SVHN,  CIFAR10,  CIFAR100,  and  Tiny-Ima-\ngeNet.  The  compared  methods  include  VGG19  [18],  ResNet50  [2],\nViT [4], Swin [9], and CaiT [11], covering the classic CNN architec-\nture  and  the  latest  Transformer-based  neural  network.  Notably,  the\nbiological  interpretability  of  these  methods  has  shown  a  gradual\nincrease, transitioning from CNN to Transformer models. The results\npresented in Table 1 demonstrate the clear superiority of DVT over\nits peers. Notably, the advantages of DVT become more pronounced\nas  the  classification  difficulty  of  the  datasets  increases,  reinforcing\nour  conclusion  that  DVT  excels  at  approximating  complex  target\n \nLinear project and position embedding\n1 2 3 414 1516 0…\n…\n1 2 3 414 1516 … Class\nHorse, Cat, … …\nLearnable param\nSelf-attention\nSynapse layer\nDendrite layer\nSoma layer\nVision \ntransformer\nDendritic\n0\nLinear projection\nResidual connection\n1\n5\n9\n13\n2\n6\n10\n14\n3\n7\n11\n15\n4\n8\n12\n16\nnetwork\n \nFig. 2. The framework of DVT to recognize image.\n \n 540 IEEE/CAA JOURNAL OF AUTOMATICA SINICA, VOL. 11, NO. 2, FEBRUARY 2024\nfunctions. Furthermore, the accuracy of each model increases as its\ninterpretability  improves,  highlighting  the  advantage  of  employing\nbiologically interpretable models for image recognition problems.\n3) Ablation study: We delve deeper into the spatial and temporal\ncomplexity  of  DVT.  It  exhibits  an  increased  number  of  learnable\nparameters  and  higher  FLOPs  compared  to  the  original  ViT.  More\nspecifically,  we  introduce  a  linear  layer  between  the  extracted  fea-\ntures  and  the  classification  outcomes.  As  presented  in Table 2,  the\nsizes  of  the  added  linear  layers  are  160  and  576,  respectively.  The\nparameters of the ViT-160 and the FLOP of the ViT-576 are almost\nsimilar to those in the DVT. This allows us to perform comparisons\nto highlight the performance advantages of DVT. The backbone net-\nworks of all model architectures are the same. Therefore, only learn-\nable  parameters  and  FLOPs  of  their  classification  networks  are\ncounted.  Notably,  a  simple  stacking  of  linear  layers  and  increasing\ntheir  size  not  only  fails  to  enhance  accuracy  but  also  leads  to\ndegraded network performance due to heightened learning difficulty.\nIn  contrast,  DVT  relies  on  a  sophisticated  architecture  to  perform\nefficient calculations with fewer parameters, thereby enhancing net-\nwork performance.\n \nTable 2.  Ablation Study On IFAR10\nArchitecture Params FLOPs Accuracy\nViT 1.92 K 1.93 K 93.01\nViT-160 32.49 K 32.32 K 92.92\nViT-576 117.32 K 117.31 K 92.68\nDVT 34.19 K 108.58 K 94.15\n \n \nConclusion: In this study, we introduce DVT, a dendritic learning-\nincorporated vision Transformer, specifically designed for universal\nimage  recognition  tasks  inspired  by  dendritic  neurons  in  neuro-\nscience. The incorporation of a highly biologically interpretable den-\ndritic architecture enables DVT to excel in handling complex nonlin-\near  classification  problems.  Our  experimental  results  highlight  the\nsubstantial improvement achieved by DVT compared to the current\nstate-of-the-art  methods  on  four  general  datasets.  Moreover,  these\nfindings  affirm  our  hypothesis  that  networks  with  high  biological\ninterpretability  in  architecture  also  exhibit  superior  performance  in\nimage recognition tasks.\nAcknowledgments: This  work  was  partially  supported  by  the\nJapan  Society  for  the  Promotion  of  Science  (JSPS)  KAKENHI\n(JP22H03643),  Japan  Science  and  Technology  Agency  (JST)  Sup-\nport  for  Pioneering  Research  Initiated  by  the  Next  Generation\n(SPRING)  (JPMJSP2145),  and  JST  through  the  Establishment  of\nUniversity Fellowships towards the Creation of Science Technology\nInnovation (JPMJFS2115).\nReferences\n S.  Minaee,  Y.  Boykov,  F.  Porikli,  A.  Plaza,  N.  Kehtarnavaz,  and  D.\nTerzopoulos, “Image  segmentation  using  deep  learning:  A  survey,”\nIEEE Trans. Pattern Analysis and Machine Intelligence, vol.\n 44, no. 7,\npp. 3523–3542, 2021.\n[1]\n K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proc.  IEEE  Conf.  Computer  Vision  and  Pattern\nRecognition, 2016, pp. 770–778.\n[2]\n G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected  convolutional  networks,” in Proc.  IEEE  Conf.  Computer\nVision and Pattern Recognition, 2017, pp. 4700–4708.\n[3]\n A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T.\nUnterthiner,  M.  Dehghani,  M.  Minderer,  G.  Heigold,  S.  Gelly,  J.\nUszkoreit,  and  N.  Houlsby, “An  image  is  worth  16×16  words:\nTransformers  for  image  recognition  at  scale,” in Proc.  Int.  Conf.\nLearning Representations, 2021.\n[4]\n S.  Gao,  M.  Zhou,  Y.  Wang,  J.  Cheng,  H.  Yachi,  and  J.  Wang,\n“Dendritic  neuron  model  with  effective  learning  algorithms  for\nclassification,  approximation,  and  prediction,” IEEE  Trans.  Neural\nNetworks and Learning Systems, vol.\n 30, no. 2, pp. 601–614, 2019.\n[5]\n W.  S.  McCulloch  and  W.  Pitts, “A  logical  calculus  of  the  ideas\nimmanent  in  nervous  activity,” The  Bulletin  of  Mathematical\nBiophysics, vol.\n 5, pp. 115–133, 1943.\n[6]\n D. T. Tran, S. Kiranyaz, M. Gabbouj, and A. Iosifidis, “Heterogeneous\nmultilayer  generalized  operational  perceptron,” IEEE  Trans.  Neural\nNetworks and Learning Systems, vol.\n 31, no. 3, pp. 710–724, 2019.\n[7]\n A. Taherkhani, A. Belatreche, Y. Li, G. Cosma, L. P. Maguire, and T.\nM. McGinnity, “A review of learning in biologically plausible spiking\nneural networks,” Neural Networks, vol. 122, pp. 253–272, 2020.\n[8]\n Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin  transformer:  Hierarchical  vision  transformer  using  shifted\nwindows,” in Proc.  IEEE/CVF  Int.  Conf.  Computer  Vision,  2021,  pp.\n10012–10022.\n[9]\n T.  Xiao,  M.  Singh,  E.  Mintun,  T.  Darrell,  P.  Dollár,  and  R.  Girshick,\n“Early convolutions help transformers see better,” Advances in Neural\nInformation Processing Systems, vol. 34, pp. 30392–30400, 2021.\n[10]\n H.  Touvron,  M.  Cord,  A.  Sablayrolles,  G.  Synnaeve,  and  H.  Jégou,\n“Going deeper with image transformers,” in Proc. IEEE/CVF Int. Conf.\nComputer Vision, 2021, pp. 32–42.\n[11]\n C. Koch, T. Poggio, and V. Torre, “Retinal ganglion cells: A functional\ninterpretation  of  dendritic  morphology,” Philosophical  Transa.  Royal\nSociety of London, vol.\n 298, no. 1090, pp. 227–263, 1982.\n[12]\n Y.  Yu,  Z.  Lei,  Y.  Wang,  T.  Zhang,  C.  Peng,  and  S.  Gao, “Improving\ndendritic  neuron  model  with  dynamic  scale-free  network-based\ndifferential  evolution,” IEEE/CAA  J.  Automa.  Sinica,  vol.\n 9,  no. 1,\npp. 99–110, 2021.\n[13]\n H.  He,  S.  Gao,  T.  Jin,  S.  Sato,  and  X.  Zhang, “A  seasonal-trend\ndecomposition-based  dendritic  neuron  model  for  financial  time  series\nprediction,” Applied Soft Computing, vol.\n 108, p. 107488, 2021.\n[14]\n S.  Lee,  S.  Lee,  and  B.  Song, “Improving  vision  transformers  to  learn\nsmall-size dataset from scratch,” IEEE Access, vol. 10, p. 123, 2022.\n[15]\n R.  Yuste, “Dendritic  spines  and  distributed  circuits,” Neuron,  vol. 71,\nno. 5, pp. 772–781, 2011.\n[16]\n X.  Wu,  X.  Liu,  W.  Li,  and  Q.  Wu, “Improved  expressivity  through\ndendritic neural networks,” Advances in Neural Information Processing\nSystems, vol. 31, pp. 8057–8068, 2018.\n[17]\n K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale  image  recognition,” in Proc.  Int.  Conf.  Learning\nRepresentations, 2015.\n[18]\n \nTable 1.  Accuracy Comparison on Four Datasets\nSVHN CIFAR10 CIFAR100 ImageNet\nVGG19 96.98 90.57 61.77 41.77\nResNet50 97.30 91.16 68.14 54.38\nViT 97.31 93.01 69.31 45.14\nSwin 97.42 93.78 70.33 48.13\nCaiT 97.66 92.94 71.80 54.66\nDVT 97.72 94.15 72.20 54.78\n \n \n0 2 4 8 16 6432\nNumber of dendritic branches\n90.5\n91.0\n91.5\n92.0\n92.5\n93.0\n93.5\n94.0Accuracy (%)\nLearning rate\n1E−3\n3E−3\n5E−3\n7E−3\n9E−3\n \nFig. 3. The analysis of dendrite and learning rate in CIFAR10.\n \nZHANG et al.: DVT FOR IMAGE RECOGNITION 541 ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6752737760543823
    },
    {
      "name": "Computer science",
      "score": 0.6339097023010254
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6113112568855286
    },
    {
      "name": "Architecture",
      "score": 0.5653395056724548
    },
    {
      "name": "Computer vision",
      "score": 0.41116490960121155
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34537625312805176
    },
    {
      "name": "Engineering",
      "score": 0.2666167616844177
    },
    {
      "name": "Electrical engineering",
      "score": 0.0831182599067688
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I42766147",
      "name": "University of Toyama",
      "country": "JP"
    }
  ]
}