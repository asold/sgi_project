{
  "title": "Incremental Transformer with Deliberation Decoder for Document Grounded Conversations",
  "url": "https://openalex.org/W2963186354",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3113339720",
      "name": "Li, Zekang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353459990",
      "name": "Niu Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A94544940",
      "name": "Meng, Fandong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097009727",
      "name": "Feng Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1892229073",
      "name": "Li Qian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1950278301",
      "name": "Zhou Jie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2799176105",
    "https://openalex.org/W2963789888",
    "https://openalex.org/W2586847566",
    "https://openalex.org/W2891501508",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2900654821",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2798858969",
    "https://openalex.org/W2979478117",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2752047430",
    "https://openalex.org/W2888296173",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2952243835",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2952230306",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2891826200",
    "https://openalex.org/W2898875342",
    "https://openalex.org/W3006065545"
  ],
  "abstract": "Document Grounded Conversations is a task to generate dialogue responses when chatting about the content of a given document. Obviously, document knowledge plays a critical role in Document Grounded Conversations, while existing dialogue models do not exploit this kind of knowledge effectively enough. In this paper, we propose a novel Transformer-based architecture for multi-turn document grounded conversations. In particular, we devise an Incremental Transformer to encode multi-turn utterances along with knowledge in related documents. Motivated by the human cognitive process, we design a two-pass decoder (Deliberation Decoder) to improve context coherence and knowledge correctness. Our empirical study on a real-world Document Grounded Dataset proves that responses generated by our model significantly outperform competitive baselines on both context coherence and knowledge relevance.",
  "full_text": "Incremental Transformer with Deliberation Decoder\nfor Document Grounded Conversations\nZekang Li†♦, Cheng Niu‡, Fandong Meng‡∗, Yang Feng♦, Qian Li♠, Jie Zhou‡\n†Dian Group, School of Electronic Information and Communications\nHuazhong University of Science and Technology\n‡Pattern Recognition Center, WeChat AI, Tencent Inc, China\n♦Key Laboratory of Intelligent Information Processing\nInstitute of Computing Technology, Chinese Academy of Sciences\n♠School of Computer Science and Engineering, Northeastern University, China\nzekangli97@gmail.com, {chengniu,fandongmeng,jiezhou}@tencent.com\nfengyang@ict.ac.cn, qianli@stumail.neu.edu.cn\nAbstract\nDocument Grounded Conversations is a task\nto generate dialogue responses when chatting\nabout the content of a given document. Ob-\nviously, document knowledge plays a critical\nrole in Document Grounded Conversations,\nwhile existing dialogue models do not exploit\nthis kind of knowledge effectively enough. In\nthis paper, we propose a novel Transformer-\nbased architecture for multi-turn document\ngrounded conversations. In particular, we de-\nvise an Incremental Transformer to encode\nmulti-turn utterances along with knowledge\nin related documents. Motivated by the hu-\nman cognitive process, we design a two-pass\ndecoder (Deliberation Decoder) to improve\ncontext coherence and knowledge correctness.\nOur empirical study on a real-world Document\nGrounded Dataset proves that responses gen-\nerated by our model signiﬁcantly outperform\ncompetitive baselines on both context coher-\nence and knowledge relevance.\n1 Introduction\nPast few years have witnessed the rapid develop-\nment of dialogue systems. Based on the sequence-\nto-sequence framework (Sutskever et al., 2014),\nmost models are trained in an end-to-end man-\nner with large corpora of human-to-human di-\nalogues and have obtained impressive success\n(Shang et al., 2015; Vinyals and Le, 2015; Li et al.,\n2016; Serban et al., 2016). While there is still\na long way for reaching the ultimate goal of di-\nalogue systems, which is to be able to talk like\nhumans. And one of the essential intelligence\nto achieve this goal is the ability to make use of\nknowledge.\n∗ ∗Fandong Meng is the corresponding author of the pa-\nper. This work was done when Zekang Li was interning at\nPattern Recognition Center, WeChat AI, Tencent.\nThere are several works on dialogue sys-\ntems exploiting knowledge. The Mem2Seq\n(Madotto et al., 2018) incorporates structured\nknowledge into the end-to-end task-oriented di-\nalogue. Liu et al. (2018) introduces fact-\nmatching and knowledge-diffusion to generate\nmeaningful, diverse and natural responses using\nstructured knowledge triplets. Ghazvininejad\net al. (2018), Parthasarathi and Pineau (2018),\nYavuz et al. (2018), Dinan et al. (2018) and\nLo and Chen (2019) apply unstructured text facts\nin open-domain dialogue systems. These works\nmainly focus on integrating factoid knowledge\ninto dialogue systems, while factoid knowledge\nrequires a lot of work to build up, and is only\nlimited to expressing precise facts. Documents as\na knowledge source provide a wide spectrum of\nknowledge, including but not limited to factoid,\nevent updates, subjective opinion, etc. Recently,\nintensive research has been applied on using\ndocuments as knowledge sources for Question-\nAnswering (Chen et al., 2017; Huang et al., 2018;\nYu et al., 2018; Rajpurkar et al., 2018; Reddy\net al., 2018).\nThe Document Grounded Conversation is a task\nto generate natural dialogue responses when chat-\nting about the content of a speciﬁc document. This\ntask requires to integrate document knowledge\nwith the multi-turn dialogue history. Different\nfrom previous knowledge grounded dialogue sys-\ntems, Document Grounded Conversations utilize\ndocuments as the knowledge source, and hence\nare able to employ a wide spectrum of knowl-\nedge. And the Document Grounded Conversations\nis also different from document QA since the con-\ntextual consistent conversation response should be\ngenerated. To address the Document Grounded\nConversation task, it is important to: 1) Exploit\ndocument knowledge which are relevant to the\narXiv:1907.08854v3  [cs.CL]  31 Jul 2019\nconversation; 2) Develop a uniﬁed representation\ncombining multi-turn utterances along with the\nrelevant document knowledge.\nIn this paper, we propose a novel and effec-\ntive Transformer-based (Vaswani et al., 2017) ar-\nchitecture for Document Grounded Conversations,\nnamed Incremental Transformer with Deliberation\nDecoder. The encoder employs a transformer ar-\nchitecture to incrementally encode multi-turn his-\ntory utterances, and incorporate document knowl-\nedge into the the multi-turn context encoding pro-\ncess. The decoder is a two-pass decoder similar\nto the Deliberation Network in Neural Machine\nTranslation (Xia et al., 2017), which is designed\nto improve the context coherence and knowledge\ncorrectness of the responses. The ﬁrst-pass de-\ncoder focuses on contextual coherence, while the\nsecond-pass decoder reﬁnes the result of the ﬁrst-\npass decoder by consulting the relevant document\nknowledge, and hence increases the knowledge\nrelevance and correctness. This is motivated by\nhuman cognition process. In real-world human\nconversations, people usually ﬁrst make a draft on\nhow to respond the previous utterance, and then\nconsummate the answer or even raise questions by\nconsulting background knowledge.\nWe test the effectiveness of our proposed model\non Document Grounded Conversations Dataset\n(Zhou et al., 2018). Experiment results show that\nour model is capable of generating responses of\nmore context coherence and knowledge relevance.\nSometimes document knowledge is even well used\nto guide the following conversations. Both auto-\nmatic and manual evaluations show that our model\nsubstantially outperforms the competitive base-\nlines.\nOur contributions are as follows:\n•We build a novel Incremental Transformer\nto incrementally encode multi-turn utterances\nwith document knowledge together.\n•We are the ﬁrst to apply a two-pass decoder\nto generate responses for document grounded\nconversations. Two decoders focus on con-\ntext coherence and knowledge correctness re-\nspectively.\n2 Approach\n2.1 Problem Statement\nOur goal is to incorporate the relevant doc-\nument knowledge into multi-turn conversations.\nUtterance k-1\nUtterance k-2\nUtterance k\nDocument k-2\nIncremental\nTransformer\nIncremental\nTransformer\nIncremental\nTransformer\nSecond-pass Decoder\nSelf-Attentive \nEncoder\nSelf-Attentive \nEncoder\nDocument k-1\nSelf-Attentive \nEncoder\nDocument k\nSelf-Attentive\nEncoder\nŏ ŏ\nŏ\nUtterance k+1\nFirst-pass Decoder\nDocument k+1\nSelf-Attentive\nEncoder\nFirst-pass output\nSelf-Attentive\nEncoder\nDeliberation Decoder\nIncremental Transformer \nEncoder\nFigure 1: The framework of Incremental Transformer\nwith Deliberation Decoder for Document Grounded\nConversations.\nFormally, let U = u(1),..., u(k),..., u(K) be a\nwhole conversation composed of K utterances.\nWe use u(k) = u(k)\n1 ,...,u (k)\ni ,...,u (k)\nI to denote\nthe k-th utterance containing I words, where u(k)\ni\ndenotes the i-th word in the k-th utterance. For\neach utterance u(k), likewise, there is a speciﬁed\nrelevant document s(k) = s(k)\n1 ,...,s (k)\nj ,...,s (k)\nJ ,\nwhich represents the document related to the k-\nth utterance containing J words. We deﬁne the\ndocument grounded conversations task as gen-\nerating a response u(k+1) given its related doc-\nument s(k+1) and previous k utterances U≤k\nwith related documents S≤k, where U≤k =\nu(1),..., u(k) and S≤k = s(1),..., s(k). Note that\ns(k),s(k+1),..., s(k+n) may be the same.\nTherefore, the probability to generate the re-\nsponse u(k+1) is computed as:\nP(u(k+1)|U≤k,S≤k+1; θ)\n= ∏I\ni=1 P(uk+1\ni |U≤k,S≤k+1,u(k+1)\n<i ; θ)\n(1)\nwhere u(k+1)\n<i = u(k+1)\n1 ,...,u (k+1)\ni−1 .\n2.2 Model Description\nFigure 1 shows the framework of the proposed\nIncremental Transformer with Deliberation De-\nUtterance\nEmbedding\nKnowledge\nAttention\nSelf-Attention\nContext\nAttention\nFeed-Forward\nTarget\nEmbedding\nSelf-Attention\nContext\nAttention\nUtterance\nAttention\nFeed-Forward\nTarget\nEmbedding\nSelf-Attention\nKnowledge\nAttention\nFirst-Pass\nAttention\nFeed-Forward\nDocument/\nUtterance\nEmbedding\nFeed-Forward\nSelf-Attention\nTarget\nEmbedding\nSelf-Attention\nContext\nAttention\nKnowledge\nAttention\nFeed-Forward\n(a) (c) (d) (e)\nUtterance\nEmbedding\nKnowledge\nAttention\nSelf-Attention\nFeed-Forward\n(b)\nSoftmaxSoftmax Softmax\n(1) (2)\nFigure 2: (1) Detailed architecture of model components. (a) The Self-Attentive Encoder(SA). (b) Incremental\nTransformer (ITE). (c) Deliberation Decoder (DD). (2) Simpliﬁed version of our proposed model used to verify\nthe validity of our proposed Incremental Transformer Encoder and Deliberation Decoder. (d) Knowledge-Attention\nTransformer(KAT). (e) Context-Knowledge-Attention Decoder (CKAD).\ncoder. Please refer to Figure 2 (1) for more details.\nIt consists of three components:\n1) Self-Attentive Encoder (SA) (in orange) is\na transformer encoder as described in (Vaswani\net al., 2017), which encodes the document knowl-\nedge and the current utterance independently.\n2) Incremental Transformer Encoder (ITE) (on\nthe top) is a uniﬁed transformer encoder which en-\ncodes multi-turn utterances with knowledge repre-\nsentation using an incremental encoding scheme.\nThis module takes previous utterancesu(i) and the\ndocument s(i)’s SA representation as input, and\nuse attention mechanism to incrementally build up\nthe representation of relevant context and docu-\nment knowledge.\n3) Deliberation Decoder (DD) (on the bottom)\nis a two-pass uniﬁed transformer decoder for bet-\nter generating the next response. The ﬁrst-pass de-\ncoder takes current utteranceu(k)’s SA representa-\ntion and ITE output as input, and mainly relies on\nconversation context for response generation. The\nsecond-pass decoder takes the SA representation\nof the ﬁrst pass result and the relevant document\ns(k+1)’s SA representation as input, and uses doc-\nument knowledge to further reﬁne the response.\nSelf-Attentive Encoder\nAs document knowledge often includes several\nsentences, it’s important to capture long-range\ndependencies and identify relevant information.\nWe use multi-head self-attention (Vaswani et al.,\n2017) to compute the representation of document\nknowledge.\nAs shown in Figure 2 (a), we use a self-\nattentive encoder to compute the representation\nof the related document knowledge s(k). The in-\nput (In(k)\ns ) of the encoder is a sequence of docu-\nment words embedding with positional encoding\nadded.(Vaswani et al., 2017):\nIn(k)\ns = [s(k)\n1 ,..., s(k)\nJ ] (2)\ns(k)\nj = esj + PE(j) (3)\nwhere esj is the word embedding of s(k)\nj and\nPE(·) denotes positional encoding function.\nThe Self-Attentive encoder contains a stack of\nNx identical layers. Each layer has two sub-\nlayers. The ﬁrst sub-layer is a multi-head self-\nattention ( MultiHead) (Vaswani et al., 2017).\nMultiHead(Q,K,V) is a multi-head attention\nfunction that takes a query matrix Q, a key ma-\ntrix K, and a value matrix V as input. In cur-\nrent case, Q = K = V. That’s why it’s called\nself-attention. And the second sub-layer is a sim-\nple, position-wise fully connected feed-forward\nnetwork ( FFN). This FFN consists of two lin-\near transformations with a ReLU activation in be-\ntween. (Vaswani et al., 2017).\nA(1) = MultiHead(In(k)\ns ,In(k)\ns ,In(k)\ns ) (4)\nD(1) = FFN(A(1)) (5)\nFFN(x) = max(0,xW1 + b1)W2 + b2 (6)\nwhere A(1) is the hidden state computed by multi-\nhead attention at the ﬁrst layer, D(1) denotes the\nrepresentation of s(k) after the ﬁrst layer. Note\nthat residual connection and layer normalization\nare used in each sub-layer, which are omitted in\nthe presentation for simplicity. Please refer to\n(Vaswani et al., 2017) for more details.\nFor each layer, repeat this process:\nA(n) = MultiHead(D(n−1),D(n−1),D(n−1))\n(7)\nD(n) = FFN(A(n)) (8)\nwhere n= 1,...,N s and D(0) = In(k)\ns .\nWe use SAs(·) to denote this whole process:\nd(k) = D(Nx) = SAs(s(k)) (9)\nwhere d(k) is the ﬁnal representation for the docu-\nment knowledge s(k).\nSimilarly, for each utterance u(k), we use\nIn(k)\nu = [u(k)\n1 ,..., u(k)\nI ] to represent the sequence\nof the position-aware word embedding. Then the\nsame Self-Attentive Encoder is used to compute\nthe representation of current utterance u(k), and\nwe use SAu(u(k)) to denote this encoding result.\nThe Self-Attentive Encoder is also used to encode\nthe document s(k+1) and the ﬁrst pass decoding re-\nsults in the second pass of the decoder. Note that\nSAs and SAu have the same architecture but dif-\nferent parameters. More details about this will be\nmentioned in the following sections.\nIncremental Transformer Encoder\nTo encode multi-turn document grounded ut-\nterances effectively, we design an Incremental\nTransformer Encoder. Incremental Transformer\nuses multi-head attention to incorporate document\nknowledge and context into the current utterance’s\nencoding process. This process can be stated re-\ncursively as follows:\nc(k) = ITE(c(k−1),d(k),In(k)\nu ) (10)\nwhere ITE(·) denotes the encoding function, c(k)\ndenotes the context state after encoding utterance\nu(k), c(k−1) is the context state after encoding last\nutterance u(k−1), d(k) is the representation of doc-\nument s(k) and In(k)\nu is the embedding of current\nutterance u(k).\nAs shown in Figure 2 (b), we use a stack of Nu\nidentical layers to encodeu(k). Each layer consists\nof four sub-layers. The ﬁrst sub-layer is a multi-\nhead self-attention:\nB(n) = MultiHead(C(n−1),C(n−1),C(n−1))\n(11)\nwhere n = 1,...,N u, C(n−1) is the output of the\nlast layer and C(0) = In(k)\nu . The second sub-layer\nis a multi-head knowledge attention:\nE(n) = MultiHead(B(n),d(k),d(k)) (12)\nThe third sub-layer is a multi-head context atten-\ntion:\nF(n) = MultiHead(E(n),c(k−1),c(k−1)) (13)\nwhere c(k−1) is the representation of the previous\nutterances. That’s why we called the encoder ”In-\ncremental Transformer”. The fourth sub-layer is\na position-wise fully connected feed-forward net-\nwork:\nC(n) = FFN(F(n)) (14)\nWe use c(k) to denote the ﬁnal representation at\nNu-th layer:\nc(k) = C(Nu) (15)\nDeliberation Decoder\nMotivated by the real-world human cognitive pro-\ncess, we design a Deliberation Decoder contain-\ning two decoding passes to improve the knowledge\nrelevance and context coherence. The ﬁrst-pass\ndecoder takes the representation of current utter-\nance SAu(u(k)) and context c(k) as input and fo-\ncuses on how to generate responses contextual co-\nherently. The second-pass decoder takes the rep-\nresentation of the ﬁrst-pass decoding results and\nrelated document s(k+1) as input and focuses on\nincreasing knowledge usage and guiding the fol-\nlowing conversations within the scope of the given\ndocument.\nWhen generating the i-th response word u(k+1)\ni ,\nwe have the generated words u(k+1)\n<i as input\n(Vaswani et al., 2017). We use In(k+1)\nr to denote\nthe matrix representation of u(k+1)\n<i as following:\nIn(k+1)\nr = [u(k+1)\n0 ,u(k+1)\n1 ,..., u(k+1)\ni−1 ] (16)\nwhere u(k+1)\n0 is the vector representation of\nsentence-start token.\nAs shown in Figure 2 (c), the Deliberation\nDecoder consists of a ﬁrst-pass decoder and a\nsecond-pass decoder. These two decoders have\nthe same architecture but different input for sub-\nlayers. Both decoders are composed of a stack\nof Ny identical layers. Each layer has four sub-\nlayers. For the ﬁrst-pass decoder, the ﬁrst sub-\nlayer is a multi-head self-attention:\nG(n)\n1 = MultiHead(R(n−1)\n1 ,R(n−1)\n1 ,R(n−1)\n1 )\n(17)\nwhere n = 1,...,N y, R(n−1)\n1 is the output of the\nprevious layer, and R(0)\n1 = In(k+1)\nr . The second\nsub-layer is a multi-head context attention:\nH(n)\n1 = MultiHead(G(n)\n1 ,c(k),c(k)) (18)\nwhere c(k) is the representation of context u≤k.\nThe third sub-layer is a multi-head utterance at-\ntention:\nM(n)\n1 = MultiHead(H(n)\n1 , SAu(u(k)),\nSAu(u(k)))\n(19)\nwhere SAu(·) is a Self-Attentive Encoder which\nencodes latest utterance u(k). Eq. (18) mainly en-\ncodes the context and document knowledge rele-\nvant to the latest utterance, while Eq. (19) encodes\nthe latest utterance directly. We hope optimal per-\nformance can be achieved by combining both.\nThe fourth sub-layer is a position-wise fully\nconnected feed-forward network:\nR(n)\n1 = FFN(M(n)\n1 ) (20)\nAfter Ny layers, we use softmax to get the words\nprobabilities decoded by ﬁrst-pass decoder:\nP(ˆ u(k+1)\n(1) ) = softmax(R(Ny)\n1 ) (21)\nwhere ˆ u(k+1)\n(1) is the response decoded by the ﬁrst-\npass decoder. For second-pass decoder:\nG(n)\n2 = MultiHead(R(n−1)\n2 ,R(n−1)\n2 ,R(n−1)\n2 )\n(22)\nH(n)\n2 = MultiHead(G(n)\n2 ,d(k+1),d(k+1)) (23)\nM(n)\n2 = MultiHead(H(n)\n2 , SAu(ˆ u(k+1)\n(1) ),\nSAu(ˆ u(k+1)\n(1) ))\n(24)\nR(n)\n2 = FFN(M(n)\n2 ) (25)\nP(ˆ u(k+1)\n(2) ) = softmax(R(Ny)\n2 ) (26)\nwhere R(n−1)\n2 is the counterpart to R(n−1)\n1 in pass\ntwo decoder, referring to the output of the previ-\nous layer. d(k+1) is the representation of docu-\nment s(k+1) using Self-Attentive Encoder, ˆ u(k+1)\n(2)\nis the output words after the second-pass decoder.\nTraining\nIn contrast to the original Deliberation Network\n(Xia et al., 2017), where they propose a com-\nplex joint learning framework using Monte Carlo\nMethod, we minimize the following loss as Xiong\net al. (2018) do:\nLmle = Lmle1 + Lmle2 (27)\nLmle1 = −\nK∑\nk=1\nI∑\ni=1\nlog P(ˆ u(k+1)\n(1)i ) (28)\nLmle2 = −\nK∑\nk=1\nI∑\ni=1\nlog P(ˆ u(k+1)\n(2)i ) (29)\n3 Experiments\n3.1 Dataset\nWe evaluate our model using the Document\nGrounded Conversations Dataset (Zhou et al.,\n2018). There are 72922 utterances for training,\n3626 utterances for validation and 11577 utter-\nances for testing. The utterances can be either ca-\nsual chats or document grounded. Note that we\nconsider consequent utterances of the same per-\nson as one utterance. For example, we consider A:\nHello! B: Hi! B: How’s it going? as A: Hello!\nB: Hi! How’s it going? . And there is a related\ndocument given for every several consequent ut-\nterances, which may contain movie name, casts,\nintroduction, ratings, and some scenes. The aver-\nage length of documents is about 200. Please refer\nto (Zhou et al., 2018) for more details.\n3.2 Baselines\nWe compare our proposed model with the fol-\nlowing state-of-the-art baselines:\nModels not using document knowledge:\nSeq2Seq: A simple encoder-decoder model\n(Shang et al., 2015; Vinyals and Le, 2015) with\nglobal attention (Luong et al., 2015). We concate-\nnate utterances context to a long sentence as input.\nHRED: A hierarchical encoder-decoder model\n(Serban et al., 2016), which is composed of\na word-level LSTM for each sentence and a\nsentence-level LSTM connecting utterances.\nTransformer: The state-of-the-art NMT model\nbased on multi-head attention (Vaswani et al.,\n2017). We concatenate utterances context to a\nlong sentence as its input.\nModels using document knowledge:\nSeq2Seq (+knowledge) and HRED (+knowl-\nedge) are based on Seq2Seq and HRED respec-\ntively. They both concatenate document knowl-\nedge representation and last decoding output em-\nbedding as input when decoding. Please refer to\n(Zhou et al., 2018) for more details.\nWizard Transformer : A Transformer-based\nmodel for multi-turn open-domain dialogue with\nunstructured text facts (Dinan et al., 2018). It con-\ncatenates context utterances and text facts to a long\nKnowledge Context\nModel PPL BLEU(%) Fluency Relevance Coherence\nSeq2Seq without knowledge 80.93 0.38 1.62 0.18 0.54\nHRED without knowledge 80.84 0.43 1.25 0.18 0.30\nTransformer without knowledge 87.32 0.36 1.60 0.29 0.67\nSeq2Seq (+knowledge) 78.47 0.39 1.50 0.22 0.61\nHRED (+knowledge) 79.12 0.77 1.56 0.35 0.47\nWizard Transformer 70.30 0.66 1.62 0.47 0.56\nITE+DD(ours) 15.11 0.95 1.67 0.56 0.90\nITE+CKAD(ours) 64.97 0.86 1.68 0.50 0.82\nKAT(ours) 65.36 0.58 1.58 0.33 0.78\nTable 1: Automatic evaluation and manual evaluation results for baselines and our proposed models.\nKnowledge Context\nModel Relevance(%) Coherence(%)\nWizard 64/25/11 58/28/14\nITE+CKAD 67/16/17 40/37/23\nITE+DD 64/16/20 38/34/28\nTable 2: The percent(%) of score (0/1/2) of Knowledge\nRelevance and Context Coherence for Wizard Trans-\nformer, ITE+CKAD and ITE+DD.\nsequence as input. We replace the text facts with\ndocument knowledge.\nHere, we also conduct an ablation study to il-\nlustrate the validity of our proposed Incremental\nTransformer Encoder and Deliberation Decoder.\nITE+CKAD: It uses Incremental Trans-\nformer Encoder (ITE) as encoder and Context-\nKnowledge-Attention Decoder (CKAD) as shown\nin Figure 2 (e). This setup is to test the validity of\nthe deliberation decoder.\nKnowledge-Attention Transformer (KAT) :\nAs shown in Figure 2 (d), the encoder of this\nmodel is a simpliﬁed version of Incremental\nTransformer Encoder (ITE), which doesn’t have\ncontext-attention sub-layer. We concatenate ut-\nterances context to a long sentence as its in-\nput. The decoder of the model is a simpliﬁed\nContext-Knowledge-Attention Decoder (CKAD).\nIt doesn’t have context-attention sub-layer either.\nThis setup is to test how effective the context has\nbeen exploited in the full model.\n3.3 Experiment Setup\nWe use OpenNMT-py1 (Klein et al., 2017) as\nthe code framework2. For all models, the hidden\nsize is set to 512. For rnn-based models (Seq2Seq,\nHRED), 3-layer bidirectional LSTM (Hochreiter\n1https://github.com/OpenNMT/OpenNMT-py\n2The code and models are available at https://\ngithub.com/lizekang/ITDD\nand Schmidhuber, 1997) and 1-layer LSTM is ap-\nplied for encoder and decoder respectively. For\ntransformer-based models, the layers of both en-\ncoder and decoder are set to 3. The number of\nattention heads in multi-head attention is 8 and\nthe ﬁlter size is 2048. The word embedding is\nshared by utterances, knowledge and generated re-\nsponses. The dimension of word embedding is set\nto 512 empirically. We use Adam (Kingma and\nBa, 2014) for optimization. When decoding, beam\nsize is set to 5. We use the previous three utter-\nances and its related documents as input.\n3.4 Evaluation Metrics\nAutomatic Evaluation : We adopt perplexity\n(PPL) and BLEU (Papineni et al., 2002) to au-\ntomatically evaluate the response generation per-\nformance. Models are evaluated using perplexity\nof the gold response as described in (Dinan et al.,\n2018). Lower perplexity indicates better perfor-\nmance. BLEU measures n-gram overlap between\na generated response and a gold response. How-\never, since there is only one reference for each re-\nsponse and there may exist multiple feasible re-\nsponses, BLEU scores are extremely low. We\ncompute BLEU score by the multi-bleu.perl3\nManual Evaluation: Manual evaluations are es-\nsential for dialogue generation. We randomly\nsampled 30 conversations containing 606 utter-\nances from the test set and obtained 5454 utter-\nances from the nine models. We have annotators\nscore these utterances given its previous utterances\nand related documents. We deﬁned three metrics\n- ﬂuency, knowledge relevance (Liu et al., 2018)\nand context coherence for manual evaluation. All\nthese metrics are scored 0/1/2.\nﬂuency: Whether the response is natural and\n3https://github.com/google/seq2seq/\nblob/master/bin/tools/multi-bleu.perl\nDocument\nmovienamedespicable me... cast: steve carell as gru... rotten tomatoes: 81%... it is the debut\nﬁlm of illumination entertainment. ... gru, a supervillain, is disheartened when an unknown\nsupervillain steals the great pyramid of giza. gru, with the assistance of his colleague dr. Nefario\nand his minions, resolves to one-up this mystery rival by shrinking and stealing the moon. as it\nwould be costly to steal the moon,gru seeks a loan from the bank of evil. ... upon learning that\nan up-and-comingvillainnamed vector was responsible for the pyramid theft ...\nUtterance 1 what do you think aboutdespicable me?\nITE+DD i think it is an awesome movie,what about you?\nWizard i think it’s a great movie!\nUtterance 2 i rather liked it. it wasn’t what i was expecting but it was still good.\nITE+DD yeah, it makes me laugh, even when they make a movie about it.\nWizard it’s a great movie for kids and adults.\nUtterance 3 i like it too.funny and heart warming at the same time.\nITE+DD i think it’s interesting thatgru decided to take a loan from the bank of evil.\nWizard i think they should have made a sequel.\nUtterance 4 gru was a really interesting character, too. i liked steve carell and the voice he\nused for gru.\nITE+DD yeah, now that you mention it, what did you think of theminions?\nWizard i think the entire cast was great.\nUtterance 5 yes, he did an excellent job portraying the character.81% in ratingsi think is great.\nITE+DD yes, 81% is a great score!\nWizard i also liked thevillainof the movie.\nTable 3: Responses generated by Incremental Transformer with Deliberation Decoder( ITE+DD) and the Wizard\nTransformer(Wizard). These utterances are from continuous dialogues. ITE+DD and Wizard generate responses\nusing context utterances and a given document. Note that Utterance i+ 1is the gold response of Utterance i.\nﬂuent. Score 0 represents not ﬂuent and incom-\nprehensible; 1 represents partially ﬂuent but still\ncomprehensible; 2 represents totally ﬂuent.\nknowledge relevance : Whether the response\nuses relevant and correct knowledge. Score 0 rep-\nresents no relevant knowledge; 1 represents con-\ntaining relevant knowledge but not correct; 2 rep-\nresents containing relevant knowledge and correct.\ncontext coherence : Whether the response is\ncoherent with the context and guides the follow-\ning utterances. Score 0 represents not coherent or\nleading the dialogue to an end; 1 represents coher-\nent with the utterance history but not guiding the\nfollowing utterances; 2 represents coherent with\nutterance history and guiding the next utterance.\n3.5 Experimental Results\nTable 1 shows the automatic and manual evalu-\nation results for both the baseline and our models.\nIn manual evaluation, among baselines, Wizard\nTransformer and RNN without knowledge have\nthe highest ﬂuency of 1.62 and Wizard obtains the\nhighest knowledge relevance of 0.47 while Trans-\nformer without knowledge gets the highest context\ncoherence of 0.67. For all models, ITE+CKAD\nobtains the highest ﬂuency of 1.68 and ITE+DD\nhas the highest Knowledge Relevance of 0.56 and\nhighest Context Coherence of 0.90.\nIn automatic evaluation, our proposed model\nhas lower perplexity and higher BLEU scores than\nbaselines. For BLEU, HRED with knowledge ob-\ntains the highest BLEU score of 0.77 among the\nbaselines. And ITE+DD gets 0.95 BLEU score,\nwhich is the highest among all the models. For\nperplexity, Wizard Transformer obtains the low-\nest perplexity of 70.30 among baseline models and\nITE+DD has remarkably lower perplexity of 15.11\nthan all the other models. A detailed analysis is in\nSection 3.6.\n3.6 Analysis and Discussion\nTo our surprise, ITE+DD reaches an extremely\nlow ground truth perplexity. We ﬁnd that the\nground truth perplexity after the ﬁrst-pass decod-\ning is only similar to the ITE+CKAD. It shows\nthat the second-pass decoder utilizes the docu-\nment knowledge well, and dramatically reduced\nthe ground truth perplexity.\nAs shown in Table 2, ITE+DD has a higher per-\ncent of score 2 both on Knowledge Relevance and\nID Utterance Two-pass Responses\n1 I think rachel mcadams\nhad an even better role\nas regina george\nhowever! would you\nagree?\ni’m not a fan of\nkristen bell, but i think\nshe did a great job.\ni’m not a huge fan of\nrachel mcadams, but\nhe did a great job.\n2 yeah, I guess that’s\nalways worth it, and a\ntruce was made as well.\nyeah, not only does\nshe reconcile with the\nplastics.\nyeah, she reconciles\nwithjanis, damienand\naaron.\n3 i liked the scene where\nbuzz thinks he’s a big\nshot hero but then the\ncamera reveals him to\nbe a tiny toy.\ni think that’s one of\nthe best scenes in the\nmovie.\noh, i think that is\nwhat makes the movie\nunique as well. have\nyou seen any of the\nother pixar movies?\nTable 4: Examples of the two pass decoding. Under-\nlined texts are the differences between two results. For\neach case, the ﬁrst-pass response is on the top.\nContext Coherence than ITE+CKAD. This result\nalso demonstrates that Deliberation Decoder can\nimprove the knowledge correctness and guide the\nfollowing conversations better.\nAlthough the perplexity of ITE+CKAD is only\nslightly better than KAT, the BLEU score, Flu-\nency, Knowledge Relevance and Context Coher-\nence of ITE+CKAD all signiﬁcantly outperform\nthose of KAT model, which indicates that Incre-\nmental Transformer can deal with multi-turn doc-\nument grounded conversations better.\nWizard Transformer has a great performance\non Knowledge Relevance only second to our pro-\nposed Incremental Transformer. However, its\nscore on Context Coherence is lower than some\nother baselines. As shown in Table 2, Wizard\nTransformer has Knowledge Relevance score 1 re-\nsults twice more than score 2 results, which indi-\ncates that the model tends to generate responses\nwith related knowledge but not correct. And\nthe poor performance on Context Coherence also\nshows Wizard Transformer does not respond to the\nprevious utterance well. This shows the limitation\nof representing context and document knowledge\nby simple concatenation.\n3.7 Case Study\nIn this section, we list some examples to show\nthe effectiveness of our proposed model.\nTable 3 lists some responses generated by our\nproposed Incremental Transformer with Delibera-\ntion Decoder (ITE+DD) and Wizard Transformer\n(which achieves overall best performance among\nbaseline models). Our proposed model can gener-\nate better responses than Wizard Transformer on\nknowledge relevance and context coherence.\nTo demonstrate the effectiveness of the two-\npass decoder, we compare the results from the\nﬁrst-pass decoding and the second-pass decoding.\nTable 4 shows the improvement after the second-\npass decoding. For Case 1, the second-pass de-\ncoding result revises the knowledge error in the\nﬁrst-pass decoding result. For Case 2, the second-\npass decoder uses more detailed knowledge than\nthe ﬁrst-pass one. For Case 3, the second-pass de-\ncoder cannot only respond to the previous utter-\nance but also guide the following conversations by\nasking some knowledge related questions.\n4 Related Work\nThe closest work to ours lies in the area of open-\ndomain dialogue system incorporating unstruc-\ntured knowledge. Ghazvininejad et al. (2018)\nuses an extended Encoder-Decoder where the de-\ncoder is provided with an encoding of both the\ncontext and the external knowledge. Parthasarathi\nand Pineau (2018) uses an architecture containing\na Bag-of-Words Memory Network fact encoder\nand an RNN decoder. Dinan et al. (2018) com-\nbines Memory Network architectures to retrieve,\nread and condition on knowledge, and Trans-\nformer architectures to provide text representa-\ntion and generate outputs. Different from these\nworks, we greatly enhance the Transformer ar-\nchitectures to handle the document knowledge in\nmulti-turn dialogue from two aspects: 1) using at-\ntention mechanism to combine document knowl-\nedge and context utterances; and 2) exploiting in-\ncremental encoding scheme to encode multi-turn\nknowledge aware conversations.\nOur work is also inspired by several works in\nother areas. Zhang et al. (2018) introduces docu-\nment context into Transformer on document-level\nNeural Machine Translation (NMT) task. Guan\net al. (2018) devises the incremental encoding\nscheme based on rnn for story ending genera-\ntion task. In our work, we design an Incremental\nTransformer to achieve a knowledge-aware con-\ntext representation using an incremental encoding\nscheme. Xia et al. (2017) ﬁrst proposes Deliber-\nation Network based on rnn on NMT task. Our\nDeliberation Decoder is different in two aspects:\n1) We clearly devise the two decoders targeting\ncontext and knowledge respectively; 2) Our sec-\nond pass decoder directly ﬁne tunes the ﬁrst pass\nresult, while theirs uses both the hidden states and\nresults from the ﬁrst pass.\n5 Conclusion and Future Work\nIn this paper, we propose an Incremental Trans-\nformer with Deliberation Decoder for the task of\nDocument Grounded Conversations. Through an\nincremental encoding scheme, the model achieves\na knowledge-aware and context-aware conversa-\ntion representation. By imitating the real-world\nhuman cognitive process, we propose a Delibera-\ntion Decoder to optimize knowledge relevance and\ncontext coherence. Empirical results show that the\nproposed model can generate responses with much\nmore relevance, correctness, and coherence com-\npared with the state-of-the-art baselines. In the fu-\nture, we plan to apply reinforcement learning to\nfurther improve the performance.\n6 Acknowledgments\nThis work is supported by 2018 Tencent Rhino-\nBird Elite Training Program, National Natural\nScience Foundation of China (NO. 61662077,\nNO.61876174) and National Key R&D Program\nof China (NO.YS2017YFGH001428). We sin-\ncerely thank the anonymous reviewers for their\nthorough reviewing and valuable suggestions.\nReferences\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , volume 1,\npages 1870–1879.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley. 2018. A knowledge-grounded neural\nconversation model. In Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence.\nJian Guan, Yansen Wang, and Minlie Huang. 2018.\nStory ending generation with incremental encod-\ning and commonsense knowledge. arXiv preprint\narXiv:1808.10113.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nHsin-Yuan Huang, Eunsol Choi, and Wen-tau Yih.\n2018. Flowqa: Grasping ﬂow in history for con-\nversational machine comprehension. arXiv preprint\narXiv:1810.06683.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander M. Rush. 2017. Open-\nNMT: Open-source toolkit for neural machine trans-\nlation. In Proc. ACL.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting objec-\ntive function for neural conversation models. InPro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n110–119.\nShuman Liu, Hongshen Chen, Zhaochun Ren, Yang\nFeng, Qun Liu, and Dawei Yin. 2018. Knowledge\ndiffusion for neural dialogue generation. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), volume 1, pages 1489–1498.\nHao-Tong Ye Kai-Ling Lo and Shang-Yu Su Yun-Nung\nChen. 2019. Knowledge-grounded response gen-\neration with deep attentional latent-variable model.\nThirty-Third AAAI Conference on Artiﬁcial Intelli-\ngence.\nThang Luong, Hieu Pham, and Christopher D Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412–1421.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung.\n2018. Mem2seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog sys-\ntems. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), volume 1, pages 1468–1478.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei\njing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. pages 311–318.\nPrasanna Parthasarathi and Joelle Pineau. 2018. Ex-\ntending neural generative conversational model us-\ning external knowledge sources. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 690–695.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you dont know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), volume 2, pages\n784–789.\nSiva Reddy, Danqi Chen, and Christopher D Manning.\n2018. Coqa: A conversational question answering\nchallenge. arXiv preprint arXiv:1808.07042.\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In Thirtieth AAAI\nConference on Artiﬁcial Intelligence.\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015.\nNeural responding machine for short-text conversa-\ntion. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), vol-\nume 1, pages 1577–1586.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nOriol Vinyals and Quoc Le. 2015. A neural conversa-\ntional model. arXiv preprint arXiv:1506.05869.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In Advances in Neural Information Process-\ning Systems, pages 1784–1794.\nHao Xiong, Zhongjun He, Hua Wu, and Haifeng Wang.\n2018. Modeling coherence for discourse neural ma-\nchine translation. arXiv preprint arXiv:1811.05683.\nSemih Yavuz, Abhinav Rastogi, Guan-lin Chao, Dilek\nHakkani-T¨ur, and Amazon Alexa AI. 2018. Deep-\ncopy: Grounded response generation with hierarchi-\ncal pointer networks. Advances in Neural Informa-\ntion Processing Systems.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018. Qanet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. arXiv preprint arXiv:1804.09541.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 533–542.\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\nBlack. 2018. A dataset for document grounded con-\nversations. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Pro-\ncessing, pages 708–713.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7845453023910522
    },
    {
      "name": "Deliberation",
      "score": 0.6944426894187927
    },
    {
      "name": "Transformer",
      "score": 0.6596700549125671
    },
    {
      "name": "Correctness",
      "score": 0.6038439869880676
    },
    {
      "name": "Exploit",
      "score": 0.5716558694839478
    },
    {
      "name": "Grounded theory",
      "score": 0.5222783088684082
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.47791725397109985
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3382067084312439
    },
    {
      "name": "Natural language processing",
      "score": 0.33647388219833374
    },
    {
      "name": "Qualitative research",
      "score": 0.13228610157966614
    },
    {
      "name": "Sociology",
      "score": 0.07886448502540588
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210090176",
      "name": "Institute of Computing Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I87182695",
      "name": "Universidad del Noreste",
      "country": "MX"
    }
  ]
}