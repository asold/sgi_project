{
  "title": "Exploring the frontiers of LLMs in psychological applications: a comprehensive review",
  "url": "https://openalex.org/W4412354832",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3212336017",
      "name": "Luoma Ke",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2104600475",
      "name": "Song Tong",
      "affiliations": [
        "Beijing Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A1963231033",
      "name": "Peng Cheng",
      "affiliations": [
        "China Research Institute for Science Popularization"
      ]
    },
    {
      "id": "https://openalex.org/A2270087390",
      "name": "Kaiping Peng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A3212336017",
      "name": "Luoma Ke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104600475",
      "name": "Song Tong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1963231033",
      "name": "Peng Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2270087390",
      "name": "Kaiping Peng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2977128309",
    "https://openalex.org/W4400678448",
    "https://openalex.org/W4382583857",
    "https://openalex.org/W4360888837",
    "https://openalex.org/W4378718176",
    "https://openalex.org/W4323568782",
    "https://openalex.org/W4396608701",
    "https://openalex.org/W4296154596",
    "https://openalex.org/W4386981810",
    "https://openalex.org/W4313262066",
    "https://openalex.org/W4400608537",
    "https://openalex.org/W4402580984",
    "https://openalex.org/W4388000629",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4379933116",
    "https://openalex.org/W4318919287",
    "https://openalex.org/W4386790712",
    "https://openalex.org/W4386790659",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4386840320",
    "https://openalex.org/W4368755092",
    "https://openalex.org/W4381163899",
    "https://openalex.org/W6977310008",
    "https://openalex.org/W4386893393",
    "https://openalex.org/W2101714982",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W4327518740",
    "https://openalex.org/W4353113083",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4393153538",
    "https://openalex.org/W4392103729",
    "https://openalex.org/W4385459268",
    "https://openalex.org/W4393231947",
    "https://openalex.org/W4388202310",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4382240547",
    "https://openalex.org/W4390086837",
    "https://openalex.org/W4393987306",
    "https://openalex.org/W2137014522",
    "https://openalex.org/W4386907387",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W4367844176",
    "https://openalex.org/W4380763235",
    "https://openalex.org/W4402193006",
    "https://openalex.org/W4385452929",
    "https://openalex.org/W4361020574",
    "https://openalex.org/W4399320026",
    "https://openalex.org/W4387378202",
    "https://openalex.org/W4385066501",
    "https://openalex.org/W2594908799",
    "https://openalex.org/W4380886221",
    "https://openalex.org/W4387947500",
    "https://openalex.org/W2036735145",
    "https://openalex.org/W4385507660",
    "https://openalex.org/W4221083716",
    "https://openalex.org/W4312091691",
    "https://openalex.org/W4384155269",
    "https://openalex.org/W4386272709",
    "https://openalex.org/W4360939370",
    "https://openalex.org/W4399253720",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4366991679",
    "https://openalex.org/W4403863303",
    "https://openalex.org/W4361230825",
    "https://openalex.org/W4401518309",
    "https://openalex.org/W4307653761",
    "https://openalex.org/W4312091865",
    "https://openalex.org/W4389072967",
    "https://openalex.org/W6602046083",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4387158222",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4385988359",
    "https://openalex.org/W4379929613",
    "https://openalex.org/W4401357303",
    "https://openalex.org/W4399210824",
    "https://openalex.org/W4393267827",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W4319166659",
    "https://openalex.org/W2969341057",
    "https://openalex.org/W4298181341",
    "https://openalex.org/W4388520388",
    "https://openalex.org/W4392873825",
    "https://openalex.org/W4383501206",
    "https://openalex.org/W2113352630",
    "https://openalex.org/W4384154918",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4377942506",
    "https://openalex.org/W4360943653",
    "https://openalex.org/W2122955905",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W4390723916",
    "https://openalex.org/W4386827094",
    "https://openalex.org/W4385156310",
    "https://openalex.org/W4385708679",
    "https://openalex.org/W4386874836",
    "https://openalex.org/W4404333646",
    "https://openalex.org/W4367368990",
    "https://openalex.org/W4391996564",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W4384470461",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4307325444",
    "https://openalex.org/W4387820722",
    "https://openalex.org/W4385682029",
    "https://openalex.org/W4386910317",
    "https://openalex.org/W4379958473",
    "https://openalex.org/W4289598673",
    "https://openalex.org/W4387390364",
    "https://openalex.org/W4317757464",
    "https://openalex.org/W2177554891",
    "https://openalex.org/W4386242180",
    "https://openalex.org/W4366990161",
    "https://openalex.org/W4387665053",
    "https://openalex.org/W4283321795",
    "https://openalex.org/W4381149642",
    "https://openalex.org/W4310917376",
    "https://openalex.org/W4391484411",
    "https://openalex.org/W4391655051",
    "https://openalex.org/W2175873029",
    "https://openalex.org/W4362655418",
    "https://openalex.org/W3128912454",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4400448985",
    "https://openalex.org/W4383058631",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4385490607",
    "https://openalex.org/W4385430086",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W6609090796",
    "https://openalex.org/W6947893166",
    "https://openalex.org/W4376504356",
    "https://openalex.org/W1849277567",
    "https://openalex.org/W4387356292",
    "https://openalex.org/W6600005967",
    "https://openalex.org/W4393196641",
    "https://openalex.org/W4387635121",
    "https://openalex.org/W4361267822",
    "https://openalex.org/W4381556776",
    "https://openalex.org/W4375870056"
  ],
  "abstract": "Abstract This review explores the frontiers of large language models (LLMs) in psychological applications. Psychology has undergone several theoretical changes, and the current use of artificial intelligence (AI) and machine learning, particularly LLMs, promises to open up new research directions. We aim to provide a detailed exploration of how LLMs are transforming psychological research. We discuss the impact of LLMs across various branches of psychology—including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology—highlighting their ability to model patterns, cognition, and behavior similar to those observed in humans. Furthermore, we explore the ability of such models to generate coherent, contextually relevant text, offering innovative tools for literature reviews, hypothesis generation, experimental designs, experimental subjects, and data analysis in psychology. We emphasize the importance of addressing technical and ethical challenges, including data privacy, the ethics of using LLMs in psychological research, and the need for a deeper understanding of these models’ limitations. Researchers should use LLMs responsibly in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, this review provides a comprehensive overview of the current state of LLMs in psychology, exploring the potential benefits and challenges. We hope it can serve as a call to action for researchers to responsibly leverage LLMs’ advantages while addressing the associated risks.",
  "full_text": "Accepted: 5 June 2025 / Published online: 12 July 2025\n© The Author(s) 2025\nExtended author information available on the last page of the article\nExploring the frontiers of LLMs in psychological applications: \na comprehensive review\nLuoma Ke1 · Song Tong2,3 · Peng Cheng4 · Kaiping Peng1\nArtificial Intelligence Review (2025) 58:305\nhttps://doi.org/10.1007/s10462-025-11297-5\nAbstract\nThis review explores the frontiers of large language models (LLMs) in psychological ap -\nplications. Psychology has undergone several theoretical changes, and the current use of \nartificial intelligence (AI) and machine learning, particularly LLMs, promises to open up \nnew research directions. We aim to provide a detailed exploration of how LLMs are trans -\nforming psychological research. We discuss the impact of LLMs across various branches \nof psychology—including cognitive and behavioral, clinical and counseling, educational \nand developmental, and social and cultural psychology—highlighting their ability to mod -\nel patterns, cognition, and behavior similar to those observed in humans. Furthermore, we \nexplore the ability of such models to generate coherent, contextually relevant text, offer -\ning innovative tools for literature reviews, hypothesis generation, experimental designs, \nexperimental subjects, and data analysis in psychology. We emphasize the importance of \naddressing technical and ethical challenges, including data privacy, the ethics of using \nLLMs in psychological research, and the need for a deeper understanding of these models’ \nlimitations. Researchers should use LLMs responsibly in psychological studies, adhering \nto ethical standards and considering the potential consequences of deploying these tech -\nnologies in sensitive areas. Overall, this review provides a comprehensive overview of \nthe current state of LLMs in psychology, exploring the potential benefits and challenges. \nWe hope it can serve as a call to action for researchers to responsibly leverage LLMs’ \nadvantages while addressing the associated risks.\nKeywords Large language models (LLMs) · Machine learning · Artificial intelligence \n(AI) · Psychology · Research methods\n1 Introduction\nArtificial intelligence (AI) has a history spanning nearly seven decades, beginning with the \n1956 Dartmouth Conference. The field has recently been revolutionized with the advent \nof large language models (LLMs) such as ChatGPT, Google’s Bard, and Meta’s LLaMA. \nAmong them, GPT-4, in particular, could signify a paradigm shift given its impressive \n1 3\nL. Ke et al.\ncapabilities (e.g., solving difficult tasks in math, coding, vision, medicine, law, and psy -\nchology) (Bubeck et al. 2023), exemplifying the concept of “AI for science” (Wang et al. \n2023). LLMs mark a critical juncture in the evolution of machine learning and AI, propelled \nby their expansive size and sophisticated neural architectures that incorporate attention \nmechanisms (Vaswani et al. 2017). These models incorporate cognitive principles (Binz \nand Schulz 2023a) and exhibit emergent properties comparable to those seen in complex \nphysical systems (Wei et al. 2022). This has enhanced their ability to process and represent \nconcepts and high-level semantics (J. Li et al. 2022a) while also deepening our insights into \nhuman cognitive processes (Sejnowski 2022). In psychological applications, these develop-\nments are reshaping interactions among data, language, and the environment (De Bot et al. \n2007; Demszky et al. 2023), contributing significantly to various fields, including clinical \n(Thirunavukarasu et al. 2023), developmental (Frank 2023; Hagendorff 2023), and social \npsychology (Hardy et al. 2023; J. Zhang et al. 2023a). Moreover, LLMs have had profound \neffects on psychological research methods, offering novel approaches and tools for explora-\ntion and analysis.\n1.1 The LLM concept: from machine learning to capability emergence\nGenerative AI evolved from advances in pattern recognition capability. While convolutional \nneural networks (CNNs) excelled at recognizing objects and concepts, the next challenge \nwas to use this recognition capability for a generation. For example, if a CNN can identify \n“age” in portraits, we can use that understanding to modify “age” in any portrait. This gener-\native approach first succeeded in computer vision through models such as generative adver-\nsarial networks (Goodfellow et al. 2020) and deconvolution (Zeiler 2014), which could \ncreate realistic images based on learned patterns. The same generative principles were then \napplied to language, leading to LLMs that could generate contextually relevant text. LLMs \nrepresent a particularly significant leap in the capabilities of generative AI. These models \nare designed to process natural language text and generate contextually relevant text. LLMs \nlike GPT-4, LLaMA, Claude, and Gemini leverage the transformer architecture (Vaswani et \nal. 2017), which employs sophisticated neural networks and attention mechanisms to revo-\nlutionize natural language processing. Each model is optimized uniquely to enhance per -\nformance across diverse tasks. For instance, LLaMA focuses on efficient training processes \n(Touvron et al. 2023), Claude emphasizes safety and alignment (Li et al. 2024), and Gemini \nintegrates advanced reasoning capabilities (Rane et al. 2024). LLaMA’s open-source nature \nallows local deployment and efficient training, making it suitable for psychological studies \nneeding rapid iteration or customization, such as behavioral modeling (Binz and Schulz \n2023a. Claude, designed for safety and alignment, is less commonly used in psychology \nresearch and more oriented toward knowledge-based tasks (Li et al. 2024). GPT-4, with its \nlarge-scale parameters and broad training data, supports a wide range of tasks, including \ncognitive simulations and clinical assessments. These differences guide model selection \nbased on research needs like accessibility, task specificity, or data scale.\nWhile these models highlight the versatility of LLMs, it is essential to distinguish \nbetween specific products designed for particular interactions, such as ChatGPT for conver-\nsational applications, and the broader capabilities of LLMs that extend beyond chat inter -\nfaces to include text generation, summarization, translation, and embedding extraction. This \nrange of applications demonstrates that LLMs’ capabilities are emergent, manifesting new \n1 3\n305 Page 2 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nabilities as the model size increases. Performance improvements on log-log scales some -\ntimes experience “breaks” where unexpected capabilities emerge from complex interactions \nwithin the models (Wei et al. 2022).\nAt the heart of LLMs is the transformer architecture, a deep neural network with an atten-\ntion mechanism that efficiently processes sequential data in parallel (Vaswani et al. 2017); \nthis works in a manner somewhat similar to human brain functions. This architecture has \nrevolutionized the field of natural language processing. The self-attention mechanism of \nthe transformer architecture captures contextual relationships in textual data, allowing for \nmore sophisticated language understanding. Notably, the “large” in LLM refers to the many \nparameters and massive amounts of training data used to fine-tune these models, typically \nbillions of parameters and terabytes of text (Binz and Schulz 2023b), in addition to “master-\ning the world” (Yildirim and Paul 2023).\nThe process of large language modeling, from machine learning to the emergence of \ncompetence, can be divided into several key stages. (1) Pretraining: LLMs are pretrained on \nlarge amounts of textual data to learn intricate linguistic, syntactic, and textual structures, \nwhere the model learns to predict the next token through unsupervised learning, result -\ning in a base model that captures the statistical patterns of language (P. Liu et al. 2023b). \n(2) Alignment: Supervised learning is used to create a foundation model that can better \ninteract with users in the intended ways, which typically involves instruction tuning and \nreinforcement learning based on human feedback. After the foundation model is available, \ndomain-specific fine-tuning can adapt the model for particular applications (Liu et al. 2022). \nThis fine-tuning process ensures the model can generate contextually relevant responses and \nengage in meaningful conversations or tasks. Through these stages of development, LLMs \ndemonstrate increasingly sophisticated text-generation capabilities, including response gen-\neration, content summarization, translation, and compositional text generation (Bubeck et \nal. 2023). The ability to effectively process and represent context is a critical factor underly-\ning the observed emergence of advanced capabilities in these models. Finally, LLMs exhibit \n“observed capability emergence” when integrated into various applications and systems, \nin addition to performing tasks that require a deep understanding of language and context, \nthus often achieving human-like or superhuman performance in specific experimental tasks, \nsuch as analogical reasoning (Webb et al. 2023), creativity (Stevenson et al. 2022), and emo-\ntion recognition (Patel and Fan 2023).\nTherefore, LLMs can provide valuable insights into how such technologies can simulate \nor augment processes traditionally associated with human cognition. Specifically, LLMs \nmaintain a balance between logical processing and the use of cognitive shortcuts (heuris -\ntics), and they adapt their reasoning strategies to optimize between accuracy and effort. This \naligns with the principles of resource-rational human cognition, as discussed in dual-process \ntheory (Mukherjee & Chang 2024). For instance, LLMs generate and process natural lan -\nguage, demonstrating structural and functional parallels with certain aspects of human lin -\nguistic and cognitive mechanisms (Goertzel 2023). These parallels allow for the exploration \nof AI applications in areas such as cognitive psychology (Sartori and Orrù 2023), language \nacquisition (Jungherr 2023), and even mental health (Lamichhane 2023). Moreover, the \nstudy of LLMs contributes to our understanding of the human mind, offering a computa -\ntional perspective on language processing, decision-making (Sha et al. 2023), and learning \nmechanisms (Hendel et al. 2023). The fusion of such disciplines could drive advancements \n1 3\nPage 3 of 35 305\nL. Ke et al.\nin AI and provide a computational framework for investigating processes related to human \ncognition.\n1.2 Psychology and AI\nPsychology, as a science that explores the human mind and behavior, has undergone sig -\nnificant theoretical changes since the late nineteenth century, with psychoanalysis and \nbehaviorism extending to cognitive psychology (Hothersall and Lovett 2022). This history \nmarks a shift in the focus of psychology research, reflecting the academic trend of moving \nfrom observing behavioral manifestations to exploring in-depth psychological connotations. \nEach of these phases has led to a deepening understanding of the psycho-cognitive pro -\ncesses of human beings.\nUnderstanding human psycho-cognitive processes is therefore crucial for psychology. \nIn clinical and counseling psychology, research on cognitive psychology supports diag -\nnosing and treating psychological disorders. It deepens our understanding of the psycho -\nlogical mechanisms underlying emotions, stress, and human behavior. Psychotherapies such \nas cognitive-behavioral therapy (Hofmann et al. 2012) and psychodynamic therapy have \nbecome essential tools for promoting mental health and emotional regulation. In educational \nand developmental psychology, the development of cognitive psychology has fostered a \ndeeper understanding of the roles of perceptual and affective factors in learning processes \n(Glaser 1984), which has led to innovations in teaching methods and learning strategies. In \nsocial and cultural psychology, cognitive psychology research helps explain individuals’ \nbehaviors and mental processes in different social and cultural contexts, exploring how \ncultural differences affect cognitive patterns, values, and behavioral norms, especially in \nthe context of globalization, interaction, and integration. In social psychology, meanwhile, \ncognitive psychology research on group behavior, social influence, prejudice, and discrimi-\nnation holds great value for promoting social harmony and mutual understanding (Park and \nJudd 2005).\nAI is becoming an increasingly influential tool in psycho-cognitive research. Simon \n(1979) was among the first to recognize the potential of computational models to simulate \naspects of human cognitive processes. Currently, LLMs can process and generate human-\nlike texts and perform certain tasks in a manner similar to human cognition (Bubeck et al. \n2023). LLMs also offer a unique computational perspective for the study of human cognition. \nFor example, GPT-3 can solve vignette-based tasks similar to or better than human subjects \nand can perform rational decision-making based on descriptions, outperforming humans in \nthe multiarmed bandit task (Binz and Schulz 2023b). Furthermore, after extensive testing, \nGPT-3 is able to solve complex analogical problems at levels comparable to human perfor-\nmance, and analogical reasoning is an essential hallmark of human intelligence (Webb et \nal. 2023). Moreover, fine-tuning across multiple tasks can allow LLMs to predict human \nbehavior in previously unseen tasks—that is, LLMs can be adapted to general-purpose cog-\nnitive models (Binz and Schulz 2023a), potentially opening up new research directions that \ncould transform cognitive psychology and behavioral science in general.\nNewell (1990) offered a structured framework for analyzing human behavior, categoriz-\ning cognitive and behavioral processes into four distinct layers based on their time scales \n(Fig. 1a). At the biological level, the focus is on physiological and neural processes occurring \nat rapid time scales, ranging from milliseconds to one second. This level can include neural \n1 3\n305 Page 4 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nresponses and sensory processing, which form the foundation of human cognition. The cog-\nnitive level pertains to mechanisms such as attention, perception, and short-term memory, \nwhich operate at intermediate time scales, typically between one second and one minute. \nThese processes enable fundamental cognitive functions. At the rational level, the frame -\nwork considers more complex cognitive activities such as problem-solving, planning, and \ndecision-making. Such activities occur over longer time scales, spanning several minutes to \na few hours, and involve sustained cognitive engagement. Finally, the social level considers \nbehaviors shaped by social interaction and cultural influence, operating at the longest time \nscale, ranging from hours to days or longer. This level concerns the effects of social com -\nmunication, group behavior, and cultural influences on cognition. It underscores the multi -\nfaceted nature of human behavior, highlighting the relationship between rapid physiological \nprocesses and the more prolonged, socially influenced aspects of human cognition. Figure 1 \nintegrates this framework by mapping psychological domains (e.g., cognitive, social) onto \nthese timescales, demonstrating LLMs’ ability to simulate behaviors—from short-term pro-\ncesses like memory retrieval to long-term phenomena like cultural trends. Emergent prop -\nerties (e.g., cognitive simulation) connect these domains to practical research tools (e.g., \nstimuli generation), with bidirectional influence refining both applications and properties.\nTherefore, by analyzing LLM application across these four levels (Fig. 1a), it is possible \nto further explore their potential for modeling and studying human cognition and behav -\nior (Fig. 1b), as well as their unique role in psycho-cognitive processes. Recent research \nhas revealed significant advancements in LLMs’ ability to perform complex human-like \ncognitive and social tasks (Grossmann et al. 2023; Marjieh et al. 2023; Orru et al. 2023; \nPal et al. 2023; Stevenson et al. 2022; Webb et al. 2023). For instance, Grossmann et al. \n(2023) and Marjieh et al. ( 2023) demonstrated LLMs’ proficiency in simulating human \nFig. 1 LLMs in Psychological Research Across Timescales. (a) Domains (e.g., Cognitive & Behavioral, \nSocial & Cultural) mapped to timescales of behavior; (b) Emergent properties (e.g., cognitive simulation) \nenabling domain-specific modeling; (c) LLMs as research tools (e.g., stimuli generation). Double-sided \narrows indicate that emergent properties bridge domains and tools, supporting applications (e.g., memory \nretrieval) and refining properties through usage\n \n1 3\nPage 5 of 35 305\nL. Ke et al.\nsocial interactions and perceptual processing, respectively. Orru et al. (2023) and (Webb et \nal. 2023) highlighted LLMs’ capabilities in complex problem-solving and reasoning while \nHagendorff et al. ( 2023) focused on decision-making processes. Stevenson et al. ( 2022) \ndocumented LLMs’ potential for creativity, and Patel and Fan ( 2023) demonstrated their \nemotion-recognition abilities. Taken together, such findings highlight the expanding role \nof LLMs in representing and augmenting human cognitive and social functions, marking \nsignificant progress in AI research.\nAs general-purpose cognitive models (Binz and Schulz 2023a), LLMs offer new per -\nspectives and approaches for research in the fields of cognitive and behavioral psychology, \nclinical and counseling psychology, educational and developmental psychology, and social \nand cultural psychology, at different time scales of human behavior (Fig. 1a). LLMs can \nalso be used as research aids (Fig. 1c) to help psychologists with everything from literature \nreviews (Aydın and Karaarslan 2022; Qureshi et al. 2023), experimental subjects (Dillion \net al. 2023; Hutson 2023), and data analysis (Patel and Fan 2023; Peters and Matz 2023; \nRathje et al. 2023) to promote scholarly communication: academic writing (Dergaa et al. \n2023; Stokel-Walker 2022) or peer review (Chiang and Lee 2023; Van Dis et al. 2023). \nThus, LLMs can potentially become research assistants for psychologists, helping them \nimprove their research efficiency.\n1.3 Objectives and significance of the present review\nThis review aims to provide a comprehensive analysis of the applications and effects of \nLLMs in psychological research. To ensure a systematic and rigorous review, we estab -\nlished specific inclusion and exclusion criteria. The review focuses on literature published \nbetween 2020 and 2024, sourced from relevant academic databases, including Google \nScholar, arXiv, and Web of Science. Our initial keyword selection—“GPT-3”, “ChatGPT”, \n“GPT-4”, “large language models”, and “psychology”—was determined during the litera -\nture collection in October 2023, when GPT-based models were predominant in psychologi-\ncal research (e.g., Binz and Schulz 2023b; Bubeck et al. 2023). At the time, open-source \nmodels like LLaMA (Touvron et al. 2023) and proprietary models like Claude had limited \npsychology-specific applications. To maintain a focused scope, we did not retrospectively \nexpand search terms but included diverse LLMs via manual screening. To reflect recent \ndevelopments, an updated search incorporated studies from 2024.\nTo bolster the integrity of our data extraction process, two interdisciplinary researchers \n(male, 33 and 41 years) specializing in information science and psychology conducted the \nencoding and screening. Our inclusion criteria required the selected studies to (1) explore \nthe application or analysis of LLMs in psychological contexts; (2) be peer-reviewed journal \narticles or high-impact conference proceedings; and (3) present empirical data, theoretical \ndiscussions, or methodological advancements. We selectively included preprint articles if \nthey addressed emerging trends or filled notable gaps in the literature. Articles without a \npsychological focus or those addressing non-LLM-based AI systems were excluded. The \nstudy selection involved screening 191 identified studies, analyzing 100 full-text articles, \nand ultimately including 47 studies categorized into various psychological subfields. Each \nof these studies met stringent inclusion criteria, ensuring they contributed meaningfully to \nour understanding of LLMs in psychological research.\n1 3\n305 Page 6 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nIn this review, we systematically examine the use of LLMs in various psychological \ndomains by analyzing their application at different behavioral time scales. The rest of the \npaper is structured as follows. In Sect. 2, we explore LLMs in cognitive and behavioral \npsychology. In Sect. 3, the roles of LLMs in clinical and counseling psychology are dis -\ncussed. Subsequently, educational and developmental psychology are addressed in Sect. 4, \nfollowed by social and cultural psychology in Sect. 5, outlining LLMs’ contributions to \neach area. While psychological techniques are occasionally utilized to assess the capabili -\nties of LLMs, this approach is employed to enhance understanding of their suitability and \npotential as instruments for psychological research. The primary focus of this review is how \nLLMs facilitate and advance psychological research across these domains. For a deeper \nunderstanding of the effect of LLMs on psychological research, an overview of LLMs’ \npotential as tools for scientific research is given in Sect. 6. In Sect. 7, the challenges and \nfuture research directions with regard to applying LLMs to psychological contexts are pro-\nvided. Finally, conclusions are presented in Sect. 8, with a summary of LLMs’ applications \nin psychology and recommendations for future work. Importantly, we propose strategies for \nintegrating LLMs into psychological research and provide insights into interpreting such \nmodels from a psychological standpoint, contributing to their safety and interpretability.\n2 LLMs in cognitive and behavioral psychology\nWithin the multilevel time scales of human behavior (Newell 1990), cognitive and behav-\nioral psychology has primarily focused on the study of cognitive processes at sub-hourly \ntime scales, which encompass human engagement in perception, memory, thinking, deci -\nsion-making, problem-solving, and conscious planning. Cognitive and behavioral psychol-\nogy typically uses experimental methods to study these cognitive processes, controlling \nand observing behaviors and responses under specific conditions. The recent emergence \nof LLMs has reinvigorated the discussion on whether such models might exhibit patterns \nresembling human cognitive processes; if so, it may be possible to study the “cognitive \nprocesses” of LLMs, which could provide valuable insights into human cognitive phenom-\nena and serve as a valuable addition to existing research methods in cognitive psychology. \nThe foundational technology underlying large language models (LLMs) is the generative \npre-trained transformer (GPT) architecture, which employs deep neural networks to process \nand generate human-like text. GPT models function through mechanisms, such as attention \nmechanisms and token prediction, enabling them to capture complex linguistic patterns and \ngenerate contextually coherent outputs. These foundational technologies have transformed \nnatural language processing (NLP) by expanding the capacity for both comprehension and \ngeneration of text across diverse applications, from conversational agents to content cre -\nation (Brown et al. 2020; Vaswani et al. 2017). The incorporation of such architectures into \npsychological research has initiated discussions regarding their potential to simulate cogni-\ntive phenomena.\nBinz and Schulz ( 2023a) found that fine-tuning multiple tasks enabled an LLM to pre -\ndict human behavior in previously unseen tasks, suggesting that LLMs can be adapted to \nbecome generalist cognitive models. In another study, the same authors tested GPT-3 using \ntools from cognitive psychology and showed that it made better decisions than humans and \noutperformed them in the multiarmed bandit task (Binz and Schulz 2023b). Other studies \n1 3\nPage 7 of 35 305\nL. Ke et al.\nhave shown that LLMs can display perceptual judgment (Marjieh et al. 2023), reasoning \n(Webb et al. 2023), and decision-making abilities (Hagendorff et al. 2023), as well as cre -\nativity (Stevenson et al. 2022) and problem-solving (Orru et al. 2023). One study found that \nan LLM had the mental ability of a seven-year-old child based on a false-belief task (which \nis considered the gold standard for testing theory of mind in humans) (Kosinski 2024). \nExploring the reasoning capabilities and decision-making processes of LLMs, Hagendorff  \net al. (2023) designed a series of semantic illusion and cognitive reflection tests designed to \nelicit intuitive but erroneous responses (these are conventionally used to study human rea -\nsoning and decision-making) and then ran the tests for LLMs. They conducted an analysis of \nmodel performance on a Cognitive Reflection Test (CRT) task and a semantic illusion task \nto elucidate their cognitive processes, drawing upon System 1 and System 2 thinking, as \nconceptualized by Daniel Kahneman in his seminal work Thinking, Fast, and Slow (Kahn -\neman 2011), which represent fundamental constructs for understanding human cognitive \nprocesses. System 1 refers to intuitive and automatic thinking, whereas System 2 involves \nrational, deliberate decision-making processes. This framework provides a theoretical basis \nfor interpreting how LLMs simulate human-like cognitive behaviors during these tasks. \nThey observe how these models show correct responses in these tasks and avoid pitfalls. \nThe performance of the models in the CRT task were further evaluated by preventing them \nfrom chain-thinking to reason. The results showed that as model size and language capabil-\nity increased, the LLMs increasingly exhibited human-like intuitive thinking (System 1) and \nthe associated cognitive errors. Table 1 provides a summary of the applications of LLMs to \ncognitive and behavioral psychology.\nBeyond theoretical evaluations, LLMs have demonstrated practical value in experimen-\ntal psychology, particularly in stimulus generation and experimental design (Zhuang et al. \n2023). Dubey et al. ( 2024), for instance, used DALL-E 2 1 to create realistic visual stimuli \ndepicting car-free urban environments, which influenced participants’ attitudes toward sus-\ntainable policies. Such tools streamline the stimulus design process by providing control, \ndiversity, and scalability. Similarly, LLMs have been employed in hardware testing to gen-\nerate tailored stimuli, outperforming traditional methods in specific scenarios (Zhang et \nal. 2023b). Charness et al. ( 2023) further demonstrated the use of LLMs for enhancing \nexperimental workflows by refining task instructions, ensuring consistency, and monitoring \nparticipant engagement. By leveraging their flexibility and scalability, LLMs can provide \nnovel methods for advancing experimental psychology. These applications facilitate the \nexploration of complex cognitive phenomena and the development of innovative research \ndesigns while also complementing traditional psychological research frameworks (Sriniva-\nsan et al. 2023). However, the interpretation of LLM outputs requires careful contextualiza-\ntion to avoid overstating their capabilities or equating them with human cognitive processes.\n1 Note: Although DALL-E 2 is not an LLM, we included this study due to its reliance on Transformer-based \nsemantic understanding, a cornerstone of LLM research, and its demonstrated utility in generating controlled \nvisual stimuli for psychological experiments.\n1 3\n305 Page 8 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nReferences Research Question Research method Key finding\nHuman-like Cognitive Abilities\nBinz and \nSchulz \n(2023b)\nHow does GPT-3 \nperform on cognitive \npsychology tasks, \nincluding decision-\nmaking, information \nsearch, and causal \nreasoning?\nGPT-3 was tested using ca-\nnonical cognitive psychology \nexperiments and compared to \nhuman performance.\nGPT-3 excels in decision-making \nand reinforcement learning but \nstruggles with task perturbations, \ndirected exploration, and causal \nreasoning.\nStevenson et \nal. (2022)\nCan GPT-3 generate \ncreative solutions \ncomparable to humans \nin Guilford’s Alterna-\ntive Uses Test (AUT)?\nGPT-3’s responses to AUT \nwere evaluated for original-\nity, usefulness, surprise, \nand flexibility, using expert \nratings and semantic distance \nanalysis, and compared with \nhuman data.\nHumans currently outperform \nGPT-3 in creativity, but GPT-3 \nshows potential to close the gap \nin future, raising questions about \nAI creativity and its evaluation.\nMarjieh et al. \n(2023)\nCan LLMs recover \nperceptual informa-\ntion from language, \nand how do they \nreflect cross-linguistic \nvariations?\nGPT-3, GPT-3.5, GPT-4 were \ntested on six psychophysical \ndatasets and a multilingual \ncolor-naming task to compare \ntheir outputs with human \nperceptual data.\nLLMs like GPT-4 align closely \nwith human perceptual data, \nrecover representations such as \nthe color wheel, and reflect cross-\nlinguistic perceptual variations, \ndemonstrating their ability to \nextract perceptual information \nfrom language.\nLoconte et al. \n(2023)\nHow do various \nLLMs perform on \nneuropsychological \ntests assessing pre-\nfrontal functions \ncompared to human \ncognitive abilities?\nGPT-3.5, GPT-4,were evalu-\nated on tasks related to plan-\nning, semantic understanding, \nand Theory of Mind.\nFindings indicate that GPT-4 \ngenerally meets normative human \nstandards, whereas Claude2, and \nLlama2 show variable and often \nlimited abilities, particularly in \nplanning and Theory of Mind, \nunderscoring the challenges \nin mimicking complex human \ncognitive functions.\nDhingra et al. \n(2023)\nHow does GPT-4 \nperform on cogni-\ntive psychology \ntasks compared to \nprior state-of-the-art \nmodels?\nGPT-4 was evaluated on \ncognitive psychology da-\ntasets (CommonsenseQA, \nSuperGLUE, MATH, HANS) \nto analyze its integration \nof cognitive processes with \ncontextual information.\nGPT-4 demonstrates high ac-\ncuracy on cognitive psychology \ntasks, surpassing prior models, \nand showcases significant poten-\ntial to bridge human and machine \nreasoning.\nHagendorff \n(2024)\nCan modern LLMs \nunderstand and apply \ndeception strategies?\nExperiments tested LLMs on \ninducing false beliefs, using \nchain-of-thought reasoning, \nand displaying Machiavellian \nbehavior in simple and com-\nplex deception scenarios.\nGPT-4 demonstrates advanced \ndeceptive behavior, succeeding in \n99.16% of simple and 71.46% of \ncomplex scenarios, highlighting \nthe emergence of sophisticated \ndeception abilities absent in \nearlier models.\nExperimental Methodologies for Cognitive Research Using LLMs\nTable 1 Applications of large Language models (LLMs) in cognitive and behavioral psychology study\n1 3\nPage 9 of 35 305\nL. Ke et al.\n3 LLMs in clinical and counseling psychology\nClinical and counseling psychology focuses on assessing, diagnosing, treating, and prevent-\ning mental health problems. These processes often involve medium- to long-term periods. \nIn the multilevel time scales of human behavior (Newell 1990), clinical and counseling psy-\nchology involves assessing everyday behavioral acts (about a few hours to a day), habitual \nthinking (about a day to a few months), and psychological disorders (a few months to many \nyears), among others (Fig. 1). The application of LLMs in clinical and counseling psychol-\nogy can be broadly divided into two categories: psychological assessment and psychologi -\ncal intervention. Psychological assessment focuses on improving the ecological validity, \nscalability, and accuracy of measuring mental health states while psychological interven -\ntions consider how LLMs can be used for scalable and personalized mental health support, \nsuch as life coaching. According to related reports, there has been a public rush to use LLMs \nsuch as GPT for mental health screening and treatment (Demszky et al. 2023). LLMs are \nexpected to be used in clinical psychology and counseling because they can parse human \nlanguage and generate human-like responses, categorize text, and flexibly adapt conversa -\ntional styles representing different theoretical orientations (Stade et al. 2023). This leads to \nthe following question: How do LLMs work in psychotherapy, and can they replace human \npsychotherapists?\nAn LLM is a basic generalized model with the ability to learn from small samples \n(Brown et al. 2020), which allows it to quickly become an “expert” in the clinical and coun-\nseling domain with only a small amount of data to learn from. For example, LLMs trained \non clinical content can identify more specific factors of change that can help psychologists \nunderstand the process of clinical intervention, thus opening the black box of psychotherapy \n(Schueller and Morris 2023). Regarding psychological assessment, studies have demon -\nstrated that LLMs can effectively recognize emotions (Sharma et al. 2023) and respond \nappropriately. They can also perform complex mental health evaluations (Patel and Fan \n2023; Schaaff et al. 2023), such as suicide risk assessment and schizophrenia prognosis. For \nexample, Elyoseph and Levkovich (2024) found that GPT-4, Google Bard, and Claude pro-\nReferences Research Question Research method Key finding\nBinz and \nSchulz \n(2023a)\nCan fine-tuned \nMata’s LLaMA ac-\ncurately model human \nbehavior?\nLLaMA was fine-tuned on \npsychological experiment data \nand tested on decision-making \ntasks and unseen behaviors.\nFine-tuned LLaMA outperforms \ntraditional cognitive models, \naccurately model individual be-\nhavior, and predict unseen human \nresponses.\nDubey et al. \n(2024)\nHow can generative \nAI tools be utilized \nto streamline the cre-\nation of experimental \nstimuli in psychologi-\ncal research and influ-\nence public attitudes \ntoward sustainable \npolicies?\nDALL-E 2 was employed \nto generate realistic visual \nstimuli of car-free urban en-\nvironments, which were then \npresented to participants to \nmeasure attitudes toward \nsustainable policies.\nBy using DALL-E 2, the study \ndemonstrated that generative \nAI tools can enhance the design \nprocess of experimental stimuli, \noffering greater control, diversity, \nand scalability, thereby effec-\ntively influencing participants’ \nattitudes.\nNote: The AUT is a psychological test that measures creativity by asking participants to think of as many \nuses as possible for a common object; DALL-E 2 is developed by OpenAI that generates detailed and \nrealistic images from textual descriptions to explore AI’s potential in creative fields\nTable 1 (continued) \n1 3\n305 Page 10 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nduced evaluations consistent with professional benchmarks in treated schizophrenia cases, \nthough GPT-3.5 exhibited overly pessimistic predictions. Other research has shown that \nGPT-3.5 excels in clinical psychiatric cases, achieving top grades in diagnosis and man -\nagement across 61% of cases, with only minor discrepancies in more complex scenarios \n(D’Souza et al. 2023).\nIn psychological interventions, LLMs have shown significant potential for delivering \nscalable and personalized mental health support (Blyler and Seligman 2023a, b). For exam-\npleBlyler and Seligman (2023a) demonstrated that GPT-4 can generate personalized thera-\npeutic strategies by analyzing narrative identities. These strategies were found to be valid \nand reasonable, highlighting GPT-4’s utility as a supportive tool in therapy and coaching. \nFor peer-to-peer mental health support, Sharma et al. (2023) designed an AI system offering \nreal-time empathic feedback, which improved overall empathy by 19.6% and significantly \nboosted self-efficacy among users struggling to provide support. Furthermore, J. M. Liu \net al. ( 2023a) evaluated ChatCounselor, a model trained on a domain-specific dataset of \npsychologist–client conversations, and found it outperformed open-source models, thereby \ndemonstrating the importance of domain-specific training for improving counseling capa -\nbilities. Table 2 summarizes the applications of LLMs to clinical and counseling psychology.\nThe above research cases, which demonstrate LLMs’ ability to provide clinicians with \nadequate mental health support (Schueller and Morris 2023), hold promise for addressing \ninsufficient capacity in the mental health care system and offering more individualized treat-\nment services, even potentially fully automating psychotherapy in the future (Stade et al. \n2023). It is essential to ensure that LLM is safe and privacy-protective in psychotherapy.\n4 LLMs in educational and developmental psychology\nEducational and developmental psychology is concerned with learning processes, knowl -\nedge accumulation, skill development, and changes in individual psychology in educational \nenvironments. Educational and developmental psychology is mainly positioned at the \nrelatively medium- to long-term level (Newell 1990), reflecting the ongoing learning and \ndevelopment that characterize the educational process. A national survey found that only 3 \nmonths after the public release of GPT, 40% of US teachers used it weekly for lesson plan-\nning, highlighting the growing impact of LLMs in education.\nTable 3 summarizes the applications of LLMs in educational and developmental psy -\nchology, which can be broadly divided into two categories: developmental research with \nLLMs and using LLMs for education and learning applications. Here, developmental \nresearch seeks to determine whether LLMs can simulate human developmental processes \n(e.g., theory of mind and emotional reasoning) and how such capabilities might advance \nour understanding of human cognitive and emotional development. Kosinski ( 2024), for \ninstance, tested different LLMs in 40 false-belief tasks and found that GPT-4 achieved \n75% accuracy, comparable to the performance of a six-year-old child, while older mod -\nels performed significantly worse. Vzorinab et al. ( 2024) used the Mayer–Salovey–Caruso \nEmotional Intelligence Test (MSCEIT) to evaluate GPT-4’s emotional intelligence. While \nGPT-4 excelled at understanding and managing emotions, its reflective analysis resembled \nthe early developmental stages of human emotional reasoning.\n1 3\nPage 11 of 35 305\nL. Ke et al.\nTable 2 Applications of LLMs in clinical and counseling psychology studies\nReferences Research question Research method Key finding\nPsychological Assessment Using LLMs\nElyoseph and \nLevkovich \n(2023)\nHow effective and \naccurate are Chat-\nGPT in assessing \nsuicide risk?\nThe study evaluated ChatGPT’s \nanalysis of a text vignette \ndepicting a hypothetical patient \nwith varied psychological \nstates, comparing its assess-\nments to those of mental health \nprofessionals.\nThe study found that ChatGPT \nconsistently underestimated sui-\ncide risk and mental resilience \ncompared to mental health \nprofessionals, suggesting that \nreliance on ChatGPT for sui-\ncide risk assessment could lead \nto inaccurately low evaluations.\nElyoseph and \nLevkovich \n(2024)\nHow do LLMs com-\npare to mental health \nprofessionals in as-\nsessing schizophre-\nnia prognosis and \ntreatment outcomes?\nVignettes were used to compare \nthe assessments of four LLMs \n(GPT-3.5, GPT-4, Bard, Claude) \nagainst professional and public \nbenchmarks.\nGPT-4, Bard, and Claude \naligned with professional views \non treated cases, while GPT-3.5 \nwas overly pessimistic.\nD’Souza et al. \n(2023)\nHow effective is \nChatGPT (3.5) in \naddressing clinical \npsychiatric cases and \nsupporting mental \nhealth care?\nChatGPT was tested on 100 \nclinical psychiatric vignettes, \nand expert psychiatrists \ngraded its responses across 10 \ncategories.\nChatGPT excelled in manage-\nment and diagnosis, earning top \ngrades in 61% of cases, with no \nmajor errors but minor discrep-\nancies in complex cases.\nSufyan et al. \n(2024)\nHow do the social \nintelligence (SI) \nlevels of LLMs \ncompare to human \npsychologists?\nSocial intelligence scores of \nChatGPT (4), Bard, and Bing \nwere compared with 180 \ncounseling psychology students \n(bachelor’s and PhD levels).\nChatGPT surpassed all psy-\nchologists in social intelligence, \nBing outperformed most bach-\nelor’s and some PhDs, while \nBard aligned with bachelor’s \nstudents but fell behind PhDs.\nPsychological Interventions with LLMs\nBlyler and \nSeligman \n(2023a)\nCan GPT-4 gener-\nate personalized \ntherapeutic strategies \nbased on narrative \nidentity?\nGPT-4 analyzed five narrative \nidentities to recommend tailored \ninterventions.\nGPT-4 effectively crafted \npersonalized strategies, \ndemonstrating its potential as a \nsupportive tool for therapy and \ncoaching.\nBlyler and \nSeligman \n(2023b)\nCan GPT-4 gener-\nate accurate and \ninsightful personal \nnarratives to sup-\nport self-discovery \nin therapy and \ncoaching?\nGPT-4 processed 50 stream-of-\nconsciousness thoughts from 26 \nparticipants to create personal-\nized narratives, which partici-\npants evaluated for accuracy, \nsurprise, and self-insight.\n96% of participants rated the \nnarratives as accurate, and \n73% reported gaining new \nself-insights, suggesting GPT-\n4’s potential for enhancing \nself-discovery in therapeutic \ncontexts.\nSharma et al. \n(2023)\nCan AI enhance \nempathy in peer-to-\npeer mental health \nsupport?\nTested HAILEY which based on \nGPT-2, which offering real-time \nempathic feedback, in a trial \nwith 300 peer supporters on \nTalkLife.\nHAILEY improved empathy by \n19.6% overall and 38.9% for \nthose struggling with support, \nboosting self-efficacy without \ncreating reliance.\nJ. M. Liu et al. \n(2023a)\nHow to improve \nLLM in provid-\ning mental health \nsupport compared to \nother models?\nChatCounselor, trained on the \nPsych8k dataset of 260 psychol-\nogist-client conversations, was \nevaluated using the Counseling \nBench with real-world counsel-\ning questions and psychological \nmetrics.\nChatCounselor outperforms \nLLaMA, ChatGLM and ap-\nproaches GPT-4’s performance, \nhighlighting the impact of \ndomain-specific training on \ncounseling capabilities.\n1 3\n305 Page 12 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nReferences Research question Research method Key finding\nDevelopmental Research with LLMs\nKosinski \n(2024)\nCan LLMs demon-\nstrate theory of mind \n(ToM) abilities?\nEleven different LLMs were \ntested on 40 false-belief tasks, \nrequiring success in eight \nrelated scenarios per task. Per-\nformance was compared across \nmodel versions.\nGPT-4 achieved 75% accura-\ncy, comparable to 6-year-old \nchildren, while older models \nperformed significantly \nworse.\nVzorinab et al. \n(2024)\nHow does GPT-4’s \nemotional intelligence \nalign with develop-\nmental patterns in \nhuman emotional \nreasoning?\nGPT-4 was evaluated using \nthe Mayer-Salovey-Caruso \nEmotional Intelligence Test \n(MSCEIT) through text-based \nprompts.\nGPT-4 excels in understand-\ning and managing emotions \nbut shows limited reflective \nanalysis, resembling early de-\nvelopmental stages in human \nemotional reasoning.\nTrott et al. \n(2023)\nHow does exposure to \nlanguage influence the \ndevelopment of theory \nof mind in humans \nand AI?\nA linguistic False Belief Task \nwas presented to humans and \nGPT-3 to assess belief attribu-\ntion abilities.\nGPT-3’s partial success \nsuggests that while language \nexposure contributes to belief \nreasoning, other developmen-\ntal mechanisms unique to \nhumans are crucial for fully \ndeveloping theory of mind.\nLLMs for Education and Learning Applications\nStojanov \n(2023)\nHow effective is GPT \nas a learning aid in \nscaffolding under-\nstanding of a specific \ntopic?\nAn autoethnographic study \nexploring the author’s personal \nexperience using ChatGPT(3.5) \nto learn about its technical \naspects.\nChatGPT supports learning \nthrough motivating feedback \nbut often provides superficial, \ninconsistent, and contradic-\ntory responses, risking over-\nestimation of knowledge.\nJyothy et al. \n(2024)\nWhat factors influence \nthe adoption of LLMs \nlike ChatGPT in \nlearning, teaching, and \nresearch?\nThe Fogg Behavior Model \n(FBM) was applied to analyze \nthe motivations, abilities, and \nperceptions of students, teach-\ners, and researchers toward \nLLM use.\nUser motivation and ability \ndrive LLM adoption, but lim-\nitations like teacher hesitance \nand technical challenges \nhinder broader integration.\nLogacheva et \nal. (2024)\nCan GPT-4 gener-\nate personalized \nprogramming exercises \nto enhance student \nengagement and \nlearning?\nGPT-4-generated exercises \nwere evaluated in an introduc-\ntory programming course by \nstudents and instructors for \nquality and engagement.\nGPT-4 effectively produced \nhigh-quality, engaging \nexercises, offering personal-\nized and scalable practice \nmaterials for programming \neducation.\nMachin et al. \n(2024)\nCan GPT demonstrate \npsychological literacy \ncomparable to subject \nmatter experts (SMEs) \nin psychology research \nmethods?\nGPT rated 13 research sce-\nnarios, and its responses were \nstatistically compared to SME \nevaluations.\nGPT showed strong align-\nment with SME ratings \n(r = 0.73–0.80), indicating its \npotential to match SME-level \npsychological literacy.\nGhafouri \n(2024)\nCan a ChatGPT-based \nrapport-building proto-\ncol (CGRBP) enhance \nL2 (Second Lan-\nguage) grit in English \nlearners?\nA 16-week experimental study \ncompared 30 EFL learners (15 \nexperimental, 15 control) using \npre-test post-test ANCOV A \nanalysis.\nCGRBP significantly \nimproved L2 grit, demon-\nstrating its potential to foster \nemotional support and learn-\ning motivation.\nTable 3 Applications of LLMs in educational and developmental psychology studies\n1 3\nPage 13 of 35 305\nL. Ke et al.\nThe study of using LLMs for education and learning applications focuses on leverag -\ning LLMs to address challenges in education, such as providing personalized learning and \nimproving learning motivation. LLMs learn from massive amounts of data taken from \nbooks and the Internet (Binz and Schulz 2023b) and can be used as more knowledgeable \nlearning aids (Stojanov 2023), provide personalized learning experiences (Kasneci et al. \n2023), and enhance the motivation to learn (Ali et al. 2023). Baillifard et al. ( 2024), for \ninstance, found that an AI tutor powered by GPT-3 improved academic performance by \nup to 15 percentile points through personalized learning strategies. Stojanov ( 2023) used \nthe following approach to explore GPT’s potential as a learning tool: First, he set learning \nobjectives and had “conversations” with GPT about its functionality for 4 h. For the next \n3 h, he continued the discussion with GPT and watched some relevant videos on YouTube. \nHe experienced positive feedback from his interactions with GPT and found it to be a moti-\nvating and relevant learning experience.\n5 LLMs in social and cultural psychology\nSocial and cultural psychology explores how individuals interact with and are influenced by \ntheir social and cultural environments. It focused on interpersonal dynamics, group behav -\nior, social cognition, and the long-term formation and transformation of attitudes and norms \n(Tajfel 1982). Such phenomena occur at various time scales, from immediate social interac-\ntions to cultural changes evolving over several years (Newell 1990). LLMs provide valuable \ntools for advancing social and cultural psychology. By analyzing textual datasets, simulat -\ning social interactions, and modeling human-like behaviors, LLMs can provide insights into \nthe dynamics of social cognition, group processes, and cultural norms (Salah et al. 2023). \nTheir scalability and ability to quantify patterns across time scales make them powerful \ninstruments for examining human interactions in diverse contexts.\nResearch on LLMs in social and cultural psychology can be categorized into three main \nareas: cultural and cognitive understanding, social interactions and behavioral simulations, \nand practical applications. First, LLMs have many similarities with humans regarding social \ncognition. For example, research has found that LLMs reflect a variety of typical human \ncognitive biases in judgment and decision-making, such as the anchoring effect, the repre -\nReferences Research question Research method Key finding\nGhafouri et al. \n(2024)\nCan ChatGPT match \nexpert psychological \nliteracy in evaluating \nresearch methods?\nThe study compared responses \nfrom ChatGPT to 13 psy-\nchological research method \nscenarios against ratings by \nsubject matter experts.\nChatGPT’s responses cor-\nrelated strongly with expert \nevaluations, suggesting its \npotential as an educational \ntool in psychology, though its \nusage should be approached \nwith caution.\nBaillifard et al. \n(2024)\nCan AI tutors improve \nacademic performance \nthrough personalized \nlearning strategies?\nA semester-long study with \n51 psychology students using \na GPT-3-powered AI tutor for \npersonalized retrieval practice \nand progress modeling.\nActive AI tutor use improved \ngrades by up to 15 percentile \npoints, with strong alignment \nbetween AI predictions and \nexam results.\nNote: Theory of mind (ToM) is the cognitive ability to attribute mental states to oneself and others; the \nFogg Behavior Model (FBM) explains behavior as a product of motivation, ability, and prompts\nTable 3 (continued) \n1 3\n305 Page 14 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nsentativeness heuristic, and base-rate neglect (Talboy and Fuller 2023). In addition, cultural \npsychology research has identified significant differences in the cognitive processes of East-\nerners and Westerners when processing information and making judgments (Nisbett et al. \n2001); in this regard, LLM consistently favors holistic Eastern ways of thinking (Jin et al. \n2023). Second, LLMs have been shown to characterize human groups in social interaction \nsettings. It has been shown that LLMs can replicate the results of Milgram’s electroshock \nexperiments (Aher et al. 2023), show better gaming abilities in specific games (Akata et al. \n2023), and exhibit different risk-taking and prosocial behaviors under different emotional \nstates (Yukun et al. 2023).\nThird, LLMs are increasingly used as proxies for human participants in psychological \nresearch. One study, for example, explored the potential of LLMs to serve as valid proxies \nfor specific human subgroups in social science research; it found that LLMs contained infor-\nmation that went far beyond superficial similarity, reflecting the complex interplay between \nideas, attitudes, and sociocultural contexts that characterizes human attitudes (Argyle et \nal. 2022). In addition, LLM has been tested for personality and values, obtaining results \ncomparable to those for human samples, indicating their potential as psychological research \ntools (Miotto et al. 2022). Within this broader perspective, industrial and organizational \npsychology has increasingly employed LLMs, particularly in employee selection and work-\nplace optimization, demonstrating their broader utility for understanding human behavior \nin structured environments. For example, LLMs have been shown to improve the accuracy \nand efficiency of recruitment systems in terms of assessing candidate fit and simulating \nworkplace behaviors (Du et al. 2024). This approach can help mitigate biases and expand \naccessibility to a broader range of candidates. LLMs have also been integrated into systems \nsuch as PALR (personalization-aware LLMs for recommendation) to dynamically align \nindividual capabilities with organizational needs. Such systems significantly reduce inef -\nficiencies in hiring processes and enhance predictions about job performance by identify -\ning nuanced compatibility factors in resumes and cover letters (Yang et al. 2023). Beyond \nindividual applications, LLMs have contributed to understanding broader organizational \ncultures and transformational dynamics by providing insights into how group interactions \nand leadership styles influence workplace outcomes (Noy and Zhang 2023). In the context \nof employee productivity, experiments using LLMs have revealed substantial benefits. For \ninstance, professionals using ChatGPT in workplace writing tasks improved productivity by \nreducing task completion time by 40% and enhancing output quality by 18%, indicating its \npotential to effectively augment mid-level professional tasks (Noy and Zhang 2023). Simi-\nlarly, research on creativity has demonstrated LLMs’ ability to help solve organizational \nproblems requiring innovative thinking (Lee and Chung 2024). Table 4 summarizes the \napplications of LLMs to social and cultural psychology.\nLLMs have many applications in social and cultural psychology, allowing us to test theo-\nries and hypotheses about human behavior in social and cultural interaction settings. Zhao \net al. (2024), for instance, examined whether AI chatbots can adjust their financial decisions \nand prosocial behaviors based on emotional cues, similar to humans. It was hypothesized \nthat bots would take fewer risks when exposed to fear cues and more risks with joy cues. \nEmotional primes (fear, joy, or neutral) were applied, and investment decisions were ana -\nlyzed. Additionally, prosocial responses, such as donating to a sick friend, were measured to \nassess how LLMs adapt behaviorally under emotional influences. These findings highlight \nLLMs’ ability to model complex social dynamics and cultural influences. The next sec -\n1 3\nPage 15 of 35 305\nL. Ke et al.\nReferences Research question Research method Key finding\nCultural and Cognitive Understanding with LLMs\nAtari et al. \n(2023)\nDo LLMs exhibit \nbiases toward WEIRD \n(Western, Educated, \nIndustrialized, Rich, \nDemocratic) societ-\nies in psychological \ntasks?\nLLMs’ responses on psy-\nchological measures were \ncompared to cross-cultural \nhuman data.\nLLMs closely align with \nWEIRD cognitive patterns but \nshow declining accuracy with \nnon-WEIRD populations (r \n= -0.70), revealing a WEIRD \nbias.\nJin et al. \n(2023)\nDoes GPT exhibit cul-\ntural cognitive traits \naligned with Eastern \nor Western thinking?\nGPT was evaluated using \ncognitive and value judgment \nscales.\nGPT leans towards Eastern \nholistic thinking in cognitive \ntasks but shows no cultural \nbias in value judgments, likely \ninfluenced by its training data \nand methods.\nSchaaff et al. \n(2023)\nHow empathetic is \nGPT compared to \nhumans?\nGPT’s empathy was evaluated \nthrough emotion recognition \ntasks, conversational analysis, \nand five empathy-related \nquestionnaires.\nGPT accurately identified \nemotions in 91.7% of cases, \nshowed parallel emotions \nin 70.7%, and scored below \naverage humans but above \nindividuals with Asperger syn-\ndrome on empathy measures.\nPatel and Fan \n(2023)\nCan LLMs like \nBard, GPT-3.5, and \nGPT-4 match human \nempathy and emotion \nidentification?\nEmpathy and emotional under-\nstanding were assessed using \nTAS-20 (Toronto Alexithymia \nScale-20) and EQ-60 (Emo-\ntional Quotient Inventory-60), \ncomparing LLM responses to \nhuman benchmarks.\nGPT-4 approached human-\nlevel emotional intelligence, \noutperforming Bard and GPT-\n3.5, which showed alexithymic \ntendencies.\nX. Wang et al. \n(2023)\nHow do LLMs \ncompare to hu-\nmans in emotional \nintelligence?\nA psychometric assessment \nfocusing on Emotion Under-\nstanding was developed and \napplied to mainstream LLMs, \nbenchmarking them against \nover 500 human participants.\nGPT-4 scored higher than 89% \nof humans in emotional intel-\nligence, with LLMs showing \nabove-average emotional intel-\nligence but using non-human \nmechanisms influenced by \nmodel design.\nX. Li et al. \n(2022b)\nAre LLMs psycho-\nlogically safe, and \nhow can fine-tuning \nimprove their safety?\nLLMs were assessed using \nthe Short Dark Triad (SD-3), \nBig Five Inventory (BFI), and \nwell-being tests to evalu-\nate personality traits and the \nimpact of fine-tuning.\nLLMs exhibit elevated dark \ntraits but show improved well-\nbeing and psychological safety \nwith targeted fine-tuning.\nMiotto et al. \n(2022)\nWhat are GPT-3’s \npersonality traits and \nvalues as assessed by \nvalidated psychologi-\ncal tools?\nAdministered validated person-\nality and values measurement \ntools to GPT-3, including a \nmodel response memory to \nassess value alignment.\nGPT-3 exhibits personality \ntraits and values similar to \nhuman samples, providing \ninitial evidence of psychologi-\ncal assessment in LLMs.\nSocial Interactions and Behavioral Simulations\nZhao et al. \n(2024)\nCan LLMs like GPT \nadapt responses to \nemotional primes in \ndecision-making?\nTested GPT-4 and 3.5 with \nscenarios eliciting positive, \nnegative, or neutral emotions.\nGPT-4 showed distinct \nemotional response patterns, \nexceeding GPT-3.5, indicating \nadvanced modulation but no \ntrue emotions.\nTable 4 Applications of LLMs in social and cultural psychology studies\n1 3\n305 Page 16 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nReferences Research question Research method Key finding\nAher et al. \n(2023)\nCan LLMs accu-\nrately simulate human \nbehaviors, and what \nbiases emerge in their \nsimulations?\nIntroduced Turing Experiments \nto evaluate LLMs against find-\nings from classic behavioral \nstudies.\nLLMs replicated most findings \nbut showed “hyper-accuracy \ndistortion,” raising concerns for \napplications in education and \nthe arts.\nAbramski et \nal. (2023)\nDo LLMs exhibit bi-\nases toward math and \nSTEM, and how do \nthese biases compare \nacross models and \nwith humans?\nUsing Behavioral Forma Men-\ntis Networks, biases in GPT-3, \nGPT-3.5, GPT-4, and high \nschool students were analyzed \nthrough a language generation \ntask.\nNewer LLMs (GPT-4) show re-\nduced negative bias and richer \nsemantic associations toward \nmath and STEM compared \nto older models and humans, \nsuggesting advancements in \nreducing stereotypes.\nAlmeida et al. \n(2024)\nHow do state-of-the-\nart LLMs reason about \nmoral and legal issues, \nand how do their \nresponses align with \nhuman judgments?\nEight experimental psychol-\nogy studies were replicated \nusing Google’s Gemini Pro, \nAnthropic’s Claude 2.1, GPT-\n4, and Meta’s Llama 2 Chat \n70b. Model responses were \ncompared to human responses \nto assess alignment and sys-\ntematic differences.\nGPT-4 showed the best human \nalignment among LLMs \nbut exaggerated effects and \nreduced variance, highlighting \nbiases that limit their suit-\nability as substitutes for human \nparticipants in psychological \nresearch.\nPractical Applications with LLMs\nNoy and \nZhang (2023)\nHow does generative \nAI affect employee \nproductivity?\nWorkplace experiments with \nChatGPT on writing tasks.\nChatGPT improved productivi-\nty (40% faster task completion, \n18% better quality).\nLee and \nChung (2024)\nHow does ChatGPT \ninfluence creativity?\nMeasured creativity using \nassociative and divergent \nthinking tasks.\nChatGPT supports incremental \ncreativity but is less effective \nfor radical innovation.\nYang et al. \n(2023)\nCan LLMs personalize \njob recommendations?\nPALR(personalization-aware \nLLMs for recommenda-\ntion) integrated interaction \ndata with LLMs for dynamic \nrecommendations.\nPALR enhanced predictions of \njob performance and improved \nrole matching.\nDu et al. \n(2024)\nHow can LLMs \nimprove job \nrecommendations?\nUsed GANs with LLMs to \nrefine low-quality resumes.\nGAN-based systems predicted \nbetter job fit and reduced hiring \ninefficiencies.\nAkata et al. \n(2023)\nHow do LLMs \nperform in social \ninteraction tasks \ninvolving cooperation \nand coordination?\nLLMs played repeated two-\nplayer games (e.g., Prisoner’s \nDilemma, Battle of the Sexes) \nwith other LLMs and human-\nlike strategies to analyze their \nbehavior.\nLLMs perform well in self-\ninterest-driven games but strug-\ngle with coordination, with \nGPT-4 showing unforgiving \nbehavior in the Prisoner’s Di-\nlemma and difficulty adopting \nsimple coordination strategies.\nSuri et al. \n(2024)\nDoes GPT exhibit \nheuristics and context-\nsensitive responses \nsimilar to those \nobserved in human \ndecision-making?\nFour studies tested GPT’s \nresponses to prompts designed \nto assess cognitive biases \n(anchoring, representativeness, \navailability heuristic, framing \neffect, endowment effect) and \ncompared them to human \nparticipant responses.\nGPT demonstrated biases con-\nsistent with human heuristics \nacross all studies, suggesting \nthat language patterns alone \nmay contribute to these effects, \nindependent of human cogni-\ntive and affective processes.\nTable 4 (continued) \n1 3\nPage 17 of 35 305\nL. Ke et al.\ntion broadens this perspective, exploring LLMs’ potential as versatile research tools for \npsychologists.\n6 LLMs as research tools in psychology\nSections 2–5 illustrate LLMs’ applications across cognitive, clinical, educational, and \nsocial psychology, revealing their potential to transform research practices. Together, \nthese advancements speed up psychological research with new tools, encourage collabo -\nration with fields like computer science and linguistics, and improve theoretical models \nthrough behavioral simulation—key ways LLMs advance psychology. Building on these \nfoundations, this section explores LLMs as versatile research tools in psychology, support-\ning diverse tasks such as systematic reviews, literature review, and experimental design \n(Table 5). By reducing subjective bias and minimizing human variability in tasks like stimu-\nlus generation (Sect. 2standardized assessments (Sect. 3), and data interpretation (Sect. 4), \nLLMs enhance objectivity and efficiency across these applications.\nFor instance, LLMs can automate systematic reviews and meta-analyses, revolutionizing \nevidence synthesis and provide actionable insights for psychologists, as grounded in cogni-\ntive and behavioral principles (e.g., in Sects. 2 and 3). This capacity extends to enhancing \nReferences Research question Research method Key finding\nPark et al. \n(2022)\nHow can designers \npredict and refine \nsocial behaviors in \nlarge-scale social \ncomputing systems \nbefore deployment?\nDeveloped “social simulacra,” \nan LLM-driven simulation that \ngenerates realistic community \ninteractions based on design \ninputs (goals, rules, personas), \nallowing scenario testing and \niterative design refinement.\nSocial simulacra accurately \nmimicked real community \nbehavior, supported “what \nif?” scenario exploration, and \nhelped designers improve sys-\ntem designs before large-scale \ndeployment.\nSap et al. \n(2022)\nCan LLMs demon-\nstrate social intel-\nligence and Theory of \nMind (ToM)?\nLLMs were evaluated using \nSocialIQa (social intents and \nreactions) and ToMi (mental \nstates and realities), with \nresults contextualized through \npragmatics theories.\nLLMs, including GPT-4, per-\nform below human levels (55% \non SocialIQa, 60% on ToMi), \nindicating that scaling alone \ndoes not yield ToM, highlight-\ning the need for person-centric \nNLP approaches.\nArgyle et al. \n(2022)\nCan GPT-3 reliably \nemulate human sub-\npopulations for social \nscience research?\nGPT-3 was conditioned on \nsociodemographic backstories \nfrom U.S. surveys, creating \n“silicon samples,” which were \ncompared to human survey \ndata.\nGPT-3 exhibits nuanced, de-\nmographically aligned biases, \nindicating its potential as a tool \nfor studying human behavior \nand societal dynamics.\nPark et al. \n(2024a, b)\nCan GPT-3.5 simulate \nhuman participants \nand replicate social \nscience study results?\nReplicated 14 Many Labs 2 \nstudies using GPT-3.5, analyz-\ning response patterns and the \n“correct answer” effect through \npre-registered and exploratory \nstudies.\nGPT-3.5 replicated 37.5% of \nstudy results but exhibited \nuniform responses (“correct \nanswer” effect) and skewed \nconservative in moral founda-\ntion surveys, questioning its \nreliability and diversity as a \nhuman participant substitute.\nNote: WEIRD (Western, Educated, Industrialized, Rich, Democratic) refers to societies that represent a \nminority of the global population but are often overrepresented in psychological research\nTable 4 (continued) \n1 3\n305 Page 18 of 35\nExploring the frontiers of LLMs in psychological applications: a…\npsychologists’ workflows, building on productivity improvements noted in Sect. 5, through \ntools like literature review, hypothesis generation, experimental design, experimental sub -\njects, and data analysis (Table 5).\n6.1 Automated literature review and meta-analysis\nConducting a literature review meta-analysis is a complex, arduous process that requires \nsignificant time and expertise (Michelson and Reuter 2019). Nature reported that research-\ners have used GPT as a research assistant to summarize literature (Dis et al. 2023). In one \nstudy, researchers used GPT to complete certain systematic literature review tasks (Qureshi \net al. 2023). In another study, a literature review article was created using GPT with the \napplication of digital twins in the health field; the results showed that knowledge compila -\ntion and representation were accelerated with the help of LLMs. However, their academic \nvalidity needs to be further verified (Aydın and Karaarslan 2022). Researchers have also \nspecifically trained LLMs to support the practical needs of scientific research (Taylor et al. \n2022), including the ability to perform systematic literature reviews.\nTable 5 LLMs as research tools in psychology study\nTopic Related study\nLiterature review LLMs can summarize the researched literature (Dis et al. 2023), complete \nliterature review tasks (Qureshi et al. 2023), and create literature review \narticles (Aydın and Karaarslan 2022), at the same time, there are LLM that has \nbeen specially trained to accomplish systematic literature reviews (Taylor et al. \n2022)。\nHypothesis generation LLMs can generate hypotheses from scientific literature, make inferences \nbased on scientific data, and then clarify their conclusions through interpreta-\ntion (Zheng et al. 2023), and can quickly and automatically test these research \nhypotheses and learn from mistakes.\nExperimental design LLMs provide text-based material for experimental design, thereby optimiz-\ning the research process and reducing experimental complexity. By employing \nthese models, researchers can easily create experimental stimuli, develop test \nitems, and even simulate interactive sessions in controlled environments (Aher \net al. 2023; Akata et al. 2023), providing a high degree of control and precision \nto the experimental process.\nExperimental subjects LLMs can simulate some human behaviors and responses, which provides an \nopportunity to test theories and hypotheses about human behavior (Grossmann \net al. 2023), their use in place of human participation in experiments saves \ntime and costs and can be applied to some experiments where human participa-\ntion is not appropriate(Hutson 2023), they can be combined with factors such \nas the specific research topic, the task, and the sample, and the use of LLM as \nan alternative to research participants where appropriate(Dillion et al. 2023).\nData analysis LLMs can efficiently analyze massive amounts of textual data to gain insights \ninto human behavior and emotions at an unprecedented scale (Patel and Fan \n2023), can analyze textual data in multiple languages, and accurately detect \nmental structures within it (Rathje et al. 2023), can draw mental profiles from \nsocial media data (Peters and Matz 2023)。\nScholarly \nCommunication\nLLMs can also help humans in writing(Dergaa et al. 2023; Stokel-Walker \n2022; Van Dis et al. 2023). LLMs were used in two natural language process-\ning tasks and a human expert to assess the quality of the text, and the results \nof the assessment were consistent with those of the human expert (Chiang and \nLee 2023), LLMs offer the opportunity to get things done quickly, from Ph.D. \nstudents struggling to finish their dissertations, to peer reviewers submitting \nanalyses under time pressure(Van Dis et al. 2023).\n1 3\nPage 19 of 35 305\nL. Ke et al.\nRecent studies have highlighted how LLMs can efficiently support meta-analysis. For \ninstance, Luo et al. (2024) demonstrated that LLMs can screen literature, extract data, and \ngenerate statistical codes for meta-analyses, significantly reducing workload while main -\ntaining recall rates comparable to manual curation. Tong et al. (2024) used LLMs to extract \ncausal pairs from 43,312 psychology articles, achieving an 86.98% success rate in pair \nextraction through adaptive prompting. As discussed in Sect. 5, LLMs have shown strong \ncapabilities in extracting causal relationships from large textual datasets, underscoring \ntheir potential to streamline evidence synthesis for systematic reviews and meta-analyses. \nNevertheless, while LLMs excel in organizing qualitative data and identifying conceptual \npatterns, they face challenges in extracting the precise numerical data necessary for meta-\nanalyses. For example, although LLM-based tools can retrieve and summarize outcome \nmeasures, manual validation remains essential to ensure accuracy, especially when process-\ning complex figures or tables.\nIn summary, LLMs can speed up the process of literature review and meta-analysis. \nResearchers can use such models to systematically review and synthesize existing research, \nimproving the efficiency of evidence-based psychology.\n6.2 Hypothesis generation and experimental design\nHypothesis-driven research is at the core of scientific activity. LLMs can generate hypoth -\neses from scientific literature, make inferences based on data, and then clarify conclusions \nthrough interpretation (Banker et al. 2024; Zheng et al. 2023). Although LLMs are capable \nof becoming “hypothesis machines,” their logical and mathematical derivation capabilities \nstill need improvement to eliminate factual errors, quickly test hypotheses, and learn from \nmistakes (Park et al. 2024a, b). As innovative tools, LLMs have great potential for use in \npsychological experiments, given their ability to provide text-based material for experimen-\ntal designs, thus optimizing the research process and reducing experimental complexity. \nUsing such models researchers can easily create experimental stimuli, develop test items, \nand even simulate interactive sessions in controlled environments (Aher et al. 2023; Akata \net al. 2023), providing a high degree of control and precision in the experimental process.\nIn conclusion, LLMs provide powerful, flexible tools for psychological research, from \nhypothesis generation to experimental design, which can help researchers achieve more \nprecise, efficient research goals.\n6.3 LLMs as subjects in psychological experiments\nAlthough LLMs can simulate some human behaviors and responses—which provides \nan opportunity to test theories and hypotheses about human behavior (Grossmann et al. \n2023)—there is still some controversy on whether LLMs can be used as a substitute for \nhuman subjects in psychological research. While recognizing that certain problems persist \n(e.g., biases and insufficiently trained data), some researchers have suggested that LLMs \ncan be used as substitutes for human participants to save time and cost and can be applied \nto experiments that are not suitable for human participation (Hutson 2023). Others have \nproposed using LLMs as an alternative method of studying participants when appropriate, \nbased on their performance in conjunction with factors such as specific research topics, \ntasks, and samples (Dillion et al. 2023). However, it is also believed that although LLMs \n1 3\n305 Page 20 of 35\nExploring the frontiers of LLMs in psychological applications: a…\ncan significantly affect scientific research, they are unlikely to replace human participants \nin any meaningful way (Harding et al. 2023). At the same time, some studies of LLMs as \nsubjects have shown that LLMs perform similarly to humans (Orru et al. 2023; P. S. Park et \nal. 2024a, b), which might indicate LLMs’ potential to replace humans as subjects.\nIn conclusion, although LLMs can simulate human judgment, their simulation of human \nthinking remains limited, and their output should be validated and interpreted with caution \nwhen used as psychological subjects.\n6.4 Tools for data analysis\nVarious forms of AI have long been used to analyze psychological data, such as flight data \nfor pilot screening (Ke et al. 2023). Machine learning algorithms facilitate the processing \nof large datasets, identifying patterns and correlations that might otherwise be overlooked. \nHowever, LLMs take this capability to a new level; they can efficiently analyze massive \namounts of textual data on an unprecedented scale to derive insights into human behavior \nand emotions (Patel and Fan 2023). For psychological research, this means faster and more \ncomprehensive data analysis, leading to more reliable, nuanced findings. LLMs can analyze \ntextual data in multiple languages, accurately detect psychological structures within them \n(Rathje et al. 2023), and generate psychological profiles from social media data (Peters \nand Matz 2023). LLMs have also demonstrated a degree of competence in the medical \nfield; LLMs can, for example, predict the optimal neuroradiographic imaging modality for \na given clinical presentation. Yet, LLMs cannot outperform experienced neuroradiologists, \nsuggesting the need for continued improvement in the medical context (Nazario-Johnson \net al. 2023). These findings demonstrate the great potential of LLMs for evaluating and \nanalyzing data.\n6.5 Promoting scholarly communication\nScholarly communication is a cornerstone of academic research, encompassing the pro -\ncesses of creating, evaluating, and disseminating knowledge. It includes writing research \npapers, conducting peer reviews, and ensuring that findings are communicated transpar -\nently and ethically. In psychology, this process is particularly complex owing to the field’s \ndiverse theoretical frameworks and methodological approaches, ranging from experimental \nto qualitative research. The discipline’s focus on human behavior and its intersection with \ntechnology demands precise and ethical communication practices.\nIt has been argued that LLMs currently cannot completely replace human writing and \ninstead can only answer questions and generate naturally fluent and informative content \nbut with no real intelligence—i.e., text based on patterns of previously seen words (Stokel-\nWalker 2022). In one study, students used GPT as an aid in their writing. The experimental \ngroup that used GPT was found to be similar to the control group in terms of writing qual -\nity, speed, and authenticity; the authors suggested that this could be because experienced \nresearchers can better guide GPT to produce high-quality information. By contrast, stu -\ndents—who have less writing experience than researchers—found that GPT did not per -\nform as effectively (Bašić et al. 2023). Another article discussed the prospects and potential \nthreats of GPT in academic writing, emphasizing that using GPT in academic research \nshould prioritize peer-reviewed scholarly sources. Yet, GPT’s potential advantages for aca-\n1 3\nPage 21 of 35 305\nL. Ke et al.\ndemic research, including the handling of large amounts of textual data and the automatic \ngeneration of abstracts and research questions (Dergaa et al. 2023), were highlighted. Fur-\nthermore, LLMs can potentially be used for peer review (Van Dis et al. 2023). The deci -\nsions/judgments of LLMs in a text-evaluation task were found to be consistent with those of \nhuman experts (Chiang and Lee 2023).\nIn conclusion, LLMs such as GPT are potent tools for scholarly communication in psy -\nchology, capable of processing large amounts of textual data and automating tasks that were \npreviously done manually. They can be used to scan academic papers and extract essential \ndetails, generate objective and unbiased abstracts, and create research questions in social \npsychology (Banker et al. 2024; Tong et al. 2024). However, researchers must exercise \ncaution when using them as they can also introduce false or biased information into papers, \nleading to unintentional plagiarism and the misattribution of concepts (Van Dis et al. 2023).\n7 Challenges and future directions\n7.1 Challenges and limitations\nLLMs have enormous potential to simulate complex cognitive processes, providing \nresearchers with new tools to explore the mechanisms of human cognition and behavior \nfor wide-ranging application in various fields, including clinical and counseling psychol -\nogy, educational and developmental psychology, and social and cultural psychology. How-\never, LLM output should not be mistaken for the presence of thought but instead viewed \nas complex pattern matching based on probabilistic modeling (Floridi and Chiriatti 2020). \nAlthough LLMs show impressive performance, this differs from consciousness or genuine \nunderstanding. The interpretation of LLMs’ capabilities must be based on an understand -\ning of their limitations and the nature of their operations, which might differ fundamentally \nfrom human cognition. It is essential, then, to focus on the potential of LLMs in psychologi-\ncal research while also acknowledging the technical and ethical challenges that might arise.\nFirst, despite the emergence of LLM competence (Wei et al. 2022), its internal working \nmechanism remains a black box from a cognitive and behavioral psychology perspective. \nFor example, LLMs perform impressively on tasks requiring formal linguistic competence \n(including knowledge of the rules and patterns of a particular language) but fail many tests \nrequiring functional competence (the set of cognitive abilities needed to understand and \nuse language in the real world) (Mahowald et al. 2023). They excel in analogical and moral \nreasoning tasks but perform poorly on spatial reasoning tasks (Agrawal 2023).\nSecond, while LLMs have accelerated the use of AI in clinical and counseling psycho -\ntherapy, privacy and ethical issues might arise (Graber-Stiehl 2023). For example, gate -\nkeepers, patients, and even mental health professionals who use GPT to assess suicide risk \nor improve decision-making might receive inaccurate assessments that underestimate risk \n(Elyoseph and Levkovich 2023) or bias clinician decision-making, which can lead to health-\ncare inequities (Pal et al. 2023). In addition, LLMs in psychiatry research and practice have \nbeen associated with potential bias and privacy violations (Zhong et al. 2023).\nThird, LLMs face application challenges in fields such as educational, developmental, \nand social and cultural psychology. It is evident that when applied in education, LLMs have \nthe potential for output bias and misuse (Kasneci et al. 2023). One study found that texts \n1 3\n305 Page 22 of 35\nExploring the frontiers of LLMs in psychological applications: a…\ngenerated by GPT were not always consistent or logical and sometimes even contradictory \n(Stojanov 2023). In the field of social and cultural psychology, LLMs exhibit cognitive \nbiases (Talboy and Fuller 2023) and cultural biases (Atari et al. 2023) similar to those of \nhumans, in addition to implicitly darker personality patterns (Li et al. 2022b). Bender et al. \n(2021) suggested that training data for LLMs might reflect social biases that continue to be \nperpetuated in research settings.\nFinally, LLMs have some limitations as aids to scientific research. With regard to writing, \nfor example, LLMs currently cannot fully replace humans. Instead, they answer questions \nand generate naturally flowing, informative content lacking real intelligence (Stokel-Walker \n2022). Although macrolanguage models can simulate human judgment when used as exper-\nimental subjects, there are still limits to their “understanding” of human thought (Dillion et \nal. 2023). Van Dis et al. (2023) noted that LLMs might accelerate innovation, shorten publi-\ncation times, and increase scientific diversity and equality. However, they might also reduce \nthe quality and transparency of research and fundamentally alter scientists’ autonomy as \nhuman researchers.\nIn summary, while LLMs offer extraordinary capabilities for psychological research, they \nalso present challenges related to bias, ethical issues, data security, transparency, and tech -\nnical expertise. Researchers should be fully aware of these challenges when using LLMs \nand adopt the following steps for ethical use: First, disclose model details and methods \ntransparently to ensure reproducibility. Second, verify outputs against literature or experts \nto address inaccuracies and misinformation. Third, use diverse training data to reduce cul -\ntural or gender biases. Fourth, in sensitive areas like mental health, limit use to assist—not \nreplace—judgment and train users to interpret outputs critically. These steps, supported \nby recent studies (Abdurahman et al. 2024; Guo et al. 2024; Porsdam Mann et al. 2024), \naddress ethical concerns in psychological research. Table 6 summarizes the challenges and \nlimitations of LLMs in psychological applications.\n7.2 Future directions and emergent trends\nCurrently, LLMs are used in different areas of psychology, including cognitive and behav -\nioral, clinical and counseling, educational and developmental, and social and cultural psy -\nchology. As the capabilities of LLMs are further enhanced, their potential applications in \npsychology will continue to develop.\nFirst, in the field of cognitive and behavioral psychology, with the emergence of multi -\nmodal LLMs (OpenAI 2023), it is possible to combine visual and auditory information with \ntextual data to better understand and model emotions, behaviors, and mental states for cog-\nnition. However, neuroimaging data can be used to inform the architectures and parameters \nof LLMs and integrate that information with traditional textual data to create more accurate \nand biologically sound models of human language and thought.\nSecond, in the field of clinical and counseling psychology, on the one hand, personal \ndata, such as social media posts, medical records, or wearable device data, can be used to \ncreate tailored, personalized LLMs that provide more accurate and relevant insights into an \nindividual’s state of mind. At the same time, the strengths of human clinical and counseling \nexpertise can be combined with the scalability and computational power of LLMs to create \nnew diagnostic treatment and intervention tools. In addition, in educational and develop -\nmental psychology and social and cultural psychology, it is essential to build ethical LLMs \n1 3\nPage 23 of 35 305\nL. Ke et al.\nChallenges Author Details\nCognitive and Behavioral Psychology\nLack of Real-World \nUnderstanding\nMitchell \n(2023)\nLLMs lack real-world understanding, abstract reasoning, and intent \ncomprehension.\nLack of \nMeta-knowledge\nStella et al. \n(2023)\nLLMs fabricate information (hallucination) and lack curiosity/\nmeta-knowledge.\nCausal Reasoning and \nCreativity\nSartori and \nOrrù (2023)\nPoor causal reasoning, dependence on biased training data, lack of \ncreativity and imagination.\nMulti-Step Reasoning \nLimitations\nGoertzel \n(2023)\nPoor multi-step reasoning, lack of autonomy, poor real-world \nunderstanding.\nCommon Sense \nReasoning\nPeng et al. \n(2023)\nForgetting knowledge in new tasks, poor common-sense reasoning, \ninconsistent problem-solving.\nModel Behavior \nChallenges\nHoltzman \net al. \n(2023)\nLack of interpretability and formal behavioral descriptions makes \nsystematic analysis difficult.\nPsycholinguistic \nFeatures\nSeals and \nShalin \n(2023)\nGPT and human-generated analogies differed in these stylistic \ndimensions, these lexical features, their choice of words for these \nfeatures and these devices that help readers understand text. GPT \nmay lack human cognitive and psycholinguistic features when \ngenerating analogies.\nClinic and Counseling Psychology\nTechnical Limita-\ntions& Patient Con-\nnection Issues\nStade et al. \n(2023)\nDifficulty assessing suicide risk, substance abuse, safety issues, \nand interpreting nonverbal cues& Problems forming therapeutic \nrelationships, interpreting nonverbal behaviors.\nEducation and Development Psychology\nIntegrity and Ethics \nIssues\nLi et al. \n(2023)\nAcademic integrity concerns, misinformation, data privacy, and \nimpact on critical thinking.\nBias and Over-Reli-\nance& Multilingual \nSupport Challenges\nKasneci et \nal. (2023)\nInsufficient personalization, bias in teaching, over-reliance on \nmodels reduces creativity.& Limited support for diverse languages \nand equitable access.\nSocial and Culture Psychology\nLiability and Privacy \nIssues\nFecher et \nal. (2023)\nLiability issues: challenging traditional mechanisms of authorship \nand liability. Bias issues: affecting the objectivity and impartial-\nity of science. Privacy and data protection issues: may be privacy \nissues with the training data of LLMs. Intellectual property issues: \npotential legal disputes. Environmental issues: generating large \namounts of carbon emissions, which can have a negative impact on \nthe environment.\nGlobal Diversity \nIgnorance\nAtari et al. \n(2023)\nIgnoring global psychological diversity (e.g., tend to favor the \npsychological characteristics of WEIRD societies) and which can \nlead to prejudice and discrimination against people of other cultures \nand backgrounds. Differences in values and moral judgments and \nwhich can lead to problems of communication and understanding \nin multicultural societies. Self-identity and perceived social roles \nand which may lead to stereotypes and misconceptions about non-\nWEIRD populations).\nCultural and Ethical \nTensions\nPark et al. \n(2024a, b)\nReduced innovation and development, bias and discrimination, \nculture clash and conflict, differences in values and morals and \nentrenchment of the status quo.\nSocial Context \nLimitations\nSalah et al. \n(2023)\nLimited understanding of social context: Although GPT performs \nwell in syntax and general semantics, it still has limitations in \ncapturing the nuances of social language. Ethical challenges: AI-\ngenerated fake content can lead to ethical issues including digital \npersonhood, informed consent, potential manipulation, and the \nimplications of using AI to simulate human interactions.\nTable 6 Challenges and limitations of LLMs in psychological applications\n1 3\n305 Page 24 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nChallenges Author Details\nBias and Misleading \nOutputs\nHayes \n(2023)\nPotential biases: if the training data contain biases, LLMs may learn \nand replicate them. Data privacy and consent issues: Text generated \nusing LLMs may involve data privacy and consent issues. Output \nmay be non-humanly understandable: although LLMs generate text \nthat closely resembles human language, they do not truly under-\nstand the content and may generate absurd or misleading responses.\nTraining Data Bias Miotto et \nal. (2022)\nBias and discrimination: LLMs may be affected by biases in the \ntraining data, which can produce unfair results, such as reinforcing \nsexism in the translation of job advertisements. Responsibility and \ncontrol: Due to the complexity of language models, it is difficult to \ndetermine who is responsible for the model’s output, which can lead \nto attribution of problems and lack of controls.\nPropagation of Harm Bender et \nal. (2021)\nPotential Harm: LLMs may lead to the propagation of harmful ideas \nsuch as stereotyping, discrimination, and extremism, and may lead \nto misinformation and bullying when generating text. Data bias and \nunfairness: leading to potential harm to marginalized communities. \nAutomating bias: exacerbating existing biases and discrimination. \nEnhancement of authoritative viewpoints: LLMs may reinforce \ndominant viewpoints in the training data, further undermining \nmarginalized people.\nAlignment Challenges Tamkin et \nal. (2021)\nAlignment: In order to better align models with human values, \nalgorithmic improvements are needed to increase factual accuracy \nand robustness against adversarial samples. In addition, appropriate \nvalues need to be made explicit for different usage scenarios. Soci-\netal Impact: Widespread use of LLMs may lead to problems such as \ninformation leakage and amplification of bias.\nMisuse of LLMs Brown et \nal. (2020)\nMisuse of language modeling: GPT-3 may be used to generate fake \nnews, spread extremist ideas, conduct cyber-attacks and other mali-\ncious uses. Fairness, bias, and representation: GPT-3 may carry bias \nagainst gender, race, and religion, among others, sparking related \ncontroversies. News generation: News generated by GPT-3 may \nbe difficult to distinguish from real news, leading to confusing and \nmisleading information.\nResearch Tools\nPlagiarism and Copy-\nright Issues\nSallam \n(2023)\nPlagiarism: content generated by GPT may be considered plagia-\nrized, violating academic norms. Copyright issues: Is the generated \ncontent owned by GPT or by the user? Transparency issues: The \nworkings of GPT may not be transparent, making it difficult for \nusers to understand the source of generated content. Liability issues: \nwho is responsible for GPT when generating incorrect content?\nTransparency \nLimitations\nGupta et al. \n(2023)\nTransparency and Explanation: The working mechanism of genera-\ntive AI models may be difficult to explain, which may lead users \nto doubt the credibility of the generated content. Legal and Ethical \nIssues: Generative AI models may involve intellectual property, \nprivacy, and ethical issues, requiring attention to compliance with \nrelevant laws and regulations during use.\nAcademic Integrity \nConcerns\nDergaa et \nal. (2023)\nIntegration of erroneous or biased information. Problems with cit-\ning original sources and authors. Impact on academic integrity and \nquality. Increased inequity and inequality: Difficulty in recognizing \nAI-generated content. Academic evaluation and recognition issues. \nDirect replacement for academic researchers: GPT is not a complete \nreplacement for academic researchers as it has limitations in certain \ntypes of academic research.\nTable 6 (continued) \n1 3\nPage 25 of 35 305\nL. Ke et al.\nand ensure they are designed and deployed in a way that respects privacy and uses data \nfairly and responsibly.\nUltimately, LLMs represent a systematic project whose future development cannot be \nachieved without the interdisciplinary collaboration of researchers in diverse fields such as \npsychology, computer science, and linguistics. For psychology researchers, accessible open-\nsource LLM frameworks and tools might become an integral part of their future research \nefforts. Table 7 summarizes LLMs’ future directions and emergent trends in psychological \napplication.\n8 Conclusion\nWith the rapid development of AI technologies, especially the continuous advancement of \nLLMs, machine learning has reached the point where it can recognize and generate human \nlanguage. This development is not simply a technological breakthrough for the field of psy-\nchology, but it opens the door to a range of potential applications.\nFirst, in the field of cognitive and behavioral psychology, LLMs are excelling in a vari -\nety of cognitive tasks. Although there are still limitations in causal cognition and planning, \nthese models resurrect the principle of association, demonstrating the ability to associate \nat a distance and reason in complex ways. At the same time, the ability to adapt LLMs to \ncognitive models is a significant strength of psychological research, allowing for new explo-\nrations of human cognitive and behavioral processing mechanisms.\nSecond, in clinical and counseling psychology, LLMs can be used as preliminary diagnos-\ntic tools for mental health. While traditional mental health diagnosis relies on the experience \nof professionals and direct interaction with patients, LLMs can quickly identify potential \nmental health problems, such as depression and anxiety, by analyzing an individual’s verbal \nexpressions and textual content. Importantly, while such diagnoses cannot wholly replace \nprofessional psychological assessment, they can serve as an effective adjunct to help psy -\nchologists understand a patient’s condition more quickly, or play a role in primary mental \nhealth interventions. Meanwhile, personalized psychological intervention is another critical \napplication direction for LLMs. By combining information about an individual’s health data \nand lifestyle habits, these models can provide tailored psychological advice and intervention \nprograms. Such personalized approaches could be crucial for improving the effectiveness of \npsychological interventions.\nChallenges Author Details\nPrivacy and Bias \nRisks\nPeters \nand Matz \n(2023)\nUser privacy: LLMs can infer psychological traits from a user’s \nsocial media data, which may violate the user’s privacy. Potential \nbias: LLMs may create potential bias in the inference process, \nwhich may lead to unfair treatment of specific groups (e.g., gender, \nage, etc.). Data security: if the inferential power of LLMs is used \nmaliciously, it may lead to data leakage, with serious implications \nfor users’ mental health.\nMisconduct and \nLimitations\nY . Liu et al. \n(2023c)\nAcademic misconduct: GPT may be used for academic cheating, \nsuch as generating false papers or assignments. Challenges in the \nmedical field: GPT has limitations in medical image analysis, which \nmay lead to wrong diagnosis and jeopardize patients’ health.\nTable 6 (continued) \n1 3\n305 Page 26 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nAuthor Future directions and emergent trends\nCognition and Behavior\nD’Oria (2023) Delving into Human-Computer Interaction (HCI) to understand AI’s ability to mimic \nhuman behavior. Exploring how AI language modeling can be applied in the human sci-\nences to improve research efficiency and quality\nCrockett \nand Messeri \n(2023)\nFocus on the costs of adopting alternative human narratives in cognitive science research, \nsuch as masking the human labor behind them and the impact on human well-being. Con-\ncern about the impact of technological developments on scientific work and human un-\nderstanding to ensure that cognitive scientists remain proactive in technological advances.\nBinz and \nSchulz \n(2023b)\nExplore ways to make LLMs more stable and robust in the face of descriptive tasks.\nInvestigate whether LLMs can learn to explore purposefully and how to better utilize \ncausal knowledge in tasks. Analyze the performance of LLMs in different tasks and \ncontexts to see if they can adapt like humans. Explore how LLMs develop and refine their \ncognitive abilities during natural interactions with humans.\nHuang and \nChang (2022)\nImprove the reasoning ability of LLMs to encourage reasoning by optimizing training \ndata, model architecture, and optimization goals. Develop more appropriate evaluation \nmethods and benchmarks to measure the reasoning ability of LLMs to better reflect the \ntrue reasoning ability of the models. Investigate the potential of LLMs in different appli-\ncations (e.g., problem solving, decision making and planning tasks). Explore other forms \nof reasoning (e.g., inductive and retrospective reasoning).\nClinic and Counseling\nAbd-Alrazaq \net al. (2019)\nDevelop more chatbots for people with mental illness, especially for those with disorders \nsuch as schizophrenia, obsessive-compulsive disorder and bipolar disorder.\nImplement more chatbots in developing countries to address the shortage of mental health \nprofessionals. Conduct more randomized controlled trials to evaluate the effectiveness of \nchatbots in mental health.\nStade et al. \n(2023)\nDeveloping new therapeutic techniques and evidence-based practices (EBPs). Focus on \nevidence-based practices first: to create meaningful clinical impact in the short term, clin-\nical LLM applications based on existing evidence-based psychotherapies and techniques \nwill have the greatest chance of success. Involve interdisciplinary collaboration. Focuses \non therapist and patient trust and usability. Criteria for designing effective clinical LLMs.\nDemszky et \nal. (2023)\nDevelopment of high-quality cornerstone datasets: these datasets need to encompass pop-\nulations and psychological constructs of interest and be associated with psychologically \nimportant outcomes (e.g., actual behaviors, mindfulness, health, and mental well-being). \nFocus on future research directions in consumer neuroscience and clinical neuroscience: \nresearch in these areas may involve the neural systems of marketing-related behaviors, \ndecision neuroscience, neuroeconomics, and more.\nEducation and Development\nHagendorff \n(2023)\nDevelopmental psychology: examining how LLMs develop cognitively, socially, and \nemotionally over the lifespan and how these models can be optimized for specific tasks \nand situations. Learning psychology: studying how LLMs acquire and retain knowledge \nand skills, and how to optimize these models to improve learning.\nSociety and Culture\nSap et al. \n(2022)\nExplore more interactive and empirical training methods to help LLMs acquire true \nsocial intelligence and theoretical mental abilities. Investigate ways to combine static text \nwith rich social intelligence and interaction data to improve social intelligence in LLMs. \nInvestigate the theoretical-psychological abilities of LLMs in more naturalistic settings to \nreveal their performance in real-world scenarios.\nArgyle et al. \n(2022)\nInvestigate the algorithmic fidelity of the GPT-3 model and how appropriate conditioning \ncan allow the model to accurately simulate the response distributions of various human \nsubgroups. Created “in silico samples” by conditioning on the socio-demographic back-\ngrounds of real human participants in multiple large U.S. surveys.\nTable 7 Future directions and emergent trends of LLMs in psychological applications\n1 3\nPage 27 of 35 305\nL. Ke et al.\nThird, LLMs have the same potential for application in both educational and develop -\nmental psychology and social and cultural psychology. For example, LLMs provide inter -\nactive and personalized learning experiences or generate research tasks based on real-life \napplications that increase motivation and enhance learning. In addition, by analyzing large \namounts of social media data, these models can help researchers track and analyze public \nsentiment changes to better understand psycho-social dynamics.\nFinally, in psychological research, LLMs can drastically improve research efficiency. \nResearchers can use these models to quickly organize and analyze large amounts of litera -\nture, thus saving time. These models can also assist with experimental design, data analysis, \nand even promoting scholarly communication, making psychological research more effi -\ncient and precise.\nIn light of the above, LLMs have promising applications for psychology, such as research \nsupport, cognitive modeling, individualized intervention, and personalized learning. LLMs \nalso have the potential to dramatically improve our understanding of human communica -\ntion, thought processes, and behaviors, leading to the development of more comprehensive \ntheories of mind and cognitive science. At the same time, it is important to be aware of the \nrelated risks and challenges and to ensure adherence to ethical standards, especially with \nregard to individual privacy and data security. It is also important to recognize that no mat-\nter how technologically advanced they are, LLMs can only partially replace the judgment \nand experience of human professionals. Therefore, such models should be viewed as an aid \nrather than an all-in-one solution.\nAcknowledgments The authors thank Zengyi Li (NEOMA Business School), Wenqing Zhang (Tsinghua \nUniversity), and Wenxu Li (King’s College London) for their assistance and support during the preparation \nof this study.\nAuthor Future directions and emergent trends\nSchaaff et al. \n(2023)\nDeveloping more advanced models: to more accurately capture the emotional context \nof conversations and improve emotional understanding and expression. Measuring the \nemotional capabilities of bots: to investigate how to assess the emotional capabilities of \nchatbots in order to better understand how they behave when interacting with humans. \nExplore the use of GPT as a support tool: investigate how GPT can be used to support \npeople more empathetically and improve human well-being.\nZiems et al. \n(2023)\nCross-cultural CSS research: future research should separately consider the utility of \nLLMs for cross-cultural CSS in order to better serve social science research in different \ncultural contexts. Future research could explore contrastive or causal explanations in \nLLMs. New paradigms for social science and AI collaboration.\nResearch Tools\nVan Dis et al. \n(2023)\nInvest in truly open LLMs: develop and implement open-source AI technologies to \nincrease transparency and democratic control. Embrace the advantages of AI: utilize AI to \naccelerate innovation and breakthroughs at all academic stages, while focusing on issues \nof ethics and human autonomy. Broaden the discussion: organize international forums \nto discuss the development and responsible use of LLMs in research, including issues of \ndiversity and inequality.\nFecher et al. \n(2023)\nAnalyzing the risks and opportunities of LLMs for science systems. Examining how \nLLMs affect academic quality assurance mechanisms, academic misconduct, and scien-\ntific integrity. Exploring the impact of LLMs on academic reputation, evaluation systems, \nand knowledge dissemination. Examining how to balance the potential benefits from \nLLMs with adherence to scientific principles.\nTable 7 (continued) \n1 3\n305 Page 28 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nAuthor contributions L.K. conceived the study’s main conceptual ideas, wrote the main manuscript text; S.T. \ndesigned the framework, provided supervision and manuscript’s revision,; P. C. assisted in the formulation \nof research objectives and goals. K.P. provided supervision and guidance throughout the project. All authors \nreviewed the manuscript.\nFunding This work was supported by the China Research Institute for Science Popularization Project: \n“Exploration of the Theoretical Framework and Practical Pathways for Science Popularization Services in \nHigh-Quality Development” (Grant No. 240212); the Tsinghua University Initiative Scientific Research Pro-\ngram (Grant No. 20213080008); and self-funded projects of the Institute for Global Industry, Tsinghua Uni-\nversity (Grant Nos. 202-296-001, 2024-06-18-LXHT002, and 2024-09-23-LXHT008).\nData availability No datasets were generated or analysed during the current study.\nDeclarations\nConflict of interest The authors declare no competing interests.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons \nlicence, and indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted \nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAbd-Alrazaq AA, Alajlani M, Alalwan AA, Bewick BM, Gardner P, Househ M (2019) An overview of the \nfeatures of chatbots in mental health: A scoping review. Int J Med Informatics 132:103978.  h t t p s :  / / d o i  . \no r g / 1  0 . 1 0  1 6 / j .  i j m e d  i n f . 2 0  1 9 . 1  0 3 9 7 8\nAbdurahman S, Atari M, Karimi-Malekabadi F, Xue MJ, Trager J, Park PS, Dehghani M (2024) Perils and \nopportunities in using large Language models in psychological research. PNAS nexus 3(7):pgae245\nAbramski K, Citraro S, Lombardi L, Rossetti G, Stella M (2023) Cognitive network science reveals bias in \ngpt-3, gpt-3.5 turbo, and gpt-4 mirroring math anxiety in high-school students. Big Data Cogn Comput \n7(3):124\nAgrawal S (2023) Are LLMs the master of all trades? Exploring Domain-Agnostic reasoning skills of LLMs. \nArXiv Preprint. https://doi.org/10.48550/arxiv.2303.12810\nAher G, Arriaga RI, Kalai AT (2023) Using large language models to simulate multiple humans and replicate \nhuman subject studies Proceedings of the 40th international conference on machine learning, Honolulu, \nHawaii, USA\nAkata E, Schulz L, Coda-Forno J, Oh SJ, Bethge M, Schulz E (2023) Playing repeated games with large \nlanguage models. arXiv preprint. https://doi.org/10.48550/arXiv.2305.16867\nAli JKM, Shamsan MAA, Hezam TA, Mohammed AAQ (2023) Impact of ChatGPT on learning motivation. \nJ Engl Stud Arabia Felix 2(1):41–49. https://doi.org/10.56540/jesaf.v2i1.51\nAlmeida GF, Nunes JL, Engelmann N, Wiegmann A, de Araújo M (2024) Exploring the psychology of llms’ \nmoral and legal reasoning. Artif Intell 333:104145\nArgyle LP, Busby EC, Fulda N, Gubler J, Rytting C, Wingate D (2022) Out of one, many: using language \nmodels to simulate human samples. arXiv preprint. https://doi.org/10.48550/arXiv.2209.06899\nAtari M, Xue MJ, Park PS, Blasi DE, Henrich J (2023) Which humans? PsyArXiv preprint.  h t t p s : / / d o i . o r g / \n1 0 . 3 1 2 3 4 / o s f . i o / 5 b 2 6 t       \nAydın Ö, Karaarslan E (2022) OpenAI ChatGPT generated literature review: digital twin in healthcare. \nEmerg Comput Technol 222–31. https://doi.org/10.2139/ssrn.4308687\nBaillifard A, Gabella M, Lavenex PB, Martarelli CS (2024) Effective learning with a personal AI tutor: a case \nstudy. Educ Inform Technol 1–16. https://doi.org/10.1007/s10639-024-12888-5\n1 3\nPage 29 of 35 305\nL. Ke et al.\nBanker S, Chatterjee P, Mishra H, Mishra A (2024) Machine-assisted social psychology hypothesis genera -\ntion. Am Psychol 79(6):789\nBašić Ž, Banovac A, Kružić I, Jerković I (2023) ChatGPT-3.5 as writing assistance in students’ essays. \nHumanit Soc Sci Commun 10(1). https://doi.org/10.1057/s41599-023-02269-7\nBender EM, Gebru T, McMillan-Major A, Shmitchell S (2021) On the dangers of stochastic parrots: can \nlanguage models be too big? Proceedings of the 2021 ACM conference on fairness, accountability, and \ntransparency, virtual event, Canada. https://doi.org/10.1145/3442188.3445922\nBinz M, Schulz E (2023a) Turning large language models into cognitive models. ArXiv Preprint.  h t t p s : / / d o i \n. o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 6 . 0 3 9 1 7       \nBinz M, Schulz E (2023b) Using cognitive psychology to understand GPT-3. Proc Natl Acad Sci USA \n120(6):e2218523120. https://doi.org/10.1073/pnas.2218523120\nBlyler AP, Seligman MEP (2023a) AI assistance for coaches and therapists. J Posit Psychol 1–13.  h t t p s :  / / d o i  \n. o r g / 1  0 . 1 0  8 0 / 1 7  4 3 9 7 6  0 . 2 0 2 3  . 2 2 5  7 6 4 2\nBlyler AP, Seligman MEP (2023b) Personal narrative and stream of consciousness: an AI approach. J Posit \nPsychol 1–7.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  8 0 / 1 7  4 3 9 7 6  0 . 2 0 2 3  . 2 2 5  7 6 6 6\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell \nA, Agarwal S, Herbert-V oss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler DM, Wu J, Winter \nC, Amodei D (2020) Language models are Few-Shot learners. ArXiv Preprint.  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 \n0 / a r X i v . 2 0 0 5 . 1 4 1 6 5       \nBubeck S, Chandrasekaran V , Eldan R, Gehrke J, Horvitz EK, Kamar E, Lee P, Lee YT, Li Y-F, Lundberg \nSM, Nori H, Palangi H, Ribeiro MT, Zhang Y (2023) Sparks of artificial general intelligence: early \nexperiments with GPT-4. ArXiv Preprint. https://doi.org/10.48550/arXiv.2303.12712\nCharness G, Jabarian B, List JA (2023) Generation next: experimentation with AI\nChiang C-H, Lee H-y (2023) Can large Language models be an alternative to human evaluations? ArXiv \nPreprint. https://doi.org/10.48550/arXiv.2305.01937\nCrockett M, Messeri L (2023) Should large Language models replace human participants? PsyArXiv Pre -\nprint. https://doi.org/10.31234/osf.io/4zdx9\nD’Oria M (2023) Can AI Language models improve human sciences research? A phenomenological analysis \nand future directions. Encyclopaideia 27(66):77–92. https://doi.org/10.6092/issn.1825-8670/16554\nD’Souza RF, Amanullah S, Mathew M, Surapaneni KM (2023) Appraising the performance of ChatGPT in \npsychiatry using 100 clinical case vignettes. Asian J Psychiatry 89:103770\nDe Bot K, Lowie W, Verspoor M (2007) A dynamic systems theory approach to second Language acquisition. \nBiling Lang Cogn 10(1):7–21. https://doi.org/10.1017/S1366728906002732\nDemszky D, Yang D, Yeager DS, Bryan CJ, Clapper M, Chandhok S, Eichstaedt JC, Hecht C, Jamieson J, \nJohnson M, Jones M, Krettek-Cobb D, Lai L, JonesMitchell N, Ong DC, Dweck CS, Gross JJ, Pen -\nnebaker JW (2023) Using large Language models in psychology. Nat Reviews Psychol 2(11):688–701. \nhttps://doi.org/10.1038/s44159-023-00241-5\nDergaa I, Chamari K, Zmijewski P, Ben Saad H (2023) From human writing to artificial intelligence gener -\nated text: examining the prospects and potential threats of ChatGPT in academic writing. Biology Sport \n40(2):615–622.  h t t p s :  / / d o i  . o r g / 1  0 . 5 1  1 4 / b i  o l s p o  r t . 2 0 2  3 . 1 2  5 6 2 3\nDhingra S, Singh M, Sb V , Malviya N, Gill S (2023) Mind meets machine: Unravelling GPT-4’s cognitive \npsychology. arXiv preprint, arXiv:2303.11436. https://doi.org/10.48550/arXiv.2303.11436\nDillion D, Tandon N, Gu Y , Gray K (2023) Can AI Language models replace human participants? Trends \nCogn Sci 27(7):597–600. https://doi.org/10.1016/j.tics.2023.04.008\nDu Y , Luo D, Yan R, Wang X, Liu H, Zhu H, Song Y , Zhang J (2024) Enhancing job recommendation \nthrough llm-based generative adversarial networks. Proceedings of the AAAI Conference on Artificial \nIntelligence\nDubey R, Hardy MD, Griffiths TL, Bhui R (2024) AI-generated visuals of car-free US cities help improve \nsupport for sustainable policies. Nat Sustain 7(4):399–403\nElyoseph Z, Levkovich I (2023) Beyond human expertise- the promise and limitations of ChatGPT in suicide \nrisk assessment. Front Psychiatry 14. https://doi.org/10.3389/fpsyt.2023.1213141\nElyoseph Z, Levkovich I (2024) Comparing the perspectives of generative AI, mental health experts, and the \ngeneral public on schizophrenia recovery: case vignette study. Jmir Mental Health 11:e53043\nFecher B, Hebing M, Laufer M, Pohle J, Sofsky F (2023) Friend or foe? Exploring the implications of large \nLanguage models on the science system. AI Soc. https://doi.org/10.1007/s00146-023-01791-1\nFloridi L, Chiriatti M (2020) GPT-3: its nature, scope, limits, and consequences. Mind Mach 30(4):681–694. \nhttps://doi.org/10.1007/s11023-020-09548-1\nFrank MC (2023) Baby steps in evaluating the capacities of large language models. Nat Reviews Psychol \n2(8):451–452. https://doi.org/10.1038/s44159-023-00211-x\nGhafouri M (2024) ChatGPT: the catalyst for teacher-student rapport and grit development in L2 class. \nSystem 120:103209\n1 3\n305 Page 30 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nGhafouri M, Hassaskhah J, Mahdavi-Zafarghandi A (2024) From virtual assistant to writing mentor: explor-\ning the impact of a ChatGPT-based writing instruction protocol on EFL teachers’ self-efficacy and learn-\ners’ writing skill. Lang Teach Res. https://doi.org/10.1177/13621688241239764\nGlaser R (1984) Education and thinking: the role of knowledge. Am Psychol 39(2):93\nGoertzel B (2023) Generative AI vs. AGI: the cognitive strengths and weaknesses of modern LLMs. ArXiv \nPreprint. https://doi.org/10.48550/arXiv.2309.10371\nGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2020) \nGenerative adversarial networks. Commun ACM 63(11):139–144\nGraber-Stiehl I (2023) Is the world ready for AI-powered therapy? Nature 617:22–24.  h t t p s : / / d o i . o r g / 1 0 . 1 0 \n3 8 / d 4 1 5 8 6 - 0 2 3 - 0 1 4 7 3 - 4       \nGrossmann I, Feinberg M, Parker DC, Christakis NA, Tetlock PE, Cunningham WA (2023) AI and the trans-\nformation of social science research. Science 380(6650):1108–1109.  h t t p s : / / d o i . o r g / 1 0 . 1 1 2 6 / s c i e n c e . a \nd i 1 7 7 8       \nGuo Z, Lai A, Thygesen JH, Farrington J, Keen T, Li K (2024) Large Language models for mental health \napplications: systematic review. JMIR Mental Health 11(1):1–20\nGupta M, Akiri C, Aryal K, Parker E, Praharaj L (2023) From ChatGPT to threatgpt: impact of generative AI in \ncybersecurity and privacy. IEEE Access 11:80218–80245. https://doi.org/10.1109/access.2023.3300381\nHagendorff T (2023) Machine psychology: investigating emergent capabilities and behavior in large Lan -\nguage models using psychological methods. ArXiv Preprint. https://doi.org/10.48550/arXiv.2303.13988\nHagendorff T (2024) Deception abilities emerged in large language models. Proc Natl Acad Sci \n121(24):e2317967121\nHagendorff T, Fabi S, Kosinski M (2023) Human-like intuitive behavior and reasoning biases emerged in \nlarge Language models but disappeared in ChatGPT. Nat Comput Sci 3(10):833–838.  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 3 5 8 8 - 0 2 3 - 0 0 5 2 7 - x       \nHarding J, D’Alessandro W, Laskowski NG, Long R (2023) AI Language models cannot replace human \nresearch participants. AI Soc. https://doi.org/10.1007/s00146-023-01725-x\nHardy M, Sucholutsky I, Thompson B, Griffiths T (2023) Large language models meet cognitive science: \nLlms as tools, models, and participants. Proceedings of the annual meeting of the cognitive science \nsociety\nHayes A (2023) Conversing with qualitative data: enhancing qualitative research through large Language \nmodels (LLMs). PsyArXiv Preprint. https://doi.org/10.31235/osf.io/yms8p\nHendel R, Geva M, Globerson A (2023) In-Context learning creates task vectors. ArXiv Preprint.  h t t p s : / / d o i \n. o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 1 0 . 1 5 9 1 6       \nHofmann SG, Asnaani A, V onk IJ, Sawyer AT, Fang A (2012) The efficacy of cognitive behavioral therapy: A \nreview of Meta-analyses. Cognit Ther Res 36(5):427–440. https://doi.org/10.1007/s10608-012-9476-1\nHoltzman A, West P, Zettlemoyer L (2023) Generative models as a complex systems science: how can we make \nsense of large Language model behavior? ArXiv Preprint. https://doi.org/10.48550/arXiv.2308.00189\nHothersall D, Lovett BJ (2022) History of psychology. Cambridge University Press\nHuang J, Chang KC-C (2022) Towards reasoning in large Language models: A survey. ArXiv Preprint. \nhttps://doi.org/10.48550/arXiv.2212.10403\nHutson M (2023) Doing research with human subjects is costly and cumbersome.can AI chatbots replace \nthem? Science 381(6654):121–123. https://doi.org/10.1126/science.adj6791\nJin C, Zhang S, Shu T, Cui Z (2023) The cultural psychology of large Language models: is ChatGPT a holis-\ntic or analytic thinker? ArXiv Preprint. https://doi.org/10.48550/arXiv.2308.14242\nJungherr A (2023) Using ChatGPT and other large language model (LLM) applications for academic paper \nassignments. https://doi.org/10.31235/osf.io/d84q6\nJyothy S, Kolil VK, Raman R, Achuthan K (2024) Exploring large Language models as an integrated tool \nfor learning, teaching, and research through the Fogg behavior model: a comprehensive mixed-methods \nanalysis. Cogent Eng 11(1):2353494\nKahneman D (2011) Thinking, fast and slow. Farrar, Straus and Giroux\nKasneci E, Sessler K, Küchemann S, Bannert M, Dementieva D, Fischer F, Gasser U, Groh G, Günnemann \nS, Hüllermeier E, Krusche S, Kutyniok G, Michaeli T, Nerdel C, Pfeffer J, Poquet O, Sailer M, Schmidt \nA, Seidel T, Kasneci G (2023) ChatGPT for good? On opportunities and challenges of large Language \nmodels for education. Learn Individual Differences 103. https://doi.org/10.1016/j.lindif.2023.102274\nKe L, Zhang G, He J, Li Y , Li Y , Liu X, Fang P (2023) Pilot selection in the era of virtual reality: algorithms \nfor accurate and interpretable machine learning models. Aerospace 10(5).  h t t p s : / / d o i . o r g / 1 0 . 3 3 9 0 / a e r o \ns p a c e 1 0 0 5 0 3 9 4       \nKosinski M (2024) Evaluating large Language models in theory of Mind tasks. Proc Natl Acad Sci \n121(45):e2405460121\nLamichhane B (2023) Evaluation of ChatGPT for NLP-based mental health applications. ArXiv Preprint. \nhttps://doi.org/10.48550/arXiv.2303.15727\n1 3\nPage 31 of 35 305\nL. Ke et al.\nLee BC, Chung J (2024) An empirical investigation of the impact of ChatGPT on creativity. Nat Hum Behav \n8(10):1906–1914\nLi J, Tang T, Zhao WX, Nie J-Y , Wen J-R (2022a) Pretrained Language Models for Text Generation: a Sur-\nvey. arXiv preprint, arXiv:2201.05273. https://doi.org/10.48550/arXiv.2201.05273\nLi X, Li Y , Liu L, Bing L, Joty S (2022b) Does GPT-3 demonstrate psychopathy? Evaluating large Language \nmodels from a psychological perspective. ArXiv Preprint. https://doi.org/10.48550/arXiv.2212.10529\nLi M, Enkhtur A, Cheng F, Yamamoto BA (2023) Ethical implications of ChatGPT in higher education: A \nscoping review. ArXiv Preprint. https://doi.org/10.48550/arXiv.2311.14378\nLi T, Lu J, Chu C, Zeng T, Zheng Y , Li M, Huang H, Wu B, Liu Z, Ma K (2024) Scisafeeval: a compre -\nhensive benchmark for safety alignment of large language models in scientific tasks. arXiv preprint \narXiv:2410.03769\nLiu X, Ji K, Fu Y , Tam W, Du Z, Yang Z, Tang J (2022) P-Tuning: Prompt Tuning Can Be Comparable to \nFine-tuning Across Scales and Tasks. Proceedings of the 60th Annual Meeting of the Association for \nComputational Linguistics (V olume 2: Short Papers), Dublin, Ireland\nLiu JM, Li D, Cao H, Ren T, Liao Z, Wu J (2023a) ChatCounselor: A large Language models for mental \nhealth support. ArXiv Preprint. https://doi.org/10.48550/arXiv.2309.15461. arXiv:2309.15461\nLiu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G (2023b) Pre-train, prompt, and predict: A systematic \nsurvey of prompting methods in natural Language processing. ACM-CSUR 55(9):1–35.  h t t p s : / / d o i . o \nr g / 1 0 . 1 1 4 5 / 3 5 6 0 8 1 5       \nLiu Y , Han T, Ma S, Zhang J, Yang Y , Tian J, He H, Li A, He M, Liu Z, Wu Z, Zhao L, Zhu D, Li X, Qiang \nN, Shen D, Liu T, Ge B (2023c) Summary of ChatGPT-Related research and perspective towards the \nfuture of large Language models. Meta-Radiology 1(2). https://doi.org/10.1016/j.metrad.2023.100017\nLoconte R, Orrù G, Tribastone M, Pietrini P, Sartori G (2023) Challenging chatgpt’s intelligence with human \ntools: A neuropsychological investigation on prefrontal functioning of a large Language model. SSRN \nPreprint. https://doi.org/10.2139/ssrn.4471829\nLogacheva E, Hellas A, Prather J, Sarsa S, Leinonen J (2024) Evaluating Contextually Personalized Pro -\ngramming Exercises Created with Generative AI. Proceedings of the 2024 ACM Conference on Inter -\nnational Computing Education Research-V olume 1\nLuo X, Chen F, Zhu D, Wang L, Wang Z, Liu H, Lyu M, Wang Y , Wang Q, Chen Y (2024) Potential roles \nof large Language models in the production of systematic reviews and Meta-Analyses. J Med Internet \nRes 26:e56780\nMachin MA, Machin TM, Gasson N (2024) Comparing ChatGPT with experts’ responses to scenarios that \nassess psychological literacy. Psychol Learn Teach. https://doi.org/10.1177/14757257241241592\nMahowald K, Ivanova AA, Blank IA, Kanwisher N, Tenenbaum JB, Fedorenko E (2023) Dissociating Lan-\nguage and thought in large Language models: a cognitive perspective. ArXiv Preprint.  h t t p s : / / d o i . o r g / 1 \n0 . 4 8 5 5 0 / a r X i v . 2 3 0 1 . 0 6 6 2 7       \nMarjieh R, Sucholutsky I, Rijn Pv, Jacoby N, Griffiths TL (2023) Large Language models predict human \nsensory judgments across six modalities. ArXiv Preprint. https://doi.org/10.48550/arXiv.2302.01308\nMichelson M, Reuter K (2019) The significant cost of systematic reviews and meta-analyses: A call for \ngreater involvement of machine learning to assess the promise of clinical trials. Contemp Clin Trials \nCommun 16:100443. https://doi.org/10.1016/j.conctc.2019.100443\nMiotto M, Rossberg N, Kleinberg B (2022) Who is GPT-3? An exploration of personality, values and demo-\ngraphics. ArXiv Preprint. https://doi.org/10.48550/arXiv.2209.14338\nMitchell M (2023) AI’s challenge of understanding the world. Science 382(6671).  h t t p s : / / d o i . o r g / 1 0 . 1 1 2 6 / s \nc i e n c e . a d m 8 1 7 5       \nMukherjee A, Chang, H H (2024) Heuristic reasoning in AI: Instrumental use and mimetic absorption. arXiv \npreprint. https://doi.org/10.48550/arXiv.2403.09404\nNazario-Johnson L, Zaki HA, Tung GA (2023) Use of large Language models to predict neuroimaging. J Am \nColl Radiol 20(10):1004–1009. https://doi.org/10.1016/j.jacr.2023.06.008\nNewell A (1990) Unified theories of cognition. Harvard University Press\nNisbett RE, Peng K, Choi I, Norenzayan A (2001) Culture and systems of thought: holistic versus analytic \ncognition. Psychol Rev 108(2):291–310. https://doi.org/10.1037/0033-295x.108.2.291\nNoy S, Zhang W (2023) Experimental evidence on the productivity effects of generative artificial intelli -\ngence. Science 381(6654):187–192\nOpenAI (2023) GPT-4 technical report. ArXiv Preprint. https://doi.org/10.48550/arXiv.2303.08774\nOrru G, Piarulli A, Conversano C, Gemignani A (2023) Human-like problem-solving abilities in large Lan -\nguage models using ChatGPT. Front Artif Intell 6:1199350. https://doi.org/10.3389/frai.2023.1199350\nPal R, Garg H, Patel S, Sethi T (2023) Bias amplification in intersectional subpopulations for clinical phe -\nnotyping by large Language models. MedRxiv Preprint. https://doi.org/10.1101/2023.03.22.23287585\n1 3\n305 Page 32 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nPark B, Judd CM (2005) Rethinking the link between categorization and prejudice within the social cogni -\ntion perspective. Personality Social Psychol Rev 9(2):108–130.  h t t p s : / / d o i . o r g / 1 0 . 1 2 0 7 / s 1 5 3 2 7 9 5 7 p s p \nr 0 9 0 2 _ 2       \nPark JS, Popowski L, Cai C, Morris MR, Liang P, Bernstein MS (2022) Social simulacra: Creating populated \nprototypes for social computing systems. Proceedings of the 35th Annual ACM Symposium on User \nInterface Software and Technology\nPark PS, Schoenegger P, Zhu C (2024a) Diminished diversity-of-thought in a standard large Language \nmodel. Behav Res Methods 1–17. https://doi.org/10.3758/s13428-023-02307-x\nPark YJ, Kaplan D, Ren Z, Hsu C-W, Li C, Xu H, Li S, Li J (2024b) Can ChatGPT be used to generate sci -\nentific hypotheses? J Materiomics 10(3):578–584\nPatel SC, Fan J (2023) Identification and Description of Emotions by Current Large Language Models. \nbioRxiv preprint. https://doi.org/10.1101/2023.07.17.549421\nPeng Y , Han J, Zhang Z, Fan L, Liu T, Qi S, Feng X, Ma Y , Wang Y , Zhu S-C (2023) The Tong test: evaluating \nartificial general intelligence through dynamic embodied physical and social interactions. Engineering. \nhttps://doi.org/10.1016/j.eng.2023.07.006\nPeters H, Matz S (2023) Large Language models can infer psychological dispositions of social media users. \nArXiv Preprint. https://doi.org/10.48550/arXiv.2309.08631\nPorsdam Mann S, Vazirani AA, Aboy M, Earp BD, Minssen T, Cohen IG, Savulescu J (2024) Guidelines for \nethical use and acknowledgement of large Language models in academic writing. Nat Mach Intell 1–3. \nhttps://doi.org/10.1038/s42256-024-00922-7\nQureshi R, Shaughnessy D, Gill KAR, Robinson KA, Li T, Agai E (2023) Are ChatGPT and large Language \nmodels the answer to bringing Us closer to systematic review automation? Syst Reviews 12(1):72. \nhttps://doi.org/10.1186/s13643-023-02243-z\nRane N, Choudhary S, Rane J (2024) Gemini versus ChatGPT: applications, performance, architecture, capa-\nbilities, and implementation. Performance, Architecture, Capabilities, and Implementation (February \n13, 2024)\nRathje S, Mirea D-M, Sucholutsky I, Marjieh R, Robertson C, Bavel JJV (2023) GPT is an effective tool \nfor multilingual psychological text analysis. PsyArXiv Preprint. https://doi.org/10.31234/osf.io/sekf5\nSalah M, Halbusi A, H., Abdelfattah F (2023) May the force of text data analysis be with you: unleashing \nthe power of generative AI for social psychology research. Computers Hum Behavior: Artif Hum 1(2). \nhttps://doi.org/10.1016/j.chbah.2023.100006\nSallam M (2023) ChatGPT utility in healthcare education, research, and practice: systematic review on the \npromising perspectives and valid concerns. Healthc (Basel) 11(6).  h t t p s : / / d o i . o r g / 1 0 . 3 3 9 0 / h e a l t h c a r e 1 \n1 0 6 0 8 8 7       \nSap M, LeBras R, Fried D, Choi Y (2022) Neural Theory-of-Mind? On the Limits of Social Intelligence in \nLarge LMs. arXiv preprint. https://doi.org/10.48550/arXiv.2210.13312\nSartori G, Orrù G (2023) Language models and psychological sciences. Front Psychol 14.  h t t p s : / / d o i . o r g / 1 0 \n. 3 3 8 9 / f p s y g . 2 0 2 3 . 1 2 7 9 3 1 7       \nSchaaff K, Reinig C, Schlippe T (2023) Exploring ChatGPT’s empathic abilities. arXiv preprint.  h t t p s : / / d o i . \no r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 8 . 0 3 5 2 7       \nSchueller SM, Morris RR (2023) Clinical science and practice in the age of large Language models and \ngenerative artificial intelligence. J Consult Clin Psychol 91(10):559–561.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 7 / c c p \n0 0 0 0 8 4 8       \nSeals SM, Shalin VL (2023) Long-form analogies generated by ChatGPT lack human-like psycholinguistic \nproperties. ArXiv Preprint. https://doi.org/10.48550/arxiv.2306.04537\nSejnowski T (2022) Large language models and the reverse turing test. arXiv preprint.  h t t p s : / / d o i . o r g / 1 0 . 4 8 \n5 5 0 / a r x i v . 2 2 0 7 . 1 4 3 8 2       \nSha H, Mu Y , Jiang Y , Chen L, Xu C, Luo P, Li E, Tomizuka S, Zhan M, W., Ding M (2023) LanguageMPC: \nlarge language models as decision makers for autonomous driving. arXiv preprint, arXiv:2310.03026. \nhttps://doi.org/10.48550/arXiv.2310.03026\nSharma A, Lin IW, Miner AS, Atkins DC, Althoff T (2023) Human–AI collaboration enables more empathic \nconversations in text-based peer-to-peer mental health support. Nat Mach Intell 5(1):46–57.  h t t p s : / / d o i \n. o r g / 1 0 . 1 0 3 8 / s 4 2 2 5 6 - 0 2 2 - 0 0 5 9 3 - 2       \nSimon HA (1979) Information processing models of cognition. Ann Rev Psychol 30(1):363–396.  h t t p s :   /  / d o  i \n. o r  g /  1 0 .  1 1  4 6 /  a n n u  r  e v  . p   s . 3 0  . 0 2  0 1 7 9 . 0 0 2 0 5 1\nSrinivasan R, Inakoshi H, Uchino K (2023) Leveraging cognitive science for testing large language models. \n2023 IEEE International Conference On Artificial Intelligence Testing (AITest)\nStade EC, Stirman SW, Ungar L, Boland CL, Schwartz HA, Yaden DB, Sedoc J, Derubeis RJ, Willer R, Eich-\nstaedt JC (2023) Large language models could change the future of behavioral healthcare: A proposal \nfor responsible development and evaluation. PsyArXiv Preprint. https://doi.org/10.31234/osf.io/cuzvr\n1 3\nPage 33 of 35 305\nL. Ke et al.\nStella M, Hills TT, Kenett YN (2023) Using cognitive psychology to understand GPT-like models needs to \nextend beyond human biases. Proc Natl Acad Sci USA 120(43):e2312911120.  h t t p s : / / d o i . o r g / 1 0 . 1 0 7 3 \n/ p n a s . 2 3 1 2 9 1 1 1 2 0       \nStevenson C, Smal I, Baas M, Grasman R, Maas HVD (2022) Putting GPT-3’s Creativity to the (Alternative \nUses) Test. arXiv preprint. https://doi.org/10.48550/arXiv.2206.08932\nStojanov A (2023) Learning with ChatGPT 3.5 as a more knowledgeable other: an autoethnographic study. \nInt J Educational Technol High Educ 20(1). https://doi.org/10.1186/s41239-023-00404-7\nStokel-Walker C (2022) AI bot ChatGPT writes smart essays — should professors worry? Nature.  h t t p s : / / d o \ni . o r g / 1 0 . 1 0 3 8 / d 4 1 5 8 6 - 0 2 2 - 0 4 3 9 7 - 7       \nSufyan NS, Fadhel FH, Alkhathami SS, Mukhadi JY (2024) Artificial intelligence and social intelligence: \npreliminary comparison study between AI models and psychologists. Front Psychol 15:1353022\nSuri G, Slater LR, Ziaee A, Nguyen M (2024) Do large Language models show decision heuristics similar to \nhumans? A case study using GPT-3.5. J Exp Psychol Gen. https://doi.org/10.1037/xge0001547\nTajfel H (1982) Social psychology of intergroup relations. Ann Rev Psychol 33(1):1–39\nTalboy AN, Fuller E (2023) Challenging the appearance of machine intelligence: Cognitive bias in LLMs. \narXiv preprint. https://doi.org/10.48550/arXiv.2304.01358\nTamkin A, Brundage M, Clark J, Ganguli D (2021) Understanding the capabilities, limitations, and societal \nimpact of large language models. arXiv preprint. https://doi.org/10.48550/arXiv.2102.02503\nTaylor R, Kardas M, Cucurull G, Scialom T, Hartshorn A, Saravia E, Poulton A, Kerkez V , Stojnic R (2022) \nGalactica: a large language model for science. arXiv preprint. https://doi.org/10.48550/arXiv.2211.09085\nThirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW (2023) Large Language mod-\nels in medicine. Nat Med 29(8):1930–1940. https://doi.org/10.1038/s41591-023-02448-8\nTong S, Mao K, Huang Z, Zhao Y , Peng K (2024) Automating psychological hypothesis generation with AI: \nwhen large Language models Meet causal graph. Humanit Social Sci Commun 11(1):896.  h t t p s : / / d o i . o \nr g / 1 0 . 1 0 5 7 / s 4 1 5 9 9 - 0 2 4 - 0 3 4 0 7 - 5       \nTouvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y , Bashlykov N, Batra S, Bhargava P, Bhosale \nS (2023) Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288\nTrott S, Jones C, Chang T, Michaelov J, Bergen B (2023) Do large Language models know what humans \nknow? Cogn Sci 47(7):1–21\nVan Dis EA, Bollen J, Zuidema W, van Rooij R, Bockting CL (2023) ChatGPT: five priorities for research. \nNature 614(7947):224–226. https://doi.org/10.1038/d41586-023-00288-7\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention \nis all you need. Adv Neural Inf Process Syst 30\nVzorinab GD, Bukinichac AM, Sedykha A V , Vetrovab II, Sergienkob EA (2024) The emotional intelligence \nof the GPT-4 large language model. Psychol Russia: State Art 17(2):85–99\nWang H, Fu T, Du Y , Gao W, Huang K, Liu Z, Chandak P, Liu S, Van Katwyk P, Deac A, Anandkumar A, \nBergen K, Gomes CP, Ho S, Kohli P, Lasenby J, Leskovec J, Liu TY , Manrai A, Zitnik M (2023) Sci-\nentific discovery in the age of artificial intelligence. Nature 620(7972):47–60.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / \ns 4 1 5 8 6 - 0 2 3 - 0 6 2 2 1 - 2       \nWebb T, Holyoak KJ, Lu H (2023) Emergent analogical reasoning in large Language models. Nat Hum \nBehav 7(9):1526–1541. https://doi.org/10.1038/s41562-023-01659-w\nWei J, Tay Y , Bommasani R, Raffel C, Zoph B, Borgeaud S, Yogatama D, Bosma M, Zhou D, Metzler D, Chi \nEH, Hashimoto T, Vinyals O, Liang P, Dean J, Fedus W (2022) Emergent abilities of large language \nmodels. arXiv preprint. https://doi.org/10.48550/arXiv.2206.07682\nYang F, Chen Z, Jiang Z, Cho E, Huang X, Lu Y (2023) Palr: Personalization aware llms for recommenda -\ntion. arXiv preprint arXiv:2305.07622\nYildirim I, Paul LA (2023) From task structures to world models: what do LLMs know? ArXiv Preprint. \narXiv:2310.04276 https://doi.org/10.48550/arXiv.2310.04276\nYukun Z, Xu L, Huang Z, Peng K, Seligman M, Li E, Yu F (2023) AI chatbot responds to emotional Cuing. \nPsyArXiv Preprint. https://doi.org/10.31234/osf.io/9ymfz\nZeiler M (2014) Visualizing and understanding convolutional networks. European conference on computer \nvision/arXiv\nZhang J, Xu X, Deng S (2023a) Exploring collaboration mechanisms for LLM agents: A social psychology \nview. ArXiv Preprint. arXiv:2310.02124 https://doi.org/10.48550/arXiv.2310.02124\nZhang Z, Chadwick G, McNally H, Zhao Y , Mullins R (2023b) Llm4dv: Using large language models for \nhardware test stimuli generation. arXiv preprint arXiv:2310.04535\nZhao Y , Huang Z, Seligman M, Peng K (2024) Risk and prosocial behavioural cues elicit human-like response \npatterns from AI chatbots. Sci Rep 14(1):7095\nZheng Y , Koh HY , Ju J, Nguyen ATN, May LT, Webb GI, Pan S (2023) Large language models for scientific \nsynthesis, inference and explanation. arXiv preprint. https://doi.org/10.48550/arXiv.2310.07984\n1 3\n305 Page 34 of 35\nExploring the frontiers of LLMs in psychological applications: a…\nZhong Y , Chen YJ, Zhou Y , Lyu YA, Yin JJ, Gao YJ (2023) The artificial intelligence large Language models \nand neuropsychiatry practice and research ethic. Asian J Psychiatry 84:103577.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 \n6 / j . a j p . 2 0 2 3 . 1 0 3 5 7 7       \nZhuang Y , Liu Q, Ning Y , Huang W, Lv R, Huang Z, Zhao G, Zhang Z, Mao Q, Wang S, Chen E (2023) \nEfficiently measuring the cognitive ability of LLMs: an adaptive testing perspective. arXiv preprint.  h t \nt p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 6 . 1 0 5 1 2       \nZiems C, Held W, Shaikh O, Chen J, Zhang Z, Yang D (2023) Can large language models transform compu-\ntational social science? arXiv preprint. https://doi.org/10.48550/arXiv.2305.03514\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nAuthors and Affiliations\nLuoma Ke1 · Song Tong2,3 · Peng Cheng4 · Kaiping Peng1\n \r Song Tong\ns.tong@bnu.edu.cn\n \r Kaiping Peng\npengkp@tsinghua.edu.cn\n1 Department of Psychological and Cognitive Sciences, Tsinghua University, Beijing, China\n2 Department of Psychology, Faculty of Arts and Sciences, Beijing Normal University, Zhuhai, \nChina\n3 Beijing Key Laboratory of Applied Experimental Psychology, National Demonstration \nCenter for Experimental Psychology Education (Beijing Normal University), Beijing Normal \nUniversity, Beijing, China\n4 China Research Institute for Science Popularization, Beijing, China\n1 3\nPage 35 of 35 305",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5303559899330139
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I25254941",
      "name": "Beijing Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210129493",
      "name": "China Research Institute for Science Popularization",
      "country": "CN"
    }
  ]
}