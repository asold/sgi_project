{
  "title": "The emergence of number and syntax units in LSTM language models",
  "url": "https://openalex.org/W2952208026",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4282320463",
      "name": "Lakretz, Yair",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226831784",
      "name": "Kruszewski, Germán",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282320431",
      "name": "Desbordes, Théo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4201025914",
      "name": "Hupkes, Dieuwke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1964523432",
      "name": "Dehaene, Stanislas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2230649300",
      "name": "Baroni Marco",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2864832950",
    "https://openalex.org/W2768794963",
    "https://openalex.org/W2891343966",
    "https://openalex.org/W2798727047",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W2524611247",
    "https://openalex.org/W2141554956",
    "https://openalex.org/W2531381952",
    "https://openalex.org/W2964222268",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2037504148",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2892296496",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W2606837722",
    "https://openalex.org/W2964117978",
    "https://openalex.org/W2529194139",
    "https://openalex.org/W2963614302"
  ],
  "abstract": "Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two `number units'. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.",
  "full_text": "The emergence of number and syntax units in LSTM language models\nYair Lakretz\nCognitive Neuroimaging Unit\nNeuroSpin center\n91191, Gif-sur-Yvette, France\nyair.lakretz@gmail.com\nGerman Kruszewski\nFacebook AI Research\nParis, France\ngermank@gmail.com\nTheo Desbordes\nFacebook AI Research\nParis, France\ntdesbordes@fb.com\nDieuwke Hupkes\nILLC, University of Amsterdam\nAmsterdam, Netherlands\nd.hupkes@uva.nl\nStanislas Dehaene\nCognitive Neuroimaging Unit\nNeuroSpin center\n91191, Gif-sur-Yvette, France\nstanislas.dehaene@gmail.com\nMarco Baroni\nFacebook AI Research\nParis, France\nmbaroni@fb.com\nAbstract\nRecent work has shown that LSTMs trained on\na generic language modeling objective capture\nsyntax-sensitive generalizations such as long-\ndistance number agreement. We have however\nno mechanistic understanding of how they ac-\ncomplish this remarkable feat. Some have\nconjectured it depends on heuristics that do not\ntruly take hierarchical structure into account.\nWe present here a detailed study of the inner\nmechanics of number tracking in LSTMs at\nthe single neuron level. We discover that long-\ndistance number information is largely man-\naged by two “number units”. Importantly, the\nbehaviour of these units is partially controlled\nby other units independently shown to track\nsyntactic structure. We conclude that LSTMs\nare, to some extent, implementing genuinely\nsyntactic processing mechanisms, paving the\nway to a more general understanding of gram-\nmatical encoding in LSTMs.\n1 Introduction\nIn the last years, recurrent neural networks\n(RNNs), and particularly long-short-term-memory\n(LSTM) architectures (Hochreiter and Schmidhu-\nber, 1997), have been successfully applied to a\nvariety of NLP tasks. This has spurred interest\nin whether these generic sequence-processing de-\nvices are discovering genuine structural properties\nof language in their training data, or whether their\nsuccess can be explained by opportunistic surface-\npattern-based heuristics.\nUntil now, this debate has mostly relied on\n“behavioural” evidence: The LSTM had been\ntreated as a black box, and its capacities had\nbeen indirectly inferred by its performance on\nlinguistic tasks. In this study, we took a com-\nplementary approach inspired by neuroscience:\nWe thoroughly investigated the inner dynamics of\nan LSTM language model performing a number\nagreement task, striving to achieve a mechanis-\ntic understanding of how it accomplishes it. We\nfound that the LSTM had specialized two “grand-\nmother” cells (Bowers, 2009) to carry number fea-\ntures from the subject to the verb across the in-\ntervening material.1 Interestingly, the LSTM also\n1In the neuroscientiﬁc literature, “grandmother” cells are\n(sets of) neurons coding for speciﬁc information, e.g., about\nyour grandmother, in a non-distributed manner.\narXiv:1903.07435v2  [cs.CL]  2 Apr 2019\npossesses a more distributed mechanism to predict\nnumber when subject and verb are close, with the\ngrandmother number cells only playing a crucial\nrole in more difﬁcult long-distance cases. Cru-\ncially, we independently identiﬁed a set of cells\ntracking syntactic structure, and found that one\nof them encodes the presence of an embedded\nphrase separating the main subject-verb depen-\ndency, and has strong efferent connections to the\nlong-distance number cells, suggesting that the\nnetwork relies on genuine syntactic information to\nregulate agreement-feature percolation.\nOur analysis thus provides direct evidence for\nthe claim that LSTMs trained on unannotated cor-\npus data, despite lacking signiﬁcant linguistic pri-\nors, learn to perform structure-dependent linguis-\ntic operations. In turn, this suggests that raw lin-\nguistic input and generic memory mechanisms,\nsuch as those implemented in LSTMs, may sufﬁce\nto trigger the induction of non-trivial grammatical\nrules.\n2 Related work\nStarting with the seminal work of Linzen et al.\n(2016), a long-distance number agreement task\nhas emerged as a standard way to probe the syn-\ntactic capabilities of neural language models. In\nthe number agreement task, a model is asked to\npredict the verb in a sentence where the subject\nand main verb are separated by one or more inter-\nvening nouns (“the boy near the cars greets. . . ”)\nand evaluated based on how often it predicts the\nright verb form.\nFollowing mixed initial results by Linzen and\ncolleagues and Bernardy and Lappin (2017), Gu-\nlordava et al. (2018) and Kuncoro et al. (2018b)\nhave robustly established that LSTM language\nmodels achieve near-human performance on the\nagreement task. While Gulordava and colleagues\nprovided some evidence that the LSTMs are re-\nlying on genuine syntactic generalizations, Kun-\ncoro et al. (2018a) and Linzen and Leonard (2018)\nsuggested that the LSTM achievements can, at\nleast in part, be accounted by superﬁcial heuristics\n(e.g., “percolate the number of the ﬁrst noun in a\nsentence”). Other recent work has extended syn-\ntax probing to other phenomena such as negative\npolarity items and island constraints (Chowdhury\nand Zamparelli, 2018; Jumelet and Hupkes, 2018;\nMarvin and Linzen, 2018; Wilcox et al., 2018).\nWhile Linzen et al. (2016) presented intrigu-\ning qualitative data showing cells that track gram-\nmatical number in a network directly trained on\nthe agreement task, most of the following work\nfocused on testing the network output behaviour,\nrather than on understanding how the latter fol-\nlows from the inner representations of the net-\nwork. Another research line studied linguistic\nprocessing in neural networks through ‘diagnos-\ntic classiﬁers’, that is, classiﬁers trained to predict\na certain property from network activations (e.g.,\nGelderloos and Chrupała, 2016; Adi et al., 2017;\nAlain and Bengio, 2017; Hupkes et al., 2018).\nThis approach may give insight into which infor-\nmation is encoded by the network in different lay-\ners or at different time points, but it only provides\nindirect evidence about the speciﬁc mechanics of\nlinguistic processing in the network.\nOther studies are closer to our approach in\nthat they attempt to attribute function to spe-\nciﬁc network cells, often by means of visual-\nization (Karpathy et al., 2016; Li et al., 2016;\nTang et al., 2017). Radford et al. (2017), for\nexample, detected a “sentiment” grandmother\ncell in a language-model-trained network. Ke-\nmentchedjhieva and Lopez (2018) recently found\na character-level RNN to track morpheme bound-\naries in a single cell. We are however not aware\nof others studies systematically characterizing the\nprocessing of a linguistic phenomenon at the level\nof RNN cell dynamics, as is the attempt in the\nstudy hereby presented.\n3 Setup\nLanguage model We study the pretrained\nLSTM language model made available by Gu-\nlordava et al. (2018). This model is composed\nof a 650-dimensional embedding layer, two 650-\ndimensional hidden layers, and an output layer\nwith vocabulary size 50,000. The model was\ntrained on Wikipedia data, without ﬁne-tuning for\nnumber agreement, and obtained perplexity close\nto state of the art in the experiments of Gulordava\net al.2\nNumber-agreement tasks We complement\nanalysis of the naturalistic, corpus-derived\nnumber-agreement test set of Linzen et al. (2016),\nin the version made available by Gulordava et al.\n(2018), with synthetically generated data-sets.\n2Key ﬁndings reported below were also replicated with\nthe same model trained with different initialization seeds and\nvariations with different hyper-parameters.\nSimple the boy greetsthe guy\nAdv the boy probably greets the guy\n2Adv the boy most probably greets the guy\nCoAdv the boy openly and deliberately greets the guy\nNamePP the boy near Pat greets the guy\nNounPP the boy near the car greets the guy\nNounPPAdvthe boy near the car kindly greets the guy\nTable 1: NA tasks illustrated by representative singular\nsentences.\nEach synthetic number-agreement task (NA-task)\ninstantiates a ﬁxed syntactic structure with varied\nlexical material, in order to probe subject-verb\nnumber agreement in controlled and increasingly\nchallenging setups. 3 The different structures\nare illustrated in Table 1, where all forms are in\nthe singular. Distinct sentences were randomly\ngenerated by selecting words from pools of 20\nsubject/object nouns, 15 verbs, 10 adverbs, 5\nprepositions, 10 proper nouns and 10 location\nnouns. The items were selected so that their\ncombination would not lead to semantic anoma-\nlies. For each NA-task, we generated singular\nand plural versions of each sentence. We refer\nto each such version as a condition. For NA-\ntasks that have other nouns occurring between\nsubject and main verb, we also systematically\nvary their number, resulting in two congruent\nand two incongruent conditions. For example,\nthe NounPP sentence in the table illustrates the\ncongruent SS (singular-singular) condition and\nthe corresponding sentence in the incongruent\nPS (plural-singular) condition is: “the boys near\nthe car greet the guy”. For all NA-tasks, each\ncondition consisted of 600 sentences\nSyntactic depth data-set We probed the im-\nplicit syntax-parsing abilities of the model by test-\ning whether its representations predict the syn-\ntactic depth of the words they process. Follow-\ning Nelson et al. (2017), this was operational-\nized as predicting the number of open syntactic\nnodes at each word, given the canonical syntac-\ntic parse of a sentence. We generated a data-set\nof sentences with unambiguous but varied syntac-\ntic structures and annotated them with the number\nof open nodes at each word. For example: “Ten 1\nreally2 ecstatic3 cousins3 of4 four5 teachers6 are2\nquickly3 laughing4”, where indexes show the cor-\n3We exclude, for the time being, agreement across a rel-\native clause, as it comes with the further complication of ac-\ncounting for the extra agreement process taking place inside\nthe relative clause.\nresponding number of open nodes. Since syntactic\ndepth is naturally correlated with the position of a\nword in a sentence, we used a data-point sampling\nstrategy to de-correlate these factors. For each\nlength between 2 and 25 words, we randomly gen-\nerated 300 sentences. From this set, we randomly\npicked examples uniformly covering all possible\nposition-depth combinations within the 7-12 posi-\ntion and 3-8 depth ranges. The ﬁnal data-set con-\ntains 4,033 positions from 1,303 sentences.4\n4 Experiments\nTo successfully perform the NA-task, the LSTM\nshould: (1) encode and store the grammatical\nnumber of the subject; and (2) track the main\nsubject-verb syntactic dependency. The latter in-\nformation is important for identifying the time\nperiod during which subject number should be\nstored, output and then updated by the network.\nThis section describes the ‘neural circuit’ that en-\ncodes and processes this information in the LSTM.\n4.1 Long-range number units\nWe ﬁrst tested the performance of the LSTM on\nthe Linzen’s data and on the NA-tasks in Table 1.\nFollowing Linzen et al. (2016) and later work, we\ncomputed the likelihood that the LSTM assigns to\nthe main verb of each sentence given the preced-\ning context and compared it to the likelihood it as-\nsigns to the wrong verb inﬂection. Accuracy in a\ngiven condition was measured as the proportion of\nsentences in this condition for which the model as-\nsigned a higher likelihood to the correct verb form\nthan to the wrong one.\nNetwork performance is reported in Table 2\n(right column – ‘Full’). We ﬁrst note that our\nresults on the Linzen NA-task conﬁrm those re-\nported in Gulordava et al. (2018). For the other\nNA-tasks, results show that some tasks and condi-\ntions are more difﬁcult than others. For example,\nperformance on the Simple (0-distance) NA-task\nis better than that on the Co-Adv NA-task, which\nin turn is better than that of the nounPP tasks.\nSecond, as expected, incongruent conditions (the\nnumber-mismatch conditions of namePP, nounPP\nand nounPPAdv) reduce network performance.\n4All our data-sets are available at: https:\n//github.com/FAIRNS/Number_and_syntax_\nunits_in_LSTM_LMs.\nNA task C Ablated Full776 988\nSimple S - - 100\nAdv S - - 100\n2Adv S - - 99.9\nCoAdv S - 82 98.7\nnamePP SS - - 99.3\nnounPP SS - - 99.2\nnounPP SP - 54.2 87.2\nnounPPAdv SS - - 99.5\nnounPPAdv SP - 54.0 91.2\nSimple P - - 100\nAdv P - - 99.6\n2Adv P - - 99.3\nCoAdv P 79.2 - 99.3\nnamePP PS 39.9 - 68.9\nnounPP PS 48.0 - 92.0\nnounPP PP 78.3 - 99.0\nnounPPAdv PS 63.7 - 99.2\nnounPPAdv PP - - 99.8\nLinzen - 75.3 - 93.9\nTable 2: Ablation-experiments results: Percentage ac-\ncuracy in all NA-tasks. Full: non-ablated model, C:\ncondition, S: singular, P: plural. Red: Singular subject,\nBlue: Plural subject. Performance reduction less than\n10% is denoted by ‘-’.\nThird, for long-range dependencies, reliably en-\ncoding singular subject across an interfering noun\nis more difﬁcult than a plural subject: for both\nnounPP and nounPPAdv, PS is easier than SP. A\npossible explanation for this ﬁnding is that in En-\nglish the plural form is almost always more fre-\nquent than the singular one, as the latter only\nmarks third person singular, whereas the former\nis identical to the inﬁnitive and other forms. Thus,\nif the network reverts to unigram probabilities, it\nwill tend to prefer the plural.\nLooking for number units through ablation\nNumber information may be stored in the network\nin either a local, sparse, or a distributed way, de-\npending on the fraction of active units that carry it.\nWe hypothesized that if the network uses a local or\nsparse coding, meaning that there’s a small set of\nunits that encode number information, then ablat-\ning these units would lead to a drastic decrease in\nperformance in the NA-tasks. To test this, we ab-\nlated each unit of the network, one at a time, by\nﬁxing its activation to zero, and tested on the NA-\ntasks.\nTwo units were found to have exceptional ef-\nfect on network performance (Table 2, 776 and\n988 columns).5 Ablating them reduced network\nperformance by more than 10% across various\nconditions, and, importantly, they were the only\nunits whose ablation consistently brought network\nperformance to around chance level in the more\ndifﬁcult incongruent conditions of the namePP,\nnounPP and nounPPAdv tasks.\nMoreover, the ablation effect depended on the\ngrammatical number of the subject: ablating\n776 signiﬁcantly reduced network performance\nonly if the subject was plural (P, PS or PP condi-\ntions) and 988 only if the subject was singular (S,\nSP or SS conditions). In what follows, we will\ntherefore refer to these units as the ‘plural’ and\n‘singular’ units, respectively, or long-range (LR)\nnumber units when referring to both. Finally, we\nnote that although the Linzen NA-task contained\nmixed stimuli from many types of conditions, the\nplural unit was found to have a substantial effect\non average on network performance. The singu-\nlar unit didn’t show a similar effect in this case,\nwhich highlights the importance of using carefully\ncrafted stimuli, as in the nounPP and nounPPAdv\ntasks, for understanding network dynamics. Taken\ntogether, these results suggest a highly local cod-\ning scheme of grammatical number when process-\ning long-range dependencies.\nVisualizing gate and cell-state dynamics To\nunderstand the functioning of the number units,\nwe now look into their gate and state dynam-\nics during sentence processing. We focus on the\nnounPP NA-task, which is the simplest NA-task\nthat includes a long-range dependency with an in-\nterfering noun, in both SP and PS conditions.\nRecall the standard LSTM memory update and\noutput rules (Hochreiter and Schmidhuber, 1997):\nCt = ft ◦Ct−1 + it ◦˜Ct (1)\nht = ot ◦tanh(Ct), (2)\nwhere ft, it, ot ∈ (0, 1) are gating scalars com-\nputed by the network, and ˜Ct ∈(−1, 1) is an up-\ndate candidate for cell value.\nConsider now how a number unit may reliably\nencode and store subject number across interfering\nnouns. Figure 1c exempliﬁes this for a singular\nunit, showing the desired gate and cell dynamics.\n5Units 1-650 belong to the ﬁrst layer, 651-1300 to the sec-\nond. All units detected by our analyses come from the latter.\n−1.5\n1.5\n˜Ct\n0\n1\nit\n0\n1\nft\n−1.5\n1.5\nCt\nThe boy(s) near the car(s) greet(s) the\n0\n1\not\n(a) 988 (singular)\n−1.5\n1.5\n˜Ct\n0\n1\nit\n0\n1\nft\n−1.5\n1.5\nCt\nThe boy(s) near the car(s) greet(s) the\n0\n1\not (b) 776 (plural)\n−1.5\n1.5\n˜Ct\n0\n1\nit\n0\n1\nft\n−1.5\n1.5\nCt\nThe boy(s) near the car(s) greet(s) the\n0\n1\not\n(c) Prediction (singular)\n(d) Efferent weights of the LR-units (776 and 988 ), the\nsyntax unit (1150 ; section 4.3) and two arbitrary units\n(651 and 1300 ).\nFigure 1: (a) to (c) – Cell and gate activations during processing of sentences with a prepositional phrase between\nsubject and verb. Values in (a) and (b) are averaged across all condition sentences, with error bars showing standard\ndeviations. (d) – Efferent weights of speciﬁc units at the output layer to singular and plural verb forms.\nThe four conditions are represented with separated\ncurves - red for singular subject, blue for plural,\nand dashed lines for incongruent conditions. Gate\nand cell activity at time points unrelated to solving\nthe NA-task are masked with white, as we do not\nmake precise predictions for them.\nThe update rule of the LSTM cell has two terms\n(Eq. 1). 6 In the ﬁrst, ft ◦Ct−1, the forget gate\ncontrols whether to keep the previous cell content\n(ft = 1: perfect remembering) or forget it (ft = 0:\ncomplete forgetting). In the second, it ◦ ˜Ct, the\ninput gate controls whether the information cur-\nrently presented to the network, as encoded by ˜Ct,\nshould be written onto the cell ( it = 1: full ac-\ncess) or not ( it = 0). The singular unit can thus\nuse these gates to reliably store number informa-\n6We abuse notation here, using the symbols denoting\nwhole layers in equations (1) and (2) to denote the compo-\nnents of single cells.\ntion across long-range dependencies. Speciﬁcally,\nthe unit can (enumeration follows the same or-\nder as the panels in Figure 1c): (1) encode sub-\nject number via ˜Ctsubject with different values for\nsingular and plural; (2) open the input gate only\nwhen a singular subject is presented ( itsubject = 1\nin red curves only) and protect it from interfering\nnouns (it = 0, tsubject < t < tverb); (3) at the\nsame time, clear the cell from previously stored\ninformation ( ftsubject = 0) and then store sub-\nject number across the entire dependency ( ft =\n1, tsubject < t < tverb); (4) this will result in\nstable encoding of subject number in the cell Ct\nthroughout the dependency; (5) ﬁnally, output sub-\nject number at the right moment, when predicting\nthe verb form (otverb−1 = 1) (Eq. 2).\nFigures 1a and 1b present the actual gate and\ncell dynamics of the singular and plural units.\nBoth units follow the general solution for reliable\nnumber storage described above. Note that for ˜Ct\nand it, and as a result also for Ct, the plural unit\n‘mirrors’ the singular unit with respect to subject\nnumber (red curves of PP and PS vs. blue curves\nof SS and SP). This is in accordance with the re-\nsults of the ablation experiments, which showed\nthat ablating these units had an effect that de-\npended on the grammatical number of the subject\n(Table 2). This provides complementary support\nfor the identiﬁcation of these units as ‘singular’\nand ‘plural’.\nA single divergence between the solution de-\npicted in Figure 1c and the actual dynamics of the\nnumber units is that input gate activity is smaller,\nbut not zero, at the time step immediately fol-\nlowing the subject. One speculative explanation\nis that this might be useful to process compound\nnouns. In these cases, subject number information\nis stored with the second noun, whereas in the case\nof simple nouns there is no ‘risk’ of encountering\nan interfering noun immediately after the subject,\nmaking the delay in closing the gate safe.\nThe singular and plural units had emerged at the\nsecond layer of the network. This seems appropri-\nate since number information needs to be directly\nprojected to the output layer for correct verb-form\nprediction. Moreover, number-unit output should\nbe projected differently to singular and plural verb\nforms in the output layer, only increasing activ-\nity in output units representing the suitable form.\nFor example, for the singular unit, since singu-\nlar subjects are encoded with a negative value\n(Ctverb−1 < −1 in ﬁgure 1a), the more negative\nits efferent weights to singular verb forms in the\noutput layer, the higher the probabilities of these\nverb forms would be. Figure 1d shows the effer-\nent weights of the LR-number units to all verbs\nin our data-sets. We found that, indeed, the effer-\nent weights to the singular and plural verb forms\nare segregated from each other, with weight signs\nthat correspond to the negative encoding of sub-\nject number used by both singular and plural units.\nTwo other arbitrary units, 651 and 1300 , and the\nsyntax unit 1150 to be described below (Section\n4.3) do not have segregated efferent weights to\nverb forms, as expected.\n4.2 Short-range number information\nPerformance on the easier NA-tasks (Simple, Adv,\n2Adv) was not impaired by single-unit ablations.\nThis suggests that number may be encoded also\nFigure 2: Generalization across time of subject-number\nprediction. Error bars represent standard deviations\nacross cross-validation splits.\nelsewhere in the network, perhaps via a more dis-\ntributed code. To verify this, we tested whether\nsubject number can be decoded from the whole\npattern of activities in the network (excluding the\ntwo LR-number units) and whether this decoding\nis stable across time (see Giulianelli et al., 2018,\nfor similar observations and related methods). We\nexpected this distributed activity to track number\nin a small time window after the subject, but, un-\nlike the LR-number units, to be affected by incon-\ngruent intervening nouns.\nWe trained a linear model to predict the gram-\nmatical number of the subject from network activ-\nity in response to the presentation of the subject,\nand tested its prediction on test sets from all time\npoints (King and Dehaene, 2014), in incongruent\nconditions only of the nounPP task. We used Area\nunder of Curve (AUC) to evaluate model perfor-\nmance. Figure 2 shows decoding across time of\nsubject number from cell activity of each number\nunit separately and from cell activity of the entire\nnetwork without these two units (‘Full model mi-\nnus LR-units’). Results show that number infor-\nmation can be efﬁciently decoded from other units\nin the network, and that this information can be\ncarried for several time steps (relatively high AUC\nup to the second determiner). However, the way\nin which these units encode number is sensitive to\nthe last encountered noun, with AUC decreasing\nto zero around the second noun (‘cars’), whereas\ntest performance of the models trained on cell ac-\ntivity of the LR-number units is consistently high.\nThis conﬁrms that number prediction is supported\nboth by the LR-number units, and by distributed\nactivation patterns of other short-range (SR) num-\nber units. The latter, however, are not syntax-\nsensitive, and simply encode the number of the last\nnoun encountered.\n(a) 2Adv\n (b) nounPP\n (c) subject relative\n(d) Two embeddings with subject relatives\nFigure 3: Cell activity of syntax unit 1150 while processing various syntactic structures. Values averaged across\nall stimuli in an NA-task, with error bars representing standard deviations. Relative clause NA-task stimuli were\nspeciﬁcally generated for this visualization.\nA full description of the SR-number units is be-\nyond our scope. However, we note that 10 SR-\nnumber units in the second layer of the network\nwere identiﬁed, which had efferent weights with\na similar segregated structure as that of the LR\nunits (Figure 1d). These units were indeed sen-\nsitive to the last encountered noun: subject num-\nber could be decoded from single-unit cell activ-\nity during its presentation (AUC > 0.9), but ac-\ntivity ‘swaps’ once an interfering noun appears\n(i.e., AUC decreases to zero in a generalization-\nacross-time analysis). Finally, to validate the role\nof SR-number units in encoding number for eas-\nier NA-tasks, we ablated both SR and LR number\nunits (12 in total) or SR units only (10 in total)\nand evaluated network performance on these NA-\ntasks. Both experiments resulted in a signiﬁcant\nreduction in task performance compared to 1,000\nrandom equi-size ablations ( p < 0.01 in all ‘eas-\nier’ tasks).\nIntriguingly, we observed qualitatively that LR\nunits are almost always making the right predic-\ntion, even when the network predicts the wrong\nnumber. The wrong outcome, in such cases, might\nbe due to interference from the syntax-insensitive\nSR units. We leave the study of LR-SR unit inter-\nplay to future work.\n4.3 Syntax units\nWe saw how the input and forget gates of the LR-\nnumber units control the ﬂow of subject-number\ninformation. It remains unclear, however, how the\ndynamics of these gates are controlled by the net-\nwork. We hypothesized that other units in the net-\nwork may encode information about the syntac-\ntic structure of the sentence, and thus about the\nsubject-verb dependency. These units could then\ncontrol and coordinate the opening and closing of\nthe input and forget gates of the number units.\nTo identify such ’syntax’ units, we tested from\nwhich units syntactic information can be efﬁ-\nciently decoded. We used depth of the syntac-\ntic tree as a proxy for syntactic structure (Nel-\nson et al., 2017) and trained an L2-regularized\nregression model to predict syntactic tree-depth\nfrom the hidden-state activity of all units. In all\nexperiments, we used the data presented in Sec-\ntion 3 above and performed a nested 5-fold cross-\nvalidation procedure. Word frequency, which was\nadded as a covariate to the model, had a negligi-\nble effect on the results. Syntactic tree-depth was\nfound to be efﬁciently decodable from network\nactivity ( R2\ntest−set = 0.85 ±0.009; covariate-\ncorrected). A small subset of ‘syntax’ units had\nrelatively high weights in the regression model\n(mean weight = 7.6×10−4, SD=7.86×10−2; cut-\noff for outlier weights was set to three SDs). Since\nthe interpretation of the regression weights may\ndepend on possible correlations among the fea-\ntures, we also tested the causal effect of these units\non NA-task performance. Ablating the syntax\nunits together resulted in signiﬁcant performance\nreduction in NA-tasks that have an interfering\nnoun: Linzen NA-task: p = 0.024, nounPPAdv-\nSP: p = 0 .011, nounPPAdv-PS: p = 0 .034,\nnounPP-SP: p <0.001 and marginally signiﬁcant\nin nounPP-PS: p = 0.052 (compared to 1000 ran-\ndom ablations of subsets of units of the same size).\nTo gain further insight regarding the functioning\nof the syntax units, we next visualized their gate\nand cell dynamics during sentence processing. We\nfound that cell activity of unit 1150 , which also\n(a) Input gate\n (b) Forget gate\nFigure 4: Connectivity among the syntax unit 1150 and LR-number units 776 and 988 . Projecting units are on\nthe table rows. Blue background highlights outlier values ( |z −score|> 3). Weights from the syntax unit are\nmarked in red and are explicitly labeled in the plots, which show the overall distributions of afferent weights to\neach number unit.\nhad one of the highest weights in the regression\nmodel, was remarkably structured. The activity\nof this unit increases across the entire subject-\nverb dependency and drops abruptly right after.\nFigures 3a and 3b show cell activity of this unit\nduring the processing of stimuli from the 2Adv\nand nounPP tasks. We found the same dynamics\nin cases where another verb occurs between sub-\nject and main verb, as in subject relatives (Figure\n3c), and in exceptionally long-distance dependen-\ncies with two interfering nouns and verbs (Figure\n3d). Taken together, these results suggest that unit\n1150 consistently encodes subject-verb dependen-\ncies in a syntax-sensitive manner. Other syntax\nunits did not show an easily interpretable dynam-\nics and had no clear interactions with the number\nunits in the analysis discussed next. This suggests\nthat they perform different syntactic, or possibly\nother, functions.\n4.4 Syntax-number units connections\nWe ﬁnally look at the connections that were\nlearned by the LSTM between syntax unit 1150 ,\nwhich appears to be more closely involved in\ntracking subject-verb agreement, and the LR num-\nber units, as well as at the connections between the\nLR-number units themselves. For each unit pair,\nthere are 4 connection types, one for each com-\nponent of the target cell (to the 3 gates and to the\nupdate candidate). We focus on input and forget\ngates, as they control the ﬂow and storage of num-\nber information.\nFigures 4a and 4b show the distributions of all\nafferent recurrent weights to the input and forget\ngates of the LR-number units, scaled by the maxi-\nmal activity ht of the pre-synaptic units during the\nnounPP task (this scaling evaluates theeffective in-\nput to the units and did not change the conclusions\ndescribed below). We found that the weights from\nthe syntax unit to the forget gate of both 776 and\n988 are exceptionally high in the positive direc-\ntion compared to all other afferent connections\nin the network ( z −score = 8.1, 11.2, respec-\ntively) and those to their input gates exception-\nally negative (z −score = −16.2, −7.2). Since\nthe cell activity of syntax unit 1150 is positive\nacross the entire subject-verb dependency (e.g.,\nFigure 3d), the connectivity from the syntax unit\ndrives the number unit forget gates towards one\n(Wf\n776,1150h1150 ≫ 0 and Wf\n988,1150h1150 ≫ 0;\ntsubject < t < tverb) and their input gates towards\nzero (Wi\n776,1150h1150 ≪0 and Wi\n988,1150h1150 ≪\n0). Looking at the right-hand-side of Eq. (1), this\nmeans that the ﬁrst term becomes dominant and\nthe second vanishes, suggesting that, across the\nentire dependency, the syntax unit conveys a ‘re-\nmember ﬂag’ to the number units. Similarly, when\nthe activity of the syntax unit becomes negative at\nthe end of the dependency, it conveys an ‘update\nﬂag’.\nLast, we note that the reciprocal connectivity\nbetween the two LR-number units is always pos-\nitive, to both input and forget gates (with |z −\nscore|> 3 for the 776 -to-988 direction). Since\ntheir activity is negative throughout the subject-\nverb dependency (Figures 1a and 1b), this means\nthat they are mutually inhibiting, thus steering to-\nwards an unequivocal signal about the grammati-\ncal number of the subject to the output layer.\n5 Summary and discussion\nWe provided the ﬁrst detailed description of\nthe underlying mechanism by which an LSTM\nlanguage-model performs long-distance number\nagreement. Strikingly, simply training an LSTM\non a language-model objective on raw corpus data\nbrought about single units carrying exceptionally\nspeciﬁc linguistic information. Three of these\nunits were found to form a highly interactive lo-\ncal network, which makes up the central part of a\n‘neural’ circuit performing long-distance number\nagreement.\nOne of these units encodes and stores gram-\nmatical number information when the main sub-\nject of a sentence is singular, and it successfully\ncarries this information across long-range depen-\ndencies. Another unit similarly encodes plurality.\nThese number units show that a highly local en-\ncoding of linguistic features can emerge in LSTMs\nduring language-model training, as was previously\nsuggested by theoretical studies of artiﬁcial neural\nnetworks (e.g., Bowers, 2009) and in neuroscience\n(e.g., Kutter et al., 2018).\nOur analysis also identiﬁed units whose activity\ncorrelates with syntactic complexity. These units,\nas a whole, affect performance on the agreement\ntasks. We further found that one of them encodes\nthe main subject-verb dependency across various\nsyntactic constructions. Moreover, the highest af-\nferent weights to the forget and input gates of\nboth LR-number units were from this unit. A\nnatural interpretation is that this unit propagates\nsyntax-based remember and update ﬂags that con-\ntrol when the number units store and release infor-\nmation.\nFinally, number is also redundantly encoded in\na more distributed way, but the latter mechanism\nis unable to carry information across embedded\nsyntactic structures. The computational burden of\ntracking number information thus gave rise to two\ntypes of units in the network, encoding similar in-\nformation with distinct properties and dynamics.\nThe relationship we uncovered and character-\nized between syntax and number units suggests\nthat agreement in an LSTM language-model can-\nnot be entirely explained away by superﬁcial\nheuristics, and the networks have, to some extent,\nlearned to build and exploit structure-based syn-\ntactic representations, akin to those conjectured to\nsupport human-sentence processing.\nIn future work, we intend to explore how the en-\ncoding pattern we found varies across network ar-\nchitectures and hyperparameters, as well as across\nlanguages and domains. We also would like to\ninvestigate the timecourse of emergence of the\nfound behaviour over training time.\nMore generally, we hope that our study will\ninspire more analyses of the inner dynamics of\nLSTMs and other sequence-processing networks,\ncomplementing the currently popular “black-box\nprobing” approach. Besides bringing about a\nmechanistic understanding of language process-\ning in artiﬁcial models, this could inform work\non human-sentence processing. Indeed, our study\nyields particular testable predictions on brain dy-\nnamics, given that the computational burden of\nlong-distance agreement remains the same for ar-\ntiﬁcial and biological neural network, despite im-\nplementation differences and different data sizes\nrequired for language acquisition. We conjecture\na similar distinction between SR and LR units to\nbe found in the human brain, as well as an in-\nteraction between syntax-processing and feature-\ncarrying units such as the LR units, and plan to\ntest these in future work.\nAcknowledgments\nWe would like to thank Kristina Gulordava, Jean-\nRemi King, Tal Linzen, Gabriella Vigliocco and\nChristophe Pallier for helpful feedback and com-\nments on the work.\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2017. Fine-grained\nanalysis of sentence embeddings using auxil-\niary prediction tasks. In Proceedings of ICLR\nConference Track , Toulon, France. Published\nonline: https://openreview.net/group?\nid=ICLR.cc/2017/conference.\nGuillaume Alain and Yoshua Bengio. 2017. Under-\nstanding intermediate layers using linear classiﬁer\nprobes. In Proceedings of ICLR Conference Track,\nToulon, France.\nJean-Philippe Bernardy and Shalom Lappin. 2017. Us-\ning deep neural networks to learn syntactic agree-\nment. Linguistic Issues in Language Technology ,\n15(2):1–15.\nJeffrey Bowers. 2009. On the biological plausibility of\ngrandmother cells: Implications for neural network\ntheories in psychology and neuroscience. Psycho-\nlogical Review, 116(1):220–251.\nShammur Chowdhury and Roberto Zamparelli. 2018.\nRNN simulations of grammaticality judgments on\nlong-distance dependencies. In Proceedings of\nCOLING, pages 133–144, Santa Fe, NM.\nLieke Gelderloos and Grzegorz Chrupała. 2016. From\nphonemes to images: levels of representation in\na recurrent neural model of visually-grounded lan-\nguage learning. In Proceedings of COLING 2016,\nthe 26th International Conference on Computational\nLinguistics: Technical Papers, pages 1309–1319.\nMario Giulianelli, Jack Harding, Florian Mohnert,\nDieuwke Hupkes, and Willem Zuidema. 2018. Un-\nder the hood: Using diagnostic classiﬁers to investi-\ngate and improve how language models track agree-\nment information. In Proceedings of the EMNLP\nBlackboxNLP Workshop, pages 240–248, Brussels,\nBelgium.\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of NAACL, pages 1195–1205, New Or-\nleans, LA.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation ,\n9(8):1735–1780.\nDieuwke Hupkes, Sara Veldhoen, and Willem\nZuidema. 2018. Visualisation and ’diagnostic classi-\nﬁers’ reveal how recurrent and recursive neural net-\nworks process hierarchical structure. Journal of Ar-\ntiﬁcial Intelligence Research, 61:907–926.\nJaap Jumelet and Dieuwke Hupkes. 2018. Do lan-\nguage models understand anything? on the ability\nof lstms to understand negative polarity items. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 222–231.\nAndrej Karpathy, Justin Johnson, and Fei-Fei Li.\n2016. Visualizing and understanding recur-\nrent networks. In Proceedings of ICLR Work-\nshop Track , San Juan, Puerto Rico. Published\nonline: https://openreview.net/group?\nid=ICLR.cc/2016/workshop.\nYova Kementchedjhieva and Adam Lopez. 2018. ‘In-\ndicatements’ that character language models learn\nEnglish morpho-syntactic units and regularities. In\nProceedings of the EMNLP Workshop on analyzing\nand interpreting neural networks for NLP, Brussels,\nBelgium. In press.\nJean-R´emi King and Stanislas Dehaene. 2014. Charac-\nterizing the dynamics of mental representations: The\ntemporal generalization method. Trends in Cogni-\ntive Sciences, 18(4):203–210.\nAdhiguna Kuncoro, Chris Dyer, John Hale, and Phil\nBlunsom. 2018a. The perils of natural behavioral\ntests for unnatural models: The case of number\nagreement. Poster presented at the Learning Lan-\nguage in Humans and in Machines conference, on-\nline at: https://osf.io/view/L2HM/.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018b.\nLSTMs can learn syntax-sensitive dependencies\nwell, but modeling structure makes them better. In\nProceedings of ACL, pages 1426–1436, Melbourne,\nAustralia.\nEsther Kutter, Jan Bostroem, Christian Elger, Florian\nMormann, and Andreas Nieder. 2018. Single neu-\nrons in the human brain encode numbers. Neuron,\n100(3):753–761.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural mod-\nels in NLP. In Proceedings of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies (NAACL-\nHLT), pages 681–691.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nTal Linzen and Brian Leonard. 2018. Distinct patterns\nof syntactic agreement errors in recurrent networks\nand humans. In Proceedings of CogSci, pages 692–\n697, Austin, TX.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202.\nMatthew Nelson, Imen El Karoui, Kristof Giber, Xi-\naofang Yang, Laurent Cohen, Hilda Koopman, Syd-\nney Cash, Lionel Naccache, John Hale, Christophe\nPallier, and Stanislas Dehaene. 2017. Neurophysio-\nlogical dynamics of phrase-structure building during\nsentence processing. Proceedings of the National\nAcademy of Sciences, 114(18):E3669–E3678.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. https://arxiv.org/abs/1704.\n01444.\nZhiyuan Tang, Ying Shi, Dong Wang, Yang Feng,\nand Shiyue Zhang. 2017. Memory visualization for\ngated recurrent neural networks in speech recogni-\ntion. In Acoustics, Speech and Signal Processing\n(ICASSP), 2017 IEEE International Conference on,\npages 2736–2740. IEEE.\nEthan Wilcox, Roger Levy, Takashi Morita, and\nRichard Futrell. 2018. What do RNN language\nmodels learn about ﬁller–gap dependencies? In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 211–221.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8257776498794556
    },
    {
      "name": "Heuristics",
      "score": 0.8205384016036987
    },
    {
      "name": "Syntax",
      "score": 0.7895160913467407
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6156144738197327
    },
    {
      "name": "Natural language processing",
      "score": 0.5919501781463623
    },
    {
      "name": "Encoding (memory)",
      "score": 0.5299134254455566
    },
    {
      "name": "Language model",
      "score": 0.452392041683197
    },
    {
      "name": "Tracking (education)",
      "score": 0.4451279044151306
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    }
  ]
}