{
  "title": "Defending Backdoor Attacks on Vision Transformer via Patch Processing",
  "url": "https://openalex.org/W4382468565",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5080642445",
      "name": "Khoa D. Doan",
      "affiliations": [
        "VinUniversity"
      ]
    },
    {
      "id": "https://openalex.org/A5071172709",
      "name": "Yingjie Lao",
      "affiliations": [
        "Clemson University"
      ]
    },
    {
      "id": "https://openalex.org/A5100681631",
      "name": "Peng Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100435494",
      "name": "Ping Li",
      "affiliations": [
        "LinkedIn (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2782217514",
    "https://openalex.org/W3013758358",
    "https://openalex.org/W6776469819",
    "https://openalex.org/W2915783303",
    "https://openalex.org/W3204773485",
    "https://openalex.org/W6761100106",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2900018096",
    "https://openalex.org/W2966689772",
    "https://openalex.org/W3208279478",
    "https://openalex.org/W2971661634",
    "https://openalex.org/W3211600423",
    "https://openalex.org/W4306808694",
    "https://openalex.org/W3201941857",
    "https://openalex.org/W4281608574",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2905379489",
    "https://openalex.org/W2914712270",
    "https://openalex.org/W3118734924",
    "https://openalex.org/W2981207549",
    "https://openalex.org/W2942091739",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2936118975",
    "https://openalex.org/W3015146382",
    "https://openalex.org/W4283803103",
    "https://openalex.org/W2807363941",
    "https://openalex.org/W2791319131",
    "https://openalex.org/W6743986261",
    "https://openalex.org/W3039176595",
    "https://openalex.org/W6746172121",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3143373604",
    "https://openalex.org/W6795975431",
    "https://openalex.org/W6795475546",
    "https://openalex.org/W3173053527",
    "https://openalex.org/W3123895164",
    "https://openalex.org/W3016194608",
    "https://openalex.org/W2970200861",
    "https://openalex.org/W3111491915",
    "https://openalex.org/W6768126957",
    "https://openalex.org/W6637162671",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W6756333562",
    "https://openalex.org/W2966104011",
    "https://openalex.org/W2926701059",
    "https://openalex.org/W2934843808",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3203430276",
    "https://openalex.org/W6769858516",
    "https://openalex.org/W6810563863",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W4283819233",
    "https://openalex.org/W6842280032",
    "https://openalex.org/W4290876045",
    "https://openalex.org/W4221142474",
    "https://openalex.org/W4289533844",
    "https://openalex.org/W3163966458",
    "https://openalex.org/W4288093767",
    "https://openalex.org/W3216991452",
    "https://openalex.org/W2774423163",
    "https://openalex.org/W4289300166",
    "https://openalex.org/W3130788031",
    "https://openalex.org/W3207101810",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W4310286738",
    "https://openalex.org/W4287122830",
    "https://openalex.org/W1673923490",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W3023402713",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2966038277",
    "https://openalex.org/W2985913519",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2929803724",
    "https://openalex.org/W3015678314",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3008191164",
    "https://openalex.org/W3121099749",
    "https://openalex.org/W4313153605",
    "https://openalex.org/W3035574168",
    "https://openalex.org/W2970335439",
    "https://openalex.org/W2996800219",
    "https://openalex.org/W2753783305",
    "https://openalex.org/W3142085127",
    "https://openalex.org/W2898759955",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4214564822",
    "https://openalex.org/W3195462295",
    "https://openalex.org/W3217539178",
    "https://openalex.org/W3107337211",
    "https://openalex.org/W4312957757"
  ],
  "abstract": "Vision Transformers (ViTs) have a radically different architecture with significantly less inductive bias than Convolutional Neural Networks. Along with the improvement in performance, security and robustness of ViTs are also of great importance to study. In contrast to many recent works that exploit the robustness of ViTs against adversarial examples, this paper investigates a representative causative attack, i.e., backdoor. We first examine the vulnerability of ViTs against various backdoor attacks and find that ViTs are also quite vulnerable to existing attacks. However, we observe that the clean-data accuracy and backdoor attack success rate of ViTs respond distinctively to patch transformations before the positional encoding. Then, based on this finding, we propose an effective method for ViTs to defend both patch-based and blending-based trigger backdoor attacks via patch processing. The performances are evaluated on several benchmark datasets, including CIFAR10, GTSRB, and TinyImageNet, which show the proposedds defense is very successful in mitigating backdoor attacks for ViTs. To the best of our knowledge, this paper presents the first defensive strategy that utilizes a unique characteristic of ViTs against backdoor attacks.",
  "full_text": "Defending Backdoor Attacks on Vision Transformer via Patch Processing\nKhoa D. Doan1, Yingjie Lao2, Peng Yang3, Ping Li4\n1College of Engineering and Computer Science, VinUniversity\n2Electrical and Computer Engineering, Clemson University, Clemson, SC 29634, USA\n3Meta Corporation, Bellevue, W A 98004, USA\n4LinkedIn Corporation, Bellevue, W A 98004, USA\nkhoa.dd@vinuni.edu.vn, ylao@clemson.edu, pengyang01@gmail.com, pinli@linkedin.com\nAbstract\nVision Transformers (ViTs) have a radically different archi-\ntecture with significantly less inductive bias than Convolu-\ntional Neural Networks. Along with the improvement in per-\nformance, security and robustness of ViTs are also of great\nimportance to study. In contrast to many recent works that\nexploit the robustness of ViTs against adversarial examples,\nthis paper investigates a representative causative attack, i.e.,\nbackdoor. We first examine the vulnerability of ViTs against\nvarious backdoor attacks and find that ViTs are also quite\nvulnerable to existing attacks. However, we observe that the\nclean-data accuracy and backdoor attack success rate of ViTs\nrespond distinctively to patch transformations before the po-\nsitional encoding. Then, based on this finding, we propose\nan effective method for ViTs to defend both patch-based and\nblending-based trigger backdoor attacks via patch process-\ning. The performances are evaluated on several benchmark\ndatasets, including CIFAR10, GTSRB, and TinyImageNet,\nwhich show the proposed defense is very successful in miti-\ngating backdoor attacks for ViTs. To the best of our knowl-\nedge, this paper presents the first defensive strategy that uti-\nlizes a unique characteristic of ViTs against backdoor attacks.\n1 Introduction\nThe versatility of machine learning makes it a promising\ntechnology for implementing a wide variety of complex sys-\ntems such as autonomous driving (Grigorescu et al. 2020;\nCaesar et al. 2020), intrusion detection (Vinayakumar et al.\n2019; Berman et al. 2019), communication (Huang et al.\n2020), and pandemic mitigation (Oh, Park, and Ye 2020; Al-\nimadadi et al. 2020) systems, retrieval (Doan, Yang, and Li\n2022), etc. These examples also illustrate that a large por-\ntion of safety-critical applications is benefited from the evo-\nlution of machine learning, which meanwhile requires high\ndegrees of security and trustworthiness of these technolo-\ngies (Yang, Lao, and Li 2021; Lao et al. 2022a,b; Zhao,\nLao, and Li 2022; Zhao and Lao 2022). Unfortunately, vul-\nnerabilities have emerged from many aspects of machine\nlearning and a wide body of research has been investigated\nrecently to exploit both these vulnerabilities and defensive\nmeasures to mitigate attacks against machine learning, es-\npecially for deep learning systems (Szegedy et al. 2014; Liu\net al. 2018a; Akhtar and Mian 2018).\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nOne such vulnerability, backdoor attack, allows an adver-\nsary with access to the model’s training phase the possibil-\nity of injecting backdoors to maliciously alter the machine\nlearning model behavior (Liu et al. 2018b; Chen et al. 2017).\nThese backdoor injection attacks poison the training data or\nmodify the learning algorithm such that an association be-\ntween a specific adversarial input “trigger” and an adversar-\nial output “behavior” is formed. A trigger is typically lo-\ncally superimposed on a clean image with an image pattern\n(i.e., patch-based) (Gu et al. 2019; Liu et al. 2018b) or glob-\nally blended (i.e., blending-based) (Liu et al. 2020; Nguyen\nand Tran 2021; Doan, Lao, and Li 2021; Doan et al. 2021;\nDoan, Lao, and Li 2022) for improving the stealthiness. The\ncompromised model will continue to behave normally as in-\ntended under the typical usage scenarios with clean inputs.\nBut by exposing the model to the correct triggers, a user\nwith the prerequisite knowledge can then directly control the\nmodel’s prediction.\nAs machine learning continues to improve upon its cur-\nrent success, developers must understand both the vulner-\nabilities that machine learning brings and valid methods\nin overcoming these weaknesses. One recent major ad-\nvance in computer vision tasks is the vision transformer\n(ViT) (Dosovitskiy et al. 2021), which adapts the multi-head\nself-attention mechanism from the natural language process-\ning (NLP) tasks. Specifically, during ViT’s training, images\nare pre-processed as patches, which are treated similarly to\nwords in NLP. It has been shown that ViT can achieve com-\nparable or even better performance to state-of-the-art con-\nvolutional neural network (CNN) architectures on various\nvision tasks (Dosovitskiy et al. 2021; Liu et al. 2021; Wang\net al. 2021; Touvron et al. 2021; Yuan et al. 2021; Gkelios,\nBoutalis, and Chatzichristofis 2021; Khan et al. 2021; Chen,\nYu, and Li 2021; Yu et al. 2022; Yu and Li 2022).\nWhile switching from convolution to self-attention has\nshown promising outcomes in tackling these vision tasks\nfrom the performance perspective, the implications of such\nfundamental differences on security and robustness are also\nof paramount importance to study. Several recent works ex-\namined the performance of ViT against adversarial exam-\nples (Mao et al. 2022; Benz et al. 2021; Bhojanapalli et al.\n2021; Naseer et al. 2021; Mahmood, Mahmood, and van\nDijk 2021; Shao et al. 2021; Naseer et al. 2022; Joshi, Ja-\ngatap, and Hegde 2021). However, the vulnerability of ViT\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n506\nagainst backdoor attacks and the corresponding countermea-\nsures have not been extensively studied. In fact, to the best\nof our knowledge, only one very recent work looked at this\ndirection (Lv et al. 2021), which proposed a data-free back-\ndoor embedding attack against the vision transformer net-\nworks. In contrast to this prior work, we focus on the defen-\nsive side. Aligning with the processing of ViT that divides\nan image into patches, we mainly study the implications of\npatch transformations on image classification tasks in this\npaper. Specifically, we utilize two techniques, namely Patch-\nDrop and PatchShuffle, which randomly drop and shuffle\npatches of an image, respectively. Under these patch pro-\ncessing, we find that ViT exhibits a different characteristic\nfrom CNNs and also responds distinctively between clean\nsamples and backdoor samples. Specifically, PatchDrop is\neffective in detecting patch-based backdoor attacks, while\nPatchShuffle can successfully mitigate blending-based back-\ndoor attacks. Therefore, based on patch processing, we pro-\npose a novel defensive solution to combat backdoor attacks.\nThe contributions of this paper are summarized below:\n• We first perform an empirical study on the vulnerability of\nViTs against both patch-based and blending-based back-\ndoor attacks and find ViTs are still quite vulnerable to\nbackdoor attacks.\n• We observe an interesting characteristic of ViTs that\nclean-data accuracy and backdoor attack success rate of\nViTs respond distinctively to patch processing before the\npositional encoding, which is not seen on CNN models.\n• We propose a novel defensive solution to mitigate back-\ndoor attacks on ViTs via patch processing. We analyze\ntwo processing methods, i.e., PatchDrop and PatchShuffle,\nand examine their effectiveness in reducing the attack suc-\ncess rate (ASR) of backdoor attacks. In particular, Patch-\nDrop and PatchShuffle are effective in detecting patch-\nbased and blending-based backdoor attacks, respectively.\nTogether, they are used to effectively detect the backdoor\nsamples without prior knowledge of whether the attack is\npatch-based or blending-based.\n• We comprehensively evaluate the performance of the pro-\nposed techniques on a wide range of benchmark settings,\nincluding CIFAR10, GTSRB, and TinyImageNet.\n2 Related Work\nPrevious works on deep neural network (DNN) backdoor\ninjection have understood the attack as the process of in-\ntroducing malicious modifications to a model, F(·), trained\nto classify the dataset (X ,Y ). These changes force an asso-\nciation with specific input triggers, (∆, m), to the desired\nmodel output, yt (Gu et al. 2019; Liu et al. 2018b; Bag-\ndasaryan and Shmatikov 2021; Yao et al. 2019). Through\nEquation (1), the trigger can be superimposed on any input\nsuch that a poisoned input is formed.\nP(x, m,∆) = x ◦ (1 − m) + ∆◦ m (1)\nHere we use ◦ to denote the element-wise product and m is\na mask used to determine the region of the input containing\nthe trigger pattern, ∆. In essence, the adversarial goal is to\nforce the model to minimize the compound loss function:\nM(Fω(x), y)+ c·D(F(P(x, m,∆)), F(xt)), instead of the\noriginal benign loss such as cross-entropy loss, whereD(·, ·)\ndefines the similarity between the model’s actual behavior\nand a target behavior described by the input xt while the\nconstant c is used to balance the terms (Yao et al. 2019).\nThe main methodologies used to inject this functionality\ninto the model are contaminating the training data (Chen\net al. 2017; Liu et al. 2018b; Gu et al. 2019; Saha, Sub-\nramanya, and Pirsiavash 2020), altering the training algo-\nrithm (Bagdasaryan and Shmatikov 2021) or overwriting/re-\ntraining the model parameters after deployment (Dumford\nand Scheirer 2020). Besides the original patch-based trig-\nger (Gu et al. 2019), various blending-based trigger pat-\nterns have also been proposed, including blended (Chen\net al. 2017), sinusoidal strips (SIG) (Barni, Kallas, and Tondi\n2019), reflection (ReFool) (Liu et al. 2020), and warping\n(WaNet) (Nguyen and Tran 2021). Note that in order to dif-\nferentiate from the patch used in describing the processing\nof ViTs, we limit the usage of patch for backdoor attacks\nto only “patch-based”. In other words, only “patch-based”\nrefers to the backdoor attack, while all the other usages\nof “patch” are related to the ViTs in this paper. For the\nbackdoor embedding attack on ViT (Lv et al. 2021), it seeks\nto catch most attention of the victim model by leveraging the\nunique attention mechanism.\nOn the other hand, several categories of defensive solu-\ntions have been proposed to combat backdoor attacks in past\nyears (Chen et al. 2019a; Tran, Li, and Madry 2018; Gao\net al. 2019; Liu, Xie, and Srivastava 2017; Li et al. 2020; Liu,\nDolan-Gavitt, and Garg 2018; Cheng et al. 2020; Wang et al.\n2019; Chen et al. 2019b; Qiao, Yang, and Li 2019). One di-\nrection is to remove, detect, or mismatch the trigger of inputs\nthrough certain processing or transformations of the input\nimages (Liu, Xie, and Srivastava 2017; Li et al. 2020; Doan,\nAbbasnejad, and Ranasinghe 2020; Udeshi et al. 2022; Qiu\net al. 2021; Gao et al. 2019). Note that most of these defen-\nsive methods are model-agnostic and mainly target at pro-\ncessing the inputs. Our proposed defensive method follows\na similar concept as these input processing methods. For in-\nstance, similar to STRIP (Gao et al. 2019) that examines the\nentropy in predicted classes after a set of input perturbations\nto check any violation of the input-dependence property of\na benign model, we leverage the distinctive performance be-\ntween the clean sample and backdoor sample against patch\nprocessing to detect malicious behaviors. Another advantage\nof such methods, including the proposed one, is that they\nonly require access to clean samples, which is a more prac-\ntical setting for defending backdoor attacks.\n3 Backdoor Attacks on ViT\n3.1 Threat Model\nWe follow the typical threat model of DNN backdoor at-\ntacks (Gu et al. 2019) that a user wishes to establish a model\nfor a specific image classification task by training with data\nprovided by a third party. We assume the adversary has the\ncapability of injecting poisoned data samples into the train-\ning dataset, but cannot modify the model architecture, the\ntraining setting, or the inference pipeline. Since the user will\n507\nClean\n Backdoor\n(a) GTSRB/BadNets\nClean\n Backdoor (b) TinyImageNet/ReFool\nFigure 1: Clean and backdoor samples with local patch-\nbased trigger (a square in bottom right corner) and global\nblending-based trigger (an embedded reflection).\ncheck the accuracy of the trained model on a held-out vali-\ndation dataset (clean samples), the adversarial goal is to em-\nbed a backdoor into the model through data contamination\nwithout degrading the clean-data accuracy over the image\nclassification task. In other words, the model should produce\nmalicious behavior only on images with the trigger for the\nbackdoor, while performing normally otherwise.\n3.2 Attack Experimental Results\nTo understand the security threat on ViTs against the back-\ndoor attacks, we consider two most popular approaches of\ncreating the backdoor triggers: local patch-based triggers,\nBadNets (Gu et al. 2019) and SinglePixel (Bagdasaryan and\nShmatikov 2021), and global blending-based triggers, Re-\nFool (Liu et al. 2020) and WaNet (Nguyen and Tran 2021).\nWe evaluate the performance on CIFAR10, GTSRB, and\nTinyImageNet datasets.\nSpecifically, we perform the attack experiment by poison-\ning the training dataset and the corresponding ground-truth\nlabels. For each training dataset, similar to prior works (Gu\net al. 2019; Nguyen and Tran 2021; Liu et al. 2020), we se-\nlect a small number of samples (less than 10%) and apply\nthe corresponding trigger on each of the selected images.\nFigure 1 shows some examples of both patch-based and\nblending-based backdoor samples. The labels of the poi-\nsoned samples are also changed to the target label. The poi-\nsoned training data are then used to train the image classi-\nfication model. Then, we perform training using two ViT\nvariants, the original ViT (Dosovitskiy et al. 2021) and\nDeiT (Touvron et al. 2021), and several other popular CNN\nmodel architectures, including Vgg11 (Simonyan and Zis-\nserman 2014), ResNet18 (He et al. 2016), and Big Transfer\n(BiT) (Kolesnikov et al. 2020). Note that the models are pre-\ntrained on ImageNet-21k and fine-tuned on the correspond-\ning dataset to ensure a consistent experimentation frame-\nwork. This setup is influenced by the fact that large-scale\nViTs and BiT are not trained from scratch on smaller-scale\ndatasets to prevent overfitting. Each trained model is then\nevaluated on the held-out test sets of clean and backdoor\nsamples. The backdoor samples are applied with the triggers\nthat are generated using the same mechanism in the corre-\nsponding attack strategy for the evaluation.\nIn Tables 1 and 2, we show the clean-data and backdoor-\ndata performance of the trained models for BadNets and\nWaNet, respectively. We can observe that the trained ViT\nand DeiT with the backdoors have similar, high clean-data\naccuracies to that of the corresponding benign models (still\nDataset ViT DeiT Vgg11 BiT\nClean Attack Clean Attack Clean Attack Clean Attack\nCIFAR10 98.93 98.47 98.82 97.82 93.44 96.95 98.51 97.09\nGTSRB 98.68 96.46 98.55 95.62 98.05 91.21 98.71 94.77\nT-Imagenet 86.46 98.02 87.76 95.77 61.94 88.57 80.99 96.94\nTable 1: Patch-based Backdoor Attack (BadNets)\nDataset ViT DeiT Vgg11 BiT\nClean Attack Clean Attack Clean Attack Clean Attack\nCIFAR10 97.88 99.98 97.92 99.99 95.06 99.71 97.85 99.99\nGTSRB 99.08 99.74 97.55 98.27 98.75 99.48 99.23 99.98\nT-Imagenet 77.48 99.99 83.90 98.53 64.96 99.21 75.90 99.99\nTable 2: Blending-based Backdoor Attack (WaNet)\noutperforming other CNN models). However, when the trig-\ngers are present, the probabilities of the poisoned ViT mod-\nels to predict the target label (i.e., ASR) are also quite high,\nwhich are above 96% on all datasets. In other words, ViTs\nare at least as vulnerable against backdoor attacks as the\nCNN models. In fact, the patch-based backdoor attack on\nViTs seems to be even slightly more successful than on other\nCNN models, which further validates the need for studying\nthe backdoor attacks and countermeasures on ViTs. We ob-\nserve similar results for SinglePixel and ReFool attacks.\n4 Backdoor Attacks vs. Patch Processing\nWe have shown that backdoor attacks are still quite suc-\ncessful on ViTs. Besides, as we discussed above, it has also\nrecently been shown that ViTs are vulnerable against other\ntypes of attacks, although they exhibit certain degrees of im-\nprovement in robustness against the transferability of adver-\nsarial examples (Mahmood, Mahmood, and van Dijk 2021;\nShao et al. 2021). While these features of ViTs are simi-\nlar to the CNNs, ViTs have also been shown to be more\nrobust toward occlusions, distributional shifts, and permu-\ntation (Naseer et al. 2021). Here, we extend the robustness\nstudy of the receptive fields of ViTs with respect to the back-\ndoor attack models and compare their performance to CNNs.\n4.1 Patch Processing\nFollowing the existing defensive methods that process im-\nages at the input space for detecting backdoor attacks (Liu,\nXie, and Srivastava 2017; Li et al. 2020; Doan, Abbasnejad,\nand Ranasinghe 2020; Udeshi et al. 2022; Qiu et al. 2021;\nGao et al. 2019), we study the performance of the back-\ndoor attacks on ViT models through input transformations\nthat align with the characteristic of ViTs, i.e., patch process-\ning where the content of the image is randomly perturbed.\nSpecifically, each input imagex is represented as a sequence\nof patches with L elements: {xi}i=1,..,L. Note that the patch\nxi does not necessarily have the same size as the patch size\nused in the pre-trained ViT model. Perturbing the image’s\npatches is equivalent to modifying its content. Here, we fo-\ncus on the question: How does perturbation influence the\nreceptive field of ViTs on image patches when various back-\ndoor triggers are present ? We denote the patch processing\non x with a function R and consider the following strategies\nfor performing the patch processing:\n508\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCIFAR10-ViT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCIFAR10-Vgg11\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCIFAR10-ResNet18\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCIFAR10-ResNet/BiT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n(a) CIFAR10\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-ViT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-Vgg11\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-ResNet18\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-ResNet/BiT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n(b) GTSRB\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nTinyImageNet-ViT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nTinyImageNet-Vgg11\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nTinyImageNet-ResNet18\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n0.2 0.4 0.6 0.8\nPercentage of Patches Dropped\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nTinyImageNet-ResNet/BiT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n(c) TinyImageNet\nFigure 2: Performance of clean-data accuracy and backdoor attacks with dropped patches on ViT, Vgg11, ResNet18, and BiT.\n• PatchDrop. Similar to Naseer et al. (2021), we randomly\ndrop M patches from the total L patches of an image x.\nWe divide the image into L = l × l patches that belong\nto a spatial grid of l × l. The number of dropped patches\nindicates the information loss on the image content.\n• PatchShuffle. We randomly shuffle the L patches of an\nimage x. The L patches are created in similar spatial grids\nas those of PatchDrop. PatchShuffle does not remove the\ncontent of the image but can significantly impact the re-\nceptive fields of the models.\nNote that similar forms of patch transformations on ViTs\nhave been considered in prior works (Naseer et al. 2021;\nShao et al. 2021), but not in the context of backdoor attacks.\n4.2 Performance of Backdoor Attacks against\nPatch Processing\nWe first study the trends of backdoor ASR and clean-data\naccuracy with respect to patch processing on the correspond-\ning test set for each dataset. The results are reported in Fig-\nures 2 and 3 for BadNets and ReFool, respectively.\nFor patch-based attacks with PatchDrop, we observe that\nthe clean-data performances of ViT only drop slightly on\nCIFAR10 and GTSRB even when almost 50% of the image\ncontent is removed. In contrast, the clean-data performances\ndrop much more significantly in all the other three CNNs.\nOn TinyImageNet, the clean-data performance of ViT drops\nmore than in the other datasets. However, when the backdoor\ntriggers are present, the attack success rate on ViT decreases\nsignificantly, even with a slight loss in the content of the im-\nages. In comparison, backdoor attacks on CNNs are more ro-\nbust to PatchDrop. Interestingly, if we continue to drop more\npatches, the ASR on the CNNs suddenly increases in several\nexperiments. A possible explanation is that CNN models and\nbackdoor attacks rely on smaller regions of the image than\nViT for prediction and achieving the target classes, respec-\ntively, which makes the clean-data accuracy of ViT more ro-\nbust to patch processing. We also notice another important\nresult: the variance in the predictions of the poisoned models\nis higher for backdoor samples than for the clean samples.\nWe summarize the observations for patch-based attacks with\nrespect to PatchDrop as follows:\n• Clean-data accuracy sensitivity: ViT << CNN\n• ASR sensitivity: ViT > CNN\n• Gap between accuracy and ASR: ViT > CNN\nHowever, for blending-based attacks with PatchDrop, we\ndo not observe a consistent difference between the ViTs\nand CNNs, although ViTs are more robust with respect to\nthe clean-data accuracy and ASR. Since the trigger is well-\nblended into the images across the entire pixel space, as\nin ReFool and WaNet, PatchDrop tends to be less impact-\nful on the backdoor, similar to the robustness of the models\non the foreground objects. However, for blending-based at-\ntacks with PatchShuffle, we observe that the clean-data per-\nformances of ViT drop significantly. In contrast, the ASRs\nonly drop slightly. Such robustness of the trigger is consis-\ntent across various patch sizes (i.e., |xi|) on all datasets. For\nthe CNNs, the gaps between clean-data accuracy and ASR\nare smaller; in some cases, e.g., Vgg11, the gap can be-\ncome significantly narrow. In previous studies, ViTs exhibit\nhigh robustness against patch transformation for larger patch\n509\n10 20 30 40 50 60\nPatch Size\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCIFAR10-ViT\nBD/Clean\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.00\n0.25\n0.50\n0.75Accuracy\nCIFAR10-Vgg11\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCIFAR10-ResNet18\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nCIFAR10-ResNet/BiT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n(a) CIFAR10\n10 20 30 40 50 60\nPatch Size\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-ViT\nBD/Clean\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-Vgg11\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-ResNet18\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.25\n0.50\n0.75\n1.00Accuracy\nGTSRB-ResNet/BiT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n(b) GTSRB\n10 20 30 40 50 60\nPatch Size\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nTinyImageNet-ViT\nBD/Clean\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.00\n0.25\n0.50\n0.75Accuracy\nTinyImageNet-Vgg11\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nTinyImageNet-ResNet18\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n10 20 30 40 50 60\nPatch Size\n0.00\n0.25\n0.50\n0.75\n1.00Accuracy\nTinyImageNet-ResNet/BiT\nBD/Clean,M=0\nBenign/Clean\nBD/Clean\nBD/Attack\n(c) TinyImageNet\nFigure 3: Performance of clean-data accuracy and backdoor attacks with dropped patches on ViT, Vgg11, ResNet18, and BiT.\nsizes (Naseer et al. 2021). Under the proposed PatchShuffle,\nthe significant robustness of the trigger across all patch sizes,\nespecially the smaller sizes, is interesting. Such performance\ncan possibly be explained that ViTs learn and generalize the\nspatial invariance of the triggers extremely well. We summa-\nrize the observations for blending-based attacks with respect\nto PatchShuffle as follows:\n• Clean-data accuracy sensitivity: ViT > CNN\n• ASR sensitivity: ViT << CNN\n• Gap between accuracy and ASR: ViT > CNN\nIn summary, ViT has distinguishable performance be-\ntween clean-data performance and ASR against certain\npatch processing techniques: the ASR drops significantly\non ViT for patch-based attacks with PatchDrop, while\nthe clean-data performance drops significantly on ViT for\nblending-based trigger attacks with PatchShuffle. As a\nresult, for both cases, ViT has a larger gap between accu-\nracy and ASR than CNN. It is important to note that these\ncharacteristics are not observed on CNN models. Therefore,\nthe observed impact of patch processing against backdoor\nattacks is unique to ViT.\n5 Novel Defensive Solution for ViT\n5.1 Methodology\nBased on our observations above, we propose an effec-\ntive backdoor detection algorithm that can successfully de-\ntect and then remove the poison samples from a backdoor-\ninjected ViT model with high success rates. The key intu-\nition in our algorithm is that the patch processing strate-\ngies affect ViTs’ predictions on the backdoor samples differ-\nently from the predictive function of the model on the clean\ndata. Our defense algorithm exploits the frequency that ViTs\nchange their predictions on the same sample under different\ntrials of a patch processing strategy and use a threshold to\nassess if a sample is clean or poisoned. Our defense mecha-\nnism only requires access to a small set of K clean samples\n(less than 1000 on the studied datasets), which can be easily\nobtained from the held-out validation dataset, for selecting\nthe threshold. When no such clean samples are available, we\nshow that the defenses are still very effective, which enables\nmuch wider applicability of the proposed method. The pro-\nposed defense consists of the following steps:\n• Step 1 (Offline): For the small set of clean sam-\nples, randomly apply PatchDrop and PatchTranslate on\neach image for T trials. For each sample x, we cal-\nculate Fd(x) = PT\nt=1 1{F(x) ̸= F(R(t)\nd (x))} and\nAlgorithm 1: Patch Processing-based Backdoor Detection\nInput: Sample x, Threshold kd (PatchDrop), Threshold ks\n(PatchShuffle)\nOutput: Clean or Backdoor Decision\n1: function F(x)\n2: t ← 0, Fd(x) ← 0, Fs(x) ← 0, Predict ˆy = F(x)\n3: repeat\n4: t ← t + 1\n5: ˆyt = F(Rt(x)), Fd(x) ← Fd(x) + 1if ˆyt ̸= ˆy\n6: ˆyt = F(Rs(x)), Fs(x) ← Fs(x) + 1if ˆyt ̸= ˆy\n7: until t = T\n8: return Fd(x) and Fs(x)\n9: end function\n10: If Fd(x) > kd or Fs(x) < ks, x is Backdoor\n11: Otherwise, x is Clean\n510\n10 20 30 40 50 60\nPatches Purturbed\n0.2\n0.4\n0.6\n0.8\n1.0TPR\nResNet18\nVgg11\n10 20 30 40 50 60\nPatches Purturbed\n0.2\n0.4\n0.6\n0.8\n1.0TPR\nResNet18\nVgg11\n10 20 30 40 50 60\nPatches Purturbed\n0.2\n0.4\n0.6\n0.8\n1.0TPR\nResNet18\nVgg11\n0.2\n0.4\n0.6\n0.8\n1.0\nTNR\nCIFAR10\nResNet18\nVgg11\n0.2\n0.4\n0.6\n0.8\n1.0\nTNR\nGTSRB\nResNet18\nVgg11\n0.2\n0.4\n0.6\n0.8\n1.0\nTNR\nTinyImageNet\nResNet18\nVgg11\n(a) CNN Models\n10 20 30 40 50 60\nPatches Purturbed\n0.2\n0.4\n0.6\n0.8\n1.0TPR\nViT\nDeiT\n10 20 30 40 50 60\nPatches Purturbed\n0.2\n0.4\n0.6\n0.8\n1.0TPR\nViT\nDeiT\n10 20 30 40 50 60\nPatches Purturbed\n0.2\n0.4\n0.6\n0.8\n1.0TPR\nViT\nDeiT\n0.2\n0.4\n0.6\n0.8\n1.0\nTNR\nCIFAR10\nViT\nDeiT\n0.2\n0.4\n0.6\n0.8\n1.0\nTNR\nGTSRB\nViT\nDeiT\n0.2\n0.4\n0.6\n0.8\n1.0\nTNR\nTinyImageNet\nViT\nDeiT\n(b) ViT Models\nFigure 4: TPR and TNR for different numbers of dropped patches (in a spatial grid of 8 × 8) for CNNs (ResNet18 and Vgg11)\nand ViTs (ViT and DeiT). TPR represents the detection rate; TNR represents clean-sample mis-detection rate.\nFs(x) = PT\nt=1 1{F(x) ̸= F(R(t)\ns (x))}, where R(t)\nd and\nR(t)\ns denotes the random application of PatchDrop and\nPatchShuffle, respectively, at trialt. Intuitively,Fd(x) and\nFs(x) estimate the probabilities that the predicted labels\non x change to something else after the patch processing.\n• Step 2 (Offline): Given the sample {Fd(xi)}i=1,..,K or\n{Fs(xi)}i=1,..,K created in Step 1, we set the threshold\nparameter kd and ks for PatchDrop and PatchShuffle, re-\nspectively, to the values at the nth percentiles to ensure a\nsmall false positive rate, as follows:\n– For PatchDrop, we typically select a large value (e.g.,\n90th percentile). This is because ASRs significantly de-\ncrease under patch processing such as PatchDrop.\n– For PatchShuffle, we typically select a small value (e.g.,\n10th percentile). This is because ASRs do not drop un-\nder patch processing such as PatchShuffle while the\nclean-data accuracies are more affected.\n• Step 3 (During Inference): For a sample, we randomly\napply PatchDrop and PatchShuffle for T trials and record\nthe number of label changes, Fd(x) and Fs(x), respec-\ntively. IfFd(x) is greater than the selectedkd threshold for\nPatchDrop or Fs(x) is smaller than the selected ks thresh-\nold for PatchShuffle, we flag the sample as a backdoor\nsample. Otherwise, x is determined as a clean sample.\nNote that, the proposed approach does not assume the\nknowledge of the type of the backdoor attack, which ensures\nits practicality in various scenarios. Furthermore, when the\nmodel is benign, i.e., without the backdoor attack, because\nof the percentile selection rules, only a very small fraction\nof samples will be identified as false negative. Formally,\nour defense approach follows a similar strategy as that of\nan anomaly detector. Thus, more sophisticated anomaly de-\ntection approaches can be used to improve the detection rate\nwhile keeping the false negative rate low; however, this is\nbeyond the scope of this paper. The details of the detection\nalgorithm are presented in Algorithm 1.\n5.2 Analysis of Patch Processing-based Defense\nWe first provide a qualitative analysis of the proposed de-\nfense strategy for detecting both patch-based and blending-\n5 10 15 20 25 30\nPatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0TPR\n5 10 15 20 25 30\nPatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0TNR\n \n \n \nViT\nDeiT \nResNet18 \nVgg11 \nBiT\n \n  \nViT \nDeiT \nResNet18\nVgg11 \nBiT\n(a) CIFAR10\n5 10 15 20 25 30\nPatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0TPR\n5 10 15 20 25 30\nPatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0TNR\n \n \n \n \n \n \nViT\nDeiT \nResNet18 \nVgg11 \nBiT\n \n  \nViT \nDeiT \nResNet18 \nVgg11 \nBiT\n(b) GTSRB\n5 10 15 20 25 30\nPatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0TPR\n5 10 15 20 25 30\nPatch Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0TNR\n \n  \n \n \n \n \nViT\nDeiT \nResNet18 \nVgg11 \nBiT\n \n \n \n \nViT\nDeiT \nResNet18 \nVgg11\nBiT\n(c) TinyImageNet\nFigure 5: TPR and TNR for different sizes of processed\npatches for CNN models (ResNet18 and Vgg11) and ViTs\n(ViT and DeiT) under ReFool backdoor attack.\n511\nDataset ViT DeiT Vgg11 ResNet18 BiT\nTPR TNR TPR TNR TPR TNR TPR TNR TPR TNR\nCIFAR10 90.08 99.48 91.88 96.18 88.12 62.88 88.80 89.33 90.00 87.72\nGTSRB 94.89 98.78 93.62 97.66 20.15 80.70 93.80 89.99 93.89 92.91\nTinyImageNet 95.80 64.75 95.80 64.73 81.30 20.51 99.00 56.48 98.60 42.64\nTable 3: TPR (best bolded) and TNR (best underlined) of detecting backdoor samples in BadNets’ poisoned models.\nDataset ViT DeiT Vgg11 ResNet18 BiT\nTPR TNR TPR TNR TPR TNR TPR TNR TPR TNR\nCIFAR10 90.00 83.20 66.80 81.50 18.00 82.80 48.90 89.30 79.10 66.90\nGTSRB 84.10 95.60 83.60 93.60 80.00 98.50 80.20 96.60 83.30 92.10\nTinyImageNet 90.90 87.70 85.30 85.00 69.90 97.10 12.00 96.00 5.10 98.90\nTable 4: TPR (best bolded) and TNR (best underlined) of detecting backdoor samples in ReFool’s poisoned models.\nbased backdoor samples from the corresponding backdoor-\ninjected ViT and CNN models.\nPatch-based Attacks Figure 4 illustrates the true posi-\ntive rate (TPR) and true negative rate (TNR) for the patch-\nbased attack, BadNets, when varying the number of dropped\npatches d in PatchDrop when the spatial grid is8×8. Recall\nthat the TPR and TNR indicate the backdoor detection rate\nand the percentage of clean samples that are not falsely de-\ntected as backdoor samples, respectively. As we can observe,\nthe defensive solution with PatchDrop works better for ViT\nmodels than for CNN models such as ResNet18 and Vgg11.\nFurthermore, dropping 10% of the patches can consistently\nachieve higher TPR and TNR across different datasets. The\neffectiveness of this defense on ViTs is because the backdoor\nperformance is more sensitive to PatchDrop, as discussed in\nthe previous section.\nBlending-based Attacks Figure 5 illustrates the TPR and\nTNR when defending against ReFool with various sizes of\nthe processed patches in PatchShuffle. As we can observe,\nPatchShuffle generally achieves higher TPRs in ViTs than\nin CNN models. More importantly, when the patch size is\nsimilar to that of the trained patch size in ViTs, defending\nagainst ViTs is consistently effective.\n6 Defense Experimental Results\nThis section presents the empirical results in defending\nagainst the backdoor attacks. In real-world settings, the de-\nfender does not know which attack is performed by the ad-\nversary. To this end, we consider two practical scenarios.\nIn the first scenario, the backdoor is successfully injected\ninto the trained model and the victim defends against back-\ndoor attacks (i.e., alleviates its effectiveness) by filtering the\nbackdoor samples during inference. In this experiment, TPR\nand TNR are reported, as they demonstrate how likely the\ndefense method identifies the backdoor samples and how\nlikely the clean samples are not falsely flagged as backdoor\nsamples, respectively. We also assume that a small set of\nclean samples are available. The values at the 90th and 10th\nof the empirical distributions of Fd(x) and Fs(x), for all\nclean samples x, are selected as the thresholds kd and ks for\nPatchDrop and PatchShuffle, respectively.\nIn the second scenario, we consider an extreme case\nwhere the defender is also the model trainer who receives a\npossibly poisoned training dataset. The defender aims to ob-\ntain the trained model that is free of the backdoor. Here, the\nclean samples are not available, which makes the defending\ntask very difficult. Defending using the proposed defensive\nsolution consists of the following steps:\n(i) train the model on the training dataset for some epochs,\n(ii) use the possibly poisoned, trained model to detect the\nbackdoor samples, and\n(iii) remove the backdoor samples from the training dataset\nand re-train the model on the filtered dataset.\nWe observe that training the model for 50 epochs in step\n(i) is sufficient for the backdoor to be inserted into the model\nif the training dataset is poisoned and for the clean-data ac-\ncuracy to reach an acceptable performance compared to its\noptimal value (a few percents difference, e.g., > 90% in CI-\nFAR10). Ideally, we train the models until they reach the\noptimal accuracies, but this can add significant computation\nto the training process while only adding a minor improve-\nment in the defense. Therefore, we use 50 epochs on all ex-\nperiments. In step (ii), we consider the threshold kd = 0 for\nPatchDrop, and ks = T for PatchShuffle.\n6.1 Defending against the Poisoned Model\nTable 3 and Table 4 present the defense results when the\ndefender aims to detect whether a sample is a backdoor or\nclean sample during inference under the local patch-based\nattack, BadNets, and the global blending-based attack, Re-\nFool, respectively. As we can observe, the proposed defen-\nsive solution achieves comparable TPRs (i.e., successfully\ndetects the backdoor samples) in both ViTs (>90%) and\nCNN models (>88%) across different datasets under Bad-\nNets attacks. However, the TNRs of ViTs, including ViT\nand DeiT, are significantly better than those of CNN mod-\nels, including Vgg11, ResNet18 and BiT. Specifically, the\nproposed defense method only falsely detects clean sam-\nples as backdoor samples less than 3% of the time in the\ntrained ViTs, but more than 10% of the time in the trained\nCNN models. Under ReFool attacks, the defensive solution\nachieves the best TPR for ViT, while its TNRs are also very\nhigh. While the TNRs of ResNet18 and BiT are higher than\nthose of ViTs, their TPRs are significantly lower, especially\nin the larger-scale TinyImageNet dataset. Overall, we can\nconclude that the proposed defense method is consistently\nmore effective in ViTs than in CNN models.\n512\nDataset ViT DeiT ResNet18 BiT\nClean Attack Clean Attack Clean Attack Clean Attack\nCIFAR10 98.94 10.01 98.74 09.96 92.30 15.77 97.10 10.39\n+0.01 –89.8 –0.08 –89.8 –4.53 –88.9 –1.43 –89.3\nGTSRB 98.38 0.48 97.98 0.47 96.58 0.46 96.89 0.48\n–0.31 –99.51 –0.58 –99.5 –2.31 –99.5 –1.85 –99.5\nT-Imagenet 85.57 0.51 88.77 0.50 64.79 0.55 72.34 0.52\n–1.03 –99.4 +1.15 –99.4 –5.65 –99.4 –10.6 –99.4\nTable 5: Clean-data accuracy (beset bolded) and ASR after\nremoving the backdoor samples and retraining the models.\nItalicized values are relative changes w.r.t the models trained\nwithout removing backdoor samples.\n6.2 Defending against the Poisoned Training Data\nWe present the clean-data accuracies and ASRs after re-\ntraining the models on the filtered data, as described in the\nsecond scenario, in Table 5. The attack method is patch-\nbased. We can observe that the proposed defense method\nsuccessfully reduces the ASRs much closer to ASRs of ran-\ndom guesses in both ViTs and CNNs on all datasets. How-\never, in ViTs, the clean-data accuracies are preserved, while\nin CNN models, the clean-data accuracies drop more than\n4.5% for ResNet18 and almost 1.5% for BiT. The results for\nVgg11 are worse than those of ResNet18 and BiT and are re-\nported in supplement materials. As discussed in the previous\nexperiment, a non-trivial number of clean samples can be\nfalsely detected as backdoor samples in CNN models using\nthe proposed patch-processing approach. Thus, while most\nbackdoor samples are removed from the training datasets,\nthe number of clean training samples is also reduced, which\nleads to the drop in clean-data performance in CNN mod-\nels. We can also notice that by employing a large-scale pre-\ntrained model (i.e., BiT), the drop in performance can be\nmitigated compared to smaller models, such as ResNet18\nand Vgg11. Nevertheless, we can still observe that the pro-\nposed defense is more effective for ViTs than for CNN mod-\nels.\nIn conclusion, while ViT is vulnerable to patch-based\nbackdoor attacks, the proposed simple-yet-effective patch-\nprocessing-based defense can detect backdoor samples with\na high detection rate while maintaining a low FNR. Because\nViT is robust against patch processing on the clean data, pro-\ncessing the images with these strategies can be utilized to\nobtain useful yet tangible traces for effectively distinguish-\ning the predictions between the clean and backdoor samples.\n7 Conclusion\nThis paper studied several aspects of backdoor attacks\nagainst ViT. We first perform an empirical study on the vul-\nnerability of ViT against both patch-based and blending-\nbased backdoor attacks. Then, based upon our observation\nthat ViT exhibits distinguishable performance between clean\nsamples and backdoor samples against patch processing, we\nproposed a novel defensive solution to counter backdoor at-\ntacks on ViT, which is able to reduce the backdoor attack\nsuccess rate significantly. Two patch processing methods are\ninvestigated. The effectiveness of the proposed techniques is\ncomprehensively evaluated. To the best of our knowledge,\nthis paper presented the first defensive strategy that utilizes\na unique characteristic of ViT against backdoor attacks.\nAcknowledgments\nThe research was conducted while all authors worked at\nBaidu Cognitive Computing Lab – 10900 NE 8th St. Belle-\nvue, W A 98004, USA.\nReferences\nAkhtar, N.; and Mian, A. 2018. Threat of Adversarial At-\ntacks on Deep Learning in Computer Vision: A Survey.\nIEEE Access, 6: 14410–14430.\nAlimadadi, A.; Aryal, S.; Manandhar, I.; Munroe, P. B.; Joe,\nB.; and Cheng, X. 2020. Artificial intelligence and ma-\nchine learning to fight COVID-19. Physiological Genomics,\n52(4): 200–202.\nBagdasaryan, E.; and Shmatikov, V . 2021. Blind Back-\ndoors in Deep Learning Models. In Proceedings of the 30th\nUSENIX Security Symposium, (USENIX Security), 1505–\n1521.\nBarni, M.; Kallas, K.; and Tondi, B. 2019. A New Backdoor\nAttack in CNNs by Training Set Corruption Without Label\nPoisoning. In Proceedings of the 2019 IEEE International\nConference on Image Processing (ICIP), 101–105. Taipei.\nBenz, P.; Ham, S.; Zhang, C.; Karjauv, A.; and Kweon, I. S.\n2021. Adversarial Robustness Comparison of Vision Trans-\nformer and MLP-Mixer to CNNs. In Proceedings of the\n32nd British Machine Vision Conference (BMVC), 25. On-\nline.\nBerman, D. S.; Buczak, A. L.; Chavis, J. S.; and Corbett,\nC. L. 2019. A survey of deep learning methods for cyber\nsecurity. Information, 10(4): 122.\nBhojanapalli, S.; Chakrabarti, A.; Glasner, D.; Li, D.; Un-\nterthiner, T.; and Veit, A. 2021. Understanding Robustness\nof Transformers for Image Classification. In Proceedings of\nthe 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), 10211–10221. Montreal, Canada.\nCaesar, H.; Bankiti, V .; Lang, A. H.; V ora, S.; Liong, V . E.;\nXu, Q.; Krishnan, A.; Pan, Y .; Baldan, G.; and Beijbom, O.\n2020. nuScenes: A Multimodal Dataset for Autonomous\nDriving. In Proceedings of the 2020 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n11618–11628. Seattle, W A.\nChen, B.; Carvalho, W.; Baracaldo, N.; Ludwig, H.; Ed-\nwards, B.; Lee, T.; Molloy, I. M.; and Srivastava, B. 2019a.\nDetecting Backdoor Attacks on Deep Neural Networks by\nActivation Clustering. In Proceedings of the Workshop on\nArtificial Intelligence Safety. Honolulu, HI.\nChen, H.; Fu, C.; Zhao, J.; and Koushanfar, F. 2019b.\nDeepInspect: A Black-box Trojan Detection and Mitigation\nFramework for Deep Neural Networks. In Proceedings of\nthe Twenty-Eighth International Joint Conference on Artifi-\ncial Intelligence (IJCAI), 4658–4664. Macao, China.\nChen, S.; Yu, T.; and Li, P. 2021. MVT: Multi-view Vision\nTransformer for 3D Object Recognition. In Proceedings of\nthe 32nd British Machine Vision Conference (BMVC), 349.\nOnline.\n513\nChen, X.; Liu, C.; Li, B.; Lu, K.; and Song, D. 2017. Tar-\ngeted Backdoor Attacks on Deep Learning Systems Using\nData Poisoning. arXiv preprint arXiv:1712.05526.\nCheng, H.; Xu, K.; Liu, S.; Chen, P.-Y .; Zhao, P.; and Lin, X.\n2020. Defending against Backdoor Attack on Deep Neural\nNetworks. arXiv preprint arXiv:2002.12162.\nDoan, B. G.; Abbasnejad, E.; and Ranasinghe, D. C. 2020.\nFebruus: Input Purification Defense Against Trojan Attacks\non Deep Neural Network Systems. InProceedings of the An-\nnual Computer Security Applications Conference (ACSAC),\n897–912. Virtual Event / Austin, TX.\nDoan, K.; Lao, Y .; and Li, P. 2021. Backdoor Attack with\nImperceptible Input and Latent Modification. InAdvances in\nNeural Information Processing Systems (NeurIPS), 18944–\n18957. virtual.\nDoan, K.; Lao, Y .; and Li, P. 2022. Marksman Backdoor:\nBackdoor Attacks with Arbitrary Target Class. In Advances\nin Neural Information Processing Systems (NeurIPS). New\nOrleans, LA.\nDoan, K.; Lao, Y .; Zhao, W.; and Li, P. 2021. LIRA:\nLearnable, Imperceptible and Robust Backdoor Attacks. In\nProceedings of the 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), 11946–11956. Montreal,\nCanada.\nDoan, K. D.; Yang, P.; and Li, P. 2022. One Loss for Quan-\ntization: Deep Hashing with Discrete Wasserstein Distribu-\ntional Matching. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n9447–9457. New Orleans, LA.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In Proceedings of the 9th Interna-\ntional Conference on Learning Representations (ICLR). Vir-\ntual Event, Austria.\nDumford, J.; and Scheirer, W. J. 2020. Backdooring Con-\nvolutional Neural Networks via Targeted Weight Perturba-\ntions. In Proceedings of the 2020 IEEE International Joint\nConference on Biometrics (IJCB), 1–9. Houston, TX.\nGao, Y .; Xu, C.; Wang, D.; Chen, S.; Ranasinghe, D. C.; and\nNepal, S. 2019. STRIP: a defence against trojan attacks on\ndeep neural networks. In Proceedings of the 35th Annual\nComputer Security Applications Conference (ACSAC), 113–\n125. San Juan, PR.\nGkelios, S.; Boutalis, Y . S.; and Chatzichristofis, S. A. 2021.\nInvestigating the Vision Transformer Model for Image Re-\ntrieval Tasks. In Proceedings of the 17th International\nConference on Distributed Computing in Sensor Systems\n(DCOSS), 367–373. Pafos, Cyprus.\nGrigorescu, S.; Trasnea, B.; Cocias, T.; and Macesanu, G.\n2020. A survey of deep learning techniques for autonomous\ndriving. Journal of Field Robotics, 37(3): 362–386.\nGu, T.; Liu, K.; Dolan-Gavitt, B.; and Garg, S. 2019. Bad-\nNets: Evaluating Backdooring Attacks on Deep Neural Net-\nworks. IEEE Access, 7: 47230–47244.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. InProceedings of the 2016\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 770–778. Las Vegas, NV .\nHuang, H.; Guo, S.; Gui, G.; Yang, Z.; Zhang, J.; Sari, H.;\nand Adachi, F. 2020. Deep Learning for Physical-Layer 5G\nWireless Techniques: Opportunities, Challenges and Solu-\ntions. IEEE Wirel. Commun., 27(1): 214–222.\nJoshi, A.; Jagatap, G.; and Hegde, C. 2021. Adver-\nsarial Token Attacks on Vision Transformers. arXiv\npreprint:2110.04337.\nKhan, S.; Naseer, M.; Hayat, M.; Zamir, S. W.; Khan, F. S.;\nand Shah, M. 2021. Transformers in vision: A survey.arXiv\npreprint:2101.01169.\nKolesnikov, A.; Beyer, L.; Zhai, X.; Puigcerver, J.; Yung, J.;\nGelly, S.; and Houlsby, N. 2020. Big transfer (bit): General\nvisual representation learning. In Proceedings of the 16th\nEuropean Conference on Computer Vision (ECCV), Part V,\n491–507. Glasgow, UK.\nLao, Y .; Yang, P.; Zhao, W.; and Li, P. 2022a. Identification\nfor Deep Neural Network: Simply Adjusting Few Weights!\nIn Proceedings of the 38th IEEE International Conference\non Data Engineering (ICDE), 1328–1341. Kuala Lumpur,\nMalaysia.\nLao, Y .; Zhao, W.; Yang, P.; and Li, P. 2022b. DeepAuth:\nA DNN Authentication Framework by Model-Unique and\nFragile Signature Embedding. In Proceedings of the Thirty-\nSixth AAAI Conference on Artificial Intelligence (AAAI) .\nVirtual.\nLi, Y .; Zhai, T.; Wu, B.; Jiang, Y .; Li, Z.; and Xia, S. 2020.\nRethinking the Trigger of Backdoor Attack. arXiv preprint\narXiv:2004.04692.\nLiu, K.; Dolan-Gavitt, B.; and Garg, S. 2018. Fine-Pruning:\nDefending Against Backdooring Attacks on Deep Neural\nNetworks. In Proceedings of the 21st International Sym-\nposium on Research in Attacks, Intrusions, and Defenses\n(RAID), 273–294. Heraklion, Crete, Greece.\nLiu, Q.; Li, P.; Zhao, W.; Cai, W.; Yu, S.; and Leung, V .\nC. M. 2018a. A Survey on Security Threats and Defen-\nsive Techniques of Machine Learning: A Data Driven View.\nIEEE Access, 6: 12103–12117.\nLiu, Y .; Ma, S.; Aafer, Y .; Lee, W.; Zhai, J.; Wang, W.; and\nZhang, X. 2018b. Trojaning Attack on Neural Networks.\nIn Proceedings of the 25th Annual Network and Distributed\nSystem Security Symposium (NDSS). San Diego, CA.\nLiu, Y .; Ma, X.; Bailey, J.; and Lu, F. 2020. Reflection Back-\ndoor: A Natural Backdoor Attack on Deep Neural Networks.\nIn Proceedings of the 16th European Conference on Com-\nputer Vision (ECCV), Part X, 182–199. Glasgow, UK.\nLiu, Y .; Xie, Y .; and Srivastava, A. 2017. Neural Trojans. In\nProceedings of the 2017 IEEE International Conference on\nComputer Design (ICCD), 45–48. Boston, MA.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. InProceedings of\nthe 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), 9992–10002. Montreal, Canada.\n514\nLv, P.; Ma, H.; Zhou, J.; Liang, R.; Chen, K.; Zhang, S.;\nand Yang, Y . 2021. DBIA: Data-free Backdoor Injec-\ntion Attack against Transformer Networks. arXiv preprint\narXiv:2111.11870.\nMahmood, K.; Mahmood, R.; and van Dijk, M. 2021. On\nthe Robustness of Vision Transformers to Adversarial Ex-\namples. In Proceedings of the 2021 IEEE/CVF International\nConference on Computer Vision (ICCV), 7818–7827. Mon-\ntreal, Canada.\nMao, X.; Qi, G.; Chen, Y .; Li, X.; Duan, R.; Ye, S.; He, Y .;\nand Xue, H. 2022. Towards robust vision transformer. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 12042–12051. New\nOrleans, LA.\nNaseer, M.; Ranasinghe, K.; Khan, S.; Hayat, M.; Khan,\nF. S.; and Yang, M. 2021. Intriguing Properties of Vision\nTransformers. In Advances in Neural Information Process-\ning Systems (NeurIPS), 23296–23308. virtual.\nNaseer, M.; Ranasinghe, K.; Khan, S.; Khan, F. S.; and\nPorikli, F. 2022. On Improving Adversarial Transferability\nof Vision Transformers. In Proceedings of the Tenth Inter-\nnational Conference on Learning Representations (ICLR) .\nVirtual Event.\nNguyen, T. A.; and Tran, A. T. 2021. WaNet - Impercepti-\nble Warping-based Backdoor Attack. In Proceedings of the\n9th International Conference on Learning Representations\n(ICLR). Virtual Event, Austria.\nOh, Y .; Park, S.; and Ye, J. C. 2020. Deep Learning COVID-\n19 Features on CXR Using Limited Training Data Sets.\nIEEE Trans. Medical Imaging, 39(8): 2688–2700.\nQiao, X.; Yang, Y .; and Li, H. 2019. Defending\nNeural Backdoors via Generative Distribution Modeling.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 14004–14013. Vancouver, Canada.\nQiu, H.; Zeng, Y .; Guo, S.; Zhang, T.; Qiu, M.; and Thurais-\ningham, B. M. 2021. DeepSweep: An Evaluation Frame-\nwork for Mitigating DNN Backdoor Attacks using Data\nAugmentation. In Proceedings of the ACM Asia Conference\non Computer and Communications Security (ASIA CCS) ,\n363–377. Virtual Event, Hong Kong.\nSaha, A.; Subramanya, A.; and Pirsiavash, H. 2020. Hidden\nTrigger Backdoor Attacks. In Proceedings of the Thirty-\nFourth AAAI Conference on Artificial Intelligence (AAAI) ,\n11957–11965. New York, NY .\nShao, R.; Shi, Z.; Yi, J.; Chen, P.-Y .; and Hsieh, C.-J. 2021.\nOn the adversarial robustness of visual transformers. arXiv\npreprint:2103.15670.\nSimonyan, K.; and Zisserman, A. 2014. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556.\nSzegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan,\nD.; Goodfellow, I. J.; and Fergus, R. 2014. Intriguing prop-\nerties of neural networks. In Proceedings of the 2nd Inter-\nnational Conference on Learning Representations (ICLR) .\nBanff, Canada.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In Proceedings\nof the 38th International Conference on Machine Learning\n(ICML), 10347–10357. Virtual Event.\nTran, B.; Li, J.; and Madry, A. 2018. Spectral Signatures in\nBackdoor Attacks. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 8011–8021. Montr´eal, Canada.\nUdeshi, S.; Peng, S.; Woo, G.; Loh, L.; Rawshan, L.; and\nChattopadhyay, S. 2022. Model Agnostic Defence Against\nBackdoor Attacks in Machine Learning. IEEE Trans. Re-\nliab., 71(2): 880–895.\nVinayakumar, R.; Alazab, M.; Soman, K.; Poornachandran,\nP.; Al-Nemrat, A.; and Venkatraman, S. 2019. Deep learning\napproach for intelligent intrusion detection system. IEEE\nAccess, 7: 41525–41550.\nWang, B.; Yao, Y .; Shan, S.; Li, H.; Viswanath, B.; Zheng,\nH.; and Zhao, B. Y . 2019. Neural Cleanse: Identifying and\nMitigating Backdoor Attacks in Neural Networks. In Pro-\nceedings of the 2019 IEEE Symposium on Security and Pri-\nvacy (SP), 707–723. San Francisco, CA.\nWang, W.; Xie, E.; Li, X.; Fan, D.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid Vision Trans-\nformer: A Versatile Backbone for Dense Prediction without\nConvolutions. In Proceedings of the 2021 IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 548–558.\nMontreal, Canada.\nYang, P.; Lao, Y .; and Li, P. 2021. Robust Watermarking\nfor Deep Neural Networks via Bi-level Optimization. In\nProceedings of the 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), 14821–14830. Montreal,\nCanada.\nYao, Y .; Li, H.; Zheng, H.; and Zhao, B. Y . 2019. Latent\nBackdoor Attacks on Deep Neural Networks. In Proceed-\nings of the 2019 ACM SIGSAC Conference on Computer and\nCommunications Security (CCS), 2041–2055. London, UK.\nYu, T.; and Li, P. 2022. Degenerate Swin to Win: Plain\nWindow-based Transformer without Sophisticated Opera-\ntions. arXiv preprint arXiv:2211.14255.\nYu, T.; Zhao, G.; Li, P.; and Yu, Y . 2022. BOAT: Bilateral\nLocal Attention Vision Transformer. In Proceedings of the\n33rd British Machine Vision Conference (BMVC). London,\nUK.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.; Tay,\nF. E. H.; Feng, J.; and Yan, S. 2021. Tokens-to-Token ViT:\nTraining Vision Transformers from Scratch on ImageNet.\nIn Proceedings of the 2021 IEEE/CVF International Con-\nference on Computer Vision (ICCV), 538–547. Montreal,\nCanada.\nZhao, B.; and Lao, Y . 2022. CLPA: Clean-label poisoning\navailability attacks using generative adversarial nets. InPro-\nceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 36, 9162–9170.\nZhao, W.; Lao, Y .; and Li, P. 2022. Integrity Authenti-\ncation in Tree Models. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data\nMining (KDD), 2585–2593. Washington, DC.\n515",
  "topic": "Backdoor",
  "concepts": [
    {
      "name": "Backdoor",
      "score": 0.9986506700515747
    },
    {
      "name": "Computer science",
      "score": 0.6767807602882385
    },
    {
      "name": "Exploit",
      "score": 0.6698935031890869
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.565756618976593
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4677104353904724
    },
    {
      "name": "Computer security",
      "score": 0.46687859296798706
    },
    {
      "name": "Transformer",
      "score": 0.41092824935913086
    },
    {
      "name": "Machine learning",
      "score": 0.3838021159172058
    },
    {
      "name": "Engineering",
      "score": 0.1720339059829712
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210142044",
      "name": "VinUniversity",
      "country": "VN"
    },
    {
      "id": "https://openalex.org/I8078737",
      "name": "Clemson University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1316064682",
      "name": "LinkedIn (United States)",
      "country": "US"
    }
  ],
  "cited_by": 14
}