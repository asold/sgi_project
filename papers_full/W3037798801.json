{
  "title": "Transformers are RNNs: Fast Autoregressive Transformers with Linear\\n Attention",
  "url": "https://openalex.org/W3037798801",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4287479182",
      "name": "Katharopoulos, Angelos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747763071",
      "name": "Vyas, Apoorv",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745263781",
      "name": "Pappas, Nikolaos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743120667",
      "name": "Fleuret, François",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2964324019",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970423380",
    "https://openalex.org/W2176412452",
    "https://openalex.org/W3120633509",
    "https://openalex.org/W2964089206",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W2970401203",
    "https://openalex.org/W36903255",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2024490156",
    "https://openalex.org/W2131462252",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2127141656",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2983902802",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964122153",
    "https://openalex.org/W2968917279",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2994759459"
  ],
  "abstract": "Transformers achieve remarkable performance in several tasks but due to their\\nquadratic complexity, with respect to the input's length, they are\\nprohibitively slow for very long sequences. To address this limitation, we\\nexpress the self-attention as a linear dot-product of kernel feature maps and\\nmake use of the associativity property of matrix products to reduce the\\ncomplexity from $\\\\mathcal{O}\\\\left(N^2\\\\right)$ to $\\\\mathcal{O}\\\\left(N\\\\right)$,\\nwhere $N$ is the sequence length. We show that this formulation permits an\\niterative implementation that dramatically accelerates autoregressive\\ntransformers and reveals their relationship to recurrent neural networks. Our\\nlinear transformers achieve similar performance to vanilla transformers and\\nthey are up to 4000x faster on autoregressive prediction of very long\\nsequences.\\n",
  "full_text": "Transformers are RNNs:\nFast Autoregressive Transformers with Linear Attention\nAngelos Katharopoulos 1 2 Apoorv Vyas 1 2 Nikolaos Pappas 3 Franc ¸ois Fleuret2 4 *\nAbstract\nTransformers achieve remarkable performance in\nseveral tasks but due to their quadratic complex-\nity, with respect to the input’s length, they are\nprohibitively slow for very long sequences. To ad-\ndress this limitation, we express the self-attention\nas a linear dot-product of kernel feature maps and\nmake use of the associativity property of matrix\nproducts to reduce the complexity from O\n(\nN2)\nto O(N), where N is the sequence length. We\nshow that this formulation permits an iterative\nimplementation that dramatically accelerates au-\ntoregressive transformers and reveals their rela-\ntionship to recurrent neural networks. Our lin-\near transformersachieve similar performance to\nvanilla transformers and they are up to 4000x\nfaster on autoregressive prediction of very long\nsequences.\n1. Introduction\nTransformer models were originally introduced by Vaswani\net al. (2017) in the context of neural machine translation\n(Sutskever et al., 2014; Bahdanau et al., 2015) and have\ndemonstrated impressive results on a variety of tasks dealing\nwith natural language (Devlin et al., 2019), audio (Sperber\net al., 2018), and images (Parmar et al., 2019). Apart from\ntasks with ample supervision, transformers are also effec-\ntive in transferring knowledge to tasks with limited or no\nsupervision when they are pretrained with autoregressive\n(Radford et al., 2018; 2019) or masked language modeling\nobjectives (Devlin et al., 2019; Yang et al., 2019; Song et al.,\n2019; Liu et al., 2020).\nHowever, these beneﬁts often come with a very high compu-\ntational and memory cost. The bottleneck is mainly caused\n1Idiap Research Institute, Switzerland 2EPFL, Switzerland\n3University of Washington, Seattle, USA4University of Geneva,\nSwitzerland. *Work done at Idiap. Correspondence to: Angelos\nKatharopoulos <ﬁrstname.lastname@idiap.ch>.\nProceedings of the37 th International Conference on Machine\nLearning, Online, PMLR 119, 2020. Copyright 2020 by the au-\nthor(s).\nby the global receptive ﬁeld of self-attention, which pro-\ncesses contexts of N inputs with a quadratic memory and\ntime complexity O\n(\nN2)\n. As a result, in practice trans-\nformers are slow to train and their context is limited. This\ndisrupts temporal coherence and hinders the capturing of\nlong-term dependencies. Dai et al. (2019) addressed the lat-\nter by attending to memories from previous contexts albeit\nat the expense of computational efﬁciency.\nLately, researchers shifted their attention to approaches that\nincrease the context length without sacriﬁcing efﬁciency.\nTowards this end, Child et al. (2019) introduced sparse\nfactorizations of the attention matrix to reduce the self-\nattention complexity toO\n(\nN\n√\nN\n)\n. Kitaev et al. (2020) fur-\nther reduced the complexity to O(Nlog N) using locality-\nsensitive hashing. This made scaling to long sequences\npossible. Even though the aforementioned models can be\nefﬁciently trained on large sequences, they do not speed-up\nautoregressive inference.\nIn this paper, we introduce the linear transformermodel\nthat signiﬁcantly reduces the memory footprint and scales\nlinearly with respect to the context length. We achieve this\nby using a kernel-based formulation of self-attention and\nthe associative property of matrix products to calculate the\nself-attention weights ( §3.2). Using our linear formula-\ntion, we also express causal masking with linear complexity\nand constant memory (§3.3). This reveals the relation be-\ntween transformers and RNNs, which enables us to perform\nautoregressive inference orders of magnitude faster (§3.4).\nOur evaluation on image generation and automatic speech\nrecognition demonstrates that linear transformercan reach\nthe performance levels of transformer, while being up to\nthree orders of magnitude faster during inference.\n2. Related Work\nIn this section, we provide an overview of the most relevant\nworks that seek to address the large memory and computa-\ntional requirements of transformers. Furthermore, we dis-\ncuss methods that theoretically analyze the core component\nof the transformer model, namely self-attention. Finally,\nwe present another line of work that seeks to alleviate the\nsoftmax bottleneck in the attention computation.\narXiv:2006.16236v3  [cs.LG]  31 Aug 2020\nTransformers are RNNs\n2.1. Efﬁcient Transformers\nExisting works seek to improve memory efﬁciency in\ntransformers through weight pruning (Michel et al., 2019),\nweight factorization (Lan et al., 2020), weight quantization\n(Zafrir et al., 2019) or knowledge distillation. Clark et al.\n(2020) proposed a new pretraining objective called replaced\ntoken detection that is more sample efﬁcient and reduces the\noverall computation. Lample et al. (2019) used product-key\nattention to increase the capacity of any layer with negligible\ncomputational overhead.\nReducing the memory or computational requirements with\nthese methods leads to training or inference time speedups,\nbut, fundamentally, the time complexity is still quadratic\nwith respect to the sequence length which hinders scaling\nto long sequences. In contrast, we show that our method\nreduces both memory and time complexity of transformers\nboth theoretically (§3.2) and empirically (§4.1).\nAnother line of research aims at increasing the “context” of\nself-attention in transformers. Context refers to the maxi-\nmum part of the sequence that is used for computing self-\nattention. Dai et al. (2019) introduced Transformer-XL\nwhich achieves state-of-the-art in language modeling by\nlearning dependencies beyond a ﬁxed length context without\ndisrupting the temporal coherence. However, maintaining\nprevious contexts in memory introduces signiﬁcant addi-\ntional computational cost. In contrast, Sukhbaatar et al.\n(2019) extended the context length signiﬁcantly by learning\nthe optimal attention span per attention head, while main-\ntaining control over the memory footprint and computation\ntime. Note that both approaches have the same asymptotic\ncomplexity as the vanilla model. In contrast, we improve the\nasymptotic complexity of the self-attention, which allows\nus to use signiﬁcantly larger context.\nMore related to our model are the works of Child et al.\n(2019) and Kitaev et al. (2020). The former (Child et al.,\n2019) introduced sparse factorizations of the attention ma-\ntrix reducing the overall complexity from quadratic to\nO\n(\nN\n√\nN\n)\nfor generative modeling of long sequences.\nMore recently, Kitaev et al. (2020) proposed Reformer. This\nmethod further reduces complexity to O(Nlog N) by us-\ning locality-sensitive hashing (LSH) to perform fewer dot\nproducts. Note that in order to be able to use LSH, Reformer\nconstrains the keys, for the attention, to be identical to the\nqueries. As a result this method cannot be used for decoding\ntasks where the keys need to be different from the queries.\nIn comparison, linear transformersimpose no constraints\non the queries and keys and scale linearly with respect to the\nsequence length. Furthermore, they can be used to perform\ninference in autoregressive tasks three orders of magnitude\nfaster, achieving comparable performance in terms of vali-\ndation perplexity.\n2.2. Understanding Self-Attention\nThere have been few efforts to better understand self-\nattention from a theoretical perspective. Tsai et al. (2019)\nproposed a kernel-based formulation of attention in trans-\nformers which considers attention as applying a kernel\nsmoother over the inputs with the kernel scores being the\nsimilarity between inputs. This formulation provides a bet-\nter way to understand attention components and integrate\nthe positional embedding. In contrast, we use the kernel\nformulation to speed up the calculation of self-attention and\nlower its computational complexity. Also, we observe that\nif a kernel with positive similarity scores is applied on the\nqueries and keys, linear attention converges normally.\nMore recently, Cordonnier et al. (2020) provided theoret-\nical proofs and empirical evidence that a multi-head self-\nattention with sufﬁcient number of heads can express any\nconvolutional layer. Here, we instead show that a self-\nattention layer trained with an autoregressive objective can\nbe seen as a recurrent neural network and this observation\ncan be used to signiﬁcantly speed up inference time of au-\ntoregressive transformer models.\n2.3. Linearized softmax\nFor many years, softmax has been the bottleneck for train-\ning classiﬁcation models with a large number of categories\n(Goodman, 2001; Morin & Bengio, 2005; Mnih & Hinton,\n2009). Recent works (Blanc & Rendle, 2017; Rawat et al.,\n2019), have approximated softmax with a linear dot product\nof feature maps to speed up the training through sampling.\nInspired from these works, we linearize the softmax atten-\ntion in transformers. Concurrently with this work, Shen\net al. (2020) explored the use of linearized attention for the\ntask of object detection in images. In comparison, we do not\nonly linearize the attention computation, but also develop\nan autoregressive transformer model with linear complex-\nity and constant memory for both inference and training.\nMoreover, we show that through the lens of kernels, every\ntransformer can be seen as a recurrent neural network.\n3. Linear Transformers\nIn this section, we formalize our proposed linear trans-\nformer. We present that changing the attention from the tra-\nditional softmax attention to a feature map based dot product\nattention results in better time and memory complexity as\nwell as a causal model that can perform sequence generation\nin linear time, similar to a recurrent neural network.\nInitially, in §3.1, we introduce a formulation for the trans-\nformer architecture introduced in (Vaswani et al., 2017).\nSubsequently, in §3.2 and §3.3 we present our proposed\nlinear transformerand ﬁnally, in §3.4 we rewrite the trans-\nformer as a recurrent neural network.\nTransformers are RNNs\n3.1. Transformers\nLet x∈RN×F denote a sequence of N feature vectors of\ndimensions F. A transformer is a function T : RN×F →\nRN×F deﬁned by the composition of Ltransformer layers\nT1(·),...,T L(·) as follows,\nTl(x) = fl(Al(x) + x). (1)\nThe function fl(·) transforms each feature independently of\nthe others and is usually implemented with a small two-layer\nfeedforward network. Al(·) is the self attention function and\nis the only part of the transformer that acts across sequences.\nThe self attention function Al(·) computes, for every posi-\ntion, a weighted average of the feature representations of\nall other positions with a weight proportional to a similar-\nity score between the representations. Formally, the input\nsequence xis projected by three matrices WQ ∈RF×D,\nWK ∈RF×D and WV ∈RF×M to corresponding rep-\nresentations Q, K and V. The output for all positions,\nAl(x) = V′, is computed as follows,\nQ= xWQ,\nK = xWK,\nV = xWV,\nAl(x) = V′= softmax\n(QKT\n√\nD\n)\nV.\n(2)\nNote that in the previous equation, the softmax function is\napplied rowwise to QKT. Following common terminology,\nthe Q, Kand V are referred to as the “queries”, “keys” and\n“values” respectively.\nEquation 2 implements a speciﬁc form of self-attention\ncalled softmax attention where the similarity score is the\nexponential of the dot product between a query and a key.\nGiven that subscripting a matrix with ireturns the i-th row\nas a vector, we can write a generalized attention equation\nfor any similarity function as follows,\nV′\ni =\n∑N\nj=1 sim (Qi,Kj) Vj\n∑N\nj=1 sim (Qi,Kj)\n. (3)\nEquation 3 is equivalent to equation 2 if we substitute the\nsimilarity function with sim (q,k) = exp\n(\nqT k√\nD\n)\n.\n3.2. Linearized Attention\nThe deﬁnition of attention in equation 2 is generic and can be\nused to deﬁne several other attention implementations such\nas polynomial attention or RBF kernel attention (Tsai et al.,\n2019). Note that the only constraint we need to impose\nto sim (·), in order for equation 3 to deﬁne an attention\nfunction, is to be non-negative. This includes all kernels\nk(x,y) : R2×F →R+.\nGiven such a kernel with a feature representation φ(x) we\ncan rewrite equation 2 as follows,\nV′\ni =\n∑N\nj=1 φ(Qi)T φ(Kj) Vj\n∑N\nj=1 φ(Qi)T φ(Kj)\n, (4)\nand then further simplify it by making use of the associative\nproperty of matrix multiplication to\nV′\ni =\nφ(Qi)T ∑N\nj=1 φ(Kj) VT\nj\nφ(Qi)T ∑N\nj=1 φ(Kj)\n. (5)\nThe above equation is simpler to follow when the numerator\nis written in vectorized form as follows,\n(\nφ(Q) φ(K)T\n)\nV = φ(Q)\n(\nφ(K)T V\n)\n. (6)\nNote that the feature map φ(·) is applied rowwise to the\nmatrices Qand K.\nFrom equation 2, it is evident that the computational cost of\nsoftmax attention scales with O\n(\nN2)\n, where N represents\nthe sequence length. The same is true for the memory re-\nquirements because the full attention matrix must be stored\nto compute the gradients with respect to the queries, keys\nand values. In contrast, our proposed linear transformer\nfrom equation 5 has time and memory complexityO(N) be-\ncause we can compute ∑N\nj=1 φ(Kj) VT\nj and ∑N\nj=1 φ(Kj)\nonce and reuse them for every query.\n3.2.1. F EATURE MAPS AND COMPUTATIONAL COST\nFor softmax attention, the total cost in terms of multiplica-\ntions and additions scales as O\n(\nN2 max (D,M)\n)\n, where\nDis the dimensionality of the queries and keys and M is\nthe dimensionality of the values. On the contrary, for linear\nattention, we ﬁrst compute the feature maps of dimension-\nality C. Subsequently, computing the new values requires\nO(NCM) additions and multiplications.\nThe previous analysis does not take into account the choice\nof kernel and feature function. Note that the feature func-\ntion that corresponds to the exponential kernel is inﬁnite\ndimensional, which makes the linearization of exact soft-\nmax attention infeasible. On the other hand, the polynomial\nkernel, for example, has an exact ﬁnite dimensional feature\nmap and has been shown to work equally well with the expo-\nnential or RBF kernel (Tsai et al., 2019). The computational\ncost for a linearized polynomial transformer of degree 2\nis O\n(\nND2M\n)\n. This makes the computational complexity\nfavorable when N >D2. Note that this is true in practice\nsince we want to be able to process sequences with tens of\nthousands of elements.\nFor our experiments, that deal with smaller sequences, we\nemploy a feature map that results in a positive similarity\nTransformers are RNNs\nfunction as deﬁned below,\nφ(x) = elu(x) + 1, (7)\nwhere elu(·) denotes the exponential linear unit (Clevert\net al., 2015) activation function. We preferelu(·) over relu(·)\nto avoid setting the gradients to 0 when xis negative. This\nfeature map results in an attention function that requires\nO(NDM) multiplications and additions. In our experi-\nmental section, we show that the feature map of equation 7\nperforms on par to the full transformer, while signiﬁcantly\nreducing the computational and memory requirements.\n3.3. Causal Masking\nThe transformer architecture can be used to efﬁciently train\nautoregressive models by masking the attention computa-\ntion such that the i-th position can only be inﬂuenced by\na position jif and only if j ≤i, namely a position cannot\nbe inﬂuenced by the subsequent positions. Formally, this\ncausal masking changes equation 3 as follows,\nV′\ni =\n∑i\nj=1 sim (Qi,Kj) Vj\n∑i\nj=1 sim (Qi,Kj)\n. (8)\nFollowing the reasoning of §3.2, we linearize the masked\nattention as described below,\nV′\ni =\nφ(Qi)T ∑i\nj=1 φ(Kj) VT\nj\nφ(Qi)T ∑i\nj=1 φ(Kj)\n. (9)\nBy introducing Si and Zi as follows,\nSi =\ni∑\nj=1\nφ(Kj) VT\nj , (10)\nZi =\ni∑\nj=1\nφ(Kj) , (11)\nwe can simplify equation 9 to\nV′\ni = φ(Qi)T Si\nφ(Qi)T Zi\n. (12)\nNote that, Si and Zi can be computed from Si−1 and Zi−1\nin constant time hence making the computational complex-\nity of linear transformers with causal masking linear with\nrespect to the sequence length.\n3.3.1. G RADIENT COMPUTATION\nA naive implementation of equation 12, in any deep learning\nframework, requires storing all intermediate values Si in\norder to compute the gradients. This increases the mem-\nory consumption by max (D,M) times; thus hindering the\napplicability of causal linear attention to longer sequences\nor deeper models. To address this, we derive the gradients\nof the numerator in equation 9 as cumulative sums. This\nallows us to compute both the forward and backward pass\nof causal linear attention in linear time and constant mem-\nory. A detailed derivation is provided in the supplementary\nmaterial.\nGiven the numerator ¯Vi and the gradient of a scalar loss\nfunction with respect to the numerator ∇¯ViL, we derive\n∇φ(Qi)L, ∇φ(Ki)Land ∇ViLas follows,\n∇φ(Qi)L= ∇¯ViL\n\n\ni∑\nj=1\nφ(Kj) VT\nj\n\n\nT\n, (13)\n∇φ(Ki)L=\n\n\nN∑\nj=i\nφ(Qj)\n(\n∇¯Vj L\n)T\n\nVi , (14)\n∇ViL=\n\n\nN∑\nj=i\nφ(Qj)\n(\n∇¯Vj L\n)T\n\n\nT\nφ(Ki) . (15)\nThe cumulative sum terms in equations 9, 13-15 are com-\nputed in linear time and require constant memory with re-\nspect to the sequence length. This results in an algorithm\nwith computational complexity O(NCM) and memory\nO(Nmax (C,M)) for a given feature map of C dimen-\nsions. A pseudocode implementation of the forward and\nbackward pass of the numerator is given in algorithm 1.\n3.3.2. T RAINING AND INFERENCE\nWhen training an autoregressive transformer model the full\nground truth sequence is available. This makes layerwise\nparallelism possible both for fl(·) of equation 1 and the\nattention computation. As a result, transformers are more\nefﬁcient to train than recurrent neural networks. On the\nother hand, during inference the output for timestep iis the\ninput for timestep i+ 1. This makes autoregressive models\nimpossible to parallelize. Moreover, the cost per timestep\nfor transformers is not constant; instead, it scales with the\nsquare of the current sequence length because attention must\nbe computed for all previous timesteps.\nOur proposed linear transformermodel combines the best\nof both worlds. When it comes to training, the computations\ncan be parallelized and take full advantage of GPUs or other\naccelerators. When it comes to inference, the cost per time\nand memory for one prediction is constant for our model.\nThis means we can simply store the φ(Kj) VT\nj matrix as an\ninternal state and update it at every time step like a recurrent\nneural network. This results in inference thousands of\ntimes faster than other transformer models.\nTransformers are RNNs\n3.4. Transformers are RNNs\nIn literature, transformer models are considered to be a fun-\ndamentally different approach to recurrent neural networks.\nHowever, from the causal masking formulation in §3.3 and\nthe discussion in the previous section, it becomes evident\nthat any transformer layer with causal masking can be writ-\nten as a model that, given an input, modiﬁes an internal state\nand then predicts an output, namely a Recurrent Neural\nNetwork (RNN). Note that, in contrast to Universal Trans-\nformers (Dehghani et al., 2018), we consider the recurrence\nwith respect to time and not depth.\nIn the following equations, we formalize the transformer\nlayer of equation 1 as a recurrent neural network. The\nresulting RNN has two hidden states, namely the attention\nmemory sand the normalizer memory z. We use subscripts\nto denote the timestep in the recurrence.\ns0 = 0, (16)\nz0 = 0, (17)\nsi = si−1 + φ(xiWK) (xiWV)T , (18)\nzi = zi−1 + φ(xiWK) , (19)\nyi = fl\n(\nφ(xiWQ)T si\nφ(xiWQ)T zi\n+ xi\n)\n. (20)\nIn the above equations, xi denotes the i-th input and yi the\ni-th output for a speciﬁc transformer layer. Note that our\nformulation does not impose any constraint on the feature\nfunction and it can be used for representing any transformer\nmodel, in theory even the ones using softmax attention. This\nformulation is a ﬁrst step towards better understanding the\nrelationship between transformers and popular recurrent net-\nworks (Hochreiter & Schmidhuber, 1997) and the processes\nused for storing and retrieving information.\n4. Experiments\nIn this section, we analyze experimentally the performance\nof the proposed linear transformer. Initially, in §4.1, we\nevaluate the linearized attention in terms of computational\ncost, memory consumption and convergence on synthetic\ndata. To further showcase the effectiveness oflinear trans-\nformers, we evaluate our model on two real-world appli-\ncations, image generation in §4.2 and automatic speech\nrecognition in §4.3. We show that our model achieves\ncompetitive performance with respect to the state-of-the-art\ntransformer architectures, while requiring signiﬁcantly less\nGPU memory and computation.\nThroughout our experiments, we compare our model with\ntwo baselines, the full transformer with softmax attention\nand the Reformer (Kitaev et al., 2020), the latter being a\nstate-of-the-art accelerated transformer architecture. For the\nReformer, we use a PyTorch reimplementation of the pub-\nAlgorithm 1 Linear transformers with causal masking\nfunction forward(φ(Q), φ(K), V):\nV′←0, S ←0\nfor i= 1,...,N do\nS ←S+ φ(Ki) VT\ni equation 10\n¯Vi ←φ(Qi) S\nend\nreturn ¯V\nend\nfunction backward(φ(Q), φ(K), V, G):\n/* G is the gradient of the loss\nwith respect to the output of\nforward */\nS ←0, ∇φ(Q)L← 0\nfor i= 1,...,N do\nS ←S+ φ(Ki) VT\ni\n∇φ(Qi)L← GiST equation 13\nend\nS ←0, ∇φ(K)L← 0, ∇VL← 0\nfor i= N,..., 1 do\nS ←S+ φ(Qi) GT\ni\n∇ViL← STφ(Ki) equation 15\n∇φ(Ki)L← SVi equation 14\nend\nreturn ∇φ(Q)L, ∇φ(K)L, ∇VL\nend\nlished code and for the full transformer we use the default\nPyTorch implementation. Note that for Reformer, we do\nnot use the reversible layers, however, this does not affect\nthe results as we only measure the memory consumption\nwith respect to the self attention layer. In all experiments,\nwe use softmax (Vaswani et al., 2017) to refer to the stan-\ndard transformer architecture, linear for our proposedlinear\ntransformers and lsh-X for Reformer (Kitaev et al., 2020),\nwhere X denotes the hashing rounds.\nFor training the linear transformers, we use the feature map\nof equation 7. Our PyTorch (Paszke et al., 2019) code with\ndocumentation and examples can be found at https://\nlinear-transformers.com/. The constant memory\ngradient computation of equations 13-15 is implemented in\napproximately 200 lines of CUDA code.\n4.1. Synthetic Tasks\n4.1.1. C ONVERGENCE ANALYSIS\nTo examine the convergence properties oflinear transform-\ners we train on an artiﬁcal copy task with causal masking.\nNamely, the transformers have to copy a series of symbols\nsimilar to the sequence duplication task of Kitaev et al.\n(2020). We use a sequence of maximum length 128 with 10\nTransformers are RNNs\n29 210 211 212 213 214 215 216\nSequence Length\n100\n101\n102\nTime (milliseconds)\n29 210 211 212 213 214 215 216\nSequence Length\n101\n102\n103\nGPU Memory (MB)\nlinear(ours)\nsoftmax\nlsh-1\nlsh-4\nlsh-8\nFigure 1: Comparison of the computational requirements for a forward/backward pass for Reformer (lsh-X), softmax\nattention and linear attention. Linear and Reformer models scale linearly with the sequence length unlike softmax which\nscales with the square of the sequence length both in memory and time. Full details of the experiment can be found in §4.1.\n0 2000 4000 6000 8000 10000\nGradient steps\n10□4\n10□3\n10□2\n10□1\n100\nCross Entropy Loss\nlinear (ours)\nsoftmax\nlsh-4\nFigure 2: Convergence comparison of softmax, linear and\nreformer attention on a sequence duplication task. linear\nconverges stably and reaches the same ﬁnal performance as\nsoftmax. The details of the experiment are in §4.1.\ndifferent symbols separated by a dedicated separator symbol.\nFor all three methods, we train a 4 layer transformer with\n8 attention heads using a batch size of 64 and the RAdam\noptimizer (Liu et al., 2019) with a learning rate of 10−3\nwhich is reduced to 10−4 after 3000 updates. Figure 2 de-\npicts the loss with respect to the number of gradient steps.\nWe observe that linear converges smoothly and reaches a\nlower loss than lsh due to the lack of noise introduced by\nhashing. In particular, it reaches the same loss as softmax.\n4.1.2. MEMORY AND COMPUTATIONAL REQUIREMENTS\nIn this subsection, we compare transformers with respect\nto their computational and memory requirements. We com-\npute the attention and the gradients for a synthetic input\nwith varying sequence lengths N ∈{29,210,..., 216}and\nmeasure the peak allocated GPU memory and required time\nfor each variation of transformer. We scale the batch size\ninversely with the sequence length and report the time and\nmemory per sample in the batch.\nEvery method is evaluated up to the maximum sequence\nlength that ﬁts the GPU memory. For this benchmark we\nuse an NVidia GTX 1080 Ti with 11GB of memory. This\nresults in a maximum sequence length of 4,096 elements\nfor softmax and 16,384 for lsh-4 and lsh-8. As expected,\nsoftmax scales quadratically with respect to the sequence\nlength. Our method is faster and requires less memory than\nthe baselines for every conﬁguration, as seen in ﬁgure 1.\nWe observe that both Reformer and linear attention scale\nlinearly with the sequence length. Note that although the\nasymptotic complexity for Reformer is O(Nlog N), log N\nis small enough and does not affect the computation time.\n4.2. Image Generation\nTransformers have shown great results on the task of condi-\ntional or unconditional autoregressive generation (Radford\net al., 2019; Child et al., 2019), however, sampling from\ntransformers is slow due to the task being inherently se-\nquential and the memory scaling with the square of the\nsequence length. In this section, we train causally masked\ntransformers to predict images pixel by pixel. Our achieved\nperformance in terms of bits per dimension is on par with\nsoftmax attention while being able to generate images more\nthan 1,000 times faster and with constant memory per\nimage from the ﬁrst to the last pixel. We refer the reader\nto our supplementary for comparisons in terms of training\nevolution, quality of generated images and time to generate\na single image. In addition, we also compare with a faster\nsoftmax transformer that caches the keys and values during\ninference, in contrast to the PyTorch implementation.\n4.2.1. MNIST\nFirst, we evaluate our model on image generation with au-\ntoregressive transformers on the widely used MNIST dataset\n(LeCun et al., 2010). The architecture for this experiment\ncomprises 8 attention layers with 8 attention heads each. We\nTransformers are RNNs\nMethod Bits/dim Images/sec\nSoftmax 0.621 0.45 (1 ×)\nLSH-1 0.745 0.68 (1.5 ×)\nLSH-4 0.676 0.27 (0.6 ×)\nLinear (ours) 0.644 142.8 (317 ×)\nTable 1: Comparison of autoregressive image generation of\nMNIST images. Our linear transformers achieve almost the\nsame bits/dim as the full softmax attention but more than\n300 times higher throughput in image generation. The full\ndetails of the experiment are in §4.2.1.\nset the embedding size to 256 which is 32 dimensions per\nhead. Our feed forward dimensions are 4 times larger than\nour embedding size. We model the output with a mixture\nof 10 logistics as introduced by Salimans et al. (2017). We\nuse the RAdam optimizer with a learning rate of 10−4 and\ntrain all models for 250 epochs. For the reformer baseline,\nwe use 1 and 4 hashing rounds. Furthermore, as suggested\nin Kitaev et al. (2020), we use 64 buckets and chunks with\napproximately 32 elements. In particular, we divide the\n783 long input sequence to 27 chunks of 29 elements each.\nSince the sequence length is realtively small, namely only\n784 pixels, to remove differences due to different batch sizes\nwe use a batch size of 10 for all methods.\nTable 1 summarizes the results. We observe that linear\ntransformers achieve almost the same performance, in terms\nof ﬁnal perplexity, as softmax transformers while being\nable to generate images more than 300 times faster. This is\nachieved due to the low memory requirements of our model,\nwhich is able to simultaneously generate 10,000 MNIST\nimages with a single GPU. In particular, the memory is\nconstant with respect to the sequence length because the\nonly thing that needs to be stored between pixels are the\nsi and zi values as described in equations 18 and 19. On\nthe other hand, both softmax and Reformer require memory\nthat increases with the length of the sequence.\nImage completions and unconditional samples from our\nMNIST model can be seen in ﬁgure 3. We observe that\nour linear transformer generates very convincing samples\nwith sharp boundaries and no noise. In the case of image\ncompletion, we also observe that the transformer learns to\nuse the same stroke style and width as the original image\neffectively attending over long temporal distances. Note that\nas the achieved perplexity is more or less the same for all\nmodels, we do not observe qualitative differences between\nthe generated samples from different models.\n4.2.2. CIFAR-10\nThe beneﬁts of our linear formulation increase as the se-\nquence length increases. To showcase that, we train 16 layer\nMethod Bits/dim Images/sec\nSoftmax 3.47 0.004 (1 ×)\nLSH-1 3.39 0.015 (3.75 ×)\nLSH-4 3.51 0.005 (1.25 ×)\nLinear (ours) 3.40 17.85 (4,462 ×)\nTable 2: We train autoregressive transformers for 1 week\non a single GPU to generate CIFAR-10 images. Our linear\ntransformer completes 3 times more epochs than softmax,\nwhich results in better perplexity. Our model generates\nimages 4,000×faster than the baselines. The full details of\nthe experiment are in §4.2.2.\ntransformers to generate CIFAR-10 images (Krizhevsky\net al., 2009). For each layer we use the same conﬁguration\nas in the previous experiment. For Reformer, we use again\n64 buckets and 83 chunks of 37 elements, which is approx-\nimately 32, as suggested in the paper. Since the sequence\nlength is almost 4 times larger than for the previous exper-\niment, the full transformer can only be used with a batch\nsize of 1 in the largest GPU that is available to us, namely\nan NVidia P40 with 24GB of memory. For both the linear\ntransformer and reformer, we use a batch size of 4. All\nmodels are trained for 7 days. We report results in terms of\nbits per dimension and image generation throughput in table\n2. Note that although the main point of this experiment is\nnot the ﬁnal perplexity, it is evident that as the sequence\nlength grows, the fast transformer models become increas-\ningly more efﬁcient per GPU hour, achieving better scores\nthan their slower counterparts.\nAs the memory and time to generate a single pixel scales\nquadratically with the number of pixels for both Reformer\nand softmax attention, the increase in throughput for our lin-\near transformer is even more pronounced. In particular, for\nevery image generated by the softmax transformer, our\nmethod can generate 4,460 images . Image completions\nand unconditional samples from our model can be seen in\nﬁgure 4. We observe that our model generates images with\nspatial consistency and can complete images convincigly\nwithout signiﬁcantly hindering the recognition of the image\ncategory. For instance, in ﬁgure 4b, all images have success-\nfully completed the dog’s nose (ﬁrst row) or the windshield\nof the truck (last row).\n4.3. Automatic Speech Recognition\nTo show that our method can also be used for non-\nautoregressive tasks, we evaluate the performance of linear\ntransformers in end-to-end automatic speech recognition\nusing Connectionist Temporal Classiﬁcation (CTC) loss\n(Graves et al., 2006). In this setup, we predict a distribu-\ntion over phonemes for each input frame in a non autore-\nTransformers are RNNs\nUnconditional samples\nImage completion\n(a)\n (b)\n (c)\nFigure 3: Unconditional samples and image completions\ngenerated by our method for MNIST. (a) depicts the oc-\ncluded orignal images, (b) the completions and (c) the orig-\ninal. Our model achieves comparable bits/dimension to\nsoftmax, while having more than 300 times higher through-\nput, generating 142 images/second. For details see §4.2.1.\nMethod Validation PER Time/epoch (s)\nBi-LSTM 10.94 1047\nSoftmax 5.12 2711\nLSH-4 9.33 2250\nLinear (ours) 8.08 824\nTable 3: Performance comparison in automatic speech\nrecognition on the WSJ dataset. The results are given in\nthe form of phoneme error rate (PER) and training time per\nepoch. Our model outperforms the LSTM and Reformer\nwhile being faster to train and evaluate. Details of the exper-\niment can be found in §4.3.\ngressive fashion. We use the 80 hour WSJ dataset (Paul\n& Baker, 1992) with 40-dimensional mel-scale ﬁlterbanks\nwithout temporal differences as features. The dataset con-\ntains sequences with 800 frames on average and a maximum\nsequence length of 2,400 frames. For this task, we also com-\npare with a bidirectional LSTM (Hochreiter & Schmidhuber,\n1997) with 3 layers of hidden size 320. We use the Adam\noptimizer (Kingma & Ba, 2014) with a learning rate of10−3\nwhich is reduced when the validation error stops decreas-\ning. For the transformer models, we use 9 layers with 6\nheads with the same embedding dimensions as for the im-\nage experiments. As an optimizer, we use RAdam with an\ninitial learning rate of 10−4 that is divided by 2 when the\nvalidation error stops decreasing.\nAll models are evaluated in terms of phoneme error rate\n(PER) and training time per epoch. We observe that linear\nUnconditional samples\nImage completion\n(a)\n (b)\n (c)\nFigure 4: Unconditional samples and image completions\ngenerated by our method for CIFAR-10. (a) depicts the\noccluded orignal images, (b) the completions and (c) the\noriginal. As the sequence length grows linear transformers\nbecome more efﬁcient compared to softmax attention. Our\nmodel achieves more than 4,000 times higher throughput\nand generates 17.85 images/second. For details see §4.2.2.\noutperforms the recurrent network baseline and Reformer\nboth in terms of performance and speed by a large margin, as\nseen in table 3. Note that the softmax transformer, achieves\nlower phone error rate in comparison to all baselines, but\nis signiﬁcantly slower. In particular, linear transformer\nis more than 3×faster per epoch. We provide training\nevolution plots in the supplementary.\n5. Conclusions\nIn this work, we presented linear transformer, a model that\nsigniﬁcantly reduces the memory and computational cost\nof the original transformers. In particular, by exploiting\nthe associativity property of matrix products we are able to\ncompute the self-attention in time and memory that scales\nlinearly with respect to the sequence length. We show that\nour model can be used with causal masking and still retain\nits linear asymptotic complexities. Finally, we express the\ntransformer model as a recurrent neural network, which\nallows us to perform inference on autoregressive tasks thou-\nsands of time faster.\nThis property opens a multitude of directions for future\nresearch regarding the storage and retrieval of information\nin both RNNs and transformers. Another line of research\nto be explored is related to the choice of feature map for\nlinear attention. For instance, approximating the RBF kernel\nwith random Fourier features could allow us to use models\npretrained with softmax attention.\nTransformers are RNNs\nAcknowledgements\nAngelos Katharopoulos was supported by the Swiss Na-\ntional Science Foundation under grant numbers FNS-30209\n”ISUL” and FNS-30224 ”CORTI”. Apoorv Vyas was sup-\nported by the Swiss National Science Foundation under\ngrant number FNS-30213 ”SHISSM”. Nikolaos Pappas was\nsupported by the Swiss National Science Foundation under\ngrant number P400P2 183911 ”UNISON”.\nReferences\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.\nIn Proceedings of the 5th International Conference on\nLearning Representations, San Diego, CA, USA, 2015.\nBlanc, G. and Rendle, S. Adaptive sampled softmax with\nkernel based sampling. arXiv preprint arXiv:1712.00527,\n2017.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nClark, K., Luong, M.-T., Le, Q. V ., and Manning, C. D.\nELECTRA: Pre-training text encoders as discriminators\nrather than generators. In International Conference on\nLearning Representations, 2020.\nClevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast\nand accurate deep network learning by exponential linear\nunits (ELUs). arXiv preprint arXiv:1511.07289, 2015.\nCordonnier, J.-B., Loukas, A., and Jaggi, M. On the relation-\nship between self-attention and convolutional layers. In\nInternational Conference on Learning Representations,\n2020.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q., and\nSalakhutdinov, R. Transformer-XL: Attentive language\nmodels beyond a ﬁxed-length context. In Proceedings of\nthe 57th Annual Meeting of the Association for Compu-\ntational Linguistics, pp. 2978–2988, Florence, Italy, July\n2019. Association for Computational Linguistics.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and\nKaiser, Ł. Universal transformers. arXiv preprint\narXiv:1807.03819, 2018.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pp. 4171–\n4186, Minneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics.\nGoodman, J. Classes for fast maximum entropy training.\nIn 2001 IEEE International Conference on Acoustics,\nSpeech, and Signal Processing. Proceedings (Cat. No.\n01CH37221), volume 1, pp. 561–564. IEEE, 2001.\nGraves, A., Fern ´andez, S., Gomez, F., and Schmidhuber,\nJ. Connectionist temporal classiﬁcation: labelling un-\nsegmented sequence data with recurrent neural networks.\nIn Proceedings of the 23rd international conference on\nMachine learning, pp. 369–376, 2006.\nHochreiter, S. and Schmidhuber, J. Long short-term memory.\nNeural computation, 9(8):1735–1780, 1997.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The\nefﬁcient transformer. arXiv preprint arXiv:2001.04451,\n2020.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nLample, G., Sablayrolles, A., Ranzato, M. A., Denoyer,\nL., and Jegou, H. Large memory layers with product\nkeys. In Wallach, H., Larochelle, H., Beygelzimer, A.,\nd ´Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.), Advances\nin Neural Information Processing Systems 32, pp. 8546–\n8557. Curran Associates, Inc., 2019.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\nand Soricut, R. Albert: A lite bert for self-supervised\nlearning of language representations. In International\nConference on Learning Representations, 2020.\nLeCun, Y ., Cortes, C., and Burges, C. Mnist handwritten\ndigit database. 2010.\nLiu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and\nHan, J. On the variance of the adaptive learning rate and\nbeyond. arXiv preprint arXiv:1908.03265, 2019.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach, 2020.\nMichel, P., Levy, O., and Neubig, G. Are sixteen heads\nreally better than one? In Wallach, H., Larochelle, H.,\nBeygelzimer, A., d’ Alch´e-Buc, F., Fox, E., and Garnett,\nR. (eds.), Advances in Neural Information Processing\nSystems 32, pp. 14014–14024. Curran Associates, Inc.,\n2019.\nMnih, A. and Hinton, G. E. A scalable hierarchical dis-\ntributed language model. In Advances in neural informa-\ntion processing systems, pp. 1081–1088, 2009.\nTransformers are RNNs\nMorin, F. and Bengio, Y . Hierarchical probabilistic neural\nnetwork language model. In Aistats, volume 5, pp. 246–\n252. Citeseer, 2005.\nParmar, N., Ramachandran, P., Vaswani, A., Bello, I., Lev-\nskaya, A., and Shlens, J. Stand-alone self-attention in\nvision models. In Wallach, H., Larochelle, H., Beygelz-\nimer, A., d’ Alch´e-Buc, F., Fox, E., and Garnett, R. (eds.),\nAdvances in Neural Information Processing Systems 32,\npp. 68–80. Curran Associates, Inc., 2019.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. In Advances in Neural Information\nProcessing Systems, pp. 8024–8035, 2019.\nPaul, D. B. and Baker, J. M. The design for the wall street\njournal-based csr corpus. In Proceedings of the workshop\non Speech and Natural Language, pp. 357–362. Associa-\ntion for Computational Linguistics, 1992.\nRadford, A., Narasimhan, K., Salimans, T., , and Sutskever,\nI. Improving language understanding by generative pre-\ntraining. In OpenAI report, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. OpenAI Blog, 1(8):9, 2019.\nRawat, A. S., Chen, J., Yu, F. X. X., Suresh, A. T., and\nKumar, S. Sampled softmax with random fourier features.\nIn Advances in Neural Information Processing Systems,\npp. 13834–13844, 2019.\nSalimans, T., Karpathy, A., Chen, X., and Kingma, D. P.\nPixelcnn++: Improving the pixelcnn with discretized lo-\ngistic mixture likelihood and other modiﬁcations. arXiv\npreprint arXiv:1701.05517, 2017.\nShen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efﬁ-\ncient attention: Attention with linear complexities. arXiv\npreprint arXiv:1812.01243, 2020.\nSong, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y . MASS:\nMasked sequence to sequence pre-training for language\ngeneration. In Chaudhuri, K. and Salakhutdinov, R. (eds.),\nProceedings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Machine\nLearning Research, pp. 5926–5936, Long Beach, Califor-\nnia, USA, 09–15 Jun 2019. PMLR.\nSperber, M., Niehues, J., Neubig, G., Stker, S., and Waibel,\nA. Self-attentional acoustic models. In 19th Annual Con-\nference of the International Speech Communication Asso-\nciation (InterSpeech 2018), Hyderabad, India, September\n2018.\nSukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.\nAdaptive attention span in transformers. In Proceedings\nof the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pp. 331–335, Florence, Italy, July\n2019. Association for Computational Linguistics.\nSutskever, I., Vinyals, O., and Le, Q. V . Sequence to se-\nquence learning with neural networks. In Advances in\nNeural Information Processing Systems 27, pp. 3104–\n3112. Curran Associates, Inc., 2014.\nTsai, Y .-H. H., Bai, S., Yamada, M., Morency, L.-P., and\nSalakhutdinov, R. Transformer dissection: An uniﬁed\nunderstanding for transformer’s attention via the lens of\nkernel. In Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pp. 4343–4352, Hong Kong,\nChina, November 2019. Association for Computational\nLinguistics.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\nis all you need. In NIPS, 2017.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J. G., Salakhut-\ndinov, R., and Le, Q. V . Xlnet: Generalized autore-\ngressive pretraining for language understanding. CoRR,\nabs/1906.08237, 2019.\nZafrir, O., Boudoukh, G., Izsak, P., and Wasserblat, M.\nQ8BERT: quantized 8bit BERT. CoRR, abs/1910.06188,\n2019.\nTransformers are RNNs\nSupplementary Material for\nTransformers are RNNs:\nFast Autoregressive Transformers with Linear Attention\nA. Gradient Derivation\nIn the ﬁrst section of our supplementary material, we derive in detail the gradients for causally masked linear transformers\nand show that they can be computed in linear time and constant memory. In particular, we derive the gradients of a scalar\nloss with respect to the numerator of the following equation,\nV′\ni =\nφ(Qi)T ∑i\nj=1 φ(Kj) VT\nj\nφ(Qi)T ∑i\nj=1 φ(Kj)\n. (21)\nThe gradient with respect to the denominator and the fraction are efﬁciently handled by autograd. Without loss of generality,\nwe can assume that Qand Kalready contain the vectors mapped by φ(·), hence given the numerator\n¯Vi = QT\ni\ni∑\nj=1\nKjVT\nj , (22)\nand ∇¯VLwe seek to compute ∇QL, ∇KLand ∇VL. Note that Q∈RN×D, K ∈RN×D and V ∈RN×M. To derive the\ngradients, we ﬁrst express the above equation for a single element without using vector notation,\n¯Vie =\nD∑\nd=1\nQid\ni∑\nj=1\nKjdVje =\nD∑\nd=1\ni∑\nj=1\nQidKjdVje. (23)\nSubsequently we can start deriving the gradients for Qby taking the partial derivative for any Qlt, as follows\n∂L\n∂Qlt\n=\nM∑\ne=1\n∂L\n∂¯Vle\n∂¯Vle\n∂Qlt\n=\nM∑\ne=1\n∂L\n∂¯Vle\n\n\nl∑\nj=1\nKjtVje\n\n. (24)\nIf we write the above equation as a matrix product of gradients it becomes,\n∇QiL= ∇¯ViL\n\n\ni∑\nj=1\nKjVT\nj\n\n\nT\n, (25)\nproving equation 13 from the main paper. In equation 24 we made use of the fact that Qlt only affects ¯Vl hence we do not\nneed to sum over ito compute the gradients. However, for K and V this is not the case. In particular, Kj affects all ¯Vi\nwhere i≥j. Consequently, we can write the partial derivative of the loss with respect to Klt as follows,\n∂L\n∂Klt\n=\nM∑\ne=1\nN∑\ni=l\n∂L\n∂¯Vie\n∂¯Vie\n∂Klt\n=\nM∑\ne=1\nN∑\ni=l\n∂L\n∂¯Vie\n∂\n(∑D\nd=1\n∑i\nj=1 QidKjdVje\n)\n∂Klt\n=\nM∑\ne=1\nN∑\ni=l\n∂L\n∂¯Vie\nQitVle.\n(26)\nAs for Qwe can now write the gradient in vectorized form,\n∇KiL=\n\n\nN∑\nj=i\nQj\n(\n∇¯Vj L\n)T\n\nVi, (27)\nTransformers are RNNs\nproving equation 14 from the paper. Following the same reasoning, we can compute the partial derivative of the loss with\nrespect to Vlt and prove equation 15. Note that the cumulative sum matrices for the gradient with respect to Qand Khave\nthe same size, however one is computed in the forward direction (summing from 1 to N) similarly to the forward pass and\nthe other is computed in the backwards direction (summing from N to 1) similar to backpropagation through time done in\nRNNs.\nB. Training Evolution\nIn ﬁgure 5 we present the training evolution of all transformer models in our experiments. For the MNIST experiment\n(Fig. 5a) we train all methods for 250 epochs. The sequence length is small enough so that the training time does not\nvary signiﬁcantly for all methods. We observe that our method converges on par with softmax attention outperforming\nsigniﬁcantly both reformer variants.\nOn the other hand, for CIFAR-10 (Fig. 5b) we train all methods for a ﬁxed amount of time, namely 7 days. We observe that\nlsh-1 and linear complete signiﬁcantly more epochs than softmax and lsh-4 and achieve better performance. This gap is\nexpected to increase with a further increase in sequence length.\nFinally, in our last experiment on automatic speech recognition (Fig. 5c), softmax outperforms signiﬁcantly both Reformer\nand linear in terms of convergence. Note that linear is 3×faster per epoch which means it has completed approximately 4\ntimes more epochs in comparison to softmax. Even though softmax attention is better in this task, we observe that linear\ntransformers signiﬁcantly outperform Reformer both in terms of convergence and ﬁnal performance.\n0 50 100 150 200 250\nEpochs\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTest bpd softmax\nlsh-1\nlsh-4\nlinear (ours)\n(a) MNIST\n0 10 20 30 40 50 60\nEpochs\n3.4\n3.6\n3.8\n4.0\n4.2\nTest bpd (b) CIFAR-10\n0 50 100 150 200\nWall-clock time (hours)\n5\n10\n15\n20\n25\n30Validation PER (c) Speech Recognition\nFigure 5: Training evolution of transformers for all our experiments. It can be observed that linear transformersconverge\nconsistently faster than Reformer and in the autoregressive experiments on par with softmax. For MNIST all methods are\ntrained for 250 epochs while for CIFAR we train for 7 days. In the speech recognition experiments all methods are trained\nto convergence. The details of the experiments can be found in §4.2.1, §4.2.2 and §4.3 in the main paper.\nC. Image Generation Throughput Discussion\nC.1. Stateful softmax attention\nIn §4.2 of the main paper, we report the image generation throughput and we compare withsoftmax transformer and lsh. In\nthis section we create another baseline, denoted as stateful-softmax, that implements a softmax autoregressive transformer\nas a recurrent model. Namely, all the keys and values are saved and then passed to the model again when predicting the next\nelement of the sequence. The state of this recurrent model is the set of keys and values which has size proportional to the\nsequence length. This is qualitatively different to our proposed model that has a state with ﬁxed dimensions and computing\nthe i-th state given the previous one has ﬁxed computational cost regardless of i.\nTable 4 summarizes the results. We observe that stateful-softmax is signiﬁcantly faster than vanilla transformers. However,\nits complexity is still quadratic with respect to the sequence length and our forumlation is more than 50×faster for CIFAR-10.\nMoreover, we would like to point out that implementing a similar stateful attention for Reformer is not a trivial task as the\nsorting and chunking operations need to be performed each time a new input is provided.\nTransformers are RNNs\nMethod Bits/dim Images/sec\nSoftmax 0.621 0.45 (1 ×)\nStateful-softmax 0.621 7.56 (16.8 ×)\nLSH-1 0.745 0.68 (1.5 ×)\nLSH-4 0.676 0.27 (0.6 ×)\nLinear (ours) 0.644 142.8 (317 ×)\n(a) Image generation on MNIST\nMethod Bits/dim Images/sec\nSoftmax 3.47 0.004 (1 ×)\nStateful-softmax 3.47 0.32 (80 ×)\nLSH-1 3.39 0.015 (3.75 ×)\nLSH-4 3.51 0.005 (1.25 ×)\nLinear (ours) 3.40 17.85 (4,462 ×)\n(b) Image generation on CIFAR-10\nTable 4: Comparison of autoregressive image generation throughput of MNIST and CIFAR-10 images. The experiment can\nbe found in §4.2 in the main paper. For stateful-softmax we save the keys and values and reuse them for predicting the next\nelement. A detailed description of this extra baseline can be found in §C.1.\nC.2. Equalizing the batch size\nIn the previous sections we evaluate the throughput of all transformer variants for the task of autoregressive image generation.\nHowever, another important factor to consider is latency, namely the total time required to produce a single image. To this\nend, we use a batch size of 1 and measure the time required by all methods to generate a single image. In addition to running\nthe inference on the GPU, we also evaluate the time required on CPU. The results are reported in table 5.\nMethod Seconds (CPU) Seconds (GPU)\nSoftmax 72.6 (13.2 ×) 10.2 (1.4 ×)\nStateful-softmax 7.4 (1.3 ×) 10.4 (1.42 ×)\nLSH-1 46.0 (8.3 ×) 19.2 (2.6 ×)\nLSH-4 112.0 (20 ×) 55.8 (7.6 ×)\nLinear (ours) 5.5 (1×) 7.3 (1×)\n(a) Image generation on MNIST\nMethod Seconds (CPU) Seconds (GPU)\nSoftmax 8651.4 (191.8 ×) 300.1 (4.9 ×)\nStateful-softmax 71.9 (1.6 ×) 70.4 (1.14 ×)\nLSH-1 2318.9 (51.4 ×) 221.6 (3.6 ×)\nLSH-4 5263.7 (116.7 ×) 683.9 (11.1 ×)\nLinear (ours) 45.1 (1×) 61.3 (1×)\n(b) Image generation on CIFAR-10\nTable 5: Comparison of the time required to generate a single image with autoregressive transformers on MNIST and\nCIFAR-10. We run all methods with a batch size of 1 both on CPU and GPU and report the total time in seconds. For all\nnumbers in the table, lower is better.\nWe observe that all methods underutilize the GPU and achieve signiﬁcantly smaller image generation throughput than the\none shown in table 4. The proposed linear transformer is faster than all the methods and in particular it is almost 6.6 ×faster\nthan softmax transformers for generating an image on CIFAR-10. Note that our linear autoregressive transformer is the only\nmethod that is faster on the CPU than on the GPU in every case. This is due to the fact that computing the attention as an\nRNN has such a low cost that the main computational bottleneck becomes the inevitable outer loop over the sequence.\nD. Qualitative Results on Image Generation\nIn this section we provide qualitative results for our image generation experiments. Since the perplexity of all models is\napproximately the same, as expected, the qualitative differences are not signiﬁcant. A rather interesting observation however\nis that the Reformer models provide signiﬁcantly fewer variations in their unconditional samples. Moreover, we observe that\nimage completion is a signiﬁcantly easier task than unconditional generation as all models perform signiﬁcantly better.\nTransformers are RNNs\n(a) Softmax\n (b) Linear (ours)\n(c) LSH-1\n (d) LSH-4\nFigure 6: Unconditional samples from the transformer models trained with MNIST. See §4.2.1 in the main paper.\nTransformers are RNNs\n(a) Occluded\n (b) Softmax\n (c) Linear (ours)\n (d) LSH-1\n (e) LSH-4\n (f) Original\nFigure 7: MNIST digit completion from all trained models. See §4.2.1 in the main paper.\nTransformers are RNNs\n(a) Softmax\n (b) Linear (ours)\n(c) LSH-1\n (d) LSH-4\nFigure 8: Unconditional samples from the transformer models trained with CIFAR-10. See §4.2.2 in the main paper.\nTransformers are RNNs\n(a) Occluded\n (b) Softmax\n (c) Linear (ours)\n (d) LSH-1\n (e) LSH-4\n (f) Original\nFigure 9: CIFAR-10 image completions from all trained transformer models. See §4.2.2 in the main paper.",
  "topic": "Autoregressive model",
  "concepts": [
    {
      "name": "Autoregressive model",
      "score": 0.8354194164276123
    },
    {
      "name": "Transformer",
      "score": 0.6838200092315674
    },
    {
      "name": "Quadratic equation",
      "score": 0.5742597579956055
    },
    {
      "name": "Computer science",
      "score": 0.4875011146068573
    },
    {
      "name": "Algorithm",
      "score": 0.4057255685329437
    },
    {
      "name": "Mathematics",
      "score": 0.3659849762916565
    },
    {
      "name": "Voltage",
      "score": 0.30664384365081787
    },
    {
      "name": "Engineering",
      "score": 0.1676291823387146
    },
    {
      "name": "Econometrics",
      "score": 0.12020301818847656
    },
    {
      "name": "Electrical engineering",
      "score": 0.10069838166236877
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210102939",
      "name": "IAP Research (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I7495430",
      "name": "Idiap Research Institute",
      "country": "CH"
    }
  ]
}