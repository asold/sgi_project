{
  "title": "Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: study protocol for a prospective, randomised experiment using patient vignettes",
  "url": "https://openalex.org/W4400804042",
  "year": 2024,
  "authors": [
    {
      "id": null,
      "name": "Kämmer, Juliane E",
      "affiliations": [
        "University Hospital of Bern",
        "University of Bern"
      ]
    },
    {
      "id": "https://openalex.org/A4377979744",
      "name": "Hautz, Wolf E.",
      "affiliations": [
        "University of Bern",
        "University Hospital of Bern"
      ]
    },
    {
      "id": null,
      "name": "Krummrey, Gert",
      "affiliations": [
        "Bern University of Applied Sciences"
      ]
    },
    {
      "id": null,
      "name": "Sauter, Thomas C",
      "affiliations": [
        "University of Bern",
        "University Hospital of Bern"
      ]
    },
    {
      "id": null,
      "name": "Penders, Dorothea",
      "affiliations": [
        "Charité - Universitätsmedizin Berlin"
      ]
    },
    {
      "id": null,
      "name": "Birrenbach, Tanja",
      "affiliations": [
        "University Hospital of Bern",
        "University of Bern"
      ]
    },
    {
      "id": null,
      "name": "Bienefeld, Nadine",
      "affiliations": [
        "ETH Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2107872096",
    "https://openalex.org/W4311542542",
    "https://openalex.org/W2154651644",
    "https://openalex.org/W2168490582",
    "https://openalex.org/W2944015619",
    "https://openalex.org/W2101756318",
    "https://openalex.org/W2163581778",
    "https://openalex.org/W2298759628",
    "https://openalex.org/W4319460874",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4382343782",
    "https://openalex.org/W4392818978",
    "https://openalex.org/W2239951476",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4389178822",
    "https://openalex.org/W4385230848",
    "https://openalex.org/W6859443379",
    "https://openalex.org/W4385379701",
    "https://openalex.org/W4377289493",
    "https://openalex.org/W4385580499",
    "https://openalex.org/W4386819477",
    "https://openalex.org/W4361299673",
    "https://openalex.org/W4282916733",
    "https://openalex.org/W4396832329",
    "https://openalex.org/W4390749717",
    "https://openalex.org/W4320920036",
    "https://openalex.org/W2545846385",
    "https://openalex.org/W6861000901",
    "https://openalex.org/W4313894269",
    "https://openalex.org/W2025221131",
    "https://openalex.org/W4381546750",
    "https://openalex.org/W3005338805",
    "https://openalex.org/W2141222241",
    "https://openalex.org/W2962052654",
    "https://openalex.org/W2120230938",
    "https://openalex.org/W2034960795",
    "https://openalex.org/W2510575414",
    "https://openalex.org/W2153893045",
    "https://openalex.org/W2098341608",
    "https://openalex.org/W4311611160",
    "https://openalex.org/W2169860454",
    "https://openalex.org/W1998832934",
    "https://openalex.org/W1994106556",
    "https://openalex.org/W2140353941",
    "https://openalex.org/W2125611482",
    "https://openalex.org/W2548421507",
    "https://openalex.org/W2167373494",
    "https://openalex.org/W2029907219",
    "https://openalex.org/W1979617571",
    "https://openalex.org/W2001981791",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4389156617",
    "https://openalex.org/W1951724000",
    "https://openalex.org/W2072500831",
    "https://openalex.org/W2970837303",
    "https://openalex.org/W2083119019",
    "https://openalex.org/W2619682269",
    "https://openalex.org/W2078852878",
    "https://openalex.org/W1975402326",
    "https://openalex.org/W4283731530",
    "https://openalex.org/W4388452582",
    "https://openalex.org/W4389951577",
    "https://openalex.org/W2094938541",
    "https://openalex.org/W2007567868",
    "https://openalex.org/W3030790076",
    "https://openalex.org/W4390114552",
    "https://openalex.org/W4398240750",
    "https://openalex.org/W3207483985",
    "https://openalex.org/W4396218105",
    "https://openalex.org/W4389802502",
    "https://openalex.org/W2087484885"
  ],
  "abstract": "Introduction Versatile large language models (LLMs) have the potential to augment diagnostic decision-making by assisting diagnosticians, thanks to their ability to engage in open-ended, natural conversations and their comprehensive knowledge access. Yet the novelty of LLMs in diagnostic decision-making introduces uncertainties regarding their impact. Clinicians unfamiliar with the use of LLMs in their professional context may rely on general attitudes towards LLMs more broadly, potentially hindering thoughtful use and critical evaluation of their input, leading to either over-reliance and lack of critical thinking or an unwillingness to use LLMs as diagnostic aids. To address these concerns, this study examines the influence on the diagnostic process and outcomes of interacting with an LLM compared with a human coach, and of prior training vs no training for interacting with either of these ‘coaches’. Our findings aim to illuminate the potential benefits and risks of employing artificial intelligence (AI) in diagnostic decision-making. Methods and analysis We are conducting a prospective, randomised experiment with N=158 fourth-year medical students from Charité Medical School, Berlin, Germany. Participants are asked to diagnose patient vignettes after being assigned to either a human coach or ChatGPT and after either training or no training (both between-subject factors). We are specifically collecting data on the effects of using either of these ‘coaches’ and of additional training on information search, number of hypotheses entertained, diagnostic accuracy and confidence. Statistical methods will include linear mixed effects models. Exploratory analyses of the interaction patterns and attitudes towards AI will also generate more generalisable knowledge about the role of AI in medicine. Ethics and dissemination The Bern Cantonal Ethics Committee considered the study exempt from full ethical review (BASEC No: Req-2023-01396). All methods will be conducted in accordance with relevant guidelines and regulations. Participation is voluntary and informed consent will be obtained. Results will be published in peer-reviewed scientific medical journals. Authorship will be determined according to the International Committee of Medical Journal Editors guidelines.",
  "full_text": "1\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access  \nEffects of interacting with a large \nlanguage model compared with a \nhuman coach on the clinical diagnostic \nprocess and outcomes among fourth-\n \nyear medical students: study protocol \nfor a prospective, randomised \nexperiment using patient\n \nvignettes\nJuliane E Kämmer    ,1 Wolf E Hautz    ,1 Gert Krummrey    ,2 \nThomas C Sauter\n    ,1 Dorothea Penders    ,3,4 Tanja Birrenbach    ,1 \nNadine Bienefeld\n    5\nTo cite: Kämmer JE, Hautz WE, \nKrummrey G, et al.  Effects of \ninteracting with a large language \nmodel compared with a human \ncoach on the clinical diagnostic \nprocess and outcomes among \nfourth-\n year medical students:\n \nstudy protocol for a prospective, \nrandomised experiment using \npatient vignettes. BMJ Open \n2024;14:e087469. doi:10.1136/\nbmjopen-2024-087469\n ► Prepublication history \nand additional supplemental \nmaterial for this paper are \navailable online. To view these \nfiles, please visit the journal \nonline (https://doi.org/10.1136/\n \nbmjopen-2024-087469).\nReceived 10 \nApril 2024\nAccepted 02 July 2024\nFor numbered affiliations see \nend of article.\nCorrespondence to\nDr Juliane E Kämmer;  \n \njuliane.\n \nkaemmer@\n \nunibe.\n \nch\nProtocol\n© Author(s) (or their \nemployer(s)) 2024. Re-\n use \npermitted under CC BY\n-\n NC.\n No \ncommercial re-\n use.\n See rights \nand permissions. Published by \nBMJ.\nABSTRACT\nIntroduction V ersatile large language models (LLMs) \nhave the potential to augment diagnostic decision-\n making \nby assisting dia\ngnosticians, thanks to their ability to \nengage in open-\n ended,\n natural conversations and their \ncomprehensive knowledge access. Yet the novelty of LLMs \nin diagnostic decision-\n making introduces uncertainties \nregarding their impact.\n Clinicians unfamiliar with the use \nof LLMs in their professional context may rely on general \nattitudes towards LLMs more broadly, potentially hindering \nthoughtful use and critical evaluation of their input, leading \nto either over-\n reliance and lack of critical thinking or an \nunwillingness to use LLMs as dia\ngnostic aids. To address \nthese concerns, this study examines the influence on the \ndiagnostic process and outcomes of interacting with an \nLLM compared with a human coach, and of prior training \nvs no training for interacting with either of these ‘coaches’. \nOur findings aim to illuminate the potential benefits and \nrisks of employing artificial intelligence (AI) in diagnostic \ndecision-\n \nmaking.\nMethods and analysis\n W\ne are conducting a prospective, \nrandomised experiment with N=158 fourth-\n year medical \nstudents from Charité Medical School,\n Berlin, Germany. \nParticipants are asked to diagnose patient vignettes after \nbeing assigned to either a human coach or ChatGPT and \nafter either training or no training (both between-\n subject \nfactors).\n We are specifically collecting data on the effects \nof using either of these ‘coaches’ and of additional training \non information search, number of hypotheses entertained, \ndiagnostic accuracy and confidence. Statistical methods \nwill include linear mixed effects models. Exploratory \nanalyses of the interaction patterns and attitudes towards \nAI will also generate more generalisable knowledge about \nthe role of AI in medicine.\nEthics and dissemination\n The Bern Cantonal Ethics \nCommittee considered the stud\ny exempt from full ethical \nreview (BASEC No: Req-\n 2023-\n 01396).\n All methods will \nbe conducted in accordance with relevant guidelines \nand regulations. Participation is voluntary and informed \nconsent will be obtained. Results will be published in \npeer-\n reviewed scientific medical journals.\n Authorship will \nbe determined according to the International Committee of \nMedical Journal Editors guidelines.\nINTRODUCTION\nMedical diagnostic errors, defined as wrong, \ndelayed or missed diagnoses, pose a serious \nthreat to quality of care and patient safety, \naffecting 5%–15% of the patients who \npresent to healthcare systems.\n1–3 In the 2015 \nlandmark report ‘Improving Diagnosis in \nHealthcare’, the US National Academy of \nMedicine warned that ‘most people will \nexperience a diagnostic error throughout \ntheir lifetime, sometimes with devastating \nconsequences’.\n4 Importantly, among harmful \ndiagnostic errors, 84% are preventable but at \nthe same time have higher rates of mortality \nSTRENGTHS AND LIMITATIONS OF THIS STUDY\n ⇒ The study is a prospective randomised controlled \nstudy of advanced medical students diagnosing \ncomplex patient cases.\n ⇒ The study includes a comparison of consultations \nwith either a large language model or a human \ncoach, enhancing the clinical validity of the study.\n ⇒ The detailed analysis of both the diagnostic pro-\ncess and its outcomes adds depth to the research \nfindings.\n ⇒ Only advanced medical students are included in the \nstudy, potentially constraining the generalisability of \nthe results to broader medical student populations.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n2\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access  \nthan other types of error (29% vs 7%). 5 6 In a systematic \nreview of malpractice claims worldwide, diagnostic errors \nwere the most common and most expensive type of claim, \nreflecting 26%–63% of all cases.\n7 Consequently, there is \nan urgent need for improving diagnostic decision-\n making \nin healthcare.\nIn recent years, specialised computerised diagnostic \ndecision \nsupport systems such as differential diagnosis \ngenerators have been developed, showing the poten-\ntial to improve the quality of diagnoses.\n8 Additionally, \nsince large language models (LLMs) based on genera-\ntive pre-\n trained transformer (GPT) methodology have \nbeen \nwidely disseminated, applications such as ChatGPT \n(Open AI) have raised hopes that such tools will become \na valuable asset for (medical) education,\n9 10 as well as for \nconsultation and clinical decision support. 11–15 Recently, \nresearchers have endeavoured to explore ChatGPT’s \npotential and limitations in the healthcare domain, \ntesting its medical proficiency. Across countries, they \nhave demonstrated its ability to successfully pass medical \nlicensing exams,\n9 10 16 17  which may render ChatGPT-  \nbased chatbots a particularly useful resource for junior \nphysicians. Thus, by leveraging their broad medical \nknowledge base, their capacity to engage in open-\n ended, \nnatural conversations and \ntheir ability to process complex \n(patient) data, ChatGPT-\n based chatbots have the poten\n-\ntial to augment diagnostic decision-\n making processes 18 \nand assist learners in medical education settings.10\nHowever, the novelty of LLMs in diagnostic decision-  \nmaking introduces uncertainties regarding their impact. \nClinicians \nunfamiliar with using LLMs in their profes-\nsional context may rely on general positive or negative \nattitudes towards artificial intelligence (AI), potentially \nhindering thoughtful use and critical evaluation of their \ninput, leading to either over-\n reliance and lack of critical \nthinking or the neglect of AI’\ns potential.19–23 It is, there-\nfore, imperative to comprehensively explore the extent, \napplication and constraints of LLMs in clinical decision \nsupport to guarantee their conscientious and efficient \nimplementation in practice.\n12 18 24 25  To address these \nconcerns, this prospective, randomised controlled clinical \nvignette study examines the influence of decision support \nusing an LLM (ChatGPT) on the diagnostic process \nand outcomes compared with that of a human coach. \nThis will advance the understanding of how human–AI \ncollaboration can be leveraged to enhance diagnostic \ndecision-\n \nmaking.\nLeveraging AI for enhanced diagnostic decision-making\nWhat makes an LLM such as ChatGPT a potentially useful \ncoach during the diagnostic journey? In their review of \nrecent literature on ChatGPT in clinical decision support, \nFerdush et al\n18 listed a number of relevant attributes: For \nexample, (a) LLMs can analyse patient data and take into \naccount relevant clinical guidelines, understand complex \nmedical information and aid in data interpretation; using \nidentified patterns in patient data, LLMs can propose rele-\nvant differential diagnoses of high accuracy,\n26 potentially \ncounteracting premature closure. 27 (b) Thanks to their \nvast knowledge base of similar cases reported in medical \nliterature, LLMs can remind professionals of rare or \ncomplex diseases typically in danger of being overlooked. \n(c) LLMs possess pertinent knowledge spanning multiple \nmedical specialties and healthcare settings, making them \na useful resource in any specialty and allowing the inte-\ngration of information from different medical domains. \n(d) With LLMs, healthcare professionals can access clin-\nical guidelines and best practices in real time and from \none source, which supports them in making informed \ndecisions.\n18 Last, (e) LLMs may take over the role of advi-\nsors,28 29  and (peer) coaches or teachers 30 31  who guide \nlearners through the diagnostic process by reminding \nthem of important steps to take or differential diagnoses \nto consider.\nThere are also potential drawbacks to consider in the \ncontext of diagnostic decision-\n making: (a) LLMs have \nbeen obser\nved to occasionally miss relevant patient \ninformation, exhibit hallucinations (ie, confident yet \nwrong responses), display biases stemming from biased \ntraining data (eg, due to under-\n representation \nof certain \ndemographics) and show limited contextual under -\nstanding.18 (b) Further, there is the fear that over- reliance \non LLMs may lead to reduced learning opportunities 11 \nand deskilling and hence an increased risk of diagnostic \nerrors in the long run. Last and contrary to this, (c) clini-\ncians may refute insights provided by LLMs as they tend \nto overlook the support offered by computerised diag-\nnostic decision support systems.\n22\nThus, given the novelty of LLMs and the lack of expe-\nrience with using GPTs in the diagnostic process and for \nmedical education, a deeper exploration of the bene-\nfits, limitations and possible applications of LLMs for \nmedical diagnosis and education is warranted. Our study, \ntherefore, aims to (a) investigate the effects of an LLM \n(ChatGPT) on the diagnostic process, accuracy, number \nof diagnostic hypotheses and user confidence and (b) \nexplore how the LLM is used during diagnosis. As LLMs \ngenerate human-\n like text responses in conversational \nsettings, we compare the use of ChatGPT assistance with \nthat of assistance from a human coach with more experi\n-\nence, the usual resource for junior physicians in medical \neducational settings.\n32\nThe role of the hypothesis space for diagnostic error\nOf the multiple reasons for diagnostic error (such as \ntechnical failures or poorly cooperating patients), cogni-\ntive factors such as faulty information synthesis most \nfrequently contribute to diagnostic error.\n6 33 To illustrate, \n89% of diagnostic error malpractice claims involved fail-\nures in clinical reasoning, the largest study on such claims \nfound.\n34\nDecades of research into clinical reasoning, diag-\nnostic decision-\n making, or one of its many synonyms \nprovide some insights into possible causes and remedies \nof diagnostic error\n.27 It is now well established that clini-\ncians generate diagnostic hypotheses within minutes of \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n3\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access\nan encounter with a patient, 35 36  sometimes even much \nfaster.37 These initial hypotheses are of paramount impor-\ntance for the accuracy of the final diagnosis because \nclinicians hardly ever add other hypotheses to the diag-\nnoses they consider later on.\n35 This is an important point \nbecause—in contrast to the process of scientific inquiry—\nphysicians tend to conduct diagnostic tests that confirm \ntheir initial hypothesis rather than potentially refuting \nit.\n35 38 Furthermore, they distort incoming additional find-\nings in favour of the initial idea. 39 40 What distinguishes \nexpert diagnosticians from novices is neither faster nor \nmore but just better initial hypotheses.\n41 42  This under -\nstanding of the importance of the initial hypothesis for \nthe accuracy of the final diagnosis aligns well with the \nobservation that the most commonly observed biases in \nclinical reasoning—availability bias, confirmation bias, \nsatisfaction of search and premature closure\n27 43–47 —all \nrelate to the space of initially considered differential \ndiagnoses.\nGiven that broadening the differential diagnoses can \nmitigate diagnostic errors,\n48–51 it appears imperative to \nraise awareness among diagnosticians about this possi-\nbility. Furthermore, the quality of LLM output and advice \nis sensitive to the formulation of inquiries.\n52 53 Therefore, \nproviding single training instructions that offer a ratio-\nnale for expanding the hypothesis space in diagnostic \ndecision-\n making, along with practical illustrations on \nhow to effectively elicit information from their coaches \n(whether human or ChatGPT) will likely enhance the \ncoaches’ \nimpact. This single training will improve partic-\nipants’ reasoning and ability to leverage the coach’s assis-\ntance, leading to better diagnostic outcomes, such as an \nincreased number and relevance of diagnostic hypotheses \nand greater accuracy in the final diagnosis. Consequently, \nwe will examine the impact of instructional training \n(training vs no training) along with human versus AI assis-\ntance. We aim to provide insights that elucidate the neces-\nsary guidance for the effective use of LLMs in diagnostic \ndecision-\n \nmaking.\nMETHODS AND ANALYSIS\nThis study seeks to elucidate the differential (or analo-\ngous) use patterns between users of ChatGPT and those \nusing a human coach in the context of diagnostic decision-\n \nmaking, along with their respective impacts \non the diag-\nnostic process and outcomes as well as user confidence. \nThere is also significant practical interest in examining \nwhether ChatGPT exhibits a more pronounced bene-\nficial effect on diagnostic accuracy and the quantity of \ndifferential diagnoses considered, potentially attributable \nto its heightened computational capabilities.\n12 Addition-\nally, we seek to assess whether brief instructional training \nemphasising the importance of expanding the hypothesis \nspace augments these effects. To achieve this, our primary \nfocus is on modelling the dependent variables diagnostic \naccuracy and number of generated differential diagnoses \nusing linear mixed-\n effects models54 in R.55\nWe have been collecting data during an online experi-\nment with medical students at the Charité Medical School \nin Berlin. Students have been invited to participate via \nmailing lists in exchange for financial remuneration (€35 \nper participant). Data collection began on 22 April 2024 \nand is planned to last until the end of June 2024. The \nstudy has a randomised, single-\n blind study design with a \n2×2 factorial design, with the source of assistance (human \ncoach vs ChatGPT) and training (training vs no training) \nas between-\n subjects \nfactors (see figure\n \n1). Participants are \nrandomly assigned to the type of assistance they receive \nand the training/no training condition.\nSample size\nA sample size of N=158 was determined using G*Power \nV.3.1.9.7\n56 for a 2×2 analysis of variance (ANOVA), to detect \na practically relevant medium effect size with α=0.05 and \nβ=0.80. Each of the four subgroups is randomly assigned \nan approximately equal number of participants.\nInclusion and exclusion\nAll (N=640) fourth-\n year medical \nstudents (in a 6-\n year \nprogramme) from Charité Medical School in Berlin are \neligible to take part in the study\n. Students are recruited \nvia faculty mailing lists, posters and online platforms of \nthe Charité Skills Lab. Students 18 years or older who \nsign the informed consent can be included. Coaches \nin the ‘human condition’ are two medical interns who \nhave recently completed their sixth year of studies at the \nCharité Medical School, have passed their state examina-\ntion and are now working in the hospital. Human coaches \nare thus 2 years more advanced than the participants. \nThey are paid €20 per hour.\nMain study procedures\nData collection is taking place remotely in two online \nsessions (see figure\n \n1). In the first session, students provide \ntheir written informed consent (see online supplemental \ninformation) and watch a short general introduction \nvideo on the idea and methods of LLMs to level off poten-\ntial differences in experience with LLMs among partici-\npants. For this, a freely available, up-\n to-\n date introductor\ny \nvideo was chosen (https://youtu.be/2IK3DFHRFfw?si=\n \nuSnEBQv2mhPmIOis). Then, participants fill in a short \nbaseline sur\nvey (via https://www.soscisurvey.de) on their \nmedical expertise, attitudes towards and experience with \nChatGPT and other forms of AI, and their demographics \n(see online supplemental e table 1 for an overview of all \nquestionnaires and our OSF repository https://osf.io/\n \ncbpr3/?view_only=e5e94231ddd546b491c2e07f43f02c88\n \nfor all original items and their English translation). To \nensure that participants completed the first session, they \nare asked to send a codeword (‘Psychologie’), which is \nprovided on the last slide of the survey, by email to the \nexperimenter.\nThe second session is administered via MS Teams. \nUp to six students are invited to the same session. On \narrival, participants are welcomed by the experimenter \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n4\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access  \nand receive a short introduction to the study. Then, \nparticipants are randomly assigned to the human or AI \ncondition and training or no training subgroup by the \nexperimenters using a computer-\n generated randomis\n-\nation process. Participants are blinded to the training \nversus no training condition but are aware of the random \nallocation procedure to the human versus AI condition \n(from the general study information; see online supple-\nmental information). Participants are sent to individual \nbreakout rooms and receive a link to access their experi-\nmental session. They then work individually on the exper-\niment in their breakout room with the opportunity to \nchat with the experimenter in case of problems or ques-\ntions. After finishing, they return to the meeting room \nand are informed about the debriefing (which comes at a \nlater date; see Debriefing below), thanked and dismissed. \nExperimenters note all deviations from the protocols, \ntechnical issues and participants’ comments so that the \nquality of data collection can be evaluated.\nGet to know\nThe experimental session starts with a get-\n to-\n know \nphase \ndesigned to acquaint participants with their respective \nmode of assistance, whether the human coach ‘Toni’ or \nChatGPT. This short introduction highlights the strengths \nof each coach, such as Toni’s background in medicine, \nincluding successful completion of medical studies and \npractical medical experience, and ChatGPT’s expansive \nFigure 1 Study design. AI, artificial intelligence; ChatGPT , OpenAI’s generative pre-  trained transformer; LLMs, lar ge language \nmodels; R, randomisation.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n5\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access\nknowledge base (see online supplemental informa-\ntion). Participants are also made aware of the limitations \ninherent to each coach, such as Toni’s potential knowl-\nedge gaps compared with a senior physician and the \npossibility of ‘hallucinations’ with ChatGPT. This initial \nstep is crucial in addressing participants’ onboarding \nneeds, facilitating their evaluation of the capabilities and \nintentions of their human or ChatGPT coach.\n57 By estab-\nlishing familiarity and understanding of the strengths and \nlimitations, participants can begin to develop trust in their \nrespective coach, which is vital for effective collaboration \nand decision-\n \nmaking.58 The get-  to-  know phase does not \ncontain any examples of when and how to interact with \nthe coach, which is only part of the training.\nT\nraining\nAfterward, participants either see the training instruc-\ntions on the screen (training condition) or not \n(no-\n training condition), depending on the subgroup \nthey are randomly assigned to. The training instruc\n-\ntions are designed to heighten awareness regarding the \npotential for diagnostic errors and delineate three prev-\nalent factors contributing to diagnostic errors: limited \nknowledge, premature closure and overconfidence.\n1 59  \nThese are briefly explained. Additionally, the instructions \nprovide exemplar inquiries that participants may pose \nto their respective coach (whether human or ChatGPT) \nto effectively navigate these three challenges (see online \nsupplemental information for complete instructions). \nThe training instructions are no longer available once the \nparticipant proceeds to the next page.\nTask: diagnose cases\nThe main task is then to diagnose two patient cases (in \nrandom order). The cases are based on published cases \nof real patients\n43 60  and represent ambiguous emer -\ngency cases with a known correct diagnosis but a main \ncompeting diagnosis that has to be considered (case 1: \npulmonary embolism vs myocardial infarction; case 2: \naortic dissection vs stroke). On the patient case page, \npatient information including ECGs, laboratory results \nof blood samples and patient history is presented in a \npatient chart. On the same page, participants have access \nto a field in which to chat with their coaches, who reply \nin real time. Participants are instructed not to use any \nother sources of information than those on the screen. \nParticipants are asked to record all differential diag-\nnoses considered in a separate field on the same page. \nAll clicks, chats and entries are logged with time stamps. \nFigure\n \n2 shows a screenshot of a patient case page (in \nFigure 2 Scr eenshot of a patient case page. Starting on the left, there is a window showing the current step within the \nexperiment and the patient chart with several subcategories, above the field for entering the differential diagnoses; on the right \nis the chat window (here, in the artificial intelligence condition).\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n6\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access  \nGerman). When leaving the patient case page, partici-\npants are asked to assess the likelihood of each diagnosis \ngenerated (on a Visual Analogue Scale of 0–100), to \nprovide a reason for their most likely diagnosis (open \nanswer) and to report their intended next steps if this \nwere a real patient (open answer).\nHuman versus AI coach\nThe LLM used in this study is OpenAI’s ChatGPT \n(version gpt-\n 4–0613, DeploymentName=‘GPT\n-\n 4’, MaxT\no-\nkens=1000, Temperature=1.0f), accessed via the appli-\ncation programming interface provided by Microsoft \nAzure’s cloud platform (hosted in the ‘Switzerland North’ \ndata centre).\nThe human coach is randomly drawn from the two \nmedical interns who serve as coaches and who received a \n5-\n hour training on the study purpose, the chat system and \nthe philosophy of peer teaching 30 and deliberate reflec-\ntion,61 as well as scripts with standard answers to frequent \nrequests (as identified in a pilot phase) to ensure that \nthey could reply quickly and in a standardised way. Both \nhuman coaches are introduced by the unisex name ‘Toni’ \nto avoid potential gender bias and to keep their identi-\nties confidential. Human coaches sit at their computer \nat home and chat via the experimental interface with \nthe participant. The interface was created using Micro-\nsoft’s ‘Blazor Server App’ web framework. Both ChatGPT \nand the human coach received the instruction to act as \na medical coach and accompany fourth-\n year \nmedical \nstudents through the diagnostic process, including asking \nguiding questions such as ‘Which findings support/\noppose your hypothesis?’ following the logic of deliberate \nreflection\n61 62 (for the complete instructions, see system \nprompt in online supplemental information).\nQuestionnaire per case\nFollowing each patient case, participants respond to ques-\ntions pertaining to their case perception, encompassing \nfactors such as perceived difficulty and familiarity with the \ndiagnosis, as well as their assessment of the competence \nand support provided by the coaches (online supple-\nmental e table1).\nFinal questionnaire\nA final questionnaire is administered after completion of \nboth patient cases to assess the perceived usefulness of,\n63 \nsatisfaction with64 and credibility of the coaches.65\nDebriefing\nOn re-\n entering the virtual meeting room, participants are \ntold \nabout future debriefing, thanked and dismissed by \nthe experimenter. Following the data collection phase, a \ncomprehensive written debriefing will be provided. This \ndebriefing will include solutions to the patient cases, an \ninformation package containing the training instructions \n(also in the no-\n training condition), as well as links to addi\n-\ntional resources on clinical reasoning and LLMs.\nPilot study\nIn a pilot study involving N=11 fourth-\n year medical \nstudents and medical interns (Mage=26 years, SD=4.9, 55% \nfemale), the case material was tested for intelligibility \nand feasibility without assistance from a human coach or \nChatGPT. Diagnoses were elicited as free text responses. \nFor case 1, the correct diagnosis (pulmonary embolism) \nwas listed by 27% of participants as the most likely diag-\nnosis, and in case 2, the correct diagnosis (aortic dissec-\ntion) by 0%, confirming that we had adequately selected \ndifficult cases to prevent any ceiling effects.\nData to be analysed\nData will be in the form of questionnaires, process \nmeasures (eg, timestamps of clicks), chat protocols and \nratings. Data will be entered into a web-\n based database \nthat fulfils the requirements of the Swiss Human Research \nAct. \nParticipants will be asked to generate a ‘study ID,’ \nwhich guarantees their anonymity but allows for matching \nbaseline surveys with the data collected during the exper-\nimental session. All data will be digital. Only authorised \nstudy personnel will have access to personal informa-\ntion (eg, email address) during data collection. Any data \nshared with external parties (eg, collaborators) will be \ndeidentified to remove all personally identifiable infor -\nmation. Only anonymised, coded data will be published \ntogether with DOIs in the OSF repository to make them \nfindable. Primary and secondary endpoints as well as \ncontrol variables are listed in table\n \n1.\nStatistical analyses\nData analysis will be conducted with R.\n55 For statistical \nanalyses, we will use generalised linear mixed models \n(GLMMs), complemented by suitable post hoc tech-\nniques, particularly for subgroup analyses. Standard \ndescriptive statistics and graphical representations will be \nemployed, along with normality testing to assess assump-\ntions for the proper application of parametric testing \nmethodologies. Prior to data analysis, data quality will be \nchecked by, for example, range checks for data values. To \nevaluate the randomisation procedure to the conditions, \nwe will compare the four groups regarding their demo-\ngraphics (eg, age, gender, prior experience with LLMs) \nwith ANOVAs. To determine whether participants in the \ntraining condition read the training instructions, we will \ncompare the time they spent on the page with a minimum \nreading time threshold. This threshold will be set slightly \nbelow the average time spent on the page by participants \nin the no-\n training condition.\nT\no determine the accuracy of the differential diagnoses, \nfirst, they will be automatically coded to International \nClassification of Diseases (10th revision; ICD-\n 10) codes \nusing a proprietar\ny German-\n language natural language \nprocessing engine (A\nverbis Health Discovery, https://\n \naverbis.com), which maps ICD-\n 10-\n German modification \ncodes to unstructured text. 50% of the diagnoses will be \nrandomly selected for cross-\n checking by two expert raters, \nblinded to the condition, to ensure the accuracy of the \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n7\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access\nautomated ICD matching. If accuracy of this automated \nmatching turns out to be below 95%, the proportion will \nbe increased to 60%, 70% and so forth for human cross-\n \nchecking. Then, these codes will be compared with the \ncorrect codes of the two cases. Accuracy will be calculated \nas the number of steps required within the ICD taxonomy \nto get from one diagnosis to the other\n, as described \nelsewhere.62\nTo assess the impact of the type of assistance and \ntraining on the primary and secondary outcome variables, \nwe will conduct successively more complex GLMMs, 54 \nstarting with participant ID and item ID as random inter-\ncepts, and gender and conditions as fixed effects. The \ndependent variables will include diagnostic accuracy, \nthe number of differential diagnoses and the secondary \nendpoints (see table\n \n1). Sensitivity analyses are planned to \ncheck the robustness of our findings. These will include \nalternative model specifications, assessing interaction \neffects, applying different methods for handling missing \ndata (eg, imputation methods, complete case analysis) \nTable 1 Overview of variables of inter est\nVariable Values Explanation, example Type of data\nPrimary endpoints\n \n Diagnostic accuracy of the most likely \ndiagnosis/3 most likely diagnoses\nRange 0–n Number of steps r\nequired within the ICD \ntaxonomy to get from the listed diagnosis \nto the correct diagnosis\n70\nAutomated coding (see \ntext), checked by 2 blinded \nphysicians\n \n Number of diagnoses Range 1–n Number of dif\nferential diagnoses \ngenerated\nProcess measure\nSecondary endpoints\n \n \nInformation search Range 1–5 Number of pieces of diagnostic \ninformation acquired\nProcess measure\n \n Diversity of diagnoses A\nverage distance between ICD-\n 10 codes \nof dif\nferential diagnoses generated\nCalculated post hoc\n \n Confidence in diagnoses Range 0–100 Rated likelihood per diagnosis per case Survey\n \n Satisfaction with coach/per\nceived \nusefulness of coach\nRange 1–5 Average of 2 items Survey\n \n Time on case Duration in min:sec Pr\nocess measure\nControl variables\n \n \nGender male, female, other Survey\n \n \nMedical knowledge Range 0–200; median \nsplit (competent vs less \ncompetent)\nAverage score of last 3 progress tests \nmedicine\n71\nSurvey\n \n Experience working with LLMs Descriptive statistics of fr\nequency, \nconfidence\nSurvey\n \n General trust in LLMs Range 1–5 A\nverage value of 6-\n item scale Survey\n \n Cr\nedibility of ChatGPT during case Range 1–5 Average value of 5-\n item cr\nedibility \nsubscale of the TMS scale65\nSurvey\nExploratory analyses of\n \n T\nype of reasons for most likely diagnosis Categories to be \ndetermined\nFor example, occurrence of typical \nsymptoms, advice of coach, guessing\nCategorised by 2 trained \nblinded raters\n \n T\nype of next steps Categories to be \ndetermined\nFor example, initiation of specific \ntherapeutic steps, further consultation \nwith senior physician\nCategorised by 2 trained \nblinded raters\n \n Per\nception of case and coach Range 1–7 Descriptive statistics of case difficulty, \nperceived own and coach expertise\nSurvey\n \n Chat with coach Categories to be \ndetermined\nFor example, time point, fr\nequency \nof different types of requests (eg, \nconfirmation, falsification), length of chats\nCategorised by 2 trained \nblinded raters\n \n Accuracy of answers of coach T\nype of mistakes by human coaches vs \nChatGPT\nCategorised by 2 trained \nblinded raters\n \n Reactions to mistakes by participant Categories to be \ndetermined\nFor example, clarifying questions Categorised by 2 trained \nblinded raters\n \n Impact of coach on diagnostic list Categories to be \ndetermined\nFor example, adding or r\nemoving \ndiagnoses\nCategorised by 2 trained \nblinded raters\nChatGPT, OpenAI’s generative pre- trained transformer; ICD, Inter national Classification of Diseases; LLM, large language model; TMS, transactive \nmemory system.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n8\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access  \nand subgroup analyses. For example, we will successively \ninclude more control variables, such as participants’ \nmedical competence\n41 42 and general trust in LLMs, 58 66 \nto account for potential confounders and gain a deeper \nunderstanding of the conditions under which LLMs are \nmost effective.\nIn preparation for the qualitative analysis of prompts \nand usage patterns of coaches, all chat interactions and \nopen answers will be coded using MAXQDA software. \nCoding categories (eg, confirmatory or knowledge \nquestions) will be derived inductively and deductively \nby trained raters with domain knowledge. Two trained \nraters will independently code the material, blinded to \nthe conditions (human coach vs ChatGPT, training vs no \ntraining). Rater agreement will be reported as coefficient \nkappa. Exploratory analyses and subgroup analyses will \nbe conducted to characterise successful and unsuccessful \nprompts and the differences between consulting a human \ncoach versus ChatGPT. Further, the timing of using the \ncoach (early or late in the process), the frequency and \ntype of errors made by the coaches and the impact of \nthe (correct or incorrect) diagnoses proposed by the \ncoaches on the diagnoses listed by the participants will \nbe explored.\nPatient and public involvement\nWe intend to disseminate the main results to the partic-\nipants and public in a format that is suitable for a non-\n \nspecialist audience. There was no patient nor public \ninvolvement in the design and conduct of the study\n.\nETHICS AND DISSEMINATION\nThis is a prospective, randomised controlled experi-\nmental study. Participant anonymity for participants \nwill be respected at all times by anonymisation of their \ndata. The Bern Cantonal Ethics Committee considered \nthe study exempt from full ethical review (BASEC No: \nReq-\n 2023-\n 01396). All methods will be carried out in \naccordance with relevant guidelines and regulations. \nAll students will participate \nvoluntarily and will sign an \ninformed consent after receiving written and oral infor -\nmation about the study.\nResults will be presented at scientific meetings. Results \nwill be published in peer-\n reviewed scientific \njournals and \nauthorship will be determined according to International \nCommittee of Medical Journal Editors guidelines.\nDISCUSSION\nOur study has several strengths. First, it is a prospective \nrandomised controlled experiment involving advanced \nmedical students diagnosing complex patient cases, \nallowing us to investigate both diagnostic outcomes and \nprocesses. Second, the study compares consultations \nwith either an LLM or a human coach, both of which are \npractically relevant advisors for medical students solving \ncomplex cases. Third, the detailed analysis of both the \ndiagnostic process and its outcomes will provide a deeper \ninsight into the research findings.\nOur study also has several limitations. First, it focuses \nsolely on fourth-\n year medical students, which may restrict \nthe generalisability of the results to a broader medical \nstudent population or to residents and practising physi\n-\ncians. Also, the study is set within a medical education \ncontext, involving complex cases that are challenging for \nthis level of training. Second, only approximately half \nof our questionnaires have been validated by previous \nresearch. This is due to the lack of suitable instruments, \ngiven the novelty of our study’s focus. For instance, we \nwere unable to find scientifically validated questions that \nassess trust in an AI chat partner. Third, although we plan \nto conduct in-\n depth qualitative analyses of the interac\n-\ntions between participants and either human coaches \nor ChatGPT, insights into the underlying mechanisms of \nhow AI influences decision-\n making processes will still be \nlimited to our setting. More research in various medical \n(education) contexts is needed to better understand the \nway users perceive and interact with AI tools.\n24 25 31 67 68 Last, \nwe acknowledge that integrating AI into medical diagnos-\ntics is not just a technological upgrade but also introduces \ncomplex ethical dilemmas and practical implementation \nchallenges that require thorough exploration.\n19 69  In \nour study, we point participants to the limitations and \npotential biases of ChatGPT (and human coaches), but \nany considerations to integrate ChatGPT into medical \neducation need to be accompanied by additional ethical \nconsiderations and dedicated training programmes as \npart of the medical curriculum.\nAuthor affiliations\n1Department of Emergency Medicine, Inselspital University Hospital Bern, University \nof Bern, Bern, Switzerland\n2Institute for Medical Informatics (I4MI), Bern University of Applied Sciences, Bern, \nSwitzerland\n3Department of Anesthesiology and Operative Intensive Care Medicine CCM & CVK, \nCharité Universitätsmedizin Berlin, Berlin, Germany\n4Lernzentrum (Skills Lab), Charité Universitätsmedizin Berlin, Berlin, Germany\n5Department of Management, Technology, and Economics, ETH Zurich, Zurich, \nSwitzerland\nX Juliane E Kämmer @julianekaemmer\nAcknowledgements\n The authors would like to thank \nAnita Todd for language \nediting as well as Tobias Bolte, Phillip Cyrenius, Robin Runge, Leonie Dahms and \nAnna Wittenstein for their support with data collection.\nContributors\n JEK,\n WEH, GK, TCS, DP , TB and NB designed the study. GK \nprogrammed the experiment. JEK and NB will analyse the data. JEK wrote the \nmanuscript. All authors revised the manuscript critically. All authors gave their \napproval of the final version of the manuscript and agreed to be accountable for \nall aspects of the published work. JEK is the guarantor. ChatGPT3.5 was used for \nlanguage editing for selected parts of the manuscript to improve language, and \ninput was cross-\n checked by a na\ntive speaker.\nFunding\n This work is supported by funding from the Swiss Na\ntional Science \nFoundation NRP77 Digital Transformation Programme (Grant no. 187331).\nDisclaimer\n The funding organisa\ntion had no role in the design of the study, \ncollection, analysis, and interpretation of data, or in writing the manuscript.\nCompeting interests\n None dec\nlared.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n9\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access\nPatient and public involvement statement P atients and/or the public were \nnot involved in the design, or conduct, or reporting, or dissemination plans of this \nresearch.\nPatient consent for publication\n Not a\npplicable.\nProvenance and peer review\n Not commissioned; externally peer reviewed.\nSupplemental material\n This content has been supplied by the author(s).\n It has \nnot been vetted by BMJ Publishing Group Limited (BMJ) and may not have been \npeer-\n reviewed.\n Any opinions or recommendations discussed are solely those \nof the author(s) and are not endorsed by BMJ. BMJ disclaims all liability and \nresponsibility arising from any reliance placed on the content. Where the content \nincludes any translated material, BMJ does not warrant the accuracy and reliability \nof the translations (including but not limited to local regulations, clinical guidelines, \nterminology, drug names and drug dosages), and is not responsible for any error \nand/or omissions arising from translation and adaptation or otherwise.\nOpen access\n This is an open access artic\nle distributed in accordance with the \nCreative Commons Attribution Non Commercial (CC BY-\n NC 4.0) license,\n which \npermits others to distribute, remix, adapt, build upon this work non-\n commercially\n, \nand license their derivative works on different terms, provided the original work is \nproperly cited, appropriate credit is given, any changes made indicated, and the use \nis non-\n commercial.\n See: http://creativecommons.org/licenses/by-nc/4.0/.\nORCID iDs\nJuliane E Kämmer http://orcid.org/0000-0001-6042-8453\nWolf E Hautz http://orcid.org/0000-0002-2445-984X\nGert Krummrey http://orcid.org/0000-0002-8397-2336\nThomas C Sauter http://orcid.org/0000-0002-6646-5789\nDorothea Penders http://orcid.org/0000-0003-1795-3791\nTanja Birrenbach http://orcid.org/0000-0002-3046-0900\nNadine Bienefeld http://orcid.org/0000-0003-4200-8695\nREFERENCES\n 1  Ber ner ES, Graber ML. Overconfidence as a cause of diagnostic \nerror in medicine. Am J Med 2008;121:S2–23. \n 2\n Newman-\n T\noker DE, Peterson SM, Badihian S, et al. Diagnostic errors \nin the emergency department: a systematic review. In: Agency for \nhealthcare research and quality (AHRQ). 2022. Available: https://\n \nef\nfectivehealthcare.ahrq.gov/products/diagnostic-errors-emergency/\n \nr\nesearch\n 3\n Singh H, Meyer AND, Thomas EJ. The fr\nequency of diagnostic errors \nin outpatient care: estimations from three large observational studies \ninvolving US adult populations. BMJ Qual Saf 2014;23:727–31. \n 4\n Miller BT\n, BaloghEP , eds. Committee on diagnostic error in health \ncare, board on health care services, Institute of medicine, the \nNational academies of sciences, engineering, and medicine. In: \nImproving diagnosis in health care. Washington, D.C: National \nAcademies Press, 2015. Available: http://www.nap.edu/catalog/\n \n21794 [accessed 15 Nov 2019].\n 5\n Hautz WE, Kämmer JE, Hautz SC, \net al. Diagnostic error increases \nmortality and length of hospital stay in patients presenting through \nthe emergency room. Scand J Trauma Resusc Emerg Med \n2019;27:54. \n 6\n Zwaan L, de Bruijne M, W\nagner C, et al. Patient record review of the \nincidence, consequences, and causes of diagnostic adverse events. \nArch Intern Med 2010;170:1015–21. \n 7\n W\nallace E, Lowry J, Smith SM, et al. The epidemiology of \nmalpractice claims in primary care: a systematic review. BMJ Open \n2013;3:e002929. \n 8\n Riches N, Panagioti M, Alam R, \net al. The effectiveness of electronic \ndifferential diagnoses (DDX) generators: a systematic review and \nmeta-\n \nanalysis. PLOS ONE 2016;11:e0148991. \n 9\n Gilson A, Safranek CW\n, Huang T, et al. How does ChatGpt \nperform on the United States medical licensing examination? The \nimplications of large language models for medical education and \nknowledge assessment. JMIR Med Educ 2023;9:e45312. \n 10\n Kung TH, Cheatham M, Medenilla A, \net al. Performance of ChatGpt \non USMLE: potential for AI-\n assisted medical education using lar\nge \nlanguage models. PLOS Dig Health 2023;2:e0000198. \n \n11\n Ber\nşe S, Akça K, Dirgar E, et al. The role and potential contributions \nof the artificial intelligence language model ChatGpt. Ann Biomed \nEng 2024;52:130–3. \n 12\n Goh E, Gallo R, Hom J, \net al. Influence of a large language model \non diagnostic reasoning: a randomized clinical vignette study health \nInformatics. medRxiv [Preprint] 2024. \n 13\n Lee P\n, Bubeck S, Petro J. Limits, and risks of GPT-\n 4 as an AI chatbot \nfor medicine. \nN Engl J Med 2023;388:1233–9. \n 14\n Sallam M. ChatGPT utility in healthcar\ne education, research, and \npractice: systematic review on the promising perspectives and valid \nconcerns. Healthc (Basel) 2023.:887. \n 15\n W\nachter RM, Brynjolfsson E. Will generative artificial intelligence \ndeliver on its promise in health care? JAMA 2024;331:65. \n 16\n Alessandri Bonetti M, Gior\ngino R, Gallo Afflitto G, et al. How does \nChatGpt perform on the Italian residency admission national \nexam compared to 15,869 medical graduates? Ann Biomed Eng \n2023;52:745–9. \n 17\n Scaioli G, Lo Mor\no G, Conrado F , et al. Exploring the potential \nof ChatGpt for clinical reasoning and decision-\n making: a cr\noss-\n \nsectional study on the Italian medical r\nesidency exam. Ann Ist Super \nSanita 2023;59:267–70. \n 18\n Fer\ndush J, Begum M, Hossain ST. ChatGpt and clinical decision \nsupport: scope, application, and limitations. Ann Biomed Eng \n2023;52:1119–24. \n 19\n Bienefeld N, Boss JM, Lüthy R, \net al. Solving the explainable AI \nconundrum by bridging clinicians’ needs and developers’ goals. NPJ \nDigit Med 2023;6:94. \n 20\n Bienefeld N, Kolbe M, Camen G, \net al. Human-\n AI teaming: \nleveraging transactive memory and speaking up for enhanced team \nef\nfectiveness. Front Psychol 2023;14:1208019. \n 21\n Kerstan S, Bienefeld N, Gr\note G. Choosing human over AI doctors? \nHow comparative trust associations and knowledge relate to risk and \nbenefit perceptions of AI in healthcare. Risk Anal 2024;44:939–57. \n 22\n Mar\ncin T, Hautz SC, Singh H, et al. Effects of a computerised \ndiagnostic decision support tool on diagnostic quality in \nemergency departments: study protocol of the DDx-\n BRO\n \nmulticentre cluster randomised cross-\n over trial. \nBMJ Open \n2023;13:e072649. \n 23\n Grunhut J, Mar\nques O, Wyatt ATM. Needs, challenges, and \napplications of artificial intelligence in medical education curriculum. \nJMIR Med Educ 2022;8:e35587. \n 24\n Zhang S, Y\nu J, Xu X. Rethinking human-\n AI collaboration in complex \nmedical decision making: a case study in sepsis diagnosis. \nIn: Pr\noceedings of the CHI Conference on Human Factors in \nComputing Systems, CHI 24; Honolulu HI USA, May 11, 2024:1–18. \n10.1145/3613904.3642343 Available: https://dl.acm.org/doi/\n  \npr oceedings/10.1145/3613904\n 25\n Blease C, W\northen A, Torous J. Psychiatrists’ experiences and \nopinions of generative artificial intelligence in mental healthcare: an \nonline mixed methods survey. Psychiatry Res 2024;333:115724. \n 26\n Hir\nosawa T, Harada Y , Yokose M, et al. Diagnostic accuracy of \ndifferential-\n diagnosis lists generated by generative pr\netrained \ntransformer 3 chatbot for clinical vignettes with common \nchief complaints: a pilot study. Int J Environ Res Public Health \n2023;20:3378. \n 27\n Norman GR, Monteir\no SD, Sherbino J, et al. The causes of errors \nin clinical reasoning: cognitive biases, knowledge deficits, and dual \nprocess thinking. Acad Med 2017;92:23–30. \n 28\n Lu Z, W\nang D, Yin M. Does more advice help? The effects of second \nopinions in AI-\n assisted decision making. \nProc ACM Hum-\n Comput \nInteract\n 2024;8:1–31. \n 29\n Kämmer JE, Choshen-\n Hillel S, Müller\n-\n T\nrede J, et al. A systematic \nreview of empirical studies on advice-\n based decisions in behavioral \nand or\nganizational research. Decision2023;10:107–37. \n 30\n T\nen Cate O, Durning S. Dimensions and psychology of peer teaching \nin medical education. Med Teach 2007;29:546–52. \n 31\n Mollick ER, Mollick L. Assigning AI: seven appr\noaches for students, \nwith prompts. SSRN J 2023. \n 32\n Hautz WE, Hautz SC, Kämmer JE. Whether two heads ar\ne better \nthan one is the wrong question (though sometimes they are). Adv \nHealth Sci Educ Theory Pract 2020;25:905–11. \n 33\n Graber ML, Franklin N, Gor\ndon R. Diagnostic error in internal \nmedicine. Arch Intern Med 2005;165:1493–9. \n 34\n Newman-\n T\noker DE, Schaffer AC, Yu-\n Moe CW\n, et al. Serious \nmisdiagnosis-\n r\nelated harms in malpractice claims: the 'big \nthree'–vascular events, infections, and cancers. Diagnosis (Berl) \n2019;6:227–40. \n 35\n Norman GR. Resear\nch in clinical reasoning: past history and current \ntrends. Med Educ 2005;39:418–27. \n 36\n Pelaccia T\n, Tardif J, Triby E, et al. How and when do expert \nemergency physicians generate and evaluate diagnostic \nhypotheses? A qualitative study using head-\n mounted video cued-\n \nr\necall interviews. Ann Emerg Med 2014;64:575–85. \n 37\n Evans KK, Haygood TM, Cooper J, \net al. A half-\n second glimpse often \nlets radiologists identify br\neast cancer cases even when viewing \nthe mammogram of the opposite breast. Proc Natl Acad Sci U S A \n2016;113:10292–7. \nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as \n10\nKämmer JE, et al. BMJ Open 2024;14:e087469. doi:10.1136/bmjopen-2024-087469\nOpen access  \n 38  Kostopoulou O, Mousoulis C, Delaney B. Information sear ch and \ninformation distortion in the diagnosis of an ambiguous presentation. \nJudgm decis mak 2009;4:408–19. \n 39\n Kostopoulou O, Russo JE, Keenan G, \net al. Information distortion in \nphysicians’ diagnostic judgments. Med Decis Making 2012;32:831–9. \n 40\n Kourtidis P\n, Nurek M, Delaney B, et al. Influences of early diagnostic \nsuggestions on clinical reasoning. Cogn Res Princ Implic 2022;7:103. \n 41\n Barr\nows HS, Norman GR, Neufeld VR, et al. The clinical reasoning \nprocess of randomly selected physicians in general medical practice. \nClin Invest Med 1982;5:49–55.\n 42\n Hobu PPM, Schmidt HG, Boshuizen HP\nA, et al. Contextual factors \nin the activation of first diagnostic hypotheses: expert-\n novice \ndif\nferences. Med Educ 1987;21:471–6. \n 43\n Kumar B, Kanna B, Kumar S. The pitfalls of pr\nemature closure: \nclinical decision-\n making in a case of aortic dissection. \nBMJ Case \nRep 2011;2011:bcr0820114594. \n 44\n Kruglanski A\nW, Webster DM. Motivated closing of the mind: ‘seizing’ \nand 'freezing'. Psychol Rev 1996;103:263–83. \n 45\n Eva KW\n, Cunnington JPW. The difficulty with experience: does \npractice increase susceptibility to premature closure. J Contin Educ \nHealth Prof 2006;26:192–8. \n 46\n Norman G. The bias in r\nesearching cognitive bias. Adv in Health Sci \nEduc 2014;19:291–5. \n 47\n Saposnik G, Redelmeier D, Ruf\nf CC, et al. Cognitive biases \nassociated with medical decisions: a systematic review. BMC Med \nInform Decis Mak 2016;16:138. \n 48\n Ely JW\n, Kaldjian LC, D’Alessandro DM. Diagnostic errors in primary \ncare: lessons learned. J Am Board Fam Med 2012;25:87–97. \n 49\n Singh H, Giar\ndina TD, Meyer AND, et al. Types and origins of \ndiagnostic errors in primary care settings. JAMA Intern Med \n2013;173:418–25. \n 50\n Kostopoulou O, Lionis C, Angelaki A, \net al. Early diagnostic \nsuggestions improve accuracy of family physicians: a randomized \ncontrolled trial in Greece. Fam Pract 2015;32:323–8. \n 51\n Kostopoulou O, Dever\neaux-\n W\nalsh C, Delaney BC. Missing celiac \ndisease in family medicine: the importance of hypothesis generation. \nMed Decis Making 2009;29:282–90. \n 52\n Meskó B. Pr\nompt engineering as an important emerging skill for \nmedical professionals: tutorial. J Med Internet Res 2023;25:e50638. \n 53\n Nori H, Lee YT\n, Zhang S, et al. Can generalist foundation models \noutcompete special-\n purpose tuning? Case study in medicine. 2023. \nA\nvailable: http://arxiv.org/abs/2311.16452\n 54\n Bates D, Mächler M, Bolker B, \net al. Fitting linear mixed-\n ef\nfects \nmodels using Lme4. J Stat Softw 2014. \n 55\n R Cor\ne Team. R: A language and environment for statistical \ncomputing. R Foundation for Statistical Computing; 2018. Available: \nhttps://www.R-project.org/\n 56  Faul F , Erdfelder E, Lang A-  G, et al. G*Power 3: a flexible statistical \npower analysis program for the social, behavioral, and biomedical \nsciences. Behav Res Methods 2007;39:175–91. \n 57\n Cai CJ, Winter S, Steiner D, \net al. Hello AI: uncovering the \nonboarding needs of medical practitioners for human-\n AI\n \ncollaborative decision-\n \nmaking. Proc ACM Hum-\n Comput Interact\n \n2019;3:1–24. \n 58\n Schrah GE, Dalal RS, Sniezek JA. No decision-\n maker is an island: \nintegrating expert advice with information acquisition. \nJ Behav Decis \nMaking 2006;19:43–60. \n 59\n Gäbler M. Denkfehler BEI diagnostischen entscheidungen. \nWien Med \nWochenschr 2017;167:333–42. \n 60\n Kunina-\n Habenicht O, Hautz WE, Knigge M, \net al. Assessing clinical \nreasoning (ASCLIRE): instrument development and validation. Adv \nHealth Sci Educ 2015;20:1205–24. \n 61\n Mamede S, Schmidt HG, Penaforte JC. Ef\nfects of reflective practice \non the accuracy of medical diagnoses. Med Educ 2008;42:468–75. \n 62\n Mamede S, Schmidt HG. Deliberate r\neflection and clinical \nreasoning: founding ideas and empirical findings. Med Educ \n2023;57:76–85. \n 63\n Nagendran M, Festor P\n, Komorowski M, et al. Quantifying the impact \nof AI recommendations with explanations on prescription decision \nmaking. NPJ Digit Med 2023;6:206. \n 64\n Jo H, Bang Y\n. Analyzing ChatGpt adoption drivers with the TOEK \nframework. Sci Rep 2023;13:22606. \n 65\n Lewis K. Measuring transactive memory systems in the field: scale \ndevelopment and validation. \nJ Appl Psychol 2003;88:587–604. \n 66\n Lee J, Moray N. T\nrust, control strategies and allocation of function in \nhuman-\n \nmachine systems. Ergonomics 1992;35:1243–70. \n 67\n W\nang D, Churchill E, Maes P , et al. From human-\n human collaboration \nto human-\n AI collaboration: designing AI systems that can work \ntogether with people. Extended Abstracts of the 2020 CHI \nConfer\nence on Human Factors in Computing Systems; August 22, \n2020:1–6. 10.1145/3334480.3381069 Available: https://dl.acm.org/\n \ndoi/10.1145/3334480.3381069\n 68\n T\nangadulrat P , Sono S, Tangtrakulwanich B. Using ChatGpt for \nclinical practice and medical education: cross-\n sectional survey of \nmedical students’ and physicians’ per\nceptions. JMIR Med Educ \n2023;9:e50658. \n 69\n Bienefeld N, Keller E, Gr\note G. Human-\n AI teaming in the ICU: a \ncomparative analysis of data scientists’ and clinicians’ assessments \non AI augmentation and automation at work. \nJ Med Internet Res \n[Preprint]. \n 70\n Hautz WE, Kündig MM, T\nschanz R, et al. Automated identification of \ndiagnostic labelling errors in medicine. Diagnosis (Berl) 2021;9:241–9. \n 71\n Osterber\ng K, Kölbel S, Brauns K. The progress test medizin. GMS J \nMed Educ 2006;23:Doc46.\nProtected by copyright, including for uses related to text and data mining, AI training, and similar technologies.\n . by guest on November 5, 2025 http://bmjopen.bmj.com/Downloaded from 18 July 2024. 10.1136/bmjopen-2024-087469 on BMJ Open: first published as ",
  "topic": "Medicine",
  "concepts": [
    {
      "name": "Medicine",
      "score": 0.948503851890564
    },
    {
      "name": "Protocol (science)",
      "score": 0.7192761898040771
    },
    {
      "name": "Physical therapy",
      "score": 0.45767661929130554
    },
    {
      "name": "Prospective cohort study",
      "score": 0.4391429126262665
    },
    {
      "name": "Medical physics",
      "score": 0.42371881008148193
    },
    {
      "name": "Alternative medicine",
      "score": 0.30284515023231506
    },
    {
      "name": "Pathology",
      "score": 0.20628038048744202
    }
  ]
}