{
  "title": "SpecTNT: a Time-Frequency Transformer for Music Audio",
  "url": "https://openalex.org/W3207444892",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286969995",
      "name": "Lu, Wei-Tsung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4283672818",
      "name": "Wang, Ju-Chiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3212099473",
      "name": "Won, Minz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225437360",
      "name": "Choi, Keunwoo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2359761821",
      "name": "Song, Xuchen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2932319787",
    "https://openalex.org/W2296557957",
    "https://openalex.org/W2059652044",
    "https://openalex.org/W1982446897",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W2395935897",
    "https://openalex.org/W2124539664",
    "https://openalex.org/W2407685581",
    "https://openalex.org/W1556219185",
    "https://openalex.org/W1583001605",
    "https://openalex.org/W3139211892",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2001426554",
    "https://openalex.org/W2952395326",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2990093647",
    "https://openalex.org/W3191088441",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2963369619",
    "https://openalex.org/W2138365710",
    "https://openalex.org/W3015289235",
    "https://openalex.org/W2773294482",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3162583214",
    "https://openalex.org/W2577202280",
    "https://openalex.org/W2905804369",
    "https://openalex.org/W2296724634",
    "https://openalex.org/W2902480954",
    "https://openalex.org/W3029858316",
    "https://openalex.org/W2991377373",
    "https://openalex.org/W2963451564",
    "https://openalex.org/W2571951934",
    "https://openalex.org/W1979955134",
    "https://openalex.org/W2494980014",
    "https://openalex.org/W2963535133",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2986673441",
    "https://openalex.org/W2989581421"
  ],
  "abstract": "Transformers have drawn attention in the MIR field for their remarkable performance shown in natural language processing and computer vision. However, prior works in the audio processing domain mostly use Transformer as a temporal feature aggregator that acts similar to RNNs. In this paper, we propose SpecTNT, a Transformer-based architecture to model both spectral and temporal sequences of an input time-frequency representation. Specifically, we introduce a novel variant of the Transformer-in-Transformer (TNT) architecture. In each SpecTNT block, a spectral Transformer extracts frequency-related features into the frequency class token (FCT) for each frame. Later, the FCTs are linearly projected and added to the temporal embeddings (TEs), which aggregate useful information from the FCTs. Then, a temporal Transformer processes the TEs to exchange information across the time axis. By stacking the SpecTNT blocks, we build the SpecTNT model to learn the representation for music signals. In experiments, SpecTNT demonstrates state-of-the-art performance in music tagging and vocal melody extraction, and shows competitive performance for chord recognition. The effectiveness of SpecTNT and other design choices are further examined through ablation studies.",
  "full_text": "SpecTNT: A TIME-FREQUENCY TRANSFORMER FOR MUSIC AUDIO\nWei-Tsung Lu1 Ju-Chiang Wang1 Minz Won1,2 Keunwoo Choi1 Xuchen Song1\n1 ByteDance, Mountain View, California, United States\n2 Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain\n{weitsung.lu, ju-chiang.wang, minzwon, keunwoo.choi, xuchen.song}@bytedance.com\nABSTRACT\nTransformers have drawn attention in the MIR ﬁeld\nfor their remarkable performance shown in natural lan-\nguage processing and computer vision. However, prior\nworks in the audio processing domain mostly use Trans-\nformer as a temporal feature aggregator that acts similar to\nRNNs. In this paper, we propose SpecTNT, a Transformer-\nbased architecture to model both spectral and temporal se-\nquences of an input time-frequency representation. Specif-\nically, we introduce a novel variant of the Transformer-in-\nTransformer (TNT) architecture. In each SpecTNT block,\na spectral Transformer extracts frequency-related features\ninto the frequency class token (FCT) for each frame. Later,\nthe FCTs are linearly projected and added to the tempo-\nral embeddings (TEs), which aggregate useful information\nfrom the FCTs. Then, a temporal Transformer processes\nthe TEs to exchange information across the time axis.\nBy stacking the SpecTNT blocks, we build the SpecTNT\nmodel to learn the representation for music signals. In ex-\nperiments, SpecTNT demonstrates state-of-the-art perfor-\nmance in music tagging and vocal melody extraction, and\nshows competitive performance for chord recognition. The\neffectiveness of SpecTNT and other design choices are fur-\nther examined through ablation studies.\n1. INTRODUCTION\nDeep learning models have been actively used in recent\nmusic information retrieval (MIR) research. Although the\nspirit of deep learning is end-to-end learning, however, var-\nious assumptions are made during making design choices\nof deep learning models.\nRegarding assumptions on spectrograms, the most pop-\nular form of music audio representation in deep learning,\nthe time-axis is often considered to be the axis ofsequence\nwhile the frequency-axis is the axis of feature. For ex-\nample, in [1, 2], recurrent layers were applied to model a\nspectrogram as a sequence of spectra. In [3], convolutional\nlayers were used to aggregate features over time after the\n© W.-T. Lu, J.-C. Wang, M. Won, K. Choi, and X. Song. Li-\ncensed under a Creative Commons Attribution 4.0 International License\n(CC BY 4.0). Attribution: W.-T. Lu, J.-C. Wang, M. Won, K. Choi, and\nX. Song, “SpecTNT: a Time-Frequency Transformer for Music Audio”,\nin Proc. of the 22nd Int. Society for Music Information Retrieval Conf.,\nOnline, 2021.\nﬁrst convolutional layer models multiple frames of spec-\ntra as feature. On the other hand, in [4], two-dimensional\nconvolutional layers were used, equating the frequency-\nand time-axes. There are also hybrid approaches, such as\nconvolutional recurrent neural networks (CRNN) [5] and\nconvolutional Transformer [6], in which recurrent layers\nor Transformer are applied along the time axis.\nIn spectrograms, it is well known that there are mean-\ningful spectral patterns. Different music components exist\nin different frequency ranges, and there is a very strong\nspectral correlation called harmonics. Since a normal\nconvolutional layer can model local patterns only, sev-\neral approaches have been proposed to model harmonics\nalong the frequency axis. Harmonic Constant-Q Transform\n(HCQT) is a novel multi-channel time-frequency represen-\ntation that was proposed to overcome the limitation by im-\nproving the input representation of audio [7]. Harmonic\nCNNs (convolutional neural networks) [8] are designed\nto model the harmonic pattern by modifying the convolu-\ntional ﬁlters. However, these solutions only model some of\nthe spectral patterns, reminding the need for a more gen-\neral solution with higher ﬂexibility.\nTransformers have successfully demonstrated their abil-\nity to model the sequential data with long-term (inter-) de-\npendency and invariance. This is achieved by multiple as-\npects of Transformers. First, the key-query mechanism\nenables modeling the relationship of every combination\nof the instances. Second, positional encoding helps the\nmodel to take the order of instances into account. Through\nstacked attention layers, the input sequence is transformed\ninto a sequence of representations that are based on the\ninter-dependency of the input. The prior works in audio\nanalysis are mostly based on a similar, naive approach\nwhere Transformer is used as a temporal feature aggrega-\ntor that acts similar to RNNs (recurrent neural networks),\nwith few exceptions such as [9].\nRecently, Transformer in Transformer (TNT), a variant\nof Transformer that arranges two Transformers in a hi-\nerarchical manner, was proposed [10] for image recogni-\ntion. In TNT, an inner (lower-level) Transformer is applied\nto extract the local pixel-level embeddings, and then the\npixel-level embeddings are projected to the patch-level em-\nbedding space which is later handled by an outer (higher-\nlevel) Transformer to summarize a global representation.\nOne can simply apply TNT for audio by treating a song\nas an image, which is comprised of a sequence of frames\n(patches) while the frequency bins within frames are con-\narXiv:2110.09127v1  [cs.SD]  18 Oct 2021\nsidered as pixels. However, from our pilot study, we ﬁnd\nthis approach results in unstable training and only achieves\nsimilar performance compared to using the original Trans-\nformer. This is possibly because the amount of training\ndata is insufﬁcient or the interpretation of frequency se-\nquences is different from that of pixel sequences. There-\nfore, non-trivial modiﬁcations from the original idea of\nTNT should be made.\nIn this paper, we propose SpecTNT, a time-frequency\ntransformer that models spectrograms as a sequence along\nboth time- and frequency-axes. Similar to TNT, SpecTNT\nuses two Transformers hierarchically. However, the tem-\nporal local embeddings extracted from the inner Trans-\nformer are not directly sent to the outer Transformer. In-\nstead, a special token called frequency class token(FCT)\nis appended to aggregate the important spectral features\nof each frame. The FCT is then projected to the global\n(temporal) embedding space to enable the information ex-\nchange across the time axis. This design allows the impor-\ntant local information is passed to the outer Transformer\nthrough FCT while reducing the dimensionality of the data\nﬂow compared to the original TNT. As a result, it helps\nSpecTNTs to perform well on audio-related tasks even\nwith smaller datasets.\nOur contributions can be summarized as follows: (1)\nto the best of our knowledge, our work is the ﬁrst attempt\nto leverage TNT-based architecture to learn the representa-\ntions for audio; (2) we propose SpecTNT, a novel modiﬁ-\ncation of TNT to better ﬁt the music data for MIR tasks; (3)\nwe conduct extensive experiments to demonstrate the ca-\npability of SpecTNT in various MIR tasks – vocal melody\nextraction, music auto-tagging, and chord recognition.\n2. RELATED WORK\nIn this section, we review the literature in music tagging,\nvocal melody extraction, and chord recognition – three\nwell-deﬁned MIR tasks adopted in the experiments to eval-\nuate SpecTNT. Due to space limitation, we focus on the re-\ncent trends since the adoption of deep learning approaches.\nMusic tagging is a multi-label classiﬁcation task that\nannotates a music audio clip with various types of la-\nbels such as genres (rock, jazz), instruments (vocal, guitar,\ndrums), and mood (happy, sad) [11]. Since a CNN-based\napproach has been ﬁrst introduced [3], various advanced\narchitectures have been used including a two-dimensional\nCNN [4], a sample-level CNN [12], and a two-dimensional\nResNet [13]. Due to the open nature of the tag set, among\nMIR tasks, music tagging is relatively a vague task – The\nexact mechanism of annotating tags is not fully known.\nThis aspect suits well for the fundamental motivation of\ndeep learning, which is, to reduce inductive bias and let\nthe data speak [14].\nThe goal of vocal melody extraction is to estimate the\nF0 frequency of the (dominant) vocal track in given mix-\ntures. Various deep learning methods have been adopted:\na fully-connected neural network with Hidden Markov\nModel [15], a bidirectional long short-term memory net-\nwork [1], a CNN [7], encoder-decoder networks [16, 17]\nFigure 1. The block diagram of the whole SpecTNT. The\ndetails of positional encoding and SpecTNT module are\nillustrated in Figure 2 and 3, respectively.\nand a CRNN [18]. Recently, a frequency-temporal atten-\ntion module was introduced in [19] to learn the relevant\nregions for predictions. Some special representations are\nproposed including HCQT [7], a combination of frequency\nand periodicity [20], and source-separated tracks [21, 22].\nChord recognition is a MIR task to “produce a time-\nvarying symbolic representation of the signal in terms of\nchord labels” [23]. Compared to music tagging, we clearly\nunderstand how chords of music signals can be decided –\nThey are based on the combination of the present musical\nnotes. Therefore, models have been designed to take ad-\nvantage of note representations such as constant-Q trans-\nform (CQT) or chromagram. The early deep learning-\nbased chord recognition models are based on a RNN [24]\nand a CNN [25]. Later, a CRNN has been used in [23] to\ncombine the merits of RNNs and CNNs. More recently,\n(bi-directional) Transformer was used, achieving state-of-\nthe-art performance [26, 27].\n3. METHODS\nAs illustrated in Figure 1, the proposed SpecTNT architec-\nture consists of a convolutional module, positional encod-\ning, SpecTNT module, and output module.\nThe input time-frequency representation is ﬁrst pro-\ncessed with a stack of convolutional layers for local feature\naggregation. Then, the positional information is added to\nthe data. In the SpecTNT module, the intermediate rep-\nresentation is fed into a stack of SpecTNT blocks. Lastly,\nthe output module projects the ﬁnal embedding into the de-\nsired dimension for different tasks. We detail each module\nin the following subsections.\n3.1 Convolutional module\nThe purpose of this convolutional module is to employ\ndifferent strategies for generating intermediate representa-\ntions with pooling or striding convolution techniques de-\npending on the nature of the task. Let the input time-\nfrequency representation be S ∈ RT×F×K where T is\nthe number of time-steps, F is the number of frequency\nbins, and K is the number of channels. S is ﬁrst passed\ninto a stack of convolutional layers. We utilize the residual\nunit proposed in [28] to be the basic building block of the\nconvolutional module. The representation after the con-\nvolutional module is denoted as S′ = [S′\n1,S′\n2,...,S ′\nˆT] ∈\nRˆT×ˆF×ˆK, where ˆF, ˆT, and ˆK are the numbers of fre-\nquency bins, time-steps, and channels, respectively.\nFigure 2. An illustration of the application of Fre-\nquency class token (FCT) and frequency positional encod-\ning (FPE). F refers to the number of frequency bins of the\ninput time-frequency representation.\n3.2 Frequency Class Token\nAs depicted in Figure 2,Frequency class token(FCT) is an\nembedding vector initialized with all zeros to serve as the\nplaceholder and deﬁned as ct = 01×ˆK. Let S′\nt ∈RˆF×ˆK\ndenote the input data at each time-step t. The input data\nand FCT are concatenated as following:\nS′′\nt = Concat[ct,S′\nt]. (1)\nHere, the role of FCT ct is similar to the classiﬁcation to-\nken [29]. It is expected to extract spectral features from\neach frequency bin of the t-th frame during the spectral\nself-attention in the later stages.\n3.3 Positional encoding\nIn the original Transformer paper, a sinusoidal positional\nencoding was added to the input sequence to make the fol-\nlowing layers aware of the order of input elements [30].\nFrom a similar motivation, we adopt a learnable positional\nembedding to encode the sequence order of frequency bins.\nWe encode the positional information of frequencies by\nadding the frequency positional embedding (FPE) to the\ndata S′′. FPE is a learnable matrix Eφ ∈R( ˆF+1)×ˆK. The\naddition process is done at each time-step t:\nˆSt = S′′\nt ⊕Eφ, (2)\nwhere ⊕is the element-wise addition, and the resulting\nFCTs are denoted by ˆC = [ˆc1,ˆc2,..., ˆcˆT]. Then, the re-\nsulting representation ˆSt is able to carry information about\npitch and timbre to the following attention layers. For ex-\nample, a pitch in the signal can lead to high energy at a spe-\nciﬁc frequency bin, and the positional embedding makes\nFCT aware of the position of that frequency\n3.4 Transformer in Transformer (TNT)\nInspired by the architecture in [10], we design a SpecTNT\nblock to handle audio data, as depicted in Figure 3. The\nSpecTNT block holds two data ﬂows: spectral embedding\n(SE) and temporal embedding (TE). The two data ﬂows\nare respectively processed with two Transformer encoders,\nFigure 3. The block diagram of a SpecTNT block. Tensors\nand modules are illustrated with non-rounded and rounded\nrectangles, respectively. We specify the non-batch shape\nof tensors for clarity, and explain (a) – (d) in the main text.\nnamely temporal Transformer and spectral Transformer.\nBecause the SpecTNT block is repeated multiple times in\nthe SpecTNT module, we introduce a notation lto specify\nthe layer index for both SE and TE.\nIn the following sections, we explain each component\nof a SpecTNT block (Section 3.4.1 through Section 3.4.3)\nand the entire procedure (Section 3.4.4).\n3.4.1 Temporal Embedding\nIn the proposed model, we introduce the temporal embed-\nding (TE) to distribute the information of FCTs across the\ntime axis. We can write the TE at layer las:\nEl = [el\n1,el\n2,...,e l\nˆT], (3)\nwhere el\nt ∈R1×D is a TE vector at time t and D is the\nnumber of features. In practice, TE is a learnable matrix\nand is initialized randomly as E0 ∈RˆT×D prior to enter-\ning the ﬁrst SpecTNT block.\nThere are two bridges between the spectral and tempo-\nral data ﬂows. We use FCTs, the ﬁrst frequency bin of\nSEs, for this communication. First, TE sends information\nto FCTs by passing el\nt to a linear projection layer. Then,\nthe projected D-dimensional vectors are added to FCTs\n(Figure 3-(a)). Second, after spectral transformer encoder\n(Figure 3-(c)), FCTs (purple arrays) are projected back to\nK-dimension (Figure 3-(d)). Note that TE also has a skip-\nconnection (Figure 3-(b)).\n3.4.2 Spectral Embedding\nThe output from the positional encoding, ˆS, will serve as\nan input SE for the ﬁrst SpecTNT block and is denoted as\nˆS0. As mentioned above, SE includes FCTs, which help\naggregate useful spectral information from the local. As a\ngeneral notation, we write the data ﬂow of SE as:\nˆSl =\n[\n[ˆcl\n1, ˆSl\n1],[ˆcl\n2, ˆSl\n2],..., [ˆcl\nˆT, ˆSl\nˆT]\n]\n, (4)\nwhere l = 0,1,...,L , and cl\nt and ˆSl\nt are respectively the\nFCTs of l-th layer and spectral data at time-step t. Then,\nSE can interact with the TE through FCTs, so the local\nspectral features can be processed in a temporal and global\nmanner.\n3.4.3 Transformer Encoder\nA Transformer encoder is composed of three components:\nmulti-head self-attention (MHSA), feed-forward network\n(FFN), and layer normalization (LN).\nSelf-attention (SA) [30] plays the pivotal role in a\nTransformer encoder. It takes three inputs: Q ∈RT×dq ,\nK ∈RT×dk and V ∈RT×dv which represent the queries,\nkeys, and values, respectively. T is the number of time-\nsteps, and dq, dk and dv indicate the dimension of features\nfor Q, K, and V, respectively. The output is the weighted\nsum over the values based on the dot product similarity\nbetween queries and keys at the corresponding time-step.\nThe MHSA module [30] is an extension of SA. It splits\nthe three inputs Q, Kand V along their feature dimension\ninto hnumber of “heads” and performs multiple SA’s, each\non a head, in parallel. The outputs of heads are then con-\ncatenated and linearly projected into the ﬁnal output. The\nFFN module has two linear layers with a GELU activa-\ntion function in the middle. We also adopt the pre-norm\nresidual units [31] to stabilize the training.\nWith the three components, the Transformer encoder\n(either spectral or temporal) is built and denoted by\nXl = Enc(Xl−1), (5)\nwhere the operations within it can be written as\nX′\nl−1 = Xl−1 + MHSA(LN(Xl−1)),\nXl = X′\nl−1 + FFN(LN(X′\nl−1)). (6)\n3.4.4 Stacking SpecTNT Blocks\nWe stack three SpecTNT blocks for the SpecTNT module.\nThe module starts with inputting the initial SE, ˆS0, and the\ninitial TE, E0, to the ﬁrst SpecTNT block.\nFor a SpecTNT block, there are four steps. First, each\nFCT vector in ˆSl−1 is updated by adding the linear projec-\ntion of the associated TE vector (Figure 3-(a)):\n˜cl−1\nt = ˆcl−1\nt ⊕Linear(el−1\nt ), (7)\nwhere Linear(·) is a shared linear layer. Second, the SE\n˜Sl−1 (with the updated FCTs [˜cl−1\nt ] ˆT\nt=1 at the ﬁrst row) is\npassed through the spectral Transformer (Figure 3-(c)):\nˆSl = SpecEnc(˜Sl−1). (8)\nThird, each FCT vector in ˆSl is linearly projected and\nadded back to the corresponding TE vector (Figure 3-(d)):\n˜el−1\nt = el−1\nt ⊕Linear(ˆcl\nt). (9)\nFinally, we propose to encode only the updated TE (i.e.,\n˜El−1 = [˜el−1\nt ] ˆT\nt=1), instead of TE + SE, with the temporal\nTransformer:\nEl = TempEnc(˜El−1). (10)\nThis operation builds up the relationship along the time\naxis and is the key role that leads to better model and data\nefﬁciency. We consider the temporal Transformer only\nneeds to see the information of the frequency bins which\nare attended by the FCT and such design largely reduces\nthe size of the model and also improves the performance\non smaller datasets in preliminary experiments.\nTask (pf, pt) (k, d) (hk, hd) od\nMusic tagging (1, 4) (96, 96) (4, 8) 50\nV ocal melody extraction(4, 1) (128, 128) (8, 8) (T, 481)\nChord recognition (1, 1) (64, 256) (4, 8) (T, 25)\nTable 1. Settings of SpecTNT for different tasks, wherepf\nand pt represent the pooling ratio we apply along the fre-\nquency and time axis in the convolutional module,kand d\nare the feature dimension, hk and hd denotes the number\nof heads for the spectral and temporal transformer encoder\nrespectively, and ﬁnally, od represents the output dimen-\nsion, T indicates frame-wise predictions.\n3.5 Output Module\nThe output TE of the 3rd SpecTNT block, E3, can be used\ntowards the ﬁnal output. For frame-wise prediction tasks\nsuch as vocal melody extraction and chord recognition, we\nfeed each TE vector e3\nt into a shared fully-connected layer\nwith sigmoid or softmax function for ﬁnal output. For\nsong-level prediction tasks such as music tagging, we ini-\ntialize a temporal class tokenϵl (l= 0) concatenated at the\nfront of El:\nˆEl = [ϵl,el\n1,el\n2,...,e l\nˆT], (11)\nNote that ϵl does not have an associated FCT in SE, but is\nfor aggregating TE vectors along the time axis. Finally, we\nfeed ϵ3 to a fully-connected layer, followed by a sigmoid\nlayer, to get the probability output.\n4. EXPERIMENTS\nIn this section, we evaluate SpecTNT on various types of\nMIR tasks to demonstrate its effectiveness and versatility.\nWe choose three MIR tasks – music tagging, vocal melody\nextraction, and chord recognition.\n4.1 Implementation\nSpecTNT is implemented using Pytorch [32]. Due to the\ndifference in dataset sizes and the natures of tasks, we use\ndifferent hyper-parameters for the tasks as shown in Ta-\nble 1. All models include dropout with a rate of0.15 in the\nTransformers of the TNT modules. We use AdamW [33]\nas the learning optimizer. The initial learning rates are set\nto 10−3 for vocal melody extraction, 5 ×10−4 for mu-\nsic tagging and chord recognition, and a weight decay of\n5 ×10−3 is set for all the tasks.\nFor the input representation of music tagging, we re-\nsample the audio at the 22,050 Hz and use an input\nlength of 4.54 second. Log-magnitude mel-spectrograms\nare computed with 128 mel ﬁlter banks, 1024 samples of\nHann window, and a hop size of 512 samples. For vocal\nmelody extraction, input waveforms are re-sampled at the\n16,000 Hz sample rate. We take 3-second segments input\nand their log-magnitude spectrograms are computed with\n2048 samples of Hann window and a hop size of 320 sam-\nples. For chord recognition, we try two types of input rep-\nresentation. The ﬁrst input type is 24-dimensional chroma\nfeatures with a frame rate of 46 ms [34]. Out of the whole\ntrack, we use 400 frames as an input. The second input\ntype is CQT, which is computed from a 18.2 second audio\nat the 22,050 Hz sample rate. The CQT includes six oc-\ntaves starting from C1 (32.70 Hz) with 24 bins per octave,\nand is based on a hop size of 2048.\n4.2 Ablations Study\nTo validate the design choices we make, we consider three\nvarious models by progressively removing the components\nof SpecTNT as follows.\nA1: Remove the operation of (a) in Figure 1 (i.e., Eq.\n7) and initialize the FCTs as learnable vectors.\nA2: Neglect the FCTs but use the full spectral embed-\ndings for operations (a) and (c) in Figure 1 (i.e., Eq. 7\nand Eq. 9). The resulting model can be seen as using the\noriginal TNT block [10].\nA3: Remove the data ﬂow of spectral embedding, so\nthe model is reduced to the original Transformer [30] for\naggregating the input sequence in a traditional way.\nIn the following evaluations of different tasks, we will\ninclude the results of the three variants for comparison.\n4.3 Music Auto-tagging\nDatasets Million song dataset (MSD) [35] consists\nof one million audio previews and a subset of it has\ncrowd-sourced music tags. Typically, a subset with\nthe 50 most frequent tags are used with randomly split\ntrain/validation/test sets [4]. However, these tags are noisy\nand the random split without considering artist overlaps\nmay cause unintended information leakage. Therefore, we\ntake advantage of manually cleaned 50 tags from a previ-\nous work [36] and split the dataset based on artist names\nso that there is no overlapped artists among the train-\ning/validation/test sets. As a result, we use 233,147 tracks,\nof which 70%, 15%, and 15% are allocated for training,\nvalidation, and test sets, respectively. During training, we\napply random data augmentation to the input waveform\nfollowing the pipeline introduced in [37].\nBaseline Models Two baselines methods are compared.\nThe ﬁrst is CNNSA [6], which employs a convolutional\nfront-end and a transformer encoder to aggregate the tem-\nporal feature. The second baseline [13] uses 7-layer short-\nchunk CNN with residual connection, followed by a fully-\nconnect layer for ﬁnal output. This model has shown state-\nof-the-art performance in music auto-tagging. We utilize\nthe original implementation of [13] to train the baseline\nunder the same conﬁguration as our proposed model.\nEvaluation Metrics Area Under Precision Recall Curve\n(PR-AUC) and Area Under Receiver Operating Character-\nistic curve (ROC-AUC) are used.\nResults The results of music auto-tagging are summa-\nrized in Table 2. SpecTNT outperforms prior state-of-\nthe-art models in both metrics. In the ablation study, A1\nperforms the worst, while A2 and A3 show similar re-\nsults to SpecTNT. This can be explained from the perspec-\nMethod ROC-AUC PR-AUC\nShort-chunk CNN + Res 91.55 37.08\nCNNSA 91.57 37.09\nSpecTNT 92.08 38.62\nA1 91.92 37.85\nA2 92.07 38.59\nA3 92.06 38.46\nTable 2. Results (in %) for automatic music tagging.\ntive of data distribution: the top 50 tags of MSD dataset\nare mostly related to genre and style, both of which need\nenough temporal information to characterize. In A1, the\nprocess of updating FCTs with TEs is removed and this\nmay interfere the temporal information ﬂow being shared\nwith the spectral data and cause the performance drop.\nBy looking into the precision scores of individual tags\nwhere SpecTNT outperforms A3, we observe that instru-\nmental tags such as “piano” and “guitar” can beneﬁt from\nSpecTNT, because they may require more spectral infor-\nmation to model well. This shows the beneﬁt of adding\nthe spectral transformer. Also, the smaller performance\ndifference among SpecTNT, A2, and A3 indicates that the\nsize of MSD dataset might be enough to support architec-\ntures with less prior knowledge. That is, A2 and A3 are\nable to sufﬁciently learn from MSD the useful information\nwithout further utilizing FCTs to interact with the temporal\nembeddings.\n4.4 Vocal Melody Extraction\nDatasets We use two datasets to train the models: MIR1K\n[38], which includes 1000 Chinese karaoke clips, and\na 48-song subset of MedleyDB [39] that includes vocal\ntracks. Since the training sets are relative small, we adopt a\npipeline with four steps of augmentation techniques. There\nis a chance for each step to be applied to a training sam-\nple: i) pitch-shifting by up to ±2 semitones (with 100%\nchance), ii) replacing the original background track with a\nrandomly selected, different background track (with 50%\nchance), iii) changing the gain of the vocal within [−4,2]\ndB (with 100 % chance), and iv) completely removing the\nbackground track (with 10% chance).\nWe choose three test sets for evaluation: ADC2004,\nMIREX05, and MedleyDB. For ADC2004 and MIREX05,\nwe only use the samples that have melody sung by hu-\nman voice. This results in 12 samples from ADC2004\nand 9 samples from MIREX05. For MedleyDB, we only\nuse the songs that have singing voice included in their\n“MELODY2” annotations, yielding 12 songs. The ground-\ntruth pitches are obtained from the MELODY2 annotations\nwithin the intervals marked as “female singer” or “male\nsinger.” These 12 songs are not included in training.\nBaseline Models We compare our model with two base-\nline models. The ﬁrst baseline is the joint detection and\nclassiﬁcation model (JDC) [18] based on CRNN. We use\nthe most representative architecture, called “Main” in [18].\nDataset ADC2004 MIREX05 MedelyDB\nMethod OA RPA VR OA RPA VR OA RPA VR\nJDC 71.2 68.1 73.1 86.0 80.7 85.8 77.0 64.8 73.9\nFTANet 71.2 69.3 72.9 89.9 86.5 91.2 79.4 66.0 72.0\nSpecTNT 85.3 85.0 88.3 91.7 90.4 95.2 78.4 77.9 87.4\nA1 84.8 84.2 89.1 90.2 88.3 94.1 77.9 75.0 85.4\nA2 84.9 84.5 88.3 89.7 87.7 93.1 79.3 75.6 84.0\nA3 84.5 83.7 87.2 88.9 87.2 92.0 74.5 72.7 83.1\nTable 3. Results (in %) for vocal melody extraction.\nThe second baseline is the frequency-temporal attention\nnetwork (FTANet) [19], which is a CNN-based model\nthat employs attention mechanism along the frequency and\ntime axis. We re-implemented JDC and FTANet using Py-\ntorch and used the suggested hyper-parameters in [18, 19].\nBoth models are trained under the same conﬁguration (e.g.,\ndata split and augmentation process) as our model.\nEvaluation Metrics Overall Accuracy (OA), Raw Pitch\nAccuracy (RPA), and V oice Recall (VR) are adopted for\nevaluation. We use mir_eval library [40] to compute the\nperformance values with a tolerance range of 50 cents.\nResults Table 3 shows the results for vocal melody extrac-\ntion. SpecTNT outperforms the baselines by a large mar-\ngin in terms of RPA and VR. To the best of our knowledge,\nthis is the ﬁrst attempt to apply Transformers to this task\nand the results demonstrate its superiority over the CNN\nand CRNN counterparts. It is worth noting that FTANet is\ntrained with an input representation speciﬁcally designed\nfor pitch detection [20], but our model works well with\nspectrogram input. In addition, A3 shows the largest per-\nformance drop, and this demonstrates the usefulness of\nspectral Transformer when training on smaller data.\n4.5 Chord Recognition\nDatasets We use the Billboard dataset to evaluate\nSpecTNT for the chord recognition task. The dataset con-\ntains 890 pieces selected from the Billboard chart slots\n[34]. Following [27], duplicates pieces are ﬁrst removed\nto leave 739 unique pieces in total. The ofﬁcial release of\nthe dataset only comes with 24-D chroma vectors, which\nmight be insufﬁcient to fully demonstrate the effectiveness\nof SpecTNT. Therefore, we manually collected the audio\nﬁles based on the provided meta-data. Due to the poten-\ntial version mismatch between our audio ﬁles and that for\nofﬁcial chord annotations, we applied dynamic time warp-\ning (DTW) [41] to validate each song. Speciﬁcally, we\nﬁrst replicated the chroma features of the ofﬁcial release\nusing Sonic Annotator [42] on our audio ﬁles, and then\ncalculated the alignment cost between the two versions of\nchromagrams for each song using DTW. We selected 462\nsongs with the lowest alignment costs. The songs with\nID’s smaller than 1000 are used for training and the re-\nmaining for testing. To augment the training data (chroma\nand audio), we shifted the pitches by up to ±6 semitones.\nFor evaluation, we adopt the “maj/min” label set with 25\nclasses, where 24 are major and minor triads across the 12\nMethod Chroma CQT\nCR2 78.92 73.38\nBTC 77.98 73.92\nHT 82.68 -\nSpecTNT 80.47 75.62\nA1 80.10 74.83\nA2 78.76 74.44\nA3 77.69 74.99\nTable 4. Results (in %) for chord recognition task\nsemitones plus an additional “no chord” class.\nBaseline Models We compare to three baseline models:\ni) CR2 model from [23], which is a CRNN-based model,\nii) a bi-directional Transformer (BTC) [26], and iii) Har-\nmony Transformer (HT) [27]. BTC and HT are known to\nbe the current state-of-the-art models for chord recogni-\ntion. For CR2 and BTC, we use the ofﬁcial implementa-\ntions with the suggested default settings for both chroma-\ngram and CQT inputs. For HT, we report the chromagram-\nbased results in [27], since the train/test data split and data\naugmentation are very similar to us. We did not conduct\nexperiments using HT with CQT input because non-trivial\nmodiﬁcations are required for the model.\nEvaluation Metrics The Weighted Chord Symbol Recall\n(WCSR) score is reported as evaluation metric. WCSR is\nthe percentage of correctly identiﬁed frames and can be\ncomputed by tc\nta\n×100(%), where tc is the duration of the\ncorrectly predicted segments, andta is the total duration of\nthe test segments.\nResults Table 4 shows the results for chord recognition.\nFor “Chroma” case, the full Billboard dataset is used. For\n“CQT” case, the 462 songs with audio are used. From the\nresults, SpecTNT can outperform all the baselines except\nHT (with chromagram input). However, HT may beneﬁt\nfrom joint training with an additional segmentation loss,\nso the comparison could be unfair. Compared to BTC and\nCR2, SpecTNT achieves better performance for both types\nof input. For the ablation study, since we used less data\nfor CQT input, A2, which is the largest model, may suffer\nfrom over-ﬁtting and thus performs the worst.\n5. CONCLUSION\nWe proposed SpecTNT, a novel Transformer architecture\nthat models spectrograms along both the time and fre-\nquency axes. The introduction of FCT enables effective\ncommunication between the spectral embeddings and tem-\nporal embeddings, maximizing the beneﬁt of Transformer\nencoder for ﬂexible, local, and global modeling. In ex-\nperiments, SpecTNT has demonstrated state-of-the-art per-\nformance in music tagging and vocal melody extraction\nand shown competitive performance in chord recognition.\nFor future work, we plan to apply SpecTNT to other MIR\ntasks, such as beat tracking and structure segmentation.\n6. REFERENCES\n[1] F. Rigaud and M. Radenen, “Singing voice melody\ntranscription using deep neural networks.” in 17th\nInternational Society of Music Information Retrieval\nconference (ISMIR), New York, USA, 2016, pp. 737–\n743.\n[2] S. Leglaive, R. Hennequin, and R. Badeau, “Singing\nvoice detection with deep recurrent neural networks,”\nin Proceedings of IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2015, pp. 121–125.\n[3] S. Dieleman and B. Schrauwen, “End-to-end learning\nfor music audio,” inProceedings of IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), 2014, pp. 6964–6968.\n[4] K. Choi, G. Fazekas, and M. Sandler, “Automatic tag-\nging using deep convolutional neural networks,” in\n17th International Society of Music Information Re-\ntrieval conference (ISMIR), New York, USA, 2016.\n[5] K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Con-\nvolutional recurrent neural networks for music classi-\nﬁcation,” in Proceedings of IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), 2017, pp. 2392–2396.\n[6] M. Won, S. Chun, and X. Serra, “Toward interpretable\nmusic tagging with self-attention,” arXiv preprint\narXiv:1906.04972, 2019.\n[7] R. M. Bittner, B. McFee, J. Salamon, P. Li, and J. P.\nBello, “Deep salience representations for f0 estima-\ntion in polyphonic music.” in 18th International So-\nciety of Music Information Retrieval conference (IS-\nMIR), Suzhou, China, 2017, pp. 63–70.\n[8] M. Won, S. Chun, O. Nieto, and X. Serra, “Data-driven\nharmonic ﬁlters for audio representation learning,”\nProceedings of International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), 2020.\n[9] A. Zadeh, T. Ma, S. Poria, and L.-P. Morency,\n“Wildmix dataset and spectro-temporal transformer\nmodel for monoaural audio source separation,”\narXiv:1911.09783, 2019.\n[10] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and\nY . Wang, “Transformer in transformer,”arXiv preprint\narXiv:2103.00112, 2021.\n[11] P. Lamere, “Social tagging and music information re-\ntrieval,”Journal of new music research, vol. 37, no. 2,\npp. 101–114, 2008.\n[12] J. Lee, J. Park, K. L. Kim, and J. Nam, “Sample-level\ndeep convolutional neural networks for music auto-\ntagging using raw waveforms,” in Sound and Music\nComputing Conference (SMC), 2017.\n[13] M. Won, A. Ferraro, D. Bogdanov, and X. Serra, “Eval-\nuation of cnn-based automatic music tagging models,”\nIn Proc. of Sound and Music Computing (SMC), 2020.\n[14] J. Nam, K. Choi, J. Lee, S.-Y . Chou, and Y .-H. Yang,\n“Deep learning for audio-based music classiﬁcation\nand tagging: Teaching computers to distinguish rock\nfrom bach,”IEEE signal processing magazine, vol. 36,\nno. 1, pp. 41–51, 2018.\n[15] S. Kum, C. Oh, and J. Nam, “Melody extraction on\nvocal segments using multi-column deep neural net-\nworks.” in 17th International Society of Music Infor-\nmation Retrieval conference (ISMIR), New York, USA,\n2016, pp. 819–825.\n[16] W. T. Lu, L. Su et al., “V ocal melody extraction\nwith semantic segmentation and audio-symbolic do-\nmain transfer learning.” in 19th International Society\nof Music Information Retrieval conference (ISMIR),\nParis, France, 2018, pp. 521–528.\n[17] T.-H. Hsieh, L. Su, and Y .-H. Yang, “A streamlined\nencoder/decoder architecture for melody extraction,”\nin Proceedings of IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp. 156–160.\n[18] S. Kum and J. Nam, “Joint detection and classiﬁcation\nof singing voice melody using convolutional recurrent\nneural networks,” Applied Sciences, vol. 9, no. 7, p.\n1324, 2019.\n[19] S. Yu, X. Sun, Y . Yu, and W. Li, “Frequency-temporal\nattention network for singing melody extraction,” in\nProceedings of IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021.\n[20] L. Su and Y .-H. Yang, “Combining spectral and tem-\nporal representations for multipitch estimation of poly-\nphonic music,” IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 23, no. 10, pp.\n1600–1612, 2015.\n[21] A. Jansson, R. M. Bittner, S. Ewert, and T. Weyde,\n“Joint singing voice separation and f0 estimation with\ndeep u-net architectures,” in 2019 27th European Sig-\nnal Processing Conference (EUSIPCO). IEEE, 2019,\npp. 1–5.\n[22] Y . Gao, X. Zhang, and W. Li, “V ocal melody extraction\nvia hrnet-based singing voice separation and encoder-\ndecoder-based f0 estimation,” Electronics, vol. 10,\nno. 3, p. 298, 2021.\n[23] B. McFee and J. P. Bello, “Structured training for large-\nvocabulary chord recognition.” in 18th International\nSociety of Music Information Retrieval conference (IS-\nMIR), Suzhou, China, 2017, pp. 188–194.\n[24] N. Boulanger-Lewandowski, Y . Bengio, and P. Vin-\ncent, “Audio chord recognition with recurrent neu-\nral networks.” in 14th International Society of Music\nInformation Retrieval conference (ISMIR), Curitiba,\nBrazil. Citeseer, 2013, pp. 335–340.\n[25] E. J. Humphrey and J. P. Bello, “Rethinking auto-\nmatic chord recognition with convolutional neural net-\nworks,” in2012 11th International Conference on Ma-\nchine Learning and Applications, vol. 2. IEEE, 2012,\npp. 357–362.\n[26] J. Park, K. Choi, S. Jeon, D. Kim, and J. Park, “A\nbi-directional transformer for musical chord recogni-\ntion,” in20th International Society for Music Informa-\ntion Retrieval Conference (ISMIR), Delft, The Nether-\nlands, 2019.\n[27] T.-P. Chen and L. Su, “Harmony transformer: Incorpo-\nrating chord segmentation into harmony recognition,”\nneural networks, vol. 12, p. 15, 2019.\n[28] K. He, X. Zhang, S. Ren, and J. Sun, “Identity map-\npings in deep residual networks,” in European confer-\nence on computer vision. Springer, 2016, pp. 630–\n645.\n[29] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova,\n“BERT: Pre-training of deep bidirectional transform-\ners for language understanding,” in Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers). Minneapolis, Minnesota: Association for Com-\nputational Linguistics, Jun. 2019, pp. 4171–4186.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin,\n“Attention is all you need,”31st Conference on Neural\nInformation Processing Systems, 2017.\n[31] R. Xiong, Y . Yang, D. He, K. Zheng, S. Zheng,\nC. Xing, H. Zhang, Y . Lan, L. Wang, and T. Liu,\n“On layer normalization in the transformer architec-\nture,” in International Conference on Machine Learn-\ning. PMLR, 2020, pp. 10 524–10 533.\n[32] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Brad-\nbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito,\nM. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,\nL. Fang, J. Bai, and S. Chintala, “Pytorch: An imper-\native style, high-performance deep learning library,” in\nAdvances in Neural Information Processing Systems\n32. Curran Associates, Inc., 2019, pp. 8024–8035.\n[33] I. Loshchilov and F. Hutter, “Decoupled weight decay\nregularization,” inInternational Conference on Learn-\ning Representations, 2019.\n[34] J. A. Burgoyne, J. Wild, and I. Fujinaga, “An expert\nground truth set for audio chord recognition and mu-\nsic analysis.” in12th International Society of Music In-\nformation Retrieval conference (ISMIR), Miami, USA,\nvol. 11, 2011, pp. 633–638.\n[35] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and\nP. Lamere, “The million song dataset,” inProceedings\nof International Conference on Music Information Re-\ntrieval (ISMIR), 2011.\n[36] M. Won, S. Oramas, O. Nieto, F. Gouyon, and X. Serra,\n“Multimodal metric learning for tag-based music re-\ntrieval,” in Proceedings of IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), Toronto, Canada, 2020.\n[37] J. Spijkervet and J. A. Burgoyne, “Contrastive\nlearning of musical representations,” arXiv preprint\narXiv:2103.09410, 2021.\n[38] C.-L. Hsu and J.-S. R. Jang, “On the improvement of\nsinging voice separation for monaural recordings us-\ning the mir-1k dataset,” IEEE Transactions on Audio,\nSpeech, and Language Processing, vol. 18, no. 2, pp.\n310–319, 2009.\n[39] R. M. Bittner, J. Salamon, M. Tierney, M. Mauch,\nC. Cannam, and J. P. Bello, “Medleydb: A multitrack\ndataset for annotation-intensive mir research.” in 15th\nInternational Society of Music Information Retrieval\nconference (ISMIR), Taipei, Taiwan, vol. 14, 2014, pp.\n155–160.\n[40] C. Raffel, B. McFee, E. J. Humphrey, J. Salamon,\nO. Nieto, D. Liang, D. P. Ellis, and C. C. Raffel,\n“mir_eval: A transparent implementation of common\nmir metrics,” in In Proceedings of the 15th Interna-\ntional Society for Music Information Retrieval confer-\nence (ISMIR), Taipei, Taiwan. Citeseer, 2014.\n[41] M. Müller, “Dynamic time warping,” Information re-\ntrieval for music and motion, pp. 69–84, 2007.\n[42] C. Cannam, M. O. Jewell, C. Rhodes, M. Sandler, and\nM. d’Inverno, “Linked data and you: Bringing mu-\nsic research software into the semantic web,” Journal\nof New Music Research, vol. 39, no. 4, pp. 313–325,\n2010.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.696806788444519
    },
    {
      "name": "Computer science",
      "score": 0.6650881767272949
    },
    {
      "name": "Speech recognition",
      "score": 0.4842962324619293
    },
    {
      "name": "Architecture",
      "score": 0.4430699348449707
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34836798906326294
    },
    {
      "name": "Engineering",
      "score": 0.17687949538230896
    },
    {
      "name": "Electrical engineering",
      "score": 0.13343194127082825
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}