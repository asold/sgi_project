{
  "title": "Evaluation of ChatGPT and Gemini Large Language Models for Pharmacometrics with NONMEM",
  "url": "https://openalex.org/W4393863993",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5093026410",
      "name": "Euibeom Shin",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2030560145",
      "name": "Yifan Yu",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A1987257384",
      "name": "Robert R. Bies",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2154842653",
      "name": "Murali Ramanathan",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4377942506",
    "https://openalex.org/W4378574344",
    "https://openalex.org/W1507448733",
    "https://openalex.org/W2944383003",
    "https://openalex.org/W2091458190",
    "https://openalex.org/W4386153819",
    "https://openalex.org/W4387446063",
    "https://openalex.org/W2950199458",
    "https://openalex.org/W4385786385",
    "https://openalex.org/W3159893665",
    "https://openalex.org/W6600384961",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4366269319"
  ],
  "abstract": "<title>Abstract</title> Purpose To assess the ChatGPT 4.0 (ChatGPT) and Gemini Ultra 1.0 (Gemini) large language models on tasks relevant to NONMEM coding in pharmacometrics and clinical pharmacology settings. Methods ChatGPT and Gemini performance on tasks mimicking real-world applications of NONMEM was assessed. The tasks ranged from providing a curriculum for learning NONMEM and an overview of NONMEM code structure to generating code. Prompts to elicit NONMEM code for a linear pharmacokinetic (PK) model with oral administration and a more complex one-compartment model with two parallel first-order absorption mechanisms were investigated. The prompts for all tasks were presented in lay language. The code was carefully reviewed for errors by two experienced NONMEM experts, and the revisions needed to run the code successfully were identified. Results ChatGPT and Gemini provided useful NONMEM curriculum structures combining foundational knowledge with advanced concepts (e.g., covariate modeling and Bayesian approaches) and practical skills, including NONMEM code structure and syntax. Large language models (LLMs) provided an informative summary of the NONMEM control stream structure and outlined the key NM-TRAN records needed. ChatGPT and Gemini were able to generate applicable code blocks for the NONMEM control stream from the lay language prompts for the three coding tasks. The control streams contained focal structural and NONMEM syntax errors that required revision before they could be executed without errors and warnings. Conclusions LLMs may be useful in pharmacometrics for efficiently generating an initial coding template for modeling projects. However, the output can contain errors that require correction.",
  "full_text": "Evaluation of ChatGPT and Gemini Large Language\nModels for Pharmacometrics with NONMEM\nEuibeom ShinÂ \nUniversity at Buffalo, The State University of New York\nYifan YuÂ \nUniversity at Buffalo, The State University of New York\nRobert R. BiesÂ \nUniversity at Buffalo, The State University of New York\nMurali RamanathanÂ \nUniversity at Buffalo, The State University of New York\nResearch Article\nKeywords: Pharmacometrics, ChatGPT, Pharmacokinetics, Drug Development, Arti\u0000cial intelligence,\nGenerative AI, Modeling, Nonlinear mixed effects, NONMEM\nPosted Date: April 3rd, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-4189234/v1\nLicense: ï‰ ï“§ This work is licensed under a Creative Commons Attribution 4.0 International License. Â \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Journal of Pharmacokinetics and\nPharmacodynamics on April 24th, 2024. See the published version at https://doi.org/10.1007/s10928-\n024-09921-y.\nShin et al. \nPage 1 of 26 \nEvaluation of ChatGPT and Gemini Large Language Models for \nPharmacometrics with NONMEM \nEuibeom Shin, Yifan Yu, Robert R. Bies, and Murali Ramanathan \n \nDepartment of Pharmaceutical Sciences, University at Buffalo, The State University of New \nYork, Buffalo, NY, USA. \n \n \nCORRESPONDING AUTHOR: Murali Ramanathan \n355 Pharmacy, Department of Pharmaceutical Sciences \nState University of New York, Buffalo, Buffalo, NY 14214-8033. \n(716)-645-4846 and FAX 716-829-6569. E-mail Murali@Buffalo.Edu \nRunning Head: ChatGPT and Gemini for Pharmacometrics with NONMEM \nKeywords: Pharmacometrics, ChatGPT, Pharmacokinetics, Drug Development, Artificial \nintelligence, Generative AI, Modeling, Nonlinear mixed effects, NONMEM \nWord Count: Title: 86 Characters,  Running Head: 50 characters, Abstract: 247 words, \nIntroduction to Discussion: 3262 words. References: 28. Tables: 0. Figures: 4. \nFinancial Conflicts: See disclosure statement.  \nConfidentiality: Use of the information in this manuscript for commercial, non-commercial, \nresearch or purposes other than peer review not permitted prior to publication without \nexpressed written permission of the author.  \n  \nShin et al. \nPage 2 of 26 \nABSTRACT \nPurpose: To assess the ChatGPT 4.0 (ChatGPT) and Gemini Ultra 1.0 (Gemini) large language \nmodels on tasks relevant to NONMEM coding in pharmacometrics and clinical pharmacology \nsettings. \nMethods: ChatGPT and Gemini performance on tasks mimicking real-world applications of \nNONMEM was assessed. The tasks ranged from providing a curriculum for learning \nNONMEM and an overview of NONMEM code structure to generating code. Prompts to elicit \nNONMEM code for a linear pharmacokinetic (PK) model with oral administration and a more \ncomplex one-compartment model with two parallel first-order absorption mechanisms were \ninvestigated. The prompts for all tasks were presented in lay language. The code was \ncarefully reviewed for errors by two experienced NONMEM experts, and the revisions \nneeded to run the code successfully were identified. \nResults: ChatGPT and Gemini provided useful NONMEM curriculum structures combining \nfoundational knowledge with advanced concepts (e.g., covariate modeling and Bayesian \napproaches) and practical skills, including NONMEM code structure and syntax. Large \nlanguage models (LLMs) provided an informative summary of the NONMEM control stream \nstructure and outlined the key NM-TRAN records needed. ChatGPT and Gemini were able to \ngenerate applicable code blocks for the NONMEM control stream from the lay language \nprompts for the three coding tasks. The control streams contained focal structural and \nNONMEM syntax errors that required revision before they could be executed without errors \nand warnings. \nShin et al. \nPage 3 of 26 \nConclusions: LLMs may be useful in pharmacometrics for efficiently generat ing an initial \ncoding template for modeling projects. However, the output can contain errors that require \ncorrection. \n  \nShin et al. \nPage 4 of 26 \nINTRODUCTION \nLarge language models (LLM), as exemplified by ChatGPT from OpenAI, Gemini from Google, \nLlama from Meta, and Claude from Anthropic (1-5), are widely viewed as an important  \nadvancement in the field of artificial intelligence (AI) as they are capable of emulating \nhuman-like text generation and comprehension (6). Usage of LLM has increased rapidly \nsince they are versatile, user-friendly, and can assist with diverse tasks, e.g., engaging in \ncasual conversations and document editing to solving complex, problem-oriented queries \n(7). LLM can generate software code and language-based text with a degree of sophistication \nthat suggests potential utility in pharmacometrics (8). \nPharmacometrics approaches enable analysis of the time courses and variability of drug \nconcentrations (9) and are leveraged to inform dosage recommendations and therapeutic \nstrategies (10). NONMEM is a software package for implementing nonlinear mixed effects \nregression methods widely used by pharmacometric ians ( 11). NONMEM employs the \nNMTRAN language for coding, which we refer to hereinafter simply as NONMEM code . \nNONMEM-coded nonlinear mixed effects regression analyses of compartmental \npharmacokinetic models are commonly summarized in the new drug applications submitted \nto regulatory agencies such as the Food and Drug Administration and European Medicines \nAgency to support marketing approval for innovator products (12). \nChatGPT, Gemini, and other LLM have already proven useful for generating code in \ncommonly used programming languages such as C++, Python, and R. Thus, integration of \nChatGPT into a NONMEM workflow presents an emerging opportunity to enhance the \nproductivity of coding tasks for pharmacometrics modeling ( 13, 14). In addition to coding , \nShin et al. \nPage 5 of 26 \nChatGPT could potentially streamline aspects of modeling documentation and be used for \nlearning NONMEM and pharmacometrics.  \nThe key research aim was to assess the capabilities and limitations of ChatGPT 4.0 and \nGemini Ultra 1.0 in interpreting lay language prompts for pharmacometric modeling tasks \ndesigned to elicit corresponding NONMEM code. The tasks were selected to mirror the \npractical applications of NONMEM in pharmacometrics and clinical pharmacology settings \n(10) and covered a range of activities, from providing a curriculum for learning and \nunderstanding NONMEM code structure to developing code for different pharmacokinetic \n(PK) models.  The NONMEM tasks included cod ing for a linear PK model with oral \nadministration and a more complex one-compartment model with two parallel first-order \nabsorption mechanisms.  \nShin et al. \nPage 6 of 26 \nMETHODS \nChatGPT Methods \nThe default version of ChatGPT 4.0 (1)  and Gemini Ultra 1.0 (2) were run at chat.openai.com \nand gemini.google.com, respectively, on a MacBook Air computer running macOS Ventura \n13.5.1. Screenshots from individual ChatGPT runs were saved.  \nChatGPT 4.0 and Gemini Ultra 1.0 are hereinafter referred to as ChatGPT and Gemini, \nrespectively. \nCase Studies \nTwo types of prompts were provided to ChatGPT and Gemini to generate output. The first \nrelated to teaching elements of NONMEM (Case Studies 1 and 2), and the second related to  \ngenerated NONMEM code for a specific population PK example (Case Studies 3A and 3B).  \nWe assessed ChatGPT and Gemini â€™s capability to comprehend and instruct on \npharmacokinetic/pharmacodynamic (PK/PD) modeling with NONMEM, focusing on a one-\ncompartment model analysis.  \nA structured sequence of prompts was designed to engage the LLM in a step-by-step process, \nbeginning with an explanation of NONMEM principles and culminating in a practical \ndemonstration of NONMEM code. For convenience of presentation, Case Study 1 was a \ncurricular framework for learning NONMEM in context, Case Study 2 was a request for a \nspecific explanation of NONMEM code, whereas Case Study 3A and Case Study 3B evaluated \nthe ability of ChatGPT and Gemini to generate NONMEM code for specific pharmacokinetic \nmodels. The following prompts were entered for the four Case Studies. \nShin et al. \nPage 7 of 26 \nCase Study 1. If you were a teacher, what topics related to NONMEM would you teach \nstudents? \nCase Study 2.  You are an expert NONMEM code writer for population PKPD in the \npharmaceutical industry. You are always accurate and precise when formulating \nNONMEM code. Please describe the structure of NONMEM code using a basic example. \nCase Study 3A. Can you provide the NONMEM code for a linear PK model with oral \nadministration? \nCase Study 3B. Provide NONMEM code for a one-compartment model with two parallel \nabsorptions and linear elimination. \nCase 1, Case 2, and Case 3A were run together in a single chat session, and Case 3B was \nrunning on a separate chat.  \nThe prompt engineering strategy included persona crafting to embody an expert NONMEM \ncoder, role assignment to align with instructional tasks, and sequential prompting to \nmaintain context throughout the interaction.  \nThe code generated was reviewed carefully for errors and omissions by two individuals \n(authors: YY, RRB) with NONMEM expertise. \nShin et al. \nPage 8 of 26 \nRESULTS \nCase Study 1: Curricular Framework for Teaching NONMEM  \nCase Study 1. If you were a teacher, what topics related to NONMEM would you teach students? \nFigure 1 compares NONMEM related topics provided by ChatGPT and Gemini results. \nChatGPT: The ChatGPT response reflects its utility for structuring a NONMEM curriculum \nthat combines foundational knowledge with advanced concepts and practical skills. \nThe topic list proposed by ChatGPT starts with pharmacometrics principles to establish a \nfoundation before progressing to advanced aspects. It provided technical requirements for \nNONMEM software installation, setup, and data management. \nThe response also included a detailed exploration of NONMEM code structure and syntax to \nequip learners with coding skills. Advanced topics, like covariate modeling and Bayesian \napproaches were integrated and showcasing the curriculumâ€™s depth. Practical ap plications, \nincluding simulations and predictive model checks, are also covered.  \nMoreover, the topic list addressed regulatory considerations and documentation practices \nin PKPD modeling, aligning with industry standards. It also keeps pace with current trends \nand future directions in pharmacometrics, ensuring contemporary relevance. \nGemini: The overall outline of Geminiâ€™s syllabus resembled the ChatGPTâ€™s in that it listed the \nintroduction to PKPD, the fundamental structure of the NONMEM, basic structural PK models, \nand intermediate/advanced topics.  \nGemini suggested learning foundational concepts of pharmacometrics and NONMEM \nShin et al. \nPage 9 of 26 \nincluding inter-individual variability and covariate modeling and then covering topics \nrelated to the fundamentals of more intermediate/advanced population agnostic \npharmacokinetic models such as Michaelis-Menten elimination and target-mediated drug \ndisposition. It further provided the model evaluation and diagnostic methods, including the \nvisual predictive checks and bootstrap techniques. Gemini included some basic goodness of \nfit approaches under an advanced topics heading. Gemini ended with a similar emphasis as \nChatGPT, practicing the analyzing real-world PKPD datasets. \nCase Study 2. Explanation of NONMEM Code \nCase Study 2 . You are an expert NONMEM code writer for population PKPD in the \npharmaceutical industry. You are always accurate and precise when fo rmulating NONMEM \ncode. Please describe the structure of NONMEM code using a basic example. \nFigure 2 summarizes the ChatGPT and Gemini results for Case Study 2. \nChatGPT: ChatGPT effectively communicated complex pharmacometrics concepts and \nprovided an informative summary of NONMEM code structure. \nThe key components of NONMEM code, starting with the Problem Statement ($ğ‘ƒğ‘…ğ‘‚ğµ ), \nwhich functions as the modelâ€™s title or description , were provided. This was followed by \nInput Data Specification ($ğ·ğ´ğ‘‡ğ´) for defining the dataset file and format, and Input Variable \nDefinitions ($ğ¼ğ‘ğ‘ƒğ‘ˆğ‘‡), detailing essential variables such as ğ¼ğ·, ğ‘‡ğ¼ğ‘€ğ¸, ğ·ğ‘‰, and ğ´ğ‘€ğ‘‡. \nThe Model Specification ( $ğ‘ƒğ‘…ğ¸ğ·  or $ğ‘€ğ‘‚ğ·ğ¸ğ¿ ) section containing model equations, \nEstimation Method ($ğ¸ğ‘†ğ‘‡ğ¼ğ‘€ğ´ğ‘‡ğ¼ğ‘‚ğ‘) for the selecting parameter estimation algorithm, and \nOutput Specification ( $ğ‘‡ğ´ğµğ¿ğ¸ ) for defining the output format were also described. The \nresponse also included sections such as $ğ¶ğ‘‚ğ‘‰  for specifying the covariance matrix and \nShin et al. \nPage 10 of 26 \n$ğ‘‡ğ»ğ¸ğ‘‡ğ´ for providing starting parameter estimates and bounds.  \nChatGPT also provided a basic pharmacokinetic model as an example (Figure 2), clarifying \nkey parameters such as clearance ( ğ¶ğ¿), volume of distribution ( ğ‘‰), and absorption rate \nconstant (ğ¾ğ´). \nFigure 2 also summarizes the errors in the model components. First, ChatGPT incorrectly \nused $ğ‘ƒğ´ğ‘…ğ´ğ‘€  to define initial parameter estimates instead of $ğ‘‡ğ»ğ¸ğ‘‡ğ´ , which is the \nappropriate block for defining initial estimates for fixed effects parameters. Secondly, \nChatGPT provided an incorrect example for $ğ‘ƒğ‘…ğ¸ğ·. ChatGPT used $ğ‘ƒğ‘…ğ¸ğ· to code a one \ncompartment model with first order absorption, without using ğ‘ƒğ‘…ğ¸ğ·ğ‘ƒğ‘ƒ, a library of pre-\nwritten modeling in NONMEM. However, it incorrectly stated that $ğ‘ƒğ‘…ğ¸ğ· is for â€œpredictive \nmodelâ€, and the equations presented by ChatGPT within $ğ‘ƒğ‘…ğ¸ğ· were incorrect. For the one-\ncompartment, first-order absorption model, the Bateman function should be used to \ncalculate concentration in the central compartment:  \nğ‘Œ = ğ´ğ‘€ğ‘‡ âˆ—ğ¾ğ´/(ğ‘‰ âˆ—ğ¾ğ´ âˆ’ ğ¶ğ¿/ğ‘‰)  âˆ—  (ğ¸ğ‘‹ğ‘ƒ(âˆ’ğ¶ğ¿/ğ‘‰ âˆ— ğ‘‡ğ¼ğ‘€ğ¸) âˆ’ ğ¸ğ‘‹ğ‘ƒ(âˆ’ğ¾ğ´ âˆ— ğ‘‡ğ¼ğ‘€ğ¸)) \nMoreover, In the final part of ChatGPTâ€™s answer, it mistakenly identified $ğ¶ğ‘‚ğ‘‰ and $ğ‘‡ğ»ğ¸ğ‘‡ğ´ \nas â€œControl Terminationâ€. \nAdditionally, ChatGPT did not provide any introduction for $ğ‘ƒğ¾ , which is an important \nmodel component for modeling the values of basic and additional PK parameters when using \nğ‘ƒğ‘…ğ¸ğ·ğ‘ƒğ‘ƒ. \nGemini: Gemini provided the core structures of the NOMEMâ€™s NM -TRAN control records at \nthe beginning and the simple one-compartment PK examples code. The structure of \nShin et al. \nPage 11 of 26 \nNONMEM codes was listed in the order of the real workflow: problem statement \n($ğ‘ƒğ‘…ğ‘‚ğµğ¿ğ¸ğ‘€), data input ( $ğ¼ğ‘ğ‘ƒğ‘ˆğ‘‡, $ğ·ğ´ğ‘‡ğ´), model subroutine selection ( $ğ‘†ğ‘ˆğµğ‘…ğ‘‚ğ‘ˆğ‘‡ğ¼ğ‘ğ¸), \nPK parameters ( $ğ‘ƒğ¾), defining the residual error ( $ğ¸ğ‘…ğ‘…ğ‘‚ğ‘…) and outputting the results \n($ğ‘‡ğ´ğµğ¿ğ¸). However, the simple one-compartment PK example was missing the crucial initial \nestimate code blocks: initial estimates for the OMEGA matrix ( $ğ‘‚ğ‘€ğ¸ğºğ´), initial estimates \nand bounds of fixed effects parameters ( $ğ‘‡ğ»ğ¸ğ‘‡ğ´), and initial estimates for the NONMEM \nSIGMA matrix ( $ğ‘†ğ¼ğºğ‘€ğ´). Additionally, no inter-individual variability was specified in the \n$PK block (i.e., the THETAâ€™s were not associated with a function that linked them to the inter-\nindividual variability, e.g., ğ‘‡ğ»ğ¸ğ‘‡ğ´(1) âˆ— ğ‘’ğ‘¥ğ‘(ğ¸ğ‘‡ğ´(1)) etc. \nCase Study 3. NONMEM Coding \nCase Study 3A . Can you provide the NONMEM code for a linear PK model with oral \nadministration? \nFigure 3 summarizes the ChatGPT and Gemini results for Case Study 3A. \nChatGPT: ChatGPT generated an instructive example of NONMEM code for a basic linear PK \nmodel with oral administration. The response begins with the $ğ‘ƒğ‘…ğ‘‚ğµ statement, effectively \nsetting the context as a basic linear PK model for oral drug administration. The inclusion of \n$ğ·ğ´ğ‘‡ğ´ and $ğ¼ğ‘ğ‘ƒğ‘ˆğ‘‡ sections, specifying the datafile and defining essential data columns like \nğ¼ğ·, ğ‘‡ğ¼ğ‘€ğ¸, ğ´ğ‘€ğ‘‡, and ğ·ğ‘‰.  \nIn the $ğ‘†ğ‘ˆğµğ‘…ğ‘‚ğ‘ˆğ‘‡ğ¼ğ‘ğ¸ğ‘†  section, ChatGPT appropriately select ed ğ´ğ·ğ‘‰ğ´ğ‘2  and ğ‘‡ğ‘…ğ´ğ‘ğ‘†2 , \nsuitable for a one-compartment model with first-order absorption and elimination. The $ğ‘ƒğ¾ \nsection, which is used to model the values of basic and additional PK parameters , was well-\nconstructed, with clear definitions for the absorption rate constant (ğ¾ğ´), clearance (ğ¶ğ¿), and \nShin et al. \nPage 12 of 26 \nvolume of distribution (ğ‘‰), along with the calculation for the elimination rate constant (ğ¾).  \nChatGPTâ€™s choice of the first order conditional estimation with interaction (FOCE \nINTERACTION) method in the $ğ¸ğ‘†ğ‘‡ğ¼ğ‘€ğ´ğ‘‡ğ¼ğ‘‚ğ‘  section, and the request for covariance \nanalysis ( $ğ¶ğ‘‚ğ‘‰ğ´ğ‘…ğ¼ğ´ğ‘ğ¶ğ¸ ) and output specifications ( $ğ‘‡ğ´ğµğ¿ğ¸ ), demonstrates an \nâ€œunderstandingâ€ of the modeling process in NONMEM. \nThe $ğ¸ğ‘…ğ‘…ğ‘‚ğ‘… block incorrectly specified the residual error model. The formulas did not align \nwith the standard additive or proportional error models and conflicted with the proportional \nerror indicated in the $ğ‘†ğ¼ğºğ‘€ğ´ block. For a proportional error model, the correct equation is:  \nğ‘Œ = ğ¼ğ‘ƒğ‘…ğ¸ğ· + ğ¼ğ‘ƒğ‘…ğ¸ğ· âˆ— ğ¸ğ‘ƒğ‘†(1) \nThe LLMâ€™s choice of the FOCE INTERACTION method in the $ğ¸ğ‘†ğ‘‡ğ¼ğ‘€ğ´ğ‘‡ğ¼ğ‘‚ğ‘  section was \nproblematic, leading to error messages upon running the code, necessitating a revision to \nğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· = 1 or ğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· = ğ¶ğ‘‚ğ‘ğ·. In the $ğ‘‡ğ´ğµğ¿ğ¸, previously calculated elimination rate \nconstant was not included in the output. \nSeveral revisions made to address the errors in the control stream before NONMEM would \nrun without errors and warnings. These are presented sequentially for illustrative purposes \nin the Supplementary File. \nGemini: Gemini generated a NONMEM control stream for a one-compartment model with \nfirst-order absorption, which is also provided in Case 2 as an example of the NONMEM code. \nThe response provided reasonable $ğ‘ƒğ‘…ğ‘‚ğµğ¿ğ¸ğ‘€ for the correct input item in $ğ¼ğ‘ğ‘ƒğ‘ˆğ‘‡ section. \nHowever, in $ğ·ğ´ğ‘‡ğ´  block, Gemini did not correctly ignore the first row containing the \ncolumn headers of the input data set.  \nIn the $ğ‘†ğ‘ˆğµğ‘…ğ‘‚ğ‘ˆğ‘‡ğ¼ğ‘ğ¸ğ‘† , Gemini appropriately selected ğ´ğ·ğ‘‰ğ´ğ‘2  ğ‘‡ğ‘…ğ´ğ‘ğ‘†2  for one-\nShin et al. \nPage 13 of 26 \ncompartment model with first order absorption.  However, it also generated $ğ‘€ğ‘‚ğ·ğ¸ğ¿ that \ncannot be used in the selected subroutine. This redundant code block will cause errors when \nrunning the model in NONMEM.  \nGemini correctly selected ğ¶ğ¿ , ğ‘‰ , and ğ¾ğ´  as the PK parameters in $ğ‘ƒğ¾ , and included \nexponential BSV on ğ¶ğ¿ and ğ‘‰. However, incorrect parameter names ( ğ‘‡ğ‘‰ğ¶ğ¿ and ğ‘‡ğ‘‰ğ‘‰) were \nused. In addition, the scaling factor ğ‘†2 = ğ‘‰ is missing in $ğ‘ƒğ¾. \nThe $ğ¸ğ‘†ğ‘‡ğ¼ğ‘€ğ´ğ‘‡ğ¼ğ‘‚ğ‘  block is problematic as Gemini used ğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· = ğ¹ğ‘‚ğ¶ğ¸ğ¼ to specify \nğ¹ğ‘‚ğ¶ğ¸ğ¼  method, which should be coded as ğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· = 1 ğ¼ğ‘ğ‘‡ğ¸ğ‘…ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘ or ğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· =\nğ¶ğ‘‚ğ‘ğ· ğ¼ğ‘ğ‘‡ğ¸ğ‘…ğ´ğ¶ğ‘‡ğ¼ğ‘‚ğ‘. Gemini also chose an inappropriate term and value for number of \nsignificant digits using ğ‘†ğ¼ğº = 0.2 instead of an integer value for ğ‘ğ‘†ğ¼ğº. Significant digits are \nspecified when using the differential equation subroutines (where a solution tolerance is \nspecified), not when using the algebraic solutions for simple PK models in PREDPP. \nAs for the output table, Gemini included ğ¼ğ‘ƒğ‘…ğ¸ğ· and ğ·ğ‘‰ğ‘…ğ¸ğ‘†, which were not defined in the \nprevious section. More importantly, Gemini missed $ğ‘‡ğ»ğ¸ğ‘‡ğ´, $ğ‘‚ğ‘€ğ¸ğºğ´, and $ğ‘†ğ¼ğºğ‘€ğ´ in the \ncontrol stream, leaving the control stream incomplete. This error also occurred in Case 2. \nCase Study 3B. Provide NONMEM code for a one-compartment model with two parallel \nabsorptions and linear elimination. \nFigure 4 summarizes the ChatGPT and Gemini results for Case Study 3B. \nChatGPT: ChatGPT produced a NONMEM control stream for a one-compartment model with \ntwo parallel first order absorptions, which is a user-written model, that contained the correct \ncode blocks. ChatGPT provided a correct $ğ‘ƒğ‘…ğ‘‚ğµğ¿ğ¸ğ‘€ code block and defined the key data \nitems correctly in the dataset in $ğ¼ğ‘ğ‘ƒğ‘ˆğ‘‡ block. However, in the $ğ·ğ´ğ‘‡ğ´ code block, ChatGPT \nShin et al. \nPage 14 of 26 \nincorrectly included ğ¼ğºğ‘ğ‘‚ğ‘…ğ¸ = ğ¶ğ‘€ğ‘‡(3).  \nIn the $SUBROUTINE section, ChatGPT inappropriately selected ğ´ğ·ğ‘‰ğ´ğ‘2  and TRANS2, \nwhich generated an error. For the control stream using differential equations, ğ´ğ·ğ‘‰ğ´ğ‘6, \nğ´ğ·ğ‘‰ğ´ğ‘8, ğ´ğ·ğ‘‰ğ´ğ‘9 or ğ´ğ·ğ‘‰ğ´ğ‘13 should be selected, and a relative tolerance ğ‘‡ğ‘‚ğ¿ needed to \nbe specified. \nIn the $ğ‘€ğ‘‚ğ·ğ¸ğ¿ section, ChatGPT correctly included two absorption depots and a central \ncompartment. However, ChatGPT incorrectly defined the central compartment as the default \ndosing compartment.  \nIn the $ğ‘ƒğ¾  section, ChatGPT correctly defined two first-order absorption rate constants \n(ğ¾ğ´1 and ğ¾ğ´2), clearance ( ğ¶ğ¿), and volume of distribution ( ğ‘‰) along with their between-\nsubject variability (ğµğ‘†ğ‘‰). The ChatGPT code failed to distribute the dose into two different \nabsorption pathways because a fraction parameter was not included in $PK. The presence of \nredundant code regarding the ğ¸ğ‘‡ğ´ parameters also resulted in an error message. The $ğ·ğ¸ğ‘† \nsection had the correct ordinary differential equations for this model.  \nCertain error patterns identified in Case Study 3A persisted in Case Study 3B, e.g., the \n$ğ¸ğ‘…ğ‘…ğ‘‚ğ‘…  code block had incorrect standard deviation and proportional error model \nformulae. In addition, ChatGPT only provided a single initial estimate for the ğ‘‚ğ‘€ğ¸ğºğ´ matrix, \nand the code erroneously contained ğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· = ğ¹ğ‘‚ğ¶ğ¸ to specify ğ¹ğ‘‚ğ¶ğ¸ method instead of \nthe correct ğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· = 1 or ğ‘€ğ¸ğ‘‡ğ»ğ‘‚ğ· = ğ¶ğ‘‚ğ‘ğ·. \nThe revisions made to address the errors and to obtain a NONMEM run without errors and \nwarnings are summarized in the Supplementary File. \nShin et al. \nPage 15 of 26 \nGemini: Gemini provided a NONMEM control stream for a one-compartment model with \nparallel absorption and linear elimination. After generating a reasonable $ğ‘ƒğ‘…ğ‘‚ğµğ¿ğ¸ğ‘€ block, \nit correctly coded $INPUT and $ğ·ğ´ğ‘‡ğ´, with required input items. \nGemini correctly selected ğ´ğ·ğ‘‰ğ´ğ‘6 as the subroutine, as this user-written model required \ndifferential equation solver. It also provided a reasonable value for the relative tolerance \nğ‘‡ğ‘‚ğ¿. However, the $ğ‘€ğ‘‚ğ·ğ¸ğ¿ generated by Gemini is problematic and should be revised to \nğ¶ğ‘‚ğ‘€ğ‘ƒ = (ğ·ğ¸ğ‘ƒğ‘‚ğ‘‡1) ğ¶ğ‘‚ğ‘€ğ‘ƒ = (ğ·ğ¸ğ‘ƒğ‘‚ğ‘‡2) ğ¶ğ‘‚ğ‘€ğ‘ƒ = (ğ¶ğ¸ğ‘ğ‘‡). \nIn $ğ‘ƒğ¾, Gemini correctly defined all the required PK parameters in this PK model, including \nğ¹1, ğ¹2, ğ¾ğ´1, ğ¾ğ´2, ğ‘‰, and ğ¶ğ¿. However, it provided an incorrect scale factor for the central \ncompartment and did not specify any inter-individual variability in the parameters. \nThe $ğ¸ğ‘…ğ‘…ğ‘‚ğ‘… block incorrectly specified the residual error model. A correct $ğ¸ğ‘†ğ‘‡ was coded \nspecifying FOCE method with interaction. Additionally, no initial estimates were provided \nfor the ğ‘‚ğ‘€ğ¸ğºğ´ matrix. It's worth noting that Gemini missed including $ğ·ğ¸ğ‘†, $ğ‘‡ğ»ğ¸ğ‘‡ğ´, and \n$ğ‘‡ğ´ğµğ¿ğ¸ in the control stream. These components are essential and the model will not run \nwithout them. \nShin et al. \nPage 16 of 26 \nDISCUSSION \nWe evaluated ChatGPT and Gemini in several case studies that would enable us to delineate \nthe strengths and limitations of using LLM for NONMEM coding and to identify patterns in \nthe coding errors that occurred. We focused on NONMEM because it is the most widely used \ncoding framework in pharmacometrics. However, because most versions of NONMEM are \nmarketed commercially and not freely available to open-source software users ( 15), the \nvolume of NONMEM code readily accessible online is modest compared to more widely used \nlanguages such as C++, Python, and R. This makes NONMEM a challenging framework for \nLLM, which depends on the availability of adequate training data.  \nThe strength of ChatGPT for NONMEM coding is that it can use the information from a short \nprompt provided by the user in lay language and provide a template of code containing the \nkey components of a NONMEM control stream. The ChatGPT output is a good starting point \nfor a coding project. The performance of ChatGPT on the NONMEM coding tasks contained \nflaws and had focal structural (e.g., in Case Study 3B, the fraction of dose distributed between \nthe parallel absorption compartments was not included ) and NONMEM syntax (e.g., the \nFOCE method was incorrectly coded in both Case Study 3A and 3B) errors that require \ncorrection. We also found odd code snippets, e.g., NONMEM does not have $ğ‘ƒğ´ğ‘…ğ´ğ‘€ code \nblock. We were surprised at the overall quality of the syllabus outline generated by ChatGPT \nin Case Study 1, given that pharmacometrics is a niche sub-specialty in the pharmaceutical \nindustry, and there are only a few academic institutions that conduct cutting-edge research \nprojects or provide training with NONMEM (16).  \nThe code from both ChatGPT-4 and Gemini Ultra 1.0 are imperfect at generating NONMEM \nShin et al. \nPage 17 of 26 \ncode. Shared error patterns were seen across both LLMs, e.g., conflicting code streams and \nconsistently selecting the incorrect method. However, each LLM had areas of strength and \nweakness. ChatGPT performed well in providing a structural outline of the control streams \nby providing required NM-TRAN control records that Gemini omitted ($ğ‘†ğ¼ğºğ‘€ğ´, $ğ‘‚ğ‘€ğ¸ğºğ´, \n$ğ‘‡ğ»ğ¸ğ‘‡ğ´). Geminiâ€™s code was more carefully fine -tuned, and in Case 3B, it contained the \nfractions of dose absorbed through the two parallel absorption pathways and the correct \nremoval of column names, which ChatGPT missed.  \nA weakness of our study is that we did not compare the NONMEM coding performance of \nChatGPT to other LLM other than Gemini, e.g., Llama and Claude (1, 3, 5, 17). We also did not \nexamine ChatGPT in the context of other software tools such as mrgsolve (18), nlmxr (19, 20), \nthe Mixtran used by Monolix (21), and STAN (22) that might be used by pharmacometricians. \nComparatively, ChatGPT, Gemini, and other LLM are more proficient at generating code and \nit is projected that the adoption of LLM could yield significant productivity improvements \nfor programmers. Cloesmeijer et al. found that ChatGPT can be used to obtain PK parameters \nfrom the literature, code a population model and generate visualizations in R (23). However, \nCloesmeijer et al. indicated that the performance of ChatGPT for NONMEM was poor but did \nnot present research results  (23). In previous work, we found that R code generated with \nChatGPT is of satisfactory quality ( 24). The accuracy of ChatGPT in calculations involving \nexponential, logarithmic, and trigonometric functions, which are common in \npharmacometrics, is only ~50% ( 24-26). Thus, conducting numerical PK and \npharmacometrics directly within the ChatGPT interface should be avoided for now. The \ncitations generated by earlier versions of ChatGPT (e.g., ChatGPT 3.5) often contained errors \nShin et al. \nPage 18 of 26 \nthat were referred to as â€œartificial hallucinationsâ€, but th is issue has been substantially \nimproved with the new algorithm (24, 27, 28). Similarly, we expect that new enhancements \nto LLM will quickly mitigate the high frequency of error rates in numerical calculations.  \nIn conclusion, our results show that the utility of ChatGPT and Gemini for NONMEM coding \nmight be metaphorically viewed as a cup that is currently half full. Although ChatGPT and \nGemini may be useful for generating early versions of code, the resulting code requires \ncareful review and changes before it can be run. However, LLMs are rapidly improving in \ntheir capabilities, and it might not be long before they are proficient at many NONMEM \ncoding tasks. \n  \nShin et al. \nPage 19 of 26 \nFUNDING INFORMATION \nThis is unfunded research. Support from Grant MS190096 from the Department of Defense \nMultiple Sclerosis Research Program for the Office of the Congressionally Directed Medical \nResearch Programs (CDMRP) to the Ramanathan laboratory is gratefully acknowledged. \nCONFLICT OF INTEREST DISCLOSURE \nEuibeom Shin and Yifan Yu have no conflicts.  \nDr. Bies receives grant funding from NIDA, NIAID, NICHD, USAID, Bill and Melind a Gates \nFoundation and serves as a consultant for Advanced Biosciences Laboratories (NIAID). In \nthe past he has served as a consultant for Lumos Biopharma through NGT Biopharma \nConsultants. \nDr. Murali Ramanathan received research funding from the National Multiple Sclerosis \nSociety, Department of Defense, National Science Foundation, and National Institute of \nNeurological Diseases and Stroke. He receives royalty from a self-published textbook. \nAUTHOR CONTRIBUTIONS \nEuibeom Shin â€“ Data analysis, manuscript preparation. \nYifan Yu â€“ Data analysis, manuscript preparation. \nRobert Bies â€“ Manuscript preparation. \nMurali Ramanathan â€“ Study concept and design, data analysis, manuscript preparation. \n  \nShin et al. \nPage 20 of 26 \nREFERENCES \n(1) OpenAI. ChatGPT Jan 2024 edn. (2024). \n(2) Gemini Team  et al. Gemini: A Family of Highly Capable Multimodal Models.  2023. \n(3) Meta. Llama 2: open source, free for research and commercial use.   (2024). \n(4) Touvron, H.  et al. Llama 2: Open Foundation and Fine-Tuned Chat Models.  2023. \n(5) Anthropic. Meet Claude.   (2024). \n(6) Orru, G., Piarulli, A., Conversano, C. & Gemignani, A. Human-like problem-solving \nabilities in large language models using ChatGPT. Front Artif Intell  6, 1199350 (2023). \n(7) Roumeliotis, K.I. & Tselikas, N.D. ChatGPT and Open-AI Models: A Preliminary Review. \nFuture Internet  15, 192 (2023). \n(8) Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language \nunderstanding by generative pre-training.  (2018). \n(9) Owen, J.S. & Fiedler-Kelly, J. Introduction to population \npharmacokinetic/pharmacodynamic analysis with nonlinear mixed effects models  (John \nWiley & Sons:  2014). \n(10) PÃ©tricoul, O., Cosson, V., Fuseau, E. & Marchand, M. Population models for drug \nabsorption and enterohepatic recycling. Pharmacometrics: the science of quantitative \npharmacology, 345-82 (2007). \n(11) Bauer, R.J. NONMEM Tutorial Part I: Description of Commands and Options, With \nSimple Examples of Population Analysis. CPT Pharmacometrics Syst Pharmacol  8, 525-\n37 (2019). \n(12) Sun, H.  et al.  Population pharmacokinetics: a regulatory perspective. Clinical \npharmacokinetics  37, 41-58 (1999). \n(13) Cloesmeijer, M.E., Janssen, A., Koopman, S.F., Cnossen, M.H., MathÃ´t, R.A. & \nconsortium, S. ChatGPT in pharmacometrics? Potential opportunities and limitations. \nBritish journal of clinical pharmacology  90, 360-5 (2024). \n(14) Shin, E. & Ramanathan, M. Evaluation of prompt engineering strategies for \npharmacokinetic data analysis with the ChatGPT large language model. Journal of \nPharmacokinetics and Pharmacodynamics, 1-8 (2023). \n(15) Fidler, M.  et al.  Nonlinear Mixed-Effects Model Development and Simulation Using \nnlmixr and Related R Open-Source Packages. CPT Pharmacometrics Syst Pharmacol  8, \n621-33 (2019). \n(16) Bonate, P.L.  et al. Training the next generation of pharmacometric modelers: a multisector \nperspective. J Pharmacokinet Pharmacodyn,  (2023). \n(17) Google AI. Bard Large language model.   (2023). \n(18) Baron, K. mrgsolve: Simulate from ODE-Based Models. R package version 1.4.1.   \n(Metrum Research Group, Tariffville, CT, 2024). \n(19) Fidler, M., Hooijmaijers, R., Schoemaker, R., Wilkins, J.J., Xiong, Y. & Wang, W. R and \nnlmixr as a gateway between statistics and pharmacometrics. CPT Pharmacometrics Syst \nPharmacol  10, 283-5 (2021). \n(20) Fidler, M.  et al. nlmixr: an R package for population PKPD modeling.   (2019). \n(21) Anonymous. Monolix documentation . <https://monolix.lixoft.com/single-page/> (2023). \nAccessed March 13, 2024 2024. \n(22) Anonymous. Stan Reference Manual. Vol. Version 2.34 (NumFOCUS, 2011). \nShin et al. \nPage 21 of 26 \n(23) Cloesmeijer, M.E., Janssen, A., Koopman, S.F., Cnossen, M.H., Mathot, R.A.A. & \nSymphony consortium. ChatGPT in pharmacometrics? Potential opportunities and \nlimitations. Br J Clin Pharmacol  90, 360-5 (2024). \n(24) Shin, E. & Ramanathan, M. Evaluation of prompt engineering strategies for \npharmacokinetic data analysis with the ChatGPT large language model. J Pharmacokinet \nPharmacodyn,  (2023). \n(25) Frieder, S.  et al.  Mathematical capabilities of ChatGPT. arXiv, arXiv:2301.13867v2 \n(2023). \n(26) Yuan, Z., Yuan, H., Tan, C., Wang, W. & Huang, S. How well do large language models \nperform in arithmetic tasks? arXiv, arXiv:2304.02015 (2023). \n(27) Alkaissi, H. & McFarlane, S.I. Artificial Hallucinations in ChatGPT: Implications in \nScientific Writing. Cureus J Med Science  15,  (2023). \n(28) Beutel, G., Geerits, E. & Kielstein, J.T. Artificial hallucination: GPT on LSD? Crit Care  \n27, 148 (2023). \n  \nShin et al. \nPage 22 of 26 \nFIGURE LEGENDS \nFigure 1. Topics related to NONMEM identified by ChatGPT and Gemini \nFigure 2. The ChatGPT prompt and the resulting output ChatGPT for Case 1A. The errors and \nomissions in the code are highlighted in rectangles and described in the gold boxes. \nFigure 3 . Results from ChatGPT (left) and Gemini (right) for the linear pharmacokinetic \ncompartmental model with oral absorption. The ChatGPT and Gemini explanation of key \ncomponents is cropped to improve the readability of the figure. The errors and omissions in \nthe code are highlighted in rectangles and described in the gold boxes. \nFigure 4. Results from ChatGPT (left) and Gemini (right) for a one-compartment model with \ntwo parallel absorption pathways and linear elimination. The ChatGPT and Gemini \nexplanation of key components is cropped to improve readability of the figure. The errors \nand omissions in the coded are highlighted in rectangles and described in the gold boxes. \n \n  \nShin et al. \nPage 23 of 26 \nFIGURE 1 \n \n\nShin et al. \nPage 24 of 26 \nFIGURE 2 \n \n\nShin et al. \nPage 25 of 26 \nFIGURE 3 \n \n\nShin et al. \nPage 26 of 26 \nFIGURE 4 \n \n\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nNONMEMGPTSupplementaryFile.docx",
  "topic": "NONMEM",
  "concepts": [
    {
      "name": "NONMEM",
      "score": 0.7156033515930176
    },
    {
      "name": "Computer science",
      "score": 0.44966331124305725
    },
    {
      "name": "Econometrics",
      "score": 0.3672142028808594
    },
    {
      "name": "Population",
      "score": 0.3013259172439575
    },
    {
      "name": "Mathematics",
      "score": 0.25343310832977295
    },
    {
      "name": "Medicine",
      "score": 0.2382965087890625
    },
    {
      "name": "Environmental health",
      "score": 0.05831068754196167
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63190737",
      "name": "University at Buffalo, State University of New York",
      "country": "US"
    }
  ]
}