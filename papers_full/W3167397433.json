{
  "title": "AMR Parsing with Action-Pointer Transformer",
  "url": "https://openalex.org/W3167397433",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2113878559",
      "name": "jiawei Zhou",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2072731134",
      "name": "Tahira Naseem",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2129506641",
      "name": "Ramón Fernandez Astudillo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132510150",
      "name": "Radu Florian",
      "affiliations": [
        "Harvard University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963161698",
    "https://openalex.org/W4293430466",
    "https://openalex.org/W2949555952",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3173677700",
    "https://openalex.org/W2970917831",
    "https://openalex.org/W2917156138",
    "https://openalex.org/W2963679213",
    "https://openalex.org/W3104890489",
    "https://openalex.org/W2987673972",
    "https://openalex.org/W2251071803",
    "https://openalex.org/W2953243654",
    "https://openalex.org/W2962767889",
    "https://openalex.org/W2951963554",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2964164798",
    "https://openalex.org/W3106209546",
    "https://openalex.org/W4294990258",
    "https://openalex.org/W2296308987",
    "https://openalex.org/W3035246734",
    "https://openalex.org/W2250375035",
    "https://openalex.org/W2963712120",
    "https://openalex.org/W2760379373",
    "https://openalex.org/W3023672669",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2758846169",
    "https://openalex.org/W2251823395",
    "https://openalex.org/W2586559132",
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2786103815",
    "https://openalex.org/W2963370123",
    "https://openalex.org/W2619249607",
    "https://openalex.org/W3104681577",
    "https://openalex.org/W2890212927",
    "https://openalex.org/W3093568530",
    "https://openalex.org/W2799072540",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2799190973",
    "https://openalex.org/W2470862100",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2149837184",
    "https://openalex.org/W2971087717",
    "https://openalex.org/W2951391755",
    "https://openalex.org/W3035586188",
    "https://openalex.org/W2964146228",
    "https://openalex.org/W2798347870",
    "https://openalex.org/W2952254971",
    "https://openalex.org/W2962947230",
    "https://openalex.org/W2970796523"
  ],
  "abstract": "Jiawei Zhou, Tahira Naseem, Ramón Fernandez Astudillo, Radu Florian. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5585–5598\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5585\nAMR Parsing with Action-Pointer Transformer\nJiawei Zhou\u0006 Tahira Naseem∗ Ramón Fernandez Astudillo∗ Radu Florian∗\n\u0006Harvard University ∗IBM Research\n\u0006jzhou02@g.harvard.edu ∗{tnaseem,raduf}@us.ibm.com ∗ramon.astudillo@ibm.com\nAbstract\nAbstract Meaning Representation parsing is\na sentence-to-graph prediction task where tar-\nget nodes are not explicitly aligned to sen-\ntence tokens. However, since graph nodes\nare semantically based on one or more sen-\ntence tokens, implicit alignments can be de-\nrived. Transition-based parsers operate over\nthe sentence from left to right, capturing this\ninductive bias via alignments at the cost of\nlimited expressiveness. In this work, we pro-\npose a transition-based system that combines\nhard-attention over sentences with a target-\nside action pointer mechanism to decouple\nsource tokens from node representations and\naddress alignments. We model the transi-\ntions as well as the pointer mechanism through\nstraightforward modiﬁcations within a single\nTransformer architecture. Parser state and\ngraph structure information are efﬁciently en-\ncoded using attention heads. We show that\nour action-pointer approach leads to increased\nexpressiveness and attains large gains (+1.6\npoints) against the best transition-based AMR\nparser in very similar conditions. While using\nno graph re-categorization, our single model\nyields the second best SMATCH score on AMR\n2.0 (81.8), which is further improved to 83.4\nwith silver data and ensemble decoding.\n1 Introduction\nAbstract Meaning Representation (AMR) (Ba-\nnarescu et al., 2013) is a sentence level semantic\nformalism encoding who does what to whomin the\nform of a rooted directed acyclic graph. Nodes rep-\nresent concepts such as entities or predicates which\nare not explicitly aligned to words, and edges repre-\nsent relations such as subject/object (see Figure 1).\nAMR parsing, the task of generating the graph\nfrom a sentence, is nowadays tackled with se-\nquence to sequence models parametrized with neu-\nral networks. There are two broad categories of\nmethods that are highly effective in recent years.\nTransition-based approaches predict a sequence of\nboy\nwant-01\ngo-02 city name\nYorkNew\nARG0\nARG0 ARG4 name\nop1 op2ARG1\nFigure 1: AMR graph expressing the meaning of the\nsentence The boy wants to go to New York.\nactions given the sentence. These actions gener-\nate the graph while processing tokens left-to-right\nthrough the sentence and store intermediate rep-\nresentations in memories such as stack and buffer\n(Wang et al., 2015; Damonte et al., 2016; Balles-\nteros and Al-Onaizan, 2017; Vilares and Gómez-\nRodríguez, 2018; Naseem et al., 2019; Astudillo\net al., 2020; Lee et al., 2020). General graph-based\napproaches, on the other hand, directly predict\nnodes and edges in sequential order from graph\ntraversals such as breath ﬁrst search or depth ﬁrst\nsearch (Zhang et al., 2019a,b; Cai and Lam, 2019,\n2020). While not modeling the local semantic cor-\nrespondence between graph nodes and source to-\nkens, the approaches achieve strong results without\nrestrictions of transition-based approaches, but of-\nten require graph re-categorization, a form of graph\nnormalization, for optimal performance.\nThe strong left-to-right constraint of transition-\nbased parsers provides a form of inductive bias\nthat ﬁts AMR characteristics. AMR nodes are very\noften normalized versions of sentence tokens and\nlocality between words and nodes is frequently\npreserved. The fact that transition-based systems\nfor AMR have alignments as the core of their ex-\nplanatory model also guarantees that they produce\nreliable alignments at decoding time, which are\nuseful for applications utilizing AMR parses. De-\nspite these advantages, transition-based systems\nstill suffer in situations when multiple nodes are\nbest explained as aligned to one sentence token or\nnone. Furthermore, long distance edges in AMR,\ne.g. re-entrancies, require excessive use ofSWAP or\n5586\nI                  offer                                                   a                                           solution                                            to the problem .Tokens:\nActions:\nGraph:\nCOPY       COPY  LA(1:ARG0)  SHIFT   REDUCE   PRED(thing)  RA(2:ARG1)  PRED(solve-01)  RA(6:ARG2-of)\ni offer thing solve-01ARG0 ARG1 ARG2-of\n1 2 3 4 5 6 7 8 9\nFigure 2: Source tokens, target actions and AMR graph for the sentence I offer a solution to the problem(partially\nparsed). The black arrow marks the current token cursor position. The circles contain the action indices (used as\nids), black circles indicate node creating actions. Only these actions are available for edge attachments. Notice\nthat the edge actions (at steps 3, 7 and 9) explicitly refer to past nodes using the id of the action that created the\nnode. The other participant of the edge action is implicitly assumed to be the most recently created graph node.\nequivalent actions, leading to very long action se-\nquences. This in turn affects both a model’s ability\nto learn and its decoding speed.\nIn this work, we propose the Action-Pointer\nTransition (APT) system which combines the ad-\nvantages of both the transition-based approaches\nand more general graph-generation approaches.\nWe focus on predicting an action sequence that\ncan build the graph from a source sentence. The\ncore idea is to put the target action sequence to a\ndual use – as a mechanism for graph generation as\nwell as the representation of the graph itself. In-\nspired by recent progress in pointer-based parsers\n(Ma et al., 2018a; Fernández-González and Gómez-\nRodríguez, 2020), we replace the stack and buffer\nby a cursor that moves from left to right and in-\ntroduce a pointer network (Vinyals et al., 2015)\nas mechanism for edge creation. Unlike previous\nworks, we use the pointer mechanism on the target\nside, pointing to past node generation actions to\ncreate edges. This eliminates the node generation\nand attachment restrictions of previous transition-\nbased parsers. It is also more natural for graph\ngeneration, essentially resembling the generation\nprocess in the graph-based approaches, but keeping\nthe graph and source aligned.\nWe model both the action generation and the\npointer prediction with a single Transformer model\n(Vaswani et al., 2017). We relate target node and\nsource token representations through masking of\ncross-attention mechanism, similar to Astudillo\net al. (2020) but simply with monotonic action-\nsource alignment driven by cursor positions, rather\nthan stack and buffer contents. Finally we also em-\nbed the AMR graph structural information in the\ntarget decoder by re-purposing edge-creating steps,\nand propose a novel step-wise incremental graph\nmessage passing method (Gilmer et al., 2017) en-\nabled by the decoder self-attention mechanism.\nExperiments on AMR 1.0, AMR 2.0, and AMR\n3.0 benchmark datasets show the effectiveness of\nour APT system. We outperform the best transition-\nbased systems while using sensibly shorter action\nsequences, and achieve better performance than all\nprevious approaches with similar size of training\nparameters.\n2 AMR Generation with Action-Pointer\nFigure 2 shows a partially parsed example of a\nsource sentence, a transition action sequence and\nthe AMR graph for the proposed transitions. Given\na source sentence x = x1,x2,...,x S, our transi-\ntion system works by scanning the sentence from\nleft to right using a cursor ct ∈{1,2,...,S }. Cur-\nsor movement is controlled by three actions:\nSHIFT moves cursor one position to the right,\nsuch that ct+1 = ct + 1.\nREDUCE is a special SHIFT indicating that no\naction was performed at current cursor position.\nMERGE merges tokens xct and xct+1 and\nSHIFT s. Merged tokens act as a single token under\nthe position of the last token merged.\nAt cursor position ct we can generate any sub-\ngraph through following actions:\nCOPY creates a node by copying the word under\nxct . Since AMR nodes are often lemmas or prop-\nbank frames, two versions of this action exist to\ncopy the lemma of xct or provide the ﬁrst sense\n(frame −01) constructed from the lemma. This cov-\ners a large portion of the total AMR nodes. It also\nhelps generalize for predictions of unseen nodes.\nWe use an external lemmatizer1 for this action.\n1https://spacy.io/.\n5587\nPRED (LABEL ) creates a node with name LABEL\nfrom the node names seen at train time.\nSUBGRAPH (LABEL ) produces an entire sub-\ngraph indexed by label LABEL . Any future attach-\nments can only be made to the root of the subgraph.\nLA(ID,LABEL ) creates an arc with LABEL from\nlast generated node to a previous node at position\nID. Note that we can only point to past node gener-\nating actions in the action history.\nRA(ID,LABEL ) creates an arc withLABEL to last\ngenerated node from a previous node at positionID.\nUsing the above actions, it is easy to derive an or-\nacle action sequence given gold-graph information\nand initial word to node alignments. For current\ncursor position, all the nodes aligned to it are gener-\nated using SUBGRAPH (), COPY or PRED () actions.\nEach node prediction action is followed by edge\ncreation actions. Edges connecting to closer nodes\nare generated before the farther ones. When multi-\nple connected nodes are aligned to one token, they\nare traversed in pre-order for node generation. A\ndetailed description of oracle algorithm is given in\nAppendix B.\nThe use of a cursor variable ct decouples node\nreference from source tokens, allowing to produce\nmultiple nodes and edges (see Figure 3), even the\nentire AMR graph if necessary, from a single token.\nThis provides more expressiveness and ﬂexibility\nthan previous transition-based AMR parsers, while\nkeeping a strong inductive bias. The only restric-\ntion is that all inbound or outbound edges between\ncurrent node and all previously produced nodes\nneed to be generated before predicting a new node\nor shifting the cursor. This does not limit the oracle\ncoverage, however, for trained parsers, it leads to a\nsmall percentage of disconnected graphs in decod-\ning. Furthermore, nodes within the SUBGRAPH ()\naction can not be reached for edge creation. The\nuse of SUBGRAPH () action, initially introduced in\nBallesteros and Al-Onaizan (2017), is reduced in\nthis work to cases where no such edges are ex-\npected, which is mainly the case for dates and\nnamed-entities.\nCompared to previous oracles (Ballesteros and\nAl-Onaizan, 2017; Naseem et al., 2019; Astudillo\net al., 2020), the action-pointer does not use aSWAP\naction. It can establish an edge between the last\npredicted node and any previous node, since edges\nare created by pointing to decoder representations.\nActionsSentenceGraphCOPY_LEMMAyouropinion matters SHIFTyouropinionmatters PRED(thing)your opinionmattersPRED(opine-01)your opinionmattersRA(3,ARG1-of)your opinionmatters LA(1,ARG0)your opinionmattersSHIFTyour opinionmattersCOPY_SENSE01your opinion mattersLA(3,ARG0)your opinion mattersARG0\n34\n8\n765\n1\n9 ARG1-ofyouthingopine-01matter-01\nARG1-ofthingopine-01matter-01you\nARG1-ofthingopine-01you\nARG1-ofthingopine-01you\nARG1-ofthingopine-01youthingopine-01youthingyou\nyou\nARG0\nARG0\nARG0\nARG0\n2 you\nFigure 3: Step-by-step actions on the sentence your\nopinion matters. Creates subgraph from a single word\n(thing :ARG1-of opine-01) and allows attachment to all\nits nodes. Cursor is at underlined words (post-action).\nThis oracle is expected to work with generic\nAMR aligners. For this work, we use the align-\nments generation method of Astudillo et al. (2020),\nwhich generates many-to-many alignments. It is a\ncombination of Expectation Maximization based\nalignments of Pourdamghani et al. (2014) and rule\nbase alignments of Flanigan et al. (2014). Any re-\nmaining unaligned nodes are aligned based on their\ngraph proximity to unaligned tokens. For more de-\ntails, we refer the reader to the works of Astudillo\net al. (2020) and Naseem et al. (2019).\n3 Action-Pointer Transformer\n3.1 Basic Architecture\nThe backbone of our model is the encoder-decoder\nTransformer (Vaswani et al., 2017), combined with\na pointer network (Vinyals et al., 2015). The prob-\nability of an action sequence y = y1,y2,...,y T\nfor input tokens x = x1,x2,...,x S is given in our\nmodel by\nP(y |x) =\nT∏\nt=1\nP(yt |y<t,x)\n=\nT∏\nt=1\nP(at |a<t,p<t,x) P(pt |a≤t,p<t,x)\n(1)\nwhere at each time step t, we decompose the target\naction yt into the pointer-removed action and the\npointer value with yt = (at,pt). A dummy pointer\npt = nullis ﬁxed for non-edge actions, so that\nP(pt |a≤t,p<t,x) = [P(pt |a<t,p<t,x)]γ(at)\n5588\nwhere γ(at) is an indicator variable set to 0 if at is\nnot an edge action and 1 otherwise.\nGiven a sequence to sequence Transformer\nmodel with N encoder layers and M decoder lay-\ners, each decoder layer is deﬁned by\ndm\nt = FFm(CAm(SAm(dm−1\nt ,dm−1\n≤t ),eN))\nwhere FFm(), CAm() and SAm() are feed-\nforward, multi-head cross-attention and multi-head\nself-attention components respectively2. eN is the\noutput of last encoder layer and dm−1 is the output\nof the previous decoder layer, with d0\n≤t initialized\nto be the embeddings of the action history y<t con-\ncatenated with a special start symbol.\nThe distribution over actions is given by\nP(at |a<t,p<t,x) = softmax\n(\nW ·dM\nt\n)\nat\nwhere W are the output vocabulary embeddings,\nand the edge pointer distribution is given by\nP(pt |a<t,p<t,x) =\nsoftmax\n(\n(KM ·dM−1\n≤t )T ·QM ·dM−1\nt\n)\npt\nwhere KM, QM are key and query matrices of 1\nhead of the last decoder self-attention layerSAM().\nThe top layer self-attention is a natural choice for\nthe pointer network, since it is likely to have high\nvalues for the nodes involved in the edge direction\nand label prediction. Although the edge action and\nits pointing value are both output at the same step,\nthe specialized pointer head is also part of the over-\nall self-attention mechanism used to compute the\nmodel’s hidden representations, thus making ac-\ntions distribution aware of the pointer distribution.\nOur transition system moves the cursor ct over\nthe source from left to right during parsing, essen-\ntially maintaining a monotonic alignment between\ntarget actions and source tokens. We encode the\nalignment ct with hard attentions in cross-attention\nheads CAm() with m= 1···M at every decoder\nlayer. We mask one head of the cross-attention to\nsee only the aligned source token at ct, and aug-\nment it with another head masked to see only po-\nsitions > ct. This is similar to the hard attention\nin Peng et al. (2018) and parser state encoding in\nAstudillo et al. (2020).\nAs in prior works, we restrict the output space of\nour model to only allow valid actions given x,y<t.\nThe restriction is not only enforced at inference, but\n2Each of these are wrapped around with residual, dropout\nand layer normalization operations removed for simplicity.\nis also internalized with the model during training\nso that the model can always focus on relevant\naction subsets when making predictions.\n3.2 Incremental Graph Embedding\nIncrementally generated graphs are usually mod-\neled via graph neural networks (Li et al., 2018),\nwhere a node’s representation is updated from the\ncollection of it’s neighboring nodes’ representa-\ntions by message passing (Gilmer et al., 2017).\nHowever, this requires re-computation of all node\nrepresentations every time the graph is modiﬁed,\nwhich is expensive, prohibiting its use in previous\ngraph-based AMR parsing works (Cai and Lam,\n2020). To better utilize the intermediate topologi-\ncal graph information without losing the efﬁcient\nparallelization of Transformer, we propose to use\nthe edge creation actions as updated views of each\nnode, that encode this node’s neighboring subgraph.\nThis does not change the past computations and\ncan be done by altering the hard masking of the\nself-attention heads of decoder layers SAm() . By\ninterpreting the decoder layers as implementing\nmessage passing vertically, we can fully encode\ngraphs up to depth M.\nGiven a node generating action at = v, it\nis followed by k ≥ 0 edge generating ac-\ntions at+1,at+2,...,a t+k that connect the cur-\nrent node with previous nodes, pointed by\npt+1,pt+2,...,p t+k positions on the target side.\nThis also deﬁnes kgraph modiﬁcations, expanding\nthe graph neighborhood on the current node. Fig-\nure 4 shows an example for the sentence The boy\nwants to go, with node prediction actions at posi-\ntions t= 2,4,8, with kbeing 0, 1, 2, respectively.\nWe use the steps from tto t+ kin the Transformer\ndecoder to encode this expanding neighborhood.\nIn particular, we ﬁx the decoder input as the cur-\nrent node action vfor these steps, as illustrated in\nthe input actions in Figure 4. At each intermediate\nstep τ ∈[t,t + k], 2 decoder self-attention heads\nSAm() are restricted to only attend to the direct\ngraph neighbors of the current node, represented by\nprevious nodes at positionspt,pt+1,··· ,pτ as well\nas the current position τ. This essentially builds\nsub-sequences of node representations with richer\ngraph information step by step, and we use the last\nreference of the same node for pointing positions\nwhen generating new edges. Moreover, when prop-\nagating this masking pattern along mlayers, each\nnode encodes its m-hop neighborhood information.\n5589\n<s>\nIndex:\nREDUCE\n1\nREDUCE\nCOPY\n2\nCOPY\nSHIFT\nboy\n3\nSHIFT\nCOPY\n4\nCOPY\nLA(2)\nwant-01\n5\nCOPY\nSHIFT\nwant-01\n6\nSHIFT\nREDUCE\n7\nREDUCE\nPRED\n8\nRA(5)\nPRED\ngo-02\n9\nLA(2)\nPRED\ngo-02\n10\nSHIFT\nPRED\ngo-02\nFigure 4: Encoding graph with 2 decoder layers for the sentence The boy wants to go. From top to bottom: target\noutput action sequence, masked decoder self-attention, input action history and partial graph. Edge-creating action\nsteps in the action history are used to hold updated node representations. Action labels and edge direction treatment\nare removed for clarity.\nThis deﬁnes a message passing procedure as shown\nin Figure 4, encoding the compositional relations\nbetween nodes. Since the edges have directions in-\ndicated by LA and RA, we also encode the direction\ninformation by separating the two heads with each\nonly considering one direction.\n4 Training and Inference\nOur model is trained by maximizing the log like-\nlihood of Equation (1). The valid action space,\naction-source alignment ct, and the graph embed-\nding mask at each step tare pre-calculated at train-\ning time. For inference, we modify the beam search\nalgorithm to jointly search for actions and edge\npointers and combine them to ﬁnd the action se-\nquence that maximizes Equation (1). We also con-\nsider hard constraints in the searching process such\nas valid output actions and valid target pointing\nvalues at different steps to ensure an AMR graph\nis recoverable. For the structural information that\nis extracted from the parsing state such as ct and\ngraph embedding masks, we compute them on the\nﬂy at each new step of decoding based on the cur-\nrent results, which are then used by the model for\nthe next step decoding. We detail our search algo-\nrithm in Appendix C.\n5 Experimental Setup\nData and Evaluation We test our approach\non two widely used AMR parsing benchmark\ndatasets: AMR 2.0 (LDC2017T10) and AMR\n1.0 (LDC2014T12). The AMR graphs are all\nhuman annotated. The two datasets have 36521\nand 10312 training AMRs, respectively, and share\n1368 development AMRs and 1371 testing AMRs3.\nWe also report results on the latest AMR 3.0\n(LDC2020T02) dataset, which is larger in size but\nhas not been fully explored, with 55635 training\nAMRs and 1722 and 1898 AMRs for development\nand testing set. Wiki links are removed in the pre-\nprocessing of data, and we run a wikiﬁcation ap-\nproach in post-processing to recover Wikipedia en-\ntries in the AMR graphs as in Naseem et al. (2019).\nFor evaluation, we use the SMATCH (F1) scores4\n(Cai and Knight, 2013) and further the ﬁne-grained\nevaluation metrics (Damonte et al., 2016) to assess\nthe model’s AMR parsing performance.\nModel Conﬁguration Our base setup has 6 lay-\ners and 4 attention heads for both the Transformer\nencoder and decoder, with model size 256 and feed-\nforward size 512. We also compare with a small\nmodel with 3 layers in encoder and decoder but\nidentical otherwise. The pointer network is always\ntied with one target self-attention head of the top\ndecoder layer. We use the cross-attention of all\ndecoder layers for action-source alignment. For\ngraph embedding, we use 2 heads of the bottom\n3 layers for the base model and bottom 2 layers\nfor the small model. We use contextualized embed-\ndings extracted from the pre-trained RoBERTa (Liu\net al., 2019) large model for the source sentence,\nwith average of all layer states and BPE tokens\nmapped to words by averaging as in (Lee et al.,\n2020). The pre-trained embeddings are ﬁxed. For\n3Although there are annotation revisions from AMR 1.0\nto AMR 2.0. Link to data: https://amr.isi.edu/download.html.\n4There are small variations of SMATCH computation due\nto the stochastic nature of graph matching algorithm.\n5590\nTransition system Avg. #actions Oracle SMATCH\nNaseem et al. (2019)∗ 73.6 93.3\nAstudillo et al. (2020)∗ 76.2 98.0\nOurs 41.6 98.9\nTable 1: Average number of actions and oracle\nSMATCH on AMR 2.0 training data. The average\nsource length is 18.9. ∗ from author correspondence.\ntarget actions we train our own embeddings along\nwith the model.\nImplementation Details We use the Adam opti-\nmizer with β1 of 0.9 and β2 of 0.98 for training.\nEach data batch has 3584 maximum number of to-\nkens, and the learning rate schedule is the same as\nVaswani et al. (2017), where we use the maximum\nlearning rate of 5e−4 with 4000 warm-up steps.\nWe use a dropout rate of 0.3 and label smoothing\nrate of 0.01. We train all the models for a maxi-\nmum number of 120 epochs, and average the best\n5 epoch checkpoints among the last 40 checkpoints\nbased on the SMATCH scores on the development\ndata with greedy decoding. We use a default beam\nsize of 10 for decoding. We implement our model5\nwith the FAIRSEQ toolkit (Ott et al., 2019). All\nmodels are trained and tested on a single Nvidia\nTitan RTX GPU. Training takes about 10 hours on\nAMR 2.0 and 3.5 hours on AMR 1.0.\n6 Results and Analysis\n6.1 Main Results\nOracle Actions Table 1 compares the oracle data\nSMATCH and average action sequence length on\nthe AMR 2.0 training set among recent transition\nsystems. Our approach yields much shorter action\nsequences due to the target-side pointing mecha-\nnism. It has also the best coverage on training AMR\ngraphs, due to the ﬂexibility of our transitions that\ncan capture the majority of graph components. We\nchose not to tackle a number of small corner cases,\nsuch as disconnected subgraphs for a token, that\naccount for the missing oracle performance.\nParsing Performance We compare our action-\npointer transition/Transformer (APT) model with\nexisting approaches in Table 26. We indicate the\nuse of pre-trained BERT or RoBERTa embeddings\n5Available under https://github.com/IBM/\ntransition-amr-parser.\n6We exclude Xu et al. (2020) AMR 1.0 numbers since they\nreport 16833 train sentences, not 10312.\nCorpus Model S MATCH(%)\nAMR\n1.0\nPust et al. (2015) 67.1\nFlanigan et al. (2016) 66.0\nWang and Xue (2017)G 68.1\nGuo and Lu (2018)G 68.3±0.4\nZhang et al. (2019a)B,G 70.2±0.1\nZhang et al. (2019b)B,G 71.3±0.1\nCai and Lam (2020)B 74.0\nCai and Lam (2020)B,G 75.4\nAstudillo et al. (2020)∗R 76.9±0.1\nLee et al. (2020)R(85K silver) 78.2±0.1\nAPT smallR 78.2 / 78.2±0.0\nAPT baseR 78.5 / 78.3±0.1\nAPT smallRp.e. 79.7\nAPT baseRp.e. 79.8\nAMR\n2.0\nVan Noord and Bos (2017) 71.0\nGroschwitz et al. (2018)G 71.0\nLyu and Titov (2018)G 74.4±0.2\nCai and Lam (2019) 73.2\nLindemann et al. (2019) 75.3 ±0.1\nNaseem et al. (2019)B 75.5\nZhang et al. (2019a)B,G 76.3±0.1\nZhang et al. (2019b)B,G 77.0±0.1\nCai and Lam (2020)B 78.7\nCai and Lam (2020)B,G 80.2\nAstudillo et al. (2020)∗R 80.2±0.0\nBevilacqua et al. (2021)\u0006 83.8\nXu et al. (2020) (4M silver) 80.2\nLee et al. (2020)R(85K silver) 81.3 ±0.0\nBevilacqua et al. (2021)\u0006(200K silver)84.3\nAPT smallR 81.7 / 81.5±0.2\nAPT baseR 81.8 / 81.7±0.1\nAPT smallRp.e. 82.5\nAPT baseRp.e. 82.8\nAPT baseR(70K Silver) 82.8 / 82.6 ±0.1\nAPT baseR(70K Silver) p.e. 83.4\nAMR\n3.0\nLyu et al. (2020) 75.8\nBevilacqua et al. (2021)\u0006 83.0\nAPT baseR 80.4 / 80.3±0.1\nAPT baseRp.e. 81.2\nTable 2: S MATCH scores on AMR 1.0, 2.0, and 3.0\ntest sets. APT is our model. B or R indicates pre-\ntrained BERT or RoBERTa embeddings,G use of graph\nre-categorization, ∗ improved results reported in Lee\net al. (2020). \u0006 denotes concurrent work based on ﬁne-\ntuning pre-trained BART large models. We report the\nbest/average score ±standard deviation over 3 seeds.\np.e. is partial ensemble decoding with 3 seed models.\n(from large models) with B or R, and graph re-\ncategorization with G. Graph re-categorization\n(Lyu and Titov, 2018; Zhang et al., 2019a; Cai\nand Lam, 2020; Bevilacqua et al., 2021) removes\nnode senses and groups certain nodes together such\nas named entities in pre-processing. It reverts these\nback in post-processing with the help of a name\nentity recognizer. We report results over 3 runs for\neach model with different random seeds. Given that\nwe use ﬁxed pre-trained embeddings, it becomes\ncomputationally cheap to build a partial ensemble\n5591\nModel Fixed Extra\nFeatures\nTrained\nParam.\nSMATCH\nAMR 2.0\nZhang et al. (2019a)B,G BERT 66.1ME 76.3\nCai and Lam (2020)B BERT 27.1M 78.7\nCai and Lam (2020)B,G BERT 26.1M 80.2\nAstudillo et al. (2020)R RoBERTa 21.7M 80.2\nXu et al. (2020) (4M silver) - 239.1M 80.2\nBevilacqua et al. (2021)\u0006 - 411.8M 83.8\nAPT smallR RoBERTa 17.5M 81.7\nAPT baseR RoBERTa 21.4M 81.8\nAPT smallRp.e. RoBERTa 52.5M 82.5\nAPT baseRp.e. RoBERTa 64.3M 82.8\nTable 3: Comparison of model parametrization sizes\nand SMATCH scores on AMR 2.0 test set. Model sizes\nof previous works are obtained from their ofﬁcially re-\nleased pre-trained models. E is an estimate by remov-\ning BERT parameters in the released model, where a\nBERT base model is trained together which is different\nfrom the paper description. \u0006 denotes concurrent work\nbased on ﬁne-tuning pre-trained BART large models.\nthat uses the average probability of 3 models from\ndifferent seeds which we denote as p.e.\nWith the exception of the recent BART-based\nmodel Bevilacqua et al. (2021), we outperform all\npreviously published approaches, both with our\nsmall and base models. Our best single-model\nparsing scores are 81.8 on AMR 2.0 and 78.5 on\nAMR 1.0, which improves 1.6 points over the pre-\nvious best model trained only with gold data. Our\nsmall model only trails the base model by a small\nmargin and we achieve high performance on small\nAMR 1.0 dataset, indicating that our approach ben-\neﬁts from having good inductive bias towards the\nproblem so that the learning is efﬁcient. More re-\nmarkably, we even surpass the scores reported in\nLee et al. (2020) combining various self-learning\ntechniques and utilizing 85K extra sentences for\nself-annotation (silver data). For the most recent\nAMR 3.0 dataset, we report our results for future\nreference.\nAdditionally, the partial ensemble decoding\nproves to be simple and effective in boosting\nthe model performance, which consistently brings\nmore than 1 point gain for AMR 1.0 and 2.0. It\nshould be noted that the ensemble decoding is only\n20% slower than a single model.\nWe thus use this ensemble to annotate the 85K\nsentence set used in (Lee et al., 2020). After remov-\ning parses with detached nodes we obtained 70K\nmodel-annotated silver data sentences. Adding\nthese for training regularly, we achieve our best\nscore of 83.4 with ensemble on AMR 2.0.\nModel Size In Table 3, we compare parameter\nsizes of recently published models alongside their\nparsing performances on AMR 2.0. Similar to our\napproach, most models use large pre-trained mod-\nels to extract contextualized embeddings as ﬁxed\nfeatures, with the exception of Xu et al. (2020),\nwhich is a seq-to-seq pre-training approach on large\namount of data, and Bevilacqua et al. (2021), which\ndirectly ﬁne-tunes a seq-to-seq BART large (Lewis\net al., 2019) model.7 Except the large BART model,\nour APT small (3 layers) has the least number of\ntrained parameters yet already surpasses all the pre-\nvious models. This justiﬁes our method is highly\nefﬁcient in learning for AMR parsing. Moreover,\nwith the small parameter size, the partial ensemble\nis an appealing way to improve parsing quality with\nminor decoding overhead. Although more perfor-\nmant, direct ﬁne-tuning of pre-trained seq-to-seq\nmodels such as BART would require prohibitively\nlarge numbers to perform an ensemble.\nFine-grained Results Table 4 shows the ﬁne-\ngrained AMR 2.0 evaluation (Damonte et al., 2016)\nof APT and previous models with comparable train-\nable parameter sizes. Our model achieves the best\nscores among all sub-tasks except negations and\nwikiﬁcation, handled by post-processing on the\nbest performing approach. We obtain large im-\nprovement on edge related sub-tasks including SRL\n(ARG arcs) and Reentrancies, proving the effective-\nness of our target-side pointer mechanism.\n6.2 Analysis\nAblation of Model Components We evaluate\nthe contribution of different components in our\nmodel in Table 5. The top part of the table shows\neffects of 2 major components that utilize parser\nstate information and the graph structural infor-\nmation in the Transformer decoder. The baseline\nmodel is a free Transformer model with pointers\n(row 1), which is greatly increased by including\nthe monotonic action-source alignment via hard\nattention (row 2) on both AMR 1.0 and AMR 2.0\ncorpus, and combining it with the graph embedding\n(row 3) gives further improvements of 0.3 and 0.2\nfor AMR 1.0 and AMR 2.0. This highlights that\ninjecting hard encoded structural information in the\nTransformer decoder greatly helps our problem.\n7Here we focus on trainable parameters for learning\nefﬁciency. For deployment the total number of parame-\nters should be considered, where all the models relying on\nBERT/RoBERTa features would be on the similar level.\n5592\nModel S MATCHUnlabeled No WSD Concepts Named Ent. Negations Wikiﬁcation Reentrancies SRL\nVan Noord and Bos (2017) 71.0 74 72 82 79 62 65 52 66\nGroschwitz et al. (2018)G 71.0 74 72 84 78 57 71 49 64\nLyu and Titov (2018)G 74.4 77.1 75.5 85.9 86.0 58.4 75.7 52.3 69.8\nCai and Lam (2019) 73.2 77.0 74.2 84.4 82.0 62.9 73.2 55.3 66.7\nNaseem et al. (2019)B 75.5 80 76 86 83 67 80 56 72\nZhang et al. (2019a)B,G 76.3 79.0 76.8 84.8 77.9 75.2 85.8 60.0 69.7\nZhang et al. (2019b)B,G 77.0 80 78 86 79 77 86 61 71\nCai and Lam (2020)B,G 80.2 82.8 80.8 88.1 81.1 78.9 86.3 64.6 74.2\nAstudillo et al. (2020)∗R 80.2 84.2 80.7 88.1 87.5 64.5 78.8 70.3 78.2\nAPT smallR 81.7 85.4 82.2 88.9 88.9 67.5 78.7 70.6 80.7\nAPT baseR 81.8 85.5 82.3 88.7 88.5 69.7 78.8 71.1 80.8\nTable 4: Fine-grained F1 scores on the AMR 2.0 test set. B/R and G marks uses of pre-trained BERT/RoBERTa\nembeddings and graph re-categorization processing. ∗ We cite improved results reported in Lee et al. (2020). We\nreport results with our single best model for fair comparison.\nModel Conﬁguration S MATCH(%)\nMono.\nAlignment\nGraph\nembedding\nAMR\n1.0\nAMR\n2.0\n72.2±0.4 77.5±0.2\n\u0013 78.0±0.1 81.5±0.1\n\u0013 \u0013 78.3±0.1 81.7±0.1\nNo subspace restriction 78.0 ±0.1 80.9±0.1\nRoBERTa base embeddings 78.0±0.1 81.3±0.1\nBERT large embeddings 77.7 ±0.1 81.4±0.1\nTable 5: Ablation study of model components. The\nanalysis is with our base model size.\nData oracle\nvariation\nSMATCH(%)\nTrain oracle Model test\nNone 98.9 81.7 ±0.1\nNo subgraph breakdown 97.8 80.6 ±0.1\nCreate farther edges ﬁrst 98.9 81.4 ±0.2\nPost-order subgraph traversal 98.9 81.8 ±0.1\nTable 6: Results of model performance with different\ndata oracles on AMR 2.0 corpus.\nThe bottom part of Table 5 evaluates the con-\ntribution of output space restriction for target and\ninput pre-trained embeddings for source, respec-\ntively. Removing the restriction for target output\nspace i.e. the valid actions, hurts the model perfor-\nmance, as the model may not be able to learn the\nunderlying rules that govern the target sequence\nrestrictions. Switching the RoBERTa large em-\nbeddings to RoBERTa base or BERT large also\nhurts the performance (although score drops are\nonly 0.3 ∼0.6), indicating that the contextual em-\nbeddings from large and better pre-trained models\nbetter equip the parser to capture semantic relations\nin the source sentence.\nEffect of Oracle Setup As our model directly\nlearns from the oracle actions, we study how the\nupstream transition system affects the model per-\nformance by varying transition setups in Table 6.\nWe try three variations of the oracle. In the ﬁrst\nsetup, we measure the impact of breaking down\nSUBGRAPH action into individual node generation\nand attachment actions. We do this by using the\nSUBGRAPH for all cases of multi-node alignments.\nThis degrades the parser performance and oracle\nSMATCH considerably, dropping by absolute 1.1\npoints. This is expected, since SUBGRAPH action\nmakes internal nodes of the subgraph unattachable.\nIn the second setup, we vary the order of edge\ncreation actions. We reverse it so that the edges\nconnecting farther nodes are built ﬁrst. Although\nthis does not affect the oracle score, we observe that\nthe model performance on this oracle drops by 0.3.\nThe reason might be that the easy close-range edge\nbuilding actions become harder when pushed far-\nther, also making easy decisions ﬁrst is less prone\nto error propagation. Finally, we also change the\norder in which the various nodes connected to a\ntoken are created. Instead of generating the nodes\nfrom the root downwards, we perform a post-order\ntraversal, where leaves are generated before parents.\nThis also does not affect oracle score, however it\ngave a minor gain in parser performance.\nEffect of Beam Size Figure 5 shows perfor-\nmance for different beam sizes. Ideally, if the\nmodel is more certain and accurate in making right\npredictions at different steps, the decoding perfor-\nmance should be less impacted by beam size. The\nresults show that performance improves with beam\nsize, but the gains saturate at beam size 3. This in-\ndicates that a smaller beam size can be considered\n5593\n2 4 6 8 10\nbeam size\n81.0\n81.2\n81.4\n81.6\n81.8Smatch (%)\nbase model\nsmall model\nFigure 5: Effect of decoding beam size for S MATCH ,\nwith our best single models on AMR 2.0 test set.\nfor application scenarios with time constraints.\n7 Related Work\nWith the exception of Astudillo et al. (2020), other\nworks introducing stack and buffer information into\nsequence-to-sequence attention parsers (Liu and\nZhang, 2017; Zhang et al., 2017; Buys and Blun-\nsom, 2017), are based on RNNs and do not at-\ntain high performances. Liu and Zhang (2017);\nZhang et al. (2017) tackle dependency parsing\nand propose modiﬁed attention mechanisms while\nBuys and Blunsom (2017) predicts semantic graphs\njointly with their alignments and compares stack-\nbased with latent and ﬁxed alignments. Compared\nto the stack-Transformer (Astudillo et al., 2020),\nwe propose the use of an action pointing mecha-\nnism to decouple word and node representation,\nremove the need for stack and buffer and model\ngraph structure on the decoder side. We show that\nthese improvements yield superior performance\nwhile exploiting the same inductive biases with\nlittle train data or small models.\nVilares and Gómez-Rodríguez (2018) proposed\nan AMR -CONVINGTON system for unrestricted non-\nprojective AMR parsing, comparing the current\nword with all previous words for arc attachment\nas we propose. However, their comparison is done\nwith sequential actions whereas we use an efﬁcient\npointer mechanism to parallelize the process.\nRegarding the use of pointer mechanisms for\narc attachment, Ma et al. (2018b) proposed the\nstack-pointer network to build partial graph repre-\nsentations, and Fernández-González and Gómez-\nRodríguez (2020) adopted pointers along with the\nleft-to-right scan of the sentence, greatly improv-\ning the efﬁciency. Compared with these works, we\ntackle a more general text-to-graph problem, where\nnodes are only loosely related to words, by utilizing\nthe action-pointer mechanism. Our method is also\nable to build up to depth M graph representations\nwith M decoding layers.\nWhile not explicitly stated, graph-based ap-\nproaches (Zhang et al., 2019a; Cai and Lam, 2020)\ngenerate edges with a pointing mechanism, either\nwith a deep biafﬁne classiﬁer (Dozat and Man-\nning, 2018) or with attention (Vaswani et al., 2017).\nThey also model inductive biases indirectly through\ngraph re-categorization, detailed in Section 6.1,\nwhich requires a name entity recognition system\nat test time. Re-categorization was proposed in\nLyu and Titov (2018), which reformulated align-\nments as a differentiable permutation problem, in-\nterpretable as another form of inductive bias.\nFinally, augmenting seq-to-seq models with\ngraph structures has been explored in various NLP\nareas, including machine translation (Hashimoto\nand Tsuruoka, 2017; Moussallem et al., 2019), text\nclassiﬁcation (Lu et al., 2020), AMR to text gener-\nation (Zhu et al., 2019), etc. Most of these works\nmodel graph structure in the encoder since the com-\nplete source sentence and graph are known. We\nembed a dynamic graph in the Transformer decoder\nduring parsing. This is similar to broad graph gener-\nation approaches (Li et al., 2018) relying on graph\nneural networks (Li et al., 2019), but our approach\nis much more efﬁcient as we do not require heavy\nre-computation of node representations.\n8 Conclusion\nWe present an Action-Pointer mechanism that can\nnaturally handle the generation of arbitrary graph\nconstructs, including re-entrancies and multiple\nnodes per token. Our structural modeling with\nincremental encoding of parser and graph states\nbased on a single Transformer architecture proves\nto be highly effective, obtaining the best results\non all AMR corpora among models with similar\nlearnable parameter sizes. An interesting future\nexploration is on combining our system with large\npre-trained models such as BART, as directly ﬁne-\ntuning on the latter shows great potential in boost-\ning the performance (Bevilacqua et al., 2021). Al-\nthough we focus on AMR graphs in this work, our\nsystem can essentially be adopted to any task gen-\nerating graphs from texts where copy mechanisms\nor hard-attention plays a central role.\n5594\nReferences\nRamon Fernandez Astudillo, Miguel Ballesteros,\nTahira Naseem, Austin Blodgett, and Radu Flo-\nrian. 2020. Transition-based parsing with stack-\ntransformers. arXiv preprint arXiv:2010.10669.\nMiguel Ballesteros and Yaser Al-Onaizan. 2017.\nAmr parsing using stack-lstms. arXiv preprint\narXiv:1707.07755.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Grifﬁtt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract meaning representation\nfor sembanking. In Proceedings of the 7th linguistic\nannotation workshop and interoperability with dis-\ncourse, pages 178–186.\nMichele Bevilacqua, Rexhina Blloshmi, and Roberto\nNavigli. 2021. One spring to rule them both: Sym-\nmetric amr semantic parsing and generation without\na complex pipeline.\nJan Buys and Phil Blunsom. 2017. Robust incremen-\ntal neural semantic graph parsing. arXiv preprint\narXiv:1704.07092.\nDeng Cai and Wai Lam. 2019. Core semantic ﬁrst: A\ntop-down approach for amr parsing. arXiv preprint\narXiv:1909.04303.\nDeng Cai and Wai Lam. 2020. Amr parsing via\ngraph-sequence iterative inference. arXiv preprint\narXiv:2004.05572.\nShu Cai and Kevin Knight. 2013. Smatch: an evalua-\ntion metric for semantic feature structures. In Pro-\nceedings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers), pages 748–752.\nMarco Damonte, Shay B Cohen, and Giorgio Satta.\n2016. An incremental parser for abstract meaning\nrepresentation. arXiv preprint arXiv:1608.06111.\nTimothy Dozat and Christopher D Manning. 2018.\nSimpler but more accurate semantic dependency\nparsing. arXiv preprint arXiv:1807.01396.\nDaniel Fernández-González and Carlos Gómez-\nRodríguez. 2020. Transition-based semantic\ndependency parsing with pointer networks. arXiv\npreprint arXiv:2005.13344.\nJeffrey Flanigan, Chris Dyer, Noah A Smith, and\nJaime G Carbonell. 2016. Cmu at semeval-2016\ntask 8: Graph-based amr parsing with inﬁnite ramp\nloss. In Proceedings of the 10th International\nWorkshop on Semantic Evaluation (SemEval-2016),\npages 1202–1206.\nJeffrey Flanigan, Sam Thomson, Jaime G Carbonell,\nChris Dyer, and Noah A Smith. 2014. A discrim-\ninative graph-based parser for the abstract mean-\ning representation. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1426–\n1436.\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley,\nOriol Vinyals, and George E Dahl. 2017. Neu-\nral message passing for quantum chemistry. arXiv\npreprint arXiv:1704.01212.\nJonas Groschwitz, Matthias Lindemann, Meaghan\nFowlie, Mark Johnson, and Alexander Koller. 2018.\nAmr dependency parsing with a typed semantic al-\ngebra. arXiv preprint arXiv:1805.11465.\nZhijiang Guo and Wei Lu. 2018. Better transition-\nbased amr parsing with a reﬁned search space. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages\n1712–1722.\nKazuma Hashimoto and Yoshimasa Tsuruoka. 2017.\nNeural machine translation with source-side latent\ngraph parsing. arXiv preprint arXiv:1702.02265.\nYoung-Suk Lee, Ramon Fernandez Astudillo, Tahira\nNaseem, Revanth Gangi Reddy, Radu Florian,\nand Salim Roukos. 2020. Pushing the limits of\namr parsing with self-learning. arXiv preprint\narXiv:2010.10673.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461.\nMichael Lingzhi Li, Meng Dong, Jiawei Zhou, and\nAlexander M Rush. 2019. A hierarchy of graph neu-\nral networks based on learnable local features.arXiv\npreprint arXiv:1911.05256.\nYujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu,\nand Peter Battaglia. 2018. Learning deep generative\nmodels of graphs. arXiv preprint arXiv:1803.03324.\nMatthias Lindemann, Jonas Groschwitz, and Alexan-\nder Koller. 2019. Compositional semantic\nparsing across graphbanks. arXiv preprint\narXiv:1906.11746.\nJiangming Liu and Yue Zhang. 2017. Encoder-decoder\nshift-reduce syntactic parsing. In Proceedings of\nthe 15th International Conference on Parsing Tech-\nnologies, pages 105–114, Pisa, Italy. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhibin Lu, Pan Du, and Jian-Yun Nie. 2020. Vgcn-bert:\nAugmenting bert with graph embedding for text clas-\nsiﬁcation. In European Conference on Information\nRetrieval, pages 369–382. Springer.\n5595\nChunchuan Lyu, Shay B Cohen, and Ivan Titov. 2020.\nA differentiable relaxation of graph segmentation\nand alignment for amr parsing. arXiv preprint\narXiv:2010.12676.\nChunchuan Lyu and Ivan Titov. 2018. Amr parsing\nas graph prediction with latent alignment. arXiv\npreprint arXiv:1805.05286.\nXuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,\nGraham Neubig, and Eduard Hovy. 2018a. Stack-\npointer networks for dependency parsing. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1403–1414, Melbourne, Australia.\nAssociation for Computational Linguistics.\nXuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng,\nGraham Neubig, and Eduard Hovy. 2018b. Stack-\npointer networks for dependency parsing. arXiv\npreprint arXiv:1805.01087.\nDiego Moussallem, Mihael Ar ˇcan, Axel-\nCyrille Ngonga Ngomo, and Paul Buitelaar. 2019.\nAugmenting neural machine translation with knowl-\nedge graphs. arXiv preprint arXiv:1902.08816.\nTahira Naseem, Abhishek Shah, Hui Wan, Radu\nFlorian, Salim Roukos, and Miguel Ballesteros.\n2019. Rewarding smatch: Transition-based amr\nparsing with reinforcement learning. arXiv preprint\narXiv:1905.13370.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensi-\nble toolkit for sequence modeling. arXiv preprint\narXiv:1904.01038.\nXiaochang Peng, Linfeng Song, Daniel Gildea, and\nGiorgio Satta. 2018. Sequence-to-sequence models\nfor cache transition systems. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1842–1852.\nNima Pourdamghani, Yang Gao, Ulf Hermjakob, and\nKevin Knight. 2014. Aligning english strings with\nabstract meaning representation graphs. In Proceed-\nings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n425–429.\nMichael Pust, Ulf Hermjakob, Kevin Knight, Daniel\nMarcu, and Jonathan May. 2015. Parsing english\ninto abstract meaning representation using syntax-\nbased machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1143–1154.\nRik Van Noord and Johan Bos. 2017. Neural seman-\ntic parsing by character-based translation: Experi-\nments with abstract meaning representations. arXiv\npreprint arXiv:1705.09980.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAshwin K Vijayakumar, Michael Cogswell, Ram-\nprasath R Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. 2016. Diverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models. arXiv preprint arXiv:1610.02424.\nDavid Vilares and Carlos Gómez-Rodríguez. 2018. A\ntransition-based algorithm for unrestricted amr pars-\ning. arXiv preprint arXiv:1805.09007.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in neural in-\nformation processing systems, pages 2692–2700.\nChuan Wang and Nianwen Xue. 2017. Getting the\nmost out of amr parsing. In Proceedings of the\n2017 conference on empirical methods in natural\nlanguage processing, pages 1257–1268.\nChuan Wang, Nianwen Xue, and Sameer Pradhan.\n2015. A transition-based algorithm for amr parsing.\nIn Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 366–375.\nDongqin Xu, Junhui Li, Muhua Zhu, Min Zhang, and\nGuodong Zhou. 2020. Improving amr parsing with\nsequence-to-sequence pre-training. arXiv preprint\narXiv:2010.01771.\nSheng Zhang, Xutai Ma, Kevin Duh, and Ben-\njamin Van Durme. 2019a. Amr parsing as\nsequence-to-graph transduction. arXiv preprint\narXiv:1905.08704.\nSheng Zhang, Xutai Ma, Kevin Duh, and Ben-\njamin Van Durme. 2019b. Broad-coverage se-\nmantic parsing as transduction. arXiv preprint\narXiv:1909.02607.\nZhirui Zhang, Shujie Liu, Mu Li, Ming Zhou, and En-\nhong Chen. 2017. Stack-based multi-layer attention\nfor transition-based dependency parsing. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1677–\n1682, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nJiawei Zhou and Alexander M Rush. 2019. Simple un-\nsupervised summarization by contextual matching.\narXiv preprint arXiv:1907.13337.\nJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min\nZhang, and Guodong Zhou. 2019. Modeling graph\nstructure in transformer for better amr-to-text gener-\nation. arXiv preprint arXiv:1909.00136.\n5596\nA A More Detailed Example of\nAction-Pointer Transitions\nWe present a step-by-step walk-through of our ac-\ntions on a less trivial example for generating the\nAMR in Figure 6. The sentence contains a named\nentity which also demonstrates the MERGE and\nSUBGRAPH usage of our transition system.\nB Action-Pointer Oracle\nFor a given sentence, at every oracle step, apply the\nactions in the order listed below. Continue until the\nsource cursor moves past the last token.\n1. If cursor is on a non-ﬁnal token of a span\naligned to a node, keep moving cursor (and\ncollecting tokens) with MERGE till it reaches\nthe ﬁnal token of the span.\n2. If there is a matching pattern for current to-\nken(s) in SUBGRAPH () action dictionary:\n• Apply matching SUBGRAPH () action.\n• Generate edges between the root of the\nsub-graph and past nodes through LA(),\nRA(). Generate closer edges ﬁrst.\nOtherwise, for all nodes aligned to the current\ntoken, in top-down order:\n• Generate node through COPY (lemma\nor ﬁrst sense), and if not possible then\nthrough PRED ().\n• Generate edges between the last nodes\nand past nodes through LA(), RA(). Gen-\nerate closer edges ﬁrst.\n3. If no action performed at step 2, move cursor\nwith REDUCE otherwise, move cursor with\nSHIFT .\nC Action-Pointer Decoding\nWe outline the decoding algorithm for our model\nin Algorithm 1, to combine the actions with point-\ners, as well as taking in parsing states and graph\nstructures for the model during the decoding steps.\nDetailed beam search process is ignored. Although\ntacking the speciﬁc problem of AMR graph genera-\ntion with pointers, our constraint decoding process\nis a modiﬁed beam search algorithm with different\ncomponents and step-wise controls, among others\n(Vijayakumar et al., 2016; Zhou and Rush, 2019).\nD Number of Parameters\nOur model is a single Transformer (Vaswani et al.,\n2017) model. The pointer distribution, action-\nsource alignment encoding from parsing state, and\nstructural graph embedding are all contained in cer-\ntain attention layers and heads, without introducing\nany extra parameters on original Transformer. We\nﬁx our model size and all the embedding size to\nbe 256, and the feedforward hidden size in Trans-\nformer as 512. And they are the same for our base\nmodel with 6 layers and 4 heads and our small\nmodel with 3 layers and 4 heads, both for encoder\nand decoder.\nWe use pre-trained RoBERTa embeddings for\nthe source token embeddings. The embeddings\nare extracted in pre-processing and ﬁxed. The\nRoBERTa model parameters are ﬁxed and not\ntrained with our model. We have a projection layer\nto project the RoBERTa embedding size 1024/768\nto our model size 256.\nThe target side dictionary is built from all the\noracle actions without pointers on training data.\nThe dictionary size for AMR 1.0 is 4640, for AMR\n2.0 is 9288, and for AMR 3.0 is 11680. We build\nthe target action embeddings along with the model\nfor the action prediction on top of Transformer\ndecoder. The dictionary embedding size is ﬁxed at\n256.\nOverall, the total number of parameters for our\n6 layer base model is 14,852,096 on AMR 1.0,\n21,438,464 on AMR 2.0, and 25,550,848 on AMR\n3.0 (difference is in target dictionary embedding\nsize). The total number of parameter for our 3\nlayer small model is 10,898,432 for AMR 1.0 and\n17,484,800 on AMR 2.0 (difference is in target\ndictionary embedding size).\n5597\n0COPY_LEMMAOnlyMao Zedong thought can save the nation1SHIFT Only MaoZedong thought can save the nation2MERGEOnly Mao Zedongthought can save the nation3SUBGRAPH(person,name) Only Mao Zedong thought can save the nation4SHIFT Only Mao Zedong thoughtcan save the nation5PRED(thing)Only Mao Zedong thoughtcan save the nation6LA(0,mod)Only Mao Zedong thoughtcan save the nation7PRED(think-01) Only Mao Zedong thoughtcan save the nation8RA(5,ARG1-of) Only Mao Zedong thoughtcan save the nation\n9LA(3,ARG0)Only Mao Zedong thoughtcan save the nation\n10SHIFT Only Mao Zedong thought cansave the nation\n11PRED(possible-01) Only Mao Zedong thought cansave the nation\n12SHIFT Only Mao Zedong thought cansavethe nation\n13PRED(save-02) Only Mao Zedong thought can savethe nation\n14RA(11,ARG1)Only Mao Zedong thought can savethe nation\n15LA(5,ARG0)Only Mao Zedong thought can savethe nation\n16SHIFT Only Mao Zedong thought can save thenation\n17REDUCEOnly Mao Zedong thought can save the nation\n18COPY_LEMMAOnly Mao Zedong thought can save the nation\n19RA(13,ARG1)Only Mao Zedong thought can save the nation\nonlyonlyonlyonlypersononlypersononlypersonthingonlypersonthingmodonlypersonthingthink-01mod\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01save-02ARG1ARG0 nation\nonlypersonthingthink-01mod ARG1-ofonlypersonthingthink-01mod ARG1-ofARG0\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01save-02\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01\nonlypersonthingthink-01mod ARG1-ofARG0\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01save-02ARG1onlypersonthingthink-01mod ARG1-ofARG0 possible-01save-02ARG1ARG0\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01save-02ARG1ARG0\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01save-02ARG1ARG0\nonlypersonthingthink-01mod ARG1-ofARG0 possible-01save-02ARG1ARG0 nation\nARG1\nFigure 6: Step-by-step actions based on our action-pointer transition system. We illustrate the use of MERGE and\nSUBGRAPH with the named entity of a person’s name in this example. The source cursor after the action is applied\nis pointing at words underlined, and the partially built graph is shown in the right-most column.\n5598\nAlgorithm 1: Constrained beam search for\naction-pointer decoding\nInput: Initial token a0 =</s>, beam size k,\nmax step Tmax, action dictionary D\nwithout pointers, model M that\noutputs both distribution over Dand\nthe pointer distribution from\nself-attention\nOutput: Decoded results\ny1 = (a1,p1),y2 =\n(a2,p2),··· ,yT = (aT,pT)\ninitialization: step t= 1, kstate machines\nwhile t<= Tmax do\n1) Get the valid action dictionary\nDt ⊂D, previous node action\npositions Nt ⊂{0,1,2,...,t },\ncurrent token cursor ct, and current\ngraph Gt (all from the corresponding\nstate machines);\n2) Input preﬁx a0,a1,··· ,at−1 and\nDt,ct,Gt into model, get output\ndistribution P(at|y<t), and the\nself-attention distribution Q(p) from\npointer head with pover {0,1,...,t };\n3) Take the most likely valid pointer\nvalue, with p∗= argmaxp∈Nt Q(p),\nand its score q∗= maxp∈Nt Q(p);\nfor each possible actionafrom Ddo\nif ais an edge actionthen\ncombine the action probability\nwith pointer probability\nP(yt) =P(at|y<t) ·q∗, with\nyt = (a,p∗)\nelse\nset P(yt) =P(at|y<t), with\nyt = (a,null)\nend\nend\nDo beam search with P(yt) over yt to\nget kdecoded results;\nApply the corresponding actions with\nthe kstate machines to update parser\nstates and partial graphs for each beam\ncandidate.\nend",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.7750444412231445
    },
    {
      "name": "Computer science",
      "score": 0.6823120713233948
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.6629922986030579
    },
    {
      "name": "Transformer",
      "score": 0.5561408996582031
    },
    {
      "name": "Natural language processing",
      "score": 0.4405898451805115
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4005482792854309
    },
    {
      "name": "Programming language",
      "score": 0.3774080276489258
    },
    {
      "name": "Engineering",
      "score": 0.17365163564682007
    },
    {
      "name": "Electrical engineering",
      "score": 0.1575414538383484
    },
    {
      "name": "Voltage",
      "score": 0.048243969678878784
    }
  ]
}