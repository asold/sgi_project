{
  "title": "Diversity and language technology: how language modeling bias causes epistemic injustice",
  "url": "https://openalex.org/W4391282676",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2476140647",
      "name": "Paula Helm",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2295317697",
      "name": "Gabor Bella",
      "affiliations": [
        "IMT Atlantique",
        "Laboratoire des Sciences et Techniques de l’Information de la Communication et de la Connaissance",
        "Université de Bretagne Occidentale",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2123356487",
      "name": "Gertraud Koch",
      "affiliations": [
        "Universität Hamburg"
      ]
    },
    {
      "id": "https://openalex.org/A164943294",
      "name": "Fausto Giunchiglia",
      "affiliations": [
        "University of Trento"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1555354714",
    "https://openalex.org/W4283269670",
    "https://openalex.org/W6697477984",
    "https://openalex.org/W3216577334",
    "https://openalex.org/W6638208828",
    "https://openalex.org/W6794977174",
    "https://openalex.org/W3167873515",
    "https://openalex.org/W2507975203",
    "https://openalex.org/W4285275721",
    "https://openalex.org/W6778661971",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2996844929",
    "https://openalex.org/W3112849432",
    "https://openalex.org/W4285123703",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W4323051415",
    "https://openalex.org/W4242071405",
    "https://openalex.org/W2909212904",
    "https://openalex.org/W2989361680",
    "https://openalex.org/W6632595698",
    "https://openalex.org/W2006447892",
    "https://openalex.org/W4298286109",
    "https://openalex.org/W2740960106",
    "https://openalex.org/W7023606580",
    "https://openalex.org/W4323851652",
    "https://openalex.org/W2504995794",
    "https://openalex.org/W2318516124",
    "https://openalex.org/W3146083582",
    "https://openalex.org/W2007619812",
    "https://openalex.org/W4288096731",
    "https://openalex.org/W4388265233",
    "https://openalex.org/W3172917028",
    "https://openalex.org/W3196248941",
    "https://openalex.org/W2130645129",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W4388832261",
    "https://openalex.org/W1964045210",
    "https://openalex.org/W4285251426",
    "https://openalex.org/W593705254",
    "https://openalex.org/W1509982784",
    "https://openalex.org/W4244736247",
    "https://openalex.org/W2997585375",
    "https://openalex.org/W2620652927",
    "https://openalex.org/W2728235978",
    "https://openalex.org/W6621507322",
    "https://openalex.org/W6685225694",
    "https://openalex.org/W2791285506",
    "https://openalex.org/W4285273714",
    "https://openalex.org/W3103585759",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3160317075",
    "https://openalex.org/W6632884790",
    "https://openalex.org/W2212352435",
    "https://openalex.org/W2036333904",
    "https://openalex.org/W2029150930",
    "https://openalex.org/W3155618984",
    "https://openalex.org/W3173660000",
    "https://openalex.org/W2034609093",
    "https://openalex.org/W6650455210",
    "https://openalex.org/W4223485168",
    "https://openalex.org/W4385571864",
    "https://openalex.org/W2038721957",
    "https://openalex.org/W4205756177",
    "https://openalex.org/W4285664076",
    "https://openalex.org/W2139419483",
    "https://openalex.org/W3045025511"
  ],
  "abstract": "Abstract It is well known that AI-based language technology—large language models, machine translation systems, multilingual dictionaries, and corpora—is currently limited to three percent of the world’s most widely spoken, financially and politically backed languages. In response, recent efforts have sought to address the “digital language divide” by extending the reach of large language models to “underserved languages.” We show how some of these efforts tend to produce flawed solutions that adhere to a hard-wired representational preference for certain languages, which we call language modeling bias. Language modeling bias is a specific and under-studied form of linguistic bias were language technology by design favors certain languages, dialects, or sociolects with respect to others. We show that language modeling bias can result in systems that, while being precise regarding languages and cultures of dominant powers, are limited in the expression of socio-culturally relevant notions of other communities. We further argue that at the root of this problem lies a systematic tendency of technology developer communities to apply a simplistic understanding of diversity which does not do justice to the more profound differences that languages, and ultimately the communities that speak them, embody. Drawing on the concept of epistemic injustice, we point to the broader ethico-political implications and show how it can lead not only to a disregard for valuable aspects of diversity but also to an under-representation of the needs of marginalized language communities. Finally, we present an alternative socio-technical approach that is designed to tackle some of the analyzed problems.",
  "full_text": "Vol.:(0123456789)\nEthics and Information Technology (2024) 26:8 \nhttps://doi.org/10.1007/s10676-023-09742-6\nORIGINAL PAPER\nDiversity and language technology:  \nhow language modeling bias causes epistemic injustice\nPaula Helm1  · Gábor Bella2 · Gertraud Koch3 · Fausto Giunchiglia4\nAccepted: 13 December 2023 / Published online: 27 January 2024 \n© The Author(s) 2024\nAbstract\nIt is well known that AI-based language technology—large language models, machine translation systems, multilingual \ndictionaries, and corpora—is currently limited to three percent of the world’s most widely spoken, financially and politically \nbacked languages. In response, recent efforts have sought to address the “digital language divide” by extending the reach of \nlarge language models to “underserved languages.” We show how some of these efforts tend to produce flawed solutions that \nadhere to a hard-wired representational preference for certain languages, which we call language modeling bias. Language \nmodeling bias is a specific and under-studied form of linguistic bias were language technology by design favors certain \nlanguages, dialects, or sociolects with respect to others. We show that language modeling bias can result in systems that, \nwhile being precise regarding languages and cultures of dominant powers, are limited in the expression of socio-culturally \nrelevant notions of other communities. We further argue that at the root of this problem lies a systematic tendency of technol-\nogy developer communities to apply a simplistic understanding of diversity which does not do justice to the more profound \ndifferences that languages, and ultimately the communities that speak them, embody. Drawing on the concept of epistemic \ninjustice, we point to the broader ethico-political implications and show how it can lead not only to a disregard for valuable \naspects of diversity but also to an under-representation of the needs of marginalized language communities. Finally, we \npresent an alternative socio-technical approach that is designed to tackle some of the analyzed problems.\nKeywords Language technology · Diversity · Digital divide · Epistemic injustice · Linguistic bias · Language modeling \nbias · Lexical gaps · Large language models\nIntroduction\nAt the latest since the release of products such as DeepL \nor ChatGPT, AI-supported language technologies are well \non their way to becoming mainstream and thus an integral \npart of everyday communication and work routines. As such, \nthey shape social relationships and influence processes of \nknowledge production and proliferation. Following science \nand technology scholar Langdon Winner, language technolo-\ngies can be defined as inherently political because they war-\nrant processes of profound social change (Winner, 1988). \nGiven that language technologies are both sociotechnical \nand inherently political, it is important to ask how they privi-\nlege certain points of view and how their specific design is \ninfluenced by the interests and idea(l)s of certain groups \nof people. From an ethical point of view, LT applications \nthus require appropriate reflection of their inherent biases \nto prevent discriminatory consequences for marginalized \ngroups of people.\nPaula Helm and Gábor Bella have contributed equally to this paper.\n * Paula Helm \n p.m.helm@uva.nl\n Gábor Bella \n gabor.bella@imt-atlantique.fr\n Gertraud Koch \n gertraud.koch@uni-hamburg.de\n Fausto Giunchiglia \n fausto.giunchiglia@unitn.it\n1 University of Amsterdam, Amsterdam, The Netherlands\n2 Lab-STICC CNRS UMR 628, IMT Atlantique, Brest, France\n3 University of Hamburg, Hamburg, Germany\n4 University of Trento, Trento, Italy\n P . Helm et al.\n8 Page 2 of 15\nWhen it comes to questions of linguistic bias in NLP, \nHovy and Prabhumoye (2021) identify five sources of bias: \n(1) the data,(2) the annotation process, (3) the models, (4) \nthe input representation process and (5) the research design \nfor studying bias. Following up on this, Blodgett et al. (2020) \ndevelop three recommendations for further research: (a) \nbetter consideration of social hierarchies and the language \nideologies created or transported through the systems, (b) \nthe specification of the normative dimensions applied in the \nanalysis with respect to what is harmful and what benefi -\ncial when systems are applied, and (c) value-sensitive and \ncommunity oriented perspectives of NLP systems in use in \nlanguage practice. While we take these recommendations as \na starting point, we also acknowledge two limitations: one is \nthe focus on linguistic dimensions, which does not pay suf-\nficient attention to the biases in the design of the respective \ntechnologies and the methodologies behind them, and the \nother is the lack of a more explicit discussion of the ethico-\npolitical dimensions of the problem.\nWith this paper, we aim to take a first step towards fill-\ning these two gaps by extending the understanding of bias \nin NLP to another dimension of bias at the technological \nlevel: hard-wired, but mostly unintentional by-design pref-\nerences for certain languages. We further point out the dis-\ncriminatory consequences caused by what we term language \nmodeling bias. These are, we admit, difficult to pin down, \nperhaps even more so than is the case with racial or sexist \nbiases in NLP (Bender et al., 2021). This is because the type \nof bias we are pointing at unfolds its problematic effects not \nprimarily at the individual but at the systemic level. Never -\ntheless, it is important to pay attention to it because it has \nfar-reaching effects on the equal opportunities of different \nlanguage communities in terms of self-representation, epis-\ntemic self-determination, and communicative participation \n(Nyabola, 2018).\nApart from the research around linguistic bias in NLP, \nrecent debates on what has been coined digital language \ndivide are also central when dealing with language mod-\neling bias (Zaug et al., 2022; Young, 2015). Digital lan-\nguage divide refers to the gap between languages with and \nwithout a considerable representation within the worldwide \ndigital infrastructure. As shown by Kornai (2013) about \n10 years ago, less than 5% of the world’s 7–8000 languages \nhave a remotely significant representation on the Internet, \nand despite the progresses of a decade, the gap has barely \nshrunk (Joshi et al., 2020). The political dimension of the \ndivide is most evident when reconstructing the argument of \nsize, which of course matters in the rapid upscaling of digi-\ntal support for certain languages, but is at once a result of \nimperialist politics and far from the only determining factor \nin digital support. Consider, for example, that the number \nof Wikipedia pages for Kiswahili, one of the major African \nlanguages with about 80 million speakers, is as high as for \nBreton, an endangered Celtic language in western France \nwith about 200 thousand speakers (according to optimistic \nestimates).1  The former, although widely spoken receives \nlittle support and if so, mostly top-down, while the latter \nbenefits from culture preservation programs.\nFor many members of language communities such as \nKiswahili, digital representation is an urgent project (Nyab-\nola, 2018). Following this demand, in the field of language \ntechnology, riding the wave of the recent breakthrough of \nneural AI, the last decade saw a surge in multilingual lan -\nguage tools and resources for ‘under-resourced languages.’ \nResearchers have tried to enable technologies such as \nmachine translation, natural language processing, or speech \nrecognition, to an ever larger scope of languages. However, \nmany such efforts are based on a vision according to which, \nwith the help of AI, already successfully developed and \napplied methods and systems that are designed and sought \nof from an anglo-centric culture of technology develop -\nment, are one-to-one adopted to other contexts (Bird, 2020; \nSchwartz, 2022). This approach to bridging the divide leads \nto a misalignment between the interests and solutions of \nthe former and the living realities of the latter (Helm et al., \n2023). Worse, due to general ignorance of the more profound \ndimensions of linguistic diversity and ultimately the cultural \ndifferences that meaningful diversity embodies, major qual-\nity problems in the results are neglected, which, as we will \nshow, can result in far-reaching forms of westernized cul-\ntural homogenization and epistemic injustice (Spivak, 1988).\nFor these reasons, in this paper, we do not echo the call \nfor bridging the divide by simply digitizing and integrating \nall the world’s languages into existing large-scale techno-\nlogical infrastructures. Instead, what we are concerned with, \nis the structural inequalities expressed in the disparity of lin-\nguistic representations, the causes and consequences of that \ninequality, and the question how it can be addressed in ways \nthat do not end up contributing to neocolonial dynamics.  \nThe first of our paper’s three contributions is thus to define \nand outline in more detail the phenomenon of language mod-\neling bias. A resource or tool exhibits language modeling \nbias if, by design, it is not capable of adequately representing \nor processing certain languages while it is for others. As we \nshow with several cases of lexical gaps, language modeling \nbias is closely related to a second key concept, linguistic \ndiversity, which refers to linguistic constructs and ultimately \nideas that are difficult to translate into certain languages. \nWe argue that if we are to do justice to the more profound \ndimensions of diversity expressed in different languages and \nprevent epistemic injustice by means of language modeling \n1 According to https:// meta. wikim edia. org/ wiki/ List_ of_ Wikip edias, \nretrieved on 1 May 2023.\nDiversity and language technology: how language modeling bias causes epistemic injustice  \n Page 3 of 15 8\nbias, we need to pay attention to precisely these constructs \nand the cultural particularities they reveal.\nIn our second contribution, we detail how the causes of \nlanguage modeling bias are in part a consequence of the \nflawed methods by which language technology is currently \ndeveloped. In doing so, we refer primarily to the academic \napparatus of knowledge and technology production, but also \noutline how this apparatus is intertwined with the private \nsector and its interests. To support our claim, we analyze \nhow many databases and language processing systems that \nare purportedly multilingual have been developed from \nthe perspective of a single language (English). Without a \ncomprehensive understanding of linguistic diversity that \ngoes beyond simply representing another language in a pre-\nexisting model, they run the risk of only superficially filling \na language gap, while on closer inspection being ineffective \nat supporting the values that closing the gap is intended to \npromote.\nOur third contribution addresses the ethico-political \nimplications of our analysis. We show how simplistic repre-\nsentations of diversity can lead to inevitably false represen-\ntations of particular languages, which, when they penetrate \npreviously under-served communities, can lead to dialectic \ndynamics by perpetuating existing or new forms of epistemic \ninjustice, which we outline below in more detail. As an alter-\nnative, we make a case for a language resource development \ninitiative we call LiveLanguage, which is grounded on rigor-\nous co-design, thereby reflecting, supporting and accounting \nfor diversity in a much more principled and systemic manner \nthan any top-down approach can (Saad-Sulonen et al., 2018; \nSmith et al., 2021).\nIn line with these contributions, the paper is organized \nas follows. In Sect. “Diversity as an ethical norm”, we \ndefine and discuss the notion of (linguistic) diversity as \nwell as epistemic injustice, respectively. Section “Bias in \nlanguage technology” is devoted to the definition of (lan -\nguage modeling) bias, the various forms it can take and its \ncauses. Section “Ethical concerns with biases in language \ntechnology” tackles the normative consequences that follow \nfrom the language modeling bias we identified. Finally, in \nSect. “Addressing epistemic injustice in language technol-\nogy: the livelanguage initiative”, we discuss approaches to \nco- and participatory design in language technology and \nclarify some of the conditions that we see need to be fulfilled \nin order to avoid ethical harms.\nDiversity as an ethical norm\nAlthough our point of departure is the normative one of \nprotecting, promoting, and preserving diversity for the sake \nof epistemic justice, we are wary of the problems that come \nwith naively celebrating it without proper conceptualization \n(Helm et al., 2022). Acknowledging that diversity is a moral-\nepistemic hybrid (Potthast, 2014), we differentiate between \nan understanding of it as a descriptive and a normative con-\ncept, to better distinguish between (a) the actual notions \nof difference that underlie our understanding of linguistic \ndiversity as a design strategy, and (b) the values we associate \nwith it as the objective of our work.\nDiversity: a moral–epistemic hybrid\nA closer look at the ethics policies of large tech compa -\nnies reveals that while diversity is regularly listed as a \ncore corporate value, it is often reduced to simplistic but \neasily measurable categories such as gender, race, or age. \nRuha Benjamin aptly described this as “cosmetic diversity” \n(Benjamin, 2019, p. 24). Cosmetic diversity is problematic \nfor several reasons. First, because it clouds our eyes to the \nambiguity of diversity as an instrumental and thus condi-\ntional value. Second, because such portrayals often lead to a \ntreatment of diversity as a resource that can be “exploited.” \nPolitical philosopher Iris Young, however, warned already \nin the 1980 s against such capitalist appropriations of the \nconcept, where diversity is instrumentalized as something \nthat “enriches me” or as a means of optimally valorizing \npeople. Instead, diversity is about how we can live together \nin an inclusive, participatory, and nondiscriminatory way \n(Young, 1990).\nTo clarify this difference, anthropologist Anna Tsing \nspeaks of “meaningful diversity,” that is, diversity that \nchanges things, as opposed to scalable diversity, which \naccepts only what can be incorporated into pre-existing \nstandards without further adaptation (Tsing, 2012). Tsing’s \ndistinction between meaningful and scalable diversity is \ninstructive here, as it highlights exactly the difference that \nwe want to point out when criticising current attempts to \nincrease linguistic diversity in language technology, in ways \nthat simply extend systems already in place. These attempts, \nwe will show, fail to account for the more profound cultural \nand epistemological differences, which are incorporated \nwithin different languages and which, as we claim, should \nbe at the heart of diversification efforts. This, however, \nrequires much more profound adoptions all the way through \nthe methodological, modeling, design, and implementation \ncircle.\nLinguistic diversity\nAs a design strategy, diversity helps define differences \nbetween entities, such as languages, and point out their \nunique features (e.g. words or expressions that cannot be \ntranslated easily into other languages, notions that only make \nsense to specific speaker communities). The terms language \ndiversity and linguistic diversity are often used to refer to the \n P . Helm et al.\n8 Page 4 of 15\nover seven thousand languages existing in the world, and to \nthe wide-ranging differences among them (Giunchiglia et al., \n2018). The association of diversity to language implies the \npreservation of the variedness of the world’s linguistic land-\nscape. In the field of linguistics, diversity is not a technical \nterm and is therefore usually used in an informal way, with a \nfew notable exceptions. Greenberg (1956) defined linguistic \ndiversity as the probability of two persons speaking the same \nlanguage in a certain geographic area. Rijkhoff et al. (1993), \ninstead, apply the term (informally) to sets of languages, and \nunderstands the ‘variedness’ of the languages in terms of \ntheir genetic relationships.\nWith the aim of assessing instances of language technol-\nogy in terms of their representation of linguistic diversity \n(or the lack thereof), we draw on the previous distinction \nbetween meaningful and scalable diversity and relate it to \nlanguage technology. This helps to critically scrutinize exist-\ning attempts at closing the digital language divide: whether \na given language technology does justice to the normative \ndimensions of diversity, representing the wide-ranging \nsemantic and grammatical specificity’s of the languages to \nwhich it is applied.\nA technology can be qualified as doing justice to lin-\nguistic diversity in a meaningful way if it is able to \nprocess and represent different linguistic means avail-\nable in different languages even when the most well \nrepresented languages do not provide an equivalent \nmeans and thus can only indirectly or approximately \nexpress the idea.\nThe most straightforward examples of linguistic diversity \nare found in lexical semantics, in relation to the well-known \nphenomenon of untranslatability. One example from the \ndomain of kinship (the diversity of which is well docu-\nmented) is the Maori word teina: it means elder brother  \nif it is pronounced by a male speaker, and elder sister if it \nis pronounced by a female. In translation to English, this \nconcept can only be expressed in an approximate way. \nAnother example is the phenomenon of inalienable posses-\nsion, widely present in Native American and Australasian \nlanguages, where abstract—yet for us natural—concepts \nsuch as mother or head (as a body part) cannot be expressed \nas single words (free morphemes), but only together with \ntheir possessor (i.e. as the combination of two bound mor -\nphemes): my mother, your head.\nMotivating our normative stance on the importance of \nproperly dealing with diversity when building or expanding \nlanguage technology, we claim that, for native speakers, such \nlanguage-specific terms are often inextricably embedded in \nthe local context. For a speaker in South India, choosing \nthe correct term out of 16 possible terms to designate one’s \ncousin—depending on gender, age, the mother’s or father’s \nside, etc.—is a basic requirement of politeness and culture, \nwhile in other languages, there is only one single term exist-\ning for cousin. Although kinship is a prime example of lin-\nguistic diversity, it can also be reflecting of geographical \nspecifics of particular regions. For example, in the Italian \nAlps, the word malga, designating a typical mountain res-\ntaurant with no equivalent outside the Alpine region, is an \nimportant everyday term with a strong connection to south \nalpine tradition and culture.\nFrom the perspective of computational linguists and engi-\nneers, in contrast, diversity represents a boundary beyond \nwhich algorithms do not scale. Given the persistent and \nincreasing scaling pressures in the field, which we will out-\nline in more detail in the next section, it is a well-understood \ntemptation to simply ignore such long-tail phenomena and \nconcentrate on the more high-level representation as an easy \nway to achieve scalable diversity. Yet, it is not always impos-\nsible to reconcile the engineer’s inclination for automation \nwith an accurate computational representation of linguistic \ndiversity. One solution is to rely on the vast scientific data \non linguistic typology produced by experts through the last \ncentury. Giunchiglia et al. (2017) used a quantified measure \nof the diversity of sets of languages for the prediction of the \nuniversality or specificity of linguistic phenomena. Khishig-\nsuren et al. (2022 ) used results from in-depth, local field \nstudies to better understand the meaning of family relations \nin order to produce accurate kinship terminologies in no less \nthan 600 languages. In Bella et al. (2020), an about 10-thou-\nsand-word formal lexicon of Scottish Gaelic was co-created \nby local language experts, including locally specific terms \nnot directly translatable to English or most other languages.\nThese examples show that the technical representation \nof meaningful linguistic diversity is not only a question of \nfeasibility, but also one of normative orientation and the \nrelated priorizations leading to an intensified investment in \nengagements with local communities and co-creation efforts.\nEpistemic (in)justice\nWe have already pointed out the importance of rigorous con-\nceptual work for the meaning we attach to the normative \nconcepts that guide our efforts. It is equally important to \nclarify what is lost or which kinds of harms are done when \nthese norms are violated, that is, when diversity is simpli-\nfied in such a way that its normative dimensions are eroded. \nIn the introduction, we used the term “epistemic injustice” \nbecause it not only describes well the homogenization that \ncan result from loss of diversity, but also situates that loss \nand the attached harms within a broader context of global \ninequalities.\nThe term ”epistemic injustice” was introduced by phi-\nlosopher Miranda Fricker (2009) and refers to a typology of \ninjustice that is distinct from the injustice caused by the ineq-\nuitable distribution of epistemic goods, such as educational \nDiversity and language technology: how language modeling bias causes epistemic injustice  \n Page 5 of 15 8\nmaterials, books, or information technologies. It is there-\nfore very useful in accurately understanding and naming \nthe problems that arise when language modeling bias per -\nsists despite, or because of, the broad extension of language \ntechnologies to a variety of languages and communities. \nRather than focusing on the issue of distribution of resources \n(Goldman, 2002), epistemic injustice, as understood here, \naddresses the harms that occur at a more subtle level when \npeople are unequally valued in their capacity as bearers and \npractitioners of different forms of knowledge (Coady, 2010). \nAccording to Fricker’s analysis, the most important forms \nof epistemic injustice include forms of exclusion and silenc-\ning, the systematic distortion or misrepresentation of certain \npeople’s meanings or contributions, and the undervaluing of \ntheir status in communicative practices.\nEpistemic injustice also has a clear political connotation \nin that it disproportionately affects groups of people who are \nalready disadvantaged because of their social identities, such \nas race, gender, class, or disability. In addition to the ineq-\nuitable distribution of resources, epistemic injustice affects \nthe ways in which knowledge and experiences are recog -\nnized, valued, or discredited by others. It manifests itself in \ntwo main forms: testimonial and hermeneutic injustice. The \nsecond of which is most relevant to the present case. Her -\nmeneutic injustice refers to a situation in which a person or \ngroup is disadvantaged because their experiences or social \nrealities are not acknowledged or understood due to a lack of \nconcepts, vocabulary, or frameworks. In such cases, it may \nbe difficult for individuals to articulate their experiences or \nseek redress because this particular, rather subtle but no less \nrelevant form of injustice is not adequately recognized or \nunderstood by society (Fricker, 2009).\nFrom an overarching perspective, the concept of epis-\ntemic injustice also needs to be situated historically, as it can \nbe understood as a further development of Gayatri Chakra-\nvorty Spivak’s notion of epistemic colonization (Spivak, \n1988). Epistemic colonization refers to the processes by \nwhich one’s culture’s knowledge systems, beliefs, and ways \nof knowing are imposed on another culture or community, \noften as a direct, indirect, or late consequence of coloniza-\ntion or imperialism. This involves the domination of a par -\nticular theory of knowledge (in the present case, it may be a \nbelief in the universal power of AI systems developed in the \nWest) over others, often marginalizing or suppressing local \nknowledge systems and ways of understanding the world.\nEpistemic injustice, understood as rooted in a history of \nepistemic colonization, can lead not only to individual but \nalso to structural harm, as it is usually accompanied by a \nloss of cultural diversity and leads to a form of homogeni-\nzation or violent cultural appropriation that ultimately ben-\nefits those who caused the injustice. In this way, existing \npower imbalances are perpetuated as imperialist knowledge \nbecomes the standard against which all other knowledge \nis measured. This can entrench structural dependencies. \nEpistemic injustice, as we understand it here and use it to \ncritically assess the effects caused by current initiatives to \nexpand language technology, builds on historically estab-\nlished inequalities and need to be understood in this crony -\nism. In our view, counter-designs and strategies can only \nfunction if they take this broader context into account.\nIn what follows, we lay out how recent attempts to close \nthe language gap through distributing epistemic resources \nbut without accounting for meaningful diversity are at risk \nof contributing to epistemic injustice. To do so, we elaborate \non what language modeling bias means as a counterpart to \nlinguistic diversity, and why it is much more of an ethico-\npolitical matter than it might appear from a purely techno-\nlogical or linguistic perspective.\nBias in language technology\nTo understand how bias plays out in language technol -\nogy, it is important to consider how linguistic bias, a well-\nresearched area, gets interwoven with algorithmic bias, \nanother well-researched area.\nIn the context of digital technology, the notion of bias has \ngained much attention and was prominently problematized \nas it has been identified as one of various sources of auto-\nmated discrimination (Barocas and Selbst, 2016). So far, \nalgorithmic bias has been used mainly to refer to patterns \nof stereotypes and preferences towards social groups, most \noften concerning learning-based language processing sys-\ntems (Blodgett et al., 2020). In terms of social groups, stud-\nies have focused on gender, ethnicity, and race, but also other \nforms of bias (religion-related, age-related, political, etc.) \n(Friedman & Nissenbaum, 1996). To, then again, systema-\ntize linguistic bias, which is at the focus of communication \nstudies, Hovy and Prabhumoye (2021) identify five sources: \n(1) the training data, (2) the annotation process, (3) the mod-\nels, (4) the input representation process and (5) the research \ndesign for studying the biases. All these five sources we also \nfind to be relevant when dealing with algorithmic bias in, \nsay image recognition or ADM-systems (De-Arteaga et al., \n2019a; Schwemmer et al., 2020).\nAs it can lead to various forms of harm, it is important \nto problematize both linguistic and algorithmic bias, and \nparticularly their interplay. Yet, we also recognize that bias \nis omnipresent and that, even if it is usually associated with \na negative connotation, it actually need not be harmful to \ndiversity per se. For example, when affirmative action serves \nto counteract the unequal representation of otherwise mar -\nginalized groups, bias may well be intentional and desirable. \nContrary to a blanket critique of bias as a phenomenon in \nitself, we accept that all knowledge, all insights, and even all \ndata are situated, i.e. they always reflect a particular point \n P . Helm et al.\n8 Page 6 of 15\nof view in space and time that is influenced by culture, his-\ntory, politics, economics, epistemology, and so on (Haraway, \n1988; Gitelman, 2013).\nUnbiasedness is therefore a deceptive goal that, instead of \nsolving social problems, reproduces problematic ideas, such \nas the unrealistic imaginary that technology can be neutral \n(Beer, 2017). It is therefore important to be upfront about \nwhen and for what reasons a certain bias is problematic and \nneeds to be combated, and that this combating does usually \nnot lead to no bias, but to a different, ideally more just bias \n(Harding, 1995). Linguistic bias, for example, is harmful to \ndiversity when it perpetuates existing or produces new forms \nof hermeneutic injustices related to already vulnerable, and/\nor marginalized language communities. Such bias calls for \ncounteraction. When such linguistic bias is then reproduced \nthrough LLMs that are geared toward the correct representa-\ntion of languages of colonial powers, but disregard the par -\nticularities of other languages that are also spoken by many \npeople or are at risk of extinction, this demands change. To \nenact such change sustainably, it is instrumental to invest \nwork into unraveling how linguistic bias and algorithmic \nbias interact to emerge as a new subform of bias, which we \ncall language modeling bias.\nLanguage modeling bias\nThe subject of language modeling bias are not just languages \nper se but also the design of language technology: corpora, \nlexical databases, dictionaries, machine translation systems, \nword vector models, etc. Language modeling bias is present \nin all of them, but it is easiest to observe with respect to \nmultilingual resources and tools, where the relative correct-\nness and completeness for each language can be observed \nand compared. We define it as follows:\nLanguage modeling bias is observed when the tech-\nnology, by design, represents, interprets, or processes \ncontent less accurately in certain languages than in \nothers, thereby forcing speakers of the disadvantaged \nlanguage to simplify or adapt their communication, \n(self-)representation, and expression when using that \ntechnology to fit the default incorporated in the privi-\nleged language.\nBias manifests itself through linguistic or cultural inaccu-\nracies in the way a language is processed or represented. \nBy emphasising the by-design aspect of language modeling \nbias, our definition is deliberately focusing on the repre-\nsentational, rather than the allocational harm of bias in lan-\nguage technology. Thus, language modeling bias is not only \nconcerned with the scarcity of data on certain languages or \nwith biases within linguistic devices, but rather with how \nthese biases are amplified through structural bias built into \nlanguage processing algorithms, representational models, \nresources, or methodologies.\nWe thus situate language modeling bias as a specific form \nof algorithmic bias that is observed in language technology. \nWe differentiate it, from other amply studied forms of algo-\nrithmic bias in language-based AI, such as semantic repre-\nsentation bias, that are not primarily linguistically defined \nDe-Arteaga et al. (2019b). In opposition, the direct subjects \nof language modeling bias are in fact languages, dialects, \nand sociolects, while its indirect subjects are, of course, \nthe speakers themselves. A second, crucial distinguishing \nfeature from other forms of bias is that language modeling \nbias concerns technology design: inherent limitations within \nthe structure of language databases, neural AI systems, and \nlanguage processing algorithms. We clearly distinguish this \nissue from out-of-scope problems related to the underlying \ndata (corpora), frequently included under the umbrella term \nof algorithmic bias: data availability, such as differences in \nthe sizes of training corpora between well-resourced and \nunder-resourced languages, or data quality, such as socio-\ncultural stereotypes encoded within training corpora. Focus-\ning on these problems would lead to solutions that confirm \nto scalabe as opposed to meaningful ideas of diversity, such \nas simply generating more language data to be fed into pre-\ndefined LLMs.\nThe social groups affected by language modeling bias \nare clearly the communities of speakers of underrepresented \nlanguages, however heterogeneous they may be otherwise \n(according to social status, culture, gender, race, ethnicity, \nreligion, etc.). Being the native or second-language speaker \nof a language variety determines one’s access to information, \nand the language technology that enables this access affects \none’s ability to communicate, on the Web or elsewhere. To \nour knowledge, the term language modeling bias  has not \nbeen used as a analytical device or design strategy in any \nway similar to ours while many of the underlying neoco-\nlonial mechanisms have, however, been pointed out (Bird, \n2022; Schwartz, 2022).\nIn terms of actual bias in AI systems and data, research \nconcerning the representation (or the lack thereof) of the \nvernaculars of social groups within language resources is \nclosest to ours. Here, however, we want to go a step further \nin pointing out how, both in the field of engineering and \ntechnology advancement as well as in ethics, policy and \ndevelopment aid, the language communities themselves \nare left out of the process. These attempts or projects, with \nAradau and Blanke (2022) can be described as techniques \nof governing emerging technology, which while striving for \ndiversity as one of their goals, turn those most affected by \nthe results into what philosopher Jacques Rancière has called \nthe “part of those who have no part” (Ranciere, 1998, p. 30). \nIt is the technology developers and designers residing in the \nbig companies as well as influential academic institutions \nDiversity and language technology: how language modeling bias causes epistemic injustice  \n Page 7 of 15 8\nthat fashion themselves as the experts who are called upon \nto embed linguistic diversity into their tools and expand \nthem under the normative guise of inclusion. However, in \nthe process, language modeling bias is reproduced because \nthe Western perspective is taken as the norm and the subjects \nof diversity remain at the outside. It is this form of exclu-\nsion which at the same time allows for and is mobilized by \ninvestments in scalable diversity (Fig. 1).\nForms of language modeling bias\nIn the following we provide examples of language modeling \nbias from mainstream AI-based language technology: neural \nlanguage models, machine translation systems, and multilin-\ngual lexical databases.\nNeural language models\nThe general trend of AI-based (neural) language technology \nis to rely on as little prior knowledge about languages as \npossible, instead obtaining all such knowledge through cor-\npus-based learning. While such a design avoids any obvious \nalgorithmic bias towards any particular language, it creates \na strong dependency on the quality and the size of the train-\ning corpus. The well-known consequence of this is a prefer-\nence of mainstream AI towards the languages of dominant \ncultures with a strong web presence, with gigabyte-sized \npre-trained large language models that have led to not one \nbut a long series of breakthrough improvements on language \nunderstanding and generation tasks. The same technology, \ndue to the lack of corpora, provides low-quality results, or \nnone at all, to under-resourced languages.\nWhile the dependence of neural language models on \nlarge training corpora appears to be a transparent and seem-\ningly language-agnostic constraint, the architectural choices \nunderlying such models can still lead to language modeling \nbias. White and Cotterell (2021) show that the word predic-\ntion performance of the Long Short-Term Memory (LSTM) \nneural network architecture is less sensitive to word order \nthan that of the Transformer architecture. As they experi-\nmentally show, the Transformer appears to have a bias \ntowards the (rarely occurring) verb–subject–object (VSO) \nword order, while showing lower performance on the (very \nfrequent) SOV and SVO word orders, all other parameters \nbeing equal. Moving to morphology, Zevallos and Bel \n(2023) study subword-frequency-based, language-agnostic \ntokenization (i.e. word splitting) algorithms, such as Byte \nPair Encoding (Sennrich et al., 2015), that are typically \nused to preprocess corpora fed to large language models. \nThey experimentally show that such methods tend to train \nslower on morphologically complex (synthetic, agglutinate) \nlanguages, meaning that more training data are required for \nthese languages to achieve the same performance on down-\nstream language understanding tasks. Replacing the lan-\nguage-agnostic tokenization algorithm by language-specific \nmorphological segmentation allows language models to train \nmore efficiently over smaller corpora.\nMachine translation\nMachine translation (MT) is the flagship task of AI-based \nlanguage technology. Without claiming to be exhaustive, we \npoint out three aspects of current MT technologies where \nlanguage modeling bias can be observed: the non-handling \nof untranslatability, the variedness of vocabulary and gram-\nmar, and the use of a pivot language.\nToday’s top MT systems, such as DeepL and Google \nTranslate, make systematic mistakes over untranslatable \nterms, betraying the fact that this phenomenon is not specifi-\ncally addressed by these tools. The screenshots (a) and (b) \nin Fig. 2, taken from a mainstream machine translator, show \nexamples of erroneous translations due to untranslatability. \nAs reported by (Khishigsuren et al., 2022), when translating \nthe English sentence My brother is three years younger than \nme to Hungarian, Mongolian, Korean, or Japanese, syntacti-\ncally correct yet semantically absurd results are obtained:\nHungarian: *A bátyám három évvel fiatalabb nálam.\nJapanese: *私の兄は私より3歳年下ですす.\nKorean: *형은 나보다 세 살 아래다.\nMongolian: *Ах маань надаас гурван насаар дүү.\nThese languages either have no equivalent word for \nbrother (as in Mongolian) or, when they do, the equivalent \nword is rare (as fiútestvér in Hungarian). Based on train-\ning corpus frequencies, the MT system ends up choosing a \nsemantically unsuitable word, such as bátyám meaning my \nelder brother , resulting in My elder brother is three years \nyounger than me.\nA second form of bias concerns the variedness of vocab-\nulary and grammar in MT output. Vanmassenhove et al. \n(2021) quantitatively compare the lexical and grammati-\ncal ‘richness’ of original and machine-translated text. They \nreport that both lexicon and morphology tend to become \npoorer in machine-translated text with respect to the original \n(untranslated) corpora: for example, features of number or \ngender for nouns tend to decrease. This is a form of language \nmodeling bias against morphologically rich languages.\nA third form of language modeling bias in MT is their \nuse of English as a pivot language when translating between \nFig. 1  Biased cross-lingual mapping of words about various forms of \n‘rice’ from a popular multilingual lexical database\n P . Helm et al.\n8 Page 8 of 15\ntwo languages other than English. This practice is explained \nby the relative scarcity of bilingual training corpora for \nsuch language pairs, as well as scalability. Example (c) in \nFig. 2 shows the case of French-to-Italian translation of a \nsentence meaning my (female) cousin married a tall man.  \nWhile French and Italian (as do most languages) use dif-\nferent words for male and female cousins (cousin/cousine, \ncugino/cugina), English does not. The result is that the gen-\nder of the cousin is ‘lost in translation’ and, as a form of \ncombined linguistic and gender bias, it appears as a male in \nthe translation.\nMultilingual lexical databases\nAs a generalisation of bilingual dictionaries, the 2000s saw \nthe appearance of multilingual lexical databases that map \nwords, based on their meanings, across a large number of \nlanguages. While these resources proved to be extremely \nuseful in cross-lingual applications, looking under the \nhood—into their underlying models of lexical meaning—\nreveals varying levels of limited expressivity and bias.\nAs shown by Giunchiglia et al. ( 2023), some of these \nmultilingual databases interconnect words from hundreds of \nlanguages, mapping the words of each language to a collec-\ntion of roughly 100 thousand word meanings (so-called syn-\nsets) obtained from the English Princeton WordNet (Miller, \n1998). On the one hand, this choice makes practical sense, \nas among all similar resources, WordNet provides by far the \nbroadest and most precise semantic coverage. On the other \nhand, using the lexical concepts of WordNet to describe \nthe lexicons of all other languages results in a strong bias \ntowards the English language and Anglo-Saxon culture in \ngeneral, as the expressivity of the database is limited to \nnotions for which a word exists in English (Giunchiglia \net al., 2023; Bella et al., 2022). Figure  1 provides a simple \nexample from the food domain, known to be culturally, and \nthus also linguistically, diverse. It shows how a biased lexi-\ncal database maps together words in Swahili and Japanese \nmeaning uncooked rice, cooked rice, and uncooked brown \nrice. The degree of information loss is flagrant: while both \nSwahili and Japanese provide fine-grained lexicalizations \nabout the various forms of rice, the many-to-many map-\nping that results from passing through English masks all \nfine-grained differences, resulting in both a loss of detail \nand incorrect translations when one moves from Swahili to \nJapanese or vice versa. The diversity-diminishing bias is also \nfound in other domains that are well-known to be diverse \nacross languages: family relationships, school systems, etc.\nMethodological causes of language modeling bias\nBecause bias is most problematic when it perpetuates exist-\ning power relations that can contribute to far reaching harms \nsuch as hermeneutic injustice, any critique of technological \nbias should ideally include at least a brief genealogy of the \norigins of bias and the ways in which different social groups \nare harmed or benefited in different ways. The following \nsubsection highlights how well-intended but unreflective \n(a)\n(b)\n(c)\nFig. 2  Examples of language modeling bias in machine translation. \na The lack of an equivalent common word in Hungarian for brother \nresults in an erroneous translation meaning my elder brother is three \nyears younger than me. b The lack of an equivalent term for rice in \nSwahili results in an erroneous translation meaning this raw rice is \ntasty. c The systematic use of English as pivot language results in an \nerroneous change of gender when translating between French and \nItalian\nDiversity and language technology: how language modeling bias causes epistemic injustice  \n Page 9 of 15 8\nattitudes in computational linguistics contribute to the crea-\ntion of language technologies that are adverse to meaningful \ndiversity. It also reveals that computational linguistics has \nnot developed in isolation, but is situated within epistemic \nhierarchies that are both reflective of and contributing to \nsocio-economic power asymmetries. Analyzing this kind of \nsituatedness of research cultures is crucial for clarifying why \nfocusing on language inclusion and expansion is not enough \nto promote diversity, but may, when being scaled, ultimately \neven reproduce entrenched dynamics of epistemic injustice.\nIn the last 50 years, research in Computer Science has \nbeen dominated by a strong Anglo-Saxon influence, reflect-\ning turn-of-century power dynamics. In Computational \nLinguistics, this bias was apparent across all prestigious \npublications, conferences, and journals of the field: an \nunspoken convention required for research to be considered \nas competent to be applied and demonstrated in English. \nThus, English has not only been the lingua franca of scien-\ntific communication, but also the de facto standard subject \nmatter of research. This is not to say that scientific results \non other languages were not published, but they were gen-\nerally considered by the community (paper reviewers, jour-\nnal editors, etc.) as ‘language-specific’ and therefore less \nlikely to be relevant to a wide audience. Publications about \nlanguages other than English were relegated to second-tier \nor niche journals and venues. Schwartz (2022) reports that \nbetween 2013 and 2021, 83% of papers accepted at ACL—\nthe flagship conference in Computational Linguistics—were \nexplicitly or implicitly about English and 97% were about \nIndo-European languages.\nDespite these numbers, the 2010s saw an emerging \ninterest in multilingual language technology, and of a new \nresearch sub-field targeting ‘low-resource’ (or ‘under-\nresourced’) languages, previously neglected by mainstream \nresearch. This change of scope is tightly related to the blaz-\ning progress of deep-learning-based AI on English (and also \non some other well-supported languages such as Spanish or \nChinese). Problems that were earlier considered as exceed-\ningly hard, such as machine translation, have suddenly been \nsolved with impressive results. For researchers, the ‘major’ \nlanguages were not providing suitably interesting challenges \nanymore, apart from incremental research pushing the accu-\nracy boundary. Low-resource languages seemed like a prom-\nising horizon.\nThe new fascination with ‘low-resource languages’ does \nnot mean that, say, Mongolian speech synthesis has suddenly \nbecome of mainstream scientific interest. In line with the \n‘zero-shot’ data-driven ethos (Bird, 2020) of recent deep AI \nresearch that shuns any use of prior results from linguists \nand field workers, low-resource language research is only \nworthy of a top publication as long as (1) it provides a solu-\ntion for multiple, preferably tens or hundreds of languages \nat the same time; (2) it involves mainstream AI technology, \ni.e. neural networks; and (3) it requires very little to no \nknowledge from experts or speakers of the languages tar -\ngeted. The typical low-resource research contribution thus \nscrapes web content, such as Wikipedia pages, written in the \nlanguages in question, often without any understanding of \ntheir quality or content (Lignos et al., 2022). It then trains \nor fine-tunes deep learning models based on the data, and \nfinally demonstrates a few percentages of increase in qual-\nity (precision, recall, BLEU, etc.) over one of the standard \ntasks in computational linguistics, such as named entity \nrecognition or machine translation, against corpora that the \nresearchers themselves cannot read. This practice is certainly \nnot in line with what we earlier described as accounting for \nmeaningful diversity.\nSimultaneously, many highly populated but under-\nresourced Global South countries were identified by high-\ntech companies (and digital platforms in particular) as still \nunsatisfied markets with a potential for data scraping and \ninfrastructural advancements. What happened during the \nfollowing 10 years has been described as a ‘race’ in which \ndigital platforms swamped African and South East Asian \ncountries, in order to be the first to secure the loyalty of wast \nnew customer bases (Arora, 2019; Benjamin, 2019).\nAlso, in the field of technology ethics, Silicon Valley has \nset agendas over the past decade by pumping large amounts \nof money into an academic system that otherwise faces scar-\ncity measures and budget cuts (Ochigame, 2019). This has \nled to two types of ethics increasingly taking hold: firstly, \nthe type that embraces the notion that it is primarily more \ntechnology, and in particular the expansion of AI, whereby \nexisting problems can be solved, and secondly, the type of \nethics that can be easily transferred into existing systems \nand infrastructures and from there automated and imple-\nmented en masse. Both can be demonstrated in the often \noverly simplistic ways in which racial and gender biases are \ntackled by developing fairness measures, and can equally \nbe mirrored for the appropriation and handling of calls for \nbetter acknowledgment of diversity.\nThe industrial appropriation of academic ethics research \nand its influence on the respective notions of justice as fair-\nness and diversity as demographics go hand in hand with \nthe broader ranging imaginary that large scale technological \ninnovation will serve as a panacea for wide-ranging prob-\nlems (Pfotenhauer and Jasanoff, 2017 ). Following Anna \nTsing, however, we understand scalability not as an intrinsic \nproperty of a solution or product (of any kind), but as some-\nthing that stems from emphasizing certain aspects at the cost \nof others. Ironically, then, for an innovation to be scalable, it \nmust be designed to reduce the complexity of a problem and \nits associated solution to isolated parameters that can fairly \neasily be abstracted from the context of the specific domain \nor community for which it was developed (Engel, 2016). \nThis abstraction work makes the innovation generalizable \n P . Helm et al.\n8 Page 10 of 15\nand thus scalable in that the number of languages can be \nsignificantly increased without major adaptation (Tsing, \n2012). This is exactly what is happening when existing neu-\nral language technology is applied indiscriminately to any \nlanguage without adaptation.\nThese problems are exacerbated against the backdrop of \na postcolonial computer culture (Irani et al., 2010). In this \ncontext, recent Data4D efforts have been criticized, not only \nin terms of their “white savior” ethos, but in some cases even \nto the point of using development goals as free riders to \ninvade vulnerable populations and extract their data (Taylor \nand Broeders, 2015). Whatever the intentions, the choice of \nproblems to focus on is often driven by either the incentives \nof academic communities or by industry pressure or, most \nlikely, a combination thereof. This leads to a gap between \nthe solutions offered and the diverse needs of communities.\nEthical concerns with biases in language \ntechnology\nThe consequences of research being done under these con-\nditions raise a multitude of ethical concerns with regard to \npotential epistemic injustice being done. Most of these con-\ncerns are related to the rather ill-defined attempts to promote \ndiversity by adopting the top-down scalable solutions that \nAI-approaches warrant and which, by the nature of their \ndesign, can only respond to a simplistic idea of diversity. \nWith Western researchers unilaterally setting developmental \ngoals and providing technological solutions to reach them, \nthey effectively and most ironically, silence the actual speak-\ners. This silencing does not regard the lexical representation \nor distribution of epistemic goods, which may in fact be \nincreased, but the types of problem definitions and the cor -\nresponding designs of technical solutions.\nYet, language resources that are, at least partly, hand-\ncrafted and co-designed, are rarely deemed competitive \nbecause they are much harder to scale as they are by defi-\nnition not generalizable. If technical innovation is, how -\never, narrowly defined as the expansion of AI via Neural \nLanguage Models, and social innovation as the scaling of \ncosmetic diversity, then this will not only lead to a neglect \nof small languages. It will also affect large language com-\nmunities when the social realities and worldviews anchored \nin their language cannot be expressed through dominant \nanglo-centric models. Given the importance and possibili-\nties of language technology in the struggle for hermeneutic \njustice, defined as the equal recognition of distinct socio-\nculturally situated experiences or realities, there is a strong \ncase to be made for the socio-technical innovation potential \nof co-designed and customized systems that can do justice \nto linguistic diversity.\nDespite this potential, critical commentators on AI lan-\nguage technology point out how well-intended research \ngoals such as “technology-based revitalization” regularly \nmisinterpret the needs of local communities (Bender et al., \n2021; Bird, 2022). In most cases, native speakers are not \ninvolved in the process, or if they are, they are taking on \nsubordinate roles such as commentator, validator, tester, \nor worse, data extractor (Helm et al., 2023). Instead of co-\ncreating on an equal footing, in many cases the analytical, \nhigh-level work is done in technology labs of Western uni-\nversities or companies, where the languages being studied \nare often not even understood by the people working on \nthem, let alone the cultures they represent (Arora, 2016). \nSometimes they do not even know if they are using the right \nlanguage, as observed in the case of automated Wikipedia \nscraping (Lignos et al., 2022).\nAdded to this is the neglect of the meaning and relevance \nof language variations, which goes beyond the mere trans-\nmission of information (Bender et al., 2021). The Kenyan \nwriter and scholar Ngugi wa Thiong’o (1986), for example, \nhas done extensive research on the cultural diversity embed-\nded in Kenya’s multilingual heritage. Language conveys the \nsituatedness of knowledge. It influences how we see our \nworld, which visions we follow, how we perceive colors, \ntastes, time. In Kiswahili, Arabic and Amharic, for example, \ntime is measured not only from one hour to the next, but in \nrelation to sunrise and sunset. Thus, 7 a.m. depends on the \nseason because it is always the first hour after sunrise. This \nunderstanding, which better adapts social life to the dynamic \nrhythm of the year, does not translate one-to-one into the \nmuch less dynamic but more definite Anglo-American sys-\ntem for communicating about time, and it would be lost if \nwe all had to squeeze ourselves into English. It is this kind \nof situated knowledge embedded within languages, which \ncannot be captured by one-size-fits-all design, nor by AI, no \nmatter how technically sophisticated. It is something that has \nto be done by people who know what they are talking about, \nliterally. These dimensions are in danger of getting lost to a \nculture that limits its normative horizon to large scale techni-\ncal innovation and expansion as ends in themselves.\nGiven these differences, which go beyond lexical rep-\nresentation, we contend that it is unfair to require all of us \nto conform to communication norms that have emerged \nfrom the perspective of English-speaking (or, increasingly, \nChinese-speaking) users if we are to take advantage of the \nopportunities that language technologies provide. In other \nwords: can we accept language modeling biases as a fact of \nthe digital age, or do we consider them ethically unaccep-\ntable? Following our reasoning, the latter is obviously the \ncase. Having analyzed the various forms language modeling \ncan take, we argue that just as entrenched gender biases can \nbe found in image recognition systems, with far-reaching \nconsequences for equal opportunities for non-cis people, \nDiversity and language technology: how language modeling bias causes epistemic injustice  \n Page 11 of 15 8\nthis is the case for linguistic biases in language technology. \nExcept that the latter form has to do with the misrepresenta-\ntion or disregard of cultural or “social factors” embodied in \nand expressed through language (Hovy & Yang, 2021). This \ndisregard does not necessarily manifest itself in direct acts of \ndiscrimination, but in systemic forms of hermeneutic injus-\ntice. While these forms are difficult to grasp in their wider \nimplications, in the most severe instances, they do reveal \nthemselves in their harmful impacts on the life chances of \nindividuals (as we can already see from the dramatic conse-\nquences that errors in automatic translation can have in the \ncontext of asylum procedures, (Bhuiyan, 2023)).\nThe challenge now is to understand these errors early on \nnot just as incidental glitches, but as the result of broader, \nsocio-historical inequalities that manifest themselves in the \nperformance of widespread language technologies (Brous-\nsard, 2023). When lexical gaps caused by biased translation \ntechnologies become a normal condition of multilingual \ncommunication, then this will lead to a situation where \none person can express herself perfectly well in her own \nlanguage, while another one is limited in her expression of \nher social realities and the experiences that emerge from \nthese. This is nothing else than hermeneutic injustice play -\ning out in concrete practice. To defend this rather strong \nclaim, it is important to consider the power relations rooted \nin a colonial history marked not only by necropolitical but \nalso empistemic violence. The political theorist Ali Mazrui \nand Mazrui (1999) has studied the influence of English on \nAfrican culture and argues that this influence changes the \nself-perceptions and social practices of African peoples. \nThis need not be bad per se. Creole creates something new \nthat we can greatly appreciate; cultural encounters can be \nenriching and broaden perspectives. The problem, then, is \nnot cultural mixing and matching as such; on the contrary. \nRather, it is the dominance of certain cultures over the oth-\ners, that tends to be reflected or even perpetuated in AI lan-\nguage technology and which needs to be overcome if we are \nto promote epistemic justice.\nAddressing epistemic injustice in language \ntechnology: the Live Language initiative\nA case presenting a specific effort for embedding values in \nlanguage technology is the LiveLanguage initiative (Bella \net al., 2022). LiveLanguage combines diversity-aware design \n(Helm et al., 2022) with a collaborative resource develop-\nment methodology that strives to promote epistemic justice \nfor marginalized language communities.\nIn terms of technology, LiveLanguage focuses on the \ndevelopment of diversity-aware lexico-semantic resources. \nThe Universal Knowledge Core (UKC) lexical database 2 \ncovers the lexicons of over 2,000 languages (to varying \ndegrees of completeness). What makes the UKC aware \nof meaningful diversity is its simultaneous representation \nof, on the one hand, what is shared across languages and \ncultures, such as words with equivalent or similar mean-\nings or a common etymology and, on the other hand, what \nmakes them different: untranslatable terms, lexical gaps, \nor language-specific grammar (see Fig.  3 for an example: \nlanguages that do and that do not lexicalize the family rela-\ntionship of female sibling). It is through the integrated repre-\nsentation of cross-linguistic unity and diversity, both on the \nsurface and in semantics, that the UKC confronts language \nmodeling bias within lexical resources (Giunchiglia et al., \n2023).\nThe diversity-aware lexicons, contained in the UKC in the \nform of an extensive lexico-semantic knowledge graph, are \nfreely downloadable from the LiveLanguage data catalog.3 \nThey can be useful as reference knowledge for the evaluation \nof AI applications, and as input resources in order to com-\nplement corpus-based training. The use of lexical databases \nas reference knowledge has a long tradition in computational \nlinguistics, such as in word sense disambiguation where \nlexicons are used as catalogues of word senses (Agirre & \nEdmonds, 2007). More specifically, LiveLanguage data was \nused to perform meaning-level evaluation of machine trans-\nlation systems over hard-to-translate sentences (Khishig-\nsuren et al., 2022). Likewise, cross-lingual cognate pairs \nfrom the UKC (Batsuren, Bella and Giunchiglia, 2022) have \nbeen employed to evaluate word embedding models (Zouhar \net al., 2023).\nLiveLanguage and the UKC integrate both existing \nthird-party resources and linguistic data collected through \ncollaborations with universities. Examples of such collabo-\nrations include Mongolia (Batsuren et al., 2019), Scotland \n(Bella et al., 2020), India (Chandran Nair et al., 2022), \nPalestine (Khalilia et al., 2023), and South Africa (Dibitso \net al., 2019). Striving to ensure that such collaborations \nare beneficial to local speaker communities and to avoid \nexploitative practices, LiveLanguage collaborations adopt \na methodology based on co-creation and local empower -\nment, with the following characteristics: (a) representatives \nof local communities are leading the formulation of prob-\nlems and needs, as well as the subsequent specifications of \nthe language resources to be developed; (b) tools, infrastruc-\nture, and know-how are provided to local communities if \nneeded, in order to embed solutions sustainably; (c) intellec-\ntual property rights stay with the local community; (d) lan-\nguage resources are integrated into the global LiveLanguage \n2 http:// ukc. datas cient ia. eu.\n3 https:// datas cient iafou ndati on. github. io/ LiveL angua ge/.\n P . Helm et al.\n8 Page 12 of 15\necosystem, giving worldwide visibility to the results through \nthe UKC database and the LiveLanguage data catalog.\nThat said, it needs to be noted that accounting for diver -\nsity and power asymmetries in language technology is not \na fixed state, but a process and situated procedure which \nrequires continual adaption to the variety of linguistic phe-\nnomena and different communities’ needs. Therefore, we \nembrace the contributions of design anthropologists such \nas Smith et al. (2021) who advocate mutual learning and \nthus consider all participants in the process simultaneously \nas researchers and beneficiaries. Specifically with regard \nto language technology, we echo Bird’s call for a focus on \nknowledge transfer beyond language, as generational loss of \nknowledge about local history, practices, etc. is often one of \nthe main reasons for interest in language preservation, and \ngives rise to deliberate promotion of digitization.\nConclusions\nIn this paper, we have shown that simply applying existing \nlanguage technology to ever larger sets of languages does not \nautomatically serve the goal of bridging the digital language \ndivide, as technology does not always generalize across lan-\nguages. We argue that current technological approaches to \naddressing the “low-resource language problem” can even be \ndetrimental rather than beneficial from the point of view of \npreserving linguistic diversity. Profound differences lie not \nonly within diverse grammatical structures but also across \npeople’s social practices, worldviews, and situated knowl-\nedges embedded within linguistic expression. Through the \nLiveLanguage initiative, we try to make such differences \nmanifest by formally representing cross-lingual diversity in \nboth grammar and semantics, for domains such as kinship \nrelations, educational systems, color, time, or food.\nFurthermore, we point out that problems of ethnocen-\ntric language technology development are rooted within a \ncolonial past, which is still potent today. Given these cir -\ncumstances, there is an urgent need to be aware not only of \nthe well-known forms of linguistic bias in AI systems that \nreproduce human biases encoded in large web corpora, but \nalso to pay due attention to the language modeling bias that \nstems from language technology design itself. As we show, \ntechnological expansion that is based on biased tools is not \nonly detrimental in terms of inadequate description of lan-\nguage, but actually contributes to a form of injustice that we \nidentify as epistemic, or, more precisely, hermeneutic in its \nform and effect. This form of injustice is not aimed at the \ndistribution of epistemic goods, which is indeed encouraged \nby recent efforts to expand multilingual language technolo-\ngies. Rather, it is about the lack of recognition of certain \nforms of knowledge, modes of expression, and social reali-\nties that are evident in the diversity-related phenomena we \nhave identified. This form of injustice is not only problem-\natic in itself, but also troubling in that it can be understood \nas an extension of colonial domination.\nIn light of these criticisms, we conclude that any efforts to \nextend existing language technologies that are not based on \na rigorous approach to co-creation with the language com-\nmunities in question should be fundamentally re-framed. By \nrigorous, we attribute approaches based on a critical stance \ntoward the privileges of whiteness that avoids any kind of \nwhite savoir-faire and instead conceives of the process as \nan opportunity for mutual learning in which neither party \nis superior to the other. It is this attitude and its related \npractices that will contribute to promoting and maintaining \nmeaningful, as opposed to cosmetic diversity.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nFig. 3  Screenshot from the \nwebsite of the UKC lexical \ndatabase, showing languages \nthat lexicalize the concept of \nfemale sibling (white-contoured \ndots) and those where it is \nknown to be a lexical gap \n(black-contoured dots)\n\nDiversity and language technology: how language modeling bias causes epistemic injustice  \n Page 13 of 15 8\nReferences\nAgirre, E., & Edmonds, P. (2007). Word sense disambiguation: Algo-\nrithms and applications. Springer.\nAradau, C., & Blanke, T. (2022). Algorithmic reason: The new govern-\nment of self and other. Oxford University Press.\nArora, P. (2016). Bottom of the data pyramid: Big data and the global \nsouth. International Journal of Communication, 10(1), 1–19.\nArora, P. (2019). The next billion users: Digital life beyond the west. \nHarvard University Press.\nBarocas, S., & Selbst, A. D. (2016). Big data’s disparate impact. Cali-\nfornia Law Review, 104(3), 671–732.\nBatsuren, K., Ganbold, A., Chagnaa, A., Giunchiglia, F. (2019). Build-\ning the mongolian wordnet. In: Proceedings of the 10th Global \nWordnet Conference (pp.238–244).\nBatsuren, K., Bella, G., & Giunchiglia, F. (2022). A large and evolving \ncognate database. Language Resources and Evaluation, 56(1), \n165–189.\nBeer, D. (2017). The social power of algorithms. Information, Com-\nmunication & Society, 20(1), 1–13. https:// doi. org/ 10. 1080/ 13691 \n18X. 2016. 12161 47\nBella, G., Batsuren, K., Khishigsuren, T., Giunchiglia, F. (2022). Lin-\nguistic diversity and bias in online dictionaries. University of \nBayreuth African Studies Online,173.\nBella, G., Byambadorj, E., Chandrashekar, Y., Batsuren, K., Cheema, \nD., Giunchiglia, F. (2022). Language diversity: Visible to humans, \nexploitableby machines. In: Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics: System \nDemonstrations (pp. 156–165).\nBella, G., McNeill, F., Gorman, R., Donnaíle, C.Ó., MacDonald, K., \nChandrashekar, Y., Giunchiglia, F. (2020). A major wordnet for \na minority language: Scottish gaelic. In: Proceedings of the 12th \nLanguage Resources and Evaluation Conference (pp. 2812–2818).\nBender, E. M., Gebru, T., McMillan-Major, A., Shmitchell, S. \n(2021). On the dangers of stochastic parrots: Can language \nmodels be too big? Proceedings of the 2021 acm conference \non fairness, accountability, and transparency (p. 610–623). \nNew York, NY, USA: Association for Computing Machinery. \nRetrieved from https://dl.acm.org/doi/10.1145/3442188.3445922 \n10.1145/3442188.3445922\nBenjamin, R. (2019). Race After Technology: Abolitionist Tools for \nthe New Jim Code (1. edition ed.). Polity.\nBhuiyan, J. (2023, September). Lost in ai translation: growing reli-\nance on language apps jeopardizes some asylum applications. \nThe Guardian. Retrieved from https://www.theguardian.com/\nus-news/2023/sep/07/asylumseekers-ai-translation-apps\nBird, S. (2020, December). Decolonising speech and language technol-\nogy. Proceedings of the 28th international conference on compu-\ntational linguistics (pp. 3504–3519). Barcelona, Spain (Online): \nInternational Committee on Computational Linguistics. Retrieved \nfrom https://aclanthology.org/2020.colingmain.313 10.18653/\nv1/2020.coling-main.313\nBird, S. (2022, May). Local languages, third spaces, and other high-\nresource scenarios. Proceedings of the 60th annual meeting of the \nassociation for computational linguistics (volume 1: Long papers) \n(pp. 7817–7829). Dublin, Ireland: Association for Computational \nLinguistics. Retrieved from https://aclanthology.org/2022.acl-\nlong.539 10.18653/v1/2022.acl-long.539\nBlodgett, S.L., Barocas, S., Daumé III, H., Wallach, H. (2020). Lan-\nguage (technology) is power: A critical survey of “bias” in nlp. \nIn: Proceedings of the 58th Annual Meeting of the Association \nfor Computational Linguistics (pp. 5454–5476).\nBroussard, M. (2023). More than a glitch: Confronting race, gender, \nand ability bias in tech. The MIT Press.\nChandran Nair, N., Velayuthan, R.S., Chandrashekar, Y., Bella, G., \nGiunchiglia, F. (2022, June). IndoUKC: A concept-centered \nIndian multilingual lexicalresource. Proceedings of the Thirteenth \nLanguage Resources and Evaluation Conference (pp. 2833–2840). \nMarseille, France: European Language Resources Association. \nRetrieved from https://aclanthology.org/2022.lrec-1.303\nCoady, D. (2010). Two concepts of epistemic injustice. Episteme, 7(2), \n101–113. https:// doi. org/ 10. 3366/ epi. 2010. 0001\nDe-Arteaga, M., Romanov, A., Wallach, H., Chayes, J., Borgs, C., \nChouldechova, A., . . . Kalai, A.T. (2019a). Bias in bios: A case \nstudy of semantic representation bias in a high-stakes setting. , \n120–128. Retrieved from https:// doi. org/ 10. 1145/ 32875 60. 32875 \n72\nDe-Arteaga, M., Romanov, A., Wallach, H., Chayes, J., Borgs, C., \nChouldechova, A., . . . Kalai, A.T. (2019b). Bias in bios: A case \nstudy of semantic representation bias in a high-stakes setting. \nProceedings of the Conference on Fairness, Accountability, and \nTransparency (p. 120–128). Association for Computing Machin-\nery. Retrieved from https:// doi. org/ 10. 1145/ 32875 60. 32875  \n7210.1145/3287560.3287572\nDibitso, M. A., Owolawi, P. A., Ojo, S. O. (2019). Context-driven \ncorpus-based model for automatic text segmentation and part of \nspeech tagging in setswana using opennlp tool. Modeling and \nusing context: 11th International and Interdisciplinary Confer -\nence, Context 2019, November 20–22, 2019, proceedings 11 (pp. \n62–73).\nEngel, J. S. (2016). Global clusters of innovation: Entrepreneurial \nengines of economic growth around the world (Reprint (edition). \nEdward Elgar Pub.\nFricker, M. (2009). Epistemic injustice: Power and the ethics of know-\ning. Oxford University Press.\nFriedman, B., & Nissenbaum, H. (1996). Bias in computer systems. \nACM Transactions on Information Systems, 14(3), 330–347. \nhttps:// doi. org/ 10. 1145/ 230538. 230561\nGitelman, L. (2013). Raw data is an oxymoron. MIT Press.\nGiunchiglia, F., Batsuren, K., Bella, G. (2017). Understanding and \nexploiting language diversity. Ijcai (pp. 4009–4017).\nGiunchiglia, F., Batsuren, K., Freihat, A. A. (2018). One world–seven \nthousand languages. Proceedings 19th International Conference \non Computational Linguistics and Intelligent Text Processing, \nCicling2018, (pp. 18-24) March 2018.\nGiunchiglia, F., Bella, G., Nair, N. C., Chi, Y., & Xu, H. (2023). Rep-\nresenting interlingual meaning in lexical databases. Artificial \nIntelligence Review. https:// doi. org/ 10. 1007/ s10462- 023- 10427-1\nGoldman, A. I. (2002). 51the unity of the epistemic virtues. Pathways to \nknowledge: Private and Ublic. In Pathways to knowledge: Oxford \nUniversity Press.\nGreenberg, J. H. (1956). The measurement of linguistic diversity. Lan-\nguage, 32(1), 109–115.\nHaraway, D. (1988). Situated knowledges: The science question in \nfeminism and the privilege of partial perspective. Feminist Stud-\nies, 14(3), 575. https:// doi. org/ 10. 2307/ 31780 66\nHarding, S. (1995). Strong objectivity: A response to the new objectiv-\nity question. Synthese, 104(3), 331–349.\nHelm, P., Michael, L., Schelenz, L. (2022, Jul). Diversity by design? \nbalancing the inclusion and protection of users in an online social \nplatform. Proceedings of the 2022 aaai/acm Conference on ai, \nEthics, and Society (p. 324–334). Association for Computing \nMachinery. Retrieved from https:// doi. org/ 10. 1145/ 35140 94.  \n35341 4910.1145/3514094.3534149\nHelm, P., de Götzen, A., Cernuzzi, L., Hume, A., Diwakar, S., Ruiz \nCorrea, S., & Gatica-Perez, D. (2023). Diversity and neocolonial-\nism in big data research: Avoiding extractivism while struggling \nwith paternalism. Big Data & Society. https:// doi. org/ 10. 1177/ \n20539 51723 12068 02\n P . Helm et al.\n8 Page 14 of 15\nHovy, D., & Yang, D. (2021, June). The importance of modeling social \nfactors of language: Theory and practice. K. Toutanova et al. \n(Eds.), Proceedings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: \nHuman language technologies (pp. 588–602). Online: Association \nfor Computational Linguistics. Retrieved from https://aclanthol-\nogy.org/2021.naacl-main.49 10.18653/v1/2021.naacl-main.49\nHovy, D., & Prabhumoye, S. (2021). Five sources of bias in natural \nlanguage processing. Language and Linguistics Compass, 15(8), \ne12432. https:// doi. org/ 10. 1111/ lnc3. 12432\nIrani, L., Vertesi, J., Dourish, P., Philip, K., Grinter, R.E. (2010, Apr). \nPostcolonial computing: a lens on design and development. Pro-\nceedings of the Sigchi Conference on Human Factors in Com-\nputing Systems (p. 1311–1320). Association for Computing \nMachinery. Retrieved from https:// doi. org/ 10. 1145/ 17533 26.  \n17535 2210.1145/1753326.1753522\nJoshi, P., Santy, S., Budhiraja, A., Bali, K., Choudhury, M. (2020, July). \nThe state and fate of linguistic diversity and inclusion in the NLP \nworld. D. Jurafsky, J. Chai, N. Schluter, & J. Tetreault (Eds.), \nProceedings of the 58th Annual Meeting of the Association for \nComputational Linguistics (pp. 6282–6293). Online: Association \nfor Computational Linguistics. Retrieved from https://aclanthol-\nogy.org/2020.acl-main.560 10.18653/v1/2020.acl-main.560\nKhalilia, H., Bella, G., Freihat, A.A., Darma, S., Giunchiglia, F. (2023). \nLexical diversity in kinship across languages and dialects. To \nappear in Frontiers in Psychology, special issue on the adaptive \nvalue of language diversity. https://arxiv.org/abs/2308.13056 [cs.\nCL]\nKhishigsuren, T., Bella, G., Batsuren, K., Freihat, A.A., Nair, N.C., \nGanbold, A., Giunchiglia, F. (2022). Using linguistic typology to \nenrich multilingual lexicons: the case of lexical gaps in kinship. \narXiv preprint arXiv: 2204. 05049.\nKornai, A. (2013). Digital language death. PloS one, 8(10), e77056.\nLignos, C., Holley, N., Palen-Michel, C., Sälevä, J. (2022, May). \nToward more meaningful resources for lower-resourced lan-\nguages. Findings of the association for computational linguistics: \nAcl 2022 (pp. 523–532). Dublin, Ireland: Association for Compu-\ntational Linguistics. Retrieved from https://aclanthology.org/2022.\nfindings-acl.44 10.18653/v1/2022.findings-acl.44\nMazrui, A. M., & Mazrui, A. A. (1999). The political culture of \nlanguage: Swahili, society and the state. Global Academic \nPublishing.\nMiller, G. A. (1998). Wordnet: An electronic lexical database. MIT \npress.\nNyabola, N. (2018). Digital democracy, analogue politics: How the \ninternet era is transforming politics in kenya. Zed Books.\nOchigame, R. (2019, Dec). How big tech manipulates academia \nto avoid regulation. Retrieved from https://theintercept.\ncom/2019/12/20/mit-ethical-aiartificial-intelligence/\nPfotenhauer, S., & Jasanoff, S. (2017). Panacea or diagnosis? Imaginar-\nies of innovation and the ‘Mit model’ in three political cultures. \nSocial Studies of Science, 47(6), 783–810. https:// doi. org/ 10. 1177/ \n03063 12717 706110\nPotthast, T. (2014). The values of biodiversity: philosophical consid-\nerations connecting theory and practice. Concepts and values in \nbiodiversity. Routledge.\nRanciere, J. (1998). Disagreement: Politics and philosophy. University \nof Minnesota Press.\nRijkhoff, J., Bakker, D., Hengeveld, K., & Kahrel, P. (1993). A method \nof language sampling. Studies in Language. International Journal \nsponsored by the Foundation, 17(1), 169–203.\nSaad-Sulonen, J., Eriksson, E., Halskov, K., Karasti, H., & Vines, J. \n(2018). Unfolding participation over time: Temporal lenses in \nparticipatory design. CoDesign, 14(1), 4–16. https:// doi. org/ 10. \n1080/ 15710 882. 2018. 14267 73\nSchwartz, L. (2022, May). Primum Non Nocere: Before working with \nIndigenous data, the ACL must confront ongoing colonialism. \nProceedings of the 60th Annual Meeting of the Association \nfor Computational Linguistics (vol. 2: Short papers) (pp. 724–\n731). Dublin, Ireland: Association for Computational Linguis-\ntics. Retrieved from https://aclanthology.org/2022.acl-short.82 \n10.18653/v1/2022.acl-short.82\nSchwemmer, C., Knight, C., Bello-Pardo, E. D., Oklobdzija, S., \nSchoonvelde, M., & Lockhart, J. W. (2020). Diagnosing gender \nbias in image recognition systems. Socius. https:// doi. org/ 10. 1177/ \n23780 23120 967171\nSennrich, R., Haddow, B., Birch, A. (2015). Neural machine transla-\ntion of rare words with subword units. arXiv preprint arXiv: 1508. \n07909.\nSmith, R.C., Winschiers-Theophilus, H., Loi, D., de Paula, R.A., \nKambunga, A.P., Samuel, M.M., Zaman, T. (2021). Decoloniz -\ning design practices: Towards pluriversality. Extended Abstracts \nof the 2021 Chi Conference on Human Factors in Computing \nSystems. Association for Computing Machinery. Retrieved from \nhttps:// doi. org/ 10. 1145/ 34117 63. 34413 34\nSpivak, G. C. (1988). Can the subaltern speak. In L. Grossberg & \nC. Nelson (Eds.), Marxism and the interpretation of culture (pp. \n66–111). University of Illinois Press.\nTaylor, L., & Broeders, D. (2015). August). In the name of Devel-\nopment: Power, profit and the datafication of the global South. \nGeoforum, 64, 229–237. https:// doi. org/ 10. 1016/j. geofo rum. 2015. \n07. 002\nThiong’o, N. w. (1986). Decolonising the mind: The politics of lan-\nguage in african literature. N.H: Heinemann, Oxford.\nTsing, A. L. (2012). On nonscalability: The living world is not ame-\nnable to precision-nested scales. Common Knowledge, 18(3), \n505–524. https:// doi. org/ 10. 1215/ 09617 54X- 16304 24\nVanmassenhove, E., Shterionov, D., Gwilliam, M. (2021, April). \nMachine translationese: Effects of algorithmic bias on linguis-\ntic complexity in machine translation. Proceedings of the 16th \nConference of the European Chapter of the Association for Com-\nputational Linguistics: Main volume (pp. 2203– 2213). Online: \nAssociation for Computational Linguistics. Retrieved from \nhttps://aclanthology.org/2021.eacl-main.188 10.18653/v1/2021.\neacl-main.188\nWhite, J.C., & Cotterell, R. (2021, August). Examining the inductive \nbias of neural language models with artificial languages. Proceed-\nings of the 59th Annual Meeting of the Association for Computa-\ntional Linguistics and the 11th International Joint Conference on \nNatural Language Processing (vol. 1: Long papers) (pp. 454–463). \nOnline: Association for Computational Linguistics Retrieved from \nhttps://aclanthology.org/2021.acl-long.38 10.18653/v1/2021.\nacl-long.38\nWinner, L. (1988). The whale and the reactor: A search for limits \nin an age of high technology (Reprint (Edition). University of \nChicago Press.\nYoung, H. (2015). The digital language divide. Retrieved from https://\nlabs.theguardian.com/digital-language-divide/\nYoung, I. M. (1990). Justice and the politics of difference. Princeton \nUniversity Press.\nZaugg, I.A., Hossain, A., Molloy, B. (2022, Apr). Digitally-disadvan-\ntaged languages. Internet Policy Review, 11(2). Retrieved from \nhttps://policyreview.info/glossary/digitally-disadvantaged-lan-\nguages 10.14763/2022.2.1654\nZevallos, R., & Bel, N. (2023). Hints on the data for language modeling \nof synthetic languages with transformers. Proceedings of the 61st \nAnnual Meeting of the Association for Computational Linguistics \n(vol. 1: Long papers) (pp. 12508–12522).\nZouhar, V., Chang, K., Cui, C., Carlson, N., Robinson, N., Sachan, M., \nMortensen, D. (2023). Pwesuite: Phonetic word embeddings and \ntasks they facilitate. arXiv preprint arXiv: 2304. 02541.\nDiversity and language technology: how language modeling bias causes epistemic injustice  \n Page 15 of 15 8\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6116225719451904
    },
    {
      "name": "Injustice",
      "score": 0.5303595662117004
    },
    {
      "name": "Diversity (politics)",
      "score": 0.5283784866333008
    },
    {
      "name": "Linguistics",
      "score": 0.4890211522579193
    },
    {
      "name": "Representation (politics)",
      "score": 0.4583224356174469
    },
    {
      "name": "Sociology",
      "score": 0.43900078535079956
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3250797390937805
    },
    {
      "name": "Politics",
      "score": 0.31071728467941284
    },
    {
      "name": "Psychology",
      "score": 0.19243884086608887
    },
    {
      "name": "Political science",
      "score": 0.158937007188797
    },
    {
      "name": "Social psychology",
      "score": 0.12433171272277832
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I4210127572",
      "name": "IMT Atlantique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I161929037",
      "name": "Université de Bretagne Occidentale",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I159176309",
      "name": "Universität Hamburg",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I193223587",
      "name": "University of Trento",
      "country": "IT"
    }
  ],
  "cited_by": 39
}