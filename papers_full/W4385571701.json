{
  "title": "What Makes Pre-trained Language Models Better Zero-shot Learners?",
  "url": "https://openalex.org/W4385571701",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2168374293",
      "name": "Jing-Hui Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126530578",
      "name": "Dongsheng Zhu",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2141939313",
      "name": "Weidong Han",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2056298114",
      "name": "Rui Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A114965032",
      "name": "Brian Mac Namee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2006335743",
      "name": "Fei Tan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3197298549",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4385573504",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221140922",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4286769130",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W4382202624",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W4206214875",
    "https://openalex.org/W2889326414",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3182414670",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3198377975"
  ],
  "abstract": "Current methods for prompt learning in zero-shot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a real-world zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2288–2303\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nWhat Makes Pre-trained Language Models Better Zero-shot Learners?\nJinghui Lu1, Dongsheng Zhu+ 2, Weidong Han+ 2,\nRui Zhao 1, Brian Mac Namee 3, Fei Tan∗1\n1 SenseTime Research\n2 Fudan University\n3 School of Computer Science, University College Dublin\n{lujinghui1, zhaorui, tanfei}@sensetime.com\n{dszhu20, wdhan20}@fudan.edu.cn\n{brian.macnamee}@ucd.ie\nAbstract\nCurrent methods for prompt learning in zero-\nshot scenarios widely rely on a development set\nwith sufficient human-annotated data to select\nthe best-performing prompt template a poste-\nriori. This is not ideal because in a real-world\nzero-shot scenario of practical relevance, no\nlabelled data is available. Thus, we propose\na simple yet effective method for screening\nreasonable prompt templates in zero-shot text\nclassification: Perplexity Selection (Perplec-\ntion). We hypothesize that language discrep-\nancy can be used to measure the efficacy of\nprompt templates, and thereby develop a sub-\nstantiated perplexity-based scheme allowing\nfor forecasting the performance of prompt tem-\nplates in advance. Experiments show that our\nmethod leads to improved prediction perfor-\nmance in a realistic zero-shot setting, eliminat-\ning the need for any labelled examples.\n1 Introduction\nPrompt learning has been demonstrated to be a\nsuccessful remedy for challenges associated with\npre-training and fine-tuning paradigm, especially\nin zero/few-shot scenarios (Gao et al., 2021; Schick\nand Schütze, 2021a,b; Tam et al., 2021; Lu et al.,\n2022a).\nResearch has repeatedly shown that various\ntransformer-based language models can benefit\nfrom prompt learning. For example, decoder-only\nmodels, such as those in the GPT family (Brown\net al., 2020), can better generalise to unseen cases\nby prefixing inputs with a few training examples\n(in natural language). This is known as in-context\nlearning (Brown et al., 2020; Xie et al., 2021; Liu\net al., 2022a). Encoder-decoder models, such as\nT5 (Raffel et al., 2020) or BART (Lewis et al.,\n2020), can leverage prompt learning to train ver-\nsatile models for multiple tasks (Khashabi et al.,\n+Work was done during internship at SenseTime Research\n*Corresponding author\n2020; Lester et al., 2021). Bidirectional encoder-\nonly models, such as those in the BERT family (De-\nvlin et al., 2018; Liu et al., 2019), can also manifest\nimpressive zero-shot capacity when given proper\nprompts. These prompts often take the form of\npre-training tasks, such as next sentence predic-\ntion (Sun et al., 2022) or masked language model-\ning (MLM) (Gao et al., 2021; Schick and Schütze,\n2021a,b; Tam et al., 2021)—also known as cloze-\nstyle prompt learning.\nDespite its success in encoder-only models,\ncloze-style prompt learning is sensitive to the spe-\ncific involved templates. Multiple studies have\nshown that the design and choice of prompt tem-\nplates greatly affect the effectiveness of zero-shot\nlearning (Tam et al., 2021; Zhao et al., 2021; Ru-\nbin et al., 2022). Ideally, they are supposed to be\nas close as possible to the language used in down-\nstream task. For example, in a sentiment analy-\nsis task, a suitable template may be “[very/not]\npleased. ”that carries emotional information. How-\never, other templates can also be used here like\n“[very/not] good. ”.\nAs shown in Table 1, the performance of zero-\nshot learning using different sentiment-bearing\ntemplates can fluctuate significantly with different\nprompt templates. For the ECOMMERCE dataset,\nthe template “[very/not] pleased. ” achieves the\nbest zero-shot accuracy of 73.12%, while using\nthe template “[very/not] good. ”results in an accu-\nracy of only 55.68%—which is only slightly better\nthan random guessing. Additionally, if we choose\na sentiment-irrelevant template “[yellow/green]\nblack. ”, the accuracy significantly drops to 50.49%,\nindicating that the model has no classification abil-\nity. This shows that the performance of the model\nis largely shaped by templates used. Therefore,\nselecting the most appropriate templates for down-\nstream tasks is crucial in zero-shot learning.\nCurrent prompt learning methods still rely on\na development set of human-annotated data for\n2288\nDataset 1. [very/not] pleased. 2. [very/not] good. 3. [extremely/less] pleased. 4. [yellow/green] black.\nPPL Acc.(%) PPL Acc.(%) PPL Acc.(%) PPL Acc.(%)\nDOUBAN 24.61 57.12 40.93 50.98 28.80 56.68 71.01 51.31\nWEIBO 19.78 61.79 30.37 51.16 22.34 58.35 44.45 50.92\nWAIMAI 16.44 67.80 23.34 53.15 19.68 69.72 36.07 48.49\nECOMMERCE 14.07 73.12 18.45 55.68 16.88 67.49 28.56 50.49\nTable 1: Summary of mean perplexity scores and zero-shot accuracy of different prompt templates.\npost-hoc template selection (Tam et al., 2021; Sun\net al., 2022; Gao et al., 2021; Liu et al., 2021a): all\ncandidate templates are evaluated using the devel-\nopment set and the best-performing one is chosen.\nThis requires human annotators and does not align\nwell with realistic zero-shot learning scenarios in\nwhich no human-annotated data is available. To ad-\ndress this problem, we propose a truly annotation-\nfree perplexity-based template selection method for\nzero-shot prompt learning: Perplexity Selection\n(Perplection). Experiments show that Perplection\nis highly likely to select the most effective template\naccommodating true zero-shot scenarios.\nIn this paper, we first describe cloze-style prompt\nlearning and corresponding terminologies in Sec-\ntion 2. Then, in Section 3, we present our hy-\npothesis that underpins the work. Based on this\nhypothesis, in Section 4 we detail Perplection that\nuses perplexity to select templates a priori without\nthe need of any annotated examples. Section 5 de-\nscribes a pilot study and in Section 6, we present re-\nalistic experiments that show that Perplection leads\nto performance on par with other zero-shot prompt\nmethods that utilise a development set. Finally,\nSection 7 discusses the underlying rationales and\nthe potential impact of the work in a large language\nmodels (LLM) era.\nTo the best of our knowledge, we spearhead the\nperformance screening of prompt templates for a\nrealistic zero-shot text classification without using\nany human-annotated data.*\n2 Preliminaries\nIn this section, we describe basic concepts and\nterminologies associated with prompt learning.\n2.1 Prompt Learning\nNote that the prompting settings and terminolo-\ngies used in this work are mainly derived from the\nwork that focuses on manual/automatic cloze-style\ndiscrete templates (Gao et al., 2021; Schick and\n*Code is available at https://github.com/\nGeorgeLuImmortal/Perplection_ACL2023.\nSchütze, 2021a,b; Tam et al., 2021). As text clas-\nsification is well studied in prompt-based learning\ntasks (Liu et al., 2021a), we use a simple binary\nsentiment analysis task to demonstrate zero-shot\nprompt learning in our work. Specifically, given an\ninput text x, for example “I love this movie. ”, we\nare interested in classifying the sentiment polarity,\ny, of this input text, i.e., ++ for positive or −−for\nnegative. The cloze-style prompt method modifies\nthe input x and output y to further exploit the capa-\nbilities of pre-trained language models. Formally,\nwe first manipulate input text x to construct a new\ninput text, x′, by prefixing (or suffixing) x with a\ntemplate text sequence, t, that includes a“[MASK]”\ntoken. So, x′= [x, t] or x′= [t, x]. For example,\nif we have an input x =“I love this movie. ”and we\ndecide to prefix a template t =“Overall, it was a\n[MASK] movie. ”, x′will become “Overall, it was\na [MASK] movie. I love this movie. ”.\nNext, x′ is fed into a language model to pre-\ndict the likelihood with which different tokens fill\n“[MASK]”. This can be achieved by applying an\nMLM head. Usually, researchers use prior knowl-\nedge to limit the set of potential filled tokens to\nthose relevant to the task of interest. For example,\nin the sentiment classification example only two\ntokens would be considered: “good” and ‘bad”.\nWe call each of these a label word, w, (Liu et al.,\n2021a). Finally, we define a mapping function (or\nverbaliser) (Liu et al., 2021a), v, to reverse the pre-\ndicted label word back to the target y, for example\n{good:++, bad:−−}. In this way the prompting\nmethod unifies a binary classification objective into\nan MLM objective, reusing a MLM head to per-\nform zero-shot prediction.\n2.2 Language Discrepancy and Objective Gap\nPrevious research (Liu et al., 2021a) has shown that\nprompt learning can help pre-trained language mod-\nels better adapt to downstream tasks by bridging\nthe gap between pre-training and the downstream\ntask. To be specific, prompt learning allows pre-\ntrained language models to take on a greater role\n2289\nin prediction, rather than just extracting features.\nIn light of the above finding, we identify two ob-\nstacles to combining pre-training and a downstream\ntask: language discrepancy and the objective gap.\nThe objective gap describes the difference in train-\ning objectives between pre-training (e.g., next sen-\ntence prediction or MLM) and a downstream task\n(e.g., sequence classification or sequence labelling).\nLanguage discrepancy refers to the linguistic dif-\nferences between a pre-training corpus and down-\nstream datasets, including different vocabularies,\nword frequencies, syntactic arrangements, etc.\n3 Hypotheses\nThis section proposes two hypotheses that under-\npin our work, and describes the way they interpret\nobservations in the literature.\n3.1 Hypothesis I: Cloze-style Prompting\nOffers a Better Feature Space\nOur first hypothesis is that the use of a cloze-style\nprompt in text classification alters the input data\ndistribution in a way that encourages the input data\nto be more effectively represented in a new fea-\nture space. To illustrate this, Figure 2 presents a\nUMAP (McInnes et al., 2018) visualisation of a\nsentiment analysis dataset, WEIBO, with and with-\nout prompt templates. It is obvious that after being\nprompted with a task-specific template,“[very/not]\npleased. ”, data from different classes is much better\nseparated within the resultant feature space (Figure\n2(b)) than when no prompt template is used (Figure\n2(a)). This shows that a pre-trained language model\ncan inherit zero-shot capabilities when given ap-\npropriate prompts, even without using any human-\nannotated examples.\nSo how do pre-trained language models con-\nstruct such effective feature spaces? We conjec-\nture that this is because some knowledge of down-\nstream tasks has been implicitly encoded into mod-\nels through pre-training (e.g., MLM for encoder-\nonly model or Next Word Prediction for decoder-\nonly models). Prompt learning finds a method to\nuncover the knowledge obtained in pre-training.\nTherefore, in this paper, we refer to this feature\nspace as the “pre-trained feature space”.\n3.2 Hypothesis II: Language Discrepancy\nMeasures the Efficacy of Prompting\nAdditionally, we aim to understand what makes a\ntemplate effective at forming a useful pre-trained\nSuch a bad movie!+Example: [very/not] pleasedPPL: 18.17[very/not] okPPL: 27.56[very/not] goodPPL: 19.84[very/not] fond of itPPL: 11.28\nPLM:\nCompute Perplexity\n[MASK] fond of it. Such a bad movie!Zero-shot Prediction\nTemplates:\nFigure 1: The procedure of the Perplection approach.\nfeature space. We believe that the difference in\nlanguage between pre-training corpora and down-\nstream datasets after prompting can be used to as-\nsess the effectiveness of templates.\nFigure 2(c) shows an example. When the text in-\nputs are given a prompt that is unlikely to be used in\nsentiment analysis texts, “[yellow/green] black. ”,\nthe data from different classes is not well separated\nin the feature space (as compared to Figure 2(b)).\nWe believe that this is because models rarely en-\ncounter the text “yellow black” or “green black”\nprefixed in a sentiment-bearing text in the pre-\ntraining corpora, and that this language discrepancy\nlimits the model’s ability to effectively represent\nthe data. In contrast, expressions like “[very/not]\npleased. ”(Figure 2(b)) are often used in context\nrelated to emotions and therefore appear more fre-\nquently together with sentiment-bearing text in the\npre-training corpora. This makes it easier for the\nmodel to form a useful pre-trained feature space.\nBroadly speaking, we suppose that the objective\ngap has been greatly reduced by reformulating\nthe downstream task to use a prompt in text clas-\nsification. The inconsistency is largely due to the\nlanguage differences between the pre-training data\nand the downstream data. Using prompt templates\nhelps to align the downstream text with the text\nin a pre-training corpus with respect to language\ndiscrepancy. The smaller the language discrepancy\nbetween the pre-training data and the downstream\ndata that are being prompted, the more likely it is\nthat the data will be represented well in the feature\nspace, resulting in better zero-shot performance.\n4 Method\nAs discussed in Section 3, a heuristic approach can\nbe employed to select the most effective templates\nin zero-shot text classification. One way to do this\nis to utilise language discrepancy to “forecast” the\nperformance of different prompt templates. Specif-\n2290\n0 2 4 6 8 10\n1\n2\n3\n4\n5\n6\n7\nPositive\nNegative\n(a) No template\n4\n 2\n 0 2 4 6 8\n2\n0\n2\n4\n6\nPositive\nNegative (b) [very/not] pleased.\n6\n 4\n 2\n 0 2\n2\n0\n2\n4\n6\nPositive\nNegative (c) [yellow/green] black.\nFigure 2: UMAP visualisation of a sentiment analysis dataset WEIBO: (a) no template, (b) task-relevant template,\nand (c) irrelevant template. (Best viewed in color.)\nically, the prompt template that results in the low-\nest language discrepancy when prefixed to a given\ninput text can be considered the most effective.\nHowever, how can the language discrepancy be-\ntween downstream text and pre-training corpora\nbe measured? In this study, we propose using per-\nplexity (Brown et al., 1992) as an approximation\nof language discrepancy.\nPerplexity is one of the most common metrics\nfor evaluating language models, and is defined as\nthe exponential average negative log-likelihood of\na sequence:\nPPL(x) = exp\n{\n−1\nt\nt∑\ni\nlog pθ(xi |x<i)\n}\n(1)\nwhere x = [ x1, x2, ..., xt] is a tokenised text\nsequence; and log pθ(xi |x < i) is the log-\nlikelihood of the ith token conditioned on the pre-\nceding tokens x < i computed by a language\nmodel. Intuitively, given a certain language model,\nlower perplexity for a corpus of sentences indicates\na model is familiar with that corpus. Basically, the\nlanguage model with the lowest perplexity is cho-\nsen as the most reliable proxy for modelling the\ndistribution of the pre-training corpus.\nAnalogously, we assume that prompt templates\nresulting in low perplexity when prefixed to a given\ninput are likely to be effective templates, eliminat-\ning the need for a human-annotated development\nset, which is required in most previous work (Liu\net al., 2021a; Lester et al., 2021; Gao et al., 2021).\nSpecifically, as shown in Figure 1, we prefix origi-\nnal input x with various prompt templates to form\nnew prompted texts. For each template, since we\nhave two label words (i.e., “very” and “not”), one\noriginal input x will generate two prompted texts\n(i.e., “Very pleased. Such a bad movie!” and “Not\npleased. Such a bad movie!”). Then we compute\nthe mean perplexity score of these two prompted\ntexts as the score for the template. Finally, the\ntemplate (where the label words will be replaced\nwith \"[MASK]\" token) with lowest score is selected\nto be prefixed to the original input, constructing\nnew input x′(i.e., “[MASK] pleased. Such a bad\nmovie!”) to perform a zero-shot prediction. This is\nquite different from previous methods with dataset-\nspecific (Gao et al., 2021; Sun et al., 2022) or class-\nspecific templates (Zhou et al., 2022). We refer to\nthe method as Perplexity Selection (Perplection).\n5 Pilot Study\nThe aim of the pilot study described in this\nsection was to qualitatively validate the hypotheses\nproposed in Section 3, and to examine the utility\nof perplexity as a metric for screening prompt\ntemplates (another study that examines the utility\nof perplexity is presented in Appendix D). To this\nend, we manually curated four prompt templates as\nshown in Table 1. We then analysed the perplexity\nand zero-shot performance of each template,\nseeking to determine whether there is a correlation\nbetween perplexity and zero-shot performance.\n5.1 Datasets\nWe conducted the pilot study using four publicly\navailable Chinese sentiment analysis datasets from\nvarious domains. These datasets are: DOUBAN,\na movie review dataset; WEIBO, a social media\ncomment dataset; WAIMAI, a takeaway comment\n2291\ndataset; ECOMMERCE, an e-commerce dataset.\n5.2 Perplexity\nWe use the Chinese RoBERTa model* as the back-\nbone pre-trained model. Given a pre-trained lan-\nguage model, we use it to compute the mean\nperplexity of downstream datasets that are being\nprompted, to approximate the language discrep-\nancy. That is, lower perplexity indicates smaller\nlanguage discrepancy between the pre-training cor-\npus and the prompted downstream dataset.\nNote that perplexity, as originally defined,\napplies specifically to causal language models\n(i.e., autoregressive language models). As sug-\ngested in previous work (Liu et al., 2019; Salazar\net al., 2020), perplexity for bidirectional mod-\nels like BERT/RoBERTa can be made analogous\nto that for causal language models by replacing\nlog pθ(xi |x < i) with log pθ(xi |c) in Equation\n1. Here, c refers to the context text, which is the\nwhole sentence except for the ith token. This sug-\ngests that the perplexity of each token is not only\nconditioned on the preceding tokens but also the\nsucceeding tokens. We added a template to each\nexample, replaced the “[MASK]” with label words\nfrom the prediction problem, and calculated the\naverage perplexity for each example. We then av-\neraged the perplexity scores of all examples to get\nthe overall perplexity of the dataset.\nDuring preliminary experiments, however, we\nfound that this definition of perplexity has the draw-\nback of favouring longer sentences. That is, a sen-\ntence is assigned a lower perplexity, not because the\npre-trained language model is more able to model\nthis sentence (i.e., low language discrepancy), but\nrather because the text is longer. We conjecture that\nthis is due to the penalty term in Equation 1 that\ndivides the sum of log-likelihood by the sequence\nlength t. The detail of our preliminary experiments\nregarding perplexity are provided in Appendix A.\nThe focus of this pilot study, however, is to illus-\ntrate the impact of language discrepancy rather than\nfinding useful measures of perplexity. So, to mit-\nigate against the drawbacks of the perplexity def-\ninition the four datasets used in our experiments\nwere subsampled to include only sentences with\nbetween 14 and 15 words, as well as to enforce\na 50:50 class balance. Also, all hand-crafted tem-\nplates have similar lengths (in Chinese).\n*Available at https://huggingface.co/hfl/\nchinese-roberta-wwm-ext.\n5.3 Zero-shot Result Analysis\nThe accuracies achieved using different prompt\ntemplates for four datasets are shown in Table\n1. These results demonstrate that prompt learn-\ning can equip a pre-trained language model with\nzero-shot capability when proper templates are pro-\nvided. However, the performance of Template 4\n(i.e., “[yellow/green] black”) demonstrates that\n“unusual” prompting (i.e., texts that models are un-\nlikely to see during pre-training) has limited contri-\nbution to zero-shot prediction, which is consistent\nwith our expectation.\nTo conclude, the results of the pilot study verify\nour hypothesis that in prompt learning, task-related\ntemplates are more useful in shaping a good pre-\ntrained feature space. The big difference between\nzero-shot performance across different prompting\napproaches in the pilot study shows that it is cru-\ncial to search for ideal prompt templates in prompt\nlearning. We argue that this problem can be ad-\ndressed by using perplexity as discussed in the\nfollowing subsection.\n5.3.1 Perplexity Analysis\nTable 1 also conveys a very clear message that as\nperplexity goes up, the zero-shot performance be-\ncomes worse. For example, the perplexity of Tem-\nplate 1 decreases from 24.61 (DOUBAN), to 19.78\n(WEIBO), to 16.44 ( WAIMAI), to 13.71 ( ECOM-\nMERCE); while the zero-shot accuracy consis-\ntently increases from 57.12 (DOUBAN), to 61.79\n(WEIBO), to 67.80 ( WAIMAI), to 73.12 ( ECOM-\nMERCE). This pattern can also be observed for\nTemplates 2 and 3. Furthermore, when compar-\ning sentiment-bearing templates (Templates 1-3)\nto the sentiment-irrelevant template (Template 4)\nacross datasets, it is evident that the sentiment-\nirrelevant template consistently yields the highest\nperplexity and the lowest accuracy. The experimen-\ntal results can partially verify our hypotheses that\nas the language discrepancy decreases (i.e., lower\nperplexity), it is easier for prompts to align down-\nstream data to a pre-trained feature space. The next\nsection describes experiments that show how the\nPerplection approach takes advantage of this.\n6 Experiments\nIn this section, we demonstrate the proposed Per-\nplection approach in a more realistic and useful\nexperimental setting to verify whether we can use\nlanguage discrepancy to forecast the efficacy of\n2292\nBinary Classification Multi-class Classification\nManual Templates DOUBAN WEIBO W AIMAI ECOMMERCE EPRSTMT TNEWS CSLDCP IFLYTEK\nMRandomB 57.89 60.37 69.31 71.61 62.26 24.90 27.57 45.29\nMPerplectionB 59.86 64.71 79.01 81.78 67.86 29.05 23.36 47.76\nMRandomR 55.72 60.47 66.43 72.49 67.40 24.56 26.95 44.94\nMPerplectionR 60.74 66.50 75.49 85.12 76.89 35.92 36.75 55.88\nAutomatic Templates\nARandomB 54.27 52.39 56.57 58.52 53.18 28.45 37.77 51.17\nAPerplectionB 53.07 57.60 53.15 68.16 55.24 25.67 38.74 51.29\nARandomR 53.83 52.50 56.02 58.83 53.14 25.72 41.31 49.29\nAPerplectionR 59.21 67.04 72.19 73.94 53.11 27.34 39.31 51.18\nTable 2: Results for text classification datasets. B and R stand for BERT and RoBERTa models, respectively. The\nbolded entries represent the superior performance of the Perplection variant compared to its random counterpart.\nThe underlined entries denote the top-performing method among all variants.\nBinary Classification Multi-class Classification\nState-of-the-art Methods DOUBAN WEIBO W AIMAI ECOMMERCE EPRSTMT TNEWS CSLDCP IFLYTEK\nZero-PET(Schick and Schütze, 2021a) 51.64 51.52 56.71 60.82 59.51 22.58 32.19 75.29\nNSP-BERT(Sun et al., 2022) 60.85 68.58 83.69 91.11 79.67 49.55 48.43 78.82\nMPerplectionR 60.74 66.50 75.49 85.12 76.89 35.92 36.75 55.88\nTable 3: A comparison of the performance of Perplection with that of recent state-of-the-art methods.\nID Manual Template (binary) Manual Template (multi-class) Automatic Template ( TNEWS)\n1 [MASK] satisfied This belongs to [MASK] New [MASK] ：\n2 [MASK] fond of it The words belong to [MASK] Good [MASK] ：\n3 [MASK] pleased Actually it is [MASK] 《[MASK]》\n4 [MASK] pretty good Probably it is [MASK] Good [MASK] ！\n5 [MASK] happy The direction is [MASK] Net [MASK] ：\n6 [MASK] good This is due to [MASK] Good [MASK]|\n7 [MASK] ok Put it into [MASK] New [MASK]|\n8 - It means [MASK] . [MASK] ！\n9 - Obviously counted as [MASK] Good [MASK] ，\n10 - Obviously it is [MASK] In [MASK] ，\n11 - - New [MASK]:\nTable 4: The templates used for binary sentiment analysis and topic multi-class classification datasets. Due to space\nconsiderations, for automatically generated templates, we only present templates used in TNEWS. The red text\ndenotes Chinese punctuation marks. More details are provided in Appendix B.\nprompt templates for zero-shot classification.\n6.1 Datasets\nIn addition to the datasets mentioned in Section\n5.1, we also utilise four text classification datasets\nfrom the FewCLUE benchmark (Xu et al., 2021):\nEPRSTMT (e-commerce comment sentiment\nanalysis), CSLDCP (scientific literature subject\nclassification), TNEWS (news classification), and\nIFLYTEK (APP description topic classification).\nTo evaluate whether Perplection can be extended\nto other languages, we also evaluate Perplection\non three English datasets: SST-2 (sentiment\nanalysis) (Wang et al., 2018), TweetEval (hate\nspeech detection) (Barbieri et al., 2020), and AG\nNews (multi-class topic classification) (Zhang\net al., 2015). Note that in contrast to the pilot study,\nin these experiments we did not subsample the\ndatasets to make their sentences the same length.\n6.2 Setup\nAll manually crafted templates are presented in Ta-\nble 4. All the verbalisers and manual templates for\nEnglish datasets can be seen in Appendix C. We\nperform Perplection based on these manually de-\nsigned templates (MPerplection). If perplexity is\nan ideal metric, the performance of this method will\nbe better than random template-example matching\n(MRandom). We then construct a more aggres-\nsive setting where templates are generated auto-\nmatically by LM-BFF algorithm (Gao et al., 2021)\n(more detail is included in Appendix B) and ap-\n2293\nply similar template selection procedures to those\ndescribed for manually crafted templates. These\nare dubbed APerplection and ARandom. In or-\nder to obtain a robust assessment of the random\nvariants, we conduct five independent runs of the\nexperiments using different random seeds and re-\nport the average results. Note that both manually\ncrafted and automatically generated templates are\nconstructed to have similar lengths.\nWe report the results based on both RoBERTa\nand BERT* to demonstrate the proposed method\nis agnostic to the pre-trained model used. We also\nreport the performance of another two state-of-\nthe-art zero-shot prompting-based methods: NSP-\nBERT (Sun et al., 2022), and Zero-PET (Schick\nand Schütze, 2021a; Xu et al., 2021). They are\nstrong baselines whose settings comply with the\ncorresponding work (further implementation de-\ntails are provided in Appendix C).\n6.3 Results\nComparison to random baselines: The results\nof the Perplection variants and their corresponding\nrandom counterparts were compared in Table\n2. It can be seen that when using manually\ncrafted templates with both BERT and RoBERTa,\nPerplection was able to actively select more useful\ntemplates compared to the random selection,\nas indicated by the significant improvement in\nperformance (MRandomB vs. MPerplectionB\nand MRandomR vs. MPerplectionR). Also, when\nusing automatically generated templates, Perplec-\ntion is able to choose more effective templates,\nparticularly when using RoBERTa (ARandomR vs.\nAPerplectionR). These findings suggest that the\ntemplates selected by perplexity are more useful\nand deliver better performance. However, results\nalso show that Perplection is less effective when\nautomatically generated templates are used, which\nwill be discussed in the next section.\nManual templates vs. automatic templates: Ta-\nble 2 shows that variants using manually generated\ntemplates outperform their counterparts using auto-\nmatically generated templates. We conjecture that\nthe poor quality of automatically generated tem-\nplates may hinder the performance of Perplection.\nIn other words, the pool of automatically gener-\nated templates may be insufficient in diversity for\nPerplection to have an impact.\n*https://huggingface.co/bert-base-chinese\nDatasets EPRSTMT TNEWS CSLDCP IFLYTEK\nManual Std. 57.26 68.39 1.51 6.28\nAutomatic Std. 32.78 50.50 1.45 5.46\nTable 5: Comparison of perplexity standard deviation.\nDatasets SST-2 TweetEval AG News Avg.\nMRandomB 67.13 52.39 41.31 53.61\nMPerplectionB 68.17 53.67 43.92 55.25\nMRandomR 58.79 54.65 36.85 50.09\nMPerplectionR 57.96 55.16 42.30 51.81\nTable 6: Results for three English classification datasets.\nAs illustrated in Table 4, the majority of auto-\nmatic template texts display minimal variations\nand lack coherence, which is in stark contrast to\nthe manual templates. In this case, templates tend\nto generate similar perplexities, leading to little\ndistinction between them based on perplexity. To\nillustrate this, we report the standard deviation of\nperplexity for both manual templates and automatic\ntemplates in Table 5. It can be observed that for\nall datasets, the standard deviation of perplexity for\nmanual templates is higher than that of automatic\ntemplates, showing that perplexity is more useful\nwhen the templates are of higher diversity.\nIt is suspected that the quality of the automati-\ncally generated templates is constrained by the ca-\npacity of the pre-trained T5 model. We believe that\nthis can be improved by changing the T5 backbone\nor resorting to other methods that automatically\ngenerate templates using annotation information\n(Lester et al., 2021; Liu et al., 2021b; Li and Liang,\n2021; Liu et al., 2022b). We leave these explo-\nrations for future work.\nComparison to state-of-the-art approaches:\nWe compare our best performing method (MPer-\nplectionR) with other state-of-the-art zero-shot\nmethods, results are shown in Table 3. We find\nthat the performance of Perplection consistently\nsurpasses Zero-PET for all datasets by a large mar-\ngin except for TNEWS, and is competitive with\nNSP-BERT in some datasets such as DOUBAN\n(60.74 vs. 60.85). Note that both Zero-PET and\nNSP-BERT used a human-annotated development\nset to select the most suitable templates while Per-\nplection does not require any annotated data.\nFor the IFLYTEK dataset, Perplection seems less\ncompetitive as compared to Zero-PET and NSP-\nBERT. Specifically, the latter two methods heav-\nily rely on the post-hoc selected template “This\n2294\nis a [MASK] app. ” (see Appendix C) with the\ndevelopment set quite close to target domain of\ninterest, whereas Perplection has more generic tem-\nplates (in Table 4, those prompts are task-related\nbut not domain-relevant). Thus, the suboptimal\nperformance of Perplection can also be explained\nby our hypothesis that generic templates are less\neffective at aligning the downstream data into a\npre-trained feature space compared to those fine-\ngrained domain-specific templates. We suspect that\nthis can be addressed by providing Perplection with\nseveral domain-related fine-grained templates to se-\nlect from. We leave these explorations for future\nwork. All observations, however, show that it is\neffective to use perplexity to rate templates and\nselect desired ones accordingly.\nResults on English datasets: Table 6 compares\nthe performance of Perplection to random baselines\non three English datasets. Perplection consistently\ntops the comparison in almost all cases except for\nSST-2 with RoBERTa. This observation supports\nthe supposition that Perplection is agnostic to\nthe pre-trained model used, and shows that it is\npromising to extrapolate results to other languages.\n6.4 In-depth Analysis\nWe conduct an in-depth analysis based on MPer-\nplectionR. For brevity, we apply each manual\nprompting setting to all examples from the four\ndatasets (i.e., DOUBAN, WEIBO, WAIMAI, ECOM-\nMERCE) and aggregate the accuracy score as a\npost-hoc measurement of template quality. For\neach template, we also compute its frequency of\nbeing selected. The results are presented in Figure\n3. It shows that templates with lower perplexity\nare more likely to achieve better performance. To\nbe specific, there is 60% chance for Perplection\nto select the second best performing template (i.e.,\n“[MASK] fond of it. ”) and around 10% chance to\nselect the best performing template (i.e., “[MASK]\nsatisfied. ”). For templates with no discriminative\nability e.g., “[MASK] good. ”and “[MASK] ok. ”,\nour method has almost no chance to select them.\nMost importantly, the selection based on perplexity\nis annotation-agnostic and allows us to “foresee”\nthe result to some extent without the need of a\nhuman-annotated development set. To conclude,\nthe results demonstrate that perplexity is a reason-\nable metric for evaluating prompting settings.\n60 40 20 0 20 40 60 80\nFrequency (left) vs. Mean Accuracy (right)\n[MASK] ok.\n[MASK] good.\n[MASK] happy.\n[MASK] pretty good.\n[MASK] pleased.\n[MASK] fond of it.\n[MASK] satisfied.\nFigure 3: Normalised frequency of being selected vs.\ntemplate quality measured by mean accuracy.\n7 Discussion\nWhat contributes better zero-shot learners?\nThis work empirically reveals that the large lan-\nguage discrepancy between the pre-training cor-\npora and the downstream data may hinder the zero-\nshot generalization. On top of that, we develop a\nperplexity-based scheme that leverages cloze-style\nprompt templates to bridge language discrepancy\nand thus, fully releases the potential of pre-trained\nlanguage models. The significance of this work\nlies in its pioneering study of a feasible objective\nfor optimising REALISTIC zero-shot prompting\ntemplates. The idea may be applied to various\nvariations (e.g., continuous prompts) beyond the\ndiscrete prompts currently being studied.\nWhy REALISTIC zero-shot matters? In this\nwork, we constantly emphasise a realistic zero-shot\nscenarios (no labelled data), as opposed to the exist-\ning zero-shot setting in the field of NLP (Xu et al.,\n2021; Sun et al., 2022) or Multi-modality (Radford\net al., 2021), where a development set is available\nfor template selection or hyper-parameter tuning.\nRealistic zero-shot can be quite appealing for in-\ndustrial scenarios and thus, this research opens up\na new avenue for research in the field of zero-shot\nlearning, probably inspiring follow-up studies in\nbroader tasks for advancing the zero-shot learn-\ning in industrial applications (especially in many\nlow-resource scenarios).\nPotential impact in the LLM era. In light of\nthe advancements in large language models (LLM)\nbased on the decoder-only architecture (Zhao et al.,\n2023), searching for effective instructions or in-\ncontext demonstration examples (Zhang et al.,\n2022) has become an essential challenge. Per-\nplection can be seamlessly applied to decoder-\nonly models for searching effective instructions/in-\ncontext examples for various natural language gen-\n2295\neration (NLG) tasks. We make our code available\nfor replication and further extension to NLG tasks\nby the community.\n8 Conclusion\nWe developed Perplexity Selection Prompt (Per-\nplection) a method that enables real-world zero-\nshot text classification without the use of any\nhuman-annotated data. A pilot study demonstrated\nthat Perplexity can be an effective measure of the\nefficacy of templates. Experimental results show\nthat, for datasets in both English and Chinese, our\nmethod can boost zero-shot performance of cloze-\nstyle prompt learning in binary sentiment analysis\nas well as multi-class classification, without using\na development set. Further in-depth analysis sup-\nports the observation that Perplection can “foresee”\nthe efficacy of prompt templates.\n9 Limitations\nIn this study, we mainly utilised the BERT fam-\nily of models for Chinese text classification tasks.\nGiven the similarity with respect to transformer lan-\nguage models and pre-training paradigms, as well\nas the preliminary results on English datasets as dis-\ncussed in Section 6.3, we may be able to extrapolate\nthe results to other architectures/tasks/languages.\nFor example, Perplection can be seamlessly ap-\nply to decoder-only models (e.g., GLM (Du et al.,\n2022), LLaMA (Touvron et al., 2023)) to see\nwhether it can boost the performance for those\nNLG tasks. But further investigation is needed\nto verify the utility of findings on other model ar-\nchitectures, tasks, and languages. In the future, we\nexpect to see Perplection applied to different NLG\ntasks such as seq2seq information extraction (Lu\net al., 2022b), question answering, arithmetic rea-\nsoning, machine translation or even multi-modality\ntasks.\nAlso, utilising Perplection may exacerbate the\ninherent limitations of pre-trained language mod-\nels. We suspect that, in instances where the model\nhas not been exposed to certain texts or concepts\nduring pre-training, reliance on perplexity for tem-\nplate selection may result in subpar performance.\nIn the future, we expect to explore whether we can\nalleviate this problem by certain annotation-free\nmethods, such as continuous self-supervised train-\ning with downstream data, or extend our method in\na few-shot setting where limited label information\nis available.\nBesides, the use of perplexity as a metric has the\ndrawback of favoring long texts, which forces us\nto design templates of the same length. Therefore,\na length-agnostic metric can be considered as an\nalternative.\n10 Ethics Statement\nWe honor the ACL Code of Ethics. No private\ndata or non-public information was used in this\nwork. We conducted our research in an objective\nand unbiased manner. We take full responsibility\nfor the content of this paper and stand behind the\naccuracy and integrity of our work.\nAcknowledgements\nWe would like to thank anonymous reviewers for\ntheir insightful comments to help improve the paper.\nThis publication has emanated from research con-\nducted with the support of SenseTime Research.\nReferences\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644–1650, Online. Association for Computational\nLinguistics.\nPeter F. Brown, Stephen A. Della Pietra, Vincent J.\nDella Pietra, Jennifer C. Lai, and Robert L. Mercer.\n1992. An estimate of an upper bound for the entropy\nof English. Computational Linguistics, 18(1):31–40.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\n2296\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1896–1907, Online. Association\nfor Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022a. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJinghui Lu, Linyi Yang, Brian Namee, and Yue Zhang.\n2022a. A rationale-centric framework for human-\nin-the-loop machine learning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n6986–6996, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nJinghui Lu, Rui Zhao, Brian Mac Namee, and Fei Tan.\n2022b. Punifiedner: a prompting-based unified ner\nsystem for diverse datasets. ArXiv, abs/2211.14838.\nLeland McInnes, John Healy, Nathaniel Saul, and Lukas\nGroßberger. 2018. Umap: Uniform manifold ap-\nproximation and projection. Journal of Open Source\nSoftware, 3(29):861.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655–2671, Seattle, United States.\nAssociation for Computational Linguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2699–2712, Online. Association for Computational\nLinguistics.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\n2297\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nYi Sun, Yu Zheng, Chao Hao, and Hangping Qiu.\n2022. NSP-BERT: A prompt-based few-shot learner\nthrough an original pre-training task —— next sen-\ntence prediction. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 3233–3250, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nDerek Tam, Rakesh R. Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4980–4991,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and\nTengyu Ma. 2021. An explanation of in-context learn-\ning as implicit bayesian inference. arXiv preprint\narXiv:2111.02080.\nLiang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei\nZhang, Huilin Xu, Hu Yuan, Guoao Wei, Xiang Pan,\nXin Tian, Libo Qin, et al. 2021. Fewclue: A chi-\nnese few-shot learning evaluation benchmark. arXiv\npreprint arXiv:2107.07498.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In NIPS.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-\ntive example selection for in-context learning. InPro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 9134–\n9148, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12697–12706.\nPMLR.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022. Learning to prompt for vision-\nlanguage models. International Journal of Computer\nVision, 130(9):2337–2348.\nA Issue of Perplexity\nWe find that the current perplexity definition has\nthe drawback of favouring longer sentences. That\nis, a sentence is assigned a lower perplexity, not\nbecause the pre-trained language model can more\neasily model this sentence (i.e., lower language dis-\ncrepancy), but rather because the text is longer. We\nfirst use a simple comparison to demonstrate this\nas shown in Table 7. We calculate the perplexity\nof a meaningful sentence “Auntie: Don’t be too\ntired [haha]” which is 17.21. However, if we pre-\nfix this sentence with a long sequence of nonsense\nwords, the perplexity even gets lower, i.e., 5.85.\nWe then conduct a large scale test to see the cor-\nrelation between perplexity and text length. The\nresults are presented in Figure 4, it is obvious that\nthe avg. perplexity is inversely proportional to avg.\ntext length. In other words, a low perplexity of a\nsentence is partially contributed by a low language\ndiscrepancy but more likely to be contributed by a\nlong text, which challenges our use of perplexity\nto measure language discrepency.\nFigure 4: Line chart of average perplexity and average\ntext length across different datasets. The x-axis repre-\nsents the dataset, the blue line is the mean perplexity\nscore while the orange line is the mean text length.\n2298\nText in Chinese Translation Perplexity\n阿姨：不要太累了[哈哈] Auntie: Don’t be too tired [haha] 17.21\n撒娇大法，啊的身份拉升大盘撒娇大法，啊\n的身份拉盘。阿姨：不要太累了[哈哈]\nCoquetry Dafa, ah’s identity pulls up the big market Coquettish Dafa,\nah’s identity pulls the plate. Auntie: Don’t be too tired [haha]5.85\nTable 7: Comparison of a long nonsense sentence with a short fluent sentence.\nDataset Mapping\nTNEWS\n{100:’故事’ (story),101:’文化’ (cultural),102:’娱乐’ (entertainment),103:’体育’ (sports),\n104:’财经’ (finance),106:’房产’ (real estate),107:’汽车’ (automobile),108:’教育’ (education),\n109:’科技’ (technology),110:’军事’ (military),112:’旅游’ (trip),113:’国际’ (world-wide),\n114:’股票’ (stock),115:’农业’ (agricultural),116:’电竞’ (e-sports)}\nCSLDCP\n{’材料科学与工程’: ’材料’ (Materials),’力学’: ’力学’ (Mechanics),\n’园艺学’: ’园艺’ (Horticulture),’水产’: ’水产’ (Aquaculture), ’航空宇航科学与技术’: ’航空’ (Aerospace Science),\n’建筑学’: ’建筑’ (Architecture),’林学/林业工程’: ’林业’ (Forestry ), ’天文学’: ’天文’ (Astronomy),\n’机械工程’: ’机械’ (Mechanical),’地理学’: ’地理’ (Geography), ’大气科学’: ’大气’ (Atmospheric Science),\n’测绘科学与技术’: ’测绘’ (Geodesy),’军事学’: ’军事’ (Military Science),’新闻传播学’: ’新闻’ (Journalism),\n’植物保护’: ’植物’ (Plant)}\nIFLYTEK\n{107: ’团购’ (group buy),110: ’超市’ (supermarket),113: ’办公’ (office),18: ’动作’ (motion),2: ’免费’ (free),\n30: ’情侣’ (dating),3: ’租车’ (ride-hailing),42: ’百科’ (encyclopedia),48: ’音乐’ (music), 64: ’民航’ (airline),\n75: ’汽车’ (automobile), 87: ’美妆’ (makeup),89: ’餐饮’ (food),91: ’运动’ (fitness),92: ’支付’ (payment)}\nTable 8: The mapping of class names to label words with equal length. Translations are provided in brackets.\nTask Perplection Zero-PET NSP-BERT\nSentiment Analysis datasets(i.e., W AIMAI, WEIBO,DOUBAN, ECOMMERCE,EPRSTMT)\nTemplate1: [MASK]满意。([MASK] satisfied.)Template2: [MASK]喜欢。([MASK] font of it.)Template3: [MASK]高兴。([MASK] pleased.)Template4: [MASK]可以。([MASK] pretty good.)Template5: [MASK]开心。([MASK] happy.)Template6: [MASK]好。([MASK] good.)Template7: [MASK]行。([MASK] ok.)\nLabel words:很;不(very; not)\nTemplate:这次买的东西很[MASK]。(The things I bought this time is very [MASK].)\nLabel words:好;差(good; bad)\nTemplate:这次买的东西很[MASK].(The things I bought this time is very [MASK].)\nLabel words:好;差(good; bad)\nTNEWS Template1:这属于是[MASK]。(This belongs to [MASK])Template2:此话属于[MASK]。(The words belong to [MASK])Template3:实际上，[MASK]。(Actually it is [MASK])Template4:应该算是[MASK]。(Probably it is [MASK])Template5:方向为[MASK]。(The direction is [MASK])Template6:归功于[MASK]。(This is due to [MASK])Template7:给它放到[MASK]。(Put it into [MASK])Template8:它意思是[MASK]。(It means [MASK])Template9:明显算[MASK]。(Obviously counted as [MASK])Template10:显而易见[MASK]。(Obviously it is [MASK])\nLabel words (TNEWS):故事;文化;娱乐...(story; cultural; entertainment ...)\nLabel words (CSLDCP):材料;力学;园艺...(Materials; Mechanics; Horticulture...)\nLabel words (IFLYTEK):团购;超市;办公...(group buy; supermarket; office...)\nTemplate:这是一则[MASK]新闻。(This is a [MASK] news.)\nLabel words:故事;文化;娱乐... (story; cultural; entertainment...)\nTemplate:这是一则[MASK]新闻. (This is a [MASK] news.)\nLabel words:故事;文化;娱乐... (story; cultural; entertainment...)\nCSLDCP Template:这是一篇[MASK]论文。(This is a [MASK] paper.)\nLabel words:材料;力学;园艺... (Materials; Mechanics; Horticulture...)\nTemplate:这是一则[MASK]论文. (This is a [MASK] paper.)\nLabel words:材料;力学;园艺... (Materials; Mechanics; Horticulture...)\nIFLYTEK Template:这是一款[MASK]类软件。(This is a [MASK] app.)\nLabel words:团购;超市;办公... (group buy; supermarket; office...)\nTemplate:这是一则[MASK]类软件. (This is a [MASK] app.)\nLabel words:团购;超市;办公... (group buy; supermarket; office...)\nTable 9: Manually generated templates and label words for Perplection, and other baselines Zero-PET and NSP-\nBERT. For Perplection and Zero-PET, we prefix the template. For NSP-BERT, we suffix the template as suggested\nin (Sun et al., 2022). Due to space considerations, we have omitted some label words, which can be referred to in\nTable 8. Translations are provided in brackets.\nB Automatic Template Generation\nSimilar to Gao et al. (2021), for the DOUBAN,\nWEIBO, WAIMAI, and ECOMMERCE datasets we\nfix the verbaliser to {very: ++, not: −−}, and use\nT5-v1.1-base-chinese* to automatically generate\ntemplates. Specifically, Gao et al. (2021) assume a\n*https://huggingface.co/uer/\nt5-base-chinese-cluecorpussmall .\nfew-shot scenario using ground truth label word as\nwell as corresponding examples to generate a num-\nber templates. They then sort generated templates\nbased on the aggregated generation probability (the\ncalculation of generation probability also needs la-\nbel information) of the whole training set. However,\nour experiment assumes a zero-shot scenario with\nno labelled data. Thus, for each dataset, we first\nrandomly sample 50 examples from the pool. For\n2299\nDataset Templates Label Words\nSST-2\nTemplate1: that sounds like [MASK]\nTemplate2: this is obviously [MASK]\nTemplate3: it should be [MASK]\nTemplate4: actually, it’s [MASK]\nTemplate4: in fact, it’s [MASK]\nTemplate5: it’s very [MASK]\nTemplate6: it is [MASK]\nTemplate7: I mean it’s [MASK]\nTemplate8: it means [MASK]\nTemplate10: I think [MASK]\n{’negative’: ’negative’, ’positive’: ’positive’}\nTweetEval\nTemplate1: that sounds like [MASK]\nTemplate2: this is obviously [MASK]\nTemplate3: it should be [MASK]\nTemplate4: actually, it’s [MASK]\nTemplate4: in fact, it’s [MASK]\nTemplate5: it’s very [MASK]\nTemplate6: it is [MASK]\nTemplate7: I mean it’s [MASK]\nTemplate8: it’s like [MASK]\nTemplate10: whatever it is [MASK]\n{0: ’positive’, 1: ’negative’}\nAG News\nTemplate1: this is [MASK]\nTemplate2: it is [MASK]\nTemplate3: I mean [MASK]\nTemplate4: actually, answer is [MASK]\nTemplate5: it should be [MASK]\nTemplate6: in fact, it’s [MASK]\nTemplate7: the sentence is [MASK]\nTemplate8: it belongs to [MASK]\nTemplate9: this news is [MASK]\nTemplate10: in my opinion [MASK]\n0: ’world’, 1: ’sports’, 2: ’business’, 3: ’science’\nTable 10: Manually generated templates and label words for Perplection in English datasets.\neach example, we use label words indicating both\nsentiments to generate templates, one for each sen-\ntiment, resulting in 100 templates in total. Then we\nremove duplicate templates, leaving around 59-73\ntemplates remain per dataset respectively.\nFor the EPRSTMT, TNEWS, CSLDCP, and IFLY-\nTEK datasets, whose automatically generated tem-\nplates have been made available,*, we directly use\nthose existing generated templates. We remove\nduplicate templates and around 11-22 templates\nremain per dataset. All automatically generated\ntemplates can be seen at URL masked for anony-\nmous review.\nDatasets 1. [very/not] pleased. 2. [yellow/red] black.\nPPLg PPLr Diff. PPL g PPLr Diff.\nDouban 24.10 25.12 -1.02 67.91 74.11 -6.20\nWeibo 19.17 20.39 -1.22 44.39 44.51 -0.12\nWaimai 16.06 16.82 -0.76 22.60 24.07 -0.20\nOnline-shopping13.55 14.58 -1.03 28.51 28.61 -0.10\nTable 11: Mean perplexity of prompting with ground\ntruth label word (PPL g), prompting with reversed la-\nbel word (PPLr), and difference between two templates\ncomputed by PPLg minus PPLr (Diff.).\n*https://github.com/CLUEbenchmark/FewCLUE/\ntree/main/baselines/models_pytorch/LM-BFF/my_\nauto_template.\nC Implementation Details\nIn the implementation of Zero-PET, we use the pre-\ntrained Chinese-RoBERTa-wwm-ext model, which\nis identical to the model employed in Perplection.\nFor NSP-BERT, we use google BERT-Chinese.*\nTemplates and label words for both baselines fol-\nlow the best-performing setting reported in (Sun\net al., 2022; Xu et al., 2021), as shown in Table 9.\nThe manual generated templates (in Chinese) for\nPerplection are also shown in Table 9. A conver-\nsion is conducted to map class names to label words\nfollowing (Xu et al., 2021) to ensure all prefixed\ntexts have similar length, as shown in Table 8. For\nthe CSLDCP and IFLYTEK datasets we randomly\nsubsample 15 classes to facilitate the experiments.\nIn the implementation of English Perplection\nand its random counterparts, we use the pre-trained\nBERT-base-uncased* and RoBERTa-base* models.\nTemplates and label words for English experiments\nare shown in Table 10. All experiments are con-\nducted on a Tesla V100 GPU with 32GB memory.\n*https://huggingface.co/bert-base-chinese.\n*https://huggingface.co/bert-base-uncased\n*https://huggingface.co/roberta-base\n2300\nD Reverse Label Words\nTo briefly verify whether perplexity can be used\nto measure the quality of prompting, we perform\na very simple experiment where we compute the\nmean perplexity score of prompted input x′with\n“[MASK]” filled by ground truth label words for\neach dataset (called PPLg ). Then we reverse the\nlabel words filled in previous input examples (e.g.,\nwe change “very pleased. ”to “not pleased. ”in a\npositive sentiment example) and recompute mean\nperplexity score (called PPLr). Note that this ex-\nperiment is based on RoBERTa. The results of this\nare shown in Table 11.\nFirst, we notice that in Setting 1 (i.e.,“[very/not]\npleased. ”), the mean perplexity of PPLg is always\nsmaller than that of PPLr by a clear margin which\nis encouraging. This shows that the pre-trained\nmodel can perceive the change of semantics in\ntexts. When we see the perplexity of Setting 2 (i.e.,\n“[yellow/red] black. ”, we find out the magnitude of\nchange is much smaller, which demonstrates that re-\nplacing label words makes almost no difference to\nmodels if domain-irrelevant prompting is applied.\n2301\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 9 Limitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 9 Limitations\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1 Introduction\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 5.2 Issue of Perplexity, Section 6.2 Setup, Appendix C Implementation Details\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n2302\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5.2 Issue of Perplexity, Section 6.2 Setup, Appendix A Issue of Perplexity, Appendix B\nAutomatic Template Generation, Appendix C Implementation Details,\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 6.3 Results, Section 6.4 In-depth Analysis\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 5.2 Perplexity, Section 6.2 Setup, Appendix A Issue of Perplexity, Appendix B Automatic\nTemplate Generation, Appendix C Implementation Details,\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n2303",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9040166735649109
    },
    {
      "name": "Computer science",
      "score": 0.8020878434181213
    },
    {
      "name": "Template",
      "score": 0.6542115807533264
    },
    {
      "name": "Relevance (law)",
      "score": 0.6288127899169922
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.6285942196846008
    },
    {
      "name": "Shot (pellet)",
      "score": 0.588228166103363
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5834453701972961
    },
    {
      "name": "Language model",
      "score": 0.5534048676490784
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.49834275245666504
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4980940818786621
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.47259458899497986
    },
    {
      "name": "One shot",
      "score": 0.4554630517959595
    },
    {
      "name": "Machine learning",
      "score": 0.4518463611602783
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4310082793235779
    },
    {
      "name": "Ideal (ethics)",
      "score": 0.4217422902584076
    },
    {
      "name": "Data mining",
      "score": 0.2284134030342102
    },
    {
      "name": "Mathematics",
      "score": 0.10845485329627991
    },
    {
      "name": "Programming language",
      "score": 0.0720197856426239
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "cited_by": 5
}