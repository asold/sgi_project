{
  "title": "PLM-AS: Pre-trained Language Models Augmented with Scanpaths for Sentiment Classification",
  "url": "https://openalex.org/W4317829471",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2123594176",
      "name": "Duo Yang",
      "affiliations": [
        "University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A2795629946",
      "name": "Nora Hollenstein",
      "affiliations": [
        "University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A2123594176",
      "name": "Duo Yang",
      "affiliations": [
        "University of Copenhagen"
      ]
    },
    {
      "id": "https://openalex.org/A2795629946",
      "name": "Nora Hollenstein",
      "affiliations": [
        "University of Copenhagen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3089286977",
    "https://openalex.org/W4220759550",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W4292793998",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963948415",
    "https://openalex.org/W2905110202",
    "https://openalex.org/W3169754410",
    "https://openalex.org/W2250338295",
    "https://openalex.org/W2964194677",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3166275168",
    "https://openalex.org/W2739890004",
    "https://openalex.org/W2788643701",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2053631137",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W4295160097",
    "https://openalex.org/W2013112874",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W3041498386",
    "https://openalex.org/W2513122578",
    "https://openalex.org/W3042199158",
    "https://openalex.org/W2757442264",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Recent research demonstrated that deep neural networks could generate meaningful feature representations from both eye-tracking data and sentences without designing handcrafted features, which achieved competitive performance across cognitive NLP tasks, such as sentiment classification over gaze datasets, but the previous works mainly encode the text and gaze data separately without considering the interaction between these two modalities or applying large-scaled pre-trained models. To address these challenges, we introduce PLM-AS, a novel framework to take full advantage of textual and eye-tracking features by sequence modeling in a highly interactive way for multimodal fusion. It is also the first attempt to combine large-scaled pre-trained models with eye-tracking features in the cognitive reading task. We show that PLM-AS captures cognitive signals from eye-tracking data and shows improved performance on sentiment classification within and across three datasets of different domains.",
  "full_text": "PLM-AS: Pre-trained Language Models Augmented with\nScanpaths for Sentiment Classification\nDuo Yang∗ and Nora Hollenstein\nCenter for Language Technology, University of Copenhagen\nAbstract\nRecent research demonstrated that deep neural net-\nworks could generate meaningful feature represen-\ntations from both eye-tracking data and sentences\nwithout designing handcrafted features, which\nachieved competitive performance across cognitive\nNLP tasks, such as sentiment classification over\ngaze datasets, but the previous works mainly en-\ncode the text and gaze data separately without con-\nsidering the interaction between these two modali-\nties or applying large-scaled pre-trained models. To\naddress these challenges, we introduce PLM-AS, a\nnovel framework to take full advantage of textual\nand eye-tracking features by sequence modeling in\na highly interactive way for multimodal fusion. It is\nalso the first attempt to combine large-scaled pre-\ntrained models with eye-tracking features in the\ncognitive reading task. We show that PLM-AS cap-\ntures cognitive signals from eye-tracking data and\nshows improved performance in sentiment classifi-\ncation within and across three datasets of different\ndomains.\n1 Introduction\nRecent research studies have shown that eye-\ntracking features reflect cognitive information and\nlead to stable improvement in natural language\ntasks [1, 12, 16, 24, 28], such as sentiment clas-\nsification [14, 17], sarcasm detection [18], named\nentity recognition [8], coreference resolution [3]. It\ncan be mainly explained in the following aspects:\n1) Entities, lengthy and complex words could catch\nattention more easily than common words in terms\nof lexical level. 2) While implicit expressions may\n∗Corresponding Author: yd0301@outlook.com\nalso lead to a longer duration of fixation with a sec-\nond check in terms of semantic level when reading\nthe content. 3) Sentiment judgment is an auxiliary\ntask to content comprehension [2], the participants\nwill face difficulty in comprehension and will review\nthe whole sentence several times due to the complex\nphrasal structure in terms of syntactic level [11].\nIn all these aspects, human gaze data can provide\na wealth of cognitive information for content com-\nprehension and support sentiment classification at\nthe same time.\nThere have been breakthroughs in many fields\nby the advances in deep neural architectures in the\nrecent decade, research on how to model text and\nhuman gaze data with deep neural networks was\nalso conducted. A convolutional neural network\nwas first applied to learn feature representations\nfrom both text and human gaze data [19], and the\ngaze component in their model handled with two\nfundamental eye-tracking attributes, including fix-\nation and saccade. Another multi-task deep neu-\nral framework based on recurrent neural network\nLSTM also achieved competitive performance with\ngaze features [20]. The sentence-level attention cor-\nresponding to fixated words and adjacent words\ncould also be applied to sentiment classification [2].\nThese works have limited capabilities from two as-\npects: (a) The text and gaze representations were\nlearned by these neural networks without any in-\nteractions between two modalities, and the mod-\nels just concatenated the final outputs for multi-\nmodal fusion (b) It is difficult to apply large-scaled\ntransformer architecture directly within such a two-\ntower framework.\nIn this paper, instead of encoding two differ-\nent modalities separately with neural networks, we\npropose a novel neural network structure that al-\nlows us to encode text modality first and then per-\nhttps://doi.org/10.7557/18.6797\n© The author(s). Licensee Septentrio Academic Publishing, Tromsø, Norway. This is an open access article distributed\nunder the terms and conditions of the Creative Commons Attribution license\n(http://creativecommons.org/licenses/by/4.0/).\n1\nform sequence modeling leveraging the fixation or-\nder of words from gaze scanpaths intuitively with\nthe support of pre-trained language models. We\nuse the abbreviation PLM-AS for our proposed\nmodel Pre-trained Language Models Augmented\nwith Scanpaths in the paper. We conduct various\nexperiments for evaluation based on different gaze\ndatasets, ETSA-I Dataset, ETSA-II Dataset and\nZuCo Dataset released by [9, 11, 18], respectively.\nIn summary, our contributions in this work are:\n(1) We introduce a novel framework PLM-AS,\ncombing the text representation with eye-tracking\ndata in a more intuitive way than previous early-\nfusion models, specifically by leveraging the pro-\ncessing information encoded in the fixation order.\n(2) It is also the first attempt to combine contextu-\nalized embeddings from pre-trained language mod-\nels with eye-tracking data over gaze datasets on\nsentiment classification.\n(3) We conduct a series of controlled studies by\norganizing the outputs from pre-trained language\nmodels to investigate the impact of eye fixations\ntowards the framework, e.g., fixation words, fixa-\ntion order.\n(4) We also test the cross-corpus capabilities of this\nPLM-AS framework and analyze the results in the\naspect of generalization.\n2 Motivation\nThe concept of scanpath is first proposed by [22],\nwhich refers to the trajectories (paths) of the eyes\nwhen scanning the visual field and viewing and\nanalyzing any kind of visual information. When\nit comes to human reading [25], the scanpath\nmainly demonstrates the sequence of eye fixations\n(50–1500 ms pause of viewing a fragment of text),\nrevealing the saccades (a quick movement between\ntwo or more phases of fixation in the same direc-\ntion) and the regression (backward saccade to a\npreviously visited fragment).\nThe inspiration for our proposed model is that\nhuman reading is not a linear process in only one\ndirection strictly, but the trajectory of the eye-\nmovements could still be organized as the time se-\nries of eye fixations, and the composition of this\nnew sequence is highly overlapped with the text it-\nself, which means that we could represent the fixa-\ntion sequence using different fragments of text, and\nit should work naturally in recurrent neural net-\nFigure 1: The scanpath of reading the sentence S\nfrom ETSA-II Dataset [18]. The fixation sequence\nrecords the positions of fixation words in the sen-\ntence, following the time series of eye fixations.\nworks [26], e.g. GRU architecture [5], due to its\nsequential nature. Since the annotations are evalu-\nated by the subjects, this means that the cognitive\ninformation would be automatically included in the\nscanpaths when they read the sentences. Applying\nthe eye-tracking scanpaths into the deep neural net-\nworks directly is equivalent to combining the cog-\nnitive features with the corresponding text features\nin a more intuitive way.\nOur framework would follow this way: 1) Firstly,\ncontextualized word representation is generated\nby transformer architecture over the reading sen-\ntences, and we take full advantage of the final layer\nfrom BERT [7] as text representation; 2) By re-\ntrieving the features of the corresponding positions\nstep by step from BERT according to the index\nsequences of fixated words (scanpaths), we would\nhave the text feature sequences in the fixation or-\nder; 3) Since recurrent neural network is designed\nfor sequence modeling in deep learning, the new\ngenerated feature sequences are regarded as the in-\nput of the scanpath encoder, the GRU architecture\n[4], for the final multimodal fusion; 4) According to\nthe actual length of the fixation sequence, the out-\nput of scanpath encoder in the final step is picked\nup for sentiment polarity prediction.\n2\nFigure 2: PLM-AS: Pre-trained Language Models Augmented with Scanpaths\n3 Proposed Model\nIn this section, we introduce the PLM-AS frame-\nwork with contextualized representation and the\nscanpath encoder, and give the details on loss func-\ntions over different label settings for sentiment clas-\nsification.\nText Representation : Given a sentence S,\neach word would be cut into subword level by\nwordpiece tokenizer before the BERT architecture.\nThen we have subword sequencex0 with two special\ntokens [CLS] and [SEP] inserted at the beginning\nand the end of the sequence, respectively.\nx0 = [[CLS], w1, ..., wS−1, [SEP ]] (1)\nEach token could be represented by the con-\ncatenation of word embedding, position embed-\nding, and segment embedding, and undergoes bidi-\nrectional multi-head self-attention across multiple\ntransformer blocks:\nx\n′\nl = MSA (LN(xl−1)) +xl−1, l= 1...L (2)\nxl = MLP (LN(x\n′\nl)) +x\n′\nl, l= 1...L (3)\nwhere\nMSA (X) =Watt[Att1(X), ..., Attm(X)]⊤ (4)\nAtti(X) = softmax((WQiX)⊤WKi X)p\nD/m\n(WVi X)⊤ (5)\nFinally, we have the output sequence v from the fi-\nnal layer of BERT as a text representation (P refers\nto the dimension of hidden layer in BERT):\nv = xL (6)\nScanpath Encoder : Given a subword-based\nfixation index sequence f, we retrieve the features\nof the corresponding position step by step from the\noutput sequence v according to the index sequence\nof fixated words, then generate the new scanpath\nfeature sequence s (N refers to the set of the fixa-\ntion word index)\nf = [f1, f2, f3, ..., fm], fi ∈ N (7)\nsi = vfi , i ∈ M (8)\ns = [s1, s2, s3, ..., sm], si ∈ RP (9)\nThe scanpath feature sequence s is then passed to\nthe GRU architecture and we have the output se-\nquence o from the scanpath encoder: ( Q refers to\nthe output dimension of the scanpath encoder)\noi = GRUscanpath(si), i ∈ M (10)\no = [o1, o2, o3, ..., om], oi ∈ RQ (11)\nFinally, we use the output in the last time step\nfor training and evaluation ( t refers to the actual\nlength of the fixation index sequence).\nofinal = ot (12)\nSentiment Polarity Classification : For the\nfinal classification, we take the outputs of GRU in\nthe last step as the final features, according to the\nactual length of fixation index sequence.\n3\n1) Binary classification : The feature vector is\nthen passed to the linear layer with a sigmoid acti-\nvation function to predict the sentiment label{0,1},\npositive or negative.\nLBCE = − 1\nN\nNX\ni=1\nyi · log(ˆy) + (1− yi) · log(1 − ˆy) (13)\nWe optimize the model with binary cross-entropy\nloss between the true labels and the predicted val-\nues during the training stage.\n2) Multi-label classification The feature vector\nis passed to the linear layer with a softmax function,\nwe pick up the index with the highest probability\nas the sentiment label {0,1,2}, positive, negative,\nor neutral.\nLCE = − 1\nN\nNX\ni=1\n3X\nj=1\nyij · log(ˆyj) (14)\nWe optimize the model with softmax cross-\nentropy loss between the true labels and the pre-\ndicted values during the training stage.\n4 Experiment Setup\n4.1 Datasets\nAll the experiments followed the principle: a pair of\none single scanpath and one sentence was treated\nas an example, instead of multiple reading scan-\npaths with one sentence, so we reconstructed the\nexamples in this way over three cognitive datasets.\nApart from that, we removed some examples with\nsenseless annotation results from readers.\nETSA-I: We also worked on another cogni-\ntive reading dataset, Eye-Tracking and Sentiment\nAnalysis-I, which have been used by [11] for senti-\nment classification. The dataset contains 1059 sen-\ntences in total from movie reviews and tweets, and\nthe annotations come from five subjects, including\neye-tracking data recorded by a remote eye-tracker\nTobii TX 300 with sentiment labels (positive, neg-\native, and neutral) for each sentence.\nETSA-II: We first applied our proposed frame-\nwork based on the cognitive reading dataset re-\nleased by [18]. The original dataset, Eye-Tracking\nand Sentiment Analysis-II Dataset, mainly supple-\nmented with advanced eye-movement information\nover NLP dataset, contains fixation sequence data\nwith 383 positive and 611 negative sentences, in-\ncluding sarcastic quotes, short movie reviews, and\ntweets. Eye-tracking data from 7 subjects are all\nincluded for each sentence, recorded by an SR-\nResearch Eyelink-1000 eye-tracker during the read-\ning.\nZuCo: Experiments were also carried out for\ncross-domain learning based on a cognitive dataset,\nthe Zurich Cognitive Language Processing Corpus\nreleased by [9], combining EEG and eye-tracking\nrecordings from subjects reading natural sentences\nas a resource for the investigation of the human\nreading process in adult English native speakers.\nThis dataset includes simultaneous EEG and eye-\ntracking signals collected from 12 subjects during\nnatural text reading, but in this case, we just ex-\ntracted the textual features and the gaze features.\nThe gaze data was recorded by an SR-Research\nEyelink-1000 Plus eye-tracker. The corpus contains\n400 sentences in total, of which 140 are positive, 123\nare negative, and 137 are neutral, including movie\nreviews and biographical sentences.\n4.2 Parameter Settings\nAs for ETSA-I and ETSA-II datasets, we sim-\nply split the dataset into two subsets, 90% of the\ndataset are treated as training samples, while 10%\nof them are used for validation. We follow the\ninstruction in [21] to perform 25 runs for each\nmodel setting with the different random initializa-\ntion, using the same data split and the same hyper-\nparameter settings, and the final results are aver-\naged over these runs. The training is performed\nfor 20 epochs with the batch size of 16, we adopt\nthe AdamW optimizer by [15] with a learning rate\nof 0.0002 to minimize the loss and the default set-\ntings in PyTorch framework are kept unchanged,\nthe learning rate is linearly increased for the first\n10% of steps and linearly decayed to zero afterward.\nAll these settings are applied to BERT and the\nscanpath encoder equally. The scanpath encoder is\ndesigned as a single-direction GRU with one recur-\nrent layer, the hidden size of GRU is set to 768, and\nthe dropout with 0.1 are applied to the recurrent\nlayer in GRU. We initialize the hidden state of scan-\npath encoder by using the special token [CLS] out-\nputs from the final layer of BERT. Our implementa-\ntion uses the PyTorch framework, and pre-trained\nmodels are loaded from HuggingFace Transform-\n4\nETSA-II ETSA-I\nConfiguration P R F P R F\nTraditional\nsystems based on\ntextual features\n* Na¨ ıve Bayes 63.0 59.4 61.14 50.7 50.1 50.39\nMulti-layered Perceptron 69.0 69.2 69.2 66.8 66.8 66.8\nSVM (Linear Kernel) 72.8 73.2 72.6 70.3 70.3 70.3\nCNN architectures*\n[19]\nText only 72.17 70.91 71.53 60.51 59.66 60.08\nGaze only 65.2 60.35 62.68 52.52 51.49 52.0\nText and Gaze 79.89 74.86 77.3 63.93 60.13 62.0\nBERT Text only 89.81 91.67 89.74 83.61 82.75 82.95\nPLM-AS Text and Gaze 90.09 91.75 90.48 84.34 83.6 83.82\nTable 1: Performance evaluation over cognitive reading datasets [11, 18]. Except for removing a few\nnoisy samples, we applied the same way to split the datasets as the previous work (*) did in [19]. We\nreport macro-averaged precision (P), recall (R), and F1 score (F).\ners [27], an open source machine learning library in\nPython.\n5 Performance Evaluation\nSimilar to previous cognitive studies in [19], we\nevaluate the PLM-AS over two cognitive reading\ndatasets for sentiment classification task. The goal\nof our experiments is to investigate if the proposed\nmodel could take full advantage of textual and eye-\ntracking features for multimodal fusion over sen-\ntiment classification task and analyze where the\nimprovement comes from by controlled baselines.\nTable 1 presents the performance of the previous\nworks and our proposed model. In addition, we\nalso evaluate our proposed model in cross-domain\nlearning over three different datasets, shown in Ta-\nble 3.\nSingle modality vs. Multimodality : The\nprevious works in [19] show that CNN architectures\nlearned from both text and eye-tracking data out-\nperform those settings with single modality only.\nHowever, applying large-scaled pre-trained models\nhas become the mainstream approach across differ-\nent natural language tasks. The results show that\nthe BERT model become another strong baseline\non this task, even with text input only, but our\nproposed framework, PLM-AS, could perform mul-\ntimodal fusion and beat the new baseline over both\nthese datasets by taking advantages of large-scaled\npre-trained models and gaze features at the same\ntime. It would always be good to replace the BERT\nwith other advanced pre-trained models for text\nrepresentation, e.g. RoBERTa in [13] to achieve\nmore gains over all these related settings, but it is\nnot our main research purpose here.\nEffect of fixation words (a) : We consider\nthe fixation words are selected subconsciously by\nthe human cognitive process during reading, con-\ntributing to sentiment judgments after the content\ncomprehension [2]. We also question if our pro-\nposed model could work smoothly with random\nword choices instead of this kind of certain word\nchoices from human. To further investigate this\nquestion in PLM-AS, we carried out our first con-\ntrolled baseline by randomly shuffling the BERT\noutputs before feeding them into the scanpath en-\ncoder, the results, Table 2, show that the perfor-\nmance of PLM-AS is better than the setting (a)\nover both datasets, to some extent, all these word\nchoices selected during the natural reading by hu-\nman share the common ground in cognition and\nsupport the sentiment judgments within our pro-\nposed model.\nEffect of fixation order (b) : The core idea of\nour proposed model is to capture the eye-tracking\nfeatures by the fixation sequences, which provide\ncognitive information about the word choices and\nfixation order. To better understand the impact of\nthe fixation order in PLM-AS, we try to shuffle the\nfixation order before feeding them into the scan-\npath encoder but with the word choices remained.\nUnsurprisingly, PLM-AS is better than the shuf-\nfled setting (b) from Table 2, which means that the\nfixation words could not contribute to the overall\nperformance individually without the order infor-\nmation in PLM-AS, at least not in such a RNN\nsequential model setting [26] of the scanpath en-\n5\nETSA-II ETSA-I\nConfiguration P R F P R F\nBERT Text only 89.81 91.67 89.74 83.61 82.75 82.95\nPLM-AS Text and Gaze 90.09 91.75 90.48 84.34 83.6 83.82\nOther baselines\nShuffle BERT outputs (a) 89 91.1 89.26 83.56 82.3 82.73\nShuffle fixation sequence (b) 89.38 91.45 89.81 83.4 82.51 82.81\nReplace fix. sequence with natural text (c) 89.35 91.5 89.78 84.05 83.13 83.38\nTable 2: Performance evaluation based on controlled baselines. Noted that the text inputs of pre-\ntrained language model stay the same without any shuffle or replacements in order to provide the text\nrepresentation across all these settings, but in the next stage of encoding with GRU: (a) we create a\nsubword-based index sequence with random words from the sentence to replace the fixation sequence; (b)\nwe create another index sequence by shuffling the fixation sequence; (c) we replace the fixation sequence\nwith natural text, the same order as text inputs for the pre-trained language model.\ncoder. RNN architecture might not be the only\noption for modeling the fixation feature sequences,\nespecially in capturing the order information, but\nwe left it to future research.\nEffect of encoder architecture (c) : We also\nquestion that the improvement might come from\nthe scanpath encoder itself rather than the eye-\ntracking features, so we carried out our third con-\ntrolled baseline by replacing the fixation sequences\nwith word sequences of natural text, the only differ-\nence between this setting and BERT is by adding an\nextra GRU architecture, and it becomes a text-only\nsetting. The results in Table 2 show that the perfor-\nmance of this setting (c) is close to the BERT base-\nline but lower than the performance of PLM-AS,\nwhich indicates the GRU architecture itself with-\nout any cognitive features might not contribute a\nlot to the overall performance of PLM-AS.\nCross-domain evaluation: Apart from these\ncontrolled baselines, we also perform a cross-\ndomain evaluation based on the ZuCo dataset. The\nresults in Table 3 show that our PLM-AS frame-\nwork can achieve more competitive cross-domain\nperformance to the BERT baseline while the mod-\nTrain Test Models P R F\nETSA-I ZuCo BERT 87.97 87.47 86.9\nPLM-AS 87.66 86.5 85.77\nETSA-II ZuCo BERT 67.11 67.67 62.66\nPLM-AS 68.7 68.53 64\nTable 3: Cross-domain evaluation over three\ndatasets.\nels are trained on ETSA-II Dataset rather than\nETSA-I Dataset. Noted that the reading texts in\nthe ETSA-II Dataset are collected from two popu-\nlar sarcastic quote websites, Tweet and the Amazon\nMovie Corpus, [23], with a higher level of complex-\nity and diversity than the ETSA-I Dataset and the\nZuCo dataset. Nearly half of the reading texts in\nthe ETSA-II Dataset are sarcastic, it could be as-\nsumed that the eye-tracking data (scanpaths) in\nETSA-II Dataset would be more abundant and di-\nverse, which improve the overall performance. In\naddition, human scanpaths might vary not only\nfrom the text domains but also from person to per-\nson due to reading behaviors. Instead of provid-\ning eye-tracking features on average across all the\nsubjects at a time, our PLM-AS framework might\nlearn the reading patterns from a certain group of\nsubjects and face challenges in generalizing these\nlearned reading patterns to other subjects in such a\nsubject-based sample construction. When it comes\nto the testing stage of cross-domain measurements,\nto some extent, the inconsistency between datasets’\nsubjects should also be considered for the undesir-\nable results when the models are trained on the\nETSA-I Dataset.\n6 Conclusion\nIn this paper, we propose a novel framework to fully\ncombine text representations with eye-tracking fea-\ntures by scanpath modeling and carry out experi-\nments to evaluate our model (PLM-AS) for sen-\ntiment classification. The results show that PLM-\nAS captures cognitive signals from the eye-tracking\ndata and shows improved performance on senti-\n6\nment classification within and across three datasets\nof different domains. This indicates that the order\nof fixation during text reading carries linguistic in-\nformation that is useful for NLP tasks.\nSince all the experiments are carried out on small\ndatasets with limited texts and unstable results are\nobserved when applying large-scale language mod-\nels, we decide to follow the evaluation strategy in\n[21], to obtain convincing results. Since it is not al-\nways practical to obtain related eye-tracking data\nfor augmentation at the test time, many research\nstudies have been proposed for gaze feature predic-\ntion on text [10] and image [6] in recent years. How-\never, scanpath prediction on text has not yet been\nexplored sufficiently, which could be investigated\nas an auxiliary task over different NLP challenges\nduring training, to be free of this limitation.\nReferences\n[1] M. Barrett and N. Hollenstein. Sequence la-\nbelling and sequence classification with gaze:\nNovel uses of eye-tracking data for natural\nlanguage processing. Language and Linguis-\ntics Compass , 14(11):1–16, sep 2020. doi:\n10.1111/lnc3.12396.\n[2] X. Chen, J. Mao, Y. Liu, M. Zhang, and\nS. Ma. Investigating human reading behav-\nior during sentiment judgment. International\nJournal of Machine Learning and Cybernet-\nics, 13(8):2283–2296, mar 2022. doi: 10.1007/\ns13042-022-01523-9.\n[3] J. Cheri, A. Mishra, and P. Bhattacharyya.\nLeveraging annotators’ gaze behaviour for\ncoreference resolution. In Proceedings of\nthe 7th Workshop on Cognitive Aspects of\nComputational Language Learning, pages 22–\n26, Berlin, Aug. 2016. Association for Com-\nputational Linguistics. doi: 10.18653/v1/\nW16-1904.\n[4] K. Cho, B. van Merrienboer, D. Bahdanau,\nand Y. Bengio. On the properties of neu-\nral machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth\nWorkshop on Syntax, Semantics and Struc-\nture in Statistical Translation. Association for\nComputational Linguistics, 2014. doi: 10.\n3115/v1/w14-4012.\n[5] K. Cho, B. van Merrienboer, C. Gulcehre,\nD. Bahdanau, F. Bougares, H. Schwenk, and\nY. Bengio. Learning phrase representations us-\ning RNN encoder–decoder for statistical ma-\nchine translation. In Proceedings of the 2014\nConference on Empirical Methods in Natu-\nral Language Processing (EMNLP) . Associa-\ntion for Computational Linguistics, 2014. doi:\n10.3115/v1/d14-1179.\n[6] R. A. J. de Belen, T. Bednarz, and A. Sowmya.\nScanpathNet: A recurrent mixture den-\nsity network for scanpath prediction. In\n2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops\n(CVPRW). IEEE, jun 2022. doi: 10.1109/\ncvprw56347.2022.00549.\n[7] J. Devlin, M.-W. Chang, K. Lee, and\nK. Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the As-\nsociation for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long\nand Short Papers) , pages 4171–4186, Min-\nneapolis, Minnesota, June 2019. Association\nfor Computational Linguistics. doi: 10.18653/\nv1/N19-1423.\n[8] N. Hollenstein and C. Zhang. Entity recogni-\ntion at first sight:. In Proceedings of the 2019\nConference of the North. Association for Com-\nputational Linguistics, 2019. doi: 10.18653/\nv1/n19-1001.\n[9] N. Hollenstein, J. Rotsztejn, M. Troendle,\nA. Pedroni, C. Zhang, and N. Langer. ZuCo,\na simultaneous EEG and eye-tracking resource\nfor natural sentence reading. Scientific Data ,\n5(1), dec 2018. doi: 10.1038/sdata.2018.291.\n[10] N. Hollenstein, E. Chersoni, C. L. Jacobs,\nY. Oseki, L. Pr´ evot, and E. Santus. CMCL\n2021 shared task on eye-tracking prediction.\nIn Proceedings of the Workshop on Cognitive\nModeling and Computational Linguistics . As-\nsociation for Computational Linguistics, 2021.\ndoi: 10.18653/v1/2021.cmcl-1.7.\n7\n[11] A. Joshi, A. Mishra, N. Senthamilselvan, and\nP. Bhattacharyya. Measuring sentiment an-\nnotation complexity of text. In Proceedings\nof the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2:\nShort Papers). Association for Computational\nLinguistics, 2014. doi: 10.3115/v1/p14-2007.\n[12] S. Klerke, Y. Goldberg, and A. Søgaard. Im-\nproving sentence compression by learning to\npredict gaze. In Proceedings of the 2016\nConference of the North American Chapter\nof the Association for Computational Linguis-\ntics: Human Language Technologies . Associa-\ntion for Computational Linguistics, 2016. doi:\n10.18653/v1/n16-1179.\n[13] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi,\nD. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V. Stoyanov. RoBERTa: A robustly opti-\nmized BERT pretraining approach, 2020. doi:\n10.48550/arXiv.1907.11692.\n[14] Y. Long, L. Qin, R. Xiang, M. Li, and C.-R.\nHuang. A cognition based attention model for\nsentiment analysis. In Proceedings of the 2017\nConference on Empirical Methods in Natural\nLanguage Processing. Association for Compu-\ntational Linguistics, 2017. doi: 10.18653/v1/\nd17-1048.\n[15] I. Loshchilov and F. Hutter. Decoupled\nweight decay regularization. In 7th Inter-\nnational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019 . OpenReview.net, 2019. doi:\n10.48550/arXiv.1711.05101.\n[16] S. Mathias, D. Kanojia, A. Mishra, and\nP. Bhattacharya. A survey on using gaze\nbehaviour for natural language processing.\nIn Proceedings of the Twenty-Ninth Interna-\ntional Joint Conference on Artificial Intelli-\ngence, IJCAI-20 , pages 4907–4913. Interna-\ntional Joint Conferences on Artificial Intelli-\ngence Organization, 2020. doi: 10.24963/ijcai.\n2020/683.\n[17] E. S. McGuire and N. Tomuro. Sentiment\nanalysis with cognitive attention supervision.\nProceedings of the Canadian Conference on\nArtificial Intelligence , jun 2021. doi: 10.\n21428/594757db.90170c50.\n[18] A. Mishra and P. Bhattacharyya. Predicting\nreaders’ sarcasm understandability by mod-\neling gaze behavior. In Cognitively Inspired\nNatural Language Processing , pages 99–115.\nSpringer Singapore, 2018. doi: 10.1007/\n978-981-13-1516-9 5.\n[19] A. Mishra, K. Dey, and P. Bhattacharyya.\nLearning cognitive features from gaze data for\nsentiment and sarcasm classification using con-\nvolutional neural network. In Proceedings of\nthe 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long\nPapers). Association for Computational Lin-\nguistics, 2017. doi: 10.18653/v1/p17-1035.\n[20] A. Mishra, S. Tamilselvam, R. Dasgupta,\nS. Nagar, and K. Dey. Cognition-cognizant\nsentiment analysis with multitask subjectiv-\nity summarization based on annotators’ gaze\nbehavior. Proceedings of the AAAI Confer-\nence on Artificial Intelligence, 32(1), 2018. doi:\n10.1609/aaai.v32i1.12068.\n[21] M. Mosbach, M. Andriushchenko, and\nD. Klakow. On the stability of fine-tuning\nBERT: Misconceptions, explanations, and\nstrong baselines. In International Conference\non Learning Representations , 2021. doi:\n10.48550/arXiv.2006.04884.\n[22] D. Noton and L. Stark. Scanpaths in eye\nmovements during pattern perception. Sci-\nence, 171(3968):308–311, jan 1971. doi: 10.\n1126/science.171.3968.308.\n[23] B. Pang and L. Lee. Seeing stars: Exploiting\nclass relationships for sentiment categorization\nwith respect to rating scales. In Proceedings\nof the 43rd Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL’05) ,\npages 115–124, Ann Arbor, Michigan, June\n2005. Association for Computational Linguis-\ntics. doi: 10.3115/1219840.1219855.\n[24] B. Plank. Keystroke dynamics as signal for\nshallow syntactic parsing. In Proceedings of\nCOLING 2016, the 26th International Con-\nference on Computational Linguistics: Techni-\ncal Papers, pages 609–619, Osaka, Japan, Dec.\n8\n2016. The COLING 2016 Organizing Commit-\ntee. doi: 10.48550/arXiv.1610.03321.\n[25] K. Rayner. Eye movements in reading and\ninformation processing: 20 years of research.\nPsychological Bulletin , 124(3):372–422, 1998.\ndoi: 10.1037/0033-2909.124.3.372.\n[26] D. E. Rumelhart, G. E. Hinton, and R. J.\nWilliams. Learning representations by back-\npropagating errors. Nature, 323(6088):533–\n536, oct 1986. doi: 10.1038/323533a0.\n[27] T. Wolf, L. Debut, V. Sanh, J. Chaumond,\nC. Delangue, A. Moi, P. Cistac, T. Rault,\nR. Louf, M. Funtowicz, et al. Transform-\ners: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Pro-\ncessing: System Demonstrations , pages 38–\n45, Online, Oct. 2020. Association for Compu-\ntational Linguistics. doi: 10.18653/v1/2020.\nemnlp-demos.6.\n[28] R. Zhang, A. Saran, B. Liu, Y. Zhu, S. Guo,\nS. Niekum, D. Ballard, and M. Hayhoe. Hu-\nman gaze assisted artificial intelligence: A re-\nview. In Proceedings of the Twenty-Ninth In-\nternational Joint Conference on Artificial In-\ntelligence. International Joint Conferences on\nArtificial Intelligence Organization, jul 2020.\ndoi: 10.24963/ijcai.2020/689.\n9",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7967308163642883
    },
    {
      "name": "Eye tracking",
      "score": 0.7287386655807495
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6233800053596497
    },
    {
      "name": "Gaze",
      "score": 0.6179249882698059
    },
    {
      "name": "ENCODE",
      "score": 0.5057501792907715
    },
    {
      "name": "Task (project management)",
      "score": 0.4784422218799591
    },
    {
      "name": "Modalities",
      "score": 0.4781304895877838
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4439300000667572
    },
    {
      "name": "Machine learning",
      "score": 0.36770856380462646
    },
    {
      "name": "Natural language processing",
      "score": 0.3590432405471802
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I124055696",
      "name": "University of Copenhagen",
      "country": "DK"
    }
  ],
  "cited_by": 10
}