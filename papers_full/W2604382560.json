{
  "title": "Learning Simpler Language Models with the Delta Recurrent Neural Network Framework.",
  "url": "https://openalex.org/W2604382560",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A5084332360",
      "name": "Alexander G. Ororbia",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020917394",
      "name": "Tomáš Mikolov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5020881468",
      "name": "David Reitter",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2100649405",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2141703670",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3133056632",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2027857109",
    "https://openalex.org/W2953061907",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2164418233",
    "https://openalex.org/W2149359396",
    "https://openalex.org/W2319453305",
    "https://openalex.org/W2469894155",
    "https://openalex.org/W1589104004",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2949626814",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2781217231",
    "https://openalex.org/W2115197214"
  ],
  "abstract": "Learning useful information across long time lags is a critical and difficult\nproblem for temporal neural models in tasks like language modeling. Existing\narchitectures that address the issue are often complex and costly to train. The\nDelta Recurrent Neural Network (Delta-RNN) framework is a simple and\nhigh-performing design that unifies previously proposed gated neural models.\nThe Delta-RNN models maintain longer-term memory by learning to interpolate\nbetween a fast-changing data-driven representation and a slowly changing,\nimplicitly stable state. This requires hardly any more parameters than a\nclassical simple recurrent network. The models outperform popular complex\narchitectures, such as the Long Short Term Memory (LSTM) and the Gated\nRecurrent Unit (GRU) and achieve state-of-the art performance in language\nmodeling at character and word levels and yield comparable performance at the\nsubword level.",
  "full_text": null,
  "topic": "Recurrent neural network",
  "concepts": [
    {
      "name": "Recurrent neural network",
      "score": 0.9010472893714905
    },
    {
      "name": "Computer science",
      "score": 0.7938718199729919
    },
    {
      "name": "Language model",
      "score": 0.6314411759376526
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6050182580947876
    },
    {
      "name": "Representation (politics)",
      "score": 0.5809323787689209
    },
    {
      "name": "Artificial neural network",
      "score": 0.5381404161453247
    },
    {
      "name": "Deep learning",
      "score": 0.530274510383606
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.49616679549217224
    },
    {
      "name": "Machine learning",
      "score": 0.4562044143676758
    },
    {
      "name": "Word (group theory)",
      "score": 0.4212418794631958
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}